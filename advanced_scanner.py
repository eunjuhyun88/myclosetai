#!/usr/bin/env python3
"""
ğŸ”¥ ì™„ì „í•œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²€ìƒ‰ ìŠ¤í¬ë¦½íŠ¸ - ìˆ˜ì •ëœ ë²„ì „
=======================================================

MyCloset AI í”„ë¡œì íŠ¸ì— íŠ¹í™”ëœ ì™„ì „í•œ ëª¨ë¸ ìŠ¤ìºë„ˆ
- ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ì¶° ì •í™•í•œ ê²½ë¡œ íƒì§€
- conda í™˜ê²½ ìš°ì„  ê²€ìƒ‰
- MyCloset AI 8ë‹¨ê³„ ìë™ ë¶„ë¥˜
- ì™„ì „í•œ ë³´ê³ ì„œ ë° ì„¤ì • íŒŒì¼ ìƒì„±

ì‚¬ìš©ë²•:
    python quick_scanner.py                    # ê¸°ë³¸ ìŠ¤ìº”
    python quick_scanner.py --verbose          # ìƒì„¸ ì¶œë ¥
    python quick_scanner.py --organize         # ìŠ¤ìº” + ì„¤ì • ìƒì„±
    python quick_scanner.py --deep             # ë”¥ ìŠ¤ìº”
    python quick_scanner.py --conda-first      # conda ìš°ì„ 
"""

import os
import sys
import json
import shutil
import time
import hashlib
import subprocess
import platform
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime
import re
import glob
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ModelInfo:
    """ì™„ì „í•œ ëª¨ë¸ ì •ë³´"""
    name: str
    path: str
    absolute_path: str
    size_mb: float
    size_gb: float
    framework: str
    model_type: str
    step_candidate: str
    confidence: float
    is_valid: bool
    is_in_project: bool
    is_in_conda: bool
    conda_env_name: Optional[str]
    parent_directory: str
    created_time: str
    modified_time: str
    checksum: str
    companion_files: List[str]
    importance_score: float
    extension: str

@dataclass 
class ScanStatistics:
    """ìŠ¤ìº” í†µê³„"""
    total_files_scanned: int = 0
    models_found: int = 0
    total_size_gb: float = 0.0
    scan_duration: float = 0.0
    conda_models: int = 0
    project_models: int = 0
    system_models: int = 0
    valid_models: int = 0
    framework_distribution: Dict[str, int] = None
    step_distribution: Dict[str, int] = None
    
    def __post_init__(self):
        if self.framework_distribution is None:
            self.framework_distribution = {}
        if self.step_distribution is None:
            self.step_distribution = {}

class CompleteModelScanner:
    """ì™„ì „í•œ AI ëª¨ë¸ ìŠ¤ìºë„ˆ"""
    
    def __init__(self, verbose: bool = True, conda_first: bool = False, deep_scan: bool = False):
        self.verbose = verbose
        self.conda_first = conda_first
        self.deep_scan = deep_scan
        self.scan_start_time = time.time()
        
        # í˜„ì¬ ìœ„ì¹˜ ë° í”„ë¡œì íŠ¸ êµ¬ì¡° íŒŒì•…
        self.current_dir = Path.cwd()
        self.project_root = self._find_project_root()
        self.ai_models_dir = self._find_ai_models_dir()
        
        # ê²°ê³¼ ì €ì¥
        self.found_models: List[ModelInfo] = []
        self.scan_locations: Dict[str, List[str]] = {}
        self.errors: List[str] = []
        self.warnings: List[str] = []
        
        # conda í™˜ê²½ íƒì§€
        self.conda_environments = self._detect_conda_environments()
        self.current_conda_env = os.environ.get('CONDA_DEFAULT_ENV')
        
        # ëª¨ë¸ ë¶„ë¥˜ íŒ¨í„´ ì´ˆê¸°í™”
        self._init_classification_patterns()
        
        logger.info(f"ğŸš€ CompleteModelScanner ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {self.project_root}")
        logger.info(f"ğŸ¤– AI ëª¨ë¸ ë””ë ‰í† ë¦¬: {self.ai_models_dir}")
        logger.info(f"ğŸ conda í™˜ê²½: {len(self.conda_environments)}ê°œ ë°œê²¬")
        
    def _find_project_root(self) -> Path:
        """í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì •í™•íˆ ì°¾ê¸°"""
        current = self.current_dir
        
        # í˜„ì¬ ë””ë ‰í† ë¦¬ê°€ mycloset-aiì¸ ê²½ìš°
        if current.name == 'mycloset-ai':
            return current
        
        # backend ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰ëœ ê²½ìš°
        if current.name == 'backend':
            if (current.parent / 'frontend').exists():
                return current.parent
            return current
        
        # ë¶€ëª¨ ë””ë ‰í† ë¦¬ë“¤ ê²€ì‚¬
        for parent in current.parents:
            if parent.name == 'mycloset-ai':
                return parent
            # backendì™€ frontendê°€ ê°™ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ì°¾ê¸°
            if (parent / 'backend').exists() and (parent / 'frontend').exists():
                return parent
        
        # ê¸°ë³¸ê°’ìœ¼ë¡œ í˜„ì¬ ë””ë ‰í† ë¦¬
        return current
    
    def _find_ai_models_dir(self) -> Optional[Path]:
        """AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ì°¾ê¸° (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)"""
        candidates = [
            # 1ìˆœìœ„: backend ë‚´ë¶€
            self.project_root / "backend" / "ai_models",
            self.current_dir / "ai_models",  # backendì—ì„œ ì‹¤í–‰ ì‹œ
            
            # 2ìˆœìœ„: í”„ë¡œì íŠ¸ ë£¨íŠ¸
            self.project_root / "ai_models",
            
            # 3ìˆœìœ„: í˜„ì¬ ìœ„ì¹˜ ê¸°ì¤€
            self.current_dir / "backend" / "ai_models",
            self.current_dir.parent / "backend" / "ai_models",
            
            # 4ìˆœìœ„: ê¸°íƒ€
            self.project_root / "models",
            self.project_root / "checkpoints"
        ]
        
        for candidate in candidates:
            if candidate.exists() and candidate.is_dir():
                # ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
                try:
                    if any(candidate.iterdir()):
                        return candidate
                except PermissionError:
                    continue
        
        return None
    
    def _detect_conda_environments(self) -> Dict[str, Path]:
        """conda í™˜ê²½ ìë™ íƒì§€"""
        environments = {}
        
        try:
            # conda infoë¡œ í™˜ê²½ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            result = subprocess.run(
                ['conda', 'env', 'list', '--json'],
                capture_output=True, text=True, timeout=10
            )
            
            if result.returncode == 0:
                env_data = json.loads(result.stdout)
                for env_path in env_data.get('envs', []):
                    env_name = Path(env_path).name
                    environments[env_name] = Path(env_path)
                    
        except (subprocess.SubprocessError, json.JSONDecodeError, FileNotFoundError):
            # conda ëª…ë ¹ì–´ ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ íƒì§€
            conda_bases = [
                Path.home() / "miniconda3" / "envs",
                Path.home() / "anaconda3" / "envs", 
                Path("/opt/anaconda3/envs"),
                Path("/opt/miniconda3/envs")
            ]
            
            for base in conda_bases:
                if base.exists():
                    for env_dir in base.iterdir():
                        if env_dir.is_dir() and (env_dir / "bin" / "python").exists():
                            environments[env_dir.name] = env_dir
        
        return environments
    
    def _init_classification_patterns(self):
        """AI ëª¨ë¸ ë¶„ë¥˜ íŒ¨í„´ ì´ˆê¸°í™”"""
        
        # í”„ë ˆì„ì›Œí¬ íŒ¨í„´
        self.framework_patterns = {
            'pytorch': {
                'extensions': ['.pth', '.pt', '.bin'],
                'magic_bytes': [b'PK', b'\x80', b'PYTORCH'],
                'indicators': ['torch', 'pytorch', 'state_dict']
            },
            'safetensors': {
                'extensions': ['.safetensors'],
                'magic_bytes': [b'{"'],
                'indicators': ['safetensors', 'huggingface']
            },
            'tensorflow': {
                'extensions': ['.pb', '.h5', '.tflite'],
                'magic_bytes': [b'\x08', b'\x89HDF'],
                'indicators': ['tensorflow', 'keras', 'saved_model']
            },
            'onnx': {
                'extensions': ['.onnx'],
                'magic_bytes': [b'\x08\x01'],
                'indicators': ['onnx', 'opset']
            },
            'diffusers': {
                'extensions': ['.bin', '.safetensors'],
                'magic_bytes': [],
                'indicators': ['diffusion', 'unet', 'vae', 'scheduler']
            }
        }
        
        # MyCloset AI 8ë‹¨ê³„ íŒ¨í„´ (ì •êµí™”)
        self.step_patterns = {
            'step_01_human_parsing': {
                'patterns': [
                    r'human.*pars.*', r'graphonomy', r'schp', r'atr.*', r'lip.*',
                    r'parsing.*human', r'self.*correction.*human',
                    r'human.*segmentation', r'body.*parsing'
                ],
                'models': ['Graphonomy', 'Self-Correction-Human-Parsing', 'ATR', 'LIP'],
                'keywords': ['human', 'parsing', 'segmentation', 'body', 'person']
            },
            'step_02_pose_estimation': {
                'patterns': [
                    r'pose.*estimation', r'openpose', r'mediapipe.*pose', r'dwpose',
                    r'body.*pose', r'keypoint.*detection', r'skeleton.*detection',
                    r'pose.*net', r'human.*pose'
                ],
                'models': ['OpenPose', 'MediaPipe', 'DWPose', 'PoseNet'],
                'keywords': ['pose', 'keypoint', 'skeleton', 'joint', 'landmark']
            },
            'step_03_cloth_segmentation': {
                'patterns': [
                    r'cloth.*seg.*', r'u2net', r'sam.*', r'segment.*anything',
                    r'mask.*rcnn', r'deeplabv3', r'segmentation.*cloth',
                    r'garment.*seg.*', r'clothing.*mask'
                ],
                'models': ['U2Net', 'SAM', 'MaskRCNN', 'DeepLabV3'],
                'keywords': ['cloth', 'garment', 'clothing', 'mask', 'segment']
            },
            'step_04_geometric_matching': {
                'patterns': [
                    r'geometric.*match.*', r'gmm.*', r'tps.*', r'spatial.*transform',
                    r'warping.*grid', r'flow.*estimation', r'optical.*flow',
                    r'matching.*network'
                ],
                'models': ['GMM', 'TPS', 'FlowNet', 'PWCNet'],
                'keywords': ['geometric', 'matching', 'flow', 'transform', 'warp']
            },
            'step_05_cloth_warping': {
                'patterns': [
                    r'cloth.*warp.*', r'tom.*', r'viton.*warp', r'deformation',
                    r'elastic.*transform', r'thin.*plate.*spline', r'warping.*net',
                    r'garment.*warp.*'
                ],
                'models': ['TOM', 'VITON', 'TPS-Warp'],
                'keywords': ['warp', 'deformation', 'elastic', 'spline', 'transform']
            },
            'step_06_virtual_fitting': {
                'patterns': [
                    r'virtual.*fit.*', r'ootdiffusion', r'stable.*diffusion',
                    r'diffusion.*unet', r'try.*on', r'outfit.*diffusion',
                    r'viton.*hd', r'hr.*viton', r'virtual.*tryon'
                ],
                'models': ['OOTDiffusion', 'VITON-HD', 'HR-VITON', 'StableDiffusion'],
                'keywords': ['virtual', 'fitting', 'tryon', 'diffusion', 'generation']
            },
            'step_07_post_processing': {
                'patterns': [
                    r'post.*process.*', r'enhancement', r'super.*resolution',
                    r'sr.*net', r'esrgan', r'real.*esrgan', r'upscal.*',
                    r'denoise.*', r'refine.*', r'enhance.*'
                ],
                'models': ['ESRGAN', 'Real-ESRGAN', 'SRResNet', 'EDSR'],
                'keywords': ['enhancement', 'super', 'resolution', 'upscale', 'denoise']
            },
            'step_08_quality_assessment': {
                'patterns': [
                    r'quality.*assess.*', r'clip.*', r'aesthetic.*', r'scoring',
                    r'evaluation', r'metric.*', r'lpips', r'ssim', r'fid.*',
                    r'perceptual.*loss'
                ],
                'models': ['CLIP', 'LPIPS', 'FID', 'SSIM'],
                'keywords': ['quality', 'assessment', 'metric', 'evaluation', 'score']
            }
        }
        
        # ëª¨ë¸ íƒ€ì… íŒ¨í„´
        self.model_type_patterns = {
            'diffusion_model': [r'diffusion', r'stable.*diffusion', r'ootd', r'unet'],
            'clip_model': [r'clip', r'vit.*patch', r'vision.*transformer'],
            'pose_model': [r'pose', r'openpose', r'dwpose', r'keypoint'],
            'segmentation_model': [r'segment', r'u2net', r'mask', r'sam'],
            'parsing_model': [r'parsing', r'human.*parsing', r'atr', r'schp'],
            'warping_model': [r'warp', r'tom', r'tps', r'flow'],
            'checkpoint': [r'checkpoint', r'ckpt', r'epoch', r'step'],
            'config_file': [r'config', r'setup', r'tokenizer']
        }
    
    def scan_complete_system(self) -> List[ModelInfo]:
        """ì™„ì „í•œ ì‹œìŠ¤í…œ ìŠ¤ìº”"""
        logger.info("ğŸš€ ì™„ì „í•œ AI ëª¨ë¸ ì‹œìŠ¤í…œ ìŠ¤ìº” ì‹œì‘")
        logger.info("=" * 80)
        
        scan_start = time.time()
        
        # 1. ìŠ¤ìº” ê²½ë¡œ ìƒì„±
        scan_paths = self._generate_scan_paths()
        
        # 2. ë³‘ë ¬ ìŠ¤ìº” ì‹¤í–‰
        logger.info(f"ğŸ” {len(scan_paths)}ê°œ ìœ„ì¹˜ì—ì„œ ëª¨ë¸ ê²€ìƒ‰...")
        all_files = self._scan_all_paths(scan_paths)
        
        if not all_files:
            logger.warning("âŒ AI ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self._debug_scan_paths(scan_paths)
            return []
        
        # 3. ëª¨ë¸ ë¶„ì„
        logger.info(f"ğŸ§  {len(all_files):,}ê°œ íŒŒì¼ AI ë¶„ì„ ì¤‘...")
        self._analyze_all_models(all_files)
        
        # 4. í›„ì²˜ë¦¬
        self._post_process_results()
        
        # 5. ê²°ê³¼ ì¶œë ¥
        scan_duration = time.time() - scan_start
        self._print_complete_results(scan_duration)
        
        return self.found_models
    
    def _generate_scan_paths(self) -> List[Path]:
        """ìŠ¤ìº” ê²½ë¡œ ìƒì„± (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)"""
        paths = []
        
        # 1ìˆœìœ„: conda í™˜ê²½ (conda_first ì˜µì…˜ ì‹œ)
        if self.conda_first:
            for env_name, env_path in self.conda_environments.items():
                conda_paths = [
                    env_path / "lib" / "python3.11" / "site-packages",
                    env_path / "lib" / "python3.10" / "site-packages",
                    env_path / "share",
                    env_path / "models"
                ]
                paths.extend([p for p in conda_paths if self._is_accessible(p)])
        
        # 2ìˆœìœ„: í”„ë¡œì íŠ¸ ë‚´ë¶€
        project_paths = [
            self.ai_models_dir,
            self.project_root / "models",
            self.project_root / "checkpoints",
            self.project_root / "weights"
        ]
        paths.extend([p for p in project_paths if p and self._is_accessible(p)])
        
        # 3ìˆœìœ„: ì‚¬ìš©ì ë””ë ‰í† ë¦¬
        home = Path.home()
        user_paths = [
            home / "Downloads",
            home / "Documents",
            home / "Desktop",
            home / ".cache" / "huggingface",
            home / ".cache" / "torch",
            home / ".cache" / "diffusers",
            home / ".local" / "lib"
        ]
        paths.extend([p for p in user_paths if self._is_accessible(p)])
        
        # 4ìˆœìœ„: ì‹œìŠ¤í…œ ì „ì²´ (deep_scan ì˜µì…˜ ì‹œ)
        if self.deep_scan:
            system_paths = []
            system = platform.system().lower()
            
            if system == "darwin":  # macOS
                system_paths = [
                    Path("/opt/homebrew/lib"),
                    Path("/usr/local/lib"),
                    Path("/opt")
                ]
            elif system == "linux":
                system_paths = [
                    Path("/opt"),
                    Path("/usr/local/lib"),
                    Path("/usr/share")
                ]
            
            paths.extend([p for p in system_paths if self._is_accessible(p)])
        
        # conda í™˜ê²½ì´ conda_firstê°€ ì•„ë‹ ë•Œ ì¶”ê°€
        if not self.conda_first:
            for env_name, env_path in self.conda_environments.items():
                conda_paths = [
                    env_path / "lib" / "python3.11" / "site-packages",
                    env_path / "lib" / "python3.10" / "site-packages"
                ]
                paths.extend([p for p in conda_paths if self._is_accessible(p)])
        
        # ì¤‘ë³µ ì œê±°
        return self._remove_duplicate_paths(paths)
    
    def _is_accessible(self, path: Path) -> bool:
        """ê²½ë¡œ ì ‘ê·¼ ê°€ëŠ¥ì„± í™•ì¸"""
        try:
            if not path.exists():
                return False
            if not os.access(path, os.R_OK):
                return False
            
            # ë³´í˜¸ëœ ê²½ë¡œ ì œì™¸
            path_str = str(path).lower()
            protected = ['/system/', '/private/', '/dev/', '/proc/', 'keychain', 'security']
            return not any(p in path_str for p in protected)
            
        except (PermissionError, OSError):
            return False
    
    def _remove_duplicate_paths(self, paths: List[Path]) -> List[Path]:
        """ì¤‘ë³µ ê²½ë¡œ ì œê±°"""
        unique_paths = []
        sorted_paths = sorted(set(paths), key=lambda p: len(str(p)))
        
        for path in sorted_paths:
            is_child = False
            for existing in unique_paths:
                try:
                    if path != existing and path.is_relative_to(existing):
                        is_child = True
                        break
                except (ValueError, OSError):
                    continue
            
            if not is_child:
                unique_paths.append(path)
        
        return unique_paths
    
    def _scan_all_paths(self, scan_paths: List[Path]) -> List[Path]:
        """ëª¨ë“  ê²½ë¡œ ìŠ¤ìº”"""
        all_files = []
        
        for path in scan_paths:
            if self.verbose:
                logger.info(f"ğŸ“‚ ìŠ¤ìº” ì¤‘: {path}")
            
            files = self._scan_single_path(path)
            if files:
                all_files.extend(files)
                self.scan_locations[str(path)] = [str(f) for f in files]
                
                total_size = sum(f.stat().st_size for f in files if f.exists()) / (1024**2)
                if self.verbose:
                    logger.info(f"  âœ… {len(files)}ê°œ íŒŒì¼ ë°œê²¬ ({total_size:.1f}MB)")
        
        return all_files
    
    def _scan_single_path(self, path: Path) -> List[Path]:
        """ë‹¨ì¼ ê²½ë¡œ ìŠ¤ìº”"""
        found_files = []
        
        try:
            if not self._is_accessible(path):
                return found_files
            
            # ëª¨ë¸ í™•ì¥ì ê²€ìƒ‰
            model_extensions = ['.pth', '.pt', '.bin', '.safetensors', '.ckpt', 
                              '.h5', '.pb', '.onnx', '.pkl', '.model', '.weights']
            
            for ext in model_extensions:
                pattern = f"**/*{ext}"
                for file_path in path.rglob(pattern):
                    if (file_path.is_file() and 
                        self._is_model_file(file_path) and
                        not self._should_skip_file(file_path)):
                        found_files.append(file_path)
                        
        except Exception as e:
            if self.verbose:
                logger.warning(f"âš ï¸ ìŠ¤ìº” ì˜¤ë¥˜ {path}: {e}")
            self.errors.append(f"ìŠ¤ìº” ì‹¤íŒ¨ {path}: {e}")
        
        return found_files
    
    def _is_model_file(self, file_path: Path) -> bool:
        """ëª¨ë¸ íŒŒì¼ ì—¬ë¶€ í™•ì¸"""
        try:
            size_bytes = file_path.stat().st_size
            size_mb = size_bytes / (1024 * 1024)
            
            # í¬ê¸° ì œí•œ (0.1MB ~ 50GB)
            return 0.1 <= size_mb <= 50 * 1024
            
        except OSError:
            return False
    
    def _should_skip_file(self, file_path: Path) -> bool:
        """ê±´ë„ˆë›¸ íŒŒì¼ ì—¬ë¶€"""
        path_str = str(file_path).lower()
        skip_patterns = [
            'node_modules', '__pycache__', '.git', '.cache/pip',
            'trash', 'recycle', 'temp', 'tmp', '.ds_store'
        ]
        return any(pattern in path_str for pattern in skip_patterns)
    
    def _analyze_all_models(self, model_files: List[Path]):
        """ëª¨ë“  ëª¨ë¸ ë¶„ì„"""
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¶„ì„ ì†ë„ í–¥ìƒ
        with ThreadPoolExecutor(max_workers=min(len(model_files), 8)) as executor:
            futures = [executor.submit(self._analyze_single_model, f) for f in model_files]
            
            for future in as_completed(futures):
                try:
                    model_info = future.result()
                    if model_info and model_info.is_valid:
                        self.found_models.append(model_info)
                except Exception as e:
                    self.errors.append(f"ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_single_model(self, file_path: Path) -> Optional[ModelInfo]:
        """ë‹¨ì¼ ëª¨ë¸ ìƒì„¸ ë¶„ì„"""
        try:
            # íŒŒì¼ ê¸°ë³¸ ì •ë³´
            stat_info = file_path.stat()
            size_bytes = stat_info.st_size
            size_mb = size_bytes / (1024 * 1024)
            size_gb = size_mb / 1024
            
            # ì²´í¬ì„¬ ê³„ì‚° (ìƒ˜í”Œë§)
            checksum = self._calculate_checksum(file_path, size_mb)
            
            # í”„ë ˆì„ì›Œí¬ ë¶„ë¥˜
            framework = self._classify_framework(file_path)
            
            # ëª¨ë¸ íƒ€ì… ë¶„ë¥˜
            model_type = self._classify_model_type(file_path)
            
            # Step ë¶„ë¥˜
            step_candidate, confidence = self._classify_step(file_path)
            
            # í™˜ê²½ ì •ë³´
            is_in_project = self._is_in_project(file_path)
            conda_info = self._check_conda_environment(file_path)
            
            # ê²€ì¦
            is_valid = self._validate_model(file_path, framework)
            
            # ë™ë°˜ íŒŒì¼
            companion_files = self._find_companion_files(file_path)
            
            # ì¤‘ìš”ë„ ì ìˆ˜
            importance_score = self._calculate_importance(
                file_path, size_gb, framework, step_candidate, confidence, is_in_project
            )
            
            return ModelInfo(
                name=file_path.name,
                path=str(file_path),
                absolute_path=str(file_path.absolute()),
                size_mb=size_mb,
                size_gb=size_gb,
                framework=framework,
                model_type=model_type,
                step_candidate=step_candidate,
                confidence=confidence,
                is_valid=is_valid,
                is_in_project=is_in_project,
                is_in_conda=conda_info['is_conda'],
                conda_env_name=conda_info['env_name'],
                parent_directory=file_path.parent.name,
                created_time=datetime.fromtimestamp(stat_info.st_ctime).isoformat(),
                modified_time=datetime.fromtimestamp(stat_info.st_mtime).isoformat(),
                checksum=checksum,
                companion_files=companion_files,
                importance_score=importance_score,
                extension=file_path.suffix.lower()
            )
            
        except Exception as e:
            if self.verbose:
                logger.warning(f"ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    def _calculate_checksum(self, file_path: Path, size_mb: float) -> str:
        """ì²´í¬ì„¬ ê³„ì‚° (ìƒ˜í”Œë§ ë°©ì‹)"""
        try:
            hasher = hashlib.md5()
            
            with open(file_path, 'rb') as f:
                if size_mb > 100:  # 100MB ì´ìƒì€ ìƒ˜í”Œë§
                    # ì‹œì‘, ì¤‘ê°„, ë ë¶€ë¶„ë§Œ í•´ì‹œ
                    chunks = [f.read(1024*1024)]  # ì‹œì‘ 1MB
                    try:
                        f.seek(int(size_mb * 1024 * 512))
                        chunks.append(f.read(1024*1024))  # ì¤‘ê°„ 1MB
                        f.seek(-1024*1024, 2)
                        chunks.append(f.read(1024*1024))  # ë 1MB
                    except:
                        pass
                    
                    for chunk in chunks:
                        if chunk:
                            hasher.update(chunk)
                else:
                    # ì‘ì€ íŒŒì¼ì€ ì „ì²´ í•´ì‹œ
                    while True:
                        chunk = f.read(8192)
                        if not chunk:
                            break
                        hasher.update(chunk)
            
            return hasher.hexdigest()[:16]
            
        except Exception:
            return "unknown"
    
    def _classify_framework(self, file_path: Path) -> str:
        """í”„ë ˆì„ì›Œí¬ ë¶„ë¥˜"""
        extension = file_path.suffix.lower()
        path_str = str(file_path).lower()
        
        # í™•ì¥ì ê¸°ë°˜
        for framework, info in self.framework_patterns.items():
            if extension in info['extensions']:
                # ì¶”ê°€ ì§€ì‹œì í™•ì¸
                if info['indicators']:
                    for indicator in info['indicators']:
                        if indicator in path_str:
                            return framework
                return framework
        
        # ë°”ì´ë„ˆë¦¬ íŒŒì¼ ì„¸ë¶€ ë¶„ë¥˜
        if extension == '.bin':
            if any(term in path_str for term in ['pytorch', 'torch', 'transformers']):
                return 'pytorch'
            elif 'tensorflow' in path_str:
                return 'tensorflow'
            else:
                return 'binary'
        
        return 'unknown'
    
    def _classify_model_type(self, file_path: Path) -> str:
        """ëª¨ë¸ íƒ€ì… ë¶„ë¥˜"""
        path_str = str(file_path).lower()
        
        for model_type, patterns in self.model_type_patterns.items():
            for pattern in patterns:
                if re.search(pattern, path_str):
                    return model_type
        
        return 'unknown'
    
    def _classify_step(self, file_path: Path) -> Tuple[str, float]:
        """MyCloset AI 8ë‹¨ê³„ ë¶„ë¥˜"""
        path_str = str(file_path).lower()
        name_str = file_path.name.lower()
        parent_str = file_path.parent.name.lower()
        
        best_step = "unknown"
        best_confidence = 0.0
        
        for step_name, step_info in self.step_patterns.items():
            confidence = 0.0
            
            # íŒ¨í„´ ë§¤ì¹­
            for pattern in step_info['patterns']:
                try:
                    if re.search(pattern, path_str):
                        confidence = max(confidence, 0.9)
                    elif re.search(pattern, name_str):
                        confidence = max(confidence, 0.8)
                    elif re.search(pattern, parent_str):
                        confidence = max(confidence, 0.7)
                except re.error:
                    clean_pattern = pattern.replace(r'\.*', '').replace('.*', '')
                    if clean_pattern in path_str:
                        confidence = max(confidence, 0.6)
            
            # ëª¨ë¸ëª… ë§¤ì¹­
            for model in step_info['models']:
                if model.lower() in path_str:
                    confidence = max(confidence, 0.85)
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            keyword_matches = sum(1 for kw in step_info['keywords'] if kw in path_str)
            if keyword_matches > 0:
                confidence = max(confidence, 0.5 + (keyword_matches * 0.1))
            
            if confidence > best_confidence:
                best_confidence = confidence
                best_step = step_name
        
        return best_step, best_confidence
    
    def _is_in_project(self, file_path: Path) -> bool:
        """í”„ë¡œì íŠ¸ ë‚´ë¶€ ì—¬ë¶€"""
        try:
            return file_path.is_relative_to(self.project_root)
        except ValueError:
            return False
    
    def _check_conda_environment(self, file_path: Path) -> Dict[str, Any]:
        """conda í™˜ê²½ í™•ì¸"""
        result = {'is_conda': False, 'env_name': None}
        
        for env_name, env_path in self.conda_environments.items():
            try:
                if file_path.is_relative_to(env_path):
                    result['is_conda'] = True
                    result['env_name'] = env_name
                    break
            except ValueError:
                continue
        
        return result
    
    def _validate_model(self, file_path: Path, framework: str) -> bool:
        """ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            if file_path.stat().st_size < 1024:
                return False
            
            with open(file_path, 'rb') as f:
                header = f.read(100)
            
            if framework == 'pytorch' and (b'PK' in header or b'\x80' in header):
                return True
            elif framework == 'safetensors' and b'{' in header:
                return True
            elif framework == 'tensorflow':
                return len(header) > 10
            elif framework == 'onnx':
                return b'onnx' in header.lower() or len(header) > 50
            else:
                return True
                
        except Exception:
            return False
    
    def _find_companion_files(self, file_path: Path) -> List[str]:
        """ë™ë°˜ íŒŒì¼ ì°¾ê¸°"""
        companions = []
        parent_dir = file_path.parent
        
        companion_patterns = [
            'config.json', 'config.yaml', 'tokenizer.json',
            'model_config.json', 'pytorch_model.bin'
        ]
        
        for pattern in companion_patterns:
            companion_path = parent_dir / pattern
            if companion_path.exists() and companion_path != file_path:
                companions.append(str(companion_path))
        
        return companions
    
    def _calculate_importance(
        self, file_path: Path, size_gb: float, framework: str, 
        step: str, confidence: float, is_in_project: bool
    ) -> float:
        """ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # í¬ê¸° ì ìˆ˜ (ìµœëŒ€ 30ì )
        score += min(size_gb * 10, 30)
        
        # ì‹ ë¢°ë„ ì ìˆ˜ (ìµœëŒ€ 25ì )
        score += confidence * 25
        
        # í”„ë ˆì„ì›Œí¬ ì ìˆ˜ (ìµœëŒ€ 15ì )
        framework_scores = {
            'pytorch': 15, 'safetensors': 12, 'diffusers': 10,
            'transformers': 8, 'tensorflow': 6, 'onnx': 4
        }
        score += framework_scores.get(framework, 0)
        
        # Step ì ìˆ˜ (ìµœëŒ€ 20ì )
        if step != 'unknown':
            score += 20
        
        # ìœ„ì¹˜ ì ìˆ˜ (ìµœëŒ€ 10ì )
        if is_in_project:
            score += 10
        elif any(file_path.is_relative_to(env) for env in self.conda_environments.values()):
            score += 5
        
        return min(score, 100.0)
    
    def _post_process_results(self):
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        if not self.found_models:
            return
        
        # ì¤‘ë³µ íƒì§€ (ì²´í¬ì„¬ ê¸°ë°˜)
        checksum_groups = {}
        for model in self.found_models:
            if model.checksum != "unknown":
                if model.checksum not in checksum_groups:
                    checksum_groups[model.checksum] = []
                checksum_groups[model.checksum].append(model)
        
        # ì¤‘ë³µëœ ê²ƒë“¤ í‘œì‹œ
        for checksum, models in checksum_groups.items():
            if len(models) > 1:
                # ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì„ ì›ë³¸ìœ¼ë¡œ ì„¤ì •
                primary = max(models, key=lambda m: m.importance_score)
                for model in models:
                    if model != primary:
                        model.importance_score *= 0.8  # ì¤‘ë³µ íŒ¨ë„í‹°
    
    def _debug_scan_paths(self, scan_paths: List[Path]):
        """ìŠ¤ìº” ê²½ë¡œ ë””ë²„ê·¸"""
        logger.info("ğŸ” ìŠ¤ìº” ê²½ë¡œ ë””ë²„ê·¸:")
        
        for i, path in enumerate(scan_paths, 1):
            exists = "âœ…" if path.exists() else "âŒ"
            logger.info(f"  {i:2d}. {exists} {path}")
            
            if path.exists():
                try:
                    items = list(path.iterdir())
                    logger.info(f"      ğŸ“ {len(items)}ê°œ í•­ëª©")
                    
                    # ëª¨ë¸ íŒŒì¼ ì§ì ‘ ê²€ìƒ‰
                    model_files = []
                    for item in items[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì²´í¬
                        if item.is_file() and item.suffix.lower() in ['.pth', '.pt', '.bin']:
                            model_files.append(item)
                    
                    if model_files:
                        logger.info(f"      ğŸ¤– {len(model_files)}ê°œ ëª¨ë¸ íŒŒì¼ ë°œê²¬")
                        for mf in model_files:
                            size_mb = mf.stat().st_size / (1024*1024)
                            logger.info(f"        - {mf.name} ({size_mb:.1f}MB)")
                    
                except Exception as e:
                    logger.info(f"      âŒ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    def _print_complete_results(self, scan_duration: float):
        """ì™„ì „í•œ ê²°ê³¼ ì¶œë ¥"""
        stats = self._calculate_statistics(scan_duration)
        
        print("\n" + "=" * 100)
        print("ğŸ¯ MyCloset AI - ì™„ì „í•œ AI ëª¨ë¸ ìŠ¤ìº” ê²°ê³¼")
        print("=" * 100)
        
        # ê¸°ë³¸ ì •ë³´
        print(f"ğŸ• ìŠ¤ìº” ì‹œê°„: {scan_duration:.1f}ì´ˆ")
        print(f"ğŸ’» ì‹œìŠ¤í…œ: {platform.system()} {platform.machine()}")
        print(f"ğŸ í˜„ì¬ conda í™˜ê²½: {self.current_conda_env or 'None'}")
        
        # ìŠ¤ìº” í†µê³„
        print(f"\nğŸ“Š ìŠ¤ìº” í†µê³„:")
        print(f"   ğŸ“ ìŠ¤ìº” ìœ„ì¹˜: {len(self.scan_locations)}ê³³")
        print(f"   ğŸ“„ ê²€ì‚¬ íŒŒì¼: {stats.total_files_scanned:,}ê°œ")
        print(f"   ğŸ¤– ë°œê²¬ ëª¨ë¸: {stats.models_found:,}ê°œ")
        print(f"   ğŸ’¾ ì´ ìš©ëŸ‰: {stats.total_size_gb:.2f}GB")
        print(f"   âœ… ìœ íš¨ ëª¨ë¸: {stats.valid_models}ê°œ")
        
        if not self.found_models:
            print("\nâŒ AI ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self._print_suggestions()
            return
        
        # ìœ„ì¹˜ë³„ ë¶„í¬
        print(f"\nğŸ“ ìœ„ì¹˜ë³„ ë¶„í¬:")
        print(f"   ğŸ conda í™˜ê²½: {stats.conda_models}ê°œ")
        print(f"   ğŸ  í”„ë¡œì íŠ¸ ë‚´ë¶€: {stats.project_models}ê°œ")
        print(f"   ğŸŒ ì‹œìŠ¤í…œ ì „ì²´: {stats.system_models}ê°œ")
        
        # conda í™˜ê²½ë³„ ìƒì„¸
        if stats.conda_models > 0:
            conda_dist = {}
            for model in self.found_models:
                if model.is_in_conda and model.conda_env_name:
                    conda_dist[model.conda_env_name] = conda_dist.get(model.conda_env_name, 0) + 1
            
            print(f"   conda í™˜ê²½ë³„:")
            for env_name, count in sorted(conda_dist.items()):
                env_size = sum(m.size_gb for m in self.found_models if m.conda_env_name == env_name)
                print(f"     - {env_name}: {count}ê°œ ({env_size:.1f}GB)")
        
        # í”„ë ˆì„ì›Œí¬ ë¶„í¬
        print(f"\nğŸ”§ í”„ë ˆì„ì›Œí¬ë³„ ë¶„í¬:")
        for fw, count in sorted(stats.framework_distribution.items(), key=lambda x: x[1], reverse=True):
            fw_size = sum(m.size_gb for m in self.found_models if m.framework == fw)
            percentage = (count / stats.models_found) * 100
            print(f"   - {fw}: {count}ê°œ ({fw_size:.2f}GB, {percentage:.1f}%)")
        
        # MyCloset AI Step ë¶„í¬
        if stats.step_distribution:
            print(f"\nğŸ¯ MyCloset AI Stepë³„ ë¶„í¬ (ì‹ ë¢°ë„ 50%+):")
            step_names = {
                'step_01_human_parsing': '1ï¸âƒ£ Human Parsing',
                'step_02_pose_estimation': '2ï¸âƒ£ Pose Estimation',
                'step_03_cloth_segmentation': '3ï¸âƒ£ Cloth Segmentation',
                'step_04_geometric_matching': '4ï¸âƒ£ Geometric Matching',
                'step_05_cloth_warping': '5ï¸âƒ£ Cloth Warping',
                'step_06_virtual_fitting': '6ï¸âƒ£ Virtual Fitting',
                'step_07_post_processing': '7ï¸âƒ£ Post Processing',
                'step_08_quality_assessment': '8ï¸âƒ£ Quality Assessment'
            }
            
            for step, count in sorted(stats.step_distribution.items()):
                if count > 0:
                    display_name = step_names.get(step, step)
                    step_size = sum(m.size_gb for m in self.found_models 
                                  if m.step_candidate == step and m.confidence >= 0.5)
                    print(f"   {display_name}: {count}ê°œ ({step_size:.1f}GB)")
        
        # ìƒìœ„ ì¤‘ìš” ëª¨ë¸ë“¤
        print(f"\nğŸ† ì¤‘ìš”ë„ ìƒìœ„ ëª¨ë¸ë“¤:")
        top_models = sorted(self.found_models, key=lambda x: x.importance_score, reverse=True)[:15]
        
        for i, model in enumerate(top_models, 1):
            location_icon = "ğŸ" if model.is_in_conda else "ğŸ " if model.is_in_project else "ğŸŒ"
            step_info = ""
            if model.confidence >= 0.5 and model.step_candidate != 'unknown':
                step_num = model.step_candidate.split('_')[1] if '_' in model.step_candidate else '?'
                step_info = f" | ğŸ¯ Step {step_num}"
            
            confidence_icon = "ğŸŸ¢" if model.confidence >= 0.8 else "ğŸŸ¡" if model.confidence >= 0.5 else "ğŸ”´"
            
            print(f"  {i:2d}. {model.name}")
            print(f"      ğŸ“ {model.path}")
            print(f"      ğŸ“Š {model.size_gb:.2f}GB | {model.framework} | {model.model_type}")
            print(f"      {location_icon} {model.conda_env_name or model.parent_directory} | "
                  f"{confidence_icon} {model.confidence:.2f} | â­ {model.importance_score:.1f}{step_info}")
        
        # ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        recommendations = []
        
        if stats.conda_models > 0 and stats.project_models == 0:
            recommendations.append(f"ğŸ”„ conda í™˜ê²½ì˜ {stats.conda_models}ê°œ ëª¨ë¸ì„ í”„ë¡œì íŠ¸ë¡œ ì—°ê²° ê³ ë ¤")
        
        if stats.system_models > stats.project_models:
            recommendations.append(f"ğŸ“¦ ì‹œìŠ¤í…œì˜ {stats.system_models}ê°œ ëª¨ë¸ì„ í”„ë¡œì íŠ¸ë¡œ í†µí•© ê³ ë ¤")
        
        large_models = [m for m in self.found_models if m.size_gb > 2.0]
        if large_models:
            recommendations.append(f"ğŸ“¦ 2GB+ ëŒ€ìš©ëŸ‰ ëª¨ë¸ {len(large_models)}ê°œ ìµœì í™” ê²€í† ")
        
        unclassified = [m for m in self.found_models if m.step_candidate == 'unknown']
        if unclassified:
            recommendations.append(f"ğŸ¯ ë¯¸ë¶„ë¥˜ ëª¨ë¸ {len(unclassified)}ê°œ ìˆ˜ë™ ê²€í†  í•„ìš”")
        
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        
        if not recommendations:
            print("   âœ… í˜„ì¬ ëª¨ë¸ êµ¬ì„±ì´ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        
        # ì˜¤ë¥˜ ë° ê²½ê³ 
        if self.errors or self.warnings:
            print(f"\nâš ï¸ ë°œê²¬ëœ ë¬¸ì œë“¤:")
            if self.errors:
                print(f"   âŒ ì˜¤ë¥˜ {len(self.errors)}ê°œ:")
                for error in self.errors[:3]:
                    print(f"     - {error}")
                if len(self.errors) > 3:
                    print(f"     ... ì™¸ {len(self.errors) - 3}ê°œ")
    
    def _calculate_statistics(self, scan_duration: float) -> ScanStatistics:
        """í†µê³„ ê³„ì‚°"""
        stats = ScanStatistics()
        
        # ê¸°ë³¸ í†µê³„
        stats.total_files_scanned = sum(len(files) for files in self.scan_locations.values())
        stats.models_found = len(self.found_models)
        stats.total_size_gb = sum(m.size_gb for m in self.found_models)
        stats.scan_duration = scan_duration
        
        # ìœ„ì¹˜ë³„ í†µê³„
        stats.conda_models = sum(1 for m in self.found_models if m.is_in_conda)
        stats.project_models = sum(1 for m in self.found_models if m.is_in_project)
        stats.system_models = stats.models_found - stats.conda_models - stats.project_models
        stats.valid_models = sum(1 for m in self.found_models if m.is_valid)
        
        # ë¶„í¬ í†µê³„
        for model in self.found_models:
            # í”„ë ˆì„ì›Œí¬ ë¶„í¬
            fw = model.framework
            stats.framework_distribution[fw] = stats.framework_distribution.get(fw, 0) + 1
            
            # Step ë¶„í¬ (ì‹ ë¢°ë„ 0.5+ ë§Œ)
            if model.confidence >= 0.5:
                step = model.step_candidate
                stats.step_distribution[step] = stats.step_distribution.get(step, 0) + 1
        
        return stats
    
    def _print_suggestions(self):
        """ì œì•ˆì‚¬í•­ ì¶œë ¥"""
        print("\nğŸ’¡ AI ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•´ë³´ì„¸ìš”:")
        
        print("\nğŸ” ê²€ìƒ‰ í™•ì¥:")
        print("   python quick_scanner.py --deep                 # ì „ì²´ ì‹œìŠ¤í…œ ë”¥ ìŠ¤ìº”")
        print("   python quick_scanner.py --conda-first          # conda í™˜ê²½ ìš°ì„  ìŠ¤ìº”")
        print("   python quick_scanner.py --verbose              # ìƒì„¸ ì§„í–‰ ê³¼ì • ì¶œë ¥")
        
        print("\nğŸ“ ì˜ˆìƒ ëª¨ë¸ ìœ„ì¹˜:")
        expected_locations = [
            ("ğŸ  í”„ë¡œì íŠ¸", self.ai_models_dir),
            ("ğŸ“¥ ë‹¤ìš´ë¡œë“œ", Path.home() / "Downloads"),
            ("ğŸ’¾ HuggingFace", Path.home() / ".cache" / "huggingface"),
            ("ğŸ”¥ PyTorch", Path.home() / ".cache" / "torch")
        ]
        
        for desc, location in expected_locations:
            exists = "âœ…" if location and location.exists() else "âŒ"
            print(f"   {exists} {desc}: {location}")
        
        print("\nğŸ conda í™˜ê²½ë³„ í™•ì¸:")
        for env_name, env_path in self.conda_environments.items():
            site_packages = env_path / "lib" / "python3.11" / "site-packages"
            exists = "âœ…" if site_packages.exists() else "âŒ"
            print(f"   {exists} {env_name}: {site_packages}")
        
        print("\nğŸ”§ ìˆ˜ë™ í™•ì¸ ëª…ë ¹ì–´:")
        print(f"   find {self.project_root} -name '*.pth' -o -name '*.pt' -o -name '*.bin'")
        print(f"   ls -la {self.ai_models_dir}/ 2>/dev/null")
        print(f"   find ~ -name '*.pth' -size +1M 2>/dev/null | head -10")
    
    def generate_config_files(self, output_dir: str = "generated_configs") -> List[str]:
        """ì„¤ì • íŒŒì¼ë“¤ ìƒì„±"""
        if not self.found_models:
            logger.warning("âŒ ì„¤ì • íŒŒì¼ì„ ìƒì„±í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        output_path = self.project_root / output_dir
        output_path.mkdir(exist_ok=True)
        
        generated_files = []
        
        # 1. JSON ê²°ê³¼ íŒŒì¼
        json_file = self._generate_json_config(output_path)
        generated_files.append(json_file)
        
        # 2. Python ì„¤ì • íŒŒì¼
        python_file = self._generate_python_config(output_path)
        generated_files.append(python_file)
        
        # 3. conda í™˜ê²½ ì„¤ì •
        if any(m.is_in_conda for m in self.found_models):
            conda_file = self._generate_conda_config(output_path)
            generated_files.append(conda_file)
        
        logger.info(f"ğŸ“ {len(generated_files)}ê°œ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_path}")
        return generated_files
    
    def _generate_json_config(self, output_path: Path) -> str:
        """JSON ì„¤ì • íŒŒì¼ ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = output_path / f"model_scan_result_{timestamp}.json"
        
        config_data = {
            "scan_info": {
                "timestamp": datetime.now().isoformat(),
                "project_root": str(self.project_root),
                "ai_models_dir": str(self.ai_models_dir),
                "scan_duration": time.time() - self.scan_start_time,
                "conda_environments": list(self.conda_environments.keys()),
                "current_conda_env": self.current_conda_env
            },
            "statistics": asdict(self._calculate_statistics(time.time() - self.scan_start_time)),
            "models": [asdict(model) for model in self.found_models],
            "scan_locations": self.scan_locations,
            "errors": self.errors,
            "warnings": self.warnings
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False, default=str)
        
        return str(json_file)
    
    def _generate_python_config(self, output_path: Path) -> str:
        """Python ì„¤ì • íŒŒì¼ ìƒì„±"""
        python_file = output_path / "model_paths_config.py"
        
        config_content = f'''#!/usr/bin/env python3
"""
MyCloset AI ëª¨ë¸ ê²½ë¡œ ì„¤ì • - ìë™ ìƒì„±ë¨
ìƒì„± ì‹œê°„: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
ë°œê²¬ëœ ëª¨ë¸: {len(self.found_models)}ê°œ
"""

from pathlib import Path
from typing import Dict, List, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = Path(__file__).parent.parent
AI_MODELS_ROOT = PROJECT_ROOT / "ai_models"

# ë°œê²¬ëœ ëª¨ë¸ ê²½ë¡œë“¤
SCANNED_MODELS = {{
'''
        
        for i, model in enumerate(self.found_models):
            safe_name = model.name.replace('.', '_').replace('-', '_').replace(' ', '_')
            config_content += f'''    "{safe_name}": {{
        "name": "{model.name}",
        "path": Path(r"{model.path}"),
        "framework": "{model.framework}",
        "step": "{model.step_candidate}",
        "confidence": {model.confidence:.3f},
        "size_mb": {model.size_mb:.1f},
        "importance": {model.importance_score:.1f},
        "is_in_project": {model.is_in_project},
        "is_in_conda": {model.is_in_conda},
        "conda_env": "{model.conda_env_name or ''}"
    }},
'''
        
        config_content += '''}}

# Stepë³„ ëª¨ë¸ ë§¤í•‘ (ì‹ ë¢°ë„ 50% ì´ìƒ)
STEP_MODELS = {
'''
        
        # Stepë³„ ëª¨ë¸ ê·¸ë£¹í™”
        step_models = {}
        for model in self.found_models:
            if model.confidence >= 0.5:
                step = model.step_candidate
                if step not in step_models:
                    step_models[step] = []
                safe_name = model.name.replace('.', '_').replace('-', '_').replace(' ', '_')
                step_models[step].append(safe_name)
        
        for step, models in step_models.items():
            config_content += f'    "{step}": {models},\n'
        
        config_content += '''}

# í”„ë ˆì„ì›Œí¬ë³„ ëª¨ë¸ ë§¤í•‘
FRAMEWORK_MODELS = {
'''
        
        framework_models = {}
        for model in self.found_models:
            fw = model.framework
            if fw not in framework_models:
                framework_models[fw] = []
            safe_name = model.name.replace('.', '_').replace('-', '_').replace(' ', '_')
            framework_models[fw].append(safe_name)
        
        for fw, models in framework_models.items():
            config_content += f'    "{fw}": {models},\n'
        
        config_content += f'''}}

def get_model_path(model_name: str) -> Optional[Path]:
    """ëª¨ë¸ ê²½ë¡œ ë°˜í™˜"""
    for key, info in SCANNED_MODELS.items():
        if model_name.lower() in key.lower() or model_name.lower() in info["name"].lower():
            return info["path"]
    return None

def get_step_models(step: str) -> List[str]:
    """Stepë³„ ëª¨ë¸ ëª©ë¡"""
    return STEP_MODELS.get(step, [])

def get_framework_models(framework: str) -> List[str]:
    """í”„ë ˆì„ì›Œí¬ë³„ ëª¨ë¸ ëª©ë¡"""
    return FRAMEWORK_MODELS.get(framework, [])

def get_best_model_for_step(step: str) -> Optional[str]:
    """Stepë³„ ìµœê³  ì¤‘ìš”ë„ ëª¨ë¸"""
    step_models = get_step_models(step)
    if not step_models:
        return None
    
    best_model = None
    best_score = 0
    
    for model_key in step_models:
        if model_key in SCANNED_MODELS:
            score = SCANNED_MODELS[model_key]["importance"]
            if score > best_score:
                best_score = score
                best_model = model_key
    
    return best_model

def list_available_models() -> Dict[str, dict]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
    available = {{}}
    for key, info in SCANNED_MODELS.items():
        if info["path"].exists():
            available[key] = info
    return available

def get_conda_models(env_name: str = None) -> List[str]:
    """conda í™˜ê²½ë³„ ëª¨ë¸ ëª©ë¡"""
    conda_models = []
    for key, info in SCANNED_MODELS.items():
        if info["is_in_conda"]:
            if env_name is None or info["conda_env"] == env_name:
                conda_models.append(key)
    return conda_models

if __name__ == "__main__":
    print("ğŸ¤– MyCloset AI ëª¨ë¸ ì„¤ì •")
    print("=" * 50)
    
    available = list_available_models()
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {{len(available)}}ê°œ")
    
    print("\\nStepë³„ ëª¨ë¸:")
    for step, models in STEP_MODELS.items():
        if models:
            step_name = step.replace('step_', '').replace('_', ' ').title()
            print(f"  {{step_name}}: {{len(models)}}ê°œ")
    
    print("\\ní”„ë ˆì„ì›Œí¬ë³„ ë¶„í¬:")
    for fw, models in FRAMEWORK_MODELS.items():
        print(f"  {{fw}}: {{len(models)}}ê°œ")
'''
        
        with open(python_file, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        return str(python_file)
    
    def _generate_conda_config(self, output_path: Path) -> str:
        """conda í™˜ê²½ ì„¤ì • íŒŒì¼ ìƒì„±"""
        conda_file = output_path / "conda_model_config.py"
        
        conda_models = [m for m in self.found_models if m.is_in_conda]
        
        config_content = f'''#!/usr/bin/env python3
"""
MyCloset AI - conda í™˜ê²½ë³„ ëª¨ë¸ ì„¤ì •
ìƒì„± ì‹œê°„: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
conda ëª¨ë¸: {len(conda_models)}ê°œ
"""

from pathlib import Path
from typing import Dict, List

# conda í™˜ê²½ ì •ë³´
CONDA_ENVIRONMENTS = {{
'''
        
        for env_name, env_path in self.conda_environments.items():
            config_content += f'    "{env_name}": Path(r"{env_path}"),\n'
        
        config_content += f'''}}

CURRENT_CONDA_ENV = "{self.current_conda_env or 'None'}"

# conda í™˜ê²½ë³„ ëª¨ë¸ ë§¤í•‘
CONDA_MODELS = {{
'''
        
        for env_name in self.conda_environments.keys():
            env_models = [m for m in conda_models if m.conda_env_name == env_name]
            if env_models:
                config_content += f'    "{env_name}": [\n'
                for model in env_models:
                    config_content += f'        "{model.path}",\n'
                config_content += f'    ],\n'
        
        config_content += '''}

def get_conda_model_paths(env_name: str) -> List[str]:
    """conda í™˜ê²½ë³„ ëª¨ë¸ ê²½ë¡œ ëª©ë¡"""
    return CONDA_MODELS.get(env_name, [])

def get_current_env_models() -> List[str]:
    """í˜„ì¬ conda í™˜ê²½ì˜ ëª¨ë¸ë“¤"""
    if CURRENT_CONDA_ENV != "None":
        return get_conda_model_paths(CURRENT_CONDA_ENV)
    return []

def list_conda_environments() -> List[str]:
    """conda í™˜ê²½ ëª©ë¡"""
    return list(CONDA_ENVIRONMENTS.keys())

if __name__ == "__main__":
    print("ğŸ MyCloset AI conda í™˜ê²½ ëª¨ë¸ ì„¤ì •")
    print("=" * 50)
    
    print(f"í˜„ì¬ í™˜ê²½: {CURRENT_CONDA_ENV}")
    print(f"ì´ í™˜ê²½: {len(CONDA_ENVIRONMENTS)}ê°œ")
    
    for env_name in CONDA_ENVIRONMENTS.keys():
        models = get_conda_model_paths(env_name)
        print(f"  {env_name}: {len(models)}ê°œ ëª¨ë¸")
'''
        
        with open(conda_file, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        return str(conda_file)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ì™„ì „í•œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²€ìƒ‰ ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python quick_scanner.py                           # ê¸°ë³¸ ìŠ¤ìº”
  python quick_scanner.py --verbose                 # ìƒì„¸ ì¶œë ¥
  python quick_scanner.py --organize                # ìŠ¤ìº” + ì„¤ì • ìƒì„±
  python quick_scanner.py --deep                    # ë”¥ ìŠ¤ìº”
  python quick_scanner.py --conda-first             # conda ìš°ì„ 
  python quick_scanner.py --deep --organize         # ì™„ì „ ìŠ¤ìº” + ì„¤ì •
        """
    )
    
    parser.add_argument('--verbose', action='store_true', help='ìƒì„¸ ì¶œë ¥')
    parser.add_argument('--organize', action='store_true', help='ìŠ¤ìº” í›„ ì„¤ì • íŒŒì¼ ìƒì„±')
    parser.add_argument('--deep', action='store_true', help='ì „ì²´ ì‹œìŠ¤í…œ ë”¥ ìŠ¤ìº”')
    parser.add_argument('--conda-first', action='store_true', help='conda í™˜ê²½ ìš°ì„  ìŠ¤ìº”')
    parser.add_argument('--output-dir', type=str, default='generated_configs', help='ì„¤ì • íŒŒì¼ ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--min-size', type=float, default=0.1, help='ìµœì†Œ íŒŒì¼ í¬ê¸° (MB)')
    parser.add_argument('--max-size', type=float, default=50.0, help='ìµœëŒ€ íŒŒì¼ í¬ê¸° (GB)')
    
    args = parser.parse_args()
    
    try:
        # ìŠ¤ìºë„ˆ ì´ˆê¸°í™”
        scanner = CompleteModelScanner(
            verbose=args.verbose,
            conda_first=args.conda_first,
            deep_scan=args.deep
        )
        
        # ì™„ì „í•œ ì‹œìŠ¤í…œ ìŠ¤ìº” ì‹¤í–‰
        models = scanner.scan_complete_system()
        
        # ì„¤ì • íŒŒì¼ ìƒì„± (ì˜µì…˜)
        if args.organize and models:
            config_files = scanner.generate_config_files(args.output_dir)
            print(f"\nğŸ“ ìƒì„±ëœ ì„¤ì • íŒŒì¼:")
            for config_file in config_files:
                print(f"   âœ… {config_file}")
        
        # ì™„ë£Œ ë©”ì‹œì§€
        print(f"\nâœ… ìŠ¤ìº” ì™„ë£Œ!")
        print(f"ğŸ¤– ë°œê²¬ëœ ëª¨ë¸: {len(models)}ê°œ")
        
        if models:
            conda_models = sum(1 for m in models if m.is_in_conda)
            project_models = sum(1 for m in models if m.is_in_project)
            total_size = sum(m.size_gb for m in models)
            avg_importance = sum(m.importance_score for m in models) / len(models)
            
            print(f"ğŸ conda ëª¨ë¸: {conda_models}ê°œ")
            print(f"ğŸ  í”„ë¡œì íŠ¸ ëª¨ë¸: {project_models}ê°œ")
            print(f"ğŸ’¾ ì´ ìš©ëŸ‰: {total_size:.2f}GB")
            print(f"â­ í‰ê·  ì¤‘ìš”ë„: {avg_importance:.1f}/100")
            
            # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
            print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
            if args.organize:
                print("1. generated_configs/ í´ë”ì˜ ì„¤ì • íŒŒì¼ë“¤ í™•ì¸")
                print("2. model_paths_config.pyë¥¼ í”„ë¡œì íŠ¸ì— import")
                print("3. get_model_path() í•¨ìˆ˜ë¡œ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©")
            else:
                print("1. python quick_scanner.py --organize  # ì„¤ì • íŒŒì¼ ìƒì„±")
                print("2. ì¤‘ë³µ ëª¨ë¸ ì •ë¦¬ ë° í”„ë¡œì íŠ¸ í†µí•©")
                print("3. conda í™˜ê²½ ëª¨ë¸ ì—°ê²°")
        else:
            print("\nğŸ” ëª¨ë¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒì„ ì‹œë„í•´ë³´ì„¸ìš”:")
            print("1. python quick_scanner.py --deep --verbose")
            print("2. python quick_scanner.py --conda-first")
            print("3. ì‹¤ì œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€ í™•ì¸")
        
        return 0 if models else 1
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 1
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())