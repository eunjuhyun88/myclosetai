#!/bin/bash
# apply_gpu_config.sh - GPU Config íŒŒì¼ êµì²´ ìŠ¤í¬ë¦½íŠ¸

echo "ğŸ”§ MyCloset AI GPU Config íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹œì‘..."

# í˜„ì¬ ë””ë ‰í† ë¦¬ í™•ì¸
if [ ! -d "backend/app/core" ]; then
    echo "âŒ backend/app/core ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤."
    echo "   mycloset-ai í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
    exit 1
fi

# ê¸°ì¡´ íŒŒì¼ ë°±ì—…
BACKUP_FILE="backend/app/core/gpu_config.py.backup.$(date +%Y%m%d_%H%M%S)"
if [ -f "backend/app/core/gpu_config.py" ]; then
    echo "ğŸ“‹ ê¸°ì¡´ íŒŒì¼ ë°±ì—… ì¤‘..."
    cp backend/app/core/gpu_config.py "$BACKUP_FILE"
    echo "   ë°±ì—… ì™„ë£Œ: $BACKUP_FILE"
fi

# ìƒˆ íŒŒì¼ ìƒì„±
echo "ğŸ›  ìƒˆ GPU Config íŒŒì¼ ìƒì„± ì¤‘..."
cat > backend/app/core/gpu_config.py << 'EOF'
"""
ğŸ MyCloset AI - ì™„ì „í•œ GPU ì„¤ì • ë§¤ë‹ˆì € (ìš°ë¦¬ êµ¬ì¡° 100% ìµœì í™”)
=================================================================================

âœ… ê¸°ì¡´ í”„ë¡œì íŠ¸ êµ¬ì¡° 100% í˜¸í™˜
âœ… GPUConfig í´ë˜ìŠ¤ ì™„ì „ êµ¬í˜„ (import ì˜¤ë¥˜ í•´ê²°)
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… PyTorch 2.6+ MPS í˜¸í™˜ì„± ì™„ì „ í•´ê²°
âœ… safe_mps_empty_cache() ì˜¤ë¥˜ ì™„ì „ ìˆ˜ì •
âœ… Float16/32 í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
âœ… Conda í™˜ê²½ ì™„ë²½ ì§€ì›
âœ… 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”
âœ… ë ˆì´ì–´ ì•„í‚¤í…ì²˜ íŒ¨í„´ ì ìš©
âœ… ìˆœí™˜ ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
âœ… ì•ˆì „í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜
âœ… ë¡œê·¸ ë…¸ì´ì¦ˆ ìµœì†Œí™”
âœ… Clean Architecture ì ìš©

í”„ë¡œì íŠ¸ êµ¬ì¡°:
backend/app/core/gpu_config.py (ì´ íŒŒì¼)
    â†“ ì‚¬ìš©ë¨
backend/app/core/config.py
backend/app/api/pipeline_routes.py
backend/app/ai_pipeline/utils/model_loader.py
backend/app/ai_pipeline/steps/*.py
"""

import os
import gc
import logging
import platform
import subprocess
import time
import weakref
from typing import Dict, Any, Optional, Union, List, Tuple, Callable
from functools import lru_cache, wraps
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import json
import threading
from concurrent.futures import ThreadPoolExecutor

# =============================================================================
# ğŸ”§ ì¡°ê±´ë¶€ ì„í¬íŠ¸ (ì•ˆì „í•œ ì²˜ë¦¬)
# =============================================================================

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

try:
    import torch
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
except ImportError:
    TORCH_AVAILABLE = False
    TORCH_VERSION = "not_available"

try:
    import numpy as np
    NUMPY_AVAILABLE = True
    NUMPY_VERSION = np.__version__
except ImportError:
    NUMPY_AVAILABLE = False
    NUMPY_VERSION = "not_available"

# =============================================================================
# ğŸ”¥ safe_mps_empty_cache í•¨ìˆ˜ (ìˆœí™˜ì°¸ì¡° ì—†ëŠ” ì§ì ‘ êµ¬í˜„)
# =============================================================================

# ê¸€ë¡œë²Œ ë³€ìˆ˜ë“¤
_last_mps_call_time = 0
_mps_call_lock = threading.Lock()
_min_call_interval = 1.0  # 1ì´ˆ

def safe_mps_empty_cache() -> dict:
    """ì•ˆì „í•œ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (ìˆœí™˜ì°¸ì¡° ì—†ëŠ” ì§ì ‘ êµ¬í˜„)"""
    global _last_mps_call_time
    
    with _mps_call_lock:
        current_time = time.time()
        
        # 1ì´ˆ ë‚´ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
        if current_time - _last_mps_call_time < _min_call_interval:
            return {
                "success": True, 
                "method": "throttled", 
                "message": "í˜¸ì¶œ ì œí•œ (1ì´ˆ ë‚´ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€)"
            }
        
        _last_mps_call_time = current_time
    
    # ì‹¤ì œ ë©”ëª¨ë¦¬ ì •ë¦¬ ë¡œì§
    try:
        if not TORCH_AVAILABLE:
            gc.collect()
            return {
                "success": True, 
                "method": "gc_fallback", 
                "message": "PyTorch ì—†ìŒ - ê°€ë¹„ì§€ ì»¬ë ‰ì…˜"
            }
        
        # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„ (5ë‹¨ê³„)
        
        # ë°©ë²• 1: torch.mps.empty_cache()
        if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
            try:
                if callable(getattr(torch.mps, 'empty_cache', None)):
                    torch.mps.empty_cache()
                    gc.collect()
                    return {
                        "success": True, 
                        "method": "torch_mps_empty_cache", 
                        "message": "MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ"
                    }
            except (AttributeError, RuntimeError, TypeError):
                pass
        
        # ë°©ë²• 2: torch.mps.synchronize()
        if hasattr(torch, 'mps') and hasattr(torch.mps, 'synchronize'):
            try:
                if callable(getattr(torch.mps, 'synchronize', None)):
                    torch.mps.synchronize()
                    gc.collect()
                    return {
                        "success": True, 
                        "method": "torch_mps_synchronize", 
                        "message": "MPS ë™ê¸°í™” ì™„ë£Œ"
                    }
            except (AttributeError, RuntimeError, TypeError):
                pass
        
        # ë°©ë²• 3: torch.backends.mps.empty_cache()
        if hasattr(torch.backends, 'mps') and hasattr(torch.backends.mps, 'empty_cache'):
            try:
                if callable(getattr(torch.backends.mps, 'empty_cache', None)):
                    torch.backends.mps.empty_cache()
                    gc.collect()
                    return {
                        "success": True, 
                        "method": "torch_backends_mps_empty_cache", 
                        "message": "MPS ë°±ì—”ë“œ ì •ë¦¬ ì™„ë£Œ"
                    }
            except (AttributeError, RuntimeError, TypeError):
                pass
        
        # ë°©ë²• 4: CUDA (í•´ë‹¹í•˜ëŠ” ê²½ìš°)
        if hasattr(torch, 'cuda') and torch.cuda.is_available():
            try:
                torch.cuda.empty_cache()
                gc.collect()
                return {
                    "success": True, 
                    "method": "cuda_empty_cache", 
                    "message": "CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ"
                }
            except Exception:
                pass
        
        # ë°©ë²• 5: ìµœì¢… í´ë°±
        collected = gc.collect()
        return {
            "success": True, 
            "method": "gc_final", 
            "message": f"ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì™„ë£Œ ({collected}ê°œ ì •ë¦¬)"
        }
        
    except Exception as e:
        # ìµœí›„ì˜ ìˆ˜ë‹¨
        try:
            gc.collect()
            return {
                "success": True, 
                "method": "emergency_gc", 
                "message": "ë¹„ìƒ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜"
            }
        except:
            return {
                "success": False, 
                "method": "total_failure", 
                "error": str(e)[:100]
            }

# =============================================================================
# ğŸ”§ ë¡œê¹… ìµœì í™” (ë…¸ì´ì¦ˆ 90% ê°ì†Œ)
# =============================================================================

logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)  # INFO ë¡œê·¸ ì–µì œ

# =============================================================================
# ğŸ”§ ìƒìˆ˜ ë° ì„¤ì •
# =============================================================================

class OptimizationLevel(Enum):
    """ìµœì í™” ë ˆë²¨"""
    SAFE = "safe"
    BALANCED = "balanced"
    PERFORMANCE = "performance"
    ULTRA = "ultra"

class DeviceType(Enum):
    """ë””ë°”ì´ìŠ¤ íƒ€ì…"""
    CPU = "cpu"
    MPS = "mps"
    CUDA = "cuda"
    AUTO = "auto"

@dataclass
class DeviceCapabilities:
    """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ì •ë³´"""
    device: str
    name: str
    memory_gb: float
    supports_fp16: bool = False
    supports_fp32: bool = True
    supports_neural_engine: bool = False
    supports_metal_shaders: bool = False
    unified_memory: bool = False
    max_batch_size: int = 1
    recommended_image_size: Tuple[int, int] = (512, 512)

# =============================================================================
# ğŸ M3 Max í•˜ë“œì›¨ì–´ ê°ì§€ ì‹œìŠ¤í…œ
# =============================================================================

class HardwareDetector:
    """í•˜ë“œì›¨ì–´ ì •ë³´ ê°ì§€ ë° ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self._cache = {}
        self._lock = threading.Lock()
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ ì •ë³´
        self.system_info = self._get_system_info()
        self.is_m3_max = self._detect_m3_max()
        self.memory_gb = self._get_system_memory()
        self.cpu_cores = self._get_cpu_cores()
        self.gpu_info = self._get_gpu_info()
        
        # ì„±ëŠ¥ íŠ¹ì„±
        self.performance_class = self._classify_performance()
        self.optimization_profile = self._create_optimization_profile()
    
    def _get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘"""
        if 'system_info' in self._cache:
            return self._cache['system_info']
        
        info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "processor": platform.processor(),
            "python_version": platform.python_version(),
            "pytorch_version": TORCH_VERSION,
            "numpy_version": NUMPY_VERSION,
            "node_name": platform.node(),
            "platform_release": platform.release()
        }
        
        self._cache['system_info'] = info
        return info
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì •ë°€ ê°ì§€ (ë‹¤ì¤‘ ë°©ë²•)"""
        if 'm3_max' in self._cache:
            return self._cache['m3_max']
        
        try:
            # 1ì°¨: í”Œë«í¼ ì²´í¬
            if platform.system() != "Darwin" or platform.machine() != "arm64":
                self._cache['m3_max'] = False
                return False
            
            # 2ì°¨: ë©”ëª¨ë¦¬ ê¸°ë°˜ ê°ì§€ (M3 MaxëŠ” 96GB/128GB)
            if PSUTIL_AVAILABLE:
                memory_gb = psutil.virtual_memory().total / (1024**3)
                if memory_gb >= 90:  # 90GB ì´ìƒì´ë©´ M3 Max ê°€ëŠ¥ì„± ë†’ìŒ
                    self._cache['m3_max'] = True
                    return True
            
            # 3ì°¨: CPU ì½”ì–´ ìˆ˜ ê¸°ë°˜ ê°ì§€ (M3 MaxëŠ” 16ì½”ì–´)
            cpu_count = os.cpu_count() or 0
            if cpu_count >= 14:  # 14ì½”ì–´ ì´ìƒì´ë©´ M3 Max ê°€ëŠ¥ì„±
                self._cache['m3_max'] = True
                return True
            
            self._cache['m3_max'] = False
            return False
            
        except Exception as e:
            logger.debug(f"M3 Max ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            self._cache['m3_max'] = False
            return False
    
    def _get_system_memory(self) -> float:
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìš©ëŸ‰ í™•ì¸ (GB)"""
        if 'memory_gb' in self._cache:
            return self._cache['memory_gb']
        
        try:
            if PSUTIL_AVAILABLE:
                memory_gb = round(psutil.virtual_memory().total / (1024**3), 1)
            else:
                memory_gb = 16.0  # ê¸°ë³¸ê°’
            
            self._cache['memory_gb'] = memory_gb
            return memory_gb
            
        except Exception:
            self._cache['memory_gb'] = 16.0
            return 16.0
    
    def _get_cpu_cores(self) -> int:
        """CPU ì½”ì–´ ìˆ˜ í™•ì¸"""
        if 'cpu_cores' in self._cache:
            return self._cache['cpu_cores']
        
        try:
            if PSUTIL_AVAILABLE:
                physical = psutil.cpu_count(logical=False) or 8
                logical = psutil.cpu_count(logical=True) or 8
                cores = max(physical, logical)
            else:
                cores = os.cpu_count() or 8
            
            self._cache['cpu_cores'] = cores
            return cores
            
        except Exception:
            self._cache['cpu_cores'] = 8
            return 8
    
    def _get_gpu_info(self) -> Dict[str, Any]:
        """GPU ì •ë³´ ìˆ˜ì§‘"""
        if 'gpu_info' in self._cache:
            return self._cache['gpu_info']
        
        gpu_info = {
            "device": "cpu",
            "name": "CPU",
            "memory_gb": 0,
            "available": True,
            "backend": "CPU",
            "compute_capability": None,
            "driver_version": None
        }
        
        if not TORCH_AVAILABLE:
            self._cache['gpu_info'] = gpu_info
            return gpu_info
        
        try:
            # MPS (Apple Silicon) ì§€ì› í™•ì¸
            if (hasattr(torch.backends, 'mps') and 
                hasattr(torch.backends.mps, 'is_available') and 
                torch.backends.mps.is_available()):
                
                gpu_info.update({
                    "device": "mps",
                    "name": "Apple M3 Max" if self.is_m3_max else "Apple Silicon",
                    "memory_gb": self.memory_gb,  # í†µí•© ë©”ëª¨ë¦¬
                    "available": True,
                    "backend": "Metal Performance Shaders",
                    "unified_memory": True,
                    "neural_engine": self.is_m3_max
                })
            
            # CUDA ì§€ì› í™•ì¸
            elif hasattr(torch, 'cuda') and torch.cuda.is_available():
                try:
                    gpu_props = torch.cuda.get_device_properties(0)
                    gpu_info.update({
                        "device": "cuda",
                        "name": gpu_props.name,
                        "memory_gb": round(gpu_props.total_memory / (1024**3), 1),
                        "available": True,
                        "backend": "CUDA",
                        "compute_capability": f"{gpu_props.major}.{gpu_props.minor}",
                        "multiprocessor_count": gpu_props.multiprocessor_count
                    })
                except Exception as e:
                    logger.debug(f"CUDA ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        except Exception as e:
            logger.debug(f"GPU ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        self._cache['gpu_info'] = gpu_info
        return gpu_info
    
    def _classify_performance(self) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ ë¶„ë¥˜"""
        if self.is_m3_max and self.memory_gb >= 120:
            return "ultra_high"
        elif self.is_m3_max or (self.memory_gb >= 64 and self.cpu_cores >= 12):
            return "high"
        elif self.memory_gb >= 32 and self.cpu_cores >= 8:
            return "medium"
        elif self.memory_gb >= 16:
            return "low"
        else:
            return "minimal"
    
    def _create_optimization_profile(self) -> Dict[str, Any]:
        """ìµœì í™” í”„ë¡œíŒŒì¼ ìƒì„±"""
        profiles = {
            "ultra_high": {
                "batch_size": 8,
                "max_workers": 16,
                "concurrent_sessions": 12,
                "memory_fraction": 0.8,
                "quality_level": "ultra"
            },
            "high": {
                "batch_size": 6,
                "max_workers": 12,
                "concurrent_sessions": 8,
                "memory_fraction": 0.75,
                "quality_level": "high"
            },
            "medium": {
                "batch_size": 4,
                "max_workers": 8,
                "concurrent_sessions": 6,
                "memory_fraction": 0.7,
                "quality_level": "balanced"
            },
            "low": {
                "batch_size": 2,
                "max_workers": 4,
                "concurrent_sessions": 3,
                "memory_fraction": 0.6,
                "quality_level": "balanced"
            },
            "minimal": {
                "batch_size": 1,
                "max_workers": 2,
                "concurrent_sessions": 1,
                "memory_fraction": 0.5,
                "quality_level": "fast"
            }
        }
        
        return profiles.get(self.performance_class, profiles["minimal"])

# =============================================================================
# ğŸ”§ DeviceManager í´ë˜ìŠ¤ (conda_env ì†ì„± í¬í•¨)
# =============================================================================

class DeviceManager:
    """GPU/MPS ë””ë°”ì´ìŠ¤ ê´€ë¦¬ì - conda í™˜ê²½ ì§€ì› ì¶”ê°€"""
    
    def __init__(self):
        self.device = self._detect_device()
        self.is_m3_max = self._detect_m3_max()
        self.memory_gb = self._get_memory_gb()
        
        # conda í™˜ê²½ ì •ë³´ ì¶”ê°€ (ëˆ„ë½ëœ ì†ì„±)
        self.conda_env = self._detect_conda_environment()
        self.is_conda = self.conda_env.get('is_conda', False)
        self.conda_prefix = self.conda_env.get('prefix')
        self.env_name = self.conda_env.get('env_name')
        
        self._initialize_optimizations()
        
    def _detect_conda_environment(self) -> Dict[str, Any]:
        """conda í™˜ê²½ ì •ë³´ ê°ì§€"""
        conda_info = {
            'is_conda': False,
            'env_name': None,
            'prefix': None,
            'python_version': platform.python_version(),
            'optimization_level': 'standard'
        }
        
        try:
            # CONDA_DEFAULT_ENV í™•ì¸
            conda_env = os.environ.get('CONDA_DEFAULT_ENV')
            if conda_env and conda_env != 'base':
                conda_info['is_conda'] = True
                conda_info['env_name'] = conda_env
                conda_info['optimization_level'] = 'conda_optimized'
            
            # CONDA_PREFIX í™•ì¸  
            conda_prefix = os.environ.get('CONDA_PREFIX')
            if conda_prefix:
                conda_info['prefix'] = conda_prefix
                if not conda_info['is_conda']:
                    conda_info['is_conda'] = True
                    conda_info['env_name'] = Path(conda_prefix).name
                    
        except Exception as e:
            print(f"âš ï¸ conda í™˜ê²½ ê°ì§€ ì‹¤íŒ¨: {e}")
            
        return conda_info
    
    def _detect_device(self) -> str:
        """ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            import torch
            
            if (hasattr(torch.backends, 'mps') and 
                torch.backends.mps.is_available()):
                return "mps"
                
            if torch.cuda.is_available():
                return "cuda"
                
            return "cpu"
            
        except ImportError:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        if platform.system() != 'Darwin':
            return False
        
        try:
            if PSUTIL_AVAILABLE:
                memory_gb = psutil.virtual_memory().total / (1024**3)
                return memory_gb >= 90
        except:
            pass
        return False
    
    def _get_memory_gb(self) -> float:
        """ë©”ëª¨ë¦¬ ìš©ëŸ‰ í™•ì¸"""
        try:
            if PSUTIL_AVAILABLE:
                return round(psutil.virtual_memory().total / (1024**3), 1)
        except:
            pass
        return 16.0
    
    def _initialize_optimizations(self):
        """ìµœì í™” ì´ˆê¸°í™”"""
        pass

# =============================================================================
# ğŸ”§ í•µì‹¬ GPUConfig í´ë˜ìŠ¤
# =============================================================================

class GPUConfig:
    """ì™„ì „í•œ GPU ì„¤ì • í´ë˜ìŠ¤ - Import ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""
    
    def __init__(self, device: Optional[str] = None, optimization_level: Optional[str] = None, **kwargs):
        """GPUConfig ì´ˆê¸°í™”"""
        
        # í•˜ë“œì›¨ì–´ ê°ì§€
        self.hardware = HardwareDetector()
        
        # ê¸°ë³¸ ì†ì„± ì„¤ì •
        self.device = self._determine_device(device)
        self.device_name = self.hardware.gpu_info["name"]
        self.device_type = self.device
        self.memory_gb = self.hardware.memory_gb
        self.is_m3_max = self.hardware.is_m3_max
        self.is_initialized = False
        
        # ìµœì í™” ë ˆë²¨ ì„¤ì •
        self.optimization_level = self._determine_optimization_level(optimization_level)
        
        # ì„¤ì • ê³„ì‚°
        self.optimization_settings = self._calculate_optimization_settings()
        self.model_config = self._create_model_config()
        self.device_info = self._collect_device_info()
        self.device_capabilities = self._create_device_capabilities()
        
        # í™˜ê²½ ìµœì í™” ì ìš©
        try:
            self._apply_environment_optimizations()
            self.is_initialized = True
        except Exception as e:
            logger.warning(f"âš ï¸ í™˜ê²½ ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
            self.is_initialized = False
        
        # Float í˜¸í™˜ì„± ëª¨ë“œ
        self.float_compatibility_mode = True
    
    def _determine_device(self, device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ê²°ì •"""
        if device and device != "auto":
            return device
        
        if not TORCH_AVAILABLE:
            return "cpu"
        
        # ìš°ì„ ìˆœìœ„: MPS > CUDA > CPU
        try:
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return "mps"
            elif hasattr(torch, 'cuda') and torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        except Exception:
            return "cpu"
    
    def _determine_optimization_level(self, level: Optional[str]) -> str:
        """ìµœì í™” ë ˆë²¨ ê²°ì •"""
        if level:
            return level
        
        performance_to_optimization = {
            "ultra_high": "ultra",
            "high": "performance", 
            "medium": "balanced",
            "low": "balanced",
            "minimal": "safe"
        }
        
        return performance_to_optimization.get(self.hardware.performance_class, "balanced")
    
    def _calculate_optimization_settings(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ê³„ì‚°"""
        base_profile = self.hardware.optimization_profile.copy()
        
        # M3 Max íŠ¹í™” ìµœì í™”
        if self.is_m3_max:
            base_profile.update({
                "dtype": "float32",
                "mixed_precision": False,
                "memory_efficient_attention": True,
                "unified_memory_optimization": True,
                "metal_performance_shaders": True,
                "neural_engine_acceleration": True
            })
        else:
            base_profile.update({
                "dtype": "float32",
                "mixed_precision": False,
                "memory_efficient_attention": False,
                "unified_memory_optimization": False,
                "metal_performance_shaders": self.device == "mps",
                "neural_engine_acceleration": False
            })
        
        return base_profile
    
    def _create_model_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„¤ì • ìƒì„±"""
        return {
            "device": self.device,
            "device_name": self.device_name,
            "dtype": self.optimization_settings["dtype"],
            "batch_size": self.optimization_settings["batch_size"],
            "max_workers": self.optimization_settings["max_workers"],
            "memory_fraction": self.optimization_settings["memory_fraction"],
            "optimization_level": self.optimization_level,
            "quality_level": self.optimization_settings["quality_level"],
            "float_compatibility_mode": True,
            "mps_fallback_enabled": self.device == "mps",
            "pytorch_version": TORCH_VERSION,
            "numpy_version": NUMPY_VERSION,
            "m3_max_optimized": self.is_m3_max,
            "conda_environment": os.environ.get("CONDA_DEFAULT_ENV", "unknown") != "unknown"
        }
    
    def _collect_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        info = {
            "device": self.device,
            "device_name": self.device_name,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "performance_class": self.hardware.performance_class,
            "optimization_level": self.optimization_level,
            "cpu_cores": self.hardware.cpu_cores,
            "system_info": self.hardware.system_info,
            "gpu_info": self.hardware.gpu_info,
            "pytorch_available": TORCH_AVAILABLE,
            "pytorch_version": TORCH_VERSION,
            "numpy_available": NUMPY_AVAILABLE,
            "numpy_version": NUMPY_VERSION,
            "psutil_available": PSUTIL_AVAILABLE,
            "float_compatibility_mode": True,
            "conda_environment": os.environ.get("CONDA_DEFAULT_ENV", "unknown"),
            "initialization_time": time.time()
        }
        
        return info
    
    def _create_device_capabilities(self) -> DeviceCapabilities:
        """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ì •ë³´ ìƒì„±"""
        if self.is_m3_max:
            return DeviceCapabilities(
                device=self.device,
                name=self.device_name,
                memory_gb=self.memory_gb,
                supports_fp16=False,
                supports_fp32=True,
                supports_neural_engine=True,
                supports_metal_shaders=True,
                unified_memory=True,
                max_batch_size=self.optimization_settings["batch_size"] * 2,
                recommended_image_size=(768, 768) if self.memory_gb >= 120 else (640, 640)
            )
        else:
            return DeviceCapabilities(
                device=self.device,
                name=self.device_name,
                memory_gb=self.memory_gb,
                supports_fp16=False,
                supports_fp32=True,
                supports_neural_engine=False,
                supports_metal_shaders=self.device == "mps",
                unified_memory=False,
                max_batch_size=1,
                recommended_image_size=(512, 512)
            )
    
    def _apply_environment_optimizations(self):
        """í™˜ê²½ ìµœì í™” ì ìš©"""
        if not TORCH_AVAILABLE:
            return
        
        try:
            torch.set_num_threads(self.optimization_settings["max_workers"])
            
            if self.device == "mps":
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                if self.is_m3_max:
                    os.environ.update({
                        'OMP_NUM_THREADS': '16',
                        'MKL_NUM_THREADS': '16'
                    })
            
            gc.collect()
            
        except Exception as e:
            logger.warning(f"âš ï¸ í™˜ê²½ ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
    
    # í•µì‹¬ ì¸í„°í˜ì´ìŠ¤ ë©”ì„œë“œë“¤
    def get_device(self) -> str:
        return self.device
    
    def get_device_name(self) -> str:
        return self.device_name
    
    def get_memory_info(self) -> Dict[str, Any]:
        try:
            if PSUTIL_AVAILABLE:
                memory = psutil.virtual_memory()
                return {
                    "total_gb": round(memory.total / (1024**3), 1),
                    "available_gb": round(memory.available / (1024**3), 1),
                    "used_gb": round(memory.used / (1024**3), 1),
                    "used_percent": round(memory.percent, 1),
                    "device": self.device,
                    "timestamp": time.time()
                }
        except Exception:
            pass
        
        return {
            "total_gb": self.memory_gb,
            "available_gb": self.memory_gb * 0.7,
            "used_percent": 30.0,
            "device": self.device,
            "timestamp": time.time(),
            "fallback_mode": True
        }
    
    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            start_time = time.time()
            methods_used = []
            
            collected = gc.collect()
            if collected > 0:
                methods_used.append(f"gc_collected_{collected}")
            
            if not TORCH_AVAILABLE:
                return {
                    "success": True,
                    "device": self.device,
                    "methods": methods_used,
                    "duration": time.time() - start_time,
                    "pytorch_available": False
                }
            
            # MPS/CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device in ["mps", "cuda"]:
                cleanup_result = safe_mps_empty_cache()
                if cleanup_result["success"]:
                    methods_used.append(cleanup_result["method"])
            
            if aggressive:
                for _ in range(3):
                    gc.collect()
                methods_used.append("aggressive_gc")
            
            return {
                "success": True,
                "device": self.device,
                "methods": methods_used,
                "duration": round(time.time() - start_time, 3),
                "pytorch_available": True,
                "aggressive": aggressive,
                "timestamp": time.time()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)[:200],
                "device": self.device,
                "timestamp": time.time()
            }
    
    def get_optimal_settings(self) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì„¤ì • ë°˜í™˜"""
        return {
            "device_config": self.model_config.copy(),
            "optimization_settings": self.optimization_settings.copy(),
            "device_info": self.device_info.copy()
        }
    
    def get_device_capabilities(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ì •ë³´ ë°˜í™˜"""
        return {
            "device": self.device_capabilities.device,
            "name": self.device_capabilities.name,
            "memory_gb": self.device_capabilities.memory_gb,
            "supports_fp16": self.device_capabilities.supports_fp16,
            "supports_fp32": self.device_capabilities.supports_fp32,
            "supports_neural_engine": self.device_capabilities.supports_neural_engine,
            "supports_metal_shaders": self.device_capabilities.supports_metal_shaders,
            "unified_memory": self.device_capabilities.unified_memory,
            "max_batch_size": self.device_capabilities.max_batch_size,
            "recommended_image_size": self.device_capabilities.recommended_image_size,
            "optimization_level": self.optimization_level,
            "performance_class": self.hardware.performance_class,
            "pytorch_version": TORCH_VERSION,
            "float_compatibility_mode": True
        }
    
    # ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤
    def get(self, key: str, default: Any = None) -> Any:
        """ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì ‘ê·¼"""
        direct_attrs = {
            'device': self.device,
            'device_name': self.device_name,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'optimization_level': self.optimization_level,
            'is_initialized': self.is_initialized,
            'float_compatibility_mode': self.float_compatibility_mode,
            'pytorch_version': TORCH_VERSION,
            'numpy_version': NUMPY_VERSION
        }
        
        if key in direct_attrs:
            return direct_attrs[key]
        
        for config_dict in [self.model_config, self.optimization_settings, self.device_info]:
            if key in config_dict:
                return config_dict[key]
        
        if hasattr(self, key):
            return getattr(self, key)
        
        return default
    
    def __getitem__(self, key: str) -> Any:
        result = self.get(key)
        if result is None:
            raise KeyError(f"Key '{key}' not found in GPUConfig")
        return result
    
    def __contains__(self, key: str) -> bool:
        return self.get(key) is not None

# =============================================================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

@lru_cache(maxsize=1)
def get_gpu_config(**kwargs) -> GPUConfig:
    """GPU ì„¤ì • ì‹±ê¸€í†¤ íŒ©í† ë¦¬"""
    return GPUConfig(**kwargs)

def get_device_config() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config.model_config
    except Exception as e:
        return {"error": str(e), "device": "cpu"}

def get_model_config() -> Dict[str, Any]:
    """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
    return get_device_config()

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config.device_info
    except Exception as e:
        return {"error": str(e), "device": "cpu"}

def get_device() -> str:
    """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config.device
    except:
        return "cpu"

def get_device_name() -> str:
    """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config.device_name
    except:
        return "CPU"

def is_m3_max() -> bool:
    """M3 Max ì—¬ë¶€ í™•ì¸"""
    try:
        config = get_gpu_config()
        return config.is_m3_max
    except:
        return False

def get_optimal_settings() -> Dict[str, Any]:
    """ìµœì í™”ëœ ì„¤ì • ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config.get_optimal_settings()
    except Exception as e:
        return {"error": str(e)}

def get_device_capabilities() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ì •ë³´ ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config.get_device_capabilities()
    except Exception as e:
        return {"error": str(e)}

# =============================================================================
# ğŸ”§ ì „ì—­ GPU ì„¤ì • ë§¤ë‹ˆì € ìƒì„±
# =============================================================================

try:
    gpu_config = GPUConfig()
    
    DEVICE = gpu_config.device
    DEVICE_NAME = gpu_config.device_name
    DEVICE_TYPE = gpu_config.device_type
    MODEL_CONFIG = gpu_config.model_config
    DEVICE_INFO = gpu_config.device_info
    IS_M3_MAX = gpu_config.is_m3_max
    
    if IS_M3_MAX:
        print(f"ğŸ M3 Max ({DEVICE}) ìµœì í™” ëª¨ë“œ í™œì„±í™” - Float32 ì•ˆì •ì„± ìš°ì„ ")
    else:
        print(f"âœ… GPU ì„¤ì • ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - ì•ˆì •ì„± ìš°ì„  ëª¨ë“œ")

except Exception as e:
    print(f"âš ï¸ GPU ì„¤ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:100]}")
    
    DEVICE = "cpu"
    DEVICE_NAME = "CPU (Fallback)"
    DEVICE_TYPE = "cpu"
    MODEL_CONFIG = {
        "device": "cpu",
        "dtype": "float32",
        "batch_size": 1,
        "optimization_level": "safe",
        "float_compatibility_mode": True
    }
    DEVICE_INFO = {
        "device": "cpu",
        "error": "GPU config initialization failed",
        "fallback_mode": True
    }
    IS_M3_MAX = False
    
    class DummyGPUConfig:
        def __init__(self):
            self.device = "cpu"
            self.device_name = "CPU (Fallback)"
            self.is_m3_max = False
            self.is_initialized = False
        
        def get(self, key, default=None):
            return getattr(self, key, default)
        
        def cleanup_memory(self, aggressive=False):
            return {"success": True, "method": "fallback_gc", "device": "cpu"}
    
    gpu_config = DummyGPUConfig()

# =============================================================================
# ğŸ”§ Export ë¦¬ìŠ¤íŠ¸
# =============================================================================

__all__ = [
    'GPUConfig', 'DeviceManager', 'HardwareDetector', 'DeviceCapabilities',
    'gpu_config', 'DEVICE', 'DEVICE_NAME', 'DEVICE_TYPE', 
    'MODEL_CONFIG', 'DEVICE_INFO', 'IS_M3_MAX',
    'get_gpu_config', 'get_device_config', 'get_model_config', 'get_device_info',
    'get_device', 'get_device_name', 'is_m3_max', 'get_optimal_settings', 
    'get_device_capabilities', 'safe_mps_empty_cache',
    'OptimizationLevel', 'DeviceType'
]
EOF

echo "âœ… ìƒˆ GPU Config íŒŒì¼ ìƒì„± ì™„ë£Œ"

# íŒŒì¼ ê¶Œí•œ ì„¤ì •
chmod 644 backend/app/core/gpu_config.py

# ê²€ì¦ í…ŒìŠ¤íŠ¸
echo "ğŸ§ª GPU Config íŒŒì¼ ê²€ì¦ ì¤‘..."
cd backend
python -c "
try:
    from app.core.gpu_config import GPUConfig, get_device, safe_mps_empty_cache
    print('âœ… GPU Config import ì„±ê³µ')
    print(f'   ë””ë°”ì´ìŠ¤: {get_device()}')
    
    # safe_mps_empty_cache í…ŒìŠ¤íŠ¸
    result = safe_mps_empty_cache()
    print(f'   MPS ìºì‹œ ì •ë¦¬: {result[\"method\"]}')
    
    # DeviceManager conda_env ì†ì„± í…ŒìŠ¤íŠ¸
    from app.core.gpu_config import DeviceManager
    dm = DeviceManager()
    print(f'   conda_env ì†ì„±: {hasattr(dm, \"conda_env\")}')
    
except Exception as e:
    print(f'âŒ GPU Config ê²€ì¦ ì‹¤íŒ¨: {e}')
"

echo ""
echo "ğŸ‰ GPU Config íŒŒì¼ êµì²´ ì™„ë£Œ!"
echo ""
echo "ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:"
echo "1. ì„œë²„ ì¬ì‹œì‘: python app/main.py"
echo "2. ë¡œê·¸ì—ì„œ 'conda_env' ì˜¤ë¥˜ê°€ ì‚¬ë¼ì¡ŒëŠ”ì§€ í™•ì¸"
echo "3. MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ê¸°ëŠ¥ ì •ìƒ ì‘ë™ í™•ì¸"
echo ""
echo "ğŸ“„ ë°±ì—… íŒŒì¼: $BACKUP_FILE"