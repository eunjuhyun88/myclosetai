#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI ëª¨ë¸ ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ
âœ… ì•ˆì „í•œ ì¤‘ë³µ ì œê±°
âœ… ë””ë ‰í† ë¦¬ êµ¬ì¡° ìµœì í™”  
âœ… auto_model_detector.py ê°œì„ 
âœ… conda í™˜ê²½ ìµœì í™”
"""

import os
import sys
import shutil
import hashlib
import json
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime
from typing import Dict, List, Set, Tuple, Optional, Any
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ModelCleanupSystem:
    """AI ëª¨ë¸ ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, base_path: str = "backend/ai_models"):
        self.base_path = Path(base_path)
        self.backup_path = Path("backup_models_" + datetime.now().strftime("%Y%m%d_%H%M%S"))
        self.analysis_result = {}
        self.cleanup_plan = {}
        
        # ë‹¨ê³„ë³„ í‘œì¤€ ë””ë ‰í† ë¦¬ êµ¬ì¡°
        self.standard_structure = {
            "step_01_human_parsing": ["schp", "graphonomy", "densepose", "parsing"],
            "step_02_pose_estimation": ["openpose", "dwpose", "coco", "pose"],
            "step_03_cloth_segmentation": ["u2net", "cloth", "segmentation", "mask"],
            "step_04_geometric_matching": ["tps", "geometric", "matching", "transform"],
            "step_05_cloth_warping": ["warping", "cloth_warping", "deformation"],
            "step_06_virtual_fitting": ["viton", "diffusion", "fitting", "ootd"],
            "step_07_post_processing": ["postprocess", "refinement", "enhancement"],
            "step_08_quality_assessment": ["quality", "assessment", "metric"]
        }
        
    def run_complete_cleanup(self, dry_run: bool = True) -> Dict[str, Any]:
        """ì™„ì „í•œ ì •ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            logger.info("ğŸš€ AI ëª¨ë¸ ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ ì‹œì‘")
            
            # 1ë‹¨ê³„: í˜„ì¬ ìƒíƒœ ë¶„ì„
            logger.info("ğŸ“Š 1ë‹¨ê³„: í˜„ì¬ ìƒíƒœ ë¶„ì„")
            self.analysis_result = self.analyze_current_state()
            
            # 2ë‹¨ê³„: ì •ë¦¬ ê³„íš ìˆ˜ë¦½
            logger.info("ğŸ“‹ 2ë‹¨ê³„: ì •ë¦¬ ê³„íš ìˆ˜ë¦½")
            self.cleanup_plan = self.create_cleanup_plan()
            
            # 3ë‹¨ê³„: ì•ˆì „í•œ ë°±ì—… (dry_runì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
            if not dry_run:
                logger.info("ğŸ’¾ 3ë‹¨ê³„: ì•ˆì „í•œ ë°±ì—…")
                self.create_safety_backup()
            
            # 4ë‹¨ê³„: ì¤‘ë³µ íŒŒì¼ ì œê±°
            logger.info("ğŸ—‚ï¸ 4ë‹¨ê³„: ì¤‘ë³µ íŒŒì¼ ì •ë¦¬")
            duplicate_results = self.cleanup_duplicates(dry_run=dry_run)
            
            # 5ë‹¨ê³„: ë””ë ‰í† ë¦¬ êµ¬ì¡° ìµœì í™”
            logger.info("ğŸ“ 5ë‹¨ê³„: ë””ë ‰í† ë¦¬ êµ¬ì¡° ìµœì í™”")
            structure_results = self.optimize_directory_structure(dry_run=dry_run)
            
            # 6ë‹¨ê³„: auto_model_detector.py ê°œì„ 
            logger.info("ğŸ” 6ë‹¨ê³„: auto_model_detector.py ê°œì„ ")
            detector_results = self.improve_auto_detector(dry_run=dry_run)
            
            # 7ë‹¨ê³„: ìµœì¢… ê²€ì¦
            logger.info("âœ… 7ë‹¨ê³„: ìµœì¢… ê²€ì¦")
            validation_results = self.validate_cleanup()
            
            # ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
            final_report = self.generate_final_report(
                duplicate_results, structure_results, 
                detector_results, validation_results
            )
            
            logger.info("ğŸ‰ AI ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ!")
            return final_report
            
        except Exception as e:
            logger.error(f"âŒ ì •ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def analyze_current_state(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ìƒì„¸ ë¶„ì„"""
        try:
            analysis = {
                "total_files": 0,
                "total_size_gb": 0.0,
                "by_extension": defaultdict(int),
                "by_step": defaultdict(int),
                "duplicates": [],
                "large_files": [],
                "broken_links": [],
                "file_details": []
            }
            
            # ëª¨ë“  íŒŒì¼ ìŠ¤ìº”
            for root, dirs, files in os.walk(self.base_path):
                for file in files:
                    file_path = Path(root) / file
                    
                    try:
                        if not file_path.exists():
                            analysis["broken_links"].append(str(file_path))
                            continue
                            
                        size_mb = file_path.stat().st_size / (1024 * 1024)
                        analysis["total_size_gb"] += size_mb / 1024
                        analysis["total_files"] += 1
                        
                        # í™•ì¥ìë³„ ë¶„ë¥˜
                        ext = file_path.suffix.lower()
                        analysis["by_extension"][ext] += 1
                        
                        # ë‹¨ê³„ë³„ ë¶„ë¥˜
                        step = self.classify_by_step(file_path)
                        analysis["by_step"][step] += 1
                        
                        # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì¶”ì 
                        if size_mb > 1000:  # 1GB ì´ìƒ
                            analysis["large_files"].append({
                                "path": str(file_path),
                                "size_gb": round(size_mb / 1024, 2)
                            })
                        
                        # íŒŒì¼ ìƒì„¸ ì •ë³´
                        analysis["file_details"].append({
                            "path": str(file_path.relative_to(self.base_path)),
                            "size_mb": round(size_mb, 2),
                            "extension": ext,
                            "step": step,
                            "modified": file_path.stat().st_mtime
                        })
                        
                    except Exception as e:
                        logger.warning(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
            
            # ì¤‘ë³µ íƒì§€
            analysis["duplicates"] = self.find_duplicates()
            
            return analysis
            
        except Exception as e:
            logger.error(f"ìƒíƒœ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def find_duplicates(self) -> List[Dict[str, Any]]:
        """ì •êµí•œ ì¤‘ë³µ íŒŒì¼ íƒì§€"""
        try:
            # 1. í¬ê¸° ê¸°ë°˜ ê·¸ë£¹í™”
            size_groups = defaultdict(list)
            
            for file_info in self.analysis_result.get("file_details", []):
                if file_info["size_mb"] > 10:  # 10MB ì´ìƒë§Œ
                    size_groups[file_info["size_mb"]].append(file_info)
            
            # 2. ë™ì¼ í¬ê¸° ê·¸ë£¹ì—ì„œ í•´ì‹œ ë¹„êµ
            duplicates = []
            
            for size_mb, files in size_groups.items():
                if len(files) > 1:
                    # í•´ì‹œ ê·¸ë£¹í™”
                    hash_groups = defaultdict(list)
                    
                    for file_info in files:
                        try:
                            file_path = self.base_path / file_info["path"]
                            file_hash = self.get_file_hash(file_path)
                            hash_groups[file_hash].append(file_info)
                        except Exception as e:
                            logger.warning(f"í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨ {file_info['path']}: {e}")
                    
                    # ì‹¤ì œ ì¤‘ë³µ (ë™ì¼ í•´ì‹œ)
                    for file_hash, duplicate_files in hash_groups.items():
                        if len(duplicate_files) > 1:
                            duplicates.append({
                                "hash": file_hash,
                                "size_mb": size_mb,
                                "files": duplicate_files,
                                "waste_mb": size_mb * (len(duplicate_files) - 1)
                            })
            
            # 3. ë²„ì „ ë²ˆí˜¸ ì¤‘ë³µ íƒì§€ (_01, _02 ë“±)
            version_duplicates = self.find_version_duplicates()
            duplicates.extend(version_duplicates)
            
            return duplicates
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ íƒì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def find_version_duplicates(self) -> List[Dict[str, Any]]:
        """ë²„ì „ ë²ˆí˜¸ ì¤‘ë³µ íƒì§€"""
        import re
        
        version_groups = defaultdict(list)
        
        for file_info in self.analysis_result.get("file_details", []):
            # ë²„ì „ ë²ˆí˜¸ ì œê±°í•œ ê¸°ë³¸ ì´ë¦„
            path = file_info["path"]
            base_name = re.sub(r'_\d+(\.(pth|pt|safetensors|bin|onnx))?$', '', Path(path).stem)
            
            version_groups[base_name].append(file_info)
        
        version_duplicates = []
        for base_name, files in version_groups.items():
            if len(files) > 1:
                total_size = sum(f["size_mb"] for f in files)
                max_size = max(f["size_mb"] for f in files)
                
                version_duplicates.append({
                    "type": "version_duplicate",
                    "base_name": base_name,
                    "files": files,
                    "total_size_mb": total_size,
                    "waste_mb": total_size - max_size  # ê°€ì¥ í° íŒŒì¼ ì œì™¸
                })
        
        return version_duplicates
    
    def create_cleanup_plan(self) -> Dict[str, Any]:
        """ì •ë¦¬ ê³„íš ìˆ˜ë¦½"""
        try:
            plan = {
                "files_to_remove": [],
                "files_to_move": [],
                "directories_to_create": [],
                "estimated_savings_gb": 0.0,
                "safety_checks": []
            }
            
            # ì¤‘ë³µ íŒŒì¼ ì œê±° ê³„íš
            for duplicate_group in self.analysis_result.get("duplicates", []):
                if duplicate_group.get("type") == "version_duplicate":
                    # ë²„ì „ ì¤‘ë³µì˜ ê²½ìš° ê°€ì¥ í° íŒŒì¼ë§Œ ìœ ì§€
                    files = duplicate_group["files"]
                    largest_file = max(files, key=lambda x: x["size_mb"])
                    
                    for file_info in files:
                        if file_info != largest_file:
                            plan["files_to_remove"].append({
                                "path": file_info["path"],
                                "reason": f"ë²„ì „ ì¤‘ë³µ - {largest_file['path']} ìœ ì§€",
                                "size_mb": file_info["size_mb"]
                            })
                            plan["estimated_savings_gb"] += file_info["size_mb"] / 1024
                
                elif "hash" in duplicate_group:
                    # í•´ì‹œ ì¤‘ë³µì˜ ê²½ìš° ì²« ë²ˆì§¸ íŒŒì¼ë§Œ ìœ ì§€
                    files = duplicate_group["files"]
                    keep_file = files[0]
                    
                    for file_info in files[1:]:
                        plan["files_to_remove"].append({
                            "path": file_info["path"],
                            "reason": f"í•´ì‹œ ì¤‘ë³µ - {keep_file['path']} ìœ ì§€",
                            "size_mb": file_info["size_mb"]
                        })
                        plan["estimated_savings_gb"] += file_info["size_mb"] / 1024
            
            # ë””ë ‰í† ë¦¬ êµ¬ì¡° ìµœì í™” ê³„íš
            self.plan_directory_optimization(plan)
            
            return plan
            
        except Exception as e:
            logger.error(f"ì •ë¦¬ ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨: {e}")
            return {}
    
    def plan_directory_optimization(self, plan: Dict[str, Any]):
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ìµœì í™” ê³„íš"""
        try:
            # í˜„ì¬ íŒŒì¼ë“¤ì˜ ë‹¨ê³„ë³„ ë¶„í¬ í™•ì¸
            step_files = defaultdict(list)
            
            for file_info in self.analysis_result.get("file_details", []):
                step = self.classify_by_step(Path(file_info["path"]))
                if step != "unknown":
                    step_files[step].append(file_info)
            
            # í‘œì¤€ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¡œ ì´ë™ ê³„íš
            for step, files in step_files.items():
                target_dir = f"organized/{step}"
                
                if target_dir not in [d["path"] for d in plan["directories_to_create"]]:
                    plan["directories_to_create"].append({
                        "path": target_dir,
                        "purpose": f"{step} ëª¨ë¸ë“¤ ì •ë¦¬"
                    })
                
                for file_info in files:
                    current_path = file_info["path"]
                    if not current_path.startswith(f"organized/{step}/"):
                        new_path = f"{target_dir}/{Path(current_path).name}"
                        
                        plan["files_to_move"].append({
                            "from": current_path,
                            "to": new_path,
                            "reason": f"{step} ë””ë ‰í† ë¦¬ë¡œ ì •ë¦¬"
                        })
            
        except Exception as e:
            logger.error(f"ë””ë ‰í† ë¦¬ ìµœì í™” ê³„íš ì‹¤íŒ¨: {e}")
    
    def cleanup_duplicates(self, dry_run: bool = True) -> Dict[str, Any]:
        """ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ ì‹¤í–‰"""
        try:
            results = {
                "removed_files": [],
                "saved_space_gb": 0.0,
                "errors": []
            }
            
            for item in self.cleanup_plan.get("files_to_remove", []):
                try:
                    file_path = self.base_path / item["path"]
                    
                    if dry_run:
                        logger.info(f"[DRY RUN] ì œê±° ì˜ˆì •: {file_path}")
                        results["removed_files"].append(item["path"])
                        results["saved_space_gb"] += item["size_mb"] / 1024
                    else:
                        if file_path.exists():
                            file_path.unlink()
                            logger.info(f"âœ… ì œê±° ì™„ë£Œ: {file_path}")
                            results["removed_files"].append(item["path"])
                            results["saved_space_gb"] += item["size_mb"] / 1024
                        else:
                            logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}")
                            
                except Exception as e:
                    error_msg = f"íŒŒì¼ ì œê±° ì‹¤íŒ¨ {item['path']}: {e}"
                    logger.error(error_msg)
                    results["errors"].append(error_msg)
            
            return results
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"errors": [str(e)]}
    
    def optimize_directory_structure(self, dry_run: bool = True) -> Dict[str, Any]:
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ìµœì í™”"""
        try:
            results = {
                "created_directories": [],
                "moved_files": [],
                "errors": []
            }
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            for dir_info in self.cleanup_plan.get("directories_to_create", []):
                try:
                    dir_path = self.base_path / dir_info["path"]
                    
                    if dry_run:
                        logger.info(f"[DRY RUN] ë””ë ‰í† ë¦¬ ìƒì„± ì˜ˆì •: {dir_path}")
                        results["created_directories"].append(dir_info["path"])
                    else:
                        dir_path.mkdir(parents=True, exist_ok=True)
                        logger.info(f"âœ… ë””ë ‰í† ë¦¬ ìƒì„±: {dir_path}")
                        results["created_directories"].append(dir_info["path"])
                        
                except Exception as e:
                    error_msg = f"ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨ {dir_info['path']}: {e}"
                    logger.error(error_msg)
                    results["errors"].append(error_msg)
            
            # íŒŒì¼ ì´ë™
            for move_info in self.cleanup_plan.get("files_to_move", []):
                try:
                    src_path = self.base_path / move_info["from"]
                    dst_path = self.base_path / move_info["to"]
                    
                    if dry_run:
                        logger.info(f"[DRY RUN] ì´ë™ ì˜ˆì •: {src_path} â†’ {dst_path}")
                        results["moved_files"].append(move_info)
                    else:
                        if src_path.exists():
                            dst_path.parent.mkdir(parents=True, exist_ok=True)
                            shutil.move(str(src_path), str(dst_path))
                            logger.info(f"âœ… ì´ë™ ì™„ë£Œ: {src_path} â†’ {dst_path}")
                            results["moved_files"].append(move_info)
                        else:
                            logger.warning(f"âš ï¸ ì†ŒìŠ¤ íŒŒì¼ ì—†ìŒ: {src_path}")
                            
                except Exception as e:
                    error_msg = f"íŒŒì¼ ì´ë™ ì‹¤íŒ¨ {move_info}: {e}"
                    logger.error(error_msg)
                    results["errors"].append(error_msg)
            
            return results
            
        except Exception as e:
            logger.error(f"êµ¬ì¡° ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"errors": [str(e)]}
    
    def improve_auto_detector(self, dry_run: bool = True) -> Dict[str, Any]:
        """auto_model_detector.py ê°œì„ """
        try:
            results = {
                "improvements": [],
                "new_patterns": [],
                "optimizations": []
            }
            
            # ì‹¤ì œ íŒŒì¼ë“¤ ê¸°ë°˜ìœ¼ë¡œ íŒ¨í„´ ê°œì„ 
            file_patterns = self.analyze_file_patterns()
            
            # ìƒˆë¡œìš´ auto_model_detector.py ì½”ë“œ ìƒì„±
            improved_detector = self.generate_improved_detector(file_patterns)
            
            if dry_run:
                logger.info("[DRY RUN] auto_model_detector.py ê°œì„  ë¯¸ë¦¬ë³´ê¸°")
                results["improvements"] = ["íŒ¨í„´ ë§¤ì¹­ ê°œì„ ", "ìºì‹± ìµœì í™”", "ì„±ëŠ¥ í–¥ìƒ"]
                results["new_patterns"] = list(file_patterns.keys())
            else:
                # ë°±ì—… í›„ ê°œì„ ëœ ë²„ì „ ì €ì¥
                detector_path = Path("backend/app/ai_pipeline/utils/auto_model_detector.py")
                if detector_path.exists():
                    backup_path = detector_path.with_suffix('.py.backup')
                    shutil.copy2(detector_path, backup_path)
                    logger.info(f"âœ… ë°±ì—… ìƒì„±: {backup_path}")
                
                with open(detector_path, 'w', encoding='utf-8') as f:
                    f.write(improved_detector)
                
                logger.info("âœ… auto_model_detector.py ê°œì„  ì™„ë£Œ")
                results["improvements"] = ["íŒŒì¼ ì €ì¥ ì™„ë£Œ"]
            
            return results
            
        except Exception as e:
            logger.error(f"auto_detector ê°œì„  ì‹¤íŒ¨: {e}")
            return {"errors": [str(e)]}
    
    def analyze_file_patterns(self) -> Dict[str, List[str]]:
        """ì‹¤ì œ íŒŒì¼ë“¤ ê¸°ë°˜ íŒ¨í„´ ë¶„ì„"""
        patterns = defaultdict(list)
        
        for file_info in self.analysis_result.get("file_details", []):
            step = file_info["step"]
            if step != "unknown":
                file_name = Path(file_info["path"]).name
                patterns[step].append(file_name)
        
        return dict(patterns)
    
    def generate_improved_detector(self, file_patterns: Dict[str, List[str]]) -> str:
        """ê°œì„ ëœ auto_model_detector.py ì½”ë“œ ìƒì„±"""
        
        improved_code = '''#!/usr/bin/env python3
"""
ğŸ”¥ ê°œì„ ëœ AI ëª¨ë¸ ìë™ íƒì§€ ì‹œìŠ¤í…œ
âœ… ì‹¤ì œ íŒŒì¼ ê¸°ë°˜ íŒ¨í„´ ë§¤ì¹­
âœ… ìºì‹± ìµœì í™”
âœ… ì„±ëŠ¥ í–¥ìƒ
"""

import os
import re
import json
import time
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class ModelMatch:
    """ëª¨ë¸ ë§¤ì¹­ ê²°ê³¼"""
    file_path: Path
    confidence: float
    step_name: str
    file_size_mb: float
    match_reason: str

class ImprovedAutoModelDetector:
    """ê°œì„ ëœ AI ëª¨ë¸ ìë™ íƒì§€ê¸°"""
    
    def __init__(self, base_path: str = "backend/ai_models"):
        self.base_path = Path(base_path)
        self.cache = {}
        self.cache_file = self.base_path / ".detector_cache.json"
        
        # ì‹¤ì œ íŒŒì¼ ê¸°ë°˜ íŒ¨í„´
        self.step_patterns = {
'''

        # ì‹¤ì œ íŒŒì¼ íŒ¨í„´ ì¶”ê°€
        for step, files in file_patterns.items():
            improved_code += f'            "{step}": [\n'
            
            # íŒŒì¼ëª…ì—ì„œ íŒ¨í„´ ì¶”ì¶œ
            unique_patterns = set()
            for file_name in files[:10]:  # ìƒìœ„ 10ê°œë§Œ
                # í™•ì¥ì ì œê±°í•˜ê³  íŒ¨í„´ ìƒì„±
                base_name = Path(file_name).stem.lower()
                if len(base_name) > 3:
                    pattern = f".*{re.escape(base_name[:5])}.*"
                    unique_patterns.add(pattern)
            
            for pattern in sorted(unique_patterns)[:5]:  # ìƒìœ„ 5ê°œë§Œ
                improved_code += f'                r"{pattern}",\n'
            
            improved_code += '            ],\n'
        
        improved_code += '''        }
        
        self.load_cache()
    
    def find_best_model_for_step(self, step_name: str) -> Optional[ModelMatch]:
        """Stepì— ìµœì í™”ëœ ëª¨ë¸ ì°¾ê¸°"""
        try:
            cache_key = f"best_model_{step_name}"
            
            # ìºì‹œ í™•ì¸
            if cache_key in self.cache:
                cached_result = self.cache[cache_key]
                if time.time() - cached_result["timestamp"] < 3600:  # 1ì‹œê°„ ìºì‹œ
                    return self.dict_to_model_match(cached_result["result"])
            
            # ì‹¤ì œ ê²€ìƒ‰
            candidates = []
            patterns = self.step_patterns.get(step_name, [])
            
            for pattern in patterns:
                matches = self.scan_files_by_pattern(pattern)
                candidates.extend(matches)
            
            # ìµœê³  í›„ë³´ ì„ íƒ
            best_match = self.select_best_candidate(candidates, step_name)
            
            # ìºì‹œ ì €ì¥
            if best_match:
                self.cache[cache_key] = {
                    "result": self.model_match_to_dict(best_match),
                    "timestamp": time.time()
                }
                self.save_cache()
            
            return best_match
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ê²€ìƒ‰ ì‹¤íŒ¨ {step_name}: {e}")
            return None
    
    def scan_files_by_pattern(self, pattern: str) -> List[ModelMatch]:
        """íŒ¨í„´ìœ¼ë¡œ íŒŒì¼ ìŠ¤ìº”"""
        matches = []
        
        try:
            for file_path in self.base_path.rglob("*"):
                if file_path.is_file() and re.match(pattern, file_path.name, re.IGNORECASE):
                    try:
                        size_mb = file_path.stat().st_size / (1024 * 1024)
                        
                        # ëª¨ë¸ íŒŒì¼ë§Œ (1MB ì´ìƒ)
                        if size_mb >= 1.0 and file_path.suffix.lower() in ['.pth', '.pt', '.safetensors', '.bin', '.onnx']:
                            match = ModelMatch(
                                file_path=file_path,
                                confidence=0.8,  # ê¸°ë³¸ ì‹ ë¢°ë„
                                step_name="",
                                file_size_mb=size_mb,
                                match_reason=f"íŒ¨í„´ ë§¤ì¹­: {pattern}"
                            )
                            matches.append(match)
                            
                    except Exception as e:
                        logger.warning(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
                        
        except Exception as e:
            logger.error(f"íŒ¨í„´ ìŠ¤ìº” ì‹¤íŒ¨ {pattern}: {e}")
        
        return matches
    
    def select_best_candidate(self, candidates: List[ModelMatch], step_name: str) -> Optional[ModelMatch]:
        """ìµœê³  í›„ë³´ ì„ íƒ"""
        if not candidates:
            return None
        
        # ì ìˆ˜ ê³„ì‚°
        for candidate in candidates:
            score = 0.0
            
            # íŒŒì¼ í¬ê¸° ì ìˆ˜ (ì ë‹¹í•œ í¬ê¸°ê°€ ì¢‹ìŒ)
            if 10 <= candidate.file_size_mb <= 1000:
                score += 0.3
            elif 1000 <= candidate.file_size_mb <= 5000:
                score += 0.2
            else:
                score += 0.1
            
            # íŒŒì¼ëª… ê´€ë ¨ì„± ì ìˆ˜
            file_name = candidate.file_path.name.lower()
            if step_name.split('_')[-1] in file_name:  # ë‹¨ê³„ í‚¤ì›Œë“œ í¬í•¨
                score += 0.4
            
            # í™•ì¥ì ì ìˆ˜
            if candidate.file_path.suffix.lower() in ['.pth', '.safetensors']:
                score += 0.2
            elif candidate.file_path.suffix.lower() in ['.pt', '.bin']:
                score += 0.1
            
            candidate.confidence = min(score, 1.0)
            candidate.step_name = step_name
        
        # ìµœê³  ì ìˆ˜ ë°˜í™˜
        return max(candidates, key=lambda x: x.confidence)
    
    def load_cache(self):
        """ìºì‹œ ë¡œë“œ"""
        try:
            if self.cache_file.exists():
                with open(self.cache_file, 'r') as f:
                    self.cache = json.load(f)
        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.cache = {}
    
    def save_cache(self):
        """ìºì‹œ ì €ì¥"""
        try:
            with open(self.cache_file, 'w') as f:
                json.dump(self.cache, f, indent=2)
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def model_match_to_dict(self, match: ModelMatch) -> Dict:
        """ModelMatchë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "file_path": str(match.file_path),
            "confidence": match.confidence,
            "step_name": match.step_name,
            "file_size_mb": match.file_size_mb,
            "match_reason": match.match_reason
        }
    
    def dict_to_model_match(self, data: Dict) -> ModelMatch:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ ModelMatchë¡œ ë³€í™˜"""
        return ModelMatch(
            file_path=Path(data["file_path"]),
            confidence=data["confidence"],
            step_name=data["step_name"],
            file_size_mb=data["file_size_mb"],
            match_reason=data["match_reason"]
        )

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
auto_detector = ImprovedAutoModelDetector()

def find_model_for_step(step_name: str) -> Optional[str]:
    """Stepìš© ëª¨ë¸ ì°¾ê¸° (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    match = auto_detector.find_best_model_for_step(step_name)
    return str(match.file_path) if match else None

def get_all_available_models() -> Dict[str, List[str]]:
    """ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë°˜í™˜"""
    results = {}
    
    for step_name in auto_detector.step_patterns.keys():
        match = auto_detector.find_best_model_for_step(step_name)
        if match:
            results[step_name] = [str(match.file_path)]
        else:
            results[step_name] = []
    
    return results
'''
        
        return improved_code
    
    def validate_cleanup(self) -> Dict[str, Any]:
        """ì •ë¦¬ ê²°ê³¼ ê²€ì¦"""
        try:
            validation = {
                "success": True,
                "remaining_files": 0,
                "total_size_gb": 0.0,
                "structure_valid": True,
                "detector_working": True,
                "issues": []
            }
            
            # í˜„ì¬ ìƒíƒœ ì¬ë¶„ì„
            current_state = self.analyze_current_state()
            validation["remaining_files"] = current_state.get("total_files", 0)
            validation["total_size_gb"] = current_state.get("total_size_gb", 0.0)
            
            # êµ¬ì¡° ê²€ì¦
            for step_dir in self.standard_structure.keys():
                expected_path = self.base_path / "organized" / step_dir
                if not expected_path.exists():
                    validation["issues"].append(f"ë””ë ‰í† ë¦¬ ëˆ„ë½: {step_dir}")
                    validation["structure_valid"] = False
            
            # íƒì§€ê¸° ê²€ì¦
            try:
                # ê°„ë‹¨í•œ íƒì§€ í…ŒìŠ¤íŠ¸
                from backend.app.ai_pipeline.utils.auto_model_detector import auto_detector
                test_result = auto_detector.find_best_model_for_step("step_01_human_parsing")
                if not test_result:
                    validation["issues"].append("auto_detector ì‘ë™ ë¶ˆì•ˆì •")
                    validation["detector_working"] = False
            except Exception as e:
                validation["issues"].append(f"auto_detector ë¡œë“œ ì‹¤íŒ¨: {e}")
                validation["detector_working"] = False
            
            return validation
            
        except Exception as e:
            logger.error(f"ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def generate_final_report(self, *results) -> Dict[str, Any]:
        """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            duplicate_results, structure_results, detector_results, validation_results = results
            
            report = {
                "cleanup_summary": {
                    "total_savings_gb": round(duplicate_results.get("saved_space_gb", 0), 2),
                    "files_removed": len(duplicate_results.get("removed_files", [])),
                    "files_moved": len(structure_results.get("moved_files", [])),
                    "directories_created": len(structure_results.get("created_directories", [])),
                    "detector_improved": len(detector_results.get("improvements", [])) > 0
                },
                
                "before_after": {
                    "before": {
                        "total_files": self.analysis_result.get("total_files", 0),
                        "total_size_gb": round(self.analysis_result.get("total_size_gb", 0), 2),
                        "duplicates": len(self.analysis_result.get("duplicates", []))
                    },
                    "after": {
                        "total_files": validation_results.get("remaining_files", 0),
                        "total_size_gb": validation_results.get("total_size_gb", 0),
                        "structure_optimized": validation_results.get("structure_valid", False)
                    }
                },
                
                "recommendations": [
                    "ğŸ”§ conda í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰",
                    "ğŸ“Š ì •ê¸°ì ì¸ ëª¨ë¸ ì •ë¦¬ (ì›” 1íšŒ)",
                    "ğŸ’¾ ëŒ€ìš©ëŸ‰ ëª¨ë¸ì˜ ì™¸ë¶€ ì €ì¥ì†Œ í™œìš©",
                    "ğŸ” auto_model_detector ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§",
                    "ğŸ“ í‘œì¤€ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìœ ì§€"
                ],
                
                "success": validation_results.get("success", False),
                "issues": validation_results.get("issues", [])
            }
            
            return report
            
        except Exception as e:
            logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def classify_by_step(self, file_path: Path) -> str:
        """íŒŒì¼ì„ AI ë‹¨ê³„ë³„ë¡œ ë¶„ë¥˜"""
        try:
            path_str = str(file_path).lower()
            file_name = file_path.name.lower()
            
            # ë””ë ‰í† ë¦¬ëª…ìœ¼ë¡œ ë¶„ë¥˜
            for step, keywords in self.standard_structure.items():
                if step in path_str:
                    return step
                
                # í‚¤ì›Œë“œë¡œ ë¶„ë¥˜
                for keyword in keywords:
                    if keyword in file_name or keyword in path_str:
                        return step
            
            # íŠ¹ë³„ íŒ¨í„´ë“¤
            if any(pattern in file_name for pattern in ['schp', 'graphonomy', 'parsing']):
                return "step_01_human_parsing"
            elif any(pattern in file_name for pattern in ['openpose', 'dwpose', 'pose']):
                return "step_02_pose_estimation"
            elif any(pattern in file_name for pattern in ['viton', 'diffusion', 'ootd']):
                return "step_06_virtual_fitting"
            elif any(pattern in file_name for pattern in ['u2net', 'segment', 'cloth']):
                return "step_03_cloth_segmentation"
            
            return "unknown"
            
        except Exception as e:
            logger.warning(f"íŒŒì¼ ë¶„ë¥˜ ì‹¤íŒ¨ {file_path}: {e}")
            return "unknown"
    
    def get_file_hash(self, file_path: Path, chunk_size: int = 8192) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ë¹ ë¥¸ ë°©ë²•)"""
        try:
            hash_obj = hashlib.md5()
            
            with open(file_path, 'rb') as f:
                # í° íŒŒì¼ì˜ ê²½ìš° ì²« ë¶€ë¶„ë§Œ í•´ì‹œ
                if file_path.stat().st_size > 100 * 1024 * 1024:  # 100MB ì´ìƒ
                    chunk = f.read(chunk_size * 100)  # ì²˜ìŒ 800KBë§Œ
                else:
                    chunk = f.read()
                
                hash_obj.update(chunk)
            
            return hash_obj.hexdigest()
            
        except Exception as e:
            logger.warning(f"í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨ {file_path}: {e}")
            return "unknown"
    
    def create_safety_backup(self):
        """ì•ˆì „í•œ ë°±ì—… ìƒì„±"""
        try:
            logger.info(f"ğŸ’¾ ë°±ì—… ìƒì„± ì¤‘: {self.backup_path}")
            
            # ì¤‘ìš” íŒŒì¼ë“¤ë§Œ ë°±ì—… (100MB ì´ìƒ)
            important_files = []
            
            for file_info in self.analysis_result.get("file_details", []):
                if file_info["size_mb"] > 100:  # 100MB ì´ìƒ
                    important_files.append(file_info)
            
            # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
            self.backup_path.mkdir(parents=True, exist_ok=True)
            
            # ì¤‘ìš” íŒŒì¼ë“¤ ë°±ì—…
            for file_info in important_files[:20]:  # ìƒìœ„ 20ê°œë§Œ
                try:
                    src_path = self.base_path / file_info["path"]
                    dst_path = self.backup_path / file_info["path"]
                    
                    dst_path.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(src_path, dst_path)
                    
                    logger.info(f"âœ… ë°±ì—… ì™„ë£Œ: {file_info['path']}")
                    
                except Exception as e:
                    logger.warning(f"ë°±ì—… ì‹¤íŒ¨ {file_info['path']}: {e}")
            
            # ë°±ì—… ì •ë³´ ì €ì¥
            backup_info = {
                "timestamp": datetime.now().isoformat(),
                "original_path": str(self.base_path),
                "backup_path": str(self.backup_path),
                "files_backed_up": len(important_files),
                "analysis_result": self.analysis_result
            }
            
            with open(self.backup_path / "backup_info.json", 'w') as f:
                json.dump(backup_info, f, indent=2)
            
            logger.info(f"âœ… ë°±ì—… ì™„ë£Œ: {len(important_files)}ê°œ ì¤‘ìš” íŒŒì¼")
            
        except Exception as e:
            logger.error(f"ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='AI ëª¨ë¸ ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ')
    parser.add_argument('--dry-run', action='store_true', help='ì‹¤ì œ ë³€ê²½ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°ë§Œ')
    parser.add_argument('--path', default='backend/ai_models', help='AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # ì •ë¦¬ ì‹œìŠ¤í…œ ì‹¤í–‰
    cleanup_system = ModelCleanupSystem(args.path)
    
    try:
        result = cleanup_system.run_complete_cleanup(dry_run=args.dry_run)
        
        print("\n" + "="*80)
        print("ğŸ‰ AI ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ!")
        print("="*80)
        
        if result.get("success"):
            print(f"ğŸ’¾ ì ˆì•½ëœ ìš©ëŸ‰: {result['cleanup_summary']['total_savings_gb']:.2f}GB")
            print(f"ğŸ—‚ï¸ ì œê±°ëœ íŒŒì¼: {result['cleanup_summary']['files_removed']}ê°œ")
            print(f"ğŸ“ ì´ë™ëœ íŒŒì¼: {result['cleanup_summary']['files_moved']}ê°œ")
            print(f"ğŸ” auto_detector ê°œì„ : {'ì™„ë£Œ' if result['cleanup_summary']['detector_improved'] else 'ê±´ë„ˆëœ€'}")
            
            if result.get("issues"):
                print(f"\nâš ï¸ ì£¼ì˜ì‚¬í•­:")
                for issue in result["issues"]:
                    print(f"   â€¢ {issue}")
        else:
            print(f"âŒ ì •ë¦¬ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
        
        # ê¶Œì¥ì‚¬í•­ ì¶œë ¥
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for rec in result.get("recommendations", []):
            print(f"   â€¢ {rec}")
            
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()