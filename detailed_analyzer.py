#!/usr/bin/env python3
"""
ğŸ” AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„¸ ì¤‘ë³µ ë¶„ì„
ì‹¤ì œë¡œ ì–´ë–¤ íŒŒì¼ë“¤ì´ ì¤‘ë³µë˜ê³  ìˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ íŒŒì•…
"""

import os
import sys
import json
from pathlib import Path
from collections import defaultdict, Counter
import re

def analyze_duplicates():
    """ì¤‘ë³µ íŒ¨í„´ ìƒì„¸ ë¶„ì„"""
    base_path = Path("backend/ai_models")
    
    print("ğŸ” ì¤‘ë³µ íŒŒì¼ íŒ¨í„´ ë¶„ì„ ì¤‘...")
    
    # íŒŒì¼ ìˆ˜ì§‘
    all_files = []
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if any(file.endswith(ext) for ext in ['.pth', '.pt', '.safetensors', '.bin', '.onnx']):
                file_path = Path(root) / file
                try:
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    all_files.append({
                        'name': file,
                        'path': str(file_path.relative_to(base_path)),
                        'size_mb': round(size_mb, 2),
                        'dir': Path(root).name
                    })
                except:
                    pass
    
    print(f"ğŸ“Š ì´ {len(all_files)}ê°œ ëª¨ë¸ íŒŒì¼ ë°œê²¬")
    
    # ì¤‘ë³µ íŒ¨í„´ ë¶„ì„
    analyze_version_duplicates(all_files)
    analyze_size_duplicates(all_files)
    analyze_name_patterns(all_files)
    analyze_large_files(all_files)
    
def analyze_version_duplicates(files):
    """ë²„ì „ ë²ˆí˜¸ ì¤‘ë³µ ë¶„ì„ (_01, _02 ë“±)"""
    print("\nğŸ“‹ ë²„ì „ ë²ˆí˜¸ ì¤‘ë³µ ë¶„ì„:")
    
    # ê¸°ë³¸ ì´ë¦„ìœ¼ë¡œ ê·¸ë£¹í™”
    base_groups = defaultdict(list)
    
    for file in files:
        # ë²„ì „ ë²ˆí˜¸ ì œê±° íŒ¨í„´
        base_name = re.sub(r'_\d+(\.(pth|pt|safetensors|bin|onnx))?$', '', file['name'])
        base_name = re.sub(r'\.(pth|pt|safetensors|bin|onnx)$', '', base_name)
        base_groups[base_name].append(file)
    
    # ì¤‘ë³µì´ ìˆëŠ” ê·¸ë£¹ë§Œ ì¶œë ¥
    version_duplicates = []
    for base_name, group in base_groups.items():
        if len(group) > 1:
            version_duplicates.append((base_name, group))
    
    # í¬ê¸° ìˆœ ì •ë ¬
    version_duplicates.sort(key=lambda x: sum(f['size_mb'] for f in x[1]), reverse=True)
    
    print(f"   ë°œê²¬ëœ ë²„ì „ ì¤‘ë³µ ê·¸ë£¹: {len(version_duplicates)}ê°œ")
    
    total_waste = 0
    for base_name, group in version_duplicates[:15]:  # ìƒìœ„ 15ê°œë§Œ
        sizes = [f['size_mb'] for f in group]
        waste = sum(sizes) - max(sizes)  # ê°€ì¥ í° íŒŒì¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€
        total_waste += waste
        
        print(f"   ğŸ“ {base_name}:")
        for file in sorted(group, key=lambda x: x['size_mb'], reverse=True):
            print(f"      â””â”€ {file['name']} ({file['size_mb']:.1f}MB)")
        print(f"      ğŸ’¾ ì ˆì•½ ê°€ëŠ¥: {waste:.1f}MB")
    
    print(f"\nğŸ’° ì´ ì ˆì•½ ê°€ëŠ¥ ìš©ëŸ‰: {total_waste/1024:.2f}GB")

def analyze_size_duplicates(files):
    """ë™ì¼í•œ í¬ê¸° íŒŒì¼ ë¶„ì„"""
    print("\nğŸ“ ë™ì¼ í¬ê¸° íŒŒì¼ ë¶„ì„:")
    
    size_groups = defaultdict(list)
    for file in files:
        if file['size_mb'] > 10:  # 10MB ì´ìƒë§Œ
            size_groups[file['size_mb']].append(file)
    
    same_size = [(size, group) for size, group in size_groups.items() if len(group) > 1]
    same_size.sort(key=lambda x: x[0], reverse=True)
    
    print(f"   ë™ì¼ í¬ê¸° ê·¸ë£¹: {len(same_size)}ê°œ")
    
    for size_mb, group in same_size[:10]:
        print(f"   ğŸ“ {size_mb}MB ({len(group)}ê°œ íŒŒì¼):")
        for file in group:
            print(f"      â””â”€ {file['name']}")

def analyze_name_patterns(files):
    """íŒŒì¼ëª… íŒ¨í„´ ë¶„ì„"""
    print("\nğŸ”¤ íŒŒì¼ëª… íŒ¨í„´ ë¶„ì„:")
    
    # ê³µí†µ ì ‘ë‘ì‚¬/ì ‘ë¯¸ì‚¬ ì°¾ê¸°
    prefixes = Counter()
    suffixes = Counter()
    
    for file in files:
        name = file['name'].lower()
        
        # ì ‘ë‘ì‚¬ (ì²˜ìŒ ëª‡ ê¸€ì)
        if len(name) > 5:
            prefixes[name[:5]] += 1
            
        # ì ‘ë¯¸ì‚¬ (í™•ì¥ì ì œì™¸)
        base_name = name.rsplit('.', 1)[0]
        if len(base_name) > 5:
            suffixes[base_name[-5:]] += 1
    
    print("   ğŸ“ ê³µí†µ ì ‘ë‘ì‚¬ (ìƒìœ„ 10ê°œ):")
    for prefix, count in prefixes.most_common(10):
        if count > 3:
            print(f"      {prefix}*: {count}ê°œ")
    
    print("   ğŸ“ ê³µí†µ ì ‘ë¯¸ì‚¬ (ìƒìœ„ 10ê°œ):")
    for suffix, count in suffixes.most_common(10):
        if count > 3:
            print(f"      *{suffix}: {count}ê°œ")

def analyze_large_files(files):
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ ë¶„ì„"""
    print("\nğŸ“¦ ëŒ€ìš©ëŸ‰ íŒŒì¼ ìƒì„¸ ë¶„ì„:")
    
    large_files = [f for f in files if f['size_mb'] > 1000]  # 1GB ì´ìƒ
    large_files.sort(key=lambda x: x['size_mb'], reverse=True)
    
    print(f"   1GB ì´ìƒ íŒŒì¼: {len(large_files)}ê°œ")
    
    total_large = sum(f['size_mb'] for f in large_files)
    print(f"   ì´ ìš©ëŸ‰: {total_large/1024:.2f}GB")
    
    # ë””ë ‰í† ë¦¬ë³„ ë¶„í¬
    dir_sizes = defaultdict(float)
    for file in large_files:
        dir_sizes[file['dir']] += file['size_mb']
    
    print("   ğŸ“‚ ë””ë ‰í† ë¦¬ë³„ ëŒ€ìš©ëŸ‰ íŒŒì¼:")
    for dir_name, total_size in sorted(dir_sizes.items(), key=lambda x: x[1], reverse=True):
        print(f"      {dir_name}: {total_size/1024:.2f}GB")
    
    print("\n   ğŸ“‹ ê°œë³„ íŒŒì¼ ëª©ë¡:")
    for file in large_files[:15]:
        print(f"      {file['name']}: {file['size_mb']/1024:.2f}GB ({file['dir']})")

def generate_cleanup_suggestions():
    """ì •ë¦¬ ì œì•ˆì‚¬í•­"""
    print("\n" + "="*60)
    print("ğŸ’¡ ì •ë¦¬ ì œì•ˆì‚¬í•­")
    print("="*60)
    
    suggestions = [
        "ğŸ—‚ï¸  ë²„ì „ ë²ˆí˜¸ê°€ ìˆëŠ” ì¤‘ë³µ íŒŒì¼ë“¤ ì¤‘ ìµœì‹  ë²„ì „ë§Œ ìœ ì§€",
        "ğŸ”— ìì£¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì€ ì‹¬ë³¼ë¦­ ë§í¬ë¡œ í†µí•©",
        "ğŸ“¦ 1GB ì´ìƒ ëŒ€ìš©ëŸ‰ ëª¨ë¸ë“¤ì˜ ì™¸ë¶€ ì €ì¥ì†Œ ì´ë™",
        "ğŸ·ï¸  íŒŒì¼ëª… í‘œì¤€í™” (ì¼ê´€ëœ ëª…ëª… ê·œì¹™ ì ìš©)",
        "ğŸ“ ë‹¨ê³„ë³„ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì •ë¦¬",
        "ğŸ” ì‹¤ì œ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íŒŒì¼ ì‹ë³„ í›„ ì œê±°",
    ]
    
    for i, suggestion in enumerate(suggestions, 1):
        print(f"{i}. {suggestion}")
    
    print(f"\nì˜ˆìƒ ì ˆì•½ íš¨ê³¼:")
    print(f"   â€¢ ì¤‘ë³µ ì œê±°: ~50-80GB")
    print(f"   â€¢ ë¯¸ì‚¬ìš© íŒŒì¼ ì œê±°: ~20-30GB") 
    print(f"   â€¢ ì´ ì ˆì•½ ê°€ëŠ¥: ~70-110GB (í˜„ì¬ 185GBì˜ 40-60%)")

if __name__ == "__main__":
    analyze_duplicates()
    generate_cleanup_suggestions()