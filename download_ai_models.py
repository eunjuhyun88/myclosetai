#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê³ ì‚¬ì–‘ AI ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ v4.0
===============================================================================

âœ… ì‹¤ì œ ì ‘ê·¼ ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ë§Œ í¬í•¨ (404/401 ì˜¤ë¥˜ í•´ê²°)
âœ… Step 05 ClothWarpingìš© ì‹¤ì¡´ AI ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ
âœ… Hugging Face Hub + ì§ì ‘ ë‹¤ìš´ë¡œë“œ í•˜ì´ë¸Œë¦¬ë“œ
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
âœ… ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ë° ì²´í¬ì„¬ ê²€ì¦
âœ… ìë™ ì¬ì‹œë„ ë° ì—ëŸ¬ ë³µêµ¬
âœ… ì§„í–‰ë¥  í‘œì‹œ ë° ìƒì„¸ ë¡œê·¸

Author: MyCloset AI Team
Date: 2025-07-25
Version: 4.0 (Real Accessible Models)
"""

import os
import sys
import asyncio
import threading
import time
import hashlib
import json
import subprocess
import platform
import urllib.request
import urllib.error
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
import logging

# ==============================================
# ğŸ”§ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ë° ì„¤ì¹˜
# ==============================================

def install_required_packages():
    """í•„ìˆ˜ íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜"""
    required_packages = [
        'huggingface_hub',
        'requests', 
        'tqdm',
        'torch',
        'torchvision',
        'transformers',
        'accelerate'
    ]
    
    for package in required_packages:
        try:
            __import__(package.replace('-', '_'))
        except ImportError:
            print(f"ğŸ“¦ {package} ì„¤ì¹˜ ì¤‘...")
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤í–‰
install_required_packages()

# ì´ì œ ì•ˆì „í•˜ê²Œ import
try:
    from huggingface_hub import snapshot_download, hf_hub_download, login
    from transformers import AutoTokenizer, AutoModel, CLIPProcessor, CLIPModel
    import torch
    import requests
    from tqdm import tqdm
    import concurrent.futures
    HF_HUB_AVAILABLE = True
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")
    HF_HUB_AVAILABLE = False

# ==============================================
# ğŸ”§ ì‹œìŠ¤í…œ ê°ì§€ ë° ì„¤ì •
# ==============================================

def detect_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€"""
    system_info = {
        'platform': platform.system(),
        'machine': platform.machine(),
        'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
        'python_version': platform.python_version(),
        'is_m3_max': False,
        'available_memory_gb': 8,
        'torch_available': False,
        'mps_available': False,
        'cuda_available': False
    }
    
    # M3 Max ê°ì§€
    try:
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                capture_output=True, text=True, timeout=5
            )
            if 'M3' in result.stdout:
                system_info['is_m3_max'] = True
                system_info['available_memory_gb'] = 128  # M3 Max í†µí•© ë©”ëª¨ë¦¬
    except:
        pass
    
    # PyTorch ë° ê°€ì†ê¸° ê°ì§€
    try:
        import torch
        system_info['torch_available'] = True
        system_info['mps_available'] = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
        system_info['cuda_available'] = torch.cuda.is_available()
        
        if system_info['cuda_available']:
            system_info['available_memory_gb'] = max(system_info['available_memory_gb'], 
                                                    torch.cuda.get_device_properties(0).total_memory // (1024**3))
    except:
        pass
    
    return system_info

SYSTEM_INFO = detect_system_info()

# ==============================================
# ğŸ¤– ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê³ ì‚¬ì–‘ AI ëª¨ë¸ ëª©ë¡ (ê²€ì¦ëœ ê²ƒë§Œ)
# ==============================================

@dataclass
class AIModelInfo:
    """AI ëª¨ë¸ ì •ë³´"""
    name: str
    repo_id: str
    model_type: str
    size_gb: float
    description: str
    required_memory_gb: float
    priority: int = 5
    files: List[str] = field(default_factory=list)
    subfolder: str = ""
    revision: str = "main"
    use_auth_token: bool = False
    local_dir: str = ""
    download_url: str = ""  # ì§ì ‘ ë‹¤ìš´ë¡œë“œ URL

# ğŸ”¥ ì‹¤ì œ ì¡´ì¬í•˜ê³  ì ‘ê·¼ ê°€ëŠ¥í•œ ê³ ì‚¬ì–‘ AI ëª¨ë¸ ëª©ë¡
VERIFIED_AI_MODELS = {
    # ==============================================
    # ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ AI ëª¨ë¸ë“¤ (í™•ì¸ë¨)
    # ==============================================
    
    "clip_vit_large": AIModelInfo(
        name="CLIP ViT-Large-Patch14",
        repo_id="openai/clip-vit-large-patch14",
        model_type="image_processing",
        size_gb=1.7,
        description="CLIP ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ - ì§€ëŠ¥ì  ì´ë¯¸ì§€ ì²˜ë¦¬",
        required_memory_gb=4,
        priority=10,
        files=["pytorch_model.bin", "config.json", "preprocessor_config.json"],
        local_dir="models/image_processing/clip_large"
    ),
    
    "clip_vit_base": AIModelInfo(
        name="CLIP ViT-Base-Patch32",
        repo_id="openai/clip-vit-base-patch32",
        model_type="image_processing",
        size_gb=0.6,
        description="CLIP ë² ì´ìŠ¤ ëª¨ë¸ - ê²½ëŸ‰ ì´ë¯¸ì§€ ì²˜ë¦¬",
        required_memory_gb=2,
        priority=9,
        files=["pytorch_model.bin", "config.json", "preprocessor_config.json"],
        local_dir="models/image_processing/clip_base"
    ),
    
    "dinov2_large": AIModelInfo(
        name="DINOv2 Large",
        repo_id="facebook/dinov2-large",
        model_type="feature_extraction",
        size_gb=1.1,
        description="DINOv2 ëŒ€í˜• ëª¨ë¸ - ìê¸°ì§€ë„ í•™ìŠµ íŠ¹ì§• ì¶”ì¶œ",
        required_memory_gb=4,
        priority=8,
        files=["pytorch_model.bin", "config.json"],
        local_dir="models/feature_extraction/dinov2_large"
    ),
    
    # ==============================================
    # ğŸ­ ì„¸ê·¸ë©˜í…Œì´ì…˜ AI ëª¨ë¸ë“¤ (í™•ì¸ë¨)
    # ==============================================
    
    "sam_vit_huge": AIModelInfo(
        name="SAM ViT-Huge",
        repo_id="facebook/sam-vit-huge",
        model_type="segmentation",
        size_gb=2.56,
        description="SAM ê±°ëŒ€ ëª¨ë¸ - ëª¨ë“  ê°ì²´ ì„¸ê·¸ë©˜í…Œì´ì…˜",
        required_memory_gb=8,
        priority=10,
        files=["pytorch_model.bin", "config.json"],
        local_dir="models/segmentation/sam_huge"
    ),
    
    "sam_vit_base": AIModelInfo(
        name="SAM ViT-Base",
        repo_id="facebook/sam-vit-base",
        model_type="segmentation",
        size_gb=0.9,
        description="SAM ë² ì´ìŠ¤ ëª¨ë¸ - ê²½ëŸ‰ ì„¸ê·¸ë©˜í…Œì´ì…˜",
        required_memory_gb=4,
        priority=9,
        files=["pytorch_model.bin", "config.json"],
        local_dir="models/segmentation/sam_base"
    ),
    
    "detr_resnet50": AIModelInfo(
        name="DETR ResNet-50",
        repo_id="facebook/detr-resnet-50",
        model_type="object_detection",
        size_gb=0.17,
        description="DETR ê°ì²´ ê²€ì¶œ ëª¨ë¸",
        required_memory_gb=4,
        priority=8,
        files=["pytorch_model.bin", "config.json"],
        local_dir="models/object_detection/detr_resnet50"
    ),
    
    # ==============================================
    # ğŸƒ í¬ì¦ˆ ì¶”ì • AI ëª¨ë¸ë“¤ (ì§ì ‘ ë‹¤ìš´ë¡œë“œ)
    # ==============================================
    
    "ultralytics_yolov8n": AIModelInfo(
        name="YOLOv8n (Ultralytics)",
        repo_id="ultralytics/yolov8",
        model_type="object_detection",
        size_gb=0.006,
        description="YOLOv8 ë‚˜ë…¸ - ì´ˆê²½ëŸ‰ ê°ì²´ ê²€ì¶œ",
        required_memory_gb=1,
        priority=10,
        files=["yolov8n.pt"],
        local_dir="models/object_detection/yolov8n",
        download_url="https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt"
    ),
    
    "ultralytics_yolov8s": AIModelInfo(
        name="YOLOv8s (Ultralytics)", 
        repo_id="ultralytics/yolov8",
        model_type="object_detection",
        size_gb=0.022,
        description="YOLOv8 ìŠ¤ëª° - ê²½ëŸ‰ ê°ì²´ ê²€ì¶œ",
        required_memory_gb=2,
        priority=9,
        files=["yolov8s.pt"],
        local_dir="models/object_detection/yolov8s",
        download_url="https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt"
    ),
    
    "ultralytics_yolov8n_pose": AIModelInfo(
        name="YOLOv8n-Pose (Ultralytics)",
        repo_id="ultralytics/yolov8",
        model_type="pose_estimation", 
        size_gb=0.006,
        description="YOLOv8 ë‚˜ë…¸ í¬ì¦ˆ - ì´ˆê²½ëŸ‰ í¬ì¦ˆ ì¶”ì •",
        required_memory_gb=1,
        priority=10,
        files=["yolov8n-pose.pt"],
        local_dir="models/pose_estimation/yolov8n_pose",
        download_url="https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt"
    ),
    
    # ==============================================
    # ğŸ¨ ì´ë¯¸ì§€ í–¥ìƒ AI ëª¨ë¸ë“¤ (ì§ì ‘ ë‹¤ìš´ë¡œë“œ)
    # ==============================================
    
    "real_esrgan_x4": AIModelInfo(
        name="Real-ESRGAN x4",
        repo_id="xinntao/Real-ESRGAN",
        model_type="image_enhancement",
        size_gb=0.067,
        description="Real-ESRGAN 4x ì—…ìŠ¤ì¼€ì¼ë§",
        required_memory_gb=2,
        priority=9,
        files=["RealESRGAN_x4plus.pth"],
        local_dir="models/image_enhancement/real_esrgan",
        download_url="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth"
    ),
    
    "real_esrgan_anime": AIModelInfo(
        name="Real-ESRGAN Anime6B",
        repo_id="xinntao/Real-ESRGAN",
        model_type="image_enhancement",
        size_gb=0.017,
        description="Real-ESRGAN ì• ë‹ˆë©”ì´ì…˜ íŠ¹í™” ì—…ìŠ¤ì¼€ì¼ë§",
        required_memory_gb=2,
        priority=8,
        files=["RealESRGAN_x4plus_anime_6B.pth"],
        local_dir="models/image_enhancement/real_esrgan_anime",
        download_url="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth"
    ),
    
    # ==============================================
    # ğŸ§  ê¸°ì´ˆ AI ëª¨ë¸ë“¤ (í™•ì¸ë¨)
    # ==============================================
    
    "bert_base": AIModelInfo(
        name="BERT Base Uncased",
        repo_id="bert-base-uncased",
        model_type="language_model",
        size_gb=0.44,
        description="BERT ë² ì´ìŠ¤ í…ìŠ¤íŠ¸ ì´í•´ ëª¨ë¸",
        required_memory_gb=2,
        priority=7,
        files=["pytorch_model.bin", "config.json", "tokenizer.json"],
        local_dir="models/language/bert_base"
    ),
    
    "bert_large": AIModelInfo(
        name="BERT Large Uncased",
        repo_id="bert-large-uncased",
        model_type="language_model",
        size_gb=1.3,
        description="BERT ëŒ€í˜• í…ìŠ¤íŠ¸ ì´í•´ ëª¨ë¸",
        required_memory_gb=4,
        priority=6,
        files=["pytorch_model.bin", "config.json", "tokenizer.json"],
        local_dir="models/language/bert_large"
    ),
    
    "resnet50": AIModelInfo(
        name="ResNet-50",
        repo_id="microsoft/resnet-50",
        model_type="feature_extraction",
        size_gb=0.098,
        description="ResNet-50 ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ",
        required_memory_gb=2,
        priority=8,
        files=["pytorch_model.bin", "config.json"],
        local_dir="models/feature_extraction/resnet50"
    ),
    
    "resnet152": AIModelInfo(
        name="ResNet-152",
        repo_id="microsoft/resnet-152", 
        model_type="feature_extraction",
        size_gb=0.23,
        description="ResNet-152 ê¹Šì€ íŠ¹ì§• ì¶”ì¶œ",
        required_memory_gb=3,
        priority=7,
        files=["pytorch_model.bin", "config.json"],
        local_dir="models/feature_extraction/resnet152"
    ),
    
    # ==============================================
    # ğŸ¯ í™•ì¥ëœ AI ëª¨ë¸ë“¤
    # ==============================================
    
    "vit_base": AIModelInfo(
        name="Vision Transformer Base",
        repo_id="google/vit-base-patch16-224",
        model_type="feature_extraction",
        size_gb=0.33,
        description="Vision Transformer ë² ì´ìŠ¤ ëª¨ë¸",
        required_memory_gb=2,
        priority=8,
        files=["pytorch_model.bin", "config.json"],
        local_dir="models/feature_extraction/vit_base"
    ),
    
    "vit_large": AIModelInfo(
        name="Vision Transformer Large",
        repo_id="google/vit-large-patch16-224", 
        model_type="feature_extraction",
        size_gb=1.2,
        description="Vision Transformer ëŒ€í˜• ëª¨ë¸",
        required_memory_gb=4,
        priority=7,
        files=["pytorch_model.bin", "config.json"],
        local_dir="models/feature_extraction/vit_large"
    ),
    
    "dpt_large": AIModelInfo(
        name="DPT Large (Depth Estimation)",
        repo_id="Intel/dpt-large",
        model_type="depth_estimation",
        size_gb=1.3,
        description="DPT ëŒ€í˜• ê¹Šì´ ì¶”ì • ëª¨ë¸",
        required_memory_gb=4,
        priority=7,
        files=["pytorch_model.bin", "config.json"],
        local_dir="models/depth_estimation/dpt_large"
    ),
    
    # ==============================================
    # ğŸ”¥ ê³ ì„±ëŠ¥ Diffusion ëª¨ë¸ë“¤ (ë©”ëª¨ë¦¬ ì¶©ë¶„ì‹œ)
    # ==============================================
    
    "stable_diffusion_2_1": AIModelInfo(
        name="Stable Diffusion 2.1",
        repo_id="stabilityai/stable-diffusion-2-1",
        model_type="image_generation",
        size_gb=5.2,
        description="Stable Diffusion 2.1 ì´ë¯¸ì§€ ìƒì„±",
        required_memory_gb=12,
        priority=6,
        files=["unet/diffusion_pytorch_model.safetensors", "vae/diffusion_pytorch_model.safetensors"],
        local_dir="models/image_generation/sd21"
    ),
    
    "stable_diffusion_xl": AIModelInfo(
        name="Stable Diffusion XL Base",
        repo_id="stabilityai/stable-diffusion-xl-base-1.0",
        model_type="image_generation",
        size_gb=13.5,
        description="Stable Diffusion XL ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±",
        required_memory_gb=24,
        priority=5,
        files=["unet/diffusion_pytorch_model.safetensors", "vae/diffusion_pytorch_model.safetensors"],
        local_dir="models/image_generation/sdxl"
    ),
    
    # ==============================================
    # ğŸ“± ëª¨ë°”ì¼/ê²½ëŸ‰ ëª¨ë¸ë“¤
    # ==============================================
    
    "mobilenet_v3": AIModelInfo(
        name="MobileNet v3 Large",
        repo_id="google/mobilenet_v3_large_100_224",
        model_type="feature_extraction", 
        size_gb=0.021,
        description="MobileNet v3 ê²½ëŸ‰ íŠ¹ì§• ì¶”ì¶œ",
        required_memory_gb=1,
        priority=9,
        files=["pytorch_model.bin", "config.json"],
        local_dir="models/feature_extraction/mobilenet_v3"
    ),
    
    "efficientnet_b0": AIModelInfo(
        name="EfficientNet B0",
        repo_id="google/efficientnet-b0",
        model_type="feature_extraction",
        size_gb=0.02,
        description="EfficientNet B0 íš¨ìœ¨ì  íŠ¹ì§• ì¶”ì¶œ",
        required_memory_gb=1,
        priority=9,
        files=["pytorch_model.bin", "config.json"],
        local_dir="models/feature_extraction/efficientnet_b0"
    )
}

# ==============================================
# ğŸ”§ ê°œì„ ëœ ë‹¤ìš´ë¡œë“œ ê´€ë¦¬ í´ë˜ìŠ¤
# ==============================================

class VerifiedAIModelDownloader:
    """ê²€ì¦ëœ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê´€ë¦¬ì"""
    
    def __init__(self, base_dir: str = "ai_models", max_workers: int = 4):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        self.max_workers = max_workers
        self.downloaded_models = {}
        self.failed_downloads = {}
        
        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(__name__)
        self._setup_logging()
        
        # ì‹œìŠ¤í…œ ì •ë³´ ë¡œê·¸
        self.logger.info(f"ğŸ ì‹œìŠ¤í…œ ì •ë³´: M3 Max={SYSTEM_INFO['is_m3_max']}, "
                        f"ë©”ëª¨ë¦¬={SYSTEM_INFO['available_memory_gb']}GB, "
                        f"MPS={SYSTEM_INFO['mps_available']}")
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def filter_models_by_memory(self, models: Dict[str, AIModelInfo]) -> Dict[str, AIModelInfo]:
        """ë©”ëª¨ë¦¬ ìš©ëŸ‰ì— ë”°ë¥¸ ëª¨ë¸ í•„í„°ë§"""
        available_memory = SYSTEM_INFO['available_memory_gb']
        filtered_models = {}
        
        for model_id, model_info in models.items():
            if model_info.required_memory_gb <= available_memory:
                filtered_models[model_id] = model_info
            else:
                self.logger.warning(f"âš ï¸ {model_info.name} ê±´ë„ˆëœ€ - í•„ìš” ë©”ëª¨ë¦¬: {model_info.required_memory_gb}GB > ì‚¬ìš©ê°€ëŠ¥: {available_memory}GB")
        
        return filtered_models
    
    def get_recommended_models(self, priority_threshold: int = 8) -> Dict[str, AIModelInfo]:
        """ì¶”ì²œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        memory_filtered = self.filter_models_by_memory(VERIFIED_AI_MODELS)
        
        recommended = {
            model_id: model_info 
            for model_id, model_info in memory_filtered.items() 
            if model_info.priority >= priority_threshold
        }
        
        return dict(sorted(recommended.items(), key=lambda x: x[1].priority, reverse=True))
    
    def calculate_total_size(self, models: Dict[str, AIModelInfo]) -> float:
        """ì´ ë‹¤ìš´ë¡œë“œ í¬ê¸° ê³„ì‚°"""
        return sum(model.size_gb for model in models.values())
    
    async def download_model_async(self, model_id: str, model_info: AIModelInfo) -> bool:
        """ë‹¨ì¼ ëª¨ë¸ ë¹„ë™ê¸° ë‹¤ìš´ë¡œë“œ (ê°œì„ ëœ ë²„ì „)"""
        try:
            self.logger.info(f"ğŸ“¥ {model_info.name} ë‹¤ìš´ë¡œë“œ ì‹œì‘ ({model_info.size_gb:.3f}GB)")
            
            local_path = self.base_dir / model_info.local_dir
            local_path.mkdir(parents=True, exist_ok=True)
            
            # ë‹¤ìš´ë¡œë“œ ë°©ë²• ì„ íƒ (ìš°ì„ ìˆœìœ„)
            success = False
            
            # 1. ì§ì ‘ ë‹¤ìš´ë¡œë“œ URLì´ ìˆëŠ” ê²½ìš°
            if model_info.download_url:
                success = await self._direct_url_download(model_info, local_path)
            
            # 2. Hugging Face Hub ì‹œë„ (ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ì‹œ)
            if not success and HF_HUB_AVAILABLE:
                success = await self._hf_download_safe(model_info, local_path)
            
            # 3. ë°±ì—… ë‹¤ìš´ë¡œë“œ ì‹œë„
            if not success:
                success = await self._backup_download(model_info, local_path)
            
            if success:
                self.downloaded_models[model_id] = {
                    'name': model_info.name,
                    'path': str(local_path),
                    'size_gb': model_info.size_gb,
                    'download_time': time.time(),
                    'model_type': model_info.model_type
                }
                self.logger.info(f"âœ… {model_info.name} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                self.failed_downloads[model_id] = model_info.name
                self.logger.error(f"âŒ {model_info.name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ {model_info.name} ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            self.failed_downloads[model_id] = f"{model_info.name} - {str(e)}"
            return False
    
    async def _direct_url_download(self, model_info: AIModelInfo, local_path: Path) -> bool:
        """ì§ì ‘ URLì—ì„œ ë‹¤ìš´ë¡œë“œ"""
        try:
            if not model_info.download_url:
                return False
            
            file_name = model_info.download_url.split('/')[-1]
            file_path = local_path / file_name
            
            self.logger.info(f"  ğŸŒ ì§ì ‘ ë‹¤ìš´ë¡œë“œ: {model_info.download_url}")
            
            # requestsë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œ
            response = requests.get(model_info.download_url, stream=True, timeout=30)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(file_path, 'wb') as f, tqdm(
                desc=f"ğŸ“¥ {file_name}",
                total=total_size,
                unit='B',
                unit_scale=True,
                unit_divisor=1024
            ) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))
            
            self.logger.debug(f"  âœ… ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_name}")
            return True
            
        except Exception as e:
            self.logger.debug(f"  âŒ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def _hf_download_safe(self, model_info: AIModelInfo, local_path: Path) -> bool:
        """ì•ˆì „í•œ Hugging Face Hub ë‹¤ìš´ë¡œë“œ"""
        try:
            if model_info.files:
                # íŠ¹ì • íŒŒì¼ë“¤ë§Œ ë‹¤ìš´ë¡œë“œ
                for file_name in model_info.files:
                    try:
                        file_path = hf_hub_download(
                            repo_id=model_info.repo_id,
                            filename=file_name,
                            subfolder=model_info.subfolder,
                            revision=model_info.revision,
                            use_auth_token=model_info.use_auth_token,
                            local_dir=str(local_path),
                            local_dir_use_symlinks=False
                        )
                        self.logger.debug(f"  ğŸ“ HF {file_name} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                    except Exception as file_e:
                        self.logger.debug(f"  âš ï¸ HF {file_name} ê±´ë„ˆëœ€: {file_e}")
                        continue
            else:
                # ì „ì²´ repo ë‹¤ìš´ë¡œë“œ
                snapshot_download(
                    repo_id=model_info.repo_id,
                    revision=model_info.revision,
                    use_auth_token=model_info.use_auth_token,
                    local_dir=str(local_path),
                    local_dir_use_symlinks=False
                )
            
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if any(local_path.iterdir()):
                return True
            else:
                return False
            
        except Exception as e:
            self.logger.debug(f"  âŒ HF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def _backup_download(self, model_info: AIModelInfo, local_path: Path) -> bool:
        """ë°±ì—… ë‹¤ìš´ë¡œë“œ ë°©ë²•ë“¤ ì‹œë„"""
        try:
            # GitHub releases íŒ¨í„´ë“¤ ì‹œë„
            backup_urls = []
            
            # Ultralytics íŒ¨í„´
            if "ultralytics" in model_info.repo_id.lower():
                for file_name in model_info.files:
                    backup_urls.append(f"https://github.com/ultralytics/assets/releases/download/v0.0.0/{file_name}")
            
            # Real-ESRGAN íŒ¨í„´  
            if "real-esrgan" in model_info.name.lower():
                for file_name in model_info.files:
                    backup_urls.extend([
                        f"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/{file_name}",
                        f"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/{file_name}"
                    ])
            
            # ë°±ì—… URLë“¤ ì‹œë„
            for url in backup_urls:
                try:
                    file_name = url.split('/')[-1]
                    file_path = local_path / file_name
                    
                    response = requests.get(url, stream=True, timeout=30)
                    if response.status_code == 200:
                        total_size = int(response.headers.get('content-length', 0))
                        
                        with open(file_path, 'wb') as f, tqdm(
                            desc=f"ğŸ“¥ ë°±ì—… {file_name}",
                            total=total_size,
                            unit='B',
                            unit_scale=True
                        ) as pbar:
                            for chunk in response.iter_content(chunk_size=8192):
                                f.write(chunk)
                                pbar.update(len(chunk))
                        
                        self.logger.debug(f"  âœ… ë°±ì—… ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {file_name}")
                        return True
                        
                except Exception as e:
                    self.logger.debug(f"  ë°±ì—… URL {url} ì‹¤íŒ¨: {e}")
                    continue
            
            return False
            
        except Exception as e:
            self.logger.debug(f"  âŒ ë°±ì—… ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def download_models_parallel(self, models: Dict[str, AIModelInfo]) -> Dict[str, bool]:
        """ë³‘ë ¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ê°œì„ ëœ ë²„ì „)"""
        results = {}
        
        # ìš°ì„ ìˆœìœ„ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_models = sorted(models.items(), key=lambda x: x[1].priority, reverse=True)
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # íƒœìŠ¤í¬ ìƒì„±
            future_to_model = {
                executor.submit(asyncio.run, self.download_model_async(model_id, model_info)): model_id
                for model_id, model_info in sorted_models
            }
            
            # ì™„ë£Œëœ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_model):
                model_id = future_to_model[future]
                try:
                    success = future.result()
                    results[model_id] = success
                except Exception as e:
                    self.logger.error(f"âŒ {model_id} ë‹¤ìš´ë¡œë“œ íƒœìŠ¤í¬ ì‹¤íŒ¨: {e}")
                    results[model_id] = False
        
        return results
    
    def create_model_config(self) -> Dict[str, Any]:
        """ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
        config = {
            'system_info': SYSTEM_INFO,
            'download_info': {
                'downloaded_at': time.time(),
                'total_models': len(self.downloaded_models),
                'total_size_gb': sum(model['size_gb'] for model in self.downloaded_models.values()),
                'base_directory': str(self.base_dir),
                'verified_models': True
            },
            'models': self.downloaded_models,
            'failed_models': self.failed_downloads,
            'model_categories': {}
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
        for model_id, model_data in self.downloaded_models.items():
            model_type = model_data['model_type']
            if model_type not in config['model_categories']:
                config['model_categories'][model_type] = []
            config['model_categories'][model_type].append(model_id)
        
        config_path = self.base_dir / 'verified_model_config.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ“‹ ê²€ì¦ëœ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
        return config
    
    def print_download_summary(self):
        """ë‹¤ìš´ë¡œë“œ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ‰ ê²€ì¦ëœ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ìš”ì•½")
        print("="*80)
        
        if self.downloaded_models:
            print(f"âœ… ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸: {len(self.downloaded_models)}ê°œ")
            total_size = sum(model['size_gb'] for model in self.downloaded_models.values())
            print(f"ğŸ“Š ì´ ë‹¤ìš´ë¡œë“œ í¬ê¸°: {total_size:.2f}GB")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì¶œë ¥
            categories = {}
            for model_data in self.downloaded_models.values():
                model_type = model_data['model_type']
                if model_type not in categories:
                    categories[model_type] = []
                categories[model_type].append(model_data)
            
            for category, models in categories.items():
                print(f"\nğŸ“‚ {category.upper().replace('_', ' ')}:")
                for model in models:
                    print(f"  âœ“ {model['name']} ({model['size_gb']:.3f}GB)")
                    print(f"    ê²½ë¡œ: {model['path']}")
        
        if self.failed_downloads:
            print(f"\nâŒ ì‹¤íŒ¨í•œ ëª¨ë¸: {len(self.failed_downloads)}ê°œ")
            for model_id, model_name in self.failed_downloads.items():
                print(f"  âœ— {model_name}")
        
        print(f"\nğŸ ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"  - M3 Max: {SYSTEM_INFO['is_m3_max']}")
        print(f"  - ì‚¬ìš©ê°€ëŠ¥ ë©”ëª¨ë¦¬: {SYSTEM_INFO['available_memory_gb']}GB")
        print(f"  - MPS ê°€ì†: {SYSTEM_INFO['mps_available']}")
        print(f"  - CUDA ê°€ì†: {SYSTEM_INFO['cuda_available']}")
        print(f"  - Conda í™˜ê²½: {SYSTEM_INFO['conda_env']}")
        
        print("\nğŸš€ ëª¨ë¸ ì‚¬ìš©ë²•:")
        print("  from app.ai_pipeline.steps.step_05_cloth_warping import ClothWarpingStep")
        print("  step = ClothWarpingStep(ai_model_enabled=True)")
        print("  result = await step.process(cloth_image, person_image)")
        print("="*80)

# ==============================================
# ğŸš€ ë©”ì¸ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ë“¤ (ê°œì„ ëœ ë²„ì „)
# ==============================================

async def download_essential_verified_models():
    """í•„ìˆ˜ ê²€ì¦ ëª¨ë¸ë“¤ë§Œ ë‹¤ìš´ë¡œë“œ (ë¹ ë¥¸ ì„¤ì¹˜)"""
    print("ğŸš€ í•„ìˆ˜ ê²€ì¦ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    
    essential_models = {
        "clip_vit_base": VERIFIED_AI_MODELS["clip_vit_base"],
        "sam_vit_base": VERIFIED_AI_MODELS["sam_vit_base"],
        "ultralytics_yolov8n": VERIFIED_AI_MODELS["ultralytics_yolov8n"],
        "ultralytics_yolov8n_pose": VERIFIED_AI_MODELS["ultralytics_yolov8n_pose"],
        "real_esrgan_x4": VERIFIED_AI_MODELS["real_esrgan_x4"],
        "bert_base": VERIFIED_AI_MODELS["bert_base"],
        "resnet50": VERIFIED_AI_MODELS["resnet50"],
        "mobilenet_v3": VERIFIED_AI_MODELS["mobilenet_v3"]
    }
    
    downloader = VerifiedAIModelDownloader(max_workers=3)
    filtered_models = downloader.filter_models_by_memory(essential_models)
    
    total_size = downloader.calculate_total_size(filtered_models)
    print(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ì˜ˆìƒ í¬ê¸°: {total_size:.2f}GB")
    print(f"ğŸ”¢ ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ìˆ˜: {len(filtered_models)}ê°œ")
    
    if input("\nê²€ì¦ëœ í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() != 'y':
        print("ë‹¤ìš´ë¡œë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    results = await downloader.download_models_parallel(filtered_models)
    downloader.create_model_config()
    downloader.print_download_summary()
    
    return results

async def download_recommended_verified_models():
    """ì¶”ì²œ ê²€ì¦ ëª¨ë¸ë“¤ ë‹¤ìš´ë¡œë“œ (ê· í˜•ìˆëŠ” ì„¤ì¹˜)"""
    print("ğŸŒŸ ì¶”ì²œ ê²€ì¦ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    
    downloader = VerifiedAIModelDownloader(max_workers=4)
    recommended_models = downloader.get_recommended_models(priority_threshold=7)
    
    total_size = downloader.calculate_total_size(recommended_models)
    print(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ì˜ˆìƒ í¬ê¸°: {total_size:.2f}GB")
    print(f"ğŸ”¢ ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ìˆ˜: {len(recommended_models)}ê°œ")
    
    print(f"\nğŸ“‹ ì¶”ì²œ ê²€ì¦ ëª¨ë¸ ëª©ë¡:")
    for model_id, model_info in recommended_models.items():
        print(f"  â€¢ {model_info.name} ({model_info.size_gb:.3f}GB) - {model_info.description}")
    
    if input("\nê²€ì¦ëœ ì¶”ì²œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() != 'y':
        print("ë‹¤ìš´ë¡œë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    results = await downloader.download_models_parallel(recommended_models)
    downloader.create_model_config()
    downloader.print_download_summary()
    
    return results

async def download_all_verified_models():
    """ëª¨ë“  ê²€ì¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì™„ì „ ì„¤ì¹˜)"""
    print("ğŸ”¥ ëª¨ë“  ê²€ì¦ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    
    downloader = VerifiedAIModelDownloader(max_workers=6)
    all_models = downloader.filter_models_by_memory(VERIFIED_AI_MODELS)
    
    total_size = downloader.calculate_total_size(all_models)
    print(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ì˜ˆìƒ í¬ê¸°: {total_size:.2f}GB")
    print(f"ğŸ”¢ ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ìˆ˜: {len(all_models)}ê°œ")
    
    if total_size > 30:
        print("âš ï¸ ê²½ê³ : 30GB ì´ìƒì˜ ëŒ€ìš©ëŸ‰ ë‹¤ìš´ë¡œë“œì…ë‹ˆë‹¤!")
    
    print(f"\nğŸ“‹ ì „ì²´ ê²€ì¦ ëª¨ë¸ ëª©ë¡:")
    categories = {}
    for model_info in all_models.values():
        if model_info.model_type not in categories:
            categories[model_info.model_type] = []
        categories[model_info.model_type].append(model_info)
    
    for category, models in categories.items():
        print(f"\n  ğŸ“‚ {category.upper().replace('_', ' ')}:")
        for model in models:
            print(f"    â€¢ {model.name} ({model.size_gb:.3f}GB)")
    
    if input(f"\n{total_size:.2f}GB ê²€ì¦ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() != 'y':
        print("ë‹¤ìš´ë¡œë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    results = await downloader.download_models_parallel(all_models)
    downloader.create_model_config()
    downloader.print_download_summary()
    
    return results

async def download_custom_verified_models(model_ids: List[str]):
    """ì‚¬ìš©ì ì§€ì • ê²€ì¦ ëª¨ë¸ë“¤ ë‹¤ìš´ë¡œë“œ"""
    print(f"ğŸ¯ ì‚¬ìš©ì ì§€ì • ê²€ì¦ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {len(model_ids)}ê°œ")
    
    custom_models = {}
    for model_id in model_ids:
        if model_id in VERIFIED_AI_MODELS:
            custom_models[model_id] = VERIFIED_AI_MODELS[model_id]
        else:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ ID: {model_id}")
    
    if not custom_models:
        print("âŒ ìœ íš¨í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    downloader = VerifiedAIModelDownloader(max_workers=4)
    filtered_models = downloader.filter_models_by_memory(custom_models)
    
    total_size = downloader.calculate_total_size(filtered_models)
    print(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ì˜ˆìƒ í¬ê¸°: {total_size:.2f}GB")
    
    results = await downloader.download_models_parallel(filtered_models)
    downloader.create_model_config()
    downloader.print_download_summary()
    
    return results

def list_verified_models():
    """ê²€ì¦ëœ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
    print("\nğŸ¤– ê²€ì¦ëœ ê³ ì‚¬ì–‘ AI ëª¨ë¸ ëª©ë¡")
    print("="*80)
    
    # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì •ë¦¬
    categories = {}
    for model_id, model_info in VERIFIED_AI_MODELS.items():
        if model_info.model_type not in categories:
            categories[model_info.model_type] = []
        categories[model_info.model_type].append((model_id, model_info))
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì¶œë ¥
    for category, models in categories.items():
        print(f"\nğŸ“‚ {category.upper().replace('_', ' ')}:")
        
        for model_id, model_info in sorted(models, key=lambda x: x[1].priority, reverse=True):
            memory_ok = "âœ…" if model_info.required_memory_gb <= SYSTEM_INFO['available_memory_gb'] else "âŒ"
            download_method = "ğŸŒ ì§ì ‘" if model_info.download_url else "ğŸ¤— HF Hub"
            
            print(f"  {memory_ok} {model_id} ({download_method})")
            print(f"      ì´ë¦„: {model_info.name}")
            print(f"      í¬ê¸°: {model_info.size_gb:.3f}GB")
            print(f"      í•„ìš” ë©”ëª¨ë¦¬: {model_info.required_memory_gb}GB")
            print(f"      ì„¤ëª…: {model_info.description}")
            print(f"      ìš°ì„ ìˆœìœ„: {model_info.priority}/10")
            if model_info.download_url:
                print(f"      ì§ì ‘ URL: {model_info.download_url[:60]}...")
            print()
    
    total_size = sum(model.size_gb for model in VERIFIED_AI_MODELS.values())
    compatible_models = sum(1 for model in VERIFIED_AI_MODELS.values() 
                           if model.required_memory_gb <= SYSTEM_INFO['available_memory_gb'])
    
    print(f"ğŸ“Š í†µê³„:")
    print(f"  - ì „ì²´ ê²€ì¦ ëª¨ë¸ ìˆ˜: {len(VERIFIED_AI_MODELS)}ê°œ")
    print(f"  - í˜¸í™˜ ê°€ëŠ¥ ëª¨ë¸: {compatible_models}ê°œ")
    print(f"  - ì „ì²´ í¬ê¸°: {total_size:.2f}GB")
    print(f"  - ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {SYSTEM_INFO['available_memory_gb']}GB")
    print(f"  - ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì§€ì›: {sum(1 for m in VERIFIED_AI_MODELS.values() if m.download_url)}ê°œ")

# ==============================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)
# ==============================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”¥ MyCloset AI - ê²€ì¦ëœ ê³ ì‚¬ì–‘ AI ëª¨ë¸ ë‹¤ìš´ë¡œë” v4.0")
    print("="*65)
    print("âœ… ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ë“¤ë§Œ í¬í•¨ (404/401 ì˜¤ë¥˜ í•´ê²°)")
    print("ğŸŒ ì§ì ‘ ë‹¤ìš´ë¡œë“œ + Hugging Face Hub í•˜ì´ë¸Œë¦¬ë“œ")
    print("="*65)
    print(f"ğŸ M3 Max: {SYSTEM_INFO['is_m3_max']}")
    print(f"ğŸ’¾ ì‚¬ìš©ê°€ëŠ¥ ë©”ëª¨ë¦¬: {SYSTEM_INFO['available_memory_gb']}GB")
    print(f"âš¡ MPS ê°€ì†: {SYSTEM_INFO['mps_available']}")
    print(f"ğŸ Conda í™˜ê²½: {SYSTEM_INFO['conda_env']}")
    print("="*65)
    
    while True:
        print("\nğŸ¯ ê²€ì¦ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ğŸš€ í•„ìˆ˜ ê²€ì¦ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ (ë¹ ë¥¸ ì„¤ì¹˜, ~2GB)")
        print("2. ğŸŒŸ ì¶”ì²œ ê²€ì¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ê· í˜•ìˆëŠ” ì„¤ì¹˜, ~8GB)")  
        print("3. ğŸ”¥ ëª¨ë“  ê²€ì¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì™„ì „ ì„¤ì¹˜, ~25GB)")
        print("4. ğŸ“‹ ê²€ì¦ëœ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë³´ê¸°")
        print("5. ğŸ¯ ì‚¬ìš©ì ì§€ì • ê²€ì¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
        print("6. âŒ ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ (1-6): ").strip()
        
        try:
            if choice == '1':
                asyncio.run(download_essential_verified_models())
                break
            elif choice == '2':
                asyncio.run(download_recommended_verified_models())
                break
            elif choice == '3':
                asyncio.run(download_all_verified_models())
                break
            elif choice == '4':
                list_verified_models()
            elif choice == '5':
                print("\nğŸ“‹ ê²€ì¦ëœ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ID ëª©ë¡:")
                for i, model_id in enumerate(VERIFIED_AI_MODELS.keys(), 1):
                    model_info = VERIFIED_AI_MODELS[model_id]
                    status = "âœ…" if model_info.required_memory_gb <= SYSTEM_INFO['available_memory_gb'] else "âŒ"
                    print(f"  {i:2d}. {status} {model_id} ({model_info.size_gb:.3f}GB)")
                
                model_input = input("\në‹¤ìš´ë¡œë“œí•  ëª¨ë¸ IDë“¤ì„ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„): ").strip()
                if model_input:
                    model_ids = [mid.strip() for mid in model_input.split(',')]
                    asyncio.run(download_custom_verified_models(model_ids))
                    break
            elif choice == '6':
                print("ğŸ‘‹ ê²€ì¦ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            else:
                print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1-6 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
                
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()