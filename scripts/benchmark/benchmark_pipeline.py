#!/usr/bin/env python3
"""
MyCloset AI - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬
M3 Max ìµœì í™” ì„±ëŠ¥ ì¸¡ì • ë° ë¶„ì„

ì„±ëŠ¥ ì¸¡ì • í•­ëª©:
- ê° ë‹¨ê³„ë³„ ì²˜ë¦¬ ì‹œê°„
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (RAM/GPU)
- CPU/GPU ì‚¬ìš©ë¥ 
- í’ˆì§ˆ ì ìˆ˜
- ì „ì²´ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ëŸ‰
"""

import os
import sys
import time
import json
import psutil
import platform
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict

import torch
import numpy as np
from PIL import Image
import cv2

# ìƒ‰ìƒ ì¶œë ¥
class Colors:
    BLUE = '\033[94m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    PURPLE = '\033[95m'
    CYAN = '\033[96m'
    BOLD = '\033[1m'
    END = '\033[0m'

def log_info(msg): print(f"{Colors.BLUE}â„¹ï¸  {msg}{Colors.END}")
def log_success(msg): print(f"{Colors.GREEN}âœ… {msg}{Colors.END}")
def log_warning(msg): print(f"{Colors.YELLOW}âš ï¸  {msg}{Colors.END}")
def log_error(msg): print(f"{Colors.RED}âŒ {msg}{Colors.END}")
def log_step(msg): print(f"{Colors.PURPLE}ğŸ”„ {msg}{Colors.END}")

@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    step_name: str
    processing_time: float
    memory_before: float
    memory_after: float
    memory_peak: float
    cpu_usage: float
    gpu_usage: Optional[float]
    input_size: Tuple[int, int]
    output_size: Tuple[int, int]
    quality_score: float
    error_message: Optional[str] = None

@dataclass
class SystemInfo:
    """ì‹œìŠ¤í…œ ì •ë³´ ë°ì´í„° í´ë˜ìŠ¤"""
    os_type: str
    os_version: str
    architecture: str
    cpu_model: str
    cpu_cores: int
    total_ram: float
    gpu_model: Optional[str]
    gpu_memory: Optional[float]
    pytorch_version: str
    device_type: str
    mps_available: bool
    cuda_available: bool

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.process = psutil.Process()
        self.start_time = None
        self.memory_tracker = []
        
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.start_time = time.perf_counter()
        self.memory_tracker = []
        
    def get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB)"""
        return self.process.memory_info().rss / (1024**3)
    
    def get_cpu_usage(self) -> float:
        """CPU ì‚¬ìš©ë¥  (%)"""
        return self.process.cpu_percent(interval=0.1)
    
    def get_gpu_usage(self) -> Optional[float]:
        """GPU ì‚¬ìš©ë¥  (%) - MPS/CUDA"""
        try:
            if torch.backends.mps.is_available():
                # MPSëŠ” ì§ì ‘ì ì¸ ì‚¬ìš©ë¥  ì¸¡ì •ì´ ì–´ë ¤ì›€
                return None
            elif torch.cuda.is_available():
                return torch.cuda.utilization()
            return None
        except:
            return None
    
    def stop_monitoring(self) -> float:
        """ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ ë° ê²½ê³¼ ì‹œê°„ ë°˜í™˜"""
        if self.start_time:
            return time.perf_counter() - self.start_time
        return 0.0

class MockPipelineStep:
    """ì‹¤ì œ AI ëª¨ë¸ ëŒ€ì‹  ì‚¬ìš©í•  ëª¨í‚¹ í´ë˜ìŠ¤"""
    
    def __init__(self, step_name: str, complexity: float = 1.0):
        self.step_name = step_name
        self.complexity = complexity
        
    def process(self, input_data: np.ndarray) -> Tuple[np.ndarray, float]:
        """ê°€ìƒ ì²˜ë¦¬ ìˆ˜í–‰"""
        # ë³µì¡ë„ì— ë”°ë¥¸ ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
        processing_time = self.complexity * 0.5  # ê¸°ë³¸ 0.5ì´ˆ * ë³µì¡ë„
        
        # CPU ë¶€í•˜ ì‹œë®¬ë ˆì´ì…˜
        for _ in range(int(self.complexity * 1000000)):
            _ = np.random.random()
        
        # ë©”ëª¨ë¦¬ í• ë‹¹ ì‹œë®¬ë ˆì´ì…˜
        temp_data = np.random.random((
            int(input_data.shape[0] * self.complexity),
            int(input_data.shape[1] * self.complexity),
            input_data.shape[2]
        )).astype(np.float32)
        
        time.sleep(processing_time)
        
        # í’ˆì§ˆ ì ìˆ˜ ì‹œë®¬ë ˆì´ì…˜ (ë³µì¡ë„ê°€ ë†’ì„ìˆ˜ë¡ í’ˆì§ˆ ì¢‹ìŒ)
        quality_score = min(0.95, 0.6 + self.complexity * 0.2)
        
        # ê²°ê³¼ í¬ê¸°ëŠ” ì…ë ¥ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€
        result = input_data.copy()
        
        return result, quality_score

class PipelineBenchmark:
    """8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self, device: str = "auto"):
        self.device = self._setup_device(device)
        self.monitor = PerformanceMonitor()
        self.results: List[BenchmarkResult] = []
        
        # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìŠ¤í…ë“¤ (ì‹¤ì œ ëª¨ë¸ ëŒ€ì‹  ëª¨í‚¹)
        self.pipeline_steps = {
            "01_human_parsing": MockPipelineStep("Human Parsing", 2.0),
            "02_pose_estimation": MockPipelineStep("Pose Estimation", 1.5),
            "03_cloth_segmentation": MockPipelineStep("Cloth Segmentation", 1.8),
            "04_geometric_matching": MockPipelineStep("Geometric Matching", 1.2),
            "05_cloth_warping": MockPipelineStep("Cloth Warping", 1.0),
            "06_virtual_fitting": MockPipelineStep("Virtual Fitting", 3.0),
            "07_post_processing": MockPipelineStep("Post Processing", 0.8),
            "08_quality_assessment": MockPipelineStep("Quality Assessment", 0.5)
        }
        
    def _setup_device(self, device: str) -> torch.device:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device == "auto":
            if torch.backends.mps.is_available():
                device = "mps"
            elif torch.cuda.is_available():
                device = "cuda"
            else:
                device = "cpu"
        
        torch_device = torch.device(device)
        log_info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {torch_device}")
        return torch_device
    
    def get_system_info(self) -> SystemInfo:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        gpu_model = None
        gpu_memory = None
        
        if torch.backends.mps.is_available():
            gpu_model = "Apple Silicon GPU (MPS)"
            # MPS ë©”ëª¨ë¦¬ëŠ” ì‹œìŠ¤í…œ RAMê³¼ ê³µìœ ë˜ë¯€ë¡œ ë³„ë„ ì¸¡ì • ì–´ë ¤ì›€
            gpu_memory = None
        elif torch.cuda.is_available():
            gpu_model = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        
        return SystemInfo(
            os_type=platform.system(),
            os_version=platform.release(),
            architecture=platform.machine(),
            cpu_model=platform.processor() or "Unknown",
            cpu_cores=psutil.cpu_count(),
            total_ram=psutil.virtual_memory().total / (1024**3),
            gpu_model=gpu_model,
            gpu_memory=gpu_memory,
            pytorch_version=torch.__version__,
            device_type=str(self.device),
            mps_available=torch.backends.mps.is_available(),
            cuda_available=torch.cuda.is_available()
        )
    
    def create_test_input(self, size: Tuple[int, int] = (512, 512)) -> np.ndarray:
        """í…ŒìŠ¤íŠ¸ìš© ì…ë ¥ ì´ë¯¸ì§€ ìƒì„±"""
        # RGB ì´ë¯¸ì§€ ìƒì„± (ì‚¬ëŒ ì‹¤ë£¨ì—£ í˜•íƒœ)
        image = np.zeros((size[1], size[0], 3), dtype=np.uint8)
        
        # ê°„ë‹¨í•œ ì‚¬ëŒ í˜•íƒœ ê·¸ë¦¬ê¸°
        center_x, center_y = size[0] // 2, size[1] // 2
        
        # ë¨¸ë¦¬ (ì›)
        cv2.circle(image, (center_x, center_y - 100), 30, (255, 255, 255), -1)
        
        # ëª¸í†µ (ì‚¬ê°í˜•)
        cv2.rectangle(image, 
                     (center_x - 40, center_y - 70),
                     (center_x + 40, center_y + 50),
                     (255, 255, 255), -1)
        
        # íŒ”
        cv2.rectangle(image,
                     (center_x - 70, center_y - 50),
                     (center_x - 40, center_y),
                     (255, 255, 255), -1)
        cv2.rectangle(image,
                     (center_x + 40, center_y - 50),
                     (center_x + 70, center_y),
                     (255, 255, 255), -1)
        
        # ë‹¤ë¦¬
        cv2.rectangle(image,
                     (center_x - 25, center_y + 50),
                     (center_x - 10, center_y + 120),
                     (255, 255, 255), -1)
        cv2.rectangle(image,
                     (center_x + 10, center_y + 50),
                     (center_x + 25, center_y + 120),
                     (255, 255, 255), -1)
        
        return image.astype(np.float32) / 255.0
    
    def benchmark_step(self, step_name: str, input_data: np.ndarray) -> BenchmarkResult:
        """ê°œë³„ íŒŒì´í”„ë¼ì¸ ìŠ¤í… ë²¤ì¹˜ë§ˆí¬"""
        step = self.pipeline_steps[step_name]
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.device.type == "mps":
            torch.mps.empty_cache()
        elif self.device.type == "cuda":
            torch.cuda.empty_cache()
        
        # ì´ˆê¸° ìƒíƒœ ì¸¡ì •
        memory_before = self.monitor.get_memory_usage()
        
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.monitor.start_monitoring()
        
        try:
            # ìŠ¤í… ì‹¤í–‰
            output_data, quality_score = step.process(input_data)
            
            # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            processing_time = self.monitor.stop_monitoring()
            
            # ìµœì¢… ìƒíƒœ ì¸¡ì •
            memory_after = self.monitor.get_memory_usage()
            memory_peak = max(memory_before, memory_after)
            cpu_usage = self.monitor.get_cpu_usage()
            gpu_usage = self.monitor.get_gpu_usage()
            
            return BenchmarkResult(
                step_name=step_name,
                processing_time=processing_time,
                memory_before=memory_before,
                memory_after=memory_after,
                memory_peak=memory_peak,
                cpu_usage=cpu_usage,
                gpu_usage=gpu_usage,
                input_size=input_data.shape[:2],
                output_size=output_data.shape[:2],
                quality_score=quality_score
            )
            
        except Exception as e:
            processing_time = self.monitor.stop_monitoring()
            return BenchmarkResult(
                step_name=step_name,
                processing_time=processing_time,
                memory_before=memory_before,
                memory_after=self.monitor.get_memory_usage(),
                memory_peak=memory_before,
                cpu_usage=0.0,
                gpu_usage=None,
                input_size=input_data.shape[:2],
                output_size=(0, 0),
                quality_score=0.0,
                error_message=str(e)
            )
    
    def run_full_pipeline(self, input_sizes: List[Tuple[int, int]] = [(256, 256), (512, 512), (1024, 1024)]) -> Dict:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        all_results = {}
        
        for size in input_sizes:
            log_step(f"í•´ìƒë„ {size[0]}x{size[1]} ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
            
            # í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
            test_input = self.create_test_input(size)
            current_data = test_input
            
            size_results = []
            total_start_time = time.perf_counter()
            
            # 8ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰
            for step_name in self.pipeline_steps.keys():
                log_info(f"  ì‹¤í–‰ ì¤‘: {step_name}")
                
                result = self.benchmark_step(step_name, current_data)
                size_results.append(result)
                
                if result.error_message:
                    log_error(f"    ì˜¤ë¥˜: {result.error_message}")
                    break
                else:
                    log_success(f"    ì™„ë£Œ: {result.processing_time:.2f}s, í’ˆì§ˆ: {result.quality_score:.2f}")
            
            total_time = time.perf_counter() - total_start_time
            
            all_results[f"{size[0]}x{size[1]}"] = {
                "steps": size_results,
                "total_time": total_time,
                "average_quality": np.mean([r.quality_score for r in size_results if r.error_message is None]),
                "total_memory_used": max([r.memory_peak for r in size_results]) - min([r.memory_before for r in size_results]),
                "throughput": 1.0 / total_time if total_time > 0 else 0.0
            }
        
        return all_results
    
    def generate_report(self, results: Dict, output_file: Optional[str] = None) -> str:
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        system_info = self.get_system_info()
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""
{Colors.BOLD}ğŸ”¥ MyCloset AI - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸{Colors.END}
{'='*80}

ğŸ“… ì‹¤í–‰ ì‹œê°„: {timestamp}
ğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´:
   OS: {system_info.os_type} {system_info.os_version} ({system_info.architecture})
   CPU: {system_info.cpu_model} ({system_info.cpu_cores} cores)
   RAM: {system_info.total_ram:.1f} GB
   GPU: {system_info.gpu_model or 'N/A'}
   GPU ë©”ëª¨ë¦¬: {f'{system_info.gpu_memory:.1f} GB' if system_info.gpu_memory else 'N/A'}
   PyTorch: {system_info.pytorch_version}
   ë””ë°”ì´ìŠ¤: {system_info.device_type}
   MPS ì§€ì›: {'âœ…' if system_info.mps_available else 'âŒ'}
   CUDA ì§€ì›: {'âœ…' if system_info.cuda_available else 'âŒ'}

ğŸ“Š ì„±ëŠ¥ ê²°ê³¼ ìš”ì•½:
"""
        
        # í•´ìƒë„ë³„ ê²°ê³¼ í…Œì´ë¸”
        report += f"\n{'í•´ìƒë„':<12} {'ì´ ì‹œê°„':<10} {'ì²˜ë¦¬ëŸ‰':<12} {'í‰ê·  í’ˆì§ˆ':<12} {'ë©”ëª¨ë¦¬ ì‚¬ìš©':<12}\n"
        report += "-" * 70 + "\n"
        
        for resolution, data in results.items():
            throughput = data['throughput']
            quality = data['average_quality']
            memory = data['total_memory_used']
            total_time = data['total_time']
            
            report += f"{resolution:<12} {total_time:<10.2f}s {throughput:<12.2f}/s {quality:<12.2f} {memory:<12.2f}GB\n"
        
        # ë‹¨ê³„ë³„ ìƒì„¸ ë¶„ì„
        report += f"\nğŸ“‹ ë‹¨ê³„ë³„ ìƒì„¸ ë¶„ì„:\n"
        
        for resolution, data in results.items():
            report += f"\nğŸ”¸ {resolution} í•´ìƒë„:\n"
            report += f"{'ë‹¨ê³„':<25} {'ì‹œê°„':<8} {'ë©”ëª¨ë¦¬':<10} CPU{'%':<4} {'í’ˆì§ˆ':<6}\n"
            report += "-" * 55 + "\n"
            
            for step_result in data['steps']:
                if step_result.error_message:
                    report += f"{step_result.step_name:<25} ERROR    -         -    -\n"
                else:
                    memory_delta = step_result.memory_after - step_result.memory_before
                    report += f"{step_result.step_name:<25} {step_result.processing_time:<8.2f}s {memory_delta:<+8.2f}GB {step_result.cpu_usage:<5.1f} {step_result.quality_score:<6.2f}\n"
        
        # ì„±ëŠ¥ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­
        report += f"\nğŸ’¡ ì„±ëŠ¥ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­:\n"
        
        # ìµœì  í•´ìƒë„ ì°¾ê¸°
        best_resolution = max(results.keys(), key=lambda x: results[x]['average_quality'] / results[x]['total_time'])
        report += f"   ğŸ¯ ê¶Œì¥ í•´ìƒë„: {best_resolution} (í’ˆì§ˆ/ì‹œê°„ ë¹„ìœ¨ ìµœì )\n"
        
        # ë³‘ëª© ë‹¨ê³„ ì°¾ê¸°
        all_steps_time = {}
        for data in results.values():
            for step in data['steps']:
                if step.step_name not in all_steps_time:
                    all_steps_time[step.step_name] = []
                all_steps_time[step.step_name].append(step.processing_time)
        
        bottleneck_step = max(all_steps_time.keys(), key=lambda x: np.mean(all_steps_time[x]))
        bottleneck_time = np.mean(all_steps_time[bottleneck_step])
        report += f"   âš ï¸  ë³‘ëª© ë‹¨ê³„: {bottleneck_step} (í‰ê·  {bottleneck_time:.2f}s)\n"
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        max_memory = max([data['total_memory_used'] for data in results.values()])
        if max_memory > 8.0:
            report += f"   ğŸ”´ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {max_memory:.1f}GB (ìµœì í™” í•„ìš”)\n"
        elif max_memory > 4.0:
            report += f"   ğŸŸ¡ ë³´í†µ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {max_memory:.1f}GB (í—ˆìš© ë²”ìœ„)\n"
        else:
            report += f"   ğŸŸ¢ ë‚®ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {max_memory:.1f}GB (íš¨ìœ¨ì )\n"
        
        # ë””ë°”ì´ìŠ¤ë³„ ê¶Œì¥ì‚¬í•­
        if system_info.device_type == "mps":
            report += f"\nğŸ M3 Max ìµœì í™” ê¶Œì¥ì‚¬í•­:\n"
            report += f"   â€¢ FP16 ëª¨ë“œ í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ì ˆì•½\n"
            report += f"   â€¢ ë°°ì¹˜ í¬ê¸° ì¡°ì •ìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ í–¥ìƒ\n"
            report += f"   â€¢ ëª¨ë¸ ì–‘ìí™”ë¡œ ì†ë„ ê°œì„ \n"
        elif system_info.device_type == "cuda":
            report += f"\nğŸš€ CUDA ìµœì í™” ê¶Œì¥ì‚¬í•­:\n"
            report += f"   â€¢ CUDA Streams í™œìš©ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬\n"
            report += f"   â€¢ TensorRT ìµœì í™” ì ìš©\n"
            report += f"   â€¢ í˜¼í•© ì •ë°€ë„ í›ˆë ¨ í™œìš©\n"
        else:
            report += f"\nğŸ’» CPU ìµœì í™” ê¶Œì¥ì‚¬í•­:\n"
            report += f"   â€¢ OpenMP ìŠ¤ë ˆë“œ ìˆ˜ ì¡°ì •\n"
            report += f"   â€¢ ONNX ëŸ°íƒ€ì„ í™œìš©\n"
            report += f"   â€¢ ëª¨ë¸ ê²½ëŸ‰í™” ì ìš©\n"
        
        # íŒŒì¼ë¡œ ì €ì¥
        if output_file:
            # ìƒ‰ìƒ ì½”ë“œ ì œê±°í•œ ë²„ì „ìœ¼ë¡œ ì €ì¥
            clean_report = report
            for color in [Colors.BLUE, Colors.GREEN, Colors.YELLOW, Colors.RED, Colors.PURPLE, Colors.CYAN, Colors.BOLD, Colors.END]:
                clean_report = clean_report.replace(color, "")
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(clean_report)
            log_success(f"ë¦¬í¬íŠ¸ ì €ì¥ë¨: {output_file}")
        
        return report

def stress_test(benchmark: PipelineBenchmark, duration: int = 60) -> Dict:
    """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ (ì§€ì •ëœ ì‹œê°„ ë™ì•ˆ ì—°ì† ì‹¤í–‰)"""
    log_step(f"{duration}ì´ˆ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    start_time = time.time()
    iteration_count = 0
    results = []
    
    test_input = benchmark.create_test_input((512, 512))
    
    while time.time() - start_time < duration:
        iteration_start = time.time()
        
        # ëœë¤í•˜ê²Œ ëª‡ ê°œ ìŠ¤í…ë§Œ ì‹¤í–‰ (ë¶€í•˜ ì¡°ì ˆ)
        steps_to_run = np.random.choice(list(benchmark.pipeline_steps.keys()), 
                                       size=np.random.randint(3, 6), 
                                       replace=False)
        
        iteration_results = []
        for step_name in steps_to_run:
            result = benchmark.benchmark_step(step_name, test_input)
            iteration_results.append(result)
        
        iteration_time = time.time() - iteration_start
        results.append({
            'iteration': iteration_count,
            'time': iteration_time,
            'steps': len(steps_to_run),
            'memory_peak': max([r.memory_peak for r in iteration_results])
        })
        
        iteration_count += 1
        
        if iteration_count % 10 == 0:
            log_info(f"ë°˜ë³µ: {iteration_count}, í‰ê·  ì‹œê°„: {np.mean([r['time'] for r in results[-10:]]):.2f}s")
    
    total_time = time.time() - start_time
    
    return {
        'total_time': total_time,
        'iterations': iteration_count,
        'average_iteration_time': np.mean([r['time'] for r in results]),
        'max_memory': max([r['memory_peak'] for r in results]),
        'throughput': iteration_count / total_time,
        'stability_score': 1.0 - (np.std([r['time'] for r in results]) / np.mean([r['time'] for r in results]))
    }

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="MyCloset AI - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        "--device",
        choices=["auto", "cpu", "mps", "cuda"],
        default="auto",
        help="ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: auto)"
    )
    parser.add_argument(
        "--resolutions",
        default="256x256,512x512,1024x1024",
        help="í…ŒìŠ¤íŠ¸í•  í•´ìƒë„ë“¤ (ê¸°ë³¸ê°’: 256x256,512x512,1024x1024)"
    )
    parser.add_argument(
        "--output",
        type=str,
        help="ê²°ê³¼ ë¦¬í¬íŠ¸ ì €ì¥ íŒŒì¼ëª…"
    )
    parser.add_argument(
        "--stress-test",
        type=int,
        metavar="SECONDS",
        help="ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì´ˆ ë‹¨ìœ„)"
    )
    parser.add_argument(
        "--json-output",
        type=str,
        help="JSON í˜•íƒœë¡œ ê²°ê³¼ ì €ì¥"
    )
    parser.add_argument(
        "--quick",
        action="store_true",
        help="ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (512x512ë§Œ ì‹¤í–‰)"
    )
    
    args = parser.parse_args()
    
    # í—¤ë”
    print(f"\n{Colors.BOLD}{Colors.BLUE}âš¡ MyCloset AI - íŒŒì´í”„ë¼ì¸ ë²¤ì¹˜ë§ˆí¬{Colors.END}")
    print("=" * 60)
    
    # í•´ìƒë„ íŒŒì‹±
    if args.quick:
        resolutions = [(512, 512)]
    else:
        resolutions = []
        for res_str in args.resolutions.split(','):
            width, height = map(int, res_str.split('x'))
            resolutions.append((width, height))
    
    # ë²¤ì¹˜ë§ˆí¬ ê°ì²´ ìƒì„±
    benchmark = PipelineBenchmark(device=args.device)
    
    # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
    if args.stress_test:
        stress_results = stress_test(benchmark, args.stress_test)
        
        print(f"\nğŸ“Š ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   ì´ ì‹œê°„: {stress_results['total_time']:.1f}ì´ˆ")
        print(f"   ë°˜ë³µ íšŸìˆ˜: {stress_results['iterations']}")
        print(f"   í‰ê·  ë°˜ë³µ ì‹œê°„: {stress_results['average_iteration_time']:.2f}ì´ˆ")
        print(f"   ì²˜ë¦¬ëŸ‰: {stress_results['throughput']:.2f} iterations/sec")
        print(f"   ì•ˆì •ì„± ì ìˆ˜: {stress_results['stability_score']:.2f}")
        print(f"   ìµœëŒ€ ë©”ëª¨ë¦¬: {stress_results['max_memory']:.2f}GB")
        
        # JSON ì €ì¥
        if args.json_output:
            stress_results['timestamp'] = datetime.now().isoformat()
            stress_results['system_info'] = asdict(benchmark.get_system_info())
            
            with open(args.json_output, 'w') as f:
                json.dump(stress_results, f, indent=2)
            log_success(f"JSON ê²°ê³¼ ì €ì¥ë¨: {args.json_output}")
    
    else:
        # ì¼ë°˜ ë²¤ì¹˜ë§ˆí¬
        log_info(f"í…ŒìŠ¤íŠ¸ í•´ìƒë„: {[f'{w}x{h}' for w, h in resolutions]}")
        
        results = benchmark.run_full_pipeline(resolutions)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        output_file = args.output or f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        report = benchmark.generate_report(results, output_file)
        
        # í™”ë©´ì— ì¶œë ¥
        print(report)
        
        # JSON ì €ì¥
        if args.json_output:
            json_data = {
                'timestamp': datetime.now().isoformat(),
                'system_info': asdict(benchmark.get_system_info()),
                'results': {}
            }
            
            for resolution, data in results.items():
                json_data['results'][resolution] = {
                    'total_time': data['total_time'],
                    'average_quality': data['average_quality'],
                    'total_memory_used': data['total_memory_used'],
                    'throughput': data['throughput'],
                    'steps': [asdict(step) for step in data['steps']]
                }
            
            with open(args.json_output, 'w') as f:
                json.dump(json_data, f, indent=2)
            log_success(f"JSON ê²°ê³¼ ì €ì¥ë¨: {args.json_output}")
    
    log_success("ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ! ğŸ‰")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        log_warning("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        log_error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)