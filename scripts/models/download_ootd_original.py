#!/usr/bin/env python3
"""
ğŸ”¥ OOTDiffusion ì›ë³¸ ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë” v2.0
âœ… ì‹¤ì œ ê³ í’ˆì§ˆ OOTDiffusion ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
âœ… Hugging Face Hub ì™„ì „ ì§€ì›
âœ… M3 Max 128GB ìµœì í™”
âœ… ë„¤íŠ¸ì›Œí¬ ì•ˆì •ì„± ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
âœ… ì§„í–‰ë¥  í‘œì‹œ ë° ë‹¤ì¤‘ ì†ŒìŠ¤ ì§€ì›
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
"""

import os
import sys
import time
import json
import shutil
import hashlib
import requests
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from urllib.parse import urlparse
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from enum import Enum

# ì§„í–‰ë¥  í‘œì‹œ
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False

# Hugging Face Hub
try:
    from huggingface_hub import hf_hub_download, snapshot_download, login, HfApi
    from huggingface_hub.utils import HfHubHTTPError
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False

# Git LFS ì§€ì›
def check_git_lfs():
    """Git LFS ì„¤ì¹˜ í™•ì¸"""
    try:
        import subprocess
        subprocess.run(["git", "lfs", "version"], capture_output=True, check=True)
        return True
    except:
        return False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DownloadStatus(Enum):
    PENDING = "pending"
    DOWNLOADING = "downloading"
    COMPLETED = "completed"
    FAILED = "failed"
    VERIFIED = "verified"

@dataclass
class ModelFile:
    """ëª¨ë¸ íŒŒì¼ ì •ë³´"""
    name: str
    size_mb: float
    url: str
    local_path: Path
    md5_hash: Optional[str] = None
    priority: int = 1
    description: str = ""

class OOTDiffusionDownloader:
    """OOTDiffusion ì›ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë”"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.ai_models_dir = project_root / "ai_models"
        self.download_dir = self.ai_models_dir / "downloads" / "ootdiffusion_original"
        self.hf_cache_dir = self.ai_models_dir / "huggingface_cache"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.download_dir.mkdir(parents=True, exist_ok=True)
        self.hf_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Hugging Face API
        self.hf_api = HfApi() if HF_AVAILABLE else None
        
        # ë‹¤ìš´ë¡œë“œ í†µê³„
        self.stats = {
            "total_files": 0,
            "completed_files": 0,
            "failed_files": 0,
            "total_size_mb": 0,
            "downloaded_mb": 0,
            "start_time": time.time()
        }
        
    def get_ootd_model_catalog(self) -> Dict[str, Any]:
        """OOTDiffusion ëª¨ë¸ ì¹´íƒˆë¡œê·¸ ë°˜í™˜"""
        return {
            # ğŸ”¥ í•µì‹¬ OOTDiffusion ëª¨ë¸ë“¤
            "ootdiffusion_main": {
                "repo_id": "levihsu/OOTDiffusion",
                "description": "ë©”ì¸ OOTDiffusion ëª¨ë¸ - ìµœê³  í’ˆì§ˆ",
                "priority": 1,
                "size_gb": 8.5,
                "files": [
                    "checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors",
                    "checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton/config.json",
                    "checkpoints/ootd/ootd_dc/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors",
                    "checkpoints/ootd/ootd_dc/checkpoint-36000/unet_vton/config.json",
                ]
            },
            
            "stable_diffusion_inpaint": {
                "repo_id": "runwayml/stable-diffusion-inpainting",
                "description": "Stable Diffusion Inpainting ëª¨ë¸",
                "priority": 2,
                "size_gb": 4.2,
                "files": [
                    "unet/diffusion_pytorch_model.safetensors",
                    "unet/config.json",
                    "vae/diffusion_pytorch_model.safetensors",
                    "vae/config.json",
                    "text_encoder/pytorch_model.bin",
                    "text_encoder/config.json",
                ]
            },
            
            "clip_vit_large": {
                "repo_id": "openai/clip-vit-large-patch14",
                "description": "CLIP Vision Transformer ëª¨ë¸",
                "priority": 3,
                "size_gb": 1.7,
                "files": [
                    "pytorch_model.bin",
                    "config.json",
                ]
            }
        }

    def check_system_requirements(self) -> bool:
        """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
        logger.info("ğŸ” ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì¤‘...")
        
        # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
        free_space_gb = shutil.disk_usage(self.ai_models_dir).free / (1024**3)
        required_space_gb = 20  # ì—¬ìœ ë¶„ í¬í•¨
        
        if free_space_gb < required_space_gb:
            logger.error(f"âŒ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: {free_space_gb:.1f}GB (í•„ìš”: {required_space_gb}GB)")
            return False
            
        logger.info(f"âœ… ë””ìŠ¤í¬ ê³µê°„ ì¶©ë¶„: {free_space_gb:.1f}GB")
        
        # Git LFS í™•ì¸
        if check_git_lfs():
            logger.info("âœ… Git LFS ì‚¬ìš© ê°€ëŠ¥")
        else:
            logger.warning("âš ï¸ Git LFS ì—†ìŒ - HTTP ë‹¤ìš´ë¡œë“œ ì‚¬ìš©")
            
        # Hugging Face Hub í™•ì¸
        if HF_AVAILABLE:
            logger.info("âœ… Hugging Face Hub ì‚¬ìš© ê°€ëŠ¥")
        else:
            logger.warning("âš ï¸ Hugging Face Hub ì—†ìŒ - ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹œë„")
            
        return True

    def download_with_huggingface_hub(self, repo_id: str, files: List[str]) -> bool:
        """Hugging Face Hubìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ"""
        if not HF_AVAILABLE:
            return False
            
        try:
            logger.info(f"ğŸ¤— Hugging Face Hubìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ: {repo_id}")
            
            # ì „ì²´ ì €ì¥ì†Œ ìŠ¤ëƒ…ìƒ· ë‹¤ìš´ë¡œë“œ
            local_dir = self.download_dir / repo_id.replace("/", "_")
            
            snapshot_download(
                repo_id=repo_id,
                local_dir=str(local_dir),
                cache_dir=str(self.hf_cache_dir),
                local_files_only=False,
                token=None,  # ê³µê°œ ëª¨ë¸ì´ë¯€ë¡œ í† í° ë¶ˆí•„ìš”
                resume_download=True,
                max_workers=4,
                tqdm_class=tqdm if TQDM_AVAILABLE else None
            )
            
            logger.info(f"âœ… {repo_id} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_dir}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Hugging Face ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {repo_id}: {e}")
            return False

    def download_file_direct(self, url: str, local_path: Path, chunk_size: int = 8192) -> bool:
        """ì§ì ‘ HTTP ë‹¤ìš´ë¡œë“œ"""
        try:
            logger.info(f"ğŸŒ ì§ì ‘ ë‹¤ìš´ë¡œë“œ: {url}")
            
            # ë¶€ë¶„ ë‹¤ìš´ë¡œë“œ ì§€ì›ì„ ìœ„í•œ í—¤ë”
            headers = {}
            resume_pos = 0
            
            if local_path.exists():
                resume_pos = local_path.stat().st_size
                headers['Range'] = f'bytes={resume_pos}-'
                logger.info(f"ğŸ“‚ ê¸°ì¡´ íŒŒì¼ ë°œê²¬ - ì¬ê°œ: {resume_pos} bytes")
            
            response = requests.get(url, headers=headers, stream=True, timeout=30)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            if resume_pos > 0:
                total_size += resume_pos
                
            # ë””ë ‰í† ë¦¬ ìƒì„±
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            mode = 'ab' if resume_pos > 0 else 'wb'
            with open(local_path, mode) as f:
                if TQDM_AVAILABLE:
                    with tqdm(
                        total=total_size, 
                        initial=resume_pos,
                        unit='B', 
                        unit_scale=True,
                        desc=local_path.name
                    ) as pbar:
                        for chunk in response.iter_content(chunk_size=chunk_size):
                            if chunk:
                                f.write(chunk)
                                pbar.update(len(chunk))
                else:
                    for chunk in response.iter_content(chunk_size=chunk_size):
                        if chunk:
                            f.write(chunk)
                            
            logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def verify_file_integrity(self, file_path: Path, expected_md5: Optional[str] = None) -> bool:
        """íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦"""
        if not file_path.exists():
            return False
            
        # íŒŒì¼ í¬ê¸° í™•ì¸ (ìµœì†Œ 1MB)
        file_size_mb = file_path.stat().st_size / (1024 * 1024)
        if file_size_mb < 1:
            logger.warning(f"âš ï¸ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŒ: {file_path} ({file_size_mb:.1f}MB)")
            return False
            
        # MD5 í•´ì‹œ í™•ì¸ (ì„ íƒì )
        if expected_md5:
            try:
                with open(file_path, 'rb') as f:
                    file_hash = hashlib.md5(f.read()).hexdigest()
                    if file_hash != expected_md5:
                        logger.error(f"âŒ MD5 ë¶ˆì¼ì¹˜: {file_path}")
                        return False
            except Exception as e:
                logger.warning(f"âš ï¸ MD5 í™•ì¸ ì‹¤íŒ¨: {e}")
                
        return True

    def setup_model_links(self):
        """ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ì„ ì‹œìŠ¤í…œì— ì—°ê²°"""
        logger.info("ğŸ”— ëª¨ë¸ ë§í¬ ì„¤ì • ì¤‘...")
        
        # ë°±ì—”ë“œ AI ëª¨ë¸ ê²½ë¡œë“¤
        backend_paths = [
            self.project_root / "backend" / "ai_models",
            self.project_root / "backend" / "app" / "ai_pipeline" / "models" / "downloads"
        ]
        
        for backend_path in backend_paths:
            backend_path.mkdir(parents=True, exist_ok=True)
            
            # ì‹¬ë³¼ë¦­ ë§í¬ ë˜ëŠ” ë³µì‚¬
            ootd_link = backend_path / "ootdiffusion"
            if not ootd_link.exists():
                try:
                    # ì‹¬ë³¼ë¦­ ë§í¬ ì‹œë„
                    ootd_link.symlink_to(self.download_dir)
                    logger.info(f"âœ… ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±: {ootd_link}")
                except OSError:
                    # ì‹¬ë³¼ë¦­ ë§í¬ ì‹¤íŒ¨ì‹œ í•˜ë“œë§í¬ ì‹œë„
                    logger.info(f"ğŸ“‹ ë””ë ‰í† ë¦¬ ë³µì‚¬: {backend_path}")
                    
        # Hugging Face ìºì‹œ ì—°ê²°
        hf_target = self.hf_cache_dir / "models--levihsu--OOTDiffusion"
        if self.download_dir.exists() and not hf_target.exists():
            try:
                hf_target.parent.mkdir(parents=True, exist_ok=True)
                hf_target.symlink_to(self.download_dir / "levihsu_OOTDiffusion")
                logger.info(f"âœ… HF ìºì‹œ ë§í¬ ìƒì„±: {hf_target}")
            except:
                logger.warning("âš ï¸ HF ìºì‹œ ë§í¬ ìƒì„± ì‹¤íŒ¨")

    def download_ootdiffusion_models(self, models: List[str] = None) -> bool:
        """OOTDiffusion ëª¨ë¸ë“¤ ë‹¤ìš´ë¡œë“œ"""
        if not self.check_system_requirements():
            return False
            
        catalog = self.get_ootd_model_catalog()
        target_models = models or list(catalog.keys())
        
        logger.info(f"ğŸš€ OOTDiffusion ë‹¤ìš´ë¡œë“œ ì‹œì‘: {len(target_models)}ê°œ ëª¨ë¸")
        
        success_count = 0
        
        for model_name in target_models:
            if model_name not in catalog:
                logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}")
                continue
                
            model_info = catalog[model_name]
            repo_id = model_info["repo_id"]
            
            logger.info(f"ğŸ“¦ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_name} ({model_info['description']})")
            
            # Hugging Face Hub ë‹¤ìš´ë¡œë“œ ì‹œë„
            if self.download_with_huggingface_hub(repo_id, model_info["files"]):
                success_count += 1
                continue
                
            # ì§ì ‘ ë‹¤ìš´ë¡œë“œ ëŒ€ì²´ ë°©ë²•
            logger.info(f"ğŸ”„ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹œë„: {model_name}")
            
            # GitHub Release ë˜ëŠ” ëŒ€ì²´ URLë“¤
            alternative_urls = self.get_alternative_download_urls(model_name)
            
            downloaded = False
            for url in alternative_urls:
                local_path = self.download_dir / model_name / Path(url).name
                if self.download_file_direct(url, local_path):
                    downloaded = True
                    break
                    
            if downloaded:
                success_count += 1
            else:
                logger.error(f"âŒ ëª¨ë“  ë‹¤ìš´ë¡œë“œ ë°©ë²• ì‹¤íŒ¨: {model_name}")
                
        # ê²°ê³¼ ë³´ê³ 
        total_models = len(target_models)
        logger.info(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {success_count}/{total_models}")
        
        if success_count > 0:
            self.setup_model_links()
            self.generate_download_report()
            
        return success_count == total_models

    def get_alternative_download_urls(self, model_name: str) -> List[str]:
        """ëŒ€ì²´ ë‹¤ìš´ë¡œë“œ URLë“¤"""
        urls = {
            "ootdiffusion_main": [
                "https://github.com/levihsu/OOTDiffusion/releases/download/v1.0/ootd_hd.safetensors",
                "https://github.com/levihsu/OOTDiffusion/releases/download/v1.0/ootd_dc.safetensors",
            ],
            "stable_diffusion_inpaint": [
                "https://huggingface.co/runwayml/stable-diffusion-inpainting/resolve/main/unet/diffusion_pytorch_model.safetensors",
            ]
        }
        return urls.get(model_name, [])

    def generate_download_report(self):
        """ë‹¤ìš´ë¡œë“œ ë³´ê³ ì„œ ìƒì„±"""
        report_file = self.download_dir / "download_report.json"
        
        report = {
            "timestamp": time.time(),
            "date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "project_root": str(self.project_root),
            "download_directory": str(self.download_dir),
            "statistics": self.stats,
            "system_info": {
                "python_version": sys.version,
                "platform": sys.platform,
                "huggingface_hub_available": HF_AVAILABLE,
                "tqdm_available": TQDM_AVAILABLE,
                "git_lfs_available": check_git_lfs()
            },
            "downloaded_models": []
        }
        
        # ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘
        for model_dir in self.download_dir.iterdir():
            if model_dir.is_dir():
                model_info = {
                    "name": model_dir.name,
                    "path": str(model_dir),
                    "size_mb": sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file()) / (1024*1024),
                    "file_count": len(list(model_dir.rglob('*'))),
                    "key_files": [str(f.relative_to(model_dir)) for f in model_dir.rglob('*.safetensors')][:5]
                }
                report["downloaded_models"].append(model_info)
        
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
            
        logger.info(f"ğŸ“‹ ë‹¤ìš´ë¡œë“œ ë³´ê³ ì„œ ìƒì„±: {report_file}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”¥ OOTDiffusion ì›ë³¸ ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë” v2.0")
    print("=" * 60)
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ìë™ ê°ì§€
    current_dir = Path.cwd()
    project_candidates = [
        current_dir,
        current_dir / "mycloset-ai",
        current_dir.parent / "mycloset-ai",
        Path("/Users/gimdudeul/MVP/mycloset-ai")
    ]
    
    project_root = None
    for candidate in project_candidates:
        if (candidate / "backend").exists():
            project_root = candidate
            break
            
    if not project_root:
        print("âŒ MyCloset AI í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return False
        
    print(f"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    
    # ë‹¤ìš´ë¡œë” ì´ˆê¸°í™”
    downloader = OOTDiffusionDownloader(project_root)
    
    # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
    print("\nğŸš€ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    
    # ëª¨ë¸ ì„ íƒ (ì „ì²´ ë˜ëŠ” í•µì‹¬ë§Œ)
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--essential":
        models = ["ootdiffusion_main"]
        print("ğŸ“¦ í•µì‹¬ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ")
    else:
        models = None
        print("ğŸ“¦ ì „ì²´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
    
    success = downloader.download_ootdiffusion_models(models)
    
    if success:
        print("\nğŸ‰ OOTDiffusion ì›ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        print("=" * 60)
        print("âœ… ê³ í’ˆì§ˆ ì‹¤ì œ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤")
        print("âœ… ì„œë²„ ì¬ì‹œì‘ í›„ ì‹¤ì œ AI ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. cd backend && python app/main.py")
        print("2. http://localhost:8000/docs ì—ì„œ API í…ŒìŠ¤íŠ¸")
        print("3. ê°€ìƒ í”¼íŒ… APIë¡œ ì‹¤ì œ ê³ í’ˆì§ˆ ê²°ê³¼ í™•ì¸")
        
        return True
    else:
        print("\nâš ï¸ ì¼ë¶€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        print("ì„œë²„ëŠ” í´ë°± ëª¨ë“œë¡œ ê³„ì† ì‘ë™í•©ë‹ˆë‹¤")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)