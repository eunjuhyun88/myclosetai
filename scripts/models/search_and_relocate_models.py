#!/usr/bin/env python3
"""
ğŸ” MyCloset AI - ëª¨ë¸ ê²€ìƒ‰ ë° ì¬ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸
M3 Max conda í™˜ê²½ ìµœì í™” ë²„ì „

ê¸°ëŠ¥:
- ì „ì²´ ì‹œìŠ¤í…œì—ì„œ AI ëª¨ë¸ íŒŒì¼ ê²€ìƒ‰
- ì¤‘ë³µ íŒŒì¼ íƒì§€ (ì²´í¬ì„¬ ê¸°ë°˜)
- ì•ˆì „í•œ ì¬ë°°ì¹˜ (ë³µì‚¬ë³¸ ìƒì„±, ìˆœì„œ ë²ˆí˜¸ ìë™ ì¶”ê°€)
- conda í™˜ê²½ í˜¸í™˜

ì‚¬ìš©ë²•:
python search_and_relocate_models.py --scan-only     # ê²€ìƒ‰ë§Œ
python search_and_relocate_models.py --relocate     # ê²€ìƒ‰ í›„ ì¬ë°°ì¹˜
python search_and_relocate_models.py --target-dir ./ai_models  # ì‚¬ìš©ì ì§€ì • ê²½ë¡œ
"""

import os
import sys
import hashlib
import shutil
import json
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
from dataclasses import dataclass, field
from collections import defaultdict
import argparse
import re

# ì•ˆì „í•œ import (conda í™˜ê²½ í˜¸í™˜)
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("âš ï¸ tqdm ì—†ìŒ. ì§„í–‰ë¥  í‘œì‹œ ë¶ˆê°€")

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

@dataclass
class ModelFile:
    """AI ëª¨ë¸ íŒŒì¼ ì •ë³´"""
    path: Path
    name: str
    size_mb: float
    extension: str
    checksum: str = ""
    model_type: str = "unknown"
    framework: str = "unknown"
    confidence: float = 0.0
    duplicate_group: int = 0
    metadata: Dict = field(default_factory=dict)

class ModelSearcher:
    """AI ëª¨ë¸ ê²€ìƒ‰ê¸°"""
    
    def __init__(self, target_dir: Optional[Path] = None):
        self.target_dir = target_dir or Path.cwd() / "ai_models"
        self.discovered_models: List[ModelFile] = []
        self.duplicate_groups: Dict[str, List[ModelFile]] = defaultdict(list)
        
        # AI ëª¨ë¸ íŒŒì¼ í™•ì¥ì íŒ¨í„´
        self.model_extensions = {
            '.pth', '.pt', '.bin', '.safetensors', '.ckpt', '.h5', 
            '.pb', '.onnx', '.tflite', '.pkl', '.pickle', '.model',
            '.weights', '.params', '.caffemodel', '.prototxt'
        }
        
        # AI ëª¨ë¸ í‚¤ì›Œë“œ íŒ¨í„´
        self.ai_keywords = [
            # Framework íŠ¹ì •
            'pytorch', 'tensorflow', 'torch', 'transformers', 'diffusers',
            'huggingface', 'openai', 'anthropic', 'stability',
            
            # ëª¨ë¸ ì•„í‚¤í…ì²˜
            'resnet', 'vgg', 'inception', 'mobilenet', 'efficientnet',
            'bert', 'gpt', 'clip', 'vit', 'swin', 'deit',
            'unet', 'vae', 'gan', 'diffusion', 'stable',
            
            # CV íƒœìŠ¤í¬
            'detection', 'segmentation', 'classification', 'pose',
            'parsing', 'openpose', 'yolo', 'rcnn', 'ssd', 'sam',
            'u2net', 'graphonomy', 'schp', 'atr', 'hrnet',
            
            # NLP íƒœìŠ¤í¬  
            'language', 'text', 'embedding', 'tokenizer',
            
            # Virtual Try-on íŠ¹í™”
            'viton', 'tryon', 'ootd', 'garment', 'cloth', 'fashion',
            'warping', 'fitting', 'geometric', 'matching',
            
            # ì¼ë°˜ AI
            'pretrained', 'checkpoint', 'model', 'weights',
            'encoder', 'decoder', 'backbone', 'head'
        ]
        
        # ê²€ìƒ‰ ê²½ë¡œë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
        self.search_paths = self._get_search_paths()
        
        print(f"ğŸ¯ íƒ€ê²Ÿ ë””ë ‰í† ë¦¬: {self.target_dir}")
        print(f"ğŸ” ê²€ìƒ‰ ê²½ë¡œ: {len(self.search_paths)}ê°œ")
        
    def _get_search_paths(self) -> List[Path]:
        """ê²€ìƒ‰ ê²½ë¡œ ëª©ë¡ ìƒì„±"""
        paths = []
        
        # 1. í˜„ì¬ í”„ë¡œì íŠ¸ ê²½ë¡œë“¤
        current_dir = Path.cwd()
        project_paths = [
            current_dir / "ai_models",
            current_dir / "backend" / "ai_models", 
            current_dir / "models",
            current_dir / "checkpoints",
            current_dir / "weights",
            current_dir.parent / "ai_models",
        ]
        
        # 2. ì‹œìŠ¤í…œ ìºì‹œ ê²½ë¡œë“¤
        home = Path.home()
        cache_paths = [
            home / ".cache" / "huggingface" / "hub",
            home / ".cache" / "huggingface" / "transformers",
            home / ".cache" / "torch" / "hub",
            home / ".cache" / "torch" / "checkpoints",
            home / ".cache" / "models",
            home / ".torch" / "models",
        ]
        
        # 3. conda í™˜ê²½ ê²½ë¡œë“¤
        conda_paths = []
        conda_env = os.environ.get('CONDA_DEFAULT_ENV')
        if conda_env:
            conda_base = os.environ.get('CONDA_PREFIX', home / "anaconda3")
            conda_paths.extend([
                Path(conda_base) / "envs" / conda_env / "lib" / "python3.10" / "site-packages",
                Path(conda_base) / "pkgs",
            ])
        
        # 4. ì¼ë°˜ì ì¸ ë‹¤ìš´ë¡œë“œ ê²½ë¡œë“¤
        download_paths = [
            home / "Downloads",
            home / "Desktop",
            home / "Documents" / "AI_Models",
            home / "Documents" / "models",
        ]
        
        # ëª¨ë“  ê²½ë¡œ ë³‘í•©
        all_paths = project_paths + cache_paths + conda_paths + download_paths
        
        # ì¡´ì¬í•˜ê³  ì ‘ê·¼ ê°€ëŠ¥í•œ ê²½ë¡œë§Œ í•„í„°ë§
        valid_paths = []
        for path in all_paths:
            try:
                if path.exists() and path.is_dir() and os.access(path, os.R_OK):
                    valid_paths.append(path)
            except (OSError, PermissionError):
                continue
                
        return valid_paths
    
    def _is_ai_model_file(self, file_path: Path) -> Tuple[bool, float]:
        """AI ëª¨ë¸ íŒŒì¼ì¸ì§€ íŒë‹¨ (ì‹ ë¢°ë„ í¬í•¨)"""
        try:
            file_name = file_path.name.lower()
            file_stem = file_path.stem.lower()
            path_str = str(file_path).lower()
            
            # í™•ì¥ì í™•ì¸
            if file_path.suffix.lower() not in self.model_extensions:
                return False, 0.0
            
            confidence = 0.1  # ê¸°ë³¸ í™•ì¥ì ì ìˆ˜
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            keyword_matches = 0
            for keyword in self.ai_keywords:
                if keyword in file_name or keyword in path_str:
                    keyword_matches += 1
                    confidence += 0.15
            
            # íŒŒì¼ í¬ê¸° ê³ ë ¤ (AI ëª¨ë¸ì€ ë³´í†µ 1MB ì´ìƒ)
            try:
                size_mb = file_path.stat().st_size / (1024 * 1024)
                if size_mb >= 1.0:
                    confidence += 0.2
                if size_mb >= 10.0:
                    confidence += 0.2
                if size_mb >= 100.0:
                    confidence += 0.3
            except OSError:
                pass
            
            # ê²½ë¡œ ê¸°ë°˜ ì¶”ê°€ ì ìˆ˜
            path_keywords = ['models', 'checkpoints', 'weights', 'pretrained', 'hub']
            for path_keyword in path_keywords:
                if path_keyword in path_str:
                    confidence += 0.1
            
            # íŠ¹ë³„í•œ íŒŒì¼ëª… íŒ¨í„´
            special_patterns = [
                r'.*model.*\.(pth|pt|bin)$',
                r'.*checkpoint.*\.(pth|ckpt)$', 
                r'.*weights.*\.(pth|h5)$',
                r'pytorch_model\.bin$',
                r'.*diffusion.*\.(pth|safetensors)$'
            ]
            
            for pattern in special_patterns:
                if re.match(pattern, file_name):
                    confidence += 0.3
                    break
            
            # ìµœì¢… íŒë‹¨ (ì‹ ë¢°ë„ 0.3 ì´ìƒì´ë©´ AI ëª¨ë¸ë¡œ ê°„ì£¼)
            is_ai_model = confidence >= 0.3
            return is_ai_model, min(confidence, 1.0)
            
        except Exception:
            return False, 0.0
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚° (ì¤‘ë³µ íƒì§€ìš©)"""
        try:
            hash_sha256 = hashlib.sha256()
            with open(file_path, 'rb') as f:
                # í° íŒŒì¼ì„ ìœ„í•´ ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸°
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()[:16]  # ì²˜ìŒ 16ìë¦¬ë§Œ ì‚¬ìš©
        except Exception as e:
            print(f"âš ï¸ ì²´í¬ì„¬ ê³„ì‚° ì‹¤íŒ¨ {file_path}: {e}")
            return ""
    
    def _detect_model_type(self, file_path: Path) -> Tuple[str, str]:
        """ëª¨ë¸ íƒ€ì…ê³¼ í”„ë ˆì„ì›Œí¬ ì¶”ì •"""
        file_name = file_path.name.lower()
        path_str = str(file_path).lower()
        
        # í”„ë ˆì„ì›Œí¬ íƒì§€
        framework = "unknown"
        if any(keyword in path_str for keyword in ['pytorch', 'torch', '.pth', '.pt']):
            framework = "pytorch"
        elif any(keyword in path_str for keyword in ['tensorflow', '.pb', '.h5']):
            framework = "tensorflow"
        elif '.onnx' in file_name:
            framework = "onnx"
        elif '.safetensors' in file_name:
            framework = "safetensors"
        
        # ëª¨ë¸ íƒ€ì… íƒì§€
        model_type = "unknown"
        if any(keyword in file_name for keyword in ['parsing', 'schp', 'atr', 'graphonomy']):
            model_type = "human_parsing"
        elif any(keyword in file_name for keyword in ['pose', 'openpose', 'yolo.*pose']):
            model_type = "pose_estimation"
        elif any(keyword in file_name for keyword in ['u2net', 'segmentation', 'sam']):
            model_type = "segmentation"
        elif any(keyword in file_name for keyword in ['diffusion', 'stable', 'ootd', 'viton']):
            model_type = "virtual_fitting"
        elif any(keyword in file_name for keyword in ['clip', 'vit', 'bert', 'gpt']):
            model_type = "foundation_model"
        elif any(keyword in file_name for keyword in ['resnet', 'mobilenet', 'efficientnet']):
            model_type = "backbone"
        elif any(keyword in file_name for keyword in ['esrgan', 'enhancement']):
            model_type = "post_processing"
        
        return model_type, framework
    
    def search_models(self) -> List[ModelFile]:
        """ëª¨ë“  ê²½ë¡œì—ì„œ AI ëª¨ë¸ ê²€ìƒ‰"""
        print("ğŸ” AI ëª¨ë¸ ê²€ìƒ‰ ì‹œì‘...")
        
        all_files = []
        
        # ê° ê²½ë¡œì—ì„œ íŒŒì¼ ìˆ˜ì§‘
        for search_path in self.search_paths:
            print(f"ğŸ“‚ ê²€ìƒ‰ ì¤‘: {search_path}")
            try:
                for file_path in search_path.rglob("*"):
                    if file_path.is_file():
                        all_files.append(file_path)
            except (PermissionError, OSError) as e:
                print(f"âš ï¸ ì ‘ê·¼ ë¶ˆê°€: {search_path} - {e}")
                continue
        
        print(f"ğŸ“Š ì´ {len(all_files)}ê°œ íŒŒì¼ ë°œê²¬, AI ëª¨ë¸ í•„í„°ë§ ì¤‘...")
        
        # AI ëª¨ë¸ íŒŒì¼ í•„í„°ë§
        iterator = tqdm(all_files, desc="AI ëª¨ë¸ ë¶„ì„") if TQDM_AVAILABLE else all_files
        
        for file_path in iterator:
            try:
                is_ai, confidence = self._is_ai_model_file(file_path)
                if not is_ai:
                    continue
                
                # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
                stat = file_path.stat()
                size_mb = stat.st_size / (1024 * 1024)
                
                # ì²´í¬ì„¬ ê³„ì‚° (1GB ë¯¸ë§Œ íŒŒì¼ë§Œ)
                checksum = ""
                if size_mb < 1024:  # 1GB ë¯¸ë§Œ
                    checksum = self._calculate_checksum(file_path)
                
                model_type, framework = self._detect_model_type(file_path)
                
                model_file = ModelFile(
                    path=file_path,
                    name=file_path.name,
                    size_mb=size_mb,
                    extension=file_path.suffix.lower(),
                    checksum=checksum,
                    model_type=model_type,
                    framework=framework,
                    confidence=confidence,
                    metadata={
                        'modified_time': stat.st_mtime,
                        'relative_path': str(file_path.relative_to(file_path.anchor)),
                    }
                )
                
                self.discovered_models.append(model_file)
                
            except (OSError, PermissionError):
                continue
        
        print(f"âœ… {len(self.discovered_models)}ê°œ AI ëª¨ë¸ ë°œê²¬!")
        return self.discovered_models
    
    def find_duplicates(self) -> Dict[str, List[ModelFile]]:
        """ì¤‘ë³µ íŒŒì¼ ì°¾ê¸° (ì²´í¬ì„¬ ê¸°ë°˜)"""
        print("ğŸ” ì¤‘ë³µ íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        
        checksum_groups = defaultdict(list)
        
        for model in self.discovered_models:
            if model.checksum:  # ì²´í¬ì„¬ì´ ìˆëŠ” íŒŒì¼ë§Œ
                checksum_groups[model.checksum].append(model)
        
        # ì¤‘ë³µì´ ìˆëŠ” ê·¸ë£¹ë§Œ ì„ ë³„
        duplicates = {k: v for k, v in checksum_groups.items() if len(v) > 1}
        
        print(f"ğŸ“Š ì¤‘ë³µ ê·¸ë£¹: {len(duplicates)}ê°œ")
        for checksum, files in duplicates.items():
            print(f"   ì²´í¬ì„¬ {checksum}: {len(files)}ê°œ íŒŒì¼")
            for file in files:
                print(f"     - {file.path} ({file.size_mb:.1f}MB)")
        
        self.duplicate_groups = duplicates
        return duplicates
    
    def generate_report(self, output_file: Optional[Path] = None) -> Dict:
        """ê²€ìƒ‰ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            "scan_info": {
                "timestamp": time.time(),
                "search_paths": [str(p) for p in self.search_paths],
                "total_files_found": len(self.discovered_models),
                "total_size_gb": sum(m.size_mb for m in self.discovered_models) / 1024,
                "duplicate_groups": len(self.duplicate_groups)
            },
            "models_by_type": defaultdict(list),
            "models_by_framework": defaultdict(list),
            "models_by_size": {"small": [], "medium": [], "large": []},
            "duplicates": {},
            "all_models": []
        }
        
        # ëª¨ë¸ ë¶„ë¥˜
        for model in self.discovered_models:
            # íƒ€ì…ë³„
            report["models_by_type"][model.model_type].append({
                "path": str(model.path),
                "name": model.name,
                "size_mb": model.size_mb,
                "confidence": model.confidence
            })
            
            # í”„ë ˆì„ì›Œí¬ë³„
            report["models_by_framework"][model.framework].append({
                "path": str(model.path),
                "name": model.name,
                "size_mb": model.size_mb
            })
            
            # í¬ê¸°ë³„
            if model.size_mb < 100:
                category = "small"
            elif model.size_mb < 1000:
                category = "medium"  
            else:
                category = "large"
            
            report["models_by_size"][category].append({
                "path": str(model.path),
                "name": model.name,
                "size_mb": model.size_mb
            })
            
            # ì „ì²´ ëª©ë¡
            report["all_models"].append({
                "path": str(model.path),
                "name": model.name,
                "size_mb": model.size_mb,
                "type": model.model_type,
                "framework": model.framework,
                "checksum": model.checksum,
                "confidence": model.confidence
            })
        
        # ì¤‘ë³µ íŒŒì¼ ì •ë³´
        for checksum, files in self.duplicate_groups.items():
            report["duplicates"][checksum] = [
                {
                    "path": str(f.path),
                    "name": f.name,
                    "size_mb": f.size_mb
                } for f in files
            ]
        
        # íŒŒì¼ ì €ì¥
        if output_file:
            output_file.parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")
        
        return report
    
    def create_relocation_plan(self, copy_mode: bool = True) -> Dict:
        """ì¬ë°°ì¹˜ ê³„íš ìƒì„± - Stepë³„ êµ¬ì¡°ë¡œ ê°œì„ """
        print("ğŸ“‹ ì¬ë°°ì¹˜ ê³„íš ìƒì„± ì¤‘...")
        
        # Stepë³„ íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ êµ¬ì¡° (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜)
        base_checkpoints = self.target_dir / "app" / "ai_pipeline" / "models" / "checkpoints"
        target_structure = {
            "step_01_human_parsing": base_checkpoints / "step_01_human_parsing",
            "step_02_pose_estimation": base_checkpoints / "step_02_pose_estimation", 
            "step_03_cloth_segmentation": base_checkpoints / "step_03_cloth_segmentation",
            "step_04_geometric_matching": base_checkpoints / "step_04_geometric_matching",
            "step_05_cloth_warping": base_checkpoints / "step_05_cloth_warping",
            "step_06_virtual_fitting": base_checkpoints / "step_06_virtual_fitting",
            "step_07_post_processing": base_checkpoints / "step_07_post_processing", 
            "step_08_quality_assessment": base_checkpoints / "step_08_quality_assessment",
            "misc": base_checkpoints / "misc"
        }
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for dir_path in target_structure.values():
            dir_path.mkdir(parents=True, exist_ok=True)
        
        relocation_plan = {
            "copy_mode": copy_mode,
            "target_structure": {k: str(v) for k, v in target_structure.items()},
            "operations": [],
            "conflicts": [],
            "summary": {
                "total_files": len(self.discovered_models),
                "total_size_gb": sum(m.size_mb for m in self.discovered_models) / 1024,
                "operations_count": 0
            }
        }
        
        # íŒŒì¼ëª… ì¶©ëŒ ë°©ì§€ìš© ì¹´ìš´í„° (Stepë³„ë¡œ ê´€ë¦¬)
        filename_counters = defaultdict(lambda: defaultdict(int))
        
        for model in self.discovered_models:
            # Stepë³„ íƒ€ê²Ÿ í´ë” ê²°ì • (ëª¨ë¸ íƒ€ì…ê³¼ ê²½ë¡œ ë¶„ì„)
            target_folder = self._determine_step_folder(model, target_structure)
            
            # íƒ€ê²Ÿ íŒŒì¼ëª… ìƒì„± (ì¤‘ë³µ ì‹œ ìˆœì„œ ë²ˆí˜¸ ì¶”ê°€)
            base_name = model.path.stem
            extension = model.path.suffix
            folder_name = target_folder.name
            
            filename_counters[folder_name][model.name] += 1
            if filename_counters[folder_name][model.name] > 1:
                target_name = f"{base_name}_{filename_counters[folder_name][model.name]:02d}{extension}"
            else:
                target_name = model.name
            
            target_path = target_folder / target_name
            
            operation = {
                "source": str(model.path),
                "target": str(target_path),
                "method": "copy" if copy_mode else "move",
                "size_mb": model.size_mb,
                "model_type": model.model_type,
                "framework": model.framework,
                "reason": f"Size: {model.size_mb:.1f}MB, Type: {model.model_type}"
            }
            
            # ì¶©ëŒ í™•ì¸
            if target_path.exists():
                operation["conflict"] = True
                relocation_plan["conflicts"].append(operation)
            else:
                operation["conflict"] = False
                relocation_plan["operations"].append(operation)
        
        relocation_plan["summary"]["operations_count"] = len(relocation_plan["operations"])
        
        return relocation_plan
    
    def _determine_step_folder(self, model: ModelFile, target_structure: Dict[str, Path]) -> Path:
        """ëª¨ë¸ íƒ€ì…ê³¼ íŒŒì¼ëª… ë¶„ì„í•˜ì—¬ ì˜¬ë°”ë¥¸ Step í´ë” ê²°ì •"""
        file_name = model.path.name.lower()
        path_str = str(model.path).lower()
        
        # Step 01: Human Parsing
        if (model.model_type == "human_parsing" or 
            any(keyword in file_name for keyword in [
                'parsing', 'schp', 'atr', 'lip', 'graphonomy', 'densepose', 
                'exp-schp-201908301523-atr', 'exp-schp-201908261155-lip',
                'segformer_b2_clothes', 'humanparsing'
            ]) or
            any(keyword in path_str for keyword in [
                'human_parsing', 'step_01', 'step_1'
            ])):
            return target_structure["step_01_human_parsing"]
        
        # Step 02: Pose Estimation  
        elif (model.model_type == "pose_estimation" or
              any(keyword in file_name for keyword in [
                  'pose', 'openpose', 'body_pose_model', 'yolov8n-pose',
                  'pose_landmark', 'mediapipe', 'pose_deploy'
              ]) or
              any(keyword in path_str for keyword in [
                  'pose_estimation', 'openpose', 'step_02', 'step_2'
              ])):
            return target_structure["step_02_pose_estimation"]
        
        # Step 03: Cloth Segmentation
        elif (model.model_type == "segmentation" or
              any(keyword in file_name for keyword in [
                  'u2net', 'mobile_sam', 'sam_vit', 'cloth_segmentation',
                  'background_removal', 'segmentation'
              ]) or
              any(keyword in path_str for keyword in [
                  'cloth_segmentation', 'step_03', 'step_3', 'u2net'
              ])):
            return target_structure["step_03_cloth_segmentation"]
        
        # Step 04: Geometric Matching
        elif (any(keyword in file_name for keyword in [
                'gmm', 'geometric', 'matching', 'tps_network', 
                'geometric_matching', 'lightweight_gmm'
              ]) or
              any(keyword in path_str for keyword in [
                  'geometric_matching', 'step_04', 'step_4'
              ])):
            return target_structure["step_04_geometric_matching"]
        
        # Step 05: Cloth Warping
        elif (any(keyword in file_name for keyword in [
                'warping', 'cloth_warping', 'tom_final', 'tps'
              ]) or
              any(keyword in path_str for keyword in [
                  'cloth_warping', 'step_05', 'step_5'
              ])):
            return target_structure["step_05_cloth_warping"]
        
        # Step 06: Virtual Fitting
        elif (model.model_type == "virtual_fitting" or
              any(keyword in file_name for keyword in [
                  'ootd', 'diffusion', 'vton', 'viton', 'unet_vton',
                  'text_encoder', 'vae', 'stable_diffusion', 'ootdiffusion'
              ]) or
              any(keyword in path_str for keyword in [
                  'virtual_fitting', 'ootdiffusion', 'step_06', 'step_6',
                  'stable-diffusion'
              ])):
            return target_structure["step_06_virtual_fitting"]
        
        # Step 07: Post Processing
        elif (model.model_type == "post_processing" or
              any(keyword in file_name for keyword in [
                  'esrgan', 'realesrgan', 'gfpgan', 'codeformer',
                  'enhancement', 'super_resolution', 'post_processing'
              ]) or
              any(keyword in path_str for keyword in [
                  'post_processing', 'step_07', 'step_7'
              ])):
            return target_structure["step_07_post_processing"]
        
        # Step 08: Quality Assessment
        elif (any(keyword in file_name for keyword in [
                'clip', 'quality', 'assessment', 'vit_base', 'vit_large'
              ]) or
              any(keyword in path_str for keyword in [
                  'quality_assessment', 'step_08', 'step_8', 'clip-vit'
              ])):
            return target_structure["step_08_quality_assessment"]
        
        # ê¸°íƒ€ ë¶„ë¥˜ë˜ì§€ ì•Šì€ íŒŒì¼ë“¤
        else:
            return target_structure["misc"]
    
    def execute_relocation(self, plan: Dict, dry_run: bool = True) -> bool:
        """ì¬ë°°ì¹˜ ì‹¤í–‰"""
        if dry_run:
            print("ğŸ” DRY RUN - ì‹¤ì œ íŒŒì¼ ì´ë™ ì—†ìŒ")
        else:
            print("ğŸš€ ì‹¤ì œ ì¬ë°°ì¹˜ ì‹œì‘...")
        
        operations = plan["operations"]
        iterator = tqdm(operations, desc="íŒŒì¼ ì¬ë°°ì¹˜") if TQDM_AVAILABLE else operations
        
        success_count = 0
        error_count = 0
        
        for operation in iterator:
            source = Path(operation["source"])
            target = Path(operation["target"])
            method = operation["method"]
            
            if dry_run:
                print(f"{'COPY' if method == 'copy' else 'MOVE'}: {source} â†’ {target}")
                success_count += 1
                continue
            
            try:
                # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ìƒì„±
                target.parent.mkdir(parents=True, exist_ok=True)
                
                if method == "copy":
                    shutil.copy2(source, target)
                    print(f"âœ… ë³µì‚¬ ì™„ë£Œ: {target.name}")
                else:
                    shutil.move(str(source), str(target))
                    print(f"âœ… ì´ë™ ì™„ë£Œ: {target.name}")
                
                success_count += 1
                
            except Exception as e:
                print(f"âŒ ì‹¤íŒ¨: {source} â†’ {target}: {e}")
                error_count += 1
        
        print(f"\nğŸ“Š ì¬ë°°ì¹˜ ê²°ê³¼:")
        print(f"   ì„±ê³µ: {success_count}ê°œ")
        print(f"   ì‹¤íŒ¨: {error_count}ê°œ")
        
        return error_count == 0

def main():
    parser = argparse.ArgumentParser(description="AI ëª¨ë¸ ê²€ìƒ‰ ë° ì¬ë°°ì¹˜ ë„êµ¬")
    parser.add_argument("--scan-only", action="store_true", help="ê²€ìƒ‰ë§Œ ìˆ˜í–‰")
    parser.add_argument("--relocate", action="store_true", help="ê²€ìƒ‰ í›„ ì¬ë°°ì¹˜")
    parser.add_argument("--target-dir", type=Path, default=Path.cwd() / "backend", help="íƒ€ê²Ÿ ë””ë ‰í† ë¦¬")
    parser.add_argument("--dry-run", action="store_true", help="ì‹¤ì œ ì´ë™ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜")
    parser.add_argument("--output", type=Path, default="model_search_report.json", help="ë¦¬í¬íŠ¸ íŒŒì¼ëª…")
    
    args = parser.parse_args()
    
    print("ğŸ¤– MyCloset AI ëª¨ë¸ ì¬ë°°ì¹˜ ë„êµ¬ v2.0")
    print("=" * 50)
    print(f"ğŸ¯ íƒ€ê²Ÿ: backend/app/ai_pipeline/models/checkpoints/")
    
    # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
    searcher = ModelSearcher(target_dir=args.target_dir)
    
    # ëª¨ë¸ ê²€ìƒ‰
    models = searcher.search_models()
    if not models:
        print("âŒ AI ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¤‘ë³µ ì°¾ê¸°
    duplicates = searcher.find_duplicates()
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report = searcher.generate_report(output_file=args.output)
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:")
    print(f"   ì´ ëª¨ë¸ ìˆ˜: {len(models)}ê°œ")
    print(f"   ì´ í¬ê¸°: {sum(m.size_mb for m in models) / 1024:.1f}GB")
    print(f"   ì¤‘ë³µ ê·¸ë£¹: {len(duplicates)}ê°œ")
    
    print("\nğŸ·ï¸ ëª¨ë¸ íƒ€ì…ë³„ ë¶„í¬:")
    type_counts = defaultdict(int)
    for model in models:
        type_counts[model.model_type] += 1
    for model_type, count in sorted(type_counts.items()):
        print(f"   {model_type}: {count}ê°œ")
    
    print("\nğŸ“ Stepë³„ ì˜ˆìƒ ë°°ì¹˜:")
    step_preview = defaultdict(int)
    target_structure = {
        "step_01_human_parsing": Path("step_01_human_parsing"),
        "step_02_pose_estimation": Path("step_02_pose_estimation"),
        "step_03_cloth_segmentation": Path("step_03_cloth_segmentation"),
        "step_04_geometric_matching": Path("step_04_geometric_matching"),
        "step_05_cloth_warping": Path("step_05_cloth_warping"),
        "step_06_virtual_fitting": Path("step_06_virtual_fitting"),
        "step_07_post_processing": Path("step_07_post_processing"),
        "step_08_quality_assessment": Path("step_08_quality_assessment"),
        "misc": Path("misc")
    }
    
    for model in models:
        step_folder = searcher._determine_step_folder(model, target_structure)
        step_preview[step_folder.name] += 1
    
    for step, count in sorted(step_preview.items()):
        print(f"   {step}: {count}ê°œ")
    
    # ì¬ë°°ì¹˜ ìˆ˜í–‰ (ìš”ì²­ëœ ê²½ìš°)
    if args.relocate or not args.scan_only:
        print("\n" + "=" * 50)
        plan = searcher.create_relocation_plan(copy_mode=True)  # í•­ìƒ ë³µì‚¬ ëª¨ë“œ
        
        print(f"ğŸ“‹ ì¬ë°°ì¹˜ ê³„íš:")
        print(f"   ì´ë™í•  íŒŒì¼: {len(plan['operations'])}ê°œ")
        print(f"   ì¶©ëŒ íŒŒì¼: {len(plan['conflicts'])}ê°œ")
        
        if plan['conflicts']:
            print("\nâš ï¸ ì¶©ëŒ íŒŒì¼ë“¤ (ì¼ë¶€ í‘œì‹œ):")
            for conflict in plan['conflicts'][:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                print(f"   {conflict['target']}")
        
        # ì‚¬ìš©ì í™•ì¸
        if not args.dry_run:
            confirm = input("\nì¬ë°°ì¹˜ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? [y/N]: ")
            if confirm.lower() != 'y':
                print("âŒ ì¬ë°°ì¹˜ ì·¨ì†Œ")
                return
        
        # ì¬ë°°ì¹˜ ì‹¤í–‰
        success = searcher.execute_relocation(plan, dry_run=args.dry_run)
        
        if success:
            print("âœ… ì¬ë°°ì¹˜ ì™„ë£Œ!")
            print(f"\nğŸ“ ì •ë¦¬ëœ êµ¬ì¡°:")
            print(f"   backend/app/ai_pipeline/models/checkpoints/")
            for step in sorted(step_preview.keys()):
                if step_preview[step] > 0:
                    print(f"   â”œâ”€â”€ {step}/ ({step_preview[step]}ê°œ)")
        else:
            print("âš ï¸ ì¼ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()