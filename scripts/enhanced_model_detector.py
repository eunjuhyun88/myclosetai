#!/usr/bin/env python3
"""
ğŸ”§ MyCloset AI - ëª¨ë¸ íƒì§€ê¸° íŒ¨ì¹˜ v2.0
âœ… ì‹¤ì œ íŒŒì¼ êµ¬ì¡°ì— ë§ëŠ” ê²½ë¡œ ì„¤ì •
âœ… 229GB ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš©
"""

import logging
from pathlib import Path
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

# ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜ ê²€ìƒ‰ ê²½ë¡œ
ENHANCED_SEARCH_PATHS = ['/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_01_human_parsing', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_02_pose_estimation', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_03_cloth_segmentation', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_04_geometric_matching', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_05_cloth_warping', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_07_post_processing', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_08_quality_assessment', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_01_human_parsing', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_02_pose_estimation', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_03_cloth_segmentation', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_04_geometric_matching', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_05_cloth_warping', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_06_virtual_fitting', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_07_post_processing', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_08_quality_assessment', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/organized/step_01_human_parsing', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/organized/step_02_pose_estimation', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/organized/step_03_cloth_segmentation', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/organized/step_04_geometric_matching', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/organized/step_05_cloth_warping', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/organized/step_06_virtual_fitting', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/organized/step_07_post_processing', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/organized/step_08_quality_assessment', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/ai_models2/step_01_human_parsing', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/ai_models2/step_02_pose_estimation', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/ai_models2/step_03_cloth_segmentation', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/ai_models2/step_04_geometric_matching', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/ai_models2/step_05_cloth_warping', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/ai_models2/step_06_virtual_fitting', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/ai_models2/step_07_post_processing', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/ai_models2/step_08_quality_assessment', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/Graphonomy', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/openpose', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/OOTDiffusion', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/HR-VITON', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/u2net', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/clip_vit_large', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/idm_vton', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/fashion_clip', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/sam2_large', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/huggingface_cache', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/cache/huggingface', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/auxiliary_models', '/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/individual_models']

class EnhancedModelDetector:
    """í–¥ìƒëœ ëª¨ë¸ íƒì§€ê¸°"""
    
    def __init__(self):
        self.search_paths = [Path(p) for p in ENHANCED_SEARCH_PATHS if Path(p).exists()]
        logger.info(f"ğŸ” ìœ íš¨í•œ ê²€ìƒ‰ ê²½ë¡œ: {len(self.search_paths)}ê°œ")
        
    def scan_all_models(self) -> Dict[str, List[Dict[str, Any]]]:
        """ëª¨ë“  ëª¨ë¸ ìŠ¤ìº”"""
        detected_models = {}
        
        for search_path in self.search_paths:
            try:
                models = self._scan_directory(search_path)
                
                # Step ì´ë¦„ ì¶”ì¶œ
                step_name = self._extract_step_name(search_path)
                if step_name not in detected_models:
                    detected_models[step_name] = []
                
                detected_models[step_name].extend(models)
                
                if models:
                    logger.info(f"ğŸ“ {search_path.name}: {len(models)}ê°œ ëª¨ë¸ ë°œê²¬")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ìŠ¤ìº” ì‹¤íŒ¨ {search_path}: {e}")
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        for step_name in detected_models:
            detected_models[step_name] = self._deduplicate_models(detected_models[step_name])
        
        return detected_models
    
    def _scan_directory(self, directory: Path) -> List[Dict[str, Any]]:
        """ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
        models = []
        
        # ëª¨ë¸ íŒŒì¼ í™•ì¥ì
        model_extensions = {'.pth', '.pt', '.bin', '.safetensors', '.onnx', '.pkl'}
        
        try:
            # í•˜ìœ„ ë””ë ‰í† ë¦¬ê¹Œì§€ ì¬ê·€ì ìœ¼ë¡œ ìŠ¤ìº”
            for file_path in directory.rglob("*"):
                if file_path.is_file() and file_path.suffix.lower() in model_extensions:
                    try:
                        stat = file_path.stat()
                        size_mb = stat.st_size / (1024 * 1024)
                        
                        # ìµœì†Œ í¬ê¸° í•„í„° (0.1MB ì´ìƒ)
                        if size_mb >= 0.1:
                            model_info = {
                                'file_path': str(file_path),
                                'file_name': file_path.name,
                                'size_mb': round(size_mb, 1),
                                'last_modified': stat.st_mtime,
                                'confidence': self._calculate_confidence(file_path.name, size_mb),
                                'relative_path': str(file_path.relative_to(directory))
                            }
                            models.append(model_info)
                            
                    except Exception as e:
                        logger.debug(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
                        
        except Exception as e:
            logger.warning(f"ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì‹¤íŒ¨ {directory}: {e}")
        
        # í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬ (í° íŒŒì¼ ìš°ì„ )
        models.sort(key=lambda x: x['size_mb'], reverse=True)
        
        return models
    
    def _extract_step_name(self, path: Path) -> str:
        """ê²½ë¡œì—ì„œ Step ì´ë¦„ ì¶”ì¶œ"""
        path_str = str(path).lower()
        
        # Step íŒ¨í„´ ë§¤ì¹­
        step_patterns = {
            'human_parsing': ['step_01', 'human_parsing', 'graphonomy', 'schp'],
            'pose_estimation': ['step_02', 'pose_estimation', 'openpose', 'hrnet'],
            'cloth_segmentation': ['step_03', 'cloth_segmentation', 'u2net', 'rembg'],
            'geometric_matching': ['step_04', 'geometric_matching', 'gmm', 'tps'],
            'cloth_warping': ['step_05', 'cloth_warping', 'warping'],
            'virtual_fitting': ['step_06', 'virtual_fitting', 'viton', 'ootd', 'diffusion'],
            'post_processing': ['step_07', 'post_processing', 'enhancement', 'super_resolution'],
            'quality_assessment': ['step_08', 'quality_assessment', 'clip', 'aesthetic']
        }
        
        for step_name, keywords in step_patterns.items():
            if any(keyword in path_str for keyword in keywords):
                return step_name
        
        return 'unknown'
    
    def _calculate_confidence(self, filename: str, size_mb: float) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.0
        filename_lower = filename.lower()
        
        # íŒŒì¼ëª… ê¸°ë°˜ ì ìˆ˜
        high_confidence_keywords = ['schp', 'openpose', 'u2net', 'viton', 'ootd', 'clip']
        if any(keyword in filename_lower for keyword in high_confidence_keywords):
            confidence += 0.6
        
        medium_confidence_keywords = ['model', 'checkpoint', 'pytorch', 'final', 'best']
        if any(keyword in filename_lower for keyword in medium_confidence_keywords):
            confidence += 0.3
        
        # í¬ê¸° ê¸°ë°˜ ì ìˆ˜
        if 10 <= size_mb <= 5000:  # 10MB ~ 5GB (ì ì • ë²”ìœ„)
            confidence += 0.4
        elif size_mb > 5000:  # 5GB ì´ìƒ (ëŒ€ìš©ëŸ‰ ëª¨ë¸)
            confidence += 0.2
        elif size_mb >= 1:  # 1MB ì´ìƒ
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _deduplicate_models(self, models: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ëª¨ë¸ ì œê±°"""
        seen_names = set()
        unique_models = []
        
        for model in models:
            name = model['file_name']
            if name not in seen_names:
                seen_names.add(name)
                unique_models.append(model)
        
        return unique_models

# ì „ì—­ íƒì§€ê¸° ì¸ìŠ¤í„´ìŠ¤
enhanced_detector = EnhancedModelDetector()

def scan_enhanced_models() -> Dict[str, List[Dict[str, Any]]]:
    """í–¥ìƒëœ ëª¨ë¸ ìŠ¤ìº” ì‹¤í–‰"""
    return enhanced_detector.scan_all_models()

def patch_existing_detector(detector_instance):
    """ê¸°ì¡´ íƒì§€ê¸° íŒ¨ì¹˜"""
    if hasattr(detector_instance, 'search_paths'):
        detector_instance.search_paths = enhanced_detector.search_paths
        logger.info("âœ… ê¸°ì¡´ íƒì§€ê¸° ê²€ìƒ‰ ê²½ë¡œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    return detector_instance
