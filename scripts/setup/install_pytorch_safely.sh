#!/bin/bash

echo "ğŸ”¥ PyTorch ì•ˆì „ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ (Conda í™˜ê²½)"
echo "============================================"

# 1. í˜„ì¬ í™˜ê²½ í™•ì¸
echo "ğŸ” 1. í˜„ì¬ í™˜ê²½ í™•ì¸..."
echo "Conda í™˜ê²½: ${CONDA_DEFAULT_ENV:-'ì—†ìŒ'}"
echo "Python ë²„ì „: $(python --version 2>/dev/null || echo 'í™•ì¸ë¶ˆê°€')"
echo "ì‹œìŠ¤í…œ: $(uname -sm)"

if [[ "$CONDA_DEFAULT_ENV" == "" ]]; then
    echo "âŒ Conda í™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "conda activate mycloset"
    exit 1
fi

# 2. ê¸°ì¡´ PyTorch ì™„ì „ ì œê±° (ì¶©ëŒ ë°©ì§€)
echo "ğŸ—‘ï¸ 2. ê¸°ì¡´ PyTorch ì™„ì „ ì œê±° ì¤‘..."
conda remove pytorch torchvision torchaudio pytorch-cuda cpuonly -y --force 2>/dev/null || true
pip uninstall torch torchvision torchaudio -y 2>/dev/null || true

# Apple Silicon ê°ì§€
ARCH=$(uname -m)
SYSTEM=$(uname -s)

if [[ "$ARCH" == "arm64" && "$SYSTEM" == "Darwin" ]]; then
    echo "ğŸ Apple Silicon (M1/M2/M3) ê°ì§€ë¨"
    IS_APPLE_SILICON=true
else
    echo "ğŸ–¥ï¸ Intel/AMD ì‹œìŠ¤í…œ ê°ì§€ë¨"
    IS_APPLE_SILICON=false
fi

# 3. NumPy í˜¸í™˜ì„± ë¨¼ì € í•´ê²° (ê°€ì¥ ì¤‘ìš”!)
echo "ğŸ“¦ 3. NumPy í˜¸í™˜ì„± í•´ê²° ì¤‘..."
conda install numpy=1.24.3 -y --force-reinstall

# 4. ì‹œìŠ¤í…œë³„ PyTorch ì„¤ì¹˜
echo "ğŸ”¥ 4. PyTorch ì•ˆì „ ì„¤ì¹˜ ì¤‘..."

if [[ "$IS_APPLE_SILICON" == true ]]; then
    echo "ğŸ Apple Siliconìš© PyTorch ì„¤ì¹˜..."
    
    # Apple Siliconìš© ì•ˆì • ë²„ì „ ì„¤ì¹˜
    conda install pytorch=2.0.1 torchvision=0.15.2 -c pytorch -y
    
    # MPS ê´€ë ¨ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    export PYTORCH_ENABLE_MPS_FALLBACK=1
    export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
    
else
    # Intel/AMDìš© PyTorch ì„¤ì¹˜
    echo "ğŸ–¥ï¸ Intel/AMDìš© PyTorch ì„¤ì¹˜..."
    
    # CUDA í™•ì¸
    if command -v nvidia-smi &> /dev/null; then
        echo "ğŸ”¥ NVIDIA GPU ê°ì§€ë¨ - CUDA ë²„ì „ ì„¤ì¹˜"
        conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia -y
    else
        echo "ğŸ’» CPU ë²„ì „ ì„¤ì¹˜"
        conda install pytorch torchvision cpuonly -c pytorch -y
    fi
fi

# 5. ì•ˆì •ì„±ì„ ìœ„í•œ ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
echo "ğŸ“¦ 5. ì•ˆì •ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
conda install -c conda-forge \
    pillow=10.0.0 \
    scipy=1.10.1 \
    scikit-image=0.21.0 \
    opencv=4.8.0 \
    -y

# 6. í™˜ê²½ë³€ìˆ˜ ì„¤ì • (Segfault ë°©ì§€)
echo "âš™ï¸ 6. ì•ˆì •ì„± í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì¤‘..."

# í˜„ì¬ ì„¸ì…˜ìš©
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4
export OPENBLAS_NUM_THREADS=4
export VECLIB_MAXIMUM_THREADS=4
export NUMEXPR_NUM_THREADS=4

if [[ "$IS_APPLE_SILICON" == true ]]; then
    export PYTORCH_ENABLE_MPS_FALLBACK=1
    export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
fi

# conda í™˜ê²½ì— ì˜êµ¬ ì„¤ì •
CONDA_ENV_PATH=$CONDA_PREFIX/etc/conda/activate.d
mkdir -p $CONDA_ENV_PATH

cat > $CONDA_ENV_PATH/pytorch_env.sh << 'EOF'
# PyTorch ì•ˆì •ì„± í™˜ê²½ë³€ìˆ˜
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4
export OPENBLAS_NUM_THREADS=4
export VECLIB_MAXIMUM_THREADS=4
export NUMEXPR_NUM_THREADS=4
export PYTORCH_ENABLE_MPS_FALLBACK=1
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
EOF

# 7. PyTorch ì„¤ì¹˜ í™•ì¸ ë° í…ŒìŠ¤íŠ¸
echo "ğŸ§ª 7. PyTorch ì„¤ì¹˜ í™•ì¸ ì¤‘..."

cat > test_pytorch_installation.py << 'EOF'
#!/usr/bin/env python3
"""
PyTorch ì„¤ì¹˜ í™•ì¸ ë° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
"""

import sys
import os

def test_pytorch_installation():
    """PyTorch ì„¤ì¹˜ ë° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”¥ PyTorch ì„¤ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    print("=" * 50)
    
    try:
        import torch
        print(f"âœ… PyTorch ë²„ì „: {torch.__version__}")
        
        # ê¸°ë³¸ í…ì„œ ìƒì„± í…ŒìŠ¤íŠ¸
        x = torch.tensor([1.0, 2.0, 3.0])
        print(f"âœ… í…ì„œ ìƒì„± ì„±ê³µ: {x}")
        
        # ê¸°ë³¸ ì—°ì‚° í…ŒìŠ¤íŠ¸
        y = x * 2 + 1
        print(f"âœ… ê¸°ë³¸ ì—°ì‚° ì„±ê³µ: {y}")
        
        # ë””ë°”ì´ìŠ¤ í™•ì¸
        print("\nğŸ–¥ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤:")
        
        # CPU í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
        print("  âœ… CPU: ì‚¬ìš© ê°€ëŠ¥")
        cpu_tensor = torch.randn(3, 3, device='cpu')
        print(f"     CPU í…ì„œ í…ŒìŠ¤íŠ¸: {cpu_tensor.shape}")
        
        # CUDA í™•ì¸
        if torch.cuda.is_available():
            print(f"  âœ… CUDA: ì‚¬ìš© ê°€ëŠ¥ ({torch.cuda.get_device_name()})")
            cuda_tensor = torch.randn(3, 3, device='cuda')
            print(f"     CUDA í…ì„œ í…ŒìŠ¤íŠ¸: {cuda_tensor.shape}")
            recommended_device = "cuda"
        else:
            print("  â„¹ï¸ CUDA: ì‚¬ìš© ë¶ˆê°€")
            
        # MPS (Apple Silicon) í™•ì¸
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print("  âœ… MPS (Apple Silicon): ì‚¬ìš© ê°€ëŠ¥")
            try:
                mps_tensor = torch.randn(3, 3, device='mps')
                print(f"     MPS í…ì„œ í…ŒìŠ¤íŠ¸: {mps_tensor.shape}")
                if 'recommended_device' not in locals():
                    recommended_device = "mps"
            except Exception as e:
                print(f"  âš ï¸ MPS í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                print("     CPU ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤")
                recommended_device = "cpu"
        else:
            print("  â„¹ï¸ MPS: ì‚¬ìš© ë¶ˆê°€")
            
        if 'recommended_device' not in locals():
            recommended_device = "cpu"
            
        print(f"\nğŸ¯ ê¶Œì¥ ë””ë°”ì´ìŠ¤: {recommended_device}")
        
        # ê°„ë‹¨í•œ ì‹ ê²½ë§ í…ŒìŠ¤íŠ¸
        print("\nğŸ§  ì‹ ê²½ë§ í…ŒìŠ¤íŠ¸...")
        model = torch.nn.Linear(3, 2)
        test_input = torch.randn(1, 3)
        
        with torch.no_grad():
            output = model(test_input)
            
        print(f"âœ… ì‹ ê²½ë§ í…ŒìŠ¤íŠ¸ ì„±ê³µ: ì…ë ¥ {test_input.shape} â†’ ì¶œë ¥ {output.shape}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if recommended_device == "cuda":
            print(f"\nğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB")
        elif recommended_device == "mps":
            print("\nğŸ’¾ í†µí•© ë©”ëª¨ë¦¬ ì‚¬ìš© (Apple Silicon)")
        
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print(f"âœ… PyTorchê°€ ì •ìƒì ìœ¼ë¡œ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ¯ MyCloset AIì—ì„œ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {recommended_device}")
        
        return True, recommended_device
        
    except ImportError as e:
        print(f"âŒ PyTorch ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        return False, "none"
    except Exception as e:
        print(f"âŒ PyTorch í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False, "cpu"

def test_ai_dependencies():
    """AI ê´€ë ¨ ì˜ì¡´ì„± í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“¦ AI ì˜ì¡´ì„± í…ŒìŠ¤íŠ¸...")
    
    dependencies = [
        ("numpy", "ë„˜íŒŒì´"),
        ("PIL", "Pillow (ì´ë¯¸ì§€ ì²˜ë¦¬)"),
        ("cv2", "OpenCV (ì»´í“¨í„° ë¹„ì „)"),
        ("scipy", "SciPy (ê³¼í•™ ê³„ì‚°)"),
        ("skimage", "scikit-image (ì´ë¯¸ì§€ ì²˜ë¦¬)")
    ]
    
    for package, description in dependencies:
        try:
            if package == "PIL":
                import PIL
                print(f"  âœ… {description}: {PIL.__version__}")
            elif package == "cv2":
                import cv2
                print(f"  âœ… {description}: {cv2.__version__}")
            elif package == "skimage":
                import skimage
                print(f"  âœ… {description}: {skimage.__version__}")
            else:
                module = __import__(package)
                version = getattr(module, '__version__', 'unknown')
                print(f"  âœ… {description}: {version}")
        except ImportError:
            print(f"  âŒ {description}: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")

if __name__ == "__main__":
    print(f"ğŸ Python: {sys.version}")
    print(f"ğŸ’» í”Œë«í¼: {sys.platform}")
    
    success, device = test_pytorch_installation()
    test_ai_dependencies()
    
    if success:
        print("\n" + "="*50)
        print("ğŸ‰ ì„¤ì¹˜ ì™„ë£Œ! MyCloset AI Backendë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ¯ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {device}")
        print("ğŸš€ ì‹¤í–‰: uvicorn app.main:app --reload --host 0.0.0.0 --port 8000")
    else:
        print("\n" + "="*50)
        print("âŒ PyTorch ì„¤ì¹˜ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        print("ğŸ”§ í•´ê²° ë°©ë²•:")
        print("   conda install pytorch torchvision -c pytorch -y")
        sys.exit(1)
EOF

python test_pytorch_installation.py

if [[ $? -eq 0 ]]; then
    echo ""
    echo "ğŸ‰ PyTorch ì„¤ì¹˜ ì„±ê³µ!"
    
    # 8. ë°±ì—”ë“œ main.pyë¥¼ PyTorch ì§€ì› ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸
    echo "ğŸ”§ 8. ë°±ì—”ë“œë¥¼ PyTorch ì§€ì› ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ì¤‘..."
    
    cat > app/main_with_pytorch.py << 'EOF'
"""
MyCloset AI Backend - PyTorch ì§€ì› ë²„ì „
ì‹¤ì œ AI ê¸°ëŠ¥ì„ í¬í•¨í•œ ì™„ì „í•œ ë°±ì—”ë“œ
"""

import sys
import os
from pathlib import Path
import logging
from datetime import datetime
import platform
import asyncio
import time
import uuid

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import JSONResponse

# ì•ˆì „í•œ ì„¤ì • ë¡œë“œ
try:
    from app.core.config import settings
    CONFIG_LOADED = True
    print("âœ… ì„¤ì • ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
    CONFIG_LOADED = False
    
    # í´ë°± ì„¤ì •
    class FallbackSettings:
        APP_NAME = "MyCloset AI Backend"
        APP_VERSION = "1.0.0"
        DEBUG = True
        CORS_ORIGINS = ["http://localhost:3000", "http://localhost:5173"]
        MAX_UPLOAD_SIZE = 52428800
        ALLOWED_EXTENSIONS = ["jpg", "jpeg", "png", "webp", "bmp"]
    
    settings = FallbackSettings()

# PyTorch ë° AI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
TORCH_AVAILABLE = False
DEVICE_TYPE = "cpu"
DEVICE_INFO = "ì•Œ ìˆ˜ ì—†ìŒ"

try:
    import torch
    import torchvision.transforms as transforms
    from PIL import Image
    import numpy as np
    
    TORCH_AVAILABLE = True
    print("âœ… PyTorch ë¡œë“œ ì„±ê³µ")
    
    # ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ
    if torch.cuda.is_available():
        DEVICE_TYPE = "cuda"
        DEVICE_INFO = f"NVIDIA GPU ({torch.cuda.get_device_name()})"
        print(f"ğŸ”¥ CUDA GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        DEVICE_TYPE = "mps"
        DEVICE_INFO = "Apple Silicon (Metal)"
        print("ğŸ Apple Silicon MPS ì‚¬ìš©")
    else:
        DEVICE_TYPE = "cpu"
        DEVICE_INFO = "CPU"
        print("ğŸ’» CPU ì‚¬ìš©")
        
except ImportError as e:
    print(f"âš ï¸ PyTorch ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ê¸°ë³¸ ê¸°ëŠ¥ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="MyCloset AI Backend",
    description="PyTorch ê¸°ë°˜ AI ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ",
    version="1.0.0-pytorch"
)

# CORS ë¯¸ë“¤ì›¨ì–´
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Gzip ì••ì¶•
app.add_middleware(GZipMiddleware, minimum_size=1000)

# ì •ì  íŒŒì¼ ì„œë¹™
static_path = project_root / "static"
if static_path.exists():
    app.mount("/static", StaticFiles(directory=str(static_path)), name="static")

# AI ëª¨ë¸ ì´ˆê¸°í™” (ê°„ë‹¨í•œ ì˜ˆì œ)
if TORCH_AVAILABLE:
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    image_transform = transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # ê°„ë‹¨í•œ ë°ëª¨ ëª¨ë¸ (ì‹¤ì œë¡œëŠ” ë³µì¡í•œ ê°€ìƒ í”¼íŒ… ëª¨ë¸)
    demo_model = torch.nn.Sequential(
        torch.nn.Conv2d(3, 64, kernel_size=3, padding=1),
        torch.nn.ReLU(),
        torch.nn.AdaptiveAvgPool2d((1, 1)),
        torch.nn.Flatten(),
        torch.nn.Linear(64, 10)
    ).to(DEVICE_TYPE)
    
    demo_model.eval()
    print(f"âœ… ë°ëª¨ AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({DEVICE_TYPE})")

@app.get("/")
async def root():
    return {
        "message": "MyCloset AI Backend (PyTorch ì§€ì›) ğŸ”¥",
        "version": "1.0.0-pytorch",
        "environment": "Conda",
        "conda_env": os.getenv("CONDA_DEFAULT_ENV", "unknown"),
        "python_version": platform.python_version(),
        "status": "healthy",
        "docs": "/docs",
        "config_loaded": CONFIG_LOADED,
        "ai": {
            "torch_available": TORCH_AVAILABLE,
            "device": DEVICE_TYPE,
            "device_info": DEVICE_INFO,
            "models_loaded": TORCH_AVAILABLE
        },
        "features": {
            "virtual_fitting": TORCH_AVAILABLE,
            "ai_processing": TORCH_AVAILABLE,
            "image_upload": True,
            "api_docs": True
        }
    }

@app.get("/api/health")
async def health_check():
    """ìƒì„¸ í—¬ìŠ¤ì²´í¬"""
    
    health_data = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0-pytorch",
        "environment": {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "conda_env": os.getenv("CONDA_DEFAULT_ENV", "unknown"),
            "architecture": platform.machine()
        },
        "ai": {
            "torch_available": TORCH_AVAILABLE,
            "device_type": DEVICE_TYPE,
            "device_info": DEVICE_INFO
        },
        "config": {
            "loaded": CONFIG_LOADED,
            "cors_origins": len(settings.CORS_ORIGINS) if hasattr(settings, 'CORS_ORIGINS') else 0
        },
        "services": {
            "virtual_fitting": "available" if TORCH_AVAILABLE else "disabled",
            "image_processing": "available",
            "file_upload": "available"
        }
    }
    
    # PyTorchê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì¶”ê°€ ì •ë³´
    if TORCH_AVAILABLE:
        health_data["ai"].update({
            "torch_version": torch.__version__,
            "models_loaded": True,
            "memory_allocated": f"{torch.cuda.memory_allocated() // 1024**2}MB" if DEVICE_TYPE == "cuda" else "N/A"
        })
    
    return health_data

async def process_image_with_ai(person_image: Image.Image, clothing_image: Image.Image) -> dict:
    """AIë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì²˜ë¦¬"""
    
    if not TORCH_AVAILABLE:
        raise HTTPException(500, "AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        person_tensor = image_transform(person_image).unsqueeze(0).to(DEVICE_TYPE)
        clothing_tensor = image_transform(clothing_image).unsqueeze(0).to(DEVICE_TYPE)
        
        # 2. AI ëª¨ë¸ ì¶”ë¡  (ë°ëª¨ ë²„ì „)
        with torch.no_grad():
            # ì‹¤ì œë¡œëŠ” ë³µì¡í•œ ê°€ìƒ í”¼íŒ… ë¡œì§
            person_features = demo_model(person_tensor)
            clothing_features = demo_model(clothing_tensor)
            
            # ê°„ë‹¨í•œ í˜¸í™˜ì„± ì ìˆ˜ ê³„ì‚°
            similarity = torch.cosine_similarity(person_features, clothing_features, dim=1)
            confidence = float(similarity.item())
            
        return {
            "ai_processed": True,
            "confidence": abs(confidence),
            "fit_score": min(abs(confidence) + 0.2, 1.0),
            "device_used": DEVICE_TYPE,
            "processing_successful": True
        }
        
    except Exception as e:
        logger.error(f"AI ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(500, f"AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/api/virtual-tryon")
async def virtual_tryon_endpoint(
    person_image: UploadFile = File(..., description="ì‚¬ìš©ì ì‚¬ì§„"),
    clothing_image: UploadFile = File(..., description="ì˜ë¥˜ ì‚¬ì§„"),
    height: float = Form(..., description="ì‹ ì¥ (cm)"),
    weight: float = Form(..., description="ì²´ì¤‘ (kg)")
):
    """PyTorch ê¸°ë°˜ ê°€ìƒ í”¼íŒ… API"""
    
    # íŒŒì¼ ê²€ì¦
    if not person_image.content_type.startswith("image/"):
        raise HTTPException(400, "ì‚¬ìš©ì ì´ë¯¸ì§€ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.")
    
    if not clothing_image.content_type.startswith("image/"):
        raise HTTPException(400, "ì˜ë¥˜ ì´ë¯¸ì§€ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.")
    
    try:
        session_id = str(uuid.uuid4())
        start_time = time.time()
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        person_img = Image.open(person_image.file).convert('RGB')
        clothing_img = Image.open(clothing_image.file).convert('RGB')
        
        # AI ì²˜ë¦¬ (PyTorch ì‚¬ìš©)
        if TORCH_AVAILABLE:
            ai_result = await process_image_with_ai(person_img, clothing_img)
            processing_type = "AI ê¸°ë°˜ ì²˜ë¦¬"
        else:
            # í´ë°±: ê¸°ë³¸ ì²˜ë¦¬
            ai_result = {
                "ai_processed": False,
                "confidence": 0.75,
                "fit_score": 0.80,
                "device_used": "cpu",
                "processing_successful": True
            }
            processing_type = "ê¸°ë³¸ ì²˜ë¦¬"
        
        processing_time = time.time() - start_time
        
        # BMI ê³„ì‚°
        bmi = round(weight / ((height/100) ** 2), 1)
        bmi_status = "ì •ìƒ" if 18.5 <= bmi <= 25 else "í™•ì¸ í•„ìš”"
        
        return {
            "success": True,
            "session_id": session_id,
            "message": f"{processing_type}ë¡œ ê°€ìƒ í”¼íŒ…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!",
            "data": {
                "person_image": {
                    "filename": person_image.filename,
                    "content_type": person_image.content_type,
                    "size": f"{person_img.size[0]}x{person_img.size[1]}"
                },
                "clothing_image": {
                    "filename": clothing_image.filename,
                    "content_type": clothing_image.content_type,
                    "size": f"{clothing_img.size[0]}x{clothing_img.size[1]}"
                },
                "measurements": {
                    "height": f"{height}cm",
                    "weight": f"{weight}kg",
                    "bmi": bmi,
                    "bmi_status": bmi_status
                }
            },
            "ai_analysis": {
                "torch_used": TORCH_AVAILABLE,
                "device": ai_result["device_used"],
                "confidence": round(ai_result["confidence"], 3),
                "fit_score": round(ai_result["fit_score"], 3),
                "processing_type": processing_type
            },
            "processing": {
                "time_seconds": round(processing_time, 2),
                "status": "completed"
            },
            "recommendations": [
                f"í• ì ìˆ˜: {round(ai_result['fit_score']*100)}%" if ai_result['fit_score'] > 0.8 else "ì‚¬ì´ì¦ˆ í™•ì¸ ê¶Œì¥",
                f"AI ì‹ ë¢°ë„: {round(ai_result['confidence']*100)}%",
                f"ì²´í˜• ë¶„ì„: {bmi_status} (BMI: {bmi})"
            ]
        }
        
    except Exception as e:
        logger.error(f"ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(500, f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/api/models")
async def get_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ëª©ë¡"""
    
    models = [
        {
            "id": "demo_pytorch",
            "name": "PyTorch ë°ëª¨ ëª¨ë¸",
            "status": "available" if TORCH_AVAILABLE else "unavailable",
            "device": DEVICE_TYPE,
            "description": "PyTorch ê¸°ë°˜ ê°€ìƒ í”¼íŒ… ë°ëª¨",
            "features": ["AI ë¶„ì„", "í• ì ìˆ˜", "ì‹ ë¢°ë„ ì¸¡ì •"] if TORCH_AVAILABLE else ["ê¸°ë³¸ ê¸°ëŠ¥ë§Œ"]
        }
    ]
    
    if TORCH_AVAILABLE:
        models.extend([
            {
                "id": "ootd_diffusion",
                "name": "OOT-Diffusion",
                "status": "preparing",
                "device": DEVICE_TYPE,
                "description": "ê³ í’ˆì§ˆ Diffusion ê¸°ë°˜ ê°€ìƒ í”¼íŒ…",
                "features": ["ê³ í•´ìƒë„", "ìì—°ìŠ¤ëŸ¬ìš´ í•©ì„±", "ì •í™•í•œ í”¼íŒ…"]
            }
        ])
    
    return {
        "models": models,
        "default": "demo_pytorch" if TORCH_AVAILABLE else "basic",
        "environment": {
            "torch_available": TORCH_AVAILABLE,
            "device": DEVICE_TYPE,
            "device_info": DEVICE_INFO,
            "conda_env": os.getenv("CONDA_DEFAULT_ENV")
        },
        "capabilities": {
            "ai_processing": TORCH_AVAILABLE,
            "gpu_acceleration": DEVICE_TYPE in ["cuda", "mps"],
            "real_time": True
        }
    }

@app.get("/api/torch-test")
async def test_pytorch_functionality():
    """PyTorch ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    
    if not TORCH_AVAILABLE:
        raise HTTPException(503, "PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # ê°„ë‹¨í•œ í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸
        start_time = time.time()
        
        x = torch.randn(100, 100, device=DEVICE_TYPE)
        y = torch.randn(100, 100, device=DEVICE_TYPE)
        z = torch.mm(x, y)
        
        processing_time = time.time() - start_time
        
        return {
            "status": "success",
            "device": DEVICE_TYPE,
            "device_info": DEVICE_INFO,
            "test_results": {
                "tensor_size": "100x100",
                "operation": "matrix_multiplication",
                "processing_time_ms": round(processing_time * 1000, 2),
                "result_shape": list(z.shape),
                "memory_allocated": f"{torch.cuda.memory_allocated() // 1024**2}MB" if DEVICE_TYPE == "cuda" else "N/A"
            },
            "torch_version": torch.__version__,
            "message": "PyTorchê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!"
        }
        
    except Exception as e:
        raise HTTPException(500, f"PyTorch í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

@app.on_event("startup")
async def startup_event():
    logger.info("ğŸš€ MyCloset AI Backend (PyTorch ì§€ì›) ì‹œì‘ë¨")
    logger.info(f"ğŸ Conda í™˜ê²½: {os.getenv('CONDA_DEFAULT_ENV', 'unknown')}")
    logger.info(f"ğŸ”§ ì„¤ì • ë¡œë“œ: {'ì„±ê³µ' if CONFIG_LOADED else 'í´ë°± ì‚¬ìš©'}")
    logger.info(f"ğŸ”¥ PyTorch: {'ì‚¬ìš© ê°€ëŠ¥' if TORCH_AVAILABLE else 'ì—†ìŒ'}")
    logger.info(f"ğŸ’» ë””ë°”ì´ìŠ¤: {DEVICE_TYPE}")
    
    if TORCH_AVAILABLE:
        logger.info(f"ğŸ¯ AI ëª¨ë¸: ë¡œë“œë¨ ({DEVICE_TYPE})")
    else:
        logger.warning("âš ï¸ AI ê¸°ëŠ¥ ì œí•œë¨ (PyTorch ì—†ìŒ)")
    
    # í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±
    directories = ["static/uploads", "static/results", "logs"]
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
    
    logger.info("âœ… ëª¨ë“  ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("ğŸ›‘ MyCloset AI Backend ì¢…ë£Œë¨")
    if TORCH_AVAILABLE and DEVICE_TYPE == "cuda":
        torch.cuda.empty_cache()
        logger.info("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ PyTorch ì§€ì› ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
EOF

    # 9. PyTorch ì§€ì› ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    cat > run_with_pytorch.sh << 'EOF'
#!/bin/bash

echo "ğŸ”¥ MyCloset AI Backend - PyTorch ì§€ì› ë²„ì „ ì‹¤í–‰"
echo "=============================================="

# Conda í™˜ê²½ í™•ì¸
if [[ "$CONDA_DEFAULT_ENV" == "" ]]; then
    echo "âŒ Conda í™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "conda activate mycloset"
    exit 1
fi

echo "âœ… Conda í™˜ê²½: $CONDA_DEFAULT_ENV"

# PyTorch í™•ì¸
echo "ğŸ”¥ PyTorch ìƒíƒœ í™•ì¸ ì¤‘..."
python -c "
import torch
print(f'âœ… PyTorch: {torch.__version__}')

if torch.cuda.is_available():
    print(f'âœ… CUDA: {torch.cuda.get_device_name()}')
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    print('âœ… MPS (Apple Silicon): ì‚¬ìš© ê°€ëŠ¥')
else:
    print('âœ… CPU ëª¨ë“œ')
" 2>/dev/null || {
    echo "âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!"
    echo "ì„¤ì¹˜: conda install pytorch torchvision -c pytorch -y"
    exit 1
}

echo ""
echo "ğŸŒ PyTorch ì§€ì› ì„œë²„ ì‹œì‘ ì¤‘..."
echo "ğŸ“± ë©”ì¸: http://localhost:8000"
echo "ğŸ“š API ë¬¸ì„œ: http://localhost:8000/docs"
echo "ğŸ”§ í—¬ìŠ¤ì²´í¬: http://localhost:8000/api/health"
echo "ğŸ§ª PyTorch í…ŒìŠ¤íŠ¸: http://localhost:8000/api/torch-test"
echo "ğŸ­ ê°€ìƒ í”¼íŒ…: http://localhost:8000/api/virtual-tryon"
echo ""
echo "â¹ï¸ ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”"
echo ""

# PyTorch ì§€ì› ë²„ì „ ì‹¤í–‰
uvicorn app.main_with_pytorch:app --reload --host 0.0.0.0 --port 8000
EOF

    chmod +x run_with_pytorch.sh
    
    echo ""
    echo "ğŸ‰ PyTorch ì„¤ì¹˜ ë° ë°±ì—”ë“œ ì—…ë°ì´íŠ¸ ì™„ë£Œ!"
    echo ""
    echo "ğŸš€ ì‹¤í–‰ ë°©ë²•:"
    echo "   ./run_with_pytorch.sh"
    echo ""
    echo "ğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•:"
    echo "   python test_pytorch_installation.py"
    echo ""
    echo "ğŸ“± ì‹¤í–‰ í›„ ì ‘ì†: http://localhost:8000"
    echo "ğŸ”¥ PyTorch í…ŒìŠ¤íŠ¸: http://localhost:8000/api/torch-test"
    
else
    echo ""
    echo "âŒ PyTorch ì„¤ì¹˜ ì‹¤íŒ¨"
    echo "ğŸ”§ ìˆ˜ë™ ì„¤ì¹˜ ë°©ë²•:"
    echo "   conda install pytorch torchvision -c pytorch -y"
    echo "   python test_pytorch_installation.py"
fi