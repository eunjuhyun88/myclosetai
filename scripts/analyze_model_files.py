# scripts/analyze_model_files.py
"""
ğŸ”¥ MyCloset AI - ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ë¶„ì„ ë° ìµœì  ë§¤í•‘ ìƒì„±
370GB ëª¨ë¸ íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ì—¬ ê° Stepì— ìµœì  ëª¨ë¸ ë§¤í•‘
"""

import os
import sys
from pathlib import Path
import json
import logging
from typing import Dict, List, Tuple
from collections import defaultdict

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ModelFileAnalyzer:
    """AI ëª¨ë¸ íŒŒì¼ ë¶„ì„ê¸°"""
    
    def __init__(self, backend_dir: str = "backend"):
        self.backend_dir = Path(backend_dir)
        self.ai_models_dir = self.backend_dir / "ai_models"
        
        # Stepë³„ í‚¤ì›Œë“œ ë§¤í•‘
        self.step_keywords = {
            "step_01_human_parsing": [
                "schp", "atr", "lip", "graphonomy", "parsing", "human", 
                "densepose", "segment", "body"
            ],
            "step_02_pose_estimation": [
                "pose", "openpose", "body_pose", "hand_pose", "joint",
                "keypoint", "skeleton", "coco", "mpii"
            ],
            "step_03_cloth_segmentation": [
                "u2net", "cloth", "segment", "sam", "mask", "background",
                "removal", "matting", "rmbg"
            ],
            "step_04_geometric_matching": [
                "geometric", "gmm", "matching", "tps", "transformation",
                "warping", "correspondence", "alignment"
            ],
            "step_05_cloth_warping": [
                "warp", "tps", "transformation", "deformation", "grid",
                "flow", "displacement", "thin_plate"
            ],
            "step_06_virtual_fitting": [
                "viton", "hrviton", "ootd", "diffusion", "fitting", "try_on",
                "virtual", "vae", "unet", "stable_diffusion"
            ],
            "step_07_post_processing": [
                "esrgan", "super_resolution", "enhance", "gfpgan", "codeformer",
                "real_esrgan", "swinir", "restoration", "upscale"
            ],
            "step_08_quality_assessment": [
                "lpips", "quality", "metric", "score", "assessment",
                "clip", "vgg", "resnet", "feature"
            ]
        }
        
        # ëª¨ë¸ ìš°ì„ ìˆœìœ„ (íŒŒì¼ëª… íŒ¨í„´)
        self.priority_patterns = {
            "step_01_human_parsing": [
                "exp-schp-201908301523-atr.pth",  # ìµœê³  ìš°ì„ ìˆœìœ„
                "exp-schp-201908261155-lip.pth",
                "graphonomy.pth",
                "schp_atr.pth"
            ],
            "step_02_pose_estimation": [
                "body_pose_model.pth",
                "openpose.pth", 
                "openpose_05.pth",
                "hand_pose_model.pth"
            ],
            "step_03_cloth_segmentation": [
                "u2net.pth",
                "u2net_backup.pth",
                "sam_vit_h_4b8939.pth"
            ],
            "step_06_virtual_fitting": [
                "hrviton_final.pth",
                "diffusion_pytorch_model.bin",
                "ootd_hd_unet.bin",
                "vae_model.bin"
            ]
        }
        
    def get_file_size_mb(self, file_path: Path) -> float:
        """íŒŒì¼ í¬ê¸°ë¥¼ MBë¡œ ë°˜í™˜"""
        try:
            return file_path.stat().st_size / (1024 * 1024)
        except:
            return 0.0
            
    def classify_model_file(self, file_path: Path) -> Tuple[str, float]:
        """íŒŒì¼ì„ ì ì ˆí•œ Stepìœ¼ë¡œ ë¶„ë¥˜í•˜ê³  ì‹ ë¢°ë„ ë°˜í™˜"""
        filename = file_path.name.lower()
        parent_dir = file_path.parent.name.lower()
        
        step_scores = defaultdict(float)
        
        # 1. ë””ë ‰í† ë¦¬ëª…ìœ¼ë¡œ ìš°ì„  ë¶„ë¥˜
        if parent_dir.startswith("step_"):
            step_scores[parent_dir] += 10.0
            
        # 2. í‚¤ì›Œë“œ ë§¤ì¹­
        for step_name, keywords in self.step_keywords.items():
            for keyword in keywords:
                if keyword in filename or keyword in parent_dir:
                    step_scores[step_name] += 1.0
                    
        # 3. ìš°ì„ ìˆœìœ„ íŒ¨í„´ ë§¤ì¹­
        for step_name, patterns in self.priority_patterns.items():
            for pattern in patterns:
                if pattern.lower() in filename:
                    step_scores[step_name] += 5.0
                    
        if not step_scores:
            return "unknown", 0.0
            
        best_step = max(step_scores.items(), key=lambda x: x[1])
        return best_step[0], best_step[1]
        
    def analyze_all_models(self) -> Dict:
        """ëª¨ë“  ëª¨ë¸ íŒŒì¼ ë¶„ì„"""
        logger.info("ğŸ” AI ëª¨ë¸ íŒŒì¼ ì „ì²´ ë¶„ì„ ì‹œì‘...")
        
        results = {
            "total_files": 0,
            "total_size_gb": 0.0,
            "by_step": defaultdict(list),
            "by_extension": defaultdict(int),
            "recommendations": {}
        }
        
        # ëª¨ë“  AI ëª¨ë¸ íŒŒì¼ íƒìƒ‰
        for ext in ["*.pth", "*.bin", "*.pkl"]:
            for model_file in self.ai_models_dir.rglob(ext):
                if "cleanup_backup" in str(model_file):
                    continue  # ë°±ì—… íŒŒì¼ ì œì™¸
                    
                size_mb = self.get_file_size_mb(model_file)
                step_name, confidence = self.classify_model_file(model_file)
                
                file_info = {
                    "path": str(model_file.relative_to(self.ai_models_dir)),
                    "name": model_file.name,
                    "size_mb": round(size_mb, 2),
                    "extension": model_file.suffix,
                    "confidence": round(confidence, 2),
                    "parent_dir": model_file.parent.name
                }
                
                results["by_step"][step_name].append(file_info)
                results["by_extension"][model_file.suffix] += 1
                results["total_files"] += 1
                results["total_size_gb"] += size_mb / 1024
                
        results["total_size_gb"] = round(results["total_size_gb"], 2)
        
        # Stepë³„ ì¶”ì²œ ëª¨ë¸ ì„ ì •
        self.generate_recommendations(results)
        
        return results
        
    def generate_recommendations(self, results: Dict):
        """ê° Stepë³„ ì¶”ì²œ ëª¨ë¸ ìƒì„±"""
        logger.info("ğŸ¯ Stepë³„ ìµœì  ëª¨ë¸ ì¶”ì²œ ìƒì„± ì¤‘...")
        
        for step_name, files in results["by_step"].items():
            if step_name == "unknown" or not files:
                continue
                
            # ì‹ ë¢°ë„ì™€ í¬ê¸°ë¡œ ì •ë ¬
            sorted_files = sorted(files, key=lambda x: (-x["confidence"], -x["size_mb"]))
            
            recommendations = {
                "primary": None,
                "alternatives": [],
                "total_files": len(files),
                "total_size_mb": sum(f["size_mb"] for f in files)
            }
            
            # ê¸°ë³¸ ëª¨ë¸ ì„ ì •
            if sorted_files:
                recommendations["primary"] = sorted_files[0]
                recommendations["alternatives"] = sorted_files[1:min(4, len(sorted_files))]
                
            results["recommendations"][step_name] = recommendations
            
    def create_optimized_config(self, results: Dict) -> Dict:
        """ìµœì í™”ëœ ëª¨ë¸ ì„¤ì • ìƒì„±"""
        logger.info("âš™ï¸ ìµœì í™”ëœ ëª¨ë¸ ì„¤ì • ìƒì„± ì¤‘...")
        
        config = {
            "version": "1.0",
            "generated_by": "ModelFileAnalyzer",
            "total_models": results["total_files"],
            "total_size_gb": results["total_size_gb"],
            "step_configs": {}
        }
        
        for step_name, rec in results["recommendations"].items():
            if step_name == "unknown" or not rec["primary"]:
                continue
                
            step_config = {
                "enabled": True,
                "primary_model": {
                    "path": rec["primary"]["path"],
                    "name": rec["primary"]["name"],
                    "size_mb": rec["primary"]["size_mb"],
                    "confidence": rec["primary"]["confidence"]
                },
                "alternative_models": [
                    {
                        "path": alt["path"], 
                        "name": alt["name"],
                        "size_mb": alt["size_mb"]
                    } for alt in rec["alternatives"]
                ],
                "total_available": rec["total_files"]
            }
            
            config["step_configs"][step_name] = step_config
            
        return config
        
    def save_analysis_results(self, results: Dict, config: Dict):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        logger.info("ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = self.backend_dir / "analysis_results"
        output_dir.mkdir(exist_ok=True)
        
        # ìƒì„¸ ë¶„ì„ ê²°ê³¼
        with open(output_dir / "model_analysis_detailed.json", "w") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
            
        # ìµœì í™” ì„¤ì •
        with open(output_dir / "optimized_model_config.json", "w") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
            
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_summary_report(results, output_dir)
        
        logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_dir}")
        
    def generate_summary_report(self, results: Dict, output_dir: Path):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_file = output_dir / "analysis_summary.md"
        
        with open(report_file, "w", encoding="utf-8") as f:
            f.write("# MyCloset AI ëª¨ë¸ ë¶„ì„ ë¦¬í¬íŠ¸\n\n")
            f.write(f"## ğŸ“Š ì „ì²´ í˜„í™©\n")
            f.write(f"- **ì´ ëª¨ë¸ íŒŒì¼**: {results['total_files']}ê°œ\n")
            f.write(f"- **ì´ í¬ê¸°**: {results['total_size_gb']:.1f}GB\n")
            f.write(f"- **í™•ì¥ìë³„ ë¶„í¬**: {dict(results['by_extension'])}\n\n")
            
            f.write("## ğŸ¯ Stepë³„ ì¶”ì²œ ëª¨ë¸\n\n")
            for step_name, rec in results["recommendations"].items():
                if step_name == "unknown" or not rec["primary"]:
                    continue
                    
                f.write(f"### {step_name}\n")
                f.write(f"- **ì¶”ì²œ ëª¨ë¸**: {rec['primary']['name']}\n")
                f.write(f"- **í¬ê¸°**: {rec['primary']['size_mb']:.1f}MB\n")
                f.write(f"- **ì‹ ë¢°ë„**: {rec['primary']['confidence']:.1f}\n")
                f.write(f"- **ì „ì²´ ì˜µì…˜**: {rec['total_files']}ê°œ\n\n")
                
        logger.info(f"ğŸ“‹ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±: {report_file}")
        
    def run(self):
        """ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("ğŸš€ AI ëª¨ë¸ íŒŒì¼ ë¶„ì„ ì‹œì‘")
        
        if not self.ai_models_dir.exists():
            logger.error(f"âŒ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.ai_models_dir}")
            return False
            
        # 1. ëª¨ë“  ëª¨ë¸ ë¶„ì„
        results = self.analyze_all_models()
        
        # 2. ìµœì í™” ì„¤ì • ìƒì„±
        config = self.create_optimized_config(results)
        
        # 3. ê²°ê³¼ ì €ì¥
        self.save_analysis_results(results, config)
        
        # 4. ìš”ì•½ ì¶œë ¥
        logger.info("ğŸ‰ ë¶„ì„ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ {results['total_files']}ê°œ íŒŒì¼, {results['total_size_gb']:.1f}GB")
        
        for step_name, rec in results["recommendations"].items():
            if step_name != "unknown" and rec["primary"]:
                logger.info(f"ğŸ¯ {step_name}: {rec['primary']['name']} ({rec['primary']['size_mb']:.1f}MB)")
                
        return True

if __name__ == "__main__":
    analyzer = ModelFileAnalyzer()
    success = analyzer.run()
    sys.exit(0 if success else 1)