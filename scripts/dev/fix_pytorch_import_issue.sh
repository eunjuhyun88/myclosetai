#!/bin/bash

echo "ðŸ”§ PyTorch Import ë¬¸ì œ í•´ê²° ì¤‘..."
echo "================================"

# 1. í˜„ìž¬ í™˜ê²½ ìƒíƒœ ì§„ë‹¨
echo "ðŸ” 1. í™˜ê²½ ìƒíƒœ ì§„ë‹¨..."
echo "Conda í™˜ê²½: ${CONDA_DEFAULT_ENV:-'ì—†ìŒ'}"
echo "Python ê²½ë¡œ: $(which python)"
echo "Conda ê²½ë¡œ: $(which conda)"

# 2. conda íŒ¨í‚¤ì§€ ëª©ë¡ì—ì„œ PyTorch í™•ì¸
echo ""
echo "ðŸ“¦ 2. ì„¤ì¹˜ëœ PyTorch íŒ¨í‚¤ì§€ í™•ì¸..."
conda list | grep torch || echo "âŒ torch íŒ¨í‚¤ì§€ê°€ ëª©ë¡ì— ì—†ìŠµë‹ˆë‹¤"

# 3. í™˜ê²½ ìƒˆë¡œê³ ì¹¨
echo ""
echo "ðŸ”„ 3. conda í™˜ê²½ ìƒˆë¡œê³ ì¹¨..."
conda deactivate 2>/dev/null || true
conda activate mycloset

# 4. ê²½ë¡œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì§ì ‘ ì„¤ì¹˜
echo ""
echo "ðŸ”§ 4. PyTorch ì§ì ‘ ìž¬ì„¤ì¹˜..."

# ê¸°ì¡´ torch ì™„ì „ ì œê±°
echo "ðŸ—‘ï¸ ê¸°ì¡´ PyTorch ì™„ì „ ì œê±° ì¤‘..."
pip uninstall torch torchvision torchaudio -y 2>/dev/null || true
conda remove pytorch torchvision torchaudio -y --force 2>/dev/null || true

# conda ì±„ë„ ìš°ì„ ìˆœìœ„ ìž¬ì„¤ì •
echo "ðŸ”„ conda ì±„ë„ ìž¬ì„¤ì •..."
conda config --add channels pytorch
conda config --add channels conda-forge

# Apple Siliconìš© ìµœì‹  PyTorch ì„¤ì¹˜
echo "ðŸ”¥ Apple Siliconìš© PyTorch ìž¬ì„¤ì¹˜..."
conda install pytorch torchvision -c pytorch -y --force-reinstall

# pipë¡œë„ ë°±ì—… ì„¤ì¹˜ ì‹œë„
echo "ðŸ”„ pip ë°±ì—… ì„¤ì¹˜..."
pip install torch torchvision --no-deps --force-reinstall

# 5. ì„¤ì¹˜ ê²€ì¦
echo ""
echo "ðŸ§ª 5. ì„¤ì¹˜ ê²€ì¦..."

# Pythonì—ì„œ ì§ì ‘ í…ŒìŠ¤íŠ¸
python -c "
import sys
print(f'Python ê²½ë¡œ: {sys.executable}')
print(f'Python ë²„ì „: {sys.version}')
print('ì„¤ì¹˜ëœ ëª¨ë“ˆ ê²½ë¡œë“¤:')
for path in sys.path:
    print(f'  {path}')
print()

try:
    import torch
    print(f'âœ… PyTorch ì„±ê³µ: {torch.__version__}')
    print(f'ðŸ“ PyTorch ê²½ë¡œ: {torch.__file__}')
    
    # ê°„ë‹¨í•œ í…ì„œ í…ŒìŠ¤íŠ¸
    x = torch.tensor([1, 2, 3])
    print(f'âœ… í…ì„œ í…ŒìŠ¤íŠ¸: {x}')
    
    # ë””ë°”ì´ìŠ¤ í™•ì¸
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print('âœ… MPS (Apple Silicon) ì‚¬ìš© ê°€ëŠ¥')
        device = 'mps'
    else:
        print('â„¹ï¸ CPU ëª¨ë“œ')
        device = 'cpu'
    
    # ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸
    test_tensor = torch.randn(3, 3, device=device)
    print(f'âœ… {device} ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {test_tensor.shape}')
    
    print()
    print('ðŸŽ‰ PyTorch ì„¤ì¹˜ ë° í…ŒìŠ¤íŠ¸ ì„±ê³µ!')
    
except ImportError as e:
    print(f'âŒ PyTorch import ì‹¤íŒ¨: {e}')
    print()
    print('ðŸ”§ ì¶”ê°€ í•´ê²° ë°©ë²•ì„ ì‹œë„í•©ë‹ˆë‹¤...')
" || {
    echo ""
    echo "âŒ ì—¬ì „ížˆ ë¬¸ì œê°€ ìžˆìŠµë‹ˆë‹¤. ì¶”ê°€ í•´ê²°ì±…ì„ ì‹œë„í•©ë‹ˆë‹¤..."
    
    # 6. ëŒ€ì•ˆ í•´ê²°ì±…
    echo ""
    echo "ðŸ”§ 6. ëŒ€ì•ˆ í•´ê²°ì±… ì‹œë„..."
    
    # conda-forge ì±„ë„ë¡œ ì‹œë„
    echo "ðŸ“¦ conda-forge ì±„ë„ë¡œ ì„¤ì¹˜ ì‹œë„..."
    conda install pytorch torchvision -c conda-forge -y --force-reinstall
    
    # ë˜ëŠ” nightly ë²„ì „ ì‹œë„
    echo "ðŸ“¦ nightly ë²„ì „ ì‹œë„..."
    pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cpu --force-reinstall
    
    # ìµœì¢… í…ŒìŠ¤íŠ¸
    python -c "import torch; print(f'ìµœì¢… í…ŒìŠ¤íŠ¸: PyTorch {torch.__version__} ì„¤ì¹˜ë¨')" || {
        echo ""
        echo "âŒ ëª¨ë“  ìžë™ í•´ê²° ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        echo ""
        echo "ðŸ”§ ìˆ˜ë™ í•´ê²° ë°©ë²•:"
        echo "1. conda í™˜ê²½ ìž¬ìƒì„±:"
        echo "   conda deactivate"
        echo "   conda remove -n mycloset --all -y"
        echo "   conda create -n mycloset python=3.10 -y"
        echo "   conda activate mycloset"
        echo "   conda install pytorch torchvision -c pytorch -y"
        echo ""
        echo "2. ë˜ëŠ” pipë§Œ ì‚¬ìš©:"
        echo "   pip install torch torchvision"
        echo ""
        exit 1
    }
}

# 7. ì„±ê³µí–ˆë‹¤ë©´ ë°±ì—”ë“œ ì—…ë°ì´íŠ¸
echo ""
echo "ðŸŽ‰ PyTorch ì„¤ì¹˜ ì„±ê³µ! ë°±ì—”ë“œ ì—…ë°ì´íŠ¸ ì¤‘..."

# PyTorch ë²„ì „ í™•ì¸
TORCH_VERSION=$(python -c "import torch; print(torch.__version__)" 2>/dev/null || echo "unknown")
DEVICE_TYPE=$(python -c "
import torch
if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    print('mps')
elif torch.cuda.is_available():
    print('cuda')
else:
    print('cpu')
" 2>/dev/null || echo "cpu")

echo "âœ… PyTorch ë²„ì „: $TORCH_VERSION"
echo "âœ… ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: $DEVICE_TYPE"

# 8. ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ ì„œë²„ ìƒì„±
echo ""
echo "ðŸš€ 8. PyTorch í…ŒìŠ¤íŠ¸ ì„œë²„ ìƒì„± ì¤‘..."

cat > test_pytorch_server.py << 'EOF'
#!/usr/bin/env python3
"""
PyTorch í…ŒìŠ¤íŠ¸ ì„œë²„
ì„¤ì¹˜ í™•ì¸ ë° ê°„ë‹¨í•œ AI ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    import torch
    import torchvision.transforms as transforms
    from PIL import Image
    import numpy as np
    TORCH_AVAILABLE = True
    
    # ë””ë°”ì´ìŠ¤ ì„ íƒ
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        DEVICE = "mps"
        DEVICE_INFO = "Apple Silicon (Metal)"
    elif torch.cuda.is_available():
        DEVICE = "cuda"
        DEVICE_INFO = f"NVIDIA GPU ({torch.cuda.get_device_name()})"
    else:
        DEVICE = "cpu"
        DEVICE_INFO = "CPU"
        
    print(f"âœ… PyTorch ë¡œë“œ ì„±ê³µ: {torch.__version__}")
    print(f"ðŸŽ¯ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {DEVICE} ({DEVICE_INFO})")
    
except ImportError as e:
    print(f"âŒ PyTorch ë¡œë“œ ì‹¤íŒ¨: {e}")
    TORCH_AVAILABLE = False
    DEVICE = "none"
    DEVICE_INFO = "PyTorch ì—†ìŒ"

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from datetime import datetime
import time

app = FastAPI(
    title="PyTorch í…ŒìŠ¤íŠ¸ ì„œë²„",
    description="PyTorch ì„¤ì¹˜ í™•ì¸ ë° AI ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {
        "message": "PyTorch í…ŒìŠ¤íŠ¸ ì„œë²„ ðŸ”¥",
        "torch_available": TORCH_AVAILABLE,
        "torch_version": torch.__version__ if TORCH_AVAILABLE else "ì—†ìŒ",
        "device": DEVICE,
        "device_info": DEVICE_INFO,
        "status": "ready" if TORCH_AVAILABLE else "pytorch_missing",
        "test_endpoints": {
            "tensor_test": "/test/tensor",
            "performance": "/test/performance",
            "memory": "/test/memory"
        }
    }

@app.get("/test/tensor")
async def test_tensor_operations():
    """ê¸°ë³¸ í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸"""
    if not TORCH_AVAILABLE:
        raise HTTPException(503, "PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        start_time = time.time()
        
        # ê¸°ë³¸ í…ì„œ ìƒì„±
        x = torch.randn(100, 100, device=DEVICE)
        y = torch.randn(100, 100, device=DEVICE)
        
        # í–‰ë ¬ ê³±ì…ˆ
        z = torch.mm(x, y)
        
        # í†µê³„ ê³„ì‚°
        mean_val = torch.mean(z).item()
        std_val = torch.std(z).item()
        
        processing_time = time.time() - start_time
        
        return {
            "success": True,
            "device": DEVICE,
            "tensor_shape": list(z.shape),
            "statistics": {
                "mean": round(mean_val, 4),
                "std": round(std_val, 4)
            },
            "processing_time_ms": round(processing_time * 1000, 2),
            "message": "í…ì„œ ì—°ì‚°ì´ ì •ìƒì ìœ¼ë¡œ ìž‘ë™í•©ë‹ˆë‹¤!"
        }
        
    except Exception as e:
        raise HTTPException(500, f"í…ì„œ ì—°ì‚° ì‹¤íŒ¨: {str(e)}")

@app.get("/test/performance")
async def test_performance():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    if not TORCH_AVAILABLE:
        raise HTTPException(503, "PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        results = {}
        
        # CPU vs í˜„ìž¬ ë””ë°”ì´ìŠ¤ ì„±ëŠ¥ ë¹„êµ
        for device in ["cpu", DEVICE] if DEVICE != "cpu" else ["cpu"]:
            start_time = time.time()
            
            # 1000x1000 í–‰ë ¬ ê³±ì…ˆ 10íšŒ
            for _ in range(10):
                a = torch.randn(1000, 1000, device=device)
                b = torch.randn(1000, 1000, device=device)
                c = torch.mm(a, b)
            
            total_time = time.time() - start_time
            results[device] = {
                "total_time_seconds": round(total_time, 3),
                "avg_time_per_operation_ms": round(total_time * 100, 2),  # 10íšŒ ë°˜ë³µì´ë¯€ë¡œ *100
                "operations_per_second": round(10 / total_time, 1)
            }
        
        return {
            "success": True,
            "torch_version": torch.__version__,
            "benchmark_results": results,
            "recommendation": f"{DEVICE} ì‚¬ìš©ì„ ê¶Œìž¥í•©ë‹ˆë‹¤" if DEVICE != "cpu" else "GPUê°€ ìžˆë‹¤ë©´ ë” ë¹ ë¥¸ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤"
        }
        
    except Exception as e:
        raise HTTPException(500, f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

@app.get("/test/memory")
async def test_memory():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
    if not TORCH_AVAILABLE:
        raise HTTPException(503, "PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        results = {
            "device": DEVICE,
            "torch_version": torch.__version__
        }
        
        if DEVICE == "cuda":
            # CUDA ë©”ëª¨ë¦¬ ì •ë³´
            results["cuda_memory"] = {
                "total_mb": torch.cuda.get_device_properties(0).total_memory // 1024**2,
                "allocated_mb": torch.cuda.memory_allocated() // 1024**2,
                "cached_mb": torch.cuda.memory_reserved() // 1024**2
            }
        elif DEVICE == "mps":
            # MPSëŠ” í†µí•© ë©”ëª¨ë¦¬ ì‚¬ìš©
            results["mps_info"] = {
                "unified_memory": True,
                "note": "Apple Siliconì€ í†µí•© ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤"
            }
        
        # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ í• ë‹¹ í…ŒìŠ¤íŠ¸
        test_tensor = torch.randn(1000, 1000, device=DEVICE)
        tensor_size_mb = test_tensor.element_size() * test_tensor.nelement() / 1024**2
        
        results["test_allocation"] = {
            "tensor_shape": list(test_tensor.shape),
            "tensor_size_mb": round(tensor_size_mb, 2),
            "allocation_successful": True
        }
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del test_tensor
        if DEVICE == "cuda":
            torch.cuda.empty_cache()
        elif DEVICE == "mps":
            torch.mps.empty_cache()
            
        return {
            "success": True,
            "memory_info": results
        }
        
    except Exception as e:
        raise HTTPException(500, f"ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    print("ðŸ”¥ PyTorch í…ŒìŠ¤íŠ¸ ì„œë²„ ì‹œìž‘...")
    print(f"âœ… PyTorch: {'ì‚¬ìš© ê°€ëŠ¥' if TORCH_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
    print(f"ðŸŽ¯ ë””ë°”ì´ìŠ¤: {DEVICE}")
    print("")
    print("ðŸ“± ì ‘ì† ì£¼ì†Œ: http://localhost:8001")
    print("ðŸ“š API ë¬¸ì„œ: http://localhost:8001/docs")
    print("ðŸ§ª í…ì„œ í…ŒìŠ¤íŠ¸: http://localhost:8001/test/tensor")
    print("âš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸: http://localhost:8001/test/performance")
    print("ðŸ’¾ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸: http://localhost:8001/test/memory")
    print("")
    
    uvicorn.run(app, host="0.0.0.0", port=8001, reload=False)
EOF

echo "âœ… PyTorch í…ŒìŠ¤íŠ¸ ì„œë²„ ìƒì„± ì™„ë£Œ"

# 9. ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > run_pytorch_test.sh << 'EOF'
#!/bin/bash

echo "ðŸ”¥ PyTorch í…ŒìŠ¤íŠ¸ ì„œë²„ ì‹¤í–‰"
echo "========================="

# Conda í™˜ê²½ í™•ì¸
if [[ "$CONDA_DEFAULT_ENV" != "mycloset" ]]; then
    echo "âŒ mycloset conda í™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "conda activate mycloset"
    exit 1
fi

# PyTorch ê°„ë‹¨ í™•ì¸
python -c "import torch; print(f'âœ… PyTorch {torch.__version__} ì¤€ë¹„ë¨')" || {
    echo "âŒ PyTorchê°€ ì œëŒ€ë¡œ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    exit 1
}

echo "ðŸš€ í…ŒìŠ¤íŠ¸ ì„œë²„ ì‹œìž‘ ì¤‘..."
python test_pytorch_server.py
EOF

chmod +x run_pytorch_test.sh

echo ""
echo "ðŸŽ‰ ëª¨ë“  ì„¤ì • ì™„ë£Œ!"
echo ""
echo "ðŸ§ª PyTorch í…ŒìŠ¤íŠ¸ ë°©ë²•:"
echo "   python test_pytorch_installation.py"
echo ""
echo "ðŸš€ í…ŒìŠ¤íŠ¸ ì„œë²„ ì‹¤í–‰:"
echo "   ./run_pytorch_test.sh"
echo ""
echo "ðŸ“± í…ŒìŠ¤íŠ¸ ì„œë²„ ì ‘ì†: http://localhost:8001"