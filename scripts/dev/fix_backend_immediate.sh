#!/bin/bash

echo "ğŸ”§ MyCloset AI ë°±ì—”ë“œ ì¦‰ì‹œ ìˆ˜ì • ì¤‘..."

cd backend

# 1. pydantic-settings ì„¤ì¹˜ (config ë¡œë“œ ë¬¸ì œ í•´ê²°)
echo "ğŸ“¦ 1. ëˆ„ë½ëœ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
conda install -c conda-forge pydantic-settings python-dotenv -y
pip install python-multipart aiofiles

# 2. ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ config.py ìƒì„±
echo "âš™ï¸ 2. ì•ˆì •ì ì¸ config.py ìƒì„± ì¤‘..."

cat > app/core/config.py << 'EOF'
from typing import List
import os

class Settings:
    """ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ ì„¤ì • í´ë˜ìŠ¤"""
    
    # App ê¸°ë³¸ ì„¤ì •
    APP_NAME: str = "MyCloset AI Backend"
    APP_VERSION: str = "1.0.0"
    DEBUG: bool = True
    HOST: str = "0.0.0.0"
    PORT: int = 8000
    
    # CORS ì„¤ì • (ì•ˆì „í•œ ê¸°ë³¸ê°’)
    CORS_ORIGINS: List[str] = [
        "http://localhost:3000",
        "http://localhost:5173", 
        "http://localhost:8080"
    ]
    
    # íŒŒì¼ ì—…ë¡œë“œ ì„¤ì •
    MAX_UPLOAD_SIZE: int = 52428800  # 50MB
    ALLOWED_EXTENSIONS: List[str] = ["jpg", "jpeg", "png", "webp", "bmp"]
    
    # AI ëª¨ë¸ ì„¤ì •
    DEFAULT_MODEL: str = "demo"
    USE_GPU: bool = False  # ì•ˆì •ì„±ì„ ìœ„í•´ False
    DEVICE: str = "cpu"
    IMAGE_SIZE: int = 512
    MAX_WORKERS: int = 2
    BATCH_SIZE: int = 1
    
    # ë¡œê¹…
    LOG_LEVEL: str = "INFO"
    
    # ê²½ë¡œ
    UPLOAD_PATH: str = "static/uploads"
    RESULT_PATH: str = "static/results"
    MODEL_PATH: str = "ai_models"
    
    def __init__(self):
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°’ ì½ê¸° (ìˆìœ¼ë©´)
        self.DEBUG = os.getenv("DEBUG", "true").lower() == "true"
        self.HOST = os.getenv("HOST", "0.0.0.0")
        self.PORT = int(os.getenv("PORT", 8000))
        
        # CORS_ORIGINS í™˜ê²½ë³€ìˆ˜ ì²˜ë¦¬
        cors_env = os.getenv("CORS_ORIGINS")
        if cors_env:
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            self.CORS_ORIGINS = [origin.strip() for origin in cors_env.split(",")]

# ì „ì—­ ì„¤ì • ê°ì²´
settings = Settings()
EOF

echo "âœ… ì•ˆì •ì ì¸ config.py ìƒì„± ì™„ë£Œ"

# 3. ì‘ë™í•˜ëŠ” main.py ìƒì„±
echo "ğŸ”§ 3. ì‘ë™í•˜ëŠ” main.py ìƒì„± ì¤‘..."

cat > app/main.py << 'EOF'
import sys
import os
from pathlib import Path
import logging
from datetime import datetime
import platform

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import JSONResponse

# ì•ˆì „í•œ ì„¤ì • ë¡œë“œ
try:
    from app.core.config import settings
    CONFIG_LOADED = True
    print("âœ… ì„¤ì • ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
    CONFIG_LOADED = False
    
    # í´ë°± ì„¤ì •
    class FallbackSettings:
        APP_NAME = "MyCloset AI Backend"
        APP_VERSION = "1.0.0"
        DEBUG = True
        CORS_ORIGINS = ["http://localhost:3000", "http://localhost:5173"]
        MAX_UPLOAD_SIZE = 52428800
        ALLOWED_EXTENSIONS = ["jpg", "jpeg", "png", "webp", "bmp"]
    
    settings = FallbackSettings()

# PyTorch ì•ˆì „ í™•ì¸
TORCH_AVAILABLE = False
DEVICE_TYPE = "cpu"
try:
    import torch
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        DEVICE_TYPE = "mps"
        DEVICE_INFO = "Apple Silicon (MPS)"
    elif torch.cuda.is_available():
        DEVICE_TYPE = "cuda"
        DEVICE_INFO = "NVIDIA GPU"
    else:
        DEVICE_TYPE = "cpu"
        DEVICE_INFO = "CPU"
except ImportError:
    DEVICE_INFO = "PyTorch ì—†ìŒ"
    print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="MyCloset AI Backend",
    description="AI ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ (ì•ˆì • ë²„ì „)",
    version="1.0.0"
)

# CORS ë¯¸ë“¤ì›¨ì–´
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Gzip ì••ì¶•
app.add_middleware(GZipMiddleware, minimum_size=1000)

# ì •ì  íŒŒì¼ ì„œë¹™
static_path = project_root / "static"
if static_path.exists():
    app.mount("/static", StaticFiles(directory=str(static_path)), name="static")

@app.get("/")
async def root():
    return {
        "message": "MyCloset AI Backendì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤! âœ¨",
        "version": "1.0.0",
        "environment": "Conda",
        "conda_env": os.getenv("CONDA_DEFAULT_ENV", "unknown"),
        "python_version": platform.python_version(),
        "status": "healthy",
        "docs": "/docs",
        "config_loaded": CONFIG_LOADED,
        "torch_available": TORCH_AVAILABLE,
        "device": DEVICE_TYPE,
        "features": {
            "virtual_fitting": True,
            "image_upload": True,
            "api_docs": True
        }
    }

@app.get("/api/health")
async def health_check():
    """ìƒì„¸ í—¬ìŠ¤ì²´í¬"""
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0",
        "environment": {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "conda_env": os.getenv("CONDA_DEFAULT_ENV", "unknown"),
            "architecture": platform.machine()
        },
        "ai": {
            "torch_available": TORCH_AVAILABLE,
            "device_type": DEVICE_TYPE,
            "device_info": DEVICE_INFO
        },
        "config": {
            "loaded": CONFIG_LOADED,
            "cors_origins": len(settings.CORS_ORIGINS) if hasattr(settings, 'CORS_ORIGINS') else 0
        },
        "services": {
            "virtual_fitting": "available",
            "image_processing": "available",
            "file_upload": "available"
        }
    }

@app.post("/api/virtual-tryon")
async def virtual_tryon_endpoint(
    person_image: UploadFile = File(..., description="ì‚¬ìš©ì ì‚¬ì§„"),
    clothing_image: UploadFile = File(..., description="ì˜ë¥˜ ì‚¬ì§„"),
    height: float = Form(..., description="ì‹ ì¥ (cm)"),
    weight: float = Form(..., description="ì²´ì¤‘ (kg)")
):
    """ê°€ìƒ í”¼íŒ… API"""
    
    # íŒŒì¼ ê²€ì¦
    if not person_image.content_type.startswith("image/"):
        raise HTTPException(400, "ì‚¬ìš©ì ì´ë¯¸ì§€ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.")
    
    if not clothing_image.content_type.startswith("image/"):
        raise HTTPException(400, "ì˜ë¥˜ ì´ë¯¸ì§€ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.")
    
    # íŒŒì¼ í¬ê¸° ì²´í¬
    max_size = getattr(settings, 'MAX_UPLOAD_SIZE', 52428800)  # 50MB
    
    try:
        # ê°„ë‹¨í•œ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        import time
        import uuid
        
        session_id = str(uuid.uuid4())
        start_time = time.time()
        
        # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ (1-3ì´ˆ)
        await asyncio.sleep(2)
        
        processing_time = time.time() - start_time
        
        return {
            "success": True,
            "session_id": session_id,
            "message": "ê°€ìƒ í”¼íŒ…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!",
            "data": {
                "person_image": {
                    "filename": person_image.filename,
                    "content_type": person_image.content_type,
                    "size": f"{person_image.size if hasattr(person_image, 'size') else 'unknown'} bytes"
                },
                "clothing_image": {
                    "filename": clothing_image.filename,
                    "content_type": clothing_image.content_type,
                    "size": f"{clothing_image.size if hasattr(clothing_image, 'size') else 'unknown'} bytes"
                },
                "measurements": {
                    "height": f"{height}cm",
                    "weight": f"{weight}kg",
                    "bmi": round(weight / ((height/100) ** 2), 1)
                }
            },
            "processing": {
                "time_seconds": round(processing_time, 2),
                "device": DEVICE_TYPE,
                "torch_available": TORCH_AVAILABLE
            },
            "result": {
                "confidence": 0.92,
                "fit_score": 0.88,
                "recommendation": "ì¢‹ì€ í•ì…ë‹ˆë‹¤!" if 18.5 <= weight / ((height/100) ** 2) <= 25 else "ì‚¬ì´ì¦ˆë¥¼ í™•ì¸í•´ë³´ì„¸ìš”."
            },
            "note": "í˜„ì¬ ë°ëª¨ ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì‹¤ì œ AI ëª¨ë¸ì€ ê³§ í†µí•©ë  ì˜ˆì •ì…ë‹ˆë‹¤."
        }
        
    except Exception as e:
        raise HTTPException(500, f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/api/models")
async def get_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ëª©ë¡"""
    return {
        "models": [
            {
                "id": "demo",
                "name": "ë°ëª¨ ëª¨ë“œ",
                "status": "available",
                "device": DEVICE_TYPE,
                "description": "ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© ê°€ìƒ í”¼íŒ…",
                "features": ["ë¹ ë¥¸ ì²˜ë¦¬", "ê¸°ë³¸ í•©ì„±", "í…ŒìŠ¤íŠ¸ ëª¨ë“œ"]
            },
            {
                "id": "ootd_diffusion",
                "name": "OOT-Diffusion",
                "status": "preparing",
                "device": DEVICE_TYPE,
                "description": "ê³ í’ˆì§ˆ Diffusion ê¸°ë°˜ ê°€ìƒ í”¼íŒ…",
                "features": ["ê³ í•´ìƒë„", "ìì—°ìŠ¤ëŸ¬ìš´ í•©ì„±", "ì •í™•í•œ í”¼íŒ…"]
            }
        ],
        "default": "demo",
        "environment": {
            "torch_available": TORCH_AVAILABLE,
            "device": DEVICE_TYPE,
            "conda_env": os.getenv("CONDA_DEFAULT_ENV")
        },
        "status": "ëª¨ë“  ê¸°ë³¸ ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤"
    }

@app.get("/api/status")
async def get_system_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    return {
        "backend": "running",
        "database": "not_required",
        "ai_models": "demo_mode",
        "file_storage": "available",
        "config": "loaded" if CONFIG_LOADED else "fallback",
        "torch": "available" if TORCH_AVAILABLE else "not_installed",
        "timestamp": datetime.now().isoformat()
    }

# ì¶”ê°€ë¡œ í•„ìš”í•œ import
import asyncio

@app.on_event("startup")
async def startup_event():
    logger.info("ğŸš€ MyCloset AI Backend ì‹œì‘ë¨")
    logger.info(f"ğŸ Conda í™˜ê²½: {os.getenv('CONDA_DEFAULT_ENV', 'unknown')}")
    logger.info(f"ğŸ”§ ì„¤ì • ë¡œë“œ: {'ì„±ê³µ' if CONFIG_LOADED else 'í´ë°± ì‚¬ìš©'}")
    logger.info(f"ğŸ”¥ PyTorch: {'ì‚¬ìš© ê°€ëŠ¥' if TORCH_AVAILABLE else 'ì—†ìŒ'}")
    logger.info(f"ğŸ’» ë””ë°”ì´ìŠ¤: {DEVICE_TYPE}")
    logger.info("ğŸ“ í•„ìˆ˜ ë””ë ‰í† ë¦¬ í™•ì¸ ì¤‘...")
    
    # í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±
    directories = ["static/uploads", "static/results", "logs"]
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
    
    logger.info("âœ… ëª¨ë“  ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("ğŸ›‘ MyCloset AI Backend ì¢…ë£Œë¨")

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
EOF

echo "âœ… ì‘ë™í•˜ëŠ” main.py ìƒì„± ì™„ë£Œ"

# 4. í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±
echo "ğŸ“ 4. í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„± ì¤‘..."
mkdir -p static/{uploads,results}
mkdir -p logs
touch static/uploads/.gitkeep
touch static/results/.gitkeep
touch logs/.gitkeep

# 5. ê°„ë‹¨í•œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
echo "ğŸ“œ 5. ê°„ë‹¨í•œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘..."

cat > run_fixed.sh << 'EOF'
#!/bin/bash

echo "ğŸš€ MyCloset AI Backend - ìˆ˜ì •ëœ ë²„ì „ ì‹¤í–‰"
echo "=========================================="

# Conda í™˜ê²½ í™•ì¸
if [[ "$CONDA_DEFAULT_ENV" == "" ]]; then
    echo "âŒ Conda í™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "conda activate mycloset"
    exit 1
fi

echo "âœ… Conda í™˜ê²½: $CONDA_DEFAULT_ENV"

# íŒ¨í‚¤ì§€ í™•ì¸
echo "ğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸ ì¤‘..."

# FastAPI í™•ì¸
python -c "import fastapi; print(f'âœ… FastAPI: {fastapi.__version__}')" 2>/dev/null || {
    echo "âŒ FastAPIê°€ ì—†ìŠµë‹ˆë‹¤. ì„¤ì¹˜: conda install fastapi uvicorn -y"
    exit 1
}

# ì„œë²„ ì‹œì‘
echo ""
echo "ğŸŒ ì„œë²„ ì‹œì‘ ì¤‘..."
echo "ğŸ“± ì ‘ì† ì£¼ì†Œ: http://localhost:8000"
echo "ğŸ“š API ë¬¸ì„œ: http://localhost:8000/docs"
echo "ğŸ”§ í—¬ìŠ¤ì²´í¬: http://localhost:8000/api/health"
echo "ğŸ§ª ê°€ìƒ í”¼íŒ… í…ŒìŠ¤íŠ¸: http://localhost:8000/api/virtual-tryon"
echo ""
echo "â¹ï¸ ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”"
echo ""

uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
EOF

chmod +x run_fixed.sh

# 6. í˜„ì¬ í™˜ê²½ ì •ë³´ ì¶œë ¥
echo "ğŸ“Š 6. í˜„ì¬ í™˜ê²½ ì •ë³´"
echo "===================="

echo "ğŸ Conda í™˜ê²½: ${CONDA_DEFAULT_ENV:-'ì—†ìŒ'}"
echo "ğŸ Python: $(python --version 2>/dev/null || echo 'í™•ì¸ë¶ˆê°€')"

# íŒ¨í‚¤ì§€ í™•ì¸
echo "ğŸ“¦ ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€:"
python -c "import fastapi; print(f'  âœ… FastAPI: {fastapi.__version__}')" 2>/dev/null || echo "  âŒ FastAPI ì—†ìŒ"
python -c "import uvicorn; print(f'  âœ… Uvicorn: {uvicorn.__version__}')" 2>/dev/null || echo "  âŒ Uvicorn ì—†ìŒ"
python -c "import pydantic; print(f'  âœ… Pydantic: {pydantic.__version__}')" 2>/dev/null || echo "  âŒ Pydantic ì—†ìŒ"
python -c "import torch; print(f'  âœ… PyTorch: {torch.__version__}')" 2>/dev/null || echo "  â„¹ï¸ PyTorch ì—†ìŒ (ì„ íƒì‚¬í•­)"

echo ""
echo "ğŸ‰ ìˆ˜ì • ì™„ë£Œ!"
echo ""
echo "ğŸš€ ì‹¤í–‰ ë°©ë²•:"
echo "   ./run_fixed.sh"
echo ""
echo "ğŸ”§ ë¬¸ì œê°€ ìˆë‹¤ë©´:"
echo "   conda install fastapi uvicorn python-multipart -y"
echo "   pip install pydantic-settings python-dotenv"
echo ""
echo "ğŸ“± ì‹¤í–‰ í›„ ì ‘ì†: http://localhost:8000"   