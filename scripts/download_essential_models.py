#!/usr/bin/env python3
"""
ğŸ MyCloset AI - í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (conda í™˜ê²½)
===============================================================
âœ… conda í™˜ê²½ ìµœì í™”
âœ… M3 Max ë©”ëª¨ë¦¬ ê³ ë ¤
âœ… ì•ˆì „í•œ ë‹¤ìš´ë¡œë“œ
âœ… ì§„í–‰ë¥  í‘œì‹œ
âœ… ì—ëŸ¬ ë³µêµ¬
"""

import os
import sys
import requests
import hashlib
from pathlib import Path
from typing import Dict, Any
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ê²½ë¡œ
PROJECT_ROOT = Path(__file__).parent
AI_MODELS_DIR = PROJECT_ROOT / "backend" / "ai_models"

# í•„ìˆ˜ ëª¨ë¸ ì •ë³´
ESSENTIAL_MODELS = {
    "step_01_human_parsing": {
        "file_name": "exp-schp-201908301523-atr.pth",
        "url": "https://drive.google.com/uc?id=1ruJg-hPABjf5_WW3WQ18E_1DdQWPpWGS",
        "size_mb": 255.1,
        "md5": None  # í•„ìš” ì‹œ ì¶”ê°€
    },
    "step_03_cloth_segmentation": {
        "file_name": "u2net.pth", 
        "url": "https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ",
        "size_mb": 168.1,
        "md5": None
    },
    "step_06_virtual_fitting": {
        "file_name": "sam_vit_h_4b8939.pth",
        "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth",
        "size_mb": 2445.7,
        "md5": "4b8939a88964f0f4ff5f5b2642c598a6"
    }
}

def download_file(url: str, dest_path: Path, expected_size_mb: float = None) -> bool:
    """íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì§„í–‰ë¥  í‘œì‹œ)"""
    try:
        logger.info(f"ë‹¤ìš´ë¡œë“œ ì‹œì‘: {dest_path.name}")
        
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        
        dest_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(dest_path, 'wb') as f:
            downloaded = 0
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    
                    if total_size > 0:
                        percent = (downloaded / total_size) * 100
                        print(f"\rì§„í–‰ë¥ : {percent:.1f}% ({downloaded // (1024*1024)}MB)", end='')
        
        print()  # ìƒˆ ì¤„
        
        # í¬ê¸° ê²€ì¦
        actual_size_mb = dest_path.stat().st_size / (1024 * 1024)
        if expected_size_mb and abs(actual_size_mb - expected_size_mb) > expected_size_mb * 0.1:
            logger.warning(f"í¬ê¸° ë¶ˆì¼ì¹˜: ì˜ˆìƒ {expected_size_mb}MB, ì‹¤ì œ {actual_size_mb:.1f}MB")
        
        logger.info(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {dest_path.name} ({actual_size_mb:.1f}MB)")
        return True
        
    except Exception as e:
        logger.error(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {dest_path.name}: {e}")
        if dest_path.exists():
            dest_path.unlink()
        return False

def main():
    """ë©”ì¸ ë‹¤ìš´ë¡œë“œ í”„ë¡œì„¸ìŠ¤"""
    logger.info("ğŸ MyCloset AI í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    
    # conda í™˜ê²½ í™•ì¸
    conda_env = os.environ.get('CONDA_DEFAULT_ENV')
    if conda_env:
        logger.info(f"âœ… conda í™˜ê²½: {conda_env}")
    else:
        logger.warning("âš ï¸ conda í™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•ŠìŒ")
    
    success_count = 0
    total_count = len(ESSENTIAL_MODELS)
    
    for step_name, model_info in ESSENTIAL_MODELS.items():
        step_dir = AI_MODELS_DIR / step_name
        model_path = step_dir / model_info["file_name"]
        
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if model_path.exists():
            actual_size_mb = model_path.stat().st_size / (1024 * 1024)
            expected_size_mb = model_info["size_mb"]
            
            if abs(actual_size_mb - expected_size_mb) < expected_size_mb * 0.1:
                logger.info(f"âœ… ì´ë¯¸ ì¡´ì¬: {model_info['file_name']}")
                success_count += 1
                continue
            else:
                logger.info(f"ğŸ”„ í¬ê¸° ë¶ˆì¼ì¹˜ë¡œ ì¬ë‹¤ìš´ë¡œë“œ: {model_info['file_name']}")
                model_path.unlink()
        
        # ë‹¤ìš´ë¡œë“œ ì‹œë„
        if download_file(model_info["url"], model_path, model_info["size_mb"]):
            success_count += 1
    
    # ê²°ê³¼ ìš”ì•½
    logger.info(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {success_count}/{total_count}")
    
    if success_count == total_count:
        logger.info("ğŸ‰ ëª¨ë“  í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì„±ê³µ!")
        
        # ë¹ ë¥¸ ê²€ì¦
        logger.info("ğŸ” ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸...")
        try:
            sys.path.insert(0, str(PROJECT_ROOT / "backend"))
            from app.ai_pipeline.utils.model_loader import ModelLoader
            
            loader = ModelLoader()
            loader.scan_available_models()
            logger.info("âœ… ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    else:
        logger.error(f"âŒ {total_count - success_count}ê°œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
