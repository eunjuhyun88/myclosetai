# scripts/move_checkpoint_files.py
"""
ğŸ”¥ MyCloset AI - ì‹¤ì œ íŒŒì¼ ì´ë™ì„ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ìˆ˜ì •
================================================================================
âœ… ì‹¬ë³¼ë¦­ ë§í¬ ëŒ€ì‹  ì‹¤ì œ íŒŒì¼ ì´ë™
âœ… ì†ŒìŠ¤ íŒŒì¼ ì‚­ì œ í›„ íƒ€ê²Ÿ ìœ„ì¹˜ì—ë§Œ íŒŒì¼ ìœ ì§€
âœ… ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ìµœì  íŒŒì¼ ìœ„ì¹˜ ê²°ì •
âœ… ë°±ì—… ìƒì„± ë° ì•ˆì „í•œ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
âœ… ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ ë° ê³µê°„ ì ˆì•½
================================================================================
"""

import os
import sys
import shutil
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Set
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CheckpointFileMover:
    """ì‹¤ì œ íŒŒì¼ ì´ë™ì„ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
    
    def __init__(self, backend_dir: str = "backend"):
        self.backend_dir = Path(backend_dir)
        self.ai_models_dir = self.backend_dir / "ai_models"
        self.backup_dir = self.ai_models_dir / "move_backup"
        
        # ë¶„ì„ ê²°ê³¼ ë¡œë“œ
        self.analysis_results = self._load_analysis_results()
        
        # ì½”ë“œì—ì„œ ì°¾ëŠ” íŒŒì¼ â†’ ì‹¤ì œ ì´ë™í•  ìœ„ì¹˜ ë§¤í•‘
        self.target_mappings = {
            # Virtual Fittingì—ì„œ ì°¾ëŠ” íŒŒì¼ë“¤ì„ í•´ë‹¹ ìœ„ì¹˜ë¡œ ì´ë™
            "step_06_virtual_fitting/body_pose_model.pth": "step_02_pose_estimation/body_pose_model.pth",
            "step_06_virtual_fitting/hrviton_final_01.pth": "step_06_virtual_fitting/hrviton_final.pth",
            "step_06_virtual_fitting/exp-schp-201908261155-lip.pth": "step_01_human_parsing/exp-schp-201908261155-lip.pth",
            "step_06_virtual_fitting/exp-schp-201908301523-atr.pth": "step_01_human_parsing/exp-schp-201908301523-atr.pth",
            
            # Human Parsing í‘œì¤€í™” (ì½”ë“œì—ì„œ ì°¾ëŠ” íŒŒì¼ëª…ìœ¼ë¡œ ì´ë™)
            "step_01_human_parsing/exp-schp-201908261155-lip_22.pth": "step_01_human_parsing/exp-schp-201908261155-lip.pth",
            "step_01_human_parsing/graphonomy_08.pth": "step_01_human_parsing/graphonomy.pth",
            "step_01_human_parsing/exp-schp-201908301523-atr_30.pth": "step_01_human_parsing/exp-schp-201908301523-atr.pth",
            
            # Pose Estimation í‘œì¤€í™”
            "step_02_pose_estimation/body_pose_model_41.pth": "step_02_pose_estimation/body_pose_model.pth",
            "step_02_pose_estimation/openpose_08.pth": "step_02_pose_estimation/openpose.pth",
            
            # Cloth Warping
            "step_05_cloth_warping/tom_final_01.pth": "step_05_cloth_warping/tom_final.pth",
        }
        
        # ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì¶”ì²œ íŒŒì¼ë“¤ì„ í‘œì¤€ ì´ë¦„ìœ¼ë¡œ ì´ë™
        self.recommended_moves = self._generate_recommended_moves()
        
    def _load_analysis_results(self) -> Dict:
        """ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
        analysis_file = self.backend_dir / "analysis_results" / "optimized_model_config.json"
        
        if analysis_file.exists():
            try:
                with open(analysis_file, 'r') as f:
                    results = json.load(f)
                logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {analysis_file}")
                return results
            except Exception as e:
                logger.warning(f"âš ï¸ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        return {}
        
    def _generate_recommended_moves(self) -> Dict[str, str]:
        """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì¶”ì²œ ì´ë™ ë§¤í•‘ ìƒì„±"""
        moves = {}
        
        if not self.analysis_results or "step_configs" not in self.analysis_results:
            return moves
            
        # ê° Stepì˜ ì¶”ì²œ ëª¨ë¸ì„ í‘œì¤€ ì´ë¦„ìœ¼ë¡œ ì´ë™
        step_standard_names = {
            "step_01_human_parsing": {
                "primary": "human_parsing_primary.pth",
                "atr": "exp-schp-201908301523-atr.pth", 
                "lip": "exp-schp-201908261155-lip.pth",
                "graphonomy": "graphonomy.pth"
            },
            "step_02_pose_estimation": {
                "primary": "pose_estimation_primary.pth",
                "body": "body_pose_model.pth",
                "openpose": "openpose.pth",
                "hand": "hand_pose_model.pth"
            },
            "step_03_cloth_segmentation": {
                "primary": "cloth_segmentation_primary.pth",
                "u2net": "u2net.pth",
                "sam": "sam_vit_h.pth"
            },
            "step_06_virtual_fitting": {
                "primary": "virtual_fitting_primary.pth",
                "hrviton": "hrviton_final.pth",
                "diffusion": "diffusion_pytorch_model.bin",
                "ootd": "ootd_hd_unet.bin"
            }
        }
        
        for step_name, config in self.analysis_results["step_configs"].items():
            if step_name in step_standard_names:
                primary_model = config["primary_model"]
                standard_names = step_standard_names[step_name]
                
                # ì¶”ì²œ ëª¨ë¸ì„ primaryë¡œ ì´ë™
                current_path = primary_model["path"]
                target_path = f"{step_name}/{standard_names['primary']}"
                moves[target_path] = current_path
                
        return moves
        
    def create_backup(self) -> bool:
        """ì´ë™ ì „ ë°±ì—… ìƒì„±"""
        logger.info("ğŸ“‹ ì´ë™ ì „ ë°±ì—… ìƒì„± ì¤‘...")
        
        try:
            # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = self.backup_dir / f"before_move_{timestamp}"
            backup_path.mkdir(parents=True, exist_ok=True)
            
            # ì´ë™í•  íŒŒì¼ë“¤ì˜ í˜„ì¬ ìƒíƒœ ê¸°ë¡
            backup_manifest = {
                "timestamp": timestamp,
                "total_files": 0,
                "moves_planned": {},
                "file_checksums": {}
            }
            
            # ì´ë™ ê³„íš ê¸°ë¡
            all_moves = {**self.target_mappings, **self.recommended_moves}
            
            for target_path, source_path in all_moves.items():
                source_full = self.ai_models_dir / source_path
                target_full = self.ai_models_dir / target_path
                
                backup_manifest["moves_planned"][target_path] = {
                    "source": source_path,
                    "source_exists": source_full.exists(),
                    "target_exists": target_full.exists(),
                    "source_size": source_full.stat().st_size if source_full.exists() else 0
                }
                
            # ë°±ì—… ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥
            with open(backup_path / "move_manifest.json", "w") as f:
                json.dump(backup_manifest, f, indent=2)
                
            logger.info(f"âœ… ë°±ì—… ìƒì„± ì™„ë£Œ: {backup_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            return False
            
    def find_best_source_file(self, target_path: str) -> Path:
        """íƒ€ê²Ÿ ê²½ë¡œì— ê°€ì¥ ì í•©í•œ ì†ŒìŠ¤ íŒŒì¼ ì°¾ê¸°"""
        # 1. ëª…ì‹œì  ë§¤í•‘ í™•ì¸
        if target_path in self.target_mappings:
            source_path = self.ai_models_dir / self.target_mappings[target_path]
            if source_path.exists():
                return source_path
                
        # 2. ì¶”ì²œ ë§¤í•‘ í™•ì¸
        if target_path in self.recommended_moves:
            source_path = self.ai_models_dir / self.recommended_moves[target_path]
            if source_path.exists():
                return source_path
                
        # 3. ìœ ì‚¬í•œ ì´ë¦„ì˜ íŒŒì¼ ì°¾ê¸°
        target_name = Path(target_path).name
        target_step = Path(target_path).parent.name
        
        # ê°™ì€ ìŠ¤í… ë‚´ì—ì„œ ìœ ì‚¬í•œ íŒŒì¼ ì°¾ê¸°
        step_dir = self.ai_models_dir / target_step
        if step_dir.exists():
            for file_path in step_dir.glob("*.pth"):
                if self._files_are_similar(file_path.name, target_name):
                    return file_path
                    
        # 4. ì „ì²´ì—ì„œ ìœ ì‚¬í•œ íŒŒì¼ ì°¾ê¸°
        for file_path in self.ai_models_dir.rglob("*.pth"):
            if "cleanup_backup" in str(file_path):
                continue
            if self._files_are_similar(file_path.name, target_name):
                return file_path
                
        return None
        
    def _files_are_similar(self, file1: str, file2: str) -> bool:
        """ë‘ íŒŒì¼ëª…ì´ ìœ ì‚¬í•œì§€ í™•ì¸"""
        # í™•ì¥ì ì œê±°
        name1 = Path(file1).stem.lower()
        name2 = Path(file2).stem.lower()
        
        # ì •í™•íˆ ì¼ì¹˜
        if name1 == name2:
            return True
            
        # í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ì„± í™•ì¸
        keywords1 = set(name1.replace('_', ' ').replace('-', ' ').split())
        keywords2 = set(name2.replace('_', ' ').replace('-', ' ').split())
        
        # ê³µí†µ í‚¤ì›Œë“œê°€ 50% ì´ìƒ
        if keywords1 and keywords2:
            intersection = keywords1.intersection(keywords2)
            union = keywords1.union(keywords2)
            similarity = len(intersection) / len(union)
            return similarity >= 0.5
            
        return False
        
    def move_file_safely(self, source_path: Path, target_path: Path) -> bool:
        """ì•ˆì „í•˜ê²Œ íŒŒì¼ ì´ë™"""
        try:
            # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ìƒì„±
            target_path.parent.mkdir(parents=True, exist_ok=True)
            
            # íƒ€ê²Ÿ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš°
            if target_path.exists():
                # ê°™ì€ íŒŒì¼ì¸ì§€ í™•ì¸
                if source_path.samefile(target_path):
                    logger.info(f"ğŸ”„ ì´ë¯¸ ê°™ì€ íŒŒì¼: {target_path.name}")
                    return True
                    
                # í¬ê¸° ë¹„êµ
                source_size = source_path.stat().st_size
                target_size = target_path.stat().st_size
                
                if source_size > target_size:
                    # ì†ŒìŠ¤ê°€ ë” í¬ë©´ êµì²´
                    logger.info(f"ğŸ”„ ë” í° íŒŒì¼ë¡œ êµì²´: {target_path.name} ({source_size/1024/1024:.1f}MB > {target_size/1024/1024:.1f}MB)")
                    target_path.unlink()  # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
                elif source_size == target_size:
                    # ê°™ì€ í¬ê¸°ë©´ ì†ŒìŠ¤ ì‚­ì œ
                    logger.info(f"â™»ï¸ ì¤‘ë³µ íŒŒì¼ ì œê±°: {source_path.name}")
                    source_path.unlink()
                    return True
                else:
                    # íƒ€ê²Ÿì´ ë” í¬ë©´ ì†ŒìŠ¤ë§Œ ì‚­ì œ
                    logger.info(f"ğŸ—‘ï¸ ë” ì‘ì€ íŒŒì¼ ì œê±°: {source_path.name}")
                    source_path.unlink()
                    return True
                    
            # ì‹¤ì œ íŒŒì¼ ì´ë™
            shutil.move(str(source_path), str(target_path))
            
            size_mb = target_path.stat().st_size / (1024 * 1024)
            logger.info(f"âœ… íŒŒì¼ ì´ë™ ì™„ë£Œ: {source_path.name} â†’ {target_path.name} ({size_mb:.1f}MB)")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì´ë™ ì‹¤íŒ¨ {source_path} â†’ {target_path}: {e}")
            return False
            
    def perform_strategic_moves(self) -> Dict[str, int]:
        """ì „ëµì  íŒŒì¼ ì´ë™ ìˆ˜í–‰"""
        logger.info("ğŸš€ ì „ëµì  íŒŒì¼ ì´ë™ ì‹œì‘...")
        
        stats = {
            "moved": 0,
            "skipped": 0,
            "failed": 0,
            "removed_duplicates": 0
        }
        
        # 1. ì½”ë“œì—ì„œ ì°¾ëŠ” í•„ìˆ˜ íŒŒì¼ë“¤ ë¨¼ì € ì´ë™
        logger.info("ğŸ“‹ 1ë‹¨ê³„: í•„ìˆ˜ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì´ë™")
        
        required_files = [
            "step_06_virtual_fitting/body_pose_model.pth",
            "step_06_virtual_fitting/hrviton_final_01.pth", 
            "step_06_virtual_fitting/exp-schp-201908261155-lip.pth",
            "step_06_virtual_fitting/exp-schp-201908301523-atr.pth",
            "step_01_human_parsing/exp-schp-201908261155-lip_22.pth",
            "step_01_human_parsing/graphonomy_08.pth",
            "step_01_human_parsing/exp-schp-201908301523-atr_30.pth",
            "step_02_pose_estimation/body_pose_model_41.pth",
            "step_02_pose_estimation/openpose_08.pth",
            "step_05_cloth_warping/tom_final_01.pth"
        ]
        
        for target_path_str in required_files:
            target_path = self.ai_models_dir / target_path_str
            
            if target_path.exists():
                logger.info(f"âœ… ì´ë¯¸ ì¡´ì¬: {target_path_str}")
                continue
                
            # ìµœì  ì†ŒìŠ¤ íŒŒì¼ ì°¾ê¸°
            source_path = self.find_best_source_file(target_path_str)
            
            if source_path:
                if self.move_file_safely(source_path, target_path):
                    stats["moved"] += 1
                else:
                    stats["failed"] += 1
            else:
                logger.warning(f"âš ï¸ ì†ŒìŠ¤ íŒŒì¼ ì—†ìŒ: {target_path_str}")
                stats["skipped"] += 1
                
        # 2. ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ìµœì í™”
        logger.info("ğŸ“‹ 2ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ìµœì í™”")
        
        if self.analysis_results and "step_configs" in self.analysis_results:
            for step_name, config in self.analysis_results["step_configs"].items():
                primary_model = config["primary_model"]
                
                # ì¶”ì²œ ëª¨ë¸ì„ í‘œì¤€ ìœ„ì¹˜ë¡œ ì´ë™
                source_path = self.ai_models_dir / primary_model["path"]
                target_path = self.ai_models_dir / step_name / f"{step_name}_primary.pth"
                
                if source_path.exists() and not target_path.exists():
                    if self.move_file_safely(source_path, target_path):
                        stats["moved"] += 1
                    else:
                        stats["failed"] += 1
                        
        return stats
        
    def cleanup_duplicates(self) -> int:
        """ì¤‘ë³µ íŒŒì¼ ì •ë¦¬"""
        logger.info("ğŸ§¹ ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ ì¤‘...")
        
        removed_count = 0
        file_groups = {}
        
        # íŒŒì¼ì„ í¬ê¸°ë³„ë¡œ ê·¸ë£¹í™”
        for model_file in self.ai_models_dir.rglob("*.pth"):
            if "cleanup_backup" in str(model_file):
                continue
                
            try:
                size = model_file.stat().st_size
                if size not in file_groups:
                    file_groups[size] = []
                file_groups[size].append(model_file)
            except:
                continue
                
        # ê°™ì€ í¬ê¸°ì˜ íŒŒì¼ë“¤ ì¤‘ ì¤‘ë³µ ì œê±°
        for size, files in file_groups.items():
            if len(files) < 2:
                continue
                
            # íŒŒì¼ëª…ìœ¼ë¡œ ì •ë ¬ (ë” ê°„ë‹¨í•œ ì´ë¦„ì„ ìš°ì„ )
            files.sort(key=lambda x: (len(x.name), x.name))
            
            # ì²« ë²ˆì§¸ íŒŒì¼ ìœ ì§€, ë‚˜ë¨¸ì§€ ì‚­ì œ
            keep_file = files[0]
            
            for duplicate_file in files[1:]:
                try:
                    duplicate_file.unlink()
                    removed_count += 1
                    logger.info(f"ğŸ—‘ï¸ ì¤‘ë³µ íŒŒì¼ ì œê±°: {duplicate_file.name}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ì¤‘ë³µ íŒŒì¼ ì œê±° ì‹¤íŒ¨ {duplicate_file}: {e}")
                    
        return removed_count
        
    def run(self) -> bool:
        """ì „ì²´ ì´ë™ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("ğŸš€ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì´ë™ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        
        if not self.ai_models_dir.exists():
            logger.error(f"âŒ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.ai_models_dir}")
            return False
            
        # 1. ë°±ì—… ìƒì„±
        if not self.create_backup():
            logger.error("âŒ ë°±ì—… ìƒì„± ì‹¤íŒ¨, ì¤‘ë‹¨")
            return False
            
        # 2. ì „ëµì  íŒŒì¼ ì´ë™
        move_stats = self.perform_strategic_moves()
        
        # 3. ì¤‘ë³µ íŒŒì¼ ì •ë¦¬
        removed_duplicates = self.cleanup_duplicates()
        move_stats["removed_duplicates"] = removed_duplicates
        
        # 4. ê²°ê³¼ ë¦¬í¬íŠ¸
        logger.info("ğŸ‰ íŒŒì¼ ì´ë™ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ë™ í†µê³„:")
        logger.info(f"   - ì´ë™ ì™„ë£Œ: {move_stats['moved']}ê°œ")
        logger.info(f"   - ìŠ¤í‚µ: {move_stats['skipped']}ê°œ") 
        logger.info(f"   - ì‹¤íŒ¨: {move_stats['failed']}ê°œ")
        logger.info(f"   - ì¤‘ë³µ ì œê±°: {move_stats['removed_duplicates']}ê°œ")
        
        # 5. ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
        logger.info("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info("   1. python run_server.py ì‹¤í–‰")
        logger.info("   2. ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì—ëŸ¬ í™•ì¸")
        logger.info("   3. í•„ìš”ì‹œ ì¶”ê°€ íŒŒì¼ ì´ë™")
        
        return move_stats["failed"] == 0

if __name__ == "__main__":
    mover = CheckpointFileMover()
    success = mover.run()
    sys.exit(0 if success else 1)