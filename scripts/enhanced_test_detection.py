#!/usr/bin/env python3
"""
ğŸ” í–¥ìƒëœ ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸
âœ… ì‹¤ì œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
âœ… íŒŒì¼ í¬ê¸° ë° íƒ€ì… ê²€ì¦
âœ… Stepë³„ ë§¤í•‘ í…ŒìŠ¤íŠ¸
"""

import os
import sys
from pathlib import Path

def test_enhanced_model_detection():
    """í–¥ìƒëœ ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” í–¥ìƒëœ ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    print("="*60)
    
    project_root = Path(__file__).parent
    ai_models_root = project_root / "backend" / "ai_models"
    
    if not ai_models_root.exists():
        print("âŒ AI ëª¨ë¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return
    
    print(f"ğŸ“ AI ëª¨ë¸ ë£¨íŠ¸: {ai_models_root}")
    print(f"   í¬ê¸°: {get_directory_size(ai_models_root):.1f}GB")
    print("")
    
    # Stepë³„ ê²€ì‚¬
    step_info = {
        1: {"name": "Human Parsing", "keywords": ["human", "parsing", "schp", "graphonomy"]},
        2: {"name": "Pose Estimation", "keywords": ["pose", "openpose", "body", "hrnet"]},
        3: {"name": "Cloth Segmentation", "keywords": ["cloth", "segment", "u2net", "rembg"]},
        4: {"name": "Geometric Matching", "keywords": ["geometric", "matching", "gmm", "tps"]},
        5: {"name": "Cloth Warping", "keywords": ["warp", "warping", "tom", "tps"]},
        6: {"name": "Virtual Fitting", "keywords": ["viton", "ootd", "diffusion", "fitting"]},
        7: {"name": "Post Processing", "keywords": ["enhance", "super", "resolution", "post"]},
        8: {"name": "Quality Assessment", "keywords": ["quality", "clip", "aesthetic", "assessment"]}
    }
    
    total_models = 0
    total_size_gb = 0
    
    for step_num, info in step_info.items():
        step_dir = ai_models_root / f"step_{step_num:02d}_{info['name'].lower().replace(' ', '_')}"
        
        print(f"ğŸ”§ Step {step_num:02d}: {info['name']}")
        print(f"   ğŸ“ ê²½ë¡œ: {step_dir}")
        
        if not step_dir.exists():
            print(f"   âŒ ë””ë ‰í† ë¦¬ ì—†ìŒ")
            continue
            
        # ëª¨ë¸ íŒŒì¼ ê²€ìƒ‰
        model_extensions = ['.pth', '.pt', '.bin', '.safetensors', '.onnx', '.pkl']
        model_files = []
        
        for ext in model_extensions:
            model_files.extend(list(step_dir.glob(f"*{ext}")))
            model_files.extend(list(step_dir.glob(f"**/*{ext}")))  # í•˜ìœ„ ë””ë ‰í† ë¦¬ë„ ê²€ìƒ‰
        
        # ì¤‘ë³µ ì œê±° ë° ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ í•„í„°ë§
        valid_files = []
        for f in model_files:
            try:
                if f.exists() and f.is_file():
                    valid_files.append(f)
            except:
                continue
        model_files = list(set(valid_files))
        
        if model_files:
            print(f"   âœ… {len(model_files)}ê°œ ëª¨ë¸ íŒŒì¼ ë°œê²¬")
            
            # í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬ (ì•ˆì „í•˜ê²Œ)
            try:
                model_files.sort(key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)
            except:
                print(f"   âš ï¸ ì¼ë¶€ íŒŒì¼ ì •ë ¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            
            step_size_gb = 0
            for i, model_file in enumerate(model_files[:5]):  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                try:
                    size_mb = model_file.stat().st_size / (1024 * 1024)
                    size_gb = size_mb / 1024
                    step_size_gb += size_gb
                    
                    # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                    relevance = calculate_relevance(model_file.name.lower(), info['keywords'])
                    relevance_emoji = "ğŸ¯" if relevance > 0.5 else "ğŸ”" if relevance > 0.2 else "â“"
                    
                    print(f"      {relevance_emoji} {model_file.name}")
                    print(f"         í¬ê¸°: {size_mb:.1f}MB | ê´€ë ¨ì„±: {relevance:.2f}")
                    
                except Exception as e:
                    print(f"      âŒ {model_file.name} - ì˜¤ë¥˜: {e}")
            
            if len(model_files) > 5:
                print(f"      ... ì™¸ {len(model_files) - 5}ê°œ")
            
            print(f"   ğŸ“Š ì´ í¬ê¸°: {step_size_gb:.2f}GB")
            total_models += len(model_files)
            total_size_gb += step_size_gb
        else:
            print(f"   âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
        
        print("")
    
    print("="*60)
    print("ğŸ“Š ì „ì²´ ìš”ì•½")
    print("="*60)
    print(f"ğŸ” ì´ {total_models}ê°œ ëª¨ë¸ íŒŒì¼ ë°œê²¬")
    print(f"ğŸ’¾ ì´ í¬ê¸°: {total_size_gb:.2f}GB")
    
    if total_models > 0:
        print(f"âœ… ëª¨ë¸ íƒì§€ ì„±ê³µ! ì´ì œ ì„œë²„ê°€ ì •ìƒ ì‘ë™í•  ê²ƒì…ë‹ˆë‹¤.")
        
        # ì¶”ê°€ ê²€ì¦
        print("")
        print("ğŸ” ì¶”ê°€ ê²€ì¦...")
        
        # ì¤‘ìš” ëª¨ë¸ ì¡´ì¬ í™•ì¸
        critical_steps = [1, 2, 6, 8]  # Human Parsing, Pose, Virtual Fitting, Quality
        critical_found = 0
        
        for step_num in critical_steps:
            step_name = step_info[step_num]['name']
            step_dir = ai_models_root / f"step_{step_num:02d}_{step_name.lower().replace(' ', '_')}"
            
            if step_dir.exists():
                model_files = list(step_dir.glob("*.pth")) + list(step_dir.glob("*.bin"))
                if model_files:
                    critical_found += 1
                    print(f"   âœ… {step_name}: í•µì‹¬ ëª¨ë¸ ì¡´ì¬")
                else:
                    print(f"   âš ï¸ {step_name}: í•µì‹¬ ëª¨ë¸ ëˆ„ë½")
            else:
                print(f"   âŒ {step_name}: ë””ë ‰í† ë¦¬ ì—†ìŒ")
        
        print(f"ğŸ“Š í•µì‹¬ Step: {critical_found}/{len(critical_steps)} ì¤€ë¹„ë¨")
        
        if critical_found >= 2:
            print("ğŸ‰ ìµœì†Œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡± - AI íŒŒì´í”„ë¼ì¸ ì‘ë™ ê°€ëŠ¥!")
        else:
            print("âš ï¸ ì¶”ê°€ ëª¨ë¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            
    else:
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        print("   1. ëª¨ë¸ íŒŒì¼ ë³µì‚¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
        print("   2. ê²½ë¡œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”")
    
    print("")
    print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
    if total_models > 0:
        print("   1. python3 backend/app/main.py (ì„œë²„ ì‹œì‘)")
        print("   2. ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8000/docs ì ‘ì†")
    else:
        print("   1. ëª¨ë¸ íŒŒì¼ ë³µì‚¬ ìŠ¤í¬ë¦½íŠ¸ ì¬ì‹¤í–‰")
        print("   2. ë°±ì—… ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë¸ ë³µêµ¬")

def get_directory_size(directory: Path) -> float:
    """ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚° (GB)"""
    try:
        total_size = sum(f.stat().st_size for f in directory.rglob('*') if f.is_file())
        return total_size / (1024 ** 3)
    except:
        return 0.0

def calculate_relevance(filename: str, keywords: list) -> float:
    """íŒŒì¼ëª…ê³¼ í‚¤ì›Œë“œì˜ ê´€ë ¨ì„± ê³„ì‚°"""
    relevance = 0.0
    filename_lower = filename.lower()
    
    for keyword in keywords:
        if keyword.lower() in filename_lower:
            relevance += 1.0 / len(keywords)
    
    # ì¶”ê°€ ì ìˆ˜
    if any(word in filename_lower for word in ['model', 'checkpoint', 'final', 'best']):
        relevance += 0.2
    
    return min(relevance, 1.0)

if __name__ == "__main__":
    test_enhanced_model_detection()