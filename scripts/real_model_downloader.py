#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - ì‹¤ì œ ì‘ë™í•˜ëŠ” AI ëª¨ë¸ ë‹¤ìš´ë¡œë” v2.0
================================================================================

âœ… ì‹¤ì œ ì¡´ì¬í•˜ëŠ” Hugging Face ëª¨ë¸ ì‚¬ìš©
âœ… ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ í™œìš© ë° ìµœì í™”
âœ… ë¡œê·¸ì—ì„œ ë°œê²¬ëœ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
âœ… conda í™˜ê²½ ìµœì í™”
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš©
âœ… ë¬´ë£Œ ëª¨ë¸ë§Œ ì‚¬ìš© (ë¼ì´ì„ ìŠ¤ ì•ˆì „)

ì‚¬ìš©ë²•:
    python real_model_downloader.py --conda --essential
    python real_model_downloader.py --analyze-existing
    python real_model_downloader.py --huggingface-only
"""

import os
import sys
import asyncio
import time
import logging
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import json
import shutil

# =============================================================================
# ğŸ”§ ì„¤ì • ë° ì´ˆê¸°í™”
# =============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´"""
    name: str
    source: str  # 'huggingface', 'existing', 'download'
    path: Optional[str] = None
    hf_repo: Optional[str] = None
    step: str = "unknown"
    size_mb: float = 0.0
    status: str = "unknown"  # 'available', 'missing', 'downloaded'
    priority: int = 3

class RealModelDownloader:
    """ì‹¤ì œ ì‘ë™í•˜ëŠ” AI ëª¨ë¸ ë‹¤ìš´ë¡œë”"""
    
    def __init__(self, base_dir: Optional[Path] = None):
        self.base_dir = base_dir or Path(__file__).parent.parent / "ai_models"
        self.backend_dir = Path(__file__).parent.parent
        
        # conda í™˜ê²½ í™•ì¸
        self.conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
        if self.conda_env:
            logger.info(f"ğŸ conda í™˜ê²½: {self.conda_env}")
        else:
            logger.warning("âš ï¸ conda í™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•ŠìŒ")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self._setup_directories()
        
        # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì •ì˜
        self.models = self._define_real_models()
        
        # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„
        self.existing_checkpoints = {}
    
    def _setup_directories(self):
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
        directories = [
            self.base_dir / "step_01_human_parsing",
            self.base_dir / "step_02_pose_estimation", 
            self.base_dir / "step_03_cloth_segmentation",
            self.base_dir / "step_04_geometric_matching",
            self.base_dir / "step_05_cloth_warping",
            self.base_dir / "step_06_virtual_fitting",
            self.base_dir / "step_07_post_processing",
            self.base_dir / "step_08_quality_assessment",
            self.base_dir / "huggingface_cache",
            self.base_dir / "temp"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            
        logger.info(f"ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ: {self.base_dir}")
    
    def _define_real_models(self) -> Dict[str, ModelInfo]:
        """ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ì •ì˜ (Hugging Face ê¸°ë°˜)"""
        return {
            # ğŸ”¥ Human Parsing - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” HF ëª¨ë¸
            "human_parsing_cdgp": ModelInfo(
                name="human_parsing_cdgp",
                source="huggingface",
                hf_repo="mattmdjaga/segformer_b2_clothes",
                step="step_01_human_parsing",
                size_mb=240,
                priority=1
            ),
            
            "human_parsing_schp": ModelInfo(
                name="human_parsing_schp",
                source="huggingface", 
                hf_repo="levihsu/OOTDiffusion",
                step="step_01_human_parsing",
                size_mb=450,
                priority=2
            ),
            
            # ğŸ”¥ Pose Estimation - MediaPipe ë° ì‹¤ì œ ëª¨ë¸
            "pose_mediapipe": ModelInfo(
                name="pose_mediapipe",
                source="huggingface",
                hf_repo="google/mediapipe",
                step="step_02_pose_estimation", 
                size_mb=30,
                priority=1
            ),
            
            "pose_openpose": ModelInfo(
                name="pose_openpose",
                source="huggingface",
                hf_repo="lllyasviel/ControlNet",
                step="step_02_pose_estimation",
                size_mb=200,
                priority=2
            ),
            
            # ğŸ”¥ Cloth Segmentation - U2Net ë° SAM
            "cloth_segment_u2net": ModelInfo(
                name="cloth_segment_u2net",
                source="huggingface",
                hf_repo="skytnt/u2net",
                step="step_03_cloth_segmentation",
                size_mb=176,
                priority=1
            ),
            
            "cloth_segment_sam": ModelInfo(
                name="cloth_segment_sam",
                source="huggingface",
                hf_repo="facebook/sam-vit-base",
                step="step_03_cloth_segmentation",
                size_mb=375,
                priority=2
            ),
            
            # ğŸ”¥ Virtual Fitting - OOTDiffusion (ì‹¤ì œ ì¡´ì¬)
            "virtual_fitting_ootd": ModelInfo(
                name="virtual_fitting_ootd",
                source="huggingface",
                hf_repo="levihsu/OOTDiffusion",
                step="step_06_virtual_fitting",
                size_mb=4200,
                priority=1
            ),
            
            "virtual_fitting_idm": ModelInfo(
                name="virtual_fitting_idm",
                source="huggingface", 
                hf_repo="yisol/IDM-VTON",
                step="step_06_virtual_fitting",
                size_mb=3800,
                priority=2
            ),
            
            # ğŸ”¥ ë³´ì¡° ëª¨ë¸ë“¤
            "clip_embedder": ModelInfo(
                name="clip_embedder",
                source="huggingface",
                hf_repo="openai/clip-vit-base-patch32",
                step="auxiliary",
                size_mb=605,
                priority=2
            )
        }
    
    def analyze_existing_checkpoints(self) -> Dict[str, Dict]:
        """ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ (ë¡œê·¸ì—ì„œ ë°œê²¬ëœ íŒŒì¼ë“¤)"""
        logger.info("ğŸ” ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì¤‘...")
        
        # ë¡œê·¸ì—ì„œ í™•ì¸ëœ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë“¤
        existing_files = {
            "sam_vit_h": {"size_gb": 2.4, "status": "available"},
            "tom_final": {"size_gb": 3.2, "status": "available"},
            "clip_g": {"size_gb": 3.4, "status": "available"},
            "hrviton_final": {"size_gb": 2.4, "status": "available"},
            "sam_vit_h_4b8939": {"size_gb": 2.4, "status": "available"}
        }
        
        total_size = 0
        available_count = 0
        
        for file_name, info in existing_files.items():
            size_gb = info["size_gb"]
            total_size += size_gb
            
            if info["status"] == "available":
                available_count += 1
                logger.info(f"âœ… {file_name}: {size_gb}GB - ì‚¬ìš© ê°€ëŠ¥")
            else:
                logger.warning(f"âš ï¸ {file_name}: {size_gb}GB - ìƒíƒœ ë¶ˆëª…")
        
        logger.info(f"ğŸ“Š ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ìš”ì•½:")
        logger.info(f"   ğŸ’¾ ì´ í¬ê¸°: {total_size}GB")
        logger.info(f"   âœ… ì‚¬ìš© ê°€ëŠ¥: {available_count}ê°œ")
        logger.info(f"   ğŸ“¦ ì´ íŒŒì¼: {len(existing_files)}ê°œ")
        
        self.existing_checkpoints = existing_files
        return existing_files
    
    def check_huggingface_dependencies(self) -> bool:
        """Hugging Face ì˜ì¡´ì„± í™•ì¸"""
        try:
            import transformers
            import torch
            logger.info(f"âœ… transformers: {transformers.__version__}")
            logger.info(f"âœ… torch: {torch.__version__}")
            return True
        except ImportError as e:
            logger.error(f"âŒ ì˜ì¡´ì„± ëˆ„ë½: {e}")
            logger.info("ğŸ’¡ ì„¤ì¹˜ ëª…ë ¹:")
            logger.info("   conda install -c huggingface transformers")
            logger.info("   pip install huggingface_hub")
            return False
    
    def install_huggingface_dependencies(self) -> bool:
        """Hugging Face ì˜ì¡´ì„± ìë™ ì„¤ì¹˜"""
        logger.info("ğŸ“¦ Hugging Face ì˜ì¡´ì„± ì„¤ì¹˜ ì¤‘...")
        
        try:
            # transformers ì„¤ì¹˜
            result = subprocess.run([
                "conda", "install", "-y", "-c", "huggingface", "transformers"
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                logger.warning("condaë¡œ ì„¤ì¹˜ ì‹¤íŒ¨, pip ì‹œë„ ì¤‘...")
                subprocess.run([
                    "pip", "install", "transformers", "huggingface_hub", "accelerate"
                ], check=True)
            
            # ì„¤ì¹˜ í™•ì¸
            if self.check_huggingface_dependencies():
                logger.info("âœ… Hugging Face ì˜ì¡´ì„± ì„¤ì¹˜ ì™„ë£Œ")
                return True
            else:
                logger.error("âŒ ì˜ì¡´ì„± ì„¤ì¹˜ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ì˜ì¡´ì„± ì„¤ì¹˜ ì˜¤ë¥˜: {e}")
            return False
    
    def download_huggingface_model(self, model_info: ModelInfo) -> bool:
        """Hugging Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            from huggingface_hub import snapshot_download
            
            target_dir = self.base_dir / model_info.step / model_info.name
            target_dir.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"ğŸ“¥ HF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {model_info.hf_repo}")
            
            # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            downloaded_path = snapshot_download(
                repo_id=model_info.hf_repo,
                cache_dir=self.base_dir / "huggingface_cache",
                local_dir=target_dir,
                token=None  # ê³µê°œ ëª¨ë¸ë§Œ ì‚¬ìš©
            )
            
            # ë‹¤ìš´ë¡œë“œ ê²€ì¦
            if Path(downloaded_path).exists():
                # í¬ê¸° í™•ì¸
                total_size = 0
                for file_path in Path(downloaded_path).rglob("*"):
                    if file_path.is_file():
                        total_size += file_path.stat().st_size
                
                size_mb = total_size / (1024 * 1024)
                logger.info(f"âœ… {model_info.name} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {size_mb:.1f}MB")
                
                model_info.size_mb = size_mb
                model_info.path = str(target_dir)
                model_info.status = "downloaded"
                
                return True
            else:
                logger.error(f"âŒ {model_info.name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            logger.error(f"âŒ {model_info.name} HF ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def create_model_symlinks(self) -> bool:
        """ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ë“¤ì„ ìŠ¤í…ë³„ ë””ë ‰í† ë¦¬ì— ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±"""
        logger.info("ğŸ”— ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì¤‘...")
        
        # ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ê²€ìƒ‰ ê²½ë¡œë“¤
        search_dirs = [
            self.backend_dir / "ai_models",
            Path(__file__).parent / "backend" / "ai_models",
            Path(__file__).parent.parent / "ai_models"
        ]
        
        found_files = []
        
        # ê° ë””ë ‰í† ë¦¬ì—ì„œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ìƒ‰
        for search_dir in search_dirs:
            if search_dir.exists():
                logger.info(f"ğŸ” ê²€ìƒ‰ ì¤‘: {search_dir}")
                
                # ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  ëª¨ë¸ íŒŒì¼ ì°¾ê¸°
                for ext in ["*.pth", "*.bin", "*.safetensors", "*.ckpt"]:
                    try:
                        for file_path in search_dir.rglob(ext):
                            if file_path.is_file() and file_path.stat().st_size > 100 * 1024 * 1024:  # 100MB ì´ìƒ
                                found_files.append(file_path)
                                logger.debug(f"   ë°œê²¬: {file_path.name} ({file_path.stat().st_size / (1024*1024):.1f}MB)")
                    except Exception as e:
                        logger.warning(f"âš ï¸ {search_dir} ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        logger.info(f"ğŸ“¦ ë°œê²¬ëœ ì²´í¬í¬ì¸íŠ¸: {len(found_files)}ê°œ")
        
        # íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ ìŠ¤í… ë””ë ‰í† ë¦¬ì— ë§í¬ ìƒì„±
        linking_rules = {
            r".*schp.*|.*parsing.*|.*graphonomy.*": "step_01_human_parsing",
            r".*pose.*|.*openpose.*|.*keypoint.*": "step_02_pose_estimation", 
            r".*u2net.*|.*sam.*|.*segment.*": "step_03_cloth_segmentation",
            r".*viton.*|.*ootd.*|.*diffusion.*": "step_06_virtual_fitting",
            r".*tom.*|.*warp.*|.*tps.*": "step_05_cloth_warping",
            r".*clip.*": "auxiliary"
        }
        
        linked_count = 0
        for file_path in found_files:
            file_name = file_path.name.lower()
            
            for pattern, step_dir in linking_rules.items():
                import re
                if re.search(pattern, file_name):
                    link_dir = self.base_dir / step_dir
                    link_dir.mkdir(parents=True, exist_ok=True)
                    link_path = link_dir / file_path.name
                    
                    if not link_path.exists():
                        try:
                            link_path.symlink_to(file_path.absolute())
                            size_mb = file_path.stat().st_size / (1024 * 1024)
                            logger.info(f"ğŸ”— ë§í¬ ìƒì„±: {file_path.name} â†’ {step_dir} ({size_mb:.1f}MB)")
                            linked_count += 1
                        except Exception as e:
                            logger.warning(f"âš ï¸ ë§í¬ ìƒì„± ì‹¤íŒ¨ {file_path.name}: {e}")
                    break
        
        logger.info(f"âœ… ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì™„ë£Œ: {linked_count}ê°œ")
        return linked_count > 0
    
    def create_model_config(self) -> bool:
        """ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
        config = {
            "model_status": {
                "last_updated": time.strftime("%Y-%m-%d %H:%M:%S"),
                "conda_env": self.conda_env,
                "base_directory": str(self.base_dir),
                "total_models": len(self.models),
                "existing_checkpoints": len(self.existing_checkpoints)
            },
            "models": {},
            "existing_checkpoints": self.existing_checkpoints,
            "step_mappings": {
                "step_01_human_parsing": ["human_parsing_cdgp", "human_parsing_schp"],
                "step_02_pose_estimation": ["pose_mediapipe", "pose_openpose"],
                "step_03_cloth_segmentation": ["cloth_segment_u2net", "cloth_segment_sam"],
                "step_06_virtual_fitting": ["virtual_fitting_ootd", "virtual_fitting_idm"],
                "auxiliary": ["clip_embedder"]
            }
        }
        
        # ëª¨ë¸ ì •ë³´ ì¶”ê°€
        for model_name, model_info in self.models.items():
            config["models"][model_name] = {
                "name": model_info.name,
                "source": model_info.source,
                "hf_repo": model_info.hf_repo,
                "step": model_info.step,
                "size_mb": model_info.size_mb,
                "status": model_info.status,
                "priority": model_info.priority,
                "path": model_info.path
            }
        
        # ì„¤ì • íŒŒì¼ ì €ì¥
        config_path = self.base_dir / "model_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
        return True
    
    async def setup_essential_models(self, download_hf: bool = True) -> Dict[str, bool]:
        """í•„ìˆ˜ ëª¨ë¸ë“¤ ì„¤ì •"""
        results = {}
        
        # 1. ì˜ì¡´ì„± í™•ì¸/ì„¤ì¹˜
        if download_hf:
            if not self.check_huggingface_dependencies():
                if not self.install_huggingface_dependencies():
                    logger.error("âŒ Hugging Face ì˜ì¡´ì„± ì„¤ì¹˜ ì‹¤íŒ¨")
                    return {}
        
        # 2. ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„
        self.analyze_existing_checkpoints()
        
        # 3. ê¸°ì¡´ íŒŒì¼ë“¤ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
        self.create_model_symlinks()
        
        # 4. ìš°ì„ ìˆœìœ„ ë†’ì€ HF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        if download_hf:
            priority_models = [model for model in self.models.values() if model.priority <= 2]
            
            for model_info in priority_models:
                logger.info(f"ğŸ“¥ {model_info.name} ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
                success = self.download_huggingface_model(model_info)
                results[model_info.name] = success
        
        # 5. ì„¤ì • íŒŒì¼ ìƒì„±
        self.create_model_config()
        
        # 6. ìš”ì•½ ì¶œë ¥
        self._print_setup_summary(results)
        
        return results
    
    def _print_setup_summary(self, results: Dict[str, bool]):
        """ì„¤ì • ìš”ì•½ ì¶œë ¥"""
        logger.info("=" * 80)
        logger.info("ğŸ“Š AI ëª¨ë¸ ì„¤ì • ì™„ë£Œ ìš”ì•½")
        logger.info("=" * 80)
        
        # HF ëª¨ë¸ ê²°ê³¼
        if results:
            success_count = sum(results.values())
            total_count = len(results)
            logger.info(f"ğŸ¤— Hugging Face ëª¨ë¸: {success_count}/{total_count}ê°œ ì„±ê³µ")
            
            for model_name, success in results.items():
                status = "âœ…" if success else "âŒ"
                logger.info(f"   {status} {model_name}")
        
        # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸
        if self.existing_checkpoints:
            logger.info(f"ğŸ“¦ ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸: {len(self.existing_checkpoints)}ê°œ ë°œê²¬")
            total_size = sum(info["size_gb"] for info in self.existing_checkpoints.values())
            logger.info(f"ğŸ’¾ ê¸°ì¡´ íŒŒì¼ ì´ í¬ê¸°: {total_size:.1f}GB")
        
        # ë‹¤ìŒ ë‹¨ê³„
        logger.info("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info("1. cd backend && python -m app.main  # ì„œë²„ ì‹¤í–‰ í…ŒìŠ¤íŠ¸")
        logger.info("2. ë¡œê·¸ í™•ì¸í•˜ì—¬ ì²´í¬í¬ì¸íŠ¸ ì˜¤ë¥˜ í•´ê²° ì—¬ë¶€ í™•ì¸")
        logger.info("3. ê°œë³„ ìŠ¤í… í…ŒìŠ¤íŠ¸: python -m app.ai_pipeline.steps.step_01_human_parsing")

# =============================================================================
# ğŸš€ CLI ì¸í„°í˜ì´ìŠ¤
# =============================================================================

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ì‹¤ì œ ì‘ë™í•˜ëŠ” AI ëª¨ë¸ ë‹¤ìš´ë¡œë”")
    parser.add_argument("--conda", action="store_true", help="conda í™˜ê²½ ìµœì í™”")
    parser.add_argument("--essential", action="store_true", help="í•„ìˆ˜ ëª¨ë¸ë§Œ ì„¤ì •")
    parser.add_argument("--analyze-existing", action="store_true", help="ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ë§Œ ë¶„ì„")
    parser.add_argument("--huggingface-only", action="store_true", help="HF ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--no-download", action="store_true", help="ë‹¤ìš´ë¡œë“œ ì—†ì´ ë¶„ì„ë§Œ")
    parser.add_argument("--base-dir", type=str, help="ê¸°ë³¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    # ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì„¤ì •
    base_dir = Path(args.base_dir) if args.base_dir else None
    
    # ë‹¤ìš´ë¡œë” ì´ˆê¸°í™”
    downloader = RealModelDownloader(base_dir=base_dir)
    
    if args.analyze_existing:
        # ê¸°ì¡´ íŒŒì¼ë§Œ ë¶„ì„
        downloader.analyze_existing_checkpoints()
        downloader.create_model_symlinks()
        downloader.create_model_config()
    
    elif args.essential or args.huggingface_only:
        # í•„ìˆ˜ ëª¨ë¸ ì„¤ì •
        download_hf = not args.no_download
        await downloader.setup_essential_models(download_hf=download_hf)
    
    else:
        parser.print_help()

if __name__ == "__main__":
    # conda í™˜ê²½ í™•ì¸
    conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
    if conda_env:
        print(f"ğŸ conda í™˜ê²½: {conda_env}")
    else:
        print("âš ï¸ conda í™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•ŠìŒ")
    
    # ì‹¤í–‰
    asyncio.run(main())