#!/usr/bin/env python3
"""
ğŸ” MyCloset AI - ìˆ˜ì •ëœ ëª¨ë¸ ìë™ ìŠ¤ìº” ìŠ¤í¬ë¦½íŠ¸ (macOS ìµœì í™”)
================================================================

macOS ê¶Œí•œ ë¬¸ì œ í•´ê²° ë° ì•ˆì „í•œ ê²½ë¡œ ìŠ¤ìº”ìœ¼ë¡œ ìˆ˜ì •

ì§€ì› ëª¨ë¸ í˜•ì‹:
- PyTorch: .pth, .pt, .bin
- TensorFlow: .pb, .h5, .tflite  
- ONNX: .onnx
- Safetensors: .safetensors
- Pickle: .pkl, .p
- Caffe: .caffemodel, .prototxt

ì‚¬ìš©ë²•:
    python scan_models.py                    # ì•ˆì „í•œ ê¸°ë³¸ ìŠ¤ìº”
    python scan_models.py --safe             # ì•ˆì „ ëª¨ë“œ (ê¶Œí•œ ë¬¸ì œ ë°©ì§€)
    python scan_models.py --path ./ai_models # íŠ¹ì • ê²½ë¡œ ìŠ¤ìº”
    python scan_models.py --create-config    # ì„¤ì • íŒŒì¼ ìƒì„±
"""

import os
import sys
import time
import json
import hashlib
import argparse
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import platform
import glob

# ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ ê°„ë‹¨í•œ êµ¬í˜„
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    class tqdm:
        def __init__(self, iterable, **kwargs):
            self.iterable = iterable
            self.total = len(iterable) if hasattr(iterable, '__len__') else None
            self.desc = kwargs.get('desc', '')
            self.current = 0
            
        def __iter__(self):
            for item in self.iterable:
                yield item
                self.current += 1
                if self.total:
                    print(f"\r{self.desc}: {self.current}/{self.total} ({self.current/self.total*100:.1f}%)", end='', flush=True)
            print()

# ==============================================
# ğŸ¯ ëª¨ë¸ ì •ë³´ ë°ì´í„° í´ë˜ìŠ¤
# ==============================================

@dataclass
class ModelInfo:
    """AI ëª¨ë¸ ì •ë³´"""
    name: str
    path: str
    size_mb: float
    format: str
    framework: str
    created_time: str
    modified_time: str
    checksum: str
    is_valid: bool
    model_type: str = "unknown"
    step_candidate: str = "unknown"
    confidence: float = 0.0
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

# ==============================================
# ğŸ” ìˆ˜ì •ëœ ëª¨ë¸ ìŠ¤ìºë„ˆ í´ë˜ìŠ¤
# ==============================================

class AIModelScanner:
    """AI ëª¨ë¸ ìë™ ìŠ¤ìº” ë° ë¶„ë¥˜ (macOS ìµœì í™”)"""
    
    def __init__(self, verbose: bool = True, safe_mode: bool = True):
        self.verbose = verbose
        self.safe_mode = safe_mode
        self.found_models: List[ModelInfo] = []
        self.scan_stats = {
            'total_files_scanned': 0,
            'models_found': 0,
            'total_size_gb': 0.0,
            'scan_time': 0.0,
            'errors': []
        }
        
        # ìŠ¤ìº”í•  íŒŒì¼ í™•ì¥ì
        self.model_extensions = {
            '.pth': 'pytorch',
            '.pt': 'pytorch', 
            '.bin': 'pytorch',
            '.safetensors': 'safetensors',
            '.onnx': 'onnx',
            '.pb': 'tensorflow',
            '.h5': 'tensorflow',
            '.tflite': 'tensorflow',
            '.pkl': 'pickle',
            '.p': 'pickle',
            '.caffemodel': 'caffe',
            '.prototxt': 'caffe'
        }
        
        # MyCloset AI Stepë³„ ëª¨ë¸ íŒ¨í„´
        self.step_patterns = {
            'step_01_human_parsing': [
                'human.*parsing', 'graphonomy', 'schp', 'atr', 'lip',
                'parsing', 'segmentation.*human'
            ],
            'step_02_pose_estimation': [
                'pose.*estimation', 'openpose', 'mediapipe', 'pose.*net',
                'body.*pose', 'keypoint', 'skeleton'
            ],
            'step_03_cloth_segmentation': [
                'cloth.*seg', 'u2net', 'sam', 'segment.*anything',
                'mask.*rcnn', 'deeplabv3', 'segmentation.*cloth'
            ],
            'step_04_geometric_matching': [
                'geometric.*matching', 'gmm', 'tps', 'spatial.*transform',
                'warping.*grid', 'flow.*estimation'
            ],
            'step_05_cloth_warping': [
                'cloth.*warp', 'tom', 'viton.*warp', 'deformation',
                'elastic.*transform', 'thin.*plate.*spline'
            ],
            'step_06_virtual_fitting': [
                'virtual.*fitting', 'ootdiffusion', 'stable.*diffusion',
                'diffusion.*unet', 'text2img', 'img2img', 'viton',
                'try.*on', 'outfit'
            ],
            'step_07_post_processing': [
                'post.*process', 'enhancement', 'super.*resolution',
                'srresnet', 'esrgan', 'real.*esrgan', 'upscal',
                'denoise', 'refine'
            ],
            'step_08_quality_assessment': [
                'quality.*assessment', 'clip', 'aesthetic', 'scoring',
                'evaluation', 'metric', 'lpips', 'ssim'
            ]
        }
        
        # ì•ˆì „í•œ ìŠ¤ìº” ê²½ë¡œ (ê¶Œí•œ ë¬¸ì œ ë°©ì§€)
        self.safe_paths = self._get_safe_scan_paths()
        
    def _get_safe_scan_paths(self) -> List[str]:
        """ê¶Œí•œ ë¬¸ì œê°€ ì—†ëŠ” ì•ˆì „í•œ ìŠ¤ìº” ê²½ë¡œ"""
        home = Path.home()
        
        # ê¸°ë³¸ ì•ˆì „ ê²½ë¡œ
        safe_paths = [
            # í˜„ì¬ í”„ë¡œì íŠ¸
            "./ai_models",
            "./models", 
            "./checkpoints",
            "./weights",
            "./pretrained",
            ".",
            
            # ì‚¬ìš©ì ì ‘ê·¼ ê°€ëŠ¥ ê²½ë¡œ
            str(home / "Downloads"),
            str(home / "Documents"),
            str(home / "Desktop"),
            
            # Python/AI ê´€ë ¨ ìºì‹œ (ì•ˆì „í•¨)
            str(home / ".cache" / "huggingface"),
            str(home / ".cache" / "torch"),
            str(home / ".cache" / "transformers"),
            str(home / ".local" / "lib"),
        ]
        
        # macOS íŠ¹í™” ì•ˆì „ ê²½ë¡œ
        if platform.system().lower() == "darwin":
            safe_paths.extend([
                str(home / "Library" / "Caches" / "pip"),
                str(home / "Library" / "Caches" / "huggingface"),
                "/opt/homebrew/lib",  # Homebrew ì„¤ì¹˜ ê²½ë¡œ
                "/usr/local/lib",     # ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ
            ])
        
        # ì¡´ì¬í•˜ê³  ì½ê¸° ê°€ëŠ¥í•œ ê²½ë¡œë§Œ ë°˜í™˜
        valid_paths = []
        for path_str in safe_paths:
            path = Path(path_str)
            try:
                if path.exists() and os.access(path, os.R_OK):
                    valid_paths.append(path_str)
            except (PermissionError, OSError):
                continue
        
        return valid_paths
    
    def scan_system(
        self, 
        custom_paths: List[str] = None,
        deep_scan: bool = False,
        max_workers: int = 2  # macOSì—ì„œ ì•ˆì •ì„±ì„ ìœ„í•´ ì¤„ì„
    ) -> List[ModelInfo]:
        """ì‹œìŠ¤í…œ AI ëª¨ë¸ ìŠ¤ìº” (ì•ˆì „ ëª¨ë“œ)"""
        
        print("ğŸ” AI ëª¨ë¸ ìŠ¤ìº” ì‹œì‘...")
        print(f"ğŸ–¥ï¸ ì‹œìŠ¤í…œ: {platform.system()} {platform.release()}")
        
        if self.safe_mode:
            print("ğŸ›¡ï¸ ì•ˆì „ ëª¨ë“œ: ê¶Œí•œ ë¬¸ì œê°€ ìˆëŠ” ê²½ë¡œëŠ” ìë™ìœ¼ë¡œ ê±´ë„ˆëœ€")
        
        start_time = time.time()
        
        # ìŠ¤ìº” ê²½ë¡œ ê²°ì •
        if custom_paths:
            # ì‚¬ìš©ì ì§€ì • ê²½ë¡œ ê²€ì¦
            scan_paths = []
            for path_str in custom_paths:
                path = Path(path_str)
                if path.exists() and os.access(path, os.R_OK):
                    scan_paths.append(path)
                else:
                    print(f"âš ï¸ ì ‘ê·¼ ë¶ˆê°€ëŠ¥í•œ ê²½ë¡œ ê±´ë„ˆëœ€: {path_str}")
        elif deep_scan and not self.safe_mode:
            print("âš ï¸ ë”¥ ìŠ¤ìº”ì€ ê¶Œí•œ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. --safe ì˜µì…˜ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            scan_paths = self._get_deep_scan_paths()
        else:
            scan_paths = [Path(p) for p in self.safe_paths]
        
        print(f"ğŸ“‚ ìŠ¤ìº” ê²½ë¡œ: {len(scan_paths)}ê°œ")
        for i, path in enumerate(scan_paths):
            if i < 5:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                print(f"   - {path}")
            elif i == 5:
                print(f"   ... ì™¸ {len(scan_paths) - 5}ê°œ")
        
        # ì•ˆì „í•œ ìŠ¤ìº” ì‹¤í–‰
        all_files = []
        for path in scan_paths:
            try:
                files = self._find_model_files_safe(path)
                all_files.extend(files)
                if self.verbose:
                    print(f"ğŸ“ {path}: {len(files)}ê°œ íŒŒì¼ ë°œê²¬")
            except Exception as e:
                error_msg = f"ê²½ë¡œ ìŠ¤ìº” ì‹¤íŒ¨ {path}: {e}"
                self.scan_stats['errors'].append(error_msg)
                if self.verbose:
                    print(f"âš ï¸ {error_msg}")
        
        print(f"\nğŸ” ì´ {len(all_files)}ê°œ ëª¨ë¸ íŒŒì¼ ë°œê²¬, ë¶„ì„ ì‹œì‘...")
        
        if len(all_files) == 0:
            print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self._suggest_alternatives()
            return []
        
        # ìˆœì°¨ ì²˜ë¦¬ (ì•ˆì •ì„± ìš°ì„ )
        if len(all_files) > 100 or not self.safe_mode:
            # ë³‘ë ¬ ì²˜ë¦¬
            processed_files = self._process_files_parallel(all_files, max_workers)
        else:
            # ìˆœì°¨ ì²˜ë¦¬ (ë” ì•ˆì „)
            processed_files = self._process_files_sequential(all_files)
        
        # ìŠ¤ìº” í†µê³„ ì—…ë°ì´íŠ¸
        self.scan_stats['scan_time'] = time.time() - start_time
        self.scan_stats['models_found'] = len(self.found_models)
        self.scan_stats['total_size_gb'] = sum(m.size_mb for m in self.found_models) / 1024
        
        self._print_scan_results()
        return self.found_models
    
    def _process_files_sequential(self, files: List[Path]) -> int:
        """íŒŒì¼ ìˆœì°¨ ì²˜ë¦¬ (ì•ˆì „)"""
        processed = 0
        
        for i, file_path in enumerate(files):
            if self.verbose and i % 10 == 0:
                print(f"ğŸ” ì²˜ë¦¬ ì¤‘: {i+1}/{len(files)} ({(i+1)/len(files)*100:.1f}%)")
            
            try:
                model_info = self._analyze_model_file(file_path)
                if model_info and model_info.is_valid:
                    self.found_models.append(model_info)
                processed += 1
                self.scan_stats['total_files_scanned'] += 1
            except Exception as e:
                error_msg = f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}"
                self.scan_stats['errors'].append(error_msg)
        
        return processed
    
    def _process_files_parallel(self, files: List[Path], max_workers: int) -> int:
        """íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬"""
        processed = 0
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(self._analyze_model_file, file_path): file_path 
                      for file_path in files}
            
            if TQDM_AVAILABLE:
                progress = tqdm(as_completed(futures), total=len(futures), desc="ëª¨ë¸ ë¶„ì„")
            else:
                progress = as_completed(futures)
                
            for future in progress:
                try:
                    model_info = future.result()
                    if model_info and model_info.is_valid:
                        self.found_models.append(model_info)
                    processed += 1
                    self.scan_stats['total_files_scanned'] += 1
                except Exception as e:
                    error_msg = f"ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {e}"
                    self.scan_stats['errors'].append(error_msg)
        
        return processed
    
    def _get_deep_scan_paths(self) -> List[Path]:
        """ë”¥ ìŠ¤ìº”ìš© ê²½ë¡œ (ì£¼ì˜: ê¶Œí•œ ë¬¸ì œ ê°€ëŠ¥)"""
        system = platform.system().lower()
        
        if system == "darwin":  # macOS
            return [
                Path("/Users"),  # ì „ì²´ /ë¥¼ í”¼í•˜ê³  Usersë§Œ
                Path("/opt"),
                Path("/usr/local")
            ]
        elif system == "linux":
            return [Path("/home"), Path("/opt"), Path("/usr")]
        else:
            return [Path.home()]
    
    def _find_model_files_safe(self, root_path: Path) -> List[Path]:
        """ì•ˆì „í•œ ëª¨ë¸ íŒŒì¼ ì°¾ê¸° (ê¶Œí•œ ì˜¤ë¥˜ ë°©ì§€)"""
        model_files = []
        
        try:
            # ê° í™•ì¥ìë³„ë¡œ glob íŒ¨í„´ ì‚¬ìš© (ë” ì•ˆì „)
            for ext in self.model_extensions.keys():
                try:
                    # ì¬ê·€ì  glob ì‚¬ìš©
                    pattern = str(root_path / f"**/*{ext}")
                    files = glob.glob(pattern, recursive=True)
                    
                    for file_str in files:
                        file_path = Path(file_str)
                        if file_path.is_file() and os.access(file_path, os.R_OK):
                            model_files.append(file_path)
                
                except (PermissionError, OSError) as e:
                    if self.verbose:
                        print(f"âš ï¸ {ext} íŒŒì¼ ê²€ìƒ‰ ì¤‘ ê¶Œí•œ ì˜¤ë¥˜: {e}")
                    continue
                
        except (PermissionError, OSError) as e:
            if self.verbose:
                print(f"âš ï¸ ê²½ë¡œ ì ‘ê·¼ ê¶Œí•œ ì˜¤ë¥˜ {root_path}: {e}")
        
        return model_files
    
    def _analyze_model_file(self, file_path: Path) -> Optional[ModelInfo]:
        """ê°œë³„ ëª¨ë¸ íŒŒì¼ ë¶„ì„ (ì•ˆì „ ì²˜ë¦¬)"""
        try:
            # íŒŒì¼ ì ‘ê·¼ ê¶Œí•œ í™•ì¸
            if not os.access(file_path, os.R_OK):
                return None
            
            # ê¸°ë³¸ íŒŒì¼ ì •ë³´
            stat = file_path.stat()
            size_mb = stat.st_size / (1024 * 1024)
            
            # ë„ˆë¬´ ì‘ì€ íŒŒì¼ì€ ì œì™¸ (0.5MB ë¯¸ë§Œ)
            if size_mb < 0.5:
                return None
            
            # íŒŒì¼ í˜•ì‹ í™•ì¸
            suffix = file_path.suffix.lower()
            framework = self.model_extensions.get(suffix, "unknown")
            
            # ì²´í¬ì„¬ ê³„ì‚° (ì•ˆì „í•˜ê²Œ)
            checksum = self._calculate_checksum_safe(file_path, size_mb)
            
            # Step ë¶„ë¥˜
            step_candidate, confidence = self._classify_model_step(file_path.name.lower())
            
            # ëª¨ë¸ íƒ€ì… ì¶”ë¡ 
            model_type = self._infer_model_type(file_path, framework)
            
            # ê²€ì¦ (ì•ˆì „í•˜ê²Œ)
            is_valid = self._validate_model_file_safe(file_path, framework)
            
            model_info = ModelInfo(
                name=file_path.name,
                path=str(file_path.absolute()),
                size_mb=round(size_mb, 2),
                format=suffix,
                framework=framework,
                created_time=time.ctime(stat.st_ctime),
                modified_time=time.ctime(stat.st_mtime),
                checksum=checksum,
                is_valid=is_valid,
                model_type=model_type,
                step_candidate=step_candidate,
                confidence=confidence,
                metadata={
                    'parent_dir': file_path.parent.name,
                    'depth': len(file_path.parts),
                    'has_config': self._check_config_files_safe(file_path.parent)
                }
            )
            
            return model_info
            
        except (PermissionError, OSError, IOError) as e:
            # ê¶Œí•œ ê´€ë ¨ ì˜¤ë¥˜ëŠ” ì¡°ìš©íˆ ë¬´ì‹œ
            return None
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    def _calculate_checksum_safe(self, file_path: Path, size_mb: float) -> str:
        """ì•ˆì „í•œ ì²´í¬ì„¬ ê³„ì‚°"""
        try:
            hasher = hashlib.md5()
            
            with open(file_path, 'rb') as f:
                if size_mb > 50:  # 50MB ì´ìƒì€ ìƒ˜í”Œë§
                    # ì²˜ìŒ 512KB
                    chunk = f.read(512 * 1024)
                    if chunk:
                        hasher.update(chunk)
                    
                    # ì¤‘ê°„ ì§€ì 
                    try:
                        f.seek(int(size_mb * 1024 * 512))  # ì¤‘ê°„ ì§€ì 
                        chunk = f.read(512 * 1024)
                        if chunk:
                            hasher.update(chunk)
                    except:
                        pass
                    
                    # ë 512KB
                    try:
                        f.seek(-512 * 1024, 2)
                        chunk = f.read(512 * 1024)
                        if chunk:
                            hasher.update(chunk)
                    except:
                        pass
                else:
                    # ì‘ì€ íŒŒì¼ì€ ì „ì²´ ì½ê¸°
                    while True:
                        chunk = f.read(8192)
                        if not chunk:
                            break
                        hasher.update(chunk)
            
            return hasher.hexdigest()[:12]
            
        except Exception:
            return "unknown"
    
    def _classify_model_step(self, filename: str) -> Tuple[str, float]:
        """íŒŒì¼ëª…ìœ¼ë¡œ Step ë¶„ë¥˜"""
        best_step = "unknown"
        best_confidence = 0.0
        
        for step_name, patterns in self.step_patterns.items():
            confidence = 0.0
            
            for pattern in patterns:
                # ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­
                import re
                try:
                    if re.search(pattern.replace('.*', '.*?'), filename, re.IGNORECASE):
                        confidence = max(confidence, 0.8)
                    elif pattern.lower() in filename:
                        confidence = max(confidence, 0.6)
                    elif any(word in filename for word in pattern.split('.*') if word):
                        confidence = max(confidence, 0.4)
                except:
                    # ì •ê·œì‹ ì˜¤ë¥˜ ì‹œ ë‹¨ìˆœ ë¬¸ìì—´ ë§¤ì¹­
                    if pattern.lower() in filename:
                        confidence = max(confidence, 0.5)
            
            if confidence > best_confidence:
                best_confidence = confidence
                best_step = step_name
        
        return best_step, best_confidence
    
    def _infer_model_type(self, file_path: Path, framework: str) -> str:
        """ëª¨ë¸ íƒ€ì… ì¶”ë¡ """
        filename = file_path.name.lower()
        
        # ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ íŒ¨í„´
        if any(arch in filename for arch in ['resnet', 'vgg', 'densenet', 'efficientnet']):
            return "cnn_backbone"
        elif any(arch in filename for arch in ['transformer', 'bert', 'gpt', 'clip']):
            return "transformer"
        elif any(arch in filename for arch in ['unet', 'vae', 'diffusion']):
            return "generative"
        elif any(arch in filename for arch in ['pose', 'keypoint', 'openpose']):
            return "pose_estimation"
        elif any(arch in filename for arch in ['segment', 'mask', 'parsing']):
            return "segmentation"
        else:
            return "unknown"
    
    def _validate_model_file_safe(self, file_path: Path, framework: str) -> bool:
        """ì•ˆì „í•œ ëª¨ë¸ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            # íŒŒì¼ í¬ê¸° ê¸°ë°˜ ê¸°ë³¸ ê²€ì¦
            file_size = file_path.stat().st_size
            if file_size < 1024:  # 1KB ë¯¸ë§Œì€ ìœ íš¨í•˜ì§€ ì•ŠìŒ
                return False
            
            # í™•ì¥ì ê¸°ë°˜ ê²€ì¦
            suffix = file_path.suffix.lower()
            if suffix in ['.pth', '.pt', '.bin']:
                return self._validate_pytorch_model_safe(file_path)
            elif suffix in ['.pb', '.h5']:
                return self._validate_tensorflow_model_safe(file_path)
            elif suffix == '.onnx':
                return self._validate_onnx_model_safe(file_path)
            else:
                return True  # ë‹¤ë¥¸ í˜•ì‹ì€ ì¼ë‹¨ ìœ íš¨ë¡œ ê°„ì£¼
                
        except Exception:
            return False
    
    def _validate_pytorch_model_safe(self, file_path: Path) -> bool:
        """ì•ˆì „í•œ PyTorch ëª¨ë¸ ê²€ì¦"""
        try:
            # í—¤ë”ë§Œ ì½ì–´ì„œ ë¹ ë¥¸ ê²€ì¦
            with open(file_path, 'rb') as f:
                header = f.read(1024)
            
            # PyTorch/Pickle ë§¤ì§ ë°”ì´íŠ¸ í™•ì¸
            pytorch_markers = [b'PK', b'\x80', b'PYTORCH', b'PICKLE']
            if any(marker in header for marker in pytorch_markers):
                return True
            
            # torch ëª¨ë“ˆì´ ìˆìœ¼ë©´ ì‹¤ì œ ë¡œë“œ ì‹œë„ (ì‘ì€ íŒŒì¼ë§Œ)
            try:
                import torch
                if file_path.stat().st_size < 10 * 1024 * 1024:  # 10MB ë¯¸ë§Œ
                    torch.load(file_path, map_location='cpu')
                    return True
            except ImportError:
                pass
            except Exception:
                pass
            
            return True  # í™•ì¥ìê°€ ë§ìœ¼ë©´ ì¼ë‹¨ ìœ íš¨ë¡œ ê°„ì£¼
            
        except Exception:
            return False
    
    def _validate_tensorflow_model_safe(self, file_path: Path) -> bool:
        """ì•ˆì „í•œ TensorFlow ëª¨ë¸ ê²€ì¦"""
        try:
            suffix = file_path.suffix.lower()
            
            if suffix == '.pb':
                # Protocol Buffer ë§¤ì§ ë°”ì´íŠ¸ í™•ì¸
                with open(file_path, 'rb') as f:
                    header = f.read(100)
                return len(header) > 10  # ê¸°ë³¸ì ì¸ í¬ê¸° í™•ì¸
                
            elif suffix == '.h5':
                # HDF5 ë§¤ì§ ë°”ì´íŠ¸ í™•ì¸
                with open(file_path, 'rb') as f:
                    header = f.read(8)
                return header.startswith(b'\x89HDF')
                
            return True
            
        except Exception:
            return False
    
    def _validate_onnx_model_safe(self, file_path: Path) -> bool:
        """ì•ˆì „í•œ ONNX ëª¨ë¸ ê²€ì¦"""
        try:
            with open(file_path, 'rb') as f:
                header = f.read(100)
            
            # ONNX ê´€ë ¨ ë°”ì´íŠ¸ íŒ¨í„´ í™•ì¸
            return b'onnx' in header.lower() or len(header) > 50
            
        except Exception:
            return False
    
    def _check_config_files_safe(self, directory: Path) -> bool:
        """ì•ˆì „í•œ ì„¤ì • íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        try:
            config_patterns = [
                'config.json', 'config.yaml', 'config.yml',
                'model_config.json', 'tokenizer.json',
                'pytorch_model.bin', 'model.safetensors'
            ]
            
            for pattern in config_patterns:
                config_file = directory / pattern
                if config_file.exists() and os.access(config_file, os.R_OK):
                    return True
            
            return False
            
        except (PermissionError, OSError):
            return False
    
    def _suggest_alternatives(self):
        """ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ëŒ€ì•ˆ ì œì•ˆ"""
        print("\nğŸ’¡ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ ì‹œë„í•´ë³´ì„¸ìš”:")
        print("   1. íŠ¹ì • ê²½ë¡œ ì§€ì •: python scan_models.py --path ~/Downloads")
        print("   2. í”„ë¡œì íŠ¸ í´ë”ë§Œ: python scan_models.py --path ./ai_models")
        print("   3. Hugging Face ìºì‹œ: python scan_models.py --path ~/.cache/huggingface")
        print("   4. ìµœì†Œ í¬ê¸° ì¤„ì´ê¸°: ìŠ¤í¬ë¦½íŠ¸ ë‚´ size_mb < 0.1 ë¡œ ìˆ˜ì •")
        print("\nğŸ“ ì¼ë°˜ì ì¸ ëª¨ë¸ ìœ„ì¹˜:")
        print("   - ./ai_models/")
        print("   - ./models/") 
        print("   - ~/Downloads/")
        print("   - ~/.cache/huggingface/")
        print("   - ~/.cache/torch/")
    
    def _print_scan_results(self):
        """ìŠ¤ìº” ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"ğŸ‰ AI ëª¨ë¸ ìŠ¤ìº” ì™„ë£Œ!")
        print(f"{'='*60}")
        
        stats = self.scan_stats
        print(f"ğŸ“Š ìŠ¤ìº” í†µê³„:")
        print(f"   - ìŠ¤ìº” ì‹œê°„: {stats['scan_time']:.1f}ì´ˆ")
        print(f"   - ê²€ì‚¬í•œ íŒŒì¼: {stats['total_files_scanned']:,}ê°œ")
        print(f"   - ë°œê²¬í•œ ëª¨ë¸: {stats['models_found']:,}ê°œ")
        print(f"   - ì´ í¬ê¸°: {stats['total_size_gb']:.2f}GB")
        
        if stats['errors']:
            print(f"   - ê²½ê³ /ì˜¤ë¥˜: {len(stats['errors'])}ê°œ")
        
        if len(self.found_models) == 0:
            return
        
        # Stepë³„ ë¶„ë¥˜ ê²°ê³¼
        print(f"\nğŸ¯ Stepë³„ ëª¨ë¸ ë¶„ë¥˜:")
        step_counts = {}
        for model in self.found_models:
            step = model.step_candidate
            if step not in step_counts:
                step_counts[step] = 0
            step_counts[step] += 1
        
        step_names = {
            'step_01_human_parsing': 'Human Parsing',
            'step_02_pose_estimation': 'Pose Estimation',
            'step_03_cloth_segmentation': 'Cloth Segmentation',
            'step_04_geometric_matching': 'Geometric Matching',
            'step_05_cloth_warping': 'Cloth Warping',
            'step_06_virtual_fitting': 'Virtual Fitting',
            'step_07_post_processing': 'Post Processing',
            'step_08_quality_assessment': 'Quality Assessment'
        }
        
        for step, count in sorted(step_counts.items()):
            if count > 0:
                display_name = step_names.get(step, step)
                print(f"   - {display_name}: {count}ê°œ")
        
        # í”„ë ˆì„ì›Œí¬ë³„ í†µê³„
        framework_counts = {}
        for model in self.found_models:
            fw = model.framework
            framework_counts[fw] = framework_counts.get(fw, 0) + 1
        
        print(f"\nğŸ”§ í”„ë ˆì„ì›Œí¬ë³„ ë¶„ë¥˜:")
        for fw, count in framework_counts.items():
            print(f"   - {fw}: {count}ê°œ")
        
        # ìƒìœ„ ëª¨ë¸ë“¤ í‘œì‹œ
        print(f"\nğŸ† ë°œê²¬ëœ ì£¼ìš” ëª¨ë¸ë“¤:")
        sorted_models = sorted(self.found_models, key=lambda x: x.size_mb, reverse=True)
        
        for i, model in enumerate(sorted_models[:10]):
            print(f"   {i+1:2d}. {model.name}")
            print(f"       ğŸ“ {model.path}")
            print(f"       ğŸ“Š {model.size_mb:.1f}MB | {model.framework}")
            if model.confidence > 0.5:
                step_display = step_names.get(model.step_candidate, model.step_candidate)
                print(f"       ğŸ¯ {step_display} (ì‹ ë¢°ë„: {model.confidence:.1f})")
            print()
    
    def generate_config_files(self, output_dir: str = "."):
        """ì„¤ì • íŒŒì¼ë“¤ ìƒì„±"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        print(f"\nğŸ“ ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘... -> {output_path}")
        
        # JSON ìš”ì•½ íŒŒì¼
        self._generate_json_summary(output_path)
        
        # Python ì„¤ì • íŒŒì¼  
        self._generate_python_config(output_path)
        
        # YAML ì„¤ì • íŒŒì¼ (PyYAML ìˆì„ ë•Œë§Œ)
        self._generate_yaml_config(output_path)
        
        # Shell í™˜ê²½ ë³€ìˆ˜ íŒŒì¼
        self._generate_env_file(output_path)
        
        print(f"âœ… ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ!")
    
    def _generate_json_summary(self, output_path: Path):
        """JSON ìš”ì•½ íŒŒì¼ ìƒì„±"""
        summary = {
            'scan_info': {
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'system': platform.system(),
                'python_version': platform.python_version(),
                'total_models': len(self.found_models),
                'total_size_gb': self.scan_stats['total_size_gb'],
                'scan_time': self.scan_stats['scan_time']
            },
            'models': [asdict(model) for model in self.found_models],
            'statistics': self.scan_stats
        }
        
        json_file = output_path / "discovered_models.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"   âœ… JSON ìš”ì•½: {json_file}")
    
    def _generate_python_config(self, output_path: Path):
        """Python ì„¤ì • íŒŒì¼ ìƒì„±"""
        
        config_content = f'''"""
AI ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ìë™ ìƒì„±)
Generated by MyCloset AI Model Scanner
ìŠ¤ìº” ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}
"""

from pathlib import Path
from typing import Dict, List, Optional

# ìŠ¤ìº” ì •ë³´
SCAN_TIMESTAMP = "{time.strftime('%Y-%m-%d %H:%M:%S')}"
TOTAL_MODELS_FOUND = {len(self.found_models)}
TOTAL_SIZE_GB = {self.scan_stats['total_size_gb']:.2f}

# ë°œê²¬ëœ ëª¨ë¸ë“¤
DISCOVERED_MODELS = {{
'''
        
        # Stepë³„ ëª¨ë¸ ë¶„ë¥˜
        step_models = {}
        for model in self.found_models:
            step = model.step_candidate
            if step not in step_models:
                step_models[step] = []
            step_models[step].append(model)
        
        for step_name, models in step_models.items():
            config_content += f'    "{step_name}": [\n'
            
            for model in models:
                config_content += f'''        {{
            "name": "{model.name}",
            "path": Path(r"{model.path}"),
            "size_mb": {model.size_mb},
            "framework": "{model.framework}",
            "confidence": {model.confidence:.2f},
            "checksum": "{model.checksum}",
            "model_type": "{model.model_type}"
        }},
'''
            config_content += '    ],\n'
        
        config_content += '''}

def get_models_for_step(step_name: str) -> List[Dict]:
    """íŠ¹ì • Stepì˜ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return DISCOVERED_MODELS.get(step_name, [])

def get_best_model_for_step(step_name: str) -> Optional[Dict]:
    """íŠ¹ì • Stepì˜ ìµœì  ëª¨ë¸ ë°˜í™˜"""
    models = get_models_for_step(step_name)
    if not models:
        return None
    
    # ì‹ ë¢°ë„ì™€ í¬ê¸°ë¥¼ ê³ ë ¤í•´ì„œ ìµœì  ëª¨ë¸ ì„ íƒ
    def score_model(m):
        confidence_score = m["confidence"]
        size_score = min(m["size_mb"] / 100, 1.0)  # 100MBë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
        return confidence_score * 0.7 + size_score * 0.3
    
    best_model = max(models, key=score_model)
    return best_model

def get_all_model_paths() -> List[str]:
    """ëª¨ë“  ëª¨ë¸ ê²½ë¡œ ëª©ë¡ ë°˜í™˜"""
    paths = []
    for step_models in DISCOVERED_MODELS.values():
        for model in step_models:
            paths.append(str(model["path"]))
    return paths

def validate_model_exists(model_dict: Dict) -> bool:
    """ëª¨ë¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    return model_dict["path"].exists()

def list_models_by_framework(framework: str) -> List[Dict]:
    """í”„ë ˆì„ì›Œí¬ë³„ ëª¨ë¸ ëª©ë¡"""
    result = []
    for step_models in DISCOVERED_MODELS.values():
        for model in step_models:
            if model["framework"] == framework:
                result.append(model)
    return result

def get_largest_models(limit: int = 5) -> List[Dict]:
    """í¬ê¸°ê°€ í° ëª¨ë¸ë“¤ ë°˜í™˜"""
    all_models = []
    for step_models in DISCOVERED_MODELS.values():
        all_models.extend(step_models)
    
    return sorted(all_models, key=lambda m: m["size_mb"], reverse=True)[:limit]

# í¸ì˜ í•¨ìˆ˜ë“¤
def print_scan_summary():
    """ìŠ¤ìº” ìš”ì•½ ì¶œë ¥"""
    print(f"ğŸ” AI ëª¨ë¸ ìŠ¤ìº” ê²°ê³¼ (ìŠ¤ìº” ì‹œê°„: {{SCAN_TIMESTAMP}})")
    print(f"ğŸ“Š ì´ {{TOTAL_MODELS_FOUND}}ê°œ ëª¨ë¸ ë°œê²¬ ({{TOTAL_SIZE_GB:.2f}}GB)")
    print()
    
    for step_name, models in DISCOVERED_MODELS.items():
        if models and step_name != "unknown":
            print(f"  ğŸ¯ {{step_name}}: {{len(models)}}ê°œ")
            for model in models[:3]:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                print(f"     - {{model['name']}} ({{model['size_mb']}}MB)")
            if len(models) > 3:
                print(f"     ... ì™¸ {{len(models)-3}}ê°œ")
            print()

if __name__ == "__main__":
    print_scan_summary()
'''
        
        py_file = output_path / "model_paths.py"
        with open(py_file, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        print(f"   âœ… Python ì„¤ì •: {py_file}")
    
    def _generate_yaml_config(self, output_path: Path):
        """YAML ì„¤ì • íŒŒì¼ ìƒì„± (ì„ íƒì )"""
        try:
            import yaml
            
            # Stepë³„ ëª¨ë¸ ê·¸ë£¹í™”
            step_models = {}
            for model in self.found_models:
                step = model.step_candidate
                if step not in step_models:
                    step_models[step] = []
                
                step_models[step].append({
                    'name': model.name,
                    'path': model.path,
                    'size_mb': model.size_mb,
                    'framework': model.framework,
                    'confidence': model.confidence,
                    'checksum': model.checksum,
                    'model_type': model.model_type
                })
            
            yaml_config = {
                'scan_info': {
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'total_models': len(self.found_models),
                    'total_size_gb': round(self.scan_stats['total_size_gb'], 2),
                    'scan_time_seconds': round(self.scan_stats['scan_time'], 1)
                },
                'models_by_step': step_models
            }
            
            yaml_file = output_path / "models_config.yaml"
            with open(yaml_file, 'w', encoding='utf-8') as f:
                yaml.dump(yaml_config, f, default_flow_style=False, allow_unicode=True)
            
            print(f"   âœ… YAML ì„¤ì •: {yaml_file}")
            
        except ImportError:
            print("   âš ï¸ PyYAMLì´ ì—†ì–´ YAML íŒŒì¼ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            print("      ì„¤ì¹˜: pip install pyyaml")
    
    def _generate_env_file(self, output_path: Path):
        """í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ìƒì„±"""
        
        env_content = f"""#!/bin/bash
# AI ëª¨ë¸ í™˜ê²½ ë³€ìˆ˜ (ìë™ ìƒì„±)
# Generated by MyCloset AI Model Scanner on {time.strftime('%Y-%m-%d %H:%M:%S')}

# ìŠ¤ìº” ì •ë³´
export MYCLOSET_MODELS_SCAN_DATE="{time.strftime('%Y-%m-%d')}"
export MYCLOSET_TOTAL_MODELS="{len(self.found_models)}"
export MYCLOSET_TOTAL_SIZE_GB="{self.scan_stats['total_size_gb']:.2f}"

# Stepë³„ ìµœì  ëª¨ë¸ ê²½ë¡œ
"""
        
        # ê° Stepë³„ ìµœì  ëª¨ë¸ ê²½ë¡œ
        step_models = {}
        for model in self.found_models:
            step = model.step_candidate
            if step not in step_models or model.confidence > step_models[step].confidence:
                step_models[step] = model
        
        for step_name, model in step_models.items():
            if model.confidence > 0.5 and step_name.startswith('step_'):  # ì‹ ë¢°ë„ ë†’ì€ ê²ƒë§Œ
                env_var_name = step_name.upper()
                env_content += f'export {env_var_name}_MODEL_PATH="{model.path}"\n'
                env_content += f'export {env_var_name}_MODEL_SIZE="{model.size_mb}"\n'
                env_content += f'export {env_var_name}_MODEL_FRAMEWORK="{model.framework}"\n\n'
        
        env_content += f'''
# í¸ì˜ í•¨ìˆ˜ë“¤
mycloset_list_models() {{
    echo "ğŸ” MyCloset AI ë°œê²¬ëœ ëª¨ë¸ë“¤:"
    echo "   ì´ $MYCLOSET_TOTAL_MODELSê°œ ëª¨ë¸ ($MYCLOSET_TOTAL_SIZE_GB GB)"
    echo "   ìŠ¤ìº”ì¼: $MYCLOSET_MODELS_SCAN_DATE"
    echo ""
    env | grep "STEP_.*_MODEL_PATH" | sort
}}

# ì‚¬ìš©ë²• ì¶œë ¥
if [[ "${{BASH_SOURCE[0]}}" == "${{0}}" ]]; then
    echo "ğŸ” MyCloset AI ëª¨ë¸ í™˜ê²½ë³€ìˆ˜ ë¡œë“œë¨"
    echo "ğŸ“Š ì´ $MYCLOSET_TOTAL_MODELSê°œ ëª¨ë¸ ($MYCLOSET_TOTAL_SIZE_GB GB)"
    echo ""
    echo "ğŸ’¡ ì‚¬ìš©ë²•:"
    echo "   source {output_path / "models_env.sh"}"
    echo "   mycloset_list_models"
    echo "   echo \\$STEP_01_HUMAN_PARSING_MODEL_PATH"
fi
'''
        
        env_file = output_path / "models_env.sh"
        with open(env_file, 'w', encoding='utf-8') as f:
            f.write(env_content)
        
        # ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬ (Unix ê³„ì—´)
        try:
            import stat
            env_file.chmod(env_file.stat().st_mode | stat.S_IEXEC)
        except:
            pass
        
        print(f"   âœ… í™˜ê²½ë³€ìˆ˜ íŒŒì¼: {env_file}")

# ==============================================
# ğŸš€ CLI ì¸í„°í˜ì´ìŠ¤
# ==============================================

def main():
    parser = argparse.ArgumentParser(
        description="MyCloset AI ëª¨ë¸ ìë™ ìŠ¤ìº” ë„êµ¬ (macOS ìµœì í™”)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python scan_models.py                           # ì•ˆì „í•œ ê¸°ë³¸ ìŠ¤ìº”
  python scan_models.py --safe                    # ì•ˆì „ ëª¨ë“œ (ê¶Œì¥)
  python scan_models.py --path ./ai_models        # íŠ¹ì • ê²½ë¡œ ìŠ¤ìº”
  python scan_models.py --create-config           # ì„¤ì • íŒŒì¼ ìƒì„±
  python scan_models.py --path ~/Downloads --create-config  # Downloads ìŠ¤ìº” + ì„¤ì • ìƒì„±
        """
    )
    
    parser.add_argument(
        '--path', '-p',
        type=str,
        nargs='+',
        help='ìŠ¤ìº”í•  íŠ¹ì • ê²½ë¡œë“¤'
    )
    
    parser.add_argument(
        '--safe',
        action='store_true',
        default=True,
        help='ì•ˆì „ ëª¨ë“œ (ê¶Œí•œ ë¬¸ì œ ë°©ì§€, ê¸°ë³¸ê°’)'
    )
    
    parser.add_argument(
        '--unsafe',
        action='store_true',
        help='ì•ˆì „ ëª¨ë“œ í•´ì œ (ê¶Œí•œ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥)'
    )
    
    parser.add_argument(
        '--deep',
        action='store_true',
        help='ë”¥ ìŠ¤ìº” (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼, --unsafeì™€ í•¨ê»˜ ì‚¬ìš©)'
    )
    
    parser.add_argument(
        '--create-config', '-c',
        action='store_true',
        help='ì„¤ì • íŒŒì¼ë“¤ ìë™ ìƒì„±'
    )
    
    parser.add_argument(
        '--output-dir', '-o',
        type=str,
        default='./configs',
        help='ì„¤ì • íŒŒì¼ ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ./configs)'
    )
    
    parser.add_argument(
        '--workers', '-w',
        type=int,
        default=2,
        help='ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: 2, macOS ì•ˆì •ì„±)'
    )
    
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help='ì¡°ìš©í•œ ëª¨ë“œ (ìµœì†Œ ì¶œë ¥)'
    )
    
    args = parser.parse_args()
    
    # ì•ˆì „ ëª¨ë“œ ì„¤ì •
    safe_mode = args.safe and not args.unsafe
    
    if args.deep and safe_mode:
        print("âš ï¸ ë”¥ ìŠ¤ìº”ì€ --unsafe ì˜µì…˜ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ì„¸ìš”")
        print("   ì˜ˆ: python scan_models.py --deep --unsafe")
        return 1
    
    # ìŠ¤ìºë„ˆ ì´ˆê¸°í™”
    scanner = AIModelScanner(verbose=not args.quiet, safe_mode=safe_mode)
    
    # ìŠ¤ìº” ì‹¤í–‰
    try:
        models = scanner.scan_system(
            custom_paths=args.path,
            deep_scan=args.deep,
            max_workers=args.workers
        )
        
        # ì„¤ì • íŒŒì¼ ìƒì„±
        if args.create_config and models:
            scanner.generate_config_files(args.output_dir)
            
            # ì‚¬ìš©ë²• ì•ˆë‚´
            print(f"\nğŸ“‹ ìƒì„±ëœ ì„¤ì • íŒŒì¼ ì‚¬ìš©ë²•:")
            print(f"   Python: from {args.output_dir.replace('./', '')}.model_paths import get_best_model_for_step")
            print(f"   Shell:  source {args.output_dir}/models_env.sh")
            print(f"   JSON:   cat {args.output_dir}/discovered_models.json")
        
        return 0 if models else 1
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 1
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if not args.quiet:
            import traceback
            traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())