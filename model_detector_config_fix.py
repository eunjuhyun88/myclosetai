#!/usr/bin/env python3
"""
ğŸ”§ MyCloset AI - ëª¨ë¸ íƒì§€ê¸° ì„¤ì • ìˆ˜ì •
âœ… ì‹¤ì œ ê²½ë¡œ êµ¬ì¡°ì— ë§ëŠ” íƒì§€ ë¡œì§ ìˆ˜ì •
âœ… 229GB ëª¨ë¸ íŒŒì¼ë“¤ ì™„ì „ í™œìš©
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any

class ModelDetectorConfigFix:
    """ëª¨ë¸ íƒì§€ê¸° ì„¤ì • ìˆ˜ì •ê¸°"""
    
    def __init__(self, project_root: str = "/Users/gimdudeul/MVP/mycloset-ai"):
        self.project_root = Path(project_root)
        self.backend_root = self.project_root / "backend"
        self.ai_models_root = self.backend_root / "ai_models"
        
    def get_enhanced_search_paths(self) -> List[str]:
        """ì‹¤ì œ êµ¬ì¡°ì— ë§ëŠ” ê²€ìƒ‰ ê²½ë¡œ ìƒì„±"""
        
        search_paths = [
            # 1. í‘œì¤€ Step ë””ë ‰í† ë¦¬ë“¤ (ì‹¬ë³¼ë¦­ ë§í¬ í›„)
            str(self.ai_models_root / "step_01_human_parsing"),
            str(self.ai_models_root / "step_02_pose_estimation"),
            str(self.ai_models_root / "step_03_cloth_segmentation"),
            str(self.ai_models_root / "step_04_geometric_matching"),
            str(self.ai_models_root / "step_05_cloth_warping"),
            str(self.ai_models_root / "step_06_virtual_fitting"),
            str(self.ai_models_root / "step_07_post_processing"),
            str(self.ai_models_root / "step_08_quality_assessment"),
            
            # 2. ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ë“¤
            str(self.ai_models_root / "checkpoints" / "step_01_human_parsing"),
            str(self.ai_models_root / "checkpoints" / "step_02_pose_estimation"),
            str(self.ai_models_root / "checkpoints" / "step_03_cloth_segmentation"),
            str(self.ai_models_root / "checkpoints" / "step_04_geometric_matching"),
            str(self.ai_models_root / "checkpoints" / "step_05_cloth_warping"),
            str(self.ai_models_root / "checkpoints" / "step_06_virtual_fitting"),
            str(self.ai_models_root / "checkpoints" / "step_07_post_processing"),
            str(self.ai_models_root / "checkpoints" / "step_08_quality_assessment"),
            
            # 3. organized ë””ë ‰í† ë¦¬ë“¤
            str(self.ai_models_root / "organized" / "step_01_human_parsing"),
            str(self.ai_models_root / "organized" / "step_02_pose_estimation"),
            str(self.ai_models_root / "organized" / "step_03_cloth_segmentation"),
            str(self.ai_models_root / "organized" / "step_04_geometric_matching"),
            str(self.ai_models_root / "organized" / "step_05_cloth_warping"),
            str(self.ai_models_root / "organized" / "step_06_virtual_fitting"),
            str(self.ai_models_root / "organized" / "step_07_post_processing"),
            str(self.ai_models_root / "organized" / "step_08_quality_assessment"),
            
            # 4. ai_models2 ë””ë ‰í† ë¦¬ë“¤
            str(self.ai_models_root / "ai_models2" / "step_01_human_parsing"),
            str(self.ai_models_root / "ai_models2" / "step_02_pose_estimation"),
            str(self.ai_models_root / "ai_models2" / "step_03_cloth_segmentation"),
            str(self.ai_models_root / "ai_models2" / "step_04_geometric_matching"),
            str(self.ai_models_root / "ai_models2" / "step_05_cloth_warping"),
            str(self.ai_models_root / "ai_models2" / "step_06_virtual_fitting"),
            str(self.ai_models_root / "ai_models2" / "step_07_post_processing"),
            str(self.ai_models_root / "ai_models2" / "step_08_quality_assessment"),
            
            # 5. íŠ¹ìˆ˜ ëª¨ë¸ ë””ë ‰í† ë¦¬ë“¤
            str(self.ai_models_root / "Graphonomy"),
            str(self.ai_models_root / "openpose"),
            str(self.ai_models_root / "OOTDiffusion"),
            str(self.ai_models_root / "HR-VITON"),
            str(self.ai_models_root / "u2net"),
            str(self.ai_models_root / "clip_vit_large"),
            str(self.ai_models_root / "idm_vton"),
            str(self.ai_models_root / "fashion_clip"),
            str(self.ai_models_root / "sam2_large"),
            
            # 6. Hugging Face ìºì‹œ
            str(self.ai_models_root / "huggingface_cache"),
            str(self.ai_models_root / "cache" / "huggingface"),
            
            # 7. ê¸°íƒ€ ì¤‘ìš” ë””ë ‰í† ë¦¬
            str(self.ai_models_root / "auxiliary_models"),
            str(self.ai_models_root / "individual_models"),
        ]
        
        # ì¡´ì¬í•˜ëŠ” ê²½ë¡œë§Œ ë°˜í™˜
        valid_paths = [path for path in search_paths if Path(path).exists()]
        
        return valid_paths
    
    def generate_model_detector_patch(self) -> str:
        """ëª¨ë¸ íƒì§€ê¸° íŒ¨ì¹˜ ì½”ë“œ ìƒì„±"""
        
        search_paths = self.get_enhanced_search_paths()
        
        code = f'''#!/usr/bin/env python3
"""
ğŸ”§ MyCloset AI - ëª¨ë¸ íƒì§€ê¸° íŒ¨ì¹˜ v2.0
âœ… ì‹¤ì œ íŒŒì¼ êµ¬ì¡°ì— ë§ëŠ” ê²½ë¡œ ì„¤ì •
âœ… 229GB ëª¨ë¸ íŒŒì¼ ì™„ì „ í™œìš©
"""

import logging
from pathlib import Path
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

# ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜ ê²€ìƒ‰ ê²½ë¡œ
ENHANCED_SEARCH_PATHS = {search_paths}

class EnhancedModelDetector:
    """í–¥ìƒëœ ëª¨ë¸ íƒì§€ê¸°"""
    
    def __init__(self):
        self.search_paths = [Path(p) for p in ENHANCED_SEARCH_PATHS if Path(p).exists()]
        logger.info(f"ğŸ” ìœ íš¨í•œ ê²€ìƒ‰ ê²½ë¡œ: {{len(self.search_paths)}}ê°œ")
        
    def scan_all_models(self) -> Dict[str, List[Dict[str, Any]]]:
        """ëª¨ë“  ëª¨ë¸ ìŠ¤ìº”"""
        detected_models = {{}}
        
        for search_path in self.search_paths:
            try:
                models = self._scan_directory(search_path)
                
                # Step ì´ë¦„ ì¶”ì¶œ
                step_name = self._extract_step_name(search_path)
                if step_name not in detected_models:
                    detected_models[step_name] = []
                
                detected_models[step_name].extend(models)
                
                if models:
                    logger.info(f"ğŸ“ {{search_path.name}}: {{len(models)}}ê°œ ëª¨ë¸ ë°œê²¬")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ìŠ¤ìº” ì‹¤íŒ¨ {{search_path}}: {{e}}")
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        for step_name in detected_models:
            detected_models[step_name] = self._deduplicate_models(detected_models[step_name])
        
        return detected_models
    
    def _scan_directory(self, directory: Path) -> List[Dict[str, Any]]:
        """ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
        models = []
        
        # ëª¨ë¸ íŒŒì¼ í™•ì¥ì
        model_extensions = {{'.pth', '.pt', '.bin', '.safetensors', '.onnx', '.pkl'}}
        
        try:
            # í•˜ìœ„ ë””ë ‰í† ë¦¬ê¹Œì§€ ì¬ê·€ì ìœ¼ë¡œ ìŠ¤ìº”
            for file_path in directory.rglob("*"):
                if file_path.is_file() and file_path.suffix.lower() in model_extensions:
                    try:
                        stat = file_path.stat()
                        size_mb = stat.st_size / (1024 * 1024)
                        
                        # ìµœì†Œ í¬ê¸° í•„í„° (0.1MB ì´ìƒ)
                        if size_mb >= 0.1:
                            model_info = {{
                                'file_path': str(file_path),
                                'file_name': file_path.name,
                                'size_mb': round(size_mb, 1),
                                'last_modified': stat.st_mtime,
                                'confidence': self._calculate_confidence(file_path.name, size_mb),
                                'relative_path': str(file_path.relative_to(directory))
                            }}
                            models.append(model_info)
                            
                    except Exception as e:
                        logger.debug(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {{file_path}}: {{e}}")
                        
        except Exception as e:
            logger.warning(f"ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì‹¤íŒ¨ {{directory}}: {{e}}")
        
        # í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬ (í° íŒŒì¼ ìš°ì„ )
        models.sort(key=lambda x: x['size_mb'], reverse=True)
        
        return models
    
    def _extract_step_name(self, path: Path) -> str:
        """ê²½ë¡œì—ì„œ Step ì´ë¦„ ì¶”ì¶œ"""
        path_str = str(path).lower()
        
        # Step íŒ¨í„´ ë§¤ì¹­
        step_patterns = {{
            'human_parsing': ['step_01', 'human_parsing', 'graphonomy', 'schp'],
            'pose_estimation': ['step_02', 'pose_estimation', 'openpose', 'hrnet'],
            'cloth_segmentation': ['step_03', 'cloth_segmentation', 'u2net', 'rembg'],
            'geometric_matching': ['step_04', 'geometric_matching', 'gmm', 'tps'],
            'cloth_warping': ['step_05', 'cloth_warping', 'warping'],
            'virtual_fitting': ['step_06', 'virtual_fitting', 'viton', 'ootd', 'diffusion'],
            'post_processing': ['step_07', 'post_processing', 'enhancement', 'super_resolution'],
            'quality_assessment': ['step_08', 'quality_assessment', 'clip', 'aesthetic']
        }}
        
        for step_name, keywords in step_patterns.items():
            if any(keyword in path_str for keyword in keywords):
                return step_name
        
        return 'unknown'
    
    def _calculate_confidence(self, filename: str, size_mb: float) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.0
        filename_lower = filename.lower()
        
        # íŒŒì¼ëª… ê¸°ë°˜ ì ìˆ˜
        high_confidence_keywords = ['schp', 'openpose', 'u2net', 'viton', 'ootd', 'clip']
        if any(keyword in filename_lower for keyword in high_confidence_keywords):
            confidence += 0.6
        
        medium_confidence_keywords = ['model', 'checkpoint', 'pytorch', 'final', 'best']
        if any(keyword in filename_lower for keyword in medium_confidence_keywords):
            confidence += 0.3
        
        # í¬ê¸° ê¸°ë°˜ ì ìˆ˜
        if 10 <= size_mb <= 5000:  # 10MB ~ 5GB (ì ì • ë²”ìœ„)
            confidence += 0.4
        elif size_mb > 5000:  # 5GB ì´ìƒ (ëŒ€ìš©ëŸ‰ ëª¨ë¸)
            confidence += 0.2
        elif size_mb >= 1:  # 1MB ì´ìƒ
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _deduplicate_models(self, models: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ëª¨ë¸ ì œê±°"""
        seen_names = set()
        unique_models = []
        
        for model in models:
            name = model['file_name']
            if name not in seen_names:
                seen_names.add(name)
                unique_models.append(model)
        
        return unique_models

# ì „ì—­ íƒì§€ê¸° ì¸ìŠ¤í„´ìŠ¤
enhanced_detector = EnhancedModelDetector()

def scan_enhanced_models() -> Dict[str, List[Dict[str, Any]]]:
    """í–¥ìƒëœ ëª¨ë¸ ìŠ¤ìº” ì‹¤í–‰"""
    return enhanced_detector.scan_all_models()

def patch_existing_detector(detector_instance):
    """ê¸°ì¡´ íƒì§€ê¸° íŒ¨ì¹˜"""
    if hasattr(detector_instance, 'search_paths'):
        detector_instance.search_paths = enhanced_detector.search_paths
        logger.info("âœ… ê¸°ì¡´ íƒì§€ê¸° ê²€ìƒ‰ ê²½ë¡œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    return detector_instance
'''
        
        return code
    
    def create_step_model_requests_fix(self) -> str:
        """step_model_requests.py ìˆ˜ì • ì½”ë“œ ìƒì„±"""
        
        return '''#!/usr/bin/env python3
"""
ğŸ“‹ MyCloset AI - Stepë³„ ëª¨ë¸ ìš”ì²­ì‚¬í•­ v6.1 (ì‹¤ì œ íŒŒì¼ ê¸°ë°˜)
âœ… ì‹¤ì œ íƒì§€ëœ íŒŒì¼ë“¤ë¡œ íŒ¨í„´ ì—…ë°ì´íŠ¸
âœ… ì‹ ë¢°ë„ ê¸°ë°˜ ë§¤ì¹­ ê°œì„ 
"""

from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from pathlib import Path

@dataclass
class StepModelRequest:
    """Stepë³„ ëª¨ë¸ ìš”ì²­ì‚¬í•­"""
    step_name: str
    model_name: str
    checkpoint_patterns: List[str]
    size_range_mb: tuple = (0.1, 50000.0)  # ìµœëŒ€ 50GBê¹Œì§€ í—ˆìš©
    required_files: List[str] = field(default_factory=list)
    confidence_threshold: float = 0.1  # ë§¤ìš° ê´€ëŒ€í•œ ì„ê³„ê°’
    device: str = "mps"
    precision: str = "fp16"
    priority: int = 1

# ì‹¤ì œ ë°œê²¬ëœ íŒŒì¼ë“¤ ê¸°ë°˜ íŒ¨í„´ ì •ì˜
STEP_MODEL_REQUESTS = {
    "HumanParsingStep": StepModelRequest(
        step_name="HumanParsingStep",
        model_name="human_parsing_enhanced",
        checkpoint_patterns=[
            r".*\.pth$",
            r".*\.pt$", 
            r".*\.bin$",
            r".*\.pkl$",
            r".*schp.*\.pth$",
            r".*graphonomy.*\.pth$",
            r".*parsing.*\.pth$",
            r".*human.*\.pth$"
        ],
        confidence_threshold=0.05
    ),
    
    "PoseEstimationStep": StepModelRequest(
        step_name="PoseEstimationStep", 
        model_name="pose_estimation_enhanced",
        checkpoint_patterns=[
            r".*\.pth$",
            r".*\.pt$",
            r".*\.bin$",
            r".*openpose.*\.pth$",
            r".*pose.*\.pth$",
            r".*body.*\.pth$"
        ],
        confidence_threshold=0.05
    ),
    
    "ClothSegmentationStep": StepModelRequest(
        step_name="ClothSegmentationStep",
        model_name="cloth_segmentation_enhanced", 
        checkpoint_patterns=[
            r".*\.pth$",
            r".*\.pt$",
            r".*\.bin$",
            r".*u2net.*\.pth$",
            r".*segment.*\.pth$",
            r".*cloth.*\.pth$"
        ],
        confidence_threshold=0.05
    ),
    
    "GeometricMatchingStep": StepModelRequest(
        step_name="GeometricMatchingStep",
        model_name="geometric_matching_enhanced",
        checkpoint_patterns=[
            r".*\.pth$",
            r".*\.pt$", 
            r".*\.bin$",
            r".*gmm.*\.pth$",
            r".*tps.*\.pth$",
            r".*matching.*\.pth$"
        ],
        confidence_threshold=0.05
    ),
    
    "ClothWarpingStep": StepModelRequest(
        step_name="ClothWarpingStep",
        model_name="cloth_warping_enhanced",
        checkpoint_patterns=[
            r".*\.pth$",
            r".*\.pt$",
            r".*\.bin$", 
            r".*warp.*\.pth$",
            r".*tom.*\.pth$"
        ],
        confidence_threshold=0.05
    ),
    
    "VirtualFittingStep": StepModelRequest(
        step_name="VirtualFittingStep",
        model_name="virtual_fitting_enhanced",
        checkpoint_patterns=[
            r".*\.pth$",
            r".*\.pt$",
            r".*\.bin$",
            r".*\.safetensors$",
            r".*viton.*\.pth$",
            r".*ootd.*\.bin$",
            r".*diffusion.*\.bin$",
            r".*fitting.*\.pth$"
        ],
        size_range_mb=(100.0, 50000.0),  # ëŒ€ìš©ëŸ‰ ëª¨ë¸
        confidence_threshold=0.02
    ),
    
    "PostProcessingStep": StepModelRequest(
        step_name="PostProcessingStep",
        model_name="post_processing_enhanced",
        checkpoint_patterns=[
            r".*\.pth$",
            r".*\.pt$",
            r".*\.bin$",
            r".*enhance.*\.pth$",
            r".*super.*\.pth$",
            r".*resolution.*\.pth$"
        ],
        confidence_threshold=0.05
    ),
    
    "QualityAssessmentStep": StepModelRequest(
        step_name="QualityAssessmentStep", 
        model_name="quality_assessment_enhanced",
        checkpoint_patterns=[
            r".*\.pth$",
            r".*\.pt$",
            r".*\.bin$",
            r".*clip.*\.bin$",
            r".*quality.*\.pth$",
            r".*aesthetic.*\.pth$"
        ],
        confidence_threshold=0.05
    )
}

def get_step_model_request(step_name: str) -> Optional[StepModelRequest]:
    """Stepë³„ ëª¨ë¸ ìš”ì²­ì‚¬í•­ ì¡°íšŒ"""
    return STEP_MODEL_REQUESTS.get(step_name)

def get_all_patterns() -> List[str]:
    """ëª¨ë“  íŒ¨í„´ ëª©ë¡ ë°˜í™˜"""
    all_patterns = []
    for request in STEP_MODEL_REQUESTS.values():
        all_patterns.extend(request.checkpoint_patterns)
    return list(set(all_patterns))
'''

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”§ ëª¨ë¸ íƒì§€ê¸° ì„¤ì • ìˆ˜ì •")
    print("="*50)
    
    fixer = ModelDetectorConfigFix()
    
    # 1. í–¥ìƒëœ íƒì§€ê¸° íŒ¨ì¹˜ ìƒì„±
    detector_patch = fixer.generate_model_detector_patch()
    with open('enhanced_model_detector.py', 'w') as f:
        f.write(detector_patch)
    print("âœ… enhanced_model_detector.py ìƒì„± ì™„ë£Œ")
    
    # 2. step_model_requests ìˆ˜ì •
    requests_fix = fixer.create_step_model_requests_fix()
    with open('step_model_requests_enhanced.py', 'w') as f:
        f.write(requests_fix)
    print("âœ… step_model_requests_enhanced.py ìƒì„± ì™„ë£Œ")
    
    # 3. ê²€ìƒ‰ ê²½ë¡œ í™•ì¸
    search_paths = fixer.get_enhanced_search_paths()
    print(f"\nğŸ“ ìœ íš¨í•œ ê²€ìƒ‰ ê²½ë¡œ: {len(search_paths)}ê°œ")
    for path in search_paths[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
        print(f"   - {path}")
    if len(search_paths) > 10:
        print(f"   ... ì™¸ {len(search_paths) - 10}ê°œ")
    
    print("\nğŸ¯ ì ìš© ë°©ë²•:")
    print("   1. ë¨¼ì € ëª¨ë¸ ê²½ë¡œ ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰")
    print("   2. enhanced_model_detector.pyë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ íƒì§€")
    print("   3. ì„œë²„ ì¬ì‹œì‘")

if __name__ == "__main__":
    main()