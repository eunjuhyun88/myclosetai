#!/usr/bin/env python3
"""
MyCloset AI ì™„ì „í•œ ì‹œìŠ¤í…œ ëª¨ë¸ ìŠ¤ìºë„ˆ
ì „ì²´ ì‹œìŠ¤í…œì˜ ëª¨ë“  AI ëª¨ë¸, ì²´í¬í¬ì¸íŠ¸, ì„¤ì • íŒŒì¼ì„ ì™„ì „íˆ ìŠ¤ìº”
"""

import os
import sys
import json
import hashlib
import platform
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import mimetypes
import time

class CompleteSystemModelScanner:
    def __init__(self, deep_scan: bool = False, verbose: bool = True):
        self.project_root = Path.cwd()
        self.deep_scan = deep_scan
        self.verbose = verbose
        self.start_time = time.time()
        
        # ìŠ¤ìº” ê²°ê³¼ ì €ì¥
        self.scan_results = {
            "scan_info": {
                "timestamp": datetime.now().isoformat(),
                "system": platform.system(),
                "machine": platform.machine(),
                "python_version": platform.python_version(),
                "project_root": str(self.project_root),
                "deep_scan": deep_scan,
                "scan_duration": 0
            },
            "locations": {},
            "models_by_type": {},
            "duplicates": {},
            "statistics": {},
            "issues": [],
            "recommendations": []
        }
        
        # AI ëª¨ë¸ íŒŒì¼ í™•ì¥ìë“¤ (í™•ì¥)
        self.model_extensions = {
            # PyTorch
            '.pth': 'pytorch',
            '.pt': 'pytorch', 
            '.bin': 'pytorch_bin',
            '.pkl': 'pickle',
            
            # Safetensors (HuggingFace)
            '.safetensors': 'safetensors',
            
            # ONNX
            '.onnx': 'onnx',
            
            # TensorFlow
            '.pb': 'tensorflow',
            '.h5': 'tensorflow',
            '.tflite': 'tensorflow_lite',
            '.keras': 'keras',
            
            # ì²´í¬í¬ì¸íŠ¸
            '.ckpt': 'checkpoint',
            '.checkpoint': 'checkpoint',
            '.weights': 'weights',
            '.model': 'generic_model',
            
            # ê¸°íƒ€
            '.npz': 'numpy',
            '.npy': 'numpy',
            '.joblib': 'joblib'
        }
        
        # ì„¤ì • íŒŒì¼ë“¤
        self.config_extensions = {
            '.json': 'json_config',
            '.yaml': 'yaml_config',
            '.yml': 'yaml_config',
            '.toml': 'toml_config',
            '.ini': 'ini_config',
            '.cfg': 'cfg_config',
            '.config': 'config',
            '.txt': 'text_config'
        }
        
        # ì œì™¸í•  ë””ë ‰í† ë¦¬ (ì„±ëŠ¥ ë° ë³´ì•ˆ)
        self.exclude_dirs = {
            '__pycache__', '.git', '.svn', '.hg',
            'node_modules', '.npm', 'bower_components',
            '.vscode', '.idea', '.vs',
            'venv', 'env', '.env', 'virtualenv',
            '.pytest_cache', '.tox', '.coverage',
            'logs', 'log', 'temp', 'tmp',
            '.cache', 'cache', '.npm',
            'Trash', '.Trash', '.recycle.bin',
            'System Volume Information',
            '$RECYCLE.BIN', 'pagefile.sys', 'hiberfil.sys'
        }
        
        # ì‹œìŠ¤í…œ ë³´í˜¸ ê²½ë¡œ (ìŠ¤ìº”í•˜ì§€ ì•ŠìŒ)
        self.protected_paths = {
            '/System', '/Library/System', '/private',
            '/dev', '/proc', '/sys', '/var/log',
            '/etc/shadow', '/etc/passwd'
        }

    def get_all_scan_paths(self) -> List[Path]:
        """ìŠ¤ìº”í•  ëª¨ë“  ê²½ë¡œ ìˆ˜ì§‘"""
        paths = []
        system = platform.system().lower()
        
        # 1. í˜„ì¬ í”„ë¡œì íŠ¸ (ìµœìš°ì„ )
        paths.append(self.project_root)
        
        # 2. í™ˆ ë””ë ‰í† ë¦¬
        home = Path.home()
        paths.append(home)
        
        # 3. ì¼ë°˜ì ì¸ AI ëª¨ë¸ ì €ì¥ ìœ„ì¹˜ë“¤
        common_paths = [
            home / ".cache",
            home / ".cache" / "huggingface",
            home / ".cache" / "torch",
            home / ".cache" / "diffusers",
            home / ".local" / "share",
            home / "Downloads",
            home / "Documents",
            home / "Desktop"
        ]
        
        # 4. ì‹œìŠ¤í…œë³„ ì¶”ê°€ ê²½ë¡œ
        if system == "darwin":  # macOS
            common_paths.extend([
                Path("/Applications"),
                Path("/opt"),
                Path("/usr/local"),
                Path("/opt/homebrew"),
                home / "Library" / "Caches",
                home / "Library" / "Application Support"
            ])
        elif system == "linux":
            common_paths.extend([
                Path("/opt"),
                Path("/usr/local"),
                Path("/usr/share"),
                Path("/var/lib"),
                Path("/home")
            ])
        elif system == "windows":
            common_paths.extend([
                Path("C:/Program Files"),
                Path("C:/Program Files (x86)"),
                Path("C:/Users"),
                home / "AppData"
            ])
        
        # 5. Deep scanì¸ ê²½ìš° ë£¨íŠ¸ë¶€í„° ìŠ¤ìº”
        if self.deep_scan:
            if system == "darwin":
                paths.append(Path("/"))
            elif system == "linux":
                paths.append(Path("/"))
            elif system == "windows":
                for drive in "CDEFGH":
                    drive_path = Path(f"{drive}:/")
                    if drive_path.exists():
                        paths.append(drive_path)
        
        # ì¡´ì¬í•˜ëŠ” ê²½ë¡œë§Œ í•„í„°ë§
        valid_paths = []
        for path in paths + common_paths:
            if path.exists() and path.is_dir():
                valid_paths.append(path)
        
        # ì¤‘ë³µ ì œê±° (ë¶€ëª¨-ìì‹ ê´€ê³„ í™•ì¸)
        unique_paths = []
        for path in valid_paths:
            is_child = False
            for existing in unique_paths:
                try:
                    if path.is_relative_to(existing):
                        is_child = True
                        break
                except (ValueError, OSError):
                    continue
            if not is_child:
                unique_paths.append(path)
        
        return unique_paths

    def is_protected_path(self, path: Path) -> bool:
        """ë³´í˜¸ëœ ê²½ë¡œì¸ì§€ í™•ì¸"""
        path_str = str(path)
        for protected in self.protected_paths:
            if path_str.startswith(protected):
                return True
        return False

    def should_skip_directory(self, directory: Path) -> bool:
        """ë””ë ‰í† ë¦¬ë¥¼ ê±´ë„ˆë›¸ì§€ ê²°ì •"""
        dir_name = directory.name.lower()
        
        # ì œì™¸ ë””ë ‰í† ë¦¬ í™•ì¸
        if dir_name in self.exclude_dirs:
            return True
            
        # ë³´í˜¸ëœ ê²½ë¡œ í™•ì¸
        if self.is_protected_path(directory):
            return True
            
        # ìˆ¨ê²¨ì§„ ë””ë ‰í† ë¦¬ (ì„ íƒì  ì œì™¸)
        if dir_name.startswith('.') and dir_name not in {'.cache', '.local'}:
            return True
            
        # ë„ˆë¬´ ê¸´ ê²½ë¡œ (Windows í˜¸í™˜ì„±)
        if len(str(directory)) > 250:
            return True
            
        return False

    def get_file_info(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """íŒŒì¼ ì •ë³´ ìˆ˜ì§‘"""
        try:
            if not file_path.exists() or not file_path.is_file():
                return None
                
            stat_info = file_path.stat()
            size_bytes = stat_info.st_size
            size_mb = round(size_bytes / (1024 * 1024), 2)
            
            # ë„ˆë¬´ ì‘ì€ íŒŒì¼ì€ ì œì™¸ (1KB ë¯¸ë§Œ)
            if size_bytes < 1024:
                return None
            
            # íŒŒì¼ í•´ì‹œ (ì¤‘ë³µ íƒì§€ìš©)
            file_hash = self.get_file_hash(file_path)
            
            # íŒŒì¼ íƒ€ì… ì¶”ì •
            file_type = self.classify_file_type(file_path)
            
            return {
                "path": str(file_path),
                "name": file_path.name,
                "size_bytes": size_bytes,
                "size_mb": size_mb,
                "size_gb": round(size_mb / 1024, 3),
                "modified": datetime.fromtimestamp(stat_info.st_mtime).isoformat(),
                "hash": file_hash,
                "extension": file_path.suffix.lower(),
                "framework": self.model_extensions.get(file_path.suffix.lower(), 'unknown'),
                "file_type": file_type,
                "relative_to_home": str(file_path.relative_to(Path.home())) if file_path.is_relative_to(Path.home()) else None,
                "relative_to_project": str(file_path.relative_to(self.project_root)) if file_path.is_relative_to(self.project_root) else None
            }
            
        except (PermissionError, OSError, ValueError) as e:
            if self.verbose:
                print(f"âš ï¸ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ {file_path}: {e}")
            return None

    def get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ íƒì§€ìš©)"""
        try:
            with open(file_path, 'rb') as f:
                # íŒŒì¼ í¬ê¸°ì— ë”°ë¼ ì½ì„ ë°”ì´íŠ¸ ìˆ˜ ì¡°ì •
                if file_path.stat().st_size > 100 * 1024 * 1024:  # 100MB ì´ìƒ
                    content = f.read(16384)  # 16KBë§Œ ì½ê¸°
                else:
                    content = f.read(65536)  # 64KB ì½ê¸°
                return hashlib.md5(content).hexdigest()
        except:
            return "unknown"

    def classify_file_type(self, file_path: Path) -> str:
        """íŒŒì¼ íƒ€ì… ë¶„ë¥˜"""
        name_lower = file_path.name.lower()
        path_lower = str(file_path).lower()
        
        # AI ëª¨ë¸ íƒ€ì… ì¶”ì •
        if any(keyword in path_lower for keyword in ['clip', 'vit']):
            return 'clip_model'
        elif any(keyword in path_lower for keyword in ['diffusion', 'stable', 'ootd']):
            return 'diffusion_model'
        elif any(keyword in path_lower for keyword in ['pose', 'openpose', 'body']):
            return 'pose_model'
        elif any(keyword in path_lower for keyword in ['segment', 'u2net', 'mask']):
            return 'segmentation_model'
        elif any(keyword in path_lower for keyword in ['parsing', 'human', 'atr']):
            return 'parsing_model'
        elif any(keyword in path_lower for keyword in ['vton', 'viton', 'tryon']):
            return 'virtual_tryon_model'
        elif 'checkpoint' in path_lower or 'ckpt' in name_lower:
            return 'checkpoint'
        elif any(keyword in path_lower for keyword in ['config', 'setup', 'settings']):
            return 'config_file'
        else:
            return 'unknown_model'

    def scan_directory_safe(self, directory: Path) -> List[Dict[str, Any]]:
        """ì•ˆì „í•œ ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
        found_files = []
        
        try:
            if not directory.exists() or not directory.is_dir():
                return found_files
                
            if self.should_skip_directory(directory):
                return found_files
            
            # ê¶Œí•œ í™•ì¸
            if not os.access(directory, os.R_OK):
                return found_files
            
            # ì¬ê·€ì ìœ¼ë¡œ íŒŒì¼ ì°¾ê¸°
            for item in directory.iterdir():
                try:
                    if item.is_file():
                        # ëª¨ë¸ íŒŒì¼ì¸ì§€ í™•ì¸
                        if item.suffix.lower() in self.model_extensions:
                            file_info = self.get_file_info(item)
                            if file_info:
                                found_files.append(file_info)
                        # ì„¤ì • íŒŒì¼ì¸ì§€ í™•ì¸ (AI ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì‹œ)
                        elif item.suffix.lower() in self.config_extensions:
                            if any(keyword in str(item).lower() for keyword in 
                                  ['model', 'ai', 'ml', 'torch', 'tensorflow', 'onnx', 'diffusion']):
                                file_info = self.get_file_info(item)
                                if file_info:
                                    found_files.append(file_info)
                    
                    elif item.is_dir() and not self.should_skip_directory(item):
                        # ì¬ê·€ì ìœ¼ë¡œ í•˜ìœ„ ë””ë ‰í† ë¦¬ ìŠ¤ìº”
                        sub_files = self.scan_directory_safe(item)
                        found_files.extend(sub_files)
                        
                except (PermissionError, OSError):
                    continue
                    
        except (PermissionError, OSError) as e:
            if self.verbose:
                print(f"âš ï¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì‹¤íŒ¨ {directory}: {e}")
        
        return found_files

    def run_parallel_scan(self, scan_paths: List[Path]) -> Dict[str, List[Dict]]:
        """ë³‘ë ¬ ìŠ¤ìº” ì‹¤í–‰"""
        results = {}
        
        print(f"ğŸ” {len(scan_paths)}ê°œ ìœ„ì¹˜ ë³‘ë ¬ ìŠ¤ìº” ì‹œì‘...")
        
        # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=min(8, len(scan_paths))) as executor:
            # ê° ê²½ë¡œì— ëŒ€í•´ ìŠ¤ìº” ì‘ì—… ì œì¶œ
            future_to_path = {
                executor.submit(self.scan_directory_safe, path): path 
                for path in scan_paths
            }
            
            # ì™„ë£Œëœ ì‘ì—…ë“¤ ìˆ˜ì§‘
            for future in as_completed(future_to_path):
                path = future_to_path[future]
                try:
                    files = future.result()
                    if files:
                        results[str(path)] = files
                        if self.verbose:
                            total_size = sum(f['size_mb'] for f in files)
                            print(f"âœ… {path}: {len(files)}ê°œ íŒŒì¼ ({total_size:.1f}MB)")
                    else:
                        if self.verbose:
                            print(f"ğŸ“‚ {path}: ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
                            
                except Exception as e:
                    self.scan_results["issues"].append(f"ìŠ¤ìº” ì‹¤íŒ¨ {path}: {e}")
                    if self.verbose:
                        print(f"âŒ {path}: ìŠ¤ìº” ì‹¤íŒ¨ - {e}")
        
        return results

    def analyze_scan_results(self, scan_results: Dict[str, List[Dict]]):
        """ìŠ¤ìº” ê²°ê³¼ ë¶„ì„"""
        print("\nğŸ“Š ìŠ¤ìº” ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        all_files = []
        for files in scan_results.values():
            all_files.extend(files)
        
        # 1. ìœ„ì¹˜ë³„ ì •ë¦¬
        self.scan_results["locations"] = scan_results
        
        # 2. íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
        type_groups = {}
        for file_info in all_files:
            file_type = file_info["file_type"]
            if file_type not in type_groups:
                type_groups[file_type] = []
            type_groups[file_type].append(file_info)
        
        self.scan_results["models_by_type"] = type_groups
        
        # 3. ì¤‘ë³µ íŒŒì¼ íƒì§€
        hash_groups = {}
        for file_info in all_files:
            file_hash = file_info["hash"]
            if file_hash != "unknown":
                if file_hash not in hash_groups:
                    hash_groups[file_hash] = []
                hash_groups[file_hash].append(file_info)
        
        # ì¤‘ë³µ íŒŒì¼ë“¤ë§Œ ì¶”ì¶œ
        duplicates = {h: files for h, files in hash_groups.items() if len(files) > 1}
        self.scan_results["duplicates"] = duplicates
        
        # 4. í†µê³„ ê³„ì‚°
        total_files = len(all_files)
        total_size_mb = sum(f["size_mb"] for f in all_files)
        total_size_gb = total_size_mb / 1024
        
        framework_stats = {}
        for file_info in all_files:
            framework = file_info["framework"]
            if framework not in framework_stats:
                framework_stats[framework] = {"count": 0, "size_mb": 0}
            framework_stats[framework]["count"] += 1
            framework_stats[framework]["size_mb"] += file_info["size_mb"]
        
        self.scan_results["statistics"] = {
            "total_files": total_files,
            "total_size_mb": total_size_mb,
            "total_size_gb": total_size_gb,
            "framework_distribution": framework_stats,
            "duplicate_groups": len(duplicates),
            "duplicate_waste_mb": sum(
                sum(f["size_mb"] for f in files[1:])  # ì²« ë²ˆì§¸ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ê°€ ë‚­ë¹„
                for files in duplicates.values()
            ) if duplicates else 0
        }

    def generate_recommendations(self):
        """ì •ë¦¬ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        print("ğŸ’¡ ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘...")
        
        recommendations = []
        stats = self.scan_results["statistics"]
        
        # 1. ì¤‘ë³µ íŒŒì¼ ì •ë¦¬
        if stats["duplicate_waste_mb"] > 100:  # 100MB ì´ìƒ ë‚­ë¹„
            recommendations.append({
                "type": "duplicate_cleanup",
                "priority": "high",
                "title": "ì¤‘ë³µ íŒŒì¼ ì •ë¦¬",
                "description": f"{stats['duplicate_groups']}ê°œ ê·¸ë£¹ì˜ ì¤‘ë³µ íŒŒì¼ë¡œ {stats['duplicate_waste_mb']:.1f}MB ë‚­ë¹„",
                "potential_savings": f"{stats['duplicate_waste_mb']:.1f}MB",
                "action": "run_duplicate_cleanup"
            })
        
        # 2. í”„ë¡œì íŠ¸ ì™¸ë¶€ ëª¨ë¸ë“¤ í†µí•©
        external_models = []
        for location, files in self.scan_results["locations"].items():
            if not Path(location).is_relative_to(self.project_root):
                external_models.extend(files)
        
        if external_models:
            external_size = sum(f["size_mb"] for f in external_models)
            recommendations.append({
                "type": "model_consolidation", 
                "priority": "medium",
                "title": "ì™¸ë¶€ ëª¨ë¸ í†µí•©",
                "description": f"í”„ë¡œì íŠ¸ ì™¸ë¶€ì— {len(external_models)}ê°œ ëª¨ë¸ ({external_size:.1f}MB)",
                "action": "move_external_models"
            })
        
        # 3. ëŒ€ìš©ëŸ‰ íŒŒì¼ ìµœì í™”
        large_files = [f for f in self.get_all_files() if f["size_mb"] > 1000]  # 1GB ì´ìƒ
        if large_files:
            recommendations.append({
                "type": "large_file_optimization",
                "priority": "low", 
                "title": "ëŒ€ìš©ëŸ‰ íŒŒì¼ ìµœì í™”",
                "description": f"{len(large_files)}ê°œì˜ 1GB+ íŒŒì¼ ìµœì í™” ê°€ëŠ¥",
                "action": "optimize_large_models"
            })
        
        self.scan_results["recommendations"] = recommendations

    def get_all_files(self) -> List[Dict]:
        """ëª¨ë“  ìŠ¤ìº”ëœ íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        all_files = []
        for files in self.scan_results["locations"].values():
            all_files.extend(files)
        return all_files

    def print_detailed_summary(self):
        """ìƒì„¸í•œ ìŠ¤ìº” ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ¯ MyCloset AI ì™„ì „í•œ ì‹œìŠ¤í…œ ëª¨ë¸ ìŠ¤ìº” ê²°ê³¼")
        print("="*80)
        
        scan_info = self.scan_results["scan_info"]
        stats = self.scan_results["statistics"]
        
        # ê¸°ë³¸ ì •ë³´
        print(f"ğŸ• ìŠ¤ìº” ì‹œê°„: {scan_info['timestamp']}")
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {scan_info['scan_duration']:.1f}ì´ˆ")
        print(f"ğŸ’» ì‹œìŠ¤í…œ: {scan_info['system']} {scan_info['machine']}")
        print(f"ğŸ“‚ í”„ë¡œì íŠ¸: {scan_info['project_root']}")
        print(f"ğŸ” ìŠ¤ìº” ëª¨ë“œ: {'ì „ì²´ ì‹œìŠ¤í…œ' if scan_info['deep_scan'] else 'ì£¼ìš” ìœ„ì¹˜'}")
        
        # ì „ì²´ í†µê³„
        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"  ğŸ“„ ì´ íŒŒì¼ ìˆ˜: {stats['total_files']:,}ê°œ")
        print(f"  ğŸ’¾ ì´ í¬ê¸°: {stats['total_size_gb']:.2f}GB ({stats['total_size_mb']:.1f}MB)")
        print(f"  ğŸ“ ìŠ¤ìº” ìœ„ì¹˜: {len(self.scan_results['locations'])}ê³³")
        
        # í”„ë ˆì„ì›Œí¬ë³„ ë¶„í¬
        print(f"\nğŸ”§ í”„ë ˆì„ì›Œí¬ë³„ ë¶„í¬:")
        for framework, info in sorted(stats['framework_distribution'].items(), 
                                    key=lambda x: x[1]['size_mb'], reverse=True):
            print(f"  - {framework}: {info['count']}ê°œ ({info['size_mb']:.1f}MB)")
        
        # íŒŒì¼ íƒ€ì…ë³„ ë¶„í¬
        print(f"\nğŸ¯ íŒŒì¼ íƒ€ì…ë³„ ë¶„í¬:")
        for file_type, files in sorted(self.scan_results['models_by_type'].items(),
                                     key=lambda x: len(x[1]), reverse=True):
            total_size = sum(f['size_mb'] for f in files)
            print(f"  - {file_type}: {len(files)}ê°œ ({total_size:.1f}MB)")
        
        # ìœ„ì¹˜ë³„ ë¶„í¬
        print(f"\nğŸ“ ìœ„ì¹˜ë³„ ë¶„í¬:")
        for location, files in sorted(self.scan_results['locations'].items(),
                                    key=lambda x: len(x[1]), reverse=True):
            total_size = sum(f['size_mb'] for f in files)
            location_display = location if len(location) < 60 else "..." + location[-57:]
            print(f"  - {location_display}: {len(files)}ê°œ ({total_size:.1f}MB)")
        
        # ì¤‘ë³µ íŒŒì¼
        if self.scan_results["duplicates"]:
            print(f"\nğŸ”„ ì¤‘ë³µ íŒŒì¼:")
            print(f"  ê·¸ë£¹ ìˆ˜: {stats['duplicate_groups']}ê°œ")
            print(f"  ì ˆì•½ ê°€ëŠ¥: {stats['duplicate_waste_mb']:.1f}MB")
        
        # ì£¼ìš” íŒŒì¼ë“¤ (í¬ê¸° ìˆœ)
        print(f"\nğŸ† ì£¼ìš” íŒŒì¼ë“¤ (í¬ê¸° ìˆœ Top 10):")
        all_files = self.get_all_files()
        top_files = sorted(all_files, key=lambda x: x['size_mb'], reverse=True)[:10]
        
        for i, file_info in enumerate(top_files, 1):
            path_display = file_info['path']
            if len(path_display) > 70:
                path_display = "..." + path_display[-67:]
            print(f"  {i:2d}. {file_info['name']}")
            print(f"      ğŸ“ {path_display}")
            print(f"      ğŸ“Š {file_info['size_mb']:.1f}MB | {file_info['framework']} | {file_info['file_type']}")
        
        # ê¶Œì¥ì‚¬í•­
        if self.scan_results["recommendations"]:
            print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            for rec in self.scan_results["recommendations"]:
                priority_emoji = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}
                emoji = priority_emoji.get(rec["priority"], "âšª")
                print(f"  {emoji} {rec['title']}: {rec['description']}")
        
        # ë¬¸ì œì‚¬í•­
        if self.scan_results["issues"]:
            print(f"\nâš ï¸ ë°œê²¬ëœ ë¬¸ì œë“¤:")
            for issue in self.scan_results["issues"][:5]:  # ìµœëŒ€ 5ê°œ
                print(f"  - {issue}")

    def save_results(self, output_file: str = None) -> Path:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"complete_model_scan_{timestamp}.json"
        
        output_path = self.project_root / output_file
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.scan_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {output_path}")
        return output_path

    def run_complete_scan(self) -> Path:
        """ì™„ì „í•œ ì‹œìŠ¤í…œ ìŠ¤ìº” ì‹¤í–‰"""
        print("ğŸš€ MyCloset AI ì™„ì „í•œ ì‹œìŠ¤í…œ ëª¨ë¸ ìŠ¤ìº” ì‹œì‘")
        print("="*80)
        
        try:
            # 1. ìŠ¤ìº” ê²½ë¡œ ìˆ˜ì§‘
            scan_paths = self.get_all_scan_paths()
            print(f"ğŸ“‚ ì´ {len(scan_paths)}ê°œ ìœ„ì¹˜ ìŠ¤ìº” ì˜ˆì •")
            
            if self.verbose:
                for i, path in enumerate(scan_paths):
                    print(f"  {i+1:2d}. {path}")
                print()
            
            # 2. ë³‘ë ¬ ìŠ¤ìº” ì‹¤í–‰
            scan_results = self.run_parallel_scan(scan_paths)
            
            # 3. ê²°ê³¼ ë¶„ì„
            self.analyze_scan_results(scan_results)
            
            # 4. ê¶Œì¥ì‚¬í•­ ìƒì„±
            self.generate_recommendations()
            
            # 5. ìŠ¤ìº” ì‹œê°„ ê¸°ë¡
            self.scan_results["scan_info"]["scan_duration"] = time.time() - self.start_time
            
            # 6. ê²°ê³¼ ì¶œë ¥
            self.print_detailed_summary()
            
            # 7. ê²°ê³¼ ì €ì¥
            output_file = self.save_results()
            
            print(f"\nâœ… ì™„ì „í•œ ì‹œìŠ¤í…œ ìŠ¤ìº” ì™„ë£Œ!")
            print(f"ğŸ“Š ìƒì„¸ ê²°ê³¼: {output_file}")
            print(f"\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
            print("1. ìŠ¤ìº” ê²°ê³¼ë¥¼ í™•ì¸í•˜ì—¬ ì¤‘ë³µ íŒŒì¼ ì •ë¦¬")
            print("2. ì™¸ë¶€ ëª¨ë¸ë“¤ì„ í”„ë¡œì íŠ¸ë¡œ ì´ë™")
            print("3. í‘œì¤€ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì ìš©")
            
            return output_file
            
        except Exception as e:
            print(f"âŒ ìŠ¤ìº” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="MyCloset AI ì™„ì „í•œ ì‹œìŠ¤í…œ ëª¨ë¸ ìŠ¤ìºë„ˆ")
    parser.add_argument("--deep", action="store_true", help="ì „ì²´ ì‹œìŠ¤í…œ ë”¥ ìŠ¤ìº” (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)")
    parser.add_argument("--quiet", action="store_true", help="ìµœì†Œ ì¶œë ¥ ëª¨ë“œ")
    parser.add_argument("--output", "-o", help="ì¶œë ¥ íŒŒì¼ëª…")
    
    args = parser.parse_args()
    
    # ë”¥ ìŠ¤ìº” ê²½ê³ 
    if args.deep:
        print("âš ï¸  ì „ì²´ ì‹œìŠ¤í…œ ë”¥ ìŠ¤ìº”ì€ ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        response = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? [y/N]: ")
        if response.lower() not in ['y', 'yes']:
            print("ìŠ¤ìº”ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
    
    scanner = CompleteSystemModelScanner(
        deep_scan=args.deep,
        verbose=not args.quiet
    )
    
    result_file = scanner.run_complete_scan()
    
    if result_file:
        print(f"\nğŸ‰ ìŠ¤ìº” ì™„ë£Œ! ê²°ê³¼: {result_file}")
    else:
        print("âŒ ìŠ¤ìº” ì‹¤íŒ¨")
        sys.exit(1)

if __name__ == "__main__":
    main()