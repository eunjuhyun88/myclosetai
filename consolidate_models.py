#!/usr/bin/env python3
"""
ğŸ”¥ AI ëª¨ë¸ ì™„ì „ í†µí•© ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸ v3.0
ëª¨ë“  ì¤‘ë³µëœ .pth íŒŒì¼ë“¤ì„ backend/app/ai_pipeline/modelsë¡œ ì™„ì „ í†µí•©
âœ… ì¤‘ë³µ ì œê±° - ê°™ì€ íŒŒì¼ì€ í•˜ë‚˜ë§Œ ìœ ì§€  
âœ… Stepë³„ ìë™ ë¶„ë¥˜
âœ… íŒŒì¼ëª… ëë¶€ë¶„ ê¸°ì¤€ ìŠ¤ë§ˆíŠ¸ ë§¤ì¹­
âœ… ì•ˆì „í•œ ë³µì‚¬ (ì›ë³¸ ìœ ì§€)
"""

import os
import shutil
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Set
import hashlib
import re

class SmartModelConsolidator:
    """ìŠ¤ë§ˆíŠ¸ AI ëª¨ë¸ ì™„ì „ í†µí•© ì •ë¦¬ê¸°"""
    
    def __init__(self):
        self.project_root = Path("/Users/gimdudeul/MVP/mycloset-ai")
        self.target_path = self.project_root / "backend/app/ai_pipeline/models"
        self.search_paths = [
            self.project_root / "ai_models",
            self.project_root / "backend/ai_models"  
        ]
        
        # Stepë³„ ìµœì¢… ì •ë¦¬ êµ¬ì¡° 
        self.target_structure = {
            "step_01_human_parsing": [
                r".*schp.*atr.*\.pth$",
                r".*schp.*lip.*\.pth$", 
                r".*exp-schp.*atr.*\.pth$",
                r".*exp-schp.*lip.*\.pth$",
                r".*graphonomy.*\.pth$",
                r".*human.*parsing.*\.pth$",
                r".*atr.*model.*\.pth$",
                r".*lip.*model.*\.pth$"
            ],
            "step_02_pose_estimation": [
                r".*openpose.*\.pth$",
                r".*body.*pose.*model.*\.pth$",
                r".*hand.*pose.*model.*\.pth$"
            ],
            "step_03_cloth_segmentation": [
                r".*u2net.*\.pth$",
                r".*sam.*vit.*\.pth$",
                r".*cloth.*seg.*\.pth$",
                r".*model\.pth$"  # cloth_segmentation í´ë”ì˜ model.pth
            ],
            "step_04_geometric_matching": [
                r".*gmm.*\.pth$",
                r".*geometric.*\.pth$"
            ],
            "step_05_cloth_warping": [
                r".*tom.*\.pth$", 
                r".*warp.*\.pth$"
            ],
            "step_06_virtual_fitting": [
                r".*hrviton.*\.pth$",
                r".*ootd.*\.pth$",
                r".*vton.*\.pth$",
                r".*virtual.*fitting.*\.pth$"
            ],
            "step_07_post_processing": [
                r".*codeformer.*\.pth$",
                r".*gfpgan.*\.pth$", 
                r".*real.*esrgan.*\.pth$",
                r".*swinir.*\.pth$",
                r".*enhancer.*\.pth$",
                r".*upscale.*\.pth$"
            ],
            "step_08_quality_assessment": [
                r".*clip.*\.pth$",
                r".*quality.*\.pth$"
            ]
        }
        
        # ê²°ê³¼ ì¶”ì 
        self.results = {
            "found_files": [],
            "moved_files": [],
            "duplicates_removed": [],
            "errors": []
        }
        
    def find_all_pth_files(self) -> List[Path]:
        """ëª¨ë“  .pth íŒŒì¼ ì°¾ê¸°"""
        print("ğŸ” ì „ì²´ .pth íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        
        all_files = []
        for search_path in self.search_paths:
            if search_path.exists():
                pth_files = list(search_path.rglob("*.pth"))
                all_files.extend(pth_files)
                print(f"ğŸ“‚ {search_path}: {len(pth_files)}ê°œ ë°œê²¬")
        
        # ì¤‘ë³µ ê²½ë¡œ ì œê±° (ì‹¤ì œ ê°™ì€ íŒŒì¼)
        unique_files = []
        seen_paths = set()
        
        for file_path in all_files:
            try:
                resolved_path = file_path.resolve()
                if resolved_path not in seen_paths:
                    seen_paths.add(resolved_path)
                    unique_files.append(file_path)
            except:
                unique_files.append(file_path)
        
        print(f"ğŸ“Š ì´ ë°œê²¬: {len(all_files)}ê°œ, ê³ ìœ  íŒŒì¼: {len(unique_files)}ê°œ")
        
        # í¬ê¸°ë³„ ì •ë ¬ (í° íŒŒì¼ë¶€í„° - ë” ì™„ì „í•œ ëª¨ë¸ì¼ ê°€ëŠ¥ì„±)
        unique_files.sort(key=lambda x: self.get_file_size(x), reverse=True)
        
        return unique_files
    
    def get_file_size(self, file_path: Path) -> int:
        """íŒŒì¼ í¬ê¸° ê°€ì ¸ì˜¤ê¸°"""
        try:
            return file_path.stat().st_size
        except:
            return 0
    
    def get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except:
            return ""
    
    def classify_by_filename(self, file_path: Path) -> Tuple[str, float]:
        """íŒŒì¼ëª… ëë¶€ë¶„ ê¸°ì¤€ìœ¼ë¡œ Step ë¶„ë¥˜"""
        filename = file_path.name.lower()
        
        # Stepë³„ íŒ¨í„´ ë§¤ì¹­
        for step_name, patterns in self.target_structure.items():
            for pattern in patterns:
                if re.search(pattern, filename):
                    # íŒŒì¼ í¬ê¸°ë¡œ ìš°ì„ ìˆœìœ„ ê²°ì • (í° íŒŒì¼ì´ ë” ì™„ì „)
                    size_mb = self.get_file_size(file_path) / (1024 * 1024)
                    confidence = min(10.0, 5.0 + (size_mb / 100))  # í¬ê¸°ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
                    return step_name, confidence
        
        # íŠ¹ë³„í•œ ê²½ìš°ë“¤
        special_cases = {
            "hrviton_final.pth": ("step_06_virtual_fitting", 10.0),
            "openpose_body.pth": ("step_02_pose_estimation", 10.0), 
            "u2net.pth": ("step_03_cloth_segmentation", 10.0),
            "graphonomy.pth": ("step_01_human_parsing", 10.0)
        }
        
        for special_name, (step, score) in special_cases.items():
            if special_name in filename:
                return step, score
        
        return "misc", 0.0
    
    def remove_duplicates(self, file_list: List[Path]) -> List[Path]:
        """ì¤‘ë³µ íŒŒì¼ ì œê±° (í•´ì‹œ ê¸°ì¤€)"""
        print("ğŸ”„ ì¤‘ë³µ íŒŒì¼ ì œê±° ì¤‘...")
        
        hash_to_file = {}
        unique_files = []
        
        for file_path in file_list:
            file_hash = self.get_file_hash(file_path)
            
            if not file_hash:
                unique_files.append(file_path)  # í•´ì‹œ ì‹¤íŒ¨ì‹œ ì¼ë‹¨ í¬í•¨
                continue
                
            if file_hash in hash_to_file:
                # ì¤‘ë³µ ë°œê²¬ - ë” ì¢‹ì€ ìœ„ì¹˜ì˜ íŒŒì¼ ì„ íƒ
                existing_file = hash_to_file[file_hash]
                
                # backup í´ë”ì— ìˆëŠ” íŒŒì¼ì€ ì œì™¸
                if "backup" in str(file_path).lower():
                    self.results["duplicates_removed"].append({
                        "removed": str(file_path),
                        "kept": str(existing_file),
                        "reason": "backup_file"
                    })
                    continue
                elif "backup" in str(existing_file).lower():
                    # ê¸°ì¡´ íŒŒì¼ì´ ë°±ì—…ì´ë©´ ìƒˆ íŒŒì¼ë¡œ êµì²´
                    hash_to_file[file_hash] = file_path
                    unique_files = [f for f in unique_files if f != existing_file]
                    unique_files.append(file_path)
                    self.results["duplicates_removed"].append({
                        "removed": str(existing_file),
                        "kept": str(file_path), 
                        "reason": "replaced_backup"
                    })
                else:
                    # íŒŒì¼ í¬ê¸°ë¡œ ê²°ì • (í° íŒŒì¼ ìš°ì„ )
                    if self.get_file_size(file_path) > self.get_file_size(existing_file):
                        hash_to_file[file_hash] = file_path
                        unique_files = [f for f in unique_files if f != existing_file]
                        unique_files.append(file_path)
                        self.results["duplicates_removed"].append({
                            "removed": str(existing_file),
                            "kept": str(file_path),
                            "reason": "larger_file"
                        })
                    else:
                        self.results["duplicates_removed"].append({
                            "removed": str(file_path),
                            "kept": str(existing_file),
                            "reason": "smaller_file"
                        })
            else:
                hash_to_file[file_hash] = file_path
                unique_files.append(file_path)
        
        print(f"ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {len(file_list) - len(unique_files)}ê°œ")
        return unique_files
    
    def copy_file_safely(self, source: Path, target_dir: Path, new_name: str = None) -> bool:
        """ì•ˆì „í•œ íŒŒì¼ ë³µì‚¬ - ì¤‘ë³µë˜ë©´ ë²ˆí˜¸ ì¶”ê°€"""
        try:
            target_dir.mkdir(parents=True, exist_ok=True)
            
            if new_name:
                base_name = Path(new_name).stem
                extension = Path(new_name).suffix
            else:
                base_name = source.stem
                extension = source.suffix
                
            target_path = target_dir / f"{base_name}{extension}"
            
            # ì¤‘ë³µë˜ë©´ ë²ˆí˜¸ ì¶”ê°€ (_01, _02, ...)
            counter = 1
            while target_path.exists():
                target_path = target_dir / f"{base_name}_{counter:02d}{extension}"
                counter += 1
            
            if counter > 1:
                print(f"ğŸ”¢ ì´ë¦„ ë³€ê²½: {source.name} â†’ {target_path.name}")
            
            shutil.copy2(source, target_path)
            
            # ë³µì‚¬ ê²€ì¦
            if target_path.exists() and target_path.stat().st_size > 0:
                size_mb = target_path.stat().st_size / (1024 * 1024)
                self.results["moved_files"].append({
                    "source": str(source),
                    "target": str(target_path),
                    "size_mb": round(size_mb, 1)
                })
                return True
            else:
                raise Exception("ë³µì‚¬ ê²€ì¦ ì‹¤íŒ¨")
                
        except Exception as e:
            self.results["errors"].append({
                "file": str(source),
                "error": str(e)
            })
            return False
    
    def consolidate_all_models(self) -> bool:
        """ë©”ì¸ í†µí•© í”„ë¡œì„¸ìŠ¤"""
        print("ğŸš€ AI ëª¨ë¸ ì™„ì „ í†µí•© ì‹œì‘!")
        print("=" * 60)
        
        # 1. ëª¨ë“  .pth íŒŒì¼ ì°¾ê¸°
        all_files = self.find_all_pth_files()
        self.results["found_files"] = [str(f) for f in all_files]
        
        if not all_files:
            print("âŒ .pth íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # 2. ëª¨ë“  íŒŒì¼ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì¤‘ë³µ ì œê±° ì•ˆí•¨)
        unique_files = all_files
        print("ğŸ“‹ ëª¨ë“  íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ì´ë™í•©ë‹ˆë‹¤ (ì¤‘ë³µ ì œê±° ì•ˆí•¨)")
        
        # 3. ëŒ€ìƒ ë””ë ‰í† ë¦¬ ì¤€ë¹„
        checkpoints_dir = self.target_path / "checkpoints"
        checkpoints_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ”„ ëª¨ë“  ëª¨ë¸ ì´ë™ ì¤‘... ({len(unique_files)}ê°œ)")
        
        step_counts = {}
        misc_count = 0
        
        # 4. íŒŒì¼ë³„ ë¶„ë¥˜ ë° ì´ë™
        for file_path in unique_files:
            try:
                step_name, confidence = self.classify_by_filename(file_path)
                
                if step_name == "misc":
                    # ë¶„ë¥˜ë˜ì§€ ì•Šì€ íŒŒì¼ì€ misc í´ë”ë¡œ
                    target_dir = checkpoints_dir / "misc"
                    print(f"â“ ë¶„ë¥˜ ë¶ˆê°€: {file_path.name}")
                    misc_count += 1
                else:
                    # Stepë³„ í´ë”ë¡œ ì´ë™
                    target_dir = checkpoints_dir / step_name
                    print(f"âœ… {step_name}: {file_path.name} ({confidence:.1f})")
                    step_counts[step_name] = step_counts.get(step_name, 0) + 1
                
                # íŒŒì¼ ë³µì‚¬
                self.copy_file_safely(file_path, target_dir)
                
            except Exception as e:
                print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
                self.results["errors"].append({
                    "file": str(file_path),
                    "error": str(e)
                })
        
        # 5. ê²°ê³¼ ì¶œë ¥
        self.print_final_summary(step_counts, misc_count)
        self.save_consolidation_report()
        
        return len(self.results["errors"]) == 0
    
    def print_final_summary(self, step_counts: Dict[str, int], misc_count: int):
        """ìµœì¢… ê²°ê³¼ ìš”ì•½"""
        print("\n" + "=" * 60)
        print("ğŸ‰ AI ëª¨ë¸ ì™„ì „ í†µí•© ì™„ë£Œ!")
        print("=" * 60)
        
        total_found = len(self.results["found_files"])
        total_moved = len(self.results["moved_files"])
        total_errors = len(self.results["errors"])
        
        print(f"ğŸ“Š ì „ì²´ ë°œê²¬: {total_found}ê°œ")
        print(f"âœ… ëª¨ë‘ ì´ë™: {total_moved}ê°œ")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {total_errors}ê°œ")
        
        if step_counts:
            print(f"\nğŸ“ Stepë³„ ì •ë¦¬ ê²°ê³¼:")
            for step_name in sorted(step_counts.keys()):
                count = step_counts[step_name]
                print(f"  {step_name}: {count}ê°œ")
            
            if misc_count > 0:
                print(f"  misc (ë¶„ë¥˜ ë¶ˆê°€): {misc_count}ê°œ")
        
        print(f"\nğŸ“ ìµœì¢… ì •ë¦¬ ìœ„ì¹˜:")
        print(f"  {self.target_path}/checkpoints/")
        for step_name in sorted(step_counts.keys()):
            print(f"    â”œâ”€â”€ {step_name}/")
        if misc_count > 0:
            print(f"    â””â”€â”€ misc/")
    
    def save_consolidation_report(self):
        """í†µí•© ë¦¬í¬íŠ¸ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.target_path / f"consolidation_report_{timestamp}.json"
        
        report = {
            "timestamp": timestamp,
            "summary": {
                "total_found": len(self.results["found_files"]),
                "total_moved": len(self.results["moved_files"]),
                "duplicates_removed": len(self.results["duplicates_removed"]),
                "errors": len(self.results["errors"])
            },
            "details": self.results
        }
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“Š ìƒì„¸ ë¦¬í¬íŠ¸: {report_path}")
        except Exception as e:
            print(f"âš ï¸ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸ¤– MyCloset AI ëª¨ë¸ ì™„ì „ í†µí•© ì •ë¦¬ê¸° v3.0")
    print("=" * 60)
    
    consolidator = SmartModelConsolidator()
    
    # ê²½ë¡œ í™•ì¸
    for path in consolidator.search_paths:
        if path.exists():
            print(f"âœ… ì†ŒìŠ¤ ê²½ë¡œ: {path}")
        else:
            print(f"âŒ ì†ŒìŠ¤ ê²½ë¡œ ì—†ìŒ: {path}")
    
    print(f"ğŸ“ í†µí•© ëŒ€ìƒ: {consolidator.target_path}")
    
    # í™•ì¸
    response = input("\nğŸ”„ ëª¨ë“  .pth íŒŒì¼ì„ ë³µì‚¬í•´ì„œ ì´ë™í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ì¤‘ë³µ ì œê±° ì•ˆí•¨) (y/N): ").strip().lower()
    if response not in ['y', 'yes']:
        print("âŒ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return False
    
    # ì‹¤í–‰
    success = consolidator.consolidate_all_models()
    
    if success:
        print("\nğŸ‰ ëª¨ë“  ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ì´ì œ ì¤‘ë³µ ì—†ì´ ê¹”ë”í•˜ê²Œ ì •ë¦¬ëœ ëª¨ë¸ë“¤ì„ ì‚¬ìš©í•˜ì„¸ìš”!")
    else:
        print("\nâš ï¸ ì¼ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    return success

if __name__ == "__main__":
    main()