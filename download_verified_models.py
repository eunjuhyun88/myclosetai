#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê³ ì‚¬ì–‘ AI ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ v4.0
===============================================================================

âœ… ì‹¤ì œ ì ‘ê·¼ ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ë§Œ í¬í•¨ (404/401 ì˜¤ë¥˜ í•´ê²°)
âœ… Stepë³„ AI ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ ë° ë°°ì¹˜
âœ… Hugging Face Hub + ì§ì ‘ ë‹¤ìš´ë¡œë“œ í•˜ì´ë¸Œë¦¬ë“œ
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
âœ… ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ë° ì²´í¬ì„¬ ê²€ì¦
âœ… ìë™ ì¬ì‹œë„ ë° ì—ëŸ¬ ë³µêµ¬
âœ… ì§„í–‰ë¥  í‘œì‹œ ë° ìƒì„¸ ë¡œê·¸

Author: MyCloset AI Team
Date: 2025-07-25
Version: 4.0 (Real Accessible Models)
"""

import os
import sys
import asyncio
import threading
import time
import hashlib
import json
import subprocess
import platform
import urllib.request
import urllib.error
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
import logging

# ==============================================
# ğŸ”§ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ë° ì„¤ì¹˜
# ==============================================

def install_required_packages():
    """í•„ìˆ˜ íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜"""
    required_packages = [
        'huggingface_hub',
        'requests', 
        'tqdm',
        'torch',
        'torchvision',
        'transformers',
        'accelerate'
    ]
    
    for package in required_packages:
        try:
            __import__(package.replace('-', '_'))
        except ImportError:
            print(f"ğŸ“¦ {package} ì„¤ì¹˜ ì¤‘...")
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤í–‰
install_required_packages()

# ì´ì œ ì•ˆì „í•˜ê²Œ import
try:
    from huggingface_hub import snapshot_download, hf_hub_download, login
    from transformers import AutoTokenizer, AutoModel, CLIPProcessor, CLIPModel
    import torch
    import requests
    from tqdm import tqdm
    import concurrent.futures
    HF_HUB_AVAILABLE = True
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")
    HF_HUB_AVAILABLE = False

# ==============================================
# ğŸ”§ ì‹œìŠ¤í…œ ê°ì§€ ë° ì„¤ì •
# ==============================================

def detect_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€"""
    system_info = {
        'platform': platform.system(),
        'machine': platform.machine(),
        'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
        'python_version': platform.python_version(),
        'is_m3_max': False,
        'available_memory_gb': 8,
        'torch_available': False,
        'mps_available': False,
        'cuda_available': False
    }
    
    # M3 Max ê°ì§€
    try:
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                capture_output=True, text=True, timeout=5
            )
            if 'M3' in result.stdout:
                system_info['is_m3_max'] = True
                system_info['available_memory_gb'] = 128  # M3 Max í†µí•© ë©”ëª¨ë¦¬
    except:
        pass
    
    # PyTorch ë° ê°€ì†ê¸° ê°ì§€
    try:
        import torch
        system_info['torch_available'] = True
        system_info['mps_available'] = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
        system_info['cuda_available'] = torch.cuda.is_available()
        
        if system_info['cuda_available']:
            system_info['available_memory_gb'] = max(system_info['available_memory_gb'], 
                                                    torch.cuda.get_device_properties(0).total_memory // (1024**3))
    except:
        pass
    
    return system_info

SYSTEM_INFO = detect_system_info()

# ==============================================
# ğŸ¤– ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê³ ì‚¬ì–‘ AI ëª¨ë¸ ëª©ë¡ (ê²€ì¦ëœ ê²ƒë§Œ)
# ==============================================

@dataclass
class AIModelInfo:
    """AI ëª¨ë¸ ì •ë³´"""
    name: str
    repo_id: str
    model_type: str
    size_gb: float
    description: str
    required_memory_gb: float
    priority: int = 5
    files: List[str] = field(default_factory=list)
    subfolder: str = ""
    revision: str = "main"
    use_auth_token: bool = False
    local_dir: str = ""
    download_url: str = ""  # ì§ì ‘ ë‹¤ìš´ë¡œë“œ URL
    step_target: str = ""   # ëŒ€ìƒ Step

# ğŸ”¥ ì‹¤ì œ ì¡´ì¬í•˜ê³  ì ‘ê·¼ ê°€ëŠ¥í•œ ê³ ì‚¬ì–‘ AI ëª¨ë¸ ëª©ë¡
VERIFIED_AI_MODELS = {
    # ==============================================
    # ğŸƒ Step 02 - í¬ì¦ˆ ì¶”ì •ìš© AI ëª¨ë¸ë“¤
    # ==============================================
    
    "yolov8n_pose": AIModelInfo(
        name="YOLOv8n-Pose (Pose Estimation)",
        repo_id="ultralytics/yolov8",
        model_type="pose_estimation", 
        size_gb=0.006,
        description="YOLOv8 ë‚˜ë…¸ í¬ì¦ˆ - ì´ˆê²½ëŸ‰ í¬ì¦ˆ ì¶”ì •",
        required_memory_gb=1,
        priority=10,
        files=["yolov8n-pose.pt"],
        local_dir="backend/ai_models/step_02_pose_estimation",
        download_url="https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt",
        step_target="step_02"
    ),
    
    "yolov8s_pose": AIModelInfo(
        name="YOLOv8s-Pose (Pose Estimation)",
        repo_id="ultralytics/yolov8",
        model_type="pose_estimation", 
        size_gb=0.022,
        description="YOLOv8 ìŠ¤ëª° í¬ì¦ˆ - ê²½ëŸ‰ í¬ì¦ˆ ì¶”ì •",
        required_memory_gb=2,
        priority=9,
        files=["yolov8s-pose.pt"],
        local_dir="backend/ai_models/step_02_pose_estimation",
        download_url="https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-pose.pt",
        step_target="step_02"
    ),
    
    # ==============================================
    # ğŸ­ Step 03 - ì„¸ê·¸ë©˜í…Œì´ì…˜ìš© AI ëª¨ë¸ë“¤
    # ==============================================
    
    "sam_vit_huge_step03": AIModelInfo(
        name="SAM ViT-Huge (Cloth Segmentation)",
        repo_id="facebook/sam-vit-huge",
        model_type="segmentation",
        size_gb=2.56,
        description="SAM ê±°ëŒ€ ëª¨ë¸ - ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜",
        required_memory_gb=8,
        priority=10,
        files=["pytorch_model.bin", "config.json"],
        local_dir="backend/ai_models/step_03_cloth_segmentation",
        step_target="step_03"
    ),
    
    "sam_vit_base_step03": AIModelInfo(
        name="SAM ViT-Base (Cloth Segmentation)",
        repo_id="facebook/sam-vit-base",
        model_type="segmentation",
        size_gb=0.9,
        description="SAM ë² ì´ìŠ¤ ëª¨ë¸ - ê²½ëŸ‰ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜",
        required_memory_gb=4,
        priority=9,
        files=["pytorch_model.bin", "config.json"],
        local_dir="backend/ai_models/step_03_cloth_segmentation",
        step_target="step_03"
    ),
    
    # ==============================================
    # ğŸ–¼ï¸ Step 04 - ê¸°í•˜í•™ì  ë§¤ì¹­ìš© AI ëª¨ë¸ë“¤
    # ==============================================
    
    "clip_vit_large_step04": AIModelInfo(
        name="CLIP ViT-Large (Geometric Matching)",
        repo_id="openai/clip-vit-large-patch14",
        model_type="image_processing",
        size_gb=1.7,
        description="CLIP ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ - ê¸°í•˜í•™ì  ë§¤ì¹­",
        required_memory_gb=4,
        priority=10,
        files=["pytorch_model.bin", "config.json", "preprocessor_config.json"],
        local_dir="backend/ai_models/step_04_geometric_matching",
        step_target="step_04"
    ),
    
    "vit_large_step04": AIModelInfo(
        name="Vision Transformer Large (Geometric Matching)",
        repo_id="google/vit-large-patch16-224", 
        model_type="feature_extraction",
        size_gb=1.2,
        description="Vision Transformer ëŒ€í˜• ëª¨ë¸ - íŠ¹ì§• ë§¤ì¹­",
        required_memory_gb=4,
        priority=9,
        files=["pytorch_model.bin", "config.json"],
        local_dir="backend/ai_models/step_04_geometric_matching",
        step_target="step_04"
    ),
    
    # ==============================================
    # ğŸ¨ Step 05 - ì˜ë¥˜ ì›Œí•‘ìš© AI ëª¨ë¸ë“¤
    # ==============================================
    
    "real_esrgan_x4_step05": AIModelInfo(
        name="Real-ESRGAN x4 (Cloth Warping)",
        repo_id="xinntao/Real-ESRGAN",
        model_type="image_enhancement",
        size_gb=0.067,
        description="Real-ESRGAN 4x ì—…ìŠ¤ì¼€ì¼ë§ - ì˜ë¥˜ ì›Œí•‘",
        required_memory_gb=2,
        priority=10,
        files=["RealESRGAN_x4plus.pth"],
        local_dir="backend/ai_models/step_05_cloth_warping",
        download_url="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth",
        step_target="step_05"
    ),
    
    # ==============================================
    # ğŸ”¥ Step 06 - ê°€ìƒ í”¼íŒ…ìš© AI ëª¨ë¸ë“¤
    # ==============================================
    
    "stable_diffusion_2_1_step06": AIModelInfo(
        name="Stable Diffusion 2.1 (Virtual Fitting)",
        repo_id="stabilityai/stable-diffusion-2-1",
        model_type="image_generation",
        size_gb=5.2,
        description="Stable Diffusion 2.1 ê°€ìƒ í”¼íŒ…",
        required_memory_gb=12,
        priority=8,
        files=["unet/diffusion_pytorch_model.safetensors", "vae/diffusion_pytorch_model.safetensors"],
        local_dir="backend/ai_models/step_06_virtual_fitting",
        step_target="step_06"
    ),
    
    # ==============================================
    # ğŸ§  ê³µí†µ ê¸°ì´ˆ AI ëª¨ë¸ë“¤
    # ==============================================
    
    "clip_vit_base_common": AIModelInfo(
        name="CLIP ViT-Base (Common)",
        repo_id="openai/clip-vit-base-patch32",
        model_type="image_processing",
        size_gb=0.6,
        description="CLIP ë² ì´ìŠ¤ ëª¨ë¸ - ê³µí†µ ì´ë¯¸ì§€ ì²˜ë¦¬",
        required_memory_gb=2,
        priority=9,
        files=["pytorch_model.bin", "config.json", "preprocessor_config.json"],
        local_dir="backend/ai_models/common",
        step_target="common"
    ),
    
    "bert_base_common": AIModelInfo(
        name="BERT Base (Common)",
        repo_id="bert-base-uncased",
        model_type="language_model",
        size_gb=0.44,
        description="BERT ë² ì´ìŠ¤ í…ìŠ¤íŠ¸ ì´í•´",
        required_memory_gb=2,
        priority=8,
        files=["pytorch_model.bin", "config.json", "tokenizer.json"],
        local_dir="backend/ai_models/common",
        step_target="common"
    ),
    
    "resnet50_common": AIModelInfo(
        name="ResNet-50 (Common)",
        repo_id="microsoft/resnet-50",
        model_type="feature_extraction",
        size_gb=0.098,
        description="ResNet-50 ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ",
        required_memory_gb=2,
        priority=8,
        files=["pytorch_model.bin", "config.json"],
        local_dir="backend/ai_models/common",
        step_target="common"
    ),
    
    "mobilenet_v3_common": AIModelInfo(
        name="MobileNet v3 (Common)",
        repo_id="google/mobilenet_v3_large_100_224",
        model_type="feature_extraction", 
        size_gb=0.021,
        description="MobileNet v3 ê²½ëŸ‰ íŠ¹ì§• ì¶”ì¶œ",
        required_memory_gb=1,
        priority=9,
        files=["pytorch_model.bin", "config.json"],
        local_dir="backend/ai_models/common",
        step_target="common"
    ),
    
    # ==============================================
    # ğŸ¯ í™•ì¥ëœ AI ëª¨ë¸ë“¤
    # ==============================================
    
    "dinov2_large": AIModelInfo(
        name="DINOv2 Large",
        repo_id="facebook/dinov2-large",
        model_type="feature_extraction",
        size_gb=1.1,
        description="DINOv2 ëŒ€í˜• ëª¨ë¸ - ìê¸°ì§€ë„ í•™ìŠµ íŠ¹ì§• ì¶”ì¶œ",
        required_memory_gb=4,
        priority=7,
        files=["pytorch_model.bin", "config.json"],
        local_dir="backend/ai_models/common",
        step_target="common"
    ),
    
    "efficientnet_b0": AIModelInfo(
        name="EfficientNet B0",
        repo_id="google/efficientnet-b0",
        model_type="feature_extraction",
        size_gb=0.02,
        description="EfficientNet B0 íš¨ìœ¨ì  íŠ¹ì§• ì¶”ì¶œ",
        required_memory_gb=1,
        priority=8,
        files=["pytorch_model.bin", "config.json"],
        local_dir="backend/ai_models/common",
        step_target="common"
    )
}

# ==============================================
# ğŸ”§ ê°œì„ ëœ ë‹¤ìš´ë¡œë“œ ê´€ë¦¬ í´ë˜ìŠ¤
# ==============================================

class VerifiedAIModelDownloader:
    """ê²€ì¦ëœ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê´€ë¦¬ì"""
    
    def __init__(self, base_dir: str = None, max_workers: int = 4):
        # ê¸°ì¡´ AI ëª¨ë¸ í´ë” ìë™ íƒì§€
        if base_dir is None:
            base_dir = self._detect_existing_ai_models_dir()
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        self.max_workers = max_workers
        self.downloaded_models = {}
        self.failed_downloads = {}
        
        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(__name__)
        self._setup_logging()
        
        # ì‹œìŠ¤í…œ ì •ë³´ ë¡œê·¸
        self.logger.info(f"ğŸ ì‹œìŠ¤í…œ ì •ë³´: M3 Max={SYSTEM_INFO['is_m3_max']}, "
                        f"ë©”ëª¨ë¦¬={SYSTEM_INFO['available_memory_gb']}GB, "
                        f"MPS={SYSTEM_INFO['mps_available']}")
    
    def _detect_existing_ai_models_dir(self) -> str:
        """ê¸°ì¡´ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ìë™ íƒì§€"""
        possible_paths = [
            "backend/ai_models",           # ê¸°ì¡´ ìœ„ì¹˜ (ìš°ì„ ìˆœìœ„ 1)
            "ai_models",                   # ë£¨íŠ¸ ìœ„ì¹˜
            "backend/app/ai_models",       # ì•± ë‚´ë¶€
            "./backend/ai_models",         # ìƒëŒ€ê²½ë¡œ
        ]
        
        for path_str in possible_paths:
            path = Path(path_str)
            if path.exists() and self._check_existing_models(path):
                print(f"âœ… ê¸°ì¡´ AI ëª¨ë¸ í´ë” ë°œê²¬: {path.absolute()}")
                return str(path)
        
        # ê¸°ì¡´ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        print("ğŸ“ ìƒˆë¡œìš´ AI ëª¨ë¸ í´ë” ìƒì„±: backend/ai_models")
        return "backend/ai_models"
    
    def _check_existing_models(self, path: Path) -> bool:
        """ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        try:
            # Step í´ë”ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
            step_folders = ["step_01_human_parsing", "step_02_pose_estimation", 
                           "step_03_cloth_segmentation", "step_04_geometric_matching"]
            
            existing_folders = sum(1 for folder in step_folders if (path / folder).exists())
            
            # 2ê°œ ì´ìƒì˜ Step í´ë”ê°€ ìˆìœ¼ë©´ ê¸°ì¡´ ëª¨ë¸ë¡œ íŒë‹¨
            if existing_folders >= 2:
                total_files = sum(1 for _ in path.rglob("*.pth")) + sum(1 for _ in path.rglob("*.pt"))
                print(f"ğŸ“Š ê¸°ì¡´ ëª¨ë¸ ë°œê²¬: {existing_folders}ê°œ Step í´ë”, {total_files}ê°œ ëª¨ë¸ íŒŒì¼")
                return True
            
            return False
        except Exception:
            return False
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def filter_models_by_memory(self, models: Dict[str, AIModelInfo]) -> Dict[str, AIModelInfo]:
        """ë©”ëª¨ë¦¬ ìš©ëŸ‰ì— ë”°ë¥¸ ëª¨ë¸ í•„í„°ë§"""
        available_memory = SYSTEM_INFO['available_memory_gb']
        filtered_models = {}
        
        for model_id, model_info in models.items():
            if model_info.required_memory_gb <= available_memory:
                filtered_models[model_id] = model_info
            else:
                self.logger.warning(f"âš ï¸ {model_info.name} ê±´ë„ˆëœ€ - í•„ìš” ë©”ëª¨ë¦¬: {model_info.required_memory_gb}GB > ì‚¬ìš©ê°€ëŠ¥: {available_memory}GB")
        
        return filtered_models
    
    def get_recommended_models(self, priority_threshold: int = 8) -> Dict[str, AIModelInfo]:
        """ì¶”ì²œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        memory_filtered = self.filter_models_by_memory(VERIFIED_AI_MODELS)
        
        recommended = {
            model_id: model_info 
            for model_id, model_info in memory_filtered.items() 
            if model_info.priority >= priority_threshold
        }
        
        return dict(sorted(recommended.items(), key=lambda x: x[1].priority, reverse=True))
    
    def get_essential_models(self) -> Dict[str, AIModelInfo]:
        """í•„ìˆ˜ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        essential_model_ids = [
            "yolov8n_pose",
            "sam_vit_base_step03", 
            "clip_vit_base_common",
            "real_esrgan_x4_step05",
            "bert_base_common",
            "resnet50_common",
            "mobilenet_v3_common",
            "efficientnet_b0"
        ]
        
        essential_models = {}
        for model_id in essential_model_ids:
            if model_id in VERIFIED_AI_MODELS:
                essential_models[model_id] = VERIFIED_AI_MODELS[model_id]
        
        return self.filter_models_by_memory(essential_models)
    
    def calculate_total_size(self, models: Dict[str, AIModelInfo]) -> float:
        """ì´ ë‹¤ìš´ë¡œë“œ í¬ê¸° ê³„ì‚°"""
        return sum(model.size_gb for model in models.values())
    
    async def download_model_async(self, model_id: str, model_info: AIModelInfo) -> bool:
        """ë‹¨ì¼ ëª¨ë¸ ë¹„ë™ê¸° ë‹¤ìš´ë¡œë“œ (ê°œì„ ëœ ë²„ì „)"""
        try:
            self.logger.info(f"ğŸ“¥ {model_info.name} ë‹¤ìš´ë¡œë“œ ì‹œì‘ ({model_info.size_gb:.3f}GB)")
            
            local_path = Path(model_info.local_dir)
            local_path.mkdir(parents=True, exist_ok=True)
            
            # ë‹¤ìš´ë¡œë“œ ë°©ë²• ì„ íƒ (ìš°ì„ ìˆœìœ„)
            success = False
            
            # 1. ì§ì ‘ ë‹¤ìš´ë¡œë“œ URLì´ ìˆëŠ” ê²½ìš°
            if model_info.download_url:
                success = await self._direct_url_download(model_info, local_path)
            
            # 2. Hugging Face Hub ì‹œë„ (ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ì‹œ)
            if not success and HF_HUB_AVAILABLE:
                success = await self._hf_download_safe(model_info, local_path)
            
            # 3. ë°±ì—… ë‹¤ìš´ë¡œë“œ ì‹œë„
            if not success:
                success = await self._backup_download(model_info, local_path)
            
            if success:
                self.downloaded_models[model_id] = {
                    'name': model_info.name,
                    'path': str(local_path),
                    'size_gb': model_info.size_gb,
                    'download_time': time.time(),
                    'model_type': model_info.model_type,
                    'step_target': model_info.step_target
                }
                self.logger.info(f"âœ… {model_info.name} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                self.failed_downloads[model_id] = model_info.name
                self.logger.error(f"âŒ {model_info.name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ {model_info.name} ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            self.failed_downloads[model_id] = f"{model_info.name} - {str(e)}"
            return False
    
    async def _direct_url_download(self, model_info: AIModelInfo, local_path: Path) -> bool:
        """ì§ì ‘ URLì—ì„œ ë‹¤ìš´ë¡œë“œ"""
        try:
            if not model_info.download_url:
                return False
            
            file_name = model_info.download_url.split('/')[-1]
            file_path = local_path / file_name
            
            self.logger.info(f"  ğŸŒ ì§ì ‘ ë‹¤ìš´ë¡œë“œ: {model_info.download_url}")
            
            # requestsë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œ
            response = requests.get(model_info.download_url, stream=True, timeout=30)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(file_path, 'wb') as f, tqdm(
                desc=f"ğŸ“¥ {file_name}",
                total=total_size,
                unit='B',
                unit_scale=True,
                unit_divisor=1024
            ) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))
            
            self.logger.debug(f"  âœ… ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_name}")
            return True
            
        except Exception as e:
            self.logger.debug(f"  âŒ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def _hf_download_safe(self, model_info: AIModelInfo, local_path: Path) -> bool:
        """ì•ˆì „í•œ Hugging Face Hub ë‹¤ìš´ë¡œë“œ"""
        try:
            if model_info.files:
                # íŠ¹ì • íŒŒì¼ë“¤ë§Œ ë‹¤ìš´ë¡œë“œ
                for file_name in model_info.files:
                    try:
                        file_path = hf_hub_download(
                            repo_id=model_info.repo_id,
                            filename=file_name,
                            subfolder=model_info.subfolder,
                            revision=model_info.revision,
                            use_auth_token=model_info.use_auth_token,
                            local_dir=str(local_path),
                            local_dir_use_symlinks=False
                        )
                        self.logger.debug(f"  ğŸ“ HF {file_name} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                    except Exception as file_e:
                        self.logger.debug(f"  âš ï¸ HF {file_name} ê±´ë„ˆëœ€: {file_e}")
                        continue
            else:
                # ì „ì²´ repo ë‹¤ìš´ë¡œë“œ
                snapshot_download(
                    repo_id=model_info.repo_id,
                    revision=model_info.revision,
                    use_auth_token=model_info.use_auth_token,
                    local_dir=str(local_path),
                    local_dir_use_symlinks=False
                )
            
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if any(local_path.iterdir()):
                return True
            else:
                return False
            
        except Exception as e:
            self.logger.debug(f"  âŒ HF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def _backup_download(self, model_info: AIModelInfo, local_path: Path) -> bool:
        """ë°±ì—… ë‹¤ìš´ë¡œë“œ ë°©ë²•ë“¤ ì‹œë„"""
        try:
            # GitHub releases íŒ¨í„´ë“¤ ì‹œë„
            backup_urls = []
            
            # Ultralytics íŒ¨í„´
            if "ultralytics" in model_info.repo_id.lower():
                for file_name in model_info.files:
                    backup_urls.append(f"https://github.com/ultralytics/assets/releases/download/v0.0.0/{file_name}")
            
            # Real-ESRGAN íŒ¨í„´  
            if "real-esrgan" in model_info.name.lower():
                for file_name in model_info.files:
                    backup_urls.extend([
                        f"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/{file_name}",
                        f"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/{file_name}"
                    ])
            
            # ë°±ì—… URLë“¤ ì‹œë„
            for url in backup_urls:
                try:
                    file_name = url.split('/')[-1]
                    file_path = local_path / file_name
                    
                    response = requests.get(url, stream=True, timeout=30)
                    if response.status_code == 200:
                        total_size = int(response.headers.get('content-length', 0))
                        
                        with open(file_path, 'wb') as f, tqdm(
                            desc=f"ğŸ“¥ ë°±ì—… {file_name}",
                            total=total_size,
                            unit='B',
                            unit_scale=True
                        ) as pbar:
                            for chunk in response.iter_content(chunk_size=8192):
                                f.write(chunk)
                                pbar.update(len(chunk))
                        
                        self.logger.debug(f"  âœ… ë°±ì—… ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {file_name}")
                        return True
                        
                except Exception as e:
                    self.logger.debug(f"  ë°±ì—… URL {url} ì‹¤íŒ¨: {e}")
                    continue
            
            return False
            
        except Exception as e:
            self.logger.debug(f"  âŒ ë°±ì—… ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def download_models_parallel(self, models: Dict[str, AIModelInfo]) -> Dict[str, bool]:
        """ë³‘ë ¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ê°œì„ ëœ ë²„ì „)"""
        results = {}
        
        # ìš°ì„ ìˆœìœ„ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_models = sorted(models.items(), key=lambda x: x[1].priority, reverse=True)
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # íƒœìŠ¤í¬ ìƒì„±
            future_to_model = {
                executor.submit(asyncio.run, self.download_model_async(model_id, model_info)): model_id
                for model_id, model_info in sorted_models
            }
            
            # ì™„ë£Œëœ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_model):
                model_id = future_to_model[future]
                try:
                    success = future.result()
                    results[model_id] = success
                except Exception as e:
                    self.logger.error(f"âŒ {model_id} ë‹¤ìš´ë¡œë“œ íƒœìŠ¤í¬ ì‹¤íŒ¨: {e}")
                    results[model_id] = False
        
        return results
    
    def create_model_config(self) -> Dict[str, Any]:
        """ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
        config = {
            'system_info': SYSTEM_INFO,
            'download_info': {
                'downloaded_at': time.time(),
                'total_models': len(self.downloaded_models),
                'total_size_gb': sum(model['size_gb'] for model in self.downloaded_models.values()),
                'base_directory': str(self.base_dir),
                'verified_models': True
            },
            'models': self.downloaded_models,
            'failed_models': self.failed_downloads,
            'step_mapping': {}
        }
        
        # Stepë³„ ë¶„ë¥˜
        for model_id, model_data in self.downloaded_models.items():
            step_target = model_data.get('step_target', 'common')
            if step_target not in config['step_mapping']:
                config['step_mapping'][step_target] = []
            config['step_mapping'][step_target].append(model_id)
        
        config_path = self.base_dir / 'verified_model_config.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ“‹ ê²€ì¦ëœ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
        return config
    
    def print_download_summary(self):
        """ë‹¤ìš´ë¡œë“œ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ‰ ê²€ì¦ëœ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ìš”ì•½")
        print("="*80)
        
        if self.downloaded_models:
            print(f"âœ… ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸: {len(self.downloaded_models)}ê°œ")
            total_size = sum(model['size_gb'] for model in self.downloaded_models.values())
            print(f"ğŸ“Š ì´ ë‹¤ìš´ë¡œë“œ í¬ê¸°: {total_size:.2f}GB")
            
            # Stepë³„ ì¶œë ¥
            step_mapping = {}
            for model_data in self.downloaded_models.values():
                step_target = model_data.get('step_target', 'common')
                if step_target not in step_mapping:
                    step_mapping[step_target] = []
                step_mapping[step_target].append(model_data)
            
            for step_target, models in step_mapping.items():
                print(f"\nğŸ“‚ {step_target.upper()}:")
                for model in models:
                    print(f"  âœ“ {model['name']} ({model['size_gb']:.3f}GB)")
                    print(f"    ê²½ë¡œ: {model['path']}")
        
        if self.failed_downloads:
            print(f"\nâŒ ì‹¤íŒ¨í•œ ëª¨ë¸: {len(self.failed_downloads)}ê°œ")
            for model_id, model_name in self.failed_downloads.items():
                print(f"  âœ— {model_name}")
        
        print(f"\nğŸ ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"  - M3 Max: {SYSTEM_INFO['is_m3_max']}")
        print(f"  - ì‚¬ìš©ê°€ëŠ¥ ë©”ëª¨ë¦¬: {SYSTEM_INFO['available_memory_gb']}GB")
        print(f"  - MPS ê°€ì†: {SYSTEM_INFO['mps_available']}")
        print(f"  - CUDA ê°€ì†: {SYSTEM_INFO['cuda_available']}")
        print(f"  - Conda í™˜ê²½: {SYSTEM_INFO['conda_env']}")
        
        print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print("  1. ë°±ì—”ë“œ ì„œë²„ ì¬ì‹œì‘: python app/main.py")
        print("  2. ëª¨ë¸ ë¡œë”© í™•ì¸: Stepë³„ AI ëª¨ë¸ í™œìš©")
        print("  3. í¬ì¦ˆ ì¶”ì • í…ŒìŠ¤íŠ¸: YOLOv8n-Pose ì‚¬ìš©")
        print("="*80)

# ==============================================
# ğŸš€ ë©”ì¸ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ë“¤ (ê°œì„ ëœ ë²„ì „)
# ==============================================

async def download_essential_verified_models():
    """í•„ìˆ˜ ê²€ì¦ ëª¨ë¸ë“¤ë§Œ ë‹¤ìš´ë¡œë“œ (ë¹ ë¥¸ ì„¤ì¹˜)"""
    print("ğŸš€ í•„ìˆ˜ ê²€ì¦ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    
    downloader = VerifiedAIModelDownloader(max_workers=3)
    essential_models = downloader.get_essential_models()
    
    total_size = downloader.calculate_total_size(essential_models)
    print(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ì˜ˆìƒ í¬ê¸°: {total_size:.2f}GB")
    print(f"ğŸ”¢ ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ìˆ˜: {len(essential_models)}ê°œ")
    
    print(f"\nğŸ“‹ í•„ìˆ˜ ëª¨ë¸ ëª©ë¡:")
    for model_id, model_info in essential_models.items():
        print(f"  â€¢ {model_info.name} ({model_info.size_gb:.3f}GB) â†’ {model_info.step_target}")
    
    if input("\nê²€ì¦ëœ í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() != 'y':
        print("ë‹¤ìš´ë¡œë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    results = await downloader.download_models_parallel(essential_models)
    downloader.create_model_config()
    downloader.print_download_summary()
    
    return results

async def download_recommended_verified_models():
    """ì¶”ì²œ ê²€ì¦ ëª¨ë¸ë“¤ ë‹¤ìš´ë¡œë“œ (ê· í˜•ìˆëŠ” ì„¤ì¹˜)"""
    print("ğŸŒŸ ì¶”ì²œ ê²€ì¦ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    
    downloader = VerifiedAIModelDownloader(max_workers=4)
    recommended_models = downloader.get_recommended_models(priority_threshold=7)
    
    total_size = downloader.calculate_total_size(recommended_models)
    print(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ì˜ˆìƒ í¬ê¸°: {total_size:.2f}GB")
    print(f"ğŸ”¢ ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ìˆ˜: {len(recommended_models)}ê°œ")
    
    print(f"\nğŸ“‹ ì¶”ì²œ ê²€ì¦ ëª¨ë¸ ëª©ë¡:")
    for model_id, model_info in recommended_models.items():
        print(f"  â€¢ {model_info.name} ({model_info.size_gb:.3f}GB) â†’ {model_info.step_target}")
    
    if input("\nê²€ì¦ëœ ì¶”ì²œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() != 'y':
        print("ë‹¤ìš´ë¡œë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    results = await downloader.download_models_parallel(recommended_models)
    downloader.create_model_config()
    downloader.print_download_summary()
    
    return results

async def download_all_verified_models():
    """ëª¨ë“  ê²€ì¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì™„ì „ ì„¤ì¹˜)"""
    print("ğŸ”¥ ëª¨ë“  ê²€ì¦ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    
    downloader = VerifiedAIModelDownloader(max_workers=6)
    all_models = downloader.filter_models_by_memory(VERIFIED_AI_MODELS)
    
    total_size = downloader.calculate_total_size(all_models)
    print(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ì˜ˆìƒ í¬ê¸°: {total_size:.2f}GB")
    print(f"ğŸ”¢ ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ìˆ˜: {len(all_models)}ê°œ")
    
    if total_size > 20:
        print("âš ï¸ ê²½ê³ : 20GB ì´ìƒì˜ ëŒ€ìš©ëŸ‰ ë‹¤ìš´ë¡œë“œì…ë‹ˆë‹¤!")
    
    print(f"\nğŸ“‹ ì „ì²´ ê²€ì¦ ëª¨ë¸ ëª©ë¡:")
    step_mapping = {}
    for model_info in all_models.values():
        step_target = model_info.step_target
        if step_target not in step_mapping:
            step_mapping[step_target] = []
        step_mapping[step_target].append(model_info)
    
    for step_target, models in step_mapping.items():
        print(f"\n  ğŸ“‚ {step_target.upper()}:")
        for model in models:
            print(f"    â€¢ {model.name} ({model.size_gb:.3f}GB)")
    
    if input(f"\n{total_size:.2f}GB ê²€ì¦ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() != 'y':
        print("ë‹¤ìš´ë¡œë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    results = await downloader.download_models_parallel(all_models)
    downloader.create_model_config()
    downloader.print_download_summary()
    
    return results

def list_verified_models():
    """ê²€ì¦ëœ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
    print("\nğŸ¤– ê²€ì¦ëœ Stepë³„ AI ëª¨ë¸ ëª©ë¡")
    print("="*80)
    
    # Stepë³„ë¡œ ì •ë¦¬
    step_mapping = {}
    for model_id, model_info in VERIFIED_AI_MODELS.items():
        step_target = model_info.step_target
        if step_target not in step_mapping:
            step_mapping[step_target] = []
        step_mapping[step_target].append((model_id, model_info))
    
    # Stepë³„ ì¶œë ¥
    for step_target, models in step_mapping.items():
        print(f"\nğŸ“‚ {step_target.upper()}:")
        
        for model_id, model_info in sorted(models, key=lambda x: x[1].priority, reverse=True):
            memory_ok = "âœ…" if model_info.required_memory_gb <= SYSTEM_INFO['available_memory_gb'] else "âŒ"
            download_method = "ğŸŒ ì§ì ‘" if model_info.download_url else "ğŸ¤— HF Hub"
            
            print(f"  {memory_ok} {model_id} ({download_method})")
            print(f"      ì´ë¦„: {model_info.name}")
            print(f"      í¬ê¸°: {model_info.size_gb:.3f}GB")
            print(f"      í•„ìš” ë©”ëª¨ë¦¬: {model_info.required_memory_gb}GB")
            print(f"      ì„¤ëª…: {model_info.description}")
            print(f"      ìš°ì„ ìˆœìœ„: {model_info.priority}/10")
            print(f"      ëŒ€ìƒ ë””ë ‰í† ë¦¬: {model_info.local_dir}")
            print()
    
    total_size = sum(model.size_gb for model in VERIFIED_AI_MODELS.values())
    compatible_models = sum(1 for model in VERIFIED_AI_MODELS.values() 
                           if model.required_memory_gb <= SYSTEM_INFO['available_memory_gb'])
    
    print(f"ğŸ“Š í†µê³„:")
    print(f"  - ì „ì²´ ê²€ì¦ ëª¨ë¸ ìˆ˜: {len(VERIFIED_AI_MODELS)}ê°œ")
    print(f"  - í˜¸í™˜ ê°€ëŠ¥ ëª¨ë¸: {compatible_models}ê°œ")
    print(f"  - ì „ì²´ í¬ê¸°: {total_size:.2f}GB")
    print(f"  - ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {SYSTEM_INFO['available_memory_gb']}GB")
    print(f"  - Stepë³„ ë°°ì¹˜: ìë™ ë””ë ‰í† ë¦¬ êµ¬ì„±")

# ==============================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)
# ==============================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”¥ MyCloset AI - ê²€ì¦ëœ Stepë³„ AI ëª¨ë¸ ë‹¤ìš´ë¡œë” v4.0")
    print("="*65)
    print("âœ… ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ë“¤ë§Œ í¬í•¨ (404/401 ì˜¤ë¥˜ í•´ê²°)")
    print("ğŸ¯ Stepë³„ ìë™ ë””ë ‰í† ë¦¬ ë°°ì¹˜")
    print("ğŸŒ ì§ì ‘ ë‹¤ìš´ë¡œë“œ + Hugging Face Hub í•˜ì´ë¸Œë¦¬ë“œ")
    print("="*65)
    print(f"ğŸ M3 Max: {SYSTEM_INFO['is_m3_max']}")
    print(f"ğŸ’¾ ì‚¬ìš©ê°€ëŠ¥ ë©”ëª¨ë¦¬: {SYSTEM_INFO['available_memory_gb']}GB")
    print(f"âš¡ MPS ê°€ì†: {SYSTEM_INFO['mps_available']}")
    print(f"ğŸ Conda í™˜ê²½: {SYSTEM_INFO['conda_env']}")
    print("="*65)
    
    while True:
        print("\nğŸ¯ ê²€ì¦ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ğŸš€ í•„ìˆ˜ ê²€ì¦ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ (ë¹ ë¥¸ ì„¤ì¹˜, ~2GB)")
        print("2. ğŸŒŸ ì¶”ì²œ ê²€ì¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ê· í˜•ìˆëŠ” ì„¤ì¹˜, ~8GB)")  
        print("3. ğŸ”¥ ëª¨ë“  ê²€ì¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì™„ì „ ì„¤ì¹˜, ~15GB)")
        print("4. ğŸ“‹ ê²€ì¦ëœ Stepë³„ ëª¨ë¸ ëª©ë¡ ë³´ê¸°")
        print("5. âŒ ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ (1-5): ").strip()
        
        try:
            if choice == '1':
                asyncio.run(download_essential_verified_models())
                break
            elif choice == '2':
                asyncio.run(download_recommended_verified_models())
                break
            elif choice == '3':
                asyncio.run(download_all_verified_models())
                break
            elif choice == '4':
                list_verified_models()
            elif choice == '5':
                print("ğŸ‘‹ ê²€ì¦ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            else:
                print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1-5 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
                
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()