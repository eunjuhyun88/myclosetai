#!/usr/bin/env python3
"""
ğŸ¤– MyCloset AI - ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
M3 Max 128GB ìµœì í™” ë²„ì „

ì‚¬ìš©ë²•:
1. python download_models.py --all          # ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
2. python download_models.py --essential    # í•„ìˆ˜ ëª¨ë¸ë§Œ
3. python download_models.py --model ootd   # íŠ¹ì • ëª¨ë¸ë§Œ
"""

import os
import sys
import json
import time
import shutil
import requests
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import hashlib
import zipfile
import tarfile
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor
import argparse

# Hugging Face Hub
try:
    from huggingface_hub import hf_hub_download, snapshot_download, login
    from huggingface_hub.utils import HfHubHTTPError
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("âš ï¸ huggingface_hubê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. pip install huggingface_hub")

# Git LFS
def check_git_lfs():
    """Git LFS ì„¤ì¹˜ í™•ì¸"""
    try:
        subprocess.run(["git", "lfs", "version"], capture_output=True, check=True)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False

class ModelDownloader:
    """AI ëª¨ë¸ ë‹¤ìš´ë¡œë”"""
    
    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.models_dir = base_dir / "ai_models"
        self.models_dir.mkdir(exist_ok=True, parents=True)
        
        # í•„ìˆ˜ ëª¨ë¸ ì •ì˜ (ìš°ì„ ìˆœìœ„ ìˆœ)
        self.models_catalog = {
            # ğŸ”¥ í•„ìˆ˜ ëª¨ë¸ë“¤ (ê°€ìƒ í”¼íŒ…ìš©)
            "ootdiffusion": {
                "name": "OOTDiffusion",
                "priority": 1,
                "size_gb": 8.5,
                "required": True,
                "sources": [
                    {
                        "type": "huggingface",
                        "repo_id": "levihsu/OOTDiffusion",
                        "subfolder": None,
                        "local_dir": "checkpoints/ootdiffusion"
                    },
                    {
                        "type": "direct",
                        "url": "https://github.com/levihsu/OOTDiffusion/releases/download/v1.0/ootd_diffusion.zip",
                        "local_dir": "checkpoints/ootdiffusion"
                    }
                ],
                "description": "ìµœì‹  ê³ í’ˆì§ˆ ê°€ìƒ í”¼íŒ… ëª¨ë¸"
            },
            
            "stable_diffusion_inpaint": {
                "name": "Stable Diffusion Inpaint",
                "priority": 2,
                "size_gb": 4.2,
                "required": True,
                "sources": [
                    {
                        "type": "huggingface",
                        "repo_id": "runwayml/stable-diffusion-inpainting",
                        "subfolder": None,
                        "local_dir": "checkpoints/stable_diffusion_inpaint"
                    }
                ],
                "description": "ì¸í˜ì¸íŒ… ì „ìš© Stable Diffusion"
            },
            
            "human_parsing": {
                "name": "Human Parsing Model",
                "priority": 3,
                "size_gb": 0.3,
                "required": True,
                "sources": [
                    {
                        "type": "huggingface",
                        "repo_id": "mattmdjaga/segformer_b2_clothes",
                        "subfolder": None,
                        "local_dir": "checkpoints/human_parsing"
                    },
                    {
                        "type": "direct",
                        "url": "https://github.com/Engineering-Course/CIHP_PGN/releases/download/v1.0/schp_atr.pth",
                        "filename": "schp_atr.pth",
                        "local_dir": "checkpoints/human_parsing"
                    }
                ],
                "description": "ì¸ì²´ ë¶€ìœ„ ë¶„í•  ëª¨ë¸"
            },
            
            "pose_estimation": {
                "name": "OpenPose Body Model",
                "priority": 4,
                "size_gb": 0.2,
                "required": True,
                "sources": [
                    {
                        "type": "direct",
                        "url": "https://github.com/CMU-Perceptual-Computing-Lab/openpose/raw/master/models/pose/body_25/pose_iter_584000.caffemodel",
                        "filename": "body_pose_model.pth",
                        "local_dir": "checkpoints/openpose/ckpts"
                    },
                    {
                        "type": "huggingface", 
                        "repo_id": "yolox/yolox",
                        "filename": "yolox_nano.pth",
                        "local_dir": "checkpoints/pose_estimation"
                    }
                ],
                "description": "í¬ì¦ˆ ì¶”ì • ëª¨ë¸"
            },
            
            "sam_segmentation": {
                "name": "Segment Anything Model",
                "priority": 5,
                "size_gb": 2.4,
                "required": False,
                "sources": [
                    {
                        "type": "direct",
                        "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth",
                        "filename": "sam_vit_h_4b8939.pth",
                        "local_dir": "checkpoints/sam_vit_h"
                    }
                ],
                "description": "ê³ ì •ë°€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸"
            },
            
            # ğŸ”¥ ë³´ì¡° ëª¨ë¸ë“¤
            "clip_vit_base": {
                "name": "CLIP ViT-B/32",
                "priority": 6,
                "size_gb": 0.6,
                "required": False,
                "sources": [
                    {
                        "type": "huggingface",
                        "repo_id": "openai/clip-vit-base-patch32",
                        "subfolder": None,
                        "local_dir": "checkpoints/clip-vit-base-patch32"
                    }
                ],
                "description": "í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì´í•´ ëª¨ë¸"
            },
            
            "controlnet_openpose": {
                "name": "ControlNet OpenPose",
                "priority": 7,
                "size_gb": 1.4,
                "required": False,
                "sources": [
                    {
                        "type": "huggingface",
                        "repo_id": "lllyasviel/control_v11p_sd15_openpose",
                        "subfolder": None,
                        "local_dir": "checkpoints/controlnet_openpose"
                    }
                ],
                "description": "í¬ì¦ˆ ì œì–´ ëª¨ë¸"
            },
            
            "cloth_segmentation": {
                "name": "Clothing Segmentation",
                "priority": 8,
                "size_gb": 0.1,
                "required": False,
                "sources": [
                    {
                        "type": "huggingface",
                        "repo_id": "rajistics/u2net_cloth_seg",
                        "subfolder": None,
                        "local_dir": "checkpoints/cloth_segmentation"
                    }
                ],
                "description": "ì˜ë¥˜ ë¶„í•  ëª¨ë¸"
            }
        }
        
        self.total_essential_size = sum(
            model["size_gb"] for model in self.models_catalog.values() 
            if model["required"]
        )
        
        self.total_all_size = sum(
            model["size_gb"] for model in self.models_catalog.values()
        )
    
    def check_dependencies(self) -> bool:
        """ì˜ì¡´ì„± í™•ì¸"""
        print("ğŸ” ì˜ì¡´ì„± í™•ì¸ ì¤‘...")
        
        # Git LFS í™•ì¸
        if not check_git_lfs():
            print("âš ï¸ Git LFSê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            print("   ì„¤ì¹˜ ë°©ë²•: brew install git-lfs && git lfs install")
            return False
        
        # Hugging Face Hub í™•ì¸
        if not HF_AVAILABLE:
            print("âš ï¸ huggingface_hubê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            print("   ì„¤ì¹˜ ë°©ë²•: pip install huggingface_hub")
            return False
        
        # ì¶©ë¶„í•œ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
        available_space = shutil.disk_usage(self.models_dir).free / (1024**3)
        print(f"ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥í•œ ë””ìŠ¤í¬ ê³µê°„: {available_space:.1f}GB")
        
        if available_space < 15:  # 15GB ìµœì†Œ í•„ìš”
            print("âŒ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± (ìµœì†Œ 15GB í•„ìš”)")
            return False
        
        print("âœ… ëª¨ë“  ì˜ì¡´ì„± í™•ì¸ ì™„ë£Œ")
        return True
    
    def check_existing_models(self) -> Dict[str, bool]:
        """ê¸°ì¡´ ëª¨ë¸ í™•ì¸"""
        print("ğŸ“‚ ê¸°ì¡´ ëª¨ë¸ í™•ì¸ ì¤‘...")
        
        existing = {}
        for model_key, model_info in self.models_catalog.items():
            model_dir = self.models_dir / model_info["sources"][0]["local_dir"]
            
            # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ëª¨ë¸ì´ ìˆë‹¤ê³  ê°€ì •
            if model_dir.exists() and any(model_dir.iterdir()):
                existing[model_key] = True
                print(f"   âœ… {model_info['name']}: ì´ë¯¸ ì¡´ì¬")
            else:
                existing[model_key] = False
                print(f"   âŒ {model_info['name']}: ì—†ìŒ")
        
        return existing
    
    def download_from_huggingface(self, source: Dict, progress_callback=None) -> bool:
        """Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            repo_id = source["repo_id"]
            local_dir = self.models_dir / source["local_dir"]
            local_dir.mkdir(parents=True, exist_ok=True)
            
            print(f"ğŸ“¥ HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ: {repo_id}")
            
            if source.get("filename"):
                # íŠ¹ì • íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ
                downloaded_path = hf_hub_download(
                    repo_id=repo_id,
                    filename=source["filename"],
                    cache_dir=str(local_dir),
                    local_dir=str(local_dir),
                    local_dir_use_symlinks=False
                )
                print(f"   âœ… íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {downloaded_path}")
            else:
                # ì „ì²´ ë ˆí¬ì§€í† ë¦¬ ë‹¤ìš´ë¡œë“œ
                snapshot_download(
                    repo_id=repo_id,
                    cache_dir=str(local_dir),
                    local_dir=str(local_dir),
                    local_dir_use_symlinks=False,
                    ignore_patterns=["*.md", "*.txt", ".gitattributes"]
                )
                print(f"   âœ… ë ˆí¬ì§€í† ë¦¬ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_dir}")
            
            return True
            
        except HfHubHTTPError as e:
            print(f"   âŒ HuggingFace ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
        except Exception as e:
            print(f"   âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def download_from_direct_url(self, source: Dict, progress_callback=None) -> bool:
        """ì§ì ‘ URLì—ì„œ ë‹¤ìš´ë¡œë“œ"""
        try:
            url = source["url"]
            local_dir = self.models_dir / source["local_dir"]
            local_dir.mkdir(parents=True, exist_ok=True)
            
            filename = source.get("filename") or Path(urlparse(url).path).name
            local_path = local_dir / filename
            
            print(f"ğŸ“¥ ì§ì ‘ ë‹¤ìš´ë¡œë“œ: {url}")
            print(f"   ì €ì¥ ìœ„ì¹˜: {local_path}")
            
            # ë‹¤ìš´ë¡œë“œ
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            
            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            print(f"\r   ì§„í–‰ë¥ : {progress:.1f}%", end="", flush=True)
            
            print(f"\n   âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_path}")
            
            # ZIP íŒŒì¼ì¸ ê²½ìš° ì••ì¶• í•´ì œ
            if local_path.suffix.lower() == '.zip':
                print(f"   ğŸ“¦ ì••ì¶• í•´ì œ ì¤‘...")
                with zipfile.ZipFile(local_path, 'r') as zip_ref:
                    zip_ref.extractall(local_dir)
                local_path.unlink()  # ì••ì¶• íŒŒì¼ ì‚­ì œ
                print(f"   âœ… ì••ì¶• í•´ì œ ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            print(f"   âŒ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def download_model(self, model_key: str) -> bool:
        """íŠ¹ì • ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        if model_key not in self.models_catalog:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_key}")
            return False
        
        model_info = self.models_catalog[model_key]
        print(f"\nğŸ¤– {model_info['name']} ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        print(f"   í¬ê¸°: {model_info['size_gb']}GB")
        print(f"   ì„¤ëª…: {model_info['description']}")
        
        # ì—¬ëŸ¬ ì†ŒìŠ¤ ì‹œë„
        for i, source in enumerate(model_info["sources"]):
            print(f"\n   ğŸ“ ì†ŒìŠ¤ {i+1}/{len(model_info['sources'])} ì‹œë„...")
            
            success = False
            if source["type"] == "huggingface":
                success = self.download_from_huggingface(source)
            elif source["type"] == "direct":
                success = self.download_from_direct_url(source)
            
            if success:
                print(f"âœ… {model_info['name']} ë‹¤ìš´ë¡œë“œ ì„±ê³µ!")
                return True
            else:
                print(f"âš ï¸ ì†ŒìŠ¤ {i+1} ì‹¤íŒ¨, ë‹¤ìŒ ì†ŒìŠ¤ ì‹œë„...")
        
        print(f"âŒ {model_info['name']} ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        return False
    
    def download_essential_models(self) -> Dict[str, bool]:
        """í•„ìˆ˜ ëª¨ë¸ë“¤ ë‹¤ìš´ë¡œë“œ"""
        print(f"\nğŸ”¥ í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        print(f"ì˜ˆìƒ ì´ í¬ê¸°: {self.total_essential_size:.1f}GB")
        print("=" * 50)
        
        results = {}
        essential_models = [
            key for key, info in self.models_catalog.items() 
            if info["required"]
        ]
        
        for i, model_key in enumerate(essential_models, 1):
            print(f"\nğŸ“‹ ì§„í–‰ë¥ : {i}/{len(essential_models)}")
            results[model_key] = self.download_model(model_key)
            
            if results[model_key]:
                print(f"âœ… {i}/{len(essential_models)} ì™„ë£Œ")
            else:
                print(f"âŒ {i}/{len(essential_models)} ì‹¤íŒ¨")
        
        return results
    
    def download_all_models(self) -> Dict[str, bool]:
        """ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        print(f"\nğŸš€ ì „ì²´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        print(f"ì˜ˆìƒ ì´ í¬ê¸°: {self.total_all_size:.1f}GB")
        print("=" * 50)
        
        results = {}
        sorted_models = sorted(
            self.models_catalog.items(),
            key=lambda x: x[1]["priority"]
        )
        
        for i, (model_key, model_info) in enumerate(sorted_models, 1):
            print(f"\nğŸ“‹ ì§„í–‰ë¥ : {i}/{len(sorted_models)}")
            results[model_key] = self.download_model(model_key)
            
            if results[model_key]:
                print(f"âœ… {i}/{len(sorted_models)} ì™„ë£Œ")
            else:
                print(f"âŒ {i}/{len(sorted_models)} ì‹¤íŒ¨")
        
        return results
    
    def generate_model_paths(self):
        """ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ë“¤ì˜ ê²½ë¡œ íŒŒì¼ ìƒì„±"""
        print("\nğŸ“ ëª¨ë¸ ê²½ë¡œ íŒŒì¼ ìƒì„± ì¤‘...")
        
        paths_file = self.base_dir / "app" / "core" / "downloaded_model_paths.py"
        paths_file.parent.mkdir(parents=True, exist_ok=True)
        
        content = '''"""
ìë™ ìƒì„±ëœ ëª¨ë¸ ê²½ë¡œ íŒŒì¼
ë‹¤ìš´ë¡œë“œëœ AI ëª¨ë¸ë“¤ì˜ ê²½ë¡œ ì •ì˜
"""

from pathlib import Path

# ê¸°ë³¸ ê²½ë¡œ
MODELS_DIR = Path(__file__).parent.parent.parent / "ai_models"

# ëª¨ë¸ ê²½ë¡œë“¤
'''
        
        for model_key, model_info in self.models_catalog.items():
            local_dir = model_info["sources"][0]["local_dir"]
            var_name = model_key.upper() + "_PATH"
            content += f'{var_name} = MODELS_DIR / "{local_dir}"\n'
        
        content += '''
# ëª¨ë¸ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬
MODEL_PATHS = {
'''
        
        for model_key in self.models_catalog.keys():
            var_name = model_key.upper() + "_PATH"
            content += f'    "{model_key}": {var_name},\n'
        
        content += '''}

def get_model_path(model_name: str) -> Path:
    """ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    return MODEL_PATHS.get(model_name)

def is_model_available(model_name: str) -> bool:
    """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    path = get_model_path(model_name)
    return path and path.exists() and any(path.iterdir())
'''
        
        with open(paths_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"âœ… ëª¨ë¸ ê²½ë¡œ íŒŒì¼ ìƒì„±: {paths_file}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="MyCloset AI ëª¨ë¸ ë‹¤ìš´ë¡œë”")
    parser.add_argument("--all", action="store_true", help="ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--essential", action="store_true", help="í•„ìˆ˜ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--model", type=str, help="íŠ¹ì • ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--check", action="store_true", help="ê¸°ì¡´ ëª¨ë¸ í™•ì¸ë§Œ")
    parser.add_argument("--path", type=str, default=".", help="í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    # í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
    project_root = Path(args.path).resolve()
    backend_dir = project_root / "backend"
    
    if not backend_dir.exists():
        print(f"âŒ ë°±ì—”ë“œ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {backend_dir}")
        sys.exit(1)
    
    print("ğŸ¤– MyCloset AI ëª¨ë¸ ë‹¤ìš´ë¡œë”")
    print("=" * 50)
    print(f"ğŸ“ í”„ë¡œì íŠ¸ ê²½ë¡œ: {project_root}")
    print(f"ğŸ“ ë°±ì—”ë“œ ê²½ë¡œ: {backend_dir}")
    
    # ë‹¤ìš´ë¡œë” ì´ˆê¸°í™”
    downloader = ModelDownloader(backend_dir)
    
    # ì˜ì¡´ì„± í™•ì¸
    if not downloader.check_dependencies():
        print("âŒ ì˜ì¡´ì„± í™•ì¸ ì‹¤íŒ¨")
        sys.exit(1)
    
    # ê¸°ì¡´ ëª¨ë¸ í™•ì¸
    existing_models = downloader.check_existing_models()
    
    if args.check:
        print("\nğŸ“Š ëª¨ë¸ ìƒíƒœ ìš”ì•½:")
        for model_key, exists in existing_models.items():
            status = "âœ… ì¡´ì¬" if exists else "âŒ ì—†ìŒ"
            model_name = downloader.models_catalog[model_key]["name"]
            print(f"   {status} {model_name}")
        return
    
    # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
    start_time = time.time()
    results = {}
    
    if args.model:
        # íŠ¹ì • ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        if args.model not in downloader.models_catalog:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {args.model}")
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(downloader.models_catalog.keys())}")
            sys.exit(1)
        
        results[args.model] = downloader.download_model(args.model)
        
    elif args.essential:
        # í•„ìˆ˜ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ
        results = downloader.download_essential_models()
        
    elif args.all:
        # ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        results = downloader.download_all_models()
        
    else:
        # ê¸°ë³¸: í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        print("ğŸ“‹ ì˜µì…˜ì´ ì§€ì •ë˜ì§€ ì•Šì•„ í•„ìˆ˜ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
        print("ì „ì²´ ì˜µì…˜: --all, --essential, --model <ì´ë¦„>")
        results = downloader.download_essential_models()
    
    # ê²°ê³¼ ìš”ì•½
    duration = time.time() - start_time
    success_count = sum(1 for success in results.values() if success)
    total_count = len(results)
    
    print(f"\nğŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    print("=" * 50)
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
    print(f"ğŸ“Š ì„±ê³µë¥ : {success_count}/{total_count} ({success_count/total_count*100:.1f}%)")
    
    if success_count > 0:
        # ê²½ë¡œ íŒŒì¼ ìƒì„±
        downloader.generate_model_paths()
        
        print(f"\nâœ… ì„±ê³µí•œ ëª¨ë¸:")
        for model_key, success in results.items():
            if success:
                model_name = downloader.models_catalog[model_key]["name"]
                print(f"   âœ… {model_name}")
    
    if success_count < total_count:
        print(f"\nâŒ ì‹¤íŒ¨í•œ ëª¨ë¸:")
        for model_key, success in results.items():
            if not success:
                model_name = downloader.models_catalog[model_key]["name"]
                print(f"   âŒ {model_name}")
    
    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. ë°±ì—”ë“œ ì„œë²„ ì¬ì‹œì‘")
    print(f"   2. http://localhost:8000/api/models/status í™•ì¸")
    print(f"   3. í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ê°€ìƒ í”¼íŒ… í…ŒìŠ¤íŠ¸")

if __name__ == "__main__":
    main()