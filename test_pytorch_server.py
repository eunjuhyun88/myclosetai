#!/usr/bin/env python3
"""
PyTorch í…ŒìŠ¤íŠ¸ ì„œë²„
ì„¤ì¹˜ í™•ì¸ ë° ê°„ë‹¨í•œ AI ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    import torch
    import torchvision.transforms as transforms
    from PIL import Image
    import numpy as np
    TORCH_AVAILABLE = True
    
    # ë””ë°”ì´ìŠ¤ ì„ íƒ
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        DEVICE = "mps"
        DEVICE_INFO = "Apple Silicon (Metal)"
    elif torch.cuda.is_available():
        DEVICE = "cuda"
        DEVICE_INFO = f"NVIDIA GPU ({torch.cuda.get_device_name()})"
    else:
        DEVICE = "cpu"
        DEVICE_INFO = "CPU"
        
    print(f"âœ… PyTorch ë¡œë“œ ì„±ê³µ: {torch.__version__}")
    print(f"ğŸ¯ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {DEVICE} ({DEVICE_INFO})")
    
except ImportError as e:
    print(f"âŒ PyTorch ë¡œë“œ ì‹¤íŒ¨: {e}")
    TORCH_AVAILABLE = False
    DEVICE = "none"
    DEVICE_INFO = "PyTorch ì—†ìŒ"

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from datetime import datetime
import time

app = FastAPI(
    title="PyTorch í…ŒìŠ¤íŠ¸ ì„œë²„",
    description="PyTorch ì„¤ì¹˜ í™•ì¸ ë° AI ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {
        "message": "PyTorch í…ŒìŠ¤íŠ¸ ì„œë²„ ğŸ”¥",
        "torch_available": TORCH_AVAILABLE,
        "torch_version": torch.__version__ if TORCH_AVAILABLE else "ì—†ìŒ",
        "device": DEVICE,
        "device_info": DEVICE_INFO,
        "status": "ready" if TORCH_AVAILABLE else "pytorch_missing",
        "test_endpoints": {
            "tensor_test": "/test/tensor",
            "performance": "/test/performance",
            "memory": "/test/memory"
        }
    }

@app.get("/test/tensor")
async def test_tensor_operations():
    """ê¸°ë³¸ í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸"""
    if not TORCH_AVAILABLE:
        raise HTTPException(503, "PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        start_time = time.time()
        
        # ê¸°ë³¸ í…ì„œ ìƒì„±
        x = torch.randn(100, 100, device=DEVICE)
        y = torch.randn(100, 100, device=DEVICE)
        
        # í–‰ë ¬ ê³±ì…ˆ
        z = torch.mm(x, y)
        
        # í†µê³„ ê³„ì‚°
        mean_val = torch.mean(z).item()
        std_val = torch.std(z).item()
        
        processing_time = time.time() - start_time
        
        return {
            "success": True,
            "device": DEVICE,
            "tensor_shape": list(z.shape),
            "statistics": {
                "mean": round(mean_val, 4),
                "std": round(std_val, 4)
            },
            "processing_time_ms": round(processing_time * 1000, 2),
            "message": "í…ì„œ ì—°ì‚°ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!"
        }
        
    except Exception as e:
        raise HTTPException(500, f"í…ì„œ ì—°ì‚° ì‹¤íŒ¨: {str(e)}")

@app.get("/test/performance")
async def test_performance():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    if not TORCH_AVAILABLE:
        raise HTTPException(503, "PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        results = {}
        
        # CPU vs í˜„ì¬ ë””ë°”ì´ìŠ¤ ì„±ëŠ¥ ë¹„êµ
        for device in ["cpu", DEVICE] if DEVICE != "cpu" else ["cpu"]:
            start_time = time.time()
            
            # 1000x1000 í–‰ë ¬ ê³±ì…ˆ 10íšŒ
            for _ in range(10):
                a = torch.randn(1000, 1000, device=device)
                b = torch.randn(1000, 1000, device=device)
                c = torch.mm(a, b)
            
            total_time = time.time() - start_time
            results[device] = {
                "total_time_seconds": round(total_time, 3),
                "avg_time_per_operation_ms": round(total_time * 100, 2),  # 10íšŒ ë°˜ë³µì´ë¯€ë¡œ *100
                "operations_per_second": round(10 / total_time, 1)
            }
        
        return {
            "success": True,
            "torch_version": torch.__version__,
            "benchmark_results": results,
            "recommendation": f"{DEVICE} ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤" if DEVICE != "cpu" else "GPUê°€ ìˆë‹¤ë©´ ë” ë¹ ë¥¸ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤"
        }
        
    except Exception as e:
        raise HTTPException(500, f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

@app.get("/test/memory")
async def test_memory():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
    if not TORCH_AVAILABLE:
        raise HTTPException(503, "PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        results = {
            "device": DEVICE,
            "torch_version": torch.__version__
        }
        
        if DEVICE == "cuda":
            # CUDA ë©”ëª¨ë¦¬ ì •ë³´
            results["cuda_memory"] = {
                "total_mb": torch.cuda.get_device_properties(0).total_memory // 1024**2,
                "allocated_mb": torch.cuda.memory_allocated() // 1024**2,
                "cached_mb": torch.cuda.memory_reserved() // 1024**2
            }
        elif DEVICE == "mps":
            # MPSëŠ” í†µí•© ë©”ëª¨ë¦¬ ì‚¬ìš©
            results["mps_info"] = {
                "unified_memory": True,
                "note": "Apple Siliconì€ í†µí•© ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤"
            }
        
        # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ í• ë‹¹ í…ŒìŠ¤íŠ¸
        test_tensor = torch.randn(1000, 1000, device=DEVICE)
        tensor_size_mb = test_tensor.element_size() * test_tensor.nelement() / 1024**2
        
        results["test_allocation"] = {
            "tensor_shape": list(test_tensor.shape),
            "tensor_size_mb": round(tensor_size_mb, 2),
            "allocation_successful": True
        }
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del test_tensor
        if DEVICE == "cuda":
            torch.cuda.empty_cache()
        elif DEVICE == "mps":
            torch.mps.empty_cache()
            
        return {
            "success": True,
            "memory_info": results
        }
        
    except Exception as e:
        raise HTTPException(500, f"ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    print("ğŸ”¥ PyTorch í…ŒìŠ¤íŠ¸ ì„œë²„ ì‹œì‘...")
    print(f"âœ… PyTorch: {'ì‚¬ìš© ê°€ëŠ¥' if TORCH_AVAILABLE else 'ì‚¬ìš© ë¶ˆê°€'}")
    print(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {DEVICE}")
    print("")
    print("ğŸ“± ì ‘ì† ì£¼ì†Œ: http://localhost:8001")
    print("ğŸ“š API ë¬¸ì„œ: http://localhost:8001/docs")
    print("ğŸ§ª í…ì„œ í…ŒìŠ¤íŠ¸: http://localhost:8001/test/tensor")
    print("âš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸: http://localhost:8001/test/performance")
    print("ğŸ’¾ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸: http://localhost:8001/test/memory")
    print("")
    
    uvicorn.run(app, host="0.0.0.0", port=8001, reload=False)
