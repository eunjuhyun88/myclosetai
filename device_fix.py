#!/usr/bin/env python3
"""
ğŸ”§ GeometricMatchingStep ë””ë°”ì´ìŠ¤ ë¬¸ì œ í•´ê²°
ë¬¸ì œ: cpu ì €ì¥ ëª¨ë¸ì„ mpsë¡œ ë¡œë”©í•  ë•Œ ë””ë°”ì´ìŠ¤ ë¶ˆì¼ì¹˜
í•´ê²°: ì•ˆì „í•œ ë””ë°”ì´ìŠ¤ ë§¤í•‘ + ëª¨ë¸ ì´ë™
"""

import os
import sys
from pathlib import Path
import torch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
project_root = Path("/Users/gimdudeul/MVP/mycloset-ai")
backend_root = project_root / "backend"
sys.path.insert(0, str(backend_root))

# ì˜¬ë°”ë¥¸ ê²½ë¡œ ì„¤ì •
os.environ["AI_MODELS_ROOT"] = str(backend_root / "ai_models")
os.environ["PYTORCH_JIT_DISABLE"] = "1"

def safe_model_loading_test():
    """ì•ˆì „í•œ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("ğŸ”§ ë””ë°”ì´ìŠ¤ ì•ˆì „ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    ai_models_root = Path(os.environ["AI_MODELS_ROOT"])
    gmm_path = ai_models_root / "step_04_geometric_matching" / "gmm_final.pth"
    
    print(f"ğŸ“„ GMM íŒŒì¼: {gmm_path}")
    print(f"ğŸ“Š íŒŒì¼ ì¡´ì¬: {gmm_path.exists()}")
    
    if not gmm_path.exists():
        print("âŒ GMM íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return False
    
    # PyTorch ë””ë°”ì´ìŠ¤ í™•ì¸
    print(f"âœ… PyTorch ë²„ì „: {torch.__version__}")
    has_mps = torch.backends.mps.is_available()
    has_cuda = torch.cuda.is_available()
    
    print(f"ğŸ MPS ì‚¬ìš© ê°€ëŠ¥: {has_mps}")
    print(f"ğŸ”¥ CUDA ì‚¬ìš© ê°€ëŠ¥: {has_cuda}")
    
    # 1ë‹¨ê³„: CPUë¡œ ì•ˆì „í•˜ê²Œ ë¡œë”©
    print("\n1ï¸âƒ£ CPUë¡œ ì•ˆì „í•˜ê²Œ ë¡œë”©...")
    try:
        model_data = torch.load(gmm_path, map_location='cpu')
        print(f"âœ… CPU ë¡œë”© ì„±ê³µ: {type(model_data)}")
        
        # ëª¨ë¸ ë°ì´í„° íƒ€ì… í™•ì¸
        if isinstance(model_data, dict):
            print(f"ğŸ“‹ ë”•ì…”ë„ˆë¦¬ í‚¤ë“¤: {list(model_data.keys())[:5]}")
            if 'state_dict' in model_data:
                print(f"ğŸ”§ state_dict í¬ê¸°: {len(model_data['state_dict'])}")
        elif hasattr(model_data, 'state_dict'):
            print(f"ğŸ”§ ëª¨ë¸ state_dict í¬ê¸°: {len(model_data.state_dict())}")
        
    except Exception as e:
        print(f"âŒ CPU ë¡œë”© ì‹¤íŒ¨: {e}")
        return False
    
    # 2ë‹¨ê³„: MPSë¡œ ì•ˆì „í•˜ê²Œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)
    if has_mps:
        print("\n2ï¸âƒ£ MPSë¡œ ì•ˆì „í•˜ê²Œ ì´ë™...")
        try:
            # torch.jit ëª¨ë¸ì¸ì§€ í™•ì¸
            if hasattr(model_data, '_c'):
                print("âš ï¸ torch.jit ëª¨ë¸ ê°ì§€ë¨ - CPU ëª¨ë“œ ìœ ì§€")
                target_device = 'cpu'
            else:
                print("âœ… ì¼ë°˜ PyTorch ëª¨ë¸ - MPS ì´ë™ ì‹œë„")
                target_device = 'mps'
                
                # ì‹¤ì œ í…ì„œê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì´ë™
                if isinstance(model_data, dict) and 'state_dict' in model_data:
                    # state_dictì˜ ì²« ë²ˆì§¸ í…ì„œë§Œ í…ŒìŠ¤íŠ¸
                    first_key = list(model_data['state_dict'].keys())[0]
                    first_tensor = model_data['state_dict'][first_key]
                    test_tensor = first_tensor.to('mps')
                    print(f"âœ… í…ì„œ MPS ì´ë™ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {test_tensor.device}")
                    
        except Exception as e:
            print(f"âš ï¸ MPS ì´ë™ ì‹¤íŒ¨, CPU ëª¨ë“œ ì‚¬ìš©: {e}")
            target_device = 'cpu'
    else:
        target_device = 'cpu'
        print("\n2ï¸âƒ£ MPS ë¯¸ì§€ì› - CPU ëª¨ë“œ ì‚¬ìš©")
    
    print(f"\nğŸ¯ ìµœì¢… ë””ë°”ì´ìŠ¤: {target_device}")
    
    # 3ë‹¨ê³„: GeometricMatchingStep í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£ GeometricMatchingStep ë””ë°”ì´ìŠ¤ ì•ˆì „ í…ŒìŠ¤íŠ¸...")
    try:
        from app.ai_pipeline.steps.step_04_geometric_matching import GeometricMatchingStep
        print("âœ… GeometricMatchingStep import ì„±ê³µ")
        
        # ì•ˆì „í•œ ë””ë°”ì´ìŠ¤ë¡œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        step = GeometricMatchingStep(
            step_id=4,
            device=target_device,  # CPU ë˜ëŠ” ì•ˆì „í•˜ê²Œ í™•ì¸ëœ MPS
            config={
                "ai_models_root": str(ai_models_root),
                "force_cpu_mode": target_device == 'cpu',
                "enable_jit_compile": False
            }
        )
        print(f"âœ… GeometricMatchingStep ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ (ë””ë°”ì´ìŠ¤: {target_device})")
        
        # ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        try:
            if hasattr(step, 'initialize'):
                if asyncio.iscoroutinefunction(step.initialize):
                    import asyncio
                    success = asyncio.run(step.initialize())
                else:
                    success = step.initialize()
                
                if success:
                    print("âœ… Step ì´ˆê¸°í™” ì„±ê³µ")
                    
                    # ê°„ë‹¨í•œ ëª¨ë¸ ì ‘ê·¼ í…ŒìŠ¤íŠ¸
                    if hasattr(step, 'gmm_model') and step.gmm_model is not None:
                        print(f"âœ… GMM ëª¨ë¸ ë¡œë“œë¨: {type(step.gmm_model)}")
                        print(f"ğŸ–¥ï¸ GMM ëª¨ë¸ ë””ë°”ì´ìŠ¤: {next(step.gmm_model.parameters()).device if hasattr(step.gmm_model, 'parameters') else 'unknown'}")
                    
                    return True
                else:
                    print("âŒ Step ì´ˆê¸°í™” ì‹¤íŒ¨")
                    return False
            else:
                print("âš ï¸ initialize ë©”ì„œë“œ ì—†ìŒ - ê¸°ë³¸ ì„±ê³µìœ¼ë¡œ ê°„ì£¼")
                return True
                
        except Exception as e:
            print(f"âŒ Step ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ GeometricMatchingStep í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_device_safe_config():
    """ë””ë°”ì´ìŠ¤ ì•ˆì „ ì„¤ì • íŒŒì¼ ìƒì„±"""
    config_content = '''#!/usr/bin/env python3
"""
ë””ë°”ì´ìŠ¤ ì•ˆì „ ì„¤ì • (ìë™ ìƒì„±)
"""
import torch

def get_safe_device():
    """ì•ˆì „í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    if torch.backends.mps.is_available():
        try:
            # MPS ê°„ë‹¨ í…ŒìŠ¤íŠ¸
            test_tensor = torch.randn(1, 1).to('mps')
            return 'mps'
        except Exception:
            return 'cpu'
    elif torch.cuda.is_available():
        return 'cuda'
    else:
        return 'cpu'

def safe_model_load(model_path, target_device=None):
    """ë””ë°”ì´ìŠ¤ ì•ˆì „ ëª¨ë¸ ë¡œë”©"""
    if target_device is None:
        target_device = get_safe_device()
    
    # 1ë‹¨ê³„: CPUë¡œ ë¡œë”©
    model_data = torch.load(model_path, map_location='cpu')
    
    # 2ë‹¨ê³„: íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (torch.jit ì•„ë‹Œ ê²½ìš°ë§Œ)
    if target_device != 'cpu' and not hasattr(model_data, '_c'):
        if isinstance(model_data, dict) and 'state_dict' in model_data:
            new_state_dict = {}
            for key, tensor in model_data['state_dict'].items():
                new_state_dict[key] = tensor.to(target_device)
            model_data['state_dict'] = new_state_dict
        elif hasattr(model_data, 'to'):
            model_data = model_data.to(target_device)
    
    return model_data, target_device

if __name__ == "__main__":
    print(f"ğŸ¯ ê¶Œì¥ ë””ë°”ì´ìŠ¤: {get_safe_device()}")
'''
    
    config_file = Path("device_safe_config.py")
    config_file.write_text(config_content)
    print(f"âœ… ë””ë°”ì´ìŠ¤ ì•ˆì „ ì„¤ì • ìƒì„±: {config_file}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”§ GeometricMatchingStep ë””ë°”ì´ìŠ¤ ë¬¸ì œ í•´ê²°")
    print("=" * 60)
    
    # 1. ì•ˆì „í•œ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    success = safe_model_loading_test()
    
    # 2. ë””ë°”ì´ìŠ¤ ì•ˆì „ ì„¤ì • ìƒì„±
    create_device_safe_config()
    
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ ë””ë°”ì´ìŠ¤ ë¬¸ì œ í•´ê²° ì„±ê³µ!")
        print("âœ¨ GeometricMatchingStepì´ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ì‘ë™ ì¤‘")
        
        print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. conda activate mycloset-ai-clean")
        print("2. cd backend")
        print("3. export PYTORCH_JIT_DISABLE=1") 
        print("4. python gmm_step_test.py  # ì›ë˜ í…ŒìŠ¤íŠ¸ ì¬ì‹¤í–‰")
    else:
        print("âš ï¸ ì¶”ê°€ ë””ë²„ê¹…ì´ í•„ìš”í•©ë‹ˆë‹¤")
        print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        print("- CPU ëª¨ë“œë¡œ ê°•ì œ ì‹¤í–‰")
        print("- torch.jit ì™„ì „ ë¹„í™œì„±í™”")
        print("- ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ ê³ ë ¤")

if __name__ == "__main__":
    main()