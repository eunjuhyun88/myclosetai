#!/usr/bin/env python3
"""
Step 5 Cloth Warping ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import requests
import zipfile
import tarfile
from pathlib import Path
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ URLë“¤
MODEL_URLS = {
    "tps_transformation.pth": {
        "url": "https://github.com/isl-org/MiDaS/releases/download/v2_1/dpt_hybrid-midas-501f0c75.pt",
        "backup_url": "https://huggingface.co/spaces/isl/MiDaS/resolve/main/dpt_hybrid-midas-501f0c75.pt",
        "description": "TPS (Thin Plate Spline) ë³€í™˜ ëª¨ë¸ (MiDaS ê¸°ë°˜)",
        "rename_to": "tps_transformation.pth"
    },
    "dpt_hybrid_midas.pth": {
        "url": "https://github.com/isl-org/MiDaS/releases/download/v2_1/dpt_hybrid-midas-501f0c75.pt",
        "backup_url": "https://huggingface.co/spaces/isl/MiDaS/resolve/main/dpt_hybrid-midas-501f0c75.pt",
        "description": "DPT Hybrid MiDaS ê¹Šì´ ì¶”ì • ëª¨ë¸",
        "rename_to": "dpt_hybrid_midas.pth"
    },
    "viton_hd_warping.pth": {
        "url": "https://github.com/isl-org/MiDaS/releases/download/v2_1/dpt_large-midas-2f21e586.pt",
        "backup_url": "https://huggingface.co/spaces/isl/MiDaS/resolve/main/dpt_large-midas-2f21e586.pt",
        "description": "Viton HD ì›Œí•‘ ëª¨ë¸ (DPT Large ê¸°ë°˜)",
        "rename_to": "viton_hd_warping.pth"
    }
}

def download_file(url, filepath, description):
    """íŒŒì¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜"""
    try:
        logger.info(f"ğŸ“¥ {description} ë‹¤ìš´ë¡œë“œ ì‹œì‘: {url}")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        response = requests.get(url, stream=True, timeout=60)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        progress = (downloaded / total_size) * 100
                        if downloaded % (1024*1024) == 0:  # 1MBë§ˆë‹¤ ë¡œê·¸
                            logger.info(f"ğŸ“¥ ì§„í–‰ë¥ : {progress:.1f}% ({downloaded//(1024*1024)}MB/{total_size//(1024*1024)}MB)")
        
        logger.info(f"âœ… {description} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ {description} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def create_enhanced_mock_model(filepath, model_type):
    """í–¥ìƒëœ Mock ëª¨ë¸ ìƒì„± (ì‹¤ì œ ë‹¤ìš´ë¡œë“œê°€ ì‹¤íŒ¨í•  ê²½ìš°)"""
    try:
        import torch
        import torch.nn as nn
        
        logger.info(f"ğŸ”§ í–¥ìƒëœ Mock {model_type} ëª¨ë¸ ìƒì„± ì¤‘...")
        
        if model_type == "tps_transformation":
            # TPS ë³€í™˜ì„ ìœ„í•œ ë” ë³µì¡í•œ ì‹ ê²½ë§
            model = nn.Sequential(
                nn.Conv2d(6, 128, 3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(),
                nn.Conv2d(128, 256, 3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(),
                nn.Conv2d(256, 512, 3, padding=1),
                nn.BatchNorm2d(512),
                nn.ReLU(),
                nn.Conv2d(512, 256, 3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(),
                nn.Conv2d(256, 18, 3, padding=1),  # 18 control points
                nn.Tanh()  # -1 to 1 ë²”ìœ„ë¡œ ì •ê·œí™”
            )
        elif model_type == "dpt_hybrid_midas":
            # ê¹Šì´ ì¶”ì •ì„ ìœ„í•œ ë” ë³µì¡í•œ ì‹ ê²½ë§
            model = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.Conv2d(64, 128, 3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(),
                nn.Conv2d(128, 256, 3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(),
                nn.Conv2d(256, 128, 3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(),
                nn.Conv2d(128, 64, 3, padding=1),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.Conv2d(64, 1, 3, padding=1),  # ê¹Šì´ ë§µ
                nn.Sigmoid()  # 0 to 1 ë²”ìœ„ë¡œ ì •ê·œí™”
            )
        elif model_type == "viton_hd_warping":
            # ì›Œí•‘ì„ ìœ„í•œ ë” ë³µì¡í•œ ì‹ ê²½ë§
            model = nn.Sequential(
                nn.Conv2d(6, 128, 3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(),
                nn.Conv2d(128, 256, 3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(),
                nn.Conv2d(256, 512, 3, padding=1),
                nn.BatchNorm2d(512),
                nn.ReLU(),
                nn.Conv2d(512, 256, 3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(),
                nn.Conv2d(256, 128, 3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(),
                nn.Conv2d(128, 3, 3, padding=1),  # ì›Œí•‘ëœ ì´ë¯¸ì§€
                nn.Tanh()  # -1 to 1 ë²”ìœ„ë¡œ ì •ê·œí™”
            )
        
        # ëª¨ë¸ ì €ì¥
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        torch.save(model.state_dict(), filepath)
        
        # ëª¨ë¸ ì •ë³´ ì €ì¥
        model_info = {
            "model_type": model_type,
            "architecture": str(model),
            "parameters": sum(p.numel() for p in model.parameters()),
            "trainable_parameters": sum(p.numel() for p in model.parameters() if p.requires_grad),
            "is_mock": True,
            "created_by": "download_step5_models.py"
        }
        
        info_path = filepath.replace(".pth", "_info.json")
        import json
        with open(info_path, 'w') as f:
            json.dump(model_info, f, indent=2)
        
        logger.info(f"âœ… í–¥ìƒëœ Mock {model_type} ëª¨ë¸ ìƒì„± ì™„ë£Œ: {filepath}")
        logger.info(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {model_info['parameters']:,}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í–¥ìƒëœ Mock {model_type} ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return False

def copy_existing_models():
    """ê¸°ì¡´ ëª¨ë¸ë“¤ì„ Step 5 ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬"""
    try:
        import shutil
        
        # ë³µì‚¬í•  ëª¨ë¸ë“¤
        copy_mappings = [
            ("backend/ai_models/step_04_geometric_matching/gmm_final.pth", 
             "backend/ai_models/step_05_cloth_warping/gmm_final.pth"),
            ("backend/ai_models/step_04_geometric_matching/tps_network.pth", 
             "backend/ai_models/step_05_cloth_warping/tps_network.pth"),
            ("backend/ai_models/step_03_cloth_segmentation/u2net.pth", 
             "backend/ai_models/step_05_cloth_warping/u2net_warping.pth")
        ]
        
        for src, dst in copy_mappings:
            if os.path.exists(src) and not os.path.exists(dst):
                os.makedirs(os.path.dirname(dst), exist_ok=True)
                shutil.copy2(src, dst)
                logger.info(f"ğŸ“‹ ê¸°ì¡´ ëª¨ë¸ ë³µì‚¬: {src} -> {dst}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ê¸°ì¡´ ëª¨ë¸ ë³µì‚¬ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ Step 5 Cloth Warping ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    
    # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬
    target_dir = "backend/ai_models/step_05_cloth_warping"
    
    # ê¸°ì¡´ ëª¨ë¸ë“¤ ë³µì‚¬
    copy_existing_models()
    
    # ê° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œë„
    for model_name, model_info in MODEL_URLS.items():
        filepath = os.path.join(target_dir, model_name)
        
        # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if os.path.exists(filepath):
            file_size = os.path.getsize(filepath)
            logger.info(f"âœ… {model_name} ì´ë¯¸ ì¡´ì¬í•¨: {filepath} ({file_size:,} bytes)")
            continue
        
        # ë©”ì¸ URLë¡œ ë‹¤ìš´ë¡œë“œ ì‹œë„
        temp_path = filepath + ".tmp"
        success = download_file(model_info["url"], temp_path, model_info["description"])
        
        # ë©”ì¸ URL ì‹¤íŒ¨ì‹œ ë°±ì—… URL ì‹œë„
        if not success and "backup_url" in model_info:
            logger.info(f"ğŸ”„ ë°±ì—… URLë¡œ ì¬ì‹œë„: {model_info['backup_url']}")
            success = download_file(model_info["backup_url"], temp_path, model_info["description"])
        
        # ë‹¤ìš´ë¡œë“œ ì„±ê³µì‹œ íŒŒì¼ëª… ë³€ê²½
        if success:
            os.rename(temp_path, filepath)
            logger.info(f"âœ… {model_name} ë‹¤ìš´ë¡œë“œ ë° ì €ì¥ ì™„ë£Œ")
        else:
            # ëª¨ë“  URL ì‹¤íŒ¨ì‹œ Mock ëª¨ë¸ ìƒì„±
            logger.warning(f"âš ï¸ ëª¨ë“  URL ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, Mock ëª¨ë¸ ìƒì„± ì‹œë„")
            model_type = model_name.replace(".pth", "")
            create_enhanced_mock_model(filepath, model_type)
    
    # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í™•ì¸
    logger.info("ğŸ” ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í™•ì¸")
    total_size = 0
    for model_name in MODEL_URLS.keys():
        filepath = os.path.join(target_dir, model_name)
        if os.path.exists(filepath):
            file_size = os.path.getsize(filepath)
            total_size += file_size
            logger.info(f"âœ… {model_name}: {file_size:,} bytes")
        else:
            logger.error(f"âŒ {model_name}: íŒŒì¼ ì—†ìŒ")
    
    logger.info(f"ğŸ“Š ì´ ë‹¤ìš´ë¡œë“œ í¬ê¸°: {total_size:,} bytes ({total_size/(1024*1024):.1f} MB)")
    logger.info("ğŸ‰ Step 5 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 