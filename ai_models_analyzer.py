#!/usr/bin/env python3
"""
ğŸ” AI Models Directory Analyzer
í˜„ì¬ backend/ai_models ë””ë ‰í† ë¦¬ì˜ ì‹¤ì œ ìƒí™©ì„ ì™„ì „íˆ ë¶„ì„í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Usage:
    python ai_models_analyzer.py [--scan-only] [--detailed] [--export-json]
"""

import os
import sys
import json
import time
import hashlib
from pathlib import Path
from collections import defaultdict, Counter
from dataclasses import dataclass, asdict
from typing import Dict, List, Set, Optional, Tuple, Any
import logging

# ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ModelFileInfo:
    """ëª¨ë¸ íŒŒì¼ ì •ë³´"""
    name: str
    path: str
    size_mb: float
    extension: str
    step_category: Optional[str]
    is_checkpoint: bool
    is_config: bool
    is_link: bool
    link_target: Optional[str]
    hash_preview: Optional[str]  # íŒŒì¼ ì‹œì‘ ë¶€ë¶„ì˜ í•´ì‹œ
    potential_duplicates: List[str]
    accessibility: str  # 'readable', 'permission_denied', 'broken_link'
    pytorch_loadable: Optional[bool]
    estimated_parameters: Optional[int]

@dataclass
class DirectoryStats:
    """ë””ë ‰í† ë¦¬ í†µê³„"""
    total_files: int
    total_size_gb: float
    by_extension: Dict[str, int]
    by_step: Dict[str, int]
    duplicate_groups: List[List[str]]
    broken_links: List[str]
    large_files: List[Tuple[str, float]]  # (íŒŒì¼ëª…, í¬ê¸°GB)
    
class AIModelsAnalyzer:
    """AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ë¶„ì„ê¸°"""
    
    # ì§€ì›í•˜ëŠ” ëª¨ë¸ íŒŒì¼ í™•ì¥ì
    MODEL_EXTENSIONS = {
        '.pth', '.pt',           # PyTorch
        '.safetensors',          # SafeTensors
        '.onnx',                 # ONNX
        '.pkl', '.pickle',       # Pickle
        '.bin',                  # Binary
        '.h5',                   # HDF5
        '.tflite',               # TensorFlow Lite
        '.pb',                   # TensorFlow
        '.engine',               # TensorRT
    }
    
    # ì„¤ì • íŒŒì¼ í™•ì¥ì
    CONFIG_EXTENSIONS = {
        '.json', '.yaml', '.yml', '.txt', '.cfg', '.config', '.prototxt'
    }
    
    # ë‹¨ê³„ë³„ í‚¤ì›Œë“œ (ë””ë ‰í† ë¦¬/íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ)
    STEP_KEYWORDS = {
        'step_01_human_parsing': ['human_parsing', 'parsing', 'segformer', 'schp', 'graphonomy', 'atr', 'lip'],
        'step_02_pose_estimation': ['pose', 'openpose', 'mediapipe', 'hrnet', 'body_pose'],
        'step_03_cloth_segmentation': ['cloth', 'segmentation', 'u2net', 'sam', 'rembg', 'background'],
        'step_04_geometric_matching': ['geometric', 'matching', 'gmm', 'tps'],
        'step_05_cloth_warping': ['warping', 'tom', 'cloth_warping'],
        'step_06_virtual_fitting': ['fitting', 'ootd', 'diffusion', 'unet', 'vae', 'hrviton', 'viton'],
        'step_07_post_processing': ['post', 'enhancement', 'esrgan', 'gfpgan', 'super_resolution'],
        'step_08_quality_assessment': ['quality', 'assessment', 'clip', 'lpips']
    }
    
    def __init__(self, base_path: str = "backend/ai_models"):
        self.base_path = Path(base_path)
        self.files_info: List[ModelFileInfo] = []
        self.directory_stats = DirectoryStats(
            total_files=0,
            total_size_gb=0.0,
            by_extension=Counter(),
            by_step=Counter(),
            duplicate_groups=[],
            broken_links=[],
            large_files=[]
        )
        
    def analyze(self, detailed: bool = False) -> Dict[str, Any]:
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        logger.info(f"ğŸ” AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ë¶„ì„ ì‹œì‘: {self.base_path}")
        
        if not self.base_path.exists():
            logger.error(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.base_path}")
            return {}
            
        start_time = time.time()
        
        # 1. íŒŒì¼ ìŠ¤ìº”
        self._scan_files(detailed)
        
        # 2. ì¤‘ë³µ íŒŒì¼ íƒì§€
        self._detect_duplicates()
        
        # 3. í†µê³„ ê³„ì‚°
        self._calculate_stats()
        
        # 4. ê²°ê³¼ ì •ë¦¬
        analysis_time = time.time() - start_time
        
        result = {
            'analysis_info': {
                'base_path': str(self.base_path.absolute()),
                'analysis_time_seconds': round(analysis_time, 2),
                'total_files_scanned': len(self.files_info),
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            },
            'directory_stats': asdict(self.directory_stats),
            'files_by_category': self._categorize_files(),
            'health_report': self._generate_health_report(),
            'recommendations': self._generate_recommendations()
        }
        
        if detailed:
            result['detailed_files'] = [asdict(f) for f in self.files_info]
            
        logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ ({analysis_time:.2f}ì´ˆ)")
        return result
    
    def _scan_files(self, detailed: bool = False):
        """íŒŒì¼ ìŠ¤ìº”"""
        logger.info("ğŸ“‚ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        
        for root, dirs, files in os.walk(self.base_path):
            root_path = Path(root)
            
            for file_name in files:
                file_path = root_path / file_name
                
                try:
                    file_info = self._analyze_file(file_path, detailed)
                    if file_info:
                        self.files_info.append(file_info)
                        
                except Exception as e:
                    logger.debug(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
                    
        logger.info(f"ğŸ“Š ì´ {len(self.files_info)}ê°œ íŒŒì¼ ë°œê²¬")
    
    def _analyze_file(self, file_path: Path, detailed: bool = False) -> Optional[ModelFileInfo]:
        """ê°œë³„ íŒŒì¼ ë¶„ì„"""
        try:
            # ê¸°ë³¸ ì •ë³´
            file_stat = file_path.stat()
            size_mb = file_stat.st_size / (1024 * 1024)
            extension = file_path.suffix.lower()
            
            # ëª¨ë¸ íŒŒì¼ ë˜ëŠ” ì„¤ì • íŒŒì¼ì¸ì§€ í™•ì¸
            is_model = extension in self.MODEL_EXTENSIONS
            is_config = extension in self.CONFIG_EXTENSIONS
            
            if not (is_model or is_config):
                return None
                
            # ë§í¬ í™•ì¸
            is_link = file_path.is_symlink()
            link_target = None
            accessibility = 'readable'
            
            if is_link:
                try:
                    link_target = str(file_path.readlink())
                    if not file_path.exists():
                        accessibility = 'broken_link'
                except:
                    accessibility = 'broken_link'
            
            # ì ‘ê·¼ ê¶Œí•œ í™•ì¸
            if accessibility == 'readable' and not os.access(file_path, os.R_OK):
                accessibility = 'permission_denied'
            
            # ë‹¨ê³„ ì¹´í…Œê³ ë¦¬ ì¶”ì •
            step_category = self._guess_step_category(file_path)
            
            # ì²´í¬í¬ì¸íŠ¸ ì—¬ë¶€
            is_checkpoint = 'checkpoint' in file_path.name.lower() or extension in {'.pth', '.pt', '.safetensors'}
            
            # í•´ì‹œ ë¯¸ë¦¬ë³´ê¸° (detailed ëª¨ë“œì—ì„œë§Œ)
            hash_preview = None
            if detailed and accessibility == 'readable' and size_mb < 100:  # 100MB ë¯¸ë§Œë§Œ
                hash_preview = self._get_file_hash_preview(file_path)
            
            # PyTorch ë¡œë“œ ê°€ëŠ¥ ì—¬ë¶€ (ê°„ë‹¨ ì²´í¬)
            pytorch_loadable = None
            estimated_parameters = None
            if detailed and extension in {'.pth', '.pt'} and accessibility == 'readable' and size_mb < 500:
                pytorch_loadable, estimated_parameters = self._test_pytorch_loading(file_path)
            
            return ModelFileInfo(
                name=file_path.name,
                path=str(file_path.relative_to(self.base_path)),
                size_mb=round(size_mb, 2),
                extension=extension,
                step_category=step_category,
                is_checkpoint=is_checkpoint,
                is_config=is_config,
                is_link=is_link,
                link_target=link_target,
                hash_preview=hash_preview,
                potential_duplicates=[],
                accessibility=accessibility,
                pytorch_loadable=pytorch_loadable,
                estimated_parameters=estimated_parameters
            )
            
        except Exception as e:
            logger.debug(f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
            return None
    
    def _guess_step_category(self, file_path: Path) -> Optional[str]:
        """íŒŒì¼ ê²½ë¡œ/ì´ë¦„ìœ¼ë¡œ ë‹¨ê³„ ì¶”ì •"""
        path_str = str(file_path).lower()
        
        for step_name, keywords in self.STEP_KEYWORDS.items():
            if any(keyword in path_str for keyword in keywords):
                return step_name
                
        return None
    
    def _get_file_hash_preview(self, file_path: Path, chunk_size: int = 8192) -> str:
        """íŒŒì¼ ì‹œì‘ ë¶€ë¶„ì˜ í•´ì‹œ (ì¤‘ë³µ íƒì§€ìš©)"""
        try:
            hasher = hashlib.md5()
            with open(file_path, 'rb') as f:
                chunk = f.read(chunk_size)
                hasher.update(chunk)
            return hasher.hexdigest()[:16]
        except:
            return None
    
    def _test_pytorch_loading(self, file_path: Path) -> Tuple[bool, Optional[int]]:
        """PyTorch ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
        try:
            import torch
            
            # ì•ˆì „í•œ ë¡œë”© ì‹œë„
            checkpoint = torch.load(file_path, map_location='cpu', weights_only=True)
            
            # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶”ì •
            param_count = 0
            if isinstance(checkpoint, dict):
                for key, value in checkpoint.items():
                    if torch.is_tensor(value):
                        param_count += value.numel()
            
            return True, param_count if param_count > 0 else None
            
        except Exception:
            try:
                # weights_only=Falseë¡œ ì¬ì‹œë„
                torch.load(file_path, map_location='cpu')
                return True, None
            except:
                return False, None
    
    def _detect_duplicates(self):
        """ì¤‘ë³µ íŒŒì¼ íƒì§€"""
        logger.info("ğŸ” ì¤‘ë³µ íŒŒì¼ íƒì§€ ì¤‘...")
        
        # í¬ê¸°ì™€ í•´ì‹œë¡œ ê·¸ë£¹í™”
        size_groups = defaultdict(list)
        hash_groups = defaultdict(list)
        
        for file_info in self.files_info:
            size_groups[file_info.size_mb].append(file_info)
            if file_info.hash_preview:
                hash_groups[file_info.hash_preview].append(file_info)
        
        # ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸°
        duplicate_groups = []
        
        # ë™ì¼í•œ í¬ê¸°ì¸ íŒŒì¼ë“¤ í™•ì¸
        for size, files in size_groups.items():
            if len(files) > 1 and size > 1:  # 1MB ì´ìƒì¸ íŒŒì¼ë§Œ
                group_names = [f.name for f in files]
                duplicate_groups.append(group_names)
                
                # ê° íŒŒì¼ì— ì¤‘ë³µ ì •ë³´ ì¶”ê°€
                for file_info in files:
                    file_info.potential_duplicates = [name for name in group_names if name != file_info.name]
        
        self.directory_stats.duplicate_groups = duplicate_groups
        logger.info(f"ğŸ“Š {len(duplicate_groups)}ê°œ ì¤‘ë³µ ê·¸ë£¹ ë°œê²¬")
    
    def _calculate_stats(self):
        """í†µê³„ ê³„ì‚°"""
        logger.info("ğŸ“Š í†µê³„ ê³„ì‚° ì¤‘...")
        
        total_size_gb = 0.0
        extension_count = Counter()
        step_count = Counter()
        broken_links = []
        large_files = []
        
        for file_info in self.files_info:
            total_size_gb += file_info.size_mb / 1024
            extension_count[file_info.extension] += 1
            
            if file_info.step_category:
                step_count[file_info.step_category] += 1
            else:
                step_count['unknown'] += 1
                
            if file_info.accessibility == 'broken_link':
                broken_links.append(file_info.name)
                
            if file_info.size_mb > 1024:  # 1GB ì´ìƒ
                large_files.append((file_info.name, file_info.size_mb / 1024))
        
        # í° íŒŒì¼ ìˆœìœ¼ë¡œ ì •ë ¬
        large_files.sort(key=lambda x: x[1], reverse=True)
        
        self.directory_stats.total_files = len(self.files_info)
        self.directory_stats.total_size_gb = round(total_size_gb, 2)
        self.directory_stats.by_extension = dict(extension_count)
        self.directory_stats.by_step = dict(step_count)
        self.directory_stats.broken_links = broken_links
        self.directory_stats.large_files = large_files[:20]  # ìƒìœ„ 20ê°œ
    
    def _categorize_files(self) -> Dict[str, Any]:
        """íŒŒì¼ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜"""
        categories = {
            'model_files': [],
            'config_files': [],
            'checkpoint_files': [],
            'broken_files': [],
            'large_files': [],
            'unknown_category': []
        }
        
        for file_info in self.files_info:
            if file_info.accessibility == 'broken_link':
                categories['broken_files'].append(file_info.name)
            elif file_info.is_checkpoint:
                categories['checkpoint_files'].append(file_info.name)
            elif file_info.is_config:
                categories['config_files'].append(file_info.name)
            elif file_info.extension in self.MODEL_EXTENSIONS:
                categories['model_files'].append(file_info.name)
            else:
                categories['unknown_category'].append(file_info.name)
                
            if file_info.size_mb > 1024:
                categories['large_files'].append(f"{file_info.name} ({file_info.size_mb/1024:.1f}GB)")
        
        return categories
    
    def _generate_health_report(self) -> Dict[str, Any]:
        """ë””ë ‰í† ë¦¬ ê±´ê°• ìƒíƒœ ë³´ê³ ì„œ"""
        total_files = len(self.files_info)
        broken_count = len(self.directory_stats.broken_links)
        duplicate_count = sum(len(group) for group in self.directory_stats.duplicate_groups)
        
        # ê±´ê°• ì ìˆ˜ ê³„ì‚° (0-100)
        health_score = 100
        
        if broken_count > 0:
            health_score -= min(30, broken_count * 5)
            
        if duplicate_count > total_files * 0.3:  # 30% ì´ìƒì´ ì¤‘ë³µ
            health_score -= 20
            
        if self.directory_stats.total_size_gb > 200:  # 200GB ì´ìƒ
            health_score -= 10
        
        health_score = max(0, health_score)
        
        return {
            'health_score': health_score,
            'status': 'healthy' if health_score >= 80 else 'needs_attention' if health_score >= 60 else 'critical',
            'issues': {
                'broken_links': broken_count,
                'potential_duplicates': duplicate_count,
                'large_files_count': len(self.directory_stats.large_files),
                'permission_issues': len([f for f in self.files_info if f.accessibility == 'permission_denied'])
            },
            'recommendations_count': len(self._generate_recommendations())
        }
    
    def _generate_recommendations(self) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        # ê¹¨ì§„ ë§í¬
        if self.directory_stats.broken_links:
            recommendations.append(f"ğŸ”— {len(self.directory_stats.broken_links)}ê°œì˜ ê¹¨ì§„ ì‹¬ë³¼ë¦­ ë§í¬ ìˆ˜ì • í•„ìš”")
        
        # ì¤‘ë³µ íŒŒì¼
        if self.directory_stats.duplicate_groups:
            total_duplicates = sum(len(group) for group in self.directory_stats.duplicate_groups)
            recommendations.append(f"ğŸ“‚ {total_duplicates}ê°œì˜ ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ í•„ìš”")
        
        # ëŒ€ìš©ëŸ‰ íŒŒì¼
        if len(self.directory_stats.large_files) > 10:
            recommendations.append("ğŸ’¾ ëŒ€ìš©ëŸ‰ íŒŒì¼ë“¤ì˜ í´ë¼ìš°ë“œ ì €ì¥ì†Œ ì´ë™ ê³ ë ¤")
        
        # ë¯¸ë¶„ë¥˜ íŒŒì¼
        unknown_count = self.directory_stats.by_step.get('unknown', 0)
        if unknown_count > len(self.files_info) * 0.2:
            recommendations.append(f"ğŸ“‹ {unknown_count}ê°œ ë¯¸ë¶„ë¥˜ íŒŒì¼ì˜ ë‹¨ê³„ë³„ ì •ë¦¬ í•„ìš”")
        
        # ì „ì²´ í¬ê¸°
        if self.directory_stats.total_size_gb > 100:
            recommendations.append(f"ğŸ’¿ ì „ì²´ ìš©ëŸ‰ {self.directory_stats.total_size_gb:.1f}GB - ìš©ëŸ‰ ìµœì í™” ê²€í†  í•„ìš”")
        
        return recommendations

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="AI Models Directory Analyzer")
    parser.add_argument("--path", default="backend/ai_models", help="ë¶„ì„í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    parser.add_argument("--detailed", action="store_true", help="ìƒì„¸ ë¶„ì„ (ë” ì˜¤ë˜ ê±¸ë¦¼)")
    parser.add_argument("--export-json", help="ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥")
    parser.add_argument("--scan-only", action="store_true", help="ìŠ¤ìº”ë§Œ ìˆ˜í–‰ (PyTorch ê²€ì¦ ì œì™¸)")
    
    args = parser.parse_args()
    
    # ë¶„ì„ ì‹¤í–‰
    analyzer = AIModelsAnalyzer(args.path)
    result = analyzer.analyze(detailed=args.detailed and not args.scan_only)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ” AI MODELS DIRECTORY ANALYSIS REPORT")
    print("="*60)
    
    # ê¸°ë³¸ ì •ë³´
    info = result['analysis_info']
    print(f"ğŸ“‚ Base Path: {info['base_path']}")
    print(f"â±ï¸  Analysis Time: {info['analysis_time_seconds']}s")
    print(f"ğŸ“Š Files Scanned: {info['total_files_scanned']}")
    
    # ë””ë ‰í† ë¦¬ í†µê³„
    stats = result['directory_stats']
    print(f"\nğŸ“ˆ DIRECTORY STATISTICS")
    print(f"   Total Files: {stats['total_files']}")
    print(f"   Total Size: {stats['total_size_gb']:.2f} GB")
    print(f"   Duplicate Groups: {len(stats['duplicate_groups'])}")
    print(f"   Broken Links: {len(stats['broken_links'])}")
    
    # íŒŒì¼ í˜•ì‹ë³„ ë¶„í¬
    print(f"\nğŸ“„ BY FILE EXTENSION:")
    for ext, count in sorted(stats['by_extension'].items(), key=lambda x: x[1], reverse=True):
        print(f"   {ext}: {count} files")
    
    # ë‹¨ê³„ë³„ ë¶„í¬
    print(f"\nğŸ¯ BY AI STEP:")
    for step, count in sorted(stats['by_step'].items(), key=lambda x: x[1], reverse=True):
        print(f"   {step}: {count} files")
    
    # ê±´ê°• ìƒíƒœ
    health = result['health_report']
    print(f"\nğŸ’Š HEALTH REPORT")
    print(f"   Score: {health['health_score']}/100 ({health['status']})")
    print(f"   Issues: {health['issues']}")
    
    # ê¶Œì¥ì‚¬í•­
    print(f"\nğŸ’¡ RECOMMENDATIONS:")
    for rec in result['recommendations']:
        print(f"   â€¢ {rec}")
    
    # ëŒ€ìš©ëŸ‰ íŒŒì¼ ëª©ë¡
    if stats['large_files']:
        print(f"\nğŸ“¦ LARGE FILES (>1GB):")
        for name, size_gb in stats['large_files'][:10]:
            print(f"   {name}: {size_gb:.2f} GB")
    
    # JSON ë‚´ë³´ë‚´ê¸°
    if args.export_json:
        with open(args.export_json, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False, default=str)
        print(f"\nğŸ’¾ ê²°ê³¼ë¥¼ {args.export_json}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    
    print("\n" + "="*60)

if __name__ == "__main__":
    main()