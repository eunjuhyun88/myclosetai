#!/usr/bin/env python3
"""
ğŸ”¥ ì™„ì „í•œ AI ëª¨ë¸ ë¡œë”© ì§„ë‹¨ ì‹œìŠ¤í…œ v2.0 - ë¬´í•œë£¨í”„ ë°©ì§€ + ì™„ì „ ìƒíƒœ íŒŒì•…
backend/complete_ai_debug.py

âœ… ë¬´í•œë£¨í”„ ì™„ì „ ë°©ì§€
âœ… ë‹¨ê³„ë³„ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
âœ… AI ëª¨ë¸ ë¡œë”© ìƒíƒœ ì™„ì „ íŒŒì•…
âœ… Stepë³„ ì´ˆê¸°í™” ì•ˆì „ í…ŒìŠ¤íŠ¸
âœ… ì‹¤ì œ íŒŒì¼ í¬ê¸° ë° ê²½ë¡œ ê²€ì¦
âœ… ì˜ì¡´ì„± ìƒíƒœ ì™„ì „ ë¶„ì„
âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
âœ… M3 Max ìµœì í™” ìƒíƒœ í™•ì¸
âœ… conda í™˜ê²½ í˜¸í™˜ì„± ê²€ì¦
âœ… 229GB AI ëª¨ë¸ ì™„ì „ ë§¤í•‘
"""

import sys
import os
import time
import traceback
import logging
import asyncio
import threading
import psutil
import platform
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, TimeoutError
import weakref
import gc
from contextlib import contextmanager

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "backend"))

# =============================================================================
# ğŸ”¥ 1. ì•ˆì „ ì„¤ì • ë° ë¬´í•œë£¨í”„ ë°©ì§€ ì‹œìŠ¤í…œ
# =============================================================================

class SafetyManager:
    """ë¬´í•œë£¨í”„ ë° ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.timeout_duration = 30  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
        self.max_iterations = 10    # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
        self.initialized_instances = weakref.WeakSet()
        self.active_threads = []
        self.memory_threshold_mb = 8192  # 8GB ë©”ëª¨ë¦¬ ì„ê³„ê°’
        
    @contextmanager
    def safe_execution(self, description: str):
        """ì•ˆì „í•œ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        print(f"ğŸ”’ {description} ì•ˆì „ ì‹¤í–‰ ì‹œì‘ (íƒ€ì„ì•„ì›ƒ: {self.timeout_duration}ì´ˆ)")
        
        try:
            yield
            
        except Exception as e:
            print(f"âŒ {description} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
        finally:
            elapsed = time.time() - start_time
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024
            memory_used = end_memory - start_memory
            
            print(f"âœ… {description} ì™„ë£Œ ({elapsed:.2f}ì´ˆ, ë©”ëª¨ë¦¬: +{memory_used:.1f}MB)")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if memory_used > 500:  # 500MB ì´ìƒ ì‚¬ìš©ì‹œ ê°•ì œ ì •ë¦¬
                gc.collect()

    def check_system_resources(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            memory = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent(interval=1)
            
            return {
                'memory': {
                    'total_gb': memory.total / (1024**3),
                    'available_gb': memory.available / (1024**3),
                    'used_percent': memory.percent,
                    'available_percent': 100 - memory.percent
                },
                'cpu': {
                    'usage_percent': cpu_percent,
                    'core_count': psutil.cpu_count(),
                    'freq_current': psutil.cpu_freq().current if psutil.cpu_freq() else 0
                },
                'warnings': []
            }
        except Exception as e:
            return {'error': str(e), 'warnings': [f"ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"]}

# ì „ì—­ ì•ˆì „ ë§¤ë‹ˆì €
safety = SafetyManager()

# =============================================================================
# ğŸ”¥ 2. ë¡œê¹… ì‹œìŠ¤í…œ ì•ˆì „ ì„¤ì •
# =============================================================================

def setup_safe_logging():
    """ì•ˆì „í•œ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ëª¨ë‘ ì œê±°
    for logger_name in list(logging.Logger.manager.loggerDict.keys()):
        logger = logging.getLogger(logger_name)
        logger.handlers.clear()
        logger.propagate = False
    
    # ë£¨íŠ¸ ë¡œê±° ì¬ì„¤ì •
    root_logger = logging.getLogger()
    root_logger.handlers.clear()
    root_logger.setLevel(logging.WARNING)  # ì¤‘ë³µ ë©”ì‹œì§€ ë°©ì§€
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ë§Œ ì¶”ê°€
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.WARNING)
    formatter = logging.Formatter('%(levelname)s: %(message)s')
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    print("âœ… ì•ˆì „í•œ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")

# =============================================================================
# ğŸ”¥ 3. AI ëª¨ë¸ íŒŒì¼ ì™„ì „ ë¶„ì„ ì‹œìŠ¤í…œ
# =============================================================================

@dataclass
class ModelFileInfo:
    """ëª¨ë¸ íŒŒì¼ ì •ë³´"""
    name: str
    path: Path
    size_mb: float
    exists: bool
    accessible: bool
    file_type: str
    step_assignment: str

@dataclass
class StepInfo:
    """Step ì •ë³´"""
    name: str
    step_id: int
    module_path: str
    class_name: str
    import_success: bool
    instance_created: bool
    initialized: bool
    ai_models_loaded: List[str]
    dependencies: Dict[str, bool]
    errors: List[str]

class CompleteModelAnalyzer:
    """ì™„ì „í•œ AI ëª¨ë¸ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.model_files: List[ModelFileInfo] = []
        self.steps: Dict[str, StepInfo] = {}
        self.analysis_cache: Dict[str, Any] = {}
        
    def analyze_complete_model_structure(self) -> Dict[str, Any]:
        """ì™„ì „í•œ ëª¨ë¸ êµ¬ì¡° ë¶„ì„"""
        
        analysis_result = {
            'timestamp': time.time(),
            'system_info': self._get_system_info(),
            'model_files': self._analyze_model_files(),
            'steps_analysis': self._analyze_steps(),
            'dependencies': self._analyze_dependencies(),
            'memory_usage': self._analyze_memory_usage(),
            'recommendations': []
        }
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        analysis_result['recommendations'] = self._generate_recommendations(analysis_result)
        
        return analysis_result
    
    def _get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        try:
            return {
                'platform': {
                    'system': platform.system(),
                    'release': platform.release(),
                    'machine': platform.machine(),
                    'processor': platform.processor()
                },
                'python': {
                    'version': sys.version,
                    'path': sys.path[:5]  # ì²˜ìŒ 5ê°œë§Œ
                },
                'hardware': safety.check_system_resources(),
                'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
                'is_m3_max': 'arm64' in platform.machine().lower() and 'darwin' in platform.system().lower()
            }
        except Exception as e:
            return {'error': str(e)}
    
    def _analyze_model_files(self) -> Dict[str, Any]:
        """AI ëª¨ë¸ íŒŒì¼ ë¶„ì„"""
        
        print("ğŸ” AI ëª¨ë¸ íŒŒì¼ ë¶„ì„ ì¤‘...")
        
        model_analysis = {
            'total_files': 0,
            'total_size_gb': 0.0,
            'files_by_step': {},
            'large_files': [],
            'missing_files': [],
            'search_paths': []
        }
        
        # ê°€ëŠ¥í•œ ëª¨ë¸ ê²½ë¡œë“¤
        possible_paths = [
            Path("ai_models"),
            Path("models"),
            Path("backend/ai_models"),
            Path("../ai_models"),
            Path("./ai_models"),
            Path("checkpoints"),
            Path("weights")
        ]
        
        # ê° ê²½ë¡œì—ì„œ ëª¨ë¸ íŒŒì¼ ì°¾ê¸°
        for search_path in possible_paths:
            model_analysis['search_paths'].append(str(search_path))
            
            if not search_path.exists():
                continue
            
            print(f"  ğŸ“ ê²€ìƒ‰ ì¤‘: {search_path}")
            
            # ëª¨ë¸ íŒŒì¼ í™•ì¥ìë“¤
            model_extensions = ["*.pth", "*.safetensors", "*.bin", "*.pt", "*.ckpt", "*.pkl"]
            
            for ext in model_extensions:
                try:
                    found_files = list(search_path.rglob(ext))
                    
                    for file_path in found_files:
                        try:
                            size_bytes = file_path.stat().st_size
                            size_mb = size_bytes / (1024 * 1024)
                            
                            # Step í• ë‹¹ ì¶”ì •
                            step_assignment = self._estimate_step_assignment(file_path)
                            
                            file_info = ModelFileInfo(
                                name=file_path.name,
                                path=file_path,
                                size_mb=size_mb,
                                exists=True,
                                accessible=True,
                                file_type=file_path.suffix[1:],
                                step_assignment=step_assignment
                            )
                            
                            self.model_files.append(file_info)
                            model_analysis['total_files'] += 1
                            model_analysis['total_size_gb'] += size_mb / 1024
                            
                            # Stepë³„ ë¶„ë¥˜
                            if step_assignment not in model_analysis['files_by_step']:
                                model_analysis['files_by_step'][step_assignment] = []
                            model_analysis['files_by_step'][step_assignment].append({
                                'name': file_path.name,
                                'size_mb': size_mb,
                                'path': str(file_path)
                            })
                            
                            # ëŒ€í˜• íŒŒì¼ (100MB ì´ìƒ)
                            if size_mb >= 100:
                                model_analysis['large_files'].append({
                                    'name': file_path.name,
                                    'size_mb': size_mb,
                                    'size_gb': size_mb / 1024,
                                    'step': step_assignment,
                                    'path': str(file_path)
                                })
                                
                        except Exception as e:
                            model_analysis['missing_files'].append({
                                'path': str(file_path),
                                'error': str(e)
                            })
                            
                except Exception as e:
                    print(f"    âš ï¸ {ext} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ëŒ€í˜• íŒŒì¼ ì •ë ¬ (í¬ê¸°ìˆœ)
        model_analysis['large_files'].sort(key=lambda x: x['size_mb'], reverse=True)
        
        return model_analysis
    
    def _estimate_step_assignment(self, file_path: Path) -> str:
        """íŒŒì¼ ê²½ë¡œë¡œ Step í• ë‹¹ ì¶”ì •"""
        path_str = str(file_path).lower()
        
        step_keywords = {
            'step_01_human_parsing': ['human', 'parsing', 'graphonomy', 'atr', 'schp', 'lip'],
            'step_02_pose_estimation': ['pose', 'openpose', 'yolo', 'hrnet', 'body'],
            'step_03_cloth_segmentation': ['cloth', 'segment', 'sam', 'u2net', 'isnet'],
            'step_04_geometric_matching': ['geometric', 'matching', 'gmm', 'tps'],
            'step_05_image_generation': ['generation', 'real', 'vis', 'xl'],
            'step_06_virtual_fitting': ['fitting', 'virtual', 'ootd', 'diffusion', 'stable']
        }
        
        for step, keywords in step_keywords.items():
            if any(keyword in path_str for keyword in keywords):
                return step
        
        return 'unknown'
    
    def _analyze_steps(self) -> Dict[str, Any]:
        """Stepë³„ ë¶„ì„"""
        
        print("ğŸ” Stepë³„ ìƒíƒœ ë¶„ì„ ì¤‘...")
        
        steps_to_analyze = [
            {
                'name': 'HumanParsingStep',
                'step_id': 1,
                'module': 'app.ai_pipeline.steps.step_01_human_parsing',
                'class': 'HumanParsingStep'
            },
            {
                'name': 'PoseEstimationStep',
                'step_id': 2,
                'module': 'app.ai_pipeline.steps.step_02_pose_estimation',
                'class': 'PoseEstimationStep'
            },
            {
                'name': 'ClothSegmentationStep',
                'step_id': 3,
                'module': 'app.ai_pipeline.steps.step_03_cloth_segmentation',
                'class': 'ClothSegmentationStep'
            },
            {
                'name': 'GeometricMatchingStep',
                'step_id': 4,
                'module': 'app.ai_pipeline.steps.step_04_geometric_matching',
                'class': 'GeometricMatchingStep'
            }
        ]
        
        analysis = {
            'total_steps': len(steps_to_analyze),
            'import_success': 0,
            'instance_success': 0,
            'initialization_success': 0,
            'step_details': {}
        }
        
        for step_config in steps_to_analyze:
            step_name = step_config['name']
            
            print(f"  ğŸ”§ {step_name} ë¶„ì„ ì¤‘...")
            
            step_info = StepInfo(
                name=step_name,
                step_id=step_config['step_id'],
                module_path=step_config['module'],
                class_name=step_config['class'],
                import_success=False,
                instance_created=False,
                initialized=False,
                ai_models_loaded=[],
                dependencies={},
                errors=[]
            )
            
            # ì•ˆì „í•œ import í…ŒìŠ¤íŠ¸
            with safety.safe_execution(f"{step_name} import"):
                try:
                    module = __import__(step_config['module'], fromlist=[step_config['class']])
                    step_class = getattr(module, step_config['class'])
                    step_info.import_success = True
                    analysis['import_success'] += 1
                    
                    print(f"    âœ… Import ì„±ê³µ")
                    
                except Exception as e:
                    step_info.errors.append(f"Import ì‹¤íŒ¨: {e}")
                    print(f"    âŒ Import ì‹¤íŒ¨: {e}")
                    continue
            
            # ì•ˆì „í•œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
            with safety.safe_execution(f"{step_name} instance creation"):
                try:
                    # ì•ˆì „í•œ íŒŒë¼ë¯¸í„°ë¡œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                    step_instance = step_class(
                        device='cpu',  # ì•ˆì „í•œ ë””ë°”ì´ìŠ¤
                        strict_mode=False,  # ê´€ëŒ€í•œ ëª¨ë“œ
                    )
                    step_info.instance_created = True
                    analysis['instance_success'] += 1
                    
                    print(f"    âœ… ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ")
                    
                    # ìƒíƒœ ì •ë³´ ìˆ˜ì§‘
                    if hasattr(step_instance, 'get_status'):
                        try:
                            status = step_instance.get_status()
                            if isinstance(status, dict):
                                step_info.ai_models_loaded = status.get('ai_models_loaded', [])
                                step_info.dependencies = status.get('dependencies_injected', {})
                        except Exception as e:
                            step_info.errors.append(f"ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    
                except Exception as e:
                    step_info.errors.append(f"ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    print(f"    âŒ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    continue
            
            # ì•ˆì „í•œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ (ì„ íƒì )
            if step_info.instance_created:
                with safety.safe_execution(f"{step_name} initialization"):
                    try:
                        if hasattr(step_instance, 'initialize'):
                            # ì´ˆê¸°í™” ì‹œë„ (íƒ€ì„ì•„ì›ƒ ì ìš©)
                            if asyncio.iscoroutinefunction(step_instance.initialize):
                                # async ë©”ì„œë“œ
                                try:
                                    loop = asyncio.get_event_loop()
                                except RuntimeError:
                                    loop = asyncio.new_event_loop()
                                    asyncio.set_event_loop(loop)
                                
                                # íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë³´í˜¸
                                future = asyncio.wait_for(
                                    step_instance.initialize(), 
                                    timeout=15.0  # 15ì´ˆ íƒ€ì„ì•„ì›ƒ
                                )
                                init_result = loop.run_until_complete(future)
                            else:
                                # sync ë©”ì„œë“œ
                                init_result = step_instance.initialize()
                            
                            if init_result:
                                step_info.initialized = True
                                analysis['initialization_success'] += 1
                                print(f"    âœ… ì´ˆê¸°í™” ì„±ê³µ")
                            else:
                                step_info.errors.append("ì´ˆê¸°í™” False ë°˜í™˜")
                                print(f"    âš ï¸ ì´ˆê¸°í™” ì‹¤íŒ¨ (False ë°˜í™˜)")
                                
                    except TimeoutError:
                        step_info.errors.append("ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ (15ì´ˆ)")
                        print(f"    âš ï¸ ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ")
                    except Exception as e:
                        step_info.errors.append(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                        print(f"    âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            self.steps[step_name] = step_info
            analysis['step_details'][step_name] = {
                'import_success': step_info.import_success,
                'instance_created': step_info.instance_created,
                'initialized': step_info.initialized,
                'ai_models_loaded': step_info.ai_models_loaded,
                'dependencies': step_info.dependencies,
                'error_count': len(step_info.errors),
                'errors': step_info.errors[:3]  # ì²˜ìŒ 3ê°œ ì—ëŸ¬ë§Œ
            }
        
        return analysis
    
    def _analyze_dependencies(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ë¶„ì„"""
        
        print("ğŸ” ì˜ì¡´ì„± ë¶„ì„ ì¤‘...")
        
        dependencies = {
            'core_libraries': {},
            'ai_libraries': {},
            'project_modules': {},
            'missing_dependencies': []
        }
        
        # í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
        core_libs = {
            'torch': 'PyTorch',
            'torchvision': 'TorchVision',
            'numpy': 'NumPy',
            'PIL': 'Pillow',
            'cv2': 'OpenCV'
        }
        
        for lib, name in core_libs.items():
            try:
                module = __import__(lib)
                version = getattr(module, '__version__', 'unknown')
                dependencies['core_libraries'][name] = {
                    'installed': True,
                    'version': version,
                    'module_name': lib
                }
            except ImportError:
                dependencies['core_libraries'][name] = {
                    'installed': False,
                    'error': 'Not installed'
                }
                dependencies['missing_dependencies'].append(name)
        
        # AI ë¼ì´ë¸ŒëŸ¬ë¦¬
        ai_libs = {
            'transformers': 'Transformers',
            'diffusers': 'Diffusers',
            'ultralytics': 'Ultralytics',
            'safetensors': 'SafeTensors',
            'segment_anything': 'Segment Anything'
        }
        
        for lib, name in ai_libs.items():
            try:
                module = __import__(lib)
                version = getattr(module, '__version__', 'unknown')
                dependencies['ai_libraries'][name] = {
                    'installed': True,
                    'version': version
                }
            except ImportError:
                dependencies['ai_libraries'][name] = {
                    'installed': False,
                    'error': 'Not installed'
                }
        
        # í”„ë¡œì íŠ¸ ëª¨ë“ˆ
        project_modules = [
            'app.ai_pipeline.utils.memory_manager',
            'app.ai_pipeline.utils.model_loader',
            'app.core.config'
        ]
        
        for module_name in project_modules:
            try:
                __import__(module_name)
                dependencies['project_modules'][module_name] = {
                    'available': True
                }
            except ImportError as e:
                dependencies['project_modules'][module_name] = {
                    'available': False,
                    'error': str(e)
                }
        
        return dependencies
    
    def _analyze_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„"""
        
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            system_memory = psutil.virtual_memory()
            
            return {
                'process_memory': {
                    'rss_mb': memory_info.rss / (1024 * 1024),
                    'vms_mb': memory_info.vms / (1024 * 1024),
                    'percent': process.memory_percent()
                },
                'system_memory': {
                    'total_gb': system_memory.total / (1024**3),
                    'available_gb': system_memory.available / (1024**3),
                    'used_percent': system_memory.percent,
                    'free_gb': system_memory.free / (1024**3)
                },
                'recommendations': []
            }
        except Exception as e:
            return {'error': str(e)}
    
    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ëª¨ë¸ íŒŒì¼ ê´€ë ¨
        model_files = analysis.get('model_files', {})
        if model_files.get('total_files', 0) == 0:
            recommendations.append("âŒ AI ëª¨ë¸ íŒŒì¼ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ai_models ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        elif model_files.get('total_size_gb', 0) < 1:
            recommendations.append("âš ï¸ AI ëª¨ë¸ íŒŒì¼ì´ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ëŒ€í˜• ëª¨ë¸ë“¤ì´ ëˆ„ë½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            recommendations.append(f"âœ… AI ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_files['total_files']}ê°œ ({model_files['total_size_gb']:.1f}GB)")
        
        # Step ë¶„ì„ ê´€ë ¨
        steps = analysis.get('steps_analysis', {})
        if steps.get('import_success', 0) < steps.get('total_steps', 0):
            recommendations.append("âŒ ì¼ë¶€ Stepì˜ importê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜ì¡´ì„±ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        if steps.get('instance_success', 0) < steps.get('import_success', 0):
            recommendations.append("âš ï¸ ì¼ë¶€ Stepì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        # ì˜ì¡´ì„± ê´€ë ¨
        deps = analysis.get('dependencies', {})
        missing_deps = deps.get('missing_dependencies', [])
        if missing_deps:
            recommendations.append(f"âŒ ëˆ„ë½ëœ ì˜ì¡´ì„±: {', '.join(missing_deps)}")
        
        # ë©”ëª¨ë¦¬ ê´€ë ¨
        memory = analysis.get('memory_usage', {})
        system_mem = memory.get('system_memory', {})
        if system_mem.get('available_gb', 0) < 2:
            recommendations.append("âš ï¸ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. AI ëª¨ë¸ ë¡œë”©ì— ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return recommendations

# =============================================================================
# ğŸ”¥ 4. ë©”ì¸ ë””ë²„ê·¸ ì‹¤í–‰ê¸°
# =============================================================================

class CompleteAIDebugger:
    """ì™„ì „í•œ AI ë””ë²„ê·¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.analyzer = CompleteModelAnalyzer()
        self.start_time = time.time()
        
    def run_complete_diagnosis(self) -> Dict[str, Any]:
        """ì™„ì „í•œ ì§„ë‹¨ ì‹¤í–‰"""
        
        print("ğŸ”¥ MyCloset AI ì™„ì „ ì§„ë‹¨ ì‹œìŠ¤í…œ v2.0 ì‹œì‘")
        print("=" * 80)
        
        # ì•ˆì „ ì„¤ì •
        setup_safe_logging()
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
        print("\nğŸ“Š 1. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸")
        with safety.safe_execution("ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸"):
            system_resources = safety.check_system_resources()
            
            if 'error' not in system_resources:
                memory = system_resources['memory']
                cpu = system_resources['cpu']
                
                print(f"   ğŸ’¾ ë©”ëª¨ë¦¬: {memory['available_gb']:.1f}GB ì‚¬ìš© ê°€ëŠ¥ / {memory['total_gb']:.1f}GB ì´ëŸ‰")
                print(f"   ğŸ”¥ CPU: {cpu['usage_percent']:.1f}% ì‚¬ìš©ë¥ , {cpu['core_count']}ì½”ì–´")
                
                if memory['available_gb'] < 2:
                    print("   âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³ !")
                
                if cpu['usage_percent'] > 80:
                    print("   âš ï¸ CPU ì‚¬ìš©ë¥  ë†’ìŒ!")
            else:
                print(f"   âŒ ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {system_resources['error']}")
        
        # ì™„ì „í•œ ë¶„ì„ ì‹¤í–‰
        print("\nğŸ” 2. ì™„ì „í•œ AI ëª¨ë¸ ë¶„ì„ ì‹¤í–‰")
        
        with safety.safe_execution("ì™„ì „í•œ AI ëª¨ë¸ ë¶„ì„"):
            analysis_result = self.analyzer.analyze_complete_model_structure()
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_analysis_results(analysis_result)
        
        # ì§„ë‹¨ ì™„ë£Œ
        total_time = time.time() - self.start_time
        print(f"\nğŸ‰ ì™„ì „í•œ AI ì§„ë‹¨ ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
        
        return analysis_result
    
    def _print_analysis_results(self, analysis: Dict[str, Any]):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        
        print("\n" + "=" * 80)
        print("ğŸ“Š ì™„ì „í•œ AI ë¶„ì„ ê²°ê³¼")
        print("=" * 80)
        
        # ì‹œìŠ¤í…œ ì •ë³´
        system_info = analysis.get('system_info', {})
        if 'platform' in system_info:
            platform_info = system_info['platform']
            print(f"ğŸ–¥ï¸  ì‹œìŠ¤í…œ: {platform_info.get('system')} {platform_info.get('release')}")
            print(f"ğŸ”§ ì•„í‚¤í…ì²˜: {platform_info.get('machine')}")
            print(f"ğŸ Python: {system_info.get('python', {}).get('version', '').split()[0]}")
            print(f"ğŸŒ Conda í™˜ê²½: {system_info.get('conda_env', 'none')}")
            print(f"ğŸ M3 Max: {'Yes' if system_info.get('is_m3_max') else 'No'}")
        
        # ëª¨ë¸ íŒŒì¼ ë¶„ì„
        print(f"\nğŸ“ AI ëª¨ë¸ íŒŒì¼ ë¶„ì„:")
        model_files = analysis.get('model_files', {})
        print(f"   ğŸ“¦ ì´ íŒŒì¼: {model_files.get('total_files', 0)}ê°œ")
        print(f"   ğŸ’¾ ì´ í¬ê¸°: {model_files.get('total_size_gb', 0):.1f}GB")
        print(f"   ğŸ” ê²€ìƒ‰ ê²½ë¡œ: {len(model_files.get('search_paths', []))}ê°œ")
        
        # ëŒ€í˜• íŒŒì¼ë“¤ (ìƒìœ„ 10ê°œ)
        large_files = model_files.get('large_files', [])
        if large_files:
            print(f"\n   ğŸ”¥ ëŒ€í˜• ëª¨ë¸ íŒŒì¼ (ìƒìœ„ 10ê°œ):")
            for i, file_info in enumerate(large_files[:10]):
                print(f"      {i+1:2d}. {file_info['name']}: {file_info['size_gb']:.1f}GB ({file_info['step']})")
        
        # Stepë³„ íŒŒì¼ ë¶„í¬
        files_by_step = model_files.get('files_by_step', {})
        if files_by_step:
            print(f"\n   ğŸ“Š Stepë³„ íŒŒì¼ ë¶„í¬:")
            for step, files in files_by_step.items():
                file_count = len(files)
                total_size = sum(f['size_mb'] for f in files) / 1024
                print(f"      {step}: {file_count}ê°œ íŒŒì¼, {total_size:.1f}GB")
        
        # Step ë¶„ì„
        print(f"\nğŸš€ Stepë³„ ìƒíƒœ ë¶„ì„:")
        steps_analysis = analysis.get('steps_analysis', {})
        print(f"   ğŸ“Š Import ì„±ê³µ: {steps_analysis.get('import_success', 0)}/{steps_analysis.get('total_steps', 0)}")
        print(f"   ğŸ”§ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±: {steps_analysis.get('instance_success', 0)}/{steps_analysis.get('total_steps', 0)}")
        print(f"   âœ… ì´ˆê¸°í™” ì„±ê³µ: {steps_analysis.get('initialization_success', 0)}/{steps_analysis.get('total_steps', 0)}")
        
        # ê°œë³„ Step ìƒì„¸
        step_details = steps_analysis.get('step_details', {})
        for step_name, details in step_details.items():
            status = "âœ…" if details['initialized'] else "ğŸ”§" if details['instance_created'] else "âŒ"
            print(f"\n   {status} {step_name}:")
            print(f"      Import: {'âœ…' if details['import_success'] else 'âŒ'}")
            print(f"      ì¸ìŠ¤í„´ìŠ¤: {'âœ…' if details['instance_created'] else 'âŒ'}")
            print(f"      ì´ˆê¸°í™”: {'âœ…' if details['initialized'] else 'âŒ'}")
            
            if details['ai_models_loaded']:
                print(f"      AI ëª¨ë¸: {', '.join(details['ai_models_loaded'])}")
            
            if details['errors']:
                print(f"      ì˜¤ë¥˜: {details['errors'][0]}")  # ì²« ë²ˆì§¸ ì˜¤ë¥˜ë§Œ
        
        # ì˜ì¡´ì„± ë¶„ì„
        print(f"\nğŸ“š ì˜ì¡´ì„± ë¶„ì„:")
        dependencies = analysis.get('dependencies', {})
        
        core_libs = dependencies.get('core_libraries', {})
        installed_core = sum(1 for lib in core_libs.values() if lib.get('installed'))
        print(f"   ğŸ”§ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬: {installed_core}/{len(core_libs)}")
        
        ai_libs = dependencies.get('ai_libraries', {})
        installed_ai = sum(1 for lib in ai_libs.values() if lib.get('installed'))
        print(f"   ğŸ¤– AI ë¼ì´ë¸ŒëŸ¬ë¦¬: {installed_ai}/{len(ai_libs)}")
        
        # ëˆ„ë½ëœ ì˜ì¡´ì„±
        missing_deps = dependencies.get('missing_dependencies', [])
        if missing_deps:
            print(f"   âŒ ëˆ„ë½ëœ ì˜ì¡´ì„±: {', '.join(missing_deps)}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
        memory_usage = analysis.get('memory_usage', {})
        if 'process_memory' in memory_usage:
            process_mem = memory_usage['process_memory']
            system_mem = memory_usage['system_memory']
            
            print(f"   ğŸ”§ í”„ë¡œì„¸ìŠ¤: {process_mem.get('rss_mb', 0):.1f}MB ({process_mem.get('percent', 0):.1f}%)")
            print(f"   ğŸ–¥ï¸  ì‹œìŠ¤í…œ: {system_mem.get('used_percent', 0):.1f}% ì‚¬ìš©, {system_mem.get('available_gb', 0):.1f}GB ì‚¬ìš© ê°€ëŠ¥")
        
        # ì¶”ì²œì‚¬í•­
        print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
        recommendations = analysis.get('recommendations', [])
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        
        if not recommendations:
            print("   ğŸ‰ ëª¨ë“  ê²ƒì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!")

# =============================================================================
# ğŸ”¥ 5. ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    try:
        # ë””ë²„ê±° ìƒì„± ë° ì‹¤í–‰
        debugger = CompleteAIDebugger()
        
        # ì™„ì „í•œ ì§„ë‹¨ ì‹¤í–‰
        analysis_result = debugger.run_complete_diagnosis()
        
        # JSON ê²°ê³¼ ì €ì¥ (ì„ íƒì‚¬í•­)
        try:
            import json
            results_file = Path("complete_ai_analysis.json")
            
            # ì‹œê°„ ì •ë³´ ì¶”ê°€
            analysis_result['analysis_completed_at'] = time.time()
            analysis_result['total_analysis_time'] = time.time() - debugger.start_time
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"\nğŸ“„ ìƒì„¸ ë¶„ì„ ê²°ê³¼ê°€ {results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as save_e:
            print(f"\nâš ï¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {save_e}")
        
        return analysis_result
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        print(f"\nâŒ ì§„ë‹¨ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        return None
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        gc.collect()
        print(f"\nğŸ‘‹ ì™„ì „í•œ AI ì§„ë‹¨ ì‹œìŠ¤í…œ ì¢…ë£Œ")

if __name__ == "__main__":
    main()