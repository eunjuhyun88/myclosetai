#!/usr/bin/env python3
"""
ğŸ”¥ Ultimate AI Model Loading Debugger v4.0 - GitHub í”„ë¡œì íŠ¸ ì™„ì „ ë¶„ì„
==============================================================================
âœ… ì‹¤ì œ GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° 229GB AI ëª¨ë¸ ì™„ì „ ë¶„ì„
âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ ì›ì¸ ì™„ì „ ë¶„ì„ ë° í•´ê²°
âœ… BaseStepMixin v19.2 í˜¸í™˜ì„± ì™„ì „ ê²€ì¦
âœ… ModelLoader v5.1 ì‹¤ì œ ì‘ë™ ìƒíƒœ ê²€ì¦
âœ… StepFactory v11.0 ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ë¶„ì„
âœ… PyTorch weights_only ë¬¸ì œ í•´ê²°ì±… ì œì‹œ
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™” ìƒíƒœ í™•ì¸
âœ… ì‹¤ì œ AI Step í´ë˜ìŠ¤ë“¤ ë¡œë”© ìƒíƒœ ì™„ì „ ê²€ì¦
âœ… ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì†ìƒ ì—¬ë¶€ ì™„ì „ ê²€ì¦
âœ… ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ì„±ëŠ¥ ë¬¸ì œ ì™„ì „ ë¶„ì„
==============================================================================
"""

import sys
import os
import time
import traceback
import logging
import asyncio
import threading
import psutil
import platform
import hashlib
import json
import importlib
import inspect
import gc
import weakref
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from contextlib import contextmanager
from enum import Enum

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent.parent if Path(__file__).parent.name == "backend" else Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "backend"))

# =============================================================================
# ğŸ”¥ 1. AI ëª¨ë¸ ë¶„ì„ ë°ì´í„° í´ë˜ìŠ¤ë“¤
# =============================================================================

class CheckpointStatus(Enum):
    NOT_FOUND = "not_found"
    CORRUPTED = "corrupted" 
    LOADING_FAILED = "loading_failed"
    WEIGHTS_ONLY_FAILED = "weights_only_failed"
    DEVICE_INCOMPATIBLE = "device_incompatible"
    SUCCESS = "success"

class StepAnalysisStatus(Enum):
    IMPORT_FAILED = "import_failed"
    CLASS_NOT_FOUND = "class_not_found"
    INSTANCE_FAILED = "instance_failed"
    INIT_FAILED = "init_failed"
    DEPENDENCIES_MISSING = "dependencies_missing"
    AI_MODELS_FAILED = "ai_models_failed"
    SUCCESS = "success"

@dataclass
class CheckpointAnalysis:
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ìƒì„¸ ë¶„ì„"""
    file_path: Path
    exists: bool
    size_mb: float
    file_hash: str = ""
    
    # ë¡œë”© í…ŒìŠ¤íŠ¸ ê²°ê³¼
    pytorch_load_success: bool = False
    weights_only_success: bool = False
    safetensors_success: bool = False
    legacy_load_success: bool = False
    
    # ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ë¶„ì„
    checkpoint_keys: List[str] = field(default_factory=list)
    state_dict_structure: Dict[str, Any] = field(default_factory=dict)
    model_architecture: str = ""
    parameter_count: int = 0
    
    # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„±
    device_compatibility: Dict[str, bool] = field(default_factory=dict)
    
    # ì˜¤ë¥˜ ì •ë³´
    loading_errors: List[str] = field(default_factory=list)
    status: CheckpointStatus = CheckpointStatus.NOT_FOUND
    load_time_seconds: float = 0.0
    memory_usage_mb: float = 0.0

@dataclass
class StepAnalysis:
    """AI Step í´ë˜ìŠ¤ ìƒì„¸ ë¶„ì„"""
    step_name: str
    step_id: int
    module_path: str
    class_name: str
    
    # Import ë¶„ì„
    import_success: bool = False
    import_time: float = 0.0
    import_errors: List[str] = field(default_factory=list)
    
    # í´ë˜ìŠ¤ ë¶„ì„
    class_found: bool = False
    is_base_step_mixin: bool = False
    has_process_method: bool = False
    has_initialize_method: bool = False
    
    # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë¶„ì„
    instance_created: bool = False
    constructor_dependencies: Dict[str, Any] = field(default_factory=dict)
    instance_errors: List[str] = field(default_factory=list)
    
    # ì´ˆê¸°í™” ë¶„ì„
    initialization_success: bool = False
    initialization_time: float = 0.0
    initialization_errors: List[str] = field(default_factory=list)
    
    # ì˜ì¡´ì„± ë¶„ì„
    dependencies_resolved: Dict[str, bool] = field(default_factory=dict)
    model_loader_injected: bool = False
    memory_manager_injected: bool = False
    
    # AI ëª¨ë¸ ë¶„ì„
    ai_models_detected: List[str] = field(default_factory=list)
    checkpoints_analysis: List[CheckpointAnalysis] = field(default_factory=list)
    total_model_size_gb: float = 0.0
    
    # ì„±ëŠ¥ ë¶„ì„
    memory_footprint_mb: float = 0.0
    inference_test_success: bool = False
    inference_time_ms: float = 0.0
    
    # ì „ì²´ ìƒíƒœ
    status: StepAnalysisStatus = StepAnalysisStatus.IMPORT_FAILED
    overall_health_score: float = 0.0

@dataclass
class SystemEnvironmentAnalysis:
    """ì‹œìŠ¤í…œ í™˜ê²½ ì™„ì „ ë¶„ì„"""
    # í•˜ë“œì›¨ì–´ ì •ë³´
    cpu_info: Dict[str, Any] = field(default_factory=dict)
    memory_info: Dict[str, Any] = field(default_factory=dict)
    gpu_info: Dict[str, Any] = field(default_factory=dict)
    
    # ì†Œí”„íŠ¸ì›¨ì–´ í™˜ê²½
    python_info: Dict[str, Any] = field(default_factory=dict)
    pytorch_info: Dict[str, Any] = field(default_factory=dict)
    cuda_info: Dict[str, Any] = field(default_factory=dict)
    
    # í”„ë¡œì íŠ¸ í™˜ê²½
    project_structure: Dict[str, Any] = field(default_factory=dict)
    conda_environment: Dict[str, Any] = field(default_factory=dict)
    dependencies_status: Dict[str, bool] = field(default_factory=dict)
    
    # ì§„ë‹¨ ê²°ê³¼
    is_m3_max: bool = False
    memory_sufficient: bool = False
    cuda_available: bool = False
    mps_available: bool = False
    recommended_device: str = "cpu"

# =============================================================================
# ğŸ”¥ 2. ì•ˆì „ ì‹¤í–‰ ë§¤ë‹ˆì €
# =============================================================================

class UltimateSafetyManager:
    """ê°•í™”ëœ ì•ˆì „ ì‹¤í–‰ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.timeout_duration = 120  # 2ë¶„ íƒ€ì„ì•„ì›ƒ
        self.max_memory_gb = 8      # 8GB ë©”ëª¨ë¦¬ ì œí•œ
        self.active_operations = {}
        self.start_time = time.time()
        
    @contextmanager
    def safe_execution(self, operation_name: str, timeout: int = None, memory_limit_gb: float = None):
        """ì´ˆì•ˆì „ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸"""
        operation_id = f"{operation_name}_{int(time.time() * 1000)}"
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / (1024**3)  # GB ë‹¨ìœ„
        timeout = timeout or self.timeout_duration
        memory_limit = memory_limit_gb or self.max_memory_gb
        
        print(f"ğŸ”’ [{operation_id}] ì•ˆì „ ì‹¤í–‰ ì‹œì‘ (íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ, ë©”ëª¨ë¦¬ ì œí•œ: {memory_limit:.1f}GB)")
        
        self.active_operations[operation_id] = {
            'start_time': start_time,
            'start_memory': start_memory,
            'timeout': timeout,
            'memory_limit': memory_limit
        }
        
        try:
            # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘
            monitoring_thread = threading.Thread(
                target=self._monitor_operation,
                args=(operation_id, timeout, memory_limit),
                daemon=True
            )
            monitoring_thread.start()
            
            yield
            
        except TimeoutError:
            print(f"â° [{operation_id}] íƒ€ì„ì•„ì›ƒ ë°œìƒ ({timeout}ì´ˆ)")
            raise
        except MemoryError:
            print(f"ğŸ’¾ [{operation_id}] ë©”ëª¨ë¦¬ í•œê³„ ì´ˆê³¼ ({memory_limit:.1f}GB)")
            raise
        except Exception as e:
            print(f"âŒ [{operation_id}] ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {type(e).__name__}: {e}")
            if hasattr(e, '__traceback__'):
                tb_lines = traceback.format_tb(e.__traceback__)
                if tb_lines:
                    print(f"   ìŠ¤íƒ ì¶”ì : {tb_lines[-1].strip()}")
            raise
        finally:
            # ì •ë¦¬ ì‘ì—…
            if operation_id in self.active_operations:
                del self.active_operations[operation_id]
                
            elapsed = time.time() - start_time
            end_memory = psutil.Process().memory_info().rss / (1024**3)
            memory_used = end_memory - start_memory
            
            print(f"âœ… [{operation_id}] ì™„ë£Œ ({elapsed:.2f}ì´ˆ, ë©”ëª¨ë¦¬: +{memory_used:.2f}GB)")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if memory_used > 0.5:  # 500MB ì´ìƒ ì‚¬ìš©ì‹œ ì •ë¦¬
                gc.collect()
    
    def _monitor_operation(self, operation_id: str, timeout: float, memory_limit: float):
        """ì‘ì—… ëª¨ë‹ˆí„°ë§"""
        try:
            while operation_id in self.active_operations:
                current_time = time.time()
                operation = self.active_operations.get(operation_id)
                
                if not operation:
                    break
                
                # íƒ€ì„ì•„ì›ƒ ì²´í¬
                elapsed = current_time - operation['start_time']
                if elapsed > timeout:
                    print(f"âš ï¸ [{operation_id}] íƒ€ì„ì•„ì›ƒ ê²½ê³  ({elapsed:.1f}ì´ˆ/{timeout}ì´ˆ)")
                    break
                
                # ë©”ëª¨ë¦¬ ì²´í¬
                current_memory = psutil.Process().memory_info().rss / (1024**3)
                if current_memory > memory_limit:
                    print(f"âš ï¸ [{operation_id}] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²½ê³  ({current_memory:.1f}GB/{memory_limit:.1f}GB)")
                    break
                
                time.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì²´í¬
                
        except Exception:
            pass  # ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œì—ì„œëŠ” ì˜ˆì™¸ ë¬´ì‹œ

# ì „ì—­ ì•ˆì „ ë§¤ë‹ˆì €
safety_manager = UltimateSafetyManager()

# =============================================================================
# ğŸ”¥ 3. ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„ê¸°
# =============================================================================

class SystemEnvironmentAnalyzer:
    """ì‹œìŠ¤í…œ í™˜ê²½ ì™„ì „ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.analysis_result = SystemEnvironmentAnalysis()
        
    def analyze_complete_environment(self) -> SystemEnvironmentAnalysis:
        """ì™„ì „í•œ ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„"""
        
        print("ğŸ“Š ì‹œìŠ¤í…œ í™˜ê²½ ì™„ì „ ë¶„ì„ ì‹œì‘...")
        
        with safety_manager.safe_execution("ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„", timeout=60):
            self._analyze_hardware()
            self._analyze_software()
            self._analyze_project_structure()
            self._analyze_dependencies()
            self._make_recommendations()
        
        return self.analysis_result
    
    def _analyze_hardware(self):
        """í•˜ë“œì›¨ì–´ ë¶„ì„"""
        try:
            # CPU ì •ë³´
            self.analysis_result.cpu_info = {
                'physical_cores': psutil.cpu_count(logical=False),
                'logical_cores': psutil.cpu_count(logical=True),
                'usage_percent': psutil.cpu_percent(interval=1),
                'architecture': platform.machine(),
                'processor': platform.processor(),
                'is_apple_silicon': platform.machine() == 'arm64' and platform.system() == 'Darwin'
            }
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            memory = psutil.virtual_memory()
            self.analysis_result.memory_info = {
                'total_gb': memory.total / (1024**3),
                'available_gb': memory.available / (1024**3),
                'used_gb': memory.used / (1024**3),
                'usage_percent': memory.percent,
                'sufficient_for_ai': memory.total >= 16 * (1024**3)  # 16GB ì´ìƒ
            }
            
            # M3 Max ê°ì§€
            if self.analysis_result.cpu_info['is_apple_silicon']:
                total_memory = self.analysis_result.memory_info['total_gb']
                if total_memory >= 100:  # 128GB ëª¨ë¸
                    self.analysis_result.is_m3_max = True
            
            self.analysis_result.memory_sufficient = self.analysis_result.memory_info['available_gb'] >= 8
            
        except Exception as e:
            print(f"âŒ í•˜ë“œì›¨ì–´ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_software(self):
        """ì†Œí”„íŠ¸ì›¨ì–´ í™˜ê²½ ë¶„ì„"""
        try:
            # Python ì •ë³´
            self.analysis_result.python_info = {
                'version': sys.version.split()[0],
                'executable': sys.executable,
                'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
                'virtual_env': os.environ.get('VIRTUAL_ENV', 'none'),
                'platform': platform.platform()
            }
            
            # PyTorch ë¶„ì„
            try:
                import torch
                self.analysis_result.pytorch_info = {
                    'available': True,
                    'version': torch.__version__,
                    'cuda_available': torch.cuda.is_available(),
                    'cuda_version': torch.version.cuda if torch.cuda.is_available() else None,
                    'mps_available': hasattr(torch.backends, 'mps') and torch.backends.mps.is_available(),
                    'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0
                }
                
                # ì¶”ì²œ ë””ë°”ì´ìŠ¤ ê²°ì •
                if self.analysis_result.pytorch_info['mps_available']:
                    self.analysis_result.recommended_device = 'mps'
                    self.analysis_result.mps_available = True
                elif self.analysis_result.pytorch_info['cuda_available']:
                    self.analysis_result.recommended_device = 'cuda'
                    self.analysis_result.cuda_available = True
                else:
                    self.analysis_result.recommended_device = 'cpu'
                    
            except ImportError:
                self.analysis_result.pytorch_info = {
                    'available': False,
                    'error': 'PyTorch not installed'
                }
            
            # Conda í™˜ê²½ ë¶„ì„
            conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'none')
            self.analysis_result.conda_environment = {
                'active_env': conda_env,
                'conda_available': conda_env != 'none',
                'env_path': os.environ.get('CONDA_PREFIX', ''),
                'python_path': sys.executable
            }
            
        except Exception as e:
            print(f"âŒ ì†Œí”„íŠ¸ì›¨ì–´ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_project_structure(self):
        """í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„"""
        try:
            structure = {
                'project_root': str(project_root),
                'backend_exists': (project_root / 'backend').exists(),
                'frontend_exists': (project_root / 'frontend').exists(),
                'ai_models_dir': None,
                'ai_models_size_gb': 0.0,
                'step_modules': []
            }
            
            # AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
            possible_ai_dirs = [
                project_root / 'ai_models',
                project_root / 'backend' / 'ai_models',
                project_root / 'models'
            ]
            
            for ai_dir in possible_ai_dirs:
                if ai_dir.exists():
                    structure['ai_models_dir'] = str(ai_dir)
                    # í¬ê¸° ê³„ì‚°
                    total_size = 0
                    for file_path in ai_dir.rglob('*'):
                        if file_path.is_file():
                            total_size += file_path.stat().st_size
                    structure['ai_models_size_gb'] = total_size / (1024**3)
                    break
            
            # Step ëª¨ë“ˆ ì°¾ê¸°
            steps_dir = project_root / 'backend' / 'app' / 'ai_pipeline' / 'steps'
            if steps_dir.exists():
                for step_file in steps_dir.glob('step_*.py'):
                    structure['step_modules'].append(step_file.stem)
            
            self.analysis_result.project_structure = structure
            
        except Exception as e:
            print(f"âŒ í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_dependencies(self):
        """ì˜ì¡´ì„± ë¶„ì„"""
        try:
            dependencies = {}
            
            # í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬
            core_libs = ['torch', 'torchvision', 'numpy', 'PIL', 'cv2', 'transformers', 'safetensors']
            
            for lib in core_libs:
                try:
                    module = importlib.import_module(lib if lib != 'PIL' else 'PIL.Image')
                    dependencies[lib] = True
                except ImportError:
                    dependencies[lib] = False
            
            self.analysis_result.dependencies_status = dependencies
            
        except Exception as e:
            print(f"âŒ ì˜ì¡´ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _make_recommendations(self):
        """í™˜ê²½ ê°œì„  ì¶”ì²œì‚¬í•­ ìƒì„±"""
        # ì‹œìŠ¤í…œ ìƒíƒœì— ë”°ë¥¸ ì¶”ì²œì‚¬í•­ì€ ë‚˜ì¤‘ì— ì „ì²´ ë¶„ì„ì—ì„œ ì²˜ë¦¬
        pass

# =============================================================================
# ğŸ”¥ 4. ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ê¸°
# =============================================================================

class CheckpointAnalyzer:
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì™„ì „ ë¶„ì„ê¸°"""
    
    def __init__(self, device: str = 'cpu'):
        self.device = device
        self.torch_available = False
        self.safetensors_available = False
        
        try:
            import torch
            self.torch_available = True
            self.torch = torch
        except ImportError:
            pass
            
        try:
            from safetensors.torch import load_file
            self.safetensors_available = True
            self.safetensors_load = load_file
        except ImportError:
            pass
    
    def analyze_checkpoint(self, checkpoint_path: Path) -> CheckpointAnalysis:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì™„ì „ ë¶„ì„"""
        
        analysis = CheckpointAnalysis(
            file_path=checkpoint_path,
            exists=checkpoint_path.exists(),
            size_mb=0.0
        )
        
        if not analysis.exists:
            analysis.status = CheckpointStatus.NOT_FOUND
            return analysis
        
        # íŒŒì¼ í¬ê¸° ë° í•´ì‹œ
        try:
            stat_info = checkpoint_path.stat()
            analysis.size_mb = stat_info.st_size / (1024 * 1024)
            
            # í•´ì‹œ ê³„ì‚° (í° íŒŒì¼ì€ ìƒ˜í”Œë§)
            if analysis.size_mb < 100:  # 100MB ë¯¸ë§Œë§Œ ì „ì²´ í•´ì‹œ
                analysis.file_hash = self._calculate_file_hash(checkpoint_path)
            else:
                analysis.file_hash = self._calculate_sample_hash(checkpoint_path)
                
        except Exception as e:
            analysis.loading_errors.append(f"íŒŒì¼ ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {e}")
        
        # ë¡œë”© í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
        if self.torch_available:
            self._test_pytorch_loading(analysis)
        
        if self.safetensors_available and checkpoint_path.suffix == '.safetensors':
            self._test_safetensors_loading(analysis)
        
        # ìƒíƒœ ê²°ì •
        if analysis.pytorch_load_success or analysis.safetensors_success:
            analysis.status = CheckpointStatus.SUCCESS
        elif analysis.loading_errors:
            if any("corrupted" in error.lower() for error in analysis.loading_errors):
                analysis.status = CheckpointStatus.CORRUPTED
            elif any("weights_only" in error.lower() for error in analysis.loading_errors):
                analysis.status = CheckpointStatus.WEIGHTS_ONLY_FAILED
            else:
                analysis.status = CheckpointStatus.LOADING_FAILED
        
        return analysis
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """ì „ì²´ íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception:
            return ""
    
    def _calculate_sample_hash(self, file_path: Path, sample_size: int = 1024*1024) -> str:
        """ìƒ˜í”Œ í•´ì‹œ ê³„ì‚° (ëŒ€ìš©ëŸ‰ íŒŒì¼ìš©)"""
        try:
            hash_md5 = hashlib.md5()
            file_size = file_path.stat().st_size
            
            with open(file_path, "rb") as f:
                # ì‹œì‘ ë¶€ë¶„
                chunk = f.read(sample_size)
                hash_md5.update(chunk)
                
                # ì¤‘ê°„ ë¶€ë¶„
                if file_size > sample_size * 3:
                    f.seek(file_size // 2)
                    chunk = f.read(sample_size)
                    hash_md5.update(chunk)
                
                # ë ë¶€ë¶„
                if file_size > sample_size * 2:
                    f.seek(file_size - sample_size)
                    chunk = f.read(sample_size)
                    hash_md5.update(chunk)
            
            return hash_md5.hexdigest()
        except Exception:
            return ""
    
    def _test_pytorch_loading(self, analysis: CheckpointAnalysis):
        """PyTorch ë¡œë”© í…ŒìŠ¤íŠ¸"""
        if not self.torch_available:
            analysis.loading_errors.append("PyTorch ì—†ìŒ")
            return
        
        file_path = analysis.file_path
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / (1024**2)
        
        # 1. weights_only=True ì‹œë„
        try:
            with safety_manager.safe_execution(f"PyTorch weights_only ë¡œë”© {file_path.name}", timeout=60):
                checkpoint = self.torch.load(file_path, map_location=self.device, weights_only=True)
                analysis.weights_only_success = True
                self._analyze_checkpoint_content(analysis, checkpoint)
                
        except Exception as e:
            analysis.loading_errors.append(f"weights_only ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # 2. weights_only=False ì‹œë„
        if not analysis.weights_only_success:
            try:
                with safety_manager.safe_execution(f"PyTorch ì¼ë°˜ ë¡œë”© {file_path.name}", timeout=60):
                    checkpoint = self.torch.load(file_path, map_location=self.device, weights_only=False)
                    analysis.pytorch_load_success = True
                    self._analyze_checkpoint_content(analysis, checkpoint)
                    
            except Exception as e:
                analysis.loading_errors.append(f"ì¼ë°˜ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # 3. ë ˆê±°ì‹œ ë¡œë”© ì‹œë„
        if not analysis.pytorch_load_success and not analysis.weights_only_success:
            try:
                with safety_manager.safe_execution(f"PyTorch ë ˆê±°ì‹œ ë¡œë”© {file_path.name}", timeout=60):
                    checkpoint = self.torch.load(file_path, map_location=self.device)
                    analysis.legacy_load_success = True
                    analysis.pytorch_load_success = True
                    self._analyze_checkpoint_content(analysis, checkpoint)
                    
            except Exception as e:
                analysis.loading_errors.append(f"ë ˆê±°ì‹œ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # ì„±ëŠ¥ ì¸¡ì •
        analysis.load_time_seconds = time.time() - start_time
        end_memory = psutil.Process().memory_info().rss / (1024**2)
        analysis.memory_usage_mb = end_memory - start_memory
    
    def _test_safetensors_loading(self, analysis: CheckpointAnalysis):
        """SafeTensors ë¡œë”© í…ŒìŠ¤íŠ¸"""
        if not self.safetensors_available:
            analysis.loading_errors.append("SafeTensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
            return
        
        try:
            with safety_manager.safe_execution(f"SafeTensors ë¡œë”© {analysis.file_path.name}", timeout=60):
                checkpoint = self.safetensors_load(str(analysis.file_path))
                analysis.safetensors_success = True
                self._analyze_checkpoint_content(analysis, checkpoint)
                
        except Exception as e:
            analysis.loading_errors.append(f"SafeTensors ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _analyze_checkpoint_content(self, analysis: CheckpointAnalysis, checkpoint):
        """ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ë¶„ì„"""
        try:
            # State dict ì¶”ì¶œ
            state_dict = checkpoint
            if isinstance(checkpoint, dict):
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
            
            if isinstance(state_dict, dict):
                analysis.checkpoint_keys = list(state_dict.keys())[:50]  # ì²˜ìŒ 50ê°œë§Œ
                
                # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
                param_count = 0
                for key, tensor in state_dict.items():
                    if hasattr(tensor, 'numel'):
                        param_count += tensor.numel()
                analysis.parameter_count = param_count
                
                # ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¶”ì •
                analysis.model_architecture = self._estimate_architecture(state_dict)
                
                # êµ¬ì¡° ì •ë³´
                analysis.state_dict_structure = {
                    'total_keys': len(state_dict),
                    'tensor_keys': sum(1 for v in state_dict.values() if hasattr(v, 'shape')),
                    'parameter_count': param_count,
                    'estimated_size_mb': param_count * 4 / (1024**2) if param_count > 0 else 0  # float32 ê°€ì •
                }
                
                # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
                self._test_device_compatibility(analysis, state_dict)
            
        except Exception as e:
            analysis.loading_errors.append(f"ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _estimate_architecture(self, state_dict: dict) -> str:
        """ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¶”ì •"""
        keys = list(state_dict.keys())
        key_str = ' '.join(keys).lower()
        
        if 'backbone' in key_str:
            return "Segmentation Model (with backbone)"
        elif 'pose' in key_str or 'keypoint' in key_str:
            return "Pose Estimation Model"
        elif 'diffusion' in key_str or 'unet' in key_str:
            return "Diffusion Model"
        elif 'vit' in key_str or 'transformer' in key_str:
            return "Vision Transformer"
        elif 'resnet' in key_str or 'efficientnet' in key_str:
            return "CNN Backbone"
        elif 'sam' in key_str or 'segment' in key_str:
            return "Segmentation Model"
        elif any(keyword in key_str for keyword in ['conv', 'bn', 'relu']):
            return "Convolutional Neural Network"
        else:
            return "Unknown Architecture"
    
    def _test_device_compatibility(self, analysis: CheckpointAnalysis, state_dict: dict):
        """ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        if not self.torch_available:
            return
        
        try:
            # ì²« ë²ˆì§¸ í…ì„œë¡œ í…ŒìŠ¤íŠ¸
            first_tensor = None
            for value in state_dict.values():
                if hasattr(value, 'to'):
                    first_tensor = value
                    break
            
            if first_tensor is None:
                return
            
            # CPU í…ŒìŠ¤íŠ¸
            try:
                cpu_tensor = first_tensor.to('cpu')
                analysis.device_compatibility['cpu'] = True
            except Exception:
                analysis.device_compatibility['cpu'] = False
            
            # CUDA í…ŒìŠ¤íŠ¸
            if self.torch.cuda.is_available():
                try:
                    cuda_tensor = first_tensor.to('cuda')
                    analysis.device_compatibility['cuda'] = True
                except Exception:
                    analysis.device_compatibility['cuda'] = False
            
            # MPS í…ŒìŠ¤íŠ¸
            if hasattr(self.torch.backends, 'mps') and self.torch.backends.mps.is_available():
                try:
                    mps_tensor = first_tensor.to('mps')
                    analysis.device_compatibility['mps'] = True
                except Exception:
                    analysis.device_compatibility['mps'] = False
            
        except Exception as e:
            analysis.loading_errors.append(f"ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 5. AI Step ë¶„ì„ê¸°
# =============================================================================

class AIStepAnalyzer:
    """AI Step í´ë˜ìŠ¤ ì™„ì „ ë¶„ì„ê¸°"""
    
    def __init__(self, system_analysis: SystemEnvironmentAnalysis):
        self.system_analysis = system_analysis
        self.checkpoint_analyzer = CheckpointAnalyzer(
            device=system_analysis.recommended_device
        )
    
    def analyze_step(self, step_config: Dict[str, Any]) -> StepAnalysis:
        """AI Step ì™„ì „ ë¶„ì„"""
        
        analysis = StepAnalysis(
            step_name=step_config['step_name'],
            step_id=step_config.get('step_id', 0),
            module_path=step_config['module_path'],
            class_name=step_config['class_name']
        )
        
        print(f"\nğŸ”§ {analysis.step_name} ì™„ì „ ë¶„ì„ ì‹œì‘...")
        
        # 1. Import í…ŒìŠ¤íŠ¸
        self._test_import(analysis)
        
        # 2. í´ë˜ìŠ¤ ë¶„ì„
        if analysis.import_success:
            self._analyze_class(analysis)
        
        # 3. ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
        if analysis.class_found:
            self._test_instance_creation(analysis)
        
        # 4. ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        if analysis.instance_created:
            self._test_initialization(analysis)
        
        # 5. AI ëª¨ë¸ ë¶„ì„
        self._analyze_ai_models(analysis)
        
        # 6. ìƒíƒœ ê²°ì • ë° ì ìˆ˜ ê³„ì‚°
        self._determine_status_and_score(analysis)
        
        return analysis
    
    def _test_import(self, analysis: StepAnalysis):
        """Import í…ŒìŠ¤íŠ¸"""
        try:
            with safety_manager.safe_execution(f"{analysis.step_name} Import", timeout=30):
                start_time = time.time()
                module = importlib.import_module(analysis.module_path)
                analysis.import_time = time.time() - start_time
                analysis.import_success = True
                
                # í´ë˜ìŠ¤ ì¡´ì¬ í™•ì¸
                if hasattr(module, analysis.class_name):
                    analysis.class_found = True
                    
        except Exception as e:
            analysis.import_errors.append(str(e))
            analysis.status = StepAnalysisStatus.IMPORT_FAILED
    
    def _analyze_class(self, analysis: StepAnalysis):
        """í´ë˜ìŠ¤ êµ¬ì¡° ë¶„ì„"""
        try:
            module = importlib.import_module(analysis.module_path)
            step_class = getattr(module, analysis.class_name)
            
            # í´ë˜ìŠ¤ ë©”ì„œë“œ ê²€ì‚¬
            class_methods = [method for method in dir(step_class) if not method.startswith('_')]
            
            analysis.has_process_method = 'process' in class_methods
            analysis.has_initialize_method = 'initialize' in class_methods
            
            # BaseStepMixin ìƒì† í™•ì¸
            mro = inspect.getmro(step_class)
            analysis.is_base_step_mixin = any('BaseStepMixin' in cls.__name__ for cls in mro)
            
            print(f"   âœ… í´ë˜ìŠ¤ ë¶„ì„ ì™„ë£Œ: process={analysis.has_process_method}, init={analysis.has_initialize_method}")
            
        except Exception as e:
            analysis.import_errors.append(f"í´ë˜ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _test_instance_creation(self, analysis: StepAnalysis):
        """ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸"""
        try:
            with safety_manager.safe_execution(f"{analysis.step_name} ì¸ìŠ¤í„´ìŠ¤ ìƒì„±", timeout=60):
                module = importlib.import_module(analysis.module_path)
                step_class = getattr(module, analysis.class_name)
                
                # ìƒì„±ì íŒŒë¼ë¯¸í„° ë¶„ì„
                signature = inspect.signature(step_class.__init__)
                params = list(signature.parameters.keys())[1:]  # self ì œì™¸
                
                # ê¸°ë³¸ ì˜ì¡´ì„± ì¤€ë¹„
                constructor_args = {
                    'device': self.system_analysis.recommended_device,
                    'strict_mode': False
                }
                
                # í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ì˜ì¡´ì„±
                if 'model_loader' in params:
                    constructor_args['model_loader'] = None  # Mockìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥
                
                analysis.constructor_dependencies = constructor_args
                
                # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                step_instance = step_class(**constructor_args)
                analysis.instance_created = True
                
                # ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ í™•ì¸
                if hasattr(step_instance, 'model_loader'):
                    analysis.model_loader_injected = step_instance.model_loader is not None
                
                if hasattr(step_instance, 'memory_manager'):
                    analysis.memory_manager_injected = step_instance.memory_manager is not None
                
                print(f"   âœ… ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ")
                
        except Exception as e:
            analysis.instance_errors.append(str(e))
            analysis.status = StepAnalysisStatus.INSTANCE_FAILED
    
    def _test_initialization(self, analysis: StepAnalysis):
        """ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        if not analysis.instance_created:
            return
        
        try:
            with safety_manager.safe_execution(f"{analysis.step_name} ì´ˆê¸°í™”", timeout=90):
                module = importlib.import_module(analysis.module_path)
                step_class = getattr(module, analysis.class_name)
                step_instance = step_class(**analysis.constructor_dependencies)
                
                start_time = time.time()
                
                if hasattr(step_instance, 'initialize'):
                    if asyncio.iscoroutinefunction(step_instance.initialize):
                        # ë¹„ë™ê¸° ì´ˆê¸°í™”
                        try:
                            loop = asyncio.get_event_loop()
                        except RuntimeError:
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                        
                        result = loop.run_until_complete(
                            asyncio.wait_for(step_instance.initialize(), timeout=60.0)
                        )
                    else:
                        # ë™ê¸° ì´ˆê¸°í™”
                        result = step_instance.initialize()
                    
                    if result:
                        analysis.initialization_success = True
                        analysis.initialization_time = time.time() - start_time
                        print(f"   âœ… ì´ˆê¸°í™” ì„±ê³µ ({analysis.initialization_time:.2f}ì´ˆ)")
                    else:
                        analysis.initialization_errors.append("ì´ˆê¸°í™”ê°€ False ë°˜í™˜")
                        
                else:
                    # initialize ë©”ì„œë“œê°€ ì—†ëŠ” ê²½ìš°
                    analysis.initialization_success = True
                    print(f"   âš ï¸ initialize ë©”ì„œë“œ ì—†ìŒ (ê¸°ë³¸ ì„±ê³µ ì²˜ë¦¬)")
                    
        except TimeoutError:
            analysis.initialization_errors.append("ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ (60ì´ˆ)")
        except Exception as e:
            analysis.initialization_errors.append(str(e))
            analysis.status = StepAnalysisStatus.INIT_FAILED
    
    def _analyze_ai_models(self, analysis: StepAnalysis):
        """AI ëª¨ë¸ íŒŒì¼ ë¶„ì„"""
        try:
            # Step ID ê¸°ë°˜ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
            ai_models_base = self.system_analysis.project_structure.get('ai_models_dir')
            if not ai_models_base:
                return
                
            ai_models_path = Path(ai_models_base)
            
            # Stepë³„ ëª¨ë¸ ë””ë ‰í† ë¦¬ íŒ¨í„´
            step_patterns = [
                f"step_{analysis.step_id:02d}_*",
                f"*{analysis.step_name.lower().replace('step', '')}*",
                analysis.step_name.lower()
            ]
            
            model_files = []
            
            for pattern in step_patterns:
                matching_dirs = list(ai_models_path.glob(pattern))
                for model_dir in matching_dirs:
                    if model_dir.is_dir():
                        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸°
                        for ext in ['*.pth', '*.pt', '*.safetensors', '*.bin']:
                            found_files = list(model_dir.rglob(ext))
                            model_files.extend(found_files)
            
            # ëª¨ë¸ íŒŒì¼ ë¶„ì„
            total_size = 0
            for model_file in model_files[:10]:  # ìµœëŒ€ 10ê°œë§Œ
                if model_file.stat().st_size > 10 * 1024 * 1024:  # 10MB ì´ìƒë§Œ
                    checkpoint_analysis = self.checkpoint_analyzer.analyze_checkpoint(model_file)
                    analysis.checkpoints_analysis.append(checkpoint_analysis)
                    analysis.ai_models_detected.append(model_file.name)
                    total_size += checkpoint_analysis.size_mb
            
            analysis.total_model_size_gb = total_size / 1024
            
            if analysis.ai_models_detected:
                print(f"   ğŸ“Š AI ëª¨ë¸ {len(analysis.ai_models_detected)}ê°œ ë°œê²¬ ({analysis.total_model_size_gb:.1f}GB)")
            
        except Exception as e:
            print(f"   âš ï¸ AI ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _determine_status_and_score(self, analysis: StepAnalysis):
        """ìƒíƒœ ê²°ì • ë° ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # Import (20ì )
        if analysis.import_success:
            score += 20
        
        # í´ë˜ìŠ¤ êµ¬ì¡° (20ì )
        if analysis.class_found:
            score += 10
        if analysis.is_base_step_mixin:
            score += 5
        if analysis.has_process_method:
            score += 3
        if analysis.has_initialize_method:
            score += 2
        
        # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (20ì )
        if analysis.instance_created:
            score += 20
        
        # ì´ˆê¸°í™” (20ì )
        if analysis.initialization_success:
            score += 20
        
        # AI ëª¨ë¸ (20ì )
        if analysis.ai_models_detected:
            score += 10
            successful_checkpoints = sum(
                1 for cp in analysis.checkpoints_analysis 
                if cp.status == CheckpointStatus.SUCCESS
            )
            if successful_checkpoints > 0:
                score += 10
        
        analysis.overall_health_score = score
        
        # ìƒíƒœ ê²°ì •
        if not analysis.import_success:
            analysis.status = StepAnalysisStatus.IMPORT_FAILED
        elif not analysis.class_found:
            analysis.status = StepAnalysisStatus.CLASS_NOT_FOUND
        elif not analysis.instance_created:
            analysis.status = StepAnalysisStatus.INSTANCE_FAILED
        elif not analysis.initialization_success:
            analysis.status = StepAnalysisStatus.INIT_FAILED
        elif not analysis.ai_models_detected:
            analysis.status = StepAnalysisStatus.AI_MODELS_FAILED
        else:
            analysis.status = StepAnalysisStatus.SUCCESS

# =============================================================================
# ğŸ”¥ 6. ë©”ì¸ ë””ë²„ê¹… ì‹œìŠ¤í…œ
# =============================================================================

class UltimateAIModelDebugger:
    """ìµœê³ ê¸‰ AI ëª¨ë¸ ë””ë²„ê¹… ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.start_time = time.time()
        self.system_analysis = None
        self.step_analyses = {}
        
        # GitHub í”„ë¡œì íŠ¸ Step ì„¤ì •
        self.step_configs = [
            {
                'step_name': 'HumanParsingStep',
                'step_id': 1,
                'module_path': 'app.ai_pipeline.steps.step_01_human_parsing',
                'class_name': 'HumanParsingStep'
            },
            {
                'step_name': 'PoseEstimationStep',
                'step_id': 2,
                'module_path': 'app.ai_pipeline.steps.step_02_pose_estimation',
                'class_name': 'PoseEstimationStep'
            },
            {
                'step_name': 'ClothSegmentationStep',
                'step_id': 3,
                'module_path': 'app.ai_pipeline.steps.step_03_cloth_segmentation',
                'class_name': 'ClothSegmentationStep'
            },
            {
                'step_name': 'GeometricMatchingStep',
                'step_id': 4,
                'module_path': 'app.ai_pipeline.steps.step_04_geometric_matching',
                'class_name': 'GeometricMatchingStep'
            },
            {
                'step_name': 'ClothWarpingStep',
                'step_id': 5,
                'module_path': 'app.ai_pipeline.steps.step_05_cloth_warping',
                'class_name': 'ClothWarpingStep'
            },
            {
                'step_name': 'VirtualFittingStep',
                'step_id': 6,
                'module_path': 'app.ai_pipeline.steps.step_06_virtual_fitting',
                'class_name': 'VirtualFittingStep'
            },
            {
                'step_name': 'PostProcessingStep',
                'step_id': 7,
                'module_path': 'app.ai_pipeline.steps.step_07_post_processing',
                'class_name': 'PostProcessingStep'
            },
            {
                'step_name': 'QualityAssessmentStep',
                'step_id': 8,
                'module_path': 'app.ai_pipeline.steps.step_08_quality_assessment',
                'class_name': 'QualityAssessmentStep'
            }
        ]
    
    def run_ultimate_debugging(self) -> Dict[str, Any]:
        """ìµœê³ ê¸‰ ë””ë²„ê¹… ì‹¤í–‰"""
        
        print("ğŸ”¥" * 30)
        print("ğŸ”¥ Ultimate AI Model Loading Debugger v4.0 ì‹œì‘")
        print("ğŸ”¥ GitHub í”„ë¡œì íŠ¸ 229GB AI ëª¨ë¸ ì™„ì „ ë¶„ì„")
        print("ğŸ”¥" * 30)
        
        debug_result = {
            'timestamp': time.time(),
            'debug_version': '4.0',
            'system_analysis': {},
            'step_analyses': {},
            'overall_summary': {},
            'critical_issues': [],
            'recommendations': [],
            'performance_metrics': {}
        }
        
        try:
            # 1. ì‹œìŠ¤í…œ í™˜ê²½ ì™„ì „ ë¶„ì„
            print("\nğŸ“Š 1. ì‹œìŠ¤í…œ í™˜ê²½ ì™„ì „ ë¶„ì„")
            self.system_analysis = SystemEnvironmentAnalyzer().analyze_complete_environment()
            debug_result['system_analysis'] = self._serialize_system_analysis(self.system_analysis)
            self._print_system_analysis()
            
            # 2. AI Stepë³„ ì™„ì „ ë¶„ì„
            print("\nğŸš€ 2. AI Stepë³„ ì™„ì „ ë¶„ì„")
            step_analyzer = AIStepAnalyzer(self.system_analysis)
            
            for step_config in self.step_configs:
                try:
                    step_analysis = step_analyzer.analyze_step(step_config)
                    self.step_analyses[step_config['step_name']] = step_analysis
                    debug_result['step_analyses'][step_config['step_name']] = self._serialize_step_analysis(step_analysis)
                    
                except Exception as e:
                    print(f"âŒ {step_config['step_name']} ë¶„ì„ ì‹¤íŒ¨: {e}")
                    
            # 3. ì „ì²´ ìš”ì•½ ìƒì„±
            print("\nğŸ“Š 3. ì „ì²´ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
            debug_result['overall_summary'] = self._generate_overall_summary()
            debug_result['critical_issues'] = self._identify_critical_issues()
            debug_result['recommendations'] = self._generate_actionable_recommendations()
            debug_result['performance_metrics'] = self._calculate_performance_metrics()
            
            # 4. ê²°ê³¼ ì¶œë ¥
            self._print_debug_results(debug_result)
            
            # 5. ê²°ê³¼ ì €ì¥
            self._save_debug_results(debug_result)
            
        except Exception as e:
            print(f"\nâŒ ë””ë²„ê¹… ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            debug_result['fatal_error'] = str(e)
        
        finally:
            total_time = time.time() - self.start_time
            print(f"\nğŸ‰ Ultimate AI Model Debugging ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
            debug_result['total_debug_time'] = total_time
        
        return debug_result
    
    def _serialize_system_analysis(self, analysis: SystemEnvironmentAnalysis) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë¶„ì„ ê²°ê³¼ ì§ë ¬í™”"""
        return {
            'cpu_info': analysis.cpu_info,
            'memory_info': analysis.memory_info,
            'pytorch_info': analysis.pytorch_info,
            'project_structure': analysis.project_structure,
            'dependencies_status': analysis.dependencies_status,
            'recommendations': {
                'is_m3_max': analysis.is_m3_max,
                'memory_sufficient': analysis.memory_sufficient,
                'recommended_device': analysis.recommended_device
            }
        }
    
    def _serialize_step_analysis(self, analysis: StepAnalysis) -> Dict[str, Any]:
        """Step ë¶„ì„ ê²°ê³¼ ì§ë ¬í™”"""
        return {
            'basic_info': {
                'step_name': analysis.step_name,
                'step_id': analysis.step_id,
                'module_path': analysis.module_path,
                'class_name': analysis.class_name
            },
            'import_analysis': {
                'success': analysis.import_success,
                'time': analysis.import_time,
                'errors': analysis.import_errors
            },
            'class_analysis': {
                'found': analysis.class_found,
                'is_base_step_mixin': analysis.is_base_step_mixin,
                'has_process_method': analysis.has_process_method,
                'has_initialize_method': analysis.has_initialize_method
            },
            'instance_analysis': {
                'created': analysis.instance_created,
                'dependencies': analysis.constructor_dependencies,
                'errors': analysis.instance_errors
            },
            'initialization': {
                'success': analysis.initialization_success,
                'time': analysis.initialization_time,
                'errors': analysis.initialization_errors
            },
            'ai_models': {
                'detected': analysis.ai_models_detected,
                'total_size_gb': analysis.total_model_size_gb,
                'checkpoint_count': len(analysis.checkpoints_analysis),
                'successful_checkpoints': sum(
                    1 for cp in analysis.checkpoints_analysis 
                    if cp.status == CheckpointStatus.SUCCESS
                )
            },
            'performance': {
                'memory_footprint_mb': analysis.memory_footprint_mb,
                'health_score': analysis.overall_health_score
            },
            'status': analysis.status.value
        }
    
    def _print_system_analysis(self):
        """ì‹œìŠ¤í…œ ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        analysis = self.system_analysis
        
        print(f"   ğŸ’» í•˜ë“œì›¨ì–´:")
        print(f"      CPU: {analysis.cpu_info.get('logical_cores', 0)}ì½”ì–´ ({analysis.cpu_info.get('architecture', 'unknown')})")
        print(f"      ë©”ëª¨ë¦¬: {analysis.memory_info.get('available_gb', 0):.1f}GB ì‚¬ìš©ê°€ëŠ¥ / {analysis.memory_info.get('total_gb', 0):.1f}GB ì´ëŸ‰")
        print(f"      M3 Max: {'âœ…' if analysis.is_m3_max else 'âŒ'}")
        
        print(f"   ğŸ”¥ AI í™˜ê²½:")
        print(f"      PyTorch: {'âœ…' if analysis.pytorch_info.get('available') else 'âŒ'}")
        print(f"      ì¶”ì²œ ë””ë°”ì´ìŠ¤: {analysis.recommended_device}")
        print(f"      CUDA: {'âœ…' if analysis.cuda_available else 'âŒ'}")
        print(f"      MPS: {'âœ…' if analysis.mps_available else 'âŒ'}")
        
        print(f"   ğŸ“ í”„ë¡œì íŠ¸:")
        print(f"      AI ëª¨ë¸ ë””ë ‰í† ë¦¬: {analysis.project_structure.get('ai_models_dir', 'None')}")
        print(f"      AI ëª¨ë¸ í¬ê¸°: {analysis.project_structure.get('ai_models_size_gb', 0):.1f}GB")
        print(f"      Python í™˜ê²½: {analysis.python_info.get('conda_env', 'none')}")
    
    def _generate_overall_summary(self) -> Dict[str, Any]:
        """ì „ì²´ ìš”ì•½ ìƒì„±"""
        total_steps = len(self.step_analyses)
        successful_steps = sum(1 for analysis in self.step_analyses.values() 
                              if analysis.status == StepAnalysisStatus.SUCCESS)
        
        total_models = sum(len(analysis.ai_models_detected) for analysis in self.step_analyses.values())
        total_model_size = sum(analysis.total_model_size_gb for analysis in self.step_analyses.values())
        
        successful_checkpoints = sum(
            sum(1 for cp in analysis.checkpoints_analysis if cp.status == CheckpointStatus.SUCCESS)
            for analysis in self.step_analyses.values()
        )
        total_checkpoints = sum(len(analysis.checkpoints_analysis) for analysis in self.step_analyses.values())
        
        average_health_score = sum(analysis.overall_health_score for analysis in self.step_analyses.values()) / total_steps if total_steps > 0 else 0
        
        return {
            'steps': {
                'total': total_steps,
                'successful': successful_steps,
                'success_rate': (successful_steps / total_steps * 100) if total_steps > 0 else 0
            },
            'models': {
                'total_detected': total_models,
                'total_size_gb': total_model_size,
                'successful_checkpoints': successful_checkpoints,
                'total_checkpoints': total_checkpoints,
                'checkpoint_success_rate': (successful_checkpoints / total_checkpoints * 100) if total_checkpoints > 0 else 0
            },
            'health': {
                'average_score': average_health_score,
                'system_ready': self.system_analysis.memory_sufficient and self.system_analysis.pytorch_info.get('available', False),
                'ai_ready': successful_steps >= total_steps * 0.7  # 70% ì´ìƒ ì„±ê³µ
            }
        }
    
    def _identify_critical_issues(self) -> List[str]:
        """ì¤‘ìš” ë¬¸ì œì  ì‹ë³„"""
        issues = []
        
        # ì‹œìŠ¤í…œ ìˆ˜ì¤€ ë¬¸ì œ
        if not self.system_analysis.pytorch_info.get('available', False):
            issues.append("ğŸ”¥ CRITICAL: PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ - AI ëª¨ë¸ ì‹¤í–‰ ë¶ˆê°€")
        
        if not self.system_analysis.memory_sufficient:
            issues.append("ğŸ”¥ CRITICAL: ë©”ëª¨ë¦¬ ë¶€ì¡± - AI ëª¨ë¸ ë¡œë”©ì— ë¬¸ì œ ë°œìƒ ê°€ëŠ¥")
        
        # Step ìˆ˜ì¤€ ë¬¸ì œ
        failed_imports = [name for name, analysis in self.step_analyses.items() 
                         if not analysis.import_success]
        if failed_imports:
            issues.append(f"âŒ Import ì‹¤íŒ¨: {', '.join(failed_imports)}")
        
        failed_initialization = [name for name, analysis in self.step_analyses.items() 
                               if not analysis.initialization_success]
        if failed_initialization:
            issues.append(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {', '.join(failed_initialization)}")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¬¸ì œ
        corrupted_checkpoints = []
        for analysis in self.step_analyses.values():
            for cp in analysis.checkpoints_analysis:
                if cp.status == CheckpointStatus.CORRUPTED:
                    corrupted_checkpoints.append(cp.file_path.name)
        
        if corrupted_checkpoints:
            issues.append(f"ğŸ’¾ ì†ìƒëœ ì²´í¬í¬ì¸íŠ¸: {', '.join(corrupted_checkpoints[:3])}{'...' if len(corrupted_checkpoints) > 3 else ''}")
        
        return issues
    
    def _generate_actionable_recommendations(self) -> List[str]:
        """ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì‹œìŠ¤í…œ ê°œì„ 
        if not self.system_analysis.pytorch_info.get('available', False):
            recommendations.append("ğŸ“¦ PyTorch ì„¤ì¹˜: conda install pytorch torchvision -c pytorch")
        
        if self.system_analysis.recommended_device == 'cpu' and self.system_analysis.is_m3_max:
            recommendations.append("âš¡ M3 Max ìµœì í™”: pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu")
        
        # Stepë³„ ê°œì„ 
        for name, analysis in self.step_analyses.items():
            if not analysis.import_success:
                recommendations.append(f"ğŸ”§ {name} ì˜ì¡´ì„± í™•ì¸: ëª¨ë“ˆ ê²½ë¡œ ë° import ì˜¤ë¥˜ í•´ê²° í•„ìš”")
            
            if analysis.import_success and not analysis.initialization_success:
                recommendations.append(f"ğŸ”§ {name} ì´ˆê¸°í™” ê°œì„ : AI ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë° ê¶Œí•œ í™•ì¸")
        
        # ì„±ëŠ¥ ìµœì í™”
        total_model_size = sum(analysis.total_model_size_gb for analysis in self.step_analyses.values())
        if total_model_size > 50:  # 50GB ì´ìƒ
            recommendations.append(f"ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™”: {total_model_size:.1f}GB ëª¨ë¸ - ë°°ì¹˜ í¬ê¸° ì¡°ì • ë° ìºì‹± ì „ëµ í•„ìš”")
        
        # ì²´í¬í¬ì¸íŠ¸ ìµœì í™”
        weights_only_issues = 0
        for analysis in self.step_analyses.values():
            for cp in analysis.checkpoints_analysis:
                if cp.status == CheckpointStatus.WEIGHTS_ONLY_FAILED:
                    weights_only_issues += 1
        
        if weights_only_issues > 0:
            recommendations.append(f"ğŸ”§ PyTorch í˜¸í™˜ì„±: {weights_only_issues}ê°œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ weights_only ë¬¸ì œ - PyTorch ì—…ë°ì´íŠ¸ í•„ìš”")
        
        return recommendations
    
    def _calculate_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        total_analysis_time = time.time() - self.start_time
        
        step_times = [analysis.import_time + analysis.initialization_time 
                     for analysis in self.step_analyses.values()]
        
        return {
            'total_analysis_time_seconds': total_analysis_time,
            'average_step_analysis_time': sum(step_times) / len(step_times) if step_times else 0,
            'system_analysis_efficiency': 'efficient' if total_analysis_time < 300 else 'slow',
            'memory_usage_peak_gb': psutil.Process().memory_info().rss / (1024**3)
        }
    
    def _print_debug_results(self, debug_result: Dict[str, Any]):
        """ë””ë²„ê¹… ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“Š Ultimate AI Model Loading Debug Results")
        print("=" * 80)
        
        # ì „ì²´ ìš”ì•½
        summary = debug_result['overall_summary']
        print(f"\nğŸ¯ ì „ì²´ ìš”ì•½:")
        print(f"   Step ì„±ê³µë¥ : {summary['steps']['success_rate']:.1f}% ({summary['steps']['successful']}/{summary['steps']['total']})")
        print(f"   ì²´í¬í¬ì¸íŠ¸ ì„±ê³µë¥ : {summary['models']['checkpoint_success_rate']:.1f}% ({summary['models']['successful_checkpoints']}/{summary['models']['total_checkpoints']})")
        print(f"   ì „ì²´ AI ëª¨ë¸ í¬ê¸°: {summary['models']['total_size_gb']:.1f}GB")
        print(f"   í‰ê·  ê±´ê°•ë„: {summary['health']['average_score']:.1f}/100")
        print(f"   AI ì¤€ë¹„ ìƒíƒœ: {'âœ…' if summary['health']['ai_ready'] else 'âŒ'}")
        
        # Stepë³„ ìƒì„¸ ê²°ê³¼
        print(f"\nğŸš€ Stepë³„ ë¶„ì„ ê²°ê³¼:")
        for step_name, analysis in self.step_analyses.items():
            status_icon = "âœ…" if analysis.status == StepAnalysisStatus.SUCCESS else "âŒ"
            
            print(f"   {status_icon} {step_name} (ê±´ê°•ë„: {analysis.overall_health_score:.0f}/100)")
            print(f"      Import: {'âœ…' if analysis.import_success else 'âŒ'} | "
                  f"ì¸ìŠ¤í„´ìŠ¤: {'âœ…' if analysis.instance_created else 'âŒ'} | "
                  f"ì´ˆê¸°í™”: {'âœ…' if analysis.initialization_success else 'âŒ'}")
            
            if analysis.ai_models_detected:
                successful_cp = sum(1 for cp in analysis.checkpoints_analysis if cp.status == CheckpointStatus.SUCCESS)
                total_cp = len(analysis.checkpoints_analysis)
                print(f"      AI ëª¨ë¸: {len(analysis.ai_models_detected)}ê°œ ({analysis.total_model_size_gb:.1f}GB)")
                print(f"      ì²´í¬í¬ì¸íŠ¸: {successful_cp}/{total_cp} ì„±ê³µ")
            
            if analysis.import_errors or analysis.instance_errors or analysis.initialization_errors:
                all_errors = analysis.import_errors + analysis.instance_errors + analysis.initialization_errors
                print(f"      ì˜¤ë¥˜: {all_errors[0] if all_errors else 'None'}")
        
        # ì¤‘ìš” ë¬¸ì œì 
        if debug_result['critical_issues']:
            print(f"\nğŸ”¥ ì¤‘ìš” ë¬¸ì œì :")
            for issue in debug_result['critical_issues']:
                print(f"   {issue}")
        
        # ì¶”ì²œì‚¬í•­
        if debug_result['recommendations']:
            print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
            for i, rec in enumerate(debug_result['recommendations'], 1):
                print(f"   {i}. {rec}")
        
        # ì„±ëŠ¥ ì§€í‘œ
        metrics = debug_result['performance_metrics']
        print(f"\nğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:")
        print(f"   ì „ì²´ ë¶„ì„ ì‹œê°„: {metrics['total_analysis_time_seconds']:.1f}ì´ˆ")
        print(f"   í‰ê·  Step ë¶„ì„ ì‹œê°„: {metrics['average_step_analysis_time']:.1f}ì´ˆ")
        print(f"   ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {metrics['memory_usage_peak_gb']:.1f}GB")
        print(f"   ë¶„ì„ íš¨ìœ¨ì„±: {metrics['system_analysis_efficiency']}")
    
    def _save_debug_results(self, debug_result: Dict[str, Any]):
        """ë””ë²„ê¹… ê²°ê³¼ ì €ì¥"""
        try:
            timestamp = int(time.time())
            results_file = Path(f"ultimate_ai_debug_results_{timestamp}.json")
            
            # JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ì²˜ë¦¬
            serializable_result = self._make_json_serializable(debug_result)
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_result, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ“„ ìƒì„¸ ë””ë²„ê¹… ê²°ê³¼ê°€ {results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ìš”ì•½ ë¦¬í¬íŠ¸ë„ ì €ì¥
            summary_file = Path(f"ai_debug_summary_{timestamp}.txt")
            self._save_summary_report(summary_file, debug_result)
            print(f"ğŸ“„ ìš”ì•½ ë¦¬í¬íŠ¸ê°€ {summary_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"\nâš ï¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _make_json_serializable(self, obj):
        """JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜"""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, Path):
            return str(obj)
        elif hasattr(obj, '__dict__'):
            return self._make_json_serializable(obj.__dict__)
        else:
            return obj
    
    def _save_summary_report(self, file_path: Path, debug_result: Dict[str, Any]):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("ğŸ”¥ Ultimate AI Model Loading Debug Summary Report\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"ìƒì„± ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\n")
                f.write(f"ë””ë²„ê±° ë²„ì „: {debug_result['debug_version']}\n")
                f.write(f"ë¶„ì„ ì†Œìš” ì‹œê°„: {debug_result['total_debug_time']:.1f}ì´ˆ\n\n")
                
                # ì‹œìŠ¤í…œ ì •ë³´
                system = debug_result['system_analysis']
                f.write("ğŸ“Š ì‹œìŠ¤í…œ í™˜ê²½:\n")
                f.write(f"   í•˜ë“œì›¨ì–´: {system['cpu_info'].get('logical_cores', 0)}ì½”ì–´, "
                       f"{system['memory_info'].get('total_gb', 0):.1f}GB ë©”ëª¨ë¦¬\n")
                f.write(f"   PyTorch: {'ì‚¬ìš©ê°€ëŠ¥' if system['pytorch_info'].get('available') else 'ì—†ìŒ'}\n")
                f.write(f"   ì¶”ì²œ ë””ë°”ì´ìŠ¤: {system['recommendations']['recommended_device']}\n")
                f.write(f"   AI ëª¨ë¸ í¬ê¸°: {system['project_structure'].get('ai_models_size_gb', 0):.1f}GB\n\n")
                
                # ì „ì²´ ìš”ì•½
                summary = debug_result['overall_summary']
                f.write("ğŸ¯ ë¶„ì„ ê²°ê³¼ ìš”ì•½:\n")
                f.write(f"   Step ì„±ê³µë¥ : {summary['steps']['success_rate']:.1f}%\n")
                f.write(f"   ì²´í¬í¬ì¸íŠ¸ ì„±ê³µë¥ : {summary['models']['checkpoint_success_rate']:.1f}%\n")
                f.write(f"   í‰ê·  ê±´ê°•ë„: {summary['health']['average_score']:.1f}/100\n")
                f.write(f"   AI ì‹œìŠ¤í…œ ì¤€ë¹„ ìƒíƒœ: {'ì¤€ë¹„ë¨' if summary['health']['ai_ready'] else 'ë¬¸ì œìˆìŒ'}\n\n")
                
                # ì¤‘ìš” ë¬¸ì œì 
                if debug_result['critical_issues']:
                    f.write("ğŸ”¥ ì¤‘ìš” ë¬¸ì œì :\n")
                    for issue in debug_result['critical_issues']:
                        f.write(f"   - {issue}\n")
                    f.write("\n")
                
                # ì¶”ì²œì‚¬í•­
                if debug_result['recommendations']:
                    f.write("ğŸ’¡ ì¶”ì²œì‚¬í•­:\n")
                    for i, rec in enumerate(debug_result['recommendations'], 1):
                        f.write(f"   {i}. {rec}\n")
                    f.write("\n")
                
                # Stepë³„ ìƒì„¸ ì •ë³´
                f.write("ğŸš€ Stepë³„ ìƒì„¸ ë¶„ì„:\n")
                for step_name, step_data in debug_result['step_analyses'].items():
                    f.write(f"\n   {step_name}:\n")
                    f.write(f"      ìƒíƒœ: {step_data['status']}\n")
                    f.write(f"      ê±´ê°•ë„: {step_data['performance']['health_score']:.0f}/100\n")
                    f.write(f"      Import: {'ì„±ê³µ' if step_data['import_analysis']['success'] else 'ì‹¤íŒ¨'}\n")
                    f.write(f"      ì´ˆê¸°í™”: {'ì„±ê³µ' if step_data['initialization']['success'] else 'ì‹¤íŒ¨'}\n")
                    f.write(f"      AI ëª¨ë¸: {len(step_data['ai_models']['detected'])}ê°œ "
                           f"({step_data['ai_models']['total_size_gb']:.1f}GB)\n")
                    
                    if step_data['import_analysis']['errors']:
                        f.write(f"      ì˜¤ë¥˜: {step_data['import_analysis']['errors'][0]}\n")
                
        except Exception as e:
            print(f"ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 7. ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.WARNING,
        format='%(levelname)s: %(message)s',
        force=True
    )
    
    print(f"ğŸ”¥ Ultimate AI Model Loading Debugger v4.0")
    print(f"ğŸ”¥ GitHub í”„ë¡œì íŠ¸: MyCloset AI Pipeline")
    print(f"ğŸ”¥ Target: 229GB AI Models Complete Analysis")
    print(f"ğŸ”¥ ì‹œì‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # ë””ë²„ê±° ìƒì„± ë° ì‹¤í–‰
        debugger = UltimateAIModelDebugger()
        debug_result = debugger.run_ultimate_debugging()
        
        # ì„±ê³µ ì—¬ë¶€ í™•ì¸
        if debug_result.get('overall_summary', {}).get('health', {}).get('ai_ready', False):
            print(f"\nğŸ‰ SUCCESS: AI ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            print(f"\nâš ï¸ WARNING: AI ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ìœ„ì˜ ì¶”ì²œì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        return debug_result
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        print(f"\nâŒ ë””ë²„ê¹… ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        print(f"ì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        return None
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        gc.collect()
        print(f"\nğŸ‘‹ Ultimate AI Model Debugger ì¢…ë£Œ")

# =============================================================================
# ğŸ”¥ 8. ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def quick_checkpoint_check(checkpoint_path: str) -> bool:
    """ë¹ ë¥¸ ì²´í¬í¬ì¸íŠ¸ í™•ì¸"""
    try:
        analyzer = CheckpointAnalyzer()
        result = analyzer.analyze_checkpoint(Path(checkpoint_path))
        return result.status == CheckpointStatus.SUCCESS
    except Exception:
        return False

def quick_step_check(step_name: str) -> bool:
    """ë¹ ë¥¸ Step í™•ì¸"""
    try:
        step_configs = {
            'HumanParsingStep': 'app.ai_pipeline.steps.step_01_human_parsing',
            'PoseEstimationStep': 'app.ai_pipeline.steps.step_02_pose_estimation',
            'ClothSegmentationStep': 'app.ai_pipeline.steps.step_03_cloth_segmentation'
        }
        
        if step_name not in step_configs:
            return False
        
        module = importlib.import_module(step_configs[step_name])
        step_class = getattr(module, step_name)
        instance = step_class(device='cpu', strict_mode=False)
        return True
        
    except Exception:
        return False

def get_system_readiness_score() -> float:
    """ì‹œìŠ¤í…œ ì¤€ë¹„ë„ ì ìˆ˜ (0-100)"""
    try:
        analyzer = SystemEnvironmentAnalyzer()
        analysis = analyzer.analyze_complete_environment()
        
        score = 0.0
        
        # PyTorch (30ì )
        if analysis.pytorch_info.get('available', False):
            score += 30
        
        # ë©”ëª¨ë¦¬ (25ì )
        if analysis.memory_sufficient:
            score += 25
        
        # ë””ë°”ì´ìŠ¤ ê°€ì† (20ì )
        if analysis.recommended_device != 'cpu':
            score += 20
        
        # í”„ë¡œì íŠ¸ êµ¬ì¡° (15ì )
        if analysis.project_structure.get('ai_models_dir'):
            score += 15
        
        # ì˜ì¡´ì„± (10ì )
        deps_ready = sum(analysis.dependencies_status.values())
        total_deps = len(analysis.dependencies_status)
        if total_deps > 0:
            score += (deps_ready / total_deps) * 10
        
        return score
        
    except Exception:
        return 0.0

if __name__ == "__main__":
    main()