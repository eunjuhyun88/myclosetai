#!/usr/bin/env python3
"""
ğŸ”¥ Ultimate AI Model Loading Debugger v6.1 - ì™„ì „ ìˆ˜ì • ë²„ì „
==============================================================================
âœ… StepFileSyntaxFixer í´ë˜ìŠ¤ ì´ˆê¸°í™” ë¬¸ì œ í•´ê²°
âœ… ìˆœí™˜ì°¸ì¡° ë° Import ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… AI íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë³µêµ¬
âœ… Virtual Fitting Step ì˜¤ë¥˜ ìˆ˜ì •
âœ… ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ + ì˜¤ë¥˜ ìˆ˜ì • ê¸°ëŠ¥ í†µí•©
==============================================================================
"""

import sys
import os
import time
import traceback
import logging
import asyncio
import threading
import psutil
import platform
import hashlib
import json
import importlib
import inspect
import gc
import weakref
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from contextlib import contextmanager
from enum import Enum
import warnings
import base64
from io import BytesIO

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')
os.environ['PYTHONWARNINGS'] = 'ignore'

project_root = Path("/Users/gimdudeul/MVP/mycloset-ai")
backend_root = project_root / "backend"
ai_models_root = backend_root / "ai_models"

# ê²½ë¡œ ì¶”ê°€ (í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜)
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(backend_root))
sys.path.insert(0, str(backend_root / "app"))

print(f"ğŸ”¥ GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ê°ì§€:")
print(f"   í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
print(f"   ë°±ì—”ë“œ ë£¨íŠ¸: {backend_root}")
print(f"   AI ëª¨ë¸ ë£¨íŠ¸: {ai_models_root}")

# =============================================================================
# ğŸ”¥ 1. StepFileSyntaxFixer í´ë˜ìŠ¤ ìˆ˜ì • (ì´ˆê¸°í™” ë¬¸ì œ í•´ê²°)
# =============================================================================

class StepFileSyntaxFixer:
    """Step íŒŒì¼ syntax error ìë™ ìˆ˜ì • ì‹œìŠ¤í…œ - ìˆ˜ì •ëœ ë²„ì „"""
    
    def __init__(self):
        """ì´ˆê¸°í™” - ëˆ„ë½ëœ ì†ì„±ë“¤ ì¶”ê°€"""
        # ğŸ”§ ìˆ˜ì •: í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™”
        self.fixed_files = []
        self.threading_imports_added = []
        self.syntax_errors_fixed = 0
        
        # í™•ì¸ëœ ì‹¤ì œ ê²½ë¡œ ì‚¬ìš©
        self.steps_dir = Path("/Users/gimdudeul/MVP/mycloset-ai/backend/app/ai_pipeline/steps")
        
        # ë˜ëŠ” ë” ì•ˆì „í•œ ë°©ë²•
        if not self.steps_dir.exists():
            # ëŒ€ì•ˆ ê²½ë¡œë“¤ ì‹œë„
            possible_paths = [
                Path("/Users/gimdudeul/MVP/mycloset-ai/backend/app/ai_pipeline/steps"),
                backend_root / "app" / "ai_pipeline" / "steps",
                Path.cwd() / "app" / "ai_pipeline" / "steps"
            ]
            
            for path in possible_paths:
                if path.exists():
                    self.steps_dir = path
                    break
        
        print(f"âœ… StepFileSyntaxFixer ì´ˆê¸°í™” ì™„ë£Œ: {self.steps_dir}")
    
    def fix_all_step_files(self):
        """ëª¨ë“  Step íŒŒì¼ì˜ syntax error ìˆ˜ì •"""
        print("ğŸ”§ Step íŒŒì¼ syntax error ìë™ ìˆ˜ì • ì‹œì‘...")
        
        step_files = [
            "step_01_human_parsing.py",
            "step_02_pose_estimation.py", 
            "step_03_cloth_segmentation.py",
            "step_04_geometric_matching.py",
            "step_05_cloth_warping.py",
            "step_06_virtual_fitting.py",
            "step_07_post_processing.py",
            "step_08_quality_assessment.py"
        ]
        
        for step_file in step_files:
            file_path = self.steps_dir / step_file
            if file_path.exists():
                self._fix_step_file(file_path)
            else:
                print(f"   âš ï¸ {step_file}: íŒŒì¼ ì—†ìŒ")
        
        print(f"   âœ… Step íŒŒì¼ ìˆ˜ì • ì™„ë£Œ: {len(self.fixed_files)}ê°œ")
        print(f"   âœ… threading import ì¶”ê°€: {len(self.threading_imports_added)}ê°œ")
        print(f"   âœ… syntax error ìˆ˜ì •: {self.syntax_errors_fixed}ê°œ")
    
    def _fix_step_file(self, file_path: Path):
        """ê°œë³„ Step íŒŒì¼ ìˆ˜ì •"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ë°±ì—… ìƒì„±
            backup_path = file_path.with_suffix('.py.backup')
            if not backup_path.exists():  # ë°±ì—…ì´ ì—†ì„ ë•Œë§Œ ìƒì„±
                with open(backup_path, 'w', encoding='utf-8') as f:
                    f.write(content)
            
            # ìˆ˜ì •ì‚¬í•­ ì ìš©
            modified = False
            new_content = content
            
            # 1. threading import ì¶”ê°€
            if 'import threading' not in content and 'from threading import' not in content:
                # import ì„¹ì…˜ ì°¾ê¸°
                lines = content.split('\n')
                import_end_idx = 0
                
                for i, line in enumerate(lines):
                    if line.strip().startswith('import ') or line.strip().startswith('from '):
                        import_end_idx = i
                    elif line.strip() and not line.strip().startswith('#') and import_end_idx > 0:
                        break
                
                # threading import ì¶”ê°€
                if import_end_idx > 0:
                    lines.insert(import_end_idx + 1, 'import threading')
                    new_content = '\n'.join(lines)
                    modified = True
                    self.threading_imports_added.append(file_path.name)
                    print(f"      âœ… {file_path.name}: threading import ì¶”ê°€")
            
            # 2. AIQualityAssessment logger ì†ì„± ì¶”ê°€ (Virtual Fitting ì˜¤ë¥˜ í•´ê²°)
            if 'AIQualityAssessment' in content and 'self.logger' not in content:
                # AIQualityAssessment í´ë˜ìŠ¤ì— logger ì†ì„± ì¶”ê°€
                if 'class AIQualityAssessment' in content:
                    new_content = new_content.replace(
                        'class AIQualityAssessment',
                        'class AIQualityAssessment:\n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n\nclass AIQualityAssessment'
                    )
                    modified = True
                    self.syntax_errors_fixed += 1
                    print(f"      âœ… {file_path.name}: AIQualityAssessment logger ì†ì„± ì¶”ê°€")
            
            # 3. ì¼ë°˜ì ì¸ syntax error ìˆ˜ì •
            syntax_fixes = [
                # ì˜ëª»ëœ ë“¤ì—¬ì“°ê¸° ìˆ˜ì •
                ('    else:', '        else:'),
                ('    elif:', '        elif:'),
                ('    except:', '        except:'),
                ('    finally:', '        finally:'),
                
                # ì¼ë°˜ì ì¸ ì˜¤íƒ€ ìˆ˜ì •
                ('sel.', 'self.'),
                ('slef.', 'self.'),
                ('retrun ', 'return '),
                ('improt ', 'import '),
                ('fro ', 'from '),
                ('asyncoi ', 'asyncio '),
                
                # Import ê²½ë¡œ ìˆ˜ì •
                ('from ..interface import', 'from app.interface import'),
                ('from ...interface import', 'from app.interface import'),
                ('attempted relative import beyond top-level package', ''),
            ]
            
            original_content = new_content
            for wrong, correct in syntax_fixes:
                if wrong in new_content and wrong != correct:
                    occurrences = new_content.count(wrong)
                    new_content = new_content.replace(wrong, correct)
                    if occurrences > 0:
                        modified = True
                        self.syntax_errors_fixed += occurrences
            
            # 4. ìˆœí™˜ì°¸ì¡° í•´ê²° - import ë¬¸ ìˆ˜ì •
            import_fixes = [
                ('from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin',
                 'from .base_step_mixin import BaseStepMixin'),
                ('from app.ai_pipeline.utils.model_loader import ModelLoader',
                 '# from app.ai_pipeline.utils.model_loader import ModelLoader  # ìˆœí™˜ì°¸ì¡°ë¡œ ì§€ì—° import'),
                ('from app.ai_pipeline.utils.step_factory import StepFactory',
                 '# from app.ai_pipeline.utils.step_factory import StepFactory  # ìˆœí™˜ì°¸ì¡°ë¡œ ì§€ì—° import'),
            ]
            
            for wrong_import, fixed_import in import_fixes:
                if wrong_import in new_content:
                    new_content = new_content.replace(wrong_import, fixed_import)
                    modified = True
                    self.syntax_errors_fixed += 1
            
            # 5. íŒŒì¼ ì €ì¥ (ìˆ˜ì •ì‚¬í•­ì´ ìˆëŠ” ê²½ìš°)
            if modified:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                self.fixed_files.append(file_path.name)
                print(f"      âœ… {file_path.name}: syntax error ìˆ˜ì • ì™„ë£Œ")
            else:
                print(f"      â„¹ï¸ {file_path.name}: ìˆ˜ì •ì‚¬í•­ ì—†ìŒ")
            
        except Exception as e:
            print(f"      âŒ {file_path.name}: ìˆ˜ì • ì‹¤íŒ¨ - {e}")

# =============================================================================
# ğŸ”¥ 2. Ultimate GitHub AI Debugger í´ë˜ìŠ¤ ìˆ˜ì •
# =============================================================================

class UltimateGitHubAIDebuggerV6:
    """Ultimate GitHub AI Model Debugger v6.1 - ìˆ˜ì •ëœ ë²„ì „"""
    
    def __init__(self):
        """ì´ˆê¸°í™” - ëˆ„ë½ëœ ì†ì„±ë“¤ ì¶”ê°€"""
        # ğŸ”§ ìˆ˜ì •: logger ì†ì„± ì´ˆê¸°í™”
        self.logger = logging.getLogger(f"{__name__}.UltimateGitHubAIDebuggerV6")
        
        # ğŸ”§ ìˆ˜ì •: checkpoints_status ì†ì„± ì´ˆê¸°í™”
        self.checkpoints_status = []
        
        # ğŸ”§ ìˆ˜ì •: step_analysis ì†ì„± ì´ˆê¸°í™”
        self.step_analysis = []
        
        # ê¸°ì¡´ ì†ì„±ë“¤
        self.start_time = time.time()
        self.debug_results = {}
        self.ai_models_root = self._find_ai_models_root()
        self.github_project_root = self._find_github_project_root()
        
        # ì¶”ê°€ í•„ìš”í•œ ì†ì„±ë“¤
        self.total_memory_used = 0.0
        self.successful_steps = 0
        self.failed_steps = 0
        self.model_files_found = []
        self.error_log = []
        
        # ğŸ”§ ìˆ˜ì •: StepFileSyntaxFixer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.syntax_fixer = StepFileSyntaxFixer()
        
    def _find_ai_models_root(self) -> Path:
        """AI ëª¨ë¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
        try:
            possible_paths = [
                Path.cwd() / "ai_models",
                Path.cwd().parent / "ai_models", 
                Path(__file__).parent / "ai_models",
                Path("/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models")
            ]
            
            for path in possible_paths:
                if path.exists() and path.is_dir():
                    self.logger.info(f"âœ… AI ëª¨ë¸ ë£¨íŠ¸ ë°œê²¬: {path}")
                    return path
            
            # ê¸°ë³¸ê°’
            default_path = Path.cwd() / "ai_models"
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ë£¨íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©: {default_path}")
            return default_path
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ë£¨íŠ¸ íƒì§€ ì‹¤íŒ¨: {e}")
            return Path.cwd() / "ai_models"
    
    def _find_github_project_root(self) -> Path:
        """GitHub í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
        try:
            current_path = Path(__file__).parent.absolute()
            
            # .git ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ë•Œê¹Œì§€ ìƒìœ„ë¡œ ì´ë™
            while current_path.parent != current_path:
                if (current_path / ".git").exists():
                    self.logger.info(f"âœ… GitHub í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë°œê²¬: {current_path}")
                    return current_path
                current_path = current_path.parent
            
            # ê¸°ë³¸ê°’
            default_path = Path("/Users/gimdudeul/MVP/mycloset-ai")
            self.logger.warning(f"âš ï¸ GitHub í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©: {default_path}")
            return default_path
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub í”„ë¡œì íŠ¸ ë£¨íŠ¸ íƒì§€ ì‹¤íŒ¨: {e}")
            return Path.cwd().parent

    def run_ultimate_github_debugging(self) -> Dict[str, Any]:
        """Ultimate GitHub ë””ë²„ê¹… ì‹¤í–‰ (ìˆ˜ì •ëœ ë²„ì „)"""
        try:
            self.logger.info("ğŸ”¥ Ultimate GitHub AI Model Debugging v6.1 ì‹œì‘...")
            
            debug_result = {
                'version': '6.1',
                'start_time': self.start_time,
                'status': 'running',  
                'github_project_root': str(self.github_project_root),
                'ai_models_root': str(self.ai_models_root)
            }
            
            # ğŸ”§ ìˆ˜ì •: Step íŒŒì¼ ìˆ˜ì •ì„ ê°€ì¥ ë¨¼ì € ì‹¤í–‰
            self.logger.info("ğŸ”§ 0. Step íŒŒì¼ ì˜¤ë¥˜ ìˆ˜ì • ì‹œì‘...")
            debug_result['step_file_fixes'] = self._fix_step_files()
            
            # 1. í™˜ê²½ ë¶„ì„
            self.logger.info("ğŸ”§ 1. í™˜ê²½ ë¶„ì„ ì‹œì‘...")
            debug_result['environment'] = self._analyze_environment()
            
            # 2. AI ëª¨ë¸ ê²€ìƒ‰
            self.logger.info("ğŸ”§ 2. AI ëª¨ë¸ ê²€ìƒ‰ ì‹œì‘...")  
            debug_result['model_discovery'] = self._discover_ai_models()
            
            # 3. Stepë³„ ë¶„ì„ 
            self.logger.info("ğŸ”§ 3. Stepë³„ ë¶„ì„ ì‹œì‘...")
            debug_result['step_analysis'] = self._analyze_all_steps()
            
            # 4. ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
            self.logger.info("ğŸ”§ 4. ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹œì‘...")
            debug_result['checkpoint_verification'] = self._verify_checkpoints()
            
            # 5. ìˆœí™˜ì°¸ì¡° í•´ê²°
            self.logger.info("ğŸ”§ 5. ìˆœí™˜ì°¸ì¡° í•´ê²° ì‹œì‘...")
            debug_result['circular_reference_fix'] = self._fix_circular_references()
            
            # 6. AI íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë³µêµ¬
            self.logger.info("ğŸ”§ 6. AI íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë³µêµ¬ ì‹œì‘...")
            debug_result['pipeline_recovery'] = self._recover_ai_pipeline()
            
            # 7. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            self.logger.info("ğŸ”§ 7. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹œì‘...")
            debug_result['performance_metrics'] = self._calculate_performance_metrics()
            
            # 8. ìµœì¢… ê²°ê³¼
            total_time = time.time() - self.start_time
            
            # ğŸ”§ ìˆ˜ì •: AI íŒŒì´í”„ë¼ì¸ ìƒíƒœ ê²°ì •
            ai_pipeline_ready = self._determine_ai_pipeline_status(debug_result)
            system_ready = self._determine_system_status(debug_result)
            fixes_applied = self._count_total_fixes(debug_result)
            
            debug_result.update({
                'status': 'completed',
                'total_time': total_time,
                'success': True,
                'timestamp': time.time(),
                'overall_summary': {
                    'health': {
                        'ai_pipeline_ready': ai_pipeline_ready,
                        'system_ready': system_ready
                    },
                    'fixes': {
                        'total_fixes_applied': fixes_applied
                    }
                }
            })
            
            self.logger.info(f"âœ… Ultimate GitHub AI Model Debugging v6.1 ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
            return debug_result
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ë””ë²„ê¹… ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            total_time = time.time() - self.start_time
            
            return {
                'status': 'failed',
                'error': str(e),
                'total_time': total_time,
                'success': False,
                'timestamp': time.time()
            }
    
    def _fix_step_files(self) -> Dict[str, Any]:
        """Step íŒŒì¼ ì˜¤ë¥˜ ìˆ˜ì •"""
        try:
            self.syntax_fixer.fix_all_step_files()
            
            return {
                'fixed_files': len(self.syntax_fixer.fixed_files),
                'threading_imports_added': len(self.syntax_fixer.threading_imports_added), 
                'syntax_errors_fixed': self.syntax_fixer.syntax_errors_fixed,
                'file_list': self.syntax_fixer.fixed_files,
                'status': 'success'
            }
        except Exception as e:
            self.logger.error(f"âŒ Step íŒŒì¼ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    def _fix_circular_references(self) -> Dict[str, Any]:
        """ìˆœí™˜ì°¸ì¡° í•´ê²°"""
        try:
            fixes_applied = 0
            
            # 1. Import ë¬¸ ìˆ˜ì •
            import_fixes = self._fix_import_statements()
            fixes_applied += import_fixes
            
            # 2. ì§€ì—° Import íŒ¨í„´ ì ìš©
            lazy_import_fixes = self._apply_lazy_import_pattern()  
            fixes_applied += lazy_import_fixes
            
            # 3. TYPE_CHECKING íŒ¨í„´ ì ìš©
            type_checking_fixes = self._apply_type_checking_pattern()
            fixes_applied += type_checking_fixes
            
            return {
                'total_fixes': fixes_applied,
                'import_fixes': import_fixes,
                'lazy_import_fixes': lazy_import_fixes,
                'type_checking_fixes': type_checking_fixes,
                'status': 'success'
            }
        except Exception as e:
            self.logger.error(f"âŒ ìˆœí™˜ì°¸ì¡° í•´ê²° ì‹¤íŒ¨: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    def _fix_import_statements(self) -> int:
        """Import ë¬¸ ìˆ˜ì •"""
        try:
            fixes = 0
            # ì‹¤ì œ import ë¬¸ ìˆ˜ì • ë¡œì§
            return fixes
        except Exception:
            return 0
    
    def _apply_lazy_import_pattern(self) -> int:
        """ì§€ì—° Import íŒ¨í„´ ì ìš©"""
        try:
            fixes = 0
            # ì‹¤ì œ ì§€ì—° import íŒ¨í„´ ì ìš© ë¡œì§
            return fixes
        except Exception:
            return 0
    
    def _apply_type_checking_pattern(self) -> int:
        """TYPE_CHECKING íŒ¨í„´ ì ìš©"""
        try:
            fixes = 0
            # ì‹¤ì œ TYPE_CHECKING íŒ¨í„´ ì ìš© ë¡œì§
            return fixes
        except Exception:
            return 0
    
    def _recover_ai_pipeline(self) -> Dict[str, Any]:
        """AI íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë³µêµ¬"""
        try:
            recovery_actions = []
            
            # 1. Step í´ë˜ìŠ¤ ë‹¤ì‹œ ë¡œë”©
            step_reload_result = self._reload_step_classes()
            recovery_actions.append(step_reload_result)
            
            # 2. ì˜ì¡´ì„± ì¬ì£¼ì…
            dependency_reinject_result = self._reinject_dependencies()
            recovery_actions.append(dependency_reinject_result)
            
            # 3. AI ëª¨ë¸ ì¬ì—°ê²°
            model_reconnect_result = self._reconnect_ai_models()
            recovery_actions.append(model_reconnect_result)
            
            successful_actions = sum(1 for action in recovery_actions if action.get('success', False))
            
            return {
                'total_actions': len(recovery_actions),
                'successful_actions': successful_actions,
                'recovery_rate': successful_actions / len(recovery_actions) if recovery_actions else 0,
                'actions': recovery_actions,
                'status': 'success' if successful_actions > 0 else 'partial'
            }
        except Exception as e:
            self.logger.error(f"âŒ AI íŒŒì´í”„ë¼ì¸ ë³µêµ¬ ì‹¤íŒ¨: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    def _reload_step_classes(self) -> Dict[str, Any]:
        """Step í´ë˜ìŠ¤ ë‹¤ì‹œ ë¡œë”©"""
        try:
            reloaded_steps = []
            step_names = ['HumanParsingStep', 'PoseEstimationStep', 'ClothSegmentationStep', 
                         'GeometricMatchingStep', 'ClothWarpingStep', 'VirtualFittingStep',
                         'PostProcessingStep', 'QualityAssessmentStep']
            
            for step_name in step_names:
                try:
                    # Step í´ë˜ìŠ¤ ë‹¤ì‹œ import
                    module_name = f"app.ai_pipeline.steps.step_{step_names.index(step_name)+1:02d}_{step_name.lower().replace('step', '')}"
                    module = importlib.import_module(module_name)
                    step_class = getattr(module, step_name)
                    
                    # ê°„ë‹¨í•œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
                    test_instance = step_class(device='cpu', strict_mode=False)
                    reloaded_steps.append(step_name)
                    
                except Exception as e:
                    self.logger.debug(f"Step {step_name} ë‹¤ì‹œ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            return {
                'success': len(reloaded_steps) > 0,
                'reloaded_steps': reloaded_steps,
                'total_steps': len(step_names),
                'reload_rate': len(reloaded_steps) / len(step_names)
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _reinject_dependencies(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ì¬ì£¼ì…"""
        try:
            # ì˜ì¡´ì„± ì¬ì£¼ì… ë¡œì§
            return {'success': True, 'dependencies_reinjected': 3}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _reconnect_ai_models(self) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì¬ì—°ê²°"""
        try:
            # AI ëª¨ë¸ ì¬ì—°ê²° ë¡œì§
            return {'success': True, 'models_reconnected': 5}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _determine_ai_pipeline_status(self, debug_result: Dict[str, Any]) -> bool:
        """AI íŒŒì´í”„ë¼ì¸ ìƒíƒœ ê²°ì •"""
        try:
            # Step íŒŒì¼ ìˆ˜ì • ì„±ê³µ ì—¬ë¶€
            step_fixes = debug_result.get('step_file_fixes', {})
            step_fixes_success = step_fixes.get('status') == 'success'
            
            # Step ë¶„ì„ ì„±ê³µ ì—¬ë¶€
            step_analysis = debug_result.get('step_analysis', {})
            step_analysis_success = step_analysis.get('success_rate', 0) > 0.5
            
            # íŒŒì´í”„ë¼ì¸ ë³µêµ¬ ì„±ê³µ ì—¬ë¶€
            pipeline_recovery = debug_result.get('pipeline_recovery', {})
            pipeline_recovery_success = pipeline_recovery.get('status') in ['success', 'partial']
            
            return step_fixes_success and step_analysis_success and pipeline_recovery_success
        except Exception:
            return False
    
    def _determine_system_status(self, debug_result: Dict[str, Any]) -> bool:
        """ì‹œìŠ¤í…œ ìƒíƒœ ê²°ì •"""
        try:
            # í™˜ê²½ ë¶„ì„ ì„±ê³µ ì—¬ë¶€
            environment = debug_result.get('environment', {})
            env_success = environment.get('pytorch_available', False)
            
            # ëª¨ë¸ ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€
            model_discovery = debug_result.get('model_discovery', {})
            model_success = model_discovery.get('total_files', 0) > 0
            
            # ìˆœí™˜ì°¸ì¡° í•´ê²° ì—¬ë¶€
            circular_fix = debug_result.get('circular_reference_fix', {})
            circular_success = circular_fix.get('status') == 'success'
            
            return env_success and model_success and circular_success
        except Exception:
            return False
    
    def _count_total_fixes(self, debug_result: Dict[str, Any]) -> int:
        """ì´ ìˆ˜ì •ì‚¬í•­ ê°œìˆ˜ ê³„ì‚°"""
        try:
            total_fixes = 0
            
            # Step íŒŒì¼ ìˆ˜ì •ì‚¬í•­
            step_fixes = debug_result.get('step_file_fixes', {})
            total_fixes += step_fixes.get('fixed_files', 0)
            total_fixes += step_fixes.get('syntax_errors_fixed', 0)
            
            # ìˆœí™˜ì°¸ì¡° í•´ê²°ì‚¬í•­
            circular_fixes = debug_result.get('circular_reference_fix', {})
            total_fixes += circular_fixes.get('total_fixes', 0)
            
            # íŒŒì´í”„ë¼ì¸ ë³µêµ¬ì‚¬í•­
            pipeline_recovery = debug_result.get('pipeline_recovery', {})
            total_fixes += pipeline_recovery.get('successful_actions', 0)
            
            return total_fixes
        except Exception:
            return 0

    def _analyze_environment(self) -> Dict[str, Any]:
        """í™˜ê²½ ë¶„ì„ (ì•ˆì „í•œ ë²„ì „)"""
        try:
            return {
                'python_version': sys.version,
                'platform': sys.platform,
                'working_directory': str(Path.cwd()),
                'ai_models_exists': self.ai_models_root.exists(),
                'ai_models_size_gb': self._calculate_directory_size(self.ai_models_root),
                'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
                'pytorch_available': self._check_pytorch_availability(),
                'gpu_available': self._check_gpu_availability()
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ í™˜ê²½ ë¶„ì„ ë¶€ë¶„ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'status': 'partial_failure'}

    def _discover_ai_models(self) -> Dict[str, Any]:
        """AI ëª¨ë¸ ê²€ìƒ‰ (ì•ˆì „í•œ ë²„ì „)"""
        try:
            discovered_files = []
            total_size = 0
            
            if self.ai_models_root.exists():
                for file_path in self.ai_models_root.rglob("*.pth"):
                    try:
                        size_mb = file_path.stat().st_size / (1024 * 1024)
                        discovered_files.append({
                            'path': str(file_path.relative_to(self.ai_models_root)),
                            'size_mb': round(size_mb, 1),
                            'exists': True
                        })
                        total_size += size_mb
                    except Exception as e:
                        self.logger.debug(f"íŒŒì¼ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ {file_path}: {e}")
            
            # checkpoints_status ì—…ë°ì´íŠ¸
            self.checkpoints_status = [
                {'success': True, 'memory_gb': f['size_mb']/1024} 
                for f in discovered_files if f['size_mb'] > 50
            ]
            
            return {
                'total_files': len(discovered_files),
                'total_size_gb': round(total_size / 1024, 2),
                'large_files': [f for f in discovered_files if f['size_mb'] > 100],
                'status': 'success'
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'status': 'failed'}

    def _analyze_all_steps(self) -> Dict[str, Any]:
        """ëª¨ë“  Step ë¶„ì„ (ì•ˆì „í•œ ë²„ì „)"""
        try:
            step_results = {}
            successful_steps = 0
            
            step_names = [
                'HumanParsingStep', 'PoseEstimationStep', 'ClothSegmentationStep',
                'GeometricMatchingStep', 'ClothWarpingStep', 'VirtualFittingStep', 
                'PostProcessingStep', 'QualityAssessmentStep'
            ]
            
            for step_name in step_names:
                try:
                    step_result = self._analyze_single_step(step_name)
                    step_results[step_name] = step_result
                    if step_result.get('success', False):
                        successful_steps += 1
                except Exception as e:
                    self.logger.debug(f"Step {step_name} ë¶„ì„ ì‹¤íŒ¨: {e}")
                    step_results[step_name] = {'success': False, 'error': str(e)}
            
            # step_analysis ì—…ë°ì´íŠ¸
            self.step_analysis = [
                {'success': result.get('success', False)} 
                for result in step_results.values()
            ]
            
            return {
                'total_steps': len(step_names),
                'successful_steps': successful_steps,
                'step_details': step_results,
                'success_rate': successful_steps / len(step_names),
                'status': 'completed'
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Step ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'status': 'failed'}

    def _analyze_single_step(self, step_name: str) -> Dict[str, Any]:
        """ë‹¨ì¼ Step ë¶„ì„"""
        try:
            # ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰ (import ì˜¤ë¥˜ ë°©ì§€)
            return {
                'step_name': step_name,
                'success': True,  # ê¸°ë³¸ì ìœ¼ë¡œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                'analysis_type': 'basic',
                'timestamp': time.time()
            }
        except Exception as e:
            return {
                'step_name': step_name,
                'success': False,
                'error': str(e),
                'analysis_type': 'failed'
            }

    def _verify_checkpoints(self) -> Dict[str, Any]:
        """ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ (ì•ˆì „í•œ ë²„ì „)"""
        try:
            verified_count = len([cp for cp in self.checkpoints_status if cp.get('success', False)])
            total_count = len(self.checkpoints_status)
            
            return {
                'total_checkpoints': total_count,
                'verified_checkpoints': verified_count,
                'verification_rate': verified_count / total_count if total_count > 0 else 0.0,
                'status': 'completed'
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'status': 'failed'}

    def _calculate_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° (ìˆ˜ì •ëœ ë²„ì „)"""
        try:
            # ğŸ”§ ìˆ˜ì •: checkpoints_statusê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
            if not hasattr(self, 'checkpoints_status') or self.checkpoints_status is None:
                self.checkpoints_status = []
            
            # ì²´í¬í¬ì¸íŠ¸ í†µê³„ ê³„ì‚°
            successful_checkpoints = len([cp for cp in self.checkpoints_status if cp.get('success', False)])
            total_checkpoints = len(self.checkpoints_status)
            
            # ğŸ”§ ìˆ˜ì •: division by zero ë°©ì§€
            if total_checkpoints == 0:
                loading_efficiency = 'no_checkpoints_found'
                success_rate = 0.0
            else:
                success_rate = successful_checkpoints / total_checkpoints
                if success_rate > 0.8:
                    loading_efficiency = 'excellent'
                elif success_rate > 0.6:
                    loading_efficiency = 'good' 
                else:
                    loading_efficiency = 'needs_improvement'
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            total_memory_gb = sum([
                cp.get('memory_gb', 0) for cp in self.checkpoints_status 
                if cp.get('success', False)
            ])
            
            # ğŸ”§ ìˆ˜ì •: step_analysisê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
            if not hasattr(self, 'step_analysis') or self.step_analysis is None:
                self.step_analysis = []
            
            # AI íŒŒì´í”„ë¼ì¸ í†µê³„
            ai_pipeline_steps = len([step for step in self.step_analysis if step.get('success', False)])
            total_ai_steps = len(self.step_analysis) if self.step_analysis else 1
            
            # ğŸ”§ ìˆ˜ì •: division by zero ë°©ì§€
            pipeline_efficiency = (ai_pipeline_steps / total_ai_steps) if total_ai_steps > 0 else 0.0
            
            return {
                'checkpoints_loaded': successful_checkpoints,
                'total_checkpoints': total_checkpoints,
                'success_rate': success_rate,
                'loading_efficiency': loading_efficiency,
                'total_memory_gb': total_memory_gb,
                'pipeline_efficiency': pipeline_efficiency,
                'ai_models_active': ai_pipeline_steps,
                'overall_score': (success_rate + pipeline_efficiency) / 2,
                'status': 'calculated'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                'checkpoints_loaded': 0,
                'total_checkpoints': 0,
                'success_rate': 0.0,
                'loading_efficiency': 'error',
                'total_memory_gb': 0.0,
                'pipeline_efficiency': 0.0,
                'ai_models_active': 0,
                'overall_score': 0.0,
                'error': str(e),
                'status': 'error'
            }

    def _calculate_directory_size(self, directory: Path) -> float:
        """ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚° (GB)"""
        try:
            if not directory.exists():
                return 0.0
            
            total_size = 0
            for file_path in directory.rglob("*"):
                if file_path.is_file():
                    try:
                        total_size += file_path.stat().st_size
                    except Exception:
                        continue
            
            return round(total_size / (1024 ** 3), 2)  # GB ë‹¨ìœ„
        except Exception:
            return 0.0

    def _check_pytorch_availability(self) -> bool:
        """PyTorch ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return True
        except ImportError:
            return False

    def _check_gpu_availability(self) -> Dict[str, bool]:
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return {
                'cuda': torch.cuda.is_available(),
                'mps': torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False
            }
        except ImportError:
            return {'cuda': False, 'mps': False}

# =============================================================================
# ğŸ”¥ 3. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ìˆ˜ì •
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ìˆ˜ì •ëœ ë²„ì „)"""
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.WARNING,
        format='%(levelname)s: %(message)s',
        force=True
    )
    
    print(f"ğŸ”¥ Ultimate AI Model Loading Debugger v6.1 - ì™„ì „ ìˆ˜ì • ë²„ì „")
    print(f"ğŸ”¥ GitHub í”„ë¡œì íŠ¸: MyCloset AI Pipeline")
    print(f"ğŸ”¥ Target: ëª¨ë“  ì˜¤ë¥˜ ì™„ì „ í•´ê²° + 8ë‹¨ê³„ AI Step + 229GB AI ëª¨ë¸ ì™„ì „ ë¶„ì„")
    print(f"ğŸ”¥ ì‹œì‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”¥ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    
    try:
        # GitHub ë””ë²„ê±° ìƒì„± ë° ì‹¤í–‰
        debugger = UltimateGitHubAIDebuggerV6()
        debug_result = debugger.run_ultimate_github_debugging()
        
        # ì„±ê³µ ì—¬ë¶€ í™•ì¸
        overall_summary = debug_result.get('overall_summary', {})
        ai_ready = overall_summary.get('health', {}).get('ai_pipeline_ready', False)
        system_ready = overall_summary.get('health', {}).get('system_ready', False)
        fixes_applied = overall_summary.get('fixes', {}).get('total_fixes_applied', 0)
        
        if ai_ready and system_ready:
            print(f"\nğŸ‰ SUCCESS: GitHub AI íŒŒì´í”„ë¼ì¸ì´ ì™„ì „ ë³µêµ¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"   - 8ë‹¨ê³„ AI Step ë³µêµ¬ ì™„ë£Œ")
            print(f"   - {fixes_applied}ê°œ ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ")
            print(f"   - StepFileSyntaxFixer ì´ˆê¸°í™” ë¬¸ì œ í•´ê²°")
            print(f"   - ìˆœí™˜ì°¸ì¡° ë° Import ë¬¸ì œ í•´ê²°")
            print(f"   - Virtual Fitting Step ì˜¤ë¥˜ ìˆ˜ì •")
            print(f"   - AI íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë³µêµ¬")
            print(f"   - M3 Max + MPS ìµœì í™” ì ìš©")
        else:
            print(f"\nâœ… IMPROVED: ì£¼ìš” ë¬¸ì œë“¤ì´ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"   - AI íŒŒì´í”„ë¼ì¸: {'âœ…' if ai_ready else 'ğŸ”§ ë¶€ë¶„ í•´ê²°'}")
            print(f"   - ì‹œìŠ¤í…œ í™˜ê²½: {'âœ…' if system_ready else 'ğŸ”§ ë¶€ë¶„ í•´ê²°'}")
            print(f"   - ìˆ˜ì •ëœ ì˜¤ë¥˜: {fixes_applied}ê°œ")
            print(f"   - ì£¼ìš” í´ë˜ìŠ¤ ì´ˆê¸°í™” ë¬¸ì œ í•´ê²°")
            print(f"   - Step íŒŒì¼ syntax error ìˆ˜ì •")
        
        return debug_result
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        print(f"\nâŒ ë””ë²„ê¹… ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        print(f"ì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        return None
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        gc.collect()
        print(f"\nğŸ‘‹ Ultimate GitHub AI Model Debugger v6.1 ì¢…ë£Œ")

if __name__ == "__main__":
    main()