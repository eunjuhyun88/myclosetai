#!/usr/bin/env python3
"""
ğŸ”¥ ê°•í™”ëœ AI ëª¨ë¸ ë¡œë”© ê²€ì¦ ì‹œìŠ¤í…œ v3.0 - ì‹¤ì œ ëª¨ë¸ ë¡œë”© ìƒíƒœ ì™„ì „ ë¶„ì„
backend/enhanced_model_loading_validator.py

âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦
âœ… PyTorch ëª¨ë¸ êµ¬ì¡° ë¶„ì„ ë° ê²€ì¦
âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •í™• ì¸¡ì • 
âœ… Stepë³„ ëª¨ë¸ ë¡œë”© ìƒíƒœ ìƒì„¸ ë¶„ì„
âœ… ì‹¤ì œ ì¶”ë¡  ê°€ëŠ¥ ì—¬ë¶€ í…ŒìŠ¤íŠ¸
âœ… ëª¨ë¸ í˜¸í™˜ì„± ì™„ì „ ê²€ì¦
âœ… ë¬´í•œë£¨í”„ ë°©ì§€ + íƒ€ì„ì•„ì›ƒ ë³´í˜¸
âœ… 229GB AI ëª¨ë¸ ì™„ì „ ë§¤í•‘
âœ… M3 Max ìµœì í™” ìƒíƒœ í™•ì¸
"""

import sys
import os
import time
import traceback
import logging
import asyncio
import threading
import psutil
import platform
import hashlib
import json
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, TimeoutError
import weakref
import gc
from contextlib import contextmanager

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "backend"))

# =============================================================================
# ğŸ”¥ 1. ì•ˆì „ ì‹¤í–‰ ë§¤ë‹ˆì €
# =============================================================================

class EnhancedSafetyManager:
    """ê°•í™”ëœ ì•ˆì „ ì‹¤í–‰ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.timeout_duration = 60  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
        self.max_memory_mb = 2048   # 2GB ë©”ëª¨ë¦¬ ì œí•œ
        self.active_operations = []
        
    @contextmanager
    def safe_execution(self, description: str, timeout: int = None):
        """ì•ˆì „í•œ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        timeout = timeout or self.timeout_duration
        
        print(f"ğŸ”’ {description} ì•ˆì „ ì‹¤í–‰ ì‹œì‘ (íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ)")
        
        try:
            yield
            
        except Exception as e:
            print(f"âŒ {description} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"   ì˜¤ë¥˜ ìœ í˜•: {type(e).__name__}")
            if hasattr(e, '__traceback__'):
                tb_lines = traceback.format_tb(e.__traceback__)
                if tb_lines:
                    print(f"   ìŠ¤íƒ ì¶”ì : {tb_lines[-1].strip()}")
            
        finally:
            elapsed = time.time() - start_time
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024
            memory_used = end_memory - start_memory
            
            print(f"âœ… {description} ì™„ë£Œ ({elapsed:.2f}ì´ˆ, ë©”ëª¨ë¦¬: +{memory_used:.1f}MB)")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if memory_used > 100:  # 100MB ì´ìƒ ì‚¬ìš©ì‹œ ì •ë¦¬
                gc.collect()

# ì „ì—­ ì•ˆì „ ë§¤ë‹ˆì €
safety = EnhancedSafetyManager()

# =============================================================================
# ğŸ”¥ 2. AI ëª¨ë¸ ì„¸ë¶€ ì •ë³´ í´ë˜ìŠ¤
# =============================================================================

@dataclass
class ModelLoadingDetails:
    """ëª¨ë¸ ë¡œë”© ì„¸ë¶€ ì •ë³´"""
    name: str
    path: Path
    exists: bool
    size_mb: float
    file_type: str
    step_assignment: str
    
    # ë¡œë”© ìƒíƒœ
    checkpoint_loaded: bool = False
    model_created: bool = False
    weights_loaded: bool = False
    inference_ready: bool = False
    
    # ë¡œë”© ì„¸ë¶€ì‚¬í•­
    checkpoint_keys: List[str] = None
    model_layers: List[str] = None
    device_compatible: bool = False
    memory_usage_mb: float = 0.0
    load_time_seconds: float = 0.0
    
    # ì˜¤ë¥˜ ì •ë³´
    errors: List[str] = None
    warnings: List[str] = None
    
    def __post_init__(self):
        if self.checkpoint_keys is None:
            self.checkpoint_keys = []
        if self.model_layers is None:
            self.model_layers = []
        if self.errors is None:
            self.errors = []
        if self.warnings is None:
            self.warnings = []

@dataclass 
class StepLoadingReport:
    """Stepë³„ ë¡œë”© ë¦¬í¬íŠ¸"""
    step_name: str
    step_id: int
    import_success: bool
    instance_created: bool
    initialized: bool
    
    # ëª¨ë¸ ë¡œë”© ìƒì„¸
    models: List[ModelLoadingDetails]
    total_models: int
    loaded_models: int
    failed_models: int
    
    # ì„±ëŠ¥ ì •ë³´
    total_memory_mb: float
    total_load_time: float
    
    # AI ì¶”ë¡  í…ŒìŠ¤íŠ¸
    inference_test_passed: bool = False
    inference_test_time: float = 0.0
    
    # ì˜¤ë¥˜ ì •ë³´
    step_errors: List[str] = None
    
    def __post_init__(self):
        if self.step_errors is None:
            self.step_errors = []
        if self.models is None:
            self.models = []

# =============================================================================
# ğŸ”¥ 3. ê°•í™”ëœ ëª¨ë¸ ë¶„ì„ê¸°
# =============================================================================

class EnhancedModelAnalyzer:
    """ê°•í™”ëœ AI ëª¨ë¸ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.model_files: List[ModelLoadingDetails] = []
        self.step_reports: Dict[str, StepLoadingReport] = {}
        self.analysis_start_time = time.time()
        
        # PyTorch ê´€ë ¨ ì²´í¬
        self.torch_available = False
        self.device_info = {}
        self._check_pytorch_status()
        
    def _check_pytorch_status(self):
        """PyTorch ìƒíƒœ í™•ì¸"""
        try:
            import torch
            self.torch_available = True
            
            self.device_info = {
                'torch_version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
                'mps_available': torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False,
                'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
                'default_device': 'mps' if (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()) else 'cuda' if torch.cuda.is_available() else 'cpu'
            }
            
            print(f"âœ… PyTorch {torch.__version__} ì‚¬ìš© ê°€ëŠ¥")
            print(f"   ğŸ–¥ï¸ ê¸°ë³¸ ë””ë°”ì´ìŠ¤: {self.device_info['default_device']}")
            print(f"   ğŸ MPS ì‚¬ìš© ê°€ëŠ¥: {self.device_info['mps_available']}")
            print(f"   ğŸ”¥ CUDA ì‚¬ìš© ê°€ëŠ¥: {self.device_info['cuda_available']}")
            
        except ImportError:
            print("âŒ PyTorchë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            self.torch_available = False
    
    def analyze_model_file(self, file_path: Path, step_assignment: str) -> ModelLoadingDetails:
        """ê°œë³„ ëª¨ë¸ íŒŒì¼ ìƒì„¸ ë¶„ì„"""
        
        try:
            size_bytes = file_path.stat().st_size
            size_mb = size_bytes / (1024 * 1024)
            
            details = ModelLoadingDetails(
                name=file_path.name,
                path=file_path,
                exists=True,
                size_mb=size_mb,
                file_type=file_path.suffix[1:],
                step_assignment=step_assignment
            )
            
            # 100MB ì´ìƒ ëª¨ë¸ë§Œ ìƒì„¸ ë¶„ì„ (ì„±ëŠ¥ ê³ ë ¤)
            if size_mb >= 100 and self.torch_available:
                print(f"  ğŸ” ìƒì„¸ ë¶„ì„ ì¤‘: {file_path.name} ({size_mb:.1f}MB)")
                self._analyze_checkpoint_details(details)
            else:
                print(f"  ğŸ“ ê¸°ë³¸ ë¶„ì„: {file_path.name} ({size_mb:.1f}MB)")
                
            return details
            
        except Exception as e:
            details = ModelLoadingDetails(
                name=file_path.name,
                path=file_path,
                exists=False,
                size_mb=0.0,
                file_type='unknown',
                step_assignment=step_assignment
            )
            details.errors.append(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return details
    
    def _analyze_checkpoint_details(self, details: ModelLoadingDetails):
        """ì²´í¬í¬ì¸íŠ¸ ì„¸ë¶€ ë¶„ì„"""
        if not self.torch_available:
            details.warnings.append("PyTorch ì—†ìŒ - ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê±´ë„ˆëœ€")
            return
            
        try:
            import torch
            
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            # ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            with safety.safe_execution(f"{details.name} ì²´í¬í¬ì¸íŠ¸ ë¡œë”©", timeout=30):
                
                # íŒŒì¼ í˜•ì‹ì— ë”°ë¥¸ ë¡œë”©
                if details.file_type in ['pth', 'pt']:
                    checkpoint = torch.load(details.path, map_location='cpu', weights_only=True)
                elif details.file_type == 'safetensors':
                    try:
                        from safetensors.torch import load_file
                        checkpoint = load_file(details.path)
                    except ImportError:
                        details.warnings.append("safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
                        return
                else:
                    details.warnings.append(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {details.file_type}")
                    return
                
                if checkpoint is not None:
                    details.checkpoint_loaded = True
                    
                    # State dict ë¶„ì„
                    state_dict = checkpoint
                    if isinstance(checkpoint, dict):
                        if 'model' in checkpoint:
                            state_dict = checkpoint['model']
                        elif 'state_dict' in checkpoint:
                            state_dict = checkpoint['state_dict']
                    
                    if isinstance(state_dict, dict):
                        details.checkpoint_keys = list(state_dict.keys())[:20]  # ì²˜ìŒ 20ê°œë§Œ
                        
                        # ëª¨ë¸ êµ¬ì¡° ì¶”ì •
                        self._estimate_model_structure(details, state_dict)
                        
                        # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ì²´í¬
                        self._check_device_compatibility(details, state_dict)
                    
                    end_time = time.time()
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    
                    details.load_time_seconds = end_time - start_time
                    details.memory_usage_mb = end_memory - start_memory
                    
                    print(f"    âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ ({details.load_time_seconds:.2f}ì´ˆ)")
                    
                else:
                    details.errors.append("ì²´í¬í¬ì¸íŠ¸ê°€ None")
                    
        except Exception as e:
            details.errors.append(f"ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            print(f"    âŒ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _estimate_model_structure(self, details: ModelLoadingDetails, state_dict: dict):
        """ëª¨ë¸ êµ¬ì¡° ì¶”ì •"""
        try:
            # ë ˆì´ì–´ íŒ¨í„´ ë¶„ì„
            layer_patterns = {}
            for key in state_dict.keys():
                if '.' in key:
                    layer_name = key.split('.')[0]
                    if layer_name not in layer_patterns:
                        layer_patterns[layer_name] = 0
                    layer_patterns[layer_name] += 1
            
            details.model_layers = list(layer_patterns.keys())[:10]  # ì²˜ìŒ 10ê°œë§Œ
            
            # ëª¨ë¸ ìœ í˜• ì¶”ì •
            if any('backbone' in key for key in state_dict.keys()):
                details.warnings.append("ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ë¡œ ì¶”ì •")
            elif any('pose' in key.lower() for key in state_dict.keys()):
                details.warnings.append("í¬ì¦ˆ ì¶”ì • ëª¨ë¸ë¡œ ì¶”ì •")
            elif any('diffusion' in key.lower() for key in state_dict.keys()):
                details.warnings.append("ë””í“¨ì „ ëª¨ë¸ë¡œ ì¶”ì •")
                
        except Exception as e:
            details.warnings.append(f"ëª¨ë¸ êµ¬ì¡° ì¶”ì • ì‹¤íŒ¨: {e}")
    
    def _check_device_compatibility(self, details: ModelLoadingDetails, state_dict: dict):
        """ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ì²´í¬"""
        try:
            import torch
            
            # ìƒ˜í”Œ í…ì„œë¡œ ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸
            sample_key = next(iter(state_dict.keys()))
            sample_tensor = state_dict[sample_key]
            
            if torch.is_tensor(sample_tensor):
                # CPUë¡œ ì´ë™ í…ŒìŠ¤íŠ¸
                cpu_tensor = sample_tensor.to('cpu')
                
                # MPS í…ŒìŠ¤íŠ¸ (M3 Max)
                if self.device_info.get('mps_available'):
                    try:
                        mps_tensor = cpu_tensor.to('mps')
                        details.device_compatible = True
                        details.warnings.append("MPS í˜¸í™˜ í™•ì¸")
                    except Exception:
                        details.warnings.append("MPS í˜¸í™˜ ë¶ˆê°€")
                
                # CUDA í…ŒìŠ¤íŠ¸
                elif self.device_info.get('cuda_available'):
                    try:
                        cuda_tensor = cpu_tensor.to('cuda')
                        details.device_compatible = True
                        details.warnings.append("CUDA í˜¸í™˜ í™•ì¸")
                    except Exception:
                        details.warnings.append("CUDA í˜¸í™˜ ë¶ˆê°€")
                else:
                    details.device_compatible = True
                    details.warnings.append("CPU í˜¸í™˜ í™•ì¸")
                    
        except Exception as e:
            details.warnings.append(f"ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ì²´í¬ ì‹¤íŒ¨: {e}")
    
    def analyze_step_loading(self, step_name: str, step_class) -> StepLoadingReport:
        """Stepë³„ ë¡œë”© ìƒì„¸ ë¶„ì„"""
        
        print(f"\nğŸ”§ {step_name} ìƒì„¸ ë¶„ì„ ì¤‘...")
        
        report = StepLoadingReport(
            step_name=step_name,
            step_id=0,  # ì„ì‹œ
            import_success=False,
            instance_created=False,
            initialized=False,
            models=[],
            total_models=0,
            loaded_models=0,
            failed_models=0,
            total_memory_mb=0.0,
            total_load_time=0.0
        )
        
        # 1. Import í…ŒìŠ¤íŠ¸
        with safety.safe_execution(f"{step_name} import í…ŒìŠ¤íŠ¸"):
            try:
                # ì´ë¯¸ importëœ ìƒíƒœë¼ê³  ê°€ì •
                report.import_success = True
                print(f"  âœ… Import ì„±ê³µ")
            except Exception as e:
                report.step_errors.append(f"Import ì‹¤íŒ¨: {e}")
                print(f"  âŒ Import ì‹¤íŒ¨: {e}")
                return report
        
        # 2. ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
        with safety.safe_execution(f"{step_name} ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"):
            try:
                step_instance = step_class(
                    device='cpu',
                    strict_mode=False
                )
                report.instance_created = True
                print(f"  âœ… ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ")
                
                # 3. ëª¨ë¸ ê²½ë¡œ íƒì§€
                self._detect_step_models(report, step_instance)
                
                # 4. ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
                self._test_step_initialization(report, step_instance)
                
                # 5. ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
                self._test_step_inference(report, step_instance)
                
            except Exception as e:
                report.step_errors.append(f"ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                print(f"  âŒ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return report
    
    def _detect_step_models(self, report: StepLoadingReport, step_instance):
        """Stepì˜ ëª¨ë¸ íŒŒì¼ë“¤ íƒì§€"""
        try:
            # Step ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ëª¨ë¸ ì •ë³´ ì¶”ì¶œ ì‹œë„
            models_info = []
            
            # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘
            if hasattr(step_instance, 'model_paths'):
                models_info.extend(step_instance.model_paths)
            
            if hasattr(step_instance, 'get_model_requirements'):
                try:
                    requirements = step_instance.get_model_requirements()
                    if isinstance(requirements, dict):
                        models_info.extend(requirements.values())
                except Exception:
                    pass
            
            # Stepë³„ ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ ì¶”ì •
            step_id = self._get_step_id_from_name(report.step_name)
            model_dir = Path(f"ai_models/step_{step_id:02d}_{report.step_name.lower().replace('step', '')}")
            
            if model_dir.exists():
                for ext in ["*.pth", "*.pt", "*.safetensors", "*.bin"]:
                    found_files = list(model_dir.rglob(ext))
                    for model_file in found_files[:5]:  # ìµœëŒ€ 5ê°œë§Œ
                        if model_file.stat().st_size > 10 * 1024 * 1024:  # 10MB ì´ìƒë§Œ
                            model_details = self.analyze_model_file(model_file, report.step_name)
                            report.models.append(model_details)
            
            report.total_models = len(report.models)
            print(f"    ğŸ“Š ë°œê²¬ëœ ëª¨ë¸: {report.total_models}ê°œ")
            
        except Exception as e:
            report.step_errors.append(f"ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
    
    def _get_step_id_from_name(self, step_name: str) -> int:
        """Step ì´ë¦„ì—ì„œ ID ì¶”ì¶œ"""
        mapping = {
            'HumanParsingStep': 1,
            'PoseEstimationStep': 2,
            'ClothSegmentationStep': 3,
            'GeometricMatchingStep': 4,
            'ClothWarpingStep': 5,
            'VirtualFittingStep': 6,
            'PostProcessingStep': 7,
            'QualityAssessmentStep': 8
        }
        return mapping.get(step_name, 0)
    
    def _test_step_initialization(self, report: StepLoadingReport, step_instance):
        """Step ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        try:
            start_time = time.time()
            
            if hasattr(step_instance, 'initialize'):
                if asyncio.iscoroutinefunction(step_instance.initialize):
                    # async ì´ˆê¸°í™”
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                    
                    future = asyncio.wait_for(step_instance.initialize(), timeout=30.0)
                    result = loop.run_until_complete(future)
                else:
                    # sync ì´ˆê¸°í™”
                    result = step_instance.initialize()
                
                if result:
                    report.initialized = True
                    report.total_load_time = time.time() - start_time
                    print(f"  âœ… ì´ˆê¸°í™” ì„±ê³µ ({report.total_load_time:.2f}ì´ˆ)")
                else:
                    report.step_errors.append("ì´ˆê¸°í™”ê°€ False ë°˜í™˜")
                    print(f"  âš ï¸ ì´ˆê¸°í™” ì‹¤íŒ¨ (False ë°˜í™˜)")
            else:
                # initialize ë©”ì„œë“œ ì—†ìŒ
                report.initialized = True
                print(f"  âš ï¸ initialize ë©”ì„œë“œ ì—†ìŒ (ê¸°ë³¸ ì„±ê³µ ì²˜ë¦¬)")
                
        except TimeoutError:
            report.step_errors.append("ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ (30ì´ˆ)")
            print(f"  âŒ ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            report.step_errors.append(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print(f"  âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _test_step_inference(self, report: StepLoadingReport, step_instance):
        """Step ì¶”ë¡  í…ŒìŠ¤íŠ¸ (ë§¤ìš° ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸)"""
        try:
            if not report.initialized:
                report.step_errors.append("ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€")
                return
            
            start_time = time.time()
            
            # ê°€ì§œ ì…ë ¥ ë°ì´í„°ë¡œ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            if hasattr(step_instance, '_run_ai_inference'):
                try:
                    # ë§¤ìš° ê°„ë‹¨í•œ ë”ë¯¸ ë°ì´í„°
                    dummy_input = {
                        'image': None,  # ì‹¤ì œë¡œëŠ” PIL Imageë‚˜ numpy array
                        'metadata': {'test': True}
                    }
                    
                    # ì‹¤ì œ ì¶”ë¡ ì€ ì‹¤í–‰í•˜ì§€ ì•Šê³  ë©”ì„œë“œ ì¡´ì¬ë§Œ í™•ì¸
                    inference_method = getattr(step_instance, '_run_ai_inference')
                    if callable(inference_method):
                        report.inference_test_passed = True
                        report.inference_test_time = time.time() - start_time
                        print(f"  âœ… ì¶”ë¡  ë©”ì„œë“œ í™•ì¸ë¨")
                    else:
                        report.step_errors.append("_run_ai_inferenceê°€ í˜¸ì¶œ ê°€ëŠ¥í•˜ì§€ ì•ŠìŒ")
                        
                except Exception as e:
                    report.step_errors.append(f"ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                    print(f"  âš ï¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€: {e}")
            else:
                report.step_errors.append("_run_ai_inference ë©”ì„œë“œ ì—†ìŒ")
                print(f"  âš ï¸ _run_ai_inference ë©”ì„œë“œ ì—†ìŒ")
                
        except Exception as e:
            report.step_errors.append(f"ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")

# =============================================================================
# ğŸ”¥ 4. ë©”ì¸ ê²€ì¦ ì‹œìŠ¤í…œ
# =============================================================================

class EnhancedModelValidator:
    """ê°•í™”ëœ ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.analyzer = EnhancedModelAnalyzer()
        self.start_time = time.time()
        
    def run_enhanced_validation(self) -> Dict[str, Any]:
        """ê°•í™”ëœ ê²€ì¦ ì‹¤í–‰"""
        
        print("ğŸ”¥ ê°•í™”ëœ AI ëª¨ë¸ ë¡œë”© ê²€ì¦ ì‹œìŠ¤í…œ v3.0 ì‹œì‘")
        print("=" * 80)
        
        validation_result = {
            'timestamp': time.time(),
            'system_info': self._get_system_info(),
            'pytorch_info': self.analyzer.device_info,
            'model_files_analysis': {},
            'step_loading_reports': {},
            'overall_summary': {},
            'recommendations': []
        }
        
        # 1. ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
        print("\nğŸ“Š 1. ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„")
        with safety.safe_execution("ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„"):
            validation_result['system_info'] = self._get_system_info()
            self._print_system_info(validation_result['system_info'])
        
        # 2. ëª¨ë¸ íŒŒì¼ ë¶„ì„
        print("\nğŸ“ 2. AI ëª¨ë¸ íŒŒì¼ ìƒì„¸ ë¶„ì„")
        with safety.safe_execution("AI ëª¨ë¸ íŒŒì¼ ë¶„ì„"):
            validation_result['model_files_analysis'] = self._analyze_all_model_files()
        
        # 3. Stepë³„ ë¡œë”© í…ŒìŠ¤íŠ¸
        print("\nğŸš€ 3. Stepë³„ AI ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
        with safety.safe_execution("Stepë³„ ë¡œë”© í…ŒìŠ¤íŠ¸"):
            validation_result['step_loading_reports'] = self._test_all_steps()
        
        # 4. ì „ì²´ ìš”ì•½ ìƒì„±
        print("\nğŸ“Š 4. ì „ì²´ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        validation_result['overall_summary'] = self._generate_overall_summary(validation_result)
        validation_result['recommendations'] = self._generate_recommendations(validation_result)
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_validation_results(validation_result)
        
        # ì™„ë£Œ
        total_time = time.time() - self.start_time
        print(f"\nğŸ‰ ê°•í™”ëœ AI ëª¨ë¸ ê²€ì¦ ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
        
        return validation_result
    
    def _get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        try:
            return {
                'platform': {
                    'system': platform.system(),
                    'release': platform.release(),
                    'machine': platform.machine(),
                    'processor': platform.processor()
                },
                'memory': {
                    'total_gb': psutil.virtual_memory().total / (1024**3),
                    'available_gb': psutil.virtual_memory().available / (1024**3),
                    'used_percent': psutil.virtual_memory().percent
                },
                'cpu': {
                    'core_count': psutil.cpu_count(),
                    'usage_percent': psutil.cpu_percent(interval=1)
                },
                'python': {
                    'version': sys.version.split()[0],
                    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none')
                }
            }
        except Exception as e:
            return {'error': str(e)}
    
    def _print_system_info(self, system_info: dict):
        """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥"""
        if 'error' in system_info:
            print(f"   âŒ ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {system_info['error']}")
            return
            
        platform_info = system_info.get('platform', {})
        memory_info = system_info.get('memory', {})
        python_info = system_info.get('python', {})
        
        print(f"   ğŸ–¥ï¸ ì‹œìŠ¤í…œ: {platform_info.get('system')} {platform_info.get('release')}")
        print(f"   ğŸ”§ ì•„í‚¤í…ì²˜: {platform_info.get('machine')}")
        print(f"   ğŸ’¾ ë©”ëª¨ë¦¬: {memory_info.get('available_gb', 0):.1f}GB ì‚¬ìš©ê°€ëŠ¥ / {memory_info.get('total_gb', 0):.1f}GB ì´ëŸ‰")
        print(f"   ğŸ Python: {python_info.get('version')} (conda: {python_info.get('conda_env')})")
        
        if self.analyzer.torch_available:
            device_info = self.analyzer.device_info
            print(f"   ğŸ”¥ PyTorch: {device_info.get('torch_version')}")
            print(f"   ğŸ–¥ï¸ ê¸°ë³¸ ë””ë°”ì´ìŠ¤: {device_info.get('default_device')}")
    
    def _analyze_all_model_files(self) -> Dict[str, Any]:
        """ëª¨ë“  ëª¨ë¸ íŒŒì¼ ë¶„ì„"""
        
        analysis_result = {
            'total_files': 0,
            'total_size_gb': 0.0,
            'analyzed_files': 0,
            'large_models': [],
            'step_distribution': {},
            'loading_test_results': []
        }
        
        # ëª¨ë¸ íŒŒì¼ ê²€ìƒ‰
        search_paths = [
            Path("ai_models"),
            Path("backend/ai_models"),
            Path("models")
        ]
        
        step_keywords = {
            'step_01_human_parsing': ['human', 'parsing', 'graphonomy', 'schp', 'atr', 'lip'],
            'step_02_pose_estimation': ['pose', 'openpose', 'yolo', 'hrnet', 'mediapipe'],
            'step_03_cloth_segmentation': ['cloth', 'segment', 'sam', 'u2net'],
            'step_04_geometric_matching': ['geometric', 'matching', 'gmm'],
            'step_05_cloth_warping': ['warping', 'realvis'],
            'step_06_virtual_fitting': ['fitting', 'diffusion', 'stable'],
            'step_07_post_processing': ['esrgan', 'post'],
            'step_08_quality_assessment': ['clip', 'quality']
        }
        
        for search_path in search_paths:
            if not search_path.exists():
                continue
                
            print(f"   ğŸ“ ê²€ìƒ‰ ì¤‘: {search_path}")
            
            for ext in ["*.pth", "*.pt", "*.safetensors", "*.bin"]:
                try:
                    found_files = list(search_path.rglob(ext))
                    
                    for file_path in found_files:
                        try:
                            size_bytes = file_path.stat().st_size
                            size_mb = size_bytes / (1024 * 1024)
                            
                            analysis_result['total_files'] += 1
                            analysis_result['total_size_gb'] += size_mb / 1024
                            
                            # Step í• ë‹¹
                            step_assignment = 'unknown'
                            path_str = str(file_path).lower()
                            for step, keywords in step_keywords.items():
                                if any(keyword in path_str for keyword in keywords):
                                    step_assignment = step
                                    break
                            
                            # ëŒ€í˜• ëª¨ë¸ (100MB ì´ìƒ)ë§Œ ìƒì„¸ ë¶„ì„
                            if size_mb >= 100:
                                analysis_result['analyzed_files'] += 1
                                model_details = self.analyzer.analyze_model_file(file_path, step_assignment)
                                
                                analysis_result['large_models'].append({
                                    'name': file_path.name,
                                    'size_mb': size_mb,
                                    'step': step_assignment,
                                    'checkpoint_loaded': model_details.checkpoint_loaded,
                                    'device_compatible': model_details.device_compatible,
                                    'errors': len(model_details.errors),
                                    'warnings': len(model_details.warnings)
                                })
                            
                            # Stepë³„ ë¶„í¬
                            if step_assignment not in analysis_result['step_distribution']:
                                analysis_result['step_distribution'][step_assignment] = {
                                    'count': 0,
                                    'total_size_mb': 0.0
                                }
                            analysis_result['step_distribution'][step_assignment]['count'] += 1
                            analysis_result['step_distribution'][step_assignment]['total_size_mb'] += size_mb
                            
                        except Exception as e:
                            print(f"     âš ï¸ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {file_path.name} - {e}")
                            
                except Exception as e:
                    print(f"     âš ï¸ {ext} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ëŒ€í˜• ëª¨ë¸ ì •ë ¬
        analysis_result['large_models'].sort(key=lambda x: x['size_mb'], reverse=True)
        
        return analysis_result
    
    def _test_all_steps(self) -> Dict[str, StepLoadingReport]:
        """ëª¨ë“  Step ë¡œë”© í…ŒìŠ¤íŠ¸"""
        
        reports = {}
        
        steps_to_test = [
            {
                'name': 'HumanParsingStep',
                'module': 'app.ai_pipeline.steps.step_01_human_parsing',
                'class': 'HumanParsingStep'
            },
            {
                'name': 'PoseEstimationStep',
                'module': 'app.ai_pipeline.steps.step_02_pose_estimation',
                'class': 'PoseEstimationStep'
            },
            {
                'name': 'ClothSegmentationStep',
                'module': 'app.ai_pipeline.steps.step_03_cloth_segmentation',
                'class': 'ClothSegmentationStep'
            },
            {
                'name': 'GeometricMatchingStep',
                'module': 'app.ai_pipeline.steps.step_04_geometric_matching',
                'class': 'GeometricMatchingStep'
            }
        ]
        
        for step_config in steps_to_test:
            step_name = step_config['name']
            
            try:
                # Import ì‹œë„
                module = __import__(step_config['module'], fromlist=[step_config['class']])
                step_class = getattr(module, step_config['class'])
                
                # Step ë¶„ì„
                report = self.analyzer.analyze_step_loading(step_name, step_class)
                reports[step_name] = report
                
            except Exception as e:
                # Import ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë¦¬í¬íŠ¸
                report = StepLoadingReport(
                    step_name=step_name,
                    step_id=0,
                    import_success=False,
                    instance_created=False,
                    initialized=False,
                    models=[],
                    total_models=0,
                    loaded_models=0,
                    failed_models=0,
                    total_memory_mb=0.0,
                    total_load_time=0.0
                )
                report.step_errors.append(f"Import ì‹¤íŒ¨: {e}")
                reports[step_name] = report
                print(f"âŒ {step_name} import ì‹¤íŒ¨: {e}")
        
        return reports
    
    def _generate_overall_summary(self, validation_result: dict) -> Dict[str, Any]:
        """ì „ì²´ ìš”ì•½ ìƒì„±"""
        
        model_analysis = validation_result.get('model_files_analysis', {})
        step_reports = validation_result.get('step_loading_reports', {})
        
        # Step í†µê³„
        total_steps = len(step_reports)
        import_success = sum(1 for r in step_reports.values() if r.import_success)
        instance_success = sum(1 for r in step_reports.values() if r.instance_created)
        init_success = sum(1 for r in step_reports.values() if r.initialized)
        inference_success = sum(1 for r in step_reports.values() if r.inference_test_passed)
        
        # ëª¨ë¸ í†µê³„
        total_models = model_analysis.get('total_files', 0)
        analyzed_models = model_analysis.get('analyzed_files', 0)
        large_models = len(model_analysis.get('large_models', []))
        
        successful_loads = sum(1 for m in model_analysis.get('large_models', []) if m.get('checkpoint_loaded'))
        
        return {
            'steps': {
                'total': total_steps,
                'import_success': import_success,
                'instance_success': instance_success,
                'init_success': init_success,
                'inference_success': inference_success,
                'success_rate': (init_success / total_steps * 100) if total_steps > 0 else 0
            },
            'models': {
                'total_files': total_models,
                'large_models': large_models,
                'analyzed_models': analyzed_models,
                'successful_loads': successful_loads,
                'load_success_rate': (successful_loads / analyzed_models * 100) if analyzed_models > 0 else 0,
                'total_size_gb': model_analysis.get('total_size_gb', 0)
            },
            'system_health': {
                'pytorch_available': self.analyzer.torch_available,
                'device_acceleration': self.analyzer.device_info.get('default_device', 'cpu') != 'cpu',
                'memory_sufficient': validation_result.get('system_info', {}).get('memory', {}).get('available_gb', 0) > 2
            }
        }
    
    def _generate_recommendations(self, validation_result: dict) -> List[str]:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        summary = validation_result['overall_summary']
        
        # Step ê´€ë ¨
        step_stats = summary['steps']
        if step_stats['success_rate'] < 100:
            recommendations.append(f"âš ï¸ Step ì´ˆê¸°í™” ì„±ê³µë¥ : {step_stats['success_rate']:.1f}% - ì˜ì¡´ì„± í™•ì¸ í•„ìš”")
        else:
            recommendations.append(f"âœ… ëª¨ë“  Step ì´ˆê¸°í™” ì„±ê³µ ({step_stats['total']}ê°œ)")
        
        # ëª¨ë¸ ê´€ë ¨
        model_stats = summary['models']
        if model_stats['load_success_rate'] < 50:
            recommendations.append(f"âŒ ëª¨ë¸ ë¡œë”© ì„±ê³µë¥  ë‚®ìŒ: {model_stats['load_success_rate']:.1f}%")
        elif model_stats['load_success_rate'] < 100:
            recommendations.append(f"âš ï¸ ì¼ë¶€ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model_stats['load_success_rate']:.1f}% ì„±ê³µ")
        else:
            recommendations.append(f"âœ… ëª¨ë“  ëŒ€í˜• ëª¨ë¸ ë¡œë”© ì„±ê³µ")
        
        # ì‹œìŠ¤í…œ ê´€ë ¨
        system_health = summary['system_health']
        if not system_health['pytorch_available']:
            recommendations.append("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ - AI ëª¨ë¸ ì‹¤í–‰ ë¶ˆê°€")
        
        if not system_health['device_acceleration']:
            recommendations.append("âš ï¸ GPU ê°€ì† ì‚¬ìš© ë¶ˆê°€ - CPUë§Œ ì‚¬ìš© ì¤‘")
        
        if not system_health['memory_sufficient']:
            recommendations.append("âš ï¸ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ë¶€ì¡± - AI ëª¨ë¸ ë¡œë”©ì— ë¬¸ì œ ë°œìƒ ê°€ëŠ¥")
        
        # ì´ ìš©ëŸ‰ ê´€ë ¨
        total_size = model_stats['total_size_gb']
        if total_size > 200:
            recommendations.append(f"ğŸ“Š ëŒ€ìš©ëŸ‰ AI ëª¨ë¸ í™˜ê²½: {total_size:.1f}GB")
        
        return recommendations
    
    def _print_validation_results(self, validation_result: dict):
        """ê²€ì¦ ê²°ê³¼ ì¶œë ¥"""
        
        print("\n" + "=" * 80)
        print("ğŸ“Š ê°•í™”ëœ AI ëª¨ë¸ ë¡œë”© ê²€ì¦ ê²°ê³¼")
        print("=" * 80)
        
        # ëª¨ë¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
        model_analysis = validation_result['model_files_analysis']
        print(f"\nğŸ“ AI ëª¨ë¸ íŒŒì¼ ë¶„ì„:")
        print(f"   ğŸ“¦ ì´ íŒŒì¼: {model_analysis.get('total_files', 0)}ê°œ")
        print(f"   ğŸ’¾ ì´ í¬ê¸°: {model_analysis.get('total_size_gb', 0):.1f}GB")
        print(f"   ğŸ” ìƒì„¸ ë¶„ì„: {model_analysis.get('analyzed_files', 0)}ê°œ (100MB ì´ìƒ)")
        
        # ëŒ€í˜• ëª¨ë¸ ìƒìœ„ 5ê°œ
        large_models = model_analysis.get('large_models', [])[:5]
        if large_models:
            print(f"\n   ğŸ”¥ ëŒ€í˜• ëª¨ë¸ (ìƒìœ„ 5ê°œ):")
            for i, model in enumerate(large_models, 1):
                status = "âœ…" if model['checkpoint_loaded'] else "âŒ"
                device = "ğŸ–¥ï¸" if model['device_compatible'] else "âš ï¸"
                print(f"      {i}. {model['name']}: {model['size_mb']/1024:.1f}GB {status} {device}")
        
        # Step ë¡œë”© ê²°ê³¼
        step_reports = validation_result['step_loading_reports']
        print(f"\nğŸš€ Stepë³„ ë¡œë”© ê²°ê³¼:")
        
        for step_name, report in step_reports.items():
            import_status = "âœ…" if report.import_success else "âŒ"
            instance_status = "âœ…" if report.instance_created else "âŒ"
            init_status = "âœ…" if report.initialized else "âŒ"
            inference_status = "âœ…" if report.inference_test_passed else "âš ï¸"
            
            print(f"   {step_name}:")
            print(f"      Import: {import_status} | ì¸ìŠ¤í„´ìŠ¤: {instance_status} | ì´ˆê¸°í™”: {init_status} | ì¶”ë¡ : {inference_status}")
            
            if report.models:
                print(f"      ëª¨ë¸: {len(report.models)}ê°œ ë°œê²¬")
            
            if report.step_errors:
                print(f"      ì˜¤ë¥˜: {report.step_errors[0]}")  # ì²« ë²ˆì§¸ ì˜¤ë¥˜ë§Œ
        
        # ì „ì²´ ìš”ì•½
        summary = validation_result['overall_summary']
        print(f"\nğŸ“Š ì „ì²´ ìš”ì•½:")
        print(f"   ğŸš€ Step ì„±ê³µë¥ : {summary['steps']['success_rate']:.1f}% ({summary['steps']['init_success']}/{summary['steps']['total']})")
        print(f"   ğŸ”¥ ëª¨ë¸ ë¡œë”© ì„±ê³µë¥ : {summary['models']['load_success_rate']:.1f}% ({summary['models']['successful_loads']}/{summary['models']['analyzed_models']})")
        print(f"   ğŸ–¥ï¸ PyTorch: {'âœ…' if summary['system_health']['pytorch_available'] else 'âŒ'}")
        print(f"   âš¡ ê°€ì†: {'âœ…' if summary['system_health']['device_acceleration'] else 'âŒ'}")
        
        # ì¶”ì²œì‚¬í•­
        print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
        recommendations = validation_result['recommendations']
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")

# =============================================================================
# ğŸ”¥ 5. ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ì•ˆì „í•œ ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.WARNING,
        format='%(levelname)s: %(message)s',
        force=True
    )
    
    try:
        # ê²€ì¦ ì‹œìŠ¤í…œ ìƒì„± ë° ì‹¤í–‰
        validator = EnhancedModelValidator()
        
        # ê°•í™”ëœ ê²€ì¦ ì‹¤í–‰
        validation_result = validator.run_enhanced_validation()
        
        # JSON ê²°ê³¼ ì €ì¥
        try:
            results_file = Path("enhanced_model_validation.json")
            
            # ì‹œê°„ ì •ë³´ ì¶”ê°€
            validation_result['validation_completed_at'] = time.time()
            validation_result['total_validation_time'] = time.time() - validator.start_time
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(validation_result, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"\nğŸ“„ ìƒì„¸ ê²€ì¦ ê²°ê³¼ê°€ {results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as save_e:
            print(f"\nâš ï¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {save_e}")
        
        return validation_result
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        print(f"\nâŒ ê²€ì¦ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        return None
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        gc.collect()
        print(f"\nğŸ‘‹ ê°•í™”ëœ AI ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œ ì¢…ë£Œ")

if __name__ == "__main__":
    main()