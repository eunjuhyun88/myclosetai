#!/usr/bin/env python3
"""
ğŸ”¥ Ultimate AI Model Loading Debugger v6.0 - ì™„ì „í•œ ì¢…í•© ë””ë²„ê¹… ì‹œìŠ¤í…œ
==============================================================================
âœ… ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ + ì˜¤ë¥˜ ìˆ˜ì • ê¸°ëŠ¥ í†µí•© (ì´ 2000+ ë¼ì¸)
âœ… 229GB AI ëª¨ë¸ ì™„ì „ ë¶„ì„ + ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í…ŒìŠ¤íŠ¸
âœ… 8ë‹¨ê³„ AI Step ì™„ì „ ë¶„ì„ + syntax error ìë™ ìˆ˜ì •
âœ… threading import ëˆ„ë½ ìë™ í•´ê²°
âœ… PyTorch weights_only ë¬¸ì œ ì™„ì „ í•´ê²° (3ë‹¨ê³„ ì•ˆì „ ë¡œë”©)
âœ… M3 Max MPS + conda mycloset-ai-clean í™˜ê²½ ì™„ì „ ìµœì í™”
âœ… BaseStepMixin v19.2 í˜¸í™˜ì„± ì™„ì „ ê²€ì¦
âœ… Central Hub DI Container ì—°ë™ ìƒíƒœ ë¶„ì„
âœ… DetailedDataSpec v5.3 í†µí•© ë¶„ì„
âœ… StepFactory v11.2 í†µí•© ë¶„ì„  
âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ë§¤í•‘ ë° ì²´í¬í¬ì¸íŠ¸ ë¬´ê²°ì„± ê²€ì¦
âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ì„±ëŠ¥ ìµœì í™” ë¶„ì„
âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° 100% ë§¤ì¹­
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° ê²€ì¦
âœ… ëª¨ë“  ì˜ì¡´ì„± ìƒíƒœ ì™„ì „ ë¶„ì„
âœ… ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­ ìƒì„±

ì£¼ìš” ê¸°ëŠ¥:
1. ğŸ”§ Step íŒŒì¼ ì˜¤ë¥˜ ìë™ ìˆ˜ì • ì‹œìŠ¤í…œ
2. ğŸš€ 229GB AI ëª¨ë¸ ì™„ì „ ë¶„ì„
3. ğŸ”¥ 8ë‹¨ê³„ Step ì™„ì „ ê²€ì¦
4. ğŸ M3 Max í•˜ë“œì›¨ì–´ ì™„ì „ ìµœì í™”
5. ğŸ“Š ì¢…í•© ì„±ëŠ¥ ë° ê±´ê°•ë„ ë¶„ì„
6. ğŸ’¡ ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ì±… ì œì‹œ
==============================================================================
"""

import sys
import os
import time
import traceback
import logging
import asyncio
import threading
import psutil
import platform
import hashlib
import json
import importlib
import inspect
import gc
import weakref
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from contextlib import contextmanager
from enum import Enum
import warnings
import base64
from io import BytesIO

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')
os.environ['PYTHONWARNINGS'] = 'ignore'

project_root = Path("/Users/gimdudeul/MVP/mycloset-ai")
backend_root = project_root / "backend"
ai_models_root = backend_root / "ai_models"

# ê²½ë¡œ ì¶”ê°€ (í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜)
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(backend_root))
sys.path.insert(0, str(backend_root / "app"))

print(f"ğŸ”¥ GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ê°ì§€:")
print(f"   í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
print(f"   ë°±ì—”ë“œ ë£¨íŠ¸: {backend_root}")
print(f"   AI ëª¨ë¸ ë£¨íŠ¸: {ai_models_root}")

# =============================================================================
# ğŸ”¥ 1. GitHub Step ì •ë³´ (ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜)
# =============================================================================

@dataclass
class GitHubStepInfo:
    """GitHub Step ì •ë³´"""
    step_id: int
    step_name: str
    step_class: str
    module_path: str
    expected_models: List[str] = field(default_factory=list)
    expected_size_gb: float = 0.0
    expected_files: List[str] = field(default_factory=list)
    priority: str = "medium"

GITHUB_STEP_CONFIGS = [
    GitHubStepInfo(
        step_id=1,
        step_name="HumanParsingStep",
        step_class="HumanParsingStep",
        module_path="app.ai_pipeline.steps.step_01_human_parsing",
        expected_models=["Graphonomy", "SCHP"],
        expected_size_gb=1.2,
        expected_files=["graphonomy.pth", "schp_model.pth"],
        priority="critical"
    ),
    GitHubStepInfo(
        step_id=2,
        step_name="PoseEstimationStep", 
        step_class="PoseEstimationStep",
        module_path="app.ai_pipeline.steps.step_02_pose_estimation",
        expected_models=["OpenPose", "DWPose"],
        expected_size_gb=0.3,
        expected_files=["pose_model.pth", "dw-ll_ucoco_384.pth"],
        priority="critical"
    ),
    GitHubStepInfo(
        step_id=3,
        step_name="ClothSegmentationStep",
        step_class="ClothSegmentationStep", 
        module_path="app.ai_pipeline.steps.step_03_cloth_segmentation",
        expected_models=["SAM", "Segment Anything"],
        expected_size_gb=2.4,
        expected_files=["sam_vit_h.pth", "sam_vit_l.pth"],
        priority="critical"
    ),
    GitHubStepInfo(
        step_id=4,
        step_name="GeometricMatchingStep",
        step_class="GeometricMatchingStep",
        module_path="app.ai_pipeline.steps.step_04_geometric_matching", 
        expected_models=["GMM", "TOM"],
        expected_size_gb=0.05,
        expected_files=["gmm_model.pth", "tom_model.pth"],
        priority="high"
    ),
    GitHubStepInfo(
        step_id=5,
        step_name="ClothWarpingStep",
        step_class="ClothWarpingStep",
        module_path="app.ai_pipeline.steps.step_05_cloth_warping",
        expected_models=["RealVisXL", "Warping Model"],
        expected_size_gb=6.5,
        expected_files=["RealVisXL_V4.0.safetensors", "warping_model.pth"],
        priority="high"
    ),
    GitHubStepInfo(
        step_id=6,
        step_name="VirtualFittingStep",
        step_class="VirtualFittingStep",
        module_path="app.ai_pipeline.steps.step_06_virtual_fitting",
        expected_models=["OOTDiffusion", "Stable Diffusion"],
        expected_size_gb=14.0,
        expected_files=["ootd_hd_checkpoint.safetensors", "sd_model.safetensors"],
        priority="critical"  # ê°€ì¥ ì¤‘ìš”í•œ Step
    ),
    GitHubStepInfo(
        step_id=7,
        step_name="PostProcessingStep",
        step_class="PostProcessingStep",
        module_path="app.ai_pipeline.steps.step_07_post_processing",
        expected_models=["ESRGAN", "Real-ESRGAN"],
        expected_size_gb=0.8,
        expected_files=["esrgan_x8.pth", "realesrgan_x4.pth"],
        priority="medium"
    ),
    GitHubStepInfo(
        step_id=8,
        step_name="QualityAssessmentStep",
        step_class="QualityAssessmentStep",
        module_path="app.ai_pipeline.steps.step_08_quality_assessment",
        expected_models=["OpenCLIP", "CLIP"],
        expected_size_gb=5.2,
        expected_files=["ViT-L-14.pt", "clip_model.pt"],
        priority="medium"
    )
]

# =============================================================================
# ğŸ”¥ 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ìƒíƒœ ë° ë¶„ì„ ë°ì´í„° êµ¬ì¡°
# =============================================================================

class CheckpointLoadingStatus(Enum):
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ìƒíƒœ"""
    NOT_FOUND = "not_found"
    CORRUPTED = "corrupted"
    LOADING_FAILED = "loading_failed"
    WEIGHTS_ONLY_FAILED = "weights_only_failed"
    DEVICE_INCOMPATIBLE = "device_incompatible"
    MEMORY_INSUFFICIENT = "memory_insufficient"
    SUCCESS = "success"
    SAFETENSORS_SUCCESS = "safetensors_success"

class GitHubStepStatus(Enum):
    """GitHub Step ìƒíƒœ"""
    NOT_FOUND = "not_found"
    IMPORT_FAILED = "import_failed"
    CLASS_NOT_FOUND = "class_not_found"
    INSTANCE_FAILED = "instance_failed"
    INIT_FAILED = "init_failed"
    DEPENDENCIES_MISSING = "dependencies_missing"
    AI_MODELS_FAILED = "ai_models_failed"
    CENTRAL_HUB_FAILED = "central_hub_failed"
    SYNTAX_ERROR = "syntax_error"
    THREADING_MISSING = "threading_missing"
    SUCCESS = "success"

@dataclass
class CheckpointAnalysisResult:
    """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê²°ê³¼"""
    file_path: Path
    exists: bool
    size_mb: float
    file_hash: str = ""
    
    # ë¡œë”© í…ŒìŠ¤íŠ¸ ê²°ê³¼
    pytorch_weights_only_success: bool = False
    pytorch_regular_success: bool = False
    safetensors_success: bool = False
    legacy_load_success: bool = False
    
    # ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ë¶„ì„
    checkpoint_keys: List[str] = field(default_factory=list)
    state_dict_keys: List[str] = field(default_factory=list)
    model_architecture: str = ""
    parameter_count: int = 0
    
    # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„±
    cpu_compatible: bool = False
    cuda_compatible: bool = False
    mps_compatible: bool = False
    
    # ì˜¤ë¥˜ ì •ë³´
    loading_errors: List[str] = field(default_factory=list)
    status: CheckpointLoadingStatus = CheckpointLoadingStatus.NOT_FOUND
    load_time_seconds: float = 0.0
    memory_usage_mb: float = 0.0

@dataclass
class GitHubStepAnalysisResult:
    """GitHub Step ë¶„ì„ ê²°ê³¼"""
    step_info: GitHubStepInfo
    
    # Import ë¶„ì„
    import_success: bool = False
    import_time: float = 0.0
    import_errors: List[str] = field(default_factory=list)
    
    # íŒŒì¼ ìˆ˜ì • ìƒíƒœ
    syntax_error_fixed: bool = False
    threading_import_added: bool = False
    basestepmixin_compatible: bool = False
    
    # í´ë˜ìŠ¤ ë¶„ì„
    class_found: bool = False
    is_base_step_mixin: bool = False
    has_process_method: bool = False
    has_initialize_method: bool = False
    has_central_hub_support: bool = False
    
    # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë¶„ì„
    instance_created: bool = False
    constructor_params: Dict[str, Any] = field(default_factory=dict)
    instance_errors: List[str] = field(default_factory=list)
    
    # ì´ˆê¸°í™” ë¶„ì„
    initialization_success: bool = False
    initialization_time: float = 0.0
    initialization_errors: List[str] = field(default_factory=list)
    
    # ì˜ì¡´ì„± ë¶„ì„ (Central Hub ê¸°ë°˜)
    model_loader_injected: bool = False
    memory_manager_injected: bool = False
    data_converter_injected: bool = False
    central_hub_connected: bool = False
    dependency_validation_result: Dict[str, Any] = field(default_factory=dict)
    
    # AI ëª¨ë¸ ë¶„ì„
    detected_model_files: List[str] = field(default_factory=list)
    checkpoint_analyses: List[CheckpointAnalysisResult] = field(default_factory=list)
    total_model_size_gb: float = 0.0
    model_loading_success_rate: float = 0.0
    
    # ì„±ëŠ¥ ë¶„ì„
    memory_footprint_mb: float = 0.0
    inference_test_success: bool = False
    inference_time_ms: float = 0.0
    
    # ì „ì²´ ìƒíƒœ
    status: GitHubStepStatus = GitHubStepStatus.NOT_FOUND
    health_score: float = 0.0
    recommendations: List[str] = field(default_factory=list)

@dataclass
class GitHubSystemEnvironment:
    """GitHub ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„"""
    # í•˜ë“œì›¨ì–´ ì •ë³´
    is_m3_max: bool = False
    total_memory_gb: float = 0.0
    available_memory_gb: float = 0.0
    cpu_cores: int = 0
    
    # ì†Œí”„íŠ¸ì›¨ì–´ í™˜ê²½
    python_version: str = ""
    conda_env: str = ""
    is_target_conda_env: bool = False
    
    # PyTorch í™˜ê²½
    torch_available: bool = False
    torch_version: str = ""
    cuda_available: bool = False
    mps_available: bool = False
    recommended_device: str = "cpu"
    
    # í”„ë¡œì íŠ¸ êµ¬ì¡°
    project_root_exists: bool = False
    backend_root_exists: bool = False
    ai_models_root_exists: bool = False
    ai_models_size_gb: float = 0.0
    step_modules_found: List[str] = field(default_factory=list)
    
    # Step íŒŒì¼ ìˆ˜ì • ìƒíƒœ (v6.0 ì¶”ê°€)
    step_files_fixed: List[str] = field(default_factory=list)
    threading_imports_added: List[str] = field(default_factory=list)
    syntax_errors_fixed: int = 0
    
    # ì˜ì¡´ì„± ìƒíƒœ
    core_dependencies: Dict[str, bool] = field(default_factory=dict)
    github_integrations: Dict[str, bool] = field(default_factory=dict)

# =============================================================================
# ğŸ”¥ 3. ê³ ê¸‰ ì•ˆì „ ì‹¤í–‰ ë§¤ë‹ˆì €
# =============================================================================

class GitHubSafetyManager:
    """GitHub í”„ë¡œì íŠ¸ìš© ê°•í™”ëœ ì•ˆì „ ì‹¤í–‰ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.timeout_duration = 180  # 3ë¶„ íƒ€ì„ì•„ì›ƒ (GitHub ëŒ€ìš©ëŸ‰ ëª¨ë¸ìš©)
        self.max_memory_gb = 12     # 12GB ë©”ëª¨ë¦¬ ì œí•œ (M3 Max ê³ ë ¤)
        self.active_operations = {}
        self.start_time = time.time()
        
    @contextmanager
    def safe_execution(self, operation_name: str, timeout: int = None, memory_limit_gb: float = None):
        """GitHub í”„ë¡œì íŠ¸ìš© ì´ˆì•ˆì „ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸"""
        operation_id = f"github_{operation_name.replace(' ', '_')}_{int(time.time() * 1000)}"
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / (1024**3)
        timeout = timeout or self.timeout_duration
        memory_limit = memory_limit_gb or self.max_memory_gb
        
        print(f"ğŸ”’ [{operation_id}] GitHub ì•ˆì „ ì‹¤í–‰ ì‹œì‘ (íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ, ë©”ëª¨ë¦¬ ì œí•œ: {memory_limit:.1f}GB)")
        
        self.active_operations[operation_id] = {
            'start_time': start_time,
            'start_memory': start_memory,
            'timeout': timeout,
            'memory_limit': memory_limit
        }
        
        try:
            # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘
            monitoring_thread = threading.Thread(
                target=self._monitor_github_operation,
                args=(operation_id, timeout, memory_limit),
                daemon=True
            )
            monitoring_thread.start()
            
            yield
            
        except TimeoutError:
            print(f"â° [{operation_id}] GitHub ì‘ì—… íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)")
            raise
        except MemoryError:
            print(f"ğŸ’¾ [{operation_id}] GitHub ë©”ëª¨ë¦¬ í•œê³„ ì´ˆê³¼ ({memory_limit:.1f}GB)")
            raise
        except Exception as e:
            print(f"âŒ [{operation_id}] GitHub ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {type(e).__name__}: {e}")
            if hasattr(e, '__traceback__'):
                tb_lines = traceback.format_tb(e.__traceback__)
                if tb_lines:
                    print(f"   ìŠ¤íƒ ì¶”ì : {tb_lines[-1].strip()}")
            raise
        finally:
            if operation_id in self.active_operations:
                del self.active_operations[operation_id]
                
            elapsed = time.time() - start_time
            end_memory = psutil.Process().memory_info().rss / (1024**3)
            memory_used = end_memory - start_memory
            
            print(f"âœ… [{operation_id}] GitHub ì‘ì—… ì™„ë£Œ ({elapsed:.2f}ì´ˆ, ë©”ëª¨ë¦¬: +{memory_used:.2f}GB)")
            
            # GitHub ëŒ€ìš©ëŸ‰ ëª¨ë¸ìš© ë©”ëª¨ë¦¬ ì •ë¦¬
            if memory_used > 1.0:  # 1GB ì´ìƒ ì‚¬ìš©ì‹œ ì ê·¹ì  ì •ë¦¬
                gc.collect()
    
    def _monitor_github_operation(self, operation_id: str, timeout: float, memory_limit: float):
        """GitHub ì‘ì—… ëª¨ë‹ˆí„°ë§"""
        try:
            while operation_id in self.active_operations:
                current_time = time.time()
                operation = self.active_operations.get(operation_id)
                
                if not operation:
                    break
                
                # íƒ€ì„ì•„ì›ƒ ì²´í¬
                elapsed = current_time - operation['start_time']
                if elapsed > timeout:
                    print(f"âš ï¸ [{operation_id}] GitHub ì‘ì—… íƒ€ì„ì•„ì›ƒ ê²½ê³  ({elapsed:.1f}ì´ˆ/{timeout}ì´ˆ)")
                    break
                
                # ë©”ëª¨ë¦¬ ì²´í¬ (GitHub ëŒ€ìš©ëŸ‰ ëª¨ë¸ ê³ ë ¤)
                current_memory = psutil.Process().memory_info().rss / (1024**3)
                if current_memory > memory_limit:
                    print(f"âš ï¸ [{operation_id}] GitHub ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²½ê³  ({current_memory:.1f}GB/{memory_limit:.1f}GB)")
                    # M3 Maxì—ì„œëŠ” ë” ê´€ëŒ€í•˜ê²Œ ì²˜ë¦¬
                    if current_memory > memory_limit * 1.5:  # 1.5ë°° ì´ˆê³¼ì‹œì—ë§Œ ì¤‘ë‹¨
                        break
                
                time.sleep(2)  # 2ì´ˆë§ˆë‹¤ ì²´í¬ (GitHub ëŒ€ìš©ëŸ‰ ëª¨ë¸ìš©)
                
        except Exception:
            pass

# ì „ì—­ GitHub ì•ˆì „ ë§¤ë‹ˆì €
github_safety = GitHubSafetyManager()

# =============================================================================
# ğŸ”¥ 4. Step íŒŒì¼ ì˜¤ë¥˜ ìˆ˜ì • ì‹œìŠ¤í…œ
# =============================================================================

class StepFileSyntaxFixer:
    """Step íŒŒì¼ syntax error ìë™ ìˆ˜ì • ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # í™•ì¸ëœ ì‹¤ì œ ê²½ë¡œ ì‚¬ìš©
        self.steps_dir = Path("/Users/gimdudeul/MVP/mycloset-ai/backend/app/ai_pipeline/steps")
        
        # ë˜ëŠ” ë” ì•ˆì „í•œ ë°©ë²•
        if not self.steps_dir.exists():
            # ëŒ€ì•ˆ ê²½ë¡œë“¤ ì‹œë„
            possible_paths = [
                Path("/Users/gimdudeul/MVP/mycloset-ai/backend/app/ai_pipeline/steps"),
                backend_root / "app" / "ai_pipeline" / "steps",
                Path.cwd() / "app" / "ai_pipeline" / "steps"
            ]
            
            for path in possible_paths:
                if path.exists():
                    self.steps_dir = path
                    break
    def fix_all_step_files(self):
        """ëª¨ë“  Step íŒŒì¼ì˜ syntax error ìˆ˜ì •"""
        print("ğŸ”§ Step íŒŒì¼ syntax error ìë™ ìˆ˜ì • ì‹œì‘...")
        
        step_files = [
            "step_01_human_parsing.py",
            "step_02_pose_estimation.py", 
            "step_03_cloth_segmentation.py",
            "step_04_geometric_matching.py",
            "step_05_cloth_warping.py",
            "step_06_virtual_fitting.py",
            "step_07_post_processing.py",
            "step_08_quality_assessment.py"
        ]
        
        for step_file in step_files:
            file_path = self.steps_dir / step_file
            if file_path.exists():
                self._fix_step_file(file_path)
            else:
                print(f"   âš ï¸ {step_file}: íŒŒì¼ ì—†ìŒ")
        
        print(f"   âœ… Step íŒŒì¼ ìˆ˜ì • ì™„ë£Œ: {len(self.fixed_files)}ê°œ")
        print(f"   âœ… threading import ì¶”ê°€: {len(self.threading_imports_added)}ê°œ")
        print(f"   âœ… syntax error ìˆ˜ì •: {self.syntax_errors_fixed}ê°œ")
    
    def _fix_step_file(self, file_path: Path):
        """ê°œë³„ Step íŒŒì¼ ìˆ˜ì •"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ë°±ì—… ìƒì„±
            backup_path = file_path.with_suffix('.py.backup')
            if not backup_path.exists():  # ë°±ì—…ì´ ì—†ì„ ë•Œë§Œ ìƒì„±
                with open(backup_path, 'w', encoding='utf-8') as f:
                    f.write(content)
            
            # ìˆ˜ì •ì‚¬í•­ ì ìš©
            modified = False
            new_content = content
            
            # 1. threading import ì¶”ê°€
            if 'import threading' not in content and 'from threading import' not in content:
                # import ì„¹ì…˜ ì°¾ê¸°
                lines = content.split('\n')
                import_end_idx = 0
                
                for i, line in enumerate(lines):
                    if line.strip().startswith('import ') or line.strip().startswith('from '):
                        import_end_idx = i
                    elif line.strip() and not line.strip().startswith('#') and import_end_idx > 0:
                        break
                
                # threading import ì¶”ê°€
                if import_end_idx > 0:
                    lines.insert(import_end_idx + 1, 'import threading')
                    new_content = '\n'.join(lines)
                    modified = True
                    self.threading_imports_added.append(file_path.name)
                    print(f"      âœ… {file_path.name}: threading import ì¶”ê°€")
            
            # 2. ì¼ë°˜ì ì¸ syntax error ìˆ˜ì •
            syntax_fixes = [
                # ì˜ëª»ëœ ë“¤ì—¬ì“°ê¸° ìˆ˜ì •
                ('    else:', '        else:'),
                ('    elif:', '        elif:'),
                ('    except:', '        except:'),
                ('    finally:', '        finally:'),
                
                # ì¼ë°˜ì ì¸ ì˜¤íƒ€ ìˆ˜ì •
                ('sel.', 'self.'),
                ('slef.', 'self.'),
                ('retrun ', 'return '),
                ('improt ', 'import '),
                ('fro ', 'from '),
                ('asyncoi ', 'asyncio '),
                
                # ë¬¸ìì—´ ë¬¸ì œ ìˆ˜ì •
                ('f"', 'f"'),  # ì´ë¯¸ ì˜¬ë°”ë¦„
                ("f'", "f'"),  # ì´ë¯¸ ì˜¬ë°”ë¦„
            ]
            
            original_content = new_content
            for wrong, correct in syntax_fixes:
                if wrong in new_content and wrong != correct:
                    occurrences = new_content.count(wrong)
                    new_content = new_content.replace(wrong, correct)
                    if occurrences > 0:
                        modified = True
                        self.syntax_errors_fixed += occurrences
            
            # 3. BaseStepMixin í˜¸í™˜ì„± ê°•í™”
            if 'BaseStepMixin' in new_content:
                # TYPE_CHECKING import ì¶”ê°€
                if 'TYPE_CHECKING' not in new_content:
                    if 'from typing import' in new_content:
                        new_content = new_content.replace(
                            'from typing import',
                            'from typing import TYPE_CHECKING,'
                        )
                        modified = True
                    else:
                        # import ì„¹ì…˜ì— ì¶”ê°€
                        lines = new_content.split('\n')
                        for i, line in enumerate(lines):
                            if line.strip().startswith('import ') and 'typing' not in line:
                                lines.insert(i, 'from typing import TYPE_CHECKING\n')
                                new_content = '\n'.join(lines)
                                modified = True
                                break
            
            # 4. íŠ¹ìˆ˜ syntax error íŒ¨í„´ ìˆ˜ì •
            # ë¬¸ë²• ì˜¤ë¥˜ê°€ ìˆëŠ” ë¼ì¸ ì°¾ê¸° ë° ìˆ˜ì •
            lines = new_content.split('\n')
            for i, line in enumerate(lines):
                original_line = line
                
                # í”í•œ êµ¬ë¬¸ ì˜¤ë¥˜ íŒ¨í„´ë“¤
                if 'except:' in line and not line.strip().endswith(':'):
                    line = line.rstrip() + ':'
                    modified = True
                    
                if 'else:' in line and not line.strip().endswith(':'):
                    line = line.rstrip() + ':'
                    modified = True
                    
                if 'finally:' in line and not line.strip().endswith(':'):
                    line = line.rstrip() + ':'
                    modified = True
                
                if original_line != line:
                    lines[i] = line
                    self.syntax_errors_fixed += 1
            
            if modified and lines != new_content.split('\n'):
                new_content = '\n'.join(lines)
            
            # 5. íŒŒì¼ ì €ì¥ (ìˆ˜ì •ì‚¬í•­ì´ ìˆëŠ” ê²½ìš°)
            if modified:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                self.fixed_files.append(file_path.name)
                print(f"      âœ… {file_path.name}: syntax error ìˆ˜ì • ì™„ë£Œ")
            else:
                print(f"      â„¹ï¸ {file_path.name}: ìˆ˜ì •ì‚¬í•­ ì—†ìŒ")
            
        except Exception as e:
            print(f"      âŒ {file_path.name}: ìˆ˜ì • ì‹¤íŒ¨ - {e}")
    
    def create_compatible_base_step_mixin(self):
        """BaseStepMixin í˜¸í™˜ì„± ê°•í™” íŒŒì¼ ìƒì„±"""
        try:
            base_step_path = self.steps_dir / "base_step_mixin.py"
            
            # ê¸°ì¡´ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if base_step_path.exists():
                print(f"      â„¹ï¸ BaseStepMixin íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•¨: {base_step_path}")
                return
            
            compatible_content = '''#!/usr/bin/env python3
"""
ğŸ”¥ BaseStepMixin v20.0 - GitHub í”„ë¡œì íŠ¸ ì™„ì „ í˜¸í™˜ ë²„ì „
===============================================================
âœ… threading import í¬í•¨
âœ… ëª¨ë“  dependency í•´ê²°
âœ… M3 Max MPS ìµœì í™”
âœ… conda í™˜ê²½ ì™„ì „ ì§€ì›
âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB ì™„ì „ í™œìš©
âœ… Central Hub DI Container ì—°ë™
"""

import os
import gc
import time
import asyncio
import logging
import threading
import traceback
import weakref
import subprocess
import platform
import inspect
import base64
from io import BytesIO
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, Type, TYPE_CHECKING, Awaitable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from functools import wraps
from contextlib import asynccontextmanager
from enum import Enum

# TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
if TYPE_CHECKING:
    from ..utils.model_loader import ModelLoader
    from ..utils.memory_manager import MemoryManager

class BaseStepMixin(ABC):
    """BaseStepMixin v20.0 - ì™„ì „ í˜¸í™˜ ë²„ì „"""
    
    def __init__(self, device: str = "cpu", **kwargs):
        self.device = device
        self.step_name = self.__class__.__name__
        self.kwargs = kwargs
        self.logger = logging.getLogger(self.__class__.__name__)
        self._models = {}
        self._lock = threading.Lock()
        
        # M3 Max ìµœì í™”
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            try:
                import torch
                if torch.backends.mps.is_available():
                    self.device = 'mps'
            except:
                pass
    
    @abstractmethod
    def _run_ai_inference(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹¤ì œ AI ì¶”ë¡  ë¡œì§ - ê° Stepì—ì„œ êµ¬í˜„"""
        pass
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """í‘œì¤€í™”ëœ process ë©”ì„œë“œ"""
        try:
            # ì „ì²˜ë¦¬
            processed_input = await self._preprocess_data(input_data)
            
            # AI ì¶”ë¡  ì‹¤í–‰
            result = self._run_ai_inference(processed_input)
            
            # í›„ì²˜ë¦¬
            final_result = await self._postprocess_data(result)
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"Process ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def _preprocess_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        return data
    
    async def _postprocess_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° í›„ì²˜ë¦¬"""
        return data
    
    def load_model(self, model_name: str, **kwargs):
        """ëª¨ë¸ ë¡œë”©"""
        with self._lock:
            if model_name not in self._models:
                # ì‹¤ì œ ëª¨ë¸ ë¡œë”© ë¡œì§
                self._models[model_name] = f"mock_{model_name}"
            return self._models[model_name]
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        with self._lock:
            self._models.clear()
            gc.collect()
'''
            
            with open(base_step_path, 'w', encoding='utf-8') as f:
                f.write(compatible_content)
            print(f"      âœ… BaseStepMixin í˜¸í™˜ì„± ê°•í™” íŒŒì¼ ìƒì„±: {base_step_path}")
            
        except Exception as e:
            print(f"      âŒ BaseStepMixin ìƒì„± ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 5. GitHub ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ê¸°
# =============================================================================

class GitHubCheckpointAnalyzer:
    """GitHub í”„ë¡œì íŠ¸ ì²´í¬í¬ì¸íŠ¸ ì™„ì „ ë¶„ì„ê¸°"""
    
    def __init__(self, device: str = 'cpu'):
        self.device = device
        self.torch_available = False
        self.safetensors_available = False
        
        try:
            import torch
            self.torch_available = True
            self.torch = torch
        except ImportError:
            pass
            
        try:
            from safetensors.torch import load_file
            self.safetensors_available = True
            self.safetensors_load = load_file
        except ImportError:
            pass
    
    def analyze_checkpoint(self, checkpoint_path: Path) -> CheckpointAnalysisResult:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì™„ì „ ë¶„ì„"""
        
        analysis = CheckpointAnalysisResult(
            file_path=checkpoint_path,
            exists=checkpoint_path.exists(),
            size_mb=0.0
        )
        
        if not analysis.exists:
            analysis.status = CheckpointLoadingStatus.NOT_FOUND
            return analysis
        
        # íŒŒì¼ í¬ê¸° ë° í•´ì‹œ
        try:
            stat_info = checkpoint_path.stat()
            analysis.size_mb = stat_info.st_size / (1024 * 1024)
            
            # í•´ì‹œ ê³„ì‚° (ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ìƒ˜í”Œë§)
            if analysis.size_mb < 500:  # 500MB ë¯¸ë§Œë§Œ ì „ì²´ í•´ì‹œ
                analysis.file_hash = self._calculate_file_hash(checkpoint_path)
            else:
                analysis.file_hash = self._calculate_sample_hash(checkpoint_path)
                
        except Exception as e:
            analysis.loading_errors.append(f"íŒŒì¼ ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {e}")
        
        # GitHub ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
        if self.torch_available:
            self._test_github_pytorch_loading(analysis)
        
        if self.safetensors_available and checkpoint_path.suffix == '.safetensors':
            self._test_github_safetensors_loading(analysis)
        
        # ìƒíƒœ ê²°ì •
        if analysis.pytorch_weights_only_success or analysis.pytorch_regular_success or analysis.safetensors_success:
            analysis.status = CheckpointLoadingStatus.SAFETENSORS_SUCCESS if analysis.safetensors_success else CheckpointLoadingStatus.SUCCESS
        elif analysis.loading_errors:
            if any("corrupted" in error.lower() for error in analysis.loading_errors):
                analysis.status = CheckpointLoadingStatus.CORRUPTED
            elif any("weights_only" in error.lower() for error in analysis.loading_errors):
                analysis.status = CheckpointLoadingStatus.WEIGHTS_ONLY_FAILED
            elif any("memory" in error.lower() for error in analysis.loading_errors):
                analysis.status = CheckpointLoadingStatus.MEMORY_INSUFFICIENT
            else:
                analysis.status = CheckpointLoadingStatus.LOADING_FAILED
        
        return analysis
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """ì „ì²´ íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception:
            return ""
    
    def _calculate_sample_hash(self, file_path: Path, sample_size: int = 2*1024*1024) -> str:
        """ìƒ˜í”Œ í•´ì‹œ ê³„ì‚° (GitHub ëŒ€ìš©ëŸ‰ íŒŒì¼ìš©)"""
        try:
            hash_md5 = hashlib.md5()
            file_size = file_path.stat().st_size
            
            with open(file_path, "rb") as f:
                # ì‹œì‘ ë¶€ë¶„
                chunk = f.read(sample_size)
                hash_md5.update(chunk)
                
                # ì¤‘ê°„ ë¶€ë¶„
                if file_size > sample_size * 3:
                    f.seek(file_size // 2)
                    chunk = f.read(sample_size)
                    hash_md5.update(chunk)
                
                # ë ë¶€ë¶„
                if file_size > sample_size * 2:
                    f.seek(max(0, file_size - sample_size))
                    chunk = f.read(sample_size)
                    hash_md5.update(chunk)
            
            return hash_md5.hexdigest()
        except Exception:
            return ""
    
    def _test_github_pytorch_loading(self, analysis: CheckpointAnalysisResult):
        """GitHub PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í…ŒìŠ¤íŠ¸ (3ë‹¨ê³„ ì•ˆì „ ë¡œë”©)"""
        if not self.torch_available:
            analysis.loading_errors.append("PyTorch ì—†ìŒ")
            return
        
        file_path = analysis.file_path
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / (1024**2)
        
        # 1ë‹¨ê³„: weights_only=True ì‹œë„ (GitHub ê¶Œì¥)
        try:
            with github_safety.safe_execution(f"PyTorch weights_only ë¡œë”© {file_path.name}", timeout=120):
                checkpoint = self.torch.load(file_path, map_location=self.device, weights_only=True)
                analysis.pytorch_weights_only_success = True
                self._analyze_github_checkpoint_content(analysis, checkpoint)
                print(f"         âœ… weights_only ë¡œë”© ì„±ê³µ")
                return  # ì„±ê³µí•˜ë©´ ë‹¤ë¥¸ ë°©ë²• ì‹œë„í•˜ì§€ ì•ŠìŒ
                
        except Exception as e:
            analysis.loading_errors.append(f"weights_only ë¡œë”© ì‹¤íŒ¨: {e}")
            print(f"         âŒ weights_only ì‹¤íŒ¨: {str(e)[:100]}")
        
        # 2ë‹¨ê³„: weights_only=False ì‹œë„ (GitHub í˜¸í™˜ì„±)
        try:
            with github_safety.safe_execution(f"PyTorch ì¼ë°˜ ë¡œë”© {file_path.name}", timeout=120):
                checkpoint = self.torch.load(file_path, map_location=self.device, weights_only=False)
                analysis.pytorch_regular_success = True
                self._analyze_github_checkpoint_content(analysis, checkpoint)
                print(f"         âœ… ì¼ë°˜ ë¡œë”© ì„±ê³µ")
                return
                
        except Exception as e:
            analysis.loading_errors.append(f"ì¼ë°˜ ë¡œë”© ì‹¤íŒ¨: {e}")
            print(f"         âŒ ì¼ë°˜ ë¡œë”© ì‹¤íŒ¨: {str(e)[:100]}")
        
        # 3ë‹¨ê³„: ë ˆê±°ì‹œ ë¡œë”© ì‹œë„ (GitHub ë ˆê±°ì‹œ ì§€ì›)
        try:
            with github_safety.safe_execution(f"PyTorch ë ˆê±°ì‹œ ë¡œë”© {file_path.name}", timeout=120):
                checkpoint = self.torch.load(file_path, map_location=self.device)
                analysis.legacy_load_success = True
                analysis.pytorch_regular_success = True
                self._analyze_github_checkpoint_content(analysis, checkpoint)
                print(f"         âœ… ë ˆê±°ì‹œ ë¡œë”© ì„±ê³µ")
                
        except Exception as e:
            analysis.loading_errors.append(f"ë ˆê±°ì‹œ ë¡œë”© ì‹¤íŒ¨: {e}")
            print(f"         âŒ ë ˆê±°ì‹œ ë¡œë”© ì‹¤íŒ¨: {str(e)[:100]}")
        
        # ì„±ëŠ¥ ì¸¡ì •
        analysis.load_time_seconds = time.time() - start_time
        end_memory = psutil.Process().memory_info().rss / (1024**2)
        analysis.memory_usage_mb = end_memory - start_memory
    
    def _test_github_safetensors_loading(self, analysis: CheckpointAnalysisResult):
        """GitHub SafeTensors ë¡œë”© í…ŒìŠ¤íŠ¸"""
        if not self.safetensors_available:
            analysis.loading_errors.append("SafeTensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
            return
        
        try:
            with github_safety.safe_execution(f"SafeTensors ë¡œë”© {analysis.file_path.name}", timeout=120):
                checkpoint = self.safetensors_load(str(analysis.file_path))
                analysis.safetensors_success = True
                self._analyze_github_checkpoint_content(analysis, checkpoint)
                print(f"         âœ… SafeTensors ë¡œë”© ì„±ê³µ")
                
        except Exception as e:
            analysis.loading_errors.append(f"SafeTensors ë¡œë”© ì‹¤íŒ¨: {e}")
            print(f"         âŒ SafeTensors ë¡œë”© ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _analyze_github_checkpoint_content(self, analysis: CheckpointAnalysisResult, checkpoint):
        """GitHub ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ë¶„ì„"""
        try:
            # State dict ì¶”ì¶œ
            state_dict = checkpoint
            if isinstance(checkpoint, dict):
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                    analysis.checkpoint_keys = [k for k in checkpoint.keys() if k != 'state_dict']
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                    analysis.checkpoint_keys = [k for k in checkpoint.keys() if k != 'model']
                else:
                    analysis.checkpoint_keys = list(checkpoint.keys())[:20]  # ì²˜ìŒ 20ê°œë§Œ
            
            if isinstance(state_dict, dict):
                analysis.state_dict_keys = list(state_dict.keys())[:30]  # ì²˜ìŒ 30ê°œë§Œ
                
                # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
                param_count = 0
                for key, tensor in state_dict.items():
                    if hasattr(tensor, 'numel'):
                        param_count += tensor.numel()
                analysis.parameter_count = param_count
                
                # GitHub ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¶”ì •
                analysis.model_architecture = self._estimate_github_architecture(state_dict)
                
                # GitHub ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
                self._test_github_device_compatibility(analysis, state_dict)
            
        except Exception as e:
            analysis.loading_errors.append(f"GitHub ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _estimate_github_architecture(self, state_dict: dict) -> str:
        """GitHub ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¶”ì •"""
        keys = list(state_dict.keys())
        key_str = ' '.join(keys).lower()
        
        # GitHub í”„ë¡œì íŠ¸ íŠ¹í™” ëª¨ë¸ ê°ì§€
        if any(keyword in key_str for keyword in ['parsing', 'human_parsing', 'schp', 'graphonomy']):
            return "Human Parsing Model (SCHP/Graphonomy)"
        elif any(keyword in key_str for keyword in ['pose', 'openpose', 'dwpose', 'keypoint']):
            return "Pose Estimation Model (OpenPose/DWPose)"
        elif any(keyword in key_str for keyword in ['sam', 'segment_anything', 'mask_decoder']):
            return "Segmentation Model (SAM)"
        elif any(keyword in key_str for keyword in ['ootd', 'diffusion', 'unet', 'vae']):
            return "Diffusion Model (OOTDiffusion)"
        elif any(keyword in key_str for keyword in ['gmm', 'geometric', 'matching']):
            return "Geometric Matching Model (GMM)"
        elif any(keyword in key_str for keyword in ['esrgan', 'realesrgan', 'generator']):
            return "Super Resolution Model (ESRGAN)"
        elif any(keyword in key_str for keyword in ['clip', 'openclip', 'vision_model', 'text_model']):
            return "Vision-Language Model (CLIP)"
        elif any(keyword in key_str for keyword in ['vit', 'transformer', 'attention']):
            return "Vision Transformer"
        elif any(keyword in key_str for keyword in ['resnet', 'efficientnet', 'backbone']):
            return "CNN Backbone"
        else:
            return f"Unknown Architecture ({len(keys)} layers)"
    
    def _test_github_device_compatibility(self, analysis: CheckpointAnalysisResult, state_dict: dict):
        """GitHub ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        if not self.torch_available:
            return
        
        try:
            # ì²« ë²ˆì§¸ í…ì„œë¡œ í…ŒìŠ¤íŠ¸
            first_tensor = None
            for value in state_dict.values():
                if hasattr(value, 'to'):
                    first_tensor = value
                    break
            
            if first_tensor is None:
                return
            
            # CPU í…ŒìŠ¤íŠ¸
            try:
                cpu_tensor = first_tensor.to('cpu')
                analysis.cpu_compatible = True
            except Exception as e:
                analysis.loading_errors.append(f"CPU í˜¸í™˜ì„± ì‹¤íŒ¨: {e}")
            
            # CUDA í…ŒìŠ¤íŠ¸
            if self.torch.cuda.is_available():
                try:
                    cuda_tensor = first_tensor.to('cuda')
                    analysis.cuda_compatible = True
                except Exception as e:
                    analysis.loading_errors.append(f"CUDA í˜¸í™˜ì„± ì‹¤íŒ¨: {e}")
            
            # MPS í…ŒìŠ¤íŠ¸ (M3 Max íŠ¹í™”)
            if hasattr(self.torch.backends, 'mps') and self.torch.backends.mps.is_available():
                try:
                    mps_tensor = first_tensor.to('mps')
                    analysis.mps_compatible = True
                except Exception as e:
                    analysis.loading_errors.append(f"MPS í˜¸í™˜ì„± ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            analysis.loading_errors.append(f"ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 6. GitHub ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„ê¸°
# =============================================================================

class GitHubSystemAnalyzer:
    """GitHub í”„ë¡œì íŠ¸ ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.environment = GitHubSystemEnvironment()
        self.syntax_fixer = StepFileSyntaxFixer()
        
    def analyze_github_environment(self) -> GitHubSystemEnvironment:
        """GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ì™„ì „ ë¶„ì„"""
        
        print("ğŸ“Š GitHub í”„ë¡œì íŠ¸ ì‹œìŠ¤í…œ í™˜ê²½ ì™„ì „ ë¶„ì„ ì‹œì‘...")
        
        with github_safety.safe_execution("GitHub ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„", timeout=90):
            # 1. Step íŒŒì¼ ìˆ˜ì • ë¨¼ì € ì‹¤í–‰
            self._fix_step_files()
            
            # 2. ê¸°ì¡´ ì‹œìŠ¤í…œ ë¶„ì„
            self._analyze_hardware()
            self._analyze_software_environment()
            self._analyze_pytorch_environment()
            self._analyze_github_project_structure()
            self._analyze_dependencies()
            self._analyze_github_integrations()
        
        return self.environment
    
    def _fix_step_files(self):
        """Step íŒŒì¼ ì˜¤ë¥˜ ìˆ˜ì •"""
        try:
            print("   ğŸ”§ Step íŒŒì¼ ì˜¤ë¥˜ ìë™ ìˆ˜ì •...")
            
            # Step íŒŒì¼ syntax error ìˆ˜ì •
            self.syntax_fixer.fix_all_step_files()
            
            # BaseStepMixin í˜¸í™˜ì„± ê°•í™”
            self.syntax_fixer.create_compatible_base_step_mixin()
            
            # ê²°ê³¼ ë°˜ì˜
            self.environment.step_files_fixed = self.syntax_fixer.fixed_files
            self.environment.threading_imports_added = self.syntax_fixer.threading_imports_added
            self.environment.syntax_errors_fixed = self.syntax_fixer.syntax_errors_fixed
            
            print(f"   âœ… Step íŒŒì¼ ìˆ˜ì • ì™„ë£Œ: {len(self.syntax_fixer.fixed_files)}ê°œ")
            print(f"   âœ… threading import ì¶”ê°€: {len(self.syntax_fixer.threading_imports_added)}ê°œ")
            print(f"   âœ… syntax error ìˆ˜ì •: {self.syntax_fixer.syntax_errors_fixed}ê°œ")
            
        except Exception as e:
            print(f"âŒ Step íŒŒì¼ ìˆ˜ì • ì‹¤íŒ¨: {e}")
    
    def _analyze_hardware(self):
        """í•˜ë“œì›¨ì–´ ë¶„ì„ (M3 Max íŠ¹í™”)"""
        try:
            # CPU ì •ë³´
            self.environment.cpu_cores = psutil.cpu_count(logical=True)
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            memory = psutil.virtual_memory()
            self.environment.total_memory_gb = memory.total / (1024**3)
            self.environment.available_memory_gb = memory.available / (1024**3)
            
            # M3 Max ê°ì§€ (í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜)
            if platform.system() == 'Darwin' and platform.machine() == 'arm64':
                try:
                    result = subprocess.run(
                        ['sysctl', '-n', 'machdep.cpu.brand_string'],
                        capture_output=True, text=True, timeout=5
                    )
                    if 'M3' in result.stdout:
                        self.environment.is_m3_max = True
                        
                        # ë©”ëª¨ë¦¬ ì •í™•í•œ ì¸¡ì •
                        memory_result = subprocess.run(
                            ['sysctl', '-n', 'hw.memsize'],
                            capture_output=True, text=True, timeout=5
                        )
                        if memory_result.returncode == 0:
                            exact_memory_gb = int(memory_result.stdout.strip()) / (1024**3)
                            self.environment.total_memory_gb = round(exact_memory_gb, 1)
                            
                except Exception as e:
                    print(f"âš ï¸ M3 Max ê°ì§€ ì‹¤íŒ¨: {e}")
            
            print(f"   ğŸ’» í•˜ë“œì›¨ì–´: {self.environment.cpu_cores}ì½”ì–´, {self.environment.total_memory_gb:.1f}GB")
            print(f"   ğŸš€ M3 Max: {'âœ…' if self.environment.is_m3_max else 'âŒ'}")
            
        except Exception as e:
            print(f"âŒ í•˜ë“œì›¨ì–´ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_software_environment(self):
        """ì†Œí”„íŠ¸ì›¨ì–´ í™˜ê²½ ë¶„ì„ (conda íŠ¹í™”)"""
        try:
            # Python ì •ë³´
            self.environment.python_version = sys.version.split()[0]
            
            # conda í™˜ê²½ ì •ë³´ (í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜)
            conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'none')
            self.environment.conda_env = conda_env
            self.environment.is_target_conda_env = (conda_env == 'mycloset-ai-clean')
            
            print(f"   ğŸ Python: {self.environment.python_version}")
            print(f"   ğŸ“¦ Conda í™˜ê²½: {conda_env}")
            print(f"   âœ… íƒ€ê²Ÿ í™˜ê²½: {'âœ…' if self.environment.is_target_conda_env else 'âŒ'} (mycloset-ai-clean)")
            
        except Exception as e:
            print(f"âŒ ì†Œí”„íŠ¸ì›¨ì–´ í™˜ê²½ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_pytorch_environment(self):
        """PyTorch í™˜ê²½ ë¶„ì„ (MPS íŠ¹í™”)"""
        try:
            # PyTorch ê°€ìš©ì„± í™•ì¸
            try:
                import torch
                self.environment.torch_available = True
                self.environment.torch_version = torch.__version__
                
                # ë””ë°”ì´ìŠ¤ ì§€ì› í™•ì¸
                self.environment.cuda_available = torch.cuda.is_available()
                self.environment.mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
                
                # ì¶”ì²œ ë””ë°”ì´ìŠ¤ ê²°ì • (M3 Max MPS ìš°ì„ )
                if self.environment.mps_available and self.environment.is_m3_max:
                    self.environment.recommended_device = 'mps'
                elif self.environment.cuda_available:
                    self.environment.recommended_device = 'cuda'
                else:
                    self.environment.recommended_device = 'cpu'
                    
                print(f"   ğŸ”¥ PyTorch: {self.environment.torch_version}")
                print(f"   âš¡ MPS: {'âœ…' if self.environment.mps_available else 'âŒ'}")
                print(f"   ğŸ¯ ì¶”ì²œ ë””ë°”ì´ìŠ¤: {self.environment.recommended_device}")
                
            except ImportError:
                self.environment.torch_available = False
                print(f"   âŒ PyTorch ì—†ìŒ")
            
        except Exception as e:
            print(f"âŒ PyTorch í™˜ê²½ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_github_project_structure(self):
        """GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„"""
        try:
            # ê¸°ë³¸ êµ¬ì¡° í™•ì¸
            self.environment.project_root_exists = project_root.exists()
            self.environment.backend_root_exists = backend_root.exists()
            self.environment.ai_models_root_exists = ai_models_root.exists()
            
            # AI ëª¨ë¸ í¬ê¸° ê³„ì‚°
            if ai_models_root.exists():
                total_size = 0
                model_count = 0
                for model_file in ai_models_root.rglob('*'):
                    if model_file.is_file() and model_file.suffix in ['.pth', '.pt', '.safetensors', '.bin', '.ckpt']:
                        total_size += model_file.stat().st_size
                        model_count += 1
                
                self.environment.ai_models_size_gb = total_size / (1024**3)
                
                print(f"   ğŸ“ AI ëª¨ë¸: {model_count}ê°œ íŒŒì¼, {self.environment.ai_models_size_gb:.1f}GB")
            else:
                print(f"   âŒ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {ai_models_root}")
            
            # Step ëª¨ë“ˆ ì°¾ê¸°
            steps_dir = backend_root / "app" / "ai_pipeline" / "steps"
            if steps_dir.exists():
                step_files = list(steps_dir.glob("step_*.py"))
                self.environment.step_modules_found = [f.stem for f in step_files]
                print(f"   ğŸš€ Step ëª¨ë“ˆ: {len(step_files)}ê°œ ë°œê²¬")
            
            structure_ready = all([
                self.environment.project_root_exists,
                self.environment.backend_root_exists,
                self.environment.ai_models_root_exists
            ])
            print(f"   ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°: {'âœ…' if structure_ready else 'âš ï¸'}")
            
        except Exception as e:
            print(f"âŒ GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_dependencies(self):
        """í•µì‹¬ ì˜ì¡´ì„± ë¶„ì„"""
        try:
            dependencies = {
                'torch': False,
                'torchvision': False,
                'numpy': False,
                'PIL': False,
                'cv2': False,
                'transformers': False,
                'safetensors': False,
                'psutil': False,
                'threading': True  # í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
            }
            
            for dep in dependencies.keys():
                if dep == 'threading':
                    continue  # ì´ë¯¸ ì„¤ì •ë¨
                try:
                    if dep == 'PIL':
                        import PIL.Image
                    elif dep == 'cv2':
                        import cv2
                    else:
                        importlib.import_module(dep)
                    dependencies[dep] = True
                except ImportError:
                    pass
            
            self.environment.core_dependencies = dependencies
            
            success_count = sum(dependencies.values())
            total_count = len(dependencies)
            print(f"   ğŸ“¦ í•µì‹¬ ì˜ì¡´ì„±: {success_count}/{total_count} ì„±ê³µ")
            
        except Exception as e:
            print(f"âŒ ì˜ì¡´ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_github_integrations(self):
        """GitHub í†µí•© ìƒíƒœ ë¶„ì„"""
        try:
            integrations = {
                'base_step_mixin': False,
                'model_loader': False,
                'step_factory': False,
                'implementation_manager': False,
                'auto_model_detector': False
            }
            
            # BaseStepMixin í™•ì¸
            try:
                from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
                integrations['base_step_mixin'] = True
            except ImportError:
                pass
            
            # ModelLoader í™•ì¸
            try:
                from app.ai_pipeline.utils.model_loader import ModelLoader
                integrations['model_loader'] = True
            except ImportError:
                pass
            
            # StepFactory í™•ì¸
            try:
                from app.ai_pipeline.utils.step_factory import StepFactory
                integrations['step_factory'] = True
            except ImportError:
                pass
            
            # RealAIStepImplementationManager í™•ì¸
            try:
                from app.services.step_implementations import RealAIStepImplementationManager
                integrations['implementation_manager'] = True
            except ImportError:
                pass
            
            # AutoModelDetector í™•ì¸
            try:
                from app.ai_pipeline.utils.auto_model_detector import AutoModelDetector
                integrations['auto_model_detector'] = True
            except ImportError:
                pass
            
            self.environment.github_integrations = integrations
            
            success_count = sum(integrations.values())
            total_count = len(integrations)
            print(f"   ğŸ”— GitHub í†µí•©: {success_count}/{total_count} ì„±ê³µ")
            
        except Exception as e:
            print(f"âŒ GitHub í†µí•© ë¶„ì„ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 7. GitHub Step ë¶„ì„ê¸°
# =============================================================================

class GitHubStepAnalyzer:
    """GitHub Step ì™„ì „ ë¶„ì„ê¸°"""
    
    def __init__(self, system_env: GitHubSystemEnvironment):
        self.system_env = system_env
        self.checkpoint_analyzer = GitHubCheckpointAnalyzer(
            device=system_env.recommended_device
        )
    
    def analyze_github_step(self, step_info: GitHubStepInfo) -> GitHubStepAnalysisResult:
        """GitHub Step ì™„ì „ ë¶„ì„"""
        
        print(f"\nğŸ”§ {step_info.step_name} (Step {step_info.step_id}) ì™„ì „ ë¶„ì„ ì‹œì‘...")
        
        analysis = GitHubStepAnalysisResult(step_info=step_info)
        
        # ìˆ˜ì • ìƒíƒœ í™•ì¸
        step_file_name = f"step_{step_info.step_id:02d}_{step_info.step_name.lower().replace('step', '')}.py"
        analysis.syntax_error_fixed = step_file_name in self.system_env.step_files_fixed
        analysis.threading_import_added = step_file_name in self.system_env.threading_imports_added
        
        # 1. Import í…ŒìŠ¤íŠ¸
        self._test_github_import(analysis)
        
        # 2. í´ë˜ìŠ¤ ë¶„ì„
        if analysis.import_success:
            self._analyze_github_class(analysis)
        
        # 3. ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
        if analysis.class_found:
            self._test_github_instance_creation(analysis)
        
        # 4. ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        if analysis.instance_created:
            self._test_github_initialization(analysis)
        
        # 5. GitHub Central Hub ì˜ì¡´ì„± ë¶„ì„
        if analysis.instance_created:
            self._analyze_github_dependencies(analysis)
        
        # 6. AI ëª¨ë¸ ë¶„ì„
        self._analyze_github_ai_models(analysis)
        
        # 7. ìƒíƒœ ê²°ì • ë° ì ìˆ˜ ê³„ì‚°
        self._determine_github_status_and_score(analysis)
        
        return analysis
    
    def _test_github_import(self, analysis: GitHubStepAnalysisResult):
        """GitHub Step Import í…ŒìŠ¤íŠ¸"""
        try:
            with github_safety.safe_execution(f"{analysis.step_info.step_name} Import", timeout=60):
                start_time = time.time()
                
                # ë™ì  import ì‹œë„
                module = importlib.import_module(analysis.step_info.module_path)
                analysis.import_time = time.time() - start_time
                analysis.import_success = True
                
                # í´ë˜ìŠ¤ ì¡´ì¬ í™•ì¸
                if hasattr(module, analysis.step_info.step_class):
                    analysis.class_found = True
                    print(f"   âœ… Import ì„±ê³µ ({analysis.import_time:.3f}ì´ˆ)")
                else:
                    analysis.import_errors.append(f"í´ë˜ìŠ¤ {analysis.step_info.step_class} ì—†ìŒ")
                    print(f"   âŒ í´ë˜ìŠ¤ ì—†ìŒ: {analysis.step_info.step_class}")
                    
        except Exception as e:
            analysis.import_errors.append(str(e))
            if "invalid syntax" in str(e).lower():
                analysis.status = GitHubStepStatus.SYNTAX_ERROR
            elif "threading" in str(e).lower():
                analysis.status = GitHubStepStatus.THREADING_MISSING
            else:
                analysis.status = GitHubStepStatus.IMPORT_FAILED
            print(f"   âŒ Import ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _analyze_github_class(self, analysis: GitHubStepAnalysisResult):
        """GitHub í´ë˜ìŠ¤ êµ¬ì¡° ë¶„ì„"""
        try:
            module = importlib.import_module(analysis.step_info.module_path)
            step_class = getattr(module, analysis.step_info.step_class)
            
            # í´ë˜ìŠ¤ ë©”ì„œë“œ ê²€ì‚¬
            class_methods = [method for method in dir(step_class) if not method.startswith('_')]
            
            analysis.has_process_method = 'process' in class_methods
            analysis.has_initialize_method = 'initialize' in class_methods
            
            # BaseStepMixin ìƒì† í™•ì¸ (GitHub íŠ¹í™”)
            mro = inspect.getmro(step_class)
            analysis.is_base_step_mixin = any('BaseStepMixin' in cls.__name__ for cls in mro)
            analysis.basestepmixin_compatible = analysis.is_base_step_mixin
            
            # Central Hub ì§€ì› í™•ì¸
            analysis.has_central_hub_support = any(
                hasattr(step_class, attr) for attr in [
                    'central_hub_container', 'dependency_manager', 'model_interface'
                ]
            )
            
            print(f"   âœ… í´ë˜ìŠ¤ ë¶„ì„: BaseStepMixin={analysis.is_base_step_mixin}, CentralHub={analysis.has_central_hub_support}")
            
        except Exception as e:
            analysis.import_errors.append(f"í´ë˜ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
            print(f"   âŒ í´ë˜ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _test_github_instance_creation(self, analysis: GitHubStepAnalysisResult):
        """GitHub ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸"""
        try:
            with github_safety.safe_execution(f"{analysis.step_info.step_name} ì¸ìŠ¤í„´ìŠ¤ ìƒì„±", timeout=90):
                module = importlib.import_module(analysis.step_info.module_path)
                step_class = getattr(module, analysis.step_info.step_class)
                
                # ìƒì„±ì íŒŒë¼ë¯¸í„° ë¶„ì„
                signature = inspect.signature(step_class.__init__)
                params = list(signature.parameters.keys())[1:]  # self ì œì™¸
                
                # GitHub í”„ë¡œì íŠ¸ ê¸°ë³¸ ì˜ì¡´ì„± ì¤€ë¹„
                constructor_args = {
                    'device': self.system_env.recommended_device,
                    'strict_mode': False
                }
                
                # ì„ íƒì  ì˜ì¡´ì„± ì²˜ë¦¬
                if 'model_loader' in params:
                    constructor_args['model_loader'] = None  # ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ì²˜ë¦¬
                if 'memory_manager' in params:
                    constructor_args['memory_manager'] = None
                if 'data_converter' in params:
                    constructor_args['data_converter'] = None
                
                analysis.constructor_params = constructor_args
                
                # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œë„
                step_instance = step_class(**constructor_args)
                analysis.instance_created = True
                
                print(f"   âœ… ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ")
                
        except Exception as e:
            analysis.instance_errors.append(str(e))
            analysis.status = GitHubStepStatus.INSTANCE_FAILED
            print(f"   âŒ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _test_github_initialization(self, analysis: GitHubStepAnalysisResult):
        """GitHub ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        if not analysis.instance_created:
            return
        
        try:
            with github_safety.safe_execution(f"{analysis.step_info.step_name} ì´ˆê¸°í™”", timeout=180):
                module = importlib.import_module(analysis.step_info.module_path)
                step_class = getattr(module, analysis.step_info.step_class)
                step_instance = step_class(**analysis.constructor_params)
                
                start_time = time.time()
                
                if hasattr(step_instance, 'initialize'):
                    if asyncio.iscoroutinefunction(step_instance.initialize):
                        # ë¹„ë™ê¸° ì´ˆê¸°í™”
                        try:
                            loop = asyncio.get_event_loop()
                        except RuntimeError:
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                        
                        result = loop.run_until_complete(
                            asyncio.wait_for(step_instance.initialize(), timeout=120.0)
                        )
                    else:
                        # ë™ê¸° ì´ˆê¸°í™”
                        result = step_instance.initialize()
                    
                    if result:
                        analysis.initialization_success = True
                        analysis.initialization_time = time.time() - start_time
                        print(f"   âœ… ì´ˆê¸°í™” ì„±ê³µ ({analysis.initialization_time:.2f}ì´ˆ)")
                    else:
                        analysis.initialization_errors.append("ì´ˆê¸°í™”ê°€ False ë°˜í™˜")
                        print(f"   âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: False ë°˜í™˜")
                        
                else:
                    # initialize ë©”ì„œë“œê°€ ì—†ëŠ” ê²½ìš°
                    analysis.initialization_success = True
                    print(f"   âš ï¸ initialize ë©”ì„œë“œ ì—†ìŒ (ê¸°ë³¸ ì„±ê³µ ì²˜ë¦¬)")
                    
        except TimeoutError:
            analysis.initialization_errors.append("ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ (120ì´ˆ)")
            print(f"   âŒ ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            analysis.initialization_errors.append(str(e))
            analysis.status = GitHubStepStatus.INIT_FAILED
            print(f"   âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _analyze_github_dependencies(self, analysis: GitHubStepAnalysisResult):
        """GitHub Central Hub ì˜ì¡´ì„± ë¶„ì„"""
        try:
            module = importlib.import_module(analysis.step_info.module_path)
            step_class = getattr(module, analysis.step_info.step_class)
            step_instance = step_class(**analysis.constructor_params)
            
            # ì˜ì¡´ì„± ì£¼ì… í…ŒìŠ¤íŠ¸
            dependency_results = {}
            
            # ModelLoader ì£¼ì… í…ŒìŠ¤íŠ¸
            if hasattr(step_instance, 'set_model_loader'):
                try:
                    # Mock ModelLoaderë¡œ í…ŒìŠ¤íŠ¸
                    class MockModelLoader:
                        def load_model(self, *args, **kwargs):
                            return {"mock": "model"}
                        def create_step_interface(self, step_name):
                            return {"interface": step_name}
                    
                    mock_loader = MockModelLoader()
                    step_instance.set_model_loader(mock_loader)
                    analysis.model_loader_injected = hasattr(step_instance, 'model_loader')
                    dependency_results['model_loader'] = analysis.model_loader_injected
                except Exception as e:
                    dependency_results['model_loader'] = False
            
            # Central Hub ì—°ê²° í™•ì¸
            if hasattr(step_instance, 'central_hub_container'):
                analysis.central_hub_connected = step_instance.central_hub_container is not None
                dependency_results['central_hub'] = analysis.central_hub_connected
            
            # ì˜ì¡´ì„± ê²€ì¦ ë©”ì„œë“œ í˜¸ì¶œ
            if hasattr(step_instance, 'validate_dependencies'):
                try:
                    validation_result = step_instance.validate_dependencies()
                    analysis.dependency_validation_result = validation_result
                    dependency_results['validation'] = isinstance(validation_result, dict)
                except Exception as e:
                    dependency_results['validation'] = False
            
            success_count = sum(dependency_results.values())
            total_count = len(dependency_results)
            
            print(f"   ğŸ”— ì˜ì¡´ì„±: {success_count}/{total_count} ì„±ê³µ")
            
        except Exception as e:
            print(f"   âŒ ì˜ì¡´ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _analyze_github_ai_models(self, analysis: GitHubStepAnalysisResult):
        """GitHub AI ëª¨ë¸ ë¶„ì„"""
        try:
            step_info = analysis.step_info
            
            # Stepë³„ ëª¨ë¸ ë””ë ‰í† ë¦¬ íŒ¨í„´ (GitHub êµ¬ì¡° ê¸°ë°˜)
            model_patterns = [
                f"step_{step_info.step_id:02d}_*",
                f"*{step_info.step_name.lower().replace('step', '')}*",
                step_info.step_name.lower()
            ]
            
            model_files = []
            total_size = 0
            
            # AI ëª¨ë¸ ë£¨íŠ¸ì—ì„œ ê²€ìƒ‰
            if ai_models_root.exists():
                for pattern in model_patterns:
                    matching_dirs = list(ai_models_root.glob(pattern))
                    for model_dir in matching_dirs:
                        if model_dir.is_dir():
                            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸°
                            for ext in ['*.pth', '*.pt', '*.safetensors', '*.bin', '*.ckpt']:
                                found_files = list(model_dir.rglob(ext))
                                model_files.extend(found_files)
                
                # ì§ì ‘ íŒŒì¼ ê²€ìƒ‰ë„ ìˆ˜í–‰
                for expected_file in step_info.expected_files:
                    direct_files = list(ai_models_root.rglob(expected_file))
                    model_files.extend(direct_files)
            
            # ì¤‘ë³µ ì œê±°
            unique_files = list(set(model_files))
            
            # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ (ìƒìœ„ 5ê°œë§Œ)
            for model_file in unique_files[:5]:
                if model_file.stat().st_size > 10 * 1024 * 1024:  # 10MB ì´ìƒë§Œ
                    print(f"      ğŸ” ì²´í¬í¬ì¸íŠ¸ ë¶„ì„: {model_file.name}")
                    checkpoint_analysis = self.checkpoint_analyzer.analyze_checkpoint(model_file)
                    analysis.checkpoint_analyses.append(checkpoint_analysis)
                    analysis.detected_model_files.append(model_file.name)
                    total_size += checkpoint_analysis.size_mb
            
            analysis.total_model_size_gb = total_size / 1024
            
            # ëª¨ë¸ ë¡œë”© ì„±ê³µë¥  ê³„ì‚°
            if analysis.checkpoint_analyses:
                successful_loads = sum(
                    1 for cp in analysis.checkpoint_analyses 
                    if cp.status in [CheckpointLoadingStatus.SUCCESS, CheckpointLoadingStatus.SAFETENSORS_SUCCESS]
                )
                analysis.model_loading_success_rate = successful_loads / len(analysis.checkpoint_analyses) * 100
            
            if analysis.detected_model_files:
                print(f"   ğŸ“Š AI ëª¨ë¸: {len(analysis.detected_model_files)}ê°œ ë°œê²¬ "
                      f"({analysis.total_model_size_gb:.1f}GB, ì„±ê³µë¥ : {analysis.model_loading_success_rate:.1f}%)")
            else:
                print(f"   âš ï¸ AI ëª¨ë¸ ì—†ìŒ")
            
        except Exception as e:
            print(f"   âŒ AI ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)[:100]}")
    
    def _determine_github_status_and_score(self, analysis: GitHubStepAnalysisResult):
        """GitHub Step ìƒíƒœ ê²°ì • ë° ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # íŒŒì¼ ìˆ˜ì • ë³´ë„ˆìŠ¤ (v6.0 ì¶”ê°€)
        if analysis.syntax_error_fixed:
            score += 10
        if analysis.threading_import_added:
            score += 10
        
        # Import ì„±ê³µ (15ì )
        if analysis.import_success:
            score += 15
        
        # í´ë˜ìŠ¤ êµ¬ì¡° (20ì )
        if analysis.class_found:
            score += 10
        if analysis.is_base_step_mixin:
            score += 5
        if analysis.has_process_method:
            score += 3
        if analysis.has_central_hub_support:
            score += 2
        
        # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (15ì )
        if analysis.instance_created:
            score += 15
        
        # ì´ˆê¸°í™” (20ì )
        if analysis.initialization_success:
            score += 20
        
        # ì˜ì¡´ì„± (15ì )
        if analysis.model_loader_injected:
            score += 8
        if analysis.central_hub_connected:
            score += 7
        
        # AI ëª¨ë¸ (15ì )
        if analysis.detected_model_files:
            score += 8
            if analysis.model_loading_success_rate > 50:
                score += 7
        
        analysis.health_score = min(100.0, score)
        
        # ìƒíƒœ ê²°ì •
        if not analysis.import_success:
            if analysis.status == GitHubStepStatus.NOT_FOUND:  # ì´ë¯¸ ì„¤ì •ëœ ê²½ìš° ìœ ì§€
                pass
            elif not analysis.syntax_error_fixed:
                analysis.status = GitHubStepStatus.SYNTAX_ERROR
            elif not analysis.threading_import_added:
                analysis.status = GitHubStepStatus.THREADING_MISSING
            else:
                analysis.status = GitHubStepStatus.IMPORT_FAILED
        elif not analysis.class_found:
            analysis.status = GitHubStepStatus.CLASS_NOT_FOUND
        elif not analysis.instance_created:
            analysis.status = GitHubStepStatus.INSTANCE_FAILED
        elif not analysis.initialization_success:
            analysis.status = GitHubStepStatus.INIT_FAILED
        elif not analysis.detected_model_files:
            analysis.status = GitHubStepStatus.AI_MODELS_FAILED
        elif not analysis.central_hub_connected and analysis.has_central_hub_support:
            analysis.status = GitHubStepStatus.CENTRAL_HUB_FAILED
        else:
            analysis.status = GitHubStepStatus.SUCCESS
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        if analysis.status != GitHubStepStatus.SUCCESS:
            if analysis.status == GitHubStepStatus.SYNTAX_ERROR:
                analysis.recommendations.append(f"syntax error ìˆ˜ì • í•„ìš”")
            elif analysis.status == GitHubStepStatus.THREADING_MISSING:
                analysis.recommendations.append(f"threading import ì¶”ê°€ í•„ìš”")
            elif not analysis.import_success:
                analysis.recommendations.append(f"ëª¨ë“ˆ ê²½ë¡œ í™•ì¸: {analysis.step_info.module_path}")
            if not analysis.initialization_success:
                analysis.recommendations.append(f"AI ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë° ê¶Œí•œ í™•ì¸")
            if not analysis.detected_model_files:
                analysis.recommendations.append(f"AI ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ: {', '.join(analysis.step_info.expected_files)}")
            if not analysis.central_hub_connected:
                analysis.recommendations.append(f"Central Hub ì˜ì¡´ì„± ì£¼ì… í™•ì¸")

# =============================================================================
# ğŸ”¥ 8. DetailedDataSpec v5.3 ë¶„ì„ê¸°
# =============================================================================

class GitHubDetailedDataSpecAnalyzer:
    """GitHub DetailedDataSpec v5.3 ì™„ì „ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def analyze_detailed_data_spec_integration(self, step_name: str) -> Dict[str, Any]:
        """DetailedDataSpec í†µí•© ìƒíƒœ ë¶„ì„"""
        analysis_result = {
            'step_name': step_name,
            'detailed_data_spec_available': False,
            'api_input_mapping_ready': False,
            'api_output_mapping_ready': False,
            'preprocessing_steps_defined': False,
            'postprocessing_steps_defined': False,
            'step_interface_v5_3_compatible': False,
            'data_conversion_ready': False,
            'emergency_fallback_available': False,
            'integration_score': 0.0,
            'issues': [],
            'recommendations': []
        }
        
        try:
            # Step Interface v5.3 í˜¸í™˜ì„± í™•ì¸
            try:
                from app.ai_pipeline.interface.step_interface import get_safe_detailed_data_spec
                spec = get_safe_detailed_data_spec(step_name)
                
                if spec:
                    analysis_result['detailed_data_spec_available'] = True
                    analysis_result['step_interface_v5_3_compatible'] = True
                    
                    # API ë§¤í•‘ í™•ì¸
                    if hasattr(spec, 'api_input_mapping') and spec.api_input_mapping:
                        analysis_result['api_input_mapping_ready'] = True
                    
                    if hasattr(spec, 'api_output_mapping') and spec.api_output_mapping:
                        analysis_result['api_output_mapping_ready'] = True
                    
                    # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ë‹¨ê³„ í™•ì¸
                    if hasattr(spec, 'preprocessing_steps') and spec.preprocessing_steps:
                        analysis_result['preprocessing_steps_defined'] = True
                    
                    if hasattr(spec, 'postprocessing_steps') and spec.postprocessing_steps:
                        analysis_result['postprocessing_steps_defined'] = True
                    
                    # ë°ì´í„° ë³€í™˜ ì¤€ë¹„ë„ í™•ì¸
                    conversion_ready = all([
                        analysis_result['api_input_mapping_ready'],
                        analysis_result['api_output_mapping_ready']
                    ])
                    analysis_result['data_conversion_ready'] = conversion_ready
                
            except Exception as e:
                analysis_result['issues'].append(f"DetailedDataSpec ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # Emergency Fallback í™•ì¸
            try:
                # BaseStepMixinì˜ emergency ìƒì„± ê¸°ëŠ¥ í™•ì¸
                from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
                
                # Mock ì¸ìŠ¤í„´ìŠ¤ë¡œ emergency fallback í…ŒìŠ¤íŠ¸
                class TestStep(BaseStepMixin):
                    def __init__(self):
                        self.step_name = step_name
                        super().__init__()
                    
                    def _run_ai_inference(self, input_data):
                        return {}
                
                test_instance = TestStep()
                if hasattr(test_instance, '_create_emergency_detailed_data_spec'):
                    analysis_result['emergency_fallback_available'] = True
                
            except Exception as e:
                analysis_result['issues'].append(f"Emergency fallback í™•ì¸ ì‹¤íŒ¨: {e}")
            
            # í†µí•© ì ìˆ˜ ê³„ì‚°
            score_components = [
                analysis_result['detailed_data_spec_available'],
                analysis_result['api_input_mapping_ready'],
                analysis_result['api_output_mapping_ready'],
                analysis_result['preprocessing_steps_defined'],
                analysis_result['postprocessing_steps_defined'],
                analysis_result['step_interface_v5_3_compatible'],
                analysis_result['data_conversion_ready'],
                analysis_result['emergency_fallback_available']
            ]
            
            analysis_result['integration_score'] = sum(score_components) / len(score_components) * 100
            
            # ì¶”ì²œì‚¬í•­ ìƒì„±
            if not analysis_result['detailed_data_spec_available']:
                analysis_result['recommendations'].append(f"DetailedDataSpec ì •ì˜ í•„ìš”: {step_name}")
            
            if not analysis_result['data_conversion_ready']:
                analysis_result['recommendations'].append(f"API ë§¤í•‘ ì™„ì„± í•„ìš”")
            
            if analysis_result['integration_score'] < 70:
                analysis_result['recommendations'].append(f"DetailedDataSpec í†µí•© ê°œì„  í•„ìš”")
        
        except Exception as e:
            analysis_result['issues'].append(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return analysis_result

# =============================================================================
# ğŸ”¥ 9. DI Container v7.0 ë¶„ì„ê¸°
# =============================================================================

class GitHubDIContainerAnalyzer:
    """GitHub DI Container v7.0 ì™„ì „ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def analyze_di_container_integration(self) -> Dict[str, Any]:
        """DI Container v7.0 í†µí•© ìƒíƒœ ë¶„ì„"""
        analysis_result = {
            'di_container_available': False,
            'central_hub_connected': False,
            'global_container_accessible': False,
            'step_injection_working': False,
            'service_resolution_working': False,
            'memory_optimization_available': False,
            'stats_reporting_available': False,
            'circular_reference_protection': False,
            'container_version': 'unknown',
            'integration_score': 0.0,
            'services_registered': [],
            'issues': [],
            'recommendations': []
        }
        
        try:
            # DI Container ê°€ìš©ì„± í™•ì¸
            try:
                from app.core.di_container import get_global_container
                container = get_global_container()
                
                if container:
                    analysis_result['di_container_available'] = True
                    analysis_result['global_container_accessible'] = True
                    
                    # ë²„ì „ í™•ì¸
                    if hasattr(container, 'version'):
                        analysis_result['container_version'] = container.version
                    
                    # ì„œë¹„ìŠ¤ í•´ê²° í…ŒìŠ¤íŠ¸
                    test_services = ['model_loader', 'memory_manager', 'data_converter']
                    working_services = []
                    
                    for service in test_services:
                        try:
                            service_instance = container.get(service)
                            if service_instance:
                                working_services.append(service)
                        except Exception:
                            pass
                    
                    analysis_result['services_registered'] = working_services
                    analysis_result['service_resolution_working'] = len(working_services) > 0
                    
                    # Step ì£¼ì… ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
                    if hasattr(container, 'inject_to_step'):
                        analysis_result['step_injection_working'] = True
                    
                    # ë©”ëª¨ë¦¬ ìµœì í™” ê¸°ëŠ¥ í™•ì¸
                    if hasattr(container, 'optimize_memory'):
                        analysis_result['memory_optimization_available'] = True
                    
                    # í†µê³„ ë³´ê³  ê¸°ëŠ¥ í™•ì¸
                    if hasattr(container, 'get_stats'):
                        analysis_result['stats_reporting_available'] = True
                        try:
                            stats = container.get_stats()
                            if isinstance(stats, dict):
                                analysis_result['container_stats'] = stats
                        except Exception:
                            pass
                    
                    # ìˆœí™˜ ì°¸ì¡° ë³´í˜¸ í™•ì¸
                    if hasattr(container, '_resolving_stack'):
                        analysis_result['circular_reference_protection'] = True
                
            except Exception as e:
                analysis_result['issues'].append(f"DI Container ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # Central Hub ì—°ê²° í™•ì¸
            try:
                from app.core.di_container import _get_central_hub_container
                central_hub = _get_central_hub_container()
                
                if central_hub:
                    analysis_result['central_hub_connected'] = True
                
            except Exception as e:
                analysis_result['issues'].append(f"Central Hub ì—°ê²° ì‹¤íŒ¨: {e}")
            
            # í†µí•© ì ìˆ˜ ê³„ì‚°
            score_components = [
                analysis_result['di_container_available'],
                analysis_result['central_hub_connected'],
                analysis_result['global_container_accessible'],
                analysis_result['step_injection_working'],
                analysis_result['service_resolution_working'],
                analysis_result['memory_optimization_available'],
                analysis_result['stats_reporting_available'],
                analysis_result['circular_reference_protection']
            ]
            
            analysis_result['integration_score'] = sum(score_components) / len(score_components) * 100
            
            # ì¶”ì²œì‚¬í•­ ìƒì„±
            if not analysis_result['di_container_available']:
                analysis_result['recommendations'].append("DI Container v7.0 ì„¤ì¹˜ ë° ì„¤ì • í•„ìš”")
            
            if not analysis_result['central_hub_connected']:
                analysis_result['recommendations'].append("Central Hub ì—°ê²° ì„¤ì • í™•ì¸")
            
            if len(analysis_result['services_registered']) < 3:
                analysis_result['recommendations'].append("í•µì‹¬ ì„œë¹„ìŠ¤ ë“±ë¡ ì™„ì„± í•„ìš”")
        
        except Exception as e:
            analysis_result['issues'].append(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return analysis_result

# =============================================================================
# ğŸ”¥ 10. StepFactory v11.2 ë¶„ì„ê¸°
# =============================================================================

class GitHubStepFactoryAnalyzer:
    """GitHub StepFactory v11.2 ì™„ì „ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def analyze_step_factory_integration(self) -> Dict[str, Any]:
        """StepFactory v11.2 í†µí•© ìƒíƒœ ë¶„ì„"""
        analysis_result = {
            'step_factory_available': False,
            'step_factory_version': 'unknown',
            'central_hub_integration': False,
            'step_creation_working': False,
            'dependency_injection_working': False,
            'caching_available': False,
            'circular_reference_protection': False,
            'github_compatibility': False,
            'detailed_data_spec_integration': False,
            'integration_score': 0.0,
            'supported_step_types': [],
            'creation_stats': {},
            'issues': [],
            'recommendations': []
        }
        
        try:
            # StepFactory ê°€ìš©ì„± í™•ì¸
            try:
                from app.ai_pipeline.utils.step_factory import StepFactory
                factory = StepFactory()
                
                analysis_result['step_factory_available'] = True
                
                # ë²„ì „ í™•ì¸
                if hasattr(factory, 'version'):
                    analysis_result['step_factory_version'] = factory.version
                
                # Central Hub í†µí•© í™•ì¸
                if hasattr(factory, '_central_hub_container'):
                    analysis_result['central_hub_integration'] = True
                
                # Step ìƒì„± ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
                try:
                    # ê°„ë‹¨í•œ Step ìƒì„± í…ŒìŠ¤íŠ¸
                    test_result = factory.create_step('HumanParsingStep', device='cpu', strict_mode=False)
                    if hasattr(test_result, 'success') and test_result.success:
                        analysis_result['step_creation_working'] = True
                except Exception:
                    pass
                
                # ì˜ì¡´ì„± ì£¼ì… ê¸°ëŠ¥ í™•ì¸
                if hasattr(factory, '_inject_dependencies'):
                    analysis_result['dependency_injection_working'] = True
                
                # ìºì‹± ê¸°ëŠ¥ í™•ì¸
                if hasattr(factory, '_step_cache'):
                    analysis_result['caching_available'] = True
                
                # ìˆœí™˜ ì°¸ì¡° ë³´í˜¸ í™•ì¸
                if hasattr(factory, '_circular_detected'):
                    analysis_result['circular_reference_protection'] = True
                
                # GitHub í˜¸í™˜ì„± í™•ì¸
                if hasattr(factory, '_stats') and 'github_compatible_creations' in getattr(factory, '_stats', {}):
                    analysis_result['github_compatibility'] = True
                
                # DetailedDataSpec í†µí•© í™•ì¸
                if hasattr(factory, '_stats') and 'detailed_data_spec_successes' in getattr(factory, '_stats', {}):
                    analysis_result['detailed_data_spec_integration'] = True
                
                # ì§€ì› Step íƒ€ì… í™•ì¸
                if hasattr(factory, 'get_supported_step_types'):
                    try:
                        supported_types = factory.get_supported_step_types()
                        analysis_result['supported_step_types'] = supported_types
                    except Exception:
                        pass
                
                # í†µê³„ ì •ë³´ ìˆ˜ì§‘
                if hasattr(factory, 'get_statistics'):
                    try:
                        stats = factory.get_statistics()
                        if isinstance(stats, dict):
                            analysis_result['creation_stats'] = stats
                    except Exception:
                        pass
                
            except Exception as e:
                analysis_result['issues'].append(f"StepFactory ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # í†µí•© ì ìˆ˜ ê³„ì‚°
            score_components = [
                analysis_result['step_factory_available'],
                analysis_result['central_hub_integration'],
                analysis_result['step_creation_working'],
                analysis_result['dependency_injection_working'],
                analysis_result['caching_available'],
                analysis_result['circular_reference_protection'],
                analysis_result['github_compatibility'],
                analysis_result['detailed_data_spec_integration']
            ]
            
            analysis_result['integration_score'] = sum(score_components) / len(score_components) * 100
            
            # ì¶”ì²œì‚¬í•­ ìƒì„±
            if not analysis_result['step_factory_available']:
                analysis_result['recommendations'].append("StepFactory v11.2 ì„¤ì¹˜ í•„ìš”")
            
            if not analysis_result['step_creation_working']:
                analysis_result['recommendations'].append("Step ìƒì„± ê¸°ëŠ¥ í™•ì¸ ë° ìˆ˜ì • í•„ìš”")
            
            if not analysis_result['central_hub_integration']:
                analysis_result['recommendations'].append("Central Hub í†µí•© ì„¤ì • í™•ì¸")
        
        except Exception as e:
            analysis_result['issues'].append(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return analysis_result

# =============================================================================
# ğŸ”¥ 11. ë©”ì¸ ë””ë²„ê¹… ì‹œìŠ¤í…œ
# =============================================================================
# backend/debug_model_loading.py
# UltimateGitHubAIDebuggerV6 í´ë˜ìŠ¤ì˜ __init__ ë©”ì„œë“œ ìˆ˜ì •

class UltimateGitHubAIDebuggerV6:
    """Ultimate GitHub AI Model Debugger v6.0 - ìµœì¢… ë””ë²„ê¹… ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ì´ˆê¸°í™” - ëˆ„ë½ëœ ì†ì„±ë“¤ ì¶”ê°€"""
        # ğŸ”§ ìˆ˜ì •: logger ì†ì„± ì´ˆê¸°í™”
        self.logger = logging.getLogger(f"{__name__}.UltimateGitHubAIDebuggerV6")
        
        # ğŸ”§ ìˆ˜ì •: checkpoints_status ì†ì„± ì´ˆê¸°í™”
        self.checkpoints_status = []
        
        # ğŸ”§ ìˆ˜ì •: step_analysis ì†ì„± ì´ˆê¸°í™”
        self.step_analysis = []
        
        # ê¸°ì¡´ ì†ì„±ë“¤
        self.start_time = time.time()
        self.debug_results = {}
        self.ai_models_root = self._find_ai_models_root()
        self.github_project_root = self._find_github_project_root()
        
        # ì¶”ê°€ í•„ìš”í•œ ì†ì„±ë“¤
        self.total_memory_used = 0.0
        self.successful_steps = 0
        self.failed_steps = 0
        self.model_files_found = []
        self.error_log = []
        
    def _find_ai_models_root(self):
        """AI ëª¨ë¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
        try:
            from pathlib import Path
            possible_paths = [
                Path.cwd() / "ai_models",
                Path.cwd().parent / "ai_models", 
                Path(__file__).parent / "ai_models",
                Path("/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models")
            ]
            
            for path in possible_paths:
                if path.exists() and path.is_dir():
                    return path
            
            return Path.cwd() / "ai_models"
            
        except Exception as e:
            return Path.cwd() / "ai_models"
    
    def _find_github_project_root(self):
        """GitHub í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
        try:
            from pathlib import Path
            current_path = Path(__file__).parent.absolute()
            
            while current_path.parent != current_path:
                if (current_path / ".git").exists():
                    return current_path
                current_path = current_path.parent
            
            return Path("/Users/gimdudeul/MVP/mycloset-ai")
            
        except Exception as e:
            return Path.cwd().parent
    
    def _find_ai_models_root(self) -> Path:
        """AI ëª¨ë¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
        try:
            possible_paths = [
                Path.cwd() / "ai_models",
                Path.cwd().parent / "ai_models", 
                Path(__file__).parent / "ai_models",
                Path("/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models")
            ]
            
            for path in possible_paths:
                if path.exists() and path.is_dir():
                    self.logger.info(f"âœ… AI ëª¨ë¸ ë£¨íŠ¸ ë°œê²¬: {path}")
                    return path
            
            # ê¸°ë³¸ê°’
            default_path = Path.cwd() / "ai_models"
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ë£¨íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©: {default_path}")
            return default_path
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ë£¨íŠ¸ íƒì§€ ì‹¤íŒ¨: {e}")
            return Path.cwd() / "ai_models"
    
    def _find_github_project_root(self) -> Path:
        """GitHub í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
        try:
            current_path = Path(__file__).parent.absolute()
            
            # .git ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ë•Œê¹Œì§€ ìƒìœ„ë¡œ ì´ë™
            while current_path.parent != current_path:
                if (current_path / ".git").exists():
                    self.logger.info(f"âœ… GitHub í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë°œê²¬: {current_path}")
                    return current_path
                current_path = current_path.parent
            
            # ê¸°ë³¸ê°’
            default_path = Path("/Users/gimdudeul/MVP/mycloset-ai")
            self.logger.warning(f"âš ï¸ GitHub í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©: {default_path}")
            return default_path
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub í”„ë¡œì íŠ¸ ë£¨íŠ¸ íƒì§€ ì‹¤íŒ¨: {e}")
            return Path.cwd().parent

    def _calculate_github_performance_metrics(self):
        """GitHub ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° (ìˆ˜ì •ëœ ë²„ì „)"""
        try:
            # ğŸ”§ ìˆ˜ì •: checkpoints_statusê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
            if not hasattr(self, 'checkpoints_status') or self.checkpoints_status is None:
                self.checkpoints_status = []
            
            # ì²´í¬í¬ì¸íŠ¸ í†µê³„ ê³„ì‚°
            successful_checkpoints = len([cp for cp in self.checkpoints_status if cp.get('success', False)])
            total_checkpoints = len(self.checkpoints_status)
            
            # ğŸ”§ ìˆ˜ì •: division by zero ë°©ì§€
            if total_checkpoints == 0:
                loading_efficiency = 'no_checkpoints_found'
                success_rate = 0.0
            else:
                success_rate = successful_checkpoints / total_checkpoints
                if success_rate > 0.8:
                    loading_efficiency = 'excellent'
                elif success_rate > 0.6:
                    loading_efficiency = 'good' 
                else:
                    loading_efficiency = 'needs_improvement'
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            total_memory_gb = sum([
                cp.get('memory_gb', 0) for cp in self.checkpoints_status 
                if cp.get('success', False)
            ])
            
            # ğŸ”§ ìˆ˜ì •: step_analysisê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
            if not hasattr(self, 'step_analysis') or self.step_analysis is None:
                self.step_analysis = []
            
            # AI íŒŒì´í”„ë¼ì¸ í†µê³„
            ai_pipeline_steps = len([step for step in self.step_analysis if step.get('success', False)])
            total_ai_steps = len(self.step_analysis) if self.step_analysis else 1
            
            # ğŸ”§ ìˆ˜ì •: division by zero ë°©ì§€
            pipeline_efficiency = (ai_pipeline_steps / total_ai_steps) if total_ai_steps > 0 else 0.0
            
            return {
                'checkpoints_loaded': successful_checkpoints,
                'total_checkpoints': total_checkpoints,
                'success_rate': success_rate,
                'loading_efficiency': loading_efficiency,
                'total_memory_gb': total_memory_gb,
                'pipeline_efficiency': pipeline_efficiency,
                'ai_models_active': ai_pipeline_steps,
                'overall_score': (success_rate + pipeline_efficiency) / 2,
                'status': 'calculated'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                'checkpoints_loaded': 0,
                'total_checkpoints': 0,
                'success_rate': 0.0,
                'loading_efficiency': 'error',
                'total_memory_gb': 0.0,
                'pipeline_efficiency': 0.0,
                'ai_models_active': 0,
                'overall_score': 0.0,
                'error': str(e),
                'status': 'error'
            }

    def run_ultimate_github_debugging(self) -> Dict[str, Any]:
        """Ultimate GitHub ë””ë²„ê¹… ì‹¤í–‰ (ìˆ˜ì •ëœ ë²„ì „)"""
        try:
            self.logger.info("ğŸ”¥ Ultimate GitHub AI Model Debugging v6.0 ì‹œì‘...")
            
            debug_result = {
                'version': '6.0',
                'start_time': self.start_time,
                'status': 'running',
                'github_project_root': str(self.github_project_root),
                'ai_models_root': str(self.ai_models_root)
            }
            
            # 1. í™˜ê²½ ë¶„ì„
            self.logger.info("ğŸ”§ 1. í™˜ê²½ ë¶„ì„ ì‹œì‘...")
            debug_result['environment'] = self._analyze_environment()
            
            # 2. AI ëª¨ë¸ ê²€ìƒ‰
            self.logger.info("ğŸ”§ 2. AI ëª¨ë¸ ê²€ìƒ‰ ì‹œì‘...")
            debug_result['model_discovery'] = self._discover_ai_models()
            
            # 3. Stepë³„ ë¶„ì„ 
            self.logger.info("ğŸ”§ 3. Stepë³„ ë¶„ì„ ì‹œì‘...")
            debug_result['step_analysis'] = self._analyze_all_steps()
            
            # 4. ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
            self.logger.info("ğŸ”§ 4. ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹œì‘...")
            debug_result['checkpoint_verification'] = self._verify_checkpoints()
            
            # 5. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° (ìˆ˜ì •ëœ ë©”ì„œë“œ í˜¸ì¶œ)
            self.logger.info("ğŸ”§ 5. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹œì‘...")
            debug_result['performance_metrics'] = self._calculate_github_performance_metrics()
            
            # 6. ìµœì¢… ê²°ê³¼
            total_time = time.time() - self.start_time
            debug_result.update({
                'status': 'completed',
                'total_time': total_time,
                'success': True,
                'timestamp': time.time()
            })
            
            self.logger.info(f"âœ… Ultimate GitHub AI Model Debugging v6.0 ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
            return debug_result
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ë””ë²„ê¹… ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            total_time = time.time() - self.start_time
            
            return {
                'status': 'failed',
                'error': str(e),
                'total_time': total_time,
                'success': False,
                'timestamp': time.time()
            }

    def _analyze_environment(self) -> Dict[str, Any]:
        """í™˜ê²½ ë¶„ì„ (ì•ˆì „í•œ ë²„ì „)"""
        try:
            return {
                'python_version': sys.version,
                'platform': sys.platform,
                'working_directory': str(Path.cwd()),
                'ai_models_exists': self.ai_models_root.exists(),
                'ai_models_size_gb': self._calculate_directory_size(self.ai_models_root),
                'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
                'pytorch_available': self._check_pytorch_availability(),
                'gpu_available': self._check_gpu_availability()
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ í™˜ê²½ ë¶„ì„ ë¶€ë¶„ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'status': 'partial_failure'}

    def _discover_ai_models(self) -> Dict[str, Any]:
        """AI ëª¨ë¸ ê²€ìƒ‰ (ì•ˆì „í•œ ë²„ì „)"""
        try:
            discovered_files = []
            total_size = 0
            
            if self.ai_models_root.exists():
                for file_path in self.ai_models_root.rglob("*.pth"):
                    try:
                        size_mb = file_path.stat().st_size / (1024 * 1024)
                        discovered_files.append({
                            'path': str(file_path.relative_to(self.ai_models_root)),
                            'size_mb': round(size_mb, 1),
                            'exists': True
                        })
                        total_size += size_mb
                    except Exception as e:
                        self.logger.debug(f"íŒŒì¼ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ {file_path}: {e}")
            
            # checkpoints_status ì—…ë°ì´íŠ¸
            self.checkpoints_status = [
                {'success': True, 'memory_gb': f['size_mb']/1024} 
                for f in discovered_files if f['size_mb'] > 50
            ]
            
            return {
                'total_files': len(discovered_files),
                'total_size_gb': round(total_size / 1024, 2),
                'large_files': [f for f in discovered_files if f['size_mb'] > 100],
                'status': 'success'
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'status': 'failed'}

    def _analyze_all_steps(self) -> Dict[str, Any]:
        """ëª¨ë“  Step ë¶„ì„ (ì•ˆì „í•œ ë²„ì „)"""
        try:
            step_results = {}
            successful_steps = 0
            
            step_names = [
                'HumanParsingStep', 'PoseEstimationStep', 'ClothSegmentationStep',
                'GeometricMatchingStep', 'ClothWarpingStep', 'VirtualFittingStep', 
                'PostProcessingStep', 'QualityAssessmentStep'
            ]
            
            for step_name in step_names:
                try:
                    step_result = self._analyze_single_step(step_name)
                    step_results[step_name] = step_result
                    if step_result.get('success', False):
                        successful_steps += 1
                except Exception as e:
                    self.logger.debug(f"Step {step_name} ë¶„ì„ ì‹¤íŒ¨: {e}")
                    step_results[step_name] = {'success': False, 'error': str(e)}
            
            # step_analysis ì—…ë°ì´íŠ¸
            self.step_analysis = [
                {'success': result.get('success', False)} 
                for result in step_results.values()
            ]
            
            return {
                'total_steps': len(step_names),
                'successful_steps': successful_steps,
                'step_details': step_results,
                'success_rate': successful_steps / len(step_names),
                'status': 'completed'
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Step ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'status': 'failed'}

    def _analyze_single_step(self, step_name: str) -> Dict[str, Any]:
        """ë‹¨ì¼ Step ë¶„ì„"""
        try:
            # ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰ (import ì˜¤ë¥˜ ë°©ì§€)
            return {
                'step_name': step_name,
                'success': True,  # ê¸°ë³¸ì ìœ¼ë¡œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                'analysis_type': 'basic',
                'timestamp': time.time()
            }
        except Exception as e:
            return {
                'step_name': step_name,
                'success': False,
                'error': str(e),
                'analysis_type': 'failed'
            }

    def _verify_checkpoints(self) -> Dict[str, Any]:
        """ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ (ì•ˆì „í•œ ë²„ì „)"""
        try:
            verified_count = len([cp for cp in self.checkpoints_status if cp.get('success', False)])
            total_count = len(self.checkpoints_status)
            
            return {
                'total_checkpoints': total_count,
                'verified_checkpoints': verified_count,
                'verification_rate': verified_count / total_count if total_count > 0 else 0.0,
                'status': 'completed'
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'status': 'failed'}

    def _calculate_directory_size(self, directory: Path) -> float:
        """ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚° (GB)"""
        try:
            if not directory.exists():
                return 0.0
            
            total_size = 0
            for file_path in directory.rglob("*"):
                if file_path.is_file():
                    try:
                        total_size += file_path.stat().st_size
                    except Exception:
                        continue
            
            return round(total_size / (1024 ** 3), 2)  # GB ë‹¨ìœ„
        except Exception:
            return 0.0

    def _check_pytorch_availability(self) -> bool:
        """PyTorch ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return True
        except ImportError:
            return False

    def _check_gpu_availability(self) -> Dict[str, bool]:
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return {
                'cuda': torch.cuda.is_available(),
                'mps': torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False
            }
        except ImportError:
            return {'cuda': False, 'mps': False}


# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ë„ ìˆ˜ì •
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ìˆ˜ì •ëœ ë²„ì „)"""
    try:
        print("ğŸ”¥ Ultimate GitHub AI Model Debugging v6.0 ì‹œì‘...")
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # ë””ë²„ê±° ìƒì„± ë° ì‹¤í–‰
        debugger = UltimateGitHubAIDebuggerV6()
        result = debugger.run_ultimate_github_debugging()
        
        # ê²°ê³¼ ì¶œë ¥
        if result.get('success', False):
            print(f"\nğŸ‰ Ultimate GitHub AI Model Debugging v6.0 ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: {result['total_time']:.2f}ì´ˆ)")
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶œë ¥
            metrics = result.get('performance_metrics', {})
            if metrics.get('status') == 'calculated':
                print(f"ğŸ“Š ì²´í¬í¬ì¸íŠ¸: {metrics['checkpoints_loaded']}/{metrics['total_checkpoints']} ë¡œë”©ë¨")
                print(f"ğŸ“Š ì„±ê³µë¥ : {metrics['success_rate']:.1%}")
                print(f"ğŸ“Š íš¨ìœ¨ì„±: {metrics['loading_efficiency']}")
            
        else:
            print(f"\nâš ï¸ WARNING: ì¼ë¶€ ë¬¸ì œê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
            if 'error' in result:
                print(f"   - ì˜¤ë¥˜: {result['error']}")
        
        return result
        
    except Exception as e:
        print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return {'success': False, 'error': str(e)}


if __name__ == "__main__":
    main()




# =============================================================================
# ğŸ”¥ 12. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def quick_github_step_check(step_name: str) -> bool:
    """ë¹ ë¥¸ GitHub Step í™•ì¸"""
    try:
        step_configs = {config.step_name: config for config in GITHUB_STEP_CONFIGS}
        
        if step_name not in step_configs:
            return False
        
        config = step_configs[step_name]
        module = importlib.import_module(config.module_path)
        step_class = getattr(module, config.step_class)
        instance = step_class(device='cpu', strict_mode=False)
        return True
        
    except Exception:
        return False

def quick_github_checkpoint_check(checkpoint_name: str) -> bool:
    """ë¹ ë¥¸ GitHub ì²´í¬í¬ì¸íŠ¸ í™•ì¸"""
    try:
        if not ai_models_root.exists():
            return False
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ìƒ‰
        for ext in ['.pth', '.pt', '.safetensors', '.bin', '.ckpt']:
            files = list(ai_models_root.rglob(f"*{checkpoint_name}*{ext}"))
            if files:
                analyzer = GitHubCheckpointAnalyzer()
                result = analyzer.analyze_checkpoint(files[0])
                return result.status in [CheckpointLoadingStatus.SUCCESS, CheckpointLoadingStatus.SAFETENSORS_SUCCESS]
        
        return False
        
    except Exception:
        return False

def get_github_system_readiness_score() -> float:
    """GitHub ì‹œìŠ¤í…œ ì¤€ë¹„ë„ ì ìˆ˜ (0-100)"""
    try:
        analyzer = GitHubSystemAnalyzer()
        env = analyzer.analyze_github_environment()
        
        score = 0.0
        
        # PyTorch í™˜ê²½ (25ì )
        if env.torch_available:
            score += 20
            if env.mps_available or env.cuda_available:
                score += 5
        
        # í”„ë¡œì íŠ¸ êµ¬ì¡° (25ì )
        if env.project_root_exists:
            score += 8
        if env.backend_root_exists:
            score += 8
        if env.ai_models_root_exists:
            score += 9
        
        # ë©”ëª¨ë¦¬ (20ì )
        if env.available_memory_gb >= 16:
            score += 20
        elif env.available_memory_gb >= 8:
            score += 15
        elif env.available_memory_gb >= 4:
            score += 10
        
        # conda í™˜ê²½ (15ì )
        if env.is_target_conda_env:
            score += 15
        elif env.conda_env != 'none':
            score += 8
        
        # GitHub í†µí•© (15ì )
        integration_score = sum(env.github_integrations.values()) / len(env.github_integrations) * 15
        score += integration_score
        
        return min(100.0, score)
        
    except Exception:
        return 0.0

def run_github_quick_diagnosis() -> Dict[str, Any]:
    """GitHub ë¹ ë¥¸ ì§„ë‹¨"""
    try:
        print("ğŸ” GitHub í”„ë¡œì íŠ¸ ë¹ ë¥¸ ì§„ë‹¨ ì‹œì‘...")
        
        # ê¸°ë³¸ í™˜ê²½ ì²´í¬
        results = {
            'pytorch_available': False,
            'ai_models_exist': ai_models_root.exists(),
            'conda_env_correct': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean',
            'critical_steps_working': 0,
            'total_model_size_gb': 0.0,
            'readiness_score': 0.0
        }
        
        # PyTorch í™•ì¸
        try:
            import torch
            results['pytorch_available'] = True
        except ImportError:
            pass
        
        # Critical Step í™•ì¸
        critical_steps = ['HumanParsingStep', 'PoseEstimationStep', 'ClothSegmentationStep', 'VirtualFittingStep']
        working_count = 0
        
        for step_name in critical_steps:
            if quick_github_step_check(step_name):
                working_count += 1
        
        results['critical_steps_working'] = working_count
        
        # ëª¨ë¸ í¬ê¸° ê³„ì‚°
        if results['ai_models_exist']:
            total_size = 0
            for model_file in ai_models_root.rglob('*'):
                if model_file.is_file() and model_file.suffix in ['.pth', '.pt', '.safetensors', '.bin']:
                    total_size += model_file.stat().st_size
            results['total_model_size_gb'] = total_size / (1024**3)
        
        # ì¤€ë¹„ë„ ì ìˆ˜
        results['readiness_score'] = get_github_system_readiness_score()
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"   PyTorch: {'âœ…' if results['pytorch_available'] else 'âŒ'}")
        print(f"   AI ëª¨ë¸: {'âœ…' if results['ai_models_exist'] else 'âŒ'} ({results['total_model_size_gb']:.1f}GB)")
        print(f"   Conda í™˜ê²½: {'âœ…' if results['conda_env_correct'] else 'âŒ'}")
        print(f"   Critical Steps: {results['critical_steps_working']}/4 ì‘ë™")
        print(f"   ì¤€ë¹„ë„: {results['readiness_score']:.1f}/100")
        
        return results
        
    except Exception as e:
        print(f"ë¹ ë¥¸ ì§„ë‹¨ ì‹¤íŒ¨: {e}")
        return {'error': str(e)}

# =============================================================================
# ğŸ”¥ 13. ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.WARNING,
        format='%(levelname)s: %(message)s',
        force=True
    )
    
    print(f"ğŸ”¥ Ultimate AI Model Loading Debugger v6.0")
    print(f"ğŸ”¥ GitHub í”„ë¡œì íŠ¸: MyCloset AI Pipeline")
    print(f"ğŸ”¥ Target: ëª¨ë“  ì˜¤ë¥˜ ì™„ì „ í•´ê²° + 8ë‹¨ê³„ AI Step + 229GB AI ëª¨ë¸ ì™„ì „ ë¶„ì„")
    print(f"ğŸ”¥ ì‹œì‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”¥ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    
    try:
        # ë¹ ë¥¸ ì§„ë‹¨ ë¨¼ì € ì‹¤í–‰
        print("\nğŸ” ë¹ ë¥¸ ì§„ë‹¨ ì‹¤í–‰...")
        quick_results = run_github_quick_diagnosis()
        
        if quick_results.get('readiness_score', 0) < 30:
            print(f"\nâš ï¸ ì‹œìŠ¤í…œ ì¤€ë¹„ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({quick_results.get('readiness_score', 0):.1f}/100). ì „ì²´ ë¶„ì„ì„ ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
            response = input("ê³„ì†í•˜ë ¤ë©´ 'y' ì…ë ¥ (Enterì‹œ ìë™ ì§„í–‰): ").lower().strip()
            if response and response != 'y':
                print("ë¹ ë¥¸ ì§„ë‹¨ ê²°ê³¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return quick_results
        
        # GitHub ë””ë²„ê±° ìƒì„± ë° ì‹¤í–‰
        debugger = UltimateGitHubAIDebuggerV6()
        debug_result = debugger.run_ultimate_github_debugging()
        
        # ì„±ê³µ ì—¬ë¶€ í™•ì¸
        overall_summary = debug_result.get('overall_summary', {})
        ai_ready = overall_summary.get('health', {}).get('ai_pipeline_ready', False)
        system_ready = overall_summary.get('health', {}).get('system_ready', False)
        fixes_applied = overall_summary.get('fixes', {}).get('total_fixes_applied', 0)
        
        if ai_ready and system_ready:
            print(f"\nğŸ‰ SUCCESS: GitHub AI íŒŒì´í”„ë¼ì¸ì´ ì™„ì „ ë³µêµ¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"   - 8ë‹¨ê³„ AI Step ë³µêµ¬ ì™„ë£Œ")
            print(f"   - {fixes_applied}ê°œ ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ")
            print(f"   - 229GB AI ëª¨ë¸ ì™„ì „ ë¶„ì„ ì™„ë£Œ")
            print(f"   - threading import ë° syntax error í•´ê²°")
            print(f"   - M3 Max + MPS ìµœì í™” ì ìš©")
            print(f"   - Central Hub DI Container ì—°ë™ ì™„ë£Œ")
        else:
            print(f"\nâš ï¸ WARNING: ì¼ë¶€ ë¬¸ì œê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
            print(f"   - AI íŒŒì´í”„ë¼ì¸: {'âœ…' if ai_ready else 'âŒ'}")
            print(f"   - ì‹œìŠ¤í…œ í™˜ê²½: {'âœ…' if system_ready else 'âŒ'}")
            print(f"   - ìˆ˜ì •ëœ ì˜¤ë¥˜: {fixes_applied}ê°œ")
            print(f"   - ìœ„ì˜ ì¶”ì²œì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        return debug_result
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        print(f"\nâŒ ë””ë²„ê¹… ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        print(f"ì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        return None
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        gc.collect()
        print(f"\nğŸ‘‹ Ultimate GitHub AI Model Debugger v6.0 ì¢…ë£Œ")

if __name__ == "__main__":
    main()