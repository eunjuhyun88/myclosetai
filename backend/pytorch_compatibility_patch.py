#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - PyTorch 2.7+ í˜¸í™˜ì„± ì™„ì „ íŒ¨ì¹˜ v2.0
================================================================================
âœ… weights_only=True ê¸°ë³¸ê°’ ë³€ê²½ ë¬¸ì œ í•´ê²°
âœ… Legacy .tar í˜•ì‹ ëª¨ë¸ ìë™ ë³€í™˜
âœ… TorchScript ëª¨ë¸ í˜¸í™˜ì„± ì²˜ë¦¬
âœ… ì•ˆì „í•œ ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ ì œê³µ
âœ… ëŒ€ëŸ‰ ëª¨ë¸ ì¼ê´„ ì²˜ë¦¬
================================================================================
"""

import os
import torch
import logging
import warnings
from pathlib import Path
from typing import Optional, Dict, Any, List
import shutil
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PyTorchCompatibilityPatcher:
    """PyTorch 2.7+ í˜¸í™˜ì„± íŒ¨ì¹˜ í´ë˜ìŠ¤"""
    
    def __init__(self, ai_models_root: str = "ai_models"):
        self.ai_models_root = Path(ai_models_root)
        self.fixed_count = 0
        self.failed_count = 0
        self.skipped_count = 0
        self.backup_dir = self.ai_models_root / "backup_originals"
        
        # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        self.backup_dir.mkdir(exist_ok=True)
        
        logger.info("ğŸ”¥ PyTorch í˜¸í™˜ì„± íŒ¨ì²˜ v2.0 ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ“ AI ëª¨ë¸ ë£¨íŠ¸: {self.ai_models_root}")
        logger.info(f"ğŸ’¾ ë°±ì—… ë””ë ‰í† ë¦¬: {self.backup_dir}")
    
    def safe_load_checkpoint(self, model_path: Path, map_location: str = 'cpu') -> Optional[Dict[str, Any]]:
        """ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (3ë‹¨ê³„ í´ë°±)"""
        try:
            # 1ë‹¨ê³„: ì•ˆì „ ëª¨ë“œ ì‹œë„
            try:
                return torch.load(model_path, map_location=map_location, weights_only=True)
            except Exception as e1:
                logger.debug(f"ì•ˆì „ ëª¨ë“œ ì‹¤íŒ¨: {e1}")
                
                # 2ë‹¨ê³„: í˜¸í™˜ì„± ëª¨ë“œ ì‹œë„  
                try:
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        return torch.load(model_path, map_location=map_location, weights_only=False)
                except Exception as e2:
                    logger.debug(f"í˜¸í™˜ì„± ëª¨ë“œ ì‹¤íŒ¨: {e2}")
                    
                    # 3ë‹¨ê³„: TorchScript ì‹œë„
                    try:
                        return torch.jit.load(model_path, map_location=map_location)
                    except Exception as e3:
                        logger.error(f"ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {e3}")
                        return None
                        
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ì „ ì‹¤íŒ¨: {e}")
            return None
    
    def fix_single_model(self, model_path: Path) -> bool:
        """ë‹¨ì¼ ëª¨ë¸ íŒŒì¼ ìˆ˜ì •"""
        try:
            # íŒŒì¼ ì¡´ì¬ ë° ìœ íš¨ì„± ê²€ì‚¬
            if not model_path.exists():
                logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {model_path}")
                self.skipped_count += 1
                return False
                
            if not model_path.is_file():
                logger.warning(f"âš ï¸ íŒŒì¼ì´ ì•„ë‹˜: {model_path}")
                self.skipped_count += 1
                return False
                
            # ì‹¬ë³¼ë¦­ ë§í¬ ì²˜ë¦¬
            if model_path.is_symlink():
                resolved_path = model_path.resolve()
                if not resolved_path.exists():
                    logger.warning(f"ğŸ”— ê¹¨ì§„ ì‹¬ë³¼ë¦­ ë§í¬ ì œê±°: {model_path}")
                    model_path.unlink()
                    self.skipped_count += 1
                    return False
                model_path = resolved_path
            
            # íŒŒì¼ í¬ê¸° ê²€ì‚¬
            try:
                file_size = model_path.stat().st_size
                if file_size == 0:
                    logger.warning(f"âš ï¸ íŒŒì¼ í¬ê¸° 0: {model_path}")
                    self.skipped_count += 1
                    return False
            except OSError as e:
                logger.warning(f"âš ï¸ íŒŒì¼ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {model_path} - {e}")
                self.skipped_count += 1
                return False
            
            logger.info(f"ğŸ”§ ìˆ˜ì • ì¤‘: {model_path} ({file_size / (1024**2):.1f}MB)")
            
            # ë°±ì—… ìƒì„± (ì•ˆì „í•œ ê²½ë¡œë¡œ)
            try:
                backup_path = self.backup_dir / f"{model_path.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.backup"
                shutil.copy2(model_path, backup_path)
                logger.debug(f"ğŸ’¾ ë°±ì—… ìƒì„±: {backup_path}")
            except Exception as backup_error:
                logger.warning(f"âš ï¸ ë°±ì—… ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {backup_error}")
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = self.safe_load_checkpoint(model_path)
            if checkpoint is None:
                logger.error(f"âŒ ë¡œë”© ì‹¤íŒ¨: {model_path}")
                self.failed_count += 1
                return False
            
            # ì•ˆì „í•œ í˜•íƒœë¡œ ì¬ì €ì¥
            temp_path = model_path.with_suffix(f'.tmp_{os.getpid()}')
            
            try:
                # TorchScript ëª¨ë¸ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                if hasattr(checkpoint, 'save'):
                    checkpoint.save(str(temp_path))
                else:
                    torch.save(
                        checkpoint, 
                        temp_path, 
                        _use_new_zipfile_serialization=True
                    )
                
                # ì›ë³¸ êµì²´
                if temp_path.exists():
                    model_path.unlink()
                    temp_path.rename(model_path)
                    
                    logger.info(f"âœ… ìˆ˜ì • ì™„ë£Œ: {model_path}")
                    self.fixed_count += 1
                    return True
                else:
                    logger.error(f"âŒ ì„ì‹œ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {temp_path}")
                    self.failed_count += 1
                    return False
                    
            except Exception as save_error:
                logger.error(f"âŒ ì €ì¥ ì‹¤íŒ¨ {model_path}: {save_error}")
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if temp_path.exists():
                    temp_path.unlink()
                self.failed_count += 1
                return False
            
        except Exception as e:
            logger.error(f"âŒ ìˆ˜ì • ì‹¤íŒ¨ {model_path}: {e}")
            self.failed_count += 1
            return False
    
    def find_problematic_models(self) -> List[Path]:
        """ë¬¸ì œê°€ ìˆëŠ” ëª¨ë¸ íŒŒì¼ë“¤ ì°¾ê¸°"""
        problematic_models = []
        
        # ì•Œë ¤ì§„ ë¬¸ì œ ëª¨ë¸ë“¤
        known_problems = [
            "u2net.pth",
            "hrviton_final.pth", 
            "lpips_alex.pth",
            "graphonomy_damaged.pth",
            "ViT-L-14.pt",
            "ViT-B-32.pt"
        ]
        
        for pattern in known_problems:
            try:
                found_files = list(self.ai_models_root.rglob(pattern))
                for file_path in found_files:
                    if file_path.exists() and file_path.is_file():
                        problematic_models.append(file_path)
            except Exception as e:
                logger.warning(f"âš ï¸ íŒ¨í„´ ê²€ìƒ‰ ì‹¤íŒ¨ {pattern}: {e}")
                continue
        
        # í¬ê¸°ê°€ 0ì´ê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” íŒŒì¼ë“¤
        for ext in ['*.pth', '*.pt', '*.bin', '*.safetensors']:
            try:
                for model_file in self.ai_models_root.rglob(ext):
                    try:
                        # íŒŒì¼ ì¡´ì¬ ë° ì ‘ê·¼ ê°€ëŠ¥ í™•ì¸
                        if not model_file.exists() or not model_file.is_file():
                            continue
                            
                        # ì‹¬ë³¼ë¦­ ë§í¬ì¸ ê²½ìš° ì‹¤ì œ íŒŒì¼ í™•ì¸
                        if model_file.is_symlink():
                            if not model_file.resolve().exists():
                                logger.warning(f"ğŸ”— ê¹¨ì§„ ì‹¬ë³¼ë¦­ ë§í¬ ë°œê²¬: {model_file}")
                                model_file.unlink()  # ê¹¨ì§„ ë§í¬ ì œê±°
                                continue
                        
                        # íŒŒì¼ í¬ê¸° í™•ì¸
                        if model_file.stat().st_size == 0:
                            problematic_models.append(model_file)
                            
                    except (OSError, FileNotFoundError) as e:
                        logger.warning(f"âš ï¸ íŒŒì¼ ì ‘ê·¼ ì‹¤íŒ¨ {model_file}: {e}")
                        continue
                        
            except Exception as e:
                logger.warning(f"âš ï¸ í™•ì¥ì ê²€ìƒ‰ ì‹¤íŒ¨ {ext}: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_models = []
        seen_paths = set()
        for model in problematic_models:
            if model not in seen_paths:
                unique_models.append(model)
                seen_paths.add(model)
        
        return unique_models
    
    def patch_all_models(self) -> Dict[str, int]:
        """ëª¨ë“  ëª¨ë¸ ì¼ê´„ íŒ¨ì¹˜"""
        logger.info("ğŸš€ PyTorch í˜¸í™˜ì„± íŒ¨ì¹˜ ì‹œì‘...")
        
        # ë¬¸ì œ ëª¨ë¸ë“¤ ì°¾ê¸°
        problematic_models = self.find_problematic_models()
        
        logger.info(f"ğŸ“‹ ë°œê²¬ëœ ë¬¸ì œ ëª¨ë¸: {len(problematic_models)}ê°œ")
        
        for model_path in problematic_models:
            logger.info(f"ğŸ”§ ì²˜ë¦¬ ì¤‘: {model_path.relative_to(self.ai_models_root)}")
            
            if model_path.stat().st_size == 0:
                logger.warning(f"ğŸ—‘ï¸ í¬ê¸° 0 íŒŒì¼ ì‚­ì œ: {model_path}")
                model_path.unlink()
                self.skipped_count += 1
                continue
            
            self.fix_single_model(model_path)
        
        # ì¶”ê°€: Legacy .tar í˜•ì‹ ëª¨ë¸ ë³€í™˜
        self._convert_legacy_tar_models()
        
        # ê²°ê³¼ ë°˜í™˜
        results = {
            'fixed': self.fixed_count,
            'failed': self.failed_count, 
            'skipped': self.skipped_count,
            'total_processed': len(problematic_models)
        }
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ PyTorch í˜¸í™˜ì„± íŒ¨ì¹˜ ì™„ë£Œ!")
        logger.info(f"âœ… ìˆ˜ì • ì„±ê³µ: {self.fixed_count}ê°œ")
        logger.info(f"âŒ ìˆ˜ì • ì‹¤íŒ¨: {self.failed_count}ê°œ") 
        logger.info(f"âš ï¸ ê±´ë„ˆëœ€: {self.skipped_count}ê°œ")
        logger.info(f"ğŸ“Š ì´ ì²˜ë¦¬: {len(problematic_models)}ê°œ")
        logger.info("=" * 60)
        
        return results
    
    def _convert_legacy_tar_models(self):
        """Legacy .tar í˜•ì‹ ëª¨ë¸ë“¤ ë³€í™˜"""
        logger.info("ğŸ”„ Legacy .tar í˜•ì‹ ëª¨ë¸ ê²€ì‚¬...")
        
        legacy_models = [
            "hrviton_final.pth",
            "lpips_alex.pth"
        ]
        
        for model_name in legacy_models:
            found_files = list(self.ai_models_root.rglob(model_name))
            for model_path in found_files:
                if model_path.exists() and model_path.stat().st_size > 0:
                    logger.info(f"ğŸ”„ Legacy ëª¨ë¸ ë³€í™˜: {model_path}")
                    self.fix_single_model(model_path)
    
    def create_safe_loading_wrapper(self):
        """ì•ˆì „í•œ ë¡œë”© ë˜í¼ í•¨ìˆ˜ ìƒì„±"""
        wrapper_code = '''
def safe_torch_load(file_path, map_location='cpu', **kwargs):
    """PyTorch 2.7+ í˜¸í™˜ ì•ˆì „ ë¡œë”© í•¨ìˆ˜"""
    import torch
    import warnings
    from pathlib import Path
    
    file_path = Path(file_path)
    
    try:
        # 1ë‹¨ê³„: ì•ˆì „ ëª¨ë“œ
        return torch.load(file_path, map_location=map_location, weights_only=True, **kwargs)
    except Exception:
        try:
            # 2ë‹¨ê³„: í˜¸í™˜ì„± ëª¨ë“œ
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                return torch.load(file_path, map_location=map_location, weights_only=False, **kwargs)
        except Exception:
            try:
                # 3ë‹¨ê³„: TorchScript
                return torch.jit.load(file_path, map_location=map_location)
            except Exception as e:
                raise RuntimeError(f"ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {e}")

# ì‚¬ìš© ì˜ˆì‹œ:
# checkpoint = safe_torch_load("model.pth")
'''
        
        wrapper_file = self.ai_models_root / "safe_loading_utils.py"
        wrapper_file.write_text(wrapper_code)
        logger.info(f"âœ… ì•ˆì „ ë¡œë”© ë˜í¼ ìƒì„±: {wrapper_file}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”¥ MyCloset AI - PyTorch 2.7+ í˜¸í™˜ì„± íŒ¨ì¹˜ v2.0")
    print("=" * 60)
    
    # íŒ¨ì¹˜ ì‹¤í–‰
    patcher = PyTorchCompatibilityPatcher()
    results = patcher.patch_all_models()
    
    # ì•ˆì „ ë¡œë”© ë˜í¼ ìƒì„±
    patcher.create_safe_loading_wrapper()
    
    print("\nğŸ“‹ ê¶Œì¥ ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. python enhanced_model_loading_validator.py")
    print("   2. python test_complete_ai_inference.py")
    print("   3. í•„ìš”ì‹œ ./fix_missing_models.sh ì‹¤í–‰")
    
    return results

if __name__ == "__main__":
    main()