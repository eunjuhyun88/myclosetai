#!/usr/bin/env python3
"""
ModelLoader ìºì‹œ ë¬´íš¨í™” ë° ì‹¤ì œ ëŒ€í˜• ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
ì‹¤í–‰: python model_loader_cache_fix.py
"""

import sys
import os
sys.path.append('.')

import time
import torch
from pathlib import Path

def test_model_loader_cache_fix():
    """ModelLoader ìºì‹œ ë¬¸ì œ í•´ê²° ë° í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ¯ ModelLoader ìºì‹œ ë¬´íš¨í™” ë° ì§ì ‘ ë¡œë”© í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    try:
        from app.ai_pipeline.utils.model_loader import get_global_model_loader
        
        # ModelLoader ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        loader = get_global_model_loader()
        print("âœ… ModelLoader ì¸ìŠ¤í„´ìŠ¤ íšë“")
        
        # ìºì‹œ ê°•ì œ ë¬´íš¨í™”
        if hasattr(loader, '_loaded_models'):
            loader._loaded_models.clear()
            print("ğŸ§¹ ê¸°ì¡´ ìºì‹œ ì œê±°")
        
        if hasattr(loader, 'loaded_models'):
            loader.loaded_models.clear()
            print("ğŸ§¹ loaded_models ìºì‹œ ì œê±°")
        
        # ì‹¤ì œ ëŒ€í˜• ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        large_models = {
            'sam_vit_h_direct': 'ai_models/sam_vit_h_4b8939.pth',  # 2.4GB
            'human_parsing_direct': 'ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/humanparsing/exp-schp-201908261155-lip.pth',  # 255MB
            'open_clip_direct': 'ai_models/ultra_models/clip_vit_g14/open_clip_pytorch_model.bin'  # 5.2GB
        }
        
        for model_name, model_path in large_models.items():
            file_path = Path(model_path)
            
            if not file_path.exists():
                print(f"âŒ íŒŒì¼ ì—†ìŒ: {model_path}")
                continue
                
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            print(f"\nğŸ”„ {model_name} í…ŒìŠ¤íŠ¸ ({file_size_mb:.1f}MB)")
            
            # 1. ì§ì ‘ PyTorch ë¡œë”© (ê¸°ì¤€ì„ )
            try:
                start = time.time()
                direct_model = torch.load(file_path, map_location='cpu', weights_only=False)
                direct_time = time.time() - start
                
                # ì‹¤ì œ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
                total_params = 0
                state_dict = direct_model
                if isinstance(direct_model, dict):
                    if 'state_dict' in direct_model:
                        state_dict = direct_model['state_dict']
                    elif 'model' in direct_model:
                        state_dict = direct_model['model']
                
                for key, tensor in state_dict.items():
                    if hasattr(tensor, 'numel'):
                        total_params += tensor.numel()
                
                print(f"  ğŸ“Š ì§ì ‘ ë¡œë”©: {direct_time:.2f}s, {total_params:,} íŒŒë¼ë¯¸í„°")
                
                # 2. ModelLoader available_modelsì— ê°•ì œ ì¶”ê°€
                loader.available_models[model_name] = {
                    'path': str(file_path),
                    'size_mb': file_size_mb,
                    'model_type': 'large_model',
                    'device': 'mps',
                    'verified': True,
                    'params_count': total_params
                }
                
                # 3. ModelLoaderë¡œ ë¡œë”© (ìºì‹œ ì—†ì´)
                start = time.time()
                loader_model = loader.load_model(model_name)
                loader_time = time.time() - start
                
                print(f"  ğŸ”§ ModelLoader: {loader_time:.2f}s, {type(loader_model)}")
                
                # 4. ê²°ê³¼ ë¹„êµ
                if loader_model is not None:
                    if isinstance(loader_model, dict) and len(loader_model) > 100:
                        print(f"  âœ… ì„±ê³µ! ì‹¤ì œ ëŒ€í˜• ëª¨ë¸ ë¡œë”©ë¨ ({len(loader_model)} í‚¤)")
                    else:
                        print(f"  âš ï¸ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²°ê³¼: {len(loader_model) if isinstance(loader_model, dict) else 'N/A'} í‚¤")
                else:
                    print("  âŒ ModelLoader ì‹¤íŒ¨")
                    
            except Exception as e:
                print(f"  âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # 5. ModelLoader ë‚´ë¶€ ìƒíƒœ í™•ì¸
        print(f"\nğŸ“Š ModelLoader ìƒíƒœ:")
        print(f"  - ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(loader.available_models)}")
        print(f"  - ë¡œë“œëœ ëª¨ë¸: {len(getattr(loader, '_loaded_models', {}))}")
        print(f"  - ìºì‹œ í¬ê¸°: {len(getattr(loader, 'loaded_models', {}))}")
        
        # 6. ì¶”ì²œ í•´ê²°ì±…
        print(f"\nğŸ¯ ë¬¸ì œ í•´ê²° ë°©ì•ˆ:")
        print(f"  1. ìºì‹œ ë¬´íš¨í™” ë©”ì„œë“œ ì¶”ê°€")
        print(f"  2. íŒŒì¼ í¬ê¸° ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ìˆ˜ì •")
        print(f"  3. ì§ì ‘ ê²½ë¡œ ì§€ì • ì˜µì…˜ ì¶”ê°€")
        print(f"  4. ê²€ì¦ ê°•í™” (íŒŒë¼ë¯¸í„° ìˆ˜ ê¸°ì¤€)")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_model_loader_cache_fix()