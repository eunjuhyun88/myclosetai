#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI í–¥ìƒëœ ëª¨ë¸ ë¶„ì„ê¸° v2.0 (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
================================================================================
âœ… ì‹¤ì œ ìŠ¤ìº” ê²°ê³¼(116ê°œ ëª¨ë¸, 123.68GB) ê¸°ë°˜ ìµœì í™”
âœ… Stepë³„ ì •í™•í•œ ë¶„ë¥˜ ë° ìš°ì„ ìˆœìœ„
âœ… ì¤‘ë³µ ëª¨ë¸ í†µí•© ë° ìµœì í™” ì œì•ˆ
âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ë° ê²½ê³ 
âœ… ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ ê³ ë ¤ì‚¬í•­
âœ… conda í™˜ê²½ + M3 Max ìµœì í™”
"""

import os
import sys
import json
import time
import hashlib
from pathlib import Path
from typing import Dict, List, Tuple, Any, Set
from dataclasses import dataclass, field
from collections import defaultdict, Counter
import warnings

@dataclass
class EnhancedModelInfo:
    """í–¥ìƒëœ ëª¨ë¸ ì •ë³´ (ì‹¤ì œ ë°ì´í„° êµ¬ì¡° ê¸°ë°˜)"""
    name: str
    path: str
    size_mb: float
    extension: str
    step_category: str
    is_large: bool  # 1GB+
    is_ultra_large: bool  # 5GB+
    is_pytorch_valid: bool = False
    duplicate_group: str = ""
    memory_footprint_estimate: float = 0.0
    production_priority: int = 3  # 1=critical, 2=important, 3=normal, 4=optional
    optimization_suggestion: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)

class EnhancedAIModelAnalyzer:
    """í–¥ìƒëœ AI ëª¨ë¸ ë¶„ì„ê¸° (ì‹¤ì œ ìŠ¤ìº” ê²°ê³¼ ìµœì í™”)"""
    
    def __init__(self, search_root: str = "."):
        self.search_root = Path(search_root)
        self.models: List[EnhancedModelInfo] = []
        self.duplicate_groups: Dict[str, List[str]] = defaultdict(list)
        self.memory_estimates: Dict[str, float] = {}
        
        # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ Step ë¶„ë¥˜ ê°œì„ 
        self.step_patterns = {
            "virtual_fitting": {
                "patterns": ["virtual", "ootd", "diffusion", "stable-diffusion", "sdxl", "v1-5"],
                "priority": 1,  # ìµœê³  ìš°ì„ ìˆœìœ„
                "expected_size_range": (1000, 8000),  # 1GB ~ 8GB
                "critical_files": ["v1-5-pruned", "diffusion_pytorch_model"]
            },
            "cloth_segmentation": {
                "patterns": ["cloth", "seg", "sam", "u2net", "RealVis"],
                "priority": 1,
                "expected_size_range": (100, 7000),
                "critical_files": ["sam_vit_h_4b8939", "RealVisXL"]
            },
            "human_parsing": {
                "patterns": ["human", "parsing", "schp", "graphonomy", "atr", "lip"],
                "priority": 2,
                "expected_size_range": (200, 6000),
                "critical_files": ["exp-schp-201908261155-atr", "graphonomy"]
            },
            "pose_estimation": {
                "patterns": ["pose", "openpose", "yolo", "body"],
                "priority": 2,
                "expected_size_range": (10, 1500),
                "critical_files": ["openpose", "yolov8n-pose"]
            },
            "geometric_matching": {
                "patterns": ["geometric", "tps", "gmm", "ViT-L"],
                "priority": 3,
                "expected_size_range": (50, 1500),
                "critical_files": ["tps_network", "ViT-L-14"]
            },
            "post_processing": {
                "patterns": ["post", "enhance", "GFPGAN", "esrgan", "ip-adapter"],
                "priority": 3,
                "expected_size_range": (100, 2000),
                "critical_files": ["GFPGAN", "ip-adapter"]
            },
            "quality_assessment": {
                "patterns": ["quality", "clip", "lpips", "ViT-B"],
                "priority": 4,
                "expected_size_range": (300, 2000),
                "critical_files": ["lpips_vgg", "ViT-B-32"]
            },
            "cloth_warping": {
                "patterns": ["warping", "warp", "tom", "photomaker"],
                "priority": 3,
                "expected_size_range": (500, 1000),
                "critical_files": ["photomaker-v1"]
            }
        }
        
        # PyTorch ì„¤ì •
        try:
            import torch
            self.torch_available = True
            print("âœ… PyTorch ì‚¬ìš© ê°€ëŠ¥")
        except ImportError:
            self.torch_available = False
            print("âš ï¸ PyTorch ì—†ìŒ - ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰")
    
    def scan_and_analyze(self) -> List[EnhancedModelInfo]:
        """ì™„ì „í•œ ìŠ¤ìº” ë° ë¶„ì„"""
        print(f"ğŸ” í–¥ìƒëœ AI ëª¨ë¸ ìŠ¤ìº” ì‹œì‘: {self.search_root.absolute()}")
        print("=" * 80)
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ìŠ¤ìº”
        self._scan_models()
        
        # 2ë‹¨ê³„: ì¤‘ë³µ ê°ì§€
        self._detect_duplicates()
        
        # 3ë‹¨ê³„: ë©”ëª¨ë¦¬ ì˜ˆì¸¡
        self._estimate_memory_usage()
        
        # 4ë‹¨ê³„: ìš°ì„ ìˆœìœ„ ë° ìµœì í™” ì œì•ˆ
        self._analyze_optimization_opportunities()
        
        print(f"âœ… í–¥ìƒëœ ìŠ¤ìº” ì™„ë£Œ: {len(self.models)}ê°œ ëª¨ë¸ ë¶„ì„")
        return self.models
    
    def _scan_models(self):
        """ê¸°ë³¸ ëª¨ë¸ ìŠ¤ìº”"""
        extensions = [".pth", ".pt", ".bin", ".safetensors", ".ckpt", ".pkl", ".pickle", ".h5", ".onnx"]
        
        for ext in extensions:
            pattern = f"**/*{ext}"
            for model_path in self.search_root.rglob(pattern):
                if self._should_include_file(model_path):
                    model_info = self._create_enhanced_model_info(model_path)
                    if model_info:
                        self.models.append(model_info)
        
        # í¬ê¸°ìˆœ ì •ë ¬
        self.models.sort(key=lambda x: x.size_mb, reverse=True)
    
    def _should_include_file(self, file_path: Path) -> bool:
        """íŒŒì¼ í¬í•¨ ì—¬ë¶€ ê²°ì • (ì‹¤ì œ ë°ì´í„° ê¸°ì¤€ ìµœì í™”)"""
        exclude_dirs = ["__pycache__", ".git", "node_modules", ".pytest_cache", "logs"]
        if any(exclude in str(file_path) for exclude in exclude_dirs):
            return False
        
        try:
            size_mb = file_path.stat().st_size / (1024 * 1024)
            return size_mb >= 10.0  # 10MB ì´ìƒë§Œ
        except:
            return False
    
    def _create_enhanced_model_info(self, file_path: Path) -> EnhancedModelInfo:
        """í–¥ìƒëœ ëª¨ë¸ ì •ë³´ ìƒì„±"""
        try:
            size_mb = file_path.stat().st_size / (1024 * 1024)
            
            # Step ì¹´í…Œê³ ë¦¬ ë° ìš°ì„ ìˆœìœ„ ê°ì§€
            step_category, priority = self._detect_step_and_priority(str(file_path))
            
            # PyTorch ê²€ì¦
            is_pytorch_valid = False
            metadata = {}
            if self.torch_available and file_path.suffix in [".pth", ".pt"]:
                is_pytorch_valid, metadata = self._validate_pytorch_model(file_path)
            
            return EnhancedModelInfo(
                name=file_path.stem,
                path=str(file_path.relative_to(self.search_root)),
                size_mb=round(size_mb, 2),
                extension=file_path.suffix,
                step_category=step_category,
                is_large=size_mb >= 1000,  # 1GB+
                is_ultra_large=size_mb >= 5000,  # 5GB+
                is_pytorch_valid=is_pytorch_valid,
                production_priority=priority,
                metadata=metadata
            )
            
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {file_path} - {e}")
            return None
    
    def _detect_step_and_priority(self, file_path: str) -> Tuple[str, int]:
        """Step ì¹´í…Œê³ ë¦¬ ë° ìš°ì„ ìˆœìœ„ ê°ì§€"""
        file_path_lower = file_path.lower()
        file_name_lower = Path(file_path).name.lower()
        
        for category, info in self.step_patterns.items():
            patterns = info["patterns"]
            
            # íŒŒì¼ëª… ìš°ì„  ë§¤ì¹­
            if any(pattern in file_name_lower for pattern in patterns):
                return category, info["priority"]
            
            # ê²½ë¡œ ë§¤ì¹­
            if any(pattern in file_path_lower for pattern in patterns):
                return category, info["priority"]
        
        return "unknown", 4
    
    def _validate_pytorch_model(self, file_path: Path) -> Tuple[bool, Dict]:
        """PyTorch ëª¨ë¸ ê²€ì¦ (ê²½ê³  ì–µì œ)"""
        try:
            import torch
            
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                checkpoint = torch.load(file_path, map_location='cpu', weights_only=True)
            
            metadata = {
                "type": str(type(checkpoint)),
                "keys": list(checkpoint.keys()) if isinstance(checkpoint, dict) else [],
                "tensor_count": 0,
                "total_params": 0
            }
            
            if isinstance(checkpoint, dict):
                for key, value in checkpoint.items():
                    if hasattr(value, 'numel'):
                        metadata["tensor_count"] += 1
                        metadata["total_params"] += value.numel()
            
            return True, metadata
            
        except Exception as e:
            return False, {"error": str(e)}
    
    def _detect_duplicates(self):
        """ì¤‘ë³µ ëª¨ë¸ ê°ì§€ (ì´ë¦„ ë° í¬ê¸° ê¸°ì¤€)"""
        print("ğŸ” ì¤‘ë³µ ëª¨ë¸ ê°ì§€ ì¤‘...")
        
        # ì´ë¦„ê³¼ í¬ê¸°ê°€ ë¹„ìŠ·í•œ ëª¨ë¸ë“¤ ê·¸ë£¹í™”
        name_size_groups = defaultdict(list)
        
        for model in self.models:
            # ì´ë¦„ ì •ê·œí™” (ë²„ì „, í™•ì¥ì ì œê±°)
            normalized_name = model.name.lower()
            normalized_name = normalized_name.replace('_v1', '').replace('_v2', '').replace('.fp16', '')
            
            # í¬ê¸°ë¥¼ 100MB ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
            size_bucket = round(model.size_mb / 100) * 100
            
            key = f"{normalized_name}_{size_bucket}"
            name_size_groups[key].append(model)
        
        # ì¤‘ë³µ ê·¸ë£¹ ì‹ë³„
        for group_key, group_models in name_size_groups.items():
            if len(group_models) > 1:
                self.duplicate_groups[group_key] = [m.path for m in group_models]
                for model in group_models:
                    model.duplicate_group = group_key
    
    def _estimate_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡"""
        print("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ì¤‘...")
        
        for model in self.models:
            # ê¸°ë³¸ ì¶”ì •: íŒŒì¼ í¬ê¸° * 1.5 (ë¡œë”© ì˜¤ë²„í—¤ë“œ)
            base_estimate = model.size_mb * 1.5
            
            # PyTorch ëª¨ë¸ì˜ ê²½ìš° ë” ì •í™•í•œ ì¶”ì •
            if model.is_pytorch_valid and model.metadata.get("total_params", 0) > 0:
                # íŒŒë¼ë¯¸í„° ìˆ˜ ê¸°ë°˜ ì¶”ì • (fp32 ê¸°ì¤€)
                params = model.metadata["total_params"]
                param_memory_mb = (params * 4) / (1024 * 1024)  # 4 bytes per param
                model.memory_footprint_estimate = max(base_estimate, param_memory_mb)
            else:
                model.memory_footprint_estimate = base_estimate
    
    def _analyze_optimization_opportunities(self):
        """ìµœì í™” ê¸°íšŒ ë¶„ì„"""
        print("ğŸ¯ ìµœì í™” ê¸°íšŒ ë¶„ì„ ì¤‘...")
        
        for model in self.models:
            suggestions = []
            
            # ì¤‘ë³µ íŒŒì¼ ì œì•ˆ
            if model.duplicate_group:
                suggestions.append("ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ ê°€ëŠ¥")
            
            # í¬ê¸° ìµœì í™” ì œì•ˆ
            if model.is_ultra_large:
                suggestions.append("ì–‘ìí™”(fp16) ê³ ë ¤")
            elif model.size_mb > 2000:
                suggestions.append("ì••ì¶• ë˜ëŠ” pruning ê³ ë ¤")
            
            # ì‚¬ìš© ë¹ˆë„ ê¸°ë°˜ ì œì•ˆ
            if model.step_category == "unknown":
                suggestions.append("ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” íŒŒì¼ì¼ ê°€ëŠ¥ì„±")
            elif model.production_priority >= 4:
                suggestions.append("ì„ íƒì  ë¡œë”© ê³ ë ¤")
            
            model.optimization_suggestion = "; ".join(suggestions) if suggestions else "ìµœì í™”ë¨"
    
    def print_enhanced_summary(self):
        """í–¥ìƒëœ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“Š í–¥ìƒëœ AI ëª¨ë¸ ë¶„ì„ ê²°ê³¼")
        print("=" * 80)
        
        # ì „ì²´ í†µê³„
        total_count = len(self.models)
        large_count = sum(1 for m in self.models if m.is_large)
        ultra_large_count = sum(1 for m in self.models if m.is_ultra_large)
        total_size_gb = sum(m.size_mb for m in self.models) / 1024
        total_memory_gb = sum(m.memory_footprint_estimate for m in self.models) / 1024
        valid_pytorch = sum(1 for m in self.models if m.is_pytorch_valid)
        duplicate_count = len(self.duplicate_groups)
        
        print(f"ğŸ“ˆ ì „ì²´ í†µê³„:")
        print(f"   ì´ ëª¨ë¸ íŒŒì¼: {total_count}ê°œ")
        print(f"   ëŒ€í˜• ëª¨ë¸ (1GB+): {large_count}ê°œ")
        print(f"   ì´ˆëŒ€í˜• ëª¨ë¸ (5GB+): {ultra_large_count}ê°œ")
        print(f"   ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: {total_size_gb:.2f} GB")
        print(f"   ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {total_memory_gb:.2f} GB")
        print(f"   PyTorch ìœ íš¨: {valid_pytorch}ê°œ")
        print(f"   ì¤‘ë³µ ê·¸ë£¹: {duplicate_count}ê°œ")
        
        # ìš°ì„ ìˆœìœ„ë³„ ë¶„ë¥˜
        priority_counts = Counter(m.production_priority for m in self.models)
        priority_names = {1: "Critical", 2: "Important", 3: "Normal", 4: "Optional"}
        
        print(f"\nğŸ¯ ìš°ì„ ìˆœìœ„ë³„ ë¶„ë¥˜:")
        for priority in sorted(priority_counts.keys()):
            count = priority_counts[priority]
            name = priority_names.get(priority, "Unknown")
            print(f"   {name:10s}: {count:3d}ê°œ")
    
    def print_optimization_report(self):
        """ìµœì í™” ë³´ê³ ì„œ ì¶œë ¥"""
        print(f"\nğŸ”§ ìµœì í™” ê¶Œì¥ì‚¬í•­")
        print("=" * 80)
        
        # 1. ì¤‘ë³µ íŒŒì¼ ì •ë¦¬
        if self.duplicate_groups:
            print("ğŸ“ ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ ê¶Œì¥:")
            for group_key, paths in list(self.duplicate_groups.items())[:5]:
                print(f"   ê·¸ë£¹ {group_key[:30]}:")
                for path in paths[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                    print(f"     - {path}")
                if len(paths) > 3:
                    print(f"     ... ì™¸ {len(paths)-3}ê°œ")
        
        # 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²½ê³ 
        high_memory_models = [m for m in self.models if m.memory_footprint_estimate > 5000]
        if high_memory_models:
            print(f"\nğŸ’¾ ê³ ë©”ëª¨ë¦¬ ëª¨ë¸ (5GB+ ì˜ˆìƒ):")
            for model in high_memory_models[:5]:
                print(f"   {model.name[:40]:<40} {model.memory_footprint_estimate:>8.1f}MB")
        
        # 3. ìµœì í™” ì œì•ˆ ìš”ì•½
        optimization_counts = Counter()
        for model in self.models:
            if model.optimization_suggestion != "ìµœì í™”ë¨":
                for suggestion in model.optimization_suggestion.split("; "):
                    optimization_counts[suggestion] += 1
        
        if optimization_counts:
            print(f"\nğŸ¯ ìµœì í™” ê¸°íšŒ ìš”ì•½:")
            for suggestion, count in optimization_counts.most_common(5):
                print(f"   {suggestion:<30} {count:3d}ê°œ ëª¨ë¸")
        
        # 4. ìŠ¤í† ë¦¬ì§€ ì ˆì•½ ì¶”ì •
        potential_savings = 0
        for group_models in self.duplicate_groups.values():
            if len(group_models) > 1:
                # ê°€ì¥ í° íŒŒì¼ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ ì‹œ ì ˆì•½ëŸ‰
                models_in_group = [m for m in self.models if m.path in group_models]
                if models_in_group:
                    total_group_size = sum(m.size_mb for m in models_in_group)
                    largest_size = max(m.size_mb for m in models_in_group)
                    potential_savings += total_group_size - largest_size
        
        if potential_savings > 0:
            print(f"\nğŸ’¾ ì¤‘ë³µ ì œê±° ì‹œ ì ˆì•½ ê°€ëŠ¥: {potential_savings/1024:.2f} GB")
    
    def print_step_production_readiness(self):
        """Stepë³„ í”„ë¡œë•ì…˜ ì¤€ë¹„ë„ í‰ê°€"""
        print(f"\nğŸš€ Stepë³„ í”„ë¡œë•ì…˜ ì¤€ë¹„ë„")
        print("=" * 80)
        
        step_readiness = {}
        for step_name, step_info in self.step_patterns.items():
            step_models = [m for m in self.models if m.step_category == step_name]
            
            if not step_models:
                readiness = "âŒ ëª¨ë¸ ì—†ìŒ"
            else:
                critical_files = step_info["critical_files"]
                has_critical = any(
                    any(cf in model.name for cf in critical_files) 
                    for model in step_models
                )
                
                valid_models = [m for m in step_models if m.is_pytorch_valid]
                large_models = [m for m in step_models if m.is_large]
                
                if has_critical and valid_models:
                    readiness = "âœ… ì¤€ë¹„ë¨"
                elif has_critical:
                    readiness = "âš ï¸ ê²€ì¦ í•„ìš”"
                else:
                    readiness = "ğŸ”§ ëª¨ë¸ ë¯¸ì™„ì„±"
            
            step_readiness[step_name] = readiness
            
            total_size = sum(m.size_mb for m in step_models) / 1024
            print(f"   {step_name.replace('_', ' ').title():<20} {readiness:<12} ({len(step_models):2d}ê°œ, {total_size:5.1f}GB)")
    
    def export_enhanced_json(self, output_file: str = "enhanced_ai_models_analysis.json"):
        """í–¥ìƒëœ JSON ë‚´ë³´ë‚´ê¸°"""