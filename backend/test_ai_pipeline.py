#!/usr/bin/env python3
# backend/test_ai_pipeline.py - ì™„ì „í•œ AI íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

import asyncio
import time
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
current_file = Path(__file__).absolute()
backend_root = current_file.parent
sys.path.insert(0, str(backend_root))

async def test_model_loading():
    """AI ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª AI ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        from app.ai_pipeline.utils.model_loader import get_global_model_loader
        
        model_loader = get_global_model_loader()
        
        # í•µì‹¬ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸
        test_models = [
            'human_parsing_graphonomy',
            'cloth_segmentation_u2net', 
            'virtual_fitting_ootdiffusion',
            'pose_estimation_openpose'
        ]
        
        results = {}
        
        for model_name in test_models:
            start_time = time.time()
            try:
                model = model_loader.get_model(model_name)
                load_time = time.time() - start_time
                
                if model:
                    results[model_name] = {
                        'status': 'âœ… ì„±ê³µ',
                        'load_time': f'{load_time:.3f}ì´ˆ',
                        'type': str(type(model))
                    }
                else:
                    results[model_name] = {
                        'status': 'âŒ ì‹¤íŒ¨',
                        'load_time': f'{load_time:.3f}ì´ˆ',
                        'type': 'None'
                    }
                    
            except Exception as e:
                load_time = time.time() - start_time
                results[model_name] = {
                    'status': f'âŒ ì˜¤ë¥˜: {str(e)[:50]}...',
                    'load_time': f'{load_time:.3f}ì´ˆ',
                    'type': 'Error'
                }
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ëª¨ë¸ ë¡œë”© ê²°ê³¼:")
        print("=" * 80)
        for model_name, result in results.items():
            print(f"{model_name:30} | {result['status']:20} | {result['load_time']:10} | {result['type']}")
        print("=" * 80)
        
        return results
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {}

async def test_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸...")
    
    try:
        import psutil
        import torch
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
        memory = psutil.virtual_memory()
        print(f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory.total / (1024**3):.1f}GB")
        print(f"ì‚¬ìš© ì¤‘: {memory.used / (1024**3):.1f}GB ({memory.percent:.1f}%)")
        print(f"ì‚¬ìš© ê°€ëŠ¥: {memory.available / (1024**3):.1f}GB")
        
        # GPU ë©”ëª¨ë¦¬ (M3 Max)
        if torch.backends.mps.is_available():
            print(f"ğŸ M3 Max MPS: ì‚¬ìš© ê°€ëŠ¥")
            
            # ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
            test_tensor = torch.randn(1000, 1000, device='mps')
            print(f"âœ… MPS í…ì„œ ìƒì„± ì„±ê³µ: {test_tensor.shape}")
            
            del test_tensor
            torch.mps.empty_cache()
            print("âœ… MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        else:
            print("âš ï¸ MPS ì‚¬ìš© ë¶ˆê°€")
            
    except Exception as e:
        print(f"âŒ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

async def test_image_processing():
    """ì´ë¯¸ì§€ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
    
    try:
        from PIL import Image
        import torch
        import torchvision.transforms as transforms
        import numpy as np
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        dummy_image = Image.new('RGB', (512, 512), (255, 128, 0))
        print("âœ… ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±: 512x512")
        
        # ì´ë¯¸ì§€ ë³€í™˜ íŒŒì´í”„ë¼ì¸
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # CPU ì²˜ë¦¬
        start_time = time.time()
        cpu_tensor = transform(dummy_image)
        cpu_time = time.time() - start_time
        print(f"CPU ì´ë¯¸ì§€ ë³€í™˜: {cpu_time:.4f}ì´ˆ")
        
        # GPU ì²˜ë¦¬ (M3 Max)
        if torch.backends.mps.is_available():
            start_time = time.time()
            gpu_tensor = transform(dummy_image).to('mps')
            gpu_time = time.time() - start_time
            print(f"ğŸ M3 Max GPU ë³€í™˜: {gpu_time:.4f}ì´ˆ")
            print(f"ì„±ëŠ¥ í–¥ìƒ: {cpu_time/gpu_time:.1f}ë°°")
        else:
            print("âš ï¸ GPU ì²˜ë¦¬ ë¶ˆê°€")
            
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ MyCloset AI ì™„ì „í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # 1. ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    model_results = await test_model_loading()
    
    # 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸  
    await test_memory_usage()
    
    # 3. ì´ë¯¸ì§€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    await test_image_processing()
    
    # 4. ì¢…í•© ê²°ê³¼
    print("\nğŸ¯ ì¢…í•© ê²°ê³¼:")
    print("=" * 60)
    
    success_count = sum(1 for result in model_results.values() if 'âœ… ì„±ê³µ' in result['status'])
    total_count = len(model_results)
    
    if success_count == total_count and total_count > 0:
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ì‹œìŠ¤í…œì´ ì™„ë²½í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤.")
    elif success_count > 0:
        print(f"âš ï¸ ë¶€ë¶„ ì„±ê³µ: {success_count}/{total_count} ëª¨ë¸ ë¡œë“œë¨")
    else:
        print("âŒ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    print("\nâœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

if __name__ == "__main__":
    asyncio.run(main())