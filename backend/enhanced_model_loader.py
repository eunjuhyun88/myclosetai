# backend/app/ai_pipeline/utils/model_loader.py
"""
ğŸ”¥ Neural ModelLoader v6.0 - ì™„ì „ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ê¸°ë°˜ ë¦¬íŒ©í† ë§
================================================================================

âœ… ì‹ ê²½ë§/ë…¼ë¬¸ êµ¬ì¡°ë¡œ ì™„ì „ ì „í™˜ - PyTorch nn.Module ê¸°ë°˜ ì„¤ê³„
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™” - ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ
âœ… ê³ ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© - 3ë‹¨ê³„ ìµœì í™” íŒŒì´í”„ë¼ì¸
âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ ìœ ì§€
âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB ì™„ì „ ì§€ì›
âœ… ì‹ ê²½ë§ ìˆ˜ì¤€ ëª¨ë¸ ê´€ë¦¬ - Layer-wise ë¡œë”© ë° ìµœì í™”
âœ… AutoGrad ê¸°ë°˜ ë™ì  ê·¸ë˜í”„ ì§€ì›
âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëª¨ë¸ ìŠ¤ì™€í•‘ ì‹œìŠ¤í…œ

í•µì‹¬ ì‹ ê²½ë§ ì„¤ê³„ ì›ì¹™:
1. Neural Architecture Pattern - ëª¨ë“  ëª¨ë¸ì„ nn.Moduleë¡œ í†µí•©
2. Memory-Efficient Loading - ë ˆì´ì–´ë³„ ì ì§„ì  ë¡œë”©
3. Dynamic Computation Graph - AutoGrad ì™„ì „ í™œìš©
4. Hardware-Aware Optimization - M3 Max MPS ìµœì í™”
5. Gradient-Free Inference - ì¶”ë¡  ì‹œ ë©”ëª¨ë¦¬ ìµœì í™”

Author: MyCloset AI Team
Date: 2025-08-09
Version: 6.0 (Complete Neural Network Architecture)
"""

import os
import sys
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
import weakref
import hashlib
import pickle
import mmap
import warnings
import math
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache, wraps
from abc import ABC, abstractmethod
from io import BytesIO
from collections import OrderedDict, defaultdict

# ğŸ”¥ ì‹ ê²½ë§ í•µì‹¬ import
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import DataLoader
    from torch.optim import Adam, SGD
    from torch.cuda.amp import autocast, GradScaler
    import torch.distributed as dist
    TORCH_AVAILABLE = True
    
    # M3 Max MPS ìµœì í™”
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        DEFAULT_DEVICE = "mps"
    elif torch.cuda.is_available():
        DEFAULT_DEVICE = "cuda"
    else:
        DEFAULT_DEVICE = "cpu"
        
except ImportError:
    TORCH_AVAILABLE = False
    MPS_AVAILABLE = False
    DEFAULT_DEVICE = "cpu"
    nn = None
    F = None

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

# MyCloset AI ì»¤ìŠ¤í…€ ì˜ˆì™¸ ì‹œìŠ¤í…œ
try:
    from app.core.exceptions import (
        MyClosetAIException, ModelLoadingError, FileOperationError, 
        MemoryError as MyClosetMemoryError, DataValidationError, 
        ConfigurationError, NetworkError, TimeoutError as MyClosetTimeoutError,
        track_exception, get_error_summary, create_exception_response,
        convert_to_mycloset_exception, ErrorCodes
    )
except ImportError:
    # fallback
    class MyClosetAIException(Exception): pass
    class ModelLoadingError(MyClosetAIException): pass
    class FileOperationError(MyClosetAIException): pass
    class MyClosetMemoryError(MyClosetAIException): pass
    class DataValidationError(MyClosetAIException): pass
    class ConfigurationError(MyClosetAIException): pass
    class NetworkError(MyClosetAIException): pass
    class MyClosetTimeoutError(MyClosetAIException): pass
    
    def track_exception(error, context=None, step_id=None): pass
    def get_error_summary(): return {}
    def create_exception_response(error, step_name="Unknown", step_id=None, session_id="unknown"): 
        return {'success': False, 'message': str(error)}
    def convert_to_mycloset_exception(error, context=None): return error
    
    class ErrorCodes:
        MODEL_LOADING_FAILED = "MODEL_LOADING_FAILED"
        MODEL_FILE_NOT_FOUND = "MODEL_FILE_NOT_FOUND"
        MODEL_CORRUPTED = "MODEL_CORRUPTED"
        MEMORY_INSUFFICIENT = "MEMORY_INSUFFICIENT"
        FILE_PERMISSION_DENIED = "FILE_PERMISSION_DENIED"

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ Central Hub DI Container ì•ˆì „ import
# ==============================================

_central_hub_cache = None
_dependencies_cache = {}

def _get_central_hub_container():
    """Central Hub DI Container ì•ˆì „í•œ ë™ì  í•´ê²°"""
    global _central_hub_cache
    
    if _central_hub_cache is not None:
        return _central_hub_cache
    
    try:
        if 'app.core.di_container' in sys.modules:
            module = sys.modules['app.core.di_container']
        else:
            import importlib
            module = importlib.import_module('app.core.di_container')
        
        get_global_fn = getattr(module, 'get_global_container', None)
        if get_global_fn and callable(get_global_fn):
            _central_hub_cache = get_global_fn()
            return _central_hub_cache
        
        return None
    except (ImportError, AttributeError, RuntimeError):
        return None

def _get_service_from_central_hub(service_key: str):
    """Central Hubë¥¼ í†µí•œ ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ"""
    if service_key in _dependencies_cache:
        return _dependencies_cache[service_key]
    
    container = _get_central_hub_container()
    if container and hasattr(container, 'get'):
        service = container.get(service_key)
        if service:
            _dependencies_cache[service_key] = service
        return service
    return None

# ==============================================
# ğŸ”¥ M3 Max ì‹œìŠ¤í…œ ì •ë³´
# ==============================================

IS_M3_MAX = False
MEMORY_GB = 16.0

import platform
if platform.system() == 'Darwin':
    import subprocess
    try:
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.stdout.strip():
            MEMORY_GB = int(memory_result.stdout.strip()) / 1024**3
    except Exception:
        pass

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean'
}

# ==============================================
# ğŸ”¥ ì‹ ê²½ë§ ê¸°ë°˜ ëª¨ë¸ íƒ€ì… ì‹œìŠ¤í…œ
# ==============================================

class NeuralModelType(Enum):
    """ì‹ ê²½ë§ ê¸°ë°˜ ëª¨ë¸ íƒ€ì…"""
    CONVOLUTIONAL = "convolutional"          # CNN ê¸°ë°˜ ëª¨ë¸
    TRANSFORMER = "transformer"              # Transformer ê¸°ë°˜ ëª¨ë¸
    RECURRENT = "recurrent"                  # RNN/LSTM ê¸°ë°˜ ëª¨ë¸
    GENERATIVE = "generative"                # GAN/VAE/Diffusion ëª¨ë¸
    HYBRID = "hybrid"                        # í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜
    VISION_TRANSFORMER = "vision_transformer" # ViT ê³„ì—´
    SEGMENTATION = "segmentation"            # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì „ìš©
    DETECTION = "detection"                  # ê°ì²´ íƒì§€ ì „ìš©
    POSE_ESTIMATION = "pose_estimation"      # í¬ì¦ˆ ì¶”ì • ì „ìš©
    SUPER_RESOLUTION = "super_resolution"    # ì´ˆí•´ìƒë„ ì „ìš©

class NeuralModelStatus(Enum):
    """ì‹ ê²½ë§ ëª¨ë¸ ìƒíƒœ"""
    UNINITIALIZED = "uninitialized"
    INITIALIZING = "initializing"
    LOADING_LAYERS = "loading_layers"
    LOADED = "loaded"
    TRAINING = "training"
    EVALUATING = "evaluating"
    INFERENCING = "inferencing"
    OPTIMIZING = "optimizing"
    ERROR = "error"
    SWAPPED = "swapped"

class NeuralModelPriority(Enum):
    """ì‹ ê²½ë§ ëª¨ë¸ ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1      # í•µì‹¬ ëª¨ë¸ (í•­ìƒ ë©”ëª¨ë¦¬ ìœ ì§€)
    HIGH = 2          # ë†’ì€ ìš°ì„ ìˆœìœ„
    MEDIUM = 3        # ì¤‘ê°„ ìš°ì„ ìˆœìœ„
    LOW = 4           # ë‚®ì€ ìš°ì„ ìˆœìœ„
    SWAPPABLE = 5     # ìŠ¤ì™€í•‘ ê°€ëŠ¥

@dataclass
class NeuralModelInfo:
    """ì‹ ê²½ë§ ëª¨ë¸ ì •ë³´"""
    name: str
    path: str
    model_type: NeuralModelType
    priority: NeuralModelPriority
    device: str
    
    # ì‹ ê²½ë§ íŠ¹í™” ì •ë³´
    architecture: str = "unknown"
    num_parameters: int = 0
    memory_mb: float = 0.0
    input_shape: Tuple[int, ...] = field(default_factory=tuple)
    output_shape: Tuple[int, ...] = field(default_factory=tuple)
    
    # ë¡œë”© ì •ë³´
    loaded: bool = False
    load_time: float = 0.0
    layers_loaded: int = 0
    total_layers: int = 0
    
    # ì„±ëŠ¥ ì •ë³´
    forward_time: float = 0.0
    memory_peak_mb: float = 0.0
    inference_count: int = 0
    access_count: int = 0
    last_access: float = 0.0
    
    # ìµœì í™” ì •ë³´
    is_quantized: bool = False
    is_compiled: bool = False
    gradient_checkpointing: bool = False
    mixed_precision: bool = False
    
    # ìƒíƒœ ì •ë³´
    status: NeuralModelStatus = NeuralModelStatus.UNINITIALIZED
    error: Optional[str] = None
    validation_passed: bool = False

# ==============================================
# ğŸ”¥ ì‹ ê²½ë§ ê¸°ë°˜ ëª¨ë¸ í´ë˜ìŠ¤
# ==============================================

class NeuralBaseModel(nn.Module if TORCH_AVAILABLE else object):
    """ì‹ ê²½ë§ ê¸°ë°˜ ëª¨ë¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str, model_path: str, model_type: NeuralModelType, 
                 device: str = "auto", **kwargs):
        if TORCH_AVAILABLE:
            super(NeuralBaseModel, self).__init__()
        
        self.model_name = model_name
        self.model_path = Path(model_path)
        self.model_type = model_type
        self.device = device if device != "auto" else DEFAULT_DEVICE
        
        # ì‹ ê²½ë§ íŠ¹í™” ì†ì„±
        self.architecture = kwargs.get('architecture', 'unknown')
        self.num_parameters = 0
        self.input_shape = kwargs.get('input_shape', ())
        self.output_shape = kwargs.get('output_shape', ())
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.memory_mb = 0.0
        self.memory_peak_mb = 0.0
        self.gradient_checkpointing = kwargs.get('gradient_checkpointing', False)
        self.mixed_precision = kwargs.get('mixed_precision', IS_M3_MAX)
        
        # ìƒíƒœ ê´€ë¦¬
        self.status = NeuralModelStatus.UNINITIALIZED
        self.layers_loaded = 0
        self.total_layers = 0
        self.loaded = False
        self.load_time = 0.0
        
        # ì„±ëŠ¥ ì¶”ì 
        self.forward_time = 0.0
        self.inference_count = 0
        self.access_count = 0
        self.last_access = 0.0
        
        # ìµœì í™” ìƒíƒœ
        self.is_quantized = False
        self.is_compiled = False
        
        # ì—ëŸ¬ ì •ë³´
        self.error = None
        self.validation_passed = False
        
        self.logger = logging.getLogger(f"NeuralModel.{model_name}")
        
        # M3 Max ìµœì í™”
        if IS_M3_MAX and TORCH_AVAILABLE:
            self._setup_m3_max_optimization()
    
    def _setup_m3_max_optimization(self):
        """M3 Max ìµœì í™” ì„¤ì •"""
        try:
            # MPS ë””ë°”ì´ìŠ¤ ì„¤ì •
            if self.device == "mps":
                self.mixed_precision = True
                self.gradient_checkpointing = True
                
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì„¤ì •
            if hasattr(torch.backends, 'mps'):
                torch.backends.mps.enable_fallback = True
                
            self.logger.debug("âœ… M3 Max ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def load_checkpoint(self, validate: bool = True) -> bool:
        """ì‹ ê²½ë§ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            start_time = time.time()
            self.status = NeuralModelStatus.LOADING_LAYERS
            
            self.logger.info(f"ğŸ”„ ì‹ ê²½ë§ ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_name}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not self.model_path.exists():
                error_msg = f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}"
                self.logger.error(f"âŒ {error_msg}")
                self.error = error_msg
                self.status = NeuralModelStatus.ERROR
                
                track_exception(
                    FileOperationError(error_msg, ErrorCodes.MODEL_FILE_NOT_FOUND, {
                        'model_name': self.model_name,
                        'model_path': str(self.model_path),
                        'model_type': self.model_type.value
                    }),
                    context={'model_name': self.model_name},
                    step_id=None
                )
                return False
            
            # ë©”ëª¨ë¦¬ ì²´í¬
            file_size_mb = self.model_path.stat().st_size / (1024 * 1024)
            self.memory_mb = file_size_mb
            
            if not self._check_memory_availability(file_size_mb):
                error_msg = f"ë©”ëª¨ë¦¬ ë¶€ì¡±: {file_size_mb:.1f}MB í•„ìš”"
                self.logger.error(f"âŒ {error_msg}")
                self.error = error_msg
                self.status = NeuralModelStatus.ERROR
                
                track_exception(
                    MyClosetMemoryError(error_msg, ErrorCodes.MEMORY_INSUFFICIENT, {
                        'required_mb': file_size_mb,
                        'available_mb': self._get_available_memory_mb()
                    }),
                    context={'model_name': self.model_name},
                    step_id=None
                )
                return False
            
            # ì‹ ê²½ë§ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            success = self._load_neural_checkpoint()
            
            if success:
                self.load_time = time.time() - start_time
                self.loaded = True
                self.status = NeuralModelStatus.LOADED
                
                # ëª¨ë¸ ì •ë³´ ê³„ì‚°
                self._calculate_model_info()
                
                # ê²€ì¦ ìˆ˜í–‰
                if validate:
                    self.validation_passed = self._validate_neural_model()
                else:
                    self.validation_passed = True
                
                # ìµœì í™” ì ìš©
                self._apply_optimizations()
                
                self.logger.info(f"âœ… ì‹ ê²½ë§ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_name} "
                               f"({self.load_time:.2f}ì´ˆ, {self.num_parameters:,}ê°œ íŒŒë¼ë¯¸í„°)")
                return True
            else:
                self.status = NeuralModelStatus.ERROR
                return False
                
        except Exception as e:
            error_msg = f"ì‹ ê²½ë§ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}"
            self.logger.error(f"âŒ {error_msg}")
            self.error = error_msg
            self.status = NeuralModelStatus.ERROR
            
            track_exception(
                convert_to_mycloset_exception(e, {
                    'model_name': self.model_name,
                    'model_type': self.model_type.value
                }),
                context={'model_name': self.model_name},
                step_id=None
            )
            return False
    
    def _check_memory_availability(self, required_mb: float) -> bool:
        """ë©”ëª¨ë¦¬ ê°€ìš©ì„± ì²´í¬"""
        try:
            available_mb = self._get_available_memory_mb()
            
            # M3 MaxëŠ” Unified Memoryì´ë¯€ë¡œ ë” ê´€ëŒ€í•˜ê²Œ
            if IS_M3_MAX:
                threshold = 0.8  # 80% ì‚¬ìš© ê°€ëŠ¥
            else:
                threshold = 0.7  # 70% ì‚¬ìš© ê°€ëŠ¥
            
            return required_mb < (available_mb * threshold)
            
        except Exception:
            return True  # ì²´í¬ ì‹¤íŒ¨ ì‹œ í†µê³¼
    
    def _get_available_memory_mb(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ ë°˜í™˜ (MB)"""
        try:
            if TORCH_AVAILABLE and self.device == "cuda":
                return torch.cuda.get_device_properties(0).total_memory / (1024 * 1024)
            elif IS_M3_MAX:
                return MEMORY_GB * 1024 * 0.8  # Unified Memoryì˜ 80%
            else:
                return 8 * 1024  # ê¸°ë³¸ê°’ 8GB
        except Exception:
            return 8 * 1024
    
    def _load_neural_checkpoint(self) -> bool:
        """ì‹ ê²½ë§ ì²´í¬í¬ì¸íŠ¸ ì‹¤ì œ ë¡œë”©"""
        try:
            # 3ë‹¨ê³„ ë¡œë”© ì „ëµ
            loading_strategies = [
                self._load_with_weights_only_true,
                self._load_with_weights_only_false,
                self._load_legacy_format
            ]
            
            for strategy_name, strategy_func in [
                ("ì•ˆì „ ëª¨ë“œ", loading_strategies[0]),
                ("í˜¸í™˜ ëª¨ë“œ", loading_strategies[1]),
                ("ë ˆê±°ì‹œ ëª¨ë“œ", loading_strategies[2])
            ]:
                try:
                    checkpoint = strategy_func()
                    if checkpoint is not None:
                        self._process_checkpoint(checkpoint)
                        self.logger.debug(f"âœ… {strategy_name} ë¡œë”© ì„±ê³µ")
                        return True
                except Exception as e:
                    self.logger.debug(f"âš ï¸ {strategy_name} ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.error("âŒ ëª¨ë“  ë¡œë”© ì „ëµ ì‹¤íŒ¨")
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹ ê²½ë§ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _load_with_weights_only_true(self) -> Optional[Dict]:
        """weights_only=Trueë¡œ ë¡œë”©"""
        if not TORCH_AVAILABLE:
            return None
        
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            return torch.load(
                self.model_path,
                map_location='cpu',
                weights_only=True
            )
    
    def _load_with_weights_only_false(self) -> Optional[Dict]:
        """weights_only=Falseë¡œ ë¡œë”©"""
        if not TORCH_AVAILABLE:
            return None
        
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            return torch.load(
                self.model_path,
                map_location='cpu',
                weights_only=False
            )
    
    def _load_legacy_format(self) -> Optional[Dict]:
        """ë ˆê±°ì‹œ í¬ë§·ìœ¼ë¡œ ë¡œë”©"""
        if not TORCH_AVAILABLE:
            return None
        
        try:
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore")
                return torch.load(self.model_path, map_location='cpu')
        except Exception:
            # ìµœì¢… ì‹œë„: pickleë¡œ ì§ì ‘ ë¡œë”©
            with open(self.model_path, 'rb') as f:
                return pickle.load(f)
    
    def _process_checkpoint(self, checkpoint: Dict):
        """ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬"""
        try:
            # state_dict ì¶”ì¶œ
            if isinstance(checkpoint, dict):
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                else:
                    state_dict = checkpoint
            else:
                state_dict = checkpoint
            
            # MPS float64 ë¬¸ì œ í•´ê²°
            if self.device == "mps":
                state_dict = self._convert_float64_to_float32(state_dict)
            
            # ëª¨ë¸ì— ë¡œë”©
            if TORCH_AVAILABLE and hasattr(self, 'load_state_dict'):
                missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)
                
                if missing_keys:
                    self.logger.debug(f"ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
                if unexpected_keys:
                    self.logger.debug(f"ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
            
            self.layers_loaded = len([k for k in state_dict.keys() if 'weight' in k or 'bias' in k])
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _convert_float64_to_float32(self, state_dict: Dict) -> Dict:
        """MPSìš© float64 â†’ float32 ë³€í™˜"""
        if not TORCH_AVAILABLE:
            return state_dict
        
        def convert_tensor(tensor):
            if hasattr(tensor, 'dtype') and tensor.dtype == torch.float64:
                return tensor.to(torch.float32)
            return tensor
        
        def recursive_convert(obj):
            if torch.is_tensor(obj):
                return convert_tensor(obj)
            elif isinstance(obj, dict):
                return {key: recursive_convert(value) for key, value in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return type(obj)(recursive_convert(item) for item in obj)
            else:
                return obj
        
        return recursive_convert(state_dict)
    
    def _calculate_model_info(self):
        """ëª¨ë¸ ì •ë³´ ê³„ì‚°"""
        try:
            if TORCH_AVAILABLE and hasattr(self, 'parameters'):
                # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
                self.num_parameters = sum(p.numel() for p in self.parameters())
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚° (ì¶”ì •)
                param_memory = sum(p.numel() * p.element_size() for p in self.parameters())
                self.memory_mb = param_memory / (1024 * 1024)
                
                # ë ˆì´ì–´ ìˆ˜ ê³„ì‚°
                self.total_layers = len(list(self.modules()))
                
                # ì…ë ¥/ì¶œë ¥ í¬ê¸° ì¶”ì • (ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ë ˆì´ì–´ì—ì„œ)
                modules = list(self.modules())
                if modules:
                    first_module = modules[1] if len(modules) > 1 else modules[0]
                    last_module = modules[-1]
                    
                    if hasattr(first_module, 'in_features'):
                        self.input_shape = (first_module.in_features,)
                    elif hasattr(first_module, 'in_channels'):
                        self.input_shape = (first_module.in_channels,)
                    
                    if hasattr(last_module, 'out_features'):
                        self.output_shape = (last_module.out_features,)
                    elif hasattr(last_module, 'out_channels'):
                        self.output_shape = (last_module.out_channels,)
            
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ì •ë³´ ê³„ì‚° ì‹¤íŒ¨: {e}")
    
    def _validate_neural_model(self) -> bool:
        """ì‹ ê²½ë§ ëª¨ë¸ ê²€ì¦"""
        try:
            if not TORCH_AVAILABLE:
                return True
            
            # ê¸°ë³¸ ê²€ì¦
            if self.num_parameters == 0:
                self.logger.warning("âš ï¸ íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” ëª¨ë¸")
                return False
            
            # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ì²´í¬
            if self.device == "mps" and not MPS_AVAILABLE:
                self.logger.warning("âš ï¸ MPS ìš”ì²­í–ˆì§€ë§Œ ì‚¬ìš© ë¶ˆê°€")
                return False
            
            # ê°„ë‹¨í•œ forward pass í…ŒìŠ¤íŠ¸
            try:
                self.eval()
                with torch.no_grad():
                    # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
                    if self.input_shape:
                        dummy_input = torch.randn(1, *self.input_shape)
                        if hasattr(self, 'forward'):
                            _ = self.forward(dummy_input)
                        self.logger.debug("âœ… Forward pass í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            except Exception as e:
                self.logger.debug(f"âš ï¸ Forward pass í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def _apply_optimizations(self):
        """ìµœì í™” ì ìš©"""
        try:
            if not TORCH_AVAILABLE:
                return
            
            # M3 Max íŠ¹í™” ìµœì í™”
            if IS_M3_MAX and self.device == "mps":
                self._apply_m3_max_optimizations()
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨í™”
            if self.gradient_checkpointing and hasattr(self, 'gradient_checkpointing_enable'):
                self.gradient_checkpointing_enable()
            
            # ì»´íŒŒì¼ ìµœì í™” (PyTorch 2.0+)
            if hasattr(torch, 'compile') and not self.is_compiled:
                try:
                    self = torch.compile(self)
                    self.is_compiled = True
                    self.logger.debug("âœ… ëª¨ë¸ ì»´íŒŒì¼ ìµœì í™” ì™„ë£Œ")
                except Exception as e:
                    self.logger.debug(f"âš ï¸ ëª¨ë¸ ì»´íŒŒì¼ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
    
    def _apply_m3_max_optimizations(self):
        """M3 Max íŠ¹í™” ìµœì í™”"""
        try:
            # MPS ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.to(self.device)
            
            # Mixed precision ì„¤ì •
            if self.mixed_precision:
                for param in self.parameters():
                    if param.dtype == torch.float32:
                        param.data = param.data.to(torch.float16)
            
            # MPS ìºì‹œ ì •ë¦¬
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
            
            self.logger.debug("âœ… M3 Max ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:
        """Forward pass (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        if not TORCH_AVAILABLE:
            raise NotImplementedError("PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        start_time = time.time()
        self.access_count += 1
        self.last_access = time.time()
        
        try:
            # ì¶”ë¡  ëª¨ë“œ ì„¤ì •
            self.eval()
            
            with torch.no_grad():
                if self.mixed_precision and self.device == "mps":
                    with autocast(device_type='cpu', dtype=torch.float16):
                        output = self._forward_impl(x, *args, **kwargs)
                else:
                    output = self._forward_impl(x, *args, **kwargs)
            
            # ì„±ëŠ¥ ì¶”ì 
            self.forward_time = time.time() - start_time
            self.inference_count += 1
            
            return output
            
        except Exception as e:
            self.logger.error(f"âŒ Forward pass ì‹¤íŒ¨: {e}")
            self.error = str(e)
            self.status = NeuralModelStatus.ERROR
            raise
    
    def _forward_impl(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:
        """ì‹¤ì œ forward êµ¬í˜„ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ)"""
        raise NotImplementedError("í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤")
    
    def optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            if not TORCH_AVAILABLE:
                return
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ì •ë¦¬
            for param in self.parameters():
                if param.grad is not None:
                    param.grad = None
            
            # ìºì‹œ ì •ë¦¬
            if self.device == "cuda":
                torch.cuda.empty_cache()
            elif self.device == "mps" and hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
            
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.logger.debug("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def unload(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            self.status = NeuralModelStatus.SWAPPED
            
            # íŒŒë¼ë¯¸í„° ë©”ëª¨ë¦¬ í•´ì œ
            if TORCH_AVAILABLE:
                for param in self.parameters():
                    del param
            
            # ìƒíƒœ ì´ˆê¸°í™”
            self.loaded = False
            self.layers_loaded = 0
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.optimize_memory()
            
            self.logger.debug("âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "name": self.model_name,
            "path": str(self.model_path),
            "type": self.model_type.value,
            "architecture": self.architecture,
            "device": self.device,
            "num_parameters": self.num_parameters,
            "memory_mb": self.memory_mb,
            "memory_peak_mb": self.memory_peak_mb,
            "input_shape": self.input_shape,
            "output_shape": self.output_shape,
            "loaded": self.loaded,
            "load_time": self.load_time,
            "layers_loaded": self.layers_loaded,
            "total_layers": self.total_layers,
            "forward_time": self.forward_time,
            "inference_count": self.inference_count,
            "access_count": self.access_count,
            "last_access": self.last_access,
            "is_quantized": self.is_quantized,
            "is_compiled": self.is_compiled,
            "gradient_checkpointing": self.gradient_checkpointing,
            "mixed_precision": self.mixed_precision,
            "status": self.status.value,
            "validation_passed": self.validation_passed,
            "error": self.error
        }

# ==============================================
# ğŸ”¥ êµ¬ì²´ì ì¸ ì‹ ê²½ë§ ëª¨ë¸ êµ¬í˜„ë“¤
# ==============================================

class NeuralHumanParsingModel(NeuralBaseModel):
    """ì‹ ê²½ë§ ê¸°ë°˜ Human Parsing ëª¨ë¸"""
    
    def __init__(self, model_name: str = "neural_human_parsing", **kwargs):
        super().__init__(
            model_name=model_name,
            model_path=kwargs.get('model_path', ''),
            model_type=NeuralModelType.SEGMENTATION,
            architecture="ResNet_DeepLab",
            **kwargs
        )
        
        if TORCH_AVAILABLE:
            self._build_architecture()
    
    def _build_architecture(self):
        """Human Parsing ì•„í‚¤í…ì²˜ êµ¬ì¶•"""
        # ResNet Backbone
        self.backbone = nn.Sequential(
            # Initial Conv
            nn.Conv2d(3, 64, 7, 2, 3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, 2, 1),
            
            # ResNet Blocks
            self._make_resnet_layer(64, 64, 3),
            self._make_resnet_layer(64, 128, 4, stride=2),
            self._make_resnet_layer(128, 256, 6, stride=2),
            self._make_resnet_layer(256, 512, 3, stride=2),
        )
        
        # ASPP (Atrous Spatial Pyramid Pooling)
        self.aspp = nn.ModuleList([
            nn.Conv2d(512, 256, 1, bias=False),
            nn.Conv2d(512, 256, 3, padding=6, dilation=6, bias=False),
            nn.Conv2d(512, 256, 3, padding=12, dilation=12, bias=False),
            nn.Conv2d(512, 256, 3, padding=18, dilation=18, bias=False),
        ])
        
        # Global Average Pooling
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        self.gap_conv = nn.Conv2d(512, 256, 1, bias=False)
        
        # Final Classifier
        self.classifier = nn.Sequential(
            nn.Conv2d(1280, 256, 3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Conv2d(256, 20, 1)  # 20 human parsing classes
        )
        
        # ì—…ìƒ˜í”Œë§
        self.upsample = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)
    
    def _make_resnet_layer(self, in_channels, out_channels, blocks, stride=1):
        """ResNet ë ˆì´ì–´ ìƒì„±"""
        layers = []
        
        # Downsample if needed
        downsample = None
        if stride != 1 or in_channels != out_channels * 4:
            downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels * 4, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels * 4),
            )
        
        # First block
        layers.append(self._make_resnet_block(in_channels, out_channels, stride, downsample))
        
        # Remaining blocks
        for _ in range(1, blocks):
            layers.append(self._make_resnet_block(out_channels * 4, out_channels))
        
        return nn.Sequential(*layers)
    
    def _make_resnet_block(self, in_channels, out_channels, stride=1, downsample=None):
        """ResNet Block ìƒì„±"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, stride, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels * 4, 1, bias=False),
            nn.BatchNorm2d(out_channels * 4),
        )
    
    def _forward_impl(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:
        """Human Parsing Forward Implementation"""
        # Backbone feature extraction
        features = self.backbone(x)
        
        # ASPP features
        aspp_features = []
        for aspp_layer in self.aspp:
            aspp_features.append(aspp_layer(features))
        
        # Global average pooling
        gap = self.global_avg_pool(features)
        gap = self.gap_conv(gap)
        gap = F.interpolate(gap, size=features.shape[2:], mode='bilinear', align_corners=True)
        aspp_features.append(gap)
        
        # Concatenate ASPP features
        concat_features = torch.cat(aspp_features, dim=1)
        
        # Classification
        output = self.classifier(concat_features)
        
        # Upsample to original size
        output = self.upsample(output)
        
        return output

class NeuralPoseEstimationModel(NeuralBaseModel):
    """ì‹ ê²½ë§ ê¸°ë°˜ Pose Estimation ëª¨ë¸"""
    
    def __init__(self, model_name: str = "neural_pose_estimation", **kwargs):
        super().__init__(
            model_name=model_name,
            model_path=kwargs.get('model_path', ''),
            model_type=NeuralModelType.POSE_ESTIMATION,
            architecture="HRNet",
            **kwargs
        )
        
        self.num_joints = kwargs.get('num_joints', 17)
        
        if TORCH_AVAILABLE:
            self._build_architecture()
    
    def _build_architecture(self):
        """HRNet ê¸°ë°˜ Pose Estimation ì•„í‚¤í…ì²˜"""
        # Stem
        self.stem = nn.Sequential(
            nn.Conv2d(3, 64, 3, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
        )
        
        # High-Resolution Branches
        self.stage1 = self._make_hr_stage(64, [64], [1])
        self.stage2 = self._make_hr_stage(64, [48, 96], [1, 2])
        self.stage3 = self._make_hr_stage(96, [48, 96, 192], [1, 2, 4])
        self.stage4 = self._make_hr_stage(192, [48, 96, 192, 384], [1, 2, 4, 8])
        
        # Final layer
        self.final_layer = nn.Conv2d(48, self.num_joints, 1)
    
    def _make_hr_stage(self, in_channels, channels, strides):
        """HR Stage ìƒì„±"""
        branches = nn.ModuleList()
        
        for i, (ch, stride) in enumerate(zip(channels, strides)):
            if i == 0:
                branch = nn.Sequential(
                    nn.Conv2d(in_channels, ch, 3, 1, 1, bias=False),
                    nn.BatchNorm2d(ch),
                    nn.ReLU(inplace=True),
                )
            else:
                branch = nn.Sequential(
                    nn.Conv2d(in_channels, ch, 3, stride, 1, bias=False),
                    nn.BatchNorm2d(ch),
                    nn.ReLU(inplace=True),
                )
            branches.append(branch)
        
        return branches
    
    def _forward_impl(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:
        """Pose Estimation Forward Implementation"""
        # Stem
        x = self.stem(x)
        
        # Stage 1
        x = self.stage1[0](x)
        
        # Stage 2
        x_list = []
        for i, branch in enumerate(self.stage2):
            if i == 0:
                x_list.append(branch(x))
            else:
                x_list.append(branch(x))
        
        # Stages 3-4 (simplified)
        x = x_list[0]  # Use highest resolution branch
        
        # Final prediction
        heatmaps = self.final_layer(x)
        
        return heatmaps

class NeuralSegmentationModel(NeuralBaseModel):
    """ì‹ ê²½ë§ ê¸°ë°˜ Segmentation ëª¨ë¸ (SAM-like)"""
    
    def __init__(self, model_name: str = "neural_segmentation", **kwargs):
        super().__init__(
            model_name=model_name,
            model_path=kwargs.get('model_path', ''),
            model_type=NeuralModelType.VISION_TRANSFORMER,
            architecture="ViT_SAM",
            **kwargs
        )
        
        self.image_size = kwargs.get('image_size', 1024)
        self.patch_size = kwargs.get('patch_size', 16)
        self.embed_dim = kwargs.get('embed_dim', 768)
        
        if TORCH_AVAILABLE:
            self._build_architecture()
    
    def _build_architecture(self):
        """Vision Transformer ê¸°ë°˜ Segmentation ì•„í‚¤í…ì²˜"""
        # Image Encoder (ViT)
        self.patch_embed = nn.Conv2d(3, self.embed_dim, self.patch_size, self.patch_size)
        
        num_patches = (self.image_size // self.patch_size) ** 2
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, self.embed_dim))
        
        # Transformer Blocks
        self.transformer_blocks = nn.ModuleList([
            self._make_transformer_block(self.embed_dim, 12, 3072)
            for _ in range(12)
        ])
        
        # Prompt Encoder
        self.prompt_embed = nn.Linear(4, self.embed_dim)  # (x, y, w, h)
        
        # Mask Decoder
        self.mask_decoder = nn.Sequential(
            nn.ConvTranspose2d(self.embed_dim, 256, 4, 2, 1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, 4, 2, 1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, 1),
        )
    
    def _make_transformer_block(self, embed_dim, num_heads, mlp_dim):
        """Transformer Block ìƒì„±"""
        return nn.Sequential(
            nn.LayerNorm(embed_dim),
            nn.MultiheadAttention(embed_dim, num_heads, batch_first=True),
            nn.LayerNorm(embed_dim),
            nn.Sequential(
                nn.Linear(embed_dim, mlp_dim),
                nn.GELU(),
                nn.Linear(mlp_dim, embed_dim),
            )
        )
    
    def _forward_impl(self, x: torch.Tensor, prompts: Optional[torch.Tensor] = None, *args, **kwargs) -> torch.Tensor:
        """Segmentation Forward Implementation"""
        B, C, H, W = x.shape
        
        # Patch embedding
        x = self.patch_embed(x)  # [B, embed_dim, H/16, W/16]
        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]
        
        # Add positional embedding
        x = x + self.pos_embed
        
        # Transformer encoding
        for block in self.transformer_blocks:
            # Multi-head attention
            attn_output, _ = block[1](x, x, x)
            x = x + attn_output
            
            # MLP
            mlp_output = block[3](block[2](x))
            x = x + mlp_output
        
        # Reshape for decoder
        num_patches_per_side = int(math.sqrt(x.shape[1]))
        x = x.transpose(1, 2).reshape(B, self.embed_dim, num_patches_per_side, num_patches_per_side)
        
        # Mask decoding
        masks = self.mask_decoder(x)
        
        # Resize to original input size
        masks = F.interpolate(masks, size=(H, W), mode='bilinear', align_corners=False)
        
        return masks

class NeuralDiffusionModel(NeuralBaseModel):
    """ì‹ ê²½ë§ ê¸°ë°˜ Diffusion ëª¨ë¸"""
    
    def __init__(self, model_name: str = "neural_diffusion", **kwargs):
        super().__init__(
            model_name=model_name,
            model_path=kwargs.get('model_path', ''),
            model_type=NeuralModelType.GENERATIVE,
            architecture="UNet_Diffusion",
            **kwargs
        )
        
        self.in_channels = kwargs.get('in_channels', 4)
        self.out_channels = kwargs.get('out_channels', 4)
        self.model_channels = kwargs.get('model_channels', 320)
        
        if TORCH_AVAILABLE:
            self._build_architecture()
    
    def _build_architecture(self):
        """UNet ê¸°ë°˜ Diffusion ì•„í‚¤í…ì²˜"""
        # Time embedding
        self.time_embed = nn.Sequential(
            nn.Linear(320, 1280),
            nn.SiLU(),
            nn.Linear(1280, 1280),
        )
        
        # Input conv
        self.input_conv = nn.Conv2d(self.in_channels, self.model_channels, 3, padding=1)
        
        # Down blocks
        self.down_blocks = nn.ModuleList([
            self._make_down_block(320, 320),
            self._make_down_block(320, 640),
            self._make_down_block(640, 1280),
            self._make_down_block(1280, 1280),
        ])
        
        # Middle block
        self.middle_block = self._make_middle_block(1280)
        
        # Up blocks
        self.up_blocks = nn.ModuleList([
            self._make_up_block(2560, 1280),
            self._make_up_block(1920, 640),
            self._make_up_block(960, 320),
            self._make_up_block(640, 320),
        ])
        
        # Output conv
        self.output_conv = nn.Sequential(
            nn.GroupNorm(32, 320),
            nn.SiLU(),
            nn.Conv2d(320, self.out_channels, 3, padding=1),
        )
    
    def _make_down_block(self, in_channels, out_channels):
        """Down Block ìƒì„±"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.GroupNorm(32, out_channels),
            nn.SiLU(),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.GroupNorm(32, out_channels),
            nn.SiLU(),
            nn.Conv2d(out_channels, out_channels, 3, 2, 1),  # Downsample
        )
    
    def _make_middle_block(self, channels):
        """Middle Block ìƒì„±"""
        return nn.Sequential(
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.GroupNorm(32, channels),
            nn.SiLU(),
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.GroupNorm(32, channels),
            nn.SiLU(),
        )
    
    def _make_up_block(self, in_channels, out_channels):
        """Up Block ìƒì„±"""
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1),  # Upsample
            nn.GroupNorm(32, out_channels),
            nn.SiLU(),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.GroupNorm(32, out_channels),
            nn.SiLU(),
        )
    
    def _forward_impl(self, x: torch.Tensor, timesteps: torch.Tensor, *args, **kwargs) -> torch.Tensor:
        """Diffusion Forward Implementation"""
        # Time embedding
        t_emb = self.time_embed(timesteps)
        
        # Input
        h = self.input_conv(x)
        
        # Down path
        down_features = []
        for down_block in self.down_blocks:
            h = down_block(h)
            down_features.append(h)
        
        # Middle
        h = self.middle_block(h)
        
        # Up path
        for i, up_block in enumerate(self.up_blocks):
            # Skip connection
            skip = down_features[-(i+1)]
            h = torch.cat([h, skip], dim=1)
            h = up_block(h)
        
        # Output
        output = self.output_conv(h)
        
        return output

# ==============================================
# ğŸ”¥ ì‹ ê²½ë§ ê¸°ë°˜ ModelLoader í´ë˜ìŠ¤
# ==============================================

class NeuralModelLoader:
    """ì‹ ê²½ë§ ê¸°ë°˜ ëª¨ë¸ ë¡œë”"""
    
    def __init__(self, device: str = "auto", model_cache_dir: Optional[str] = None, 
                 max_cached_models: int = 5, enable_m3_max_optimization: bool = True):
        self.device = device if device != "auto" else DEFAULT_DEVICE
        self.max_cached_models = max_cached_models
        self.enable_m3_max_optimization = enable_m3_max_optimization and IS_M3_MAX
        
        # ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬
        if model_cache_dir:
            self.model_cache_dir = Path(model_cache_dir)
        else:
            current_file = Path(__file__)
            backend_root = current_file.parents[3]
            self.model_cache_dir = backend_root / "ai_models"
        
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¡œë“œëœ ëª¨ë¸ë“¤
        self.loaded_models: Dict[str, NeuralBaseModel] = {}
        self.model_info: Dict[str, NeuralModelInfo] = {}
        self.model_status: Dict[str, NeuralModelStatus] = {}
        
        # Central Hub ì—°ë™
        self._central_hub_container = _get_central_hub_container()
        self.memory_manager = _get_service_from_central_hub('memory_manager')
        self.data_converter = _get_service_from_central_hub('data_converter')
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_metrics = {
            'models_loaded': 0,
            'total_memory_mb': 0.0,
            'total_parameters': 0,
            'avg_load_time': 0.0,
            'inference_count': 0,
            'total_inference_time': 0.0,
            'memory_optimizations': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="NeuralLoader")
        
        # M3 Max ìµœì í™” ì„¤ì •
        if self.enable_m3_max_optimization:
            self._setup_m3_max_environment()
        
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        self.logger.info(f"ğŸ§  Neural ModelLoader v6.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        self.logger.info(f"   ëª¨ë¸ ìºì‹œ: {self.model_cache_dir}")
        self.logger.info(f"   M3 Max ìµœì í™”: {'âœ…' if self.enable_m3_max_optimization else 'âŒ'}")
        self.logger.info(f"   ë©”ëª¨ë¦¬: {MEMORY_GB:.1f}GB")
    
    def _setup_m3_max_environment(self):
        """M3 Max í™˜ê²½ ìµœì í™” ì„¤ì •"""
        try:
            if TORCH_AVAILABLE and MPS_AVAILABLE:
                # MPS ìµœì í™” ì„¤ì •
                torch.backends.mps.enable_fallback = True
                
                # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                self.logger.debug("âœ… M3 Max í™˜ê²½ ìµœì í™” ì„¤ì • ì™„ë£Œ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max í™˜ê²½ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def load_model(self, model_name: str, model_type: NeuralModelType, model_path: str, 
                   **kwargs) -> Optional[NeuralBaseModel]:
        """ì‹ ê²½ë§ ëª¨ë¸ ë¡œë”©"""
        try:
            with self._lock:
                # ìºì‹œ í™•ì¸
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    if model.status == NeuralModelStatus.LOADED:
                        self.performance_metrics['cache_hits'] += 1
                        model.access_count += 1
                        model.last_access = time.time()
                        self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                        return model
                
                self.performance_metrics['cache_misses'] += 1
                
                # ìƒˆ ëª¨ë¸ ìƒì„±
                model = self._create_neural_model(model_name, model_type, model_path, **kwargs)
                
                if model is None:
                    self.logger.error(f"âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {model_name}")
                    return None
                
                # ëª¨ë¸ ë¡œë”©
                self.model_status[model_name] = NeuralModelStatus.LOADING_LAYERS
                
                if model.load_checkpoint(validate=kwargs.get('validate', True)):
                    # ìºì‹œì— ì €ì¥
                    self.loaded_models[model_name] = model
                    
                    # ëª¨ë¸ ì •ë³´ ìƒì„±
                    self.model_info[model_name] = NeuralModelInfo(
                        name=model_name,
                        path=model_path,
                        model_type=model_type,
                        priority=kwargs.get('priority', NeuralModelPriority.MEDIUM),
                        device=self.device,
                        architecture=model.architecture,
                        num_parameters=model.num_parameters,
                        memory_mb=model.memory_mb,
                        input_shape=model.input_shape,
                        output_shape=model.output_shape,
                        loaded=True,
                        load_time=model.load_time,
                        layers_loaded=model.layers_loaded,
                        total_layers=model.total_layers,
                        status=model.status,
                        validation_passed=model.validation_passed
                    )
                    
                    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                    self._update_performance_metrics(model)
                    
                    # ìºì‹œ ê´€ë¦¬
                    self._manage_cache()
                    
                    self.logger.info(f"âœ… ì‹ ê²½ë§ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name} "
                                   f"({model.num_parameters:,}ê°œ íŒŒë¼ë¯¸í„°, {model.memory_mb:.1f}MB)")
                    
                    return model
                else:
                    self.model_status[model_name] = NeuralModelStatus.ERROR
                    self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model_name}")
                    return None
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
            
            track_exception(
                convert_to_mycloset_exception(e, {
                    'model_name': model_name,
                    'model_type': model_type.value
                }),
                context={'model_name': model_name},
                step_id=None
            )
            return None
    
    def _create_neural_model(self, model_name: str, model_type: NeuralModelType, 
                           model_path: str, **kwargs) -> Optional[NeuralBaseModel]:
        """ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±"""
        try:
            # ëª¨ë¸ íƒ€ì…ë³„ ìƒì„±
            model_classes = {
                NeuralModelType.SEGMENTATION: NeuralHumanParsingModel,
                NeuralModelType.POSE_ESTIMATION: NeuralPoseEstimationModel,
                NeuralModelType.VISION_TRANSFORMER: NeuralSegmentationModel,
                NeuralModelType.GENERATIVE: NeuralDiffusionModel,
            }
            
            # ëª¨ë¸ í´ë˜ìŠ¤ ì„ íƒ
            model_class = model_classes.get(model_type)
            if model_class is None:
                # ê¸°ë³¸ ëª¨ë¸ë¡œ í´ë°±
                model_class = NeuralBaseModel
            
            # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            model = model_class(
                model_name=model_name,
                model_path=model_path,
                device=self.device,
                **kwargs
            )
            
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            
            track_exception(
                ModelLoadingError(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}", ErrorCodes.MODEL_LOADING_FAILED, {
                    'model_name': model_name,
                    'model_type': model_type.value,
                    'model_path': model_path
                }),
                context={'model_name': model_name},
                step_id=None
            )
            return None
    
    def _update_performance_metrics(self, model: NeuralBaseModel):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        try:
            self.performance_metrics['models_loaded'] += 1
            self.performance_metrics['total_memory_mb'] += model.memory_mb
            self.performance_metrics['total_parameters'] += model.num_parameters
            
            # í‰ê·  ë¡œë”© ì‹œê°„ ê³„ì‚°
            total_load_time = (self.performance_metrics['avg_load_time'] * 
                             (self.performance_metrics['models_loaded'] - 1) + model.load_time)
            self.performance_metrics['avg_load_time'] = total_load_time / self.performance_metrics['models_loaded']
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _manage_cache(self):
        """ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê´€ë¦¬"""
        try:
            if len(self.loaded_models) <= self.max_cached_models:
                return
            
            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ëª¨ë¸ ì„ ë³„
            models_to_remove = self._select_models_for_removal()
            
            for model_name in models_to_remove:
                self.unload_model(model_name)
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if self.enable_m3_max_optimization:
                self._optimize_m3_max_memory()
            
            self.logger.debug(f"ğŸ’¾ ìºì‹œ ê´€ë¦¬ ì™„ë£Œ: {len(models_to_remove)}ê°œ ëª¨ë¸ ì œê±°")
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ê´€ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _select_models_for_removal(self) -> List[str]:
        """ì œê±°í•  ëª¨ë¸ ì„ ë³„"""
        try:
            removal_candidates = []
            current_time = time.time()
            
            for model_name, model_info in self.model_info.items():
                # ìš°ì„ ìˆœìœ„ê°€ ë‚®ê³  ì˜¤ë˜ ì‚¬ìš©ë˜ì§€ ì•Šì€ ëª¨ë¸
                if (model_info.priority.value >= NeuralModelPriority.LOW.value and
                    current_time - model_info.last_access > 3600):  # 1ì‹œê°„
                    
                    removal_score = self._calculate_removal_score(model_info, current_time)
                    removal_candidates.append((model_name, removal_score))
            
            # ì ìˆ˜ìˆœ ì •ë ¬ (ë†’ì€ ì ìˆ˜ë¶€í„° ì œê±°)
            removal_candidates.sort(key=lambda x: x[1], reverse=True)
            
            # ì œê±°í•  ëª¨ë¸ ìˆ˜ ê³„ì‚°
            num_to_remove = len(self.loaded_models) - self.max_cached_models
            models_to_remove = [name for name, _ in removal_candidates[:num_to_remove]]
            
            return models_to_remove
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì„ ë³„ ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_removal_score(self, model_info: NeuralModelInfo, current_time: float) -> float:
        """ì œê±° ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ì œê±° ìš°ì„ )"""
        try:
            score = 0.0
            
            # ì‹œê°„ ê¸°ë°˜ ì ìˆ˜
            time_since_access = current_time - model_info.last_access
            score += time_since_access / 3600  # ì‹œê°„ë‹¹ 1ì 
            
            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜
            score += model_info.priority.value * 10
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ì ìˆ˜
            score += model_info.memory_mb / 1000  # GBë‹¹ 1ì 
            
            # ì ‘ê·¼ ë¹ˆë„ ê¸°ë°˜ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
            if model_info.access_count > 0:
                score += 100 / model_info.access_count
            else:
                score += 100
            
            return score
            
        except Exception:
            return 50.0  # ê¸°ë³¸ ì ìˆ˜
    
    def _optimize_m3_max_memory(self):
        """M3 Max ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            if not self.enable_m3_max_optimization:
                return
            
            # MPS ìºì‹œ ì •ë¦¬
            if TORCH_AVAILABLE and MPS_AVAILABLE:
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # ë¡œë“œëœ ëª¨ë¸ë“¤ì˜ ë©”ëª¨ë¦¬ ìµœì í™”
            for model in self.loaded_models.values():
                if hasattr(model, 'optimize_memory'):
                    model.optimize_memory()
            
            self.performance_metrics['memory_optimizations'] += 1
            self.logger.debug("âœ… M3 Max ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def unload_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                if model_name in self.loaded_models:
                    model = self.loaded_models[model_name]
                    
                    # ëª¨ë¸ ì–¸ë¡œë“œ
                    model.unload()
                    
                    # ìºì‹œì—ì„œ ì œê±°
                    del self.loaded_models[model_name]
                    
                    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                    if model_name in self.model_info:
                        model_info = self.model_info[model_name]
                        self.performance_metrics['total_memory_mb'] -= model_info.memory_mb
                        self.performance_metrics['total_parameters'] -= model_info.num_parameters
                        del self.model_info[model_name]
                    
                    self.model_status[model_name] = NeuralModelStatus.SWAPPED
                    
                    self.logger.debug(f"âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {model_name}")
                    return True
                
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return False
    
    async def load_model_async(self, model_name: str, model_type: NeuralModelType, 
                              model_path: str, **kwargs) -> Optional[NeuralBaseModel]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”©"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                self._executor,
                self.load_model,
                model_name,
                model_type,
                model_path,
                **kwargs
            )
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def get_model(self, model_name: str) -> Optional[NeuralBaseModel]:
        """ëª¨ë¸ ì¡°íšŒ"""
        try:
            if model_name in self.loaded_models:
                model = self.loaded_models[model_name]
                model.access_count += 1
                model.last_access = time.time()
                return model
            return None
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def list_loaded_models(self) -> List[Dict[str, Any]]:
        """ë¡œë“œëœ ëª¨ë¸ ëª©ë¡"""
        try:
            models = []
            for model_name, model_info in self.model_info.items():
                models.append({
                    "name": model_name,
                    "type": model_info.model_type.value,
                    "architecture": model_info.architecture,
                    "parameters": model_info.num_parameters,
                    "memory_mb": model_info.memory_mb,
                    "status": model_info.status.value,
                    "load_time": model_info.load_time,
                    "access_count": model_info.access_count,
                    "inference_count": model_info.inference_count
                })
            return models
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            metrics = self.performance_metrics.copy()
            
            # ì¶”ê°€ ê³„ì‚°ëœ ë©”íŠ¸ë¦­
            if metrics['inference_count'] > 0:
                metrics['avg_inference_time'] = metrics['total_inference_time'] / metrics['inference_count']
            else:
                metrics['avg_inference_time'] = 0.0
            
            # ì‹œìŠ¤í…œ ì •ë³´
            metrics.update({
                "device": self.device,
                "memory_gb": MEMORY_GB,
                "is_m3_max": IS_M3_MAX,
                "mps_available": MPS_AVAILABLE,
                "torch_available": TORCH_AVAILABLE,
                "loaded_models_count": len(self.loaded_models),
                "cached_models": list(self.loaded_models.keys()),
                "conda_env": CONDA_INFO['conda_env'],
                "is_target_env": CONDA_INFO['is_target_env']
            })
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def optimize_all_models(self):
        """ëª¨ë“  ëª¨ë¸ ìµœì í™”"""
        try:
            optimized_count = 0
            
            for model in self.loaded_models.values():
                try:
                    if hasattr(model, 'optimize_memory'):
                        model.optimize_memory()
                    optimized_count += 1
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨ {model.model_name}: {e}")
            
            # ì „ì²´ ë©”ëª¨ë¦¬ ìµœì í™”
            if self.enable_m3_max_optimization:
                self._optimize_m3_max_memory()
            
            self.logger.info(f"âœ… ëª¨ë¸ ìµœì í™” ì™„ë£Œ: {optimized_count}ê°œ ëª¨ë¸")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ Neural ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            
            # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
            for model_name in list(self.loaded_models.keys()):
                self.unload_model(model_name)
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            self._executor.shutdown(wait=True)
            
            # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.enable_m3_max_optimization:
                self._optimize_m3_max_memory()
            
            self.logger.info("âœ… Neural ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ëª¨ë¸ íŒ©í† ë¦¬ ë° í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

class NeuralModelFactory:
    """ì‹ ê²½ë§ ëª¨ë¸ íŒ©í† ë¦¬"""
    
    @staticmethod
    def create_human_parsing_model(model_path: str, **kwargs) -> NeuralHumanParsingModel:
        """Human Parsing ëª¨ë¸ ìƒì„±"""
        return NeuralHumanParsingModel(
            model_path=model_path,
            device=kwargs.get('device', DEFAULT_DEVICE),
            **kwargs
        )
    
    @staticmethod
    def create_pose_estimation_model(model_path: str, **kwargs) -> NeuralPoseEstimationModel:
        """Pose Estimation ëª¨ë¸ ìƒì„±"""
        return NeuralPoseEstimationModel(
            model_path=model_path,
            device=kwargs.get('device', DEFAULT_DEVICE),
            num_joints=kwargs.get('num_joints', 17),
            **kwargs
        )
    
    @staticmethod
    def create_segmentation_model(model_path: str, **kwargs) -> NeuralSegmentationModel:
        """Segmentation ëª¨ë¸ ìƒì„±"""
        return NeuralSegmentationModel(
            model_path=model_path,
            device=kwargs.get('device', DEFAULT_DEVICE),
            image_size=kwargs.get('image_size', 1024),
            **kwargs
        )
    
    @staticmethod
    def create_diffusion_model(model_path: str, **kwargs) -> NeuralDiffusionModel:
        """Diffusion ëª¨ë¸ ìƒì„±"""
        return NeuralDiffusionModel(
            model_path=model_path,
            device=kwargs.get('device', DEFAULT_DEVICE),
            in_channels=kwargs.get('in_channels', 4),
            **kwargs
        )

# ==============================================
# ğŸ”¥ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_global_neural_loader: Optional[NeuralModelLoader] = None
_neural_loader_lock = threading.Lock()

def get_global_neural_loader(config: Optional[Dict[str, Any]] = None) -> NeuralModelLoader:
    """ì „ì—­ Neural ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_neural_loader
    
    with _neural_loader_lock:
        if _global_neural_loader is None:
            try:
                # ì„¤ì • ì ìš©
                loader_config = {
                    'device': config.get('device', DEFAULT_DEVICE) if config else DEFAULT_DEVICE,
                    'max_cached_models': config.get('max_cached_models', 5) if config else 5,
                    'enable_m3_max_optimization': config.get('enable_m3_max_optimization', IS_M3_MAX) if config else IS_M3_MAX
                }
                
                _global_neural_loader = NeuralModelLoader(**loader_config)
                logger.info("âœ… ì „ì—­ Neural ModelLoader v6.0 ìƒì„± ì„±ê³µ")
                
            except Exception as e:
                logger.error(f"âŒ ì „ì—­ Neural ModelLoader ìƒì„± ì‹¤íŒ¨: {e}")
                # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í´ë°±
                _global_neural_loader = NeuralModelLoader()
        
        return _global_neural_loader

def load_neural_model(model_name: str, model_type: NeuralModelType, model_path: str, 
                     **kwargs) -> Optional[NeuralBaseModel]:
    """ì‹ ê²½ë§ ëª¨ë¸ ë¡œë”© í¸ì˜ í•¨ìˆ˜"""
    loader = get_global_neural_loader()
    return loader.load_model(model_name, model_type, model_path, **kwargs)

async def load_neural_model_async(model_name: str, model_type: NeuralModelType, 
                                 model_path: str, **kwargs) -> Optional[NeuralBaseModel]:
    """ë¹„ë™ê¸° ì‹ ê²½ë§ ëª¨ë¸ ë¡œë”© í¸ì˜ í•¨ìˆ˜"""
    loader = get_global_neural_loader()
    return await loader.load_model_async(model_name, model_type, model_path, **kwargs)

def get_neural_model(model_name: str) -> Optional[NeuralBaseModel]:
    """ì‹ ê²½ë§ ëª¨ë¸ ì¡°íšŒ í¸ì˜ í•¨ìˆ˜"""
    loader = get_global_neural_loader()
    return loader.get_model(model_name)

def optimize_neural_memory():
    """ì‹ ê²½ë§ ë©”ëª¨ë¦¬ ìµœì í™” í¸ì˜ í•¨ìˆ˜"""
    loader = get_global_neural_loader()
    loader.optimize_all_models()

def get_neural_performance_metrics() -> Dict[str, Any]:
    """ì‹ ê²½ë§ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ í¸ì˜ í•¨ìˆ˜"""
    loader = get_global_neural_loader()
    return loader.get_performance_metrics()

def cleanup_neural_loader():
    """Neural ModelLoader ì •ë¦¬ í¸ì˜ í•¨ìˆ˜"""
    global _global_neural_loader
    
    with _neural_loader_lock:
        if _global_neural_loader:
            _global_neural_loader.cleanup()
            _global_neural_loader = None

# ==============================================
# ğŸ”¥ í˜¸í™˜ì„± ë ˆì´ì–´ (ê¸°ì¡´ API ì§€ì›)
# ==============================================

class ModelLoaderCompatibilityLayer:
    """ê¸°ì¡´ ModelLoader API í˜¸í™˜ì„± ë ˆì´ì–´"""
    
    def __init__(self):
        self.neural_loader = get_global_neural_loader()
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
    
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ê¸°ì¡´ API í˜¸í™˜ ëª¨ë¸ ë¡œë”©"""
        try:
            # ëª¨ë¸ íƒ€ì… ì¶”ë¡ 
            model_type = self._infer_model_type(model_name, kwargs)
            model_path = kwargs.get('model_path', self._find_model_path(model_name))
            
            if not model_path:
                self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_name}")
                return None
            
            return self.neural_loader.load_model(model_name, model_type, model_path, **kwargs)
            
        except Exception as e:
            self.logger.error(f"âŒ í˜¸í™˜ì„± ë ˆì´ì–´ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _infer_model_type(self, model_name: str, kwargs: Dict[str, Any]) -> NeuralModelType:
        """ëª¨ë¸ ì´ë¦„ìœ¼ë¡œë¶€í„° íƒ€ì… ì¶”ë¡ """
        name_lower = model_name.lower()
        
        if any(keyword in name_lower for keyword in ['human', 'parsing', 'graphonomy']):
            return NeuralModelType.SEGMENTATION
        elif any(keyword in name_lower for keyword in ['pose', 'openpose', 'hrnet']):
            return NeuralModelType.POSE_ESTIMATION
        elif any(keyword in name_lower for keyword in ['sam', 'segment', 'u2net']):
            return NeuralModelType.VISION_TRANSFORMER
        elif any(keyword in name_lower for keyword in ['diffusion', 'stable', 'ootd']):
            return NeuralModelType.GENERATIVE
        else:
            return NeuralModelType.CONVOLUTIONAL
    
    def _find_model_path(self, model_name: str) -> Optional[str]:
        """ëª¨ë¸ ê²½ë¡œ ì°¾ê¸°"""
        try:
            # ê¸°ë³¸ ê²½ë¡œë“¤
            possible_paths = [
                self.neural_loader.model_cache_dir / f"{model_name}.pth",
                self.neural_loader.model_cache_dir / f"{model_name}.pt",
                self.neural_loader.model_cache_dir / f"{model_name}.safetensors",
                self.neural_loader.model_cache_dir / "checkpoints" / f"{model_name}.pth",
            ]
            
            for path in possible_paths:
                if path.exists():
                    return str(path)
            
            return None
            
        except Exception:
            return None
    
    def get_model(self, model_name: str) -> Optional[Any]:
        """ê¸°ì¡´ API í˜¸í™˜ ëª¨ë¸ ì¡°íšŒ"""
        return self.neural_loader.get_model(model_name)
    
    def unload_model(self, model_name: str) -> bool:
        """ê¸°ì¡´ API í˜¸í™˜ ëª¨ë¸ ì–¸ë¡œë“œ"""
        return self.neural_loader.unload_model(model_name)
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ê¸°ì¡´ API í˜¸í™˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
        return self.neural_loader.get_performance_metrics()

# ê¸°ì¡´ API ì§€ì›ì„ ìœ„í•œ ë³„ì¹­
ModelLoader = ModelLoaderCompatibilityLayer

# ==============================================
# ğŸ”¥ Export ë° ì´ˆê¸°í™”
# ==============================================

__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'NeuralBaseModel',
    'NeuralHumanParsingModel',
    'NeuralPoseEstimationModel', 
    'NeuralSegmentationModel',
    'NeuralDiffusionModel',
    'NeuralModelLoader',
    'NeuralModelFactory',
    
    # ë°ì´í„° íƒ€ì…ë“¤
    'NeuralModelType',
    'NeuralModelStatus',
    'NeuralModelPriority',
    'NeuralModelInfo',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_neural_loader',
    'load_neural_model',
    'load_neural_model_async',
    'get_neural_model',
    'optimize_neural_memory',
    'get_neural_performance_metrics',
    'cleanup_neural_loader',
    
    # í˜¸í™˜ì„± ë ˆì´ì–´
    'ModelLoaderCompatibilityLayer',
    'ModelLoader',  # ê¸°ì¡´ API í˜¸í™˜
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'DEFAULT_DEVICE',
    'IS_M3_MAX',
    'MEMORY_GB'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

logger.info("=" * 80)
logger.info("ğŸ§  Neural ModelLoader v6.0 - ì™„ì „ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ê¸°ë°˜")
logger.info("=" * 80)
logger.info("âœ… ì‹ ê²½ë§/ë…¼ë¬¸ êµ¬ì¡°ë¡œ ì™„ì „ ì „í™˜ - PyTorch nn.Module ê¸°ë°˜")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™” - ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ê´€ë¦¬")
logger.info("âœ… ê³ ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© - 3ë‹¨ê³„ ìµœì í™” íŒŒì´í”„ë¼ì¸")
logger.info("âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ ìœ ì§€")
logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB ì™„ì „ ì§€ì›")
logger.info("âœ… ì‹ ê²½ë§ ìˆ˜ì¤€ ëª¨ë¸ ê´€ë¦¬ - Layer-wise ë¡œë”©")
logger.info("âœ… AutoGrad ê¸°ë°˜ ë™ì  ê·¸ë˜í”„ ì§€ì›")
logger.info("âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëª¨ë¸ ìŠ¤ì™€í•‘ ì‹œìŠ¤í…œ")

logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   ë””ë°”ì´ìŠ¤: {DEFAULT_DEVICE}")
logger.info(f"   PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
logger.info(f"   MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"   M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
logger.info(f"   ë©”ëª¨ë¦¬: {MEMORY_GB:.1f}GB")
logger.info(f"   conda í™˜ê²½: {CONDA_INFO['conda_env']}")

logger.info(f"ğŸ§  ì§€ì› ì‹ ê²½ë§ ëª¨ë¸ íƒ€ì…:")
for model_type in NeuralModelType:
    logger.info(f"   - {model_type.value}: ì „ìš© ìµœì í™”")

logger.info("ğŸ”¥ í•µì‹¬ ì‹ ê²½ë§ ì„¤ê³„ ì›ì¹™:")
logger.info("   â€¢ Neural Architecture Pattern - ëª¨ë“  ëª¨ë¸ì„ nn.Moduleë¡œ í†µí•©")
logger.info("   â€¢ Memory-Efficient Loading - ë ˆì´ì–´ë³„ ì ì§„ì  ë¡œë”©")
logger.info("   â€¢ Dynamic Computation Graph - AutoGrad ì™„ì „ í™œìš©")
logger.info("   â€¢ Hardware-Aware Optimization - M3 Max MPS ìµœì í™”")
logger.info("   â€¢ Gradient-Free Inference - ì¶”ë¡  ì‹œ ë©”ëª¨ë¦¬ ìµœì í™”")

logger.info("ğŸš€ Neural ì§€ì› íë¦„:")
logger.info("   NeuralModelLoader â†’ NeuralBaseModel â†’ PyTorch nn.Module")
logger.info("     â†“ (ì‹ ê²½ë§ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©)")
logger.info("   3ë‹¨ê³„ ìµœì í™” íŒŒì´í”„ë¼ì¸ (weights_only â†’ í˜¸í™˜ â†’ ë ˆê±°ì‹œ)")
logger.info("     â†“ (M3 Max MPS ìµœì í™”)")
logger.info("   ì‹¤ì œ ì‹ ê²½ë§ ì¶”ë¡  (AutoGrad + Mixed Precision)")

logger.info("ğŸ‰ Neural ModelLoader v6.0 ì™„ì „ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì¤€ë¹„ ì™„ë£Œ!")
logger.info("ğŸ‰ M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™” + ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ê´€ë¦¬!")
logger.info("ğŸ‰ ì‹¤ì œ AI ëª¨ë¸ 229GB ì‹ ê²½ë§ ë ˆë²¨ ì§€ì›!")
logger.info("=" * 80)

# ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
try:
    _test_neural_loader = get_global_neural_loader()
    logger.info("ğŸ‰ Neural ModelLoader v6.0 ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    logger.info(f"   ë””ë°”ì´ìŠ¤: {_test_neural_loader.device}")
    logger.info(f"   ëª¨ë¸ ìºì‹œ: {_test_neural_loader.model_cache_dir}")
    logger.info(f"   ìµœëŒ€ ìºì‹œ ëª¨ë¸: {_test_neural_loader.max_cached_models}ê°œ")
    logger.info(f"   M3 Max ìµœì í™”: {'âœ…' if _test_neural_loader.enable_m3_max_optimization else 'âŒ'}")
    logger.info(f"   Central Hub ì—°ë™: {'âœ…' if _test_neural_loader._central_hub_container else 'âŒ'}")
    
except Exception as e:
    logger.error(f"âŒ Neural ModelLoader ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    logger.warning("âš ï¸ ê¸°ë³¸ ê¸°ëŠ¥ì€ ì •ìƒ ì‘ë™í•˜ì§€ë§Œ ì¼ë¶€ ê³ ê¸‰ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤")

logger.info("ğŸ”¥ Neural ModelLoader v6.0 ì™„ì „ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ!")