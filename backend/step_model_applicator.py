#!/usr/bin/env python3
"""
ğŸ”¥ Stepë³„ ëª¨ë¸ ì ìš© ë„êµ¬
========================

ê° Stepì— ì‹¤ì œë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ë“¤ì„ ì°¾ê³  ì ìš©í•˜ëŠ” ë„êµ¬

Author: MyCloset AI Team
Date: 2025-08-08
Version: 1.0
"""

import os
import sys
import json
import shutil
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

# PyTorch ê´€ë ¨
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# SafeTensors ê´€ë ¨
try:
    import safetensors
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False

logger = logging.getLogger(__name__)

class StepModelApplicator:
    """Stepë³„ ëª¨ë¸ ì ìš© ë„êµ¬"""
    
    def __init__(self):
        self.step_models = {}
        self.applied_models = {}
        
        # Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ì •ì˜
        self.step_requirements = {
            'step_01': {
                'name': 'Human Parsing',
                'required_models': ['graphonomy', 'u2net', 'deeplabv3plus'],
                'model_paths': {
                    'graphonomy': 'backend/ai_models/step_01_human_parsing/graphonomy.pth',
                    'u2net': 'backend/ai_models/step_01_human_parsing/u2net.pth',
                    'deeplabv3plus': 'backend/ai_models/step_01_human_parsing/deeplabv3plus.pth'
                },
                'fallback_models': [
                    'backend/ai_models/Graphonomy/inference.pth',
                    'backend/ai_models/Graphonomy/model.safetensors',
                    'backend/ai_models/Self-Correction-Human-Parsing/exp-schp-201908261155-atr.pth'
                ]
            },
            'step_02': {
                'name': 'Pose Estimation',
                'required_models': ['hrnet', 'openpose', 'yolo'],
                'model_paths': {
                    'hrnet': 'backend/ai_models/step_02_pose_estimation/hrnet_w48_coco_384x288.pth',
                    'openpose': 'backend/ai_models/step_02_pose_estimation/openpose.pth',
                    'yolo': 'backend/ai_models/step_02_pose_estimation/yolov8n-pose.pt'
                },
                'fallback_models': [
                    'backend/ai_models/step_02_pose_estimation/hrnet_w48_coco_256x192.pth',
                    'backend/ai_models/step_02_pose_estimation/body_pose_model.pth',
                    'backend/ai_models/openpose.pth'
                ]
            },
            'step_03': {
                'name': 'Cloth Segmentation',
                'required_models': ['sam', 'u2net', 'deeplabv3'],
                'model_paths': {
                    'sam': 'backend/ai_models/step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
                    'u2net': 'backend/ai_models/step_03_cloth_segmentation/u2net.pth',
                    'deeplabv3': 'backend/ai_models/step_03_cloth_segmentation/deeplabv3_resnet101_ultra.pth'
                },
                'fallback_models': [
                    'backend/ai_models/step_03_cloth_segmentation/sam_vit_l_0b3195.pth',
                    'backend/ai_models/step_03_cloth_segmentation/mobile_sam.pt',
                    'backend/ai_models/step_03_cloth_segmentation/deeplabv3_resnet101_coco.pth'
                ]
            },
            'step_04': {
                'name': 'Geometric Matching',
                'required_models': ['gmm', 'tps', 'raft'],
                'model_paths': {
                    'gmm': 'backend/ai_models/step_04_geometric_matching/gmm_final.pth',
                    'tps': 'backend/ai_models/step_04_geometric_matching/tps_network.pth',
                    'raft': 'backend/ai_models/step_04_geometric_matching/raft-things.pth'
                },
                'fallback_models': [
                    'backend/ai_models/step_04_geometric_matching/raft-small.pth',
                    'backend/ai_models/step_04_geometric_matching/raft-sintel.pth',
                    'backend/ai_models/step_04_geometric_matching/raft-kitti.pth'
                ]
            },
            'step_05': {
                'name': 'Cloth Warping',
                'required_models': ['tom', 'viton_hd', 'dpt'],
                'model_paths': {
                    'tom': 'backend/ai_models/step_05_cloth_warping/tom_final.pth',
                    'viton_hd': 'backend/ai_models/step_05_cloth_warping/viton_hd_warping.pth',
                    'dpt': 'backend/ai_models/step_05_cloth_warping/dpt_hybrid_midas.pth'
                },
                'fallback_models': [
                    'backend/ai_models/step_05_cloth_warping/tps_transformation.pth',
                    'backend/ai_models/step_05_cloth_warping/vgg19_warping.pth',
                    'backend/ai_models/dpt_hybrid-midas-501f0c75.pt'
                ]
            },
            'step_06': {
                'name': 'Virtual Fitting',
                'required_models': ['stable_diffusion', 'ootd', 'viton_hd'],
                'model_paths': {
                    'stable_diffusion': 'backend/ai_models/step_06_virtual_fitting/stable_diffusion_4.8gb.pth',
                    'ootd': 'backend/ai_models/step_06_virtual_fitting/ootd_3.2gb.pth',
                    'viton_hd': 'backend/ai_models/step_06_virtual_fitting/viton_hd_2.1gb.pth'
                },
                'fallback_models': [
                    'backend/ai_models/step_06_virtual_fitting/hrviton_final.pth',
                    'backend/ai_models/step_06_virtual_fitting/ootd_checkpoint.pth',
                    'backend/ai_models/step_06_virtual_fitting/validated_checkpoints/ootd_checkpoint.pth'
                ]
            },
            'step_07': {
                'name': 'Post Processing',
                'required_models': ['real_esrgan', 'swinir', 'gfpgan'],
                'model_paths': {
                    'real_esrgan': 'backend/ai_models/step_07_post_processing/RealESRGAN_x4plus.pth',
                    'swinir': 'backend/ai_models/step_07_post_processing/swinir_real_sr_x4_large.pth',
                    'gfpgan': 'backend/ai_models/step_07_post_processing/GFPGAN.pth'
                },
                'fallback_models': [
                    'backend/ai_models/step_07_post_processing/densenet161_enhance.pth',
                    'backend/ai_models/step_07_post_processing/RealESRGAN_x2plus.pth',
                    'backend/ai_models/step_07_post_processing/ESRGAN_x8.pth'
                ]
            },
            'step_08': {
                'name': 'Quality Assessment',
                'required_models': ['clip', 'lpips', 'alex'],
                'model_paths': {
                    'clip': 'backend/ai_models/step_08_quality_assessment/clip_vit_b32.pth',
                    'lpips': 'backend/ai_models/step_08_quality_assessment/lpips_alex.pth',
                    'alex': 'backend/ai_models/step_08_quality_assessment/alex.pth'
                },
                'fallback_models': [
                    'backend/ai_models/step_08_quality_assessment/ViT-L-14.pt',
                    'backend/ai_models/step_08_quality_assessment/ViT-B-32.pt',
                    'backend/ai_models/step_08_quality_assessment/open_clip_pytorch_model.bin'
                ]
            }
        }
    
    def find_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ì°¾ê¸°"""
        print("ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ê²€ìƒ‰ ì¤‘...")
        
        for step_key, step_info in self.step_requirements.items():
            print(f"\nğŸ¯ {step_info['name']} ({step_key})")
            
            available_models = {}
            
            # í•„ìˆ˜ ëª¨ë¸ í™•ì¸
            for model_name, model_path in step_info['model_paths'].items():
                if Path(model_path).exists():
                    available_models[model_name] = {
                        'path': model_path,
                        'type': 'required',
                        'valid': self._validate_model(model_path)
                    }
                    print(f"   âœ… {model_name}: {Path(model_path).name}")
                else:
                    print(f"   âŒ {model_name}: íŒŒì¼ ì—†ìŒ")
            
            # ëŒ€ì²´ ëª¨ë¸ í™•ì¸
            for fallback_path in step_info['fallback_models']:
                if Path(fallback_path).exists():
                    model_name = Path(fallback_path).stem
                    available_models[model_name] = {
                        'path': fallback_path,
                        'type': 'fallback',
                        'valid': self._validate_model(fallback_path)
                    }
                    print(f"   ğŸ”„ {model_name}: {Path(fallback_path).name} (ëŒ€ì²´)")
            
            self.step_models[step_key] = available_models
    
    def _validate_model(self, model_path: str) -> bool:
        """ëª¨ë¸ ìœ íš¨ì„± ê²€ì¦"""
        try:
            if model_path.endswith('.safetensors') and SAFETENSORS_AVAILABLE:
                with safetensors.safe_open(model_path, framework="pt", device="cpu") as f:
                    keys = list(f.keys())
                return len(keys) > 0
            else:
                model_data = torch.load(model_path, map_location='cpu', weights_only=True)
                if isinstance(model_data, dict):
                    if 'state_dict' in model_data:
                        return len(model_data['state_dict']) > 0
                    else:
                        return len(model_data) > 0
                else:
                    return True
        except Exception as e:
            return False
    
    def apply_models_to_steps(self):
        """ê° Stepì— ëª¨ë¸ ì ìš©"""
        print("\nğŸ”§ ê° Stepì— ëª¨ë¸ ì ìš© ì¤‘...")
        
        for step_key, step_info in self.step_requirements.items():
            print(f"\nğŸ¯ {step_info['name']} ({step_key}) ì ìš©")
            
            available_models = self.step_models.get(step_key, {})
            
            if not available_models:
                print(f"   âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                continue
            
            # ìœ íš¨í•œ ëª¨ë¸ë“¤ë§Œ í•„í„°ë§
            valid_models = {name: info for name, info in available_models.items() if info['valid']}
            
            if not valid_models:
                print(f"   âŒ ìœ íš¨í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                continue
            
            # ëª¨ë¸ ì ìš©
            applied_models = []
            for model_name, model_info in valid_models.items():
                if self._apply_model_to_step(step_key, model_name, model_info):
                    applied_models.append(model_name)
            
            self.applied_models[step_key] = applied_models
            print(f"   âœ… ì ìš© ì™„ë£Œ: {', '.join(applied_models)}")
    
    def _apply_model_to_step(self, step_key: str, model_name: str, model_info: Dict[str, Any]) -> bool:
        """ê°œë³„ ëª¨ë¸ì„ Stepì— ì ìš©"""
        try:
            model_path = model_info['path']
            
            # Stepë³„ ë””ë ‰í† ë¦¬ ìƒì„±
            step_dir = Path(f"backend/ai_models/{step_key}")
            step_dir.mkdir(parents=True, exist_ok=True)
            
            # ëª¨ë¸ íŒŒì¼ ë³µì‚¬
            target_path = step_dir / f"{model_name}.pth"
            
            if model_path.endswith('.safetensors'):
                # SafeTensorsë¥¼ PyTorchë¡œ ë³€í™˜
                if SAFETENSORS_AVAILABLE:
                    with safetensors.safe_open(model_path, framework="pt", device="cpu") as f:
                        keys = list(f.keys())
                        state_dict = {key: f.get_tensor(key) for key in keys}
                    
                    torch.save({'state_dict': state_dict}, target_path)
                    print(f"   ğŸ”„ {model_name}: SafeTensors â†’ PyTorch ë³€í™˜ ì™„ë£Œ")
                    return True
            else:
                # PyTorch ëª¨ë¸ ë³µì‚¬
                shutil.copy2(model_path, target_path)
                print(f"   ğŸ“‹ {model_name}: ëª¨ë¸ ë³µì‚¬ ì™„ë£Œ")
                return True
            
        except Exception as e:
            print(f"   âŒ {model_name}: ì ìš© ì‹¤íŒ¨ - {e}")
            return False
    
    def create_step_configs(self):
        """Stepë³„ ì„¤ì • íŒŒì¼ ìƒì„±"""
        print("\nğŸ“‹ Stepë³„ ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")
        
        for step_key, step_info in self.step_requirements.items():
            print(f"\nğŸ¯ {step_info['name']} ({step_key}) ì„¤ì • ìƒì„±")
            
            applied_models = self.applied_models.get(step_key, [])
            
            if not applied_models:
                print(f"   âš ï¸ ì ìš©ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                continue
            
            # ì„¤ì • íŒŒì¼ ìƒì„±
            config = {
                'step_name': step_info['name'],
                'step_key': step_key,
                'applied_models': applied_models,
                'model_paths': {},
                'created_at': datetime.now().isoformat()
            }
            
            # ëª¨ë¸ ê²½ë¡œ ì¶”ê°€
            for model_name in applied_models:
                model_path = f"backend/ai_models/{step_key}/{model_name}.pth"
                config['model_paths'][model_name] = model_path
            
            # ì„¤ì • íŒŒì¼ ì €ì¥
            config_path = f"backend/ai_models/{step_key}/step_config.json"
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            print(f"   âœ… ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
    
    def update_pipeline_config(self):
        """íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸"""
        print("\nğŸ”§ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì¤‘...")
        
        pipeline_config = {
            'pipeline_name': 'MyCloset AI Virtual Try-On Pipeline',
            'version': '1.0',
            'steps': {},
            'created_at': datetime.now().isoformat()
        }
        
        for step_key, step_info in self.step_requirements.items():
            applied_models = self.applied_models.get(step_key, [])
            
            pipeline_config['steps'][step_key] = {
                'name': step_info['name'],
                'applied_models': applied_models,
                'status': 'ready' if applied_models else 'missing_models'
            }
        
        # íŒŒì´í”„ë¼ì¸ ì„¤ì • ì €ì¥
        config_path = "backend/ai_models/pipeline_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(pipeline_config, f, indent=2, ensure_ascii=False)
        
        print(f"   âœ… íŒŒì´í”„ë¼ì¸ ì„¤ì • ì €ì¥: {config_path}")
    
    def generate_application_report(self):
        """ì ìš© ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("ğŸ”¥ Stepë³„ ëª¨ë¸ ì ìš© ë¦¬í¬íŠ¸")
        report.append("=" * 80)
        report.append(f"ğŸ“… ì ìš© ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        total_applied = 0
        total_required = 0
        
        for step_key, step_info in self.step_requirements.items():
            applied_models = self.applied_models.get(step_key, [])
            required_models = list(step_info['model_paths'].keys())
            
            total_applied += len(applied_models)
            total_required += len(required_models)
            
            status = "âœ… ì™„ë£Œ" if applied_models else "âŒ ë¯¸ì™„ë£Œ"
            report.append(f"ğŸ¯ {step_info['name']} ({step_key}): {status}")
            
            if applied_models:
                report.append(f"   âœ… ì ìš©ëœ ëª¨ë¸: {', '.join(applied_models)}")
            else:
                report.append(f"   âŒ í•„ìš”í•œ ëª¨ë¸: {', '.join(required_models)}")
            
            report.append("")
        
        # ì „ì²´ í†µê³„
        report.append("ğŸ“Š ì „ì²´ í†µê³„")
        report.append("-" * 50)
        report.append(f"   ğŸ” ì´ í•„ìš” ëª¨ë¸: {total_required}ê°œ")
        report.append(f"   âœ… ì ìš© ì™„ë£Œ: {total_applied}ê°œ")
        report.append(f"   ğŸ“ˆ ì ìš©ë¥ : {(total_applied/total_required*100):.1f}%" if total_required > 0 else "   ğŸ“ˆ ì ìš©ë¥ : 0%")
        report.append("")
        
        return "\n".join(report)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”¥ Stepë³„ ëª¨ë¸ ì ìš© ë„êµ¬")
    print("=" * 80)
    
    # ì ìš©ê¸° ì´ˆê¸°í™”
    applicator = StepModelApplicator()
    
    # 1. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ì°¾ê¸°
    print("\nğŸ“‹ 1ë‹¨ê³„: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ê²€ìƒ‰")
    applicator.find_available_models()
    
    # 2. ê° Stepì— ëª¨ë¸ ì ìš©
    print("\nğŸ”§ 2ë‹¨ê³„: ê° Stepì— ëª¨ë¸ ì ìš©")
    applicator.apply_models_to_steps()
    
    # 3. Stepë³„ ì„¤ì • íŒŒì¼ ìƒì„±
    print("\nğŸ“‹ 3ë‹¨ê³„: Stepë³„ ì„¤ì • íŒŒì¼ ìƒì„±")
    applicator.create_step_configs()
    
    # 4. íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸
    print("\nğŸ”§ 4ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸")
    applicator.update_pipeline_config()
    
    # 5. ì ìš© ë¦¬í¬íŠ¸ ìƒì„±
    print("\nğŸ“‹ 5ë‹¨ê³„: ì ìš© ë¦¬í¬íŠ¸ ìƒì„±")
    report = applicator.generate_application_report()
    print(report)
    
    # 6. ë¦¬í¬íŠ¸ ì €ì¥
    with open("step_model_application_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"\nğŸ’¾ ì ìš© ë¦¬í¬íŠ¸ ì €ì¥: step_model_application_report.txt")
    print("\nğŸ‰ Stepë³„ ëª¨ë¸ ì ìš© ì™„ë£Œ!")

if __name__ == "__main__":
    main()
