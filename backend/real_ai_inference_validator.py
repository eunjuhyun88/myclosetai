#!/usr/bin/env python3
"""
ğŸ”¥ Real AI Inference Validator v7.0 - ì‹¤ì œ ì¶”ë¡  ì™„ì „ ê²€ì¦ ì‹œìŠ¤í…œ
===============================================================================
âœ… Mock/í´ë°± ì—†ëŠ” 100% ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ê²€ì¦
âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ ì „ì²˜ë¦¬ â†’ ì¶”ë¡  â†’ í›„ì²˜ë¦¬ ì „ ê³¼ì • ê²€ì¦
âœ… ê° Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ì •ìƒ ì‘ë™ ì—¬ë¶€ í™•ì¸
âœ… BaseStepMixin _run_ai_inference() ë©”ì„œë“œ ì‹¤ì œ ì‹¤í–‰
âœ… Central Hub DI Container ì—°ë™ ìƒíƒœ ê²€ì¦
âœ… M3 Max MPS ë””ë°”ì´ìŠ¤ ìµœì í™” ì¶”ë¡  í…ŒìŠ¤íŠ¸
âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
âœ… ì¶”ë¡  ì„±ëŠ¥ ë° ê²°ê³¼ í’ˆì§ˆ ê²€ì¦
âœ… ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ì¦ ë° ë¬´ê²°ì„± ì²´í¬
âœ… GPU/MPS í…ì„œ ì—°ì‚° ì •ìƒ ì‘ë™ ê²€ì¦
===============================================================================
"""

import sys
import os
import time
import traceback
import logging
import asyncio
import threading
import psutil
import platform
import hashlib
import json
import importlib
import inspect
import gc
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
from contextlib import contextmanager
from enum import Enum
import base64
from io import BytesIO

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')
os.environ['PYTHONWARNINGS'] = 'ignore'

# í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ì •
current_file = Path(__file__).resolve()
project_root = current_file.parent
backend_root = project_root / 'backend'
ai_models_root = backend_root / "ai_models"

# ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(backend_root))
sys.path.insert(0, str(backend_root / "app"))

print(f"ğŸ”¥ Real AI Inference Validator v7.0 ì‹œì‘")
print(f"   í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
print(f"   AI ëª¨ë¸ ë£¨íŠ¸: {ai_models_root}")

# =============================================================================
# ğŸ”¥ 1. ì‹¤ì œ ì¶”ë¡  ê²€ì¦ ë°ì´í„° êµ¬ì¡°
# =============================================================================

class RealInferenceStatus(Enum):
    """ì‹¤ì œ ì¶”ë¡  ìƒíƒœ"""
    NOT_TESTED = "not_tested"
    MODEL_LOADING_FAILED = "model_loading_failed"
    CHECKPOINT_MISSING = "checkpoint_missing"
    CHECKPOINT_CORRUPTED = "checkpoint_corrupted"
    PREPROCESSING_FAILED = "preprocessing_failed"
    INFERENCE_FAILED = "inference_failed"
    POSTPROCESSING_FAILED = "postprocessing_failed"
    TENSOR_OPERATION_FAILED = "tensor_operation_failed"
    DEVICE_INCOMPATIBLE = "device_incompatible"
    MEMORY_INSUFFICIENT = "memory_insufficient"
    MOCK_FALLBACK_DETECTED = "mock_fallback_detected"
    SUCCESS = "success"

@dataclass
class RealInferenceResult:
    """ì‹¤ì œ ì¶”ë¡  ê²€ì¦ ê²°ê³¼"""
    step_name: str
    step_id: int
    
    # ëª¨ë¸ ë¡œë”© ê²€ì¦
    model_loading_success: bool = False
    checkpoint_loaded: bool = False
    checkpoint_size_mb: float = 0.0
    checkpoint_hash: str = ""
    model_parameters_count: int = 0
    
    # ë””ë°”ì´ìŠ¤ ê²€ì¦
    device_used: str = "cpu"
    device_compatible: bool = False
    mps_optimized: bool = False
    tensor_operations_working: bool = False
    
    # ì‹¤ì œ ì¶”ë¡  ê²€ì¦
    preprocessing_success: bool = False
    inference_success: bool = False
    postprocessing_success: bool = False
    total_inference_time: float = 0.0
    
    # ê²°ê³¼ ê²€ì¦
    output_shape_valid: bool = False
    output_data_type_valid: bool = False
    output_range_valid: bool = False
    confidence_score: float = 0.0
    
    # Mock/í´ë°± ê°ì§€
    mock_detected: bool = False
    fallback_used: bool = False
    real_ai_model_used: bool = False
    
    # ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥
    peak_memory_mb: float = 0.0
    memory_efficiency: str = "unknown"
    inference_fps: float = 0.0
    
    # ì˜¤ë¥˜ ì •ë³´
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    status: RealInferenceStatus = RealInferenceStatus.NOT_TESTED
    
    # ìƒì„¸ ì •ë³´
    model_info: Dict[str, Any] = field(default_factory=dict)
    inference_details: Dict[str, Any] = field(default_factory=dict)

# =============================================================================
# ğŸ”¥ 2. ì‹¤ì œ ì¶”ë¡  ê²€ì¦ ì‹œìŠ¤í…œ
# =============================================================================

class RealAIInferenceValidator:
    """ì‹¤ì œ AI ì¶”ë¡  ì™„ì „ ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.torch_available = False
        self.pil_available = False
        self.numpy_available = False
        self.cv2_available = False
        
        # ì˜ì¡´ì„± í™•ì¸
        self._check_dependencies()
        
        # GitHub Step ì„¤ì • (í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜)
        self.github_steps = [
            {
                'step_id': 1,
                'step_name': 'HumanParsingStep',
                'module_path': 'app.ai_pipeline.steps.step_01_human_parsing',
                'class_name': 'HumanParsingStep',
                'expected_models': ['graphonomy.pth', 'schp_model.pth'],
                'priority': 'critical'
            },
            {
                'step_id': 2,
                'step_name': 'PoseEstimationStep',
                'module_path': 'app.ai_pipeline.steps.step_02_pose_estimation', 
                'class_name': 'PoseEstimationStep',
                'expected_models': ['pose_model.pth', 'dw-ll_ucoco_384.pth'],
                'priority': 'critical'
            },
            {
                'step_id': 3,
                'step_name': 'ClothSegmentationStep',
                'module_path': 'app.ai_pipeline.steps.step_03_cloth_segmentation',
                'class_name': 'ClothSegmentationStep', 
                'expected_models': ['sam_vit_h_4b8939.pth', 'u2net_alternative.pth'],
                'priority': 'critical'
            },
            {
                'step_id': 4,
                'step_name': 'GeometricMatchingStep',
                'module_path': 'app.ai_pipeline.steps.step_04_geometric_matching',
                'class_name': 'GeometricMatchingStep',
                'expected_models': ['gmm_model.pth', 'tom_model.pth'],
                'priority': 'high'
            },
            {
                'step_id': 5,
                'step_name': 'ClothWarpingStep',
                'module_path': 'app.ai_pipeline.steps.step_05_cloth_warping',
                'class_name': 'ClothWarpingStep',
                'expected_models': ['RealVisXL_V4.0.safetensors', 'warping_model.pth'],
                'priority': 'high'
            },
            {
                'step_id': 6,
                'step_name': 'VirtualFittingStep',
                'module_path': 'app.ai_pipeline.steps.step_06_virtual_fitting',
                'class_name': 'VirtualFittingStep',
                'expected_models': ['ootd_hd_checkpoint.safetensors', 'sd_model.safetensors'],
                'priority': 'critical'
            },
            {
                'step_id': 7,
                'step_name': 'PostProcessingStep',
                'module_path': 'app.ai_pipeline.steps.step_07_post_processing',
                'class_name': 'PostProcessingStep',
                'expected_models': ['esrgan_x8.pth', 'realesrgan_x4.pth'],
                'priority': 'medium'
            },
            {
                'step_id': 8,
                'step_name': 'QualityAssessmentStep',
                'module_path': 'app.ai_pipeline.steps.step_08_quality_assessment',
                'class_name': 'QualityAssessmentStep',
                'expected_models': ['ViT-L-14.pt', 'clip_model.pt'],
                'priority': 'medium'
            }
        ]
    
    def _check_dependencies(self):
        """ì˜ì¡´ì„± í™•ì¸"""
        try:
            import torch
            self.torch_available = True
            self.torch = torch
        except ImportError:
            pass
            
        try:
            from PIL import Image
            self.pil_available = True
            self.pil = Image
        except ImportError:
            pass
            
        try:
            import numpy as np
            self.numpy_available = True
            self.numpy = np
        except ImportError:
            pass
            
        try:
            import cv2
            self.cv2_available = True
            self.cv2 = cv2
        except ImportError:
            pass
    
    def validate_real_inference_for_step(self, step_config: Dict[str, Any]) -> RealInferenceResult:
        """Stepë³„ ì‹¤ì œ ì¶”ë¡  ì™„ì „ ê²€ì¦"""
        
        result = RealInferenceResult(
            step_name=step_config['step_name'],
            step_id=step_config['step_id']
        )
        
        print(f"\nğŸ”¥ {step_config['step_name']} ì‹¤ì œ ì¶”ë¡  ê²€ì¦ ì‹œì‘...")
        
        try:
            # 1. Step í´ë˜ìŠ¤ ë¡œë”© ë° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            step_instance = self._create_step_instance(step_config, result)
            if not step_instance:
                result.status = RealInferenceStatus.MODEL_LOADING_FAILED
                return result
            
            # 2. ì‹¤ì œ ëª¨ë¸ ë¡œë”© ê²€ì¦
            if not self._validate_model_loading(step_instance, result):
                return result
            
            # 3. ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ê²€ì¦
            if not self._validate_device_compatibility(step_instance, result):
                return result
            
            # 4. ì‹¤ì œ ì¶”ë¡  ì‹¤í–‰ ê²€ì¦
            if not self._validate_real_inference_execution(step_instance, result):
                return result
            
            # 5. Mock/í´ë°± ê°ì§€
            self._detect_mock_fallback(step_instance, result)
            
            # 6. ì„±ëŠ¥ ë° ê²°ê³¼ í’ˆì§ˆ ê²€ì¦
            self._validate_inference_quality(step_instance, result)
            
            # Final ìƒíƒœ ê²°ì •
            if result.real_ai_model_used and result.inference_success and not result.mock_detected:
                result.status = RealInferenceStatus.SUCCESS
                print(f"   âœ… ì‹¤ì œ AI ì¶”ë¡  ì™„ì „ ê²€ì¦ ì„±ê³µ!")
            else:
                result.status = RealInferenceStatus.MOCK_FALLBACK_DETECTED
                print(f"   âš ï¸ Mock/í´ë°± ê°ì§€ë¨")
            
        except Exception as e:
            result.errors.append(f"ê²€ì¦ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            result.status = RealInferenceStatus.INFERENCE_FAILED
            print(f"   âŒ ê²€ì¦ ì‹¤íŒ¨: {str(e)[:100]}")
        
        return result
    
    def _create_step_instance(self, step_config: Dict[str, Any], result: RealInferenceResult) -> Any:
        """Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Central Hub DI Container ì‚¬ìš©)"""
        try:
            # ğŸ”¥ Central Hub DI Containerë¥¼ í†µí•œ Step ìƒì„±
            print(f"   ğŸ”„ Central Hub DI Containerë¥¼ í†µí•œ Step ìƒì„± ì‹œë„...")
            
            # Central Hub DI Container ì¡°íšŒ
            try:
                import importlib
                di_module = importlib.import_module('app.core.di_container')
                central_hub_container = di_module.get_global_container()
                
                if central_hub_container:
                    print(f"   âœ… Central Hub DI Container ì—°ê²°ë¨")
                    
                    # StepFactoryë¥¼ í†µí•œ Step ìƒì„±
                    step_factory = central_hub_container.get('step_factory')
                    if step_factory:
                        print(f"   âœ… StepFactory ë°œê²¬")
                        
                        # StepTypeìœ¼ë¡œ ë³€í™˜ (StepFactoryê°€ ì¸ì‹í•˜ëŠ” í˜•ì‹)
                        step_name = step_config['step_name']
                        if step_name == 'HumanParsingStep':
                            step_type = 'human_parsing'
                        elif step_name == 'PoseEstimationStep':
                            step_type = 'pose_estimation'
                        elif step_name == 'ClothSegmentationStep':
                            step_type = 'cloth_segmentation'
                        elif step_name == 'GeometricMatchingStep':
                            step_type = 'geometric_matching'
                        elif step_name == 'ClothWarpingStep':
                            step_type = 'cloth_warping'
                        elif step_name == 'VirtualFittingStep':
                            step_type = 'virtual_fitting'
                        elif step_name == 'PostProcessingStep':
                            step_type = 'post_processing'
                        elif step_name == 'QualityAssessmentStep':
                            step_type = 'quality_assessment'
                        else:
                            step_type = step_name.lower().replace('step', '').replace('_', '')
                        
                        step_instance = step_factory.create_step(step_type)
                        
                        if step_instance:
                            print(f"   âœ… Central Hubë¥¼ í†µí•œ Step ìƒì„± ì„±ê³µ")
                            
                            # BaseStepMixin ìƒì† í™•ì¸
                            from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
                            if isinstance(step_instance, BaseStepMixin):
                                print(f"   âœ… BaseStepMixin ìƒì† í™•ì¸")
                            else:
                                result.warnings.append("BaseStepMixin ìƒì†ë˜ì§€ ì•ŠìŒ")
                            
                            return step_instance
                        else:
                            print(f"   âš ï¸ Central Hub Step ìƒì„± ì‹¤íŒ¨ - ì§ì ‘ ìƒì„± ì‹œë„")
                    else:
                        print(f"   âš ï¸ StepFactory ì—†ìŒ - ì§ì ‘ ìƒì„± ì‹œë„")
                else:
                    print(f"   âš ï¸ Central Hub Container ì—†ìŒ - ì§ì ‘ ìƒì„± ì‹œë„")
            except Exception as e:
                print(f"   âš ï¸ Central Hub ì—°ê²° ì‹¤íŒ¨: {e} - ì§ì ‘ ìƒì„± ì‹œë„")
            
            # ğŸ”„ í´ë°±: ì§ì ‘ ìƒì„± (ê¸°ì¡´ ë°©ì‹)
            print(f"   ğŸ”„ ì§ì ‘ Step ìƒì„± (í´ë°±)...")
            
            # ë™ì  import
            module = importlib.import_module(step_config['module_path'])
            step_class = getattr(module, step_config['class_name'])
            
            # ìµœì  ë””ë°”ì´ìŠ¤ ê²°ì •
            device = 'cpu'
            if self.torch_available:
                if hasattr(self.torch.backends, 'mps') and self.torch.backends.mps.is_available():
                    device = 'mps'
                    result.mps_optimized = True
                elif self.torch.cuda.is_available():
                    device = 'cuda'
            
            result.device_used = device
            
            # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© í™œì„±í™”)
            try:
                # ë¨¼ì € ê¸°ë³¸ ë§¤ê°œë³€ìˆ˜ë¡œ ì‹œë„
                step_instance = step_class(device=device)
            except Exception as e1:
                print(f"   âš ï¸ ê¸°ë³¸ ë§¤ê°œë³€ìˆ˜ë¡œ ìƒì„± ì‹¤íŒ¨: {e1}")
                try:
                    # ì¶”ê°€ ë§¤ê°œë³€ìˆ˜ë¡œ ì‹œë„
                    step_instance = step_class(device=device, strict_mode=False)
                except Exception as e2:
                    print(f"   âš ï¸ ì¶”ê°€ ë§¤ê°œë³€ìˆ˜ë¡œ ìƒì„± ì‹¤íŒ¨: {e2}")
                    # ë§ˆì§€ë§‰ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ ì—†ì´ ì‹œë„
                    step_instance = step_class()
            
            # BaseStepMixin ìƒì† í™•ì¸
            from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
            if isinstance(step_instance, BaseStepMixin):
                print(f"   âœ… BaseStepMixin ìƒì† í™•ì¸")
            else:
                result.warnings.append("BaseStepMixin ìƒì†ë˜ì§€ ì•ŠìŒ")
            
            return step_instance
            
        except Exception as e:
            result.errors.append(f"Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _validate_model_loading(self, step_instance: Any, result: RealInferenceResult) -> bool:
        """ì‹¤ì œ ëª¨ë¸ ë¡œë”© ê²€ì¦"""
        try:
            print(f"   ğŸ” ëª¨ë¸ ë¡œë”© ê²€ì¦...")
            
            # ì´ˆê¸°í™” ì‹œë„
            if hasattr(step_instance, 'initialize'):
                if asyncio.iscoroutinefunction(step_instance.initialize):
                    # ë¹„ë™ê¸° ì´ˆê¸°í™”
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                    
                    init_success = loop.run_until_complete(
                        asyncio.wait_for(step_instance.initialize(), timeout=180.0)
                    )
                else:
                    # ë™ê¸° ì´ˆê¸°í™”
                    init_success = step_instance.initialize()
                
                if not init_success:
                    result.errors.append("ì´ˆê¸°í™” ì‹¤íŒ¨")
                    result.status = RealInferenceStatus.MODEL_LOADING_FAILED
                    return False
            
            # ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸
            if hasattr(step_instance, 'has_model') and not step_instance.has_model:
                result.errors.append("has_model = False")
                result.status = RealInferenceStatus.MODEL_LOADING_FAILED
                return False
            
            # AI ëª¨ë¸ ì¡´ì¬ í™•ì¸
            if hasattr(step_instance, 'ai_models'):
                if not step_instance.ai_models or all(model is None for model in step_instance.ai_models.values()):
                    result.errors.append("AI ëª¨ë¸ì´ None")
                    result.status = RealInferenceStatus.MODEL_LOADING_FAILED
                    return False
                
                # ë¡œë”©ëœ ëª¨ë¸ ì •ë³´
                loaded_models = []
                total_params = 0
                
                for model_name, model in step_instance.ai_models.items():
                    if model is not None:
                        loaded_models.append(model_name)
                        
                        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
                        if hasattr(model, 'parameters'):
                            try:
                                params = sum(p.numel() for p in model.parameters())
                                total_params += params
                            except Exception:
                                pass
                
                result.model_info['loaded_models'] = loaded_models
                result.model_parameters_count = total_params
                result.model_loading_success = len(loaded_models) > 0
                
                print(f"      âœ… ë¡œë”©ëœ ëª¨ë¸: {loaded_models}")
                print(f"      ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
                
            # ì²´í¬í¬ì¸íŠ¸ ì •ë³´
            self._analyze_checkpoint_info(step_instance, result)
            
            return result.model_loading_success
            
        except Exception as e:
            result.errors.append(f"ëª¨ë¸ ë¡œë”© ê²€ì¦ ì‹¤íŒ¨: {e}")
            result.status = RealInferenceStatus.MODEL_LOADING_FAILED
            return False
    
    def _analyze_checkpoint_info(self, step_instance: Any, result: RealInferenceResult):
        """ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ë¶„ì„"""
        try:
            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸°
            step_name = result.step_name.lower()
            step_id = result.step_id
            
            # ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë“¤
            checkpoint_paths = []
            
            if ai_models_root.exists():
                # Stepë³„ ë””ë ‰í† ë¦¬ íŒ¨í„´
                patterns = [
                    f"step_{step_id:02d}_*",
                    f"*{step_name.replace('step', '').lower()}*",
                    f"checkpoints/step_{step_id:02d}_*"
                ]
                
                for pattern in patterns:
                    matching_dirs = list(ai_models_root.glob(pattern))
                    for model_dir in matching_dirs:
                        if model_dir.is_dir():
                            for ext in ['*.pth', '*.pt', '*.safetensors', '*.bin', '*.ckpt']:
                                checkpoint_paths.extend(model_dir.rglob(ext))
            
            # ì²« ë²ˆì§¸ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„
            if checkpoint_paths:
                checkpoint_file = checkpoint_paths[0]
                
                # íŒŒì¼ í¬ê¸°
                result.checkpoint_size_mb = checkpoint_file.stat().st_size / (1024 * 1024)
                
                # í•´ì‹œ (ë¹ ë¥¸ ìƒ˜í”Œ í•´ì‹œ)
                result.checkpoint_hash = self._calculate_quick_hash(checkpoint_file)
                result.checkpoint_loaded = True
                
                print(f"      ğŸ“ ì²´í¬í¬ì¸íŠ¸: {checkpoint_file.name} ({result.checkpoint_size_mb:.1f}MB)")
                
        except Exception as e:
            result.warnings.append(f"ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _calculate_quick_hash(self, file_path: Path) -> str:
        """ë¹ ë¥¸ í•´ì‹œ ê³„ì‚°"""
        try:
            hash_md5 = hashlib.md5()
            file_size = file_path.stat().st_size
            sample_size = min(1024 * 1024, file_size)  # ìµœëŒ€ 1MB ìƒ˜í”Œë§
            
            with open(file_path, "rb") as f:
                chunk = f.read(sample_size)
                hash_md5.update(chunk)
            
            return hash_md5.hexdigest()[:16]  # ì²˜ìŒ 16ìë§Œ
        except Exception:
            return ""
    
    def _validate_device_compatibility(self, step_instance: Any, result: RealInferenceResult) -> bool:
        """ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ê²€ì¦"""
        try:
            print(f"   ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ê²€ì¦...")
            
            if not self.torch_available:
                result.warnings.append("PyTorch ì—†ìŒ")
                return True  # CPU ëª¨ë“œë¡œ ê³„ì† ì§„í–‰
            
            # ê¸°ë³¸ í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸
            try:
                device = result.device_used
                test_tensor = self.torch.randn(4, 4).to(device)
                result_tensor = test_tensor * 2.0 + 1.0
                
                # ê²°ê³¼ ê²€ì¦
                if result_tensor.shape == (4, 4) and result_tensor.device.type == device:
                    result.tensor_operations_working = True
                    result.device_compatible = True
                    print(f"      âœ… {device} í…ì„œ ì—°ì‚° ì •ìƒ")
                
            except Exception as e:
                result.errors.append(f"í…ì„œ ì—°ì‚° ì‹¤íŒ¨: {e}")
                result.status = RealInferenceStatus.TENSOR_OPERATION_FAILED
                return False
            
            # MPS íŠ¹í™” í…ŒìŠ¤íŠ¸
            if result.device_used == 'mps':
                try:
                    # float64 â†’ float32 ë³€í™˜ í…ŒìŠ¤íŠ¸
                    test_tensor_64 = self.torch.randn(2, 2, dtype=self.torch.float64)
                    test_tensor_32 = test_tensor_64.to(self.torch.float32).to('mps')
                    _ = test_tensor_32 + 1.0
                    result.mps_optimized = True
                    print(f"      âœ… MPS float64â†’float32 ë³€í™˜ ì •ìƒ")
                except Exception as e:
                    result.warnings.append(f"MPS ìµœì í™” ì‹¤íŒ¨: {e}")
            
            return True
            
        except Exception as e:
            result.errors.append(f"ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            result.status = RealInferenceStatus.DEVICE_INCOMPATIBLE
            return False
    
    def _validate_real_inference_execution(self, step_instance: Any, result: RealInferenceResult) -> bool:
        """ì‹¤ì œ ì¶”ë¡  ì‹¤í–‰ ê²€ì¦"""
        try:
            print(f"   ğŸ§  ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰ ê²€ì¦...")
            
            # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
            start_memory = psutil.Process().memory_info().rss / (1024 * 1024)
            start_time = time.time()
            
            # ë”ë¯¸ ì…ë ¥ ë°ì´í„° ìƒì„±
            test_input = self._create_test_input(step_instance, result)
            if not test_input:
                result.status = RealInferenceStatus.PREPROCESSING_FAILED
                return False
            
            result.preprocessing_success = True
            
            # ì‹¤ì œ _run_ai_inference ë©”ì„œë“œ í˜¸ì¶œ
            if not hasattr(step_instance, '_run_ai_inference'):
                result.errors.append("_run_ai_inference ë©”ì„œë“œ ì—†ìŒ")
                result.status = RealInferenceStatus.INFERENCE_FAILED
                return False
            
            # ì‹¤ì œ ì¶”ë¡  ì‹¤í–‰
            print(f"      ğŸ”¥ _run_ai_inference() ì‹¤í–‰...")
            ai_result = step_instance._run_ai_inference(test_input)
            
            # ì¶”ë¡  ì‹œê°„ ê³„ì‚°
            inference_time = time.time() - start_time
            result.total_inference_time = inference_time
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            end_memory = psutil.Process().memory_info().rss / (1024 * 1024)
            result.peak_memory_mb = end_memory - start_memory
            
            if result.peak_memory_mb < 500:
                result.memory_efficiency = "excellent"
            elif result.peak_memory_mb < 1000:
                result.memory_efficiency = "good"
            else:
                result.memory_efficiency = "high"
            
            # ê²°ê³¼ ê²€ì¦
            if not self._validate_inference_result(ai_result, result):
                return False
            
            result.inference_success = True
            result.postprocessing_success = True
            
            print(f"      âœ… ì¶”ë¡  ì„±ê³µ ({inference_time:.3f}ì´ˆ, ë©”ëª¨ë¦¬: {result.peak_memory_mb:.1f}MB)")
            
            return True
            
        except Exception as e:
            result.errors.append(f"ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            result.status = RealInferenceStatus.INFERENCE_FAILED
            return False
    
    def _create_test_input(self, step_instance: Any, result: RealInferenceResult) -> Dict[str, Any]:
        """Stepë³„ í…ŒìŠ¤íŠ¸ ì…ë ¥ ë°ì´í„° ìƒì„±"""
        try:
            step_name = result.step_name
            
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì…ë ¥
            test_input = {}
            
            if self.numpy_available and self.pil_available:
                # 512x512 RGB ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
                dummy_image_np = self.numpy.random.randint(0, 255, (512, 512, 3), dtype=self.numpy.uint8)
                dummy_image_pil = self.pil.fromarray(dummy_image_np)
                
                # Stepë³„ íŠ¹í™” ì…ë ¥
                if 'HumanParsing' in step_name:
                    test_input = {
                        'person_image': dummy_image_pil,
                        'parsing_classes': 20
                    }
                elif 'PoseEstimation' in step_name:
                    test_input = {
                        'person_image': dummy_image_pil,
                        'keypoint_threshold': 0.1
                    }
                elif 'ClothSegmentation' in step_name:
                    test_input = {
                        'clothing_image': dummy_image_pil,
                        'person_image': dummy_image_pil
                    }
                elif 'GeometricMatching' in step_name:
                    test_input = {
                        'person_image': dummy_image_pil,
                        'clothing_image': dummy_image_pil,
                        'person_parsing': dummy_image_np
                    }
                elif 'ClothWarping' in step_name:
                    test_input = {
                        'clothing_image': dummy_image_pil,
                        'person_parsing': dummy_image_np,
                        'pose_keypoints': self.numpy.random.rand(18, 3).tolist()
                    }
                elif 'VirtualFitting' in step_name:
                    test_input = {
                        'person_image': dummy_image_pil,
                        'clothing_image': dummy_image_pil,
                        'warped_cloth': dummy_image_pil
                    }
                elif 'PostProcessing' in step_name:
                    test_input = {
                        'fitted_image': dummy_image_pil,
                        'enhancement_level': 0.8
                    }
                elif 'QualityAssessment' in step_name:
                    test_input = {
                        'result_image': dummy_image_pil,
                        'original_person': dummy_image_pil,
                        'target_clothing': dummy_image_pil
                    }
                else:
                    # ë²”ìš© ì…ë ¥
                    test_input = {
                        'input_image': dummy_image_pil,
                        'data': dummy_image_np
                    }
            else:
                # numpy/PIL ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ë°ì´í„°
                test_input = {
                    'data': {'test': True},
                    'input_size': (512, 512)
                }
            
            return test_input
            
        except Exception as e:
            result.errors.append(f"í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _validate_inference_result(self, ai_result: Any, result: RealInferenceResult) -> bool:
        """ì¶”ë¡  ê²°ê³¼ ê²€ì¦"""
        try:
            if not ai_result:
                result.errors.append("ì¶”ë¡  ê²°ê³¼ê°€ None")
                return False
            
            if not isinstance(ai_result, dict):
                result.errors.append("ì¶”ë¡  ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜")
                return False
            
            # ê¸°ë³¸ í•„ë“œ í™•ì¸
            required_fields = ['success', 'processing_time']
            missing_fields = [field for field in required_fields if field not in ai_result]
            
            if missing_fields:
                result.warnings.append(f"ëˆ„ë½ëœ í•„ë“œ: {missing_fields}")
            
            # ì„±ê³µ ì—¬ë¶€ í™•ì¸
            if 'success' in ai_result and not ai_result['success']:
                result.errors.append("ì¶”ë¡  ê²°ê³¼ success=False")
                return False
            
            # ê²°ê³¼ ë°ì´í„° íƒ€ì… í™•ì¸
            if 'result' in ai_result:
                output_data = ai_result['result']
                
                if self.numpy_available and isinstance(output_data, self.numpy.ndarray):
                    result.output_shape_valid = len(output_data.shape) >= 2
                    result.output_data_type_valid = True
                    
                    # ê°’ ë²”ìœ„ í™•ì¸ (ì´ë¯¸ì§€ì¸ ê²½ìš°)
                    if output_data.dtype in [self.numpy.uint8, self.numpy.float32]:
                        if self.numpy.all((output_data >= 0) & (output_data <= 255)):
                            result.output_range_valid = True
                
                elif self.pil_available and hasattr(output_data, 'size'):
                    # PIL Image í™•ì¸
                    result.output_shape_valid = len(output_data.size) == 2
                    result.output_data_type_valid = True
                    result.output_range_valid = True
                
                elif isinstance(output_data, (list, tuple)):
                    result.output_data_type_valid = True
                    result.output_shape_valid = len(output_data) > 0
            
            # ì‹ ë¢°ë„ ì ìˆ˜
            if 'confidence' in ai_result:
                try:
                    confidence = float(ai_result['confidence'])
                    result.confidence_score = confidence
                except (ValueError, TypeError):
                    result.warnings.append("confidence ê°’ì´ ìˆ«ìê°€ ì•„ë‹˜")
            
            # ìƒì„¸ ì •ë³´ ì €ì¥
            result.inference_details = {
                'result_keys': list(ai_result.keys()),
                'processing_time': ai_result.get('processing_time', 0),
                'device_used': ai_result.get('device_used', 'unknown'),
                'model_loaded': ai_result.get('model_loaded', False),
                'step_name': ai_result.get('step_name', result.step_name)
            }
            
            return True
            
        except Exception as e:
            result.errors.append(f"ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def _detect_mock_fallback(self, step_instance: Any, result: RealInferenceResult):
        """Mock/í´ë°± ì‚¬ìš© ê°ì§€"""
        try:
            print(f"   ğŸ” Mock/í´ë°± ê°ì§€...")
            
            # Mock ê°ì§€ íŒ¨í„´ë“¤
            mock_indicators = [
                'mock_result',
                'fallback_result', 
                'dummy_output',
                'test_result',
                'placeholder'
            ]
            
            fallback_indicators = [
                'fallback_used',
                'emergency_mode',
                'mock_detected',
                'no_model_available'
            ]
            
            # Step ìƒíƒœ í™•ì¸
            if hasattr(step_instance, 'get_status'):
                status = step_instance.get_status()
                
                # Mock ê°ì§€
                for indicator in mock_indicators:
                    if indicator in str(status).lower():
                        result.mock_detected = True
                        break
                
                # í´ë°± ê°ì§€  
                for indicator in fallback_indicators:
                    if indicator in str(status).lower():
                        result.fallback_used = True
                        break
            
            # AI ëª¨ë¸ ì‹¤ì œ ì‚¬ìš© í™•ì¸
            if hasattr(step_instance, 'ai_models') and step_instance.ai_models:
                # ëª¨ë“  ëª¨ë¸ì´ Noneì´ ì•„ë‹ˆë©´ ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©
                real_models = [model for model in step_instance.ai_models.values() if model is not None]
                result.real_ai_model_used = len(real_models) > 0
                
                if result.real_ai_model_used:
                    print(f"      âœ… ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš© í™•ì¸: {len(real_models)}ê°œ")
                else:
                    print(f"      âš ï¸ AI ëª¨ë¸ì´ ëª¨ë‘ None")
                    result.mock_detected = True
            
            # BaseStepMixinì˜ ì‹¤ì œ ì¶”ë¡  ë©”ì„œë“œ í™•ì¸
            if hasattr(step_instance, '_run_ai_inference'):
                # ë©”ì„œë“œ ì†ŒìŠ¤ ì½”ë“œ í™•ì¸ (ê°„ì ‘ì  Mock ê°ì§€)
                try:
                    import inspect
                    source = inspect.getsource(step_instance._run_ai_inference)
                    
                    if any(keyword in source.lower() for keyword in ['mock', 'dummy', 'fallback', 'placeholder']):
                        result.mock_detected = True
                        result.warnings.append("_run_ai_inferenceì— Mock íŒ¨í„´ ê°ì§€")
                        
                except Exception:
                    pass
            
            # ìµœì¢… íŒì •
            if not result.mock_detected and not result.fallback_used and result.real_ai_model_used:
                print(f"      âœ… ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš© í™•ì¸")
            else:
                print(f"      âš ï¸ Mock/í´ë°± ì‚¬ìš© ì˜ì‹¬")
                
        except Exception as e:
            result.warnings.append(f"Mock ê°ì§€ ì‹¤íŒ¨: {e}")
    
    def _validate_inference_quality(self, step_instance: Any, result: RealInferenceResult):
        """ì¶”ë¡  í’ˆì§ˆ ë° ì„±ëŠ¥ ê²€ì¦"""
        try:
            print(f"   ğŸ“Š ì¶”ë¡  í’ˆì§ˆ ë° ì„±ëŠ¥ ê²€ì¦...")
            
            # FPS ê³„ì‚°
            if result.total_inference_time > 0:
                result.inference_fps = 1.0 / result.total_inference_time
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¬í‰ê°€
            if result.peak_memory_mb > 0:
                if result.peak_memory_mb < 200:
                    result.memory_efficiency = "excellent"
                elif result.peak_memory_mb < 500:
                    result.memory_efficiency = "good"  
                elif result.peak_memory_mb < 1000:
                    result.memory_efficiency = "moderate"
                else:
                    result.memory_efficiency = "high"
            
            # Stepë³„ íŠ¹í™” í’ˆì§ˆ í™•ì¸
            step_name = result.step_name
            
            if 'HumanParsing' in step_name:
                # Human Parsing í’ˆì§ˆ í™•ì¸
                if result.output_shape_valid and result.confidence_score > 0.8:
                    result.warnings.append("Human Parsing í’ˆì§ˆ ì–‘í˜¸")
                    
            elif 'VirtualFitting' in step_name:
                # Virtual Fitting í’ˆì§ˆ í™•ì¸ (ê°€ì¥ ì¤‘ìš”)
                if result.inference_success and result.total_inference_time < 10.0:
                    result.warnings.append("Virtual Fitting ì„±ëŠ¥ ì–‘í˜¸")
                elif result.total_inference_time > 30.0:
                    result.warnings.append("Virtual Fitting ì„±ëŠ¥ ì €í•˜")
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_factors = [
                result.model_loading_success,
                result.inference_success,
                result.real_ai_model_used,
                not result.mock_detected,
                result.output_shape_valid,
                result.output_data_type_valid,
                result.device_compatible
            ]
            
            quality_score = sum(quality_factors) / len(quality_factors) * 100
            result.confidence_score = max(result.confidence_score, quality_score / 100)
            
            print(f"      ğŸ“ˆ í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f}%, FPS: {result.inference_fps:.2f}")
            
        except Exception as e:
            result.warnings.append(f"í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 3. ì „ì²´ ì‹œìŠ¤í…œ ê²€ì¦ ë§¤ë‹ˆì €  
# =============================================================================

class RealAISystemValidator:
    """ì „ì²´ AI ì‹œìŠ¤í…œ ì‹¤ì œ ì¶”ë¡  ê²€ì¦ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.validator = RealAIInferenceValidator()
        self.results = {}
        self.system_metrics = {}
        
    def validate_entire_ai_pipeline(self) -> Dict[str, Any]:
        """ì „ì²´ AI íŒŒì´í”„ë¼ì¸ ì‹¤ì œ ì¶”ë¡  ê²€ì¦"""
        
        print("ğŸ”¥" * 60)
        print("ğŸ”¥ Real AI Inference Validator v7.0 - ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²€ì¦")
        print("ğŸ”¥ Target: Mock/í´ë°± ì—†ëŠ” 100% ì‹¤ì œ AI ì¶”ë¡  ê²€ì¦")
        print("ğŸ”¥" * 60)
        
        validation_report = {
            'timestamp': time.time(),
            'validator_version': '7.0',
            'total_steps': len(self.validator.github_steps),
            'step_results': {},
            'system_summary': {},
            'critical_issues': [],
            'performance_metrics': {},
            'recommendations': []
        }
        
        start_time = time.time()
        
        try:
            # 1. ì‹œìŠ¤í…œ í™˜ê²½ í™•ì¸
            self._check_system_environment()
            
            # 2. ê° Stepë³„ ì‹¤ì œ ì¶”ë¡  ê²€ì¦
            print(f"\nğŸ“Š 8ë‹¨ê³„ AI Step ì‹¤ì œ ì¶”ë¡  ê²€ì¦ ì‹œì‘...")
            
            for step_config in self.validator.github_steps:
                try:
                    print(f"\n{'='*60}")
                    result = self.validator.validate_real_inference_for_step(step_config)
                    self.results[step_config['step_name']] = result
                    validation_report['step_results'][step_config['step_name']] = self._serialize_result(result)
                    
                except Exception as e:
                    print(f"âŒ {step_config['step_name']} ê²€ì¦ ì‹¤íŒ¨: {e}")
                    validation_report['step_results'][step_config['step_name']] = {
                        'error': str(e),
                        'status': 'validation_failed'
                    }
            
            # 3. ì „ì²´ ë¶„ì„ ë° ìš”ì•½
            validation_report['system_summary'] = self._generate_system_summary()
            validation_report['critical_issues'] = self._identify_critical_issues()
            validation_report['performance_metrics'] = self._calculate_performance_metrics()
            validation_report['recommendations'] = self._generate_recommendations()
            
            # 4. ê²°ê³¼ ì¶œë ¥
            self._print_validation_results(validation_report)
            
            # 5. ê²°ê³¼ ì €ì¥
            self._save_validation_results(validation_report)
            
        except Exception as e:
            print(f"\nâŒ ì „ì²´ ê²€ì¦ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            validation_report['fatal_error'] = str(e)
            
        finally:
            total_time = time.time() - start_time
            validation_report['total_validation_time'] = total_time
            print(f"\nğŸ‰ Real AI Inference Validation ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
        
        return validation_report
    
    def _check_system_environment(self):
        """ì‹œìŠ¤í…œ í™˜ê²½ í™•ì¸"""
        print(f"\nğŸ–¥ï¸ ì‹œìŠ¤í…œ í™˜ê²½ í™•ì¸...")
        
        # í•˜ë“œì›¨ì–´ ì •ë³´
        cpu_count = psutil.cpu_count(logical=True)
        memory_info = psutil.virtual_memory()
        total_memory_gb = memory_info.total / (1024**3)
        available_memory_gb = memory_info.available / (1024**3)
        
        print(f"   ğŸ’» CPU: {cpu_count}ì½”ì–´")
        print(f"   ğŸ’¾ ë©”ëª¨ë¦¬: {available_memory_gb:.1f}GB ì‚¬ìš©ê°€ëŠ¥ / {total_memory_gb:.1f}GB ì „ì²´")
        
        # M3 Max ê°ì§€
        is_m3_max = False
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            try:
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=5)
                if 'M3' in result.stdout:
                    is_m3_max = True
                    print(f"   ğŸš€ M3 Max ê°ì§€ë¨")
            except Exception:
                pass
        
        # PyTorch í™˜ê²½
        if self.validator.torch_available:
            torch = self.validator.torch
            print(f"   ğŸ”¥ PyTorch: {torch.__version__}")
            
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                print(f"   âš¡ MPS: ì‚¬ìš© ê°€ëŠ¥")
            elif torch.cuda.is_available():
                print(f"   ğŸ¯ CUDA: ì‚¬ìš© ê°€ëŠ¥")
            else:
                print(f"   ğŸ–¥ï¸ CPU ëª¨ë“œ")
        else:
            print(f"   âŒ PyTorch ì—†ìŒ")
        
        # AI ëª¨ë¸ ë””ë ‰í† ë¦¬
        if ai_models_root.exists():
            total_size = sum(f.stat().st_size for f in ai_models_root.rglob('*') if f.is_file())
            total_size_gb = total_size / (1024**3)
            print(f"   ğŸ“ AI ëª¨ë¸: {total_size_gb:.1f}GB")
        else:
            print(f"   âŒ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ")
        
        self.system_metrics = {
            'cpu_count': cpu_count,
            'total_memory_gb': total_memory_gb,
            'available_memory_gb': available_memory_gb,
            'is_m3_max': is_m3_max,
            'torch_available': self.validator.torch_available,
            'ai_models_size_gb': total_size_gb if ai_models_root.exists() else 0
        }
    
    def _serialize_result(self, result: RealInferenceResult) -> Dict[str, Any]:
        """ê²°ê³¼ ì§ë ¬í™”"""
        return {
            'step_name': result.step_name,
            'step_id': result.step_id,
            'status': result.status.value,
            'model_loading_success': result.model_loading_success,
            'checkpoint_loaded': result.checkpoint_loaded,
            'checkpoint_size_mb': result.checkpoint_size_mb,
            'model_parameters_count': result.model_parameters_count,
            'device_used': result.device_used,
            'device_compatible': result.device_compatible,
            'mps_optimized': result.mps_optimized,
            'tensor_operations_working': result.tensor_operations_working,
            'preprocessing_success': result.preprocessing_success,
            'inference_success': result.inference_success,
            'postprocessing_success': result.postprocessing_success,
            'total_inference_time': result.total_inference_time,
            'output_shape_valid': result.output_shape_valid,
            'output_data_type_valid': result.output_data_type_valid,
            'output_range_valid': result.output_range_valid,
            'confidence_score': result.confidence_score,
            'mock_detected': result.mock_detected,
            'fallback_used': result.fallback_used,
            'real_ai_model_used': result.real_ai_model_used,
            'peak_memory_mb': result.peak_memory_mb,
            'memory_efficiency': result.memory_efficiency,
            'inference_fps': result.inference_fps,
            'errors': result.errors,
            'warnings': result.warnings,
            'model_info': result.model_info,
            'inference_details': result.inference_details
        }
    
    def _generate_system_summary(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìš”ì•½ ìƒì„±"""
        total_steps = len(self.results)
        successful_steps = sum(1 for result in self.results.values() 
                              if result.status == RealInferenceStatus.SUCCESS)
        
        real_ai_steps = sum(1 for result in self.results.values() 
                           if result.real_ai_model_used and not result.mock_detected)
        
        mock_detected_steps = sum(1 for result in self.results.values() 
                                 if result.mock_detected)
        
        critical_steps = ['HumanParsingStep', 'PoseEstimationStep', 'ClothSegmentationStep', 'VirtualFittingStep']
        critical_success = sum(1 for step_name in critical_steps 
                              if step_name in self.results and 
                              self.results[step_name].status == RealInferenceStatus.SUCCESS)
        
        # ì„±ëŠ¥ í†µê³„
        inference_times = [result.total_inference_time for result in self.results.values()
                          if result.total_inference_time > 0]
        avg_inference_time = sum(inference_times) / len(inference_times) if inference_times else 0
        
        memory_usage = [result.peak_memory_mb for result in self.results.values()
                       if result.peak_memory_mb > 0]
        avg_memory_usage = sum(memory_usage) / len(memory_usage) if memory_usage else 0
        
        return {
            'total_steps': total_steps,
            'successful_steps': successful_steps,
            'success_rate': (successful_steps / total_steps * 100) if total_steps > 0 else 0,
            'real_ai_steps': real_ai_steps,
            'real_ai_rate': (real_ai_steps / total_steps * 100) if total_steps > 0 else 0,
            'mock_detected_steps': mock_detected_steps,
            'critical_steps_success': critical_success,
            'critical_steps_total': len(critical_steps),
            'avg_inference_time': avg_inference_time,
            'avg_memory_usage': avg_memory_usage,
            'system_ready': successful_steps >= 6 and real_ai_steps >= 4,
            'pipeline_validated': successful_steps == total_steps and mock_detected_steps == 0
        }
    
    def _identify_critical_issues(self) -> List[str]:
        """ì¤‘ìš” ë¬¸ì œì  ì‹ë³„"""
        issues = []
        
        # ì‹œìŠ¤í…œ ìˆ˜ì¤€ ë¬¸ì œ
        if not self.validator.torch_available:
            issues.append("ğŸ”¥ CRITICAL: PyTorch ì—†ìŒ - AI ì¶”ë¡  ë¶ˆê°€ëŠ¥")
        
        if self.system_metrics.get('available_memory_gb', 0) < 4:
            issues.append("ğŸ”¥ CRITICAL: ë©”ëª¨ë¦¬ ë¶€ì¡± - ëŒ€ìš©ëŸ‰ ëª¨ë¸ ë¡œë”© ë¶ˆê°€")
        
        # Stepë³„ ë¬¸ì œ
        failed_steps = []
        mock_steps = []
        
        for step_name, result in self.results.items():
            if result.status != RealInferenceStatus.SUCCESS:
                failed_steps.append(f"{step_name}({result.status.value})")
            
            if result.mock_detected or result.fallback_used:
                mock_steps.append(step_name)
        
        if failed_steps:
            issues.append(f"âŒ FAILED STEPS: {', '.join(failed_steps)}")
        
        if mock_steps:
            issues.append(f"âš ï¸ MOCK/FALLBACK DETECTED: {', '.join(mock_steps)}")
        
        # Critical Step í™•ì¸
        critical_steps = ['HumanParsingStep', 'VirtualFittingStep', 'ClothSegmentationStep']
        failed_critical = []
        
        for step_name in critical_steps:
            if step_name in self.results:
                result = self.results[step_name] 
                if result.status != RealInferenceStatus.SUCCESS or result.mock_detected:
                    failed_critical.append(step_name)
        
        if failed_critical:
            issues.append(f"ğŸ”¥ CRITICAL STEPS FAILED: {', '.join(failed_critical)}")
        
        return issues
    
    def _calculate_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        
        # ì¶”ë¡  ì‹œê°„ í†µê³„
        inference_times = [result.total_inference_time for result in self.results.values()
                          if result.total_inference_time > 0]
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„  
        memory_usage = [result.peak_memory_mb for result in self.results.values()
                       if result.peak_memory_mb > 0]
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° í†µê³„
        model_params = [result.model_parameters_count for result in self.results.values()
                       if result.model_parameters_count > 0]
        
        return {
            'inference_time': {
                'min': min(inference_times) if inference_times else 0,
                'max': max(inference_times) if inference_times else 0,
                'avg': sum(inference_times) / len(inference_times) if inference_times else 0,
                'total': sum(inference_times) if inference_times else 0
            },
            'memory_usage': {
                'min_mb': min(memory_usage) if memory_usage else 0,
                'max_mb': max(memory_usage) if memory_usage else 0,
                'avg_mb': sum(memory_usage) / len(memory_usage) if memory_usage else 0,
                'total_mb': sum(memory_usage) if memory_usage else 0
            },
            'model_complexity': {
                'total_parameters': sum(model_params) if model_params else 0,
                'avg_parameters_per_step': sum(model_params) / len(model_params) if model_params else 0,
                'loaded_models_count': len([r for r in self.results.values() if r.model_loading_success])
            },
            'device_utilization': {
                'mps_optimized_steps': len([r for r in self.results.values() if r.mps_optimized]),
                'device_compatible_steps': len([r for r in self.results.values() if r.device_compatible]),
                'tensor_operations_working': len([r for r in self.results.values() if r.tensor_operations_working])
            }
        }
    
    def _generate_recommendations(self) -> List[str]:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì‹œìŠ¤í…œ ê°œì„ 
        if not self.validator.torch_available:
            recommendations.append("ğŸ“¦ PyTorch ì„¤ì¹˜: pip install torch torchvision")
            
        if self.system_metrics.get('available_memory_gb', 0) < 8:
            recommendations.append("ğŸ’¾ ë©”ëª¨ë¦¬ ì¦ì„¤ ë˜ëŠ” ë©”ëª¨ë¦¬ ì •ë¦¬ í•„ìš”")
        
        # Stepë³„ ê°œì„ ì‚¬í•­
        for step_name, result in self.results.items():
            if result.status == RealInferenceStatus.MODEL_LOADING_FAILED:
                recommendations.append(f"ğŸ”§ {step_name}: ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ê²½ë¡œ í™•ì¸")
                
            elif result.status == RealInferenceStatus.CHECKPOINT_MISSING:
                recommendations.append(f"ğŸ“ {step_name}: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ëˆ„ë½ - ë‹¤ìš´ë¡œë“œ í•„ìš”")
                
            elif result.mock_detected:
                recommendations.append(f"âš ï¸ {step_name}: Mock ëª¨ë“œ ê°ì§€ - ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© í™•ì¸")
                
            elif result.fallback_used:
                recommendations.append(f"ğŸ”„ {step_name}: í´ë°± ëª¨ë“œ - ë©”ì¸ ëª¨ë¸ ë¡œë”© ë¬¸ì œ í•´ê²°")
        
        # ì„±ëŠ¥ ìµœì í™”
        slow_steps = [name for name, result in self.results.items() 
                     if result.total_inference_time > 10.0]
        if slow_steps:
            recommendations.append(f"âš¡ ì„±ëŠ¥ ìµœì í™” í•„ìš”: {', '.join(slow_steps)}")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        high_memory_steps = [name for name, result in self.results.items()
                           if result.peak_memory_mb > 1000]
        if high_memory_steps:
            recommendations.append(f"ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” í•„ìš”: {', '.join(high_memory_steps)}")
            
        # MPS ìµœì í™” (M3 Maxì¸ ê²½ìš°)
        if self.system_metrics.get('is_m3_max', False):
            non_mps_steps = [name for name, result in self.results.items()
                           if not result.mps_optimized and result.device_used != 'mps']
            if non_mps_steps:
                recommendations.append(f"ğŸ M3 Max MPS ìµœì í™” í•„ìš”: {', '.join(non_mps_steps)}")
        
        return recommendations
    
    def _print_validation_results(self, validation_report: Dict[str, Any]):
        """ê²€ì¦ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“Š Real AI Inference Validation Results v7.0")
        print("=" * 80)
        
        # ì „ì²´ ìš”ì•½
        summary = validation_report['system_summary']
        print(f"\nğŸ¯ ì „ì²´ ìš”ì•½:")
        print(f"   Step ì„±ê³µë¥ : {summary['success_rate']:.1f}% ({summary['successful_steps']}/{summary['total_steps']})")
        print(f"   ì‹¤ì œ AI ì‚¬ìš©ë¥ : {summary['real_ai_rate']:.1f}% ({summary['real_ai_steps']}/{summary['total_steps']})")
        print(f"   Mock ê°ì§€: {summary['mock_detected_steps']}ê°œ Step")
        print(f"   Critical Step ì„±ê³µ: {summary['critical_steps_success']}/{summary['critical_steps_total']}")
        print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {summary['avg_inference_time']:.3f}ì´ˆ")
        print(f"   í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©: {summary['avg_memory_usage']:.1f}MB")
        print(f"   ì‹œìŠ¤í…œ ì¤€ë¹„: {'âœ…' if summary['system_ready'] else 'âŒ'}")
        print(f"   íŒŒì´í”„ë¼ì¸ ê²€ì¦: {'âœ…' if summary['pipeline_validated'] else 'âŒ'}")
        
        # Stepë³„ ìƒì„¸ ê²°ê³¼
        print(f"\nğŸš€ Stepë³„ ì‹¤ì œ ì¶”ë¡  ê²€ì¦ ê²°ê³¼:")
        
        for step_name, result in self.results.items():
            status_icon = "âœ…" if result.status == RealInferenceStatus.SUCCESS else "âŒ"
            real_ai_icon = "ğŸ§ " if result.real_ai_model_used and not result.mock_detected else "ğŸ­"
            
            print(f"   {status_icon} {real_ai_icon} Step {result.step_id}: {step_name}")
            print(f"      ìƒíƒœ: {result.status.value}")
            print(f"      ëª¨ë¸ ë¡œë”©: {'âœ…' if result.model_loading_success else 'âŒ'}")
            print(f"      ì‹¤ì œ ì¶”ë¡ : {'âœ…' if result.inference_success else 'âŒ'}")
            print(f"      ì‹¤ì œ AI ì‚¬ìš©: {'âœ…' if result.real_ai_model_used else 'âŒ'}")
            print(f"      Mock ê°ì§€: {'âŒ' if result.mock_detected else 'âœ…'}")
            print(f"      ì¶”ë¡  ì‹œê°„: {result.total_inference_time:.3f}ì´ˆ")
            print(f"      ë©”ëª¨ë¦¬: {result.peak_memory_mb:.1f}MB ({result.memory_efficiency})")
            print(f"      ë””ë°”ì´ìŠ¤: {result.device_used}")
            
            if result.errors:
                print(f"      âŒ ì˜¤ë¥˜: {result.errors[0]}")
            if result.warnings:
                print(f"      âš ï¸ ê²½ê³ : {result.warnings[0]}")
        
        # ì¤‘ìš” ë¬¸ì œì 
        if validation_report['critical_issues']:
            print(f"\nğŸ”¥ ì¤‘ìš” ë¬¸ì œì :")
            for issue in validation_report['critical_issues']:
                print(f"   {issue}")
        
        # ì¶”ì²œì‚¬í•­
        if validation_report['recommendations']:
            print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
            for i, rec in enumerate(validation_report['recommendations'], 1):
                print(f"   {i}. {rec}")
        
        # ì„±ëŠ¥ ì§€í‘œ
        metrics = validation_report['performance_metrics']
        print(f"\nğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:")
        print(f"   ì¶”ë¡  ì‹œê°„: í‰ê·  {metrics['inference_time']['avg']:.3f}ì´ˆ (ìµœëŒ€: {metrics['inference_time']['max']:.3f}ì´ˆ)")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©: í‰ê·  {metrics['memory_usage']['avg_mb']:.1f}MB (ìµœëŒ€: {metrics['memory_usage']['max_mb']:.1f}MB)")
        print(f"   ì´ ëª¨ë¸ íŒŒë¼ë¯¸í„°: {metrics['model_complexity']['total_parameters']:,}")
        print(f"   MPS ìµœì í™”: {metrics['device_utilization']['mps_optimized_steps']}ê°œ Step")
    
    def _save_validation_results(self, validation_report: Dict[str, Any]):
        """ê²€ì¦ ê²°ê³¼ ì €ì¥"""
        try:
            timestamp = int(time.time())
            
            # JSON ê²°ê³¼ ì €ì¥
            results_file = Path(f"real_ai_inference_validation_v7_{timestamp}.json")
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(validation_report, f, indent=2, ensure_ascii=False, default=str)
            
            # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
            summary_file = Path(f"real_ai_inference_summary_v7_{timestamp}.md")
            self._save_summary_report(summary_file, validation_report)
            
            print(f"\nğŸ“„ ìƒì„¸ ê²°ê³¼: {results_file}")
            print(f"ğŸ“„ ìš”ì•½ ë¦¬í¬íŠ¸: {summary_file}")
            
        except Exception as e:
            print(f"\nâš ï¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_summary_report(self, file_path: Path, validation_report: Dict[str, Any]):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write("# ğŸ”¥ Real AI Inference Validation Report v7.0\n\n")
                f.write(f"**ìƒì„± ì‹œê°„**: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\n")
                f.write(f"**ê²€ì¦ ëŒ€ìƒ**: MyCloset AI Pipeline (8ë‹¨ê³„)\n")
                f.write(f"**ê²€ì¦ ì†Œìš” ì‹œê°„**: {validation_report['total_validation_time']:.1f}ì´ˆ\n\n")
                
                # ì‹œìŠ¤í…œ í™˜ê²½
                f.write("## ğŸ–¥ï¸ ì‹œìŠ¤í…œ í™˜ê²½\n\n")
                f.write(f"- **CPU**: {self.system_metrics.get('cpu_count', 0)}ì½”ì–´\n")
                f.write(f"- **ë©”ëª¨ë¦¬**: {self.system_metrics.get('available_memory_gb', 0):.1f}GB ì‚¬ìš©ê°€ëŠ¥ / {self.system_metrics.get('total_memory_gb', 0):.1f}GB ì „ì²´\n")
                f.write(f"- **M3 Max**: {'âœ…' if self.system_metrics.get('is_m3_max', False) else 'âŒ'}\n")
                f.write(f"- **PyTorch**: {'âœ…' if self.system_metrics.get('torch_available', False) else 'âŒ'}\n")
                f.write(f"- **AI ëª¨ë¸**: {self.system_metrics.get('ai_models_size_gb', 0):.1f}GB\n\n")
                
                # ì „ì²´ ìš”ì•½
                summary = validation_report['system_summary']
                f.write("## ğŸ¯ ê²€ì¦ ê²°ê³¼ ìš”ì•½\n\n")
                f.write(f"- **Step ì„±ê³µë¥ **: {summary['success_rate']:.1f}% ({summary['successful_steps']}/{summary['total_steps']})\n")
                f.write(f"- **ì‹¤ì œ AI ì‚¬ìš©ë¥ **: {summary['real_ai_rate']:.1f}% ({summary['real_ai_steps']}/{summary['total_steps']})\n")
                f.write(f"- **Mock ê°ì§€**: {summary['mock_detected_steps']}ê°œ Step\n")
                f.write(f"- **Critical Step ì„±ê³µ**: {summary['critical_steps_success']}/{summary['critical_steps_total']}\n")
                f.write(f"- **í‰ê·  ì¶”ë¡  ì‹œê°„**: {summary['avg_inference_time']:.3f}ì´ˆ\n")
                f.write(f"- **í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©**: {summary['avg_memory_usage']:.1f}MB\n")
                f.write(f"- **ì‹œìŠ¤í…œ ì¤€ë¹„**: {'ì¤€ë¹„ë¨' if summary['system_ready'] else 'ë¬¸ì œìˆìŒ'}\n")
                f.write(f"- **íŒŒì´í”„ë¼ì¸ ê²€ì¦**: {'ì™„ì „ ê²€ì¦ë¨' if summary['pipeline_validated'] else 'ë¶€ë¶„ ê²€ì¦ë¨'}\n\n")
                
                # ì¤‘ìš” ë¬¸ì œì 
                if validation_report['critical_issues']:
                    f.write("## ğŸ”¥ ì¤‘ìš” ë¬¸ì œì \n\n")
                    for issue in validation_report['critical_issues']:
                        f.write(f"- {issue}\n")
                    f.write("\n")
                
                # ì¶”ì²œì‚¬í•­
                if validation_report['recommendations']:
                    f.write("## ğŸ’¡ ì¶”ì²œì‚¬í•­\n\n")
                    for i, rec in enumerate(validation_report['recommendations'], 1):
                        f.write(f"{i}. {rec}\n")
                    f.write("\n")
                
                # Stepë³„ ìƒì„¸ ì •ë³´
                f.write("## ğŸš€ Stepë³„ ê²€ì¦ ê²°ê³¼\n\n")
                for step_name, result in self.results.items():
                    f.write(f"### Step {result.step_id}: {step_name}\n\n")
                    f.write(f"- **ìƒíƒœ**: {result.status.value}\n")
                    f.write(f"- **ì‹¤ì œ AI ì‚¬ìš©**: {'âœ…' if result.real_ai_model_used else 'âŒ'}\n")
                    f.write(f"- **Mock ê°ì§€**: {'âŒ' if result.mock_detected else 'âœ…'}\n")
                    f.write(f"- **ì¶”ë¡  ì„±ê³µ**: {'âœ…' if result.inference_success else 'âŒ'}\n")
                    f.write(f"- **ì¶”ë¡  ì‹œê°„**: {result.total_inference_time:.3f}ì´ˆ\n")
                    f.write(f"- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: {result.peak_memory_mb:.1f}MB ({result.memory_efficiency})\n")
                    f.write(f"- **ë””ë°”ì´ìŠ¤**: {result.device_used}\n")
                    f.write(f"- **ì‹ ë¢°ë„**: {result.confidence_score:.3f}\n")
                    
                    if result.model_parameters_count > 0:
                        f.write(f"- **ëª¨ë¸ íŒŒë¼ë¯¸í„°**: {result.model_parameters_count:,}ê°œ\n")
                    
                    if result.errors:
                        f.write(f"- **ì˜¤ë¥˜**: {result.errors[0]}\n")
                    
                    f.write("\n")
                
        except Exception as e:
            print(f"ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 4. ë¹ ë¥¸ ê²€ì¦ ë„êµ¬ë“¤
# =============================================================================

def quick_real_inference_check(step_name: str) -> bool:
    """ë¹ ë¥¸ ì‹¤ì œ ì¶”ë¡  í™•ì¸"""
    try:
        validator = RealAIInferenceValidator()
        
        # Step ì„¤ì • ì°¾ê¸°
        step_config = None
        for config in validator.github_steps:
            if config['step_name'] == step_name:
                step_config = config
                break
        
        if not step_config:
            return False
        
        # ë¹ ë¥¸ ê²€ì¦
        result = validator.validate_real_inference_for_step(step_config)
        
        return (result.status == RealInferenceStatus.SUCCESS and 
                result.real_ai_model_used and 
                not result.mock_detected)
        
    except Exception:
        return False

def get_ai_pipeline_readiness_score() -> float:
    """AI íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ë„ ì ìˆ˜ (0-100)"""
    try:
        validator = RealAIInferenceValidator()
        
        score = 0.0
        total_weight = 100.0
        
        # PyTorch í™˜ê²½ (20ì )
        if validator.torch_available:
            score += 20
        
        # ë©”ëª¨ë¦¬ (15ì )
        memory_info = psutil.virtual_memory()
        available_gb = memory_info.available / (1024**3)
        if available_gb >= 16:
            score += 15
        elif available_gb >= 8:
            score += 10
        elif available_gb >= 4:
            score += 5
        
        # AI ëª¨ë¸ í¬ê¸° (20ì )
        if ai_models_root.exists():
            total_size = sum(f.stat().st_size for f in ai_models_root.rglob('*') if f.is_file())
            total_size_gb = total_size / (1024**3)
            
            if total_size_gb >= 200:  # 229GB ëª©í‘œ
                score += 20
            elif total_size_gb >= 100:
                score += 15
            elif total_size_gb >= 50:
                score += 10
            elif total_size_gb >= 10:
                score += 5
        
        # Critical Step í…ŒìŠ¤íŠ¸ (45ì )
        critical_steps = ['HumanParsingStep', 'VirtualFittingStep', 'ClothSegmentationStep']
        critical_weight = 45 / len(critical_steps)
        
        for step_name in critical_steps:
            if quick_real_inference_check(step_name):
                score += critical_weight
        
        return min(100.0, score)
        
    except Exception:
        return 0.0

def run_critical_steps_validation() -> Dict[str, Any]:
    """Critical Stepë§Œ ë¹ ë¥¸ ê²€ì¦"""
    try:
        print("ğŸ”¥ Critical Steps ì‹¤ì œ ì¶”ë¡  ë¹ ë¥¸ ê²€ì¦...")
        
        validator = RealAIInferenceValidator()
        critical_steps = ['HumanParsingStep', 'VirtualFittingStep', 'ClothSegmentationStep', 'PoseEstimationStep']
        
        results = {}
        for step_name in critical_steps:
            print(f"   ğŸ” {step_name} ê²€ì¦ ì¤‘...")
            
            # Step ì„¤ì • ì°¾ê¸°
            step_config = None
            for config in validator.github_steps:
                if config['step_name'] == step_name:
                    step_config = config
                    break
            
            if step_config:
                try:
                    result = validator.validate_real_inference_for_step(step_config)
                    results[step_name] = {
                        'success': result.status == RealInferenceStatus.SUCCESS,
                        'real_ai_used': result.real_ai_model_used,
                        'mock_detected': result.mock_detected,
                        'inference_time': result.total_inference_time,
                        'status': result.status.value
                    }
                    
                    status = "âœ…" if result.status == RealInferenceStatus.SUCCESS and result.real_ai_model_used else "âŒ"
                    print(f"      {status} {step_name}: {result.status.value}")
                    
                except Exception as e:
                    results[step_name] = {'error': str(e)}
                    print(f"      âŒ {step_name}: {str(e)[:50]}")
            else:
                results[step_name] = {'error': 'Step ì„¤ì • ì—†ìŒ'}
                print(f"      âŒ {step_name}: Step ì„¤ì • ì—†ìŒ")
        
        # ìš”ì•½
        successful = sum(1 for r in results.values() if r.get('success', False) and r.get('real_ai_used', False))
        total = len(critical_steps)
        
        summary = {
            'critical_steps_validated': f"{successful}/{total}",
            'success_rate': (successful / total * 100) if total > 0 else 0,
            'all_critical_ready': successful == total,
            'results': results
        }
        
        print(f"   ğŸ¯ Critical Steps ê²€ì¦ ì™„ë£Œ: {successful}/{total} ì„±ê³µ")
        
        return summary
        
    except Exception as e:
        print(f"âŒ Critical Steps ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {'error': str(e)}

# =============================================================================
# ğŸ”¥ 5. ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print(f"ğŸ”¥ Real AI Inference Validator v7.0")
    print(f"ğŸ”¥ Target: Mock/í´ë°± ì—†ëŠ” 100% ì‹¤ì œ AI ì¶”ë¡  ê²€ì¦")
    print(f"ğŸ”¥ ì‹œì‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # ë¹ ë¥¸ ì¤€ë¹„ë„ ì²´í¬
        print("\nğŸ” AI íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ë„ ì²´í¬...")
        readiness_score = get_ai_pipeline_readiness_score()
        print(f"   ì¤€ë¹„ë„ ì ìˆ˜: {readiness_score:.1f}/100")
        
        if readiness_score < 30:
            print(f"\nâš ï¸ ì‹œìŠ¤í…œ ì¤€ë¹„ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. Critical Stepsë§Œ ê²€ì¦í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
            response = input("Critical Stepsë§Œ ê²€ì¦: 'c', ì „ì²´ ê²€ì¦: 'f', ì¢…ë£Œ: Enter : ").lower().strip()
            
            if response == 'c':
                # Critical Stepsë§Œ ê²€ì¦
                critical_results = run_critical_steps_validation()
                
                if critical_results.get('all_critical_ready', False):
                    print(f"\nğŸ‰ SUCCESS: Critical Steps ëª¨ë‘ ì‹¤ì œ AI ì¶”ë¡  ê²€ì¦ ì™„ë£Œ!")
                else:
                    print(f"\nâš ï¸ WARNING: ì¼ë¶€ Critical Stepsì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
                
                return critical_results
            
            elif response != 'f':
                print("ê²€ì¦ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return None
        
        # ì „ì²´ ê²€ì¦ ì‹¤í–‰
        system_validator = RealAISystemValidator()
        validation_report = system_validator.validate_entire_ai_pipeline()
        
        # ìµœì¢… íŒì •
        summary = validation_report.get('system_summary', {})
        pipeline_validated = summary.get('pipeline_validated', False)
        system_ready = summary.get('system_ready', False)
        real_ai_rate = summary.get('real_ai_rate', 0)
        
        if pipeline_validated and real_ai_rate >= 80:
            print(f"\nğŸ‰ SUCCESS: AI íŒŒì´í”„ë¼ì¸ ì‹¤ì œ ì¶”ë¡  ì™„ì „ ê²€ì¦ ì™„ë£Œ!")
            print(f"   - 8ë‹¨ê³„ ëª¨ë“  Step ì‹¤ì œ AI ì¶”ë¡  ê²€ì¦")
            print(f"   - Mock/í´ë°± ì‚¬ìš© ì—†ìŒ í™•ì¸")
            print(f"   - ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë° ì¶”ë¡  ì •ìƒ ì‘ë™")
            print(f"   - ì‹¤ì œ AI ì‚¬ìš©ë¥ : {real_ai_rate:.1f}%")
        elif system_ready and real_ai_rate >= 60:
            print(f"\nâœ… GOOD: AI íŒŒì´í”„ë¼ì¸ ëŒ€ë¶€ë¶„ ì •ìƒ ì‘ë™")
            print(f"   - ì‹¤ì œ AI ì‚¬ìš©ë¥ : {real_ai_rate:.1f}%")
            print(f"   - ì¼ë¶€ ê°œì„ ì‚¬í•­ í™•ì¸ í•„ìš”")
        else:
            print(f"\nâš ï¸ WARNING: AI íŒŒì´í”„ë¼ì¸ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            print(f"   - ì‹¤ì œ AI ì‚¬ìš©ë¥ : {real_ai_rate:.1f}%")
            print(f"   - ìƒì„¸ ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        return validation_report
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        print(f"\nâŒ ê²€ì¦ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        print(f"ì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        return None
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        gc.collect()
        print(f"\nğŸ‘‹ Real AI Inference Validator v7.0 ì¢…ë£Œ")

if __name__ == "__main__":
    main()