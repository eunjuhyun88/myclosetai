#!/usr/bin/env python3
"""
ğŸ”¥ Enhanced ModelLoader v2.0 â†’ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ + auto_detector í†µí•©
================================================================================

âœ… ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ + auto_detector í†µí•©
âœ… ì§€ì—° ë¡œë”© (Lazy Loading)
âœ… ìŠ¤ë§ˆíŠ¸ ìºì‹± ì‹œìŠ¤í…œ
âœ… ê¸°ì¡´ API 100% í˜¸í™˜ì„±
âœ… Step íŒŒì¼ë“¤ê³¼ ì™„ì „ í˜¸í™˜

Author: MyCloset AI Team
Date: 2024-08-09
Version: 2.0
"""

import os
import sys
import gc
import time
import json
import logging
import asyncio
import threading
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Type, Set, Callable
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache, wraps
from collections import defaultdict, OrderedDict

# PyTorch imports
import torch
import torch.nn as nn

# ê¸°ì¡´ ëª¨ë¸ ë¡œë” í˜¸í™˜ì„±ì„ ìœ„í•œ imports
try:
    from app.ai_pipeline.utils.model_loader import (
        RealStepModelType, RealModelStatus, RealModelPriority,
        RealStepModelInfo, RealStepModelRequirement, RealAIModel,
        RealStepModelInterface, ModelLoader as BaseModelLoader
    )
except ImportError:
    # Fallback for standalone usage
    class RealStepModelType(Enum):
        HUMAN_PARSING = "human_parsing"
        POSE_ESTIMATION = "pose_estimation"
        CLOTH_SEGMENTATION = "cloth_segmentation"
        GEOMETRIC_MATCHING = "geometric_matching"
        CLOTH_WARPING = "cloth_warping"
        VIRTUAL_FITTING = "virtual_fitting"
        POST_PROCESSING = "post_processing"
        QUALITY_ASSESSMENT = "quality_assessment"

    class RealModelStatus(Enum):
        NOT_LOADED = "not_loaded"
        LOADING = "loading"
        LOADED = "loaded"
        ERROR = "error"
        VALIDATING = "validating"

    class RealModelPriority(Enum):
        PRIMARY = 1
        SECONDARY = 2
        FALLBACK = 3
        OPTIONAL = 4

    @dataclass
    class RealStepModelInfo:
        name: str
        path: str
        step_type: RealStepModelType
        priority: RealModelPriority
        device: str
        memory_mb: float = 0.0
        loaded: bool = False
        load_time: float = 0.0
        checkpoint_data: Optional[Any] = None
        model_type: str = "BaseModel"
        size_gb: float = 0.0
        requires_checkpoint: bool = True
        error: Optional[str] = None
        validation_passed: bool = False

    @dataclass
    class RealStepModelRequirement:
        step_name: str
        step_id: int
        step_type: RealStepModelType
        required_models: List[str] = field(default_factory=list)
        optional_models: List[str] = field(default_factory=list)
        primary_model: Optional[str] = None
        model_configs: Dict[str, Any] = field(default_factory=dict)
        batch_size: int = 1
        precision: str = "fp32"
        memory_limit_mb: Optional[float] = None

    class RealAIModel:
        def __init__(self, model_name: str, model_path: str, step_type: RealStepModelType, device: str = "auto"):
            self.model_name = model_name
            self.model_path = model_path
            self.step_type = step_type
            self.device = device
            self.model_instance = None
            self.checkpoint_data = None
            self.loaded = False
            self.load_time = 0.0
            self.error = None

        def load(self, validate: bool = True) -> bool:
            return True

        def get_model_instance(self) -> Optional[Any]:
            return self.model_instance

        def unload(self):
            self.model_instance = None
            self.checkpoint_data = None
            self.loaded = False

    class RealStepModelInterface:
        def __init__(self, model_loader, step_name: str, step_type: RealStepModelType):
            self.model_loader = model_loader
            self.step_name = step_name
            self.step_type = step_type

        def get_model(self, model_name: Optional[str] = None) -> Optional[RealAIModel]:
            return None

        def register_requirements(self, requirements: Dict[str, Any]):
            pass

# auto_model_detector í†µí•©
try:
    from app.ai_pipeline.utils.auto_model_detector import (
        get_global_detector, OptimizedModelDetector, OptimizedDetectedModel,
        quick_model_detection, detect_ultra_large_models, find_model_by_name
    )
    AUTO_DETECTOR_AVAILABLE = True
except ImportError:
    AUTO_DETECTOR_AVAILABLE = False
    get_global_detector = None
    OptimizedModelDetector = None
    OptimizedDetectedModel = None
    quick_model_detection = None
    detect_ultra_large_models = None
    find_model_by_name = None

# ==============================================
# ğŸ”¥ í†µí•© ë°ì´í„° êµ¬ì¡°
# ==============================================

@dataclass
class IntegratedModelInfo:
    """í†µí•©ëœ ëª¨ë¸ ì •ë³´ (ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ + auto_detector)"""
    name: str
    path: str
    step_type: RealStepModelType
    file_size_mb: float
    file_size_gb: float
    
    # auto_detector ì •ë³´
    auto_detector_info: Optional[Dict[str, Any]] = None
    
    # í†µí•©ëœ ë©”íƒ€ë°ì´í„°
    ai_class: str = "BaseRealAIModel"
    confidence_score: float = 0.0
    priority_score: float = 0.0
    is_valid: bool = True
    error: Optional[str] = None
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ í†µí•© ì •ë³´ ê³„ì‚°"""
        self.file_size_gb = self.file_size_mb / 1024
        
        # auto_detector ì •ë³´ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if self.auto_detector_info:
            self.ai_class = self.auto_detector_info.get('ai_model_info', {}).get('ai_class', self.ai_class)
            self.confidence_score = self.auto_detector_info.get('confidence', self.confidence_score)
            self.priority_score = self.auto_detector_info.get('priority_info', {}).get('priority_score', self.priority_score)

class ModelCache:
    """ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_size: int = 10, max_memory_gb: float = 8.0):
        self.max_size = max_size
        self.max_memory_gb = max_memory_gb
        self.cache: OrderedDict[str, RealAIModel] = OrderedDict()
        self.access_count: Dict[str, int] = defaultdict(int)
        self.last_access: Dict[str, float] = {}
        self.memory_usage_gb = 0.0
        self._lock = threading.RLock()
    
    def get(self, model_name: str) -> Optional[RealAIModel]:
        """ìºì‹œì—ì„œ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        with self._lock:
            if model_name in self.cache:
                # LRU ì—…ë°ì´íŠ¸
                model = self.cache.pop(model_name)
                self.cache[model_name] = model
                
                # ì ‘ê·¼ í†µê³„ ì—…ë°ì´íŠ¸
                self.access_count[model_name] += 1
                self.last_access[model_name] = time.time()
                
                return model
            return None
    
    def put(self, model_name: str, model: RealAIModel) -> bool:
        """ìºì‹œì— ëª¨ë¸ ì¶”ê°€"""
        with self._lock:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚° (ì¶”ì •)
            model_size_gb = getattr(model, 'size_gb', 0.5)  # ê¸°ë³¸ê°’ 0.5GB
            
            # ìºì‹œ í¬ê¸° ì œí•œ í™•ì¸
            if len(self.cache) >= self.max_size:
                self._evict_least_used()
            
            # ë©”ëª¨ë¦¬ ì œí•œ í™•ì¸
            if self.memory_usage_gb + model_size_gb > self.max_memory_gb:
                self._evict_by_memory()
            
            # ëª¨ë¸ ì¶”ê°€
            self.cache[model_name] = model
            self.access_count[model_name] = 1
            self.last_access[model_name] = time.time()
            self.memory_usage_gb += model_size_gb
            
            return True
    
    def _evict_least_used(self):
        """ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ ëª¨ë¸ ì œê±°"""
        if not self.cache:
            return
        
        # ì ‘ê·¼ íšŸìˆ˜ì™€ ë§ˆì§€ë§‰ ì ‘ê·¼ ì‹œê°„ì„ ê³ ë ¤í•œ ì ìˆ˜ ê³„ì‚°
        scores = {}
        current_time = time.time()
        
        for name in self.cache.keys():
            access_count = self.access_count.get(name, 0)
            last_access = self.last_access.get(name, 0)
            time_factor = max(1, (current_time - last_access) / 3600)  # ì‹œê°„ë‹¹ ê°ì†Œ
            scores[name] = access_count / time_factor
        
        # ê°€ì¥ ë‚®ì€ ì ìˆ˜ì˜ ëª¨ë¸ ì œê±°
        least_used = min(scores.keys(), key=lambda x: scores[x])
        self._remove_model(least_used)
    
    def _evict_by_memory(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ì œê±°"""
        if not self.cache:
            return
        
        # ê°€ì¥ í° ëª¨ë¸ë¶€í„° ì œê±°
        model_sizes = {}
        for name, model in self.cache.items():
            model_sizes[name] = getattr(model, 'size_gb', 0.5)
        
        largest_model = max(model_sizes.keys(), key=lambda x: model_sizes[x])
        self._remove_model(largest_model)
    
    def _remove_model(self, model_name: str):
        """ëª¨ë¸ ì œê±°"""
        if model_name in self.cache:
            model = self.cache.pop(model_name)
            model_size_gb = getattr(model, 'size_gb', 0.5)
            self.memory_usage_gb -= model_size_gb
            
            # ëª¨ë¸ ì–¸ë¡œë“œ
            if hasattr(model, 'unload'):
                model.unload()
            
            # í†µê³„ ì •ë¦¬
            self.access_count.pop(model_name, None)
            self.last_access.pop(model_name, None)
    
    def clear(self):
        """ìºì‹œ ì „ì²´ ì •ë¦¬"""
        with self._lock:
            for model_name in list(self.cache.keys()):
                self._remove_model(model_name)
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        with self._lock:
            return {
                'cache_size': len(self.cache),
                'max_size': self.max_size,
                'memory_usage_gb': self.memory_usage_gb,
                'max_memory_gb': self.max_memory_gb,
                'cached_models': list(self.cache.keys()),
                'access_counts': dict(self.access_count),
                'last_access': dict(self.last_access)
            }

# ==============================================
# ğŸ”¥ ê°œì„ ëœ ModelLoader í´ë˜ìŠ¤
# ==============================================

class EnhancedModelLoader:
    """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ + auto_detector í†µí•© ê¸°ë°˜ ê°œì„ ëœ ëª¨ë¸ ë¡œë”"""
    
    def __init__(self, 
                 device: str = "auto",
                 model_cache_dir: Optional[str] = None,
                 max_cached_models: int = 10,
                 max_memory_gb: float = 8.0,
                 enable_auto_detector: bool = True,
                 **kwargs):
        
        # ë¡œê±° ì„¤ì •
        self.logger = logging.getLogger(f"{__name__}.EnhancedModelLoader")
        
        # ê¸°ë³¸ ì„¤ì •
        self.device = self._setup_device(device)
        self.model_cache_dir = Path(model_cache_dir) if model_cache_dir else Path("ai_models")
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.cache = ModelCache(max_cached_models, max_memory_gb)
        
        # auto_detector í†µí•©
        self.enable_auto_detector = enable_auto_detector and AUTO_DETECTOR_AVAILABLE
        self.auto_detector = None
        if self.enable_auto_detector:
            try:
                self.auto_detector = get_global_detector()
                self.logger.info("âœ… auto_detector í†µí•© ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ auto_detector í†µí•© ì‹¤íŒ¨: {e}")
                self.enable_auto_detector = False
        
        # ëª¨ë¸ ê´€ë¦¬
        self.loaded_models: Dict[str, RealAIModel] = {}
        self.model_info: Dict[str, RealStepModelInfo] = {}
        self.model_status: Dict[str, RealModelStatus] = {}
        self.integrated_model_info: Dict[str, IntegratedModelInfo] = {}
        
        # Step ìš”êµ¬ì‚¬í•­
        self.step_requirements: Dict[str, RealStepModelRequirement] = {}
        self.step_interfaces: Dict[str, RealStepModelInterface] = {}
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_metrics = {
            'models_loaded': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'total_memory_mb': 0.0,
            'error_count': 0,
            'total_load_time': 0.0,
            'auto_detector_hits': 0
        }
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        self._executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="EnhancedModelLoader")
        
        # ë°ì´í„° ë¡œë“œ
        self._load_checkpoint_analysis()
        self._load_auto_detector_data()
        self._integrate_model_data()
        
        self.logger.info(f"ğŸš€ EnhancedModelLoader v2.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ“± Device: {self.device}")
        self.logger.info(f"ğŸ’¾ Cache: {max_cached_models} models, {max_memory_gb}GB")
        self.logger.info(f"ğŸ” Auto Detector: {'Enabled' if self.enable_auto_detector else 'Disabled'}")
    
    def _setup_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return device
    
    def _load_checkpoint_analysis(self):
        """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
        try:
            analysis_file = Path("comprehensive_checkpoint_analysis.json")
            if analysis_file.exists():
                with open(analysis_file, 'r', encoding='utf-8') as f:
                    analysis_data = json.load(f)
                
                self.checkpoint_analysis = analysis_data
                self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê²°ê³¼ ë¡œë“œ: {len(analysis_data.get('checkpoints', {}))}ê°œ")
            else:
                self.checkpoint_analysis = {}
                self.logger.warning("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.checkpoint_analysis = {}
    
    def _load_auto_detector_data(self):
        """auto_detector ë°ì´í„° ë¡œë“œ"""
        if not self.enable_auto_detector or not self.auto_detector:
            self.auto_detector_data = {}
            return
        
        try:
            # auto_detectorì—ì„œ ëª¨ë“  ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            detected_models = self.auto_detector.detect_all_models()
            self.auto_detector_data = {
                name: model.to_dict() for name, model in detected_models.items()
            }
            self.logger.info(f"âœ… auto_detector ë°ì´í„° ë¡œë“œ: {len(self.auto_detector_data)}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ auto_detector ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.auto_detector_data = {}
    
    def _integrate_model_data(self):
        """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ê³¼ auto_detector ë°ì´í„° í†µí•©"""
        try:
            self.logger.info("ğŸ”„ ëª¨ë¸ ë°ì´í„° í†µí•© ì‹œì‘...")
            
            # auto_detector ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í†µí•©
            for model_name, auto_info in self.auto_detector_data.items():
                try:
                    # í†µí•© ëª¨ë¸ ì •ë³´ ìƒì„±
                    integrated_info = IntegratedModelInfo(
                        name=model_name,
                        path=auto_info.get('path', ''),
                        step_type=self._map_step_type(auto_info.get('step_class', '')),
                        file_size_mb=auto_info.get('size_mb', 0.0),
                        auto_detector_info=auto_info,
                        ai_class=auto_info.get('ai_model_info', {}).get('ai_class', 'BaseRealAIModel'),
                        confidence_score=auto_info.get('confidence', 0.0),
                        priority_score=auto_info.get('priority_info', {}).get('priority_score', 0.0)
                    )
                    
                    self.integrated_model_info[model_name] = integrated_info
                    
                except Exception as e:
                    self.logger.error(f"âŒ ëª¨ë¸ í†µí•© ì‹¤íŒ¨ {model_name}: {e}")
                    continue
            
            self.logger.info(f"âœ… ëª¨ë¸ ë°ì´í„° í†µí•© ì™„ë£Œ: {len(self.integrated_model_info)}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë°ì´í„° í†µí•© ì‹¤íŒ¨: {e}")
    
    def _map_step_type(self, step_class: str) -> RealStepModelType:
        """Step í´ë˜ìŠ¤ëª…ì„ RealStepModelTypeìœ¼ë¡œ ë§¤í•‘"""
        step_class_lower = step_class.lower()
        
        if 'human' in step_class_lower or 'parsing' in step_class_lower:
            return RealStepModelType.HUMAN_PARSING
        elif 'pose' in step_class_lower:
            return RealStepModelType.POSE_ESTIMATION
        elif 'segmentation' in step_class_lower:
            return RealStepModelType.CLOTH_SEGMENTATION
        elif 'geometric' in step_class_lower or 'matching' in step_class_lower:
            return RealStepModelType.GEOMETRIC_MATCHING
        elif 'warping' in step_class_lower:
            return RealStepModelType.CLOTH_WARPING
        elif 'fitting' in step_class_lower or 'virtual' in step_class_lower:
            return RealStepModelType.VIRTUAL_FITTING
        elif 'post' in step_class_lower:
            return RealStepModelType.POST_PROCESSING
        elif 'quality' in step_class_lower or 'assessment' in step_class_lower:
            return RealStepModelType.QUALITY_ASSESSMENT
        
        return RealStepModelType.HUMAN_PARSING  # ê¸°ë³¸ê°’
    
    def load_model(self, model_name: str, step_type: Optional[RealStepModelType] = None, **kwargs) -> Optional[RealAIModel]:
        """ëª¨ë¸ ë¡œë”© (í†µí•© ë°ì´í„° ê¸°ë°˜)"""
        try:
            # ìºì‹œ í™•ì¸
            cached_model = self.cache.get(model_name)
            if cached_model:
                self.performance_metrics['cache_hits'] += 1
                return cached_model
            
            self.performance_metrics['cache_misses'] += 1
            
            # í†µí•© ëª¨ë¸ ì •ë³´ í™•ì¸
            integrated_info = self.integrated_model_info.get(model_name)
            if integrated_info:
                self.performance_metrics['auto_detector_hits'] += 1
                
                # í†µí•© ì •ë³´ì—ì„œ ê²½ë¡œì™€ íƒ€ì… ê°€ì ¸ì˜¤ê¸°
                model_path = integrated_info.path
                if not step_type:
                    step_type = integrated_info.step_type
                
                # ëª¨ë¸ ë¡œë”©
                start_time = time.time()
                
                model = RealAIModel(model_name, model_path, step_type, self.device)
                success = model.load()
                
                if success:
                    # í†µí•© ì •ë³´ë¥¼ ëª¨ë¸ì— ì¶”ê°€
                    model.integrated_info = integrated_info
                    
                    # ìºì‹œì— ì¶”ê°€
                    self.cache.put(model_name, model)
                    
                    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                    load_time = time.time() - start_time
                    self.performance_metrics['total_load_time'] += load_time
                    self.performance_metrics['models_loaded'] += 1
                    
                    self.logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name} ({load_time:.2f}s)")
                    return model
            
            # í†µí•© ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹œë„
            return self._fallback_load_model(model_name, step_type, **kwargs)
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model_name} - {e}")
            self.performance_metrics['error_count'] += 1
        
        return None
    
    def _fallback_load_model(self, model_name: str, step_type: Optional[RealStepModelType] = None, **kwargs) -> Optional[RealAIModel]:
        """ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ ë¡œë”© (í´ë°±)"""
        # ëª¨ë¸ ê²½ë¡œ ì°¾ê¸°
        model_path = self._find_model_path(model_name)
        if not model_path:
            self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_name}")
            return None
        
        # Step íƒ€ì… ì¶”ì •
        if not step_type:
            step_type = self._infer_step_type(model_name, model_path)
        
        # ëª¨ë¸ ë¡œë”©
        start_time = time.time()
        
        model = RealAIModel(model_name, model_path, step_type, self.device)
        success = model.load()
        
        if success:
            # ìºì‹œì— ì¶”ê°€
            self.cache.put(model_name, model)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            load_time = time.time() - start_time
            self.performance_metrics['total_load_time'] += load_time
            self.performance_metrics['models_loaded'] += 1
            
            self.logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (í´ë°±): {model_name} ({load_time:.2f}s)")
            return model
        
        return None
    
    def _find_model_path(self, model_name: str) -> Optional[str]:
        """ëª¨ë¸ ê²½ë¡œ ì°¾ê¸°"""
        # í†µí•© ì •ë³´ì—ì„œ ë¨¼ì € ì°¾ê¸°
        if model_name in self.integrated_model_info:
            return self.integrated_model_info[model_name].path
        
        # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê²°ê³¼ì—ì„œ ê²½ë¡œ ì°¾ê¸°
        if self.checkpoint_analysis:
            for path, analysis in self.checkpoint_analysis.get('checkpoints', {}).items():
                if model_name in path or model_name in os.path.basename(path):
                    return path
        
        # ê¸°ë³¸ ê²½ë¡œ ì‹œë„
        possible_paths = [
            self.model_cache_dir / f"{model_name}.pth",
            self.model_cache_dir / f"{model_name}.pt",
            self.model_cache_dir / f"{model_name}.bin",
            self.model_cache_dir / f"{model_name}.safetensors"
        ]
        
        for path in possible_paths:
            if path.exists():
                return str(path)
        
        return None
    
    def _infer_step_type(self, model_name: str, model_path: str) -> RealStepModelType:
        """Step íƒ€ì… ì¶”ì •"""
        # íŒŒì¼ëª… ê¸°ë°˜ ì¶”ì •
        model_name_lower = model_name.lower()
        path_lower = model_path.lower()
        
        if any(x in model_name_lower for x in ['graphonomy', 'schp', 'deeplab', 'human']):
            return RealStepModelType.HUMAN_PARSING
        elif any(x in model_name_lower for x in ['hrnet', 'openpose', 'yolo', 'pose']):
            return RealStepModelType.POSE_ESTIMATION
        elif any(x in model_name_lower for x in ['sam', 'u2net', 'segmentation']):
            return RealStepModelType.CLOTH_SEGMENTATION
        elif any(x in model_name_lower for x in ['gmm', 'tps', 'raft', 'geometric']):
            return RealStepModelType.GEOMETRIC_MATCHING
        elif any(x in model_name_lower for x in ['warping', 'viton', 'hrviton']):
            return RealStepModelType.CLOTH_WARPING
        elif any(x in model_name_lower for x in ['diffusion', 'ootd', 'stable']):
            return RealStepModelType.VIRTUAL_FITTING
        elif any(x in model_name_lower for x in ['esrgan', 'gfpgan', 'swinir', 'enhance']):
            return RealStepModelType.POST_PROCESSING
        elif any(x in model_name_lower for x in ['clip', 'lpips', 'quality']):
            return RealStepModelType.QUALITY_ASSESSMENT
        
        # ê²½ë¡œ ê¸°ë°˜ ì¶”ì •
        if 'step_01' in path_lower:
            return RealStepModelType.HUMAN_PARSING
        elif 'step_02' in path_lower:
            return RealStepModelType.POSE_ESTIMATION
        elif 'step_03' in path_lower:
            return RealStepModelType.CLOTH_SEGMENTATION
        elif 'step_04' in path_lower:
            return RealStepModelType.GEOMETRIC_MATCHING
        elif 'step_05' in path_lower:
            return RealStepModelType.CLOTH_WARPING
        elif 'step_06' in path_lower:
            return RealStepModelType.VIRTUAL_FITTING
        elif 'step_07' in path_lower:
            return RealStepModelType.POST_PROCESSING
        elif 'step_08' in path_lower:
            return RealStepModelType.QUALITY_ASSESSMENT
        
        return RealStepModelType.HUMAN_PARSING  # ê¸°ë³¸ê°’
    
    def create_step_interface(self, step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> RealStepModelInterface:
        """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ê¸°ì¡´ API í˜¸í™˜)"""
        step_type = self._infer_step_type_from_name(step_name)
        
        interface = RealStepModelInterface(self, step_name, step_type)
        
        if step_requirements:
            interface.register_requirements(step_requirements)
        
        self.step_interfaces[step_name] = interface
        return interface
    
    def _infer_step_type_from_name(self, step_name: str) -> RealStepModelType:
        """Step ì´ë¦„ìœ¼ë¡œë¶€í„° íƒ€ì… ì¶”ì •"""
        step_name_lower = step_name.lower()
        
        if 'human' in step_name_lower or 'parsing' in step_name_lower:
            return RealStepModelType.HUMAN_PARSING
        elif 'pose' in step_name_lower:
            return RealStepModelType.POSE_ESTIMATION
        elif 'segmentation' in step_name_lower or 'cloth' in step_name_lower:
            return RealStepModelType.CLOTH_SEGMENTATION
        elif 'geometric' in step_name_lower or 'matching' in step_name_lower:
            return RealStepModelType.GEOMETRIC_MATCHING
        elif 'warping' in step_name_lower:
            return RealStepModelType.CLOTH_WARPING
        elif 'fitting' in step_name_lower or 'virtual' in step_name_lower:
            return RealStepModelType.VIRTUAL_FITTING
        elif 'post' in step_name_lower or 'enhance' in step_name_lower:
            return RealStepModelType.POST_PROCESSING
        elif 'quality' in step_name_lower or 'assessment' in step_name_lower:
            return RealStepModelType.QUALITY_ASSESSMENT
        
        return RealStepModelType.HUMAN_PARSING
    
    def get_integrated_model_info(self, model_name: str) -> Optional[IntegratedModelInfo]:
        """í†µí•©ëœ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return self.integrated_model_info.get(model_name)
    
    def list_available_models(self, step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (í†µí•© ë°ì´í„° ê¸°ë°˜)"""
        results = []
        
        for model_name, integrated_info in self.integrated_model_info.items():
            try:
                # í•„í„°ë§
                if step_class and integrated_info.step_type.value != step_class.lower():
                    continue
                if model_type and integrated_info.auto_detector_info.get('model_type') != model_type:
                    continue
                
                # ê²°ê³¼ ìƒì„±
                result = {
                    'name': model_name,
                    'path': integrated_info.path,
                    'step_type': integrated_info.step_type.value,
                    'file_size_mb': integrated_info.file_size_mb,
                    'file_size_gb': integrated_info.file_size_gb,
                    'ai_class': integrated_info.ai_class,
                    'confidence_score': integrated_info.confidence_score,
                    'priority_score': integrated_info.priority_score,
                    'is_valid': integrated_info.is_valid,
                    'has_auto_detector_info': integrated_info.auto_detector_info is not None
                }
                
                results.append(result)
                
            except Exception as e:
                self.logger.error(f"âŒ ëª¨ë¸ ì •ë³´ ë³€í™˜ ì‹¤íŒ¨ {model_name}: {e}")
                continue
        
        return results
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        cache_stats = self.cache.get_stats()
        
        return {
            **self.performance_metrics,
            'cache_stats': cache_stats,
            'loaded_models_count': len(self.loaded_models),
            'step_interfaces_count': len(self.step_interfaces),
            'integrated_models_count': len(self.integrated_model_info),
            'auto_detector_enabled': self.enable_auto_detector,
            'device': self.device
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ìºì‹œ ì •ë¦¬
            self.cache.clear()
            
            # ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            self._executor.shutdown(wait=True)
            
            self.logger.info("âœ… EnhancedModelLoader ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ==============================================
# ğŸ”¥ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==============================================

_global_enhanced_model_loader = None

def get_global_enhanced_model_loader() -> EnhancedModelLoader:
    """ì „ì—­ EnhancedModelLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_enhanced_model_loader
    if _global_enhanced_model_loader is None:
        _global_enhanced_model_loader = EnhancedModelLoader()
    return _global_enhanced_model_loader

def initialize_enhanced_model_loader(**kwargs) -> EnhancedModelLoader:
    """EnhancedModelLoader ì´ˆê¸°í™”"""
    global _global_enhanced_model_loader
    _global_enhanced_model_loader = EnhancedModelLoader(**kwargs)
    return _global_enhanced_model_loader

# ê¸°ì¡´ API í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ë“¤
def get_model(model_name: str) -> Optional[RealAIModel]:
    """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ API í˜¸í™˜)"""
    loader = get_global_enhanced_model_loader()
    return loader.load_model(model_name)

def create_step_interface(step_name: str, step_requirements: Optional[Dict[str, Any]] = None) -> RealStepModelInterface:
    """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ê¸°ì¡´ API í˜¸í™˜)"""
    loader = get_global_enhanced_model_loader()
    return loader.create_step_interface(step_name, step_requirements)

def get_performance_metrics() -> Dict[str, Any]:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜ (ê¸°ì¡´ API í˜¸í™˜)"""
    loader = get_global_enhanced_model_loader()
    return loader.get_performance_metrics()

def list_available_models(step_class: Optional[str] = None, model_type: Optional[str] = None) -> List[Dict[str, Any]]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (ê¸°ì¡´ API í˜¸í™˜)"""
    loader = get_global_enhanced_model_loader()
    return loader.list_available_models(step_class, model_type)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    # EnhancedModelLoader ì´ˆê¸°í™”
    loader = EnhancedModelLoader()
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶œë ¥
    metrics = loader.get_performance_metrics()
    print("EnhancedModelLoader v2.0 ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
    print(json.dumps(metrics, indent=2, ensure_ascii=False))
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥
    available_models = loader.list_available_models()
    print(f"\nì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(available_models)}ê°œ")
    for model in available_models[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
        print(f"  - {model['name']}: {model['file_size_mb']:.1f}MB ({model['ai_class']})")
    
    # ì •ë¦¬
    loader.cleanup()
