#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - ëˆ„ë½ ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì • ì‹œìŠ¤í…œ
===============================================================================
âœ… Step 01-05 í•„ìˆ˜ ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ
âœ… conda í™˜ê²½ ìµœì í™”
âœ… ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ê²½ë¡œ ì„¤ì •
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… ì²´í¬í¬ì¸íŠ¸ ê²€ì¦ ë° ì„¤ì •
===============================================================================
"""

import os
import sys
import requests
import hashlib
import shutil
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
import json
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('model_download.log')
    ]
)
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 1. ëˆ„ë½ ëª¨ë¸ ì •ì˜ (ì‹¤ì œ í•„ìš”í•œ ëª¨ë¸ë“¤)
# ==============================================

@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´"""
    name: str
    step_name: str
    file_name: str
    download_url: str
    file_size_mb: float
    sha256_hash: Optional[str]
    description: str
    priority: int  # 1=í•„ìˆ˜, 2=ê¶Œì¥, 3=ì„ íƒ
    target_path: str

# ğŸ”¥ ì‹¤ì œ í•„ìš”í•œ ëˆ„ë½ ëª¨ë¸ë“¤
MISSING_MODELS = {
    # Step 01: Human Parsing (í•„ìˆ˜)
    "human_parsing_atr": ModelInfo(
        name="human_parsing_atr",
        step_name="HumanParsingStep", 
        file_name="exp-schp-201908301523-atr.pth",
        download_url="https://drive.google.com/uc?id=1k4dllHpu0bdx38J7H28rVVLpU-kOHmnH",
        file_size_mb=255.1,
        sha256_hash=None,
        description="Self-Correction Human Parsing (SCHP) ATR ëª¨ë¸ - ì¸ì²´ íŒŒì‹±",
        priority=1,
        target_path="step_01_human_parsing"
    ),
    
    "graphonomy_lip": ModelInfo(
        name="graphonomy_lip",
        step_name="HumanParsingStep",
        file_name="graphonomy_lip.pth",
        download_url="https://drive.google.com/uc?id=1P_jkP5fZ8-sj8BIb4oJzZfHk7uJyuWi-",
        file_size_mb=255.1,
        sha256_hash=None,
        description="Graphonomy LIP ëª¨ë¸ - ê³ í’ˆì§ˆ ì¸ì²´ íŒŒì‹±",
        priority=2,
        target_path="step_01_human_parsing"
    ),
    
    # Step 02: Pose Estimation (í•„ìˆ˜)
    "openpose_body": ModelInfo(
        name="openpose_body",
        step_name="PoseEstimationStep",
        file_name="body_pose_model.pth",
        download_url="https://www.dropbox.com/s/llpxd14is7gyj0z/body_pose_model.pth",
        file_size_mb=199.6,
        sha256_hash=None,
        description="OpenPose Body ëª¨ë¸ - 18-í‚¤í¬ì¸íŠ¸ í¬ì¦ˆ ì¶”ì •",
        priority=1,
        target_path="step_02_pose_estimation"
    ),
    
    "openpose_full": ModelInfo(
        name="openpose_full",
        step_name="PoseEstimationStep", 
        file_name="openpose.pth",
        download_url="https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases/download/v1.7.0/pose_coco.caffemodel",
        file_size_mb=200.0,
        sha256_hash=None,
        description="OpenPose Full ëª¨ë¸ - ì „ì‹  í¬ì¦ˆ ì¶”ì •",
        priority=1,
        target_path="step_02_pose_estimation"
    ),
    
    # Step 03: Cloth Segmentation (í•„ìˆ˜)
    "u2net_cloth": ModelInfo(
        name="u2net_cloth",
        step_name="ClothSegmentationStep",
        file_name="u2net.pth", 
        download_url="https://github.com/xuebinqin/U-2-Net/releases/download/v1.0/u2net.pth",
        file_size_mb=168.1,
        sha256_hash="347c3d51b944ce1cd92e8e3de4f35734e1a3cf82e7ecb86b9fcbb695d9a94e33",
        description="UÂ²-Net ëª¨ë¸ - ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜",
        priority=1,
        target_path="step_03_cloth_segmentation"
    ),
    
    # Step 04: Geometric Matching (ê¶Œì¥)
    "geometric_matching": ModelInfo(
        name="geometric_matching",
        step_name="GeometricMatchingStep",
        file_name="gmm.pth",
        download_url="https://github.com/sergeywong/cp-vton/releases/download/checkpoint/gmm_final.pth",
        file_size_mb=50.0,
        sha256_hash=None,
        description="Geometric Matching Module - CP-VTON",
        priority=2,
        target_path="step_04_geometric_matching"
    ),
    
    # Step 05: Cloth Warping (ê¶Œì¥)
    "cloth_warping_tom": ModelInfo(
        name="cloth_warping_tom",
        step_name="ClothWarpingStep",
        file_name="tom.pth",
        download_url="https://github.com/sergeywong/cp-vton/releases/download/checkpoint/tom_final.pth",
        file_size_mb=100.0,
        sha256_hash=None,
        description="Try-On Module - CP-VTON ì˜ë¥˜ ì›Œí•‘",
        priority=2,
        target_path="step_05_cloth_warping"
    ),
    
    # ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ëª¨ë¸ë“¤
    "sam_vit_h": ModelInfo(
        name="sam_vit_h", 
        step_name="ClothSegmentationStep",
        file_name="sam_vit_h_4b8939.pth",
        download_url="https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth",
        file_size_mb=2445.7,
        sha256_hash="a7bf3b02f3ebf1267aba913ff637d9a2d5c33d3173bb679e46d9f338c26f262e",
        description="Segment Anything Model (SAM) ViT-H - ê³ ì •ë°€ ì„¸ê·¸ë©˜í…Œì´ì…˜",
        priority=3,
        target_path="step_03_cloth_segmentation"
    )
}

# ==============================================
# ğŸ”¥ 2. í”„ë¡œì íŠ¸ ê²½ë¡œ ê´€ë¦¬
# ==============================================

class ProjectPathManager:
    """í”„ë¡œì íŠ¸ ê²½ë¡œ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.project_root = self._find_project_root()
        self.backend_dir = self.project_root / "backend"
        self.ai_models_dir = self.backend_dir / "ai_models"
        
    def _find_project_root(self) -> Path:
        """í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°"""
        current = Path(__file__).resolve()
        
        # backend í´ë” ì°¾ê¸°
        for _ in range(10):
            if current.name == 'backend':
                return current.parent  # mycloset-ai ë£¨íŠ¸
            if current.parent == current:
                break
            current = current.parent
        
        # í˜„ì¬ ê²½ë¡œì—ì„œ ì¶”ì¸¡
        current = Path.cwd()
        if 'mycloset-ai' in str(current):
            while current.name != 'mycloset-ai' and current.parent != current:
                current = current.parent
            return current
        
        # ê¸°ë³¸ ê²½ë¡œ
        return Path.home() / "MVP" / "mycloset-ai"
    
    def get_step_dir(self, step_path: str) -> Path:
        """Stepë³„ ë””ë ‰í† ë¦¬ ê²½ë¡œ"""
        return self.ai_models_dir / step_path
    
    def ensure_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        dirs_to_create = [
            self.ai_models_dir,
            self.ai_models_dir / "step_01_human_parsing",
            self.ai_models_dir / "step_02_pose_estimation", 
            self.ai_models_dir / "step_03_cloth_segmentation",
            self.ai_models_dir / "step_04_geometric_matching",
            self.ai_models_dir / "step_05_cloth_warping",
            self.ai_models_dir / "step_06_virtual_fitting",  # ì´ë¯¸ ì¡´ì¬
            self.ai_models_dir / "step_07_post_processing",
            self.ai_models_dir / "step_08_quality_assessment"  # ì´ë¯¸ ì¡´ì¬
        ]
        
        for dir_path in dirs_to_create:
            dir_path.mkdir(parents=True, exist_ok=True)
            logger.info(f"âœ… ë””ë ‰í† ë¦¬ ìƒì„±: {dir_path}")

# ==============================================
# ğŸ”¥ 3. ëª¨ë¸ ë‹¤ìš´ë¡œë”
# ==============================================

class ModelDownloader:
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê´€ë¦¬ì"""
    
    def __init__(self, path_manager: ProjectPathManager):
        self.path_manager = path_manager
        self.download_stats = {
            "total_models": 0,
            "downloaded": 0,
            "skipped": 0,
            "failed": 0,
            "total_size_mb": 0
        }
    
    def download_file_with_progress(self, url: str, file_path: Path, expected_size_mb: float) -> bool:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì§„í–‰ë¥  í‘œì‹œ)"""
        try:
            logger.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {file_path.name} ({expected_size_mb:.1f}MB)")
            
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        
                        # ì§„í–‰ë¥  ì¶œë ¥
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            print(f"\r  ì§„í–‰ë¥ : {progress:.1f}% ({downloaded/(1024*1024):.1f}MB)", end='', flush=True)
            
            print()  # ìƒˆ ì¤„
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            actual_size_mb = file_path.stat().st_size / (1024 * 1024)
            logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_path.name} ({actual_size_mb:.1f}MB)")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {file_path.name}: {e}")
            return False
    
    def verify_file_hash(self, file_path: Path, expected_hash: str) -> bool:
        """íŒŒì¼ í•´ì‹œ ê²€ì¦"""
        if not expected_hash:
            return True  # í•´ì‹œê°€ ì—†ìœ¼ë©´ íŒ¨ìŠ¤
            
        try:
            sha256_hash = hashlib.sha256()
            with open(file_path, "rb") as f:
                for byte_block in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(byte_block)
            
            actual_hash = sha256_hash.hexdigest()
            if actual_hash.lower() == expected_hash.lower():
                logger.info(f"âœ… í•´ì‹œ ê²€ì¦ ì„±ê³µ: {file_path.name}")
                return True
            else:
                logger.error(f"âŒ í•´ì‹œ ë¶ˆì¼ì¹˜: {file_path.name}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ í•´ì‹œ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def download_model(self, model_info: ModelInfo) -> bool:
        """ê°œë³„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        target_dir = self.path_manager.get_step_dir(model_info.target_path)
        target_file = target_dir / model_info.file_name
        
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if target_file.exists():
            actual_size_mb = target_file.stat().st_size / (1024 * 1024)
            if abs(actual_size_mb - model_info.file_size_mb) < 10:  # 10MB ì˜¤ì°¨ í—ˆìš©
                logger.info(f"â­ï¸ ì´ë¯¸ ì¡´ì¬: {model_info.file_name} ({actual_size_mb:.1f}MB)")
                self.download_stats["skipped"] += 1
                return True
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        target_dir.mkdir(parents=True, exist_ok=True)
        
        # ë‹¤ìš´ë¡œë“œ ì‹œë„
        success = self.download_file_with_progress(
            model_info.download_url, 
            target_file, 
            model_info.file_size_mb
        )
        
        if success:
            # í•´ì‹œ ê²€ì¦
            if model_info.sha256_hash:
                if not self.verify_file_hash(target_file, model_info.sha256_hash):
                    logger.error(f"âŒ {model_info.file_name} í•´ì‹œ ê²€ì¦ ì‹¤íŒ¨")
                    target_file.unlink()  # ì‚­ì œ
                    success = False
            
            if success:
                self.download_stats["downloaded"] += 1
                self.download_stats["total_size_mb"] += model_info.file_size_mb
                logger.info(f"ğŸ‰ {model_info.file_name} ë‹¤ìš´ë¡œë“œ ì„±ê³µ!")
        
        if not success:
            self.download_stats["failed"] += 1
        
        return success

# ==============================================
# ğŸ”¥ 4. conda í™˜ê²½ ì„¤ì • ê´€ë¦¬ì
# ==============================================

class CondaEnvironmentManager:
    """conda í™˜ê²½ ì„¤ì • ê´€ë¦¬ì"""
    
    def __init__(self):
        self.conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
        self.conda_prefix = os.environ.get('CONDA_PREFIX', '')
        self.is_m3_max = self._detect_m3_max()
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            import platform
            if platform.processor() == 'arm' and platform.system() == 'Darwin':
                # macOS ARM64
                with open('/proc/cpuinfo', 'r') as f:
                    cpu_info = f.read()
                    return 'Apple M3 Max' in cpu_info
        except:
            pass
        
        # ê°„ë‹¨í•œ ARM64 macOS ê°ì§€
        import os
        return 'arm64' in str(os.uname()) if hasattr(os, 'uname') else False
    
    def check_conda_environment(self) -> Dict[str, Any]:
        """conda í™˜ê²½ ì²´í¬"""
        info = {
            "conda_active": bool(self.conda_env),
            "conda_env": self.conda_env,
            "conda_prefix": self.conda_prefix,
            "is_m3_max": self.is_m3_max,
            "python_version": f"{sys.version_info.major}.{sys.version_info.minor}",
            "required_packages": [],
            "missing_packages": []
        }
        
        # í•„ìˆ˜ íŒ¨í‚¤ì§€ ì²´í¬
        required_packages = {
            'torch': 'PyTorch',
            'torchvision': 'TorchVision', 
            'numpy': 'NumPy',
            'pillow': 'PIL',
            'opencv-python': 'OpenCV',
            'requests': 'Requests'
        }
        
        for package, display_name in required_packages.items():
            try:
                __import__(package.replace('-', '_'))
                info["required_packages"].append(display_name)
            except ImportError:
                info["missing_packages"].append(display_name)
        
        return info
    
    def print_conda_setup_guide(self):
        """conda ì„¤ì • ê°€ì´ë“œ ì¶œë ¥"""
        print("""
ğŸ MyCloset AI - conda í™˜ê²½ ì„¤ì • ê°€ì´ë“œ (í•„ìˆ˜ ëª¨ë¸ìš©)

# 1. conda í™˜ê²½ ìƒì„±
conda create -n mycloset-ai python=3.9 -y
conda activate mycloset-ai

# 2. PyTorch ì„¤ì¹˜ (M3 Max ìµœì í™”)
conda install pytorch torchvision torchaudio -c pytorch -y

# 3. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
conda install numpy pillow opencv -c conda-forge -y
pip install requests tqdm

# 4. AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì‚¬í•­)
pip install transformers diffusers rembg

# 5. ê²€ì¦
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import torch; print(f'MPS ì‚¬ìš©ê°€ëŠ¥: {torch.backends.mps.is_available()}')"

# 6. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
python missing_model_downloader.py --download-all
        """)

# ==============================================
# ğŸ”¥ 5. ë©”ì¸ ê´€ë¦¬ì í´ë˜ìŠ¤
# ==============================================

class MissingModelManager:
    """ëˆ„ë½ ëª¨ë¸ í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self):
        self.path_manager = ProjectPathManager()
        self.downloader = ModelDownloader(self.path_manager)
        self.conda_manager = CondaEnvironmentManager()
        
        logger.info(f"ğŸ  í”„ë¡œì íŠ¸ ë£¨íŠ¸: {self.path_manager.project_root}")
        logger.info(f"ğŸ¤– AI ëª¨ë¸ ê²½ë¡œ: {self.path_manager.ai_models_dir}")
    
    def analyze_current_state(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ë¶„ì„"""
        state = {
            "project_info": {
                "root_path": str(self.path_manager.project_root),
                "ai_models_path": str(self.path_manager.ai_models_dir),
                "ai_models_exists": self.path_manager.ai_models_dir.exists()
            },
            "conda_info": self.conda_manager.check_conda_environment(),
            "existing_models": self._scan_existing_models(),
            "missing_models": self._identify_missing_models(),
            "recommendations": []
        }
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        if not state["conda_info"]["conda_active"]:
            state["recommendations"].append("conda í™˜ê²½ í™œì„±í™” í•„ìš”")
        
        if state["conda_info"]["missing_packages"]:
            state["recommendations"].append(f"íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•„ìš”: {', '.join(state['conda_info']['missing_packages'])}")
        
        if state["missing_models"]:
            total_missing_size = sum(MISSING_MODELS[m]["file_size_mb"] for m in state["missing_models"])
            state["recommendations"].append(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í•„ìš”: {len(state['missing_models'])}ê°œ ({total_missing_size:.1f}MB)")
        
        return state
    
    def _scan_existing_models(self) -> Dict[str, List[str]]:
        """ê¸°ì¡´ ëª¨ë¸ ìŠ¤ìº”"""
        existing = {}
        
        if not self.path_manager.ai_models_dir.exists():
            return existing
        
        # ê° Step í´ë” ìŠ¤ìº”
        for step_dir in self.path_manager.ai_models_dir.iterdir():
            if step_dir.is_dir():
                step_name = step_dir.name
                models = []
                
                for model_file in step_dir.rglob('*'):
                    if model_file.is_file() and model_file.suffix in ['.pth', '.pt', '.bin', '.ckpt', '.safetensors']:
                        size_mb = model_file.stat().st_size / (1024 * 1024)
                        models.append(f"{model_file.name} ({size_mb:.1f}MB)")
                
                if models:
                    existing[step_name] = models
        
        return existing
    
    def _identify_missing_models(self) -> List[str]:
        """ëˆ„ë½ëœ ëª¨ë¸ ì‹ë³„"""
        missing = []
        
        for model_key, model_info in MISSING_MODELS.items():
            target_dir = self.path_manager.get_step_dir(model_info.target_path)
            target_file = target_dir / model_info.file_name
            
            if not target_file.exists():
                missing.append(model_key)
        
        return missing
    
    def print_status_report(self):
        """ìƒíƒœ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        state = self.analyze_current_state()
        
        print("="*70)
        print("ğŸ” MyCloset AI - ëª¨ë¸ ìƒíƒœ ë¶„ì„")
        print("="*70)
        
        # í”„ë¡œì íŠ¸ ì •ë³´
        print(f"ğŸ  í”„ë¡œì íŠ¸ ë£¨íŠ¸: {state['project_info']['root_path']}")
        print(f"ğŸ¤– AI ëª¨ë¸ ê²½ë¡œ: {state['project_info']['ai_models_path']}")
        print(f"   ì¡´ì¬ ì—¬ë¶€: {'âœ…' if state['project_info']['ai_models_exists'] else 'âŒ'}")
        
        # conda í™˜ê²½
        conda = state['conda_info']
        print(f"\nğŸ conda í™˜ê²½:")
        print(f"   í™œì„±í™”: {'âœ…' if conda['conda_active'] else 'âŒ'}")
        print(f"   í™˜ê²½ëª…: {conda['conda_env'] or 'None'}")
        print(f"   Python: {conda['python_version']}")
        print(f"   M3 Max: {'âœ…' if conda['is_m3_max'] else 'âŒ'}")
        
        if conda['missing_packages']:
            print(f"   âš ï¸ ëˆ„ë½ íŒ¨í‚¤ì§€: {', '.join(conda['missing_packages'])}")
        
        # ê¸°ì¡´ ëª¨ë¸
        print(f"\nğŸ“¦ ê¸°ì¡´ ëª¨ë¸:")
        if state['existing_models']:
            for step, models in state['existing_models'].items():
                print(f"   {step}:")
                for model in models:
                    print(f"     - {model}")
        else:
            print("   ì—†ìŒ")
        
        # ëˆ„ë½ ëª¨ë¸
        print(f"\nâŒ ëˆ„ë½ ëª¨ë¸ ({len(state['missing_models'])}ê°œ):")
        if state['missing_models']:
            total_size = 0
            for model_key in state['missing_models']:
                model_info = MISSING_MODELS[model_key]
                priority_emoji = "ğŸ”¥" if model_info.priority == 1 else "â­" if model_info.priority == 2 else "ğŸ’¡"
                print(f"   {priority_emoji} {model_info.file_name} ({model_info.file_size_mb:.1f}MB) - {model_info.description}")
                total_size += model_info.file_size_mb
            print(f"   ğŸ“Š ì´ ë‹¤ìš´ë¡œë“œ í¬ê¸°: {total_size:.1f}MB ({total_size/1024:.1f}GB)")
        else:
            print("   ì—†ìŒ - ëª¨ë“  ëª¨ë¸ì´ ì¤€ë¹„ë¨! âœ…")
        
        # ê¶Œì¥ì‚¬í•­
        if state['recommendations']:
            print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            for i, rec in enumerate(state['recommendations'], 1):
                print(f"   {i}. {rec}")
        
        print("="*70)
    
    def download_missing_models(self, priority_filter: Optional[int] = None) -> bool:
        """ëˆ„ë½ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        missing_models = self._identify_missing_models()
        
        if not missing_models:
            logger.info("âœ… ëª¨ë“  ëª¨ë¸ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤!")
            return True
        
        # ìš°ì„ ìˆœìœ„ í•„í„°ë§
        if priority_filter:
            missing_models = [m for m in missing_models if MISSING_MODELS[m].priority <= priority_filter]
        
        if not missing_models:
            logger.info(f"âœ… ìš°ì„ ìˆœìœ„ {priority_filter} ì´í•˜ ëª¨ë¸ë“¤ì´ ëª¨ë‘ ì¡´ì¬í•©ë‹ˆë‹¤!")
            return True
        
        logger.info(f"ğŸ“¥ {len(missing_models)}ê°œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.path_manager.ensure_directories()
        
        # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
        success_count = 0
        for model_key in missing_models:
            model_info = MISSING_MODELS[model_key]
            
            if self.downloader.download_model(model_info):
                success_count += 1
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ í›„)
            if model_info.file_size_mb > 500:
                import gc
                gc.collect()
        
        # ê²°ê³¼ ë¦¬í¬íŠ¸
        stats = self.downloader.download_stats
        print("\n" + "="*50)
        print("ğŸ“Š ë‹¤ìš´ë¡œë“œ ê²°ê³¼:")
        print(f"   âœ… ì„±ê³µ: {stats['downloaded']}ê°œ")
        print(f"   â­ï¸ ê±´ë„ˆëœ€: {stats['skipped']}ê°œ") 
        print(f"   âŒ ì‹¤íŒ¨: {stats['failed']}ê°œ")
        print(f"   ğŸ“ ì´ ë‹¤ìš´ë¡œë“œ: {stats['total_size_mb']:.1f}MB")
        print("="*50)
        
        return stats['failed'] == 0
    
    def create_model_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
        config = {
            "version": "1.0",
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "project_root": str(self.path_manager.project_root),
            "ai_models_path": str(self.path_manager.ai_models_dir),
            "models": {}
        }
        
        # ê° ëª¨ë¸ ì •ë³´ ì¶”ê°€
        for model_key, model_info in MISSING_MODELS.items():
            target_path = self.path_manager.get_step_dir(model_info.target_path) / model_info.file_name
            
            config["models"][model_key] = {
                "name": model_info.name,
                "step_name": model_info.step_name,
                "file_name": model_info.file_name,
                "file_path": str(target_path),
                "file_size_mb": model_info.file_size_mb,
                "description": model_info.description,
                "priority": model_info.priority,
                "exists": target_path.exists(),
                "file_size_actual": target_path.stat().st_size / (1024 * 1024) if target_path.exists() else 0
            }
        
        # ì„¤ì • íŒŒì¼ ì €ì¥
        config_path = self.path_manager.ai_models_dir / "model_config.json"
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
        return config

# ==============================================
# ğŸ”¥ 6. CLI ì¸í„°í˜ì´ìŠ¤
# ==============================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='MyCloset AI - ëˆ„ë½ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë„êµ¬')
    parser.add_argument('--status', action='store_true', help='í˜„ì¬ ìƒíƒœë§Œ í™•ì¸')
    parser.add_argument('--download-all', action='store_true', help='ëª¨ë“  ëˆ„ë½ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ')
    parser.add_argument('--download-priority', type=int, choices=[1, 2, 3], 
                       help='ì§€ì •ëœ ìš°ì„ ìˆœìœ„ ì´í•˜ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ (1=í•„ìˆ˜, 2=ê¶Œì¥, 3=ì„ íƒ)')
    parser.add_argument('--conda-guide', action='store_true', help='conda ì„¤ì • ê°€ì´ë“œ ì¶œë ¥')
    parser.add_argument('--create-config', action='store_true', help='ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±')
    
    args = parser.parse_args()
    
    # ê´€ë¦¬ì ì´ˆê¸°í™”
    manager = MissingModelManager()
    
    # conda ê°€ì´ë“œ
    if args.conda_guide:
        manager.conda_manager.print_conda_setup_guide()
        return
    
    # ìƒíƒœ í™•ì¸
    manager.print_status_report()
    
    if args.status:
        return
    
    # ì„¤ì • íŒŒì¼ ìƒì„±
    if args.create_config:
        manager.create_model_config()
        print("âœ… ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ")
        return
    
    # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
    if args.download_all:
        success = manager.download_missing_models()
        if success:
            print("ğŸ‰ ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            manager.create_model_config()
        else:
            print("âš ï¸ ì¼ë¶€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            return 1
    
    elif args.download_priority:
        success = manager.download_missing_models(priority_filter=args.download_priority)
        if success:
            print(f"ğŸ‰ ìš°ì„ ìˆœìœ„ {args.download_priority} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            manager.create_model_config()
        else:
            print("âš ï¸ ì¼ë¶€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            return 1
    
    else:
        print("\nğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
        print("   python missing_model_downloader.py --download-priority 1  # í•„ìˆ˜ ëª¨ë¸ë§Œ")
        print("   python missing_model_downloader.py --download-all         # ëª¨ë“  ëª¨ë¸")
        print("   python missing_model_downloader.py --conda-guide          # conda ì„¤ì • ê°€ì´ë“œ")

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code or 0)
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)