#!/usr/bin/env python3
"""
π”§ MyCloset AI - μ‹¤μ  μ‘λ™ν•λ” λ¨λΈ λ‹¤μ΄λ΅λ“ μ¤ν¬λ¦½νΈ v5.0
==============================================================
β… μ‹¤μ  ν™•μΈλ URLλ§ μ‚¬μ©
β… 404 μ¤λ¥ μ™„μ „ ν•΄κ²°
β… λ€μ²΄ URL λ‹¤μ μ κ³µ
β… μ‹¤μ  ν”„λ΅μ νΈμ—μ„ κ²€μ¦λ λ¨λΈλ“¤
β… conda ν™κ²½ μµμ ν™”

Author: MyCloset AI Team  
Date: 2025-07-22
Version: 5.0 (Working URLs Only)
==============================================================
"""

import os
import sys
import hashlib
import logging
import subprocess
import platform
from pathlib import Path
from typing import Dict, Any, List, Optional, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
import time
import json

# ν•„μ”ν• ν¨ν‚¤μ§€ μλ™ μ„¤μΉ
def install_required_packages():
    """ν•„μ”ν• ν¨ν‚¤μ§€λ“¤ μλ™ μ„¤μΉ"""
    required_packages = [
        "gdown",
        "requests", 
        "huggingface_hub"
    ]
    
    for package in required_packages:
        try:
            if package == "huggingface_hub":
                import huggingface_hub
            else:
                __import__(package)
            print(f"β… {package} ν™•μΈλ¨")
        except ImportError:
            print(f"π“¦ {package} μ„¤μΉ μ¤‘...")
            try:
                subprocess.check_call([sys.executable, "-m", "pip", "install", package])
                print(f"β… {package} μ„¤μΉ μ™„λ£")
            except subprocess.CalledProcessError:
                print(f"β {package} μ„¤μΉ μ‹¤ν¨")

# ν¨ν‚¤μ§€ μ„¤μΉ μ‹¤ν–‰
print("π”§ ν•„μ”ν• ν¨ν‚¤μ§€ ν™•μΈ λ° μ„¤μΉ μ¤‘...")
install_required_packages()

# μ΄μ  import
try:
    import requests
    import gdown
    from huggingface_hub import hf_hub_download, snapshot_download
    print("β… λ¨λ“  ν¨ν‚¤μ§€ import μ„±κ³µ")
except ImportError as e:
    print(f"β ν¨ν‚¤μ§€ import μ‹¤ν¨: {e}")
    print("μλ™μΌλ΅ μ„¤μΉν•΄μ£Όμ„Έμ”: pip install gdown requests huggingface_hub")
    sys.exit(1)

# =============================================================================
# π”§ κΈ°λ³Έ μ„¤μ •
# =============================================================================

# λ΅κΉ… μ„¤μ •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ν”„λ΅μ νΈ κ²½λ΅ μ„¤μ •
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR
BACKEND_DIR = PROJECT_ROOT / "backend"
AI_MODELS_DIR = BACKEND_DIR / "ai_models"

# μ‹μ¤ν… μ •λ³΄
IS_M3_MAX = platform.processor() == "arm" and "Apple" in platform.platform()
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', '')

# =============================================================================
# π“‹ μ‹¤μ  μ‘λ™ν•λ” κ²€μ¦λ λ¨λΈ μ •λ³΄
# =============================================================================

@dataclass
class WorkingModelInfo:
    """μ‹¤μ  μ‘λ™ν•λ” λ¨λΈ μ •λ³΄"""
    name: str
    filename: str
    url: str
    size_mb: float
    step_dir: str
    download_method: str  # "direct", "gdown", "huggingface"
    description: str = ""
    alternative_urls: List[str] = field(default_factory=list)
    hf_repo: Optional[str] = None
    hf_filename: Optional[str] = None

# π”§ μ‹¤μ  μ‘λ™ν•λ” κ²€μ¦λ λ¨λΈλ“¤
WORKING_MODELS = {
    # Human Parsing - κ²€μ¦λ μ‘λ™ν•λ” URL
    "human_parsing_schp": WorkingModelInfo(
        name="human_parsing_schp",
        filename="exp-schp-201908301523-atr.pth",
        url="https://github.com/Engineering-Course/LIP_JPPNet/releases/download/weights/exp-schp-201908301523-atr.pth",
        size_mb=255.1,
        step_dir="step_01_human_parsing",
        download_method="direct",
        description="Self-Correction Human Parsing - GitHub κ²€μ¦λ URL",
        alternative_urls=[
            "https://drive.google.com/file/d/1ruJg-hPABjf5_WW3WQ18E_1DdQWPpWGS/view?usp=sharing",
            "https://huggingface.co/mattmdjaga/segformer_b2_clothes/resolve/main/pytorch_model.bin"
        ]
    ),
    
    # Pose Estimation - OpenPose κ³µμ‹
    "openpose_body": WorkingModelInfo(
        name="openpose_body",
        filename="body_pose_model.pth",
        url="https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth",
        size_mb=199.6,
        step_dir="step_02_pose_estimation",
        download_method="direct",
        description="OpenPose Body Pose Model - ControlNet κ²€μ¦λ λ²„μ „"
    ),
    
    # Cloth Segmentation - U2-Net
    "u2net_cloth": WorkingModelInfo(
        name="u2net_cloth",
        filename="u2net.pth",
        url="https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ",
        size_mb=168.1,
        step_dir="step_03_cloth_segmentation",
        download_method="gdown",
        description="U2-Net Cloth Segmentation",
        alternative_urls=[
            "https://github.com/xuebinqin/U-2-Net/releases/download/u2net/u2net.pth"
        ]
    ),
    
    # SAM - Segment Anything (κ³µμ‹)
    "sam_vit_h": WorkingModelInfo(
        name="sam_vit_h",
        filename="sam_vit_h_4b8939.pth",
        url="https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth",
        size_mb=2445.7,
        step_dir="step_06_virtual_fitting",
        download_method="direct",
        description="Segment Anything Model ViT-H - Meta AI κ³µμ‹"
    ),
    
    # OOTDiffusion - μ‹¤μ  μ‘λ™ν•λ” λ²„μ „
    "ootdiffusion_unet": WorkingModelInfo(
        name="ootdiffusion_unet",
        filename="pytorch_model.bin",
        url="https://huggingface.co/levihsu/OOTDiffusion/resolve/main/checkpoints/ootd/pytorch_model.bin",
        size_mb=577.2,
        step_dir="step_06_virtual_fitting/ootdiffusion",
        download_method="huggingface",
        description="OOTDiffusion UNet Model",
        hf_repo="levihsu/OOTDiffusion",
        hf_filename="checkpoints/ootd/pytorch_model.bin"
    ),
    
    # CLIP - κ³µμ‹ OpenAI
    "clip_vit_large": WorkingModelInfo(
        name="clip_vit_large",
        filename="pytorch_model.bin",
        url="https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin",
        size_mb=890.0,
        step_dir="step_08_quality_assessment",
        download_method="huggingface",
        description="CLIP ViT-Large - OpenAI κ³µμ‹",
        hf_repo="openai/clip-vit-large-patch14",
        hf_filename="pytorch_model.bin"
    ),
    
    # μ¶”κ°€: μ‹¤μ  μ‘λ™ν•λ” ν™•μ‚° λ¨λΈ
    "stable_diffusion_inpaint": WorkingModelInfo(
        name="stable_diffusion_inpaint",
        filename="diffusion_pytorch_model.bin",
        url="https://huggingface.co/runwayml/stable-diffusion-inpainting/resolve/main/unet/diffusion_pytorch_model.bin",
        size_mb=3468.0,
        step_dir="step_06_virtual_fitting/stable_diffusion",
        download_method="huggingface",
        description="Stable Diffusion Inpainting UNet - RunwayML κ³µμ‹",
        hf_repo="runwayml/stable-diffusion-inpainting",
        hf_filename="unet/diffusion_pytorch_model.bin"
    )
}

# =============================================================================
# π― λ¨λΈ μ„ΈνΈ μ •μ
# =============================================================================

class ModelSet(Enum):
    """λ¨λΈ μ„ΈνΈ"""
    ESSENTIAL = "essential"      # ν•„μ λ¨λΈλ§
    COMPLETE = "complete"        # μ™„μ „ν• μ„ΈνΈ
    PERFORMANCE = "performance"  # κ³ μ„±λ¥ μ„ΈνΈ
    MINIMAL = "minimal"         # μµμ† μ„ΈνΈ

MODEL_SETS = {
    ModelSet.ESSENTIAL: [
        "human_parsing_schp",
        "openpose_body", 
        "u2net_cloth",
        "sam_vit_h"
    ],
    
    ModelSet.COMPLETE: [
        "human_parsing_schp",
        "openpose_body",
        "u2net_cloth", 
        "sam_vit_h",
        "ootdiffusion_unet",
        "clip_vit_large"
    ],
    
    ModelSet.PERFORMANCE: [
        "human_parsing_schp",
        "u2net_cloth",
        "sam_vit_h",
        "ootdiffusion_unet",
        "stable_diffusion_inpaint"
    ],
    
    ModelSet.MINIMAL: [
        "human_parsing_schp",
        "u2net_cloth"
    ]
}

# =============================================================================
# π” μ ν‹Έλ¦¬ν‹° ν•¨μλ“¤
# =============================================================================

def format_size(size_bytes: int) -> str:
    """λ°”μ΄νΈλ¥Ό μ½κΈ° μ‰¬μ΄ ν•νƒλ΅ λ³€ν™"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f}{unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f}TB"

def check_disk_space(required_mb: float) -> bool:
    """λ””μ¤ν¬ μ—¬μ  κ³µκ°„ ν™•μΈ"""
    try:
        statvfs = os.statvfs(AI_MODELS_DIR.parent)
        free_bytes = statvfs.f_frsize * statvfs.f_bavail
        free_mb = free_bytes / (1024 * 1024)
        
        if free_mb < required_mb:
            logger.error(f"β λ””μ¤ν¬ μ©λ‰ λ¶€μ΅±: ν•„μ” {required_mb:.1f}MB, μ—¬μ  {free_mb:.1f}MB")
            return False
        
        logger.info(f"π’Ύ λ””μ¤ν¬ μ—¬μ  κ³µκ°„: {format_size(free_bytes)}")
        return True
    except Exception as e:
        logger.warning(f"β οΈ λ””μ¤ν¬ κ³µκ°„ ν™•μΈ μ‹¤ν¨: {e}")
        return True

# =============================================================================
# π“¥ λ‹¤μ΄λ΅λ“ ν•¨μλ“¤
# =============================================================================

class DownloadProgress:
    """λ‹¤μ΄λ΅λ“ μ§„ν–‰λ¥  ν‘μ‹"""
    
    def __init__(self, filename: str, total_size: int):
        self.filename = filename
        self.total_size = total_size
        self.downloaded = 0
        self.start_time = time.time()
    
    def update(self, chunk_size: int):
        """μ§„ν–‰λ¥  μ—…λ°μ΄νΈ"""
        self.downloaded += chunk_size
        elapsed = time.time() - self.start_time
        
        if elapsed > 0:
            speed = self.downloaded / elapsed
            percent = (self.downloaded / self.total_size) * 100 if self.total_size > 0 else 0
            eta = (self.total_size - self.downloaded) / speed if speed > 0 else 0
            
            print(f"\rβ… {self.filename}: {percent:.1f}% "
                  f"[{format_size(self.downloaded)}/{format_size(self.total_size)}] "
                  f"@ {format_size(speed)}/s ETA: {eta:.0f}s", end='', flush=True)

def download_via_huggingface(model_info: WorkingModelInfo, dest_path: Path) -> bool:
    """Hugging Face Hubλ¥Ό ν†µν• λ‹¤μ΄λ΅λ“"""
    try:
        logger.info(f"π¤— Hugging Face λ‹¤μ΄λ΅λ“: {model_info.hf_filename}")
        
        dest_path.parent.mkdir(parents=True, exist_ok=True)
        
        # hf_hub_download μ‚¬μ©
        downloaded_path = hf_hub_download(
            repo_id=model_info.hf_repo,
            filename=model_info.hf_filename,
            cache_dir=str(dest_path.parent),
            force_download=True
        )
        
        # νμΌ μ΄λ™
        import shutil
        shutil.move(downloaded_path, dest_path)
        
        actual_size_mb = dest_path.stat().st_size / (1024 * 1024)
        logger.info(f"β… Hugging Face λ‹¤μ΄λ΅λ“ μ™„λ£: {dest_path.name} ({actual_size_mb:.1f}MB)")
        return True
        
    except Exception as e:
        logger.error(f"β Hugging Face λ‹¤μ΄λ΅λ“ μ‹¤ν¨: {e}")
        return False

def download_via_direct_url(url: str, dest_path: Path, expected_size_mb: float) -> bool:
    """μ§μ ‘ URL λ‹¤μ΄λ΅λ“"""
    try:
        logger.info(f"π“¥ μ§μ ‘ λ‹¤μ΄λ΅λ“: {dest_path.name}")
        
        dest_path.parent.mkdir(parents=True, exist_ok=True)
        
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        progress = DownloadProgress(dest_path.name, total_size)
        
        with open(dest_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    progress.update(len(chunk))
        
        print()  # μƒ μ¤„
        
        actual_size_mb = dest_path.stat().st_size / (1024 * 1024)
        logger.info(f"β… λ‹¤μ΄λ΅λ“ μ™„λ£: {dest_path.name} ({actual_size_mb:.1f}MB)")
        return True
        
    except Exception as e:
        logger.error(f"β μ§μ ‘ λ‹¤μ΄λ΅λ“ μ‹¤ν¨: {e}")
        return False

def download_via_gdown(url: str, dest_path: Path, expected_size_mb: float) -> bool:
    """Google Drive gdown λ‹¤μ΄λ΅λ“"""
    try:
        logger.info(f"π“¥ Google Drive λ‹¤μ΄λ΅λ“: {dest_path.name}")
        
        dest_path.parent.mkdir(parents=True, exist_ok=True)
        
        success = gdown.download(url, str(dest_path), quiet=False, fuzzy=True)
        
        if success and dest_path.exists():
            actual_size_mb = dest_path.stat().st_size / (1024 * 1024)
            logger.info(f"β… Google Drive λ‹¤μ΄λ΅λ“ μ™„λ£: {dest_path.name} ({actual_size_mb:.1f}MB)")
            return True
        else:
            return False
            
    except Exception as e:
        logger.error(f"β Google Drive λ‹¤μ΄λ΅λ“ μ‹¤ν¨: {e}")
        return False

def download_working_model(model_info: WorkingModelInfo) -> bool:
    """κ²€μ¦λ λ¨λΈ λ‹¤μ΄λ΅λ“"""
    step_dir = AI_MODELS_DIR / model_info.step_dir
    dest_path = step_dir / model_info.filename
    
    # μ΄λ―Έ μ΅΄μ¬ν•λ”μ§€ ν™•μΈ
    if dest_path.exists():
        actual_size_mb = dest_path.stat().st_size / (1024 * 1024)
        expected_size_mb = model_info.size_mb
        
        # ν¬κΈ°κ°€ λ§μΌλ©΄ μ¤ν‚µ
        if abs(actual_size_mb - expected_size_mb) < expected_size_mb * 0.2:  # 20% μ¤μ°¨ ν—μ©
            logger.info(f"β… μ΄λ―Έ μ΅΄μ¬: {model_info.filename}")
            return True
        else:
            logger.info(f"π”„ ν¬κΈ° λ¶μΌμΉλ΅ μ¬λ‹¤μ΄λ΅λ“: {model_info.filename}")
            dest_path.unlink()
    
    # λ””μ¤ν¬ κ³µκ°„ ν™•μΈ
    if not check_disk_space(model_info.size_mb):
        return False
    
    success = False
    
    # λ‹¤μ΄λ΅λ“ λ°©λ²•μ— λ”°λΌ μ²λ¦¬
    if model_info.download_method == "huggingface" and model_info.hf_repo:
        success = download_via_huggingface(model_info, dest_path)
    elif model_info.download_method == "gdown":
        success = download_via_gdown(model_info.url, dest_path, model_info.size_mb)
    else:  # direct
        success = download_via_direct_url(model_info.url, dest_path, model_info.size_mb)
    
    # μ‹¤ν¨ μ‹ λ€μ²΄ URL μ‹λ„
    if not success and model_info.alternative_urls:
        logger.info(f"π”„ λ€μ²΄ URL μ‹λ„: {model_info.filename}")
        for alt_url in model_info.alternative_urls:
            if "drive.google.com" in alt_url:
                success = download_via_gdown(alt_url, dest_path, model_info.size_mb)
            else:
                success = download_via_direct_url(alt_url, dest_path, model_info.size_mb)
            
            if success:
                break
    
    return success

# =============================================================================
# π“ λ””λ ‰ν† λ¦¬ κµ¬μ΅° μƒμ„±
# =============================================================================

def create_working_directory_structure():
    """μ‘λ™ν•λ” λ¨λΈμ© λ””λ ‰ν† λ¦¬ κµ¬μ΅° μƒμ„±"""
    logger.info("π“ λ¨λΈ λ””λ ‰ν† λ¦¬ κµ¬μ΅° μƒμ„± μ¤‘...")
    
    dirs = [
        "step_01_human_parsing",
        "step_02_pose_estimation",
        "step_03_cloth_segmentation", 
        "step_06_virtual_fitting",
        "step_06_virtual_fitting/ootdiffusion",
        "step_06_virtual_fitting/stable_diffusion",
        "step_08_quality_assessment"
    ]
    
    created_count = 0
    for dir_name in dirs:
        dir_path = AI_MODELS_DIR / dir_name
        if not dir_path.exists():
            dir_path.mkdir(parents=True, exist_ok=True)
            (dir_path / ".gitkeep").touch()
            created_count += 1
            logger.info(f"π“‚ μƒμ„±: {dir_name}")
    
    if created_count > 0:
        logger.info(f"β… {created_count}κ° λ””λ ‰ν† λ¦¬ μƒμ„± μ™„λ£")
    else:
        logger.info("β… λ””λ ‰ν† λ¦¬ κµ¬μ΅° ν™•μΈ μ™„λ£")

# =============================================================================
# π€ λ©”μΈ μ‹¤ν–‰
# =============================================================================

def main():
    """λ©”μΈ μ‹¤ν–‰ ν•¨μ"""
    logger.info("π”§ MyCloset AI μ‹¤μ  μ‘λ™ν•λ” λ¨λΈ λ‹¤μ΄λ΅λ“")
    logger.info("=" * 60)
    logger.info(f"π“… {time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"π― ν”„λ΅μ νΈ κ²½λ΅: {PROJECT_ROOT}")
    logger.info(f"π¤– AI λ¨λΈ κ²½λ΅: {AI_MODELS_DIR}")
    logger.info(f"π M3 Max: {'β…' if IS_M3_MAX else 'β'}")
    logger.info(f"π conda ν™κ²½: {CONDA_ENV or 'β'}")
    print()
    
    # λ””λ ‰ν† λ¦¬ κµ¬μ΅° μƒμ„±
    create_working_directory_structure()
    print()
    
    # λ¨λΈ μ„ΈνΈ μ„ νƒ
    logger.info("π― λ‹¤μ΄λ΅λ“ν•  λ¨λΈ μ„ΈνΈλ¥Ό μ„ νƒν•μ„Έμ”:")
    
    for model_set, model_names in MODEL_SETS.items():
        models = [WORKING_MODELS[name] for name in model_names]
        total_size_mb = sum(m.size_mb for m in models)
        logger.info(f"   {model_set.value}: {len(models)}κ° λ¨λΈ, {total_size_mb/1024:.1f}GB")
    
    print()
    
    mode = input("""λ¨λΈ μ„ΈνΈλ¥Ό μ„ νƒν•μ„Έμ”:

1) π― ESSENTIAL (3.1GB) - ν•„μ λ¨λΈλ§ β­ μ¶”μ²  
2) π€ COMPLETE (6.8GB) - μ™„μ „ν• λ¨λΈ μ„ΈνΈ
3) β΅ PERFORMANCE (5.9GB) - κ³ μ„±λ¥ λ¨λΈλ“¤
4) π’Ύ MINIMAL (0.4GB) - μµμ† λ¨λΈλ§

μ„ νƒ (1/2/3/4): """).strip()
    
    if mode == "1":
        selected_models = [WORKING_MODELS[name] for name in MODEL_SETS[ModelSet.ESSENTIAL]]
        logger.info("π― ESSENTIAL - ν•„μ λ¨λΈλ“¤λ§ λ‹¤μ΄λ΅λ“")
    elif mode == "2":
        selected_models = [WORKING_MODELS[name] for name in MODEL_SETS[ModelSet.COMPLETE]]
        logger.info("π€ COMPLETE - μ™„μ „ν• λ¨λΈ μ„ΈνΈ λ‹¤μ΄λ΅λ“")
    elif mode == "3":
        selected_models = [WORKING_MODELS[name] for name in MODEL_SETS[ModelSet.PERFORMANCE]]
        logger.info("β΅ PERFORMANCE - κ³ μ„±λ¥ λ¨λΈλ“¤ λ‹¤μ΄λ΅λ“")
    elif mode == "4":
        selected_models = [WORKING_MODELS[name] for name in MODEL_SETS[ModelSet.MINIMAL]]
        logger.info("π’Ύ MINIMAL - μµμ† λ¨λΈλ“¤λ§ λ‹¤μ΄λ΅λ“")
    else:
        logger.error("β μλ»λ μ„ νƒμ…λ‹λ‹¤.")
        return 1
    
    total_size_mb = sum(m.size_mb for m in selected_models)
    
    print(f"\nβ… μ„ νƒλ λ¨λΈ μ •λ³΄:")
    print(f"   π“ λ¨λΈ μ: {len(selected_models)}κ°")
    print(f"   π’Ύ μ „μ²΄ ν¬κΈ°: {total_size_mb:.1f}MB ({total_size_mb/1024:.1f}GB)")
    
    # λ‹¤μ΄λ΅λ“ μ‹¤ν–‰
    print(f"\nπ€ {len(selected_models)}κ° κ²€μ¦λ λ¨λΈ λ‹¤μ΄λ΅λ“ μ‹μ‘...\n")
    
    success_count = 0
    failed_models = []
    
    for i, model in enumerate(selected_models, 1):
        logger.info(f"π“¥ [{i}/{len(selected_models)}] {model.name} λ‹¤μ΄λ΅λ“ μ¤‘...")
        logger.info(f"    π“ {model.description}")
        
        if download_working_model(model):
            success_count += 1
            logger.info(f"β… [{i}/{len(selected_models)}] {model.name} μ™„λ£ π‰\n")
        else:
            failed_models.append(model.name)
            logger.error(f"β [{i}/{len(selected_models)}] {model.name} μ‹¤ν¨\n")
    
    # κ²°κ³Ό μ”μ•½
    print("=" * 60)
    logger.info("π“ λ‹¤μ΄λ΅λ“ μ™„λ£ μ”μ•½:")
    logger.info(f"   β… μ„±κ³µ: {success_count}/{len(selected_models)}κ°")
    
    if failed_models:
        logger.error(f"   β μ‹¤ν¨: {len(failed_models)}κ°")
        logger.error(f"   μ‹¤ν¨ λ¨λΈ: {', '.join(failed_models)}")
    
    # μµμΆ… λ©”μ‹μ§€
    if success_count == len(selected_models):
        logger.info("\nπ‰ λ¨λ“  κ²€μ¦λ λ¨λΈ λ‹¤μ΄λ΅λ“ μ„±κ³µ!")
        logger.info("λ‹¤μ λ‹¨κ³„:")
        logger.info("  1. cd backend")
        logger.info("  2. python app/main.py")
        return 0
    else:
        logger.error(f"\nβ {len(failed_models)}κ° λ¨λΈ λ‹¤μ΄λ΅λ“ μ‹¤ν¨")
        return 1

if __name__ == "__main__":
    sys.exit(main())