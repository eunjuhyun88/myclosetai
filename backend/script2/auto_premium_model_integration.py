#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - ìë™ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™ ìŠ¤í¬ë¦½íŠ¸ v2.0
===============================================================================
âœ… ê¸°ì¡´ Step êµ¬í˜„ì²´ë“¤ì— ìµœê³ ê¸‰ AI ëª¨ë¸ ìë™ ì—°ë™
âœ… ModelLoaderì™€ ì™„ë²½ í˜¸í™˜
âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™”
âœ… M3 Max 128GB ì™„ì „ í™œìš©

ì‹¤í–‰: python auto_premium_model_integration.py
"""

import sys
import os
import asyncio
import logging
import torch
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def integrate_premium_models_to_existing_steps():
    """ê¸°ì¡´ Step êµ¬í˜„ì²´ë“¤ì— í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ìë™ ì—°ë™"""
    
    print("ğŸ”¥ MyCloset AI Premium ëª¨ë¸ ìë™ ì—°ë™ ì‹œì‘...")
    print("="*60)
    
    try:
        # 1. ModelLoader ê°€ì ¸ì˜¤ê¸°
        from app.ai_pipeline.utils.model_loader import get_global_model_loader
        model_loader = get_global_model_loader()
        
        print("âœ… ModelLoader ê°€ì ¸ì˜¤ê¸° ì„±ê³µ")
        
        # 2. Premium ëª¨ë¸ ì„ íƒê¸° ê°€ì ¸ì˜¤ê¸°
        sys.path.append('.')
        from premium_ai_model_mapping import PremiumAIModelSelector, PREMIUM_AI_MODELS_BY_STEP
        
        # M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš©
        selector = PremiumAIModelSelector(available_memory_gb=128.0)
        selected_models = selector.select_best_models_for_all_steps()
        
        print(f"âœ… í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì„ íƒ ì™„ë£Œ: {len(selected_models)}ê°œ")
        
        # 3. ê° Step êµ¬í˜„ì²´ì— í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™
        integration_results = {}
        
        # Step 01: Human Parsing
        result = await integrate_step_01_premium(model_loader, selected_models)
        integration_results["HumanParsingStep"] = result
        
        # Step 02: Pose Estimation  
        result = await integrate_step_02_premium(model_loader, selected_models)
        integration_results["PoseEstimationStep"] = result
        
        # Step 03: Cloth Segmentation
        result = await integrate_step_03_premium(model_loader, selected_models)
        integration_results["ClothSegmentationStep"] = result
        
        # Step 06: Virtual Fitting (í•µì‹¬!)
        result = await integrate_step_06_premium(model_loader, selected_models)
        integration_results["VirtualFittingStep"] = result
        
        # Step 08: Quality Assessment
        result = await integrate_step_08_premium(model_loader, selected_models)
        integration_results["QualityAssessmentStep"] = result
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ‰ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™ ê²°ê³¼:")
        print("="*60)
        
        success_count = 0
        for step_name, result in integration_results.items():
            status = "âœ…" if result["success"] else "âŒ"
            print(f"{status} {step_name}: {result['message']}")
            if result["success"]:
                success_count += 1
                if "model_info" in result:
                    info = result["model_info"]
                    print(f"    ğŸ“¦ ëª¨ë¸: {info['name']}")
                    print(f"    ğŸ“Š íŒŒë¼ë¯¸í„°: {info['parameters']:,}ê°œ")
                    print(f"    ğŸ’¾ ë©”ëª¨ë¦¬: {info['memory_gb']:.1f}GB")
        
        print(f"\nğŸ“ˆ ì´ ì—°ë™ ì„±ê³µ: {success_count}/{len(integration_results)}ê°œ")
        
        if success_count > 0:
            print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„: FastAPI ì„œë²„ ì‹¤í–‰")
            print("cd backend && python -m app.main")
        
        return integration_results
        
    except Exception as e:
        logger.error(f"âŒ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

async def integrate_step_01_premium(model_loader, selected_models):
    """Step 01 Human Parsingì— í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™"""
    try:
        if "HumanParsingStep" not in selected_models:
            return {"success": False, "message": "ì„ íƒëœ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—†ìŒ"}
        
        premium_model = selected_models["HumanParsingStep"]
        
        print(f"\nğŸ”„ Step 01 í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™: {premium_model.name}")
        
        # Step 01 êµ¬í˜„ì²´ ê°€ì ¸ì˜¤ê¸°
        from app.ai_pipeline.steps.step_01_human_parsing import HumanParsingStep
        
        # í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë¡œë“œ
        model_path = premium_model.file_path
        if not os.path.exists(model_path):
            return {"success": False, "message": f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}"}
        
        # SCHP ëª¨ë¸ ë¡œë”© (66M íŒŒë¼ë¯¸í„°)
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # ModelLoaderì— ë“±ë¡
        model_loader.register_premium_model(
            step_class="HumanParsingStep",
            model_name=premium_model.name,
            model_checkpoint=checkpoint,
            model_info={
                "parameters": premium_model.parameters,
                "performance_score": premium_model.performance_score,
                "memory_requirement_gb": premium_model.memory_requirement_gb
            }
        )
        
        return {
            "success": True,
            "message": "SCHP í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™ ì„±ê³µ",
            "model_info": {
                "name": premium_model.name,
                "parameters": premium_model.parameters,
                "memory_gb": premium_model.memory_requirement_gb
            }
        }
        
    except Exception as e:
        return {"success": False, "message": f"ì—°ë™ ì‹¤íŒ¨: {e}"}

async def integrate_step_02_premium(model_loader, selected_models):
    """Step 02 Pose Estimationì— í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™"""
    try:
        if "PoseEstimationStep" not in selected_models:
            return {"success": False, "message": "ì„ íƒëœ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—†ìŒ"}
        
        premium_model = selected_models["PoseEstimationStep"]
        
        print(f"\nğŸ”„ Step 02 í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™: {premium_model.name}")
        
        # OpenPose ëª¨ë¸ ë¡œë”© (52M íŒŒë¼ë¯¸í„°)
        model_path = premium_model.file_path
        if not os.path.exists(model_path):
            return {"success": False, "message": f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}"}
        
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # ModelLoaderì— ë“±ë¡
        model_loader.register_premium_model(
            step_class="PoseEstimationStep", 
            model_name=premium_model.name,
            model_checkpoint=checkpoint,
            model_info={
                "parameters": premium_model.parameters,
                "performance_score": premium_model.performance_score,
                "keypoints": 25  # COCO 25ê°œ í‚¤í¬ì¸íŠ¸
            }
        )
        
        return {
            "success": True,
            "message": "OpenPose í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™ ì„±ê³µ",
            "model_info": {
                "name": premium_model.name,
                "parameters": premium_model.parameters,
                "memory_gb": premium_model.memory_requirement_gb
            }
        }
        
    except Exception as e:
        return {"success": False, "message": f"ì—°ë™ ì‹¤íŒ¨: {e}"}

async def integrate_step_03_premium(model_loader, selected_models):
    """Step 03 Cloth Segmentationì— í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™"""
    try:
        if "ClothSegmentationStep" not in selected_models:
            return {"success": False, "message": "ì„ íƒëœ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—†ìŒ"}
        
        premium_model = selected_models["ClothSegmentationStep"]
        
        print(f"\nğŸ”„ Step 03 í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™: {premium_model.name}")
        
        # SAM ViT-H ëª¨ë¸ ë¡œë”© (641M íŒŒë¼ë¯¸í„°!)
        model_path = premium_model.file_path
        if not os.path.exists(model_path):
            return {"success": False, "message": f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}"}
        
        # SAM íŠ¹í™” ë¡œë”©
        try:
            # SAM íŒ¨í‚¤ì§€ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            from segment_anything import sam_model_registry, SamPredictor
            sam_model = sam_model_registry["vit_h"](checkpoint=model_path)
            sam_predictor = SamPredictor(sam_model)
            
            # ModelLoaderì— ë“±ë¡
            model_loader.register_premium_model(
                step_class="ClothSegmentationStep",
                model_name=premium_model.name, 
                model_checkpoint={"sam_model": sam_model, "sam_predictor": sam_predictor},
                model_info={
                    "parameters": premium_model.parameters,
                    "performance_score": premium_model.performance_score,
                    "model_type": "SAM_ViT_H"
                }
            )
            
        except ImportError:
            # SAM íŒ¨í‚¤ì§€ ì—†ìœ¼ë©´ ì¼ë°˜ ë¡œë”©
            checkpoint = torch.load(model_path, map_location='cpu')
            model_loader.register_premium_model(
                step_class="ClothSegmentationStep",
                model_name=premium_model.name,
                model_checkpoint=checkpoint,
                model_info={
                    "parameters": premium_model.parameters,
                    "performance_score": premium_model.performance_score
                }
            )
        
        return {
            "success": True,
            "message": "SAM ViT-H í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™ ì„±ê³µ",
            "model_info": {
                "name": premium_model.name,
                "parameters": premium_model.parameters,
                "memory_gb": premium_model.memory_requirement_gb
            }
        }
        
    except Exception as e:
        return {"success": False, "message": f"ì—°ë™ ì‹¤íŒ¨: {e}"}

async def integrate_step_06_premium(model_loader, selected_models):
    """Step 06 Virtual Fittingì— í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™ (í•µì‹¬!)"""
    try:
        if "VirtualFittingStep" not in selected_models:
            return {"success": False, "message": "ì„ íƒëœ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—†ìŒ"}
        
        premium_model = selected_models["VirtualFittingStep"]
        
        print(f"\nğŸ”„ Step 06 í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™: {premium_model.name}")
        print(f"    ğŸ”¥ í•µì‹¬ ê°€ìƒí”¼íŒ… ëª¨ë¸ - {premium_model.parameters:,} íŒŒë¼ë¯¸í„°!")
        
        # OOTDiffusion HD ëª¨ë¸ ë¡œë”© (859M íŒŒë¼ë¯¸í„°!)
        model_path = premium_model.file_path
        model_dir = os.path.dirname(model_path)
        
        if not os.path.exists(model_path):
            return {"success": False, "message": f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}"}
        
        try:
            # Diffusersë¡œ UNet ë¡œë”©
            from diffusers import UNet2DConditionModel
            unet_model = UNet2DConditionModel.from_pretrained(
                model_dir,
                torch_dtype=torch.float16,  # M3 Max ë©”ëª¨ë¦¬ ìµœì í™”
                variant="fp16",
                use_safetensors=True
            )
            
            # ModelLoaderì— ë“±ë¡
            model_loader.register_premium_model(
                step_class="VirtualFittingStep",
                model_name=premium_model.name,
                model_checkpoint={"unet": unet_model},
                model_info={
                    "parameters": premium_model.parameters,
                    "performance_score": premium_model.performance_score,
                    "model_type": "OOTDiffusion_HD",
                    "resolution": "1024px"
                }
            )
            
        except ImportError:
            # Diffusers ì—†ìœ¼ë©´ safetensorsë¡œ ì§ì ‘ ë¡œë”©
            import safetensors.torch
            checkpoint = safetensors.torch.load_file(model_path)
            
            model_loader.register_premium_model(
                step_class="VirtualFittingStep",
                model_name=premium_model.name,
                model_checkpoint=checkpoint,
                model_info={
                    "parameters": premium_model.parameters,
                    "performance_score": premium_model.performance_score
                }
            )
        
        return {
            "success": True,
            "message": "OOTDiffusion HD í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™ ì„±ê³µ",
            "model_info": {
                "name": premium_model.name,
                "parameters": premium_model.parameters,
                "memory_gb": premium_model.memory_requirement_gb
            }
        }
        
    except Exception as e:
        return {"success": False, "message": f"ì—°ë™ ì‹¤íŒ¨: {e}"}

async def integrate_step_08_premium(model_loader, selected_models):
    """Step 08 Quality Assessmentì— í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™"""
    try:
        if "QualityAssessmentStep" not in selected_models:
            return {"success": False, "message": "ì„ íƒëœ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—†ìŒ"}
        
        premium_model = selected_models["QualityAssessmentStep"]
        
        print(f"\nğŸ”„ Step 08 í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™: {premium_model.name}")
        
        # CLIP ViT-L ëª¨ë¸ ë¡œë”© (782M íŒŒë¼ë¯¸í„°!)
        model_path = premium_model.file_path
        if not os.path.exists(model_path):
            return {"success": False, "message": f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}"}
        
        try:
            # OpenCLIPìœ¼ë¡œ ë¡œë”©
            import open_clip
            clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(
                'ViT-L-14',
                pretrained=model_path
            )
            
            # ModelLoaderì— ë“±ë¡
            model_loader.register_premium_model(
                step_class="QualityAssessmentStep",
                model_name=premium_model.name,
                model_checkpoint={
                    "clip_model": clip_model,
                    "clip_preprocess": clip_preprocess
                },
                model_info={
                    "parameters": premium_model.parameters,
                    "performance_score": premium_model.performance_score,
                    "model_type": "CLIP_ViT_L"
                }
            )
            
        except ImportError:
            # OpenCLIP ì—†ìœ¼ë©´ ì¼ë°˜ ë¡œë”©
            checkpoint = torch.load(model_path, map_location='cpu')
            model_loader.register_premium_model(
                step_class="QualityAssessmentStep",
                model_name=premium_model.name,
                model_checkpoint=checkpoint,
                model_info={
                    "parameters": premium_model.parameters,
                    "performance_score": premium_model.performance_score
                }
            )
        
        return {
            "success": True,
            "message": "CLIP ViT-L í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™ ì„±ê³µ",
            "model_info": {
                "name": premium_model.name,
                "parameters": premium_model.parameters,
                "memory_gb": premium_model.memory_requirement_gb
            }
        }
        
    except Exception as e:
        return {"success": False, "message": f"ì—°ë™ ì‹¤íŒ¨: {e}"}

# ModelLoaderì— í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë“±ë¡ ë©”ì„œë“œ ì¶”ê°€
def add_premium_model_registration_to_model_loader():
    """ModelLoaderì— í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë“±ë¡ ê¸°ëŠ¥ ì¶”ê°€"""
    
    registration_code = '''
def register_premium_model(self, step_class: str, model_name: str, model_checkpoint: Any, model_info: Dict[str, Any]):
    """í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ì„ ModelLoaderì— ë“±ë¡"""
    try:
        # í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì €ì¥ì†Œ ì´ˆê¸°í™”
        if not hasattr(self, '_premium_models'):
            self._premium_models = {}
        
        if step_class not in self._premium_models:
            self._premium_models[step_class] = {}
        
        # í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë“±ë¡
        self._premium_models[step_class][model_name] = {
            "checkpoint": model_checkpoint,
            "info": model_info,
            "loaded_at": time.time()
        }
        
        # available_modelsì—ë„ ì¶”ê°€
        self.available_models[model_name] = {
            "name": model_name,
            "file_path": "premium_model",
            "size_mb": model_info.get("memory_requirement_gb", 0) * 1024,
            "step_class": step_class,
            "priority": 100,  # ìµœê³  ìš°ì„ ìˆœìœ„
            "loaded": True,
            "premium": True
        }
        
        self.logger.info(f"âœ… í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë“±ë¡ ì„±ê³µ: {model_name} ({step_class})")
        return True
        
    except Exception as e:
        self.logger.error(f"âŒ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")
        return False

def get_premium_model(self, step_class: str, model_name: str = None):
    """ë“±ë¡ëœ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
    if not hasattr(self, '_premium_models') or step_class not in self._premium_models:
        return None
    
    step_models = self._premium_models[step_class]
    
    if model_name:
        return step_models.get(model_name, {}).get("checkpoint")
    else:
        # ì²« ë²ˆì§¸ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë°˜í™˜
        if step_models:
            first_model = next(iter(step_models.values()))
            return first_model.get("checkpoint")
    
    return None
'''
    
    print("ğŸ”§ ModelLoaderì— í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë“±ë¡ ê¸°ëŠ¥ ì¶”ê°€ ì½”ë“œ:")
    print("="*50)
    print(registration_code)
    
    return registration_code

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ MyCloset AI Premium ëª¨ë¸ ìë™ ì—°ë™ ì‹œì‘!")
    
    # 1. ModelLoaderì— í”„ë¦¬ë¯¸ì—„ ê¸°ëŠ¥ ì¶”ê°€
    add_premium_model_registration_to_model_loader()
    
    # 2. í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ë“¤ ìë™ ì—°ë™
    results = await integrate_premium_models_to_existing_steps()
    
    # 3. ê²°ê³¼ ì¶œë ¥
    if "error" not in results:
        success_count = sum(1 for r in results.values() if r.get("success", False))
        print(f"\nğŸ‰ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—°ë™ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ!")
        
        if success_count > 0:
            print("\nğŸ”¥ ë‹¤ìŒ ë‹¨ê³„: ì‹¤ì œ AI ì‹œìŠ¤í…œ ì‹¤í–‰")
            print("cd backend")
            print("python -m app.main")
    else:
        print(f"âŒ ì—°ë™ ì‹¤íŒ¨: {results['error']}")

if __name__ == "__main__":
    # conda í™˜ê²½ í™•ì¸
    conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'none')
    print(f"ğŸ í˜„ì¬ conda í™˜ê²½: {conda_env}")
    
    if conda_env != 'mycloset-ai-clean':
        print("âš ï¸ ê¶Œì¥: conda activate mycloset-ai-clean")
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(main())