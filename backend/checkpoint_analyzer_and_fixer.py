#!/usr/bin/env python3
"""
π”¥ μ²΄ν¬ν¬μΈνΈ λ¶„μ„ λ° νΈν™μ„± μμ • λ„κµ¬
====================================

λ¨λ“  μ²΄ν¬ν¬μΈνΈ νμΌλ“¤μ„ λ¶„μ„ν•κ³  νΈν™μ„± λ¬Έμ λ¥Ό μμ •ν•λ” λ„κµ¬

Author: MyCloset AI Team
Date: 2025-08-08
Version: 1.0
"""

import os
import sys
import time
import logging
import traceback
import json
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, field
from datetime import datetime
import shutil

# PyTorch κ΄€λ ¨
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# NumPy κ΄€λ ¨
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class CheckpointInfo:
    """μ²΄ν¬ν¬μΈνΈ μ •λ³΄"""
    path: str
    size_mb: float = 0.0
    exists: bool = False
    valid: bool = False
    structure_type: str = "unknown"
    architecture_hints: List[str] = field(default_factory=list)
    issues: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    fixed: bool = False

class CheckpointAnalyzerAndFixer:
    """μ²΄ν¬ν¬μΈνΈ λ¶„μ„ λ° μμ • λ„κµ¬"""
    
    def __init__(self, root_dir: str = "."):
        self.root_dir = Path(root_dir)
        self.checkpoints = {}
        self.analysis_results = {}
        
    def find_all_checkpoints(self) -> List[str]:
        """λ¨λ“  μ²΄ν¬ν¬μΈνΈ νμΌ μ°ΎκΈ°"""
        checkpoint_files = []
        
        # λ‹¤μ–‘ν• ν™•μ¥μ κ²€μƒ‰
        extensions = ["*.pth", "*.pt", "*.safetensors", "*.ckpt", "*.bin"]
        
        for ext in extensions:
            files = list(self.root_dir.rglob(ext))
            checkpoint_files.extend([str(f) for f in files])
        
        # μ¤‘λ³µ μ κ±° λ° μ •λ ¬
        checkpoint_files = sorted(list(set(checkpoint_files)))
        
        logger.info(f"π” λ°κ²¬λ μ²΄ν¬ν¬μΈνΈ νμΌ: {len(checkpoint_files)}κ°")
        return checkpoint_files
    
    def analyze_checkpoint(self, checkpoint_path: str) -> CheckpointInfo:
        """κ°λ³„ μ²΄ν¬ν¬μΈνΈ λ¶„μ„"""
        info = CheckpointInfo(path=checkpoint_path)
        
        try:
            if not Path(checkpoint_path).exists():
                info.issues.append("νμΌμ΄ μ΅΄μ¬ν•μ§€ μ•μ")
                return info
            
            info.exists = True
            info.size_mb = Path(checkpoint_path).stat().st_size / (1024 * 1024)
            
            # π”¥ λ‹¤μ–‘ν• λ΅λ”© λ°©λ²• μ‹λ„
            checkpoint = None
            loading_method = None
            
            # λ°©λ²• 1: weights_only=True (μ•μ „ν• λ°©λ²•)
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
                loading_method = 'weights_only_true'
                info.recommendations.append("μ•μ „ν• weights_only=Trueλ΅ λ΅λ”©λ¨")
            except Exception as e1:
                # λ°©λ²• 2: weights_only=False (μ „ν†µμ μΈ λ°©λ²•)
                try:
                    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                    loading_method = 'weights_only_false'
                    info.recommendations.append("weights_only=Falseλ΅ λ΅λ”©λ¨ (λ³΄μ• μ£Όμ)")
                except Exception as e2:
                    # λ°©λ²• 3: TorchScript λ¨λΈ
                    try:
                        checkpoint = torch.jit.load(checkpoint_path, map_location='cpu')
                        loading_method = 'torchscript'
                        info.recommendations.append("TorchScript λ¨λΈλ΅ λ΅λ”©λ¨")
                    except Exception as e3:
                        # λ°©λ²• 4: SafeTensors (λ³„λ„ λΌμ΄λΈλ¬λ¦¬ ν•„μ”)
                        try:
                            from safetensors import safe_open
                            with safe_open(checkpoint_path, framework="pt", device="cpu") as f:
                                checkpoint = {key: f.get_tensor(key) for key in f.keys()}
                            loading_method = 'safetensors'
                            info.recommendations.append("SafeTensorsλ΅ λ΅λ”©λ¨")
                        except Exception as e4:
                            # λ°©λ²• 5: SafeTensors (keys() λ©”μ„λ“ μ‚¬μ©)
                            try:
                                from safetensors import safe_open
                                with safe_open(checkpoint_path, framework="pt", device="cpu") as f:
                                    keys = list(f.keys())
                                    checkpoint = {key: f.get_tensor(key) for key in keys}
                                loading_method = 'safetensors_keys'
                                info.recommendations.append("SafeTensors (keys)λ΅ λ΅λ”©λ¨")
                            except Exception as e5:
                                info.issues.append(f"λ¨λ“  λ΅λ”© λ°©λ²• μ‹¤ν¨: {e5}")
                                return info
            
            # π”¥ κµ¬μ΅° νƒ€μ… λ¶„λ¥ λ° κ²€μ¦
            if isinstance(checkpoint, dict):
                info.structure_type = 'dict'
                
                # λ‹¤μ–‘ν• κµ¬μ΅° νƒ€μ… μ²λ¦¬
                if 'state_dict' in checkpoint:
                    # ν‘μ¤€ PyTorch λ¨λΈ
                    info.structure_type = 'state_dict'
                    state_dict = checkpoint['state_dict']
                    
                    # μ•„ν‚¤ν…μ² κ°μ§€
                    info.architecture_hints = self._detect_architecture_from_keys(state_dict.keys())
                    info.valid = True
                    info.recommendations.append("ν‘μ¤€ state_dict κµ¬μ΅°")
                    
                elif 'model' in checkpoint:
                    # λ¨λΈ λνΌ κµ¬μ΅°
                    info.structure_type = 'model_wrapper'
                    info.valid = True
                    info.recommendations.append("λ¨λΈ λνΌ κµ¬μ΅°")
                    
                elif 'weights' in checkpoint:
                    # κ°€μ¤‘μΉλ§ μλ” κµ¬μ΅°
                    info.structure_type = 'weights_only'
                    info.valid = True
                    info.recommendations.append("κ°€μ¤‘μΉ μ „μ© κµ¬μ΅°")
                    
                elif 'parameters' in checkpoint:
                    # νλΌλ―Έν„°λ§ μλ” κµ¬μ΅°
                    info.structure_type = 'parameters_only'
                    info.valid = True
                    info.recommendations.append("νλΌλ―Έν„° μ „μ© κµ¬μ΅°")
                    
                else:
                    # μ»¤μ¤ν…€ λ”•μ…”λ„λ¦¬ κµ¬μ΅°
                    info.structure_type = 'custom_dict'
                    
                    # μ»¤μ¤ν…€ κµ¬μ΅°μ—μ„λ„ νλΌλ―Έν„° μ°ΎκΈ° μ‹λ„
                    total_params = 0
                    param_keys = []
                    
                    # π”¥ μ¤‘μ²©λ κµ¬μ΅° μ²λ¦¬ (RealESRGAN λ“±)
                    def extract_tensors(obj, prefix=""):
                        nonlocal total_params, param_keys
                        if isinstance(obj, torch.Tensor):
                            total_params += obj.numel()
                            param_keys.append(prefix)
                        elif isinstance(obj, dict):
                            for key, value in obj.items():
                                new_prefix = f"{prefix}.{key}" if prefix else key
                                extract_tensors(value, new_prefix)
                    
                    extract_tensors(checkpoint)
                    
                    if total_params > 0:
                        info.architecture_hints = self._detect_architecture_from_keys(param_keys)
                        info.valid = True
                        info.recommendations.append("μ»¤μ¤ν…€ κµ¬μ΅°μ—μ„ νλΌλ―Έν„° λ°κ²¬")
                    else:
                        info.issues.append("νλΌλ―Έν„°λ¥Ό μ°Ύμ„ μ μ—†μ")
                        info.recommendations.append("μ»¤μ¤ν…€ κµ¬μ΅° κ²€μ¦ ν•„μ”")
                        
            elif isinstance(checkpoint, torch.Tensor):
                # μ§μ ‘ ν…μ„ ν•νƒ
                info.structure_type = 'tensor'
                info.valid = True
                info.recommendations.append("μ§μ ‘ ν…μ„ ν•νƒ")
                
            elif hasattr(checkpoint, 'state_dict'):
                # TorchScript λ¨λΈ
                info.structure_type = 'torchscript'
                try:
                    state_dict = checkpoint.state_dict()
                    info.architecture_hints = self._detect_architecture_from_keys(state_dict.keys())
                    info.valid = True
                    info.recommendations.append("TorchScript λ¨λΈ")
                except Exception as e:
                    info.issues.append(f"TorchScript state_dict μ ‘κ·Ό μ‹¤ν¨: {e}")
                    
            else:
                info.structure_type = str(type(checkpoint))
                info.issues.append(f"μ§€μ›ν•μ§€ μ•λ” νƒ€μ…: {type(checkpoint)}")
                
        except Exception as e:
            info.issues.append(f"μ²΄ν¬ν¬μΈνΈ λ¶„μ„ μ¤‘ μ¤λ¥: {e}")
        
        return info
    
    def _detect_architecture_from_keys(self, keys: List[str]) -> List[str]:
        """ν‚¤ λ©λ΅μ—μ„ μ•„ν‚¤ν…μ² κ°μ§€"""
        hints = []
        
        # ν™•μ¥λ μ•„ν‚¤ν…μ² ν‚¤μ›λ“
        architecture_keywords = {
            'graphonomy': ['backbone', 'decoder', 'classifier', 'schp', 'hrnet'],
            'u2net': ['stage1', 'stage2', 'stage3', 'stage4', 'side', 'u2net'],
            'deeplabv3plus': ['backbone', 'decoder', 'classifier', 'aspp', 'deeplab'],
            'gmm': ['feature_extraction', 'regression', 'gmm', 'geometric', 'pretrained.model'],
            'tps': ['localization_net', 'grid_generator', 'tps', 'transformation'],
            'raft': ['feature_encoder', 'context_encoder', 'flow_head', 'raft'],
            'sam': ['image_encoder', 'prompt_encoder', 'mask_decoder', 'sam'],
            'stable_diffusion': ['unet', 'vae', 'text_encoder', 'diffusion', 'model'],
            'ootd': ['unet_vton', 'unet_garm', 'vae', 'ootd'],
            'real_esrgan': ['body', 'upsampling', 'esrgan', 'real_esrgan'],
            'swinir': ['layers', 'patch_embed', 'norm', 'swin', 'swinir'],
            'clip': ['visual', 'transformer', 'text_projection', 'clip'],
            'vit': ['cls_token', 'pos_embed', 'patch_embed', 'blocks', 'attn', 'mlp'],
            'hrnet': ['hrnet', 'stage', 'transition', 'hrnet_w'],
            'openpose': ['pose', 'body', 'hand', 'face', 'openpose'],
            'yolo': ['yolo', 'detect', 'anchor', 'yolov'],
            'mediapipe': ['mediapipe', 'landmark', 'pose'],
            'viton': ['viton', 'vton', 'warping', 'tom'],
            'dpt': ['dpt', 'depth', 'midas'],
            'efficientnet': ['efficientnet', 'efficient'],
            'resnet': ['resnet', 'residual'],
            'mobilenet': ['mobilenet', 'mobile'],
            'densenet': ['densenet', 'dense'],
            'unet': ['down_blocks', 'up_blocks', 'conv_in', 'conv_out', 'time_embedding'],
            'diffusion': ['down_blocks', 'up_blocks', 'time_embedding', 'conv_in', 'conv_out']
        }
        
        for arch_name, keywords in architecture_keywords.items():
            matches = sum(1 for keyword in keywords if any(keyword.lower() in key.lower() for key in keys))
            if matches > 0:
                hints.append(f"{arch_name} (λ§¤μΉ­: {matches}κ°)")
        
        return hints
    
    def analyze_all_checkpoints(self) -> Dict[str, Any]:
        """λ¨λ“  μ²΄ν¬ν¬μΈνΈ λ¶„μ„"""
        print("π” λ¨λ“  μ²΄ν¬ν¬μΈνΈ νμΌ κ²€μƒ‰ μ¤‘...")
        checkpoint_files = self.find_all_checkpoints()
        
        print(f"π“ μ΄ {len(checkpoint_files)}κ°μ μ²΄ν¬ν¬μΈνΈ νμΌ λ°κ²¬")
        
        results = {
            'analysis_time': datetime.now().isoformat(),
            'total_checkpoints': len(checkpoint_files),
            'valid_checkpoints': 0,
            'invalid_checkpoints': 0,
            'checkpoints': {},
            'architecture_summary': {},
            'issues_summary': {},
            'recommendations': []
        }
        
        for i, checkpoint_path in enumerate(checkpoint_files, 1):
            print(f"π” λ¶„μ„ μ¤‘... ({i}/{len(checkpoint_files)}): {Path(checkpoint_path).name}")
            
            info = self.analyze_checkpoint(checkpoint_path)
            self.checkpoints[checkpoint_path] = info
            
            # κ²°κ³Ό μ €μ¥
            results['checkpoints'][checkpoint_path] = {
                'size_mb': info.size_mb,
                'exists': info.exists,
                'valid': info.valid,
                'structure_type': info.structure_type,
                'architecture_hints': info.architecture_hints,
                'issues': info.issues,
                'recommendations': info.recommendations
            }
            
            # ν†µκ³„ μ—…λ°μ΄νΈ
            if info.valid:
                results['valid_checkpoints'] += 1
            else:
                results['invalid_checkpoints'] += 1
            
            # μ•„ν‚¤ν…μ² μ”μ•½
            for hint in info.architecture_hints:
                arch_name = hint.split(' (')[0]
                if arch_name not in results['architecture_summary']:
                    results['architecture_summary'][arch_name] = 0
                results['architecture_summary'][arch_name] += 1
            
            # λ¬Έμ μ  μ”μ•½
            for issue in info.issues:
                if issue not in results['issues_summary']:
                    results['issues_summary'][issue] = 0
                results['issues_summary'][issue] += 1
        
        return results
    
    def fix_checkpoint_compatibility(self, checkpoint_path: str) -> bool:
        """μ²΄ν¬ν¬μΈνΈ νΈν™μ„± μμ •"""
        if checkpoint_path not in self.checkpoints:
            print(f"β μ²΄ν¬ν¬μΈνΈ μ •λ³΄κ°€ μ—†μ: {checkpoint_path}")
            return False
        
        info = self.checkpoints[checkpoint_path]
        
        if not info.exists:
            print(f"β νμΌμ΄ μ΅΄μ¬ν•μ§€ μ•μ: {checkpoint_path}")
            return False
        
        if info.valid:
            print(f"β… μ΄λ―Έ μ ν¨ν• μ²΄ν¬ν¬μΈνΈ: {checkpoint_path}")
            return True
        
        print(f"π”§ νΈν™μ„± μμ • μ‹λ„: {checkpoint_path}")
        
        try:
            # λ°±μ—… μƒμ„±
            backup_path = f"{checkpoint_path}.backup"
            if not Path(backup_path).exists():
                shutil.copy2(checkpoint_path, backup_path)
                print(f"π“¦ λ°±μ—… μƒμ„±: {backup_path}")
            
            # μ²΄ν¬ν¬μΈνΈ λ΅λ”©
            checkpoint = None
            
            # λ‹¤μ–‘ν• λ΅λ”© λ°©λ²• μ‹λ„
            for method in ['weights_only_true', 'weights_only_false', 'torchscript', 'safetensors']:
                try:
                    if method == 'weights_only_true':
                        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
                    elif method == 'weights_only_false':
                        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                    elif method == 'torchscript':
                        checkpoint = torch.jit.load(checkpoint_path, map_location='cpu')
                    elif method == 'safetensors':
                        from safetensors import safe_open
                        with safe_open(checkpoint_path, framework="pt", device="cpu") as f:
                            checkpoint = {key: f.get_tensor(key) for key in f.keys()}
                    
                    print(f"β… {method}λ΅ λ΅λ”© μ„±κ³µ")
                    break
                except Exception as e:
                    print(f"β {method} λ΅λ”© μ‹¤ν¨: {e}")
                    continue
            
            if checkpoint is None:
                print(f"β λ¨λ“  λ΅λ”© λ°©λ²• μ‹¤ν¨")
                return False
            
            # νΈν™μ„± μμ •
            fixed_checkpoint = self._fix_checkpoint_structure(checkpoint, info)
            
            if fixed_checkpoint is not None:
                # μμ •λ μ²΄ν¬ν¬μΈνΈ μ €μ¥
                torch.save(fixed_checkpoint, checkpoint_path)
                print(f"β… νΈν™μ„± μμ • μ™„λ£: {checkpoint_path}")
                
                # μ •λ³΄ μ—…λ°μ΄νΈ
                info.fixed = True
                info.valid = True
                info.recommendations.append("νΈν™μ„± μμ • μ™„λ£")
                
                return True
            else:
                print(f"β νΈν™μ„± μμ • μ‹¤ν¨")
                return False
                
        except Exception as e:
            print(f"β νΈν™μ„± μμ • μ¤‘ μ¤λ¥: {e}")
            return False
    
    def _fix_checkpoint_structure(self, checkpoint: Any, info: CheckpointInfo) -> Optional[Any]:
        """μ²΄ν¬ν¬μΈνΈ κµ¬μ΅° μμ •"""
        try:
            if isinstance(checkpoint, dict):
                # 1. state_dict ν‚¤ λ§¤ν•‘ μμ •
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                    fixed_state_dict = self._fix_state_dict_keys(state_dict)
                    checkpoint['state_dict'] = fixed_state_dict
                    return checkpoint
                
                # 2. μ§μ ‘ λ”•μ…”λ„λ¦¬μΈ κ²½μ°
                else:
                    fixed_dict = self._fix_state_dict_keys(checkpoint)
                    return {'state_dict': fixed_dict}
            
            elif hasattr(checkpoint, 'state_dict'):
                # TorchScript λ¨λΈ
                state_dict = checkpoint.state_dict()
                fixed_state_dict = self._fix_state_dict_keys(state_dict)
                return {'state_dict': fixed_state_dict}
            
            else:
                return checkpoint
                
        except Exception as e:
            print(f"β κµ¬μ΅° μμ • μ¤‘ μ¤λ¥: {e}")
            return None
    
    def _fix_state_dict_keys(self, state_dict: Dict[str, Any]) -> Dict[str, Any]:
        """state_dict ν‚¤ μμ •"""
        fixed_state_dict = {}
        
        # ν‚¤ λ§¤ν•‘ κ·μΉ™
        key_mappings = {
            # TPS κ΄€λ ¨ ν‚¤ λ§¤ν•‘
            'control_points': 'tps_control_points',
            'weights': 'tps_weights', 
            'affine_params': 'tps_affine_params',
            
            # GMM κ΄€λ ¨ ν‚¤ λ§¤ν•‘
            'pretrained.model': 'backbone',
            'feature_extraction': 'encoder',
            'regression': 'decoder',
            
            # μΌλ°μ μΈ ν‚¤ λ§¤ν•‘
            'module.': '',  # DataParallel μ κ±°
            'model.': '',   # λ¨λΈ λνΌ μ κ±°
        }
        
        for key, tensor in state_dict.items():
            new_key = key
            
            # ν‚¤ λ§¤ν•‘ μ μ©
            for old_pattern, new_pattern in key_mappings.items():
                if old_pattern in new_key:
                    new_key = new_key.replace(old_pattern, new_pattern)
                    break
            
            fixed_state_dict[new_key] = tensor
        
        return fixed_state_dict
    
    def generate_compatibility_report(self) -> str:
        """νΈν™μ„± λ¦¬ν¬νΈ μƒμ„±"""
        report = []
        report.append("π”¥ μ²΄ν¬ν¬μΈνΈ νΈν™μ„± λ¶„μ„ λ¦¬ν¬νΈ")
        report.append("=" * 60)
        report.append(f"π“… λ¶„μ„ μ‹κ°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # μ „μ²΄ ν†µκ³„
        total = len(self.checkpoints)
        valid = sum(1 for info in self.checkpoints.values() if info.valid)
        invalid = total - valid
        
        report.append(f"π“ μ „μ²΄ μ²΄ν¬ν¬μΈνΈ: {total}κ°")
        report.append(f"β… μ ν¨ν• μ²΄ν¬ν¬μΈνΈ: {valid}κ°")
        report.append(f"β λ¬΄ν¨ν• μ²΄ν¬ν¬μΈνΈ: {invalid}κ°")
        report.append("")
        
        # μ•„ν‚¤ν…μ²λ³„ μ”μ•½
        architecture_counts = {}
        for info in self.checkpoints.values():
            for hint in info.architecture_hints:
                arch_name = hint.split(' (')[0]
                architecture_counts[arch_name] = architecture_counts.get(arch_name, 0) + 1
        
        if architecture_counts:
            report.append("π—οΈ μ•„ν‚¤ν…μ²λ³„ λ¶„ν¬:")
            for arch, count in sorted(architecture_counts.items(), key=lambda x: x[1], reverse=True):
                report.append(f"   {arch}: {count}κ°")
            report.append("")
        
        # λ¬Έμ μ λ³„ μ”μ•½
        issue_counts = {}
        for info in self.checkpoints.values():
            for issue in info.issues:
                issue_counts[issue] = issue_counts.get(issue, 0) + 1
        
        if issue_counts:
            report.append("π¨ μ£Όμ” λ¬Έμ μ :")
            for issue, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
                report.append(f"   {issue}: {count}κ°")
            report.append("")
        
        # μƒμ„Έ λ¶„μ„ κ²°κ³Ό
        report.append("π“‹ μƒμ„Έ λ¶„μ„ κ²°κ³Ό:")
        for checkpoint_path, info in self.checkpoints.items():
            status = "β…" if info.valid else "β"
            report.append(f"{status} {Path(checkpoint_path).name}")
            report.append(f"   π“ ν¬κΈ°: {info.size_mb:.1f}MB")
            report.append(f"   π—οΈ κµ¬μ΅°: {info.structure_type}")
            
            if info.architecture_hints:
                report.append(f"   π›οΈ μ•„ν‚¤ν…μ²: {', '.join(info.architecture_hints)}")
            
            if info.issues:
                report.append(f"   β οΈ λ¬Έμ μ : {', '.join(info.issues)}")
            
            if info.recommendations:
                report.append(f"   π’΅ κ¶μ¥μ‚¬ν•­: {', '.join(info.recommendations)}")
            
            report.append("")
        
        return "\n".join(report)
    
    def save_analysis_results(self, output_path: str = "checkpoint_analysis_results.json"):
        """λ¶„μ„ κ²°κ³Ό μ €μ¥"""
        results = {
            'analysis_time': datetime.now().isoformat(),
            'total_checkpoints': len(self.checkpoints),
            'valid_checkpoints': sum(1 for info in self.checkpoints.values() if info.valid),
            'invalid_checkpoints': sum(1 for info in self.checkpoints.values() if not info.valid),
            'checkpoints': {}
        }
        
        for checkpoint_path, info in self.checkpoints.items():
            results['checkpoints'][checkpoint_path] = {
                'size_mb': info.size_mb,
                'exists': info.exists,
                'valid': info.valid,
                'structure_type': info.structure_type,
                'architecture_hints': info.architecture_hints,
                'issues': info.issues,
                'recommendations': info.recommendations,
                'fixed': info.fixed
            }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"π’Ύ λ¶„μ„ κ²°κ³Ό μ €μ¥: {output_path}")

def main():
    """λ©”μΈ ν•¨μ"""
    print("π”¥ μ²΄ν¬ν¬μΈνΈ λ¶„μ„ λ° νΈν™μ„± μμ • λ„κµ¬")
    print("=" * 60)
    
    # λ¶„μ„κΈ° μ΄κΈ°ν™”
    analyzer = CheckpointAnalyzerAndFixer()
    
    # 1. λ¨λ“  μ²΄ν¬ν¬μΈνΈ λ¶„μ„
    print("\nπ“‹ 1λ‹¨κ³„: λ¨λ“  μ²΄ν¬ν¬μΈνΈ λ¶„μ„")
    results = analyzer.analyze_all_checkpoints()
    
    # 2. λ¶„μ„ κ²°κ³Ό μ¶λ ¥
    print("\nπ“ λ¶„μ„ κ²°κ³Ό:")
    print(f"   μ΄ μ²΄ν¬ν¬μΈνΈ: {results['total_checkpoints']}κ°")
    print(f"   μ ν¨ν• μ²΄ν¬ν¬μΈνΈ: {results['valid_checkpoints']}κ°")
    print(f"   λ¬΄ν¨ν• μ²΄ν¬ν¬μΈνΈ: {results['invalid_checkpoints']}κ°")
    
    # 3. νΈν™μ„± μμ • μ‹λ„
    print("\nπ”§ 2λ‹¨κ³„: νΈν™μ„± μμ • μ‹λ„")
    fixed_count = 0
    
    for checkpoint_path, info in analyzer.checkpoints.items():
        if not info.valid and info.exists:
            if analyzer.fix_checkpoint_compatibility(checkpoint_path):
                fixed_count += 1
    
    print(f"\nβ… μμ • μ™„λ£: {fixed_count}κ°")
    
    # 4. μµμΆ… λ¦¬ν¬νΈ μƒμ„±
    print("\nπ“‹ 3λ‹¨κ³„: μµμΆ… λ¦¬ν¬νΈ μƒμ„±")
    report = analyzer.generate_compatibility_report()
    print(report)
    
    # 5. κ²°κ³Ό μ €μ¥
    analyzer.save_analysis_results()
    
    print("\nπ‰ μ²΄ν¬ν¬μΈνΈ λ¶„μ„ λ° μμ • μ™„λ£!")

if __name__ == "__main__":
    main()
