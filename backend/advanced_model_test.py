#!/usr/bin/env python3
"""
ğŸš€ MyCloset AI ëª¨ë¸ ê³ ê¸‰ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- ëª¨ë“  AI ëª¨ë¸ì˜ ì‹¤ì œ ë¡œë”© í…ŒìŠ¤íŠ¸
- M3 Max ìµœì í™” í™•ì¸
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬
"""

import os
import sys
import time
import psutil
import gc
from pathlib import Path
from typing import Dict, Any, Optional
import warnings
warnings.filterwarnings("ignore")

def log_info(msg: str):
    print(f"â„¹ï¸  {msg}")

def log_success(msg: str):
    print(f"âœ… {msg}")

def log_warning(msg: str):
    print(f"âš ï¸  {msg}")

def log_error(msg: str):
    print(f"âŒ {msg}")

def log_benchmark(msg: str):
    print(f"â±ï¸  {msg}")

class AdvancedModelTester:
    """ê³ ê¸‰ AI ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.base_dir = Path("ai_models")
        self.checkpoints_dir = self.base_dir / "checkpoints"
        self.device = self._detect_device()
        self.test_results = {}
        self.memory_baseline = self._get_memory_usage()
        
    def _detect_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            import torch
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        except ImportError:
            return "cpu"
    
    def _get_memory_usage(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"""
        memory = psutil.virtual_memory()
        return {
            "total_gb": memory.total / (1024**3),
            "used_gb": memory.used / (1024**3),
            "available_gb": memory.available / (1024**3),
            "percent": memory.percent
        }
    
    def _measure_time_and_memory(self, func, *args, **kwargs):
        """ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        
        start_memory = self._get_memory_usage()
        start_time = time.time()
        
        try:
            result = func(*args, **kwargs)
            success = True
            error = None
        except Exception as e:
            result = None
            success = False
            error = str(e)
        
        end_time = time.time()
        end_memory = self._get_memory_usage()
        
        return {
            "result": result,
            "success": success,
            "error": error,
            "execution_time": end_time - start_time,
            "memory_used_mb": (end_memory["used_gb"] - start_memory["used_gb"]) * 1024,
            "peak_memory_gb": end_memory["used_gb"]
        }
    
    def test_segformer_human_parsing(self):
        """Segformer ì¸ì²´ íŒŒì‹± í…ŒìŠ¤íŠ¸"""
        log_info("Segformer ì¸ì²´ íŒŒì‹± í…ŒìŠ¤íŠ¸...")
        
        def load_segformer():
            from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation
            
            model_path = self.checkpoints_dir / "step_01_human_parsing/segformer_b2_clothes"
            processor = SegformerImageProcessor.from_pretrained(str(model_path))
            model = SegformerForSemanticSegmentation.from_pretrained(str(model_path))
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if self.device != "cpu":
                model = model.to(self.device)
            
            # ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
            import torch
            import numpy as np
            from PIL import Image
            
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (512x512 RGB)
            dummy_image = Image.fromarray(np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8))
            inputs = processor(images=dummy_image, return_tensors="pt")
            
            if self.device != "cpu":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
            
            return {
                "model_size_mb": sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2),
                "output_shape": logits.shape,
                "num_classes": logits.shape[1] if len(logits.shape) > 1 else 0,
                "device": str(next(model.parameters()).device)
            }
        
        result = self._measure_time_and_memory(load_segformer)
        
        if result["success"]:
            info = result["result"]
            log_success(f"Segformer: {info['num_classes']}í´ë˜ìŠ¤, {info['model_size_mb']:.1f}MB, "
                       f"{result['execution_time']:.1f}ì´ˆ, ë©”ëª¨ë¦¬: +{result['memory_used_mb']:.0f}MB")
            log_info(f"  ë””ë°”ì´ìŠ¤: {info['device']}, ì¶œë ¥: {info['output_shape']}")
        else:
            log_error(f"Segformer ì‹¤íŒ¨: {result['error']}")
        
        self.test_results["segformer"] = result
        return result["success"]
    
    def test_u2net_models(self):
        """UÂ²-Net ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸"""
        log_info("UÂ²-Net ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
        
        # PyTorch ë²„ì „ í…ŒìŠ¤íŠ¸
        def test_pytorch_u2net():
            import torch
            model_path = self.checkpoints_dir / "step_03_cloth_segmentation/u2net.pth"
            
            # ëª¨ë¸ ë¡œë“œ (ì‹¤ì œ UÂ²-Net êµ¬ì¡°ê°€ í•„ìš”í•˜ì§€ë§Œ ì²´í¬í¬ì¸íŠ¸ë§Œ í™•ì¸)
            checkpoint = torch.load(model_path, map_location=self.device)
            
            return {
                "checkpoint_keys": list(checkpoint.keys()) if isinstance(checkpoint, dict) else ["single_tensor"],
                "file_size_mb": model_path.stat().st_size / (1024**2)
            }
        
        # ONNX ë²„ì „ í…ŒìŠ¤íŠ¸
        def test_onnx_u2net():
            import onnxruntime as ort
            
            model_path = self.checkpoints_dir / "step_03_cloth_segmentation/u2net.onnx"
            
            # ONNX ì„¸ì…˜ ìƒì„±
            providers = ['CPUExecutionProvider']
            if self.device == "cuda":
                providers.insert(0, 'CUDAExecutionProvider')
            
            session = ort.InferenceSession(str(model_path), providers=providers)
            
            # ì…ë ¥/ì¶œë ¥ ì •ë³´
            input_info = session.get_inputs()[0]
            output_info = session.get_outputs()[0]
            
            # ë”ë¯¸ ì¶”ë¡ 
            import numpy as np
            dummy_input = np.random.randn(*input_info.shape).astype(np.float32)
            output = session.run([output_info.name], {input_info.name: dummy_input})
            
            return {
                "input_shape": input_info.shape,
                "output_shape": output[0].shape,
                "input_type": input_info.type,
                "providers": session.get_providers(),
                "file_size_mb": model_path.stat().st_size / (1024**2)
            }
        
        # PyTorch í…ŒìŠ¤íŠ¸
        pytorch_result = self._measure_time_and_memory(test_pytorch_u2net)
        if pytorch_result["success"]:
            info = pytorch_result["result"]
            log_success(f"UÂ²-Net PyTorch: {info['file_size_mb']:.1f}MB, "
                       f"{pytorch_result['execution_time']:.1f}ì´ˆ")
        else:
            log_error(f"UÂ²-Net PyTorch ì‹¤íŒ¨: {pytorch_result['error']}")
        
        # ONNX í…ŒìŠ¤íŠ¸
        onnx_result = self._measure_time_and_memory(test_onnx_u2net)
        if onnx_result["success"]:
            info = onnx_result["result"]
            log_success(f"UÂ²-Net ONNX: {info['input_shape']} â†’ {info['output_shape']}, "
                       f"{onnx_result['execution_time']:.1f}ì´ˆ")
            log_info(f"  Providers: {', '.join(info['providers'])}")
        else:
            log_error(f"UÂ²-Net ONNX ì‹¤íŒ¨: {onnx_result['error']}")
        
        self.test_results["u2net_pytorch"] = pytorch_result
        self.test_results["u2net_onnx"] = onnx_result
        
        return pytorch_result["success"] and onnx_result["success"]
    
    def test_mediapipe_pose(self):
        """MediaPipe í¬ì¦ˆ ì¶”ì • í…ŒìŠ¤íŠ¸"""
        log_info("MediaPipe í¬ì¦ˆ ì¶”ì • í…ŒìŠ¤íŠ¸...")
        
        def test_mediapipe():
            import mediapipe as mp
            import numpy as np
            import cv2
            
            # MediaPipe í¬ì¦ˆ ëª¨ë¸ ë¡œë“œ
            mp_pose = mp.solutions.pose
            
            # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ì‚¬ëŒ í˜•íƒœì™€ ìœ ì‚¬í•˜ê²Œ)
            dummy_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            # ì¤‘ì•™ì— ì‚¬ëŒ í˜•íƒœ íŒ¨í„´ ì¶”ê°€ (ëœë“œë§ˆí¬ ê°ì§€ í™•ë¥  ë†’ì´ê¸°)
            dummy_image[150:350, 250:450] = [100, 150, 200]  # ì‚¬ëŒ ëª¸ì²´ ì˜ì—­
            
            with mp_pose.Pose(
                static_image_mode=True,
                model_complexity=1,
                enable_segmentation=False,
                min_detection_confidence=0.3  # ë”ë¯¸ ì´ë¯¸ì§€ì´ë¯€ë¡œ ë‚®ì€ ì„ê³„ê°’
            ) as pose:
                # BGR to RGB ë³€í™˜
                rgb_image = cv2.cvtColor(dummy_image, cv2.COLOR_BGR2RGB)
                results = pose.process(rgb_image)
            
            return {
                "pose_detected": results.pose_landmarks is not None,
                "num_landmarks": len(results.pose_landmarks.landmark) if results.pose_landmarks else 0,
                "image_shape": dummy_image.shape,
                "mediapipe_version": getattr(mp, '__version__', 'unknown'),
                "opencv_version": getattr(cv2, '__version__', 'unknown'),
                "detection_confidence": 0.3,
                "model_complexity": 1
            }
        
        result = self._measure_time_and_memory(test_mediapipe)
        
        if result["success"]:
            info = result["result"]
            log_success(f"MediaPipe í¬ì¦ˆ: {info['num_landmarks']}ê°œ ëœë“œë§ˆí¬, "
                       f"{result['execution_time']:.1f}ì´ˆ")
            log_info(f"  MediaPipe: {info['mediapipe_version']}, "
                    f"OpenCV: {info['opencv_version']}")
            if info['pose_detected']:
                log_info(f"  í¬ì¦ˆ ê°ì§€ ì„±ê³µ (33ê°œ ì¤‘ {info['num_landmarks']}ê°œ)")
            else:
                log_info(f"  ë”ë¯¸ ì´ë¯¸ì§€ì—ì„œ í¬ì¦ˆ ë¯¸ê°ì§€ (ì •ìƒ)")
        else:
            log_error(f"MediaPipe ì‹¤íŒ¨: {result['error']}")
        
        self.test_results["mediapipe"] = result
        return result["success"]
    
    def test_clip_models(self):
        """CLIP ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸"""
        log_info("CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
        
        def test_clip_base():
            from transformers import CLIPModel, CLIPProcessor
            import torch
            
            model_path = self.base_dir / "clip-vit-base-patch32"
            
            processor = CLIPProcessor.from_pretrained(str(model_path))
            model = CLIPModel.from_pretrained(str(model_path))
            
            if self.device != "cpu":
                model = model.to(self.device)
            
            # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì„ë² ë”© í…ŒìŠ¤íŠ¸
            import numpy as np
            from PIL import Image
            
            dummy_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
            dummy_text = ["a person wearing clothes"]
            
            inputs = processor(text=dummy_text, images=dummy_image, return_tensors="pt", padding=True)
            
            if self.device != "cpu":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs)
                image_embeds = outputs.image_embeds
                text_embeds = outputs.text_embeds
            
            return {
                "model_size_mb": sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2),
                "image_embed_dim": image_embeds.shape[-1],
                "text_embed_dim": text_embeds.shape[-1],
                "device": str(next(model.parameters()).device)
            }
        
        result = self._measure_time_and_memory(test_clip_base)
        
        if result["success"]:
            info = result["result"]
            log_success(f"CLIP Base: {info['model_size_mb']:.1f}MB, "
                       f"ì„ë² ë”© ì°¨ì›: {info['image_embed_dim']}, {result['execution_time']:.1f}ì´ˆ")
            log_info(f"  ë””ë°”ì´ìŠ¤: {info['device']}")
        else:
            log_error(f"CLIP ì‹¤íŒ¨: {result['error']}")
        
        self.test_results["clip"] = result
        return result["success"]
    
    def test_ootdiffusion_checkpoint(self):
        """OOTDiffusion ì²´í¬í¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""
        log_info("OOTDiffusion ì²´í¬í¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸...")
        
        def test_ootd():
            import torch
            
            # UNet ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ í™•ì¸
            unet_garm_path = (self.checkpoints_dir / 
                            "ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/"
                            "unet_garm/diffusion_pytorch_model.safetensors")
            
            unet_vton_path = (self.checkpoints_dir / 
                            "ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/"
                            "unet_vton/diffusion_pytorch_model.safetensors")
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ë¡œë”©ì€ ë©”ëª¨ë¦¬ê°€ ë§ì´ í•„ìš”í•˜ë¯€ë¡œ íŒŒì¼ë§Œ í™•ì¸)
            garm_exists = unet_garm_path.exists()
            vton_exists = unet_vton_path.exists()
            
            sizes = {}
            if garm_exists:
                sizes["garm_mb"] = unet_garm_path.stat().st_size / (1024**2)
            if vton_exists:
                sizes["vton_mb"] = unet_vton_path.stat().st_size / (1024**2)
            
            return {
                "garm_available": garm_exists,
                "vton_available": vton_exists,
                "sizes": sizes,
                "total_checkpoints": len(list((self.checkpoints_dir / "ootdiffusion").rglob("*.safetensors")))
            }
        
        result = self._measure_time_and_memory(test_ootd)
        
        if result["success"]:
            info = result["result"]
            log_success(f"OOTDiffusion: GARM({info['garm_available']}), "
                       f"VTON({info['vton_available']}), "
                       f"ì´ {info['total_checkpoints']}ê°œ ì²´í¬í¬ì¸íŠ¸")
            if info["sizes"]:
                size_info = ", ".join([f"{k}: {v:.0f}MB" for k, v in info["sizes"].items()])
                log_info(f"  í¬ê¸°: {size_info}")
        else:
            log_error(f"OOTDiffusion ì‹¤íŒ¨: {result['error']}")
        
        self.test_results["ootdiffusion"] = result
        return result["success"]
    
    def benchmark_performance(self):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        log_info("ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰...")
        
        # ë””ë°”ì´ìŠ¤ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        if self.device == "mps":
            log_benchmark("M3 Max MPS ìµœì í™” í…ŒìŠ¤íŠ¸")
            try:
                import torch
                
                # PyTorch MPS ì§€ì› í™•ì¸
                if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    # MPS ê¸°ë³¸ ì—°ì‚° í…ŒìŠ¤íŠ¸
                    x = torch.randn(1000, 1000, device=self.device)
                    y = torch.randn(1000, 1000, device=self.device)
                    
                    start_time = time.time()
                    for _ in range(100):
                        z = torch.matmul(x, y)
                    
                    # MPS ë™ê¸°í™” (ìˆëŠ” ê²½ìš°)
                    if hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                    
                    end_time = time.time()
                    
                    log_benchmark(f"MPS í–‰ë ¬ ê³±ì…ˆ (1000x1000, 100íšŒ): {end_time - start_time:.2f}ì´ˆ")
                    
                    # ë©”ëª¨ë¦¬ ì •ë³´
                    if hasattr(torch.mps, 'current_allocated_memory'):
                        mps_memory = torch.mps.current_allocated_memory() / (1024**2)
                        log_benchmark(f"MPS ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {mps_memory:.0f}MB")
                else:
                    log_warning("MPS ì§€ì›ë˜ì§€ ì•ŠìŒ - PyTorch ë²„ì „ í™•ì¸ í•„ìš”")
                    
            except Exception as e:
                log_warning(f"MPS ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        
        elif self.device == "cuda":
            log_benchmark("CUDA ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
            try:
                import torch
                
                # CUDA ê¸°ë³¸ ì—°ì‚° í…ŒìŠ¤íŠ¸
                x = torch.randn(1000, 1000, device=self.device)
                y = torch.randn(1000, 1000, device=self.device)
                
                start_time = time.time()
                for _ in range(100):
                    z = torch.matmul(x, y)
                torch.cuda.synchronize()
                end_time = time.time()
                
                log_benchmark(f"CUDA í–‰ë ¬ ê³±ì…ˆ (1000x1000, 100íšŒ): {end_time - start_time:.2f}ì´ˆ")
                
                # GPU ë©”ëª¨ë¦¬ ì •ë³´
                gpu_memory = torch.cuda.memory_allocated() / (1024**2)
                log_benchmark(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {gpu_memory:.0f}MB")
                
            except Exception as e:
                log_warning(f"CUDA ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        
        else:
            log_benchmark("CPU ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
            try:
                import torch
                
                # CPU ê¸°ë³¸ ì—°ì‚° í…ŒìŠ¤íŠ¸
                x = torch.randn(1000, 1000)
                y = torch.randn(1000, 1000)
                
                start_time = time.time()
                for _ in range(100):
                    z = torch.matmul(x, y)
                end_time = time.time()
                
                log_benchmark(f"CPU í–‰ë ¬ ê³±ì…ˆ (1000x1000, 100íšŒ): {end_time - start_time:.2f}ì´ˆ")
                
            except Exception as e:
                log_warning(f"CPU ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        current_memory = self._get_memory_usage()
        memory_used = current_memory["used_gb"] - self.memory_baseline["used_gb"]
        
        log_benchmark(f"ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: +{memory_used:.1f}GB")
        log_benchmark(f"í˜„ì¬ ë©”ëª¨ë¦¬: {current_memory['used_gb']:.1f}GB / {current_memory['total_gb']:.1f}GB "
                     f"({current_memory['percent']:.1f}%)")
    
    def generate_report(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        log_info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±...")
        
        successful_tests = sum(1 for result in self.test_results.values() if result["success"])
        total_tests = len(self.test_results)
        
        report = {
            "device": self.device,
            "success_rate": f"{successful_tests}/{total_tests}",
            "total_execution_time": sum(r["execution_time"] for r in self.test_results.values()),
            "total_memory_used_mb": sum(r["memory_used_mb"] for r in self.test_results.values()),
            "system_info": {
                "memory": self._get_memory_usage(),
                "cpu_count": psutil.cpu_count(),
                "platform": sys.platform
            },
            "test_details": self.test_results
        }
        
        # JSONìœ¼ë¡œ ì €ì¥
        import json
        report_path = Path("model_test_report.json")
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        log_success(f"ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        return report

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ MyCloset AI ëª¨ë¸ ê³ ê¸‰ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    tester = AdvancedModelTester()
    
    log_info(f"ë””ë°”ì´ìŠ¤: {tester.device}")
    log_info(f"ì´ˆê¸° ë©”ëª¨ë¦¬: {tester.memory_baseline['used_gb']:.1f}GB / "
             f"{tester.memory_baseline['total_gb']:.1f}GB")
    
    print("\nğŸ§ª AI ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
    print("=" * 30)
    
    # ê° ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tests = [
        ("Segformer ì¸ì²´ íŒŒì‹±", tester.test_segformer_human_parsing),
        ("UÂ²-Net ëª¨ë¸ë“¤", tester.test_u2net_models),
        ("MediaPipe í¬ì¦ˆ", tester.test_mediapipe_pose),
        ("CLIP ëª¨ë¸", tester.test_clip_models),
        ("OOTDiffusion", tester.test_ootdiffusion_checkpoint)
    ]
    
    successful_tests = 0
    for test_name, test_func in tests:
        try:
            log_info(f"{test_name} ì‹œì‘...")
            if test_func():
                successful_tests += 1
            print()  # ì¤„ë°”ê¿ˆ
        except KeyboardInterrupt:
            log_warning("ì‚¬ìš©ìê°€ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤")
            break
        except Exception as e:
            log_error(f"{test_name} ì˜ˆì™¸ ë°œìƒ: {e}")
    
    # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    print("âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 20)
    tester.benchmark_performance()
    
    # ê²°ê³¼ ë¦¬í¬íŠ¸
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("=" * 20)
    report = tester.generate_report()
    
    print(f"âœ… ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {successful_tests}/{len(tests)}")
    print(f"ğŸ• ì´ ì‹¤í–‰ ì‹œê°„: {report['total_execution_time']:.1f}ì´ˆ")
    print(f"ğŸ’¾ ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©: {report['total_memory_used_mb']:.0f}MB")
    
    if successful_tests == len(tests):
        print("\nğŸ‰ ëª¨ë“  AI ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤!")
        print("ğŸš€ ì´ì œ python3 run_server.py ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    else:
        print(f"\nâš ï¸  {len(tests) - successful_tests}ê°œ ëª¨ë¸ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
        print("ğŸ“ model_test_report.json íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  í…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        print(f"\n\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()