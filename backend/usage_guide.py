#!/usr/bin/env python3
"""
ğŸ¯ MyCloset-AI Model Architectures ì‚¬ìš© ê°€ì´ë“œ
================================================================================
âœ… ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©ë²•
âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
âœ… ëª¨ë¸ ê´€ë¦¬
================================================================================
"""

import sys
import os
import torch
import numpy as np
from pathlib import Path
import psutil

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

def basic_model_usage():
    """ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©ë²• - ì „ì²˜ë¦¬ ê°•í™”"""
    print("\n" + "="*60)
    print("ğŸ¯ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©ë²• (ì „ì²˜ë¦¬ ê°•í™”)")
    print("="*60)
    
    try:
        from app.ai_pipeline.utils.model_architectures import (
            OpenPoseModel, CompleteModelWrapper, OpenPosePreprocessor
        )
        
        # 1. ì§ì ‘ ëª¨ë¸ ì‚¬ìš© (ì „ì²˜ë¦¬ í¬í•¨)
        print("\nğŸ“Œ 1. ì§ì ‘ ëª¨ë¸ ì‚¬ìš© (ì „ì²˜ë¦¬ í¬í•¨)")
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (HWC í˜•íƒœ)
        dummy_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        print(f"   ì…ë ¥ ì´ë¯¸ì§€ í˜•íƒœ: {dummy_image.shape}")
        
        # ì „ì²˜ë¦¬ê¸° ìƒì„±
        preprocessor = OpenPosePreprocessor()
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        processed_tensor = preprocessor(dummy_image)
        print(f"   ì „ì²˜ë¦¬ í›„ í…ì„œ í˜•íƒœ: {processed_tensor.shape}")
        
        # ëª¨ë¸ ìƒì„± ë° ì¶”ë¡ 
        model = OpenPoseModel()
        with torch.no_grad():
            output = model(processed_tensor)
        
        print(f"   ëª¨ë¸ ì¶œë ¥ í˜•íƒœ: {output.shape}")
        print("   âœ… ì§ì ‘ ëª¨ë¸ ì‚¬ìš© ì„±ê³µ")
        
        # 2. CompleteModelWrapper ì‚¬ìš©
        print("\nğŸ“Œ 2. CompleteModelWrapper ì‚¬ìš©")
        
        # ë˜í¼ ìƒì„±
        wrapper = CompleteModelWrapper(model, 'openpose')
        
        # ë‹¤ì–‘í•œ ì…ë ¥ í˜•íƒœë¡œ í…ŒìŠ¤íŠ¸
        test_inputs = [
            dummy_image,  # NumPy ë°°ì—´
            "test_image.jpg",  # íŒŒì¼ ê²½ë¡œ (ì‹¤ì œ íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ ì˜ˆì™¸ ë°œìƒ ì˜ˆìƒ)
        ]
        
        for i, test_input in enumerate(test_inputs):
            try:
                if isinstance(test_input, str):
                    print(f"   í…ŒìŠ¤íŠ¸ {i+1}: íŒŒì¼ ê²½ë¡œ ì…ë ¥ (ì˜ˆì™¸ ë°œìƒ ì˜ˆìƒ)")
                else:
                    print(f"   í…ŒìŠ¤íŠ¸ {i+1}: NumPy ë°°ì—´ ì…ë ¥")
                
                result = wrapper(test_input)
                print(f"   ê²°ê³¼ í˜•íƒœ: {type(result)}")
                if isinstance(result, dict):
                    print(f"   ê²°ê³¼ í‚¤: {list(result.keys())}")
                print("   âœ… CompleteModelWrapper ì‚¬ìš© ì„±ê³µ")
                
            except Exception as e:
                if isinstance(test_input, str):
                    print(f"   âš ï¸ ì˜ˆìƒëœ ì˜¤ë¥˜ (íŒŒì¼ ì—†ìŒ): {e}")
                else:
                    print(f"   âŒ ì˜¤ë¥˜: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©ë²• ì‹¤íŒ¨: {e}")
        return False

def checkpoint_loading_usage():
    """2. ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‚¬ìš©ë²•"""
    print("\nğŸ¯ 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‚¬ìš©ë²•")
    print("=" * 50)
    
    try:
        from app.ai_pipeline.utils.model_architectures import (
            ModelArchitectureFactory,
            AdvancedKeyMapper
        )
        
        # 2-1. ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ë° ëª¨ë¸ ìƒì„±
        print("\nğŸ“Œ 2-1. ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ë° ëª¨ë¸ ìƒì„±")
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ (ì‹¤ì œ íŒŒì¼ì´ ìˆë‹¤ë©´)
        checkpoint_path = "./models/openpose_checkpoint.pth"
        
        # ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹¤í–‰
        if os.path.exists(checkpoint_path):
            # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„
            analysis = {
                'architecture_type': 'openpose',
                'model_name': 'openpose',
                'checkpoint_path': checkpoint_path
            }
            
            # ëª¨ë¸ ìƒì„±
            model = ModelArchitectureFactory.create_model_from_analysis(analysis)
            
            if model:
                print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                print(f"   - ëª¨ë¸ íƒ€ì…: {analysis['architecture_type']}")
                print(f"   - ëª¨ë¸ ì´ë¦„: {analysis['model_name']}")
            else:
                print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ì–´ì„œ ë”ë¯¸ ë¶„ì„ìœ¼ë¡œ ì§„í–‰")
        else:
            print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ì–´ì„œ ë”ë¯¸ ë¶„ì„ìœ¼ë¡œ ì§„í–‰")
        
        # 2-2. ê³ ê¸‰ í‚¤ ë§¤í•‘ ì‚¬ìš©
        print("\nğŸ“Œ 2-2. ê³ ê¸‰ í‚¤ ë§¤í•‘ ì‚¬ìš©")
        key_mapper = AdvancedKeyMapper()
        
        # ë”ë¯¸ ì²´í¬í¬ì¸íŠ¸ ìƒì„±
        dummy_checkpoint = {
            'model_state_dict': {
                'conv1.weight': torch.randn(64, 3, 7, 7),
                'conv1.bias': torch.randn(64),
                'bn1.weight': torch.randn(64),
                'bn1.bias': torch.randn(64)
            }
        }
        
        # ë”ë¯¸ ëª¨ë¸ ìƒì„±
        from app.ai_pipeline.utils.model_architectures import OpenPoseModel
        dummy_model = OpenPoseModel()
        
        # í‚¤ ë§¤í•‘ í…ŒìŠ¤íŠ¸
        mapping_success = key_mapper.map_checkpoint(dummy_checkpoint, dummy_model, 'openpose')
        
        print(f"âœ… í‚¤ ë§¤í•‘ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"   - ë§¤í•‘ ì„±ê³µ: {mapping_success}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return False

def pipeline_usage():
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ - ë°ì´í„° ê²€ì¦ ê°•í™”"""
    print("\n" + "="*60)
    print("ğŸ¯ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë°ì´í„° ê²€ì¦ ê°•í™”)")
    print("="*60)
    
    try:
        from app.ai_pipeline.utils.model_architectures import (
            IntegratedInferenceEngine, CompleteModelWrapper, 
            OpenPoseModel, HRNetPoseModel, GraphonomyModel
        )
        
        # ì—”ì§„ ìƒì„±
        engine = IntegratedInferenceEngine()
        
        # ë”ë¯¸ ëª¨ë¸ë“¤ ìƒì„± ë° ë“±ë¡
        print("\nğŸ“Œ ëª¨ë¸ ë“±ë¡")
        
        # OpenPose ëª¨ë¸
        openpose_model = CompleteModelWrapper(OpenPoseModel(), 'openpose')
        engine.register_model('pose_estimation', openpose_model)
        print("   âœ… pose_estimation ëª¨ë¸ ë“±ë¡")
        
        # HRNet ëª¨ë¸
        hrnet_model = CompleteModelWrapper(HRNetPoseModel(), 'hrnet')
        engine.register_model('pose_estimation_hrnet', hrnet_model)
        print("   âœ… pose_estimation_hrnet ëª¨ë¸ ë“±ë¡")
        
        # Graphonomy ëª¨ë¸
        graphonomy_model = CompleteModelWrapper(GraphonomyModel(), 'graphonomy')
        engine.register_model('human_parsing', graphonomy_model)
        print("   âœ… human_parsing ëª¨ë¸ ë“±ë¡")
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        print("\nğŸ“Œ íŒŒì´í”„ë¼ì¸ ìƒì„±")
        
        # ë‹¨ì¼ ëª¨ë¸ íŒŒì´í”„ë¼ì¸
        engine.create_pipeline('single_pose', ['pose_estimation'])
        print("   âœ… single_pose íŒŒì´í”„ë¼ì¸ ìƒì„±")
        
        # ë³µí•© íŒŒì´í”„ë¼ì¸
        engine.create_pipeline('pose_and_parsing', ['pose_estimation', 'human_parsing'])
        print("   âœ… pose_and_parsing íŒŒì´í”„ë¼ì¸ ìƒì„±")
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        dummy_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
        print("\nğŸ“Œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸")
        
        # 1. ë‹¨ì¼ ëª¨ë¸ íŒŒì´í”„ë¼ì¸
        print("\n   ğŸ”„ single_pose íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        try:
            result = engine.run_pipeline('single_pose', dummy_image)
            print(f"   ê²°ê³¼: {result['success']}")
            if result['success']:
                print(f"   ì‹¤í–‰ ì‹œê°„: {result['total_time']:.2f}ì´ˆ")
                print(f"   ê²°ê³¼ í‚¤: {list(result['results'].keys())}")
            else:
                print(f"   ì˜¤ë¥˜: {result.get('error', 'Unknown error')}")
        except Exception as e:
            print(f"   âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        
        # 2. ë³µí•© íŒŒì´í”„ë¼ì¸ (ë°ì´í„° ê²€ì¦ ê°•í™”)
        print("\n   ğŸ”„ pose_and_parsing íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        try:
            result = engine.run_pipeline('pose_and_parsing', dummy_image)
            print(f"   ê²°ê³¼: {result['success']}")
            if result['success']:
                print(f"   ì‹¤í–‰ ì‹œê°„: {result['total_time']:.2f}ì´ˆ")
                print(f"   ë‹¨ê³„ë³„ ê²°ê³¼:")
                for model_name, step_result in result['results'].items():
                    print(f"     - {model_name}: {type(step_result)}")
            else:
                print(f"   ì˜¤ë¥˜: {result.get('error', 'Unknown error')}")
                print(f"   ì‹¤íŒ¨ ë‹¨ê³„: {result.get('failed_step', 'Unknown')}")
        except Exception as e:
            print(f"   âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        
        # 3. ì˜ëª»ëœ ì…ë ¥ í…ŒìŠ¤íŠ¸ (ê²€ì¦ ê°•í™” í™•ì¸)
        print("\n   ğŸ”„ ì˜ëª»ëœ ì…ë ¥ í…ŒìŠ¤íŠ¸ (ê²€ì¦ ê°•í™”)")
        try:
            result = engine.run_pipeline('single_pose', None)  # None ì…ë ¥
            print(f"   ê²°ê³¼: {result['success']}")
            if not result['success']:
                print(f"   ì˜ˆìƒëœ ì˜¤ë¥˜: {result.get('error', 'Unknown error')}")
        except Exception as e:
            print(f"   âœ… ê²€ì¦ ì˜¤ë¥˜ (ì˜ˆìƒë¨): {e}")
        
        # ì„±ëŠ¥ ë¦¬í¬íŠ¸
        print("\nğŸ“Œ ì„±ëŠ¥ ë¦¬í¬íŠ¸")
        report = engine.get_performance_report()
        print(f"   ë“±ë¡ëœ ëª¨ë¸: {len(report['registered_models'])}")
        print(f"   ë“±ë¡ëœ íŒŒì´í”„ë¼ì¸: {len(report['available_pipelines'])}")
        print(f"   ìºì‹œ í¬ê¸°: {report['cache_size']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return False

def monitoring_usage():
    """4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‚¬ìš©ë²•"""
    print("\nğŸ¯ 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‚¬ìš©ë²•")
    print("=" * 50)
    
    try:
        from app.ai_pipeline.utils.model_architectures import RealTimePerformanceMonitor
        
        # 4-1. ì„±ëŠ¥ ëª¨ë‹ˆí„° ìƒì„±
        print("\nğŸ“Œ 4-1. ì„±ëŠ¥ ëª¨ë‹ˆí„° ìƒì„±")
        monitor = RealTimePerformanceMonitor()
        
        print(f"âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„° ìƒì„± ì™„ë£Œ")
        print(f"   - ê¸°ë³¸ ì„ê³„ê°’: {monitor.thresholds}")
        
        # 4-2. ëª¨ë‹ˆí„°ë§ ì‹œì‘
        print("\nğŸ“Œ 4-2. ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        monitor_id = monitor.start_monitoring('test_model', 'inference')
        
        print(f"âœ… ëª¨ë‹ˆí„°ë§ ì‹œì‘ ì™„ë£Œ")
        print(f"   - ëª¨ë‹ˆí„° ID: {monitor_id}")
        
        # 4-3. ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        print("\nğŸ“Œ 4-3. ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸")
        monitor.update_metrics(monitor_id, accuracy=0.85, execution_time=2.5)
        
        print(f"âœ… ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
        # 4-4. ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
        print("\nğŸ“Œ 4-4. ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")
        final_result = monitor.stop_monitoring(monitor_id, {'accuracy': 0.85})
        
        print(f"âœ… ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ ì™„ë£Œ")
        print(f"   - ì‹¤í–‰ ì‹œê°„: {final_result['execution_time']:.2f}ì´ˆ")
        print(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_result['memory_usage']:.1f}%")
        print(f"   - CPU ì‚¬ìš©ëŸ‰: {final_result['cpu_usage']:.1f}%")
        
        # 4-5. ì„±ëŠ¥ ìš”ì•½ í™•ì¸
        print("\nğŸ“Œ 4-5. ì„±ëŠ¥ ìš”ì•½ í™•ì¸")
        summary = monitor.get_performance_summary('test_model')
        
        print(f"âœ… ì„±ëŠ¥ ìš”ì•½ ìƒì„± ì™„ë£Œ")
        print(f"   - ì´ ì‹¤í–‰ íšŸìˆ˜: {summary['total_runs']}")
        print(f"   - í‰ê·  ì‹¤í–‰ ì‹œê°„: {summary['avg_execution_time']:.2f}ì´ˆ")
        print(f"   - í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {summary['avg_memory_usage']:.1f}%")
        
        # 4-6. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        print("\nğŸ“Œ 4-6. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸")
        system_status = monitor.get_system_status()
        
        print(f"âœ… ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì™„ë£Œ")
        print(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {system_status['memory']['percent']:.1f}%")
        print(f"   - CPU ì‚¬ìš©ëŸ‰: {system_status['cpu']['percent']:.1f}%")
        print(f"   - ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: {system_status['disk']['percent']:.1f}%")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")
        return False

def model_management_usage():
    """5. ëª¨ë¸ ê´€ë¦¬ ì‚¬ìš©ë²•"""
    print("\nğŸ¯ 5. ëª¨ë¸ ê´€ë¦¬ ì‚¬ìš©ë²•")
    print("=" * 50)
    
    try:
        from app.ai_pipeline.utils.model_architectures import AdvancedModelManager
        
        # 5-1. ëª¨ë¸ ê´€ë¦¬ì ìƒì„±
        print("\nğŸ“Œ 5-1. ëª¨ë¸ ê´€ë¦¬ì ìƒì„±")
        manager = AdvancedModelManager("./models")
        
        print(f"âœ… ëª¨ë¸ ê´€ë¦¬ì ìƒì„± ì™„ë£Œ")
        print(f"   - ê¸°ë³¸ ê²½ë¡œ: {manager.base_path}")
        print(f"   - ìë™ ê´€ë¦¬ ì„¤ì •: {manager.auto_management}")
        
        # 5-2. ëª¨ë¸ ë“±ë¡
        print("\nğŸ“Œ 5-2. ëª¨ë¸ ë“±ë¡")
        model_info = manager.register_model(
            'openpose_model',
            './models/openpose.pth',
            '1.0.0',
            dependencies=['torch', 'numpy'],
            metadata={'description': 'OpenPose í¬ì¦ˆ ì¶”ì • ëª¨ë¸', 'author': 'CMU'}
        )
        
        print(f"âœ… ëª¨ë¸ ë“±ë¡ ì™„ë£Œ")
        print(f"   - ëª¨ë¸ ì´ë¦„: {model_info['name']}")
        print(f"   - ë²„ì „: {model_info['version']}")
        print(f"   - ìƒíƒœ: {model_info['state']}")
        print(f"   - ì˜ì¡´ì„±: {model_info['dependencies']}")
        
        # 5-3. ëª¨ë¸ ì •ë³´ ì¡°íšŒ
        print("\nğŸ“Œ 5-3. ëª¨ë¸ ì •ë³´ ì¡°íšŒ")
        retrieved_info = manager.get_model('openpose_model')
        
        print(f"âœ… ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì™„ë£Œ")
        print(f"   - ì‚¬ìš© íšŸìˆ˜: {retrieved_info['usage_count']}")
        print(f"   - ë§ˆì§€ë§‰ ì‚¬ìš©: {retrieved_info['last_used']}")
        
        # 5-4. ë°±ì—… ìƒì„±
        print("\nğŸ“Œ 5-4. ë°±ì—… ìƒì„±")
        backup_info = manager.create_backup('openpose_model', 'initial_backup')
        
        print(f"âœ… ë°±ì—… ìƒì„± ì™„ë£Œ")
        print(f"   - ë°±ì—… ì´ë¦„: {backup_info['backup_name']}")
        print(f"   - ì›ë³¸ ë²„ì „: {backup_info['original_version']}")
        
        # 5-5. ëª¨ë¸ ì—…ë°ì´íŠ¸
        print("\nğŸ“Œ 5-5. ëª¨ë¸ ì—…ë°ì´íŠ¸")
        updated_info = manager.update_model(
            'openpose_model',
            './models/openpose_v2.pth',
            '2.0.0',
            changelog='ì„±ëŠ¥ ê°œì„  ë° ì •í™•ë„ í–¥ìƒ'
        )
        
        print(f"âœ… ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        print(f"   - ìƒˆ ë²„ì „: {updated_info['version']}")
        print(f"   - ë³€ê²½ ë¡œê·¸: {updated_info['changelog']}")
        
        # 5-6. ë²„ì „ íˆìŠ¤í† ë¦¬ í™•ì¸
        print("\nğŸ“Œ 5-6. ë²„ì „ íˆìŠ¤í† ë¦¬ í™•ì¸")
        versions = manager.get_model_versions('openpose_model')
        
        print(f"âœ… ë²„ì „ íˆìŠ¤í† ë¦¬ í™•ì¸ ì™„ë£Œ")
        print(f"   - ë²„ì „ ëª©ë¡: {versions}")
        
        # 5-7. ëª¨ë¸ í†µê³„ í™•ì¸
        print("\nğŸ“Œ 5-7. ëª¨ë¸ í†µê³„ í™•ì¸")
        stats = manager.get_model_statistics('openpose_model')
        
        print(f"âœ… ëª¨ë¸ í†µê³„ í™•ì¸ ì™„ë£Œ")
        print(f"   - ì‚¬ìš© íšŸìˆ˜: {stats['usage_count']}")
        print(f"   - ë°±ì—… ìˆ˜: {stats['backup_count']}")
        print(f"   - ë²„ì „ ìˆ˜: {stats['version_count']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ê´€ë¦¬ ì‹¤íŒ¨: {e}")
        return False

def complete_workflow_example():
    """ì™„ì „í•œ ì›Œí¬í”Œë¡œìš° ì˜ˆì œ - í†µí•© ì‹œìŠ¤í…œ"""
    print("\n" + "="*60)
    print("ğŸ¯ ì™„ì „í•œ ì›Œí¬í”Œë¡œìš° ì˜ˆì œ (í†µí•© ì‹œìŠ¤í…œ)")
    print("="*60)
    
    try:
        from app.ai_pipeline.utils.model_architectures import (
            IntegratedInferenceEngine, RealTimePerformanceMonitor, 
            AdvancedModelManager, CompleteModelWrapper, OpenPoseModel
        )
        
        # 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("\nğŸ“Œ 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        
        # ì—”ì§„ ìƒì„±
        engine = IntegratedInferenceEngine()
        
        # ëª¨ë‹ˆí„° ìƒì„±
        monitor = RealTimePerformanceMonitor()
        
        # ëª¨ë¸ ê´€ë¦¬ì ìƒì„±
        manager = AdvancedModelManager("./models")
        
        print("   âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 2. ëª¨ë¸ ë“±ë¡ ë° ê´€ë¦¬
        print("\nğŸ“Œ 2. ëª¨ë¸ ë“±ë¡ ë° ê´€ë¦¬")
        
        # ëª¨ë¸ ë“±ë¡
        openpose_model = CompleteModelWrapper(OpenPoseModel(), 'openpose')
        engine.register_model('pose_estimation', openpose_model)
        
        # ëª¨ë¸ ê´€ë¦¬ìì— ë“±ë¡
        manager.register_model(
            'pose_estimation', 
            './models/openpose_model.pth', 
            '1.0.0',
            dependencies=['torch', 'numpy'],
            metadata={'type': 'pose_estimation', 'framework': 'pytorch'}
        )
        
        print("   âœ… ëª¨ë¸ ë“±ë¡ ë° ê´€ë¦¬ ì™„ë£Œ")
        
        # 3. íŒŒì´í”„ë¼ì¸ ìƒì„±
        print("\nğŸ“Œ 3. íŒŒì´í”„ë¼ì¸ ìƒì„±")
        
        engine.create_pipeline('fashion_analysis', ['pose_estimation'])
        print("   âœ… fashion_analysis íŒŒì´í”„ë¼ì¸ ìƒì„±")
        
        # 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        print("\nğŸ“Œ 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        monitor_id = monitor.start_monitoring('fashion_analysis', 'pipeline_execution')
        print(f"   ëª¨ë‹ˆí„°ë§ ID: {monitor_id}")
        
        # 5. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        print("\nğŸ“Œ 5. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        dummy_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        
        # ì‹¤í–‰
        result = engine.run_pipeline('fashion_analysis', dummy_image)
        
        # 6. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
        print("\nğŸ“Œ 6. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")
        
        final_metrics = monitor.stop_monitoring(monitor_id, {
            'execution_time': result.get('total_time', 0),
            'success': result.get('success', False),
            'memory_usage': psutil.virtual_memory().percent / 100
        })
        
        print(f"   ìµœì¢… ë©”íŠ¸ë¦­: {final_metrics}")
        
        # 7. ê²°ê³¼ ë¶„ì„
        print("\nğŸ“Œ 7. ê²°ê³¼ ë¶„ì„")
        
        print(f"   íŒŒì´í”„ë¼ì¸ ì„±ê³µ: {result.get('success', False)}")
        if result.get('success'):
            print(f"   ì´ ì‹¤í–‰ ì‹œê°„: {result.get('total_time', 0):.2f}ì´ˆ")
            print(f"   ê²°ê³¼ í‚¤: {list(result.get('results', {}).keys())}")
        
        # 8. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        print("\nğŸ“Œ 8. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸")
        
        # ì„±ëŠ¥ ìš”ì•½
        performance_summary = monitor.get_performance_summary()
        print(f"   ì„±ëŠ¥ ìš”ì•½: {len(performance_summary)} í•­ëª©")
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        system_status = monitor.get_system_status()
        print(f"   ì‹œìŠ¤í…œ ìƒíƒœ: CPU {system_status['cpu']['percent']:.1f}%, ë©”ëª¨ë¦¬ {system_status['memory']['percent']:.1f}%")
        
        # ğŸ”¥ ModelLoader í†µí•© ìƒíƒœ í™•ì¸
        print("\nğŸ“Œ ModelLoader í†µí•© ìƒíƒœ í™•ì¸")
        try:
            from app.ai_pipeline.utils.model_loader import get_global_model_loader, initialize_global_model_loader
            
            # ModelLoader ì´ˆê¸°í™”
            success = initialize_global_model_loader()
            if success:
                model_loader = get_global_model_loader()
                if model_loader:
                    print("   âœ… ModelLoader Central Hub í†µí•© ì„±ê³µ")
                    
                    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
                    if hasattr(model_loader, 'list_available_models'):
                        available_models = model_loader.list_available_models()
                        print(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(available_models)}ê°œ")
                        
                        # ëª¨ë¸ë³„ ë¶„ë¥˜
                        step_models = {}
                        for model in available_models:
                            step_class = model.get('step_class', 'Unknown')
                            if step_class not in step_models:
                                step_models[step_class] = 0
                            step_models[step_class] += 1
                        
                        print("   - Stepë³„ ëª¨ë¸ ë¶„í¬:")
                        for step_class, count in step_models.items():
                            print(f"     * {step_class}: {count}ê°œ")
                    
                    print(f"   - ë””ë°”ì´ìŠ¤: {model_loader.device}")
                else:
                    print("   âŒ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨")
            else:
                print("   âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"   âŒ ModelLoader í†µí•© í™•ì¸ ì‹¤íŒ¨: {e}")
        
        # ëª¨ë¸ í†µê³„
        model_stats = manager.get_model_statistics()
        print(f"   ë“±ë¡ëœ ëª¨ë¸: {len(model_stats)}")
        
        print("\nğŸ‰ ì™„ì „í•œ ì›Œí¬í”Œë¡œìš° ì„±ê³µ!")
        return True
        
    except Exception as e:
        print(f"âŒ ì™„ì „í•œ ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ MyCloset-AI Model Architectures ì‚¬ìš© ê°€ì´ë“œ")
    print("=" * 60)
    
    # ê° ì‚¬ìš©ë²• í…ŒìŠ¤íŠ¸
    tests = [
        ("ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©ë²•", basic_model_usage),
        ("ì²´í¬í¬ì¸íŠ¸ ë¡œë”©", checkpoint_loading_usage),
        ("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰", pipeline_usage),
        ("ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§", monitoring_usage),
        ("ëª¨ë¸ ê´€ë¦¬", model_management_usage),
        ("ì™„ì „í•œ ì›Œí¬í”Œë¡œìš°", complete_workflow_example)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            results.append((test_name, False))
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š ì‚¬ìš© ê°€ì´ë“œ ì‹¤í–‰ ê²°ê³¼")
    print("=" * 60)
    
    for test_name, result in results:
        status = "âœ… ì„±ê³µ" if result else "âŒ ì‹¤íŒ¨"
        print(f"   {test_name}: {status}")
    
    success_count = sum(1 for _, result in results if result)
    total_count = len(results)
    
    print(f"\nğŸ¯ ì „ì²´ ê²°ê³¼: {success_count}/{total_count} ì„±ê³µ")
    
    if success_count == total_count:
        print("ğŸ‰ ëª¨ë“  ì‚¬ìš©ë²•ì´ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âš ï¸ ì¼ë¶€ ì‚¬ìš©ë²•ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
