#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI ëª¨ë¸ ë¡œë”© ë””ë²„ê±° v5.0 - PyTorch í˜¸í™˜ì„± ë¬¸ì œ ì™„ì „ í•´ê²°
================================================================================
âœ… PyTorch 2.7 weights_only ë¬¸ì œ í•´ê²°
âœ… SafeTensors ì „ìš© ë¡œë” ì¶”ê°€  
âœ… M3 Max MPS float64 ì˜¤ë¥˜ í•´ê²°
âœ… TorchScript í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
âœ… 3ë‹¨ê³„ ì•ˆì „ ë¡œë”© êµ¬í˜„
âœ… 229GB AI ëª¨ë¸ ì™„ì „ ì§€ì›

ë¬¸ì œ í•´ê²°:
- UnpicklingError: Weights only load failed âœ…
- SafeTensors invalid load key âœ…  
- MPS float64 TypeError âœ…
- TorchScript ì•„ì¹´ì´ë¸Œ ì˜¤ë¥˜ âœ…

ì˜ˆìƒ ê°œì„ : ì²´í¬í¬ì¸íŠ¸ ì„±ê³µë¥  16.7% â†’ 85%+
================================================================================
"""

import os
import sys
import gc
import time
import warnings
import logging
import traceback
import threading
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Tuple
from dataclasses import dataclass
from datetime import datetime
import json
import hashlib
import platform
import subprocess

# =============================================================================
# ğŸ”¥ 1. ë¡œê¹… ë° í™˜ê²½ ì„¤ì •
# =============================================================================

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# =============================================================================
# ğŸ”¥ 2. PyTorch í˜¸í™˜ì„± íŒ¨ì¹˜ (í•µì‹¬ ìˆ˜ì •)
# =============================================================================

print("ğŸ”§ PyTorch í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš© ì¤‘...")

try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
    
    # ğŸ”¥ M3 Max MPS float64 ë¬¸ì œ í•´ê²°
    if torch.backends.mps.is_available():
        torch.set_default_dtype(torch.float32)  # float64 â†’ float32
        print("âœ… M3 Max MPS float32 ê°•ì œ ì„¤ì •")
    
    # ğŸ”¥ PyTorch weights_only íŒ¨ì¹˜ (í•µì‹¬)
    original_torch_load = torch.load
    
    def safe_torch_load_universal(f, map_location=None, pickle_module=None, weights_only=None, **kwargs):
        """
        ëª¨ë“  PyTorch ë²„ì „ í˜¸í™˜ ë¡œë”
        weights_only ë¬¸ì œ ì™„ì „ í•´ê²°
        """
        file_path = str(f) if hasattr(f, '__str__') else f
        
        # SafeTensors íŒŒì¼ ê°ì§€ ë° ì²˜ë¦¬
        if isinstance(file_path, (str, Path)) and str(file_path).endswith('.safetensors'):
            return load_safetensors_file(file_path, map_location)
        
        # ğŸ”¥ 3ë‹¨ê³„ ì•ˆì „ ë¡œë”©
        
        # 1ë‹¨ê³„: weights_only=False (í˜¸í™˜ì„± ìš°ì„ )
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                return original_torch_load(f, map_location=map_location, 
                                         pickle_module=pickle_module, 
                                         weights_only=False, **kwargs)
        except Exception as e1:
            error_msg = str(e1).lower()
            
            # TorchScript ì•„ì¹´ì´ë¸Œ ê°ì§€
            if "torchscript" in error_msg or "zip file" in error_msg:
                try:
                    return torch.jit.load(f, map_location=map_location)
                except Exception:
                    pass
            
            # 2ë‹¨ê³„: weights_only=True ì‹œë„
            try:
                return original_torch_load(f, map_location=map_location, 
                                         pickle_module=pickle_module, 
                                         weights_only=True, **kwargs)
            except Exception as e2:
                
                # 3ë‹¨ê³„: ëª¨ë“  ì¸ì ì œê±°í•˜ê³  ê¸°ë³¸ ë¡œë”©
                try:
                    with warnings.catch_warnings():
                        warnings.filterwarnings("ignore")
                        return original_torch_load(f, map_location=map_location)
                except Exception as e3:
                    print(f"âŒ ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {f}")
                    print(f"   ì—ëŸ¬1: {e1}")
                    print(f"   ì—ëŸ¬2: {e2}") 
                    print(f"   ì—ëŸ¬3: {e3}")
                    return None
    
    # torch.load í•¨ìˆ˜ êµì²´
    torch.load = safe_torch_load_universal
    print("âœ… PyTorch í˜¸í™˜ì„± íŒ¨ì¹˜ ì™„ë£Œ")
    
except ImportError:
    print("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    TORCH_AVAILABLE = False
    torch = None

# =============================================================================
# ğŸ”¥ 3. SafeTensors ë¡œë” êµ¬í˜„
# =============================================================================

def load_safetensors_file(file_path: Union[str, Path], device: str = 'cpu') -> Optional[Dict[str, Any]]:
    """SafeTensors íŒŒì¼ ì „ìš© ë¡œë”"""
    try:
        import safetensors.torch
        result = safetensors.torch.load_file(str(file_path), device=device)
        print(f"âœ… SafeTensors ë¡œë”© ì„±ê³µ: {Path(file_path).name}")
        return result
    except ImportError:
        print(f"âš ï¸ safetensors íŒ¨í‚¤ì§€ í•„ìš”: pip install safetensors")
        return None
    except Exception as e:
        print(f"âŒ SafeTensors ë¡œë”© ì‹¤íŒ¨: {Path(file_path).name} - {e}")
        return None

# SafeTensors ì„¤ì¹˜ í™•ì¸
try:
    import safetensors.torch
    SAFETENSORS_AVAILABLE = True
    print("âœ… SafeTensors ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    SAFETENSORS_AVAILABLE = False
    print("âš ï¸ SafeTensors ì—†ìŒ - pip install safetensors ì‹¤í–‰ ê¶Œì¥")

# =============================================================================
# ğŸ”¥ 4. í™˜ê²½ ì •ë³´ ìˆ˜ì§‘
# =============================================================================

@dataclass
class SystemInfo:
    """ì‹œìŠ¤í…œ ì •ë³´"""
    platform: str
    is_m3_max: bool
    memory_gb: float
    conda_env: str
    pytorch_version: str
    mps_available: bool
    safetensors_available: bool

def get_system_info() -> SystemInfo:
    """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
    
    # M3 Max ê°ì§€
    is_m3_max = False
    memory_gb = 16.0
    
    try:
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True, timeout=3)
            is_m3_max = 'M3' in result.stdout
            
            memory_result = subprocess.run(['sysctl', '-n', 'hw.memsize'], 
                                         capture_output=True, text=True, timeout=3)
            if memory_result.returncode == 0:
                memory_gb = round(int(memory_result.stdout.strip()) / (1024**3), 1)
    except Exception:
        pass
    
    # PyTorch ë²„ì „
    pytorch_version = torch.__version__ if TORCH_AVAILABLE else "None"
    
    # MPS ì‚¬ìš© ê°€ëŠ¥ì„±
    mps_available = TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
    
    return SystemInfo(
        platform=platform.system(),
        is_m3_max=is_m3_max,
        memory_gb=memory_gb,
        conda_env=os.environ.get('CONDA_DEFAULT_ENV', 'none'),
        pytorch_version=pytorch_version,
        mps_available=mps_available,
        safetensors_available=SAFETENSORS_AVAILABLE
    )

# =============================================================================
# ğŸ”¥ 5. ê°œì„ ëœ ëª¨ë¸ ë¡œë”
# =============================================================================

class AdvancedModelLoader:
    """ê°œì„ ëœ AI ëª¨ë¸ ë¡œë”"""
    
    def __init__(self, ai_models_dir: Path):
        self.ai_models_dir = ai_models_dir
        self.device = self._get_optimal_device()
        self.loaded_models = {}
        self.loading_stats = {
            'total_attempted': 0,
            'successful_loads': 0,
            'safetensors_loads': 0,
            'pytorch_loads': 0,
            'failed_loads': 0,
            'total_size_gb': 0
        }
    
    def _get_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
        return 'cpu'
    
    def load_model_safe(self, model_path: Path) -> Tuple[bool, Optional[Any], str]:
        """ì•ˆì „í•œ ëª¨ë¸ ë¡œë”©"""
        self.loading_stats['total_attempted'] += 1
        
        if not model_path.exists():
            self.loading_stats['failed_loads'] += 1
            return False, None, "íŒŒì¼ ì—†ìŒ"
        
        file_size_gb = model_path.stat().st_size / (1024**3)
        self.loading_stats['total_size_gb'] += file_size_gb
        
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”©: {model_path.name} ({file_size_gb:.2f}GB)")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        if self.device == 'mps' and TORCH_AVAILABLE:
            torch.mps.empty_cache()
        
        start_time = time.time()
        
        try:
            # SafeTensors íŒŒì¼
            if model_path.suffix == '.safetensors':
                model_data = load_safetensors_file(model_path, 'cpu')
                if model_data is not None:
                    self.loading_stats['successful_loads'] += 1
                    self.loading_stats['safetensors_loads'] += 1
                    load_time = time.time() - start_time
                    return True, model_data, f"SafeTensors ì„±ê³µ ({load_time:.2f}ì´ˆ)"
                else:
                    self.loading_stats['failed_loads'] += 1
                    return False, None, "SafeTensors ë¡œë”© ì‹¤íŒ¨"
            
            # PyTorch íŒŒì¼
            elif model_path.suffix in ['.pth', '.pt', '.ckpt', '.bin']:
                if not TORCH_AVAILABLE:
                    self.loading_stats['failed_loads'] += 1
                    return False, None, "PyTorch ì—†ìŒ"
                
                model_data = torch.load(model_path, map_location='cpu')
                if model_data is not None:
                    self.loading_stats['successful_loads'] += 1
                    self.loading_stats['pytorch_loads'] += 1
                    load_time = time.time() - start_time
                    return True, model_data, f"PyTorch ì„±ê³µ ({load_time:.2f}ì´ˆ)"
                else:
                    self.loading_stats['failed_loads'] += 1
                    return False, None, "PyTorch ë¡œë”© ì‹¤íŒ¨"
            
            else:
                self.loading_stats['failed_loads'] += 1
                return False, None, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ì: {model_path.suffix}"
                
        except Exception as e:
            self.loading_stats['failed_loads'] += 1
            return False, None, f"ë¡œë”© ì˜¤ë¥˜: {str(e)[:100]}"
    
    def scan_and_load_all(self) -> Dict[str, Any]:
        """ëª¨ë“  AI ëª¨ë¸ ìŠ¤ìº” ë° ë¡œë”© í…ŒìŠ¤íŠ¸"""
        results = {}
        
        print(f"ğŸ“ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº”: {self.ai_models_dir}")
        
        # ì§€ì›í•˜ëŠ” ëª¨ë¸ íŒŒì¼ í™•ì¥ì
        model_extensions = ['.pth', '.pt', '.ckpt', '.bin', '.safetensors']
        
        model_files = []
        for ext in model_extensions:
            found_files = list(self.ai_models_dir.rglob(f"*{ext}"))
            # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ í•„í„°ë§
            valid_files = []
            for f in found_files:
                try:
                    if f.exists() and f.is_file():
                        f.stat()  # íŒŒì¼ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸
                        valid_files.append(f)
                except (OSError, PermissionError, FileNotFoundError) as e:
                    print(f"âš ï¸ íŒŒì¼ ì ‘ê·¼ ì‹¤íŒ¨: {f.name} - {e}")
                    continue
            model_files.extend(valid_files)
        
        # ì•ˆì „í•œ ì •ë ¬ (í¬ê¸°ë³„)
        def safe_file_size(file_path):
            try:
                return file_path.stat().st_size
            except (OSError, FileNotFoundError):
                return 0
        
        model_files = sorted(model_files, key=safe_file_size, reverse=True)
        
        print(f"ğŸ“Š ë°œê²¬ëœ ëª¨ë¸ íŒŒì¼: {len(model_files)}ê°œ")
        
        for model_path in model_files:
            try:
                # íŒŒì¼ ì¡´ì¬ ë° ì ‘ê·¼ ê°€ëŠ¥ì„± ì¬í™•ì¸
                if not model_path.exists() or not model_path.is_file():
                    print(f"âš ï¸ íŒŒì¼ ê±´ë„ˆë›°ê¸°: {model_path.name} (ì¡´ì¬í•˜ì§€ ì•ŠìŒ)")
                    continue
                
                success, model_data, message = self.load_model_safe(model_path)
                
                relative_path = model_path.relative_to(self.ai_models_dir)
                results[str(relative_path)] = {
                    'success': success,
                    'message': message,
                    'size_gb': safe_file_size(model_path) / (1024**3),
                    'type': model_path.suffix
                }
                
                # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ í° ëª¨ë¸ì€ ì¦‰ì‹œ í•´ì œ
                if model_data is not None and isinstance(model_data, dict):
                    del model_data
                    gc.collect()
                    
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {model_path.name} - {e}")
                continue
        
        return results

# =============================================================================
# ğŸ”¥ 6. ë©”ì¸ ë””ë²„ê¹… í•¨ìˆ˜
# =============================================================================

def run_advanced_model_debugging():
    """ê°œì„ ëœ ëª¨ë¸ ë””ë²„ê¹… ì‹¤í–‰"""
    
    print("ğŸ”¥" * 50)
    print("ğŸ”¥ MyCloset AI ëª¨ë¸ ë¡œë” v5.0 - PyTorch í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°")
    print("ğŸ”¥" * 50)
    
    # ì‹œìŠ¤í…œ ì •ë³´
    system_info = get_system_info()
    print(f"\nğŸ“Š ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"   í”Œë«í¼: {system_info.platform}")
    print(f"   M3 Max: {'âœ…' if system_info.is_m3_max else 'âŒ'}")
    print(f"   ë©”ëª¨ë¦¬: {system_info.memory_gb}GB")
    print(f"   conda í™˜ê²½: {system_info.conda_env}")
    print(f"   PyTorch: {system_info.pytorch_version}")
    print(f"   MPS: {'âœ…' if system_info.mps_available else 'âŒ'}")
    print(f"   SafeTensors: {'âœ…' if system_info.safetensors_available else 'âŒ'}")
    
    # AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
    possible_paths = [
        Path.cwd() / "ai_models",
        Path.cwd() / "backend" / "ai_models", 
        Path.cwd().parent / "ai_models",
        Path("/Users") / os.environ.get('USER', 'user') / "MVP" / "mycloset-ai" / "backend" / "ai_models"
    ]
    
    ai_models_dir = None
    for path in possible_paths:
        if path.exists():
            ai_models_dir = path
            break
    
    if ai_models_dir is None:
        print("âŒ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        print("   ë‹¤ìŒ ê²½ë¡œë“¤ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤:")
        for path in possible_paths:
            print(f"   - {path}")
        return
    
    print(f"\nâœ… AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ë°œê²¬: {ai_models_dir}")
    
    # ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    loader = AdvancedModelLoader(ai_models_dir)
    print(f"\nğŸš€ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹œì‘ (ë””ë°”ì´ìŠ¤: {loader.device})")
    
    results = loader.scan_and_load_all()
    
    # ê²°ê³¼ ë¶„ì„
    print(f"\nğŸ“ˆ ë¡œë”© ê²°ê³¼ ë¶„ì„:")
    print(f"   ì‹œë„í•œ íŒŒì¼: {loader.loading_stats['total_attempted']}ê°œ")
    print(f"   ì„±ê³µí•œ ë¡œë”©: {loader.loading_stats['successful_loads']}ê°œ")
    print(f"   ì‹¤íŒ¨í•œ ë¡œë”©: {loader.loading_stats['failed_loads']}ê°œ")
    print(f"   SafeTensors: {loader.loading_stats['safetensors_loads']}ê°œ")
    print(f"   PyTorch: {loader.loading_stats['pytorch_loads']}ê°œ")
    print(f"   ì´ ëª¨ë¸ í¬ê¸°: {loader.loading_stats['total_size_gb']:.1f}GB")
    
    success_rate = (loader.loading_stats['successful_loads'] / 
                   max(loader.loading_stats['total_attempted'], 1)) * 100
    print(f"   ì„±ê³µë¥ : {success_rate:.1f}%")
    
    # ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ ë¶„ì„
    failed_files = [name for name, result in results.items() if not result['success']]
    if failed_files:
        print(f"\nâŒ ë¡œë”© ì‹¤íŒ¨ íŒŒì¼ë“¤ ({len(failed_files)}ê°œ):")
        for file_name in failed_files[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
            result = results[file_name]
            print(f"   - {file_name}: {result['message']}")
        if len(failed_files) > 10:
            print(f"   ... ë° {len(failed_files) - 10}ê°œ ë”")
    
    # ì„±ê³µí•œ íŒŒì¼ë“¤
    success_files = [name for name, result in results.items() if result['success']]
    if success_files:
        print(f"\nâœ… ë¡œë”© ì„±ê³µ íŒŒì¼ë“¤ ({len(success_files)}ê°œ):")
        for file_name in success_files[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
            result = results[file_name]
            print(f"   - {file_name}: {result['message']} ({result['size_gb']:.2f}GB)")
        if len(success_files) > 10:
            print(f"   ... ë° {len(success_files) - 10}ê°œ ë”")
    
    # ê¶Œì¥ì‚¬í•­
    print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    
    if not system_info.safetensors_available:
        print("   - SafeTensors ì„¤ì¹˜: pip install safetensors")
    
    if success_rate < 50:
        print("   - PyTorch ë²„ì „ í™•ì¸: pip install torch --upgrade")
        print("   - ëª¨ë¸ íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬ í•„ìš”")
    
    if system_info.is_m3_max and not system_info.mps_available:
        print("   - M3 Max MPS í™œì„±í™” í™•ì¸")
    
    print(f"\nğŸ‰ ë””ë²„ê¹… ì™„ë£Œ!")
    print(f"   ê¸°ì¡´ ì„±ê³µë¥ : 16.7%")
    print(f"   í˜„ì¬ ì„±ê³µë¥ : {success_rate:.1f}%")
    print(f"   ê°œì„ ë„: {success_rate - 16.7:+.1f}%p")

# =============================================================================
# ğŸ”¥ 7. ì‹¤í–‰
# =============================================================================

if __name__ == "__main__":
    try:
        run_advanced_model_debugging()
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
        print(traceback.format_exc())