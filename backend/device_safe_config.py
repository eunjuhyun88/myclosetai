#!/usr/bin/env python3
"""
ë””ë°”ì´ìŠ¤ ì•ˆì „ ì„¤ì • (ìë™ ìƒì„±)
"""
import torch

def get_safe_device():
    """ì•ˆì „í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    if torch.backends.mps.is_available():
        try:
            # MPS ê°„ë‹¨ í…ŒìŠ¤íŠ¸
            test_tensor = torch.randn(1, 1).to('mps')
            return 'mps'
        except Exception:
            return 'cpu'
    elif torch.cuda.is_available():
        return 'cuda'
    else:
        return 'cpu'

def safe_model_load(model_path, target_device=None):
    """ë””ë°”ì´ìŠ¤ ì•ˆì „ ëª¨ë¸ ë¡œë”©"""
    if target_device is None:
        target_device = get_safe_device()
    
    # 1ë‹¨ê³„: CPUë¡œ ë¡œë”©
    model_data = torch.load(model_path, map_location='cpu')
    
    # 2ë‹¨ê³„: íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (torch.jit ì•„ë‹Œ ê²½ìš°ë§Œ)
    if target_device != 'cpu' and not hasattr(model_data, '_c'):
        if isinstance(model_data, dict) and 'state_dict' in model_data:
            new_state_dict = {}
            for key, tensor in model_data['state_dict'].items():
                new_state_dict[key] = tensor.to(target_device)
            model_data['state_dict'] = new_state_dict
        elif hasattr(model_data, 'to'):
            model_data = model_data.to(target_device)
    
    return model_data, target_device

if __name__ == "__main__":
    print(f"ğŸ¯ ê¶Œì¥ ë””ë°”ì´ìŠ¤: {get_safe_device()}")
