#!/usr/bin/env python3
"""
MyCloset AI - AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ìë™ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
M3 Max 128GB ìµœì í™” ë²„ì „ - ì‹¤ì œ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

í•„ìš”í•œ ëª¨ë¸ë“¤:
1. OOTDiffusion - ìµœì‹  ê°€ìƒ í”¼íŒ… ëª¨ë¸
2. Human Parsing - ì¸ì²´ ë¶„í•  ëª¨ë¸ (Graphonomy, SCHP)
3. UÂ²-Net - ë°°ê²½ ì œê±° ë° ì˜ë¥˜ ë¶„í• 
4. Stable Diffusion - ê¸°ë³¸ ë””í“¨ì „ ëª¨ë¸
5. CLIP - í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì´í•´ ëª¨ë¸

ì‚¬ìš©ë²•:
    python3 backend/scripts/download_ai_models.py
"""

import os
import sys
import time
import logging
import shutil
from pathlib import Path
from typing import Dict, List, Optional
import subprocess
import hashlib
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

class AIModelDownloader:
    """AI ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.models_dir = self.project_root / "ai_models"
        self.checkpoints_dir = self.models_dir / "checkpoints"
        self.configs_dir = self.models_dir / "configs"
        self.temp_dir = self.models_dir / "temp"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.checkpoints_dir.mkdir(parents=True, exist_ok=True)
        self.configs_dir.mkdir(parents=True, exist_ok=True)
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        
        # ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ëª©ë¡
        self.models = {
            "ootdiffusion": {
                "name": "OOTDiffusion",
                "repo_id": "levihsu/OOTDiffusion",
                "local_dir": "ootdiffusion_hf",
                "size_gb": 8.0,
                "priority": 1,
                "step": "step_06_virtual_fitting",
                "description": "ìµœì‹  ê³ í’ˆì§ˆ ê°€ìƒ í”¼íŒ… ëª¨ë¸",
                "required_files": ["checkpoints/ootd", "configs"]
            },
            "human_parsing": {
                "name": "Human Parsing (Graphonomy)",
                "repo_id": "mattmdjaga/human_parsing",
                "local_dir": "human_parsing",
                "size_gb": 0.5,
                "priority": 2,
                "step": "step_01_human_parsing",
                "description": "ì¸ì²´ ë¶„í•  ëª¨ë¸ (20ê°œ ë¶€ìœ„)",
                "required_files": ["atr_model.pth", "lip_model.pth"]
            },
            "u2net": {
                "name": "UÂ²-Net Background Removal",
                "repo_id": "skytnt/u2net",
                "local_dir": "u2net",
                "size_gb": 0.2,
                "priority": 3,
                "step": "step_03_cloth_segmentation",
                "description": "ë°°ê²½ ì œê±° ë° ì˜ë¥˜ ë¶„í•  ëª¨ë¸",
                "required_files": ["u2net.pth"]
            },
            "stable_diffusion": {
                "name": "Stable Diffusion v1.5",
                "repo_id": "runwayml/stable-diffusion-v1-5",
                "local_dir": "stable-diffusion-v1-5",
                "size_gb": 4.0,
                "priority": 4,
                "step": "step_06_virtual_fitting",
                "description": "ê¸°ë³¸ ë””í“¨ì „ ëª¨ë¸",
                "required_files": ["pytorch_model.bin", "config.json"]
            },
            "clip_vit_base": {
                "name": "CLIP ViT-B/32",
                "repo_id": "openai/clip-vit-base-patch32",
                "local_dir": "clip-vit-base-patch32",
                "size_gb": 0.6,
                "priority": 5,
                "step": "auxiliary",
                "description": "í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì´í•´ ëª¨ë¸",
                "required_files": ["pytorch_model.bin", "config.json"]
            },
            "clip_vit_large": {
                "name": "CLIP ViT-L/14",
                "repo_id": "openai/clip-vit-large-patch14",
                "local_dir": "clip-vit-large-patch14",
                "size_gb": 1.6,
                "priority": 6,
                "step": "auxiliary",
                "description": "ëŒ€í˜• CLIP ëª¨ë¸ (ê³ ì„±ëŠ¥)",
                "required_files": ["pytorch_model.bin", "config.json"]
            }
        }
        
        self.total_size_gb = sum(model["size_gb"] for model in self.models.values())
    
    def check_dependencies(self) -> bool:
        """í•„ìš”í•œ ì˜ì¡´ì„± í™•ì¸"""
        logger.info("ğŸ” ì˜ì¡´ì„± í™•ì¸ ì¤‘...")
        
        required_packages = [
            "transformers",
            "diffusers", 
            "torch",
            "huggingface_hub",
            "accelerate"
        ]
        
        missing_packages = []
        
        for package in required_packages:
            try:
                __import__(package)
                logger.info(f"âœ… {package} ì„¤ì¹˜ë¨")
            except ImportError:
                missing_packages.append(package)
                logger.warning(f"âŒ {package} ëˆ„ë½")
        
        if missing_packages:
            logger.error(f"âŒ ëˆ„ë½ëœ íŒ¨í‚¤ì§€: {', '.join(missing_packages)}")
            logger.info("ğŸ“ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
            logger.info(f"pip install {' '.join(missing_packages)}")
            return False
        
        logger.info("âœ… ëª¨ë“  ì˜ì¡´ì„± í™•ì¸ ì™„ë£Œ")
        return True
    
    def check_disk_space(self) -> bool:
        """ë””ìŠ¤í¬ ê³µê°„ í™•ì¸"""
        logger.info("ğŸ’¾ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ì¤‘...")
        
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
            statvfs = os.statvfs(self.models_dir)
            free_bytes = statvfs.f_frsize * statvfs.f_bavail
            free_gb = free_bytes / (1024**3)
            
            logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ê³µê°„: {free_gb:.1f} GB")
            logger.info(f"ğŸ“¦ í•„ìš”í•œ ê³µê°„: {self.total_size_gb:.1f} GB")
            
            if free_gb < self.total_size_gb + 5:  # 5GB ì—¬ìœ  ê³µê°„
                logger.warning("âš ï¸ ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
                return False
            
            logger.info("âœ… ë””ìŠ¤í¬ ê³µê°„ ì¶©ë¶„")
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ì‹¤íŒ¨: {e}")
            return True
    
    def download_model(self, model_key: str, model_info: Dict) -> bool:
        """ê°œë³„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        logger.info(f"ğŸ“¥ {model_info['name']} ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        logger.info(f"   ğŸ“ í¬ê¸°: {model_info['size_gb']} GB")
        logger.info(f"   ğŸ“ ì„¤ëª…: {model_info['description']}")
        
        local_path = self.checkpoints_dir / model_info['local_dir']
        
        # ì´ë¯¸ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if local_path.exists() and self.verify_model_integrity(local_path, model_info):
            logger.info(f"âœ… {model_info['name']} ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨")
            return True
        
        try:
            # Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œ
            from huggingface_hub import snapshot_download
            
            logger.info(f"ğŸ”„ {model_info['name']} ë‹¤ìš´ë¡œë“œ ì¤‘...")
            start_time = time.time()
            
            # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
            snapshot_download(
                repo_id=model_info['repo_id'],
                local_dir=str(local_path),
                resume_download=True,
                local_dir_use_symlinks=False
            )
            
            elapsed_time = time.time() - start_time
            logger.info(f"âœ… {model_info['name']} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({elapsed_time:.1f}ì´ˆ)")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ {model_info['name']} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def verify_model_integrity(self, model_path: Path, model_info: Dict) -> bool:
        """ëª¨ë¸ ë¬´ê²°ì„± í™•ì¸"""
        try:
            # í•„ìˆ˜ íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
            required_files = model_info.get('required_files', [])
            
            for file_pattern in required_files:
                found_files = list(model_path.rglob(file_pattern))
                if not found_files:
                    logger.warning(f"âš ï¸ í•„ìˆ˜ íŒŒì¼ ëˆ„ë½: {file_pattern}")
                    return False
            
            # í´ë” í¬ê¸° í™•ì¸ (ëŒ€ëµì )
            total_size = sum(f.stat().st_size for f in model_path.rglob('*') if f.is_file())
            size_gb = total_size / (1024**3)
            
            expected_size = model_info['size_gb']
            if size_gb < expected_size * 0.8:  # 80% ì´ìƒì´ë©´ OK
                logger.warning(f"âš ï¸ ëª¨ë¸ í¬ê¸° ë¶€ì¡±: {size_gb:.1f}GB < {expected_size:.1f}GB")
                return False
            
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë¬´ê²°ì„± í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def create_model_config(self):
        """ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
        logger.info("ğŸ“ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")
        
        # YAML ì„¤ì • ìƒì„±
        config_content = {
            "models": {},
            "device_config": {
                "auto_detect": True,
                "preferred_device": "mps",  # M3 Max ìµœì í™”
                "fallback_device": "cpu",
                "use_fp16": True,
                "batch_size": 4
            },
            "pipeline_config": {
                "image_size": [512, 512],
                "quality_level": "high",
                "enable_caching": True,
                "max_cache_size_gb": 8.0
            }
        }
        
        # ê° ëª¨ë¸ ì„¤ì • ì¶”ê°€
        for model_key, model_info in self.models.items():
            config_content["models"][model_key] = {
                "name": model_info["name"],
                "path": f"ai_models/checkpoints/{model_info['local_dir']}",
                "step": model_info["step"],
                "enabled": True,
                "priority": model_info["priority"],
                "size_gb": model_info["size_gb"]
            }
        
        # YAML íŒŒì¼ë¡œ ì €ì¥
        config_path = self.configs_dir / "models_config.yaml"
        with open(config_path, 'w') as f:
            import yaml
            yaml.dump(config_content, f, default_flow_style=False, allow_unicode=True)
        
        logger.info(f"âœ… ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
        
        # Python ì„¤ì • íŒŒì¼ë„ ìƒì„±
        python_config_path = self.project_root / "app" / "core" / "model_paths.py"
        self.create_python_config(python_config_path)
    
    def create_python_config(self, output_path: Path):
        """Python ì„¤ì • íŒŒì¼ ìƒì„±"""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        content = '''# app/core/model_paths.py
"""
AI ëª¨ë¸ ê²½ë¡œ ì„¤ì • - ìë™ ìƒì„±ë¨
"""

from pathlib import Path
from typing import Dict, Optional

# ê¸°ë³¸ ê²½ë¡œ
AI_MODELS_ROOT = Path(__file__).parent.parent.parent / "ai_models"
CHECKPOINTS_ROOT = AI_MODELS_ROOT / "checkpoints"

# ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ë“¤
DOWNLOADED_MODELS = {
'''
        
        # ê° ëª¨ë¸ ì •ë³´ ì¶”ê°€
        for model_key, model_info in self.models.items():
            content += f'''    "{model_key}": {{
        "name": "{model_info['name']}",
        "path": CHECKPOINTS_ROOT / "{model_info['local_dir']}",
        "step": "{model_info['step']}",
        "priority": {model_info['priority']},
        "size_gb": {model_info['size_gb']},
        "enabled": True
    }},
'''
        
        content += '''}

def get_model_path(model_key: str) -> Optional[Path]:
    """ëª¨ë¸ ê²½ë¡œ ë°˜í™˜"""
    model_info = DOWNLOADED_MODELS.get(model_key)
    if model_info:
        return model_info["path"]
    return None

def is_model_available(model_key: str) -> bool:
    """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    model_path = get_model_path(model_key)
    return model_path and model_path.exists()

def get_step_model(step_name: str) -> Optional[str]:
    """íŠ¹ì • ë‹¨ê³„ì˜ ëª¨ë¸ ë°˜í™˜"""
    for model_key, model_info in DOWNLOADED_MODELS.items():
        if model_info["step"] == step_name:
            return model_key
    return None

def get_all_available_models() -> Dict[str, Dict]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ ë°˜í™˜"""
    return {
        key: info for key, info in DOWNLOADED_MODELS.items()
        if is_model_available(key)
    }
'''
        
        with open(output_path, 'w') as f:
            f.write(content)
        
        logger.info(f"âœ… Python ì„¤ì • íŒŒì¼ ìƒì„±: {output_path}")
    
    def show_download_summary(self):
        """ë‹¤ìš´ë¡œë“œ ìš”ì•½ í‘œì‹œ"""
        logger.info("ğŸ“Š ë‹¤ìš´ë¡œë“œ ìš”ì•½:")
        logger.info("=" * 60)
        
        total_downloaded = 0
        for model_key, model_info in self.models.items():
            local_path = self.checkpoints_dir / model_info['local_dir']
            
            if local_path.exists():
                size = sum(f.stat().st_size for f in local_path.rglob('*') if f.is_file())
                size_gb = size / (1024**3)
                total_downloaded += size_gb
                
                logger.info(f"âœ… {model_info['name']}: {size_gb:.1f} GB")
            else:
                logger.info(f"âŒ {model_info['name']}: ë‹¤ìš´ë¡œë“œ ì•ˆë¨")
        
        logger.info(f"ğŸ“¦ ì´ ë‹¤ìš´ë¡œë“œ í¬ê¸°: {total_downloaded:.1f} GB")
    
    def run(self):
        """ì „ì²´ ë‹¤ìš´ë¡œë“œ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("ğŸ¤– MyCloset AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        logger.info("=" * 60)
        
        # 1. ì˜ì¡´ì„± í™•ì¸
        if not self.check_dependencies():
            logger.error("âŒ ì˜ì¡´ì„± í™•ì¸ ì‹¤íŒ¨")
            return False
        
        # 2. ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
        if not self.check_disk_space():
            response = input("âš ï¸ ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if response.lower() != 'y':
                logger.info("âŒ ë‹¤ìš´ë¡œë“œ ì·¨ì†Œë¨")
                return False
        
        # 3. ë‹¤ìš´ë¡œë“œ í™•ì¸
        logger.info(f"ğŸ“¥ ë‹¤ìŒ {len(self.models)}ê°œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤:")
        for model_key, model_info in self.models.items():
            logger.info(f"   - {model_info['name']} ({model_info['size_gb']} GB)")
        
        logger.info(f"ğŸ“¦ ì´ í¬ê¸°: {self.total_size_gb:.1f} GB")
        logger.info("â° ì˜ˆìƒ ì†Œìš” ì‹œê°„: 30ë¶„ ~ 2ì‹œê°„")
        
        response = input("\nê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if response.lower() != 'y':
            logger.info("âŒ ë‹¤ìš´ë¡œë“œ ì·¨ì†Œë¨")
            return False
        
        # 4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìš°ì„ ìˆœìœ„ìˆœ)
        logger.info("ğŸš€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        start_time = time.time()
        
        success_count = 0
        sorted_models = sorted(self.models.items(), key=lambda x: x[1]['priority'])
        
        for model_key, model_info in sorted_models:
            if self.download_model(model_key, model_info):
                success_count += 1
            
            # ì§„í–‰ë¥  í‘œì‹œ
            progress = (success_count / len(self.models)) * 100
            logger.info(f"ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% ({success_count}/{len(self.models)})")
        
        # 5. ì„¤ì • íŒŒì¼ ìƒì„±
        self.create_model_config()
        
        # 6. ê²°ê³¼ ì¶œë ¥
        elapsed_time = time.time() - start_time
        logger.info("=" * 60)
        logger.info(f"ğŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ! ({elapsed_time:.1f}ì´ˆ)")
        logger.info(f"âœ… ì„±ê³µ: {success_count}/{len(self.models)} ëª¨ë¸")
        
        if success_count == len(self.models):
            logger.info("ğŸš€ ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ! ì´ì œ ì‹¤ì œ AI ì¶”ë¡ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
        else:
            logger.warning(f"âš ï¸ {len(self.models) - success_count}ê°œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        
        self.show_download_summary()
        
        # 7. ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
        logger.info("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info("1. python3 app/main.py  # ì„œë²„ ì‹¤í–‰")
        logger.info("2. http://localhost:8000/docs  # API ë¬¸ì„œ í™•ì¸")
        logger.info("3. ì‹¤ì œ AI ê°€ìƒ í”¼íŒ… í…ŒìŠ¤íŠ¸!")
        
        return success_count == len(self.models)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        downloader = AIModelDownloader()
        success = downloader.run()
        
        if success:
            print("\nğŸ‰ ëª¨ë“  AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            print("ì´ì œ ì‹¤ì œ AI ì¶”ë¡ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
        else:
            print("\nâš ï¸ ì¼ë¶€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            print("ë‹¤ì‹œ ì‹¤í–‰í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
            
    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()