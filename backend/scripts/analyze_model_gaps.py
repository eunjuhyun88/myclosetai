
#!/usr/bin/env python3
"""
ğŸ” 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ê°­ ë¶„ì„
í˜„ì¬ ë³´ìœ  vs í•„ìš” ëª¨ë¸ ë¶„ì„ ë° ë‹¤ìš´ë¡œë“œ ìš°ì„ ìˆœìœ„ ì œì•ˆ
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ModelRequirement:
    """ëª¨ë¸ ìš”êµ¬ì‚¬í•­"""
    step: str
    step_name: str
    required_models: List[str]
    alternative_models: List[str]
    current_status: str  # "available", "missing", "partial"
    priority: int  # 1=í•„ìˆ˜, 2=ì¤‘ìš”, 3=ê¶Œì¥
    size_estimate_gb: float

class PipelineModelGapAnalyzer:
    """8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ê°­ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.pipeline_requirements = self._define_pipeline_requirements()
        self.current_models = self._load_current_models()
        
    def _define_pipeline_requirements(self) -> Dict[str, ModelRequirement]:
        """8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ì •ì˜"""
        return {
            "step_01": ModelRequirement(
                step="step_01_human_parsing",
                step_name="ì¸ì²´ íŒŒì‹± (Human Parsing)",
                required_models=[
                    "Graphonomy ATR weights",
                    "Graphonomy LIP weights", 
                    "Human parsing model weights"
                ],
                alternative_models=[
                    "Self-Correction-Human-Parsing",
                    "MediaPipe Selfie Segmentation",
                    "DeepLabV3+ Human"
                ],
                current_status="partial",
                priority=1,
                size_estimate_gb=0.5
            ),
            
            "step_02": ModelRequirement(
                step="step_02_pose_estimation", 
                step_name="í¬ì¦ˆ ì¶”ì • (Pose Estimation)",
                required_models=[
                    "OpenPose Body Model",
                    "OpenPose pose weights"
                ],
                alternative_models=[
                    "MediaPipe Pose",
                    "PoseNet",
                    "DensePose"
                ],
                current_status="available",
                priority=1,
                size_estimate_gb=0.2
            ),
            
            "step_03": ModelRequirement(
                step="step_03_cloth_segmentation",
                step_name="ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Cloth Segmentation)", 
                required_models=[
                    "UÂ²-Net pretrained weights",
                    "UÂ²-Net human segmentation", 
                    "Cloth-specific segmentation model"
                ],
                alternative_models=[
                    "RemBG",
                    "SAM (Segment Anything)",
                    "DeepLabV3+ clothing"
                ],
                current_status="missing",
                priority=1,
                size_estimate_gb=0.3
            ),
            
            "step_04": ModelRequirement(
                step="step_04_geometric_matching",
                step_name="ê¸°í•˜í•™ì  ë§¤ì¹­ (Geometric Matching)",
                required_models=[
                    "HR-VITON GMM weights",
                    "Geometric Matching Module",
                    "TPS transformation model"
                ],
                alternative_models=[
                    "VITON GMM",
                    "CP-VTON matching",
                    "Custom geometric matching"
                ],
                current_status="missing",
                priority=2,
                size_estimate_gb=0.1
            ),
            
            "step_05": ModelRequirement(
                step="step_05_cloth_warping",
                step_name="ì˜ë¥˜ ì›Œí•‘ (Cloth Warping)",
                required_models=[
                    "HR-VITON TOM weights",
                    "Try-On Module weights",
                    "Cloth warping model"
                ],
                alternative_models=[
                    "VITON TOM",
                    "CP-VTON warping",
                    "PF-AFN warping"
                ],
                current_status="missing",
                priority=2,
                size_estimate_gb=0.2
            ),
            
            "step_06": ModelRequirement(
                step="step_06_virtual_fitting",
                step_name="ê°€ìƒ í”¼íŒ… ìƒì„± (Virtual Fitting)",
                required_models=[
                    "Stable Diffusion v1.5",
                    "OOTDiffusion weights",
                    "ControlNet pose"
                ],
                alternative_models=[
                    "SDXL",
                    "HR-VITON full",
                    "ACGPN"
                ],
                current_status="available",
                priority=1,
                size_estimate_gb=10.0
            ),
            
            "step_07": ModelRequirement(
                step="step_07_post_processing",
                step_name="í›„ì²˜ë¦¬ (Post Processing)",
                required_models=[
                    "Real-ESRGAN weights",
                    "Face enhancement model",
                    "Upscaling model"
                ],
                alternative_models=[
                    "GFPGAN",
                    "CodeFormer",
                    "SwinIR"
                ],
                current_status="missing",
                priority=3,
                size_estimate_gb=0.1
            ),
            
            "step_08": ModelRequirement(
                step="step_08_quality_assessment",
                step_name="í’ˆì§ˆ í‰ê°€ (Quality Assessment)",
                required_models=[
                    "LPIPS model",
                    "FID calculation model",
                    "Quality metrics"
                ],
                alternative_models=[
                    "CLIP quality assessment",
                    "Custom metrics",
                    "SSIM/PSNR only"
                ],
                current_status="missing",
                priority=3,
                size_estimate_gb=0.1
            )
        }
    
    def _load_current_models(self) -> Dict[str, Any]:
        """í˜„ì¬ ë³´ìœ  ëª¨ë¸ ì •ë³´ ë¡œë“œ"""
        try:
            from app.core.optimized_model_paths import ANALYZED_MODELS
            return ANALYZED_MODELS
        except ImportError:
            logger.warning("optimized_model_pathsë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŒ")
            return {}
    
    def analyze_gaps(self) -> Dict[str, Any]:
        """ëª¨ë¸ ê°­ ë¶„ì„"""
        logger.info("ğŸ” 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ê°­ ë¶„ì„ ì‹œì‘...")
        
        analysis_result = {
            "pipeline_completeness": {},
            "missing_models": [],
            "download_priorities": [],
            "current_coverage": {},
            "recommendations": []
        }
        
        total_steps = len(self.pipeline_requirements)
        ready_steps = 0
        
        for step_key, requirement in self.pipeline_requirements.items():
            step_analysis = self._analyze_step_requirement(requirement)
            analysis_result["pipeline_completeness"][step_key] = step_analysis
            
            if step_analysis["status"] == "ready":
                ready_steps += 1
            elif step_analysis["status"] == "missing":
                analysis_result["missing_models"].extend(step_analysis["missing_models"])
        
        # ì „ì²´ ì™„ì„±ë„ ê³„ì‚°
        completeness = ready_steps / total_steps
        analysis_result["overall_completeness"] = completeness
        
        # ë‹¤ìš´ë¡œë“œ ìš°ì„ ìˆœìœ„ ìƒì„±
        analysis_result["download_priorities"] = self._generate_download_priorities()
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        analysis_result["recommendations"] = self._generate_recommendations(completeness)
        
        self._display_analysis_results(analysis_result)
        
        return analysis_result
    
    def _analyze_step_requirement(self, requirement: ModelRequirement) -> Dict[str, Any]:
        """ê°œë³„ ë‹¨ê³„ ìš”êµ¬ì‚¬í•­ ë¶„ì„"""
        step_analysis = {
            "step": requirement.step,
            "step_name": requirement.step_name,
            "status": "missing",
            "available_models": [],
            "missing_models": [],
            "alternatives_available": [],
            "priority": requirement.priority
        }
        
        # í˜„ì¬ ëª¨ë¸ê³¼ ë§¤ì¹­
        available_count = 0
        total_required = len(requirement.required_models)
        
        for required_model in requirement.required_models:
            found = False
            for current_model_key, current_model_info in self.current_models.items():
                if self._is_model_match(required_model, current_model_key, current_model_info):
                    step_analysis["available_models"].append(current_model_key)
                    available_count += 1
                    found = True
                    break
            
            if not found:
                step_analysis["missing_models"].append(required_model)
        
        # ëŒ€ì²´ ëª¨ë¸ í™•ì¸
        for alt_model in requirement.alternative_models:
            for current_model_key, current_model_info in self.current_models.items():
                if self._is_model_match(alt_model, current_model_key, current_model_info):
                    step_analysis["alternatives_available"].append(current_model_key)
        
        # ìƒíƒœ ê²°ì •
        if available_count >= total_required:
            step_analysis["status"] = "ready"
        elif available_count > 0 or step_analysis["alternatives_available"]:
            step_analysis["status"] = "partial"
        else:
            step_analysis["status"] = "missing"
        
        return step_analysis
    
    def _is_model_match(self, required_model: str, current_key: str, current_info: Dict) -> bool:
        """ëª¨ë¸ ë§¤ì¹­ í™•ì¸"""
        required_lower = required_model.lower()
        current_key_lower = current_key.lower()
        current_name_lower = current_info.get("name", "").lower()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        keywords_map = {
            "stable diffusion": ["stable", "diffusion"],
            "ootdiffusion": ["ootd", "diffusion"],
            "graphonomy": ["graphonomy"],
            "openpose": ["openpose", "pose"],
            "u2net": ["u2net", "background"],
            "human parsing": ["human", "parsing"],
            "gmm": ["gmm", "geometric"],
            "tom": ["tom", "try-on"],
            "esrgan": ["esrgan", "upscal"],
            "clip": ["clip", "vit"]
        }
        
        for pattern, keywords in keywords_map.items():
            if pattern in required_lower:
                if any(keyword in current_key_lower or keyword in current_name_lower 
                      for keyword in keywords):
                    return current_info.get("ready", False)
        
        return False
    
    def _generate_download_priorities(self) -> List[Dict[str, Any]]:
        """ë‹¤ìš´ë¡œë“œ ìš°ì„ ìˆœìœ„ ìƒì„±"""
        priorities = [
            # ìš°ì„ ìˆœìœ„ 1 (í•„ìˆ˜)
            {
                "model": "UÂ²-Net Human Segmentation",
                "step": "step_03_cloth_segmentation", 
                "reason": "ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ í•„ìˆ˜ - íŒŒì´í”„ë¼ì¸ í•µì‹¬",
                "url": "https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ",
                "filename": "u2net.pth",
                "size_gb": 0.176,
                "priority": 1
            },
            {
                "model": "Graphonomy ATR weights",
                "step": "step_01_human_parsing",
                "reason": "ì¸ì²´ íŒŒì‹± ì •í™•ë„ í–¥ìƒ",
                "url": "https://drive.google.com/uc?id=1ruJg4lqR_jgQPj-9K0PP-L2vJERYOxLP",
                "filename": "graphonomy_atr.pth", 
                "size_gb": 0.178,
                "priority": 1
            },
            {
                "model": "Graphonomy LIP weights",
                "step": "step_01_human_parsing",
                "reason": "ì¸ì²´ íŒŒì‹± ëŒ€ì²´ ëª¨ë¸",
                "url": "https://drive.google.com/uc?id=1k4dllHpu0bdx38J7H28rVVLpU-kOHmnH",
                "filename": "graphonomy_lip.pth",
                "size_gb": 0.178,
                "priority": 1
            },
            
            # ìš°ì„ ìˆœìœ„ 2 (ì¤‘ìš”)
            {
                "model": "HR-VITON GMM weights",
                "step": "step_04_geometric_matching",
                "reason": "ê¸°í•˜í•™ì  ë§¤ì¹­ ì •í™•ë„",
                "url": "https://drive.google.com/uc?id=1WJkwlCJXFWsEgdNGWSoXDhpqtNmwcaVY",
                "filename": "gmm_final.pth",
                "size_gb": 0.045,
                "priority": 2
            },
            {
                "model": "HR-VITON TOM weights", 
                "step": "step_05_cloth_warping",
                "reason": "ì˜ë¥˜ ì›Œí•‘ í’ˆì§ˆ í–¥ìƒ",
                "url": "https://drive.google.com/uc?id=1YJU5kNNL8Y-CqaXq-hOjJlh2hZ3s2qY",
                "filename": "tom_final.pth",
                "size_gb": 0.089,
                "priority": 2
            },
            
            # ìš°ì„ ìˆœìœ„ 3 (ê¶Œì¥)
            {
                "model": "Real-ESRGAN",
                "step": "step_07_post_processing",
                "reason": "ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ",
                "url": "https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth",
                "filename": "RealESRGAN_x4plus.pth",
                "size_gb": 0.067,
                "priority": 3
            }
        ]
        
        return sorted(priorities, key=lambda x: x["priority"])
    
    def _generate_recommendations(self, completeness: float) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if completeness < 0.3:
            recommendations.append("ğŸš¨ íŒŒì´í”„ë¼ì¸ ì™„ì„±ë„ê°€ 30% ë¯¸ë§Œì…ë‹ˆë‹¤. ì¦‰ì‹œ í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        elif completeness < 0.6:
            recommendations.append("âš ï¸ íŒŒì´í”„ë¼ì¸ì´ ë¶€ë¶„ì ìœ¼ë¡œ ì‘ë™ ê°€ëŠ¥í•©ë‹ˆë‹¤. í•µì‹¬ ëª¨ë¸ë“¤ì„ ìš°ì„  ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
        else:
            recommendations.append("âœ… íŒŒì´í”„ë¼ì¸ì´ ëŒ€ë¶€ë¶„ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ì„±ëŠ¥ í–¥ìƒ ëª¨ë¸ë“¤ì„ ì¶”ê°€í•˜ì„¸ìš”.")
        
        recommendations.extend([
            "ğŸ¯ 1ìˆœìœ„: UÂ²-Net (ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜) - íŒŒì´í”„ë¼ì¸ í•µì‹¬ ë‹¨ê³„",
            "ğŸ¯ 2ìˆœìœ„: Graphonomy weights (ì¸ì²´ íŒŒì‹±) - ì •í™•ë„ í–¥ìƒ", 
            "ğŸ’¡ ëŒ€ì²´ì•ˆ: MediaPipe + RemBGë¡œ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… êµ¬í˜„ ê°€ëŠ¥",
            "ğŸ”§ ë‹¨ê³„ì  ì ‘ê·¼: í•„ìˆ˜ ëª¨ë¸(1-2GB) â†’ ì„±ëŠ¥ ëª¨ë¸(ì¶”ê°€ 1GB) â†’ ê³ ê¸‰ ëª¨ë¸"
        ])
        
        return recommendations
    
    def _display_analysis_results(self, analysis: Dict[str, Any]):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        logger.info("ğŸ“Š 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ê°­ ë¶„ì„ ê²°ê³¼")
        logger.info("=" * 60)
        
        completeness = analysis["overall_completeness"]
        logger.info(f"ğŸ¯ ì „ì²´ ì™„ì„±ë„: {completeness:.1%}")
        
        logger.info(f"\nğŸ“‹ ë‹¨ê³„ë³„ ìƒíƒœ:")
        for step_key, step_analysis in analysis["pipeline_completeness"].items():
            status_emoji = {
                "ready": "âœ…",
                "partial": "âš ï¸", 
                "missing": "âŒ"
            }
            emoji = status_emoji.get(step_analysis["status"], "â“")
            logger.info(f"   {emoji} {step_analysis['step_name']}: {step_analysis['status']}")
            
            if step_analysis["available_models"]:
                logger.info(f"      ë³´ìœ : {', '.join(step_analysis['available_models'][:2])}")
            if step_analysis["missing_models"]:
                logger.info(f"      ë¶€ì¡±: {', '.join(step_analysis['missing_models'][:2])}")
        
        logger.info(f"\nğŸš€ ë‹¤ìš´ë¡œë“œ ìš°ì„ ìˆœìœ„ (ìƒìœ„ 5ê°œ):")
        for i, priority_item in enumerate(analysis["download_priorities"][:5], 1):
            logger.info(f"   {i}. {priority_item['model']} ({priority_item['size_gb']:.2f}GB)")
            logger.info(f"      â†’ {priority_item['reason']}")
        
        logger.info(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for rec in analysis["recommendations"]:
            logger.info(f"   {rec}")
    
    def create_download_script(self, analysis: Dict[str, Any]):
        """ë¶€ì¡±í•œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        logger.info("ğŸ“ ë¶€ì¡±í•œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘...")
        
        script_content = '''#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - í•„ìˆ˜ ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ
8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ì„±ì„ ìœ„í•œ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë‹¤ìš´ë¡œë“œ
"""

import os
import sys
import gdown
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def download_priority_models():
    """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    
    print("ğŸ”¥ MyCloset AI - í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
    print("=" * 50)
    
    base_dir = Path("ai_models/checkpoints")
    base_dir.mkdir(parents=True, exist_ok=True)
    
    # ìš°ì„ ìˆœìœ„ ëª¨ë¸ë“¤
    models = [
'''
        
        # ìš°ì„ ìˆœìœ„ ëª¨ë¸ë“¤ ì¶”ê°€
        for model in analysis["download_priorities"][:6]:  # ìƒìœ„ 6ê°œ
            script_content += f'''        {{
            "name": "{model['model']}",
            "step": "{model['step']}",
            "url": "{model['url']}", 
            "filename": "{model['filename']}",
            "size_gb": {model['size_gb']},
            "priority": {model['priority']},
            "reason": "{model['reason']}"
        }},
'''
        
        script_content += '''    ]
    
    total_size = sum(model["size_gb"] for model in models)
    logger.info(f"ğŸ“¦ ë‹¤ìš´ë¡œë“œ ì˜ˆì •: {len(models)}ê°œ ëª¨ë¸ ({total_size:.2f}GB)")
    
    success_count = 0
    
    for i, model in enumerate(models, 1):
        logger.info(f"\\n[{i}/{len(models)}] {model['name']} ë‹¤ìš´ë¡œë“œ ì¤‘...")
        logger.info(f"   ì´ìœ : {model['reason']}")
        logger.info(f"   í¬ê¸°: {model['size_gb']:.2f}GB")
        
        # ë‹¨ê³„ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        step_dir = base_dir / model["step"]
        step_dir.mkdir(exist_ok=True)
        output_path = step_dir / model["filename"]
        
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if output_path.exists():
            file_size_mb = output_path.stat().st_size / (1024 * 1024)
            if file_size_mb > 10:  # 10MB ì´ìƒì´ë©´ ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                logger.info(f"   âœ… ì´ë¯¸ ì¡´ì¬í•¨: {model['name']} ({file_size_mb:.1f}MB)")
                success_count += 1
                continue
        
        try:
            if "drive.google.com" in model["url"]:
                # Google Drive ë‹¤ìš´ë¡œë“œ
                success = gdown.download(model["url"], str(output_path), quiet=False)
                if success:
                    logger.info(f"   âœ… {model['name']} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                    success_count += 1
                else:
                    logger.error(f"   âŒ {model['name']} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            else:
                logger.info(f"   âš ï¸ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ í•„ìš”: {model['url']}")
                logger.info(f"      ë‹¤ìš´ë¡œë“œ í›„ {output_path}ì— ì €ì¥í•˜ì„¸ìš”")
                
        except Exception as e:
            logger.error(f"   âŒ {model['name']} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    logger.info(f"\\nğŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {success_count}/{len(models)}ê°œ")
    
    if success_count >= len(models) * 0.8:  # 80% ì´ìƒ ì„±ê³µ
        logger.info("âœ… í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì„±ê³µ! ì´ì œ íŒŒì´í”„ë¼ì¸ì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        logger.info("\\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info("   python scripts/analyze_checkpoints.py  # ëª¨ë¸ ì¬ìŠ¤ìº”")
        logger.info("   python scripts/test_loaded_models.py   # íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    else:
        logger.warning(f"âš ï¸ ì¼ë¶€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨. ìˆ˜ë™ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    try:
        # gdown ì„¤ì¹˜ í™•ì¸
        import gdown
    except ImportError:
        print("âŒ gdownì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜: pip install gdown")
        sys.exit(1)
    
    download_priority_models()
'''
        
        # ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ì €ì¥
        script_path = Path("scripts/download_missing_models.py")
        with open(script_path, 'w') as f:
            f.write(script_content)
        
        # ì‹¤í–‰ ê¶Œí•œ ì¶”ê°€
        script_path.chmod(0o755)
        
        logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: {script_path}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ” MyCloset AI - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ê°­ ë¶„ì„")
    print("=" * 60)
    
    analyzer = PipelineModelGapAnalyzer()
    analysis_result = analyzer.analyze_gaps()
    
    # ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    analyzer.create_download_script(analysis_result)
    
    # ê²°ê³¼ ì €ì¥
    result_path = Path("ai_models/pipeline_gap_analysis.json")
    result_path.parent.mkdir(exist_ok=True)
    with open(result_path, 'w') as f:
        json.dump(analysis_result, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š ì™„ì„±ë„: {analysis_result['overall_completeness']:.1%}")
    print(f"ğŸ“ ë¶„ì„ ê²°ê³¼: {result_path}")
    print(f"ğŸ“ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸: scripts/download_missing_models.py")
    
    total_download_size = sum(item["size_gb"] for item in analysis_result["download_priorities"][:5])
    print(f"ğŸ’¾ í•„ìˆ˜ ë‹¤ìš´ë¡œë“œ í¬ê¸°: {total_download_size:.2f}GB")
    
    print(f"\nğŸš€ ë‹¤ìŒ ì‹¤í–‰:")
    print(f"   python scripts/download_missing_models.py")
    
    return True

if __name__ == "__main__":
    main()
