#!/usr/bin/env python3
"""
ğŸ MyCloset AI Backend - ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
âœ… M3 Max ìµœì í™” ì„¤ì •
âœ… ê°œë°œ/í”„ë¡œë•ì…˜ í™˜ê²½ ëŒ€ì‘
âœ… ìë™ ì˜ì¡´ì„± ì²´í¬
âœ… ìƒì„¸ ë¡œê¹…
"""

import os
import sys
import time
import subprocess
import platform
from pathlib import Path

# ìƒ‰ìƒ ì¶œë ¥ì„ ìœ„í•œ ANSI ì½”ë“œ
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def print_colored(message: str, color: str = Colors.OKGREEN):
    """ìƒ‰ìƒ ë©”ì‹œì§€ ì¶œë ¥"""
    print(f"{color}{message}{Colors.ENDC}")

def print_header(message: str):
    """í—¤ë” ë©”ì‹œì§€ ì¶œë ¥"""
    print_colored(f"\nğŸš€ {message}", Colors.HEADER + Colors.BOLD)
    print_colored("=" * (len(message) + 3), Colors.HEADER)

def print_success(message: str):
    """ì„±ê³µ ë©”ì‹œì§€ ì¶œë ¥"""
    print_colored(f"âœ… {message}", Colors.OKGREEN)

def print_info(message: str):
    """ì •ë³´ ë©”ì‹œì§€ ì¶œë ¥"""
    print_colored(f"â„¹ï¸  {message}", Colors.OKBLUE)

def print_warning(message: str):
    """ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥"""
    print_colored(f"âš ï¸  {message}", Colors.WARNING)

def print_error(message: str):
    """ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥"""
    print_colored(f"âŒ {message}", Colors.FAIL)

def check_python_version():
    """Python ë²„ì „ í™•ì¸"""
    version = sys.version_info
    required_major, required_minor = 3, 9
    
    if version.major < required_major or (version.major == required_major and version.minor < required_minor):
        print_error(f"Python {required_major}.{required_minor}+ í•„ìš”. í˜„ì¬: {version.major}.{version.minor}.{version.micro}")
        return False
    
    print_success(f"Python {version.major}.{version.minor}.{version.micro}")
    return True

def detect_system():
    """ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€"""
    system = platform.system()
    machine = platform.machine()
    
    print_info(f"ìš´ì˜ì²´ì œ: {system}")
    print_info(f"ì•„í‚¤í…ì²˜: {machine}")
    
    # M3 Max ê°ì§€
    is_m3_max = False
    if system == "Darwin" and machine == "arm64":
        try:
            # macOSì—ì„œ CPU ì •ë³´ í™•ì¸
            result = subprocess.run(
                ["system_profiler", "SPHardwareDataType"], 
                capture_output=True, 
                text=True, 
                timeout=5
            )
            if "Apple M3 Max" in result.stdout:
                is_m3_max = True
                print_success("ğŸ Apple M3 Max ê°ì§€ë¨!")
            else:
                print_info("ğŸ Apple Silicon ê°ì§€ë¨ (M3 Max ì•„ë‹˜)")
        except:
            print_warning("ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸ ì‹¤íŒ¨")
    
    return {
        "system": system,
        "machine": machine,
        "is_m3_max": is_m3_max
    }

def check_virtual_env():
    """ê°€ìƒí™˜ê²½ í™•ì¸"""
    if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        venv_path = sys.prefix
        print_success(f"ê°€ìƒí™˜ê²½ í™œì„±í™”ë¨: {venv_path}")
        return True
    else:
        print_warning("ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return False

def check_dependencies():
    """í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸"""
    dependencies = [
        ("fastapi", "FastAPI"),
        ("uvicorn", "Uvicorn"),
        ("torch", "PyTorch"),
        ("PIL", "Pillow"),
        ("psutil", "psutil")
    ]
    
    missing = []
    for module, name in dependencies:
        try:
            __import__(module)
            print_success(f"{name} ì„¤ì¹˜ë¨")
        except ImportError:
            print_error(f"{name} ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            missing.append(name)
    
    if missing:
        print_error(f"ëˆ„ë½ëœ íŒ¨í‚¤ì§€: {', '.join(missing)}")
        print_info("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install -r requirements.txt")
        return False
    
    return True

def check_pytorch_mps():
    """PyTorch MPS ì§€ì› í™•ì¸"""
    try:
        import torch
        print_success(f"PyTorch {torch.__version__}")
        
        if torch.backends.mps.is_available():
            print_success("ğŸ MPS (Metal Performance Shaders) ì‚¬ìš© ê°€ëŠ¥")
            return True
        else:
            print_warning("MPS ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            return False
    except ImportError:
        print_error("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return False

def setup_environment(system_info):
    """í™˜ê²½ ë³€ìˆ˜ ì„¤ì •"""
    env_vars = {}
    
    # ê¸°ë³¸ ì„¤ì •
    env_vars.update({
        "APP_NAME": "MyCloset AI Backend",
        "APP_VERSION": "3.0.0",
        "DEBUG": "True",
        "HOST": "0.0.0.0",
        "PORT": "8000"
    })
    
    # M3 Max ìµœì í™” ì„¤ì •
    if system_info["is_m3_max"]:
        env_vars.update({
            "PYTORCH_ENABLE_MPS_FALLBACK": "1",
            "PYTORCH_MPS_HIGH_WATERMARK_RATIO": "0.0",
            "OMP_NUM_THREADS": "16",
            "MKL_NUM_THREADS": "16",
            "DEVICE_TYPE": "mps"
        })
        print_success("ğŸ M3 Max ìµœì í™” í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ë¨")
    else:
        env_vars.update({
            "OMP_NUM_THREADS": "4",
            "MKL_NUM_THREADS": "4",
            "DEVICE_TYPE": "cpu"
        })
        print_info("CPU ìµœì í™” í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ë¨")
    
    # í™˜ê²½ ë³€ìˆ˜ ì ìš©
    for key, value in env_vars.items():
        os.environ[key] = value
    
    return env_vars

def find_main_module():
    """main.py ëª¨ë“ˆ ê²½ë¡œ ì°¾ê¸°"""
    possible_paths = [
        "app.main:app",
        "main:app",
        "app:app"
    ]
    
    for path in possible_paths:
        module_path = path.split(":")[0].replace(".", "/") + ".py"
        if Path(module_path).exists():
            print_success(f"ë©”ì¸ ëª¨ë“ˆ ë°œê²¬: {path}")
            return path
    
    # ê¸°ë³¸ê°’
    main_path = "app.main:app"
    print_info(f"ê¸°ë³¸ ëª¨ë“ˆ ì‚¬ìš©: {main_path}")
    return main_path

def get_server_config(system_info):
    """ì„œë²„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    config = {
        "host": os.getenv("HOST", "0.0.0.0"),
        "port": int(os.getenv("PORT", 8000)),
        "reload": os.getenv("DEBUG", "True").lower() == "true",
        "workers": 1,  # M3 Max GPU ë©”ëª¨ë¦¬ ê³µìœ  ì´ìŠˆ ë°©ì§€
        "log_level": "info",
        "access_log": True,
        "timeout_keep_alive": 30,
        "limit_concurrency": 1000,
        "limit_max_requests": 10000
    }
    
    # M3 Max íŠ¹í™” ì„¤ì •
    if system_info["is_m3_max"]:
        config.update({
            "workers": 1,  # ë‹¨ì¼ ì›Œì»¤ (GPU ë©”ëª¨ë¦¬ ìµœì í™”)
            "limit_concurrency": 500,  # ë™ì‹œ ì—°ê²° ì œí•œ (ë©”ëª¨ë¦¬ ê³ ë ¤)
        })
        print_info("ğŸ M3 Max ìµœì í™” ì„œë²„ ì„¤ì • ì ìš©")
    
    return config

def run_server():
    """ì„œë²„ ì‹¤í–‰"""
    print_header("MyCloset AI Backend ì„œë²„ ì‹œì‘")
    
    # 1. Python ë²„ì „ í™•ì¸
    if not check_python_version():
        sys.exit(1)
    
    # 2. ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€
    system_info = detect_system()
    
    # 3. ê°€ìƒí™˜ê²½ í™•ì¸
    venv_active = check_virtual_env()
    if not venv_active:
        print_warning("ê°€ìƒí™˜ê²½ í™œì„±í™”ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤")
        response = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if response.lower() != 'y':
            print_info("ê°€ìƒí™˜ê²½ í™œì„±í™” í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”")
            print_info("ëª…ë ¹ì–´: source venv/bin/activate")
            sys.exit(1)
    
    # 4. ì˜ì¡´ì„± í™•ì¸
    if not check_dependencies():
        sys.exit(1)
    
    # 5. PyTorch MPS í™•ì¸
    mps_available = check_pytorch_mps()
    
    # 6. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    env_vars = setup_environment(system_info)
    
    # 7. ë©”ì¸ ëª¨ë“ˆ í™•ì¸
    main_module = find_main_module()
    
    # 8. ì„œë²„ ì„¤ì •
    config = get_server_config(system_info)
    
    # 9. ì„œë²„ ì‹œì‘ ì •ë³´ ì¶œë ¥
    print_header("ì„œë²„ ì‹¤í–‰ ì •ë³´")
    print_info(f"ğŸ“ ì£¼ì†Œ: http://{config['host']}:{config['port']}")
    print_info(f"ğŸ“– API ë¬¸ì„œ: http://{config['host']}:{config['port']}/docs")
    print_info(f"ğŸ”„ ìë™ ë¦¬ë¡œë“œ: {'âœ…' if config['reload'] else 'âŒ'}")
    print_info(f"ğŸ‘¥ ì›Œì»¤ ìˆ˜: {config['workers']}")
    print_info(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {'MPS' if mps_available else 'CPU'}")
    print_info(f"ğŸ M3 Max ìµœì í™”: {'âœ…' if system_info['is_m3_max'] else 'âŒ'}")
    
    # 10. ì„œë²„ ì‹¤í–‰
    try:
        import uvicorn
        
        print_header("ì„œë²„ ì‹œì‘ ì¤‘...")
        print_info("ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”")
        
        time.sleep(1)  # ë©”ì‹œì§€ ì¶œë ¥ ëŒ€ê¸°
        
        uvicorn.run(
            main_module,
            host=config["host"],
            port=config["port"],
            reload=config["reload"],
            workers=config["workers"],
            log_level=config["log_level"],
            access_log=config["access_log"],
            timeout_keep_alive=config["timeout_keep_alive"],
            limit_concurrency=config["limit_concurrency"],
            limit_max_requests=config["limit_max_requests"],
            loop="auto"
        )
        
    except KeyboardInterrupt:
        print_success("\nğŸ›‘ ì„œë²„ê°€ ì•ˆì „í•˜ê²Œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
    except ImportError:
        print_error("Uvicornì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        print_info("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install uvicorn")
        sys.exit(1)
    except Exception as e:
        print_error(f"ì„œë²„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ë¡œ ë³€ê²½
    script_dir = Path(__file__).parent
    os.chdir(script_dir)
    
    print_colored("""
ğŸ MyCloset AI Backend Server
=============================
M3 Max 128GB ìµœì í™” ë²„ì „
8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì§€ì›
""", Colors.HEADER + Colors.BOLD)
    
    run_server()

if __name__ == "__main__":
    main()