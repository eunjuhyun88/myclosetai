#!/usr/bin/env python3
"""
ğŸ” checkpoints ë””ë ‰í† ë¦¬ ìƒì„¸ ë¶„ì„
80GBì˜ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ë“¤ì„ ë¶„ì„í•˜ê³  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ì‹ë³„
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CheckpointAnalyzer:
    """ì²´í¬í¬ì¸íŠ¸ ìƒì„¸ ë¶„ì„ê¸°"""
    
    def __init__(self, checkpoints_dir: str = "ai_models/checkpoints"):
        self.checkpoints_dir = Path(checkpoints_dir)
        self.analyzed_models = {}
        
        logger.info(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬: {self.checkpoints_dir.absolute()}")
        
        if not self.checkpoints_dir.exists():
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.checkpoints_dir}")
            raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.checkpoints_dir}")
    
    def analyze_all_checkpoints(self) -> Dict[str, Any]:
        """ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ìƒì„¸ ë¶„ì„"""
        logger.info("ğŸ” ì²´í¬í¬ì¸íŠ¸ ìƒì„¸ ë¶„ì„ ì‹œì‘...")
        logger.info(f"ğŸ’¾ ì´ ìš©ëŸ‰: 80.3GB - ë§¤ìš° í° ê·œëª¨!")
        
        # ê° ì„œë¸Œë””ë ‰í† ë¦¬ ë¶„ì„
        for subdir in sorted(self.checkpoints_dir.iterdir()):
            if subdir.is_dir():
                self._analyze_model_directory(subdir)
        
        # ë£¨íŠ¸ ë ˆë²¨ íŒŒì¼ë“¤ ë¶„ì„
        self._analyze_root_files()
        
        # ë¶„ì„ ê²°ê³¼ ìš”ì•½
        self._show_analysis_summary()
        
        return self.analyzed_models
    
    def _analyze_model_directory(self, model_dir: Path):
        """ê°œë³„ ëª¨ë¸ ë””ë ‰í† ë¦¬ ë¶„ì„"""
        model_name = model_dir.name
        logger.info(f"\nğŸ“¦ ë¶„ì„ ì¤‘: {model_name}")
        
        # íŒŒì¼ í†µê³„
        all_files = list(model_dir.rglob("*"))
        model_files = [f for f in all_files if f.is_file()]
        
        # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
        file_types = {}
        checkpoint_files = []
        config_files = []
        
        for file_path in model_files:
            ext = file_path.suffix.lower()
            size_mb = file_path.stat().st_size / (1024 * 1024)
            
            # íŒŒì¼ íƒ€ì… ì¹´ìš´íŠ¸
            file_types[ext] = file_types.get(ext, 0) + 1
            
            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‹ë³„
            if ext in ['.pth', '.pt', '.bin', '.ckpt', '.pkl', '.caffemodel']:
                checkpoint_files.append({
                    'name': file_path.name,
                    'path': str(file_path.relative_to(model_dir)),
                    'size_mb': round(size_mb, 1),
                    'type': ext
                })
            
            # ì„¤ì • íŒŒì¼ ì‹ë³„
            elif ext in ['.json', '.yaml', '.yml', '.txt'] or 'config' in file_path.name.lower():
                config_files.append({
                    'name': file_path.name,
                    'path': str(file_path.relative_to(model_dir)),
                    'size_mb': round(size_mb, 1)
                })
        
        # ì´ í¬ê¸° ê³„ì‚°
        total_size_mb = sum(f.stat().st_size for f in model_files) / (1024 * 1024)
        
        # ëª¨ë¸ íƒ€ì… ì¶”ì •
        model_type = self._estimate_model_type(model_name, checkpoint_files, config_files)
        
        # ì‚¬ìš© ê°€ëŠ¥ì„± íŒë‹¨
        is_ready = len(checkpoint_files) > 0 and total_size_mb > 1  # 1MB ì´ìƒ
        
        self.analyzed_models[model_name] = {
            'name': model_name,
            'path': str(model_dir),
            'type': model_type,
            'ready': is_ready,
            'total_files': len(model_files),
            'total_size_mb': round(total_size_mb, 1),
            'file_types': file_types,
            'checkpoints': checkpoint_files,
            'configs': config_files,
            'step': self._map_to_pipeline_step(model_type),
            'priority': self._get_priority(model_type, total_size_mb)
        }
        
        # ë¡œê·¸ ì¶œë ¥
        status = "âœ…" if is_ready else "âš ï¸"
        logger.info(f"   {status} {model_name}: {len(checkpoint_files)}ê°œ ì²´í¬í¬ì¸íŠ¸, {total_size_mb:.1f}MB")
        if checkpoint_files:
            for ckpt in checkpoint_files[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                logger.info(f"      - {ckpt['name']} ({ckpt['size_mb']}MB)")
            if len(checkpoint_files) > 3:
                logger.info(f"      ... ë° {len(checkpoint_files) - 3}ê°œ ë”")
    
    def _analyze_root_files(self):
        """ë£¨íŠ¸ ë ˆë²¨ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ ë¶„ì„"""
        root_files = []
        
        for file_path in self.checkpoints_dir.glob("*"):
            if file_path.is_file():
                ext = file_path.suffix.lower()
                if ext in ['.pth', '.pt', '.bin', '.ckpt', '.pkl']:
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    root_files.append({
                        'name': file_path.name,
                        'size_mb': round(size_mb, 1),
                        'type': ext
                    })
        
        if root_files:
            logger.info(f"\nğŸ“„ ë£¨íŠ¸ ë ˆë²¨ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤:")
            self.analyzed_models['_root_files'] = {
                'name': 'Root Level Files',
                'type': 'mixed',
                'ready': True,
                'files': root_files,
                'total_size_mb': sum(f['size_mb'] for f in root_files),
                'step': 'auxiliary',
                'priority': 99
            }
            
            for file_info in root_files:
                logger.info(f"   ğŸ“„ {file_info['name']} ({file_info['size_mb']}MB)")
    
    def _estimate_model_type(self, model_name: str, checkpoints: List, configs: List) -> str:
        """ëª¨ë¸ íƒ€ì… ì¶”ì •"""
        name_lower = model_name.lower()
        
        # ì´ë¦„ ê¸°ë°˜ ì¶”ì •
        if any(keyword in name_lower for keyword in ['ootd', 'diffusion', 'stable']):
            return 'diffusion'
        elif any(keyword in name_lower for keyword in ['viton', 'tryon', 'fitting']):
            return 'virtual_tryon'
        elif any(keyword in name_lower for keyword in ['parsing', 'segmentation', 'human']):
            return 'human_parsing'
        elif any(keyword in name_lower for keyword in ['pose', 'openpose', 'keypoint']):
            return 'pose_estimation'
        elif any(keyword in name_lower for keyword in ['u2net', 'background', 'removal']):
            return 'cloth_segmentation'
        elif any(keyword in name_lower for keyword in ['gmm', 'geometric', 'matching']):
            return 'geometric_matching'
        elif any(keyword in name_lower for keyword in ['cloth', 'garment', 'warping']):
            return 'cloth_warping'
        elif any(keyword in name_lower for keyword in ['detectron', 'rcnn', 'detection']):
            return 'detection'
        elif any(keyword in name_lower for keyword in ['clip', 'vit', 'text']):
            return 'text_image'
        else:
            # íŒŒì¼ í¬ê¸°ë¡œ ì¶”ì •
            total_size = sum(c['size_mb'] for c in checkpoints)
            if total_size > 3000:  # 3GB ì´ìƒ
                return 'diffusion'
            elif total_size > 500:  # 500MB ì´ìƒ
                return 'virtual_tryon'
            elif total_size > 100:  # 100MB ì´ìƒ
                return 'human_parsing'
            else:
                return 'auxiliary'
    
    def _map_to_pipeline_step(self, model_type: str) -> str:
        """ëª¨ë¸ íƒ€ì…ì„ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë¡œ ë§¤í•‘"""
        mapping = {
            'human_parsing': 'step_01_human_parsing',
            'pose_estimation': 'step_02_pose_estimation',
            'cloth_segmentation': 'step_03_cloth_segmentation',
            'geometric_matching': 'step_04_geometric_matching',
            'cloth_warping': 'step_05_cloth_warping',
            'virtual_tryon': 'step_06_virtual_fitting',
            'diffusion': 'step_06_virtual_fitting',
            'detection': 'auxiliary',
            'text_image': 'auxiliary'
        }
        return mapping.get(model_type, 'auxiliary')
    
    def _get_priority(self, model_type: str, size_mb: float) -> int:
        """ëª¨ë¸ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        base_priority = {
            'diffusion': 1,
            'virtual_tryon': 2,
            'human_parsing': 3,
            'pose_estimation': 4,
            'cloth_segmentation': 5,
            'geometric_matching': 6,
            'cloth_warping': 7,
            'detection': 8,
            'text_image': 9
        }.get(model_type, 99)
        
        # í¬ê¸°ê°€ í´ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ (ë” ì™„ì„±ë„ ë†’ì€ ëª¨ë¸ë¡œ ê°„ì£¼)
        if size_mb > 5000:  # 5GB ì´ìƒ
            return base_priority
        elif size_mb > 1000:  # 1GB ì´ìƒ
            return base_priority + 1
        elif size_mb > 100:  # 100MB ì´ìƒ
            return base_priority + 2
        else:
            return base_priority + 3
    
    def _show_analysis_summary(self):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
        logger.info(f"\nğŸ“Š ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
        logger.info("=" * 60)
        
        total_models = len(self.analyzed_models)
        ready_models = sum(1 for model in self.analyzed_models.values() if model.get('ready', False))
        total_size = sum(model.get('total_size_mb', 0) for model in self.analyzed_models.values())
        
        logger.info(f"ğŸ“¦ ë¶„ì„ëœ ëª¨ë¸/ë””ë ‰í† ë¦¬: {total_models}ê°œ")
        logger.info(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {ready_models}ê°œ")
        logger.info(f"ğŸ’¾ ì´ í¬ê¸°: {total_size:.1f} MB ({total_size/1024:.1f} GB)")
        
        # íƒ€ì…ë³„ ë¶„ë¥˜
        type_summary = {}
        for model_info in self.analyzed_models.values():
            model_type = model_info.get('type', 'unknown')
            if model_type not in type_summary:
                type_summary[model_type] = {'count': 0, 'size_mb': 0, 'ready': 0}
            type_summary[model_type]['count'] += 1
            type_summary[model_type]['size_mb'] += model_info.get('total_size_mb', 0)
            if model_info.get('ready', False):
                type_summary[model_type]['ready'] += 1
        
        logger.info(f"\nğŸ“‹ íƒ€ì…ë³„ ë¶„ë¥˜:")
        for model_type, stats in type_summary.items():
            logger.info(f"   {model_type}: {stats['ready']}/{stats['count']}ê°œ ì‚¬ìš©ê°€ëŠ¥, {stats['size_mb']:.1f}MB")
        
        # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        logger.info(f"\nğŸ¯ ìš°ì„ ìˆœìœ„ë³„ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:")
        ready_models = [(name, info) for name, info in self.analyzed_models.items() 
                       if info.get('ready', False)]
        ready_models.sort(key=lambda x: x[1].get('priority', 99))
        
        for name, info in ready_models[:10]:  # ìƒìœ„ 10ê°œë§Œ
            step = info.get('step', 'auxiliary')
            size = info.get('total_size_mb', 0)
            priority = info.get('priority', 99)
            logger.info(f"   {priority:2d}. {name} ({size:.1f}MB) - {step}")
    
    def create_optimized_model_config(self):
        """ìµœì í™”ëœ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
        logger.info("ğŸ“ ìµœì í™”ëœ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ë§Œ í•„í„°ë§
        ready_models = {name: info for name, info in self.analyzed_models.items() 
                       if info.get('ready', False)}
        
        # ë‹¨ê³„ë³„ ìµœê³  ìš°ì„ ìˆœìœ„ ëª¨ë¸ ì„ íƒ
        step_best_models = {}
        for model_name, model_info in ready_models.items():
            step = model_info.get('step', 'auxiliary')
            priority = model_info.get('priority', 99)
            
            if step not in step_best_models or priority < step_best_models[step]['priority']:
                step_best_models[step] = {
                    'model_name': model_name,
                    'priority': priority,
                    'info': model_info
                }
        
        # Python ì„¤ì • íŒŒì¼ ìƒì„±
        config_content = '''# app/core/optimized_model_paths.py
"""
ìµœì í™”ëœ AI ëª¨ë¸ ê²½ë¡œ ì„¤ì • - ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜
ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ë“¤ë¡œë§Œ êµ¬ì„±
"""

from pathlib import Path
from typing import Dict, Optional, List, Any

# ê¸°ë³¸ ê²½ë¡œ
AI_MODELS_ROOT = Path(__file__).parent.parent.parent / "ai_models"
CHECKPOINTS_ROOT = AI_MODELS_ROOT / "checkpoints"

# ë¶„ì„ëœ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ë“¤
ANALYZED_MODELS = {
'''
        
        for model_name, model_info in ready_models.items():
            if model_name.startswith('_'):  # íŠ¹ìˆ˜ í•­ëª© ì œì™¸
                continue
            config_content += f'''    "{model_name}": {{
        "name": "{model_info['name']}",
        "type": "{model_info['type']}",
        "step": "{model_info['step']}",
        "path": CHECKPOINTS_ROOT / "{model_name}",
        "ready": {model_info['ready']},
        "size_mb": {model_info['total_size_mb']},
        "priority": {model_info['priority']},
        "checkpoints": {model_info['checkpoints'][:3]},  # ìƒìœ„ 3ê°œë§Œ
        "total_checkpoints": {len(model_info['checkpoints'])}
    }},
'''
        
        config_content += '''}

# ë‹¨ê³„ë³„ ìµœì  ëª¨ë¸ ë§¤í•‘
STEP_OPTIMAL_MODELS = {
'''
        
        for step, best_model in step_best_models.items():
            config_content += f'''    "{step}": "{best_model['model_name']}",
'''
        
        config_content += '''}

def get_optimal_model_for_step(step: str) -> Optional[str]:
    """ë‹¨ê³„ë³„ ìµœì  ëª¨ë¸ ë°˜í™˜"""
    return STEP_OPTIMAL_MODELS.get(step)

def get_model_checkpoints(model_name: str) -> List[Dict]:
    """ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ë°˜í™˜"""
    if model_name in ANALYZED_MODELS:
        return ANALYZED_MODELS[model_name]["checkpoints"]
    return []

def get_largest_checkpoint(model_name: str) -> Optional[str]:
    """ëª¨ë¸ì˜ ê°€ì¥ í° ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜ (ë³´í†µ ë©”ì¸ ëª¨ë¸)"""
    checkpoints = get_model_checkpoints(model_name)
    if not checkpoints:
        return None
    
    largest = max(checkpoints, key=lambda x: x['size_mb'])
    return largest['path']

def get_ready_models_by_type(model_type: str) -> List[str]:
    """íƒ€ì…ë³„ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤"""
    return [name for name, info in ANALYZED_MODELS.items() 
            if info["type"] == model_type and info["ready"]]

def get_diffusion_models() -> List[str]:
    """Diffusion ëª¨ë¸ë“¤ (OOTD ë“±)"""
    return get_ready_models_by_type("diffusion")

def get_virtual_tryon_models() -> List[str]:
    """ê°€ìƒ í”¼íŒ… ëª¨ë¸ë“¤ (HR-VITON ë“±)"""
    return get_ready_models_by_type("virtual_tryon")

def get_human_parsing_models() -> List[str]:
    """ì¸ì²´ íŒŒì‹± ëª¨ë¸ë“¤"""
    return get_ready_models_by_type("human_parsing")

def get_model_info(model_name: str) -> Optional[Dict]:
    """ëª¨ë¸ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
    return ANALYZED_MODELS.get(model_name)

def list_all_ready_models() -> Dict[str, Dict]:
    """ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì •ë³´"""
    return ANALYZED_MODELS.copy()

# ë¹ ë¥¸ ì ‘ê·¼ í•¨ìˆ˜ë“¤
def get_best_diffusion_model() -> Optional[str]:
    """ìµœê³  ì„±ëŠ¥ Diffusion ëª¨ë¸"""
    return get_optimal_model_for_step("step_06_virtual_fitting")

def get_best_human_parsing_model() -> Optional[str]:
    """ìµœê³  ì„±ëŠ¥ ì¸ì²´ íŒŒì‹± ëª¨ë¸"""  
    return get_optimal_model_for_step("step_01_human_parsing")

def get_model_path(model_name: str) -> Optional[Path]:
    """ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë°˜í™˜"""
    if model_name in ANALYZED_MODELS:
        return ANALYZED_MODELS[model_name]["path"]
    return None

def get_checkpoint_path(model_name: str, checkpoint_name: Optional[str] = None) -> Optional[Path]:
    """íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    model_path = get_model_path(model_name)
    if not model_path:
        return None
    
    if checkpoint_name:
        return model_path / checkpoint_name
    else:
        # ê°€ì¥ í° ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜
        largest_ckpt = get_largest_checkpoint(model_name)
        return model_path / largest_ckpt if largest_ckpt else None

# ì‚¬ìš© í†µê³„
ANALYSIS_STATS = {
    "total_models": len(ANALYZED_MODELS),
    "total_size_gb": sum(info["size_mb"] for info in ANALYZED_MODELS.values()) / 1024,
    "models_by_step": {step: len([m for m in ANALYZED_MODELS.values() if m["step"] == step]) 
                      for step in set(info["step"] for info in ANALYZED_MODELS.values())},
    "largest_model": max(ANALYZED_MODELS.items(), key=lambda x: x[1]["size_mb"])[0] if ANALYZED_MODELS else None
}
'''
        
        # íŒŒì¼ ì €ì¥
        config_path = Path("app/core/optimized_model_paths.py")
        config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        logger.info(f"âœ… ìµœì í™”ëœ Python ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
        
        # ModelLoader ì—°ë™ íŒŒì¼ë„ ìƒì„±
        self._create_modelloader_integration()
    
    def _create_modelloader_integration(self):
        """ModelLoader ì™„ì „ ì—°ë™ íŒŒì¼ ìƒì„±"""
        logger.info("ğŸ”§ ModelLoader ì™„ì „ ì—°ë™ íŒŒì¼ ìƒì„± ì¤‘...")
        
        integration_content = '''# app/ai_pipeline/utils/checkpoint_model_loader.py
"""
ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ ModelLoader ì™„ì „ ì—°ë™
ì‹¤ì œ ë‹¤ìš´ë¡œë“œëœ 80GB ì²´í¬í¬ì¸íŠ¸ë“¤ í™œìš©
"""

from app.ai_pipeline.utils.model_loader import ModelLoader, ModelConfig, ModelType
from app.core.optimized_model_paths import (
    ANALYZED_MODELS, get_optimal_model_for_step, 
    get_checkpoint_path, get_largest_checkpoint
)
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class CheckpointModelLoader(ModelLoader):
    """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ í™•ì¥ ModelLoader"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._register_analyzed_models()
    
    def _register_analyzed_models(self):
        """ë¶„ì„ëœ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ë“¤ ìë™ ë“±ë¡"""
        logger.info("ğŸ“¦ ë¶„ì„ëœ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ë“¤ ë“±ë¡ ì¤‘...")
        
        registered_count = 0
        
        for model_name, model_info in ANALYZED_MODELS.items():
            if not model_info["ready"]:
                continue
                
            try:
                # ModelType ë§¤í•‘
                model_type = self._map_to_model_type(model_info["type"])
                if not model_type:
                    continue
                
                # ê°€ì¥ í° ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
                main_checkpoint = get_largest_checkpoint(model_name)
                checkpoint_path = get_checkpoint_path(model_name, main_checkpoint) if main_checkpoint else None
                
                # ëª¨ë¸ ì„¤ì • ìƒì„±
                model_config = ModelConfig(
                    name=model_info["name"],
                    model_type=model_type,
                    model_class=self._get_model_class(model_info["type"]),
                    checkpoint_path=str(checkpoint_path) if checkpoint_path else None,
                    input_size=(512, 512),
                    device=self.device
                )
                
                # ëª¨ë¸ ë“±ë¡
                self.register_model(model_name, model_config)
                registered_count += 1
                
                logger.info(f"   âœ… {model_name}: {model_info['name']}")
                
            except Exception as e:
                logger.warning(f"   âš ï¸ {model_name} ë“±ë¡ ì‹¤íŒ¨: {e}")
        
        logger.info(f"ğŸ“¦ ì´ {registered_count}ê°œ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ")
    
    def _map_to_model_type(self, analysis_type: str) -> Optional[ModelType]:
        """ë¶„ì„ íƒ€ì…ì„ ModelTypeìœ¼ë¡œ ë§¤í•‘"""
        mapping = {
            'diffusion': ModelType.DIFFUSION,
            'virtual_tryon': ModelType.VIRTUAL_FITTING,
            'human_parsing': ModelType.HUMAN_PARSING,
            'pose_estimation': ModelType.POSE_ESTIMATION,
            'cloth_segmentation': ModelType.CLOTH_SEGMENTATION,
            'geometric_matching': ModelType.GEOMETRIC_MATCHING,
            'cloth_warping': ModelType.CLOTH_WARPING,
            'detection': ModelType.SEGMENTATION,
            'text_image': ModelType.DIFFUSION
        }
        return mapping.get(analysis_type)
    
    def _get_model_class(self, analysis_type: str) -> str:
        """ë¶„ì„ íƒ€ì…ì—ì„œ ëª¨ë¸ í´ë˜ìŠ¤ëª… ì¶”ì¶œ"""
        mapping = {
            'diffusion': 'StableDiffusionPipeline',
            'virtual_tryon': 'HRVITONModel',
            'human_parsing': 'GraphonomyModel',
            'pose_estimation': 'OpenPoseModel',
            'cloth_segmentation': 'U2NetModel',
            'geometric_matching': 'GeometricMatchingModel',
            'cloth_warping': 'HRVITONModel',
            'detection': 'DetectronModel',
            'text_image': 'CLIPModel'
        }
        return mapping.get(analysis_type, 'BaseModel')
    
    async def load_optimal_model_for_step(self, step: str, **kwargs):
        """ë‹¨ê³„ë³„ ìµœì  ëª¨ë¸ ë¡œë“œ"""
        optimal_model = get_optimal_model_for_step(step)
        if not optimal_model:
            logger.warning(f"âš ï¸ {step}ì— ëŒ€í•œ ìµœì  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
        
        logger.info(f"ğŸ¯ {step} ìµœì  ëª¨ë¸ ë¡œë“œ: {optimal_model}")
        return await self.load_model(optimal_model, **kwargs)

# ì „ì—­ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë”
_global_checkpoint_loader: Optional[CheckpointModelLoader] = None

def get_checkpoint_model_loader(**kwargs) -> CheckpointModelLoader:
    """ì „ì—­ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë” ë°˜í™˜"""
    global _global_checkpoint_loader
    if _global_checkpoint_loader is None:
        _global_checkpoint_loader = CheckpointModelLoader(**kwargs)
    return _global_checkpoint_loader

async def load_best_model_for_step(step: str, **kwargs):
    """ë‹¨ê³„ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ"""
    loader = get_checkpoint_model_loader()
    return await loader.load_optimal_model_for_step(step, **kwargs)

# ë¹ ë¥¸ ì ‘ê·¼ í•¨ìˆ˜ë“¤
async def load_best_diffusion_model(**kwargs):
    """ìµœê³  ì„±ëŠ¥ Diffusion ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_06_virtual_fitting", **kwargs)

async def load_best_human_parsing_model(**kwargs):
    """ìµœê³  ì„±ëŠ¥ ì¸ì²´ íŒŒì‹± ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_01_human_parsing", **kwargs)

async def load_best_pose_model(**kwargs):
    """ìµœê³  ì„±ëŠ¥ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_02_pose_estimation", **kwargs)
'''
        
        # íŒŒì¼ ì €ì¥
        integration_path = Path("app/ai_pipeline/utils/checkpoint_model_loader.py")
        integration_path.parent.mkdir(parents=True, exist_ok=True)
        with open(integration_path, 'w', encoding='utf-8') as f:
            f.write(integration_content)
        
        logger.info(f"âœ… ModelLoader ì™„ì „ ì—°ë™ íŒŒì¼ ìƒì„±: {integration_path}")
    
    def create_analysis_report(self):
        """ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        logger.info("ğŸ“‹ ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        report = {
            "analysis_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_size_gb": sum(model.get('total_size_mb', 0) for model in self.analyzed_models.values()) / 1024,
            "analyzed_models": self.analyzed_models,
            "summary": {
                "total_models": len(self.analyzed_models),
                "ready_models": sum(1 for model in self.analyzed_models.values() if model.get('ready', False)),
                "largest_model": max(self.analyzed_models.items(), 
                                   key=lambda x: x[1].get('total_size_mb', 0))[0] if self.analyzed_models else None,
                "models_by_type": {}
            }
        }
        
        # íƒ€ì…ë³„ í†µê³„
        for model_info in self.analyzed_models.values():
            model_type = model_info.get('type', 'unknown')
            if model_type not in report["summary"]["models_by_type"]:
                report["summary"]["models_by_type"][model_type] = {
                    "count": 0, "ready": 0, "total_size_mb": 0
                }
            
            report["summary"]["models_by_type"][model_type]["count"] += 1
            report["summary"]["models_by_type"][model_type]["total_size_mb"] += model_info.get('total_size_mb', 0)
            if model_info.get('ready', False):
                report["summary"]["models_by_type"][model_type]["ready"] += 1
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = self.checkpoints_dir / "checkpoint_analysis_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±: {report_path}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ” MyCloset AI - ì²´í¬í¬ì¸íŠ¸ ìƒì„¸ ë¶„ì„")
    print("=" * 50)
    
    try:
        analyzer = CheckpointAnalyzer("ai_models/checkpoints")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„
        analyzed_models = analyzer.analyze_all_checkpoints()
        
        if not analyzed_models:
            logger.warning("âš ï¸ ë¶„ì„í•  ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ìµœì í™”ëœ ì„¤ì • íŒŒì¼ ìƒì„±
        analyzer.create_optimized_model_config()
        analyzer.create_analysis_report()
        
        print(f"\nğŸ‰ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ {len(analyzed_models)}ê°œ í•­ëª© ë¶„ì„")
        ready_count = sum(1 for m in analyzed_models.values() if m.get('ready', False))
        print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {ready_count}ê°œ")
        
        print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        print(f"   - app/core/optimized_model_paths.py (ìµœì í™”ëœ ê²½ë¡œ ì„¤ì •)")
        print(f"   - app/ai_pipeline/utils/checkpoint_model_loader.py (ModelLoader ì—°ë™)")
        print(f"   - ai_models/checkpoints/checkpoint_analysis_report.json (ìƒì„¸ ë³´ê³ ì„œ)")
        
        print(f"\nğŸš€ ì‚¬ìš© ë°©ë²•:")
        print(f"   from app.ai_pipeline.utils.checkpoint_model_loader import load_best_diffusion_model")
        print(f"   model = await load_best_diffusion_model()")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)