#!/usr/bin/env python3
"""
ğŸ” AI ëª¨ë¸ íŒŒì¼ ìë™ íƒì§€ ë° ì¬ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸
âœ… ëª¨ë“  ê²½ë¡œì—ì„œ ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ë“¤ íƒì§€
âœ… ì˜¬ë°”ë¥¸ ìœ„ì¹˜ë¡œ ìë™ ì´ë™ ë° ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
"""

import os
import sys
import shutil
import hashlib
import sqlite3
import json
import time
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('model_finder.log')
    ]
)
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ” ëª¨ë¸ íŒŒì¼ ì •ì˜ ë° íŒ¨í„´
# ==============================================

@dataclass
class ModelFileInfo:
    """ëª¨ë¸ íŒŒì¼ ì •ë³´"""
    name: str
    patterns: List[str]
    step: str
    required: bool
    min_size_mb: float
    max_size_mb: float
    target_path: str
    priority: int = 1
    alternative_names: List[str] = field(default_factory=list)
    file_types: List[str] = field(default_factory=lambda: ['.pth', '.pt', '.bin', '.safetensors'])

# ì‹¤ì œ í•„ìš”í•œ AI ëª¨ë¸ë“¤ ì •ì˜ (í™•ì¥ëœ íŒ¨í„´)
REQUIRED_MODELS = {
    # Step 01: Human Parsing
    "human_parsing_graphonomy": ModelFileInfo(
        name="human_parsing_graphonomy",
        patterns=[
            r".*graphonomy.*\.pth$",
            r".*schp.*atr.*\.pth$", 
            r".*human.*parsing.*\.pth$",
            r".*atr.*model.*\.pth$",
            r".*lip.*parsing.*\.pth$",
            r".*segmentation.*human.*\.pth$"
        ],
        step="step_01_human_parsing",
        required=True,
        min_size_mb=50,
        max_size_mb=500,
        target_path="ai_models/checkpoints/step_01_human_parsing/graphonomy.pth",
        priority=1,
        alternative_names=["schp_atr.pth", "atr_model.pth", "human_parsing.pth"]
    ),
    
    # Step 02: Pose Estimation  
    "pose_estimation_openpose": ModelFileInfo(
        name="pose_estimation_openpose",
        patterns=[
            r".*openpose.*\.pth$",
            r".*pose.*model.*\.pth$",
            r".*body.*pose.*\.pth$",
            r".*coco.*pose.*\.pth$",
            r".*pose.*estimation.*\.pth$"
        ],
        step="step_02_pose_estimation", 
        required=True,
        min_size_mb=10,
        max_size_mb=1000,
        target_path="ai_models/checkpoints/step_02_pose_estimation/openpose.pth",
        priority=1,
        alternative_names=["body_pose_model.pth", "pose_model.pth", "openpose_model.pth"]
    ),
    
    # Step 03: Cloth Segmentation
    "cloth_segmentation_u2net": ModelFileInfo(
        name="cloth_segmentation_u2net", 
        patterns=[
            r".*u2net.*\.pth$",
            r".*cloth.*segmentation.*\.pth$",
            r".*segmentation.*cloth.*\.pth$",
            r".*u2netp.*\.pth$",
            r".*cloth.*mask.*\.pth$"
        ],
        step="step_03_cloth_segmentation",
        required=True, 
        min_size_mb=10,
        max_size_mb=200,
        target_path="ai_models/checkpoints/step_03_cloth_segmentation/u2net.pth",
        priority=1,
        alternative_names=["u2net.pth", "cloth_seg.pth", "segmentation.pth"]
    ),
    
    # Step 04: Geometric Matching
    "geometric_matching_gmm": ModelFileInfo(
        name="geometric_matching_gmm",
        patterns=[
            r".*gmm.*\.pth$",
            r".*geometric.*matching.*\.pth$", 
            r".*tps.*\.pth$",
            r".*matching.*\.pth$",
            r".*alignment.*\.pth$"
        ],
        step="geometric_matching",
        required=True,
        min_size_mb=1,
        max_size_mb=100, 
        target_path="ai_models/checkpoints/gmm_final.pth",
        priority=2,
        alternative_names=["gmm_final.pth", "geometric.pth", "matching.pth"]
    ),
    
    # Step 05: Cloth Warping  
    "cloth_warping_tom": ModelFileInfo(
        name="cloth_warping_tom",
        patterns=[
            r".*tom.*\.pth$",
            r".*cloth.*warping.*\.pth$",
            r".*warping.*\.pth$", 
            r".*try.*on.*\.pth$",
            r".*viton.*\.pth$"
        ],
        step="cloth_warping",
        required=True,
        min_size_mb=10,
        max_size_mb=200,
        target_path="ai_models/checkpoints/tom_final.pth", 
        priority=2,
        alternative_names=["tom_final.pth", "warping.pth", "cloth_warp.pth"]
    ),
    
    # Step 06: Virtual Fitting (Diffusion Models)
    "virtual_fitting_hrviton": ModelFileInfo(
        name="virtual_fitting_hrviton", 
        patterns=[
            r".*hrviton.*\.pth$",
            r".*hr.*viton.*\.pth$",
            r".*viton.*hd.*\.pth$",
            r".*virtual.*fitting.*\.pth$",
            r".*diffusion.*viton.*\.pth$"
        ],
        step="virtual_fitting",
        required=True,
        min_size_mb=100, 
        max_size_mb=2000,
        target_path="ai_models/checkpoints/hrviton_final.pth",
        priority=1,
        alternative_names=["hrviton_final.pth", "hr_viton.pth", "viton_hd.pth"]
    ),
    
    # Diffusion Models (ëŒ€ìš©ëŸ‰)
    "stable_diffusion": ModelFileInfo(
        name="stable_diffusion",
        patterns=[
            r".*stable.*diffusion.*\.safetensors$",
            r".*sd.*v1.*5.*\.safetensors$",
            r".*diffusion.*pytorch.*model\.bin$",
            r".*unet.*diffusion.*\.bin$",
            r".*v1-5-pruned.*\.safetensors$"
        ],
        step="diffusion_models",
        required=False,
        min_size_mb=2000,
        max_size_mb=8000, 
        target_path="ai_models/diffusion/stable-diffusion-v1-5",
        priority=2,
        alternative_names=["model.safetensors", "pytorch_model.bin"],
        file_types=['.safetensors', '.bin']
    ),
    
    # CLIP Models
    "clip_vit_base": ModelFileInfo(
        name="clip_vit_base",
        patterns=[
            r".*clip.*vit.*base.*\.bin$",
            r".*clip.*base.*patch.*\.bin$", 
            r".*pytorch.*model\.bin$"
        ],
        step="quality_assessment",
        required=False,
        min_size_mb=400,
        max_size_mb=1000,
        target_path="ai_models/clip-vit-base-patch32/pytorch_model.bin",
        priority=3,
        alternative_names=["pytorch_model.bin"],
        file_types=['.bin']
    )
}

# ==============================================
# ğŸ” ê³ ê¸‰ íŒŒì¼ íƒì§€ê¸° í´ë˜ìŠ¤  
# ==============================================

class AIModelFinder:
    """AI ëª¨ë¸ íŒŒì¼ ìë™ íƒì§€ ë° ë¶„ì„"""
    
    def __init__(self, project_root: Optional[Path] = None):
        """ì´ˆê¸°í™”"""
        if project_root is None:
            # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
            current_file = Path(__file__).resolve()
            self.project_root = current_file.parent.parent  # scripts -> backend
        else:
            self.project_root = Path(project_root)
        
        self.backend_dir = self.project_root
        self.found_models: Dict[str, List[Dict]] = {}
        self.scan_stats = {
            "total_files_scanned": 0,
            "potential_models_found": 0,
            "confirmed_models": 0,
            "scan_duration": 0.0,
            "errors": 0
        }
        
        # ê²€ìƒ‰ ê²½ë¡œë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
        self.search_paths = self._get_comprehensive_search_paths()
        
        logger.info(f"ğŸ” AI ëª¨ë¸ íƒì§€ê¸° ì´ˆê¸°í™” - í”„ë¡œì íŠ¸ ë£¨íŠ¸: {self.project_root}")
        logger.info(f"ğŸ“ ê²€ìƒ‰ ê²½ë¡œ: {len(self.search_paths)}ê°œ")

    def _get_comprehensive_search_paths(self) -> List[Path]:
        """í¬ê´„ì ì¸ ê²€ìƒ‰ ê²½ë¡œ ëª©ë¡ ìƒì„±"""
        paths = []
        
        # 1. í”„ë¡œì íŠ¸ ë‚´ë¶€ ê²½ë¡œë“¤
        project_paths = [
            self.backend_dir / "ai_models",
            self.backend_dir / "app" / "ai_pipeline" / "models", 
            self.backend_dir / "app" / "models",
            self.backend_dir / "models",
            self.backend_dir / "checkpoints",
            self.backend_dir / "weights",
            self.backend_dir / "pretrained",
            self.backend_dir / ".." / "models",  # mycloset-ai/models
            self.backend_dir / ".." / "ai_models",  # mycloset-ai/ai_models
        ]
        
        # 2. ì‚¬ìš©ì í™ˆ ë””ë ‰í† ë¦¬ ê²½ë¡œë“¤  
        home = Path.home()
        home_paths = [
            home / ".cache" / "huggingface" / "hub",
            home / ".cache" / "torch" / "hub", 
            home / ".cache" / "transformers",
            home / "Downloads",
            home / "Desktop",
            home / "Documents" / "AI_Models",
            home / "models",
            home / "ai_models"
        ]
        
        # 3. ì‹œìŠ¤í…œ ê³µí†µ ê²½ë¡œë“¤
        system_paths = [
            Path("/opt/ml/models"),
            Path("/usr/local/share/models"),
            Path("/tmp/models"),
            Path("/var/cache/models")
        ]
        
        # 4. conda/pip ì„¤ì¹˜ ê²½ë¡œë“¤ 
        conda_paths = self._get_conda_model_paths()
        
        # 5. ì™¸ë¶€ ì €ì¥ì†Œ ê²½ë¡œë“¤ (macOS ê¸°ì¤€)
        if sys.platform == "darwin":
            external_paths = [
                Path("/Volumes") / "ì™¸ì¥í•˜ë“œ" / "AI_Models",  # ì¼ë°˜ì ì¸ ì™¸ì¥í•˜ë“œ
                Path("/Volumes") / "USB" / "models", 
                Path("/Volumes") / "SSD" / "ai_models"
            ]
            # ì‹¤ì œ ë§ˆìš´íŠ¸ëœ ë³¼ë¥¨ë“¤ í™•ì¸
            if Path("/Volumes").exists():
                for volume in Path("/Volumes").iterdir():
                    if volume.is_dir() and not volume.name.startswith('.'):
                        external_paths.extend([
                            volume / "AI_Models",
                            volume / "models", 
                            volume / "checkpoints",
                            volume / "Downloads"
                        ])
            system_paths.extend(external_paths)
        
        # ëª¨ë“  ê²½ë¡œ ê²°í•© (ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ)
        all_paths = project_paths + home_paths + system_paths + conda_paths
        paths = [p for p in all_paths if p.exists() and p.is_dir()]
        
        # ì¤‘ë³µ ì œê±° (ì‹¤ì œ ê²½ë¡œ ê¸°ì¤€)
        unique_paths = []
        seen_paths = set()
        for path in paths:
            try:
                real_path = path.resolve()
                if real_path not in seen_paths:
                    unique_paths.append(path)
                    seen_paths.add(real_path)
            except:
                continue
                
        return unique_paths
    
    def _get_conda_model_paths(self) -> List[Path]:
        """conda í™˜ê²½ì˜ ëª¨ë¸ ê²½ë¡œë“¤ íƒì§€"""
        conda_paths = []
        
        try:
            # conda í™˜ê²½ ê²½ë¡œ ì°¾ê¸°
            conda_env = os.environ.get('CONDA_PREFIX')
            if conda_env:
                conda_env_path = Path(conda_env)
                conda_paths.extend([
                    conda_env_path / "lib" / "python3.11" / "site-packages" / "transformers",
                    conda_env_path / "lib" / "python3.11" / "site-packages" / "diffusers", 
                    conda_env_path / "share" / "models",
                    conda_env_path / "models"
                ])
            
            # conda ì„¤ì¹˜ ë£¨íŠ¸ ê²½ë¡œ
            conda_root = os.environ.get('CONDA_ROOT') or Path.home() / "miniforge3"
            if Path(conda_root).exists():
                conda_paths.extend([
                    Path(conda_root) / "pkgs",
                    Path(conda_root) / "envs" / "mycloset-ai" / "lib" / "python3.11" / "site-packages"
                ])
                
        except Exception as e:
            logger.debug(f"conda ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
        
        return conda_paths

    def scan_all_paths(self, max_workers: int = 4, max_depth: int = 6) -> Dict[str, List[Dict]]:
        """ëª¨ë“  ê²½ë¡œì—ì„œ AI ëª¨ë¸ íŒŒì¼ ìŠ¤ìº”"""
        logger.info("ğŸ” ì „ì²´ ê²½ë¡œ AI ëª¨ë¸ íŒŒì¼ ìŠ¤ìº” ì‹œì‘...")
        start_time = time.time()
        
        self.found_models.clear()
        self.scan_stats = {
            "total_files_scanned": 0,
            "potential_models_found": 0, 
            "confirmed_models": 0,
            "scan_duration": 0.0,
            "errors": 0
        }
        
        # ë³‘ë ¬ ìŠ¤ìº” ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_path = {
                executor.submit(self._scan_directory, path, max_depth): path 
                for path in self.search_paths
            }
            
            for future in as_completed(future_to_path):
                search_path = future_to_path[future]
                try:
                    path_results = future.result()
                    if path_results:
                        self._merge_scan_results(path_results)
                        logger.info(f"âœ… ê²½ë¡œ ìŠ¤ìº” ì™„ë£Œ: {search_path} ({len(path_results)} ëª¨ë¸)")
                except Exception as e:
                    logger.error(f"âŒ ê²½ë¡œ ìŠ¤ìº” ì‹¤íŒ¨ {search_path}: {e}")
                    self.scan_stats["errors"] += 1
        
        self.scan_stats["scan_duration"] = time.time() - start_time
        self.scan_stats["confirmed_models"] = len(self.found_models)
        
        logger.info(f"âœ… ì „ì²´ ìŠ¤ìº” ì™„ë£Œ: {self.scan_stats['confirmed_models']}ê°œ ëª¨ë¸ ë°œê²¬")
        self._print_scan_summary()
        
        return self.found_models
    
    def _scan_directory(self, directory: Path, max_depth: int, current_depth: int = 0) -> Dict[str, List[Dict]]:
        """ë””ë ‰í† ë¦¬ ì¬ê·€ ìŠ¤ìº”"""
        results = {}
        
        if current_depth > max_depth:
            return results
            
        try:
            # ê¶Œí•œ í™•ì¸
            if not os.access(directory, os.R_OK):
                return results
                
            items = list(directory.iterdir())
            files = [item for item in items if item.is_file()]
            subdirs = [item for item in items if item.is_dir() and not item.name.startswith('.')]
            
            # íŒŒì¼ë“¤ ê²€ì‚¬
            for file_path in files:
                try:
                    self.scan_stats["total_files_scanned"] += 1
                    
                    model_info = self._analyze_file(file_path)
                    if model_info:
                        model_key = model_info['model_key']
                        if model_key not in results:
                            results[model_key] = []
                        results[model_key].append(model_info)
                        self.scan_stats["potential_models_found"] += 1
                        
                except Exception as e:
                    logger.debug(f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
                    continue
            
            # í•˜ìœ„ ë””ë ‰í† ë¦¬ ì¬ê·€ ìŠ¤ìº”
            for subdir in subdirs:
                # ì œì™¸í•  ë””ë ‰í† ë¦¬ë“¤
                if subdir.name in ['__pycache__', '.git', 'node_modules', '.vscode', '.idea', '.pytest_cache']:
                    continue
                    
                try:
                    subdir_results = self._scan_directory(subdir, max_depth, current_depth + 1)
                    self._merge_scan_results(subdir_results, results)
                except Exception as e:
                    logger.debug(f"í•˜ìœ„ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì˜¤ë¥˜ {subdir}: {e}")
                    continue
        
        except Exception as e:
            logger.debug(f"ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì˜¤ë¥˜ {directory}: {e}")
        
        return results
    
    def _analyze_file(self, file_path: Path) -> Optional[Dict]:
        """íŒŒì¼ ë¶„ì„í•˜ì—¬ AI ëª¨ë¸ì¸ì§€ íŒë‹¨"""
        try:
            # ê¸°ë³¸ í•„í„°ë§
            if not self._is_potential_ai_model(file_path):
                return None
            
            file_stat = file_path.stat()
            file_size_mb = file_stat.st_size / (1024 * 1024)
            
            # ë„ˆë¬´ ì‘ì€ íŒŒì¼ ì œì™¸
            if file_size_mb < 0.5:
                return None
            
            # ëª¨ë¸ ë§¤ì¹­
            matched_models = self._match_against_known_models(file_path, file_size_mb)
            
            if not matched_models:
                return None
            
            # ê°€ì¥ ì í•©í•œ ëª¨ë¸ ì„ íƒ
            best_match = max(matched_models, key=lambda x: x['confidence'])
            
            if best_match['confidence'] < 0.3:
                return None
            
            # ì²´í¬ì„¬ ê³„ì‚° (ì‘ì€ íŒŒì¼ë§Œ)
            checksum = None
            if file_size_mb < 100:
                checksum = self._calculate_checksum(file_path)
            
            return {
                'model_key': best_match['model_key'],
                'file_path': str(file_path),
                'file_size_mb': file_size_mb,
                'confidence': best_match['confidence'],
                'checksum': checksum,
                'last_modified': file_stat.st_mtime,
                'target_path': best_match['target_path'],
                'priority': best_match['priority'],
                'step': best_match['step']
            }
            
        except Exception as e:
            logger.debug(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    def _is_potential_ai_model(self, file_path: Path) -> bool:
        """AI ëª¨ë¸ íŒŒì¼ ê°€ëŠ¥ì„± í™•ì¸"""
        # í™•ì¥ì ì²´í¬
        ai_extensions = {'.pth', '.pt', '.bin', '.safetensors', '.onnx', '.pkl', '.h5', '.pb'}
        if file_path.suffix.lower() not in ai_extensions:
            return False
        
        # íŒŒì¼ëª… íŒ¨í„´ ì²´í¬  
        file_name = file_path.name.lower()
        ai_keywords = [
            'model', 'checkpoint', 'weight', 'state_dict', 'pytorch_model',
            'diffusion', 'transformer', 'bert', 'clip', 'vit', 'resnet',
            'pose', 'parsing', 'segmentation', 'u2net', 'openpose',
            'viton', 'hrviton', 'stable', 'unet', 'vae'
        ]
        
        return any(keyword in file_name for keyword in ai_keywords)
    
    def _match_against_known_models(self, file_path: Path, file_size_mb: float) -> List[Dict]:
        """ì•Œë ¤ì§„ ëª¨ë¸ íŒ¨í„´ê³¼ ë§¤ì¹­"""
        matches = []
        file_path_str = str(file_path).lower()
        file_name = file_path.name.lower()
        
        for model_key, model_info in REQUIRED_MODELS.items():
            confidence = 0.0
            
            # íŒ¨í„´ ë§¤ì¹­
            import re
            for pattern in model_info.patterns:
                if re.search(pattern, file_path_str, re.IGNORECASE):
                    confidence += 20.0
                    break
            
            # ëŒ€ì²´ ì´ë¦„ ë§¤ì¹­
            for alt_name in model_info.alternative_names:
                if alt_name.lower() in file_name:
                    confidence += 15.0
                    break
            
            # íŒŒì¼ í¬ê¸° ë²”ìœ„ í™•ì¸
            if model_info.min_size_mb <= file_size_mb <= model_info.max_size_mb:
                confidence += 10.0
            elif file_size_mb < model_info.min_size_mb:
                confidence -= 15.0
            elif file_size_mb > model_info.max_size_mb:
                confidence -= 5.0
            
            # íŒŒì¼ í™•ì¥ì í™•ì¸
            if file_path.suffix.lower() in model_info.file_types:
                confidence += 5.0
            
            # ê²½ë¡œ ê¸°ë°˜ ì ìˆ˜
            path_parts = file_path.parts
            for part in path_parts:
                if model_info.step in part.lower():
                    confidence += 8.0
                    break
            
            # ìš°ì„ ìˆœìœ„ ë³´ë„ˆìŠ¤
            if model_info.priority == 1:
                confidence += 3.0
            
            if confidence > 0:
                matches.append({
                    'model_key': model_key,
                    'confidence': min(confidence / 50.0, 1.0),  # ì •ê·œí™”
                    'target_path': model_info.target_path,
                    'priority': model_info.priority,
                    'step': model_info.step
                })
        
        return matches
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """íŒŒì¼ SHA256 ì²´í¬ì„¬ ê³„ì‚°"""
        try:
            hash_sha256 = hashlib.sha256()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()[:16]  # ì²˜ìŒ 16ìë§Œ
        except Exception:
            return None
    
    def _merge_scan_results(self, new_results: Dict, target: Optional[Dict] = None):
        """ìŠ¤ìº” ê²°ê³¼ ë³‘í•©"""
        if target is None:
            target = self.found_models
            
        for model_key, model_list in new_results.items():
            if model_key not in target:
                target[model_key] = []
            target[model_key].extend(model_list)
    
    def _print_scan_summary(self):
        """ìŠ¤ìº” ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logger.info("=" * 70)
        logger.info("ğŸ¯ AI ëª¨ë¸ íŒŒì¼ ìŠ¤ìº” ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 70)
        logger.info(f"ğŸ“Š ìŠ¤ìº”ëœ íŒŒì¼: {self.scan_stats['total_files_scanned']:,}ê°œ")
        logger.info(f"ğŸ” ì ì¬ì  ëª¨ë¸: {self.scan_stats['potential_models_found']}ê°œ")
        logger.info(f"âœ… í™•ì¸ëœ ëª¨ë¸: {self.scan_stats['confirmed_models']}ê°œ")
        logger.info(f"â±ï¸ ìŠ¤ìº” ì‹œê°„: {self.scan_stats['scan_duration']:.2f}ì´ˆ")
        logger.info(f"âŒ ì—ëŸ¬: {self.scan_stats['errors']}ê°œ")
        
        if self.found_models:
            logger.info("\nğŸ“ ë°œê²¬ëœ ëª¨ë¸ë³„ íŒŒì¼ ìˆ˜:")
            for model_key, files in self.found_models.items():
                total_size = sum(f['file_size_mb'] for f in files) / 1024
                logger.info(f"  {model_key}: {len(files)}ê°œ íŒŒì¼ ({total_size:.2f}GB)")

# ==============================================
# ğŸš€ ëª¨ë¸ ì¬ë°°ì¹˜ ë° ê´€ë¦¬ì í´ë˜ìŠ¤
# ==============================================

class AIModelRelocator:
    """AI ëª¨ë¸ íŒŒì¼ ì¬ë°°ì¹˜ ë° ê´€ë¦¬"""
    
    def __init__(self, project_root: Path, found_models: Dict[str, List[Dict]]):
        self.project_root = Path(project_root)
        self.backend_dir = self.project_root
        self.found_models = found_models
        
        # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.target_base = self.backend_dir / "ai_models"
        self.checkpoints_dir = self.target_base / "checkpoints"
        self.diffusion_dir = self.target_base / "diffusion"
        self.clip_dir = self.target_base / "clip-vit-base-patch32"
        
        # ì¬ë°°ì¹˜ í†µê³„
        self.relocate_stats = {
            "copied": 0,
            "symlinked": 0,
            "skipped": 0,
            "errors": 0,
            "total_size_gb": 0.0
        }
        
        logger.info(f"ğŸš€ AI ëª¨ë¸ ì¬ë°°ì¹˜ê¸° ì´ˆê¸°í™” - íƒ€ê²Ÿ: {self.target_base}")
    
    def create_directory_structure(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
        logger.info("ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì¤‘...")
        
        directories = [
            self.target_base,
            self.checkpoints_dir,
            self.checkpoints_dir / "step_01_human_parsing", 
            self.checkpoints_dir / "step_02_pose_estimation",
            self.checkpoints_dir / "step_03_cloth_segmentation",
            self.diffusion_dir,
            self.diffusion_dir / "stable-diffusion-v1-5",
            self.clip_dir
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            logger.debug(f"âœ… ë””ë ‰í† ë¦¬ ìƒì„±: {directory}")
        
        logger.info("âœ… ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ")
    
    def relocate_all_models(self, copy_large_files: bool = False, create_symlinks: bool = True) -> Dict[str, Any]:
        """ëª¨ë“  ëª¨ë¸ì„ ì ì ˆí•œ ìœ„ì¹˜ë¡œ ì¬ë°°ì¹˜"""
        logger.info("ğŸš€ AI ëª¨ë¸ íŒŒì¼ ì¬ë°°ì¹˜ ì‹œì‘...")
        
        self.create_directory_structure()
        
        relocate_plan = self._create_relocate_plan()
        
        if not relocate_plan:
            logger.warning("âš ï¸ ì¬ë°°ì¹˜í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return {"success": False, "message": "No models to relocate"}
        
        logger.info(f"ğŸ“‹ ì¬ë°°ì¹˜ ê³„íš: {len(relocate_plan)}ê°œ ëª¨ë¸")
        
        # ì¬ë°°ì¹˜ ì‹¤í–‰
        results = {}
        for model_key, plan in relocate_plan.items():
            try:
                result = self._relocate_single_model(
                    model_key, plan, copy_large_files, create_symlinks
                )
                results[model_key] = result
                
                if result["success"]:
                    logger.info(f"âœ… {model_key} ì¬ë°°ì¹˜ ì™„ë£Œ")
                else:
                    logger.error(f"âŒ {model_key} ì¬ë°°ì¹˜ ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
                    
            except Exception as e:
                logger.error(f"âŒ {model_key} ì¬ë°°ì¹˜ ì¤‘ ì˜ˆì™¸: {e}")
                results[model_key] = {"success": False, "error": str(e)}
                self.relocate_stats["errors"] += 1
        
        # ìµœì¢… ê²°ê³¼
        summary = self._create_relocate_summary(results)
        self._print_relocate_summary(summary)
        
        return summary
    
    def _create_relocate_plan(self) -> Dict[str, Dict]:
        """ì¬ë°°ì¹˜ ê³„íš ìƒì„±"""
        plan = {}
        
        for model_key, files in self.found_models.items():
            if not files:
                continue
            
            # ê°€ì¥ ì í•©í•œ íŒŒì¼ ì„ íƒ (ì‹ ë¢°ë„ + í¬ê¸° + ìµœì‹ ì„±)
            best_file = max(files, key=lambda f: (
                f['confidence'],
                -abs(f['file_size_mb'] - self._get_expected_size(model_key)),
                f['last_modified']
            ))
            
            target_path = self.backend_dir / best_file['target_path']
            
            plan[model_key] = {
                "source_path": Path(best_file['file_path']),
                "target_path": target_path,
                "file_size_mb": best_file['file_size_mb'],
                "confidence": best_file['confidence'],
                "priority": best_file['priority'],
                "checksum": best_file.get('checksum'),
                "alternatives": [Path(f['file_path']) for f in files if f != best_file]
            }
        
        return plan
    
    def _get_expected_size(self, model_key: str) -> float:
        """ëª¨ë¸ë³„ ì˜ˆìƒ í¬ê¸° ë°˜í™˜"""
        if model_key in REQUIRED_MODELS:
            model_info = REQUIRED_MODELS[model_key]
            return (model_info.min_size_mb + model_info.max_size_mb) / 2
        return 100.0  # ê¸°ë³¸ê°’
    
    def _relocate_single_model(
        self, 
        model_key: str, 
        plan: Dict, 
        copy_large_files: bool, 
        create_symlinks: bool
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ëª¨ë¸ ì¬ë°°ì¹˜"""
        source_path = plan["source_path"]
        target_path = plan["target_path"]
        file_size_mb = plan["file_size_mb"]
        
        try:
            # ì†ŒìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not source_path.exists():
                return {"success": False, "error": "Source file not found"}
            
            # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ìƒì„±
            target_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ì´ë¯¸ íƒ€ê²Ÿì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
            if target_path.exists():
                # í¬ê¸° ë¹„êµ
                existing_size = target_path.stat().st_size / (1024 * 1024)
                if abs(existing_size - file_size_mb) < 1.0:  # 1MB ì˜¤ì°¨ í—ˆìš©
                    logger.info(f"â­ï¸ {model_key} ì´ë¯¸ ì¡´ì¬ (í¬ê¸° ìœ ì‚¬)")
                    self.relocate_stats["skipped"] += 1
                    return {
                        "success": True, 
                        "action": "skipped", 
                        "reason": "Target already exists with similar size"
                    }
                else:
                    # ë°±ì—… ìƒì„±
                    backup_path = target_path.with_suffix(f".backup_{int(time.time())}")
                    shutil.move(target_path, backup_path)
                    logger.info(f"ğŸ“¦ ê¸°ì¡´ íŒŒì¼ ë°±ì—…: {backup_path.name}")
            
            # ì¬ë°°ì¹˜ ë°©ë²• ê²°ì •
            action = self._determine_relocate_action(file_size_mb, copy_large_files, create_symlinks)
            
            if action == "copy":
                shutil.copy2(source_path, target_path)
                self.relocate_stats["copied"] += 1
                logger.info(f"ğŸ“‹ ë³µì‚¬ ì™„ë£Œ: {target_path.name} ({file_size_mb:.1f}MB)")
                
            elif action == "symlink":
                # ìƒëŒ€ ê²½ë¡œë¡œ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì‹œë„
                try:
                    relative_source = os.path.relpath(source_path, target_path.parent)
                    target_path.symlink_to(relative_source)
                    self.relocate_stats["symlinked"] += 1
                    logger.info(f"ğŸ”— ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±: {target_path.name}")
                except OSError:
                    # ì‹¬ë³¼ë¦­ ë§í¬ ì‹¤íŒ¨ì‹œ ë³µì‚¬ë¡œ í´ë°±
                    shutil.copy2(source_path, target_path)
                    self.relocate_stats["copied"] += 1
                    logger.info(f"ğŸ“‹ ì‹¬ë³¼ë¦­ ë§í¬ ì‹¤íŒ¨, ë³µì‚¬ë¡œ ëŒ€ì²´: {target_path.name}")
                    
            elif action == "hardlink":
                try:
                    os.link(source_path, target_path)
                    logger.info(f"ğŸ”— í•˜ë“œ ë§í¬ ìƒì„±: {target_path.name}")
                except OSError:
                    # í•˜ë“œ ë§í¬ ì‹¤íŒ¨ì‹œ ë³µì‚¬ë¡œ í´ë°±
                    shutil.copy2(source_path, target_path)
                    self.relocate_stats["copied"] += 1
                    logger.info(f"ğŸ“‹ í•˜ë“œ ë§í¬ ì‹¤íŒ¨, ë³µì‚¬ë¡œ ëŒ€ì²´: {target_path.name}")
            
            # ê¶Œí•œ ì„¤ì •
            target_path.chmod(0o644)
            
            # ê²€ì¦
            if target_path.exists():
                actual_size = target_path.stat().st_size / (1024 * 1024)
                self.relocate_stats["total_size_gb"] += actual_size / 1024
                
                return {
                    "success": True,
                    "action": action,
                    "source": str(source_path),
                    "target": str(target_path),
                    "size_mb": actual_size
                }
            else:
                return {"success": False, "error": "Target file not created"}
                
        except Exception as e:
            logger.error(f"âŒ {model_key} ì¬ë°°ì¹˜ ì‹¤íŒ¨: {e}")
            self.relocate_stats["errors"] += 1
            return {"success": False, "error": str(e)}
    
    def _determine_relocate_action(self, file_size_mb: float, copy_large_files: bool, create_symlinks: bool) -> str:
        """ì¬ë°°ì¹˜ ë°©ë²• ê²°ì •"""
        # ëŒ€ìš©ëŸ‰ íŒŒì¼ (1GB ì´ìƒ)
        if file_size_mb > 1000:
            if copy_large_files:
                return "copy"
            elif create_symlinks:
                return "symlink" 
            else:
                return "hardlink"
        
        # ì¤‘ê°„ í¬ê¸° íŒŒì¼ (100MB ~ 1GB)
        elif file_size_mb > 100:
            if create_symlinks:
                return "symlink"
            else:
                return "copy"
        
        # ì‘ì€ íŒŒì¼ (100MB ë¯¸ë§Œ)
        else:
            return "copy"
    
    def _create_relocate_summary(self, results: Dict[str, Dict]) -> Dict[str, Any]:
        """ì¬ë°°ì¹˜ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        successful = [k for k, v in results.items() if v.get("success")]
        failed = [k for k, v in results.items() if not v.get("success")]
        
        return {
            "success": len(failed) == 0,
            "total_models": len(results),
            "successful_count": len(successful),
            "failed_count": len(failed),
            "successful_models": successful,
            "failed_models": failed,
            "relocate_stats": self.relocate_stats,
            "results": results
        }
    
    def _print_relocate_summary(self, summary: Dict[str, Any]):
        """ì¬ë°°ì¹˜ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logger.info("=" * 70)
        logger.info("ğŸ¯ AI ëª¨ë¸ ì¬ë°°ì¹˜ ê²°ê³¼ ìš”ì•½")  
        logger.info("=" * 70)
        logger.info(f"ğŸ“Š ì´ ëª¨ë¸: {summary['total_models']}ê°œ")
        logger.info(f"âœ… ì„±ê³µ: {summary['successful_count']}ê°œ")
        logger.info(f"âŒ ì‹¤íŒ¨: {summary['failed_count']}ê°œ")
        
        stats = summary['relocate_stats']
        logger.info(f"ğŸ“‹ ë³µì‚¬: {stats['copied']}ê°œ")
        logger.info(f"ğŸ”— ì‹¬ë³¼ë¦­ ë§í¬: {stats['symlinked']}ê°œ") 
        logger.info(f"â­ï¸ ìŠ¤í‚µ: {stats['skipped']}ê°œ")
        logger.info(f"ğŸ’¾ ì´ í¬ê¸°: {stats['total_size_gb']:.2f}GB")
        
        if summary['successful_models']:
            logger.info("\nâœ… ì„±ê³µí•œ ëª¨ë¸ë“¤:")
            for model in summary['successful_models']:
                logger.info(f"  - {model}")
        
        if summary['failed_models']:
            logger.info("\nâŒ ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤:")
            for model in summary['failed_models']:
                error = summary['results'][model].get('error', 'Unknown')
                logger.info(f"  - {model}: {error}")

# ==============================================
# ğŸ”§ ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ í´ë˜ìŠ¤  
# ==============================================

class ConfigUpdater:
    """ì„¤ì • íŒŒì¼ ìë™ ì—…ë°ì´íŠ¸"""
    
    def __init__(self, project_root: Path, relocate_summary: Dict[str, Any]):
        self.project_root = Path(project_root)
        self.backend_dir = self.project_root  
        self.summary = relocate_summary
        
    def update_all_configs(self):
        """ëª¨ë“  ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸"""
        logger.info("ğŸ”§ ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        # 1. ModelLoader ì„¤ì • ì—…ë°ì´íŠ¸
        self._update_model_loader_config()
        
        # 2. í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ìƒì„±
        self._create_env_file()
        
        # 3. ëª¨ë¸ ê²½ë¡œ ì„¤ì • íŒŒì¼ ìƒì„±
        self._create_model_paths_config()
        
        # 4. ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ì—…ë°ì´íŠ¸
        self._update_startup_script()
        
        logger.info("âœ… ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    def _update_model_loader_config(self):
        """ModelLoader ì„¤ì • ì—…ë°ì´íŠ¸"""
        config_content = f'''# Auto-generated model paths configuration
"""
ìë™ ìƒì„±ëœ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
Generated at: {time.strftime("%Y-%m-%d %H:%M:%S")}
"""

from pathlib import Path

# ë² ì´ìŠ¤ ê²½ë¡œ
BACKEND_DIR = Path(__file__).parent.parent
AI_MODELS_DIR = BACKEND_DIR / "ai_models"

# ì‹¤ì œ íƒì§€ëœ ëª¨ë¸ ê²½ë¡œë“¤
ACTUAL_MODEL_PATHS = {{
'''

        # ì„±ê³µí•œ ëª¨ë¸ë“¤ì˜ ê²½ë¡œ ì¶”ê°€
        for model_key in self.summary.get('successful_models', []):
            result = self.summary['results'][model_key]
            if result.get('success'):
                target_path = result.get('target')
                if target_path:
                    config_content += f'    "{model_key}": "{target_path}",\n'

        config_content += '''
}

# ëª¨ë¸ ê°€ìš©ì„± ì²´í¬
MODEL_AVAILABILITY = {
'''

        for model_key in REQUIRED_MODELS.keys():
            is_available = model_key in self.summary.get('successful_models', [])
            config_content += f'    "{model_key}": {is_available},\n'

        config_content += '''
}

def get_model_path(model_key: str) -> str:
    """ëª¨ë¸ ê²½ë¡œ ë°˜í™˜"""
    return ACTUAL_MODEL_PATHS.get(model_key, "")

def is_model_available(model_key: str) -> bool:
    """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    return MODEL_AVAILABILITY.get(model_key, False)

def get_available_models() -> list:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return [k for k, v in MODEL_AVAILABILITY.items() if v]
'''

        # íŒŒì¼ ì €ì¥
        config_path = self.backend_dir / "app" / "core" / "actual_model_paths.py"
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        logger.info(f"âœ… ModelLoader ì„¤ì • ì—…ë°ì´íŠ¸: {config_path}")
    
    def _create_env_file(self):
        """í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ìƒì„±"""
        env_content = f'''# MyCloset AI Environment Configuration
# Generated at: {time.strftime("%Y-%m-%d %H:%M:%S")}

# ëª¨ë¸ ëª¨ë“œ ì„¤ì •
MYCLOSET_MODE=development
MYCLOSET_SIMULATION=false

# AI ëª¨ë¸ ê²½ë¡œ
AI_MODELS_ROOT=./ai_models
CHECKPOINTS_DIR=./ai_models/checkpoints

# ëª¨ë¸ ê°€ìš©ì„±
'''

        for model_key, is_available in zip(REQUIRED_MODELS.keys(), 
                                          [k in self.summary.get('successful_models', []) for k in REQUIRED_MODELS.keys()]):
            env_name = f"MODEL_{model_key.upper()}_AVAILABLE"
            env_content += f'{env_name}={str(is_available).lower()}\n'

        env_content += f'''
# ì‹œìŠ¤í…œ ì„¤ì •
PYTORCH_ENABLE_MPS_FALLBACK=1
PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.8
OMP_NUM_THREADS=16

# ë¡œê¹…
LOG_LEVEL=INFO
'''

        # .env íŒŒì¼ ì €ì¥
        env_path = self.backend_dir / ".env"
        with open(env_path, 'w', encoding='utf-8') as f:
            f.write(env_content)
        
        logger.info(f"âœ… í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ìƒì„±: {env_path}")
    
    def _create_model_paths_config(self):
        """ëª¨ë¸ ê²½ë¡œ ì„¤ì • JSON íŒŒì¼ ìƒì„±"""
        config_data = {
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "project_root": str(self.project_root),
            "ai_models_dir": str(self.backend_dir / "ai_models"),
            "model_paths": {},
            "model_availability": {},
            "relocate_summary": self.summary
        }
        
        for model_key in REQUIRED_MODELS.keys():
            is_available = model_key in self.summary.get('successful_models', [])
            config_data["model_availability"][model_key] = is_available
            
            if is_available:
                result = self.summary['results'][model_key]
                config_data["model_paths"][model_key] = result.get('target', '')
        
        # JSON íŒŒì¼ ì €ì¥
        json_path = self.backend_dir / "app" / "core" / "model_paths.json"
        json_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… ëª¨ë¸ ê²½ë¡œ JSON ì„¤ì •: {json_path}")
    
    def _update_startup_script(self):
        """ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ì—…ë°ì´íŠ¸"""
        script_content = f'''#!/bin/bash
# MyCloset AI Startup Script
# Generated at: {time.strftime("%Y-%m-%d %H:%M:%S")}

echo "ğŸš€ MyCloset AI ì‹œì‘ ì¤‘..."

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
if [ -f .env ]; then
    export $(cat .env | grep -v '^#' | xargs)
fi

# conda í™˜ê²½ í™œì„±í™” í™•ì¸
if [ -z "$CONDA_PREFIX" ]; then
    echo "âš ï¸ conda í™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
    echo "ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: conda activate mycloset-ai"
    exit 1
fi

# ëª¨ë¸ ê°€ìš©ì„± ì²´í¬
echo "ğŸ” ëª¨ë¸ ê°€ìš©ì„± ì²´í¬..."
'''

        available_count = len(self.summary.get('successful_models', []))
        total_count = len(REQUIRED_MODELS)
        
        script_content += f'''
echo "ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_count}/{total_count}ê°œ"
'''

        for model_key in self.summary.get('successful_models', []):
            script_content += f'echo "âœ… {model_key}"\n'

        for model_key in self.summary.get('failed_models', []):
            script_content += f'echo "âŒ {model_key}"\n'

        script_content += '''
# ì„œë²„ ì‹œì‘
echo "ğŸŒ ì„œë²„ ì‹œì‘ ì¤‘..."
python3 app/main.py

echo "âœ… MyCloset AI ì„œë²„ ì‹œì‘ ì™„ë£Œ"
'''

        # ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ì €ì¥
        script_path = self.backend_dir / "start_mycloset.sh"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
        script_path.chmod(0o755)
        
        logger.info(f"âœ… ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ì—…ë°ì´íŠ¸: {script_path}")

# ==============================================
# ğŸ¯ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ==============================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=" * 70)
    logger.info("ğŸ” AI ëª¨ë¸ íŒŒì¼ ìë™ íƒì§€ ë° ì¬ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    logger.info("=" * 70)
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ í™•ì¸
    current_file = Path(__file__).resolve()
    project_root = current_file.parent.parent  # scripts -> backend
    
    logger.info(f"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    
    try:
        # 1ë‹¨ê³„: ëª¨ë¸ íŒŒì¼ íƒì§€
        logger.info("\nğŸ” 1ë‹¨ê³„: AI ëª¨ë¸ íŒŒì¼ ì „ì²´ íƒì§€")
        finder = AIModelFinder(project_root)
        found_models = finder.scan_all_paths(max_workers=4, max_depth=6)
        
        if not found_models:
            logger.error("âŒ íƒì§€ëœ AI ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            logger.info("ğŸ’¡ ë‹¤ìŒì„ í™•ì¸í•´ë³´ì„¸ìš”:")
            logger.info("   - ai_models í´ë”ê°€ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆëŠ”ì§€")
            logger.info("   - ëª¨ë¸ íŒŒì¼ë“¤ì´ ì˜¬ë°”ë¥¸ í™•ì¥ì(.pth, .bin ë“±)ë¥¼ ê°€ì§€ëŠ”ì§€")
            logger.info("   - íŒŒì¼ ê¶Œí•œì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€")
            return False
        
        # 2ë‹¨ê³„: ëª¨ë¸ ì¬ë°°ì¹˜
        logger.info("\nğŸš€ 2ë‹¨ê³„: AI ëª¨ë¸ íŒŒì¼ ì¬ë°°ì¹˜")
        relocator = AIModelRelocator(project_root, found_models)
        
        # ì‚¬ìš©ì ì˜µì…˜ (ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •)
        copy_large_files = False  # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ì‹¬ë³¼ë¦­ ë§í¬ ì‚¬ìš©
        create_symlinks = True    # ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± í™œì„±í™”
        
        relocate_summary = relocator.relocate_all_models(copy_large_files, create_symlinks)
        
        # 3ë‹¨ê³„: ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
        logger.info("\nğŸ”§ 3ë‹¨ê³„: ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸")
        config_updater = ConfigUpdater(project_root, relocate_summary)
        config_updater.update_all_configs()
        
        # ìµœì¢… ê²°ê³¼
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ‰ AI ëª¨ë¸ ìë™ íƒì§€ ë° ì¬ë°°ì¹˜ ì™„ë£Œ!")
        logger.info("=" * 70)
        
        successful_count = relocate_summary.get('successful_count', 0)
        total_count = relocate_summary.get('total_models', 0)
        
        logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {successful_count}/{total_count}ê°œ ëª¨ë¸ ì„±ê³µ")
        
        if successful_count > 0:
            logger.info("âœ… ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            logger.info(f"   cd {project_root}")
            logger.info("   ./start_mycloset.sh")
            logger.info("\në˜ëŠ”:")
            logger.info("   python3 app/main.py")
        else:
            logger.error("âŒ ì¬ë°°ì¹˜ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ íŒŒì¼ë“¤ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        return successful_count > 0
        
    except Exception as e:
        logger.error(f"âŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)