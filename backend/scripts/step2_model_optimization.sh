#!/bin/bash
# ================================================================
# MyCloset AI - 2ë‹¨ê³„: AI ëª¨ë¸ ì—°ê²° ìµœì í™” ì‹¤í–‰
# ================================================================

set -e

echo "ğŸš€ MyCloset AI - 2ë‹¨ê³„: AI ëª¨ë¸ ì—°ê²° ìµœì í™”"
echo "=================================================================="

# í™˜ê²½ í™•ì¸
if [[ "$CONDA_DEFAULT_ENV" != "mycloset-ai" ]]; then
    echo "âš ï¸ conda í™˜ê²½ í™œì„±í™” í•„ìš”"
    source "$(conda info --base)/etc/profile.d/conda.sh"
    conda activate mycloset-ai
fi

echo "âœ… í™˜ê²½: $CONDA_DEFAULT_ENV"
echo "âœ… Python: $(python --version)"

# ================================================================
# 2ë‹¨ê³„-1: ëª¨ë¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ë° ì„ ë³„
# ================================================================

echo ""
echo "ğŸ“‹ 2ë‹¨ê³„-1: ëª¨ë¸ ìŠ¤ìº” ë° ì„ ë³„"

python3 -c "
import os
import sys
import json
from pathlib import Path
from typing import Dict, List, Optional

print('ğŸ” AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì¤‘...')

# ëª¨ë¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
model_paths = [
    './ai_models',
    '../ai_models', 
    './backend/ai_models',
    '../backend/ai_models',
    './app/ai_pipeline/models/ai_models'
]

found_models = {}
total_size = 0

for path_str in model_paths:
    path = Path(path_str)
    if path.exists():
        print(f'ğŸ“ ë°œê²¬: {path.resolve()}')
        
        # ëª¨ë¸ íŒŒì¼ ìŠ¤ìº”
        extensions = ['.pth', '.bin', '.safetensors', '.ckpt']
        for ext in extensions:
            for model_file in path.rglob(f'*{ext}'):
                size_mb = model_file.stat().st_size / (1024 * 1024)
                total_size += size_mb
                
                # 8ë‹¨ê³„ ë¶„ë¥˜
                name_lower = model_file.name.lower()
                if 'human' in name_lower or 'parsing' in name_lower:
                    step = 'step_01_human_parsing'
                elif 'pose' in name_lower or 'openpose' in name_lower:
                    step = 'step_02_pose_estimation'
                elif 'cloth' in name_lower or 'segment' in name_lower or 'u2net' in name_lower:
                    step = 'step_03_cloth_segmentation'
                elif 'geometric' in name_lower or 'gmm' in name_lower:
                    step = 'step_04_geometric_matching'
                elif 'warp' in name_lower or 'tom' in name_lower:
                    step = 'step_05_cloth_warping'
                elif 'diffusion' in name_lower or 'virtual' in name_lower or 'oot' in name_lower:
                    step = 'step_06_virtual_fitting'
                elif 'post' in name_lower or 'esrgan' in name_lower:
                    step = 'step_07_post_processing'
                elif 'clip' in name_lower or 'quality' in name_lower:
                    step = 'step_08_quality_assessment'
                else:
                    step = 'unknown'
                
                found_models[model_file.name] = {
                    'path': str(model_file),
                    'size_mb': round(size_mb, 1),
                    'step': step,
                    'priority': 'high' if size_mb > 500 else 'medium' if size_mb > 100 else 'low'
                }

print(f'\\nğŸ“Š ìŠ¤ìº” ê²°ê³¼:')
print(f'  - ì´ ëª¨ë¸: {len(found_models)}ê°œ')
print(f'  - ì´ í¬ê¸°: {total_size/1024:.1f}GB')

# ë‹¨ê³„ë³„ ë¶„ë¥˜
step_counts = {}
for model_info in found_models.values():
    step = model_info['step']
    step_counts[step] = step_counts.get(step, 0) + 1

print(f'\\nğŸ“‹ ë‹¨ê³„ë³„ ë¶„ë¥˜:')
for step, count in sorted(step_counts.items()):
    print(f'  - {step}: {count}ê°œ')

# í•µì‹¬ ëª¨ë¸ ì„ ë³„ (ê° ë‹¨ê³„ë³„ ìµœëŒ€ 2ê°œ)
essential_models = {}
for step in ['step_01_human_parsing', 'step_02_pose_estimation', 'step_03_cloth_segmentation', 
             'step_04_geometric_matching', 'step_05_cloth_warping', 'step_06_virtual_fitting']:
    
    step_models = [(name, info) for name, info in found_models.items() if info['step'] == step]
    step_models.sort(key=lambda x: x[1]['size_mb'], reverse=True)  # í° ê²ƒë¶€í„° (ë³´í†µ ë” ì¢‹ìŒ)
    
    for i, (name, info) in enumerate(step_models[:2]):  # ìµœëŒ€ 2ê°œ
        essential_models[name] = info
        print(f'âœ… ì„ ë³„: {step} -> {name} ({info[\"size_mb\"]}MB)')

# ì„¤ì • ì €ì¥
config = {
    'scan_results': {
        'total_models': len(found_models),
        'total_size_gb': round(total_size/1024, 1),
        'step_counts': step_counts
    },
    'selected_models': essential_models,
    'optimization_target': {
        'memory_limit_gb': 32,
        'device': 'mps',
        'precision': 'float32'
    }
}

with open('model_scan_results.json', 'w') as f:
    json.dump(config, f, indent=2)

print(f'\\nâœ… ìŠ¤ìº” ì™„ë£Œ: model_scan_results.json ì €ì¥')
print(f'ğŸ“Š ì„ ë³„ëœ í•µì‹¬ ëª¨ë¸: {len(essential_models)}ê°œ')
"

echo ""
echo "âœ… 2ë‹¨ê³„-1 ì™„ë£Œ: ëª¨ë¸ ìŠ¤ìº” ë° ì„ ë³„"

# ================================================================
# 2ë‹¨ê³„-2: ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ í•´ê²°
# ================================================================

echo ""
echo "ğŸ“‹ 2ë‹¨ê³„-2: ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ í•´ê²°"

python3 -c "
import json
from pathlib import Path

print('ğŸ—ºï¸ ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ í•´ê²° ì¤‘...')

# ìŠ¤ìº” ê²°ê³¼ ë¡œë“œ
with open('model_scan_results.json', 'r') as f:
    config = json.load(f)

selected_models = config['selected_models']

# ê²½ë¡œ ë§¤í•‘ ìƒì„±
path_mappings = {}
duplicates_removed = 0

for model_name, model_info in selected_models.items():
    real_path = Path(model_info['path']).resolve()
    
    if real_path.exists():
        # ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
        try:
            rel_path = real_path.relative_to(Path.cwd())
            path_mappings[model_name] = str(rel_path)
        except ValueError:
            path_mappings[model_name] = str(real_path)
        
        print(f'âœ… ë§¤í•‘: {model_name} -> {path_mappings[model_name]}')
    else:
        print(f'âŒ ê²½ë¡œ ì—†ìŒ: {model_name} -> {model_info[\"path\"]}')

# ì¤‘ë³µ ì œê±° (ê°™ì€ íŒŒì¼ì„ ê°€ë¦¬í‚¤ëŠ” ëª¨ë¸ë“¤)
path_to_models = {}
for model_name, path in path_mappings.items():
    real_path = str(Path(path).resolve())
    if real_path not in path_to_models:
        path_to_models[real_path] = []
    path_to_models[real_path].append(model_name)

# ì¤‘ë³µëœ ê²½ìš° ê°€ì¥ ê°„ë‹¨í•œ ì´ë¦„ë§Œ ìœ ì§€
final_mappings = {}
for real_path, model_names in path_to_models.items():
    if len(model_names) > 1:
        # ê°€ì¥ ê°„ë‹¨í•œ ì´ë¦„ ì„ íƒ
        best_name = min(model_names, key=len)
        final_mappings[best_name] = path_mappings[best_name]
        duplicates_removed += len(model_names) - 1
        print(f'ğŸ”„ ì¤‘ë³µ ì œê±°: {model_names} -> {best_name}')
    else:
        model_name = model_names[0]
        final_mappings[model_name] = path_mappings[model_name]

# ì—…ë°ì´íŠ¸ëœ ì„¤ì • ì €ì¥
config['path_mappings'] = final_mappings
config['optimization_stats'] = {
    'original_models': len(selected_models),
    'final_models': len(final_mappings),
    'duplicates_removed': duplicates_removed
}

with open('model_scan_results.json', 'w') as f:
    json.dump(config, f, indent=2)

print(f'\\nâœ… ê²½ë¡œ ë§¤í•‘ ì™„ë£Œ:')
print(f'  - ìµœì¢… ëª¨ë¸: {len(final_mappings)}ê°œ')
print(f'  - ì¤‘ë³µ ì œê±°: {duplicates_removed}ê°œ')
"

echo ""
echo "âœ… 2ë‹¨ê³„-2 ì™„ë£Œ: ê²½ë¡œ ë§¤í•‘ í•´ê²°"

# ================================================================
# 2ë‹¨ê³„-3: í†µí•© ëª¨ë¸ ë¡œë” ìƒì„±
# ================================================================

echo ""
echo "ğŸ“‹ 2ë‹¨ê³„-3: í†µí•© ëª¨ë¸ ë¡œë” ìƒì„±"

# í†µí•© ëª¨ë¸ ë¡œë” í´ë˜ìŠ¤ ìƒì„±
cat > optimized_model_loader.py << 'EOF'
"""
MyCloset AI - ìµœì í™”ëœ í†µí•© ëª¨ë¸ ë¡œë”
M3 Max 128GB ì „ìš© ìµœì í™” ë²„ì „
"""

import json
import torch
import asyncio
import logging
from pathlib import Path
from typing import Dict, Any, Optional
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OptimizedModelLoader:
    """M3 Max ìµœì í™” ëª¨ë¸ ë¡œë”"""
    
    def __init__(self, config_path: str = "model_scan_results.json"):
        with open(config_path, 'r') as f:
            self.config = json.load(f)
        
        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
        self.loaded_models = {}
        self.load_times = {}
        
        logger.info(f"ğŸ M3 Max ëª¨ë¸ ë¡œë” ì´ˆê¸°í™”: {self.device}")
        logger.info(f"ğŸ“¦ ê´€ë¦¬ ëŒ€ìƒ ëª¨ë¸: {len(self.config['path_mappings'])}ê°œ")
    
    async def load_model_async(self, model_name: str) -> Optional[torch.nn.Module]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë”©"""
        if model_name in self.loaded_models:
            logger.info(f"ğŸ”„ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
            return self.loaded_models[model_name]
        
        if model_name not in self.config['path_mappings']:
            logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ ì—†ìŒ: {model_name}")
            return None
        
        model_path = self.config['path_mappings'][model_name]
        
        try:
            start_time = time.time()
            logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
            
            # M3 Max MPS ìµœì í™” ë¡œë”©
            if self.device.type == 'mps':
                # CPUì—ì„œ ë¡œë“œ í›„ MPSë¡œ ì´ë™ (ì•ˆì „í•œ ë°©ë²•)
                model = torch.load(model_path, map_location='cpu', weights_only=False)
                if hasattr(model, 'to'):
                    model = model.to(self.device)
            else:
                model = torch.load(model_path, map_location=self.device, weights_only=False)
            
            # í‰ê°€ ëª¨ë“œ ì„¤ì •
            if hasattr(model, 'eval'):
                model.eval()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if hasattr(model, 'half') and self.device.type == 'mps':
                # MPSì—ì„œ float16 ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
                try:
                    model = model.half()
                except:
                    logger.warning(f"âš ï¸ float16 ë³€í™˜ ì‹¤íŒ¨, float32 ìœ ì§€: {model_name}")
            
            load_time = time.time() - start_time
            self.loaded_models[model_name] = model
            self.load_times[model_name] = load_time
            
            logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name} ({load_time:.2f}ì´ˆ)")
            return model
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def unload_model(self, model_name: str):
        """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)"""
        if model_name in self.loaded_models:
            model = self.loaded_models[model_name]
            if hasattr(model, 'cpu'):
                model.cpu()
            del self.loaded_models[model_name]
            logger.info(f"ğŸ—‘ï¸ ëª¨ë¸ ì–¸ë¡œë“œ: {model_name}")
    
    def get_step_model(self, step: str) -> Optional[str]:
        """ë‹¨ê³„ë³„ ê¶Œì¥ ëª¨ë¸ ë°˜í™˜"""
        for model_name, model_info in self.config['selected_models'].items():
            if model_info['step'] == step:
                return model_name
        return None
    
    def get_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        if self.device.type == 'mps':
            try:
                total_memory = torch.mps.current_allocated_memory() / (1024**2)  # MB
                return {"mps_memory_mb": total_memory}
            except:
                return {"mps_memory_mb": 0}
        return {"cpu_memory": "N/A"}
    
    def get_statistics(self) -> Dict[str, Any]:
        """ë¡œë” í†µê³„"""
        return {
            "device": str(self.device),
            "loaded_models": len(self.loaded_models),
            "available_models": len(self.config['path_mappings']),
            "load_times": self.load_times,
            "memory_usage": self.get_memory_usage()
        }

# ì „ì—­ ë¡œë” ì¸ìŠ¤í„´ìŠ¤
_global_loader = None

def get_model_loader() -> OptimizedModelLoader:
    """ì „ì—­ ëª¨ë¸ ë¡œë” ë°˜í™˜"""
    global _global_loader
    if _global_loader is None:
        _global_loader = OptimizedModelLoader()
    return _global_loader

async def load_essential_models():
    """í•„ìˆ˜ ëª¨ë¸ë“¤ ì‚¬ì „ ë¡œë”©"""
    loader = get_model_loader()
    
    essential_steps = [
        'step_01_human_parsing',
        'step_02_pose_estimation', 
        'step_03_cloth_segmentation',
        'step_06_virtual_fitting'
    ]
    
    loaded_count = 0
    for step in essential_steps:
        model_name = loader.get_step_model(step)
        if model_name:
            model = await loader.load_model_async(model_name)
            if model:
                loaded_count += 1
    
    logger.info(f"âœ… í•„ìˆ˜ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}/{len(essential_steps)}")
    return loaded_count

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    async def test_loader():
        loader = get_model_loader()
        print("ğŸ“Š ëª¨ë¸ ë¡œë” í†µê³„:")
        stats = loader.get_statistics()
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        print("\nğŸ”„ í•„ìˆ˜ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸:")
        loaded_count = await load_essential_models()
        
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {loaded_count}ê°œ ëª¨ë¸ ë¡œë”© ì„±ê³µ")
    
    asyncio.run(test_loader())
EOF

echo ""
echo "âœ… 2ë‹¨ê³„-3 ì™„ë£Œ: í†µí•© ëª¨ë¸ ë¡œë” ìƒì„±"

# ================================================================
# 2ë‹¨ê³„-4: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦
# ================================================================

echo ""
echo "ğŸ“‹ 2ë‹¨ê³„-4: ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸"

python3 optimized_model_loader.py

echo ""
echo "âœ… 2ë‹¨ê³„-4 ì™„ë£Œ: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦"

# ================================================================
# 2ë‹¨ê³„ ì™„ë£Œ ìš”ì•½
# ================================================================

echo ""
echo "ğŸ‰ 2ë‹¨ê³„: AI ëª¨ë¸ ì—°ê²° ìµœì í™” ì™„ë£Œ!"
echo "=================================================================="

echo ""
echo "ğŸ“Š ì™„ë£Œëœ ì‘ì—…:"
echo "  âœ… ëª¨ë¸ ìŠ¤ìº” ë° ì„ ë³„"
echo "  âœ… ê²½ë¡œ ë§¤í•‘ í•´ê²°"  
echo "  âœ… í†µí•© ëª¨ë¸ ë¡œë” ìƒì„±"
echo "  âœ… í…ŒìŠ¤íŠ¸ ë° ê²€ì¦"

echo ""
echo "ğŸ“ ìƒì„±ëœ íŒŒì¼:"
echo "  - model_scan_results.json: ëª¨ë¸ ìŠ¤ìº” ê²°ê³¼ ë° ì„¤ì •"
echo "  - optimized_model_loader.py: í†µí•© ëª¨ë¸ ë¡œë”"

echo ""
echo "ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (3ë‹¨ê³„ - ì„œë²„ í†µí•©):"
echo "  1. ê¸°ì¡´ ì„œë²„ì— ëª¨ë¸ ë¡œë” í†µí•©"
echo "  2. API ì—”ë“œí¬ì¸íŠ¸ ì—°ê²°"
echo "  3. ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"

echo ""
echo "ğŸ’¡ ì‚¬ìš©ë²•:"
echo "  from optimized_model_loader import get_model_loader"
echo "  loader = get_model_loader()"
echo "  model = await loader.load_model_async('model_name')"

echo ""
echo "ğŸ“Š ëª¨ë¸ í˜„í™© í™•ì¸:"
echo "  cat model_scan_results.json | jq '.scan_results'"