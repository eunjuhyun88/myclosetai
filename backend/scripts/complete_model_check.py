#!/usr/bin/env python3
"""
ğŸ” ì™„ì „í•œ AI ëª¨ë¸ ì²´í¬ ë° ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
- ëª¨ë“  í•„ìš”í•œ ëª¨ë¸ íŒŒì¼ í™•ì¸
- ëˆ„ë½ëœ ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ
- M3 Max ìµœì í™” ì„¤ì •
"""

import os
import sys
import json
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import subprocess
import time
from urllib.parse import urlparse
import hashlib

def log_info(msg: str):
    print(f"â„¹ï¸  {msg}")

def log_success(msg: str):
    print(f"âœ… {msg}")

def log_warning(msg: str):
    print(f"âš ï¸  {msg}")

def log_error(msg: str):
    print(f"âŒ {msg}")

def log_download(msg: str):
    print(f"ğŸ“¥ {msg}")

class AIModelChecker:
    """AI ëª¨ë¸ ì²´í¬ ë° ê´€ë¦¬"""
    
    def __init__(self):
        self.base_dir = Path("ai_models")
        self.checkpoints_dir = self.base_dir / "checkpoints"
        self.missing_models = []
        self.available_models = []
        self.model_configs = self._get_model_requirements()
        
    def _get_model_requirements(self) -> Dict:
        """í•„ìš”í•œ ëª¨ë¸ë“¤ê³¼ ìš”êµ¬ì‚¬í•­ ì •ì˜"""
        return {
            # Step 1: Human Parsing (ì¸ì²´ íŒŒì‹±)
            "segformer_human_parsing": {
                "path": "checkpoints/step_01_human_parsing/segformer_b2_clothes",
                "files": [
                    "config.json",
                    "pytorch_model.bin",
                    "preprocessor_config.json"
                ],
                "size_mb": 200,
                "description": "Segformer ì¸ì²´ íŒŒì‹± ëª¨ë¸",
                "huggingface_id": "mattmdjaga/segformer_b2_clothes",
                "priority": "high"
            },
            
            "graphonomy_atr": {
                "path": "checkpoints/step_01_human_parsing",
                "files": ["graphonomy_atr.pth"],
                "size_mb": 85,
                "description": "Graphonomy ATR ëª¨ë¸",
                "download_url": "https://drive.google.com/file/d/1ruJg4lqR_jgQPj-9K0PP-L2vJERYOxLP/view",
                "priority": "high"
            },
            
            "graphonomy_lip": {
                "path": "checkpoints/step_01_human_parsing", 
                "files": ["graphonomy_lip.pth"],
                "size_mb": 85,
                "description": "Graphonomy LIP ëª¨ë¸",
                "priority": "medium"
            },
            
            # Step 2: Pose Estimation (í¬ì¦ˆ ì¶”ì •)
            "mediapipe_pose": {
                "path": "checkpoints/step_02_pose_estimation",
                "files": [
                    "pose_landmarker.task",
                    "pose_landmark_full.tflite"
                ],
                "size_mb": 15,
                "description": "MediaPipe í¬ì¦ˆ ì¶”ì •",
                "priority": "high"
            },
            
            "yolov8_pose": {
                "path": "checkpoints/step_02_pose_estimation",
                "files": ["yolov8n-pose.pt"],
                "size_mb": 6,
                "description": "YOLOv8 í¬ì¦ˆ ëª¨ë¸",
                "priority": "medium"
            },
            
            # Step 3: Cloth Segmentation (ì˜ë¥˜ ë¶„í• )
            "u2net_pytorch": {
                "path": "checkpoints/step_03_cloth_segmentation",
                "files": ["u2net.pth"],
                "size_mb": 176,
                "description": "UÂ²-Net PyTorch ëª¨ë¸",
                "priority": "high"
            },
            
            "u2net_onnx": {
                "path": "checkpoints/step_03_cloth_segmentation",
                "files": ["u2net.onnx"],
                "size_mb": 176,
                "description": "UÂ²-Net ONNX ëª¨ë¸",
                "priority": "high"
            },
            
            "mobile_sam": {
                "path": "checkpoints/step_03_cloth_segmentation",
                "files": ["mobile_sam.pt"],
                "size_mb": 40,
                "description": "Mobile SAM ëª¨ë¸",
                "priority": "medium"
            },
            
            # CLIP Models (í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì„ë² ë”©)
            "clip_vit_base": {
                "path": "clip-vit-base-patch32",  # checkpoints ë°–ì— ìˆìŒ
                "files": [
                    "config.json",
                    "model.safetensors",  # ë˜ëŠ” pytorch_model.bin
                    "preprocessor_config.json",
                    "tokenizer.json",
                    "vocab.json"
                ],
                "size_mb": 600,
                "description": "CLIP ViT-Base ëª¨ë¸",
                "huggingface_id": "openai/clip-vit-base-patch32",
                "priority": "critical"
            },
            
            "clip_vit_large": {
                "path": "checkpoints/clip-vit-large-patch14",
                "files": [
                    "config.json",
                    "model.safetensors",
                    "pytorch_model.bin"
                ],
                "size_mb": 1700,
                "description": "CLIP ViT-Large ëª¨ë¸",
                "priority": "medium"
            },
            
            # OOTDiffusion (ê°€ìƒ í”¼íŒ…)
            "ootdiffusion_checkpoints": {
                "path": "checkpoints/ootdiffusion/checkpoints/ootd",
                "files": [
                    "model_index.json",
                    "ootd_hd/checkpoint-36000/unet_garm/diffusion_pytorch_model.safetensors",
                    "ootd_hd/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors"
                ],
                "size_mb": 3400,
                "description": "OOTDiffusion ì²´í¬í¬ì¸íŠ¸",
                "priority": "critical"
            },
            
            # Stable Diffusion Base
            "stable_diffusion_v15": {
                "path": "checkpoints/stable-diffusion-v1-5",
                "files": [
                    "model_index.json",
                    "unet/diffusion_pytorch_model.safetensors",
                    "vae/diffusion_pytorch_model.safetensors",
                    "text_encoder/pytorch_model.bin"
                ],
                "size_mb": 4000,
                "description": "Stable Diffusion v1.5",
                "huggingface_id": "runwayml/stable-diffusion-v1-5",
                "priority": "critical"
            },
            
            # Post Processing
            "realesrgan": {
                "path": "checkpoints/step_07_post_processing",
                "files": ["RealESRGAN_x4plus.pth"],
                "size_mb": 67,
                "description": "RealESRGAN ì—…ìŠ¤ì¼€ì¼ëŸ¬",
                "priority": "low"
            }
        }
    
    def check_all_models(self) -> Dict:
        """ëª¨ë“  ëª¨ë¸ ì²´í¬"""
        log_info("AI ëª¨ë¸ ì™„ì „ ì²´í¬ ì‹œì‘...")
        print("=" * 60)
        
        results = {
            "critical_missing": [],
            "high_missing": [],
            "medium_missing": [],
            "low_missing": [],
            "available": [],
            "total_missing_size_gb": 0,
            "issues": []
        }
        
        for model_name, config in self.model_configs.items():
            status = self._check_single_model(model_name, config)
            
            if status["available"]:
                results["available"].append({
                    "name": model_name,
                    "description": config["description"],
                    "size_mb": status.get("actual_size_mb", 0)
                })
                log_success(f"{config['description']}: ì •ìƒ")
            else:
                priority = config.get("priority", "medium")
                missing_info = {
                    "name": model_name,
                    "description": config["description"],
                    "size_mb": config.get("size_mb", 0),
                    "path": config["path"],
                    "missing_files": status["missing_files"],
                    "has_huggingface": "huggingface_id" in config
                }
                
                results[f"{priority}_missing"].append(missing_info)
                results["total_missing_size_gb"] += config.get("size_mb", 0) / 1024
                
                missing_files_str = ", ".join(status["missing_files"][:3])
                if len(status["missing_files"]) > 3:
                    missing_files_str += f" ì™¸ {len(status['missing_files'])-3}ê°œ"
                
                log_error(f"{config['description']}: ëˆ„ë½ ({missing_files_str})")
        
        # CLIP íŠ¹ë³„ ì²´í¬ (ê²½ë¡œ ë¬¸ì œ ë•Œë¬¸ì—)
        self._special_clip_check(results)
        
        return results
    
    def _check_single_model(self, model_name: str, config: Dict) -> Dict:
        """ê°œë³„ ëª¨ë¸ ì²´í¬"""
        model_path = self.base_dir / config["path"]
        missing_files = []
        available_files = []
        total_size = 0
        
        for required_file in config["files"]:
            file_path = model_path / required_file
            if file_path.exists():
                available_files.append(required_file)
                total_size += file_path.stat().st_size
            else:
                missing_files.append(required_file)
        
        # CLIP ëª¨ë¸ì˜ ê²½ìš° safetensors ë˜ëŠ” pytorch_model.bin ì¤‘ í•˜ë‚˜ë§Œ ìˆì–´ë„ OK
        if model_name.startswith("clip") and "model.safetensors" in missing_files:
            pytorch_model = model_path / "pytorch_model.bin"
            if pytorch_model.exists():
                missing_files.remove("model.safetensors")
                available_files.append("pytorch_model.bin")
                total_size += pytorch_model.stat().st_size
        
        return {
            "available": len(missing_files) == 0,
            "missing_files": missing_files,
            "available_files": available_files,
            "actual_size_mb": total_size / (1024**2)
        }
    
    def _special_clip_check(self, results: Dict):
        """CLIP ëª¨ë¸ íŠ¹ë³„ ì²´í¬ (ì—¬ëŸ¬ ìœ„ì¹˜ í™•ì¸)"""
        log_info("CLIP ëª¨ë¸ íŠ¹ë³„ ì²´í¬...")
        
        possible_clip_paths = [
            "ai_models/clip-vit-base-patch32",
            "ai_models/checkpoints/clip-vit-base-patch32", 
            "ai_models/checkpoints/shared_encoder/clip-vit-base-patch32"
        ]
        
        clip_found = False
        for clip_path in possible_clip_paths:
            path = Path(clip_path)
            if path.exists():
                safetensors = path / "model.safetensors"
                pytorch_model = path / "pytorch_model.bin"
                config_file = path / "config.json"
                
                if (safetensors.exists() or pytorch_model.exists()) and config_file.exists():
                    log_success(f"CLIP ëª¨ë¸ ë°œê²¬: {clip_path}")
                    clip_found = True
                    
                    # ê²°ê³¼ì—ì„œ CLIP missing ì œê±°
                    for priority in ["critical_missing", "high_missing", "medium_missing"]:
                        results[priority] = [m for m in results[priority] if not m["name"].startswith("clip")]
                    
                    # availableì— ì¶”ê°€
                    results["available"].append({
                        "name": "clip_vit_base",
                        "description": "CLIP ViT-Base ëª¨ë¸ (ë°œê²¬ë¨)",
                        "size_mb": (safetensors.stat().st_size if safetensors.exists() 
                                  else pytorch_model.stat().st_size) / (1024**2),
                        "path": str(clip_path)
                    })
                    break
        
        if not clip_found:
            log_warning("CLIP ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    def download_missing_models(self, results: Dict, priorities: List[str] = ["critical", "high"]):
        """ëˆ„ë½ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        log_info("ëˆ„ë½ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        
        total_downloaded = 0
        
        for priority in priorities:
            missing_key = f"{priority}_missing"
            if missing_key not in results:
                continue
                
            for model_info in results[missing_key]:
                model_name = model_info["name"]
                config = self.model_configs[model_name]
                
                log_download(f"{model_info['description']} ë‹¤ìš´ë¡œë“œ ì¤‘...")
                
                if self._download_model(model_name, config):
                    log_success(f"{model_info['description']} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                    total_downloaded += 1
                else:
                    log_error(f"{model_info['description']} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        
        log_info(f"ì´ {total_downloaded}ê°œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    
    def _download_model(self, model_name: str, config: Dict) -> bool:
        """ê°œë³„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            # HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ ê²½ìš°
            if "huggingface_id" in config:
                return self._download_from_huggingface(model_name, config)
            
            # íŠ¹ë³„í•œ ë‹¤ìš´ë¡œë“œ ë¡œì§ì´ í•„ìš”í•œ ëª¨ë¸ë“¤
            if model_name == "u2net_pytorch":
                return self._download_u2net()
            elif model_name == "mediapipe_pose":
                return self._download_mediapipe_pose()
            elif model_name.startswith("graphonomy"):
                return self._download_graphonomy(model_name)
            
            log_warning(f"{model_name}: ìë™ ë‹¤ìš´ë¡œë“œ ë°©ë²•ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
            
        except Exception as e:
            log_error(f"{model_name} ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def _download_from_huggingface(self, model_name: str, config: Dict) -> bool:
        """HuggingFaceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            from transformers import AutoModel, AutoProcessor, AutoTokenizer
            
            model_path = self.base_dir / config["path"]
            model_path.mkdir(parents=True, exist_ok=True)
            
            hf_id = config["huggingface_id"]
            
            # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ë‹¤ìš´ë¡œë“œ
            if model_name == "segformer_human_parsing":
                from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor
                model = SegformerForSemanticSegmentation.from_pretrained(hf_id)
                processor = SegformerImageProcessor.from_pretrained(hf_id)
                model.save_pretrained(model_path)
                processor.save_pretrained(model_path)
                
            elif model_name.startswith("clip"):
                from transformers import CLIPModel, CLIPProcessor
                model = CLIPModel.from_pretrained(hf_id)
                processor = CLIPProcessor.from_pretrained(hf_id)
                model.save_pretrained(model_path)
                processor.save_pretrained(model_path)
                
            elif model_name == "stable_diffusion_v15":
                from diffusers import StableDiffusionPipeline
                pipeline = StableDiffusionPipeline.from_pretrained(hf_id)
                pipeline.save_pretrained(model_path)
            
            return True
            
        except ImportError as e:
            log_error(f"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ: {e}")
            return False
        except Exception as e:
            log_error(f"HuggingFace ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _download_u2net(self) -> bool:
        """UÂ²-Net ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            import urllib.request
            
            url = "https://github.com/xuebinqin/U-2-Net/raw/master/saved_models/u2net/u2net.pth"
            save_path = self.checkpoints_dir / "step_03_cloth_segmentation/u2net.pth"
            save_path.parent.mkdir(parents=True, exist_ok=True)
            
            log_download("UÂ²-Net ë‹¤ìš´ë¡œë“œ ì¤‘...")
            urllib.request.urlretrieve(url, save_path)
            
            # ONNX ë²„ì „ë„ ë³€í™˜ (ì„ íƒì‚¬í•­)
            self._convert_u2net_to_onnx(save_path)
            
            return True
            
        except Exception as e:
            log_error(f"UÂ²-Net ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _download_mediapipe_pose(self) -> bool:
        """MediaPipe í¬ì¦ˆ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            import urllib.request
            
            urls = {
                "pose_landmarker.task": "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task",
                "pose_landmark_full.tflite": "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/1/pose_landmarker_full.tflite"
            }
            
            save_dir = self.checkpoints_dir / "step_02_pose_estimation"
            save_dir.mkdir(parents=True, exist_ok=True)
            
            for filename, url in urls.items():
                save_path = save_dir / filename
                log_download(f"MediaPipe {filename} ë‹¤ìš´ë¡œë“œ ì¤‘...")
                urllib.request.urlretrieve(url, save_path)
            
            return True
            
        except Exception as e:
            log_error(f"MediaPipe ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _download_graphonomy(self, model_name: str) -> bool:
        """Graphonomy ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìˆ˜ë™ ì•ˆë‚´)"""
        log_warning(f"{model_name}: ìˆ˜ë™ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤")
        log_info("Graphonomy ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë°©ë²•:")
        log_info("1. https://github.com/Gaoyiminggithub/Graphonomy ë°©ë¬¸")
        log_info("2. pre-trained ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
        log_info(f"3. ai_models/checkpoints/step_01_human_parsing/ í´ë”ì— ì €ì¥")
        return False
    
    def _convert_u2net_to_onnx(self, pytorch_path: Path):
        """UÂ²-Net PyTorchë¥¼ ONNXë¡œ ë³€í™˜"""
        try:
            import torch
            import torch.onnx
            
            # ê°„ë‹¨í•œ UÂ²-Net ëª¨ë¸ ë¡œë“œ ë° ë³€í™˜ ë¡œì§
            # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” UÂ²-Net ëª¨ë¸ êµ¬ì¡°ê°€ í•„ìš”)
            log_info("UÂ²-Net ONNX ë³€í™˜ì€ ë³„ë„ë¡œ ì§„í–‰í•˜ì„¸ìš”")
            
        except Exception as e:
            log_warning(f"ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
    
    def generate_install_script(self, results: Dict) -> str:
        """ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        script_lines = [
            "#!/bin/bash",
            "# AI ëª¨ë¸ ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸",
            "",
            "echo 'ğŸš€ MyCloset AI ëª¨ë¸ ì„¤ì¹˜ ì‹œì‘'",
            "echo '================================'",
            "",
            "# Python í™˜ê²½ í™•ì¸",
            "if ! command -v python3 &> /dev/null; then",
            "    echo 'âŒ Python3ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'",
            "    exit 1",
            "fi",
            "",
            "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜",
            "echo 'ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì¤‘...'",
            "pip install transformers diffusers torch torchvision onnxruntime",
            "pip install mediapipe opencv-python pillow",
            "",
            "# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ",
            "echo 'ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...'",
            "python3 -c \"",
            "import sys",
            "sys.path.append('.')",
            "from complete_model_check import AIModelChecker",
            "checker = AIModelChecker()",
            "results = checker.check_all_models()",
            "checker.download_missing_models(results, ['critical', 'high'])",
            "\"",
            "",
            "echo 'âœ… ì„¤ì¹˜ ì™„ë£Œ!'"
        ]
        
        return "\n".join(script_lines)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ” MyCloset AI ëª¨ë¸ ì™„ì „ ì²´í¬")
    print("=" * 50)
    
    checker = AIModelChecker()
    
    # 1. ëª¨ë“  ëª¨ë¸ ì²´í¬
    results = checker.check_all_models()
    
    # 2. ê²°ê³¼ ìš”ì•½
    print("\nğŸ“Š ì²´í¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 30)
    print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(results['available'])}ê°œ")
    print(f"ğŸ”´ ì¹˜ëª…ì  ëˆ„ë½: {len(results['critical_missing'])}ê°œ")
    print(f"ğŸŸ¡ ë†’ì€ ìš°ì„ ìˆœìœ„ ëˆ„ë½: {len(results['high_missing'])}ê°œ")
    print(f"ğŸŸ¢ ì¤‘ê°„ ìš°ì„ ìˆœìœ„ ëˆ„ë½: {len(results['medium_missing'])}ê°œ")
    print(f"ğŸ“¦ ì´ ëˆ„ë½ ìš©ëŸ‰: {results['total_missing_size_gb']:.1f} GB")
    
    # 3. ëˆ„ë½ëœ ëª¨ë¸ ìƒì„¸ ì •ë³´
    if results['critical_missing'] or results['high_missing']:
        print("\nğŸš¨ ìš°ì„  ë‹¤ìš´ë¡œë“œ í•„ìš”í•œ ëª¨ë¸ë“¤:")
        for model in results['critical_missing'] + results['high_missing']:
            print(f"   - {model['description']} ({model['size_mb']} MB)")
            if model['has_huggingface']:
                print(f"     ğŸ“¥ ìë™ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥")
            else:
                print(f"     âš ï¸  ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ í•„ìš”")
    
    # 4. ìë™ ë‹¤ìš´ë¡œë“œ ì œì•ˆ
    if results['critical_missing'] or results['high_missing']:
        print(f"\nğŸ¤– ìë™ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ", end="")
        if input().lower() == 'y':
            checker.download_missing_models(results, ['critical', 'high'])
        else:
            log_info("ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
            log_info("python3 complete_model_check.py --download")
    
    # 5. ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    install_script = checker.generate_install_script(results)
    with open("install_ai_models.sh", "w") as f:
        f.write(install_script)
    os.chmod("install_ai_models.sh", 0o755)
    log_success("ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±ë¨: install_ai_models.sh")
    
    # 6. ìµœì¢… ì•ˆë‚´
    print("\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
    if results['critical_missing']:
        print("1. ì¹˜ëª…ì  ëª¨ë¸ë“¤ì„ ë¨¼ì € ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”")
        print("2. ./install_ai_models.sh ì‹¤í–‰")
    print("3. python3 scripts/test/test_final_models.py ë¡œ í…ŒìŠ¤íŠ¸")
    print("4. ì„œë²„ ì‹œì‘: python3 run_server.py")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="AI ëª¨ë¸ ì²´í¬ ë° ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--download", action="store_true", help="ìë™ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰")
    parser.add_argument("--priority", choices=["critical", "high", "medium", "low"], 
                       default="high", help="ë‹¤ìš´ë¡œë“œ ìš°ì„ ìˆœìœ„")
    
    args = parser.parse_args()
    
    if args.download:
        checker = AIModelChecker()
        results = checker.check_all_models()
        priorities = ["critical", args.priority] if args.priority != "critical" else ["critical"]
        checker.download_missing_models(results, priorities)
    else:
        main()