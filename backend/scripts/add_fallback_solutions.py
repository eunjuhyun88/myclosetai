# backend/scripts/add_fallback_solutions.py
"""
ğŸ”„ MyCloset AI - ëŒ€ì²´ ì†”ë£¨ì…˜ í™•ì¥ ê°€ì´ë“œ v1.0
âœ… ìƒˆë¡œìš´ ëŒ€ì²´ ì†”ë£¨ì…˜ì„ ê³„ì† ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œ
âœ… ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ ì†”ë£¨ì…˜ ì§€ì›
âœ… ìë™ ê°€ìš©ì„± ì²´í¬ ë° ë“±ë¡
âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ í¬í•¨

ì‚¬ìš©ë²•:
1. ì´ íŒŒì¼ì„ ì°¸ê³ í•´ì„œ ìƒˆë¡œìš´ ì†”ë£¨ì…˜ êµ¬í˜„
2. register_new_solution() í•¨ìˆ˜ë¡œ ë“±ë¡
3. ìë™ìœ¼ë¡œ PipelineManagerì—ì„œ ì‚¬ìš© ê°€ëŠ¥

í™•ì¥ ê°€ëŠ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤:
- TensorFlow/Keras ëª¨ë¸
- ONNX Runtime ëª¨ë¸  
- Dlib ì–¼êµ´ ì²˜ë¦¬
- ImageIO ê¸°ë³¸ ì²˜ë¦¬
- Matplotlib ì‹œê°í™”
- í´ë¼ìš°ë“œ API (OpenAI, Replicate ë“±)
- ì›¹ ê¸°ë°˜ ì²˜ë¦¬ ì„œë¹„ìŠ¤
"""

import os
import sys
import logging
import time
import asyncio
import json
from pathlib import Path
from typing import Dict, Any, List, Callable, Optional, Union, Tuple
from dataclasses import dataclass
import inspect

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = Path(__file__).parent.parent
sys.path.insert(0, str(current_dir))

# MyCloset AI ì‹œìŠ¤í…œ import
try:
    from app.ai_pipeline.pipeline_manager import (
        register_fallback_solution,
        get_fallback_registry,
        FallbackSolutionRegistry
    )
    from app.ai_pipeline.utils.auto_model_detector import (
        EnhancedModelDetector,
        create_enhanced_detector
    )
    MYCLOSET_AVAILABLE = True
except ImportError:
    MYCLOSET_AVAILABLE = False
    print("âš ï¸ MyCloset AI ì‹œìŠ¤í…œì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”„ 1. TensorFlow/Keras ê¸°ë°˜ ì†”ë£¨ì…˜ë“¤
# ==============================================

def add_tensorflow_solutions():
    """TensorFlow/Keras ê¸°ë°˜ ëŒ€ì²´ ì†”ë£¨ì…˜ë“¤ ì¶”ê°€"""
    
    def tensorflow_image_classification(image, **kwargs):
        """TensorFlow ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜"""
        try:
            import tensorflow as tf
            import numpy as np
            
            # ì‚¬ì „ í›ˆë ¨ëœ MobileNetV2 ì‚¬ìš© (ê²½ëŸ‰)
            model = tf.keras.applications.MobileNetV2(
                weights='imagenet',
                include_top=True,
                input_shape=(224, 224, 3)
            )
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if hasattr(image, 'shape'):
                if len(image.shape) == 3:
                    # í¬ê¸° ì¡°ì •
                    resized = tf.image.resize(image, [224, 224])
                    preprocessed = tf.keras.applications.mobilenet_v2.preprocess_input(resized)
                    batch_input = tf.expand_dims(preprocessed, 0)
                    
                    # ì˜ˆì¸¡
                    predictions = model.predict(batch_input, verbose=0)
                    decoded = tf.keras.applications.mobilenet_v2.decode_predictions(predictions, top=3)[0]
                    
                    # ì˜ë¥˜ ê´€ë ¨ í´ë˜ìŠ¤ í•„í„°ë§
                    clothing_classes = ['jersey', 'sweatshirt', 'cardigan', 'suit', 'dress', 'skirt']
                    clothing_predictions = [pred for pred in decoded 
                                          if any(cls in pred[1].lower() for cls in clothing_classes)]
                    
                    return {
                        "success": True,
                        "predictions": [{"class": pred[1], "confidence": float(pred[2])} 
                                      for pred in clothing_predictions],
                        "all_predictions": [{"class": pred[1], "confidence": float(pred[2])} 
                                          for pred in decoded],
                        "confidence": max([float(pred[2]) for pred in decoded]) if decoded else 0.0,
                        "method": "tensorflow_mobilenetv2"
                    }
            
            return {"success": False, "error": "Invalid image format"}
            
        except ImportError:
            return {"success": False, "error": "TensorFlow not available"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def tensorflow_semantic_segmentation(image, **kwargs):
        """TensorFlow ê¸°ë°˜ ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜"""
        try:
            import tensorflow as tf
            import numpy as np
            
            # DeepLab v3+ ëª¨ë¸ (ì‚¬ì „ í›ˆë ¨ë¨)
            # ì‹¤ì œë¡œëŠ” TensorFlow Hubì—ì„œ ë¡œë“œí•´ì•¼ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ êµ¬í˜„
            
            # ê°„ë‹¨í•œ U-Net ìŠ¤íƒ€ì¼ ì„¸ê·¸ë©˜í…Œì´ì…˜
            if hasattr(image, 'shape'):
                height, width = image.shape[:2]
                
                # ìƒ‰ìƒ ê¸°ë°˜ ê°„ë‹¨ ì„¸ê·¸ë©˜í…Œì´ì…˜
                hsv = tf.image.rgb_to_hsv(tf.cast(image, tf.float32) / 255.0)
                
                # ì‚¬ëŒ/ì˜ë¥˜ ì˜ì—­ ì¶”ì • (ìƒ‰ìƒ ê¸°ë°˜)
                h, s, v = tf.split(hsv, 3, axis=-1)
                
                # í”¼ë¶€ìƒ‰ ë²”ìœ„ (ëŒ€ëµì )
                skin_mask = tf.logical_and(
                    tf.logical_and(h >= 0.0, h <= 0.1),
                    tf.logical_and(s >= 0.2, s <= 0.8)
                )
                
                # ì˜ë¥˜ ì˜ì—­ (í”¼ë¶€ìƒ‰ì´ ì•„ë‹Œ ì˜ì—­)
                cloth_mask = tf.logical_not(skin_mask)
                
                return {
                    "success": True,
                    "segmentation_mask": tf.cast(cloth_mask, tf.uint8).numpy() * 255,
                    "skin_mask": tf.cast(skin_mask, tf.uint8).numpy() * 255,
                    "confidence": 0.6,
                    "method": "tensorflow_color_segmentation"
                }
            
            return {"success": False, "error": "Invalid image format"}
            
        except ImportError:
            return {"success": False, "error": "TensorFlow not available"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    # ì†”ë£¨ì…˜ë“¤ ë“±ë¡
    if MYCLOSET_AVAILABLE:
        register_fallback_solution(
            "tensorflow_image_classification",
            tensorflow_image_classification,
            "quality_assessment",
            confidence=0.75,
            requirements=["tensorflow"],
            description="TensorFlow MobileNetV2 ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜"
        )
        
        register_fallback_solution(
            "tensorflow_semantic_segmentation",
            tensorflow_semantic_segmentation,
            "cloth_segmentation",
            confidence=0.6,
            requirements=["tensorflow"],
            description="TensorFlow ê¸°ë°˜ ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜"
        )

# ==============================================
# ğŸ”„ 2. ONNX Runtime ê¸°ë°˜ ì†”ë£¨ì…˜ë“¤
# ==============================================

def add_onnx_solutions():
    """ONNX Runtime ê¸°ë°˜ ëŒ€ì²´ ì†”ë£¨ì…˜ë“¤ ì¶”ê°€"""
    
    def onnx_inference_generic(image, model_path: str = None, **kwargs):
        """ONNX ëª¨ë¸ ë²”ìš© ì¶”ë¡ """
        try:
            import onnxruntime as ort
            import numpy as np
            
            if not model_path or not Path(model_path).exists():
                return {"success": False, "error": "ONNX model path not found"}
            
            # ONNX ì„¸ì…˜ ìƒì„±
            session = ort.InferenceSession(model_path)
            
            # ì…ë ¥ ì •ë³´ í™•ì¸
            input_info = session.get_inputs()[0]
            input_name = input_info.name
            input_shape = input_info.shape
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if hasattr(image, 'shape'):
                # ì…ë ¥ í¬ê¸°ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ
                target_height, target_width = input_shape[2], input_shape[3]
                
                # ê°„ë‹¨í•œ ë¦¬ì‚¬ì´ì¦ˆ (OpenCV ì—†ì´)
                from PIL import Image as PILImage
                pil_image = PILImage.fromarray(image)
                resized = pil_image.resize((target_width, target_height))
                
                # ì •ê·œí™”
                input_array = np.array(resized).astype(np.float32) / 255.0
                input_array = input_array.transpose(2, 0, 1)  # HWC -> CHW
                input_array = np.expand_dims(input_array, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                
                # ì¶”ë¡ 
                outputs = session.run(None, {input_name: input_array})
                
                return {
                    "success": True,
                    "raw_output": outputs[0] if outputs else None,
                    "output_shape": outputs[0].shape if outputs else None,
                    "confidence": 0.7,
                    "method": "onnx_generic_inference"
                }
            
            return {"success": False, "error": "Invalid image format"}
            
        except ImportError:
            return {"success": False, "error": "ONNX Runtime not available"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def onnx_face_detection(image, **kwargs):
        """ONNX ê¸°ë°˜ ì–¼êµ´ ê°ì§€ (ê°€ìƒì˜ ëª¨ë¸)"""
        try:
            import onnxruntime as ort
            import numpy as np
            
            # ì‹¤ì œë¡œëŠ” ì–¼êµ´ ê°ì§€ ONNX ëª¨ë¸ì„ ë¡œë“œí•´ì•¼ í•¨
            # ì—¬ê¸°ì„œëŠ” ì¤‘ì•™ ì˜ì—­ì„ ì–¼êµ´ë¡œ ê°€ì •í•˜ëŠ” mock êµ¬í˜„
            
            if hasattr(image, 'shape'):
                height, width = image.shape[:2]
                
                # ìƒë‹¨ 1/3 ì˜ì—­ì„ ì–¼êµ´ ì˜ì—­ìœ¼ë¡œ ê°€ì •
                face_y1 = 0
                face_y2 = height // 3
                face_x1 = width // 4
                face_x2 = 3 * width // 4
                
                face_boxes = [{
                    "x1": face_x1,
                    "y1": face_y1,
                    "x2": face_x2,
                    "y2": face_y2,
                    "confidence": 0.6
                }]
                
                return {
                    "success": True,
                    "faces": face_boxes,
                    "face_count": len(face_boxes),
                    "confidence": 0.6,
                    "method": "onnx_mock_face_detection"
                }
            
            return {"success": False, "error": "Invalid image format"}
            
        except ImportError:
            return {"success": False, "error": "ONNX Runtime not available"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    # ì†”ë£¨ì…˜ë“¤ ë“±ë¡
    if MYCLOSET_AVAILABLE:
        register_fallback_solution(
            "onnx_inference_generic",
            onnx_inference_generic,
            "virtual_fitting",
            confidence=0.7,
            requirements=["onnxruntime", "pillow"],
            description="ONNX Runtime ë²”ìš© ëª¨ë¸ ì¶”ë¡ "
        )
        
        register_fallback_solution(
            "onnx_face_detection",
            onnx_face_detection,
            "human_parsing",
            confidence=0.6,
            requirements=["onnxruntime"],
            description="ONNX ê¸°ë°˜ ì–¼êµ´ ê°ì§€"
        )

# ==============================================
# ğŸ”„ 3. Dlib ê¸°ë°˜ ì†”ë£¨ì…˜ë“¤
# ==============================================

def add_dlib_solutions():
    """Dlib ê¸°ë°˜ ëŒ€ì²´ ì†”ë£¨ì…˜ë“¤ ì¶”ê°€"""
    
    def dlib_face_landmarks(image, **kwargs):
        """Dlib ê¸°ë°˜ ì–¼êµ´ ëœë“œë§ˆí¬ ê²€ì¶œ"""
        try:
            import dlib
            import numpy as np
            
            # Dlib ê°ì§€ê¸° ì´ˆê¸°í™”
            detector = dlib.get_frontal_face_detector()
            
            # 68ì  ëœë“œë§ˆí¬ ì˜ˆì¸¡ê¸° (ë‹¤ìš´ë¡œë“œ í•„ìš”)
            predictor_path = kwargs.get("predictor_path", "shape_predictor_68_face_landmarks.dat")
            
            if Path(predictor_path).exists():
                predictor = dlib.shape_predictor(predictor_path)
            else:
                # ì˜ˆì¸¡ê¸° ì—†ìœ¼ë©´ ê°ì§€ë§Œ ìˆ˜í–‰
                predictor = None
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2).astype(np.uint8)
            else:
                gray = image
            
            # ì–¼êµ´ ê°ì§€
            faces = detector(gray)
            
            results = []
            for face in faces:
                face_info = {
                    "x": face.left(),
                    "y": face.top(),
                    "width": face.width(),
                    "height": face.height(),
                    "confidence": 1.0  # Dlibì€ ì‹ ë¢°ë„ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ
                }
                
                # ëœë“œë§ˆí¬ ê²€ì¶œ (ì˜ˆì¸¡ê¸°ê°€ ìˆëŠ” ê²½ìš°)
                if predictor:
                    landmarks = predictor(gray, face)
                    face_info["landmarks"] = [
                        {"x": landmarks.part(i).x, "y": landmarks.part(i).y}
                        for i in range(68)
                    ]
                
                results.append(face_info)
            
            return {
                "success": True,
                "faces": results,
                "face_count": len(results),
                "confidence": 0.8,
                "method": "dlib_face_landmarks"
            }
            
        except ImportError:
            return {"success": False, "error": "Dlib not available"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def dlib_shape_analysis(image, **kwargs):
        """Dlib ê¸°ë°˜ í˜•íƒœ ë¶„ì„"""
        try:
            import dlib
            import numpy as np
            
            # HOG ê¸°ë°˜ ê°ì²´ ê°ì§€ê¸° í•™ìŠµ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
            # ì‹¤ì œë¡œëŠ” ì˜ë¥˜ í˜•íƒœ ê°ì§€ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ ëª¨ë¸ì´ í•„ìš”
            
            if hasattr(image, 'shape'):
                height, width = image.shape[:2]
                
                # ê°„ë‹¨í•œ í˜•íƒœ ë¶„ì„ (ì—ì§€ ê¸°ë°˜)
                if len(image.shape) == 3:
                    gray = np.mean(image, axis=2).astype(np.uint8)
                else:
                    gray = image
                
                # ê°„ë‹¨í•œ ì—ì§€ ê²€ì¶œ
                edges = np.zeros_like(gray)
                for i in range(1, height-1):
                    for j in range(1, width-1):
                        gx = gray[i-1:i+2, j-1:j+2] * np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
                        gy = gray[i-1:i+2, j-1:j+2] * np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])
                        edges[i, j] = min(255, int(np.sqrt(np.sum(gx)**2 + np.sum(gy)**2)))
                
                # í˜•íƒœ íŠ¹ì§• ì¶”ì¶œ
                edge_density = np.sum(edges > 50) / edges.size
                
                return {
                    "success": True,
                    "edge_map": edges,
                    "edge_density": edge_density,
                    "shape_features": {
                        "edge_density": edge_density,
                        "avg_edge_strength": np.mean(edges),
                        "max_edge_strength": np.max(edges)
                    },
                    "confidence": 0.5,
                    "method": "dlib_shape_analysis"
                }
            
            return {"success": False, "error": "Invalid image format"}
            
        except ImportError:
            return {"success": False, "error": "Dlib not available"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    # ì†”ë£¨ì…˜ë“¤ ë“±ë¡
    if MYCLOSET_AVAILABLE:
        register_fallback_solution(
            "dlib_face_landmarks",
            dlib_face_landmarks,
            "human_parsing",
            confidence=0.8,
            requirements=["dlib"],
            description="Dlib ê¸°ë°˜ ì–¼êµ´ ëœë“œë§ˆí¬ ê²€ì¶œ"
        )
        
        register_fallback_solution(
            "dlib_shape_analysis",
            dlib_shape_analysis,
            "quality_assessment",
            confidence=0.5,
            requirements=["dlib"],
            description="Dlib ê¸°ë°˜ í˜•íƒœ ë¶„ì„"
        )

# ==============================================
# ğŸ”„ 4. í´ë¼ìš°ë“œ API ê¸°ë°˜ ì†”ë£¨ì…˜ë“¤
# ==============================================

def add_cloud_api_solutions():
    """í´ë¼ìš°ë“œ API ê¸°ë°˜ ëŒ€ì²´ ì†”ë£¨ì…˜ë“¤ ì¶”ê°€"""
    
    def openai_vision_analysis(image, api_key: str = None, **kwargs):
        """OpenAI GPT-4 Vision ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            import requests
            import base64
            import io
            from PIL import Image as PILImage
            
            if not api_key:
                api_key = os.getenv("OPENAI_API_KEY")
            
            if not api_key:
                return {"success": False, "error": "OpenAI API key required"}
            
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            if hasattr(image, 'shape'):
                pil_image = PILImage.fromarray(image)
            else:
                pil_image = image
            
            # ì´ë¯¸ì§€ í¬ê¸° ìµœì í™” (API ì œí•œ ê³ ë ¤)
            pil_image.thumbnail((512, 512), PILImage.Resampling.LANCZOS)
            
            buffer = io.BytesIO()
            pil_image.save(buffer, format="JPEG", quality=85)
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            
            # OpenAI API ìš”ì²­
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            prompt = kwargs.get("prompt", 
                "Analyze this fashion image. Describe the clothing items, colors, style, and suggest improvements.")
            
            payload = {
                "model": "gpt-4-vision-preview",
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}"
                                }
                            }
                        ]
                    }
                ],
                "max_tokens": 500
            }
            
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                analysis = result["choices"][0]["message"]["content"]
                
                return {
                    "success": True,
                    "analysis": analysis,
                    "confidence": 0.9,
                    "method": "openai_gpt4_vision",
                    "usage": result.get("usage", {})
                }
            else:
                return {
                    "success": False,
                    "error": f"OpenAI API error: {response.status_code}",
                    "details": response.text
                }
                
        except ImportError:
            return {"success": False, "error": "Requests library not available"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def replicate_ai_processing(image, model_name: str = None, **kwargs):
        """Replicate.com AI ëª¨ë¸ ê¸°ë°˜ ì²˜ë¦¬"""
        try:
            import requests
            import base64
            import io
            import time
            from PIL import Image as PILImage
            
            api_token = kwargs.get("api_token") or os.getenv("REPLICATE_API_TOKEN")
            if not api_token:
                return {"success": False, "error": "Replicate API token required"}
            
            # ê¸°ë³¸ ëª¨ë¸ (ì˜ˆ: ë°°ê²½ ì œê±° ëª¨ë¸)
            if not model_name:
                model_name = "cjwbw/rembg:fb8af171cfa1616ddcf1242c093f9c46bcada5ad4cf6f2fbe8b81b330ec5c003"
            
            # ì´ë¯¸ì§€ ì¤€ë¹„
            if hasattr(image, 'shape'):
                pil_image = PILImage.fromarray(image)
            else:
                pil_image = image
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (ReplicateëŠ” URL í•„ìš”)
            buffer = io.BytesIO()
            pil_image.save(buffer, format="PNG")
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            
            # Replicate API ìš”ì²­
            headers = {
                "Authorization": f"Token {api_token}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "version": model_name.split(":")[-1],
                "input": {
                    "image": f"data:image/png;base64,{image_base64}"
                }
            }
            
            # ì˜ˆì¸¡ ì‹œì‘
            response = requests.post(
                f"https://api.replicate.com/v1/predictions",
                headers=headers,
                json=payload,
                timeout=10
            )
            
            if response.status_code == 201:
                prediction = response.json()
                prediction_id = prediction["id"]
                
                # ê²°ê³¼ ëŒ€ê¸° (ê°„ë‹¨í•œ í´ë§)
                for _ in range(30):  # ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°
                    status_response = requests.get(
                        f"https://api.replicate.com/v1/predictions/{prediction_id}",
                        headers=headers,
                        timeout=5
                    )
                    
                    if status_response.status_code == 200:
                        status_data = status_response.json()
                        
                        if status_data["status"] == "succeeded":
                            return {
                                "success": True,
                                "result_url": status_data["output"],
                                "confidence": 0.8,
                                "method": "replicate_ai",
                                "model": model_name
                            }
                        elif status_data["status"] == "failed":
                            return {
                                "success": False,
                                "error": "Replicate processing failed",
                                "details": status_data.get("error")
                            }
                    
                    time.sleep(1)
                
                return {"success": False, "error": "Replicate processing timeout"}
            else:
                return {
                    "success": False,
                    "error": f"Replicate API error: {response.status_code}",
                    "details": response.text
                }
                
        except ImportError:
            return {"success": False, "error": "Requests library not available"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    # ì†”ë£¨ì…˜ë“¤ ë“±ë¡
    if MYCLOSET_AVAILABLE:
        register_fallback_solution(
            "openai_vision_analysis",
            openai_vision_analysis,
            "quality_assessment",
            confidence=0.9,
            requirements=["requests", "pillow"],
            description="OpenAI GPT-4 Vision ê¸°ë°˜ íŒ¨ì…˜ ì´ë¯¸ì§€ ë¶„ì„"
        )
        
        register_fallback_solution(
            "replicate_ai_processing",
            replicate_ai_processing,
            "cloth_segmentation",
            confidence=0.8,
            requirements=["requests", "pillow"],
            description="Replicate.com AI ëª¨ë¸ ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬"
        )

# ==============================================
# ğŸ”„ 5. ì›¹ ê¸°ë°˜ ì²˜ë¦¬ ì„œë¹„ìŠ¤ë“¤
# ==============================================

def add_web_service_solutions():
    """ì›¹ ê¸°ë°˜ ì²˜ë¦¬ ì„œë¹„ìŠ¤ë“¤ ì¶”ê°€"""
    
    def remove_bg_api(image, **kwargs):
        """Remove.bg API ê¸°ë°˜ ë°°ê²½ ì œê±°"""
        try:
            import requests
            import io
            from PIL import Image as PILImage
            
            api_key = kwargs.get("api_key") or os.getenv("REMOVE_BG_API_KEY")
            if not api_key:
                return {"success": False, "error": "Remove.bg API key required"}
            
            # ì´ë¯¸ì§€ ì¤€ë¹„
            if hasattr(image, 'shape'):
                pil_image = PILImage.fromarray(image)
            else:
                pil_image = image
            
            buffer = io.BytesIO()
            pil_image.save(buffer, format="PNG")
            buffer.seek(0)
            
            # Remove.bg API ìš”ì²­
            response = requests.post(
                "https://api.remove.bg/v1.0/removebg",
                files={"image_file": buffer},
                data={"size": "auto"},
                headers={"X-Api-Key": api_key},
                timeout=30
            )
            
            if response.status_code == 200:
                # ê²°ê³¼ ì´ë¯¸ì§€ ì²˜ë¦¬
                result_image = PILImage.open(io.BytesIO(response.content))
                result_array = np.array(result_image)
                
                # ì•ŒíŒŒ ì±„ë„ì„ ë§ˆìŠ¤í¬ë¡œ ë³€í™˜
                if result_array.shape[2] == 4:
                    mask = result_array[:, :, 3]
                    rgb_image = result_array[:, :, :3]
                else:
                    mask = np.ones(result_array.shape[:2], dtype=np.uint8) * 255
                    rgb_image = result_array
                
                return {
                    "success": True,
                    "segmented_image": rgb_image,
                    "segmentation_mask": mask,
                    "confidence": 0.9,
                    "method": "remove_bg_api"
                }
            else:
                return {
                    "success": False,
                    "error": f"Remove.bg API error: {response.status_code}",
                    "details": response.text
                }
                
        except ImportError:
            return {"success": False, "error": "Requests library not available"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def unsplash_style_reference(query: str = "fashion", **kwargs):
        """Unsplash API ê¸°ë°˜ ìŠ¤íƒ€ì¼ ì°¸ì¡° ì´ë¯¸ì§€ ê²€ìƒ‰"""
        try:
            import requests
            
            api_key = kwargs.get("api_key") or os.getenv("UNSPLASH_ACCESS_KEY")
            if not api_key:
                return {"success": False, "error": "Unsplash API key required"}
            
            # Unsplash API ìš”ì²­
            headers = {"Authorization": f"Client-ID {api_key}"}
            params = {
                "query": query,
                "per_page": 5,
                "orientation": "portrait"
            }
            
            response = requests.get(
                "https://api.unsplash.com/search/photos",
                headers=headers,
                params=params,
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                
                style_references = []
                for photo in data.get("results", []):
                    style_references.append({
                        "url": photo["urls"]["regular"],
                        "thumbnail": photo["urls"]["small"],
                        "description": photo.get("description", ""),
                        "tags": photo.get("tags", []),
                        "photographer": photo["user"]["name"]
                    })
                
                return {
                    "success": True,
                    "style_references": style_references,
                    "total_results": data.get("total", 0),
                    "confidence": 0.7,
                    "method": "unsplash_style_reference"
                }
            else:
                return {
                    "success": False,
                    "error": f"Unsplash API error: {response.status_code}",
                    "details": response.text
                }
                
        except ImportError:
            return {"success": False, "error": "Requests library not available"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    # ì†”ë£¨ì…˜ë“¤ ë“±ë¡
    if MYCLOSET_AVAILABLE:
        register_fallback_solution(
            "remove_bg_api",
            remove_bg_api,
            "cloth_segmentation",
            confidence=0.9,
            requirements=["requests", "pillow"],
            description="Remove.bg API ê¸°ë°˜ ë°°ê²½ ì œê±°"
        )
        
        register_fallback_solution(
            "unsplash_style_reference",
            unsplash_style_reference,
            "quality_assessment",
            confidence=0.7,
            requirements=["requests"],
            description="Unsplash API ê¸°ë°˜ ìŠ¤íƒ€ì¼ ì°¸ì¡° ê²€ìƒ‰"
        )

# ==============================================
# ğŸ”„ 6. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë° í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ
# ==============================================

class FallbackSolutionBenchmark:
    """ëŒ€ì²´ ì†”ë£¨ì…˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹"""
    
    def __init__(self):
        self.results = {}
        self.logger = logging.getLogger(f"{__name__}.Benchmark")
    
    def benchmark_solution(self, solution_name: str, test_image_path: str, iterations: int = 3):
        """ì†”ë£¨ì…˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        try:
            from PIL import Image
            import numpy as np
            
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ
            test_image = np.array(Image.open(test_image_path))
            
            # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            times = []
            success_count = 0
            confidence_scores = []
            
            registry = get_fallback_registry()
            
            for i in range(iterations):
                start_time = time.time()
                
                result = registry.execute_solution(solution_name, test_image)
                
                execution_time = time.time() - start_time
                times.append(execution_time)
                
                if result.get("success", False):
                    success_count += 1
                    confidence = result.get("confidence", 0.0)
                    confidence_scores.append(confidence)
            
            # ê²°ê³¼ ê³„ì‚°
            avg_time = np.mean(times) if times else 0
            success_rate = success_count / iterations
            avg_confidence = np.mean(confidence_scores) if confidence_scores else 0
            
            benchmark_result = {
                "solution_name": solution_name,
                "avg_execution_time": avg_time,
                "success_rate": success_rate,
                "avg_confidence": avg_confidence,
                "iterations": iterations,
                "individual_times": times,
                "confidence_scores": confidence_scores,
                "timestamp": time.time()
            }
            
            self.results[solution_name] = benchmark_result
            
            self.logger.info(f"âœ… {solution_name} ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")
            self.logger.info(f"  í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_time:.3f}ì´ˆ")
            self.logger.info(f"  ì„±ê³µë¥ : {success_rate:.1%}")
            self.logger.info(f"  í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
            
            return benchmark_result
            
        except Exception as e:
            self.logger.error(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨ {solution_name}: {e}")
            return {"error": str(e)}
    
    def benchmark_all_solutions(self, test_image_path: str):
        """ëª¨ë“  ë“±ë¡ëœ ì†”ë£¨ì…˜ ë²¤ì¹˜ë§ˆí¬"""
        try:
            registry = get_fallback_registry()
            
            for solution_name in registry.solutions.keys():
                self.benchmark_solution(solution_name, test_image_path)
            
            return self.get_benchmark_summary()
            
        except Exception as e:
            self.logger.error(f"ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def get_benchmark_summary(self):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½"""
        try:
            if not self.results:
                return {"message": "No benchmark results available"}
            
            summary = {
                "total_solutions": len(self.results),
                "fastest_solution": None,
                "most_reliable_solution": None,
                "highest_confidence_solution": None,
                "detailed_results": self.results
            }
            
            # ê°€ì¥ ë¹ ë¥¸ ì†”ë£¨ì…˜
            fastest = min(self.results.items(), key=lambda x: x[1].get("avg_execution_time", float('inf')))
            summary["fastest_solution"] = {
                "name": fastest[0],
                "avg_time": fastest[1]["avg_execution_time"]
            }
            
            # ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†”ë£¨ì…˜
            most_reliable = max(self.results.items(), key=lambda x: x[1].get("success_rate", 0))
            summary["most_reliable_solution"] = {
                "name": most_reliable[0],
                "success_rate": most_reliable[1]["success_rate"]
            }
            
            # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ ì†”ë£¨ì…˜
            highest_confidence = max(self.results.items(), key=lambda x: x[1].get("avg_confidence", 0))
            summary["highest_confidence_solution"] = {
                "name": highest_confidence[0],
                "avg_confidence": highest_confidence[1]["avg_confidence"]
            }
            
            return summary
            
        except Exception as e:
            return {"error": str(e)}

# ==============================================
# ğŸ”„ 7. ìƒˆë¡œìš´ ì†”ë£¨ì…˜ ì¶”ê°€ í…œí”Œë¦¿
# ==============================================

def create_custom_solution_template():
    """ìƒˆë¡œìš´ ëŒ€ì²´ ì†”ë£¨ì…˜ ì¶”ê°€ í…œí”Œë¦¿"""
    
    template_code = '''
def my_custom_solution(image, **kwargs):
    """
    ì»¤ìŠ¤í…€ ëŒ€ì²´ ì†”ë£¨ì…˜ í…œí”Œë¦¿
    
    Args:
        image: ì…ë ¥ ì´ë¯¸ì§€ (numpy array ë˜ëŠ” PIL Image)
        **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°ë“¤
    
    Returns:
        Dict[str, Any]: ì²˜ë¦¬ ê²°ê³¼
        {
            "success": bool,           # ì„±ê³µ ì—¬ë¶€
            "result": Any,            # ì£¼ìš” ê²°ê³¼
            "confidence": float,      # ì‹ ë¢°ë„ (0.0-1.0)
            "method": str,           # ì‚¬ìš©ëœ ë°©ë²•
            "error": str             # ì˜¤ë¥˜ ë©”ì‹œì§€ (ì‹¤íŒ¨ì‹œ)
        }
    """
    try:
        # 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
        import numpy as np
        # import your_library
        
        # 2. ì…ë ¥ ê²€ì¦
        if not hasattr(image, 'shape'):
            return {"success": False, "error": "Invalid image format"}
        
        # 3. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        if len(image.shape) == 3:
            # RGB ì´ë¯¸ì§€ ì²˜ë¦¬
            height, width, channels = image.shape
        else:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬
            height, width = image.shape
            channels = 1
        
        # 4. ì‹¤ì œ ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
        # TODO: ì—¬ê¸°ì— ì‹¤ì œ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
        
        # ì˜ˆì‹œ: ê°„ë‹¨í•œ ì´ë¯¸ì§€ ì²˜ë¦¬
        processed_image = image.copy()
        
        # 5. ê²°ê³¼ ë°˜í™˜
        return {
            "success": True,
            "processed_image": processed_image,
            "confidence": 0.7,  # ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ ì‹ ë¢°ë„ ê³„ì‚°
            "method": "my_custom_algorithm",
            "metadata": {
                "input_size": (height, width),
                "processing_time": 0.1  # ì‹¤ì œ ì¸¡ì • ê¶Œì¥
            }
        }
        
    except ImportError as e:
        return {"success": False, "error": f"Required library not available: {e}"}
    except Exception as e:
        return {"success": False, "error": str(e)}

# ì†”ë£¨ì…˜ ë“±ë¡
register_fallback_solution(
    "my_custom_solution",                    # ê³ ìœ í•œ ì´ë¦„
    my_custom_solution,                      # í•¨ìˆ˜ ê°ì²´
    "cloth_segmentation",                    # Step ì¹´í…Œê³ ë¦¬
    confidence=0.7,                          # ì˜ˆìƒ ì‹ ë¢°ë„
    requirements=["numpy"],                  # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
    description="ë‚˜ë§Œì˜ ì»¤ìŠ¤í…€ ì´ë¯¸ì§€ ì²˜ë¦¬ ì†”ë£¨ì…˜"  # ì„¤ëª…
)
'''
    
    return template_code

# ==============================================
# ğŸ”„ 8. ë©”ì¸ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ==============================================

def register_all_fallback_solutions():
    """ëª¨ë“  ëŒ€ì²´ ì†”ë£¨ì…˜ë“¤ì„ ë“±ë¡"""
    print("ğŸ”„ ëŒ€ì²´ ì†”ë£¨ì…˜ë“¤ ë“±ë¡ ì‹œì‘...")
    
    try:
        add_tensorflow_solutions()
        print("âœ… TensorFlow ì†”ë£¨ì…˜ë“¤ ë“±ë¡ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ TensorFlow ì†”ë£¨ì…˜ ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    try:
        add_onnx_solutions()
        print("âœ… ONNX ì†”ë£¨ì…˜ë“¤ ë“±ë¡ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ ONNX ì†”ë£¨ì…˜ ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    try:
        add_dlib_solutions()
        print("âœ… Dlib ì†”ë£¨ì…˜ë“¤ ë“±ë¡ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ Dlib ì†”ë£¨ì…˜ ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    try:
        add_cloud_api_solutions()
        print("âœ… í´ë¼ìš°ë“œ API ì†”ë£¨ì…˜ë“¤ ë“±ë¡ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ í´ë¼ìš°ë“œ API ì†”ë£¨ì…˜ ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    try:
        add_web_service_solutions()
        print("âœ… ì›¹ ì„œë¹„ìŠ¤ ì†”ë£¨ì…˜ë“¤ ë“±ë¡ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ ì›¹ ì„œë¹„ìŠ¤ ì†”ë£¨ì…˜ ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    print("ğŸ‰ ëª¨ë“  ëŒ€ì²´ ì†”ë£¨ì…˜ ë“±ë¡ ì™„ë£Œ!")

def list_available_solutions():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ€ì²´ ì†”ë£¨ì…˜ë“¤ ì¡°íšŒ"""
    if not MYCLOSET_AVAILABLE:
        print("âŒ MyCloset AI ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    registry = get_fallback_registry()
    solutions = registry.solutions
    
    print(f"\nğŸ“‹ ë“±ë¡ëœ ëŒ€ì²´ ì†”ë£¨ì…˜ë“¤ ({len(solutions)}ê°œ)")
    print("=" * 60)
    
    # Step ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™”
    by_category = {}
    for name, info in solutions.items():
        category = info["step_category"]
        if category not in by_category:
            by_category[category] = []
        by_category[category].append((name, info))
    
    for category, solution_list in by_category.items():
        print(f"\nğŸ”¸ {category.upper()}")
        for name, info in solution_list:
            status = "âœ…" if info["is_available"] else "âŒ"
            confidence = info["confidence"]
            description = info["description"]
            requirements = ", ".join(info["requirements"])
            
            print(f"  {status} {name}")
            print(f"     ì‹ ë¢°ë„: {confidence:.2f} | ìš”êµ¬ì‚¬í•­: {requirements}")
            print(f"     ì„¤ëª…: {description}")

def test_solution(solution_name: str, test_image_path: str = None):
    """íŠ¹ì • ì†”ë£¨ì…˜ í…ŒìŠ¤íŠ¸"""
    if not MYCLOSET_AVAILABLE:
        print("âŒ MyCloset AI ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    registry = get_fallback_registry()
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± (ì‹¤ì œ ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°)
    if not test_image_path:
        import numpy as np
        test_image = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
        print("ğŸ“¸ ëœë¤ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±")
    else:
        from PIL import Image
        test_image = np.array(Image.open(test_image_path))
        print(f"ğŸ“¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ: {test_image_path}")
    
    print(f"\nğŸ§ª {solution_name} í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    start_time = time.time()
    result = registry.execute_solution(solution_name, test_image)
    execution_time = time.time() - start_time
    
    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
    
    if result.get("success", False):
        print("âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        print(f"ğŸ¯ ì‹ ë¢°ë„: {result.get('confidence', 0):.3f}")
        print(f"ğŸ”§ ë°©ë²•: {result.get('method', 'Unknown')}")
        
        # ê²°ê³¼ í‚¤ë“¤ ì¶œë ¥
        result_keys = [k for k in result.keys() if k not in ['success', 'confidence', 'method']]
        if result_keys:
            print(f"ğŸ“Š ê²°ê³¼ í‚¤ë“¤: {', '.join(result_keys)}")
    else:
        print("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        print(f"ğŸš¨ ì˜¤ë¥˜: {result.get('error', 'Unknown error')}")

def show_usage_guide():
    """ì‚¬ìš©ë²• ê°€ì´ë“œ ì¶œë ¥"""
    guide = """
ğŸ”„ MyCloset AI ëŒ€ì²´ ì†”ë£¨ì…˜ í™•ì¥ ê°€ì´ë“œ

ğŸ“š 1. ìƒˆë¡œìš´ ì†”ë£¨ì…˜ ì¶”ê°€ ë°©ë²•:
   1) ì´ íŒŒì¼ì„ ì°¸ê³ í•´ì„œ í•¨ìˆ˜ êµ¬í˜„
   2) register_fallback_solution()ìœ¼ë¡œ ë“±ë¡
   3) PipelineManagerì—ì„œ ìë™ ì‚¬ìš© ê°€ëŠ¥

ğŸ› ï¸ 2. ì§€ì›ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤:
   - TensorFlow/Keras (ë”¥ëŸ¬ë‹ ëª¨ë¸)
   - ONNX Runtime (ê²½ëŸ‰ ì¶”ë¡ )
   - Dlib (ì–¼êµ´/í˜•íƒœ ì²˜ë¦¬)
   - OpenAI/Replicate (í´ë¼ìš°ë“œ AI)
   - Remove.bg/Unsplash (ì›¹ ì„œë¹„ìŠ¤)
   - ê¸°íƒ€ Python ë¼ì´ë¸ŒëŸ¬ë¦¬

ğŸ§ª 3. í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí‚¹:
   - test_solution(solution_name) : ê°œë³„ í…ŒìŠ¤íŠ¸
   - benchmark_all_solutions() : ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
   - list_available_solutions() : ë“±ë¡ëœ ì†”ë£¨ì…˜ ì¡°íšŒ

ğŸ“ 4. ì†”ë£¨ì…˜ êµ¬í˜„ ê°€ì´ë“œë¼ì¸:
   - ì…ë ¥: image (numpy array ë˜ëŠ” PIL Image)
   - ì¶œë ¥: Dict with success, result, confidence, method
   - ì˜¤ë¥˜ ì²˜ë¦¬: try-exceptë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
   - ìš”êµ¬ì‚¬í•­: requirements ë¦¬ìŠ¤íŠ¸ì— ëª…ì‹œ

ğŸ”— 5. ìë™ ì—°ë™:
   - PipelineManagerê°€ ìë™ìœ¼ë¡œ ê°ì§€
   - Step ì‹¤íŒ¨ì‹œ ìë™ìœ¼ë¡œ ëŒ€ì²´ ì†”ë£¨ì…˜ ì‚¬ìš©
   - ì‹ ë¢°ë„ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ìë™ ê²°ì •

ğŸ’¡ 6. ì¶”ê°€ í™•ì¥ ì˜ˆì‹œ:
   - ìƒˆë¡œìš´ AI ëª¨ë¸ (Hugging Face Hub)
   - ì»¤ìŠ¤í…€ ì•Œê³ ë¦¬ì¦˜ (OpenCV í™•ì¥)
   - ì™¸ë¶€ API ì„œë¹„ìŠ¤
   - í•˜ë“œì›¨ì–´ ê°€ì† (CUDA, Metal)

ì‚¬ìš© ì˜ˆì‹œ:
  python add_fallback_solutions.py --register-all
  python add_fallback_solutions.py --list
  python add_fallback_solutions.py --test opencv_segmentation
  python add_fallback_solutions.py --benchmark
"""
    print(guide)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="MyCloset AI ëŒ€ì²´ ì†”ë£¨ì…˜ ê´€ë¦¬")
    parser.add_argument("--register-all", action="store_true", help="ëª¨ë“  ì†”ë£¨ì…˜ ë“±ë¡")
    parser.add_argument("--list", action="store_true", help="ë“±ë¡ëœ ì†”ë£¨ì…˜ ëª©ë¡")
    parser.add_argument("--test", type=str, help="íŠ¹ì • ì†”ë£¨ì…˜ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--test-image", type=str, help="í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ ê²½ë¡œ")
    parser.add_argument("--benchmark", action="store_true", help="ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    parser.add_argument("--template", action="store_true", help="ì»¤ìŠ¤í…€ ì†”ë£¨ì…˜ í…œí”Œë¦¿ ì¶œë ¥")
    parser.add_argument("--help-guide", action="store_true", help="ìƒì„¸ ì‚¬ìš©ë²• ê°€ì´ë“œ")
    
    args = parser.parse_args()
    
    if args.help_guide:
        show_usage_guide()
    elif args.register_all:
        register_all_fallback_solutions()
    elif args.list:
        list_available_solutions()
    elif args.test:
        test_solution(args.test, args.test_image)
    elif args.benchmark:
        benchmark = FallbackSolutionBenchmark()
        if args.test_image:
            summary = benchmark.benchmark_all_solutions(args.test_image)
            print(f"\nğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½:")
            print(json.dumps(summary, indent=2, ensure_ascii=False))
        else:
            print("âŒ ë²¤ì¹˜ë§ˆí¬ë¥¼ ìœ„í•´ --test-image ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    elif args.template:
        template = create_custom_solution_template()
        print("ğŸ“ ì»¤ìŠ¤í…€ ì†”ë£¨ì…˜ í…œí”Œë¦¿:")
        print(template)
    else:
        print("ğŸ”„ MyCloset AI ëŒ€ì²´ ì†”ë£¨ì…˜ í™•ì¥ ì‹œìŠ¤í…œ")
        print("ì‚¬ìš©ë²•: python add_fallback_solutions.py --help")
        print("ìƒì„¸ ê°€ì´ë“œ: python add_fallback_solutions.py --help-guide")

print("\nâœ… ëŒ€ì²´ ì†”ë£¨ì…˜ í™•ì¥ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
print("ğŸ’¡ ìƒˆë¡œìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ APIë¥¼ ì´ìš©í•´ì„œ ê³„ì† ì†”ë£¨ì…˜ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
print("ğŸ“š --help-guide ì˜µì…˜ìœ¼ë¡œ ìƒì„¸í•œ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.")