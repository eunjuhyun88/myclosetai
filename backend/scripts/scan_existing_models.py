#!/usr/bin/env python3
"""
ğŸ” ê¸°ì¡´ AI ëª¨ë¸ ìŠ¤ìº” ë° ì„¤ì • ìƒì„±ê¸°
í˜„ì¬ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ë“¤ì„ ì¸ì‹í•˜ê³  ìš°ë¦¬ ModelLoaderì™€ ì—°ë™
"""

import os
import json
import yaml
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ExistingModelScanner:
    """ê¸°ì¡´ AI ëª¨ë¸ ìŠ¤ìº” ë° ì„¤ì •"""
    
    def __init__(self, ai_models_path: str = "ai_models"):
        self.ai_models_dir = Path(ai_models_path)
        self.scanned_models = {}
        self.model_configs = {}
        
        logger.info(f"ğŸ“ AI ëª¨ë¸ ë””ë ‰í† ë¦¬: {self.ai_models_dir.absolute()}")
        
        if not self.ai_models_dir.exists():
            logger.error(f"âŒ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.ai_models_dir}")
            raise FileNotFoundError(f"AI ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.ai_models_dir}")
    
    def scan_all_models(self) -> Dict[str, Any]:
        """ëª¨ë“  AI ëª¨ë¸ ìŠ¤ìº”"""
        logger.info("ğŸ” ê¸°ì¡´ AI ëª¨ë¸ ìŠ¤ìº” ì‹œì‘...")
        
        # í˜„ì¬ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì¶œë ¥
        self._show_directory_structure()
        
        # ê° ëª¨ë¸ë³„ ìŠ¤ìº”
        self._scan_ootdiffusion()
        self._scan_hr_viton()
        self._scan_graphonomy()
        self._scan_openpose()
        self._scan_detectron2()
        self._scan_self_correction_parsing()
        self._scan_checkpoints()
        self._scan_additional_files()
        
        # ìŠ¤ìº” ê²°ê³¼ ìš”ì•½
        self._show_scan_summary()
        
        return self.scanned_models
    
    def _show_directory_structure(self):
        """í˜„ì¬ ë””ë ‰í† ë¦¬ êµ¬ì¡° í‘œì‹œ"""
        logger.info("ğŸ“‚ í˜„ì¬ AI ëª¨ë¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°:")
        logger.info("=" * 50)
        
        for item in sorted(self.ai_models_dir.iterdir()):
            if item.is_dir():
                file_count = len(list(item.rglob("*")))
                size_mb = self._get_directory_size(item)
                logger.info(f"ğŸ“ {item.name}/ ({file_count} í•­ëª©, {size_mb} MB)")
            else:
                size_mb = round(item.stat().st_size / (1024 * 1024), 1)
                logger.info(f"ğŸ“„ {item.name} ({size_mb} MB)")
    
    def _scan_ootdiffusion(self):
        """OOTDiffusion ëª¨ë¸ ìŠ¤ìº”"""
        possible_dirs = ["OOTDiffusion", "oot_diffusion", "ootdiffusion"]
        
        for dir_name in possible_dirs:
            model_dir = self.ai_models_dir / dir_name
            if model_dir.exists():
                logger.info(f"âœ… OOTDiffusion ë°œê²¬: {dir_name}")
                
                # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸°
                checkpoints = self._find_checkpoints(model_dir, [
                    "*.bin", "*.pth", "*.ckpt", "pytorch_model.bin"
                ])
                
                # ì„¤ì • íŒŒì¼ ì°¾ê¸°
                config_files = self._find_configs(model_dir, [
                    "config.json", "model_index.json", "*.yaml", "*.yml"
                ])
                
                self.scanned_models["ootdiffusion"] = {
                    "name": "OOTDiffusion",
                    "type": "diffusion",
                    "step": "step_06_virtual_fitting",
                    "path": str(model_dir),
                    "checkpoints": checkpoints,
                    "configs": config_files,
                    "size_mb": self._get_directory_size(model_dir),
                    "files_count": len(list(model_dir.rglob("*"))),
                    "ready": len(checkpoints) > 0,
                    "priority": 1  # ìµœìš°ì„ 
                }
                break
    
    def _scan_hr_viton(self):
        """HR-VITON ëª¨ë¸ ìŠ¤ìº”"""
        model_dir = self.ai_models_dir / "HR-VITON"
        if model_dir.exists():
            logger.info(f"âœ… HR-VITON ë°œê²¬")
            
            # ì„œë¸Œëª¨ë¸ë“¤ ì°¾ê¸°
            submodels = {
                "gmm": self._find_checkpoints(model_dir, ["*gmm*.pth", "*GMM*.pth"]),
                "tom": self._find_checkpoints(model_dir, ["*tom*.pth", "*TOM*.pth"]),
                "full": self._find_checkpoints(model_dir, ["*final*.pth", "*complete*.pth"])
            }
            
            self.scanned_models["hr_viton"] = {
                "name": "HR-VITON",
                "type": "virtual_tryon",
                "step": "step_05_cloth_warping",
                "path": str(model_dir),
                "submodels": submodels,
                "size_mb": self._get_directory_size(model_dir),
                "files_count": len(list(model_dir.rglob("*"))),
                "ready": any(len(files) > 0 for files in submodels.values()),
                "priority": 2
            }
    
    def _scan_graphonomy(self):
        """Graphonomy ëª¨ë¸ ìŠ¤ìº”"""
        model_dir = self.ai_models_dir / "Graphonomy"
        if model_dir.exists():
            logger.info(f"âœ… Graphonomy ë°œê²¬")
            
            checkpoints = self._find_checkpoints(model_dir, [
                "*.pth", "*.pt", "*inference*.pth", "*final*.pth"
            ])
            
            self.scanned_models["graphonomy"] = {
                "name": "Graphonomy",
                "type": "human_parsing",
                "step": "step_01_human_parsing",
                "path": str(model_dir),
                "checkpoints": checkpoints,
                "size_mb": self._get_directory_size(model_dir),
                "files_count": len(list(model_dir.rglob("*"))),
                "ready": len(checkpoints) > 0,
                "priority": 3
            }
    
    def _scan_openpose(self):
        """OpenPose ëª¨ë¸ ìŠ¤ìº”"""
        model_dir = self.ai_models_dir / "openpose"
        if model_dir.exists():
            logger.info(f"âœ… OpenPose ë°œê²¬")
            
            # Caffe ëª¨ë¸ ì°¾ê¸°
            caffe_models = self._find_checkpoints(model_dir, [
                "*.caffemodel", "*.prototxt"
            ])
            
            # PyTorch ëª¨ë¸ ì°¾ê¸°
            pytorch_models = self._find_checkpoints(model_dir, [
                "*.pth", "*.pt"
            ])
            
            self.scanned_models["openpose"] = {
                "name": "OpenPose",
                "type": "pose_estimation",
                "step": "step_02_pose_estimation",
                "path": str(model_dir),
                "caffe_models": caffe_models,
                "pytorch_models": pytorch_models,
                "size_mb": self._get_directory_size(model_dir),
                "files_count": len(list(model_dir.rglob("*"))),
                "ready": len(caffe_models) > 0 or len(pytorch_models) > 0,
                "priority": 4
            }
    
    def _scan_detectron2(self):
        """Detectron2 ëª¨ë¸ ìŠ¤ìº”"""
        model_dir = self.ai_models_dir / "detectron2"
        if model_dir.exists():
            logger.info(f"âœ… Detectron2 ë°œê²¬")
            
            # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ë“¤ ì°¾ê¸°
            models = self._find_checkpoints(model_dir, [
                "*.pkl", "*.pth", "*.pt"
            ])
            
            self.scanned_models["detectron2"] = {
                "name": "Detectron2",
                "type": "detection_segmentation",
                "step": "auxiliary",
                "path": str(model_dir),
                "models": models,
                "size_mb": self._get_directory_size(model_dir),
                "files_count": len(list(model_dir.rglob("*"))),
                "ready": len(models) > 0,
                "priority": 5
            }
    
    def _scan_self_correction_parsing(self):
        """Self-Correction Human Parsing ëª¨ë¸ ìŠ¤ìº”"""
        model_dir = self.ai_models_dir / "Self-Correction-Human-Parsing"
        if model_dir.exists():
            logger.info(f"âœ… Self-Correction Human Parsing ë°œê²¬")
            
            checkpoints = self._find_checkpoints(model_dir, [
                "*.pth", "*.pt", "*parsing*.pth"
            ])
            
            self.scanned_models["self_correction_parsing"] = {
                "name": "Self-Correction Human Parsing",
                "type": "human_parsing",
                "step": "step_01_human_parsing",
                "path": str(model_dir),
                "checkpoints": checkpoints,
                "size_mb": self._get_directory_size(model_dir),
                "files_count": len(list(model_dir.rglob("*"))),
                "ready": len(checkpoints) > 0,
                "priority": 6
            }
    
    def _scan_checkpoints(self):
        """checkpoints ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
        checkpoints_dir = self.ai_models_dir / "checkpoints"
        if checkpoints_dir.exists():
            logger.info(f"âœ… checkpoints ë””ë ‰í† ë¦¬ ë°œê²¬")
            
            # ê° ì„œë¸Œë””ë ‰í† ë¦¬ ìŠ¤ìº”
            checkpoint_models = {}
            for subdir in checkpoints_dir.iterdir():
                if subdir.is_dir():
                    files = self._find_checkpoints(subdir, [
                        "*.pth", "*.pt", "*.bin", "*.ckpt", "*.pkl", "*.caffemodel"
                    ])
                    if files:
                        checkpoint_models[subdir.name] = {
                            "path": str(subdir),
                            "files": files,
                            "size_mb": self._get_directory_size(subdir)
                        }
            
            if checkpoint_models:
                self.scanned_models["checkpoints"] = {
                    "name": "Additional Checkpoints",
                    "type": "mixed",
                    "step": "auxiliary",
                    "path": str(checkpoints_dir),
                    "models": checkpoint_models,
                    "size_mb": self._get_directory_size(checkpoints_dir),
                    "files_count": len(list(checkpoints_dir.rglob("*"))),
                    "ready": True,
                    "priority": 7
                }
    
    def _scan_additional_files(self):
        """ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì˜ ì¶”ê°€ ëª¨ë¸ íŒŒì¼ë“¤"""
        additional_files = {}
        
        # ì•Œë ¤ì§„ ëª¨ë¸ íŒŒì¼ íŒ¨í„´ë“¤
        patterns = ["*.pth", "*.pt", "*.bin", "*.ckpt", "*.pkl"]
        
        for pattern in patterns:
            for file_path in self.ai_models_dir.glob(pattern):
                if file_path.is_file():
                    size_mb = round(file_path.stat().st_size / (1024 * 1024), 1)
                    additional_files[file_path.name] = {
                        "path": str(file_path),
                        "size_mb": size_mb
                    }
        
        if additional_files:
            logger.info(f"âœ… ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì¶”ê°€ íŒŒì¼ë“¤ ë°œê²¬: {len(additional_files)}ê°œ")
            
            self.scanned_models["additional_files"] = {
                "name": "Additional Model Files",
                "type": "mixed",
                "step": "auxiliary",
                "path": str(self.ai_models_dir),
                "files": additional_files,
                "ready": True,
                "priority": 8
            }
    
    def _find_checkpoints(self, directory: Path, patterns: List[str]) -> List[str]:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ ì°¾ê¸°"""
        found_files = []
        for pattern in patterns:
            for file_path in directory.rglob(pattern):
                if file_path.is_file():
                    found_files.append(str(file_path.relative_to(directory)))
        return found_files
    
    def _find_configs(self, directory: Path, patterns: List[str]) -> List[str]:
        """ì„¤ì • íŒŒì¼ë“¤ ì°¾ê¸°"""
        return self._find_checkpoints(directory, patterns)
    
    def _get_directory_size(self, directory: Path) -> float:
        """ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚° (MB)"""
        total_size = 0
        try:
            for file_path in directory.rglob("*"):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
        except:
            pass
        return round(total_size / (1024 * 1024), 1)
    
    def _show_scan_summary(self):
        """ìŠ¤ìº” ê²°ê³¼ ìš”ì•½"""
        logger.info("\nğŸ“Š ìŠ¤ìº” ê²°ê³¼ ìš”ì•½:")
        logger.info("=" * 60)
        
        total_models = len(self.scanned_models)
        ready_models = sum(1 for model in self.scanned_models.values() if model.get("ready", False))
        total_size = sum(model.get("size_mb", 0) for model in self.scanned_models.values())
        
        logger.info(f"ğŸ“¦ ë°œê²¬ëœ ëª¨ë¸: {total_models}ê°œ")
        logger.info(f"âœ… ì‚¬ìš© ê°€ëŠ¥: {ready_models}ê°œ")
        logger.info(f"ğŸ’¾ ì´ í¬ê¸°: {total_size:.1f} MB ({total_size/1024:.1f} GB)")
        
        logger.info(f"\nğŸ“‹ ë°œê²¬ëœ ëª¨ë¸ ëª©ë¡:")
        for model_key, model_info in self.scanned_models.items():
            status = "âœ…" if model_info.get("ready", False) else "âš ï¸"
            name = model_info.get("name", model_key)
            size = model_info.get("size_mb", 0)
            step = model_info.get("step", "unknown")
            logger.info(f"   {status} {name} ({size:.1f} MB) - {step}")
    
    def create_model_paths_config(self):
        """app/core/model_paths.py ìƒì„±"""
        logger.info("ğŸ Python ëª¨ë¸ ê²½ë¡œ ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")
        
        config_content = '''# app/core/model_paths.py
"""
AI ëª¨ë¸ ê²½ë¡œ ì„¤ì • - ìë™ ìƒì„±ë¨
ê¸°ì¡´ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ë“¤ì˜ ì‹¤ì œ ê²½ë¡œ ë§¤í•‘
"""

from pathlib import Path
from typing import Dict, Optional, List

# ê¸°ë³¸ ê²½ë¡œ
AI_MODELS_ROOT = Path(__file__).parent.parent.parent / "ai_models"

# ìŠ¤ìº”ëœ ëª¨ë¸ ì •ë³´
SCANNED_MODELS = {
'''
        
        # ìŠ¤ìº”ëœ ëª¨ë¸ë“¤ ì¶”ê°€
        for model_key, model_info in self.scanned_models.items():
            config_content += f'''    "{model_key}": {{
        "name": "{model_info['name']}",
        "type": "{model_info['type']}",
        "step": "{model_info['step']}",
        "path": AI_MODELS_ROOT / "{Path(model_info['path']).name}",
        "ready": {model_info['ready']},
        "size_mb": {model_info['size_mb']},
        "priority": {model_info.get('priority', 99)}
    }},
'''
        
        config_content += '''}

# ë‹¨ê³„ë³„ ëª¨ë¸ ë§¤í•‘
STEP_TO_MODELS = {
    "step_01_human_parsing": ["graphonomy", "self_correction_parsing"],
    "step_02_pose_estimation": ["openpose"],
    "step_03_cloth_segmentation": [],  # U2Net ë“± ì¶”ê°€ í•„ìš”
    "step_04_geometric_matching": [],  # HR-VITON GMM
    "step_05_cloth_warping": ["hr_viton"],  # HR-VITON TOM
    "step_06_virtual_fitting": ["ootdiffusion", "hr_viton"],
    "step_07_post_processing": [],
    "step_08_quality_assessment": []
}

def get_model_path(model_key: str) -> Optional[Path]:
    """ëª¨ë¸ ê²½ë¡œ ë°˜í™˜"""
    if model_key in SCANNED_MODELS:
        return SCANNED_MODELS[model_key]["path"]
    return None

def is_model_ready(model_key: str) -> bool:
    """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    if model_key in SCANNED_MODELS:
        model_info = SCANNED_MODELS[model_key]
        return model_info["ready"] and model_info["path"].exists()
    return False

def get_ready_models() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
    return [key for key, info in SCANNED_MODELS.items() if info["ready"]]

def get_models_for_step(step: str) -> List[str]:
    """íŠ¹ì • ë‹¨ê³„ì— ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤"""
    available_models = []
    for model_key in STEP_TO_MODELS.get(step, []):
        if is_model_ready(model_key):
            available_models.append(model_key)
    return available_models

def get_primary_model_for_step(step: str) -> Optional[str]:
    """ë‹¨ê³„ë³„ ì£¼ìš” ëª¨ë¸ ë°˜í™˜ (ìš°ì„ ìˆœìœ„ ê¸°ì¤€)"""
    models = get_models_for_step(step)
    if not models:
        return None
    
    # ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
    models_with_priority = [(model, SCANNED_MODELS[model]["priority"]) for model in models]
    models_with_priority.sort(key=lambda x: x[1])
    
    return models_with_priority[0][0] if models_with_priority else None

def get_ootdiffusion_path() -> Optional[Path]:
    """OOTDiffusion ê²½ë¡œ ë°˜í™˜"""
    return get_model_path("ootdiffusion")

def get_hr_viton_path() -> Optional[Path]:
    """HR-VITON ê²½ë¡œ ë°˜í™˜"""
    return get_model_path("hr_viton")

def get_graphonomy_path() -> Optional[Path]:
    """Graphonomy ê²½ë¡œ ë°˜í™˜"""
    return get_model_path("graphonomy")

def get_openpose_path() -> Optional[Path]:
    """OpenPose ê²½ë¡œ ë°˜í™˜"""
    return get_model_path("openpose")

def get_model_info(model_key: str) -> Optional[Dict]:
    """ëª¨ë¸ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
    return SCANNED_MODELS.get(model_key)

def list_all_models() -> Dict[str, Dict]:
    """ëª¨ë“  ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
    return SCANNED_MODELS.copy()
'''
        
        # app/core ë””ë ‰í† ë¦¬ ìƒì„±
        app_core_dir = Path("app/core")
        app_core_dir.mkdir(parents=True, exist_ok=True)
        
        config_path = app_core_dir / "model_paths.py"
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        logger.info(f"âœ… Python ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
    
    def create_yaml_config(self):
        """YAML ì„¤ì • íŒŒì¼ ìƒì„±"""
        logger.info("ğŸ“ YAML ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")
        
        config = {
            "version": "1.0",
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "ai_models_directory": str(self.ai_models_dir.absolute()),
            "scanned_models": self.scanned_models,
            "model_loader_config": {
                "device": "mps",  # M3 Max ê¸°ë³¸ê°’
                "use_fp16": True,
                "max_cached_models": 8,
                "lazy_loading": True,
                "optimization_enabled": True
            },
            "pipeline_config": {
                "default_image_size": [512, 512],
                "batch_size": 1,
                "quality_level": "balanced"
            }
        }
        
        # YAML íŒŒì¼ ì €ì¥
        config_path = self.ai_models_dir / "scanned_models_config.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        logger.info(f"âœ… YAML ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
    
    def create_json_summary(self):
        """JSON ìš”ì•½ íŒŒì¼ ìƒì„±"""
        logger.info("ğŸ“‹ JSON ìš”ì•½ íŒŒì¼ ìƒì„± ì¤‘...")
        
        summary = {
            "scan_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_models": len(self.scanned_models),
            "ready_models": sum(1 for model in self.scanned_models.values() if model.get("ready", False)),
            "total_size_mb": sum(model.get("size_mb", 0) for model in self.scanned_models.values()),
            "models": {}
        }
        
        # ê°„ë‹¨í•œ ëª¨ë¸ ì •ë³´ë§Œ
        for model_key, model_info in self.scanned_models.items():
            summary["models"][model_key] = {
                "name": model_info["name"],
                "type": model_info["type"],
                "ready": model_info["ready"],
                "size_mb": model_info["size_mb"],
                "path": model_info["path"]
            }
        
        # JSON íŒŒì¼ ì €ì¥
        summary_path = self.ai_models_dir / "model_scan_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… JSON ìš”ì•½ íŒŒì¼ ìƒì„±: {summary_path}")
    
    def update_model_loader_registry(self):
        """ModelLoaderìš© ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸ íŒŒì¼ ìƒì„±"""
        logger.info("ğŸ”§ ModelLoader ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸ íŒŒì¼ ìƒì„± ì¤‘...")
        
        registry_update = '''# app/ai_pipeline/utils/model_registry_update.py
"""
ìŠ¤ìº”ëœ ëª¨ë¸ë“¤ì„ ModelLoaderì— ìë™ ë“±ë¡
"""

from app.ai_pipeline.utils.model_loader import ModelConfig, ModelType
from pathlib import Path

def update_model_registry(model_loader):
    """ìŠ¤ìº”ëœ ëª¨ë¸ë“¤ì„ ModelLoaderì— ë“±ë¡"""
    
    # ê¸°ë³¸ ê²½ë¡œ
    ai_models_root = Path("ai_models")
    
    # ìŠ¤ìº”ëœ ëª¨ë¸ë“¤ ë“±ë¡
'''
        
        # ê° ëª¨ë¸ë³„ ë“±ë¡ ì½”ë“œ ìƒì„±
        for model_key, model_info in self.scanned_models.items():
            if not model_info.get("ready", False):
                continue
                
            model_type = self._map_to_model_type(model_info["type"])
            if model_type:
                registry_update += f'''
    # {model_info["name"]}
    model_loader.register_model(
        "{model_key}",
        ModelConfig(
            name="{model_info['name']}",
            model_type=ModelType.{model_type},
            model_class="{self._get_model_class(model_info['type'])}",
            checkpoint_path=str(ai_models_root / "{Path(model_info['path']).name}"),
            input_size=(512, 512),
            device="mps"
        )
    )'''
        
        registry_update += '''

def get_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return ['''
        
        for model_key, model_info in self.scanned_models.items():
            if model_info.get("ready", False):
                registry_update += f'"{model_key}", '
        
        registry_update += ''']

# ì‚¬ìš© ì˜ˆì‹œ:
# from app.ai_pipeline.utils.model_loader import ModelLoader
# from app.ai_pipeline.utils.model_registry_update import update_model_registry
# 
# loader = ModelLoader()
# update_model_registry(loader)
# model = await loader.load_model("ootdiffusion")
'''
        
        # íŒŒì¼ ì €ì¥
        registry_path = Path("app/ai_pipeline/utils/model_registry_update.py")
        registry_path.parent.mkdir(parents=True, exist_ok=True)
        with open(registry_path, 'w', encoding='utf-8') as f:
            f.write(registry_update)
        
        logger.info(f"âœ… ModelLoader ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸ íŒŒì¼ ìƒì„±: {registry_path}")
    
    def _map_to_model_type(self, scan_type: str) -> Optional[str]:
        """ìŠ¤ìº” íƒ€ì…ì„ ModelTypeìœ¼ë¡œ ë§¤í•‘"""
        mapping = {
            "diffusion": "DIFFUSION",
            "virtual_tryon": "VIRTUAL_FITTING",
            "human_parsing": "HUMAN_PARSING",
            "pose_estimation": "POSE_ESTIMATION",
            "detection_segmentation": "SEGMENTATION"
        }
        return mapping.get(scan_type)
    
    def _get_model_class(self, scan_type: str) -> str:
        """ìŠ¤ìº” íƒ€ì…ì—ì„œ ëª¨ë¸ í´ë˜ìŠ¤ëª… ì¶”ì¶œ"""
        mapping = {
            "diffusion": "StableDiffusionPipeline",
            "virtual_tryon": "HRVITONModel",
            "human_parsing": "GraphonomyModel",
            "pose_estimation": "OpenPoseModel",
            "detection_segmentation": "DetectronModel"
        }
        return mapping.get(scan_type, "BaseModel")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ” MyCloset AI - ê¸°ì¡´ ëª¨ë¸ ìŠ¤ìº” ë° ì„¤ì • ìƒì„±")
    print("=" * 50)
    
    try:
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ai_models ì°¾ê¸°
        scanner = ExistingModelScanner("ai_models")
        
        # ëª¨ë¸ ìŠ¤ìº”
        scanned_models = scanner.scan_all_models()
        
        if not scanned_models:
            logger.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ì„¤ì • íŒŒì¼ë“¤ ìƒì„±
        scanner.create_model_paths_config()
        scanner.create_yaml_config()
        scanner.create_json_summary()
        scanner.update_model_loader_registry()
        
        print(f"\nğŸ‰ ëª¨ë¸ ìŠ¤ìº” ë° ì„¤ì • ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ {len(scanned_models)}ê°œ ëª¨ë¸ ë°œê²¬")
        print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {sum(1 for m in scanned_models.values() if m.get('ready', False))}ê°œ")
        
        # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
        print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        print(f"   - app/core/model_paths.py (Python ì„¤ì •)")
        print(f"   - ai_models/scanned_models_config.yaml (YAML ì„¤ì •)")
        print(f"   - ai_models/model_scan_summary.json (JSON ìš”ì•½)")
        print(f"   - app/ai_pipeline/utils/model_registry_update.py (ModelLoader ì—°ë™)")
        
        print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"   1. app/core/model_paths.pyë¥¼ import í•˜ì—¬ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©")
        print(f"   2. ModelLoaderì—ì„œ update_model_registry() í•¨ìˆ˜ í˜¸ì¶œ")
        print(f"   3. ê° stepì—ì„œ get_primary_model_for_step() ì‚¬ìš©")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)