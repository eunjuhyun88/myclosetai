#!/usr/bin/env python3
"""
PyTorch 2.1.2 MPS Ìò∏ÌôòÏÑ± ÏàòÏ†ï Ìå®Ïπò
M3 Max ÌôòÍ≤ΩÏóêÏÑú torch.backends.mps.empty_cache() Ïò§Î•ò Ìï¥Í≤∞
"""

import os
import sys
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any
import re

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('pytorch_mps_fix.log')
    ]
)
logger = logging.getLogger(__name__)

class MPSCompatibilityFixer:
    """PyTorch 2.1.2 MPS Ìò∏ÌôòÏÑ± ÏàòÏ†ïÍ∏∞"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path(".")
        self.backend_root = self.project_root / "backend"
        self.fixed_files = []
        self.backup_files = []
        
        # ÏàòÏ†ïÌï† ÌååÏùºÎì§ (Ïö∞ÏÑ†ÏàúÏúÑ Ïàú)
        self.target_files = [
            "app/core/gpu_config.py",
            "app/services/model_manager.py", 
            "app/ai_pipeline/utils/memory_manager.py",
            "app/ai_pipeline/pipeline_manager.py",
            "app/api/pipeline_routes.py",
            "app/ai_pipeline/steps/step_08_quality_assessment.py"
        ]
        
        # Ìå®Ïπò Ìå®ÌÑ¥Îì§
        self.patch_patterns = [
            # torch.backends.mps.empty_cache() ‚Üí torch.mps.empty_cache()
            (
                r'torch\.backends\.mps\.empty_cache\(\)',
                'torch.mps.empty_cache() if hasattr(torch.mps, "empty_cache") else None'
            ),
            # if hasattr(torch.backends.mps, 'empty_cache') ‚Üí if hasattr(torch.mps, 'empty_cache')
            (
                r'hasattr\(torch\.backends\.mps,\s*[\'"]empty_cache[\'"]?\)',
                'hasattr(torch.mps, "empty_cache")'
            ),
            # torch.backends.mps.is_available() ‚Üí torch.backends.mps.is_available()  (Ïú†ÏßÄ)
            # torch.mps.synchronize() Ï∂îÍ∞Ä ÏßÄÏõê
        ]
        
    def create_backup(self, file_path: Path) -> bool:
        """ÌååÏùº Î∞±ÏóÖ ÏÉùÏÑ±"""
        try:
            backup_path = file_path.with_suffix(f"{file_path.suffix}.backup_mps_fix")
            if backup_path.exists():
                logger.info(f"üîÑ Í∏∞Ï°¥ Î∞±ÏóÖ ÌååÏùº ÎçÆÏñ¥Ïì∞Í∏∞: {backup_path}")
            
            backup_path.write_text(file_path.read_text(encoding='utf-8'))
            self.backup_files.append(str(backup_path))
            logger.info(f"üíæ Î∞±ÏóÖ ÏÉùÏÑ±: {backup_path}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Î∞±ÏóÖ ÏÉùÏÑ± Ïã§Ìå® {file_path}: {e}")
            return False
    
    def generate_mps_compatibility_code(self) -> str:
        """MPS Ìò∏ÌôòÏÑ± ÏΩîÎìú ÏÉùÏÑ±"""
        return '''
def safe_mps_empty_cache():
    """PyTorch 2.1.2 Ìò∏Ìôò MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨"""
    try:
        import torch
        if hasattr(torch.mps, 'empty_cache'):
            torch.mps.empty_cache()
            return True
        elif hasattr(torch.mps, 'synchronize'):
            torch.mps.synchronize()
            return True
        else:
            import gc
            gc.collect()
            return False
    except Exception as e:
        logger.warning(f"MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
        return False
'''
    
    def fix_gpu_config_file(self, file_path: Path) -> bool:
        """gpu_config.py ÏàòÏ†ï"""
        try:
            content = file_path.read_text(encoding='utf-8')
            
            # Í∏∞Ï°¥ ÏûòÎ™ªÎêú Ìå®ÌÑ¥ ÏàòÏ†ï
            fixes = [
                # torch.backends.mps.empty_cache() ÏàòÏ†ï
                (
                    r'torch\.backends\.mps\.empty_cache\(\)',
                    'torch.mps.empty_cache()'
                ),
                # hasattr Ï≤¥ÌÅ¨ ÏàòÏ†ï
                (
                    r'hasattr\(torch\.backends\.mps,\s*[\'"]empty_cache[\'"]?\)',
                    'hasattr(torch.mps, "empty_cache")'
                ),
                # Ìò∏ÌôòÏÑ± Ï≤¥ÌÅ¨ Î°úÏßÅ Í∞úÏÑ†
                (
                    r'elif hasattr\(torch\.backends\.mps, \'empty_cache\'\):.*?torch\.backends\.mps\.empty_cache\(\)',
                    '''elif hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                        result["method"] = "mps_empty_cache"
                        logger.info("‚úÖ torch.mps.empty_cache() Ïã§Ìñâ ÏôÑÎ£å")'''
                )
            ]
            
            modified = False
            for pattern, replacement in fixes:
                if re.search(pattern, content):
                    content = re.sub(pattern, replacement, content)
                    modified = True
            
            if modified:
                file_path.write_text(content, encoding='utf-8')
                logger.info(f"‚úÖ GPU Config ÏàòÏ†ï ÏôÑÎ£å: {file_path}")
                return True
            else:
                logger.info(f"‚ÑπÔ∏è GPU Config ÏàòÏ†ï Î∂àÌïÑÏöî: {file_path}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå GPU Config ÏàòÏ†ï Ïã§Ìå® {file_path}: {e}")
            return False
    
    def fix_model_manager_file(self, file_path: Path) -> bool:
        """model_manager.py ÏàòÏ†ï"""
        try:
            content = file_path.read_text(encoding='utf-8')
            
            # Í∏∞Ï°¥ ÏûòÎ™ªÎêú Ìå®ÌÑ¥Îì§ ÏàòÏ†ï
            fixes = [
                # torch.backends.mps.empty_cache() ÏàòÏ†ï
                (
                    r'torch\.backends\.mps\.empty_cache\(\)',
                    '''if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    elif hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()'''
                ),
                # hasattr Ï≤¥ÌÅ¨ ÏàòÏ†ï
                (
                    r'if hasattr\(torch\.backends\.mps,\s*[\'"]empty_cache[\'"]?\):.*?torch\.backends\.mps\.empty_cache\(\)',
                    '''if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                        logger.info("‚úÖ MPS Ï∫êÏãú Ï†ïÎ¶¨ ÏôÑÎ£å")'''
                ),
                # Ï†ïÎ≥¥ Î°úÍ∑∏ ÏàòÏ†ï
                (
                    r'logger\.info\("‚ÑπÔ∏è MPS empty_cache ÎØ∏ÏßÄÏõê \(PyTorch 2\.5\.1\)"\)',
                    'logger.info("‚ÑπÔ∏è MPS empty_cache ÎØ∏ÏßÄÏõê (PyTorch 2.1.2)")'
                )
            ]
            
            modified = False
            for pattern, replacement in fixes:
                if re.search(pattern, content):
                    content = re.sub(pattern, replacement, content, flags=re.DOTALL)
                    modified = True
            
            if modified:
                file_path.write_text(content, encoding='utf-8')
                logger.info(f"‚úÖ Model Manager ÏàòÏ†ï ÏôÑÎ£å: {file_path}")
                return True
            else:
                logger.info(f"‚ÑπÔ∏è Model Manager ÏàòÏ†ï Î∂àÌïÑÏöî: {file_path}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Model Manager ÏàòÏ†ï Ïã§Ìå® {file_path}: {e}")
            return False
    
    def fix_memory_manager_file(self, file_path: Path) -> bool:
        """memory_manager.py ÏàòÏ†ï"""
        try:
            content = file_path.read_text(encoding='utf-8')
            
            # MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Î∂ÄÎ∂Ñ ÏàòÏ†ï
            fixes = [
                # torch.mps.empty_cache() Ìò∏ÌôòÏÑ± Ï≤¥ÌÅ¨
                (
                    r'if hasattr\(torch\.mps,\s*[\'"]empty_cache[\'"]?\):.*?torch\.mps\.empty_cache\(\)',
                    '''if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    elif hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()'''
                ),
                # ÏòàÏô∏ Ï≤òÎ¶¨ Í∞úÏÑ†
                (
                    r'except:.*?pass',
                    '''except Exception as e:
                    logger.warning(f"MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")'''
                )
            ]
            
            modified = False
            for pattern, replacement in fixes:
                if re.search(pattern, content):
                    content = re.sub(pattern, replacement, content, flags=re.DOTALL)
                    modified = True
            
            if modified:
                file_path.write_text(content, encoding='utf-8')
                logger.info(f"‚úÖ Memory Manager ÏàòÏ†ï ÏôÑÎ£å: {file_path}")
                return True
            else:
                logger.info(f"‚ÑπÔ∏è Memory Manager ÏàòÏ†ï Î∂àÌïÑÏöî: {file_path}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Memory Manager ÏàòÏ†ï Ïã§Ìå® {file_path}: {e}")
            return False
    
    def fix_pipeline_manager_file(self, file_path: Path) -> bool:
        """pipeline_manager.py ÏàòÏ†ï"""
        try:
            content = file_path.read_text(encoding='utf-8')
            
            # torch.mps.empty_cache() ÏßÅÏ†ë Ìò∏Ï∂ú ÏàòÏ†ï
            fixes = [
                (
                    r'torch\.mps\.empty_cache\(\)',
                    '''if hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
            elif hasattr(torch.mps, 'synchronize'):
                torch.mps.synchronize()'''
                ),
                # hasattr Ï≤¥ÌÅ¨ ÏàòÏ†ï
                (
                    r'if hasattr\(torch\.backends,\s*[\'"]mps[\'"]?\).*?torch\.mps\.empty_cache\(\)',
                    '''if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                elif hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()'''
                )
            ]
            
            modified = False
            for pattern, replacement in fixes:
                if re.search(pattern, content):
                    content = re.sub(pattern, replacement, content, flags=re.DOTALL)
                    modified = True
            
            if modified:
                file_path.write_text(content, encoding='utf-8')
                logger.info(f"‚úÖ Pipeline Manager ÏàòÏ†ï ÏôÑÎ£å: {file_path}")
                return True
            else:
                logger.info(f"‚ÑπÔ∏è Pipeline Manager ÏàòÏ†ï Î∂àÌïÑÏöî: {file_path}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Pipeline Manager ÏàòÏ†ï Ïã§Ìå® {file_path}: {e}")
            return False
    
    def fix_pipeline_routes_file(self, file_path: Path) -> bool:
        """pipeline_routes.py ÏàòÏ†ï"""
        try:
            content = file_path.read_text(encoding='utf-8')
            
            # torch.mp Í¥ÄÎ†® ÏàòÏ†ï
            fixes = [
                (
                    r'torch\.mp',
                    'torch.mps'
                ),
                # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Î°úÏßÅ Í∞úÏÑ†
                (
                    r'if pipeline\.device == \'mps\'.*?torch\.backends\.mps\.is_available\(\):.*?torch\.mp',
                    '''if pipeline.device == 'mps' and torch.backends.mps.is_available():
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                elif hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()'''
                )
            ]
            
            modified = False
            for pattern, replacement in fixes:
                if re.search(pattern, content):
                    content = re.sub(pattern, replacement, content, flags=re.DOTALL)
                    modified = True
            
            if modified:
                file_path.write_text(content, encoding='utf-8')
                logger.info(f"‚úÖ Pipeline Routes ÏàòÏ†ï ÏôÑÎ£å: {file_path}")
                return True
            else:
                logger.info(f"‚ÑπÔ∏è Pipeline Routes ÏàòÏ†ï Î∂àÌïÑÏöî: {file_path}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Pipeline Routes ÏàòÏ†ï Ïã§Ìå® {file_path}: {e}")
            return False
    
    def fix_quality_assessment_file(self, file_path: Path) -> bool:
        """step_08_quality_assessment.py ÏàòÏ†ï"""
        try:
            content = file_path.read_text(encoding='utf-8')
            
            # MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Î∂ÄÎ∂Ñ ÏàòÏ†ï
            fixes = [
                # torch.backends.mps.empty_cache() ÏàòÏ†ï
                (
                    r'if hasattr\(torch\.backends\.mps,\s*[\'"]empty_cache[\'"]?\):.*?torch\.backends\.mps\.empty_cache\(\)',
                    '''if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()'''
                ),
                # torch.mps.synchronize() Ï∂îÍ∞Ä ÏßÄÏõê
                (
                    r'elif hasattr\(torch\.mps,\s*[\'"]synchronize[\'"]?\):.*?torch\.mps\.synchronize\(\)',
                    '''elif hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()'''
                )
            ]
            
            modified = False
            for pattern, replacement in fixes:
                if re.search(pattern, content):
                    content = re.sub(pattern, replacement, content, flags=re.DOTALL)
                    modified = True
            
            if modified:
                file_path.write_text(content, encoding='utf-8')
                logger.info(f"‚úÖ Quality Assessment ÏàòÏ†ï ÏôÑÎ£å: {file_path}")
                return True
            else:
                logger.info(f"‚ÑπÔ∏è Quality Assessment ÏàòÏ†ï Î∂àÌïÑÏöî: {file_path}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Quality Assessment ÏàòÏ†ï Ïã§Ìå® {file_path}: {e}")
            return False
    
    def create_mps_utility_module(self) -> bool:
        """MPS Ïú†Ìã∏Î¶¨Ìã∞ Î™®Îìà ÏÉùÏÑ±"""
        try:
            utils_dir = self.backend_root / "app" / "utils"
            utils_dir.mkdir(exist_ok=True)
            
            mps_utils_path = utils_dir / "mps_utils.py"
            
            mps_utils_content = '''"""
MPS Ïú†Ìã∏Î¶¨Ìã∞ Î™®Îìà - PyTorch 2.1.2 Ìò∏Ìôò
M3 Max ÌôòÍ≤ΩÏóêÏÑú ÏïàÏ†ÑÌïú MPS Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨
"""

import logging
import gc
from typing import Dict, Any, Optional
import torch

logger = logging.getLogger(__name__)

class MPSMemoryManager:
    """M3 Max MPS Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê"""
    
    def __init__(self):
        self.is_available = torch.backends.mps.is_available()
        self.supports_empty_cache = hasattr(torch.mps, 'empty_cache')
        self.supports_synchronize = hasattr(torch.mps, 'synchronize')
        
        logger.info(f"üçé MPS Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ï¥àÍ∏∞Ìôî")
        logger.info(f"   - MPS ÏÇ¨Ïö© Í∞ÄÎä•: {self.is_available}")
        logger.info(f"   - empty_cache ÏßÄÏõê: {self.supports_empty_cache}")
        logger.info(f"   - synchronize ÏßÄÏõê: {self.supports_synchronize}")
    
    def safe_empty_cache(self) -> Dict[str, Any]:
        """ÏïàÏ†ÑÌïú MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨"""
        result = {
            "success": False,
            "method": "none",
            "message": "MPS ÏÇ¨Ïö© Î∂àÍ∞Ä"
        }
        
        if not self.is_available:
            return result
        
        try:
            if self.supports_empty_cache:
                torch.mps.empty_cache()
                result.update({
                    "success": True,
                    "method": "mps_empty_cache",
                    "message": "torch.mps.empty_cache() Ïã§Ìñâ ÏôÑÎ£å"
                })
                logger.info("‚úÖ torch.mps.empty_cache() Ïã§Ìñâ ÏôÑÎ£å")
                
            elif self.supports_synchronize:
                torch.mps.synchronize()
                result.update({
                    "success": True,
                    "method": "mps_synchronize",
                    "message": "torch.mps.synchronize() Ïã§Ìñâ ÏôÑÎ£å"
                })
                logger.info("‚úÖ torch.mps.synchronize() Ïã§Ìñâ ÏôÑÎ£å")
                
            else:
                gc.collect()
                result.update({
                    "success": True,
                    "method": "gc_collect",
                    "message": "Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖòÏúºÎ°ú ÎåÄÏ≤¥"
                })
                logger.info("‚úÖ Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖòÏúºÎ°ú Î©îÎ™®Î¶¨ Ï†ïÎ¶¨")
            
            return result
            
        except Exception as e:
            result.update({
                "success": False,
                "method": "error",
                "message": f"MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}"
            })
            logger.error(f"‚ùå MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            return result
    
    def get_compatibility_info(self) -> Dict[str, Any]:
        """MPS Ìò∏ÌôòÏÑ± Ï†ïÎ≥¥ Ï°∞Ìöå"""
        return {
            "pytorch_version": torch.__version__,
            "mps_available": self.is_available,
            "mps_built": torch.backends.mps.is_built(),
            "empty_cache_support": self.supports_empty_cache,
            "synchronize_support": self.supports_synchronize,
            "recommended_method": (
                "mps_empty_cache" if self.supports_empty_cache 
                else "mps_synchronize" if self.supports_synchronize 
                else "gc_collect"
            )
        }

# Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§
_mps_manager = None

def get_mps_manager() -> MPSMemoryManager:
    """Ï†ÑÏó≠ MPS Í¥ÄÎ¶¨Ïûê Î∞òÌôò"""
    global _mps_manager
    if _mps_manager is None:
        _mps_manager = MPSMemoryManager()
    return _mps_manager

def safe_mps_empty_cache() -> Dict[str, Any]:
    """ÏïàÏ†ÑÌïú MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ (Ìï®ÏàòÌòï Ïù∏ÌÑ∞ÌéòÏù¥Ïä§)"""
    return get_mps_manager().safe_empty_cache()

def get_mps_compatibility_info() -> Dict[str, Any]:
    """MPS Ìò∏ÌôòÏÑ± Ï†ïÎ≥¥ Ï°∞Ìöå (Ìï®ÏàòÌòï Ïù∏ÌÑ∞ÌéòÏù¥Ïä§)"""
    return get_mps_manager().get_compatibility_info()
'''
            
            mps_utils_path.write_text(mps_utils_content, encoding='utf-8')
            logger.info(f"‚úÖ MPS Ïú†Ìã∏Î¶¨Ìã∞ Î™®Îìà ÏÉùÏÑ±: {mps_utils_path}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå MPS Ïú†Ìã∏Î¶¨Ìã∞ Î™®Îìà ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return False
    
    def run_fix(self) -> Dict[str, Any]:
        """Ï†ÑÏ≤¥ ÏàòÏ†ï Ïã§Ìñâ"""
        logger.info("üîß PyTorch 2.1.2 MPS Ìò∏ÌôòÏÑ± ÏàòÏ†ï ÏãúÏûë")
        
        results = {
            "success": False,
            "fixed_files": [],
            "failed_files": [],
            "backup_files": [],
            "total_files": len(self.target_files)
        }
        
        # ÌååÏùºÎ≥Ñ ÏàòÏ†ï Ìï®Ïàò Îß§Ìïë
        fix_functions = {
            "app/core/gpu_config.py": self.fix_gpu_config_file,
            "app/services/model_manager.py": self.fix_model_manager_file,
            "app/ai_pipeline/utils/memory_manager.py": self.fix_memory_manager_file,
            "app/ai_pipeline/pipeline_manager.py": self.fix_pipeline_manager_file,
            "app/api/pipeline_routes.py": self.fix_pipeline_routes_file,
            "app/ai_pipeline/steps/step_08_quality_assessment.py": self.fix_quality_assessment_file
        }
        
        # Í∞Å ÌååÏùº ÏàòÏ†ï
        for file_path_str in self.target_files:
            file_path = self.backend_root / file_path_str
            
            if not file_path.exists():
                logger.warning(f"‚ö†Ô∏è ÌååÏùº ÏóÜÏùå: {file_path}")
                results["failed_files"].append(file_path_str)
                continue
            
            # Î∞±ÏóÖ ÏÉùÏÑ±
            if not self.create_backup(file_path):
                results["failed_files"].append(file_path_str)
                continue
            
            # ÌååÏùº ÏàòÏ†ï
            fix_function = fix_functions.get(file_path_str)
            if fix_function:
                if fix_function(file_path):
                    results["fixed_files"].append(file_path_str)
                else:
                    results["failed_files"].append(file_path_str)
            else:
                logger.warning(f"‚ö†Ô∏è ÏàòÏ†ï Ìï®Ïàò ÏóÜÏùå: {file_path_str}")
                results["failed_files"].append(file_path_str)
        
        # MPS Ïú†Ìã∏Î¶¨Ìã∞ Î™®Îìà ÏÉùÏÑ±
        if self.create_mps_utility_module():
            results["fixed_files"].append("app/utils/mps_utils.py")
        
        # Í≤∞Í≥º ÏßëÍ≥Ñ
        results["backup_files"] = self.backup_files
        results["success"] = len(results["failed_files"]) == 0
        
        logger.info(f"üéâ ÏàòÏ†ï ÏôÑÎ£å: {len(results['fixed_files'])}/{results['total_files']}")
        logger.info(f"‚úÖ ÏÑ±Í≥µ: {results['fixed_files']}")
        if results["failed_files"]:
            logger.warning(f"‚ùå Ïã§Ìå®: {results['failed_files']}")
        
        return results
    
    def rollback(self) -> bool:
        """Î∞±ÏóÖÏóêÏÑú Î°§Î∞±"""
        try:
            logger.info("üîÑ Î°§Î∞± ÏãúÏûë")
            
            for backup_file_str in self.backup_files:
                backup_path = Path(backup_file_str)
                if not backup_path.exists():
                    continue
                
                original_path = backup_path.with_suffix('')
                original_path.write_text(backup_path.read_text(encoding='utf-8'))
                logger.info(f"üîÑ Î°§Î∞±: {original_path}")
            
            logger.info("‚úÖ Î°§Î∞± ÏôÑÎ£å")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Î°§Î∞± Ïã§Ìå®: {e}")
            return False

def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    try:
        # ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Ï∞æÍ∏∞
        current_dir = Path.cwd()
        project_root = current_dir
        
        # backend ÎîîÎ†âÌÜ†Î¶¨ Ï∞æÍ∏∞
        if not (project_root / "backend").exists():
            if "backend" in str(current_dir):
                project_root = current_dir.parent
            else:
                logger.error("‚ùå backend ÎîîÎ†âÌÜ†Î¶¨Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
                return False
        
        # ÏàòÏ†ï Ïã§Ìñâ
        fixer = MPSCompatibilityFixer(project_root)
        results = fixer.run_fix()
        
        # Í≤∞Í≥º Ï∂úÎ†•
        print("\n" + "="*60)
        print("üéâ PyTorch 2.1.2 MPS Ìò∏ÌôòÏÑ± ÏàòÏ†ï ÏôÑÎ£å!")
        print("="*60)
        print(f"‚úÖ ÏÑ±Í≥µ: {len(results['fixed_files'])}/{results['total_files']}")
        print(f"üìÅ ÏàòÏ†ïÎêú ÌååÏùº: {results['fixed_files']}")
        
        if results['failed_files']:
            print(f"‚ùå Ïã§Ìå®Ìïú ÌååÏùº: {results['failed_files']}")
        
        print(f"üíæ Î∞±ÏóÖ ÌååÏùº: {len(results['backup_files'])}Í∞ú")
        print("\nüöÄ Ïù¥Ï†ú ÏÑúÎ≤ÑÎ•º Îã§Ïãú ÏãúÏûëÌï¥Ï£ºÏÑ∏Ïöî:")
        print("   cd backend && python app/main.py")
        
        return results['success']
        
    except Exception as e:
        logger.error(f"‚ùå ÏàòÏ†ï ÎèÑÍµ¨ Ïã§Ìñâ Ïã§Ìå®: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)