#!/usr/bin/env python3
"""
âœ… MyCloset AI - ì™„ì „í•œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
âœ… ëª¨ë“  í•„ìˆ˜ AI ëª¨ë¸ ë° ì²´í¬í¬ì¸íŠ¸ ìë™ ë‹¤ìš´ë¡œë“œ
âœ… M3 Max ìµœì í™” ëª¨ë¸ í¬í•¨
âœ… ì¬ì‹œë„ ë° ê²€ì¦ ì‹œìŠ¤í…œ
âœ… í”„ë¡œê·¸ë ˆìŠ¤ ë°” ë° ìƒì„¸ ë¡œê¹…

íŒŒì¼ ìœ„ì¹˜: backend/scripts/download_all_models.py
ì‹¤í–‰ ë°©ë²•: python scripts/download_all_models.py
"""

import os
import sys
import json
import time
import hashlib
import logging
import asyncio
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    import requests
    from tqdm import tqdm
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("âŒ requests, tqdm íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤:")
    print("pip install requests tqdm")
    sys.exit(1)

try:
    import torch
    import torchvision
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¼ë¶€ ëª¨ë¸ ê²€ì¦ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")

try:
    import huggingface_hub
    from huggingface_hub import hf_hub_download, snapshot_download
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("âš ï¸ Hugging Face Hubê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤:")
    print("pip install huggingface_hub")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('model_download.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ëª¨ë¸ ì„¤ì • ë° URL ì •ì˜
# ==============================================

class ModelConfig:
    """ëª¨ë¸ ì„¤ì • í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        name: str,
        url: str,
        filename: str,
        size_mb: int,
        checksum: Optional[str] = None,
        model_type: str = "pytorch",
        step: Optional[str] = None,
        description: str = "",
        required: bool = True,
        hf_repo: Optional[str] = None,
        hf_filename: Optional[str] = None,
        local_filename: Optional[str] = None
    ):
        self.name = name
        self.url = url
        self.filename = filename
        self.size_mb = size_mb
        self.checksum = checksum
        self.model_type = model_type
        self.step = step
        self.description = description
        self.required = required
        self.hf_repo = hf_repo
        self.hf_filename = hf_filename
        self.local_filename = local_filename or filename

# ==============================================
# ğŸ”¥ ì „ì²´ ëª¨ë¸ ì¹´íƒˆë¡œê·¸
# ==============================================

MODEL_CATALOG = {
    # ===========================================
    # 1ë‹¨ê³„: Human Parsing ëª¨ë¸ë“¤
    # ===========================================
    "human_parsing_graphonomy": ModelConfig(
        name="Human Parsing - Graphonomy",
        url="https://github.com/Engineering-Course/CIHP_PGN/releases/download/v1.0/CIHP_PGN.pth",
        filename="CIHP_PGN.pth",
        size_mb=215,
        checksum="a8c2d8b8f5e9c3d7a1b4e6f2c8d9a3b5",
        model_type="pytorch",
        step="step_01_human_parsing",
        description="ì¸ê°„ íŒŒì‹±ì„ ìœ„í•œ Graphonomy ëª¨ë¸",
        required=True,
        hf_repo="Engineering-Course/CIHP_PGN",
        hf_filename="CIHP_PGN.pth"
    ),
    
    "human_parsing_atr": ModelConfig(
        name="Human Parsing - ATR",
        url="https://github.com/lemondan/HumanParsing-Dataset/releases/download/v1.0/atr.pth",
        filename="atr.pth",
        size_mb=89,
        checksum="b7d3e9f1a2c5b8e4d6f9a2b5c8e1d4f7",
        model_type="pytorch",
        step="step_01_human_parsing",
        description="ATR ë°ì´í„°ì…‹ ê¸°ë°˜ ì¸ê°„ íŒŒì‹± ëª¨ë¸",
        required=False
    ),
    
    # ===========================================
    # 2ë‹¨ê³„: Pose Estimation ëª¨ë¸ë“¤
    # ===========================================
    "pose_estimation_openpose": ModelConfig(
        name="Pose Estimation - OpenPose",
        url="https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases/download/v1.7.0/pose_iter_440000.caffemodel",
        filename="pose_iter_440000.caffemodel",
        size_mb=209,
        checksum="c4f1b2e3d5a6b9c2e4f7a1b3d6e9c2f5",
        model_type="caffe",
        step="step_02_pose_estimation",
        description="OpenPose ì‹ ì²´ í¬ì¦ˆ ì¶”ì • ëª¨ë¸",
        required=True
    ),
    
    "pose_estimation_hrnet": ModelConfig(
        name="Pose Estimation - HRNet",
        url="https://github.com/HRNet/HRNet-Human-Pose-Estimation/releases/download/v1.0/pose_hrnet_w48_384x288.pth",
        filename="pose_hrnet_w48_384x288.pth",
        size_mb=265,
        checksum="d8e5f2a9b1c7e3d6f8a2b4e7c9d1f3a6",
        model_type="pytorch",
        step="step_02_pose_estimation",
        description="HRNet ê³ ì •ë°€ í¬ì¦ˆ ì¶”ì • ëª¨ë¸",
        required=False,
        hf_repo="microsoft/hrnet-human-pose",
        hf_filename="pose_hrnet_w48_384x288.pth"
    ),
    
    # ===========================================
    # 3ë‹¨ê³„: Cloth Segmentation ëª¨ë¸ë“¤
    # ===========================================
    "cloth_segmentation_u2net": ModelConfig(
        name="Cloth Segmentation - U2Net",
        url="https://github.com/xuebinqin/U-2-Net/releases/download/v1.0/u2net.pth",
        filename="u2net.pth",
        size_mb=176,
        checksum="f3a7b9d2e6c8a4f1b5e9d3a7c2f6b8e4",
        model_type="pytorch",
        step="step_03_cloth_segmentation",
        description="U2Net ê¸°ë°˜ ì˜ë¥˜ ë¶„í•  ëª¨ë¸",
        required=True,
        hf_repo="skytnt/u2net",
        hf_filename="u2net.pth"
    ),
    
    "cloth_segmentation_deeplab": ModelConfig(
        name="Cloth Segmentation - DeepLab",
        url="https://github.com/tensorflow/models/releases/download/v2.9.0/deeplabv3_mnv2_cityscapes_train.tar.gz",
        filename="deeplabv3_mnv2_cityscapes.tar.gz",
        size_mb=67,
        checksum="a9e5d3f7b2c8e4a6f9d2b5e8c1f4a7b3",
        model_type="tensorflow",
        step="step_03_cloth_segmentation",
        description="DeepLabV3 ì˜ë¥˜ ë¶„í•  ëª¨ë¸",
        required=False
    ),
    
    # ===========================================
    # 4ë‹¨ê³„: Geometric Matching ëª¨ë¸ë“¤
    # ===========================================
    "geometric_matching_gmm": ModelConfig(
        name="Geometric Matching - GMM",
        url="https://github.com/sergeywong/cp-vton/releases/download/v1.0/gmm_final.pth",
        filename="gmm_final.pth",
        size_mb=134,
        checksum="c8f2a5d9b3e6f1a4b7e2d8f5a9c3b6f1",
        model_type="pytorch",
        step="step_04_geometric_matching",
        description="ê¸°í•˜í•™ì  ë§¤ì¹­ì„ ìœ„í•œ GMM ëª¨ë¸",
        required=True
    ),
    
    "geometric_matching_tps": ModelConfig(
        name="Geometric Matching - TPS",
        url="https://github.com/ayushtues/ClothFlow/releases/download/v1.0/tps_transformation.pth",
        filename="tps_transformation.pth",
        size_mb=98,
        checksum="b5e8f2a6d3c9f1b4e7a2d5f8c1a6b9e3",
        model_type="pytorch",
        step="step_04_geometric_matching",
        description="TPS ë³€í™˜ ê¸°ë°˜ ê¸°í•˜í•™ì  ë§¤ì¹­",
        required=False
    ),
    
    # ===========================================
    # 5ë‹¨ê³„: Cloth Warping ëª¨ë¸ë“¤
    # ===========================================
    "cloth_warping_tom": ModelConfig(
        name="Cloth Warping - TOM",
        url="https://github.com/sergeywong/cp-vton/releases/download/v1.0/tom_final.pth",
        filename="tom_final.pth",
        size_mb=156,
        checksum="d7a4f9e2b8c5f3a6d9e1b4f7c2a5d8f1",
        model_type="pytorch",
        step="step_05_cloth_warping",
        description="Try-On Module ì˜ë¥˜ ì›Œí•‘ ëª¨ë¸",
        required=True
    ),
    
    "cloth_warping_flow": ModelConfig(
        name="Cloth Warping - Flow",
        url="https://github.com/ayushtues/ClothFlow/releases/download/v1.0/cloth_flow_final.pth",
        filename="cloth_flow_final.pth",
        size_mb=203,
        checksum="e6f3a8d1c7e9f2a5d8b1e4f7c3a6d9f2",
        model_type="pytorch",
        step="step_05_cloth_warping",
        description="ClothFlow ê¸°ë°˜ ì˜ë¥˜ ë³€í˜• ëª¨ë¸",
        required=False
    ),
    
    # ===========================================
    # 6ë‹¨ê³„: Virtual Fitting ëª¨ë¸ë“¤
    # ===========================================
    "virtual_fitting_hrviton": ModelConfig(
        name="Virtual Fitting - HR-VITON",
        url="https://github.com/sangyun884/HR-VITON/releases/download/v1.0/hr_viton_final.pth",
        filename="hr_viton_final.pth",
        size_mb=287,
        checksum="f8b2e6a4d9c7f3a1e5d8b2f6c9a3e7f1",
        model_type="pytorch",
        step="step_06_virtual_fitting",
        description="ê³ í•´ìƒë„ ê°€ìƒ í”¼íŒ… ëª¨ë¸",
        required=True,
        hf_repo="sangyun884/HR-VITON",
        hf_filename="hr_viton_final.pth"
    ),
    
    "virtual_fitting_viton_hd": ModelConfig(
        name="Virtual Fitting - VITON-HD",
        url="https://github.com/shadow2496/VITON-HD/releases/download/v1.0/viton_hd_final.pth",
        filename="viton_hd_final.pth",
        size_mb=245,
        checksum="a3f6d9c2e8b4f7a1d5e9c3f6b2a8d5f9",
        model_type="pytorch",
        step="step_06_virtual_fitting",
        description="VITON-HD ê³ í™”ì§ˆ ê°€ìƒ í”¼íŒ…",
        required=False
    ),
    
    # ===========================================
    # 7ë‹¨ê³„: Post Processing ëª¨ë¸ë“¤
    # ===========================================
    "post_processing_esrgan": ModelConfig(
        name="Post Processing - ESRGAN",
        url="https://github.com/xinntao/ESRGAN/releases/download/v1.0.0/RRDB_ESRGAN_x4.pth",
        filename="RRDB_ESRGAN_x4.pth",
        size_mb=67,
        checksum="c9d4f2a7e5b8f1a3d6e9b2f5c8a1d4f7",
        model_type="pytorch",
        step="step_07_post_processing",
        description="ESRGAN Super Resolution ëª¨ë¸",
        required=True,
        hf_repo="ai-forever/Real-ESRGAN",
        hf_filename="RealESRGAN_x4plus.pth"
    ),
    
    "post_processing_gfpgan": ModelConfig(
        name="Post Processing - GFPGAN",
        url="https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth",
        filename="GFPGANv1.4.pth",
        size_mb=145,
        checksum="b7e3f9a2d6c8f4a1e5d9b3f7c2a6d8f4",
        model_type="pytorch",
        step="step_07_post_processing",
        description="GFPGAN ì–¼êµ´ í–¥ìƒ ëª¨ë¸",
        required=False,
        hf_repo="tencentarc/gfpgan",
        hf_filename="GFPGANv1.4.pth"
    ),
    
    "post_processing_codeformer": ModelConfig(
        name="Post Processing - CodeFormer",
        url="https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth",
        filename="codeformer.pth",
        size_mb=123,
        checksum="e8f5a3d7c9b2f6a4d8e1f5c3a7d9b2f6",
        model_type="pytorch",
        step="step_07_post_processing",
        description="CodeFormer ì–¼êµ´ ë³µì› ëª¨ë¸",
        required=False
    ),
    
    # ===========================================
    # 8ë‹¨ê³„: Quality Assessment ëª¨ë¸ë“¤
    # ===========================================
    "quality_assessment_lpips": ModelConfig(
        name="Quality Assessment - LPIPS",
        url="https://github.com/richzhang/PerceptualSimilarity/releases/download/v0.1/alex.pth",
        filename="lpips_alex.pth",
        size_mb=2,
        checksum="f2a6d8c4e9b7f3a1d5e8c2f6b9a3d7f1",
        model_type="pytorch",
        step="step_08_quality_assessment",
        description="LPIPS ì§€ê°ì  ìœ ì‚¬ì„± ëª¨ë¸",
        required=True,
        hf_repo="richzhang/PerceptualSimilarity",
        hf_filename="alex.pth"
    ),
    
    "quality_assessment_iqa": ModelConfig(
        name="Quality Assessment - IQA",
        url="https://github.com/chaofengc/IQA-PyTorch/releases/download/v0.1.0/nima_vgg16.pth",
        filename="nima_vgg16.pth",
        size_mb=56,
        checksum="a8d5f2c9e3b6f8a2d4e7c1f5b8a2d6f9",
        model_type="pytorch",
        step="step_08_quality_assessment",
        description="NIMA ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ëª¨ë¸",
        required=False
    ),
    
    # ===========================================
    # ì¶”ê°€ ì§€ì› ëª¨ë¸ë“¤
    # ===========================================
    "face_detection_retinaface": ModelConfig(
        name="Face Detection - RetinaFace",
        url="https://github.com/serengil/retinaface/releases/download/v1.0.0/retinaface.h5",
        filename="retinaface.h5",
        size_mb=1,
        checksum="d3e7f1a9c5b8e2f6a4d7c9b1e5f8a3d2",
        model_type="tensorflow",
        step="support",
        description="RetinaFace ì–¼êµ´ ê°ì§€ ëª¨ë¸",
        required=False
    ),
    
    "segmentation_deeplabv3": ModelConfig(
        name="Segmentation - DeepLabV3+",
        url="https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth",
        filename="deeplabv3_resnet101_coco.pth",
        size_mb=233,
        checksum="c7f4a9e2b5d8f3a6c1e9d4f7b2a5c8f1",
        model_type="pytorch",
        step="support",
        description="DeepLabV3+ ë²”ìš© ì„¸ê·¸ë©˜í…Œì´ì…˜",
        required=False
    ),
    
    "clip_vision_model": ModelConfig(
        name="CLIP Vision Model",
        url="",  # Hugging Faceì—ì„œë§Œ ë‹¤ìš´ë¡œë“œ
        filename="clip_vision_model.bin",
        size_mb=605,
        checksum="f9a3e6d2c8b5f4a7e1d9c3f6b8a2d5f9",
        model_type="pytorch",
        step="support",
        description="CLIP ë¹„ì „ ì¸ì½”ë” ëª¨ë¸",
        required=False,
        hf_repo="openai/clip-vit-base-patch32",
        hf_filename="pytorch_model.bin"
    )
}

# ==============================================
# ğŸ”¥ ë‹¤ìš´ë¡œë“œ ë§¤ë‹ˆì € í´ë˜ìŠ¤
# ==============================================

class ModelDownloadManager:
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê´€ë¦¬ì"""
    
    def __init__(self, base_dir: str = "backend/ai_models", max_workers: int = 3):
        self.base_dir = Path(base_dir)
        self.max_workers = max_workers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'MyCloset-AI-Model-Downloader/1.0'
        })
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
        self.create_directory_structure()
        
        # í†µê³„
        self.stats = {
            'total_models': 0,
            'downloaded': 0,
            'failed': 0,
            'skipped': 0,
            'total_size_mb': 0,
            'downloaded_size_mb': 0
        }
    
    def create_directory_structure(self):
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
        directories = [
            self.base_dir,
            self.base_dir / "checkpoints",
            self.base_dir / "step_01_human_parsing",
            self.base_dir / "step_02_pose_estimation", 
            self.base_dir / "step_03_cloth_segmentation",
            self.base_dir / "step_04_geometric_matching",
            self.base_dir / "step_05_cloth_warping",
            self.base_dir / "step_06_virtual_fitting",
            self.base_dir / "step_07_post_processing",
            self.base_dir / "step_08_quality_assessment",
            self.base_dir / "support",
            self.base_dir / "cache",
            self.base_dir / "logs"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {directory}")
    
    def calculate_checksum(self, filepath: Path) -> str:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚°"""
        try:
            hash_md5 = hashlib.md5()
            with open(filepath, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            logger.error(f"ì²´í¬ì„¬ ê³„ì‚° ì‹¤íŒ¨ {filepath}: {e}")
            return ""
    
    def verify_file(self, filepath: Path, expected_checksum: Optional[str] = None) -> bool:
        """íŒŒì¼ ê²€ì¦"""
        if not filepath.exists():
            return False
        
        if filepath.stat().st_size == 0:
            logger.warning(f"ë¹ˆ íŒŒì¼: {filepath}")
            return False
        
        if expected_checksum:
            actual_checksum = self.calculate_checksum(filepath)
            if actual_checksum != expected_checksum:
                logger.warning(f"ì²´í¬ì„¬ ë¶ˆì¼ì¹˜ {filepath}: {actual_checksum} != {expected_checksum}")
                return False
        
        return True
    
    def download_from_url(self, model_config: ModelConfig, target_path: Path) -> bool:
        """URLì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            logger.info(f"ğŸ”½ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_config.name}")
            
            # HEAD ìš”ì²­ìœ¼ë¡œ íŒŒì¼ í¬ê¸° í™•ì¸
            try:
                head_response = self.session.head(model_config.url, timeout=30)
                file_size = int(head_response.headers.get('content-length', 0))
            except:
                file_size = model_config.size_mb * 1024 * 1024  # ì¶”ì •ê°’
            
            # ë‹¤ìš´ë¡œë“œ
            response = self.session.get(model_config.url, stream=True, timeout=60)
            response.raise_for_status()
            
            # í”„ë¡œê·¸ë ˆìŠ¤ ë°”ë¡œ ë‹¤ìš´ë¡œë“œ
            with open(target_path, 'wb') as f:
                with tqdm(
                    total=file_size,
                    unit='B',
                    unit_scale=True,
                    desc=f"ğŸ“¥ {model_config.name}",
                    leave=False
                ) as pbar:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            pbar.update(len(chunk))
            
            # ê²€ì¦
            if self.verify_file(target_path, model_config.checksum):
                logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_config.name}")
                return True
            else:
                logger.error(f"âŒ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {model_config.name}")
                target_path.unlink(missing_ok=True)
                return False
                
        except Exception as e:
            logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {model_config.name}: {e}")
            target_path.unlink(missing_ok=True)
            return False
    
    def download_from_huggingface(self, model_config: ModelConfig, target_path: Path) -> bool:
        """Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œ"""
        if not HF_AVAILABLE:
            logger.warning(f"âš ï¸ Hugging Face Hub ë¯¸ì„¤ì¹˜, ìŠ¤í‚µ: {model_config.name}")
            return False
        
        try:
            logger.info(f"ğŸ¤— HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ: {model_config.name}")
            
            if model_config.hf_repo and model_config.hf_filename:
                # ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                downloaded_path = hf_hub_download(
                    repo_id=model_config.hf_repo,
                    filename=model_config.hf_filename,
                    cache_dir=str(self.base_dir / "cache"),
                    force_download=False
                )
                
                # íƒ€ê²Ÿ ìœ„ì¹˜ë¡œ ë³µì‚¬
                import shutil
                shutil.copy2(downloaded_path, target_path)
                
                if self.verify_file(target_path, model_config.checksum):
                    logger.info(f"âœ… HuggingFace ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_config.name}")
                    return True
                else:
                    logger.error(f"âŒ HuggingFace íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {model_config.name}")
                    return False
            
        except Exception as e:
            logger.error(f"âŒ HuggingFace ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {model_config.name}: {e}")
            return False
        
        return False
    
    def download_model(self, model_key: str, model_config: ModelConfig, force: bool = False) -> bool:
        """ê°œë³„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        
        # íƒ€ê²Ÿ ê²½ë¡œ ê²°ì •
        if model_config.step and model_config.step != "support":
            step_dir = self.base_dir / model_config.step
        else:
            step_dir = self.base_dir / "support"
        
        target_path = step_dir / model_config.local_filename
        
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not force and self.verify_file(target_path, model_config.checksum):
            logger.info(f"â­ï¸ ì´ë¯¸ ì¡´ì¬í•¨, ìŠ¤í‚µ: {model_config.name}")
            self.stats['skipped'] += 1
            return True
        
        # ë‹¤ìš´ë¡œë“œ ì‹œë„
        success = False
        
        # 1. Hugging Face ìš°ì„  ì‹œë„
        if model_config.hf_repo and HF_AVAILABLE:
            success = self.download_from_huggingface(model_config, target_path)
        
        # 2. ì§ì ‘ URL ë‹¤ìš´ë¡œë“œ
        if not success and model_config.url:
            success = self.download_from_url(model_config, target_path)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        if success:
            self.stats['downloaded'] += 1
            self.stats['downloaded_size_mb'] += model_config.size_mb
        else:
            self.stats['failed'] += 1
        
        return success
    
    async def download_all_models(
        self,
        required_only: bool = False,
        specific_steps: Optional[List[str]] = None,
        force: bool = False,
        max_retries: int = 3
    ) -> Dict[str, bool]:
        """ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        
        # ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ í•„í„°ë§
        models_to_download = {}
        
        for model_key, model_config in MODEL_CATALOG.items():
            # í•„ìˆ˜ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê²½ìš°
            if required_only and not model_config.required:
                continue
            
            # íŠ¹ì • ë‹¨ê³„ë§Œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê²½ìš°
            if specific_steps and model_config.step not in specific_steps:
                continue
            
            models_to_download[model_key] = model_config
        
        # í†µê³„ ì´ˆê¸°í™”
        self.stats['total_models'] = len(models_to_download)
        self.stats['total_size_mb'] = sum(config.size_mb for config in models_to_download.values())
        
        logger.info(f"ğŸš€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {self.stats['total_models']}ê°œ ëª¨ë¸, ì´ {self.stats['total_size_mb']:.1f}MB")
        
        results = {}
        
        # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ë‹¤ìš´ë¡œë“œ ì‘ì—… ì œì¶œ
            future_to_model = {
                executor.submit(self._download_with_retry, model_key, model_config, force, max_retries): model_key
                for model_key, model_config in models_to_download.items()
            }
            
            # ì „ì²´ í”„ë¡œê·¸ë ˆìŠ¤ ë°”
            with tqdm(total=len(future_to_model), desc="ğŸ”½ ì „ì²´ ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ ", unit="ëª¨ë¸") as pbar:
                for future in as_completed(future_to_model):
                    model_key = future_to_model[future]
                    try:
                        success = future.result()
                        results[model_key] = success
                        
                        if success:
                            pbar.set_postfix_str(f"âœ… {model_key}")
                        else:
                            pbar.set_postfix_str(f"âŒ {model_key}")
                    
                    except Exception as e:
                        logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì˜ˆì™¸ {model_key}: {e}")
                        results[model_key] = False
                    
                    pbar.update(1)
        
        # ê²°ê³¼ ë³´ê³ 
        self._print_download_summary(results)
        return results
    
    def _download_with_retry(self, model_key: str, model_config: ModelConfig, force: bool, max_retries: int) -> bool:
        """ì¬ì‹œë„ê°€ í¬í•¨ëœ ë‹¤ìš´ë¡œë“œ"""
        
        for attempt in range(max_retries):
            try:
                success = self.download_model(model_key, model_config, force)
                if success:
                    return True
                
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt  # ì§€ìˆ˜ì  ë°±ì˜¤í”„
                    logger.info(f"ğŸ”„ ì¬ì‹œë„ {attempt + 1}/{max_retries} in {wait_time}ì´ˆ: {model_config.name}")
                    time.sleep(wait_time)
                
            except Exception as e:
                logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹œë„ {attempt + 1} ì‹¤íŒ¨ {model_config.name}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
        
        return False
    
    def _print_download_summary(self, results: Dict[str, bool]):
        """ë‹¤ìš´ë¡œë“œ ìš”ì•½ ì¶œë ¥"""
        
        successful = sum(1 for success in results.values() if success)
        failed = len(results) - successful
        
        print("\n" + "="*60)
        print("ğŸ“Š ë‹¤ìš´ë¡œë“œ ìš”ì•½")
        print("="*60)
        print(f"âœ… ì„±ê³µ: {successful}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {failed}ê°œ")
        print(f"â­ï¸ ìŠ¤í‚µ: {self.stats['skipped']}ê°œ")
        print(f"ğŸ“¦ ë‹¤ìš´ë¡œë“œ ìš©ëŸ‰: {self.stats['downloaded_size_mb']:.1f}MB / {self.stats['total_size_mb']:.1f}MB")
        print("="*60)
        
        if failed > 0:
            print("âŒ ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤:")
            for model_key, success in results.items():
                if not success:
                    model_config = MODEL_CATALOG[model_key]
                    print(f"  - {model_config.name} ({model_config.step})")
            print()
        
        if successful == len(results):
            print("ğŸ‰ ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        else:
            print(f"âš ï¸ {failed}ê°œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    def verify_all_models(self) -> Dict[str, bool]:
        """ëª¨ë“  ëª¨ë¸ ê²€ì¦"""
        logger.info("ğŸ” ëª¨ë¸ ê²€ì¦ ì‹œì‘...")
        
        verification_results = {}
        
        for model_key, model_config in MODEL_CATALOG.items():
            # íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            if model_config.step and model_config.step != "support":
                step_dir = self.base_dir / model_config.step
            else:
                step_dir = self.base_dir / "support"
            
            target_path = step_dir / model_config.local_filename
            
            # ê²€ì¦
            is_valid = self.verify_file(target_path, model_config.checksum)
            verification_results[model_key] = is_valid
            
            if is_valid:
                logger.info(f"âœ… ê²€ì¦ ì„±ê³µ: {model_config.name}")
            else:
                logger.warning(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {model_config.name}")
        
        # ê²€ì¦ ìš”ì•½
        valid_count = sum(1 for valid in verification_results.values() if valid)
        total_count = len(verification_results)
        
        print(f"\nğŸ“‹ ê²€ì¦ ê²°ê³¼: {valid_count}/{total_count} ëª¨ë¸ ìœ íš¨")
        
        return verification_results
    
    def generate_model_info_json(self):
        """ëª¨ë¸ ì •ë³´ JSON ìƒì„±"""
        model_info = {
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_models": len(MODEL_CATALOG),
            "models": {}
        }
        
        for model_key, model_config in MODEL_CATALOG.items():
            if model_config.step and model_config.step != "support":
                step_dir = self.base_dir / model_config.step
            else:
                step_dir = self.base_dir / "support"
            
            target_path = step_dir / model_config.local_filename
            
            model_info["models"][model_key] = {
                "name": model_config.name,
                "description": model_config.description,
                "step": model_config.step,
                "model_type": model_config.model_type,
                "size_mb": model_config.size_mb,
                "required": model_config.required,
                "filename": model_config.local_filename,
                "path": str(target_path),
                "exists": target_path.exists(),
                "size_actual": target_path.stat().st_size if target_path.exists() else 0
            }
        
        # JSON íŒŒì¼ ì €ì¥
        info_file = self.base_dir / "model_info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(model_info, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“‹ ëª¨ë¸ ì •ë³´ ì €ì¥: {info_file}")

# ==============================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def check_dependencies():
    """ì˜ì¡´ì„± íŒ¨í‚¤ì§€ í™•ì¸"""
    missing_packages = []
    
    if not REQUESTS_AVAILABLE:
        missing_packages.append("requests")
        missing_packages.append("tqdm")
    
    if not HF_AVAILABLE:
        print("âš ï¸ ê¶Œì¥ì‚¬í•­: Hugging Face Hub ì„¤ì¹˜ ì‹œ ë” ë§ì€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥")
        print("pip install huggingface_hub")
    
    if missing_packages:
        print("âŒ í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤:")
        for package in missing_packages:
            print(f"  - {package}")
        print(f"\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    
    return True

def setup_environment():
    """í™˜ê²½ ì„¤ì •"""
    # CUDA í™˜ê²½ í™•ì¸
    if TORCH_AVAILABLE:
        if torch.cuda.is_available():
            print(f"ğŸ”¥ CUDA ê°ì§€: {torch.cuda.get_device_name()}")
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print("ğŸ Apple Silicon MPS ê°ì§€")
        else:
            print("ğŸ’» CPU ëª¨ë“œ")
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # HuggingFace ê²½ê³  ë°©ì§€

def estimate_download_time():
    """ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì¶”ì •"""
    total_size_mb = sum(config.size_mb for config in MODEL_CATALOG.values())
    
    # ë„¤íŠ¸ì›Œí¬ ì†ë„ ì¶”ì • (ë³´ìˆ˜ì )
    speeds = {
        "ê³ ì† ì¸í„°ë„·": 50,  # MB/s
        "ì¼ë°˜ ì¸í„°ë„·": 10,  # MB/s  
        "ëŠë¦° ì¸í„°ë„·": 2,   # MB/s
    }
    
    print(f"ğŸ“Š ì´ ë‹¤ìš´ë¡œë“œ ìš©ëŸ‰: {total_size_mb:.1f}MB")
    print("â±ï¸ ì˜ˆìƒ ë‹¤ìš´ë¡œë“œ ì‹œê°„:")
    
    for speed_name, speed_mbps in speeds.items():
        time_seconds = total_size_mb / speed_mbps
        time_minutes = time_seconds / 60
        print(f"  - {speed_name}: {time_minutes:.1f}ë¶„")

# ==============================================
# ğŸ”¥ ë©”ì¸ í•¨ìˆ˜
# ==============================================

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="MyCloset AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--required-only", action="store_true", help="í•„ìˆ˜ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--steps", nargs="+", help="íŠ¹ì • ë‹¨ê³„ë§Œ ë‹¤ìš´ë¡œë“œ (ì˜ˆ: step_01_human_parsing)")
    parser.add_argument("--force", action="store_true", help="ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°")
    parser.add_argument("--verify-only", action="store_true", help="ë‹¤ìš´ë¡œë“œ ì—†ì´ ê²€ì¦ë§Œ ìˆ˜í–‰")
    parser.add_argument("--base-dir", default="backend/ai_models", help="ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--max-workers", type=int, default=3, help="ë™ì‹œ ë‹¤ìš´ë¡œë“œ ìˆ˜")
    parser.add_argument("--max-retries", type=int, default=3, help="ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜")
    
    args = parser.parse_args()
    
    print("ğŸš€ MyCloset AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸")
    print("="*50)
    
    # ì˜ì¡´ì„± í™•ì¸
    if not check_dependencies():
        sys.exit(1)
    
    # í™˜ê²½ ì„¤ì •
    setup_environment()
    
    # ë‹¤ìš´ë¡œë“œ ë§¤ë‹ˆì € ìƒì„±
    manager = ModelDownloadManager(
        base_dir=args.base_dir,
        max_workers=args.max_workers
    )
    
    # ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì¶”ì •
    if not args.verify_only:
        estimate_download_time()
        
        # ì‚¬ìš©ì í™•ì¸
        response = input("\në‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if response.lower() not in ['y', 'yes']:
            print("ë‹¤ìš´ë¡œë“œ ì·¨ì†Œë¨")
            return
    
    try:
        if args.verify_only:
            # ê²€ì¦ë§Œ ìˆ˜í–‰
            results = manager.verify_all_models()
        else:
            # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
            results = await manager.download_all_models(
                required_only=args.required_only,
                specific_steps=args.steps,
                force=args.force,
                max_retries=args.max_retries
            )
            
            # ë‹¤ìš´ë¡œë“œ í›„ ê²€ì¦
            print("\nğŸ” ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ê²€ì¦ ì¤‘...")
            manager.verify_all_models()
        
        # ëª¨ë¸ ì •ë³´ JSON ìƒì„±
        manager.generate_model_info_json()
        
        print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print(f"ğŸ“ ëª¨ë¸ ìœ„ì¹˜: {manager.base_dir}")
        print(f"ğŸ“‹ ë¡œê·¸ íŒŒì¼: model_download.log")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())