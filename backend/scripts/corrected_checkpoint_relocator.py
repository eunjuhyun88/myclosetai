#!/usr/bin/env python3
"""
ğŸ”§ ì •í™•í•œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ë¶„ì„ ë° ì˜¬ë°”ë¥¸ ì¬ë°°ì¹˜
âœ… ì‹¤ì œ íŒŒì¼ ìœ„ì¹˜ ì •í™•íˆ íƒì§€
âœ… SAM ëª¨ë¸ ë¶„ë¦¬
âœ… ì˜¬ë°”ë¥¸ ëª¨ë¸ë§Œ ë§¤ì¹­
âœ… ì‹¤ì œ ê²½ë¡œ ê¸°ë°˜ ì¬ë°°ì¹˜
"""

import os
import sys
import json
import shutil
import time
from pathlib import Path
from typing import Dict, Any, List, Optional
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')
logger = logging.getLogger(__name__)

class CorrectedCheckpointRelocator:
    """ì •í™•í•œ ì²´í¬í¬ì¸íŠ¸ ì¬ë°°ì¹˜ê¸°"""
    
    def __init__(self):
        self.project_root = Path.cwd()
        self.ai_models_root = self.project_root / "ai_models"
        self.target_dir = self.ai_models_root / "checkpoints"
        
        # ì •í™•í•œ ëª¨ë¸ ë§¤í•‘ (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë“¤ë§Œ)
        self.correct_model_mapping = {
            "human_parsing_graphonomy": {
                "target": "step_01_human_parsing/graphonomy.pth",
                "candidates": [
                    "checkpoints/human_parsing/schp_atr.pth",
                    "checkpoints/step_01_human_parsing/schp_atr.pth",
                    "checkpoints/human_parsing/atr_model.pth"
                ]
            },
            "pose_estimation_openpose": {
                "target": "step_02_pose_estimation/openpose.pth", 
                "candidates": [
                    "checkpoints/pose_estimation/body_pose_model.pth",
                    "checkpoints/step_02_pose_estimation/body_pose_model.pth",
                    "checkpoints/openpose/body_pose_model.pth"
                ]
            },
            "cloth_segmentation_u2net": {
                "target": "step_03_cloth_segmentation/u2net.pth",
                "candidates": [
                    "checkpoints/step_03_cloth_segmentation/u2net.pth",
                    "checkpoints/cloth_segmentation/u2net.pth",
                    "step_03_cloth_segmentation/u2net.pth"
                ]
            },
            "geometric_matching_gmm": {
                "target": "step_04_geometric_matching/gmm_final.pth",
                "candidates": [
                    "checkpoints/step_04_geometric_matching/lightweight_gmm.pth",
                    "checkpoints/step_04_geometric_matching/geometric_matching_base/geometric_matching_base.pth",
                    "checkpoints/step_04_geometric_matching/tps_transformation_model/tps_network.pth",
                    "checkpoints/step_04/step_04_geometric_matching_base/geometric_matching_base.pth"
                ]
            },
            "cloth_warping_tom": {
                "target": "step_05_cloth_warping/tom_final.pth",
                "candidates": [
                    "checkpoints/step_06_virtual_fitting/diffusion_pytorch_model.bin",
                    "checkpoints/stable-diffusion-v1-5/unet/diffusion_pytorch_model.safetensors",
                    "checkpoints/ootdiffusion/checkpoints/ootd/unet/diffusion_pytorch_model.bin"
                ]
            },
            "virtual_fitting_hrviton": {
                "target": "step_06_virtual_fitting/hrviton_final.pth",
                "candidates": [
                    "checkpoints/hrviton_final.pth",
                    "checkpoints/step_06_virtual_fitting/hrviton_final.pth",
                    "checkpoints/stable-diffusion-v1-5/unet/diffusion_pytorch_model.safetensors",
                    "checkpoints/ootdiffusion/checkpoints/ootd/unet/diffusion_pytorch_model.bin"
                ]
            }
        }
        
    def find_correct_source_files(self) -> Dict[str, Any]:
        """ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì •í™•í•œ ì†ŒìŠ¤ íŒŒì¼ë“¤ ì°¾ê¸°"""
        logger.info("ğŸ” ì •í™•í•œ ì†ŒìŠ¤ íŒŒì¼ íƒì§€ ì¤‘...")
        
        found_models = {}
        
        for model_type, config in self.correct_model_mapping.items():
            logger.info(f"\nğŸ“‹ {model_type} íƒì§€ ì¤‘...")
            
            found_file = None
            file_info = None
            
            for candidate in config["candidates"]:
                # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ê²½ë¡œ ì‹œë„
                possible_paths = [
                    self.ai_models_root / candidate,
                    self.project_root / candidate,
                    self.project_root / "backend" / "ai_models" / candidate
                ]
                
                for path in possible_paths:
                    if path.exists() and path.is_file():
                        file_size = path.stat().st_size / (1024 * 1024)  # MB
                        
                        # í¬ê¸° ê¸°ë°˜ ê²€ì¦ (ë„ˆë¬´ ì‘ì€ íŒŒì¼ ì œì™¸)
                        if file_size > 1.0:  # 1MB ì´ìƒ
                            found_file = path
                            file_info = {
                                "source": str(path),
                                "size_mb": file_size,
                                "target": config["target"],
                                "confidence": self._calculate_confidence(model_type, path)
                            }
                            logger.info(f"   âœ… ë°œê²¬: {path.name} ({file_size:.1f}MB)")
                            break
                
                if found_file:
                    break
            
            if found_file:
                found_models[model_type] = file_info
            else:
                logger.warning(f"   âŒ {model_type} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return found_models
    
    def _calculate_confidence(self, model_type: str, file_path: Path) -> float:
        """íŒŒì¼ëª…ê³¼ ê²½ë¡œ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        file_name = file_path.name.lower()
        file_size_mb = file_path.stat().st_size / (1024 * 1024)
        
        confidence = 0.5  # ê¸°ë³¸ ì‹ ë¢°ë„
        
        # íŒŒì¼ëª… ê¸°ë°˜ ì‹ ë¢°ë„
        if model_type == "human_parsing_graphonomy":
            if "schp" in file_name or "atr" in file_name:
                confidence += 0.4
            if 200 < file_size_mb < 300:  # ì ì ˆí•œ í¬ê¸°
                confidence += 0.1
                
        elif model_type == "pose_estimation_openpose":
            if "body_pose" in file_name or "openpose" in file_name:
                confidence += 0.4
            if 150 < file_size_mb < 250:
                confidence += 0.1
                
        elif model_type == "cloth_segmentation_u2net":
            if "u2net" in file_name:
                confidence += 0.4
            if 150 < file_size_mb < 200:
                confidence += 0.1
                
        elif model_type == "geometric_matching_gmm":
            if any(x in file_name for x in ["gmm", "geometric", "tps"]):
                confidence += 0.4
            if 1 < file_size_mb < 50:  # GMMì€ ë³´í†µ ì‘ìŒ
                confidence += 0.1
                
        elif model_type in ["cloth_warping_tom", "virtual_fitting_hrviton"]:
            if any(x in file_name for x in ["diffusion", "unet", "hrviton"]):
                confidence += 0.4
            if file_size_mb > 1000:  # í° diffusion ëª¨ë¸
                confidence += 0.1
        
        # SAM ëª¨ë¸ ì œì™¸ (ì‹ ë¢°ë„ ëŒ€í­ ê°ì†Œ)
        if "sam_vit" in file_name:
            confidence = 0.1  # SAM ëª¨ë¸ì€ ê±°ì˜ ì‚¬ìš© ì•ˆí•¨
        
        return min(confidence, 1.0)
    
    def create_directory_structure(self):
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
        logger.info("ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì¤‘...")
        
        directories = [
            "step_01_human_parsing",
            "step_02_pose_estimation",
            "step_03_cloth_segmentation", 
            "step_04_geometric_matching",
            "step_05_cloth_warping",
            "step_06_virtual_fitting",
            "step_07_post_processing",
            "step_08_quality_assessment"
        ]
        
        for directory in directories:
            dir_path = self.target_dir / directory
            dir_path.mkdir(parents=True, exist_ok=True)
            
        logger.info("âœ… ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ")
    
    def relocate_models(self, found_models: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ ì¬ë°°ì¹˜ ì‹¤í–‰"""
        logger.info("ğŸš€ ì •í™•í•œ ëª¨ë¸ ì¬ë°°ì¹˜ ì‹¤í–‰ ì¤‘...")
        
        results = {
            "success": [],
            "failed": [],
            "total_size_mb": 0
        }
        
        for i, (model_type, model_info) in enumerate(found_models.items(), 1):
            logger.info(f"\nğŸ“‹ [{i}/{len(found_models)}] {model_type}")
            
            try:
                source_path = Path(model_info["source"])
                target_path = self.target_dir / model_info["target"]
                
                # ì†ŒìŠ¤ íŒŒì¼ ì¬í™•ì¸
                if not source_path.exists():
                    results["failed"].append({
                        "model": model_type,
                        "error": f"ì†ŒìŠ¤ íŒŒì¼ ì—†ìŒ: {source_path}"
                    })
                    continue
                
                # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ìƒì„±
                target_path.parent.mkdir(parents=True, exist_ok=True)
                
                # ê¸°ì¡´ íŒŒì¼ ì œê±°
                if target_path.exists():
                    if target_path.is_symlink():
                        target_path.unlink()
                    else:
                        backup_path = target_path.with_suffix(f".backup_{int(time.time())}")
                        shutil.move(target_path, backup_path)
                        logger.info(f"   ğŸ“¦ ë°±ì—…: {backup_path.name}")
                
                # ì¬ë°°ì¹˜ ì‹¤í–‰ (ì‹¬ë³¼ë¦­ ë§í¬ ìš°ì„ )
                if model_info["size_mb"] > 50:  # 50MB ì´ìƒì€ ì‹¬ë³¼ë¦­ ë§í¬
                    target_path.symlink_to(source_path.resolve())
                    logger.info(f"   ğŸ”— ì‹¬ë³¼ë¦­ ë§í¬: {target_path.name}")
                else:
                    shutil.copy2(source_path, target_path)
                    logger.info(f"   ğŸ“‹ ë³µì‚¬: {target_path.name}")
                
                # ì„±ê³µ ê¸°ë¡
                results["success"].append({
                    "model": model_type,
                    "source": str(source_path),
                    "target": str(target_path),
                    "size_mb": model_info["size_mb"],
                    "confidence": model_info["confidence"]
                })
                
                results["total_size_mb"] += model_info["size_mb"]
                logger.info(f"   âœ… ì™„ë£Œ ({model_info['size_mb']:.1f}MB, ì‹ ë¢°ë„: {model_info['confidence']:.2f})")
                
            except Exception as e:
                error_msg = f"ì¬ë°°ì¹˜ ì‹¤íŒ¨: {e}"
                logger.error(f"   âŒ {error_msg}")
                results["failed"].append({
                    "model": model_type,
                    "error": error_msg
                })
        
        return results
    
    def verify_relocate(self, results: Dict[str, Any]) -> bool:
        """ì¬ë°°ì¹˜ ê²°ê³¼ ê²€ì¦"""
        logger.info("\nğŸ” ì¬ë°°ì¹˜ ê²°ê³¼ ê²€ì¦ ì¤‘...")
        
        success_count = len(results["success"])
        failed_count = len(results["failed"])
        
        # ì„±ê³µí•œ íŒŒì¼ë“¤ ì¡´ì¬ í™•ì¸
        verified_count = 0
        for success in results["success"]:
            target_path = Path(success["target"])
            if target_path.exists():
                verified_count += 1
                logger.info(f"   âœ… ê²€ì¦ë¨: {target_path.name}")
            else:
                logger.warning(f"   âš ï¸ íƒ€ê²Ÿ íŒŒì¼ ì—†ìŒ: {target_path}")
        
        logger.info(f"\nğŸ“Š ê²€ì¦ ê²°ê³¼:")
        logger.info(f"   âœ… ì„±ê³µ: {success_count}ê°œ")
        logger.info(f"   âœ… ê²€ì¦ë¨: {verified_count}ê°œ") 
        logger.info(f"   âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
        logger.info(f"   ğŸ’¾ ì´ í¬ê¸°: {results['total_size_mb']:.1f}MB")
        
        return verified_count > 0
    
    def generate_corrected_config(self, results: Dict[str, Any]):
        """ìˆ˜ì •ëœ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
        logger.info("ğŸ”§ ìˆ˜ì •ëœ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")
        
        # ì„±ê³µí•œ ëª¨ë¸ë“¤ë¡œ ì„¤ì • ìƒì„±
        model_paths = {}
        for success in results["success"]:
            model_type = success["model"]
            target_path = success["target"]
            
            # ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            relative_path = str(Path(target_path).relative_to(self.project_root))
            model_paths[model_type] = relative_path
        
        # Python ì„¤ì • íŒŒì¼ ìƒì„±
        config_content = f'''# app/core/corrected_model_paths.py
"""
ìˆ˜ì •ëœ AI ëª¨ë¸ ê²½ë¡œ ì„¤ì • - ì •í™•í•œ ë§¤ì¹­ ê¸°ë°˜
ìë™ ìƒì„±ë¨: {time.strftime('%Y-%m-%d %H:%M:%S')}
"""

from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = Path(__file__).parent.parent.parent

# ì •í™•íˆ ë§¤ì¹­ëœ ëª¨ë¸ ê²½ë¡œë“¤
CORRECTED_MODEL_PATHS = {{
'''
        
        for model_type, path in model_paths.items():
            config_content += f'    "{model_type}": PROJECT_ROOT / "{path}",\n'
        
        config_content += f'''
}}

# ModelLoader í˜¸í™˜ ê²½ë¡œ ë§¤í•‘
MODEL_LOADER_PATHS = {{
    # Step 01: Human Parsing
    "human_parsing_graphonomy": {{
        "primary": CORRECTED_MODEL_PATHS.get("human_parsing_graphonomy"),
        "alternatives": []
    }},
    
    # Step 02: Pose Estimation  
    "pose_estimation_openpose": {{
        "primary": CORRECTED_MODEL_PATHS.get("pose_estimation_openpose"),
        "alternatives": []
    }},
    
    # Step 03: Cloth Segmentation
    "cloth_segmentation_u2net": {{
        "primary": CORRECTED_MODEL_PATHS.get("cloth_segmentation_u2net"),
        "alternatives": []
    }},
    
    # Step 04: Geometric Matching
    "geometric_matching_gmm": {{
        "primary": CORRECTED_MODEL_PATHS.get("geometric_matching_gmm"),
        "alternatives": []
    }},
    
    # Step 05: Cloth Warping
    "cloth_warping_tom": {{
        "primary": CORRECTED_MODEL_PATHS.get("cloth_warping_tom"),
        "alternatives": []
    }},
    
    # Step 06: Virtual Fitting
    "virtual_fitting_hrviton": {{
        "primary": CORRECTED_MODEL_PATHS.get("virtual_fitting_hrviton"),
        "alternatives": []
    }}
}}

def get_model_path(model_type: str) -> Path:
    """ëª¨ë¸ íƒ€ì…ìœ¼ë¡œ ê²½ë¡œ ë°˜í™˜"""
    return CORRECTED_MODEL_PATHS.get(model_type, None)

def is_model_available(model_type: str) -> bool:
    """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    path = get_model_path(model_type)
    return path is not None and path.exists()

def get_all_available_models() -> Dict[str, str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ ë°˜í™˜"""
    available = {{}}
    for model_type, path in CORRECTED_MODEL_PATHS.items():
        if path.exists():
            available[model_type] = str(path)
    return available

# ì´ ì¬ë°°ì¹˜ ì •ë³´
RELOCATE_SUMMARY = {{
    "total_models": {len(model_paths)},
    "total_size_mb": {results["total_size_mb"]:.1f},
    "generation_time": "{time.strftime('%Y-%m-%d %H:%M:%S')}",
    "corrected_issues": [
        "SAM ëª¨ë¸ ë¶„ë¦¬",
        "ì •í™•í•œ ê²½ë¡œ ë§¤ì¹­",
        "ì‹¤ì œ ì¡´ì¬ íŒŒì¼ë§Œ ì‚¬ìš©"
    ]
}}
'''
        
        # íŒŒì¼ ì €ì¥
        config_file = self.project_root / "app" / "core" / "corrected_model_paths.py"
        config_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        logger.info(f"âœ… ìˆ˜ì •ëœ ì„¤ì • íŒŒì¼ ìƒì„±: {config_file}")
        
        # JSON ì„¤ì •ë„ ìƒì„±
        json_config = {
            "corrected_models": model_paths,
            "relocate_summary": {
                "total_models": len(model_paths),
                "total_size_mb": results["total_size_mb"],
                "generation_time": time.strftime('%Y-%m-%d %H:%M:%S'),
                "success_count": len(results["success"]),
                "failed_count": len(results["failed"])
            },
            "success_details": results["success"],
            "failed_details": results["failed"]
        }
        
        json_file = self.project_root / "app" / "core" / "corrected_models.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… JSON ì„¤ì • íŒŒì¼ ìƒì„±: {json_file}")
    
    def print_final_report(self, results: Dict[str, Any]):
        """ìµœì¢… ë¦¬í¬íŠ¸ ì¶œë ¥"""
        logger.info("\n" + "="*70)
        logger.info("ğŸ‰ ì •í™•í•œ ì²´í¬í¬ì¸íŠ¸ ì¬ë°°ì¹˜ ì™„ë£Œ!")
        logger.info("="*70)
        
        if results["success"]:
            logger.info("âœ… ì„±ê³µì ìœ¼ë¡œ ì¬ë°°ì¹˜ëœ ëª¨ë¸ë“¤:")
            for success in results["success"]:
                logger.info(f"   - {success['model']}")
                logger.info(f"     í¬ê¸°: {success['size_mb']:.1f}MB")
                logger.info(f"     ì‹ ë¢°ë„: {success['confidence']:.2f}")
                logger.info(f"     íƒ€ê²Ÿ: {Path(success['target']).name}")
        
        if results["failed"]:
            logger.info("\nâŒ ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤:")
            for failed in results["failed"]:
                logger.info(f"   - {failed['model']}: {failed['error']}")
        
        logger.info(f"\nğŸ’¾ ì´ ì¬ë°°ì¹˜ í¬ê¸°: {results['total_size_mb']:.1f}MB")
        logger.info("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info("   python3 app/main.py")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("="*70)
    logger.info("ğŸ”§ ì •í™•í•œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ë¶„ì„ ë° ì¬ë°°ì¹˜")
    logger.info("="*70)
    
    relocator = CorrectedCheckpointRelocator()
    
    # 1. ì •í™•í•œ ì†ŒìŠ¤ íŒŒì¼ë“¤ ì°¾ê¸°
    found_models = relocator.find_correct_source_files()
    
    if not found_models:
        logger.error("âŒ ì¬ë°°ì¹˜í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        return False
    
    logger.info(f"\nğŸ“Š ë°œê²¬ëœ ëª¨ë¸: {len(found_models)}ê°œ")
    
    # 2. ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
    relocator.create_directory_structure()
    
    # 3. ëª¨ë¸ ì¬ë°°ì¹˜ ì‹¤í–‰
    results = relocator.relocate_models(found_models)
    
    # 4. ê²°ê³¼ ê²€ì¦
    if relocator.verify_relocate(results):
        # 5. ì„¤ì • íŒŒì¼ ìƒì„±
        relocator.generate_corrected_config(results)
        
        # 6. ìµœì¢… ë¦¬í¬íŠ¸
        relocator.print_final_report(results)
        
        return True
    else:
        logger.error("âŒ ì¬ë°°ì¹˜ ê²€ì¦ ì‹¤íŒ¨")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)