#!/usr/bin/env python3
"""
üîç MyCloset AI - ÏàòÏ†ïÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑùÍ∏∞
‚úÖ Í∏∞Ï°¥ 127.2GB Î∂ÑÏÑù Í≤∞Í≥º ÌôúÏö©
‚úÖ ÎàÑÎùΩÎêú ÌÇ§ Î¨∏Ï†ú Ìï¥Í≤∞
‚úÖ Ï¶âÏãú Ïã§Ìñâ Í∞ÄÎä•
"""

import os
import sys
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional

# ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÎ°ú ÏÑ§Ï†ï
current_file = Path(__file__).resolve()
backend_dir = current_file.parent.parent
project_root = backend_dir.parent
sys.path.insert(0, str(backend_dir))

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger(__name__)

class FixedCheckpointAnalyzer:
    """ÏàòÏ†ïÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑùÍ∏∞"""
    
    def __init__(self):
        self.backend_dir = backend_dir
        self.checkpoints_dir = backend_dir / "ai_models" / "checkpoints"
        
        # Í∏∞Ï°¥ Î∂ÑÏÑù Í≤∞Í≥º Í∏∞Î∞ò Î™®Îç∏ Îç∞Ïù¥ÌÑ∞ (Ïã§Ï†ú ÌÑ∞ÎØ∏ÎÑê Ï∂úÎ†• Í∏∞Î∞ò)
        self.analyzed_models = self._create_analyzed_models_from_output()
        
        logger.info("üîç ÏàòÏ†ïÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        logger.info(f"üìÅ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÎîîÎ†âÌÜ†Î¶¨: {self.checkpoints_dir}")
        logger.info(f"üìä Î∂ÑÏÑùÎêú Î™®Îç∏: {len(self.analyzed_models)}Í∞ú")
    
    def _create_analyzed_models_from_output(self) -> Dict[str, Dict]:
        """ÌÑ∞ÎØ∏ÎÑê Ï∂úÎ†• Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú Î™®Îç∏ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±"""
        
        # Ïã§Ï†ú ÌÑ∞ÎØ∏ÎÑê Ï∂úÎ†•ÏóêÏÑú ÌôïÏù∏Îêú Î™®Îç∏Îì§
        models_data = {
            "ootdiffusion": {
                "name": "OOTDiffusion",
                "type": "diffusion",
                "step": "step_06_virtual_fitting",
                "ready": True,
                "total_size_mb": 15129.3,
                "priority": 1,
                "checkpoints": [
                    {"name": "pytorch_model.bin", "path": "pytorch_model.bin", "size_mb": 469.5},
                    {"name": "diffusion_pytorch_model.bin", "path": "diffusion_pytorch_model.bin", "size_mb": 319.2},
                    {"name": "body_pose_model.pth", "path": "body_pose_model.pth", "size_mb": 199.6}
                ],
                "total_checkpoints": 5
            },
            
            "ootdiffusion_hf": {
                "name": "OOTDiffusion HF",
                "type": "diffusion", 
                "step": "step_06_virtual_fitting",
                "ready": True,
                "total_size_mb": 15129.3,
                "priority": 1,
                "checkpoints": [
                    {"name": "pytorch_model.bin", "path": "pytorch_model.bin", "size_mb": 469.5},
                    {"name": "diffusion_pytorch_model.bin", "path": "diffusion_pytorch_model.bin", "size_mb": 319.2},
                    {"name": "body_pose_model.pth", "path": "body_pose_model.pth", "size_mb": 199.6}
                ],
                "total_checkpoints": 5
            },
            
            "stable-diffusion-v1-5": {
                "name": "Stable Diffusion v1.5",
                "type": "diffusion",
                "step": "step_06_virtual_fitting",
                "ready": True,
                "total_size_mb": 45070.6,
                "priority": 2,
                "checkpoints": [
                    {"name": "v1-5-pruned.ckpt", "path": "v1-5-pruned.ckpt", "size_mb": 7346.9},
                    {"name": "v1-5-pruned-emaonly.ckpt", "path": "v1-5-pruned-emaonly.ckpt", "size_mb": 4067.8},
                    {"name": "pytorch_model.fp16.bin", "path": "pytorch_model.fp16.bin", "size_mb": 234.8}
                ],
                "total_checkpoints": 11
            },
            
            "human_parsing": {
                "name": "Human Parsing",
                "type": "human_parsing",
                "step": "step_01_human_parsing",
                "ready": True,
                "total_size_mb": 1288.3,
                "priority": 3,
                "checkpoints": [
                    {"name": "schp_atr.pth", "path": "schp_atr.pth", "size_mb": 255.1},
                    {"name": "optimizer.pt", "path": "optimizer.pt", "size_mb": 209.0},
                    {"name": "rng_state.pth", "path": "rng_state.pth", "size_mb": 0.0}
                ],
                "total_checkpoints": 8
            },
            
            "step_01_human_parsing": {
                "name": "Step 01 Human Parsing",
                "type": "human_parsing",
                "step": "step_01_human_parsing", 
                "ready": True,
                "total_size_mb": 1787.7,
                "priority": 3,
                "checkpoints": [
                    {"name": "densepose_rcnn_R_50_FPN_s1x.pkl", "path": "densepose_rcnn_R_50_FPN_s1x.pkl", "size_mb": 243.9},
                    {"name": "graphonomy_lip.pth", "path": "graphonomy_lip.pth", "size_mb": 255.1},
                    {"name": "lightweight_parsing.pth", "path": "lightweight_parsing.pth", "size_mb": 0.5}
                ],
                "total_checkpoints": 11
            },
            
            "pose_estimation": {
                "name": "Pose Estimation",
                "type": "pose_estimation",
                "step": "step_02_pose_estimation",
                "ready": True,
                "total_size_mb": 10095.6,
                "priority": 4,
                "checkpoints": [
                    {"name": "sk_model.pth", "path": "sk_model.pth", "size_mb": 16.4},
                    {"name": "upernet_global_small.pth", "path": "upernet_global_small.pth", "size_mb": 196.8},
                    {"name": "latest_net_G.pth", "path": "latest_net_G.pth", "size_mb": 303.5}
                ],
                "total_checkpoints": 23
            },
            
            "step_02_pose_estimation": {
                "name": "Step 02 Pose Estimation",
                "type": "pose_estimation",
                "step": "step_02_pose_estimation",
                "ready": True,
                "total_size_mb": 273.6,
                "priority": 4,
                "checkpoints": [
                    {"name": "openpose.pth", "path": "openpose.pth", "size_mb": 199.6},
                    {"name": "yolov8n-pose.pt", "path": "yolov8n-pose.pt", "size_mb": 6.5}
                ],
                "total_checkpoints": 2
            },
            
            "openpose": {
                "name": "OpenPose",
                "type": "pose_estimation",
                "step": "step_02_pose_estimation",
                "ready": True,
                "total_size_mb": 539.7,
                "priority": 4,
                "checkpoints": [
                    {"name": "body_pose_model.pth", "path": "body_pose_model.pth", "size_mb": 199.6},
                    {"name": "hand_pose_model.pth", "path": "hand_pose_model.pth", "size_mb": 140.5}
                ],
                "total_checkpoints": 3
            },
            
            "cloth_segmentation": {
                "name": "Cloth Segmentation",
                "type": "cloth_segmentation",
                "step": "step_03_cloth_segmentation",
                "ready": True,
                "total_size_mb": 803.2,
                "priority": 5,
                "checkpoints": [
                    {"name": "model.pth", "path": "model.pth", "size_mb": 168.5},
                    {"name": "pytorch_model.bin", "path": "pytorch_model.bin", "size_mb": 168.4}
                ],
                "total_checkpoints": 2
            },
            
            "step_03_cloth_segmentation": {
                "name": "Step 03 Cloth Segmentation",
                "type": "cloth_segmentation",
                "step": "step_03_cloth_segmentation",
                "ready": True,
                "total_size_mb": 206.7,
                "priority": 5,
                "checkpoints": [
                    {"name": "mobile_sam.pt", "path": "mobile_sam.pt", "size_mb": 38.8}
                ],
                "total_checkpoints": 1
            },
            
            "step_04_geometric_matching": {
                "name": "Step 04 Geometric Matching",
                "type": "geometric_matching",
                "step": "step_04_geometric_matching",
                "ready": True,
                "total_size_mb": 33.2,
                "priority": 6,
                "checkpoints": [
                    {"name": "gmm_final.pth", "path": "gmm_final.pth", "size_mb": 4.1},
                    {"name": "lightweight_gmm.pth", "path": "lightweight_gmm.pth", "size_mb": 4.1},
                    {"name": "tps_network.pth", "path": "tps_network.pth", "size_mb": 2.1}
                ],
                "total_checkpoints": 4
            },
            
            "step_05_cloth_warping": {
                "name": "Step 05 Cloth Warping",
                "type": "cloth_warping",
                "step": "step_05_cloth_warping",
                "ready": True,
                "total_size_mb": 3279.2,
                "priority": 7,
                "checkpoints": [
                    {"name": "tom_final.pth", "path": "tom_final.pth", "size_mb": 3279.1},
                    {"name": "lightweight_warping.pth", "path": "lightweight_warping.pth", "size_mb": 0.1}
                ],
                "total_checkpoints": 2
            },
            
            "step_06_virtual_fitting": {
                "name": "Step 06 Virtual Fitting",
                "type": "virtual_tryon",
                "step": "step_06_virtual_fitting",
                "ready": True,
                "total_size_mb": 20854.2,
                "priority": 1,
                "checkpoints": [
                    {"name": "hrviton_final.pth", "path": "hrviton_final.pth", "size_mb": 2445.7},
                    {"name": "diffusion_pytorch_model.bin", "path": "diffusion_pytorch_model.bin", "size_mb": 3279.1},
                    {"name": "pytorch_model.bin", "path": "pytorch_model.bin", "size_mb": 469.5}
                ],
                "total_checkpoints": 7
            },
            
            "step_07_post_processing": {
                "name": "Step 07 Post Processing",
                "type": "auxiliary",
                "step": "step_07_post_processing",
                "ready": True,
                "total_size_mb": 63.9,
                "priority": 8,
                "checkpoints": [
                    {"name": "RealESRGAN_x4plus.pth", "path": "RealESRGAN_x4plus.pth", "size_mb": 63.9}
                ],
                "total_checkpoints": 1
            },
            
            "sam": {
                "name": "SAM (Segment Anything Model)",
                "type": "auxiliary",
                "step": "auxiliary",
                "ready": True,
                "total_size_mb": 2445.7,
                "priority": 8,
                "checkpoints": [
                    {"name": "sam_vit_h_4b8939.pth", "path": "sam_vit_h_4b8939.pth", "size_mb": 2445.7}
                ],
                "total_checkpoints": 1
            },
            
            "clip-vit-base-patch32": {
                "name": "CLIP ViT Base",
                "type": "text_image",
                "step": "auxiliary",
                "ready": True,
                "total_size_mb": 580.7,
                "priority": 9,
                "checkpoints": [
                    {"name": "pytorch_model.bin", "path": "pytorch_model.bin", "size_mb": 577.2}
                ],
                "total_checkpoints": 1
            },
            
            "grounding_dino": {
                "name": "Grounding DINO",
                "type": "auxiliary",
                "step": "auxiliary",
                "ready": True,
                "total_size_mb": 1318.2,
                "priority": 9,
                "checkpoints": [
                    {"name": "pytorch_model.bin", "path": "pytorch_model.bin", "size_mb": 659.9}
                ],
                "total_checkpoints": 1
            }
        }
        
        # Í∞Å Î™®Îç∏Ïóê Í≤ΩÎ°ú Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        for model_name, model_info in models_data.items():
            model_info["path"] = str(self.checkpoints_dir / model_name)
        
        return models_data
    
    def create_optimized_model_config(self):
        """ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ ÏÑ§Ï†ï ÌååÏùº ÏÉùÏÑ±"""
        logger.info("üìù ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ ÏÑ§Ï†ï ÌååÏùº ÏÉùÏÑ± Ï§ë...")
        
        # Îã®Í≥ÑÎ≥Ñ ÏµúÏ†Å Î™®Îç∏ Îß§Ìïë
        step_optimal_models = {
            "step_01_human_parsing": "step_01_human_parsing",
            "step_02_pose_estimation": "step_02_pose_estimation", 
            "step_03_cloth_segmentation": "step_03_cloth_segmentation",
            "step_04_geometric_matching": "step_04_geometric_matching",
            "step_05_cloth_warping": "step_05_cloth_warping",
            "step_06_virtual_fitting": "step_06_virtual_fitting",
            "step_07_post_processing": "step_07_post_processing",
            "auxiliary": "sam"
        }
        
        # Python ÏÑ§Ï†ï ÌååÏùº ÏÉùÏÑ±
        config_content = f'''# app/core/optimized_model_paths.py
"""
ÏµúÏ†ÅÌôîÎêú AI Î™®Îç∏ Í≤ΩÎ°ú ÏÑ§Ï†ï - Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù Í∏∞Î∞ò
Ïã§Ï†ú ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Îì§Î°úÎßå Íµ¨ÏÑ±
ÏÉùÏÑ±Ïùº: {time.strftime("%Y-%m-%d %H:%M:%S")}
Î∂ÑÏÑùÎêú Î™®Îç∏: {len(self.analyzed_models)}Í∞ú
Ï¥ù ÌÅ¨Í∏∞: {sum(m["total_size_mb"] for m in self.analyzed_models.values())/1024:.1f}GB
"""

from pathlib import Path
from typing import Dict, Optional, List, Any

# Í∏∞Î≥∏ Í≤ΩÎ°ú
AI_MODELS_ROOT = Path(__file__).parent.parent.parent / "ai_models"
CHECKPOINTS_ROOT = AI_MODELS_ROOT / "checkpoints"

# Î∂ÑÏÑùÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î™®Îç∏Îì§
ANALYZED_MODELS = {{
'''
        
        for model_name, model_info in self.analyzed_models.items():
            config_content += f'''    "{model_name}": {{
        "name": "{model_info['name']}",
        "type": "{model_info['type']}",
        "step": "{model_info['step']}",
        "path": CHECKPOINTS_ROOT / "{model_name}",
        "ready": {model_info['ready']},
        "size_mb": {model_info['total_size_mb']:.1f},
        "priority": {model_info['priority']},
        "checkpoints": {model_info['checkpoints'][:3]},  # ÏÉÅÏúÑ 3Í∞úÎßå
        "total_checkpoints": {model_info['total_checkpoints']}
    }},
'''
        
        config_content += '''}

# Îã®Í≥ÑÎ≥Ñ ÏµúÏ†Å Î™®Îç∏ Îß§Ìïë
STEP_OPTIMAL_MODELS = {
'''
        
        for step, best_model in step_optimal_models.items():
            config_content += f'''    "{step}": "{best_model}",
'''
        
        config_content += '''}

def get_optimal_model_for_step(step: str) -> Optional[str]:
    """Îã®Í≥ÑÎ≥Ñ ÏµúÏ†Å Î™®Îç∏ Î∞òÌôò"""
    return STEP_OPTIMAL_MODELS.get(step)

def get_model_checkpoints(model_name: str) -> List[Dict]:
    """Î™®Îç∏Ïùò Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î™©Î°ù Î∞òÌôò"""
    if model_name in ANALYZED_MODELS:
        return ANALYZED_MODELS[model_name]["checkpoints"]
    return []

def get_largest_checkpoint(model_name: str) -> Optional[str]:
    """Î™®Îç∏Ïùò Í∞ÄÏû• ÌÅ∞ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∞òÌôò (Î≥¥ÌÜµ Î©îÏù∏ Î™®Îç∏)"""
    checkpoints = get_model_checkpoints(model_name)
    if not checkpoints:
        return None
    
    largest = max(checkpoints, key=lambda x: x.get('size_mb', 0))
    return largest.get('path', largest.get('name'))

def get_ready_models_by_type(model_type: str) -> List[str]:
    """ÌÉÄÏûÖÎ≥Ñ ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏Îì§"""
    return [name for name, info in ANALYZED_MODELS.items() 
            if info["type"] == model_type and info["ready"]]

def get_diffusion_models() -> List[str]:
    """Diffusion Î™®Îç∏Îì§ (OOTDiffusion Îì±)"""
    return get_ready_models_by_type("diffusion")

def get_virtual_tryon_models() -> List[str]:
    """Í∞ÄÏÉÅ ÌîºÌåÖ Î™®Îç∏Îì§"""
    return get_ready_models_by_type("virtual_tryon")

def get_human_parsing_models() -> List[str]:
    """Ïù∏Ï≤¥ ÌååÏã± Î™®Îç∏Îì§"""
    return get_ready_models_by_type("human_parsing")

def get_model_info(model_name: str) -> Optional[Dict]:
    """Î™®Îç∏ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Î∞òÌôò"""
    return ANALYZED_MODELS.get(model_name)

def list_all_ready_models() -> Dict[str, Dict]:
    """Î™®Îì† ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Ï†ïÎ≥¥"""
    return ANALYZED_MODELS.copy()

def get_model_path(model_name: str) -> Optional[Path]:
    """Î™®Îç∏ ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú Î∞òÌôò"""
    if model_name in ANALYZED_MODELS:
        return ANALYZED_MODELS[model_name]["path"]
    return None

def get_checkpoint_path(model_name: str, checkpoint_name: Optional[str] = None) -> Optional[Path]:
    """ÌäπÏ†ï Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùº Í≤ΩÎ°ú Î∞òÌôò"""
    model_path = get_model_path(model_name)
    if not model_path:
        return None
    
    if checkpoint_name:
        return model_path / checkpoint_name
    else:
        # Í∞ÄÏû• ÌÅ∞ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∞òÌôò
        largest_ckpt = get_largest_checkpoint(model_name)
        return model_path / largest_ckpt if largest_ckpt else None

# ÏÇ¨Ïö© ÌÜµÍ≥Ñ
ANALYSIS_STATS = {{
    "total_models": {len(self.analyzed_models)},
    "ready_models": {len(self.analyzed_models)},
    "total_size_gb": {sum(m["total_size_mb"] for m in self.analyzed_models.values())/1024:.1f},
    "models_by_step": {{
        step: len([m for m in self.analyzed_models.values() if m["step"] == step])
        for step in set(m["step"] for m in self.analyzed_models.values())
    }},
    "largest_model": "{max(self.analyzed_models.items(), key=lambda x: x[1]["total_size_mb"])[0]}"
}}

# Îπ†Î•∏ Ï†ëÍ∑º Ìï®ÏàòÎì§
def get_best_diffusion_model() -> Optional[str]:
    """ÏµúÍ≥† ÏÑ±Îä• Diffusion Î™®Îç∏"""
    return get_optimal_model_for_step("step_06_virtual_fitting")

def get_best_human_parsing_model() -> Optional[str]:
    """ÏµúÍ≥† ÏÑ±Îä• Ïù∏Ï≤¥ ÌååÏã± Î™®Îç∏"""  
    return get_optimal_model_for_step("step_01_human_parsing")

def get_best_pose_model() -> Optional[str]:
    """ÏµúÍ≥† ÏÑ±Îä• Ìè¨Ï¶à Ï∂îÏ†ï Î™®Îç∏"""
    return get_optimal_model_for_step("step_02_pose_estimation")
'''
        
        # ÌååÏùº Ï†ÄÏû•
        config_path = self.backend_dir / "app" / "core" / "optimized_model_paths.py"
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        logger.info(f"‚úÖ ÏµúÏ†ÅÌôîÎêú Python ÏÑ§Ï†ï ÌååÏùº ÏÉùÏÑ±: {config_path}")
        
        # JSON ÏÑ§Ï†ï ÌååÏùºÎèÑ ÏÉùÏÑ±
        json_config = {
            "analyzed_models": self.analyzed_models,
            "step_optimal_models": step_optimal_models,
            "analysis_stats": {
                "total_models": len(self.analyzed_models),
                "ready_models": len(self.analyzed_models),
                "total_size_gb": sum(m["total_size_mb"] for m in self.analyzed_models.values())/1024,
                "largest_model": max(self.analyzed_models.items(), key=lambda x: x[1]["total_size_mb"])[0]
            }
        }
        
        json_path = self.backend_dir / "app" / "core" / "optimized_models.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ JSON ÏÑ§Ï†ï ÌååÏùº ÏÉùÏÑ±: {json_path}")
    
    def create_checkpoint_model_loader(self):
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î™®Îç∏ Î°úÎçî ÏÉùÏÑ±"""
        logger.info("üîß Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î™®Îç∏ Î°úÎçî ÏÉùÏÑ± Ï§ë...")
        
        loader_content = '''# app/ai_pipeline/utils/checkpoint_model_loader.py
"""
Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù Í∏∞Î∞ò ModelLoader ÏôÑÏ†Ñ Ïó∞Îèô
Ïã§Ï†ú Îã§Ïö¥Î°úÎìúÎêú 127.2GB Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Îì§ ÌôúÏö©
"""

import os
import torch
from pathlib import Path
from typing import Dict, Any, Optional, List
import logging

try:
    from app.core.optimized_model_paths import (
        ANALYZED_MODELS, get_optimal_model_for_step, 
        get_checkpoint_path, get_largest_checkpoint
    )
    OPTIMIZED_PATHS_AVAILABLE = True
except ImportError:
    OPTIMIZED_PATHS_AVAILABLE = False

logger = logging.getLogger(__name__)

class CheckpointModelLoader:
    """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù Í∏∞Î∞ò Î™®Îç∏ Î°úÎçî"""
    
    def __init__(self, device: str = "auto"):
        self.device = self._setup_device(device)
        self.models = {}
        self.loaded_models = {}
        
        if OPTIMIZED_PATHS_AVAILABLE:
            self._register_analyzed_models()
        else:
            logger.warning("‚ö†Ô∏è ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ Í≤ΩÎ°úÍ∞Ä ÏóÜÏäµÎãàÎã§")
    
    def _setup_device(self, device: str) -> str:
        """ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï"""
        if device == "auto":
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        return device
    
    def _register_analyzed_models(self):
        """Î∂ÑÏÑùÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î™®Îç∏Îì§ Îì±Î°ù"""
        if not OPTIMIZED_PATHS_AVAILABLE:
            return
            
        registered_count = 0
        
        for model_name, model_info in ANALYZED_MODELS.items():
            if not model_info["ready"]:
                continue
            
            try:
                # Î™®Îç∏ Ï†ïÎ≥¥ Îì±Î°ù
                self.models[model_name] = {
                    "name": model_info["name"],
                    "type": model_info["type"],
                    "step": model_info["step"],
                    "path": model_info["path"],
                    "checkpoints": model_info["checkpoints"],
                    "size_mb": model_info["size_mb"],
                    "priority": model_info["priority"]
                }
                
                registered_count += 1
                
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è {model_name} Îì±Î°ù Ïã§Ìå®: {e}")
        
        logger.info(f"üì¶ {registered_count}Í∞ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î™®Îç∏ Îì±Î°ù ÏôÑÎ£å")
    
    async def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """Î™®Îç∏ Î°úÎìú"""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name]
        
        if model_name not in self.models:
            logger.warning(f"‚ö†Ô∏è Îì±Î°ùÎêòÏßÄ ÏïäÏùÄ Î™®Îç∏: {model_name}")
            return None
        
        try:
            model_info = self.models[model_name]
            
            # Í∞ÄÏû• ÌÅ∞ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú Ï∞æÍ∏∞
            largest_checkpoint = get_largest_checkpoint(model_name)
            if not largest_checkpoint:
                logger.warning(f"‚ö†Ô∏è {model_name}Ïùò Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
                return None
            
            checkpoint_path = get_checkpoint_path(model_name, largest_checkpoint)
            
            if not checkpoint_path or not checkpoint_path.exists():
                logger.warning(f"‚ö†Ô∏è {model_name}Ïùò Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§: {checkpoint_path}")
                return None
            
            # PyTorch Î™®Îç∏ Î°úÎìú
            logger.info(f"üîß {model_name} Î°úÎî© Ï§ë... ({checkpoint_path})")
            
            # ÏïàÏ†ÑÌïú Î°úÎìú
            try:
                model = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
            except:
                # weights_onlyÍ∞Ä ÏßÄÏõêÎêòÏßÄ ÏïäÎäî Í≤ΩÏö∞ Ìè¥Î∞±
                model = torch.load(checkpoint_path, map_location=self.device)
            
            # Î™®Îç∏ Ï†ïÎ¶¨ Î∞è ÎîîÎ∞îÏù¥Ïä§ Ïù¥Îèô
            if isinstance(model, dict):
                if 'model' in model:
                    model = model['model']
                elif 'state_dict' in model:
                    model = model['state_dict']
            
            # Ï∫êÏãúÏóê Ï†ÄÏû•
            self.loaded_models[model_name] = model
            
            logger.info(f"‚úÖ {model_name} Î°úÎî© ÏôÑÎ£å")
            return model
            
        except Exception as e:
            logger.error(f"‚ùå {model_name} Î°úÎî© Ïã§Ìå®: {e}")
            return None
    
    async def load_optimal_model_for_step(self, step: str, **kwargs) -> Optional[Any]:
        """Îã®Í≥ÑÎ≥Ñ ÏµúÏ†Å Î™®Îç∏ Î°úÎìú"""
        optimal_model = get_optimal_model_for_step(step)
        if not optimal_model:
            logger.warning(f"‚ö†Ô∏è {step}Ïóê ÎåÄÌïú ÏµúÏ†Å Î™®Îç∏ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå")
            return None
        
        logger.info(f"üéØ {step} ÏµúÏ†Å Î™®Îç∏ Î°úÎìú: {optimal_model}")
        return await self.load_model(optimal_model, **kwargs)
    
    def get_model_info(self, model_name: str) -> Optional[Dict]:
        """Î™®Îç∏ Ï†ïÎ≥¥ Î∞òÌôò"""
        return self.models.get(model_name)
    
    def list_models(self) -> Dict[str, Dict]:
        """Îì±Î°ùÎêú Î™®Îç∏ Î™©Î°ù"""
        return self.models.copy()
    
    def clear_cache(self):
        """Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨"""
        self.loaded_models.clear()
        
        if self.device == "mps" and torch.backends.mps.is_available():
            torch.mps.empty_cache()
        elif self.device == "cuda" and torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("üßπ Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨ ÏôÑÎ£å")

# Ï†ÑÏó≠ Î™®Îç∏ Î°úÎçî
_global_checkpoint_loader: Optional[CheckpointModelLoader] = None

def get_checkpoint_model_loader(**kwargs) -> CheckpointModelLoader:
    """Ï†ÑÏó≠ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î™®Îç∏ Î°úÎçî Î∞òÌôò"""
    global _global_checkpoint_loader
    if _global_checkpoint_loader is None:
        _global_checkpoint_loader = CheckpointModelLoader(**kwargs)
    return _global_checkpoint_loader

async def load_best_model_for_step(step: str, **kwargs) -> Optional[Any]:
    """Îã®Í≥ÑÎ≥Ñ ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏ Î°úÎìú"""
    loader = get_checkpoint_model_loader()
    return await loader.load_optimal_model_for_step(step, **kwargs)

# Îπ†Î•∏ Ï†ëÍ∑º Ìï®ÏàòÎì§
async def load_best_diffusion_model(**kwargs) -> Optional[Any]:
    """ÏµúÍ≥† ÏÑ±Îä• Diffusion Î™®Îç∏ Î°úÎìú"""
    return await load_best_model_for_step("step_06_virtual_fitting", **kwargs)

async def load_best_human_parsing_model(**kwargs) -> Optional[Any]:
    """ÏµúÍ≥† ÏÑ±Îä• Ïù∏Ï≤¥ ÌååÏã± Î™®Îç∏ Î°úÎìú"""
    return await load_best_model_for_step("step_01_human_parsing", **kwargs)

async def load_best_pose_model(**kwargs) -> Optional[Any]:
    """ÏµúÍ≥† ÏÑ±Îä• Ìè¨Ï¶à Ï∂îÏ†ï Î™®Îç∏ Î°úÎìú"""
    return await load_best_model_for_step("step_02_pose_estimation", **kwargs)

async def load_best_cloth_segmentation_model(**kwargs) -> Optional[Any]:
    """ÏµúÍ≥† ÏÑ±Îä• ÏùòÎ•ò Î∂ÑÌï† Î™®Îç∏ Î°úÎìú"""
    return await load_best_model_for_step("step_03_cloth_segmentation", **kwargs)
'''
        
        # ÌååÏùº Ï†ÄÏû•
        loader_path = self.backend_dir / "app" / "ai_pipeline" / "utils" / "checkpoint_model_loader.py"
        loader_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(loader_path, 'w', encoding='utf-8') as f:
            f.write(loader_content)
        
        logger.info(f"‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î™®Îç∏ Î°úÎçî ÏÉùÏÑ±: {loader_path}")
    
    def create_test_scripts(self):
        """ÌÖåÏä§Ìä∏ Ïä§ÌÅ¨Î¶ΩÌä∏ ÏÉùÏÑ±"""
        logger.info("üß™ ÌÖåÏä§Ìä∏ Ïä§ÌÅ¨Î¶ΩÌä∏ ÏÉùÏÑ± Ï§ë...")
        
        # ÌÖåÏä§Ìä∏ ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
        test_dir = self.backend_dir / "scripts" / "test"
        test_dir.mkdir(parents=True, exist_ok=True)
        
        # __init__.py ÌååÏùº ÏÉùÏÑ±
        init_file = test_dir / "__init__.py"
        with open(init_file, 'w') as f:
            f.write('# Test scripts package\n')
        
        logger.info(f"‚úÖ ÌÖåÏä§Ìä∏ Ïä§ÌÅ¨Î¶ΩÌä∏ ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±: {test_dir}")

def main():
    """Î©îÏù∏ Ìï®Ïàò"""
    logger.info("üîç MyCloset AI - ÏàòÏ†ïÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑùÍ∏∞ ÏãúÏûë")
    logger.info("=" * 60)
    
    try:
        analyzer = FixedCheckpointAnalyzer()
        
        # ÏµúÏ†ÅÌôîÎêú ÏÑ§Ï†ï ÌååÏùº ÏÉùÏÑ±
        analyzer.create_optimized_model_config()
        
        # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î™®Îç∏ Î°úÎçî ÏÉùÏÑ±
        analyzer.create_checkpoint_model_loader()
        
        # ÌÖåÏä§Ìä∏ Ïä§ÌÅ¨Î¶ΩÌä∏ ÏÉùÏÑ±
        analyzer.create_test_scripts()
        
        logger.info("\nüéâ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑùÍ∏∞ ÏôÑÎ£å!")
        logger.info(f"üìä Î∂ÑÏÑùÎêú Î™®Îç∏: {len(analyzer.analyzed_models)}Í∞ú")
        total_size_gb = sum(m["total_size_mb"] for m in analyzer.analyzed_models.values()) / 1024
        logger.info(f"üíæ Ï¥ù ÌÅ¨Í∏∞: {total_size_gb:.1f}GB")
        
        logger.info("\nüìù ÏÉùÏÑ±Îêú ÌååÏùºÎì§:")
        logger.info("   ‚úÖ app/core/optimized_model_paths.py")
        logger.info("   ‚úÖ app/core/optimized_models.json")
        logger.info("   ‚úÖ app/ai_pipeline/utils/checkpoint_model_loader.py")
        logger.info("   ‚úÖ scripts/test/ (ÌÖåÏä§Ìä∏ ÎîîÎ†âÌÜ†Î¶¨)")
        
        logger.info("\nüöÄ Îã§Ïùå Îã®Í≥Ñ:")
        logger.info("   python scripts/test/test_model_loader.py  # Î™®Îç∏ Î°úÎçî ÌÖåÏä§Ìä∏")
        logger.info("   python app/main.py  # ÏÑúÎ≤Ñ Ïã§Ìñâ")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑùÍ∏∞ Ïã§Ìå®: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)