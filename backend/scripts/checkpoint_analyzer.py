#!/usr/bin/env python3
"""
ğŸ” MyCloset AI - ìˆ˜ì •ëœ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ê¸°
âœ… ê¸°ì¡´ 127.2GB ë¶„ì„ ê²°ê³¼ í™œìš©
âœ… ëˆ„ë½ëœ í‚¤ ë¬¸ì œ í•´ê²°
âœ… ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥
"""

import os
import sys
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
current_file = Path(__file__).resolve()
backend_dir = current_file.parent.parent
project_root = backend_dir.parent
sys.path.insert(0, str(backend_dir))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger(__name__)

class FixedCheckpointAnalyzer:
    """ìˆ˜ì •ëœ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.backend_dir = backend_dir
        self.checkpoints_dir = backend_dir / "ai_models" / "checkpoints"
        
        # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ëª¨ë¸ ë°ì´í„° (ì‹¤ì œ í„°ë¯¸ë„ ì¶œë ¥ ê¸°ë°˜)
        self.analyzed_models = self._create_analyzed_models_from_output()
        
        logger.info("ğŸ” ìˆ˜ì •ëœ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬: {self.checkpoints_dir}")
        logger.info(f"ğŸ“Š ë¶„ì„ëœ ëª¨ë¸: {len(self.analyzed_models)}ê°œ")
    
    def _create_analyzed_models_from_output(self) -> Dict[str, Dict]:
        """í„°ë¯¸ë„ ì¶œë ¥ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ ë°ì´í„° ìƒì„±"""
        
        # ì‹¤ì œ í„°ë¯¸ë„ ì¶œë ¥ì—ì„œ í™•ì¸ëœ ëª¨ë¸ë“¤
        models_data = {
            "ootdiffusion": {
                "name": "OOTDiffusion",
                "type": "diffusion",
                "step": "step_06_virtual_fitting",
                "ready": True,
                "total_size_mb": 15129.3,
                "priority": 1,
                "checkpoints": [
                    {"name": "pytorch_model.bin", "path": "pytorch_model.bin", "size_mb": 469.5},
                    {"name": "diffusion_pytorch_model.bin", "path": "diffusion_pytorch_model.bin", "size_mb": 319.2},
                    {"name": "body_pose_model.pth", "path": "body_pose_model.pth", "size_mb": 199.6}
                ],
                "total_checkpoints": 5
            },
            
            "ootdiffusion_hf": {
                "name": "OOTDiffusion HF",
                "type": "diffusion", 
                "step": "step_06_virtual_fitting",
                "ready": True,
                "total_size_mb": 15129.3,
                "priority": 1,
                "checkpoints": [
                    {"name": "pytorch_model.bin", "path": "pytorch_model.bin", "size_mb": 469.5},
                    {"name": "diffusion_pytorch_model.bin", "path": "diffusion_pytorch_model.bin", "size_mb": 319.2},
                    {"name": "body_pose_model.pth", "path": "body_pose_model.pth", "size_mb": 199.6}
                ],
                "total_checkpoints": 5
            },
            
            "stable-diffusion-v1-5": {
                "name": "Stable Diffusion v1.5",
                "type": "diffusion",
                "step": "step_06_virtual_fitting",
                "ready": True,
                "total_size_mb": 45070.6,
                "priority": 2,
                "checkpoints": [
                    {"name": "v1-5-pruned.ckpt", "path": "v1-5-pruned.ckpt", "size_mb": 7346.9},
                    {"name": "v1-5-pruned-emaonly.ckpt", "path": "v1-5-pruned-emaonly.ckpt", "size_mb": 4067.8},
                    {"name": "pytorch_model.fp16.bin", "path": "pytorch_model.fp16.bin", "size_mb": 234.8}
                ],
                "total_checkpoints": 11
            },
            
            "human_parsing": {
                "name": "Human Parsing",
                "type": "human_parsing",
                "step": "step_01_human_parsing",
                "ready": True,
                "total_size_mb": 1288.3,
                "priority": 3,
                "checkpoints": [
                    {"name": "schp_atr.pth", "path": "schp_atr.pth", "size_mb": 255.1},
                    {"name": "optimizer.pt", "path": "optimizer.pt", "size_mb": 209.0},
                    {"name": "rng_state.pth", "path": "rng_state.pth", "size_mb": 0.0}
                ],
                "total_checkpoints": 8
            },
            
            "step_01_human_parsing": {
                "name": "Step 01 Human Parsing",
                "type": "human_parsing",
                "step": "step_01_human_parsing", 
                "ready": True,
                "total_size_mb": 1787.7,
                "priority": 3,
                "checkpoints": [
                    {"name": "densepose_rcnn_R_50_FPN_s1x.pkl", "path": "densepose_rcnn_R_50_FPN_s1x.pkl", "size_mb": 243.9},
                    {"name": "graphonomy_lip.pth", "path": "graphonomy_lip.pth", "size_mb": 255.1},
                    {"name": "lightweight_parsing.pth", "path": "lightweight_parsing.pth", "size_mb": 0.5}
                ],
                "total_checkpoints": 11
            },
            
            "pose_estimation": {
                "name": "Pose Estimation",
                "type": "pose_estimation",
                "step": "step_02_pose_estimation",
                "ready": True,
                "total_size_mb": 10095.6,
                "priority": 4,
                "checkpoints": [
                    {"name": "sk_model.pth", "path": "sk_model.pth", "size_mb": 16.4},
                    {"name": "upernet_global_small.pth", "path": "upernet_global_small.pth", "size_mb": 196.8},
                    {"name": "latest_net_G.pth", "path": "latest_net_G.pth", "size_mb": 303.5}
                ],
                "total_checkpoints": 23
            },
            
            "step_02_pose_estimation": {
                "name": "Step 02 Pose Estimation",
                "type": "pose_estimation",
                "step": "step_02_pose_estimation",
                "ready": True,
                "total_size_mb": 273.6,
                "priority": 4,
                "checkpoints": [
                    {"name": "openpose.pth", "path": "openpose.pth", "size_mb": 199.6},
                    {"name": "yolov8n-pose.pt", "path": "yolov8n-pose.pt", "size_mb": 6.5}
                ],
                "total_checkpoints": 2
            },
            
            "openpose": {
                "name": "OpenPose",
                "type": "pose_estimation",
                "step": "step_02_pose_estimation",
                "ready": True,
                "total_size_mb": 539.7,
                "priority": 4,
                "checkpoints": [
                    {"name": "body_pose_model.pth", "path": "body_pose_model.pth", "size_mb": 199.6},
                    {"name": "hand_pose_model.pth", "path": "hand_pose_model.pth", "size_mb": 140.5}
                ],
                "total_checkpoints": 3
            },
            
            "cloth_segmentation": {
                "name": "Cloth Segmentation",
                "type": "cloth_segmentation",
                "step": "step_03_cloth_segmentation",
                "ready": True,
                "total_size_mb": 803.2,
                "priority": 5,
                "checkpoints": [
                    {"name": "model.pth", "path": "model.pth", "size_mb": 168.5},
                    {"name": "pytorch_model.bin", "path": "pytorch_model.bin", "size_mb": 168.4}
                ],
                "total_checkpoints": 2
            },
            
            "step_03_cloth_segmentation": {
                "name": "Step 03 Cloth Segmentation",
                "type": "cloth_segmentation",
                "step": "step_03_cloth_segmentation",
                "ready": True,
                "total_size_mb": 206.7,
                "priority": 5,
                "checkpoints": [
                    {"name": "mobile_sam.pt", "path": "mobile_sam.pt", "size_mb": 38.8}
                ],
                "total_checkpoints": 1
            },
            
            "step_04_geometric_matching": {
                "name": "Step 04 Geometric Matching",
                "type": "geometric_matching",
                "step": "step_04_geometric_matching",
                "ready": True,
                "total_size_mb": 33.2,
                "priority": 6,
                "checkpoints": [
                    {"name": "gmm_final.pth", "path": "gmm_final.pth", "size_mb": 4.1},
                    {"name": "lightweight_gmm.pth", "path": "lightweight_gmm.pth", "size_mb": 4.1},
                    {"name": "tps_network.pth", "path": "tps_network.pth", "size_mb": 2.1}
                ],
                "total_checkpoints": 4
            },
            
            "step_05_cloth_warping": {
                "name": "Step 05 Cloth Warping",
                "type": "cloth_warping",
                "step": "step_05_cloth_warping",
                "ready": True,
                "total_size_mb": 3279.2,
                "priority": 7,
                "checkpoints": [
                    {"name": "tom_final.pth", "path": "tom_final.pth", "size_mb": 3279.1},
                    {"name": "lightweight_warping.pth", "path": "lightweight_warping.pth", "size_mb": 0.1}
                ],
                "total_checkpoints": 2
            },
            
            "step_06_virtual_fitting": {
                "name": "Step 06 Virtual Fitting",
                "type": "virtual_tryon",
                "step": "step_06_virtual_fitting",
                "ready": True,
                "total_size_mb": 20854.2,
                "priority": 1,
                "checkpoints": [
                    {"name": "hrviton_final.pth", "path": "hrviton_final.pth", "size_mb": 2445.7},
                    {"name": "diffusion_pytorch_model.bin", "path": "diffusion_pytorch_model.bin", "size_mb": 3279.1},
                    {"name": "pytorch_model.bin", "path": "pytorch_model.bin", "size_mb": 469.5}
                ],
                "total_checkpoints": 7
            },
            
            "step_07_post_processing": {
                "name": "Step 07 Post Processing",
                "type": "auxiliary",
                "step": "step_07_post_processing",
                "ready": True,
                "total_size_mb": 63.9,
                "priority": 8,
                "checkpoints": [
                    {"name": "RealESRGAN_x4plus.pth", "path": "RealESRGAN_x4plus.pth", "size_mb": 63.9}
                ],
                "total_checkpoints": 1
            },
            
            "sam": {
                "name": "SAM (Segment Anything Model)",
                "type": "auxiliary",
                "step": "auxiliary",
                "ready": True,
                "total_size_mb": 2445.7,
                "priority": 8,
                "checkpoints": [
                    {"name": "sam_vit_h_4b8939.pth", "path": "sam_vit_h_4b8939.pth", "size_mb": 2445.7}
                ],
                "total_checkpoints": 1
            },
            
            "clip-vit-base-patch32": {
                "name": "CLIP ViT Base",
                "type": "text_image",
                "step": "auxiliary",
                "ready": True,
                "total_size_mb": 580.7,
                "priority": 9,
                "checkpoints": [
                    {"name": "pytorch_model.bin", "path": "pytorch_model.bin", "size_mb": 577.2}
                ],
                "total_checkpoints": 1
            },
            
            "grounding_dino": {
                "name": "Grounding DINO",
                "type": "auxiliary",
                "step": "auxiliary",
                "ready": True,
                "total_size_mb": 1318.2,
                "priority": 9,
                "checkpoints": [
                    {"name": "pytorch_model.bin", "path": "pytorch_model.bin", "size_mb": 659.9}
                ],
                "total_checkpoints": 1
            }
        }
        
        # ê° ëª¨ë¸ì— ê²½ë¡œ ì •ë³´ ì¶”ê°€
        for model_name, model_info in models_data.items():
            model_info["path"] = str(self.checkpoints_dir / model_name)
        
        return models_data
    
    def create_optimized_model_config(self):
        """ìµœì í™”ëœ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
        logger.info("ğŸ“ ìµœì í™”ëœ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")
        
        # ë‹¨ê³„ë³„ ìµœì  ëª¨ë¸ ë§¤í•‘
        step_optimal_models = {
            "step_01_human_parsing": "step_01_human_parsing",
            "step_02_pose_estimation": "step_02_pose_estimation", 
            "step_03_cloth_segmentation": "step_03_cloth_segmentation",
            "step_04_geometric_matching": "step_04_geometric_matching",
            "step_05_cloth_warping": "step_05_cloth_warping",
            "step_06_virtual_fitting": "step_06_virtual_fitting",
            "step_07_post_processing": "step_07_post_processing",
            "auxiliary": "sam"
        }
        
        # Python ì„¤ì • íŒŒì¼ ìƒì„±
        config_content = f'''# app/core/optimized_model_paths.py
"""
ìµœì í™”ëœ AI ëª¨ë¸ ê²½ë¡œ ì„¤ì • - ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜
ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ë“¤ë¡œë§Œ êµ¬ì„±
ìƒì„±ì¼: {time.strftime("%Y-%m-%d %H:%M:%S")}
ë¶„ì„ëœ ëª¨ë¸: {len(self.analyzed_models)}ê°œ
ì´ í¬ê¸°: {sum(m["total_size_mb"] for m in self.analyzed_models.values())/1024:.1f}GB
"""

from pathlib import Path
from typing import Dict, Optional, List, Any

# ê¸°ë³¸ ê²½ë¡œ
AI_MODELS_ROOT = Path(__file__).parent.parent.parent / "ai_models"
CHECKPOINTS_ROOT = AI_MODELS_ROOT / "checkpoints"

# ë¶„ì„ëœ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ë“¤
ANALYZED_MODELS = {{
'''
        
        for model_name, model_info in self.analyzed_models.items():
            config_content += f'''    "{model_name}": {{
        "name": "{model_info['name']}",
        "type": "{model_info['type']}",
        "step": "{model_info['step']}",
        "path": CHECKPOINTS_ROOT / "{model_name}",
        "ready": {model_info['ready']},
        "size_mb": {model_info['total_size_mb']:.1f},
        "priority": {model_info['priority']},
        "checkpoints": {model_info['checkpoints'][:3]},  # ìƒìœ„ 3ê°œë§Œ
        "total_checkpoints": {model_info['total_checkpoints']}
    }},
'''
        
        config_content += '''}

# ë‹¨ê³„ë³„ ìµœì  ëª¨ë¸ ë§¤í•‘
STEP_OPTIMAL_MODELS = {
'''
        
        for step, best_model in step_optimal_models.items():
            config_content += f'''    "{step}": "{best_model}",
'''
        
        config_content += '''}

def get_optimal_model_for_step(step: str) -> Optional[str]:
    """ë‹¨ê³„ë³„ ìµœì  ëª¨ë¸ ë°˜í™˜"""
    return STEP_OPTIMAL_MODELS.get(step)

def get_model_checkpoints(model_name: str) -> List[Dict]:
    """ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ë°˜í™˜"""
    if model_name in ANALYZED_MODELS:
        return ANALYZED_MODELS[model_name]["checkpoints"]
    return []

def get_largest_checkpoint(model_name: str) -> Optional[str]:
    """ëª¨ë¸ì˜ ê°€ì¥ í° ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜ (ë³´í†µ ë©”ì¸ ëª¨ë¸)"""
    checkpoints = get_model_checkpoints(model_name)
    if not checkpoints:
        return None
    
    largest = max(checkpoints, key=lambda x: x.get('size_mb', 0))
    return largest.get('path', largest.get('name'))

def get_ready_models_by_type(model_type: str) -> List[str]:
    """íƒ€ì…ë³„ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤"""
    return [name for name, info in ANALYZED_MODELS.items() 
            if info["type"] == model_type and info["ready"]]

def get_diffusion_models() -> List[str]:
    """Diffusion ëª¨ë¸ë“¤ (OOTDiffusion ë“±)"""
    return get_ready_models_by_type("diffusion")

def get_virtual_tryon_models() -> List[str]:
    """ê°€ìƒ í”¼íŒ… ëª¨ë¸ë“¤"""
    return get_ready_models_by_type("virtual_tryon")

def get_human_parsing_models() -> List[str]:
    """ì¸ì²´ íŒŒì‹± ëª¨ë¸ë“¤"""
    return get_ready_models_by_type("human_parsing")

def get_model_info(model_name: str) -> Optional[Dict]:
    """ëª¨ë¸ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
    return ANALYZED_MODELS.get(model_name)

def list_all_ready_models() -> Dict[str, Dict]:
    """ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì •ë³´"""
    return ANALYZED_MODELS.copy()

def get_model_path(model_name: str) -> Optional[Path]:
    """ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë°˜í™˜"""
    if model_name in ANALYZED_MODELS:
        return ANALYZED_MODELS[model_name]["path"]
    return None

def get_checkpoint_path(model_name: str, checkpoint_name: Optional[str] = None) -> Optional[Path]:
    """íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    model_path = get_model_path(model_name)
    if not model_path:
        return None
    
    if checkpoint_name:
        return model_path / checkpoint_name
    else:
        # ê°€ì¥ í° ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜
        largest_ckpt = get_largest_checkpoint(model_name)
        return model_path / largest_ckpt if largest_ckpt else None

# ì‚¬ìš© í†µê³„
ANALYSIS_STATS = {{
    "total_models": {len(self.analyzed_models)},
    "ready_models": {len(self.analyzed_models)},
    "total_size_gb": {sum(m["total_size_mb"] for m in self.analyzed_models.values())/1024:.1f},
    "models_by_step": {{
        step: len([m for m in self.analyzed_models.values() if m["step"] == step])
        for step in set(m["step"] for m in self.analyzed_models.values())
    }},
    "largest_model": "{max(self.analyzed_models.items(), key=lambda x: x[1]["total_size_mb"])[0]}"
}}

# ë¹ ë¥¸ ì ‘ê·¼ í•¨ìˆ˜ë“¤
def get_best_diffusion_model() -> Optional[str]:
    """ìµœê³  ì„±ëŠ¥ Diffusion ëª¨ë¸"""
    return get_optimal_model_for_step("step_06_virtual_fitting")

def get_best_human_parsing_model() -> Optional[str]:
    """ìµœê³  ì„±ëŠ¥ ì¸ì²´ íŒŒì‹± ëª¨ë¸"""  
    return get_optimal_model_for_step("step_01_human_parsing")

def get_best_pose_model() -> Optional[str]:
    """ìµœê³  ì„±ëŠ¥ í¬ì¦ˆ ì¶”ì • ëª¨ë¸"""
    return get_optimal_model_for_step("step_02_pose_estimation")
'''
        
        # íŒŒì¼ ì €ì¥
        config_path = self.backend_dir / "app" / "core" / "optimized_model_paths.py"
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        logger.info(f"âœ… ìµœì í™”ëœ Python ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
        
        # JSON ì„¤ì • íŒŒì¼ë„ ìƒì„±
        json_config = {
            "analyzed_models": self.analyzed_models,
            "step_optimal_models": step_optimal_models,
            "analysis_stats": {
                "total_models": len(self.analyzed_models),
                "ready_models": len(self.analyzed_models),
                "total_size_gb": sum(m["total_size_mb"] for m in self.analyzed_models.values())/1024,
                "largest_model": max(self.analyzed_models.items(), key=lambda x: x[1]["total_size_mb"])[0]
            }
        }
        
        json_path = self.backend_dir / "app" / "core" / "optimized_models.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… JSON ì„¤ì • íŒŒì¼ ìƒì„±: {json_path}")
    
    def create_checkpoint_model_loader(self):
        """ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë” ìƒì„±"""
        logger.info("ğŸ”§ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë” ìƒì„± ì¤‘...")
        
        loader_content = '''# app/ai_pipeline/utils/checkpoint_model_loader.py
"""
ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ ModelLoader ì™„ì „ ì—°ë™
ì‹¤ì œ ë‹¤ìš´ë¡œë“œëœ 127.2GB ì²´í¬í¬ì¸íŠ¸ë“¤ í™œìš©
"""

import os
import torch
from pathlib import Path
from typing import Dict, Any, Optional, List
import logging

try:
    from app.core.optimized_model_paths import (
        ANALYZED_MODELS, get_optimal_model_for_step, 
        get_checkpoint_path, get_largest_checkpoint
    )
    OPTIMIZED_PATHS_AVAILABLE = True
except ImportError:
    OPTIMIZED_PATHS_AVAILABLE = False

logger = logging.getLogger(__name__)

class CheckpointModelLoader:
    """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê¸°ë°˜ ëª¨ë¸ ë¡œë”"""
    
    def __init__(self, device: str = "auto"):
        self.device = self._setup_device(device)
        self.models = {}
        self.loaded_models = {}
        
        if OPTIMIZED_PATHS_AVAILABLE:
            self._register_analyzed_models()
        else:
            logger.warning("âš ï¸ ìµœì í™”ëœ ëª¨ë¸ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤")
    
    def _setup_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device == "auto":
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        return device
    
    def _register_analyzed_models(self):
        """ë¶„ì„ëœ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ë“¤ ë“±ë¡"""
        if not OPTIMIZED_PATHS_AVAILABLE:
            return
            
        registered_count = 0
        
        for model_name, model_info in ANALYZED_MODELS.items():
            if not model_info["ready"]:
                continue
            
            try:
                # ëª¨ë¸ ì •ë³´ ë“±ë¡
                self.models[model_name] = {
                    "name": model_info["name"],
                    "type": model_info["type"],
                    "step": model_info["step"],
                    "path": model_info["path"],
                    "checkpoints": model_info["checkpoints"],
                    "size_mb": model_info["size_mb"],
                    "priority": model_info["priority"]
                }
                
                registered_count += 1
                
            except Exception as e:
                logger.warning(f"   âš ï¸ {model_name} ë“±ë¡ ì‹¤íŒ¨: {e}")
        
        logger.info(f"ğŸ“¦ {registered_count}ê°œ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ")
    
    async def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ"""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name]
        
        if model_name not in self.models:
            logger.warning(f"âš ï¸ ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {model_name}")
            return None
        
        try:
            model_info = self.models[model_name]
            
            # ê°€ì¥ í° ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì°¾ê¸°
            largest_checkpoint = get_largest_checkpoint(model_name)
            if not largest_checkpoint:
                logger.warning(f"âš ï¸ {model_name}ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            checkpoint_path = get_checkpoint_path(model_name, largest_checkpoint)
            
            if not checkpoint_path or not checkpoint_path.exists():
                logger.warning(f"âš ï¸ {model_name}ì˜ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
                return None
            
            # PyTorch ëª¨ë¸ ë¡œë“œ
            logger.info(f"ğŸ”§ {model_name} ë¡œë”© ì¤‘... ({checkpoint_path})")
            
            # ì•ˆì „í•œ ë¡œë“œ
            try:
                model = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
            except:
                # weights_onlyê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° í´ë°±
                model = torch.load(checkpoint_path, map_location=self.device)
            
            # ëª¨ë¸ ì •ë¦¬ ë° ë””ë°”ì´ìŠ¤ ì´ë™
            if isinstance(model, dict):
                if 'model' in model:
                    model = model['model']
                elif 'state_dict' in model:
                    model = model['state_dict']
            
            # ìºì‹œì— ì €ì¥
            self.loaded_models[model_name] = model
            
            logger.info(f"âœ… {model_name} ë¡œë”© ì™„ë£Œ")
            return model
            
        except Exception as e:
            logger.error(f"âŒ {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    async def load_optimal_model_for_step(self, step: str, **kwargs) -> Optional[Any]:
        """ë‹¨ê³„ë³„ ìµœì  ëª¨ë¸ ë¡œë“œ"""
        optimal_model = get_optimal_model_for_step(step)
        if not optimal_model:
            logger.warning(f"âš ï¸ {step}ì— ëŒ€í•œ ìµœì  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
        
        logger.info(f"ğŸ¯ {step} ìµœì  ëª¨ë¸ ë¡œë“œ: {optimal_model}")
        return await self.load_model(optimal_model, **kwargs)
    
    def get_model_info(self, model_name: str) -> Optional[Dict]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return self.models.get(model_name)
    
    def list_models(self) -> Dict[str, Dict]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        return self.models.copy()
    
    def clear_cache(self):
        """ëª¨ë¸ ìºì‹œ ì •ë¦¬"""
        self.loaded_models.clear()
        
        if self.device == "mps" and torch.backends.mps.is_available():
            torch.mps.empty_cache()
        elif self.device == "cuda" and torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("ğŸ§¹ ëª¨ë¸ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

# ì „ì—­ ëª¨ë¸ ë¡œë”
_global_checkpoint_loader: Optional[CheckpointModelLoader] = None

def get_checkpoint_model_loader(**kwargs) -> CheckpointModelLoader:
    """ì „ì—­ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë” ë°˜í™˜"""
    global _global_checkpoint_loader
    if _global_checkpoint_loader is None:
        _global_checkpoint_loader = CheckpointModelLoader(**kwargs)
    return _global_checkpoint_loader

async def load_best_model_for_step(step: str, **kwargs) -> Optional[Any]:
    """ë‹¨ê³„ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ"""
    loader = get_checkpoint_model_loader()
    return await loader.load_optimal_model_for_step(step, **kwargs)

# ë¹ ë¥¸ ì ‘ê·¼ í•¨ìˆ˜ë“¤
async def load_best_diffusion_model(**kwargs) -> Optional[Any]:
    """ìµœê³  ì„±ëŠ¥ Diffusion ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_06_virtual_fitting", **kwargs)

async def load_best_human_parsing_model(**kwargs) -> Optional[Any]:
    """ìµœê³  ì„±ëŠ¥ ì¸ì²´ íŒŒì‹± ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_01_human_parsing", **kwargs)

async def load_best_pose_model(**kwargs) -> Optional[Any]:
    """ìµœê³  ì„±ëŠ¥ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_02_pose_estimation", **kwargs)

async def load_best_cloth_segmentation_model(**kwargs) -> Optional[Any]:
    """ìµœê³  ì„±ëŠ¥ ì˜ë¥˜ ë¶„í•  ëª¨ë¸ ë¡œë“œ"""
    return await load_best_model_for_step("step_03_cloth_segmentation", **kwargs)
'''
        
        # íŒŒì¼ ì €ì¥
        loader_path = self.backend_dir / "app" / "ai_pipeline" / "utils" / "checkpoint_model_loader.py"
        loader_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(loader_path, 'w', encoding='utf-8') as f:
            f.write(loader_content)
        
        logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë” ìƒì„±: {loader_path}")
    
    def create_test_scripts(self):
        """í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘...")
        
        # í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
        test_dir = self.backend_dir / "scripts" / "test"
        test_dir.mkdir(parents=True, exist_ok=True)
        
        # __init__.py íŒŒì¼ ìƒì„±
        init_file = test_dir / "__init__.py"
        with open(init_file, 'w') as f:
            f.write('# Test scripts package\n')
        
        logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±: {test_dir}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸ” MyCloset AI - ìˆ˜ì •ëœ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ê¸° ì‹œì‘")
    logger.info("=" * 60)
    
    try:
        analyzer = FixedCheckpointAnalyzer()
        
        # ìµœì í™”ëœ ì„¤ì • íŒŒì¼ ìƒì„±
        analyzer.create_optimized_model_config()
        
        # ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë” ìƒì„±
        analyzer.create_checkpoint_model_loader()
        
        # í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        analyzer.create_test_scripts()
        
        logger.info("\nğŸ‰ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ê¸° ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ë¶„ì„ëœ ëª¨ë¸: {len(analyzer.analyzed_models)}ê°œ")
        total_size_gb = sum(m["total_size_mb"] for m in analyzer.analyzed_models.values()) / 1024
        logger.info(f"ğŸ’¾ ì´ í¬ê¸°: {total_size_gb:.1f}GB")
        
        logger.info("\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        logger.info("   âœ… app/core/optimized_model_paths.py")
        logger.info("   âœ… app/core/optimized_models.json")
        logger.info("   âœ… app/ai_pipeline/utils/checkpoint_model_loader.py")
        logger.info("   âœ… scripts/test/ (í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬)")
        
        logger.info("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info("   python scripts/test/test_model_loader.py  # ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸")
        logger.info("   python app/main.py  # ì„œë²„ ì‹¤í–‰")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ê¸° ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)