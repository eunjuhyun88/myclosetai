#!/usr/bin/env python3
"""
ğŸ”§ MyCloset AI - ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
âœ… ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
âœ… M3 Max 128GB ìµœì í™”
âœ… ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ê²€ì¦
"""

import os
import sys
import time
import asyncio
import logging
import json
from pathlib import Path
from typing import Dict, Any, Optional, List
import psutil

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
current_file = Path(__file__).resolve()
backend_dir = current_file.parent.parent
project_root = backend_dir.parent
sys.path.insert(0, str(backend_dir))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger(__name__)

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    from app.core.optimized_model_paths import (
        ANALYZED_MODELS, get_optimal_model_for_step,
        get_model_checkpoints, get_largest_checkpoint
    )
    OPTIMIZED_PATHS_AVAILABLE = True
except ImportError:
    OPTIMIZED_PATHS_AVAILABLE = False

try:
    from app.ai_pipeline.utils.checkpoint_model_loader import (
        CheckpointModelLoader, get_checkpoint_model_loader
    )
    CHECKPOINT_LOADER_AVAILABLE = True
except ImportError:
    CHECKPOINT_LOADER_AVAILABLE = False

class ModelLoaderTester:
    """ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.backend_dir = backend_dir
        self.device = self._detect_device()
        self.test_results = {}
        
        logger.info(f"ğŸ”§ ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸ ì´ˆê¸°í™”")
        logger.info(f"ğŸ“ ë°±ì—”ë“œ ë””ë ‰í† ë¦¬: {self.backend_dir}")
        logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        
    def _detect_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ íƒì§€"""
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
        return "cpu"
    
    def test_optimized_paths(self) -> bool:
        """ìµœì í™”ëœ ê²½ë¡œ ì„¤ì • í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ“‹ 1. ìµœì í™”ëœ ê²½ë¡œ ì„¤ì • í…ŒìŠ¤íŠ¸")
        
        if not OPTIMIZED_PATHS_AVAILABLE:
            logger.error("âŒ app.core.optimized_model_paths ëª¨ë“ˆì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            logger.info("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ìƒì„±í•˜ì„¸ìš”: python scripts/checkpoint_analyzer.py")
            return False
        
        try:
            # ë¶„ì„ëœ ëª¨ë¸ ì •ë³´ í™•ì¸
            model_count = len(ANALYZED_MODELS)
            logger.info(f"   âœ… ë¶„ì„ëœ ëª¨ë¸: {model_count}ê°œ")
            
            # ë‹¨ê³„ë³„ ìµœì  ëª¨ë¸ í™•ì¸
            steps = [
                "step_01_human_parsing",
                "step_02_pose_estimation", 
                "step_03_cloth_segmentation",
                "step_06_virtual_fitting"
            ]
            
            for step in steps:
                optimal_model = get_optimal_model_for_step(step)
                if optimal_model:
                    logger.info(f"   âœ… {step}: {optimal_model}")
                else:
                    logger.warning(f"   âš ï¸ {step}: ìµœì  ëª¨ë¸ ì—†ìŒ")
            
            self.test_results["optimized_paths"] = True
            return True
            
        except Exception as e:
            logger.error(f"âŒ ìµœì í™”ëœ ê²½ë¡œ ì„¤ì • í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.test_results["optimized_paths"] = False
            return False
    
    def test_checkpoint_files(self) -> bool:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        logger.info("ğŸ“‹ 2. ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì¡´ì¬ í™•ì¸")
        
        if not OPTIMIZED_PATHS_AVAILABLE:
            logger.error("âŒ ìµœì í™”ëœ ê²½ë¡œ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤")
            return False
        
        try:
            existing_models = 0
            missing_models = 0
            
            for model_name, model_info in ANALYZED_MODELS.items():
                if not model_info.get('ready', False):
                    continue
                    
                # ëª¨ë¸ ê²½ë¡œ í™•ì¸
                model_path = model_info.get('path')
                if model_path and Path(model_path).exists():
                    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ í™•ì¸
                    checkpoints = model_info.get('checkpoints', [])
                    if checkpoints:
                        largest_checkpoint = max(checkpoints, key=lambda x: x.get('size_mb', 0))
                        checkpoint_path = Path(model_path) / largest_checkpoint['path']
                        
                        if checkpoint_path.exists():
                            size_mb = checkpoint_path.stat().st_size / (1024 * 1024)
                            logger.info(f"   âœ… {model_name}: {largest_checkpoint['name']} ({size_mb:.1f}MB)")
                            existing_models += 1
                        else:
                            logger.warning(f"   âŒ {model_name}: {checkpoint_path} ì—†ìŒ")
                            missing_models += 1
                    else:
                        logger.warning(f"   âš ï¸ {model_name}: ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì—†ìŒ")
                        missing_models += 1
                else:
                    logger.warning(f"   âŒ {model_name}: {model_path} ê²½ë¡œ ì—†ìŒ")
                    missing_models += 1
            
            logger.info(f"   ğŸ“Š ì¡´ì¬í•˜ëŠ” ëª¨ë¸: {existing_models}ê°œ, ëˆ„ë½ëœ ëª¨ë¸: {missing_models}ê°œ")
            
            self.test_results["checkpoint_files"] = {
                "existing": existing_models,
                "missing": missing_models,
                "success": missing_models == 0
            }
            
            return missing_models == 0
            
        except Exception as e:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸ ì‹¤íŒ¨: {e}")
            self.test_results["checkpoint_files"] = {"success": False, "error": str(e)}
            return False
    
    async def test_model_loader_creation(self) -> bool:
        """ëª¨ë¸ ë¡œë” ìƒì„± í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ“‹ 3. ëª¨ë¸ ë¡œë” ìƒì„± í…ŒìŠ¤íŠ¸")
        
        if not CHECKPOINT_LOADER_AVAILABLE:
            logger.error("âŒ CheckpointModelLoaderë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            logger.info("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ìƒì„±í•˜ì„¸ìš”: python scripts/checkpoint_analyzer.py")
            return False
        
        try:
            # ëª¨ë¸ ë¡œë” ìƒì„±
            logger.info("   ğŸ”§ CheckpointModelLoader ìƒì„± ì¤‘...")
            model_loader = CheckpointModelLoader(device=self.device)
            
            # ë“±ë¡ëœ ëª¨ë¸ í™•ì¸
            registered_models = len(model_loader.models)
            logger.info(f"   âœ… ë“±ë¡ëœ ëª¨ë¸: {registered_models}ê°œ")
            
            # ê¸€ë¡œë²Œ ëª¨ë¸ ë¡œë” í™•ì¸
            global_loader = get_checkpoint_model_loader(device=self.device)
            global_models = len(global_loader.models)
            logger.info(f"   âœ… ê¸€ë¡œë²Œ ë¡œë” ëª¨ë¸: {global_models}ê°œ")
            
            self.test_results["model_loader_creation"] = {
                "registered_models": registered_models,
                "global_models": global_models,
                "success": registered_models > 0
            }
            
            return registered_models > 0
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë” ìƒì„± ì‹¤íŒ¨: {e}")
            self.test_results["model_loader_creation"] = {"success": False, "error": str(e)}
            return False
    
    async def test_pytorch_model_loading(self) -> bool:
        """PyTorch ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ“‹ 4. PyTorch ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
        
        if not TORCH_AVAILABLE:
            logger.error("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False
        
        if not CHECKPOINT_LOADER_AVAILABLE:
            logger.error("âŒ CheckpointModelLoaderë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        try:
            model_loader = get_checkpoint_model_loader(device=self.device)
            
            # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ì„ íƒ (ê°€ì¥ ì‘ì€ ëª¨ë¸ë¶€í„°)
            test_models = [
                "step_04_geometric_matching",
                "step_03_cloth_segmentation",
                "step_01_human_parsing"
            ]
            
            loaded_models = 0
            failed_models = 0
            
            for model_name in test_models:
                try:
                    logger.info(f"   ğŸ”§ {model_name} ë¡œë”© ì‹œë„...")
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                    memory_before = psutil.virtual_memory().percent
                    
                    # ëª¨ë¸ ë¡œë”© ì‹œë„
                    model = await model_loader.load_optimal_model_for_step(model_name)
                    
                    if model:
                        memory_after = psutil.virtual_memory().percent
                        memory_used = memory_after - memory_before
                        
                        logger.info(f"   âœ… {model_name} ë¡œë”© ì„±ê³µ")
                        logger.info(f"   ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.1f}% ì¦ê°€")
                        loaded_models += 1
                    else:
                        logger.warning(f"   âŒ {model_name} ë¡œë”© ì‹¤íŒ¨ (None ë°˜í™˜)")
                        failed_models += 1
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    if TORCH_AVAILABLE and self.device == "mps":
                        torch.mps.empty_cache()
                    elif TORCH_AVAILABLE and self.device == "cuda":
                        torch.cuda.empty_cache()
                    
                except Exception as e:
                    logger.error(f"   âŒ {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
                    failed_models += 1
                
                # ë©”ëª¨ë¦¬ ê³¼ë¶€í•˜ ë°©ì§€
                await asyncio.sleep(1)
            
            logger.info(f"   ğŸ“Š ë¡œë”© ì„±ê³µ: {loaded_models}ê°œ, ì‹¤íŒ¨: {failed_models}ê°œ")
            
            self.test_results["pytorch_loading"] = {
                "loaded": loaded_models,
                "failed": failed_models,
                "success": loaded_models > 0
            }
            
            return loaded_models > 0
            
        except Exception as e:
            logger.error(f"âŒ PyTorch ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.test_results["pytorch_loading"] = {"success": False, "error": str(e)}
            return False
    
    def test_memory_optimization(self) -> bool:
        """ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ“‹ 5. ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸")
        
        try:
            # ë©”ëª¨ë¦¬ ì •ë³´ í™•ì¸
            memory_info = psutil.virtual_memory()
            total_gb = memory_info.total / (1024**3)
            available_gb = memory_info.available / (1024**3)
            used_percent = memory_info.percent
            
            logger.info(f"   ğŸ“Š ì´ ë©”ëª¨ë¦¬: {total_gb:.1f}GB")
            logger.info(f"   ğŸ“Š ì‚¬ìš© ê°€ëŠ¥: {available_gb:.1f}GB")
            logger.info(f"   ğŸ“Š ì‚¬ìš©ë¥ : {used_percent:.1f}%")
            
            # M3 Max ìµœì í™” í™•ì¸
            is_m3_max = (
                sys.platform == "darwin" and 
                os.uname().machine == "arm64" and
                total_gb > 100  # 128GB ë©”ëª¨ë¦¬
            )
            
            if is_m3_max:
                logger.info("   âœ… M3 Max 128GB í™˜ê²½ ê°ì§€ë¨")
                
                # MPS ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
                if TORCH_AVAILABLE and torch.backends.mps.is_available():
                    logger.info("   âœ… MPS (Metal Performance Shaders) ì‚¬ìš© ê°€ëŠ¥")
                    
                    # MPS ìµœì í™” í™˜ê²½ ë³€ìˆ˜ í™•ì¸
                    mps_env = os.environ.get('PYTORCH_ENABLE_MPS_FALLBACK', '0')
                    logger.info(f"   ğŸ“Š MPS í´ë°±: {mps_env}")
                    
                    # Neural Engine ìµœì í™” í™•ì¸
                    omp_threads = os.environ.get('OMP_NUM_THREADS', '1')
                    logger.info(f"   ğŸ“Š OpenMP ìŠ¤ë ˆë“œ: {omp_threads}")
                    
                else:
                    logger.warning("   âš ï¸ MPSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            else:
                logger.info("   â„¹ï¸ ì¼ë°˜ ì‹œìŠ¤í…œ í™˜ê²½")
            
            self.test_results["memory_optimization"] = {
                "total_gb": total_gb,
                "available_gb": available_gb,
                "used_percent": used_percent,
                "is_m3_max": is_m3_max,
                "mps_available": TORCH_AVAILABLE and torch.backends.mps.is_available() if TORCH_AVAILABLE else False,
                "success": available_gb > 10  # ìµœì†Œ 10GB ì—¬ìœ  ë©”ëª¨ë¦¬
            }
            
            return available_gb > 10
            
        except Exception as e:
            logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.test_results["memory_optimization"] = {"success": False, "error": str(e)}
            return False
    
    def generate_test_report(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "device": self.device,
            "torch_available": TORCH_AVAILABLE,
            "test_results": self.test_results,
            "summary": {
                "total_tests": len(self.test_results),
                "passed_tests": sum(1 for result in self.test_results.values() 
                                  if isinstance(result, dict) and result.get('success', False) or result is True),
                "failed_tests": sum(1 for result in self.test_results.values() 
                                  if isinstance(result, dict) and not result.get('success', True) or result is False),
                "overall_success": all(
                    result.get('success', False) if isinstance(result, dict) else result 
                    for result in self.test_results.values()
                )
            }
        }
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = self.backend_dir / "scripts" / "test" / "model_loader_test_report.json"
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥: {report_path}")
        return report

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("ğŸ”§ MyCloset AI - ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 60)
    
    tester = ModelLoaderTester()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tests = [
        ("ìµœì í™”ëœ ê²½ë¡œ ì„¤ì •", tester.test_optimized_paths),
        ("ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì¡´ì¬", tester.test_checkpoint_files),
        ("ëª¨ë¸ ë¡œë” ìƒì„±", tester.test_model_loader_creation),
        ("PyTorch ëª¨ë¸ ë¡œë”©", tester.test_pytorch_model_loading),
        ("ë©”ëª¨ë¦¬ ìµœì í™”", tester.test_memory_optimization)
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        try:
            logger.info(f"\nğŸ“‹ {test_name} í…ŒìŠ¤íŠ¸ ì‹œì‘...")
            
            if asyncio.iscoroutinefunction(test_func):
                success = await test_func()
            else:
                success = test_func()
            
            if success:
                logger.info(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ í†µê³¼")
                passed += 1
            else:
                logger.error(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                failed += 1
                
        except Exception as e:
            logger.error(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì˜ˆì™¸: {e}")
            failed += 1
    
    # í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±
    report = tester.generate_test_report()
    
    # ê²°ê³¼ ìš”ì•½
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    logger.info("=" * 60)
    logger.info(f"âœ… í†µê³¼: {passed}ê°œ")
    logger.info(f"âŒ ì‹¤íŒ¨: {failed}ê°œ")
    logger.info(f"ğŸ“Š ì„±ê³µë¥ : {passed/(passed+failed)*100:.1f}%")
    
    if report['summary']['overall_success']:
        logger.info("ğŸ‰ ì „ì²´ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ëª¨ë¸ ë¡œë”ê°€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        
        logger.info("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info("   python app/main.py  # ì„œë²„ ì‹¤í–‰")
        logger.info("   ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8000 ì ‘ì†")
        
        return True
    else:
        logger.error("âŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”.")
        
        logger.info("\nğŸ’¡ í•´ê²° ë°©ë²•:")
        if not tester.test_results.get("optimized_paths", True):
            logger.info("   python scripts/checkpoint_analyzer.py  # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„")
        if not tester.test_results.get("checkpoint_files", {}).get("success", True):
            logger.info("   python scripts/corrected_checkpoint_relocator.py  # ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬")
        
        return False

if __name__ == "__main__":
    import sys
    
    try:
        result = asyncio.run(main())
        sys.exit(0 if result else 1)
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}")
        sys.exit(1)
