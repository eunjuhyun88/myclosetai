#!/usr/bin/env python3
"""
ğŸ” MyCloset AI - ëª¨ë¸ íƒì§€ ë° êµ¬ì¡° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ë“¤ì„ ì°¾ê³  êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ main.py ìë™ ìƒì„±

ì‚¬ìš©ë²•:
1. python scripts/analyze_models.py  # ëª¨ë¸ íƒì§€ ë° ë¶„ì„
2. python scripts/generate_main.py   # ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ main.py ìƒì„±
"""

import os
import sys
import json
import time
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import logging
from datetime import datetime

# ===============================================================
# ğŸ”§ 1. ëª¨ë¸ íƒì§€ ìŠ¤í¬ë¦½íŠ¸
# ===============================================================

class AIModelDetector:
    """AI ëª¨ë¸ íŒŒì¼ íƒì§€ ë° ë¶„ì„"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.backend_dir = project_root / "backend"
        self.models_dir = self.backend_dir / "ai_models"
        
        # ë¡œê±° ì„¤ì •
        logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')
        self.logger = logging.getLogger(__name__)
        
        # ì•Œë ¤ì§„ ëª¨ë¸ íŒ¨í„´ë“¤
        self.known_models = {
            "ootdiffusion": {
                "patterns": ["ootd", "ootdiffusion", "diffusion"],
                "files": ["pytorch_model.bin", "model.safetensors", "config.json", "model_index.json"],
                "type": "diffusion",
                "description": "OOTDiffusion ê°€ìƒ í”¼íŒ… ëª¨ë¸"
            },
            "viton": {
                "patterns": ["viton", "hr-viton", "cp-viton"],
                "files": ["pytorch_model.bin", "model.safetensors", "generator.pth"],
                "type": "gan",
                "description": "VITON ê³„ì—´ ê°€ìƒ í”¼íŒ… ëª¨ë¸"
            },
            "human_parsing": {
                "patterns": ["parsing", "human", "segment", "schp"],
                "files": ["exp-schp-201908261155-pascal.pth", "model_final.pth", "latest.pth"],
                "type": "segmentation",
                "description": "ì¸ì²´ íŒŒì‹± ëª¨ë¸"
            },
            "pose_estimation": {
                "patterns": ["pose", "openpose", "hrnet", "keypoint"],
                "files": ["pose_iter_440000.caffemodel", "pose_deploy.prototxt", "hrnet.pth"],
                "type": "pose",
                "description": "í¬ì¦ˆ ì¶”ì • ëª¨ë¸"
            },
            "densepose": {
                "patterns": ["densepose", "dense"],
                "files": ["model_final.pkl", "DensePose_ResNet50_FPN_s1x.pkl"],
                "type": "densepose",
                "description": "DensePose ëª¨ë¸"
            },
            "cloth_segmentation": {
                "patterns": ["cloth", "clothing", "garment"],
                "files": ["cloth_segm.pth", "garment_seg.pth"],
                "type": "segmentation",
                "description": "ì˜ë¥˜ ë¶„í•  ëª¨ë¸"
            }
        }
    
    def scan_directory(self, directory: Path, max_depth: int = 5) -> List[Dict[str, Any]]:
        """ë””ë ‰í† ë¦¬ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ìŠ¤ìº”í•˜ì—¬ ëª¨ë¸ íŒŒì¼ë“¤ì„ ì°¾ìŒ"""
        found_files = []
        
        if not directory.exists():
            self.logger.warning(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {directory}")
            return found_files
        
        def _scan_recursive(current_dir: Path, current_depth: int = 0):
            if current_depth > max_depth:
                return
            
            try:
                for item in current_dir.iterdir():
                    if item.is_file():
                        file_info = self.analyze_file(item)
                        if file_info["is_model_file"]:
                            found_files.append(file_info)
                    elif item.is_dir() and not item.name.startswith('.'):
                        _scan_recursive(item, current_depth + 1)
            except PermissionError:
                self.logger.warning(f"ê¶Œí•œ ì—†ìŒ: {current_dir}")
            except Exception as e:
                self.logger.error(f"ìŠ¤ìº” ì¤‘ ì˜¤ë¥˜ ({current_dir}): {e}")
        
        _scan_recursive(directory)
        return found_files
    
    def analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """ê°œë³„ íŒŒì¼ ë¶„ì„"""
        file_info = {
            "path": str(file_path),
            "relative_path": str(file_path.relative_to(self.project_root)),
            "name": file_path.name,
            "suffix": file_path.suffix,
            "size": 0,
            "size_mb": 0,
            "is_model_file": False,
            "model_type": None,
            "confidence": 0,
            "parent_dir": file_path.parent.name,
            "created_time": None,
            "modified_time": None,
            "file_hash": None
        }
        
        try:
            stat = file_path.stat()
            file_info["size"] = stat.st_size
            file_info["size_mb"] = round(stat.st_size / (1024 * 1024), 2)
            file_info["created_time"] = datetime.fromtimestamp(stat.st_ctime).isoformat()
            file_info["modified_time"] = datetime.fromtimestamp(stat.st_mtime).isoformat()
            
            # í° íŒŒì¼ë“¤ë§Œ í•´ì‹œ ê³„ì‚° (ëª¨ë¸ íŒŒì¼ì€ ë³´í†µ í¼)
            if stat.st_size > 1024 * 1024:  # 1MB ì´ìƒ
                file_info["file_hash"] = self.calculate_file_hash(file_path)
        except Exception as e:
            self.logger.warning(f"íŒŒì¼ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ ({file_path}): {e}")
        
        # ëª¨ë¸ íŒŒì¼ ì—¬ë¶€ íŒë‹¨
        is_model, model_type, confidence = self.identify_model_type(file_path)
        file_info["is_model_file"] = is_model
        file_info["model_type"] = model_type
        file_info["confidence"] = confidence
        
        return file_info
    
    def identify_model_type(self, file_path: Path) -> Tuple[bool, Optional[str], float]:
        """íŒŒì¼ì´ ëª¨ë¸ íŒŒì¼ì¸ì§€ íŒë‹¨í•˜ê³  íƒ€ì… ì¶”ì •"""
        file_name = file_path.name.lower()
        parent_name = file_path.parent.name.lower()
        path_str = str(file_path).lower()
        
        # ëª¨ë¸ íŒŒì¼ í™•ì¥ì
        model_extensions = {'.pth', '.pt', '.bin', '.safetensors', '.pkl', '.h5', '.pb', '.onnx', '.caffemodel'}
        
        if file_path.suffix.lower() not in model_extensions:
            return False, None, 0.0
        
        # í¬ê¸° ê¸°ë°˜ í•„í„°ë§ (ëª¨ë¸ íŒŒì¼ì€ ë³´í†µ 1MB ì´ìƒ)
        try:
            if file_path.stat().st_size < 1024 * 1024:  # 1MB ë¯¸ë§Œ
                return False, None, 0.0
        except:
            pass
        
        # ê° ëª¨ë¸ íƒ€ì…ë³„ ë§¤ì¹­
        best_match = None
        best_confidence = 0.0
        
        for model_key, model_info in self.known_models.items():
            confidence = 0.0
            
            # íŒ¨í„´ ë§¤ì¹­
            for pattern in model_info["patterns"]:
                if pattern in path_str:
                    confidence += 0.3
                if pattern in file_name:
                    confidence += 0.4
                if pattern in parent_name:
                    confidence += 0.2
            
            # íŠ¹ì • íŒŒì¼ëª… ë§¤ì¹­
            for known_file in model_info["files"]:
                if known_file.lower() in file_name:
                    confidence += 0.5
                    break
            
            if confidence > best_confidence:
                best_confidence = confidence
                best_match = model_key
        
        # ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’
        if best_confidence >= 0.3:
            return True, best_match, best_confidence
        
        # í™•ì¥ìë§Œìœ¼ë¡œë„ ëª¨ë¸ íŒŒì¼ë¡œ ê°„ì£¼ (ë‚®ì€ ì‹ ë¢°ë„)
        return True, "unknown", 0.1
    
    def calculate_file_hash(self, file_path: Path, chunk_size: int = 8192) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ê°ì§€ìš©)"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                # í° íŒŒì¼ì€ ì•ë¶€ë¶„ë§Œ í•´ì‹œ ê³„ì‚°
                chunk = f.read(chunk_size * 10)  # ì•½ 80KB
                hash_md5.update(chunk)
            return hash_md5.hexdigest()[:16]  # ì• 16ìë§Œ
        except Exception as e:
            self.logger.warning(f"í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨ ({file_path}): {e}")
            return "unknown"
    
    def analyze_model_structure(self, model_files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë°œê²¬ëœ ëª¨ë¸ë“¤ì˜ êµ¬ì¡° ë¶„ì„"""
        analysis = {
            "total_files": len(model_files),
            "total_size_mb": sum(f["size_mb"] for f in model_files),
            "model_types": {},
            "directories": {},
            "recommendations": [],
            "potential_issues": []
        }
        
        # íƒ€ì…ë³„ ê·¸ë£¹í™”
        for file_info in model_files:
            model_type = file_info["model_type"] or "unknown"
            
            if model_type not in analysis["model_types"]:
                analysis["model_types"][model_type] = {
                    "count": 0,
                    "total_size_mb": 0,
                    "files": [],
                    "confidence_avg": 0
                }
            
            type_info = analysis["model_types"][model_type]
            type_info["count"] += 1
            type_info["total_size_mb"] += file_info["size_mb"]
            type_info["files"].append(file_info)
            type_info["confidence_avg"] = sum(f["confidence"] for f in type_info["files"]) / len(type_info["files"])
        
        # ë””ë ‰í† ë¦¬ë³„ ê·¸ë£¹í™”
        for file_info in model_files:
            parent_dir = file_info["parent_dir"]
            if parent_dir not in analysis["directories"]:
                analysis["directories"][parent_dir] = {
                    "count": 0,
                    "size_mb": 0,
                    "model_types": set()
                }
            
            dir_info = analysis["directories"][parent_dir]
            dir_info["count"] += 1
            dir_info["size_mb"] += file_info["size_mb"]
            dir_info["model_types"].add(file_info["model_type"])
        
        # setì„ listë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)
        for dir_name, dir_info in analysis["directories"].items():
            dir_info["model_types"] = list(dir_info["model_types"])
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        self.generate_recommendations(analysis)
        
        return analysis
    
    def generate_recommendations(self, analysis: Dict[str, Any]):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = analysis["recommendations"]
        issues = analysis["potential_issues"]
        
        # í•„ìˆ˜ ëª¨ë¸ í™•ì¸
        required_models = ["ootdiffusion", "human_parsing", "pose_estimation"]
        found_types = set(analysis["model_types"].keys())
        
        missing_models = []
        for required in required_models:
            if required not in found_types:
                missing_models.append(required)
        
        if missing_models:
            issues.append(f"í•„ìˆ˜ ëª¨ë¸ ëˆ„ë½: {', '.join(missing_models)}")
            recommendations.append("ëˆ„ë½ëœ í•„ìˆ˜ ëª¨ë¸ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ai_models ë””ë ‰í† ë¦¬ì— ë°°ì¹˜í•˜ì„¸ìš”")
        
        # ëª¨ë¸ í’ˆì§ˆ í™•ì¸
        for model_type, type_info in analysis["model_types"].items():
            if type_info["confidence_avg"] < 0.5:
                issues.append(f"{model_type} ëª¨ë¸ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŒ ({type_info['confidence_avg']:.2f})")
        
        # í¬ê¸° ì´ìƒ í™•ì¸
        for model_type, type_info in analysis["model_types"].items():
            if type_info["total_size_mb"] < 10:  # 10MB ë¯¸ë§Œ
                issues.append(f"{model_type} ëª¨ë¸ì´ ë„ˆë¬´ ì‘ìŒ ({type_info['total_size_mb']:.1f}MB)")
            elif type_info["total_size_mb"] > 10000:  # 10GB ì´ˆê³¼
                recommendations.append(f"{model_type} ëª¨ë¸ì´ ë§¤ìš° í¼ ({type_info['total_size_mb']:.1f}MB), ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ í•„ìš”")
        
        # ì¼ë°˜ì ì¸ ì¶”ì²œì‚¬í•­
        if analysis["total_files"] > 0:
            recommendations.append("ë°œê²¬ëœ ëª¨ë¸ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ main.py ìë™ ìƒì„± ê°€ëŠ¥")
            recommendations.append("ê° ëª¨ë¸ì˜ config íŒŒì¼ë„ í•¨ê»˜ í™•ì¸í•˜ì„¸ìš”")
        else:
            recommendations.append("AI ëª¨ë¸ì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ. ai_models ë””ë ‰í† ë¦¬ì— ëª¨ë¸ íŒŒì¼ë“¤ì„ ë°°ì¹˜í•˜ì„¸ìš”")
    
    def save_analysis_report(self, model_files: List[Dict[str, Any]], analysis: Dict[str, Any], output_file: Path):
        """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        report = {
            "scan_info": {
                "timestamp": datetime.now().isoformat(),
                "project_root": str(self.project_root),
                "models_directory": str(self.models_dir),
                "scan_duration": None
            },
            "discovered_files": model_files,
            "analysis": analysis,
            "summary": {
                "total_model_files": len(model_files),
                "total_size_mb": analysis["total_size_mb"],
                "model_types_found": list(analysis["model_types"].keys()),
                "ready_for_integration": len(analysis["potential_issues"]) == 0
            }
        }
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            self.logger.info(f"âœ… ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {output_file}")
        except Exception as e:
            self.logger.error(f"âŒ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def run_full_analysis(self) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        self.logger.info("ğŸ” AI ëª¨ë¸ íƒì§€ ë° ë¶„ì„ ì‹œì‘")
        start_time = time.time()
        
        # ì—¬ëŸ¬ ê²½ë¡œì—ì„œ ëª¨ë¸ íƒì§€
        search_paths = [
            self.models_dir,
            self.backend_dir / "models",
            self.backend_dir / "checkpoints",
            self.project_root / "models",
            self.project_root / "checkpoints"
        ]
        
        all_model_files = []
        
        for search_path in search_paths:
            if search_path.exists():
                self.logger.info(f"ğŸ“‚ ìŠ¤ìº” ì¤‘: {search_path}")
                found_files = self.scan_directory(search_path)
                all_model_files.extend(found_files)
                self.logger.info(f"   â””â”€ ë°œê²¬ëœ ëª¨ë¸ íŒŒì¼: {len(found_files)}ê°œ")
        
        # ì¤‘ë³µ ì œê±° (í•´ì‹œ ê¸°ë°˜)
        unique_files = {}
        for file_info in all_model_files:
            key = (file_info["file_hash"], file_info["size"])
            if key not in unique_files:
                unique_files[key] = file_info
        
        model_files = list(unique_files.values())
        
        # êµ¬ì¡° ë¶„ì„
        analysis = self.analyze_model_structure(model_files)
        
        # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        duration = time.time() - start_time
        
        # ê²°ê³¼ ì¶œë ¥
        self.logger.info(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ ({duration:.2f}ì´ˆ)")
        self.logger.info(f"ğŸ“Š ë°œê²¬ëœ ëª¨ë¸ íŒŒì¼: {len(model_files)}ê°œ")
        self.logger.info(f"ğŸ’¾ ì´ í¬ê¸°: {analysis['total_size_mb']:.1f}MB")
        self.logger.info(f"ğŸ¤– ëª¨ë¸ íƒ€ì…: {list(analysis['model_types'].keys())}")
        
        if analysis["potential_issues"]:
            self.logger.warning("âš ï¸  ì ì¬ì  ë¬¸ì œ:")
            for issue in analysis["potential_issues"]:
                self.logger.warning(f"   - {issue}")
        
        if analysis["recommendations"]:
            self.logger.info("ğŸ’¡ ì¶”ì²œì‚¬í•­:")
            for rec in analysis["recommendations"]:
                self.logger.info(f"   - {rec}")
        
        return model_files, analysis


# ===============================================================
# ğŸ”§ 2. main.py ìƒì„±ê¸°
# ===============================================================

class MainPyGenerator:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ main.py ìƒì„±"""
    
    def __init__(self, analysis_file: Path):
        self.analysis_file = analysis_file
        self.logger = logging.getLogger(__name__)
        
        # ë¶„ì„ ê²°ê³¼ ë¡œë“œ
        with open(analysis_file, 'r', encoding='utf-8') as f:
            self.report = json.load(f)
        
        self.model_files = self.report["discovered_files"]
        self.analysis = self.report["analysis"]
    
    def generate_model_imports(self) -> str:
        """ëª¨ë¸ë³„ import êµ¬ë¬¸ ìƒì„±"""
        imports = []
        model_types = self.analysis["model_types"]
        
        # ê¸°ë³¸ imports
        imports.extend([
            "import torch",
            "import torch.nn as nn",
            "from pathlib import Path",
            "import numpy as np",
            "from PIL import Image",
            "import cv2",
            "import logging"
        ])
        
        # ëª¨ë¸ë³„ íŠ¹í™” imports
        if "ootdiffusion" in model_types:
            imports.extend([
                "from diffusers import StableDiffusionPipeline, AutoencoderKL",
                "from transformers import CLIPTextModel, CLIPTokenizer"
            ])
        
        if "human_parsing" in model_types:
            imports.extend([
                "import torchvision.transforms as transforms",
                "from torchvision.models import segmentation"
            ])
        
        if "pose_estimation" in model_types:
            imports.extend([
                "import mediapipe as mp",
                "# import openpose  # OpenPose ì„¤ì¹˜ ì‹œ"
            ])
        
        return "\n".join(imports)
    
    def generate_model_paths(self) -> str:
        """ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì •ì˜ ìƒì„±"""
        paths = []
        paths.append("# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì •ì˜")
        paths.append("MODELS_DIR = Path(__file__).parent / 'ai_models'")
        paths.append("")
        
        # ë°œê²¬ëœ ëª¨ë¸ íŒŒì¼ë“¤ì„ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
        model_paths = {}
        for file_info in self.model_files:
            model_type = file_info["model_type"]
            if model_type not in model_paths:
                model_paths[model_type] = []
            model_paths[model_type].append(file_info)
        
        # ê° ëª¨ë¸ íƒ€ì…ë³„ ê²½ë¡œ ìƒì„±
        for model_type, files in model_paths.items():
            if model_type == "unknown":
                continue
            
            paths.append(f"# {model_type.upper()} ëª¨ë¸")
            for i, file_info in enumerate(files):
                rel_path = file_info["relative_path"].replace("backend/", "")
                var_name = f"{model_type.upper()}_MODEL_{i+1}" if len(files) > 1 else f"{model_type.upper()}_MODEL"
                paths.append(f'{var_name} = MODELS_DIR / "{rel_path.replace("ai_models/", "")}"')
            paths.append("")
        
        return "\n".join(paths)
    
    def generate_model_classes(self) -> str:
        """ëª¨ë¸ í´ë˜ìŠ¤ë“¤ ìƒì„±"""
        classes = []
        model_types = self.analysis["model_types"]
        
        # ê¸°ë³¸ ëª¨ë¸ ë§¤ë‹ˆì € í´ë˜ìŠ¤
        classes.append('''
class ModelManager:
    """AI ëª¨ë¸ ë§¤ë‹ˆì € - ëª¨ë“  ëª¨ë¸ í†µí•© ê´€ë¦¬"""
    
    def __init__(self, device="auto"):
        self.device = self._setup_device(device)
        self.models = {}
        self.logger = logging.getLogger(__name__)
        
    def _setup_device(self, device):
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return device
    
    async def load_all_models(self):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        self.logger.info(f"ğŸ¤– ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì‹œì‘ (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # ëª¨ë¸ë³„ ë¡œë“œ
''')
        
        # ê° ëª¨ë¸ íƒ€ì…ë³„ ë¡œë” ì¶”ê°€
        for model_type in model_types:
            if model_type == "unknown":
                continue
            
            method_name = f"load_{model_type}_model"
            classes.append(f"        await self.{method_name}()")
        
        classes.append('''        
        self.logger.info("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def get_model(self, model_type: str):
        """íŠ¹ì • ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        return self.models.get(model_type)
''')
        
        # ê° ëª¨ë¸ë³„ ë¡œë“œ ë©”ì„œë“œ ìƒì„±
        for model_type in model_types:
            if model_type == "unknown":
                continue
            
            class_code = self.generate_model_loader(model_type)
            classes.append(class_code)
        
        return "\n".join(classes)
    
    def generate_model_loader(self, model_type: str) -> str:
        """íŠ¹ì • ëª¨ë¸ íƒ€ì…ì˜ ë¡œë” ë©”ì„œë“œ ìƒì„±"""
        if model_type == "ootdiffusion":
            return '''
    async def load_ootdiffusion_model(self):
        """OOTDiffusion ëª¨ë¸ ë¡œë“œ"""
        try:
            # ì‹¤ì œ OOTDiffusion ëª¨ë¸ ë¡œë“œ ë¡œì§
            from diffusers import StableDiffusionImg2ImgPipeline
            
            model_path = OOTDIFFUSION_MODEL
            if model_path.exists():
                self.models["ootdiffusion"] = StableDiffusionImg2ImgPipeline.from_pretrained(
                    str(model_path.parent),
                    torch_dtype=torch.float16 if self.device != "cpu" else torch.float32,
                    safety_checker=None,
                    requires_safety_checker=False
                ).to(self.device)
                self.logger.info("âœ… OOTDiffusion ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                self.logger.warning(f"âš ï¸ OOTDiffusion ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
        except Exception as e:
            self.logger.error(f"âŒ OOTDiffusion ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
'''
        
        elif model_type == "human_parsing":
            return '''
    async def load_human_parsing_model(self):
        """ì¸ì²´ íŒŒì‹± ëª¨ë¸ ë¡œë“œ"""
        try:
            # ì‹¤ì œ ì¸ì²´ íŒŒì‹± ëª¨ë¸ ë¡œë“œ ë¡œì§
            model_path = HUMAN_PARSING_MODEL
            if model_path.exists():
                # PyTorch ëª¨ë¸ ë¡œë“œ
                model = torch.load(model_path, map_location=self.device)
                self.models["human_parsing"] = model
                self.logger.info("âœ… ì¸ì²´ íŒŒì‹± ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                self.logger.warning(f"âš ï¸ ì¸ì²´ íŒŒì‹± ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
        except Exception as e:
            self.logger.error(f"âŒ ì¸ì²´ íŒŒì‹± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
'''
        
        elif model_type == "pose_estimation":
            return '''
    async def load_pose_estimation_model(self):
        """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ë¡œë“œ"""
        try:
            # ì‹¤ì œ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ë¡œë“œ ë¡œì§
            model_path = POSE_ESTIMATION_MODEL
            if model_path.exists():
                # MediaPipe ë˜ëŠ” OpenPose ëª¨ë¸ ë¡œë“œ
                import mediapipe as mp
                self.models["pose_estimation"] = mp.solutions.pose.Pose(
                    static_image_mode=True,
                    model_complexity=2,
                    enable_segmentation=True,
                    min_detection_confidence=0.5
                )
                self.logger.info("âœ… í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                self.logger.warning(f"âš ï¸ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
'''
        
        else:
            # ì¼ë°˜ì ì¸ ëª¨ë¸ ë¡œë”
            return f'''
    async def load_{model_type}_model(self):
        """{model_type} ëª¨ë¸ ë¡œë“œ"""
        try:
            model_path = {model_type.upper()}_MODEL
            if model_path.exists():
                # ì¼ë°˜ì ì¸ PyTorch ëª¨ë¸ ë¡œë“œ
                model = torch.load(model_path, map_location=self.device)
                self.models["{model_type}"] = model
                self.logger.info("âœ… {model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                self.logger.warning(f"âš ï¸ {model_type} ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {{model_path}}")
        except Exception as e:
            self.logger.error(f"âŒ {model_type} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {{e}}")
'''
    
    def generate_processing_functions(self) -> str:
        """ì‹¤ì œ AI ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ìƒì„±"""
        functions = []
        model_types = self.analysis["model_types"]
        
        # ê°€ìƒ í”¼íŒ… í•¨ìˆ˜ (í•µì‹¬)
        if "ootdiffusion" in model_types:
            functions.append('''
async def process_virtual_fitting_real(person_image: bytes, clothing_image: bytes, model_manager: ModelManager) -> Dict[str, Any]:
    """ì‹¤ì œ OOTDiffusion ëª¨ë¸ì„ ì‚¬ìš©í•œ ê°€ìƒ í”¼íŒ…"""
    try:
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        person_pil = Image.open(io.BytesIO(person_image)).convert("RGB")
        clothing_pil = Image.open(io.BytesIO(clothing_image)).convert("RGB")
        
        # ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
        ootd_model = model_manager.get_model("ootdiffusion")
        if not ootd_model:
            raise Exception("OOTDiffusion ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
        
        # ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬
        # TODO: ì‹¤ì œ OOTDiffusion íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ
        result_image = ootd_model(
            image=person_pil,
            clothing=clothing_pil,
            num_inference_steps=20,
            guidance_scale=7.5
        ).images[0]
        
        # ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜
        buffer = io.BytesIO()
        result_image.save(buffer, format='JPEG', quality=85)
        fitted_image_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        return {
            "fitted_image": fitted_image_base64,
            "fit_score": 0.88,
            "confidence": 0.92,
            "processing_method": "OOTDiffusion_Real",
            "model_version": "v2.1"
        }
        
    except Exception as e:
        logger.error(f"ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        # í´ë°±ìœ¼ë¡œ ë”ë¯¸ ì´ë¯¸ì§€ ë°˜í™˜
        dummy_image = Image.new('RGB', (512, 768), color=(135, 206, 235))
        buffer = io.BytesIO()
        dummy_image.save(buffer, format='JPEG', quality=85)
        fitted_image_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        return {
            "fitted_image": fitted_image_base64,
            "fit_score": 0.60,
            "confidence": 0.50,
            "processing_method": "Fallback_Dummy",
            "error": str(e)
        }
''')
        
        # ì¸ì²´ íŒŒì‹± í•¨ìˆ˜
        if "human_parsing" in model_types:
            functions.append('''
async def process_human_parsing_real(image_data: bytes, model_manager: ModelManager) -> Dict[str, Any]:
    """ì‹¤ì œ ì¸ì²´ íŒŒì‹± ëª¨ë¸ ì‚¬ìš©"""
    try:
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image = Image.open(io.BytesIO(image_data)).convert("RGB")
        
        # ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
        parsing_model = model_manager.get_model("human_parsing")
        if not parsing_model:
            raise Exception("ì¸ì²´ íŒŒì‹± ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
        
        # ì‹¤ì œ ì¸ì²´ íŒŒì‹± ì²˜ë¦¬
        # TODO: ì‹¤ì œ íŒŒì‹± ë¡œì§ êµ¬í˜„
        
        return {
            "detected_parts": 18,
            "total_parts": 20,
            "confidence": 0.93,
            "parts": ["head", "torso", "arms", "legs", "hands", "feet"],
            "result_image": None  # íŒŒì‹± ê²°ê³¼ ì´ë¯¸ì§€ base64
        }
        
    except Exception as e:
        logger.error(f"ì‹¤ì œ ì¸ì²´ íŒŒì‹± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            "detected_parts": 15,
            "total_parts": 20,
            "confidence": 0.75,
            "parts": ["head", "torso", "arms", "legs"],
            "error": str(e)
        }
''')
        
        return "\n".join(functions)
    
    def generate_main_py(self, output_file: Path):
        """ì™„ì „í•œ main.py ìƒì„±"""
        main_py_content = f'''"""
ğŸ MyCloset AI Backend - ì‹¤ì œ AI ëª¨ë¸ í†µí•© ë²„ì „
âœ… ìë™ ìƒì„±ë¨: {datetime.now().isoformat()}
âœ… íƒì§€ëœ ëª¨ë¸: {list(self.analysis["model_types"].keys())}
âœ… ì´ ëª¨ë¸ íŒŒì¼: {len(self.model_files)}ê°œ
âœ… ì´ í¬ê¸°: {self.analysis["total_size_mb"]:.1f}MB
"""

import os
import sys
import time
import logging
import asyncio
import json
import io
import base64
import uuid
from contextlib import asynccontextmanager
from typing import Dict, Any, Optional, List
from pathlib import Path
from PIL import Image
import psutil

{self.generate_model_imports()}

# FastAPI ë° ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from fastapi import FastAPI, HTTPException, File, UploadFile, Form, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel
import uvicorn

# ===============================================================
# ğŸ”§ ê²½ë¡œ ë° ì„¤ì •
# ===============================================================

current_file = Path(__file__).resolve()
app_dir = current_file.parent
backend_dir = app_dir.parent
project_root = backend_dir.parent

{self.generate_model_paths()}

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(name)s | %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(backend_dir / "logs" / f"mycloset-ai-{{time.strftime('%Y%m%d')}}.log")
    ]
)
logger = logging.getLogger(__name__)

# ===============================================================
# ğŸ”§ M3 Max GPU ì„¤ì •
# ===============================================================

try:
    import torch
    
    IS_M3_MAX = (
        sys.platform == "darwin" and 
        os.uname().machine == "arm64" and
        torch.backends.mps.is_available()
    )
    
    if IS_M3_MAX:
        DEVICE = "mps"
        DEVICE_NAME = "Apple M3 Max"
        os.environ.update({{
            'PYTORCH_ENABLE_MPS_FALLBACK': '1',
            'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
            'OMP_NUM_THREADS': '16',
            'MKL_NUM_THREADS': '16'
        }})
        
        memory_info = psutil.virtual_memory()
        TOTAL_MEMORY_GB = memory_info.total / (1024**3)
        AVAILABLE_MEMORY_GB = memory_info.available / (1024**3)
        
        logger.info(f"ğŸ M3 Max ê°ì§€ë¨")
        logger.info(f"ğŸ’¾ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {{TOTAL_MEMORY_GB:.1f}}GB (ì‚¬ìš©ê°€ëŠ¥: {{AVAILABLE_MEMORY_GB:.1f}}GB)")
        
    else:
        DEVICE = "cpu"
        DEVICE_NAME = "CPU"
        TOTAL_MEMORY_GB = 8.0
        AVAILABLE_MEMORY_GB = 4.0
        
except ImportError as e:
    logger.warning(f"PyTorch ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {{e}}")
    DEVICE = "cpu"
    DEVICE_NAME = "CPU"
    IS_M3_MAX = False
    TOTAL_MEMORY_GB = 8.0
    AVAILABLE_MEMORY_GB = 4.0

# ===============================================================
# ğŸ”§ AI ëª¨ë¸ í†µí•© ê´€ë¦¬
# ===============================================================

{self.generate_model_classes()}

# ì „ì—­ ëª¨ë¸ ë§¤ë‹ˆì €
model_manager = None

# ===============================================================
# ğŸ”§ ì‹¤ì œ AI ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ===============================================================

{self.generate_processing_functions()}

# ===============================================================
# ğŸ”§ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ (ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©)
# ===============================================================

async def process_virtual_fitting(all_data: Dict) -> Dict[str, Any]:
    """7ë‹¨ê³„: ê°€ìƒ í”¼íŒ… - ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš©"""
    global model_manager
    
    if not model_manager or not model_manager.models:
        logger.warning("âš ï¸ AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ - ë”ë¯¸ ëª¨ë“œë¡œ ì‹¤í–‰")
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (í´ë°±)
        dummy_image = Image.new('RGB', (512, 768), color=(135, 206, 235))
        buffer = io.BytesIO()
        dummy_image.save(buffer, format='JPEG', quality=85)
        fitted_image_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        return {{
            "fitted_image": fitted_image_base64,
            "fit_score": 0.70,
            "confidence": 0.60,
            "processing_method": "Dummy_Fallback",
            "model_version": "fallback"
        }}
    
    # ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©
    try:
        logger.info("ğŸ¤– ì‹¤ì œ AI ëª¨ë¸ë¡œ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì¤‘...")
        
        person_image = all_data.get("person_image")
        clothing_image = all_data.get("clothing_image")
        
        if not person_image or not clothing_image:
            raise Exception("ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì‹¤í–‰
        result = await process_virtual_fitting_real(person_image, clothing_image, model_manager)
        
        logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ë¡œ ê°€ìƒ í”¼íŒ… ì™„ë£Œ")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì œ ëª¨ë¸ ì²˜ë¦¬ ì‹¤íŒ¨, ë”ë¯¸ ëª¨ë“œë¡œ í´ë°±: {{e}}")
        
        # í´ë°±ìœ¼ë¡œ ë”ë¯¸ ì´ë¯¸ì§€ ë°˜í™˜
        dummy_image = Image.new('RGB', (512, 768), color=(255, 200, 200))
        buffer = io.BytesIO()
        dummy_image.save(buffer, format='JPEG', quality=85)
        fitted_image_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        return {{
            "fitted_image": fitted_image_base64,
            "fit_score": 0.65,
            "confidence": 0.55,
            "processing_method": "Error_Fallback",
            "model_version": "fallback",
            "error": str(e)
        }}

# ë‹¤ë¥¸ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ë„ ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
async def process_human_parsing(image_data: bytes) -> Dict[str, Any]:
    """3ë‹¨ê³„: ì¸ì²´ íŒŒì‹± - ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©"""
    global model_manager
    
    if model_manager and "human_parsing" in model_manager.models:
        return await process_human_parsing_real(image_data, model_manager)
    else:
        # ë”ë¯¸ ì‘ë‹µ (í´ë°±)
        await asyncio.sleep(1.0)
        return {{
            "detected_parts": 16,
            "total_parts": 20,
            "confidence": 0.80,
            "parts": ["head", "torso", "arms", "legs", "hands", "feet"],
            "processing_method": "fallback"
        }}

# ===============================================================
# ğŸ”§ FastAPI ì•± ìˆ˜ëª…ì£¼ê¸° (ëª¨ë¸ ë¡œë”© í¬í•¨)
# ===============================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜ëª…ì£¼ê¸° ê´€ë¦¬ - AI ëª¨ë¸ ë¡œë”©"""
    global model_manager
    
    # === ì‹œì‘ ì´ë²¤íŠ¸ ===
    logger.info("ğŸš€ MyCloset AI Backend ì‹œì‘ë¨ (ì‹¤ì œ AI ëª¨ë¸ ë²„ì „)")
    logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {{DEVICE_NAME}} ({{DEVICE}})")
    logger.info(f"ğŸ M3 Max: {{'âœ…' if IS_M3_MAX else 'âŒ'}}")
    
    # AI ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    try:
        logger.info("ğŸ¤– AI ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì¤‘...")
        model_manager = ModelManager(device=DEVICE)
        
        # ëª¨ë“  ëª¨ë¸ ë¡œë“œ
        await model_manager.load_all_models()
        
        logger.info("âœ… ëª¨ë“  AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        logger.info(f"ğŸ“‹ ë¡œë“œëœ ëª¨ë¸: {{list(model_manager.models.keys())}}")
        
    except Exception as e:
        logger.error(f"âŒ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {{e}}")
        logger.warning("âš ï¸ ë”ë¯¸ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    
    logger.info("ğŸ‰ ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ - ìš”ì²­ ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
    
    yield
    
    # === ì¢…ë£Œ ì´ë²¤íŠ¸ ===
    logger.info("ğŸ›‘ MyCloset AI Backend ì¢…ë£Œ ì¤‘...")
    
    # ëª¨ë¸ ì •ë¦¬
    if model_manager:
        try:
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if DEVICE == "mps" and torch.backends.mps.is_available():
                torch.mps.empty_cache()
            elif DEVICE == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.info("ğŸ’¾ ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {{e}}")
    
    logger.info("âœ… ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")

# ===============================================================
# ğŸ”§ FastAPI ì•± ìƒì„± ë° ì„¤ì •
# ===============================================================

app = FastAPI(
    title="MyCloset AI",
    description="ğŸ M3 Max ìµœì í™” AI ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ - ì‹¤ì œ ëª¨ë¸ í†µí•© ë²„ì „",
    version="4.0.0-real-models",
    debug=True,
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000", "http://localhost:3001", "http://localhost:5173", 
        "http://localhost:5174", "http://localhost:8080", "http://127.0.0.1:3000",
        "http://127.0.0.1:5173", "http://127.0.0.1:5174", "http://127.0.0.1:8080"
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# Gzip ì••ì¶•
app.add_middleware(GZipMiddleware, minimum_size=1000)

# ì •ì  íŒŒì¼ ì„œë¹™
static_dir = backend_dir / "static"
static_dir.mkdir(exist_ok=True)
app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")

# ===============================================================
# ğŸ”§ API ì—”ë“œí¬ì¸íŠ¸ë“¤ (ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€)
# ===============================================================

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    global model_manager
    
    models_status = "loaded" if model_manager and model_manager.models else "fallback"
    loaded_models = list(model_manager.models.keys()) if model_manager else []
    
    return {{
        "message": f"ğŸ MyCloset AI ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤! (ì‹¤ì œ ëª¨ë¸ ë²„ì „)",
        "version": "4.0.0-real-models",
        "device": DEVICE,
        "device_name": DEVICE_NAME,
        "m3_max": IS_M3_MAX,
        "models_status": models_status,
        "loaded_models": loaded_models,
        "total_model_files": {len(self.model_files)},
        "docs": "/docs",
        "health": "/api/health",
        "timestamp": time.time()
    }}

@app.get("/api/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬"""
    global model_manager
    
    memory_info = psutil.virtual_memory()
    models_status = "healthy" if model_manager and model_manager.models else "degraded"
    
    return {{
        "status": "healthy",
        "app": "MyCloset AI",
        "version": "4.0.0-real-models",
        "device": DEVICE,
        "models_status": models_status,
        "loaded_models": list(model_manager.models.keys()) if model_manager else [],
        "memory": {{
            "available_gb": round(memory_info.available / (1024**3), 1),
            "used_percent": round(memory_info.percent, 1),
            "is_sufficient": memory_info.available > (2 * 1024**3)
        }},
        "features": {{
            "m3_max_optimized": IS_M3_MAX,
            "real_ai_models": models_status == "healthy",
            "pipeline_steps": 8,
            "websocket_support": True
        }},
        "timestamp": time.time()
    }}

@app.get("/api/models/status")
async def models_status():
    """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
    global model_manager
    
    if not model_manager:
        return {{
            "status": "not_initialized",
            "loaded_models": [],
            "available_models": [],
            "error": "ëª¨ë¸ ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
        }}
    
    return {{
        "status": "initialized",
        "loaded_models": list(model_manager.models.keys()),
        "model_device": model_manager.device,
        "total_discovered_files": {len(self.model_files)},
        "model_types_found": {list(self.analysis["model_types"].keys())},
        "memory_usage": "ì •ìƒ",
        "timestamp": time.time()
    }}

# ===============================================================
# ë‚˜ë¨¸ì§€ API ì—”ë“œí¬ì¸íŠ¸ë“¤ì€ ê¸°ì¡´ main.pyì™€ ë™ì¼
# (process_virtual_fitting í•¨ìˆ˜ë§Œ ìœ„ì—ì„œ ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •ë¨)
# ===============================================================

# ì—¬ê¸°ì— ê¸°ì¡´ main.pyì˜ ë‚˜ë¨¸ì§€ ì—”ë“œí¬ì¸íŠ¸ë“¤ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬
# (step_routes, pipeline routes, websocket ë“±)

if __name__ == "__main__":
    logger.info("ğŸ”§ ê°œë°œ ëª¨ë“œ: uvicorn ì„œë²„ ì§ì ‘ ì‹¤í–‰")
    logger.info(f"ğŸ“ ì£¼ì†Œ: http://localhost:8000")
    logger.info(f"ğŸ“– API ë¬¸ì„œ: http://localhost:8000/docs")
    logger.info(f"ğŸ¤– íƒì§€ëœ ëª¨ë¸: {list(self.analysis["model_types"].keys())}")
    logger.info(f"ğŸ“ ì´ ëª¨ë¸ íŒŒì¼: {len(self.model_files)}ê°œ")
    
    try:
        uvicorn.run(
            "app.main:app",
            host="0.0.0.0",
            port=8000,
            reload=True,
            log_level="info",
            access_log=True,
            workers=1,
            loop="auto",
            timeout_keep_alive=30,
        )
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ ì„œë²„ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì‹¤í–‰ ì‹¤íŒ¨: {{e}}")
        sys.exit(1)
'''

        # íŒŒì¼ ì €ì¥
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(main_py_content)
            self.logger.info(f"âœ… main.py ìƒì„± ì™„ë£Œ: {output_file}")
        except Exception as e:
            self.logger.error(f"âŒ main.py ìƒì„± ì‹¤íŒ¨: {e}")


# ===============================================================
# ğŸ”§ 3. ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
# ===============================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="AI ëª¨ë¸ íƒì§€ ë° main.py ìƒì„±")
    parser.add_argument("--project-root", type=Path, default=Path.cwd(), help="í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬")
    parser.add_argument("--output-dir", type=Path, default=Path.cwd() / "backend" / "scripts", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--generate-main", action="store_true", help="main.py ìƒì„± (ë¶„ì„ í›„)")
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ” MyCloset AI - ëª¨ë¸ íƒì§€ ë° ë¶„ì„ ë„êµ¬")
    print("=" * 50)
    
    # 1ë‹¨ê³„: ëª¨ë¸ íƒì§€ ë° ë¶„ì„
    print("ğŸ“‹ 1ë‹¨ê³„: AI ëª¨ë¸ íƒì§€ ë° ë¶„ì„")
    detector = AIModelDetector(args.project_root)
    model_files, analysis = detector.run_full_analysis()
    
    # ë¶„ì„ ë³´ê³ ì„œ ì €ì¥
    report_file = args.output_dir / "model_analysis_report.json"
    detector.save_analysis_report(model_files, analysis, report_file)
    
    # 2ë‹¨ê³„: main.py ìƒì„± (ì˜µì…˜)
    if args.generate_main:
        print("\nğŸ“‹ 2ë‹¨ê³„: main.py ìƒì„±")
        generator = MainPyGenerator(report_file)
        main_py_file = args.project_root / "backend" / "app" / "main_generated.py"
        generator.generate_main_py(main_py_file)
        
        print(f"\nâœ… ì™„ë£Œ!")
        print(f"ğŸ“„ ë¶„ì„ ë³´ê³ ì„œ: {report_file}")
        print(f"ğŸ ìƒì„±ëœ main.py: {main_py_file}")
        print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"   1. {main_py_file} ë‚´ìš© ê²€í† ")
        print(f"   2. ê¸°ì¡´ main.py ë°±ì—… í›„ êµì²´")
        print(f"   3. ì„œë²„ ì¬ì‹œì‘ ë° í…ŒìŠ¤íŠ¸")
    else:
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“„ ë¶„ì„ ë³´ê³ ì„œ: {report_file}")
        print(f"\nğŸ’¡ main.py ìƒì„±í•˜ë ¤ë©´:")
        print(f"   python {__file__} --generate-main")


if __name__ == "__main__":
    main()