#!/usr/bin/env python3
"""
ğŸ¤– MyCloset AI - ì™„ì „í•œ AI ëª¨ë¸ ê²€ì¦ ë° ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
âœ… ëª¨ë“  í•„ìˆ˜ AI ëª¨ë¸ ë° ì²´í¬í¬ì¸íŠ¸ ìë™ ê²€ì¦
âœ… ê³µì‹ ì†ŒìŠ¤ì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ
âœ… M3 Max ìµœì í™” ì§€ì›
âœ… ì²´í¬ì„¬ ê²€ì¦ ë° ë¬´ê²°ì„± í™•ì¸
âœ… ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ë° ìƒì„¸ ë¡œê¹…

íŒŒì¼ ìœ„ì¹˜: backend/scripts/complete_model_downloader.py
ì‹¤í–‰ ë°©ë²•: python scripts/complete_model_downloader.py
"""

import os
import sys
import json
import time
import hashlib
import logging
import asyncio
import subprocess
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import argparse
from dataclasses import dataclass
from enum import Enum

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.parent
BACKEND_ROOT = PROJECT_ROOT / "backend"
sys.path.insert(0, str(BACKEND_ROOT))

# í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸ ë° ì„¤ì¹˜
REQUIRED_PACKAGES = [
    "requests",
    "tqdm", 
    "huggingface_hub",
    "gdown",
    "gitpython"
]

def install_required_packages():
    """í•„ìˆ˜ íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜"""
    import subprocess
    import sys
    
    for package in REQUIRED_PACKAGES:
        try:
            __import__(package.replace("-", "_"))
        except ImportError:
            print(f"ğŸ“¦ {package} ì„¤ì¹˜ ì¤‘...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹œë„
try:
    install_required_packages()
    import requests
    from tqdm import tqdm
    from huggingface_hub import hf_hub_download, snapshot_download
    import gdown
    from git import Repo
    PACKAGES_AVAILABLE = True
except Exception as e:
    print(f"âŒ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
    print("ìˆ˜ë™ ì„¤ì¹˜ í•„ìš”: pip install requests tqdm huggingface_hub gdown gitpython")
    PACKAGES_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('model_download.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class ModelType(Enum):
    """ëª¨ë¸ íƒ€ì… ì—´ê±°í˜•"""
    DIFFUSION = "diffusion"
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    BACKGROUND_REMOVAL = "background_removal"
    TEXT_IMAGE = "text_image"
    AUXILIARY = "auxiliary"

class DownloadSource(Enum):
    """ë‹¤ìš´ë¡œë“œ ì†ŒìŠ¤ ì—´ê±°í˜•"""
    HUGGINGFACE = "huggingface"
    GOOGLE_DRIVE = "google_drive"
    GITHUB = "github"
    DIRECT_URL = "direct_url"

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ë°ì´í„°í´ë˜ìŠ¤"""
    name: str
    model_type: ModelType
    step: str
    priority: int
    size_mb: float
    download_source: DownloadSource
    source_url: str
    local_path: str
    checkpoints: List[Dict[str, Any]]
    required: bool = True
    sha256: Optional[str] = None
    description: str = ""

class CompleteModelDownloader:
    """ì™„ì „í•œ AI ëª¨ë¸ ë‹¤ìš´ë¡œë”"""
    
    def __init__(self, base_dir: Optional[Path] = None, device: str = "auto"):
        self.base_dir = base_dir or (BACKEND_ROOT / "ai_models")
        self.checkpoints_dir = self.base_dir / "checkpoints"
        self.temp_dir = self.base_dir / "temp"
        self.device = self._detect_device() if device == "auto" else device
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.checkpoints_dir.mkdir(parents=True, exist_ok=True)
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        
        # ë‹¤ìš´ë¡œë“œ í†µê³„
        self.stats = {
            "total_models": 0,
            "downloaded": 0,
            "verified": 0,
            "failed": 0,
            "skipped": 0,
            "total_size_mb": 0,
            "download_time": 0
        }
        
        logger.info(f"ğŸ¤– ì™„ì „í•œ ëª¨ë¸ ë‹¤ìš´ë¡œë” ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"ğŸ“ ê¸°ë³¸ ê²½ë¡œ: {self.base_dir}")
        
        # ê³µì‹ ëª¨ë¸ ì„¤ì • ë¡œë“œ
        self.models = self._load_official_model_configs()
    
    def _detect_device(self) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        try:
            import torch
            if torch.backends.mps.is_available():
                return "mps"  # Apple Silicon
            elif torch.cuda.is_available():
                return "cuda"  # NVIDIA GPU
            else:
                return "cpu"
        except ImportError:
            return "cpu"
    
    def _load_official_model_configs(self) -> Dict[str, ModelConfig]:
        """ê³µì‹ AI ëª¨ë¸ ì„¤ì • ë¡œë“œ"""
        
        models = {
            # ğŸ¯ í•µì‹¬ ê°€ìƒ í”¼íŒ… ëª¨ë¸
            "ootdiffusion": ModelConfig(
                name="ootdiffusion",
                model_type=ModelType.DIFFUSION,
                step="step_06_virtual_fitting",
                priority=1,
                size_mb=15129.3,
                download_source=DownloadSource.HUGGINGFACE,
                source_url="levihsu/OOTDiffusion",
                local_path="ootdiffusion",
                checkpoints=[
                    {"name": "ootd_diffusion.safetensors", "size_mb": 3400.0, "required": True},
                    {"name": "vae_decoder.safetensors", "size_mb": 334.6, "required": True},
                    {"name": "text_encoder.safetensors", "size_mb": 492.5, "required": True},
                    {"name": "unet.safetensors", "size_mb": 3361.7, "required": True}
                ],
                description="OOTD Diffusion - ê³ í’ˆì§ˆ ê°€ìƒ í”¼íŒ… ëª¨ë¸"
            ),
            
            # ğŸ‘¤ ì¸ì²´ íŒŒì‹± ëª¨ë¸
            "human_parsing": ModelConfig(
                name="human_parsing",
                model_type=ModelType.HUMAN_PARSING,
                step="step_01_human_parsing",
                priority=2,
                size_mb=510.1,
                download_source=DownloadSource.HUGGINGFACE,
                source_url="mattmdjaga/segformer_b2_clothes",
                local_path="human_parsing",
                checkpoints=[
                    {"name": "pytorch_model.bin", "size_mb": 255.1, "required": True},
                    {"name": "config.json", "size_mb": 0.5, "required": True}
                ],
                description="SegFormer B2 - ì¸ì²´ íŒŒì‹± ëª¨ë¸"
            ),
            
            # ğŸ¤¸ í¬ì¦ˆ ì¶”ì • ëª¨ë¸
            "pose_estimation": ModelConfig(
                name="pose_estimation", 
                model_type=ModelType.POSE_ESTIMATION,
                step="step_02_pose_estimation",
                priority=3,
                size_mb=200.5,
                download_source=DownloadSource.HUGGINGFACE,
                source_url="lllyasviel/Annotators",
                local_path="pose_estimation",
                checkpoints=[
                    {"name": "body_pose_model.pth", "size_mb": 200.0, "required": True}
                ],
                description="DWPose - ê³ ì •ë°€ í¬ì¦ˆ ì¶”ì •"
            ),
            
            # ğŸ‘• ì˜ë¥˜ ë¶„í•  ëª¨ë¸
            "cloth_segmentation": ModelConfig(
                name="cloth_segmentation",
                model_type=ModelType.CLOTH_SEGMENTATION,
                step="step_03_cloth_segmentation",
                priority=4,
                size_mb=176.3,
                download_source=DownloadSource.HUGGINGFACE,
                source_url="briaai/RMBG-1.4",
                local_path="cloth_segmentation",
                checkpoints=[
                    {"name": "model.safetensors", "size_mb": 176.0, "required": True},
                    {"name": "config.json", "size_mb": 0.3, "required": True}
                ],
                description="RMBG 1.4 - ì˜ë¥˜ ë°°ê²½ ì œê±°"
            ),
            
            # ğŸ–¼ï¸ ë°°ê²½ ì œê±° ëª¨ë¸
            "background_removal": ModelConfig(
                name="background_removal",
                model_type=ModelType.BACKGROUND_REMOVAL,
                step="auxiliary",
                priority=5,
                size_mb=176.3,
                download_source=DownloadSource.HUGGINGFACE,
                source_url="briaai/RMBG-1.4",
                local_path="background_removal",
                checkpoints=[
                    {"name": "model.safetensors", "size_mb": 176.0, "required": True}
                ],
                description="ë°°ê²½ ì œê±° ì „ìš© ëª¨ë¸"
            ),
            
            # ğŸ”— CLIP ëª¨ë¸ë“¤
            "clip_vit_base": ModelConfig(
                name="clip_vit_base",
                model_type=ModelType.TEXT_IMAGE,
                step="auxiliary",
                priority=6,
                size_mb=580.7,
                download_source=DownloadSource.HUGGINGFACE,
                source_url="openai/clip-vit-base-patch32",
                local_path="clip_vit_base",
                checkpoints=[
                    {"name": "pytorch_model.bin", "size_mb": 577.2, "required": True},
                    {"name": "config.json", "size_mb": 0.5, "required": True}
                ],
                description="CLIP ViT-Base - í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì„ë² ë”©"
            ),
            
            "clip_vit_large": ModelConfig(
                name="clip_vit_large",
                model_type=ModelType.TEXT_IMAGE,
                step="auxiliary",
                priority=7,
                size_mb=6527.1,
                download_source=DownloadSource.HUGGINGFACE,
                source_url="openai/clip-vit-large-patch14",
                local_path="clip_vit_large",
                checkpoints=[
                    {"name": "pytorch_model.bin", "size_mb": 1631.4, "required": True}
                ],
                description="CLIP ViT-Large - ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì„ë² ë”©",
                required=False  # ì„ íƒì‚¬í•­
            ),
            
            # ğŸ¨ VITON-HD (ëŒ€ì•ˆ ëª¨ë¸)
            "viton_hd": ModelConfig(
                name="viton_hd",
                model_type=ModelType.DIFFUSION,
                step="step_06_virtual_fitting",
                priority=8,
                size_mb=3500.0,
                download_source=DownloadSource.GITHUB,
                source_url="https://github.com/shadow2496/VITON-HD.git",
                local_path="viton_hd",
                checkpoints=[
                    {"name": "gen_model_020.pth", "size_mb": 500.0, "required": True},
                    {"name": "warp_model_020.pth", "size_mb": 500.0, "required": True}
                ],
                description="VITON-HD - ê³ í•´ìƒë„ ê°€ìƒ í”¼íŒ… (ëŒ€ì•ˆ)",
                required=False
            )
        }
        
        self.stats["total_models"] = len(models)
        total_size = sum(model.size_mb for model in models.values() if model.required)
        self.stats["total_size_mb"] = total_size
        
        logger.info(f"ğŸ“‹ ë¡œë“œëœ ëª¨ë¸: {len(models)}ê°œ")
        logger.info(f"ğŸ’¾ ì´ í•„ìš” ìš©ëŸ‰: {total_size:.1f}MB ({total_size/1024:.1f}GB)")
        
        return models
    
    async def verify_model(self, model_config: ModelConfig) -> Dict[str, Any]:
        """ëª¨ë¸ ê²€ì¦"""
        model_path = self.checkpoints_dir / model_config.local_path
        
        verification_result = {
            "name": model_config.name,
            "exists": False,
            "complete": False,
            "verified_checkpoints": [],
            "missing_checkpoints": [],
            "size_mb": 0,
            "status": "missing"
        }
        
        if not model_path.exists():
            verification_result["status"] = "missing"
            return verification_result
        
        verification_result["exists"] = True
        total_size = 0
        
        for checkpoint in model_config.checkpoints:
            checkpoint_path = model_path / checkpoint["name"]
            
            if checkpoint_path.exists():
                size_mb = checkpoint_path.stat().st_size / (1024 * 1024)
                total_size += size_mb
                
                verification_result["verified_checkpoints"].append({
                    "name": checkpoint["name"],
                    "size_mb": size_mb,
                    "path": str(checkpoint_path)
                })
            else:
                verification_result["missing_checkpoints"].append(checkpoint["name"])
        
        verification_result["size_mb"] = total_size
        
        # ì™„ì „ì„± í™•ì¸
        required_checkpoints = [cp for cp in model_config.checkpoints if cp.get("required", True)]
        verified_names = [cp["name"] for cp in verification_result["verified_checkpoints"]]
        
        all_required_present = all(
            cp["name"] in verified_names for cp in required_checkpoints
        )
        
        if all_required_present:
            verification_result["complete"] = True
            verification_result["status"] = "verified"
            self.stats["verified"] += 1
        else:
            verification_result["status"] = "incomplete"
        
        return verification_result
    
    async def download_from_huggingface(self, model_config: ModelConfig) -> bool:
        """Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            local_dir = self.checkpoints_dir / model_config.local_path
            local_dir.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"ğŸ“¥ Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œ: {model_config.source_url}")
            
            # snapshot_downloadë¡œ ì „ì²´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            downloaded_path = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: snapshot_download(
                    repo_id=model_config.source_url,
                    local_dir=str(local_dir),
                    local_dir_use_symlinks=False,
                    resume_download=True
                )
            )
            
            logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {downloaded_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Hugging Face ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def download_from_github(self, model_config: ModelConfig) -> bool:
        """GitHubì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            local_dir = self.checkpoints_dir / model_config.local_path
            
            if local_dir.exists():
                shutil.rmtree(local_dir)
            
            logger.info(f"ğŸ“¥ GitHubì—ì„œ í´ë¡ : {model_config.source_url}")
            
            # Git clone ì‹¤í–‰
            await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: Repo.clone_from(model_config.source_url, str(local_dir))
            )
            
            logger.info(f"âœ… GitHub í´ë¡  ì™„ë£Œ: {local_dir}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ GitHub ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def download_from_google_drive(self, model_config: ModelConfig) -> bool:
        """Google Driveì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            local_dir = self.checkpoints_dir / model_config.local_path
            local_dir.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"ğŸ“¥ Google Driveì—ì„œ ë‹¤ìš´ë¡œë“œ: {model_config.source_url}")
            
            # gdownìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
            output_path = local_dir / "model.zip"
            await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: gdown.download(model_config.source_url, str(output_path), quiet=False)
            )
            
            # ì••ì¶• í•´ì œ
            if output_path.exists():
                shutil.unpack_archive(str(output_path), str(local_dir))
                output_path.unlink()  # ì••ì¶• íŒŒì¼ ì‚­ì œ
            
            logger.info(f"âœ… Google Drive ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_dir}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Google Drive ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def download_model(self, model_config: ModelConfig) -> bool:
        """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì†ŒìŠ¤ë³„ ë¶„ê¸°)"""
        try:
            download_start = time.time()
            
            # ë‹¤ìš´ë¡œë“œ ì†ŒìŠ¤ë³„ ì²˜ë¦¬
            if model_config.download_source == DownloadSource.HUGGINGFACE:
                success = await self.download_from_huggingface(model_config)
            elif model_config.download_source == DownloadSource.GITHUB:
                success = await self.download_from_github(model_config)
            elif model_config.download_source == DownloadSource.GOOGLE_DRIVE:
                success = await self.download_from_google_drive(model_config)
            else:
                logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë‹¤ìš´ë¡œë“œ ì†ŒìŠ¤: {model_config.download_source}")
                return False
            
            download_time = time.time() - download_start
            self.stats["download_time"] += download_time
            
            if success:
                self.stats["downloaded"] += 1
                logger.info(f"âœ… {model_config.name} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({download_time:.1f}ì´ˆ)")
            else:
                self.stats["failed"] += 1
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ {model_config.name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.stats["failed"] += 1
            return False
    
    async def verify_and_download_all(self, download_optional: bool = False) -> Dict[str, Any]:
        """ëª¨ë“  ëª¨ë¸ ê²€ì¦ ë° ë‹¤ìš´ë¡œë“œ"""
        logger.info("ğŸš€ ì „ì²´ ëª¨ë¸ ê²€ì¦ ë° ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        
        verification_results = {}
        download_queue = []
        
        # 1ë‹¨ê³„: ëª¨ë“  ëª¨ë¸ ê²€ì¦
        logger.info("ğŸ” 1ë‹¨ê³„: ëª¨ë“  ëª¨ë¸ ê²€ì¦ ì¤‘...")
        
        for model_name, model_config in self.models.items():
            if not model_config.required and not download_optional:
                logger.info(f"â­ï¸ ì„ íƒì‚¬í•­ ëª¨ë¸ ìŠ¤í‚µ: {model_name}")
                self.stats["skipped"] += 1
                continue
            
            verification_result = await self.verify_model(model_config)
            verification_results[model_name] = verification_result
            
            logger.info(f"ğŸ“ {model_name}: {verification_result['status']} ({verification_result['size_mb']:.1f}MB)")
            
            if verification_result["status"] in ["missing", "incomplete"]:
                download_queue.append(model_config)
        
        # 2ë‹¨ê³„: í•„ìš”í•œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        if download_queue:
            logger.info(f"ğŸ“¥ 2ë‹¨ê³„: {len(download_queue)}ê°œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            
            for model_config in download_queue:
                logger.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_config.name} ({model_config.size_mb:.1f}MB)")
                
                success = await self.download_model(model_config)
                
                if success:
                    # ë‹¤ìš´ë¡œë“œ í›„ ì¬ê²€ì¦
                    verification_result = await self.verify_model(model_config)
                    verification_results[model_config.name] = verification_result
                    
                    if verification_result["status"] == "verified":
                        logger.info(f"âœ… {model_config.name} ì™„ì „íˆ ì„¤ì¹˜ë¨")
                    else:
                        logger.warning(f"âš ï¸ {model_config.name} ë‹¤ìš´ë¡œë“œëì§€ë§Œ ê²€ì¦ ì‹¤íŒ¨")
                else:
                    logger.error(f"âŒ {model_config.name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        else:
            logger.info("âœ… ëª¨ë“  í•„ìš”í•œ ëª¨ë¸ì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        
        return {
            "verification_results": verification_results,
            "stats": self.stats,
            "device": self.device,
            "total_models": len(self.models),
            "ready_models": len([r for r in verification_results.values() if r["status"] == "verified"])
        }
    
    def generate_model_summary(self, results: Dict[str, Any]) -> str:
        """ëª¨ë¸ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        verification_results = results["verification_results"]
        stats = results["stats"]
        
        summary = f"""
ğŸ¤– MyCloset AI ëª¨ë¸ ì„¤ì¹˜ ìš”ì•½ ë³´ê³ ì„œ
{'='*50}

ğŸ“Š ì „ì²´ í†µê³„:
  - ì´ ëª¨ë¸ ìˆ˜: {stats['total_models']}ê°œ
  - ê²€ì¦ëœ ëª¨ë¸: {stats['verified']}ê°œ
  - ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸: {stats['downloaded']}ê°œ
  - ì‹¤íŒ¨í•œ ëª¨ë¸: {stats['failed']}ê°œ
  - ìŠ¤í‚µí•œ ëª¨ë¸: {stats['skipped']}ê°œ
  - ì´ ë‹¤ìš´ë¡œë“œ ì‹œê°„: {stats['download_time']:.1f}ì´ˆ

ğŸ¯ ë””ë°”ì´ìŠ¤: {results['device']}
ğŸ’¾ ì´ ì„¤ì¹˜ ìš©ëŸ‰: {sum(r['size_mb'] for r in verification_results.values()):.1f}MB

ğŸ“‹ ëª¨ë¸ë³„ ìƒíƒœ:
"""
        
        for model_name, result in verification_results.items():
            status_emoji = {
                "verified": "âœ…",
                "incomplete": "âš ï¸",
                "missing": "âŒ"
            }.get(result["status"], "â“")
            
            summary += f"  {status_emoji} {model_name}: {result['status']} ({result['size_mb']:.1f}MB)\n"
        
        if stats['failed'] > 0:
            summary += f"\nâŒ ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤ì„ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        if results['ready_models'] == len([m for m in self.models.values() if m.required]):
            summary += f"\nğŸ‰ ëª¨ë“  í•„ìˆ˜ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!"
        
        return summary
    
    def save_results(self, results: Dict[str, Any]) -> Path:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        results_file = self.base_dir / "model_verification_results.json"
        
        # ê²°ê³¼ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        serializable_results = {
            "timestamp": time.time(),
            "device": results["device"],
            "stats": results["stats"],
            "verification_results": results["verification_results"],
            "ready_models": results["ready_models"],
            "total_models": results["total_models"]
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {results_file}")
        return results_file

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="MyCloset AI ëª¨ë¸ ê²€ì¦ ë° ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--download-optional", action="store_true", help="ì„ íƒì‚¬í•­ ëª¨ë¸ë„ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--models-dir", type=str, help="ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--device", type=str, default="auto", choices=["auto", "cuda", "mps", "cpu"], help="ë””ë°”ì´ìŠ¤ ì„¤ì •")
    
    args = parser.parse_args()
    
    print("ğŸ¤– MyCloset AI - ì™„ì „í•œ ëª¨ë¸ ê²€ì¦ ë° ë‹¤ìš´ë¡œë“œ")
    print("=" * 60)
    
    if not PACKAGES_AVAILABLE:
        print("âŒ í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # ë‹¤ìš´ë¡œë” ì´ˆê¸°í™”
    models_dir = Path(args.models_dir) if args.models_dir else None
    downloader = CompleteModelDownloader(base_dir=models_dir, device=args.device)
    
    try:
        # ëª¨ë¸ ê²€ì¦ ë° ë‹¤ìš´ë¡œë“œ
        results = await downloader.verify_and_download_all(
            download_optional=args.download_optional
        )
        
        # ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥
        summary = downloader.generate_model_summary(results)
        print(summary)
        
        # ê²°ê³¼ ì €ì¥
        results_file = downloader.save_results(results)
        
        print(f"\nğŸ“„ ìƒì„¸ ê²°ê³¼: {results_file}")
        print("\nğŸ‰ ëª¨ë¸ ê²€ì¦ ë° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

if __name__ == "__main__":
    asyncio.run(main())