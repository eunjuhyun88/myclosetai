#!/usr/bin/env python3
"""
π” μ²΄ν¬ν¬μΈνΈ νμΌ νΉν™” νƒμ§€ λ° κ²€μ¦ μ¤ν¬λ¦½νΈ
β… .pth, .pt νμΌ μ „λ¬Έ νƒμ§€
β… μ‹¤μ  PyTorch μ²΄ν¬ν¬μΈνΈ λ‚΄μ© κ²€μ¦
β… λ„λ½λ λ¨λΈ λ‹¤μ΄λ΅λ“ κ°€μ΄λ“
β… M3 Max μµμ ν™”
"""

import os
import sys
import time
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import logging

# PyTorch μ²΄ν¬ν¬μΈνΈ κ²€μ¦μ©
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# λ΅κΉ… μ„¤μ •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class CheckpointInfo:
    """μ²΄ν¬ν¬μΈνΈ νμΌ μ •λ³΄"""
    file_path: Path
    file_size_mb: float
    model_type: str
    confidence: float
    pytorch_valid: bool
    contains_state_dict: bool
    parameter_count: int
    layers_info: Dict[str, Any]
    architecture_info: Dict[str, Any]
    checksum: str

class CheckpointFinder:
    """μ²΄ν¬ν¬μΈνΈ νμΌ μ „λ¬Έ νƒμ§€κΈ°"""
    
    def __init__(self, project_root: Optional[Path] = None):
        if project_root is None:
            current_file = Path(__file__).resolve()
            self.project_root = current_file.parent.parent
        else:
            self.project_root = Path(project_root)
        
        self.backend_dir = self.project_root
        self.checkpoints: Dict[str, List[CheckpointInfo]] = {}
        
        # μ²΄ν¬ν¬μΈνΈ ν¨ν„΄ (λ” κµ¬μ²΄μ )
        self.checkpoint_patterns = {
            "human_parsing": {
                "keywords": ["human", "parsing", "atr", "schp", "graphonomy", "segmentation"],
                "expected_size_range": (50, 500),  # MB
                "required_layers": ["backbone", "classifier", "conv"]
            },
            "pose_estimation": {
                "keywords": ["pose", "openpose", "body", "keypoint", "coco"],
                "expected_size_range": (10, 1000),
                "required_layers": ["stage", "paf", "heatmap"]
            },
            "cloth_segmentation": {
                "keywords": ["u2net", "cloth", "segmentation", "mask"],
                "expected_size_range": (10, 200),
                "required_layers": ["encoder", "decoder", "side"]
            },
            "geometric_matching": {
                "keywords": ["gmm", "geometric", "matching", "tps"],
                "expected_size_range": (1, 100),
                "required_layers": ["extractor", "regression"]
            },
            "cloth_warping": {
                "keywords": ["tom", "warping", "viton", "try_on"],
                "expected_size_range": (10, 200),
                "required_layers": ["generator", "unet"]
            },
            "virtual_fitting": {
                "keywords": ["hrviton", "viton_hd", "hr_viton"],
                "expected_size_range": (100, 2000),
                "required_layers": ["unet", "vae", "text_encoder"]
            }
        }
        
        logger.info(f"π” μ²΄ν¬ν¬μΈνΈ μ „λ¬Έ νƒμ§€κΈ° μ΄κΈ°ν™” - λ£¨νΈ: {self.project_root}")

    def find_all_checkpoints(self) -> Dict[str, List[CheckpointInfo]]:
        """λ¨λ“  μ²΄ν¬ν¬μΈνΈ νμΌ νƒμ§€"""
        logger.info("π” μ²΄ν¬ν¬μΈνΈ νμΌ μ „μ²΄ νƒμ§€ μ‹μ‘...")
        
        # κ²€μƒ‰ κ²½λ΅λ“¤
        search_paths = self._get_checkpoint_search_paths()
        
        all_checkpoint_files = []
        
        # .pth, .pt νμΌλ“¤ μμ§‘
        for search_path in search_paths:
            if search_path.exists():
                logger.info(f"π“ μ²΄ν¬ν¬μΈνΈ κ²€μƒ‰ μ¤‘: {search_path}")
                checkpoint_files = self._find_checkpoint_files(search_path)
                all_checkpoint_files.extend(checkpoint_files)
                logger.info(f"   β””β”€ λ°κ²¬: {len(checkpoint_files)}κ° μ²΄ν¬ν¬μΈνΈ")
        
        logger.info(f"π“ μ΄ μ²΄ν¬ν¬μΈνΈ νμΌ: {len(all_checkpoint_files)}κ°")
        
        # κ° νμΌ λ¶„μ„
        for checkpoint_file in all_checkpoint_files:
            try:
                checkpoint_info = self._analyze_checkpoint(checkpoint_file)
                if checkpoint_info:
                    model_type = checkpoint_info.model_type
                    if model_type not in self.checkpoints:
                        self.checkpoints[model_type] = []
                    self.checkpoints[model_type].append(checkpoint_info)
                    
            except Exception as e:
                logger.debug(f"μ²΄ν¬ν¬μΈνΈ λ¶„μ„ μ‹¤ν¨ {checkpoint_file}: {e}")
        
        self._print_checkpoint_summary()
        return self.checkpoints

    def _get_checkpoint_search_paths(self) -> List[Path]:
        """μ²΄ν¬ν¬μΈνΈ κ²€μƒ‰ κ²½λ΅"""
        paths = []
        
        # ν”„λ΅μ νΈ λ‚΄λ¶€
        project_paths = [
            self.backend_dir / "ai_models",
            self.backend_dir / "checkpoints", 
            self.backend_dir / "weights",
            self.backend_dir / "models",
            self.backend_dir / "pretrained",
            self.backend_dir / "app" / "ai_pipeline" / "models",
            self.backend_dir / ".." / "ai_models",
        ]
        
        # μ‚¬μ©μ μΊμ‹
        home = Path.home()
        cache_paths = [
            home / ".cache" / "torch" / "hub",
            home / ".cache" / "huggingface" / "hub",
            home / "Downloads",
            home / "Desktop",
            home / "Documents",
        ]
        
        # conda ν™κ²½
        conda_prefix = os.environ.get('CONDA_PREFIX')
        if conda_prefix:
            conda_paths = [
                Path(conda_prefix) / "lib" / "python3.11" / "site-packages",
                Path(conda_prefix) / "share",
            ]
            cache_paths.extend(conda_paths)
        
        all_paths = project_paths + cache_paths
        return [p for p in all_paths if p.exists()]

    def _find_checkpoint_files(self, directory: Path, max_depth: int = 5) -> List[Path]:
        """λ””λ ‰ν† λ¦¬μ—μ„ μ²΄ν¬ν¬μΈνΈ νμΌ μ°ΎκΈ°"""
        checkpoint_files = []
        
        try:
            for item in directory.rglob("*"):
                if item.is_file() and item.suffix.lower() in ['.pth', '.pt']:
                    # νμΌ ν¬κΈ° ν™•μΈ (μµμ† 1MB)
                    try:
                        size_mb = item.stat().st_size / (1024 * 1024)
                        if size_mb >= 1.0:
                            checkpoint_files.append(item)
                    except:
                        continue
        except Exception as e:
            logger.debug(f"λ””λ ‰ν† λ¦¬ μ¤μΊ” μ¤λ¥ {directory}: {e}")
        
        return checkpoint_files

    def _analyze_checkpoint(self, checkpoint_file: Path) -> Optional[CheckpointInfo]:
        """μ²΄ν¬ν¬μΈνΈ νμΌ μƒμ„Έ λ¶„μ„"""
        try:
            file_size_mb = checkpoint_file.stat().st_size / (1024 * 1024)
            
            # PyTorch μ²΄ν¬ν¬μΈνΈ κ²€μ¦
            pytorch_valid = False
            contains_state_dict = False
            parameter_count = 0
            layers_info = {}
            architecture_info = {}
            
            if TORCH_AVAILABLE:
                try:
                    # μ•μ „ν• λ΅λ”©
                    checkpoint = torch.load(checkpoint_file, map_location='cpu', weights_only=True)
                    pytorch_valid = True
                    
                    # state_dict ν™•μΈ
                    if isinstance(checkpoint, dict):
                        if 'state_dict' in checkpoint:
                            state_dict = checkpoint['state_dict']
                            contains_state_dict = True
                        elif 'model' in checkpoint:
                            state_dict = checkpoint['model']
                            contains_state_dict = True
                        else:
                            # checkpoint μμ²΄κ°€ state_dictμΈ κ²½μ°
                            state_dict = checkpoint
                            contains_state_dict = True
                        
                        # νλΌλ―Έν„° μ κ³„μ‚°
                        if isinstance(state_dict, dict):
                            for key, tensor in state_dict.items():
                                if torch.is_tensor(tensor):
                                    parameter_count += tensor.numel()
                            
                            # λ μ΄μ–΄ μ •λ³΄ μ¶”μ¶
                            layers_info = self._extract_layer_info(state_dict)
                        
                        # μ•„ν‚¤ν…μ² μ •λ³΄ μ¶”μ¶
                        architecture_info = self._extract_architecture_info(checkpoint, checkpoint_file.name)
                    
                except Exception as e:
                    logger.debug(f"PyTorch λ΅λ”© μ‹¤ν¨ {checkpoint_file}: {e}")
            
            # λ¨λΈ νƒ€μ… λ¶„λ¥
            model_type = self._classify_checkpoint(checkpoint_file, file_size_mb, layers_info)
            confidence = self._calculate_confidence(checkpoint_file, file_size_mb, layers_info, model_type)
            
            if confidence < 0.3:
                return None
            
            # μ²΄ν¬μ„¬ κ³„μ‚°
            checksum = self._calculate_checksum(checkpoint_file)
            
            return CheckpointInfo(
                file_path=checkpoint_file,
                file_size_mb=file_size_mb,
                model_type=model_type,
                confidence=confidence,
                pytorch_valid=pytorch_valid,
                contains_state_dict=contains_state_dict,
                parameter_count=parameter_count,
                layers_info=layers_info,
                architecture_info=architecture_info,
                checksum=checksum
            )
            
        except Exception as e:
            logger.debug(f"μ²΄ν¬ν¬μΈνΈ λ¶„μ„ μ‹¤ν¨ {checkpoint_file}: {e}")
            return None

    def _extract_layer_info(self, state_dict: Dict) -> Dict[str, Any]:
        """λ μ΄μ–΄ μ •λ³΄ μ¶”μ¶"""
        layer_info = {
            "total_layers": len(state_dict),
            "layer_types": {},
            "key_patterns": set()
        }
        
        for key in state_dict.keys():
            # λ μ΄μ–΄ νƒ€μ… λ¶„λ¥
            if 'conv' in key.lower():
                layer_info["layer_types"]["conv"] = layer_info["layer_types"].get("conv", 0) + 1
            elif 'linear' in key.lower() or 'fc' in key.lower():
                layer_info["layer_types"]["linear"] = layer_info["layer_types"].get("linear", 0) + 1
            elif 'bn' in key.lower() or 'batch' in key.lower():
                layer_info["layer_types"]["batch_norm"] = layer_info["layer_types"].get("batch_norm", 0) + 1
            elif 'attention' in key.lower() or 'attn' in key.lower():
                layer_info["layer_types"]["attention"] = layer_info["layer_types"].get("attention", 0) + 1
            
            # ν‚¤ ν¨ν„΄ μ¶”μ¶
            key_parts = key.split('.')
            if len(key_parts) > 1:
                layer_info["key_patterns"].add(key_parts[0])
        
        layer_info["key_patterns"] = list(layer_info["key_patterns"])
        return layer_info

    def _extract_architecture_info(self, checkpoint: Dict, filename: str) -> Dict[str, Any]:
        """μ•„ν‚¤ν…μ² μ •λ³΄ μ¶”μ¶"""
        arch_info = {
            "filename": filename,
            "metadata": {}
        }
        
        # λ©”νƒ€λ°μ΄ν„° μ¶”μ¶
        meta_keys = ['arch', 'model_name', 'version', 'epoch', 'config']
        for key in meta_keys:
            if key in checkpoint:
                arch_info["metadata"][key] = str(checkpoint[key])
        
        return arch_info

    def _classify_checkpoint(self, file_path: Path, file_size_mb: float, layers_info: Dict) -> str:
        """μ²΄ν¬ν¬μΈνΈ νμΌ λ¶„λ¥"""
        file_name = file_path.name.lower()
        file_path_str = str(file_path).lower()
        
        best_type = "unknown"
        best_score = 0
        
        for model_type, pattern_info in self.checkpoint_patterns.items():
            score = 0
            
            # ν‚¤μ›λ“ λ§¤μΉ­
            for keyword in pattern_info["keywords"]:
                if keyword in file_name:
                    score += 15
                elif keyword in file_path_str:
                    score += 8
            
            # νμΌ ν¬κΈ° ν™•μΈ
            min_size, max_size = pattern_info["expected_size_range"]
            if min_size <= file_size_mb <= max_size:
                score += 10
            
            # λ μ΄μ–΄ ν¨ν„΄ ν™•μΈ
            if layers_info and "key_patterns" in layers_info:
                required_layers = pattern_info.get("required_layers", [])
                for required_layer in required_layers:
                    for pattern in layers_info["key_patterns"]:
                        if required_layer in pattern.lower():
                            score += 5
                            break
            
            if score > best_score:
                best_score = score
                best_type = model_type
        
        return best_type

    def _calculate_confidence(self, file_path: Path, file_size_mb: float, layers_info: Dict, model_type: str) -> float:
        """μ‹ λΆ°λ„ κ³„μ‚°"""
        if model_type == "unknown":
            return 0.0
        
        confidence = 0.3  # κΈ°λ³Έ μ‹ λΆ°λ„
        
        # PyTorch μ ν¨μ„±
        if layers_info:
            confidence += 0.2
        
        # νμΌ ν¬κΈ° μ μ •μ„±
        if model_type in self.checkpoint_patterns:
            min_size, max_size = self.checkpoint_patterns[model_type]["expected_size_range"]
            if min_size <= file_size_mb <= max_size:
                confidence += 0.3
        
        # λ μ΄μ–΄ κµ¬μ΅° μ μ •μ„±
        if layers_info and "layer_types" in layers_info:
            if len(layers_info["layer_types"]) > 1:
                confidence += 0.2
        
        return min(confidence, 1.0)

    def _calculate_checksum(self, file_path: Path) -> str:
        """μ²΄ν¬μ„¬ κ³„μ‚°"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                # μ²μ 1MBλ§ μ½μ–΄μ„ λΉ λ¥΄κ² μ²΄ν¬μ„¬ κ³„μ‚°
                chunk = f.read(1024 * 1024)
                hash_md5.update(chunk)
            return hash_md5.hexdigest()[:12]
        except:
            return "unknown"

    def _print_checkpoint_summary(self):
        """μ²΄ν¬ν¬μΈνΈ νƒμ§€ κ²°κ³Ό μ”μ•½"""
        logger.info("=" * 70)
        logger.info("π― μ²΄ν¬ν¬μΈνΈ νμΌ νƒμ§€ κ²°κ³Ό")
        logger.info("=" * 70)
        
        total_checkpoints = sum(len(checkpoints) for checkpoints in self.checkpoints.values())
        logger.info(f"π“ μ΄ μ ν¨ μ²΄ν¬ν¬μΈνΈ: {total_checkpoints}κ°")
        
        for model_type, checkpoints in self.checkpoints.items():
            if checkpoints:
                logger.info(f"\nπ“ {model_type}:")
                for i, checkpoint in enumerate(checkpoints[:3]):  # μƒμ„ 3κ°λ§ ν‘μ‹
                    logger.info(f"  {i+1}. {checkpoint.file_path.name}")
                    logger.info(f"     ν¬κΈ°: {checkpoint.file_size_mb:.1f}MB")
                    logger.info(f"     μ‹ λΆ°λ„: {checkpoint.confidence:.2f}")
                    logger.info(f"     νλΌλ―Έν„°: {checkpoint.parameter_count:,}κ°")
                    logger.info(f"     μ ν¨: {'β…' if checkpoint.pytorch_valid else 'β'}")
                
                if len(checkpoints) > 3:
                    logger.info(f"  ... μ™Έ {len(checkpoints) - 3}κ°")

    def generate_missing_models_report(self) -> Dict[str, Any]:
        """λ„λ½λ λ¨λΈ λ¦¬ν¬νΈ μƒμ„±"""
        required_models = [
            "human_parsing", "pose_estimation", "cloth_segmentation",
            "geometric_matching", "cloth_warping", "virtual_fitting"
        ]
        
        found_models = set(self.checkpoints.keys())
        missing_models = [model for model in required_models if model not in found_models]
        
        report = {
            "total_required": len(required_models),
            "found_count": len(found_models),
            "missing_count": len(missing_models),
            "found_models": list(found_models),
            "missing_models": missing_models,
            "download_suggestions": {}
        }
        
        # λ‹¤μ΄λ΅λ“ μ μ•
        download_suggestions = {
            "human_parsing": {
                "name": "Graphonomy Human Parsing",
                "url": "https://github.com/Gaoyiminggithub/Graphonomy",
                "files": ["graphonomy.pth", "exp-schp-201908261155-pascal-person-part.pth"]
            },
            "pose_estimation": {
                "name": "OpenPose Body Pose",
                "url": "https://github.com/CMU-Perceptual-Computing-Lab/openpose",
                "files": ["body_pose_model.pth", "pose_model.pth"]
            },
            "cloth_segmentation": {
                "name": "UΒ²-Net Cloth Segmentation", 
                "url": "https://github.com/xuebinqin/U-2-Net",
                "files": ["u2net.pth", "u2netp.pth"]
            },
            "geometric_matching": {
                "name": "CP-VTON Geometric Matching",
                "url": "https://github.com/sergeywong/cp-vton",
                "files": ["gmm_final.pth", "gmm_traintest_final.pth"]
            },
            "cloth_warping": {
                "name": "TOM (Try-On Module)",
                "url": "https://github.com/sergeywong/cp-vton", 
                "files": ["tom_final.pth", "tom_traintest_final.pth"]
            },
            "virtual_fitting": {
                "name": "HR-VITON",
                "url": "https://github.com/sangyun884/HR-VITON",
                "files": ["hrviton_final.pth", "hr_viton.pth"]
            }
        }
        
        for missing_model in missing_models:
            if missing_model in download_suggestions:
                report["download_suggestions"][missing_model] = download_suggestions[missing_model]
        
        return report

    def create_checkpoint_relocate_plan(self) -> Dict[str, Any]:
        """μ²΄ν¬ν¬μΈνΈ μ¬λ°°μΉ κ³„ν μƒμ„±"""
        plan = {
            "actions": [],
            "summary": {}
        }
        
        target_mapping = {
            "human_parsing": "ai_models/checkpoints/step_01_human_parsing/graphonomy.pth",
            "pose_estimation": "ai_models/checkpoints/step_02_pose_estimation/openpose.pth",
            "cloth_segmentation": "ai_models/checkpoints/step_03_cloth_segmentation/u2net.pth",
            "geometric_matching": "ai_models/checkpoints/gmm_final.pth", 
            "cloth_warping": "ai_models/checkpoints/tom_final.pth",
            "virtual_fitting": "ai_models/checkpoints/hrviton_final.pth"
        }
        
        for model_type, checkpoints in self.checkpoints.items():
            if checkpoints and model_type in target_mapping:
                # μµκ³  μ‹ λΆ°λ„ μ²΄ν¬ν¬μΈνΈ μ„ νƒ
                best_checkpoint = max(checkpoints, key=lambda c: c.confidence)
                
                plan["actions"].append({
                    "model_type": model_type,
                    "source": str(best_checkpoint.file_path),
                    "target": target_mapping[model_type],
                    "size_mb": best_checkpoint.file_size_mb,
                    "confidence": best_checkpoint.confidence,
                    "action": "symlink" if best_checkpoint.file_size_mb > 100 else "copy"
                })
        
        plan["summary"] = {
            "total_actions": len(plan["actions"]),
            "total_size_gb": sum(action["size_mb"] for action in plan["actions"]) / 1024
        }
        
        return plan

def main():
    """λ©”μΈ μ‹¤ν–‰ ν•¨μ"""
    logger.info("=" * 70)
    logger.info("π” μ²΄ν¬ν¬μΈνΈ νμΌ νΉν™” νƒμ§€ μ‹μ‘")
    logger.info("=" * 70)
    
    # μ²΄ν¬ν¬μΈνΈ νƒμ§€
    finder = CheckpointFinder()
    checkpoints = finder.find_all_checkpoints()
    
    if not checkpoints:
        logger.error("β μ²΄ν¬ν¬μΈνΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤")
        return False
    
    # λ„λ½λ λ¨λΈ λ¶„μ„
    missing_report = finder.generate_missing_models_report()
    
    logger.info("\n" + "=" * 70)
    logger.info("π“‹ λ„λ½λ λ¨λΈ λ¶„μ„ κ²°κ³Ό")
    logger.info("=" * 70)
    logger.info(f"π“ ν•„μ”ν• λ¨λΈ: {missing_report['total_required']}κ°")
    logger.info(f"β… λ°κ²¬λ λ¨λΈ: {missing_report['found_count']}κ°")
    logger.info(f"β λ„λ½λ λ¨λΈ: {missing_report['missing_count']}κ°")
    
    if missing_report["missing_models"]:
        logger.info("\nβ λ„λ½λ λ¨λΈλ“¤:")
        for missing_model in missing_report["missing_models"]:
            logger.info(f"  - {missing_model}")
            if missing_model in missing_report["download_suggestions"]:
                suggestion = missing_report["download_suggestions"][missing_model]
                logger.info(f"    λ‹¤μ΄λ΅λ“: {suggestion['name']}")
                logger.info(f"    URL: {suggestion['url']}")
                logger.info(f"    νμΌ: {', '.join(suggestion['files'])}")
    
    # μ¬λ°°μΉ κ³„ν
    relocate_plan = finder.create_checkpoint_relocate_plan()
    
    logger.info("\n" + "=" * 70)
    logger.info("π€ μ²΄ν¬ν¬μΈνΈ μ¬λ°°μΉ κ³„ν")
    logger.info("=" * 70)
    logger.info(f"π“ μ¬λ°°μΉ λ€μƒ: {relocate_plan['summary']['total_actions']}κ°")
    logger.info(f"π’Ύ μ΄ ν¬κΈ°: {relocate_plan['summary']['total_size_gb']:.2f}GB")
    
    for action in relocate_plan["actions"]:
        logger.info(f"\nπ“ {action['model_type']}:")
        logger.info(f"  μ†μ¤: {action['source']}")
        logger.info(f"  νƒ€κ²: {action['target']}")
        logger.info(f"  ν¬κΈ°: {action['size_mb']:.1f}MB")
        logger.info(f"  μ•΅μ…: {action['action']}")
    
    # κ²°κ³Ό νμΌ μ €μ¥
    results = {
        "checkpoints": {
            model_type: [
                {
                    "file_path": str(cp.file_path),
                    "file_size_mb": cp.file_size_mb,
                    "confidence": cp.confidence,
                    "pytorch_valid": cp.pytorch_valid,
                    "parameter_count": cp.parameter_count
                }
                for cp in checkpoints_list
            ]
            for model_type, checkpoints_list in checkpoints.items()
        },
        "missing_report": missing_report,
        "relocate_plan": relocate_plan
    }
    
    results_file = Path("checkpoint_analysis_results.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nβ… μƒμ„Έ κ²°κ³Ό μ €μ¥: {results_file}")
    
    return len(checkpoints) > 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)