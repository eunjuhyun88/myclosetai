#!/usr/bin/env python3
# backend/final_system_test.py
"""
ğŸ‰ MyCloset AI ìµœì¢… ì‹œìŠ¤í…œ ê²€ì¦
âœ… ëª¨ë“  Step ë¡œë“œ í™•ì¸
âœ… AI íŒŒì´í”„ë¼ì¸ ë™ì‘ ê²€ì¦
âœ… ì‹¤ì œ ì²˜ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
"""

import sys
import os
import asyncio
import traceback
from pathlib import Path

# ê²½ë¡œ ì„¤ì •
backend_root = Path(__file__).parent
sys.path.insert(0, str(backend_root))

def test_all_steps():
    """ëª¨ë“  Step ê°œë³„ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ê°œë³„ Step í…ŒìŠ¤íŠ¸...")
    
    steps_to_test = [
        ("HumanParsingStep", "app.ai_pipeline.steps.step_01_human_parsing"),
        ("PoseEstimationStep", "app.ai_pipeline.steps.step_02_pose_estimation"), 
        ("ClothSegmentationStep", "app.ai_pipeline.steps.step_03_cloth_segmentation"),
        ("VirtualFittingStep", "app.ai_pipeline.steps.step_06_virtual_fitting"),
        ("PostProcessingStep", "app.ai_pipeline.steps.step_07_post_processing")
    ]
    
    results = {}
    
    for step_name, module_path in steps_to_test:
        try:
            module_name = module_path.split('.')[-1]
            module = __import__(module_path, fromlist=[step_name])
            step_class = getattr(module, step_name)
            
            # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
            step_instance = step_class(device='cpu', strict_mode=False)
            status = step_instance.get_status()
            
            results[step_name] = {
                'import': True,
                'instance': True,
                'initialized': status.get('initialized', False),
                'step_name': status.get('step_name', step_name)
            }
            print(f"  âœ… {step_name}: ì •ìƒ ({status.get('step_name', 'Unknown')})")
            
        except Exception as e:
            results[step_name] = {
                'import': False,
                'instance': False,
                'error': str(e)
            }
            print(f"  âŒ {step_name}: {e}")
    
    return results

def test_pipeline_manager():
    """PipelineManager í…ŒìŠ¤íŠ¸"""
    print("\nğŸš€ PipelineManager í…ŒìŠ¤íŠ¸...")
    
    try:
        from app.ai_pipeline.pipeline_manager import (
            PipelineManager, 
            create_pipeline,
            PipelineConfig
        )
        
        # ì„¤ì • ìƒì„±
        config = PipelineConfig(
            device='cpu',
            optimize_for_m3_max=True,
            enable_caching=True
        )
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        pipeline = create_pipeline(config)
        print(f"  âœ… PipelineManager ìƒì„±: {type(pipeline)}")
        
        # ìƒíƒœ í™•ì¸
        status = pipeline.get_status()
        print(f"  ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ:")
        print(f"    - í™œì„±í™”ëœ Step: {len(status.get('active_steps', []))}")
        print(f"    - ë””ë°”ì´ìŠ¤: {status.get('device', 'unknown')}")
        print(f"    - M3 Max ìµœì í™”: {status.get('m3_max_optimized', False)}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ PipelineManager í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print(f"  ğŸ“‹ ìƒì„¸:\n{traceback.format_exc()}")
        return False

async def test_async_operations():
    """ë¹„ë™ê¸° ì‘ì—… í…ŒìŠ¤íŠ¸"""
    print("\nâš¡ ë¹„ë™ê¸° ì‘ì—… í…ŒìŠ¤íŠ¸...")
    
    try:
        # Step 01 ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸
        from app.ai_pipeline.steps.step_01_human_parsing import HumanParsingStep
        
        step = HumanParsingStep(device='cpu')
        await step.initialize_async()
        
        print(f"  âœ… ë¹„ë™ê¸° ì´ˆê¸°í™” ì„±ê³µ")
        
        # ìƒíƒœ í™•ì¸
        status = step.get_status()
        print(f"  ğŸ“Š Step ìƒíƒœ: {status.get('initialized', False)}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_model_detection():
    """AI ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¤– AI ëª¨ë¸ íƒì§€ í…ŒìŠ¤íŠ¸...")
    
    try:
        from app.ai_pipeline.utils.auto_model_detector import detect_available_models
        
        # ëª¨ë¸ íƒì§€ ì‹¤í–‰
        models = detect_available_models()
        
        print(f"  ğŸ“Š íƒì§€ëœ ëª¨ë¸: {len(models)}ê°œ")
        
        # ì£¼ìš” ëª¨ë¸ë“¤ í™•ì¸
        key_models = ['human_parsing', 'pose_estimation', 'virtual_fitting']
        for model_type in key_models:
            found = any(model_type in str(model).lower() for model in models)
            status = "âœ…" if found else "âš ï¸"
            print(f"  {status} {model_type}: {'ë°œê²¬' if found else 'ë¯¸ë°œê²¬'}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
        return False

def test_memory_optimization():
    """ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸"""
    print("\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸...")
    
    try:
        import psutil
        import torch
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
        memory = psutil.virtual_memory()
        print(f"  ğŸ’¾ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory.total // (1024**3)}GB")
        print(f"  ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥: {memory.available // (1024**3)}GB")
        
        # PyTorch MPS
        if torch.backends.mps.is_available():
            print(f"  ğŸ MPS ë””ë°”ì´ìŠ¤: ì‚¬ìš© ê°€ëŠ¥")
            
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.mps.empty_cache()
            print(f"  ğŸ§¹ MPS ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        print(f"  âŒ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def generate_system_report(step_results, pipeline_ok, async_ok, models_ok, memory_ok):
    """ì‹œìŠ¤í…œ ë³´ê³ ì„œ ìƒì„±"""
    print("\n" + "="*60)
    print("ğŸ“‹ MyCloset AI ì‹œìŠ¤í…œ ìµœì¢… ë³´ê³ ì„œ")
    print("="*60)
    
    # ì „ì²´ ìƒíƒœ
    total_tests = 5
    passed_tests = sum([
        len([r for r in step_results.values() if r.get('instance', False)]) > 0,
        pipeline_ok,
        async_ok, 
        models_ok,
        memory_ok
    ])
    
    success_rate = (passed_tests / total_tests) * 100
    
    print(f"ğŸ¯ ì „ì²´ ì„±ê³µë¥ : {success_rate:.1f}% ({passed_tests}/{total_tests})")
    
    # ìƒì„¸ ê²°ê³¼
    print(f"\nğŸ“Š ìƒì„¸ ê²°ê³¼:")
    print(f"  ğŸ§ª Step í…ŒìŠ¤íŠ¸: {'âœ…' if any(r.get('instance') for r in step_results.values()) else 'âŒ'}")
    print(f"  ğŸš€ PipelineManager: {'âœ…' if pipeline_ok else 'âŒ'}")
    print(f"  âš¡ ë¹„ë™ê¸° ì‘ì—…: {'âœ…' if async_ok else 'âŒ'}")
    print(f"  ğŸ¤– ëª¨ë¸ íƒì§€: {'âœ…' if models_ok else 'âŒ'}")
    print(f"  ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™”: {'âœ…' if memory_ok else 'âŒ'}")
    
    # Stepë³„ ìƒì„¸
    print(f"\nğŸ¯ Stepë³„ ìƒíƒœ:")
    for step_name, result in step_results.items():
        if result.get('instance'):
            print(f"  âœ… {step_name}: ì •ìƒ ë™ì‘")
        else:
            print(f"  âŒ {step_name}: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    # ê¶Œì¥ì‚¬í•­
    print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    if success_rate >= 90:
        print("  ğŸ‰ ì‹œìŠ¤í…œì´ ì™„ë²½í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤!")
        print("  ğŸš€ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    elif success_rate >= 70:
        print("  âœ… ì‹œìŠ¤í…œì´ ëŒ€ë¶€ë¶„ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        print("  ğŸ”§ ì¼ë¶€ ê°œì„ ì‚¬í•­ì´ ìˆì§€ë§Œ ê¸°ë³¸ ê¸°ëŠ¥ì€ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        print("  âš ï¸ ì¶”ê°€ ë¬¸ì œ í•´ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("  ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì •ì„ ì¬ê²€í† í•´ì£¼ì„¸ìš”.")
    
    print("="*60)

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ¯ MyCloset AI ìµœì¢… ì‹œìŠ¤í…œ ê²€ì¦ ì‹œì‘")
    print("="*60)
    
    # í™˜ê²½ ì •ë³´
    print(f"ğŸ Python: {sys.version}")
    print(f"ğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print(f"ğŸ conda í™˜ê²½: {os.environ.get('CONDA_DEFAULT_ENV', 'None')}")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    step_results = test_all_steps()
    pipeline_ok = test_pipeline_manager()
    async_ok = await test_async_operations()
    models_ok = test_model_detection()
    memory_ok = test_memory_optimization()
    
    # ìµœì¢… ë³´ê³ ì„œ
    generate_system_report(step_results, pipeline_ok, async_ok, models_ok, memory_ok)

if __name__ == "__main__":
    asyncio.run(main())