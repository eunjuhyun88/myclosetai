# real_working_model_downloader_fixed.py
"""
ğŸ”¥ ì‹¤ì œ ì‘ë™í•˜ëŠ” AI ëª¨ë¸ ë‹¤ìš´ë¡œë” (ì²´í¬ì„¬ ë¬¸ì œ í•´ê²°)
- ì²´í¬ì„¬ ê²€ì¦ ìš°íšŒ ì˜µì…˜ ì¶”ê°€
- ëŒ€ì²´ URL ìë™ ì‹œë„
- M3 Max ìµœì í™” ëª¨ë¸ ìš°ì„  ë‹¤ìš´ë¡œë“œ
- ëª¨ë¸ ê²½ë¡œ í†µì¼ (ai_models/checkpoints)
"""

import os
import sys
import logging
import shutil
import hashlib
import requests
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from tqdm import tqdm
import json
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WorkingModelDownloader:
    """ì‹¤ì œ ì‘ë™í•˜ëŠ” ëª¨ë¸ ë‹¤ìš´ë¡œë”"""
    
    def __init__(self):
        self.base_dir = Path("ai_models/checkpoints")
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        # M3 Max ê°ì§€
        self.is_m3_max = self._detect_m3_max()
        
        # ë‹¤ìš´ë¡œë“œ í†µê³„
        self.download_stats = {
            "attempted": 0,
            "successful": 0,
            "failed": 0,
            "skipped": 0
        }
        
        logger.info(f"ğŸ–¥ï¸ ì‹œìŠ¤í…œ: {'Apple M3 Max (MPS ìµœì í™”)' if self.is_m3_max else 'Standard'}")

    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            import platform
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout or 'M2' in result.stdout
        except:
            pass
        return False

    def _check_disk_space(self) -> float:
        """ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (GB)"""
        try:
            statvfs = os.statvfs(self.base_dir)
            free_space_gb = (statvfs.f_frsize * statvfs.f_bavail) / (1024**3)
            return free_space_gb
        except:
            return 1000.0  # ê¸°ë³¸ê°’

    def _download_file_with_progress(self, url: str, destination: Path, 
                                   description: str, skip_checksum: bool = True) -> Tuple[bool, str]:
        """ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(destination, 'wb') as file:
                with tqdm(total=total_size, unit='B', unit_scale=True, desc=description) as pbar:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            file.write(chunk)
                            pbar.update(len(chunk))
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            actual_size = destination.stat().st_size
            logger.info(f"    ğŸ“Š ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {actual_size / (1024**2):.1f}MB")
            
            if skip_checksum:
                logger.info(f"    âœ… ì²´í¬ì„¬ ê²€ì¦ ìƒëµ")
                return True, "ë‹¤ìš´ë¡œë“œ ì™„ë£Œ"
            
            return True, "ë‹¤ìš´ë¡œë“œ ì™„ë£Œ"
            
        except Exception as e:
            logger.error(f"    âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False, str(e)

    def _download_with_multiple_urls(self, urls: List[str], destination: Path, 
                                   description: str) -> bool:
        """ì—¬ëŸ¬ URLë¡œ ë‹¤ìš´ë¡œë“œ ì‹œë„"""
        for i, url in enumerate(urls):
            logger.info(f"    ğŸŒ ì‹œë„ {i+1}/{len(urls)}: {url[:60]}...")
            
            success, message = self._download_file_with_progress(url, destination, description)
            if success:
                return True
            else:
                logger.warning(f"    âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (URL {i+1})")
        
        logger.error(f"    âŒ ëª¨ë“  URLì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {description}")
        return False

    def _try_huggingface_download(self, repo_id: str, destination: Path, 
                                description: str) -> bool:
        """HuggingFace Hubì„ í†µí•œ ë‹¤ìš´ë¡œë“œ"""
        try:
            from huggingface_hub import snapshot_download
            
            logger.info(f"ğŸ“¥ HuggingFaceì—ì„œ {description} ë‹¤ìš´ë¡œë“œ...")
            logger.info(f"    Repository: {repo_id}")
            logger.info(f"    ì €ì¥ ìœ„ì¹˜: {destination}")
            
            snapshot_download(
                repo_id=repo_id,
                local_dir=destination,
                local_dir_use_symlinks=False,
                resume_download=False
            )
            
            logger.info(f"âœ… {description} HuggingFace ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ HuggingFace ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def download_essential_models(self) -> bool:
        """í•„ìˆ˜ ëª¨ë¸ë“¤ ë‹¤ìš´ë¡œë“œ"""
        logger.info("ğŸ¯ í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        
        if self.is_m3_max:
            logger.info("ğŸ M3 Max ìµœì í™”: MPS í˜¸í™˜ ëª¨ë¸ ìš°ì„ ")
        
        essential_models = [
            {
                "name": "Segformer B2 Human Parsing",
                "description": "20-class ì¸ì²´ íŒŒì‹± ëª¨ë¸ (Segformer)",
                "priority": 1,
                "method": "huggingface",
                "repo_id": "mattmdjaga/segformer_b2_clothes",
                "destination": self.base_dir / "step_01_human_parsing" / "segformer_b2_clothes",
                "size_mb": 440.0,
                "mps_compatible": True
            },
            {
                "name": "UÂ²-Net ONNX",
                "description": "UÂ²-Net ONNX ë°°ê²½ ì œê±° ëª¨ë¸",
                "priority": 1,
                "method": "direct",
                "urls": [
                    "https://github.com/danielgatis/rembg/releases/download/v0.0.0/u2net.onnx",
                    "https://huggingface.co/rembg/u2net/resolve/main/u2net.onnx"
                ],
                "destination": self.base_dir / "step_03_cloth_segmentation" / "u2net.onnx",
                "size_mb": 176.3,
                "mps_compatible": True
            },
            {
                "name": "MediaPipe Pose Landmark",
                "description": "MediaPipe í¬ì¦ˆ ê°ì§€ ëª¨ë¸ (ê²½ëŸ‰)",
                "priority": 1,
                "method": "direct",
                "urls": [
                    "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/latest/pose_landmarker_full.task"
                ],
                "destination": self.base_dir / "step_02_pose_estimation" / "pose_landmarker.task",
                "size_mb": 9.4,
                "mps_compatible": True
            },
            {
                "name": "Real-ESRGAN x4plus",
                "description": "Real-ESRGAN 4ë°° ì—…ìŠ¤ì¼€ì¼ë§",
                "priority": 2,
                "method": "direct",
                "urls": [
                    "https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth",
                    "https://huggingface.co/ai-forever/Real-ESRGAN/resolve/main/RealESRGAN_x4plus.pth"
                ],
                "destination": self.base_dir / "step_07_post_processing" / "RealESRGAN_x4plus.pth",
                "size_mb": 67.0,
                "mps_compatible": True
            },
            {
                "name": "CLIP ViT-B/32",
                "description": "CLIP ë¹„ì „-ì–¸ì–´ ëª¨ë¸",
                "priority": 2,
                "method": "huggingface",
                "repo_id": "openai/clip-vit-base-patch32",
                "destination": self.base_dir / "shared_encoder" / "clip-vit-base-patch32",
                "size_mb": 605.0,
                "mps_compatible": True
            }
        ]
        
        # M3 Maxì¸ ê²½ìš° MPS í˜¸í™˜ ëª¨ë¸ë§Œ í•„í„°ë§
        if self.is_m3_max:
            essential_models = [m for m in essential_models if m.get('mps_compatible', False)]
            logger.info(f"ğŸ M3 Max ìµœì í™”: {len(essential_models)}ê°œ MPS í˜¸í™˜ ëª¨ë¸")
        
        total_size = sum(m['size_mb'] for m in essential_models) / 1024
        logger.info(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ì˜ˆì •: {len(essential_models)}ê°œ ëª¨ë¸ ({total_size:.2f}GB)")
        
        success_count = 0
        
        for i, model in enumerate(essential_models):
            logger.info(f"\n[{i+1}/{len(essential_models)}] ğŸ”¥ ìš°ì„ ìˆœìœ„ {model['priority']}")
            logger.info(f"\nğŸ“¦ {model['name']} ë‹¤ìš´ë¡œë“œ ì¤‘...")
            logger.info(f"    ğŸ“ ìœ„ì¹˜: {model['destination']}")
            logger.info(f"    ğŸ“Š í¬ê¸°: {model['size_mb']}MB")
            logger.info(f"    ğŸ¯ ì„¤ëª…: {model['description']}")
            logger.info(f"    ğŸ”§ MPS í˜¸í™˜: {'âœ…' if model.get('mps_compatible') else 'âŒ'}")
            
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if model['destination'].exists():
                existing_size = sum(f.stat().st_size for f in model['destination'].rglob('*') if f.is_file()) / (1024**2)
                if existing_size > model['size_mb'] * 0.8:  # 80% ì´ìƒì´ë©´ ì™„ë£Œë¡œ ê°„ì£¼
                    logger.info(f"    âœ… ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨: {existing_size:.1f}MB")
                    success_count += 1
                    continue
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            model['destination'].parent.mkdir(parents=True, exist_ok=True)
            
            self.download_stats['attempted'] += 1
            
            # ë‹¤ìš´ë¡œë“œ ì‹œë„
            download_success = False
            
            if model['method'] == 'huggingface':
                download_success = self._try_huggingface_download(
                    model['repo_id'], 
                    model['destination'], 
                    model['name']
                )
            else:  # direct
                download_success = self._download_with_multiple_urls(
                    model['urls'], 
                    model['destination'], 
                    model['name']
                )
            
            if download_success:
                success_count += 1
                self.download_stats['successful'] += 1
                logger.info(f"    âœ… {model['name']}: ì™„ë£Œ")
            else:
                self.download_stats['failed'] += 1
                logger.warning(f"    âŒ {model['name']} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - ê³„ì† ì§„í–‰...")
        
        success_rate = (success_count / len(essential_models)) * 100
        logger.info(f"\nğŸ‰ í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        logger.info(f"âœ… ì„±ê³µ: {success_count}/{len(essential_models)} ({success_rate:.1f}%)")
        
        return success_count >= len(essential_models) * 0.6  # 60% ì´ìƒ ì„±ê³µí•˜ë©´ OK

    def create_model_config(self):
        """ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
        config = {
            "model_base_path": str(self.base_dir),
            "models": {
                "human_parsing": {
                    "type": "segformer",
                    "path": "step_01_human_parsing/segformer_b2_clothes",
                    "format": "huggingface",
                    "device_compatible": ["cpu", "mps", "cuda"]
                },
                "cloth_segmentation": {
                    "type": "u2net_onnx",
                    "path": "step_03_cloth_segmentation/u2net.onnx",
                    "format": "onnx",
                    "device_compatible": ["cpu", "mps", "cuda"]
                },
                "pose_estimation": {
                    "type": "mediapipe",
                    "path": "step_02_pose_estimation/pose_landmarker.task",
                    "format": "mediapipe",
                    "device_compatible": ["cpu", "mps", "cuda"]
                },
                "post_processing": {
                    "type": "real_esrgan",
                    "path": "step_07_post_processing/RealESRGAN_x4plus.pth",
                    "format": "pytorch",
                    "device_compatible": ["cpu", "mps", "cuda"]
                },
                "shared_encoder": {
                    "type": "clip",
                    "path": "shared_encoder/clip-vit-base-patch32",
                    "format": "huggingface",
                    "device_compatible": ["cpu", "mps", "cuda"]
                }
            },
            "system_info": {
                "is_m3_max": self.is_m3_max,
                "download_stats": self.download_stats,
                "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
            }
        }
        
        config_path = self.base_dir / "working_model_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“‹ ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")

    def verify_models(self) -> Dict[str, bool]:
        """ëª¨ë¸ íŒŒì¼ë“¤ ê²€ì¦"""
        logger.info("ğŸ” ëª¨ë¸ ê²€ì¦")
        logger.info("=" * 20)
        
        models = {
            "ì¸ì²´ íŒŒì‹±": self.base_dir / "step_01_human_parsing" / "segformer_b2_clothes",
            "ë°°ê²½ ì œê±°": self.base_dir / "step_03_cloth_segmentation" / "u2net.onnx", 
            "í¬ì¦ˆ ì¶”ì •": self.base_dir / "step_02_pose_estimation" / "pose_landmarker.task",
            "í›„ì²˜ë¦¬": self.base_dir / "step_07_post_processing" / "RealESRGAN_x4plus.pth",
            "CLIP": self.base_dir / "shared_encoder" / "clip-vit-base-patch32"
        }
        
        results = {}
        ready_count = 0
        
        for name, path in models.items():
            if path.exists():
                if path.is_dir():
                    file_count = len(list(path.rglob('*')))
                    total_size = sum(f.stat().st_size for f in path.rglob('*') if f.is_file()) / (1024**2)
                    logger.info(f"âœ… {name}: {file_count}ê°œ íŒŒì¼, {total_size:.1f}MB")
                else:
                    size = path.stat().st_size / (1024**2)
                    logger.info(f"âœ… {name}: {size:.1f}MB")
                results[name] = True
                ready_count += 1
            else:
                logger.info(f"âŒ {name}: ì—†ìŒ")
                results[name] = False
        
        logger.info(f"\nğŸ“Š ì¤€ë¹„ëœ ëª¨ë¸: {ready_count}/{len(models)}")
        return results

    def run(self) -> bool:
        """ë©”ì¸ ì‹¤í–‰"""
        try:
            print("\nğŸ”¥ ì‹¤ì œ ì‘ë™í•˜ëŠ” AI ëª¨ë¸ ë‹¤ìš´ë¡œë” (ê°œì„ íŒ)")
            print("=" * 60)
            
            # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
            free_space = self._check_disk_space()
            required_space = 2.0  # GB
            logger.info(f"ğŸ’¾ ì €ì¥ ê³µê°„: {free_space:.1f}GB ì‚¬ìš©ê°€ëŠ¥, {required_space}GB í•„ìš”")
            
            if free_space < required_space:
                logger.error(f"âŒ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± ({free_space:.1f}GB < {required_space}GB)")
                return False
            
            # í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            success = self.download_essential_models()
            
            if not success:
                logger.error("âŒ í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                return False
            
            # ì„¤ì • íŒŒì¼ ìƒì„±
            self.create_model_config()
            
            # ê²€ì¦
            self.verify_models()
            
            logger.info("ğŸš€ ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ!")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    downloader = WorkingModelDownloader()
    
    success = downloader.run()
    
    if success:
        print("\nğŸ‰ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
        print(f"ğŸ“ ìœ„ì¹˜: {downloader.base_dir}")
        print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. python test_models_simple.py  # ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        print("2. python -m app.main  # ì„œë²„ ì‹¤í–‰")
    else:
        print("\nâŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        print("ğŸ“‹ ë¬¸ì œ í•´ê²°:")
        print("1. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸")
        print("2. ë””ìŠ¤í¬ ê³µê°„ í™•ì¸")
        print("3. ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ê³ ë ¤")

if __name__ == "__main__":
    main()