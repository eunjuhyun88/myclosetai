#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - ëˆ„ë½ëœ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸
================================================================

ëˆ„ë½ëœ ëª¨ë¸ë“¤:
- schp_atr ëª¨ë¸
- schp_lip ëª¨ë¸ 
- atr_model ëª¨ë¸
- lip_model ëª¨ë¸
- ì†ìƒëœ safetensors íŒŒì¼ë“¤

Author: MyCloset AI Team
Date: 2025-07-30
"""

import os
import sys
import requests
import logging
from pathlib import Path
from typing import Dict, List
import hashlib
import shutil

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent
AI_MODELS_ROOT = PROJECT_ROOT / "backend" / "ai_models"

# ëˆ„ë½ëœ ëª¨ë¸ URL ë§¤í•‘
MISSING_MODELS = {
    "schp_atr": {
        "url": "https://github.com/PeikeLi/Self-Correction-Human-Parsing/releases/download/v1.0/exp-schp-201908261155-atr.pth",
        "target_path": AI_MODELS_ROOT / "checkpoints" / "step_01_human_parsing" / "exp-schp-201908261155-atr.pth",
        "size_mb": 255.1
    },
    "schp_lip": {
        "url": "https://github.com/PeikeLi/Self-Correction-Human-Parsing/releases/download/v1.0/exp-schp-201908261155-lip.pth", 
        "target_path": AI_MODELS_ROOT / "checkpoints" / "step_01_human_parsing" / "exp-schp-201908261155-lip.pth",
        "size_mb": 255.1
    },
    "body_pose_model": {
        "url": "http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/body_25/pose_iter_584000.caffemodel",
        "target_path": AI_MODELS_ROOT / "checkpoints" / "step_02_pose_estimation" / "body_pose_model.pth",
        "size_mb": 200.0
    },
    "sam_vit_h": {
        "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth",
        "target_path": AI_MODELS_ROOT / "checkpoints" / "step_03_cloth_segmentation" / "sam_vit_h_4b8939.pth", 
        "size_mb": 2400.0
    }
}

def download_file(url: str, target_path: Path, expected_size_mb: float = None):
    """íŒŒì¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜"""
    try:
        # ë””ë ‰í† ë¦¬ ìƒì„±
        target_path.parent.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ”„ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {url}")
        logger.info(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {target_path}")
        
        # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ê³  í¬ê¸°ê°€ ë§ìœ¼ë©´ ìŠ¤í‚µ
        if target_path.exists():
            file_size_mb = target_path.stat().st_size / (1024 * 1024)
            if expected_size_mb and abs(file_size_mb - expected_size_mb) < 10:
                logger.info(f"âœ… íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•¨: {target_path} ({file_size_mb:.1f}MB)")
                return True
        
        # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(target_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    
                    # ì§„í–‰ë¥  í‘œì‹œ
                    if total_size > 0:
                        progress = (downloaded / total_size) * 100
                        print(f"\rì§„í–‰ë¥ : {progress:.1f}% ({downloaded/(1024*1024):.1f}MB/{total_size/(1024*1024):.1f}MB)", end='')
        
        print()  # ê°œí–‰
        
        # íŒŒì¼ í¬ê¸° ê²€ì¦
        final_size_mb = target_path.stat().st_size / (1024 * 1024)
        logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {target_path} ({final_size_mb:.1f}MB)")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url} - {e}")
        return False

def verify_and_fix_corrupted_files():
    """ì†ìƒëœ íŒŒì¼ íƒì§€ ë° ìˆ˜ì •"""
    corrupted_files = []
    
    # safetensors íŒŒì¼ë“¤ ê²€ì¦
    safetensors_files = list(AI_MODELS_ROOT.rglob("*.safetensors"))
    
    for file_path in safetensors_files:
        try:
            # íŒŒì¼ í¬ê¸° í™•ì¸ (ë„ˆë¬´ ì‘ìœ¼ë©´ ì†ìƒ ì˜ì‹¬)
            if file_path.stat().st_size < 1024:  # 1KB ë¯¸ë§Œ
                corrupted_files.append(file_path)
                continue
                
            # safetensors í—¤ë” ê²€ì¦
            with open(file_path, 'rb') as f:
                header_size_bytes = f.read(8)
                if len(header_size_bytes) < 8:
                    corrupted_files.append(file_path)
                    
        except Exception as e:
            logger.warning(f"âš ï¸ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {file_path} - {e}")
            corrupted_files.append(file_path)
    
    # ì†ìƒëœ íŒŒì¼ë“¤ ë°±ì—… í›„ ì‚­ì œ
    for corrupted_file in corrupted_files:
        backup_path = corrupted_file.with_suffix(corrupted_file.suffix + '.corrupted')
        try:
            shutil.move(str(corrupted_file), str(backup_path))
            logger.info(f"ğŸ—‘ï¸ ì†ìƒëœ íŒŒì¼ ë°±ì—…: {corrupted_file} â†’ {backup_path}")
        except Exception as e:
            logger.error(f"âŒ ì†ìƒëœ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {corrupted_file} - {e}")
    
    return corrupted_files

def create_model_symlinks():
    """ëª¨ë¸ íŒŒì¼ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±"""
    symlink_mappings = {
        # SCHP ëª¨ë¸ë“¤ì„ ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œë„ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ
        AI_MODELS_ROOT / "checkpoints" / "step_01_human_parsing" / "exp-schp-201908261155-atr.pth": [
            AI_MODELS_ROOT / "checkpoints" / "step_01_human_parsing" / "atr_model.pth",
            AI_MODELS_ROOT / "checkpoints" / "step_01_human_parsing" / "schp_atr.pth"
        ],
        AI_MODELS_ROOT / "checkpoints" / "step_01_human_parsing" / "exp-schp-201908261155-lip.pth": [
            AI_MODELS_ROOT / "checkpoints" / "step_01_human_parsing" / "lip_model.pth",
            AI_MODELS_ROOT / "checkpoints" / "step_01_human_parsing" / "schp_lip.pth"
        ]
    }
    
    for source_path, target_paths in symlink_mappings.items():
        if source_path.exists():
            for target_path in target_paths:
                try:
                    if not target_path.exists():
                        target_path.symlink_to(source_path)
                        logger.info(f"ğŸ”— ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±: {target_path} â†’ {source_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì‹¤íŒ¨: {target_path} - {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸ”¥ MyCloset AI - ëˆ„ë½ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    logger.info("=" * 60)
    
    # 1. ì†ìƒëœ íŒŒì¼ ì •ë¦¬
    logger.info("ğŸ§¹ ì†ìƒëœ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    corrupted_files = verify_and_fix_corrupted_files()
    logger.info(f"ğŸ§¹ ì†ìƒëœ íŒŒì¼ {len(corrupted_files)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
    
    # 2. ëˆ„ë½ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    logger.info("ğŸ“¥ ëˆ„ë½ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    success_count = 0
    
    for model_name, model_info in MISSING_MODELS.items():
        logger.info(f"\nğŸ¯ {model_name} ë‹¤ìš´ë¡œë“œ ì¤‘...")
        
        if download_file(
            model_info["url"], 
            model_info["target_path"], 
            model_info["size_mb"]
        ):
            success_count += 1
        
    # 3. ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
    logger.info("\nğŸ”— ëª¨ë¸ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì¤‘...")
    create_model_symlinks()
    
    # 4. ê²°ê³¼ ìš”ì•½
    logger.info("\n" + "=" * 60)
    logger.info(f"ğŸ‰ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {success_count}/{len(MISSING_MODELS)}ê°œ ì„±ê³µ")
    
    if success_count == len(MISSING_MODELS):
        logger.info("âœ… ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì„±ê³µ!")
        logger.info("ğŸš€ ì´ì œ python debug_model_loading.pyë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ë³´ì„¸ìš”.")
    else:
        logger.warning("âš ï¸ ì¼ë¶€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨. ìˆ˜ë™ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()