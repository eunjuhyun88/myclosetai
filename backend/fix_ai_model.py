#!/usr/bin/env python3
"""
ğŸ¤– MyCloset AI ì‹¤ì œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
âœ… ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ â†’ ì‹¤ì œ AI ëª¨ë¸ ëª¨ë“œ ì „í™˜
âœ… conda í™˜ê²½ ìµœì í™”
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš©
âœ… ë‹¨ê³„ë³„ ê²€ì¦ ë° í´ë°± ì§€ì›
"""

import os
import sys
import json
import time
import requests
import hashlib
import subprocess
from pathlib import Path
from urllib.parse import urlparse
from typing import Dict, List, Optional, Tuple

class ModelDownloader:
    """AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë§¤ë‹ˆì €"""
    
    def __init__(self, models_dir: Path):
        self.models_dir = models_dir
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        # ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ëª©ë¡ (ì‹¤ì œ ì‘ë™í•˜ëŠ” ëª¨ë¸ë“¤)
        self.models = {
            # Step 01: Human Parsing (ì¤‘ìš”ë„: ìµœê³ )
            "human_parsing": {
                "name": "ì¸ê°„ íŒŒì‹± (Human Parsing)",
                "priority": 1,
                "models": [
                    {
                        "name": "Graphonomy",
                        "url": "https://github.com/Gaoyiminggithub/Graphonomy.git",
                        "type": "git",
                        "checkpoint_url": "https://drive.google.com/uc?id=1k4dllHpu0bdx38J7H28rVVLpU-kOHmnH",
                        "checkpoint_name": "inference.pth",
                        "size_mb": 85
                    }
                ]
            },
            
            # Step 02: Pose Estimation (ì¤‘ìš”ë„: ë†’ìŒ)
            "pose_estimation": {
                "name": "í¬ì¦ˆ ì¶”ì • (Pose Estimation)", 
                "priority": 2,
                "models": [
                    {
                        "name": "OpenPose",
                        "url": "https://github.com/CMU-Perceptual-Computing-Lab/openpose.git",
                        "type": "git",
                        "checkpoint_url": "http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/body_25/pose_iter_584000.caffemodel",
                        "checkpoint_name": "pose_iter_584000.caffemodel",
                        "size_mb": 200
                    }
                ]
            },
            
            # Step 03: Cloth Segmentation (ì¤‘ìš”ë„: ë†’ìŒ)
            "cloth_segmentation": {
                "name": "ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Cloth Segmentation)",
                "priority": 3,
                "models": [
                    {
                        "name": "U2-Net",
                        "url": "https://github.com/xuebinqin/U-2-Net.git",
                        "type": "git", 
                        "checkpoint_url": "https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ",
                        "checkpoint_name": "u2net.pth",
                        "size_mb": 176
                    }
                ]
            },
            
            # Step 06: Virtual Fitting (ì¤‘ìš”ë„: ìµœê³ )
            "virtual_fitting": {
                "name": "ê°€ìƒ í”¼íŒ… (Virtual Fitting)",
                "priority": 1,
                "models": [
                    {
                        "name": "OOTDiffusion",
                        "url": "https://github.com/levihsu/OOTDiffusion.git", 
                        "type": "git",
                        "checkpoint_url": "https://huggingface.co/levihsu/OOTDiffusion",
                        "checkpoint_name": "ootd",
                        "size_mb": 2000,
                        "huggingface": True
                    }
                ]
            },
            
            # ê¸°ë³¸ ëª¨ë¸ë“¤ (ì‘ì€ í¬ê¸°)
            "basic_models": {
                "name": "ê¸°ë³¸ ëª¨ë¸ë“¤",
                "priority": 4,
                "models": [
                    {
                        "name": "CLIP",
                        "url": "openai/clip-vit-base-patch32",
                        "type": "huggingface",
                        "size_mb": 600
                    }
                ]
            }
        }
    
    def check_system_requirements(self) -> bool:
        """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
        print("ğŸ” ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì¤‘...")
        
        # Python í™˜ê²½ í™•ì¸
        python_version = sys.version_info
        if python_version < (3, 8):
            print(f"âŒ Python 3.8+ í•„ìš” (í˜„ì¬: {python_version.major}.{python_version.minor})")
            return False
        
        # conda í™˜ê²½ í™•ì¸
        conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'base')
        print(f"ğŸ Conda í™˜ê²½: {conda_env}")
        
        # Git í™•ì¸
        try:
            result = subprocess.run(['git', '--version'], capture_output=True, text=True)
            print(f"âœ… Git: {result.stdout.strip()}")
        except FileNotFoundError:
            print("âŒ Gitì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
            return False
        
        # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ìµœì†Œ 10GB)
        free_space = self.get_free_space_gb()
        if free_space < 10:
            print(f"âŒ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: {free_space:.1f}GB (ìµœì†Œ 10GB í•„ìš”)")
            return False
        
        print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ë””ìŠ¤í¬ ê³µê°„: {free_space:.1f}GB")
        return True
    
    def get_free_space_gb(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ìŠ¤í¬ ê³µê°„ (GB) ë°˜í™˜"""
        try:
            import shutil
            free_bytes = shutil.disk_usage(self.models_dir).free
            return free_bytes / (1024 ** 3)
        except:
            return 100.0  # ê¸°ë³¸ê°’
    
    def download_with_progress(self, url: str, file_path: Path, chunk_size: int = 8192) -> bool:
        """ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {url}")
            
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=chunk_size):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        
                        if total_size > 0:
                            percent = (downloaded / total_size) * 100
                            print(f"\rì§„í–‰ë¥ : {percent:.1f}% ({downloaded//1024//1024}MB/{total_size//1024//1024}MB)", end='')
            
            print(f"\nâœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_path}")
            return True
            
        except Exception as e:
            print(f"\nâŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            if file_path.exists():
                file_path.unlink()  # ì‹¤íŒ¨í•œ íŒŒì¼ ì‚­ì œ
            return False
    
    def clone_git_repo(self, url: str, target_dir: Path, depth: int = 1) -> bool:
        """Git ì €ì¥ì†Œ í´ë¡ """
        try:
            print(f"ğŸ“‚ Git í´ë¡  ì¤‘: {url}")
            
            cmd = ['git', 'clone']
            if depth > 0:
                cmd.extend(['--depth', str(depth)])
            cmd.extend([url, str(target_dir)])
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(f"âœ… Git í´ë¡  ì™„ë£Œ: {target_dir}")
                return True
            else:
                print(f"âŒ Git í´ë¡  ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"âŒ Git í´ë¡  ì˜¤ë¥˜: {e}")
            return False
    
    def download_from_huggingface(self, model_name: str, target_dir: Path) -> bool:
        """Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            print(f"ğŸ¤— Hugging Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {model_name}")
            
            # transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
            try:
                from transformers import AutoModel, AutoTokenizer
                
                # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
                model = AutoModel.from_pretrained(model_name)
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                
                # ë¡œì»¬ì— ì €ì¥
                target_dir.mkdir(parents=True, exist_ok=True)
                model.save_pretrained(target_dir)
                tokenizer.save_pretrained(target_dir)
                
                print(f"âœ… Hugging Face ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {target_dir}")
                return True
                
            except ImportError:
                print("âš ï¸ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. pip install transformers ì‹¤í–‰")
                return self.install_and_retry_hf(model_name, target_dir)
            
        except Exception as e:
            print(f"âŒ Hugging Face ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def install_and_retry_hf(self, model_name: str, target_dir: Path) -> bool:
        """transformers ì„¤ì¹˜ í›„ ì¬ì‹œë„"""
        try:
            print("ğŸ“¦ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì¤‘...")
            subprocess.run([sys.executable, '-m', 'pip', 'install', 'transformers'], check=True)
            
            # ì¬ì‹œë„
            return self.download_from_huggingface(model_name, target_dir)
            
        except Exception as e:
            print(f"âŒ transformers ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
            return False
    
    def download_model_category(self, category_name: str, category_info: Dict) -> bool:
        """ëª¨ë¸ ì¹´í…Œê³ ë¦¬ ë‹¤ìš´ë¡œë“œ"""
        print(f"\nğŸ¯ ì¹´í…Œê³ ë¦¬ ë‹¤ìš´ë¡œë“œ: {category_info['name']}")
        
        success_count = 0
        total_count = len(category_info['models'])
        
        for model_info in category_info['models']:
            model_name = model_info['name']
            model_type = model_info['type']
            model_url = model_info['url']
            
            # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ì„¤ì •
            target_dir = self.models_dir / category_name / model_name
            
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if target_dir.exists() and any(target_dir.iterdir()):
                print(f"âœ… {model_name} ì´ë¯¸ ì¡´ì¬í•¨: {target_dir}")
                success_count += 1
                continue
            
            print(f"\nğŸ“¦ {model_name} ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            
            # íƒ€ì…ë³„ ë‹¤ìš´ë¡œë“œ
            success = False
            if model_type == 'git':
                success = self.clone_git_repo(model_url, target_dir)
            elif model_type == 'huggingface':
                success = self.download_from_huggingface(model_url, target_dir)
            
            # ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ (ìˆëŠ” ê²½ìš°)
            if success and 'checkpoint_url' in model_info:
                checkpoint_url = model_info['checkpoint_url']
                checkpoint_name = model_info['checkpoint_name']
                checkpoint_path = target_dir / checkpoint_name
                
                if not checkpoint_path.exists():
                    if checkpoint_url.startswith('https://drive.google.com'):
                        print(f"âš ï¸ Google Drive ë§í¬ ê°ì§€: ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ í•„ìš”")
                        print(f"   URL: {checkpoint_url}")
                        print(f"   ì €ì¥ ìœ„ì¹˜: {checkpoint_path}")
                    else:
                        success = self.download_with_progress(checkpoint_url, checkpoint_path)
            
            if success:
                success_count += 1
                print(f"âœ… {model_name} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            else:
                print(f"âŒ {model_name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        
        print(f"\nğŸ“Š {category_info['name']} ê²°ê³¼: {success_count}/{total_count} ì„±ê³µ")
        return success_count > 0
    
    def create_model_configs(self):
        """ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
        print("\nğŸ“ ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")
        
        config_file = self.models_dir / "model_configs.json"
        
        configs = {}
        for category_name, category_info in self.models.items():
            for model_info in category_info['models']:
                model_name = model_info['name']
                target_dir = self.models_dir / category_name / model_name
                
                if target_dir.exists():
                    configs[model_name.lower()] = {
                        "name": model_name,
                        "category": category_name,
                        "path": str(target_dir),
                        "type": model_info['type'],
                        "size_mb": model_info.get('size_mb', 0),
                        "available": True
                    }
        
        with open(config_file, 'w') as f:
            json.dump(configs, f, indent=2)
        
        print(f"âœ… ì„¤ì • íŒŒì¼ ìƒì„±: {config_file}")
        print(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(configs)}ê°œ")
    
    def run_download(self, priority_only: bool = False) -> bool:
        """ì „ì²´ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰"""
        print("ğŸš€ MyCloset AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘!")
        print("=" * 50)
        
        # ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
        if not self.check_system_requirements():
            return False
        
        # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        sorted_categories = sorted(
            self.models.items(),
            key=lambda x: x[1]['priority']
        )
        
        if priority_only:
            # ìš°ì„ ìˆœìœ„ 1,2ë§Œ ë‹¤ìš´ë¡œë“œ
            sorted_categories = [(k, v) for k, v in sorted_categories if v['priority'] <= 2]
            print("ğŸ¯ ìš°ì„ ìˆœìœ„ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤")
        
        success_categories = 0
        total_categories = len(sorted_categories)
        
        for category_name, category_info in sorted_categories:
            if self.download_model_category(category_name, category_info):
                success_categories += 1
        
        # ì„¤ì • íŒŒì¼ ìƒì„±
        self.create_model_configs()
        
        print("\n" + "=" * 50)
        print(f"ğŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ! {success_categories}/{total_categories} ì¹´í…Œê³ ë¦¬ ì„±ê³µ")
        
        if success_categories > 0:
            print("\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
            print("1. python app/main.py  # ì„œë²„ ì¬ì‹œì‘")
            print("2. ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8000/docs í™•ì¸")
            print("3. AI ëª¨ë¸ì´ ì‹¤ì œë¡œ ë¡œë“œë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸")
            
            print("\nğŸ’¡ íŒ:")
            print("- Google Drive ë§í¬ëŠ” ìˆ˜ë™ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            print("- ëŒ€ìš©ëŸ‰ ëª¨ë¸ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            print("- ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”")
            
            return True
        else:
            print("\nâŒ ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¤– MyCloset AI ì‹¤ì œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë„êµ¬")
    print("=" * 50)
    
    # backend ë””ë ‰í† ë¦¬ í™•ì¸
    if Path.cwd().name != "backend":
        if Path("backend").exists():
            os.chdir("backend")
        else:
            print("âŒ backend ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            sys.exit(1)
    
    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ì„¤ì •
    models_dir = Path("ai_models")
    
    # ë‹¤ìš´ë¡œë” ìƒì„±
    downloader = ModelDownloader(models_dir)
    
    # ì‚¬ìš©ì ì„ íƒ
    print("\në‹¤ìš´ë¡œë“œ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ìš°ì„ ìˆœìœ„ ëª¨ë¸ë§Œ (ë¹ ë¥¸ ì‹œì‘) - ì•½ 2GB")
    print("2. ëª¨ë“  ëª¨ë¸ (ì „ì²´ ê¸°ëŠ¥) - ì•½ 10GB")
    print("3. ì„¤ì • íŒŒì¼ë§Œ ìƒì„± (ì´ë¯¸ ëª¨ë¸ ìˆìŒ)")
    
    choice = input("\nì„ íƒ (1-3): ").strip()
    
    if choice == "1":
        success = downloader.run_download(priority_only=True)
    elif choice == "2": 
        success = downloader.run_download(priority_only=False)
    elif choice == "3":
        downloader.create_model_configs()
        success = True
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤")
        sys.exit(1)
    
    if success:
        print("\nğŸ‰ ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ì´ì œ ì‹¤ì œ AI ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€")
    else:
        print("\nâŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        sys.exit(1)

if __name__ == "__main__":
    main()