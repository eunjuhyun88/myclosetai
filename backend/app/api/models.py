# app/api/models.py
"""
Î™®Îç∏ Í¥ÄÎ¶¨ API ÎùºÏö∞ÌÑ∞ - M3 Max ÏµúÏ†ÅÌôî (ÏµúÏ†Å ÏÉùÏÑ±Ïûê Ìå®ÌÑ¥ Ï†ÅÏö©)
Îã®ÏàúÌï® + Ìé∏ÏùòÏÑ± + ÌôïÏû•ÏÑ± + ÏùºÍ¥ÄÏÑ±
"""

import time
import logging
import asyncio
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path

from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from fastapi.responses import JSONResponse

# ÏµúÏ†Å ÏÉùÏÑ±Ïûê Ìå®ÌÑ¥ Í∏∞Î∞ò import
try:
    from ..ai_pipeline.utils import get_memory_manager, get_model_loader
    UTILS_AVAILABLE = True
except ImportError:
    UTILS_AVAILABLE = False

try:
    from ..services.model_manager import ModelManager, get_available_models
    MODEL_MANAGER_AVAILABLE = True
except ImportError:
    MODEL_MANAGER_AVAILABLE = False

class OptimalRouterConstructor:
    """ÏµúÏ†ÅÌôîÎêú ÎùºÏö∞ÌÑ∞ ÏÉùÏÑ±Ïûê Ìå®ÌÑ¥ Î≤†Ïù¥Ïä§ ÌÅ¥ÎûòÏä§"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        # 1. ÏßÄÎä•Ï†Å ÎîîÎ∞îÏù¥Ïä§ ÏûêÎèô Í∞êÏßÄ
        self.device = self._auto_detect_device(device)
        
        # 2. Í∏∞Î≥∏ ÏÑ§Ï†ï
        self.config = config or {}
        self.router_name = self.__class__.__name__
        self.logger = logging.getLogger(f"api.{self.router_name}")
        
        # 3. ÌëúÏ§Ä ÏãúÏä§ÌÖú ÌååÎùºÎØ∏ÌÑ∞ Ï∂îÏ∂ú
        self.device_type = kwargs.get('device_type', 'auto')
        self.memory_gb = kwargs.get('memory_gb', 16.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # 4. ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî
        self.is_initialized = False

    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ÏßÄÎä•Ï†Å ÎîîÎ∞îÏù¥Ïä§ ÏûêÎèô Í∞êÏßÄ"""
        if preferred_device:
            return preferred_device

        try:
            import torch
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'

    def _detect_m3_max(self) -> bool:
        """M3 Max Ïπ© ÏûêÎèô Í∞êÏßÄ"""
        try:
            import platform
            import subprocess

            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False

class ModelsRouter(OptimalRouterConstructor):
    """
    üçé M3 Max ÏµúÏ†ÅÌôî Î™®Îç∏ Í¥ÄÎ¶¨ ÎùºÏö∞ÌÑ∞
    ÏµúÏ†Å ÏÉùÏÑ±Ïûê Ìå®ÌÑ¥ Ï†ÅÏö©
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """
        ‚úÖ ÏµúÏ†Å ÏÉùÏÑ±Ïûê - Î™®Îç∏ Í¥ÄÎ¶¨ ÎùºÏö∞ÌÑ∞ ÌäπÌôî

        Args:
            device: ÏÇ¨Ïö©Ìï† ÎîîÎ∞îÏù¥Ïä§ (None=ÏûêÎèôÍ∞êÏßÄ, 'cpu', 'cuda', 'mps')
            config: Î™®Îç∏ Í¥ÄÎ¶¨ ÏÑ§Ï†ï ÎîïÏÖîÎÑàÎ¶¨
            **kwargs: ÌôïÏû• ÌååÎùºÎØ∏ÌÑ∞Îì§
                - enable_model_caching: bool = True  # Î™®Îç∏ Ï∫êÏã±
                - max_cached_models: int = 10  # ÏµúÎåÄ Ï∫êÏãú Î™®Îç∏ Ïàò
                - auto_cleanup: bool = True  # ÏûêÎèô Ï†ïÎ¶¨
                - enable_model_warmup: bool = True  # Î™®Îç∏ ÏõåÎ∞çÏóÖ
                - allow_model_switching: bool = True  # Î™®Îç∏ Ï†ÑÌôò ÌóàÏö©
                - enable_performance_monitoring: bool = True  # ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ
        """
        super().__init__(device=device, config=config, **kwargs)
        
        # Î™®Îç∏ Í¥ÄÎ¶¨ ÌäπÌôî ÏÑ§Ï†ï
        self.enable_model_caching = kwargs.get('enable_model_caching', True)
        self.max_cached_models = kwargs.get('max_cached_models', 10)
        self.auto_cleanup = kwargs.get('auto_cleanup', True)
        self.enable_model_warmup = kwargs.get('enable_model_warmup', True)
        self.allow_model_switching = kwargs.get('allow_model_switching', True)
        self.enable_performance_monitoring = kwargs.get('enable_performance_monitoring', True)
        
        # M3 Max ÌäπÌôî ÏÑ§Ï†ï
        if self.is_m3_max:
            self.max_cached_models = 20  # M3 MaxÎäî Îçî ÎßéÏùÄ Î™®Îç∏ Ï∫êÏãú Í∞ÄÎä•
            self.enable_model_warmup = True  # Ìï≠ÏÉÅ ÏõåÎ∞çÏóÖ ÌôúÏÑ±Ìôî
        
        # Î™®Îç∏ ÏÉÅÌÉú Ï∂îÏ†Å
        self._model_load_times: Dict[str, float] = {}
        self._model_performance: Dict[str, Dict[str, Any]] = {}
        self._loading_status: Dict[str, str] = {}
        
        # ÌÜµÍ≥Ñ
        self._stats = {
            "total_load_requests": 0,
            "successful_loads": 0,
            "failed_loads": 0,
            "total_unload_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "m3_max_optimized_loads": 0
        }
        
        # FastAPI ÎùºÏö∞ÌÑ∞ ÏÉùÏÑ±
        self.router = APIRouter()
        self._setup_routes()
        
        self.logger.info(f"üì¶ Î™®Îç∏ Í¥ÄÎ¶¨ ÎùºÏö∞ÌÑ∞ Ï¥àÍ∏∞Ìôî - {self.device} (M3 Max: {self.is_m3_max})")
        
        # Ï¥àÍ∏∞Ìôî ÏôÑÎ£å
        self.is_initialized = True

    def _setup_routes(self):
        """ÎùºÏö∞ÌÑ∞ ÏóîÎìúÌè¨Ïù∏Ìä∏ ÏÑ§Ï†ï"""
        
        @self.router.get("/")
        async def list_all_models():
            """Î™®Îì† Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå"""
            return await self.get_all_models()
        
        @self.router.get("/available")
        async def list_available_models():
            """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù"""
            return await self.get_available_models()
        
        @self.router.get("/loaded")
        async def list_loaded_models():
            """Î°úÎìúÎêú Î™®Îç∏ Î™©Î°ù"""
            return await self.get_loaded_models()
        
        @self.router.post("/load/{model_name}")
        async def load_model_endpoint(
            model_name: str,
            background_tasks: BackgroundTasks,
            force_reload: bool = False,
            enable_warmup: bool = None
        ):
            """Î™®Îç∏ Î°úÎìú"""
            return await self.load_model(
                model_name=model_name,
                background_tasks=background_tasks,
                force_reload=force_reload,
                enable_warmup=enable_warmup
            )
        
        @self.router.delete("/unload/{model_name}")
        async def unload_model_endpoint(model_name: str):
            """Î™®Îç∏ Ïñ∏Î°úÎìú"""
            return await self.unload_model(model_name)
        
        @self.router.get("/status/{model_name}")
        async def get_model_status_endpoint(model_name: str):
            """Î™®Îç∏ ÏÉÅÌÉú Ï°∞Ìöå"""
            return await self.get_model_status(model_name)
        
        @self.router.post("/reload/{model_name}")
        async def reload_model_endpoint(
            model_name: str,
            background_tasks: BackgroundTasks
        ):
            """Î™®Îç∏ Ïû¨Î°úÎìú"""
            return await self.reload_model(model_name, background_tasks)
        
        @self.router.post("/warmup/{model_name}")
        async def warmup_model_endpoint(model_name: str):
            """Î™®Îç∏ ÏõåÎ∞çÏóÖ"""
            return await self.warmup_model(model_name)
        
        @self.router.get("/performance")
        async def get_model_performance():
            """Î™®Îç∏ ÏÑ±Îä• ÌÜµÍ≥Ñ"""
            return await self.get_performance_stats()
        
        @self.router.post("/clear-cache")
        async def clear_model_cache():
            """Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨"""
            return await self.clear_cache()
        
        @self.router.post("/optimize-memory")
        async def optimize_memory():
            """Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî"""
            return await self.optimize_memory()
        
        @self.router.get("/system-info")
        async def get_model_system_info():
            """Î™®Îç∏ ÏãúÏä§ÌÖú Ï†ïÎ≥¥"""
            return await self.get_system_info()

    async def get_all_models(self) -> Dict[str, Any]:
        """Î™®Îì† Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå"""
        try:
            models_info = {
                "ai_pipeline_models": {},
                "service_models": {},
                "system_info": {
                    "device": self.device,
                    "m3_max_optimized": self.is_m3_max,
                    "utils_available": UTILS_AVAILABLE,
                    "model_manager_available": MODEL_MANAGER_AVAILABLE
                }
            }
            
            # AI ÌååÏù¥ÌîÑÎùºÏù∏ Î™®Îç∏Îì§
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    models_list = model_loader.list_models()
                    models_info["ai_pipeline_models"] = models_list
            
            # ÏÑúÎπÑÏä§ Î™®Îç∏Îì§
            if MODEL_MANAGER_AVAILABLE:
                try:
                    available_models = get_available_models()
                    models_info["service_models"] = available_models
                except Exception as e:
                    models_info["service_models"] = {"error": str(e)}
            
            # Í∏∞Î≥∏ Î™®Îç∏ Ï†ïÎ≥¥
            models_info["default_models"] = {
                "human_parsing": {
                    "name": "Graphonomy",
                    "type": "semantic_segmentation",
                    "categories": 20,
                    "input_size": [512, 512]
                },
                "pose_estimation": {
                    "name": "MediaPipe",
                    "type": "pose_detection",
                    "keypoints": 33,
                    "realtime_capable": True
                },
                "cloth_segmentation": {
                    "name": "U¬≤-Net",
                    "type": "image_segmentation",
                    "specialized": "clothing"
                },
                "virtual_fitting": {
                    "name": "HR-VITON",
                    "type": "generative_model",
                    "resolution": "1024x768"
                }
            }
            
            return {
                "success": True,
                "models": models_info,
                "total_categories": len(models_info),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def get_available_models(self) -> Dict[str, Any]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù"""
        try:
            available_models = {}
            
            # AI ÌååÏù¥ÌîÑÎùºÏù∏ Î™®Îç∏ Î°úÎçî
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    registered_models = getattr(model_loader, '_model_registry', {})
                    available_models["pipeline_models"] = {
                        name: {
                            "status": "registered",
                            "format": info.get("config", {}).get("format", "unknown"),
                            "device": info.get("config", {}).get("device", "unknown"),
                            "loaded": info.get("loaded", False)
                        }
                        for name, info in registered_models.items()
                    }
            
            # Í∏∞Î≥∏ ÏÇ¨Ïö© Í∞ÄÎä• Î™®Îç∏Îì§
            available_models["built_in_models"] = {
                "human_parsing_graphonomy": {
                    "status": "available",
                    "type": "human_parsing",
                    "framework": "pytorch",
                    "m3_max_optimized": self.is_m3_max
                },
                "pose_estimation_mediapipe": {
                    "status": "available", 
                    "type": "pose_estimation",
                    "framework": "mediapipe",
                    "realtime": True
                },
                "cloth_segmentation_u2net": {
                    "status": "available",
                    "type": "cloth_segmentation", 
                    "framework": "pytorch",
                    "specialized": True
                },
                "geometric_matching": {
                    "status": "available",
                    "type": "geometric_matching",
                    "framework": "opencv_pytorch"
                },
                "cloth_warping": {
                    "status": "available",
                    "type": "cloth_warping",
                    "framework": "pytorch",
                    "physics_enabled": True
                },
                "virtual_fitting_hrviton": {
                    "status": "available",
                    "type": "virtual_fitting",
                    "framework": "pytorch",
                    "resolution": "high"
                }
            }
            
            # M3 Max Ï†ÑÏö© Î™®Îç∏Îì§
            if self.is_m3_max:
                available_models["m3_max_exclusive"] = {
                    "neural_engine_accelerated": {
                        "status": "available",
                        "type": "all_models",
                        "acceleration": "neural_engine",
                        "performance_boost": "30-50%"
                    },
                    "coreml_optimized": {
                        "status": "available",
                        "type": "inference_optimized",
                        "framework": "coreml",
                        "ultra_quality_mode": True
                    }
                }
            
            return {
                "success": True,
                "available_models": available_models,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "total_available": sum(len(models) for models in available_models.values()),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def get_loaded_models(self) -> Dict[str, Any]:
        """Î°úÎìúÎêú Î™®Îç∏ Î™©Î°ù"""
        try:
            loaded_models = {}
            
            # AI ÌååÏù¥ÌîÑÎùºÏù∏ÏóêÏÑú Î°úÎìúÎêú Î™®Îç∏Îì§
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    loaded_model_dict = getattr(model_loader, '_loaded_models', {})
                    model_registry = getattr(model_loader, '_model_registry', {})
                    
                    for model_name in loaded_model_dict.keys():
                        registry_info = model_registry.get(model_name, {})
                        load_time = self._model_load_times.get(model_name, 0)
                        
                        loaded_models[model_name] = {
                            "status": "loaded",
                            "device": model_loader.device,
                            "load_time": load_time,
                            "load_count": registry_info.get("load_count", 0),
                            "last_loaded": registry_info.get("last_loaded"),
                            "format": registry_info.get("config", {}).get("format", "unknown"),
                            "performance": self._model_performance.get(model_name, {})
                        }
            
            # ÏÑúÎπÑÏä§ Îß§ÎãàÏ†ÄÏóêÏÑú Î°úÎìúÎêú Î™®Îç∏Îì§
            if MODEL_MANAGER_AVAILABLE:
                try:
                    # ÏÑúÎπÑÏä§ Î™®Îç∏ Îß§ÎãàÏ†Ä ÏÉÅÌÉú ÌôïÏù∏
                    # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî ÏÑúÎπÑÏä§ Îß§ÎãàÏ†ÄÏùò Î°úÎìúÎêú Î™®Îç∏ Î™©Î°ùÏùÑ Í∞ÄÏ†∏Ïò¥
                    pass
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è ÏÑúÎπÑÏä§ Î™®Îç∏ Ï°∞Ìöå Ïã§Ìå®: {e}")
            
            return {
                "success": True,
                "loaded_models": loaded_models,
                "total_loaded": len(loaded_models),
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "memory_usage": await self._get_memory_usage(),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Î°úÎìúÎêú Î™®Îç∏ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def load_model(
        self,
        model_name: str,
        background_tasks: BackgroundTasks,
        force_reload: bool = False,
        enable_warmup: Optional[bool] = None
    ) -> Dict[str, Any]:
        """Î™®Îç∏ Î°úÎìú"""
        try:
            start_time = time.time()
            self._stats["total_load_requests"] += 1
            
            # ÏõåÎ∞çÏóÖ ÏÑ§Ï†ï
            if enable_warmup is None:
                enable_warmup = self.enable_model_warmup
            
            # Î°úÎî© ÏÉÅÌÉú ÏÑ§Ï†ï
            self._loading_status[model_name] = "loading"
            
            # AI ÌååÏù¥ÌîÑÎùºÏù∏ Î™®Îç∏ Î°úÎçî ÏÇ¨Ïö©
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    # Ï∫êÏãú ÌôïÏù∏
                    if not force_reload and model_name in getattr(model_loader, 'loaded_models', {}):
                        self._stats["cache_hits"] += 1
                        self._loading_status[model_name] = "loaded"
                        
                        return {
                            "success": True,
                            "message": f"Î™®Îç∏ {model_name}Ïù¥ Ïù¥ÎØ∏ Î°úÎìúÎêòÏñ¥ ÏûàÏäµÎãàÎã§ (Ï∫êÏãú ÌûàÌä∏)",
                            "model_name": model_name,
                            "cache_hit": True,
                            "device": self.device,
                            "load_time": 0.0,
                            "timestamp": datetime.now().isoformat()
                        }
                    
                    # Ïã§Ï†ú Î™®Îç∏ Î°úÎìú
                    self._stats["cache_misses"] += 1
                    
                    # Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú Î°úÎìú (ÎπÑÎèôÍ∏∞)
                    if self.enable_model_caching:
                        background_tasks.add_task(
                            self._load_model_background,
                            model_loader, model_name, enable_warmup, start_time
                        )
                        
                        return {
                            "success": True,
                            "message": f"Î™®Îç∏ {model_name} Î°úÎî©Ïù¥ Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú ÏãúÏûëÎêòÏóàÏäµÎãàÎã§",
                            "model_name": model_name,
                            "background_loading": True,
                            "device": self.device,
                            "estimated_time": self._estimate_load_time(model_name),
                            "timestamp": datetime.now().isoformat()
                        }
                    else:
                        # ÎèôÍ∏∞ Î°úÎìú
                        loaded_model = await model_loader.load_model(model_name, force_reload=force_reload)
                        
                        if loaded_model:
                            load_time = time.time() - start_time
                            self._model_load_times[model_name] = load_time
                            self._loading_status[model_name] = "loaded"
                            self._stats["successful_loads"] += 1
                            
                            if self.is_m3_max:
                                self._stats["m3_max_optimized_loads"] += 1
                            
                            # ÏõåÎ∞çÏóÖ Ïã§Ìñâ
                            if enable_warmup:
                                background_tasks.add_task(self._warmup_model_background, model_name)
                            
                            return {
                                "success": True,
                                "message": f"Î™®Îç∏ {model_name} Î°úÎìú ÏôÑÎ£å",
                                "model_name": model_name,
                                "load_time": load_time,
                                "device": self.device,
                                "m3_max_optimized": self.is_m3_max,
                                "warmup_scheduled": enable_warmup,
                                "timestamp": datetime.now().isoformat()
                            }
                        else:
                            self._loading_status[model_name] = "failed"
                            self._stats["failed_loads"] += 1
                            
                            return {
                                "success": False,
                                "message": f"Î™®Îç∏ {model_name} Î°úÎìú Ïã§Ìå®",
                                "model_name": model_name,
                                "error": "Model loader returned None",
                                "timestamp": datetime.now().isoformat()
                            }
            
            # Ìè¥Î∞±: ÏãúÎÆ¨Î†àÏù¥ÏÖò Î°úÎìú
            return await self._simulate_model_load(model_name, start_time, enable_warmup)
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î°úÎìú Ïã§Ìå® {model_name}: {e}")
            self._loading_status[model_name] = "failed"
            self._stats["failed_loads"] += 1
            
            return {
                "success": False,
                "error": str(e),
                "model_name": model_name,
                "timestamp": datetime.now().isoformat()
            }

    async def unload_model(self, model_name: str) -> Dict[str, Any]:
        """Î™®Îç∏ Ïñ∏Î°úÎìú"""
        try:
            self._stats["total_unload_requests"] += 1
            
            # AI ÌååÏù¥ÌîÑÎùºÏù∏ Î™®Îç∏ Ïñ∏Î°úÎìú
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    success = model_loader.unload_model(model_name)
                    
                    if success:
                        # ÏÉÅÌÉú Ï†ïÎ¶¨
                        self._loading_status.pop(model_name, None)
                        self._model_load_times.pop(model_name, None)
                        self._model_performance.pop(model_name, None)
                        
                        return {
                            "success": True,
                            "message": f"Î™®Îç∏ {model_name} Ïñ∏Î°úÎìú ÏôÑÎ£å",
                            "model_name": model_name,
                            "device": self.device,
                            "timestamp": datetime.now().isoformat()
                        }
                    else:
                        return {
                            "success": False,
                            "message": f"Î™®Îç∏ {model_name}Ïù¥ Î°úÎìúÎêòÏñ¥ ÏûàÏßÄ ÏïäÍ±∞ÎÇò Ïñ∏Î°úÎìú Ïã§Ìå®",
                            "model_name": model_name,
                            "timestamp": datetime.now().isoformat()
                        }
            
            # Ìè¥Î∞±: ÏãúÎÆ¨Î†àÏù¥ÏÖò Ïñ∏Î°úÎìú
            return {
                "success": True,
                "message": f"Î™®Îç∏ {model_name} Ïñ∏Î°úÎìú ÏôÑÎ£å (ÏãúÎÆ¨Î†àÏù¥ÏÖò)",
                "model_name": model_name,
                "simulation_mode": True,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ïñ∏Î°úÎìú Ïã§Ìå® {model_name}: {e}")
            return {
                "success": False,
                "error": str(e),
                "model_name": model_name,
                "timestamp": datetime.now().isoformat()
            }

    async def get_model_status(self, model_name: str) -> Dict[str, Any]:
        """Î™®Îç∏ ÏÉÅÌÉú Ï°∞Ìöå"""
        try:
            status_info = {
                "model_name": model_name,
                "loading_status": self._loading_status.get(model_name, "not_loaded"),
                "device": self.device,
                "m3_max_optimized": self.is_m3_max
            }
            
            # AI ÌååÏù¥ÌîÑÎùºÏù∏ Î™®Îç∏ Ï†ïÎ≥¥
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    model_info = model_loader.get_model_info(model_name)
                    if model_info:
                        status_info.update({
                            "pipeline_info": model_info,
                            "load_time": self._model_load_times.get(model_name),
                            "performance": self._model_performance.get(model_name, {})
                        })
            
            # Î°úÎî© ÏÉÅÌÉúÎ≥Ñ Ï∂îÍ∞Ä Ï†ïÎ≥¥
            if status_info["loading_status"] == "loaded":
                status_info["ready_for_inference"] = True
                status_info["last_used"] = time.time()  # Ïã§Ï†úÎ°úÎäî ÎßàÏßÄÎßâ ÏÇ¨Ïö© ÏãúÍ∞Ñ Ï∂îÏ†Å
            
            elif status_info["loading_status"] == "loading":
                status_info["estimated_completion"] = time.time() + self._estimate_load_time(model_name)
            
            elif status_info["loading_status"] == "failed":
                status_info["error_info"] = "Î™®Îç∏ Î°úÎî© Ï§ë Ïò§Î•ò Î∞úÏÉù"
                status_info["retry_available"] = True
            
            return {
                "success": True,
                "status": status_info,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå® {model_name}: {e}")
            return {
                "success": False,
                "error": str(e),
                "model_name": model_name,
                "timestamp": datetime.now().isoformat()
            }

    async def reload_model(self, model_name: str, background_tasks: BackgroundTasks) -> Dict[str, Any]:
        """Î™®Îç∏ Ïû¨Î°úÎìú"""
        try:
            # Î®ºÏ†Ä Ïñ∏Î°úÎìú
            unload_result = await self.unload_model(model_name)
            
            if unload_result.get("success", False):
                # Ïû†Ïãú ÎåÄÍ∏∞ (Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ ÏãúÍ∞Ñ)
                await asyncio.sleep(1.0)
                
                # Îã§Ïãú Î°úÎìú
                load_result = await self.load_model(
                    model_name=model_name,
                    background_tasks=background_tasks,
                    force_reload=True,
                    enable_warmup=True
                )
                
                if load_result.get("success", False):
                    return {
                        "success": True,
                        "message": f"Î™®Îç∏ {model_name} Ïû¨Î°úÎìú ÏôÑÎ£å",
                        "model_name": model_name,
                        "reload_time": load_result.get("load_time", 0),
                        "device": self.device,
                        "timestamp": datetime.now().isoformat()
                    }
                else:
                    return {
                        "success": False,
                        "message": f"Î™®Îç∏ {model_name} Ïû¨Î°úÎìú Ïã§Ìå® - Î°úÎìú Îã®Í≥ÑÏóêÏÑú Ïã§Ìå®",
                        "model_name": model_name,
                        "load_error": load_result.get("error"),
                        "timestamp": datetime.now().isoformat()
                    }
            else:
                return {
                    "success": False,
                    "message": f"Î™®Îç∏ {model_name} Ïû¨Î°úÎìú Ïã§Ìå® - Ïñ∏Î°úÎìú Îã®Í≥ÑÏóêÏÑú Ïã§Ìå®",
                    "model_name": model_name,
                    "unload_error": unload_result.get("error"),
                    "timestamp": datetime.now().isoformat()
                }
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ïû¨Î°úÎìú Ïã§Ìå® {model_name}: {e}")
            return {
                "success": False,
                "error": str(e),
                "model_name": model_name,
                "timestamp": datetime.now().isoformat()
            }

    async def warmup_model(self, model_name: str) -> Dict[str, Any]:
        """Î™®Îç∏ ÏõåÎ∞çÏóÖ"""
        try:
            if not self.enable_model_warmup:
                return {
                    "success": False,
                    "message": "Î™®Îç∏ ÏõåÎ∞çÏóÖÏù¥ ÎπÑÌôúÏÑ±ÌôîÎêòÏñ¥ ÏûàÏäµÎãàÎã§",
                    "model_name": model_name,
                    "timestamp": datetime.now().isoformat()
                }
            
            start_time = time.time()
            
            # ÏõåÎ∞çÏóÖ Ïã§Ìñâ (ÏãúÎÆ¨Î†àÏù¥ÏÖò)
            await asyncio.sleep(0.5 if self.is_m3_max else 1.0)
            
            warmup_time = time.time() - start_time
            
            # ÏÑ±Îä• Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏
            if model_name not in self._model_performance:
                self._model_performance[model_name] = {}
            
            self._model_performance[model_name].update({
                "last_warmup": time.time(),
                "warmup_time": warmup_time,
                "warmed_up": True,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max
            })
            
            return {
                "success": True,
                "message": f"Î™®Îç∏ {model_name} ÏõåÎ∞çÏóÖ ÏôÑÎ£å",
                "model_name": model_name,
                "warmup_time": warmup_time,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏõåÎ∞çÏóÖ Ïã§Ìå® {model_name}: {e}")
            return {
                "success": False,
                "error": str(e),
                "model_name": model_name,
                "timestamp": datetime.now().isoformat()
            }

    async def get_performance_stats(self) -> Dict[str, Any]:
        """Î™®Îç∏ ÏÑ±Îä• ÌÜµÍ≥Ñ"""
        try:
            stats = self._stats.copy()
            
            # Ï∂îÍ∞Ä Í≥ÑÏÇ∞
            if stats["total_load_requests"] > 0:
                stats["load_success_rate"] = stats["successful_loads"] / stats["total_load_requests"]
                stats["cache_hit_rate"] = stats["cache_hits"] / stats["total_load_requests"]
            else:
                stats["load_success_rate"] = 0.0
                stats["cache_hit_rate"] = 0.0
            
            # ÌèâÍ∑† Î°úÎìú ÏãúÍ∞Ñ
            if self._model_load_times:
                stats["average_load_time"] = sum(self._model_load_times.values()) / len(self._model_load_times)
                stats["fastest_load_time"] = min(self._model_load_times.values())
                stats["slowest_load_time"] = max(self._model_load_times.values())
            else:
                stats["average_load_time"] = 0.0
                stats["fastest_load_time"] = 0.0
                stats["slowest_load_time"] = 0.0
            
            # M3 Max ÏµúÏ†ÅÌôîÏú®
            if self.is_m3_max and stats["total_load_requests"] > 0:
                stats["m3_max_optimization_rate"] = stats["m3_max_optimized_loads"] / stats["total_load_requests"]
            
            return {
                "success": True,
                "performance_stats": stats,
                "model_performance": self._model_performance,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå ÏÑ±Îä• ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def clear_cache(self) -> Dict[str, Any]:
        """Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨"""
        try:
            cleared_models = []
            
            # AI ÌååÏù¥ÌîÑÎùºÏù∏ Ï∫êÏãú Ï†ïÎ¶¨
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    loaded_models = list(getattr(model_loader, '_loaded_models', {}).keys())
                    
                    for model_name in loaded_models:
                        success = model_loader.unload_model(model_name)
                        if success:
                            cleared_models.append(model_name)
            
            # ÎÇ¥Î∂Ä ÏÉÅÌÉú Ï†ïÎ¶¨
            self._model_load_times.clear()
            self._model_performance.clear()
            self._loading_status.clear()
            
            # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            memory_result = {"success": False, "error": "Memory manager not available"}
            if UTILS_AVAILABLE:
                memory_manager = get_memory_manager()
                if memory_manager:
                    memory_result = memory_manager.clear_cache(aggressive=True)
            
            return {
                "success": True,
                "message": "Î™®Îç∏ Ï∫êÏãú Ï†ïÎ¶¨ ÏôÑÎ£å",
                "cleared_models": cleared_models,
                "cleared_count": len(cleared_models),
                "memory_cleanup": memory_result,
                "device": self.device,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Ï∫êÏãú Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def optimize_memory(self) -> Dict[str, Any]:
        """Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî"""
        try:
            optimization_results = []
            
            # Î©îÎ™®Î¶¨ Îß§ÎãàÏ†ÄÎ•º ÌÜµÌïú ÏµúÏ†ÅÌôî
            if UTILS_AVAILABLE:
                memory_manager = get_memory_manager()
                if memory_manager:
                    # Î©îÎ™®Î¶¨ ÏÉÅÌÉú ÌôïÏù∏
                    memory_stats = memory_manager.get_memory_stats()
                    optimization_results.append(f"Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ: {memory_stats}")
                    
                    # Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî Ïã§Ìñâ
                    optimization_result = memory_manager.optimize_for_task("model_loading")
                    optimization_results.append(f"ÏµúÏ†ÅÌôî Í≤∞Í≥º: {optimization_result}")
                    
                    # Ï∫êÏãú Ï†ïÎ¶¨
                    cache_result = memory_manager.clear_cache(aggressive=self.is_m3_max)
                    optimization_results.append(f"Ï∫êÏãú Ï†ïÎ¶¨: {cache_result}")
            
            # Î™®Îç∏Î≥Ñ ÏµúÏ†ÅÌôî
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader and hasattr(model_loader, '_loaded_models'):
                    loaded_count = len(model_loader._loaded_models)
                    if loaded_count > self.max_cached_models:
                        # Ïò§ÎûòÎêú Î™®Îç∏Îì§ Ï†ïÎ¶¨
                        optimization_results.append(f"Ï∫êÏãúÎêú Î™®Îç∏ Ïàò ({loaded_count})Í∞Ä ÏµúÎåÄÏπòÎ•º Ï¥àÍ≥ºÌïòÏó¨ Ï†ïÎ¶¨ ÌïÑÏöî")
            
            return {
                "success": True,
                "message": "Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÏôÑÎ£å",
                "optimization_results": optimization_results,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def get_system_info(self) -> Dict[str, Any]:
        """Î™®Îç∏ ÏãúÏä§ÌÖú Ï†ïÎ≥¥"""
        try:
            system_info = {
                "device": self.device,
                "device_type": self.device_type,
                "m3_max_optimized": self.is_m3_max,
                "router_config": {
                    "enable_model_caching": self.enable_model_caching,
                    "max_cached_models": self.max_cached_models,
                    "auto_cleanup": self.auto_cleanup,
                    "enable_model_warmup": self.enable_model_warmup,
                    "allow_model_switching": self.allow_model_switching,
                    "enable_performance_monitoring": self.enable_performance_monitoring
                },
                "component_status": {
                    "utils_available": UTILS_AVAILABLE,
                    "model_manager_available": MODEL_MANAGER_AVAILABLE
                }
            }
            
            # Î©îÎ™®Î¶¨ Ï†ïÎ≥¥
            if UTILS_AVAILABLE:
                memory_manager = get_memory_manager()
                if memory_manager:
                    memory_stats = memory_manager.get_memory_stats()
                    system_info["memory_info"] = memory_stats
            
            # Î™®Îç∏ Î°úÎçî Ï†ïÎ≥¥
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    system_info["model_loader_info"] = {
                        "device": model_loader.device,
                        "use_fp16": getattr(model_loader, 'use_fp16', False),
                        "coreml_optimization": getattr(model_loader, 'coreml_optimization', False),
                        "max_cached_models": getattr(model_loader, 'max_cached_models', 0),
                        "currently_loaded": len(getattr(model_loader, '_loaded_models', {}))
                    }
            
            # M3 Max ÌäπÌôî Ï†ïÎ≥¥
            if self.is_m3_max:
                system_info["m3_max_features"] = {
                    "neural_engine": True,
                    "unified_memory": True,
                    "mps_backend": self.device == "mps",
                    "coreml_support": True,
                    "increased_cache_capacity": True,
                    "optimized_loading": True
                }
            
            return {
                "success": True,
                "system_info": system_info,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    # Ïú†Ìã∏Î¶¨Ìã∞ Î©îÏÑúÎìúÎì§
    async def _load_model_background(
        self,
        model_loader,
        model_name: str,
        enable_warmup: bool,
        start_time: float
    ):
        """Î∞±Í∑∏ÎùºÏö¥Îìú Î™®Îç∏ Î°úÎî©"""
        try:
            loaded_model = await model_loader.load_model(model_name)
            
            if loaded_model:
                load_time = time.time() - start_time
                self._model_load_times[model_name] = load_time
                self._loading_status[model_name] = "loaded"
                self._stats["successful_loads"] += 1
                
                if self.is_m3_max:
                    self._stats["m3_max_optimized_loads"] += 1
                
                # ÏõåÎ∞çÏóÖ
                if enable_warmup:
                    await self._warmup_model_background(model_name)
                
                self.logger.info(f"‚úÖ Î∞±Í∑∏ÎùºÏö¥Îìú Î™®Îç∏ Î°úÎî© ÏôÑÎ£å: {model_name} ({load_time:.2f}s)")
            else:
                self._loading_status[model_name] = "failed"
                self._stats["failed_loads"] += 1
                self.logger.error(f"‚ùå Î∞±Í∑∏ÎùºÏö¥Îìú Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {model_name}")
                
        except Exception as e:
            self._loading_status[model_name] = "failed"
            self._stats["failed_loads"] += 1
            self.logger.error(f"‚ùå Î∞±Í∑∏ÎùºÏö¥Îìú Î™®Îç∏ Î°úÎî© ÏòàÏô∏: {model_name} - {e}")

    async def _warmup_model_background(self, model_name: str):
        """Î∞±Í∑∏ÎùºÏö¥Îìú Î™®Îç∏ ÏõåÎ∞çÏóÖ"""
        try:
            await asyncio.sleep(0.5)  # ÏõåÎ∞çÏóÖ ÏãúÎÆ¨Î†àÏù¥ÏÖò
            
            if model_name not in self._model_performance:
                self._model_performance[model_name] = {}
            
            self._model_performance[model_name].update({
                "warmed_up": True,
                "warmup_time": 0.5,
                "last_warmup": time.time()
            })
            
            self.logger.info(f"üî• Î™®Îç∏ ÏõåÎ∞çÏóÖ ÏôÑÎ£å: {model_name}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ ÏõåÎ∞çÏóÖ Ïã§Ìå®: {model_name} - {e}")

    async def _simulate_model_load(self, model_name: str, start_time: float, enable_warmup: bool) -> Dict[str, Any]:
        """Î™®Îç∏ Î°úÎî© ÏãúÎÆ¨Î†àÏù¥ÏÖò"""
        try:
            # Î°úÎî© ÏãúÍ∞Ñ ÏãúÎÆ¨Î†àÏù¥ÏÖò
            load_time = 2.0 if self.is_m3_max else 4.0
            await asyncio.sleep(load_time)
            
            actual_load_time = time.time() - start_time
            self._model_load_times[model_name] = actual_load_time
            self._loading_status[model_name] = "loaded"
            self._stats["successful_loads"] += 1
            
            if self.is_m3_max:
                self._stats["m3_max_optimized_loads"] += 1
            
            return {
                "success": True,
                "message": f"Î™®Îç∏ {model_name} Î°úÎìú ÏôÑÎ£å (ÏãúÎÆ¨Î†àÏù¥ÏÖò)",
                "model_name": model_name,
                "load_time": actual_load_time,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "simulation_mode": True,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self._loading_status[model_name] = "failed"
            self._stats["failed_loads"] += 1
            raise

    def _estimate_load_time(self, model_name: str) -> float:
        """Î™®Îç∏ Î°úÎî© ÏãúÍ∞Ñ Ï∂îÏ†ï"""
        base_times = {
            "human_parsing": 3.0,
            "pose_estimation": 1.5,
            "cloth_segmentation": 2.5,
            "geometric_matching": 2.0,
            "cloth_warping": 4.0,
            "virtual_fitting": 5.0
        }
        
        # Î™®Îç∏ Ïù¥Î¶ÑÏóêÏÑú ÌÉÄÏûÖ Ï∂îÏ†ï
        for model_type, base_time in base_times.items():
            if model_type in model_name.lower():
                # M3 MaxÎäî 30-50% Îπ†Î¶Ñ
                if self.is_m3_max:
                    return base_time * 0.6
                return base_time
        
        # Í∏∞Î≥∏Í∞í
        return 3.0 if self.is_m3_max else 5.0

    async def _get_memory_usage(self) -> Dict[str, Any]:
        """Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Ï°∞Ìöå"""
        try:
            if UTILS_AVAILABLE:
                memory_manager = get_memory_manager()
                if memory_manager:
                    memory_stats = memory_manager.get_memory_stats()
                    return {
                        "system_memory": memory_stats.get("system_memory", {}),
                        "gpu_memory": memory_stats.get("gpu_memory", {}),
                        "available": True
                    }
            
            return {"available": False, "reason": "Memory manager not available"}
            
        except Exception as e:
            return {"available": False, "error": str(e)}

# Î™®Îç∏ Í¥ÄÎ¶¨ ÎùºÏö∞ÌÑ∞ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± (ÏµúÏ†Å ÏÉùÏÑ±Ïûê Ìå®ÌÑ¥)
models_router = ModelsRouter()
router = models_router.router

# Ìé∏Ïùò Ìï®ÏàòÎì§ (ÌïòÏúÑ Ìò∏ÌôòÏÑ±)
def create_models_router(
    device: Optional[str] = None,
    enable_model_caching: bool = True,
    **kwargs
) -> ModelsRouter:
    """Î™®Îç∏ Í¥ÄÎ¶¨ ÎùºÏö∞ÌÑ∞ ÏÉùÏÑ± (ÌïòÏúÑ Ìò∏Ìôò)"""
    return ModelsRouter(
        device=device,
        enable_model_caching=enable_model_caching,
        **kwargs
    )

# Î™®Îìà ÏùµÏä§Ìè¨Ìä∏
__all__ = [
    'router',
    'ModelsRouter',
    'OptimalRouterConstructor',
    'create_models_router'
]