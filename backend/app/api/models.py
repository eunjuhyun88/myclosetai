# app/api/models.py
"""
ëª¨ë¸ ê´€ë¦¬ API ë¼ìš°í„° - M3 Max ìµœì í™” (ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš©)
ë‹¨ìˆœí•¨ + í¸ì˜ì„± + í™•ì¥ì„± + ì¼ê´€ì„±
"""

import time
import logging
import asyncio
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path

from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from fastapi.responses import JSONResponse

# ìµœì  ìƒì„±ì íŒ¨í„´ ê¸°ë°˜ import
try:
    from ..ai_pipeline.utils import get_memory_manager, get_model_loader
    UTILS_AVAILABLE = True
except ImportError:
    UTILS_AVAILABLE = False

try:
    from ..services.model_manager import ModelManager, get_available_models
    MODEL_MANAGER_AVAILABLE = True
except ImportError:
    MODEL_MANAGER_AVAILABLE = False

class OptimalRouterConstructor:
    """ìµœì í™”ëœ ë¼ìš°í„° ìƒì„±ì íŒ¨í„´ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        # 1. ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._auto_detect_device(device)
        
        # 2. ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.router_name = self.__class__.__name__
        self.logger = logging.getLogger(f"api.{self.router_name}")
        
        # 3. í‘œì¤€ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        self.device_type = kwargs.get('device_type', 'auto')
        self.memory_gb = kwargs.get('memory_gb', 16.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # 4. ìƒíƒœ ì´ˆê¸°í™”
        self.is_initialized = False

    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device:
            return preferred_device

        try:
            import torch
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'

    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ìë™ ê°ì§€"""
        try:
            import platform
            import subprocess

            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False

class ModelsRouter(OptimalRouterConstructor):
    """
    ğŸ M3 Max ìµœì í™” ëª¨ë¸ ê´€ë¦¬ ë¼ìš°í„°
    ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš©
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """
        âœ… ìµœì  ìƒì„±ì - ëª¨ë¸ ê´€ë¦¬ ë¼ìš°í„° íŠ¹í™”

        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (None=ìë™ê°ì§€, 'cpu', 'cuda', 'mps')
            config: ëª¨ë¸ ê´€ë¦¬ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            **kwargs: í™•ì¥ íŒŒë¼ë¯¸í„°ë“¤
                - enable_model_caching: bool = True  # ëª¨ë¸ ìºì‹±
                - max_cached_models: int = 10  # ìµœëŒ€ ìºì‹œ ëª¨ë¸ ìˆ˜
                - auto_cleanup: bool = True  # ìë™ ì •ë¦¬
                - enable_model_warmup: bool = True  # ëª¨ë¸ ì›Œë°ì—…
                - allow_model_switching: bool = True  # ëª¨ë¸ ì „í™˜ í—ˆìš©
                - enable_performance_monitoring: bool = True  # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        """
        super().__init__(device=device, config=config, **kwargs)
        
        # ëª¨ë¸ ê´€ë¦¬ íŠ¹í™” ì„¤ì •
        self.enable_model_caching = kwargs.get('enable_model_caching', True)
        self.max_cached_models = kwargs.get('max_cached_models', 10)
        self.auto_cleanup = kwargs.get('auto_cleanup', True)
        self.enable_model_warmup = kwargs.get('enable_model_warmup', True)
        self.allow_model_switching = kwargs.get('allow_model_switching', True)
        self.enable_performance_monitoring = kwargs.get('enable_performance_monitoring', True)
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if self.is_m3_max:
            self.max_cached_models = 20  # M3 MaxëŠ” ë” ë§ì€ ëª¨ë¸ ìºì‹œ ê°€ëŠ¥
            self.enable_model_warmup = True  # í•­ìƒ ì›Œë°ì—… í™œì„±í™”
        
        # ëª¨ë¸ ìƒíƒœ ì¶”ì 
        self._model_load_times: Dict[str, float] = {}
        self._model_performance: Dict[str, Dict[str, Any]] = {}
        self._loading_status: Dict[str, str] = {}
        
        # í†µê³„
        self._stats = {
            "total_load_requests": 0,
            "successful_loads": 0,
            "failed_loads": 0,
            "total_unload_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "m3_max_optimized_loads": 0
        }
        
        # FastAPI ë¼ìš°í„° ìƒì„±
        self.router = APIRouter()
        self._setup_routes()
        
        self.logger.info(f"ğŸ“¦ ëª¨ë¸ ê´€ë¦¬ ë¼ìš°í„° ì´ˆê¸°í™” - {self.device} (M3 Max: {self.is_m3_max})")
        
        # ì´ˆê¸°í™” ì™„ë£Œ
        self.is_initialized = True

    def _setup_routes(self):
        """ë¼ìš°í„° ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •"""
        
        @self.router.get("/")
        async def list_all_models():
            """ëª¨ë“  ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
            return await self.get_all_models()
        
        @self.router.get("/available")
        async def list_available_models():
            """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
            return await self.get_available_models()
        
        @self.router.get("/loaded")
        async def list_loaded_models():
            """ë¡œë“œëœ ëª¨ë¸ ëª©ë¡"""
            return await self.get_loaded_models()
        
        @self.router.post("/load/{model_name}")
        async def load_model_endpoint(
            model_name: str,
            background_tasks: BackgroundTasks,
            force_reload: bool = False,
            enable_warmup: bool = None
        ):
            """ëª¨ë¸ ë¡œë“œ"""
            return await self.load_model(
                model_name=model_name,
                background_tasks=background_tasks,
                force_reload=force_reload,
                enable_warmup=enable_warmup
            )
        
        @self.router.delete("/unload/{model_name}")
        async def unload_model_endpoint(model_name: str):
            """ëª¨ë¸ ì–¸ë¡œë“œ"""
            return await self.unload_model(model_name)
        
        @self.router.get("/status/{model_name}")
        async def get_model_status_endpoint(model_name: str):
            """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
            return await self.get_model_status(model_name)
        
        @self.router.post("/reload/{model_name}")
        async def reload_model_endpoint(
            model_name: str,
            background_tasks: BackgroundTasks
        ):
            """ëª¨ë¸ ì¬ë¡œë“œ"""
            return await self.reload_model(model_name, background_tasks)
        
        @self.router.post("/warmup/{model_name}")
        async def warmup_model_endpoint(model_name: str):
            """ëª¨ë¸ ì›Œë°ì—…"""
            return await self.warmup_model(model_name)
        
        @self.router.get("/performance")
        async def get_model_performance():
            """ëª¨ë¸ ì„±ëŠ¥ í†µê³„"""
            return await self.get_performance_stats()
        
        @self.router.post("/clear-cache")
        async def clear_model_cache():
            """ëª¨ë¸ ìºì‹œ ì •ë¦¬"""
            return await self.clear_cache()
        
        @self.router.post("/optimize-memory")
        async def optimize_memory():
            """ë©”ëª¨ë¦¬ ìµœì í™”"""
            return await self.optimize_memory()
        
        @self.router.get("/system-info")
        async def get_model_system_info():
            """ëª¨ë¸ ì‹œìŠ¤í…œ ì •ë³´"""
            return await self.get_system_info()

    async def get_all_models(self) -> Dict[str, Any]:
        """ëª¨ë“  ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            models_info = {
                "ai_pipeline_models": {},
                "service_models": {},
                "system_info": {
                    "device": self.device,
                    "m3_max_optimized": self.is_m3_max,
                    "utils_available": UTILS_AVAILABLE,
                    "model_manager_available": MODEL_MANAGER_AVAILABLE
                }
            }
            
            # AI íŒŒì´í”„ë¼ì¸ ëª¨ë¸ë“¤
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    models_list = model_loader.list_models()
                    models_info["ai_pipeline_models"] = models_list
            
            # ì„œë¹„ìŠ¤ ëª¨ë¸ë“¤
            if MODEL_MANAGER_AVAILABLE:
                try:
                    available_models = get_available_models()
                    models_info["service_models"] = available_models
                except Exception as e:
                    models_info["service_models"] = {"error": str(e)}
            
            # ê¸°ë³¸ ëª¨ë¸ ì •ë³´
            models_info["default_models"] = {
                "human_parsing": {
                    "name": "Graphonomy",
                    "type": "semantic_segmentation",
                    "categories": 20,
                    "input_size": [512, 512]
                },
                "pose_estimation": {
                    "name": "MediaPipe",
                    "type": "pose_detection",
                    "keypoints": 33,
                    "realtime_capable": True
                },
                "cloth_segmentation": {
                    "name": "UÂ²-Net",
                    "type": "image_segmentation",
                    "specialized": "clothing"
                },
                "virtual_fitting": {
                    "name": "HR-VITON",
                    "type": "generative_model",
                    "resolution": "1024x768"
                }
            }
            
            return {
                "success": True,
                "models": models_info,
                "total_categories": len(models_info),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def get_available_models(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        try:
            available_models = {}
            
            # AI íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë¡œë”
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    registered_models = getattr(model_loader, '_model_registry', {})
                    available_models["pipeline_models"] = {
                        name: {
                            "status": "registered",
                            "format": info.get("config", {}).get("format", "unknown"),
                            "device": info.get("config", {}).get("device", "unknown"),
                            "loaded": info.get("loaded", False)
                        }
                        for name, info in registered_models.items()
                    }
            
            # ê¸°ë³¸ ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸ë“¤
            available_models["built_in_models"] = {
                "human_parsing_graphonomy": {
                    "status": "available",
                    "type": "human_parsing",
                    "framework": "pytorch",
                    "m3_max_optimized": self.is_m3_max
                },
                "pose_estimation_mediapipe": {
                    "status": "available", 
                    "type": "pose_estimation",
                    "framework": "mediapipe",
                    "realtime": True
                },
                "cloth_segmentation_u2net": {
                    "status": "available",
                    "type": "cloth_segmentation", 
                    "framework": "pytorch",
                    "specialized": True
                },
                "geometric_matching": {
                    "status": "available",
                    "type": "geometric_matching",
                    "framework": "opencv_pytorch"
                },
                "cloth_warping": {
                    "status": "available",
                    "type": "cloth_warping",
                    "framework": "pytorch",
                    "physics_enabled": True
                },
                "virtual_fitting_hrviton": {
                    "status": "available",
                    "type": "virtual_fitting",
                    "framework": "pytorch",
                    "resolution": "high"
                }
            }
            
            # M3 Max ì „ìš© ëª¨ë¸ë“¤
            if self.is_m3_max:
                available_models["m3_max_exclusive"] = {
                    "neural_engine_accelerated": {
                        "status": "available",
                        "type": "all_models",
                        "acceleration": "neural_engine",
                        "performance_boost": "30-50%"
                    },
                    "coreml_optimized": {
                        "status": "available",
                        "type": "inference_optimized",
                        "framework": "coreml",
                        "ultra_quality_mode": True
                    }
                }
            
            return {
                "success": True,
                "available_models": available_models,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "total_available": sum(len(models) for models in available_models.values()),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def get_loaded_models(self) -> Dict[str, Any]:
        """ë¡œë“œëœ ëª¨ë¸ ëª©ë¡"""
        try:
            loaded_models = {}
            
            # AI íŒŒì´í”„ë¼ì¸ì—ì„œ ë¡œë“œëœ ëª¨ë¸ë“¤
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    loaded_model_dict = getattr(model_loader, '_loaded_models', {})
                    model_registry = getattr(model_loader, '_model_registry', {})
                    
                    for model_name in loaded_model_dict.keys():
                        registry_info = model_registry.get(model_name, {})
                        load_time = self._model_load_times.get(model_name, 0)
                        
                        loaded_models[model_name] = {
                            "status": "loaded",
                            "device": model_loader.device,
                            "load_time": load_time,
                            "load_count": registry_info.get("load_count", 0),
                            "last_loaded": registry_info.get("last_loaded"),
                            "format": registry_info.get("config", {}).get("format", "unknown"),
                            "performance": self._model_performance.get(model_name, {})
                        }
            
            # ì„œë¹„ìŠ¤ ë§¤ë‹ˆì €ì—ì„œ ë¡œë“œëœ ëª¨ë¸ë“¤
            if MODEL_MANAGER_AVAILABLE:
                try:
                    # ì„œë¹„ìŠ¤ ëª¨ë¸ ë§¤ë‹ˆì € ìƒíƒœ í™•ì¸
                    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì„œë¹„ìŠ¤ ë§¤ë‹ˆì €ì˜ ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜´
                    pass
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì„œë¹„ìŠ¤ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            return {
                "success": True,
                "loaded_models": loaded_models,
                "total_loaded": len(loaded_models),
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "memory_usage": await self._get_memory_usage(),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë¡œë“œëœ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def load_model(
        self,
        model_name: str,
        background_tasks: BackgroundTasks,
        force_reload: bool = False,
        enable_warmup: Optional[bool] = None
    ) -> Dict[str, Any]:
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            start_time = time.time()
            self._stats["total_load_requests"] += 1
            
            # ì›Œë°ì—… ì„¤ì •
            if enable_warmup is None:
                enable_warmup = self.enable_model_warmup
            
            # ë¡œë”© ìƒíƒœ ì„¤ì •
            self._loading_status[model_name] = "loading"
            
            # AI íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë¡œë” ì‚¬ìš©
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    # ìºì‹œ í™•ì¸
                    if not force_reload and model_name in getattr(model_loader, 'loaded_models', {}):
                        self._stats["cache_hits"] += 1
                        self._loading_status[model_name] = "loaded"
                        
                        return {
                            "success": True,
                            "message": f"ëª¨ë¸ {model_name}ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤ (ìºì‹œ íˆíŠ¸)",
                            "model_name": model_name,
                            "cache_hit": True,
                            "device": self.device,
                            "load_time": 0.0,
                            "timestamp": datetime.now().isoformat()
                        }
                    
                    # ì‹¤ì œ ëª¨ë¸ ë¡œë“œ
                    self._stats["cache_misses"] += 1
                    
                    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¡œë“œ (ë¹„ë™ê¸°)
                    if self.enable_model_caching:
                        background_tasks.add_task(
                            self._load_model_background,
                            model_loader, model_name, enable_warmup, start_time
                        )
                        
                        return {
                            "success": True,
                            "message": f"ëª¨ë¸ {model_name} ë¡œë”©ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
                            "model_name": model_name,
                            "background_loading": True,
                            "device": self.device,
                            "estimated_time": self._estimate_load_time(model_name),
                            "timestamp": datetime.now().isoformat()
                        }
                    else:
                        # ë™ê¸° ë¡œë“œ
                        loaded_model = await model_loader.load_model(model_name, force_reload=force_reload)
                        
                        if loaded_model:
                            load_time = time.time() - start_time
                            self._model_load_times[model_name] = load_time
                            self._loading_status[model_name] = "loaded"
                            self._stats["successful_loads"] += 1
                            
                            if self.is_m3_max:
                                self._stats["m3_max_optimized_loads"] += 1
                            
                            # ì›Œë°ì—… ì‹¤í–‰
                            if enable_warmup:
                                background_tasks.add_task(self._warmup_model_background, model_name)
                            
                            return {
                                "success": True,
                                "message": f"ëª¨ë¸ {model_name} ë¡œë“œ ì™„ë£Œ",
                                "model_name": model_name,
                                "load_time": load_time,
                                "device": self.device,
                                "m3_max_optimized": self.is_m3_max,
                                "warmup_scheduled": enable_warmup,
                                "timestamp": datetime.now().isoformat()
                            }
                        else:
                            self._loading_status[model_name] = "failed"
                            self._stats["failed_loads"] += 1
                            
                            return {
                                "success": False,
                                "message": f"ëª¨ë¸ {model_name} ë¡œë“œ ì‹¤íŒ¨",
                                "model_name": model_name,
                                "error": "Model loader returned None",
                                "timestamp": datetime.now().isoformat()
                            }
            
            # í´ë°±: ì‹œë®¬ë ˆì´ì…˜ ë¡œë“œ
            return await self._simulate_model_load(model_name, start_time, enable_warmup)
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            self._loading_status[model_name] = "failed"
            self._stats["failed_loads"] += 1
            
            return {
                "success": False,
                "error": str(e),
                "model_name": model_name,
                "timestamp": datetime.now().isoformat()
            }

    async def unload_model(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            self._stats["total_unload_requests"] += 1
            
            # AI íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ì–¸ë¡œë“œ
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    success = model_loader.unload_model(model_name)
                    
                    if success:
                        # ìƒíƒœ ì •ë¦¬
                        self._loading_status.pop(model_name, None)
                        self._model_load_times.pop(model_name, None)
                        self._model_performance.pop(model_name, None)
                        
                        return {
                            "success": True,
                            "message": f"ëª¨ë¸ {model_name} ì–¸ë¡œë“œ ì™„ë£Œ",
                            "model_name": model_name,
                            "device": self.device,
                            "timestamp": datetime.now().isoformat()
                        }
                    else:
                        return {
                            "success": False,
                            "message": f"ëª¨ë¸ {model_name}ì´ ë¡œë“œë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜ ì–¸ë¡œë“œ ì‹¤íŒ¨",
                            "model_name": model_name,
                            "timestamp": datetime.now().isoformat()
                        }
            
            # í´ë°±: ì‹œë®¬ë ˆì´ì…˜ ì–¸ë¡œë“œ
            return {
                "success": True,
                "message": f"ëª¨ë¸ {model_name} ì–¸ë¡œë“œ ì™„ë£Œ (ì‹œë®¬ë ˆì´ì…˜)",
                "model_name": model_name,
                "simulation_mode": True,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return {
                "success": False,
                "error": str(e),
                "model_name": model_name,
                "timestamp": datetime.now().isoformat()
            }

    async def get_model_status(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            status_info = {
                "model_name": model_name,
                "loading_status": self._loading_status.get(model_name, "not_loaded"),
                "device": self.device,
                "m3_max_optimized": self.is_m3_max
            }
            
            # AI íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ì •ë³´
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    model_info = model_loader.get_model_info(model_name)
                    if model_info:
                        status_info.update({
                            "pipeline_info": model_info,
                            "load_time": self._model_load_times.get(model_name),
                            "performance": self._model_performance.get(model_name, {})
                        })
            
            # ë¡œë”© ìƒíƒœë³„ ì¶”ê°€ ì •ë³´
            if status_info["loading_status"] == "loaded":
                status_info["ready_for_inference"] = True
                status_info["last_used"] = time.time()  # ì‹¤ì œë¡œëŠ” ë§ˆì§€ë§‰ ì‚¬ìš© ì‹œê°„ ì¶”ì 
            
            elif status_info["loading_status"] == "loading":
                status_info["estimated_completion"] = time.time() + self._estimate_load_time(model_name)
            
            elif status_info["loading_status"] == "failed":
                status_info["error_info"] = "ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
                status_info["retry_available"] = True
            
            return {
                "success": True,
                "status": status_info,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ {model_name}: {e}")
            return {
                "success": False,
                "error": str(e),
                "model_name": model_name,
                "timestamp": datetime.now().isoformat()
            }

    async def reload_model(self, model_name: str, background_tasks: BackgroundTasks) -> Dict[str, Any]:
        """ëª¨ë¸ ì¬ë¡œë“œ"""
        try:
            # ë¨¼ì € ì–¸ë¡œë“œ
            unload_result = await self.unload_model(model_name)
            
            if unload_result.get("success", False):
                # ì ì‹œ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œê°„)
                await asyncio.sleep(1.0)
                
                # ë‹¤ì‹œ ë¡œë“œ
                load_result = await self.load_model(
                    model_name=model_name,
                    background_tasks=background_tasks,
                    force_reload=True,
                    enable_warmup=True
                )
                
                if load_result.get("success", False):
                    return {
                        "success": True,
                        "message": f"ëª¨ë¸ {model_name} ì¬ë¡œë“œ ì™„ë£Œ",
                        "model_name": model_name,
                        "reload_time": load_result.get("load_time", 0),
                        "device": self.device,
                        "timestamp": datetime.now().isoformat()
                    }
                else:
                    return {
                        "success": False,
                        "message": f"ëª¨ë¸ {model_name} ì¬ë¡œë“œ ì‹¤íŒ¨ - ë¡œë“œ ë‹¨ê³„ì—ì„œ ì‹¤íŒ¨",
                        "model_name": model_name,
                        "load_error": load_result.get("error"),
                        "timestamp": datetime.now().isoformat()
                    }
            else:
                return {
                    "success": False,
                    "message": f"ëª¨ë¸ {model_name} ì¬ë¡œë“œ ì‹¤íŒ¨ - ì–¸ë¡œë“œ ë‹¨ê³„ì—ì„œ ì‹¤íŒ¨",
                    "model_name": model_name,
                    "unload_error": unload_result.get("error"),
                    "timestamp": datetime.now().isoformat()
                }
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¬ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return {
                "success": False,
                "error": str(e),
                "model_name": model_name,
                "timestamp": datetime.now().isoformat()
            }

    async def warmup_model(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ ì›Œë°ì—…"""
        try:
            if not self.enable_model_warmup:
                return {
                    "success": False,
                    "message": "ëª¨ë¸ ì›Œë°ì—…ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤",
                    "model_name": model_name,
                    "timestamp": datetime.now().isoformat()
                }
            
            start_time = time.time()
            
            # ì›Œë°ì—… ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)
            await asyncio.sleep(0.5 if self.is_m3_max else 1.0)
            
            warmup_time = time.time() - start_time
            
            # ì„±ëŠ¥ ì •ë³´ ì—…ë°ì´íŠ¸
            if model_name not in self._model_performance:
                self._model_performance[model_name] = {}
            
            self._model_performance[model_name].update({
                "last_warmup": time.time(),
                "warmup_time": warmup_time,
                "warmed_up": True,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max
            })
            
            return {
                "success": True,
                "message": f"ëª¨ë¸ {model_name} ì›Œë°ì—… ì™„ë£Œ",
                "model_name": model_name,
                "warmup_time": warmup_time,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨ {model_name}: {e}")
            return {
                "success": False,
                "error": str(e),
                "model_name": model_name,
                "timestamp": datetime.now().isoformat()
            }

    async def get_performance_stats(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ í†µê³„"""
        try:
            stats = self._stats.copy()
            
            # ì¶”ê°€ ê³„ì‚°
            if stats["total_load_requests"] > 0:
                stats["load_success_rate"] = stats["successful_loads"] / stats["total_load_requests"]
                stats["cache_hit_rate"] = stats["cache_hits"] / stats["total_load_requests"]
            else:
                stats["load_success_rate"] = 0.0
                stats["cache_hit_rate"] = 0.0
            
            # í‰ê·  ë¡œë“œ ì‹œê°„
            if self._model_load_times:
                stats["average_load_time"] = sum(self._model_load_times.values()) / len(self._model_load_times)
                stats["fastest_load_time"] = min(self._model_load_times.values())
                stats["slowest_load_time"] = max(self._model_load_times.values())
            else:
                stats["average_load_time"] = 0.0
                stats["fastest_load_time"] = 0.0
                stats["slowest_load_time"] = 0.0
            
            # M3 Max ìµœì í™”ìœ¨
            if self.is_m3_max and stats["total_load_requests"] > 0:
                stats["m3_max_optimization_rate"] = stats["m3_max_optimized_loads"] / stats["total_load_requests"]
            
            return {
                "success": True,
                "performance_stats": stats,
                "model_performance": self._model_performance,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def clear_cache(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìºì‹œ ì •ë¦¬"""
        try:
            cleared_models = []
            
            # AI íŒŒì´í”„ë¼ì¸ ìºì‹œ ì •ë¦¬
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    loaded_models = list(getattr(model_loader, '_loaded_models', {}).keys())
                    
                    for model_name in loaded_models:
                        success = model_loader.unload_model(model_name)
                        if success:
                            cleared_models.append(model_name)
            
            # ë‚´ë¶€ ìƒíƒœ ì •ë¦¬
            self._model_load_times.clear()
            self._model_performance.clear()
            self._loading_status.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            memory_result = {"success": False, "error": "Memory manager not available"}
            if UTILS_AVAILABLE:
                memory_manager = get_memory_manager()
                if memory_manager:
                    memory_result = memory_manager.clear_cache(aggressive=True)
            
            return {
                "success": True,
                "message": "ëª¨ë¸ ìºì‹œ ì •ë¦¬ ì™„ë£Œ",
                "cleared_models": cleared_models,
                "cleared_count": len(cleared_models),
                "memory_cleanup": memory_result,
                "device": self.device,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            optimization_results = []
            
            # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ë¥¼ í†µí•œ ìµœì í™”
            if UTILS_AVAILABLE:
                memory_manager = get_memory_manager()
                if memory_manager:
                    # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                    memory_stats = memory_manager.get_memory_stats()
                    optimization_results.append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_stats}")
                    
                    # ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰
                    optimization_result = memory_manager.optimize_for_task("model_loading")
                    optimization_results.append(f"ìµœì í™” ê²°ê³¼: {optimization_result}")
                    
                    # ìºì‹œ ì •ë¦¬
                    cache_result = memory_manager.clear_cache(aggressive=self.is_m3_max)
                    optimization_results.append(f"ìºì‹œ ì •ë¦¬: {cache_result}")
            
            # ëª¨ë¸ë³„ ìµœì í™”
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader and hasattr(model_loader, '_loaded_models'):
                    loaded_count = len(model_loader._loaded_models)
                    if loaded_count > self.max_cached_models:
                        # ì˜¤ë˜ëœ ëª¨ë¸ë“¤ ì •ë¦¬
                        optimization_results.append(f"ìºì‹œëœ ëª¨ë¸ ìˆ˜ ({loaded_count})ê°€ ìµœëŒ€ì¹˜ë¥¼ ì´ˆê³¼í•˜ì—¬ ì •ë¦¬ í•„ìš”")
            
            return {
                "success": True,
                "message": "ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ",
                "optimization_results": optimization_results,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def get_system_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì‹œìŠ¤í…œ ì •ë³´"""
        try:
            system_info = {
                "device": self.device,
                "device_type": self.device_type,
                "m3_max_optimized": self.is_m3_max,
                "router_config": {
                    "enable_model_caching": self.enable_model_caching,
                    "max_cached_models": self.max_cached_models,
                    "auto_cleanup": self.auto_cleanup,
                    "enable_model_warmup": self.enable_model_warmup,
                    "allow_model_switching": self.allow_model_switching,
                    "enable_performance_monitoring": self.enable_performance_monitoring
                },
                "component_status": {
                    "utils_available": UTILS_AVAILABLE,
                    "model_manager_available": MODEL_MANAGER_AVAILABLE
                }
            }
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            if UTILS_AVAILABLE:
                memory_manager = get_memory_manager()
                if memory_manager:
                    memory_stats = memory_manager.get_memory_stats()
                    system_info["memory_info"] = memory_stats
            
            # ëª¨ë¸ ë¡œë” ì •ë³´
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    system_info["model_loader_info"] = {
                        "device": model_loader.device,
                        "use_fp16": getattr(model_loader, 'use_fp16', False),
                        "coreml_optimization": getattr(model_loader, 'coreml_optimization', False),
                        "max_cached_models": getattr(model_loader, 'max_cached_models', 0),
                        "currently_loaded": len(getattr(model_loader, '_loaded_models', {}))
                    }
            
            # M3 Max íŠ¹í™” ì •ë³´
            if self.is_m3_max:
                system_info["m3_max_features"] = {
                    "neural_engine": True,
                    "unified_memory": True,
                    "mps_backend": self.device == "mps",
                    "coreml_support": True,
                    "increased_cache_capacity": True,
                    "optimized_loading": True
                }
            
            return {
                "success": True,
                "system_info": system_info,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    async def _load_model_background(
        self,
        model_loader,
        model_name: str,
        enable_warmup: bool,
        start_time: float
    ):
        """ë°±ê·¸ë¼ìš´ë“œ ëª¨ë¸ ë¡œë”©"""
        try:
            loaded_model = await model_loader.load_model(model_name)
            
            if loaded_model:
                load_time = time.time() - start_time
                self._model_load_times[model_name] = load_time
                self._loading_status[model_name] = "loaded"
                self._stats["successful_loads"] += 1
                
                if self.is_m3_max:
                    self._stats["m3_max_optimized_loads"] += 1
                
                # ì›Œë°ì—…
                if enable_warmup:
                    await self._warmup_model_background(model_name)
                
                self.logger.info(f"âœ… ë°±ê·¸ë¼ìš´ë“œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name} ({load_time:.2f}s)")
            else:
                self._loading_status[model_name] = "failed"
                self._stats["failed_loads"] += 1
                self.logger.error(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model_name}")
                
        except Exception as e:
            self._loading_status[model_name] = "failed"
            self._stats["failed_loads"] += 1
            self.logger.error(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ëª¨ë¸ ë¡œë”© ì˜ˆì™¸: {model_name} - {e}")

    async def _warmup_model_background(self, model_name: str):
        """ë°±ê·¸ë¼ìš´ë“œ ëª¨ë¸ ì›Œë°ì—…"""
        try:
            await asyncio.sleep(0.5)  # ì›Œë°ì—… ì‹œë®¬ë ˆì´ì…˜
            
            if model_name not in self._model_performance:
                self._model_performance[model_name] = {}
            
            self._model_performance[model_name].update({
                "warmed_up": True,
                "warmup_time": 0.5,
                "last_warmup": time.time()
            })
            
            self.logger.info(f"ğŸ”¥ ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ: {model_name}")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {model_name} - {e}")

    async def _simulate_model_load(self, model_name: str, start_time: float, enable_warmup: bool) -> Dict[str, Any]:
        """ëª¨ë¸ ë¡œë”© ì‹œë®¬ë ˆì´ì…˜"""
        try:
            # ë¡œë”© ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
            load_time = 2.0 if self.is_m3_max else 4.0
            await asyncio.sleep(load_time)
            
            actual_load_time = time.time() - start_time
            self._model_load_times[model_name] = actual_load_time
            self._loading_status[model_name] = "loaded"
            self._stats["successful_loads"] += 1
            
            if self.is_m3_max:
                self._stats["m3_max_optimized_loads"] += 1
            
            return {
                "success": True,
                "message": f"ëª¨ë¸ {model_name} ë¡œë“œ ì™„ë£Œ (ì‹œë®¬ë ˆì´ì…˜)",
                "model_name": model_name,
                "load_time": actual_load_time,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "simulation_mode": True,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self._loading_status[model_name] = "failed"
            self._stats["failed_loads"] += 1
            raise

    def _estimate_load_time(self, model_name: str) -> float:
        """ëª¨ë¸ ë¡œë”© ì‹œê°„ ì¶”ì •"""
        base_times = {
            "human_parsing": 3.0,
            "pose_estimation": 1.5,
            "cloth_segmentation": 2.5,
            "geometric_matching": 2.0,
            "cloth_warping": 4.0,
            "virtual_fitting": 5.0
        }
        
        # ëª¨ë¸ ì´ë¦„ì—ì„œ íƒ€ì… ì¶”ì •
        for model_type, base_time in base_times.items():
            if model_type in model_name.lower():
                # M3 MaxëŠ” 30-50% ë¹ ë¦„
                if self.is_m3_max:
                    return base_time * 0.6
                return base_time
        
        # ê¸°ë³¸ê°’
        return 3.0 if self.is_m3_max else 5.0

    async def _get_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            if UTILS_AVAILABLE:
                memory_manager = get_memory_manager()
                if memory_manager:
                    memory_stats = memory_manager.get_memory_stats()
                    return {
                        "system_memory": memory_stats.get("system_memory", {}),
                        "gpu_memory": memory_stats.get("gpu_memory", {}),
                        "available": True
                    }
            
            return {"available": False, "reason": "Memory manager not available"}
            
        except Exception as e:
            return {"available": False, "error": str(e)}

# ëª¨ë¸ ê´€ë¦¬ ë¼ìš°í„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìµœì  ìƒì„±ì íŒ¨í„´)
models_router = ModelsRouter()
router = models_router.router

# í¸ì˜ í•¨ìˆ˜ë“¤ (í•˜ìœ„ í˜¸í™˜ì„±)
def create_models_router(
    device: Optional[str] = None,
    enable_model_caching: bool = True,
    **kwargs
) -> ModelsRouter:
    """ëª¨ë¸ ê´€ë¦¬ ë¼ìš°í„° ìƒì„± (í•˜ìœ„ í˜¸í™˜)"""
    return ModelsRouter(
        device=device,
        enable_model_caching=enable_model_caching,
        **kwargs
    )

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    'router',
    'ModelsRouter',
    'OptimalRouterConstructor',
    'create_models_router'
]