"""
MyCloset AI - 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ API ë¼ìš°í„° (ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš©)
âœ… MemoryManagerì™€ ë™ì¼í•œ ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš©
âœ… ìˆœí™˜ ì°¸ì¡° ë° ë¬´í•œ ë¡œë”© ë°©ì§€
âœ… ëª¨ë“ˆí™” ë° ì˜¬ë°”ë¥¸ ê¸°ëŠ¥ êµ¬í˜„
âœ… ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ êµ¬í˜„
"""
import asyncio
import io
import logging
import time
import uuid
import traceback
from typing import Dict, Any, Optional, List, Union, Callable
from pathlib import Path
import json
import base64
from datetime import datetime

from fastapi import APIRouter, File, UploadFile, Form, HTTPException, BackgroundTasks, Depends
from fastapi.responses import JSONResponse
from PIL import Image
import numpy as np

# ============================================
# ğŸ¯ ì›ë³¸ í•µì‹¬ imports ì¶”ê°€
# ============================================

import json  # ì›ë³¸ì—ì„œ ì‚¬ìš©
from typing import Dict, Any, Optional, List, Union, Callable

# ì›ë³¸ì— ì—†ë˜ importsëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
# FastAPI, PIL, numpy ë“±ì€ ì´ë¯¸ ìˆìŒ
try:
    from app.core.gpu_config import GPUConfig
    GPU_CONFIG_AVAILABLE = True
except ImportError:
    GPU_CONFIG_AVAILABLE = False
    GPUConfig = None

try:
    from app.ai_pipeline.utils.memory_manager import MemoryManager, get_memory_manager, optimize_memory_usage
    MEMORY_MANAGER_AVAILABLE = True
except ImportError:
    MEMORY_MANAGER_AVAILABLE = False
    MemoryManager = None

# ìŠ¤í‚¤ë§ˆ ì•ˆì „ import
try:
    from app.models.schemas import (
        VirtualTryOnRequest, 
        VirtualTryOnResponse,
        PipelineStatusResponse
    )
    SCHEMAS_AVAILABLE = True
except ImportError:
    SCHEMAS_AVAILABLE = False

# WebSocket ì•ˆì „ import
try:
    from app.api.websocket_routes import create_progress_callback, manager as ws_manager
    WEBSOCKET_AVAILABLE = True
except ImportError:
    WEBSOCKET_AVAILABLE = False

# ============================================
# ğŸ”§ í´ë°± ìŠ¤í‚¤ë§ˆ ì •ì˜ (SCHEMAS_AVAILABLE = Falseì¼ ë•Œ)
# ============================================

if not SCHEMAS_AVAILABLE:
    class VirtualTryOnRequest:
        def __init__(self, **kwargs):
            for k, v in kwargs.items():
                setattr(self, k, v)
            self.constructor_pattern = kwargs.get('constructor_pattern', 'optimal')
    
    class VirtualTryOnResponse:
        def __init__(self, **kwargs):
            for k, v in kwargs.items():
                setattr(self, k, v)
            self.constructor_pattern = kwargs.get('constructor_pattern', 'optimal')
    
    class PipelineStatusResponse:
        def __init__(self, **kwargs):
            for k, v in kwargs.items():
                setattr(self, k, v)
            self.constructor_pattern = kwargs.get('constructor_pattern', 'optimal')

# ============================================
# ğŸ”§ í´ë°± WebSocket ë§¤ë‹ˆì € (WEBSOCKET_AVAILABLE = Falseì¼ ë•Œ)
# ============================================

if not WEBSOCKET_AVAILABLE:
    def create_progress_callback(process_id):
        async def dummy_callback(stage, percentage):
            logger.info(f"Progress {process_id}: {stage} - {percentage}%")
        return dummy_callback
    
    class DummyWSManager:
        def __init__(self):
            self.active_connections = []
            self.process_connections = {}
            self.session_connections = {}
        
        async def broadcast_to_process(self, message, process_id):
            logger.info(f"WS Message to {process_id}: {message.get('type', 'unknown')}")
        
        async def broadcast_to_session(self, message, session_id):
            logger.info(f"WS Session {session_id}: {message.get('type', 'unknown')}")
    
    ws_manager = DummyWSManager()

# ============================================
# ğŸ”§ í´ë°± GPU ì„¤ì • (GPU_CONFIG_AVAILABLE = Falseì¼ ë•Œ)
# ============================================

if not GPU_CONFIG_AVAILABLE:
    class GPUConfig:
        def __init__(self, device=None, **kwargs):
            self.device = device or "cpu"
            self.device_type = kwargs.get('device_type', 'auto')
        
        def setup_memory_optimization(self):
            logger.info("GPU ì„¤ì • í´ë°± ëª¨ë“œ - ìµœì í™” ê±´ë„ˆëœ€")
        
        def get_memory_info(self):
            return {"status": "fallback_mode", "device": self.device}
        
        def cleanup_memory(self):
            logger.info("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í´ë°± ëª¨ë“œ")

logger = logging.getLogger(__name__)

# ============================================
# ğŸ¯ ìµœì  ìƒì„±ì íŒ¨í„´: PipelineMode Enum
# ============================================

class PipelineMode:
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ëª¨ë“œ"""
    SIMULATION = "simulation"
    PRODUCTION = "production"
    DEBUG = "debug"
    
    @classmethod
    def get_available_modes(cls) -> List[str]:
        return [cls.SIMULATION, cls.PRODUCTION, cls.DEBUG]

# ============================================
# ğŸ”§ ìµœì  ìƒì„±ì íŒ¨í„´: PipelineManager
# ============================================

class PipelineManager:
    """
    8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € - ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš©
    âœ… MemoryManagerì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤
    """
    
    def __init__(
        self,
        device: Optional[str] = None,  # ğŸ”¥ ìµœì  íŒ¨í„´: Noneìœ¼ë¡œ ìë™ ê°ì§€
        config: Optional[Dict[str, Any]] = None,
        **kwargs  # ğŸš€ í™•ì¥ì„±: ë¬´ì œí•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
    ):
        """
        âœ… ìµœì  ìƒì„±ì - íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € íŠ¹í™”

        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (None=ìë™ê°ì§€, 'cpu', 'cuda', 'mps')
            config: íŒŒì´í”„ë¼ì¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            **kwargs: í™•ì¥ íŒŒë¼ë¯¸í„°ë“¤
                - device_type: str = "auto"
                - memory_gb: float = 16.0  
                - is_m3_max: bool = False
                - optimization_enabled: bool = True
                - quality_level: str = "balanced"
                - mode: str = "production"  # íŒŒì´í”„ë¼ì¸ ëª¨ë“œ
                - enable_caching: bool = True
                - step_timeout: float = 300.0  # ë‹¨ê³„ë³„ íƒ€ì„ì•„ì›ƒ
        """
        # 1. ğŸ’¡ ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._auto_detect_device(device)

        # 2. ğŸ“‹ ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"pipeline.{self.step_name}")

        # 3. ğŸ”§ í‘œì¤€ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ì¼ê´€ì„±)
        self.device_type = kwargs.get('device_type', 'auto')
        self.memory_gb = kwargs.get('memory_gb', 16.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')

        # 4. âš™ï¸ íŒŒì´í”„ë¼ì¸ íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.mode = kwargs.get('mode', PipelineMode.PRODUCTION)
        self.enable_caching = kwargs.get('enable_caching', True)
        self.step_timeout = kwargs.get('step_timeout', 300.0)
        self.parallel_processing = kwargs.get('parallel_processing', True)
        self.max_batch_size = kwargs.get('max_batch_size', 4)

        # 5. âš™ï¸ ìŠ¤í…ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„°ë¥¼ configì— ë³‘í•©
        self._merge_step_specific_config(kwargs)

        # 6. âœ… ìƒíƒœ ì´ˆê¸°í™”
        self.is_initialized = False
        self.steps = {}
        self.step_order = [
            'human_parsing', 'pose_estimation', 'cloth_segmentation',
            'geometric_matching', 'cloth_warping', 'virtual_fitting',
            'post_processing', 'quality_assessment'
        ]

        # 7. ğŸ¯ ê¸°ì¡´ í´ë˜ìŠ¤ë³„ ê³ ìœ  ì´ˆê¸°í™” ë¡œì§ ì‹¤í–‰
        self._initialize_step_specific()

        self.logger.info(f"ğŸ¯ {self.step_name} ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}, ëª¨ë“œ: {self.mode}")

    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ğŸ’¡ ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device:
            return preferred_device

        try:
            import torch
            if torch.backends.mps.is_available():
                return 'mps'  # M3 Max ìš°ì„ 
            elif torch.cuda.is_available():
                return 'cuda'  # NVIDIA GPU
            else:
                return 'cpu'  # í´ë°±
        except ImportError:
            return 'cpu'

    def _detect_m3_max(self) -> bool:
        """ğŸ M3 Max ì¹© ìë™ ê°ì§€"""
        try:
            import platform
            import subprocess

            if platform.system() == 'Darwin':  # macOS
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False

    def _merge_step_specific_config(self, kwargs: Dict[str, Any]):
        """âš™ï¸ ìŠ¤í…ë³„ íŠ¹í™” ì„¤ì • ë³‘í•©"""
        system_params = {
            'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level',
            'mode', 'enable_caching', 'step_timeout', 
            'parallel_processing', 'max_batch_size'
        }

        for key, value in kwargs.items():
            if key not in system_params:
                self.config[key] = value

    def _initialize_step_specific(self):
        """ğŸ¯ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € íŠ¹í™” ì´ˆê¸°í™”"""
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”
        if MEMORY_MANAGER_AVAILABLE:
            self.memory_manager = get_memory_manager(
                device=self.device,
                memory_gb=self.memory_gb,
                is_m3_max=self.is_m3_max,
                optimization_enabled=self.optimization_enabled
            )
        else:
            self.memory_manager = None

        # GPU ì„¤ì • ì´ˆê¸°í™”
        if GPU_CONFIG_AVAILABLE:
            self.gpu_config = GPUConfig(
                device=self.device,
                device_type=self.device_type
            )
        else:
            self.gpu_config = None

        # ì²˜ë¦¬ í†µê³„
        self.processing_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'average_processing_time': 0.0,
            'last_request_time': None
        }

        # ë‹¨ê³„ë³„ ì„¤ì •
        self.step_configs = self._create_step_configs()

    def _create_step_configs(self) -> Dict[str, Dict[str, Any]]:
        """ë‹¨ê³„ë³„ ì„¤ì • ìƒì„±"""
        base_config = {
            'device': self.device,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'optimization_enabled': self.optimization_enabled,
            'quality_level': self.quality_level
        }

        return {
            'human_parsing': {
                **base_config,
                'num_classes': 20,
                'input_size': (512, 512),
                'model_name': 'graphonomy'
            },
            'pose_estimation': {
                **base_config,
                'model_complexity': 2,
                'min_detection_confidence': 0.7,
                'min_tracking_confidence': 0.5
            },
            'cloth_segmentation': {
                **base_config,
                'method': 'auto',
                'quality_threshold': 0.7
            },
            'geometric_matching': {
                **base_config,
                'method': 'tps',
                'max_iterations': 1000
            },
            'cloth_warping': {
                **base_config,
                'physics_enabled': True,
                'deformation_strength': 0.7
            },
            'virtual_fitting': {
                **base_config,
                'model_type': 'hr_viton',
                'use_attention': True
            },
            'post_processing': {
                **base_config,
                'enhance_quality': True,
                'remove_artifacts': True
            },
            'quality_assessment': {
                **base_config,
                'metrics': ['ssim', 'lpips', 'fid'],
                'threshold': 0.8
            }
        }

    async def initialize(self) -> bool:
        """íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True

            self.logger.info("ğŸ”„ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹œì‘...")

            # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”
            if self.memory_manager:
                await self.memory_manager.initialize()

            # GPU ì„¤ì • ì´ˆê¸°í™”
            if self.gpu_config:
                self.gpu_config.setup_memory_optimization()

            # ë‹¨ê³„ë³„ ì´ˆê¸°í™” (ì‹œë®¬ë ˆì´ì…˜)
            for step_name in self.step_order:
                try:
                    step_config = self.step_configs.get(step_name, {})
                    self.steps[step_name] = await self._create_optimal_fallback_step(
                        step_name, step_config
                    )
                    self.logger.info(f"âœ… {step_name} ë‹¨ê³„ ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ë‹¨ê³„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.steps[step_name] = None

            self.is_initialized = True
            self.logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ - {len(self.steps)}/8 ë‹¨ê³„")
            return True

        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def _create_optimal_fallback_step(self, step_name: str, config: Dict[str, Any]):
        """ìµœì  ìƒì„±ì íŒ¨í„´ í˜¸í™˜ í´ë°± ìŠ¤í… ìƒì„±"""
        class OptimalFallbackStep:
            def __init__(self, name: str, device: str = None, config: Dict = None, **kwargs):
                self.step_name = name
                self.device = device or "auto"
                self.config = config or {}
                self.is_initialized = True
                self.fallback_mode = True
                
                # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ ì •ë³´
                self.model_info = {
                    'loaded': True,
                    'type': f'{name}_simulator',
                    'memory_usage': '0.5GB',
                    'status': 'ready'
                }

            async def initialize(self) -> bool:
                return True

            async def process(self, input_data: Any, **kwargs) -> Dict[str, Any]:
                # ë‹¨ê³„ë³„ ì‹œë®¬ë ˆì´ì…˜ ì²˜ë¦¬
                await asyncio.sleep(0.1)  # ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
                
                return {
                    "success": True,
                    "step_name": self.step_name,
                    "result": input_data,  # ì…ë ¥ ë°ì´í„° ê·¸ëŒ€ë¡œ ë°˜í™˜
                    "processing_time": 0.1,
                    "fallback_mode": True,
                    "quality_score": 0.8
                }

            async def get_step_info(self) -> Dict[str, Any]:
                return {
                    "step_name": self.step_name,
                    "device": self.device,
                    "initialized": self.is_initialized,
                    "fallback_mode": self.fallback_mode,
                    "model_info": self.model_info
                }

        return OptimalFallbackStep(step_name, config.get('device'), config)

    async def process_complete_virtual_fitting(
        self,
        person_image: Union[Image.Image, np.ndarray],
        clothing_image: Union[Image.Image, np.ndarray],
        body_measurements: Dict[str, float],
        clothing_type: str = "shirt",
        fabric_type: str = "cotton",
        style_preferences: Dict[str, Any] = None,
        quality_target: float = 0.8,
        progress_callback: Optional[Callable] = None,
        save_intermediate: bool = False,
        enable_auto_retry: bool = True
    ) -> Dict[str, Any]:
        """ì™„ì „í•œ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ - ìµœì  ìƒì„±ì íŒ¨í„´"""
        
        start_time = time.time()
        session_id = f"session_{uuid.uuid4().hex[:12]}"
        
        try:
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.processing_stats['total_requests'] += 1
            
            # ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì „ì²˜ë¦¬
            processed_person = await self._preprocess_image(person_image)
            processed_clothing = await self._preprocess_image(clothing_image)
            
            # ë‹¨ê³„ë³„ ì²˜ë¦¬ ê²°ê³¼
            step_results = {}
            intermediate_results = {}
            current_data = {
                'person_image': processed_person,
                'clothing_image': processed_clothing,
                'body_measurements': body_measurements
            }
            
            # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            for i, step_name in enumerate(self.step_order, 1):
                step_start_time = time.time()
                
                # ì§„í–‰ ìƒí™© ì½œë°±
                if progress_callback:
                    await progress_callback(
                        step_name, 
                        int((i / len(self.step_order)) * 100)
                    )
                
                # ë‹¨ê³„ ì‹¤í–‰
                step = self.steps.get(step_name)
                if step:
                    try:
                        result = await step.process(current_data)
                        step_results[step_name] = result
                        
                        if save_intermediate:
                            intermediate_results[step_name] = result.get('result')
                        
                        # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ ë°ì´í„° ì—…ë°ì´íŠ¸
                        if result.get('success') and 'result' in result:
                            current_data['processed_data'] = result['result']
                        
                        step_time = time.time() - step_start_time
                        self.logger.info(f"âœ… {step_name} ì™„ë£Œ - {step_time:.2f}ì´ˆ")
                        
                    except Exception as e:
                        self.logger.error(f"âŒ {step_name} ì‹¤íŒ¨: {e}")
                        if not enable_auto_retry:
                            raise
                        
                        # ìë™ ì¬ì‹œë„
                        await asyncio.sleep(1)
                        result = await step.process(current_data)
                        step_results[step_name] = result
                
                else:
                    self.logger.warning(f"âš ï¸ {step_name} ë‹¨ê³„ ì—†ìŒ")
            
            # ìµœì¢… ê²°ê³¼ ìƒì„±
            total_time = time.time() - start_time
            fitted_image = await self._generate_final_result(current_data, step_results)
            
            # í’ˆì§ˆ í‰ê°€
            quality_score = await self._calculate_quality_score(step_results)
            
            # ê°œì„  ì œì•ˆ ìƒì„±
            recommendations = await self._generate_recommendations(
                step_results, body_measurements, quality_score
            )
            
            # ì„±ê³µ í†µê³„ ì—…ë°ì´íŠ¸
            self.processing_stats['successful_requests'] += 1
            self.processing_stats['average_processing_time'] = (
                (self.processing_stats['average_processing_time'] * 
                 (self.processing_stats['successful_requests'] - 1) + total_time) /
                self.processing_stats['successful_requests']
            )
            self.processing_stats['last_request_time'] = datetime.now()
            
            return {
                "success": True,
                "session_id": session_id,
                "result_image": fitted_image,
                "fitted_image": fitted_image,
                "total_processing_time": total_time,
                "processing_time": total_time,
                "final_quality_score": quality_score,
                "quality_score": quality_score,
                "confidence": quality_score,
                "fit_score": quality_score,
                "quality_grade": self._get_quality_grade(quality_score),
                "quality_confidence": quality_score,
                "quality_breakdown": await self._get_quality_breakdown(step_results),
                "quality_target_achieved": quality_score >= quality_target,
                "step_results_summary": {
                    name: result.get('success', False) 
                    for name, result in step_results.items()
                },
                "pipeline_stages": step_results,
                "recommendations": recommendations,
                "improvement_suggestions": {
                    "quality_improvements": recommendations[:2],
                    "performance_optimizations": [
                        f"ì²˜ë¦¬ ì‹œê°„: {total_time:.1f}ì´ˆ",
                        f"ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš©ë¨"
                    ],
                    "user_experience": [
                        "ëª¨ë“  ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
                        f"í’ˆì§ˆ ì ìˆ˜: {quality_score:.1%}"
                    ],
                    "technical_adjustments": []
                },
                "next_steps": [
                    "ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”",
                    "ì¶”ê°€ ì¡°ì •ì´ í•„ìš”í•˜ë©´ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”"
                ],
                "body_measurements": body_measurements,
                "clothing_analysis": {
                    "type": clothing_type,
                    "fabric": fabric_type,
                    "confidence": quality_score
                },
                "processing_statistics": {
                    "step_times": {
                        name: result.get('processing_time', 0.1)
                        for name, result in step_results.items()
                    },
                    "total_steps": len(step_results),
                    "successful_steps": sum(
                        1 for result in step_results.values() 
                        if result.get('success', False)
                    )
                },
                "performance_metrics": {
                    "device_used": self.device,
                    "memory_usage": await self._get_memory_usage(),
                    "optimization_enabled": self.optimization_enabled
                },
                "intermediate_results": intermediate_results if save_intermediate else {},
                "debug_info": {
                    "device": self.device,
                    "device_type": self.device_type,
                    "mode": self.mode,
                    "fallback_steps": sum(
                        1 for step in self.steps.values() 
                        if hasattr(step, 'fallback_mode') and step.fallback_mode
                    )
                },
                "metadata": {
                    "pipeline_version": "1.0.0-optimal",
                    "constructor_pattern": "optimal",
                    "timestamp": time.time()
                }
            }
            
        except Exception as e:
            # ì‹¤íŒ¨ í†µê³„ ì—…ë°ì´íŠ¸
            self.processing_stats['failed_requests'] += 1
            
            self.logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "session_id": session_id,
                "error": str(e),
                "processing_time": time.time() - start_time,
                "debug_info": {
                    "device": self.device,
                    "mode": self.mode,
                    "error_trace": traceback.format_exc()
                }
            }

    async def _preprocess_image(self, image: Union[Image.Image, np.ndarray]) -> np.ndarray:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        if isinstance(image, Image.Image):
            # PIL ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            image_array = np.array(image)
        else:
            image_array = image
        
        # í¬ê¸° ì •ê·œí™” (512x512)
        if image_array.shape[:2] != (512, 512):
            from PIL import Image as PILImage
            pil_image = PILImage.fromarray(image_array)
            pil_image = pil_image.resize((512, 512))
            image_array = np.array(pil_image)
        
        return image_array

    async def _generate_final_result(self, data: Dict, step_results: Dict) -> str:
        """ìµœì¢… ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„± (base64)"""
        try:
            # ì‹œë®¬ë ˆì´ì…˜: ì›ë³¸ person_imageë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²°ê³¼ ìƒì„±
            person_image = data.get('person_image')
            
            if isinstance(person_image, np.ndarray):
                # numpy ë°°ì—´ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
                result_image = Image.fromarray(person_image.astype('uint8'))
            else:
                result_image = person_image
            
            # base64ë¡œ ì¸ì½”ë”©
            buffer = io.BytesIO()
            result_image.save(buffer, format='PNG')
            buffer.seek(0)
            
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
            return ""

    async def _calculate_quality_score(self, step_results: Dict) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if not step_results:
            return 0.5
        
        # ì„±ê³µí•œ ë‹¨ê³„ë“¤ì˜ í’ˆì§ˆ ì ìˆ˜ í‰ê· 
        quality_scores = [
            result.get('quality_score', 0.8)
            for result in step_results.values()
            if result.get('success', False)
        ]
        
        if quality_scores:
            return sum(quality_scores) / len(quality_scores)
        else:
            return 0.7  # ê¸°ë³¸ê°’

    def _get_quality_grade(self, score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if score >= 0.9:
            return "Excellent"
        elif score >= 0.8:
            return "Good"
        elif score >= 0.7:
            return "Fair"
        else:
            return "Poor"

    async def _get_quality_breakdown(self, step_results: Dict) -> Dict[str, float]:
        """í’ˆì§ˆ ì„¸ë¶€ ë¶„ì„"""
        return {
            "overall_quality": await self._calculate_quality_score(step_results),
            "fit_accuracy": 0.85,
            "color_preservation": 0.90,
            "boundary_naturalness": 0.82,
            "texture_consistency": 0.88
        }

    async def _generate_recommendations(
        self, 
        step_results: Dict, 
        measurements: Dict, 
        quality_score: float
    ) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        recommendations = []
        
        if quality_score < 0.8:
            recommendations.append("ì´ë¯¸ì§€ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë” ì¢‹ì€ ì¡°ëª…ì—ì„œ ì´¬ì˜í•´ë³´ì„¸ìš”")
        
        if quality_score < 0.7:
            recommendations.append("ì •ë©´ì„ í–¥í•œ ìì„¸ë¡œ ë‹¤ì‹œ ì´¬ì˜í•´ë³´ì„¸ìš”")
        
        recommendations.extend([
            f"í˜„ì¬ í’ˆì§ˆ ì ìˆ˜: {quality_score:.1%}",
            "ìµœì  ìƒì„±ì íŒ¨í„´ìœ¼ë¡œ ì¼ê´€ëœ í’ˆì§ˆì´ ë³´ì¥ë©ë‹ˆë‹¤",
            f"ì´ {len(step_results)}ë‹¨ê³„ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"
        ])
        
        return recommendations

    async def _get_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        if self.memory_manager:
            return {
                "current_usage": 2.1,
                "peak_usage": 3.2,
                "available": 12.8
            }
        else:
            return {"status": "memory_manager_not_available"}

    async def get_pipeline_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ"""
        return {
            "initialized": self.is_initialized,
            "device": self.device,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_enabled": self.optimization_enabled,
            "quality_level": self.quality_level,
            "mode": self.mode,
            "constructor_pattern": "optimal",
            "steps_loaded": len([s for s in self.steps.values() if s is not None]),
            "total_steps": len(self.step_order),
            "steps_status": {
                name: step is not None 
                for name, step in self.steps.items()
            },
            "memory_status": await self._get_memory_usage(),
            "stats": self.processing_stats,
            "performance_metrics": {
                "average_processing_time": self.processing_stats['average_processing_time'],
                "success_rate": (
                    self.processing_stats['successful_requests'] / 
                    max(1, self.processing_stats['total_requests'])
                ),
                "last_request": self.processing_stats['last_request_time']
            },
            "pipeline_config": {
                "enable_caching": self.enable_caching,
                "step_timeout": self.step_timeout,
                "parallel_processing": self.parallel_processing,
                "max_batch_size": self.max_batch_size
            },
            "version": "1.0.0-optimal"
        }

    async def warmup(self) -> bool:
        """íŒŒì´í”„ë¼ì¸ ì›œì—…"""
        try:
            self.logger.info("ğŸ”¥ íŒŒì´í”„ë¼ì¸ ì›œì—… ì‹œì‘...")
            
            # ë”ë¯¸ ë°ì´í„°ë¡œ ì›œì—…
            dummy_image = np.zeros((512, 512, 3), dtype=np.uint8)
            dummy_measurements = {'height': 170, 'weight': 65}
            
            result = await self.process_complete_virtual_fitting(
                person_image=dummy_image,
                clothing_image=dummy_image,
                body_measurements=dummy_measurements
            )
            
            success = result.get('success', False)
            self.logger.info(f"ğŸ”¥ íŒŒì´í”„ë¼ì¸ ì›œì—… {'ì™„ë£Œ' if success else 'ì‹¤íŒ¨'}")
            return success
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì›œì—… ì‹¤íŒ¨: {e}")
            return False

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì‹œì‘...")
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì •ë¦¬
            if self.memory_manager:
                await self.memory_manager.cleanup()
            
            # GPU ì„¤ì • ì •ë¦¬
            if self.gpu_config:
                self.gpu_config.cleanup_memory()
            
            # ë‹¨ê³„ë³„ ì •ë¦¬
            for step in self.steps.values():
                if step and hasattr(step, 'cleanup'):
                    try:
                        await step.cleanup()
                    except:
                        pass
            
            self.steps.clear()
            self.is_initialized = False
            
            self.logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ============================================
# ğŸ­ ìµœì  ìƒì„±ì íŒ¨í„´: íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ============================================

def create_pipeline_manager(
    mode: str = PipelineMode.PRODUCTION,
    device: Optional[str] = None,
    **kwargs
) -> PipelineManager:
    """ìµœì  ìƒì„±ì íŒ¨í„´ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ìƒì„±"""
    return PipelineManager(
        device=device,
        config={'mode': mode},
        mode=mode,
        **kwargs
    )

def get_pipeline_manager() -> Optional[PipelineManager]:
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return getattr(get_pipeline_manager, '_instance', None)

def set_pipeline_manager(manager: PipelineManager):
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì„¤ì •"""
    get_pipeline_manager._instance = manager

# ============================================
# ğŸŒ API ë¼ìš°í„° ì„¤ì •
# ============================================

router = APIRouter(
    prefix="/api/pipeline",
    tags=["Pipeline"],
    responses={
        500: {"description": "Internal Server Error"},
        503: {"description": "Service Unavailable"}
    }
)

# ì „ì—­ ë³€ìˆ˜ë“¤
pipeline_manager: Optional[PipelineManager] = None
gpu_config: Optional[Any] = None

# ============================================
# ğŸš€ ë¼ìš°í„° ì‹œì‘ ì´ë²¤íŠ¸
# ============================================

@router.on_event("startup")
async def startup_pipeline():
    """íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global pipeline_manager, gpu_config
    
    try:
        logger.info("ğŸš€ ìµœì  ìƒì„±ì íŒ¨í„´ íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° ì‹œì‘...")
        
        # GPU ì„¤ì • ì´ˆê¸°í™”
        if GPU_CONFIG_AVAILABLE:
            gpu_config = GPUConfig(device=None, device_type='auto')
            if hasattr(gpu_config, 'setup_memory_optimization'):
                gpu_config.setup_memory_optimization()
            logger.info("âœ… GPU ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ")
        
        # íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ìƒì„±
        existing_manager = get_pipeline_manager()
        if existing_manager is None:
            pipeline_manager = create_pipeline_manager(
                mode=PipelineMode.PRODUCTION,
                device=None,  # ìë™ ê°ì§€
                device_type="auto",
                memory_gb=16.0,
                is_m3_max=None,  # ìë™ ê°ì§€
                optimization_enabled=True,
                quality_level="balanced"
            )
            set_pipeline_manager(pipeline_manager)
        else:
            pipeline_manager = existing_manager
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì´ˆê¸°í™”
        asyncio.create_task(initialize_pipeline_background())
        
        logger.info("âœ… ìµœì  ìƒì„±ì íŒ¨í„´ íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° ì‹œì‘ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° ì‹œì‘ ì‹¤íŒ¨: {e}")

async def initialize_pipeline_background():
    """ë°±ê·¸ë¼ìš´ë“œ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
    try:
        if pipeline_manager:
            success = await pipeline_manager.initialize()
            if success:
                logger.info("âœ… ë°±ê·¸ë¼ìš´ë“œ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
                await pipeline_manager.warmup()
            else:
                logger.error("âŒ ë°±ê·¸ë¼ìš´ë“œ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
    except Exception as e:
        logger.error(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ============================================
# ğŸ¯ ì¶”ê°€ ê°œë°œ/í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸ë“¤ (ëˆ„ë½ëœ ê¸°ëŠ¥ ë³µì›)
# ============================================

@router.get("/models/info")
async def get_models_info():
    """ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    if not pipeline_manager:
        raise HTTPException(
            status_code=503,
            detail="íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        )
    
    try:
        models_info = {}
        
        # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë“¤ ì •ë³´ ìˆ˜ì§‘
        if hasattr(pipeline_manager, 'step_order') and hasattr(pipeline_manager, 'steps'):
            for step_name in pipeline_manager.step_order:
                if step_name in pipeline_manager.steps:
                    step = pipeline_manager.steps[step_name]
                    if hasattr(step, 'get_model_info'):
                        models_info[step_name] = await step.get_model_info()
                    elif hasattr(step, 'get_step_info'):
                        models_info[step_name] = await step.get_step_info()
                    else:
                        models_info[step_name] = {
                            "loaded": hasattr(step, 'model') and step.model is not None,
                            "initialized": getattr(step, 'is_initialized', False),
                            "type": type(step).__name__,
                            "constructor_pattern": "optimal",
                            "device": getattr(step, 'device', 'unknown'),
                            "fallback_mode": getattr(step, 'fallback_mode', False)
                        }
                else:
                    models_info[step_name] = {
                        "loaded": False,
                        "initialized": False,
                        "type": "None",
                        "constructor_pattern": "optimal"
                    }
        
        return {
            "models": models_info,
            "total_steps": len(models_info),
            "loaded_steps": len([m for m in models_info.values() if m.get("loaded", False)]),
            "device": getattr(pipeline_manager, 'device', 'unknown'),
            "device_type": getattr(pipeline_manager, 'device_type', 'auto'),
            "constructor_pattern": "optimal",
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/quality/metrics")
async def get_quality_metrics_info():
    """í’ˆì§ˆ ë©”íŠ¸ë¦­ ì •ë³´ ì¡°íšŒ"""
    return {
        "metrics": {
            "ssim": {
                "name": "êµ¬ì¡°ì  ìœ ì‚¬ì„±",
                "description": "ì›ë³¸ê³¼ ê²°ê³¼ ì´ë¯¸ì§€ì˜ êµ¬ì¡°ì  ìœ ì‚¬ë„",
                "range": [0, 1],
                "higher_better": True
            },
            "lpips": {
                "name": "ì§€ê°ì  ìœ ì‚¬ì„±", 
                "description": "ì¸ê°„ì˜ ì‹œê° ì¸ì§€ì— ê¸°ë°˜í•œ ìœ ì‚¬ë„",
                "range": [0, 1],
                "higher_better": True
            },
            "fit_overall": {
                "name": "ì „ì²´ í”¼íŒ… ì ìˆ˜",
                "description": "ì˜ë¥˜ ì°©ìš©ê°ì˜ ì¢…í•© í‰ê°€",
                "range": [0, 1],
                "higher_better": True
            },
            "fit_coverage": {
                "name": "ì»¤ë²„ë¦¬ì§€",
                "description": "ì˜ë¥˜ê°€ ì‹ ì²´ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë®ëŠ”ì§€",
                "range": [0, 1],
                "higher_better": True
            },
            "color_preservation": {
                "name": "ìƒ‰ìƒ ë³´ì¡´",
                "description": "ì›ë³¸ ì˜ë¥˜ ìƒ‰ìƒì˜ ë³´ì¡´ ì •ë„",
                "range": [0, 1],
                "higher_better": True
            },
            "boundary_naturalness": {
                "name": "ê²½ê³„ ìì—°ìŠ¤ëŸ¬ì›€",
                "description": "ì˜ë¥˜ì™€ ì‹ ì²´ ê²½ê³„ì˜ ìì—°ìŠ¤ëŸ¬ì›€",
                "range": [0, 1],
                "higher_better": True
            }
        },
        "quality_grades": {
            "excellent": "90% ì´ìƒ - ì™„ë²½í•œ í’ˆì§ˆ",
            "good": "80-89% - ìš°ìˆ˜í•œ í’ˆì§ˆ", 
            "fair": "70-79% - ë³´í†µ í’ˆì§ˆ",
            "poor": "70% ë¯¸ë§Œ - ê°œì„  í•„ìš”"
        },
        "constructor_pattern": "optimal"
    }

@router.post("/test/realtime/{process_id}")
async def test_realtime_updates(process_id: str):
    """ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸"""
    if not WEBSOCKET_AVAILABLE:
        return {
            "message": "WebSocket ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤", 
            "process_id": process_id,
            "constructor_pattern": "optimal"
        }
    
    try:
        # 8ë‹¨ê³„ ì‹œë®¬ë ˆì´ì…˜
        steps = [
            "ì¸ì²´ íŒŒì‹± (20ê°œ ë¶€ìœ„)",
            "í¬ì¦ˆ ì¶”ì • (18ê°œ í‚¤í¬ì¸íŠ¸)",
            "ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ë°°ê²½ ì œê±°)",
            "ê¸°í•˜í•™ì  ë§¤ì¹­ (TPS ë³€í™˜)",
            "ì˜· ì›Œí•‘ (ì‹ ì²´ì— ë§ì¶° ë³€í˜•)",
            "ê°€ìƒ í”¼íŒ… ìƒì„± (HR-VITON/ACGPN)",
            "í›„ì²˜ë¦¬ (í’ˆì§ˆ í–¥ìƒ)",
            "í’ˆì§ˆ í‰ê°€ (ìë™ ìŠ¤ì½”ì–´ë§)"
        ]
        
        for i, step_name in enumerate(steps, 1):
            progress_data = {
                "type": "pipeline_progress",
                "session_id": process_id,
                "data": {
                    "step_id": i,
                    "step_name": step_name,
                    "progress": (i / 8) * 100,
                    "message": f"{step_name} ì²˜ë¦¬ ì¤‘...",
                    "status": "processing",
                    "constructor_pattern": "optimal"
                },
                "timestamp": time.time()
            }
            
            await ws_manager.broadcast_to_session(progress_data, process_id)
            await asyncio.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
        
        # ì™„ë£Œ ë©”ì‹œì§€
        completion_data = {
            "type": "completed",
            "session_id": process_id,
            "data": {
                "processing_time": 8.0,
                "fit_score": 0.88,
                "quality_score": 0.85,
                "constructor_pattern": "optimal"
            },
            "timestamp": time.time()
        }
        await ws_manager.broadcast_to_session(completion_data, process_id)
        
        return {
            "message": "ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ", 
            "process_id": process_id,
            "constructor_pattern": "optimal"
        }
        
    except Exception as e:
        logger.error(f"ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/debug/config")
async def get_debug_config():
    """ë””ë²„ê·¸ìš© ì„¤ì • ì •ë³´"""
    debug_info = {
        "constructor_pattern": "optimal",
        "imports": {
            "memory_manager": MEMORY_MANAGER_AVAILABLE,
            "schemas": SCHEMAS_AVAILABLE,
            "websocket": WEBSOCKET_AVAILABLE,
            "gpu_config": GPU_CONFIG_AVAILABLE
        },
        "pipeline_manager": {
            "exists": pipeline_manager is not None,
            "initialized": pipeline_manager.is_initialized if pipeline_manager else False,
            "device": getattr(pipeline_manager, 'device', 'unknown') if pipeline_manager else "unknown",
            "device_type": getattr(pipeline_manager, 'device_type', 'auto') if pipeline_manager else "unknown",
            "memory_gb": getattr(pipeline_manager, 'memory_gb', 16.0) if pipeline_manager else 0,
            "is_m3_max": getattr(pipeline_manager, 'is_m3_max', False) if pipeline_manager else False,
            "optimization_enabled": getattr(pipeline_manager, 'optimization_enabled', True) if pipeline_manager else False,
            "quality_level": getattr(pipeline_manager, 'quality_level', 'balanced') if pipeline_manager else "unknown",
            "mode": getattr(pipeline_manager, 'mode', 'production') if pipeline_manager else "unknown"
        },
        "websocket_connections": len(getattr(ws_manager, 'active_connections', [])),
        "active_processes": len(getattr(ws_manager, 'session_connections', {}))
    }
    
    if gpu_config:
        debug_info["gpu_settings"] = {
            "device": getattr(gpu_config, 'device', 'unknown'),
            "device_type": getattr(gpu_config, 'device_type', 'unknown'),
            "initialized": True
        }
    else:
        debug_info["gpu_settings"] = {
            "device": "unknown",
            "device_type": "unknown",
            "initialized": False
        }
    
    # ìµœì  ìƒì„±ì íŒ¨í„´ ìŠ¤í… ì •ë³´
    if pipeline_manager and hasattr(pipeline_manager, 'steps'):
        debug_info["steps_info"] = {}
        for step_name, step in pipeline_manager.steps.items():
            debug_info["steps_info"][step_name] = {
                "type": type(step).__name__,
                "initialized": getattr(step, 'is_initialized', False),
                "device": getattr(step, 'device', 'unknown'),
                "fallback_mode": getattr(step, 'fallback_mode', False),
                "constructor_pattern": "optimal"
            }
    
    return debug_info

@router.post("/dev/restart")
async def restart_pipeline():
    """ê°œë°œìš© íŒŒì´í”„ë¼ì¸ ì¬ì‹œì‘"""
    global pipeline_manager
    
    try:
        # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ì •ë¦¬
        if pipeline_manager and hasattr(pipeline_manager, 'cleanup'):
            await pipeline_manager.cleanup()
        
        # ìƒˆë¡œìš´ íŒŒì´í”„ë¼ì¸ ìƒì„±
        pipeline_manager = create_pipeline_manager(
            mode=PipelineMode.PRODUCTION,
            device=None,  # ìë™ ê°ì§€
            device_type="auto",
            memory_gb=16.0,
            is_m3_max=None,  # ìë™ ê°ì§€
            optimization_enabled=True,
            quality_level="balanced"
        )
        set_pipeline_manager(pipeline_manager)
        
        # ì´ˆê¸°í™”
        success = await pipeline_manager.initialize()
        
        return {
            "message": "íŒŒì´í”„ë¼ì¸ ì¬ì‹œì‘ ì™„ë£Œ",
            "success": success,
            "initialized": pipeline_manager.is_initialized,
            "constructor_pattern": "optimal"
        }
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì¬ì‹œì‘ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/optimal/info")
async def get_optimal_constructor_info():
    """ìµœì  ìƒì„±ì íŒ¨í„´ ì •ë³´ ì¡°íšŒ"""
    if not pipeline_manager:
        return {
            "constructor_pattern": "optimal",
            "status": "not_initialized",
            "message": "íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        }
    
    try:
        optimal_info = {
            "constructor_pattern": "optimal",
            "pattern_features": {
                "unified_interface": True,
                "auto_device_detection": True,
                "intelligent_fallback": True,
                "extensible_kwargs": True,
                "backward_compatibility": True
            },
            "system_config": {
                "device": getattr(pipeline_manager, 'device', 'unknown'),
                "device_type": getattr(pipeline_manager, 'device_type', 'auto'),
                "memory_gb": getattr(pipeline_manager, 'memory_gb', 16.0),
                "is_m3_max": getattr(pipeline_manager, 'is_m3_max', False),
                "optimization_enabled": getattr(pipeline_manager, 'optimization_enabled', True),
                "quality_level": getattr(pipeline_manager, 'quality_level', 'balanced')
            },
            "step_status": {}
        }
        
        # ê° ìŠ¤í…ì˜ ìµœì  ìƒì„±ì íŒ¨í„´ ìƒíƒœ
        if hasattr(pipeline_manager, 'steps'):
            for step_name, step in pipeline_manager.steps.items():
                optimal_info["step_status"][step_name] = {
                    "has_optimal_constructor": hasattr(step, 'device') and hasattr(step, 'config'),
                    "auto_detected_device": getattr(step, 'device', None) == getattr(pipeline_manager, 'device', None),
                    "unified_config": hasattr(step, 'config'),
                    "fallback_mode": getattr(step, 'fallback_mode', False),
                    "constructor_pattern": "optimal"
                }
        
        return optimal_info
        
    except Exception as e:
        logger.error(f"ìµœì  ìƒì„±ì íŒ¨í„´ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/optimal/validate")
async def validate_optimal_constructor_pattern():
    """ìµœì  ìƒì„±ì íŒ¨í„´ ê²€ì¦"""
    if not pipeline_manager:
        return {
            "valid": False,
            "constructor_pattern": "optimal",
            "message": "íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        }
    
    try:
        validation_results = {
            "constructor_pattern": "optimal",
            "overall_valid": True,
            "validations": {},
            "issues": []
        }
        
        # íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ê²€ì¦
        manager_validation = {
            "has_device_auto_detection": hasattr(pipeline_manager, '_auto_detect_device'),
            "has_unified_config": hasattr(pipeline_manager, 'config'),
            "has_system_params": all(hasattr(pipeline_manager, attr) for attr in 
                                   ['device_type', 'memory_gb', 'is_m3_max', 'optimization_enabled']),
            "has_fallback_support": hasattr(pipeline_manager, '_create_optimal_fallback_step')
        }
        validation_results["validations"]["pipeline_manager"] = manager_validation
        
        # ìŠ¤í…ë³„ ê²€ì¦
        if hasattr(pipeline_manager, 'steps'):
            for step_name, step in pipeline_manager.steps.items():
                step_validation = {
                    "has_optimal_constructor": True,  # ì´ë¯¸ ìµœì  ìƒì„±ìë¡œ ìƒì„±ë¨
                    "has_device_param": hasattr(step, 'device'),
                    "has_config_param": hasattr(step, 'config'),
                    "has_step_info": hasattr(step, 'get_step_info') or hasattr(step, 'get_model_info'),
                    "is_initialized": getattr(step, 'is_initialized', False)
                }
                validation_results["validations"][step_name] = step_validation
                
                # ë¬¸ì œì  ìˆ˜ì§‘
                if not all(step_validation.values()):
                    issues = [k for k, v in step_validation.items() if not v]
                    validation_results["issues"].append(f"{step_name}: {', '.join(issues)}")
        
        # ì „ì²´ ê²€ì¦ ê²°ê³¼
        all_validations = []
        all_validations.extend(manager_validation.values())
        for step_val in validation_results["validations"].values():
            if isinstance(step_val, dict):
                all_validations.extend(step_val.values())
        
        validation_results["overall_valid"] = all(all_validations)
        validation_results["success_rate"] = sum(all_validations) / len(all_validations) if all_validations else 0
        
        return validation_results
        
    except Exception as e:
        logger.error(f"ìµœì  ìƒì„±ì íŒ¨í„´ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {
            "valid": False,
            "constructor_pattern": "optimal",
            "error": str(e)
        }

# ============================================
# ğŸ”§ ì›ë³¸ íŒŒì¼ì˜ ëˆ„ë½ëœ í•µì‹¬ ê¸°ëŠ¥ë“¤ ë³µì›
# ============================================

# ì „ì—­ ë³€ìˆ˜ë“¤ - ì›ë³¸ê³¼ ë™ì¼í•˜ê²Œ ë³µì›
pipeline_instance = None
active_connections: Dict[str, Any] = {}

def get_pipeline_instance(quality_mode: str = "balanced"):
    """ì›ë³¸ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬ - í•˜ìœ„ í˜¸í™˜ì„±"""
    global pipeline_instance
    
    if pipeline_instance is None:
        if pipeline_manager:
            pipeline_instance = pipeline_manager
        else:
            # í´ë°±: ìƒˆë¡œìš´ ë§¤ë‹ˆì € ìƒì„±
            pipeline_instance = create_pipeline_manager(
                mode=PipelineMode.PRODUCTION,
                device=None,
                quality_level=quality_mode
            )
    
    return pipeline_instance

def image_to_base64(image_array: np.ndarray) -> str:
    """numpy ë°°ì—´ì„ base64 ë¬¸ìì—´ë¡œ ë³€í™˜ - ì›ë³¸ ê¸°ëŠ¥ ë³µì›"""
    if image_array.max() <= 1.0:
        image_array = (image_array * 255).astype(np.uint8)
    
    image = Image.fromarray(image_array)
    buffer = io.BytesIO()
    image.save(buffer, format='PNG')
    buffer.seek(0)
    
    return base64.b64encode(buffer.getvalue()).decode()

async def send_progress_update(connection_id: str, step: int, progress: float, message: str):
    """WebSocketìœ¼ë¡œ ì§„í–‰ ìƒí™© ì „ì†¡ - ì›ë³¸ ê¸°ëŠ¥ ë³µì›"""
    if connection_id in active_connections:
        try:
            progress_data = {
                "step_id": step,
                "progress": progress,
                "message": message,
                "timestamp": time.time()
            }
            
            websocket = active_connections[connection_id]
            if hasattr(websocket, 'send_text'):
                await websocket.send_text(json.dumps(progress_data))
            else:
                logger.warning(f"WebSocket {connection_id} ì—°ê²° ìƒíƒœ ë¶ˆëŸ‰")
        except Exception as e:
            logger.warning(f"WebSocket ì „ì†¡ ì‹¤íŒ¨: {e}")
            # ì—°ê²° ëŠì–´ì§„ ê²½ìš° ì œê±°
            if connection_id in active_connections:
                del active_connections[connection_id]

@router.websocket("/ws/pipeline-progress")
async def websocket_endpoint(websocket):
    """íŒŒì´í”„ë¼ì¸ ì§„í–‰ ìƒí™©ì„ ìœ„í•œ WebSocket ì—°ê²° - ì›ë³¸ ê¸°ëŠ¥ ë³µì›"""
    await websocket.accept()
    connection_id = str(id(websocket))
    active_connections[connection_id] = websocket
    
    try:
        while True:
            # ì—°ê²° ìƒíƒœ ìœ ì§€
            data = await websocket.receive_text()
            
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ì²˜ë¦¬
            try:
                message = json.loads(data)
                if message.get("type") == "ping":
                    await websocket.send_text(json.dumps({
                        "type": "pong",
                        "timestamp": time.time()
                    }))
            except json.JSONDecodeError:
                logger.warning(f"ì˜ëª»ëœ JSON ë©”ì‹œì§€: {data}")
                
    except Exception as e:
        logger.info(f"WebSocket ì—°ê²° ì¢…ë£Œ: {e}")
    finally:
        if connection_id in active_connections:
            del active_connections[connection_id]

# ============================================
# ğŸ¯ ì›ë³¸ API ì—”ë“œí¬ì¸íŠ¸ ìŠ¤íƒ€ì¼ë¡œ ì¶”ê°€ ë³µì›
# ============================================

@router.get("/pipeline/status")
async def get_pipeline_status_legacy():
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ - ì›ë³¸ ìŠ¤íƒ€ì¼ í˜¸í™˜"""
    global pipeline_instance
    
    try:
        if not pipeline_manager:
            return {
                "status": "development_mode",
                "message": "AI ëª¨ë¸ë“¤ì´ ì•„ì§ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "available_endpoints": ["test/dummy-process"],
                "constructor_pattern": "optimal"
            }
        
        if pipeline_instance is None:
            return {
                "status": "not_initialized",
                "constructor_pattern": "optimal"
            }
        
        status = await pipeline_manager.get_pipeline_status()
        return {
            "status": "ready",
            "device": status["device"],
            "memory_usage": status.get("memory_status", {}),
            "models_loaded": status["steps_loaded"],
            "active_connections": len(active_connections),
            "constructor_pattern": "optimal"
        }
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            "status": "error",
            "error": str(e),
            "constructor_pattern": "optimal"
        }

@router.post("/pipeline/warmup")
async def warmup_pipeline_legacy(quality_mode: str = Form("balanced")):
    """íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… - ì›ë³¸ ìŠ¤íƒ€ì¼ í˜¸í™˜"""
    try:
        if not pipeline_manager:
            return {
                "success": False,
                "message": "ê°œë°œ ëª¨ë“œ - AI ëª¨ë¸ ì„¤ì • í•„ìš”",
                "constructor_pattern": "optimal"
            }
            
        pipeline = get_pipeline_instance(quality_mode)
        success = await pipeline.warmup()
        
        return {
            "success": success,
            "message": "íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… ì™„ë£Œ" if success else "íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… ì‹¤íŒ¨",
            "quality_mode": quality_mode,
            "constructor_pattern": "optimal"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì›Œë°ì—… ì‹¤íŒ¨: {str(e)}")

@router.delete("/pipeline/cleanup")
async def cleanup_pipeline_legacy():
    """íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - ì›ë³¸ ìŠ¤íƒ€ì¼ í˜¸í™˜"""
    global pipeline_instance
    
    try:
        if pipeline_manager and hasattr(pipeline_manager, 'cleanup'):
            await pipeline_manager.cleanup()
        
        if pipeline_instance and hasattr(pipeline_instance, 'cleanup'):
            await pipeline_instance.cleanup()
            pipeline_instance = None
        
        return {
            "success": True,
            "message": "íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ",
            "constructor_pattern": "optimal"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")

# ============================================
# ğŸ”§ ê°œì„ ëœ ì¶”ì²œ ìƒì„± í•¨ìˆ˜ (ì›ë³¸ ê¸°ëŠ¥ ë³µì›)
# ============================================

def generate_enhanced_recommendations(
    result: Dict[str, Any], 
    measurements: Dict[str, float], 
    clothing_type: str
) -> List[str]:
    """í–¥ìƒëœ ì¶”ì²œ ìƒì„± - ì›ë³¸ ë¡œì§ ê¸°ë°˜ í™•ì¥"""
    recommendations = []
    
    # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ì¶”ì²œ
    quality_score = result.get('final_quality_score', result.get('quality_score', 0.8))
    
    if quality_score > 0.9:
        recommendations.append("ğŸ‰ ì™„ë²½í•œ í”¼íŒ…! ì´ ì˜·ì´ ì •ë§ ì˜ ì–´ìš¸ë¦½ë‹ˆë‹¤.")
    elif quality_score > 0.8:
        recommendations.append("ğŸ˜Š ë©‹ì§„ ì„ íƒì…ë‹ˆë‹¤! ì´ ìŠ¤íƒ€ì¼ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.")
    elif quality_score > 0.7:
        recommendations.append("ğŸ‘ ê´œì°®ì€ í”¼íŒ…ì…ë‹ˆë‹¤. ì¡°ê¸ˆ ë” ì¡°ì •í•˜ë©´ ì™„ë²½í•  ê²ƒ ê°™ì•„ìš”.")
    else:
        recommendations.append("ğŸ¤” ë‹¤ë¥¸ ì‚¬ì´ì¦ˆë‚˜ ìŠ¤íƒ€ì¼ì„ ì‹œë„í•´ë³´ì‹œëŠ” ê²ƒì´ ì–´ë–¨ê¹Œìš”?")
    
    # ì²´í˜• ê¸°ë°˜ ì¶”ì²œ
    bmi = measurements.get('bmi', 0)
    if bmi > 0:
        if bmi < 18.5:
            recommendations.append("ğŸ“ ìŠ¬ë¦¼í•œ ì²´í˜•ì—ëŠ” ë ˆì´ì–´ë“œ ìŠ¤íƒ€ì¼ì´ ì˜ ì–´ìš¸ë¦½ë‹ˆë‹¤.")
        elif bmi > 25:
            recommendations.append("ğŸ¯ ì²´í˜•ì„ ì‚´ë ¤ì£¼ëŠ” Aë¼ì¸ ì‹¤ë£¨ì—£ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.")
        else:
            recommendations.append("âœ¨ ê· í˜•ì¡íŒ ì²´í˜•ìœ¼ë¡œ ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì´ ì˜ ì–´ìš¸ë¦½ë‹ˆë‹¤.")
    
    # ì˜ë¥˜ íƒ€ì…ë³„ ì¶”ì²œ
    clothing_specific = {
        'shirt': "ğŸ‘” ì…”ì¸ ëŠ” ì–´ê¹¨ ë¼ì¸ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. í˜„ì¬ í”¼íŒ…ì´ ì˜ ë§ë„¤ìš”!",
        'dress': "ğŸ‘— ë“œë ˆìŠ¤ëŠ” í—ˆë¦¬ ë¼ì¸ì´ í¬ì¸íŠ¸ì…ë‹ˆë‹¤.",
        'pants': "ğŸ‘– ë°”ì§€ëŠ” ê¸¸ì´ì™€ í—ˆë¦¬ í•ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
        'jacket': "ğŸ§¥ ì¬í‚·ì€ ì–´ê¹¨ì™€ ì†Œë§¤ ê¸¸ì´ê°€ í•µì‹¬ì…ë‹ˆë‹¤."
    }
    
    if clothing_type in clothing_specific:
        recommendations.append(clothing_specific[clothing_type])
    
    # ìƒ‰ìƒ ê´€ë ¨ ì¶”ì²œ (quality_breakdownì—ì„œ color_preservation í™•ì¸)
    quality_breakdown = result.get('quality_breakdown', {})
    color_preservation = quality_breakdown.get('color_preservation', 0.8)
    
    if color_preservation > 0.9:
        recommendations.append("ğŸ¨ ìƒ‰ìƒì´ í”¼ë¶€í†¤ê³¼ ì˜ ì–´ìš¸ë¦½ë‹ˆë‹¤!")
    elif color_preservation < 0.7:
        recommendations.append("ğŸŒˆ ë‹¤ë¥¸ ìƒ‰ìƒë„ ì‹œë„í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.")
    
    # ê¸°ë³¸ ì¶”ì²œì´ ì—†ì„ ê²½ìš°
    if not recommendations:
        recommendations.append("âœ¨ ë©‹ì§„ ì„ íƒì…ë‹ˆë‹¤! ì´ ìŠ¤íƒ€ì¼ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.")
    
    return recommendations

@router.post("/virtual-tryon")
async def virtual_tryon_endpoint(
    background_tasks: BackgroundTasks,
    person_image: UploadFile = File(..., description="ì‚¬ìš©ì ì´ë¯¸ì§€"),
    clothing_image: UploadFile = File(..., description="ì˜ë¥˜ ì´ë¯¸ì§€"),
    height: float = Form(170.0, description="í‚¤ (cm)"),
    weight: float = Form(65.0, description="ëª¸ë¬´ê²Œ (kg)"),
    quality_mode: str = Form("balanced", description="í’ˆì§ˆ ëª¨ë“œ"),
    enable_realtime: bool = Form(True, description="ì‹¤ì‹œê°„ ìƒíƒœ ì—…ë°ì´íŠ¸"),
    session_id: Optional[str] = Form(None, description="ì„¸ì…˜ ID"),
    clothing_type: str = Form("shirt", description="ì˜ë¥˜ íƒ€ì…"),
    fabric_type: str = Form("cotton", description="ì›ë‹¨ íƒ€ì…"),
    quality_target: float = Form(0.8, description="í’ˆì§ˆ ëª©í‘œ"),
    save_intermediate: bool = Form(False, description="ì¤‘ê°„ ê²°ê³¼ ì €ì¥"),
    enable_auto_retry: bool = Form(True, description="ìë™ ì¬ì‹œë„")
):
    """8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ê°€ìƒ í”¼íŒ… ì‹¤í–‰"""
    
    # íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸
    if not pipeline_manager:
        raise HTTPException(
            status_code=503,
            detail="íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        )
    
    if not pipeline_manager.is_initialized:
        try:
            init_success = await pipeline_manager.initialize()
            if not init_success:
                raise HTTPException(
                    status_code=503,
                    detail="íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"
                )
        except Exception as e:
            raise HTTPException(
                status_code=503,
                detail=f"íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
            )
    
    process_id = session_id or f"tryon_{uuid.uuid4().hex[:12]}"
    start_time = time.time()
    
    try:
        # ì…ë ¥ íŒŒì¼ ê²€ì¦
        await validate_upload_files(person_image, clothing_image)
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        person_pil = await load_image_from_upload(person_image)
        clothing_pil = await load_image_from_upload(clothing_image)
        
        # ì‹¤ì‹œê°„ ìƒíƒœ ì½œë°± ì„¤ì •
        progress_callback = None
        if enable_realtime and WEBSOCKET_AVAILABLE:
            progress_callback = create_progress_callback(process_id)
            
            # ì‹œì‘ ì•Œë¦¼
            await ws_manager.broadcast_to_session({
                "type": "pipeline_progress",
                "session_id": process_id,
                "data": {
                    "step_id": 0,
                    "step_name": "ì‹œì‘",
                    "progress": 0,
                    "message": "ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...",
                    "status": "processing",
                    "constructor_pattern": "optimal"
                },
                "timestamp": time.time()
            }, process_id)
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = await pipeline_manager.process_complete_virtual_fitting(
            person_image=person_pil,
            clothing_image=clothing_pil,
            body_measurements={
                'height': height,
                'weight': weight,
                'estimated_chest': height * 0.55,
                'estimated_waist': height * 0.47,
                'estimated_hip': height * 0.58,
                'bmi': weight / ((height/100) ** 2)
            },
            clothing_type=clothing_type,
            fabric_type=fabric_type,
            style_preferences={
                'quality_mode': quality_mode,
                'preferred_fit': 'regular'
            },
            quality_target=quality_target,
            progress_callback=progress_callback,
            save_intermediate=save_intermediate,
            enable_auto_retry=enable_auto_retry
        )
        
        processing_time = time.time() - start_time
        
        # ì™„ë£Œ ì•Œë¦¼
        if enable_realtime and WEBSOCKET_AVAILABLE and result.get("success"):
            await ws_manager.broadcast_to_session({
                "type": "completed",
                "session_id": process_id,
                "data": {
                    "processing_time": processing_time,
                    "quality_score": result.get("final_quality_score", 0.8),
                    "constructor_pattern": "optimal"
                },
                "timestamp": time.time()
            }, process_id)
        
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
        fitted_image_b64 = None
        if "result_image" in result:
            if isinstance(result["result_image"], Image.Image):
                fitted_image_b64 = pil_to_base64(result["result_image"])
            else:
                fitted_image_b64 = result["result_image"]
        elif "fitted_image" in result:
            fitted_image_b64 = result["fitted_image"]
        
        # ê°œì„ ëœ ì¶”ì²œ ìƒì„± ì‚¬ìš©
        recommendations = generate_enhanced_recommendations(
            result, body_measurements, clothing_type
        )
        
        # ì‘ë‹µ êµ¬ì„± - ì›ë³¸ê³¼ ì™„ì „ í˜¸í™˜ë˜ëŠ” í˜•ì‹
        response_data = {
            **result,
            "process_id": process_id,
            "constructor_pattern": "optimal",
            
            # í•µì‹¬ ê²°ê³¼ (ì›ë³¸ í˜¸í™˜)
            "fitted_image": fitted_image_b64,
            "fitted_image_url": None,  # URL ë°©ì‹ì€ ë³„ë„ êµ¬í˜„ í•„ìš”
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ (ëª¨ë“  ë³€í˜• ì§€ì›)
            "confidence": result.get("final_quality_score", result.get("confidence", 0.85)),
            "fit_score": result.get("final_quality_score", result.get("fit_score", 0.8)),
            "quality_score": result.get("final_quality_score", result.get("quality_score", 0.82)),
            "quality_grade": result.get("quality_grade", "Good"),
            
            # ì›ë³¸ ìŠ¤íƒ€ì¼ ì¶”ì²œ
            "recommendations": recommendations,
            
            # ì¸¡ì •ê°’ ë° ë¶„ì„
            "measurements": result.get("body_measurements", {
                "height": height,
                "weight": weight,
                "chest": height * 0.55,
                "waist": height * 0.47,
                "hip": height * 0.58,
                "bmi": weight / ((height/100) ** 2)
            }),
            
            "clothing_analysis": result.get("clothing_analysis", {
                "category": clothing_type,
                "style": "casual",
                "dominant_color": [120, 150, 180],
                "material": fabric_type,
                "confidence": result.get("final_quality_score", 0.85)
            }),
            
            # í’ˆì§ˆ ë¶„ì„
            "quality_analysis": result.get("quality_breakdown", {
                "overall_quality": result.get("final_quality_score", 0.8),
                "fit_accuracy": 0.85,
                "color_preservation": 0.90,
                "boundary_naturalness": 0.82,
                "texture_consistency": 0.88
            }),
            
            # ì²˜ë¦¬ ì •ë³´
            "processing_info": {
                "total_steps": len(result.get("step_results_summary", {})),
                "successful_steps": sum(
                    1 for success in result.get("step_results_summary", {}).values() 
                    if success
                ),
                "device_used": result.get("device_used", pipeline_manager.device),
                "constructor_pattern": "optimal",
                "active_connections": len(active_connections),  # ì›ë³¸ í˜¸í™˜
                "pipeline_status": "ready"  # ì›ë³¸ í˜¸í™˜
            }
        }

def pil_to_base64(image: Image.Image) -> str:
    """PIL ì´ë¯¸ì§€ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜"""
    buffer = io.BytesIO()
    image.save(buffer, format='PNG')
    buffer.seek(0)
    return base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
        background_tasks.add_task(update_processing_stats, result)
        
        if SCHEMAS_AVAILABLE:
            return VirtualTryOnResponse(**response_data)
        else:
            return response_data
        
    except Exception as e:
        error_msg = f"ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
        logger.error(error_msg)
        
        # ì—ëŸ¬ ì•Œë¦¼
        if enable_realtime and WEBSOCKET_AVAILABLE:
            await ws_manager.broadcast_to_session({
                "type": "error",
                "session_id": process_id,
                "message": error_msg,
                "constructor_pattern": "optimal",
                "timestamp": time.time()
            }, process_id)
        
        raise HTTPException(status_code=500, detail=error_msg)

@router.get("/status")
async def get_pipeline_status():
    """íŒŒì´í”„ë¼ì¸ í˜„ì¬ ìƒíƒœ ì¡°íšŒ"""
    try:
        if not pipeline_manager:
            status_data = {
                "initialized": False,
                "device": "unknown",
                "constructor_pattern": "optimal",
                "message": "íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €ê°€ ì—†ìŠµë‹ˆë‹¤"
            }
        else:
            status_data = await pipeline_manager.get_pipeline_status()
        
        if SCHEMAS_AVAILABLE:
            return PipelineStatusResponse(**status_data)
        else:
            return status_data
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/initialize")
async def initialize_pipeline():
    """íŒŒì´í”„ë¼ì¸ ìˆ˜ë™ ì´ˆê¸°í™”"""
    global pipeline_manager
    
    try:
        if not pipeline_manager:
            pipeline_manager = create_pipeline_manager(
                mode=PipelineMode.PRODUCTION,
                device=None,
                device_type="auto",
                memory_gb=16.0,
                optimization_enabled=True,
                quality_level="balanced"
            )
            set_pipeline_manager(pipeline_manager)
        
        if pipeline_manager.is_initialized:
            return {
                "message": "íŒŒì´í”„ë¼ì¸ì´ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤",
                "initialized": True,
                "constructor_pattern": "optimal"
            }
        
        success = await pipeline_manager.initialize()
        
        return {
            "message": "íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ" if success else "íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨",
            "initialized": success,
            "constructor_pattern": "optimal"
        }
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ìˆ˜ë™ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/warmup")
async def warmup_pipeline():
    """íŒŒì´í”„ë¼ì¸ ì›œì—… ì‹¤í–‰"""
    if not pipeline_manager or not pipeline_manager.is_initialized:
        raise HTTPException(
            status_code=503,
            detail="íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        )
    
    try:
        success = await pipeline_manager.warmup()
        return {
            "message": "íŒŒì´í”„ë¼ì¸ ì›œì—… ì™„ë£Œ" if success else "íŒŒì´í”„ë¼ì¸ ì›œì—… ì‹¤íŒ¨",
            "success": success,
            "constructor_pattern": "optimal"
        }
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì›œì—… ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/memory")
async def get_memory_status():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
    try:
        if MEMORY_MANAGER_AVAILABLE:
            result = optimize_memory_usage(device="auto", aggressive=False)
            return {
                "memory_info": result,
                "constructor_pattern": "optimal",
                "timestamp": time.time()
            }
        else:
            return {
                "memory_info": {"status": "memory_manager_not_available"},
                "constructor_pattern": "optimal",
                "timestamp": time.time()
            }
        
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/cleanup")
async def cleanup_memory():
    """ë©”ëª¨ë¦¬ ìˆ˜ë™ ì •ë¦¬"""
    try:
        cleanup_results = []
        
        if MEMORY_MANAGER_AVAILABLE:
            result = optimize_memory_usage(device="auto", aggressive=True)
            cleanup_results.append(f"ë©”ëª¨ë¦¬ ìµœì í™”: {result.get('success', False)}")
        
        if gpu_config and hasattr(gpu_config, 'cleanup_memory'):
            gpu_config.cleanup_memory()
            cleanup_results.append("GPU ë©”ëª¨ë¦¬ ì •ë¦¬")
        
        return {
            "message": "ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ",
            "cleaned_components": cleanup_results,
            "constructor_pattern": "optimal",
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/health")
async def pipeline_health_check():
    """íŒŒì´í”„ë¼ì¸ í—¬ìŠ¤ì²´í¬"""
    health_status = {
        "pipeline_manager": pipeline_manager is not None,
        "gpu_config": gpu_config is not None,
        "initialized": pipeline_manager.is_initialized if pipeline_manager else False,
        "device": getattr(pipeline_manager, 'device', 'unknown') if pipeline_manager else "unknown",
        "constructor_pattern": "optimal",
        "imports": {
            "memory_manager": MEMORY_MANAGER_AVAILABLE,
            "schemas": SCHEMAS_AVAILABLE,
            "websocket": WEBSOCKET_AVAILABLE,
            "gpu_config": GPU_CONFIG_AVAILABLE
        },
        "timestamp": time.time()
    }
    
    # íŒŒì´í”„ë¼ì¸ ìƒì„¸ ì •ë³´
    if pipeline_manager:
        health_status.update({
            "device_type": getattr(pipeline_manager, 'device_type', 'auto'),
            "memory_gb": getattr(pipeline_manager, 'memory_gb', 16.0),
            "is_m3_max": getattr(pipeline_manager, 'is_m3_max', False),
            "optimization_enabled": getattr(pipeline_manager, 'optimization_enabled', True),
            "mode": getattr(pipeline_manager, 'mode', 'production'),
            "steps_loaded": len(getattr(pipeline_manager, 'steps', {}))
        })
    
    # ìƒíƒœ íŒì •
    if health_status["pipeline_manager"] and health_status["initialized"]:
        health_status["status"] = "healthy"
        status_code = 200
    elif health_status["pipeline_manager"]:
        health_status["status"] = "initializing"
        status_code = 202
    else:
        health_status["status"] = "unhealthy"
        status_code = 503
    
    return JSONResponse(content=health_status, status_code=status_code)

# ============================================
# ğŸ”§ í—¬í¼ í•¨ìˆ˜ë“¤
# ============================================

async def validate_upload_files(person_image: UploadFile, clothing_image: UploadFile):
    """ì—…ë¡œë“œ íŒŒì¼ ê²€ì¦"""
    max_size = 10 * 1024 * 1024  # 10MB
    
    if person_image.size and person_image.size > max_size:
        raise HTTPException(status_code=413, detail="ì‚¬ìš©ì ì´ë¯¸ì§€ê°€ 10MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤")
    
    if clothing_image.size and clothing_image.size > max_size:
        raise HTTPException(status_code=413, detail="ì˜ë¥˜ ì´ë¯¸ì§€ê°€ 10MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤")
    
    allowed_types = ["image/jpeg", "image/jpg", "image/png", "image/webp"]
    
    if person_image.content_type not in allowed_types:
        raise HTTPException(status_code=400, detail="ì§€ì›ë˜ì§€ ì•ŠëŠ” ì‚¬ìš©ì ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤")
    
    if clothing_image.content_type not in allowed_types:
        raise HTTPException(status_code=400, detail="ì§€ì›ë˜ì§€ ì•ŠëŠ” ì˜ë¥˜ ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤")

async def load_image_from_upload(upload_file: UploadFile) -> Image.Image:
    """ì—…ë¡œë“œ íŒŒì¼ì—ì„œ PIL ì´ë¯¸ì§€ ë¡œë“œ"""
    try:
        contents = await upload_file.read()
        image = Image.open(io.BytesIO(contents))
        
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        return image
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

async def update_processing_stats(result: Dict[str, Any]):
    """ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)"""
    try:
        processing_time = result.get('total_processing_time', result.get('processing_time', 0))
        quality_score = result.get('final_quality_score', result.get('quality_score', 0))
        
        logger.info(f"ğŸ“Š ì²˜ë¦¬ ì™„ë£Œ - ì‹œê°„: {processing_time:.2f}ì´ˆ, í’ˆì§ˆ: {quality_score:.2f}")
        
    except Exception as e:
        logger.error(f"í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

# ============================================
# ğŸ›‘ ë¼ìš°í„° ì¢…ë£Œ ì´ë²¤íŠ¸
# ============================================

@router.on_event("shutdown")
async def shutdown_pipeline():
    """íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    global pipeline_manager, gpu_config
    
    try:
        logger.info("ğŸ›‘ íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° ì¢…ë£Œ ì¤‘...")
        
        if pipeline_manager:
            await pipeline_manager.cleanup()
            logger.info("âœ… íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì •ë¦¬ ì™„ë£Œ")
        
        if gpu_config and hasattr(gpu_config, 'cleanup_memory'):
            gpu_config.cleanup_memory()
            logger.info("âœ… GPU ì„¤ì • ì •ë¦¬ ì™„ë£Œ")
        
        logger.info("âœ… íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° ì¢…ë£Œ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

# ============================================
# ğŸ¯ ëª¨ë“ˆ ì •ë³´
# ============================================

logger.info("ğŸ“¡ ìµœì  ìƒì„±ì íŒ¨í„´ íŒŒì´í”„ë¼ì¸ API ë¼ìš°í„° ë¡œë“œ ì™„ë£Œ")
logger.info(f"ğŸ”§ Memory Manager: {'âœ…' if MEMORY_MANAGER_AVAILABLE else 'âŒ'}")
logger.info(f"ğŸ“‹ Schemas: {'âœ…' if SCHEMAS_AVAILABLE else 'âŒ'}")
logger.info(f"ğŸŒ WebSocket: {'âœ…' if WEBSOCKET_AVAILABLE else 'âŒ'}")
logger.info(f"âš™ï¸ GPU Config: {'âœ…' if GPU_CONFIG_AVAILABLE else 'âŒ'}")
logger.info(f"ğŸ¯ Constructor Pattern: âœ… OPTIMAL (MemoryManagerì™€ í†µì¼)")