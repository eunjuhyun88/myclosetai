# app/api/health.py
"""
í—¬ìŠ¤ì²´í¬ API ë¼ìš°í„° - M3 Max ìµœì í™” (ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš©)
ë‹¨ìˆœí•¨ + í¸ì˜ì„± + í™•ì¥ì„± + ì¼ê´€ì„±
"""

import time
import logging
import asyncio
import psutil
from datetime import datetime
from typing import Dict, Any, Optional
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import JSONResponse

# ìµœì  ìƒì„±ì íŒ¨í„´ ê¸°ë°˜ import
try:
    from ..ai_pipeline.utils import get_memory_manager, get_model_loader
    UTILS_AVAILABLE = True
except ImportError:
    UTILS_AVAILABLE = False

try:
    from ..core.gpu_config import get_device, get_device_info
    GPU_CONFIG_AVAILABLE = True
except ImportError:
    GPU_CONFIG_AVAILABLE = False

class OptimalRouterConstructor:
    """
    ğŸ¯ ìµœì í™”ëœ ë¼ìš°í„° ìƒì„±ì íŒ¨í„´
    ëª¨ë“  ë¼ìš°í„°ì˜ í†µì¼ëœ ê¸°ë³¸ í´ë˜ìŠ¤
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        # 1. ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._auto_detect_device(device)
        
        # 2. ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.router_name = self.__class__.__name__
        self.logger = logging.getLogger(f"api.{self.router_name}")
        
        # 3. í‘œì¤€ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        self.device_type = kwargs.get('device_type', 'auto')
        self.memory_gb = kwargs.get('memory_gb', 16.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # 4. ë¼ìš°í„°ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„°
        self.enable_detailed_health = kwargs.get('enable_detailed_health', True)
        self.enable_system_monitoring = kwargs.get('enable_system_monitoring', True)
        self.cache_duration = kwargs.get('cache_duration', 30.0)  # 30ì´ˆ
        
        # 5. ìƒíƒœ ì´ˆê¸°í™”
        self.is_initialized = False
        self._last_check_time = 0
        self._cached_health_data = None
        
        self.logger.info(f"ğŸ¥ í—¬ìŠ¤ì²´í¬ ë¼ìš°í„° ì´ˆê¸°í™” - {self.device}")

    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device:
            return preferred_device

        if GPU_CONFIG_AVAILABLE:
            try:
                return get_device()
            except:
                pass

        try:
            import torch
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'

    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ìë™ ê°ì§€"""
        try:
            import platform
            import subprocess

            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False

class HealthRouter(OptimalRouterConstructor):
    """
    ğŸ M3 Max ìµœì í™” í—¬ìŠ¤ì²´í¬ ë¼ìš°í„°
    ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš©
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """
        âœ… ìµœì  ìƒì„±ì - í—¬ìŠ¤ì²´í¬ ë¼ìš°í„° íŠ¹í™”

        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (None=ìë™ê°ì§€, 'cpu', 'cuda', 'mps')
            config: í—¬ìŠ¤ì²´í¬ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            **kwargs: í™•ì¥ íŒŒë¼ë¯¸í„°ë“¤
                - enable_detailed_health: bool = True  # ìƒì„¸ í—¬ìŠ¤ì²´í¬
                - enable_system_monitoring: bool = True  # ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
                - cache_duration: float = 30.0  # ìºì‹œ ì§€ì† ì‹œê°„
                - include_ai_pipeline: bool = True  # AI íŒŒì´í”„ë¼ì¸ ìƒíƒœ í¬í•¨
                - include_models: bool = True  # ëª¨ë¸ ìƒíƒœ í¬í•¨
                - include_memory: bool = True  # ë©”ëª¨ë¦¬ ìƒíƒœ í¬í•¨
        """
        super().__init__(device=device, config=config, **kwargs)
        
        # í—¬ìŠ¤ì²´í¬ íŠ¹í™” ì„¤ì •
        self.include_ai_pipeline = kwargs.get('include_ai_pipeline', True)
        self.include_models = kwargs.get('include_models', True)
        self.include_memory = kwargs.get('include_memory', True)
        
        # í—¬ìŠ¤ì²´í¬ í†µê³„
        self._health_check_count = 0
        self._total_response_time = 0.0
        
        # FastAPI ë¼ìš°í„° ìƒì„±
        self.router = APIRouter()
        self._setup_routes()
        
        # ì´ˆê¸°í™” ì™„ë£Œ
        self.is_initialized = True

    def _setup_routes(self):
        """ë¼ìš°í„° ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •"""
        
        @self.router.get("/")
        async def basic_health_check():
            """ê¸°ë³¸ í—¬ìŠ¤ì²´í¬"""
            return await self.get_basic_health()
        
        @self.router.get("/detailed")
        async def detailed_health_check():
            """ìƒì„¸ í—¬ìŠ¤ì²´í¬"""
            return await self.get_detailed_health()
        
        @self.router.get("/system")
        async def system_health_check():
            """ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬ (M3 Max ìµœì í™”)"""
            return await self.get_system_health()
        
        @self.router.get("/ai-pipeline")
        async def ai_pipeline_health():
            """AI íŒŒì´í”„ë¼ì¸ í—¬ìŠ¤ì²´í¬"""
            return await self.get_ai_pipeline_health()
        
        @self.router.get("/models")
        async def models_health():
            """ëª¨ë¸ ìƒíƒœ í—¬ìŠ¤ì²´í¬"""
            return await self.get_models_health()
        
        @self.router.get("/memory")
        async def memory_health():
            """ë©”ëª¨ë¦¬ ìƒíƒœ í—¬ìŠ¤ì²´í¬"""
            return await self.get_memory_health()
        
        @self.router.get("/reset-cache")
        async def reset_health_cache():
            """í—¬ìŠ¤ì²´í¬ ìºì‹œ ë¦¬ì…‹"""
            return await self.reset_cache()

    async def get_basic_health(self) -> Dict[str, Any]:
        """ê¸°ë³¸ í—¬ìŠ¤ì²´í¬"""
        try:
            start_time = time.time()
            
            # ìºì‹œ í™•ì¸
            if self._should_use_cache():
                response_time = time.time() - start_time
                self._update_stats(response_time)
                return self._cached_health_data
            
            # ê¸°ë³¸ ìƒíƒœ ì •ë³´
            health_data = {
                "status": "healthy",
                "timestamp": datetime.now().isoformat(),
                "version": "3.0.0-m3max-optimized",
                "optimal_constructor_pattern": True,
                "device": self.device,
                "m3_max_optimized": self.is_m3_max,
                "uptime_seconds": self._get_uptime(),
                "health_check_count": self._health_check_count + 1
            }
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            response_time = time.time() - start_time
            health_data["response_time"] = round(response_time, 4)
            
            # ìºì‹œ ì—…ë°ì´íŠ¸
            self._update_cache(health_data)
            self._update_stats(response_time)
            
            return health_data
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def get_detailed_health(self) -> Dict[str, Any]:
        """ìƒì„¸ í—¬ìŠ¤ì²´í¬"""
        try:
            start_time = time.time()
            
            # ê¸°ë³¸ ì •ë³´
            basic_health = await self.get_basic_health()
            
            # ìƒì„¸ ì •ë³´ ì¶”ê°€
            detailed_info = {
                **basic_health,
                "detailed": True,
                "optimal_constructor_features": {
                    "auto_device_detection": True,
                    "unified_interface": True,
                    "extensible_config": True,
                    "m3_max_optimization": self.is_m3_max
                },
                "system_info": await self._get_system_info(),
                "performance_metrics": {
                    "total_health_checks": self._health_check_count,
                    "average_response_time": self._get_average_response_time(),
                    "cache_hit_rate": self._get_cache_hit_rate()
                }
            }
            
            # ì„ íƒì  ì •ë³´ ì¶”ê°€
            if self.include_ai_pipeline:
                detailed_info["ai_pipeline"] = await self._get_ai_pipeline_status()
            
            if self.include_models:
                detailed_info["models"] = await self._get_models_status()
            
            if self.include_memory:
                detailed_info["memory"] = await self._get_memory_status()
            
            # ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            detailed_info["processing_time"] = round(processing_time, 4)
            
            return detailed_info
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒì„¸ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def get_system_health(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬ (M3 Max ìµœì í™”)"""
        try:
            start_time = time.time()
            
            system_health = {
                "system_status": "healthy",
                "timestamp": datetime.now().isoformat(),
                "system_info": await self._get_detailed_system_info(),
                "resource_usage": await self._get_resource_usage(),
                "device_specific": await self._get_device_specific_info()
            }
            
            # M3 Max íŠ¹í™” ì •ë³´
            if self.is_m3_max:
                system_health["m3_max_specific"] = await self._get_m3_max_info()
            
            processing_time = time.time() - start_time
            system_health["processing_time"] = round(processing_time, 4)
            
            return system_health
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def get_ai_pipeline_health(self) -> Dict[str, Any]:
        """AI íŒŒì´í”„ë¼ì¸ í—¬ìŠ¤ì²´í¬"""
        try:
            if not self.include_ai_pipeline:
                return {"ai_pipeline": "disabled"}
            
            pipeline_health = {
                "ai_pipeline_status": "checking",
                "timestamp": datetime.now().isoformat(),
                "utils_available": UTILS_AVAILABLE,
                "components": {}
            }
            
            # ìœ í‹¸ë¦¬í‹° ìƒíƒœ í™•ì¸
            if UTILS_AVAILABLE:
                # Memory Manager ìƒíƒœ
                memory_manager = get_memory_manager()
                if memory_manager:
                    pipeline_health["components"]["memory_manager"] = {
                        "status": "available",
                        "device": memory_manager.device,
                        "is_m3_max": memory_manager.is_m3_max
                    }
                else:
                    pipeline_health["components"]["memory_manager"] = {"status": "unavailable"}
                
                # Model Loader ìƒíƒœ
                model_loader = get_model_loader()
                if model_loader:
                    pipeline_health["components"]["model_loader"] = {
                        "status": "available",
                        "device": model_loader.device,
                        "loaded_models": len(model_loader._loaded_models) if hasattr(model_loader, '_loaded_models') else 0
                    }
                else:
                    pipeline_health["components"]["model_loader"] = {"status": "unavailable"}
            
            # ì „ì²´ ìƒíƒœ ê²°ì •
            available_components = sum(1 for comp in pipeline_health["components"].values() if comp.get("status") == "available")
            total_components = len(pipeline_health["components"])
            
            if available_components == total_components and total_components > 0:
                pipeline_health["ai_pipeline_status"] = "healthy"
            elif available_components > 0:
                pipeline_health["ai_pipeline_status"] = "partially_healthy"
            else:
                pipeline_health["ai_pipeline_status"] = "unhealthy"
            
            return pipeline_health
            
        except Exception as e:
            self.logger.error(f"âŒ AI íŒŒì´í”„ë¼ì¸ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return {
                "ai_pipeline_status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def get_models_health(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ í—¬ìŠ¤ì²´í¬"""
        try:
            if not self.include_models:
                return {"models": "disabled"}
            
            models_health = {
                "models_status": "checking",
                "timestamp": datetime.now().isoformat(),
                "model_info": {}
            }
            
            if UTILS_AVAILABLE:
                model_loader = get_model_loader()
                if model_loader:
                    # ë¡œë“œëœ ëª¨ë¸ ì •ë³´
                    loaded_models = getattr(model_loader, '_loaded_models', {})
                    model_registry = getattr(model_loader, '_model_registry', {})
                    
                    models_health["model_info"] = {
                        "total_registered": len(model_registry),
                        "currently_loaded": len(loaded_models),
                        "device": model_loader.device,
                        "use_fp16": getattr(model_loader, 'use_fp16', False),
                        "coreml_optimization": getattr(model_loader, 'coreml_optimization', False)
                    }
                    
                    if loaded_models:
                        models_health["loaded_models"] = list(loaded_models.keys())
                    
                    models_health["models_status"] = "healthy"
                else:
                    models_health["models_status"] = "unavailable"
            else:
                models_health["models_status"] = "utils_unavailable"
            
            return models_health
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return {
                "models_status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def get_memory_health(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìƒíƒœ í—¬ìŠ¤ì²´í¬"""
        try:
            if not self.include_memory:
                return {"memory": "disabled"}
            
            memory_health = {
                "memory_status": "checking",
                "timestamp": datetime.now().isoformat(),
                "system_memory": self._get_system_memory_info()
            }
            
            if UTILS_AVAILABLE:
                memory_manager = get_memory_manager()
                if memory_manager:
                    # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ìƒíƒœ
                    memory_stats = memory_manager.get_memory_stats()
                    memory_health["memory_manager"] = {
                        "status": "available",
                        "device": memory_manager.device,
                        "memory_limit_gb": memory_manager.memory_limit_gb,
                        "is_m3_max": memory_manager.is_m3_max,
                        "stats": memory_stats
                    }
                    
                    # ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸
                    pressure_info = memory_manager.check_memory_pressure()
                    memory_health["memory_pressure"] = pressure_info
                    
                    # ìƒíƒœ ê²°ì •
                    if pressure_info.get("pressure_level") in ["low", "moderate"]:
                        memory_health["memory_status"] = "healthy"
                    elif pressure_info.get("pressure_level") == "high":
                        memory_health["memory_status"] = "warning"
                    else:
                        memory_health["memory_status"] = "critical"
                else:
                    memory_health["memory_status"] = "manager_unavailable"
            else:
                memory_health["memory_status"] = "utils_unavailable"
            
            return memory_health
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return {
                "memory_status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def reset_cache(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ì²´í¬ ìºì‹œ ë¦¬ì…‹"""
        try:
            self._cached_health_data = None
            self._last_check_time = 0
            
            return {
                "success": True,
                "message": "í—¬ìŠ¤ì²´í¬ ìºì‹œê°€ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ë¦¬ì…‹ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    def _should_use_cache(self) -> bool:
        """ìºì‹œ ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
        if not self._cached_health_data:
            return False
        
        elapsed = time.time() - self._last_check_time
        return elapsed < self.cache_duration

    def _update_cache(self, data: Dict[str, Any]):
        """ìºì‹œ ì—…ë°ì´íŠ¸"""
        self._cached_health_data = data
        self._last_check_time = time.time()

    def _update_stats(self, response_time: float):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self._health_check_count += 1
        self._total_response_time += response_time

    def _get_average_response_time(self) -> float:
        """í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°"""
        if self._health_check_count == 0:
            return 0.0
        return round(self._total_response_time / self._health_check_count, 4)

    def _get_cache_hit_rate(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚° (ê°„ë‹¨í•œ êµ¬í˜„)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ìºì‹œ í†µê³„ í•„ìš”
        return 0.85 if self._cached_health_data else 0.0

    def _get_uptime(self) -> float:
        """ê°€ë™ ì‹œê°„ ê³„ì‚°"""
        try:
            import psutil
            return time.time() - psutil.boot_time()
        except:
            return 0.0

    async def _get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
        try:
            import platform
            
            return {
                "platform": platform.system(),
                "machine": platform.machine(),
                "processor": platform.processor(),
                "python_version": platform.python_version(),
                "device": self.device,
                "device_type": self.device_type,
                "is_m3_max": self.is_m3_max
            }
        except Exception as e:
            return {"error": str(e)}

    async def _get_detailed_system_info(self) -> Dict[str, Any]:
        """ìƒì„¸ ì‹œìŠ¤í…œ ì •ë³´"""
        basic_info = await self._get_system_info()
        
        try:
            import platform
            
            detailed = {
                **basic_info,
                "platform_release": platform.release(),
                "platform_version": platform.version(),
                "architecture": platform.architecture(),
                "hostname": platform.node()
            }
            
            return detailed
        except Exception as e:
            return {**basic_info, "detailed_error": str(e)}

    async def _get_resource_usage(self) -> Dict[str, Any]:
        """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent(interval=0.1)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            memory = psutil.virtual_memory()
            
            # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
            disk = psutil.disk_usage('/')
            
            return {
                "cpu": {
                    "usage_percent": cpu_percent,
                    "count": psutil.cpu_count(),
                    "freq": psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None
                },
                "memory": {
                    "total_gb": round(memory.total / (1024**3), 2),
                    "available_gb": round(memory.available / (1024**3), 2),
                    "used_gb": round(memory.used / (1024**3), 2),
                    "usage_percent": memory.percent
                },
                "disk": {
                    "total_gb": round(disk.total / (1024**3), 2),
                    "used_gb": round(disk.used / (1024**3), 2),
                    "free_gb": round(disk.free / (1024**3), 2),
                    "usage_percent": round((disk.used / disk.total) * 100, 1)
                }
            }
        except Exception as e:
            return {"error": str(e)}

    async def _get_device_specific_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ë³„ íŠ¹í™” ì •ë³´"""
        device_info = {"device": self.device}
        
        try:
            if GPU_CONFIG_AVAILABLE:
                gpu_info = get_device_info()
                device_info.update(gpu_info)
            
            # PyTorch ì •ë³´
            try:
                import torch
                device_info["torch"] = {
                    "version": torch.__version__,
                    "mps_available": torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False,
                    "cuda_available": torch.cuda.is_available()
                }
                
                if self.device == "cuda" and torch.cuda.is_available():
                    device_info["cuda"] = {
                        "device_count": torch.cuda.device_count(),
                        "current_device": torch.cuda.current_device(),
                        "device_name": torch.cuda.get_device_name()
                    }
                
            except ImportError:
                device_info["torch"] = "not_available"
            
        except Exception as e:
            device_info["error"] = str(e)
        
        return device_info

    async def _get_m3_max_info(self) -> Dict[str, Any]:
        """M3 Max íŠ¹í™” ì •ë³´"""
        if not self.is_m3_max:
            return {"m3_max": False}
        
        try:
            m3_info = {
                "neural_engine": True,
                "unified_memory": True,
                "memory_bandwidth_gbps": 400,
                "gpu_cores": "40-core",
                "cpu_cores": "16-core (12P + 4E)",
                "optimization_features": {
                    "mps_backend": self.device == "mps",
                    "coreml_support": True,
                    "metal_performance_shaders": True
                }
            }
            
            # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì—ì„œ M3 Max ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            if UTILS_AVAILABLE:
                memory_manager = get_memory_manager()
                if memory_manager and memory_manager.is_m3_max:
                    memory_stats = memory_manager.get_memory_stats()
                    m3_info["memory_optimization"] = memory_stats.get("m3_max", {})
            
            return m3_info
            
        except Exception as e:
            return {"m3_max": True, "error": str(e)}

    def _get_system_memory_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´"""
        try:
            memory = psutil.virtual_memory()
            return {
                "total_gb": round(memory.total / (1024**3), 2),
                "available_gb": round(memory.available / (1024**3), 2),
                "used_gb": round(memory.used / (1024**3), 2),
                "usage_percent": memory.percent,
                "free_gb": round(memory.free / (1024**3), 2)
            }
        except Exception as e:
            return {"error": str(e)}

    async def _get_ai_pipeline_status(self) -> Dict[str, Any]:
        """AI íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ"""
        if not UTILS_AVAILABLE:
            return {"status": "utils_unavailable"}
        
        # ê°„ë‹¨í•œ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì²´í¬
        return {
            "utils_available": True,
            "memory_manager_available": get_memory_manager() is not None,
            "model_loader_available": get_model_loader() is not None
        }

    async def _get_models_status(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        if not UTILS_AVAILABLE:
            return {"status": "utils_unavailable"}
        
        model_loader = get_model_loader()
        if not model_loader:
            return {"status": "model_loader_unavailable"}
        
        return {
            "status": "available",
            "device": model_loader.device,
            "loaded_count": len(getattr(model_loader, '_loaded_models', {}))
        }

    async def _get_memory_status(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
        if not UTILS_AVAILABLE:
            return {"status": "utils_unavailable"}
        
        memory_manager = get_memory_manager()
        if not memory_manager:
            return {"status": "memory_manager_unavailable"}
        
        return {
            "status": "available",
            "device": memory_manager.device,
            "memory_limit_gb": memory_manager.memory_limit_gb
        }

# í—¬ìŠ¤ì²´í¬ ë¼ìš°í„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìµœì  ìƒì„±ì íŒ¨í„´)
health_router = HealthRouter()
router = health_router.router

# í¸ì˜ í•¨ìˆ˜ë“¤ (í•˜ìœ„ í˜¸í™˜ì„±)
def create_health_router(
    device: Optional[str] = None,
    enable_detailed_health: bool = True,
    **kwargs
) -> HealthRouter:
    """í—¬ìŠ¤ì²´í¬ ë¼ìš°í„° ìƒì„± (í•˜ìœ„ í˜¸í™˜)"""
    return HealthRouter(
        device=device,
        enable_detailed_health=enable_detailed_health,
        **kwargs
    )

# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
__all__ = [
    'router',
    'HealthRouter',
    'OptimalRouterConstructor',
    'create_health_router'
]