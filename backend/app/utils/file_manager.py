"""
MyCloset AI - í†µí•© íŒŒì¼ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹° v3.0
================================================
âœ… ê¸°ì¡´ FileManager ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€
âœ… ìŠ¤ë§ˆíŠ¸ ë°±ì—… ì •ì±… í†µí•©
âœ… ë°±ì—… íŒŒì¼ ìë™ ì •ë¦¬
âœ… M3 Max ìµœì í™” ìœ ì§€
âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›
âœ… ê¸°ì¡´ API 100% í˜¸í™˜
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… .bak íŒŒì¼ ìƒì„± ë°©ì§€ ë° ì •ë¦¬
"""

import os
import uuid
import aiofiles
import asyncio
import logging
import tempfile
import shutil
import hashlib
from pathlib import Path
from typing import Optional, Union, List, Dict, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
from fastapi import UploadFile, HTTPException
from PIL import Image
import io

logger = logging.getLogger(__name__)

# =============================================================================
# ğŸ”¥ ë°±ì—… ì •ì±… ì„¤ì • (ìŠ¤ë§ˆíŠ¸ ë°±ì—… ì‹œìŠ¤í…œ)
# =============================================================================

class BackupPolicy(Enum):
    """ë°±ì—… ì •ì±… ì •ì˜"""
    NONE = "none"              # ë°±ì—… ì•ˆí•¨ (ì¶”ì²œ - .bak íŒŒì¼ ìƒì„± ë°©ì§€)
    SMART = "smart"            # ìŠ¤ë§ˆíŠ¸ ë°±ì—… (ì¤‘ìš”í•œ ê²ƒë§Œ)
    TIMESTAMP = "timestamp"    # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜
    VERSION = "version"        # ë²„ì „ ê¸°ë°˜
    SESSION = "session"        # ì„¸ì…˜ ê¸°ë°˜

@dataclass
class BackupConfig:
    """ë°±ì—… ì„¤ì •"""
    policy: BackupPolicy = BackupPolicy.NONE  # ê¸°ë³¸ê°’: ë°±ì—… ì•ˆí•¨ (.bak ë°©ì§€)
    max_backups_per_file: int = 2  # ìµœëŒ€ ë°±ì—… ìˆ˜ ì œí•œ
    max_backup_age_days: int = 3   # ë°±ì—… ë³´ê´€ ê¸°ê°„ ë‹¨ì¶•
    auto_cleanup: bool = True      # ìë™ ì •ë¦¬ í™œì„±í™”
    cleanup_interval_hours: int = 12  # ì •ë¦¬ ì£¼ê¸° ë‹¨ì¶•
    backup_important_only: bool = True  # ì¤‘ìš”í•œ íŒŒì¼ë§Œ ë°±ì—…
    preserve_original: bool = True      # ì›ë³¸ ë³´ì¡´
    use_hidden_backup_dir: bool = True  # ìˆ¨ê¹€ ë””ë ‰í† ë¦¬ ì‚¬ìš©

# ì„¤ì • ìƒìˆ˜ë“¤ (ê¸°ì¡´ ìœ ì§€)
ALLOWED_EXTENSIONS = ["jpg", "jpeg", "png", "webp", "bmp", "tiff"]
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB (M3 Max ì²˜ë¦¬ ëŠ¥ë ¥ ê³ ë ¤)
MIN_IMAGE_SIZE = (100, 100)
MAX_IMAGE_SIZE = (4096, 4096)  # M3 Max ê³ í•´ìƒë„ ì²˜ë¦¬ ê°€ëŠ¥
ALLOWED_MIME_TYPES = [
    "image/jpeg", "image/jpg", "image/png", 
    "image/webp", "image/bmp", "image/tiff"
]

# ì¤‘ìš”í•œ íŒŒì¼ íŒ¨í„´ë“¤ (ë°±ì—…ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
IMPORTANT_FILE_PATTERNS = {
    "config_files": ["*config*.yaml", "*config*.yml", "main.py", "__init__.py"],
    "requirements": ["requirements*.txt", "environment*.yml"],
    "critical_scripts": ["main.py", "app.py", "server.py"],
}

# ë°±ì—…í•˜ì§€ ì•Šì„ íŒŒì¼ íŒ¨í„´ë“¤ (.bak ë°©ì§€ìš©)
EXCLUDE_BACKUP_PATTERNS = [
    "*.pyc", "*.pyo", "__pycache__/*", "*.log", "*.tmp", "*.temp",
    "*.bak", "*.backup", "*~", ".DS_Store", "Thumbs.db",
    "*.pid", "*.lock", "*.cache"
]

class UnifiedFileManager:
    """
    í†µí•© íŒŒì¼ ê´€ë¦¬ì - ê¸°ì¡´ ê¸°ëŠ¥ + ìŠ¤ë§ˆíŠ¸ ë°±ì—…
    âœ… ëª¨ë“  ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€
    âœ… ìŠ¤ë§ˆíŠ¸ ë°±ì—… ì¶”ê°€
    âœ… .bak íŒŒì¼ ìƒì„± ë°©ì§€
    âœ… ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ
    """
    def __init__(self, base_dir: Optional[str] = None, backup_config: Optional[BackupConfig] = None):
        """ì´ˆê¸°í™” - ê¸°ì¡´ FileManagerì™€ ì™„ì „ í˜¸í™˜ + backend/backend ë¬¸ì œ í•´ê²°"""
        
        # ğŸ”¥ ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì„¤ì • - backend/backend ë¬¸ì œ ì™„ì „ í•´ê²°
        if base_dir is None:            
            # âœ… í•´ê²°ëœ ì½”ë“œ: íŒŒì¼ ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ backend ê²½ë¡œ ê³„ì‚°
            current_file = Path(__file__).absolute()  # /path/to/backend/app/utils/file_manager.py
            backend_root = current_file.parent.parent.parent  # /path/to/backend/
            base_dir = str(backend_root)
            
            print(f"ğŸ”§ UnifiedFileManager ìë™ ê²½ë¡œ ì„¤ì •: {base_dir}")
        
        self.base_dir = Path(base_dir)
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ static í•˜ìœ„ë¡œ ì •ë¦¬ (ë” ê¹”ë”í•œ êµ¬ì¡°)
        self.upload_dir = self.base_dir / "static" / "uploads"
        self.results_dir = self.base_dir / "static" / "results" 
        self.temp_dir = self.base_dir / "temp"
        self.static_dir = self.base_dir / "static"
        
        # ë°±ì—… ì„¤ì • (ê¸°ë³¸ê°’: ë°±ì—… ì•ˆí•¨)
        self.backup_config = backup_config or BackupConfig()
        
        # ë°±ì—… ë””ë ‰í† ë¦¬ (ìˆ¨ê¹€ ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •)
        if self.backup_config.use_hidden_backup_dir:
            self.backup_dir = self.base_dir / ".smart_backups"
        else:
            self.backup_dir = self.base_dir / "backups"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self._ensure_directories()
        
        # M3 Max ìµœì í™” ì„¤ì • (ê¸°ì¡´ ìœ ì§€)
        self.is_m3_max = self._detect_m3_max()
        self.max_concurrent_ops = 8 if self.is_m3_max else 4
        
        # ë°±ì—… ë©”íƒ€ë°ì´í„° ì¶”ì 
        self.backup_metadata: Dict[str, Dict] = {}
        self._last_cleanup = datetime.now()
        
        # ì‹œì‘ ì‹œ ê¸°ì¡´ ë°±ì—… íŒŒì¼ ì •ë¦¬
        asyncio.create_task(self._initial_cleanup())
        
        logger.info(f"ğŸ“ UnifiedFileManager ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   Base ê²½ë¡œ: {self.base_dir}")
        logger.info(f"   Upload ê²½ë¡œ: {self.upload_dir}")  
        logger.info(f"   Results ê²½ë¡œ: {self.results_dir}")
        logger.info(f"   M3 Max: {self.is_m3_max}")
        logger.info(f"   ë°±ì—… ì •ì±…: {self.backup_config.policy.value}")


    # ğŸ¯ í•µì‹¬ ë³€ê²½ì‚¬í•­ ìš”ì•½:
    # 2. uploads â†’ static/uploads (ê¸°ì¡´ í”„ë¡œì íŠ¸ êµ¬ì¡°ì™€ ì¼ì¹˜)
    # 3. results â†’ static/results (ê¸°ì¡´ í”„ë¡œì íŠ¸ êµ¬ì¡°ì™€ ì¼ì¹˜)
    # 4. ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥ìœ¼ë¡œ ê²½ë¡œ í™•ì¸ ê°€ëŠ¥


    def _ensure_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ ìƒì„± (ê¸°ì¡´ + ë°±ì—…)"""
        directories = [
            self.upload_dir, self.results_dir, self.temp_dir, 
            self.static_dir, 
            self.static_dir / "results",
            self.static_dir / "uploads"
        ]
        
        # ë°±ì—… ì •ì±…ì´ NONEì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        if self.backup_config.policy != BackupPolicy.NONE:
            directories.append(self.backup_dir)
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            
            # .gitkeep íŒŒì¼ ìƒì„± (ë¹ˆ ë””ë ‰í† ë¦¬ ë³´ì¡´)
            gitkeep = directory / ".gitkeep"
            if not gitkeep.exists():
                gitkeep.touch()
        
        logger.debug("ğŸ“ ëª¨ë“  ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ")

    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        try:
            import platform
            import subprocess
            
            if platform.system() == 'Darwin':
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=5)
                chip_info = result.stdout.strip()
                return 'M3' in chip_info and ('Max' in chip_info or 'Pro' in chip_info)
        except:
            pass
        return False

    async def _initial_cleanup(self):
        """ì´ˆê¸°í™” ì‹œ ê¸°ì¡´ .bak íŒŒì¼ë“¤ ì •ë¦¬"""
        try:
            await asyncio.sleep(1)  # ì´ˆê¸°í™” ì™„ë£Œ í›„ ì‹¤í–‰
            
            bak_files = list(self.base_dir.rglob("*.bak"))
            backup_files = list(self.base_dir.rglob("*.backup"))
            
            if bak_files or backup_files:
                logger.info(f"ğŸ§¹ ê¸°ì¡´ ë°±ì—… íŒŒì¼ ì •ë¦¬ ì‹œì‘: .bak({len(bak_files)}ê°œ), .backup({len(backup_files)}ê°œ)")
                
                cleaned_count = 0
                for file_path in bak_files + backup_files:
                    try:
                        if file_path.exists() and file_path.is_file():
                            file_path.unlink()
                            cleaned_count += 1
                    except Exception as e:
                        logger.warning(f"âš ï¸ ë°±ì—… íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")
                
                logger.info(f"ğŸ§¹ ê¸°ì¡´ ë°±ì—… íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ ì‚­ì œ")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ì´ˆê¸° ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _should_backup_file(self, file_path: Path) -> bool:
        """íŒŒì¼ì´ ë°±ì—…ì´ í•„ìš”í•œì§€ íŒë‹¨"""
        if self.backup_config.policy == BackupPolicy.NONE:
            return False
        
        # ë°±ì—… ì œì™¸ íŒ¨í„´ í™•ì¸
        file_str = str(file_path).lower()
        for pattern in EXCLUDE_BACKUP_PATTERNS:
            if file_path.match(pattern.lower()):
                return False
        
        if not self.backup_config.backup_important_only:
            return True
        
        # ì¤‘ìš”í•œ íŒŒì¼ë§Œ ë°±ì—…
        for category, patterns in IMPORTANT_FILE_PATTERNS.items():
            for pattern in patterns:
                if file_path.match(pattern.lower()):
                    return True
        
        return False

    async def _create_smart_backup(self, file_path: Path, session_id: Optional[str] = None) -> Optional[Path]:
        """ìŠ¤ë§ˆíŠ¸ ë°±ì—… ìƒì„± (ì¡°ê±´ë¶€)"""
        try:
            if not file_path.exists() or not self._should_backup_file(file_path):
                return None
            
            # ë°±ì—… ê²½ë¡œ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            if session_id:
                backup_name = f"{file_path.stem}_{session_id}_{timestamp}{file_path.suffix}"
            else:
                backup_name = f"{file_path.stem}_{timestamp}{file_path.suffix}"
            
            backup_path = self.backup_dir / backup_name
            
            # ê¸°ì¡´ ë°±ì—… ì •ë¦¬
            await self._cleanup_old_backups_for_file(file_path)
            
            # íŒŒì¼ ë³µì‚¬
            await asyncio.to_thread(shutil.copy2, file_path, backup_path)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            self.backup_metadata[str(backup_path)] = {
                "original_path": str(file_path),
                "created_at": datetime.now().isoformat(),
                "session_id": session_id,
                "file_size": backup_path.stat().st_size
            }
            
            logger.debug(f"ğŸ“ ìŠ¤ë§ˆíŠ¸ ë°±ì—… ìƒì„±: {backup_path}")
            return backup_path
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    async def _cleanup_old_backups_for_file(self, original_path: Path):
        """íŠ¹ì • íŒŒì¼ì˜ ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬"""
        if not self.backup_config.auto_cleanup or self.backup_config.policy == BackupPolicy.NONE:
            return
        
        try:
            pattern = f"{original_path.stem}_*{original_path.suffix}"
            existing_backups = list(self.backup_dir.glob(pattern))
            
            # ê°œìˆ˜ ì œí•œ
            if len(existing_backups) >= self.backup_config.max_backups_per_file:
                existing_backups.sort(key=lambda p: p.stat().st_mtime)
                to_delete = existing_backups[:-self.backup_config.max_backups_per_file + 1]
                
                for backup_file in to_delete:
                    try:
                        backup_file.unlink()
                        if str(backup_file) in self.backup_metadata:
                            del self.backup_metadata[str(backup_file)]
                        logger.debug(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {backup_file}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ ë°±ì—… ì‚­ì œ ì‹¤íŒ¨: {e}")
        except Exception as e:
            logger.warning(f"âš ï¸ ë°±ì—… ì •ë¦¬ ì‹¤íŒ¨: {e}")

    # =============================================================================
    # ğŸ”¥ ê¸°ì¡´ FileManager API - ëª¨ë“  ë©”ì„œë“œ ìœ ì§€ (ë°±ì—… ë¡œì§ í†µí•©)
    # =============================================================================

    @staticmethod
    async def save_upload_file(
        file: UploadFile, 
        directory: Union[str, Path],
        filename: Optional[str] = None,
        max_size: Optional[int] = None
    ) -> str:
        """
        ì—…ë¡œë“œ íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì €ì¥ (ê¸°ì¡´ API ìœ ì§€)
        """
        try:
            # íŒŒì¼ í¬ê¸° ê²€ì¦
            if max_size is None:
                max_size = MAX_FILE_SIZE
            
            if hasattr(file, 'size') and file.size and file.size > max_size:
                raise HTTPException(
                    status_code=413,
                    detail=f"íŒŒì¼ í¬ê¸°ê°€ {max_size // (1024*1024)}MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤"
                )
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            directory = Path(directory)
            directory.mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„±
            if filename is None:
                file_ext = Path(file.filename).suffix if file.filename else ".tmp"
                filename = f"{uuid.uuid4().hex}{file_ext}"
            
            file_path = directory / filename
            
            # íŒŒì¼ ì €ì¥
            async with aiofiles.open(file_path, 'wb') as f:
                content = await file.read()
                
                # ì¶”ê°€ í¬ê¸° ê²€ì¦
                if len(content) > max_size:
                    raise HTTPException(
                        status_code=413,
                        detail=f"íŒŒì¼ í¬ê¸°ê°€ {max_size // (1024*1024)}MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤"
                    )
                
                await f.write(content)
            
            logger.info(f"ğŸ“ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_path}")
            return str(file_path)
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    async def save_session_file(
        self, 
        file: UploadFile, 
        session_id: str, 
        file_type: str = "upload"
    ) -> str:
        """ì„¸ì…˜ë³„ íŒŒì¼ ì €ì¥ (ìŠ¤ë§ˆíŠ¸ ë°±ì—… í†µí•©)"""
        try:
            # íŒŒì¼ ê²€ì¦
            if not self.validate_image(file):
                raise HTTPException(
                    status_code=400,
                    detail="ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤"
                )
            
            # íŒŒì¼ëª… ìƒì„±
            ext = self.get_file_extension(file.filename)
            filename = f"{session_id}_{file_type}_{uuid.uuid4().hex[:8]}.{ext}"
            
            # ì €ì¥ ê²½ë¡œ ê²°ì •
            if file_type in ["person", "clothing", "upload"]:
                save_dir = self.upload_dir
            else:
                save_dir = self.results_dir
            
            file_path = await self.save_upload_file(file, save_dir, filename)
            
            # ìŠ¤ë§ˆíŠ¸ ë°±ì—… (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
            if self.backup_config.policy != BackupPolicy.NONE:
                await self._create_smart_backup(Path(file_path), session_id)
            
            logger.info(f"ğŸ“ ì„¸ì…˜ íŒŒì¼ ì €ì¥: {file_path}")
            return file_path
            
        except Exception as e:
            logger.error(f"âŒ ì„¸ì…˜ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def validate_image(self, file: UploadFile) -> bool:
        """ì´ë¯¸ì§€ íŒŒì¼ ê²€ì¦ (ê¸°ì¡´ API ìœ ì§€)"""
        try:
            # íŒŒì¼ëª… ê²€ì¦
            if not file.filename:
                logger.warning("âš ï¸ íŒŒì¼ëª…ì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # í™•ì¥ì ê²€ì¦
            extension = self.get_file_extension(file.filename)
            if extension not in ALLOWED_EXTENSIONS:
                logger.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” í™•ì¥ì: {extension}")
                return False
            
            # MIME íƒ€ì… ê²€ì¦
            if not file.content_type or file.content_type not in ALLOWED_MIME_TYPES:
                logger.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” MIME íƒ€ì…: {file.content_type}")
                return False
            
            # íŒŒì¼ í¬ê¸° ê²€ì¦
            if hasattr(file, 'size') and file.size:
                if file.size > MAX_FILE_SIZE:
                    logger.warning(f"âš ï¸ íŒŒì¼ í¬ê¸° ì´ˆê³¼: {file.size} bytes")
                    return False
                if file.size < 1024:  # 1KB ë¯¸ë§Œ
                    logger.warning(f"âš ï¸ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤: {file.size} bytes")
                    return False
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    @staticmethod
    def validate_measurements(height: float, weight: float) -> bool:
        """ì‹ ì²´ ì¸¡ì •ê°’ ê²€ì¦ (ê¸°ì¡´ API ìœ ì§€)"""
        try:
            # í‚¤ ê²€ì¦ (cm)
            if not (100 <= height <= 250):
                logger.warning(f"âš ï¸ í‚¤ ë²”ìœ„ ì´ˆê³¼: {height}cm")
                return False
            
            # ì²´ì¤‘ ê²€ì¦ (kg)
            if not (30 <= weight <= 300):
                logger.warning(f"âš ï¸ ì²´ì¤‘ ë²”ìœ„ ì´ˆê³¼: {weight}kg")
                return False
            
            # BMI ê²€ì¦
            bmi = weight / ((height / 100) ** 2)
            if not (10 <= bmi <= 50):
                logger.warning(f"âš ï¸ BMI ë²”ìœ„ ì´ˆê³¼: {bmi}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì¸¡ì •ê°’ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    async def validate_image_content(self, image_bytes: bytes) -> bool:
        """ì´ë¯¸ì§€ ë‚´ìš© ê²€ì¦ (ê¸°ì¡´ API ìœ ì§€)"""
        try:
            image = Image.open(io.BytesIO(image_bytes))
            
            # ì´ë¯¸ì§€ í¬ê¸° ê²€ì¦
            width, height = image.size
            
            # ìµœì†Œ í¬ê¸° ê²€ì¦
            if width < MIN_IMAGE_SIZE[0] or height < MIN_IMAGE_SIZE[1]:
                logger.warning(f"âš ï¸ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤: {width}x{height}")
                return False
            
            # ìµœëŒ€ í¬ê¸° ê²€ì¦ (M3 MaxëŠ” ë” í° ì´ë¯¸ì§€ ì²˜ë¦¬ ê°€ëŠ¥)
            max_width, max_height = MAX_IMAGE_SIZE
            if self.is_m3_max:
                max_width *= 2
                max_height *= 2
            
            if width > max_width or height > max_height:
                logger.warning(f"âš ï¸ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤: {width}x{height}")
                return False
            
            # ì´ë¯¸ì§€ í˜•ì‹ ê²€ì¦
            if image.format not in ['JPEG', 'PNG', 'WEBP', 'BMP', 'TIFF']:
                logger.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {image.format}")
                return False
            
            # ìƒ‰ìƒ ëª¨ë“œ ê²€ì¦
            if image.mode not in ['RGB', 'RGBA', 'L']:
                logger.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ìƒ‰ìƒ ëª¨ë“œ: {image.mode}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ë‚´ìš© ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    @staticmethod
    async def validate_image_content_static(image_bytes: bytes) -> bool:
        """ì •ì  ë©”ì„œë“œ ë²„ì „ - ê¸°ì¡´ í•¨ìˆ˜ì™€ ì™„ì „ í˜¸í™˜"""
        try:
            image = Image.open(io.BytesIO(image_bytes))
            width, height = image.size
            
            if width < MIN_IMAGE_SIZE[0] or height < MIN_IMAGE_SIZE[1]:
                return False
                
            if width > MAX_IMAGE_SIZE[0] or height > MAX_IMAGE_SIZE[1]:
                return False
            
            return True
        except:
            return False

    def get_file_extension(self, filename: str) -> str:
        """íŒŒì¼ í™•ì¥ì ì¶”ì¶œ (ê¸°ì¡´ API ìœ ì§€)"""
        if not filename:
            return ""
        return filename.split(".")[-1].lower()

    def get_safe_filename(self, filename: str) -> str:
        """ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„± (ê¸°ì¡´ API ìœ ì§€)"""
        import re
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        safe_name = re.sub(r'[^\w\-_\.]', '_', filename)
        return safe_name[:100]  # ê¸¸ì´ ì œí•œ

    async def save_result_image(
        self, 
        image: Union[Image.Image, bytes], 
        session_id: str,
        result_type: str = "final"
    ) -> str:
        """ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ (ê¸°ì¡´ API ìœ ì§€)"""
        try:
            # íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{session_id}_{result_type}_{timestamp}.jpg"
            file_path = self.results_dir / filename
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            if isinstance(image, bytes):
                pil_image = Image.open(io.BytesIO(image))
            else:
                pil_image = image
            
            # RGB ë³€í™˜
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            
            # M3 Max ìµœì í™”ëœ ì €ì¥
            quality = 95 if self.is_m3_max else 90
            pil_image.save(file_path, "JPEG", quality=quality, optimize=True)
            
            # ì •ì  íŒŒì¼ìš© ë³µì‚¬
            static_path = self.static_dir / "results" / filename
            shutil.copy2(file_path, static_path)
            
            # ìŠ¤ë§ˆíŠ¸ ë°±ì—… (ê²°ê³¼ ì´ë¯¸ì§€ëŠ” ë°±ì—…í•˜ì§€ ì•ŠìŒ)
            
            logger.info(f"ğŸ“ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥: {file_path}")
            return str(static_path)
            
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    async def cleanup_session_files(self, session_id: str):
        """ì„¸ì…˜ íŒŒì¼ë“¤ ì •ë¦¬ (ê¸°ì¡´ API ìœ ì§€)"""
        try:
            cleaned_count = 0
            
            # ì—…ë¡œë“œ íŒŒì¼ ì •ë¦¬
            for pattern in [f"{session_id}_*"]:
                for file_path in self.upload_dir.glob(pattern):
                    try:
                        file_path.unlink()
                        cleaned_count += 1
                    except Exception as e:
                        logger.warning(f"âš ï¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for pattern in [f"{session_id}_*"]:
                for file_path in self.temp_dir.glob(pattern):
                    try:
                        file_path.unlink()
                        cleaned_count += 1
                    except Exception as e:
                        logger.warning(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")
            
            # ì˜¤ë˜ëœ ê²°ê³¼ íŒŒì¼ ì •ë¦¬ (24ì‹œê°„ í›„)
            cutoff_time = datetime.now() - timedelta(hours=24)
            for file_path in self.results_dir.glob(f"{session_id}_*"):
                try:
                    if datetime.fromtimestamp(file_path.stat().st_mtime) < cutoff_time:
                        file_path.unlink()
                        cleaned_count += 1
                except Exception as e:
                    logger.warning(f"âš ï¸ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")
            
            # ì„¸ì…˜ ê´€ë ¨ ë°±ì—… ì •ë¦¬
            if self.backup_config.policy != BackupPolicy.NONE:
                await self._cleanup_session_backups(session_id)
            
            logger.info(f"ğŸ§¹ ì„¸ì…˜ {session_id} íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ")
            return cleaned_count
            
        except Exception as e:
            logger.error(f"âŒ ì„¸ì…˜ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0

    async def _cleanup_session_backups(self, session_id: str):
        """ì„¸ì…˜ ê´€ë ¨ ë°±ì—… ì •ë¦¬"""
        try:
            if not self.backup_dir.exists():
                return
            
            pattern = f"*_{session_id}_*"
            session_backups = list(self.backup_dir.glob(pattern))
            
            for backup_file in session_backups:
                try:
                    backup_file.unlink()
                    if str(backup_file) in self.backup_metadata:
                        del self.backup_metadata[str(backup_file)]
                except Exception as e:
                    logger.warning(f"âš ï¸ ì„¸ì…˜ ë°±ì—… ì‚­ì œ ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            logger.warning(f"âš ï¸ ì„¸ì…˜ ë°±ì—… ì •ë¦¬ ì‹¤íŒ¨: {e}")

    async def cleanup_temp_files(self, max_age_hours: int = 24):
        """ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬ (ê¸°ì¡´ API ìœ ì§€)"""
        try:
            if not self.temp_dir.exists():
                return 0
            
            current_time = datetime.now()
            max_age_seconds = max_age_hours * 3600
            cleaned_count = 0
            
            for file_path in self.temp_dir.glob("*"):
                if file_path.is_file():
                    try:
                        file_age = current_time.timestamp() - file_path.stat().st_mtime
                        if file_age > max_age_seconds:
                            file_path.unlink()
                            cleaned_count += 1
                    except Exception as e:
                        logger.warning(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")
            
            logger.info(f"ğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ")
            return cleaned_count
                        
        except Exception as e:
            logger.error(f"âŒ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0

    async def get_file_info(self, file_path: Union[str, Path]) -> Dict[str, Any]:
        """íŒŒì¼ ì •ë³´ ì¡°íšŒ (ê¸°ì¡´ API ìœ ì§€)"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                return {"exists": False}
            
            stat = file_path.stat()
            
            info = {
                "exists": True,
                "name": file_path.name,
                "size": stat.st_size,
                "size_mb": round(stat.st_size / (1024 * 1024), 2),
                "created": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "extension": file_path.suffix.lower(),
                "is_image": file_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp', '.bmp']
            }
            
            # ì´ë¯¸ì§€ì¸ ê²½ìš° ì¶”ê°€ ì •ë³´
            if info["is_image"]:
                try:
                    with Image.open(file_path) as img:
                        info.update({
                            "width": img.width,
                            "height": img.height,
                            "format": img.format,
                            "mode": img.mode,
                            "resolution": f"{img.width}x{img.height}"
                        })
                except:
                    pass
            
            return info
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"exists": False, "error": str(e)}

    async def batch_process_files(
        self, 
        files: List[UploadFile], 
        session_id: str,
        operation: str = "validate"
    ) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ (M3 Max ë³‘ë ¬ ìµœì í™”) - ê¸°ì¡´ API ìœ ì§€"""
        try:
            results = []
            
            # M3 Max ë³‘ë ¬ ì²˜ë¦¬
            semaphore = asyncio.Semaphore(self.max_concurrent_ops)
            
            async def process_single_file(file: UploadFile, index: int):
                async with semaphore:
                    try:
                        if operation == "validate":
                            is_valid = self.validate_image(file)
                            return {
                                "index": index,
                                "filename": file.filename,
                                "valid": is_valid,
                                "size": getattr(file, 'size', 0)
                            }
                        elif operation == "save":
                            file_path = await self.save_session_file(
                                file, session_id, f"batch_{index}"
                            )
                            return {
                                "index": index,
                                "filename": file.filename,
                                "saved_path": file_path,
                                "success": True
                            }
                    except Exception as e:
                        return {
                            "index": index,
                            "filename": file.filename,
                            "error": str(e),
                            "success": False
                        }
            
            # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
            tasks = [
                process_single_file(file, i) 
                for i, file in enumerate(files)
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ì˜ˆì™¸ ì²˜ë¦¬
            processed_results = []
            for result in results:
                if isinstance(result, Exception):
                    processed_results.append({
                        "success": False,
                        "error": str(result)
                    })
                else:
                    processed_results.append(result)
            
            logger.info(f"ğŸ“ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(processed_results)}ê°œ íŒŒì¼")
            return processed_results
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []

    def get_storage_stats(self) -> Dict[str, Any]:
        """ì €ì¥ì†Œ í†µê³„ ì¡°íšŒ (ê¸°ì¡´ API ìœ ì§€ + ë°±ì—… í†µê³„ ì¶”ê°€)"""
        try:
            def get_dir_size(directory: Path) -> int:
                total = 0
                if directory.exists():
                    for file_path in directory.rglob('*'):
                        if file_path.is_file():
                            try:
                                total += file_path.stat().st_size
                            except:
                                pass
                return total
            
            upload_size = get_dir_size(self.upload_dir)
            results_size = get_dir_size(self.results_dir)
            temp_size = get_dir_size(self.temp_dir)
            static_size = get_dir_size(self.static_dir)
            backup_size = get_dir_size(self.backup_dir) if self.backup_dir.exists() else 0
            
            stats = {
                "directories": {
                    "upload": {
                        "path": str(self.upload_dir),
                        "size_bytes": upload_size,
                        "size_mb": round(upload_size / (1024 * 1024), 2),
                        "files": len(list(self.upload_dir.glob('*'))) if self.upload_dir.exists() else 0
                    },
                    "results": {
                        "path": str(self.results_dir),
                        "size_bytes": results_size,
                        "size_mb": round(results_size / (1024 * 1024), 2),
                        "files": len(list(self.results_dir.glob('*'))) if self.results_dir.exists() else 0
                    },
                    "temp": {
                        "path": str(self.temp_dir),
                        "size_bytes": temp_size,
                        "size_mb": round(temp_size / (1024 * 1024), 2),
                        "files": len(list(self.temp_dir.glob('*'))) if self.temp_dir.exists() else 0
                    },
                    "static": {
                        "path": str(self.static_dir),
                        "size_bytes": static_size,
                        "size_mb": round(static_size / (1024 * 1024), 2),
                        "files": len(list(self.static_dir.rglob('*'))) if self.static_dir.exists() else 0
                    },
                    "backups": {
                        "path": str(self.backup_dir),
                        "size_bytes": backup_size,
                        "size_mb": round(backup_size / (1024 * 1024), 2),
                        "files": len(list(self.backup_dir.glob('*'))) if self.backup_dir.exists() else 0,
                        "policy": self.backup_config.policy.value
                    }
                },
                "total": {
                    "size_bytes": upload_size + results_size + temp_size + static_size + backup_size,
                    "size_mb": round((upload_size + results_size + temp_size + static_size + backup_size) / (1024 * 1024), 2),
                    "size_gb": round((upload_size + results_size + temp_size + static_size + backup_size) / (1024 * 1024 * 1024), 2)
                },
                "limits": {
                    "max_file_size_mb": MAX_FILE_SIZE // (1024 * 1024),
                    "allowed_extensions": ALLOWED_EXTENSIONS,
                    "max_image_size": MAX_IMAGE_SIZE,
                    "min_image_size": MIN_IMAGE_SIZE
                },
                "optimization": {
                    "is_m3_max": self.is_m3_max,
                    "max_concurrent_ops": self.max_concurrent_ops
                },
                "backup_config": {
                    "policy": self.backup_config.policy.value,
                    "max_backups_per_file": self.backup_config.max_backups_per_file,
                    "max_age_days": self.backup_config.max_backup_age_days,
                    "auto_cleanup": self.backup_config.auto_cleanup
                }
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ ì €ì¥ì†Œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    # =============================================================================
    # ğŸ”¥ ì¶”ê°€ ë°±ì—… ê´€ë¦¬ ë©”ì„œë“œë“¤
    # =============================================================================

    async def auto_cleanup_all_backups(self):
        """ì „ì²´ ë°±ì—… ìë™ ì •ë¦¬"""
        try:
            if self.backup_config.policy == BackupPolicy.NONE:
                return 0
            
            now = datetime.now()
            
            # ì •ë¦¬ ê°„ê²© ì²´í¬
            if (now - self._last_cleanup).total_seconds() < self.backup_config.cleanup_interval_hours * 3600:
                return 0
            
            cutoff_date = now - timedelta(days=self.backup_config.max_backup_age_days)
            cleaned_count = 0
            
            if self.backup_dir.exists():
                for backup_file in self.backup_dir.glob("*"):
                    try:
                        if backup_file.is_file() and datetime.fromtimestamp(backup_file.stat().st_mtime) < cutoff_date:
                            backup_file.unlink()
                            if str(backup_file) in self.backup_metadata:
                                del self.backup_metadata[str(backup_file)]
                            cleaned_count += 1
                    except Exception as e:
                        logger.warning(f"âš ï¸ ë°±ì—… ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            self._last_cleanup = now
            
            if cleaned_count > 0:
                logger.info(f"ğŸ§¹ ìë™ ë°±ì—… ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ íŒŒì¼")
            
            return cleaned_count
                
        except Exception as e:
            logger.error(f"âŒ ë°±ì—… ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0

    def change_backup_policy(self, new_policy: BackupPolicy):
        """ë°±ì—… ì •ì±… ë³€ê²½"""
        old_policy = self.backup_config.policy
        self.backup_config.policy = new_policy
        
        logger.info(f"ğŸ“ ë°±ì—… ì •ì±… ë³€ê²½: {old_policy.value} â†’ {new_policy.value}")
        
        # NONEìœ¼ë¡œ ë³€ê²½ ì‹œ ê¸°ì¡´ ë°±ì—… ì •ë¦¬ ì œì•ˆ
        if new_policy == BackupPolicy.NONE and self.backup_dir.exists():
            asyncio.create_task(self._cleanup_all_existing_backups())

    async def _cleanup_all_existing_backups(self):
        """ëª¨ë“  ê¸°ì¡´ ë°±ì—… ì •ë¦¬ (ì •ì±…ì´ NONEìœ¼ë¡œ ë³€ê²½ë  ë•Œ)"""
        try:
            if not self.backup_dir.exists():
                return
            
            backup_files = list(self.backup_dir.glob("*"))
            cleaned_count = 0
            
            for backup_file in backup_files:
                try:
                    if backup_file.is_file():
                        backup_file.unlink()
                        cleaned_count += 1
                except Exception as e:
                    logger.warning(f"âš ï¸ ë°±ì—… ì‚­ì œ ì‹¤íŒ¨: {e}")
            
            # ë°±ì—… ë””ë ‰í† ë¦¬ ì œê±° (ë¹„ì–´ìˆë‹¤ë©´)
            try:
                if not any(self.backup_dir.iterdir()):
                    self.backup_dir.rmdir()
            except:
                pass
            
            self.backup_metadata.clear()
            
            logger.info(f"ğŸ§¹ ëª¨ë“  ë°±ì—… íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì „ì²´ ë°±ì—… ì •ë¦¬ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í˜¸í™˜ì„± í•¨ìˆ˜ë“¤ (ê¸°ì¡´ API ì™„ì „ í˜¸í™˜)
# =============================================================================

# í†µí•© ì „ì—­ íŒŒì¼ ë§¤ë‹ˆì €
_global_unified_file_manager = None

def get_file_manager() -> UnifiedFileManager:
    """ì „ì—­ íŒŒì¼ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    global _global_unified_file_manager
    if _global_unified_file_manager is None:
        # ê¸°ë³¸ê°’: ë°±ì—… ì•ˆí•¨ (.bak íŒŒì¼ ìƒì„± ë°©ì§€)
        backup_config = BackupConfig(policy=BackupPolicy.NONE)
        _global_unified_file_manager = UnifiedFileManager(backup_config=backup_config)
    return _global_unified_file_manager

def get_smart_file_manager() -> UnifiedFileManager:
    """ìŠ¤ë§ˆíŠ¸ ë°±ì—… í™œì„±í™”ëœ íŒŒì¼ ë§¤ë‹ˆì € ë°˜í™˜"""
    backup_config = BackupConfig(policy=BackupPolicy.SMART)
    return UnifiedFileManager(backup_config=backup_config)

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ê³¼ì˜ í˜¸í™˜ì„± ë˜í¼ë“¤ (ëª¨ë“  ê¸°ì¡´ ì½”ë“œê°€ ê·¸ëŒ€ë¡œ ì‘ë™)
def validate_image(file: UploadFile) -> bool:
    """ê¸°ì¡´ validate_image í•¨ìˆ˜ì™€ í˜¸í™˜"""
    return get_file_manager().validate_image(file)

def validate_measurements(height: float, weight: float) -> bool:
    """ê¸°ì¡´ validate_measurements í•¨ìˆ˜ì™€ í˜¸í™˜"""
    return UnifiedFileManager.validate_measurements(height, weight)

async def validate_image_content(image_bytes: bytes) -> bool:
    """ê¸°ì¡´ validate_image_content í•¨ìˆ˜ì™€ í˜¸í™˜"""
    return await UnifiedFileManager.validate_image_content_static(image_bytes)

# =============================================================================
# ğŸ”¥ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ìœ ì§€)
# =============================================================================

def get_file_size_str(size_bytes: int) -> str:
    """íŒŒì¼ í¬ê¸°ë¥¼ ì½ê¸° ì‰¬ìš´ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f}{unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f}TB"

def is_image_file(filename: str) -> bool:
    """íŒŒì¼ëª…ìœ¼ë¡œ ì´ë¯¸ì§€ íŒŒì¼ ì—¬ë¶€ í™•ì¸"""
    if not filename:
        return False
    ext = filename.split(".")[-1].lower()
    return ext in ALLOWED_EXTENSIONS

def generate_unique_filename(original_filename: str, prefix: str = "") -> str:
    """ìœ ë‹ˆí¬í•œ íŒŒì¼ëª… ìƒì„±"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    unique_id = uuid.uuid4().hex[:8]
    
    if original_filename:
        name, ext = os.path.splitext(original_filename)
        safe_name = get_file_manager().get_safe_filename(name)
        return f"{prefix}{safe_name}_{timestamp}_{unique_id}{ext}"
    else:
        return f"{prefix}file_{timestamp}_{unique_id}.jpg"

async def save_base64_image(
    base64_data: str, 
    save_path: Union[str, Path],
    max_size: Optional[int] = None
) -> bool:
    """Base64 ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        import base64
        
        # data:image/... í”„ë¦¬í”½ìŠ¤ ì œê±°
        if base64_data.startswith('data:image'):
            header, data = base64_data.split(',', 1)
        else:
            data = base64_data
        
        # ë””ì½”ë”©
        image_bytes = base64.b64decode(data)
        
        # í¬ê¸° ê²€ì¦
        if max_size and len(image_bytes) > max_size:
            logger.warning(f"âš ï¸ Base64 ì´ë¯¸ì§€ í¬ê¸° ì´ˆê³¼: {len(image_bytes)} bytes")
            return False
        
        # ì´ë¯¸ì§€ ê²€ì¦
        if not await validate_image_content(image_bytes):
            logger.warning("âš ï¸ Base64 ì´ë¯¸ì§€ ë‚´ìš© ê²€ì¦ ì‹¤íŒ¨")
            return False
        
        # íŒŒì¼ ì €ì¥
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        async with aiofiles.open(save_path, 'wb') as f:
            await f.write(image_bytes)
        
        logger.info(f"ğŸ“ Base64 ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {save_path}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Base64 ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

# =============================================================================
# ğŸ”¥ ë°±ì—… íŒŒì¼ ì •ë¦¬ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

async def cleanup_all_bak_files(base_dir: Optional[str] = None) -> int:
    """í”„ë¡œì íŠ¸ ì „ì²´ì˜ .bak íŒŒì¼ë“¤ ì •ë¦¬"""
    try:
        # âœ… ìˆ˜ì •ëœ ì½”ë“œ
        if base_dir is None:
            # íŒŒì¼ ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ backend ê²½ë¡œ ìë™ ê³„ì‚°
            current_file = Path(__file__).absolute()  # file_manager.py ìœ„ì¹˜
            backend_root = current_file.parent.parent.parent  # backend/ ê²½ë¡œ
            base_dir = str(backend_root)
            print(f"ğŸ”§ UnifiedFileManager ê²½ë¡œ ê³ ì •: {base_dir}")
        base_path = Path(base_dir)
        cleaned_count = 0
        
        # .bak ë° .backup íŒŒì¼ë“¤ ì°¾ê¸°
        patterns = ["*.bak", "*.backup", "*~"]
        
        for pattern in patterns:
            for file_path in base_path.rglob(pattern):
                try:
                    if file_path.is_file():
                        file_path.unlink()
                        cleaned_count += 1
                        logger.debug(f"ğŸ—‘ï¸ ë°±ì—… íŒŒì¼ ì‚­ì œ: {file_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")
        
        logger.info(f"ğŸ§¹ ì „ì²´ ë°±ì—… íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ")
        return cleaned_count
        
    except Exception as e:
        logger.error(f"âŒ ë°±ì—… íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return 0

def enable_smart_backup():
    """ìŠ¤ë§ˆíŠ¸ ë°±ì—… ì •ì±…ìœ¼ë¡œ ì „í™˜"""
    manager = get_file_manager()
    manager.change_backup_policy(BackupPolicy.SMART)
    logger.info("âœ… ìŠ¤ë§ˆíŠ¸ ë°±ì—… ì •ì±… í™œì„±í™”")

def disable_all_backup():
    """ëª¨ë“  ë°±ì—… ë¹„í™œì„±í™” (.bak íŒŒì¼ ìƒì„± ë°©ì§€)"""
    manager = get_file_manager()
    manager.change_backup_policy(BackupPolicy.NONE)
    logger.info("âœ… ëª¨ë“  ë°±ì—… ë¹„í™œì„±í™” - .bak íŒŒì¼ ìƒì„± ë°©ì§€")

logger.info("âœ… UnifiedFileManager v3.0 ë¡œë“œ ì™„ë£Œ")
logger.info("   ğŸ”§ ê¸°ì¡´ FileManager ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€")
logger.info("   ğŸ§¹ ìŠ¤ë§ˆíŠ¸ ë°±ì—… ì •ì±… í†µí•© (.bak ë°©ì§€)")
logger.info("   ğŸš€ M3 Max ìµœì í™” ë° conda í™˜ê²½ ì§€ì›")
logger.info("   ğŸ“ ê¸°ë³¸ ì„¤ì •: ë°±ì—… ë¹„í™œì„±í™” (NONE ì •ì±…)")