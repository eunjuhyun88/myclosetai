# app/ai_pipeline/utils/__init__.py
"""
ğŸ MyCloset AI í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v6.0 - ì™„ì „ ìˆ˜ì • ë²„ì „
âœ… get_step_model_interface í•¨ìˆ˜ ì¶”ê°€ (main.py í˜¸í™˜)
âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ê°œì„ 
âœ… ModelLoader í†µí•© ì™„ë²½ ì§€ì›
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… ê¸°ì¡´ íŒŒì¼ ìˆ˜ì • ì—†ì´ í•´ê²°
âœ… M3 Max 128GB ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥

ì‚¬ìš©ë²•:
1. ìƒˆë¡œìš´ Step: UnifiedStepInterface ì‚¬ìš©
2. ê¸°ì¡´ Step: ê¸°ì¡´ ë°©ì‹ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜)
3. main.py: get_step_model_interface() ì‚¬ìš© ê°€ëŠ¥
"""

import os
import sys
import logging
import threading
import asyncio
import time
from typing import Dict, Any, Optional, List, Union, Callable, Type
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
import weakref

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì‹œìŠ¤í…œ ì •ë³´ ë° ì„¤ì •
# ==============================================

@lru_cache(maxsize=1)
def _get_system_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ìºì‹œ (í•œë²ˆë§Œ ì‹¤í–‰)"""
    try:
        import platform
        
        system_info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "cpu_count": os.cpu_count() or 4,
            "python_version": ".".join(map(str, sys.version_info[:3]))
        }
        
        # M3 Max ê°ì§€
        is_m3_max = False
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            try:
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True, timeout=3)
                is_m3_max = 'M3' in result.stdout
            except:
                pass
        
        system_info["is_m3_max"] = is_m3_max
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        if PSUTIL_AVAILABLE:
            system_info["memory_gb"] = round(psutil.virtual_memory().total / (1024**3))
        else:
            system_info["memory_gb"] = 16
        
        # ë””ë°”ì´ìŠ¤ ê°ì§€
        device = "cpu"
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available():
                device = "mps"
            elif torch.cuda.is_available():
                device = "cuda"
        
        system_info["device"] = device
        return system_info
        
    except Exception as e:
        logger.warning(f"ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
        return {
            "platform": "unknown",
            "is_m3_max": False,
            "device": "cpu",
            "cpu_count": 4,
            "memory_gb": 16,
            "python_version": "3.8.0"
        }

# ì „ì—­ ì‹œìŠ¤í…œ ì •ë³´
SYSTEM_INFO = _get_system_info()

# ==============================================
# ğŸ”¥ í†µí•© ë°ì´í„° êµ¬ì¡° (ìˆœí™˜ì°¸ì¡° ì—†ìŒ)
# ==============================================

class UtilsMode(Enum):
    """ìœ í‹¸ë¦¬í‹° ëª¨ë“œ"""
    LEGACY = "legacy"        # ê¸°ì¡´ ë°©ì‹ (v3.0)
    UNIFIED = "unified"      # ìƒˆë¡œìš´ í†µí•© ë°©ì‹ (v6.0)
    HYBRID = "hybrid"        # í˜¼í•© ë°©ì‹

@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì •"""
    device: str = "auto"
    memory_gb: float = 16.0
    is_m3_max: bool = False
    optimization_enabled: bool = True
    max_workers: int = 4
    cache_enabled: bool = True
    debug_mode: bool = False

@dataclass
class StepConfig:
    """Step ì„¤ì • (ìˆœí™˜ì°¸ì¡° ì—†ëŠ” ë°ì´í„° ì „ìš©)"""
    step_name: str
    model_name: Optional[str] = None
    model_type: Optional[str] = None
    model_class: Optional[str] = None
    input_size: tuple = (512, 512)
    device: str = "auto"
    precision: str = "fp16"
    optimization_params: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´ (ìˆœí™˜ì°¸ì¡° ì—†ëŠ” ë°ì´í„° ì „ìš©)"""
    name: str
    path: str
    model_type: str
    file_size_mb: float
    confidence_score: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# ğŸ”¥ Step Model Interface (main.py í˜¸í™˜)
# ==============================================

class StepModelInterface:
    """
    ğŸ”— Stepìš© ModelLoader ì¸í„°í˜ì´ìŠ¤
    âœ… main.pyê°€ ìš”êµ¬í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ ì œê³µ
    âœ… ë¹„ë™ê¸° ë©”ì„œë“œ ì˜¬ë°”ë¥¸ êµ¬í˜„
    âœ… list_available_models ë©”ì„œë“œ í¬í•¨
    """
    
    def __init__(self, step_name: str, model_loader=None):
        self.step_name = step_name
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # ëª¨ë¸ ìºì‹œ
        self._loaded_models = {}
        self._available_models = []
        
        # ê¸°ë³¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤
        self._initialize_available_models()
        
        self.logger.info(f"ğŸ”— {step_name} ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì´ˆê¸°í™”"""
        # Stepë³„ ê¸°ë³¸ ëª¨ë¸ë“¤
        step_models = {
            "HumanParsingStep": [
                "human_parsing_graphonomy",
                "human_parsing_graphonomy_v6",
                "human_parsing_graphonomy_v5",
                "human_parsing_u2net"
            ],
            "PoseEstimationStep": [
                "pose_estimation_openpose",
                "pose_estimation_openpose_v8",
                "pose_estimation_yolov8",
                "pose_estimation_lightweight"
            ],
            "ClothSegmentationStep": [
                "cloth_segmentation_u2net",
                "cloth_segmentation_u2net_v6",
                "cloth_segmentation_rembg",
                "cloth_segmentation_sam"
            ],
            "GeometricMatchingStep": [
                "geometric_matching_tps",
                "geometric_matching_flow",
                "geometric_matching_affine"
            ],
            "ClothWarpingStep": [
                "cloth_warping_cpvton",
                "cloth_warping_flownet",
                "cloth_warping_thin_plate_spline"
            ],
            "VirtualFittingStep": [
                "virtual_fitting_stable_diffusion",
                "virtual_fitting_ootdiffusion",
                "virtual_fitting_viton_hd"
            ],
            "PostProcessingStep": [
                "post_processing_enhance",
                "post_processing_super_resolution",
                "post_processing_color_correction"
            ],
            "QualityAssessmentStep": [
                "quality_assessment_clip",
                "quality_assessment_lpips",
                "quality_assessment_fid"
            ]
        }
        
        self._available_models = step_models.get(self.step_name, [f"{self.step_name.lower()}_model"])
    
    def list_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ - main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” ë©”ì„œë“œ"""
        return self._available_models.copy()
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ - ë¹„ë™ê¸° ë©”ì„œë“œ"""
        try:
            target_model = model_name or (self._available_models[0] if self._available_models else None)
            
            if not target_model:
                self.logger.warning(f"âš ï¸ {self.step_name}ì— ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŒ")
                return self._create_fallback_model(target_model or "fallback")
            
            # ìºì‹œ í™•ì¸
            if target_model in self._loaded_models:
                self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {target_model}")
                return self._loaded_models[target_model]
            
            # ì‹¤ì œ ModelLoader ì‚¬ìš© (ìˆëŠ” ê²½ìš°)
            if self.model_loader and hasattr(self.model_loader, 'get_model'):
                try:
                    model = await self._safe_model_load(target_model)
                    if model:
                        self._loaded_models[target_model] = model
                        self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {target_model}")
                        return model
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ModelLoader ì‹¤íŒ¨, í´ë°± ì‚¬ìš©: {e}")
            
            # í´ë°± ëª¨ë¸ ìƒì„±
            fallback_model = self._create_fallback_model(target_model)
            self._loaded_models[target_model] = fallback_model
            return fallback_model
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._create_fallback_model(model_name or "error_fallback")
    
    async def _safe_model_load(self, model_name: str) -> Optional[Any]:
        """ì•ˆì „í•œ ëª¨ë¸ ë¡œë“œ"""
        try:
            if hasattr(self.model_loader, 'get_model'):
                get_model_method = getattr(self.model_loader, 'get_model')
                
                # ë¹„ë™ê¸° ë©”ì„œë“œì¸ì§€ í™•ì¸
                if asyncio.iscoroutinefunction(get_model_method):
                    return await get_model_method(model_name)
                else:
                    return get_model_method(model_name)
            
            return None
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì•ˆì „í•œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_fallback_model(self, model_name: str) -> Dict[str, Any]:
        """í´ë°± ëª¨ë¸ ìƒì„±"""
        return {
            "model_name": model_name,
            "model_type": "FallbackModel",
            "step_name": self.step_name,
            "success": True,
            "is_fallback": True,
            "confidence": 0.8,
            "message": f"í´ë°± ëª¨ë¸ ì‚¬ìš©: {model_name}",
            
            # ê¸°ë³¸ ì²˜ë¦¬ í•¨ìˆ˜
            "process": lambda x: {
                "success": True,
                "result": x,
                "confidence": 0.8,
                "processing_time": 0.1,
                "model_used": model_name
            },
            
            # ì˜ˆì¸¡ í•¨ìˆ˜ (ë¹„ë™ê¸°)
            "predict": self._fallback_predict
        }
    
    async def _fallback_predict(self, input_data: Any) -> Dict[str, Any]:
        """í´ë°± ì˜ˆì¸¡ í•¨ìˆ˜"""
        await asyncio.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜ ì§€ì—°
        return {
            "success": True,
            "prediction": input_data,
            "confidence": 0.8,
            "processing_time": 0.1,
            "step_name": self.step_name,
            "is_simulation": True
        }
    
    async def unload_models(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ - ë¹„ë™ê¸° ë©”ì„œë“œ (main.py í˜¸í™˜)"""
        try:
            unloaded_count = len(self._loaded_models)
            self._loaded_models.clear()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            safe_mps_empty_cache()
                    except:
                        pass
                elif torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            import gc
            gc.collect()
            
            self.logger.info(f"âœ… {unloaded_count}ê°œ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {self.step_name}")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def get_step_info(self) -> Dict[str, Any]:
        """Step ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": self.step_name,
            "available_models": self._available_models,
            "loaded_models": list(self._loaded_models.keys()),
            "model_loader_connected": self.model_loader is not None,
            "total_models": len(self._available_models),
            "loaded_count": len(self._loaded_models)
        }

# ==============================================
# ğŸ”¥ main.py í˜¸í™˜ í•¨ìˆ˜ (í•µì‹¬ ìˆ˜ì •ì‚¬í•­ 1)
# ==============================================

def get_step_model_interface(step_name: str, model_loader_instance=None) -> StepModelInterface:
    """
    ğŸ”¥ main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜
    âœ… import ì˜¤ë¥˜ í•´ê²°
    âœ… StepModelInterface ë°˜í™˜
    âœ… ë¹„ë™ê¸° ë©”ì„œë“œ í¬í•¨
    """
    try:
        # ModelLoader ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        if model_loader_instance is None:
            try:
                # ì „ì—­ ModelLoader ì‹œë„
                from .model_loader import get_global_model_loader
                model_loader_instance = get_global_model_loader()
                logger.debug(f"âœ… ì „ì—­ ModelLoader íšë“: {step_name}")
            except ImportError as e:
                logger.warning(f"âš ï¸ ModelLoader import ì‹¤íŒ¨: {e}")
                model_loader_instance = None
            except Exception as e:
                logger.warning(f"âš ï¸ ì „ì—­ ModelLoader íšë“ ì‹¤íŒ¨: {e}")
                model_loader_instance = None
        
        # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        interface = StepModelInterface(step_name, model_loader_instance)
        
        logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        # ì™„ì „ í´ë°± ì¸í„°í˜ì´ìŠ¤
        return StepModelInterface(step_name, None)

# ==============================================
# ğŸ”¥ í†µí•© ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € (ì˜ì¡´ì„± ì£¼ì…)
# ==============================================

class UnifiedUtilsManager:
    """
    ğŸ í†µí•© ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì €
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
    âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´
    âœ… ëª¨ë“  ê¸°ëŠ¥ í†µí•© ê´€ë¦¬
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ê°œì„ 
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.logger = logging.getLogger(f"{__name__}.UnifiedUtilsManager")
        
        # ê¸°ë³¸ ì„¤ì •
        self.system_config = SystemConfig(
            device=SYSTEM_INFO["device"],
            memory_gb=SYSTEM_INFO["memory_gb"],
            is_m3_max=SYSTEM_INFO["is_m3_max"],
            max_workers=min(SYSTEM_INFO["cpu_count"], 8)
        )
        
        # ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.initialization_time = None
        
        # ì»´í¬ë„ŒíŠ¸ ì €ì¥ì†Œ (ì•½í•œ ì°¸ì¡° ì‚¬ìš©)
        self._step_interfaces = weakref.WeakValueDictionary()
        self._model_interfaces = {}  # StepModelInterface ì €ì¥
        self._model_cache = {}
        self._service_cache = weakref.WeakValueDictionary()
        
        # í†µê³„
        self.stats = {
            "interfaces_created": 0,
            "models_loaded": 0,
            "memory_optimizations": 0,
            "total_requests": 0
        }
        
        # ë™ê¸°í™”
        self._interface_lock = threading.RLock()
        
        self._initialized = True
        self.logger.info("ğŸ¯ UnifiedUtilsManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
    
    async def initialize(self, **kwargs) -> Dict[str, Any]:
        """í†µí•© ì´ˆê¸°í™” - ë¹„ë™ê¸° ì²˜ë¦¬ ê°œì„ """
        if self.is_initialized:
            return {"success": True, "message": "Already initialized"}
        
        try:
            start_time = time.time()
            self.logger.info("ğŸš€ UnifiedUtilsManager ì´ˆê¸°í™” ì‹œì‘...")
            
            # ì„¤ì • ì—…ë°ì´íŠ¸
            for key, value in kwargs.items():
                if hasattr(self.system_config, key):
                    setattr(self.system_config, key, value)
            
            # GPU ìµœì í™” ì„¤ì •
            if self.system_config.is_m3_max and TORCH_AVAILABLE:
                try:
                    # M3 Max ìµœì í™” í™˜ê²½ë³€ìˆ˜
                    os.environ.update({
                        'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                        'OMP_NUM_THREADS': str(min(self.system_config.max_workers * 2, 16))
                    })
                    self.logger.info("âœ… M3 Max GPU ìµœì í™” ì„¤ì • ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
            
            # ModelLoader ì—°ë™ ì‹œë„
            await self._try_initialize_model_loader()
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            self.initialization_time = time.time() - start_time
            
            self.logger.info(f"ğŸ‰ UnifiedUtilsManager ì´ˆê¸°í™” ì™„ë£Œ ({self.initialization_time:.2f}s)")
            
            return {
                "success": True,
                "initialization_time": self.initialization_time,
                "system_config": self.system_config,
                "system_info": SYSTEM_INFO
            }
            
        except Exception as e:
            self.logger.error(f"âŒ UnifiedUtilsManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def _try_initialize_model_loader(self):
        """ModelLoader ì´ˆê¸°í™” ì‹œë„"""
        try:
            from .model_loader import get_global_model_loader
            self.model_loader = get_global_model_loader()
            self.logger.info("âœ… ModelLoader ì—°ë™ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ModelLoader ì—°ë™ ì‹¤íŒ¨: {e}")
            self.model_loader = None
    
    def create_step_interface(self, step_name: str, **options) -> 'UnifiedStepInterface':
        """
        Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ìƒˆë¡œìš´ ë°©ì‹)
        ìˆœí™˜ì°¸ì¡° ì—†ì´ ëª¨ë“  ê¸°ëŠ¥ ì œê³µ
        """
        try:
            with self._interface_lock:
                # ìºì‹œ í™•ì¸
                cache_key = f"{step_name}_{hash(str(options))}" if options else step_name
                
                if cache_key in self._step_interfaces:
                    self.logger.debug(f"ğŸ“‹ {step_name} ìºì‹œëœ ì¸í„°í˜ì´ìŠ¤ ë°˜í™˜")
                    return self._step_interfaces[cache_key]
                
                # ìƒˆ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                step_config = self._create_step_config(step_name, **options)
                interface = UnifiedStepInterface(self, step_config)
                
                # ìºì‹œ ì €ì¥
                self._step_interfaces[cache_key] = interface
                
                self.stats["interfaces_created"] += 1
                self.logger.info(f"ğŸ”— {step_name} í†µí•© ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                return interface
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ì¸í„°í˜ì´ìŠ¤
            return self._create_fallback_interface(step_name)
    
    def create_step_model_interface(self, step_name: str) -> StepModelInterface:
        """Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± (main.py í˜¸í™˜)"""
        try:
            if step_name in self._model_interfaces:
                return self._model_interfaces[step_name]
            
            interface = StepModelInterface(step_name, getattr(self, 'model_loader', None))
            self._model_interfaces[step_name] = interface
            
            self.logger.info(f"ğŸ”— {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
            return interface
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return StepModelInterface(step_name, None)
    
    def _create_step_config(self, step_name: str, **options) -> StepConfig:
        """Step ì„¤ì • ìƒì„±"""
        # Stepë³„ ê¸°ë³¸ ì„¤ì • (í•˜ë“œì½”ë”©ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€)
        step_defaults = {
            "HumanParsingStep": {
                "model_name": "human_parsing_graphonomy",
                "model_type": "GraphonomyModel",
                "input_size": (512, 512)
            },
            "PoseEstimationStep": {
                "model_name": "pose_estimation_openpose",
                "model_type": "OpenPoseModel",
                "input_size": (368, 368)
            },
            "ClothSegmentationStep": {
                "model_name": "cloth_segmentation_u2net",
                "model_type": "U2NetModel",
                "input_size": (320, 320)
            },
            "VirtualFittingStep": {
                "model_name": "virtual_fitting_stable_diffusion",
                "model_type": "StableDiffusionPipeline",
                "input_size": (512, 512)
            }
        }
        
        defaults = step_defaults.get(step_name, {
            "model_name": f"{step_name.lower()}_model",
            "model_type": "BaseModel",
            "input_size": (512, 512)
        })
        
        # ì„¤ì • ë³‘í•©
        config_data = {
            "step_name": step_name,
            "device": self.system_config.device,
            "precision": "fp16" if self.system_config.is_m3_max else "fp32",
            **defaults,
            **options
        }
        
        return StepConfig(**config_data)
    
    def _create_fallback_interface(self, step_name: str) -> 'UnifiedStepInterface':
        """í´ë°± ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        fallback_config = StepConfig(step_name=step_name)
        return UnifiedStepInterface(self, fallback_config, is_fallback=True)
    
    def get_or_load_model(self, model_name: str, step_config: StepConfig) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ìºì‹œ í™œìš©)"""
        try:
            # ìºì‹œ í™•ì¸
            if model_name in self._model_cache:
                self.logger.debug(f"ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                return self._model_cache[model_name]
            
            # ì‹¤ì œ ëª¨ë¸ ë¡œë“œëŠ” ì—¬ê¸°ì„œ êµ¬í˜„
            # í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜
            model_info = ModelInfo(
                name=model_name,
                path=f"ai_models/{model_name}.pth",
                model_type=step_config.model_type or "BaseModel",
                file_size_mb=150.0
            )
            
            # ìºì‹œ ì €ì¥
            self._model_cache[model_name] = model_info
            self.stats["models_loaded"] += 1
            
            self.logger.info(f"ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
            return model_info
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    async def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” - ë¹„ë™ê¸° ê°œì„ """
        try:
            import gc
            gc.collect()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.system_config.device == "mps" and torch.backends.mps.is_available():
                    if hasattr(torch.mps, 'empty_cache'):
                        safe_mps_empty_cache()
                elif self.system_config.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            for interface in self._model_interfaces.values():
                try:
                    await interface.unload_models()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ìºì‹œ ì •ë¦¬ (ì˜¤ë˜ëœ í•­ëª©)
            items_to_remove = []
            if len(self._model_cache) > 10:
                # ê°„ë‹¨í•œ LRU êµ¬í˜„
                items_to_remove = list(self._model_cache.keys())[:5]
                for key in items_to_remove:
                    del self._model_cache[key]
            
            self.stats["memory_optimizations"] += 1
            
            memory_info = {}
            if PSUTIL_AVAILABLE:
                vm = psutil.virtual_memory()
                memory_info = {
                    "total_gb": round(vm.total / (1024**3), 1),
                    "available_gb": round(vm.available / (1024**3), 1),
                    "percent": round(vm.percent, 1)
                }
            
            return {
                "success": True,
                "memory_info": memory_info,
                "cache_cleared": len(items_to_remove),
                "interfaces_cleaned": len(self._model_interfaces)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def get_status(self) -> Dict[str, Any]:
        """ìƒíƒœ ì¡°íšŒ"""
        memory_info = {}
        if PSUTIL_AVAILABLE:
            vm = psutil.virtual_memory()
            memory_info = {
                "total_gb": round(vm.total / (1024**3), 1),
                "available_gb": round(vm.available / (1024**3), 1),
                "percent": round(vm.percent, 1)
            }
        
        return {
            "initialized": self.is_initialized,
            "initialization_time": self.initialization_time,
            "system_config": self.system_config,
            "system_info": SYSTEM_INFO,
            "stats": self.stats,
            "memory_info": memory_info,
            "cache_sizes": {
                "step_interfaces": len(self._step_interfaces),
                "model_interfaces": len(self._model_interfaces),
                "models": len(self._model_cache),
                "services": len(self._service_cache)
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - ë¹„ë™ê¸° ê°œì„ """
        try:
            # ëª¨ë“  ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            for interface in self._model_interfaces.values():
                try:
                    await interface.unload_models()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            self._step_interfaces.clear()
            self._model_interfaces.clear()
            self._model_cache.clear()
            self._service_cache.clear()
            self.is_initialized = False
            
            self.logger.info("âœ… UnifiedUtilsManager ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ UnifiedUtilsManager ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ í†µí•© Step ì¸í„°í˜ì´ìŠ¤
# ==============================================

class UnifiedStepInterface:
    """
    ğŸ”— í†µí•© Step ì¸í„°í˜ì´ìŠ¤
    âœ… ìˆœí™˜ì°¸ì¡° ì—†ìŒ
    âœ… ëª¨ë“  ê¸°ëŠ¥ ì œê³µ
    âœ… ê¸°ì¡´ ë°©ì‹ê³¼ í˜¸í™˜
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ê°œì„ 
    """
    
    def __init__(self, manager: UnifiedUtilsManager, config: StepConfig, is_fallback: bool = False):
        self.manager = manager
        self.config = config
        self.is_fallback = is_fallback
        
        self.logger = logging.getLogger(f"steps.{config.step_name}")
        
        # í†µê³„ ì¶”ì 
        self._request_count = 0
        self._last_request_time = None
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ - ë¹„ë™ê¸° ê°œì„ """
        try:
            target_model = model_name or self.config.model_name
            if not target_model:
                self.logger.warning("ëª¨ë¸ ì´ë¦„ì´ ì§€ì •ë˜ì§€ ì•ŠìŒ")
                return None
            
            model = self.manager.get_or_load_model(target_model, self.config)
            
            self._request_count += 1
            self._last_request_time = time.time()
            self.manager.stats["total_requests"] += 1
            
            return model
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” - ë¹„ë™ê¸°"""
        return await self.manager.optimize_memory()
    
    def process_image(self, image_data: Any, **kwargs) -> Optional[Any]:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ (ê¸°ë³¸ êµ¬í˜„)"""
        try:
            if self.is_fallback:
                self.logger.warning(f"{self.config.step_name} í´ë°± ëª¨ë“œ - ì‹œë®¬ë ˆì´ì…˜ ì²˜ë¦¬")
                return {"success": True, "simulation": True}
            
            # ì‹¤ì œ ì´ë¯¸ì§€ ì²˜ë¦¬ ë¡œì§ì€ ê° Stepì—ì„œ êµ¬í˜„
            self.logger.info(f"{self.config.step_name} ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘")
            
            # ê¸°ë³¸ ì „ì²˜ë¦¬ (í¬ê¸° ì¡°ì • ë“±)
            if hasattr(image_data, 'resize'):
                processed_image = image_data.resize(self.config.input_size)
            else:
                processed_image = image_data
            
            return {
                "success": True,
                "processed_image": processed_image,
                "step_name": self.config.step_name,
                "processing_time": 0.1
            }
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def get_config(self) -> StepConfig:
        """ì„¤ì • ë°˜í™˜"""
        return self.config
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°˜í™˜"""
        return {
            "step_name": self.config.step_name,
            "request_count": self._request_count,
            "last_request_time": self._last_request_time,
            "is_fallback": self.is_fallback,
            "model_name": self.config.model_name
        }

# ==============================================
# ğŸ”¥ ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œ ì§€ì›)
# ==============================================

def create_step_interface(step_name: str) -> Dict[str, Any]:
    """
    ğŸ”¥ ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜ (v3.0 ë°©ì‹)
    ê¸°ì¡´ Step í´ë˜ìŠ¤ë“¤ì´ ê³„ì† ì‚¬ìš© ê°€ëŠ¥
    """
    try:
        manager = get_utils_manager()
        unified_interface = manager.create_step_interface(step_name)
        
        # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë³€í™˜
        legacy_interface = {
            "step_name": step_name,
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "version": "v6.0-legacy-compatible",
            "has_unified_utils": True,
            "unified_interface": unified_interface
        }
        
        # ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ async wrapperë¡œ ì œê³µ
        async def get_model_wrapper(model_name=None):
            return await unified_interface.get_model(model_name)
        
        async def optimize_memory_wrapper():
            return await unified_interface.optimize_memory()
        
        legacy_interface["get_model"] = get_model_wrapper
        legacy_interface["optimize_memory"] = optimize_memory_wrapper
        legacy_interface["process_image"] = unified_interface.process_image
        
        return legacy_interface
        
    except Exception as e:
        logger.error(f"âŒ {step_name} ë ˆê±°ì‹œ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        # ì™„ì „ í´ë°±
        return {
            "step_name": step_name,
            "error": str(e),
            "system_info": SYSTEM_INFO,
            "logger": logging.getLogger(f"steps.{step_name}"),
            "get_model": lambda: None,
            "optimize_memory": lambda: {"success": False},
            "process_image": lambda x, **k: None
        }

# ==============================================
# ğŸ”¥ ì „ì—­ ê´€ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

_global_manager: Optional[UnifiedUtilsManager] = None
_manager_lock = threading.Lock()

def get_utils_manager() -> UnifiedUtilsManager:
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ë§¤ë‹ˆì € ë°˜í™˜"""
    global _global_manager
    
    with _manager_lock:
        if _global_manager is None:
            _global_manager = UnifiedUtilsManager()
        return _global_manager

def initialize_global_utils(**kwargs) -> Dict[str, Any]:
    """
    ğŸ”¥ ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” - ë¹„ë™ê¸° ì²˜ë¦¬ ê°œì„ 
    main.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” ì§„ì…ì 
    """
    try:
        manager = get_utils_manager()
        
        # ë¹„ë™ê¸° ì´ˆê¸°í™” ì²˜ë¦¬
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        if loop.is_running():
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” íƒœìŠ¤í¬ ìƒì„±
            future = asyncio.create_task(manager.initialize(**kwargs))
            return {"success": True, "message": "Initialization started", "future": future}
        else:
            # ìƒˆ ë£¨í”„ì—ì„œ ì‹¤í–‰
            result = loop.run_until_complete(manager.initialize(**kwargs))
            return result
            
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def get_system_status() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    try:
        manager = get_utils_manager()
        return manager.get_status()
    except Exception as e:
        return {"error": str(e), "system_info": SYSTEM_INFO}

async def reset_global_utils():
    """ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ - ë¹„ë™ê¸° ê°œì„ """
    global _global_manager
    
    try:
        with _manager_lock:
            if _global_manager:
                await _global_manager.cleanup()
                _global_manager = None
        logger.info("âœ… ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ ì „ì—­ ìœ í‹¸ë¦¬í‹° ë¦¬ì…‹ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_unified_interface(step_name: str, **options) -> UnifiedStepInterface:
    """ìƒˆë¡œìš´ í†µí•© ì¸í„°í˜ì´ìŠ¤ ìƒì„± (ê¶Œì¥)"""
    manager = get_utils_manager()
    return manager.create_step_interface(step_name, **options)

async def optimize_system_memory() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìµœì í™” - ë¹„ë™ê¸°"""
    manager = get_utils_manager()
    return await manager.optimize_memory()

# ==============================================
# ğŸ”¥ __all__ ì •ì˜ (í•µì‹¬ ìˆ˜ì •ì‚¬í•­ 2)
# ==============================================

__all__ = [
    # ğŸ¯ í•µì‹¬ í´ë˜ìŠ¤ë“¤
    'UnifiedUtilsManager',
    'UnifiedStepInterface',
    'StepModelInterface',  # ì¶”ê°€
    'SystemConfig',
    'StepConfig',
    'ModelInfo',
    
    # ğŸ”§ ì „ì—­ í•¨ìˆ˜ë“¤
    'get_utils_manager',
    'initialize_global_utils',
    'get_system_status',
    'reset_global_utils',
    
    # ğŸ”„ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
    'create_step_interface',          # ë ˆê±°ì‹œ í˜¸í™˜
    'create_unified_interface',       # ìƒˆë¡œìš´ ë°©ì‹
    'get_step_model_interface',       # âœ… main.py í˜¸í™˜ ì¶”ê°€
    
    # ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´
    'SYSTEM_INFO',
    'optimize_system_memory',
    
    # ğŸ”§ ìœ í‹¸ë¦¬í‹°
    'UtilsMode'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ
# ==============================================

logger.info("=" * 70)
logger.info("ğŸ MyCloset AI í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ v6.0 ë¡œë“œ ì™„ë£Œ")
logger.info("âœ… get_step_model_interface í•¨ìˆ˜ ì¶”ê°€ (main.py í˜¸í™˜)")
logger.info("âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ê°œì„ ")
logger.info("âœ… StepModelInterface.list_available_models í¬í•¨")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("âœ… ê¸°ì¡´ ì½”ë“œ í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥")
logger.info("âœ… ìƒˆë¡œìš´ í†µí•© ì¸í„°í˜ì´ìŠ¤ ì œê³µ")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ: {SYSTEM_INFO['platform']} / {SYSTEM_INFO['device']}")
logger.info(f"ğŸ M3 Max: {'âœ…' if SYSTEM_INFO['is_m3_max'] else 'âŒ'}")
logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬: {SYSTEM_INFO['memory_gb']}GB")
logger.info("=" * 70)

# ì¢…ë£Œ ì‹œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
import atexit

def cleanup_on_exit():
    """ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(reset_global_utils())
        loop.close()
    except Exception as e:
        logger.warning(f"âš ï¸ ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

atexit.register(cleanup_on_exit)