# backend/app/ai_pipeline/utils/smart_model_mapper.py
"""
ğŸ”¥ SmartModelPathMapper - í†µí•© ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ ì‹œìŠ¤í…œ v1.0
================================================================================
âœ… ModelLoader + AutoDetector ì™„ì „ í†µí•©
âœ… ë™ì  ê²½ë¡œ íƒì§€ë¡œ ì›Œë‹ ì œê±°
âœ… BaseStepMixin v18.0 ì™„ì „ í˜¸í™˜
âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ (229GB) ì™„ì „ í™œìš©
âœ… conda í™˜ê²½ + M3 Max ìµœì í™”
================================================================================
"""

import os
import logging
import time
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import threading

logger = logging.getLogger(__name__)

@dataclass
class ModelMappingInfo:
    """ëª¨ë¸ ë§¤í•‘ ì •ë³´"""
    name: str
    actual_path: Optional[Path] = None
    step_class: Optional[str] = None
    ai_class: Optional[str] = None
    size_mb: float = 0.0
    priority_score: float = 0.0
    is_loaded: bool = False
    load_method: str = "load_models"
    search_patterns: List[str] = field(default_factory=list)
    fallback_paths: List[str] = field(default_factory=list)

class SmartModelPathMapper:
    """ğŸ”¥ í†µí•© ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ ì‹œìŠ¤í…œ"""
    
    def __init__(self, ai_models_root: Union[str, Path]):
        self.ai_models_root = Path(ai_models_root)
        self.logger = logging.getLogger(f"{__name__}.SmartModelPathMapper")
        
        # ê²½ë¡œ ê²€ì¦ ë° ìˆ˜ì •
        self.ai_models_root = self._validate_and_fix_path(self.ai_models_root)
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.path_cache: Dict[str, ModelMappingInfo] = {}
        self.last_scan_time = 0.0
        self.cache_valid_duration = 300  # 5ë¶„
        
        # ğŸ”¥ ì‹¤ì œ íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ í†µí•© ë§¤í•‘ í…Œì´ë¸”
        self.unified_model_mappings = self._create_unified_mappings()
        
        # í†µê³„
        self.mapping_stats = {
            "total_patterns": len(self.unified_model_mappings),
            "successful_mappings": 0,
            "failed_mappings": 0,
            "cache_hits": 0,
            "dynamic_discoveries": 0
        }
        
        self._lock = threading.RLock()
        
        self.logger.info(f"ğŸ§  SmartModelPathMapper ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ“ AI ëª¨ë¸ ë£¨íŠ¸: {self.ai_models_root}")
        self.logger.info(f"ğŸ¯ í†µí•© ë§¤í•‘ íŒ¨í„´: {len(self.unified_model_mappings)}ê°œ")
    
    def _validate_and_fix_path(self, path: Path) -> Path:
        """ê²½ë¡œ ê²€ì¦ ë° ìë™ ìˆ˜ì •"""
        try:
            if path.exists():
                return path.resolve()
            
                    # backend ì¤‘ë³µ íŒ¨í„´ ìë™ ìˆ˜ì •
        path_str = str(path)
        backend_pattern = "backend" + "/" + "backend"
        if backend_pattern in path_str:
            corrected = Path(path_str.replace(backend_pattern, "backend"))
                if corrected.exists():
                    self.logger.info(f"âœ… ê²½ë¡œ ìë™ ìˆ˜ì •: {path} â†’ {corrected}")
                    return corrected.resolve()
            
            # ìƒëŒ€ ê²½ë¡œì—ì„œ ì ˆëŒ€ ê²½ë¡œ ì°¾ê¸°
            current = Path(__file__).resolve()
            for i in range(10):
                potential = current / "ai_models"
                if potential.exists():
                    self.logger.info(f"âœ… ë™ì  ê²½ë¡œ ë°œê²¬: {potential}")
                    return potential
                
                if current.name == "backend":
                    backend_ai_models = current / "ai_models"
                    if backend_ai_models.exists():
                        return backend_ai_models
                    
                if current.parent == current:
                    break
                current = current.parent
            
            # ìµœì¢… í´ë°±
            self.logger.warning(f"âš ï¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ìƒì„± ì‹œë„: {path}")
            path.mkdir(parents=True, exist_ok=True)
            return path.resolve()
            
        except Exception as e:
            self.logger.error(f"âŒ ê²½ë¡œ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return path
    
    def _create_unified_mappings(self) -> Dict[str, Dict[str, Any]]:
        """ğŸ”¥ í†µí•© ëª¨ë¸ ë§¤í•‘ í…Œì´ë¸” ìƒì„±"""
        return {
            # Step 01: Human Parsing
            "graphonomy": {
                "step_class": "HumanParsingStep",
                "ai_class": "RealGraphonomyModel", 
                "search_patterns": [
                    "graphonomy.pth",
                    "*/graphonomy.pth",
                    "step_01_human_parsing/graphonomy.pth",
                    "Graphonomy/*/graphonomy.pth"
                ],
                "fallback_patterns": [
                    "**/graphonomy*.pth",
                    "**/Graphonomy*.pth"
                ],
                "min_size_mb": 1000,
                "priority": 1
            },
            
            "human_parsing_schp": {
                "step_class": "HumanParsingStep", 
                "ai_class": "RealSCHPModel",
                "search_patterns": [
                    "exp-schp-*-atr.pth",
                    "step_01_human_parsing/exp-schp-*-atr.pth",
                    "Self-Correction-Human-Parsing/exp-schp-*-atr.pth"
                ],
                "fallback_patterns": ["**/exp-schp*atr*.pth"],
                "min_size_mb": 250,
                "priority": 2
            },
            
            # Step 02: Pose Estimation  
            "yolov8": {
                "step_class": "PoseEstimationStep",
                "ai_class": "RealYOLOv8PoseModel",
                "search_patterns": [
                    "yolov8n-pose.pt",
                    "step_02_pose_estimation/yolov8*.pt"
                ],
                "fallback_patterns": ["**/yolov8*pose*.pt"],
                "min_size_mb": 5,
                "priority": 1
            },
            
            "openpose": {
                "step_class": "PoseEstimationStep",
                "ai_class": "RealOpenPoseModel", 
                "search_patterns": [
                    "openpose.pth",
                    "step_02_pose_estimation/openpose.pth",
                    "body_pose_model.pth"
                ],
                "fallback_patterns": ["**/openpose*.pth", "**/body_pose*.pth"],
                "min_size_mb": 90,
                "priority": 2
            },
            
            # Step 03: Cloth Segmentation
            "sam_vit_h": {
                "step_class": "ClothSegmentationStep",
                "ai_class": "RealSAMModel",
                "search_patterns": [
                    "sam_vit_h_4b8939.pth",
                    "step_03_cloth_segmentation/sam_vit_h_4b8939.pth",
                    "step_04_geometric_matching/sam_vit_h_4b8939.pth"
                ],
                "fallback_patterns": ["**/sam_vit_h*.pth"],
                "min_size_mb": 2400,
                "priority": 1
            },
            
            "u2net": {
                "step_class": "ClothSegmentationStep",
                "ai_class": "RealU2NetModel",
                "search_patterns": [
                    "u2net.pth",
                    "step_03_cloth_segmentation/u2net.pth"
                ],
                "fallback_patterns": ["**/u2net*.pth"],
                "min_size_mb": 160,
                "priority": 2
            },
            
            # Step 04: Geometric Matching
            "gmm": {
                "step_class": "GeometricMatchingStep", 
                "ai_class": "RealGMMModel",
                "search_patterns": [
                    "gmm_final.pth",
                    "step_04_geometric_matching/gmm_final.pth"
                ],
                "fallback_patterns": ["**/gmm*.pth"],
                "min_size_mb": 10,
                "priority": 1
            },
            
            # Step 05: Cloth Warping
            "realvis_xl": {
                "step_class": "ClothWarpingStep",
                "ai_class": "RealVisXLModel", 
                "search_patterns": [
                    "RealVisXL_V4.0.safetensors",
                    "step_05_cloth_warping/RealVisXL_V4.0.safetensors"
                ],
                "fallback_patterns": ["**/RealVis*.safetensors", "**/realvis*.safetensors"],
                "min_size_mb": 6500,
                "priority": 1
            },
            
            "vgg16_warping": {
                "step_class": "ClothWarpingStep",
                "ai_class": "RealVGGModel",
                "search_patterns": [
                    "vgg16_warping.pth",
                    "step_05_cloth_warping/vgg16_warping.pth"
                ],
                "fallback_patterns": ["**/vgg16*warp*.pth"],
                "min_size_mb": 500,
                "priority": 2
            },
            
            "vgg19_warping": {
                "step_class": "ClothWarpingStep", 
                "ai_class": "RealVGGModel",
                "search_patterns": [
                    "vgg19_warping.pth",
                    "step_05_cloth_warping/vgg19_warping.pth"
                ],
                "fallback_patterns": ["**/vgg19*warp*.pth"],
                "min_size_mb": 500,
                "priority": 3
            },
            
            "densenet121": {
                "step_class": "ClothWarpingStep",
                "ai_class": "RealDenseNetModel", 
                "search_patterns": [
                    "densenet121_warping.pth",
                    "step_05_cloth_warping/densenet121_warping.pth"
                ],
                "fallback_patterns": ["**/densenet121*.pth"],
                "min_size_mb": 30,
                "priority": 4
            },
            
            # Step 06: Virtual Fitting
            "ootdiffusion": {
                "step_class": "VirtualFittingStep",
                "ai_class": "RealOOTDDiffusionModel",
                "search_patterns": [
                    "diffusion_pytorch_model.bin",
                    "diffusion_pytorch_model.safetensors", 
                    "step_06_virtual_fitting/*/diffusion_pytorch_model.*"
                ],
                "fallback_patterns": ["**/diffusion_pytorch_model.*"],
                "min_size_mb": 3100,
                "priority": 1
            },
            
            # Step 07: Post Processing
            "post_processing_model": {
                "step_class": "PostProcessingStep",
                "ai_class": "RealGFPGANModel",
                "search_patterns": [
                    "sr_model.pth",
                    "step_07_post_processing/sr_model.pth",
                    "GFPGANv1.4.pth",
                    "Real-ESRGAN_x4plus.pth"
                ],
                "fallback_patterns": ["**/GFPGAN*.pth", "**/ESRGAN*.pth", "**/sr_model*.pth"],
                "min_size_mb": 300,
                "priority": 1
            },
            
            # Step 08: Quality Assessment
            "quality_assessment_clip": {
                "step_class": "QualityAssessmentStep",
                "ai_class": "RealCLIPModel",
                "search_patterns": [
                    "open_clip_pytorch_model.bin",
                    "step_08_quality_assessment/open_clip_pytorch_model.bin",
                    "ViT-L-14.pt"
                ],
                "fallback_patterns": ["**/open_clip*.bin", "**/ViT-L-14*.pt"],
                "min_size_mb": 5100,
                "priority": 1
            }
        }
    
    def get_model_path(self, model_name: str, force_refresh: bool = False) -> Optional[ModelMappingInfo]:
        """ğŸ”¥ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ + ë™ì  íƒì§€)"""
        try:
            with self._lock:
                current_time = time.time()
                
                # ìºì‹œ í™•ì¸
                if (not force_refresh and 
                    model_name in self.path_cache and 
                    current_time - self.last_scan_time < self.cache_valid_duration):
                    
                    self.mapping_stats["cache_hits"] += 1
                    return self.path_cache[model_name]
                
                # ìƒˆë¡œ íƒì§€
                mapping_info = self._discover_model_path(model_name)
                
                if mapping_info and mapping_info.actual_path:
                    self.path_cache[model_name] = mapping_info
                    self.mapping_stats["successful_mappings"] += 1
                    
                    self.logger.info(f"âœ… ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ ì„±ê³µ: {model_name} â†’ {mapping_info.actual_path} ({mapping_info.size_mb:.1f}MB)")
                    return mapping_info
                else:
                    self.mapping_stats["failed_mappings"] += 1
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ ê²½ë¡œ ì°¾ê¸° ì‹¤íŒ¨: {model_name}")
                    return None
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _discover_model_path(self, model_name: str) -> Optional[ModelMappingInfo]:
        """ëª¨ë¸ ê²½ë¡œ ë™ì  íƒì§€"""
        try:
            # í†µí•© ë§¤í•‘ì—ì„œ íŒ¨í„´ ê°€ì ¸ì˜¤ê¸°
            if model_name not in self.unified_model_mappings:
                # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                matching_key = self._find_partial_match(model_name)
                if not matching_key:
                    return self._fallback_discovery(model_name)
                model_name = matching_key
            
            mapping_config = self.unified_model_mappings[model_name]
            
            # ìš°ì„ ìˆœìœ„ ê²€ìƒ‰ íŒ¨í„´
            candidates = []
            
            # 1ë‹¨ê³„: ì •í™•í•œ íŒ¨í„´ ê²€ìƒ‰
            for pattern in mapping_config["search_patterns"]:
                found_files = self._search_by_pattern(pattern)
                candidates.extend(found_files)
            
            # 2ë‹¨ê³„: í´ë°± íŒ¨í„´ ê²€ìƒ‰
            if not candidates:
                for pattern in mapping_config.get("fallback_patterns", []):
                    found_files = self._search_by_pattern(pattern)
                    candidates.extend(found_files)
            
            # 3ë‹¨ê³„: ìµœì  í›„ë³´ ì„ íƒ
            if candidates:
                best_candidate = self._select_best_candidate(candidates, mapping_config)
                
                if best_candidate:
                    return ModelMappingInfo(
                        name=model_name,
                        actual_path=best_candidate[0],
                        step_class=mapping_config.get("step_class"),
                        ai_class=mapping_config.get("ai_class"),
                        size_mb=best_candidate[1],
                        priority_score=self._calculate_priority(best_candidate[1], mapping_config),
                        search_patterns=mapping_config["search_patterns"],
                        fallback_paths=mapping_config.get("fallback_patterns", [])
                    )
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨ {model_name}: {e}")
            return None
    
    def _search_by_pattern(self, pattern: str) -> List[Tuple[Path, float]]:
        """íŒ¨í„´ìœ¼ë¡œ íŒŒì¼ ê²€ìƒ‰"""
        try:
            candidates = []
            
            # glob íŒ¨í„´ ê²€ìƒ‰
            if '*' in pattern:
                for file_path in self.ai_models_root.glob(pattern):
                    if file_path.is_file():
                        size_mb = file_path.stat().st_size / (1024 * 1024)
                        candidates.append((file_path, size_mb))
            else:
                # ì§ì ‘ ê²½ë¡œ ê²€ìƒ‰
                direct_path = self.ai_models_root / pattern
                if direct_path.exists() and direct_path.is_file():
                    size_mb = direct_path.stat().st_size / (1024 * 1024) 
                    candidates.append((direct_path, size_mb))
            
            return candidates
            
        except Exception as e:
            self.logger.debug(f"íŒ¨í„´ ê²€ìƒ‰ ì‹¤íŒ¨ {pattern}: {e}")
            return []
    
    def _find_partial_match(self, model_name: str) -> Optional[str]:
        """ë¶€ë¶„ ë§¤ì¹­ìœ¼ë¡œ í‚¤ ì°¾ê¸°"""
        model_lower = model_name.lower()
        
        # ì§ì ‘ ë§¤ì¹­
        for key in self.unified_model_mappings.keys():
            if key.lower() == model_lower:
                return key
        
        # ë¶€ë¶„ ë§¤ì¹­
        for key in self.unified_model_mappings.keys():
            if key.lower() in model_lower or model_lower in key.lower():
                return key
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        keywords = model_lower.split('_')
        for key in self.unified_model_mappings.keys():
            key_keywords = key.lower().split('_')
            if any(kw in key_keywords for kw in keywords):
                return key
        
        return None
    
    def _select_best_candidate(self, candidates: List[Tuple[Path, float]], config: Dict[str, Any]) -> Optional[Tuple[Path, float]]:
        """ìµœì  í›„ë³´ ì„ íƒ"""
        if not candidates:
            return None
        
        min_size = config.get("min_size_mb", 0)
        
        # í¬ê¸° í•„í„°ë§
        valid_candidates = [(path, size) for path, size in candidates if size >= min_size]
        
        if not valid_candidates:
            # í¬ê¸° ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•Šì•„ë„ ê°€ì¥ í° ê²ƒ ì„ íƒ
            valid_candidates = candidates
        
        # í¬ê¸° ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬ (í° ê²ƒë¶€í„°)
        valid_candidates.sort(key=lambda x: x[1], reverse=True)
        
        return valid_candidates[0]
    
    def _calculate_priority(self, size_mb: float, config: Dict[str, Any]) -> float:
        """ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # í¬ê¸° ê¸°ë°˜ ì ìˆ˜
        import math
        if size_mb > 0:
            score += math.log10(max(size_mb, 1)) * 100
        
        # ì„¤ì • ê¸°ë°˜ ìš°ì„ ìˆœìœ„
        base_priority = config.get("priority", 5)
        score += (6 - base_priority) * 50
        
        # í¬ê¸° ì„ê³„ê°’ ë³´ë„ˆìŠ¤
        min_size = config.get("min_size_mb", 0)
        if size_mb >= min_size:
            score += 100
        
        return score
    
    def _fallback_discovery(self, model_name: str) -> Optional[ModelMappingInfo]:
        """í´ë°± íƒì§€ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        try:
            self.mapping_stats["dynamic_discoveries"] += 1
            
            keywords = model_name.lower().split('_')
            model_extensions = ['.pth', '.bin', '.safetensors', '.pt']
            
            candidates = []
            
            for ext in model_extensions:
                for file_path in self.ai_models_root.rglob(f"*{ext}"):
                    if file_path.is_file():
                        filename_lower = file_path.name.lower()
                        
                        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                        score = sum(1 for kw in keywords if kw in filename_lower)
                        if score > 0:
                            size_mb = file_path.stat().st_size / (1024 * 1024)
                            if size_mb >= 50:  # ìµœì†Œ 50MB
                                candidates.append((file_path, size_mb, score))
            
            if candidates:
                # ë§¤ì¹­ ì ìˆ˜ ìš°ì„ , í¬ê¸° ì°¨ì„ ìœ¼ë¡œ ì •ë ¬
                candidates.sort(key=lambda x: (x[2], x[1]), reverse=True)
                best = candidates[0]
                
                self.logger.info(f"ğŸ” ë™ì  íƒì§€ ì„±ê³µ: {model_name} â†’ {best[0]} (ì ìˆ˜: {best[2]})")
                
                return ModelMappingInfo(
                    name=model_name,
                    actual_path=best[0], 
                    size_mb=best[1],
                    priority_score=best[1] * best[2],
                    ai_class="BaseRealAIModel"
                )
            
            return None
            
        except Exception as e:
            self.logger.debug(f"í´ë°± íƒì§€ ì‹¤íŒ¨: {e}")
            return None
    
    def get_step_models(self, step_class: str) -> List[ModelMappingInfo]:
        """Stepë³„ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            step_models = []
            
            for model_name, config in self.unified_model_mappings.items():
                if config.get("step_class") == step_class:
                    mapping_info = self.get_model_path(model_name)
                    if mapping_info:
                        step_models.append(mapping_info)
            
            # ìš°ì„ ìˆœìœ„ ì ìˆ˜ë¡œ ì •ë ¬
            step_models.sort(key=lambda x: x.priority_score, reverse=True)
            
            return step_models
            
        except Exception as e:
            self.logger.error(f"âŒ Step ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨ {step_class}: {e}")
            return []
    
    def refresh_cache(self) -> Dict[str, Any]:
        """ìºì‹œ ìƒˆë¡œê³ ì¹¨"""
        try:
            with self._lock:
                old_cache_size = len(self.path_cache)
                self.path_cache.clear()
                self.last_scan_time = 0.0
                
                # ëª¨ë“  ëª¨ë¸ ì¬íƒì§€
                for model_name in self.unified_model_mappings.keys():
                    self.get_model_path(model_name, force_refresh=True)
                
                new_cache_size = len(self.path_cache)
                
                self.logger.info(f"âœ… ìºì‹œ ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ: {old_cache_size} â†’ {new_cache_size}")
                
                return {
                    "cache_refreshed": True,
                    "old_cache_size": old_cache_size,
                    "new_cache_size": new_cache_size,
                    "mapping_stats": self.mapping_stats
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨: {e}")
            return {"cache_refreshed": False, "error": str(e)}
    
    def get_mapping_statistics(self) -> Dict[str, Any]:
        """ë§¤í•‘ í†µê³„ ì¡°íšŒ"""
        try:
            total_cache_entries = len(self.path_cache)
            successful_mappings = sum(1 for info in self.path_cache.values() if info.actual_path)
            
            return {
                "ai_models_root": str(self.ai_models_root),
                "ai_models_root_exists": self.ai_models_root.exists(),
                "total_patterns": len(self.unified_model_mappings),
                "cache_entries": total_cache_entries,
                "successful_mappings": successful_mappings,
                "success_rate": successful_mappings / max(1, total_cache_entries),
                "last_scan_time": self.last_scan_time,
                "cache_valid": time.time() - self.last_scan_time < self.cache_valid_duration,
                "mapping_stats": self.mapping_stats.copy(),
                "step_classes": list(set(config.get("step_class") for config in self.unified_model_mappings.values())),
                "ai_classes": list(set(config.get("ai_class") for config in self.unified_model_mappings.values()))
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

# ==============================================
# ğŸ”¥ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
# ==============================================

_global_mapper: Optional[SmartModelPathMapper] = None
_mapper_lock = threading.Lock()

def get_global_smart_mapper(ai_models_root: Optional[Union[str, Path]] = None) -> SmartModelPathMapper:
    """ì „ì—­ SmartModelPathMapper ì¸ìŠ¤í„´ìŠ¤"""
    global _global_mapper
    
    with _mapper_lock:
        if _global_mapper is None or ai_models_root is not None:
            if ai_models_root is None:
                # ìë™ ê²½ë¡œ íƒì§€
                current_file = Path(__file__)
                backend_root = current_file.parents[3]  # backend/
                ai_models_root = backend_root / "ai_models"
            
            _global_mapper = SmartModelPathMapper(ai_models_root)
            
        return _global_mapper

def resolve_model_path(model_name: str) -> Optional[Path]:
    """ëª¨ë¸ ê²½ë¡œ í•´ê²° (ê°„í¸ í•¨ìˆ˜)"""
    mapper = get_global_smart_mapper()
    mapping_info = mapper.get_model_path(model_name)
    return mapping_info.actual_path if mapping_info else None

def get_step_model_paths(step_class: str) -> Dict[str, Path]:
    """Stepë³„ ëª¨ë¸ ê²½ë¡œë“¤"""
    mapper = get_global_smart_mapper()
    step_models = mapper.get_step_models(step_class)
    
    return {
        model.name: model.actual_path 
        for model in step_models 
        if model.actual_path
    }

# Export
__all__ = [
    'SmartModelPathMapper',
    'ModelMappingInfo', 
    'get_global_smart_mapper',
    'resolve_model_path',
    'get_step_model_paths'
]

logger.info("âœ… SmartModelPathMapper í†µí•© ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ”¥ ë™ì  ê²½ë¡œ íƒì§€ë¡œ ì›Œë‹ ì œê±°")
logger.info("ğŸ¯ ModelLoader + AutoDetector ì™„ì „ í†µí•©")