# ============================================================================
# ğŸ MyCloset AI - ì™„ì „í•œ __init__.py íŒŒì¼ ì‹œìŠ¤í…œ
# ============================================================================
# conda í™˜ê²½ ìš°ì„  + ìˆœí™˜ì°¸ì¡° í•´ê²° + ì—­í• ë³„ ë¶„ë¦¬ + ì§€ì—° ë¡œë”© íŒ¨í„´

# ============================================================================
# ğŸ“ backend/app/__init__.py - ë©”ì¸ íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
# ============================================================================

"""
ğŸ MyCloset AI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ íŒ¨í‚¤ì§€ v7.0
==============================================

âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™”
âœ… Python Path ìë™ ì„¤ì • (Import ì˜¤ë¥˜ ì™„ì „ í•´ê²°) 
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ì™„ì „ í™œìš©
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ (ì§€ì—° ë¡œë”© íŒ¨í„´)
âœ… 89.8GB AI ëª¨ë¸ ê²½ë¡œ ìë™ ì„¤ì •
âœ… ë ˆì´ì–´ ì•„í‚¤í…ì²˜ ì§€ì› (API â†’ Service â†’ Pipeline â†’ AI)
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ëª¨ë“  í•˜ìœ„ ëª¨ë“ˆ ì•ˆì „í•œ ë¡œë”©

í”„ë¡œì íŠ¸ êµ¬ì¡°:
backend/                   â† ì‘ì—… ë””ë ‰í† ë¦¬ (ì—¬ê¸°ì„œ python app/main.py ì‹¤í–‰)
â”œâ”€â”€ app/                   â† íŒ¨í‚¤ì§€ ë£¨íŠ¸ (ì´ íŒŒì¼ì˜ ìœ„ì¹˜)
â”‚   â”œâ”€â”€ __init__.py       â† ì´ íŒŒì¼!
â”‚   â”œâ”€â”€ main.py           â† FastAPI ì„œë²„ ì§„ì…ì 
â”‚   â”œâ”€â”€ core/             â† í•µì‹¬ ì„¤ì • ë° ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ services/         â† ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì„œë¹„ìŠ¤ë“¤
â”‚   â”œâ”€â”€ ai_pipeline/      â† AI íŒŒì´í”„ë¼ì¸ (8ë‹¨ê³„)
â”‚   â”œâ”€â”€ api/              â† REST API ì—”ë“œí¬ì¸íŠ¸
â”‚   â”œâ”€â”€ models/           â† ë°ì´í„° ëª¨ë¸/ìŠ¤í‚¤ë§ˆ
â”‚   â””â”€â”€ utils/            â† ê³µí†µ ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ ai_models/            â† AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ (89.8GB)
â””â”€â”€ static/               â† ì •ì  íŒŒì¼ ë° ì—…ë¡œë“œ

ì‘ì„±ì: MyCloset AI Team
ë‚ ì§œ: 2025-07-23
ë²„ì „: v7.0.0 (Complete Init System with Conda Priority)
"""

import os
import sys
import logging
import platform
import subprocess
import threading
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, List, Callable
import warnings

# =============================================================================
# ğŸ”¥ Step 1: conda í™˜ê²½ ìš°ì„  ì²´í¬ ë° ì„¤ì •
# =============================================================================

# conda í™˜ê²½ ê°ì§€
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', 'none')
CONDA_PREFIX = os.environ.get('CONDA_PREFIX', '')
IS_CONDA = CONDA_ENV != 'none' or bool(CONDA_PREFIX)

if IS_CONDA:
    print(f"ğŸ conda í™˜ê²½ ê°ì§€: {CONDA_ENV} at {CONDA_PREFIX}")
    
    # conda ìš°ì„  ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ì„¤ì •
    if CONDA_PREFIX:
        python_version = f"python{sys.version_info.major}.{sys.version_info.minor}"
        conda_site_packages = os.path.join(CONDA_PREFIX, 'lib', python_version, 'site-packages')
        if os.path.exists(conda_site_packages) and conda_site_packages not in sys.path:
            sys.path.insert(0, conda_site_packages)
            print(f"âœ… conda site-packages ê²½ë¡œ ì¶”ê°€: {conda_site_packages}")
    
    # conda í™˜ê²½ ìµœì í™” ì„¤ì •
    os.environ.setdefault('OMP_NUM_THREADS', str(max(1, os.cpu_count() // 2)))
    os.environ.setdefault('MKL_NUM_THREADS', str(max(1, os.cpu_count() // 2)))
    os.environ.setdefault('NUMEXPR_NUM_THREADS', str(max(1, os.cpu_count() // 2)))
else:
    print("âš ï¸ conda í™˜ê²½ì´ ë¹„í™œì„±í™”ë¨ - 'conda activate <í™˜ê²½ëª…>' ê¶Œì¥")

# =============================================================================
# ğŸ”¥ Step 2: íŒ¨í‚¤ì§€ ê²½ë¡œ ë° í™˜ê²½ ìë™ ì„¤ì •
# =============================================================================

# í˜„ì¬ íŒ¨í‚¤ì§€ì˜ ì ˆëŒ€ ê²½ë¡œ
_current_package_dir = Path(__file__).parent.absolute()
_backend_root = _current_package_dir.parent
_project_root = _backend_root.parent

# Python Path ìë™ ì„¤ì • (Import ì˜¤ë¥˜ í•´ê²°)
_paths_to_add = [
    str(_backend_root),           # backend/ ê²½ë¡œ (ê°€ì¥ ì¤‘ìš”!)
    str(_current_package_dir),    # backend/app/ ê²½ë¡œ
    str(_project_root),           # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
]

for path in _paths_to_add:
    if path not in sys.path:
        sys.path.insert(0, path)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (conda í™˜ê²½ ê³ ë ¤)
os.environ.update({
    'PYTHONPATH': f"{_backend_root}:{os.environ.get('PYTHONPATH', '')}",
    'PROJECT_ROOT': str(_project_root),
    'BACKEND_ROOT': str(_backend_root),
    'APP_ROOT': str(_current_package_dir),
    'MYCLOSET_CONDA_ENV': CONDA_ENV if IS_CONDA else 'none'
})

# ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ backendë¡œ ì„¤ì •
if Path.cwd() != _backend_root:
    try:
        os.chdir(_backend_root)
        print(f"âœ… ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •: {_backend_root}")
    except Exception as e:
        warnings.warn(f"ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì • ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ Step 3: ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ (conda í™˜ê²½ ìµœì í™”)
# =============================================================================

def _detect_m3_max() -> bool:
    """M3 Max ì¹© ê°ì§€"""
    try:
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            chip_info = result.stdout.strip()
            return 'M3' in chip_info and ('Max' in chip_info or 'Pro' in chip_info)
    except Exception:
        pass
    return False

def _get_memory_gb() -> float:
    """ë©”ëª¨ë¦¬ ìš©ëŸ‰ ê°ì§€"""
    try:
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'hw.memsize'],
                capture_output=True, text=True, timeout=5
            )
            return int(result.stdout.strip()) / (1024**3)
        else:
            try:
                import psutil
                return psutil.virtual_memory().total / (1024**3)
            except ImportError:
                return 16.0
    except Exception:
        return 16.0

def _detect_device() -> str:
    """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€ (conda í™˜ê²½ ìš°ì„ )"""
    try:
        # conda PyTorch ì²´í¬
        import torch
        
        # M3 Max MPS ìš°ì„ 
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            # conda í™˜ê²½ì—ì„œ MPS ìµœì í™” ì„¤ì •
            if IS_CONDA:
                os.environ.setdefault('PYTORCH_ENABLE_MPS_FALLBACK', '1')
                os.environ.setdefault('PYTORCH_MPS_HIGH_WATERMARK_RATIO', '0.0')
            return 'mps'
        elif torch.cuda.is_available():
            return 'cuda'
        else:
            return 'cpu'
    except ImportError:
        return 'cpu'

# ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
SYSTEM_INFO = {
    'python_version': platform.python_version(),
    'platform': platform.platform(),
    'architecture': platform.architecture()[0],
    'cpu_count': os.cpu_count(),
    'memory_gb': _get_memory_gb(),
    'is_m3_max': _detect_m3_max(),
    'device': _detect_device(),
    'is_conda': IS_CONDA,
    'conda_env': CONDA_ENV,
    'conda_prefix': CONDA_PREFIX,
    'backend_root': str(_backend_root),
    'app_root': str(_current_package_dir)
}

# AI ëª¨ë¸ ê²½ë¡œ ì„¤ì •
AI_MODEL_PATHS = {
    'ai_models_root': _backend_root / 'ai_models',
    'checkpoints': _backend_root / 'ai_models' / 'checkpoints',
    'configs': _backend_root / 'ai_models' / 'configs',
    'weights': _backend_root / 'ai_models' / 'weights'
}

# AI ëª¨ë¸ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ.setdefault("MYCLOSET_AI_MODELS_PATH", str(AI_MODEL_PATHS['ai_models_root']))

# =============================================================================
# ğŸ”¥ Step 4: ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì • (conda í™˜ê²½ ìµœì í™”)
# =============================================================================

def _setup_logging():
    """ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    logger = logging.getLogger('mycloset_ai')
    
    if logger.handlers:
        return logger
    
    logger.setLevel(logging.INFO)
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬
    log_dir = _backend_root / 'logs'
    log_dir.mkdir(exist_ok=True)
    
    # í¬ë§·í„° (conda í™˜ê²½ ì •ë³´ í¬í•¨)
    conda_info = f"conda:{CONDA_ENV}" if IS_CONDA else "pip"
    formatter = logging.Formatter(
        f'%(asctime)s - {conda_info} - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ (ì„ íƒì )
    try:
        from logging.handlers import RotatingFileHandler
        file_handler = RotatingFileHandler(
            log_dir / "app.log",
            maxBytes=10*1024*1024,  # 10MB
            backupCount=5
        )
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    except Exception as e:
        warnings.warn(f"íŒŒì¼ ë¡œê·¸ í•¸ë“¤ëŸ¬ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    return logger

# íŒ¨í‚¤ì§€ ë¡œê±° ì´ˆê¸°í™”
logger = _setup_logging()

# =============================================================================
# ğŸ”¥ Step 5: ì§€ì—° ë¡œë”© ì‹œìŠ¤í…œ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# =============================================================================

class LazyLoader:
    """ì§€ì—° ë¡œë”© í—¬í¼ í´ë˜ìŠ¤ - ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€"""
    
    def __init__(self):
        self._cache = {}
        self._loading = set()
        self._lock = threading.RLock()
    
    def load_module(self, module_name: str, package: str = None):
        """ëª¨ë“ˆ ì§€ì—° ë¡œë”©"""
        cache_key = f"{package}.{module_name}" if package else module_name
        
        with self._lock:
            # ìºì‹œì—ì„œ í™•ì¸
            if cache_key in self._cache:
                return self._cache[cache_key]
            
            # ìˆœí™˜ ë¡œë”© ë°©ì§€
            if cache_key in self._loading:
                logger.warning(f"ìˆœí™˜ì°¸ì¡° ê°ì§€: {cache_key}")
                return None
            
            self._loading.add(cache_key)
            
            try:
                import importlib
                full_module_name = f"{package}.{module_name}" if package else module_name
                module = importlib.import_module(full_module_name)
                self._cache[cache_key] = module
                logger.debug(f"âœ… ì§€ì—° ë¡œë”© ì„±ê³µ: {cache_key}")
                return module
                
            except ImportError as e:
                logger.debug(f"âš ï¸ ì§€ì—° ë¡œë”© ì‹¤íŒ¨: {cache_key} - {e}")
                self._cache[cache_key] = None
                return None
            
            except Exception as e:
                logger.error(f"âŒ ì§€ì—° ë¡œë”© ì˜¤ë¥˜: {cache_key} - {e}")
                self._cache[cache_key] = None
                return None
            
            finally:
                self._loading.discard(cache_key)
    
    def get_class(self, module_name: str, class_name: str, package: str = None):
        """í´ë˜ìŠ¤ ì§€ì—° ë¡œë”©"""
        module = self.load_module(module_name, package)
        if module:
            return getattr(module, class_name, None)
        return None

# ì „ì—­ ì§€ì—° ë¡œë”
_lazy_loader = LazyLoader()

# =============================================================================
# ğŸ”¥ Step 6: í•µì‹¬ ëª¨ë“ˆ ì§€ì—° ë¡œë”© (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# =============================================================================

def get_config():
    """Config ëª¨ë“ˆ ì§€ì—° ë¡œë”©"""
    return _lazy_loader.load_module('config', 'app.core')

def get_gpu_config():
    """GPU Config ëª¨ë“ˆ ì§€ì—° ë¡œë”©"""
    return _lazy_loader.load_module('gpu_config', 'app.core')

def get_session_manager():
    """Session Manager ëª¨ë“ˆ ì§€ì—° ë¡œë”©"""
    return _lazy_loader.load_module('session_manager', 'app.core')

def get_model_loader():
    """Model Loader ëª¨ë“ˆ ì§€ì—° ë¡œë”©"""
    return _lazy_loader.load_module('model_loader', 'app.ai_pipeline.utils')

def get_pipeline_manager():
    """Pipeline Manager ëª¨ë“ˆ ì§€ì—° ë¡œë”©"""
    return _lazy_loader.load_module('pipeline_manager', 'app.ai_pipeline')

def get_file_manager():
    """File Manager ëª¨ë“ˆ ì§€ì—° ë¡œë”©"""
    return _lazy_loader.load_module('file_manager', 'app.utils')

def get_image_utils():
    """Image Utils ëª¨ë“ˆ ì§€ì—° ë¡œë”©"""
    return _lazy_loader.load_module('image_utils', 'app.utils')

# ì§€ì—° ë¡œë”© í¸ì˜ í•¨ìˆ˜ë“¤
def safe_import(module_name: str, package: str = None):
    """ì•ˆì „í•œ ëª¨ë“ˆ import (ì§€ì—° ë¡œë”©)"""
    return _lazy_loader.load_module(module_name, package)

def safe_get_class(module_name: str, class_name: str, package: str = None):
    """ì•ˆì „í•œ í´ë˜ìŠ¤ import (ì§€ì—° ë¡œë”©)"""
    return _lazy_loader.get_class(module_name, class_name, package)

# =============================================================================
# ğŸ”¥ Step 7: íŒ¨í‚¤ì§€ ë©”íƒ€ë°ì´í„° ë° ì •ë³´
# =============================================================================

__version__ = "7.0.0"
__author__ = "MyCloset AI Team"
__description__ = "AI-powered Virtual Try-On Platform with Conda Priority"
__license__ = "Proprietary"

# íŒ¨í‚¤ì§€ ì •ë³´
PACKAGE_INFO = {
    'name': 'MyCloset AI',
    'version': __version__,
    'description': __description__,
    'author': __author__,
    'backend_root': str(_backend_root),
    'app_root': str(_current_package_dir),
    'python_version': SYSTEM_INFO['python_version'],
    'conda_env': SYSTEM_INFO['conda_env'],
    'is_conda': SYSTEM_INFO['is_conda'],
    'is_m3_max': SYSTEM_INFO['is_m3_max'],
    'device': SYSTEM_INFO['device'],
    'memory_gb': SYSTEM_INFO['memory_gb'],
    'ai_models_available': AI_MODEL_PATHS['ai_models_root'].exists()
}

# =============================================================================
# ğŸ”¥ Step 8: ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def get_package_info() -> Dict[str, Any]:
    """íŒ¨í‚¤ì§€ ì •ë³´ ë°˜í™˜"""
    return PACKAGE_INFO.copy()

def get_system_info() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
    return SYSTEM_INFO.copy()

def get_ai_model_paths() -> Dict[str, Path]:
    """AI ëª¨ë¸ ê²½ë¡œ ì •ë³´ ë°˜í™˜"""
    return AI_MODEL_PATHS.copy()

def is_conda_environment() -> bool:
    """conda í™˜ê²½ ì—¬ë¶€ í™•ì¸"""
    return SYSTEM_INFO['is_conda']

def is_m3_max() -> bool:
    """M3 Max í™˜ê²½ ì—¬ë¶€ í™•ì¸"""
    return SYSTEM_INFO['is_m3_max']

def get_device() -> str:
    """ì¶”ì²œ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return SYSTEM_INFO['device']

def validate_environment() -> Dict[str, Any]:
    """í™˜ê²½ ê²€ì¦ ë° ìƒíƒœ ë°˜í™˜"""
    validation = {
        'python_path_ok': str(_backend_root) in sys.path,
        'working_directory_ok': Path.cwd() == _backend_root,
        'ai_models_exist': AI_MODEL_PATHS['ai_models_root'].exists(),
        'conda_environment': SYSTEM_INFO['is_conda'],
        'conda_env_name': SYSTEM_INFO['conda_env'],
        'device_available': SYSTEM_INFO['device'] != 'cpu',
        'memory_sufficient': SYSTEM_INFO['memory_gb'] >= 8.0,
        'lazy_loader_ready': _lazy_loader is not None
    }
    
    validation['overall_status'] = all([
        validation['python_path_ok'],
        validation['working_directory_ok'],
        validation['memory_sufficient']
    ])
    
    return validation

# =============================================================================
# ğŸ”¥ Step 9: ì´ˆê¸°í™” ìƒíƒœ ì¶œë ¥
# =============================================================================

def _print_initialization_status():
    """ì´ˆê¸°í™” ìƒíƒœ ì¶œë ¥ (conda í™˜ê²½ ìš°ì„ )"""
    print(f"\nğŸ MyCloset AI íŒ¨í‚¤ì§€ ì´ˆê¸°í™” ì™„ë£Œ!")
    print(f"ğŸ“¦ ë²„ì „: {__version__}")
    print(f"ğŸ conda í™˜ê²½: {'âœ…' if IS_CONDA else 'âŒ'} ({CONDA_ENV})")
    print(f"ğŸ M3 Max: {'âœ…' if SYSTEM_INFO['is_m3_max'] else 'âŒ'}")
    print(f"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {SYSTEM_INFO['device']}")
    print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {SYSTEM_INFO['memory_gb']:.1f}GB")
    print(f"ğŸ“ AI ëª¨ë¸ ê²½ë¡œ: {AI_MODEL_PATHS['ai_models_root']}")
    print(f"ğŸ”— ì§€ì—° ë¡œë”©: âœ… í™œì„±í™”")
    print(f"ğŸ”§ Python Path: âœ… ì„¤ì • ì™„ë£Œ")
    
    if IS_CONDA:
        print(f"ğŸ conda ìµœì í™”: âœ… í™œì„±í™”")
        print(f"ğŸ conda ê²½ë¡œ: {CONDA_PREFIX}")
    else:
        print(f"âš ï¸  conda ë¹„í™œì„±í™” - ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ conda ì‚¬ìš© ê¶Œì¥")

# ì´ˆê¸°í™” ìƒíƒœ ì¶œë ¥
_print_initialization_status()

# =============================================================================
# ğŸ”¥ Step 10: íŒ¨í‚¤ì§€ Export
# =============================================================================

__all__ = [
    # ğŸ”¥ ë²„ì „ ì •ë³´
    '__version__',
    '__author__',
    '__description__',
    
    # ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´
    'SYSTEM_INFO',
    'PACKAGE_INFO',
    'AI_MODEL_PATHS',
    'IS_CONDA',
    'CONDA_ENV',
    
    # ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_package_info',
    'get_system_info', 
    'get_ai_model_paths',
    'is_conda_environment',
    'is_m3_max',
    'get_device',
    'validate_environment',
    
    # ğŸš€ ì§€ì—° ë¡œë”© í•¨ìˆ˜ë“¤
    'get_config',
    'get_gpu_config',
    'get_session_manager',
    'get_model_loader',
    'get_pipeline_manager',
    'get_file_manager',
    'get_image_utils',
    'safe_import',
    'safe_get_class',
]

logger.info("ğŸ‰ MyCloset AI ë©”ì¸ íŒ¨í‚¤ì§€ ì´ˆê¸°í™” ì™„ë£Œ!")
logger.info(f"ğŸ conda í™˜ê²½: {IS_CONDA} ({CONDA_ENV})")
logger.info(f"ğŸ M3 Max ìµœì í™”: {SYSTEM_INFO['is_m3_max']}")
logger.info(f"ğŸ”— ì§€ì—° ë¡œë”© ì‹œìŠ¤í…œ: í™œì„±í™”")
