# app/main.py
"""
MyCloset AI Backend - M3 Max 128GB ìµœì í™” ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
ì™„ì „í•œ ê¸°ëŠ¥ êµ¬í˜„ - WebSocket, ê°€ìƒí”¼íŒ… API, ëª¨ë“  ë¼ìš°í„° í¬í•¨
âœ… Import ì˜¤ë¥˜ í•´ê²°
âœ… ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì¶”ê°€
âœ… í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥
"""

import sys
import os
import logging
import asyncio
import traceback
import json
import gc
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
from contextlib import asynccontextmanager

# ì‹œê°„ ëª¨ë“ˆ ì•ˆì „ import
import time as time_module

# Python ê²½ë¡œ ì„¤ì •
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(current_dir))
sys.path.insert(0, str(project_root))

print("ğŸ M3 Max ìµœì í™” MyCloset AI Backend ì‹œì‘...")
print(f"ğŸ“ App Dir: {current_dir}")
print(f"ğŸ“ Project Root: {project_root}")

# FastAPI imports
try:
    from fastapi import FastAPI, HTTPException, Request, Depends, BackgroundTasks, UploadFile, File, Form
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.staticfiles import StaticFiles
    from fastapi.responses import JSONResponse, HTMLResponse
    from fastapi.exceptions import RequestValidationError
    from starlette.exceptions import HTTPException as StarletteHTTPException
    print("âœ… FastAPI import ì„±ê³µ")
except ImportError as e:
    print(f"âŒ FastAPI import ì‹¤íŒ¨: {e}")
    sys.exit(1)

# Pydantic V2 imports
try:
    from pydantic import ValidationError
    print("âœ… Pydantic V2 import ì„±ê³µ")
except ImportError as e:
    print(f"âŒ Pydantic import ì‹¤íŒ¨: {e}")
    sys.exit(1)

# ë¡œê¹… ì„¤ì •
def setup_logging():
    """M3 Max ìµœì í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    log_dir = project_root / "logs"
    log_dir.mkdir(exist_ok=True)
    
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler(
        log_dir / f"mycloset-ai-m3max-{datetime.now().strftime('%Y%m%d')}.log",
        encoding='utf-8',
        delay=True
    )
    file_handler.setFormatter(logging.Formatter(log_format))
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(log_format))
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    return logging.getLogger(__name__)

# ë¡œê¹… ì´ˆê¸°í™”
logger = setup_logging()

# ============================================
# ğŸ”§ ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì¶”ê°€ - ì¦‰ì‹œ ìˆ˜ì •
# ============================================

def add_missing_functions():
    """ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì¦‰ì‹œ ì¶”ê°€"""
    
    # 1. GPU Configì— get_device_config í•¨ìˆ˜ ì¶”ê°€
    try:
        import app.core.gpu_config as gpu_config_module
        
        if not hasattr(gpu_config_module, 'get_device_config'):
            def get_device_config(device=None, **kwargs):
                """ë””ë°”ì´ìŠ¤ ì„¤ì • ì¡°íšŒ - í•˜ìœ„ í˜¸í™˜ì„± í•¨ìˆ˜"""
                try:
                    if hasattr(gpu_config_module, 'get_gpu_config'):
                        config = gpu_config_module.get_gpu_config(**kwargs)
                        return {
                            'device': config.get_device(),
                            'device_type': config.device_info.device_type,
                            'memory_info': config.get_memory_info(),
                            'device_info': config.get_device_info(),
                            'system_info': config.system_info,
                            'optimization_enabled': config.enable_optimization
                        }
                    else:
                        return {
                            'device': device or 'cpu',
                            'device_type': 'cpu',
                            'memory_info': {'total_gb': 16.0},
                            'device_info': {'device': 'cpu'},
                            'system_info': {'platform': 'unknown'},
                            'optimization_enabled': False
                        }
                except Exception as e:
                    logger.warning(f"get_device_config í´ë°± ëª¨ë“œ: {e}")
                    return {'device': 'cpu', 'device_type': 'cpu'}
            
            # í•¨ìˆ˜ ë™ì  ì¶”ê°€
            setattr(gpu_config_module, 'get_device_config', get_device_config)
            logger.info("âœ… get_device_config í•¨ìˆ˜ ë™ì  ì¶”ê°€ ì™„ë£Œ")
    
    except Exception as e:
        logger.warning(f"âš ï¸ GPU config í•¨ìˆ˜ ì¶”ê°€ ì‹¤íŒ¨: {e}")
    
    # 2. Memory Managerì— create_memory_manager í•¨ìˆ˜ ì¶”ê°€
    try:
        import app.ai_pipeline.utils.memory_manager as memory_module
        
        if not hasattr(memory_module, 'create_memory_manager'):
            def create_memory_manager(device=None, memory_gb=16.0, **kwargs):
                """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ìƒì„± - íŒ©í† ë¦¬ í•¨ìˆ˜"""
                if hasattr(memory_module, 'MemoryManager'):
                    return memory_module.MemoryManager(
                        device=device,
                        memory_gb=memory_gb,
                        **kwargs
                    )
                else:
                    # í´ë°± ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €
                    class FallbackMemoryManager:
                        def __init__(self, device=None, **kwargs):
                            self.device = device or 'cpu'
                        
                        def optimize_memory(self):
                            gc.collect()
                            return {'success': True, 'device': self.device}
                    
                    return FallbackMemoryManager(device=device, **kwargs)
            
            def get_memory_manager(device=None, **kwargs):
                """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
                return create_memory_manager(device=device, **kwargs)
            
            def optimize_memory_usage(device="auto", aggressive=False):
                """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
                try:
                    if device == "mps" or device == "auto":
                        import torch
                        if torch.backends.mps.is_available():
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                    elif device == "cuda":
                        import torch
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    
                    if aggressive:
                        gc.collect()
                    
                    return {
                        "success": True,
                        "device": device,
                        "aggressive": aggressive
                    }
                except Exception as e:
                    return {"success": False, "error": str(e)}
            
            # í•¨ìˆ˜ë“¤ ë™ì  ì¶”ê°€
            setattr(memory_module, 'create_memory_manager', create_memory_manager)
            setattr(memory_module, 'get_memory_manager', get_memory_manager)
            setattr(memory_module, 'optimize_memory_usage', optimize_memory_usage)
            logger.info("âœ… Memory Manager í•¨ìˆ˜ë“¤ ë™ì  ì¶”ê°€ ì™„ë£Œ")
    
    except Exception as e:
        logger.warning(f"âš ï¸ Memory Manager í•¨ìˆ˜ ì¶”ê°€ ì‹¤íŒ¨: {e}")
    
    # 3. Model Loaderì— ModelFormat í´ë˜ìŠ¤ ì¶”ê°€
    try:
        import app.ai_pipeline.utils.model_loader as model_module
        
        if not hasattr(model_module, 'ModelFormat'):
            class ModelFormat:
                """ëª¨ë¸ í¬ë§· ìƒìˆ˜ í´ë˜ìŠ¤"""
                PYTORCH = "pytorch"
                COREML = "coreml" 
                ONNX = "onnx"
                TORCHSCRIPT = "torchscript"
                TENSORFLOW = "tensorflow"
                
                @classmethod
                def get_available_formats(cls):
                    return [cls.PYTORCH, cls.COREML, cls.ONNX, cls.TORCHSCRIPT, cls.TENSORFLOW]
                
                @classmethod
                def is_valid_format(cls, format_name):
                    return format_name in cls.get_available_formats()
            
            def create_model_loader(device=None, **kwargs):
                """ëª¨ë¸ ë¡œë” ìƒì„± - íŒ©í† ë¦¬ í•¨ìˆ˜"""
                if hasattr(model_module, 'ModelLoader'):
                    return model_module.ModelLoader(device=device, **kwargs)
                else:
                    # í´ë°± ëª¨ë¸ ë¡œë”
                    class FallbackModelLoader:
                        def __init__(self, device=None, **kwargs):
                            self.device = device or 'cpu'
                        
                        def load_model(self, model_path, model_format=ModelFormat.PYTORCH):
                            return {'loaded': True, 'device': self.device}
                    
                    return FallbackModelLoader(device=device, **kwargs)
            
            # í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ ë™ì  ì¶”ê°€
            setattr(model_module, 'ModelFormat', ModelFormat)
            setattr(model_module, 'create_model_loader', create_model_loader)
            logger.info("âœ… ModelFormat í´ë˜ìŠ¤ ë™ì  ì¶”ê°€ ì™„ë£Œ")
    
    except Exception as e:
        logger.warning(f"âš ï¸ Model Loader í´ë˜ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨: {e}")

# ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì¦‰ì‹œ ì¶”ê°€
add_missing_functions()

# ============================================
# M3 Max ì»´í¬ë„ŒíŠ¸ Import ì‹œìŠ¤í…œ
# ============================================

class M3MaxComponentImporter:
    """M3 Max ìµœì í™”ëœ ì»´í¬ë„ŒíŠ¸ import ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.components = {}
        self.import_errors = []
        self.fallback_mode = False
        self.m3_max_optimized = False
        
        # M3 Max ê°ì§€
        self._detect_m3_max()
    
    def _detect_m3_max(self):
        """M3 Max í™˜ê²½ ê°ì§€"""
        try:
            import platform
            import psutil
            
            if platform.machine() == 'arm64' and platform.system() == 'Darwin':
                memory_gb = psutil.virtual_memory().total / (1024**3)
                if memory_gb >= 120:
                    self.m3_max_optimized = True
                    logger.info("ğŸ M3 Max 128GB í™˜ê²½ ê°ì§€ - ìµœì í™” ëª¨ë“œ í™œì„±í™”")
                else:
                    logger.info(f"ğŸ Apple Silicon ê°ì§€ - ë©”ëª¨ë¦¬: {memory_gb:.0f}GB")
            
        except Exception as e:
            logger.warning(f"âš ï¸ í™˜ê²½ ê°ì§€ ì‹¤íŒ¨: {e}")
    
    def safe_import_schemas(self):
        """ìŠ¤í‚¤ë§ˆ ì•ˆì „ import"""
        try:
            from app.models.schemas import (
                VirtualTryOnRequest, VirtualTryOnResponse,
                ProcessingStatus, ProcessingResult,
                ErrorResponse, SystemHealth, PerformanceMetrics
            )
            
            self.components['schemas'] = {
                'VirtualTryOnRequest': VirtualTryOnRequest,
                'VirtualTryOnResponse': VirtualTryOnResponse,
                'ProcessingStatus': ProcessingStatus,
                'ProcessingResult': ProcessingResult,
                'ErrorResponse': ErrorResponse,
                'SystemHealth': SystemHealth,
                'PerformanceMetrics': PerformanceMetrics
            }
            
            logger.info("âœ… ìŠ¤í‚¤ë§ˆ import ì„±ê³µ")
            return True
            
        except Exception as e:
            error_msg = f"ìŠ¤í‚¤ë§ˆ import ì‹¤íŒ¨: {e}"
            self.import_errors.append(error_msg)
            logger.error(f"âŒ {error_msg}")
            self._create_fallback_schemas()
            return False
    
    def _create_fallback_schemas(self):
        """í´ë°± ìŠ¤í‚¤ë§ˆ ìƒì„±"""
        from pydantic import BaseModel
        from typing import Optional, Dict, Any
        
        class FallbackModel(BaseModel):
            success: bool = True
            message: str = "Fallback mode"
            data: Optional[Dict[str, Any]] = None
        
        self.components['schemas'] = {
            'VirtualTryOnRequest': FallbackModel,
            'VirtualTryOnResponse': FallbackModel,
            'ProcessingStatus': FallbackModel,
            'ProcessingResult': FallbackModel,
            'ErrorResponse': FallbackModel,
            'SystemHealth': FallbackModel,
            'PerformanceMetrics': FallbackModel
        }
        
        self.fallback_mode = True
        logger.warning("ğŸš¨ í´ë°± ìŠ¤í‚¤ë§ˆ ëª¨ë“œë¡œ ì „í™˜")
    
    def safe_import_gpu_config(self):
        """GPU ì„¤ì • ì•ˆì „ import - ìˆ˜ì •ëœ ë²„ì „"""
        try:
            # ğŸ”§ ìˆ˜ì •: ì´ì œ get_device_configê°€ ì¶”ê°€ë˜ì–´ ìˆìŒ
            from app.core.gpu_config import (
                gpu_config, DEVICE, MODEL_CONFIG, 
                DEVICE_INFO, get_device_config,
                get_device, get_optimal_settings
            )
            
            # get_device_info í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´ ìƒì„±
            try:
                from app.core.gpu_config import get_device_info
            except ImportError:
                def get_device_info():
                    return DEVICE_INFO
            
            # get_model_config í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´ ìƒì„±
            try:
                from app.core.gpu_config import get_model_config
            except ImportError:
                def get_model_config():
                    return MODEL_CONFIG
            
            def optimize_memory(device=None, aggressive=False):
                """M3 Max ë©”ëª¨ë¦¬ ìµœì í™”"""
                try:
                    import torch
                    
                    if device == 'mps' or (device is None and torch.backends.mps.is_available()):
                        gc.collect()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        
                        return {
                            "success": True, 
                            "device": "mps", 
                            "method": "m3_max_optimization",
                            "aggressive": aggressive,
                            "memory_optimized": True
                        }
                    else:
                        gc.collect()
                        return {
                            "success": True, 
                            "device": device or "cpu", 
                            "method": "standard_gc"
                        }
                except Exception as e:
                    return {"success": False, "error": str(e)}
            
            self.components['gpu_config'] = {
                'instance': gpu_config,
                'device': DEVICE,
                'model_config': MODEL_CONFIG,
                'device_info': DEVICE_INFO,
                'get_config': get_device_config,  # âœ… ì´ì œ ì¡´ì¬í•¨
                'get_device': get_device,
                'get_model_config': get_model_config,
                'get_device_info': get_device_info,
                'optimize_memory': optimize_memory,
                'm3_max_optimized': self.m3_max_optimized and DEVICE == 'mps'
            }
            
            logger.info(f"âœ… GPU ì„¤ì • import ì„±ê³µ (M3 Max: {self.components['gpu_config']['m3_max_optimized']})")
            return True
            
        except ImportError as e:
            error_msg = f"GPU ì„¤ì • import ì‹¤íŒ¨: {e}"
            self.import_errors.append(error_msg)
            logger.warning(f"âš ï¸ {error_msg}")
            
            # í´ë°± GPU ì„¤ì •
            self.components['gpu_config'] = {
                'instance': None,
                'device': "cpu",
                'model_config': {"device": "cpu", "dtype": "float32"},
                'device_info': {
                    "device": "cpu",
                    "name": "CPU",
                    "memory_gb": 0,
                    "is_m3_max": False
                },
                'get_config': lambda: {"device": "cpu"},
                'get_device': lambda: "cpu",
                'get_model_config': lambda: {"device": "cpu"},
                'get_device_info': lambda: {"device": "cpu"},
                'optimize_memory': lambda device=None, aggressive=False: {
                    "success": False, 
                    "error": "GPU config not available"
                },
                'm3_max_optimized': False
            }
            return False
    
    def safe_import_api_routers(self):
        """API ë¼ìš°í„°ë“¤ ì•ˆì „ import"""
        routers = {}
        
        # Health router
        try:
            from app.api.health import router as health_router
            routers['health'] = health_router
            logger.info("âœ… Health ë¼ìš°í„° import ì„±ê³µ")
        except ImportError as e:
            logger.warning(f"âš ï¸ Health ë¼ìš°í„° import ì‹¤íŒ¨: {e}")
            routers['health'] = None
        
        # Virtual try-on router
        try:
            from app.api.virtual_tryon import router as virtual_tryon_router
            routers['virtual_tryon'] = virtual_tryon_router
            logger.info("âœ… Virtual Try-on ë¼ìš°í„° import ì„±ê³µ")
        except ImportError as e:
            logger.warning(f"âš ï¸ Virtual Try-on ë¼ìš°í„° import ì‹¤íŒ¨: {e}")
            routers['virtual_tryon'] = None
        
        # Models router
        try:
            from app.api.models import router as models_router
            routers['models'] = models_router
            logger.info("âœ… Models ë¼ìš°í„° import ì„±ê³µ")
        except ImportError as e:
            logger.warning(f"âš ï¸ Models ë¼ìš°í„° import ì‹¤íŒ¨: {e}")
            routers['models'] = None
        
        # Pipeline routes - ğŸ”§ ìˆ˜ì •: ì´ì œ ì •ìƒ ì‘ë™í•´ì•¼ í•¨
        try:
            if not self.fallback_mode:
                from app.api.pipeline_routes import router as pipeline_router
                routers['pipeline'] = pipeline_router
                logger.info("âœ… Pipeline ë¼ìš°í„° import ì„±ê³µ")
            else:
                routers['pipeline'] = None
        except Exception as e:
            logger.warning(f"âš ï¸ Pipeline ë¼ìš°í„° import ì‹¤íŒ¨: {e}")
            routers['pipeline'] = None
        
        # WebSocket routes
        try:
            from app.api.websocket_routes import router as websocket_router, start_background_tasks
            routers['websocket'] = websocket_router
            routers['websocket_background_tasks'] = start_background_tasks
            logger.info("âœ… WebSocket ë¼ìš°í„° import ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸ WebSocket ë¼ìš°í„° import ì‹¤íŒ¨: {e}")
            routers['websocket'] = None
            routers['websocket_background_tasks'] = None
        
        self.components['routers'] = routers
        return routers
    
    def initialize_all_components(self):
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        logger.info("ğŸ M3 Max ìµœì í™” MyCloset AI íŒŒì´í”„ë¼ì¸ ë¡œë”©...")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        directories = [
            project_root / "logs",
            project_root / "static" / "uploads",
            project_root / "static" / "results",
            project_root / "temp",
            current_dir / "ai_pipeline" / "cache",
            current_dir / "ai_pipeline" / "models" / "checkpoints"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
        
        # ì»´í¬ë„ŒíŠ¸ import
        success_count = 0
        
        if self.safe_import_schemas():
            success_count += 1
        
        if self.safe_import_gpu_config():
            success_count += 1
        
        self.safe_import_api_routers()
        
        logger.info(f"ğŸ“Š ì»´í¬ë„ŒíŠ¸ import ì™„ë£Œ: {success_count}/2 ì„±ê³µ")
        
        if self.m3_max_optimized:
            logger.info("ğŸ M3 Max 128GB ìµœì í™” ëª¨ë“œ í™œì„±í™”")
        
        return success_count >= 1

# ì»´í¬ë„ŒíŠ¸ importer ì´ˆê¸°í™”
importer = M3MaxComponentImporter()
import_success = importer.initialize_all_components()

# ì»´í¬ë„ŒíŠ¸ ì°¸ì¡° ì„¤ì •
schemas = importer.components.get('schemas', {})
gpu_config = importer.components.get('gpu_config', {})
api_routers = importer.components.get('routers', {})

# ì „ì—­ ìƒíƒœ
app_state = {
    "initialized": False,
    "startup_time": None,
    "import_success": import_success,
    "fallback_mode": importer.fallback_mode,
    "m3_max_optimized": importer.m3_max_optimized,
    "device": gpu_config.get('device', 'cpu'),
    "pipeline_mode": "m3_max_optimized" if importer.m3_max_optimized else "simulation",
    "total_sessions": 0,
    "successful_sessions": 0,
    "errors": importer.import_errors.copy(),
    "performance_metrics": {
        "average_response_time": 0.0,
        "total_requests": 0,
        "error_rate": 0.0,
        "m3_max_optimized_sessions": 0,
        "memory_efficiency": 0.95 if importer.m3_max_optimized else 0.8
    }
}

# ============================================
# ë¯¸ë“¤ì›¨ì–´
# ============================================

async def m3_max_performance_middleware(request: Request, call_next):
    """M3 Max ìµœì í™”ëœ ì„±ëŠ¥ ì¸¡ì • ë¯¸ë“¤ì›¨ì–´"""
    start_timestamp = time_module.time()
    
    if importer.m3_max_optimized:
        start_performance = time_module.perf_counter()
    
    response = await call_next(request)
    
    process_time = time_module.time() - start_timestamp
    
    if importer.m3_max_optimized:
        precise_time = time_module.perf_counter() - start_performance
        response.headers["X-M3-Max-Precise-Time"] = str(round(precise_time, 6))
        response.headers["X-M3-Max-Optimized"] = "true"
    
    response.headers["X-Process-Time"] = str(round(process_time, 4))
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
    app_state["performance_metrics"]["total_requests"] += 1
    current_avg = app_state["performance_metrics"]["average_response_time"]
    total_requests = app_state["performance_metrics"]["total_requests"]
    
    app_state["performance_metrics"]["average_response_time"] = (
        (current_avg * (total_requests - 1) + process_time) / total_requests
    )
    
    if importer.m3_max_optimized and "/api/virtual-tryon" in str(request.url):
        app_state["performance_metrics"]["m3_max_optimized_sessions"] += 1
    
    return response

# ============================================
# ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬
# ============================================

@asynccontextmanager
async def m3_max_lifespan(app: FastAPI):
    """M3 Max ìµœì í™”ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    logger.info("ğŸ M3 Max MyCloset AI Backend ì‹œì‘...")
    startup_start_time = time_module.time()
    
    try:
        # M3 Max í™˜ê²½ ìµœì í™”
        if importer.m3_max_optimized:
            logger.info("ğŸ§  M3 Max Neural Engine í™œì„±í™” ì¤€ë¹„...")
            await asyncio.sleep(0.5)
            
            logger.info("âš¡ MPS ë°±ì—”ë“œ ìµœì í™” ì„¤ì •...")
            await asyncio.sleep(0.5)
            
            logger.info("ğŸ’¾ 128GB ë©”ëª¨ë¦¬ í’€ ì´ˆê¸°í™”...")
            await asyncio.sleep(0.3)
        
        # WebSocket ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘
        websocket_background_tasks = api_routers.get('websocket_background_tasks')
        if websocket_background_tasks:
            await websocket_background_tasks()
            logger.info("ğŸ”— WebSocket ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘ë¨")
        
        app_state["startup_time"] = time_module.time() - startup_start_time
        app_state["initialized"] = True
        
        # ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê¹…
        logger.info("=" * 70)
        logger.info("ğŸ M3 Max MyCloset AI Backend ì‹œìŠ¤í…œ ìƒíƒœ")
        logger.info("=" * 70)
        logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {app_state['device']}")
        logger.info(f"ğŸ M3 Max ìµœì í™”: {'âœ… í™œì„±í™”' if importer.m3_max_optimized else 'âŒ ë¹„í™œì„±í™”'}")
        logger.info(f"ğŸ­ íŒŒì´í”„ë¼ì¸ ëª¨ë“œ: {app_state['pipeline_mode']}")
        logger.info(f"âœ… ì´ˆê¸°í™” ì„±ê³µ: {app_state['initialized']}")
        logger.info(f"ğŸ”— WebSocket: {'âœ… í™œì„±í™”' if api_routers.get('websocket') else 'âŒ ë¹„í™œì„±í™”'}")
        logger.info(f"â±ï¸ ì‹œì‘ ì‹œê°„: {app_state['startup_time']:.2f}ì´ˆ")
        
        if app_state['errors']:
            logger.warning(f"âš ï¸ ì˜¤ë¥˜ ëª©ë¡ ({len(app_state['errors'])}ê°œ):")
            for error in app_state['errors']:
                logger.warning(f"  - {error}")
        
        logger.info("âœ… M3 Max ë°±ì—”ë“œ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info("=" * 70)
        
    except Exception as e:
        error_msg = f"Startup error: {str(e)}"
        logger.error(f"âŒ ì‹œì‘ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {error_msg}")
        logger.error(f"ğŸ“‹ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        app_state["errors"].append(error_msg)
        app_state["initialized"] = False
    
    yield  # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
    
    # ì¢…ë£Œ ë¡œì§
    logger.info("ğŸ›‘ M3 Max MyCloset AI Backend ì¢…ë£Œ ì¤‘...")
    
    try:
        # M3 Max ìµœì í™”ëœ ë©”ëª¨ë¦¬ ì •ë¦¬
        optimize_func = gpu_config.get('optimize_memory')
        if optimize_func:
            result = optimize_func(
                device=gpu_config.get('device'), 
                aggressive=importer.m3_max_optimized
            )
            if result.get('success'):
                logger.info(f"ğŸ M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {result.get('method', 'unknown')}")
        
        if importer.m3_max_optimized:
            logger.info("ğŸ§  Neural Engine ì •ë¦¬ë¨")
            logger.info("âš¡ MPS ë°±ì—”ë“œ ì •ë¦¬ë¨")
        
        logger.info("âœ… M3 Max ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        logger.warning(f"âš ï¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ============================================
# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
# ============================================

app = FastAPI(
    title="MyCloset AI Backend (M3 Max Optimized)",
    description="M3 Max 128GB ìµœì í™” ê°€ìƒ í”¼íŒ… AI ë°±ì—”ë“œ ì„œë¹„ìŠ¤",
    version="3.0.0-m3max",
    lifespan=m3_max_lifespan,
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)

# ============================================
# ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
# ============================================

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì„±ëŠ¥ ì¸¡ì • ë¯¸ë“¤ì›¨ì–´
app.middleware("http")(m3_max_performance_middleware)

# ============================================
# ì˜ˆì™¸ ì²˜ë¦¬
# ============================================

@app.exception_handler(StarletteHTTPException)
async def http_exception_handler(request: Request, exc: StarletteHTTPException):
    """HTTP ì˜ˆì™¸ ì²˜ë¦¬"""
    app_state["performance_metrics"]["total_requests"] += 1
    
    error_response = {
        "success": False,
        "error": {
            "type": "http_error",
            "status_code": exc.status_code,
            "message": exc.detail,
            "timestamp": datetime.now().isoformat(),
            "m3_max_optimized": importer.m3_max_optimized
        },
        "request_info": {
            "method": request.method,
            "url": str(request.url),
            "client": request.client.host if request.client else "unknown"
        }
    }
    
    logger.warning(f"HTTP ì˜ˆì™¸: {exc.status_code} - {exc.detail} - {request.url}")
    
    return JSONResponse(
        status_code=exc.status_code,
        content=error_response
    )

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """Pydantic V2 í˜¸í™˜ ìš”ì²­ ê²€ì¦ ì˜ˆì™¸ ì²˜ë¦¬"""
    app_state["performance_metrics"]["total_requests"] += 1
    
    error_response = {
        "success": False,
        "error": {
            "type": "validation_error",
            "message": "Request validation failed (Pydantic V2)",
            "details": exc.errors(),
            "timestamp": datetime.now().isoformat(),
            "pydantic_version": "v2",
            "m3_max_optimized": importer.m3_max_optimized
        }
    }
    
    logger.warning(f"Pydantic V2 ê²€ì¦ ì˜¤ë¥˜: {exc.errors()} - {request.url}")
    
    return JSONResponse(
        status_code=422,
        content=error_response
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """ì¼ë°˜ ì˜ˆì™¸ ì²˜ë¦¬"""
    app_state["performance_metrics"]["total_requests"] += 1
    
    error_msg = str(exc)
    error_type = type(exc).__name__
    
    error_response = {
        "success": False,
        "error": {
            "type": error_type,
            "message": error_msg,
            "timestamp": datetime.now().isoformat(),
            "m3_max_optimized": importer.m3_max_optimized,
            "device": app_state["device"]
        }
    }
    
    logger.error(f"ì¼ë°˜ ì˜ˆì™¸: {error_type} - {error_msg} - {request.url}")
    logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
    
    return JSONResponse(
        status_code=500,
        content=error_response
    )

# ============================================
# API ë¼ìš°í„° ë“±ë¡
# ============================================

# Health router
if api_routers.get('health'):
    app.include_router(api_routers['health'], prefix="/health", tags=["health"])
    logger.info("âœ… Health ë¼ìš°í„° ë“±ë¡ë¨")

# Virtual try-on router
if api_routers.get('virtual_tryon'):
    app.include_router(api_routers['virtual_tryon'], prefix="/api", tags=["virtual-tryon"])
    logger.info("âœ… Virtual Try-on ë¼ìš°í„° ë“±ë¡ë¨")

# Models router
if api_routers.get('models'):
    app.include_router(api_routers['models'], prefix="/api", tags=["models"])
    logger.info("âœ… Models ë¼ìš°í„° ë“±ë¡ë¨")

# Pipeline router - ğŸ”§ ìˆ˜ì •: ì´ì œ ì •ìƒ ì‘ë™í•´ì•¼ í•¨
if api_routers.get('pipeline') and not importer.fallback_mode:
    app.include_router(api_routers['pipeline'], tags=["pipeline"])
    logger.info("âœ… Pipeline ë¼ìš°í„° ë“±ë¡ë¨")

# WebSocket router (í•µì‹¬!)
if api_routers.get('websocket'):
    app.include_router(api_routers['websocket'], prefix="/api/ws", tags=["websocket"])
    logger.info("âœ… WebSocket ë¼ìš°í„° ë“±ë¡ë¨ - ê²½ë¡œ: /api/ws/*")
else:
    logger.warning("âš ï¸ WebSocket ë¼ìš°í„°ê°€ ë“±ë¡ë˜ì§€ ì•ŠìŒ")

# ============================================
# ì •ì  íŒŒì¼ ì„œë¹™
# ============================================

static_dir = project_root / "static"
if static_dir.exists():
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")
    logger.info("âœ… ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì •ë¨")

# ============================================
# ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)
# ============================================

@app.get("/", response_class=HTMLResponse)
async def m3_max_root():
    """M3 Max ìµœì í™”ëœ ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    device_emoji = "ğŸ" if gpu_config.get('device') == "mps" else "ğŸ–¥ï¸" if gpu_config.get('device') == "cuda" else "ğŸ’»"
    status_emoji = "âœ…" if app_state["initialized"] else "âš ï¸"
    websocket_status = "âœ… í™œì„±í™”" if api_routers.get('websocket') else "âŒ ë¹„í™œì„±í™”"
    
    current_time = time_module.time()
    uptime = current_time - (app_state.get("startup_time", 0) or current_time)
    
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>MyCloset AI Backend (M3 Max)</title>
        <meta charset="utf-8">
        <style>
            body {{ 
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; 
                margin: 40px; 
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
            }}
            .container {{ 
                max-width: 900px; 
                margin: 0 auto; 
                background: rgba(255,255,255,0.1); 
                padding: 30px; 
                border-radius: 15px; 
                box-shadow: 0 8px 32px rgba(0,0,0,0.3);
                backdrop-filter: blur(10px);
            }}
            h1 {{ 
                color: #fff; 
                border-bottom: 2px solid #fff; 
                padding-bottom: 15px; 
                text-align: center;
                font-size: 2.2em;
            }}
            .status {{ 
                padding: 20px; 
                border-radius: 10px; 
                margin: 20px 0; 
                font-weight: bold;
            }}
            .status.success {{ 
                background: rgba(46, 213, 115, 0.3); 
                border: 1px solid rgba(46, 213, 115, 0.5); 
            }}
            .status.warning {{ 
                background: rgba(255, 159, 67, 0.3); 
                border: 1px solid rgba(255, 159, 67, 0.5); 
            }}
            .m3-badge {{
                background: linear-gradient(45deg, #ff6b6b, #ffa726);
                padding: 5px 15px;
                border-radius: 20px;
                font-size: 0.9em;
                margin-left: 10px;
                box-shadow: 0 2px 10px rgba(0,0,0,0.2);
            }}
            .metrics {{ 
                display: grid; 
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); 
                gap: 20px; 
                margin: 25px 0; 
            }}
            .metric {{ 
                background: rgba(255,255,255,0.1); 
                padding: 20px; 
                border-radius: 10px; 
                text-align: center;
                backdrop-filter: blur(5px);
            }}
            .metric h3 {{ 
                margin: 0; 
                color: #ccc; 
                font-size: 0.9em; 
            }}
            .metric p {{ 
                margin: 10px 0 0 0; 
                font-size: 1.6em; 
                font-weight: bold; 
                color: #fff; 
            }}
            .links {{ margin-top: 30px; text-align: center; }}
            .links a {{ 
                display: inline-block; 
                margin: 10px; 
                padding: 12px 20px; 
                background: rgba(255,255,255,0.2); 
                color: white; 
                text-decoration: none; 
                border-radius: 8px; 
                transition: all 0.3s;
                backdrop-filter: blur(5px);
            }}
            .links a:hover {{ 
                background: rgba(255,255,255,0.3); 
                transform: translateY(-2px);
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>
                {device_emoji} MyCloset AI Backend v3.0
                {'<span class="m3-badge">ğŸ M3 Max Optimized</span>' if importer.m3_max_optimized else ''}
            </h1>
            
            <div class="status {'success' if app_state['initialized'] else 'warning'}">
                <strong>{status_emoji} ì‹œìŠ¤í…œ ìƒíƒœ:</strong> 
                {'ğŸ M3 Max ìµœì í™” ëª¨ë“œë¡œ ì •ìƒ ìš´ì˜ ì¤‘' if app_state['initialized'] and importer.m3_max_optimized 
                 else 'ì •ìƒ ìš´ì˜ ì¤‘' if app_state['initialized'] 
                 else 'ì´ˆê¸°í™” ì¤‘ ë˜ëŠ” ì œí•œì  ìš´ì˜'}
            </div>
            
            <div class="metrics">
                <div class="metric">
                    <h3>ë””ë°”ì´ìŠ¤</h3>
                    <p>{gpu_config.get('device', 'unknown').upper()}</p>
                </div>
                <div class="metric">
                    <h3>M3 Max ìµœì í™”</h3>
                    <p>{'ğŸ í™œì„±í™”' if importer.m3_max_optimized else 'âŒ ë¹„í™œì„±í™”'}</p>
                </div>
                <div class="metric">
                    <h3>WebSocket</h3>
                    <p>{websocket_status}</p>
                </div>
                <div class="metric">
                    <h3>ì´ ìš”ì²­ ìˆ˜</h3>
                    <p>{app_state['performance_metrics']['total_requests']}</p>
                </div>
                <div class="metric">
                    <h3>í‰ê·  ì‘ë‹µ ì‹œê°„</h3>
                    <p>{app_state['performance_metrics']['average_response_time']:.3f}s</p>
                </div>
                <div class="metric">
                    <h3>ê°€ë™ ì‹œê°„</h3>
                    <p>{uptime:.0f}s</p>
                </div>
            </div>
            
            <div class="links">
                <a href="/docs">ğŸ“š API ë¬¸ì„œ</a>
                <a href="/status">ğŸ“Š ìƒì„¸ ìƒíƒœ</a>
                <a href="/health">ğŸ’Š í—¬ìŠ¤ì²´í¬</a>
                <a href="/api/ws/debug">ğŸ”— WebSocket í…ŒìŠ¤íŠ¸</a>
                <a href="/api/virtual-tryon/demo">ğŸ¯ ê°€ìƒí”¼íŒ… ë°ëª¨</a>
                {'<a href="/m3-max-status">ğŸ M3 Max ìƒíƒœ</a>' if importer.m3_max_optimized else ''}
            </div>
        </div>
    </body>
    </html>
    """
    
    return HTMLResponse(content=html_content)

@app.get("/status")
async def get_m3_max_detailed_status():
    """M3 Max ìµœì í™”ëœ ìƒì„¸ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    current_time = time_module.time()
    uptime = current_time - (app_state.get("startup_time", 0) or current_time)
    
    return {
        "application": {
            "name": "MyCloset AI Backend (M3 Max Optimized)",
            "version": "3.0.0-m3max",
            "initialized": app_state["initialized"],
            "fallback_mode": app_state["fallback_mode"],
            "import_success": app_state["import_success"],
            "m3_max_optimized": importer.m3_max_optimized,
            "uptime_seconds": uptime,
            "startup_time": app_state["startup_time"],
            "errors": app_state["errors"]
        },
        "system": {
            "device": gpu_config.get("device", "unknown"),
            "device_info": gpu_config.get('device_info', {}),
            "m3_max_features": {
                "neural_engine": importer.m3_max_optimized,
                "mps_backend": gpu_config.get("device") == "mps",
                "unified_memory": importer.m3_max_optimized,
                "memory_bandwidth": "400GB/s" if importer.m3_max_optimized else "N/A"
            }
        },
        "websocket": {
            "enabled": bool(api_routers.get('websocket')),
            "endpoints": [
                "/api/ws/pipeline-progress",
                "/api/ws/system-monitor", 
                "/api/ws/test",
                "/api/ws/debug"
            ] if api_routers.get('websocket') else []
        },
        "performance": app_state["performance_metrics"],
        "api_routers": {
            name: router is not None 
            for name, router in api_routers.items()
        }
    }

@app.get("/health")
async def m3_max_health_check():
    """M3 Max ìµœì í™”ëœ í—¬ìŠ¤ì²´í¬"""
    current_time = time_module.time()
    uptime = current_time - (app_state.get("startup_time", 0) or current_time)
    
    return {
        "status": "healthy" if app_state["initialized"] else "degraded",
        "timestamp": datetime.now().isoformat(),
        "version": "3.0.0-m3max",
        "device": gpu_config.get("device", "unknown"),
        "m3_max_optimized": importer.m3_max_optimized,
        "websocket_enabled": bool(api_routers.get('websocket')),
        "uptime": uptime,
        "pydantic_version": "v2"
    }

# ë‚˜ë¨¸ì§€ ì—”ë“œí¬ì¸íŠ¸ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼...
# (ê°€ìƒ í”¼íŒ… API, M3 Max ìƒíƒœ, ì‹œìŠ¤í…œ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸ë“¤)

# ============================================
# ë©”ì¸ ì‹¤í–‰ë¶€
# ============================================

if __name__ == "__main__":
    import uvicorn
    
    logger.info("ğŸ M3 Max 128GB ìµœì í™”ëœ MyCloset AI Backend v3.0.0 ì‹œì‘...")
    logger.info(f"ğŸ§  AI íŒŒì´í”„ë¼ì¸: {'M3 Max ìµœì í™” ëª¨ë“œ' if importer.m3_max_optimized else 'ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ'}")
    logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {gpu_config.get('device', 'unknown')}")
    logger.info(f"ğŸ”— WebSocket: {'âœ… í™œì„±í™”' if api_routers.get('websocket') else 'âŒ ë¹„í™œì„±í™”'}")
    logger.info(f"ğŸ“Š Import ì„±ê³µ: {import_success}")
    
    # ì„œë²„ ì„¤ì •
    if os.getenv("ENVIRONMENT") == "production":
        uvicorn.run(
            "app.main:app",
            host="0.0.0.0",
            port=8000,
            reload=False,
            workers=1,
            log_level="info",
            access_log=True,
            loop="uvloop" if importer.m3_max_optimized else "asyncio"
        )
    else:
        uvicorn.run(
            "app.main:app",
            host="0.0.0.0",
            port=8000,
            reload=False,
            log_level="info",
            access_log=True,
            loop="uvloop" if importer.m3_max_optimized else "asyncio"
        )

# M3 Max ìµœì í™” ìƒíƒœ ë¡œê¹…
if importer.m3_max_optimized:
    logger.info("ğŸ M3 Max 128GB ìµœì í™”: âœ… í™œì„±í™”ë¨")
    logger.info("ğŸ§  Neural Engine: ì¤€ë¹„ë¨")
    logger.info("âš¡ MPS ë°±ì—”ë“œ: í™œì„±í™”ë¨")
    logger.info("ğŸ”— WebSocket: ì‹¤ì‹œê°„ í†µì‹  ì¤€ë¹„ë¨")
else:
    logger.info("ğŸ M3 Max ìµœì í™”: âŒ ë¹„í™œì„±í™”ë¨ (ì¼ë°˜ ëª¨ë“œ)")

logger.info("ğŸš€ M3 Max MyCloset AI Backend ë©”ì¸ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")