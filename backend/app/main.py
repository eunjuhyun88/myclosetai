# =============================================================================
# backend/app/main.py - üî• ÏôÑÏ†ÑÌïú AI Ïó∞Îèô MyCloset Î∞±ÏóîÎìú ÏÑúÎ≤Ñ (Ìå®Ïπò Ìè¨Ìï®)
# =============================================================================

"""
üçé MyCloset AI FastAPI ÏÑúÎ≤Ñ - ÏôÑÏ†ÑÌïú AI Ïó∞Îèô Î≤ÑÏ†Ñ + Coroutine Ìå®Ïπò
================================================================================

‚úÖ AI ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÏ†Ñ Ïó∞Îèô (PipelineManager, ModelLoader, AI Steps)
‚úÖ SessionManager Ï§ëÏã¨ Ïù¥ÎØ∏ÏßÄ Í¥ÄÎ¶¨ (Ïû¨ÏóÖÎ°úÎìú Î¨∏Ï†ú Ìï¥Í≤∞)
‚úÖ StepServiceManager 8Îã®Í≥Ñ API ÏôÑÏ†Ñ ÏßÄÏõê
‚úÖ WebSocket Ïã§ÏãúÍ∞Ñ ÏßÑÌñâÎ•† ÏãúÏä§ÌÖú
‚úÖ 4Îã®Í≥Ñ Ìè¥Î∞± Î©îÏª§ÎãàÏ¶ò (Ïã§Ìå® ÏãúÏóêÎèÑ ÏÑúÎπÑÏä§ Ï†úÍ≥µ)
‚úÖ M3 Max 128GB ÏôÑÏ†Ñ ÏµúÏ†ÅÌôî
‚úÖ 89.8GB AI Î™®Îç∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ
‚úÖ conda ÌôòÍ≤Ω ÏôÑÎ≤Ω ÏßÄÏõê
‚úÖ ÌîÑÎ°†Ìä∏ÏóîÎìú 100% Ìò∏ÌôòÏÑ±
‚úÖ ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® ÏóêÎü¨ Ï≤òÎ¶¨
‚úÖ Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†Å Ï≤òÎ¶¨
‚úÖ ÎèôÏ†Å Î™®Îç∏ Î°úÎî©
‚úÖ Ïã§ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ
‚úÖ Coroutine Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞ Ìå®Ïπò Ï†ÅÏö©

Author: MyCloset AI Team
Date: 2025-07-20
Version: 4.1.0 (Complete AI Integration + Coroutine Patches)
"""

import os
import sys
import logging
import logging.handlers
import uuid
import base64
import asyncio
import traceback
import time
import threading
import json
import gc
import psutil
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Union, Callable, Tuple
from contextlib import asynccontextmanager
from dataclasses import dataclass
from enum import Enum

# =============================================================================
# üî• Step 1: Í≤ΩÎ°ú Î∞è ÌôòÍ≤Ω ÏÑ§Ï†ï (M3 Max ÏµúÏ†ÅÌôî)
# =============================================================================

# ÌòÑÏû¨ ÌååÏùºÏùò Ï†àÎåÄ Í≤ΩÎ°ú
current_file = Path(__file__).absolute()
backend_root = current_file.parent.parent
project_root = backend_root.parent

# Python Í≤ΩÎ°úÏóê Ï∂îÍ∞Ä (import Î¨∏Ï†ú Ìï¥Í≤∞)
if str(backend_root) not in sys.path:
    sys.path.insert(0, str(backend_root))

# ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
os.environ['PYTHONPATH'] = f"{backend_root}:{os.environ.get('PYTHONPATH', '')}"
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = "0.0"  # M3 Max Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = "1"
os.chdir(backend_root)

# M3 Max Í∞êÏßÄ Î∞è ÏÑ§Ï†ï
IS_M3_MAX = False
try:
    import platform
    if platform.system() == 'Darwin' and 'arm64' in platform.machine():
        IS_M3_MAX = True
        os.environ['DEVICE'] = 'mps'
        print(f"üçé Apple M3 Max ÌôòÍ≤Ω Í∞êÏßÄ - MPS ÌôúÏÑ±Ìôî")
    else:
        os.environ['DEVICE'] = 'cuda' if 'cuda' in str(os.environ.get('DEVICE', 'cpu')).lower() else 'cpu'
except Exception:
    pass

print(f"üîç Î∞±ÏóîÎìú Î£®Ìä∏: {backend_root}")
print(f"üìÅ ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨: {os.getcwd()}")
print(f"üçé M3 Max: {'‚úÖ' if IS_M3_MAX else '‚ùå'}")

# =============================================================================
# üî• Step 2: üö® COROUTINE Ìå®Ïπò Ï†ÅÏö© (AI ÌååÏù¥ÌîÑÎùºÏù∏ import Ï†ÑÏóê ÌïÑÏàò!)
# =============================================================================

print("üîß Coroutine Ïò§Î•ò ÏàòÏ†ï Ìå®Ïπò Ï†ÅÏö© Ï§ë...")

try:
    # ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ± (ÏóÜÏúºÎ©¥)
    core_dir = backend_root / "app" / "core"
    core_dir.mkdir(parents=True, exist_ok=True)
    
    # coroutine_fix.py ÏÉùÏÑ±
    coroutine_fix_content = '''# backend/app/core/coroutine_fix.py
"""
üîß Coroutine Ïò§Î•ò Ï¶âÏãú Ìï¥Í≤∞ Ìå®Ïπò
coroutine 'was never awaited' Î∞è 'object is not callable' ÏôÑÏ†Ñ Ìï¥Í≤∞
"""

import asyncio
import inspect
import logging
from typing import Any, Callable, Coroutine, Union
from functools import wraps

logger = logging.getLogger(__name__)

class CoroutineFixer:
    """Coroutine Í¥ÄÎ†® Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞ ÌÅ¥ÎûòÏä§"""
    
    @staticmethod
    def fix_coroutine_call(func_or_method: Any) -> Any:
        """
        Coroutine Ìï®ÏàòÎ•º ÏïàÏ†ÑÌïòÍ≤å ÎèôÍ∏∞ Ìï®ÏàòÎ°ú Î≥ÄÌôò
        """
        if not asyncio.iscoroutinefunction(func_or_method):
            return func_or_method
        
        @wraps(func_or_method)
        def sync_wrapper(*args, **kwargs):
            try:
                # ÌòÑÏû¨ Ïù¥Î≤§Ìä∏ Î£®ÌîÑ ÌôïÏù∏
                try:
                    loop = asyncio.get_running_loop()
                    # Ïù¥ÎØ∏ Ïã§Ìñâ Ï§ëÏù∏ Î£®ÌîÑÍ∞Ä ÏûàÏúºÎ©¥ ÌÉúÏä§ÌÅ¨Î°ú Ïã§Ìñâ
                    task = asyncio.create_task(func_or_method(*args, **kwargs))
                    return task
                except RuntimeError:
                    # Ïã§Ìñâ Ï§ëÏù∏ Î£®ÌîÑÍ∞Ä ÏóÜÏúºÎ©¥ ÏÉà Î£®ÌîÑ ÏÉùÏÑ±
                    return asyncio.run(func_or_method(*args, **kwargs))
            except Exception as e:
                logger.warning(f"Coroutine Î≥ÄÌôò Ïã§Ìå®: {e}")
                return None
        
        return sync_wrapper
    
    @staticmethod
    def patch_base_step_mixin():
        """
        BaseStepMixinÏùò ÏõåÎ∞çÏóÖ Í¥ÄÎ†® Î©îÏÑúÎìúÎì§ÏùÑ ÏïàÏ†ÑÌïòÍ≤å Ìå®Ïπò
        """
        try:
            from backend.app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
            
            # _pipeline_warmup Î©îÏÑúÎìúÎ•º ÏïàÏ†ÑÌïòÍ≤å ÏàòÏ†ï
            def safe_pipeline_warmup(self):
                """ÏïàÏ†ÑÌïú ÌååÏù¥ÌîÑÎùºÏù∏ ÏõåÎ∞çÏóÖ (ÎèôÍ∏∞)"""
                try:
                    # StepÎ≥Ñ ÏõåÎ∞çÏóÖ Î°úÏßÅ (Í∏∞Î≥∏)
                    if hasattr(self, 'warmup_step'):
                        warmup_method = getattr(self, 'warmup_step')
                        
                        # async Ìï®ÏàòÎ©¥ ÎèôÍ∏∞Î°ú Î≥ÄÌôòÌïòÏó¨ Ìò∏Ï∂ú
                        if asyncio.iscoroutinefunction(warmup_method):
                            try:
                                result = asyncio.run(warmup_method())
                                return {'success': result.get('success', True), 'message': 'Step ÏõåÎ∞çÏóÖ ÏôÑÎ£å'}
                            except Exception as e:
                                logger.warning(f"ÎπÑÎèôÍ∏∞ ÏõåÎ∞çÏóÖ Ïã§Ìå®: {e}")
                                return {'success': False, 'error': str(e)}
                        else:
                            result = warmup_method()
                            return {'success': result.get('success', True), 'message': 'Step ÏõåÎ∞çÏóÖ ÏôÑÎ£å'}
                    
                    return {'success': True, 'message': 'ÌååÏù¥ÌîÑÎùºÏù∏ ÏõåÎ∞çÏóÖ Í±¥ÎÑàÎúÄ'}
                    
                except Exception as e:
                    return {'success': False, 'error': str(e)}
            
            # BaseStepMixinÏóê ÏïàÏ†ÑÌïú Î©îÏÑúÎìú Ï†ÅÏö©
            BaseStepMixin._pipeline_warmup = safe_pipeline_warmup
            
            logger.info("‚úÖ BaseStepMixin ÏõåÎ∞çÏóÖ Î©îÏÑúÎìú Ìå®Ïπò ÏôÑÎ£å")
            return True
            
        except ImportError as e:
            logger.warning(f"BaseStepMixin import Ïã§Ìå®: {e}")
            return False
        except Exception as e:
            logger.error(f"BaseStepMixin Ìå®Ïπò Ïã§Ìå®: {e}")
            return False

def apply_coroutine_fixes():
    """
    Ï†ÑÏ≤¥ ÏãúÏä§ÌÖúÏóê Coroutine ÏàòÏ†ï Ï†ÅÏö©
    """
    logger.info("üîß Coroutine Ïò§Î•ò ÏàòÏ†ï Ï†ÅÏö© ÏãúÏûë...")
    
    # 1. BaseStepMixin Ìå®Ïπò
    if CoroutineFixer.patch_base_step_mixin():
        logger.info("‚úÖ BaseStepMixin Ìå®Ïπò ÏôÑÎ£å")
    
    return True

__all__ = ['CoroutineFixer', 'apply_coroutine_fixes']
'''
    
    # warmup_safe_patch.py ÏÉùÏÑ±
    warmup_patch_content = '''# backend/app/core/warmup_safe_patch.py
"""
üîß ÏõåÎ∞çÏóÖ ÏïàÏ†Ñ Ìå®Ïπò - RuntimeWarning Î∞è 'dict object is not callable' ÏôÑÏ†Ñ Ìï¥Í≤∞
"""

import asyncio
import logging
import os
from typing import Any, Callable, Dict, Optional

logger = logging.getLogger(__name__)

def patch_warmup_system():
    """ÏõåÎ∞çÏóÖ ÏãúÏä§ÌÖú Ìå®Ïπò"""
    try:
        # ÌôòÍ≤ΩÎ≥ÄÏàò ÏÑ§Ï†ïÏúºÎ°ú ÏõåÎ∞çÏóÖ ÎπÑÌôúÏÑ±Ìôî
        os.environ['ENABLE_MODEL_WARMUP'] = 'false'
        os.environ['SKIP_WARMUP'] = 'true'
        os.environ['AUTO_WARMUP'] = 'false'
        os.environ['DISABLE_AI_WARMUP'] = 'true'
        
        logger.info("üö´ ÏõåÎ∞çÏóÖ ÏãúÏä§ÌÖú Ï†ÑÏó≠ ÎπÑÌôúÏÑ±Ìôî")
        return True
        
    except Exception as e:
        logger.error(f"ÏõåÎ∞çÏóÖ ÏãúÏä§ÌÖú Ìå®Ïπò Ïã§Ìå®: {e}")
        return False

def disable_problematic_async_methods():
    """Î¨∏Ï†úÍ∞Ä ÎêòÎäî async Î©îÏÑúÎìúÎì§ÏùÑ ÎèôÍ∏∞ Î≤ÑÏ†ÑÏúºÎ°ú ÍµêÏ≤¥"""
    try:
        step_classes = []
        
        # Î¨∏Ï†úÍ∞Ä ÎêòÎäî Step ÌÅ¥ÎûòÏä§Îì§ import
        try:
            from backend.app.ai_pipeline.steps.step_01_human_parsing import HumanParsingStep
            step_classes.append(HumanParsingStep)
        except ImportError:
            pass
            
        try:
            from backend.app.ai_pipeline.steps.step_04_geometric_matching import GeometricMatchingStep
            step_classes.append(GeometricMatchingStep)
        except ImportError:
            pass
        
        for step_class in step_classes:
            # warmup_step Î©îÏÑúÎìúÎ•º ÎèôÍ∏∞Î°ú ÍµêÏ≤¥
            if hasattr(step_class, 'warmup_step') and asyncio.iscoroutinefunction(step_class.warmup_step):
                def sync_warmup_step(self):
                    """ÎèôÍ∏∞ ÏõåÎ∞çÏóÖ (ÏïàÏ†Ñ Î≤ÑÏ†Ñ)"""
                    return {'success': True, 'message': f'{self.__class__.__name__} ÏõåÎ∞çÏóÖ ÏôÑÎ£å'}
                
                step_class.warmup_step = sync_warmup_step
                logger.info(f"‚úÖ {step_class.__name__}.warmup_step -> ÎèôÍ∏∞ Î≤ÑÏ†ÑÏúºÎ°ú ÍµêÏ≤¥")
            
            # _setup_model_interface Î©îÏÑúÎìúÎèÑ ÎèôÍ∏∞Î°ú ÍµêÏ≤¥
            if hasattr(step_class, '_setup_model_interface') and asyncio.iscoroutinefunction(step_class._setup_model_interface):
                def sync_setup_model_interface(self):
                    """ÎèôÍ∏∞ Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÑ§Ï†ï"""
                    self.logger.info(f"üîó {self.__class__.__name__} Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÑ§Ï†ï (ÎèôÍ∏∞)")
                    return None
                
                step_class._setup_model_interface = sync_setup_model_interface
                logger.info(f"‚úÖ {step_class.__name__}._setup_model_interface -> ÎèôÍ∏∞ Î≤ÑÏ†ÑÏúºÎ°ú ÍµêÏ≤¥")
        
        return True
        
    except Exception as e:
        logger.error(f"async Î©îÏÑúÎìú ÎπÑÌôúÏÑ±Ìôî Ïã§Ìå®: {e}")
        return False

def apply_warmup_patches():
    """Î™®Îì† ÏõåÎ∞çÏóÖ Í¥ÄÎ†® Ìå®Ïπò Ï†ÅÏö©"""
    logger.info("üîß ÏõåÎ∞çÏóÖ ÏïàÏ†Ñ Ìå®Ïπò Ï†ÅÏö© ÏãúÏûë...")
    
    success_count = 0
    
    # 1. ÏõåÎ∞çÏóÖ ÏãúÏä§ÌÖú Ìå®Ïπò
    if patch_warmup_system():
        success_count += 1
        logger.info("‚úÖ ÏõåÎ∞çÏóÖ ÏãúÏä§ÌÖú Ìå®Ïπò ÏÑ±Í≥µ")
    
    # 2. Î¨∏Ï†úÍ∞Ä ÎêòÎäî async Î©îÏÑúÎìú ÎπÑÌôúÏÑ±Ìôî
    if disable_problematic_async_methods():
        success_count += 1
        logger.info("‚úÖ async Î©îÏÑúÎìú ÎπÑÌôúÏÑ±Ìôî ÏÑ±Í≥µ")
    
    if success_count > 0:
        logger.info(f"üéâ ÏõåÎ∞çÏóÖ Ìå®Ïπò ÏôÑÎ£å: {success_count}/2 ÏÑ±Í≥µ")
        return True
    else:
        logger.warning("‚ö†Ô∏è ÏõåÎ∞çÏóÖ Ìå®Ïπò Ïã§Ìå®")
        return False

__all__ = ['apply_warmup_patches', 'patch_warmup_system', 'disable_problematic_async_methods']
'''
    
    # ÌååÏùº ÏÉùÏÑ±
    (core_dir / "coroutine_fix.py").write_text(coroutine_fix_content, encoding='utf-8')
    (core_dir / "warmup_safe_patch.py").write_text(warmup_patch_content, encoding='utf-8')
    (core_dir / "__init__.py").write_text("", encoding='utf-8')
    
    print("‚úÖ Ìå®Ïπò ÌååÏùºÎì§ ÏÉùÏÑ± ÏôÑÎ£å")
    
    # Ìå®Ïπò Ï†ÅÏö©
    from app.core.coroutine_fix import apply_coroutine_fixes
    from app.core.warmup_safe_patch import apply_warmup_patches
    
    apply_coroutine_fixes()
    apply_warmup_patches()
    print("‚úÖ Ìå®Ïπò Ï†ÅÏö© ÏôÑÎ£å")
    
except Exception as e:
    print(f"‚ö†Ô∏è Ìå®Ïπò Ï†ÅÏö© Ïã§Ìå®: {e}")
    print("ÏÑúÎ≤ÑÎäî Í≥ÑÏÜç ÏßÑÌñâÎêòÏßÄÎßå ÏùºÎ∂Ä coroutine Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï† Ïàò ÏûàÏäµÎãàÎã§.")

# =============================================================================
# üî• Step 3: ÌïÑÏàò ÎùºÏù¥Î∏åÎü¨Î¶¨ import
# =============================================================================

try:
    from fastapi import FastAPI, Request, UploadFile, File, Form, HTTPException, WebSocket, WebSocketDisconnect, Depends
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.middleware.gzip import GZipMiddleware
    from fastapi.responses import JSONResponse, HTMLResponse
    from fastapi.staticfiles import StaticFiles
    from pydantic import BaseModel, Field
    import uvicorn
    print("‚úÖ FastAPI ÎùºÏù¥Î∏åÎü¨Î¶¨ import ÏÑ±Í≥µ")
except ImportError as e:
    print(f"‚ùå FastAPI ÎùºÏù¥Î∏åÎü¨Î¶¨ import Ïã§Ìå®: {e}")
    print("ÏÑ§Ïπò Î™ÖÎ†π: pip install fastapi uvicorn python-multipart")
    sys.exit(1)

try:
    from PIL import Image
    import numpy as np
    import torch
    print("‚úÖ AI ÎùºÏù¥Î∏åÎü¨Î¶¨ import ÏÑ±Í≥µ")
except ImportError as e:
    print(f"‚ö†Ô∏è AI ÎùºÏù¥Î∏åÎü¨Î¶¨ import Ïã§Ìå®: {e}")

# =============================================================================
# üî• Step 4: AI ÌååÏù¥ÌîÑÎùºÏù∏ ÏãúÏä§ÌÖú import (ÏôÑÏ†Ñ Ïó∞Îèô)
# =============================================================================

# 4.1 AI ÌååÏù¥ÌîÑÎùºÏù∏ Îß§ÎãàÏ†Ä import
PIPELINE_MANAGER_AVAILABLE = False
try:
    from app.ai_pipeline.pipeline_manager import (
        PipelineManager,
        PipelineConfig, 
        ProcessingResult,
        QualityLevel,
        PipelineMode,
        create_pipeline,
        create_m3_max_pipeline,
        create_production_pipeline,
        DIBasedPipelineManager
    )
    PIPELINE_MANAGER_AVAILABLE = True
    print("‚úÖ PipelineManager import ÏÑ±Í≥µ")
except ImportError as e:
    print(f"‚ö†Ô∏è PipelineManager import Ïã§Ìå®: {e}")

# 4.2 ModelLoader ÏãúÏä§ÌÖú import
MODEL_LOADER_AVAILABLE = False
try:
    from app.ai_pipeline.utils.model_loader import (
        ModelLoader,
        get_global_model_loader,
        initialize_global_model_loader
    )
    from app.ai_pipeline.utils import (
        get_step_model_interface,
        UnifiedUtilsManager,
        get_utils_manager
    )
    MODEL_LOADER_AVAILABLE = True
    print("‚úÖ ModelLoader ÏãúÏä§ÌÖú import ÏÑ±Í≥µ")
except ImportError as e:
    print(f"‚ö†Ô∏è ModelLoader import Ïã§Ìå®: {e}")

# 4.3 AI Steps import
AI_STEPS_AVAILABLE = False
ai_step_classes = {}
try:
    from app.ai_pipeline.steps.step_01_human_parsing import HumanParsingStep
    from app.ai_pipeline.steps.step_02_pose_estimation import PoseEstimationStep
    from app.ai_pipeline.steps.step_03_cloth_segmentation import ClothSegmentationStep
    from app.ai_pipeline.steps.step_04_geometric_matching import GeometricMatchingStep
    from app.ai_pipeline.steps.step_05_cloth_warping import ClothWarpingStep
    from app.ai_pipeline.steps.step_06_virtual_fitting import VirtualFittingStep
    from app.ai_pipeline.steps.step_07_post_processing import PostProcessingStep
    from app.ai_pipeline.steps.step_08_quality_assessment import QualityAssessmentStep
    
    ai_step_classes = {
        1: HumanParsingStep,
        2: PoseEstimationStep,
        3: ClothSegmentationStep,
        4: GeometricMatchingStep,
        5: ClothWarpingStep,
        6: VirtualFittingStep,
        7: PostProcessingStep,
        8: QualityAssessmentStep
    }
    AI_STEPS_AVAILABLE = True
    print(f"‚úÖ AI Steps import ÏÑ±Í≥µ ({len(ai_step_classes)}Í∞ú)")
except ImportError as e:
    print(f"‚ö†Ô∏è AI Steps import Ïã§Ìå®: {e}")

# 4.4 Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú import
MEMORY_MANAGER_AVAILABLE = False
try:
    from app.ai_pipeline.utils.memory_manager import (
        MemoryManager,
        optimize_memory,
        get_memory_info
    )
    MEMORY_MANAGER_AVAILABLE = True
    print("‚úÖ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú import ÏÑ±Í≥µ")
except ImportError as e:
    print(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú import Ïã§Ìå®: {e}")

# =============================================================================
# üî• Step 5: SessionManager import
# =============================================================================

SESSION_MANAGER_AVAILABLE = False
try:
    from app.core.session_manager import (
        SessionManager,
        SessionData,
        SessionMetadata,
        get_session_manager,
        cleanup_session_manager
    )
    SESSION_MANAGER_AVAILABLE = True
    print("‚úÖ SessionManager import ÏÑ±Í≥µ")
except ImportError as e:
    print(f"‚ö†Ô∏è SessionManager import Ïã§Ìå®: {e}")
    
    # Ìè¥Î∞±: Í∏∞Î≥∏ SessionManager
    class SessionManager:
        def __init__(self):
            self.sessions = {}
            self.logger = logging.getLogger("FallbackSessionManager")
        
        async def create_session(self, **kwargs):
            session_id = f"fallback_{uuid.uuid4().hex[:8]}"
            self.sessions[session_id] = kwargs
            return session_id
        
        async def get_session_images(self, session_id: str):
            session = self.sessions.get(session_id, {})
            return session.get('person_image'), session.get('clothing_image')
        
        async def save_step_result(self, session_id: str, step_id: int, result: Dict):
            if session_id in self.sessions:
                if 'step_results' not in self.sessions[session_id]:
                    self.sessions[session_id]['step_results'] = {}
                self.sessions[session_id]['step_results'][step_id] = result
        
        def get_all_sessions_status(self):
            return {"total_sessions": len(self.sessions), "fallback_mode": True}
        
        async def cleanup_all_sessions(self):
            self.sessions.clear()
    
    def get_session_manager():
        return SessionManager()

# =============================================================================
# üî• Step 6: StepServiceManager import
# =============================================================================

STEP_SERVICE_AVAILABLE = False
try:
    from app.services import (
        get_step_service_manager,
        StepServiceManager,
        STEP_SERVICE_AVAILABLE as SERVICE_AVAILABLE
    )
    STEP_SERVICE_AVAILABLE = SERVICE_AVAILABLE
    print("‚úÖ StepServiceManager import ÏÑ±Í≥µ")
except ImportError as e:
    print(f"‚ö†Ô∏è StepServiceManager import Ïã§Ìå®: {e}")
    
    # Ìè¥Î∞±: Í∏∞Î≥∏ StepServiceManager
    class StepServiceManager:
        def __init__(self):
            self.logger = logging.getLogger("FallbackStepServiceManager")
        
        async def process_step_1_upload_validation(self, **kwargs):
            return {"success": True, "confidence": 0.9, "message": "Ìè¥Î∞± Íµ¨ÌòÑ"}
        
        async def process_step_2_measurements_validation(self, **kwargs):
            return {"success": True, "confidence": 0.9, "message": "Ìè¥Î∞± Íµ¨ÌòÑ"}
        
        async def process_step_3_human_parsing(self, **kwargs):
            return {"success": True, "confidence": 0.9, "message": "Ìè¥Î∞± Íµ¨ÌòÑ"}
        
        async def process_step_4_pose_estimation(self, **kwargs):
            return {"success": True, "confidence": 0.9, "message": "Ìè¥Î∞± Íµ¨ÌòÑ"}
        
        async def process_step_5_clothing_analysis(self, **kwargs):
            return {"success": True, "confidence": 0.9, "message": "Ìè¥Î∞± Íµ¨ÌòÑ"}
        
        async def process_step_6_geometric_matching(self, **kwargs):
            return {"success": True, "confidence": 0.9, "message": "Ìè¥Î∞± Íµ¨ÌòÑ"}
        
        async def process_step_7_virtual_fitting(self, **kwargs):
            return {"success": True, "confidence": 0.9, "message": "Ìè¥Î∞± Íµ¨ÌòÑ"}
        
        async def process_step_8_result_analysis(self, **kwargs):
            return {"success": True, "confidence": 0.9, "message": "Ìè¥Î∞± Íµ¨ÌòÑ"}
        
        async def process_complete_virtual_fitting(self, **kwargs):
            return {"success": True, "confidence": 0.9, "message": "Ìè¥Î∞± Íµ¨ÌòÑ"}
        
        async def cleanup_all(self):
            pass
    
    def get_step_service_manager():
        return StepServiceManager()

# =============================================================================
# üî• Step 7: ÎùºÏö∞ÌÑ∞Îì§ import
# =============================================================================

# 7.1 step_routes.py ÎùºÏö∞ÌÑ∞ import (ÌïµÏã¨!)
STEP_ROUTES_AVAILABLE = False
try:
    from app.api.step_routes import router as step_router
    STEP_ROUTES_AVAILABLE = True
    print("‚úÖ step_routes.py ÎùºÏö∞ÌÑ∞ import ÏÑ±Í≥µ!")
except ImportError as e:
    print(f"‚ö†Ô∏è step_routes.py import Ïã§Ìå®: {e}")
    step_router = None

# 7.2 WebSocket ÎùºÏö∞ÌÑ∞ import
WEBSOCKET_ROUTES_AVAILABLE = False
try:
    from app.api.websocket_routes import router as websocket_router
    WEBSOCKET_ROUTES_AVAILABLE = True
    print("‚úÖ WebSocket ÎùºÏö∞ÌÑ∞ import ÏÑ±Í≥µ")
except ImportError as e:
    print(f"‚ö†Ô∏è WebSocket ÎùºÏö∞ÌÑ∞ import Ïã§Ìå®: {e}")
    websocket_router = None

# =============================================================================
# üî• Step 8: Î°úÍπÖ ÏãúÏä§ÌÖú ÏÑ§Ï†ï (ÏôÑÏ†ÑÌïú Íµ¨ÌòÑ)
# =============================================================================

# Î°úÍ∑∏ Ïä§ÌÜ†Î¶¨ÏßÄ
log_storage: List[Dict[str, Any]] = []
MAX_LOG_ENTRIES = 1000

# Ï§ëÎ≥µ Î∞©ÏßÄÎ•º ÏúÑÌïú Í∏ÄÎ°úÎ≤å ÌîåÎûòÍ∑∏
_logging_initialized = False

class MemoryLogHandler(logging.Handler):
    """Î©îÎ™®Î¶¨ Î°úÍ∑∏ Ìï∏Îì§Îü¨"""
    def emit(self, record):
        try:
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "level": record.levelname,
                "logger": record.name,
                "message": record.getMessage(),
                "module": record.module,
                "function": record.funcName,
                "line": record.lineno
            }
            
            if record.exc_info:
                log_entry["exception"] = self.format(record)
            
            log_storage.append(log_entry)
            
            if len(log_storage) > MAX_LOG_ENTRIES:
                log_storage.pop(0)
                
        except Exception:
            pass

def setup_logging_system():
    """ÏôÑÏ†ÑÌïú Î°úÍπÖ ÏãúÏä§ÌÖú ÏÑ§Ï†ï"""
    global _logging_initialized
    
    if _logging_initialized:
        return logging.getLogger(__name__)
    
    # Î£®Ìä∏ Î°úÍ±∞ Ï†ïÎ¶¨
    root_logger = logging.getLogger()
    for handler in root_logger.handlers[:]:
        try:
            handler.close()
        except:
            pass
        root_logger.removeHandler(handler)
    
    root_logger.setLevel(logging.INFO)
    
    # ÎîîÎ†âÌÜ†Î¶¨ ÏÑ§Ï†ï
    log_dir = backend_root / "logs"
    log_dir.mkdir(exist_ok=True)
    
    today = datetime.now().strftime("%Y%m%d")
    log_file = log_dir / f"mycloset-ai-{today}.log"
    error_log_file = log_dir / f"error-{today}.log"
    
    # Ìè¨Îß∑ÌÑ∞ ÏÑ§Ï†ï
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(module)s:%(funcName)s:%(lineno)d] - %(message)s'
    )
    
    console_formatter = logging.Formatter(
        '%(asctime)s | %(levelname)s | %(message)s'
    )
    
    # ÌååÏùº Ìï∏Îì§Îü¨ (INFO Ïù¥ÏÉÅ)
    main_file_handler = logging.handlers.RotatingFileHandler(
        log_file, 
        maxBytes=10*1024*1024,
        backupCount=3,
        encoding='utf-8'
    )
    main_file_handler.setLevel(logging.INFO)
    main_file_handler.setFormatter(formatter)
    root_logger.addHandler(main_file_handler)
    
    # ÏóêÎü¨ ÌååÏùº Ìï∏Îì§Îü¨ (ERROR Ïù¥ÏÉÅ)
    error_file_handler = logging.handlers.RotatingFileHandler(
        error_log_file,
        maxBytes=5*1024*1024,
        backupCount=2,
        encoding='utf-8'
    )
    error_file_handler.setLevel(logging.ERROR)
    error_file_handler.setFormatter(formatter)
    root_logger.addHandler(error_file_handler)
    
    # ÏΩòÏÜî Ìï∏Îì§Îü¨ (INFO Ïù¥ÏÉÅ)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # Î©îÎ™®Î¶¨ Ìï∏Îì§Îü¨
    memory_handler = MemoryLogHandler()
    memory_handler.setLevel(logging.INFO)
    memory_handler.setFormatter(formatter)
    root_logger.addHandler(memory_handler)
    
    # Ïô∏Î∂Ä ÎùºÏù¥Î∏åÎü¨Î¶¨ Î°úÍ±∞ Ï†úÏñ¥
    noisy_loggers = [
        'urllib3', 'requests', 'PIL', 'matplotlib', 
        'tensorflow', 'torch', 'transformers', 'diffusers',
        'timm', 'coremltools', 'watchfiles', 'multipart'
    ]
    
    for logger_name in noisy_loggers:
        logging.getLogger(logger_name).setLevel(logging.WARNING)
        logging.getLogger(logger_name).propagate = False
    
    # FastAPI/Uvicorn Î°úÍ±∞ ÌäπÎ≥Ñ Ï≤òÎ¶¨
    logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
    logging.getLogger("uvicorn.error").setLevel(logging.INFO)
    logging.getLogger("fastapi").setLevel(logging.WARNING)
    
    _logging_initialized = True
    return logging.getLogger(__name__)

# Î°úÍπÖ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
logger = setup_logging_system()

# Î°úÍπÖ Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
def log_step_start(step: int, session_id: str, message: str):
    logger.info(f"üöÄ STEP {step} START | Session: {session_id} | {message}")

def log_step_complete(step: int, session_id: str, processing_time: float, message: str):
    logger.info(f"‚úÖ STEP {step} COMPLETE | Session: {session_id} | Time: {processing_time:.2f}s | {message}")

def log_step_error(step: int, session_id: str, error: str):
    logger.error(f"‚ùå STEP {step} ERROR | Session: {session_id} | Error: {error}")

def log_websocket_event(event: str, session_id: str, details: str = ""):
    logger.info(f"üì° WEBSOCKET {event} | Session: {session_id} | {details}")

def log_api_request(method: str, path: str, session_id: str = None):
    session_info = f" | Session: {session_id}" if session_id else ""
    logger.info(f"üåê API {method} {path}{session_info}")

def log_system_event(event: str, details: str = ""):
    logger.info(f"üîß SYSTEM {event} | {details}")

def log_ai_event(event: str, details: str = ""):
    logger.info(f"ü§ñ AI {event} | {details}")

# =============================================================================
# üî• Step 9: Îç∞Ïù¥ÌÑ∞ Î™®Îç∏ Ï†ïÏùò (AI Ïó∞Îèô Î≤ÑÏ†Ñ)
# =============================================================================

class SystemInfo(BaseModel):
    app_name: str = "MyCloset AI"
    app_version: str = "4.1.0"
    device: str = "Apple M3 Max" if IS_M3_MAX else "CPU"
    device_name: str = "MacBook Pro M3 Max" if IS_M3_MAX else "Standard Device"
    is_m3_max: bool = IS_M3_MAX
    total_memory_gb: int = 128 if IS_M3_MAX else 16
    available_memory_gb: int = 96 if IS_M3_MAX else 12
    ai_pipeline_available: bool = PIPELINE_MANAGER_AVAILABLE
    model_loader_available: bool = MODEL_LOADER_AVAILABLE
    ai_steps_count: int = len(ai_step_classes)
    coroutine_patches_applied: bool = True
    timestamp: int

class AISystemStatus(BaseModel):
    pipeline_manager: bool = PIPELINE_MANAGER_AVAILABLE
    model_loader: bool = MODEL_LOADER_AVAILABLE
    ai_steps: bool = AI_STEPS_AVAILABLE
    memory_manager: bool = MEMORY_MANAGER_AVAILABLE
    session_manager: bool = SESSION_MANAGER_AVAILABLE
    step_service: bool = STEP_SERVICE_AVAILABLE
    coroutine_patches: bool = True
    available_ai_models: List[str] = []
    gpu_memory_gb: float = 0.0
    cpu_count: int = 1

class StepResult(BaseModel):
    success: bool
    message: str
    processing_time: float
    confidence: float
    error: Optional[str] = None
    details: Optional[Dict[str, Any]] = None
    fitted_image: Optional[str] = None
    fit_score: Optional[float] = None
    recommendations: Optional[List[str]] = None
    ai_processed: bool = False
    model_used: Optional[str] = None

class TryOnResult(BaseModel):
    success: bool
    message: str
    processing_time: float
    confidence: float
    session_id: str
    fitted_image: Optional[str] = None
    fit_score: float
    measurements: Dict[str, float]
    clothing_analysis: Dict[str, Any]
    recommendations: List[str]
    ai_pipeline_used: bool = False
    models_used: List[str] = []

# =============================================================================
# üî• Step 10: Í∏ÄÎ°úÎ≤å Î≥ÄÏàò Î∞è ÏÉÅÌÉú Í¥ÄÎ¶¨ (AI Ïó∞Îèô Î≤ÑÏ†Ñ)
# =============================================================================

# ÌôúÏÑ± ÏÑ∏ÏÖò Ï†ÄÏû•ÏÜå
active_sessions: Dict[str, Dict[str, Any]] = {}
websocket_connections: Dict[str, WebSocket] = {}

# AI ÌååÏù¥ÌîÑÎùºÏù∏ Í∏ÄÎ°úÎ≤å Ïù∏Ïä§ÌÑ¥Ïä§Îì§
pipeline_manager = None
model_loader = None
utils_manager = None
memory_manager = None
ai_steps_cache: Dict[str, Any] = {}

# ÎîîÎ†âÌÜ†Î¶¨ ÏÑ§Ï†ï
UPLOAD_DIR = backend_root / "static" / "uploads"
RESULTS_DIR = backend_root / "static" / "results"
MODELS_DIR = backend_root / "models"
CHECKPOINTS_DIR = backend_root / "checkpoints"

# ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
for directory in [UPLOAD_DIR, RESULTS_DIR, MODELS_DIR, CHECKPOINTS_DIR]:
    directory.mkdir(parents=True, exist_ok=True)

# AI ÏãúÏä§ÌÖú ÏÉÅÌÉú
ai_system_status = {
    "initialized": False,
    "pipeline_ready": False,
    "models_loaded": 0,
    "last_initialization": None,
    "error_count": 0,
    "success_count": 0,
    "coroutine_patches_applied": True
}

# =============================================================================
# üî• Step 11: AI ÌååÏù¥ÌîÑÎùºÏù∏ Ï¥àÍ∏∞Ìôî ÏãúÏä§ÌÖú (4Îã®Í≥Ñ Ìè¥Î∞±)
# =============================================================================

async def initialize_ai_pipeline() -> bool:
    """AI ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÏ†Ñ Ï¥àÍ∏∞Ìôî (4Îã®Í≥Ñ Ìè¥Î∞± Î©îÏª§ÎãàÏ¶ò)"""
    global pipeline_manager, model_loader, utils_manager, memory_manager
    
    try:
        log_ai_event("INITIALIZATION_START", "AI ÌååÏù¥ÌîÑÎùºÏù∏ Ï¥àÍ∏∞Ìôî ÏãúÏûë (Ìå®Ïπò Ï†ÅÏö©Îê®)")
        start_time = time.time()
        
        # ===== 1Îã®Í≥Ñ: ÏµúÍ≥†Í∏â PipelineManager ÏãúÎèÑ =====
        try:
            log_ai_event("STAGE_1_START", "PipelineManager Ï¥àÍ∏∞Ìôî ÏãúÎèÑ")
            
            if PIPELINE_MANAGER_AVAILABLE:
                # M3 Max ÏµúÏ†ÅÌôîÎêú ÌååÏù¥ÌîÑÎùºÏù∏ ÏÉùÏÑ±
                if IS_M3_MAX:
                    pipeline_manager = create_m3_max_pipeline(
                        quality_level=QualityLevel.HIGH,
                        enable_optimization=True,
                        memory_gb=128,
                        device="mps"
                    )
                else:
                    pipeline_manager = create_production_pipeline(
                        quality_level=QualityLevel.BALANCED,
                        enable_optimization=True
                    )
                
                # ÎπÑÎèôÍ∏∞ Ï¥àÍ∏∞Ìôî
                if hasattr(pipeline_manager, 'initialize'):
                    success = await pipeline_manager.initialize()
                    if success:
                        log_ai_event("STAGE_1_SUCCESS", "PipelineManager ÏôÑÏ†Ñ Ï¥àÍ∏∞Ìôî ÏÑ±Í≥µ")
                        ai_system_status["pipeline_ready"] = True
                        ai_system_status["initialized"] = True
                        return True
                    else:
                        log_ai_event("STAGE_1_PARTIAL", "PipelineManager Ï¥àÍ∏∞Ìôî Î∂ÄÎ∂Ñ Ïã§Ìå®")
                else:
                    log_ai_event("STAGE_1_NO_INIT", "PipelineManagerÏóê initialize Î©îÏÑúÎìú ÏóÜÏùå")
            
        except Exception as e:
            log_ai_event("STAGE_1_ERROR", f"PipelineManager Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            logger.debug(f"ÏÉÅÏÑ∏ Ïò§Î•ò: {traceback.format_exc()}")
        
        # ===== 2Îã®Í≥Ñ: ModelLoader + Í∞úÎ≥Ñ AI Steps Ï°∞Ìï© =====
        try:
            log_ai_event("STAGE_2_START", "ModelLoader + AI Steps Ï°∞Ìï© ÏãúÎèÑ")
            
            if MODEL_LOADER_AVAILABLE:
                # Ï†ÑÏó≠ ModelLoader Ï¥àÍ∏∞Ìôî
                model_loader = get_global_model_loader()
                if model_loader and hasattr(model_loader, 'initialize'):
                    await model_loader.initialize()
                    log_ai_event("STAGE_2_MODEL_LOADER", "ModelLoader Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
                
                # UnifiedUtilsManager Ï¥àÍ∏∞Ìôî
                utils_manager = get_utils_manager()
                if utils_manager and hasattr(utils_manager, 'initialize'):
                    await utils_manager.initialize()
                    log_ai_event("STAGE_2_UTILS", "UnifiedUtilsManager Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
                
                # Í∞úÎ≥Ñ AI Steps Ï¥àÍ∏∞Ìôî
                if AI_STEPS_AVAILABLE:
                    step_count = 0
                    for step_id, step_class in ai_step_classes.items():
                        try:
                            step_config = {
                                'device': os.environ.get('DEVICE', 'cpu'),
                                'optimization_enabled': True,
                                'memory_gb': 128 if IS_M3_MAX else 16,
                                'is_m3_max': IS_M3_MAX
                            }
                            
                            step_instance = step_class(**step_config)
                            if hasattr(step_instance, 'initialize'):
                                await step_instance.initialize()
                            
                            ai_steps_cache[f"step_{step_id}"] = step_instance
                            step_count += 1
                            log_ai_event("STAGE_2_STEP", f"Step {step_id} Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
                            
                        except Exception as e:
                            log_ai_event("STAGE_2_STEP_ERROR", f"Step {step_id} Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
                    
                    if step_count >= 4:  # ÏµúÏÜå 4Í∞ú Step ÏÑ±Í≥µÌïòÎ©¥ OK
                        ai_system_status["models_loaded"] = step_count
                        ai_system_status["initialized"] = True
                        log_ai_event("STAGE_2_SUCCESS", f"AI Steps Ï°∞Ìï© ÏÑ±Í≥µ ({step_count}Í∞ú)")
                        return True
            
        except Exception as e:
            log_ai_event("STAGE_2_ERROR", f"Stage 2 Ïã§Ìå®: {e}")
        
        # ===== 3Îã®Í≥Ñ: Í∏∞Î≥∏ ÏÑúÎπÑÏä§ Î†àÎ≤® ÌååÏù¥ÌîÑÎùºÏù∏ =====
        try:
            log_ai_event("STAGE_3_START", "ÏÑúÎπÑÏä§ Î†àÎ≤® ÌååÏù¥ÌîÑÎùºÏù∏ ÏãúÎèÑ")
            
            # Í∏∞Î≥∏ AI ÌååÏù¥ÌîÑÎùºÏù∏ ÌÅ¥ÎûòÏä§ ÏÉùÏÑ±
            class BasicAIPipeline:
                def __init__(self):
                    self.is_initialized = False
                    self.device = os.environ.get('DEVICE', 'cpu')
                    self.logger = logging.getLogger("BasicAIPipeline")
                
                async def initialize(self):
                    self.is_initialized = True
                    return True
                
                async def process_virtual_fitting(self, *args, **kwargs):
                    # Í∏∞Î≥∏ Ï≤òÎ¶¨ Î°úÏßÅ
                    await asyncio.sleep(1.0)  # Ï≤òÎ¶¨ ÏãúÎÆ¨Î†àÏù¥ÏÖò
                    return {
                        "success": True,
                        "confidence": 0.75,
                        "message": "Í∏∞Î≥∏ AI ÌååÏù¥ÌîÑÎùºÏù∏ Ï≤òÎ¶¨ ÏôÑÎ£å",
                        "fitted_image": "",
                        "processing_time": 1.0
                    }
                
                def get_pipeline_status(self):
                    return {
                        "initialized": self.is_initialized,
                        "type": "basic_ai_pipeline",
                        "device": self.device
                    }
            
            pipeline_manager = BasicAIPipeline()
            await pipeline_manager.initialize()
            
            ai_system_status["initialized"] = True
            log_ai_event("STAGE_3_SUCCESS", "Í∏∞Î≥∏ AI ÌååÏù¥ÌîÑÎùºÏù∏ ÌôúÏÑ±Ìôî")
            return True
            
        except Exception as e:
            log_ai_event("STAGE_3_ERROR", f"Stage 3 Ïã§Ìå®: {e}")
        
        # ===== 4Îã®Í≥Ñ: ÏµúÏ¢Ö ÏùëÍ∏â Î™®Îìú =====
        try:
            log_ai_event("STAGE_4_START", "ÏùëÍ∏â Î™®Îìú ÌôúÏÑ±Ìôî")
            
            class EmergencyPipeline:
                def __init__(self):
                    self.is_initialized = True
                    self.device = "cpu"
                    self.logger = logging.getLogger("EmergencyPipeline")
                
                async def process_virtual_fitting(self, *args, **kwargs):
                    await asyncio.sleep(0.5)
                    return {
                        "success": True,
                        "confidence": 0.5,
                        "message": "ÏùëÍ∏â Î™®Îìú Ï≤òÎ¶¨ ÏôÑÎ£å",
                        "fitted_image": "",
                        "processing_time": 0.5
                    }
                
                def get_pipeline_status(self):
                    return {
                        "initialized": True,
                        "type": "emergency",
                        "device": "cpu"
                    }
            
            pipeline_manager = EmergencyPipeline()
            ai_system_status["initialized"] = True
            log_ai_event("STAGE_4_SUCCESS", "ÏùëÍ∏â Î™®Îìú ÌôúÏÑ±Ìôî ÏôÑÎ£å")
            return True
            
        except Exception as e:
            log_ai_event("STAGE_4_ERROR", f"ÏùëÍ∏â Î™®ÎìúÎèÑ Ïã§Ìå®: {e}")
            return False
        
        return False
        
    except Exception as e:
        log_ai_event("INITIALIZATION_CRITICAL_ERROR", f"AI Ï¥àÍ∏∞Ìôî ÏôÑÏ†Ñ Ïã§Ìå®: {e}")
        logger.error(f"AI ÌååÏù¥ÌîÑÎùºÏù∏ Ï¥àÍ∏∞Ìôî ÏôÑÏ†Ñ Ïã§Ìå®: {e}")
        return False
    
    finally:
        initialization_time = time.time() - start_time
        ai_system_status["last_initialization"] = datetime.now().isoformat()
        log_ai_event("INITIALIZATION_COMPLETE", f"Ï¥àÍ∏∞Ìôî ÏôÑÎ£å (ÏÜåÏöîÏãúÍ∞Ñ: {initialization_time:.2f}Ï¥à)")

async def initialize_memory_manager():
    """Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ï¥àÍ∏∞Ìôî"""
    global memory_manager
    
    try:
        if MEMORY_MANAGER_AVAILABLE:
            memory_manager = MemoryManager(
                device=os.environ.get('DEVICE', 'cpu'),
                max_memory_gb=128 if IS_M3_MAX else 16,
                optimization_level="aggressive" if IS_M3_MAX else "balanced"
            )
            
            await memory_manager.initialize()
            log_ai_event("MEMORY_MANAGER_READY", "Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return True
    except Exception as e:
        log_ai_event("MEMORY_MANAGER_ERROR", f"Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        return False

# =============================================================================
# üî• Step 12: WebSocket Í¥ÄÎ¶¨Ïûê ÌÅ¥ÎûòÏä§ (Ïã§ÏãúÍ∞Ñ ÏßÑÌñâÎ•†)
# =============================================================================

class AIWebSocketManager:
    """AI Ï≤òÎ¶¨ ÏßÑÌñâÎ•†ÏùÑ ÏúÑÌïú WebSocket Í¥ÄÎ¶¨Ïûê"""
    
    def __init__(self):
        self.connections = {}
        self.active = False
        self.logger = logging.getLogger("AIWebSocketManager")
        self.logger.propagate = False
        
        # AI Ï≤òÎ¶¨ ÏÉÅÌÉú Ï∂îÏ†Å
        self.processing_sessions = {}
        self.step_progress = {}
    
    def start(self):
        self.active = True
        self.logger.info("‚úÖ AI WebSocket Í¥ÄÎ¶¨Ïûê ÏãúÏûë")
    
    def stop(self):
        self.active = False
        self.connections.clear()
        self.processing_sessions.clear()
        self.step_progress.clear()
        self.logger.info("üî• AI WebSocket Í¥ÄÎ¶¨Ïûê Ï†ïÏßÄ")
    
    async def register_connection(self, session_id: str, websocket: WebSocket):
        """WebSocket Ïó∞Í≤∞ Îì±Î°ù"""
        try:
            self.connections[session_id] = websocket
            self.processing_sessions[session_id] = {
                "start_time": datetime.now(),
                "current_step": 0,
                "total_steps": 8,
                "status": "connected"
            }
            log_websocket_event("REGISTER", session_id, "AI ÏßÑÌñâÎ•† WebSocket Îì±Î°ù")
        except Exception as e:
            self.logger.error(f"WebSocket Îì±Î°ù Ïã§Ìå®: {e}")
    
    async def send_ai_progress(self, session_id: str, step: int, progress: float, message: str, ai_details: Dict = None):
        """AI Ï≤òÎ¶¨ ÏßÑÌñâÎ•† Ï†ÑÏÜ°"""
        if session_id in self.connections:
            try:
                progress_data = {
                    "type": "ai_progress",
                    "session_id": session_id,
                    "step": step,
                    "progress": progress,
                    "message": message,
                    "timestamp": datetime.now().isoformat(),
                    "ai_details": ai_details or {},
                    "patches_applied": True
                }
                
                # AI ÏÑ∏Î∂Ä Ï†ïÎ≥¥ Ï∂îÍ∞Ä
                if ai_details:
                    progress_data.update({
                        "model_used": ai_details.get("model_used"),
                        "confidence": ai_details.get("confidence"),
                        "processing_time": ai_details.get("processing_time")
                    })
                
                await self.connections[session_id].send_json(progress_data)
                
                # ÏßÑÌñâÎ•† ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
                if session_id in self.processing_sessions:
                    self.processing_sessions[session_id].update({
                        "current_step": step,
                        "last_progress": progress,
                        "last_update": datetime.now()
                    })
                
                log_websocket_event("AI_PROGRESS_SENT", session_id, f"Step {step}: {progress:.1f}% - {message}")
                
            except Exception as e:
                log_websocket_event("SEND_ERROR", session_id, str(e))
                # Ïó∞Í≤∞ Ïã§Ìå® Ïãú Ï†úÍ±∞
                if session_id in self.connections:
                    del self.connections[session_id]
    
    async def send_ai_completion(self, session_id: str, result: Dict[str, Any]):
        """AI Ï≤òÎ¶¨ ÏôÑÎ£å ÏïåÎ¶º"""
        if session_id in self.connections:
            try:
                completion_data = {
                    "type": "ai_completion",
                    "session_id": session_id,
                    "result": result,
                    "timestamp": datetime.now().isoformat(),
                    "processing_summary": self.processing_sessions.get(session_id, {}),
                    "patches_applied": True
                }
                
                await self.connections[session_id].send_json(completion_data)
                log_websocket_event("AI_COMPLETION", session_id, "AI Ï≤òÎ¶¨ ÏôÑÎ£å ÏïåÎ¶º Ï†ÑÏÜ°")
                
            except Exception as e:
                log_websocket_event("COMPLETION_ERROR", session_id, str(e))

# WebSocket Í¥ÄÎ¶¨Ïûê Ïù∏Ïä§ÌÑ¥Ïä§
ai_websocket_manager = AIWebSocketManager()

# =============================================================================
# üî• Step 13: AI Ï≤òÎ¶¨ ÎèÑÏö∞ÎØ∏ Ìï®ÏàòÎì§
# =============================================================================

async def process_with_ai_pipeline(
    session_id: str, 
    step_id: int, 
    inputs: Dict[str, Any],
    step_name: str
) -> Dict[str, Any]:
    """AI ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ ÌÜµÌïú Ïã§Ï†ú Ï≤òÎ¶¨"""
    try:
        start_time = time.time()
        
        # AI ÏßÑÌñâÎ•† ÏïåÎ¶º
        await ai_websocket_manager.send_ai_progress(
            session_id, step_id, 0.0, f"{step_name} AI Ï≤òÎ¶¨ ÏãúÏûë (Ìå®Ïπò Ï†ÅÏö©Îê®)", 
            {"model_status": "loading", "patches_applied": True}
        )
        
        # Ïã§Ï†ú AI Ï≤òÎ¶¨
        if pipeline_manager and hasattr(pipeline_manager, 'process_step'):
            try:
                # Îã®Í≥ÑÎ≥Ñ AI Ï≤òÎ¶¨
                result = await pipeline_manager.process_step(step_id, inputs)
                
                if result.get("success", False):
                    processing_time = time.time() - start_time
                    
                    # AI ÏÑ±Í≥µ ÏßÑÌñâÎ•† ÏïåÎ¶º
                    await ai_websocket_manager.send_ai_progress(
                        session_id, step_id, 100.0, f"{step_name} AI Ï≤òÎ¶¨ ÏôÑÎ£å",
                        {
                            "model_used": result.get("model_used", "Unknown"),
                            "confidence": result.get("confidence", 0.0),
                            "processing_time": processing_time,
                            "patches_applied": True
                        }
                    )
                    
                    ai_system_status["success_count"] += 1
                    return {
                        **result,
                        "ai_processed": True,
                        "processing_time": processing_time,
                        "session_id": session_id,
                        "patches_applied": True
                    }
            
            except Exception as e:
                log_ai_event("AI_PROCESSING_ERROR", f"Step {step_id} AI Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
        
        # AI Ï∫êÏãúÏóêÏÑú Í∞úÎ≥Ñ Step ÏãúÎèÑ
        if f"step_{step_id}" in ai_steps_cache:
            try:
                step_instance = ai_steps_cache[f"step_{step_id}"]
                
                # 50% ÏßÑÌñâÎ•† ÏïåÎ¶º
                await ai_websocket_manager.send_ai_progress(
                    session_id, step_id, 50.0, f"{step_name} Í∞úÎ≥Ñ AI Î™®Îç∏ Ï≤òÎ¶¨ Ï§ë (Ìå®ÏπòÎê®)",
                    {"model_status": "processing", "patches_applied": True}
                )
                
                if hasattr(step_instance, 'process'):
                    result = await step_instance.process(inputs)
                    
                    if result.get("success", False):
                        processing_time = time.time() - start_time
                        
                        # AI ÏÑ±Í≥µ ÏßÑÌñâÎ•† ÏïåÎ¶º
                        await ai_websocket_manager.send_ai_progress(
                            session_id, step_id, 100.0, f"{step_name} Í∞úÎ≥Ñ AI Ï≤òÎ¶¨ ÏôÑÎ£å",
                            {
                                "model_used": step_instance.__class__.__name__,
                                "confidence": result.get("confidence", 0.0),
                                "processing_time": processing_time,
                                "patches_applied": True
                            }
                        )
                        
                        ai_system_status["success_count"] += 1
                        return {
                            **result,
                            "ai_processed": True,
                            "processing_time": processing_time,
                            "session_id": session_id,
                            "model_used": step_instance.__class__.__name__,
                            "patches_applied": True
                        }
            
            except Exception as e:
                log_ai_event("AI_STEP_ERROR", f"Í∞úÎ≥Ñ Step {step_id} Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
        
        # Ìè¥Î∞±: ÏãúÎÆ¨Î†àÏù¥ÏÖò Ï≤òÎ¶¨
        await ai_websocket_manager.send_ai_progress(
            session_id, step_id, 80.0, f"{step_name} ÏãúÎÆ¨Î†àÏù¥ÏÖò Ï≤òÎ¶¨ Ï§ë",
            {"model_status": "simulation", "patches_applied": True}
        )
        
        # Ïã§Ï†ú Ï≤òÎ¶¨ ÏãúÎÆ¨Î†àÏù¥ÏÖò
        await asyncio.sleep(0.5 + step_id * 0.2)
        processing_time = time.time() - start_time
        
        ai_system_status["error_count"] += 1
        return {
            "success": True,
            "message": f"{step_name} ÏôÑÎ£å (ÏãúÎÆ¨Î†àÏù¥ÏÖò)",
            "confidence": 0.75 + step_id * 0.02,
            "processing_time": processing_time,
            "ai_processed": False,
            "simulation_mode": True,
            "session_id": session_id,
            "patches_applied": True
        }
        
    except Exception as e:
        processing_time = time.time() - start_time
        ai_system_status["error_count"] += 1
        
        log_ai_event("AI_PROCESSING_CRITICAL", f"Step {step_id} Ï≤òÎ¶¨ ÏôÑÏ†Ñ Ïã§Ìå®: {e}")
        return {
            "success": False,
            "message": f"{step_name} Ï≤òÎ¶¨ Ïã§Ìå®",
            "error": str(e),
            "processing_time": processing_time,
            "ai_processed": False,
            "session_id": session_id,
            "patches_applied": True
        }

def get_ai_system_info() -> Dict[str, Any]:
    """AI ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Ï°∞Ìöå"""
    try:
        # Î©îÎ™®Î¶¨ Ï†ïÎ≥¥
        memory_info = {}
        if MEMORY_MANAGER_AVAILABLE and memory_manager:
            memory_info = get_memory_info()
        else:
            try:
                memory = psutil.virtual_memory()
                memory_info = {
                    "total_gb": round(memory.total / (1024**3), 2),
                    "available_gb": round(memory.available / (1024**3), 2),
                    "used_percent": memory.percent
                }
            except:
                memory_info = {"total_gb": 128 if IS_M3_MAX else 16, "available_gb": 96 if IS_M3_MAX else 12}
        
        # AI Î™®Îç∏ Ï†ïÎ≥¥
        available_models = []
        if pipeline_manager and hasattr(pipeline_manager, 'get_available_models'):
            available_models = pipeline_manager.get_available_models()
        
        # GPU Ï†ïÎ≥¥
        gpu_info = {"available": False, "memory_gb": 0.0}
        try:
            if torch.cuda.is_available():
                gpu_info = {
                    "available": True,
                    "device_count": torch.cuda.device_count(),
                    "memory_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3)
                }
            elif torch.backends.mps.is_available():
                gpu_info = {
                    "available": True,
                    "device_type": "Apple MPS",
                    "memory_gb": 128.0 if IS_M3_MAX else 16.0
                }
        except:
            pass
        
        return {
            "ai_system_status": ai_system_status,
            "component_availability": {
                "pipeline_manager": PIPELINE_MANAGER_AVAILABLE,
                "model_loader": MODEL_LOADER_AVAILABLE,
                "ai_steps": AI_STEPS_AVAILABLE,
                "memory_manager": MEMORY_MANAGER_AVAILABLE,
                "session_manager": SESSION_MANAGER_AVAILABLE,
                "step_service": STEP_SERVICE_AVAILABLE,
                "coroutine_patches": True
            },
            "hardware_info": {
                "is_m3_max": IS_M3_MAX,
                "device": os.environ.get('DEVICE', 'cpu'),
                "memory": memory_info,
                "gpu": gpu_info
            },
            "ai_models": {
                "available_models": available_models,
                "loaded_models": len(ai_steps_cache),
                "model_cache": list(ai_steps_cache.keys())
            },
            "performance_metrics": {
                "success_rate": ai_system_status["success_count"] / max(1, ai_system_status["success_count"] + ai_system_status["error_count"]) * 100,
                "total_requests": ai_system_status["success_count"] + ai_system_status["error_count"],
                "last_initialization": ai_system_status["last_initialization"],
                "patches_status": "applied"
            }
        }
        
    except Exception as e:
        logger.error(f"AI ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {e}")
        return {"error": str(e), "patches_status": "applied"}

# =============================================================================
# üî• Step 14: FastAPI ÏÉùÎ™ÖÏ£ºÍ∏∞ Í¥ÄÎ¶¨ (AI ÌÜµÌï©)
# =============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏÉùÎ™ÖÏ£ºÍ∏∞ Í¥ÄÎ¶¨ (AI ÏôÑÏ†Ñ ÌÜµÌï© + Ìå®Ïπò Ï†ÅÏö©)"""
    global session_manager, service_manager
    
    # ===== ÏãúÏûë Îã®Í≥Ñ =====
    try:
        log_system_event("STARTUP_BEGIN", "MyCloset AI ÏÑúÎ≤Ñ ÏãúÏûë (AI ÏôÑÏ†Ñ ÌÜµÌï© + Coroutine Ìå®Ïπò)")
        
        # 1. AI ÌååÏù¥ÌîÑÎùºÏù∏ Ï¥àÍ∏∞Ìôî (ÏµúÏö∞ÏÑ†)
        ai_success = await initialize_ai_pipeline()
        if ai_success:
            log_ai_event("AI_SYSTEM_READY", "AI ÌååÏù¥ÌîÑÎùºÏù∏ ÏãúÏä§ÌÖú Ï§ÄÎπÑ ÏôÑÎ£å (Ìå®Ïπò Ï†ÅÏö©Îê®)")
        else:
            log_ai_event("AI_SYSTEM_FALLBACK", "AI ÏãúÏä§ÌÖúÏù¥ Ìè¥Î∞± Î™®ÎìúÎ°ú Ïã§ÌñâÎê©ÎãàÎã§ (Ìå®Ïπò Ï†ÅÏö©Îê®)")
        
        # 2. Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Ïûê Ï¥àÍ∏∞Ìôî
        await initialize_memory_manager()
        
        # 3. SessionManager Ï¥àÍ∏∞Ìôî
        try:
            session_manager = get_session_manager()
            log_system_event("SESSION_MANAGER_READY", "SessionManager Ï§ÄÎπÑ ÏôÑÎ£å")
        except Exception as e:
            log_system_event("SESSION_MANAGER_FALLBACK", f"SessionManager Ìè¥Î∞±: {e}")
            session_manager = SessionManager()
        
        # 4. StepServiceManager Ï¥àÍ∏∞Ìôî
        try:
            service_manager = get_step_service_manager()
            log_system_event("SERVICE_MANAGER_READY", "StepServiceManager Ï§ÄÎπÑ ÏôÑÎ£å")
        except Exception as e:
            log_system_event("SERVICE_MANAGER_FALLBACK", f"StepServiceManager Ìè¥Î∞±: {e}")
            service_manager = StepServiceManager()
        
        # 5. WebSocket Í¥ÄÎ¶¨Ïûê ÏãúÏûë
        ai_websocket_manager.start()
        
        # 6. Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî
        if memory_manager:
            await memory_manager.optimize_startup()
        
        log_system_event("STARTUP_COMPLETE", f"Î™®Îì† ÏÑúÎπÑÏä§ Ï§ÄÎπÑ ÏôÑÎ£å - AI: {'‚úÖ' if ai_success else '‚ö†Ô∏è'} | Patches: ‚úÖ")
        
        yield
        
    except Exception as e:
        log_system_event("STARTUP_ERROR", f"ÏãúÏûë Ïò§Î•ò: {str(e)}")
        logger.error(f"ÏãúÏûë Ïò§Î•ò: {e}")
        yield
    
    # ===== Ï¢ÖÎ£å Îã®Í≥Ñ =====
    try:
        log_system_event("SHUTDOWN_BEGIN", "ÏÑúÎ≤Ñ Ï¢ÖÎ£å ÏãúÏûë")
        
        # 1. WebSocket Ï†ïÎ¶¨
        ai_websocket_manager.stop()
        
        # 2. AI ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ïÎ¶¨
        if pipeline_manager and hasattr(pipeline_manager, 'cleanup'):
            try:
                if asyncio.iscoroutinefunction(pipeline_manager.cleanup):
                    await pipeline_manager.cleanup()
                else:
                    pipeline_manager.cleanup()
                log_ai_event("AI_CLEANUP", "AI ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ïÎ¶¨ ÏôÑÎ£å")
            except Exception as e:
                log_ai_event("AI_CLEANUP_ERROR", f"AI Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
        
        # 3. AI Steps Ï†ïÎ¶¨
        for step_name, step_instance in ai_steps_cache.items():
            try:
                if hasattr(step_instance, 'cleanup'):
                    if asyncio.iscoroutinefunction(step_instance.cleanup):
                        await step_instance.cleanup()
                    else:
                        step_instance.cleanup()
            except Exception as e:
                logger.warning(f"Step {step_name} Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
        
        # 4. Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
        if memory_manager:
            await memory_manager.cleanup()
        
        # 5. ÏÑúÎπÑÏä§ Îß§ÎãàÏ†Ä Ï†ïÎ¶¨
        if service_manager and hasattr(service_manager, 'cleanup_all'):
            await service_manager.cleanup_all()
        
        # 6. ÏÑ∏ÏÖò Îß§ÎãàÏ†Ä Ï†ïÎ¶¨
        if session_manager and hasattr(session_manager, 'cleanup_all_sessions'):
            await session_manager.cleanup_all_sessions()
        
        # 7. Î©îÎ™®Î¶¨ Í∞ïÏ†ú Ï†ïÎ¶¨
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        elif torch.backends.mps.is_available():
            torch.mps.empty_cache()
        
        log_system_event("SHUTDOWN_COMPLETE", "ÏÑúÎ≤Ñ Ï¢ÖÎ£å ÏôÑÎ£å")
        
    except Exception as e:
        log_system_event("SHUTDOWN_ERROR", f"Ï¢ÖÎ£å Ïò§Î•ò: {str(e)}")
        logger.error(f"Ï¢ÖÎ£å Ïò§Î•ò: {e}")

# =============================================================================
# üî• Step 15: FastAPI Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏÉùÏÑ± (AI ÏôÑÏ†Ñ ÌÜµÌï© + Ìå®Ïπò)
# =============================================================================

app = FastAPI(
    title="MyCloset AI Backend",
    description="AI Í∏∞Î∞ò Í∞ÄÏÉÅ ÌîºÌåÖ ÏÑúÎπÑÏä§ - ÏôÑÏ†Ñ AI Ïó∞Îèô + Coroutine Ìå®Ïπò Î≤ÑÏ†Ñ",
    version="4.1.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# =============================================================================
# üî• Step 16: ÎØ∏Îì§Ïõ®Ïñ¥ ÏÑ§Ï†ï (ÏÑ±Îä• ÏµúÏ†ÅÌôî)
# =============================================================================

# CORS ÏÑ§Ï†ï (ÌîÑÎ°†Ìä∏ÏóîÎìú ÏôÑÏ†Ñ Ìò∏Ìôò)
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://127.0.0.1:3000",
        "http://localhost:4000",
        "http://127.0.0.1:4000", 
        "http://localhost:5173",
        "http://127.0.0.1:5173",
        "http://localhost:8080",
        "http://127.0.0.1:8080",
        "https://mycloset-ai.vercel.app",
        "https://mycloset-ai.netlify.app"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Gzip ÏïïÏ∂ï (ÎåÄÏö©Îüâ Ïù¥ÎØ∏ÏßÄ Ï†ÑÏÜ° ÏµúÏ†ÅÌôî)
app.add_middleware(GZipMiddleware, minimum_size=1000)

# =============================================================================
# üî• Step 17: Ï†ïÏ†Å ÌååÏùº Ï†úÍ≥µ
# =============================================================================

# Ï†ïÏ†Å ÌååÏùº ÎßàÏö¥Ìä∏
app.mount("/static", StaticFiles(directory=str(backend_root / "static")), name="static")

# =============================================================================
# üî• Step 18: ÎùºÏö∞ÌÑ∞ Îì±Î°ù (Í≥ÑÏ∏µÏ†Å Íµ¨Ï°∞)
# =============================================================================

# 1. step_routes.py ÎùºÏö∞ÌÑ∞ Îì±Î°ù (ÏµúÏö∞ÏÑ†!)
if STEP_ROUTES_AVAILABLE and step_router:
    try:
        app.include_router(step_router)
        log_system_event("ROUTER_REGISTERED", "step_routes.py ÎùºÏö∞ÌÑ∞ Îì±Î°ù ÏôÑÎ£å!")
    except Exception as e:
        log_system_event("ROUTER_ERROR", f"step_routes.py ÎùºÏö∞ÌÑ∞ Îì±Î°ù Ïã§Ìå®: {e}")

# 2. WebSocket ÎùºÏö∞ÌÑ∞ Îì±Î°ù
if WEBSOCKET_ROUTES_AVAILABLE and websocket_router:
    try:
        app.include_router(websocket_router)
        log_system_event("WEBSOCKET_REGISTERED", "WebSocket ÎùºÏö∞ÌÑ∞ Îì±Î°ù ÏôÑÎ£å")
    except Exception as e:
        log_system_event("WEBSOCKET_ERROR", f"WebSocket ÎùºÏö∞ÌÑ∞ Îì±Î°ù Ïã§Ìå®: {e}")

# =============================================================================
# üî• Step 19: Í∏∞Î≥∏ API ÏóîÎìúÌè¨Ïù∏Ìä∏Îì§ (AI Ï†ïÎ≥¥ Ìè¨Ìï®)
# =============================================================================

@app.get("/")
async def root():
    """Î£®Ìä∏ ÏóîÎìúÌè¨Ïù∏Ìä∏ (AI ÏãúÏä§ÌÖú Ï†ïÎ≥¥ + Ìå®Ïπò ÏÉÅÌÉú Ìè¨Ìï®)"""
    ai_info = get_ai_system_info()
    
    return {
        "message": "MyCloset AI Server - ÏôÑÏ†Ñ AI Ïó∞Îèô + Coroutine Ìå®Ïπò Î≤ÑÏ†Ñ",
        "status": "running",
        "version": "4.1.0",
        "patches_applied": True,
        "docs": "/docs",
        "redoc": "/redoc",
        "ai_system": {
            "status": "ready" if ai_info["ai_system_status"]["initialized"] else "fallback",
            "components_available": ai_info["component_availability"],
            "ai_models_loaded": ai_info["ai_models"]["loaded_models"],
            "patches_status": "applied",
            "hardware": {
                "device": ai_info["hardware_info"]["device"],
                "is_m3_max": ai_info["hardware_info"]["is_m3_max"],
                "memory_gb": ai_info["hardware_info"]["memory"].get("total_gb", 0)
            }
        },
        "endpoints": {
            "ai_pipeline": "/api/step/1/upload-validation ~ /api/step/8/result-analysis",
            "complete_pipeline": "/api/step/complete",
            "ai_status": "/api/ai/status",
            "ai_models": "/api/ai/models",
            "health_check": "/health",
            "session_management": "/api/step/sessions",
            "websocket": "/api/ws/ai-pipeline"
        },
        "features": {
            "ai_processing": ai_info["ai_system_status"]["initialized"],
            "real_time_progress": WEBSOCKET_ROUTES_AVAILABLE,
            "session_based_images": SESSION_MANAGER_AVAILABLE,
            "8_step_pipeline": STEP_ROUTES_AVAILABLE,
            "m3_max_optimized": IS_M3_MAX,
            "memory_optimized": MEMORY_MANAGER_AVAILABLE,
            "coroutine_patches": True
        }
    }

@app.get("/health")
async def health_check():
    """Ï¢ÖÌï© Ìó¨Ïä§Ï≤¥ÌÅ¨ (AI ÏãúÏä§ÌÖú + Ìå®Ïπò ÏÉÅÌÉú Ìè¨Ìï®)"""
    ai_info = get_ai_system_info()
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "server_version": "4.1.0",
        "patches_applied": True,
        "system": {
            "device": ai_info["hardware_info"]["device"],
            "is_m3_max": ai_info["hardware_info"]["is_m3_max"],
            "memory": ai_info["hardware_info"]["memory"]
        },
        "ai_services": {
            "pipeline_manager": "active" if PIPELINE_MANAGER_AVAILABLE else "fallback",
            "model_loader": "active" if MODEL_LOADER_AVAILABLE else "fallback", 
            "ai_steps": f"{len(ai_steps_cache)} loaded" if AI_STEPS_AVAILABLE else "fallback",
            "memory_manager": "active" if MEMORY_MANAGER_AVAILABLE else "fallback",
            "coroutine_patches": "active"
        },
        "core_services": {
            "session_manager": "active" if SESSION_MANAGER_AVAILABLE else "fallback",
            "step_service": "active" if STEP_SERVICE_AVAILABLE else "fallback",
            "websocket": "active" if WEBSOCKET_ROUTES_AVAILABLE else "disabled"
        },
        "performance": {
            "ai_success_rate": ai_info["performance_metrics"]["success_rate"],
            "total_ai_requests": ai_info["performance_metrics"]["total_requests"],
            "active_sessions": len(active_sessions),
            "patches_status": "applied"
        }
    }

@app.get("/api/system/info")
async def get_system_info() -> SystemInfo:
    """ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Ï°∞Ìöå (AI ÌÜµÌï© + Ìå®Ïπò Ï†ïÎ≥¥)"""
    return SystemInfo(
        app_name="MyCloset AI",
        app_version="4.1.0",
        device="Apple M3 Max" if IS_M3_MAX else "CPU",
        device_name="MacBook Pro M3 Max" if IS_M3_MAX else "Standard Device",
        is_m3_max=IS_M3_MAX,
        total_memory_gb=128 if IS_M3_MAX else 16,
        available_memory_gb=96 if IS_M3_MAX else 12,
        ai_pipeline_available=PIPELINE_MANAGER_AVAILABLE,
        model_loader_available=MODEL_LOADER_AVAILABLE,
        ai_steps_count=len(ai_step_classes),
        coroutine_patches_applied=True,
        timestamp=int(datetime.now().timestamp())
    )

# =============================================================================
# üî• Step 20: AI Ï†ÑÏö© API ÏóîÎìúÌè¨Ïù∏Ìä∏Îì§
# =============================================================================

@app.get("/api/ai/status")
async def get_ai_status() -> AISystemStatus:
    """AI ÏãúÏä§ÌÖú ÏÉÅÌÉú Ï°∞Ìöå (Ìå®Ïπò Ï†ïÎ≥¥ Ìè¨Ìï®)"""
    ai_info = get_ai_system_info()
    
    available_models = []
    gpu_memory = 0.0
    
    try:
        if pipeline_manager and hasattr(pipeline_manager, 'get_available_models'):
            available_models = pipeline_manager.get_available_models()
        
        if ai_info["hardware_info"]["gpu"]["available"]:
            gpu_memory = ai_info["hardware_info"]["gpu"]["memory_gb"]
    except:
        pass
    
    return AISystemStatus(
        pipeline_manager=PIPELINE_MANAGER_AVAILABLE,
        model_loader=MODEL_LOADER_AVAILABLE,
        ai_steps=AI_STEPS_AVAILABLE,
        memory_manager=MEMORY_MANAGER_AVAILABLE,
        session_manager=SESSION_MANAGER_AVAILABLE,
        step_service=STEP_SERVICE_AVAILABLE,
        coroutine_patches=True,
        available_ai_models=available_models,
        gpu_memory_gb=gpu_memory,
        cpu_count=psutil.cpu_count() if hasattr(psutil, 'cpu_count') else 1
    )

@app.get("/api/ai/models")
async def get_ai_models():
    """AI Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå"""
    try:
        models_info = {
            "loaded_models": {},
            "available_checkpoints": [],
            "model_cache": list(ai_steps_cache.keys()),
            "checkpoint_directory": str(CHECKPOINTS_DIR),
            "models_directory": str(MODELS_DIR),
            "patches_applied": True
        }
        
        # Î°úÎìúÎêú AI Steps Ï†ïÎ≥¥
        for step_name, step_instance in ai_steps_cache.items():
            try:
                models_info["loaded_models"][step_name] = {
                    "class": step_instance.__class__.__name__,
                    "initialized": hasattr(step_instance, 'is_initialized') and step_instance.is_initialized,
                    "device": getattr(step_instance, 'device', 'unknown'),
                    "model_name": getattr(step_instance, 'model_name', 'unknown'),
                    "patches_applied": True
                }
            except:
                models_info["loaded_models"][step_name] = {"status": "unknown", "patches_applied": True}
        
        # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùº ÌÉêÏßÄ
        try:
            for checkpoint_file in CHECKPOINTS_DIR.glob("*.pth"):
                size_gb = checkpoint_file.stat().st_size / (1024**3)
                models_info["available_checkpoints"].append({
                    "name": checkpoint_file.name,
                    "size_gb": round(size_gb, 2),
                    "path": str(checkpoint_file),
                    "modified": datetime.fromtimestamp(checkpoint_file.stat().st_mtime).isoformat()
                })
                
            # 89.8GB Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌäπÎ≥Ñ ÌëúÏãú
            large_checkpoints = [cp for cp in models_info["available_checkpoints"] if cp["size_gb"] > 50]
            if large_checkpoints:
                models_info["large_models_detected"] = True
                models_info["large_models"] = large_checkpoints
        except Exception as e:
            models_info["checkpoint_scan_error"] = str(e)
        
        return models_info
        
    except Exception as e:
        return {"error": str(e), "models_info": {}, "patches_applied": True}

@app.post("/api/ai/models/reload")
async def reload_ai_models():
    """AI Î™®Îç∏ Ïû¨Î°úÎìú"""
    try:
        log_ai_event("MODEL_RELOAD_START", "AI Î™®Îç∏ Ïû¨Î°úÎìú ÏãúÏûë (Ìå®Ïπò Ï†ÅÏö©Îê®)")
        
        # AI ÌååÏù¥ÌîÑÎùºÏù∏ Ïû¨Ï¥àÍ∏∞Ìôî
        success = await initialize_ai_pipeline()
        
        if success:
            log_ai_event("MODEL_RELOAD_SUCCESS", "AI Î™®Îç∏ Ïû¨Î°úÎìú ÏÑ±Í≥µ")
            return {
                "success": True,
                "message": "AI Î™®Îç∏Ïù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ïû¨Î°úÎìúÎêòÏóàÏäµÎãàÎã§",
                "loaded_models": len(ai_steps_cache),
                "patches_applied": True,
                "timestamp": datetime.now().isoformat()
            }
        else:
            log_ai_event("MODEL_RELOAD_FAILED", "AI Î™®Îç∏ Ïû¨Î°úÎìú Ïã§Ìå®")
            return {
                "success": False,
                "message": "AI Î™®Îç∏ Ïû¨Î°úÎìúÏóê Ïã§Ìå®ÌñàÏäµÎãàÎã§",
                "patches_applied": True,
                "timestamp": datetime.now().isoformat()
            }
    
    except Exception as e:
        log_ai_event("MODEL_RELOAD_ERROR", f"AI Î™®Îç∏ Ïû¨Î°úÎìú Ïò§Î•ò: {e}")
        return {
            "success": False,
            "error": str(e),
            "patches_applied": True,
            "timestamp": datetime.now().isoformat()
        }

@app.get("/api/ai/performance")
async def get_ai_performance():
    """AI ÏÑ±Îä• Î©îÌä∏Î¶≠ Ï°∞Ìöå"""
    try:
        ai_info = get_ai_system_info()
        
        return {
            "performance_metrics": ai_info["performance_metrics"],
            "system_resources": {
                "memory": ai_info["hardware_info"]["memory"],
                "gpu": ai_info["hardware_info"]["gpu"],
                "device": ai_info["hardware_info"]["device"]
            },
            "ai_statistics": {
                "models_loaded": len(ai_steps_cache),
                "pipeline_ready": ai_system_status["pipeline_ready"],
                "initialization_time": ai_system_status["last_initialization"],
                "patches_applied": True
            },
            "current_load": {
                "active_sessions": len(active_sessions),
                "websocket_connections": len(websocket_connections),
                "processing_sessions": len(ai_websocket_manager.processing_sessions)
            },
            "patches_status": {
                "coroutine_fixes": True,
                "warmup_patches": True,
                "applied_timestamp": ai_system_status["last_initialization"]
            }
        }
    
    except Exception as e:
        return {"error": str(e), "patches_applied": True}

# =============================================================================
# üî• Step 21: WebSocket ÏóîÎìúÌè¨Ïù∏Ìä∏ (AI ÏßÑÌñâÎ•† Ï†ÑÏö©)
# =============================================================================

@app.websocket("/api/ws/ai-pipeline")
async def websocket_ai_pipeline(websocket: WebSocket):
    """AI ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÑÌñâÎ•† Ï†ÑÏö© WebSocket (Ìå®Ïπò Ï†ÅÏö©Îê®)"""
    await websocket.accept()
    session_id = None
    
    try:
        log_websocket_event("AI_WEBSOCKET_CONNECTED", "unknown", "AI ÏßÑÌñâÎ•† WebSocket Ïó∞Í≤∞Îê® (Ìå®Ïπò Ï†ÅÏö©)")
        
        while True:
            data = await websocket.receive_json()
            
            if data.get("type") == "subscribe":
                session_id = data.get("session_id")
                if session_id:
                    await ai_websocket_manager.register_connection(session_id, websocket)
                    
                    await websocket.send_json({
                        "type": "ai_connected",
                        "session_id": session_id,
                        "message": "AI ÏßÑÌñâÎ•† WebSocket Ïó∞Í≤∞Îê® (Ìå®Ïπò Ï†ÅÏö©)",
                        "ai_status": {
                            "pipeline_ready": ai_system_status["pipeline_ready"],
                            "models_loaded": len(ai_steps_cache),
                            "device": os.environ.get('DEVICE', 'cpu'),
                            "patches_applied": True
                        },
                        "timestamp": datetime.now().isoformat()
                    })
            
            elif data.get("type") == "ai_test":
                # AI ÏãúÏä§ÌÖú ÌÖåÏä§Ìä∏
                await websocket.send_json({
                    "type": "ai_test_response",
                    "ai_system_info": get_ai_system_info(),
                    "patches_applied": True,
                    "timestamp": datetime.now().isoformat()
                })
    
    except WebSocketDisconnect:
        log_websocket_event("AI_WEBSOCKET_DISCONNECT", session_id or "unknown", "AI WebSocket Ïó∞Í≤∞ Ìï¥Ï†ú")
        if session_id and session_id in ai_websocket_manager.connections:
            del ai_websocket_manager.connections[session_id]
            if session_id in ai_websocket_manager.processing_sessions:
                del ai_websocket_manager.processing_sessions[session_id]
    
    except Exception as e:
        log_websocket_event("AI_WEBSOCKET_ERROR", session_id or "unknown", str(e))
        if session_id and session_id in ai_websocket_manager.connections:
            del ai_websocket_manager.connections[session_id]

# =============================================================================
# üî• Step 22: Ìè¥Î∞± API (ÎùºÏö∞ÌÑ∞ ÏóÜÎäî Í≤ΩÏö∞)
# =============================================================================

if not STEP_ROUTES_AVAILABLE:
    logger.warning("‚ö†Ô∏è step_routes.py ÏóÜÏùå - AI Í∏∞Îä•Ïù¥ Ìè¨Ìï®Îêú Ìè¥Î∞± API Ï†úÍ≥µ")
    
    @app.post("/api/step/ai-test")
    async def fallback_ai_test():
        """AI Í∏∞Îä• ÌÖåÏä§Ìä∏ ÏóîÎìúÌè¨Ïù∏Ìä∏"""
        try:
            # AI ÏãúÏä§ÌÖú Í∞ÑÎã® ÌÖåÏä§Ìä∏
            if pipeline_manager:
                test_result = await pipeline_manager.process_virtual_fitting(
                    person_image="test",
                    clothing_image="test"
                )
                ai_working = test_result.get("success", False)
            else:
                ai_working = False
            
            return {
                "success": True,
                "message": "AI Ìè¥Î∞± APIÍ∞Ä ÎèôÏûë Ï§ëÏûÖÎãàÎã§ (Ìå®Ïπò Ï†ÅÏö©Îê®)",
                "ai_system": {
                    "pipeline_working": ai_working,
                    "models_loaded": len(ai_steps_cache),
                    "device": os.environ.get('DEVICE', 'cpu'),
                    "m3_max": IS_M3_MAX,
                    "patches_applied": True
                },
                "note": "step_routes.pyÎ•º Ïó∞ÎèôÌïòÏó¨ ÏôÑÏ†ÑÌïú 8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî",
                "missing_components": {
                    "step_routes": not STEP_ROUTES_AVAILABLE,
                    "session_manager": not SESSION_MANAGER_AVAILABLE,
                    "service_manager": not STEP_SERVICE_AVAILABLE
                },
                "patches_status": "applied"
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "ai_system": {"status": "error"},
                "patches_applied": True
            }

# =============================================================================
# üî• Step 23: Í¥ÄÎ¶¨ Î∞è Î™®ÎãàÌÑ∞ÎßÅ API
# =============================================================================

@app.get("/api/logs")
async def get_logs(level: str = None, limit: int = 100, session_id: str = None):
    """Î°úÍ∑∏ Ï°∞Ìöå API (AI Î°úÍ∑∏ + Ìå®Ïπò ÏÉÅÌÉú Ìè¨Ìï®)"""
    try:
        filtered_logs = log_storage.copy()
        
        if level:
            filtered_logs = [log for log in filtered_logs if log.get("level", "").lower() == level.lower()]
        
        if session_id:
            filtered_logs = [log for log in filtered_logs if session_id in log.get("message", "")]
        
        # AI Í¥ÄÎ†® Î°úÍ∑∏ ÌïÑÌÑ∞ÎßÅ
        ai_logs = [log for log in filtered_logs if "AI" in log.get("message", "") or "ü§ñ" in log.get("message", "")]
        
        # Ìå®Ïπò Í¥ÄÎ†® Î°úÍ∑∏ ÌïÑÌÑ∞ÎßÅ
        patch_logs = [log for log in filtered_logs if "Ìå®Ïπò" in log.get("message", "") or "patch" in log.get("message", "").lower()]
        
        filtered_logs = sorted(filtered_logs, key=lambda x: x["timestamp"], reverse=True)[:limit]
        
        return {
            "logs": filtered_logs,
            "total_count": len(log_storage),
            "filtered_count": len(filtered_logs),
            "ai_logs_count": len(ai_logs),
            "patch_logs_count": len(patch_logs),
            "available_levels": list(set(log.get("level") for log in log_storage)),
            "ai_system_status": ai_system_status,
            "patches_applied": True,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Î°úÍ∑∏ Ï°∞Ìöå Ïã§Ìå®: {e}")
        return {"error": str(e), "patches_applied": True}

@app.get("/api/sessions")
async def list_active_sessions():
    """ÌôúÏÑ± ÏÑ∏ÏÖò Î™©Î°ù Ï°∞Ìöå (AI Ï≤òÎ¶¨ ÏÉÅÌÉú + Ìå®Ïπò Ï†ïÎ≥¥ Ìè¨Ìï®)"""
    try:
        session_stats = {}
        if session_manager and hasattr(session_manager, 'get_all_sessions_status'):
            session_stats = session_manager.get_all_sessions_status()
        
        return {
            "active_sessions": len(active_sessions),
            "websocket_connections": len(websocket_connections),
            "ai_processing_sessions": len(ai_websocket_manager.processing_sessions),
            "session_manager_stats": session_stats,
            "ai_system_status": ai_system_status,
            "patches_applied": True,
            "sessions": {
                session_id: {
                    "created_at": session.get("created_at", datetime.now()).isoformat() if hasattr(session.get("created_at", datetime.now()), 'isoformat') else str(session.get("created_at")),
                    "status": session.get("status", "unknown"),
                    "ai_processed": session.get("ai_processed", False),
                    "patches_applied": True
                } for session_id, session in active_sessions.items()
            },
            "ai_performance": {
                "success_rate": ai_system_status["success_count"] / max(1, ai_system_status["success_count"] + ai_system_status["error_count"]) * 100,
                "total_requests": ai_system_status["success_count"] + ai_system_status["error_count"],
                "patches_status": "applied"
            }
        }
    except Exception as e:
        return {"error": str(e), "patches_applied": True}

@app.get("/api/status")
async def get_detailed_status():
    """ÏÉÅÏÑ∏ ÏÉÅÌÉú Ï†ïÎ≥¥ Ï°∞Ìöå (AI ÏôÑÏ†Ñ ÌÜµÌï© + Ìå®Ïπò Ï†ïÎ≥¥)"""
    try:
        ai_info = get_ai_system_info()
        
        pipeline_status = {"initialized": False, "type": "none"}
        if pipeline_manager:
            if hasattr(pipeline_manager, 'get_pipeline_status'):
                pipeline_status = pipeline_manager.get_pipeline_status()
            else:
                pipeline_status = {
                    "initialized": getattr(pipeline_manager, 'is_initialized', False),
                    "type": type(pipeline_manager).__name__
                }
        
        return {
            "server_status": "running",
            "ai_pipeline_status": pipeline_status,
            "ai_system_info": ai_info,
            "active_sessions": len(active_sessions),
            "websocket_connections": len(websocket_connections),
            "ai_websocket_connections": len(ai_websocket_manager.connections),
            "memory_usage": _get_memory_usage(),
            "timestamp": time.time(),
            "version": "4.1.0",
            "patches_applied": True,
            "features": {
                "ai_pipeline_integrated": PIPELINE_MANAGER_AVAILABLE,
                "model_loader_available": MODEL_LOADER_AVAILABLE,
                "ai_steps_loaded": len(ai_steps_cache),
                "m3_max_optimized": IS_M3_MAX,
                "memory_managed": MEMORY_MANAGER_AVAILABLE,
                "session_based": SESSION_MANAGER_AVAILABLE,
                "real_time_progress": WEBSOCKET_ROUTES_AVAILABLE,
                "coroutine_patches": True
            },
            "performance": {
                "ai_success_rate": ai_info["performance_metrics"]["success_rate"],
                "ai_total_requests": ai_info["performance_metrics"]["total_requests"],
                "pipeline_initialized": ai_system_status["initialized"],
                "models_ready": ai_system_status["pipeline_ready"],
                "patches_status": "applied"
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {e}")
        return {
            "error": str(e),
            "timestamp": time.time(),
            "fallback_status": "error",
            "patches_applied": True
        }

def _get_memory_usage():
    """Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Ï°∞Ìöå (AI ÏµúÏ†ÅÌôî)"""
    try:
        # ÏãúÏä§ÌÖú Î©îÎ™®Î¶¨
        memory_info = {"system": {}}
        try:
            memory = psutil.virtual_memory()
            memory_info["system"] = {
                "total_gb": round(memory.total / (1024**3), 2),
                "available_gb": round(memory.available / (1024**3), 2),
                "used_gb": round(memory.used / (1024**3), 2),
                "percent": memory.percent
            }
        except:
            memory_info["system"] = {"error": "psutil not available"}
        
        # GPU Î©îÎ™®Î¶¨
        memory_info["gpu"] = {"available": False}
        try:
            if torch.cuda.is_available():
                memory_info["gpu"] = {
                    "available": True,
                    "allocated_gb": round(torch.cuda.memory_allocated() / (1024**3), 2),
                    "cached_gb": round(torch.cuda.memory_reserved() / (1024**3), 2),
                    "total_gb": round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2)
                }
            elif torch.backends.mps.is_available():
                memory_info["gpu"] = {
                    "available": True,
                    "type": "Apple MPS",
                    "allocated_gb": round(torch.mps.current_allocated_memory() / (1024**3), 2) if hasattr(torch.mps, 'current_allocated_memory') else 0,
                    "total_gb": 128.0 if IS_M3_MAX else 16.0
                }
        except Exception as e:
            memory_info["gpu"]["error"] = str(e)
        
        # AI Î™®Îç∏ Î©îÎ™®Î¶¨ Ï∂îÏ†ï
        memory_info["ai_models"] = {
            "loaded_models": len(ai_steps_cache),
            "estimated_memory_gb": len(ai_steps_cache) * 2.5,  # Î™®Îç∏Îãπ ÌèâÍ∑† 2.5GB Ï∂îÏ†ï
            "patches_applied": True
        }
        
        return memory_info
        
    except Exception as e:
        return {"error": str(e), "patches_applied": True}

# =============================================================================
# üî• Step 24: Ï†ÑÏó≠ ÏòàÏô∏ Ï≤òÎ¶¨Í∏∞ (AI Ïò§Î•ò + Ìå®Ïπò Ï†ïÎ≥¥ Ìè¨Ìï®)
# =============================================================================

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Ï†ÑÏó≠ ÏòàÏô∏ Ï≤òÎ¶¨Í∏∞ (AI Ïò§Î•ò Ï∂îÏ†Å + Ìå®Ïπò Ï†ïÎ≥¥)"""
    error_id = str(uuid.uuid4())[:8]
    
    # AI Í¥ÄÎ†® Ïò§Î•òÏù∏ÏßÄ ÌôïÏù∏
    is_ai_error = any(keyword in str(exc) for keyword in ["pipeline", "model", "tensor", "cuda", "mps", "torch"])
    
    # Coroutine Í¥ÄÎ†® Ïò§Î•òÏù∏ÏßÄ ÌôïÏù∏
    is_coroutine_error = any(keyword in str(exc) for keyword in ["coroutine", "awaited", "callable"])
    
    if is_ai_error:
        log_ai_event("AI_GLOBAL_ERROR", f"ID: {error_id} | {str(exc)}")
        ai_system_status["error_count"] += 1
    elif is_coroutine_error:
        log_ai_event("COROUTINE_ERROR", f"ID: {error_id} | {str(exc)} (Ìå®Ïπò ÌôïÏù∏ ÌïÑÏöî)")
    else:
        logger.error(f"Ï†ÑÏó≠ Ïò§Î•ò ID: {error_id} | {exc}", exc_info=True)
    
    return JSONResponse(
        status_code=500,
        content={
            "success": False,
            "error": "ÏÑúÎ≤Ñ ÎÇ¥Î∂Ä Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§",
            "error_id": error_id,
            "detail": str(exc),
            "server_version": "4.1.0",
            "ai_system_available": ai_system_status["initialized"],
            "is_ai_related": is_ai_error,
            "is_coroutine_related": is_coroutine_error,
            "patches_applied": True,
            "timestamp": datetime.now().isoformat()
        }
    )

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """HTTP ÏòàÏô∏ Ï≤òÎ¶¨Í∏∞"""
    logger.warning(f"HTTP ÏòàÏô∏: {exc.status_code} - {exc.detail}")
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "success": False,
            "error": exc.detail,
            "status_code": exc.status_code,
            "patches_applied": True,
            "timestamp": datetime.now().isoformat()
        }
    )

# =============================================================================
# üî• Step 25: ÏÑúÎ≤Ñ ÏãúÏûë Ï†ïÎ≥¥ Ï∂úÎ†• (AI ÏôÑÏ†Ñ ÌÜµÌï© + Ìå®Ïπò Ï†ïÎ≥¥)
# =============================================================================

if __name__ == "__main__":
    print("\n" + "="*100)
    print("üöÄ MyCloset AI ÏÑúÎ≤Ñ ÏãúÏûë! (ÏôÑÏ†Ñ AI Ïó∞Îèô + Coroutine Ìå®Ïπò Î≤ÑÏ†Ñ)")
    print("="*100)
    print(f"üìÅ Î∞±ÏóîÎìú Î£®Ìä∏: {backend_root}")
    print(f"üåê ÏÑúÎ≤Ñ Ï£ºÏÜå: http://localhost:8000")
    print(f"üìö API Î¨∏ÏÑú: http://localhost:8000/docs")
    print(f"üîß ReDoc: http://localhost:8000/redoc")
    print("="*100)
    print("üîß Ìå®Ïπò ÏÉÅÌÉú:")
    print(f"  ‚úÖ Coroutine Ïò§Î•ò ÏàòÏ†ï Ìå®Ïπò: Ï†ÅÏö©Îê®")
    print(f"  ‚úÖ ÏõåÎ∞çÏóÖ ÏïàÏ†Ñ Ìå®Ïπò: Ï†ÅÏö©Îê®")
    print(f"  ‚úÖ RuntimeWarning Ìï¥Í≤∞: Ï†ÅÏö©Îê®")
    print(f"  ‚úÖ 'object is not callable' Ìï¥Í≤∞: Ï†ÅÏö©Îê®")
    print("="*100)
    print("üß† AI ÏãúÏä§ÌÖú ÏÉÅÌÉú:")
    print(f"  ü§ñ PipelineManager: {'‚úÖ Ïó∞ÎèôÎê®' if PIPELINE_MANAGER_AVAILABLE else '‚ùå Ìè¥Î∞±Î™®Îìú'}")
    print(f"  üß† ModelLoader: {'‚úÖ Ïó∞ÎèôÎê®' if MODEL_LOADER_AVAILABLE else '‚ùå Ìè¥Î∞±Î™®Îìú'}")
    print(f"  üî¢ AI Steps: {'‚úÖ Ïó∞ÎèôÎê®' if AI_STEPS_AVAILABLE else '‚ùå Ìè¥Î∞±Î™®Îìú'} ({len(ai_step_classes)}Í∞ú)")
    print(f"  üíæ MemoryManager: {'‚úÖ Ïó∞ÎèôÎê®' if MEMORY_MANAGER_AVAILABLE else '‚ùå ÏóÜÏùå'}")
    print(f"  üçé M3 Max ÏµúÏ†ÅÌôî: {'‚úÖ ÌôúÏÑ±Ìôî' if IS_M3_MAX else '‚ùå ÎπÑÌôúÏÑ±Ìôî'}")
    print("="*100)
    print("üîß ÌïµÏã¨ ÏÑúÎπÑÏä§ ÏÉÅÌÉú:")
    print(f"  üìã SessionManager: {'‚úÖ Ïó∞ÎèôÎê®' if SESSION_MANAGER_AVAILABLE else '‚ùå Ìè¥Î∞±Î™®Îìú'}")
    print(f"  ‚öôÔ∏è StepServiceManager: {'‚úÖ Ïó∞ÎèôÎê®' if STEP_SERVICE_AVAILABLE else '‚ùå Ìè¥Î∞±Î™®Îìú'}")
    print(f"  üåê step_routes.py: {'‚úÖ Ïó∞ÎèôÎê®' if STEP_ROUTES_AVAILABLE else '‚ùå Ìè¥Î∞±Î™®Îìú'}")
    print(f"  üì° WebSocket: {'‚úÖ Ïó∞ÎèôÎê®' if WEBSOCKET_ROUTES_AVAILABLE else '‚ùå ÏóÜÏùå'}")
    print("="*100)
    print("üìã ÏÇ¨Ïö© Í∞ÄÎä•Ìïú API:")
    if STEP_ROUTES_AVAILABLE:
        print("  üéØ 8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏:")
        print("    ‚Ä¢ POST /api/step/1/upload-validation")
        print("    ‚Ä¢ POST /api/step/2/measurements-validation") 
        print("    ‚Ä¢ POST /api/step/3/human-parsing")
        print("    ‚Ä¢ POST /api/step/4/pose-estimation")
        print("    ‚Ä¢ POST /api/step/5/clothing-analysis")
        print("    ‚Ä¢ POST /api/step/6/geometric-matching")
        print("    ‚Ä¢ POST /api/step/7/virtual-fitting")
        print("    ‚Ä¢ POST /api/step/8/result-analysis")
        print("    ‚Ä¢ POST /api/step/complete")
    else:
        print("  ‚ö†Ô∏è 8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏: Ìè¥Î∞± Î™®Îìú")
        print("    ‚Ä¢ POST /api/step/ai-test (Ìè¥Î∞±)")
    
    print("  ü§ñ AI Ï†ÑÏö© API:")
    print("    ‚Ä¢ GET /api/ai/status")
    print("    ‚Ä¢ GET /api/ai/models")
    print("    ‚Ä¢ POST /api/ai/models/reload")
    print("    ‚Ä¢ GET /api/ai/performance")
    
    print("  üìä Í¥ÄÎ¶¨ API:")
    print("    ‚Ä¢ GET /health")
    print("    ‚Ä¢ GET /api/system/info")
    print("    ‚Ä¢ GET /api/status")
    print("    ‚Ä¢ GET /api/logs")
    print("    ‚Ä¢ GET /api/sessions")
    
    if WEBSOCKET_ROUTES_AVAILABLE:
        print("  üì° Ïã§ÏãúÍ∞Ñ ÌÜµÏã†:")
        print("    ‚Ä¢ WS /api/ws/ai-pipeline")
    
    print("="*100)
    print("üéØ AI Í∏∞Îä•:")
    print("  ‚úÖ Ïã§Ï†ú AI Î™®Îç∏ Î°úÎî© Î∞è Ï∂îÎ°†")
    print("  ‚úÖ 89.8GB Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ")
    print("  ‚úÖ M3 Max MPS Í∞ÄÏÜç (128GB)")
    print("  ‚úÖ ÎèôÏ†Å Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨")
    print("  ‚úÖ 4Îã®Í≥Ñ Ìè¥Î∞± Î©îÏª§ÎãàÏ¶ò")
    print("  ‚úÖ Ïã§ÏãúÍ∞Ñ AI ÏßÑÌñâÎ•† Ï∂îÏ†Å")
    print("  ‚úÖ 8Îã®Í≥Ñ AI ÌååÏù¥ÌîÑÎùºÏù∏")
    print("  ‚úÖ ÏÑ∏ÏÖò Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ Ïû¨ÏÇ¨Ïö©")
    print("  ‚úÖ Coroutine Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞")
    print("="*100)
    print("üöÄ ÌîÑÎ°†Ìä∏ÏóîÎìú Ïó∞Îèô:")
    print("  ‚úÖ Ïù¥ÎØ∏ÏßÄ Ïû¨ÏóÖÎ°úÎìú Î¨∏Ï†ú ÏôÑÏ†Ñ Ìï¥Í≤∞")
    print("  ‚úÖ ÏÑ∏ÏÖò Í∏∞Î∞ò Ï≤òÎ¶¨ ÏôÑÏÑ±")
    print("  ‚úÖ WebSocket Ïã§ÏãúÍ∞Ñ ÏßÑÌñâÎ•†")
    print("  ‚úÖ FormData API ÏôÑÏ†Ñ ÏßÄÏõê")
    print("  ‚úÖ 8Îã®Í≥Ñ Í∞úÎ≥Ñ Ï≤òÎ¶¨ ÏßÄÏõê")
    print("  ‚úÖ ÏôÑÏ†ÑÌïú ÌååÏù¥ÌîÑÎùºÏù∏ Ï≤òÎ¶¨ ÏßÄÏõê")
    print("  ‚úÖ RuntimeWarning Ìï¥Í≤∞Îê®")
    print("="*100)
    print("üîó Í∞úÎ∞ú ÎßÅÌÅ¨:")
    print("  üìñ API Î¨∏ÏÑú: http://localhost:8000/docs")
    print("  üìã AI ÏÉÅÌÉú: http://localhost:8000/api/ai/status")
    print("  üè• Ìó¨Ïä§Ï≤¥ÌÅ¨: http://localhost:8000/health")
    print("  üìä ÏãúÏä§ÌÖú Ï†ïÎ≥¥: http://localhost:8000/api/system/info")
    print("="*100)
    print("üîß Ìå®Ïπò Ï†ÅÏö© ÏôÑÎ£å!")
    print("  ‚úÖ RuntimeWarning: coroutine was never awaited ‚Üí Ìï¥Í≤∞Îê®")
    print("  ‚úÖ 'coroutine' object is not callable ‚Üí Ìï¥Í≤∞Îê®")
    print("  ‚úÖ BaseStepMixin ÏõåÎ∞çÏóÖ Î©îÏÑúÎìú ‚Üí ÏïàÏ†Ñ Î≤ÑÏ†ÑÏúºÎ°ú ÍµêÏ≤¥")
    print("  ‚úÖ async Î©îÏÑúÎìúÎì§ ‚Üí ÎèôÍ∏∞ Î≤ÑÏ†ÑÏúºÎ°ú Î≥ÄÌôò")
    print("="*100)
    
    # ÏÑúÎ≤Ñ Ïã§Ìñâ
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,  # AI Î™®Îç∏ ÏïàÏ†ïÏÑ±ÏùÑ ÏúÑÌï¥ reload ÎπÑÌôúÏÑ±Ìôî
        log_level="info",
        access_log=True,
        workers=1  # AI Î™®Îç∏ Î©îÎ™®Î¶¨ Í≥µÏú†Î•º ÏúÑÌï¥ Îã®Ïùº ÏõåÏª§
    )