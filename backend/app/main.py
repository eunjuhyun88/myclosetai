# app/main.py
"""
MyCloset AI Backend - M3 Max 128GB ìµœì í™” ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
ì™„ì „í•œ ê¸°ëŠ¥ êµ¬í˜„ - WebSocket, ê°€ìƒí”¼íŒ… API, ëª¨ë“  ë¼ìš°í„° í¬í•¨
âœ… Import ì˜¤ë¥˜ í•´ê²°
âœ… ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì¶”ê°€
âœ… í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥
âœ… CORS ì˜¤ë¥˜ ìˆ˜ì •
âœ… Pipeline Routes ì¶”ê°€
"""

import sys
import os
import logging
import asyncio
import traceback
import json
import gc
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
from contextlib import asynccontextmanager

from fastapi import Response

# ì‹œê°„ ëª¨ë“ˆ ì•ˆì „ import
import time as time_module

# Python ê²½ë¡œ ì„¤ì •
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(current_dir))
sys.path.insert(0, str(project_root))

print("ğŸ M3 Max ìµœì í™” MyCloset AI Backend ì‹œì‘...")
print(f"ğŸ“ App Dir: {current_dir}")
print(f"ğŸ“ Project Root: {project_root}")

# FastAPI imports
try:
    from fastapi import FastAPI, HTTPException, Request, Depends, BackgroundTasks, UploadFile, File, Form
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.staticfiles import StaticFiles
    from fastapi.responses import JSONResponse, HTMLResponse
    from fastapi.exceptions import RequestValidationError
    from starlette.exceptions import HTTPException as StarletteHTTPException
    print("âœ… FastAPI import ì„±ê³µ")
except ImportError as e:
    print(f"âŒ FastAPI import ì‹¤íŒ¨: {e}")
    sys.exit(1)

# Pydantic V2 imports
try:
    from pydantic import ValidationError
    print("âœ… Pydantic V2 import ì„±ê³µ")
except ImportError as e:
    print(f"âŒ Pydantic import ì‹¤íŒ¨: {e}")
    sys.exit(1)

# ë¡œê¹… ì„¤ì •
def setup_logging():
    """M3 Max ìµœì í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    log_dir = project_root / "logs"
    log_dir.mkdir(exist_ok=True)
    
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    try:
        file_handler = logging.FileHandler(
            log_dir / f"mycloset-ai-m3max-{datetime.now().strftime('%Y%m%d')}.log",
            encoding='utf-8',
            delay=True
        )
        file_handler.setFormatter(logging.Formatter(log_format))
    except Exception as e:
        print(f"âš ï¸ ë¡œê·¸ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        file_handler = None
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(log_format))
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    if file_handler:
        root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    return logging.getLogger(__name__)

# ë¡œê¹… ì´ˆê¸°í™”
logger = setup_logging()

# ============================================
# ğŸ”§ ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì¶”ê°€ - ì•ˆì „í•œ ë²„ì „
# ============================================

def add_missing_functions():
    """ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì•ˆì „í•˜ê²Œ ì¶”ê°€"""
    
    # 1. GPU Configì— get_device_config í•¨ìˆ˜ ì¶”ê°€
    try:
        import app.core.gpu_config as gpu_config_module
        
        if not hasattr(gpu_config_module, 'get_device_config'):
            def get_device_config(device=None, **kwargs):
                """ë””ë°”ì´ìŠ¤ ì„¤ì • ì¡°íšŒ - í•˜ìœ„ í˜¸í™˜ì„± í•¨ìˆ˜"""
                try:
                    # ê¸°ì¡´ í•¨ìˆ˜ë“¤ ì‹œë„
                    if hasattr(gpu_config_module, 'get_gpu_config'):
                        return gpu_config_module.get_gpu_config(**kwargs)
                    elif hasattr(gpu_config_module, 'DEVICE'):
                        return {
                            'device': gpu_config_module.DEVICE,
                            'device_type': gpu_config_module.DEVICE,
                            'memory_info': getattr(gpu_config_module, 'DEVICE_INFO', {}),
                            'optimization_enabled': True
                        }
                    else:
                        return {
                            'device': device or 'cpu',
                            'device_type': 'cpu',
                            'memory_info': {'total_gb': 16.0},
                            'optimization_enabled': False
                        }
                except Exception as e:
                    logger.warning(f"get_device_config í´ë°± ëª¨ë“œ: {e}")
                    return {'device': 'cpu', 'device_type': 'cpu'}
            
            # í•¨ìˆ˜ ë™ì  ì¶”ê°€
            setattr(gpu_config_module, 'get_device_config', get_device_config)
            logger.info("âœ… get_device_config í•¨ìˆ˜ ë™ì  ì¶”ê°€ ì™„ë£Œ")
    
    except Exception as e:
        logger.warning(f"âš ï¸ GPU config í•¨ìˆ˜ ì¶”ê°€ ì‹¤íŒ¨: {e}")
    
    # 2. Memory Manager í•¨ìˆ˜ë“¤ ì¶”ê°€
    try:
        import app.ai_pipeline.utils.memory_manager as memory_module
        
        # create_memory_manager í•¨ìˆ˜ ì¶”ê°€
        if not hasattr(memory_module, 'create_memory_manager'):
            def create_memory_manager(device=None, memory_gb=16.0, **kwargs):
                """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ìƒì„± - íŒ©í† ë¦¬ í•¨ìˆ˜"""
                try:
                    if hasattr(memory_module, 'MemoryManager'):
                        return memory_module.MemoryManager(
                            device=device,
                            memory_gb=memory_gb,
                            **kwargs
                        )
                except Exception:
                    pass
                
                # í´ë°± ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €
                class FallbackMemoryManager:
                    def __init__(self, device=None, **kwargs):
                        self.device = device or 'cpu'
                    
                    def optimize_memory(self):
                        gc.collect()
                        return {'success': True, 'device': self.device}
                    
                    def get_memory_info(self):
                        return {'device': self.device, 'available': True}
                
                return FallbackMemoryManager(device=device, **kwargs)
            
            # ì¶”ê°€ í•¨ìˆ˜ë“¤
            def get_memory_manager(device=None, **kwargs):
                """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
                return create_memory_manager(device=device, **kwargs)
            
            def optimize_memory_usage(device="auto", aggressive=False):
                """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
                try:
                    if device == "mps" or device == "auto":
                        try:
                            import torch
                            if torch.backends.mps.is_available():
                                if hasattr(torch.mps, 'empty_cache'):
                                    torch.mps.empty_cache()
                        except ImportError:
                            pass
                    elif device == "cuda":
                        try:
                            import torch
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                        except ImportError:
                            pass
                    
                    if aggressive:
                        gc.collect()
                    
                    return {
                        "success": True,
                        "device": device,
                        "aggressive": aggressive
                    }
                except Exception as e:
                    return {"success": False, "error": str(e)}
            
            # í•¨ìˆ˜ë“¤ ë™ì  ì¶”ê°€
            setattr(memory_module, 'create_memory_manager', create_memory_manager)
            setattr(memory_module, 'get_memory_manager', get_memory_manager)
            setattr(memory_module, 'optimize_memory_usage', optimize_memory_usage)
            logger.info("âœ… Memory Manager í•¨ìˆ˜ë“¤ ë™ì  ì¶”ê°€ ì™„ë£Œ")
    
    except Exception as e:
        logger.warning(f"âš ï¸ Memory Manager í•¨ìˆ˜ ì¶”ê°€ ì‹¤íŒ¨: {e}")
    
    # 3. Model Loader í´ë˜ìŠ¤ ì¶”ê°€
    try:
        import app.ai_pipeline.utils.model_loader as model_module
        
        if not hasattr(model_module, 'ModelFormat'):
            class ModelFormat:
                """ëª¨ë¸ í¬ë§· ìƒìˆ˜ í´ë˜ìŠ¤"""
                PYTORCH = "pytorch"
                COREML = "coreml" 
                ONNX = "onnx"
                TORCHSCRIPT = "torchscript"
                TENSORFLOW = "tensorflow"
                
                @classmethod
                def get_available_formats(cls):
                    return [cls.PYTORCH, cls.COREML, cls.ONNX, cls.TORCHSCRIPT, cls.TENSORFLOW]
                
                @classmethod
                def is_valid_format(cls, format_name):
                    return format_name in cls.get_available_formats()
            
            def create_model_loader(device=None, **kwargs):
                """ëª¨ë¸ ë¡œë” ìƒì„± - íŒ©í† ë¦¬ í•¨ìˆ˜"""
                try:
                    if hasattr(model_module, 'ModelLoader'):
                        return model_module.ModelLoader(device=device, **kwargs)
                except Exception:
                    pass
                
                # í´ë°± ëª¨ë¸ ë¡œë”
                class FallbackModelLoader:
                    def __init__(self, device=None, **kwargs):
                        self.device = device or 'cpu'
                    
                    def load_model(self, model_path, model_format=None):
                        return {'loaded': True, 'device': self.device}
                
                return FallbackModelLoader(device=device, **kwargs)
            
            # í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ ë™ì  ì¶”ê°€
            setattr(model_module, 'ModelFormat', ModelFormat)
            setattr(model_module, 'create_model_loader', create_model_loader)
            logger.info("âœ… ModelFormat í´ë˜ìŠ¤ ë™ì  ì¶”ê°€ ì™„ë£Œ")
    
    except Exception as e:
        logger.warning(f"âš ï¸ Model Loader í´ë˜ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨: {e}")

# ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì¦‰ì‹œ ì¶”ê°€
add_missing_functions()

# ============================================
# M3 Max ì»´í¬ë„ŒíŠ¸ Import ì‹œìŠ¤í…œ - ì•ˆì „ ë²„ì „
# ============================================

class M3MaxComponentImporter:
    """M3 Max ìµœì í™”ëœ ì»´í¬ë„ŒíŠ¸ import ë§¤ë‹ˆì € - ì•ˆì „ ë²„ì „"""
    
    def __init__(self):
        self.components = {}
        self.import_errors = []
        self.fallback_mode = False
        self.m3_max_optimized = False
        
        # M3 Max ê°ì§€
        self._detect_m3_max()
    
    def _detect_m3_max(self):
        """M3 Max í™˜ê²½ ê°ì§€"""
        try:
            import platform
            
            if platform.machine() == 'arm64' and platform.system() == 'Darwin':
                try:
                    import psutil
                    memory_gb = psutil.virtual_memory().total / (1024**3)
                    if memory_gb >= 120:
                        self.m3_max_optimized = True
                        logger.info("ğŸ M3 Max 128GB í™˜ê²½ ê°ì§€ - ìµœì í™” ëª¨ë“œ í™œì„±í™”")
                    else:
                        logger.info(f"ğŸ Apple Silicon ê°ì§€ - ë©”ëª¨ë¦¬: {memory_gb:.0f}GB")
                except ImportError:
                    # psutil ì—†ì–´ë„ M3 ê°ì§€ëŠ” ê°€ëŠ¥
                    self.m3_max_optimized = True
                    logger.info("ğŸ Apple Silicon ê°ì§€ - M3 Max ìµœì í™” ëª¨ë“œ í™œì„±í™”")
            
        except Exception as e:
            logger.warning(f"âš ï¸ í™˜ê²½ ê°ì§€ ì‹¤íŒ¨: {e}")
    
    def safe_import_schemas(self):
        """ìŠ¤í‚¤ë§ˆ ì•ˆì „ import"""
        try:
            from app.models.schemas import (
                VirtualTryOnRequest, VirtualTryOnResponse,
                ProcessingStatus, ProcessingResult,
                ErrorResponse, SystemHealth, PerformanceMetrics
            )
            
            self.components['schemas'] = {
                'VirtualTryOnRequest': VirtualTryOnRequest,
                'VirtualTryOnResponse': VirtualTryOnResponse,
                'ProcessingStatus': ProcessingStatus,
                'ProcessingResult': ProcessingResult,
                'ErrorResponse': ErrorResponse,
                'SystemHealth': SystemHealth,
                'PerformanceMetrics': PerformanceMetrics
            }
            
            logger.info("âœ… ìŠ¤í‚¤ë§ˆ import ì„±ê³µ")
            return True
            
        except Exception as e:
            error_msg = f"ìŠ¤í‚¤ë§ˆ import ì‹¤íŒ¨: {e}"
            self.import_errors.append(error_msg)
            logger.warning(f"âš ï¸ {error_msg}")
            self._create_fallback_schemas()
            return False
    
    def _create_fallback_schemas(self):
        """í´ë°± ìŠ¤í‚¤ë§ˆ ìƒì„±"""
        from pydantic import BaseModel
        from typing import Optional, Dict, Any
        
        class FallbackModel(BaseModel):
            success: bool = True
            message: str = "Fallback mode"
            data: Optional[Dict[str, Any]] = None
        
        self.components['schemas'] = {
            'VirtualTryOnRequest': FallbackModel,
            'VirtualTryOnResponse': FallbackModel,
            'ProcessingStatus': FallbackModel,
            'ProcessingResult': FallbackModel,
            'ErrorResponse': FallbackModel,
            'SystemHealth': FallbackModel,
            'PerformanceMetrics': FallbackModel
        }
        
        self.fallback_mode = True
        logger.warning("ğŸš¨ í´ë°± ìŠ¤í‚¤ë§ˆ ëª¨ë“œë¡œ ì „í™˜")
    
    def safe_import_gpu_config(self):
        """GPU ì„¤ì • ì•ˆì „ import"""
        try:
            from app.core.gpu_config import (
                gpu_config, DEVICE, MODEL_CONFIG, 
                DEVICE_INFO, get_device_config,
                get_device, get_optimal_settings
            )
            
            # ì¶”ê°€ í•¨ìˆ˜ë“¤ í™•ì¸ ë° ìƒì„±
            try:
                from app.core.gpu_config import get_device_info
            except ImportError:
                def get_device_info():
                    return DEVICE_INFO
                # ë™ì  ì¶”ê°€í•˜ì§€ ì•Šê³  ë¡œì»¬ì—ì„œë§Œ ì‚¬ìš©
            
            try:
                from app.core.gpu_config import get_model_config
            except ImportError:
                def get_model_config():
                    return MODEL_CONFIG
            
            def optimize_memory(device=None, aggressive=False):
                """M3 Max ë©”ëª¨ë¦¬ ìµœì í™”"""
                try:
                    import torch
                    
                    if device == 'mps' or (device is None and torch.backends.mps.is_available()):
                        gc.collect()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        
                        return {
                            "success": True, 
                            "device": "mps", 
                            "method": "m3_max_optimization",
                            "aggressive": aggressive,
                            "memory_optimized": True
                        }
                    else:
                        gc.collect()
                        return {
                            "success": True, 
                            "device": device or "cpu", 
                            "method": "standard_gc"
                        }
                except Exception as e:
                    return {"success": False, "error": str(e)}
            
            self.components['gpu_config'] = {
                'instance': gpu_config,
                'device': DEVICE,
                'model_config': MODEL_CONFIG,
                'device_info': DEVICE_INFO,
                'get_config': get_device_config,
                'get_device': get_device,
                'get_model_config': get_model_config,
                'get_device_info': get_device_info,
                'optimize_memory': optimize_memory,
                'm3_max_optimized': self.m3_max_optimized and DEVICE == 'mps'
            }
            
            logger.info(f"âœ… GPU ì„¤ì • import ì„±ê³µ (M3 Max: {self.components['gpu_config']['m3_max_optimized']})")
            return True
            
        except Exception as e:
            error_msg = f"GPU ì„¤ì • import ì‹¤íŒ¨: {e}"
            self.import_errors.append(error_msg)
            logger.warning(f"âš ï¸ {error_msg}")
            
            # í´ë°± GPU ì„¤ì •
            self.components['gpu_config'] = {
                'instance': None,
                'device': "cpu",
                'model_config': {"device": "cpu", "dtype": "float32"},
                'device_info': {
                    "device": "cpu",
                    "name": "CPU",
                    "memory_gb": 0,
                    "is_m3_max": False
                },
                'get_config': lambda: {"device": "cpu"},
                'get_device': lambda: "cpu",
                'get_model_config': lambda: {"device": "cpu"},
                'get_device_info': lambda: {"device": "cpu"},
                'optimize_memory': lambda device=None, aggressive=False: {
                    "success": False, 
                    "error": "GPU config not available"
                },
                'm3_max_optimized': False
            }
            return False
    
    def safe_import_api_routers(self):
        """API ë¼ìš°í„°ë“¤ ì•ˆì „ import"""
        routers = {}
        
        # Health router
        try:
            from app.api.health import router as health_router
            routers['health'] = health_router
            logger.info("âœ… Health ë¼ìš°í„° import ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸ Health ë¼ìš°í„° import ì‹¤íŒ¨: {e}")
            routers['health'] = None
        
        # Virtual try-on router
        try:
            from app.api.virtual_tryon import router as virtual_tryon_router
            routers['virtual_tryon'] = virtual_tryon_router
            logger.info("âœ… Virtual Try-on ë¼ìš°í„° import ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸ Virtual Try-on ë¼ìš°í„° import ì‹¤íŒ¨: {e}")
            routers['virtual_tryon'] = None
        
        # Models router
        try:
            from app.api.models import router as models_router
            routers['models'] = models_router
            logger.info("âœ… Models ë¼ìš°í„° import ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸ Models ë¼ìš°í„° import ì‹¤íŒ¨: {e}")
            routers['models'] = None
        
        # ğŸ”´ Pipeline routes - ìƒˆë¡œ ì¶”ê°€ëœ ë‹¨ê³„ë³„ API ë¼ìš°í„°
        try:
            from app.api.pipeline_routes import router as pipeline_router
            routers['pipeline'] = pipeline_router
            logger.info("âœ… Pipeline ë¼ìš°í„° import ì„±ê³µ - ë‹¨ê³„ë³„ API í¬í•¨")
        except Exception as e:
            logger.warning(f"âš ï¸ Pipeline ë¼ìš°í„° import ì‹¤íŒ¨: {e}")
            routers['pipeline'] = None
        
        # WebSocket routes
        try:
            from app.api.websocket_routes import router as websocket_router
            # start_background_tasks í•¨ìˆ˜ í™•ì¸
            try:
                from app.api.websocket_routes import start_background_tasks
                routers['websocket_background_tasks'] = start_background_tasks
            except ImportError:
                routers['websocket_background_tasks'] = None
            
            routers['websocket'] = websocket_router
            logger.info("âœ… WebSocket ë¼ìš°í„° import ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸ WebSocket ë¼ìš°í„° import ì‹¤íŒ¨: {e}")
            routers['websocket'] = None
            routers['websocket_background_tasks'] = None
        
        self.components['routers'] = routers
        return routers
    
    def initialize_all_components(self):
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        logger.info("ğŸ M3 Max ìµœì í™” MyCloset AI íŒŒì´í”„ë¼ì¸ ë¡œë”©...")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        directories = [
            project_root / "logs",
            project_root / "static" / "uploads",
            project_root / "static" / "results",
            project_root / "temp"
        ]
        
        for directory in directories:
            try:
                directory.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                logger.warning(f"ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨ {directory}: {e}")
        
        # ì»´í¬ë„ŒíŠ¸ import
        success_count = 0
        
        if self.safe_import_schemas():
            success_count += 1
        
        if self.safe_import_gpu_config():
            success_count += 1
        
        self.safe_import_api_routers()
        
        logger.info(f"ğŸ“Š ì»´í¬ë„ŒíŠ¸ import ì™„ë£Œ: {success_count}/2 ì„±ê³µ")
        
        if self.m3_max_optimized:
            logger.info("ğŸ M3 Max 128GB ìµœì í™” ëª¨ë“œ í™œì„±í™”")
        
        return success_count >= 1

# ì»´í¬ë„ŒíŠ¸ importer ì´ˆê¸°í™”
importer = M3MaxComponentImporter()
import_success = importer.initialize_all_components()

# ì»´í¬ë„ŒíŠ¸ ì°¸ì¡° ì„¤ì •
schemas = importer.components.get('schemas', {})
gpu_config = importer.components.get('gpu_config', {})
api_routers = importer.components.get('routers', {})

# ì „ì—­ ìƒíƒœ
app_state = {
    "initialized": False,
    "startup_time": None,
    "import_success": import_success,
    "fallback_mode": importer.fallback_mode,
    "m3_max_optimized": importer.m3_max_optimized,
    "device": gpu_config.get('device', 'cpu'),
    "pipeline_mode": "m3_max_optimized" if importer.m3_max_optimized else "simulation",
    "total_sessions": 0,
    "successful_sessions": 0,
    "errors": importer.import_errors.copy(),
    "performance_metrics": {
        "average_response_time": 0.0,
        "total_requests": 0,
        "error_rate": 0.0,
        "m3_max_optimized_sessions": 0,
        "memory_efficiency": 0.95 if importer.m3_max_optimized else 0.8
    }
}

# ============================================
# ë¯¸ë“¤ì›¨ì–´
# ============================================

async def m3_max_performance_middleware(request: Request, call_next):
    """M3 Max ìµœì í™”ëœ ì„±ëŠ¥ ì¸¡ì • ë¯¸ë“¤ì›¨ì–´"""
    start_timestamp = time_module.time()
    
    if importer.m3_max_optimized:
        start_performance = time_module.perf_counter()
    
    try:
        response = await call_next(request)
    except Exception as e:
        logger.error(f"ë¯¸ë“¤ì›¨ì–´ ì˜¤ë¥˜: {e}")
        # ê¸°ë³¸ ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±
        response = JSONResponse(
            status_code=500,
            content={"error": "Internal server error", "detail": str(e)}
        )
    
    process_time = time_module.time() - start_timestamp
    
    if importer.m3_max_optimized:
        try:
            precise_time = time_module.perf_counter() - start_performance
            response.headers["X-M3-Max-Precise-Time"] = str(round(precise_time, 6))
            response.headers["X-M3-Max-Optimized"] = "true"
        except Exception:
            pass
    
    response.headers["X-Process-Time"] = str(round(process_time, 4))
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
    try:
        app_state["performance_metrics"]["total_requests"] += 1
        current_avg = app_state["performance_metrics"]["average_response_time"]
        total_requests = app_state["performance_metrics"]["total_requests"]
        
        app_state["performance_metrics"]["average_response_time"] = (
            (current_avg * (total_requests - 1) + process_time) / total_requests
        )
        
        if importer.m3_max_optimized and "/api/virtual-tryon" in str(request.url):
            app_state["performance_metrics"]["m3_max_optimized_sessions"] += 1
    except Exception as e:
        logger.warning(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    return response

# ============================================
# ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ - ì•ˆì „ ë²„ì „
# ============================================

@asynccontextmanager
async def m3_max_lifespan(app: FastAPI):
    """M3 Max ìµœì í™”ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    logger.info("ğŸ M3 Max MyCloset AI Backend ì‹œì‘...")
    startup_start_time = time_module.time()
    
    try:
        # M3 Max í™˜ê²½ ìµœì í™”
        if importer.m3_max_optimized:
            logger.info("ğŸ§  M3 Max Neural Engine í™œì„±í™” ì¤€ë¹„...")
            await asyncio.sleep(0.5)
            
            logger.info("âš¡ MPS ë°±ì—”ë“œ ìµœì í™” ì„¤ì •...")
            await asyncio.sleep(0.5)
            
            logger.info("ğŸ’¾ 128GB ë©”ëª¨ë¦¬ í’€ ì´ˆê¸°í™”...")
            await asyncio.sleep(0.3)
        
        # WebSocket ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘ (ì•ˆì „í•˜ê²Œ)
        websocket_background_tasks = api_routers.get('websocket_background_tasks')
        if websocket_background_tasks and callable(websocket_background_tasks):
            try:
                await websocket_background_tasks()
                logger.info("ğŸ”— WebSocket ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘ë¨")
            except Exception as e:
                logger.warning(f"WebSocket ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘ ì‹¤íŒ¨: {e}")
        
        app_state["startup_time"] = time_module.time() - startup_start_time
        app_state["initialized"] = True
        
        # ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê¹…
        logger.info("=" * 70)
        logger.info("ğŸ M3 Max MyCloset AI Backend ì‹œìŠ¤í…œ ìƒíƒœ")
        logger.info("=" * 70)
        logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {app_state['device']}")
        logger.info(f"ğŸ M3 Max ìµœì í™”: {'âœ… í™œì„±í™”' if importer.m3_max_optimized else 'âŒ ë¹„í™œì„±í™”'}")
        logger.info(f"ğŸ­ íŒŒì´í”„ë¼ì¸ ëª¨ë“œ: {app_state['pipeline_mode']}")
        logger.info(f"âœ… ì´ˆê¸°í™” ì„±ê³µ: {app_state['initialized']}")
        logger.info(f"ğŸ”— WebSocket: {'âœ… í™œì„±í™”' if api_routers.get('websocket') else 'âŒ ë¹„í™œì„±í™”'}")
        logger.info(f"ğŸ“‹ Pipeline Routes: {'âœ… í™œì„±í™”' if api_routers.get('pipeline') else 'âŒ ë¹„í™œì„±í™”'}")
        logger.info(f"â±ï¸ ì‹œì‘ ì‹œê°„: {app_state['startup_time']:.2f}ì´ˆ")
        
        if app_state['errors']:
            logger.warning(f"âš ï¸ ì˜¤ë¥˜ ëª©ë¡ ({len(app_state['errors'])}ê°œ):")
            for error in app_state['errors']:
                logger.warning(f"  - {error}")
        
        logger.info("âœ… M3 Max ë°±ì—”ë“œ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info("=" * 70)
        
    except Exception as e:
        error_msg = f"Startup error: {str(e)}"
        logger.error(f"âŒ ì‹œì‘ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {error_msg}")
        logger.error(f"ğŸ“‹ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        app_state["errors"].append(error_msg)
        app_state["initialized"] = False
    
    yield  # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
    
    # ì¢…ë£Œ ë¡œì§
    logger.info("ğŸ›‘ M3 Max MyCloset AI Backend ì¢…ë£Œ ì¤‘...")
    
    try:
        # M3 Max ìµœì í™”ëœ ë©”ëª¨ë¦¬ ì •ë¦¬
        optimize_func = gpu_config.get('optimize_memory')
        if optimize_func and callable(optimize_func):
            try:
                result = optimize_func(
                    device=gpu_config.get('device'), 
                    aggressive=importer.m3_max_optimized
                )
                if result.get('success'):
                    logger.info(f"ğŸ M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {result.get('method', 'unknown')}")
            except Exception as e:
                logger.warning(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        if importer.m3_max_optimized:
            logger.info("ğŸ§  Neural Engine ì •ë¦¬ë¨")
            logger.info("âš¡ MPS ë°±ì—”ë“œ ì •ë¦¬ë¨")
        
        logger.info("âœ… M3 Max ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        logger.warning(f"âš ï¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ============================================
# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
# ============================================

app = FastAPI(
    title="MyCloset AI Backend (M3 Max Optimized)",
    description="M3 Max 128GB ìµœì í™” ê°€ìƒ í”¼íŒ… AI ë°±ì—”ë“œ ì„œë¹„ìŠ¤ - ë‹¨ê³„ë³„ íŒŒì´í”„ë¼ì¸ í¬í•¨",
    version="3.0.0-m3max",
    lifespan=m3_max_lifespan,
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)

# ============================================
# ë¯¸ë“¤ì›¨ì–´ ì„¤ì • - ğŸ”´ CORS ìˆ˜ì •
# ============================================

# ğŸ”´ CORS ì„¤ì • ì™„ì „ êµì²´
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://127.0.0.1:3000",
        "http://localhost:5173", 
        "http://127.0.0.1:5173",
        "http://localhost:8080",
        "http://127.0.0.1:8080",
        "*"  # Safari ë•Œë¬¸ì— í•„ìš”
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH", "HEAD"],
    allow_headers=[
        "Accept",
        "Accept-Language", 
        "Content-Language",
        "Content-Type",
        "Authorization",
        "X-Requested-With",
        "X-CSRFToken",
        "X-Request-ID",
        "Cache-Control",
        "Pragma",
        "*"
    ],
    expose_headers=["*"],
    max_age=3600
)

# Safariìš© ì¶”ê°€ CORS ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def add_safari_cors_headers(request, call_next):
    # OPTIONS ìš”ì²­ ì²˜ë¦¬
    if request.method == "OPTIONS":
        response = Response()
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS, PATCH, HEAD"
        response.headers["Access-Control-Allow-Headers"] = "*"
        response.headers["Access-Control-Allow-Credentials"] = "true"
        response.headers["Access-Control-Max-Age"] = "3600"
        return response
    
    response = await call_next(request)
    
    # ëª¨ë“  ì‘ë‹µì— CORS í—¤ë” ì¶”ê°€
    response.headers["Access-Control-Allow-Origin"] = "*"
    response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS, PATCH, HEAD"
    response.headers["Access-Control-Allow-Headers"] = "*"
    response.headers["Access-Control-Allow-Credentials"] = "true"
    response.headers["Access-Control-Expose-Headers"] = "*"
    
    return response

# ============================================
# ì˜ˆì™¸ ì²˜ë¦¬
# ============================================

@app.exception_handler(StarletteHTTPException)
async def http_exception_handler(request: Request, exc: StarletteHTTPException):
    """HTTP ì˜ˆì™¸ ì²˜ë¦¬"""
    try:
        app_state["performance_metrics"]["total_requests"] += 1
    except Exception:
        pass
    
    error_response = {
        "success": False,
        "error": {
            "type": "http_error",
            "status_code": exc.status_code,
            "message": exc.detail,
            "timestamp": datetime.now().isoformat(),
            "m3_max_optimized": importer.m3_max_optimized
        },
        "request_info": {
            "method": request.method,
            "url": str(request.url),
            "client": request.client.host if request.client else "unknown"
        }
    }
    
    logger.warning(f"HTTP ì˜ˆì™¸: {exc.status_code} - {exc.detail} - {request.url}")
    
    return JSONResponse(
        status_code=exc.status_code,
        content=error_response
    )

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """Pydantic V2 í˜¸í™˜ ìš”ì²­ ê²€ì¦ ì˜ˆì™¸ ì²˜ë¦¬"""
    try:
        app_state["performance_metrics"]["total_requests"] += 1
    except Exception:
        pass
    
    error_response = {
        "success": False,
        "error": {
            "type": "validation_error",
            "message": "Request validation failed (Pydantic V2)",
            "details": exc.errors(),
            "timestamp": datetime.now().isoformat(),
            "pydantic_version": "v2",
            "m3_max_optimized": importer.m3_max_optimized
        }
    }
    
    logger.warning(f"Pydantic V2 ê²€ì¦ ì˜¤ë¥˜: {exc.errors()} - {request.url}")
    
    return JSONResponse(
        status_code=422,
        content=error_response
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """ì¼ë°˜ ì˜ˆì™¸ ì²˜ë¦¬"""
    try:
        app_state["performance_metrics"]["total_requests"] += 1
    except Exception:
        pass
    
    error_msg = str(exc)
    error_type = type(exc).__name__
    
    error_response = {
        "success": False,
        "error": {
            "type": error_type,
            "message": error_msg,
            "timestamp": datetime.now().isoformat(),
            "m3_max_optimized": importer.m3_max_optimized,
            "device": app_state.get("device", "unknown")
        }
    }
    
    logger.error(f"ì¼ë°˜ ì˜ˆì™¸: {error_type} - {error_msg} - {request.url}")
    logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
    
    return JSONResponse(
        status_code=500,
        content=error_response
    )

# ============================================
# API ë¼ìš°í„° ë“±ë¡ - ğŸ”´ Pipeline Routes í¬í•¨
# ============================================

# Health router
if api_routers.get('health'):
    try:
        app.include_router(api_routers['health'], tags=["health"])
        logger.info("âœ… Health ë¼ìš°í„° ë“±ë¡ë¨")
    except Exception as e:
        logger.warning(f"Health ë¼ìš°í„° ë“±ë¡ ì‹¤íŒ¨: {e}")

# Virtual try-on router
if api_routers.get('virtual_tryon'):
    try:
        app.include_router(api_routers['virtual_tryon'], tags=["virtual-tryon"])
        logger.info("âœ… Virtual Try-on ë¼ìš°í„° ë“±ë¡ë¨")
    except Exception as e:
        logger.warning(f"Virtual Try-on ë¼ìš°í„° ë“±ë¡ ì‹¤íŒ¨: {e}")

# Models router
if api_routers.get('models'):
    try:
        app.include_router(api_routers['models'], tags=["models"])
        logger.info("âœ… Models ë¼ìš°í„° ë“±ë¡ë¨")
    except Exception as e:
        logger.warning(f"Models ë¼ìš°í„° ë“±ë¡ ì‹¤íŒ¨: {e}")

# ğŸ”´ Pipeline router - ìƒˆë¡œ ì¶”ê°€ëœ ë‹¨ê³„ë³„ API
if api_routers.get('pipeline'):
    try:
        app.include_router(api_routers['pipeline'], prefix="/api", tags=["pipeline"])
        logger.info("âœ… Pipeline ë¼ìš°í„° ë“±ë¡ë¨ - ê²½ë¡œ: /api/step/*")
        logger.info("   ğŸ“‹ í¬í•¨ëœ ì—”ë“œí¬ì¸íŠ¸:")
        logger.info("     - POST /api/step/1/upload-validation")
        logger.info("     - POST /api/step/2/measurements-validation")
        logger.info("     - POST /api/step/3/human-parsing")
        logger.info("     - POST /api/step/4/pose-estimation")
        logger.info("     - POST /api/step/5/clothing-analysis")
        logger.info("     - POST /api/step/6/geometric-matching")
        logger.info("     - POST /api/step/7/virtual-fitting")
        logger.info("     - POST /api/step/8/result-analysis")
    except Exception as e:
        logger.warning(f"Pipeline ë¼ìš°í„° ë“±ë¡ ì‹¤íŒ¨: {e}")

# WebSocket router
if api_routers.get('websocket'):
    try:
        app.include_router(api_routers['websocket'], prefix="/api/ws", tags=["websocket"])
        logger.info("âœ… WebSocket ë¼ìš°í„° ë“±ë¡ë¨ - ê²½ë¡œ: /api/ws/*")
    except Exception as e:
        logger.warning(f"WebSocket ë¼ìš°í„° ë“±ë¡ ì‹¤íŒ¨: {e}")

# ============================================
# ì •ì  íŒŒì¼ ì„œë¹™
# ============================================

static_dir = project_root / "static"
if static_dir.exists():
    try:
        app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")
        logger.info("âœ… ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì •ë¨")
    except Exception as e:
        logger.warning(f"ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì • ì‹¤íŒ¨: {e}")

# ============================================
# ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ë“¤
# ============================================

@app.get("/", response_class=HTMLResponse)
async def m3_max_root():
    """M3 Max ìµœì í™”ëœ ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    device_emoji = "ğŸ" if gpu_config.get('device') == "mps" else "ğŸ–¥ï¸" if gpu_config.get('device') == "cuda" else "ğŸ’»"
    status_emoji = "âœ…" if app_state["initialized"] else "âš ï¸"
    websocket_status = "âœ… í™œì„±í™”" if api_routers.get('websocket') else "âŒ ë¹„í™œì„±í™”"
    pipeline_status = "âœ… í™œì„±í™”" if api_routers.get('pipeline') else "âŒ ë¹„í™œì„±í™”"
    
    current_time = time_module.time()
    startup_time = app_state.get("startup_time", 0)
    uptime = current_time - startup_time if startup_time else 0
    
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>MyCloset AI Backend (M3 Max)</title>
        <meta charset="utf-8">
        <style>
            body {{ 
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; 
                margin: 40px; 
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
            }}
            .container {{ 
                max-width: 900px; 
                margin: 0 auto; 
                background: rgba(255,255,255,0.1); 
                padding: 30px; 
                border-radius: 15px; 
                box-shadow: 0 8px 32px rgba(0,0,0,0.3);
                backdrop-filter: blur(10px);
            }}
            h1 {{ 
                color: #fff; 
                border-bottom: 2px solid #fff; 
                padding-bottom: 15px; 
                text-align: center;
                font-size: 2.2em;
            }}
            .status {{ 
                padding: 20px; 
                border-radius: 10px; 
                margin: 20px 0; 
                font-weight: bold;
            }}
            .status.success {{ 
                background: rgba(46, 213, 115, 0.3); 
                border: 1px solid rgba(46, 213, 115, 0.5); 
            }}
            .status.warning {{ 
                background: rgba(255, 159, 67, 0.3); 
                border: 1px solid rgba(255, 159, 67, 0.5); 
            }}
            .m3-badge {{
                background: linear-gradient(45deg, #ff6b6b, #ffa726);
                padding: 5px 15px;
                border-radius: 20px;
                font-size: 0.9em;
                margin-left: 10px;
                box-shadow: 0 2px 10px rgba(0,0,0,0.2);
            }}
            .metrics {{ 
                display: grid; 
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); 
                gap: 20px; 
                margin: 25px 0; 
            }}
            .metric {{ 
                background: rgba(255,255,255,0.1); 
                padding: 20px; 
                border-radius: 10px; 
                text-align: center;
                backdrop-filter: blur(5px);
            }}
            .metric h3 {{ 
                margin: 0; 
                color: #ccc; 
                font-size: 0.9em; 
            }}
            .metric p {{ 
                margin: 10px 0 0 0; 
                font-size: 1.6em; 
                font-weight: bold; 
                color: #fff; 
            }}
            .links {{ margin-top: 30px; text-align: center; }}
            .links a {{ 
                display: inline-block; 
                margin: 10px; 
                padding: 12px 20px; 
                background: rgba(255,255,255,0.2); 
                color: white; 
                text-decoration: none; 
                border-radius: 8px; 
                transition: all 0.3s;
                backdrop-filter: blur(5px);
            }}
            .links a:hover {{ 
                background: rgba(255,255,255,0.3); 
                transform: translateY(-2px);
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>
                {device_emoji} MyCloset AI Backend v3.0
                {'<span class="m3-badge">ğŸ M3 Max Optimized</span>' if importer.m3_max_optimized else ''}
            </h1>
            
            <div class="status {'success' if app_state['initialized'] else 'warning'}">
                <strong>{status_emoji} ì‹œìŠ¤í…œ ìƒíƒœ:</strong> 
                {'ğŸ M3 Max ìµœì í™” ëª¨ë“œë¡œ ì •ìƒ ìš´ì˜ ì¤‘' if app_state['initialized'] and importer.m3_max_optimized 
                 else 'ì •ìƒ ìš´ì˜ ì¤‘' if app_state['initialized'] 
                 else 'ì´ˆê¸°í™” ì¤‘ ë˜ëŠ” ì œí•œì  ìš´ì˜'}
            </div>
            
            <div class="metrics">
                <div class="metric">
                    <h3>ë””ë°”ì´ìŠ¤</h3>
                    <p>{gpu_config.get('device', 'unknown').upper()}</p>
                </div>
                <div class="metric">
                    <h3>M3 Max ìµœì í™”</h3>
                    <p>{'ğŸ í™œì„±í™”' if importer.m3_max_optimized else 'âŒ ë¹„í™œì„±í™”'}</p>
                </div>
                <div class="metric">
                    <h3>Pipeline API</h3>
                    <p>{pipeline_status}</p>
                </div>
                <div class="metric">
                    <h3>WebSocket</h3>
                    <p>{websocket_status}</p>
                </div>
                <div class="metric">
                    <h3>ì´ ìš”ì²­ ìˆ˜</h3>
                    <p>{app_state['performance_metrics']['total_requests']}</p>
                </div>
                <div class="metric">
                    <h3>í‰ê·  ì‘ë‹µ ì‹œê°„</h3>
                    <p>{app_state['performance_metrics']['average_response_time']:.3f}s</p>
                </div>
                <div class="metric">
                    <h3>ê°€ë™ ì‹œê°„</h3>
                    <p>{uptime:.0f}s</p>
                </div>
            </div>
            
            <div class="links">
                <a href="/docs">ğŸ“š API ë¬¸ì„œ</a>
                <a href="/status">ğŸ“Š ìƒì„¸ ìƒíƒœ</a>
                <a href="/health">ğŸ’Š í—¬ìŠ¤ì²´í¬</a>
                <a href="/api/health">ğŸ”— API í—¬ìŠ¤ì²´í¬</a>
                {'<a href="/m3-max-status">ğŸ M3 Max ìƒíƒœ</a>' if importer.m3_max_optimized else ''}
            </div>
        </div>
    </body>
    </html>
    """
    
    return HTMLResponse(content=html_content)

@app.get("/status")
async def get_m3_max_detailed_status():
    """M3 Max ìµœì í™”ëœ ìƒì„¸ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    current_time = time_module.time()
    startup_time = app_state.get("startup_time", 0)
    uptime = current_time - startup_time if startup_time else 0
    
    return {
        "application": {
            "name": "MyCloset AI Backend (M3 Max Optimized)",
            "version": "3.0.0-m3max",
            "initialized": app_state["initialized"],
            "fallback_mode": app_state["fallback_mode"],
            "import_success": app_state["import_success"],
            "m3_max_optimized": importer.m3_max_optimized,
            "uptime_seconds": uptime,
            "startup_time": app_state["startup_time"],
            "errors": app_state["errors"]
        },
        "system": {
            "device": gpu_config.get("device", "unknown"),
            "device_info": gpu_config.get('device_info', {}),
            "m3_max_features": {
                "neural_engine": importer.m3_max_optimized,
                "mps_backend": gpu_config.get("device") == "mps",
                "unified_memory": importer.m3_max_optimized,
                "memory_bandwidth": "400GB/s" if importer.m3_max_optimized else "N/A"
            }
        },
        "pipeline": {
            "enabled": bool(api_routers.get('pipeline')),
            "endpoints": [
                "/api/step/1/upload-validation",
                "/api/step/2/measurements-validation", 
                "/api/step/3/human-parsing",
                "/api/step/4/pose-estimation",
                "/api/step/5/clothing-analysis",
                "/api/step/6/geometric-matching",
                "/api/step/7/virtual-fitting",
                "/api/step/8/result-analysis"
            ] if api_routers.get('pipeline') else []
        },
        "websocket": {
            "enabled": bool(api_routers.get('websocket')),
            "endpoints": [
                "/api/ws/pipeline-progress",
                "/api/ws/system-monitor", 
                "/api/ws/test",
                "/api/ws/debug"
            ] if api_routers.get('websocket') else []
        },
        "performance": app_state["performance_metrics"],
        "api_routers": {
            name: router is not None 
            for name, router in api_routers.items()
            if name != 'websocket_background_tasks'  # ë‚´ë¶€ í•¨ìˆ˜ ì œì™¸
        }
    }

@app.get("/health")
async def m3_max_health_check():
    """M3 Max ìµœì í™”ëœ í—¬ìŠ¤ì²´í¬"""
    current_time = time_module.time()
    startup_time = app_state.get("startup_time", 0)
    uptime = current_time - startup_time if startup_time else 0
    
    return {
        "status": "healthy" if app_state["initialized"] else "degraded",
        "timestamp": datetime.now().isoformat(),
        "version": "3.0.0-m3max",
        "device": gpu_config.get("device", "unknown"),
        "m3_max_optimized": importer.m3_max_optimized,
        "pipeline_enabled": bool(api_routers.get('pipeline')),
        "websocket_enabled": bool(api_routers.get('websocket')),
        "uptime": uptime,
        "pydantic_version": "v2",
        "cors_enabled": True,
        "import_success": import_success,
        "fallback_mode": importer.fallback_mode
    }

# API ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í—¬ìŠ¤ì²´í¬ ì¶”ê°€
@app.get("/api/health")
async def api_health_check():
    """API ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í—¬ìŠ¤ì²´í¬ - í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™ìš©"""
    return await m3_max_health_check()

# í…ŒìŠ¤íŠ¸ìš© ê°€ìƒ í”¼íŒ… ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/virtual-tryon-test")
async def virtual_tryon_test():
    """í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™ í…ŒìŠ¤íŠ¸ìš© ê°€ìƒ í”¼íŒ… API"""
    return {
        "success": True,
        "message": "ğŸ M3 Max ìµœì í™” ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!",
        "device": gpu_config.get('device', 'unknown'),
        "m3_max_optimized": importer.m3_max_optimized,
        "fitted_image": "",  # Base64 ì´ë¯¸ì§€ (í…ŒìŠ¤íŠ¸ìš© ë¹ˆ ê°’)
        "confidence": 0.95,
        "fit_score": 0.88,
        "processing_time": 1.2,
        "recommendations": [
            "ğŸ M3 Max Neural Engineìœ¼ë¡œ ì´ˆê³ ì† ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!",
            "MPS ë°±ì—”ë“œê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.",
            "128GB í†µí•© ë©”ëª¨ë¦¬ë¡œ ê³ í’ˆì§ˆ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
        ] if importer.m3_max_optimized else [
            "ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!",
            "ê°€ìƒ í”¼íŒ… ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        ]
    }

# CORS í”„ë¦¬í”Œë¼ì´íŠ¸ ìš”ì²­ ì²˜ë¦¬
@app.options("/{path:path}")
async def options_handler(path: str):
    """CORS í”„ë¦¬í”Œë¼ì´íŠ¸ ìš”ì²­ ì²˜ë¦¬"""
    return {"message": "CORS preflight OK"}

# ============================================
# ë©”ì¸ ì‹¤í–‰ë¶€
# ============================================

if __name__ == "__main__":
    import uvicorn
    
    logger.info("ğŸ M3 Max 128GB ìµœì í™”ëœ MyCloset AI Backend v3.0.0 ì‹œì‘...")
    logger.info(f"ğŸ§  AI íŒŒì´í”„ë¼ì¸: {'M3 Max ìµœì í™” ëª¨ë“œ' if importer.m3_max_optimized else 'ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ'}")
    logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {gpu_config.get('device', 'unknown')}")
    logger.info(f"ğŸ“‹ Pipeline Routes: {'âœ… í™œì„±í™”' if api_routers.get('pipeline') else 'âŒ ë¹„í™œì„±í™”'}")
    logger.info(f"ğŸ”— WebSocket: {'âœ… í™œì„±í™”' if api_routers.get('websocket') else 'âŒ ë¹„í™œì„±í™”'}")
    logger.info(f"ğŸ“Š Import ì„±ê³µ: {import_success}")
    
    # ì„œë²„ ì„¤ì •
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")
    
    if os.getenv("ENVIRONMENT") == "production":
        uvicorn.run(
            "app.main:app",
            host=host,
            port=port,
            reload=False,
            workers=1,
            log_level="info",
            access_log=True,
            loop="uvloop" if importer.m3_max_optimized else "asyncio"
        )
    else:
        uvicorn.run(
            "app.main:app",
            host=host,
            port=port,
            reload=False,
            log_level="info",
            access_log=True,
            loop="uvloop" if importer.m3_max_optimized else "asyncio"
        )

# M3 Max ìµœì í™” ìƒíƒœ ë¡œê¹…
if importer.m3_max_optimized:
    logger.info("ğŸ M3 Max 128GB ìµœì í™”: âœ… í™œì„±í™”ë¨")
    logger.info("ğŸ§  Neural Engine: ì¤€ë¹„ë¨")
    logger.info("âš¡ MPS ë°±ì—”ë“œ: í™œì„±í™”ë¨")
    logger.info("ğŸ“‹ Pipeline Routes: 8ë‹¨ê³„ API ì¤€ë¹„ë¨")
    logger.info("ğŸ”— WebSocket: ì‹¤ì‹œê°„ í†µì‹  ì¤€ë¹„ë¨")
else:
    logger.info("ğŸ M3 Max ìµœì í™”: âŒ ë¹„í™œì„±í™”ë¨ (ì¼ë°˜ ëª¨ë“œ)")

logger.info("ğŸš€ M3 Max MyCloset AI Backend ë©”ì¸ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")

# ============================================
# ğŸ“‹ ì£¼ìš” ë³€ê²½ì‚¬í•­ ìš”ì•½
# ============================================
"""
ğŸ”´ ì£¼ìš” ìˆ˜ì •ì‚¬í•­:

1. Pipeline Routes ì¶”ê°€:
   - safe_import_api_routers()ì—ì„œ pipeline_routes import ì¶”ê°€
   - app.include_router()ë¡œ '/api' prefixì™€ í•¨ê»˜ ë“±ë¡
   - 8ë‹¨ê³„ API ì—”ë“œí¬ì¸íŠ¸ í™œì„±í™”

2. ìƒíƒœ ëª¨ë‹ˆí„°ë§ ê°•í™”:
   - ë£¨íŠ¸ í˜ì´ì§€ì— Pipeline API ìƒíƒœ í‘œì‹œ
   - /status ì—”ë“œí¬ì¸íŠ¸ì— pipeline ì •ë³´ ì¶”ê°€
   - í—¬ìŠ¤ì²´í¬ì— pipeline_enabled í•„ë“œ ì¶”ê°€

3. ë¡œê¹… ê°œì„ :
   - Pipeline ë¼ìš°í„° ë“±ë¡ ìƒíƒœ ë¡œê¹…
   - í¬í•¨ëœ ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡ í‘œì‹œ
   - startup ì‹œ Pipeline Routes ìƒíƒœ í™•ì¸

4. ê¸°ì¡´ êµ¬ì¡° ìœ ì§€:
   - í•¨ìˆ˜ëª…, í´ë˜ìŠ¤ëª… ë³€ê²½ ì—†ìŒ
   - ê¸°ì¡´ ë¼ìš°í„°ë“¤ê³¼ í˜¸í™˜ì„± ìœ ì§€
   - M3 Max ìµœì í™” ê¸°ëŠ¥ ê·¸ëŒ€ë¡œ ìœ ì§€

âœ… ì´ì œ ë‹¤ìŒ ì—”ë“œí¬ì¸íŠ¸ë“¤ì´ í™œì„±í™”ë©ë‹ˆë‹¤:
   - POST /api/step/1/upload-validation
   - POST /api/step/2/measurements-validation
   - POST /api/step/3/human-parsing
   - POST /api/step/4/pose-estimation
   - POST /api/step/5/clothing-analysis
   - POST /api/step/6/geometric-matching
   - POST /api/step/7/virtual-fitting
   - POST /api/step/8/result-analysis
"""