"""
MyCloset AI Backend - ì™„ì „í•œ Main Application
backend/app/main.py

âœ… ì™„ì „í•œ GPU ì„¤ì • ì´ˆê¸°í™”
âœ… í´ë°± ì œê±°, ì‹¤ì œ ì‘ë™ ì½”ë“œë§Œ ìœ ì§€
âœ… PipelineManager ì¤‘ì‹¬ êµ¬ì¡°
âœ… M3 Max ìµœì í™” ì™„ì „ êµ¬í˜„
âœ… ëª¨ë“  ë¼ìš°í„° ì•ˆì „í•œ ë¡œë”©
"""

import sys
import os
import logging
from pathlib import Path
from typing import Dict, Any, Optional
import time
import asyncio

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent
sys.path.insert(0, str(project_root))

# FastAPI ë° ë¯¸ë“¤ì›¨ì–´
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import JSONResponse
import uvicorn

# ===============================================================
# ğŸ”§ Core ëª¨ë“ˆ Import (í•„ìˆ˜)
# ===============================================================

# ë¡œê¹… ì„¤ì • ë¨¼ì € (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
try:
    from app.core.logging_config import setup_logging
    setup_logging()
    logger = logging.getLogger(__name__)
    logger.info("âœ… ë¡œê¹… ì„¤ì • ì™„ë£Œ")
except ImportError as e:
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    logger.warning(f"âš ï¸ ë¡œê¹… ì„¤ì • ì‹¤íŒ¨: {e}")

# Core ëª¨ë“ˆ import
try:
    from app.core import (
        settings,
        gpu_config,
        DEVICE,
        DEVICE_NAME,
        DEVICE_TYPE,
        DEVICE_INFO,
        MODEL_CONFIG,
        IS_M3_MAX,
        get_device_config,
        get_model_config,
        get_device_info,
        check_memory_available,
        optimize_memory,
        initialization_success
    )
    logger.info("âœ… Core ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logger.error(f"âŒ Core ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print(f"âŒ Core ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ì‹œìŠ¤í…œì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(1)

# Core ëª¨ë“ˆ ì´ˆê¸°í™” í™•ì¸
if not initialization_success:
    logger.error("âŒ Core ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨")
    sys.exit(1)

logger.info("âœ… Core ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")

# ===============================================================
# ğŸ”§ ì„¤ì • ê°’ ì¶”ì¶œ
# ===============================================================

# Settings ê°ì²´ì—ì„œ ì•ˆì „í•œ ì†ì„± ì ‘ê·¼
def get_setting(key: str, default: Any = None) -> Any:
    """ì„¤ì • ê°’ ì•ˆì „ ì¶”ì¶œ"""
    try:
        # ì§ì ‘ ì†ì„± ì ‘ê·¼ ì‹œë„
        if hasattr(settings, key):
            return getattr(settings, key)
        
        # ëŒ€ì†Œë¬¸ì ë³€í™˜ ì‹œë„
        key_upper = key.upper()
        if hasattr(settings, key_upper):
            return getattr(settings, key_upper)
        
        key_lower = key.lower()
        if hasattr(settings, key_lower):
            return getattr(settings, key_lower)
        
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        env_value = os.getenv(key.upper(), os.getenv(key.lower()))
        if env_value is not None:
            return env_value
        
        return default
        
    except Exception as e:
        logger.warning(f"ì„¤ì • ê°’ '{key}' ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return default

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
APP_NAME = get_setting('APP_NAME', "MyCloset AI Backend")
APP_VERSION = get_setting('APP_VERSION', "3.0.0")
DEBUG = get_setting('DEBUG', True)
HOST = get_setting('HOST', "0.0.0.0")
PORT = get_setting('PORT', 8000)

# íƒ€ì… ë³€í™˜
if isinstance(DEBUG, str):
    DEBUG = DEBUG.lower() in ['true', '1', 'yes', 'on']
if isinstance(PORT, str):
    PORT = int(PORT)

logger.info(f"ğŸ“‹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •:")
logger.info(f"  - ì´ë¦„: {APP_NAME}")
logger.info(f"  - ë²„ì „: {APP_VERSION}")
logger.info(f"  - ë””ë²„ê·¸: {DEBUG}")
logger.info(f"  - í˜¸ìŠ¤íŠ¸: {HOST}")
logger.info(f"  - í¬íŠ¸: {PORT}")

# ===============================================================
# ğŸ”¥ API ë¼ìš°í„°ë“¤ ì•ˆì „í•œ Import
# ===============================================================

api_routers = {}

# 1. Health ë¼ìš°í„° (ê¸°ë³¸)
try:
    from app.api.health import router as health_router
    api_routers['health'] = health_router
    logger.info("âœ… Health ë¼ìš°í„° ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logger.warning(f"âš ï¸ Health ë¼ìš°í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

# 2. Step Routes ë¼ìš°í„° (ë©”ì¸ - PipelineManager ì—°ê²°)
try:
    from app.api.step_routes import router as step_routes_router
    api_routers['step_routes'] = step_routes_router
    logger.info("ğŸ”¥ Step Routes ë¼ìš°í„° ë¡œë“œ ì„±ê³µ (PipelineManager ì—°ê²°)")
except ImportError as e:
    logger.warning(f"âš ï¸ Step Routes ë¼ìš°í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

# 3. WebSocket ë¼ìš°í„° (ì‹¤ì‹œê°„ í†µì‹ )
try:
    from app.api.websocket_routes import router as websocket_router
    api_routers['websocket'] = websocket_router
    logger.info("âœ… WebSocket ë¼ìš°í„° ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logger.warning(f"âš ï¸ WebSocket ë¼ìš°í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

# 4. Models ë¼ìš°í„° (ëª¨ë¸ ê´€ë¦¬)
try:
    from app.api.models import router as models_router
    api_routers['models'] = models_router
    logger.info("âœ… Models ë¼ìš°í„° ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logger.warning(f"âš ï¸ Models ë¼ìš°í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

# 5. Pipeline ë¼ìš°í„° (ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰)
try:
    from app.api.pipeline_routes import router as pipeline_router
    api_routers['pipeline'] = pipeline_router
    logger.info("âœ… Pipeline ë¼ìš°í„° ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logger.warning(f"âš ï¸ Pipeline ë¼ìš°í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

# ===============================================================
# ğŸš€ FastAPI ì•± ìƒì„±
# ===============================================================

app = FastAPI(
    title=APP_NAME,
    description=f"AI-powered virtual try-on platform (PipelineManager ì¤‘ì‹¬, {DEVICE_NAME} ìµœì í™”)",
    version=APP_VERSION,
    docs_url="/docs",
    redoc_url="/redoc",
    debug=DEBUG
)

# ===============================================================
# ğŸ›¡ï¸ ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
# ===============================================================

# CORS ë¯¸ë“¤ì›¨ì–´
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",  # React ê°œë°œ ì„œë²„
        "http://localhost:5173",  # Vite ê°œë°œ ì„œë²„
        "http://localhost:8080",  # ì¶”ê°€ ê°œë°œ ì„œë²„
        "http://127.0.0.1:3000",  # ë¡œì»¬ IP
        "http://127.0.0.1:5173",  # ë¡œì»¬ IP
        "https://mycloset-ai.vercel.app",  # ë°°í¬ìš©
        "https://*.vercel.app",  # ê¸°íƒ€ Vercel ë°°í¬
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH", "HEAD"],
    allow_headers=["*"],
)

# Gzip ì••ì¶• ë¯¸ë“¤ì›¨ì–´
app.add_middleware(GZipMiddleware, minimum_size=1000)

# ìš”ì²­ ë¡œê¹… ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def log_requests(request: Request, call_next):
    """ìš”ì²­ ë¡œê¹… ë¯¸ë“¤ì›¨ì–´"""
    start_time = time.time()
    
    # ìš”ì²­ ë¡œê·¸
    logger.info(f"ğŸ“¥ {request.method} {request.url.path}")
    
    # ì‘ë‹µ ì²˜ë¦¬
    response = await call_next(request)
    
    # ì‘ë‹µ ë¡œê·¸
    process_time = time.time() - start_time
    logger.info(f"ğŸ“¤ {request.method} {request.url.path} - {response.status_code} ({process_time:.3f}s)")
    
    return response

# ===============================================================
# ğŸ“ ì •ì  íŒŒì¼ ì„œë¹™
# ===============================================================

# ì •ì  íŒŒì¼ ë””ë ‰í† ë¦¬ ì„¤ì •
static_dir = project_root / "static"
if not static_dir.exists():
    static_dir.mkdir(parents=True, exist_ok=True)

app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")
logger.info(f"âœ… ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì •: {static_dir}")

# ì—…ë¡œë“œ ë° ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
uploads_dir = static_dir / "uploads"
uploads_dir.mkdir(parents=True, exist_ok=True)

results_dir = static_dir / "results"
results_dir.mkdir(parents=True, exist_ok=True)

# ===============================================================
# ğŸ”¥ API ë¼ìš°í„° ë“±ë¡ (PipelineManager ì¤‘ì‹¬)
# ===============================================================

# 1. Health ë¼ìš°í„° (ê¸°ë³¸ í—¬ìŠ¤ì²´í¬)
if api_routers.get('health'):
    try:
        app.include_router(api_routers['health'], prefix="/api", tags=["health"])
        logger.info("âœ… Health ë¼ìš°í„° ë“±ë¡ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"Health ë¼ìš°í„° ë“±ë¡ ì‹¤íŒ¨: {e}")

# 2. Step Routes ë¼ìš°í„° (ë©”ì¸ - 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸)
if api_routers.get('step_routes'):
    try:
        app.include_router(api_routers['step_routes'], prefix="/api/step", tags=["step-pipeline"])
        logger.info("ğŸ”¥ Step Routes ë¼ìš°í„° ë“±ë¡ ì™„ë£Œ (PipelineManager ì—°ê²°)")
        logger.info("   ğŸ“‹ ì—”ë“œí¬ì¸íŠ¸:")
        logger.info("     - POST /api/step/1/upload-validation")
        logger.info("     - POST /api/step/3/human-parsing")
        logger.info("     - POST /api/step/7/virtual-fitting")
        logger.info("     - GET /api/step/health")
    except Exception as e:
        logger.warning(f"Step Routes ë¼ìš°í„° ë“±ë¡ ì‹¤íŒ¨: {e}")

# 3. WebSocket ë¼ìš°í„° (ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©)
if api_routers.get('websocket'):
    try:
        app.include_router(api_routers['websocket'], prefix="/api/ws", tags=["websocket"])
        logger.info("âœ… WebSocket ë¼ìš°í„° ë“±ë¡ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"WebSocket ë¼ìš°í„° ë“±ë¡ ì‹¤íŒ¨: {e}")

# 4. Models ë¼ìš°í„° (AI ëª¨ë¸ ê´€ë¦¬)
if api_routers.get('models'):
    try:
        app.include_router(api_routers['models'], prefix="/api/models", tags=["models"])
        logger.info("âœ… Models ë¼ìš°í„° ë“±ë¡ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"Models ë¼ìš°í„° ë“±ë¡ ì‹¤íŒ¨: {e}")

# 5. Pipeline ë¼ìš°í„° (ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰)
if api_routers.get('pipeline'):
    try:
        app.include_router(api_routers['pipeline'], prefix="/api/pipeline", tags=["pipeline"])
        logger.info("âœ… Pipeline ë¼ìš°í„° ë“±ë¡ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"Pipeline ë¼ìš°í„° ë“±ë¡ ì‹¤íŒ¨: {e}")

# ===============================================================
# ğŸŒ ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸
# ===============================================================

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "MyCloset AI Backend is running!",
        "version": APP_VERSION,
        "architecture": "PipelineManager-centered",
        "ai_pipeline": "8-step AI pipeline",
        "optimization": f"{DEVICE_NAME} optimized",
        "gpu_config": {
            "device": DEVICE,
            "device_name": DEVICE_NAME,
            "is_m3_max": IS_M3_MAX,
            "memory_gb": gpu_config.memory_gb,
            "optimization_level": gpu_config.optimization_settings["optimization_level"]
        },
        "endpoints": {
            "docs": "/docs",
            "health": "/api/health",
            "step_pipeline": "/api/step/",
            "websocket": "/api/ws/",
            "models": "/api/models/",
            "pipeline": "/api/pipeline/"
        }
    }

@app.get("/health")
async def health_check():
    """ê¸°ë³¸ í—¬ìŠ¤ì²´í¬"""
    memory_check = check_memory_available(min_gb=1.0)
    
    return {
        "status": "healthy",
        "version": APP_VERSION,
        "architecture": "PipelineManager-centered",
        "debug": DEBUG,
        "gpu_config": {
            "device": DEVICE,
            "device_name": DEVICE_NAME,
            "is_m3_max": IS_M3_MAX,
            "memory_gb": gpu_config.memory_gb,
            "optimization_level": gpu_config.optimization_settings["optimization_level"]
        },
        "memory_status": {
            "available": memory_check.get('is_available', False),
            "system_memory_gb": memory_check.get('system_memory', {}).get('available_gb', 0)
        },
        "loaded_routers": list(api_routers.keys()),
        "initialization_success": initialization_success
    }

@app.get("/api/status")
async def api_status():
    """API ìƒíƒœ í™•ì¸"""
    device_info = get_device_info()
    model_config = get_model_config()
    
    return {
        "status": "operational",
        "timestamp": time.time(),
        "loaded_routers": list(api_routers.keys()),
        "total_routes": len(api_routers),
        "ai_pipeline_ready": "step_routes" in api_routers,
        "websocket_ready": "websocket" in api_routers,
        "gpu_status": {
            "device": DEVICE,
            "device_name": DEVICE_NAME,
            "is_m3_max": IS_M3_MAX,
            "memory_gb": gpu_config.memory_gb,
            "optimization_level": gpu_config.optimization_settings["optimization_level"],
            "is_initialized": gpu_config.is_initialized
        },
        "pipeline_optimizations": len(gpu_config.pipeline_optimizations),
        "model_config": {
            "batch_size": model_config.get("batch_size", 1),
            "dtype": model_config.get("dtype", "float32"),
            "max_workers": model_config.get("max_workers", 4),
            "concurrent_sessions": model_config.get("concurrent_sessions", 1)
        }
    }

@app.get("/api/gpu-info")
async def gpu_info():
    """GPU ì •ë³´ ìƒì„¸ ì¡°íšŒ"""
    return {
        "gpu_config": get_device_config(),
        "model_config": get_model_config(),
        "device_info": get_device_info(),
        "memory_stats": gpu_config.get_memory_stats(),
        "optimization_settings": gpu_config.optimization_settings,
        "pipeline_optimizations": gpu_config.pipeline_optimizations,
        "is_m3_max": IS_M3_MAX,
        "capabilities": {
            "supports_fp16": model_config.get("dtype") == "float16",
            "supports_neural_engine": IS_M3_MAX,
            "supports_metal_shaders": DEVICE == "mps",
            "supports_8step_pipeline": True,
            "max_batch_size": model_config.get("batch_size", 1) * 2,
            "recommended_image_size": (768, 768) if IS_M3_MAX else (512, 512)
        }
    }

@app.post("/api/optimize-memory")
async def optimize_memory_endpoint():
    """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰"""
    try:
        result = optimize_memory(aggressive=True)
        return {
            "status": "success",
            "optimization_result": result,
            "memory_stats": gpu_config.get_memory_stats(),
            "timestamp": time.time()
        }
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {str(e)}")

# ===============================================================
# ğŸ”§ ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ë²¤íŠ¸
# ===============================================================

@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì‹¤í–‰"""
    logger.info("ğŸš€ MyCloset AI Backend ì‹œì‘ë¨")
    logger.info(f"ğŸ—ï¸ ì•„í‚¤í…ì²˜: PipelineManager ì¤‘ì‹¬")
    logger.info(f"ğŸ”§ ì„¤ì •: {APP_NAME} v{APP_VERSION}")
    logger.info(f"ğŸ¤– AI íŒŒì´í”„ë¼ì¸: 8ë‹¨ê³„ í†µí•©")
    logger.info(f"ğŸ“Š ë¡œë“œëœ ë¼ìš°í„°: {len(api_routers)}ê°œ")
    
    # GPU ì„¤ì • ì •ë³´
    logger.info(f"ğŸ¯ GPU ì„¤ì •:")
    logger.info(f"  - ë””ë°”ì´ìŠ¤: {DEVICE} ({DEVICE_NAME})")
    logger.info(f"  - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
    logger.info(f"  - ë©”ëª¨ë¦¬: {gpu_config.memory_gb:.1f}GB")
    logger.info(f"  - ìµœì í™” ë ˆë²¨: {gpu_config.optimization_settings['optimization_level']}")
    logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {gpu_config.model_config['batch_size']}")
    logger.info(f"  - ì •ë°€ë„: {gpu_config.model_config['dtype']}")
    logger.info(f"  - ë™ì‹œ ì„¸ì…˜: {gpu_config.optimization_settings['concurrent_sessions']}")
    
    # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
    memory_check = check_memory_available(min_gb=2.0)
    if memory_check.get('is_available', False):
        logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ìƒíƒœ: {memory_check['system_memory']['available_gb']:.1f}GB ì‚¬ìš© ê°€ëŠ¥")
    else:
        logger.warning("âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± - ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # M3 Max íŠ¹í™” ì •ë³´
    if IS_M3_MAX:
        logger.info("ğŸ M3 Max íŠ¹í™” ê¸°ëŠ¥ í™œì„±í™”:")
        logger.info("  - Neural Engine ê°€ì†")
        logger.info("  - Metal Performance Shaders")
        logger.info("  - í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”")
        logger.info("  - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”")
        logger.info("  - ê³ í•´ìƒë„ ì²˜ë¦¬ ì§€ì›")
    
    # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ìƒíƒœ
    pipeline_count = len(gpu_config.pipeline_optimizations)
    if pipeline_count > 0:
        logger.info(f"âš™ï¸ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”: {pipeline_count}ê°œ ë‹¨ê³„ ì„¤ì •ë¨")
    
    # ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰
    try:
        optimization_result = optimize_memory()
        if optimization_result.get('success', False):
            logger.info(f"ğŸ’¾ ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {optimization_result['method']}")
    except Exception as e:
        logger.warning(f"ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")

@app.on_event("shutdown")
async def shutdown_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ì‹¤í–‰"""
    logger.info("ğŸ›‘ MyCloset AI Backend ì¢…ë£Œ ì¤‘...")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    try:
        cleanup_result = gpu_config.cleanup_memory(aggressive=True)
        if cleanup_result.get('success', False):
            logger.info(f"ğŸ’¾ ì¢…ë£Œ ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {cleanup_result['method']}")
    except Exception as e:
        logger.warning(f"ì¢…ë£Œ ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    logger.info("ğŸ›‘ MyCloset AI Backend ì¢…ë£Œë¨")

# ===============================================================
# ğŸ¯ ë©”ì¸ ì‹¤í–‰
# ===============================================================

if __name__ == "__main__":
    logger.info("ğŸš€ MyCloset AI Backend ì„œë²„ ì‹œì‘ ì¤‘...")
    logger.info(f"ğŸ“ ì£¼ì†Œ: http://{HOST}:{PORT}")
    logger.info(f"ğŸ“– API ë¬¸ì„œ: http://{HOST}:{PORT}/docs")
    logger.info(f"ğŸ—ï¸ ì•„í‚¤í…ì²˜: PipelineManager ì¤‘ì‹¬ (VirtualFitter ì œê±°)")
    logger.info(f"ğŸ¯ GPU ìµœì í™”: {DEVICE_NAME} ({DEVICE})")
    logger.info(f"ğŸ M3 Max ìµœì í™”: {'âœ…' if IS_M3_MAX else 'âŒ'}")
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    logger.info("ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´:")
    logger.info(f"  - Python: {sys.version.split()[0]}")
    logger.info(f"  - PyTorch: {gpu_config.hardware.system_info['pytorch_version']}")
    logger.info(f"  - Platform: {gpu_config.hardware.system_info['platform']}")
    logger.info(f"  - Machine: {gpu_config.hardware.system_info['machine']}")
    logger.info(f"  - CPU ì½”ì–´: {gpu_config.hardware.cpu_cores}")
    logger.info(f"  - ë©”ëª¨ë¦¬: {gpu_config.memory_gb:.1f}GB")
    
    # ê°œë°œ ëª¨ë“œ ê²½ê³ 
    if DEBUG:
        logger.warning("âš ï¸ ê°œë°œ ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ - í”„ë¡œë•ì…˜ì—ì„œëŠ” DEBUG=Falseë¡œ ì„¤ì •í•˜ì„¸ìš”")
    
    # ì„œë²„ ì‹œì‘
    try:
        uvicorn.run(
            "app.main:app",
            host=HOST,
            port=PORT,
            reload=DEBUG,
            log_level="info" if DEBUG else "warning",
            access_log=DEBUG,
            workers=1  # GPU ì‚¬ìš© ì‹œ ë‹¨ì¼ ì›Œì»¤ ê¶Œì¥
        )
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        sys.exit(1)