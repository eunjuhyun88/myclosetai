# app/main.py
"""
MyCloset AI ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
ê°œì„ ëœ PipelineManagerì™€ ì™„ì „íˆ í†µí•©ëœ ë²„ì „
"""

import sys
import os
import asyncio
import logging
import traceback
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import JSONResponse, FileResponse
from typing import Optional
import uvicorn

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë“ˆë“¤ import
try:
    from app.api.unified_routes import router as unified_router
    from app.core.config import get_settings
    from app.core.logging_config import setup_logging
    from app.ai_pipeline.pipeline_manager import PipelineManager
except ImportError as e:
    # ê°œë°œ í™˜ê²½ì—ì„œ ëª¨ë“ˆì´ ì—†ì„ ë•Œ ëŒ€ì²´
    print(f"Warning: {e}")
    unified_router = None
    
    class MockSettings:
        def __init__(self):
            self.debug = True
            self.cors_origins = ["*"]
    
    def get_settings():
        return MockSettings()
    
    def setup_logging():
        logging.basicConfig(level=logging.INFO)

# ì„¤ì • ë¡œë“œ
settings = get_settings()

# ë¡œê¹… ì„¤ì •
setup_logging()
logger = logging.getLogger(__name__)

# ê¸€ë¡œë²Œ ë³€ìˆ˜
pipeline_manager = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜ëª…ì£¼ê¸° ê´€ë¦¬"""
    
    # ì‹œì‘ ì‹œ ì‹¤í–‰
    logger.info("ğŸš€ MyCloset AI ì‹œìŠ¤í…œ ì‹œì‘ ì¤‘...")
    
    try:
        # AI íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        global pipeline_manager
        pipeline_manager = PipelineManager(
            config_path='config/pipeline_config.json',  # ì„¤ì • íŒŒì¼ ê²½ë¡œ
            device='auto'  # ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
        )
        
        # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ë¹„ë™ê¸°)
        logger.info("ğŸ¤– AI íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
        success = await pipeline_manager.initialize()
        
        if success:
            logger.info("âœ… AI íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¶œë ¥
            status = await pipeline_manager.get_pipeline_status()
            logger.info(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ìƒíƒœ: {status['steps_status']}")
        else:
            logger.warning("âš ï¸ AI íŒŒì´í”„ë¼ì¸ ë¶€ë¶„ ì´ˆê¸°í™” (ì¼ë¶€ ëª¨ë¸ ëˆ„ë½)")
        
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("app/static/uploads", exist_ok=True)
        os.makedirs("app/static/results", exist_ok=True)
        os.makedirs("app/logs", exist_ok=True)
        os.makedirs("output", exist_ok=True)
        os.makedirs("test_images", exist_ok=True)
        
        logger.info("ğŸ“ í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ")
        
        # GPU ì •ë³´ ì¶œë ¥
        try:
            import torch
            if torch.cuda.is_available():
                logger.info(f"ğŸ”¥ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                logger.info("ğŸ Apple MPS ì‚¬ìš© ê°€ëŠ¥")
            else:
                logger.info("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        except ImportError:
            logger.info("ğŸ’» PyTorch ì—†ìŒ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        
        logger.info("ğŸ‰ MyCloset AI ì‹œìŠ¤í…œ ì‹œì‘ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.error(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    yield  # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
    
    # ì¢…ë£Œ ì‹œ ì‹¤í–‰
    logger.info("ğŸ›‘ MyCloset AI ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
    
    try:
        if pipeline_manager:
            await pipeline_manager.cleanup()
            logger.info("ğŸ§¹ AI íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì™„ë£Œ")
            
        # ì¶”ê°€ ì •ë¦¬ ì‘ì—…
        logger.info("âœ… ì‹œìŠ¤í…œ ì¢…ë£Œ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
app = FastAPI(
    title="MyCloset AI Virtual Try-On",
    description="AI ê¸°ë°˜ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins if hasattr(settings, 'cors_origins') else ["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(GZipMiddleware, minimum_size=1000)

# ì •ì  íŒŒì¼ ì„œë¹™
try:
    app.mount("/static", StaticFiles(directory="app/static"), name="static")
    app.mount("/output", StaticFiles(directory="output"), name="output")
except Exception as e:
    logger.warning(f"ì •ì  íŒŒì¼ ë§ˆìš´íŠ¸ ì‹¤íŒ¨: {e}")

# API ë¼ìš°í„° ë“±ë¡
if unified_router:
    app.include_router(unified_router, prefix="", tags=["virtual-tryon"])
else:
    logger.warning("API ë¼ìš°í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ë§Œ ì œê³µë©ë‹ˆë‹¤.")

# ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "MyCloset AI Virtual Try-On System",
        "version": "2.0.0",
        "status": "running",
        "features": [
            "8ë‹¨ê³„ AI ê°€ìƒ í”¼íŒ…",
            "ì‹¤ì‹œê°„ í’ˆì§ˆ ë¶„ì„",
            "ìë™ ì˜¤ë¥˜ ë³µêµ¬",
            "M3 Max MPS ìµœì í™”"
        ],
        "docs": "/docs",
        "api_base": "/api"
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        pipeline_status = "unknown"
        device_info = "unknown"
        memory_usage = {}
        
        if pipeline_manager:
            if hasattr(pipeline_manager, 'is_initialized'):
                pipeline_status = "initialized" if pipeline_manager.is_initialized else "not_initialized"
                device_info = getattr(pipeline_manager, 'device', 'unknown')
                
                # ìƒì„¸ ìƒíƒœ ì •ë³´
                if pipeline_manager.is_initialized:
                    try:
                        status = await pipeline_manager.get_pipeline_status()
                        memory_usage = status.get('memory_usage', {})
                    except:
                        pass
            else:
                pipeline_status = "available"
        else:
            pipeline_status = "not_loaded"
        
        health_status = {
            "status": "healthy",
            "pipeline_status": pipeline_status,
            "device": device_info,
            "memory_usage": memory_usage,
            "debug_mode": getattr(settings, 'debug', False),
            "timestamp": str(asyncio.get_event_loop().time())
        }
        
        # íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ warning ìƒíƒœ
        if pipeline_status != "initialized":
            health_status["status"] = "warning"
            health_status["message"] = "Pipeline not fully initialized"
        
        return health_status
        
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜: {e}")
        return JSONResponse({
            "status": "unhealthy",
            "error": str(e),
            "timestamp": str(asyncio.get_event_loop().time())
        }, status_code=500)

@app.get("/api/system-info")
async def system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        import torch
        import cv2
        import numpy as np
        
        info = {
            "system": {
                "python_version": sys.version,
                "platform": sys.platform,
                "architecture": os.uname().machine if hasattr(os, 'uname') else "unknown"
            },
            "dependencies": {
                "pytorch_version": torch.__version__ if 'torch' in sys.modules else "Not installed",
                "opencv_version": cv2.__version__ if 'cv2' in sys.modules else "Not installed",
                "numpy_version": np.__version__ if 'np' in sys.modules else "Not installed"
            },
            "hardware": {
                "gpu_available": False,
                "gpu_info": "None",
                "cpu_count": os.cpu_count()
            },
            "pipeline": {
                "initialized": False,
                "device": "unknown",
                "steps_loaded": 0
            }
        }
        
        # GPU ì •ë³´
        if torch.cuda.is_available():
            info["hardware"]["gpu_available"] = True
            info["hardware"]["gpu_info"] = f"CUDA: {torch.cuda.get_device_name()}"
            info["hardware"]["gpu_memory"] = f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            info["hardware"]["gpu_available"] = True
            info["hardware"]["gpu_info"] = "Apple MPS"
        
        # íŒŒì´í”„ë¼ì¸ ì •ë³´
        if pipeline_manager and hasattr(pipeline_manager, 'is_initialized'):
            info["pipeline"]["initialized"] = pipeline_manager.is_initialized
            info["pipeline"]["device"] = getattr(pipeline_manager, 'device', 'unknown')
            
            if pipeline_manager.is_initialized:
                try:
                    status = await pipeline_manager.get_pipeline_status()
                    info["pipeline"]["steps_loaded"] = len(status.get('steps_status', {}))
                    info["pipeline"]["performance_metrics"] = status.get('performance_metrics', {})
                except:
                    pass
        
        return info
        
    except Exception as e:
        return {"error": f"ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"}

@app.get("/api/pipeline-status")
async def pipeline_status():
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ìƒì„¸ ì¡°íšŒ"""
    try:
        if not pipeline_manager:
            return {"error": "Pipeline manager not loaded"}
        
        if not hasattr(pipeline_manager, 'get_pipeline_status'):
            return {"error": "Pipeline status method not available"}
        
        status = await pipeline_manager.get_pipeline_status()
        return status
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return {"error": str(e)}

@app.get("/api/performance-report")
async def performance_report():
    """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¡°íšŒ"""
    try:
        if not pipeline_manager or not hasattr(pipeline_manager, 'get_performance_report'):
            return {"error": "Performance report not available"}
        
        report = await pipeline_manager.get_performance_report()
        return report
        
    except Exception as e:
        logger.error(f"ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return {"error": str(e)}

# ê°œì„ ëœ ê°€ìƒ í”¼íŒ… ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/virtual-fitting")
async def virtual_fitting_endpoint(
    person_image: UploadFile = File(...),
    clothing_image: UploadFile = File(...),
    height: float = Form(170),
    weight: float = Form(65),
    chest: Optional[float] = Form(None),
    waist: Optional[float] = Form(None),
    hip: Optional[float] = Form(None),
    clothing_type: str = Form("shirt"),
    fabric_type: str = Form("cotton"),
    quality_target: float = Form(0.8),
    save_intermediate: bool = Form(False)
):
    """
    ê°œì„ ëœ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì—”ë“œí¬ì¸íŠ¸
    """
    try:
        if not pipeline_manager or not pipeline_manager.is_initialized:
            raise HTTPException(500, "AI íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì´ë¯¸ì§€ íŒŒì¼ ì½ê¸°
        person_image_data = await person_image.read()
        clothing_image_data = await clothing_image.read()
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        from PIL import Image
        import io
        
        person_pil = Image.open(io.BytesIO(person_image_data)).convert('RGB')
        clothing_pil = Image.open(io.BytesIO(clothing_image_data)).convert('RGB')
        
        # ì‹ ì²´ ì¹˜ìˆ˜ êµ¬ì„±
        body_measurements = {
            'height': height,
            'weight': weight
        }
        if chest is not None:
            body_measurements['chest'] = chest
        if waist is not None:
            body_measurements['waist'] = waist
        if hip is not None:
            body_measurements['hip'] = hip
        
        # ì§„í–‰ë¥  ì½œë°± (ì„ íƒì )
        progress_updates = []
        
        async def progress_callback(stage: str, percentage: int):
            progress_updates.append({"stage": stage, "percentage": percentage})
            logger.info(f"ğŸ”„ ì§„í–‰ë¥ : {stage} - {percentage}%")
        
        # ê°œì„ ëœ ê°€ìƒ í”¼íŒ… ì‹¤í–‰
        logger.info(f"ğŸ¯ ê°€ìƒ í”¼íŒ… ì‹œì‘ - ì˜ë¥˜: {clothing_type}, ì¬ì§ˆ: {fabric_type}")
        
        result = await pipeline_manager.process_complete_virtual_fitting(
            person_image=person_pil,
            clothing_image=clothing_pil,
            body_measurements=body_measurements,
            clothing_type=clothing_type,
            fabric_type=fabric_type,
            quality_target=quality_target,
            progress_callback=progress_callback,
            save_intermediate=save_intermediate,
            enable_auto_retry=True
        )
        
        if result['success']:
            # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
            import uuid
            result_filename = f"result_{uuid.uuid4().hex[:8]}.jpg"
            result_path = f"output/{result_filename}"
            
            result['result_image'].save(result_path, quality=95)
            
            # ì‘ë‹µ êµ¬ì„±
            response = {
                "success": True,
                "result_image_url": f"/output/{result_filename}",
                "final_quality_score": result['final_quality_score'],
                "quality_grade": result['quality_grade'],
                "processing_time": result['total_processing_time'],
                "quality_target_achieved": result['quality_target_achieved'],
                "fit_analysis": result['fit_analysis'],
                "improvement_suggestions": result['improvement_suggestions'],
                "step_results_summary": result['step_results_summary'],
                "processing_statistics": result['processing_statistics'],
                "progress_updates": progress_updates,
                "metadata": result['metadata']
            }
            
            # ì¤‘ê°„ ê²°ê³¼ í¬í•¨ (ìš”ì²­ëœ ê²½ìš°)
            if save_intermediate and 'intermediate_results' in result:
                response['intermediate_results'] = result['intermediate_results']
            
            logger.info(f"âœ… ê°€ìƒ í”¼íŒ… ì„±ê³µ - í’ˆì§ˆ: {result['final_quality_score']:.3f}")
            
            return response
        else:
            # ì‹¤íŒ¨ ì‘ë‹µ
            logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
            
            return {
                "success": False,
                "error": result.get('error', 'Unknown error'),
                "error_type": result.get('error_type', 'processing_failure'),
                "processing_time": result.get('processing_time', 0),
                "fallback_used": result.get('fallback_used', False),
                "progress_updates": progress_updates
            }
        
    except Exception as e:
        logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ì—”ë“œí¬ì¸íŠ¸ ì˜¤ë¥˜: {e}")
        logger.error(f"ğŸ“‹ ìƒì„¸: {traceback.format_exc()}")
        
        raise HTTPException(500, f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ë³¸ ê°€ìƒ í”¼íŒ… ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/virtual-tryon-basic")
async def virtual_tryon_basic(
    person_image: UploadFile = File(...),
    clothing_image: UploadFile = File(...),
    height: float = Form(170),
    weight: float = Form(65)
):
    """
    ê¸°ë³¸ ê°€ìƒ í”¼íŒ… ì—”ë“œí¬ì¸íŠ¸ (ê¸°ì¡´ í˜¸í™˜ì„±)
    """
    try:
        # ì´ë¯¸ì§€ íŒŒì¼ ì½ê¸°
        person_image_data = await person_image.read()
        clothing_image_data = await clothing_image.read()
        
        # íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €ê°€ ìˆìœ¼ë©´ ê¸°ì¡´ ë©”ì†Œë“œ ì‚¬ìš©
        if pipeline_manager and hasattr(pipeline_manager, 'process_virtual_tryon'):
            result = await pipeline_manager.process_virtual_tryon(
                person_image=person_image_data,
                clothing_image=clothing_image_data,
                height=height,
                weight=weight
            )
            
            if result['success']:
                # ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
                import base64
                import io
                from PIL import Image
                
                fitted_image = result['fitted_image']
                if isinstance(fitted_image, str):
                    # ì´ë¯¸ base64ì¸ ê²½ìš°
                    img_str = fitted_image
                else:
                    # numpy ë°°ì—´ì´ë‚˜ ê¸°íƒ€ í˜•ì‹ì¸ ê²½ìš°
                    if hasattr(fitted_image, 'shape'):
                        # numpy ë°°ì—´
                        img = Image.fromarray(fitted_image.astype('uint8'))
                    else:
                        # PIL ì´ë¯¸ì§€
                        img = fitted_image
                    
                    buffered = io.BytesIO()
                    img.save(buffered, format="JPEG")
                    img_str = base64.b64encode(buffered.getvalue()).decode()
                
                return {
                    "success": True,
                    "fitted_image": img_str,
                    "processing_time": result.get('processing_time', 0),
                    "confidence": result.get('confidence', 0.8),
                    "fit_score": result.get('fit_score', 0.8),
                    "recommendations": result.get('recommendations', []),
                    "pipeline_results": result.get('pipeline_results', {}),
                    "measurements": {
                        "chest": 95.0,
                        "waist": 80.0,
                        "hip": 98.0,
                        "bmi": round(weight / ((height/100) ** 2), 1)
                    }
                }
            else:
                return {
                    "success": False,
                    "error": result.get('error', 'Processing failed'),
                    "processing_time": result.get('processing_time', 0)
                }
        
        # íŒŒì´í”„ë¼ì¸ì´ ì—†ëŠ” ê²½ìš° ë”ë¯¸ ì‘ë‹µ
        else:
            import base64
            import io
            from PIL import Image
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬ (ê¸°ë³¸)
            person_img = Image.open(io.BytesIO(person_image_data))
            
            # ë”ë¯¸ ê²°ê³¼
            buffered = io.BytesIO()
            person_img.save(buffered, format="JPEG")
            img_str = base64.b64encode(buffered.getvalue()).decode()
            
            return {
                "success": True,
                "fitted_image": img_str,
                "processing_time": 1.0,
                "confidence": 0.7,
                "measurements": {
                    "chest": 95.0,
                    "waist": 80.0,
                    "hip": 98.0,
                    "bmi": round(weight / ((height/100) ** 2), 1)
                },
                "fit_score": 0.75,
                "recommendations": [
                    "ì´ê²ƒì€ ë°ëª¨ ëª¨ë“œì…ë‹ˆë‹¤.",
                    "ì‹¤ì œ AI ëª¨ë¸ì„ ì„¤ì¹˜í•˜ë©´ ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                ],
                "note": "Demo mode - AI models not loaded"
            }
        
    except Exception as e:
        logger.error(f"ê¸°ë³¸ ê°€ìƒ í”¼íŒ… ì˜¤ë¥˜: {e}")
        raise HTTPException(500, f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/download-result/{filename}")
async def download_result(filename: str):
    """ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    try:
        file_path = f"output/{filename}"
        
        if not os.path.exists(file_path):
            raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return FileResponse(
            file_path,
            media_type='image/jpeg',
            filename=filename
        )
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        raise HTTPException(500, f"ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

# ì˜ˆì™¸ ì²˜ë¦¬
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """HTTP ì˜ˆì™¸ ì²˜ë¦¬"""
    logger.error(f"HTTP ì˜ˆì™¸: {exc.status_code} - {exc.detail}")
    return JSONResponse({
        "success": False,
        "error": exc.detail,
        "status_code": exc.status_code,
        "timestamp": str(asyncio.get_event_loop().time())
    }, status_code=exc.status_code)

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """ì¼ë°˜ ì˜ˆì™¸ ì²˜ë¦¬"""
    logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {exc}")
    logger.error(f"ğŸ“‹ ìƒì„¸: {traceback.format_exc()}")
    
    return JSONResponse({
        "success": False,
        "error": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "detail": str(exc) if getattr(settings, 'debug', False) else None,
        "timestamp": str(asyncio.get_event_loop().time())
    }, status_code=500)

if __name__ == "__main__":
    # ê°œë°œ ì„œë²„ ì‹¤í–‰
    logger.info("ğŸš€ ê°œë°œ ì„œë²„ ì‹œì‘...")
    
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )