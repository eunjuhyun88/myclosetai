# backend/app/main.py
"""
MyCloset AI Backend - FastAPI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
M3 Max ìµœì í™” ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ
"""

import os
import sys
import logging
from contextlib import asynccontextmanager
from pathlib import Path

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import JSONResponse
import uvicorn

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(PROJECT_ROOT / ".env")

# ë¡œê¹… ì„¤ì •
from app.core.logging_config import setup_logging
setup_logging()
logger = logging.getLogger(__name__)

# GPU ì„¤ì • ì„í¬íŠ¸
from app.core.gpu_config import gpu_config, startup_gpu_check

# API ë¼ìš°í„° ì„í¬íŠ¸
from app.api.health import router as health_router
from app.api.virtual_tryon import router as virtual_tryon_router
from app.api.models import router as models_router

# ì„¤ì • ì„í¬íŠ¸
from app.core.config import settings


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    
    # ì‹œì‘ ì‹œ ì‹¤í–‰
    logger.info("ğŸš€ MyCloset AI Backend ì‹œì‘...")
    
    # í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±
    directories = [
        settings.UPLOAD_DIR,
        settings.RESULTS_DIR,
        settings.LOGS_DIR,
        PROJECT_ROOT / "ai_models" / "checkpoints",
        PROJECT_ROOT / "ai_models" / "temp",
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
    
    # GPU ì„¤ì • ë° í…ŒìŠ¤íŠ¸
    startup_gpu_check()
    
    # AI ëª¨ë¸ ë¡œë“œ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
    try:
        from app.services.model_manager import model_manager
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì—…ë°ì´íŠ¸
        model_manager.available_models = model_manager.get_available_models()
        logger.info(f"âœ… {sum(model_manager.available_models.values())}/{len(model_manager.available_models)} ëª¨ë¸ ê°ì§€ë¨")
        
        # ì„ íƒì  ëª¨ë¸ ë¡œë“œ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
        await model_manager.load_models()
        logger.info("âœ… AI ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        logger.info("â„¹ï¸ ëª¨ë¸ì„ ë¨¼ì € ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”: python scripts/download_ai_models.py")

    logger.info("âœ… ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì‹¤í–‰
    logger.info("ğŸ›‘ MyCloset AI Backend ì¢…ë£Œ...")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    gpu_config.optimize_memory()
    
    # ëª¨ë¸ ì–¸ë¡œë“œ
    try:
        await model_manager.unload_models()
        logger.info("âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
    except:
        pass
    
    logger.info("ğŸ‘‹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì™„ë£Œ")


# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
app = FastAPI(
    title="MyCloset AI Backend",
    description="M3 Max ìµœì í™” ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ API",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# Gzip ì••ì¶• ë¯¸ë“¤ì›¨ì–´
app.add_middleware(GZipMiddleware, minimum_size=1000)

# ì •ì  íŒŒì¼ ì„œë¹™
app.mount("/static", StaticFiles(directory=settings.STATIC_DIR), name="static")


@app.middleware("http")
async def logging_middleware(request: Request, call_next):
    """ìš”ì²­ ë¡œê¹… ë¯¸ë“¤ì›¨ì–´"""
    
    import time
    
    # ìš”ì²­ ì‹œì‘ ì‹œê°„
    start_time = time.time()
    
    # ìš”ì²­ ì •ë³´ ë¡œê¹…
    logger.info(f"ğŸ“¥ {request.method} {request.url.path}")
    
    try:
        # ìš”ì²­ ì²˜ë¦¬
        response = await call_next(request)
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        process_time = time.time() - start_time
        
        # ì‘ë‹µ ë¡œê¹…
        logger.info(f"ğŸ“¤ {request.method} {request.url.path} - {response.status_code} ({process_time:.3f}s)")
        
        # ì‘ë‹µ í—¤ë”ì— ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
        response.headers["X-Process-Time"] = str(process_time)
        
        return response
        
    except Exception as e:
        # ì—ëŸ¬ ë¡œê¹…
        process_time = time.time() - start_time
        logger.error(f"âŒ {request.method} {request.url.path} - ERROR ({process_time:.3f}s): {e}")
        raise


@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """HTTP ì˜ˆì™¸ ì²˜ë¦¬"""
    
    logger.warning(f"âš ï¸ HTTP {exc.status_code}: {exc.detail}")
    
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "success": False,
            "error": exc.detail,
            "status_code": exc.status_code,
            "path": request.url.path
        }
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """ì¼ë°˜ ì˜ˆì™¸ ì²˜ë¦¬"""
    
    logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ {request.url.path}: {exc}")
    
    return JSONResponse(
        status_code=500,
        content={
            "success": False,
            "error": "Internal server error" if not settings.DEBUG else str(exc),
            "status_code": 500,
            "path": request.url.path
        }
    )


# API ë¼ìš°í„° ë“±ë¡
app.include_router(health_router, prefix="/health", tags=["Health"])
app.include_router(virtual_tryon_router, prefix="/api/virtual-tryon", tags=["Virtual Try-On"])
app.include_router(models_router, prefix="/api/models", tags=["Models"])


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    
    return {
        "service": "MyCloset AI Backend",
        "version": "1.0.0",
        "status": "running",
        "gpu_device": gpu_config.device,
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/info")
async def get_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
    
    from app.core.gpu_config import DEVICE_INFO, MODEL_CONFIG
    import psutil
    import torch
    
    # ë©”ëª¨ë¦¬ ì •ë³´
    memory = psutil.virtual_memory()
    
    # GPU ì •ë³´
    gpu_info = {
        "device": gpu_config.device,
        "available": True,
        "details": DEVICE_INFO
    }
    
    if gpu_config.device == "mps":
        gpu_info["mps_available"] = torch.backends.mps.is_available()
        gpu_info["optimization"] = "Apple M3 Max Metal"
    elif gpu_config.device == "cuda":
        gpu_info["cuda_version"] = torch.version.cuda
        gpu_info["devices_count"] = torch.cuda.device_count()
    
    # ëª¨ë¸ ìƒíƒœ
    model_status = {}
    try:
        from app.services.model_manager import model_manager
        model_status = {
            "loaded_models": list(model_manager.loaded_models.keys()),
            "total_models": len(model_manager.available_models),
        }
    except:
        model_status = {"status": "model_manager_not_ready"}
    
    return {
        "system": {
            "platform": DEVICE_INFO["platform"],
            "machine": DEVICE_INFO["machine"],
            "python_version": DEVICE_INFO["python_version"],
            "pytorch_version": DEVICE_INFO["pytorch_version"],
        },
        "memory": {
            "total_gb": round(memory.total / (1024**3), 1),
            "available_gb": round(memory.available / (1024**3), 1),
            "used_percent": memory.percent,
        },
        "gpu": gpu_info,
        "models": model_status,
        "config": {
            "debug": settings.DEBUG,
            "max_upload_size_mb": settings.MAX_UPLOAD_SIZE // (1024*1024),
            "allowed_extensions": settings.ALLOWED_EXTENSIONS,
            "device": MODEL_CONFIG["device"],
            "batch_size": MODEL_CONFIG["batch_size"],
        }
    }


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ ì„œë²„ ì‹œì‘
    uvicorn.run(
        "app.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG,
        log_level="info" if settings.DEBUG else "warning",
        access_log=settings.DEBUG,
    )