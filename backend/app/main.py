"""
MyCloset AI FastAPI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ê³¼ í”„ë¡ íŠ¸ì—”ë“œë¥¼ ì—°ê²°í•˜ëŠ” ë°±ì—”ë“œ ì„œë²„
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import JSONResponse
import logging
import time
import psutil
from datetime import datetime
from pathlib import Path

# ë¡œì»¬ ì„í¬íŠ¸
from .api.pipeline_routes import router as pipeline_router
from .core.config import settings
from .models.schemas import HealthCheck, SystemStats, MonitoringData

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=getattr(logging, settings.LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(settings.LOG_FILE),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="MyCloset AI Backend",
    description="8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ê¸°ë°˜ ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# ì‹œì‘ ì‹œê°„ ê¸°ë¡
start_time = time.time()
request_stats = {
    "total_requests": 0,
    "successful_requests": 0,
    "processing_times": [],
    "quality_scores": []
}

# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# GZIP ì••ì¶• ë¯¸ë“¤ì›¨ì–´
app.add_middleware(GZipMiddleware, minimum_size=1000)

# ì •ì  íŒŒì¼ ì„œë¹™
app.mount("/static", StaticFiles(directory="static"), name="static")

# ë¼ìš°í„° ë“±ë¡
app.include_router(pipeline_router)

# ìš”ì²­ ì²˜ë¦¬ ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def process_request_middleware(request, call_next):
    """ìš”ì²­ ì²˜ë¦¬ ë¯¸ë“¤ì›¨ì–´ - í†µê³„ ìˆ˜ì§‘"""
    start_time_req = time.time()
    request_stats["total_requests"] += 1
    
    try:
        response = await call_next(request)
        
        # ì„±ê³µí•œ ìš”ì²­ ê¸°ë¡
        if response.status_code < 400:
            request_stats["successful_requests"] += 1
            processing_time = time.time() - start_time_req
            request_stats["processing_times"].append(processing_time)
            
            # ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ë§ì´ ìŒ“ì´ì§€ ì•Šë„ë¡ ì œí•œ
            if len(request_stats["processing_times"]) > 1000:
                request_stats["processing_times"] = request_stats["processing_times"][-500:]
        
        return response
        
    except Exception as e:
        logger.error(f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": "Internal Server Error", "detail": str(e)}
        )

# ì‹œì‘/ì¢…ë£Œ ì´ë²¤íŠ¸
@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì‹¤í–‰"""
    logger.info("ğŸš€ MyCloset AI Backend ì‹œì‘")
    logger.info(f"âš™ï¸  ì„¤ì •: {settings.APP_NAME} v{settings.APP_VERSION}")
    logger.info(f"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {settings.DEVICE}")
    logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ í•œê³„: {settings.MEMORY_LIMIT_GB}GB")
    
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(settings.UPLOAD_DIR).mkdir(parents=True, exist_ok=True)
    Path(settings.RESULT_DIR).mkdir(parents=True, exist_ok=True)
    Path("logs").mkdir(exist_ok=True)
    
    logger.info("âœ… ë°±ì—”ë“œ ì‹œì‘ ì™„ë£Œ")

@app.on_event("shutdown")
async def shutdown_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ì‹¤í–‰"""
    logger.info("ğŸ›‘ MyCloset AI Backend ì¢…ë£Œ ì¤‘...")
    
    # íŒŒì´í”„ë¼ì¸ ì •ë¦¬
    try:
        from .api.pipeline_routes import pipeline_instance
        if pipeline_instance:
            pipeline_instance.cleanup()
            logger.info("ğŸ§¹ íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    logger.info("âœ… ë°±ì—”ë“œ ì¢…ë£Œ ì™„ë£Œ")

# ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "MyCloset AI Backend API",
        "version": "1.0.0",
        "status": "ìš´ì˜ ì¤‘",
        "docs": "/docs",
        "pipeline_status": "/api/pipeline/status"
    }

@app.get("/health", response_model=HealthCheck)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return HealthCheck(
        status="healthy",
        timestamp=datetime.now(),
        version="1.0.0",
        uptime=time.time() - start_time
    )

@app.get("/stats", response_model=SystemStats)
async def get_system_stats():
    """ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ"""
    avg_processing_time = (
        sum(request_stats["processing_times"]) / len(request_stats["processing_times"])
        if request_stats["processing_times"] else 0.0
    )
    
    avg_quality_score = (
        sum(request_stats["quality_scores"]) / len(request_stats["quality_scores"])
        if request_stats["quality_scores"] else 0.0
    )
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    memory_info = psutil.virtual_memory()
    peak_memory = memory_info.used / (1024**3)  # GB
    
    return SystemStats(
        total_requests=request_stats["total_requests"],
        successful_requests=request_stats["successful_requests"],
        average_processing_time=avg_processing_time,
        average_quality_score=avg_quality_score,
        peak_memory_usage=peak_memory,
        uptime=time.time() - start_time,
        last_request_time=datetime.now() if request_stats["total_requests"] > 0 else None
    )

@app.get("/monitoring", response_model=MonitoringData)
async def get_monitoring_data():
    """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë°ì´í„°"""
    
    # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë³´
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    net_io = psutil.net_io_counters()
    
    return MonitoringData(
        cpu_usage=cpu_percent,
        memory_usage=memory.percent,
        disk_usage=(disk.used / disk.total) * 100,
        network_io={
            "bytes_sent": float(net_io.bytes_sent),
            "bytes_recv": float(net_io.bytes_recv)
        },
        active_requests=0,  # ì‹¤ì œ êµ¬í˜„ì‹œ í™œì„± ìš”ì²­ ìˆ˜ ì¶”ì 
        queue_size=0,       # ì‹¤ì œ êµ¬í˜„ì‹œ ëŒ€ê¸°ì—´ í¬ê¸° ì¶”ì 
        timestamp=datetime.now()
    )

# ê°œë°œìš© ì—”ë“œí¬ì¸íŠ¸
@app.get("/dev/info")
async def development_info():
    """ê°œë°œ ì •ë³´ (ê°œë°œ ëª¨ë“œì—ì„œë§Œ)"""
    if not settings.DEBUG:
        raise HTTPException(status_code=404, detail="Not found")
    
    import torch
    import platform
    
    return {
        "system": {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "architecture": platform.machine()
        },
        "pytorch": {
            "version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "mps_available": torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False,
            "device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
        },
        "settings": {
            "app_name": settings.APP_NAME,
            "device": settings.DEVICE,
            "debug": settings.DEBUG,
            "memory_limit": f"{settings.MEMORY_LIMIT_GB}GB"
        }
    }

# ê¸€ë¡œë²Œ ì˜ˆì™¸ ì²˜ë¦¬ê¸°
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """ê¸€ë¡œë²Œ ì˜ˆì™¸ ì²˜ë¦¬"""
    logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(exc)}", exc_info=True)
    
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal Server Error",
            "message": "ì„œë²„ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "request_id": str(id(request))
        }
    )

@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """HTTP ì˜ˆì™¸ ì²˜ë¦¬"""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "status_code": exc.status_code
        }
    )

# ì»¤ìŠ¤í…€ ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.post("/api/feedback")
async def submit_feedback(feedback: dict):
    """ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘"""
    logger.info(f"ì‚¬ìš©ì í”¼ë“œë°±: {feedback}")
    
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    return {
        "success": True,
        "message": "í”¼ë“œë°±ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "feedback_id": str(time.time())
    }

@app.get("/api/examples")
async def get_example_images():
    """ì˜ˆì‹œ ì´ë¯¸ì§€ ëª©ë¡ ì¡°íšŒ"""
    examples_dir = Path("static/examples")
    examples_dir.mkdir(exist_ok=True)
    
    example_files = []
    for file_path in examples_dir.glob("*.jpg"):
        example_files.append({
            "name": file_path.stem,
            "url": f"/static/examples/{file_path.name}",
            "type": "person" if "person" in file_path.name else "clothing"
        })
    
    return {
        "examples": example_files,
        "total_count": len(example_files)
    }

if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "app.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG,
        log_level=settings.LOG_LEVEL.lower()
    )