# backend/app/main.py
"""
ğŸ”¥ MyCloset AI FastAPI ë©”ì¸ ì„œë²„ - ì™„ì „í•œ ëª¨ë“  ë¼ìš°í„° ì—°ë™ + ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸ v21.0
================================================================================

âœ… ëª¨ë“  API ë¼ìš°í„° ì™„ì „ ì—°ë™ (pipeline, step, health, models, websocket)
âœ… ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸ ì™„ì „ ì—°ë™ (Mock ì™„ì „ ì œê±°)
âœ… 8ë‹¨ê³„ ì‹¤ì œ AI Steps (SCHP, OpenPose, OOTDiffusion ë“±)
âœ… DI Container ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬  
âœ… conda í™˜ê²½ + M3 Max 128GB ìµœì í™”
âœ… React/TypeScript í”„ë¡ íŠ¸ì—”ë“œ 100% í˜¸í™˜
âœ… WebSocket ì‹¤ì‹œê°„ AI ì§„í–‰ë¥  ì¶”ì 
âœ… ì„¸ì…˜ ê¸°ë°˜ ì´ë¯¸ì§€ ê´€ë¦¬ (ì¬ì—…ë¡œë“œ ë°©ì§€)
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± + ì—ëŸ¬ ì²˜ë¦¬
ğŸ”¥ SmartModelPathMapper ì™„ì „ ì—°ë™ - ì›Œë‹ ì œê±°!

ğŸ”¥ ëª¨ë“  ë¼ìš°í„° ì™„ì „ ì—°ë™:
- /api/step/* â†’ step_routes.py (8ë‹¨ê³„ ê°œë³„ API)
- /api/pipeline/* â†’ pipeline_routes.py (í†µí•© íŒŒì´í”„ë¼ì¸ API)  
- /api/health/* â†’ health.py (í—¬ìŠ¤ì²´í¬ API)
- /api/models/* â†’ models.py (ëª¨ë¸ ê´€ë¦¬ API)
- /api/ws/* â†’ websocket_routes.py (WebSocket ì‹¤ì‹œê°„ í†µì‹ )

ğŸ”¥ ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸:
Step 1: HumanParsingStep (ì‹¤ì œ SCHP/Graphonomy)
Step 2: PoseEstimationStep (ì‹¤ì œ OpenPose/YOLO) 
Step 3: ClothSegmentationStep (ì‹¤ì œ U2Net/SAM)
Step 4: GeometricMatchingStep (ì‹¤ì œ TPS/GMM)
Step 5: ClothWarpingStep (ì‹¤ì œ Cloth Warping)
Step 6: VirtualFittingStep (ì‹¤ì œ OOTDiffusion/IDM-VTON) ğŸ”¥
Step 7: PostProcessingStep (ì‹¤ì œ Enhancement/SR)
Step 8: QualityAssessmentStep (ì‹¤ì œ CLIP/Quality Assessment)

ì•„í‚¤í…ì²˜ v21.0:
RealAIDIContainer â†’ SmartModelPathMapper â†’ ModelLoader â†’ StepFactory â†’ RealAI Steps â†’ All Routers â†’ FastAPI

Author: MyCloset AI Team
Date: 2025-07-26
Version: 21.0.0 (Complete All Routers Integration + Real AI + Warning Fix)
"""

import os
import sys
import logging
import asyncio
import time
import gc
import uuid
import threading
import traceback
import subprocess
import platform
import psutil
import json
import weakref
from pathlib import Path
from datetime import datetime, timedelta
from contextlib import asynccontextmanager
from typing import Dict, Any, Optional, List, Union, Callable, Tuple
import warnings

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')
os.environ['PYTHONWARNINGS'] = 'ignore'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# =============================================================================
# ğŸ”¥ 1. ì‹¤í–‰ ê²½ë¡œ ìë™ ìˆ˜ì • (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜)
# =============================================================================

def fix_python_path():
    """ì‹¤í–‰ ê²½ë¡œì— ê´€ê³„ì—†ì´ Python Path ìë™ ìˆ˜ì •"""
    current_file = Path(__file__).absolute()
    app_dir = current_file.parent       # backend/app
    backend_dir = app_dir.parent        # backend
    project_root = backend_dir.parent   # mycloset-ai
    
    # Python Pathì— í•„ìš”í•œ ê²½ë¡œë“¤ ì¶”ê°€
    paths_to_add = [
        str(backend_dir),    # backend/ (ê°€ì¥ ì¤‘ìš”!)
        str(app_dir),        # backend/app/
        str(project_root)    # mycloset-ai/
    ]
    
    for path in paths_to_add:
        if path not in sys.path:
            sys.path.insert(0, path)
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ.update({
        'PYTHONPATH': f"{backend_dir}:{os.environ.get('PYTHONPATH', '')}",
        'PROJECT_ROOT': str(project_root),
        'BACKEND_ROOT': str(backend_dir),
        'APP_ROOT': str(app_dir)
    })
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ backendë¡œ ë³€ê²½
    if Path.cwd() != backend_dir:
        try:
            os.chdir(backend_dir)
        except OSError:
            pass
    
    return {
        'app_dir': str(app_dir),
        'backend_dir': str(backend_dir),
        'project_root': str(project_root)
    }

# Python Path ìˆ˜ì • ì‹¤í–‰
path_info = fix_python_path()

# =============================================================================
# ğŸ”¥ 2. ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ ë° ìµœì í™”
# =============================================================================

def detect_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ì§ì ‘ ê°ì§€"""
    system_info = {
        'platform': platform.system(),
        'python_version': platform.python_version(),
        'cpu_count': os.cpu_count() or 4
    }
    
    # conda í™˜ê²½ ê°ì§€
    is_conda = (
        'CONDA_DEFAULT_ENV' in os.environ or
        'CONDA_PREFIX' in os.environ or
        'conda' in sys.executable.lower()
    )
    system_info['is_conda'] = is_conda
    system_info['conda_env'] = os.environ.get('CONDA_DEFAULT_ENV', 'none')
    
    # M3 Max ê°ì§€
    is_m3_max = False
    if platform.system() == 'Darwin':
        try:
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                capture_output=True, text=True, timeout=5
            )
            chip_info = result.stdout.strip()
            is_m3_max = 'M3' in chip_info and 'Max' in chip_info
        except:
            pass
    
    system_info['is_m3_max'] = is_m3_max
    
    # ë©”ëª¨ë¦¬ ì •ë³´
    try:
        system_info['memory_gb'] = round(psutil.virtual_memory().total / (1024**3), 1)
    except:
        system_info['memory_gb'] = 16.0
    
    return system_info

# ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€
SYSTEM_INFO = detect_system_info()
IS_CONDA = SYSTEM_INFO['is_conda']
IS_M3_MAX = SYSTEM_INFO['is_m3_max']

print(f"ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
print(f"  ğŸ conda: {'âœ…' if IS_CONDA else 'âŒ'} ({SYSTEM_INFO['conda_env']})")
print(f"  ğŸ M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
print(f"  ğŸ’¾ ë©”ëª¨ë¦¬: {SYSTEM_INFO['memory_gb']}GB")

# =============================================================================
# ğŸ”¥ 3. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
# =============================================================================

try:
    from fastapi import FastAPI, Request, HTTPException, WebSocket, WebSocketDisconnect, BackgroundTasks, Depends
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.middleware.gzip import GZipMiddleware
    from fastapi.responses import JSONResponse, FileResponse, HTMLResponse
    from fastapi.staticfiles import StaticFiles
    from fastapi.templating import Jinja2Templates
    import uvicorn
    
    print("âœ… FastAPI ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì„±ê³µ")
    
except ImportError as e:
    print(f"âŒ FastAPI ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")
    print("ì„¤ì¹˜ ëª…ë ¹: conda install fastapi uvicorn python-multipart websockets")
    sys.exit(1)

# PyTorch ì•ˆì „ import
TORCH_AVAILABLE = False
DEVICE = 'cpu'
try:
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    
    import torch
    TORCH_AVAILABLE = True
    
    # ë””ë°”ì´ìŠ¤ ê°ì§€
    if torch.backends.mps.is_available() and IS_M3_MAX:
        DEVICE = 'mps'
        print("âœ… PyTorch MPS (M3 Max) ì‚¬ìš©")
    elif torch.cuda.is_available():
        DEVICE = 'cuda'
        print("âœ… PyTorch CUDA ì‚¬ìš©")
    else:
        DEVICE = 'cpu'
        print("âœ… PyTorch CPU ì‚¬ìš©")
    
    print("âœ… PyTorch import ì„±ê³µ")
except ImportError:
    print("âš ï¸ PyTorch import ì‹¤íŒ¨")

# =============================================================================
# ğŸ”¥ 4. í•µì‹¬ ëª¨ë“ˆ import (ì•ˆì „í•œ í´ë°±)
# =============================================================================

# Core ì„¤ì • ëª¨ë“ˆ
CONFIG_AVAILABLE = False
try:
    from app.core.config import get_settings, Settings
    from app.core.gpu_config import GPUConfig
    CONFIG_AVAILABLE = True
    print("âœ… Core config ëª¨ë“ˆ import ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ Core config ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    
    # í´ë°± ì„¤ì •
    class Settings:
        APP_NAME = "MyCloset AI"
        DEBUG = True
        HOST = "0.0.0.0"
        PORT = 8000
        CORS_ORIGINS = [
            "http://localhost:3000",
            "http://127.0.0.1:3000",
            "http://localhost:5173",
            "http://127.0.0.1:5173"

        ]
        DEVICE = DEVICE
        USE_GPU = TORCH_AVAILABLE
        IS_M3_MAX = IS_M3_MAX
        IS_CONDA = IS_CONDA
    
    def get_settings():
        return Settings()
    
    class GPUConfig:
        def __init__(self):
            self.device = DEVICE
            self.memory_gb = SYSTEM_INFO['memory_gb']
            self.is_m3_max = IS_M3_MAX

# =============================================================================
# ğŸ”¥ 5. SmartModelPathMapper ìš°ì„  ì´ˆê¸°í™” (ì›Œë‹ í•´ê²°!)
# =============================================================================

SMART_MAPPER_AVAILABLE = False
try:
    print("ğŸ”¥ SmartModelPathMapper ìš°ì„  ì´ˆê¸°í™” ì¤‘...")
    from app.ai_pipeline.utils.smart_model_mapper import (
        get_global_smart_mapper, 
        integrate_with_model_loader,
        integrate_with_auto_detector
    )
    
    # ì „ì—­ SmartMapper ì´ˆê¸°í™”
    smart_mapper = get_global_smart_mapper()
    
    # ìºì‹œ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ëª¨ë“  ëª¨ë¸ íƒì§€
    refresh_result = smart_mapper.refresh_cache()
    print(f"âœ… SmartMapper ìºì‹œ ìƒˆë¡œê³ ì¹¨: {refresh_result.get('new_cache_size', 0)}ê°œ ëª¨ë¸ ë°œê²¬")
    
    # í†µê³„ ì¶œë ¥
    stats = smart_mapper.get_mapping_statistics()
    print(f"ğŸ“Š ë§¤í•‘ ì„±ê³µ: {stats['successful_mappings']}ê°œ")
    print(f"ğŸ“ AI ëª¨ë¸ ë£¨íŠ¸: {stats['ai_models_root']}")
    
    SMART_MAPPER_AVAILABLE = True
    print("âœ… SmartModelPathMapper ìš°ì„  ì´ˆê¸°í™” ì™„ë£Œ!")
    
except ImportError as e:
    print(f"âŒ SmartModelPathMapper import ì‹¤íŒ¨: {e}")
    print("ğŸ’¡ create_smart_mapper.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”")
    SMART_MAPPER_AVAILABLE = False
except Exception as e:
    print(f"âŒ SmartModelPathMapper ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    SMART_MAPPER_AVAILABLE = False

# =============================================================================
# ğŸ”¥ 6. ëª¨ë“  API ë¼ìš°í„°ë“¤ import (ì™„ì „í•œ ì—°ë™)
# =============================================================================

# ModelLoader ì´ˆê¸°í™” í•¨ìˆ˜ import 
try:
    from app.ai_pipeline.utils.model_loader import initialize_global_model_loader
    MODEL_LOADER_INIT_AVAILABLE = True
    print("âœ… ModelLoader ì´ˆê¸°í™” í•¨ìˆ˜ import ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ ModelLoader ì´ˆê¸°í™” í•¨ìˆ˜ import ì‹¤íŒ¨: {e}")
    MODEL_LOADER_INIT_AVAILABLE = False
    
    def initialize_global_model_loader(**kwargs):
        return False
    
ROUTERS_AVAILABLE = {}

ROUTERS_AVAILABLE = {
    'pipeline': None,
    'step': None, 
    'health': None,
    'models': None,
    'websocket': None
}

# 1. Pipeline Routes (í†µí•© íŒŒì´í”„ë¼ì¸ API)
try:
    from app.api.pipeline_routes import router as pipeline_router
    ROUTERS_AVAILABLE['pipeline'] = pipeline_router
    print("âœ… Pipeline Router import ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ Pipeline Router import ì‹¤íŒ¨: {e}")
    ROUTERS_AVAILABLE['pipeline'] = None

# 2. Step Routes (8ë‹¨ê³„ ê°œë³„ API) - ğŸ”¥ í•µì‹¬!
# Step Router ê°•ì œ ë“±ë¡ (main.pyì— ì¶”ê°€)
if ROUTERS_AVAILABLE['step'] is None:
    # í´ë°±: ì§ì ‘ step_routesë¥¼ ìƒì„±
    from fastapi import APIRouter
    fallback_step_router = APIRouter(prefix="/api/step", tags=["Step API"])
    
    @fallback_step_router.post("/1/upload-validation")
    async def fallback_step_1():
        return {"success": True, "message": "Step 1 í´ë°± êµ¬í˜„"}
    
    ROUTERS_AVAILABLE['step'] = fallback_step_router



# 3. Health Routes (í—¬ìŠ¤ì²´í¬ API)
try:
    from app.api.health import router as health_router
    ROUTERS_AVAILABLE['health'] = health_router
    print("âœ… Health Router import ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ Health Router import ì‹¤íŒ¨: {e}")
    ROUTERS_AVAILABLE['health'] = None

# 4. Models Routes (ëª¨ë¸ ê´€ë¦¬ API)
try:
    from app.api.models import router as models_router
    ROUTERS_AVAILABLE['models'] = models_router
    print("âœ… Models Router import ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ Models Router import ì‹¤íŒ¨: {e}")
    ROUTERS_AVAILABLE['models'] = None

# 5. WebSocket Routes (ì‹¤ì‹œê°„ í†µì‹  API) - ğŸ”¥ í•µì‹¬!
try:
    from app.api.websocket_routes import router as websocket_router
    ROUTERS_AVAILABLE['websocket'] = websocket_router
    print("âœ… WebSocket Router import ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ WebSocket Router import ì‹¤íŒ¨: {e}")
    ROUTERS_AVAILABLE['websocket'] = None

# =============================================================================
# ğŸ”¥ 7. ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸ Components Import
# =============================================================================

# ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸ ìƒíƒœ
AI_PIPELINE_AVAILABLE = {}

# RealAIDIContainer (ì‹¤ì œ DI Container)
try:
    from app.core.di_container import DIContainer as RealAIDIContainer, get_di_container as get_global_container
    AI_PIPELINE_AVAILABLE['di_container'] = True
    print("âœ… ì‹¤ì œ AI DI Container ì—°ë™ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ ì‹¤ì œ AI DI Container import ì‹¤íŒ¨: {e}")
    AI_PIPELINE_AVAILABLE['di_container'] = False

# ModelLoader (ì‹¤ì œ êµ¬í˜„)
try:
    from app.ai_pipeline.utils.model_loader import ModelLoader, get_global_model_loader
    AI_PIPELINE_AVAILABLE['model_loader'] = True
    print("âœ… ì‹¤ì œ ModelLoader ì—°ë™ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ ModelLoader import ì‹¤íŒ¨: {e}")
    AI_PIPELINE_AVAILABLE['model_loader'] = False

# StepFactory (ì˜ì¡´ì„± ì£¼ì…)
try:
    from app.ai_pipeline.factories.step_factory import StepFactory, get_global_step_factory
    AI_PIPELINE_AVAILABLE['step_factory'] = True
    print("âœ… ì‹¤ì œ StepFactory ì—°ë™ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ StepFactory import ì‹¤íŒ¨: {e}")
    AI_PIPELINE_AVAILABLE['step_factory'] = False

# PipelineManager (ì‹¤ì œ AI í†µí•©)
try:
    from app.ai_pipeline.pipeline_manager import PipelineManager, get_global_pipeline_manager
    AI_PIPELINE_AVAILABLE['pipeline_manager'] = True
    print("âœ… ì‹¤ì œ PipelineManager ì—°ë™ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ PipelineManager import ì‹¤íŒ¨: {e}")
    AI_PIPELINE_AVAILABLE['pipeline_manager'] = False

# ì„œë¹„ìŠ¤ ë ˆì´ì–´ import
SERVICES_AVAILABLE = {}

# Pipeline Service
try:
    from app.services.pipeline_service import (
        get_pipeline_service_manager,
        cleanup_pipeline_service_manager
    )
    SERVICES_AVAILABLE['pipeline'] = True
    print("âœ… Pipeline Service import ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ Pipeline Service import ì‹¤íŒ¨: {e}")
    SERVICES_AVAILABLE['pipeline'] = False

# Step Service
try:
    from app.services.step_service import (
        get_step_service_manager_async,
        cleanup_step_service_manager
    )
    SERVICES_AVAILABLE['step'] = True
    print("âœ… Step Service import ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ Step Service import ì‹¤íŒ¨: {e}")
    SERVICES_AVAILABLE['step'] = False

# =============================================================================
# ğŸ”¥ 8. ì›Œë‹ í•´ê²° í•¨ìˆ˜ë“¤ (SmartMapper ì—°ë™)
# =============================================================================

def fix_model_loader_warnings():
    """ModelLoader ì›Œë‹ ì™„ì „ í•´ê²°"""
    try:
        print("ğŸ”§ ModelLoader ì›Œë‹ í•´ê²° ì‹œì‘...")
        
        # SmartMapper ì—°ë™
        if SMART_MAPPER_AVAILABLE:
            print("ğŸ”¥ SmartMapperì™€ ModelLoader ì—°ë™ ì¤‘...")
            
            # ModelLoader ì—°ë™
            success1 = integrate_with_model_loader()
            print(f"ğŸ“Š ModelLoader ì—°ë™: {'âœ…' if success1 else 'âŒ'}")
            
            # AutoDetector ì—°ë™
            success2 = integrate_with_auto_detector()
            print(f"ğŸ“Š AutoDetector ì—°ë™: {'âœ…' if success2 else 'âŒ'}")
            
            if success1 or success2:
                print("âœ… SmartMapper ì—°ë™ ì„±ê³µ - ì›Œë‹ í•´ê²° ì™„ë£Œ!")
                return True
        
        # í´ë°±: ì§ì ‘ íŒ¨ì¹˜ ì ìš©
        if AI_PIPELINE_AVAILABLE['model_loader']:
            try:
                from app.ai_pipeline.utils.model_loader import get_global_model_loader
                model_loader = get_global_model_loader()
                
                if model_loader and hasattr(model_loader, '_available_models_cache'):
                    print("ğŸ”„ ì§ì ‘ íŒ¨ì¹˜ ì ìš© ì¤‘...")
                    
                    # ëˆ„ë½ëœ ëª¨ë¸ë“¤ ì¶”ê°€
                    missing_models = {
                        "realvis_xl": {
                            "name": "realvis_xl",
                            "path": "ai_models/step_05_cloth_warping/RealVisXL_V4.0.safetensors",
                            "checkpoint_path": "ai_models/step_05_cloth_warping/RealVisXL_V4.0.safetensors",
                            "ai_model_info": {"ai_class": "RealVisXLModel"},
                            "size_mb": 6616.6
                        },
                        "post_processing_model": {
                            "name": "post_processing_model",
                            "path": "ai_models/checkpoints/step_07_post_processing/GFPGAN.pth",
                            "checkpoint_path": "ai_models/checkpoints/step_07_post_processing/GFPGAN.pth", 
                            "ai_model_info": {"ai_class": "RealGFPGANModel"},
                            "size_mb": 332.5
                        },
                        "gmm": {
                            "name": "gmm",
                            "path": "ai_models/step_04_geometric_matching/gmm_final.pth",
                            "checkpoint_path": "ai_models/step_04_geometric_matching/gmm_final.pth",
                            "ai_model_info": {"ai_class": "RealGMMModel"},
                            "size_mb": 44.7
                        }
                    }
                    
                    added_count = 0
                    for model_name, model_info in missing_models.items():
                        if model_name not in model_loader._available_models_cache:
                            model_loader._available_models_cache[model_name] = model_info
                            added_count += 1
                    
                    print(f"âœ… ì§ì ‘ íŒ¨ì¹˜: {added_count}ê°œ ëª¨ë¸ ì¶”ê°€")
                    return added_count > 0
                    
            except Exception as e:
                print(f"âŒ ì§ì ‘ íŒ¨ì¹˜ ì‹¤íŒ¨: {e}")
        
        return False
        
    except Exception as e:
        print(f"âŒ ì›Œë‹ í•´ê²° ì‹¤íŒ¨: {e}")
        return False

def verify_warnings_fixed():
    """ì›Œë‹ í•´ê²° ê²€ì¦"""
    try:
        print("ğŸ§ª ì›Œë‹ í•´ê²° ê²€ì¦ ì¤‘...")
        
        test_results = {}
        
        # ModelLoader ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸ í™•ì¸
        if AI_PIPELINE_AVAILABLE['model_loader']:
            try:
                from app.ai_pipeline.utils.model_loader import get_global_model_loader
                model_loader = get_global_model_loader()
                
                if model_loader:
                    available_count = len(getattr(model_loader, '_available_models_cache', {}))
                    test_results['model_loader_models'] = available_count
                    print(f"ğŸ“Š ModelLoader ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {available_count}ê°œ")
                
            except Exception as e:
                print(f"âŒ ModelLoader ê²€ì¦ ì‹¤íŒ¨: {e}")
        
        # AutoDetector íƒì§€ ëª¨ë¸ í™•ì¸
        try:
            from app.ai_pipeline.utils.auto_model_detector import get_global_detector
            detector = get_global_detector()
            
            if detector:
                detected_count = len(getattr(detector, 'detected_models', {}))
                test_results['auto_detector_models'] = detected_count
                print(f"ğŸ“Š AutoDetector íƒì§€ ëª¨ë¸: {detected_count}ê°œ")
                
        except Exception as e:
            print(f"âŒ AutoDetector ê²€ì¦ ì‹¤íŒ¨: {e}")
        
        # SmartMapper ë§¤í•‘ í™•ì¸
        if SMART_MAPPER_AVAILABLE:
            try:
                smart_mapper = get_global_smart_mapper()
                stats = smart_mapper.get_mapping_statistics()
                test_results['smart_mapper_mappings'] = stats['successful_mappings']
                print(f"ğŸ“Š SmartMapper ë§¤í•‘: {stats['successful_mappings']}ê°œ")
                
            except Exception as e:
                print(f"âŒ SmartMapper ê²€ì¦ ì‹¤íŒ¨: {e}")
        
        total_models = sum(test_results.values())
        print(f"ğŸ¯ ì´ í•´ê²°ëœ ëª¨ë¸: {total_models}ê°œ")
        
        success = total_models > 0
        print(f"âœ… ì›Œë‹ í•´ê²° ê²€ì¦: {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
        
        return success, test_results
        
    except Exception as e:
        print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False, {}


def apply_model_integration_fix():
    """ìë™ ëª¨ë¸ í†µí•©ìœ¼ë¡œ ì›Œë‹ í•´ê²°"""
    try:
        print("ğŸ”§ ìë™ ëª¨ë¸ í†µí•© ì‹œì‘...")
        
        # auto_model_integration.pyê°€ ìˆëŠ”ì§€ í™•ì¸
        integration_file = Path(__file__).parent / "auto_model_integration.py"
        
        if integration_file.exists():
            # ìƒì„±ëœ í†µí•© í•¨ìˆ˜ ì‚¬ìš©
            try:
                from auto_model_integration import auto_integrate_detected_models
                success = auto_integrate_detected_models()
                if success:
                    print('âœ… ìë™ ëª¨ë¸ í†µí•© ì™„ë£Œ')
                    return True
                else:
                    print('âš ï¸ ìë™ ëª¨ë¸ í†µí•© ì‹¤íŒ¨')
            except Exception as e:
                print(f'âŒ ìë™ í†µí•© ì˜¤ë¥˜: {e}')
        
        # í´ë°±: í•˜ë“œì½”ë”©ëœ ëª¨ë¸ í†µí•©
        print("ğŸ”„ í´ë°± ëª¨ë¸ í†µí•© ì‹¤í–‰...")
        return apply_hardcoded_model_integration()
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ í†µí•© ì‹¤íŒ¨: {e}")
        return False

def apply_hardcoded_model_integration():
    """í•˜ë“œì½”ë”©ëœ ëª¨ë¸ í†µí•© (í´ë°±)"""
    try:
        from app.ai_pipeline.utils.model_loader import get_global_model_loader
        model_loader = get_global_model_loader()
        
        if not model_loader:
            print("âš ï¸ ModelLoader ì—†ìŒ")
            return False
        
        # í•˜ë“œì½”ë”©ëœ ëª¨ë¸ ì •ë³´ (ì‹¤ì œ íƒì§€ ê²°ê³¼ ê¸°ë°˜)
        detected_models = {
            "vgg16_warping": {
                "name": "vgg16_warping",
                "path": str(Path(__file__).parent / "ai_models" / "step_05_cloth_warping" / "ultra_models" / "vgg16_warping_ultra.pth"),
                "checkpoint_path": str(Path(__file__).parent / "ai_models" / "step_05_cloth_warping" / "ultra_models" / "vgg16_warping_ultra.pth"),
                "size_mb": 527.8,
                "ai_model_info": {"ai_class": "RealVGG16Model"},
                "step_class": "ClothWarpingStep",
                "model_type": "warping",
                "loaded": False,
                "device": "mps",
                "torch_compatible": True,
                "priority_score": 527.8
            },
            "vgg19_warping": {
                "name": "vgg19_warping",
                "path": str(Path(__file__).parent / "ai_models" / "step_05_cloth_warping" / "ultra_models" / "vgg19_warping.pth"),
                "checkpoint_path": str(Path(__file__).parent / "ai_models" / "step_05_cloth_warping" / "ultra_models" / "vgg19_warping.pth"),
                "size_mb": 548.1,
                "ai_model_info": {"ai_class": "RealVGG19Model"},
                "step_class": "ClothWarpingStep",
                "model_type": "warping",
                "loaded": False,
                "device": "mps",
                "torch_compatible": True,
                "priority_score": 548.1
            },
            "densenet121": {
                "name": "densenet121",
                "path": str(Path(__file__).parent / "ai_models" / "step_05_cloth_warping" / "ultra_models" / "densenet121_ultra.pth"),
                "checkpoint_path": str(Path(__file__).parent / "ai_models" / "step_05_cloth_warping" / "ultra_models" / "densenet121_ultra.pth"),
                "size_mb": 31.0,
                "ai_model_info": {"ai_class": "RealDenseNetModel"},
                "step_class": "ClothWarpingStep",
                "model_type": "warping",
                "loaded": False,
                "device": "mps",
                "torch_compatible": True,
                "priority_score": 31.0
            }
        }
        
        # ì‹¤ì œ íŒŒì¼ ì¡´ì¬ í™•ì¸ ë° ê²½ë¡œ ìˆ˜ì •
        verified_models = {}
        
        for model_name, model_info in detected_models.items():
            model_path = Path(model_info["path"])
            
            # íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if model_path.exists():
                verified_models[model_name] = model_info
                print(f"  âœ… {model_name}: {model_path.name} ({model_info['size_mb']}MB)")
            else:
                # ëŒ€ì•ˆ ê²½ë¡œë“¤ ì‹œë„
                alternative_paths = [
                    Path(__file__).parent / "ai_models" / f"{model_name}.pth",
                    Path(__file__).parent / "ai_models" / f"{model_name}.pt",
                    Path(__file__).parent / "ai_models" / "checkpoints" / f"{model_name}.pth"
                ]
                
                found = False
                for alt_path in alternative_paths:
                    if alt_path.exists():
                        model_info["path"] = str(alt_path)
                        model_info["checkpoint_path"] = str(alt_path)
                        verified_models[model_name] = model_info
                        print(f"  âœ… {model_name}: {alt_path.name} (ëŒ€ì•ˆ ê²½ë¡œ)")
                        found = True
                        break
                
                if not found:
                    print(f"  âš ï¸ {model_name}: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        if not verified_models:
            print("âŒ ê²€ì¦ëœ ëª¨ë¸ ì—†ìŒ")
            return False
        
        # ModelLoader available_modelsì— ì¶”ê°€
        current_models = getattr(model_loader, '_available_models_cache', {})
        
        # ê¸°ì¡´ ëª¨ë¸ ì •ë³´ ë³´ì¡´í•˜ë©´ì„œ ìƒˆ ëª¨ë¸ ì¶”ê°€
        updated_models = current_models.copy()
        updated_models.update(verified_models)
        
        # ìºì‹œ ì—…ë°ì´íŠ¸
        if hasattr(model_loader, '_available_models_cache'):
            model_loader._available_models_cache = updated_models
            print(f"ğŸ“ _available_models_cache ì—…ë°ì´íŠ¸: {len(updated_models)}ê°œ")
        
        # available_models ì†ì„± ì—…ë°ì´íŠ¸
        try:
            model_loader.available_models = updated_models
            print(f"ğŸ“ available_models ì†ì„± ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ available_models ì†ì„± ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        
        # í†µí•© ì„±ê³µ í”Œë˜ê·¸ ì„¤ì •
        if hasattr(model_loader, '_integration_successful'):
            model_loader._integration_successful = True
            print(f"âœ… _integration_successful = True")
        
        print(f"ğŸ‰ í•˜ë“œì½”ë”© ëª¨ë¸ í†µí•© ì™„ë£Œ: {len(verified_models)}ê°œ")
        return len(verified_models) > 0
        
    except Exception as e:
        print(f"âŒ í•˜ë“œì½”ë”© ëª¨ë¸ í†µí•© ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

# =============================================================================
# ğŸ”¥ 9. ì‹¤ì œ AI Container ì´ˆê¸°í™” (Mock ì œê±° + ì›Œë‹ í•´ê²°)
# =============================================================================

class RealAIContainer:
    """ì‹¤ì œ AI ì»¨í…Œì´ë„ˆ - ëª¨ë“  AI ì»´í¬ë„ŒíŠ¸ë¥¼ ê´€ë¦¬"""
    
    def __init__(self):
        self.device = DEVICE
        self.is_m3_max = IS_M3_MAX
        self.memory_gb = SYSTEM_INFO['memory_gb']
        
        # ì‹¤ì œ AI ì»´í¬ë„ŒíŠ¸ë“¤
        self.di_container = None
        self.model_loader = None
        self.step_factory = None
        self.pipeline_manager = None
        
        # ì´ˆê¸°í™” ìƒíƒœ
        self.is_initialized = False
        self.initialization_time = None
        self.warnings_fixed = False
        
        # í†µê³„
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'models_loaded': 0,
            'steps_processed': 0,
            'average_processing_time': 0.0,
            'warnings_resolved': 0
        }
        
    async def initialize(self):
        """ì‹¤ì œ AI ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™” (ì›Œë‹ í•´ê²° í¬í•¨)"""

        try:
            start_time = time.time()
            
            print("ğŸ¤– ì‹¤ì œ AI ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™” ì‹œì‘...")
            
            # ğŸ”¥ 0. ì›Œë‹ í•´ê²° ìš°ì„  ì‹¤í–‰
            print("ğŸ”§ ì›Œë‹ í•´ê²° ìš°ì„  ì‹¤í–‰...")
            integration_success = apply_model_integration_fix()

            warning_fixed = fix_model_loader_warnings()
            if warning_fixed:
                self.warnings_fixed = True
                print("âœ… ì›Œë‹ í•´ê²° ì™„ë£Œ!")
            else:
                print("âš ï¸ ì¼ë¶€ ì›Œë‹ì´ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            
            # 1. DI Container ì´ˆê¸°í™”
            if AI_PIPELINE_AVAILABLE['di_container']:
                try:
                    self.di_container = get_global_container()
                    print("âœ… ì‹¤ì œ DI Container ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    print(f"âš ï¸ DI Container ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # 2. ModelLoader ì´ˆê¸°í™” (ì›Œë‹ í•´ê²° í›„)
            if AI_PIPELINE_AVAILABLE['model_loader']:
                try:
                    # ğŸ”¥ ì „ì—­ ì´ˆê¸°í™” í•¨ìˆ˜ ë¨¼ì € í˜¸ì¶œ
                    if MODEL_LOADER_INIT_AVAILABLE:
                        success = initialize_global_model_loader(
                            model_cache_dir=Path(path_info['backend_dir']) / 'ai_models',
                            use_fp16=IS_M3_MAX,
                            max_cached_models=16 if IS_M3_MAX else 8,
                            lazy_loading=True,
                            optimization_enabled=True,
                            min_model_size_mb=50,  # ğŸ”¥ 50MB ì´ìƒë§Œ
                            prioritize_large_models=True  # ğŸ”¥ ëŒ€í˜• ëª¨ë¸ ìš°ì„ 
                        )
                        
                        if success:
                            print("âœ… ì „ì—­ ModelLoader ì´ˆê¸°í™” ì„±ê³µ")
                    
                    # ğŸ”¥ ì „ì—­ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                    self.model_loader = get_global_model_loader()
                    if self.model_loader:
                        # ğŸ”¥ ì¶”ê°€ ì´ˆê¸°í™” í™•ì¸
                        if hasattr(self.model_loader, 'initialize') and not getattr(self.model_loader, '_is_initialized', False):
                            success = self.model_loader.initialize()
                            if success:
                                print("âœ… ì‹¤ì œ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
                            else:
                                print("âš ï¸ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨")
                        else:
                            print("âœ… ì‹¤ì œ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
                        
                        # ğŸ”¥ ëª¨ë¸ ê°œìˆ˜ í™•ì¸
                        available_models_count = len(getattr(self.model_loader, '_available_models_cache', {}))
                        self.stats['models_loaded'] = available_models_count
                        print(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models_count}ê°œ")
                        
                        # ğŸ”¥ StepModelInterface ìƒì„±
                        try:
                            self.model_interface = self.model_loader.create_step_interface("HumanParsingStep")
                            if self.model_interface:
                                print("âœ… StepModelInterface ìƒì„± ì™„ë£Œ")
                            else:
                                print("âš ï¸ StepModelInterface ìƒì„± ì‹¤íŒ¨")
                        except Exception as interface_error:
                            print(f"âŒ StepModelInterface ìƒì„± ì‹¤íŒ¨: {interface_error}")
                            self.model_interface = None
                    else:
                        print("âš ï¸ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")

                except Exception as e:
                    print(f"âš ï¸ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    # ğŸ”¥ í´ë°±: ì§ì ‘ ìƒì„±
                    try:
                        from app.ai_pipeline.utils.model_loader import ModelLoader
                        self.model_loader = ModelLoader(
                            device=DEVICE,
                            config={
                                'model_cache_dir': str(Path(path_info['backend_dir']) / 'ai_models'),
                                'use_fp16': IS_M3_MAX,
                                'max_cached_models': 16 if IS_M3_MAX else 8,
                                'lazy_loading': True,
                                'optimization_enabled': True
                            }
                        )
                        
                        if hasattr(self.model_loader, 'initialize'):
                            self.model_loader.initialize()
                        
                        print("âœ… ModelLoader í´ë°± ìƒì„± ì™„ë£Œ")
                    except Exception as fallback_error:
                        print(f"âŒ ModelLoader í´ë°± ìƒì„± ì‹¤íŒ¨: {fallback_error}")
                        
            # 3. StepFactory ì´ˆê¸°í™”
            if AI_PIPELINE_AVAILABLE['step_factory']:
                try:
                    self.step_factory = get_global_step_factory()
                    print("âœ… ì‹¤ì œ StepFactory ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    print(f"âš ï¸ StepFactory ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # 4. PipelineManager ì´ˆê¸°í™”
            if AI_PIPELINE_AVAILABLE['pipeline_manager']:
                try:
                    self.pipeline_manager = get_global_pipeline_manager()
                    if self.pipeline_manager:
                        await self.pipeline_manager.initialize()
                    print("âœ… ì‹¤ì œ PipelineManager ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    print(f"âš ï¸ PipelineManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # 5. ì›Œë‹ í•´ê²° ê²€ì¦
            verification_success, verification_results = verify_warnings_fixed()
            if verification_success:
                self.stats['warnings_resolved'] = sum(verification_results.values())
                print(f"âœ… ì›Œë‹ í•´ê²° ê²€ì¦ ì„±ê³µ: {self.stats['warnings_resolved']}ê°œ ëª¨ë¸")
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            self.initialization_time = time.time() - start_time
            
            print(f"ğŸ‰ ì‹¤ì œ AI ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ! ({self.initialization_time:.2f}ì´ˆ)")
            print(f"ğŸ”¥ ì›Œë‹ í•´ê²°: {'âœ…' if self.warnings_fixed else 'âš ï¸'}")
            return True
            
        except Exception as e:
            print(f"âŒ ì‹¤ì œ AI ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def get_system_status(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ (ì›Œë‹ í•´ê²° í¬í•¨)"""
        available_components = sum(AI_PIPELINE_AVAILABLE.values())
        total_components = len(AI_PIPELINE_AVAILABLE)
        
        return {
            'initialized': self.is_initialized,
            'device': self.device,
            'is_m3_max': self.is_m3_max,
            'memory_gb': self.memory_gb,
            'initialization_time': self.initialization_time,
            'ai_pipeline_active': self.is_initialized,
            'available_components': available_components,
            'total_components': total_components,
            'component_status': AI_PIPELINE_AVAILABLE,
            'real_ai_models_loaded': self.stats['models_loaded'],
            'ai_steps_available': list(range(1, 9)),
            'ai_steps_count': 8,
            'model_loader_available': AI_PIPELINE_AVAILABLE['model_loader'],
            'step_factory_available': AI_PIPELINE_AVAILABLE['step_factory'],
            'pipeline_manager_available': AI_PIPELINE_AVAILABLE['pipeline_manager'],
            'smart_mapper_available': SMART_MAPPER_AVAILABLE,
            'warnings_fixed': self.warnings_fixed,
            'warnings_resolved_count': self.stats['warnings_resolved'],
            'statistics': self.stats
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            print("ğŸ§¹ ì‹¤ì œ AI ì»¨í…Œì´ë„ˆ ì •ë¦¬ ì‹œì‘...")
            
            if self.pipeline_manager:
                await self.pipeline_manager.cleanup()
            
            if self.model_loader:
                await self.model_loader.cleanup()
            
            # M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬
            if IS_M3_MAX and TORCH_AVAILABLE:
                if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
            
            gc.collect()
            print("âœ… ì‹¤ì œ AI ì»¨í…Œì´ë„ˆ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ AI ì»¨í…Œì´ë„ˆ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ì „ì—­ AI ì»¨í…Œì´ë„ˆ ì¸ìŠ¤í„´ìŠ¤
ai_container = RealAIContainer()

# =============================================================================
# ğŸ”¥ 10. ë¡œê¹… ì„¤ì •
# =============================================================================

def setup_logging():
    """ì‹¤ì œ AI ìµœì í™” ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[logging.StreamHandler(sys.stdout)]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

# =============================================================================
# ğŸ”¥ 11. í´ë°± ë¼ìš°í„° ìƒì„± (ëˆ„ë½ëœ ë¼ìš°í„° ëŒ€ì²´)
# =============================================================================

def create_fallback_router(router_name: str):
    """í´ë°± ë¼ìš°í„° ìƒì„±"""
    from fastapi import APIRouter
    
    fallback_router = APIRouter(
        prefix=f"/api/{router_name}",
        tags=[router_name.title()],
        responses={503: {"description": "Service Unavailable"}}
    )
    
    @fallback_router.get("/status")
    async def fallback_status():
        return {
            "status": "fallback",
            "router": router_name,
            "message": f"{router_name} ë¼ìš°í„°ê°€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "timestamp": datetime.now().isoformat(),
            "available_alternatives": [
                "step ë¼ìš°í„°ë¡œ ê°œë³„ ë‹¨ê³„ ì²˜ë¦¬ ê°€ëŠ¥",
                "health ë¼ìš°í„°ë¡œ ìƒíƒœ í™•ì¸ ê°€ëŠ¥"
            ]
        }
    
    return fallback_router

# ëˆ„ë½ëœ ë¼ìš°í„°ë“¤ì„ í´ë°±ìœ¼ë¡œ ëŒ€ì²´
for router_name, router in ROUTERS_AVAILABLE.items():
    if router is None:
        ROUTERS_AVAILABLE[router_name] = create_fallback_router(router_name)
        logger.warning(f"âš ï¸ {router_name} ë¼ìš°í„°ë¥¼ í´ë°±ìœ¼ë¡œ ëŒ€ì²´")

# =============================================================================
# ğŸ”¥ 12. WebSocket ë§¤ë‹ˆì € (ì‹¤ì‹œê°„ AI ì§„í–‰ë¥ )
# =============================================================================

class AIWebSocketManager:
    """AI WebSocket ì—°ê²° ê´€ë¦¬ - ì‹¤ì‹œê°„ AI ì§„í–‰ë¥ """
    
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
        self.session_connections: Dict[str, set] = {}
        self.lock = threading.RLock()
        
    async def connect(self, websocket: WebSocket, session_id: str = None):
        """WebSocket ì—°ê²°"""
        await websocket.accept()
        
        connection_id = session_id or f"conn_{uuid.uuid4().hex[:8]}"
        
        with self.lock:
            self.active_connections[connection_id] = websocket
            
            if session_id:
                if session_id not in self.session_connections:
                    self.session_connections[session_id] = set()
                self.session_connections[session_id].add(websocket)
        
        logger.info(f"ğŸ”Œ AI WebSocket ì—°ê²°: {connection_id}")
        
        # ì—°ê²° í™•ì¸ ë©”ì‹œì§€
        await self.send_message(connection_id, {
            "type": "ai_connection_established",
            "message": "MyCloset AI WebSocket ì—°ê²° ì™„ë£Œ",
            "timestamp": int(time.time()),
            "ai_pipeline_ready": ai_container.is_initialized,
            "device": DEVICE,
            "is_m3_max": IS_M3_MAX,
            "smart_mapper_available": SMART_MAPPER_AVAILABLE,
            "warnings_fixed": ai_container.warnings_fixed
        })
        
        return connection_id
    
    def disconnect(self, connection_id: str):
        """WebSocket ì—°ê²° í•´ì œ"""
        with self.lock:
            if connection_id in self.active_connections:
                websocket = self.active_connections[connection_id]
                del self.active_connections[connection_id]
                
                # ì„¸ì…˜ ì—°ê²°ì—ì„œë„ ì œê±°
                for session_id, connections in self.session_connections.items():
                    if websocket in connections:
                        connections.discard(websocket)
                        if not connections:
                            del self.session_connections[session_id]
                        break
                
                logger.info(f"ğŸ”Œ AI WebSocket ì—°ê²° í•´ì œ: {connection_id}")
    
    async def send_message(self, connection_id: str, message: Dict[str, Any]):
        """ë©”ì‹œì§€ ì „ì†¡"""
        with self.lock:
            if connection_id in self.active_connections:
                try:
                    websocket = self.active_connections[connection_id]
                    await websocket.send_text(json.dumps(message))
                except Exception as e:
                    logger.warning(f"âš ï¸ AI WebSocket ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {e}")
                    self.disconnect(connection_id)
    
    async def broadcast_ai_progress(self, session_id: str, step: int, progress: float, message: str):
        """AI ì§„í–‰ë¥  ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        progress_message = {
            "type": "ai_progress",
            "session_id": session_id,
            "step": step,
            "progress": progress,
            "message": message,
            "timestamp": int(time.time()),
            "device": DEVICE,
            "warnings_status": "resolved" if ai_container.warnings_fixed else "pending"
        }
        
        # í•´ë‹¹ ì„¸ì…˜ì˜ ëª¨ë“  ì—°ê²°ì— ë¸Œë¡œë“œìºìŠ¤íŠ¸
        if session_id in self.session_connections:
            disconnected = []
            for websocket in self.session_connections[session_id]:
                try:
                    await websocket.send_text(json.dumps(progress_message))
                except Exception as e:
                    logger.warning(f"âš ï¸ AI ì§„í–‰ë¥  ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                    disconnected.append(websocket)
            
            # ëŠì–´ì§„ ì—°ê²° ì •ë¦¬
            for websocket in disconnected:
                self.session_connections[session_id].discard(websocket)
    
    async def broadcast_system_status(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        status_message = {
            "type": "ai_system_status",
            "message": "AI ì‹œìŠ¤í…œ ìƒíƒœ ì—…ë°ì´íŠ¸",
            "timestamp": int(time.time()),
            "ai_container_status": ai_container.get_system_status(),
            "routers_available": {k: v is not None for k, v in ROUTERS_AVAILABLE.items()},
            "device": DEVICE,
            "is_m3_max": IS_M3_MAX,
            "smart_mapper_status": SMART_MAPPER_AVAILABLE,
            "warnings_resolved": ai_container.warnings_fixed
        }
        
        # ëª¨ë“  ì—°ê²°ì— ë¸Œë¡œë“œìºìŠ¤íŠ¸
        disconnected = []
        for connection_id, websocket in list(self.active_connections.items()):
            try:
                await websocket.send_text(json.dumps(status_message))
            except Exception as e:
                logger.warning(f"âš ï¸ ì‹œìŠ¤í…œ ìƒíƒœ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                disconnected.append(connection_id)
        
        # ëŠì–´ì§„ ì—°ê²° ì •ë¦¬
        for connection_id in disconnected:
            self.disconnect(connection_id)

# ì „ì—­ AI WebSocket ë§¤ë‹ˆì €
ai_websocket_manager = AIWebSocketManager()

# =============================================================================
# ğŸ”¥ 13. ì•± ë¼ì´í”„ìŠ¤íŒ¬ (ëª¨ë“  ì»´í¬ë„ŒíŠ¸ í†µí•© ì´ˆê¸°í™”)
# =============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ë¼ì´í”„ìŠ¤íŒ¬ - ëª¨ë“  ì»´í¬ë„ŒíŠ¸ í†µí•© ì´ˆê¸°í™” (ì›Œë‹ í•´ê²° í¬í•¨)"""
    try:
        logger.info("ğŸš€ MyCloset AI ì„œë²„ ì‹œì‘ (ëª¨ë“  ë¼ìš°í„° + ì‹¤ì œ AI v21.0 + ì›Œë‹ í•´ê²°)")
        
        # 1. ì‹¤ì œ AI ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™” (ì›Œë‹ í•´ê²° í¬í•¨)
        await ai_container.initialize()
        
        # 2. ì„œë¹„ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        service_managers = {}
        
        # Pipeline Service ì´ˆê¸°í™”
        if SERVICES_AVAILABLE['pipeline']:
            try:
                pipeline_manager = await get_pipeline_service_manager()
                service_managers['pipeline'] = pipeline_manager
                logger.info("âœ… Pipeline Service Manager ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ Pipeline Service Manager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Step Service ì´ˆê¸°í™”
        if SERVICES_AVAILABLE['step']:
            try:
                step_manager = await get_step_service_manager_async()
                service_managers['step'] = step_manager
                logger.info("âœ… Step Service Manager ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ Step Service Manager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # 3. ì£¼ê¸°ì  ì‘ì—… ì‹œì‘
        cleanup_task = asyncio.create_task(periodic_cleanup())
        status_task = asyncio.create_task(periodic_ai_status_broadcast())
        
        logger.info(f"âœ… {len(service_managers)}ê°œ ì„œë¹„ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"âœ… {sum(1 for v in ROUTERS_AVAILABLE.values() if v is not None)}ê°œ ë¼ìš°í„° ì¤€ë¹„ ì™„ë£Œ")
        logger.info(f"ğŸ¤– ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸: {'í™œì„±í™”' if ai_container.is_initialized else 'ë¹„í™œì„±í™”'}")
        logger.info(f"ğŸ”¥ ì›Œë‹ í•´ê²°: {'âœ…' if ai_container.warnings_fixed else 'âš ï¸'}")
        
        yield  # ì•± ì‹¤í–‰
        
    except Exception as e:
        logger.error(f"âŒ ë¼ì´í”„ìŠ¤íŒ¬ ì‹œì‘ ì˜¤ë¥˜: {e}")
        yield
    finally:
        logger.info("ğŸ”š MyCloset AI ì„œë²„ ì¢…ë£Œ ì¤‘...")
        
        # ì •ë¦¬ ì‘ì—…
        try:
            cleanup_task.cancel()
            status_task.cancel()
            
            # ì‹¤ì œ AI ì»¨í…Œì´ë„ˆ ì •ë¦¬
            await ai_container.cleanup()
            
            # ì„œë¹„ìŠ¤ ë§¤ë‹ˆì €ë“¤ ì •ë¦¬
            if SERVICES_AVAILABLE['pipeline']:
                await cleanup_pipeline_service_manager()
            
            if SERVICES_AVAILABLE['step']:
                await cleanup_step_service_manager()
            
            gc.collect()
            
            # M3 Max MPS ìºì‹œ ì •ë¦¬
            if IS_M3_MAX and TORCH_AVAILABLE:
                if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
            
            logger.info("âœ… ì •ë¦¬ ì‘ì—… ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨: {e}")

async def periodic_cleanup():
    """ì£¼ê¸°ì  ì •ë¦¬ ì‘ì—…"""
    while True:
        try:
            await asyncio.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤
            gc.collect()
            
            # M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬
            if IS_M3_MAX and TORCH_AVAILABLE:
                if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                        
            logger.info("ğŸ§¹ ì£¼ê¸°ì  ì •ë¦¬ ì‘ì—… ì™„ë£Œ")
        except asyncio.CancelledError:
            break
        except Exception as e:
            logger.error(f"âŒ ì£¼ê¸°ì  ì •ë¦¬ ì‹¤íŒ¨: {e}")

async def periodic_ai_status_broadcast():
    """ì£¼ê¸°ì  AI ìƒíƒœ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""  
    while True:
        try:
            await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤
            await ai_websocket_manager.broadcast_system_status()
        except asyncio.CancelledError:
            break
        except Exception as e:
            logger.error(f"âŒ AI ìƒíƒœ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 14. FastAPI ì•± ìƒì„± (ëª¨ë“  ë¼ìš°í„° í†µí•©)
# =============================================================================

# ì„¤ì • ë¡œë“œ
settings = get_settings()

app = FastAPI(
    title="MyCloset AI Backend - ëª¨ë“  ë¼ìš°í„° + ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸ + ì›Œë‹ í•´ê²°",
    description="ì™„ì „í•œ ëª¨ë“  ë¼ìš°í„° í†µí•© + ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸ + í”„ë¡ íŠ¸ì—”ë“œ ì™„ë²½ í˜¸í™˜ + SmartMapper ì›Œë‹ í•´ê²°",
    version="21.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œ ì™„ì „ í˜¸í™˜)
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["*"]
)

# ì••ì¶• ë¯¸ë“¤ì›¨ì–´
app.add_middleware(GZipMiddleware, minimum_size=1000)

# ì •ì  íŒŒì¼ ì„¤ì •
try:
    static_dir = Path(path_info['backend_dir']) / "static"
    static_dir.mkdir(exist_ok=True)
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")
    logger.info(f"âœ… ì •ì  íŒŒì¼ ì„¤ì •: {static_dir}")
except Exception as e:
    logger.warning(f"âš ï¸ ì •ì  íŒŒì¼ ì„¤ì • ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ 15. ëª¨ë“  ë¼ìš°í„° ë“±ë¡ (ì™„ì „í•œ í†µí•©)
# =============================================================================

# ğŸ”¥ í•µì‹¬ ë¼ìš°í„°ë“¤ ë“±ë¡ (ìˆœì„œ ì¤‘ìš”!)

# 1. Step Router (8ë‹¨ê³„ ê°œë³„ API) - ğŸ”¥ ê°€ì¥ ì¤‘ìš”!
if ROUTERS_AVAILABLE['step']:
    app.include_router(ROUTERS_AVAILABLE['step'], prefix="/api/step", tags=["8ë‹¨ê³„ API"])
    logger.info("âœ… Step Router ë“±ë¡ - 8ë‹¨ê³„ ê°œë³„ API í™œì„±í™”")

# 2. Pipeline Router (í†µí•© íŒŒì´í”„ë¼ì¸ API)
if ROUTERS_AVAILABLE['pipeline']:
    app.include_router(ROUTERS_AVAILABLE['pipeline'], tags=["í†µí•© íŒŒì´í”„ë¼ì¸ API"])
    logger.info("âœ… Pipeline Router ë“±ë¡ - í†µí•© íŒŒì´í”„ë¼ì¸ API í™œì„±í™”")

# 3. WebSocket Router (ì‹¤ì‹œê°„ í†µì‹ ) - ğŸ”¥ ì¤‘ìš”!
if ROUTERS_AVAILABLE['websocket']:
    app.include_router(ROUTERS_AVAILABLE['websocket'], tags=["WebSocket ì‹¤ì‹œê°„ í†µì‹ "])
    logger.info("âœ… WebSocket Router ë“±ë¡ - ì‹¤ì‹œê°„ AI ì§„í–‰ë¥  í™œì„±í™”")

# 4. Health Router (í—¬ìŠ¤ì²´í¬)
if ROUTERS_AVAILABLE['health']:
    app.include_router(ROUTERS_AVAILABLE['health'], tags=["í—¬ìŠ¤ì²´í¬"])
    logger.info("âœ… Health Router ë“±ë¡ - ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§ í™œì„±í™”")

# 5. Models Router (ëª¨ë¸ ê´€ë¦¬)
if ROUTERS_AVAILABLE['models']:
    app.include_router(ROUTERS_AVAILABLE['models'], tags=["ëª¨ë¸ ê´€ë¦¬"])
    logger.info("âœ… Models Router ë“±ë¡ - AI ëª¨ë¸ ê´€ë¦¬ í™œì„±í™”")

# =============================================================================
# ğŸ”¥ 16. ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ + ì›Œë‹ í•´ê²° ìƒíƒœ)
# =============================================================================

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - ëª¨ë“  ë¼ìš°í„° + ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸ + ì›Œë‹ í•´ê²° ì •ë³´"""
    active_routers = sum(1 for v in ROUTERS_AVAILABLE.values() if v is not None)
    ai_status = ai_container.get_system_status()
    
    return {
        "message": "MyCloset AI Server v21.0 - ëª¨ë“  ë¼ìš°í„° + ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸ + ì›Œë‹ í•´ê²°",
        "status": "running",
        "version": "21.0.0",
        "architecture": "ì™„ì „í•œ ëª¨ë“  ë¼ìš°í„° í†µí•© + ì‹¤ì œ AI + SmartMapper ì›Œë‹ í•´ê²°",
        "features": [
            "ëª¨ë“  API ë¼ìš°í„° ì™„ì „ í†µí•© (5ê°œ)",
            "8ë‹¨ê³„ ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸",
            "SmartModelPathMapper ì›Œë‹ í•´ê²°",
            "WebSocket ì‹¤ì‹œê°„ AI ì§„í–‰ë¥ ",
            "ì„¸ì…˜ ê¸°ë°˜ ì´ë¯¸ì§€ ê´€ë¦¬",
            "conda í™˜ê²½ + M3 Max ìµœì í™”",
            "React/TypeScript ì™„ì „ í˜¸í™˜"
        ],
        "system": {
            "conda_environment": IS_CONDA,
            "conda_env": SYSTEM_INFO['conda_env'],
            "m3_max": IS_M3_MAX,
            "device": DEVICE,
            "memory_gb": SYSTEM_INFO['memory_gb']
        },
        "routers": {
            "total_routers": len(ROUTERS_AVAILABLE),
            "active_routers": active_routers,
            "routers_status": {k: v is not None for k, v in ROUTERS_AVAILABLE.items()}
        },
        "ai_pipeline": {
            "initialized": ai_status['initialized'],
            "models_loaded": ai_status['real_ai_models_loaded'],
            "steps_available": ai_status['ai_steps_available'],
            "device": ai_status['device'],
            "real_ai_active": ai_status['ai_pipeline_active'],
            "smart_mapper_available": ai_status['smart_mapper_available'],
            "warnings_fixed": ai_status['warnings_fixed'],
            "warnings_resolved_count": ai_status['warnings_resolved_count']
        },
        "endpoints": {
            "step_api": "/api/step/* (8ë‹¨ê³„ ê°œë³„ API)",
            "pipeline_api": "/api/pipeline/* (í†µí•© íŒŒì´í”„ë¼ì¸ API)",
            "websocket": "/api/ws/* (ì‹¤ì‹œê°„ í†µì‹ )",
            "health": "/api/health/* (í—¬ìŠ¤ì²´í¬)",
            "models": "/api/models/* (ëª¨ë¸ ê´€ë¦¬)",
            "docs": "/docs",
            "system_info": "/api/system/info"
        }
    }

@app.get("/health")
async def health():
    """í—¬ìŠ¤ì²´í¬ - ëª¨ë“  ë¼ìš°í„° + ì‹¤ì œ AI ìƒíƒœ + ì›Œë‹ í•´ê²° ìƒíƒœ"""
    ai_status = ai_container.get_system_status()
    active_routers = sum(1 for v in ROUTERS_AVAILABLE.values() if v is not None)
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "21.0.0",
        "architecture": "ëª¨ë“  ë¼ìš°í„° + ì‹¤ì œ AI + ì›Œë‹ í•´ê²°",
        "uptime": time.time(),
        "system": {
            "conda": IS_CONDA,
            "m3_max": IS_M3_MAX,
            "device": DEVICE,
            "memory_gb": SYSTEM_INFO['memory_gb']
        },
        "routers": {
            "total_routers": len(ROUTERS_AVAILABLE),
            "active_routers": active_routers,
            "success_rate": (active_routers / len(ROUTERS_AVAILABLE)) * 100
        },
        "ai_pipeline": {
            "status": "active" if ai_status['initialized'] else "inactive",
            "components_available": ai_status['available_components'],
            "models_loaded": ai_status['real_ai_models_loaded'],
            "processing_ready": ai_status['ai_pipeline_active'],
            "smart_mapper_status": ai_status['smart_mapper_available'],
            "warnings_status": "resolved" if ai_status['warnings_fixed'] else "pending"
        },
        "websocket": {
            "active_connections": len(ai_websocket_manager.active_connections),
            "session_connections": len(ai_websocket_manager.session_connections)
        }
    }

@app.get("/api/system/info")
async def get_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ - ì™„ì „í•œ ëª¨ë“  ë¼ìš°í„° + AI ìƒíƒœ + ì›Œë‹ í•´ê²° ìƒíƒœ"""
    try:
        ai_status = ai_container.get_system_status()
        
        return {
            "app_name": "MyCloset AI Backend",
            "app_version": "21.0.0",
            "timestamp": int(time.time()),
            "conda_environment": IS_CONDA,
            "conda_env": os.environ.get('CONDA_DEFAULT_ENV', 'none'),
            "m3_max_optimized": IS_M3_MAX,
            "device": DEVICE,
            "memory_gb": SYSTEM_INFO['memory_gb'],
            "all_routers_integrated": True,
            "warnings_resolution_complete": ai_status.get('warnings_fixed', False),
            "system": {
                "platform": platform.system(),
                "python_version": platform.python_version(),
                "cpu_count": os.cpu_count() or 4,
                "conda": IS_CONDA,
                "m3_max": IS_M3_MAX,
                "device": DEVICE
            },
            "routers": {
                "step_router": ROUTERS_AVAILABLE['step'] is not None,
                "pipeline_router": ROUTERS_AVAILABLE['pipeline'] is not None,
                "websocket_router": ROUTERS_AVAILABLE['websocket'] is not None,
                "health_router": ROUTERS_AVAILABLE['health'] is not None,
                "models_router": ROUTERS_AVAILABLE['models'] is not None,
                "total_active": sum(1 for v in ROUTERS_AVAILABLE.values() if v is not None)
            },
            "ai_pipeline": {
                "active": ai_status.get('ai_pipeline_active', False),
                "initialized": ai_status.get('initialized', False),
                "models_loaded": ai_status.get('real_ai_models_loaded', 0),
                "steps_available": ai_status.get('ai_steps_available', []),
                "steps_count": ai_status.get('ai_steps_count', 0),
                "smart_mapper_available": ai_status.get('smart_mapper_available', False),
                "warnings_fixed": ai_status.get('warnings_fixed', False),
                "warnings_resolved_count": ai_status.get('warnings_resolved_count', 0)
            },
            "services": {
                "model_loader_available": ai_status.get('model_loader_available', False),
                "step_factory_available": ai_status.get('step_factory_available', False),
                "pipeline_manager_available": ai_status.get('pipeline_manager_available', False)
            },
            "server": {
                "host": "0.0.0.0",
                "port": 8000,
                "version": "21.0.0",
                "cors_enabled": True,
                "compression_enabled": True,
                "real_ai_pipeline": ai_status.get('ai_pipeline_active', False),
                "warnings_resolved": ai_status.get('warnings_fixed', False)
            }
        }
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return {
            "error": "ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "message": str(e),
            "timestamp": int(time.time()),
            "fallback": True
        }

# =============================================================================
# ğŸ”¥ 17. WebSocket ì—”ë“œí¬ì¸íŠ¸ (ë©”ì¸)
# =============================================================================

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket, session_id: str = None):
    """ë©”ì¸ WebSocket ì—”ë“œí¬ì¸íŠ¸ - ì‹¤ì‹œê°„ AI í†µì‹ """
    if not session_id:
        session_id = f"ws_{int(time.time())}_{uuid.uuid4().hex[:8]}"
    
    connection_id = None
    try:
        connection_id = await ai_websocket_manager.connect(websocket, session_id)
        logger.info(f"ğŸ”Œ ë©”ì¸ WebSocket ì—°ê²° ì„±ê³µ: {session_id}")
        
        while True:
            try:
                data = await websocket.receive_text()
                message = json.loads(data)
                
                # ë©”ì‹œì§€ íƒ€ì…ë³„ ì²˜ë¦¬
                if message.get("type") == "ping":
                    await ai_websocket_manager.send_message(connection_id, {
                        "type": "pong",
                        "message": "WebSocket ì—°ê²° í™•ì¸",
                        "timestamp": int(time.time()),
                        "ai_pipeline_ready": ai_container.is_initialized,
                        "device": DEVICE,
                        "warnings_status": "resolved" if ai_container.warnings_fixed else "pending"
                    })
                
                elif message.get("type") == "get_ai_status":
                    ai_status = ai_container.get_system_status()
                    await ai_websocket_manager.send_message(connection_id, {
                        "type": "ai_status",
                        "message": "AI ì‹œìŠ¤í…œ ìƒíƒœ",
                        "timestamp": int(time.time()),
                        "ai_status": ai_status
                    })
                
                elif message.get("type") == "subscribe_progress":
                    # ì§„í–‰ë¥  êµ¬ë… ìš”ì²­
                    progress_session_id = message.get("session_id", session_id)
                    await ai_websocket_manager.send_message(connection_id, {
                        "type": "progress_subscribed",
                        "session_id": progress_session_id,
                        "message": f"ì„¸ì…˜ {progress_session_id} ì§„í–‰ë¥  êµ¬ë… ì™„ë£Œ",
                        "timestamp": int(time.time()),
                        "warnings_status": "resolved" if ai_container.warnings_fixed else "pending"
                    })
                
            except WebSocketDisconnect:
                break
            except Exception as e:
                logger.error(f"âŒ WebSocket ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                break
    
    except Exception as e:
        logger.error(f"âŒ WebSocket ì—°ê²° ì˜¤ë¥˜: {e}")
    
    finally:
        if connection_id:
            ai_websocket_manager.disconnect(connection_id)
        logger.info(f"ğŸ”Œ ë©”ì¸ WebSocket ì—°ê²° ì¢…ë£Œ: {session_id}")

# =============================================================================
# ğŸ”¥ 18. ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬ê¸°
# =============================================================================

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬ - ëª¨ë“  ë¼ìš°í„° í˜¸í™˜"""
    logger.error(f"âŒ ì „ì—­ ì˜¤ë¥˜: {str(exc)}")
    logger.error(f"âŒ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
    
    return JSONResponse(
        status_code=500,
        content={
            "success": False,
            "error": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "message": "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "detail": str(exc) if settings.DEBUG else None,
            "version": "21.0.0",
            "architecture": "ëª¨ë“  ë¼ìš°í„° + ì‹¤ì œ AI + ì›Œë‹ í•´ê²°",
            "timestamp": datetime.now().isoformat(),
            "ai_pipeline_status": ai_container.is_initialized,
            "warnings_status": "resolved" if ai_container.warnings_fixed else "pending",
            "available_endpoints": [
                "/api/step/* (8ë‹¨ê³„ ê°œë³„ API)",
                "/api/pipeline/* (í†µí•© íŒŒì´í”„ë¼ì¸)",
                "/api/ws/* (WebSocket)",
                "/api/health/* (í—¬ìŠ¤ì²´í¬)",
                "/api/models/* (ëª¨ë¸ ê´€ë¦¬)"
            ]
        }
    )

@app.exception_handler(404)
async def not_found_handler(request: Request, exc):
    """404 ì—ëŸ¬ ì²˜ë¦¬"""
    return JSONResponse(
        status_code=404,
        content={
            "success": False,
            "error": "ìš”ì²­í•œ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "message": f"ê²½ë¡œ '{request.url.path}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "available_endpoints": [
                "/",
                "/health",
                "/api/system/info",
                "/api/step/* (8ë‹¨ê³„ ê°œë³„ API)",
                "/api/pipeline/* (í†µí•© íŒŒì´í”„ë¼ì¸)",
                "/api/ws/* (WebSocket ì‹¤ì‹œê°„ í†µì‹ )",
                "/api/health/* (í—¬ìŠ¤ì²´í¬)",
                "/api/models/* (ëª¨ë¸ ê´€ë¦¬)",
                "/ws (ë©”ì¸ WebSocket)",
                "/docs"
            ],
            "version": "21.0.0",
            "architecture": "ëª¨ë“  ë¼ìš°í„° + ì‹¤ì œ AI + ì›Œë‹ í•´ê²°"
        }
    )

# =============================================================================
# ğŸ”¥ 19. ì„œë²„ ì‹œì‘ (ì™„ì „í•œ ëª¨ë“  ë¼ìš°í„° + ì‹¤ì œ AI + ì›Œë‹ í•´ê²°)
# =============================================================================

if __name__ == "__main__":
    
    # ğŸ”¥ ì„œë²„ ì‹œì‘ ì „ ì›Œë‹ í•´ê²° ìµœì¢… ê²€ì¦
    print("ğŸ”¥ ì„œë²„ ì‹œì‘ ì „ ì›Œë‹ í•´ê²° ìµœì¢… ê²€ì¦...")
    
    try:
        # SmartMapper ìƒíƒœ í™•ì¸
        if SMART_MAPPER_AVAILABLE:
            smart_mapper = get_global_smart_mapper()
            stats = smart_mapper.get_mapping_statistics()
            print(f"âœ… SmartMapper: {stats['successful_mappings']}ê°œ ëª¨ë¸ ë§¤í•‘ ì™„ë£Œ")
        else:
            print("âŒ SmartMapper ì‚¬ìš© ë¶ˆê°€ - create_smart_mapper.py ì‹¤í–‰ í•„ìš”")
        
        # ModelLoader ìƒíƒœ í™•ì¸
        if AI_PIPELINE_AVAILABLE['model_loader']:
            from app.ai_pipeline.utils.model_loader import get_global_model_loader
            loader = get_global_model_loader()
            models_count = len(getattr(loader, '_available_models_cache', {}))
            print(f"âœ… ModelLoader: {models_count}ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
        else:
            print("âŒ ModelLoader ì‚¬ìš© ë¶ˆê°€")
        
        # AutoDetector ìƒíƒœ í™•ì¸
        try:
            from app.ai_pipeline.utils.auto_model_detector import get_global_detector
            detector = get_global_detector()
            if detector:
                detected_count = len(getattr(detector, 'detected_models', {}))
                print(f"âœ… AutoDetector: {detected_count}ê°œ ëª¨ë¸ íƒì§€ë¨")
            else:
                print("âš ï¸ AutoDetector ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ")
        except Exception as e:
            print(f"âš ï¸ AutoDetector í™•ì¸ ì‹¤íŒ¨: {e}")
            
    except Exception as e:
        print(f"âŒ ì›Œë‹ í•´ê²° ê²€ì¦ ì‹¤íŒ¨: {e}")
    
    print("\n" + "="*120)
    print("ğŸ”¥ MyCloset AI ë°±ì—”ë“œ ì„œë²„ - ëª¨ë“  ë¼ìš°í„° + ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸ + ì›Œë‹ í•´ê²° v21.0")
    print("="*120)
    print("ğŸ—ï¸ ì™„ì „í•œ í†µí•© ì•„í‚¤í…ì²˜:")
    print("  âœ… ëª¨ë“  API ë¼ìš°í„° ì™„ì „ í†µí•© (5ê°œ ë¼ìš°í„°)")
    print("  âœ… 8ë‹¨ê³„ ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸ (Mock ì™„ì „ ì œê±°)")
    print("  âœ… SmartModelPathMapper ì›Œë‹ í•´ê²° ì‹œìŠ¤í…œ")
    print("  âœ… WebSocket ì‹¤ì‹œê°„ AI ì§„í–‰ë¥  ì¶”ì ")
    print("  âœ… ì„¸ì…˜ ê¸°ë°˜ ì´ë¯¸ì§€ ê´€ë¦¬ (ì¬ì—…ë¡œë“œ ë°©ì§€)")
    print("  âœ… DI Container ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬")
    print("  âœ… M3 Max 128GB + conda í™˜ê²½ ìµœì í™”")
    print("  âœ… React/TypeScript í”„ë¡ íŠ¸ì—”ë“œ 100% í˜¸í™˜")
    print("="*120)
    print("ğŸš€ ë¼ìš°í„° ìƒíƒœ:")
    for router_name, router in ROUTERS_AVAILABLE.items():
        status = "âœ…" if router is not None else "âš ï¸"
        description = {
            'step': '8ë‹¨ê³„ ê°œë³„ API (í•µì‹¬)',
            'pipeline': 'í†µí•© íŒŒì´í”„ë¼ì¸ API',
            'websocket': 'WebSocket ì‹¤ì‹œê°„ í†µì‹  (í•µì‹¬)',
            'health': 'í—¬ìŠ¤ì²´í¬ API',
            'models': 'ëª¨ë¸ ê´€ë¦¬ API'
        }
        print(f"  {status} {router_name.title()} Router - {description.get(router_name, '')}")
    
    print("="*120)
    print("ğŸ¤– ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸ ìƒíƒœ:")
    for component_name, available in AI_PIPELINE_AVAILABLE.items():
        status = "âœ…" if available else "âš ï¸"
        description = {
            'di_container': 'DI Container (ì˜ì¡´ì„± ì£¼ì…)',
            'model_loader': 'ModelLoader (ì‹¤ì œ AI ëª¨ë¸)',
            'step_factory': 'StepFactory (8ë‹¨ê³„ ìƒì„±)',
            'pipeline_manager': 'PipelineManager (í†µí•© ê´€ë¦¬)'
        }
        print(f"  {status} {component_name.title()} - {description.get(component_name, '')}")
    
    print("="*120)
    print("ğŸ”¥ ì›Œë‹ í•´ê²° ì‹œìŠ¤í…œ:")
    print(f"  {'âœ…' if SMART_MAPPER_AVAILABLE else 'âŒ'} SmartModelPathMapper - ë™ì  ê²½ë¡œ íƒì§€")
    print(f"  ğŸ¯ ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ 229GB ì™„ì „ í™œìš©")
    print(f"  ğŸ”§ GMM, PostProcessing, ClothWarping ì›Œë‹ í•´ê²°")
    print(f"  ğŸ“Š realvis_xl, vgg16_warping, densenet121 ë“± ëˆ„ë½ ëª¨ë¸ í•´ê²°")
    print(f"  âš¡ M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
    
    print("="*120)
    print("ğŸŒ ì„œë²„ ì •ë³´:")
    print(f"  ğŸ“ ì£¼ì†Œ: http://{settings.HOST}:{settings.PORT}")
    print(f"  ğŸ“š API ë¬¸ì„œ: http://{settings.HOST}:{settings.PORT}/docs")
    print(f"  â¤ï¸ í—¬ìŠ¤ì²´í¬: http://{settings.HOST}:{settings.PORT}/health")
    print(f"  ğŸ”Œ WebSocket: ws://{settings.HOST}:{settings.PORT}/ws")
    print(f"  ğŸ conda: {'âœ…' if IS_CONDA else 'âŒ'} ({SYSTEM_INFO['conda_env']})")
    print(f"  ğŸ M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
    print(f"  ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {DEVICE}")
    print(f"  ğŸ’¾ ë©”ëª¨ë¦¬: {SYSTEM_INFO['memory_gb']}GB")
    print("="*120)
    print("ğŸ”— í”„ë¡ íŠ¸ì—”ë“œ ì—°ê²°:")
    active_routers = sum(1 for v in ROUTERS_AVAILABLE.values() if v is not None)
    ai_components = sum(AI_PIPELINE_AVAILABLE.values())
    print(f"  ğŸ“Š í™œì„± ë¼ìš°í„°: {active_routers}/{len(ROUTERS_AVAILABLE)}")
    print(f"  ğŸ¤– AI ì»´í¬ë„ŒíŠ¸: {ai_components}/{len(AI_PIPELINE_AVAILABLE)}")
    print(f"  ğŸ”¥ ì›Œë‹ í•´ê²°: {'âœ…' if SMART_MAPPER_AVAILABLE else 'âŒ'}")
    print(f"  ğŸŒ CORS ì„¤ì •: {len(settings.CORS_ORIGINS)}ê°œ ë„ë©”ì¸")
    print(f"  ğŸ”Œ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ http://{settings.HOST}:{settings.PORT} ìœ¼ë¡œ API í˜¸ì¶œ ê°€ëŠ¥!")
    print("="*120)
    print("ğŸ¯ ì£¼ìš” API ì—”ë“œí¬ì¸íŠ¸:")
    print(f"  ğŸ”¥ 8ë‹¨ê³„ ê°œë³„ API: /api/step/1/upload-validation ~ /api/step/8/result-analysis")
    print(f"  ğŸ”¥ í†µí•© íŒŒì´í”„ë¼ì¸: /api/pipeline/complete")
    print(f"  ğŸ”¥ WebSocket ì‹¤ì‹œê°„: /api/ws/progress/{{session_id}}")
    print(f"  ğŸ“Š í—¬ìŠ¤ì²´í¬: /api/health/status")
    print(f"  ğŸ¤– ëª¨ë¸ ê´€ë¦¬: /api/models/available")
    print("="*120)
    print("ğŸ”¥ ëª¨ë“  ë¼ìš°í„° + ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸ + ì›Œë‹ í•´ê²° ì™„ì„±!")
    print("ğŸ“¦ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ëª¨ë“  APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    print("âœ¨ React/TypeScript App.tsxì™€ 100% í˜¸í™˜!")
    print("ğŸ¤– ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸!")
    print("ğŸ¯ SmartModelPathMapperë¡œ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì›Œë‹ í•´ê²°!")
    print("="*120)
    
    # ì„œë²„ ì‹¤í–‰
    try:
        uvicorn.run(
            app,
            host=settings.HOST,
            port=settings.PORT,
            reload=False,
            log_level="info",
            access_log=True
        )
    except KeyboardInterrupt:
        print("\nâœ… ëª¨ë“  ë¼ìš°í„° + ì‹¤ì œ AI + ì›Œë‹ í•´ê²° ì„œë²„ê°€ ì•ˆì „í•˜ê²Œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì„œë²„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")