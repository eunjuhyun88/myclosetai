"""
MyCloset AI - AI ëª¨ë¸ ë§¤ë‹ˆì €
M3 Max GPUì— ìµœì í™”ëœ AI ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬ ì„œë¹„ìŠ¤
"""

import logging
import torch
from typing import Dict, Optional, Any
from pathlib import Path
import psutil
import gc

logger = logging.getLogger("mycloset.services.model_manager")

class ModelManager:
    """M3 Max ìµœì í™” AI ëª¨ë¸ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        self.loaded_models: Dict[str, Any] = {}
        self.model_configs: Dict[str, Dict] = {}
        self.memory_threshold = 0.85  # 85% ë©”ëª¨ë¦¬ ì‚¬ìš©ì‹œ ê²½ê³ 
        self.available_models: Dict[str, bool] = {}  # ìºì‹œëœ ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸ ëª©ë¡
        
        logger.info(f"ğŸ¤– ModelManager ì´ˆê¸°í™”: device={self.device}")
        self._setup_device_optimization()
    
    def _setup_device_optimization(self):
        """M3 Max ë””ë°”ì´ìŠ¤ ìµœì í™” ì„¤ì •"""
        if self.device == "mps":
            # Metal Performance Shaders ìµœì í™” (ë²„ì „ í˜¸í™˜ì„± ì²´í¬)
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
                logger.info("âœ… MPS ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            else:
                logger.info("â„¹ï¸ MPS empty_cache ë¯¸ì§€ì› (PyTorch 2.5.1)")
            
            # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
            if hasattr(torch.backends.mps, 'set_per_process_memory_fraction'):
                torch.backends.mps.set_per_process_memory_fraction(0.8)
                logger.info("âœ… MPS ë©”ëª¨ë¦¬ í• ë‹¹ ì œí•œ: 80%")
            else:
                logger.info("â„¹ï¸ MPS ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì • ë¯¸ì§€ì›")
    
    def get_available_models(self) -> Dict[str, bool]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            from app.core.model_paths import DETECTED_MODELS, is_model_available
            
            available_models = {}
            for model_key in DETECTED_MODELS.keys():
                available_models[model_key] = is_model_available(model_key)
            
            logger.info(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {sum(available_models.values())}/{len(available_models)}")
            return available_models
            
        except ImportError:
            logger.warning("âš ï¸ ëª¨ë¸ ê²½ë¡œ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {}
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {}
    
    async def load_models(self) -> bool:
        """ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¡œë“œ (ë¹„ë™ê¸°)"""
        logger.info("ğŸ”„ ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
        
        try:
            available_models = self.get_available_models()
            
            if not available_models:
                logger.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # ìš°ì„ ìˆœìœ„ ë†’ì€ ëª¨ë¸ë“¤ë§Œ ìë™ ë¡œë“œ
            priority_models = ["ootdiffusion", "sam", "stable_diffusion"]
            loaded_count = 0
            
            for model_key in priority_models:
                if available_models.get(model_key, False):
                    if self.load_model(model_key):
                        loaded_count += 1
                        logger.info(f"âœ… ìë™ ë¡œë“œ ì™„ë£Œ: {model_key}")
                    else:
                        logger.warning(f"âš ï¸ ìë™ ë¡œë“œ ì‹¤íŒ¨: {model_key}")
            
            logger.info(f"âœ… ëª¨ë¸ ìë™ ë¡œë“œ ì™„ë£Œ: {loaded_count}ê°œ")
            return loaded_count > 0
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    async def unload_models(self) -> bool:
        """ëª¨ë“  ë¡œë“œëœ ëª¨ë¸ ì–¸ë¡œë“œ (ë¹„ë™ê¸°)"""
        logger.info("ğŸ”„ ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ ì‹œì‘...")
        
        try:
            model_keys = list(self.loaded_models.keys())
            unloaded_count = 0
            
            for model_key in model_keys:
                if self.unload_model(model_key):
                    unloaded_count += 1
            
            logger.info(f"âœ… ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {unloaded_count}ê°œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def load_model(self, model_key: str, **kwargs) -> bool:
        """ëª¨ë¸ ë¡œë“œ (M3 Max ìµœì í™”)"""
        logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì‹œì‘: {model_key}")
        
        try:
            # ë©”ëª¨ë¦¬ ì²´í¬
            memory_info = self._check_memory()
            if memory_info["usage_percent"] > self.memory_threshold * 100:
                logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_info['usage_percent']:.1f}%")
                self._cleanup_memory()
            
            # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ í™•ì¸
            if model_key in self.loaded_models:
                logger.info(f"âœ… ëª¨ë¸ ì´ë¯¸ ë¡œë“œë¨: {model_key}")
                return True
            
            # ëª¨ë¸ íƒ€ì…ë³„ ë¡œë”© ì „ëµ
            model_info = self._get_model_info(model_key)
            if not model_info:
                logger.error(f"âŒ ëª¨ë¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_key}")
                return False
            
            # ì‹¤ì œ ëª¨ë¸ ë¡œë”© (í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜)
            success = self._load_model_by_type(model_key, model_info, **kwargs)
            
            if success:
                logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_key}")
                return True
            else:
                logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_key}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜ {model_key}: {e}")
            return False
    
    def unload_model(self, model_key: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        logger.info(f"ğŸ”„ ëª¨ë¸ ì–¸ë¡œë“œ: {model_key}")
        
        try:
            if model_key in self.loaded_models:
                # ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ
                del self.loaded_models[model_key]
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if self.device == "mps":
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
                
                # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                gc.collect()
                
                logger.info(f"âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ: {model_key}")
                return True
            else:
                logger.warning(f"âš ï¸ ë¡œë“œë˜ì§€ ì•Šì€ ëª¨ë¸: {model_key}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì˜¤ë¥˜ {model_key}: {e}")
            return False
    
    def get_model_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        memory_info = self._check_memory()
        
        status = {
            "loaded_models": list(self.loaded_models.keys()),
            "loaded_count": len(self.loaded_models),
            "available_models": self.get_available_models(),
            "memory_info": memory_info,
            "device": self.device,
            "device_available": torch.backends.mps.is_available() if self.device == "mps" else True
        }
        
        return status
    
    def _load_model_by_type(self, model_key: str, model_info: Dict, **kwargs) -> bool:
        """ëª¨ë¸ íƒ€ì…ë³„ ë¡œë”© ë¡œì§"""
        model_type = model_info.get("type", "unknown")
        
        # í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ ëª¨ë¸ ë¡œë”© ë¡œì§ì€ ì¶”í›„ êµ¬í˜„)
        logger.info(f"ğŸ¯ ëª¨ë¸ íƒ€ì…: {model_type}")
        
        if model_type == "virtual_tryon":
            return self._load_virtual_tryon_model(model_key, model_info, **kwargs)
        elif model_type == "segmentation":
            return self._load_segmentation_model(model_key, model_info, **kwargs)
        elif model_type == "base_diffusion":
            return self._load_diffusion_model(model_key, model_info, **kwargs)
        else:
            return self._load_generic_model(model_key, model_info, **kwargs)
    
    def _load_virtual_tryon_model(self, model_key: str, model_info: Dict, **kwargs) -> bool:
        """ê°€ìƒ í”¼íŒ… ëª¨ë¸ ë¡œë”© (ì‹œë®¬ë ˆì´ì…˜)"""
        logger.info(f"ğŸ‘— ê°€ìƒ í”¼íŒ… ëª¨ë¸ ë¡œë”©: {model_key}")
        
        # ì‹œë®¬ë ˆì´ì…˜: ì‹¤ì œë¡œëŠ” OOTDiffusion, VITON ë“±ì„ ë¡œë“œ
        self.loaded_models[model_key] = {
            "type": "virtual_tryon",
            "model": "ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸",
            "device": self.device,
            "memory_usage": "2.1GB",
            "status": "ready"
        }
        
        return True
    
    def _load_segmentation_model(self, model_key: str, model_info: Dict, **kwargs) -> bool:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë”© (ì‹œë®¬ë ˆì´ì…˜)"""
        logger.info(f"âœ‚ï¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë”©: {model_key}")
        
        # ì‹œë®¬ë ˆì´ì…˜: ì‹¤ì œë¡œëŠ” SAM ë“±ì„ ë¡œë“œ
        self.loaded_models[model_key] = {
            "type": "segmentation",
            "model": "ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸",
            "device": self.device,
            "memory_usage": "1.8GB",
            "status": "ready"
        }
        
        return True
    
    def _load_diffusion_model(self, model_key: str, model_info: Dict, **kwargs) -> bool:
        """ë””í“¨ì „ ëª¨ë¸ ë¡œë”© (ì‹œë®¬ë ˆì´ì…˜)"""
        logger.info(f"ğŸ¨ ë””í“¨ì „ ëª¨ë¸ ë¡œë”©: {model_key}")
        
        # ì‹œë®¬ë ˆì´ì…˜: ì‹¤ì œë¡œëŠ” Stable Diffusion ë“±ì„ ë¡œë“œ
        self.loaded_models[model_key] = {
            "type": "base_diffusion",
            "model": "ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸",
            "device": self.device,
            "memory_usage": "3.5GB",
            "status": "ready"
        }
        
        return True
    
    def _load_generic_model(self, model_key: str, model_info: Dict, **kwargs) -> bool:
        """ì¼ë°˜ ëª¨ë¸ ë¡œë”© (ì‹œë®¬ë ˆì´ì…˜)"""
        logger.info(f"ğŸ”§ ì¼ë°˜ ëª¨ë¸ ë¡œë”©: {model_key}")
        
        self.loaded_models[model_key] = {
            "type": model_info.get("type", "generic"),
            "model": "ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸",
            "device": self.device,
            "memory_usage": "1.2GB",
            "status": "ready"
        }
        
        return True
    
    def _get_model_info(self, model_key: str) -> Optional[Dict]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        try:
            from app.core.model_paths import get_model_info
            return get_model_info(model_key)
        except ImportError:
            logger.warning("âš ï¸ ëª¨ë¸ ê²½ë¡œ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def _check_memory(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸"""
        memory = psutil.virtual_memory()
        
        return {
            "total_gb": memory.total / (1024**3),
            "available_gb": memory.available / (1024**3),
            "used_gb": memory.used / (1024**3),
            "usage_percent": memory.percent
        }
    
    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        logger.info("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘...")
        
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.device == "mps":
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
        elif self.device == "cuda":
            torch.cuda.empty_cache()
        
        logger.info("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

# ì „ì—­ ëª¨ë¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
model_manager = ModelManager()

# í¸ì˜ í•¨ìˆ˜ë“¤
def load_model(model_key: str, **kwargs) -> bool:
    """ëª¨ë¸ ë¡œë“œ"""
    return model_manager.load_model(model_key, **kwargs)

def unload_model(model_key: str) -> bool:
    """ëª¨ë¸ ì–¸ë¡œë“œ"""
    return model_manager.unload_model(model_key)

def get_model_status() -> Dict[str, Any]:
    """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
    return model_manager.get_model_status()

def get_available_models() -> Dict[str, bool]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
    return model_manager.get_available_models()