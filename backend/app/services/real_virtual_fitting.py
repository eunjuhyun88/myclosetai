# backend/app/services/real_virtual_fitting.py
"""
ì‹¤ì œ ê°€ìƒ í”¼íŒ… í†µí•© ì„œë¹„ìŠ¤
ì¸ì²´ ë¶„ì„ â†’ 3D ëª¨ë¸ë§ â†’ ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ â†’ ë Œë”ë§
"""

import asyncio
import time
import numpy as np
from PIL import Image, ImageDraw, ImageFilter
import cv2
from typing import Dict, Any, Tuple
import logging

from app.services.human_analysis import human_analyzer
from app.services.clothing_3d_modeling import clothing_3d_modeler

logger = logging.getLogger(__name__)

class RealVirtualFittingService:
    """ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.processing_steps = [
            "ì¸ì²´ ë¶„ì„ ë° ë¶„í• ",
            "3D í¬ì¦ˆ ì¶”ì •", 
            "ì˜ë¥˜ 3D ëª¨ë¸ë§",
            "ì²´í˜•ë³„ ë³€í˜•",
            "ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜",
            "ì¡°ëª… ë° ë Œë”ë§",
            "í›„ì²˜ë¦¬ ë° í•©ì„±"
        ]
        
    async def process_virtual_fitting(
        self,
        person_image: Image.Image,
        clothing_image: Image.Image,
        height: float,
        weight: float,
        clothing_type: str = "shirt",
        quality_level: str = "high"
    ) -> Tuple[Image.Image, Dict[str, Any]]:
        """ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬"""
        
        logger.info("ğŸ¯ ì‹¤ì œ ê°€ìƒ í”¼íŒ… í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        start_time = time.time()
        
        processing_info = {
            "steps_completed": [],
            "processing_times": {},
            "quality_metrics": {},
            "errors": []
        }
        
        try:
            # 1ë‹¨ê³„: ì¸ì²´ ë¶„ì„ ë° ë¶„í• 
            step_start = time.time()
            logger.info("ğŸ‘¤ 1ë‹¨ê³„: ì¸ì²´ ë¶„ì„ ì‹œì‘...")
            
            body_analysis = await human_analyzer.analyze_human_body(person_image)
            
            if not body_analysis['pose_landmarks']['detected']:
                raise ValueError("ì¸ì²´ í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            processing_info["steps_completed"].append("ì¸ì²´ ë¶„ì„")
            processing_info["processing_times"]["human_analysis"] = time.time() - step_start
            
            # 2ë‹¨ê³„: ì˜ë¥˜ 3D ëª¨ë¸ë§
            step_start = time.time()
            logger.info("ğŸ‘• 2ë‹¨ê³„: ì˜ë¥˜ 3D ëª¨ë¸ë§ ì‹œì‘...")
            
            clothing_mesh = await clothing_3d_modeler.create_clothing_mesh(
                clothing_image, clothing_type, "cotton"
            )
            
            processing_info["steps_completed"].append("ì˜ë¥˜ 3D ëª¨ë¸ë§")
            processing_info["processing_times"]["clothing_modeling"] = time.time() - step_start
            
            # 3ë‹¨ê³„: ì²´í˜•ë³„ ì˜ë¥˜ ë³€í˜•
            step_start = time.time()
            logger.info("ğŸ”§ 3ë‹¨ê³„: ì²´í˜•ë³„ ì˜ë¥˜ ë³€í˜• ì‹œì‘...")
            
            fitted_clothing = await clothing_3d_modeler.fit_clothing_to_body(
                clothing_mesh,
                body_analysis,
                body_analysis['body_measurements']
            )
            
            processing_info["steps_completed"].append("ì²´í˜•ë³„ ë³€í˜•")
            processing_info["processing_times"]["body_fitting"] = time.time() - step_start
            
            # 4ë‹¨ê³„: ê³ í’ˆì§ˆ ë Œë”ë§ ë° í•©ì„±
            step_start = time.time()
            logger.info("ğŸ¨ 4ë‹¨ê³„: ê³ í’ˆì§ˆ ë Œë”ë§ ì‹œì‘...")
            
            result_image = await self._render_and_composite(
                person_image,
                fitted_clothing,
                body_analysis,
                quality_level
            )
            
            processing_info["steps_completed"].append("ë Œë”ë§ ë° í•©ì„±")
            processing_info["processing_times"]["rendering"] = time.time() - step_start
            
            # 5ë‹¨ê³„: í’ˆì§ˆ ë¶„ì„ ë° í›„ì²˜ë¦¬
            step_start = time.time()
            logger.info("âœ¨ 5ë‹¨ê³„: í’ˆì§ˆ ë¶„ì„ ë° í›„ì²˜ë¦¬...")
            
            final_image, quality_metrics = await self._post_process_and_analyze(
                result_image, body_analysis, fitted_clothing
            )
            
            processing_info["quality_metrics"] = quality_metrics
            processing_info["processing_times"]["post_processing"] = time.time() - step_start
            
            # ì „ì²´ ì²˜ë¦¬ ì‹œê°„
            total_time = time.time() - start_time
            processing_info["total_processing_time"] = total_time
            processing_info["success"] = True
            
            logger.info(f"âœ… ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì™„ë£Œ ({total_time:.2f}ì´ˆ)")
            
            return final_image, processing_info
            
        except Exception as e:
            error_msg = f"ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            
            processing_info["errors"].append(error_msg)
            processing_info["success"] = False
            
            # ì—ëŸ¬ ë°œìƒì‹œ ê¸°ë³¸ í•©ì„± ì´ë¯¸ì§€ ë°˜í™˜
            fallback_image = await self._create_fallback_result(person_image, clothing_image)
            
            return fallback_image, processing_info
    
    async def _render_and_composite(
        self,
        person_image: Image.Image,
        fitted_clothing: Dict[str, Any],
        body_analysis: Dict[str, Any],
        quality_level: str
    ) -> Image.Image:
        """ê³ í’ˆì§ˆ ë Œë”ë§ ë° í•©ì„±"""
        
        # ë² ì´ìŠ¤ ì´ë¯¸ì§€
        result = person_image.copy()
        width, height = result.size
        
        # 1. ì˜ë¥˜ ì˜ì—­ ë Œë”ë§
        clothing_region = self._render_3d_clothing(
            fitted_clothing['fitted_mesh'],
            fitted_clothing['adjusted_texture'],
            width, height
        )
        
        # 2. ì¡°ëª… ë¶„ì„ ë° ì ìš©
        lighting_info = self._analyze_lighting(person_image, body_analysis)
        lit_clothing = self._apply_realistic_lighting(clothing_region, lighting_info)
        
        # 3. ê·¸ë¦¼ì ìƒì„±
        shadow_map = self._generate_realistic_shadows(
            fitted_clothing['fitted_mesh'],
            lighting_info,
            width, height
        )
        
        # 4. ì˜¤í´ë£¨ì „ ì²˜ë¦¬ (ê°€ë ¤ì§€ëŠ” ë¶€ë¶„)
        occlusion_mask = self._calculate_occlusion(
            body_analysis['clothing_regions'],
            fitted_clothing['fitted_mesh']
        )
        
        # 5. ê³ ê¸‰ ë¸”ë Œë”©
        composited = self._advanced_composite(
            result, lit_clothing, shadow_map, occlusion_mask
        )
        
        # 6. í’ˆì§ˆ ë ˆë²¨ì— ë”°ë¥¸ ì¶”ê°€ ì²˜ë¦¬
        if quality_level == "high":
            composited = self._apply_high_quality_effects(composited)
        elif quality_level == "ultra":
            composited = self._apply_ultra_quality_effects(composited)
        
        return composited
    
    def _render_3d_clothing(
        self, 
        mesh: Dict[str, np.ndarray], 
        texture: np.ndarray,
        width: int, 
        height: int
    ) -> Image.Image:
        """3D ì˜ë¥˜ ë©”ì‰¬ë¥¼ 2D ì´ë¯¸ì§€ë¡œ ë Œë”ë§"""
        
        # 3D ë©”ì‰¬ë¥¼ 2Dë¡œ íˆ¬ì˜
        vertices = mesh['vertices']
        faces = mesh['faces']
        
        # ë Œë”ë§ ë²„í¼ ìƒì„±
        render_buffer = np.zeros((height, width, 3), dtype=np.uint8)
        z_buffer = np.full((height, width), -np.inf)
        
        # ê° ë©´(face)ì„ ë Œë”ë§
        for face in faces:
            if len(face) >= 3:
                # ì‚¼ê°í˜• ì •ì 
                v0, v1, v2 = vertices[face[:3]]
                
                # 2D ì¢Œí‘œë¡œ ë³€í™˜
                p0 = (int(v0[0]), int(v0[1]))
                p1 = (int(v1[0]), int(v1[1]))
                p2 = (int(v2[0]), int(v2[1]))
                
                # ì‚¼ê°í˜• ë˜ìŠ¤í„°í™” ë° í…ìŠ¤ì²˜ ë§¤í•‘
                self._rasterize_triangle(
                    render_buffer, z_buffer, texture,
                    p0, p1, p2, v0[2], v1[2], v2[2]
                )
        
        return Image.fromarray(render_buffer)
    
    def _rasterize_triangle(
        self,
        render_buffer: np.ndarray,
        z_buffer: np.ndarray,
        texture: np.ndarray,
        p0: Tuple[int, int],
        p1: Tuple[int, int], 
        p2: Tuple[int, int],
        z0: float, z1: float, z2: float
    ):
        """ì‚¼ê°í˜• ë˜ìŠ¤í„°í™”"""
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
        min_x = max(0, min(p0[0], p1[0], p2[0]))
        max_x = min(render_buffer.shape[1] - 1, max(p0[0], p1[0], p2[0]))
        min_y = max(0, min(p0[1], p1[1], p2[1]))
        max_y = min(render_buffer.shape[0] - 1, max(p0[1], p1[1], p2[1]))
        
        # ì‚¼ê°í˜• ë‚´ë¶€ í”½ì…€ë“¤ ì²˜ë¦¬
        for y in range(min_y, max_y + 1):
            for x in range(min_x, max_x + 1):
                # ë¬´ê²Œì¤‘ì‹¬ ì¢Œí‘œ ê³„ì‚°
                barycentric = self._calculate_barycentric(
                    (x, y), p0, p1, p2
                )
                
                if all(coord >= 0 for coord in barycentric):
                    # í”½ì…€ì´ ì‚¼ê°í˜• ë‚´ë¶€ì— ìˆìŒ
                    alpha, beta, gamma = barycentric
                    
                    # ê¹Šì´ ë³´ê°„
                    z = alpha * z0 + beta * z1 + gamma * z2
                    
                    # Z-ë²„í¼ í…ŒìŠ¤íŠ¸
                    if z > z_buffer[y, x]:
                        z_buffer[y, x] = z
                        
                        # í…ìŠ¤ì²˜ ì¢Œí‘œ ê³„ì‚°
                        tex_x = int((alpha * 0.1 + beta * 0.9 + gamma * 0.5) * texture.shape[1])
                        tex_y = int((alpha * 0.1 + beta * 0.1 + gamma * 0.9) * texture.shape[0])
                        
                        tex_x = np.clip(tex_x, 0, texture.shape[1] - 1)
                        tex_y = np.clip(tex_y, 0, texture.shape[0] - 1)
                        
                        # í…ìŠ¤ì²˜ ìƒ‰ìƒ ì ìš©
                        render_buffer[y, x] = texture[tex_y, tex_x]
    
    def _calculate_barycentric(
        self, 
        point: Tuple[int, int],
        p0: Tuple[int, int],
        p1: Tuple[int, int], 
        p2: Tuple[int, int]
    ) -> Tuple[float, float, float]:
        """ë¬´ê²Œì¤‘ì‹¬ ì¢Œí‘œ ê³„ì‚°"""
        
        x, y = point
        x0, y0 = p0
        x1, y1 = p1
        x2, y2 = p2
        
        # ì‚¼ê°í˜• ë©´ì  ê³„ì‚°
        area = (x1 - x0) * (y2 - y0) - (x2 - x0) * (y1 - y0)
        
        if abs(area) < 1e-10:
            return (0.0, 0.0, 0.0)
        
        # ë¬´ê²Œì¤‘ì‹¬ ì¢Œí‘œ
        alpha = ((x1 - x) * (y2 - y) - (x2 - x) * (y1 - y)) / area
        beta = ((x2 - x) * (y0 - y) - (x0 - x) * (y2 - y)) / area
        gamma = 1.0 - alpha - beta
        
        return (alpha, beta, gamma)
    
    def _analyze_lighting(self, person_image: Image.Image, body_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ì¡°ëª… ë¶„ì„"""
        
        # ì´ë¯¸ì§€ì—ì„œ ì¡°ëª… ë°©í–¥ ë° ê°•ë„ ë¶„ì„
        cv_image = cv2.cvtColor(np.array(person_image), cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        
        # ê·¸ë¼ë””ì–¸íŠ¸ ë¶„ì„ìœ¼ë¡œ ì¡°ëª… ë°©í–¥ ì¶”ì •
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        
        # í‰ê·  ê·¸ë¼ë””ì–¸íŠ¸ ë°©í–¥
        avg_grad_x = np.mean(grad_x)
        avg_grad_y = np.mean(grad_y)
        
        # ì¡°ëª… ë°©í–¥ (ì •ê·œí™”)
        light_direction = np.array([avg_grad_x, avg_grad_y, 50])  # zì¶• ê¸°ë³¸ê°’
        light_direction = light_direction / np.linalg.norm(light_direction)
        
        # ì¡°ëª… ê°•ë„ (ë°ê¸° ë¶„ì„)
        brightness = np.mean(gray)
        light_intensity = brightness / 255.0
        
        # ê·¸ë¦¼ì ë°©í–¥ (ì¡°ëª…ì˜ ë°˜ëŒ€)
        shadow_direction = -light_direction
        
        return {
            'direction': light_direction,
            'intensity': light_intensity,
            'shadow_direction': shadow_direction,
            'ambient_light': 0.3,  # í™˜ê²½ê´‘
            'color_temperature': 5500  # ìƒ‰ì˜¨ë„ (K)
        }
    
    def _apply_realistic_lighting(self, clothing_image: Image.Image, lighting_info: Dict[str, Any]) -> Image.Image:
        """ì‚¬ì‹¤ì ì¸ ì¡°ëª… ì ìš©"""
        
        cv_image = cv2.cvtColor(np.array(clothing_image), cv2.COLOR_RGB2BGR)
        
        # ì¡°ëª… ë°©í–¥ê³¼ ê°•ë„ ì ìš©
        light_direction = lighting_info['direction']
        light_intensity = lighting_info['intensity']
        
        # ë²•ì„  ë§µ ìƒì„± (ë‹¨ìˆœí™”)
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        normal_map = self._generate_normal_map(gray)
        
        # ê° í”½ì…€ì— ì¡°ëª… ê³„ì‚° ì ìš©
        lit_image = cv_image.copy().astype(np.float32)
        
        for y in range(cv_image.shape[0]):
            for x in range(cv_image.shape[1]):
                if np.any(cv_image[y, x] > 0):  # íˆ¬ëª…í•˜ì§€ ì•Šì€ í”½ì…€ë§Œ
                    # ë²•ì„  ë²¡í„°
                    normal = normal_map[y, x]
                    
                    # ë¨ë²„íŠ¸ ì¡°ëª… ëª¨ë¸
                    dot_product = max(0, np.dot(normal, light_direction))
                    lighting_factor = lighting_info['ambient_light'] + light_intensity * dot_product
                    
                    # ì¡°ëª… ì ìš©
                    lit_image[y, x] *= lighting_factor
        
        # í´ë¦¬í•‘ ë° íƒ€ì… ë³€í™˜
        lit_image = np.clip(lit_image, 0, 255).astype(np.uint8)
        
        return Image.fromarray(cv2.cvtColor(lit_image, cv2.COLOR_BGR2RGB))
    
    def _generate_normal_map(self, gray_image: np.ndarray) -> np.ndarray:
        """ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ì—ì„œ ë²•ì„  ë§µ ìƒì„±"""
        
        # ì†Œë²¨ í•„í„°ë¡œ ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚°
        grad_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)
        
        # ë²•ì„  ë²¡í„° ê³„ì‚°
        normal_map = np.zeros((gray_image.shape[0], gray_image.shape[1], 3))
        
        for y in range(gray_image.shape[0]):
            for x in range(gray_image.shape[1]):
                # ë²•ì„  ë²¡í„° (x, y, z)
                nx = -grad_x[y, x] / 255.0
                ny = -grad_y[y, x] / 255.0
                nz = 1.0
                
                # ì •ê·œí™”
                length = np.sqrt(nx*nx + ny*ny + nz*nz)
                if length > 0:
                    normal_map[y, x] = [nx/length, ny/length, nz/length]
                else:
                    normal_map[y, x] = [0, 0, 1]
        
        return normal_map
    
    def _generate_realistic_shadows(
        self, 
        mesh: Dict[str, np.ndarray], 
        lighting_info: Dict[str, Any],
        width: int, 
        height: int
    ) -> np.ndarray:
        """ì‚¬ì‹¤ì ì¸ ê·¸ë¦¼ì ìƒì„±"""
        
        shadow_map = np.zeros((height, width), dtype=np.float32)
        
        vertices = mesh['vertices']
        light_direction = lighting_info['direction']
        
        # ê° ì •ì ì—ì„œ ê·¸ë¦¼ì íˆ¬ì˜
        for vertex in vertices:
            # ê·¸ë¦¼ì íˆ¬ì˜ ê³„ì‚°
            shadow_point = vertex - light_direction * vertex[2]
            
            # 2D ì¢Œí‘œë¡œ ë³€í™˜
            shadow_x = int(shadow_point[0])
            shadow_y = int(shadow_point[1])
            
            # ê·¸ë¦¼ì ë§µì— ê·¸ë¦¼ì ì¶”ê°€
            if 0 <= shadow_x < width and 0 <= shadow_y < height:
                # ê±°ë¦¬ì— ë”°ë¥¸ ê·¸ë¦¼ì ê°•ë„
                distance = np.linalg.norm(shadow_point - vertex)
                shadow_intensity = max(0, 1.0 - distance / 100.0)
                
                # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë¶€ë“œëŸ¬ìš´ ê·¸ë¦¼ì
                self._add_soft_shadow(shadow_map, shadow_x, shadow_y, shadow_intensity)
        
        # ê·¸ë¦¼ì ë§µ í›„ì²˜ë¦¬
        shadow_map = cv2.GaussianBlur(shadow_map, (15, 15), 5)
        shadow_map = np.clip(shadow_map, 0, 0.7)  # ìµœëŒ€ ê·¸ë¦¼ì ê°•ë„ ì œí•œ
        
        return shadow_map
    
    def _add_soft_shadow(self, shadow_map: np.ndarray, x: int, y: int, intensity: float):
        """ë¶€ë“œëŸ¬ìš´ ê·¸ë¦¼ì ì¶”ê°€"""
        
        radius = 10
        for dy in range(-radius, radius + 1):
            for dx in range(-radius, radius + 1):
                sx, sy = x + dx, y + dy
                if 0 <= sx < shadow_map.shape[1] and 0 <= sy < shadow_map.shape[0]:
                    distance = np.sqrt(dx*dx + dy*dy)
                    if distance <= radius:
                        falloff = max(0, 1.0 - distance / radius)
                        shadow_map[sy, sx] = max(shadow_map[sy, sx], intensity * falloff)
    
    def _calculate_occlusion(
        self, 
        clothing_regions: Dict[str, Dict], 
        fitted_mesh: Dict[str, np.ndarray]
    ) -> np.ndarray:
        """ì˜¤í´ë£¨ì „(ê°€ë ¤ì§) ê³„ì‚°"""
        
        # ê¸°ë³¸ ë§ˆìŠ¤í¬ (ì „ì²´ íˆ¬ëª…)
        occlusion_mask = np.ones((512, 512), dtype=np.float32)
        
        # ì˜ë¥˜ê°€ ë®ëŠ” ì˜ì—­ì€ ì›ë³¸ ì‹ ì²´ ë¶€ìœ„ë¥¼ ê°€ë¦¼
        if 'upper_body' in clothing_regions:
            region = clothing_regions['upper_body']
            if 'mask' in region:
                mask = region['mask']
                if mask.shape[:2] == occlusion_mask.shape:
                    # ì˜ë¥˜ ì˜ì—­ì€ ê°€ë ¤ì§ (0ì— ê°€ê¹Œìš´ ê°’)
                    occlusion_mask[mask > 0] = 0.1
        
        return occlusion_mask
    
    def _advanced_composite(
        self,
        base_image: Image.Image,
        clothing_image: Image.Image,
        shadow_map: np.ndarray,
        occlusion_mask: np.ndarray
    ) -> Image.Image:
        """ê³ ê¸‰ í•©ì„±"""
        
        base_array = np.array(base_image).astype(np.float32)
        clothing_array = np.array(clothing_image).astype(np.float32)
        
        # ì´ë¯¸ì§€ í¬ê¸° ë§ì¶”ê¸°
        h, w = base_array.shape[:2]
        clothing_resized = cv2.resize(clothing_array, (w, h))
        shadow_resized = cv2.resize(shadow_map, (w, h))
        occlusion_resized = cv2.resize(occlusion_mask, (w, h))
        
        # í•©ì„± ê²°ê³¼
        result = base_array.copy()
        
        # ì˜ë¥˜ê°€ ìˆëŠ” ì˜ì—­ í™•ì¸
        clothing_mask = np.any(clothing_resized > 0, axis=2)
        
        for y in range(h):
            for x in range(w):
                if clothing_mask[y, x]:
                    # ì˜ë¥˜ í”½ì…€ì´ ìˆëŠ” ê²½ìš°
                    clothing_color = clothing_resized[y, x]
                    base_color = base_array[y, x]
                    
                    # ì˜¤í´ë£¨ì „ ì ìš©
                    occlusion_factor = occlusion_resized[y, x]
                    
                    # ì˜ë¥˜ì™€ ê¸°ì¡´ ì´ë¯¸ì§€ ë¸”ë Œë”©
                    alpha = 0.9  # ì˜ë¥˜ ë¶ˆíˆ¬ëª…ë„
                    blended = alpha * clothing_color + (1 - alpha) * base_color * occlusion_factor
                    
                    # ê·¸ë¦¼ì ì ìš©
                    shadow_factor = 1.0 - shadow_resized[y, x]
                    result[y, x] = blended * shadow_factor
                else:
                    # ì˜ë¥˜ê°€ ì—†ëŠ” ì˜ì—­ì—ëŠ” ê·¸ë¦¼ìë§Œ ì ìš©
                    shadow_factor = 1.0 - shadow_resized[y, x] * 0.3  # ì•½í•œ ê·¸ë¦¼ì
                    result[y, x] = base_array[y, x] * shadow_factor
        
        # í´ë¦¬í•‘ ë° íƒ€ì… ë³€í™˜
        result = np.clip(result, 0, 255).astype(np.uint8)
        
        return Image.fromarray(result)
    
    def _apply_high_quality_effects(self, image: Image.Image) -> Image.Image:
        """ê³ í’ˆì§ˆ íš¨ê³¼ ì ìš©"""
        
        # 1. ì•ˆí‹°ì•¨ë¦¬ì–´ì‹±
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        smooth = cv2.bilateralFilter(cv_image, 9, 75, 75)
        
        # 2. ìƒ‰ìƒ ë³´ì •
        lab = cv2.cvtColor(smooth, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # ë°ê¸° ì¡°ì •
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        l = clahe.apply(l)
        
        enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
        
        # 3. ì„ ëª…ë„ í–¥ìƒ
        kernel = np.array([[-1,-1,-1],
                          [-1, 9,-1],
                          [-1,-1,-1]])
        sharpened = cv2.filter2D(enhanced, -1, kernel * 0.1)
        
        return Image.fromarray(cv2.cvtColor(sharpened, cv2.COLOR_BGR2RGB))
    
    def _apply_ultra_quality_effects(self, image: Image.Image) -> Image.Image:
        """ì´ˆê³ í’ˆì§ˆ íš¨ê³¼ ì ìš©"""
        
        # ê³ í’ˆì§ˆ íš¨ê³¼ ë¨¼ì € ì ìš©
        high_quality = self._apply_high_quality_effects(image)
        
        cv_image = cv2.cvtColor(np.array(high_quality), cv2.COLOR_RGB2BGR)
        
        # 1. ê³ ê¸‰ ë…¸ì´ì¦ˆ ì œê±°
        denoised = cv2.fastNlMeansDenoisingColored(cv_image, None, 10, 10, 7, 21)
        
        # 2. ì—ì§€ ë³´ì¡´ ìŠ¤ë¬´ë”©
        smooth = cv2.edgePreservingFilter(denoised, flags=1, sigma_s=50, sigma_r=0.4)
        
        # 3. HDR í†¤ ë§¤í•‘ íš¨ê³¼
        hdr_effect = cv2.createTonemap(gamma=1.2)
        hdr_mapped = hdr_effect.process(smooth.astype(np.float32) / 255.0)
        hdr_mapped = np.clip(hdr_mapped * 255, 0, 255).astype(np.uint8)
        
        # 4. ë¯¸ì„¸ ë””í…Œì¼ í–¥ìƒ
        detail_enhanced = cv2.detailEnhance(hdr_mapped, sigma_s=10, sigma_r=0.15)
        
        return Image.fromarray(cv2.cvtColor(detail_enhanced, cv2.COLOR_BGR2RGB))
    
    async def _post_process_and_analyze(
        self,
        result_image: Image.Image,
        body_analysis: Dict[str, Any],
        fitted_clothing: Dict[str, Any]
    ) -> Tuple[Image.Image, Dict[str, Any]]:
        """í›„ì²˜ë¦¬ ë° í’ˆì§ˆ ë¶„ì„"""
        
        # 1. ìµœì¢… ì´ë¯¸ì§€ í›„ì²˜ë¦¬
        final_image = self._final_post_processing(result_image)
        
        # 2. í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
        quality_metrics = {
            'fit_quality': fitted_clothing.get('fit_quality', 0.0),
            'pose_confidence': body_analysis.get('confidence', 0.0),
            'lighting_realism': self._calculate_lighting_realism(final_image),
            'overall_quality': 0.0
        }
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        quality_metrics['overall_quality'] = (
            quality_metrics['fit_quality'] * 0.4 +
            quality_metrics['pose_confidence'] * 0.3 +
            quality_metrics['lighting_realism'] * 0.3
        )
        
        return final_image, quality_metrics
    
    def _final_post_processing(self, image: Image.Image) -> Image.Image:
        """ìµœì¢… í›„ì²˜ë¦¬"""
        
        # 1. ìƒ‰ìƒ ê· í˜• ì¡°ì •
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        
        # 2. ëŒ€ë¹„ í–¥ìƒ
        lab = cv2.cvtColor(cv_image, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8,8))
        l = clahe.apply(l)
        
        enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
        
        # 3. ìµœì¢… ì„ ëª…í™”
        kernel = np.array([[0, -1, 0],
                          [-1, 5, -1],
                          [0, -1, 0]])
        sharpened = cv2.filter2D(enhanced, -1, kernel * 0.1)
        
        return Image.fromarray(cv2.cvtColor(sharpened, cv2.COLOR_BGR2RGB))
    
    def _calculate_lighting_realism(self, image: Image.Image) -> float:
        """ì¡°ëª… ì‚¬ì‹¤ì„± ê³„ì‚°"""
        
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        
        # ê·¸ë¼ë””ì–¸íŠ¸ ë¶„ì„
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        
        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
        
        # ì¡°ëª…ì˜ ì¼ê´€ì„± ì¸¡ì • (ê·¸ë¼ë””ì–¸íŠ¸ ë¶„ì‚°ì˜ ì—­ìˆ˜)
        consistency = 1.0 / (1.0 + np.var(gradient_magnitude))
        
        return min(1.0, consistency * 2.0)
    
    async def _create_fallback_result(
        self, 
        person_image: Image.Image, 
        clothing_image: Image.Image
    ) -> Image.Image:
        """ì—ëŸ¬ ë°œìƒì‹œ ê¸°ë³¸ ê²°ê³¼ ìƒì„±"""
        
        logger.info("âš ï¸ ê¸°ë³¸ í•©ì„± ëª¨ë“œë¡œ ëŒ€ì²´")
        
        result = person_image.copy()
        
        # ê°„ë‹¨í•œ ì˜¤ë²„ë ˆì´
        clothing_resized = clothing_image.resize((200, 200))
        result.paste(clothing_resized, (150, 100), clothing_resized)
        
        # ì—ëŸ¬ í‘œì‹œ
        draw = ImageDraw.Draw(result)
        draw.text((10, 10), "Processing Error - Basic Mode", fill='red')
        
        return result

# ì „ì—­ ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì„œë¹„ìŠ¤
real_virtual_fitting = RealVirtualFittingService()