"""
8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ í†µí•© ì„œë¹„ìŠ¤
MyCloset AI ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œì˜ í•µì‹¬ ì—”ì§„
"""
import os
import time
import logging
import asyncio
from typing import Dict, Any, Optional, Tuple
from pathlib import Path
from PIL import Image
import numpy as np
import torch
from concurrent.futures import ThreadPoolExecutor
import gc

logger = logging.getLogger(__name__)

class AIVirtualTryOnPipeline:
    """8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self, device: str = "auto", memory_limit_gb: float = 8.0):
        """
        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('auto', 'cpu', 'cuda', 'mps')
            memory_limit_gb: ë©”ëª¨ë¦¬ ì‚¬ìš© ì œí•œ (GB)
        """
        self.device = self._setup_device(device)
        self.memory_limit = memory_limit_gb * 1024**3  # bytes
        self.models = {}
        self.is_initialized = False
        self.processing_stats = {
            "total_processed": 0,
            "average_time": 0.0,
            "success_rate": 0.0
        }
        
        # ìŠ¤ë ˆë“œ í’€ executor
        self.executor = ThreadPoolExecutor(max_workers=2)
        
        logger.info(f"ğŸ¤– AI Pipeline ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _setup_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device == "auto":
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        return device
    
    async def initialize_models(self) -> bool:
        """ëª¨ë“  AI ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”„ AI ëª¨ë¸ë“¤ ì´ˆê¸°í™” ì¤‘...")
            
            # 1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± ëª¨ë¸ (Graphonomy)
            await self._init_human_parsing()
            
            # 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • ëª¨ë¸ (OpenPose/MediaPipe)
            await self._init_pose_estimation()
            
            # 3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ (UÂ²-Net)
            await self._init_clothing_segmentation()
            
            # 4-5ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­ & ì›Œí•‘ (TPS)
            await self._init_geometric_warping()
            
            # 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ìƒì„± (HR-VITON/ACGPN)
            await self._init_virtual_fitting()
            
            # 7ë‹¨ê³„: í›„ì²˜ë¦¬ ëª¨ë¸
            await self._init_postprocessing()
            
            # 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ëª¨ë¸
            await self._init_quality_assessment()
            
            self.is_initialized = True
            logger.info("âœ… ëª¨ë“  AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _init_human_parsing(self):
        """1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± ëª¨ë¸ ì´ˆê¸°í™” (20ê°œ ë¶€ìœ„ ë¶„í• )"""
        try:
            # Graphonomy ë˜ëŠ” ë‹¤ë¥¸ íŒŒì‹± ëª¨ë¸ ë¡œë“œ
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ëª¨ë¸ íŒŒì¼ì„ ë¡œë“œ
            self.models['human_parsing'] = {
                'model': None,  # ì‹¤ì œ ëª¨ë¸ ê°ì²´
                'initialized': True,
                'segments': 20  # ë¶„í•  ë¶€ìœ„ ìˆ˜
            }
            logger.info("âœ… 1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì¸ì²´ íŒŒì‹± ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _init_pose_estimation(self):
        """2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ì´ˆê¸°í™” (18ê°œ í‚¤í¬ì¸íŠ¸)"""
        try:
            # MediaPipe ë˜ëŠ” OpenPose ëª¨ë¸ ë¡œë“œ
            self.models['pose_estimation'] = {
                'model': None,  # ì‹¤ì œ ëª¨ë¸ ê°ì²´
                'initialized': True,
                'keypoints': 18  # í‚¤í¬ì¸íŠ¸ ìˆ˜
            }
            logger.info("âœ… 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _init_clothing_segmentation(self):
        """3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # UÂ²-Net ë˜ëŠ” ë‹¤ë¥¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë“œ
            self.models['clothing_segmentation'] = {
                'model': None,  # ì‹¤ì œ ëª¨ë¸ ê°ì²´
                'initialized': True
            }
            logger.info("âœ… 3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _init_geometric_warping(self):
        """4-5ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­ & ì›Œí•‘ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # TPS ë³€í™˜ ëª¨ë¸ ë¡œë“œ
            self.models['geometric_warping'] = {
                'model': None,  # ì‹¤ì œ ëª¨ë¸ ê°ì²´
                'initialized': True
            }
            logger.info("âœ… 4-5ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­ & ì›Œí•‘ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ê¸°í•˜í•™ì  ì›Œí•‘ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _init_virtual_fitting(self):
        """6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ìƒì„± ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # HR-VITON, ACGPN, ë˜ëŠ” OOTDiffusion ëª¨ë¸ ë¡œë“œ
            self.models['virtual_fitting'] = {
                'model': None,  # ì‹¤ì œ ëª¨ë¸ ê°ì²´
                'initialized': True,
                'model_type': 'hr_viton'  # ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì…
            }
            logger.info("âœ… 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ìƒì„± ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _init_postprocessing(self):
        """7ë‹¨ê³„: í›„ì²˜ë¦¬ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # í’ˆì§ˆ í–¥ìƒ ëª¨ë¸ ë¡œë“œ (Super Resolution, Denoising ë“±)
            self.models['postprocessing'] = {
                'model': None,  # ì‹¤ì œ ëª¨ë¸ ê°ì²´
                'initialized': True
            }
            logger.info("âœ… 7ë‹¨ê³„: í›„ì²˜ë¦¬ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ í›„ì²˜ë¦¬ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _init_quality_assessment(self):
        """8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # ìë™ í’ˆì§ˆ ìŠ¤ì½”ì–´ë§ ëª¨ë¸ ë¡œë“œ
            self.models['quality_assessment'] = {
                'model': None,  # ì‹¤ì œ ëª¨ë¸ ê°ì²´
                'initialized': True
            }
            logger.info("âœ… 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def process_virtual_tryon(
        self, 
        person_image: Image.Image, 
        clothing_image: Image.Image,
        height: float = 170.0,
        weight: float = 65.0
    ) -> Dict[str, Any]:
        """
        ê°€ìƒ í”¼íŒ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            person_image: ì‚¬ìš©ì ì´ë¯¸ì§€
            clothing_image: ì˜ë¥˜ ì´ë¯¸ì§€
            height: í‚¤ (cm)
            weight: ëª¸ë¬´ê²Œ (kg)
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_initialized:
            raise RuntimeError("ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize_models()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        start_time = time.time()
        result = {
            "success": False,
            "fitted_image": None,
            "processing_time": 0.0,
            "confidence": 0.0,
            "fit_score": 0.0,
            "quality_score": 0.0,
            "measurements": {},
            "recommendations": [],
            "debug_info": {},
            "pipeline_stages": {}
        }
        
        try:
            logger.info("ğŸ¯ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ ì‹œì‘")
            
            # 1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± (20ê°œ ë¶€ìœ„ ë¶„í• )
            parsing_result = await self._stage_1_human_parsing(person_image)
            result["pipeline_stages"]["1_human_parsing"] = parsing_result
            
            # 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • (18ê°œ í‚¤í¬ì¸íŠ¸)
            pose_result = await self._stage_2_pose_estimation(person_image)
            result["pipeline_stages"]["2_pose_estimation"] = pose_result
            
            # 3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ë°°ê²½ ì œê±°)
            segmentation_result = await self._stage_3_clothing_segmentation(clothing_image)
            result["pipeline_stages"]["3_clothing_segmentation"] = segmentation_result
            
            # 4ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­ (TPS ë³€í™˜)
            matching_result = await self._stage_4_geometric_matching(
                person_image, clothing_image, pose_result, parsing_result
            )
            result["pipeline_stages"]["4_geometric_matching"] = matching_result
            
            # 5ë‹¨ê³„: ì˜· ì›Œí•‘ (ì‹ ì²´ì— ë§ì¶° ë³€í˜•)
            warping_result = await self._stage_5_clothing_warping(
                clothing_image, matching_result, height, weight
            )
            result["pipeline_stages"]["5_clothing_warping"] = warping_result
            
            # 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ìƒì„± (HR-VITON/ACGPN)
            fitting_result = await self._stage_6_virtual_fitting(
                person_image, warping_result, parsing_result, pose_result
            )
            result["pipeline_stages"]["6_virtual_fitting"] = fitting_result
            
            # 7ë‹¨ê³„: í›„ì²˜ë¦¬ (í’ˆì§ˆ í–¥ìƒ)
            postprocess_result = await self._stage_7_postprocessing(fitting_result["image"])
            result["pipeline_stages"]["7_postprocessing"] = postprocess_result
            
            # 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (ìë™ ìŠ¤ì½”ì–´ë§)
            quality_result = await self._stage_8_quality_assessment(
                postprocess_result["image"], person_image, clothing_image
            )
            result["pipeline_stages"]["8_quality_assessment"] = quality_result
            
            # ìµœì¢… ê²°ê³¼ ì„¤ì •
            result.update({
                "success": True,
                "fitted_image": postprocess_result["image"],
                "processing_time": time.time() - start_time,
                "confidence": fitting_result.get("confidence", 0.8),
                "fit_score": quality_result.get("fit_score", 0.7),
                "quality_score": quality_result.get("quality_score", 0.8),
                "measurements": pose_result.get("measurements", {}),
                "recommendations": quality_result.get("recommendations", [])
            })
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(result["processing_time"], True)
            
            logger.info(f"âœ… ê°€ìƒ í”¼íŒ… ì™„ë£Œ - ì²˜ë¦¬ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
            
        except Exception as e:
            result["processing_time"] = time.time() - start_time
            result["error"] = str(e)
            self._update_stats(result["processing_time"], False)
            logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ì‹¤íŒ¨: {e}")
            
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            await self._cleanup_memory()
        
        return result
    
    async def _stage_1_human_parsing(self, person_image: Image.Image) -> Dict[str, Any]:
        """1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± ì‹¤í–‰"""
        logger.info("1ï¸âƒ£ ì¸ì²´ íŒŒì‹± ì‹œì‘...")
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Graphonomy ë“±ì˜ ëª¨ë¸ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ë°ëª¨ìš© ê²°ê³¼ ë°˜í™˜
        return {
            "segments": 20,
            "parsing_map": "placeholder_parsing_map",
            "confidence": 0.92,
            "processing_time": 0.5
        }
    
    async def _stage_2_pose_estimation(self, person_image: Image.Image) -> Dict[str, Any]:
        """2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • ì‹¤í–‰"""
        logger.info("2ï¸âƒ£ í¬ì¦ˆ ì¶”ì • ì‹œì‘...")
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” MediaPipe ë˜ëŠ” OpenPose ì‚¬ìš©
        return {
            "keypoints": [[100, 150], [120, 180]],  # 18ê°œ í‚¤í¬ì¸íŠ¸
            "confidence": 0.89,
            "measurements": {
                "shoulder_width": 45.2,
                "chest_width": 38.5,
                "waist_width": 32.1,
                "hip_width": 40.3
            },
            "processing_time": 0.3
        }
    
    async def _stage_3_clothing_segmentation(self, clothing_image: Image.Image) -> Dict[str, Any]:
        """3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰"""
        logger.info("3ï¸âƒ£ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œì‘...")
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” UÂ²-Net ë“±ì˜ ëª¨ë¸ ì‚¬ìš©
        return {
            "segmented_clothing": "placeholder_segmented_image",
            "confidence": 0.94,
            "clothing_type": "shirt",
            "processing_time": 0.4
        }
    
    async def _stage_4_geometric_matching(self, person_image, clothing_image, pose_result, parsing_result) -> Dict[str, Any]:
        """4ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹¤í–‰"""
        logger.info("4ï¸âƒ£ ê¸°í•˜í•™ì  ë§¤ì¹­ ì‹œì‘...")
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” TPS ë³€í™˜ ì‚¬ìš©
        return {
            "matching_points": [[50, 60], [80, 90]],
            "transformation_matrix": "placeholder_matrix",
            "confidence": 0.87,
            "processing_time": 0.6
        }
    
    async def _stage_5_clothing_warping(self, clothing_image, matching_result, height, weight) -> Dict[str, Any]:
        """5ë‹¨ê³„: ì˜· ì›Œí•‘ ì‹¤í–‰"""
        logger.info("5ï¸âƒ£ ì˜· ì›Œí•‘ ì‹œì‘...")
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹ ì²´ ì¹˜ìˆ˜ì— ë§ì¶° ì˜·ì„ ë³€í˜•
        return {
            "warped_clothing": "placeholder_warped_image",
            "scale_factor": 1.05,
            "confidence": 0.91,
            "processing_time": 0.8
        }
    
    async def _stage_6_virtual_fitting(self, person_image, warping_result, parsing_result, pose_result) -> Dict[str, Any]:
        """6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ìƒì„± ì‹¤í–‰"""
        logger.info("6ï¸âƒ£ ê°€ìƒ í”¼íŒ… ìƒì„± ì‹œì‘...")
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” HR-VITON, ACGPN ë“±ì˜ ëª¨ë¸ ì‚¬ìš©
        return {
            "image": "placeholder_fitted_image",
            "confidence": 0.88,
            "blend_quality": 0.92,
            "processing_time": 2.1
        }
    
    async def _stage_7_postprocessing(self, fitted_image) -> Dict[str, Any]:
        """7ë‹¨ê³„: í›„ì²˜ë¦¬ ì‹¤í–‰"""
        logger.info("7ï¸âƒ£ í›„ì²˜ë¦¬ ì‹œì‘...")
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Super Resolution, Denoising ë“± ì ìš©
        return {
            "image": fitted_image,  # ê°œì„ ëœ ì´ë¯¸ì§€
            "enhancement_score": 0.85,
            "processing_time": 0.7
        }
    
    async def _stage_8_quality_assessment(self, final_image, person_image, clothing_image) -> Dict[str, Any]:
        """8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ì‹¤í–‰"""
        logger.info("8ï¸âƒ£ í’ˆì§ˆ í‰ê°€ ì‹œì‘...")
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ìë™ í’ˆì§ˆ ìŠ¤ì½”ì–´ë§ ëª¨ë¸ ì‚¬ìš©
        return {
            "quality_score": 0.86,
            "fit_score": 0.82,
            "realism_score": 0.89,
            "recommendations": [
                "ì‚¬ì´ì¦ˆë¥¼ í•œ ì¹˜ìˆ˜ í¬ê²Œ ê³ ë ¤í•´ë³´ì„¸ìš”",
                "ì–´ê¹¨ ë¼ì¸ì´ ì˜ ë§ìŠµë‹ˆë‹¤"
            ],
            "processing_time": 0.4
        }
    
    def _update_stats(self, processing_time: float, success: bool):
        """ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.processing_stats["total_processed"] += 1
        
        if success:
            # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            total = self.processing_stats["total_processed"]
            current_avg = self.processing_stats["average_time"]
            self.processing_stats["average_time"] = (
                (current_avg * (total - 1) + processing_time) / total
            )
        
        # ì„±ê³µë¥  ì—…ë°ì´íŠ¸
        success_count = self.processing_stats["total_processed"] * self.processing_stats["success_rate"]
        if success:
            success_count += 1
        self.processing_stats["success_rate"] = success_count / self.processing_stats["total_processed"]
    
    async def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.device == "cuda":
            torch.cuda.empty_cache()
        elif self.device == "mps":
            torch.mps.empty_cache()
        gc.collect()
    
    def get_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë°˜í™˜"""
        return {
            "initialized": self.is_initialized,
            "device": self.device,
            "models_loaded": len([m for m in self.models.values() if m.get("initialized", False)]),
            "total_models": len(self.models),
            "stats": self.processing_stats,
            "memory_limit_gb": self.memory_limit / (1024**3)
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("ğŸ§¹ AI Pipeline ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        # ëª¨ë¸ë“¤ ì •ë¦¬
        for model_name, model_info in self.models.items():
            if model_info.get("model"):
                del model_info["model"]
        
        self.models.clear()
        self.executor.shutdown(wait=True)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.device == "cuda":
            torch.cuda.empty_cache()
        elif self.device == "mps":
            torch.mps.empty_cache()
        gc.collect()
        
        logger.info("âœ… AI Pipeline ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# ì „ì—­ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤
pipeline_instance: Optional[AIVirtualTryOnPipeline] = None

def get_pipeline() -> AIVirtualTryOnPipeline:
    """íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global pipeline_instance
    if pipeline_instance is None:
        device = os.environ.get('DEVICE', 'auto')
        pipeline_instance = AIVirtualTryOnPipeline(device=device)
    return pipeline_instance