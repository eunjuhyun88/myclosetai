# app/core/m3_optimizer.py
"""
M3 Max 128GB íŠ¹í™” ìµœì í™” ì‹œìŠ¤í…œ
- 40ì½”ì–´ GPU + 16ì½”ì–´ Neural Engine í™œìš©
- 128GB í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”
- 400GB/s ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ê·¹ëŒ€í™”
"""

import os
import logging
import torch
import psutil
import platform
from typing import Dict, Any, Optional, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class M3MaxSpecs:
    """M3 Max í•˜ë“œì›¨ì–´ ìŠ¤í™"""
    gpu_cores: int = 40
    neural_engine_cores: int = 16
    cpu_performance_cores: int = 12
    cpu_efficiency_cores: int = 4
    total_memory_gb: int = 128
    memory_bandwidth_gbps: int = 400
    max_memory_allocation_gb: int = 100  # 80% ì‚¬ìš© ê¶Œì¥

class M3MaxOptimizer:
    """M3 Max 128GB íŠ¹í™” ìµœì í™”"""
    
    def __init__(self):
        self.specs = M3MaxSpecs()
        self.device = "mps"
        self.is_m3_max = self._verify_m3_max()
        self.optimization_config = self._create_optimization_config()
        
        logger.info("ğŸ M3 Max 128GB ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        
        if self.is_m3_max:
            self._apply_m3_max_optimizations()
        else:
            logger.warning("âš ï¸ M3 Maxê°€ ì•„ë‹Œ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘")
    
    def _verify_m3_max(self) -> bool:
        """M3 Max í™˜ê²½ í™•ì¸"""
        try:
            system_info = platform.uname()
            memory_gb = psutil.virtual_memory().total / (1024**3)
            
            # M3 Max í™•ì¸ ì¡°ê±´
            is_apple_silicon = (
                system_info.system == "Darwin" and 
                system_info.machine == "arm64"
            )
            
            has_sufficient_memory = memory_gb >= 100  # 128GB ì¤‘ 100GB+ ì¸ì‹
            has_mps = torch.backends.mps.is_available()
            
            if is_apple_silicon and has_sufficient_memory and has_mps:
                logger.info(f"âœ… M3 Max í™˜ê²½ í™•ì¸: {memory_gb:.0f}GB ë©”ëª¨ë¦¬")
                return True
            else:
                logger.info(f"âŒ M3 Max í™˜ê²½ ì•„ë‹˜: {memory_gb:.0f}GB ë©”ëª¨ë¦¬")
                return False
                
        except Exception as e:
            logger.warning(f"M3 Max í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def _create_optimization_config(self) -> Dict[str, Any]:
        """M3 Max ìµœì í™” ì„¤ì • ìƒì„±"""
        config = {
            # GPU ì„¤ì •
            "device": "mps",
            "dtype": torch.float32,  # MPS ìµœì í™”
            "memory_fraction": 0.8,  # 128GB ì¤‘ 80% í™œìš©
            
            # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
            "max_batch_size": 8,  # ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ í™œìš©
            "prefetch_factor": 4,
            "num_workers": 8,  # CPU ì½”ì–´ ìˆ˜ ë§ì¶¤
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬
            "memory_pool_size_gb": 80,  # 80GB ë©”ëª¨ë¦¬ í’€
            "cache_size_gb": 20,  # 20GB ìºì‹œ
            "swap_threshold": 0.9,
            
            # Neural Engine í™œìš©
            "neural_engine_enabled": True,
            "coreml_optimization": True,
            
            # Metal Performance Shaders ìµœì í™”
            "mps_optimization": {
                "enable_fusion": True,
                "enable_memory_efficient_attention": False,  # MPS í˜¸í™˜ì„±
                "enable_gradient_checkpointing": True,
                "max_split_size_mb": 256
            },
            
            # ëª¨ë¸ë³„ ìµœì í™”
            "model_optimizations": {
                "diffusion_models": {
                    "attention_slicing": True,
                    "cpu_offload": False,  # ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ë¡œ GPU ìœ ì§€
                    "sequential_cpu_offload": False,
                    "enable_vae_slicing": False
                },
                "segmentation_models": {
                    "tile_size": 1024,  # ê³ í•´ìƒë„ ì²˜ë¦¬
                    "overlap": 64,
                    "batch_inference": True
                },
                "pose_estimation": {
                    "multi_scale": True,
                    "high_precision": True
                }
            }
        }
        
        return config
    
    def _apply_m3_max_optimizations(self) -> None:
        """M3 Max ì‹œìŠ¤í…œ ìµœì í™” ì ìš©"""
        try:
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            env_vars = {
                # PyTorch MPS ìµœì í™”
                "PYTORCH_MPS_HIGH_WATERMARK_RATIO": "0.0",
                "PYTORCH_MPS_ALLOCATOR_POLICY": "garbage_collection",
                "PYTORCH_ENABLE_MPS_FALLBACK": "1",
                
                # M3 Max CPU ìµœì í™”
                "OMP_NUM_THREADS": str(self.specs.cpu_performance_cores),
                "MKL_NUM_THREADS": str(self.specs.cpu_performance_cores),
                "NUMBA_NUM_THREADS": str(self.specs.cpu_performance_cores),
                
                # ë©”ëª¨ë¦¬ ìµœì í™”
                "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:256",
                "MALLOC_ARENA_MAX": "4",
                
                # Metal ê°€ì†
                "METAL_DEVICE_WRAPPER_TYPE": "1",
                "METAL_PERFORMANCE_SHADERS_ENABLED": "1"
            }
            
            os.environ.update(env_vars)
            
            # PyTorch ìµœì í™” ì„¤ì •
            if torch.backends.mps.is_available():
                # MPS ë©”ëª¨ë¦¬ ê´€ë¦¬ (PyTorch ë²„ì „ í˜¸í™˜ì„± ì²´í¬)
                try:
                    # PyTorch 2.4 ì´í•˜
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                        logger.info("âœ… MPS ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                except AttributeError:
                    # PyTorch 2.5+ ëŒ€ì²´ ë°©ë²•
                    logger.info("â„¹ï¸ PyTorch 2.5+ í™˜ê²½ - ëŒ€ì²´ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‚¬ìš©")
            
            # CPU ìµœì í™”
            torch.set_num_threads(self.specs.cpu_performance_cores)
            
            logger.info("ğŸš€ M3 Max ìµœì í™” ì„¤ì • ì ìš© ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ M3 Max ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
    
    def get_optimal_batch_size(self, model_type: str = "diffusion") -> int:
        """ëª¨ë¸ íƒ€ì…ë³„ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        base_sizes = {
            "diffusion": 4,      # Stable Diffusion ê³„ì—´
            "segmentation": 8,   # U-Net ê³„ì—´
            "pose": 16,          # í¬ì¦ˆ ì¶”ì •
            "classification": 32 # ë¶„ë¥˜ ëª¨ë¸
        }
        
        base_size = base_sizes.get(model_type, 4)
        
        # M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš©ë„ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if self.is_m3_max:
            memory_multiplier = min(self.specs.total_memory_gb / 32, 4.0)  # ìµœëŒ€ 4ë°°
            optimal_size = int(base_size * memory_multiplier)
            return min(optimal_size, 16)  # ì•ˆì •ì„±ì„ ìœ„í•´ ìµœëŒ€ 16ìœ¼ë¡œ ì œí•œ
        
        return base_size
    
    def get_memory_allocation(self) -> Dict[str, int]:
        """ë©”ëª¨ë¦¬ í• ë‹¹ ê³„íš"""
        total_gb = self.specs.total_memory_gb
        
        allocation = {
            "ai_models": int(total_gb * 0.6),      # 76.8GB - AI ëª¨ë¸
            "image_cache": int(total_gb * 0.15),   # 19.2GB - ì´ë¯¸ì§€ ìºì‹œ
            "system_buffer": int(total_gb * 0.1),  # 12.8GB - ì‹œìŠ¤í…œ ë²„í¼
            "temp_processing": int(total_gb * 0.1), # 12.8GB - ì„ì‹œ ì²˜ë¦¬
            "os_reserved": int(total_gb * 0.05)    # 6.4GB - OS ì˜ˆì•½
        }
        
        return allocation
    
    def optimize_for_model(self, model_name: str) -> Dict[str, Any]:
        """íŠ¹ì • ëª¨ë¸ì— ëŒ€í•œ ìµœì í™” ì„¤ì •"""
        base_config = self.optimization_config.copy()
        
        if "diffusion" in model_name.lower():
            # Stable Diffusion ìµœì í™”
            base_config.update({
                "batch_size": self.get_optimal_batch_size("diffusion"),
                "attention_slicing": True,
                "memory_efficient_attention": False,  # MPS í˜¸í™˜ì„±
                "enable_xformers": False,  # MPSì—ì„œ ë¯¸ì§€ì›
                "gradient_checkpointing": True
            })
            
        elif "segmentation" in model_name.lower() or "u2net" in model_name.lower():
            # ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ìµœì í™”
            base_config.update({
                "batch_size": self.get_optimal_batch_size("segmentation"),
                "tile_processing": True,
                "tile_size": 1024,
                "enable_amp": False  # MPS AMP í˜¸í™˜ì„± ì´ìŠˆ
            })
            
        elif "pose" in model_name.lower() or "openpose" in model_name.lower():
            # í¬ì¦ˆ ì¶”ì • ìµœì í™”
            base_config.update({
                "batch_size": self.get_optimal_batch_size("pose"),
                "multi_scale": True,
                "nms_threshold": 0.5
            })
        
        return base_config
    
    def monitor_performance(self) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        return {
            "memory_used_gb": (memory.total - memory.available) / (1024**3),
            "memory_available_gb": memory.available / (1024**3),
            "memory_usage_percent": memory.percent,
            "cpu_usage_percent": cpu_percent,
            "estimated_ai_memory_gb": self.get_memory_allocation()["ai_models"],
            "optimization_active": self.is_m3_max,
            "device": self.device,
            "batch_size_recommendation": self.get_optimal_batch_size()
        }
    
    def create_model_config(self, model_type: str) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ ìµœì í™” ì„¤ì • ìƒì„±"""
        return {
            "device": self.device,
            "torch_dtype": torch.float32,
            "low_cpu_mem_usage": False,  # ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ ìˆìŒ
            "device_map": None,  # ë‹¨ì¼ GPU ì‚¬ìš©
            "max_memory": {0: f"{self.specs.max_memory_allocation_gb}GB"},
            "offload_folder": None,  # ë©”ëª¨ë¦¬ ì¶©ë¶„ìœ¼ë¡œ offload ë¶ˆí•„ìš”
            "offload_state_dict": False,
            "use_safetensors": True,
            "variant": "fp32",  # MPS ìµœì í™”
            
            # M3 Max íŠ¹í™” ì„¤ì •
            "enable_attention_slicing": True,
            "enable_cpu_offload": False,
            "enable_model_cpu_offload": False,
            "enable_sequential_cpu_offload": False,
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            "memory_efficient_attention": False,  # MPS í˜¸í™˜ì„±
            "use_memory_efficient_attention_xformers": False,
            "attention_slice_size": "auto",
            
            # ì„±ëŠ¥ ì„¤ì •
            "num_images_per_prompt": 1,
            "batch_size": self.get_optimal_batch_size(model_type),
            "max_batch_size": 8,
            
            # í’ˆì§ˆ ì„¤ì •
            "guidance_scale": 7.5,
            "num_inference_steps": 50,
            "scheduler": "DPMSolverMultistepScheduler"
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        return {
            "hardware": {
                "chip": "Apple M3 Max",
                "gpu_cores": self.specs.gpu_cores,
                "neural_engine": f"{self.specs.neural_engine_cores} cores",
                "cpu_cores": f"{self.specs.cpu_performance_cores} performance + {self.specs.cpu_efficiency_cores} efficiency",
                "memory": f"{self.specs.total_memory_gb}GB Unified Memory",
                "memory_bandwidth": f"{self.specs.memory_bandwidth_gbps}GB/s"
            },
            "optimization": {
                "enabled": self.is_m3_max,
                "device": self.device,
                "memory_allocation": self.get_memory_allocation(),
                "batch_size_diffusion": self.get_optimal_batch_size("diffusion"),
                "batch_size_segmentation": self.get_optimal_batch_size("segmentation")
            },
            "pytorch": {
                "version": torch.__version__,
                "mps_available": torch.backends.mps.is_available(),
                "cuda_available": torch.cuda.is_available()
            }
        }

# ì „ì—­ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤
_m3_optimizer: Optional[M3MaxOptimizer] = None

def get_m3_optimizer() -> M3MaxOptimizer:
    """M3 Max ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _m3_optimizer
    if _m3_optimizer is None:
        _m3_optimizer = M3MaxOptimizer()
    return _m3_optimizer

def is_m3_max_optimized() -> bool:
    """M3 Max ìµœì í™” í™œì„± ì—¬ë¶€"""
    optimizer = get_m3_optimizer()
    return optimizer.is_m3_max

def get_optimal_config(model_type: str = "diffusion") -> Dict[str, Any]:
    """ìµœì í™”ëœ ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
    optimizer = get_m3_optimizer()
    return optimizer.create_model_config(model_type)

# ì¶”ê°€ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
def create_m3_optimizer() -> M3MaxOptimizer:
    """M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return M3MaxOptimizer()

def get_m3_config() -> Dict[str, Any]:
    """M3 Max ì„¤ì • ë°˜í™˜"""
    optimizer = get_m3_optimizer()
    return optimizer.get_system_info()

def optimize_for_m3_max() -> bool:
    """M3 Max ìµœì í™” ì ìš©"""
    optimizer = get_m3_optimizer()
    return optimizer.is_m3_max