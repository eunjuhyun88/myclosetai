# backend/app/core/m3_optimizer.py

import os
import logging
import torch
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

class M3Optimizer:
    """
    Apple M3 Max ì „ìš© ìµœì í™” í´ë˜ìŠ¤
    """
    
    def __init__(self, device_name: str, memory_gb: float, is_m3_max: bool, optimization_level: str):
        """
        M3 ìµœì í™” ì´ˆê¸°í™”
        
        Args:
            device_name: ë””ë°”ì´ìŠ¤ ì´ë¦„ (ì˜ˆ: "Apple M3 Max")
            memory_gb: ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)
            is_m3_max: M3 Max ì—¬ë¶€
            optimization_level: ìµœì í™” ë ˆë²¨ ("maximum", "high", "medium", "basic")
        """
        self.device_name = device_name
        self.memory_gb = memory_gb
        self.is_m3_max = is_m3_max
        self.optimization_level = optimization_level
        
        logger.info(f"ğŸ M3Optimizer ì´ˆê¸°í™”: {device_name}, {memory_gb}GB, {optimization_level}")
        
        # M3 Max ì „ìš© ì„¤ì •
        if is_m3_max:
            self._apply_m3_max_optimizations()
        
        self.config = self._create_optimization_config()
        self.pipeline_settings = self._create_pipeline_settings()
    
    def _apply_m3_max_optimizations(self):
        """M3 Max ì „ìš© ìµœì í™” ì ìš©"""
        try:
            # Neural Engine í™˜ê²½ë³€ìˆ˜ ì„¤ì •
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            os.environ["PYTORCH_MPS_LOW_WATERMARK_RATIO"] = "0.0"
            
            # Metal Performance Shaders ìµœì í™”
            if torch.backends.mps.is_available():
                logger.info("ğŸ§  Neural Engine ìµœì í™” í™œì„±í™”")
                logger.info("âš™ï¸ Metal Performance Shaders í™œì„±í™”")
                
                # CPU ìŠ¤ë ˆë“œ ìµœì í™” (M3 Max 16ì½”ì–´)
                if hasattr(torch, 'set_num_threads'):
                    torch.set_num_threads(16)
                
                logger.info("âš™ï¸ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì • ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _create_optimization_config(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ìƒì„±"""
        base_config = {
            "device": "mps" if self.is_m3_max else "cpu",
            "memory_gb": self.memory_gb,
            "optimization_level": self.optimization_level
        }
        
        if self.is_m3_max:
            if self.optimization_level == "maximum":
                config = {
                    **base_config,
                    "batch_size": 4,
                    "precision": "float16",
                    "max_workers": 12,
                    "memory_fraction": 0.8,
                    "enable_neural_engine": True,
                    "pipeline_parallel": True
                }
            elif self.optimization_level == "high":
                config = {
                    **base_config,
                    "batch_size": 2,
                    "precision": "float16",
                    "max_workers": 8,
                    "memory_fraction": 0.6,
                    "enable_neural_engine": True,
                    "pipeline_parallel": False
                }
            elif self.optimization_level == "medium":
                config = {
                    **base_config,
                    "batch_size": 1,
                    "precision": "float16",
                    "max_workers": 4,
                    "memory_fraction": 0.4,
                    "enable_neural_engine": True,
                    "pipeline_parallel": False
                }
            else:  # basic
                config = {
                    **base_config,
                    "batch_size": 1,
                    "precision": "float32",
                    "max_workers": 4,
                    "memory_fraction": 0.4,
                    "enable_neural_engine": False,
                    "pipeline_parallel": False
                }
        else:
            config = base_config
        
        return config
    
    def _create_pipeline_settings(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì„¤ì • ìƒì„±"""
        if self.is_m3_max:
            return {
                "stages": 8,
                "parallel_processing": True,
                "batch_optimization": True,
                "memory_pooling": True,
                "neural_engine": True,
                "unified_memory": True,
                "mps_backend": True
            }
        else:
            return {
                "stages": 8,
                "parallel_processing": False,
                "batch_optimization": False,
                "memory_pooling": False,
                "neural_engine": False,
                "unified_memory": False,
                "mps_backend": False
            }
    
    def optimize_model(self, model):
        """ëª¨ë¸ ìµœì í™” ì ìš©"""
        if not self.is_m3_max or model is None:
            return model
            
        try:
            # MPS ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if hasattr(model, 'to'):
                model = model.to('mps')
                logger.info("ğŸ”„ ëª¨ë¸ì„ MPS ë””ë°”ì´ìŠ¤ë¡œ ì´ë™")
            
            # ì •ë°€ë„ ìµœì í™”
            if self.config.get("precision") == "float16" and hasattr(model, 'half'):
                model = model.half()
                logger.info("ğŸ”§ ëª¨ë¸ ì •ë°€ë„ë¥¼ float16ìœ¼ë¡œ ìµœì í™”")
            
            logger.info("âœ… ëª¨ë¸ M3 Max ìµœì í™” ì™„ë£Œ")
            return model
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model
    
    def get_optimization_info(self) -> Dict[str, Any]:
        """ìµœì í™” ì •ë³´ ë°˜í™˜"""
        return {
            "device_name": self.device_name,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_level": self.optimization_level,
            "config": self.config,
            "pipeline_settings": self.pipeline_settings,
            "mps_available": torch.backends.mps.is_available() if self.is_m3_max else False
        }
    
    def cleanup(self):
        """ìµœì í™” ì •ë¦¬"""
        try:
            if self.is_m3_max and torch.backends.mps.is_available():
                torch.mps.empty_cache()
                logger.info("ğŸ§¹ M3 Max ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"M3 Max ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì „ì—­ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” í•¨ìˆ˜
def initialize_global_memory_manager(device: str = "mps", memory_gb: float = 128.0):
    """
    ì „ì—­ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    
    Args:
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        memory_gb: ì´ ë©”ëª¨ë¦¬ ìš©ëŸ‰
    """
    try:
        import gc
        
        logger.info(f"ğŸ”§ ì „ì—­ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”: {device}, {memory_gb}GB")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        if device == "mps" and torch.backends.mps.is_available():
            # MPS ë©”ëª¨ë¦¬ ì„¤ì •
            logger.info(f"ğŸ M3 Max MPS ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”")
            
            # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            os.environ["PYTORCH_MPS_LOW_WATERMARK_RATIO"] = "0.0"
            
            # Unified Memory ìµœì í™”
            logger.info("ğŸ’¾ Unified Memory ìµœì í™” ì„¤ì •")
            
        logger.info("âœ… ì „ì—­ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

# M3 Max ë©”ëª¨ë¦¬ ì²´í¬ í•¨ìˆ˜
def check_memory_available(required_gb: float = 4.0) -> bool:
    """
    M3 Max ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    
    Args:
        required_gb: í•„ìš”í•œ ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)
    
    Returns:
        bool: ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    """
    try:
        import psutil
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        
        logger.info(f"ğŸ’¾ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory.total / (1024**3):.1f}GB")
        logger.info(f"ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥: {available_gb:.1f}GB") 
        logger.info(f"ğŸ’¾ ìš”êµ¬ì‚¬í•­: {required_gb:.1f}GB")
        
        # MPS ë©”ëª¨ë¦¬ í™•ì¸ (M3 Max)
        if torch.backends.mps.is_available():
            logger.info("ğŸ M3 Max Unified Memory ì‚¬ìš© ì¤‘")
            # Unified Memoryì—ì„œëŠ” ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ì™€ GPU ë©”ëª¨ë¦¬ê°€ í†µí•©
            return available_gb >= required_gb
        
        return available_gb >= required_gb
        
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # ì•ˆì „í•˜ê²Œ True ë°˜í™˜