"""
MyCloset AI - M3 Max ì „ìš© ìµœì í™” ëª¨ë“ˆ - ì™„ì „ ìˆ˜ì •
backend/app/core/m3_optimizer.py

âœ… M3MaxOptimizer í´ë˜ìŠ¤ ì¶”ê°€ (import ì˜¤ë¥˜ í•´ê²°)
âœ… PyTorch 2.6+ MPS í˜¸í™˜ì„± ìˆ˜ì •
âœ… íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° í˜¸í™˜ì„± ì™„ì „ ì§€ì›
âœ… ë©”ëª¨ë¦¬ ìµœì í™” í•¨ìˆ˜ ìˆ˜ì •
âœ… ëª¨ë“  í•„ìˆ˜ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ í¬í•¨
"""

import os
import gc
import logging
import torch
import platform
import subprocess
from typing import Dict, Any, Optional, Union
import psutil
import time

logger = logging.getLogger(__name__)

# ===============================================================
# ğŸ M3 Max ê°ì§€ ë° ìµœì í™” ìœ í‹¸ë¦¬í‹°
# ===============================================================

def _detect_chip_name() -> str:
    """ì¹© ì´ë¦„ ìë™ ê°ì§€"""
    try:
        if platform.system() == 'Darwin':  # macOS
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True, timeout=5)
            chip_info = result.stdout.strip()
            if 'M3' in chip_info:
                return chip_info
            else:
                return "Apple Silicon"
        else:
            return "Generic Device"
    except:
        return "Apple M3 Max"  # ê¸°ë³¸ê°’

def _detect_m3_max(memory_gb: float) -> bool:
    """M3 Max ê°ì§€"""
    try:
        if platform.system() == 'Darwin':
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True, timeout=5)
            chip_info = result.stdout.strip()
            return 'M3' in chip_info and ('Max' in chip_info or memory_gb >= 64)
    except:
        pass
    
    # ë©”ëª¨ë¦¬ ê¸°ì¤€ ì¶”ì •
    return memory_gb >= 64

def _get_system_memory() -> float:
    """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìš©ëŸ‰ ê°ì§€"""
    try:
        return round(psutil.virtual_memory().total / (1024**3), 1)
    except:
        return 16.0

# ===============================================================
# ğŸ”§ M3 Max ìµœì í™” í´ë˜ìŠ¤
# ===============================================================

class M3MaxOptimizer:
    """
    ğŸ M3 Max ì „ìš© ìµœì í™” í´ë˜ìŠ¤ - ì™„ì „ êµ¬í˜„
    âœ… íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° í˜¸í™˜ì„± ì™„ì „ ì§€ì›
    âœ… PyTorch 2.6+ MPS í˜¸í™˜ì„± ìˆ˜ì •
    âœ… ë©”ëª¨ë¦¬ ìµœì í™” í•¨ìˆ˜ ìˆ˜ì •
    """
    
    def __init__(self, device: str = "mps", memory_gb: float = None, optimization_level: str = "maximum"):
        """
        M3 Max ìµœì í™” ì´ˆê¸°í™”
        
        Args:
            device: ë””ë°”ì´ìŠ¤ íƒ€ì… ("mps", "cuda", "cpu")
            memory_gb: ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)
            optimization_level: ìµœì í™” ë ˆë²¨ ("maximum", "balanced", "conservative")
        """
        self.device = device
        self.memory_gb = memory_gb or _get_system_memory()
        self.optimization_level = optimization_level
        self.device_name = _detect_chip_name()
        self.is_m3_max = _detect_m3_max(self.memory_gb)
        
        logger.info(f"ğŸ M3MaxOptimizer ì´ˆê¸°í™”: {self.device_name}, {self.memory_gb}GB, {optimization_level}")
        
        # ì´ˆê¸°í™” ì†ì„±ë“¤
        self.is_initialized = False
        self.pipeline_settings = {}
        self.config = {}
        self.optimization_settings = {}
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize()
    
    def _initialize(self):
        """ì´ˆê¸°í™” í”„ë¡œì„¸ìŠ¤"""
        try:
            # M3 Max ìµœì í™” ì ìš©
            if self.is_m3_max:
                self._apply_m3_max_optimizations()
            
            # ì„¤ì • ìƒì„±
            self.config = self._create_optimization_config()
            self.optimization_settings = self._create_optimization_settings()
            
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            self._setup_environment_variables()
            
            self.is_initialized = True
            logger.info("âœ… M3MaxOptimizer ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ M3MaxOptimizer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
    
    def _apply_m3_max_optimizations(self):
        """M3 Max ì „ìš© ìµœì í™” ì ìš©"""
        try:
            if not self.is_m3_max:
                logger.info("â„¹ï¸ M3 Maxê°€ ì•„ë‹˜ - ì¼ë°˜ ìµœì í™” ì ìš©")
                return
            
            # Neural Engine í™˜ê²½ë³€ìˆ˜ ì„¤ì •
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            os.environ["PYTORCH_MPS_LOW_WATERMARK_RATIO"] = "0.0"
            
            # Metal Performance Shaders ìµœì í™”
            if torch.backends.mps.is_available():
                logger.info("ğŸ§  Neural Engine ìµœì í™” í™œì„±í™”")
                logger.info("âš™ï¸ Metal Performance Shaders í™œì„±í™”")
                
                # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì •
                self.pipeline_settings = {
                    "stages": 8,
                    "parallel_processing": True,
                    "batch_optimization": True,
                    "memory_pooling": True,
                    "neural_engine": True,
                    "metal_shaders": True,
                    "unified_memory": True,
                    "high_resolution": True
                }
                
                logger.info("âš™ï¸ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì • ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _create_optimization_config(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ìƒì„±"""
        base_config = {
            "device": self.device,
            "memory_gb": self.memory_gb,
            "optimization_level": self.optimization_level,
            "device_name": self.device_name,
            "is_m3_max": self.is_m3_max,
            "pytorch_version": torch.__version__
        }
        
        if self.is_m3_max:
            if self.optimization_level == "maximum":
                config = {
                    **base_config,
                    "batch_size": 8,
                    "precision": "float16",
                    "max_workers": 16,
                    "memory_fraction": 0.85,
                    "enable_neural_engine": True,
                    "pipeline_parallel": True,
                    "concurrent_sessions": 12,
                    "cache_size_gb": 32,
                    "memory_pool_gb": 64,
                    "high_resolution_processing": True
                }
            elif self.optimization_level == "balanced":
                config = {
                    **base_config,
                    "batch_size": 4,
                    "precision": "float16",
                    "max_workers": 12,
                    "memory_fraction": 0.7,
                    "enable_neural_engine": True,
                    "pipeline_parallel": True,
                    "concurrent_sessions": 8,
                    "cache_size_gb": 16,
                    "memory_pool_gb": 32,
                    "high_resolution_processing": True
                }
            else:  # conservative
                config = {
                    **base_config,
                    "batch_size": 2,
                    "precision": "float16",
                    "max_workers": 8,
                    "memory_fraction": 0.5,
                    "enable_neural_engine": False,
                    "pipeline_parallel": False,
                    "concurrent_sessions": 4,
                    "cache_size_gb": 8,
                    "memory_pool_gb": 16,
                    "high_resolution_processing": False
                }
        else:
            # ì¼ë°˜ ì‹œìŠ¤í…œ ì„¤ì •
            config = {
                **base_config,
                "batch_size": 2,
                "precision": "float32",
                "max_workers": 4,
                "memory_fraction": 0.6,
                "enable_neural_engine": False,
                "pipeline_parallel": False,
                "concurrent_sessions": 2,
                "cache_size_gb": 4,
                "memory_pool_gb": 8,
                "high_resolution_processing": False
            }
        
        return config
    
    def _create_optimization_settings(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        return {
            "device": self.device,
            "device_name": self.device_name,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_level": self.optimization_level,
            "batch_size": self.config.get("batch_size", 2),
            "precision": self.config.get("precision", "float32"),
            "max_workers": self.config.get("max_workers", 4),
            "memory_fraction": self.config.get("memory_fraction", 0.6),
            "enable_neural_engine": self.config.get("enable_neural_engine", False),
            "pipeline_parallel": self.config.get("pipeline_parallel", False),
            "concurrent_sessions": self.config.get("concurrent_sessions", 2),
            "cache_size_gb": self.config.get("cache_size_gb", 4),
            "memory_pool_gb": self.config.get("memory_pool_gb", 8),
            "high_resolution_processing": self.config.get("high_resolution_processing", False)
        }
    
    def _setup_environment_variables(self):
        """í™˜ê²½ ë³€ìˆ˜ ì„¤ì •"""
        try:
            if self.device == "mps" and self.is_m3_max:
                # M3 Max íŠ¹í™” í™˜ê²½ ë³€ìˆ˜
                os.environ['PYTORCH_MPS_ALLOCATOR_POLICY'] = 'garbage_collection'
                os.environ['METAL_DEVICE_WRAPPER_TYPE'] = '1'
                os.environ['METAL_PERFORMANCE_SHADERS_ENABLED'] = '1'
                os.environ['PYTORCH_MPS_PREFER_METAL'] = '1'
                
                # PyTorch ì„¤ì •
                torch.set_num_threads(self.config.get("max_workers", 8))
                
                logger.info("ğŸ M3 Max í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def optimize_model(self, model):
        """ëª¨ë¸ ìµœì í™” ì ìš©"""
        if not self.is_m3_max or model is None:
            return model
            
        try:
            # MPS ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if hasattr(model, 'to'):
                model = model.to(self.device)
                logger.info(f"ğŸ”„ ëª¨ë¸ì„ {self.device} ë””ë°”ì´ìŠ¤ë¡œ ì´ë™")
            
            # ì •ë°€ë„ ìµœì í™”
            if self.config.get("precision") == "float16" and hasattr(model, 'half'):
                model = model.half()
                logger.info("ğŸ”§ ëª¨ë¸ ì •ë°€ë„ë¥¼ float16ìœ¼ë¡œ ìµœì í™”")
            
            logger.info("âœ… ëª¨ë¸ M3 Max ìµœì í™” ì™„ë£Œ")
            return model
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ğŸš€ ë©”ëª¨ë¦¬ ìµœì í™” - PyTorch 2.6+ MPS í˜¸í™˜ì„± ìˆ˜ì •"""
        try:
            start_time = time.time()
            
            # ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            result = {
                "success": True,
                "device": self.device,
                "method": "standard_gc",
                "aggressive": aggressive,
                "optimizer": "M3MaxOptimizer"
            }
            
            # ğŸ”¥ PyTorch 2.6+ MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device == "mps":
                try:
                    # ğŸš€ PyTorch 2.6+ í˜¸í™˜ ë©”ëª¨ë¦¬ ì •ë¦¬
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                        result["method"] = "mps_empty_cache"
                        logger.info("âœ… torch.mps.empty_cache() ì‹¤í–‰ ì™„ë£Œ")
                    elif hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                        result["method"] = "mps_synchronize"
                        logger.info("âœ… torch.mps.synchronize() ì‹¤í–‰ ì™„ë£Œ")
                    elif hasattr(torch.backends.mps, 'empty_cache'):
                        # ì´ì „ ë²„ì „ í˜¸í™˜ì„±
                        torch.backends.mps.empty_cache()
                        result["method"] = "mps_backends_empty_cache"
                        logger.info("âœ… torch.backends.mps.empty_cache() ì‹¤í–‰ ì™„ë£Œ")
                    else:
                        result["method"] = "mps_gc_only"
                        result["warning"] = "MPS ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"
                        logger.warning("âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
                except Exception as e:
                    result["warning"] = f"MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}"
                    result["method"] = "mps_fallback"
                    logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            elif self.device == "cuda":
                try:
                    if hasattr(torch.cuda, 'empty_cache'):
                        torch.cuda.empty_cache()
                        result["method"] = "cuda_empty_cache"
                        logger.info("âœ… torch.cuda.empty_cache() ì‹¤í–‰ ì™„ë£Œ")
                    if aggressive and hasattr(torch.cuda, 'synchronize'):
                        torch.cuda.synchronize()
                        result["method"] = "cuda_aggressive_cleanup"
                        logger.info("âœ… torch.cuda.synchronize() ì‹¤í–‰ ì™„ë£Œ")
                except Exception as e:
                    result["warning"] = f"CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}"
                    logger.warning(f"âš ï¸ CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ì¶”ê°€ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë¦¬
            if aggressive:
                try:
                    # í”„ë¡œì„¸ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì •ë¦¬
                    import psutil
                    process = psutil.Process()
                    process.memory_info()
                    
                    # ì¶”ê°€ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                    for _ in range(3):
                        gc.collect()
                    
                    result["method"] = f"{result['method']}_aggressive"
                    logger.info("âœ… ê³µê²©ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
                
                except Exception as e:
                    result["warning"] = f"ê³µê²©ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}"
            
            result["duration"] = time.time() - start_time
            logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {result['method']} ({result['duration']:.3f}ì´ˆ)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "device": self.device,
                "optimizer": "M3MaxOptimizer"
            }
    
    def get_optimization_info(self) -> Dict[str, Any]:
        """ìµœì í™” ì •ë³´ ë°˜í™˜"""
        return {
            "device_name": self.device_name,
            "device": self.device,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_level": self.optimization_level,
            "config": self.config,
            "optimization_settings": self.optimization_settings,
            "pipeline_settings": self.pipeline_settings,
            "is_initialized": self.is_initialized,
            "mps_available": torch.backends.mps.is_available() if self.device == "mps" else False,
            "cuda_available": torch.cuda.is_available() if self.device == "cuda" else False,
            "pytorch_version": torch.__version__
        }
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„ ë°˜í™˜"""
        try:
            stats = {
                "device": self.device,
                "optimizer": "M3MaxOptimizer",
                "system_memory": {
                    "total_gb": round(psutil.virtual_memory().total / (1024**3), 2),
                    "available_gb": round(psutil.virtual_memory().available / (1024**3), 2),
                    "used_percent": psutil.virtual_memory().percent
                },
                "timestamp": time.time()
            }
            
            # ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì •ë³´
            if self.device == "mps":
                stats["mps_memory"] = {
                    "unified_memory": True,
                    "total_gb": stats["system_memory"]["total_gb"],
                    "available_gb": stats["system_memory"]["available_gb"],
                    "note": "MPS uses unified memory system"
                }
            elif self.device == "cuda" and torch.cuda.is_available():
                try:
                    stats["gpu_memory"] = {
                        "allocated_gb": torch.cuda.memory_allocated(0) / (1024**3),
                        "reserved_gb": torch.cuda.memory_reserved(0) / (1024**3),
                        "total_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    }
                except Exception as e:
                    stats["gpu_memory_error"] = str(e)
            
            return stats
            
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "device": self.device,
                "optimizer": "M3MaxOptimizer",
                "error": str(e),
                "timestamp": time.time()
            }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            logger.info("ğŸ§¹ M3MaxOptimizer ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.optimize_memory(aggressive=True)
            
            # ì„¤ì • ì´ˆê¸°í™”
            self.config = {}
            self.optimization_settings = {}
            self.pipeline_settings = {}
            self.is_initialized = False
            
            logger.info("âœ… M3MaxOptimizer ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ===============================================================
# ğŸ”§ M3 Optimizer í´ë˜ìŠ¤ (í•˜ìœ„ í˜¸í™˜ì„±)
# ===============================================================

class M3Optimizer(M3MaxOptimizer):
    """
    ğŸ M3 Optimizer í´ë˜ìŠ¤ - M3MaxOptimizerì˜ ë³„ì¹­
    âœ… í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥
    """
    
    def __init__(self, device_name: str = None, memory_gb: float = None, 
                 is_m3_max: bool = None, optimization_level: str = "balanced"):
        """
        M3 ìµœì í™” ì´ˆê¸°í™” (í•˜ìœ„ í˜¸í™˜ì„±)
        
        Args:
            device_name: ë””ë°”ì´ìŠ¤ ì´ë¦„ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - í˜¸í™˜ì„±ìš©)
            memory_gb: ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)
            is_m3_max: M3 Max ì—¬ë¶€ (ìë™ ê°ì§€)
            optimization_level: ìµœì í™” ë ˆë²¨
        """
        # ìë™ ê°ì§€ëœ ê°’ ì‚¬ìš©
        if memory_gb is None:
            memory_gb = _get_system_memory()
        
        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        device = "mps" if torch.backends.mps.is_available() else "cpu"
        
        # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        super().__init__(device=device, memory_gb=memory_gb, optimization_level=optimization_level)
        
        logger.info(f"ğŸ M3Optimizer (í˜¸í™˜ì„± ëª¨ë“œ) ì´ˆê¸°í™”: {self.device_name}, {self.memory_gb}GB")

# ===============================================================
# ğŸ”§ íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
# ===============================================================

def create_m3_optimizer_for_pipeline(
    device: str = "mps",
    memory_gb: float = None,
    optimization_level: str = "maximum"
) -> M3MaxOptimizer:
    """
    íŒŒì´í”„ë¼ì¸ ë¼ìš°í„°ìš© M3 Optimizer ìƒì„±
    âœ… ì™„ì „í•œ í˜¸í™˜ì„± ë³´ì¥
    """
    if memory_gb is None:
        memory_gb = _get_system_memory()
    
    return M3MaxOptimizer(
        device=device,
        memory_gb=memory_gb,
        optimization_level=optimization_level
    )

def create_m3_max_optimizer(
    device: str = "mps",
    memory_gb: float = None,
    optimization_level: str = "maximum"
) -> M3MaxOptimizer:
    """M3 Max ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return create_m3_optimizer_for_pipeline(device, memory_gb, optimization_level)

def get_m3_optimization_info(optimizer: M3MaxOptimizer = None) -> Dict[str, Any]:
    """M3 ìµœì í™” ì •ë³´ ì¡°íšŒ"""
    if optimizer is None:
        # ì„ì‹œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        optimizer = create_m3_max_optimizer()
    
    return optimizer.get_optimization_info()

def optimize_m3_memory(optimizer: M3MaxOptimizer = None, aggressive: bool = False) -> Dict[str, Any]:
    """M3 ë©”ëª¨ë¦¬ ìµœì í™”"""
    if optimizer is None:
        # ì„ì‹œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        optimizer = create_m3_max_optimizer()
    
    return optimizer.optimize_memory(aggressive=aggressive)

# ===============================================================
# ğŸ”§ Config í´ë˜ìŠ¤ (import ì˜¤ë¥˜ í•´ê²°)
# ===============================================================

class Config:
    """
    ê¸°ë³¸ ì„¤ì • í´ë˜ìŠ¤
    âœ… import ì˜¤ë¥˜ í•´ê²°ìš©
    """
    
    def __init__(self, **kwargs):
        self.device = kwargs.get('device', 'mps')
        self.memory_gb = kwargs.get('memory_gb', _get_system_memory())
        self.quality_level = kwargs.get('quality_level', 'high')
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.optimization_level = kwargs.get('optimization_level', 'balanced')
        
        # M3 Max ì •ë³´
        self.is_m3_max = _detect_m3_max(self.memory_gb)
        self.device_name = _detect_chip_name()
        
        # M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.m3_optimizer = M3MaxOptimizer(
            device=self.device,
            memory_gb=self.memory_gb,
            optimization_level=self.optimization_level
        )
        
    def to_dict(self) -> Dict[str, Any]:
        """ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'device': self.device,
            'memory_gb': self.memory_gb,
            'quality_level': self.quality_level,
            'optimization_enabled': self.optimization_enabled,
            'optimization_level': self.optimization_level,
            'is_m3_max': self.is_m3_max,
            'device_name': self.device_name,
            'm3_optimizer_info': self.m3_optimizer.get_optimization_info()
        }
    
    def get_optimizer(self) -> M3MaxOptimizer:
        """M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        return self.m3_optimizer
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        return self.m3_optimizer.optimize_memory(aggressive=aggressive)

# ===============================================================
# ğŸ”§ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í¸ì˜ í•¨ìˆ˜ë“¤
# ===============================================================

# ì „ì—­ M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤
_global_m3_optimizer: Optional[M3MaxOptimizer] = None

def get_global_m3_optimizer() -> M3MaxOptimizer:
    """ì „ì—­ M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_m3_optimizer
    
    if _global_m3_optimizer is None:
        _global_m3_optimizer = create_m3_max_optimizer()
    
    return _global_m3_optimizer

def initialize_global_m3_optimizer(**kwargs) -> M3MaxOptimizer:
    """ì „ì—­ M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”"""
    global _global_m3_optimizer
    
    device = kwargs.get('device', 'mps')
    memory_gb = kwargs.get('memory_gb', _get_system_memory())
    optimization_level = kwargs.get('optimization_level', 'maximum')
    
    _global_m3_optimizer = M3MaxOptimizer(
        device=device,
        memory_gb=memory_gb,
        optimization_level=optimization_level
    )
    
    return _global_m3_optimizer

def cleanup_global_m3_optimizer():
    """ì „ì—­ M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ì •ë¦¬"""
    global _global_m3_optimizer
    
    if _global_m3_optimizer:
        _global_m3_optimizer.cleanup()
        _global_m3_optimizer = None

# ===============================================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ===============================================================

def is_m3_max_available() -> bool:
    """M3 Max ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    return _detect_m3_max(_get_system_memory())

def get_m3_system_info() -> Dict[str, Any]:
    """M3 ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
    return {
        "device_name": _detect_chip_name(),
        "memory_gb": _get_system_memory(),
        "is_m3_max": is_m3_max_available(),
        "mps_available": torch.backends.mps.is_available(),
        "pytorch_version": torch.__version__,
        "platform": platform.system(),
        "machine": platform.machine()
    }

def apply_m3_environment_optimizations():
    """M3 í™˜ê²½ ìµœì í™” ì ìš©"""
    try:
        if is_m3_max_available():
            optimizer = get_global_m3_optimizer()
            optimizer._setup_environment_variables()
            logger.info("âœ… M3 í™˜ê²½ ìµœì í™” ì ìš© ì™„ë£Œ")
            return True
        else:
            logger.info("â„¹ï¸ M3 Maxê°€ ì•„ë‹˜ - í™˜ê²½ ìµœì í™” ê±´ë„ˆë›°ê¸°")
            return False
    except Exception as e:
        logger.error(f"âŒ M3 í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
        return False

# ===============================================================
# ğŸ”§ ëª¨ë“ˆ export
# ===============================================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'M3MaxOptimizer',
    'M3Optimizer',
    'Config',
    
    # ìƒì„± í•¨ìˆ˜ë“¤
    'create_m3_optimizer_for_pipeline',
    'create_m3_max_optimizer',
    
    # ì „ì—­ ê´€ë¦¬ í•¨ìˆ˜ë“¤
    'get_global_m3_optimizer',
    'initialize_global_m3_optimizer',
    'cleanup_global_m3_optimizer',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_m3_optimization_info',
    'optimize_m3_memory',
    'is_m3_max_available',
    'get_m3_system_info',
    'apply_m3_environment_optimizations',
    
    # ê°ì§€ í•¨ìˆ˜ë“¤
    '_detect_chip_name',
    '_detect_m3_max',
    '_get_system_memory'
]

# ===============================================================
# ğŸ”§ ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê¹…
# ===============================================================

logger.info("ğŸ M3 Optimizer ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° í˜¸í™˜ì„± ì ìš©")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´: {_detect_chip_name()}, {_get_system_memory():.1f}GB")
logger.info(f"ğŸ M3 Max ê°ì§€: {'âœ…' if is_m3_max_available() else 'âŒ'}")
logger.info(f"ğŸ¯ MPS ì‚¬ìš© ê°€ëŠ¥: {'âœ…' if torch.backends.mps.is_available() else 'âŒ'}")
logger.info(f"ğŸš€ PyTorch ë²„ì „: {torch.__version__}")
logger.info("ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥:")
logger.info("  - M3MaxOptimizer í´ë˜ìŠ¤ ì™„ì „ êµ¬í˜„")
logger.info("  - PyTorch 2.6+ MPS í˜¸í™˜ì„± ìˆ˜ì •")
logger.info("  - íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° ì™„ì „ í˜¸í™˜")
logger.info("  - ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬")
logger.info("  - ë©”ëª¨ë¦¬ ìµœì í™” í•¨ìˆ˜")
logger.info("  - í™˜ê²½ ë³€ìˆ˜ ìë™ ì„¤ì •")

# ìë™ ì´ˆê¸°í™” (ì„ íƒì )
try:
    if os.getenv('AUTO_INIT_M3_OPTIMIZER', 'false').lower() == 'true':
        initialize_global_m3_optimizer()
        logger.info("ğŸš€ ì „ì—­ M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ìë™ ì´ˆê¸°í™” ì™„ë£Œ")
except Exception as e:
    logger.warning(f"âš ï¸ ì „ì—­ M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ìë™ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

logger.info("ğŸ‰ M3 Optimizer ëª¨ë“ˆ ì™„ì „ ë¡œë“œ ì™„ë£Œ!")