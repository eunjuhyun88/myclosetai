# backend/app/core/m3_optimizer.py
"""
M3 Max ì „ìš© ìµœì í™” ëª¨ë“ˆ
"""
import os
import logging
import torch
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

class M3Optimizer:
    """
    Apple M3 Max ì „ìš© ìµœì í™” í´ë˜ìŠ¤
    """
    
    def __init__(self, device_name: str, memory_gb: float, is_m3_max: bool, optimization_level: str):
        """
        M3 ìµœì í™” ì´ˆê¸°í™”
        
        Args:
            device_name: ë””ë°”ì´ìŠ¤ ì´ë¦„ (ì˜ˆ: "Apple M3 Max")
            memory_gb: ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)
            is_m3_max: M3 Max ì—¬ë¶€
            optimization_level: ìµœì í™” ë ˆë²¨ ("maximum", "balanced", "conservative")
        """
        self.device_name = device_name
        self.memory_gb = memory_gb
        self.is_m3_max = is_m3_max
        self.optimization_level = optimization_level
        
        logger.info(f"ğŸ M3Optimizer ì´ˆê¸°í™”: {device_name}, {memory_gb}GB, {optimization_level}")
        
        # M3 Max ì „ìš© ì„¤ì •
        if is_m3_max:
            self._apply_m3_max_optimizations()
        
        self.config = self._create_optimization_config()
    
    def _apply_m3_max_optimizations(self):
        """M3 Max ì „ìš© ìµœì í™” ì ìš©"""
        try:
            # Neural Engine í™˜ê²½ë³€ìˆ˜ ì„¤ì •
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            os.environ["PYTORCH_MPS_LOW_WATERMARK_RATIO"] = "0.0"
            
            # Metal Performance Shaders ìµœì í™”
            if torch.backends.mps.is_available():
                logger.info("ğŸ§  Neural Engine ìµœì í™” í™œì„±í™”")
                logger.info("âš™ï¸ Metal Performance Shaders í™œì„±í™”")
                
                # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì •
                self.pipeline_settings = {
                    "stages": 8,
                    "parallel_processing": True,
                    "batch_optimization": True,
                    "memory_pooling": True,
                    "neural_engine": True
                }
                
                logger.info("âš™ï¸ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì • ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _create_optimization_config(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ìƒì„±"""
        base_config = {
            "device": "mps" if self.is_m3_max else "cpu",
            "memory_gb": self.memory_gb,
            "optimization_level": self.optimization_level
        }
        
        if self.is_m3_max:
            if self.optimization_level == "maximum":
                config = {
                    **base_config,
                    "batch_size": 4,
                    "precision": "float16",
                    "max_workers": 12,
                    "memory_fraction": 0.8,
                    "enable_neural_engine": True,
                    "pipeline_parallel": True
                }
            elif self.optimization_level == "balanced":
                config = {
                    **base_config,
                    "batch_size": 2,
                    "precision": "float16",
                    "max_workers": 8,
                    "memory_fraction": 0.6,
                    "enable_neural_engine": True,
                    "pipeline_parallel": False
                }
            else:  # conservative
                config = {
                    **base_config,
                    "batch_size": 1,
                    "precision": "float32",
                    "max_workers": 4,
                    "memory_fraction": 0.4,
                    "enable_neural_engine": False,
                    "pipeline_parallel": False
                }
        else:
            config = base_config
        
        return config
    
    def optimize_model(self, model):
        """ëª¨ë¸ ìµœì í™” ì ìš©"""
        if not self.is_m3_max or model is None:
            return model
            
        try:
            # MPS ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if hasattr(model, 'to'):
                model = model.to('mps')
                logger.info("ğŸ”„ ëª¨ë¸ì„ MPS ë””ë°”ì´ìŠ¤ë¡œ ì´ë™")
            
            # ì •ë°€ë„ ìµœì í™”
            if self.config.get("precision") == "float16" and hasattr(model, 'half'):
                model = model.half()
                logger.info("ğŸ”§ ëª¨ë¸ ì •ë°€ë„ë¥¼ float16ìœ¼ë¡œ ìµœì í™”")
            
            logger.info("âœ… ëª¨ë¸ M3 Max ìµœì í™” ì™„ë£Œ")
            return model
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model
    
    def get_optimization_info(self) -> Dict[str, Any]:
        """ìµœì í™” ì •ë³´ ë°˜í™˜"""
        return {
            "device_name": self.device_name,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_level": self.optimization_level,
            "config": self.config,
            "mps_available": torch.backends.mps.is_available() if self.is_m3_max else False
        }

# ================================================================
# backend/app/ai_pipeline/utils/memory_manager.pyì— ì¶”ê°€í•  í•¨ìˆ˜

def initialize_global_memory_manager(device: str = "mps", memory_gb: float = 128.0):
    """
    ì „ì—­ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    
    Args:
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        memory_gb: ì´ ë©”ëª¨ë¦¬ ìš©ëŸ‰
    """
    try:
        import gc
        import torch
        
        logger.info(f"ğŸ”§ ì „ì—­ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”: {device}, {memory_gb}GB")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        if device == "mps" and torch.backends.mps.is_available():
            # MPS ë©”ëª¨ë¦¬ ì„¤ì •
            logger.info(f"ğŸ M3 Max MPS ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”")
            
            # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            os.environ["PYTORCH_MPS_LOW_WATERMARK_RATIO"] = "0.0"
            
            # Unified Memory ìµœì í™”
            logger.info("ğŸ’¾ Unified Memory ìµœì í™” ì„¤ì •")
            
        logger.info("âœ… ì „ì—­ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

# ================================================================
# backend/app/ai_pipeline/utils/model_loader.pyì— ì¶”ê°€í•  í´ë˜ìŠ¤ì™€ í•¨ìˆ˜

class ModelFormat:
    """AI ëª¨ë¸ í˜•ì‹ ì •ì˜"""
    
    PYTORCH = "pytorch"
    ONNX = "onnx" 
    TENSORRT = "tensorrt"
    COREML = "coreml"  # Apple Core ML for M3 Max
    SAFETENSORS = "safetensors"
    
    @classmethod
    def get_optimized_format(cls, device: str = "mps") -> str:
        """ë””ë°”ì´ìŠ¤ì— ìµœì í™”ëœ ëª¨ë¸ í˜•ì‹ ë°˜í™˜"""
        if device == "mps":
            return cls.COREML  # M3 Maxì—ì„œëŠ” Core ML ì¶”ì²œ
        elif device == "cuda":
            return cls.TENSORRT
        return cls.PYTORCH
    
    @classmethod
    def is_supported(cls, format_name: str) -> bool:
        """ì§€ì›ë˜ëŠ” í˜•ì‹ì¸ì§€ í™•ì¸"""
        supported_formats = [cls.PYTORCH, cls.ONNX, cls.COREML, cls.SAFETENSORS]
        return format_name.lower() in [f.lower() for f in supported_formats]

def initialize_global_model_loader(device: str = "mps"):
    """ì „ì—­ ëª¨ë¸ ë¡œë” ì´ˆê¸°í™”"""
    try:
        logger.info(f"ğŸ¤– ì „ì—­ ModelLoader ì´ˆê¸°í™”: {device}")
        
        # ê¸€ë¡œë²Œ ëª¨ë¸ ë¡œë” ì„¤ì •
        loader_config = {
            "device": device,
            "cache_enabled": True,
            "lazy_loading": True,
            "memory_efficient": True
        }
        
        if device == "mps":
            loader_config.update({
                "use_neural_engine": True,
                "use_unified_memory": True,
                "optimization_level": "maximum"
            })
        
        logger.info("âœ… ì „ì—­ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ")
        return loader_config
        
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

# ================================================================
# backend/app/core/gpu_config.pyì— ì¶”ê°€í•  í•¨ìˆ˜

def check_memory_available(required_gb: float = 4.0) -> bool:
    """
    M3 Max ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    
    Args:
        required_gb: í•„ìš”í•œ ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)
    
    Returns:
        bool: ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    """
    try:
        import psutil
        import torch
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        
        logger.info(f"ğŸ’¾ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory.total / (1024**3):.1f}GB")
        logger.info(f"ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥: {available_gb:.1f}GB") 
        logger.info(f"ğŸ’¾ ìš”êµ¬ì‚¬í•­: {required_gb:.1f}GB")
        
        # MPS ë©”ëª¨ë¦¬ í™•ì¸ (M3 Max)
        if torch.backends.mps.is_available():
            logger.info("ğŸ M3 Max Unified Memory ì‚¬ìš© ì¤‘")
            # Unified Memoryì—ì„œëŠ” ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ì™€ GPU ë©”ëª¨ë¦¬ê°€ í†µí•©
            return available_gb >= required_gb
        
        return available_gb >= required_gb
        
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # ì•ˆì „í•˜ê²Œ True ë°˜í™˜