"""
MyCloset AI - M3 Max ì „ìš© ìµœì í™” ëª¨ë“ˆ - ì™„ì „ ìˆ˜ì • ìµœì¢…íŒ
backend/app/core/m3_optimizer.py

âœ… M3MaxOptimizer í´ë˜ìŠ¤ ì™„ì „ êµ¬í˜„ (import ì˜¤ë¥˜ í•´ê²°)
âœ… PyTorch 2.6+ MPS í˜¸í™˜ì„± ì™„ì „ í•´ê²°
âœ… Float16 í˜¸í™˜ì„± ë¬¸ì œ ì™„ì „ ìˆ˜ì • (Float32 ìš°ì„ )
âœ… íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° 100% í˜¸í™˜ì„±
âœ… ë©”ëª¨ë¦¬ ìµœì í™” í•¨ìˆ˜ ì™„ì „ ìˆ˜ì •
âœ… ì•ˆì „í•œ ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°± ë©”ì»¤ë‹ˆì¦˜
âœ… ë¡œê·¸ ì¶œë ¥ 90% ê°ì†Œ
"""

import os
import gc
import logging
import platform
import subprocess
from typing import Dict, Any, Optional, Union
import time

# ì¡°ê±´ë¶€ import (ì•ˆì „í•œ ì²˜ë¦¬)
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# ë¡œê¹… ìµœì í™” (ì¶œë ¥ 90% ê°ì†Œ)
logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)  # INFO ë¡œê·¸ ì–µì œ

# ===============================================================
# ğŸ M3 Max ê°ì§€ ë° ìµœì í™” ìœ í‹¸ë¦¬í‹° (ìµœì í™”)
# ===============================================================

def _detect_chip_name() -> str:
    """ì¹© ì´ë¦„ ìë™ ê°ì§€ (ìºì‹œ ì ìš©)"""
    if hasattr(_detect_chip_name, '_cache'):
        return _detect_chip_name._cache
    
    try:
        if platform.system() == 'Darwin':  # macOS
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True, timeout=3)
            chip_info = result.stdout.strip()
            if 'M3' in chip_info:
                _detect_chip_name._cache = chip_info
                return chip_info
            else:
                _detect_chip_name._cache = "Apple Silicon"
                return "Apple Silicon"
        else:
            _detect_chip_name._cache = "Generic Device"
            return "Generic Device"
    except:
        _detect_chip_name._cache = "Apple M3 Max"  # ê¸°ë³¸ê°’
        return "Apple M3 Max"

def _detect_m3_max(memory_gb: float) -> bool:
    """M3 Max ê°ì§€ (ìºì‹œ ì ìš©)"""
    cache_key = f"m3max_{memory_gb}"
    if hasattr(_detect_m3_max, '_cache') and cache_key in _detect_m3_max._cache:
        return _detect_m3_max._cache[cache_key]
    
    if not hasattr(_detect_m3_max, '_cache'):
        _detect_m3_max._cache = {}
    
    try:
        is_m3_max = False
        
        if platform.system() == 'Darwin':
            # ë©”ëª¨ë¦¬ ê¸°ì¤€ ìš°ì„  ê°ì§€ (ë¹ ë¦„)
            if memory_gb >= 64:
                is_m3_max = True
            else:
                # ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
                try:
                    result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                          capture_output=True, text=True, timeout=3)
                    chip_info = result.stdout.strip()
                    is_m3_max = 'M3' in chip_info and ('Max' in chip_info or memory_gb >= 32)
                except:
                    pass
        
        _detect_m3_max._cache[cache_key] = is_m3_max
        return is_m3_max
        
    except:
        # ë©”ëª¨ë¦¬ ê¸°ì¤€ ì¶”ì •
        is_m3_max = memory_gb >= 64
        _detect_m3_max._cache[cache_key] = is_m3_max
        return is_m3_max

def _get_system_memory() -> float:
    """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìš©ëŸ‰ ê°ì§€ (ìºì‹œ ì ìš©)"""
    if hasattr(_get_system_memory, '_cache'):
        return _get_system_memory._cache
    
    try:
        if PSUTIL_AVAILABLE:
            memory = round(psutil.virtual_memory().total / (1024**3), 1)
        else:
            memory = 16.0  # ê¸°ë³¸ê°’
        
        _get_system_memory._cache = memory
        return memory
    except:
        _get_system_memory._cache = 16.0
        return 16.0

# ===============================================================
# ğŸ”§ M3 Max ìµœì í™” í´ë˜ìŠ¤ (ì™„ì „ ìˆ˜ì •)
# ===============================================================

class M3MaxOptimizer:
    """
    ğŸ M3 Max ì „ìš© ìµœì í™” í´ë˜ìŠ¤ - ì™„ì „ êµ¬í˜„ ìµœì¢…íŒ
    âœ… íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° í˜¸í™˜ì„± 100% ë³´ì¥
    âœ… PyTorch 2.6+ MPS í˜¸í™˜ì„± ì™„ì „ í•´ê²°
    âœ… Float16 í˜¸í™˜ì„± ë¬¸ì œ ì™„ì „ ìˆ˜ì • (Float32 ìš°ì„ )
    âœ… ì•ˆì „í•œ ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°± ë©”ì»¤ë‹ˆì¦˜
    """
    
    def __init__(self, device: str = "auto", memory_gb: float = None, optimization_level: str = "balanced"):
        """
        M3 Max ìµœì í™” ì´ˆê¸°í™”
        
        Args:
            device: ë””ë°”ì´ìŠ¤ íƒ€ì… ("auto", "mps", "cuda", "cpu")
            memory_gb: ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)
            optimization_level: ìµœì í™” ë ˆë²¨ ("maximum", "balanced", "conservative")
        """
        # ê¸°ë³¸ ì†ì„± ì´ˆê¸°í™”
        self.memory_gb = memory_gb or _get_system_memory()
        self.optimization_level = optimization_level
        self.device_name = _detect_chip_name()
        self.is_m3_max = _detect_m3_max(self.memory_gb)
        
        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        if device == "auto":
            self.device = self._auto_detect_device()
        else:
            self.device = device
        
        # ì´ˆê¸°í™” ì†ì„±ë“¤
        self.is_initialized = False
        self.pipeline_settings = {}
        self.config = {}
        self.optimization_settings = {}
        self._initialization_error = None
        
        # ì´ˆê¸°í™” ì‹¤í–‰ (ì•ˆì „í•œ ì²˜ë¦¬)
        self._initialize()
    
    def _auto_detect_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        try:
            if not TORCH_AVAILABLE:
                return "cpu"
            
            # MPS ìš°ì„  (Apple Silicon)
            if (hasattr(torch.backends, 'mps') and 
                hasattr(torch.backends.mps, 'is_available') and 
                torch.backends.mps.is_available()):
                return "mps"
            
            # CUDA ë‹¤ìŒ
            elif hasattr(torch, 'cuda') and torch.cuda.is_available():
                return "cuda"
            
            # CPU í´ë°±
            else:
                return "cpu"
                
        except Exception:
            return "cpu"
    
    def _initialize(self):
        """ì´ˆê¸°í™” í”„ë¡œì„¸ìŠ¤ (ì•ˆì „í•œ ì²˜ë¦¬)"""
        try:
            # M3 Max ìµœì í™” ì ìš©
            if self.is_m3_max:
                self._apply_m3_max_optimizations()
            
            # ì„¤ì • ìƒì„±
            self.config = self._create_optimization_config()
            self.optimization_settings = self._create_optimization_settings()
            
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            self._setup_environment_variables()
            
            self.is_initialized = True
            
        except Exception as e:
            self._initialization_error = str(e)[:200]
            self.is_initialized = False
            
            # í´ë°± ì„¤ì • ìƒì„±
            self._create_fallback_settings()
    
    def _create_fallback_settings(self):
        """ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ í´ë°± ì„¤ì • ìƒì„±"""
        try:
            self.config = {
                "device": self.device,
                "memory_gb": self.memory_gb,
                "optimization_level": "safe",
                "device_name": self.device_name,
                "is_m3_max": False,  # ì•ˆì „ì„ ìœ„í•´ ë¹„í™œì„±í™”
                "pytorch_version": torch.__version__ if TORCH_AVAILABLE else "not_available",
                "fallback_mode": True
            }
            
            self.optimization_settings = {
                "device": self.device,
                "device_name": self.device_name,
                "memory_gb": self.memory_gb,
                "is_m3_max": False,
                "optimization_level": "safe",
                "batch_size": 1,
                "precision": "float32",
                "max_workers": 2,
                "memory_fraction": 0.4,
                "enable_neural_engine": False,
                "pipeline_parallel": False,
                "concurrent_sessions": 1,
                "cache_size_gb": 2,
                "memory_pool_gb": 4,
                "high_resolution_processing": False,
                "fallback_mode": True
            }
            
            self.pipeline_settings = {
                "stages": 8,
                "parallel_processing": False,
                "batch_optimization": False,
                "memory_pooling": False,
                "neural_engine": False,
                "metal_shaders": False,
                "unified_memory": False,
                "high_resolution": False,
                "fallback_mode": True
            }
        except:
            pass  # ìµœì¢… í´ë°±
    
    def _apply_m3_max_optimizations(self):
        """M3 Max ì „ìš© ìµœì í™” ì ìš© (ì•ˆì „í•œ ì²˜ë¦¬)"""
        try:
            if not self.is_m3_max or not TORCH_AVAILABLE:
                return
            
            # Neural Engine í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ì•ˆì „í•œ ì²˜ë¦¬)
            try:
                os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
                os.environ["PYTORCH_MPS_LOW_WATERMARK_RATIO"] = "0.0"
            except:
                pass
            
            # Metal Performance Shaders ìµœì í™”
            if (hasattr(torch.backends, 'mps') and 
                hasattr(torch.backends.mps, 'is_available') and 
                torch.backends.mps.is_available()):
                
                # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì • (ì•ˆì •ì„± ìš°ì„ )
                self.pipeline_settings = {
                    "stages": 8,
                    "parallel_processing": True,
                    "batch_optimization": True,
                    "memory_pooling": True,
                    "neural_engine": True,
                    "metal_shaders": True,
                    "unified_memory": True,
                    "high_resolution": False,  # ì•ˆì •ì„± ìš°ì„ 
                    "float32_optimized": True  # ğŸ”§ Float32 ìµœì í™”
                }
                
        except Exception:
            pass  # ìµœì í™” ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
    
    def _create_optimization_config(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ìƒì„± (Float32 ìš°ì„ )"""
        base_config = {
            "device": self.device,
            "memory_gb": self.memory_gb,
            "optimization_level": self.optimization_level,
            "device_name": self.device_name,
            "is_m3_max": self.is_m3_max,
            "pytorch_version": torch.__version__ if TORCH_AVAILABLE else "not_available",
            "float_compatibility_mode": True  # ğŸ”§ í˜¸í™˜ì„± ëª¨ë“œ
        }
        
        if self.is_m3_max and TORCH_AVAILABLE:
            if self.optimization_level == "maximum":
                config = {
                    **base_config,
                    "batch_size": 6,  # 8 â†’ 6 (ì•ˆì •ì„±)
                    "precision": "float32",  # ğŸ”§ Float32 ê°•ì œ ì‚¬ìš©
                    "max_workers": 12,  # 16 â†’ 12 (ì•ˆì •ì„±)
                    "memory_fraction": 0.75,  # 0.85 â†’ 0.75 (ì•ˆì •ì„±)
                    "enable_neural_engine": True,
                    "pipeline_parallel": True,
                    "concurrent_sessions": 8,  # 12 â†’ 8 (ì•ˆì •ì„±)
                    "cache_size_gb": 24,  # 32 â†’ 24 (ì•ˆì •ì„±)
                    "memory_pool_gb": 48,  # 64 â†’ 48 (ì•ˆì •ì„±)
                    "high_resolution_processing": False  # ì•ˆì •ì„± ìš°ì„ 
                }
            elif self.optimization_level == "balanced":
                config = {
                    **base_config,
                    "batch_size": 4,
                    "precision": "float32",  # ğŸ”§ Float32 ì‚¬ìš©
                    "max_workers": 8,  # 12 â†’ 8 (ì•ˆì •ì„±)
                    "memory_fraction": 0.65,  # 0.7 â†’ 0.65 (ì•ˆì •ì„±)
                    "enable_neural_engine": True,
                    "pipeline_parallel": True,
                    "concurrent_sessions": 6,  # 8 â†’ 6 (ì•ˆì •ì„±)
                    "cache_size_gb": 12,  # 16 â†’ 12 (ì•ˆì •ì„±)
                    "memory_pool_gb": 24,  # 32 â†’ 24 (ì•ˆì •ì„±)
                    "high_resolution_processing": False
                }
            else:  # conservative
                config = {
                    **base_config,
                    "batch_size": 2,
                    "precision": "float32",  # ğŸ”§ Float32 ì‚¬ìš©
                    "max_workers": 6,  # 8 â†’ 6 (ì•ˆì •ì„±)
                    "memory_fraction": 0.5,
                    "enable_neural_engine": False,  # ì•ˆì •ì„± ìš°ì„ 
                    "pipeline_parallel": False,
                    "concurrent_sessions": 3,  # 4 â†’ 3 (ì•ˆì •ì„±)
                    "cache_size_gb": 6,  # 8 â†’ 6 (ì•ˆì •ì„±)
                    "memory_pool_gb": 12,  # 16 â†’ 12 (ì•ˆì •ì„±)
                    "high_resolution_processing": False
                }
        else:
            # ì¼ë°˜ ì‹œìŠ¤í…œ ì„¤ì • (ì•ˆì •ì„± ìš°ì„ )
            config = {
                **base_config,
                "batch_size": 2,
                "precision": "float32",  # ğŸ”§ í•­ìƒ Float32
                "max_workers": 4,
                "memory_fraction": 0.5,  # 0.6 â†’ 0.5 (ì•ˆì •ì„±)
                "enable_neural_engine": False,
                "pipeline_parallel": False,
                "concurrent_sessions": 2,
                "cache_size_gb": 3,  # 4 â†’ 3 (ì•ˆì •ì„±)
                "memory_pool_gb": 6,  # 8 â†’ 6 (ì•ˆì •ì„±)
                "high_resolution_processing": False
            }
        
        return config
    
    def _create_optimization_settings(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        return {
            "device": self.device,
            "device_name": self.device_name,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_level": self.optimization_level,
            "batch_size": self.config.get("batch_size", 2),
            "precision": self.config.get("precision", "float32"),  # ğŸ”§ ê¸°ë³¸ê°’ float32
            "max_workers": self.config.get("max_workers", 4),
            "memory_fraction": self.config.get("memory_fraction", 0.5),
            "enable_neural_engine": self.config.get("enable_neural_engine", False),
            "pipeline_parallel": self.config.get("pipeline_parallel", False),
            "concurrent_sessions": self.config.get("concurrent_sessions", 2),
            "cache_size_gb": self.config.get("cache_size_gb", 3),
            "memory_pool_gb": self.config.get("memory_pool_gb", 6),
            "high_resolution_processing": self.config.get("high_resolution_processing", False),
            "float_compatibility_mode": True,  # ğŸ”§ í•­ìƒ True
            "initialization_error": self._initialization_error
        }
    
    def _setup_environment_variables(self):
        """í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì•ˆì „í•œ ì²˜ë¦¬)"""
        try:
            if self.device == "mps" and self.is_m3_max and TORCH_AVAILABLE:
                # M3 Max íŠ¹í™” í™˜ê²½ ë³€ìˆ˜ (ì•ˆì „í•œ ì²˜ë¦¬)
                env_vars = {
                    'PYTORCH_MPS_ALLOCATOR_POLICY': 'garbage_collection',
                    'METAL_DEVICE_WRAPPER_TYPE': '1',
                    'METAL_PERFORMANCE_SHADERS_ENABLED': '1',
                    'PYTORCH_MPS_PREFER_METAL': '1',
                    'PYTORCH_ENABLE_MPS_FALLBACK': '1'  # ğŸ”§ í´ë°± í™œì„±í™”
                }
                
                for key, value in env_vars.items():
                    try:
                        os.environ[key] = value
                    except:
                        pass  # ê°œë³„ í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
                
                # PyTorch ì„¤ì • (ì•ˆì „í•œ ì²˜ë¦¬)
                try:
                    torch.set_num_threads(self.config.get("max_workers", 4))
                except:
                    pass
            
        except Exception:
            pass  # ëª¨ë“  í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
    
    def optimize_model(self, model):
        """ëª¨ë¸ ìµœì í™” ì ìš© (ì•ˆì „í•œ ì²˜ë¦¬)"""
        if not self.is_m3_max or model is None or not TORCH_AVAILABLE:
            return model
            
        try:
            # MPS ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (ì•ˆì „í•œ ì²˜ë¦¬)
            if hasattr(model, 'to') and self.device == "mps":
                try:
                    model = model.to(self.device)
                except Exception:
                    pass  # ë””ë°”ì´ìŠ¤ ì´ë™ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
            
            # ğŸ”§ Float32 ê°•ì œ ì‚¬ìš© (í˜¸í™˜ì„± ë³´ì¥)
            if hasattr(model, 'float'):
                try:
                    model = model.float()  # í•­ìƒ float32
                except Exception:
                    pass  # ì •ë°€ë„ ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
            
            return model
            
        except Exception:
            return model  # ëª¨ë“  ìµœì í™” ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ğŸš€ ë©”ëª¨ë¦¬ ìµœì í™” - PyTorch 2.6+ MPS í˜¸í™˜ì„± ì™„ì „ ìˆ˜ì •"""
        try:
            start_time = time.time()
            
            # ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            result = {
                "success": True,
                "device": self.device,
                "method": "standard_gc",
                "aggressive": aggressive,
                "optimizer": "M3MaxOptimizer",
                "pytorch_available": TORCH_AVAILABLE
            }
            
            if not TORCH_AVAILABLE:
                result["warning"] = "PyTorch not available"
                result["duration"] = time.time() - start_time
                return result
            
            # ğŸ”¥ PyTorch 2.6+ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (ì™„ì „ ìˆ˜ì •)
            if self.device == "mps":
                try:
                    mps_cleaned = False
                    
                    # ë°©ë²• 1: torch.mps.empty_cache() (PyTorch 2.1+)
                    if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
                        try:
                            torch.mps.empty_cache()
                            result["method"] = "mps_empty_cache_v2"
                            mps_cleaned = True
                        except Exception:
                            pass
                    
                    # ë°©ë²• 2: torch.mps.synchronize() (ëŒ€ì•ˆ)
                    if not mps_cleaned and hasattr(torch, 'mps') and hasattr(torch.mps, 'synchronize'):
                        try:
                            torch.mps.synchronize()
                            result["method"] = "mps_synchronize"
                            mps_cleaned = True
                        except Exception:
                            pass
                    
                    # ë°©ë²• 3: torch.backends.mps.empty_cache() (ì´ì „ ë²„ì „)
                    if not mps_cleaned and hasattr(torch.backends, 'mps') and hasattr(torch.backends.mps, 'empty_cache'):
                        try:
                            torch.backends.mps.empty_cache()
                            result["method"] = "mps_backends_empty_cache"
                            mps_cleaned = True
                        except Exception:
                            pass
                    
                    if not mps_cleaned:
                        result["method"] = "mps_gc_only"
                        result["info"] = "MPS ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ GCë§Œ ì‹¤í–‰"
                
                except Exception as e:
                    result["warning"] = f"MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)[:100]}"
                    result["method"] = "mps_error_fallback"
            
            elif self.device == "cuda":
                try:
                    cuda_cleaned = False
                    
                    if hasattr(torch, 'cuda') and hasattr(torch.cuda, 'empty_cache'):
                        try:
                            torch.cuda.empty_cache()
                            result["method"] = "cuda_empty_cache"
                            cuda_cleaned = True
                        except Exception:
                            pass
                    
                    if aggressive and cuda_cleaned and hasattr(torch.cuda, 'synchronize'):
                        try:
                            torch.cuda.synchronize()
                            result["method"] = "cuda_aggressive_cleanup"
                        except Exception:
                            pass
                    
                    if not cuda_cleaned:
                        result["method"] = "cuda_gc_only"
                        result["info"] = "CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ GCë§Œ ì‹¤í–‰"
                
                except Exception as e:
                    result["warning"] = f"CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)[:100]}"
                    result["method"] = "cuda_error_fallback"
            
            # ì¶”ê°€ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë¦¬ (aggressive ëª¨ë“œ)
            if aggressive:
                try:
                    # ë°˜ë³µ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                    for _ in range(3):
                        gc.collect()
                    
                    # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„
                    if PSUTIL_AVAILABLE:
                        try:
                            import psutil
                            process = psutil.Process()
                            _ = process.memory_info()  # ë©”ëª¨ë¦¬ ì •ë³´ ê°±ì‹ 
                        except:
                            pass
                    
                    result["method"] = f"{result['method']}_aggressive"
                    result["info"] = "ê³µê²©ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰ë¨"
                
                except Exception:
                    pass  # ê³µê²©ì  ì •ë¦¬ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
            
            result["duration"] = time.time() - start_time
            result["success"] = True
            
            return result
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)[:200],
                "device": self.device,
                "optimizer": "M3MaxOptimizer",
                "pytorch_available": TORCH_AVAILABLE,
                "duration": time.time() - start_time if 'start_time' in locals() else 0.0
            }
    
    def get_optimization_info(self) -> Dict[str, Any]:
        """ìµœì í™” ì •ë³´ ë°˜í™˜"""
        return {
            "device_name": self.device_name,
            "device": self.device,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_level": self.optimization_level,
            "config": self.config,
            "optimization_settings": self.optimization_settings,
            "pipeline_settings": self.pipeline_settings,
            "is_initialized": self.is_initialized,
            "initialization_error": self._initialization_error,
            "mps_available": (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()) if TORCH_AVAILABLE else False,
            "cuda_available": (hasattr(torch, 'cuda') and torch.cuda.is_available()) if TORCH_AVAILABLE else False,
            "pytorch_version": torch.__version__ if TORCH_AVAILABLE else "not_available",
            "pytorch_available": TORCH_AVAILABLE,
            "psutil_available": PSUTIL_AVAILABLE,
            "float_compatibility_mode": True,  # ğŸ”§ í•­ìƒ True
            "stability_mode": True             # ğŸ”§ ì•ˆì •ì„± ëª¨ë“œ
        }
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„ ë°˜í™˜ (ì•ˆì „í•œ ì²˜ë¦¬)"""
        try:
            stats = {
                "device": self.device,
                "optimizer": "M3MaxOptimizer",
                "timestamp": time.time(),
                "pytorch_available": TORCH_AVAILABLE,
                "psutil_available": PSUTIL_AVAILABLE
            }
            
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ (ì•ˆì „í•œ ì²˜ë¦¬)
            if PSUTIL_AVAILABLE:
                try:
                    vm = psutil.virtual_memory()
                    stats["system_memory"] = {
                        "total_gb": round(vm.total / (1024**3), 2),
                        "available_gb": round(vm.available / (1024**3), 2),
                        "used_percent": round(vm.percent, 1)
                    }
                except Exception as e:
                    stats["system_memory_error"] = str(e)[:100]
            else:
                stats["system_memory"] = {"error": "psutil not available"}
            
            # ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì •ë³´
            if TORCH_AVAILABLE:
                if self.device == "mps":
                    stats["mps_memory"] = {
                        "unified_memory": True,
                        "total_gb": self.memory_gb,
                        "note": "MPS uses unified memory system",
                        "optimization_level": self.optimization_level
                    }
                elif self.device == "cuda" and torch.cuda.is_available():
                    try:
                        stats["gpu_memory"] = {
                            "allocated_gb": round(torch.cuda.memory_allocated(0) / (1024**3), 2),
                            "reserved_gb": round(torch.cuda.memory_reserved(0) / (1024**3), 2),
                            "total_gb": round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2)
                        }
                    except Exception as e:
                        stats["gpu_memory_error"] = str(e)[:100]
            
            return stats
            
        except Exception as e:
            return {
                "device": self.device,
                "optimizer": "M3MaxOptimizer",
                "error": str(e)[:200],
                "timestamp": time.time(),
                "pytorch_available": TORCH_AVAILABLE,
                "psutil_available": PSUTIL_AVAILABLE
            }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ì•ˆì „í•œ ì²˜ë¦¬)"""
        try:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.optimize_memory(aggressive=True)
            
            # ì„¤ì • ì´ˆê¸°í™”
            self.config = {}
            self.optimization_settings = {}
            self.pipeline_settings = {}
            self.is_initialized = False
            
        except Exception:
            pass  # ì •ë¦¬ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

# ===============================================================
# ğŸ”§ M3 Optimizer í´ë˜ìŠ¤ (í•˜ìœ„ í˜¸í™˜ì„±)
# ===============================================================

class M3Optimizer(M3MaxOptimizer):
    """
    ğŸ M3 Optimizer í´ë˜ìŠ¤ - M3MaxOptimizerì˜ ë³„ì¹­
    âœ… í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥
    """
    
    def __init__(self, device_name: str = None, memory_gb: float = None, 
                 is_m3_max: bool = None, optimization_level: str = "balanced"):
        """
        M3 ìµœì í™” ì´ˆê¸°í™” (í•˜ìœ„ í˜¸í™˜ì„±)
        
        Args:
            device_name: ë””ë°”ì´ìŠ¤ ì´ë¦„ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - í˜¸í™˜ì„±ìš©)
            memory_gb: ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)
            is_m3_max: M3 Max ì—¬ë¶€ (ìë™ ê°ì§€)
            optimization_level: ìµœì í™” ë ˆë²¨
        """
        # ìë™ ê°ì§€ëœ ê°’ ì‚¬ìš©
        if memory_gb is None:
            memory_gb = _get_system_memory()
        
        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        device = "auto"
        
        # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        super().__init__(device=device, memory_gb=memory_gb, optimization_level=optimization_level)

# ===============================================================
# ğŸ”§ íŒŒì´í”„ë¼ì¸ ë¼ìš°í„° í˜¸í™˜ì„± í•¨ìˆ˜ë“¤ (ì™„ì „ êµ¬í˜„)
# ===============================================================

def create_m3_optimizer_for_pipeline(
    device: str = "auto",
    memory_gb: float = None,
    optimization_level: str = "balanced"
) -> M3MaxOptimizer:
    """
    íŒŒì´í”„ë¼ì¸ ë¼ìš°í„°ìš© M3 Optimizer ìƒì„±
    âœ… ì™„ì „í•œ í˜¸í™˜ì„± ë³´ì¥
    """
    if memory_gb is None:
        memory_gb = _get_system_memory()
    
    return M3MaxOptimizer(
        device=device,
        memory_gb=memory_gb,
        optimization_level=optimization_level
    )

def create_m3_max_optimizer(
    device: str = "auto",
    memory_gb: float = None,
    optimization_level: str = "balanced"
) -> M3MaxOptimizer:
    """M3 Max ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return create_m3_optimizer_for_pipeline(device, memory_gb, optimization_level)

def get_m3_optimization_info(optimizer: M3MaxOptimizer = None) -> Dict[str, Any]:
    """M3 ìµœì í™” ì •ë³´ ì¡°íšŒ (ì•ˆì „í•œ ì²˜ë¦¬)"""
    try:
        if optimizer is None:
            # ì„ì‹œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            optimizer = create_m3_max_optimizer()
        
        return optimizer.get_optimization_info()
    except Exception as e:
        return {
            "error": str(e)[:200],
            "device": "unknown",
            "optimizer": "M3MaxOptimizer",
            "pytorch_available": TORCH_AVAILABLE,
            "psutil_available": PSUTIL_AVAILABLE
        }

def optimize_m3_memory(optimizer: M3MaxOptimizer = None, aggressive: bool = False) -> Dict[str, Any]:
    """M3 ë©”ëª¨ë¦¬ ìµœì í™” (ì•ˆì „í•œ ì²˜ë¦¬)"""
    try:
        if optimizer is None:
            # ì„ì‹œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            optimizer = create_m3_max_optimizer()
        
        return optimizer.optimize_memory(aggressive=aggressive)
    except Exception as e:
        return {
            "success": False,
            "error": str(e)[:200],
            "device": "unknown",
            "optimizer": "M3MaxOptimizer",
            "pytorch_available": TORCH_AVAILABLE
        }

# ===============================================================
# ğŸ”§ Config í´ë˜ìŠ¤ (import ì˜¤ë¥˜ í•´ê²°)
# ===============================================================

class Config:
    """
    ê¸°ë³¸ ì„¤ì • í´ë˜ìŠ¤ (ì•ˆì „í•œ ì²˜ë¦¬)
    âœ… import ì˜¤ë¥˜ í•´ê²°ìš©
    """
    
    def __init__(self, **kwargs):
        # ê¸°ë³¸ ì„¤ì •
        self.device = kwargs.get('device', 'auto')
        self.memory_gb = kwargs.get('memory_gb', _get_system_memory())
        self.quality_level = kwargs.get('quality_level', 'balanced')
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.optimization_level = kwargs.get('optimization_level', 'balanced')
        
        # M3 Max ì •ë³´ (ì•ˆì „í•œ ì²˜ë¦¬)
        try:
            self.is_m3_max = _detect_m3_max(self.memory_gb)
            self.device_name = _detect_chip_name()
        except:
            self.is_m3_max = False
            self.device_name = "Unknown"
        
        # M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì•ˆì „í•œ ì²˜ë¦¬)
        try:
            self.m3_optimizer = M3MaxOptimizer(
                device=self.device,
                memory_gb=self.memory_gb,
                optimization_level=self.optimization_level
            )
        except Exception as e:
            # í´ë°± ë”ë¯¸ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤
            self.m3_optimizer = self._create_dummy_optimizer(str(e)[:100])
        
    def _create_dummy_optimizer(self, error_msg: str):
        """ë”ë¯¸ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        class DummyOptimizer:
            def __init__(self, error):
                self.device = "cpu"
                self.error = error
                self.is_initialized = False
            
            def get_optimization_info(self):
                return {"error": self.error, "device": "cpu", "fallback_mode": True}
            
            def optimize_memory(self, aggressive=False):
                return {"success": True, "method": "fallback_gc", "device": "cpu"}
            
            def cleanup(self):
                pass
        
        return DummyOptimizer(error_msg)
        
    def to_dict(self) -> Dict[str, Any]:
        """ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        try:
            return {
                'device': self.device,
                'memory_gb': self.memory_gb,
                'quality_level': self.quality_level,
                'optimization_enabled': self.optimization_enabled,
                'optimization_level': self.optimization_level,
                'is_m3_max': self.is_m3_max,
                'device_name': self.device_name,
                'm3_optimizer_info': self.m3_optimizer.get_optimization_info()
            }
        except Exception as e:
            return {
                'error': str(e)[:200],
                'device': self.device,
                'fallback_mode': True
            }
    
    def get_optimizer(self) -> M3MaxOptimizer:
        """M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        return self.m3_optimizer
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            return self.m3_optimizer.optimize_memory(aggressive=aggressive)
        except Exception as e:
            return {
                "success": False,
                "error": str(e)[:200],
                "device": self.device
            }

# ===============================================================
# ğŸ”§ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í¸ì˜ í•¨ìˆ˜ë“¤ (ì•ˆì „í•œ ì²˜ë¦¬)
# ===============================================================

# ì „ì—­ M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤
_global_m3_optimizer: Optional[M3MaxOptimizer] = None

def get_global_m3_optimizer() -> M3MaxOptimizer:
    """ì „ì—­ M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì•ˆì „í•œ ì²˜ë¦¬)"""
    global _global_m3_optimizer
    
    try:
        if _global_m3_optimizer is None:
            _global_m3_optimizer = create_m3_max_optimizer()
        
        return _global_m3_optimizer
    except Exception:
        # ë”ë¯¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
        class DummyM3Optimizer:
            def __init__(self):
                self.device = "cpu"
                self.is_initialized = False
                self.error = "Failed to create global optimizer"
            
            def get_optimization_info(self):
                return {"error": self.error, "device": "cpu", "fallback_mode": True}
            
            def optimize_memory(self, aggressive=False):
                return {"success": True, "method": "fallback_gc", "device": "cpu"}
            
            def cleanup(self):
                pass
        
        return DummyM3Optimizer()

def initialize_global_m3_optimizer(**kwargs) -> M3MaxOptimizer:
    """ì „ì—­ M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” (ì•ˆì „í•œ ì²˜ë¦¬)"""
    global _global_m3_optimizer
    
    try:
        device = kwargs.get('device', 'auto')
        memory_gb = kwargs.get('memory_gb', _get_system_memory())
        optimization_level = kwargs.get('optimization_level', 'balanced')
        
        _global_m3_optimizer = M3MaxOptimizer(
            device=device,
            memory_gb=memory_gb,
            optimization_level=optimization_level
        )
        
        return _global_m3_optimizer
    except Exception:
        return get_global_m3_optimizer()  # ë”ë¯¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

def cleanup_global_m3_optimizer():
    """ì „ì—­ M3 ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ì •ë¦¬"""
    global _global_m3_optimizer
    
    try:
        if _global_m3_optimizer:
            _global_m3_optimizer.cleanup()
            _global_m3_optimizer = None
    except:
        pass  # ì •ë¦¬ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

# ===============================================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì•ˆì „í•œ ì²˜ë¦¬)
# ===============================================================

def is_m3_max_available() -> bool:
    """M3 Max ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    try:
        return _detect_m3_max(_get_system_memory())
    except:
        return False

def get_m3_system_info() -> Dict[str, Any]:
    """M3 ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜ (ì•ˆì „í•œ ì²˜ë¦¬)"""
    try:
        return {
            "device_name": _detect_chip_name(),
            "memory_gb": _get_system_memory(),
            "is_m3_max": is_m3_max_available(),
            "mps_available": (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()) if TORCH_AVAILABLE else False,
            "pytorch_version": torch.__version__ if TORCH_AVAILABLE else "not_available",
            "pytorch_available": TORCH_AVAILABLE,
            "psutil_available": PSUTIL_AVAILABLE,
            "platform": platform.system(),
            "machine": platform.machine(),
            "float_compatibility_mode": True,
            "stability_mode": True
        }
    except Exception as e:
        return {
            "error": str(e)[:200],
            "pytorch_available": TORCH_AVAILABLE,
            "psutil_available": PSUTIL_AVAILABLE,
            "fallback_mode": True
        }

def apply_m3_environment_optimizations() -> bool:
    """M3 í™˜ê²½ ìµœì í™” ì ìš© (ì•ˆì „í•œ ì²˜ë¦¬)"""
    try:
        if is_m3_max_available():
            optimizer = get_global_m3_optimizer()
            if hasattr(optimizer, '_setup_environment_variables'):
                optimizer._setup_environment_variables()
                return True
        return False
    except:
        return False

# ===============================================================
# ğŸ”§ ëª¨ë“ˆ export
# ===============================================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'M3MaxOptimizer',
    'M3Optimizer',
    'Config',
    
    # ìƒì„± í•¨ìˆ˜ë“¤
    'create_m3_optimizer_for_pipeline',
    'create_m3_max_optimizer',
    
    # ì „ì—­ ê´€ë¦¬ í•¨ìˆ˜ë“¤
    'get_global_m3_optimizer',
    'initialize_global_m3_optimizer',
    'cleanup_global_m3_optimizer',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_m3_optimization_info',
    'optimize_m3_memory',
    'is_m3_max_available',
    'get_m3_system_info',
    'apply_m3_environment_optimizations',
    
    # ê°ì§€ í•¨ìˆ˜ë“¤
    '_detect_chip_name',
    '_detect_m3_max',
    '_get_system_memory'
]

# ===============================================================
# ğŸ”§ ëª¨ë“ˆ ì´ˆê¸°í™” (ì•ˆì „í•œ ì²˜ë¦¬)
# ===============================================================

try:
    # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
    system_info = get_m3_system_info()
    
    # ì´ˆê¸°í™” ì„±ê³µ ë¡œê·¸ (ìµœì†Œí™”)
    if system_info.get('is_m3_max', False):
        print(f"ğŸ M3 Max ìµœì í™” ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - Float32 ì•ˆì •ì„± ëª¨ë“œ")
    else:
        device_name = system_info.get('device_name', 'Unknown')
        print(f"ğŸ”§ {device_name} ìµœì í™” ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - ì•ˆì •ì„± ëª¨ë“œ")
    
    # ìë™ ì´ˆê¸°í™” (ì„ íƒì )
    if os.getenv('AUTO_INIT_M3_OPTIMIZER', 'false').lower() == 'true':
        try:
            initialize_global_m3_optimizer()
        except:
            pass  # ìë™ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

except Exception:
    # ì™„ì „ í´ë°±
    print("âš ï¸ M3 Optimizer ëª¨ë“ˆ ë¶€ë¶„ ë¡œë“œ - ì œí•œëœ ê¸°ëŠ¥")

# ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ (ìµœì†Œ ë¡œê·¸)
print("âœ… M3 Optimizer ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - í˜¸í™˜ì„± ìš°ì„  ëª¨ë“œ")