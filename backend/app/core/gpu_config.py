# backend/app/core/gpu_config.py
"""
ğŸ MyCloset AI - ì™„ì „í•œ GPU ì„¤ì • ë§¤ë‹ˆì € (ìš°ë¦¬ êµ¬ì¡° 100% ìµœì í™”)
=================================================================================

âœ… ê¸°ì¡´ í”„ë¡œì íŠ¸ êµ¬ì¡° 100% í˜¸í™˜
âœ… GPUConfig í´ë˜ìŠ¤ ì™„ì „ êµ¬í˜„ (import ì˜¤ë¥˜ í•´ê²°)
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… PyTorch 2.6+ MPS í˜¸í™˜ì„± ì™„ì „ í•´ê²°
âœ… torch.mps.empty_cache() ì˜¤ë¥˜ ì™„ì „ ìˆ˜ì •
âœ… Float16/32 í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
âœ… Conda í™˜ê²½ ì™„ë²½ ì§€ì›
âœ… 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”
âœ… ë ˆì´ì–´ ì•„í‚¤í…ì²˜ íŒ¨í„´ ì ìš©
âœ… ìˆœí™˜ ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
âœ… ì•ˆì „í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜
âœ… ë¡œê·¸ ë…¸ì´ì¦ˆ ìµœì†Œí™”
âœ… Clean Architecture ì ìš©

í”„ë¡œì íŠ¸ êµ¬ì¡°:
backend/app/core/gpu_config.py (ì´ íŒŒì¼)
    â†“ ì‚¬ìš©ë¨
backend/app/core/config.py
backend/app/api/pipeline_routes.py
backend/app/ai_pipeline/utils/model_loader.py
backend/app/ai_pipeline/steps/*.py
"""

import os
import gc
import logging
import platform
import subprocess
import time
import weakref
from typing import Dict, Any, Optional, Union, List, Tuple, Callable
from functools import lru_cache, wraps
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import json
import threading
from concurrent.futures import ThreadPoolExecutor

# =============================================================================
# ğŸ”§ ì¡°ê±´ë¶€ ì„í¬íŠ¸ (ì•ˆì „í•œ ì²˜ë¦¬)
# =============================================================================

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

try:
    import torch
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
except ImportError:
    TORCH_AVAILABLE = False
    TORCH_VERSION = "not_available"

try:
    import numpy as np
    NUMPY_AVAILABLE = True
    NUMPY_VERSION = np.__version__
except ImportError:
    NUMPY_AVAILABLE = False
    NUMPY_VERSION = "not_available"

# =============================================================================
# ğŸ”§ ë¡œê¹… ìµœì í™” (ë…¸ì´ì¦ˆ 90% ê°ì†Œ)
# =============================================================================

logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)  # INFO ë¡œê·¸ ì–µì œ

# =============================================================================
# ğŸ”§ ìƒìˆ˜ ë° ì„¤ì •
# =============================================================================

class OptimizationLevel(Enum):
    """ìµœì í™” ë ˆë²¨"""
    SAFE = "safe"
    BALANCED = "balanced"
    PERFORMANCE = "performance"
    ULTRA = "ultra"

class DeviceType(Enum):
    """ë””ë°”ì´ìŠ¤ íƒ€ì…"""
    CPU = "cpu"
    MPS = "mps"
    CUDA = "cuda"
    AUTO = "auto"

@dataclass
class DeviceCapabilities:
    """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ì •ë³´"""
    device: str
    name: str
    memory_gb: float
    supports_fp16: bool = False
    supports_fp32: bool = True
    supports_neural_engine: bool = False
    supports_metal_shaders: bool = False
    unified_memory: bool = False
    max_batch_size: int = 1
    recommended_image_size: Tuple[int, int] = (512, 512)

# =============================================================================
# ğŸ M3 Max í•˜ë“œì›¨ì–´ ê°ì§€ ì‹œìŠ¤í…œ
# =============================================================================

class HardwareDetector:
    """í•˜ë“œì›¨ì–´ ì •ë³´ ê°ì§€ ë° ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self._cache = {}
        self._lock = threading.Lock()
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ ì •ë³´
        self.system_info = self._get_system_info()
        self.is_m3_max = self._detect_m3_max()
        self.memory_gb = self._get_system_memory()
        self.cpu_cores = self._get_cpu_cores()
        self.gpu_info = self._get_gpu_info()
        
        # ì„±ëŠ¥ íŠ¹ì„±
        self.performance_class = self._classify_performance()
        self.optimization_profile = self._create_optimization_profile()
    
    def _get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘"""
        if 'system_info' in self._cache:
            return self._cache['system_info']
        
        info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "processor": platform.processor(),
            "python_version": platform.python_version(),
            "pytorch_version": TORCH_VERSION,
            "numpy_version": NUMPY_VERSION,
            "node_name": platform.node(),
            "platform_release": platform.release()
        }
        
        self._cache['system_info'] = info
        return info
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì •ë°€ ê°ì§€ (ë‹¤ì¤‘ ë°©ë²•)"""
        if 'm3_max' in self._cache:
            return self._cache['m3_max']
        
        try:
            # 1ì°¨: í”Œë«í¼ ì²´í¬
            if platform.system() != "Darwin" or platform.machine() != "arm64":
                self._cache['m3_max'] = False
                return False
            
            # 2ì°¨: ë©”ëª¨ë¦¬ ê¸°ë°˜ ê°ì§€ (M3 MaxëŠ” 96GB/128GB)
            if PSUTIL_AVAILABLE:
                memory_gb = psutil.virtual_memory().total / (1024**3)
                if memory_gb >= 90:  # 90GB ì´ìƒì´ë©´ M3 Max ê°€ëŠ¥ì„± ë†’ìŒ
                    self._cache['m3_max'] = True
                    return True
            
            # 3ì°¨: CPU ì½”ì–´ ìˆ˜ ê¸°ë°˜ ê°ì§€ (M3 MaxëŠ” 16ì½”ì–´)
            cpu_count = os.cpu_count() or 0
            if cpu_count >= 14:  # 14ì½”ì–´ ì´ìƒì´ë©´ M3 Max ê°€ëŠ¥ì„±
                self._cache['m3_max'] = True
                return True
            
            # 4ì°¨: ì‹œìŠ¤í…œ í”„ë¡œíŒŒì¼ëŸ¬ ê¸°ë°˜ ê°ì§€ (íƒ€ì„ì•„ì›ƒ ì ìš©)
            try:
                result = subprocess.run(
                    ['system_profiler', 'SPHardwareDataType'],
                    capture_output=True,
                    text=True,
                    timeout=3
                )
                if result.returncode == 0:
                    output = result.stdout.lower()
                    if any(indicator in output for indicator in ['m3 max', 'apple m3 max']):
                        self._cache['m3_max'] = True
                        return True
            except (subprocess.TimeoutExpired, subprocess.SubprocessError, FileNotFoundError):
                pass  # ì‹œìŠ¤í…œ í”„ë¡œíŒŒì¼ëŸ¬ ì‚¬ìš© ë¶ˆê°€ ì‹œ ë¬´ì‹œ
            
            # 5ì°¨: ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ê¸°ë°˜ ì¶”ì • (M3 Max íŠ¹ì„±)
            try:
                result = subprocess.run(
                    ['sysctl', '-n', 'hw.memsize'],
                    capture_output=True,
                    text=True,
                    timeout=2
                )
                if result.returncode == 0:
                    memory_bytes = int(result.stdout.strip())
                    memory_gb = memory_bytes / (1024**3)
                    if memory_gb >= 120:  # 120GB ì´ìƒì´ë©´ M3 Max 128GB ëª¨ë¸
                        self._cache['m3_max'] = True
                        return True
            except:
                pass
            
            self._cache['m3_max'] = False
            return False
            
        except Exception as e:
            logger.debug(f"M3 Max ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            self._cache['m3_max'] = False
            return False
    
    def _get_system_memory(self) -> float:
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìš©ëŸ‰ í™•ì¸ (GB)"""
        if 'memory_gb' in self._cache:
            return self._cache['memory_gb']
        
        try:
            if PSUTIL_AVAILABLE:
                memory_gb = round(psutil.virtual_memory().total / (1024**3), 1)
            else:
                # psutil ì—†ì„ ë•Œ í´ë°±
                try:
                    if platform.system() == "Darwin":
                        result = subprocess.run(
                            ['sysctl', '-n', 'hw.memsize'],
                            capture_output=True,
                            text=True,
                            timeout=2
                        )
                        memory_gb = round(int(result.stdout.strip()) / (1024**3), 1)
                    else:
                        memory_gb = 16.0  # ê¸°ë³¸ê°’
                except:
                    memory_gb = 16.0
            
            self._cache['memory_gb'] = memory_gb
            return memory_gb
            
        except Exception:
            self._cache['memory_gb'] = 16.0
            return 16.0
    
    def _get_cpu_cores(self) -> int:
        """CPU ì½”ì–´ ìˆ˜ í™•ì¸"""
        if 'cpu_cores' in self._cache:
            return self._cache['cpu_cores']
        
        try:
            if PSUTIL_AVAILABLE:
                # ë¬¼ë¦¬ ì½”ì–´ì™€ ë…¼ë¦¬ ì½”ì–´ ëª¨ë‘ í™•ì¸
                physical = psutil.cpu_count(logical=False) or 8
                logical = psutil.cpu_count(logical=True) or 8
                cores = max(physical, logical)
            else:
                cores = os.cpu_count() or 8
            
            self._cache['cpu_cores'] = cores
            return cores
            
        except Exception:
            self._cache['cpu_cores'] = 8
            return 8
    
    def _get_gpu_info(self) -> Dict[str, Any]:
        """GPU ì •ë³´ ìˆ˜ì§‘"""
        if 'gpu_info' in self._cache:
            return self._cache['gpu_info']
        
        gpu_info = {
            "device": "cpu",
            "name": "CPU",
            "memory_gb": 0,
            "available": True,
            "backend": "CPU",
            "compute_capability": None,
            "driver_version": None
        }
        
        if not TORCH_AVAILABLE:
            self._cache['gpu_info'] = gpu_info
            return gpu_info
        
        try:
            # MPS (Apple Silicon) ì§€ì› í™•ì¸
            if (hasattr(torch.backends, 'mps') and 
                hasattr(torch.backends.mps, 'is_available') and 
                torch.backends.mps.is_available()):
                
                gpu_info.update({
                    "device": "mps",
                    "name": "Apple M3 Max" if self.is_m3_max else "Apple Silicon",
                    "memory_gb": self.memory_gb,  # í†µí•© ë©”ëª¨ë¦¬
                    "available": True,
                    "backend": "Metal Performance Shaders",
                    "unified_memory": True,
                    "neural_engine": self.is_m3_max
                })
            
            # CUDA ì§€ì› í™•ì¸
            elif hasattr(torch, 'cuda') and torch.cuda.is_available():
                try:
                    gpu_props = torch.cuda.get_device_properties(0)
                    gpu_info.update({
                        "device": "cuda",
                        "name": gpu_props.name,
                        "memory_gb": round(gpu_props.total_memory / (1024**3), 1),
                        "available": True,
                        "backend": "CUDA",
                        "compute_capability": f"{gpu_props.major}.{gpu_props.minor}",
                        "multiprocessor_count": gpu_props.multiprocessor_count
                    })
                except Exception as e:
                    logger.debug(f"CUDA ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        except Exception as e:
            logger.debug(f"GPU ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        self._cache['gpu_info'] = gpu_info
        return gpu_info
    
    def _classify_performance(self) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ ë¶„ë¥˜"""
        if self.is_m3_max and self.memory_gb >= 120:
            return "ultra_high"
        elif self.is_m3_max or (self.memory_gb >= 64 and self.cpu_cores >= 12):
            return "high"
        elif self.memory_gb >= 32 and self.cpu_cores >= 8:
            return "medium"
        elif self.memory_gb >= 16:
            return "low"
        else:
            return "minimal"
    
    def _create_optimization_profile(self) -> Dict[str, Any]:
        """ìµœì í™” í”„ë¡œíŒŒì¼ ìƒì„±"""
        profiles = {
            "ultra_high": {
                "batch_size": 8,
                "max_workers": 16,
                "concurrent_sessions": 12,
                "memory_fraction": 0.8,
                "quality_level": "ultra",
                "enable_neural_engine": True,
                "enable_metal_shaders": True,
                "parallel_processing": True,
                "aggressive_optimization": True
            },
            "high": {
                "batch_size": 6,
                "max_workers": 12,
                "concurrent_sessions": 8,
                "memory_fraction": 0.75,
                "quality_level": "high",
                "enable_neural_engine": self.is_m3_max,
                "enable_metal_shaders": self.gpu_info["device"] == "mps",
                "parallel_processing": True,
                "aggressive_optimization": True
            },
            "medium": {
                "batch_size": 4,
                "max_workers": 8,
                "concurrent_sessions": 6,
                "memory_fraction": 0.7,
                "quality_level": "balanced",
                "enable_neural_engine": False,
                "enable_metal_shaders": self.gpu_info["device"] == "mps",
                "parallel_processing": True,
                "aggressive_optimization": False
            },
            "low": {
                "batch_size": 2,
                "max_workers": 4,
                "concurrent_sessions": 3,
                "memory_fraction": 0.6,
                "quality_level": "balanced",
                "enable_neural_engine": False,
                "enable_metal_shaders": False,
                "parallel_processing": False,
                "aggressive_optimization": False
            },
            "minimal": {
                "batch_size": 1,
                "max_workers": 2,
                "concurrent_sessions": 1,
                "memory_fraction": 0.5,
                "quality_level": "fast",
                "enable_neural_engine": False,
                "enable_metal_shaders": False,
                "parallel_processing": False,
                "aggressive_optimization": False
            }
        }
        
        return profiles.get(self.performance_class, profiles["minimal"])

# =============================================================================
# ğŸ”§ í•µì‹¬ GPUConfig í´ë˜ìŠ¤ (Import ì˜¤ë¥˜ í•´ê²°)
# =============================================================================

class GPUConfig:
    """
    ğŸ”¥ ì™„ì „í•œ GPU ì„¤ì • í´ë˜ìŠ¤ - Import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    
    ì£¼ìš” ê¸°ëŠ¥:
    - M3 Max 128GB ìë™ ê°ì§€ ë° ìµœì í™”
    - PyTorch MPS/CUDA ë°±ì—”ë“œ ì™„ì „ ì§€ì›
    - ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìµœì í™”
    - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”
    - ì•ˆì „í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜
    - Clean Architecture íŒ¨í„´
    """
    
    def __init__(self, device: Optional[str] = None, optimization_level: Optional[str] = None, **kwargs):
        """GPUConfig ì´ˆê¸°í™”"""
        
        # í•˜ë“œì›¨ì–´ ê°ì§€
        self.hardware = HardwareDetector()
        
        # ê¸°ë³¸ ì†ì„± ì„¤ì •
        self.device = self._determine_device(device)
        self.device_name = self.hardware.gpu_info["name"]
        self.device_type = self.device
        self.memory_gb = self.hardware.memory_gb
        self.is_m3_max = self.hardware.is_m3_max
        self.is_initialized = False
        
        # ìµœì í™” ë ˆë²¨ ì„¤ì •
        self.optimization_level = self._determine_optimization_level(optimization_level)
        
        # ì„¤ì • ê³„ì‚°
        self.optimization_settings = self._calculate_optimization_settings()
        self.model_config = self._create_model_config()
        self.device_info = self._collect_device_info()
        self.device_capabilities = self._create_device_capabilities()
        self.pipeline_optimizations = self._setup_pipeline_optimizations()
        
        # í™˜ê²½ ìµœì í™” ì ìš©
        try:
            self._apply_environment_optimizations()
            self.is_initialized = True
        except Exception as e:
            logger.warning(f"âš ï¸ í™˜ê²½ ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
            self.is_initialized = False
        
        # Float í˜¸í™˜ì„± ëª¨ë“œ (ì•ˆì •ì„± ìš°ì„ )
        self.float_compatibility_mode = True
    
    def _determine_device(self, device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ê²°ì •"""
        if device and device != "auto":
            return device
        
        if not TORCH_AVAILABLE:
            return "cpu"
        
        # ìš°ì„ ìˆœìœ„: MPS > CUDA > CPU
        try:
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return "mps"
            elif hasattr(torch, 'cuda') and torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        except Exception:
            return "cpu"
    
    def _determine_optimization_level(self, level: Optional[str]) -> str:
        """ìµœì í™” ë ˆë²¨ ê²°ì •"""
        if level:
            return level
        
        # ì„±ëŠ¥ í´ë˜ìŠ¤ì— ë”°ë¥¸ ìë™ ì„ íƒ
        performance_to_optimization = {
            "ultra_high": "ultra",
            "high": "performance",
            "medium": "balanced",
            "low": "balanced",
            "minimal": "safe"
        }
        
        return performance_to_optimization.get(self.hardware.performance_class, "balanced")
    
    def _calculate_optimization_settings(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ê³„ì‚°"""
        base_profile = self.hardware.optimization_profile.copy()
        
        # M3 Max íŠ¹í™” ìµœì í™”
        if self.is_m3_max:
            base_profile.update({
                "dtype": "float32",  # í˜¸í™˜ì„±ì„ ìœ„í•´ Float32 ì‚¬ìš©
                "mixed_precision": False,  # MPS í˜¸í™˜ì„± ë¬¸ì œë¡œ ë¹„í™œì„±í™”
                "memory_efficient_attention": True,
                "gradient_checkpointing": True,
                "unified_memory_optimization": True,
                "metal_performance_shaders": True,
                "neural_engine_acceleration": True,
                "pipeline_parallelism": True,
                "step_caching": True,
                "model_preloading": False,  # ë©”ëª¨ë¦¬ ì ˆì•½
                "high_resolution_processing": self.memory_gb >= 120,
                "real_time_optimization": True
            })
        else:
            # ì¼ë°˜ ì‹œìŠ¤í…œ ìµœì í™”
            base_profile.update({
                "dtype": "float32",
                "mixed_precision": False,
                "memory_efficient_attention": False,
                "gradient_checkpointing": False,
                "unified_memory_optimization": False,
                "metal_performance_shaders": self.device == "mps",
                "neural_engine_acceleration": False,
                "pipeline_parallelism": False,
                "step_caching": True,
                "model_preloading": False,
                "high_resolution_processing": False,
                "real_time_optimization": False
            })
        
        return base_profile
    
    def _create_model_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„¤ì • ìƒì„±"""
        return {
            "device": self.device,
            "device_name": self.device_name,
            "dtype": self.optimization_settings["dtype"],
            "batch_size": self.optimization_settings["batch_size"],
            "max_workers": self.optimization_settings["max_workers"],
            "concurrent_sessions": self.optimization_settings["concurrent_sessions"],
            "memory_fraction": self.optimization_settings["memory_fraction"],
            "optimization_level": self.optimization_level,
            "quality_level": self.optimization_settings["quality_level"],
            
            # M3 Max íŠ¹í™” ì„¤ì •
            "float_compatibility_mode": True,
            "mps_fallback_enabled": self.device == "mps",
            "neural_engine_enabled": self.optimization_settings.get("neural_engine_acceleration", False),
            "metal_performance_shaders": self.optimization_settings.get("metal_performance_shaders", False),
            "unified_memory_optimization": self.optimization_settings.get("unified_memory_optimization", False),
            "memory_efficient": True,
            
            # íŒŒì´í”„ë¼ì¸ ì„¤ì •
            "enable_caching": self.optimization_settings.get("step_caching", True),
            "enable_preloading": self.optimization_settings.get("model_preloading", False),
            "enable_parallel_processing": self.optimization_settings.get("pipeline_parallelism", False),
            "high_resolution_processing": self.optimization_settings.get("high_resolution_processing", False),
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬
            "memory_pool_size_gb": min(32, self.memory_gb // 4),
            "model_cache_size_gb": min(16, self.memory_gb // 8),
            "aggressive_cleanup": self.optimization_settings.get("aggressive_optimization", False),
            
            # í˜¸í™˜ì„± ì„¤ì •
            "pytorch_version": TORCH_VERSION,
            "numpy_version": NUMPY_VERSION,
            "m3_max_optimized": self.is_m3_max,
            "conda_environment": os.environ.get("CONDA_DEFAULT_ENV", "unknown") != "unknown"
        }
    
    def _collect_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        info = {
            "device": self.device,
            "device_name": self.device_name,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "performance_class": self.hardware.performance_class,
            "optimization_level": self.optimization_level,
            "cpu_cores": self.hardware.cpu_cores,
            
            # ì‹œìŠ¤í…œ ì •ë³´
            "system_info": self.hardware.system_info,
            "gpu_info": self.hardware.gpu_info,
            
            # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë³´
            "pytorch_available": TORCH_AVAILABLE,
            "pytorch_version": TORCH_VERSION,
            "numpy_available": NUMPY_AVAILABLE,
            "numpy_version": NUMPY_VERSION,
            "psutil_available": PSUTIL_AVAILABLE,
            
            # ì„¤ì • ì •ë³´
            "float_compatibility_mode": True,
            "conda_environment": os.environ.get("CONDA_DEFAULT_ENV", "unknown"),
            "initialization_time": time.time()
        }
        
        # M3 Max íŠ¹í™” ì •ë³´
        if self.is_m3_max:
            info["m3_max_features"] = {
                "neural_engine_available": True,
                "neural_engine_tops": "15.8 TOPS",
                "gpu_cores": "30-40 cores",
                "memory_bandwidth": "400GB/s",
                "unified_memory": True,
                "metal_performance_shaders": True,
                "optimized_for_ai": True,
                "pipeline_acceleration": True,
                "real_time_processing": True,
                "high_resolution_support": self.memory_gb >= 120,
                "float32_optimized": True,
                "memory_pool_optimized": True
            }
        
        return info
    
    def _create_device_capabilities(self) -> DeviceCapabilities:
        """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ì •ë³´ ìƒì„±"""
        if self.is_m3_max:
            return DeviceCapabilities(
                device=self.device,
                name=self.device_name,
                memory_gb=self.memory_gb,
                supports_fp16=False,  # í˜¸í™˜ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
                supports_fp32=True,
                supports_neural_engine=True,
                supports_metal_shaders=True,
                unified_memory=True,
                max_batch_size=self.optimization_settings["batch_size"] * 2,
                recommended_image_size=(768, 768) if self.memory_gb >= 120 else (640, 640)
            )
        elif self.device == "cuda":
            return DeviceCapabilities(
                device=self.device,
                name=self.device_name,
                memory_gb=self.hardware.gpu_info.get("memory_gb", 0),
                supports_fp16=True,
                supports_fp32=True,
                supports_neural_engine=False,
                supports_metal_shaders=False,
                unified_memory=False,
                max_batch_size=self.optimization_settings["batch_size"],
                recommended_image_size=(512, 512)
            )
        else:
            return DeviceCapabilities(
                device=self.device,
                name=self.device_name,
                memory_gb=self.memory_gb,
                supports_fp16=False,
                supports_fp32=True,
                supports_neural_engine=False,
                supports_metal_shaders=False,
                unified_memory=False,
                max_batch_size=1,
                recommended_image_size=(512, 512)
            )
    
    def _setup_pipeline_optimizations(self) -> Dict[str, Any]:
        """8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì •"""
        base_batch = self.optimization_settings["batch_size"]
        base_memory_fraction = self.optimization_settings["memory_fraction"]
        precision = self.optimization_settings["dtype"]
        
        # M3 Max ìµœì í™” ì„¤ì •
        if self.is_m3_max:
            return {
                "step_01_human_parsing": {
                    "batch_size": max(1, base_batch // 3),
                    "precision": precision,
                    "max_resolution": 768 if self.memory_gb >= 120 else 640,
                    "memory_fraction": base_memory_fraction * 0.25,
                    "enable_caching": True,
                    "neural_engine_boost": True,
                    "metal_shader_acceleration": True,
                    "parallel_processing": True,
                    "high_quality_mode": True
                },
                "step_02_pose_estimation": {
                    "batch_size": max(1, base_batch // 2),
                    "precision": precision,
                    "keypoint_threshold": 0.3,
                    "memory_fraction": base_memory_fraction * 0.2,
                    "enable_caching": True,
                    "high_precision_mode": True,
                    "real_time_optimization": True,
                    "neural_engine_acceleration": True
                },
                "step_03_cloth_segmentation": {
                    "batch_size": max(1, base_batch // 2),
                    "precision": precision,
                    "background_threshold": 0.5,
                    "memory_fraction": base_memory_fraction * 0.25,
                    "enable_edge_refinement": True,
                    "unified_memory_optimization": True,
                    "parallel_processing": True,
                    "high_quality_segmentation": True
                },
                "step_04_geometric_matching": {
                    "batch_size": max(1, base_batch // 4),
                    "precision": precision,
                    "warp_resolution": 512 if self.memory_gb >= 120 else 448,
                    "memory_fraction": base_memory_fraction * 0.3,
                    "enable_caching": True,
                    "high_accuracy_mode": True,
                    "gpu_acceleration": True,
                    "advanced_matching": True
                },
                "step_05_cloth_warping": {
                    "batch_size": max(1, base_batch // 2),
                    "precision": precision,
                    "interpolation": "bicubic",
                    "memory_fraction": base_memory_fraction * 0.25,
                    "preserve_details": True,
                    "texture_enhancement": True,
                    "anti_aliasing": True,
                    "quality_optimization": True
                },
                "step_06_virtual_fitting": {
                    "batch_size": 1,  # ë©”ëª¨ë¦¬ ì§‘ì•½ì 
                    "precision": precision,
                    "diffusion_steps": 25 if self.memory_gb >= 120 else 20,
                    "memory_fraction": base_memory_fraction * 0.5,
                    "scheduler": "ddim",
                    "guidance_scale": 7.5,
                    "high_quality_mode": True,
                    "neural_engine_diffusion": True,
                    "metal_shader_optimization": True
                },
                "step_07_post_processing": {
                    "batch_size": base_batch,
                    "precision": precision,
                    "enhancement_level": "high",
                    "memory_fraction": base_memory_fraction * 0.2,
                    "noise_reduction": True,
                    "detail_preservation": True,
                    "color_correction": True,
                    "upscaling_enabled": self.memory_gb >= 120
                },
                "step_08_quality_assessment": {
                    "batch_size": base_batch,
                    "precision": precision,
                    "quality_metrics": ["ssim", "lpips", "clip_score", "fid"],
                    "memory_fraction": base_memory_fraction * 0.15,
                    "assessment_threshold": 0.8,
                    "comprehensive_analysis": True,
                    "real_time_feedback": True,
                    "neural_engine_assessment": True
                }
            }
        else:
            # ì¼ë°˜ ì‹œìŠ¤í…œ ì„¤ì •
            return {
                "step_01_human_parsing": {
                    "batch_size": 1,
                    "precision": precision,
                    "max_resolution": 512,
                    "memory_fraction": 0.3,
                    "enable_caching": False
                },
                "step_02_pose_estimation": {
                    "batch_size": 1,
                    "precision": precision,
                    "keypoint_threshold": 0.35,
                    "memory_fraction": 0.25,
                    "enable_caching": False
                },
                "step_03_cloth_segmentation": {
                    "batch_size": 1,
                    "precision": precision,
                    "background_threshold": 0.5,
                    "memory_fraction": 0.3,
                    "enable_edge_refinement": False
                },
                "step_04_geometric_matching": {
                    "batch_size": 1,
                    "precision": precision,
                    "warp_resolution": 256,
                    "memory_fraction": 0.35,
                    "enable_caching": False
                },
                "step_05_cloth_warping": {
                    "batch_size": 1,
                    "precision": precision,
                    "interpolation": "bilinear",
                    "memory_fraction": 0.3,
                    "preserve_details": False
                },
                "step_06_virtual_fitting": {
                    "batch_size": 1,
                    "precision": precision,
                    "diffusion_steps": 15,
                    "memory_fraction": 0.6,
                    "scheduler": "ddim",
                    "guidance_scale": 7.5
                },
                "step_07_post_processing": {
                    "batch_size": 1,
                    "precision": precision,
                    "enhancement_level": "medium",
                    "memory_fraction": 0.25,
                    "noise_reduction": False
                },
                "step_08_quality_assessment": {
                    "batch_size": 1,
                    "precision": precision,
                    "quality_metrics": ["ssim", "lpips"],
                    "memory_fraction": 0.2,
                    "assessment_threshold": 0.6
                }
            }
    
    def _apply_environment_optimizations(self):
        """í™˜ê²½ ìµœì í™” ì ìš©"""
        if not TORCH_AVAILABLE:
            return
        
        try:
            # PyTorch ìŠ¤ë ˆë“œ ì„¤ì •
            torch.set_num_threads(self.optimization_settings["max_workers"])
            
            # MPS ìµœì í™”
            if self.device == "mps":
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                # M3 Max íŠ¹í™” í™˜ê²½ ë³€ìˆ˜
                if self.is_m3_max:
                    os.environ.update({
                        'PYTORCH_MPS_ALLOCATOR_POLICY': 'garbage_collection',
                        'METAL_DEVICE_WRAPPER_TYPE': '1',
                        'METAL_PERFORMANCE_SHADERS_ENABLED': '1',
                        'PYTORCH_MPS_PREFER_METAL': '1',
                        'OMP_NUM_THREADS': '16',
                        'MKL_NUM_THREADS': '16',
                        'VECLIB_MAXIMUM_THREADS': '16'
                    })
            
            # CUDA ìµœì í™”
            elif self.device == "cuda":
                if hasattr(torch.backends, 'cudnn'):
                    torch.backends.cudnn.enabled = True
                    torch.backends.cudnn.benchmark = True
                    torch.backends.cudnn.deterministic = False
                
                os.environ.update({
                    'CUDA_LAUNCH_BLOCKING': '0',
                    'CUDA_CACHE_DISABLE': '0'
                })
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            os.environ.update({
                'MALLOC_TRIM_THRESHOLD_': '65536',
                'PYTHONHASHSEED': '0'
            })
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”
            gc.collect()
            
        except Exception as e:
            logger.warning(f"âš ï¸ í™˜ê²½ ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
    
    # =========================================================================
    # ğŸ”§ í•µì‹¬ ì¸í„°í˜ì´ìŠ¤ ë©”ì„œë“œë“¤
    # =========================================================================
    
    def get_device(self) -> str:
        """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self.device
    
    def get_device_name(self) -> str:
        """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
        return self.device_name
    
    def get_memory_info(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
        try:
            if PSUTIL_AVAILABLE:
                memory = psutil.virtual_memory()
                return {
                    "total_gb": round(memory.total / (1024**3), 1),
                    "available_gb": round(memory.available / (1024**3), 1),
                    "used_gb": round(memory.used / (1024**3), 1),
                    "used_percent": round(memory.percent, 1),
                    "device": self.device,
                    "device_memory_gb": self.hardware.gpu_info.get("memory_gb", 0),
                    "unified_memory": self.device == "mps",
                    "timestamp": time.time()
                }
        except Exception as e:
            logger.debug(f"ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        # í´ë°± ì •ë³´
        return {
            "total_gb": self.memory_gb,
            "available_gb": self.memory_gb * 0.7,
            "used_percent": 30.0,
            "device": self.device,
            "device_memory_gb": self.hardware.gpu_info.get("memory_gb", 0),
            "unified_memory": self.device == "mps",
            "timestamp": time.time(),
            "fallback_mode": True
        }
    
    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë¦¬ (PyTorch 2.6+ ì™„ì „ í˜¸í™˜)"""
        try:
            start_time = time.time()
            methods_used = []
            
            # ê¸°ë³¸ Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected = gc.collect()
            if collected > 0:
                methods_used.append(f"gc_collected_{collected}")
            
            if not TORCH_AVAILABLE:
                return {
                    "success": True,
                    "device": self.device,
                    "methods": methods_used,
                    "duration": time.time() - start_time,
                    "pytorch_available": False
                }
            
            # PyTorch ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device == "mps":
                # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬)
                mps_cleaned = False
                
                # torch.mps.empty_cache() (ìµœì‹  ë²„ì „)
                if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
                    try:
                        torch.mps.empty_cache()
                        methods_used.append("mps_empty_cache")
                        mps_cleaned = True
                    except Exception as e:
                        logger.debug(f"torch.mps.empty_cache() ì‹¤íŒ¨: {e}")
                
                # torch.mps.synchronize() (ëŒ€ì•ˆ)
                if not mps_cleaned and hasattr(torch, 'mps') and hasattr(torch.mps, 'synchronize'):
                    try:
                        torch.mps.synchronize()
                        methods_used.append("mps_synchronize")
                        mps_cleaned = True
                    except Exception as e:
                        logger.debug(f"torch.mps.synchronize() ì‹¤íŒ¨: {e}")
                
                # torch.backends.mps.empty_cache() (ì´ì „ ë²„ì „)
                if not mps_cleaned and hasattr(torch.backends, 'mps'):
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        try:
                            torch.backends.mps.empty_cache()
                            methods_used.append("mps_backends_empty_cache")
                            mps_cleaned = True
                        except Exception as e:
                            logger.debug(f"torch.backends.mps.empty_cache() ì‹¤íŒ¨: {e}")
                
                if not mps_cleaned:
                    methods_used.append("mps_cleanup_unavailable")
            
            elif self.device == "cuda":
                # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
                try:
                    if hasattr(torch, 'cuda') and hasattr(torch.cuda, 'empty_cache'):
                        torch.cuda.empty_cache()
                        methods_used.append("cuda_empty_cache")
                        
                        if aggressive and hasattr(torch.cuda, 'synchronize'):
                            torch.cuda.synchronize()
                            methods_used.append("cuda_synchronize")
                except Exception as e:
                    logger.debug(f"CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                    methods_used.append("cuda_cleanup_failed")
            
            # Aggressive ëª¨ë“œ
            if aggressive:
                for _ in range(3):
                    gc.collect()
                methods_used.append("aggressive_gc")
                
                # ì¶”ê°€ ë©”ëª¨ë¦¬ ìµœì í™”
                if PSUTIL_AVAILABLE:
                    try:
                        process = psutil.Process()
                        process.memory_info()  # ë©”ëª¨ë¦¬ ìƒíƒœ ê°±ì‹ 
                        methods_used.append("process_memory_refresh")
                    except:
                        pass
            
            return {
                "success": True,
                "device": self.device,
                "methods": methods_used,
                "duration": round(time.time() - start_time, 3),
                "pytorch_available": True,
                "aggressive": aggressive,
                "timestamp": time.time()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)[:200],
                "device": self.device,
                "duration": time.time() - start_time if 'start_time' in locals() else 0,
                "pytorch_available": TORCH_AVAILABLE,
                "timestamp": time.time()
            }
    
    def setup_memory_optimization(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •"""
        try:
            if TORCH_AVAILABLE and self.device == "cuda":
                try:
                    torch.cuda.empty_cache()
                    # ë©”ëª¨ë¦¬ ë¶„í•  ë°©ì§€
                    torch.cuda.set_per_process_memory_fraction(
                        self.optimization_settings["memory_fraction"]
                    )
                except Exception as e:
                    logger.debug(f"CUDA ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            
            # í™˜ê²½ ë³€ìˆ˜ ìµœì í™”
            os.environ['MALLOC_TRIM_THRESHOLD_'] = '65536'
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def get_optimal_settings(self) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì„¤ì • ë°˜í™˜"""
        return {
            "device_config": self.model_config.copy(),
            "optimization_settings": self.optimization_settings.copy(),
            "pipeline_optimizations": self.pipeline_optimizations.copy(),
            "device_info": self.device_info.copy(),
            "device_capabilities": {
                "device": self.device_capabilities.device,
                "name": self.device_capabilities.name,
                "memory_gb": self.device_capabilities.memory_gb,
                "supports_fp16": self.device_capabilities.supports_fp16,
                "supports_fp32": self.device_capabilities.supports_fp32,
                "supports_neural_engine": self.device_capabilities.supports_neural_engine,
                "supports_metal_shaders": self.device_capabilities.supports_metal_shaders,
                "unified_memory": self.device_capabilities.unified_memory,
                "max_batch_size": self.device_capabilities.max_batch_size,
                "recommended_image_size": self.device_capabilities.recommended_image_size
            }
        }
    
    def get_device_capabilities(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ì •ë³´ ë°˜í™˜"""
        return {
            "device": self.device_capabilities.device,
            "name": self.device_capabilities.name,
            "memory_gb": self.device_capabilities.memory_gb,
            "supports_fp16": self.device_capabilities.supports_fp16,
            "supports_fp32": self.device_capabilities.supports_fp32,
            "supports_neural_engine": self.device_capabilities.supports_neural_engine,
            "supports_metal_shaders": self.device_capabilities.supports_metal_shaders,
            "unified_memory": self.device_capabilities.unified_memory,
            "max_batch_size": self.device_capabilities.max_batch_size,
            "recommended_image_size": self.device_capabilities.recommended_image_size,
            "supports_8step_pipeline": True,
            "optimization_level": self.optimization_level,
            "performance_class": self.hardware.performance_class,
            "pytorch_version": TORCH_VERSION,
            "float_compatibility_mode": True,
            "stable_operation_mode": True
        }
    
    # =========================================================================
    # ğŸ”§ ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤ (í˜¸í™˜ì„±)
    # =========================================================================
    
    def get(self, key: str, default: Any = None) -> Any:
        """ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì ‘ê·¼"""
        # ì§ì ‘ ì†ì„± ë§¤í•‘
        direct_attrs = {
            'device': self.device,
            'device_name': self.device_name,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'optimization_level': self.optimization_level,
            'is_initialized': self.is_initialized,
            'float_compatibility_mode': self.float_compatibility_mode,
            'pytorch_version': TORCH_VERSION,
            'numpy_version': NUMPY_VERSION
        }
        
        if key in direct_attrs:
            return direct_attrs[key]
        
        # ì„¤ì • ë”•ì…”ë„ˆë¦¬ì—ì„œ ê²€ìƒ‰
        for config_dict in [self.model_config, self.optimization_settings, 
                           self.device_info, self.pipeline_optimizations]:
            if key in config_dict:
                return config_dict[key]
        
        # ê°ì²´ ì†ì„±ì—ì„œ ê²€ìƒ‰
        if hasattr(self, key):
            return getattr(self, key)
        
        return default
    
    def __getitem__(self, key: str) -> Any:
        """[] ì ‘ê·¼ì ì§€ì›"""
        result = self.get(key)
        if result is None:
            raise KeyError(f"Key '{key}' not found in GPUConfig")
        return result
    
    def __contains__(self, key: str) -> bool:
        """in ì—°ì‚°ì ì§€ì›"""
        return self.get(key) is not None
    
    def keys(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ ëª©ë¡"""
        keys = set()
        keys.update(['device', 'device_name', 'device_type', 'memory_gb', 'is_m3_max'])
        keys.update(self.model_config.keys())
        keys.update(self.optimization_settings.keys())
        keys.update(self.device_info.keys())
        return list(keys)
    
    def items(self):
        """ì•„ì´í…œ ëª©ë¡ ë°˜í™˜"""
        result = {}
        for key in self.keys():
            result[key] = self.get(key)
        return result.items()

# =============================================================================
# ğŸ”§ í™•ì¥ëœ GPU ë§¤ë‹ˆì € í´ë˜ìŠ¤
# =============================================================================

class GPUManager(GPUConfig):
    """
    í™•ì¥ëœ GPU ë§¤ë‹ˆì € - GPUConfig ìƒì†
    ì¶”ê°€ ê´€ë¦¬ ê¸°ëŠ¥ ì œê³µ
    """
    
    def __init__(self, **kwargs):
        """GPU ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        super().__init__(**kwargs)
        
        # ì¶”ê°€ ë§¤ë‹ˆì € ê¸°ëŠ¥
        self.active_models = weakref.WeakValueDictionary()
        self.memory_usage_history = []
        self.performance_metrics = {}
        self.session_cache = {}
        self._executor = ThreadPoolExecutor(max_workers=4)
        
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self._start_monitoring()
    
    def _start_monitoring(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        try:
            # ì´ˆê¸° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            self.performance_metrics = {
                "initialization_time": time.time(),
                "memory_cleanups": 0,
                "model_loads": 0,
                "pipeline_runs": 0,
                "total_processing_time": 0.0,
                "average_memory_usage": 0.0
            }
        except Exception as e:
            logger.debug(f"ëª¨ë‹ˆí„°ë§ ì‹œì‘ ì‹¤íŒ¨: {e}")
    
    def register_model(self, name: str, model: Any) -> bool:
        """ëª¨ë¸ ë“±ë¡"""
        try:
            self.active_models[name] = model
            self.performance_metrics["model_loads"] += 1
            return True
        except Exception as e:
            logger.debug(f"ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def unregister_model(self, name: str) -> bool:
        """ëª¨ë¸ í•´ì œ"""
        try:
            if name in self.active_models:
                del self.active_models[name]
                return True
            return False
        except Exception as e:
            logger.debug(f"ëª¨ë¸ í•´ì œ ì‹¤íŒ¨ {name}: {e}")
            return False
    
    def get_active_models(self) -> List[str]:
        """í™œì„± ëª¨ë¸ ëª©ë¡"""
        return list(self.active_models.keys())
    
    def cleanup_inactive_models(self) -> Dict[str, Any]:
        """ë¹„í™œì„± ëª¨ë¸ ì •ë¦¬"""
        try:
            before_count = len(self.active_models)
            # WeakValueDictionaryê°€ ìë™ìœ¼ë¡œ ì •ë¦¬
            after_count = len(self.active_models)
            
            return {
                "success": True,
                "before_count": before_count,
                "after_count": after_count,
                "cleaned": before_count - after_count
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        current_metrics = self.performance_metrics.copy()
        current_metrics.update({
            "active_models": len(self.active_models),
            "uptime": time.time() - current_metrics.get("initialization_time", time.time()),
            "memory_info": self.get_memory_info()
        })
        return current_metrics
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            if hasattr(self, '_executor'):
                self._executor.shutdown(wait=False)
        except:
            pass

# =============================================================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„±)
# =============================================================================

@lru_cache(maxsize=1)
def get_gpu_config(**kwargs) -> GPUConfig:
    """GPU ì„¤ì • ì‹±ê¸€í†¤ íŒ©í† ë¦¬"""
    return GPUConfig(**kwargs)

def get_device_config() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config.model_config
    except Exception as e:
        return {"error": str(e), "device": "cpu"}

def get_model_config() -> Dict[str, Any]:
    """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
    return get_device_config()

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config.device_info
    except Exception as e:
        return {"error": str(e), "device": "cpu"}

def get_device() -> str:
    """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config.device
    except:
        return "cpu"

def get_device_name() -> str:
    """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config.device_name
    except:
        return "CPU"

def is_m3_max() -> bool:
    """M3 Max ì—¬ë¶€ í™•ì¸"""
    try:
        config = get_gpu_config()
        return config.is_m3_max
    except:
        return False

def get_optimal_settings() -> Dict[str, Any]:
    """ìµœì í™”ëœ ì„¤ì • ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config.get_optimal_settings()
    except Exception as e:
        return {"error": str(e)}

def get_device_capabilities() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ì •ë³´ ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config.get_device_capabilities()
    except Exception as e:
        return {"error": str(e)}

def apply_optimizations() -> bool:
    """ìµœì í™” ì ìš©"""
    try:
        config = get_gpu_config()
        config.setup_memory_optimization()
        return config.is_initialized
    except:
        return False

def check_memory_available(min_gb: float = 1.0) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸"""
    try:
        config = get_gpu_config()
        memory_info = config.get_memory_info()
        
        available_gb = memory_info.get("available_gb", 0)
        is_available = available_gb >= min_gb
        
        return {
            "device": config.device,
            "min_required_gb": min_gb,
            "available_gb": available_gb,
            "is_available": is_available,
            "memory_info": memory_info,
            "timestamp": time.time()
        }
    except Exception as e:
        return {
            "device": "unknown",
            "error": str(e),
            "is_available": False,
            "min_required_gb": min_gb,
            "timestamp": time.time()
        }

def optimize_memory(aggressive: bool = False) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰"""
    try:
        config = get_gpu_config()
        return config.cleanup_memory(aggressive=aggressive)
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "device": "unknown",
            "timestamp": time.time()
        }

def get_memory_info() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config.get_memory_info()
    except Exception as e:
        return {
            "error": str(e),
            "device": "unknown",
            "timestamp": time.time()
        }

# =============================================================================
# ğŸ”§ ì „ì—­ GPU ì„¤ì • ë§¤ë‹ˆì € ìƒì„± (ì•ˆì „í•œ ì´ˆê¸°í™”)
# =============================================================================

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™” (ì•ˆì „í•œ ì²˜ë¦¬)
try:
    # GPU ì„¤ì • ë§¤ë‹ˆì € ìƒì„±
    gpu_config = GPUManager()
    
    # í¸ì˜ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ë“¤
    DEVICE = gpu_config.device
    DEVICE_NAME = gpu_config.device_name
    DEVICE_TYPE = gpu_config.device_type
    MODEL_CONFIG = gpu_config.model_config
    DEVICE_INFO = gpu_config.device_info
    IS_M3_MAX = gpu_config.is_m3_max
    
    # ì´ˆê¸°í™” ì„±ê³µ ë©”ì‹œì§€ (ìµœì†Œí™”)
    if IS_M3_MAX:
        print(f"ğŸ M3 Max ({DEVICE}) ìµœì í™” ëª¨ë“œ í™œì„±í™” - Float32 ì•ˆì •ì„± ìš°ì„ ")
    else:
        print(f"âœ… GPU ì„¤ì • ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - ì•ˆì •ì„± ìš°ì„  ëª¨ë“œ")

except Exception as e:
    # í´ë°± ì„¤ì • (ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ)
    print(f"âš ï¸ GPU ì„¤ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:100]}")
    
    # ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±
    DEVICE = "cpu"
    DEVICE_NAME = "CPU (Fallback)"
    DEVICE_TYPE = "cpu"
    MODEL_CONFIG = {
        "device": "cpu",
        "dtype": "float32",
        "batch_size": 1,
        "optimization_level": "safe",
        "float_compatibility_mode": True
    }
    DEVICE_INFO = {
        "device": "cpu",
        "error": "GPU config initialization failed",
        "fallback_mode": True
    }
    IS_M3_MAX = False
    
    # ë”ë¯¸ GPU ì„¤ì • ê°ì²´ ìƒì„±
    class DummyGPUConfig:
        def __init__(self):
            self.device = "cpu"
            self.device_name = "CPU (Fallback)"
            self.is_m3_max = False
            self.is_initialized = False
            self.optimization_settings = {"optimization_level": "safe"}
            self.model_config = MODEL_CONFIG
            self.device_info = DEVICE_INFO
        
        def get(self, key, default=None):
            return getattr(self, key, default)
        
        def cleanup_memory(self, aggressive=False):
            return {"success": True, "method": "fallback_gc", "device": "cpu"}
        
        def get_device(self):
            return self.device
        
        def get_device_name(self):
            return self.device_name
        
        def get_memory_info(self):
            return {"total_gb": 8, "available_gb": 6, "device": "cpu"}
        
        def setup_memory_optimization(self):
            pass
        
        def get_optimal_settings(self):
            return {"device": "cpu", "optimization_level": "safe"}
        
        def get_device_capabilities(self):
            return {"device": "cpu", "supports_fp32": True}
    
    gpu_config = DummyGPUConfig()

# =============================================================================
# ğŸ”§ Export ë¦¬ìŠ¤íŠ¸ (ì™„ì „í•œ API)
# =============================================================================

__all__ = [
    # ì£¼ìš” í´ë˜ìŠ¤ë“¤
    'GPUConfig', 'GPUManager', 'HardwareDetector', 'DeviceCapabilities',
    
    # ì „ì—­ ê°ì²´ë“¤
    'gpu_config', 'DEVICE', 'DEVICE_NAME', 'DEVICE_TYPE', 
    'MODEL_CONFIG', 'DEVICE_INFO', 'IS_M3_MAX',
    
    # í•µì‹¬ í•¨ìˆ˜ë“¤
    'get_gpu_config', 'get_device_config', 'get_model_config', 'get_device_info',
    'get_device', 'get_device_name', 'is_m3_max', 'get_optimal_settings', 
    'get_device_capabilities', 'apply_optimizations',
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤
    'check_memory_available', 'optimize_memory', 'get_memory_info',
    
    # ì—´ê±°í˜• í´ë˜ìŠ¤ë“¤
    'OptimizationLevel', 'DeviceType'
]

# ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë¡œê·¸ (ìµœì†Œí™”)
print("âœ… GPU ì„¤ì • ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - ì•ˆì •ì„± ìš°ì„  ëª¨ë“œ")