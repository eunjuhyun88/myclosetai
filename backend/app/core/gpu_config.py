"""
GPU μ„¤μ • λ° λ””λ°”μ΄μ¤ κ΄€λ¦¬ (νΈν™μ„± κ°μ„  λ²„μ „)
"""

import torch
import logging

logger = logging.getLogger(__name__)

def setup_device():
    """λ””λ°”μ΄μ¤ μ„¤μ •"""
    try:
        if torch.backends.mps.is_available():
            device = "mps"
            print("π Apple Silicon MPS μ‚¬μ©")
            
            # MPS μΊμ‹ μ •λ¦¬ (λ²„μ „ νΈν™μ„± μ²΄ν¬)
            try:
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            except AttributeError:
                logger.info("MPS empty_cache λ―Έμ§€μ› (PyTorch λ²„μ „ λ¬Έμ )")
                
        elif torch.cuda.is_available():
            device = "cuda"
            print("π€ NVIDIA CUDA μ‚¬μ©")
            torch.cuda.empty_cache()
        else:
            device = "cpu"
            print("π’» CPU μ‚¬μ©")
            
    except Exception as e:
        logger.warning(f"λ””λ°”μ΄μ¤ μ„¤μ • μ¤‘ μ¤λ¥: {e}")
        device = "cpu"
        print("π’» CPU μ‚¬μ© (fallback)")
    
    return device

# κΈ€λ΅λ² μ„¤μ •
DEVICE = setup_device()

MODEL_CONFIG = {
    "device": DEVICE,
    "dtype": torch.float16 if DEVICE in ["cuda", "mps"] else torch.float32,
    "max_batch_size": 4 if DEVICE != "cpu" else 1
}

gpu_config = {
    "device": DEVICE,
    "available": DEVICE != "cpu",
    "config": MODEL_CONFIG
}

DEVICE_INFO = {
    "device": DEVICE,
    "torch_version": torch.__version__,
    "mps_available": torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False,
    "cuda_available": torch.cuda.is_available()
}

logger.info(f"λ””λ°”μ΄μ¤ μ„¤μ • μ™„λ£: {DEVICE}")
