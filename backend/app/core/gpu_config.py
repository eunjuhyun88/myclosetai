# app/core/gpu_config.py
"""
MyCloset AI - GPU ì„¤ì • ë° M3 Max ìµœì í™”
ìˆ˜ì •ëœ ë²„ì „: import ì˜¤ë¥˜ í•´ê²° ë° ì™„ì „í•œ M3 Max ì§€ì›
"""
import os
import logging
import platform
import subprocess
from typing import Dict, Any, Optional, Tuple

# PyTorch ì„ íƒì  import (ì•ˆì „ ì²˜ë¦¬)
try:
    import torch
    import torch.backends.mps
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

# psutil ì„ íƒì  import
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

logger = logging.getLogger(__name__)

class GPUConfig:
    """M3 Max ìµœì í™” GPU ì„¤ì • ê´€ë¦¬ì"""
    
    def __init__(self):
        self.device = "cpu"
        self.device_name = "CPU"
        self.memory_gb = 0
        self.is_apple_silicon = False
        self.is_m3_max = False
        self.optimization_level = "basic"
        
        self._detect_hardware()
        self._configure_device()
        self._apply_optimizations()
    
    def _detect_hardware(self):
        """í•˜ë“œì›¨ì–´ ê°ì§€ ë° M3 Max íŠ¹ë³„ ì²˜ë¦¬"""
        try:
            # macOS ë° Apple Silicon ê°ì§€
            is_macos = platform.system() == "Darwin"
            
            if is_macos:
                try:
                    result = subprocess.run(
                        ["sysctl", "-n", "machdep.cpu.brand_string"], 
                        capture_output=True, 
                        text=True, 
                        timeout=5
                    )
                    cpu_brand = result.stdout.strip()
                    
                    if "Apple" in cpu_brand:
                        self.is_apple_silicon = True
                        
                        # M3 Max íŠ¹ë³„ ê°ì§€
                        if "M3 Max" in cpu_brand:
                            self.is_m3_max = True
                            self.optimization_level = "m3_max"
                            logger.info(f"ğŸ M3 Max ê°ì§€: {cpu_brand}")
                        elif "M3" in cpu_brand:
                            self.optimization_level = "m3"
                        elif "M2" in cpu_brand:
                            self.optimization_level = "m2"
                        elif "M1" in cpu_brand:
                            self.optimization_level = "m1"
                            
                        self.device_name = cpu_brand
                        
                except subprocess.TimeoutExpired:
                    logger.warning("CPU ì •ë³´ ê°ì§€ íƒ€ì„ì•„ì›ƒ")
                except Exception as e:
                    logger.warning(f"CPU ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
            
            # ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘
            if PSUTIL_AVAILABLE:
                self.memory_gb = psutil.virtual_memory().total / (1024**3)
            else:
                # í´ë°±: sysctlë¡œ ë©”ëª¨ë¦¬ ì •ë³´ íšë“ (macOS)
                if is_macos:
                    try:
                        result = subprocess.run(
                            ["sysctl", "-n", "hw.memsize"], 
                            capture_output=True, 
                            text=True, 
                            timeout=5
                        )
                        memory_bytes = int(result.stdout.strip())
                        self.memory_gb = memory_bytes / (1024**3)
                    except:
                        self.memory_gb = 16.0  # ê¸°ë³¸ê°’
                        
        except Exception as e:
            logger.error(f"í•˜ë“œì›¨ì–´ ê°ì§€ ì‹¤íŒ¨: {e}")
            self.memory_gb = 8.0  # ì•ˆì „í•œ ê¸°ë³¸ê°’
    
    def _configure_device(self):
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if not TORCH_AVAILABLE:
            self.device = "cpu"
            logger.warning("PyTorch ë¯¸ì„¤ì¹˜ - CPU ëª¨ë“œ")
            return
            
        # MPS (Apple Silicon) ìš°ì„ 
        if self.is_apple_silicon and torch.backends.mps.is_available():
            self.device = "mps"
            logger.info("âœ… MPS (Metal Performance Shaders) í™œì„±í™”")
            
        # CUDA ë‘ ë²ˆì§¸ ìš°ì„ ìˆœìœ„
        elif torch.cuda.is_available():
            self.device = "cuda"
            self.device_name = torch.cuda.get_device_name(0)
            logger.info(f"âœ… CUDA í™œì„±í™”: {self.device_name}")
            
        # CPU í´ë°±
        else:
            self.device = "cpu"
            logger.info("CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    def _apply_optimizations(self):
        """M3 Max íŠ¹í™” ìµœì í™” ì ìš©"""
        if not TORCH_AVAILABLE:
            return
            
        try:
            if self.device == "mps":
                self._optimize_mps()
            elif self.device == "cuda":
                self._optimize_cuda()
            else:
                self._optimize_cpu()
                
            logger.info(f"âœ… {self.device.upper()} ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
    
    def _optimize_mps(self):
        """MPS (Apple Silicon) ìµœì í™”"""
        try:
            # MPS í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
            
            if self.is_m3_max:
                # M3 Max íŠ¹ë³„ ìµœì í™” (128GB í†µí•© ë©”ëª¨ë¦¬)
                os.environ["PYTORCH_MPS_ALLOCATOR_POLICY"] = "garbage_collection"
                
                # ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ í™œìš©
                if self.memory_gb >= 64:
                    torch.mps.set_per_process_memory_fraction(0.8)
                    logger.info("ğŸš€ M3 Max ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™”")
            
            # MPS ì´ˆê¸°í™”
            if torch.backends.mps.is_available():
                test_tensor = torch.randn(1).to('mps')
                del test_tensor
                torch.mps.empty_cache()
                
            logger.info("ğŸ MPS ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"MPS ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_cuda(self):
        """CUDA ìµœì í™”"""
        try:
            torch.backends.cudnn.enabled = True
            torch.backends.cudnn.benchmark = True
            torch.cuda.empty_cache()
            logger.info("ğŸ® CUDA ìµœì í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"CUDA ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_cpu(self):
        """CPU ìµœì í™”"""
        try:
            if PSUTIL_AVAILABLE:
                cpu_cores = psutil.cpu_count(logical=False)
                torch.set_num_threads(min(cpu_cores, 8))
            logger.info("ğŸ–¥ï¸ CPU ìµœì í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"CPU ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def get_recommended_batch_size(self) -> int:
        """ê¶Œì¥ ë°°ì¹˜ í¬ê¸° ë°˜í™˜"""
        if self.device == "mps":
            if self.is_m3_max and self.memory_gb >= 64:
                return 2  # M3 Maxì—ì„œ ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°
            return 1
        elif self.device == "cuda":
            return 4
        else:
            return 1
    
    def get_recommended_precision(self) -> str:
        """ê¶Œì¥ ì •ë°€ë„ ë°˜í™˜"""
        if self.device in ["mps", "cuda"]:
            return "float16"  # Mixed precision ì§€ì›
        return "float32"
    
    def get_memory_fraction(self) -> float:
        """ë©”ëª¨ë¦¬ í• ë‹¹ ë¹„ìœ¨ ë°˜í™˜"""
        if self.is_m3_max and self.memory_gb >= 64:
            return 0.8  # 128GBì—ì„œ 80% ì‚¬ìš©
        elif self.device == "mps":
            return 0.7
        elif self.device == "cuda":
            return 0.8
        return 1.0
    
    def get_config_dict(self) -> Dict[str, Any]:
        """ì™„ì „í•œ ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {
            "device": self.device,
            "device_name": self.device_name,
            "memory_gb": round(self.memory_gb, 1),
            "is_apple_silicon": self.is_apple_silicon,
            "is_m3_max": self.is_m3_max,
            "optimization_level": self.optimization_level,
            "recommended_batch_size": self.get_recommended_batch_size(),
            "recommended_precision": self.get_recommended_precision(),
            "memory_fraction": self.get_memory_fraction(),
            "torch_available": TORCH_AVAILABLE,
            "mps_available": TORCH_AVAILABLE and torch.backends.mps.is_available() if TORCH_AVAILABLE else False,
            "cuda_available": TORCH_AVAILABLE and torch.cuda.is_available() if TORCH_AVAILABLE else False
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° export (ì§€ì—° ë¡œë”©)
_gpu_config_instance: Optional[GPUConfig] = None

def get_gpu_config() -> GPUConfig:
    """GPU ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _gpu_config_instance
    if _gpu_config_instance is None:
        _gpu_config_instance = GPUConfig()
    return _gpu_config_instance

def configure_gpu() -> Dict[str, Any]:
    """GPU ì„¤ì • ë° ì •ë³´ ë°˜í™˜"""
    config = get_gpu_config()
    return config.get_config_dict()

def get_optimal_device() -> str:
    """ìµœì  ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    config = get_gpu_config()
    return config.device

def is_m3_max() -> bool:
    """M3 Max ì—¬ë¶€ í™•ì¸"""
    config = get_gpu_config()
    return config.is_m3_max

def get_device_info() -> Tuple[str, str, float]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜ (device, name, memory)"""
    config = get_gpu_config()
    return config.device, config.device_name, config.memory_gb

# í™˜ê²½ ë³€ìˆ˜ ìë™ ì„¤ì •
def _set_environment_variables():
    """ìµœì í™” í™˜ê²½ ë³€ìˆ˜ ì„¤ì •"""
    # PyTorch í™˜ê²½ ë³€ìˆ˜
    os.environ.setdefault("PYTORCH_ENABLE_MPS_FALLBACK", "1")
    os.environ.setdefault("PYTORCH_MPS_HIGH_WATERMARK_RATIO", "0.0")
    
    # ë©€í‹°í”„ë¡œì„¸ì‹± ìµœì í™”
    if PSUTIL_AVAILABLE:
        cpu_count = psutil.cpu_count(logical=False)
        os.environ.setdefault("OMP_NUM_THREADS", str(min(cpu_count, 8)))
        os.environ.setdefault("MKL_NUM_THREADS", str(min(cpu_count, 8)))

# ëª¨ë“ˆ ë¡œë“œ ì‹œ ìë™ ì‹¤í–‰
_set_environment_variables()

# ì§€ì—° ë¡œë”©ì„ ìœ„í•œ í¸ì˜ ë³€ìˆ˜ë“¤ (ì‹¤ì œ ì‚¬ìš© ì‹œì—ë§Œ ì´ˆê¸°í™”)
def get_gpu_device():
    """í˜„ì¬ GPU ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return get_gpu_config().device

def get_gpu_info():
    """GPU ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
    return get_gpu_config().get_config_dict()

# Exportí•  ê²ƒë“¤ ëª…ì‹œ
__all__ = [
    'GPUConfig',
    'get_gpu_config', 
    'configure_gpu',
    'get_optimal_device',
    'is_m3_max',
    'get_device_info',
    'get_gpu_device',
    'get_gpu_info'
]