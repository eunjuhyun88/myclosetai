# app/core/gpu_config.py
"""
ìµœì  GPU ì„¤ì • ì‹œìŠ¤í…œ - ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ê´€ë¦¬
- ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€ ë° ìµœì í™”
- M3 Max íŠ¹í™” ì„¤ì •
- ë©”ëª¨ë¦¬ ìµœì í™”
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
"""
import os
import platform
import subprocess
import logging
import psutil
from typing import Dict, Any, Optional, List, Tuple, Union
from functools import lru_cache
from dataclasses import dataclass
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)

# ===============================================================
# ğŸ¯ ìµœì  GPU ì„¤ì • ë² ì´ìŠ¤ í´ë˜ìŠ¤
# ===============================================================

@dataclass
class DeviceInfo:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°ì´í„° í´ë˜ìŠ¤"""
    device: str
    device_type: str
    name: str
    memory_gb: float
    compute_capability: Optional[str] = None
    driver_version: Optional[str] = None
    is_available: bool = True
    optimization_level: str = 'balanced'
    supports_mixed_precision: bool = False
    supports_dynamic_batching: bool = False

class OptimalGPUConfigBase(ABC):
    """
    ğŸ¯ ìµœì í™”ëœ GPU ì„¤ì • ë² ì´ìŠ¤ í´ë˜ìŠ¤
    - ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€
    - ì§€ëŠ¥ì  ìµœì í™”
    - ë©”ëª¨ë¦¬ ê´€ë¦¬
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    """

    def __init__(
        self,
        preferred_device: Optional[str] = None,  # ì„ í˜¸ ë””ë°”ì´ìŠ¤
        memory_fraction: float = 0.8,  # ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„ìœ¨
        enable_optimization: bool = True,  # ìµœì í™” í™œì„±í™”
        **kwargs  # í™•ì¥ íŒŒë¼ë¯¸í„°
    ):
        """
        âœ… ìµœì  GPU ì„¤ì • ìƒì„±ì

        Args:
            preferred_device: ì„ í˜¸í•˜ëŠ” ë””ë°”ì´ìŠ¤ (None=ìë™ê°ì§€)
            memory_fraction: GPU ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„ìœ¨ (0.1~1.0)
            enable_optimization: ìµœì í™” í™œì„±í™” ì—¬ë¶€
            **kwargs: í™•ì¥ íŒŒë¼ë¯¸í„°ë“¤
                - force_cpu: bool = False
                - mixed_precision: bool = auto
                - enable_profiling: bool = False
                - memory_growth: bool = True
                - ê¸°íƒ€...
        """
        self.preferred_device = preferred_device
        self.memory_fraction = max(0.1, min(1.0, memory_fraction))
        self.enable_optimization = enable_optimization
        self.kwargs = kwargs
        
        # 1. ğŸ’¡ ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
        self.system_info = self._collect_system_info()
        
        # 2. ğŸ–¥ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ìŠ¤ìº”
        self.available_devices = self._scan_available_devices()
        
        # 3. ğŸ¯ ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ
        self.device_info = self._select_optimal_device()
        
        # 4. âš™ï¸ ë””ë°”ì´ìŠ¤ë³„ ì„¤ì • ì ìš©
        self._configure_device()
        
        # 5. ğŸš€ ìµœì í™” ì ìš©
        if self.enable_optimization:
            self._apply_optimizations()
        
        # 6. ğŸ“Š ëª¨ë‹ˆí„°ë§ ì„¤ì •
        self._setup_monitoring()
        
        logger.info(f"ğŸ¯ GPU ì„¤ì • ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device_info.device} ({self.device_info.name})")

    def _collect_system_info(self) -> Dict[str, Any]:
        """ğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        return {
            'platform': platform.system(),
            'machine': platform.machine(),
            'processor': platform.processor(),
            'cpu_count': os.cpu_count() or 4,
            'total_memory_gb': self._get_total_memory_gb(),
            'available_memory_gb': self._get_available_memory_gb(),
            'is_m3_max': self._detect_m3_max(),
            'is_container': self._detect_container(),
            'python_version': platform.python_version()
        }

    def _get_total_memory_gb(self) -> float:
        """ì´ ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)"""
        try:
            return psutil.virtual_memory().total / (1024**3)
        except:
            return 16.0  # ê¸°ë³¸ê°’

    def _get_available_memory_gb(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB)"""
        try:
            return psutil.virtual_memory().available / (1024**3)
        except:
            return 8.0  # ê¸°ë³¸ê°’

    def _detect_m3_max(self) -> bool:
        """ğŸ M3 Max ì¹© ê°ì§€"""
        if platform.system() != 'Darwin':
            return False
        
        try:
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                capture_output=True, text=True, timeout=5
            )
            cpu_info = result.stdout.strip()
            return any(chip in cpu_info for chip in ['M3', 'M2', 'M1']) and 'Max' in cpu_info
        except:
            return False

    def _detect_container(self) -> bool:
        """ğŸ³ ì»¨í…Œì´ë„ˆ í™˜ê²½ ê°ì§€"""
        indicators = [
            os.path.exists('/.dockerenv'),
            os.getenv('KUBERNETES_SERVICE_HOST') is not None,
            os.getenv('CONTAINER') is not None
        ]
        return any(indicators)

    @abstractmethod
    def _scan_available_devices(self) -> List[DeviceInfo]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ìŠ¤ìº” (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass

    def _select_optimal_device(self) -> DeviceInfo:
        """ğŸ¯ ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        
        # ê°•ì œ CPU ëª¨ë“œ
        if self.kwargs.get('force_cpu', False):
            return self._create_cpu_device_info()
        
        # ì„ í˜¸ ë””ë°”ì´ìŠ¤ê°€ ìˆê³  ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
        if self.preferred_device:
            for device in self.available_devices:
                if device.device == self.preferred_device and device.is_available:
                    return device
        
        # ìë™ ì„ íƒ: ì„±ëŠ¥ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        if self.available_devices:
            # M3 Max MPS > NVIDIA CUDA > CPU ìˆœìœ¼ë¡œ ìš°ì„ ìˆœìœ„
            priority_order = ['mps', 'cuda', 'cpu']
            
            for device_type in priority_order:
                for device in self.available_devices:
                    if device.device == device_type and device.is_available:
                        return device
        
        # í´ë°±: CPU
        return self._create_cpu_device_info()

    def _create_cpu_device_info(self) -> DeviceInfo:
        """CPU ë””ë°”ì´ìŠ¤ ì •ë³´ ìƒì„±"""
        return DeviceInfo(
            device='cpu',
            device_type='cpu',
            name=f"CPU ({self.system_info['cpu_count']} cores)",
            memory_gb=self.system_info['available_memory_gb'],
            is_available=True,
            optimization_level='basic',
            supports_mixed_precision=False,
            supports_dynamic_batching=True
        )

    @abstractmethod
    def _configure_device(self):
        """ë””ë°”ì´ìŠ¤ë³„ ì„¤ì • (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass

    @abstractmethod
    def _apply_optimizations(self):
        """ìµœì í™” ì ìš© (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass

    def _setup_monitoring(self):
        """ğŸ“Š ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        self.monitoring_enabled = self.kwargs.get('enable_profiling', False)
        self.memory_stats = {
            'allocated': 0,
            'cached': 0,
            'max_allocated': 0
        }

    # ê³µí†µ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    def get_device(self) -> str:
        """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self.device_info.device

    def get_device_info(self) -> DeviceInfo:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return self.device_info

    def get_memory_info(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
        return {
            'total_gb': self.device_info.memory_gb,
            'allocated_gb': self.memory_stats['allocated'],
            'available_gb': self.device_info.memory_gb - self.memory_stats['allocated'],
            'memory_fraction': self.memory_fraction
        }

    def optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        if self.device_info.device == 'cuda':
            self._optimize_cuda_memory()
        elif self.device_info.device == 'mps':
            self._optimize_mps_memory()
        else:
            self._optimize_cpu_memory()

    def _optimize_cuda_memory(self):
        """CUDA ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
        except ImportError:
            pass

    def _optimize_mps_memory(self):
        """MPS ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            import torch
            if torch.backends.mps.is_available():
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                elif hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()
        except ImportError:
            pass

    def _optimize_cpu_memory(self):
        """CPU ë©”ëª¨ë¦¬ ìµœì í™”"""
        import gc
        gc.collect()

    def check_memory_available(self, required_gb: float = 4.0) -> bool:
        """ë©”ëª¨ë¦¬ ê°€ìš©ì„± ì²´í¬"""
        available_gb = self.device_info.memory_gb - self.memory_stats['allocated']
        return available_gb >= required_gb

# ===============================================================
# ğŸ¯ PyTorch GPU ì„¤ì • í´ë˜ìŠ¤
# ===============================================================

class PyTorchGPUConfig(OptimalGPUConfigBase):
    """
    ğŸ¯ PyTorch ì „ìš© GPU ì„¤ì •
    - PyTorch ë°±ì—”ë“œ ìµœì í™”
    - CUDA/MPS ì„¤ì •
    - ë©”ëª¨ë¦¬ ê´€ë¦¬
    - M3 Max íŠ¹í™”
    """

    def _scan_available_devices(self) -> List[DeviceInfo]:
        """PyTorch ë””ë°”ì´ìŠ¤ ìŠ¤ìº”"""
        devices = []
        
        try:
            import torch
            
            # MPS (Apple Silicon) í™•ì¸
            if torch.backends.mps.is_available():
                memory_gb = self._estimate_mps_memory()
                devices.append(DeviceInfo(
                    device='mps',
                    device_type='apple_silicon',
                    name=f"Apple Silicon MPS ({memory_gb:.1f}GB)",
                    memory_gb=memory_gb,
                    is_available=True,
                    optimization_level='ultra' if self.system_info['is_m3_max'] else 'high',
                    supports_mixed_precision=True,
                    supports_dynamic_batching=True
                ))
            
            # CUDA í™•ì¸
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    props = torch.cuda.get_device_properties(i)
                    memory_gb = props.total_memory / (1024**3)
                    
                    devices.append(DeviceInfo(
                        device='cuda',
                        device_type='nvidia_gpu',
                        name=f"{props.name} ({memory_gb:.1f}GB)",
                        memory_gb=memory_gb,
                        compute_capability=f"{props.major}.{props.minor}",
                        is_available=True,
                        optimization_level='high',
                        supports_mixed_precision=props.major >= 7,  # Tensor Cores
                        supports_dynamic_batching=True
                    ))
            
        except ImportError:
            logger.warning("PyTorch ì—†ì´ CPUë§Œ ì‚¬ìš© ê°€ëŠ¥")
        
        # CPUëŠ” í•­ìƒ ì¶”ê°€
        devices.append(self._create_cpu_device_info())
        
        return devices

    def _estimate_mps_memory(self) -> float:
        """MPS ë©”ëª¨ë¦¬ ì¶”ì •"""
        # Apple Siliconì˜ í†µí•© ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
        # ì¼ë°˜ì ìœ¼ë¡œ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ì˜ 80% ì •ë„ë¥¼ GPUê°€ í™œìš© ê°€ëŠ¥
        system_memory = self.system_info['total_memory_gb']
        
        if self.system_info['is_m3_max']:
            # M3 MaxëŠ” ë” ë§ì€ GPU ë©”ëª¨ë¦¬ í™œìš© ê°€ëŠ¥
            return min(system_memory * 0.85, 128.0)
        else:
            return min(system_memory * 0.75, 64.0)

    def _configure_device(self):
        """PyTorch ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        try:
            import torch
            
            if self.device_info.device == 'mps':
                self._configure_mps()
            elif self.device_info.device == 'cuda':
                self._configure_cuda()
            else:
                self._configure_cpu()
                
        except ImportError:
            logger.warning("PyTorch ì„¤ì • ì‹¤íŒ¨ - ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")

    def _configure_mps(self):
        """MPS ì„¤ì •"""
        try:
            import torch
            
            # MPS í´ë°± í™œì„±í™”
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            
            # M3 Max íŠ¹í™” ì„¤ì •
            if self.system_info['is_m3_max']:
                # ê³ ì„±ëŠ¥ ëª¨ë“œ
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                # ìŠ¤ë ˆë“œ ìµœì í™” (14ì½”ì–´ M3 Max)
                optimal_threads = min(8, self.system_info['cpu_count'])
                torch.set_num_threads(optimal_threads)
                
                logger.info(f"ğŸ M3 Max MPS ìµœì í™” ì™„ë£Œ - ìŠ¤ë ˆë“œ: {optimal_threads}")
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
                
        except Exception as e:
            logger.warning(f"MPS ì„¤ì • ì‹¤íŒ¨: {e}")

    def _configure_cuda(self):
        """CUDA ì„¤ì •"""
        try:
            import torch
            
            # ë©”ëª¨ë¦¬ fraction ì„¤ì •
            if self.memory_fraction < 1.0:
                torch.cuda.set_per_process_memory_fraction(self.memory_fraction)
            
            # ë©”ëª¨ë¦¬ growth í™œì„±í™” (TensorFlow ìŠ¤íƒ€ì¼)
            if self.kwargs.get('memory_growth', True):
                os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
            
            # CuDNN ìµœì í™”
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # ë©€í‹° GPU ì§€ì›
            if torch.cuda.device_count() > 1:
                logger.info(f"ğŸ® ë‹¤ì¤‘ GPU ê°ì§€: {torch.cuda.device_count()}ê°œ")
            
            logger.info(f"ğŸ® CUDA ìµœì í™” ì™„ë£Œ - ë©”ëª¨ë¦¬ fraction: {self.memory_fraction}")
            
        except Exception as e:
            logger.warning(f"CUDA ì„¤ì • ì‹¤íŒ¨: {e}")

    def _configure_cpu(self):
        """CPU ì„¤ì •"""
        try:
            import torch
            
            # CPU ìŠ¤ë ˆë“œ ìµœì í™”
            if self.enable_optimization:
                optimal_threads = min(self.system_info['cpu_count'], 8)
                torch.set_num_threads(optimal_threads)
                
                # OpenMP ìŠ¤ë ˆë“œ ì„¤ì •
                os.environ['OMP_NUM_THREADS'] = str(optimal_threads)
                os.environ['MKL_NUM_THREADS'] = str(optimal_threads)
                
                logger.info(f"âš¡ CPU ìµœì í™” ì™„ë£Œ - ìŠ¤ë ˆë“œ: {optimal_threads}")
                
        except Exception as e:
            logger.warning(f"CPU ì„¤ì • ì‹¤íŒ¨: {e}")

    def _apply_optimizations(self):
        """PyTorch ìµœì í™” ì ìš©"""
        try:
            import torch
            
            # Mixed Precision ì„¤ì •
            if self.device_info.supports_mixed_precision and self.kwargs.get('mixed_precision', True):
                logger.info("ğŸš€ Mixed Precision í™œì„±í™”")
            
            # JIT ì»´íŒŒì¼ ìµœì í™”
            if self.enable_optimization:
                torch.jit.set_fusion_strategy([('STATIC', 2), ('DYNAMIC', 2)])
                
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if self.device_info.device != 'cpu':
                self.optimize_memory()
                
        except Exception as e:
            logger.warning(f"PyTorch ìµœì í™” ì‹¤íŒ¨: {e}")

    def get_optimal_batch_size(self, base_size: int = 4) -> int:
        """ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        if self.device_info.device == 'mps' and self.system_info['is_m3_max']:
            # M3 MaxëŠ” ë” í° ë°°ì¹˜ ê°€ëŠ¥
            memory_multiplier = self.device_info.memory_gb / 32.0
            return int(base_size * min(memory_multiplier, 4.0))
        elif self.device_info.device == 'cuda':
            # CUDAëŠ” ë©”ëª¨ë¦¬ì— ë”°ë¼
            memory_multiplier = self.device_info.memory_gb / 16.0
            return int(base_size * min(memory_multiplier, 2.0))
        else:
            # CPUëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ
            return max(1, base_size // 2)

    def get_optimal_workers(self) -> int:
        """ìµœì  ë°ì´í„°ë¡œë” ì›Œì»¤ ìˆ˜"""
        if self.device_info.device == 'cpu':
            return min(4, self.system_info['cpu_count'])
        else:
            # GPU ì‚¬ìš©ì‹œ CPU ì½”ì–´ì˜ 25% ì •ë„
            return min(4, max(1, self.system_info['cpu_count'] // 4))

# ===============================================================
# ğŸ¯ ì „ì—­ GPU ì„¤ì • ê´€ë¦¬ì
# ===============================================================

class GPUConfigManager:
    """
    ğŸ¯ ì „ì—­ GPU ì„¤ì • ê´€ë¦¬ì
    - ì‹±ê¸€í†¤ íŒ¨í„´
    - ìºì‹± ì§€ì›
    - ë™ì  ì¬ì„¤ì •
    """

    _instance = None
    _config = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def get_config(
        self, 
        framework: str = 'pytorch',
        **kwargs
    ) -> OptimalGPUConfigBase:
        """GPU ì„¤ì • ë°˜í™˜ (ìºì‹œë¨)"""
        
        cache_key = f"{framework}_{hash(frozenset(kwargs.items()))}"
        
        if self._config is None or getattr(self._config, '_cache_key', None) != cache_key:
            if framework.lower() == 'pytorch':
                self._config = PyTorchGPUConfig(**kwargs)
                self._config._cache_key = cache_key
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë ˆì„ì›Œí¬: {framework}")
        
        return self._config

    def reset_config(self):
        """ì„¤ì • ì´ˆê¸°í™”"""
        self._config = None

    def get_device_summary(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ìš”ì•½ ì •ë³´"""
        if self._config is None:
            self._config = self.get_config()
        
        return {
            'current_device': self._config.get_device(),
            'device_info': self._config.get_device_info(),
            'memory_info': self._config.get_memory_info(),
            'system_info': self._config.system_info,
            'optimization_enabled': self._config.enable_optimization
        }

# ===============================================================
# ğŸ¯ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í¸ì˜ í•¨ìˆ˜ë“¤
# ===============================================================

# ì „ì—­ GPU ì„¤ì • ë§¤ë‹ˆì €
gpu_config_manager = GPUConfigManager()

@lru_cache()
def get_gpu_config(**kwargs) -> OptimalGPUConfigBase:
    """GPU ì„¤ì • ë°˜í™˜ (ìºì‹œë¨)"""
    return gpu_config_manager.get_config(**kwargs)

# ê¸°ë³¸ GPU ì„¤ì •
gpu_config = get_gpu_config()

# í¸ì˜ ìƒìˆ˜ë“¤ (í•˜ìœ„ í˜¸í™˜ì„±)
DEVICE = gpu_config.get_device()
DEVICE_INFO = gpu_config.get_device_info()
DEVICE_TYPE = DEVICE_INFO.device_type
USE_GPU = DEVICE != 'cpu'
IS_M3_MAX = gpu_config.system_info['is_m3_max']
MEMORY_GB = DEVICE_INFO.memory_gb

# ëª¨ë¸ ì„¤ì • (í•˜ìœ„ í˜¸í™˜ì„±)
MODEL_CONFIG = {
    'device': DEVICE,
    'batch_size': gpu_config.get_optimal_batch_size() if hasattr(gpu_config, 'get_optimal_batch_size') else 4,
    'num_workers': gpu_config.get_optimal_workers() if hasattr(gpu_config, 'get_optimal_workers') else 2,
    'mixed_precision': DEVICE_INFO.supports_mixed_precision,
    'memory_fraction': gpu_config.memory_fraction
}

# í¸ì˜ í•¨ìˆ˜ë“¤
def get_device() -> str:
    """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return DEVICE

def get_device_info() -> DeviceInfo:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
    return DEVICE_INFO

def get_optimal_settings() -> Dict[str, Any]:
    """ìµœì  ì„¤ì • ë°˜í™˜"""
    return MODEL_CONFIG

def optimize_memory():
    """ë©”ëª¨ë¦¬ ìµœì í™”"""
    gpu_config.optimize_memory()

def check_memory_available(required_gb: float = 4.0) -> bool:
    """ë©”ëª¨ë¦¬ ê°€ìš©ì„± ì²´í¬"""
    return gpu_config.check_memory_available(required_gb)

def get_memory_info() -> Dict[str, float]:
    """ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
    return gpu_config.get_memory_info()

def create_custom_gpu_config(**kwargs) -> OptimalGPUConfigBase:
    """ì»¤ìŠ¤í…€ GPU ì„¤ì • ìƒì„±"""
    return PyTorchGPUConfig(**kwargs)

# M3 Max ì „ìš© ì„¤ì • ìƒì„± í•¨ìˆ˜
def create_m3_max_config(**kwargs) -> OptimalGPUConfigBase:
    """M3 Max ì „ìš© ìµœì í™” ì„¤ì •"""
    return PyTorchGPUConfig(
        preferred_device='mps',
        memory_fraction=0.85,
        enable_optimization=True,
        mixed_precision=True,
        **kwargs
    )

# ê°œë°œìš© ì„¤ì • ìƒì„± í•¨ìˆ˜
def create_development_config() -> OptimalGPUConfigBase:
    """ê°œë°œìš© GPU ì„¤ì •"""
    return PyTorchGPUConfig(
        memory_fraction=0.6,
        enable_optimization=False,
        enable_profiling=True
    )

# í”„ë¡œë•ì…˜ìš© ì„¤ì • ìƒì„± í•¨ìˆ˜
def create_production_config() -> OptimalGPUConfigBase:
    """í”„ë¡œë•ì…˜ìš© GPU ì„¤ì •"""
    return PyTorchGPUConfig(
        memory_fraction=0.9,
        enable_optimization=True,
        mixed_precision=True,
        memory_growth=True
    )

# ì´ˆê¸°í™” ë¡œê¹…
logger.info(f"ğŸ¯ GPU ì„¤ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
logger.info(f"ğŸ’» ë””ë°”ì´ìŠ¤: {DEVICE} ({DEVICE_INFO.name})")
logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬: {MEMORY_GB:.1f}GB")

if IS_M3_MAX:
    logger.info("ğŸ M3 Max ìµœì í™” í™œì„±í™”")
if USE_GPU:
    logger.info(f"ğŸ® GPU ê°€ì† í™œì„±í™”")

# ë©”ëª¨ë¦¬ ì²´í¬
if not check_memory_available(4.0):
    logger.warning("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥")

__all__ = [
    'OptimalGPUConfigBase',
    'PyTorchGPUConfig', 
    'GPUConfigManager',
    'DeviceInfo',
    'get_gpu_config',
    'gpu_config',
    'gpu_config_manager',
    # í¸ì˜ ìƒìˆ˜ë“¤
    'DEVICE',
    'DEVICE_INFO', 
    'DEVICE_TYPE',
    'USE_GPU',
    'IS_M3_MAX',
    'MEMORY_GB',
    'MODEL_CONFIG',
    # í¸ì˜ í•¨ìˆ˜ë“¤
    'get_device',
    'get_device_info',
    'get_optimal_settings',
    'optimize_memory',
    'check_memory_available',
    'get_memory_info',
    'create_custom_gpu_config',
    'create_m3_max_config',
    'create_development_config',
    'create_production_config'
]