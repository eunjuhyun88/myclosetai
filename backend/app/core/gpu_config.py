"""
GPU ë° ë””ë°”ì´ìŠ¤ ì„¤ì •
"""
import torch

# ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
if torch.cuda.is_available():
    DEVICE = "cuda"
    print(f"ğŸš€ CUDA GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    DEVICE = "mps" 
    print("ğŸ Apple Silicon MPS ì‚¬ìš©")
else:
    DEVICE = "cpu"
    print("ğŸ’» CPU ì‚¬ìš©")

# ëª¨ë¸ ì„¤ì •
MODEL_CONFIG = {
    "device": DEVICE,
    "dtype": torch.float32 if DEVICE == "mps" else torch.float16,
    "memory_fraction": 0.8,
    "enable_attention_slicing": True,
    "enable_memory_efficient_attention": DEVICE != "mps"
}

# GPU ë©”ëª¨ë¦¬ ìµœì í™”
if DEVICE == "cuda":
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True
elif DEVICE == "mps":
    # MPS ìµœì í™” ì„¤ì •
    torch.backends.mps.empty_cache()
