# app/core/gpu_config.py
"""
MyCloset AI - M3 Max 128GB ìµœì í™” GPU ì„¤ì •
Pydantic V2 í˜¸í™˜, ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì¶”ê°€
"""

import os
import logging
import torch
import platform
from typing import Dict, Any, Optional
from dataclasses import dataclass

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class GPUConfig:
    """GPU ì„¤ì • í´ë˜ìŠ¤"""
    device: str
    device_type: str
    memory_gb: float
    is_m3_max: bool
    optimization_enabled: bool
    
class M3MaxGPUManager:
    """M3 Max 128GB ì „ìš© GPU ê´€ë¦¬ì"""
    
    def __init__(self):
        self.device = None
        self.device_info = {}
        self.model_config = {}
        self.is_initialized = False
        self.m3_max_detected = False
        
        # ì´ˆê¸°í™”
        self._initialize()
    
    def _initialize(self):
        """GPU ì„¤ì • ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”§ GPU ì„¤ì • ì´ˆê¸°í™” ì‹œì‘...")
            
            # M3 Max ê°ì§€
            self._detect_m3_max()
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            self._setup_device()
            
            # ëª¨ë¸ ì„¤ì •
            self._setup_model_config()
            
            # ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘
            self._collect_device_info()
            
            # ìµœì í™” ì ìš©
            self._apply_optimizations()
            
            self.is_initialized = True
            logger.info(f"ğŸš€ GPU ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ: {self.device}")
            
        except Exception as e:
            logger.error(f"âŒ GPU ì„¤ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._fallback_cpu_setup()
    
    def _detect_m3_max(self):
        """M3 Max ê°ì§€"""
        try:
            import psutil
            
            # Apple Silicon + ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ í™•ì¸
            if platform.machine() == 'arm64' and platform.system() == 'Darwin':
                memory_gb = psutil.virtual_memory().total / (1024**3)
                
                if memory_gb >= 120:  # 128GB ê·¼ì‚¬ì¹˜
                    self.m3_max_detected = True
                    logger.info("ğŸ M3 Max 128GB í™˜ê²½ ê°ì§€!")
                else:
                    logger.info(f"ğŸ Apple Silicon ê°ì§€ - ë©”ëª¨ë¦¬: {memory_gb:.0f}GB")
            
        except Exception as e:
            logger.warning(f"âš ï¸ M3 Max ê°ì§€ ì‹¤íŒ¨: {e}")
    
    def _setup_device(self):
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        try:
            if torch.backends.mps.is_available():
                self.device = "mps"
                logger.info("ğŸ Apple Silicon MPS ê°ì§€")
                
                if self.m3_max_detected:
                    logger.info("ğŸ M3 Max íŠ¹í™” ìµœì í™” ì ìš©")
                    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'  # M3 Max ë©”ëª¨ë¦¬ ìµœì í™”
            
            elif torch.cuda.is_available():
                self.device = "cuda"
                logger.info("ğŸš€ CUDA GPU ê°ì§€")
            
            else:
                self.device = "cpu"
                logger.info("ğŸ’» CPU ëª¨ë“œë¡œ ì„¤ì •")
        
        except Exception as e:
            logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.device = "cpu"
    
    def _setup_model_config(self):
        """ëª¨ë¸ ì„¤ì • êµ¬ì„±"""
        base_config = {
            "device": self.device,
            "dtype": "float16" if self.device in ["mps", "cuda"] else "float32",
            "batch_size": 1,
            "memory_fraction": 0.8
        }
        
        if self.m3_max_detected and self.device == "mps":
            # M3 Max íŠ¹í™” ì„¤ì •
            base_config.update({
                "batch_size": 4,  # M3 MaxëŠ” ë” í° ë°°ì¹˜ ê°€ëŠ¥
                "memory_fraction": 0.6,  # 128GB ì¤‘ ì¼ë¶€ë§Œ ì‚¬ìš©
                "use_neural_engine": True,
                "metal_performance_shaders": True,
                "unified_memory_optimization": True,
                "high_resolution_processing": True
            })
            logger.info("ğŸ M3 Max íŠ¹í™” ìµœì í™” ì ìš©")
        
        elif self.device == "mps":
            # ì¼ë°˜ Apple Silicon ì„¤ì •
            base_config.update({
                "batch_size": 2,
                "memory_fraction": 0.7,
                "use_neural_engine": False
            })
        
        elif self.device == "cuda":
            # CUDA ì„¤ì •
            base_config.update({
                "batch_size": 2,
                "memory_fraction": 0.8,
                "mixed_precision": True
            })
        
        self.model_config = base_config
        logger.info(f"âš™ï¸ ëª¨ë¸ ì„¤ì • ì™„ë£Œ: ë°°ì¹˜í¬ê¸°={base_config['batch_size']}")
    
    def _collect_device_info(self):
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        try:
            import psutil
            
            self.device_info = {
                "device": self.device,
                "platform": platform.system(),
                "architecture": platform.machine(),
                "pytorch_version": torch.__version__,
                "python_version": platform.python_version()
            }
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            memory = psutil.virtual_memory()
            self.device_info.update({
                "total_memory_gb": memory.total / (1024**3),
                "available_memory_gb": memory.available / (1024**3),
                "memory_usage_percent": memory.percent
            })
            
            # ë””ë°”ì´ìŠ¤ë³„ ì •ë³´
            if self.device == "mps":
                self.device_info.update({
                    "name": "Apple Silicon GPU (MPS)",
                    "mps_available": True,
                    "is_m3_max": self.m3_max_detected,
                    "neural_engine_available": self.m3_max_detected,
                    "metal_performance_shaders": True
                })
                
                if self.m3_max_detected:
                    self.device_info.update({
                        "gpu_cores": "30-40 cores",
                        "memory_bandwidth": "400GB/s",
                        "neural_engine_tops": "15.8 TOPS"
                    })
            
            elif self.device == "cuda":
                if torch.cuda.is_available():
                    self.device_info.update({
                        "name": torch.cuda.get_device_name(0),
                        "cuda_version": torch.version.cuda,
                        "gpu_memory_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3),
                        "compute_capability": torch.cuda.get_device_capability(0)
                    })
            
            else:  # CPU
                self.device_info.update({
                    "name": "CPU",
                    "cpu_cores": psutil.cpu_count(),
                    "cpu_cores_physical": psutil.cpu_count(logical=False)
                })
            
            logger.info(f"â„¹ï¸ ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {self.device_info.get('name', 'Unknown')}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.device_info = {"device": self.device, "name": "Unknown", "error": str(e)}
    
    def _apply_optimizations(self):
        """ìµœì í™” ì ìš©"""
        try:
            if self.device == "mps":
                # MPS ìµœì í™”
                if hasattr(torch.backends.mps, 'empty_cache'):
                    # ìƒˆë¡œìš´ PyTorch ë²„ì „ì—ì„œ ì§€ì›
                    logger.info("â„¹ï¸ MPS empty_cache ì§€ì›ë¨")
                else:
                    logger.info("â„¹ï¸ MPS empty_cache ë¯¸ì§€ì› - ëŒ€ì²´ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‚¬ìš©")
                
                # M3 Max íŠ¹í™” ìµœì í™”
                if self.m3_max_detected:
                    # Metal Performance Shaders ìµœì í™”
                    os.environ['METAL_DEVICE_WRAPPER_TYPE'] = '1'
                    os.environ['METAL_PERFORMANCE_SHADERS_ENABLED'] = '1'
                    logger.info("ğŸ MPS ìµœì í™” ì™„ë£Œ")
                
                logger.info("âœ… MPS ìµœì í™” ì ìš© ì™„ë£Œ")
            
            elif self.device == "cuda":
                # CUDA ìµœì í™”
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                logger.info("âœ… CUDA ìµœì í™” ì ìš© ì™„ë£Œ")
            
            # ê³µí†µ ìµœì í™”
            torch.set_num_threads(min(8, os.cpu_count() or 4))
            
        except Exception as e:
            logger.warning(f"âš ï¸ ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
    
    def _fallback_cpu_setup(self):
        """CPU í´ë°± ì„¤ì •"""
        self.device = "cpu"
        self.model_config = {
            "device": "cpu",
            "dtype": "float32",
            "batch_size": 1,
            "memory_fraction": 0.5
        }
        self.device_info = {
            "device": "cpu",
            "name": "CPU (Fallback)",
            "error": "GPU initialization failed"
        }
        logger.warning("ğŸš¨ CPU í´ë°± ëª¨ë“œë¡œ ì„¤ì •ë¨")
    
    def get_device(self) -> str:
        """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self.device
    
    def get_device_config(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
        return {
            "device": self.device,
            "name": self.device_info.get("name", "Unknown"),
            "is_m3_max": self.m3_max_detected,
            "optimization_enabled": self.is_initialized
        }
    
    def get_model_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
        return self.model_config.copy()
    
    def get_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return self.device_info.copy()
    
    def test_device(self) -> bool:
        """ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
        try:
            # ê°„ë‹¨í•œ í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸
            device = torch.device(self.device)
            test_tensor = torch.randn(10, 10, device=device)
            result = torch.matmul(test_tensor, test_tensor.T)
            
            if self.device == "mps":
                # MPSì—ì„œ CPUë¡œ ì´ë™ í…ŒìŠ¤íŠ¸
                cpu_result = result.cpu()
                logger.info(f"âœ… {self.device} ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            else:
                logger.info(f"âœ… {self.device} ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ {self.device} ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.device == "mps":
                # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    elif hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                except:
                    pass
            
            elif self.device == "cuda":
                torch.cuda.empty_cache()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            import gc
            gc.collect()
            
            logger.info("âœ… GPU ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ GPU ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ============================================
# M3 Max ìµœì í™”ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤
# ============================================

def optimize_memory(device: Optional[str] = None, aggressive: bool = False) -> Dict[str, Any]:
    """M3 Max ìµœì í™”ëœ ë©”ëª¨ë¦¬ ìµœì í™” í•¨ìˆ˜"""
    try:
        import gc
        import psutil
        
        current_device = device or gpu_config.device
        start_memory = psutil.virtual_memory().percent
        
        # ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        result = {
            "success": True,
            "device": current_device,
            "start_memory_percent": start_memory,
            "method": "standard_gc"
        }
        
        if current_device == "mps":
            # M3 Max MPS ë©”ëª¨ë¦¬ ìµœì í™”
            try:
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                    result["method"] = "mps_empty_cache"
                elif hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()
                    result["method"] = "mps_synchronize"
                
                if aggressive and gpu_config.m3_max_detected:
                    # M3 Max íŠ¹í™” ì ê·¹ì  ì •ë¦¬
                    torch.mps.synchronize()
                    gc.collect()
                    result["method"] = "m3_max_aggressive_cleanup"
                    result["aggressive"] = True
                
            except Exception as mps_error:
                logger.warning(f"MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
                result["mps_error"] = str(mps_error)
        
        elif current_device == "cuda":
            # CUDA ë©”ëª¨ë¦¬ ìµœì í™”
            try:
                torch.cuda.empty_cache()
                if aggressive:
                    torch.cuda.synchronize()
                result["method"] = "cuda_empty_cache"
                if aggressive:
                    result["aggressive"] = True
            except Exception as cuda_error:
                logger.warning(f"CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {cuda_error}")
                result["cuda_error"] = str(cuda_error)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ìƒíƒœ í™•ì¸
        end_memory = psutil.virtual_memory().percent
        memory_freed = start_memory - end_memory
        
        result.update({
            "end_memory_percent": end_memory,
            "memory_freed_percent": memory_freed,
            "timestamp": torch.get_default_dtype(),  # ê°„ì ‘ì ì¸ ì‹œê°„ í‘œì‹œ
            "m3_max_optimized": gpu_config.m3_max_detected if 'gpu_config' in globals() else False
        })
        
        if memory_freed > 0:
            logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ {memory_freed:.1f}% ì •ë¦¬ë¨ ({result['method']})")
        
        return result
        
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "device": device or "unknown",
            "method": "failed"
        }

def get_memory_status() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    try:
        import psutil
        
        memory = psutil.virtual_memory()
        
        status = {
            "total_gb": memory.total / (1024**3),
            "available_gb": memory.available / (1024**3),
            "used_gb": memory.used / (1024**3),
            "usage_percent": memory.percent,
            "status": "good"
        }
        
        # M3 Max íŠ¹í™” ìƒíƒœ íŒì •
        if hasattr(gpu_config, 'm3_max_detected') and gpu_config.m3_max_detected:
            if memory.percent < 40:
                status["status"] = "excellent"
            elif memory.percent < 70:
                status["status"] = "good"
            elif memory.percent < 85:
                status["status"] = "moderate"
            else:
                status["status"] = "high"
        else:
            # ì¼ë°˜ í™˜ê²½
            if memory.percent < 70:
                status["status"] = "good"
            elif memory.percent < 85:
                status["status"] = "moderate"
            else:
                status["status"] = "high"
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€
        if hasattr(gpu_config, 'device') and gpu_config.device == "cuda":
            try:
                status.update({
                    "gpu_memory_allocated_gb": torch.cuda.memory_allocated() / (1024**3),
                    "gpu_memory_reserved_gb": torch.cuda.memory_reserved() / (1024**3)
                })
            except:
                pass
        
        return status
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

def check_device_compatibility() -> Dict[str, bool]:
    """ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í™•ì¸"""
    return {
        "mps_available": torch.backends.mps.is_available(),
        "cuda_available": torch.cuda.is_available(),
        "m3_max_detected": getattr(gpu_config, 'm3_max_detected', False) if 'gpu_config' in globals() else False,
        "neural_engine_available": (
            torch.backends.mps.is_available() and 
            getattr(gpu_config, 'm3_max_detected', False) if 'gpu_config' in globals() else False
        )
    }

# ============================================
# ì „ì—­ GPU ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# ============================================

# ì „ì—­ GPU ì„¤ì • ë§¤ë‹ˆì € ìƒì„±
gpu_config = M3MaxGPUManager()

# í¸ì˜ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ë“¤
DEVICE = gpu_config.device
MODEL_CONFIG = gpu_config.model_config
DEVICE_INFO = gpu_config.device_info

# ============================================
# ì£¼ìš” í•¨ìˆ˜ë“¤ (main.py í˜¸í™˜ìš©)
# ============================================

def get_device() -> str:
    """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return gpu_config.get_device()

def get_device_config() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_device_config()

def get_model_config() -> Dict[str, Any]:
    """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_model_config()

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
    return gpu_config.get_device_info()

def test_device() -> bool:
    """ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
    return gpu_config.test_device()

def cleanup_gpu_resources():
    """GPU ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    gpu_config.cleanup()

# ============================================
# ì´ˆê¸°í™” ë° ê²€ì¦
# ============================================

# ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if gpu_config.is_initialized:
    test_success = gpu_config.test_device()
    if test_success:
        logger.info("âœ… GPU ì„¤ì • ê²€ì¦ ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ GPU ì„¤ì • ê²€ì¦ ì‹¤íŒ¨ - CPU í´ë°± ê¶Œì¥")

# M3 Max ìƒíƒœ ë¡œê¹…
if gpu_config.m3_max_detected:
    logger.info("ğŸ M3 Max 128GB ìµœì í™” í™œì„±í™”:")
    logger.info(f"  - Neural Engine: {'âœ…' if MODEL_CONFIG.get('use_neural_engine') else 'âŒ'}")
    logger.info(f"  - Metal Performance Shaders: {'âœ…' if MODEL_CONFIG.get('metal_performance_shaders') else 'âŒ'}")
    logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {MODEL_CONFIG.get('batch_size', 1)}")
    logger.info(f"  - ë©”ëª¨ë¦¬ ëŒ€ì—­í­: {DEVICE_INFO.get('memory_bandwidth', 'N/A')}")

# ============================================
# Export ë¦¬ìŠ¤íŠ¸ (main.py import í˜¸í™˜)
# ============================================

__all__ = [
    # ì£¼ìš” ê°ì²´ë“¤
    'gpu_config', 'DEVICE', 'MODEL_CONFIG', 'DEVICE_INFO',
    
    # í•¨ìˆ˜ë“¤
    'get_device', 'get_device_config', 'get_model_config', 'get_device_info',
    'test_device', 'cleanup_gpu_resources',
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤ (main.pyì—ì„œ ìš”êµ¬)
    'optimize_memory', 'get_memory_status', 'check_device_compatibility',
    
    # í´ë˜ìŠ¤
    'M3MaxGPUManager', 'GPUConfig'
]