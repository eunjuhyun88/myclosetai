"""
MyCloset AI - í†µí•© ìµœì í™” GPU ì„¤ì • (M3 Max 128GB ì™„ì „ ìµœì í™”)
ğŸ”¥ í•µì‹¬ ê°œì„ ì :
- PyTorch 2.5.1 MPS ì™„ì „ í˜¸í™˜ì„±
- M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
- 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”
- ê¸°ì¡´ ì½”ë“œ 100% í˜¸í™˜ì„± ë³´ì¥
- ì¤‘ë³µ ì½”ë“œ ì œê±° ë° ì„±ëŠ¥ í–¥ìƒ
"""

import os
import platform
import logging
import psutil
import gc
import torch
import time
import json
import subprocess
from typing import Dict, Any, Optional, Union, List, Tuple
from dataclasses import dataclass, asdict
from functools import lru_cache
from pathlib import Path
import warnings

# ì„ íƒì  import ì²˜ë¦¬
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

logger = logging.getLogger(__name__)

# =============================================================================
# ğŸ”§ PyTorch ë²„ì „ í˜¸í™˜ì„± ë° ê¸°ëŠ¥ ê°ì§€
# =============================================================================

class PyTorchCompatibilityManager:
    """PyTorch 2.5.1 MPS í˜¸í™˜ì„± ê´€ë¦¬ì"""
    
    def __init__(self):
        self.pytorch_version = torch.__version__
        self.version_tuple = self._parse_version(self.pytorch_version)
        self.mps_capabilities = self._detect_mps_capabilities()
        self.cuda_capabilities = self._detect_cuda_capabilities()
        
        logger.info(f"ğŸ”§ PyTorch ë²„ì „: {self.pytorch_version}")
        logger.info(f"ğŸ MPS ê¸°ëŠ¥: {list(self.mps_capabilities.keys())}")
    
    def _parse_version(self, version_str: str) -> Tuple[int, int, int]:
        """PyTorch ë²„ì „ íŒŒì‹±"""
        try:
            parts = version_str.split('.')
            major = int(parts[0])
            minor = int(parts[1])
            patch = int(parts[2].split('+')[0])
            return (major, minor, patch)
        except:
            return (2, 5, 1)
    
    def _detect_mps_capabilities(self) -> Dict[str, bool]:
        """MPS ê¸°ëŠ¥ ê°ì§€"""
        capabilities = {}
        
        try:
            # ê¸°ë³¸ MPS ì§€ì›
            capabilities['is_available'] = hasattr(torch.backends.mps, 'is_available') and torch.backends.mps.is_available()
            capabilities['is_built'] = hasattr(torch.backends.mps, 'is_built')
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤ (PyTorch 2.5.1 í˜¸í™˜ì„±)
            capabilities['empty_cache'] = hasattr(torch.backends.mps, 'empty_cache')
            capabilities['synchronize'] = hasattr(torch.mps, 'synchronize')
            capabilities['current_allocated_memory'] = hasattr(torch.mps, 'current_allocated_memory')
            capabilities['set_per_process_memory_fraction'] = hasattr(torch.backends.mps, 'set_per_process_memory_fraction')
            
            # ê³ ê¸‰ ê¸°ëŠ¥ë“¤
            capabilities['profiler_start'] = hasattr(torch.backends.mps, 'profiler_start')
            capabilities['get_rng_state'] = hasattr(torch.mps, 'get_rng_state')
            
        except Exception as e:
            logger.warning(f"MPS ê¸°ëŠ¥ ê°ì§€ ì‹¤íŒ¨: {e}")
            capabilities = {'is_available': False}
        
        return capabilities
    
    def _detect_cuda_capabilities(self) -> Dict[str, bool]:
        """CUDA ê¸°ëŠ¥ ê°ì§€"""
        capabilities = {}
        
        try:
            capabilities['is_available'] = torch.cuda.is_available()
            capabilities['empty_cache'] = hasattr(torch.cuda, 'empty_cache')
            capabilities['synchronize'] = hasattr(torch.cuda, 'synchronize')
            capabilities['memory_allocated'] = hasattr(torch.cuda, 'memory_allocated')
            capabilities['memory_reserved'] = hasattr(torch.cuda, 'memory_reserved')
            capabilities['get_device_properties'] = hasattr(torch.cuda, 'get_device_properties')
            
        except Exception as e:
            logger.warning(f"CUDA ê¸°ëŠ¥ ê°ì§€ ì‹¤íŒ¨: {e}")
            capabilities = {'is_available': False}
        
        return capabilities
    
    def safe_mps_memory_cleanup(self) -> Dict[str, Any]:
        """ì•ˆì „í•œ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (PyTorch 2.5.1 í˜¸í™˜ì„±)"""
        result = {
            "success": False,
            "method": "none",
            "torch_version": self.pytorch_version,
            "mps_available": self.mps_capabilities.get('is_available', False)
        }
        
        if not self.mps_capabilities.get('is_available', False):
            result["error"] = "MPS not available"
            return result
        
        try:
            # PyTorch 2.5.1+ í˜¸í™˜ì„± ìˆœì°¨ ì‹œë„
            if self.mps_capabilities.get('empty_cache', False):
                torch.backends.mps.empty_cache()
                result.update({"success": True, "method": "mps_empty_cache"})
            elif self.mps_capabilities.get('synchronize', False):
                torch.mps.synchronize()
                result.update({"success": True, "method": "mps_synchronize"})
            else:
                gc.collect()
                result.update({"success": True, "method": "gc_fallback"})
            
            return result
            
        except Exception as e:
            result.update({
                "success": False,
                "error": str(e),
                "method": "failed"
            })
            return result
    
    def safe_cuda_memory_cleanup(self) -> Dict[str, Any]:
        """ì•ˆì „í•œ CUDA ë©”ëª¨ë¦¬ ì •ë¦¬"""
        result = {
            "success": False,
            "method": "none",
            "cuda_available": self.cuda_capabilities.get('is_available', False)
        }
        
        if not self.cuda_capabilities.get('is_available', False):
            result["error"] = "CUDA not available"
            return result
        
        try:
            if self.cuda_capabilities.get('empty_cache', False):
                torch.cuda.empty_cache()
                result.update({"success": True, "method": "cuda_empty_cache"})
            else:
                gc.collect()
                result.update({"success": True, "method": "gc_fallback"})
            
            return result
            
        except Exception as e:
            result.update({
                "success": False,
                "error": str(e),
                "method": "failed"
            })
            return result
    
    def get_memory_info(self, device: str) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        memory_info = {"device": device, "available": False}
        
        try:
            if device == "mps" and self.mps_capabilities.get('is_available', False):
                if torch.backends.mps.is_available():
                    memory_info["available"] = True
                    memory_info["backend"] = "MPS"
                    
                    # í˜„ì¬ í• ë‹¹ëœ ë©”ëª¨ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
                    if self.mps_capabilities.get('current_allocated_memory', False):
                        try:
                            allocated = torch.mps.current_allocated_memory()
                            memory_info["allocated_bytes"] = allocated
                            memory_info["allocated_gb"] = allocated / (1024**3)
                        except:
                            memory_info["allocated_info"] = "unavailable"
                    
                    # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ (MPSëŠ” í†µí•© ë©”ëª¨ë¦¬ ì‚¬ìš©)
                    if PSUTIL_AVAILABLE:
                        vm = psutil.virtual_memory()
                        memory_info["system_total_gb"] = vm.total / (1024**3)
                        memory_info["system_available_gb"] = vm.available / (1024**3)
                        memory_info["system_used_percent"] = vm.percent
            
            elif device == "cuda" and self.cuda_capabilities.get('is_available', False):
                if torch.cuda.is_available():
                    memory_info["available"] = True
                    memory_info["backend"] = "CUDA"
                    
                    # GPU ë©”ëª¨ë¦¬ ì •ë³´
                    if self.cuda_capabilities.get('memory_allocated', False):
                        memory_info["allocated_bytes"] = torch.cuda.memory_allocated()
                        memory_info["allocated_gb"] = torch.cuda.memory_allocated() / (1024**3)
                    
                    if self.cuda_capabilities.get('memory_reserved', False):
                        memory_info["reserved_bytes"] = torch.cuda.memory_reserved()
                        memory_info["reserved_gb"] = torch.cuda.memory_reserved() / (1024**3)
                    
                    if self.cuda_capabilities.get('get_device_properties', False):
                        props = torch.cuda.get_device_properties(0)
                        memory_info["total_gb"] = props.total_memory / (1024**3)
                        memory_info["device_name"] = props.name
            
            else:  # CPU
                memory_info["available"] = True
                memory_info["backend"] = "CPU"
                
                if PSUTIL_AVAILABLE:
                    vm = psutil.virtual_memory()
                    memory_info["total_gb"] = vm.total / (1024**3)
                    memory_info["available_gb"] = vm.available / (1024**3)
                    memory_info["used_percent"] = vm.percent
        
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ ({device}): {e}")
            memory_info["error"] = str(e)
        
        return memory_info

# =============================================================================
# ğŸ M3 Max í•˜ë“œì›¨ì–´ ê°ì§€ ë° ìµœì í™”
# =============================================================================

@dataclass
class HardwareSpecs:
    """í•˜ë“œì›¨ì–´ ì‚¬ì–‘ ì •ë³´"""
    system: str
    machine: str
    processor: str
    cpu_cores: int
    cpu_cores_physical: int
    memory_gb: float
    is_apple_silicon: bool
    is_m3_max: bool
    device_name: str
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return asdict(self)

class M3MaxDetector:
    """M3 Max ì •ë°€ ê°ì§€ ë° ìµœì í™” ì„¤ì •"""
    
    def __init__(self):
        self.hardware_specs = self._detect_hardware()
        self.optimal_device = self._select_optimal_device()
        self.optimization_settings = self._calculate_optimization_settings()
        
        logger.info(f"ğŸ” í•˜ë“œì›¨ì–´ ê°ì§€ ì™„ë£Œ: {self.hardware_specs.device_name}")
        if self.hardware_specs.is_m3_max:
            logger.info(f"ğŸ M3 Max ìµœì í™” í™œì„±í™”: {self.hardware_specs.memory_gb}GB")
    
    def _detect_hardware(self) -> HardwareSpecs:
        """í•˜ë“œì›¨ì–´ ìƒì„¸ ê°ì§€"""
        try:
            # ê¸°ë³¸ í”Œë«í¼ ì •ë³´
            system = platform.system()
            machine = platform.machine()
            processor = platform.processor()
            
            # CPU ì½”ì–´ ìˆ˜ ì •í™•íˆ ê°ì§€
            if PSUTIL_AVAILABLE:
                cpu_cores = psutil.cpu_count(logical=True) or 8
                cpu_cores_physical = psutil.cpu_count(logical=False) or 4
            else:
                cpu_cores = os.cpu_count() or 8
                cpu_cores_physical = cpu_cores // 2
            
            # ë©”ëª¨ë¦¬ ìš©ëŸ‰ ì •í™•íˆ ê°ì§€
            if PSUTIL_AVAILABLE:
                memory_gb = round(psutil.virtual_memory().total / (1024**3), 1)
            else:
                memory_gb = 16.0
            
            # Apple Silicon ê°ì§€
            is_apple_silicon = (system == "Darwin" and machine == "arm64")
            
            # M3 Max ì •ë°€ ê°ì§€
            is_m3_max = self._precision_detect_m3_max(is_apple_silicon, memory_gb, cpu_cores)
            
            # ë””ë°”ì´ìŠ¤ ì´ë¦„ ìƒì„±
            device_name = self._generate_device_name(is_apple_silicon, is_m3_max, memory_gb)
            
            return HardwareSpecs(
                system=system,
                machine=machine,
                processor=processor,
                cpu_cores=cpu_cores,
                cpu_cores_physical=cpu_cores_physical,
                memory_gb=memory_gb,
                is_apple_silicon=is_apple_silicon,
                is_m3_max=is_m3_max,
                device_name=device_name
            )
            
        except Exception as e:
            logger.error(f"í•˜ë“œì›¨ì–´ ê°ì§€ ì‹¤íŒ¨: {e}")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
            return HardwareSpecs(
                system="Unknown",
                machine="Unknown",
                processor="Unknown",
                cpu_cores=4,
                cpu_cores_physical=2,
                memory_gb=8.0,
                is_apple_silicon=False,
                is_m3_max=False,
                device_name="Unknown Device"
            )
    
    def _precision_detect_m3_max(self, is_apple_silicon: bool, memory_gb: float, cpu_cores: int) -> bool:
        """M3 Max ì •ë°€ ê°ì§€"""
        if not is_apple_silicon:
            return False
        
        # ë©”ëª¨ë¦¬ ê¸°ë°˜ ì •ë°€ íŒì •
        if memory_gb >= 120:  # 128GB M3 Max
            logger.info("ğŸ M3 Max 128GB ê°ì§€ë¨")
            return True
        elif memory_gb >= 90:  # 96GB M3 Max
            logger.info("ğŸ M3 Max 96GB ê°ì§€ë¨")
            return True
        elif cpu_cores >= 12:  # M3 MaxëŠ” 12ì½”ì–´ ì´ìƒ
            logger.info("ğŸ M3 Max (CPU ì½”ì–´ ê¸°ë°˜) ê°ì§€ë¨")
            return True
        
        # ì‹œìŠ¤í…œ í”„ë¡œíŒŒì¼ëŸ¬ë¥¼ í†µí•œ ì¶”ê°€ ê°ì§€
        try:
            if platform.system() == "Darwin":
                result = subprocess.run(
                    ['system_profiler', 'SPHardwareDataType'],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.returncode == 0:
                    output = result.stdout.lower()
                    if 'm3 max' in output:
                        logger.info("ğŸ M3 Max (ì‹œìŠ¤í…œ í”„ë¡œíŒŒì¼ëŸ¬) ê°ì§€ë¨")
                        return True
        except:
            pass
        
        return False
    
    def _generate_device_name(self, is_apple_silicon: bool, is_m3_max: bool, memory_gb: float) -> str:
        """ë””ë°”ì´ìŠ¤ ì´ë¦„ ìƒì„±"""
        if is_m3_max:
            if memory_gb >= 120:
                return "Apple M3 Max (128GB)"
            elif memory_gb >= 90:
                return "Apple M3 Max (96GB)"
            else:
                return "Apple M3 Max"
        elif is_apple_silicon:
            return "Apple Silicon"
        else:
            return "Generic Device"
    
    def _select_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        try:
            # MPS ìš°ì„  (Apple Silicon)
            if self.hardware_specs.is_apple_silicon:
                if hasattr(torch.backends.mps, 'is_available') and torch.backends.mps.is_available():
                    return "mps"
            
            # CUDA ì§€ì› í™•ì¸
            if torch.cuda.is_available():
                return "cuda"
            
            # CPU í´ë°±
            return "cpu"
            
        except Exception as e:
            logger.warning(f"ë””ë°”ì´ìŠ¤ ì„ íƒ ì‹¤íŒ¨: {e}")
            return "cpu"
    
    def _calculate_optimization_settings(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ê³„ì‚°"""
        if self.hardware_specs.is_m3_max:
            # M3 Max 128GB ì „ìš© ìµœì í™”
            return {
                "batch_size": 8 if self.hardware_specs.memory_gb >= 120 else 6,
                "max_workers": min(16, self.hardware_specs.cpu_cores),
                "concurrent_sessions": 12 if self.hardware_specs.memory_gb >= 120 else 8,
                "memory_pool_gb": min(64, self.hardware_specs.memory_gb // 2),
                "cache_size_gb": min(32, self.hardware_specs.memory_gb // 4),
                "intermediate_cache_gb": min(16, self.hardware_specs.memory_gb // 8),
                "quality_level": "ultra",
                "enable_neural_engine": True,
                "enable_mps": True,
                "optimization_level": "maximum",
                "fp16_enabled": True,
                "compilation_enabled": False,
                "memory_fraction": 0.85,
                "high_resolution_processing": True,
                "unified_memory_optimization": True,
                "metal_performance_shaders": True,
                "pipeline_parallelism": True,
                "step_caching": True,
                "model_preloading": True
            }
        elif self.hardware_specs.is_apple_silicon:
            # ì¼ë°˜ Apple Silicon ìµœì í™”
            return {
                "batch_size": 4,
                "max_workers": min(8, self.hardware_specs.cpu_cores),
                "concurrent_sessions": 6,
                "memory_pool_gb": min(16, self.hardware_specs.memory_gb // 2),
                "cache_size_gb": min(8, self.hardware_specs.memory_gb // 4),
                "intermediate_cache_gb": min(4, self.hardware_specs.memory_gb // 8),
                "quality_level": "high",
                "enable_neural_engine": False,
                "enable_mps": True,
                "optimization_level": "balanced",
                "fp16_enabled": True,
                "compilation_enabled": False,
                "memory_fraction": 0.7,
                "high_resolution_processing": False,
                "unified_memory_optimization": True,
                "metal_performance_shaders": True,
                "pipeline_parallelism": False,
                "step_caching": True,
                "model_preloading": False
            }
        else:
            # ì¼ë°˜ ì‹œìŠ¤í…œ ìµœì í™”
            return {
                "batch_size": 2,
                "max_workers": min(4, self.hardware_specs.cpu_cores),
                "concurrent_sessions": 4,
                "memory_pool_gb": min(8, self.hardware_specs.memory_gb // 2),
                "cache_size_gb": min(4, self.hardware_specs.memory_gb // 4),
                "intermediate_cache_gb": min(2, self.hardware_specs.memory_gb // 8),
                "quality_level": "balanced",
                "enable_neural_engine": False,
                "enable_mps": False,
                "optimization_level": "safe",
                "fp16_enabled": False,
                "compilation_enabled": True,
                "memory_fraction": 0.6,
                "high_resolution_processing": False,
                "unified_memory_optimization": False,
                "metal_performance_shaders": False,
                "pipeline_parallelism": False,
                "step_caching": False,
                "model_preloading": False
            }

# =============================================================================
# ğŸ¯ í†µí•© GPU ê´€ë¦¬ì (ë©”ì¸ í´ë˜ìŠ¤)
# =============================================================================

class UnifiedGPUManager:
    """í†µí•© GPU ê´€ë¦¬ì - ê¸°ì¡´ í˜¸í™˜ì„± 100% ë³´ì¥"""
    
    def __init__(self):
        """í†µí•© GPU ê´€ë¦¬ì ì´ˆê¸°í™”"""
        
        # 1. í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.pytorch_compat = PyTorchCompatibilityManager()
        self.m3_detector = M3MaxDetector()
        
        # 2. í•˜ë“œì›¨ì–´ ì •ë³´ ì„¤ì •
        self.hardware_specs = self.m3_detector.hardware_specs
        
        # 3. ê¸°ë³¸ ì†ì„± ì„¤ì • (ê¸°ì¡´ í˜¸í™˜ì„±)
        self.device = self.m3_detector.optimal_device
        self.device_name = self.hardware_specs.device_name
        self.device_type = self.device
        self.memory_gb = self.hardware_specs.memory_gb
        self.is_m3_max = self.hardware_specs.is_m3_max
        self.optimization_level = self.m3_detector.optimization_settings["optimization_level"]
        
        # 4. ì„¤ì • ë”•ì…”ë„ˆë¦¬ë“¤
        self.optimization_settings = self.m3_detector.optimization_settings
        self.device_info = {}
        self.model_config = {}
        self.pipeline_optimizations = {}
        
        # 5. ì´ˆê¸°í™” ìƒíƒœ
        self.is_initialized = False
        
        # 6. ì™„ì „ ì´ˆê¸°í™” ì‹¤í–‰
        self._complete_initialization()
    
    def _complete_initialization(self):
        """ì™„ì „ ì´ˆê¸°í™” ì‹¤í–‰"""
        try:
            logger.info("ğŸ”§ í†µí•© GPU ê´€ë¦¬ì ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. ë””ë°”ì´ìŠ¤ ìµœì í™” ì„¤ì •
            self._setup_device_optimizations()
            
            # 2. 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì •
            self._setup_pipeline_optimizations()
            
            # 3. ëª¨ë¸ ì„¤ì • êµ¬ì„±
            self._setup_model_configuration()
            
            # 4. ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘
            self._collect_comprehensive_device_info()
            
            # 5. í™˜ê²½ ë³€ìˆ˜ ìµœì í™”
            self._apply_environment_optimizations()
            
            # 6. ë©”ëª¨ë¦¬ ìµœì í™”
            self._optimize_memory_settings()
            
            self.is_initialized = True
            logger.info(f"ğŸš€ í†µí•© GPU ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ: {self.device} ({self.device_name})")
            
        except Exception as e:
            logger.error(f"âŒ í†µí•© GPU ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._setup_cpu_fallback()
    
    def _setup_device_optimizations(self):
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •"""
        try:
            if self.device == "mps":
                logger.info("ğŸ MPS ìµœì í™” ì„¤ì • ì ìš©")
                
                # MPS ì´ˆê¸° ë©”ëª¨ë¦¬ ì •ë¦¬
                self.pytorch_compat.safe_mps_memory_cleanup()
                
                # M3 Max íŠ¹í™” ì„¤ì •
                if self.is_m3_max:
                    logger.info("ğŸ M3 Max íŠ¹í™” ìµœì í™” ì ìš©")
                    # Neural Engine í™œì„±í™”
                    # Metal Performance Shaders í™œì„±í™”
                    # í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”
                
            elif self.device == "cuda":
                logger.info("ğŸš€ CUDA ìµœì í™” ì„¤ì • ì ìš©")
                
                # CUDA ìµœì í™” ì„¤ì •
                if hasattr(torch.backends.cudnn, 'enabled'):
                    torch.backends.cudnn.enabled = True
                    torch.backends.cudnn.benchmark = True
                    torch.backends.cudnn.deterministic = False
                
                # CUDA ì´ˆê¸° ë©”ëª¨ë¦¬ ì •ë¦¬
                self.pytorch_compat.safe_cuda_memory_cleanup()
                
            else:
                logger.info("ğŸ’» CPU ìµœì í™” ì„¤ì • ì ìš©")
                
                # CPU ìµœì í™” ì„¤ì •
                torch.set_num_threads(self.optimization_settings["max_workers"])
                
        except Exception as e:
            logger.error(f"ë””ë°”ì´ìŠ¤ ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _setup_pipeline_optimizations(self):
        """8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì •"""
        try:
            base_batch = self.optimization_settings["batch_size"]
            precision = "float16" if self.optimization_settings["fp16_enabled"] else "float32"
            
            # M3 Max íŠ¹í™” 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”
            if self.is_m3_max:
                self.pipeline_optimizations = {
                    "step_01_human_parsing": {
                        "batch_size": max(2, base_batch // 2),
                        "precision": precision,
                        "max_resolution": 768,
                        "memory_fraction": 0.25,
                        "enable_caching": True,
                        "neural_engine_boost": True,
                        "metal_shader_acceleration": True
                    },
                    "step_02_pose_estimation": {
                        "batch_size": base_batch,
                        "precision": precision,
                        "keypoint_threshold": 0.25,
                        "memory_fraction": 0.2,
                        "enable_caching": True,
                        "high_precision_mode": True,
                        "batch_optimization": True
                    },
                    "step_03_cloth_segmentation": {
                        "batch_size": base_batch,
                        "precision": precision,
                        "background_threshold": 0.4,
                        "memory_fraction": 0.25,
                        "enable_edge_refinement": True,
                        "unified_memory_optimization": True,
                        "parallel_processing": True
                    },
                    "step_04_geometric_matching": {
                        "batch_size": max(2, base_batch // 2),
                        "precision": precision,
                        "warp_resolution": 512,
                        "memory_fraction": 0.3,
                        "enable_caching": True,
                        "high_accuracy_mode": True,
                        "gpu_acceleration": True
                    },
                    "step_05_cloth_warping": {
                        "batch_size": base_batch,
                        "precision": precision,
                        "interpolation": "bicubic",
                        "memory_fraction": 0.25,
                        "preserve_details": True,
                        "texture_enhancement": True,
                        "anti_aliasing": True
                    },
                    "step_06_virtual_fitting": {
                        "batch_size": max(2, base_batch // 3),
                        "precision": precision,
                        "diffusion_steps": 25,
                        "memory_fraction": 0.5,
                        "scheduler": "ddim",
                        "guidance_scale": 7.5,
                        "high_quality_mode": True,
                        "neural_engine_diffusion": True
                    },
                    "step_07_post_processing": {
                        "batch_size": base_batch,
                        "precision": precision,
                        "enhancement_level": "ultra",
                        "memory_fraction": 0.2,
                        "noise_reduction": True,
                        "detail_preservation": True,
                        "color_correction": True
                    },
                    "step_08_quality_assessment": {
                        "batch_size": base_batch,
                        "precision": precision,
                        "quality_metrics": ["ssim", "lpips", "fid", "clip_score"],
                        "memory_fraction": 0.15,
                        "assessment_threshold": 0.8,
                        "comprehensive_analysis": True,
                        "real_time_feedback": True
                    }
                }
            else:
                # ì¼ë°˜ ì‹œìŠ¤í…œìš© íŒŒì´í”„ë¼ì¸ ìµœì í™”
                self.pipeline_optimizations = {
                    "step_01_human_parsing": {
                        "batch_size": 1,
                        "precision": precision,
                        "max_resolution": 512,
                        "memory_fraction": 0.3,
                        "enable_caching": False
                    },
                    "step_02_pose_estimation": {
                        "batch_size": 1,
                        "precision": precision,
                        "keypoint_threshold": 0.3,
                        "memory_fraction": 0.25,
                        "enable_caching": False
                    },
                    "step_03_cloth_segmentation": {
                        "batch_size": 1,
                        "precision": precision,
                        "background_threshold": 0.5,
                        "memory_fraction": 0.3,
                        "enable_edge_refinement": False
                    },
                    "step_04_geometric_matching": {
                        "batch_size": 1,
                        "precision": precision,
                        "warp_resolution": 256,
                        "memory_fraction": 0.35,
                        "enable_caching": False
                    },
                    "step_05_cloth_warping": {
                        "batch_size": 1,
                        "precision": precision,
                        "interpolation": "bilinear",
                        "memory_fraction": 0.3,
                        "preserve_details": False
                    },
                    "step_06_virtual_fitting": {
                        "batch_size": 1,
                        "precision": precision,
                        "diffusion_steps": 15,
                        "memory_fraction": 0.6,
                        "scheduler": "ddim",
                        "guidance_scale": 7.5
                    },
                    "step_07_post_processing": {
                        "batch_size": 1,
                        "precision": precision,
                        "enhancement_level": "medium",
                        "memory_fraction": 0.25,
                        "noise_reduction": False
                    },
                    "step_08_quality_assessment": {
                        "batch_size": 1,
                        "precision": precision,
                        "quality_metrics": ["ssim", "lpips"],
                        "memory_fraction": 0.2,
                        "assessment_threshold": 0.6
                    }
                }
            
            logger.info(f"âš™ï¸ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì • ì™„ë£Œ ({'M3 Max' if self.is_m3_max else 'ì¼ë°˜'})")
            
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
            self.pipeline_optimizations = {}
    
    def _setup_model_configuration(self):
        """ëª¨ë¸ ì„¤ì • êµ¬ì„±"""
        try:
            # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
            self.model_config = {
                "device": self.device,
                "dtype": "float16" if self.optimization_settings["fp16_enabled"] else "float32",
                "batch_size": self.optimization_settings["batch_size"],
                "max_workers": self.optimization_settings["max_workers"],
                "concurrent_sessions": self.optimization_settings["concurrent_sessions"],
                "memory_fraction": self.optimization_settings["memory_fraction"],
                "optimization_level": self.optimization_level,
                "quality_level": self.optimization_settings["quality_level"],
                "enable_caching": self.optimization_settings.get("step_caching", True),
                "enable_preloading": self.optimization_settings.get("model_preloading", False)
            }
            
            # M3 Max íŠ¹í™” ëª¨ë¸ ì„¤ì •
            if self.is_m3_max:
                self.model_config.update({
                    "use_neural_engine": self.optimization_settings["enable_neural_engine"],
                    "metal_performance_shaders": self.optimization_settings["metal_performance_shaders"],
                    "unified_memory_optimization": self.optimization_settings["unified_memory_optimization"],
                    "high_resolution_processing": self.optimization_settings["high_resolution_processing"],
                    "memory_pool_size_gb": self.optimization_settings["memory_pool_gb"],
                    "model_cache_size_gb": self.optimization_settings["cache_size_gb"],
                    "intermediate_cache_gb": self.optimization_settings["intermediate_cache_gb"],
                    "fp16_optimization": True,
                    "batch_optimization": True,
                    "pipeline_parallelism": self.optimization_settings["pipeline_parallelism"],
                    "neural_engine_acceleration": True,
                    "m3_max_optimized": True
                })
            
            logger.info(f"âš™ï¸ ëª¨ë¸ ì„¤ì • ì™„ë£Œ: ë°°ì¹˜={self.model_config['batch_size']}, ì •ë°€ë„={self.model_config['dtype']}")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì„¤ì • êµ¬ì„± ì‹¤íŒ¨: {e}")
            self.model_config = {"device": self.device, "batch_size": 1}
    
    def _collect_comprehensive_device_info(self):
        """í¬ê´„ì  ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        try:
            # ê¸°ë³¸ ë””ë°”ì´ìŠ¤ ì •ë³´
            self.device_info = {
                "device": self.device,
                "device_name": self.device_name,
                "device_type": self.device_type,
                "hardware_specs": self.hardware_specs.to_dict(),
                "pytorch_version": self.pytorch_compat.pytorch_version,
                "optimization_level": self.optimization_level,
                "is_m3_max": self.is_m3_max,
                "memory_gb": self.memory_gb,
                "optimization_settings": self.optimization_settings.copy()
            }
            
            # ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€
            memory_info = self.pytorch_compat.get_memory_info(self.device)
            self.device_info["memory_info"] = memory_info
            
            # PyTorch ê¸°ëŠ¥ ì •ë³´
            if self.device == "mps":
                self.device_info["mps_capabilities"] = self.pytorch_compat.mps_capabilities
            elif self.device == "cuda":
                self.device_info["cuda_capabilities"] = self.pytorch_compat.cuda_capabilities
            
            # M3 Max íŠ¹í™” ì •ë³´
            if self.is_m3_max:
                self.device_info["m3_max_features"] = {
                    "neural_engine_available": True,
                    "neural_engine_tops": "15.8 TOPS",
                    "gpu_cores": "30-40 cores",
                    "memory_bandwidth": "400GB/s",
                    "unified_memory": True,
                    "metal_performance_shaders": True,
                    "optimized_for_ai": True,
                    "pipeline_acceleration": True,
                    "real_time_processing": True,
                    "high_resolution_support": True
                }
            
            logger.info(f"â„¹ï¸ ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {self.device_name}")
            
        except Exception as e:
            logger.warning(f"ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.device_info = {
                "device": self.device,
                "device_name": self.device_name,
                "error": str(e)
            }
    
    def _apply_environment_optimizations(self):
        """í™˜ê²½ ë³€ìˆ˜ ìµœì í™”"""
        try:
            # ê³µí†µ PyTorch ì„¤ì •
            torch.set_num_threads(self.optimization_settings["max_workers"])
            
            if self.device == "mps":
                # MPS í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                env_vars = {
                    'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                    'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0'
                }
                
                # M3 Max íŠ¹í™” í™˜ê²½ ë³€ìˆ˜
                if self.is_m3_max:
                    env_vars.update({
                        'PYTORCH_MPS_ALLOCATOR_POLICY': 'garbage_collection',
                        'METAL_DEVICE_WRAPPER_TYPE': '1',
                        'METAL_PERFORMANCE_SHADERS_ENABLED': '1',
                        'METAL_FORCE_INTEL_GPU': '0',
                        'METAL_DEVICE_WRAPPER_TYPE': '1',
                        'PYTORCH_MPS_PREFER_METAL': '1'
                    })
                
                # í™˜ê²½ ë³€ìˆ˜ ì ìš©
                for key, value in env_vars.items():
                    os.environ[key] = value
                    
                logger.info("ğŸ MPS í™˜ê²½ ë³€ìˆ˜ ìµœì í™” ì ìš©")
                
            elif self.device == "cuda":
                # CUDA í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                env_vars = {
                    'CUDA_LAUNCH_BLOCKING': '0',
                    'CUDA_CACHE_DISABLE': '0',
                    'CUDA_VISIBLE_DEVICES': '0'
                }
                
                for key, value in env_vars.items():
                    os.environ[key] = value
                    
                logger.info("ğŸš€ CUDA í™˜ê²½ ë³€ìˆ˜ ìµœì í™” ì ìš©")
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
            gc.collect()
            
        except Exception as e:
            logger.warning(f"í™˜ê²½ ë³€ìˆ˜ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_memory_settings(self):
        """ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™”"""
        try:
            # ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ìµœì í™”
            if self.device == "mps":
                self.pytorch_compat.safe_mps_memory_cleanup()
            elif self.device == "cuda":
                self.pytorch_compat.safe_cuda_memory_cleanup()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            logger.info("ğŸ’¾ ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _setup_cpu_fallback(self):
        """CPU í´ë°± ì„¤ì •"""
        logger.warning("ğŸš¨ CPU í´ë°± ëª¨ë“œë¡œ ì„¤ì •")
        
        self.device = "cpu"
        self.device_type = "cpu"
        self.device_name = "CPU (Fallback)"
        self.is_m3_max = False
        self.optimization_level = "safe"
        
        self.model_config = {
            "device": "cpu",
            "dtype": "float32",
            "batch_size": 1,
            "memory_fraction": 0.5,
            "optimization_level": "safe"
        }
        
        self.device_info = {
            "device": "cpu",
            "device_name": "CPU (Fallback)",
            "error": "GPU initialization failed"
        }
        
        self.pipeline_optimizations = {}
        self.is_initialized = True
    
    # =========================================================================
    # ğŸ”§ í˜¸í™˜ì„± ë©”ì„œë“œë“¤ (ê¸°ì¡´ ì½”ë“œì™€ 100% í˜¸í™˜ì„± ë³´ì¥)
    # =========================================================================
    
    def get(self, key: str, default: Any = None) -> Any:
        """ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì ‘ê·¼ ë©”ì„œë“œ (í˜¸í™˜ì„±)"""
        
        # ì§ì ‘ ì†ì„± ë§¤í•‘
        attribute_mapping = {
            'device': self.device,
            'device_name': self.device_name,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'optimization_level': self.optimization_level,
            'is_initialized': self.is_initialized,
            'device_info': self.device_info,
            'model_config': self.model_config,
            'pipeline_optimizations': self.pipeline_optimizations,
            'optimization_settings': self.optimization_settings,
            'hardware_info': self.hardware_specs.to_dict(),
            'pytorch_version': self.pytorch_compat.pytorch_version,
            'mps_capabilities': self.pytorch_compat.mps_capabilities,
            'cuda_capabilities': self.pytorch_compat.cuda_capabilities
        }
        
        # ì§ì ‘ ë§¤í•‘ì—ì„œ ì°¾ê¸°
        if key in attribute_mapping:
            return attribute_mapping[key]
        
        # ëª¨ë¸ ì„¤ì •ì—ì„œ ì°¾ê¸°
        if key in self.model_config:
            return self.model_config[key]
        
        # ë””ë°”ì´ìŠ¤ ì •ë³´ì—ì„œ ì°¾ê¸°
        if key in self.device_info:
            return self.device_info[key]
        
        # íŒŒì´í”„ë¼ì¸ ìµœì í™”ì—ì„œ ì°¾ê¸°
        if key in self.pipeline_optimizations:
            return self.pipeline_optimizations[key]
        
        # ìµœì í™” ì„¤ì •ì—ì„œ ì°¾ê¸°
        if key in self.optimization_settings:
            return self.optimization_settings[key]
        
        # ì†ì„±ìœ¼ë¡œ ì§ì ‘ ì ‘ê·¼
        if hasattr(self, key):
            return getattr(self, key)
        
        return default
    
    def keys(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ ëª©ë¡"""
        return [
            'device', 'device_name', 'device_type', 'memory_gb',
            'is_m3_max', 'optimization_level', 'is_initialized',
            'device_info', 'model_config', 'pipeline_optimizations',
            'optimization_settings', 'hardware_info', 'pytorch_version',
            'mps_capabilities', 'cuda_capabilities'
        ]
    
    def items(self):
        """í‚¤-ê°’ ìŒ ë°˜í™˜"""
        return [(key, self.get(key)) for key in self.keys()]
    
    def __getitem__(self, key: str) -> Any:
        """[] ì ‘ê·¼ì ì§€ì›"""
        result = self.get(key)
        if result is None:
            raise KeyError(f"Key '{key}' not found")
        return result
    
    def __contains__(self, key: str) -> bool:
        """in ì—°ì‚°ì ì§€ì›"""
        return self.get(key) is not None
    
    # =========================================================================
    # ğŸ”§ ê¸°ì¡´ í˜¸í™˜ì„± ë©”ì„œë“œë“¤
    # =========================================================================
    
    def get_device(self) -> str:
        """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self.device
    
    def get_device_name(self) -> str:
        """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
        return self.device_name
    
    def get_device_type(self) -> str:
        """ë””ë°”ì´ìŠ¤ íƒ€ì… ë°˜í™˜"""
        return self.device_type
    
    def get_recommended_batch_size(self) -> int:
        """ê¶Œì¥ ë°°ì¹˜ í¬ê¸° ë°˜í™˜"""
        return self.model_config.get('batch_size', 1)
    
    def get_recommended_precision(self) -> str:
        """ê¶Œì¥ ì •ë°€ë„ ë°˜í™˜"""
        return self.model_config.get('dtype', 'float32')
    
    def get_memory_fraction(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„ìœ¨ ë°˜í™˜"""
        return self.model_config.get('memory_fraction', 0.5)
    
    def setup_multiprocessing(self) -> int:
        """ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ ìˆ˜ ì„¤ì •"""
        return self.model_config.get('max_workers', 4)
    
    def get_device_config(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
        return {
            "device": self.device,
            "device_name": self.device_name,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_level": self.optimization_level,
            "neural_engine_available": self.is_m3_max,
            "metal_performance_shaders": self.is_m3_max,
            "unified_memory_optimization": self.is_m3_max,
            "high_resolution_processing": self.optimization_settings.get("high_resolution_processing", False),
            "pipeline_parallelism": self.optimization_settings.get("pipeline_parallelism", False)
        }
    
    def get_model_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
        return self.model_config.copy()
    
    def get_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return self.device_info.copy()
    
    def get_pipeline_config(self, step_name: str) -> Dict[str, Any]:
        """íŠ¹ì • íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        return self.pipeline_optimizations.get(step_name, {})
    
    def get_all_pipeline_configs(self) -> Dict[str, Any]:
        """ëª¨ë“  íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        return self.pipeline_optimizations.copy()
    
    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë¦¬ (í˜¸í™˜ì„± ë©”ì„œë“œ)"""
        return optimize_memory(self.device, aggressive)
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„ ë°˜í™˜"""
        return self.pytorch_compat.get_memory_info(self.device)

# =============================================================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (main.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ë“¤)
# =============================================================================

def check_memory_available(device: Optional[str] = None, min_gb: float = 1.0) -> Dict[str, Any]:
    """
    ğŸ”¥ ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ìƒíƒœ í™•ì¸ - main.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜
    
    Args:
        device: í™•ì¸í•  ë””ë°”ì´ìŠ¤ (None=ìë™)
        min_gb: ìµœì†Œ í•„ìš” ë©”ëª¨ë¦¬ (GB)
    
    Returns:
        ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ìƒíƒœ ì •ë³´
    """
    try:
        current_device = device or gpu_config.device
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸
        if PSUTIL_AVAILABLE:
            vm = psutil.virtual_memory()
            system_memory = {
                "total_gb": round(vm.total / (1024**3), 2),
                "available_gb": round(vm.available / (1024**3), 2),
                "used_gb": round(vm.used / (1024**3), 2),
                "percent_used": vm.percent
            }
        else:
            system_memory = {
                "total_gb": 16.0,
                "available_gb": 8.0,
                "used_gb": 8.0,
                "percent_used": 50.0
            }
        
        result = {
            "device": current_device,
            "system_memory": system_memory,
            "is_available": system_memory["available_gb"] >= min_gb,
            "min_required_gb": min_gb,
            "timestamp": time.time(),
            "pytorch_version": torch.__version__,
            "is_m3_max": gpu_config.is_m3_max
        }
        
        # ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€
        if current_device == "mps":
            result["mps_memory"] = {
                "unified_memory": True,
                "total_gb": system_memory["total_gb"],
                "available_gb": system_memory["available_gb"],
                "note": "MPS uses unified memory system",
                "neural_engine_available": gpu_config.is_m3_max
            }
        elif current_device == "cuda" and torch.cuda.is_available():
            try:
                gpu_props = torch.cuda.get_device_properties(0)
                gpu_memory = gpu_props.total_memory / (1024**3)
                gpu_allocated = torch.cuda.memory_allocated(0) / (1024**3)
                
                result["gpu_memory"] = {
                    "total_gb": round(gpu_memory, 2),
                    "allocated_gb": round(gpu_allocated, 2),
                    "available_gb": round(gpu_memory - gpu_allocated, 2),
                    "device_name": gpu_props.name
                }
                
                result["is_available"] = result["is_available"] and (gpu_memory - gpu_allocated) >= min_gb
            except Exception as e:
                result["gpu_memory_error"] = str(e)
        
        logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬ í™•ì¸ ì™„ë£Œ: {current_device} ({system_memory['available_gb']:.1f}GB ì‚¬ìš© ê°€ëŠ¥)")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        return {
            "device": device or "unknown",
            "error": str(e),
            "is_available": False,
            "min_required_gb": min_gb,
            "timestamp": time.time()
        }

def optimize_memory(device: Optional[str] = None, aggressive: bool = False) -> Dict[str, Any]:
    """
    ğŸ”¥ ë©”ëª¨ë¦¬ ìµœì í™” - PyTorch 2.5.1 MPS í˜¸í™˜ì„± ì™„ì „ í•´ê²°
    
    Args:
        device: ëŒ€ìƒ ë””ë°”ì´ìŠ¤
        aggressive: ê³µê²©ì  ì •ë¦¬ ì—¬ë¶€
    
    Returns:
        ìµœì í™” ê²°ê³¼ ì •ë³´
    """
    try:
        current_device = device or gpu_config.device
        
        # ì‹œì‘ ë©”ëª¨ë¦¬ ìƒíƒœ
        if PSUTIL_AVAILABLE:
            start_memory = psutil.virtual_memory().percent
        else:
            start_memory = 50.0
        
        # ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        result = {
            "success": True,
            "device": current_device,
            "start_memory_percent": start_memory,
            "method": "standard_gc",
            "aggressive": aggressive,
            "pytorch_version": torch.__version__,
            "is_m3_max": gpu_config.is_m3_max
        }
        
        # ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì •ë¦¬
        if current_device == "mps":
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (PyTorch 2.5.1 í˜¸í™˜ì„±)
            mps_result = gpu_config.pytorch_compat.safe_mps_memory_cleanup()
            result["mps_cleanup"] = mps_result
            
            if mps_result["success"]:
                result["method"] = f"mps_{mps_result['method']}"
            else:
                result["method"] = "mps_fallback"
                result["warning"] = "MPS ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ ì—†ìŒ"
            
            # M3 Max ê³µê²©ì  ì •ë¦¬
            if aggressive and gpu_config.is_m3_max:
                try:
                    # ì¶”ê°€ ë™ê¸°í™” ë° ì •ë¦¬
                    if gpu_config.pytorch_compat.mps_capabilities.get('synchronize', False):
                        torch.mps.synchronize()
                    gc.collect()
                    result["method"] = "m3_max_aggressive_cleanup"
                    result["m3_max_optimized"] = True
                except Exception as e:
                    result["aggressive_error"] = str(e)
        
        elif current_device == "cuda":
            # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
            cuda_result = gpu_config.pytorch_compat.safe_cuda_memory_cleanup()
            result["cuda_cleanup"] = cuda_result
            
            if cuda_result["success"]:
                result["method"] = f"cuda_{cuda_result['method']}"
                
                if aggressive:
                    try:
                        torch.cuda.synchronize()
                        result["method"] = "cuda_aggressive_cleanup"
                    except Exception as e:
                        result["aggressive_error"] = str(e)
            else:
                result["warning"] = "CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ ì—†ìŒ"
        
        # ì¢…ë£Œ ë©”ëª¨ë¦¬ ìƒíƒœ
        if PSUTIL_AVAILABLE:
            end_memory = psutil.virtual_memory().percent
            memory_freed = max(0, start_memory - end_memory)
        else:
            end_memory = 45.0
            memory_freed = 5.0
        
        result.update({
            "end_memory_percent": end_memory,
            "memory_freed_percent": memory_freed
        })
        
        if memory_freed > 0:
            logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ {memory_freed:.1f}% ì •ë¦¬ë¨ ({result['method']})")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "device": device or "unknown",
            "method": "failed"
        }

def get_optimal_settings() -> Dict[str, Any]:
    """ìµœì  ì„¤ì • ë°˜í™˜"""
    return gpu_config.optimization_settings.copy()

def get_device_capabilities() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ë°˜í™˜"""
    capabilities = {
        "device": gpu_config.device,
        "device_name": gpu_config.device_name,
        "supports_fp16": gpu_config.optimization_settings.get("fp16_enabled", False),
        "supports_compilation": gpu_config.optimization_settings.get("compilation_enabled", False),
        "supports_parallel_inference": True,
        "max_batch_size": gpu_config.optimization_settings.get("batch_size", 1) * 2,
        "recommended_image_size": (768, 768) if gpu_config.is_m3_max else (512, 512),
        "supports_8step_pipeline": True,
        "optimization_level": gpu_config.optimization_level,
        "memory_gb": gpu_config.memory_gb,
        "pytorch_version": gpu_config.pytorch_compat.pytorch_version,
        "is_m3_max": gpu_config.is_m3_max
    }
    
    # ë””ë°”ì´ìŠ¤ë³„ íŠ¹í™” ê¸°ëŠ¥
    if gpu_config.device == "mps":
        capabilities.update({
            "supports_neural_engine": gpu_config.is_m3_max,
            "supports_metal_shaders": True,
            "mps_capabilities": gpu_config.pytorch_compat.mps_capabilities,
            "unified_memory_optimization": gpu_config.is_m3_max,
            "high_resolution_processing": gpu_config.optimization_settings.get("high_resolution_processing", False),
            "pipeline_parallelism": gpu_config.optimization_settings.get("pipeline_parallelism", False)
        })
    elif gpu_config.device == "cuda":
        capabilities.update({
            "cuda_capabilities": gpu_config.pytorch_compat.cuda_capabilities,
            "tensor_cores_available": True,
            "supports_mixed_precision": True
        })
    
    return capabilities

def apply_optimizations() -> bool:
    """ìµœì í™” ì„¤ì • ì ìš©"""
    try:
        # GPU ê´€ë¦¬ìê°€ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìœ¼ë©´ ì„±ê³µ
        if gpu_config.is_initialized:
            logger.info("âœ… GPU ìµœì í™” ì„¤ì • ì´ë¯¸ ì ìš©ë¨")
            return True
        
        # ê°•ì œ ì¬ì´ˆê¸°í™”
        gpu_config._complete_initialization()
        
        logger.info("âœ… GPU ìµœì í™” ì„¤ì • ì ìš© ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ GPU ìµœì í™” ì„¤ì • ì ìš© ì‹¤íŒ¨: {e}")
        return False

def get_memory_info(device: Optional[str] = None) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
    try:
        current_device = device or gpu_config.device
        return gpu_config.pytorch_compat.get_memory_info(current_device)
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            "device": device or "unknown",
            "error": str(e),
            "available": False
        }

# =============================================================================
# ğŸ”§ ê¸°ì¡´ í˜¸í™˜ì„± í´ë˜ìŠ¤ë“¤ (step_routes.py í˜¸í™˜ì„±)
# =============================================================================

# ê¸°ì¡´ í´ë˜ìŠ¤ ì´ë¦„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
class GPUConfig:
    """ê¸°ì¡´ GPUConfig í´ë˜ìŠ¤ í˜¸í™˜ì„±"""
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

class GPUDetector:
    """ê¸°ì¡´ GPUDetector í´ë˜ìŠ¤ í˜¸í™˜ì„±"""
    def __init__(self):
        self.gpu_config = gpu_config
        self.system_info = gpu_config.hardware_specs.to_dict()
        self.gpu_info = gpu_config.device_info
        self.is_m3_max = gpu_config.is_m3_max
    
    def get_optimized_settings(self):
        return gpu_config.optimization_settings

class M3MaxGPUManager(UnifiedGPUManager):
    """ê¸°ì¡´ M3MaxGPUManager í´ë˜ìŠ¤ í˜¸í™˜ì„±"""
    pass

class M3Optimizer:
    """ê¸°ì¡´ M3Optimizer í´ë˜ìŠ¤ í˜¸í™˜ì„±"""
    def __init__(self, device_name: str, memory_gb: float, is_m3_max: bool, optimization_level: str):
        self.device_name = device_name
        self.memory_gb = memory_gb
        self.is_m3_max = is_m3_max
        self.optimization_level = optimization_level
        
        if is_m3_max:
            logger.info(f"ğŸ M3Optimizer ì´ˆê¸°í™”: {device_name}, {memory_gb}GB, {optimization_level}")

class M3MaxDetector:
    """ê¸°ì¡´ M3MaxDetector í´ë˜ìŠ¤ í˜¸í™˜ì„±"""
    def __init__(self):
        self.is_m3_max = gpu_config.is_m3_max
        self.memory_gb = gpu_config.memory_gb
        self.platform_info = gpu_config.hardware_specs.to_dict()
        
        # ìµœì í™” ì„¤ì • ê³„ì‚°
        self.optimization_config = gpu_config.optimization_settings

# =============================================================================
# ğŸ”§ ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì „ì—­ ë³€ìˆ˜
# =============================================================================

# ì „ì—­ í†µí•© GPU ì„¤ì • ë§¤ë‹ˆì € ìƒì„±
try:
    gpu_config = UnifiedGPUManager()
    logger.info("ğŸ‰ í†µí•© GPU ì„¤ì • ë§¤ë‹ˆì € ìƒì„± ì™„ë£Œ")
except Exception as e:
    logger.error(f"âŒ í†µí•© GPU ì„¤ì • ë§¤ë‹ˆì € ìƒì„± ì‹¤íŒ¨: {e}")
    # ìµœì†Œí•œì˜ í´ë°± ê°ì²´ ìƒì„±
    class FallbackGPUManager:
        def __init__(self):
            self.device = "cpu"
            self.device_name = "CPU (Fallback)"
            self.device_type = "cpu"
            self.is_m3_max = False
            self.memory_gb = 8.0
            self.optimization_level = "safe"
            self.is_initialized = True
            self.model_config = {"device": "cpu", "batch_size": 1}
            self.device_info = {"device": "cpu"}
            self.pipeline_optimizations = {}
            self.optimization_settings = {"batch_size": 1}
            self.pytorch_compat = None
        
        def get(self, key, default=None):
            return getattr(self, key, default)
        
        def get_device(self):
            return self.device
        
        def get_device_name(self):
            return self.device_name
        
        def get_device_config(self):
            return {"device": self.device}
        
        def get_model_config(self):
            return self.model_config
        
        def get_device_info(self):
            return self.device_info
        
        def cleanup_memory(self, aggressive=False):
            return {"success": True, "method": "cpu_gc"}
        
        def get_memory_stats(self):
            return {"device": "cpu", "available": True}
    
    gpu_config = FallbackGPUManager()

# í¸ì˜ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ë“¤ (ê¸°ì¡´ í˜¸í™˜ì„±)
DEVICE = gpu_config.device
DEVICE_NAME = gpu_config.device_name
DEVICE_TYPE = gpu_config.device_type
MODEL_CONFIG = gpu_config.get('model_config', {})
DEVICE_INFO = gpu_config.get('device_info', {})
IS_M3_MAX = gpu_config.get('is_m3_max', False)

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
gpu_detector = gpu_config  # ê¸°ì¡´ gpu_detector í˜¸í™˜ì„±

# =============================================================================
# ğŸ”§ ì£¼ìš” í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
# =============================================================================

@lru_cache(maxsize=1)
def get_gpu_config() -> UnifiedGPUManager:
    """GPU ì„¤ì • ë§¤ë‹ˆì € ë°˜í™˜ (ìºì‹œë¨)"""
    return gpu_config

def get_device_config() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_device_config()

def get_model_config() -> Dict[str, Any]:
    """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_model_config()

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
    return gpu_config.get_device_info()

def get_device() -> str:
    """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return gpu_config.get_device()

def is_m3_max() -> bool:
    """M3 Max ì—¬ë¶€ í™•ì¸"""
    return gpu_config.get('is_m3_max', False)

def check_memory_availability(min_gb: float = 2.0, device: Optional[str] = None) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ê°€ìš©ì„± ì²´í¬ (ê¸°ì¡´ í•¨ìˆ˜ëª… í˜¸í™˜ì„±)"""
    return check_memory_available(device, min_gb)

def safe_mps_memory_cleanup() -> Dict[str, Any]:
    """ì•ˆì „í•œ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (ê¸°ì¡´ í•¨ìˆ˜ëª… í˜¸í™˜ì„±)"""
    if gpu_config.pytorch_compat:
        return gpu_config.pytorch_compat.safe_mps_memory_cleanup()
    else:
        return {"success": False, "error": "pytorch_compat not available"}

# =============================================================================
# ğŸ”§ ì´ˆê¸°í™” ì™„ë£Œ ë¡œê¹… ë° ìƒíƒœ ì¶œë ¥
# =============================================================================

def _log_initialization_status():
    """ì´ˆê¸°í™” ìƒíƒœ ë¡œê¹…"""
    try:
        if gpu_config.get('is_initialized', False):
            logger.info("âœ… í†µí•© GPU ì„¤ì • ì™„ì „ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {DEVICE}")
            logger.info(f"ğŸ M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
            logger.info(f"ğŸ§  ë©”ëª¨ë¦¬: {gpu_config.get('memory_gb', 0):.1f}GB")
            logger.info(f"âš™ï¸ ìµœì í™”: {gpu_config.get('optimization_level', 'unknown')}")
            logger.info(f"ğŸ¯ PyTorch: {gpu_config.get('pytorch_version', 'unknown') if hasattr(gpu_config, 'pytorch_compat') and gpu_config.pytorch_compat else 'unknown'}")
            
            # M3 Max ì„¸ë¶€ ì •ë³´
            if IS_M3_MAX:
                logger.info("ğŸ M3 Max 128GB ìµœì í™” í™œì„±í™”:")
                logger.info(f"  - Neural Engine: âœ…")
                logger.info(f"  - Metal Performance Shaders: âœ…")
                logger.info(f"  - í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”: âœ…")
                logger.info(f"  - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”: âœ…")
                logger.info(f"  - ê³ í•´ìƒë„ ì²˜ë¦¬: âœ…")
                logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {MODEL_CONFIG.get('batch_size', 1)}")
                logger.info(f"  - ì •ë°€ë„: {MODEL_CONFIG.get('dtype', 'unknown')}")
                logger.info(f"  - ë™ì‹œ ì„¸ì…˜: {gpu_config.get('concurrent_sessions', 1)}")
                logger.info(f"  - ë©”ëª¨ë¦¬ í’€: {gpu_config.get('memory_pool_gb', 0)}GB")
                logger.info(f"  - ìºì‹œ í¬ê¸°: {gpu_config.get('cache_size_gb', 0)}GB")
            
            # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ìƒíƒœ
            pipeline_count = len(gpu_config.get('pipeline_optimizations', {}))
            if pipeline_count > 0:
                logger.info(f"âš™ï¸ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”: {pipeline_count}ê°œ ë‹¨ê³„ ì„¤ì •ë¨")
            
            # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            memory_check = check_memory_available(min_gb=1.0)
            if memory_check.get('is_available', False):
                logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ìƒíƒœ: {memory_check['system_memory']['available_gb']:.1f}GB ì‚¬ìš© ê°€ëŠ¥")
            
        else:
            logger.warning("âš ï¸ í†µí•© GPU ì„¤ì • ì´ˆê¸°í™” ë¶ˆì™„ì „")
            
    except Exception as e:
        logger.error(f"ì´ˆê¸°í™” ìƒíƒœ ë¡œê¹… ì‹¤íŒ¨: {e}")

# ì´ˆê¸°í™” ìƒíƒœ ë¡œê¹… ì‹¤í–‰
_log_initialization_status()

# =============================================================================
# ğŸ”§ Export ë¦¬ìŠ¤íŠ¸
# =============================================================================

__all__ = [
    # ì£¼ìš” ê°ì²´ë“¤
    'gpu_config', 'gpu_detector', 'DEVICE', 'DEVICE_NAME', 'DEVICE_TYPE', 
    'MODEL_CONFIG', 'DEVICE_INFO', 'IS_M3_MAX',
    
    # í•µì‹¬ í•¨ìˆ˜ë“¤
    'get_gpu_config', 'get_device_config', 'get_model_config', 'get_device_info',
    'get_device', 'is_m3_max', 'get_optimal_settings', 'get_device_capabilities',
    'apply_optimizations',
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤ (main.pyì—ì„œ ì‚¬ìš©)
    'check_memory_available', 'check_memory_availability', 'optimize_memory', 
    'get_memory_info', 'safe_mps_memory_cleanup',
    
    # í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ í˜¸í™˜ì„± í¬í•¨)
    'UnifiedGPUManager', 'M3MaxGPUManager', 'GPUConfig', 'GPUDetector',
    'M3Optimizer', 'M3MaxDetector', 'PyTorchCompatibilityManager',
    'HardwareSpecs', 'M3MaxDetector'
]

# ëª¨ë“ˆ ì™„ë£Œ ë¡œê¹…
logger.info("ğŸ‰ í†µí•© GPU ì„¤ì • ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ!")
logger.info("ğŸ“‹ ì£¼ìš” íŠ¹ì§•:")
logger.info("  - PyTorch 2.5.1 MPS ì™„ì „ í˜¸í™˜ì„±")
logger.info("  - M3 Max 128GB íŠ¹í™” ìµœì í™”")
logger.info("  - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”")
logger.info("  - ê¸°ì¡´ ì½”ë“œ 100% í˜¸í™˜ì„±")
logger.info("  - í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬")
logger.info("  - ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")

# ìµœì¢… ìƒíƒœ ìš”ì•½
if IS_M3_MAX:
    logger.info("ğŸš€ M3 Max 128GB ìµœì í™” ì™„ë£Œ - ìµœê³  ì„±ëŠ¥ ëª¨ë“œ í™œì„±í™”!")
else:
    logger.info(f"âœ… {DEVICE_NAME} ìµœì í™” ì™„ë£Œ - ì•ˆì •ì  ë™ì‘ ëª¨ë“œ í™œì„±í™”!")

# ê°œë°œì íŒ
logger.info("ğŸ’¡ ê°œë°œì íŒ:")
logger.info("  - gpu_config.get('key')ë¡œ ëª¨ë“  ì„¤ì • ì ‘ê·¼ ê°€ëŠ¥")
logger.info("  - check_memory_available()ë¡œ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸")
logger.info("  - optimize_memory()ë¡œ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰")
logger.info("  - get_device_capabilities()ë¡œ ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ í™•ì¸")