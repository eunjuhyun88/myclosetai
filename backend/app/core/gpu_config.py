# app/core/gpu_config.py
"""
MyCloset AI - M3 Max 128GB ì™„ì „ ìµœì í™” GPU ì„¤ì •
8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ ìµœì í™”
Pydantic V2 í˜¸í™˜, ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ í¬í•¨
"""

import os
import logging
import torch
import platform
from typing import Dict, Any, Optional, Union, List
from dataclasses import dataclass
from functools import lru_cache
import gc
import json
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class GPUConfig:
    """GPU ì„¤ì • ë°ì´í„° í´ë˜ìŠ¤"""
    device: str
    device_name: str
    device_type: str
    memory_gb: float
    is_m3_max: bool
    optimization_level: str
    neural_engine_available: bool = False
    metal_performance_shaders: bool = False
    unified_memory_optimization: bool = False

class M3MaxGPUManager:
    """M3 Max 128GB ì „ìš© GPU ê´€ë¦¬ì - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.device = None
        self.device_name = ""
        self.device_type = ""
        self.memory_gb = 0.0
        self.is_m3_max = False
        self.optimization_level = "balanced"
        self.device_info = {}
        self.model_config = {}
        self.is_initialized = False
        self.neural_engine_available = False
        self.metal_performance_shaders = False
        
        # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ë³„ ìµœì í™” ì„¤ì •
        self.pipeline_optimizations = {}
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize()
    
    def _initialize(self):
        """GPU ì„¤ì • ì™„ì „ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”§ M3 Max GPU ì„¤ì • ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. í•˜ë“œì›¨ì–´ ê°ì§€
            self._detect_hardware()
            
            # 2. ë””ë°”ì´ìŠ¤ ì„¤ì •
            self._setup_device()
            
            # 3. 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì •
            self._setup_pipeline_optimizations()
            
            # 4. ëª¨ë¸ ì„¤ì •
            self._setup_model_config()
            
            # 5. ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘
            self._collect_device_info()
            
            # 6. í™˜ê²½ ìµœì í™” ì ìš©
            self._apply_optimizations()
            
            self.is_initialized = True
            logger.info(f"ğŸš€ M3 Max GPU ì„¤ì • ì™„ë£Œ: {self.device}")
            
        except Exception as e:
            logger.error(f"âŒ GPU ì„¤ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._fallback_cpu_setup()
    
    def _detect_hardware(self):
        """M3 Max í•˜ë“œì›¨ì–´ ì •ë°€ ê°ì§€"""
        try:
            import psutil
            
            system_info = {
                "platform": platform.system(),
                "machine": platform.machine(),
                "processor": platform.processor(),
            }
            
            # Apple Silicon í™•ì¸
            if (system_info["platform"] == "Darwin" and 
                system_info["machine"] == "arm64"):
                
                # ë©”ëª¨ë¦¬ í¬ê¸°ë¡œ M3 Max íŒì •
                memory_gb = psutil.virtual_memory().total / (1024**3)
                
                if memory_gb >= 120:  # 128GB M3 Max
                    self.is_m3_max = True
                    self.optimization_level = "ultra"
                    self.neural_engine_available = True
                    self.metal_performance_shaders = True
                    logger.info(f"ğŸ M3 Max 128GB ê°ì§€! ë©”ëª¨ë¦¬: {memory_gb:.0f}GB")
                    
                elif memory_gb >= 90:  # 96GB M3 Max
                    self.is_m3_max = True
                    self.optimization_level = "high"
                    self.neural_engine_available = True
                    logger.info(f"ğŸ M3 Max 96GB ê°ì§€! ë©”ëª¨ë¦¬: {memory_gb:.0f}GB")
                    
                else:  # ê¸°íƒ€ Apple Silicon
                    self.optimization_level = "balanced"
                    logger.info(f"ğŸ Apple Silicon ê°ì§€ - ë©”ëª¨ë¦¬: {memory_gb:.0f}GB")
                
                self.memory_gb = memory_gb
            
            else:
                logger.info("ğŸ–¥ï¸ ë¹„-Apple Silicon í™˜ê²½ ê°ì§€")
                self.memory_gb = psutil.virtual_memory().total / (1024**3)
            
        except Exception as e:
            logger.warning(f"âš ï¸ í•˜ë“œì›¨ì–´ ê°ì§€ ì‹¤íŒ¨: {e}")
            self.optimization_level = "safe"
    
    def _setup_device(self):
        """ë””ë°”ì´ìŠ¤ ì„¤ì • ë° MPS ìµœì í™”"""
        try:
            # MPS ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
            if torch.backends.mps.is_available():
                self.device = "mps"
                self.device_type = "mps"
                self.device_name = "Apple Silicon GPU (MPS)"
                
                # M3 Max íŠ¹í™” MPS í™˜ê²½ë³€ìˆ˜ ì„¤ì •
                if self.is_m3_max:
                    os.environ.update({
                        'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',  # 128GB ë©”ëª¨ë¦¬ ìµœì í™”
                        'PYTORCH_MPS_ALLOCATOR_POLICY': 'garbage_collection',
                        'METAL_DEVICE_WRAPPER_TYPE': '1',
                        'METAL_PERFORMANCE_SHADERS_ENABLED': '1'
                    })
                    logger.info("ğŸ M3 Max MPS í™˜ê²½ë³€ìˆ˜ ìµœì í™” ì ìš©")
                
                logger.info("ğŸ Apple Silicon MPS í™œì„±í™”")
                
            elif torch.cuda.is_available():
                self.device = "cuda"
                self.device_type = "cuda"
                self.device_name = torch.cuda.get_device_name(0)
                logger.info("ğŸš€ CUDA GPU ê°ì§€")
                
            else:
                self.device = "cpu"
                self.device_type = "cpu"
                self.device_name = "CPU"
                logger.info("ğŸ’» CPU ëª¨ë“œ ì„¤ì •")
                
        except Exception as e:
            logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self._fallback_cpu_setup()
    
    def _setup_pipeline_optimizations(self):
        """8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ë³„ ìµœì í™” ì„¤ì •"""
        
        # M3 Max 128GB ê¸°ì¤€ ìµœì í™”
        if self.is_m3_max and self.device == "mps":
            base_config = {
                "batch_size": 2,
                "precision": "float16",
                "memory_fraction": 0.6,  # 128GB ì¤‘ ì¼ë¶€ë§Œ ì‚¬ìš©
                "enable_attention_slicing": True,
                "enable_cpu_offload": False,  # 128GB RAM ì¶©ë¶„
                "concurrent_processing": True
            }
        else:
            base_config = {
                "batch_size": 1,
                "precision": "float32",
                "memory_fraction": 0.7,
                "enable_attention_slicing": True,
                "enable_cpu_offload": True,
                "concurrent_processing": False
            }
        
        # 8ë‹¨ê³„ë³„ íŠ¹í™” ì„¤ì •
        self.pipeline_optimizations = {
            "step_01_human_parsing": {
                **base_config,
                "model_precision": "float16" if self.device == "mps" else "float32",
                "max_resolution": 512,
                "enable_segmentation_cache": True
            },
            "step_02_pose_estimation": {
                **base_config,
                "openpose_precision": "float16" if self.device == "mps" else "float32",
                "keypoint_threshold": 0.3,
                "enable_pose_cache": True
            },
            "step_03_cloth_segmentation": {
                **base_config,
                "segmentation_model": "u2net",
                "background_threshold": 0.5,
                "enable_edge_refinement": True
            },
            "step_04_geometric_matching": {
                **base_config,
                "matching_algorithm": "optical_flow",
                "warp_resolution": 256,
                "enable_geometric_cache": True
            },
            "step_05_cloth_warping": {
                **base_config,
                "warp_method": "thin_plate_spline",
                "interpolation": "bilinear",
                "preserve_details": True
            },
            "step_06_virtual_fitting": {
                **base_config,
                "diffusion_steps": 20 if self.is_m3_max else 15,
                "guidance_scale": 7.5,
                "enable_safety_checker": True,
                "scheduler": "ddim"
            },
            "step_07_post_processing": {
                **base_config,
                "enhancement_level": "high" if self.is_m3_max else "medium",
                "noise_reduction": True,
                "color_correction": True
            },
            "step_08_quality_assessment": {
                **base_config,
                "quality_metrics": ["ssim", "lpips", "fid"],
                "assessment_threshold": 0.7,
                "enable_automatic_retry": True
            }
        }
        
        logger.info("âš™ï¸ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì • ì™„ë£Œ")
    
    def _setup_model_config(self):
        """ëª¨ë¸ ì„¤ì • êµ¬ì„±"""
        base_config = {
            "device": self.device,
            "dtype": "float16" if self.device in ["mps", "cuda"] else "float32",
            "batch_size": self.get_recommended_batch_size(),
            "memory_fraction": self.get_memory_fraction(),
            "optimization_level": self.optimization_level
        }
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if self.is_m3_max and self.device == "mps":
            base_config.update({
                "use_neural_engine": True,
                "metal_performance_shaders": True,
                "unified_memory_optimization": True,
                "high_resolution_processing": True,
                "concurrent_pipeline_steps": 3,  # 3ë‹¨ê³„ ë™ì‹œ ì²˜ë¦¬
                "memory_pool_size_gb": 32,  # 128GB ì¤‘ 32GB í• ë‹¹
                "model_cache_size_gb": 16,  # ëª¨ë¸ ìºì‹±ìš©
                "intermediate_cache_gb": 8   # ì¤‘ê°„ ê²°ê³¼ ìºì‹±
            })
            logger.info("ğŸ M3 Max íŠ¹í™” ëª¨ë¸ ì„¤ì • ì ìš©")
        
        self.model_config = base_config
        logger.info(f"âš™ï¸ ëª¨ë¸ ì„¤ì • ì™„ë£Œ: ë°°ì¹˜={base_config['batch_size']}, ì •ë°€ë„={base_config['dtype']}")
    
    def _collect_device_info(self):
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        try:
            import psutil
            
            base_info = {
                "device": self.device,
                "device_name": self.device_name,
                "device_type": self.device_type,
                "platform": platform.system(),
                "architecture": platform.machine(),
                "pytorch_version": torch.__version__,
                "python_version": platform.python_version(),
                "optimization_level": self.optimization_level
            }
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            memory = psutil.virtual_memory()
            base_info.update({
                "total_memory_gb": memory.total / (1024**3),
                "available_memory_gb": memory.available / (1024**3),
                "memory_usage_percent": memory.percent
            })
            
            # M3 Max íŠ¹í™” ì •ë³´
            if self.is_m3_max:
                base_info.update({
                    "is_m3_max": True,
                    "neural_engine_available": True,
                    "neural_engine_tops": "15.8 TOPS",
                    "gpu_cores": "30-40 cores",
                    "memory_bandwidth": "400GB/s",
                    "unified_memory": True,
                    "metal_performance_shaders": True,
                    "optimized_for_pipeline": "8-step virtual fitting"
                })
            
            # MPS íŠ¹í™” ì •ë³´
            if self.device == "mps":
                base_info.update({
                    "mps_available": True,
                    "mps_fallback_enabled": True,
                    "metal_api_available": True
                })
            
            # CUDA ì •ë³´
            elif self.device == "cuda":
                if torch.cuda.is_available():
                    base_info.update({
                        "cuda_version": torch.version.cuda,
                        "gpu_memory_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3),
                        "compute_capability": torch.cuda.get_device_capability(0)
                    })
            
            self.device_info = base_info
            logger.info(f"â„¹ï¸ ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {self.device_name}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.device_info = {"device": self.device, "error": str(e)}
    
    def _apply_optimizations(self):
        """í™˜ê²½ ìµœì í™” ì ìš©"""
        try:
            # PyTorch ë©€í‹°ìŠ¤ë ˆë”© ì„¤ì •
            num_threads = min(8, os.cpu_count() or 4)
            if self.is_m3_max:
                num_threads = min(12, os.cpu_count() or 8)  # M3 Max ë” ë§ì€ ìŠ¤ë ˆë“œ
            
            torch.set_num_threads(num_threads)
            
            # MPS ìµœì í™”
            if self.device == "mps":
                # M3 Max íŠ¹í™” MPS ì„¤ì •
                if self.is_m3_max:
                    # Metal Performance Shaders í™œì„±í™”
                    os.environ['METAL_PERFORMANCE_SHADERS_ENABLED'] = '1'
                    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                    
                logger.info("âœ… MPS ìµœì í™” ì ìš© ì™„ë£Œ")
            
            # CUDA ìµœì í™”
            elif self.device == "cuda":
                torch.backends.cudnn.enabled = True
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                logger.info("âœ… CUDA ìµœì í™” ì ìš© ì™„ë£Œ")
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
            gc.collect()
            
            logger.info(f"âœ… í™˜ê²½ ìµœì í™” ì™„ë£Œ (ìŠ¤ë ˆë“œ: {num_threads})")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
    
    def _fallback_cpu_setup(self):
        """CPU í´ë°± ì„¤ì •"""
        self.device = "cpu"
        self.device_type = "cpu"
        self.device_name = "CPU (Fallback)"
        self.is_m3_max = False
        self.optimization_level = "safe"
        
        self.model_config = {
            "device": "cpu",
            "dtype": "float32",
            "batch_size": 1,
            "memory_fraction": 0.5,
            "optimization_level": "safe"
        }
        
        self.device_info = {
            "device": "cpu",
            "device_name": "CPU (Fallback)",
            "error": "GPU initialization failed"
        }
        
        logger.warning("ğŸš¨ CPU í´ë°± ëª¨ë“œë¡œ ì„¤ì •ë¨")
    
    # ==========================================
    # ì ‘ê·¼ì ë©”ì„œë“œë“¤
    # ==========================================
    
    def get_device(self) -> str:
        """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self.device
    
    def get_device_name(self) -> str:
        """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
        return self.device_name
    
    def get_device_type(self) -> str:
        """ë””ë°”ì´ìŠ¤ íƒ€ì… ë°˜í™˜"""
        return self.device_type
    
    def get_recommended_batch_size(self) -> int:
        """ê¶Œì¥ ë°°ì¹˜ í¬ê¸° ë°˜í™˜"""
        if self.is_m3_max:
            return 4 if self.device == "mps" else 2
        elif self.device == "cuda":
            return 2
        else:
            return 1
    
    def get_recommended_precision(self) -> str:
        """ê¶Œì¥ ì •ë°€ë„ ë°˜í™˜"""
        if self.device in ["mps", "cuda"]:
            return "float16"
        return "float32"
    
    def get_memory_fraction(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„ìœ¨ ë°˜í™˜"""
        if self.is_m3_max:
            return 0.6  # 128GBëŠ” ì—¬ìœ ìˆê²Œ
        elif self.device == "mps":
            return 0.7
        elif self.device == "cuda":
            return 0.8
        else:
            return 0.5
    
    def setup_multiprocessing(self) -> int:
        """ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ ìˆ˜ ì„¤ì •"""
        if self.is_m3_max:
            return min(8, os.cpu_count() or 4)
        else:
            return min(4, os.cpu_count() or 2)
    
    def get_device_config(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
        return GPUConfig(
            device=self.device,
            device_name=self.device_name,
            device_type=self.device_type,
            memory_gb=self.memory_gb,
            is_m3_max=self.is_m3_max,
            optimization_level=self.optimization_level,
            neural_engine_available=self.neural_engine_available,
            metal_performance_shaders=self.metal_performance_shaders,
            unified_memory_optimization=self.is_m3_max
        ).__dict__
    
    def get_model_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
        return self.model_config.copy()
    
    def get_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return self.device_info.copy()
    
    def get_pipeline_config(self, step_name: str) -> Dict[str, Any]:
        """íŠ¹ì • íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        return self.pipeline_optimizations.get(step_name, {})
    
    def get_all_pipeline_configs(self) -> Dict[str, Any]:
        """ëª¨ë“  íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        return self.pipeline_optimizations.copy()

# ==========================================
# ì„±ëŠ¥ ë° ìµœì í™” í•¨ìˆ˜ë“¤
# ==========================================

def get_optimal_settings() -> Dict[str, Any]:
    """ìµœì  ì„¤ì • ë°˜í™˜"""
    config = get_gpu_config()
    
    optimal_settings = {
        "device": config.device,
        "device_name": config.device_name,
        "batch_size": config.get_recommended_batch_size(),
        "precision": config.get_recommended_precision(),
        "memory_fraction": config.get_memory_fraction(),
        "max_workers": config.setup_multiprocessing(),
        "optimization_level": config.optimization_level,
        "is_m3_max": config.is_m3_max,
        "memory_gb": config.memory_gb
    }
    
    # M3 Max íŠ¹í™” ì„¤ì • ì¶”ê°€
    if config.is_m3_max:
        optimal_settings.update({
            "neural_engine_enabled": True,
            "mps_optimization": True,
            "metal_performance_shaders": True,
            "memory_bandwidth": "400GB/s",
            "concurrent_sessions": 8,
            "cache_size_gb": 16,
            "pipeline_parallel_steps": 3
        })
    
    return optimal_settings

def get_device_capabilities() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ë°˜í™˜"""
    config = get_gpu_config()
    
    capabilities = {
        "supports_fp16": config.device != "cpu",
        "supports_int8": True,
        "supports_compilation": config.device in ["cuda", "cpu"],
        "supports_parallel_inference": True,
        "max_batch_size": config.get_recommended_batch_size() * 2,
        "recommended_image_size": (512, 512) if config.is_m3_max else (256, 256),
        "supports_8step_pipeline": True
    }
    
    if config.device == "mps":
        capabilities.update({
            "supports_neural_engine": config.is_m3_max,
            "supports_metal_shaders": True,
            "mps_fallback_enabled": True,
            "unified_memory_optimization": config.is_m3_max
        })
    elif config.device == "cuda":
        capabilities.update({
            "cuda_version": torch.version.cuda if hasattr(torch.version, 'cuda') else None,
            "cudnn_enabled": torch.backends.cudnn.enabled,
            "tensor_cores_available": True
        })
    
    return capabilities

def optimize_for_inference() -> Dict[str, Any]:
    """ì¶”ë¡  ìµœì í™” ì„¤ì •"""
    config = get_gpu_config()
    
    inference_settings = {
        "torch_compile": config.device in ["cuda", "cpu"],
        "channels_last": config.device == "cuda",
        "mixed_precision": config.device in ["cuda", "mps"],
        "gradient_checkpointing": False,
        "enable_cudnn_benchmark": config.device == "cuda",
        "deterministic": False,
        "memory_efficient": True,
        "pipeline_optimization": True
    }
    
    # M3 Max íŠ¹í™” ì¶”ë¡  ìµœì í™”
    if config.is_m3_max:
        inference_settings.update({
            "mps_high_watermark": 0.0,
            "mps_allocator_policy": "garbage_collection",
            "metal_api_validation": False,
            "neural_engine_priority": "high",
            "parallel_pipeline_steps": 3,
            "aggressive_memory_optimization": True
        })
    
    return inference_settings

def apply_optimizations():
    """ìµœì í™” ì„¤ì • ì ìš©"""
    config = get_gpu_config()
    settings = get_optimal_settings()
    
    try:
        # PyTorch í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        if config.device == "mps":
            os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            
            if config.is_m3_max:
                os.environ["PYTORCH_MPS_ALLOCATOR_POLICY"] = "garbage_collection"
                os.environ["METAL_PERFORMANCE_SHADERS_ENABLED"] = "1"
        
        # ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì •
        torch.set_num_threads(settings["max_workers"])
        
        # CUDA ì„¤ì •
        if config.device == "cuda":
            torch.backends.cudnn.enabled = True
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
        
        logger.info("âœ… GPU ìµœì í™” ì„¤ì • ì ìš© ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"GPU ìµœì í™” ì„¤ì • ì ìš© ì‹¤íŒ¨: {e}")
        return False

def get_memory_info() -> Dict[str, float]:
    """ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
    config = get_gpu_config()
    
    if config.device == "cuda" and torch.cuda.is_available():
        return {
            "total_memory": torch.cuda.get_device_properties(0).total_memory / 1024**3,
            "allocated_memory": torch.cuda.memory_allocated() / 1024**3,
            "cached_memory": torch.cuda.memory_reserved() / 1024**3,
            "free_memory": (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3
        }
    else:
        try:
            import psutil
            memory = psutil.virtual_memory()
            return {
                "total_memory": memory.total / 1024**3,
                "available_memory": memory.available / 1024**3,
                "used_memory": memory.used / 1024**3,
                "memory_percent": memory.percent
            }
        except ImportError:
            return {"total_memory": 0.0, "available_memory": 0.0}

def test_device_performance() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    config = get_gpu_config()
    
    try:
        import time
        
        device = torch.device(config.device)
        
        # í…ŒìŠ¤íŠ¸ í…ì„œ ìƒì„±
        x = torch.randn(1000, 1000, device=device)
        y = torch.randn(1000, 1000, device=device)
        
        # í–‰ë ¬ ê³±ì…ˆ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        start_time = time.time()
        for _ in range(100):
            z = torch.mm(x, y)
        
        if device.type == "mps":
            torch.mps.synchronize()
        elif device.type == "cuda":
            torch.cuda.synchronize()
        
        end_time = time.time()
        execution_time = end_time - start_time
        performance_score = 100 / execution_time
        
        return {
            "device": config.device,
            "device_name": config.device_name,
            "is_m3_max": config.is_m3_max,
            "performance_score": performance_score,
            "execution_time": execution_time,
            "operations_per_second": 100 / execution_time,
            "test_passed": True,
            "optimization_level": config.optimization_level
        }
        
    except Exception as e:
        logger.error(f"ë””ë°”ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {
            "device": config.device,
            "performance_score": 0.0,
            "execution_time": float('inf'),
            "operations_per_second": 0.0,
            "test_passed": False,
            "error": str(e)
        }

# ==========================================
# ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤
# ==========================================

def optimize_memory(device: Optional[str] = None, aggressive: bool = False) -> Dict[str, Any]:
    """M3 Max ìµœì í™”ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬"""
    try:
        import psutil
        
        current_device = device or gpu_config.device
        start_memory = psutil.virtual_memory().percent
        
        # ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        result = {
            "success": True,
            "device": current_device,
            "start_memory_percent": start_memory,
            "method": "standard_gc"
        }
        
        if current_device == "mps":
            # M3 Max MPS ë©”ëª¨ë¦¬ ìµœì í™”
            try:
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                    result["method"] = "mps_empty_cache"
                elif hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()
                    result["method"] = "mps_synchronize"
                
                if aggressive and gpu_config.is_m3_max:
                    # M3 Max íŠ¹í™” ì ê·¹ì  ì •ë¦¬
                    torch.mps.synchronize()
                    gc.collect()
                    result["method"] = "m3_max_aggressive_cleanup"
                    result["aggressive"] = True
                
            except Exception as mps_error:
                logger.warning(f"MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
                result["mps_error"] = str(mps_error)
        
        elif current_device == "cuda":
            try:
                torch.cuda.empty_cache()
                if aggressive:
                    torch.cuda.synchronize()
                result["method"] = "cuda_empty_cache"
                if aggressive:
                    result["aggressive"] = True
            except Exception as cuda_error:
                logger.warning(f"CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {cuda_error}")
                result["cuda_error"] = str(cuda_error)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ìƒíƒœ
        end_memory = psutil.virtual_memory().percent
        memory_freed = start_memory - end_memory
        
        result.update({
            "end_memory_percent": end_memory,
            "memory_freed_percent": memory_freed,
            "m3_max_optimized": gpu_config.is_m3_max
        })
        
        if memory_freed > 0:
            logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ {memory_freed:.1f}% ì •ë¦¬ë¨ ({result['method']})")
        
        return result
        
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "device": device or "unknown",
            "method": "failed"
        }
# backend/app/core/gpu_config.pyì— ì¶”ê°€í•  ëˆ„ë½ëœ í•¨ìˆ˜ë“¤

def check_memory_available(required_gb: float = 4.0) -> bool:
    """
    M3 Max ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    
    Args:
        required_gb: í•„ìš”í•œ ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)
    
    Returns:
        bool: ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    """
    import psutil
    import torch
    
    try:
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        
        logger.info(f"ğŸ’¾ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory.total / (1024**3):.1f}GB")
        logger.info(f"ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥: {available_gb:.1f}GB")
        logger.info(f"ğŸ’¾ ìš”êµ¬ì‚¬í•­: {required_gb:.1f}GB")
        
        # MPS ë©”ëª¨ë¦¬ í™•ì¸ (M3 Max)
        if torch.backends.mps.is_available():
            # MPSëŠ” unified memory ì‚¬ìš©
            logger.info("ğŸ M3 Max Unified Memory ì‚¬ìš© ì¤‘")
            return available_gb >= required_gb
        
        return available_gb >= required_gb
        
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # ì•ˆì „í•˜ê²Œ True ë°˜í™˜

def get_device_config() -> Dict[str, Any]:
    """
    M3 Max ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜
    
    Returns:
        Dict: ë””ë°”ì´ìŠ¤ ì„¤ì • ì •ë³´
    """
    import platform
    import torch
    
    config = {
        "device_name": "Apple M3 Max",
        "memory_gb": 128,  # M3 Max 128GB ëª¨ë¸
        "is_m3_max": True,
        "optimization_level": "maximum",
        "mps_available": torch.backends.mps.is_available(),
        "system_info": {
            "platform": platform.system(),
            "processor": platform.processor(),
            "machine": platform.machine()
        },
        "recommended_settings": {
            "batch_size": 4,
            "precision": "float16",
            "max_workers": 12,
            "memory_fraction": 0.8
        }
    }
    
    logger.info("ğŸ M3 Max ë””ë°”ì´ìŠ¤ ì„¤ì • ìƒì„±ë¨")
    return config

def initialize_global_memory_manager(device: str = "mps", memory_gb: float = 128.0):
    """
    ì „ì—­ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    
    Args:
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        memory_gb: ì´ ë©”ëª¨ë¦¬ ìš©ëŸ‰
    """
    try:
        import gc
        import torch
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        if device == "mps" and torch.backends.mps.is_available():
            # MPS ë©”ëª¨ë¦¬ ì„¤ì •
            logger.info(f"ğŸ M3 Max MPS ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”: {memory_gb}GB")
            
            # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            os.environ["PYTORCH_MPS_LOW_WATERMARK_RATIO"] = "0.0"
            
        logger.info("âœ… ì „ì—­ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# M3Optimizer í´ë˜ìŠ¤ (app/core/m3_optimizer.pyìš©)
class M3Optimizer:
    """
    M3 Max ì „ìš© ìµœì í™” í´ë˜ìŠ¤
    """
    
    def __init__(self, device_name: str, memory_gb: float, is_m3_max: bool, optimization_level: str):
        """
        M3 ìµœì í™” ì´ˆê¸°í™”
        
        Args:
            device_name: ë””ë°”ì´ìŠ¤ ì´ë¦„
            memory_gb: ë©”ëª¨ë¦¬ ìš©ëŸ‰
            is_m3_max: M3 Max ì—¬ë¶€
            optimization_level: ìµœì í™” ë ˆë²¨
        """
        self.device_name = device_name
        self.memory_gb = memory_gb
        self.is_m3_max = is_m3_max
        self.optimization_level = optimization_level
        
        logger.info(f"ğŸ M3Optimizer ì´ˆê¸°í™”: {device_name}, {memory_gb}GB, {optimization_level}")
        
        if is_m3_max:
            self._apply_m3_max_optimizations()
    
    def _apply_m3_max_optimizations(self):
        """M3 Max ì „ìš© ìµœì í™” ì ìš©"""
        try:
            import torch
            
            # Neural Engine í™œì„±í™”
            if torch.backends.mps.is_available():
                logger.info("ğŸ§  Neural Engine ìµœì í™” í™œì„±í™”")
                
                # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”
                self.pipeline_config = {
                    "stages": 8,
                    "parallel_processing": True,
                    "batch_optimization": True,
                    "memory_pooling": True
                }
                
                logger.info("âš™ï¸ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def optimize_model(self, model):
        """ëª¨ë¸ ìµœì í™”"""
        if not self.is_m3_max:
            return model
            
        try:
            import torch
            
            if hasattr(model, 'to'):
                model = model.to('mps')
                
            # ì¶”ê°€ ìµœì í™” ë¡œì§
            logger.info("âœ… ëª¨ë¸ M3 Max ìµœì í™” ì™„ë£Œ")
            return model
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model

# ModelFormat í´ë˜ìŠ¤ (app/ai_pipeline/utils/model_loader.pyìš©)  
class ModelFormat:
    """AI ëª¨ë¸ í˜•ì‹ ì •ì˜"""
    
    PYTORCH = "pytorch"
    ONNX = "onnx"
    TENSORRT = "tensorrt"
    COREML = "coreml"  # Apple Core ML for M3 Max
    
    @classmethod
    def get_optimized_format(cls, device: str = "mps") -> str:
        """ë””ë°”ì´ìŠ¤ì— ìµœì í™”ëœ ëª¨ë¸ í˜•ì‹ ë°˜í™˜"""
        if device == "mps":
            return cls.COREML  # M3 Maxì—ì„œëŠ” Core ML ì¶”ì²œ
        return cls.PYTORCH

def initialize_global_model_loader(device: str = "mps"):
    """ì „ì—­ ëª¨ë¸ ë¡œë” ì´ˆê¸°í™”"""
    try:
        from .model_loader import ModelLoader
        
        # ê¸€ë¡œë²Œ ëª¨ë¸ ë¡œë” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        global_loader = ModelLoader(device=device)
        
        logger.info(f"âœ… ì „ì—­ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ: {device}")
        return global_loader
        
    except Exception as e:
        logger.error(f"âŒ ì „ì—­ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None
def get_memory_status() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    try:
        import psutil
        
        memory = psutil.virtual_memory()
        
        status = {
            "total_gb": memory.total / (1024**3),
            "available_gb": memory.available / (1024**3),
            "used_gb": memory.used / (1024**3),
            "usage_percent": memory.percent,
            "status": "good",
            "is_m3_max": gpu_config.is_m3_max if 'gpu_config' in globals() else False
        }
        
        # M3 Max íŠ¹í™” ìƒíƒœ íŒì •
        if hasattr(gpu_config, 'is_m3_max') and gpu_config.is_m3_max:
            if memory.percent < 40:
                status["status"] = "excellent"
            elif memory.percent < 70:
                status["status"] = "good"
            elif memory.percent < 85:
                status["status"] = "moderate"
            else:
                status["status"] = "high"
        else:
            if memory.percent < 70:
                status["status"] = "good"
            elif memory.percent < 85:
                status["status"] = "moderate"
            else:
                status["status"] = "high"
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€
        if hasattr(gpu_config, 'device') and gpu_config.device == "cuda":
            try:
                status.update({
                    "gpu_memory_allocated_gb": torch.cuda.memory_allocated() / (1024**3),
                    "gpu_memory_reserved_gb": torch.cuda.memory_reserved() / (1024**3)
                })
            except:
                pass
        
        return status
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

def check_device_compatibility() -> Dict[str, bool]:
    """ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í™•ì¸"""
    return {
        "mps_available": torch.backends.mps.is_available(),
        "cuda_available": torch.cuda.is_available(),
        "m3_max_detected": getattr(gpu_config, 'is_m3_max', False) if 'gpu_config' in globals() else False,
        "neural_engine_available": (
            torch.backends.mps.is_available() and 
            getattr(gpu_config, 'is_m3_max', False) if 'gpu_config' in globals() else False
        ),
        "8step_pipeline_ready": True
    }

# ==========================================
# ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì „ì—­ ë³€ìˆ˜
# ==========================================

def _initialize_gpu_optimizations():
    """GPU ìµœì í™” ì´ˆê¸°í™”"""
    try:
        apply_optimizations()
        logger.info("ğŸš€ GPU ìµœì í™” ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"GPU ìµœì í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ì „ì—­ GPU ì„¤ì • ë§¤ë‹ˆì € ìƒì„±
gpu_config = M3MaxGPUManager()

# í¸ì˜ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ë“¤
DEVICE = gpu_config.device
DEVICE_NAME = gpu_config.device_name
DEVICE_TYPE = gpu_config.device_type
MODEL_CONFIG = gpu_config.model_config
DEVICE_INFO = gpu_config.device_info
IS_M3_MAX = gpu_config.is_m3_max

# ==========================================
# ì£¼ìš” í•¨ìˆ˜ë“¤ (í˜¸í™˜ì„±)
# ==========================================

@lru_cache(maxsize=1)
def get_gpu_config() -> M3MaxGPUManager:
    """GPU ì„¤ì • ë§¤ë‹ˆì € ë°˜í™˜ (ìºì‹œë¨)"""
    return gpu_config

def configure_gpu() -> str:
    """GPU ì„¤ì • ë° ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return gpu_config.device

def get_optimal_device() -> str:
    """ìµœì  ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return gpu_config.device

def is_m3_max() -> bool:
    """M3 Max ì—¬ë¶€ í™•ì¸"""
    return gpu_config.is_m3_max

def get_device() -> str:
    """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return gpu_config.get_device()

def get_device_config() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_device_config()

def get_model_config() -> Dict[str, Any]:
    """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_model_config()

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
    return gpu_config.get_device_info()

def test_device() -> bool:
    """ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
    result = test_device_performance()
    return result.get("test_passed", False)

def cleanup_gpu_resources():
    """GPU ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    try:
        optimize_memory(aggressive=True)
        logger.info("âœ… GPU ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ GPU ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# í¸ì˜ í•¨ìˆ˜ë“¤
def is_gpu_available() -> bool:
    """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
    return gpu_config.device != "cpu"

def is_m3_max_available() -> bool:
    """M3 Max ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
    return gpu_config.is_m3_max

def get_recommended_settings() -> Dict[str, Any]:
    """ê¶Œì¥ ì„¤ì • ë°˜í™˜"""
    return get_optimal_settings()

def get_device_name() -> str:
    """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
    return gpu_config.device_name

def get_device_type() -> str:
    """ë””ë°”ì´ìŠ¤ íƒ€ì… ë°˜í™˜"""
    return gpu_config.device_type

def get_pipeline_config(step_name: str) -> Dict[str, Any]:
    """íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_pipeline_config(step_name)

def get_all_pipeline_configs() -> Dict[str, Any]:
    """ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_all_pipeline_configs()

# ëª¨ë“ˆ ë¡œë“œì‹œ ìë™ ìµœì í™” ì ìš©
_initialize_gpu_optimizations()

# ì´ˆê¸°í™” ë° ê²€ì¦
if gpu_config.is_initialized:
    test_success = test_device()
    if test_success:
        logger.info("âœ… M3 Max GPU ì„¤ì • ê²€ì¦ ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ GPU ì„¤ì • ê²€ì¦ ì‹¤íŒ¨")

# M3 Max ìƒíƒœ ë¡œê¹…
if gpu_config.is_m3_max:
    logger.info("ğŸ M3 Max 128GB ìµœì í™” í™œì„±í™”:")
    logger.info(f"  - Neural Engine: {'âœ…' if MODEL_CONFIG.get('use_neural_engine') else 'âŒ'}")
    logger.info(f"  - Metal Performance Shaders: {'âœ…' if MODEL_CONFIG.get('metal_performance_shaders') else 'âŒ'}")
    logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {MODEL_CONFIG.get('batch_size', 1)}")
    logger.info(f"  - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”: âœ…")
    logger.info(f"  - ë©”ëª¨ë¦¬ ëŒ€ì—­í­: {DEVICE_INFO.get('memory_bandwidth', 'N/A')}")

# ==========================================
# Export ë¦¬ìŠ¤íŠ¸
# ==========================================

__all__ = [
    # ì£¼ìš” ê°ì²´ë“¤
    'gpu_config', 'DEVICE', 'DEVICE_NAME', 'DEVICE_TYPE', 'MODEL_CONFIG', 'DEVICE_INFO', 'IS_M3_MAX',
    
    # í•µì‹¬ í•¨ìˆ˜ë“¤
    'get_gpu_config', 'configure_gpu', 'get_optimal_device', 'is_m3_max',
    'get_device', 'get_device_config', 'get_model_config', 'get_device_info',
    'test_device', 'cleanup_gpu_resources',
    
    # ìµœì í™” í•¨ìˆ˜ë“¤
    'get_optimal_settings', 'get_device_capabilities', 'optimize_for_inference',
    'apply_optimizations', 'get_memory_info', 'test_device_performance',
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤
    'optimize_memory', 'get_memory_status', 'check_device_compatibility',
    
    # í¸ì˜ í•¨ìˆ˜ë“¤
    'is_gpu_available', 'is_m3_max_available', 'get_recommended_settings',
    'get_device_name', 'get_device_type',
    
    # íŒŒì´í”„ë¼ì¸ íŠ¹í™” í•¨ìˆ˜ë“¤
    'get_pipeline_config', 'get_all_pipeline_configs',
    
    # í´ë˜ìŠ¤ë“¤
    'M3MaxGPUManager', 'GPUConfig'
]