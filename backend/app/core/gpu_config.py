# app/core/gpu_config.py
"""
M3 Max GPU μ„¤μ • λ° μµμ ν™” (PyTorch 2.5.1 νΈν™)
- λ„λ½λ export ν•¨μλ“¤ μ¶”κ°€
- main.pyμ—μ„ μ”κµ¬ν•λ” λ¨λ“  ν•¨μ κµ¬ν„
"""

import os
import logging
import platform
import psutil
import torch
from typing import Dict, Any, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class GPUInfo:
    """GPU μ •λ³΄"""
    device: str
    name: str
    memory_gb: float
    is_m3_max: bool
    pytorch_version: str
    mps_available: bool

class GPUConfig:
    """M3 Max GPU μ„¤μ • λ§¤λ‹μ €"""
    
    def __init__(self):
        self.device = self._detect_optimal_device()
        self.gpu_info = self._get_gpu_info()
        self.config = self._create_config()
        
        # ν•„μ export
        self.DEVICE = self.device
        self.MODEL_CONFIG = self.config
        
        logger.info(f"π€ GPU μ„¤μ • μ΄κΈ°ν™” μ™„λ£: {self.device}")
        self._apply_optimizations()
    
    def _detect_optimal_device(self) -> str:
        """μµμ  λ””λ°”μ΄μ¤ κ°μ§€"""
        # CUDA μ°μ„  ν™•μΈ
        if torch.cuda.is_available():
            device_name = torch.cuda.get_device_name(0)
            logger.info(f"π® CUDA GPU κ°μ§€: {device_name}")
            return "cuda"
        
        # MPS (Apple Silicon) ν™•μΈ
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            logger.info("π Apple Silicon MPS κ°μ§€")
            return "mps"
        
        # CPU ν΄λ°±
        logger.info("π’» CPU λ¨λ“ μ‚¬μ©")
        return "cpu"
    
    def _get_gpu_info(self) -> GPUInfo:
        """GPU μ •λ³΄ μμ§‘"""
        system_info = platform.uname()
        total_memory = psutil.virtual_memory().total / (1024**3)
        
        # M3 Max κ°μ§€ (λ©”λ¨λ¦¬ ν¬κΈ° + ARM64 μ•„ν‚¤ν…μ²)
        is_m3_max = (
            system_info.system == "Darwin" and 
            system_info.machine == "arm64" and 
            total_memory >= 32.0  # M3 Maxλ” 36GB+ ν†µν•© λ©”λ¨λ¦¬
        )
        
        if self.device == "cuda":
            name = torch.cuda.get_device_name(0)
            memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        elif self.device == "mps":
            name = "Apple Silicon MPS"
            if is_m3_max:
                name = "Apple M3 Max (MPS)"
            memory_gb = total_memory  # MPSλ” ν†µν•© λ©”λ¨λ¦¬ μ‚¬μ©
        else:
            name = "CPU"
            memory_gb = total_memory
        
        return GPUInfo(
            device=self.device,
            name=name,
            memory_gb=memory_gb,
            is_m3_max=is_m3_max,
            pytorch_version=torch.__version__,
            mps_available=hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
        )
    
    def _create_config(self) -> Dict[str, Any]:
        """λ””λ°”μ΄μ¤λ³„ μµμ  μ„¤μ • μƒμ„±"""
        base_config = {
            "device": self.device,
            "memory_fraction": 0.8,
            "enable_attention_slicing": True,
            "mixed_precision": True,
        }
        
        if self.device == "cuda":
            base_config.update({
                "dtype": torch.float16,
                "enable_memory_efficient_attention": True,
                "enable_xformers": True,
                "enable_cpu_offload": False,
            })
        
        elif self.device == "mps":
            # M3 Max νΉν™” μ„¤μ •
            if self.gpu_info.is_m3_max:
                base_config.update({
                    "dtype": torch.float32,  # MPSλ” float32 κ¶μ¥
                    "memory_fraction": 0.85,  # M3 Maxλ” ν†µν•© λ©”λ¨λ¦¬λ΅ λ” λ†’κ²
                    "enable_memory_efficient_attention": False,  # MPS νΈν™μ„±
                    "enable_cpu_offload": False,
                    "batch_size_multiplier": 1.5,  # M3 Max μ„±λ¥ ν™μ©
                })
            else:
                base_config.update({
                    "dtype": torch.float32,
                    "memory_fraction": 0.7,
                    "enable_memory_efficient_attention": False,
                    "enable_cpu_offload": True,
                })
        
        else:  # CPU
            base_config.update({
                "dtype": torch.float32,
                "enable_memory_efficient_attention": False,
                "enable_cpu_offload": True,
                "num_threads": min(psutil.cpu_count(logical=False), 8),
            })
        
        return base_config
    
    def _apply_optimizations(self) -> None:
        """λ””λ°”μ΄μ¤λ³„ μµμ ν™” μ μ©"""
        try:
            if self.device == "cuda":
                self._optimize_cuda()
            elif self.device == "mps":
                self._optimize_mps()
            else:
                self._optimize_cpu()
            
            logger.info(f"β… {self.device.upper()} μµμ ν™” μ μ© μ™„λ£")
            
        except Exception as e:
            logger.error(f"β μµμ ν™” μ μ© μ‹¤ν¨: {e}")
    
    def _optimize_cuda(self) -> None:
        """CUDA μµμ ν™”"""
        torch.backends.cudnn.enabled = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
        # λ©”λ¨λ¦¬ κ΄€λ¦¬
        if hasattr(torch.cuda, 'empty_cache'):
            torch.cuda.empty_cache()
        
        logger.info("π® CUDA μµμ ν™” μ™„λ£")
    
    def _optimize_mps(self) -> None:
        """MPS (Apple Silicon) μµμ ν™”"""
        try:
            # ν™κ²½λ³€μ μ„¤μ •
            optimization_env = {
                "PYTORCH_MPS_HIGH_WATERMARK_RATIO": "0.0",
            }
            
            if self.gpu_info.is_m3_max:
                # M3 Max νΉν™” μµμ ν™”
                optimization_env.update({
                    "PYTORCH_MPS_ALLOCATOR_POLICY": "garbage_collection",
                    "OMP_NUM_THREADS": "8",  # M3 Max μ„±λ¥ μ½”μ–΄ μ
                    "MKL_NUM_THREADS": "8",
                })
                logger.info("π M3 Max νΉν™” μµμ ν™” μ μ©")
            
            os.environ.update(optimization_env)
            
            # PyTorch 2.5.1 νΈν™μ„± μ²΄ν¬
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
                logger.info("β… MPS μΊμ‹ μ •λ¦¬ μ™„λ£")
            else:
                logger.info("β„ΉοΈ MPS empty_cache λ―Έμ§€μ› - λ€μ²΄ λ©”λ¨λ¦¬ κ΄€λ¦¬ μ‚¬μ©")
            
            logger.info("π MPS μµμ ν™” μ™„λ£")
            
        except Exception as e:
            logger.warning(f"β οΈ MPS μµμ ν™” μ‹¤ν¨: {e}")
    
    def _optimize_cpu(self) -> None:
        """CPU μµμ ν™”"""
        try:
            # μ¤λ λ“ μ μµμ ν™”
            num_threads = min(psutil.cpu_count(logical=False), 8)
            torch.set_num_threads(num_threads)
            
            # Intel MKL μµμ ν™” (Intel CPU)
            os.environ.update({
                "OMP_NUM_THREADS": str(num_threads),
                "MKL_NUM_THREADS": str(num_threads),
                "NUMBA_NUM_THREADS": str(num_threads),
            })
            
            logger.info(f"π’» CPU μµμ ν™” μ™„λ£ (μ¤λ λ“: {num_threads})")
            
        except Exception as e:
            logger.warning(f"β οΈ CPU μµμ ν™” μ‹¤ν¨: {e}")
    
    def get_device_info(self) -> Dict[str, Any]:
        """λ””λ°”μ΄μ¤ μ •λ³΄ λ°ν™"""
        return {
            "device": self.gpu_info.device,
            "name": self.gpu_info.name,
            "memory_gb": round(self.gpu_info.memory_gb, 1),
            "is_m3_max": self.gpu_info.is_m3_max,
            "pytorch_version": self.gpu_info.pytorch_version,
            "mps_available": self.gpu_info.mps_available,
            "config": self.config
        }
    
    def test_device(self) -> bool:
        """λ””λ°”μ΄μ¤ ν…μ¤νΈ"""
        try:
            # κ°„λ‹¨ν• ν…μ„ μ—°μ‚° ν…μ¤νΈ
            test_tensor = torch.randn(1, 100, device=self.device)
            result = torch.nn.functional.relu(test_tensor)
            
            logger.info(f"β… {self.device} λ””λ°”μ΄μ¤ ν…μ¤νΈ μ„±κ³µ")
            return True
            
        except Exception as e:
            logger.error(f"β {self.device} λ””λ°”μ΄μ¤ ν…μ¤νΈ μ‹¤ν¨: {e}")
            return False
    
    def get_optimal_settings(self, model_type: str = "diffusion") -> Dict[str, Any]:
        """μµμ  μ„¤μ • λ°ν™ (λ„λ½λ ν•¨μ μ¶”κ°€)"""
        base_settings = self.config.copy()
        
        # λ¨λΈ νƒ€μ…λ³„ νΉν™” μ„¤μ •
        if model_type == "diffusion":
            if self.device == "mps" and self.gpu_info.is_m3_max:
                base_settings.update({
                    "batch_size": 2,
                    "num_inference_steps": 20,
                    "guidance_scale": 7.5,
                    "enable_attention_slicing": True
                })
            elif self.device == "cuda":
                base_settings.update({
                    "batch_size": 4,
                    "num_inference_steps": 50,
                    "guidance_scale": 7.5,
                    "enable_xformers": True
                })
            else:
                base_settings.update({
                    "batch_size": 1,
                    "num_inference_steps": 20,
                    "guidance_scale": 7.5
                })
        
        elif model_type == "segmentation":
            base_settings.update({
                "input_size": (512, 512),
                "confidence_threshold": 0.5,
                "use_tta": self.device != "cpu"
            })
        
        elif model_type == "pose_estimation":
            base_settings.update({
                "input_size": (368, 368),
                "confidence_threshold": 0.1,
                "use_tensorrt": self.device == "cuda"
            })
        
        return base_settings

# μ „μ—­ μ„¤μ • μ΄κΈ°ν™”
def initialize_gpu_config():
    """GPU μ„¤μ • μ΄κΈ°ν™” λ° μ „μ—­ λ³€μ μ„¤μ •"""
    global gpu_config, DEVICE, MODEL_CONFIG
    
    try:
        gpu_config = GPUConfig()
        DEVICE = gpu_config.DEVICE
        MODEL_CONFIG = gpu_config.MODEL_CONFIG
        
        logger.info(f"π€ GPU μ„¤μ • μ΄κΈ°ν™” μ™„λ£: {DEVICE}")
        
        # λ””λ°”μ΄μ¤ ν…μ¤νΈ
        if gpu_config.test_device():
            logger.info("β… GPU μ„¤μ • κ²€μ¦ μ™„λ£")
        else:
            logger.error("β GPU μ„¤μ • κ²€μ¦ μ‹¤ν¨")
            
        return gpu_config
        
    except Exception as e:
        logger.error(f"β GPU μ„¤μ • μ΄κΈ°ν™” μ‹¤ν¨: {e}")
        
        # μ•μ „ν• ν΄λ°± μ„¤μ •
        DEVICE = "cpu"
        MODEL_CONFIG = {
            "device": "cpu",
            "dtype": torch.float32,
            "enable_memory_efficient_attention": False,
        }
        return None

# μ΄κΈ°ν™” μ‹¤ν–‰
gpu_config = initialize_gpu_config()

# ν•μ„ νΈν™μ„±μ„ μ„ν• μ¶”κ°€ exports
if gpu_config:
    DEVICE = gpu_config.DEVICE
    MODEL_CONFIG = gpu_config.MODEL_CONFIG
    DEVICE_INFO = gpu_config.get_device_info()
else:
    DEVICE = "cpu"
    MODEL_CONFIG = {"device": "cpu", "dtype": torch.float32}
    DEVICE_INFO = {
        "device": "cpu",
        "name": "CPU",
        "memory_gb": 0,
        "is_m3_max": False,
        "pytorch_version": torch.__version__,
        "mps_available": False
    }

# ==========================================
# MAIN.PYμ—μ„ μ”κµ¬ν•λ” EXPORT ν•¨μλ“¤ μ¶”κ°€
# ==========================================

def get_device_config():
    """λ””λ°”μ΄μ¤ μ„¤μ • λ°ν™ (νΈν™μ„±)"""
    return {
        "device": DEVICE,
        "model_config": MODEL_CONFIG,
        "device_info": DEVICE_INFO
    }

def get_device():
    """λ””λ°”μ΄μ¤ μ •λ³΄ λ°ν™ (νΈν™μ„±)"""
    return DEVICE

def get_model_config():
    """λ¨λΈ μ„¤μ • λ°ν™ (νΈν™μ„±)"""
    return MODEL_CONFIG

def get_device_info():
    """λ””λ°”μ΄μ¤ μ •λ³΄ λ°ν™ (νΈν™μ„±)"""
    return DEVICE_INFO

def get_optimal_settings(model_type: str = "diffusion") -> Dict[str, Any]:
    """μµμ  μ„¤μ • λ°ν™ (main.pyμ—μ„ λ„λ½λ ν•¨μ)"""
    if gpu_config and hasattr(gpu_config, 'get_optimal_settings'):
        return gpu_config.get_optimal_settings(model_type)
    else:
        # ν΄λ°± μ„¤μ •
        return {
            "device": DEVICE,
            "dtype": torch.float32,
            "batch_size": 1,
            "memory_fraction": 0.8,
            "enable_optimization": True
        }

def set_device_optimization(enable: bool = True) -> bool:
    """λ””λ°”μ΄μ¤ μµμ ν™” μ„¤μ •/ν•΄μ """
    try:
        if gpu_config:
            if enable:
                gpu_config._apply_optimizations()
            logger.info(f"π”§ λ””λ°”μ΄μ¤ μµμ ν™” {'ν™μ„±ν™”' if enable else 'λΉ„ν™μ„±ν™”'}")
            return True
        else:
            logger.warning("GPU μ„¤μ •μ΄ μ΄κΈ°ν™”λμ§€ μ•μ")
            return False
    except Exception as e:
        logger.error(f"λ””λ°”μ΄μ¤ μµμ ν™” μ„¤μ • μ‹¤ν¨: {e}")
        return False

def get_performance_info() -> Dict[str, Any]:
    """μ„±λ¥ μ •λ³΄ λ°ν™"""
    if gpu_config:
        info = gpu_config.get_device_info()
        info.update({
            "optimization_applied": True,
            "pytorch_version": torch.__version__,
            "performance_tips": _get_performance_tips()
        })
        return info
    else:
        return {
            "device": "cpu",
            "optimization_applied": False,
            "performance_tips": ["GPU μ„¤μ •μ„ μ΄κΈ°ν™”ν•μ„Έμ”"]
        }

def _get_performance_tips() -> list:
    """μ„±λ¥ ν–¥μƒ ν λ°ν™"""
    tips = []
    
    if DEVICE == "mps":
        tips.extend([
            "MPS μ‚¬μ© μ¤‘ - M3 Maxμ— μµμ ν™”λ¨",
            "ν†µν•© λ©”λ¨λ¦¬μ μ¥μ μ„ ν™μ©ν•μ„Έμ”",
            "λ°°μΉ ν¬κΈ°λ¥Ό λλ ¤ μ„±λ¥μ„ ν–¥μƒμ‹ν‚¬ μ μμµλ‹λ‹¤"
        ])
    elif DEVICE == "cuda":
        tips.extend([
            "CUDA κ°€μ† ν™μ„±ν™”λ¨",
            "λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ„ λ¨λ‹ν„°λ§ν•μ„Έμ”",
            "νΌν•© μ •λ°€λ„ μ—°μ‚°μ„ ν™μ©ν•μ„Έμ”"
        ])
    else:
        tips.extend([
            "CPU λ¨λ“ μ‚¬μ© μ¤‘",
            "GPUκ°€ μ‚¬μ© κ°€λ¥ν•μ§€ ν™•μΈν•μ„Έμ”",
            "μ¤λ λ“ μ μµμ ν™”κ°€ μ μ©λ¨"
        ])
    
    return tips