# app/core/gpu_config.py
"""
ÏµúÏ†Å GPU ÏÑ§Ï†ï ÏãúÏä§ÌÖú - ÏßÄÎä•Ï†Å ÎîîÎ∞îÏù¥Ïä§ Í¥ÄÎ¶¨
- ÏûêÎèô ÎîîÎ∞îÏù¥Ïä§ Í∞êÏßÄ Î∞è ÏµúÏ†ÅÌôî
- M3 Max ÌäπÌôî ÏÑ§Ï†ï
- Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî
- ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ
"""
import os
import platform
import subprocess
import logging
import psutil
from typing import Dict, Any, Optional, List, Tuple, Union
from functools import lru_cache
from dataclasses import dataclass
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)

# ===============================================================
# üéØ ÏµúÏ†Å GPU ÏÑ§Ï†ï Î≤†Ïù¥Ïä§ ÌÅ¥ÎûòÏä§
# ===============================================================

@dataclass
class DeviceInfo:
    """ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥ Îç∞Ïù¥ÌÑ∞ ÌÅ¥ÎûòÏä§"""
    device: str
    device_type: str
    name: str
    memory_gb: float
    compute_capability: Optional[str] = None
    driver_version: Optional[str] = None
    is_available: bool = True
    optimization_level: str = 'balanced'
    supports_mixed_precision: bool = False
    supports_dynamic_batching: bool = False

class OptimalGPUConfigBase(ABC):
    """
    üéØ ÏµúÏ†ÅÌôîÎêú GPU ÏÑ§Ï†ï Î≤†Ïù¥Ïä§ ÌÅ¥ÎûòÏä§
    - ÏûêÎèô ÎîîÎ∞îÏù¥Ïä§ Í∞êÏßÄ
    - ÏßÄÎä•Ï†Å ÏµúÏ†ÅÌôî
    - Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨
    - ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ
    """

    def __init__(
        self,
        preferred_device: Optional[str] = None,  # ÏÑ†Ìò∏ ÎîîÎ∞îÏù¥Ïä§
        memory_fraction: float = 0.8,  # Î©îÎ™®Î¶¨ ÏÇ¨Ïö© ÎπÑÏú®
        enable_optimization: bool = True,  # ÏµúÏ†ÅÌôî ÌôúÏÑ±Ìôî
        **kwargs  # ÌôïÏû• ÌååÎùºÎØ∏ÌÑ∞
    ):
        """
        ‚úÖ ÏµúÏ†Å GPU ÏÑ§Ï†ï ÏÉùÏÑ±Ïûê

        Args:
            preferred_device: ÏÑ†Ìò∏ÌïòÎäî ÎîîÎ∞îÏù¥Ïä§ (None=ÏûêÎèôÍ∞êÏßÄ)
            memory_fraction: GPU Î©îÎ™®Î¶¨ ÏÇ¨Ïö© ÎπÑÏú® (0.1~1.0)
            enable_optimization: ÏµúÏ†ÅÌôî ÌôúÏÑ±Ìôî Ïó¨Î∂Ä
            **kwargs: ÌôïÏû• ÌååÎùºÎØ∏ÌÑ∞Îì§
                - force_cpu: bool = False
                - mixed_precision: bool = auto
                - enable_profiling: bool = False
                - memory_growth: bool = True
                - Í∏∞ÌÉÄ...
        """
        self.preferred_device = preferred_device
        self.memory_fraction = max(0.1, min(1.0, memory_fraction))
        self.enable_optimization = enable_optimization
        self.kwargs = kwargs
        
        # 1. üí° ÏãúÏä§ÌÖú Ï†ïÎ≥¥ ÏàòÏßë
        self.system_info = self._collect_system_info()
        
        # 2. üñ•Ô∏è ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎîîÎ∞îÏù¥Ïä§ Ïä§Ï∫î
        self.available_devices = self._scan_available_devices()
        
        # 3. üéØ ÏµúÏ†Å ÎîîÎ∞îÏù¥Ïä§ ÏÑ†ÌÉù
        self.device_info = self._select_optimal_device()
        
        # 4. ‚öôÔ∏è ÎîîÎ∞îÏù¥Ïä§Î≥Ñ ÏÑ§Ï†ï Ï†ÅÏö©
        self._configure_device()
        
        # 5. üöÄ ÏµúÏ†ÅÌôî Ï†ÅÏö©
        if self.enable_optimization:
            self._apply_optimizations()
        
        # 6. üìä Î™®ÎãàÌÑ∞ÎßÅ ÏÑ§Ï†ï
        self._setup_monitoring()
        
        logger.info(f"üéØ GPU ÏÑ§Ï†ï ÏôÑÎ£å - ÎîîÎ∞îÏù¥Ïä§: {self.device_info.device} ({self.device_info.name})")

    def _collect_system_info(self) -> Dict[str, Any]:
        """üñ•Ô∏è ÏãúÏä§ÌÖú Ï†ïÎ≥¥ ÏàòÏßë"""
        return {
            'platform': platform.system(),
            'machine': platform.machine(),
            'processor': platform.processor(),
            'cpu_count': os.cpu_count() or 4,
            'total_memory_gb': self._get_total_memory_gb(),
            'available_memory_gb': self._get_available_memory_gb(),
            'is_m3_max': self._detect_m3_max(),
            'is_container': self._detect_container(),
            'python_version': platform.python_version()
        }

    def _get_total_memory_gb(self) -> float:
        """Ï¥ù Î©îÎ™®Î¶¨ Ïö©Îüâ (GB)"""
        try:
            return psutil.virtual_memory().total / (1024**3)
        except:
            return 16.0  # Í∏∞Î≥∏Í∞í

    def _get_available_memory_gb(self) -> float:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î©îÎ™®Î¶¨ (GB)"""
        try:
            return psutil.virtual_memory().available / (1024**3)
        except:
            return 8.0  # Í∏∞Î≥∏Í∞í

    def _detect_m3_max(self) -> bool:
        """üçé M3 Max Ïπ© Í∞êÏßÄ"""
        if platform.system() != 'Darwin':
            return False
        
        try:
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                capture_output=True, text=True, timeout=5
            )
            cpu_info = result.stdout.strip()
            return any(chip in cpu_info for chip in ['M3', 'M2', 'M1']) and 'Max' in cpu_info
        except:
            return False

    def _detect_container(self) -> bool:
        """üê≥ Ïª®ÌÖåÏù¥ÎÑà ÌôòÍ≤Ω Í∞êÏßÄ"""
        indicators = [
            os.path.exists('/.dockerenv'),
            os.getenv('KUBERNETES_SERVICE_HOST') is not None,
            os.getenv('CONTAINER') is not None
        ]
        return any(indicators)

    @abstractmethod
    def _scan_available_devices(self) -> List[DeviceInfo]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎîîÎ∞îÏù¥Ïä§ Ïä§Ï∫î (ÏÑúÎ∏åÌÅ¥ÎûòÏä§ÏóêÏÑú Íµ¨ÌòÑ)"""
        pass

    def _select_optimal_device(self) -> DeviceInfo:
        """üéØ ÏµúÏ†Å ÎîîÎ∞îÏù¥Ïä§ ÏÑ†ÌÉù"""
        
        # Í∞ïÏ†ú CPU Î™®Îìú
        if self.kwargs.get('force_cpu', False):
            return self._create_cpu_device_info()
        
        # ÏÑ†Ìò∏ ÎîîÎ∞îÏù¥Ïä§Í∞Ä ÏûàÍ≥† ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Í≤ΩÏö∞
        if self.preferred_device:
            for device in self.available_devices:
                if device.device == self.preferred_device and device.is_available:
                    return device
        
        # ÏûêÎèô ÏÑ†ÌÉù: ÏÑ±Îä• ÏàúÏÑúÎåÄÎ°ú Ï†ïÎ†¨
        if self.available_devices:
            # M3 Max MPS > NVIDIA CUDA > CPU ÏàúÏúºÎ°ú Ïö∞ÏÑ†ÏàúÏúÑ
            priority_order = ['mps', 'cuda', 'cpu']
            
            for device_type in priority_order:
                for device in self.available_devices:
                    if device.device == device_type and device.is_available:
                        return device
        
        # Ìè¥Î∞±: CPU
        return self._create_cpu_device_info()

    def _create_cpu_device_info(self) -> DeviceInfo:
        """CPU ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥ ÏÉùÏÑ±"""
        return DeviceInfo(
            device='cpu',
            device_type='cpu',
            name=f"CPU ({self.system_info['cpu_count']} cores)",
            memory_gb=self.system_info['available_memory_gb'],
            is_available=True,
            optimization_level='basic',
            supports_mixed_precision=False,
            supports_dynamic_batching=True
        )

    @abstractmethod
    def _configure_device(self):
        """ÎîîÎ∞îÏù¥Ïä§Î≥Ñ ÏÑ§Ï†ï (ÏÑúÎ∏åÌÅ¥ÎûòÏä§ÏóêÏÑú Íµ¨ÌòÑ)"""
        pass

    @abstractmethod
    def _apply_optimizations(self):
        """ÏµúÏ†ÅÌôî Ï†ÅÏö© (ÏÑúÎ∏åÌÅ¥ÎûòÏä§ÏóêÏÑú Íµ¨ÌòÑ)"""
        pass

    def _setup_monitoring(self):
        """üìä Î™®ÎãàÌÑ∞ÎßÅ ÏÑ§Ï†ï"""
        self.monitoring_enabled = self.kwargs.get('enable_profiling', False)
        self.memory_stats = {
            'allocated': 0,
            'cached': 0,
            'max_allocated': 0
        }

    # Í≥µÌÜµ Ïú†Ìã∏Î¶¨Ìã∞ Î©îÏÑúÎìúÎì§
    def get_device(self) -> str:
        """ÌòÑÏû¨ ÎîîÎ∞îÏù¥Ïä§ Î∞òÌôò"""
        return self.device_info.device

    def get_device_info(self) -> DeviceInfo:
        """ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥ Î∞òÌôò"""
        return self.device_info

    def get_memory_info(self) -> Dict[str, float]:
        """Î©îÎ™®Î¶¨ Ï†ïÎ≥¥ Î∞òÌôò"""
        return {
            'total_gb': self.device_info.memory_gb,
            'allocated_gb': self.memory_stats['allocated'],
            'available_gb': self.device_info.memory_gb - self.memory_stats['allocated'],
            'memory_fraction': self.memory_fraction
        }

    def optimize_memory(self):
        """Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî"""
        if self.device_info.device == 'cuda':
            self._optimize_cuda_memory()
        elif self.device_info.device == 'mps':
            self._optimize_mps_memory()
        else:
            self._optimize_cpu_memory()

    def _optimize_cuda_memory(self):
        """CUDA Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî"""
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
        except ImportError:
            pass

    def _optimize_mps_memory(self):
        """MPS Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî"""
        try:
            import torch
            if torch.backends.mps.is_available():
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                elif hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()
        except ImportError:
            pass

    def _optimize_cpu_memory(self):
        """CPU Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî"""
        import gc
        gc.collect()

    def check_memory_available(self, required_gb: float = 4.0) -> bool:
        """Î©îÎ™®Î¶¨ Í∞ÄÏö©ÏÑ± Ï≤¥ÌÅ¨"""
        available_gb = self.device_info.memory_gb - self.memory_stats['allocated']
        return available_gb >= required_gb

# ===============================================================
# üéØ PyTorch GPU ÏÑ§Ï†ï ÌÅ¥ÎûòÏä§
# ===============================================================

class PyTorchGPUConfig(OptimalGPUConfigBase):
    """
    üéØ PyTorch Ï†ÑÏö© GPU ÏÑ§Ï†ï
    - PyTorch Î∞±ÏóîÎìú ÏµúÏ†ÅÌôî
    - CUDA/MPS ÏÑ§Ï†ï
    - Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨
    - M3 Max ÌäπÌôî
    """

    def _scan_available_devices(self) -> List[DeviceInfo]:
        """PyTorch ÎîîÎ∞îÏù¥Ïä§ Ïä§Ï∫î"""
        devices = []
        
        try:
            import torch
            
            # MPS (Apple Silicon) ÌôïÏù∏
            if torch.backends.mps.is_available():
                memory_gb = self._estimate_mps_memory()
                devices.append(DeviceInfo(
                    device='mps',
                    device_type='apple_silicon',
                    name=f"Apple Silicon MPS ({memory_gb:.1f}GB)",
                    memory_gb=memory_gb,
                    is_available=True,
                    optimization_level='ultra' if self.system_info['is_m3_max'] else 'high',
                    supports_mixed_precision=True,
                    supports_dynamic_batching=True
                ))
            
            # CUDA ÌôïÏù∏
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    props = torch.cuda.get_device_properties(i)
                    memory_gb = props.total_memory / (1024**3)
                    
                    devices.append(DeviceInfo(
                        device='cuda',
                        device_type='nvidia_gpu',
                        name=f"{props.name} ({memory_gb:.1f}GB)",
                        memory_gb=memory_gb,
                        compute_capability=f"{props.major}.{props.minor}",
                        is_available=True,
                        optimization_level='high',
                        supports_mixed_precision=props.major >= 7,  # Tensor Cores
                        supports_dynamic_batching=True
                    ))
            
        except ImportError:
            logger.warning("PyTorch ÏóÜÏù¥ CPUÎßå ÏÇ¨Ïö© Í∞ÄÎä•")
        
        # CPUÎäî Ìï≠ÏÉÅ Ï∂îÍ∞Ä
        devices.append(self._create_cpu_device_info())
        
        return devices

    def _estimate_mps_memory(self) -> float:
        """MPS Î©îÎ™®Î¶¨ Ï∂îÏ†ï"""
        # Apple SiliconÏùò ÌÜµÌï© Î©îÎ™®Î¶¨ ÏãúÏä§ÌÖú
        # ÏùºÎ∞òÏ†ÅÏúºÎ°ú ÏãúÏä§ÌÖú Î©îÎ™®Î¶¨Ïùò 80% Ï†ïÎèÑÎ•º GPUÍ∞Ä ÌôúÏö© Í∞ÄÎä•
        system_memory = self.system_info['total_memory_gb']
        
        if self.system_info['is_m3_max']:
            # M3 MaxÎäî Îçî ÎßéÏùÄ GPU Î©îÎ™®Î¶¨ ÌôúÏö© Í∞ÄÎä•
            return min(system_memory * 0.85, 128.0)
        else:
            return min(system_memory * 0.75, 64.0)

    def _configure_device(self):
        """PyTorch ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï"""
        try:
            import torch
            
            if self.device_info.device == 'mps':
                self._configure_mps()
            elif self.device_info.device == 'cuda':
                self._configure_cuda()
            else:
                self._configure_cpu()
                
        except ImportError:
            logger.warning("PyTorch ÏÑ§Ï†ï Ïã§Ìå® - ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏóÜÏùå")

    def _configure_mps(self):
        """MPS ÏÑ§Ï†ï"""
        try:
            import torch
            
            # MPS Ìè¥Î∞± ÌôúÏÑ±Ìôî
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            
            # M3 Max ÌäπÌôî ÏÑ§Ï†ï
            if self.system_info['is_m3_max']:
                # Í≥†ÏÑ±Îä• Î™®Îìú
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                # Ïä§Î†àÎìú ÏµúÏ†ÅÌôî (14ÏΩîÏñ¥ M3 Max)
                optimal_threads = min(8, self.system_info['cpu_count'])
                torch.set_num_threads(optimal_threads)
                
                logger.info(f"üçé M3 Max MPS ÏµúÏ†ÅÌôî ÏôÑÎ£å - Ïä§Î†àÎìú: {optimal_threads}")
            
            # Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏµúÏ†ÅÌôî
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
                
        except Exception as e:
            logger.warning(f"MPS ÏÑ§Ï†ï Ïã§Ìå®: {e}")

    def _configure_cuda(self):
        """CUDA ÏÑ§Ï†ï"""
        try:
            import torch
            
            # Î©îÎ™®Î¶¨ fraction ÏÑ§Ï†ï
            if self.memory_fraction < 1.0:
                torch.cuda.set_per_process_memory_fraction(self.memory_fraction)
            
            # Î©îÎ™®Î¶¨ growth ÌôúÏÑ±Ìôî (TensorFlow Ïä§ÌÉÄÏùº)
            if self.kwargs.get('memory_growth', True):
                os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
            
            # CuDNN ÏµúÏ†ÅÌôî
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # Î©ÄÌã∞ GPU ÏßÄÏõê
            if torch.cuda.device_count() > 1:
                logger.info(f"üéÆ Îã§Ï§ë GPU Í∞êÏßÄ: {torch.cuda.device_count()}Í∞ú")
            
            logger.info(f"üéÆ CUDA ÏµúÏ†ÅÌôî ÏôÑÎ£å - Î©îÎ™®Î¶¨ fraction: {self.memory_fraction}")
            
        except Exception as e:
            logger.warning(f"CUDA ÏÑ§Ï†ï Ïã§Ìå®: {e}")

    def _configure_cpu(self):
        """CPU ÏÑ§Ï†ï"""
        try:
            import torch
            
            # CPU Ïä§Î†àÎìú ÏµúÏ†ÅÌôî
            if self.enable_optimization:
                optimal_threads = min(self.system_info['cpu_count'], 8)
                torch.set_num_threads(optimal_threads)
                
                # OpenMP Ïä§Î†àÎìú ÏÑ§Ï†ï
                os.environ['OMP_NUM_THREADS'] = str(optimal_threads)
                os.environ['MKL_NUM_THREADS'] = str(optimal_threads)
                
                logger.info(f"‚ö° CPU ÏµúÏ†ÅÌôî ÏôÑÎ£å - Ïä§Î†àÎìú: {optimal_threads}")
                
        except Exception as e:
            logger.warning(f"CPU ÏÑ§Ï†ï Ïã§Ìå®: {e}")

    def _apply_optimizations(self):
        """PyTorch ÏµúÏ†ÅÌôî Ï†ÅÏö©"""
        try:
            import torch
            
            # Mixed Precision ÏÑ§Ï†ï
            if self.device_info.supports_mixed_precision and self.kwargs.get('mixed_precision', True):
                logger.info("üöÄ Mixed Precision ÌôúÏÑ±Ìôî")
            
            # JIT Ïª¥ÌååÏùº ÏµúÏ†ÅÌôî
            if self.enable_optimization:
                torch.jit.set_fusion_strategy([('STATIC', 2), ('DYNAMIC', 2)])
                
            # Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî
            if self.device_info.device != 'cpu':
                self.optimize_memory()
                
        except Exception as e:
            logger.warning(f"PyTorch ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")

    def get_optimal_batch_size(self, base_size: int = 4) -> int:
        """ÏµúÏ†Å Î∞∞Ïπò ÌÅ¨Í∏∞ Í≥ÑÏÇ∞"""
        if self.device_info.device == 'mps' and self.system_info['is_m3_max']:
            # M3 MaxÎäî Îçî ÌÅ∞ Î∞∞Ïπò Í∞ÄÎä•
            memory_multiplier = self.device_info.memory_gb / 32.0
            return int(base_size * min(memory_multiplier, 4.0))
        elif self.device_info.device == 'cuda':
            # CUDAÎäî Î©îÎ™®Î¶¨Ïóê Îî∞Îùº
            memory_multiplier = self.device_info.memory_gb / 16.0
            return int(base_size * min(memory_multiplier, 2.0))
        else:
            # CPUÎäî Î≥¥ÏàòÏ†ÅÏúºÎ°ú
            return max(1, base_size // 2)

    def get_optimal_workers(self) -> int:
        """ÏµúÏ†Å Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏõåÏª§ Ïàò"""
        if self.device_info.device == 'cpu':
            return min(4, self.system_info['cpu_count'])
        else:
            # GPU ÏÇ¨Ïö©Ïãú CPU ÏΩîÏñ¥Ïùò 25% Ï†ïÎèÑ
            return min(4, max(1, self.system_info['cpu_count'] // 4))

# ===============================================================
# üéØ Ï†ÑÏó≠ GPU ÏÑ§Ï†ï Í¥ÄÎ¶¨Ïûê
# ===============================================================

class GPUConfigManager:
    """
    üéØ Ï†ÑÏó≠ GPU ÏÑ§Ï†ï Í¥ÄÎ¶¨Ïûê
    - Ïã±Í∏ÄÌÜ§ Ìå®ÌÑ¥
    - Ï∫êÏã± ÏßÄÏõê
    - ÎèôÏ†Å Ïû¨ÏÑ§Ï†ï
    """

    _instance = None
    _config = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def get_config(
        self, 
        framework: str = 'pytorch',
        **kwargs
    ) -> OptimalGPUConfigBase:
        """GPU ÏÑ§Ï†ï Î∞òÌôò (Ï∫êÏãúÎê®)"""
        
        cache_key = f"{framework}_{hash(frozenset(kwargs.items()))}"
        
        if self._config is None or getattr(self._config, '_cache_key', None) != cache_key:
            if framework.lower() == 'pytorch':
                self._config = PyTorchGPUConfig(**kwargs)
                self._config._cache_key = cache_key
            else:
                raise ValueError(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌîÑÎ†àÏûÑÏõåÌÅ¨: {framework}")
        
        return self._config

    def reset_config(self):
        """ÏÑ§Ï†ï Ï¥àÍ∏∞Ìôî"""
        self._config = None

    def get_device_summary(self) -> Dict[str, Any]:
        """ÎîîÎ∞îÏù¥Ïä§ ÏöîÏïΩ Ï†ïÎ≥¥"""
        if self._config is None:
            self._config = self.get_config()
        
        return {
            'current_device': self._config.get_device(),
            'device_info': self._config.get_device_info(),
            'memory_info': self._config.get_memory_info(),
            'system_info': self._config.system_info,
            'optimization_enabled': self._config.enable_optimization
        }

# ===============================================================
# üéØ Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞è Ìé∏Ïùò Ìï®ÏàòÎì§
# ===============================================================

# Ï†ÑÏó≠ GPU ÏÑ§Ï†ï Îß§ÎãàÏ†Ä
gpu_config_manager = GPUConfigManager()

@lru_cache()
def get_gpu_config(**kwargs) -> OptimalGPUConfigBase:
    """GPU ÏÑ§Ï†ï Î∞òÌôò (Ï∫êÏãúÎê®)"""
    return gpu_config_manager.get_config(**kwargs)

# Í∏∞Î≥∏ GPU ÏÑ§Ï†ï
gpu_config = get_gpu_config()

# Ìé∏Ïùò ÏÉÅÏàòÎì§ (ÌïòÏúÑ Ìò∏ÌôòÏÑ±)
DEVICE = gpu_config.get_device()
DEVICE_INFO = gpu_config.get_device_info()
DEVICE_TYPE = DEVICE_INFO.device_type
USE_GPU = DEVICE != 'cpu'
IS_M3_MAX = gpu_config.system_info['is_m3_max']
MEMORY_GB = DEVICE_INFO.memory_gb

# Î™®Îç∏ ÏÑ§Ï†ï (ÌïòÏúÑ Ìò∏ÌôòÏÑ±)
MODEL_CONFIG = {
    'device': DEVICE,
    'batch_size': gpu_config.get_optimal_batch_size() if hasattr(gpu_config, 'get_optimal_batch_size') else 4,
    'num_workers': gpu_config.get_optimal_workers() if hasattr(gpu_config, 'get_optimal_workers') else 2,
    'mixed_precision': DEVICE_INFO.supports_mixed_precision,
    'memory_fraction': gpu_config.memory_fraction
}

# Ìé∏Ïùò Ìï®ÏàòÎì§
def get_device() -> str:
    """ÌòÑÏû¨ ÎîîÎ∞îÏù¥Ïä§ Î∞òÌôò"""
    return DEVICE

def get_device_info() -> DeviceInfo:
    """ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥ Î∞òÌôò"""
    return DEVICE_INFO

def get_optimal_settings() -> Dict[str, Any]:
    """ÏµúÏ†Å ÏÑ§Ï†ï Î∞òÌôò"""
    return MODEL_CONFIG

def optimize_memory():
    """Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî"""
    gpu_config.optimize_memory()

def check_memory_available(required_gb: float = 4.0) -> bool:
    """Î©îÎ™®Î¶¨ Í∞ÄÏö©ÏÑ± Ï≤¥ÌÅ¨"""
    return gpu_config.check_memory_available(required_gb)

def get_memory_info() -> Dict[str, float]:
    """Î©îÎ™®Î¶¨ Ï†ïÎ≥¥ Î∞òÌôò"""
    return gpu_config.get_memory_info()

def create_custom_gpu_config(**kwargs) -> OptimalGPUConfigBase:
    """Ïª§Ïä§ÌÖÄ GPU ÏÑ§Ï†ï ÏÉùÏÑ±"""
    return PyTorchGPUConfig(**kwargs)

# M3 Max Ï†ÑÏö© ÏÑ§Ï†ï ÏÉùÏÑ± Ìï®Ïàò
def create_m3_max_config(**kwargs) -> OptimalGPUConfigBase:
    """M3 Max Ï†ÑÏö© ÏµúÏ†ÅÌôî ÏÑ§Ï†ï"""
    return PyTorchGPUConfig(
        preferred_device='mps',
        memory_fraction=0.85,
        enable_optimization=True,
        mixed_precision=True,
        **kwargs
    )

# Í∞úÎ∞úÏö© ÏÑ§Ï†ï ÏÉùÏÑ± Ìï®Ïàò
def create_development_config() -> OptimalGPUConfigBase:
    """Í∞úÎ∞úÏö© GPU ÏÑ§Ï†ï"""
    return PyTorchGPUConfig(
        memory_fraction=0.6,
        enable_optimization=False,
        enable_profiling=True
    )

# ÌîÑÎ°úÎçïÏÖòÏö© ÏÑ§Ï†ï ÏÉùÏÑ± Ìï®Ïàò
def create_production_config() -> OptimalGPUConfigBase:
    """ÌîÑÎ°úÎçïÏÖòÏö© GPU ÏÑ§Ï†ï"""
    return PyTorchGPUConfig(
        memory_fraction=0.9,
        enable_optimization=True,
        mixed_precision=True,
        memory_growth=True
    )

# Ï¥àÍ∏∞Ìôî Î°úÍπÖ
logger.info(f"üéØ GPU ÏÑ§Ï†ï ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
logger.info(f"üíª ÎîîÎ∞îÏù¥Ïä§: {DEVICE} ({DEVICE_INFO.name})")
logger.info(f"üíæ Î©îÎ™®Î¶¨: {MEMORY_GB:.1f}GB")

if IS_M3_MAX:
    logger.info("üçé M3 Max ÏµúÏ†ÅÌôî ÌôúÏÑ±Ìôî")
if USE_GPU:
    logger.info(f"üéÆ GPU Í∞ÄÏÜç ÌôúÏÑ±Ìôî")

# Î©îÎ™®Î¶¨ Ï≤¥ÌÅ¨
if not check_memory_available(4.0):
    logger.warning("‚ö†Ô∏è GPU Î©îÎ™®Î¶¨ Î∂ÄÏ°± - ÏÑ±Îä• Ï†ÄÌïò Í∞ÄÎä•")

__all__ = [
    'OptimalGPUConfigBase',
    'PyTorchGPUConfig', 
    'GPUConfigManager',
    'DeviceInfo',
    'get_gpu_config',
    'gpu_config',
    'gpu_config_manager',
    # Ìé∏Ïùò ÏÉÅÏàòÎì§
    'DEVICE',
    'DEVICE_INFO', 
    'DEVICE_TYPE',
    'USE_GPU',
    'IS_M3_MAX',
    'MEMORY_GB',
    'MODEL_CONFIG',
    # Ìé∏Ïùò Ìï®ÏàòÎì§
    'get_device',
    'get_device_info',
    'get_optimal_settings',
    'optimize_memory',
    'check_memory_available',
    'get_memory_info',
    'create_custom_gpu_config',
    'create_m3_max_config',
    'create_development_config',
    'create_production_config'
]