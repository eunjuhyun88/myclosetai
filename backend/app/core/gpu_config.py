# backend/app/core/gpu_config.py
"""
M3 Max GPU ìµœì í™” ì„¤ì •
Metal Performance Shadersë¥¼ í™œìš©í•œ AI ê°€ì†
"""
import torch
import os
import subprocess
import platform
import logging

logger = logging.getLogger(__name__)

class M3GPUConfig:
    def __init__(self):
        self.device = self._get_optimal_device()
        self.memory_fraction = 0.8  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
        self.setup_optimizations()
        
    def _get_optimal_device(self):
        """ìµœì ì˜ ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        # Apple Silicon ì²´í¬
        if platform.machine() == "arm64" and platform.system() == "Darwin":
            if torch.backends.mps.is_available():
                logger.info("âœ… Apple M3 Max GPU (Metal) ì‚¬ìš© ê°€ëŠ¥")
                return "mps"
            else:
                logger.warning("âš ï¸ MPS ì‚¬ìš© ë¶ˆê°€, CPU ëª¨ë“œë¡œ ì‹¤í–‰")
                return "cpu"
        else:
            # NVIDIA GPU ì²´í¬
            if torch.cuda.is_available():
                logger.info("âœ… NVIDIA GPU (CUDA) ì‚¬ìš© ê°€ëŠ¥")
                return "cuda"
            else:
                logger.info("â„¹ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰")
                return "cpu"
    
    def setup_optimizations(self):
        """ì‹œìŠ¤í…œ ìµœì í™” ì„¤ì •"""
        if self.device == "mps":
            # Metal ìµœì í™” í™˜ê²½ë³€ìˆ˜
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            
        elif self.device == "cuda":
            # CUDA ìµœì í™”
            os.environ['CUDA_VISIBLE_DEVICES'] = '0'
            
        # ê³µí†µ ìµœì í™”
        os.environ['OMP_NUM_THREADS'] = '8'
        os.environ['MKL_NUM_THREADS'] = '8'
        
    def optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            if self.device == "mps":
                # Metal ë©”ëª¨ë¦¬ ì •ë¦¬
                torch.mps.empty_cache()
                logger.info("ğŸ§¹ MPS ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                
            elif self.device == "cuda":
                # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
                torch.cuda.empty_cache()
                logger.info("ğŸ§¹ CUDA ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        
    def get_model_config(self):
        """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
        config = {
            "device": self.device,
            "memory_efficient": True,
            "batch_size": 1,  # M3 Max/CUDA ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°
        }
        
        if self.device == "mps":
            config.update({
                "dtype": torch.float32,  # MPSëŠ” float16 ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
                "max_memory_mb": 24000,  # M3 Max ë©”ëª¨ë¦¬ í•œê³„
            })
        elif self.device == "cuda":
            config.update({
                "dtype": torch.float16,  # CUDAëŠ” float16 ì§€ì›
                "mixed_precision": True,
            })
        else:
            config.update({
                "dtype": torch.float32,
                "max_memory_mb": 8000,   # CPU ë©”ëª¨ë¦¬ ì œí•œ
            })
            
        return config
    
    def get_device_info(self):
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        info = {
            "device": self.device,
            "platform": platform.system(),
            "machine": platform.machine(),
        }
        
        if self.device == "mps":
            info.update({
                "gpu_name": "Apple M3 Max",
                "memory_available": "128GB (í†µí•© ë©”ëª¨ë¦¬)",
                "cores": "30-40 GPU ì½”ì–´",
            })
        elif self.device == "cuda":
            if torch.cuda.is_available():
                info.update({
                    "gpu_name": torch.cuda.get_device_name(0),
                    "memory_available": f"{torch.cuda.get_device_properties(0).total_memory // 1024**3}GB",
                    "cuda_version": torch.version.cuda,
                })
        
        return info
    
    def benchmark_device(self):
        """ë””ë°”ì´ìŠ¤ ë²¤ì¹˜ë§ˆí¬"""
        try:
            logger.info("ğŸ”§ ë””ë°”ì´ìŠ¤ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
            
            device = torch.device(self.device)
            
            # ê°„ë‹¨í•œ ì—°ì‚° í…ŒìŠ¤íŠ¸
            x = torch.randn(1000, 1000, device=device)
            y = torch.randn(1000, 1000, device=device)
            
            import time
            start = time.time()
            for _ in range(100):
                z = torch.mm(x, y)
            end = time.time()
            
            avg_time = (end - start) / 100
            logger.info(f"âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {avg_time:.4f}ì´ˆ/ì—°ì‚°")
            
            return {
                "success": True,
                "avg_operation_time": avg_time,
                "device": self.device
            }
            
        except Exception as e:
            logger.error(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

# ì „ì—­ GPU ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
gpu_config = M3GPUConfig()
DEVICE = gpu_config.device
MODEL_CONFIG = gpu_config.get_model_config()
DEVICE_INFO = gpu_config.get_device_info()

# ì´ˆê¸°í™” ë¡œê·¸
logger.info(f"ğŸ”§ GPU ì„¤ì • ì™„ë£Œ: {DEVICE}")
logger.info(f"ğŸ“Š ë””ë°”ì´ìŠ¤ ì •ë³´: {DEVICE_INFO}")

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (ì„ íƒì )
def run_benchmark():
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    return gpu_config.benchmark_device()