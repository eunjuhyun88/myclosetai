# backend/app/core/gpu_config.py
"""
MyCloset AI - M3 Max GPU ìµœì í™” ì„¤ì •
Apple M3 Max (128GB RAM, 30-40 GPU Core) ì „ìš© Metal Performance Shaders í™œìš©
"""

import torch
import os
import platform
import logging
import gc
import psutil
from typing import Dict, Any, Optional, Tuple
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class M3MaxGPUConfig:
    """M3 Max GPU ìµœì í™” ì„¤ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        """GPU ì„¤ì • ì´ˆê¸°í™”"""
        self.device = self._detect_optimal_device()
        self.memory_fraction = 0.8  # 128GB ì¤‘ 80% í™œìš©
        self.is_m3_max = self._check_m3_max()
        self.setup_optimizations()
        
        # ì´ˆê¸°í™” ë¡œê·¸
        logger.info(f"ğŸ”§ GPU ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ“± ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"ğŸ§  ë©”ëª¨ë¦¬ í• ë‹¹: {self.memory_fraction * 100}%")
        logger.info(f"âš¡ M3 Max ëª¨ë“œ: {self.is_m3_max}")
        
    def _detect_optimal_device(self) -> str:
        """ìµœì ì˜ ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        # Apple Silicon ì²´í¬ (M1/M2/M3)
        if platform.system() == "Darwin" and platform.machine() == "arm64":
            if torch.backends.mps.is_available():
                logger.info("âœ… Apple Silicon GPU (Metal) ê°ì§€ë¨")
                return "mps"
            else:
                logger.warning("âš ï¸ Metal ì‚¬ìš© ë¶ˆê°€, CPU ëª¨ë“œë¡œ ì‹¤í–‰")
                return "cpu"
        
        # NVIDIA GPU ì²´í¬ (ì„œë²„ í™˜ê²½ìš©)
        elif torch.cuda.is_available():
            logger.info("âœ… NVIDIA GPU (CUDA) ê°ì§€ë¨")
            return "cuda"
        
        # CPU í´ë°±
        else:
            logger.info("â„¹ï¸ GPU ì—†ìŒ, CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            return "cpu"
    
    def _check_m3_max(self) -> bool:
        """M3 Max ì¹© í™•ì¸"""
        try:
            # macOSì—ì„œ ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
            if platform.system() == "Darwin":
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                cpu_info = result.stdout.strip()
                
                # M3 Max í™•ì¸ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ê°ì§€ í•„ìš”)
                total_memory = psutil.virtual_memory().total // (1024**3)  # GB
                
                # 128GB RAMì´ë©´ M3 Maxë¡œ ì¶”ì •
                if total_memory >= 120:  # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ëŠ” ì¡°ê¸ˆ ì ìŒ
                    logger.info(f"âœ… M3 Max ê°ì§€ë¨ (RAM: {total_memory}GB)")
                    return True
                else:
                    logger.info(f"â„¹ï¸ M3 Pro/ì¼ë°˜ ê°ì§€ë¨ (RAM: {total_memory}GB)")
                    return False
            return False
            
        except Exception as e:
            logger.warning(f"M3 Max ê°ì§€ ì‹¤íŒ¨: {e}")
            return False
    
    def setup_optimizations(self):
        """ì‹œìŠ¤í…œ ìµœì í™” ì„¤ì •"""
        
        if self.device == "mps":
            # Metal Performance Shaders ìµœì í™”
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
            
            # M3 Max ì „ìš© ìµœì í™”
            if self.is_m3_max:
                os.environ['MPS_FORCE_HEAPS_OVERRIDE'] = '1'
                os.environ['MPS_MEMORY_ALLOCATION_POLICY'] = 'optimal'
                
        elif self.device == "cuda":
            # CUDA ìµœì í™” (ì„œë²„ í™˜ê²½ìš©)
            os.environ['CUDA_VISIBLE_DEVICES'] = '0'
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
            
        # ê³µí†µ CPU ìµœì í™”
        if self.is_m3_max:
            # M3 MaxëŠ” 14ì½”ì–´ (10 ì„±ëŠ¥ + 4 íš¨ìœ¨)
            os.environ['OMP_NUM_THREADS'] = '10'  # ì„±ëŠ¥ ì½”ì–´ë§Œ ì‚¬ìš©
            os.environ['MKL_NUM_THREADS'] = '10'
        else:
            os.environ['OMP_NUM_THREADS'] = '8'
            os.environ['MKL_NUM_THREADS'] = '8'
            
        # PyTorch ìµœì í™”
        torch.set_num_threads(10 if self.is_m3_max else 8)
        
        logger.info("âš™ï¸ ì‹œìŠ¤í…œ ìµœì í™” ì„¤ì • ì™„ë£Œ")
    
    def get_model_config(self) -> Dict[str, Any]:
        """AI ëª¨ë¸ë³„ ìµœì í™” ì„¤ì • ë°˜í™˜"""
        
        base_config = {
            "device": self.device,
            "memory_efficient": True,
            "batch_size": 1,  # ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°
            "gradient_checkpointing": True,  # ë©”ëª¨ë¦¬ ì ˆì•½
        }
        
        if self.device == "mps":
            # M3 Max Metal ì„¤ì •
            mps_config = {
                "dtype": torch.float32,  # MPSëŠ” float16ì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
                "attention_slicing": True,
                "cpu_offload": False,  # 128GB RAMì´ë¯€ë¡œ CPU ì˜¤í”„ë¡œë“œ ë¹„í™œì„±í™”
            }
            
            if self.is_m3_max:
                # M3 Max ì „ìš© ê³ ì„±ëŠ¥ ì„¤ì •
                mps_config.update({
                    "max_memory_mb": 24000,  # 24GB GPU ë©”ëª¨ë¦¬ í• ë‹¹
                    "enable_flash_attention": True,
                    "mixed_precision": False,  # MPSì—ì„œëŠ” ë¹„í™œì„±í™”
                })
            else:
                # M3 Pro/ì¼ë°˜ ë³´ìˆ˜ì  ì„¤ì •
                mps_config.update({
                    "max_memory_mb": 12000,
                    "enable_flash_attention": False,
                })
                
            base_config.update(mps_config)
            
        elif self.device == "cuda":
            # NVIDIA GPU ì„¤ì •
            cuda_config = {
                "dtype": torch.float16,
                "mixed_precision": True,
                "attention_slicing": False,
                "cpu_offload": False,
            }
            base_config.update(cuda_config)
            
        else:
            # CPU ì„¤ì •
            cpu_config = {
                "dtype": torch.float32,
                "max_memory_mb": 8000,
                "cpu_offload": True,
                "low_cpu_mem_usage": True,
            }
            base_config.update(cpu_config)
            
        return base_config
    
    def optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰"""
        try:
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device == "mps":
                torch.mps.empty_cache()
                logger.debug("ğŸ§¹ MPS ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                
            elif self.device == "cuda":
                torch.cuda.empty_cache()
                logger.debug("ğŸ§¹ CUDA ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def get_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        
        # ê¸°ë³¸ ì •ë³´
        info = {
            "device": self.device,
            "platform": platform.system(),
            "machine": platform.machine(),
            "python_version": platform.python_version(),
            "pytorch_version": torch.__version__,
        }
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        memory = psutil.virtual_memory()
        info["system_memory"] = {
            "total_gb": round(memory.total / (1024**3), 1),
            "available_gb": round(memory.available / (1024**3), 1),
            "used_percent": memory.percent,
        }
        
        # GPUë³„ ìƒì„¸ ì •ë³´
        if self.device == "mps":
            info["gpu_info"] = {
                "name": "Apple M3 Max" if self.is_m3_max else "Apple Silicon",
                "memory_pool": "í†µí•© ë©”ëª¨ë¦¬ (Unified Memory)",
                "cores": "30-40 GPU ì½”ì–´" if self.is_m3_max else "ì•Œ ìˆ˜ ì—†ìŒ",
                "neural_engine": "16ì½”ì–´" if self.is_m3_max else "ì•Œ ìˆ˜ ì—†ìŒ",
                "memory_bandwidth": "400GB/s" if self.is_m3_max else "ì•Œ ìˆ˜ ì—†ìŒ",
            }
            
        elif self.device == "cuda":
            if torch.cuda.is_available():
                gpu_props = torch.cuda.get_device_properties(0)
                info["gpu_info"] = {
                    "name": gpu_props.name,
                    "memory_gb": round(gpu_props.total_memory / (1024**3), 1),
                    "compute_capability": f"{gpu_props.major}.{gpu_props.minor}",
                    "multiprocessor_count": gpu_props.multiprocessor_count,
                }
        
        return info
    
    def benchmark_device(self, iterations: int = 100) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        
        logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ ë°˜ë³µ)...")
        
        try:
            device = torch.device(self.device)
            
            # í…ŒìŠ¤íŠ¸ í…ì„œ ìƒì„±
            size = 1000 if self.is_m3_max else 500
            x = torch.randn(size, size, device=device)
            y = torch.randn(size, size, device=device)
            
            # ì›Œë°ì—…
            for _ in range(10):
                _ = torch.mm(x, y)
            
            # ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
            import time
            start = time.time()
            
            for _ in range(iterations):
                z = torch.mm(x, y)
                
            end = time.time()
            
            avg_time = (end - start) / iterations
            operations_per_sec = 1.0 / avg_time
            
            result = {
                "success": True,
                "device": self.device,
                "tensor_size": f"{size}x{size}",
                "iterations": iterations,
                "total_time_sec": round(end - start, 4),
                "avg_time_per_operation_ms": round(avg_time * 1000, 4),
                "operations_per_second": round(operations_per_sec, 2),
                "result_shape": list(z.shape),
            }
            
            logger.info(f"âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {avg_time*1000:.2f}ms/ì—°ì‚°")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "device": self.device,
            }
    
    def test_ai_pipeline(self) -> bool:
        """AI íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ§ª AI íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        try:
            device = torch.device(self.device)
            
            # ê°„ë‹¨í•œ ì‹ ê²½ë§ í…ŒìŠ¤íŠ¸
            import torch.nn as nn
            
            model = nn.Sequential(
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Linear(128, 10)
            ).to(device)
            
            # í…ŒìŠ¤íŠ¸ ì…ë ¥
            batch_size = 1 if self.device == "mps" else 4
            x = torch.randn(batch_size, 512, device=device)
            
            # ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
            with torch.no_grad():
                output = model(x)
            
            logger.info(f"âœ… AI íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {output.shape}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ AI íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    def get_optimization_summary(self) -> str:
        """ìµœì í™” ì„¤ì • ìš”ì•½ ë°˜í™˜"""
        
        summary = []
        summary.append("ğŸ¯ M3 Max GPU ìµœì í™” ì„¤ì • ìš”ì•½")
        summary.append("=" * 40)
        summary.append(f"ë””ë°”ì´ìŠ¤: {self.device}")
        summary.append(f"M3 Max ëª¨ë“œ: {'í™œì„±í™”' if self.is_m3_max else 'ë¹„í™œì„±í™”'}")
        summary.append(f"ë©”ëª¨ë¦¬ í• ë‹¹: {self.memory_fraction * 100}%")
        
        if self.device == "mps":
            summary.append("Metal Performance Shaders í™œì„±í™”")
            summary.append("í†µí•© ë©”ëª¨ë¦¬ (Unified Memory) í™œìš©")
            
        config = self.get_model_config()
        summary.append(f"ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
        summary.append(f"ë°ì´í„° íƒ€ì…: {config['dtype']}")
        summary.append(f"ë©”ëª¨ë¦¬ íš¨ìœ¨ ëª¨ë“œ: {config['memory_efficient']}")
        
        return "\n".join(summary)

# ì „ì—­ GPU ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
gpu_config = M3MaxGPUConfig()

# ìì£¼ ì‚¬ìš©ë˜ëŠ” ì„¤ì •ë“¤
DEVICE = gpu_config.device
MODEL_CONFIG = gpu_config.get_model_config()
DEVICE_INFO = gpu_config.get_device_info()

# ì´ˆê¸°í™” ì‹œ ì •ë³´ ì¶œë ¥
logger.info("\n" + gpu_config.get_optimization_summary())

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def get_torch_device() -> torch.device:
    """PyTorch ë””ë°”ì´ìŠ¤ ê°ì²´ ë°˜í™˜"""
    return torch.device(DEVICE)

def optimize_model_for_device(model: torch.nn.Module) -> torch.nn.Module:
    """ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ì— ìµœì í™”"""
    model = model.to(DEVICE)
    
    if DEVICE == "mps":
        # MPS ìµœì í™”
        model.eval()  # MPSì—ì„œëŠ” eval ëª¨ë“œê°€ ë” ì•ˆì •ì 
        
    elif DEVICE == "cuda":
        # CUDA ìµœì í™”
        if MODEL_CONFIG.get("mixed_precision", False):
            model.half()
            
    return model

def create_optimized_tensor(data, dtype=None) -> torch.Tensor:
    """ë””ë°”ì´ìŠ¤ ìµœì í™”ëœ í…ì„œ ìƒì„±"""
    if dtype is None:
        dtype = MODEL_CONFIG["dtype"]
        
    if isinstance(data, torch.Tensor):
        return data.to(device=DEVICE, dtype=dtype)
    else:
        return torch.tensor(data, device=DEVICE, dtype=dtype)

# ìŠ¤íƒ€íŠ¸ì—… ì‹œ ìë™ ì‹¤í–‰
def startup_gpu_check():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ GPU ì²´í¬"""
    logger.info("ğŸš€ GPU ì„¤ì • ì‹œì‘ ê²€ì‚¬...")
    
    # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
    for key, value in DEVICE_INFO.items():
        if isinstance(value, dict):
            logger.info(f"{key}:")
            for sub_key, sub_value in value.items():
                logger.info(f"  {sub_key}: {sub_value}")
        else:
            logger.info(f"{key}: {value}")
    
    # AI íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
    gpu_config.test_ai_pipeline()
    
    logger.info("âœ… GPU ì„¤ì • ê²€ì‚¬ ì™„ë£Œ")

if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
    startup_gpu_check()
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark_result = gpu_config.benchmark_device()
    print("\nğŸ”§ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
    for key, value in benchmark_result.items():
        print(f"  {key}: {value}")