# backend/app/core/gpu_config.py
"""
GPU ì„¤ì • ë° ë””ë°”ì´ìŠ¤ ê´€ë¦¬
Apple Silicon MPS, CUDA, CPU ì§€ì›
"""

import torch
import platform
import psutil
import logging
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

class GPUConfig:
    """GPU ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self._device = self._detect_device()
        self._device_info = self._collect_device_info()
        self._initialize_settings()
        
    def _detect_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        try:
            # 1. MPS (Apple Silicon) í™•ì¸
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and torch.backends.mps.is_built():
                logger.info("ğŸ Apple Silicon MPS ê°ì§€ë¨")
                return "mps"
                
            # 2. CUDA (NVIDIA GPU) í™•ì¸
            elif torch.cuda.is_available():
                logger.info(f"ğŸ”¥ CUDA GPU ê°ì§€ë¨: {torch.cuda.get_device_name(0)}")
                return "cuda"
                
            # 3. CPU í´ë°±
            else:
                logger.info("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
                return "cpu"
                
        except Exception as e:
            logger.error(f"ë””ë°”ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨: {e}")
            return "cpu"
    
    def _collect_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        try:
            # ì‹œìŠ¤í…œ ì •ë³´
            info = {
                'device': self._device,
                'platform': platform.platform(),
                'processor': platform.processor(),
                'architecture': platform.architecture()[0],
                'pytorch_version': torch.__version__,
            }
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            memory = psutil.virtual_memory()
            info['system_memory_gb'] = round(memory.total / 1024**3, 1)
            info['available_memory_gb'] = round(memory.available / 1024**3, 1)
            
            # ë””ë°”ì´ìŠ¤ë³„ ì„¸ë¶€ ì •ë³´
            if self._device == "mps":
                info.update({
                    'mps_available': torch.backends.mps.is_available(),
                    'mps_built': torch.backends.mps.is_built(),
                    'apple_silicon': 'arm' in platform.processor().lower() or 'arm' in platform.machine().lower()
                })
                
            elif self._device == "cuda":
                info.update({
                    'cuda_version': torch.version.cuda,
                    'gpu_count': torch.cuda.device_count(),
                    'gpu_name': torch.cuda.get_device_name(0),
                    'gpu_memory_gb': round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1)
                })
            
            return info
            
        except Exception as e:
            logger.error(f"ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {'device': self._device, 'error': str(e)}
    
    def _initialize_settings(self):
        """PyTorch ì„¤ì • ì´ˆê¸°í™”"""
        try:
            import os
            
            # ìŠ¤ë ˆë“œ ìˆ˜ ì„¤ì •
            if self._device == "cpu":
                torch.set_num_threads(min(4, psutil.cpu_count()))
            
            # MPS ìµœì í™” ì„¤ì •
            if self._device == "mps":
                os.environ.setdefault('PYTORCH_ENABLE_MPS_FALLBACK', '1')
            
            # CUDA ìµœì í™” ì„¤ì •
            if self._device == "cuda":
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
            
            self._log_device_info()
            logger.info(f"âœ… PyTorch ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ - Device: {self._device}")
            
        except Exception as e:
            logger.warning(f"PyTorch ì„¤ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _log_device_info(self):
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë¡œê¹…"""
        logger.info("=" * 50)
        logger.info("ğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´")
        logger.info(f"í”Œë«í¼: {self._device_info.get('platform', 'Unknown')}")
        logger.info(f"í”„ë¡œì„¸ì„œ: {self._device_info.get('processor', 'Unknown')}")
        logger.info(f"ì•„í‚¤í…ì²˜: {self._device_info.get('architecture', 'Unknown')}")
        
        if self._device_info.get('apple_silicon'):
            logger.info("ğŸ Apple Silicon ê°ì§€ë¨")
            
        logger.info(f"PyTorch ë²„ì „: {self._device_info.get('pytorch_version', 'Unknown')}")
        logger.info(f"ë””ë°”ì´ìŠ¤: {self._device}")
        
        if self._device == "mps":
            logger.info(f"MPS ì‚¬ìš© ê°€ëŠ¥: {self._device_info.get('mps_available', False)}")
            logger.info(f"MPS ë¹Œë“œë¨: {self._device_info.get('mps_built', False)}")
            
        elif self._device == "cuda":
            logger.info(f"CUDA ë²„ì „: {self._device_info.get('cuda_version', 'Unknown')}")
            logger.info(f"GPU ê°œìˆ˜: {self._device_info.get('gpu_count', 0)}")
            logger.info(f"GPU ì´ë¦„: {self._device_info.get('gpu_name', 'Unknown')}")
            logger.info(f"GPU ë©”ëª¨ë¦¬: {self._device_info.get('gpu_memory_gb', 0)}GB")
            
        logger.info(f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {self._device_info.get('system_memory_gb', 0)}GB")
        logger.info("=" * 50)
    
    @property
    def device(self) -> str:
        """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self._device
    
    @property 
    def device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return self._device_info.copy()
    
    def get_model_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
        config = {
            "device": self._device,
            "memory_efficient": True,
            "batch_size": 1,
        }
        
        if self._device == "mps":
            config.update({
                "dtype": torch.float32,  # MPSëŠ” float16 ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
                "max_memory_mb": 16000,  # ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì œí•œ
            })
        elif self._device == "cuda":
            config.update({
                "dtype": torch.float16,  # CUDAëŠ” float16 ì§€ì›
                "mixed_precision": True,
            })
        else:
            config.update({
                "dtype": torch.float32,
                "max_memory_mb": 8000,   # CPU ë©”ëª¨ë¦¬ ì œí•œ
            })
            
        return config
    
    def optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            if self._device == "cuda":
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            elif self._device == "mps":
                torch.mps.empty_cache()
                torch.mps.synchronize()
                
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            import gc
            gc.collect()
            
            logger.debug("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def check_memory_available(self, required_gb: float = 4.0) -> bool:
        """ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            if self._device == "cuda" and torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated(0)
                available = (total_memory - allocated_memory) / 1024**3
                return available >= required_gb
                
            elif self._device == "mps":
                # MPSëŠ” ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ê³µìœ 
                memory_info = psutil.virtual_memory()
                available_gb = memory_info.available / 1024**3
                return available_gb * 0.6 >= required_gb  # MPSëŠ” 60% ì •ë„ ì‚¬ìš© ê°€ëŠ¥
                
            else:
                # CPU ë©”ëª¨ë¦¬
                memory_info = psutil.virtual_memory()
                available_gb = memory_info.available / 1024**3
                return available_gb >= required_gb
                
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
            return True  # í™•ì¸ ì‹¤íŒ¨ ì‹œ ê³„ì† ì§„í–‰


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
_gpu_config_instance = GPUConfig()

# ì „ì—­ ë³€ìˆ˜ export (ì´ì „ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± ìœ ì§€)
DEVICE = _gpu_config_instance.device
DEVICE_INFO = _gpu_config_instance.device_info
MODEL_CONFIG = _gpu_config_instance.get_model_config()

# í•¨ìˆ˜ë“¤ (ì´ì „ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± ìœ ì§€)
def get_device() -> str:
    """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return _gpu_config_instance.device

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
    return _gpu_config_instance.device_info

def get_optimal_settings() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ë³„ ìµœì  ì„¤ì • ë°˜í™˜"""
    return _gpu_config_instance.get_model_config()

def optimize_memory():
    """ë©”ëª¨ë¦¬ ìµœì í™”"""
    _gpu_config_instance.optimize_memory()

def check_memory_available(required_gb: float = 4.0) -> bool:
    """ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    return _gpu_config_instance.check_memory_available(required_gb)

# í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë„ export (ìƒˆë¡œìš´ ì½”ë“œì—ì„œ ì‚¬ìš©)
gpu_config = _gpu_config_instance

# ì´ˆê¸°í™” ë¡œê·¸
logger.info(f"ğŸ”§ GPU ì„¤ì • ì™„ë£Œ: {DEVICE}")
logger.info(f"ğŸ“Š ëª¨ë¸ ì„¤ì •: {MODEL_CONFIG}")