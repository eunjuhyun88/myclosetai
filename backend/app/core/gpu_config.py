# app/core/gpu_config.py
"""
MyCloset AI - GPU ì„¤ì • ë° ìµœì í™” (M3 Max íŠ¹í™”)
- Apple Silicon MPS ìµœì í™”
- CUDA í˜¸í™˜ì„± 
- ë©”ëª¨ë¦¬ ê´€ë¦¬
- ì„±ëŠ¥ íŠœë‹
"""
import os
import logging
import platform
from typing import Dict, Any, Optional, Tuple
import subprocess

# PyTorch ì„ íƒì  import
try:
    import torch
    import torch.backends.mps
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

# psutil ì„ íƒì  import
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

logger = logging.getLogger(__name__)

class GPUConfig:
    """GPU ì„¤ì • ë° ìµœì í™” ê´€ë¦¬ì"""
    
    def __init__(self):
        self.device = "cpu"
        self.device_name = "CPU"
        self.memory_gb = 0
        self.is_apple_silicon = False
        self.is_m3_max = False
        self.optimization_level = "basic"
        
        self._detect_hardware()
        self._configure_device()
        self._apply_optimizations()
    
    def _detect_hardware(self):
        """í•˜ë“œì›¨ì–´ ê°ì§€"""
        try:
            # macOS ì—¬ë¶€ í™•ì¸
            is_macos = platform.system() == "Darwin"
            
            if is_macos:
                # Apple Silicon ê°ì§€
                try:
                    result = subprocess.run(
                        ["sysctl", "-n", "machdep.cpu.brand_string"], 
                        capture_output=True, 
                        text=True, 
                        timeout=5
                    )
                    cpu_brand = result.stdout.strip()
                    
                    if "Apple" in cpu_brand:
                        self.is_apple_silicon = True
                        
                        # M3 Max íŠ¹ë³„ ê°ì§€
                        if "M3 Max" in cpu_brand:
                            self.is_m3_max = True
                            self.optimization_level = "m3_max"
                            logger.info(f"ğŸ M3 Max ê°ì§€: {cpu_brand}")
                        elif "M3" in cpu_brand:
                            self.optimization_level = "m3"
                            logger.info(f"ğŸ M3 ê°ì§€: {cpu_brand}")
                        elif any(m in cpu_brand for m in ["M1", "M2"]):
                            self.optimization_level = "apple_silicon"
                            logger.info(f"ğŸ Apple Silicon ê°ì§€: {cpu_brand}")
                
                except Exception as e:
                    logger.warning(f"CPU ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
            
            # ë©”ëª¨ë¦¬ ì •ë³´ 
            if PSUTIL_AVAILABLE:
                total_memory = psutil.virtual_memory().total
                self.memory_gb = total_memory / (1024 ** 3)
                logger.info(f"ğŸ’¾ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {self.memory_gb:.1f}GB")
            
        except Exception as e:
            logger.error(f"í•˜ë“œì›¨ì–´ ê°ì§€ ì‹¤íŒ¨: {e}")
    
    def _configure_device(self):
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if not TORCH_AVAILABLE:
            logger.warning("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ, CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            self.device = "cpu"
            self.device_name = "CPU"
            return
        
        # ë””ë°”ì´ìŠ¤ ìš°ì„ ìˆœìœ„: MPS > CUDA > CPU
        if torch.backends.mps.is_available() and self.is_apple_silicon:
            self.device = "mps"
            self.device_name = "Apple Silicon MPS"
            logger.info("ğŸš€ MPS (Metal Performance Shaders) í™œì„±í™”")
            
        elif torch.cuda.is_available():
            self.device = "cuda"
            gpu_name = torch.cuda.get_device_name(0)
            self.device_name = f"CUDA - {gpu_name}"
            logger.info(f"ğŸš€ CUDA í™œì„±í™”: {gpu_name}")
            
        else:
            self.device = "cpu"
            self.device_name = "CPU"
            logger.info("âš¡ CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    def _apply_optimizations(self):
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì ìš©"""
        if not TORCH_AVAILABLE:
            return
        
        try:
            # ê³µí†µ ìµœì í™”
            torch.set_grad_enabled(False)  # ì¶”ë¡  ëª¨ë“œ
            
            if self.device == "mps":
                self._optimize_mps()
            elif self.device == "cuda":
                self._optimize_cuda()
            elif self.device == "cpu":
                self._optimize_cpu()
            
            logger.info(f"âœ… {self.device.upper()} ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
    
    def _optimize_mps(self):
        """MPS (Apple Silicon) ìµœì í™”"""
        if not torch.backends.mps.is_available():
            return
        
        try:
            # MPS ìµœì í™” ì„¤ì •
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"  # ë©”ëª¨ë¦¬ ê´€ë¦¬
            
            if self.is_m3_max:
                # M3 Max íŠ¹ë³„ ìµœì í™”
                os.environ["PYTORCH_MPS_ALLOCATOR_POLICY"] = "garbage_collection"
                
                # M3 MaxëŠ” 128GB í†µí•© ë©”ëª¨ë¦¬ í™œìš© ê°€ëŠ¥
                if self.memory_gb >= 64:
                    # ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ ìµœì í™”
                    torch.backends.mps.empty_cache = lambda: None  # ìºì‹œ ê´€ë¦¬ ì»¤ìŠ¤í„°ë§ˆì´ì§•
            
            # MPS ë°±ì—”ë“œ ìµœì í™”
            torch.backends.mps.is_available()  # MPS ì´ˆê¸°í™” íŠ¸ë¦¬ê±°
            
            logger.info("ğŸ MPS ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"MPS ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_cuda(self):
        """CUDA ìµœì í™”"""
        try:
            # CUDA ìµœì í™” ì„¤ì •
            torch.backends.cudnn.enabled = True
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬
            torch.cuda.empty_cache()
            
            # ë©€í‹° GPU ì§€ì› (í•„ìš”ì‹œ)
            if torch.cuda.device_count() > 1:
                logger.info(f"ğŸš€ {torch.cuda.device_count()}ê°œ GPU ê°ì§€")
            
            logger.info("ğŸ® CUDA ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"CUDA ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_cpu(self):
        """CPU ìµœì í™”"""
        try:
            # CPU ìŠ¤ë ˆë“œ ìµœì í™”
            if hasattr(torch, 'set_num_threads'):
                # ë¬¼ë¦¬ ì½”ì–´ ìˆ˜ ê°ì§€
                physical_cores = psutil.cpu_count(logical=False) if PSUTIL_AVAILABLE else 4
                torch.set_num_threads(min(physical_cores, 8))  # ìµœëŒ€ 8ìŠ¤ë ˆë“œ
            
            # Intel MKL ìµœì í™” (Intel CPU)
            if "intel" in platform.processor().lower():
                os.environ["MKL_NUM_THREADS"] = str(min(4, psutil.cpu_count(logical=False) if PSUTIL_AVAILABLE else 4))
            
            logger.info("âš¡ CPU ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"CPU ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def get_recommended_batch_size(self, model_size: str = "medium") -> int:
        """ëª¨ë¸ í¬ê¸°ë³„ ê¶Œì¥ ë°°ì¹˜ í¬ê¸°"""
        
        # ê¸°ë³¸ ë°°ì¹˜ í¬ê¸° ë§¤í•‘
        base_batch_sizes = {
            "small": {"mps": 8, "cuda": 16, "cpu": 4},
            "medium": {"mps": 4, "cuda": 8, "cpu": 2}, 
            "large": {"mps": 2, "cuda": 4, "cpu": 1},
            "xlarge": {"mps": 1, "cuda": 2, "cpu": 1}
        }
        
        base_size = base_batch_sizes.get(model_size, base_batch_sizes["medium"])[self.device]
        
        # M3 Max íŠ¹ë³„ ì¡°ì •
        if self.is_m3_max and self.memory_gb >= 64:
            base_size = min(base_size * 2, 16)  # ìµœëŒ€ 2ë°°, ìƒí•œ 16
        
        return base_size
    
    def get_recommended_precision(self) -> str:
        """ê¶Œì¥ ì •ë°€ë„"""
        if self.device == "mps":
            # MPSëŠ” float16 ì§€ì›ì´ ì œí•œì 
            return "float32"
        elif self.device == "cuda":
            return "float16"  # GPUì—ì„œëŠ” mixed precision í™œìš©
        else:
            return "float32"
    
    def get_memory_fraction(self) -> float:
        """ì‚¬ìš©í•  ë©”ëª¨ë¦¬ ë¹„ìœ¨"""
        if self.device == "mps":
            # MPSëŠ” í†µí•© ë©”ëª¨ë¦¬, ë³´ìˆ˜ì  ì ‘ê·¼
            if self.is_m3_max and self.memory_gb >= 64:
                return 0.6  # 60% ì‚¬ìš©
            else:
                return 0.4  # 40% ì‚¬ìš©
        elif self.device == "cuda":
            return 0.8  # GPU ë©”ëª¨ë¦¬ì˜ 80%
        else:
            return 0.5  # CPU ë©”ëª¨ë¦¬ì˜ 50%
    
    def setup_multiprocessing(self):
        """ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì •"""
        if self.device == "mps":
            # MPSëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ì œí•œ
            torch.multiprocessing.set_start_method('spawn', force=True)
            return 1  # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤
        else:
            # CPU/CUDAëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ê°€ëŠ¥
            max_workers = min(4, psutil.cpu_count(logical=False) if PSUTIL_AVAILABLE else 2)
            return max_workers
    
    def get_config_dict(self) -> Dict[str, Any]:
        """ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return {
            "device": self.device,
            "device_name": self.device_name,
            "memory_gb": round(self.memory_gb, 1),
            "is_apple_silicon": self.is_apple_silicon,
            "is_m3_max": self.is_m3_max,
            "optimization_level": self.optimization_level,
            "recommended_batch_size": self.get_recommended_batch_size(),
            "recommended_precision": self.get_recommended_precision(),
            "memory_fraction": self.get_memory_fraction(),
            "max_workers": self.setup_multiprocessing()
        }

# ì „ì—­ GPU ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
_gpu_config: Optional[GPUConfig] = None

def get_gpu_config() -> GPUConfig:
    """GPU ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _gpu_config
    if _gpu_config is None:
        _gpu_config = GPUConfig()
    return _gpu_config

def configure_gpu() -> Dict[str, Any]:
    """GPU ì„¤ì • ë° ì •ë³´ ë°˜í™˜"""
    config = get_gpu_config()
    return config.get_config_dict()

def get_optimal_device() -> str:
    """ìµœì  ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    config = get_gpu_config()
    return config.device

def is_m3_max() -> bool:
    """M3 Max ì—¬ë¶€ í™•ì¸"""
    config = get_gpu_config()
    return config.is_m3_max

def get_device_info() -> Tuple[str, str, float]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜ (device, name, memory)"""
    config = get_gpu_config()
    return config.device, config.device_name, config.memory_gb

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (import ì‹œ ìë™ ì‹¤í–‰)
def _set_environment_variables():
    """í™˜ê²½ ë³€ìˆ˜ ìë™ ì„¤ì •"""
    
    # PyTorch í™˜ê²½ ë³€ìˆ˜
    os.environ.setdefault("PYTORCH_ENABLE_MPS_FALLBACK", "1")
    os.environ.setdefault("PYTORCH_MPS_HIGH_WATERMARK_RATIO", "0.0")
    
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì •
    os.environ.setdefault("OMP_NUM_THREADS", "4")
    os.environ.setdefault("MKL_NUM_THREADS", "4")
    
    # CUDA ì„¤ì • (ìˆëŠ” ê²½ìš°)
    os.environ.setdefault("CUDA_CACHE_DISABLE", "0")
    os.environ.setdefault("CUDA_LAUNCH_BLOCKING", "0")

# ëª¨ë“ˆ ë¡œë“œ ì‹œ ìë™ ì‹¤í–‰
_set_environment_variables()