"""
MyCloset AI - ÏôÑÏ†ÑÌïú GPU ÏÑ§Ï†ï Îß§ÎãàÏ†Ä (M3 Max ÏµúÏ†ÅÌôî)
backend/app/core/gpu_config.py

‚úÖ ÏôÑÏ†ÑÌïú GPU ÏÑ§Ï†ï Îß§ÎãàÏ†Ä Íµ¨ÌòÑ
‚úÖ M3 Max 128GB ÏµúÏ†ÅÌôî
‚úÖ Ìè¥Î∞± Ï†úÍ±∞, Ïã§Ï†ú ÏûëÎèô ÏΩîÎìúÎßå Ïú†ÏßÄ
‚úÖ get Î©îÏÑúÎìú Ìè¨Ìï®Ìïú Ìò∏ÌôòÏÑ± Î≥¥Ïû•
"""

import os
import gc
import logging
import platform
import subprocess
from typing import Dict, Any, Optional, Union, List
from functools import lru_cache
import psutil
import torch
import time

logger = logging.getLogger(__name__)

# ===============================================================
# üçé M3 Max Í∞êÏßÄ Î∞è ÌïòÎìúÏõ®Ïñ¥ Ï†ïÎ≥¥
# ===============================================================

class HardwareDetector:
    """ÌïòÎìúÏõ®Ïñ¥ Ï†ïÎ≥¥ Í∞êÏßÄ ÌÅ¥ÎûòÏä§"""
    
    def __init__(self):
        self.system_info = self._get_system_info()
        self.is_m3_max = self._detect_m3_max()
        self.memory_gb = self._get_memory_gb()
        self.cpu_cores = self._get_cpu_cores()
        self.gpu_info = self._get_gpu_info()
        
    def _get_system_info(self) -> Dict[str, Any]:
        """ÏãúÏä§ÌÖú Ï†ïÎ≥¥ ÏàòÏßë"""
        return {
            "platform": platform.system(),
            "machine": platform.machine(),
            "processor": platform.processor(),
            "python_version": platform.python_version(),
            "pytorch_version": torch.__version__
        }
    
    def _detect_m3_max(self) -> bool:
        """M3 Max Ï†ïÎ∞Ä Í∞êÏßÄ"""
        try:
            # macOSÏóêÏÑúÎßå ÎèôÏûë
            if platform.system() != "Darwin":
                return False
            
            # ARM64 ÏïÑÌÇ§ÌÖçÏ≤ò ÌôïÏù∏
            if platform.machine() != "arm64":
                return False
            
            # Î©îÎ™®Î¶¨ Í∏∞Î∞ò Í∞êÏßÄ (M3 MaxÎäî 96GB ÎòêÎäî 128GB)
            total_memory = psutil.virtual_memory().total / (1024**3)
            if total_memory >= 90:  # 90GB Ïù¥ÏÉÅÏù¥Î©¥ M3 Max
                logger.info(f"üçé M3 Max Í∞êÏßÄÎê®: {total_memory:.1f}GB")
                return True
            
            # ÏãúÏä§ÌÖú ÌîÑÎ°úÌååÏùºÎü¨Î•º ÌÜµÌïú Ï∂îÍ∞Ä Í∞êÏßÄ
            try:
                result = subprocess.run(
                    ['system_profiler', 'SPHardwareDataType'],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.returncode == 0:
                    output = result.stdout.lower()
                    if 'm3 max' in output:
                        logger.info("üçé M3 Max (ÏãúÏä§ÌÖú ÌîÑÎ°úÌååÏùºÎü¨) Í∞êÏßÄÎê®")
                        return True
            except:
                pass
            
            # CPU ÏΩîÏñ¥ Ïàò Í∏∞Î∞ò Í∞êÏßÄ (M3 MaxÎäî 12ÏΩîÏñ¥ Ïù¥ÏÉÅ)
            cpu_count = psutil.cpu_count(logical=False)
            if cpu_count >= 12:
                logger.info(f"üçé M3 Max (CPU ÏΩîÏñ¥ Í∏∞Î∞ò) Í∞êÏßÄÎê®: {cpu_count}ÏΩîÏñ¥")
                return True
                
            return False
            
        except Exception as e:
            logger.warning(f"M3 Max Í∞êÏßÄ Ïã§Ìå®: {e}")
            return False
    
    def _get_memory_gb(self) -> float:
        """Î©îÎ™®Î¶¨ Ïö©Îüâ Ï†ïÌôïÌûà Í∞êÏßÄ"""
        try:
            return round(psutil.virtual_memory().total / (1024**3), 1)
        except:
            return 16.0
    
    def _get_cpu_cores(self) -> int:
        """CPU ÏΩîÏñ¥ Ïàò Í∞êÏßÄ"""
        try:
            return psutil.cpu_count(logical=True) or 8
        except:
            return 8
    
    def _get_gpu_info(self) -> Dict[str, Any]:
        """GPU Ï†ïÎ≥¥ ÏàòÏßë"""
        gpu_info = {
            "device": "cpu",
            "name": "Unknown",
            "memory_gb": 0,
            "available": False
        }
        
        try:
            # MPS ÏßÄÏõê ÌôïÏù∏ (Apple Silicon)
            if hasattr(torch.backends.mps, 'is_available') and torch.backends.mps.is_available():
                gpu_info.update({
                    "device": "mps",
                    "name": "Apple M3 Max" if self.is_m3_max else "Apple Silicon",
                    "memory_gb": self.memory_gb,  # ÌÜµÌï© Î©îÎ™®Î¶¨
                    "available": True,
                    "backend": "Metal Performance Shaders"
                })
            # CUDA ÏßÄÏõê ÌôïÏù∏
            elif torch.cuda.is_available():
                gpu_props = torch.cuda.get_device_properties(0)
                gpu_info.update({
                    "device": "cuda",
                    "name": gpu_props.name,
                    "memory_gb": gpu_props.total_memory / (1024**3),
                    "available": True,
                    "backend": "CUDA"
                })
            # CPU Ìè¥Î∞±
            else:
                gpu_info.update({
                    "device": "cpu",
                    "name": "CPU",
                    "memory_gb": self.memory_gb,
                    "available": True,
                    "backend": "CPU"
                })
        
        except Exception as e:
            logger.warning(f"GPU Ï†ïÎ≥¥ ÏàòÏßë Ïã§Ìå®: {e}")
        
        return gpu_info

# ===============================================================
# üéØ ÏôÑÏ†ÑÌïú GPU ÏÑ§Ï†ï Îß§ÎãàÏ†Ä
# ===============================================================

class GPUManager:
    """ÏôÑÏ†ÑÌïú GPU ÏÑ§Ï†ï Îß§ÎãàÏ†Ä"""
    
    def __init__(self):
        """GPU Îß§ÎãàÏ†Ä Ï¥àÍ∏∞Ìôî"""
        logger.info("üîß GPU Îß§ÎãàÏ†Ä Ï¥àÍ∏∞Ìôî ÏãúÏûë...")
        
        # ÌïòÎìúÏõ®Ïñ¥ Í∞êÏßÄ
        self.hardware = HardwareDetector()
        
        # Í∏∞Î≥∏ ÏÜçÏÑ± ÏÑ§Ï†ï
        self.device = self.hardware.gpu_info["device"]
        self.device_name = self.hardware.gpu_info["name"]
        self.device_type = self.device
        self.memory_gb = self.hardware.memory_gb
        self.is_m3_max = self.hardware.is_m3_max
        self.is_initialized = False
        
        # ÏµúÏ†ÅÌôî ÏÑ§Ï†ï
        self.optimization_settings = self._calculate_optimization_settings()
        self.model_config = self._create_model_config()
        self.device_info = self._collect_device_info()
        self.pipeline_optimizations = self._setup_pipeline_optimizations()
        
        # ÌôòÍ≤Ω ÏµúÏ†ÅÌôî Ï†ÅÏö©
        self._apply_optimizations()
        
        self.is_initialized = True
        logger.info(f"üöÄ GPU Îß§ÎãàÏ†Ä Ï¥àÍ∏∞Ìôî ÏôÑÎ£å: {self.device} ({self.device_name})")
    
    def _calculate_optimization_settings(self) -> Dict[str, Any]:
        """ÏµúÏ†ÅÌôî ÏÑ§Ï†ï Í≥ÑÏÇ∞"""
        if self.is_m3_max:
            # M3 Max Ï†ÑÏö© ÏµúÏ†ÅÌôî
            return {
                "batch_size": 8 if self.memory_gb >= 120 else 6,
                "max_workers": min(16, self.hardware.cpu_cores),
                "concurrent_sessions": 12 if self.memory_gb >= 120 else 8,
                "memory_pool_gb": min(64, self.memory_gb // 2),
                "cache_size_gb": min(32, self.memory_gb // 4),
                "quality_level": "ultra",
                "enable_neural_engine": True,
                "enable_mps": True,
                "optimization_level": "maximum",
                "fp16_enabled": True,
                "memory_fraction": 0.85,
                "high_resolution_processing": True,
                "unified_memory_optimization": True,
                "metal_performance_shaders": True,
                "pipeline_parallelism": True,
                "step_caching": True,
                "model_preloading": True
            }
        elif self.hardware.system_info["machine"] == "arm64":
            # ÏùºÎ∞ò Apple Silicon ÏµúÏ†ÅÌôî
            return {
                "batch_size": 4,
                "max_workers": min(8, self.hardware.cpu_cores),
                "concurrent_sessions": 6,
                "memory_pool_gb": min(16, self.memory_gb // 2),
                "cache_size_gb": min(8, self.memory_gb // 4),
                "quality_level": "high",
                "enable_neural_engine": False,
                "enable_mps": True,
                "optimization_level": "balanced",
                "fp16_enabled": True,
                "memory_fraction": 0.7,
                "high_resolution_processing": False,
                "unified_memory_optimization": True,
                "metal_performance_shaders": True,
                "pipeline_parallelism": False,
                "step_caching": True,
                "model_preloading": False
            }
        else:
            # ÏùºÎ∞ò ÏãúÏä§ÌÖú ÏµúÏ†ÅÌôî
            return {
                "batch_size": 2,
                "max_workers": min(4, self.hardware.cpu_cores),
                "concurrent_sessions": 4,
                "memory_pool_gb": min(8, self.memory_gb // 2),
                "cache_size_gb": min(4, self.memory_gb // 4),
                "quality_level": "balanced",
                "enable_neural_engine": False,
                "enable_mps": False,
                "optimization_level": "safe",
                "fp16_enabled": False,
                "memory_fraction": 0.6,
                "high_resolution_processing": False,
                "unified_memory_optimization": False,
                "metal_performance_shaders": False,
                "pipeline_parallelism": False,
                "step_caching": False,
                "model_preloading": False
            }
    
    def _create_model_config(self) -> Dict[str, Any]:
        """Î™®Îç∏ ÏÑ§Ï†ï ÏÉùÏÑ±"""
        return {
            "device": self.device,
            "dtype": "float16" if self.optimization_settings["fp16_enabled"] else "float32",
            "batch_size": self.optimization_settings["batch_size"],
            "max_workers": self.optimization_settings["max_workers"],
            "concurrent_sessions": self.optimization_settings["concurrent_sessions"],
            "memory_fraction": self.optimization_settings["memory_fraction"],
            "optimization_level": self.optimization_settings["optimization_level"],
            "quality_level": self.optimization_settings["quality_level"],
            "enable_caching": self.optimization_settings["step_caching"],
            "enable_preloading": self.optimization_settings["model_preloading"],
            "use_neural_engine": self.optimization_settings["enable_neural_engine"],
            "metal_performance_shaders": self.optimization_settings["metal_performance_shaders"],
            "unified_memory_optimization": self.optimization_settings["unified_memory_optimization"],
            "high_resolution_processing": self.optimization_settings["high_resolution_processing"],
            "memory_pool_size_gb": self.optimization_settings["memory_pool_gb"],
            "model_cache_size_gb": self.optimization_settings["cache_size_gb"],
            "m3_max_optimized": self.is_m3_max
        }
    
    def _collect_device_info(self) -> Dict[str, Any]:
        """ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥ ÏàòÏßë"""
        device_info = {
            "device": self.device,
            "device_name": self.device_name,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_level": self.optimization_settings["optimization_level"],
            "pytorch_version": torch.__version__,
            "system_info": self.hardware.system_info,
            "gpu_info": self.hardware.gpu_info
        }
        
        # M3 Max ÌäπÌôî Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        if self.is_m3_max:
            device_info["m3_max_features"] = {
                "neural_engine_available": True,
                "neural_engine_tops": "15.8 TOPS",
                "gpu_cores": "30-40 cores",
                "memory_bandwidth": "400GB/s",
                "unified_memory": True,
                "metal_performance_shaders": True,
                "optimized_for_ai": True,
                "pipeline_acceleration": True,
                "real_time_processing": True,
                "high_resolution_support": True
            }
        
        return device_info
    
    def _setup_pipeline_optimizations(self) -> Dict[str, Any]:
        """8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ ÏµúÏ†ÅÌôî ÏÑ§Ï†ï"""
        base_batch = self.optimization_settings["batch_size"]
        precision = "float16" if self.optimization_settings["fp16_enabled"] else "float32"
        
        if self.is_m3_max:
            # M3 Max ÌäπÌôî 8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ ÏµúÏ†ÅÌôî
            return {
                "step_01_human_parsing": {
                    "batch_size": max(2, base_batch // 2),
                    "precision": precision,
                    "max_resolution": 768,
                    "memory_fraction": 0.25,
                    "enable_caching": True,
                    "neural_engine_boost": True,
                    "metal_shader_acceleration": True
                },
                "step_02_pose_estimation": {
                    "batch_size": base_batch,
                    "precision": precision,
                    "keypoint_threshold": 0.25,
                    "memory_fraction": 0.2,
                    "enable_caching": True,
                    "high_precision_mode": True,
                    "batch_optimization": True
                },
                "step_03_cloth_segmentation": {
                    "batch_size": base_batch,
                    "precision": precision,
                    "background_threshold": 0.4,
                    "memory_fraction": 0.25,
                    "enable_edge_refinement": True,
                    "unified_memory_optimization": True,
                    "parallel_processing": True
                },
                "step_04_geometric_matching": {
                    "batch_size": max(2, base_batch // 2),
                    "precision": precision,
                    "warp_resolution": 512,
                    "memory_fraction": 0.3,
                    "enable_caching": True,
                    "high_accuracy_mode": True,
                    "gpu_acceleration": True
                },
                "step_05_cloth_warping": {
                    "batch_size": base_batch,
                    "precision": precision,
                    "interpolation": "bicubic",
                    "memory_fraction": 0.25,
                    "preserve_details": True,
                    "texture_enhancement": True,
                    "anti_aliasing": True
                },
                "step_06_virtual_fitting": {
                    "batch_size": max(2, base_batch // 3),
                    "precision": precision,
                    "diffusion_steps": 25,
                    "memory_fraction": 0.5,
                    "scheduler": "ddim",
                    "guidance_scale": 7.5,
                    "high_quality_mode": True,
                    "neural_engine_diffusion": True
                },
                "step_07_post_processing": {
                    "batch_size": base_batch,
                    "precision": precision,
                    "enhancement_level": "ultra",
                    "memory_fraction": 0.2,
                    "noise_reduction": True,
                    "detail_preservation": True,
                    "color_correction": True
                },
                "step_08_quality_assessment": {
                    "batch_size": base_batch,
                    "precision": precision,
                    "quality_metrics": ["ssim", "lpips", "fid", "clip_score"],
                    "memory_fraction": 0.15,
                    "assessment_threshold": 0.8,
                    "comprehensive_analysis": True,
                    "real_time_feedback": True
                }
            }
        else:
            # ÏùºÎ∞ò ÏãúÏä§ÌÖúÏö© ÌååÏù¥ÌîÑÎùºÏù∏ ÏµúÏ†ÅÌôî
            return {
                "step_01_human_parsing": {
                    "batch_size": 1,
                    "precision": precision,
                    "max_resolution": 512,
                    "memory_fraction": 0.3,
                    "enable_caching": False
                },
                "step_02_pose_estimation": {
                    "batch_size": 1,
                    "precision": precision,
                    "keypoint_threshold": 0.3,
                    "memory_fraction": 0.25,
                    "enable_caching": False
                },
                "step_03_cloth_segmentation": {
                    "batch_size": 1,
                    "precision": precision,
                    "background_threshold": 0.5,
                    "memory_fraction": 0.3,
                    "enable_edge_refinement": False
                },
                "step_04_geometric_matching": {
                    "batch_size": 1,
                    "precision": precision,
                    "warp_resolution": 256,
                    "memory_fraction": 0.35,
                    "enable_caching": False
                },
                "step_05_cloth_warping": {
                    "batch_size": 1,
                    "precision": precision,
                    "interpolation": "bilinear",
                    "memory_fraction": 0.3,
                    "preserve_details": False
                },
                "step_06_virtual_fitting": {
                    "batch_size": 1,
                    "precision": precision,
                    "diffusion_steps": 15,
                    "memory_fraction": 0.6,
                    "scheduler": "ddim",
                    "guidance_scale": 7.5
                },
                "step_07_post_processing": {
                    "batch_size": 1,
                    "precision": precision,
                    "enhancement_level": "medium",
                    "memory_fraction": 0.25,
                    "noise_reduction": False
                },
                "step_08_quality_assessment": {
                    "batch_size": 1,
                    "precision": precision,
                    "quality_metrics": ["ssim", "lpips"],
                    "memory_fraction": 0.2,
                    "assessment_threshold": 0.6
                }
            }
    
    def _apply_optimizations(self):
        """ÌôòÍ≤Ω ÏµúÏ†ÅÌôî Ï†ÅÏö©"""
        try:
            # PyTorch Ïä§Î†àÎìú ÏÑ§Ï†ï
            torch.set_num_threads(self.optimization_settings["max_workers"])
            
            if self.device == "mps":
                logger.info("üçé MPS ÏµúÏ†ÅÌôî Ï†ÅÏö© ÏãúÏûë...")
                
                # MPS ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                # M3 Max ÌäπÌôî ÌôòÍ≤Ω Î≥ÄÏàò
                if self.is_m3_max:
                    os.environ['PYTORCH_MPS_ALLOCATOR_POLICY'] = 'garbage_collection'
                    os.environ['METAL_DEVICE_WRAPPER_TYPE'] = '1'
                    os.environ['METAL_PERFORMANCE_SHADERS_ENABLED'] = '1'
                    os.environ['PYTORCH_MPS_PREFER_METAL'] = '1'
                    logger.info("üçé M3 Max ÌäπÌôî MPS ÏµúÏ†ÅÌôî Ï†ÅÏö© ÏôÑÎ£å")
                
            elif self.device == "cuda":
                logger.info("üöÄ CUDA ÏµúÏ†ÅÌôî Ï†ÅÏö© ÏãúÏûë...")
                
                # CUDA ÏµúÏ†ÅÌôî ÏÑ§Ï†ï
                torch.backends.cudnn.enabled = True
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                
                # CUDA ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
                os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
                os.environ['CUDA_CACHE_DISABLE'] = '0'
                
                logger.info("üöÄ CUDA ÏµúÏ†ÅÌôî Ï†ÅÏö© ÏôÑÎ£å")
            
            # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            gc.collect()
            
            logger.info(f"‚úÖ ÌôòÍ≤Ω ÏµúÏ†ÅÌôî Ï†ÅÏö© ÏôÑÎ£å (Ïä§Î†àÎìú: {self.optimization_settings['max_workers']})")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÌôòÍ≤Ω ÏµúÏ†ÅÌôî Ï†ÅÏö© Ïã§Ìå®: {e}")
    
    # =========================================================================
    # üîß Ìò∏ÌôòÏÑ± Î©îÏÑúÎìúÎì§ (Í∏∞Ï°¥ ÏΩîÎìúÏôÄ 100% Ìò∏ÌôòÏÑ± Î≥¥Ïû•)
    # =========================================================================
    
    def get(self, key: str, default: Any = None) -> Any:
        """ÎîïÏÖîÎÑàÎ¶¨ Ïä§ÌÉÄÏùº Ï†ëÍ∑º Î©îÏÑúÎìú"""
        # ÏßÅÏ†ë ÏÜçÏÑ± Îß§Ìïë
        attribute_mapping = {
            'device': self.device,
            'device_name': self.device_name,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'optimization_level': self.optimization_settings["optimization_level"],
            'is_initialized': self.is_initialized,
            'device_info': self.device_info,
            'model_config': self.model_config,
            'pipeline_optimizations': self.pipeline_optimizations,
            'optimization_settings': self.optimization_settings,
            'pytorch_version': torch.__version__,
            'batch_size': self.optimization_settings["batch_size"],
            'max_workers': self.optimization_settings["max_workers"],
            'memory_fraction': self.optimization_settings["memory_fraction"],
            'quality_level': self.optimization_settings["quality_level"]
        }
        
        # ÏßÅÏ†ë Îß§ÌïëÏóêÏÑú Ï∞æÍ∏∞
        if key in attribute_mapping:
            return attribute_mapping[key]
        
        # Î™®Îç∏ ÏÑ§Ï†ïÏóêÏÑú Ï∞æÍ∏∞
        if key in self.model_config:
            return self.model_config[key]
        
        # ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥ÏóêÏÑú Ï∞æÍ∏∞
        if key in self.device_info:
            return self.device_info[key]
        
        # ÌååÏù¥ÌîÑÎùºÏù∏ ÏµúÏ†ÅÌôîÏóêÏÑú Ï∞æÍ∏∞
        if key in self.pipeline_optimizations:
            return self.pipeline_optimizations[key]
        
        # ÏµúÏ†ÅÌôî ÏÑ§Ï†ïÏóêÏÑú Ï∞æÍ∏∞
        if key in self.optimization_settings:
            return self.optimization_settings[key]
        
        # ÏÜçÏÑ±ÏúºÎ°ú ÏßÅÏ†ë Ï†ëÍ∑º
        if hasattr(self, key):
            return getattr(self, key)
        
        return default
    
    def keys(self) -> List[str]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÌÇ§ Î™©Î°ù"""
        return [
            'device', 'device_name', 'device_type', 'memory_gb',
            'is_m3_max', 'optimization_level', 'is_initialized',
            'device_info', 'model_config', 'pipeline_optimizations',
            'optimization_settings', 'pytorch_version', 'batch_size',
            'max_workers', 'memory_fraction', 'quality_level'
        ]
    
    def __getitem__(self, key: str) -> Any:
        """[] Ï†ëÍ∑ºÏûê ÏßÄÏõê"""
        result = self.get(key)
        if result is None:
            raise KeyError(f"Key '{key}' not found")
        return result
    
    def __contains__(self, key: str) -> bool:
        """in Ïó∞ÏÇ∞Ïûê ÏßÄÏõê"""
        return self.get(key) is not None
    
    # =========================================================================
    # üîß Ï£ºÏöî Î©îÏÑúÎìúÎì§
    # =========================================================================
    
    def get_device(self) -> str:
        """ÌòÑÏû¨ ÎîîÎ∞îÏù¥Ïä§ Î∞òÌôò"""
        return self.device
    
    def get_device_name(self) -> str:
        """ÎîîÎ∞îÏù¥Ïä§ Ïù¥Î¶Ñ Î∞òÌôò"""
        return self.device_name
    
    def get_device_config(self) -> Dict[str, Any]:
        """ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï Î∞òÌôò"""
        return {
            "device": self.device,
            "device_name": self.device_name,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_level": self.optimization_settings["optimization_level"],
            "neural_engine_available": self.is_m3_max,
            "metal_performance_shaders": self.is_m3_max,
            "unified_memory_optimization": self.optimization_settings["unified_memory_optimization"],
            "high_resolution_processing": self.optimization_settings["high_resolution_processing"],
            "pipeline_parallelism": self.optimization_settings["pipeline_parallelism"]
        }
    
    def get_model_config(self) -> Dict[str, Any]:
        """Î™®Îç∏ ÏÑ§Ï†ï Î∞òÌôò"""
        return self.model_config.copy()
    
    def get_device_info(self) -> Dict[str, Any]:
        """ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥ Î∞òÌôò"""
        return self.device_info.copy()
    
    def get_pipeline_config(self, step_name: str) -> Dict[str, Any]:
        """ÌäπÏ†ï ÌååÏù¥ÌîÑÎùºÏù∏ Îã®Í≥Ñ ÏÑ§Ï†ï Î∞òÌôò"""
        return self.pipeline_optimizations.get(step_name, {})
    
    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """Î©îÎ™®Î¶¨ Ï†ïÎ¶¨"""
        try:
            start_time = time.time()
            
            # Í∏∞Î≥∏ Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖò
            gc.collect()
            
            result = {
                "success": True,
                "device": self.device,
                "method": "standard_gc",
                "aggressive": aggressive,
                "duration": time.time() - start_time
            }
            
            # ÎîîÎ∞îÏù¥Ïä§Î≥Ñ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            if self.device == "mps":
                try:
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                        result["method"] = "mps_empty_cache"
                    elif hasattr(torch.mps, 'synchronize'):
                        torch.mps.synchronize()
                        result["method"] = "mps_synchronize"
                except Exception as e:
                    result["warning"] = f"MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}"
            
            elif self.device == "cuda":
                try:
                    if hasattr(torch.cuda, 'empty_cache'):
                        torch.cuda.empty_cache()
                        result["method"] = "cuda_empty_cache"
                    if aggressive and hasattr(torch.cuda, 'synchronize'):
                        torch.cuda.synchronize()
                        result["method"] = "cuda_aggressive_cleanup"
                except Exception as e:
                    result["warning"] = f"CUDA Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}"
            
            logger.info(f"üíæ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ ÏôÑÎ£å: {result['method']}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            return {
                "success": False,
                "error": str(e),
                "device": self.device
            }
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """Î©îÎ™®Î¶¨ ÌÜµÍ≥Ñ Î∞òÌôò"""
        try:
            stats = {
                "device": self.device,
                "system_memory": {
                    "total_gb": round(psutil.virtual_memory().total / (1024**3), 2),
                    "available_gb": round(psutil.virtual_memory().available / (1024**3), 2),
                    "used_percent": psutil.virtual_memory().percent
                },
                "timestamp": time.time()
            }
            
            # ÎîîÎ∞îÏù¥Ïä§Î≥Ñ Î©îÎ™®Î¶¨ Ï†ïÎ≥¥
            if self.device == "mps":
                stats["mps_memory"] = {
                    "unified_memory": True,
                    "total_gb": stats["system_memory"]["total_gb"],
                    "available_gb": stats["system_memory"]["available_gb"],
                    "note": "MPS uses unified memory system"
                }
            elif self.device == "cuda" and torch.cuda.is_available():
                try:
                    stats["gpu_memory"] = {
                        "allocated_gb": torch.cuda.memory_allocated(0) / (1024**3),
                        "reserved_gb": torch.cuda.memory_reserved(0) / (1024**3),
                        "total_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    }
                except Exception as e:
                    stats["gpu_memory_error"] = str(e)
            
            return stats
            
        except Exception as e:
            logger.error(f"Î©îÎ™®Î¶¨ ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {
                "device": self.device,
                "error": str(e),
                "timestamp": time.time()
            }

# ===============================================================
# üîß Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
# ===============================================================

def check_memory_available(device: Optional[str] = None, min_gb: float = 1.0) -> Dict[str, Any]:
    """Î©îÎ™®Î¶¨ ÏÇ¨Ïö© Í∞ÄÎä• ÏÉÅÌÉú ÌôïÏù∏"""
    try:
        current_device = device or gpu_config.device
        
        # ÏãúÏä§ÌÖú Î©îÎ™®Î¶¨ ÌôïÏù∏
        vm = psutil.virtual_memory()
        system_memory = {
            "total_gb": round(vm.total / (1024**3), 2),
            "available_gb": round(vm.available / (1024**3), 2),
            "used_gb": round(vm.used / (1024**3), 2),
            "percent_used": vm.percent
        }
        
        result = {
            "device": current_device,
            "system_memory": system_memory,
            "is_available": system_memory["available_gb"] >= min_gb,
            "min_required_gb": min_gb,
            "timestamp": time.time(),
            "pytorch_version": torch.__version__,
            "is_m3_max": gpu_config.is_m3_max
        }
        
        # ÎîîÎ∞îÏù¥Ïä§Î≥Ñ Î©îÎ™®Î¶¨ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        if current_device == "mps":
            result["mps_memory"] = {
                "unified_memory": True,
                "total_gb": system_memory["total_gb"],
                "available_gb": system_memory["available_gb"],
                "note": "MPS uses unified memory system",
                "neural_engine_available": gpu_config.is_m3_max
            }
        elif current_device == "cuda" and torch.cuda.is_available():
            try:
                gpu_props = torch.cuda.get_device_properties(0)
                gpu_memory = gpu_props.total_memory / (1024**3)
                gpu_allocated = torch.cuda.memory_allocated(0) / (1024**3)
                
                result["gpu_memory"] = {
                    "total_gb": round(gpu_memory, 2),
                    "allocated_gb": round(gpu_allocated, 2),
                    "available_gb": round(gpu_memory - gpu_allocated, 2),
                    "device_name": gpu_props.name
                }
                
                result["is_available"] = result["is_available"] and (gpu_memory - gpu_allocated) >= min_gb
            except Exception as e:
                result["gpu_memory_error"] = str(e)
        
        logger.info(f"üìä Î©îÎ™®Î¶¨ ÌôïÏù∏ ÏôÑÎ£å: {current_device} ({system_memory['available_gb']:.1f}GB ÏÇ¨Ïö© Í∞ÄÎä•)")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Î©îÎ™®Î¶¨ ÌôïÏù∏ Ïã§Ìå®: {e}")
        return {
            "device": device or "unknown",
            "error": str(e),
            "is_available": False,
            "min_required_gb": min_gb,
            "timestamp": time.time()
        }

def optimize_memory(device: Optional[str] = None, aggressive: bool = False) -> Dict[str, Any]:
    """Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî"""
    try:
        return gpu_config.cleanup_memory(aggressive)
    except Exception as e:
        logger.error(f"Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
        return {
            "success": False,
            "error": str(e),
            "device": device or "unknown"
        }

def get_optimal_settings() -> Dict[str, Any]:
    """ÏµúÏ†Å ÏÑ§Ï†ï Î∞òÌôò"""
    return gpu_config.optimization_settings.copy()

def get_device_capabilities() -> Dict[str, Any]:
    """ÎîîÎ∞îÏù¥Ïä§ Í∏∞Îä• Î∞òÌôò"""
    return {
        "device": gpu_config.device,
        "device_name": gpu_config.device_name,
        "supports_fp16": gpu_config.optimization_settings["fp16_enabled"],
        "max_batch_size": gpu_config.optimization_settings["batch_size"] * 2,
        "recommended_image_size": (768, 768) if gpu_config.is_m3_max else (512, 512),
        "supports_8step_pipeline": True,
        "optimization_level": gpu_config.optimization_settings["optimization_level"],
        "memory_gb": gpu_config.memory_gb,
        "pytorch_version": torch.__version__,
        "is_m3_max": gpu_config.is_m3_max,
        "supports_neural_engine": gpu_config.is_m3_max,
        "supports_metal_shaders": gpu_config.device == "mps",
        "unified_memory_optimization": gpu_config.optimization_settings["unified_memory_optimization"],
        "high_resolution_processing": gpu_config.optimization_settings["high_resolution_processing"],
        "pipeline_parallelism": gpu_config.optimization_settings["pipeline_parallelism"]
    }

def get_memory_info(device: Optional[str] = None) -> Dict[str, Any]:
    """Î©îÎ™®Î¶¨ Ï†ïÎ≥¥ Î∞òÌôò"""
    try:
        return gpu_config.get_memory_stats()
    except Exception as e:
        logger.error(f"Î©îÎ™®Î¶¨ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {e}")
        return {
            "device": device or "unknown",
            "error": str(e),
            "available": False
        }

# ===============================================================
# üîß Ìò∏ÌôòÏÑ± Ìï®ÏàòÎì§
# ===============================================================

@lru_cache(maxsize=1)
def get_gpu_config() -> GPUManager:
    """GPU ÏÑ§Ï†ï Îß§ÎãàÏ†Ä Î∞òÌôò"""
    return gpu_config

def get_device_config() -> Dict[str, Any]:
    """ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï Î∞òÌôò"""
    return gpu_config.get_device_config()

def get_model_config() -> Dict[str, Any]:
    """Î™®Îç∏ ÏÑ§Ï†ï Î∞òÌôò"""
    return gpu_config.get_model_config()

def get_device_info() -> Dict[str, Any]:
    """ÎîîÎ∞îÏù¥Ïä§ Ï†ïÎ≥¥ Î∞òÌôò"""
    return gpu_config.get_device_info()

def get_device() -> str:
    """ÌòÑÏû¨ ÎîîÎ∞îÏù¥Ïä§ Î∞òÌôò"""
    return gpu_config.get_device()

def is_m3_max() -> bool:
    """M3 Max Ïó¨Î∂Ä ÌôïÏù∏"""
    return gpu_config.is_m3_max

def get_device_name() -> str:
    """ÎîîÎ∞îÏù¥Ïä§ Ïù¥Î¶Ñ Î∞òÌôò"""
    return gpu_config.get_device_name()

def apply_optimizations() -> bool:
    """ÏµúÏ†ÅÌôî ÏÑ§Ï†ï Ï†ÅÏö©"""
    try:
        if gpu_config.is_initialized:
            logger.info("‚úÖ GPU ÏµúÏ†ÅÌôî ÏÑ§Ï†ï Ïù¥ÎØ∏ Ï†ÅÏö©Îê®")
            return True
        
        logger.info("‚úÖ GPU ÏµúÏ†ÅÌôî ÏÑ§Ï†ï Ï†ÅÏö© ÏôÑÎ£å")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå GPU ÏµúÏ†ÅÌôî ÏÑ§Ï†ï Ï†ÅÏö© Ïã§Ìå®: {e}")
        return False

# ===============================================================
# üîß Ï†ÑÏó≠ GPU ÏÑ§Ï†ï Îß§ÎãàÏ†Ä ÏÉùÏÑ±
# ===============================================================

# Ï†ÑÏó≠ GPU ÏÑ§Ï†ï Îß§ÎãàÏ†Ä ÏÉùÏÑ±
gpu_config = GPUManager()

# Ìé∏ÏùòÎ•º ÏúÑÌïú Ï†ÑÏó≠ Î≥ÄÏàòÎì§
DEVICE = gpu_config.device
DEVICE_NAME = gpu_config.device_name
DEVICE_TYPE = gpu_config.device_type
MODEL_CONFIG = gpu_config.model_config
DEVICE_INFO = gpu_config.device_info
IS_M3_MAX = gpu_config.is_m3_max

# ===============================================================
# üîß Ï¥àÍ∏∞Ìôî ÏôÑÎ£å Î°úÍπÖ
# ===============================================================

logger.info("‚úÖ GPU ÏÑ§Ï†ï ÏôÑÏ†Ñ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
logger.info(f"üîß ÎîîÎ∞îÏù¥Ïä§: {DEVICE}")
logger.info(f"üçé M3 Max: {'‚úÖ' if IS_M3_MAX else '‚ùå'}")
logger.info(f"üß† Î©îÎ™®Î¶¨: {gpu_config.memory_gb:.1f}GB")
logger.info(f"‚öôÔ∏è ÏµúÏ†ÅÌôî: {gpu_config.optimization_settings['optimization_level']}")
logger.info(f"üéØ PyTorch: {torch.__version__}")

# M3 Max ÏÑ∏Î∂Ä Ï†ïÎ≥¥
if IS_M3_MAX:
    logger.info("üçé M3 Max 128GB ÏµúÏ†ÅÌôî ÌôúÏÑ±Ìôî:")
    logger.info(f"  - Neural Engine: ‚úÖ")
    logger.info(f"  - Metal Performance Shaders: ‚úÖ")
    logger.info(f"  - ÌÜµÌï© Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî: ‚úÖ")
    logger.info(f"  - 8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ ÏµúÏ†ÅÌôî: ‚úÖ")
    logger.info(f"  - Í≥†Ìï¥ÏÉÅÎèÑ Ï≤òÎ¶¨: ‚úÖ")
    logger.info(f"  - Î∞∞Ïπò ÌÅ¨Í∏∞: {MODEL_CONFIG['batch_size']}")
    logger.info(f"  - Ï†ïÎ∞ÄÎèÑ: {MODEL_CONFIG['dtype']}")
    logger.info(f"  - ÎèôÏãú ÏÑ∏ÏÖò: {gpu_config.optimization_settings['concurrent_sessions']}")
    logger.info(f"  - Î©îÎ™®Î¶¨ ÌíÄ: {gpu_config.optimization_settings['memory_pool_gb']}GB")
    logger.info(f"  - Ï∫êÏãú ÌÅ¨Í∏∞: {gpu_config.optimization_settings['cache_size_gb']}GB")

# 8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ ÏµúÏ†ÅÌôî ÏÉÅÌÉú
pipeline_count = len(gpu_config.pipeline_optimizations)
if pipeline_count > 0:
    logger.info(f"‚öôÔ∏è 8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ ÏµúÏ†ÅÌôî: {pipeline_count}Í∞ú Îã®Í≥Ñ ÏÑ§Ï†ïÎê®")

# Î©îÎ™®Î¶¨ ÏÉÅÌÉú ÌôïÏù∏
memory_check = check_memory_available(min_gb=1.0)
if memory_check.get('is_available', False):
    logger.info(f"üíæ Î©îÎ™®Î¶¨ ÏÉÅÌÉú: {memory_check['system_memory']['available_gb']:.1f}GB ÏÇ¨Ïö© Í∞ÄÎä•")

# ===============================================================
# üîß Export Î¶¨Ïä§Ìä∏
# ===============================================================

__all__ = [
    # Ï£ºÏöî Í∞ùÏ≤¥Îì§
    'gpu_config', 'DEVICE', 'DEVICE_NAME', 'DEVICE_TYPE', 
    'MODEL_CONFIG', 'DEVICE_INFO', 'IS_M3_MAX',
    
    # ÌïµÏã¨ Ìï®ÏàòÎì§
    'get_gpu_config', 'get_device_config', 'get_model_config', 'get_device_info',
    'get_device', 'get_device_name', 'is_m3_max', 'get_optimal_settings', 'get_device_capabilities',
    'apply_optimizations',
    
    # Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ Ìï®ÏàòÎì§
    'check_memory_available', 'optimize_memory', 'get_memory_info',
    
    # ÌÅ¥ÎûòÏä§Îì§
    'GPUManager', 'HardwareDetector'
]

logger.info("üéâ GPU ÏÑ§Ï†ï Î™®Îìà Î°úÎìú ÏôÑÎ£å!")
logger.info("üìã Ï£ºÏöî ÌäπÏßï:")
logger.info("  - ÏôÑÏ†ÑÌïú GPU ÏÑ§Ï†ï Îß§ÎãàÏ†Ä")
logger.info("  - M3 Max 128GB ÌäπÌôî ÏµúÏ†ÅÌôî")
logger.info("  - 8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ ÏµúÏ†ÅÌôî")
logger.info("  - 100% Ìò∏ÌôòÏÑ± Î≥¥Ïû•")
logger.info("  - Ìè¥Î∞± Ï†úÍ±∞, Ïã§Ï†ú ÏûëÎèô ÏΩîÎìúÎßå Ïú†ÏßÄ")

if IS_M3_MAX:
    logger.info("üöÄ M3 Max 128GB ÏµúÏ†ÅÌôî ÏôÑÎ£å - ÏµúÍ≥† ÏÑ±Îä• Î™®Îìú ÌôúÏÑ±Ìôî!")
else:
    logger.info(f"‚úÖ {DEVICE_NAME} ÏµúÏ†ÅÌôî ÏôÑÎ£å - ÏïàÏ†ïÏ†Å ÎèôÏûë Î™®Îìú ÌôúÏÑ±Ìôî!")