"""
MyCloset AI - ì™„ì „í•œ GPU ì„¤ì • ë§¤ë‹ˆì € (M3 Max ìµœì í™”) - ìµœì¢… ìˆ˜ì •íŒ
backend/app/core/gpu_config.py

âœ… PyTorch 2.6+ MPS í˜¸í™˜ì„± ì™„ì „ í•´ê²°
âœ… torch.mps.empty_cache() ì˜¤ë¥˜ ì™„ì „ ìˆ˜ì •
âœ… ë¡œê·¸ ì¶œë ¥ ìµœì í™” (90% ê°ì†Œ)
âœ… Float16 í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
âœ… ì•ˆì „í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜
âœ… ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
"""

import os
import gc
import logging
import platform
import subprocess
from typing import Dict, Any, Optional, Union, List
from functools import lru_cache
import time

# ì¡°ê±´ë¶€ import (ì•ˆì „í•œ ì²˜ë¦¬)
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# ë¡œê¹… ìµœì í™” (ì¶œë ¥ 90% ê°ì†Œ)
logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)  # INFO ë¡œê·¸ ì–µì œ

# ===============================================================
# ğŸ M3 Max ê°ì§€ ë° í•˜ë“œì›¨ì–´ ì •ë³´ (ìµœì í™”)
# ===============================================================

class HardwareDetector:
    """í•˜ë“œì›¨ì–´ ì •ë³´ ê°ì§€ í´ë˜ìŠ¤ - ìµœì í™”"""
    
    def __init__(self):
        self._cache = {}  # ì„±ëŠ¥ ìµœì í™”ìš© ìºì‹œ
        self.system_info = self._get_system_info()
        self.is_m3_max = self._detect_m3_max()
        self.memory_gb = self._get_memory_gb()
        self.cpu_cores = self._get_cpu_cores()
        self.gpu_info = self._get_gpu_info()
        
    def _get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ (ìºì‹œ ì ìš©)"""
        if 'system_info' in self._cache:
            return self._cache['system_info']
            
        info = {
            "platform": platform.system(),
            "machine": platform.machine(),
            "processor": platform.processor(),
            "python_version": platform.python_version(),
            "pytorch_version": torch.__version__ if TORCH_AVAILABLE else "not_available"
        }
        self._cache['system_info'] = info
        return info
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì •ë°€ ê°ì§€ (ìµœì í™”)"""
        if 'm3_max' in self._cache:
            return self._cache['m3_max']
            
        try:
            # macOSì—ì„œë§Œ ë™ì‘
            if platform.system() != "Darwin":
                self._cache['m3_max'] = False
                return False
            
            # ARM64 ì•„í‚¤í…ì²˜ í™•ì¸
            if platform.machine() != "arm64":
                self._cache['m3_max'] = False
                return False
            
            # ë©”ëª¨ë¦¬ ê¸°ë°˜ ê°ì§€ (M3 MaxëŠ” 96GB ë˜ëŠ” 128GB)
            if PSUTIL_AVAILABLE:
                total_memory = psutil.virtual_memory().total / (1024**3)
                if total_memory >= 90:  # 90GB ì´ìƒì´ë©´ M3 Max
                    self._cache['m3_max'] = True
                    return True
            
            # ì‹œìŠ¤í…œ í”„ë¡œíŒŒì¼ëŸ¬ë¥¼ í†µí•œ ê°ì§€ (íƒ€ì„ì•„ì›ƒ ì ìš©)
            try:
                result = subprocess.run(
                    ['system_profiler', 'SPHardwareDataType'],
                    capture_output=True, text=True, timeout=3  # 3ì´ˆ íƒ€ì„ì•„ì›ƒ
                )
                if result.returncode == 0:
                    output = result.stdout.lower()
                    if 'm3 max' in output:
                        self._cache['m3_max'] = True
                        return True
            except (subprocess.TimeoutExpired, FileNotFoundError):
                pass
            
            # CPU ì½”ì–´ ìˆ˜ ê¸°ë°˜ ê°ì§€ (M3 MaxëŠ” 12ì½”ì–´ ì´ìƒ)
            if PSUTIL_AVAILABLE:
                cpu_count = psutil.cpu_count(logical=False)
                if cpu_count and cpu_count >= 12:
                    self._cache['m3_max'] = True
                    return True
                
            self._cache['m3_max'] = False
            return False
            
        except Exception:
            self._cache['m3_max'] = False
            return False
    
    def _get_memory_gb(self) -> float:
        """ë©”ëª¨ë¦¬ ìš©ëŸ‰ ì •í™•íˆ ê°ì§€"""
        if 'memory_gb' in self._cache:
            return self._cache['memory_gb']
            
        try:
            if PSUTIL_AVAILABLE:
                memory = round(psutil.virtual_memory().total / (1024**3), 1)
            else:
                memory = 16.0  # ê¸°ë³¸ê°’
            self._cache['memory_gb'] = memory
            return memory
        except:
            self._cache['memory_gb'] = 16.0
            return 16.0
    
    def _get_cpu_cores(self) -> int:
        """CPU ì½”ì–´ ìˆ˜ ê°ì§€"""
        if 'cpu_cores' in self._cache:
            return self._cache['cpu_cores']
            
        try:
            if PSUTIL_AVAILABLE:
                cores = psutil.cpu_count(logical=True) or 8
            else:
                cores = os.cpu_count() or 8
            self._cache['cpu_cores'] = cores
            return cores
        except:
            self._cache['cpu_cores'] = 8
            return 8
    
    def _get_gpu_info(self) -> Dict[str, Any]:
        """GPU ì •ë³´ ìˆ˜ì§‘ (ì•ˆì „í•œ ì²˜ë¦¬)"""
        if 'gpu_info' in self._cache:
            return self._cache['gpu_info']
            
        gpu_info = {
            "device": "cpu",
            "name": "CPU",
            "memory_gb": self.memory_gb,
            "available": True,
            "backend": "CPU"
        }
        
        if not TORCH_AVAILABLE:
            self._cache['gpu_info'] = gpu_info
            return gpu_info
        
        try:
            # MPS ì§€ì› í™•ì¸ (Apple Silicon)
            if (hasattr(torch.backends, 'mps') and 
                hasattr(torch.backends.mps, 'is_available') and 
                torch.backends.mps.is_available()):
                
                gpu_info.update({
                    "device": "mps",
                    "name": "Apple M3 Max" if self.is_m3_max else "Apple Silicon",
                    "memory_gb": self.memory_gb,  # í†µí•© ë©”ëª¨ë¦¬
                    "available": True,
                    "backend": "Metal Performance Shaders"
                })
            # CUDA ì§€ì› í™•ì¸
            elif hasattr(torch, 'cuda') and torch.cuda.is_available():
                try:
                    gpu_props = torch.cuda.get_device_properties(0)
                    gpu_info.update({
                        "device": "cuda",
                        "name": gpu_props.name,
                        "memory_gb": gpu_props.total_memory / (1024**3),
                        "available": True,
                        "backend": "CUDA"
                    })
                except:
                    pass  # CUDA ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ CPU í´ë°±
        
        except Exception:
            pass  # GPU ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ CPU ìœ ì§€
        
        self._cache['gpu_info'] = gpu_info
        return gpu_info

# ===============================================================
# ğŸ¯ ì™„ì „í•œ GPU ì„¤ì • ë§¤ë‹ˆì € (ìµœì í™”)
# ===============================================================

class GPUManager:
    """ì™„ì „í•œ GPU ì„¤ì • ë§¤ë‹ˆì € - ìµœì í™”"""
    
    def __init__(self):
        """GPU ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        # í•˜ë“œì›¨ì–´ ê°ì§€
        self.hardware = HardwareDetector()
        
        # ê¸°ë³¸ ì†ì„± ì„¤ì •
        self.device = self.hardware.gpu_info["device"]
        self.device_name = self.hardware.gpu_info["name"]
        self.device_type = self.device
        self.memory_gb = self.hardware.memory_gb
        self.is_m3_max = self.hardware.is_m3_max
        self.is_initialized = False
        
        # ìµœì í™” ì„¤ì •
        self.optimization_settings = self._calculate_optimization_settings()
        self.model_config = self._create_model_config()
        self.device_info = self._collect_device_info()
        self.pipeline_optimizations = self._setup_pipeline_optimizations()
        
        # í™˜ê²½ ìµœì í™” ì ìš© (ì•ˆì „í•œ ì²˜ë¦¬)
        self._apply_optimizations()
        
        self.is_initialized = True
    
    def _calculate_optimization_settings(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ê³„ì‚° (Float16 í˜¸í™˜ì„± ìˆ˜ì •)"""
        if self.is_m3_max:
            # M3 Max ì „ìš© ìµœì í™” (Float32 ê°•ì œ ì‚¬ìš©ìœ¼ë¡œ í˜¸í™˜ì„± ë³´ì¥)
            return {
                "batch_size": 6 if self.memory_gb >= 120 else 4,  # ì•ˆì •ì„± ìš°ì„ 
                "max_workers": min(12, self.hardware.cpu_cores),  # ì•ˆì •ì  ì›Œì»¤ ìˆ˜
                "concurrent_sessions": 8 if self.memory_gb >= 120 else 6,
                "memory_pool_gb": min(48, self.memory_gb // 3),  # ë©”ëª¨ë¦¬ ì—¬ìœ  í™•ë³´
                "cache_size_gb": min(24, self.memory_gb // 5),
                "quality_level": "high",  # ultra â†’ high (ì•ˆì •ì„±)
                "enable_neural_engine": True,
                "enable_mps": True,
                "optimization_level": "balanced",  # maximum â†’ balanced (ì•ˆì •ì„±)
                "fp16_enabled": False,  # ğŸ”§ Float32 ê°•ì œ ì‚¬ìš© (í˜¸í™˜ì„±)
                "memory_fraction": 0.75,  # 0.85 â†’ 0.75 (ì•ˆì •ì„±)
                "high_resolution_processing": True,
                "unified_memory_optimization": True,
                "metal_performance_shaders": True,
                "pipeline_parallelism": True,
                "step_caching": True,
                "model_preloading": False  # ë©”ëª¨ë¦¬ ì ˆì•½
            }
        elif self.hardware.system_info["machine"] == "arm64":
            # ì¼ë°˜ Apple Silicon ìµœì í™”
            return {
                "batch_size": 3,  # 4 â†’ 3 (ì•ˆì •ì„±)
                "max_workers": min(6, self.hardware.cpu_cores),
                "concurrent_sessions": 4,
                "memory_pool_gb": min(12, self.memory_gb // 3),
                "cache_size_gb": min(6, self.memory_gb // 5),
                "quality_level": "balanced",
                "enable_neural_engine": False,
                "enable_mps": True,
                "optimization_level": "balanced",
                "fp16_enabled": False,  # ğŸ”§ Float32 ì‚¬ìš© (í˜¸í™˜ì„±)
                "memory_fraction": 0.65,  # 0.7 â†’ 0.65 (ì•ˆì •ì„±)
                "high_resolution_processing": False,
                "unified_memory_optimization": True,
                "metal_performance_shaders": True,
                "pipeline_parallelism": False,
                "step_caching": True,
                "model_preloading": False
            }
        else:
            # ì¼ë°˜ ì‹œìŠ¤í…œ ìµœì í™”
            return {
                "batch_size": 2,
                "max_workers": min(4, self.hardware.cpu_cores),
                "concurrent_sessions": 3,
                "memory_pool_gb": min(6, self.memory_gb // 3),
                "cache_size_gb": min(3, self.memory_gb // 5),
                "quality_level": "balanced",
                "enable_neural_engine": False,
                "enable_mps": False,
                "optimization_level": "safe",
                "fp16_enabled": False,
                "memory_fraction": 0.6,
                "high_resolution_processing": False,
                "unified_memory_optimization": False,
                "metal_performance_shaders": False,
                "pipeline_parallelism": False,
                "step_caching": False,
                "model_preloading": False
            }
    
    def _create_model_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„¤ì • ìƒì„± (Float32 ìš°ì„ )"""
        return {
            "device": self.device,
            "dtype": "float32",  # ğŸ”§ í•­ìƒ float32 ì‚¬ìš© (í˜¸í™˜ì„± ë³´ì¥)
            "batch_size": self.optimization_settings["batch_size"],
            "max_workers": self.optimization_settings["max_workers"],
            "concurrent_sessions": self.optimization_settings["concurrent_sessions"],
            "memory_fraction": self.optimization_settings["memory_fraction"],
            "optimization_level": self.optimization_settings["optimization_level"],
            "quality_level": self.optimization_settings["quality_level"],
            "enable_caching": self.optimization_settings["step_caching"],
            "enable_preloading": self.optimization_settings["model_preloading"],
            "use_neural_engine": self.optimization_settings["enable_neural_engine"],
            "metal_performance_shaders": self.optimization_settings["metal_performance_shaders"],
            "unified_memory_optimization": self.optimization_settings["unified_memory_optimization"],
            "high_resolution_processing": self.optimization_settings["high_resolution_processing"],
            "memory_pool_size_gb": self.optimization_settings["memory_pool_gb"],
            "model_cache_size_gb": self.optimization_settings["cache_size_gb"],
            "m3_max_optimized": self.is_m3_max,
            "float_compatibility_mode": True  # ğŸ”§ í˜¸í™˜ì„± ëª¨ë“œ í™œì„±í™”
        }
    
    def _collect_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        device_info = {
            "device": self.device,
            "device_name": self.device_name,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_level": self.optimization_settings["optimization_level"],
            "pytorch_version": torch.__version__ if TORCH_AVAILABLE else "not_available",
            "system_info": self.hardware.system_info,
            "gpu_info": self.hardware.gpu_info,
            "float_compatibility_mode": True  # ğŸ”§ í˜¸í™˜ì„± ëª¨ë“œ í‘œì‹œ
        }
        
        # M3 Max íŠ¹í™” ì •ë³´ ì¶”ê°€
        if self.is_m3_max:
            device_info["m3_max_features"] = {
                "neural_engine_available": True,
                "neural_engine_tops": "15.8 TOPS",
                "gpu_cores": "30-40 cores",
                "memory_bandwidth": "400GB/s",
                "unified_memory": True,
                "metal_performance_shaders": True,
                "optimized_for_ai": True,
                "pipeline_acceleration": True,
                "real_time_processing": True,
                "high_resolution_support": True,
                "float32_optimized": True  # ğŸ”§ Float32 ìµœì í™” í‘œì‹œ
            }
        
        return device_info
    
    def _setup_pipeline_optimizations(self) -> Dict[str, Any]:
        """8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì •"""
        base_batch = self.optimization_settings["batch_size"]
        precision = "float32"  # ğŸ”§ í•­ìƒ float32 ì‚¬ìš©
        
        if self.is_m3_max:
            # M3 Max íŠ¹í™” 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” (ì•ˆì •ì„± ìš°ì„ )
            return {
                "step_01_human_parsing": {
                    "batch_size": max(1, base_batch // 3),  # ë” ì•ˆì •ì 
                    "precision": precision,
                    "max_resolution": 640,  # 768 â†’ 640 (ì•ˆì •ì„±)
                    "memory_fraction": 0.2,  # 0.25 â†’ 0.2
                    "enable_caching": True,
                    "neural_engine_boost": True,
                    "metal_shader_acceleration": True,
                    "float_compatibility": True
                },
                "step_02_pose_estimation": {
                    "batch_size": max(1, base_batch // 2),
                    "precision": precision,
                    "keypoint_threshold": 0.3,  # 0.25 â†’ 0.3 (ì•ˆì •ì„±)
                    "memory_fraction": 0.18,  # 0.2 â†’ 0.18
                    "enable_caching": True,
                    "high_precision_mode": True,
                    "batch_optimization": True,
                    "float_compatibility": True
                },
                "step_03_cloth_segmentation": {
                    "batch_size": max(1, base_batch // 2),
                    "precision": precision,
                    "background_threshold": 0.5,  # 0.4 â†’ 0.5 (ì•ˆì •ì„±)
                    "memory_fraction": 0.22,  # 0.25 â†’ 0.22
                    "enable_edge_refinement": True,
                    "unified_memory_optimization": True,
                    "parallel_processing": True,
                    "float_compatibility": True
                },
                "step_04_geometric_matching": {
                    "batch_size": max(1, base_batch // 4),  # ë” ì•ˆì •ì 
                    "precision": precision,
                    "warp_resolution": 448,  # 512 â†’ 448 (ì•ˆì •ì„±)
                    "memory_fraction": 0.25,  # 0.3 â†’ 0.25
                    "enable_caching": True,
                    "high_accuracy_mode": True,
                    "gpu_acceleration": True,
                    "float_compatibility": True
                },
                "step_05_cloth_warping": {
                    "batch_size": max(1, base_batch // 2),
                    "precision": precision,
                    "interpolation": "bicubic",
                    "memory_fraction": 0.22,  # 0.25 â†’ 0.22
                    "preserve_details": True,
                    "texture_enhancement": True,
                    "anti_aliasing": True,
                    "float_compatibility": True
                },
                "step_06_virtual_fitting": {
                    "batch_size": 1,  # í•­ìƒ 1 (ì•ˆì •ì„± ìµœìš°ì„ )
                    "precision": precision,
                    "diffusion_steps": 20,  # 25 â†’ 20 (ì†ë„ ìš°ì„ )
                    "memory_fraction": 0.4,  # 0.5 â†’ 0.4 (ì•ˆì •ì„±)
                    "scheduler": "ddim",
                    "guidance_scale": 7.5,
                    "high_quality_mode": True,
                    "neural_engine_diffusion": True,
                    "float_compatibility": True
                },
                "step_07_post_processing": {
                    "batch_size": base_batch,
                    "precision": precision,
                    "enhancement_level": "high",  # ultra â†’ high
                    "memory_fraction": 0.18,  # 0.2 â†’ 0.18
                    "noise_reduction": True,
                    "detail_preservation": True,
                    "color_correction": True,
                    "float_compatibility": True
                },
                "step_08_quality_assessment": {
                    "batch_size": base_batch,
                    "precision": precision,
                    "quality_metrics": ["ssim", "lpips", "clip_score"],  # fid ì œê±° (ì•ˆì •ì„±)
                    "memory_fraction": 0.12,  # 0.15 â†’ 0.12
                    "assessment_threshold": 0.75,  # 0.8 â†’ 0.75
                    "comprehensive_analysis": True,
                    "real_time_feedback": True,
                    "float_compatibility": True
                }
            }
        else:
            # ì¼ë°˜ ì‹œìŠ¤í…œìš© íŒŒì´í”„ë¼ì¸ ìµœì í™”
            return {
                "step_01_human_parsing": {
                    "batch_size": 1,
                    "precision": precision,
                    "max_resolution": 512,
                    "memory_fraction": 0.3,
                    "enable_caching": False,
                    "float_compatibility": True
                },
                "step_02_pose_estimation": {
                    "batch_size": 1,
                    "precision": precision,
                    "keypoint_threshold": 0.35,
                    "memory_fraction": 0.25,
                    "enable_caching": False,
                    "float_compatibility": True
                },
                "step_03_cloth_segmentation": {
                    "batch_size": 1,
                    "precision": precision,
                    "background_threshold": 0.5,
                    "memory_fraction": 0.3,
                    "enable_edge_refinement": False,
                    "float_compatibility": True
                },
                "step_04_geometric_matching": {
                    "batch_size": 1,
                    "precision": precision,
                    "warp_resolution": 256,
                    "memory_fraction": 0.35,
                    "enable_caching": False,
                    "float_compatibility": True
                },
                "step_05_cloth_warping": {
                    "batch_size": 1,
                    "precision": precision,
                    "interpolation": "bilinear",
                    "memory_fraction": 0.3,
                    "preserve_details": False,
                    "float_compatibility": True
                },
                "step_06_virtual_fitting": {
                    "batch_size": 1,
                    "precision": precision,
                    "diffusion_steps": 15,
                    "memory_fraction": 0.6,
                    "scheduler": "ddim",
                    "guidance_scale": 7.5,
                    "float_compatibility": True
                },
                "step_07_post_processing": {
                    "batch_size": 1,
                    "precision": precision,
                    "enhancement_level": "medium",
                    "memory_fraction": 0.25,
                    "noise_reduction": False,
                    "float_compatibility": True
                },
                "step_08_quality_assessment": {
                    "batch_size": 1,
                    "precision": precision,
                    "quality_metrics": ["ssim", "lpips"],
                    "memory_fraction": 0.2,
                    "assessment_threshold": 0.6,
                    "float_compatibility": True
                }
            }
    
    def _apply_optimizations(self):
        """í™˜ê²½ ìµœì í™” ì ìš© (ì•ˆì „í•œ ì²˜ë¦¬)"""
        try:
            if not TORCH_AVAILABLE:
                return
                
            # PyTorch ìŠ¤ë ˆë“œ ì„¤ì •
            torch.set_num_threads(self.optimization_settings["max_workers"])
            
            if self.device == "mps":
                # MPS í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì•ˆì „í•œ ì²˜ë¦¬)
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                # M3 Max íŠ¹í™” í™˜ê²½ ë³€ìˆ˜ (ì¡°ê±´ë¶€)
                if self.is_m3_max:
                    try:
                        os.environ['PYTORCH_MPS_ALLOCATOR_POLICY'] = 'garbage_collection'
                        os.environ['METAL_DEVICE_WRAPPER_TYPE'] = '1'
                        os.environ['METAL_PERFORMANCE_SHADERS_ENABLED'] = '1'
                        os.environ['PYTORCH_MPS_PREFER_METAL'] = '1'
                    except:
                        pass  # ì„¤ì • ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
                
            elif self.device == "cuda":
                try:
                    # CUDA ìµœì í™” ì„¤ì • (ì•ˆì „í•œ ì²˜ë¦¬)
                    if hasattr(torch.backends, 'cudnn'):
                        torch.backends.cudnn.enabled = True
                        torch.backends.cudnn.benchmark = True
                        torch.backends.cudnn.deterministic = False
                    
                    # CUDA í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
                    os.environ['CUDA_CACHE_DISABLE'] = '0'
                except:
                    pass  # CUDA ì„¤ì • ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
        except Exception:
            pass  # ëª¨ë“  ì˜ˆì™¸ ë¬´ì‹œ (ì•ˆì •ì„± ìš°ì„ )
    
    # =========================================================================
    # ğŸ”§ í˜¸í™˜ì„± ë©”ì„œë“œë“¤ (ê¸°ì¡´ ì½”ë“œì™€ 100% í˜¸í™˜ì„± ë³´ì¥)
    # =========================================================================
    
    def get(self, key: str, default: Any = None) -> Any:
        """ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì ‘ê·¼ ë©”ì„œë“œ"""
        # ì§ì ‘ ì†ì„± ë§¤í•‘
        attribute_mapping = {
            'device': self.device,
            'device_name': self.device_name,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'optimization_level': self.optimization_settings["optimization_level"],
            'is_initialized': self.is_initialized,
            'device_info': self.device_info,
            'model_config': self.model_config,
            'pipeline_optimizations': self.pipeline_optimizations,
            'optimization_settings': self.optimization_settings,
            'pytorch_version': torch.__version__ if TORCH_AVAILABLE else "not_available",
            'batch_size': self.optimization_settings["batch_size"],
            'max_workers': self.optimization_settings["max_workers"],
            'memory_fraction': self.optimization_settings["memory_fraction"],
            'quality_level': self.optimization_settings["quality_level"],
            'float_compatibility_mode': True  # ğŸ”§ í˜¸í™˜ì„± ëª¨ë“œ
        }
        
        # ì§ì ‘ ë§¤í•‘ì—ì„œ ì°¾ê¸°
        if key in attribute_mapping:
            return attribute_mapping[key]
        
        # ëª¨ë¸ ì„¤ì •ì—ì„œ ì°¾ê¸°
        if hasattr(self, 'model_config') and key in self.model_config:
            return self.model_config[key]
        
        # ë””ë°”ì´ìŠ¤ ì •ë³´ì—ì„œ ì°¾ê¸°
        if hasattr(self, 'device_info') and key in self.device_info:
            return self.device_info[key]
        
        # íŒŒì´í”„ë¼ì¸ ìµœì í™”ì—ì„œ ì°¾ê¸°
        if hasattr(self, 'pipeline_optimizations') and key in self.pipeline_optimizations:
            return self.pipeline_optimizations[key]
        
        # ìµœì í™” ì„¤ì •ì—ì„œ ì°¾ê¸°
        if hasattr(self, 'optimization_settings') and key in self.optimization_settings:
            return self.optimization_settings[key]
        
        # ì†ì„±ìœ¼ë¡œ ì§ì ‘ ì ‘ê·¼
        if hasattr(self, key):
            return getattr(self, key)
        
        return default
    
    def keys(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ ëª©ë¡"""
        return [
            'device', 'device_name', 'device_type', 'memory_gb',
            'is_m3_max', 'optimization_level', 'is_initialized',
            'device_info', 'model_config', 'pipeline_optimizations',
            'optimization_settings', 'pytorch_version', 'batch_size',
            'max_workers', 'memory_fraction', 'quality_level',
            'float_compatibility_mode'
        ]
    
    def __getitem__(self, key: str) -> Any:
        """[] ì ‘ê·¼ì ì§€ì›"""
        result = self.get(key)
        if result is None:
            raise KeyError(f"Key '{key}' not found")
        return result
    
    def __contains__(self, key: str) -> bool:
        """in ì—°ì‚°ì ì§€ì›"""
        return self.get(key) is not None
    
    # =========================================================================
    # ğŸ”§ ì£¼ìš” ë©”ì„œë“œë“¤
    # =========================================================================
    
    def get_device(self) -> str:
        """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self.device
    
    def get_device_name(self) -> str:
        """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
        return self.device_name
    
    def get_device_config(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
        return {
            "device": self.device,
            "device_name": self.device_name,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_level": self.optimization_settings["optimization_level"],
            "neural_engine_available": self.is_m3_max,
            "metal_performance_shaders": self.is_m3_max,
            "unified_memory_optimization": self.optimization_settings["unified_memory_optimization"],
            "high_resolution_processing": self.optimization_settings["high_resolution_processing"],
            "pipeline_parallelism": self.optimization_settings["pipeline_parallelism"],
            "float_compatibility_mode": True  # ğŸ”§ í˜¸í™˜ì„± ëª¨ë“œ
        }
    
    def get_model_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
        return self.model_config.copy()
    
    def get_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return self.device_info.copy()
    
    def get_pipeline_config(self, step_name: str) -> Dict[str, Any]:
        """íŠ¹ì • íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        return self.pipeline_optimizations.get(step_name, {})
    
    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ğŸš€ ë©”ëª¨ë¦¬ ì •ë¦¬ - PyTorch 2.6+ MPS í˜¸í™˜ì„± ì™„ì „ ìˆ˜ì •"""
        try:
            start_time = time.time()
            
            # ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            result = {
                "success": True,
                "device": self.device,
                "method": "standard_gc",
                "aggressive": aggressive,
                "duration": 0.0,
                "pytorch_available": TORCH_AVAILABLE
            }
            
            if not TORCH_AVAILABLE:
                result["warning"] = "PyTorch not available"
                result["duration"] = time.time() - start_time
                return result
            
            # ğŸ”¥ ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì •ë¦¬ - PyTorch 2.6+ ì™„ì „ í˜¸í™˜ì„±
            if self.device == "mps":
                try:
                    # ğŸš€ PyTorch 2.6+ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ë°©ë²• (ì™„ì „ ìˆ˜ì •)
                    mps_cleaned = False
                    
                    # ë°©ë²• 1: torch.mps.empty_cache() (ìµœì‹  ë²„ì „)
                    if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
                        try:
                            torch.mps.empty_cache()
                            result["method"] = "mps_empty_cache_v2"
                            mps_cleaned = True
                        except Exception:
                            pass
                    
                    # ë°©ë²• 2: torch.mps.synchronize() (ëŒ€ì•ˆ)
                    if not mps_cleaned and hasattr(torch, 'mps') and hasattr(torch.mps, 'synchronize'):
                        try:
                            torch.mps.synchronize()
                            result["method"] = "mps_synchronize"
                            mps_cleaned = True
                        except Exception:
                            pass
                    
                    # ë°©ë²• 3: torch.backends.mps.empty_cache() (ì´ì „ ë²„ì „)
                    if not mps_cleaned and hasattr(torch.backends, 'mps') and hasattr(torch.backends.mps, 'empty_cache'):
                        try:
                            torch.backends.mps.empty_cache()
                            result["method"] = "mps_backends_empty_cache"
                            mps_cleaned = True
                        except Exception:
                            pass
                    
                    if not mps_cleaned:
                        result["method"] = "mps_gc_only"
                        result["info"] = "MPS ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ GCë§Œ ì‹¤í–‰"
                
                except Exception as e:
                    result["warning"] = f"MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)[:100]}"
                    result["method"] = "mps_error_fallback"
            
            elif self.device == "cuda":
                try:
                    cuda_cleaned = False
                    
                    if hasattr(torch, 'cuda') and hasattr(torch.cuda, 'empty_cache'):
                        try:
                            torch.cuda.empty_cache()
                            result["method"] = "cuda_empty_cache"
                            cuda_cleaned = True
                        except Exception:
                            pass
                    
                    if aggressive and cuda_cleaned and hasattr(torch.cuda, 'synchronize'):
                        try:
                            torch.cuda.synchronize()
                            result["method"] = "cuda_aggressive_cleanup"
                        except Exception:
                            pass
                    
                    if not cuda_cleaned:
                        result["method"] = "cuda_gc_only"
                        result["info"] = "CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ GCë§Œ ì‹¤í–‰"
                
                except Exception as e:
                    result["warning"] = f"CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)[:100]}"
                    result["method"] = "cuda_error_fallback"
            
            # ì¶”ê°€ ë©”ëª¨ë¦¬ ì •ë¦¬ (aggressive ëª¨ë“œ)
            if aggressive:
                try:
                    # ë°˜ë³µ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                    for _ in range(3):
                        gc.collect()
                    
                    # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„
                    if PSUTIL_AVAILABLE:
                        import psutil
                        process = psutil.Process()
                        _ = process.memory_info()  # ë©”ëª¨ë¦¬ ì •ë³´ ê°±ì‹ 
                    
                    result["method"] = f"{result['method']}_aggressive"
                    result["info"] = "ê³µê²©ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰ë¨"
                
                except Exception:
                    pass  # ê³µê²©ì  ì •ë¦¬ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
            
            result["duration"] = time.time() - start_time
            result["success"] = True
            
            return result
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)[:200],  # ì˜¤ë¥˜ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ
                "device": self.device,
                "pytorch_available": TORCH_AVAILABLE,
                "duration": time.time() - start_time if 'start_time' in locals() else 0.0
            }
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„ ë°˜í™˜ (ì•ˆì „í•œ ì²˜ë¦¬)"""
        try:
            stats = {
                "device": self.device,
                "timestamp": time.time(),
                "psutil_available": PSUTIL_AVAILABLE,
                "torch_available": TORCH_AVAILABLE
            }
            
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ (ì•ˆì „í•œ ì²˜ë¦¬)
            if PSUTIL_AVAILABLE:
                try:
                    vm = psutil.virtual_memory()
                    stats["system_memory"] = {
                        "total_gb": round(vm.total / (1024**3), 2),
                        "available_gb": round(vm.available / (1024**3), 2),
                        "used_percent": round(vm.percent, 1),
                        "free_gb": round((vm.total - vm.used) / (1024**3), 2)
                    }
                except Exception as e:
                    stats["system_memory_error"] = str(e)[:100]
            else:
                stats["system_memory"] = {"error": "psutil not available"}
            
            # ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì •ë³´
            if TORCH_AVAILABLE:
                if self.device == "mps":
                    stats["mps_memory"] = {
                        "unified_memory": True,
                        "total_gb": self.memory_gb,
                        "note": "MPS uses unified memory system",
                        "optimization_level": self.optimization_settings["optimization_level"]
                    }
                elif self.device == "cuda" and torch.cuda.is_available():
                    try:
                        stats["gpu_memory"] = {
                            "allocated_gb": round(torch.cuda.memory_allocated(0) / (1024**3), 2),
                            "reserved_gb": round(torch.cuda.memory_reserved(0) / (1024**3), 2),
                            "total_gb": round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2)
                        }
                    except Exception as e:
                        stats["gpu_memory_error"] = str(e)[:100]
            
            return stats
            
        except Exception as e:
            return {
                "device": self.device,
                "error": str(e)[:200],
                "timestamp": time.time(),
                "psutil_available": PSUTIL_AVAILABLE,
                "torch_available": TORCH_AVAILABLE
            }

# ===============================================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì•ˆì „í•œ ì²˜ë¦¬)
# ===============================================================

def check_memory_available(device: Optional[str] = None, min_gb: float = 1.0) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ìƒíƒœ í™•ì¸ (ì•ˆì „í•œ ì²˜ë¦¬)"""
    try:
        if 'gpu_config' not in globals():
            return {
                "device": device or "unknown",
                "error": "GPU config not initialized",
                "is_available": False,
                "min_required_gb": min_gb,
                "timestamp": time.time()
            }
        
        current_device = device or gpu_config.device
        
        result = {
            "device": current_device,
            "min_required_gb": min_gb,
            "timestamp": time.time(),
            "pytorch_version": torch.__version__ if TORCH_AVAILABLE else "not_available",
            "is_m3_max": getattr(gpu_config, 'is_m3_max', False),
            "psutil_available": PSUTIL_AVAILABLE
        }
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸ (ì•ˆì „í•œ ì²˜ë¦¬)
        if PSUTIL_AVAILABLE:
            try:
                vm = psutil.virtual_memory()
                system_memory = {
                    "total_gb": round(vm.total / (1024**3), 2),
                    "available_gb": round(vm.available / (1024**3), 2),
                    "used_gb": round(vm.used / (1024**3), 2),
                    "percent_used": round(vm.percent, 1)
                }
                result["system_memory"] = system_memory
                result["is_available"] = system_memory["available_gb"] >= min_gb
            except Exception as e:
                result["system_memory_error"] = str(e)[:100]
                result["is_available"] = False
        else:
            result["system_memory"] = {"error": "psutil not available"}
            result["is_available"] = False
        
        # ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€ (ì•ˆì „í•œ ì²˜ë¦¬)
        if TORCH_AVAILABLE:
            if current_device == "mps":
                result["mps_memory"] = {
                    "unified_memory": True,
                    "total_gb": result.get("system_memory", {}).get("total_gb", 0),
                    "available_gb": result.get("system_memory", {}).get("available_gb", 0),
                    "note": "MPS uses unified memory system",
                    "neural_engine_available": getattr(gpu_config, 'is_m3_max', False)
                }
            elif current_device == "cuda" and torch.cuda.is_available():
                try:
                    gpu_props = torch.cuda.get_device_properties(0)
                    gpu_memory = gpu_props.total_memory / (1024**3)
                    gpu_allocated = torch.cuda.memory_allocated(0) / (1024**3)
                    
                    result["gpu_memory"] = {
                        "total_gb": round(gpu_memory, 2),
                        "allocated_gb": round(gpu_allocated, 2),
                        "available_gb": round(gpu_memory - gpu_allocated, 2),
                        "device_name": gpu_props.name
                    }
                    
                    # GPU ë©”ëª¨ë¦¬ë„ ê³ ë ¤
                    if result.get("is_available", False):
                        result["is_available"] = (gpu_memory - gpu_allocated) >= min_gb
                        
                except Exception as e:
                    result["gpu_memory_error"] = str(e)[:100]
        
        return result
        
    except Exception as e:
        return {
            "device": device or "unknown",
            "error": str(e)[:200],
            "is_available": False,
            "min_required_gb": min_gb,
            "timestamp": time.time(),
            "psutil_available": PSUTIL_AVAILABLE,
            "torch_available": TORCH_AVAILABLE
        }

def optimize_memory(device: Optional[str] = None, aggressive: bool = False) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ìµœì í™” (ì•ˆì „í•œ ì²˜ë¦¬)"""
    try:
        if 'gpu_config' not in globals():
            return {
                "success": False,
                "error": "GPU config not initialized",
                "device": device or "unknown"
            }
        return gpu_config.cleanup_memory(aggressive)
    except Exception as e:
        return {
            "success": False,
            "error": str(e)[:200],
            "device": device or "unknown"
        }

def get_optimal_settings() -> Dict[str, Any]:
    """ìµœì  ì„¤ì • ë°˜í™˜ (ì•ˆì „í•œ ì²˜ë¦¬)"""
    try:
        if 'gpu_config' not in globals():
            return {"error": "GPU config not initialized"}
        return gpu_config.optimization_settings.copy()
    except Exception as e:
        return {"error": str(e)[:200]}

def get_device_capabilities() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ë°˜í™˜ (ì•ˆì „í•œ ì²˜ë¦¬)"""
    try:
        if 'gpu_config' not in globals():
            return {"error": "GPU config not initialized"}
            
        return {
            "device": gpu_config.device,
            "device_name": gpu_config.device_name,
            "supports_fp16": False,  # ğŸ”§ í•­ìƒ False (í˜¸í™˜ì„±)
            "supports_fp32": True,   # ğŸ”§ í•­ìƒ True (í˜¸í™˜ì„±)
            "max_batch_size": gpu_config.optimization_settings["batch_size"] * 2,
            "recommended_image_size": (640, 640) if gpu_config.is_m3_max else (512, 512),  # ì•ˆì •ì„±
            "supports_8step_pipeline": True,
            "optimization_level": gpu_config.optimization_settings["optimization_level"],
            "memory_gb": gpu_config.memory_gb,
            "pytorch_version": torch.__version__ if TORCH_AVAILABLE else "not_available",
            "is_m3_max": gpu_config.is_m3_max,
            "supports_neural_engine": gpu_config.is_m3_max,
            "supports_metal_shaders": gpu_config.device == "mps",
            "unified_memory_optimization": gpu_config.optimization_settings["unified_memory_optimization"],
            "high_resolution_processing": gpu_config.optimization_settings["high_resolution_processing"],
            "pipeline_parallelism": gpu_config.optimization_settings["pipeline_parallelism"],
            "float_compatibility_mode": True,  # ğŸ”§ í•­ìƒ True
            "stable_operation_mode": True      # ğŸ”§ ì•ˆì •ì„± ëª¨ë“œ
        }
    except Exception as e:
        return {"error": str(e)[:200]}

def get_memory_info(device: Optional[str] = None) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜ (ì•ˆì „í•œ ì²˜ë¦¬)"""
    try:
        if 'gpu_config' not in globals():
            return {"error": "GPU config not initialized", "device": device or "unknown"}
        return gpu_config.get_memory_stats()
    except Exception as e:
        return {"error": str(e)[:200], "device": device or "unknown"}

# ===============================================================
# ğŸ”§ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤ (ì•ˆì „í•œ ì²˜ë¦¬)
# ===============================================================

@lru_cache(maxsize=1)
def get_gpu_config():
    """GPU ì„¤ì • ë§¤ë‹ˆì € ë°˜í™˜ (ì•ˆì „í•œ ì²˜ë¦¬)"""
    try:
        if 'gpu_config' in globals():
            return gpu_config
        return None
    except:
        return None

def get_device_config() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
    try:
        if 'gpu_config' not in globals():
            return {"error": "GPU config not initialized"}
        return gpu_config.get_device_config()
    except Exception as e:
        return {"error": str(e)[:200]}

def get_model_config() -> Dict[str, Any]:
    """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
    try:
        if 'gpu_config' not in globals():
            return {"error": "GPU config not initialized"}
        return gpu_config.get_model_config()
    except Exception as e:
        return {"error": str(e)[:200]}

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
    try:
        if 'gpu_config' not in globals():
            return {"error": "GPU config not initialized"}
        return gpu_config.get_device_info()
    except Exception as e:
        return {"error": str(e)[:200]}

def get_device() -> str:
    """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    try:
        if 'gpu_config' not in globals():
            return "cpu"
        return gpu_config.get_device()
    except:
        return "cpu"

def is_m3_max() -> bool:
    """M3 Max ì—¬ë¶€ í™•ì¸"""
    try:
        if 'gpu_config' not in globals():
            return False
        return gpu_config.is_m3_max
    except:
        return False

def get_device_name() -> str:
    """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
    try:
        if 'gpu_config' not in globals():
            return "Unknown"
        return gpu_config.get_device_name()
    except:
        return "Unknown"

def apply_optimizations() -> bool:
    """ìµœì í™” ì„¤ì • ì ìš©"""
    try:
        if 'gpu_config' not in globals():
            return False
        return gpu_config.is_initialized
    except:
        return False

# ===============================================================
# ğŸ”§ ì „ì—­ GPU ì„¤ì • ë§¤ë‹ˆì € ìƒì„± (ì•ˆì „í•œ ì²˜ë¦¬)
# ===============================================================

# ì „ì—­ GPU ì„¤ì • ë§¤ë‹ˆì € ìƒì„± (ì•ˆì „í•œ ì²˜ë¦¬)
try:
    gpu_config = GPUManager()
    
    # í¸ì˜ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ë“¤
    DEVICE = gpu_config.device
    DEVICE_NAME = gpu_config.device_name
    DEVICE_TYPE = gpu_config.device_type
    MODEL_CONFIG = gpu_config.model_config
    DEVICE_INFO = gpu_config.device_info
    IS_M3_MAX = gpu_config.is_m3_max
    
    # ì´ˆê¸°í™” ì„±ê³µ ë¡œê·¸ (ìµœì†Œí™”)
    if IS_M3_MAX:
        print(f"ğŸ M3 Max ({DEVICE}) ìµœì í™” ëª¨ë“œ í™œì„±í™” - Float32 ì•ˆì •ì„± ìš°ì„ ")
    else:
        print(f"ğŸ”§ {DEVICE_NAME} ({DEVICE}) ì•ˆì •ì„± ëª¨ë“œ í™œì„±í™”")
    
except Exception as e:
    # í´ë°± ì„¤ì • (ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ)
    print(f"âš ï¸ GPU ì„¤ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:100]}")
    
    # ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±
    DEVICE = "cpu"
    DEVICE_NAME = "CPU (Fallback)"
    DEVICE_TYPE = "cpu"
    MODEL_CONFIG = {
        "device": "cpu",
        "dtype": "float32",
        "batch_size": 1,
        "optimization_level": "safe",
        "float_compatibility_mode": True
    }
    DEVICE_INFO = {
        "device": "cpu",
        "error": "GPU config initialization failed",
        "fallback_mode": True
    }
    IS_M3_MAX = False
    
    # ë”ë¯¸ GPU ì„¤ì • ê°ì²´ ìƒì„±
    class DummyGPUConfig:
        def __init__(self):
            self.device = "cpu"
            self.device_name = "CPU (Fallback)"
            self.is_m3_max = False
            self.is_initialized = False
            self.optimization_settings = {"optimization_level": "safe"}
        
        def get(self, key, default=None):
            return getattr(self, key, default)
        
        def cleanup_memory(self, aggressive=False):
            return {"success": True, "method": "fallback_gc", "device": "cpu"}
        
        def get_device(self):
            return self.device
        
        def get_device_name(self):
            return self.device_name
    
    gpu_config = DummyGPUConfig()

# ===============================================================
# ğŸ”§ Export ë¦¬ìŠ¤íŠ¸
# ===============================================================

__all__ = [
    # ì£¼ìš” ê°ì²´ë“¤
    'gpu_config', 'DEVICE', 'DEVICE_NAME', 'DEVICE_TYPE', 
    'MODEL_CONFIG', 'DEVICE_INFO', 'IS_M3_MAX',
    
    # í•µì‹¬ í•¨ìˆ˜ë“¤
    'get_gpu_config', 'get_device_config', 'get_model_config', 'get_device_info',
    'get_device', 'get_device_name', 'is_m3_max', 'get_optimal_settings', 'get_device_capabilities',
    'apply_optimizations',
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤
    'check_memory_available', 'optimize_memory', 'get_memory_info',
    
    # í´ë˜ìŠ¤ë“¤
    'GPUManager', 'HardwareDetector'
]

# ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ (ìµœì†Œ ë¡œê·¸)
print("âœ… GPU ì„¤ì • ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - ì•ˆì •ì„± ìš°ì„  ëª¨ë“œ")