# app/core/gpu_config.py
"""
MyCloset AI - M3 Max 128GB ì™„ì „ ìµœì í™” GPU ì„¤ì •
GPUConfig ìƒì„±ì íŒŒë¼ë¯¸í„° ë¬¸ì œ í•´ê²°, ëˆ„ë½ëœ í´ë˜ìŠ¤ë“¤ ì¶”ê°€
"""

import os
import logging
import torch
import platform
from typing import Dict, Any, Optional, Union, List
from dataclasses import dataclass
from functools import lru_cache
import gc
import json
from pathlib import Path

logger = logging.getLogger(__name__)

# ===============================================================
# ğŸ”§ GPU ì„¤ì • ë°ì´í„° í´ë˜ìŠ¤ (íŒŒë¼ë¯¸í„° ë¬¸ì œ í•´ê²°)
# ===============================================================

@dataclass
class GPUConfig:
    """GPU ì„¤ì • ë°ì´í„° í´ë˜ìŠ¤ - ìƒì„±ì íŒŒë¼ë¯¸í„° ë¬¸ì œ í•´ê²°"""
    device: str = "mps"
    device_name: str = "Apple M3 Max"
    memory_gb: float = 128.0
    is_m3_max: bool = True
    optimization_level: str = "maximum"
    device_type: str = "apple_silicon"
    neural_engine_available: bool = True
    metal_performance_shaders: bool = True
    unified_memory_optimization: bool = True
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ê²€ì¦ ë° ì¡°ì •"""
        # M3 Maxê°€ ì•„ë‹Œ ê²½ìš° ì„¤ì • ì¡°ì •
        if not self.is_m3_max:
            self.neural_engine_available = False
            self.metal_performance_shaders = False
            self.optimization_level = "balanced"
            if self.memory_gb > 64:
                self.memory_gb = 16.0  # ì¼ë°˜ì ì¸ ê¸°ë³¸ê°’
    
    @classmethod
    def create_optimal(cls, device: str = None, auto_detect: bool = True) -> 'GPUConfig':
        """ìµœì  ì„¤ì •ìœ¼ë¡œ GPUConfig ìƒì„±"""
        if auto_detect:
            detector = M3MaxDetector()
            return cls(
                device=device or detector.get_optimal_device(),
                device_name=detector.get_device_name(),
                memory_gb=detector.memory_gb,
                is_m3_max=detector.is_m3_max,
                optimization_level="maximum" if detector.is_m3_max else "balanced",
                device_type="apple_silicon" if detector.is_apple_silicon else "generic",
                neural_engine_available=detector.is_m3_max,
                metal_performance_shaders=detector.is_m3_max,
                unified_memory_optimization=detector.is_m3_max
            )
        else:
            return cls()

# ===============================================================
# ğŸ M3 Max ê°ì§€ í´ë˜ìŠ¤
# ===============================================================

class M3MaxDetector:
    """M3 Max í™˜ê²½ ì •ë°€ ê°ì§€"""
    
    def __init__(self):
        self.platform_info = self._get_platform_info()
        self.is_apple_silicon = self._is_apple_silicon()
        self.memory_gb = self._get_memory_gb()
        self.cpu_cores = self._get_cpu_cores()
        self.is_m3_max = self._detect_m3_max()
        
    def _get_platform_info(self) -> Dict[str, str]:
        """í”Œë«í¼ ì •ë³´ ìˆ˜ì§‘"""
        return {
            "system": platform.system(),
            "machine": platform.machine(),
            "processor": platform.processor(),
            "platform": platform.platform(),
            "python_version": platform.python_version()
        }
    
    def _is_apple_silicon(self) -> bool:
        """Apple Silicon ê°ì§€"""
        return (self.platform_info["system"] == "Darwin" and 
                self.platform_info["machine"] == "arm64")
    
    def _get_memory_gb(self) -> float:
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìš©ëŸ‰(GB) ë°˜í™˜"""
        try:
            import psutil
            return round(psutil.virtual_memory().total / (1024**3), 1)
        except:
            return 8.0
    
    def _get_cpu_cores(self) -> int:
        """CPU ì½”ì–´ ìˆ˜ ë°˜í™˜"""
        try:
            import psutil
            return psutil.cpu_count(logical=False) or 4
        except:
            return 4
    
    def _detect_m3_max(self) -> bool:
        """M3 Max í™˜ê²½ ì •ë°€ ê°ì§€"""
        if not self.is_apple_silicon:
            return False
            
        # ë©”ëª¨ë¦¬ ê¸°ë°˜ íŒì • (ë” ì •í™•)
        if self.memory_gb >= 120:  # 128GB M3 Max
            return True
        elif self.memory_gb >= 90:  # 96GB M3 Max  
            return True
        elif self.cpu_cores >= 12:  # M3 MaxëŠ” 12ì½”ì–´ ì´ìƒ
            return True
            
        return False
    
    def get_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        try:
            import torch
            if torch.backends.mps.is_available() and self.is_apple_silicon:
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        except ImportError:
            return "cpu"
    
    def get_device_name(self) -> str:
        """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
        if self.is_m3_max:
            if self.memory_gb >= 120:
                return "Apple M3 Max (128GB)"
            else:
                return "Apple M3 Max (96GB)"
        elif self.is_apple_silicon:
            return "Apple Silicon"
        else:
            return "Unknown Device"
    
    def get_optimized_settings(self) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì„¤ì • ë°˜í™˜"""
        if self.is_m3_max:
            return {
                "batch_size": 8 if self.memory_gb >= 120 else 4,
                "max_workers": min(12, self.cpu_cores),
                "concurrent_sessions": 8,
                "memory_pool_gb": min(64, self.memory_gb // 2),
                "cache_size_gb": min(32, self.memory_gb // 4),
                "quality_level": "ultra",
                "enable_neural_engine": True,
                "enable_mps": True,
                "optimization_level": "maximum"
            }
        elif self.is_apple_silicon:
            return {
                "batch_size": 2,
                "max_workers": min(4, self.cpu_cores),
                "concurrent_sessions": 4,
                "memory_pool_gb": min(16, self.memory_gb // 2),
                "cache_size_gb": min(8, self.memory_gb // 4),
                "quality_level": "high",
                "enable_neural_engine": False,
                "enable_mps": True,
                "optimization_level": "balanced"
            }
        else:
            return {
                "batch_size": 1,
                "max_workers": 2,
                "concurrent_sessions": 2,
                "memory_pool_gb": 4,
                "cache_size_gb": 2,
                "quality_level": "balanced",
                "enable_neural_engine": False,
                "enable_mps": False,
                "optimization_level": "safe"
            }

# ===============================================================
# ğŸ¯ M3 Max GPU ê´€ë¦¬ì (ë©”ì¸ í´ë˜ìŠ¤)
# ===============================================================

class M3MaxGPUManager:
    """M3 Max 128GB ì „ìš© GPU ê´€ë¦¬ì"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.detector = M3MaxDetector()
        self.device = None
        self.device_name = ""
        self.device_type = ""
        self.memory_gb = 0.0
        self.is_m3_max = False
        self.optimization_level = "balanced"
        self.device_info = {}
        self.model_config = {}
        self.is_initialized = False
        
        # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ë³„ ìµœì í™” ì„¤ì •
        self.pipeline_optimizations = {}
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize()
    
    def _initialize(self):
        """GPU ì„¤ì • ì™„ì „ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”§ M3 Max GPU ì„¤ì • ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. í•˜ë“œì›¨ì–´ ì •ë³´ ì„¤ì •
            self._setup_hardware_info()
            
            # 2. ë””ë°”ì´ìŠ¤ ì„¤ì •
            self._setup_device()
            
            # 3. 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì •
            self._setup_pipeline_optimizations()
            
            # 4. ëª¨ë¸ ì„¤ì •
            self._setup_model_config()
            
            # 5. ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘
            self._collect_device_info()
            
            # 6. í™˜ê²½ ìµœì í™” ì ìš©
            self._apply_optimizations()
            
            self.is_initialized = True
            logger.info(f"ğŸš€ M3 Max GPU ì„¤ì • ì™„ë£Œ: {self.device}")
            
        except Exception as e:
            logger.error(f"âŒ GPU ì„¤ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._fallback_cpu_setup()
    
    def _setup_hardware_info(self):
        """í•˜ë“œì›¨ì–´ ì •ë³´ ì„¤ì •"""
        self.is_m3_max = self.detector.is_m3_max
        self.memory_gb = self.detector.memory_gb
        
        if self.is_m3_max:
            self.optimization_level = "maximum"
            logger.info(f"ğŸ M3 Max {self.memory_gb}GB ê°ì§€!")
        else:
            self.optimization_level = "balanced"
            logger.info(f"ğŸ’» ì¼ë°˜ í™˜ê²½ ê°ì§€: {self.memory_gb}GB")
    
    def _setup_device(self):
        """ë””ë°”ì´ìŠ¤ ì„¤ì • ë° MPS ìµœì í™”"""
        try:
            self.device = self.detector.get_optimal_device()
            self.device_name = self.detector.get_device_name()
            
            if self.device == "mps":
                self.device_type = "mps"
                
                # M3 Max íŠ¹í™” MPS í™˜ê²½ë³€ìˆ˜ ì„¤ì •
                if self.is_m3_max:
                    os.environ.update({
                        'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                        'PYTORCH_MPS_ALLOCATOR_POLICY': 'garbage_collection',
                        'METAL_DEVICE_WRAPPER_TYPE': '1',
                        'METAL_PERFORMANCE_SHADERS_ENABLED': '1'
                    })
                    logger.info("ğŸ M3 Max MPS í™˜ê²½ë³€ìˆ˜ ìµœì í™” ì ìš©")
                
                logger.info("ğŸ Apple Silicon MPS í™œì„±í™”")
                
            elif self.device == "cuda":
                self.device_type = "cuda"
                self.device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CUDA GPU"
                logger.info("ğŸš€ CUDA GPU ê°ì§€")
                
            else:
                self.device_type = "cpu"
                self.device_name = "CPU"
                logger.info("ğŸ’» CPU ëª¨ë“œ ì„¤ì •")
                
        except Exception as e:
            logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self._fallback_cpu_setup()
    
    def _setup_pipeline_optimizations(self):
        """8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ë³„ ìµœì í™” ì„¤ì •"""
        
        # ìµœì í™”ëœ ê¸°ë³¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        optimized = self.detector.get_optimized_settings()
        
        # 8ë‹¨ê³„ë³„ íŠ¹í™” ì„¤ì •
        self.pipeline_optimizations = {
            "step_01_human_parsing": {
                "batch_size": optimized["batch_size"] // 2,  # ë©”ëª¨ë¦¬ ì ˆì•½
                "precision": "float16" if self.device != "cpu" else "float32",
                "max_resolution": 512,
                "enable_segmentation_cache": True,
                "memory_fraction": 0.3
            },
            "step_02_pose_estimation": {
                "batch_size": optimized["batch_size"],
                "precision": "float16" if self.device != "cpu" else "float32",
                "keypoint_threshold": 0.3,
                "enable_pose_cache": True,
                "memory_fraction": 0.2
            },
            "step_03_cloth_segmentation": {
                "batch_size": optimized["batch_size"],
                "segmentation_model": "u2net",
                "background_threshold": 0.5,
                "enable_edge_refinement": True,
                "memory_fraction": 0.25
            },
            "step_04_geometric_matching": {
                "batch_size": optimized["batch_size"] // 2,
                "matching_algorithm": "optical_flow",
                "warp_resolution": 256,
                "enable_geometric_cache": True,
                "memory_fraction": 0.3
            },
            "step_05_cloth_warping": {
                "batch_size": optimized["batch_size"],
                "warp_method": "thin_plate_spline",
                "interpolation": "bilinear",
                "preserve_details": True,
                "memory_fraction": 0.25
            },
            "step_06_virtual_fitting": {
                "batch_size": optimized["batch_size"] // 4,  # ê°€ì¥ ë©”ëª¨ë¦¬ ì§‘ì•½ì 
                "diffusion_steps": 20 if self.is_m3_max else 15,
                "guidance_scale": 7.5,
                "enable_safety_checker": True,
                "scheduler": "ddim",
                "memory_fraction": 0.5
            },
            "step_07_post_processing": {
                "batch_size": optimized["batch_size"],
                "enhancement_level": "high" if self.is_m3_max else "medium",
                "noise_reduction": True,
                "color_correction": True,
                "memory_fraction": 0.2
            },
            "step_08_quality_assessment": {
                "batch_size": optimized["batch_size"],
                "quality_metrics": ["ssim", "lpips", "fid"],
                "assessment_threshold": 0.7,
                "enable_automatic_retry": True,
                "memory_fraction": 0.15
            }
        }
        
        logger.info("âš™ï¸ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì • ì™„ë£Œ")
    
    def _setup_model_config(self):
        """ëª¨ë¸ ì„¤ì • êµ¬ì„±"""
        optimized = self.detector.get_optimized_settings()
        
        base_config = {
            "device": self.device,
            "dtype": "float16" if self.device != "cpu" else "float32",
            "batch_size": optimized["batch_size"],
            "memory_fraction": 0.8,
            "optimization_level": self.optimization_level,
            "max_workers": optimized["max_workers"],
            "concurrent_sessions": optimized["concurrent_sessions"]
        }
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if self.is_m3_max:
            base_config.update({
                "use_neural_engine": True,
                "metal_performance_shaders": True,
                "unified_memory_optimization": True,
                "high_resolution_processing": True,
                "concurrent_pipeline_steps": 3,
                "memory_pool_size_gb": optimized["memory_pool_gb"],
                "model_cache_size_gb": optimized["cache_size_gb"],
                "intermediate_cache_gb": optimized["cache_size_gb"] // 2
            })
            logger.info("ğŸ M3 Max íŠ¹í™” ëª¨ë¸ ì„¤ì • ì ìš©")
        
        self.model_config = base_config
        logger.info(f"âš™ï¸ ëª¨ë¸ ì„¤ì • ì™„ë£Œ: ë°°ì¹˜={base_config['batch_size']}, ì •ë°€ë„={base_config['dtype']}")
    
    def _collect_device_info(self):
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        try:
            base_info = {
                "device": self.device,
                "device_name": self.device_name,
                "device_type": self.device_type,
                "platform": self.detector.platform_info["system"],
                "architecture": self.detector.platform_info["machine"],
                "pytorch_version": torch.__version__,
                "python_version": self.detector.platform_info["python_version"],
                "optimization_level": self.optimization_level,
                "total_memory_gb": self.memory_gb,
                "is_m3_max": self.is_m3_max
            }
            
            # M3 Max íŠ¹í™” ì •ë³´
            if self.is_m3_max:
                base_info.update({
                    "neural_engine_available": True,
                    "neural_engine_tops": "15.8 TOPS",
                    "gpu_cores": "30-40 cores", 
                    "memory_bandwidth": "400GB/s",
                    "unified_memory": True,
                    "metal_performance_shaders": True,
                    "optimized_for_pipeline": "8-step virtual fitting"
                })
            
            # MPS íŠ¹í™” ì •ë³´
            if self.device == "mps":
                base_info.update({
                    "mps_available": True,
                    "mps_fallback_enabled": True,
                    "metal_api_available": True
                })
            
            # CUDA ì •ë³´
            elif self.device == "cuda" and torch.cuda.is_available():
                base_info.update({
                    "cuda_version": torch.version.cuda,
                    "gpu_memory_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3),
                    "compute_capability": torch.cuda.get_device_capability(0)
                })
            
            self.device_info = base_info
            logger.info(f"â„¹ï¸ ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {self.device_name}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.device_info = {"device": self.device, "error": str(e)}
    
    def _apply_optimizations(self):
        """í™˜ê²½ ìµœì í™” ì ìš©"""
        try:
            # PyTorch ë©€í‹°ìŠ¤ë ˆë”© ì„¤ì •
            num_threads = self.detector.get_optimized_settings()["max_workers"]
            torch.set_num_threads(num_threads)
            
            # MPS ìµœì í™”
            if self.device == "mps":
                if self.is_m3_max:
                    os.environ['METAL_PERFORMANCE_SHADERS_ENABLED'] = '1'
                    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                logger.info("âœ… MPS ìµœì í™” ì ìš© ì™„ë£Œ")
            
            # CUDA ìµœì í™”
            elif self.device == "cuda":
                torch.backends.cudnn.enabled = True
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                logger.info("âœ… CUDA ìµœì í™” ì ìš© ì™„ë£Œ")
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
            gc.collect()
            
            logger.info(f"âœ… í™˜ê²½ ìµœì í™” ì™„ë£Œ (ìŠ¤ë ˆë“œ: {num_threads})")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
    
    def _fallback_cpu_setup(self):
        """CPU í´ë°± ì„¤ì •"""
        self.device = "cpu"
        self.device_type = "cpu"
        self.device_name = "CPU (Fallback)"
        self.is_m3_max = False
        self.optimization_level = "safe"
        
        self.model_config = {
            "device": "cpu",
            "dtype": "float32",
            "batch_size": 1,
            "memory_fraction": 0.5,
            "optimization_level": "safe"
        }
        
        self.device_info = {
            "device": "cpu",
            "device_name": "CPU (Fallback)",
            "error": "GPU initialization failed"
        }
        
        logger.warning("ğŸš¨ CPU í´ë°± ëª¨ë“œë¡œ ì„¤ì •ë¨")
    
    # ==========================================
    # ì ‘ê·¼ì ë©”ì„œë“œë“¤
    # ==========================================
    
    def get_device(self) -> str:
        """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self.device
    
    def get_device_name(self) -> str:
        """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
        return self.device_name
    
    def get_device_type(self) -> str:
        """ë””ë°”ì´ìŠ¤ íƒ€ì… ë°˜í™˜"""
        return self.device_type
    
    def get_recommended_batch_size(self) -> int:
        """ê¶Œì¥ ë°°ì¹˜ í¬ê¸° ë°˜í™˜"""
        return self.model_config.get('batch_size', 1)
    
    def get_recommended_precision(self) -> str:
        """ê¶Œì¥ ì •ë°€ë„ ë°˜í™˜"""
        return self.model_config.get('dtype', 'float32')
    
    def get_memory_fraction(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„ìœ¨ ë°˜í™˜"""
        return self.model_config.get('memory_fraction', 0.5)
    
    def setup_multiprocessing(self) -> int:
        """ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ ìˆ˜ ì„¤ì •"""
        return self.model_config.get('max_workers', 4)
    
    def get_device_config(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
        return GPUConfig(
            device=self.device,
            device_name=self.device_name,
            device_type=self.device_type,
            memory_gb=self.memory_gb,
            is_m3_max=self.is_m3_max,
            optimization_level=self.optimization_level,
            neural_engine_available=self.is_m3_max,
            metal_performance_shaders=self.is_m3_max,
            unified_memory_optimization=self.is_m3_max
        ).__dict__
    
    def get_model_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
        return self.model_config.copy()
    
    def get_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return self.device_info.copy()
    
    def get_pipeline_config(self, step_name: str) -> Dict[str, Any]:
        """íŠ¹ì • íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        return self.pipeline_optimizations.get(step_name, {})
    
    def get_all_pipeline_configs(self) -> Dict[str, Any]:
        """ëª¨ë“  íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        return self.pipeline_optimizations.copy()

# ===============================================================
# ğŸ¯ M3 Optimizer í´ë˜ìŠ¤ (ëˆ„ë½ëœ í´ë˜ìŠ¤ ì¶”ê°€)
# ===============================================================

class M3Optimizer:
    """M3 Max ì „ìš© ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self, device_name: str, memory_gb: float, is_m3_max: bool, optimization_level: str):
        """M3 ìµœì í™” ì´ˆê¸°í™”"""
        self.device_name = device_name
        self.memory_gb = memory_gb
        self.is_m3_max = is_m3_max
        self.optimization_level = optimization_level
        
        logger.info(f"ğŸ M3Optimizer ì´ˆê¸°í™”: {device_name}, {memory_gb}GB, {optimization_level}")
        
        if is_m3_max:
            self._apply_m3_max_optimizations()
    
    def _apply_m3_max_optimizations(self):
        """M3 Max ì „ìš© ìµœì í™” ì ìš©"""
        try:
            if torch.backends.mps.is_available():
                logger.info("ğŸ§  Neural Engine ìµœì í™” í™œì„±í™”")
                logger.info("âš™ï¸ Metal Performance Shaders í™œì„±í™”")
                
                # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”
                self.pipeline_config = {
                    "stages": 8,
                    "parallel_processing": True,
                    "batch_optimization": True,
                    "memory_pooling": True
                }
                
                logger.info("âš™ï¸ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì • ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def optimize_model(self, model):
        """ëª¨ë¸ ìµœì í™”"""
        if not self.is_m3_max:
            return model
            
        try:
            if hasattr(model, 'to'):
                model = model.to('mps')
            logger.info("âœ… ëª¨ë¸ M3 Max ìµœì í™” ì™„ë£Œ")
            return model
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model

# ===============================================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° ë° í¸ì˜ í•¨ìˆ˜ë“¤
# ===============================================================

def optimize_memory(device: Optional[str] = None, aggressive: bool = False) -> Dict[str, Any]:
    """M3 Max ìµœì í™”ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬"""
    try:
        import psutil
        
        current_device = device or gpu_config.device
        start_memory = psutil.virtual_memory().percent
        
        # ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        result = {
            "success": True,
            "device": current_device,
            "start_memory_percent": start_memory,
            "method": "standard_gc"
        }
        
        if current_device == "mps":
            try:
                # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (PyTorch ë²„ì „ë³„ ëŒ€ì‘)
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                    result["method"] = "mps_empty_cache"
                elif hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()
                    result["method"] = "mps_synchronize"
                
                if aggressive and gpu_config.is_m3_max:
                    torch.mps.synchronize()
                    gc.collect()
                    result["method"] = "m3_max_aggressive_cleanup"
                    result["aggressive"] = True
                
            except Exception as mps_error:
                logger.warning(f"MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
                result["mps_error"] = str(mps_error)
        
        elif current_device == "cuda":
            try:
                torch.cuda.empty_cache()
                if aggressive:
                    torch.cuda.synchronize()
                result["method"] = "cuda_empty_cache"
                if aggressive:
                    result["aggressive"] = True
            except Exception as cuda_error:
                logger.warning(f"CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {cuda_error}")
                result["cuda_error"] = str(cuda_error)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ìƒíƒœ
        end_memory = psutil.virtual_memory().percent
        memory_freed = start_memory - end_memory
        
        result.update({
            "end_memory_percent": end_memory,
            "memory_freed_percent": memory_freed,
            "m3_max_optimized": gpu_config.is_m3_max
        })
        
        if memory_freed > 0:
            logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ {memory_freed:.1f}% ì •ë¦¬ë¨ ({result['method']})")
        
        return result
        
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "device": device or "unknown",
            "method": "failed"
        }

def get_memory_status() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    try:
        import psutil
        
        memory = psutil.virtual_memory()
        
        status = {
            "total_gb": memory.total / (1024**3),
            "available_gb": memory.available / (1024**3),
            "used_gb": memory.used / (1024**3),
            "usage_percent": memory.percent,
            "status": "good",
            "is_m3_max": gpu_config.is_m3_max if 'gpu_config' in globals() else False
        }
        
        # ìƒíƒœ íŒì •
        if memory.percent < 40:
            status["status"] = "excellent"
        elif memory.percent < 70:
            status["status"] = "good"
        elif memory.percent < 85:
            status["status"] = "moderate"
        else:
            status["status"] = "high"
        
        return status
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

def check_device_compatibility() -> Dict[str, bool]:
    """ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í™•ì¸"""
    return {
        "mps_available": torch.backends.mps.is_available(),
        "cuda_available": torch.cuda.is_available(),
        "m3_max_detected": getattr(gpu_config, 'is_m3_max', False) if 'gpu_config' in globals() else False,
        "neural_engine_available": (
            torch.backends.mps.is_available() and 
            getattr(gpu_config, 'is_m3_max', False) if 'gpu_config' in globals() else False
        ),
        "8step_pipeline_ready": True
    }

def test_device_performance() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    config = get_gpu_config()
    
    try:
        import time
        
        device = torch.device(config.device)
        
        # í…ŒìŠ¤íŠ¸ í…ì„œ ìƒì„±
        x = torch.randn(1000, 1000, device=device)
        y = torch.randn(1000, 1000, device=device)
        
        # í–‰ë ¬ ê³±ì…ˆ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        start_time = time.time()
        for _ in range(100):
            z = torch.mm(x, y)
        
        if device.type == "mps":
            torch.mps.synchronize()
        elif device.type == "cuda":
            torch.cuda.synchronize()
        
        end_time = time.time()
        execution_time = end_time - start_time
        performance_score = 100 / execution_time
        
        return {
            "device": config.device,
            "device_name": config.device_name,
            "is_m3_max": config.is_m3_max,
            "performance_score": performance_score,
            "execution_time": execution_time,
            "operations_per_second": 100 / execution_time,
            "test_passed": True,
            "optimization_level": config.optimization_level
        }
        
    except Exception as e:
        logger.error(f"ë””ë°”ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {
            "device": config.device,
            "performance_score": 0.0,
            "execution_time": float('inf'),
            "operations_per_second": 0.0,
            "test_passed": False,
            "error": str(e)
        }

def get_optimal_settings() -> Dict[str, Any]:
    """ìµœì  ì„¤ì • ë°˜í™˜"""
    config = get_gpu_config()
    
    optimal_settings = {
        "device": config.device,
        "device_name": config.device_name,
        "batch_size": config.get_recommended_batch_size(),
        "precision": config.get_recommended_precision(),
        "memory_fraction": config.get_memory_fraction(),
        "max_workers": config.setup_multiprocessing(),
        "optimization_level": config.optimization_level,
        "is_m3_max": config.is_m3_max,
        "memory_gb": config.memory_gb
    }
    
    # M3 Max íŠ¹í™” ì„¤ì • ì¶”ê°€
    if config.is_m3_max:
        optimal_settings.update({
            "neural_engine_enabled": True,
            "mps_optimization": True,
            "metal_performance_shaders": True,
            "memory_bandwidth": "400GB/s",
            "concurrent_sessions": 8,
            "cache_size_gb": 16,
            "pipeline_parallel_steps": 3
        })
    
    return optimal_settings

def get_device_capabilities() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ë°˜í™˜"""
    config = get_gpu_config()
    
    capabilities = {
        "supports_fp16": config.device != "cpu",
        "supports_int8": True,
        "supports_compilation": config.device in ["cuda", "cpu"],
        "supports_parallel_inference": True,
        "max_batch_size": config.get_recommended_batch_size() * 2,
        "recommended_image_size": (512, 512) if config.is_m3_max else (256, 256),
        "supports_8step_pipeline": True
    }
    
    if config.device == "mps":
        capabilities.update({
            "supports_neural_engine": config.is_m3_max,
            "supports_metal_shaders": True,
            "mps_fallback_enabled": True,
            "unified_memory_optimization": config.is_m3_max
        })
    elif config.device == "cuda":
        capabilities.update({
            "cuda_version": torch.version.cuda if hasattr(torch.version, 'cuda') else None,
            "cudnn_enabled": torch.backends.cudnn.enabled,
            "tensor_cores_available": True
        })
    
    return capabilities

def optimize_for_inference() -> Dict[str, Any]:
    """ì¶”ë¡  ìµœì í™” ì„¤ì •"""
    config = get_gpu_config()
    
    inference_settings = {
        "torch_compile": config.device in ["cuda", "cpu"],
        "channels_last": config.device == "cuda",
        "mixed_precision": config.device in ["cuda", "mps"],
        "gradient_checkpointing": False,
        "enable_cudnn_benchmark": config.device == "cuda",
        "deterministic": False,
        "memory_efficient": True,
        "pipeline_optimization": True
    }
    
    # M3 Max íŠ¹í™” ì¶”ë¡  ìµœì í™”
    if config.is_m3_max:
        inference_settings.update({
            "mps_high_watermark": 0.0,
            "mps_allocator_policy": "garbage_collection",
            "metal_api_validation": False,
            "neural_engine_priority": "high",
            "parallel_pipeline_steps": 3,
            "aggressive_memory_optimization": True
        })
    
    return inference_settings

def apply_optimizations():
    """ìµœì í™” ì„¤ì • ì ìš©"""
    config = get_gpu_config()
    settings = get_optimal_settings()
    
    try:
        # PyTorch í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        if config.device == "mps":
            os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            
            if config.is_m3_max:
                os.environ["PYTORCH_MPS_ALLOCATOR_POLICY"] = "garbage_collection"
                os.environ["METAL_PERFORMANCE_SHADERS_ENABLED"] = "1"
        
        # ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì •
        torch.set_num_threads(settings["max_workers"])
        
        # CUDA ì„¤ì •
        if config.device == "cuda":
            torch.backends.cudnn.enabled = True
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
        
        logger.info("âœ… GPU ìµœì í™” ì„¤ì • ì ìš© ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"GPU ìµœì í™” ì„¤ì • ì ìš© ì‹¤íŒ¨: {e}")
        return False

def get_memory_info() -> Dict[str, float]:
    """ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
    config = get_gpu_config()
    
    if config.device == "cuda" and torch.cuda.is_available():
        return {
            "total_memory": torch.cuda.get_device_properties(0).total_memory / 1024**3,
            "allocated_memory": torch.cuda.memory_allocated() / 1024**3,
            "cached_memory": torch.cuda.memory_reserved() / 1024**3,
            "free_memory": (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3
        }
    else:
        try:
            import psutil
            memory = psutil.virtual_memory()
            return {
                "total_memory": memory.total / 1024**3,
                "available_memory": memory.available / 1024**3,
                "used_memory": memory.used / 1024**3,
                "memory_percent": memory.percent
            }
        except ImportError:
            return {"total_memory": 0.0, "available_memory": 0.0}

def check_memory_available(required_gb: float = 4.0) -> bool:
    """M3 Max ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    import psutil
    
    try:
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        
        logger.info(f"ğŸ’¾ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory.total / (1024**3):.1f}GB")
        logger.info(f"ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥: {available_gb:.1f}GB")
        logger.info(f"ğŸ’¾ ìš”êµ¬ì‚¬í•­: {required_gb:.1f}GB")
        
        # MPS ë©”ëª¨ë¦¬ í™•ì¸ (M3 Max)
        if torch.backends.mps.is_available():
            logger.info("ğŸ M3 Max Unified Memory ì‚¬ìš© ì¤‘")
            return available_gb >= required_gb
        
        return available_gb >= required_gb
        
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # ì•ˆì „í•˜ê²Œ True ë°˜í™˜

def cleanup_gpu_resources():
    """GPU ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    try:
        optimize_memory(aggressive=True)
        logger.info("âœ… GPU ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ GPU ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ==========================================
# ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì „ì—­ ë³€ìˆ˜
# ==========================================

def _initialize_gpu_optimizations():
    """GPU ìµœì í™” ì´ˆê¸°í™”"""
    try:
        apply_optimizations()
        logger.info("ğŸš€ GPU ìµœì í™” ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"GPU ìµœì í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ì „ì—­ GPU ì„¤ì • ë§¤ë‹ˆì € ìƒì„±
gpu_config = M3MaxGPUManager()

# í¸ì˜ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ë“¤
DEVICE = gpu_config.device
DEVICE_NAME = gpu_config.device_name
DEVICE_TYPE = gpu_config.device_type
MODEL_CONFIG = gpu_config.model_config
DEVICE_INFO = gpu_config.device_info
IS_M3_MAX = gpu_config.is_m3_max

# ==========================================
# ì£¼ìš” í•¨ìˆ˜ë“¤ (í˜¸í™˜ì„±)
# ==========================================

@lru_cache(maxsize=1)
def get_gpu_config() -> M3MaxGPUManager:
    """GPU ì„¤ì • ë§¤ë‹ˆì € ë°˜í™˜ (ìºì‹œë¨)"""
    return gpu_config

def configure_gpu() -> str:
    """GPU ì„¤ì • ë° ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return gpu_config.device

def get_optimal_device() -> str:
    """ìµœì  ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return gpu_config.device

def is_m3_max() -> bool:
    """M3 Max ì—¬ë¶€ í™•ì¸"""
    return gpu_config.is_m3_max

def get_device() -> str:
    """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return gpu_config.get_device()

def get_device_config() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_device_config()

def get_model_config() -> Dict[str, Any]:
    """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_model_config()

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
    return gpu_config.get_device_info()

def test_device() -> bool:
    """ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
    result = test_device_performance()
    return result.get("test_passed", False)

# í¸ì˜ í•¨ìˆ˜ë“¤
def is_gpu_available() -> bool:
    """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
    return gpu_config.device != "cpu"

def is_m3_max_available() -> bool:
    """M3 Max ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
    return gpu_config.is_m3_max

def get_recommended_settings() -> Dict[str, Any]:
    """ê¶Œì¥ ì„¤ì • ë°˜í™˜"""
    return get_optimal_settings()

def get_device_name() -> str:
    """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
    return gpu_config.device_name

def get_device_type() -> str:
    """ë””ë°”ì´ìŠ¤ íƒ€ì… ë°˜í™˜"""
    return gpu_config.device_type

def get_pipeline_config(step_name: str) -> Dict[str, Any]:
    """íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_pipeline_config(step_name)

def get_all_pipeline_configs() -> Dict[str, Any]:
    """ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_all_pipeline_configs()

# ëª¨ë“ˆ ë¡œë“œì‹œ ìë™ ìµœì í™” ì ìš©
_initialize_gpu_optimizations()

# ì´ˆê¸°í™” ë° ê²€ì¦
if gpu_config.is_initialized:
    test_success = test_device()
    if test_success:
        logger.info("âœ… M3 Max GPU ì„¤ì • ê²€ì¦ ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ GPU ì„¤ì • ê²€ì¦ ì‹¤íŒ¨")

# M3 Max ìƒíƒœ ë¡œê¹…
if gpu_config.is_m3_max:
    logger.info("ğŸ M3 Max 128GB ìµœì í™” í™œì„±í™”:")
    logger.info(f"  - Neural Engine: {'âœ…' if MODEL_CONFIG.get('use_neural_engine') else 'âŒ'}")
    logger.info(f"  - Metal Performance Shaders: {'âœ…' if MODEL_CONFIG.get('metal_performance_shaders') else 'âŒ'}")
    logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {MODEL_CONFIG.get('batch_size', 1)}")
    logger.info(f"  - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”: âœ…")
    logger.info(f"  - ë©”ëª¨ë¦¬ ëŒ€ì—­í­: {DEVICE_INFO.get('memory_bandwidth', 'N/A')}")

# ==========================================
# Export ë¦¬ìŠ¤íŠ¸
# ==========================================

__all__ = [
    # ì£¼ìš” ê°ì²´ë“¤
    'gpu_config', 'DEVICE', 'DEVICE_NAME', 'DEVICE_TYPE', 'MODEL_CONFIG', 'DEVICE_INFO', 'IS_M3_MAX',
    
    # í•µì‹¬ í•¨ìˆ˜ë“¤
    'get_gpu_config', 'configure_gpu', 'get_optimal_device', 'is_m3_max',
    'get_device', 'get_device_config', 'get_model_config', 'get_device_info',
    'test_device', 'cleanup_gpu_resources',
    
    # ìµœì í™” í•¨ìˆ˜ë“¤
    'get_optimal_settings', 'get_device_capabilities', 'optimize_for_inference',
    'apply_optimizations', 'get_memory_info', 'test_device_performance',
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤
    'optimize_memory', 'get_memory_status', 'check_device_compatibility',
    'check_memory_available',
    
    # í¸ì˜ í•¨ìˆ˜ë“¤
    'is_gpu_available', 'is_m3_max_available', 'get_recommended_settings',
    'get_device_name', 'get_device_type',
    
    # íŒŒì´í”„ë¼ì¸ íŠ¹í™” í•¨ìˆ˜ë“¤
    'get_pipeline_config', 'get_all_pipeline_configs',
    
    # í´ë˜ìŠ¤ë“¤
    'M3MaxGPUManager', 'GPUConfig', 'M3Optimizer', 'M3MaxDetector'
]