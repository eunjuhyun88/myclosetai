"""
ğŸ MyCloset AI - GPU ì„¤ì • ë§¤ë‹ˆì € (ì™„ì „ ê°œì„  ë²„ì „)
=======================================================

âœ… Clean Architecture ì ìš©
âœ… ë‹¨ì¼ ì±…ì„ ì›ì¹™ ì¤€ìˆ˜
âœ… Type Safety ì™„ë²½ ì ìš©
âœ… Conda í™˜ê²½ ì™„ì „ ì§€ì›
âœ… M3 Max ìµœì í™”
âœ… ìˆœí™˜ ì°¸ì¡° ë°©ì§€
âœ… ì—ëŸ¬ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
âœ… PyTorch 2.6+ í˜¸í™˜

í”„ë¡œì íŠ¸ êµ¬ì¡°:
backend/app/core/gpu_config.py (ì´ íŒŒì¼)
    â†“ ì‚¬ìš©ë¨
backend/app/core/config.py
backend/app/api/pipeline_routes.py
backend/app/ai_pipeline/utils/model_loader.py
backend/app/ai_pipeline/steps/*.py
"""

import os
import gc
import time
import platform
import threading
import subprocess
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, Protocol
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
import logging
import warnings

# =============================================================================
# ğŸ“¦ ì¡°ê±´ë¶€ ì„í¬íŠ¸ (ì•ˆì „í•œ ì²˜ë¦¬)
# =============================================================================

try:
    import psutil
    PSUTIL_AVAILABLE = True
    PSUTIL_VERSION = psutil.__version__
except ImportError:
    PSUTIL_AVAILABLE = False
    PSUTIL_VERSION = "not_available"

try:
    import torch
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
    TORCH_MAJOR, TORCH_MINOR = map(int, TORCH_VERSION.split('.')[:2])
except ImportError:
    TORCH_AVAILABLE = False
    TORCH_VERSION = "not_available"
    TORCH_MAJOR, TORCH_MINOR = 0, 0

try:
    import numpy as np
    NUMPY_AVAILABLE = True
    NUMPY_VERSION = np.__version__
except ImportError:
    NUMPY_AVAILABLE = False
    NUMPY_VERSION = "not_available"

# =============================================================================
# ğŸ”§ ë¡œê¹… ì„¤ì • (ë…¸ì´ì¦ˆ ìµœì†Œí™”)
# =============================================================================

# ê²½ê³  í•„í„°ë§
warnings.filterwarnings('ignore', category=UserWarning, module='torch')
warnings.filterwarnings('ignore', category=FutureWarning, module='torch')

logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)  # ë¡œê·¸ ë…¸ì´ì¦ˆ ê°ì†Œ

# =============================================================================
# ğŸ”§ ìƒìˆ˜ ë° ì—´ê±°í˜•
# =============================================================================

class DeviceType(Enum):
    """ë””ë°”ì´ìŠ¤ íƒ€ì…"""
    CPU = "cpu"
    MPS = "mps"
    CUDA = "cuda"
    AUTO = "auto"

class OptimizationLevel(Enum):
    """ìµœì í™” ë ˆë²¨"""
    SAFE = "safe"
    BALANCED = "balanced"
    PERFORMANCE = "performance"
    ULTRA = "ultra"

class PerformanceClass(Enum):
    """ì„±ëŠ¥ ë“±ê¸‰"""
    MINIMAL = "minimal"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    ULTRA_HIGH = "ultra_high"

# =============================================================================
# ğŸ“Š ë°ì´í„° í´ë˜ìŠ¤
# =============================================================================

@dataclass(frozen=True)
class SystemInfo:
    """ì‹œìŠ¤í…œ ì •ë³´"""
    platform: str
    machine: str
    processor: str
    python_version: str
    pytorch_version: str
    numpy_version: str
    is_m3_max: bool
    memory_gb: float
    cpu_cores: int
    detection_time: float = field(default_factory=time.time)

@dataclass(frozen=True)
class CondaEnvironment:
    """Conda í™˜ê²½ ì •ë³´"""
    is_conda: bool
    env_name: Optional[str] = None
    prefix: Optional[str] = None
    package_manager: str = "pip"
    python_version: str = ""
    optimization_level: str = "standard"

@dataclass(frozen=True)
class DeviceCapabilities:
    """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥"""
    device_type: str
    name: str
    memory_gb: float
    supports_fp16: bool = False
    supports_fp32: bool = True
    unified_memory: bool = False
    max_batch_size: int = 1
    recommended_image_size: Tuple[int, int] = (512, 512)
    tensor_cores: bool = False
    neural_engine: bool = False

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    memory_cleanup_count: int = 0
    memory_cleanup_avg_ms: float = 0.0
    error_count: int = 0
    uptime_seconds: float = 0.0
    last_reset: float = field(default_factory=time.time)

@dataclass(frozen=True)
class OptimizationProfile:
    """ìµœì í™” í”„ë¡œí•„"""
    batch_size: int
    max_workers: int
    memory_fraction: float
    quality_level: str
    dtype: str = "float32"
    mixed_precision: bool = False
    enable_checkpointing: bool = False

# =============================================================================
# ğŸ”§ í”„ë¡œí† ì½œ (ì¸í„°í˜ì´ìŠ¤)
# =============================================================================

class MemoryManagerProtocol(Protocol):
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ì í”„ë¡œí† ì½œ"""
    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]: ...
    def get_memory_info(self) -> Dict[str, Any]: ...

class DeviceDetectorProtocol(Protocol):
    """ë””ë°”ì´ìŠ¤ ê°ì§€ê¸° í”„ë¡œí† ì½œ"""
    def detect_device(self) -> str: ...
    def get_device_capabilities(self) -> DeviceCapabilities: ...

# =============================================================================
# ğŸ”§ ë©”ëª¨ë¦¬ ê´€ë¦¬ì (ë‹¨ì¼ ì±…ì„)
# =============================================================================

class MemoryManager:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ ì „ë‹´ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self._lock = threading.RLock()
        self._last_cleanup = 0.0
        self._cleanup_interval = 1.0  # 1ì´ˆ ê°„ê²©
        self._failure_count = 0
        self._max_failures = 3
    
    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë¦¬ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        with self._lock:
            current_time = time.time()
            
            # ì—°ì† ì‹¤íŒ¨ ì²´í¬
            if self._failure_count >= self._max_failures and not aggressive:
                return self._create_result(
                    success=True,
                    method="failure_threshold",
                    message=f"ì—°ì† ì‹¤íŒ¨ {self._failure_count}íšŒë¡œ ìŠ¤í‚µ"
                )
            
            # í˜¸ì¶œ ê°„ê²© ì²´í¬
            if current_time - self._last_cleanup < self._cleanup_interval and not aggressive:
                return self._create_result(
                    success=True,
                    method="throttled",
                    message=f"í˜¸ì¶œ ì œí•œ ({self._cleanup_interval}ì´ˆ ê°„ê²©)"
                )
            
            self._last_cleanup = current_time
            
            try:
                return self._perform_cleanup(aggressive)
            except Exception as e:
                self._failure_count += 1
                return self._create_result(
                    success=False,
                    method="error",
                    message=str(e)[:200]
                )
    
    def _perform_cleanup(self, aggressive: bool) -> Dict[str, Any]:
        """ì‹¤ì œ ë©”ëª¨ë¦¬ ì •ë¦¬ ìˆ˜í–‰"""
        start_time = time.time()
        methods = []
        
        # ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        collected = gc.collect()
        if collected > 0:
            methods.append(f"gc_collected_{collected}")
        
        if not TORCH_AVAILABLE:
            return self._create_result(
                success=True,
                method="gc_only",
                message=f"ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì™„ë£Œ ({collected}ê°œ)",
                duration=time.time() - start_time,
                methods=methods
            )
        
        # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„
        cleanup_success = False
        
        # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
        if self._try_mps_cleanup():
            methods.append("mps_cleanup")
            cleanup_success = True
        
        # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
        elif self._try_cuda_cleanup():
            methods.append("cuda_cleanup")
            cleanup_success = True
        
        # ì ê·¹ì  ì •ë¦¬
        if aggressive:
            for i in range(3):
                additional = gc.collect()
                if additional > 0:
                    collected += additional
                    methods.append(f"aggressive_gc_round_{i+1}")
                if i < 2:
                    time.sleep(0.05)  # 50ms ëŒ€ê¸°
        
        if cleanup_success:
            self._failure_count = 0  # ì„±ê³µ ì‹œ ì‹¤íŒ¨ ì¹´ìš´í„° ë¦¬ì…‹
        
        return self._create_result(
            success=True,
            method="comprehensive",
            message=f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ (ì´ {collected}ê°œ)",
            duration=time.time() - start_time,
            methods=methods,
            aggressive=aggressive
        )
    
    def _try_mps_cleanup(self) -> bool:
        """MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„"""
        try:
            # torch.mps.empty_cache() ì‹œë„
            if (hasattr(torch, 'mps') and 
                hasattr(torch.mps, 'empty_cache') and
                callable(torch.mps.empty_cache)):
                torch.mps.empty_cache()
                return True
            
            # torch.backends.mps.empty_cache() ì‹œë„
            if (hasattr(torch.backends, 'mps') and 
                hasattr(torch.backends.mps, 'empty_cache') and
                callable(torch.backends.mps.empty_cache)):
                torch.backends.mps.empty_cache()
                return True
            
            # torch.mps.synchronize() ì‹œë„
            if (hasattr(torch, 'mps') and 
                hasattr(torch.mps, 'synchronize') and
                callable(torch.mps.synchronize)):
                torch.mps.synchronize()
                return True
                
        except (AttributeError, RuntimeError, TypeError):
            pass
        
        return False
    
    def _try_cuda_cleanup(self) -> bool:
        """CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„"""
        try:
            if hasattr(torch, 'cuda') and torch.cuda.is_available():
                torch.cuda.empty_cache()
                return True
        except Exception:
            pass
        return False
    
    def _create_result(self, success: bool, method: str, message: str, 
                      duration: float = 0.0, **kwargs) -> Dict[str, Any]:
        """ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        return {
            "success": success,
            "method": method,
            "message": message,
            "duration": round(duration, 4),
            "timestamp": time.time(),
            **kwargs
        }
    
    def get_memory_info(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        info = {
            "timestamp": time.time(),
            "psutil_available": PSUTIL_AVAILABLE
        }
        
        if PSUTIL_AVAILABLE:
            try:
                memory = psutil.virtual_memory()
                info.update({
                    "total_gb": round(memory.total / (1024**3), 2),
                    "available_gb": round(memory.available / (1024**3), 2),
                    "used_gb": round(memory.used / (1024**3), 2),
                    "used_percent": round(memory.percent, 1),
                    "free_gb": round(memory.free / (1024**3), 2)
                })
            except Exception as e:
                info["error"] = str(e)[:100]
        else:
            info.update({
                "total_gb": 16.0,
                "available_gb": 12.0,
                "used_percent": 25.0,
                "fallback_mode": True
            })
        
        return info

# =============================================================================
# ğŸ”§ í•˜ë“œì›¨ì–´ ê°ì§€ê¸° (ë‹¨ì¼ ì±…ì„)
# =============================================================================

class HardwareDetector:
    """í•˜ë“œì›¨ì–´ ì •ë³´ ê°ì§€ ì „ë‹´ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self._cache: Dict[str, Any] = {}
        self._lock = threading.RLock()
        self._detection_time = time.time()
    
    @lru_cache(maxsize=1)
    def get_system_info(self) -> SystemInfo:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ (ìºì‹œë¨)"""
        return SystemInfo(
            platform=platform.system(),
            machine=platform.machine(),
            processor=platform.processor(),
            python_version=platform.python_version(),
            pytorch_version=TORCH_VERSION,
            numpy_version=NUMPY_VERSION,
            is_m3_max=self.is_m3_max(),
            memory_gb=self.get_memory_gb(),
            cpu_cores=self.get_cpu_cores(),
            detection_time=self._detection_time
        )
    
    def is_m3_max(self) -> bool:
        """M3 Max ê°ì§€ (ì •ë°€ ê²€ì‚¬)"""
        if "m3_max" in self._cache:
            return self._cache["m3_max"]
        
        with self._lock:
            try:
                # 1ì°¨: í”Œë«í¼ ì²´í¬
                if platform.system() != "Darwin" or platform.machine() != "arm64":
                    result = False
                else:
                    score = 0
                    
                    # 2ì°¨: ë©”ëª¨ë¦¬ ê¸°ë°˜ ê°ì§€ (M3 MaxëŠ” 96GB/128GB)
                    memory_gb = self.get_memory_gb()
                    if memory_gb >= 120:  # 128GB
                        score += 3
                    elif memory_gb >= 90:  # 96GB
                        score += 2
                    
                    # 3ì°¨: CPU ì½”ì–´ ìˆ˜ (M3 MaxëŠ” 16ì½”ì–´)
                    cpu_cores = self.get_cpu_cores()
                    if cpu_cores >= 16:
                        score += 2
                    elif cpu_cores >= 14:
                        score += 1
                    
                    # 4ì°¨: MPS ì§€ì› ì²´í¬
                    if self._check_mps_support():
                        score += 1
                    
                    result = score >= 3
                
                self._cache["m3_max"] = result
                return result
                
            except Exception as e:
                logger.debug(f"M3 Max ê°ì§€ ì‹¤íŒ¨: {e}")
                self._cache["m3_max"] = False
                return False
    
    def get_memory_gb(self) -> float:
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)"""
        if "memory_gb" in self._cache:
            return self._cache["memory_gb"]
        
        try:
            if PSUTIL_AVAILABLE:
                memory_gb = round(psutil.virtual_memory().total / (1024**3), 2)
            else:
                # macOS sysctl í´ë°±
                if platform.system() == "Darwin":
                    result = subprocess.run(
                        ['sysctl', 'hw.memsize'], 
                        capture_output=True, 
                        text=True, 
                        timeout=5
                    )
                    if result.returncode == 0:
                        memory_bytes = int(result.stdout.split(':')[1].strip())
                        memory_gb = round(memory_bytes / (1024**3), 2)
                    else:
                        memory_gb = 16.0
                else:
                    memory_gb = 16.0
            
            self._cache["memory_gb"] = memory_gb
            return memory_gb
            
        except Exception as e:
            logger.debug(f"ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self._cache["memory_gb"] = 16.0
            return 16.0
    
    def get_cpu_cores(self) -> int:
        """CPU ì½”ì–´ ìˆ˜"""
        if "cpu_cores" in self._cache:
            return self._cache["cpu_cores"]
        
        try:
            if PSUTIL_AVAILABLE:
                physical = psutil.cpu_count(logical=False) or 8
                logical = psutil.cpu_count(logical=True) or 8
                cores = max(physical, logical)
            else:
                cores = os.cpu_count() or 8
            
            self._cache["cpu_cores"] = cores
            return cores
            
        except Exception:
            self._cache["cpu_cores"] = 8
            return 8
    
    def detect_conda_environment(self) -> CondaEnvironment:
        """Conda í™˜ê²½ ê°ì§€"""
        conda_info = {
            'is_conda': False,
            'env_name': None,
            'prefix': None,
            'package_manager': 'pip',
            'python_version': platform.python_version(),
            'optimization_level': 'standard'
        }
        
        try:
            # CONDA_DEFAULT_ENV í™•ì¸
            conda_env = os.environ.get('CONDA_DEFAULT_ENV')
            if conda_env and conda_env != 'base':
                conda_info.update({
                    'is_conda': True,
                    'env_name': conda_env,
                    'package_manager': 'conda',
                    'optimization_level': 'conda_optimized'
                })
            
            # CONDA_PREFIX í™•ì¸
            conda_prefix = os.environ.get('CONDA_PREFIX')
            if conda_prefix:
                conda_info['prefix'] = conda_prefix
                if not conda_info['is_conda']:
                    conda_info.update({
                        'is_conda': True,
                        'env_name': Path(conda_prefix).name,
                        'package_manager': 'conda',
                        'optimization_level': 'conda_optimized'
                    })
            
            # Mamba ê°ì§€
            if conda_prefix and 'mamba' in str(conda_prefix).lower():
                conda_info['package_manager'] = 'mamba'
                conda_info['optimization_level'] = 'mamba_optimized'
        
        except Exception as e:
            logger.debug(f"Conda í™˜ê²½ ê°ì§€ ì‹¤íŒ¨: {e}")
        
        return CondaEnvironment(**conda_info)
    
    def detect_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        if not TORCH_AVAILABLE:
            return "cpu"
        
        try:
            # í™˜ê²½ ë³€ìˆ˜ ìš°ì„  í™•ì¸
            env_device = os.environ.get('DEVICE', '').lower()
            if env_device in ['cpu', 'mps', 'cuda'] and self._is_device_available(env_device):
                return env_device
            
            # ìë™ ê°ì§€: MPS > CUDA > CPU
            if self._check_mps_support():
                return "mps"
            elif hasattr(torch, 'cuda') and torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
                
        except Exception as e:
            logger.debug(f"ë””ë°”ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨: {e}")
            return "cpu"
    
    def get_device_capabilities(self, device: str) -> DeviceCapabilities:
        """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ì •ë³´ ìƒì„±"""
        system_info = self.get_system_info()
        
        if device == "mps" and system_info.is_m3_max:
            return DeviceCapabilities(
                device_type=device,
                name="Apple M3 Max",
                memory_gb=system_info.memory_gb,
                supports_fp16=False,  # MPS FP16 ì œí•œìœ¼ë¡œ ì•ˆì „í•˜ê²Œ False
                supports_fp32=True,
                unified_memory=True,
                max_batch_size=8 if system_info.memory_gb >= 120 else 4,
                recommended_image_size=(1024, 1024) if system_info.memory_gb >= 120 else (768, 768),
                neural_engine=True
            )
        elif device == "mps":
            return DeviceCapabilities(
                device_type=device,
                name="Apple Silicon",
                memory_gb=system_info.memory_gb,
                supports_fp32=True,
                unified_memory=True,
                max_batch_size=4,
                recommended_image_size=(768, 768)
            )
        elif device == "cuda" and TORCH_AVAILABLE:
            try:
                props = torch.cuda.get_device_properties(0)
                return DeviceCapabilities(
                    device_type=device,
                    name=props.name,
                    memory_gb=round(props.total_memory / (1024**3), 2),
                    supports_fp16=props.major >= 7,
                    supports_fp32=True,
                    max_batch_size=4,
                    tensor_cores=props.major >= 7
                )
            except Exception:
                pass
        
        # CPU í´ë°±
        return DeviceCapabilities(
            device_type="cpu",
            name="CPU",
            memory_gb=system_info.memory_gb,
            supports_fp32=True,
            max_batch_size=1
        )
    
    def _check_mps_support(self) -> bool:
        """MPS ì§€ì› ì—¬ë¶€ í™•ì¸"""
        if not TORCH_AVAILABLE:
            return False
        
        try:
            return (hasattr(torch.backends, 'mps') and 
                   hasattr(torch.backends.mps, 'is_available') and
                   torch.backends.mps.is_available())
        except Exception:
            return False
    
    def _is_device_available(self, device: str) -> bool:
        """ë””ë°”ì´ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        if device == "cpu":
            return True
        elif device == "mps":
            return self._check_mps_support()
        elif device == "cuda":
            return TORCH_AVAILABLE and torch.cuda.is_available()
        return False

# =============================================================================
# ğŸ”§ ìµœì í™” í”„ë¡œí•„ ë§¤ë‹ˆì € (ë‹¨ì¼ ì±…ì„)
# =============================================================================

class OptimizationProfileManager:
    """ìµœì í™” í”„ë¡œí•„ ê´€ë¦¬ ì „ë‹´ í´ë˜ìŠ¤"""
    
    def __init__(self, hardware_detector: HardwareDetector):
        self.hardware_detector = hardware_detector
        self._profiles = self._create_base_profiles()
    
    def _create_base_profiles(self) -> Dict[PerformanceClass, OptimizationProfile]:
        """ê¸°ë³¸ ìµœì í™” í”„ë¡œí•„ ìƒì„±"""
        return {
            PerformanceClass.ULTRA_HIGH: OptimizationProfile(
                batch_size=8,
                max_workers=20,
                memory_fraction=0.85,
                quality_level="ultra",
                dtype="float32",
                mixed_precision=False,
                enable_checkpointing=False
            ),
            PerformanceClass.HIGH: OptimizationProfile(
                batch_size=6,
                max_workers=16,
                memory_fraction=0.8,
                quality_level="high",
                dtype="float32",
                mixed_precision=False,
                enable_checkpointing=False
            ),
            PerformanceClass.MEDIUM: OptimizationProfile(
                batch_size=4,
                max_workers=12,
                memory_fraction=0.75,
                quality_level="balanced",
                dtype="float32",
                mixed_precision=False,
                enable_checkpointing=True
            ),
            PerformanceClass.LOW: OptimizationProfile(
                batch_size=2,
                max_workers=8,
                memory_fraction=0.6,
                quality_level="balanced",
                dtype="float32",
                mixed_precision=False,
                enable_checkpointing=True
            ),
            PerformanceClass.MINIMAL: OptimizationProfile(
                batch_size=1,
                max_workers=4,
                memory_fraction=0.5,
                quality_level="fast",
                dtype="float32",
                mixed_precision=False,
                enable_checkpointing=True
            )
        }
    
    def get_performance_class(self) -> PerformanceClass:
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •"""
        system_info = self.hardware_detector.get_system_info()
        
        if system_info.is_m3_max and system_info.memory_gb >= 120:
            return PerformanceClass.ULTRA_HIGH
        elif system_info.is_m3_max or (system_info.memory_gb >= 64 and system_info.cpu_cores >= 12):
            return PerformanceClass.HIGH
        elif system_info.memory_gb >= 32 and system_info.cpu_cores >= 8:
            return PerformanceClass.MEDIUM
        elif system_info.memory_gb >= 16:
            return PerformanceClass.LOW
        else:
            return PerformanceClass.MINIMAL
    
    def get_optimization_profile(self, optimization_level: OptimizationLevel) -> OptimizationProfile:
        """ìµœì í™” í”„ë¡œí•„ ìƒì„±"""
        performance_class = self.get_performance_class()
        base_profile = self._profiles[performance_class]
        
        # ìµœì í™” ë ˆë²¨ ì¡°ì •
        multipliers = {
            OptimizationLevel.SAFE: {"batch_size": 0.5, "memory_fraction": 0.6},
            OptimizationLevel.BALANCED: {"batch_size": 1.0, "memory_fraction": 1.0},
            OptimizationLevel.PERFORMANCE: {"batch_size": 1.2, "memory_fraction": 1.1},
            OptimizationLevel.ULTRA: {"batch_size": 1.5, "memory_fraction": 1.2}
        }
        
        multiplier = multipliers.get(optimization_level, multipliers[OptimizationLevel.BALANCED])
        
        return OptimizationProfile(
            batch_size=max(1, int(base_profile.batch_size * multiplier["batch_size"])),
            max_workers=base_profile.max_workers,
            memory_fraction=min(0.95, base_profile.memory_fraction * multiplier["memory_fraction"]),
            quality_level=base_profile.quality_level,
            dtype=base_profile.dtype,
            mixed_precision=base_profile.mixed_precision,
            enable_checkpointing=base_profile.enable_checkpointing
        )

# =============================================================================
# ğŸ”§ ë©”ì¸ GPU ì„¤ì • í´ë˜ìŠ¤ (ì¡°í•© íŒ¨í„´)
# =============================================================================

class GPUConfig:
    """GPU ì„¤ì • ë©”ì¸ í´ë˜ìŠ¤ - Clean Architecture ì ìš©"""
    
    def __init__(self, 
                 device: Optional[str] = None, 
                 optimization_level: Optional[str] = None,
                 **kwargs):
        """
        GPU ì„¤ì • ì´ˆê¸°í™”
        
        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ("cpu", "mps", "cuda", "auto")
            optimization_level: ìµœì í™” ë ˆë²¨ ("safe", "balanced", "performance", "ultra")
        """
        # ì˜ì¡´ì„± ì£¼ì…
        self._hardware_detector = HardwareDetector()
        self._memory_manager = MemoryManager()
        self._profile_manager = OptimizationProfileManager(self._hardware_detector)
        
        # ì´ˆê¸°í™” ì‹œê°„
        self._initialization_time = time.time()
        
        # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
        self._system_info = self._hardware_detector.get_system_info()
        self._conda_env = self._hardware_detector.detect_conda_environment()
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self._device = self._determine_device(device)
        self._device_capabilities = self._hardware_detector.get_device_capabilities(self._device)
        
        # ìµœì í™” ì„¤ì •
        self._optimization_level = self._determine_optimization_level(optimization_level)
        self._optimization_profile = self._profile_manager.get_optimization_profile(self._optimization_level)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self._metrics = PerformanceMetrics()
        
        # í™˜ê²½ ìµœì í™” ì ìš©
        self._is_initialized = False
        try:
            self._apply_environment_optimizations()
            self._is_initialized = True
        except Exception as e:
            logger.warning(f"í™˜ê²½ ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
    
    # =========================================================================
    # ğŸ”§ ì´ˆê¸°í™” ë©”ì„œë“œë“¤
    # =========================================================================
    
    def _determine_device(self, device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ê²°ì •"""
        if device and device != "auto":
            if self._hardware_detector._is_device_available(device):
                return device
            else:
                logger.warning(f"ìš”ì²­ëœ ë””ë°”ì´ìŠ¤ '{device}' ì‚¬ìš© ë¶ˆê°€, ìë™ ì„ íƒ")
        
        return self._hardware_detector.detect_device()
    
    def _determine_optimization_level(self, level: Optional[str]) -> OptimizationLevel:
        """ìµœì í™” ë ˆë²¨ ê²°ì •"""
        if level:
            try:
                return OptimizationLevel(level.lower())
            except ValueError:
                logger.warning(f"ì˜ëª»ëœ ìµœì í™” ë ˆë²¨ '{level}', ê¸°ë³¸ê°’ ì‚¬ìš©")
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        env_level = os.environ.get('OPTIMIZATION_LEVEL', '').lower()
        try:
            return OptimizationLevel(env_level)
        except ValueError:
            pass
        
        # ì„±ëŠ¥ í´ë˜ìŠ¤ ê¸°ë°˜ ìë™ ê²°ì •
        performance_class = self._profile_manager.get_performance_class()
        level_mapping = {
            PerformanceClass.ULTRA_HIGH: OptimizationLevel.ULTRA,
            PerformanceClass.HIGH: OptimizationLevel.PERFORMANCE,
            PerformanceClass.MEDIUM: OptimizationLevel.BALANCED,
            PerformanceClass.LOW: OptimizationLevel.BALANCED,
            PerformanceClass.MINIMAL: OptimizationLevel.SAFE
        }
        
        return level_mapping.get(performance_class, OptimizationLevel.BALANCED)
    
    def _apply_environment_optimizations(self):
        """í™˜ê²½ ìµœì í™” ì ìš©"""
        if not TORCH_AVAILABLE:
            return
        
        try:
            # PyTorch ìŠ¤ë ˆë“œ ì„¤ì •
            torch.set_num_threads(self._optimization_profile.max_workers)
            
            # ë””ë°”ì´ìŠ¤ë³„ í™˜ê²½ ë³€ìˆ˜
            if self._device == "mps":
                env_vars = {
                    'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                    'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0'
                }
                
                if self._system_info.is_m3_max:
                    env_vars.update({
                        'OMP_NUM_THREADS': '16',
                        'MKL_NUM_THREADS': '16',
                        'PYTORCH_MPS_PREFER_METAL': '1'
                    })
                
                os.environ.update(env_vars)
                
            elif self._device == "cuda":
                os.environ.update({
                    'CUDA_LAUNCH_BLOCKING': '0',
                    'CUDA_CACHE_DISABLE': '0'
                })
                
                # CUDNN ìµœì í™”
                if hasattr(torch.backends.cudnn, 'benchmark'):
                    torch.backends.cudnn.benchmark = True
                if hasattr(torch.backends.cudnn, 'deterministic'):
                    torch.backends.cudnn.deterministic = False
            
            else:  # CPU
                os.environ.update({
                    'OMP_NUM_THREADS': str(self._optimization_profile.max_workers),
                    'MKL_NUM_THREADS': str(self._optimization_profile.max_workers)
                })
            
            # Conda í™˜ê²½ ìµœì í™”
            if self._conda_env.is_conda:
                conda_vars = {
                    'CONDA_DEFAULT_ENV': self._conda_env.env_name or 'base',
                    'PYTHONUNBUFFERED': '1'
                }
                if self._conda_env.prefix:
                    conda_vars['CONDA_PREFIX'] = self._conda_env.prefix
                os.environ.update(conda_vars)
            
            # ì´ˆê¸° ë©”ëª¨ë¦¬ ì •ë¦¬
            self._memory_manager.cleanup_memory()
            
        except Exception as e:
            logger.warning(f"í™˜ê²½ ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
    
    # =========================================================================
    # ğŸ”§ ê³µê°œ ì¸í„°í˜ì´ìŠ¤
    # =========================================================================
    
    @property
    def device(self) -> str:
        """í˜„ì¬ ë””ë°”ì´ìŠ¤"""
        return self._device
    
    @property
    def device_name(self) -> str:
        """ë””ë°”ì´ìŠ¤ ì´ë¦„"""
        return self._device_capabilities.name
    
    @property
    def device_type(self) -> str:
        """ë””ë°”ì´ìŠ¤ íƒ€ì…"""
        return self._device
    
    @property
    def memory_gb(self) -> float:
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ (GB)"""
        return self._system_info.memory_gb
    
    @property
    def is_m3_max(self) -> bool:
        """M3 Max ì—¬ë¶€"""
        return self._system_info.is_m3_max
    
    @property
    def optimization_level(self) -> str:
        """ìµœì í™” ë ˆë²¨"""
        return self._optimization_level.value
    
    @property
    def is_initialized(self) -> bool:
        """ì´ˆê¸°í™” ì™„ë£Œ ì—¬ë¶€"""
        return self._is_initialized
    
    @property
    def float_compatibility_mode(self) -> bool:
        """Float í˜¸í™˜ì„± ëª¨ë“œ (í•­ìƒ True)"""
        return True
    
    def get_device(self) -> str:
        """ë””ë°”ì´ìŠ¤ ë°˜í™˜ (í˜¸í™˜ì„±)"""
        return self.device
    
    def get_device_name(self) -> str:
        """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜ (í˜¸í™˜ì„±)"""
        return self.device_name
    
    def get_memory_info(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        return self._memory_manager.get_memory_info()
    
    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        result = self._memory_manager.cleanup_memory(aggressive)
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        if result["success"]:
            self._update_metrics("memory_cleanup", result.get("duration", 0) * 1000, True)
        else:
            self._update_metrics("memory_cleanup", 0, False)
        
        return result
    
    def get_optimal_settings(self) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì„¤ì • ë°˜í™˜"""
        return {
            "device_config": self._create_model_config(),
            "optimization_settings": self._create_optimization_settings(),
            "device_info": self._create_device_info(),
            "last_updated": time.time()
        }
    
    def get_device_capabilities(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ì •ë³´ ë°˜í™˜"""
        caps = self._device_capabilities
        return {
            "device": caps.device_type,
            "name": caps.name,
            "memory_gb": caps.memory_gb,
            "supports_fp16": caps.supports_fp16,
            "supports_fp32": caps.supports_fp32,
            "unified_memory": caps.unified_memory,
            "max_batch_size": caps.max_batch_size,
            "recommended_image_size": caps.recommended_image_size,
            "tensor_cores": caps.tensor_cores,
            "neural_engine": caps.neural_engine,
            "optimization_level": self.optimization_level,
            "performance_class": self._profile_manager.get_performance_class().value,
            "pytorch_version": TORCH_VERSION,
            "float_compatibility_mode": True,
            "conda_environment": self._conda_env.is_conda,
            "last_updated": time.time()
        }
    
    def benchmark_device(self, duration_seconds: int = 10) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ë²¤ì¹˜ë§ˆí¬"""
        if not TORCH_AVAILABLE:
            return {
                "error": "PyTorch not available",
                "device": self.device
            }
        
        try:
            start_time = time.time()
            device = torch.device(self.device)
            
            # ë©”ëª¨ë¦¬ í• ë‹¹ í…ŒìŠ¤íŠ¸
            memory_tests = []
            for size in [100, 500, 1000]:
                try:
                    test_start = time.time()
                    tensor = torch.randn(size, size, device=device)
                    alloc_time = (time.time() - test_start) * 1000
                    memory_mb = (tensor.nelement() * tensor.element_size()) / (1024**2)
                    
                    memory_tests.append({
                        "size": f"{size}x{size}",
                        "allocation_time_ms": round(alloc_time, 2),
                        "memory_mb": round(memory_mb, 2)
                    })
                    del tensor
                except Exception as e:
                    memory_tests.append({
                        "size": f"{size}x{size}",
                        "error": str(e)[:100]
                    })
            
            # ì—°ì‚° ì†ë„ í…ŒìŠ¤íŠ¸
            compute_tests = []
            if memory_tests and "error" not in memory_tests[0]:
                test_tensor = torch.randn(500, 500, device=device)
                
                operations = [
                    ("matrix_multiply", lambda x: torch.mm(x, x.t())),
                    ("elementwise_ops", lambda x: x * 2 + 1),
                    ("reduction_ops", lambda x: torch.sum(x, dim=0))
                ]
                
                for op_name, op_func in operations:
                    times = []
                    try:
                        for _ in range(3):  # 3íšŒ ë°˜ë³µ
                            op_start = time.time()
                            result = op_func(test_tensor)
                            times.append((time.time() - op_start) * 1000)
                            del result
                        
                        compute_tests.append({
                            "operation": op_name,
                            "avg_time_ms": round(sum(times) / len(times), 2),
                            "min_time_ms": round(min(times), 2),
                            "max_time_ms": round(max(times), 2)
                        })
                    except Exception as e:
                        compute_tests.append({
                            "operation": op_name,
                            "error": str(e)[:100]
                        })
                
                del test_tensor
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            cleanup_start = time.time()
            cleanup_result = self.cleanup_memory(aggressive=True)
            cleanup_time = (time.time() - cleanup_start) * 1000
            
            total_duration = time.time() - start_time
            
            return {
                "device": self.device,
                "device_name": self.device_name,
                "benchmark_duration_seconds": round(total_duration, 2),
                "memory_tests": memory_tests,
                "compute_tests": compute_tests,
                "memory_cleanup_time_ms": round(cleanup_time, 2),
                "cleanup_result": cleanup_result,
                "memory_info": self.get_memory_info(),
                "device_capabilities": self.get_device_capabilities(),
                "timestamp": time.time(),
                "benchmark_success": True
            }
            
        except Exception as e:
            return {
                "device": self.device,
                "benchmark_success": False,
                "error": str(e)[:300],
                "timestamp": time.time()
            }
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ ë¦¬í¬íŠ¸"""
        return {
            "system_info": {
                "platform": self._system_info.platform,
                "machine": self._system_info.machine,
                "python_version": self._system_info.python_version,
                "pytorch_version": self._system_info.pytorch_version,
                "memory_gb": self._system_info.memory_gb,
                "cpu_cores": self._system_info.cpu_cores,
                "is_m3_max": self._system_info.is_m3_max
            },
            "conda_environment": {
                "is_conda": self._conda_env.is_conda,
                "env_name": self._conda_env.env_name,
                "package_manager": self._conda_env.package_manager,
                "optimization_level": self._conda_env.optimization_level
            },
            "device_info": {
                "device": self.device,
                "name": self.device_name,
                "capabilities": self.get_device_capabilities()
            },
            "optimization": {
                "level": self.optimization_level,
                "performance_class": self._profile_manager.get_performance_class().value,
                "profile": {
                    "batch_size": self._optimization_profile.batch_size,
                    "max_workers": self._optimization_profile.max_workers,
                    "memory_fraction": self._optimization_profile.memory_fraction,
                    "quality_level": self._optimization_profile.quality_level
                }
            },
            "metrics": {
                "memory_cleanup_count": self._metrics.memory_cleanup_count,
                "memory_cleanup_avg_ms": round(self._metrics.memory_cleanup_avg_ms, 2),
                "error_count": self._metrics.error_count,
                "uptime_hours": round((time.time() - self._metrics.last_reset) / 3600, 2)
            },
            "memory_info": self.get_memory_info(),
            "initialization_time": self._initialization_time,
            "is_initialized": self._is_initialized,
            "generation_time": time.time()
        }
    
    # =========================================================================
    # ğŸ”§ ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤ (í˜¸í™˜ì„±)
    # =========================================================================
    
    def get(self, key: str, default: Any = None) -> Any:
        """ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì ‘ê·¼"""
        # ì§ì ‘ ì†ì„± ë§¤í•‘
        direct_attrs = {
            'device': self.device,
            'device_name': self.device_name,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'optimization_level': self.optimization_level,
            'is_initialized': self.is_initialized,
            'float_compatibility_mode': self.float_compatibility_mode,
            'pytorch_version': TORCH_VERSION,
            'numpy_version': NUMPY_VERSION,
            'conda_environment': self._conda_env.is_conda,
            'conda_env_name': self._conda_env.env_name,
            'performance_class': self._profile_manager.get_performance_class().value
        }
        
        if key in direct_attrs:
            return direct_attrs[key]
        
        # ì„¤ì • ë”•ì…”ë„ˆë¦¬ì—ì„œ ê²€ìƒ‰
        try:
            model_config = self._create_model_config()
            if key in model_config:
                return model_config[key]
        except Exception:
            pass
        
        # ê°ì²´ ì†ì„±ì—ì„œ ê²€ìƒ‰
        if hasattr(self, key):
            attr = getattr(self, key)
            # ë©”ì„œë“œëŠ” í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
            if callable(attr):
                return default
            return attr
        
        return default
    
    def __getitem__(self, key: str) -> Any:
        """ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì ‘ê·¼"""
        result = self.get(key)
        if result is None:
            raise KeyError(f"Key '{key}' not found in GPUConfig")
        return result
    
    def __contains__(self, key: str) -> bool:
        """in ì—°ì‚°ì ì§€ì›"""
        return self.get(key) is not None
    
    def keys(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ ëª©ë¡"""
        return [
            'device', 'device_name', 'device_type', 'memory_gb', 'is_m3_max',
            'optimization_level', 'is_initialized', 'float_compatibility_mode',
            'pytorch_version', 'numpy_version', 'conda_environment', 'conda_env_name',
            'performance_class', 'batch_size', 'max_workers', 'memory_fraction',
            'quality_level', 'dtype'
        ]
    
    def items(self) -> List[Tuple[str, Any]]:
        """í‚¤-ê°’ ìŒ ë°˜í™˜"""
        return [(key, self.get(key)) for key in self.keys()]
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return dict(self.items())
    
    # =========================================================================
    # ğŸ”§ ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œë“¤
    # =========================================================================
    
    def _create_model_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        return {
            "device": self.device,
            "device_name": self.device_name,
            "dtype": self._optimization_profile.dtype,
            "batch_size": self._optimization_profile.batch_size,
            "max_workers": self._optimization_profile.max_workers,
            "memory_fraction": self._optimization_profile.memory_fraction,
            "optimization_level": self.optimization_level,
            "quality_level": self._optimization_profile.quality_level,
            "float_compatibility_mode": True,
            "mps_fallback_enabled": self.device == "mps",
            "pytorch_version": TORCH_VERSION,
            "numpy_version": NUMPY_VERSION,
            "m3_max_optimized": self.is_m3_max,
            "conda_environment": self._conda_env.is_conda
        }
    
    def _create_optimization_settings(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        profile = self._optimization_profile
        return {
            "batch_size": profile.batch_size,
            "max_workers": profile.max_workers,
            "memory_fraction": profile.memory_fraction,
            "quality_level": profile.quality_level,
            "dtype": profile.dtype,
            "mixed_precision": profile.mixed_precision,
            "enable_checkpointing": profile.enable_checkpointing,
            "optimization_level": self.optimization_level,
            "performance_class": self._profile_manager.get_performance_class().value
        }
    
    def _create_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        return {
            "device": self.device,
            "device_name": self.device_name,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "capabilities": self.get_device_capabilities(),
            "system_info": {
                "platform": self._system_info.platform,
                "machine": self._system_info.machine,
                "cpu_cores": self._system_info.cpu_cores,
                "python_version": self._system_info.python_version
            },
            "conda_environment": {
                "is_conda": self._conda_env.is_conda,
                "env_name": self._conda_env.env_name,
                "package_manager": self._conda_env.package_manager
            },
            "pytorch_available": TORCH_AVAILABLE,
            "pytorch_version": TORCH_VERSION,
            "numpy_available": NUMPY_AVAILABLE,
            "numpy_version": NUMPY_VERSION,
            "psutil_available": PSUTIL_AVAILABLE
        }
    
    def _update_metrics(self, operation: str, duration_ms: float, success: bool):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        try:
            if operation == "memory_cleanup" and success:
                count = self._metrics.memory_cleanup_count
                avg = self._metrics.memory_cleanup_avg_ms
                self._metrics.memory_cleanup_avg_ms = (avg * count + duration_ms) / (count + 1)
                self._metrics.memory_cleanup_count += 1
            elif not success:
                self._metrics.error_count += 1
        except Exception:
            pass  # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ

# =============================================================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

@lru_cache(maxsize=1)
def get_gpu_config(**kwargs) -> GPUConfig:
    """GPU ì„¤ì • ì‹±ê¸€í†¤ íŒ©í† ë¦¬"""
    try:
        return GPUConfig(**kwargs)
    except Exception as e:
        logger.error(f"GPUConfig ìƒì„± ì‹¤íŒ¨: {e}")
        return _create_fallback_gpu_config()

def _create_fallback_gpu_config():
    """í´ë°± GPU ì„¤ì • ê°ì²´"""
    class FallbackGPUConfig:
        def __init__(self):
            self.device = "cpu"
            self.device_name = "CPU (Fallback)"
            self.device_type = "cpu"
            self.memory_gb = 16.0
            self.is_m3_max = False
            self.optimization_level = "safe"
            self.is_initialized = False
            self.float_compatibility_mode = True
        
        def get(self, key, default=None):
            return getattr(self, key, default)
        
        def get_device(self):
            return self.device
        
        def get_device_name(self):
            return self.device_name
        
        def get_memory_info(self):
            return {"total_gb": self.memory_gb, "device": self.device, "fallback_mode": True}
        
        def cleanup_memory(self, aggressive=False):
            collected = gc.collect()
            return {"success": True, "method": "fallback_gc", "device": "cpu", "collected_objects": collected}
        
        def get_optimal_settings(self):
            return {"device_config": {"device": "cpu", "batch_size": 1, "fallback_mode": True}}
        
        def get_device_capabilities(self):
            return {"device": "cpu", "fallback_mode": True, "error": "GPUConfig initialization failed"}
        
        def __getitem__(self, key):
            return self.get(key)
        
        def __contains__(self, key):
            return self.get(key) is not None
    
    return FallbackGPUConfig()

# í¸ì˜ í•¨ìˆ˜ë“¤
def get_device_config() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config._create_model_config()
    except Exception as e:
        return {"error": str(e)[:200], "device": "cpu", "fallback_mode": True}

def get_model_config() -> Dict[str, Any]:
    """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
    return get_device_config()

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
    try:
        config = get_gpu_config()
        return config._create_device_info()
    except Exception as e:
        return {"error": str(e)[:200], "device": "cpu", "fallback_mode": True}

def get_device() -> str:
    """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    try:
        return get_gpu_config().device
    except:
        return "cpu"

def get_device_name() -> str:
    """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
    try:
        return get_gpu_config().device_name
    except:
        return "CPU (Fallback)"

def is_m3_max() -> bool:
    """M3 Max ì—¬ë¶€ í™•ì¸"""
    try:
        return get_gpu_config().is_m3_max
    except:
        return False

def get_optimal_settings() -> Dict[str, Any]:
    """ìµœì í™”ëœ ì„¤ì • ë°˜í™˜"""
    try:
        return get_gpu_config().get_optimal_settings()
    except Exception as e:
        return {"error": str(e)[:200], "fallback_config": {"device": "cpu", "batch_size": 1}}

def get_device_capabilities() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ì •ë³´ ë°˜í™˜"""
    try:
        return get_gpu_config().get_device_capabilities()
    except Exception as e:
        return {"error": str(e)[:200], "device": "cpu", "fallback_mode": True}

def cleanup_device_memory(aggressive: bool = False) -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        return get_gpu_config().cleanup_memory(aggressive=aggressive)
    except Exception as e:
        # í´ë°±: ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        collected = gc.collect()
        return {"success": True, "method": "fallback_gc", "device": "cpu", "collected_objects": collected, "error": str(e)[:100]}

def benchmark_device(duration_seconds: int = 10) -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ë²¤ì¹˜ë§ˆí¬"""
    try:
        return get_gpu_config().benchmark_device(duration_seconds)
    except Exception as e:
        return {"benchmark_success": False, "error": str(e)[:200], "device": "cpu"}

def get_performance_report() -> Dict[str, Any]:
    """ì „ì²´ ì„±ëŠ¥ ë¦¬í¬íŠ¸"""
    try:
        return get_gpu_config().get_performance_report()
    except Exception as e:
        return {
            "error": str(e)[:200],
            "basic_info": {
                "pytorch_available": TORCH_AVAILABLE,
                "numpy_available": NUMPY_AVAILABLE,
                "psutil_available": PSUTIL_AVAILABLE
            },
            "timestamp": time.time()
        }

def reset_gpu_config():
    """GPU ì„¤ì • ë¦¬ì…‹"""
    try:
        get_gpu_config.cache_clear()
        return {"success": True, "message": "GPU config reset"}
    except Exception as e:
        return {"success": False, "error": str(e)[:200]}

# =============================================================================
# ğŸ”§ ì „ì—­ GPU ì„¤ì • ë§¤ë‹ˆì € ìƒì„±
# =============================================================================

try:
    # ë©”ì¸ GPU ì„¤ì • ê°ì²´ ìƒì„±
    gpu_config = get_gpu_config()
    
    # ì „ì—­ ë³€ìˆ˜ ì„¤ì •
    DEVICE = gpu_config.device
    DEVICE_NAME = gpu_config.device_name
    DEVICE_TYPE = gpu_config.device_type
    MODEL_CONFIG = gpu_config._create_model_config()
    DEVICE_INFO = gpu_config._create_device_info()
    IS_M3_MAX = gpu_config.is_m3_max
    OPTIMIZATION_LEVEL = gpu_config.optimization_level
    CONDA_ENV = gpu_config._conda_env
    
    # ì„±ê³µ ë©”ì‹œì§€
    if IS_M3_MAX:
        print(f"ğŸ M3 Max ({DEVICE}) ì™„ì „ ìµœì í™” ëª¨ë“œ í™œì„±í™”")
        print(f"ğŸ’¾ í†µí•© ë©”ëª¨ë¦¬: {gpu_config.memory_gb}GB | ìµœì í™”: {OPTIMIZATION_LEVEL}")
        if CONDA_ENV.is_conda:
            print(f"ğŸ conda í™˜ê²½: {CONDA_ENV.env_name} | íŒ¨í‚¤ì§€ ê´€ë¦¬ì: {CONDA_ENV.package_manager}")
    else:
        print(f"âœ… GPU ì„¤ì • ì™„ì „ ë¡œë“œ - ë””ë°”ì´ìŠ¤: {DEVICE} | ìµœì í™”: {OPTIMIZATION_LEVEL}")

except Exception as e:
    print(f"âš ï¸ GPU ì„¤ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:100]}")
    
    # í´ë°± ì„¤ì •
    DEVICE = "cpu"
    DEVICE_NAME = "CPU (Fallback)"
    DEVICE_TYPE = "cpu"
    MODEL_CONFIG = {"device": "cpu", "dtype": "float32", "batch_size": 1, "fallback_mode": True}
    DEVICE_INFO = {"device": "cpu", "error": "GPU config initialization failed", "fallback_mode": True}
    IS_M3_MAX = False
    OPTIMIZATION_LEVEL = "safe"
    CONDA_ENV = CondaEnvironment(is_conda=False)
    
    # í´ë°± GPU ì„¤ì • ê°ì²´
    gpu_config = _create_fallback_gpu_config()

# =============================================================================
# ğŸ”§ Export ë¦¬ìŠ¤íŠ¸
# =============================================================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'GPUConfig', 'HardwareDetector', 'MemoryManager', 'OptimizationProfileManager',
    
    # ë°ì´í„° í´ë˜ìŠ¤ë“¤
    'SystemInfo', 'CondaEnvironment', 'DeviceCapabilities', 'PerformanceMetrics', 'OptimizationProfile',
    
    # ì—´ê±°í˜•ë“¤
    'DeviceType', 'OptimizationLevel', 'PerformanceClass',
    
    # ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° ë³€ìˆ˜ë“¤
    'gpu_config', 'DEVICE', 'DEVICE_NAME', 'DEVICE_TYPE', 
    'MODEL_CONFIG', 'DEVICE_INFO', 'IS_M3_MAX', 'OPTIMIZATION_LEVEL', 'CONDA_ENV',
    
    # íŒ©í† ë¦¬ ë° ì„¤ì • í•¨ìˆ˜ë“¤
    'get_gpu_config', 'get_device_config', 'get_model_config', 'get_device_info',
    'get_device', 'get_device_name', 'is_m3_max', 'get_optimal_settings', 
    'get_device_capabilities', 'reset_gpu_config',
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤
    'cleanup_device_memory',
    
    # ì„±ëŠ¥ ë° ë²¤ì¹˜ë§ˆí¬ í•¨ìˆ˜ë“¤
    'benchmark_device', 'get_performance_report',
    
    # ìƒìˆ˜ ë° ë²„ì „ ì •ë³´
    'TORCH_AVAILABLE', 'TORCH_VERSION', 'TORCH_MAJOR', 'TORCH_MINOR',
    'NUMPY_AVAILABLE', 'NUMPY_VERSION', 'PSUTIL_AVAILABLE', 'PSUTIL_VERSION'
]

# ëª¨ë“ˆ ì™„ë£Œ ë©”ì‹œì§€
if __name__ == "__main__":
    print("\n" + "="*80)
    print("ğŸ MyCloset AI GPU Config ì‹œìŠ¤í…œ ì™„ì „ ë¡œë“œ ì™„ë£Œ")
    print("="*80)
    
    # ê°„ë‹¨í•œ ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    try:
        config = get_gpu_config()
        print(f"ë””ë°”ì´ìŠ¤: {config.device} ({config.device_name})")
        print(f"ë©”ëª¨ë¦¬: {config.memory_gb}GB")
        print(f"ìµœì í™” ë ˆë²¨: {config.optimization_level}")
        print(f"M3 Max: {'âœ…' if config.is_m3_max else 'âŒ'}")
        print(f"conda í™˜ê²½: {'âœ…' if config._conda_env.is_conda else 'âŒ'}")
        print(f"PyTorch: {TORCH_VERSION if TORCH_AVAILABLE else 'âŒ'}")
        print(f"ì´ˆê¸°í™” ì™„ë£Œ: {'âœ…' if config.is_initialized else 'âŒ'}")
        
        # ì„±ëŠ¥ í´ë˜ìŠ¤ ì¶œë ¥
        performance_class = config._profile_manager.get_performance_class()
        print(f"ì„±ëŠ¥ í´ë˜ìŠ¤: {performance_class.value}")
        
        # ìµœì í™” í”„ë¡œí•„ ìš”ì•½
        profile = config._optimization_profile
        print(f"ë°°ì¹˜ í¬ê¸°: {profile.batch_size} | ì›Œì»¤: {profile.max_workers} | ë©”ëª¨ë¦¬: {profile.memory_fraction:.1%}")
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥ ì‹¤íŒ¨: {e}")
    
    print("="*80)