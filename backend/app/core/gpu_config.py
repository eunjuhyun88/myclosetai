"""
GPU/MPS ìµœì í™” ì„¤ì •
Apple Silicon M3 Max ë° CUDA í™˜ê²½ì—ì„œì˜ ìµœì  ì„±ëŠ¥ ì œê³µ
"""
import os
import logging
import platform
from typing import Dict, Any, Optional, Tuple
import torch
import gc
import psutil

logger = logging.getLogger(__name__)

class GPUConfig:
    """GPU/MPS ìµœì í™” ì„¤ì • ê´€ë¦¬ì"""
    
    def __init__(self):
        """GPU ì„¤ì • ì´ˆê¸°í™”"""
        self.device_type = self._detect_device()
        self.device = torch.device(self.device_type)
        self.is_apple_silicon = self._is_apple_silicon()
        
        # ë©”ëª¨ë¦¬ ì„¤ì •
        self.memory_settings = self._configure_memory()
        
        # ìµœì í™” ì„¤ì •
        self.optimization_settings = self._configure_optimization()
        
        logger.info(f"ğŸ”§ GPU ì„¤ì • ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device_type}")
        self._log_system_info()
    
    def _detect_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        # í™˜ê²½ ë³€ìˆ˜ ìš°ì„  í™•ì¸
        forced_device = os.environ.get('FORCE_DEVICE', '').lower()
        if forced_device in ['cpu', 'cuda', 'mps']:
            logger.info(f"ğŸ¯ ê°•ì œ ë””ë°”ì´ìŠ¤ ì„¤ì •: {forced_device}")
            return forced_device
        
        # Apple Silicon MPS í™•ì¸
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            if torch.backends.mps.is_built():
                logger.info("ğŸ Apple Silicon MPS ê°ì§€ë¨")
                return "mps"
        
        # CUDA í™•ì¸
        if torch.cuda.is_available():
            cuda_device_count = torch.cuda.device_count()
            logger.info(f"ğŸš€ CUDA ê°ì§€ë¨ - GPU ê°œìˆ˜: {cuda_device_count}")
            return "cuda"
        
        # CPU í´ë°±
        logger.info("ğŸ’» CPU ëª¨ë“œë¡œ ì„¤ì •")
        return "cpu"
    
    def _is_apple_silicon(self) -> bool:
        """Apple Silicon ì—¬ë¶€ í™•ì¸"""
        system = platform.system()
        machine = platform.machine()
        
        if system == "Darwin" and machine == "arm64":
            return True
        
        # M ì‹œë¦¬ì¦ˆ ì¹© ì§ì ‘ í™•ì¸
        try:
            import subprocess
            result = subprocess.run(
                ["sysctl", "-n", "machdep.cpu.brand_string"],
                capture_output=True, text=True, timeout=2
            )
            if "Apple" in result.stdout:
                return True
        except:
            pass
        
        return False
    
    def _configure_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì„¤ì •"""
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´
        system_memory = psutil.virtual_memory()
        total_memory_gb = system_memory.total / (1024**3)
        
        settings = {
            "total_system_memory_gb": total_memory_gb,
            "reserved_system_memory_gb": 4.0,  # ì‹œìŠ¤í…œìš© ì˜ˆì•½
            "max_model_memory_gb": min(total_memory_gb * 0.6, 32.0)  # ëª¨ë¸ìš© ìµœëŒ€
        }
        
        if self.device_type == "mps":
            # Apple Silicon MPS ì„¤ì •
            # M3 MaxëŠ” ë³´í†µ 36GB ë˜ëŠ” 128GB unified memory
            if total_memory_gb >= 64:
                settings.update({
                    "mps_memory_fraction": 0.7,  # 70% ì‚¬ìš©
                    "batch_size_multiplier": 1.5,
                    "enable_memory_mapping": True
                })
            else:
                settings.update({
                    "mps_memory_fraction": 0.6,  # 60% ì‚¬ìš©
                    "batch_size_multiplier": 1.0,
                    "enable_memory_mapping": True
                })
            
            # MPS íŠ¹í™” ì„¤ì •
            settings.update({
                "enable_mixed_precision": True,
                "optimize_for_inference": True,
                "enable_graph_optimization": True
            })
            
        elif self.device_type == "cuda":
            # CUDA ì„¤ì •
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            settings.update({
                "gpu_memory_gb": gpu_memory,
                "cuda_memory_fraction": min(0.8, (gpu_memory - 2) / gpu_memory),
                "enable_cudnn_benchmark": True,
                "enable_amp": True  # Automatic Mixed Precision
            })
            
            # CUDA ë©”ëª¨ë¦¬ ì„¤ì •
            try:
                torch.cuda.set_per_process_memory_fraction(settings["cuda_memory_fraction"])
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
            except Exception as e:
                logger.warning(f"CUDA ë©”ëª¨ë¦¬ ì„¤ì • ì‹¤íŒ¨: {e}")
        
        else:  # CPU
            # CPU ì„¤ì •
            cpu_count = psutil.cpu_count(logical=False)
            settings.update({
                "cpu_cores": cpu_count,
                "thread_count": min(cpu_count, 8),  # ìµœëŒ€ 8 ìŠ¤ë ˆë“œ
                "enable_mkldnn": True
            })
            
            # CPU ìµœì í™”
            torch.set_num_threads(settings["thread_count"])
        
        return settings
    
    def _configure_optimization(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì •"""
        settings = {
            "compile_models": True,  # PyTorch 2.0+ ì»´íŒŒì¼
            "use_channels_last": True,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
            "enable_jit": True,  # JIT ì»´íŒŒì¼
            "gradient_accumulation": True,
            "model_quantization": {
                "enabled": True,
                "mode": "dynamic"  # dynamic, static, qat
            }
        }
        
        if self.device_type == "mps":
            # Apple Silicon íŠ¹í™” ìµœì í™”
            settings.update({
                "mps_optimizations": {
                    "enable_fusion": True,
                    "optimize_memory_layout": True,
                    "use_metal_performance_shaders": True,
                    "enable_graph_capture": True
                },
                "batch_processing": {
                    "optimal_batch_size": 4 if self.memory_settings.get("mps_memory_fraction", 0.6) > 0.65 else 2,
                    "dynamic_batching": True
                }
            })
            
        elif self.device_type == "cuda":
            # CUDA íŠ¹í™” ìµœì í™”
            settings.update({
                "cuda_optimizations": {
                    "enable_tensor_core": True,
                    "use_half_precision": True,
                    "optimize_attention": True,
                    "enable_flash_attention": True
                },
                "batch_processing": {
                    "optimal_batch_size": 8,
                    "gradient_checkpointing": True
                }
            })
        
        return settings
    
    def get_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self.device_type
    
    def get_model_config(self, model_type: str) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ ìµœì  ì„¤ì • ë°˜í™˜"""
        base_config = {
            "device": self.device,
            "dtype": torch.float32
        }
        
        if self.device_type == "mps":
            # MPSëŠ” float16 ì§€ì› ì œí•œì 
            base_config.update({
                "dtype": torch.float32,
                "use_compile": True,
                "memory_format": torch.channels_last if self.optimization_settings["use_channels_last"] else torch.contiguous_format
            })
            
        elif self.device_type == "cuda":
            # CUDA ìµœì í™”
            base_config.update({
                "dtype": torch.float16 if self.optimization_settings["cuda_optimizations"]["use_half_precision"] else torch.float32,
                "use_compile": self.optimization_settings["compile_models"],
                "memory_format": torch.channels_last
            })
        
        # ëª¨ë¸ íƒ€ì…ë³„ íŠ¹í™” ì„¤ì •
        model_specific = self._get_model_specific_config(model_type)
        base_config.update(model_specific)
        
        return base_config
    
    def _get_model_specific_config(self, model_type: str) -> Dict[str, Any]:
        """ëª¨ë¸ íƒ€ì…ë³„ íŠ¹í™” ì„¤ì •"""
        configs = {
            "human_parsing": {
                "batch_size": 1,
                "input_size": (512, 512),
                "enable_optimization": True
            },
            "pose_estimation": {
                "batch_size": 1,
                "input_size": (256, 192),
                "keypoint_threshold": 0.3
            },
            "cloth_segmentation": {
                "batch_size": 1,
                "input_size": (320, 320),
                "enable_postprocessing": True
            },
            "virtual_fitting": {
                "batch_size": 1,
                "input_size": (512, 512),
                "num_inference_steps": 20,
                "guidance_scale": 7.5
            }
        }
        
        return configs.get(model_type, {})
    
    def optimize_model(self, model: torch.nn.Module, model_type: str) -> torch.nn.Module:
        """ëª¨ë¸ ìµœì í™” ì ìš©"""
        try:
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            model = model.to(self.device)
            
            # í‰ê°€ ëª¨ë“œ
            model.eval()
            
            # ë©”ëª¨ë¦¬ í¬ë§· ìµœì í™”
            if self.optimization_settings["use_channels_last"]:
                model = model.to(memory_format=torch.channels_last)
            
            # ì»´íŒŒì¼ ìµœì í™” (PyTorch 2.0+)
            if self.optimization_settings["compile_models"]:
                try:
                    if hasattr(torch, 'compile'):
                        if self.device_type == "mps":
                            # MPSëŠ” ì¼ë¶€ compile ê¸°ëŠ¥ ì œí•œ
                            model = torch.compile(model, mode="reduce-overhead")
                        else:
                            model = torch.compile(model, mode="max-autotune")
                        logger.info(f"âœ… {model_type} ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"ëª¨ë¸ ì»´íŒŒì¼ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
            
            # ì–‘ìí™” (ì¶”ë¡  ì „ìš©)
            if self.optimization_settings["model_quantization"]["enabled"]:
                try:
                    if self.device_type == "cpu":
                        # CPUì—ì„œë§Œ ì–‘ìí™” ì ìš©
                        model = torch.quantization.quantize_dynamic(
                            model, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8
                        )
                        logger.info(f"âœ… {model_type} ëª¨ë¸ ì–‘ìí™” ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"ëª¨ë¸ ì–‘ìí™” ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
            
            return model
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model
    
    def setup_memory_optimization(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •"""
        try:
            if self.device_type == "mps":
                # MPS ë©”ëª¨ë¦¬ ìµœì í™”
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                # Metal ì„±ëŠ¥ ìµœì í™”
                if self.is_apple_silicon:
                    os.environ['METAL_DEVICE_WRAPPER_TYPE'] = '1'
                    os.environ['METAL_DEBUG_ERROR_MODE'] = '0'
                
            elif self.device_type == "cuda":
                # CUDA ë©”ëª¨ë¦¬ ìµœì í™”
                torch.cuda.empty_cache()
                
                # CUDA ê·¸ë˜í”„ ìµœì í™”
                if hasattr(torch.cuda, 'memory_stats'):
                    logger.info("CUDA ë©”ëª¨ë¦¬ í†µê³„ í™œì„±í™”")
            
            # ê³µí†µ ìµœì í™”
            torch.backends.opt_einsum.enabled = True
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”
            gc.collect()
            
            logger.info("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def get_memory_info(self) -> Dict[str, Any]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´"""
        info = {
            "device": self.device_type,
            "system_memory": {
                "total_gb": psutil.virtual_memory().total / (1024**3),
                "available_gb": psutil.virtual_memory().available / (1024**3),
                "used_percent": psutil.virtual_memory().percent
            }
        }
        
        if self.device_type == "cuda":
            info["gpu_memory"] = {
                "allocated_gb": torch.cuda.memory_allocated() / (1024**3),
                "reserved_gb": torch.cuda.memory_reserved() / (1024**3),
                "max_allocated_gb": torch.cuda.max_memory_allocated() / (1024**3)
            }
        elif self.device_type == "mps":
            info["mps_memory"] = {
                "current_allocated_gb": torch.mps.current_allocated_memory() / (1024**3),
                "driver_allocated_gb": torch.mps.driver_allocated_memory() / (1024**3)
            }
        
        return info
    
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            # PyTorch ìºì‹œ ì •ë¦¬
            if self.device_type == "cuda":
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            elif self.device_type == "mps":
                torch.mps.empty_cache()
                torch.mps.synchronize()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _log_system_info(self):
        """ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹…"""
        logger.info("=" * 50)
        logger.info("ğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´")
        logger.info(f"í”Œë«í¼: {platform.platform()}")
        logger.info(f"í”„ë¡œì„¸ì„œ: {platform.processor()}")
        logger.info(f"ì•„í‚¤í…ì²˜: {platform.machine()}")
        
        if self.is_apple_silicon:
            logger.info("ğŸ Apple Silicon ê°ì§€ë¨")
        
        # PyTorch ì •ë³´
        logger.info(f"PyTorch ë²„ì „: {torch.__version__}")
        logger.info(f"ë””ë°”ì´ìŠ¤: {self.device_type}")
        
        if self.device_type == "mps":
            logger.info(f"MPS ì‚¬ìš© ê°€ëŠ¥: {torch.backends.mps.is_available()}")
            logger.info(f"MPS ë¹Œë“œë¨: {torch.backends.mps.is_built()}")
        elif self.device_type == "cuda":
            logger.info(f"CUDA ë²„ì „: {torch.version.cuda}")
            logger.info(f"GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
            logger.info(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB")
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        memory_info = self.get_memory_info()
        logger.info(f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory_info['system_memory']['total_gb']:.1f}GB")
        logger.info("=" * 50)


# ì „ì—­ í•¨ìˆ˜ë“¤ ì¶”ê°€ (í˜¸í™˜ì„± ìœ ì§€)
def gpu_config():
    """GPU ì„¤ì • í•¨ìˆ˜ - í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
    return GPUConfig()

def get_optimal_device():
    """ìµœì  ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    config = GPUConfig()
    return config.get_optimal_device()

def get_device_config():
    """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
    config = GPUConfig()
    return {
        'device': config.device_type,
        'memory': f"{config.memory_settings['total_system_memory_gb']:.0f}GB"
    }