# app/core/gpu_config.py
"""
MyCloset AI - M3 Max 128GB ì™„ì „ ìµœì í™” GPU ì„¤ì •
check_memory_available í•¨ìˆ˜ ì¶”ê°€, ëª¨ë“  ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì™„ì „ í•´ê²°
"""

import os
import logging
import torch
import platform
from typing import Dict, Any, Optional, Union, List
from dataclasses import dataclass
from functools import lru_cache
import gc
import json
from pathlib import Path
import psutil
import subprocess

logger = logging.getLogger(__name__)

# ===============================================================
# ğŸ”§ GPU ì„¤ì • ë°ì´í„° í´ë˜ìŠ¤ (íŒŒë¼ë¯¸í„° ë¬¸ì œ í•´ê²°)
# ===============================================================

@dataclass
class GPUConfig:
    """GPU ì„¤ì • ë°ì´í„° í´ë˜ìŠ¤ - ìƒì„±ì íŒŒë¼ë¯¸í„° ë¬¸ì œ í•´ê²°"""
    device: str = "mps"
    device_name: str = "Apple M3 Max"
    memory_gb: float = 128.0
    is_m3_max: bool = True
    optimization_level: str = "maximum"
    device_type: str = "apple_silicon"
    neural_engine_available: bool = True
    metal_performance_shaders: bool = True
    unified_memory_optimization: bool = True
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ê²€ì¦ ë° ì¡°ì •"""
        # M3 Maxê°€ ì•„ë‹Œ ê²½ìš° ì„¤ì • ì¡°ì •
        if not self.is_m3_max:
            self.neural_engine_available = False
            self.metal_performance_shaders = False
            self.optimization_level = "balanced"
            if self.memory_gb > 64:
                self.memory_gb = 16.0  # ì¼ë°˜ì ì¸ ê¸°ë³¸ê°’
    
    @classmethod
    def create_optimal(cls, device: str = None, auto_detect: bool = True) -> 'GPUConfig':
        """ìµœì  ì„¤ì •ìœ¼ë¡œ GPUConfig ìƒì„±"""
        if auto_detect:
            detector = M3MaxDetector()
            return cls(
                device=device or detector.get_optimal_device(),
                device_name=detector.get_device_name(),
                memory_gb=detector.memory_gb,
                is_m3_max=detector.is_m3_max,
                optimization_level="maximum" if detector.is_m3_max else "balanced",
                device_type="apple_silicon" if detector.is_apple_silicon else "generic",
                neural_engine_available=detector.is_m3_max,
                metal_performance_shaders=detector.is_m3_max,
                unified_memory_optimization=detector.is_m3_max
            )
        else:
            return cls()

# ===============================================================
# ğŸ M3 Max ê°ì§€ í´ë˜ìŠ¤
# ===============================================================

class M3MaxDetector:
    """M3 Max í™˜ê²½ ì •ë°€ ê°ì§€"""
    
    def __init__(self):
        self.platform_info = self._get_platform_info()
        self.is_apple_silicon = self._is_apple_silicon()
        self.memory_gb = self._get_memory_gb()
        self.cpu_cores = self._get_cpu_cores()
        self.is_m3_max = self._detect_m3_max()
        
    def _get_platform_info(self) -> Dict[str, str]:
        """í”Œë«í¼ ì •ë³´ ìˆ˜ì§‘"""
        return {
            "system": platform.system(),
            "machine": platform.machine(),
            "processor": platform.processor(),
            "platform": platform.platform(),
            "python_version": platform.python_version()
        }
    
    def _is_apple_silicon(self) -> bool:
        """Apple Silicon ê°ì§€"""
        return (self.platform_info["system"] == "Darwin" and 
                self.platform_info["machine"] == "arm64")
    
    def _get_memory_gb(self) -> float:
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìš©ëŸ‰(GB) ë°˜í™˜"""
        try:
            return round(psutil.virtual_memory().total / (1024**3), 1)
        except:
            return 8.0
    
    def _get_cpu_cores(self) -> int:
        """CPU ì½”ì–´ ìˆ˜ ë°˜í™˜"""
        try:
            return psutil.cpu_count(logical=False) or 4
        except:
            return 4
    
    def _detect_m3_max(self) -> bool:
        """M3 Max í™˜ê²½ ì •ë°€ ê°ì§€"""
        if not self.is_apple_silicon:
            return False
            
        # ë©”ëª¨ë¦¬ ê¸°ë°˜ íŒì • (ë” ì •í™•)
        if self.memory_gb >= 120:  # 128GB M3 Max
            return True
        elif self.memory_gb >= 90:  # 96GB M3 Max  
            return True
        elif self.cpu_cores >= 12:  # M3 MaxëŠ” 12ì½”ì–´ ì´ìƒ
            return True
            
        return False
    
    def get_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        try:
            if torch.backends.mps.is_available() and self.is_apple_silicon:
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        except ImportError:
            return "cpu"
    
    def get_device_name(self) -> str:
        """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
        if self.is_m3_max:
            if self.memory_gb >= 120:
                return "Apple M3 Max (128GB)"
            else:
                return "Apple M3 Max (96GB)"
        elif self.is_apple_silicon:
            return "Apple Silicon"
        else:
            return "Unknown Device"
    
    def get_optimized_settings(self) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì„¤ì • ë°˜í™˜"""
        if self.is_m3_max:
            return {
                "batch_size": 8 if self.memory_gb >= 120 else 4,
                "max_workers": min(12, self.cpu_cores),
                "concurrent_sessions": 8,
                "memory_pool_gb": min(64, self.memory_gb // 2),
                "cache_size_gb": min(32, self.memory_gb // 4),
                "quality_level": "ultra",
                "enable_neural_engine": True,
                "enable_mps": True,
                "optimization_level": "maximum"
            }
        elif self.is_apple_silicon:
            return {
                "batch_size": 2,
                "max_workers": min(4, self.cpu_cores),
                "concurrent_sessions": 4,
                "memory_pool_gb": min(16, self.memory_gb // 2),
                "cache_size_gb": min(8, self.memory_gb // 4),
                "quality_level": "high",
                "enable_neural_engine": False,
                "enable_mps": True,
                "optimization_level": "balanced"
            }
        else:
            return {
                "batch_size": 1,
                "max_workers": 2,
                "concurrent_sessions": 2,
                "memory_pool_gb": 4,
                "cache_size_gb": 2,
                "quality_level": "balanced",
                "enable_neural_engine": False,
                "enable_mps": False,
                "optimization_level": "safe"
            }

# ===============================================================
# ğŸ¯ M3 Max GPU ê´€ë¦¬ì (ë©”ì¸ í´ë˜ìŠ¤) - get ë©”ì„œë“œ ì¶”ê°€
# ===============================================================

class M3MaxGPUManager:
    """M3 Max 128GB ì „ìš© GPU ê´€ë¦¬ì - get ë©”ì„œë“œ ì¶”ê°€ë¡œ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.detector = M3MaxDetector()
        self.device = None
        self.device_name = ""
        self.device_type = ""
        self.memory_gb = 0.0
        self.is_m3_max = False
        self.optimization_level = "balanced"
        self.device_info = {}
        self.model_config = {}
        self.is_initialized = False
        
        # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ë³„ ìµœì í™” ì„¤ì •
        self.pipeline_optimizations = {}
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize()
    
    def _initialize(self):
        """GPU ì„¤ì • ì™„ì „ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”§ M3 Max GPU ì„¤ì • ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. í•˜ë“œì›¨ì–´ ì •ë³´ ì„¤ì •
            self._setup_hardware_info()
            
            # 2. ë””ë°”ì´ìŠ¤ ì„¤ì •
            self._setup_device()
            
            # 3. 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì •
            self._setup_pipeline_optimizations()
            
            # 4. ëª¨ë¸ ì„¤ì •
            self._setup_model_config()
            
            # 5. ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘
            self._collect_device_info()
            
            # 6. í™˜ê²½ ìµœì í™” ì ìš©
            self._apply_optimizations()
            
            self.is_initialized = True
            logger.info(f"ğŸš€ M3 Max GPU ì„¤ì • ì™„ë£Œ: {self.device}")
            
        except Exception as e:
            logger.error(f"âŒ GPU ì„¤ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._fallback_cpu_setup()
    
    def _setup_hardware_info(self):
        """í•˜ë“œì›¨ì–´ ì •ë³´ ì„¤ì •"""
        self.is_m3_max = self.detector.is_m3_max
        self.memory_gb = self.detector.memory_gb
        
        if self.is_m3_max:
            self.optimization_level = "maximum"
            logger.info(f"ğŸ M3 Max {self.memory_gb}GB ê°ì§€!")
        else:
            self.optimization_level = "balanced"
            logger.info(f"ğŸ’» ì¼ë°˜ í™˜ê²½ ê°ì§€: {self.memory_gb}GB")
    
    def _setup_device(self):
        """ë””ë°”ì´ìŠ¤ ì„¤ì • ë° MPS ìµœì í™”"""
        try:
            self.device = self.detector.get_optimal_device()
            self.device_name = self.detector.get_device_name()
            
            if self.device == "mps":
                self.device_type = "mps"
                
                # M3 Max íŠ¹í™” MPS í™˜ê²½ë³€ìˆ˜ ì„¤ì •
                if self.is_m3_max:
                    os.environ.update({
                        'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                        'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
                        'PYTORCH_MPS_ALLOCATOR_POLICY': 'garbage_collection',
                        'METAL_DEVICE_WRAPPER_TYPE': '1',
                        'METAL_PERFORMANCE_SHADERS_ENABLED': '1'
                    })
                    logger.info("ğŸ M3 Max MPS í™˜ê²½ë³€ìˆ˜ ìµœì í™” ì ìš©")
                
                logger.info("ğŸ Apple Silicon MPS í™œì„±í™”")
                
            elif self.device == "cuda":
                self.device_type = "cuda"
                self.device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CUDA GPU"
                logger.info("ğŸš€ CUDA GPU ê°ì§€")
                
            else:
                self.device_type = "cpu"
                self.device_name = "CPU"
                logger.info("ğŸ’» CPU ëª¨ë“œ ì„¤ì •")
                
        except Exception as e:
            logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self._fallback_cpu_setup()
    
    def _setup_pipeline_optimizations(self):
        """8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ë³„ ìµœì í™” ì„¤ì •"""
        
        # ìµœì í™”ëœ ê¸°ë³¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        optimized = self.detector.get_optimized_settings()
        
        # 8ë‹¨ê³„ë³„ íŠ¹í™” ì„¤ì •
        self.pipeline_optimizations = {
            "step_01_human_parsing": {
                "batch_size": optimized["batch_size"] // 2,  # ë©”ëª¨ë¦¬ ì ˆì•½
                "precision": "float16" if self.device != "cpu" else "float32",
                "max_resolution": 512,
                "enable_segmentation_cache": True,
                "memory_fraction": 0.3
            },
            "step_02_pose_estimation": {
                "batch_size": optimized["batch_size"],
                "precision": "float16" if self.device != "cpu" else "float32",
                "keypoint_threshold": 0.3,
                "enable_pose_cache": True,
                "memory_fraction": 0.2
            },
            "step_03_cloth_segmentation": {
                "batch_size": optimized["batch_size"],
                "segmentation_model": "u2net",
                "background_threshold": 0.5,
                "enable_edge_refinement": True,
                "memory_fraction": 0.25
            },
            "step_04_geometric_matching": {
                "batch_size": optimized["batch_size"] // 2,
                "matching_algorithm": "optical_flow",
                "warp_resolution": 256,
                "enable_geometric_cache": True,
                "memory_fraction": 0.3
            },
            "step_05_cloth_warping": {
                "batch_size": optimized["batch_size"],
                "warp_method": "thin_plate_spline",
                "interpolation": "bilinear",
                "preserve_details": True,
                "memory_fraction": 0.25
            },
            "step_06_virtual_fitting": {
                "batch_size": optimized["batch_size"] // 4,  # ê°€ì¥ ë©”ëª¨ë¦¬ ì§‘ì•½ì 
                "diffusion_steps": 20 if self.is_m3_max else 15,
                "guidance_scale": 7.5,
                "enable_safety_checker": True,
                "scheduler": "ddim",
                "memory_fraction": 0.5
            },
            "step_07_post_processing": {
                "batch_size": optimized["batch_size"],
                "enhancement_level": "high" if self.is_m3_max else "medium",
                "noise_reduction": True,
                "color_correction": True,
                "memory_fraction": 0.2
            },
            "step_08_quality_assessment": {
                "batch_size": optimized["batch_size"],
                "quality_metrics": ["ssim", "lpips", "fid"],
                "assessment_threshold": 0.7,
                "enable_automatic_retry": True,
                "memory_fraction": 0.15
            }
        }
        
        logger.info("âš™ï¸ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì • ì™„ë£Œ")
    
    def _setup_model_config(self):
        """ëª¨ë¸ ì„¤ì • êµ¬ì„±"""
        optimized = self.detector.get_optimized_settings()
        
        base_config = {
            "device": self.device,
            "dtype": "float16" if self.device != "cpu" else "float32",
            "batch_size": optimized["batch_size"],
            "memory_fraction": 0.8,
            "optimization_level": self.optimization_level,
            "max_workers": optimized["max_workers"],
            "concurrent_sessions": optimized["concurrent_sessions"]
        }
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if self.is_m3_max:
            base_config.update({
                "use_neural_engine": True,
                "metal_performance_shaders": True,
                "unified_memory_optimization": True,
                "high_resolution_processing": True,
                "concurrent_pipeline_steps": 3,
                "memory_pool_size_gb": optimized["memory_pool_gb"],
                "model_cache_size_gb": optimized["cache_size_gb"],
                "intermediate_cache_gb": optimized["cache_size_gb"] // 2
            })
            logger.info("ğŸ M3 Max íŠ¹í™” ëª¨ë¸ ì„¤ì • ì ìš©")
        
        self.model_config = base_config
        logger.info(f"âš™ï¸ ëª¨ë¸ ì„¤ì • ì™„ë£Œ: ë°°ì¹˜={base_config['batch_size']}, ì •ë°€ë„={base_config['dtype']}")
    
    def _collect_device_info(self):
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        try:
            base_info = {
                "device": self.device,
                "device_name": self.device_name,
                "device_type": self.device_type,
                "platform": self.detector.platform_info["system"],
                "architecture": self.detector.platform_info["machine"],
                "pytorch_version": torch.__version__,
                "python_version": self.detector.platform_info["python_version"],
                "optimization_level": self.optimization_level,
                "total_memory_gb": self.memory_gb,
                "is_m3_max": self.is_m3_max
            }
            
            # M3 Max íŠ¹í™” ì •ë³´
            if self.is_m3_max:
                base_info.update({
                    "neural_engine_available": True,
                    "neural_engine_tops": "15.8 TOPS",
                    "gpu_cores": "30-40 cores", 
                    "memory_bandwidth": "400GB/s",
                    "unified_memory": True,
                    "metal_performance_shaders": True,
                    "optimized_for_pipeline": "8-step virtual fitting"
                })
            
            # MPS íŠ¹í™” ì •ë³´
            if self.device == "mps":
                base_info.update({
                    "mps_available": True,
                    "mps_fallback_enabled": True,
                    "metal_api_available": True
                })
            
            # CUDA ì •ë³´
            elif self.device == "cuda" and torch.cuda.is_available():
                base_info.update({
                    "cuda_version": torch.version.cuda,
                    "gpu_memory_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3),
                    "compute_capability": torch.cuda.get_device_capability(0)
                })
            
            self.device_info = base_info
            logger.info(f"â„¹ï¸ ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {self.device_name}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.device_info = {"device": self.device, "error": str(e)}
    
    def _apply_optimizations(self):
        """í™˜ê²½ ìµœì í™” ì ìš©"""
        try:
            # PyTorch ë©€í‹°ìŠ¤ë ˆë”© ì„¤ì •
            num_threads = self.detector.get_optimized_settings()["max_workers"]
            torch.set_num_threads(num_threads)
            
            # MPS ìµœì í™”
            if self.device == "mps":
                if self.is_m3_max:
                    os.environ['METAL_PERFORMANCE_SHADERS_ENABLED'] = '1'
                    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                logger.info("âœ… MPS ìµœì í™” ì ìš© ì™„ë£Œ")
            
            # CUDA ìµœì í™”
            elif self.device == "cuda":
                torch.backends.cudnn.enabled = True
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                logger.info("âœ… CUDA ìµœì í™” ì ìš© ì™„ë£Œ")
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
            gc.collect()
            
            logger.info(f"âœ… í™˜ê²½ ìµœì í™” ì™„ë£Œ (ìŠ¤ë ˆë“œ: {num_threads})")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
    
    def _fallback_cpu_setup(self):
        """CPU í´ë°± ì„¤ì •"""
        self.device = "cpu"
        self.device_type = "cpu"
        self.device_name = "CPU (Fallback)"
        self.is_m3_max = False
        self.optimization_level = "safe"
        
        self.model_config = {
            "device": "cpu",
            "dtype": "float32",
            "batch_size": 1,
            "memory_fraction": 0.5,
            "optimization_level": "safe"
        }
        
        self.device_info = {
            "device": "cpu",
            "device_name": "CPU (Fallback)",
            "error": "GPU initialization failed"
        }
        
        logger.warning("ğŸš¨ CPU í´ë°± ëª¨ë“œë¡œ ì„¤ì •ë¨")
    
    # ==========================================
    # ğŸ”§ get ë©”ì„œë“œ ì¶”ê°€ (í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
    # ==========================================
    
    def get(self, key: str, default: Any = None) -> Any:
        """ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì ‘ê·¼ ë©”ì„œë“œ - step_routes.py í˜¸í™˜ì„±ì„ ìœ„í•´ ì¶”ê°€"""
        
        # ì£¼ìš” ì†ì„±ë“¤ì— ëŒ€í•œ ë§¤í•‘
        attribute_mapping = {
            'device': self.device,
            'device_name': self.device_name,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'optimization_level': self.optimization_level,
            'is_initialized': self.is_initialized,
            'device_info': self.device_info,
            'model_config': self.model_config,
            'pipeline_optimizations': self.pipeline_optimizations
        }
        
        # ì§ì ‘ ì†ì„± ë§¤í•‘ì—ì„œ ì°¾ê¸°
        if key in attribute_mapping:
            return attribute_mapping[key]
        
        # ëª¨ë¸ ì„¤ì •ì—ì„œ ì°¾ê¸°
        if key in self.model_config:
            return self.model_config[key]
        
        # ë””ë°”ì´ìŠ¤ ì •ë³´ì—ì„œ ì°¾ê¸°
        if key in self.device_info:
            return self.device_info[key]
        
        # íŒŒì´í”„ë¼ì¸ ìµœì í™”ì—ì„œ ì°¾ê¸°
        if key in self.pipeline_optimizations:
            return self.pipeline_optimizations[key]
        
        # ì†ì„±ìœ¼ë¡œ ì§ì ‘ ì ‘ê·¼ ì‹œë„
        if hasattr(self, key):
            return getattr(self, key)
        
        # ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
        return default
    
    def keys(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ ëª©ë¡ ë°˜í™˜"""
        return [
            'device', 'device_name', 'device_type', 'memory_gb',
            'is_m3_max', 'optimization_level', 'is_initialized',
            'device_info', 'model_config', 'pipeline_optimizations'
        ]
    
    def items(self):
        """í‚¤-ê°’ ìŒ ë°˜í™˜"""
        return [(key, self.get(key)) for key in self.keys()]
    
    def __getitem__(self, key: str) -> Any:
        """[] ì ‘ê·¼ì ì§€ì›"""
        result = self.get(key)
        if result is None:
            raise KeyError(f"Key '{key}' not found")
        return result
    
    def __contains__(self, key: str) -> bool:
        """in ì—°ì‚°ì ì§€ì›"""
        return self.get(key) is not None
    
    # ==========================================
    # ê¸°ì¡´ ì ‘ê·¼ì ë©”ì„œë“œë“¤ (í˜¸í™˜ì„± ìœ ì§€)
    # ==========================================
    
    def get_device(self) -> str:
        """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self.device
    
    def get_device_name(self) -> str:
        """ë””ë°”ì´ìŠ¤ ì´ë¦„ ë°˜í™˜"""
        return self.device_name
    
    def get_device_type(self) -> str:
        """ë””ë°”ì´ìŠ¤ íƒ€ì… ë°˜í™˜"""
        return self.device_type
    
    def get_recommended_batch_size(self) -> int:
        """ê¶Œì¥ ë°°ì¹˜ í¬ê¸° ë°˜í™˜"""
        return self.model_config.get('batch_size', 1)
    
    def get_recommended_precision(self) -> str:
        """ê¶Œì¥ ì •ë°€ë„ ë°˜í™˜"""
        return self.model_config.get('dtype', 'float32')
    
    def get_memory_fraction(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„ìœ¨ ë°˜í™˜"""
        return self.model_config.get('memory_fraction', 0.5)
    
    def setup_multiprocessing(self) -> int:
        """ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ ìˆ˜ ì„¤ì •"""
        return self.model_config.get('max_workers', 4)
    
    def get_device_config(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
        return GPUConfig(
            device=self.device,
            device_name=self.device_name,
            device_type=self.device_type,
            memory_gb=self.memory_gb,
            is_m3_max=self.is_m3_max,
            optimization_level=self.optimization_level,
            neural_engine_available=self.is_m3_max,
            metal_performance_shaders=self.is_m3_max,
            unified_memory_optimization=self.is_m3_max
        ).__dict__
    
    def get_model_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
        return self.model_config.copy()
    
    def get_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return self.device_info.copy()
    
    def get_pipeline_config(self, step_name: str) -> Dict[str, Any]:
        """íŠ¹ì • íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        return self.pipeline_optimizations.get(step_name, {})
    
    def get_all_pipeline_configs(self) -> Dict[str, Any]:
        """ëª¨ë“  íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        return self.pipeline_optimizations.copy()

# ===============================================================
# ğŸ¯ M3 Optimizer í´ë˜ìŠ¤ (ëˆ„ë½ëœ í´ë˜ìŠ¤ ì¶”ê°€)
# ===============================================================

class M3Optimizer:
    """M3 Max ì „ìš© ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self, device_name: str, memory_gb: float, is_m3_max: bool, optimization_level: str):
        """M3 ìµœì í™” ì´ˆê¸°í™”"""
        self.device_name = device_name
        self.memory_gb = memory_gb
        self.is_m3_max = is_m3_max
        self.optimization_level = optimization_level
        
        logger.info(f"ğŸ M3Optimizer ì´ˆê¸°í™”: {device_name}, {memory_gb}GB, {optimization_level}")
        
        if is_m3_max:
            self._apply_m3_max_optimizations()
    
    def _apply_m3_max_optimizations(self):
        """M3 Max ì „ìš© ìµœì í™” ì ìš©"""
        try:
            if torch.backends.mps.is_available():
                logger.info("ğŸ§  Neural Engine ìµœì í™” í™œì„±í™”")
                logger.info("âš™ï¸ Metal Performance Shaders í™œì„±í™”")
                
                # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”
                self.pipeline_config = {
                    "stages": 8,
                    "parallel_processing": True,
                    "batch_optimization": True,
                    "memory_pooling": True
                }
                
                logger.info("âš™ï¸ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™” ì„¤ì • ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def optimize_model(self, model):
        """ëª¨ë¸ ìµœì í™”"""
        if not self.is_m3_max:
            return model
            
        try:
            if hasattr(model, 'to'):
                model = model.to('mps')
            logger.info("âœ… ëª¨ë¸ M3 Max ìµœì í™” ì™„ë£Œ")
            return model
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return model

# ===============================================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° ë° í¸ì˜ í•¨ìˆ˜ë“¤ (ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì¶”ê°€)
# ===============================================================

def check_memory_available(device: Optional[str] = None, min_gb: float = 1.0) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ìƒíƒœ í™•ì¸ - ëˆ„ë½ëœ í•¨ìˆ˜ ì¶”ê°€"""
    try:
        current_device = device or gpu_config.device
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸
        virtual_memory = psutil.virtual_memory()
        system_memory = {
            "total_gb": round(virtual_memory.total / (1024**3), 2),
            "available_gb": round(virtual_memory.available / (1024**3), 2),
            "used_gb": round(virtual_memory.used / (1024**3), 2),
            "percent_used": virtual_memory.percent
        }
        
        result = {
            "device": current_device,
            "system_memory": system_memory,
            "is_available": system_memory["available_gb"] >= min_gb,
            "min_required_gb": min_gb,
            "timestamp": psutil.boot_time()
        }
        
        # ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ í™•ì¸
        if current_device == "cuda" and torch.cuda.is_available():
            try:
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                gpu_allocated = torch.cuda.memory_allocated(0) / (1024**3)
                gpu_cached = torch.cuda.memory_reserved(0) / (1024**3)
                
                result["gpu_memory"] = {
                    "total_gb": round(gpu_memory, 2),
                    "allocated_gb": round(gpu_allocated, 2),
                    "cached_gb": round(gpu_cached, 2),
                    "available_gb": round(gpu_memory - gpu_allocated, 2)
                }
                
                result["is_available"] = result["is_available"] and (gpu_memory - gpu_allocated) >= min_gb
                
            except Exception as e:
                result["gpu_memory_error"] = str(e)
        
        elif current_device == "mps":
            # MPSëŠ” í†µí•© ë©”ëª¨ë¦¬ ì‚¬ìš©
            result["mps_memory"] = {
                "unified_memory": True,
                "total_gb": system_memory["total_gb"],
                "available_gb": system_memory["available_gb"],
                "note": "MPS uses unified memory system"
            }
        
        logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬ í™•ì¸ ì™„ë£Œ: {current_device} ({system_memory['available_gb']:.1f}GB ì‚¬ìš© ê°€ëŠ¥)")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        return {
            "device": device or "unknown",
            "error": str(e),
            "is_available": False,
            "min_required_gb": min_gb
        }

def optimize_memory(device: Optional[str] = None, aggressive: bool = False) -> Dict[str, Any]:
    """M3 Max ìµœì í™”ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬"""
    try:
        current_device = device or gpu_config.device
        start_memory = psutil.virtual_memory().percent
        
        # ê¸°ë³¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        result = {
            "success": True,
            "device": current_device,
            "start_memory_percent": start_memory,
            "method": "standard_gc"
        }
        
        if current_device == "mps":
            try:
                # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ (PyTorch ë²„ì „ë³„ ëŒ€ì‘)
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                    result["method"] = "mps_empty_cache"
                elif hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()
                    result["method"] = "mps_synchronize"
                
                if aggressive and gpu_config.is_m3_max:
                    torch.mps.synchronize()
                    gc.collect()
                    result["method"] = "m3_max_aggressive_cleanup"
                    result["aggressive"] = True
                
            except Exception as mps_error:
                logger.warning(f"MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
                result["mps_error"] = str(mps_error)
        
        elif current_device == "cuda":
            try:
                torch.cuda.empty_cache()
                if aggressive:
                    torch.cuda.synchronize()
                result["method"] = "cuda_empty_cache"
                if aggressive:
                    result["aggressive"] = True
            except Exception as cuda_error:
                logger.warning(f"CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {cuda_error}")
                result["cuda_error"] = str(cuda_error)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ìƒíƒœ
        end_memory = psutil.virtual_memory().percent
        memory_freed = start_memory - end_memory
        
        result.update({
            "end_memory_percent": end_memory,
            "memory_freed_percent": memory_freed,
            "m3_max_optimized": gpu_config.is_m3_max
        })
        
        if memory_freed > 0:
            logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ {memory_freed:.1f}% ì •ë¦¬ë¨ ({result['method']})")
        
        return result
        
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "device": device or "unknown",
            "method": "failed"
        }

def get_optimal_settings() -> Dict[str, Any]:
    """ìµœì  ì„¤ì • ë°˜í™˜ - ëˆ„ë½ëœ í•¨ìˆ˜ ì¶”ê°€"""
    return gpu_config.detector.get_optimized_settings()

def get_device_capabilities() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ê¸°ëŠ¥ ë°˜í™˜"""
    config = get_gpu_config()
    
    capabilities = {
        "supports_fp16": config.device != "cpu",
        "supports_int8": True,
        "supports_compilation": config.device in ["cuda", "cpu"],
        "supports_parallel_inference": True,
        "max_batch_size": config.get_recommended_batch_size() * 2,
        "recommended_image_size": (512, 512) if config.is_m3_max else (256, 256),
        "supports_8step_pipeline": True
    }
    
    if config.device == "mps":
        capabilities.update({
            "supports_neural_engine": config.is_m3_max,
            "supports_metal_shaders": True,
            "mps_fallback_enabled": True,
            "unified_memory_optimization": config.is_m3_max
        })
    elif config.device == "cuda":
        capabilities.update({
            "cuda_version": torch.version.cuda if hasattr(torch.version, 'cuda') else None,
            "cudnn_enabled": torch.backends.cudnn.enabled,
            "tensor_cores_available": True
        })
    
    return capabilities

def apply_optimizations():
    """ìµœì í™” ì„¤ì • ì ìš©"""
    config = get_gpu_config()
    settings = get_optimal_settings()
    
    try:
        # PyTorch í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        if config.device == "mps":
            os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            
            if config.is_m3_max:
                os.environ["PYTORCH_MPS_ALLOCATOR_POLICY"] = "garbage_collection"
                os.environ["METAL_PERFORMANCE_SHADERS_ENABLED"] = "1"
        
        # ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì •
        torch.set_num_threads(settings["max_workers"])
        
        # CUDA ì„¤ì •
        if config.device == "cuda":
            torch.backends.cudnn.enabled = True
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
        
        logger.info("âœ… GPU ìµœì í™” ì„¤ì • ì ìš© ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"GPU ìµœì í™” ì„¤ì • ì ìš© ì‹¤íŒ¨: {e}")
        return False

def get_memory_info(device: Optional[str] = None) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
    try:
        current_device = device or gpu_config.device
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
        vm = psutil.virtual_memory()
        memory_info = {
            "device": current_device,
            "system_memory": {
                "total_gb": round(vm.total / (1024**3), 2),
                "available_gb": round(vm.available / (1024**3), 2),
                "used_gb": round(vm.used / (1024**3), 2),
                "percent": vm.percent
            }
        }
        
        # ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì •ë³´
        if current_device == "cuda" and torch.cuda.is_available():
            gpu_props = torch.cuda.get_device_properties(0)
            memory_info["gpu_memory"] = {
                "total_gb": round(gpu_props.total_memory / (1024**3), 2),
                "allocated_gb": round(torch.cuda.memory_allocated(0) / (1024**3), 2),
                "cached_gb": round(torch.cuda.memory_reserved(0) / (1024**3), 2)
            }
        
        elif current_device == "mps":
            memory_info["mps_memory"] = {
                "unified_memory": True,
                "total_gb": memory_info["system_memory"]["total_gb"],
                "note": "MPS uses unified memory system"
            }
        
        return memory_info
        
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            "device": device or "unknown",
            "error": str(e)
        }

# ==========================================
# ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì „ì—­ ë³€ìˆ˜
# ==========================================

def _initialize_gpu_optimizations():
    """GPU ìµœì í™” ì´ˆê¸°í™”"""
    try:
        logger.info("ğŸš€ GPU ìµœì í™” ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"GPU ìµœì í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ğŸ”¥ ì „ì—­ GPU ì„¤ì • ë§¤ë‹ˆì € ìƒì„± (get ë©”ì„œë“œ í¬í•¨)
gpu_config = M3MaxGPUManager()

# í¸ì˜ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ë“¤
DEVICE = gpu_config.device
DEVICE_NAME = gpu_config.device_name
DEVICE_TYPE = gpu_config.device_type
MODEL_CONFIG = gpu_config.model_config
DEVICE_INFO = gpu_config.device_info
IS_M3_MAX = gpu_config.is_m3_max

# ==========================================
# ì£¼ìš” í•¨ìˆ˜ë“¤ (í˜¸í™˜ì„±)
# ==========================================

@lru_cache(maxsize=1)
def get_gpu_config() -> M3MaxGPUManager:
    """GPU ì„¤ì • ë§¤ë‹ˆì € ë°˜í™˜ (ìºì‹œë¨)"""
    return gpu_config

def get_device_config() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_device_config()

def get_model_config() -> Dict[str, Any]:
    """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
    return gpu_config.get_model_config()

def get_device_info() -> Dict[str, Any]:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
    return gpu_config.get_device_info()

def get_device() -> str:
    """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return gpu_config.get_device()

def is_m3_max() -> bool:
    """M3 Max ì—¬ë¶€ í™•ì¸"""
    return gpu_config.is_m3_max

# ëª¨ë“ˆ ë¡œë“œì‹œ ìë™ ìµœì í™” ì ìš©
_initialize_gpu_optimizations()

# ì´ˆê¸°í™” ìƒíƒœ ë¡œê¹…
if gpu_config.is_initialized:
    logger.info("âœ… M3 Max GPU ì„¤ì • ê²€ì¦ ì™„ë£Œ")
    logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {DEVICE}")
    logger.info(f"ğŸ M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
else:
    logger.warning("âš ï¸ GPU ì„¤ì • ê²€ì¦ ì‹¤íŒ¨")

# M3 Max ìƒíƒœ ë¡œê¹…
if gpu_config.is_m3_max:
    logger.info("ğŸ M3 Max 128GB ìµœì í™” í™œì„±í™”:")
    logger.info(f"  - Neural Engine: {'âœ…' if MODEL_CONFIG.get('use_neural_engine') else 'âŒ'}")
    logger.info(f"  - Metal Performance Shaders: {'âœ…' if MODEL_CONFIG.get('metal_performance_shaders') else 'âŒ'}")
    logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {MODEL_CONFIG.get('batch_size', 1)}")
    logger.info(f"  - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”: âœ…")
    logger.info(f"  - ë©”ëª¨ë¦¬ ëŒ€ì—­í­: {DEVICE_INFO.get('memory_bandwidth', 'N/A')}")

# ==========================================
# Export ë¦¬ìŠ¤íŠ¸
# ==========================================

__all__ = [
    # ì£¼ìš” ê°ì²´ë“¤
    'gpu_config', 'DEVICE', 'DEVICE_NAME', 'DEVICE_TYPE', 'MODEL_CONFIG', 'DEVICE_INFO', 'IS_M3_MAX',
    
    # í•µì‹¬ í•¨ìˆ˜ë“¤
    'get_gpu_config', 'get_device_config', 'get_model_config', 'get_device_info',
    'get_device', 'is_m3_max', 'get_optimal_settings', 'get_device_capabilities',
    'apply_optimizations',
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤
    'check_memory_available', 'optimize_memory', 'get_memory_info',
    
    # í´ë˜ìŠ¤ë“¤
    'M3MaxGPUManager', 'GPUConfig', 'M3Optimizer', 'M3MaxDetector'
]

logger.info("ğŸ‰ M3 Max GPU ì„¤ì • ì™„ë£Œ! (check_memory_available í•¨ìˆ˜ ì¶”ê°€, ëª¨ë“  ëˆ„ë½ í•¨ìˆ˜ í•´ê²°)")