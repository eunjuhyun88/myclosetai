# backend/app/core/unified_session_database.py
"""
ğŸ”¥ MyCloset AI í†µí•© Session Database ì‹œìŠ¤í…œ
================================================================================

âœ… ì „ì²´ AI íŒŒì´í”„ë¼ì¸ì— ì ìš© ê°€ëŠ¥í•œ í†µí•© ë°ì´í„°ë² ì´ìŠ¤
âœ… Stepë³„ ë°ì´í„° ì €ì¥ ë° ì „ë‹¬ ìµœì í™”
âœ… ì‹¤ì‹œê°„ ì„¸ì…˜ ìœ ì§€ ë° ëª¨ë‹ˆí„°ë§
âœ… ë©”ëª¨ë¦¬ ìºì‹œì™€ ë””ìŠ¤í¬ ì €ì¥ ì—°ë™
âœ… ë°ì´í„° ì••ì¶• ë° ì••ì¶• í•´ì œ
âœ… Stepê°„ ë°ì´í„° íë¦„ ê´€ë¦¬
âœ… ì„±ëŠ¥ ìµœì í™” ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘

Author: MyCloset AI Team
Date: 2025-01-27
Version: 1.0.0
"""

import sqlite3
import json
import logging
import asyncio
import threading
import time
import hashlib
import zlib
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from contextlib import contextmanager
import traceback

# NumPyì™€ PyTorch import (ì„ íƒì )
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

logger = logging.getLogger(__name__)

# =============================================================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° ì •ì˜
# =============================================================================

@dataclass
class StepData:
    """Stepë³„ ë°ì´í„° êµ¬ì¡°"""
    step_id: int
    step_name: str
    session_id: str
    input_data: Dict[str, Any]
    output_data: Dict[str, Any]
    processing_time: float
    quality_score: float
    status: str  # 'pending', 'processing', 'completed', 'failed'
    error_message: Optional[str] = None
    created_at: datetime = None
    updated_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()
        if self.updated_at is None:
            self.updated_at = datetime.now()

@dataclass
class SessionInfo:
    """ì„¸ì…˜ ì •ë³´"""
    session_id: str
    created_at: str
    updated_at: str
    status: str
    person_image_path: str
    clothing_image_path: str
    measurements: Dict[str, Any]
    total_steps: int
    completed_steps: List[int]
    current_step: int
    progress_percent: float
    metadata: Dict[str, Any] = None  # metadata í•„ë“œ ì¶”ê°€
    
    def __post_init__(self):
        if self.completed_steps is None:
            self.completed_steps = []
        if self.updated_at is None:
            self.updated_at = datetime.now()

@dataclass
class DataFlow:
    """Stepê°„ ë°ì´í„° íë¦„ ì •ì˜"""
    source_step: int
    target_step: int
    data_type: str  # 'image', 'mask', 'keypoints', 'matrix', 'result'
    data_key: str
    required: bool = True
    validation_rules: Optional[Dict[str, Any]] = None

# =============================================================================
# ğŸ”¥ í†µí•© Session Database í´ë˜ìŠ¤
# =============================================================================

class UnifiedSessionDatabase:
    """ì „ì²´ AI íŒŒì´í”„ë¼ì¸ì„ ìœ„í•œ í†µí•© Session Database"""
    
    def __init__(self, db_path: str = "unified_sessions.db", enable_cache: bool = True):
        self.db_path = Path(db_path)
        self.enable_cache = enable_cache
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self._init_database()
        
        # ë©”ëª¨ë¦¬ ìºì‹œ
        self.session_cache = {}
        self.step_cache = {}
        self.cache_lock = threading.RLock()
        self.max_cache_size = 100
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_metrics = {
            'cache_hits': 0,
            'cache_misses': 0,
            'db_queries': 0,
            'compression_ratio': 0.0
        }
        
        # ë°ì´í„° íë¦„ ì •ì˜
        self.data_flows = self._define_data_flows()
        
        logger.info(f"âœ… UnifiedSessionDatabase ì´ˆê¸°í™” ì™„ë£Œ: {db_path}")
    
    def _init_database(self):
        """í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                # 1. ì„¸ì…˜ ë©”ì¸ í…Œì´ë¸”
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS sessions (
                        session_id TEXT PRIMARY KEY,
                        created_at TEXT NOT NULL,
                        updated_at TEXT NOT NULL,
                        status TEXT DEFAULT 'active',
                        person_image_path TEXT NOT NULL,
                        clothing_image_path TEXT NOT NULL,
                        measurements TEXT,
                        total_steps INTEGER DEFAULT 8,
                        completed_steps TEXT,
                        current_step INTEGER DEFAULT 1,
                        progress_percent REAL DEFAULT 0.0,
                        metadata TEXT
                    )
                """)
                
                # 2. Step ê²°ê³¼ í…Œì´ë¸” (ìµœì í™”ëœ êµ¬ì¡°)
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS step_results (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        session_id TEXT NOT NULL,
                        step_id INTEGER NOT NULL,
                        step_name TEXT NOT NULL,
                        input_data TEXT NOT NULL,
                        output_data TEXT NOT NULL,
                        processing_time REAL DEFAULT 0.0,
                        quality_score REAL DEFAULT 0.0,
                        status TEXT DEFAULT 'pending',
                        error_message TEXT,
                        created_at TEXT NOT NULL,
                        updated_at TEXT NOT NULL,
                        data_hash TEXT,
                        compressed_size INTEGER,
                        FOREIGN KEY (session_id) REFERENCES sessions (session_id) ON DELETE CASCADE,
                        UNIQUE(session_id, step_id)
                    )
                """)
                
                # 3. Step ê°„ ë°ì´í„° íë¦„ í…Œì´ë¸”
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS step_data_flow (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        session_id TEXT NOT NULL,
                        source_step INTEGER NOT NULL,
                        target_step INTEGER NOT NULL,
                        data_type TEXT NOT NULL,
                        data_key TEXT NOT NULL,
                        data_value TEXT,
                        data_hash TEXT,
                        required BOOLEAN DEFAULT 1,
                        validation_status TEXT DEFAULT 'pending',
                        created_at TEXT NOT NULL,
                        FOREIGN KEY (session_id) REFERENCES sessions (session_id) ON DELETE CASCADE,
                        UNIQUE(session_id, source_step, target_step, data_type)
                    )
                """)
                
                # 4. ì´ë¯¸ì§€ ë° íŒŒì¼ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS file_metadata (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        session_id TEXT NOT NULL,
                        step_id INTEGER NOT NULL,
                        file_type TEXT NOT NULL,
                        file_path TEXT NOT NULL,
                        file_size INTEGER,
                        mime_type TEXT DEFAULT 'image/jpeg',
                        metadata TEXT,
                        created_at TEXT NOT NULL,
                        FOREIGN KEY (session_id) REFERENCES sessions (session_id) ON DELETE CASCADE
                    )
                """)
                
                # 5. ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…Œì´ë¸”
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS performance_metrics (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        session_id TEXT NOT NULL,
                        step_id INTEGER NOT NULL,
                        metric_name TEXT NOT NULL,
                        metric_value REAL,
                        metric_unit TEXT,
                        timestamp TEXT NOT NULL,
                        metadata TEXT,
                        FOREIGN KEY (session_id) REFERENCES sessions (session_id) ON DELETE CASCADE
                    )
                """)
                
                # ì¸ë±ìŠ¤ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_sessions_status ON sessions (status)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_sessions_created ON sessions (created_at)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_step_results_session ON step_results (session_id)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_step_results_step ON step_results (step_id)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_step_results_status ON step_results (status)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_data_flow_session ON step_data_flow (session_id)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_data_flow_target ON step_data_flow (target_step)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_file_metadata_session ON file_metadata (session_id)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_performance_metrics_session ON performance_metrics (session_id)")
                
                conn.commit()
                logger.info("âœ… í†µí•© ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            raise
    
    def _define_data_flows(self) -> List[DataFlow]:
        """Stepê°„ ë°ì´í„° íë¦„ ì •ì˜ - AI ì¶”ë¡ ì— í•„ìš”í•œ ëª¨ë“  ë°ì´í„° í¬í•¨"""
        return [
            # ========================================
            # Step 1 (Human Parsing) -> Step 2 (Pose Estimation)
            # ========================================
            DataFlow(1, 2, 'mask', 'segmentation_mask', required=True),
            DataFlow(1, 2, 'mask_path', 'segmentation_mask_path', required=True),
            DataFlow(1, 2, 'image', 'person_image_path', required=True),
            DataFlow(1, 2, 'parsing', 'human_parsing_result', required=True),
            DataFlow(1, 2, 'confidence', 'parsing_confidence', required=False),
            
            # ========================================
            # Step 1 -> Step 3 (Cloth Segmentation)
            # ========================================
            DataFlow(1, 3, 'mask', 'segmentation_mask', required=True),
            DataFlow(1, 3, 'mask_path', 'segmentation_mask_path', required=True),
            DataFlow(1, 3, 'image', 'person_image_path', required=True),
            DataFlow(1, 3, 'parsing', 'human_parsing_result', required=True),
            
            # ========================================
            # Step 2 (Pose Estimation) -> Step 3
            # ========================================
            DataFlow(2, 3, 'keypoints', 'pose_keypoints', required=True),
            DataFlow(2, 3, 'skeleton', 'pose_skeleton', required=True),
            DataFlow(2, 3, 'confidence', 'pose_confidence', required=False),
            
            # ========================================
            # Step 2 -> Step 4 (Geometric Matching)
            # ========================================
            DataFlow(2, 4, 'keypoints', 'pose_keypoints', required=True),
            DataFlow(2, 4, 'skeleton', 'pose_skeleton', required=True),
            DataFlow(1, 4, 'mask', 'segmentation_mask', required=True),
            DataFlow(3, 4, 'mask', 'cloth_segmentation_mask', required=True),
            DataFlow(3, 4, 'mask_path', 'cloth_segmentation_mask_path', required=True),
            
            # ========================================
            # Step 3 -> Step 4
            # ========================================
            DataFlow(3, 4, 'mask', 'cloth_segmentation_mask', required=True),
            DataFlow(3, 4, 'mask_path', 'cloth_segmentation_mask_path', required=True),
            DataFlow(3, 4, 'confidence', 'cloth_confidence', required=False),
            
            # ========================================
            # Step 4 -> Step 5 (Cloth Warping)
            # ========================================
            DataFlow(4, 5, 'matrix', 'transformation_matrix', required=True),
            DataFlow(4, 5, 'confidence', 'matching_confidence', required=False),
            DataFlow(3, 5, 'mask', 'cloth_segmentation_mask', required=True),
            DataFlow(3, 5, 'mask_path', 'cloth_segmentation_mask_path', required=True),
            
            # ========================================
            # Step 5 -> Step 6 (Virtual Fitting)
            # ========================================
            DataFlow(5, 6, 'image', 'warped_clothing', required=True),
            DataFlow(5, 6, 'image_path', 'warped_clothing_path', required=True),
            DataFlow(1, 6, 'image', 'person_image_path', required=True),
            DataFlow(2, 6, 'keypoints', 'pose_keypoints', required=True),
            DataFlow(2, 6, 'skeleton', 'pose_skeleton', required=True),
            DataFlow(1, 6, 'mask', 'segmentation_mask', required=True),
            
            # ========================================
            # Step 6 -> Step 7 (Post Processing)
            # ========================================
            DataFlow(6, 7, 'image', 'fitted_image', required=True),
            DataFlow(6, 7, 'image_path', 'fitted_image_path', required=True),
            DataFlow(6, 7, 'confidence', 'fitting_confidence', required=False),
            
            # ========================================
            # Step 7 -> Step 8 (Quality Assessment)
            # ========================================
            DataFlow(7, 8, 'image', 'processed_image', required=True),
            DataFlow(7, 8, 'image_path', 'processed_image_path', required=True),
            DataFlow(7, 8, 'metadata', 'processing_metadata', required=False),
            
            # ========================================
            # Step 8 -> Step 9 (Final Output)
            # ========================================
            DataFlow(8, 9, 'image', 'final_image', required=True),
            DataFlow(8, 9, 'image_path', 'final_image_path', required=True),
            DataFlow(8, 9, 'score', 'quality_score', required=True),
            DataFlow(8, 9, 'metadata', 'assessment_metadata', required=False),
        ]
    
    @contextmanager
    def _get_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € (ê°•í™”ëœ ë½ í•´ê²° ë° ì„±ëŠ¥ ìµœì í™”)"""
        conn = None
        max_retries = 15  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
        retry_delay = 0.01  # ì´ˆê¸° ì§€ì—° ì‹œê°„ ê°ì†Œ
        
        for attempt in range(max_retries):
            try:
                # ë” ê°•ë ¥í•œ ë½ í•´ê²° ì„¤ì •
                conn = sqlite3.connect(self.db_path, check_same_thread=False, timeout=180.0)
                conn.row_factory = sqlite3.Row
                
                # ì„±ëŠ¥ ìµœì í™”ëœ PRAGMA ì„¤ì • (ë½ ë¬¸ì œ ì™„ì „ í•´ê²°)
                try:
                    conn.execute("PRAGMA journal_mode=DELETE")  # WAL ëŒ€ì‹  DELETE ëª¨ë“œ ì‚¬ìš© (ë½ ë°©ì§€)
                except:
                    pass  # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                
                try:
                    conn.execute("PRAGMA synchronous=NORMAL")
                    conn.execute("PRAGMA cache_size=100000")  # ìºì‹œ í¬ê¸° ì¦ê°€
                    conn.execute("PRAGMA temp_store=MEMORY")
                    conn.execute("PRAGMA busy_timeout=120000")  # 120ì´ˆ ëŒ€ê¸°
                    conn.execute("PRAGMA locking_mode=NORMAL")  # ì¼ë°˜ ë½ ëª¨ë“œ
                    conn.execute("PRAGMA mmap_size=67108864")  # 64MBë¡œ ì¤„ì„ (ë½ ë°©ì§€)
                    conn.execute("PRAGMA page_size=4096")  # ê¸°ë³¸ í˜ì´ì§€ í¬ê¸°
                    conn.execute("PRAGMA auto_vacuum=NONE")  # ìë™ ì •ë¦¬ ë¹„í™œì„±í™”
                    conn.execute("PRAGMA foreign_keys=OFF")  # ì™¸ë˜í‚¤ ì œì•½ ë¹„í™œì„±í™” (ì„±ëŠ¥ í–¥ìƒ)
                except:
                    pass  # ê°œë³„ PRAGMA ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                
                yield conn
                break  # ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ
                
            except sqlite3.OperationalError as e:
                if "database is locked" in str(e) and attempt < max_retries - 1:
                    wait_time = min(retry_delay * (1.8 ** attempt), 5.0)  # ì§€ìˆ˜ ë°±ì˜¤í”„ + ìµœëŒ€ ì œí•œ
                    logger.warning(f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë½ ê°ì§€ (ì‹œë„ {attempt + 1}/{max_retries}), {wait_time:.2f}ì´ˆ í›„ ì¬ì‹œë„")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
                    raise
            except Exception as e:
                logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
                raise
            finally:
                if conn:
                    try:
                        conn.close()
                    except:
                        pass
    
    # =========================================================================
    # ğŸ”¥ ì„¸ì…˜ ê´€ë¦¬ ë©”ì„œë“œë“¤
    # =========================================================================
    
    async def create_session(self, person_image_path: str, clothing_image_path: str, 
                           measurements: Dict[str, Any] = None) -> str:
        """ìƒˆ ì„¸ì…˜ ìƒì„±"""
        try:
            session_id = self._generate_session_id()
            
            session_info = SessionInfo(
                session_id=session_id,
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat(),
                status='active',
                person_image_path=person_image_path,
                clothing_image_path=clothing_image_path,
                measurements=measurements or {},
                total_steps=8,
                completed_steps=[],
                current_step=1,
                progress_percent=0.0,
                metadata={} # metadata í•„ë“œ ì´ˆê¸°í™”
            )
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            with self._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO sessions (
                        session_id, created_at, updated_at, status,
                        person_image_path, clothing_image_path, measurements,
                        total_steps, completed_steps, current_step, progress_percent, metadata
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    session_info.session_id,
                    session_info.created_at,
                    session_info.updated_at,
                    session_info.status,
                    session_info.person_image_path,
                    session_info.clothing_image_path,
                    json.dumps(session_info.measurements),
                    session_info.total_steps,
                    json.dumps(session_info.completed_steps),
                    session_info.current_step,
                    session_info.progress_percent,
                    json.dumps(session_info.metadata)
                ))
                conn.commit()
            
            # ìºì‹œì— ì €ì¥
            if self.enable_cache:
                with self.cache_lock:
                    self.session_cache[session_id] = session_info
            
            logger.info(f"âœ… ì„¸ì…˜ ìƒì„± ì™„ë£Œ: {session_id}")
            return session_id
            
        except Exception as e:
            logger.error(f"âŒ ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def get_session_info(self, session_id: str) -> Optional[SessionInfo]:
        """ì„¸ì…˜ ì •ë³´ ì¡°íšŒ - ìµœì‹  ë°ì´í„° ë°˜ì˜ ë° ë½ ë¬¸ì œ í•´ê²°"""
        try:
            # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
            if self.enable_cache and session_id in self.session_cache:
                cached_info = self.session_cache[session_id]
                # ìºì‹œëœ ë°ì´í„°ê°€ ìµœì‹ ì¸ì§€ í™•ì¸
                if await self._is_cache_fresh(session_id, cached_info):
                    self.performance_metrics['cache_hits'] += 1
                    logger.debug(f"âœ… ìºì‹œì—ì„œ ì„¸ì…˜ ì •ë³´ ì¡°íšŒ: {session_id}")
                    return cached_info
                else:
                    # ìºì‹œê°€ ì˜¤ë˜ë¨ - ì œê±°
                    del self.session_cache[session_id]
                    logger.debug(f"âš ï¸ ì˜¤ë˜ëœ ìºì‹œ ì œê±°: {session_id}")
            
            self.performance_metrics['cache_misses'] += 1
            
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìµœì‹  ì •ë³´ ì¡°íšŒ (ë½ ë¬¸ì œ í•´ê²°)
            session_info = await self._get_session_info_with_retry(session_id)
            
            if session_info:
                # ìºì‹œ ì—…ë°ì´íŠ¸
                if self.enable_cache:
                    self.session_cache[session_id] = session_info
                    self._cleanup_cache_if_needed()
                
                logger.info(f"âœ… ì„¸ì…˜ ì •ë³´ ì¡°íšŒ ì™„ë£Œ: {session_id}")
                return session_info
            else:
                logger.warning(f"âš ï¸ ì„¸ì…˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {session_id}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ ì„¸ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    async def _get_session_info_with_retry(self, session_id: str, max_retries: int = 3) -> Optional[SessionInfo]:
        """ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ ì„¸ì…˜ ì •ë³´ ì¡°íšŒ - ë½ ë¬¸ì œ í•´ê²°"""
        for attempt in range(max_retries):
            try:
                with self._get_connection() as conn:
                    cursor = conn.cursor()
                    
                    # ì„¸ì…˜ ê¸°ë³¸ ì •ë³´ ì¡°íšŒ
                    cursor.execute("""
                        SELECT session_id, created_at, updated_at, status, person_image_path, 
                               clothing_image_path, measurements, total_steps, completed_steps, 
                               current_step, progress_percent, metadata
                        FROM sessions 
                        WHERE session_id = ?
                    """, (session_id,))
                    
                    result = cursor.fetchone()
                    if result:
                        # completed_steps íŒŒì‹± (JSON ë¬¸ìì—´ -> ë¦¬ìŠ¤íŠ¸)
                        completed_steps_str = result[8]  # completed_steps ì»¬ëŸ¼
                        try:
                            completed_steps = json.loads(completed_steps_str) if completed_steps_str else []
                        except (json.JSONDecodeError, TypeError):
                            completed_steps = []
                        
                        # measurements íŒŒì‹±
                        measurements_str = result[6]  # measurements ì»¬ëŸ¼
                        try:
                            measurements = json.loads(measurements_str) if measurements_str else {}
                        except (json.JSONDecodeError, TypeError):
                            measurements = {}
                        
                        # metadata íŒŒì‹±
                        metadata_str = result[11]  # metadata ì»¬ëŸ¼
                        try:
                            metadata = json.loads(metadata_str) if metadata_str else {}
                        except (json.JSONDecodeError, TypeError):
                            metadata = {}
                        
                        session_info = SessionInfo(
                            session_id=result[0],
                            created_at=result[1],
                            updated_at=result[2],
                            status=result[3],
                            person_image_path=result[4],
                            clothing_image_path=result[5],
                            measurements=measurements,
                            total_steps=result[7],
                            completed_steps=completed_steps,
                            current_step=result[9],
                            progress_percent=result[10] or 0.0,
                            metadata=metadata
                        )
                        
                        logger.debug(f"âœ… ì„¸ì…˜ ì •ë³´ íŒŒì‹± ì™„ë£Œ: {session_id} (ì‹œë„ {attempt + 1})")
                        return session_info
                    else:
                        logger.warning(f"âš ï¸ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {session_id}")
                        return None
                        
            except sqlite3.OperationalError as e:
                if "database is locked" in str(e):
                    if attempt < max_retries - 1:
                        wait_time = 0.5 * (2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        logger.warning(f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë½ - ì¬ì‹œë„ {attempt + 1}/{max_retries} (ëŒ€ê¸° {wait_time:.1f}ì´ˆ): {e}")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ë½ ìµœì¢… ì‹¤íŒ¨: {e}")
                        raise
                else:
                    logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}")
                    raise
            except Exception as e:
                logger.error(f"âŒ ì„¸ì…˜ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(0.5)
                else:
                    raise
        
        return None

    async def _is_cache_fresh(self, session_id: str, cached_info: SessionInfo) -> bool:
        """ìºì‹œëœ ë°ì´í„°ê°€ ìµœì‹ ì¸ì§€ í™•ì¸"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ updated_at í™•ì¸
            with self._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT updated_at FROM sessions WHERE session_id = ?", (session_id,))
                result = cursor.fetchone()
                
                if result and result[0]:
                    db_updated_at = result[0]
                    cache_updated_at = cached_info.updated_at
                    
                    # ìºì‹œê°€ 5ì´ˆ ì´ë‚´ë©´ ìµœì‹ ìœ¼ë¡œ ê°„ì£¼
                    if db_updated_at == cache_updated_at:
                        return True
                    
                    logger.debug(f"âš ï¸ ìºì‹œ ì˜¤ë˜ë¨ - DB: {db_updated_at}, ìºì‹œ: {cache_updated_at}")
                    return False
                
                return False
                
        except Exception as e:
            logger.debug(f"âš ï¸ ìºì‹œ ì‹ ì„ ë„ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False  # í™•ì¸ ì‹¤íŒ¨ ì‹œ ìºì‹œ ì‚¬ìš© ì•ˆí•¨
    
    # =========================================================================
    # ğŸ”¥ Step ë°ì´í„° ê´€ë¦¬ ë©”ì„œë“œë“¤
    # =========================================================================
    
    async def save_step_result(self, session_id: str, step_id: int, step_name: str, 
                             input_data: Dict[str, Any], output_data: Dict[str, Any], 
                             processing_time: float = 0.0, quality_score: float = 0.0, 
                             status: str = 'completed', error_message: str = None) -> bool:
        """Step ê²°ê³¼ ì €ì¥ ë° ìë™ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        try:
            # Step ê²°ê³¼ ì €ì¥
            success = await self._save_step_result_internal(
                session_id, step_id, step_name, input_data, output_data, 
                processing_time, quality_score, status, error_message
            )
            
            if success and status == 'completed':
                # Step ì™„ë£Œ ì‹œ ìë™ìœ¼ë¡œ ì„¸ì…˜ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                await self._update_session_progress_automatically(session_id, step_id)
                logger.info(f"âœ… Step {step_id} ì™„ë£Œë¡œ ì¸í•œ ì„¸ì…˜ ì§„í–‰ë¥  ìë™ ì—…ë°ì´íŠ¸")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ Step ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def _save_step_result_internal(self, session_id: str, step_id: int, step_name: str,
                             input_data: Dict[str, Any], output_data: Dict[str, Any],
                             processing_time: float = 0.0, quality_score: float = 0.0,
                             status: str = 'completed', error_message: str = None) -> bool:
        """Step ê²°ê³¼ ì €ì¥ ë‚´ë¶€ ë¡œì§ (ì¤‘ë³µ ì œê±°)"""
        try:
            # ë°ì´í„° ì••ì¶•
            compressed_input = self._compress_data(input_data)
            compressed_output = self._compress_data(output_data)
            
            # ë°ì´í„° í•´ì‹œ ìƒì„±
            input_hash = self._generate_data_hash(input_data)
            output_hash = self._generate_data_hash(output_data)
            
            step_data = StepData(
                step_id=step_id,
                step_name=step_name,
                session_id=session_id,
                input_data=input_data,
                output_data=output_data,
                processing_time=processing_time,
                quality_score=quality_score,
                status=status,
                error_message=error_message
            )
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            with self._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT OR REPLACE INTO step_results (
                        session_id, step_id, step_name, input_data, output_data,
                        processing_time, quality_score, status, error_message,
                        created_at, updated_at, data_hash, compressed_size
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    session_id, step_id, step_name,
                    compressed_input, compressed_output,
                    processing_time, quality_score, status, error_message,
                    step_data.created_at.isoformat(),
                    step_data.updated_at.isoformat(),
                    f"{input_hash}_{output_hash}",
                    len(compressed_input) + len(compressed_output)
                ))
                
                # Step ê°„ ë°ì´í„° íë¦„ ì—…ë°ì´íŠ¸
                await self._update_data_flows(session_id, step_id, output_data)
                
                conn.commit()
            
            # ìºì‹œì— ì €ì¥
            if self.enable_cache:
                cache_key = f"{session_id}:{step_id}"
                with self.cache_lock:
                    self.step_cache[cache_key] = step_data
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Step ê²°ê³¼ ì €ì¥ ë‚´ë¶€ ë¡œì§ ì‹¤íŒ¨: {e}")
            return False
    
    async def get_step_result(self, session_id: str, step_id: int) -> Optional[StepData]:
        """Step ê²°ê³¼ ì¡°íšŒ"""
        try:
            cache_key = f"{session_id}:{step_id}"
            
            # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
            if self.enable_cache:
                with self.cache_lock:
                    if cache_key in self.step_cache:
                        self.performance_metrics['cache_hits'] += 1
                        return self.step_cache[cache_key]
            
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
            with self._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT * FROM step_results 
                    WHERE session_id = ? AND step_id = ?
                """, (session_id, step_id))
                
                row = cursor.fetchone()
                if row:
                    # ì••ì¶• í•´ì œ
                    input_data = self._decompress_data(row['input_data'])
                    output_data = self._decompress_data(row['output_data'])
                    
                    step_data = StepData(
                        step_id=row['step_id'],
                        step_name=row['step_name'],
                        session_id=row['session_id'],
                        input_data=input_data,
                        output_data=output_data,
                        processing_time=row['processing_time'],
                        quality_score=row['quality_score'],
                        status=row['status'],
                        error_message=row['error_message'],
                        created_at=datetime.fromisoformat(row['created_at']),
                        updated_at=datetime.fromisoformat(row['updated_at'])
                    )
                    
                    # ìºì‹œì— ì €ì¥
                    if self.enable_cache:
                        with self.cache_lock:
                            self.step_cache[cache_key] = step_data
                    
                    self.performance_metrics['cache_misses'] += 1
                    return step_data
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ Step {step_id} ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    async def get_step_input_data(self, session_id: str, step_id: int) -> Dict[str, Any]:
        """Step ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (ì´ì „ Step ê²°ê³¼ í¬í•¨) - AI ì¶”ë¡ ì— í•„ìš”í•œ ëª¨ë“  ë°ì´í„°"""
        try:
            input_data = {
                'session_id': session_id,
                'step_id': step_id,
                'timestamp': datetime.now().isoformat()
            }
            
            # ì´ì „ Step ê²°ê³¼ì—ì„œ í•„ìš”í•œ ë°ì´í„° ìˆ˜ì§‘
            required_flows = [flow for flow in self.data_flows if flow.target_step == step_id]
            
            for flow in required_flows:
                source_step = flow.source_step
                source_result = await self.get_step_result(session_id, source_step)
                
                if source_result and source_result.status == 'completed':
                    # Stepë³„ íŠ¹ë³„ ì²˜ë¦¬
                    if source_step == 1:  # Human Parsing
                        if flow.data_key == 'segmentation_mask':
                            # segmentation_mask ë˜ëŠ” segmentation_mask_path ì‚¬ìš©
                            if 'segmentation_mask' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['segmentation_mask']
                            elif 'segmentation_mask_path' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['segmentation_mask_path']
                        elif flow.data_key == 'segmentation_mask_path':
                            if 'segmentation_mask_path' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['segmentation_mask_path']
                        elif flow.data_key == 'person_image_path':
                            # person_image_path ì‚¬ìš©
                            if 'person_image_path' in source_result.input_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.input_data['person_image_path']
                        elif flow.data_key == 'human_parsing_result':
                            if 'human_parsing_result' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['human_parsing_result']
                        elif flow.data_key == 'parsing_confidence':
                            if 'confidence' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['confidence']
                    
                    elif source_step == 2:  # Pose Estimation
                        if flow.data_key == 'pose_keypoints':
                            if 'pose_keypoints' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['pose_keypoints']
                        elif flow.data_key == 'pose_skeleton':
                            if 'pose_skeleton' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['pose_skeleton']
                        elif flow.data_key == 'pose_confidence':
                            if 'confidence' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['confidence']
                    
                    elif source_step == 3:  # Cloth Segmentation
                        if flow.data_key == 'cloth_segmentation_mask':
                            if 'cloth_segmentation_mask' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['cloth_segmentation_mask']
                        elif flow.data_key == 'cloth_segmentation_mask_path':
                            if 'cloth_segmentation_mask_path' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['cloth_segmentation_mask_path']
                        elif flow.data_key == 'cloth_confidence':
                            if 'confidence' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['confidence']
                    
                    elif source_step == 4:  # Geometric Matching
                        if flow.data_key == 'transformation_matrix':
                            if 'transformation_matrix' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['transformation_matrix']
                        elif flow.data_key == 'matching_confidence':
                            if 'confidence' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['confidence']
                    
                    elif source_step == 5:  # Cloth Warping
                        if flow.data_key == 'warped_clothing':
                            if 'warped_clothing' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['warped_clothing']
                        elif flow.data_key == 'warped_clothing_path':
                            if 'warped_clothing_path' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['warped_clothing_path']
                    
                    elif source_step == 6:  # Virtual Fitting
                        if flow.data_key == 'fitted_image':
                            if 'fitted_image' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['fitted_image']
                        elif flow.data_key == 'fitted_image_path':
                            if 'fitted_image_path' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['fitted_image_path']
                        elif flow.data_key == 'fitting_confidence':
                            if 'confidence' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['confidence']
                    
                    elif source_step == 7:  # Post Processing
                        if flow.data_key == 'processed_image':
                            if 'processed_image' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['processed_image']
                        elif flow.data_key == 'processed_image_path':
                            if 'processed_image_path' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['processed_image_path']
                        elif flow.data_key == 'processing_metadata':
                            if 'processing_metadata' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['processing_metadata']
                    
                    elif source_step == 8:  # Quality Assessment
                        if flow.data_key == 'final_image':
                            if 'final_image' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['final_image']
                        elif flow.data_key == 'final_image_path':
                            if 'final_image_path' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['final_image_path']
                        elif flow.data_key == 'quality_score':
                            if 'quality_score' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['quality_score']
                        elif flow.data_key == 'assessment_metadata':
                            if 'assessment_metadata' in source_result.output_data:
                                input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data['assessment_metadata']
                    
                    else:
                        # ë‹¤ë¥¸ Stepë“¤ì˜ ê²½ìš° ê¸°ì¡´ ë¡œì§
                        if flow.data_key in source_result.output_data:
                            input_data[f"step_{source_step}_{flow.data_key}"] = source_result.output_data[flow.data_key]
                        elif flow.data_key in source_result.input_data:
                            input_data[f"step_{source_step}_{flow.data_key}"] = source_result.input_data[flow.data_key]
            
            # ì„¸ì…˜ ê¸°ë³¸ ì •ë³´ ì¶”ê°€
            session_info = await self.get_session_info(session_id)
            if session_info:
                input_data['person_image_path'] = session_info.person_image_path
                input_data['clothing_image_path'] = session_info.clothing_image_path
                input_data['measurements'] = session_info.measurements
            
            logger.info(f"âœ… Step {step_id} ì…ë ¥ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(input_data)}ê°œ í•­ëª©")
            logger.info(f"   - ë°ì´í„° í‚¤: {list(input_data.keys())}")
            
            # í•„ìˆ˜ ë°ì´í„° ëˆ„ë½ í™•ì¸
            missing_required = []
            for flow in required_flows:
                if flow.required and f"step_{flow.source_step}_{flow.data_key}" not in input_data:
                    missing_required.append(f"step_{flow.source_step}_{flow.data_key}")
            
            if missing_required:
                logger.warning(f"âš ï¸ Step {step_id}ì— í•„ìˆ˜ ë°ì´í„° ëˆ„ë½: {missing_required}")
            
            return input_data
            
        except Exception as e:
            logger.error(f"âŒ Step {step_id} ì…ë ¥ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return {'session_id': session_id, 'step_id': step_id, 'error': str(e)}
    
    # =========================================================================
    # ğŸ”¥ ë°ì´í„° íë¦„ ê´€ë¦¬ ë©”ì„œë“œë“¤
    # =========================================================================
    
    async def _update_data_flows(self, session_id: str, step_id: int, output_data: Dict[str, Any]):
        """Step ê°„ ë°ì´í„° íë¦„ ì—…ë°ì´íŠ¸ (ë½ ë¬¸ì œ í•´ê²°)"""
        try:
            # í˜„ì¬ Stepì—ì„œ ë‹¤ìŒ Stepìœ¼ë¡œ ì „ë‹¬í•  ë°ì´í„° ì‹ë³„
            outgoing_flows = [flow for flow in self.data_flows if flow.source_step == step_id]
            
            if not outgoing_flows:
                logger.debug(f"âš ï¸ Step {step_id}ì—ì„œ ë‹¤ìŒ Stepìœ¼ë¡œ ì „ë‹¬í•  ë°ì´í„° íë¦„ì´ ì—†ìŒ")
                return
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
            batch_data = []
            current_time = datetime.now().isoformat()
            
            for flow in outgoing_flows:
                if flow.data_key in output_data:
                    data_value = output_data[flow.data_key]
                    data_hash = self._generate_data_hash(data_value)
                    
                    batch_data.append((
                        session_id, flow.source_step, flow.target_step,
                        flow.data_type, flow.data_key, self._serialize_data_for_db(data_value),
                        data_hash, flow.required, 'valid', current_time
                    ))
            
            if not batch_data:
                logger.debug(f"âš ï¸ Step {step_id}ì—ì„œ ì „ë‹¬í•  ë°ì´í„°ê°€ ì—†ìŒ")
                return
            
            # ë‹¨ìˆœí•œ ì—°ê²°ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ (ë½ ë¬¸ì œ ë°©ì§€)
            try:
                conn = sqlite3.connect(self.db_path, check_same_thread=False, timeout=30.0)
                cursor = conn.cursor()
                
                # ë°°ì¹˜ ì‚½ì…/ì—…ë°ì´íŠ¸
                cursor.executemany("""
                    INSERT OR REPLACE INTO step_data_flow (
                        session_id, source_step, target_step, data_type,
                        data_key, data_value, data_hash, required,
                        validation_status, created_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, batch_data)
                
                # ì»¤ë°‹
                conn.commit()
                conn.close()
                
                logger.debug(f"âœ… Step {step_id} ë°ì´í„° íë¦„ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(batch_data)}ê°œ í•­ëª©")
                
            except Exception as e:
                logger.warning(f"âš ï¸ ë°ì´í„° íë¦„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ë½ ë¬¸ì œ): {e}")
                # ë½ ë¬¸ì œë¡œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                return
                
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° íë¦„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            # ìƒì„¸í•œ ì—ëŸ¬ ì •ë³´ ë¡œê¹…
            logger.error(f"   - Session ID: {session_id}")
            logger.error(f"   - Step ID: {step_id}")
            logger.error(f"   - Output Data Keys: {list(output_data.keys()) if output_data else 'None'}")
            logger.error(f"   - ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    async def validate_step_dependencies(self, session_id: str, step_id: int) -> Dict[str, Any]:
        """Step ì˜ì¡´ì„± ê²€ì¦"""
        try:
            required_flows = [flow for flow in self.data_flows if flow.target_step == step_id and flow.required]
            
            validation_result = {
                'valid': True,
                'missing_dependencies': [],
                'available_data': [],
                'validation_details': {}
            }
            
            for flow in required_flows:
                source_step = flow.source_step
                source_result = await self.get_step_result(session_id, source_step)
                
                if not source_result or source_result.status != 'completed':
                    validation_result['valid'] = False
                    validation_result['missing_dependencies'].append(f"Step {source_step} ë¯¸ì™„ë£Œ")
                    validation_result['validation_details'][f"step_{source_step}"] = {
                        'status': 'missing',
                        'required_data': flow.data_key
                    }
                else:
                    if flow.data_key in source_result.output_data:
                        validation_result['available_data'].append(f"Step {source_step} -> {flow.data_key}")
                        validation_result['validation_details'][f"step_{source_step}"] = {
                            'status': 'available',
                            'data_key': flow.data_key,
                            'data_type': type(source_result.output_data[flow.data_key]).__name__
                        }
                    else:
                        validation_result['valid'] = False
                        validation_result['missing_dependencies'].append(f"Step {source_step} -> {flow.data_key} ë°ì´í„° ì—†ìŒ")
                        validation_result['validation_details'][f"step_{source_step}"] = {
                            'status': 'incomplete',
                            'missing_data': flow.data_key
                        }
            
            return validation_result
            
        except Exception as e:
            logger.error(f"âŒ Step ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'valid': False, 'error': str(e)}
    
    # =========================================================================
    # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # =========================================================================
    
    def _generate_session_id(self) -> str:
        """ê³ ìœ í•œ ì„¸ì…˜ ID ìƒì„± (ì¤‘ë³µ ë°©ì§€)"""
        import uuid
        timestamp = int(time.time())
        random_part = hashlib.md5(f"{timestamp}_{uuid.uuid4()}".encode()).hexdigest()[:12]
        return f"session_{timestamp}_{random_part}"
    
    def _compress_data(self, data: Dict[str, Any]) -> bytes:
        """ë°ì´í„° ì••ì¶•"""
        try:
            # NumPy ë°°ì—´ ë“±ì„ ì•ˆì „í•˜ê²Œ ì§ë ¬í™”
            json_str = self._serialize_data_for_db(data)
            compressed = zlib.compress(json_str.encode())
            
            # ì••ì¶•ë¥  ê³„ì‚°
            original_size = len(json_str.encode())
            compressed_size = len(compressed)
            if original_size > 0:
                self.performance_metrics['compression_ratio'] = compressed_size / original_size
            
            return compressed
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì••ì¶• ì‹¤íŒ¨: {e}")
            return self._serialize_data_for_db(data).encode()
    
    def _decompress_data(self, compressed_data: bytes) -> Dict[str, Any]:
        """ë°ì´í„° ì••ì¶• í•´ì œ"""
        try:
            if compressed_data.startswith(b'{'):  # ì´ë¯¸ JSONì¸ ê²½ìš°
                return json.loads(compressed_data.decode())
            
            decompressed = zlib.decompress(compressed_data)
            return json.loads(decompressed.decode())
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _serialize_data_for_db(self, data: Any) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì„ ìœ„í•œ ë°ì´í„° ì§ë ¬í™” (NumPy ë°°ì—´ ë“± ì²˜ë¦¬)"""
        try:
            if data is None:
                return json.dumps(None)
            
            # NumPy ë°°ì—´ ì²˜ë¦¬
            if NUMPY_AVAILABLE and hasattr(data, 'tolist') and hasattr(data, 'shape'):
                # NumPy ë°°ì—´ë¡œ íŒë‹¨
                serialized = data.tolist()
            elif TORCH_AVAILABLE and hasattr(data, 'numpy') and hasattr(data, 'detach'):
                # PyTorch í…ì„œë¡œ íŒë‹¨
                serialized = data.detach().cpu().numpy().tolist()
            elif NUMPY_AVAILABLE and np and isinstance(data, np.ndarray):
                # NumPy ë°°ì—´ íƒ€ì… ì²´í¬
                serialized = data.tolist()
            elif TORCH_AVAILABLE and torch and isinstance(data, torch.Tensor):
                # PyTorch í…ì„œ íƒ€ì… ì²´í¬
                serialized = data.detach().cpu().numpy().tolist()
            else:
                serialized = data
            
            return json.dumps(serialized, default=str)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë°ì´í„° ì§ë ¬í™” ì‹¤íŒ¨: {e}")
            return json.dumps(str(data))
    
    def _generate_data_hash(self, data: Any) -> str:
        """ë°ì´í„° í•´ì‹œ ìƒì„±"""
        try:
            if isinstance(data, dict):
                data_str = json.dumps(data, sort_keys=True, default=str)
            else:
                data_str = str(data)
            return hashlib.md5(data_str.encode()).hexdigest()
        except Exception:
            return "unknown"
    
    async def _update_session_progress(self, session_id: str):
        """ì„¸ì…˜ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        try:
            # ì™„ë£Œëœ Step ìˆ˜ ê³„ì‚°
            with self._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT COUNT(*) FROM step_results 
                    WHERE session_id = ? AND status = 'completed'
                """, (session_id,))
                completed_count = cursor.fetchone()[0]
                
                # ì§„í–‰ë¥  ê³„ì‚°
                progress_percent = (completed_count / 8) * 100
                
                # ì„¸ì…˜ ì •ë³´ ì—…ë°ì´íŠ¸
                cursor.execute("""
                    UPDATE sessions 
                    SET completed_steps = ?, progress_percent = ?, updated_at = ?
                    WHERE session_id = ?
                """, (
                    json.dumps(list(range(1, completed_count + 1))),
                    progress_percent,
                    datetime.now().isoformat(),
                    session_id
                ))
                conn.commit()
                
        except Exception as e:
            logger.error(f"âŒ ì„¸ì…˜ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    async def _update_session_progress_automatically(self, session_id: str, completed_step_id: int):
        """Step ì™„ë£Œ ì‹œ ì„¸ì…˜ ì§„í–‰ë¥  ìë™ ì—…ë°ì´íŠ¸"""
        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                # í˜„ì¬ ì„¸ì…˜ ì •ë³´ ì¡°íšŒ
                cursor.execute("""
                    SELECT completed_steps, progress_percent, total_steps 
                    FROM sessions 
                    WHERE session_id = ?
                """, (session_id,))
                
                result = cursor.fetchone()
                if result:
                    current_completed = json.loads(result[0]) if result[0] else []
                    current_progress = result[1] or 0.0
                    total_steps = result[2] or 8
                    
                    # ì™„ë£Œëœ Stepì— ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
                    if completed_step_id not in current_completed:
                        current_completed.append(completed_step_id)
                        current_completed.sort()  # ì •ë ¬
                        
                        # ì§„í–‰ë¥  ê³„ì‚°
                        new_progress = (len(current_completed) / total_steps) * 100
                        
                        # ì„¸ì…˜ ì •ë³´ ì—…ë°ì´íŠ¸
                        cursor.execute("""
                            UPDATE sessions 
                            SET completed_steps = ?, progress_percent = ?, updated_at = ?
                            WHERE session_id = ?
                        """, (
                            json.dumps(current_completed),
                            new_progress,
                            datetime.now().isoformat(),
                            session_id
                        ))
                        
                        conn.commit()
                        
                        logger.info(f"âœ… ì„¸ì…˜ ì§„í–‰ë¥  ìë™ ì—…ë°ì´íŠ¸: {current_progress:.1f}% â†’ {new_progress:.1f}%")
                        logger.info(f"   - ì™„ë£Œëœ Step: {current_completed}")
                        
                        # ìºì‹œ ë¬´íš¨í™”
                        if session_id in self.session_cache:
                            del self.session_cache[session_id]
                            logger.debug(f"âœ… ì„¸ì…˜ ìºì‹œ ë¬´íš¨í™”: {session_id}")
                    else:
                        logger.debug(f"âš ï¸ Step {completed_step_id}ì´ ì´ë¯¸ ì™„ë£Œëœ Stepì— í¬í•¨ë¨")
                        
        except Exception as e:
            logger.error(f"âŒ ì„¸ì…˜ ì§„í–‰ë¥  ìë™ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _cleanup_cache_if_needed(self):
        """ìºì‹œ í¬ê¸° ì œí•œ ë° ì •ë¦¬ (ì„±ëŠ¥ ìµœì í™”)"""
        try:
            total_cache_size = len(self.session_cache) + len(self.step_cache)
            
            if total_cache_size > self.max_cache_size:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©ë“¤ ì œê±° (ì„¸ì…˜ ìºì‹œì™€ ìŠ¤í… ìºì‹œ ëª¨ë‘ ê³ ë ¤)
                items_to_remove = total_cache_size - self.max_cache_size
                
                # ì„¸ì…˜ ìºì‹œ ì •ë¦¬
                if len(self.session_cache) > self.max_cache_size // 2:
                    session_items_to_remove = len(self.session_cache) - (self.max_cache_size // 2)
                    oldest_session_keys = sorted(self.session_cache.keys(), 
                                               key=lambda k: self.session_cache[k].updated_at)[:session_items_to_remove]
                    
                    for key in oldest_session_keys:
                        del self.session_cache[key]
                    
                    logger.debug(f"âœ… ì„¸ì…˜ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {session_items_to_remove}ê°œ í•­ëª© ì œê±°")
                
                # ìŠ¤í… ìºì‹œ ì •ë¦¬
                if len(self.step_cache) > self.max_cache_size // 2:
                    step_items_to_remove = len(self.step_cache) - (self.max_cache_size // 2)
                    oldest_step_keys = sorted(self.step_cache.keys(), 
                                            key=lambda k: self.step_cache[k].updated_at if hasattr(self.step_cache[k], 'updated_at') else 0)[:step_items_to_remove]
                    
                    for key in oldest_step_keys:
                        del self.step_cache[key]
                    
                    logger.debug(f"âœ… ìŠ¤í… ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {step_items_to_remove}ê°œ í•­ëª© ì œê±°")
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
                logger.debug(f"ğŸ“Š ìºì‹œ ìƒíƒœ: ì„¸ì…˜ {len(self.session_cache)}ê°œ, ìŠ¤í… {len(self.step_cache)}ê°œ")
                
        except Exception as e:
            logger.debug(f"âš ï¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            logger.debug(f"   - ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    # =========================================================================
    # ğŸ”¥ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
    # =========================================================================
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜ (ìƒì„¸ ë¶„ì„ í¬í•¨)"""
        try:
            # ìºì‹œ íš¨ìœ¨ì„± ê³„ì‚°
            total_requests = self.performance_metrics['cache_hits'] + self.performance_metrics['cache_misses']
            cache_hit_ratio = 0.0
            if total_requests > 0:
                cache_hit_ratio = self.performance_metrics['cache_hits'] / total_requests
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
            session_cache_memory = sum(
                len(str(v)) for v in self.session_cache.values()
            ) if self.session_cache else 0
            
            step_cache_memory = sum(
                len(str(v)) for v in self.step_cache.values()
            ) if self.step_cache else 0
            
            total_cache_memory = session_cache_memory + step_cache_memory
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„
            db_efficiency = 0.0
            if self.performance_metrics['db_queries'] > 0:
                db_efficiency = (total_requests - self.performance_metrics['db_queries']) / total_requests
            
            return {
                **self.performance_metrics,
                'cache_hit_ratio': cache_hit_ratio,
                'total_requests': total_requests,
                'compression_ratio': self.performance_metrics['compression_ratio'],
                'cache_size': len(self.session_cache) + len(self.step_cache),
                'cache_memory_usage_bytes': total_cache_memory,
                'session_cache_size': len(self.session_cache),
                'step_cache_size': len(self.step_cache),
                'database_efficiency': db_efficiency,
                'memory_efficiency': total_cache_memory / (1024 * 1024) if total_cache_memory > 0 else 0,  # MB
                'performance_score': min(100, (cache_hit_ratio * 50) + (db_efficiency * 30) + (1 - self.performance_metrics['compression_ratio']) * 20)
            }
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            logger.error(f"   - ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {}
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        try:
            with self.cache_lock:
                self.session_cache.clear()
                self.step_cache.clear()
            logger.info("âœ… ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def optimize_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” (ë½ ë¬¸ì œ ë°©ì§€)"""
        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                try:
                    # ì•ˆì „í•œ ìµœì í™”ë§Œ ìˆ˜í–‰ (ë½ ë¬¸ì œ ë°©ì§€)
                    cursor.execute("ANALYZE")
                    
                    # í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸
                    cursor.execute("ANALYZE sqlite_master")
                    
                    # ë©”ëª¨ë¦¬ ìµœì í™” (ì•ˆì „í•œ ê²ƒë§Œ)
                    cursor.execute("PRAGMA optimize")
                    
                    # ì»¤ë°‹
                    conn.commit()
                    
                    logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì•ˆì „ ìµœì í™” ì™„ë£Œ")
                    
                    # ìµœì í™” í›„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ í™•ì¸
                    self._log_optimization_results()
                    
                except Exception as e:
                    # ë¡¤ë°±
                    conn.rollback()
                    logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì¤‘ ì˜¤ë¥˜: {e}")
                    raise
                    
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹¤íŒ¨: {e}")
            logger.error(f"   - ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    def _log_optimization_results(self):
        """ìµœì í™” ê²°ê³¼ ë¡œê¹…"""
        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                # ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° í™•ì¸
                cursor.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
                db_size = cursor.fetchone()[0]
                
                # í…Œì´ë¸”ë³„ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = cursor.fetchall()
                
                table_stats = {}
                for table in tables:
                    table_name = table[0]
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    count = cursor.fetchone()[0]
                    table_stats[table_name] = count
                
                logger.info(f"ğŸ“Š ìµœì í™” í›„ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ:")
                logger.info(f"   - ì „ì²´ í¬ê¸°: {db_size / (1024*1024):.2f} MB")
                logger.info(f"   - í…Œì´ë¸”ë³„ ë ˆì½”ë“œ ìˆ˜: {table_stats}")
                
        except Exception as e:
            logger.debug(f"âš ï¸ ìµœì í™” ê²°ê³¼ ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    def get_database_stats(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                stats = {}
                
                # í…Œì´ë¸”ë³„ ë ˆì½”ë“œ ìˆ˜
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = cursor.fetchall()
                
                for table in tables:
                    table_name = table[0]
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    count = cursor.fetchone()[0]
                    stats[f"{table_name}_count"] = count
                
                # ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°
                cursor.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
                db_size = cursor.fetchone()[0]
                stats['database_size_bytes'] = db_size
                stats['database_size_mb'] = db_size / (1024*1024)
                
                # ì¸ë±ìŠ¤ ì •ë³´
                cursor.execute("SELECT name FROM sqlite_master WHERE type='index'")
                indexes = cursor.fetchall()
                stats['index_count'] = len(indexes)
                
                return stats
                
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def cleanup_old_sessions(self, days_old: int = 30):
        """ì˜¤ë˜ëœ ì„¸ì…˜ ì •ë¦¬ (ë©”ëª¨ë¦¬ ë° ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½)"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days_old)
            cutoff_str = cutoff_date.isoformat()
            
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                # íŠ¸ëœì­ì…˜ ì‹œì‘
                cursor.execute("BEGIN TRANSACTION")
                
                try:
                    # ì˜¤ë˜ëœ ì„¸ì…˜ ID ì¡°íšŒ
                    cursor.execute("""
                        SELECT session_id FROM sessions 
                        WHERE created_at < ? AND status != 'active'
                    """, (cutoff_str,))
                    
                    old_sessions = cursor.fetchall()
                    old_session_ids = [row[0] for row in old_sessions]
                    
                    if old_session_ids:
                        # ê´€ë ¨ ë°ì´í„° ì‚­ì œ (CASCADEë¡œ ìë™ ì‚­ì œë¨)
                        placeholders = ','.join(['?' for _ in old_session_ids])
                        cursor.execute(f"DELETE FROM sessions WHERE session_id IN ({placeholders})", old_session_ids)
                        
                        deleted_count = len(old_session_ids)
                        conn.commit()
                        
                        logger.info(f"âœ… ì˜¤ë˜ëœ ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ ì„¸ì…˜ ì‚­ì œ")
                        
                        # ìºì‹œì—ì„œë„ ì œê±°
                        for session_id in old_session_ids:
                            if session_id in self.session_cache:
                                del self.session_cache[session_id]
                        
                        return deleted_count
                    else:
                        logger.info("âœ… ì •ë¦¬í•  ì˜¤ë˜ëœ ì„¸ì…˜ì´ ì—†ìŒ")
                        return 0
                        
                except Exception as e:
                    # íŠ¸ëœì­ì…˜ ë¡¤ë°±
                    conn.rollback()
                    logger.error(f"âŒ ì˜¤ë˜ëœ ì„¸ì…˜ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    raise
                    
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë˜ëœ ì„¸ì…˜ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0

# =============================================================================
# ğŸŒ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
# =============================================================================

_unified_session_db = None
_db_lock = threading.RLock()

def get_unified_session_database() -> UnifiedSessionDatabase:
    """ì „ì—­ í†µí•© ì„¸ì…˜ ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _unified_session_db
    
    if _unified_session_db is None:
        with _db_lock:
            if _unified_session_db is None:
                _unified_session_db = UnifiedSessionDatabase()
                logger.info("âœ… ì „ì—­ UnifiedSessionDatabase ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
    
    return _unified_session_db

def reset_unified_session_database():
    """ì „ì—­ í†µí•© ì„¸ì…˜ ë°ì´í„°ë² ì´ìŠ¤ ì¬ì„¤ì •"""
    global _unified_session_db
    with _db_lock:
        if _unified_session_db:
            _unified_session_db.clear_cache()
            _unified_session_db = None
        logger.info("âœ… ì „ì—­ UnifiedSessionDatabase ì¬ì„¤ì • ì™„ë£Œ")
