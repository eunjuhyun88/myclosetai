{
  "analyzed_models": {
    "ootdiffusion": {
      "name": "OOTDiffusion",
      "type": "diffusion",
      "step": "step_06_virtual_fitting",
      "ready": true,
      "total_size_mb": 15129.3,
      "priority": 1,
      "checkpoints": [
        {
          "name": "pytorch_model.bin",
          "path": "pytorch_model.bin",
          "size_mb": 469.5
        },
        {
          "name": "diffusion_pytorch_model.bin",
          "path": "diffusion_pytorch_model.bin",
          "size_mb": 319.2
        },
        {
          "name": "body_pose_model.pth",
          "path": "body_pose_model.pth",
          "size_mb": 199.6
        }
      ],
      "total_checkpoints": 5,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/ootdiffusion"
    },
    "ootdiffusion_hf": {
      "name": "OOTDiffusion HF",
      "type": "diffusion",
      "step": "step_06_virtual_fitting",
      "ready": true,
      "total_size_mb": 15129.3,
      "priority": 1,
      "checkpoints": [
        {
          "name": "pytorch_model.bin",
          "path": "pytorch_model.bin",
          "size_mb": 469.5
        },
        {
          "name": "diffusion_pytorch_model.bin",
          "path": "diffusion_pytorch_model.bin",
          "size_mb": 319.2
        },
        {
          "name": "body_pose_model.pth",
          "path": "body_pose_model.pth",
          "size_mb": 199.6
        }
      ],
      "total_checkpoints": 5,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/ootdiffusion_hf"
    },
    "stable-diffusion-v1-5": {
      "name": "Stable Diffusion v1.5",
      "type": "diffusion",
      "step": "step_06_virtual_fitting",
      "ready": true,
      "total_size_mb": 45070.6,
      "priority": 2,
      "checkpoints": [
        {
          "name": "v1-5-pruned.ckpt",
          "path": "v1-5-pruned.ckpt",
          "size_mb": 7346.9
        },
        {
          "name": "v1-5-pruned-emaonly.ckpt",
          "path": "v1-5-pruned-emaonly.ckpt",
          "size_mb": 4067.8
        },
        {
          "name": "pytorch_model.fp16.bin",
          "path": "pytorch_model.fp16.bin",
          "size_mb": 234.8
        }
      ],
      "total_checkpoints": 11,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/stable-diffusion-v1-5"
    },
    "human_parsing": {
      "name": "Human Parsing",
      "type": "human_parsing",
      "step": "step_01_human_parsing",
      "ready": true,
      "total_size_mb": 1288.3,
      "priority": 3,
      "checkpoints": [
        {
          "name": "schp_atr.pth",
          "path": "schp_atr.pth",
          "size_mb": 255.1
        },
        {
          "name": "optimizer.pt",
          "path": "optimizer.pt",
          "size_mb": 209.0
        },
        {
          "name": "rng_state.pth",
          "path": "rng_state.pth",
          "size_mb": 0.0
        }
      ],
      "total_checkpoints": 8,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/human_parsing"
    },
    "step_01_human_parsing": {
      "name": "Step 01 Human Parsing",
      "type": "human_parsing",
      "step": "step_01_human_parsing",
      "ready": true,
      "total_size_mb": 1787.7,
      "priority": 3,
      "checkpoints": [
        {
          "name": "densepose_rcnn_R_50_FPN_s1x.pkl",
          "path": "densepose_rcnn_R_50_FPN_s1x.pkl",
          "size_mb": 243.9
        },
        {
          "name": "graphonomy_lip.pth",
          "path": "graphonomy_lip.pth",
          "size_mb": 255.1
        },
        {
          "name": "lightweight_parsing.pth",
          "path": "lightweight_parsing.pth",
          "size_mb": 0.5
        }
      ],
      "total_checkpoints": 11,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_01_human_parsing"
    },
    "pose_estimation": {
      "name": "Pose Estimation",
      "type": "pose_estimation",
      "step": "step_02_pose_estimation",
      "ready": true,
      "total_size_mb": 10095.6,
      "priority": 4,
      "checkpoints": [
        {
          "name": "sk_model.pth",
          "path": "sk_model.pth",
          "size_mb": 16.4
        },
        {
          "name": "upernet_global_small.pth",
          "path": "upernet_global_small.pth",
          "size_mb": 196.8
        },
        {
          "name": "latest_net_G.pth",
          "path": "latest_net_G.pth",
          "size_mb": 303.5
        }
      ],
      "total_checkpoints": 23,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/pose_estimation"
    },
    "step_02_pose_estimation": {
      "name": "Step 02 Pose Estimation",
      "type": "pose_estimation",
      "step": "step_02_pose_estimation",
      "ready": true,
      "total_size_mb": 273.6,
      "priority": 4,
      "checkpoints": [
        {
          "name": "openpose.pth",
          "path": "openpose.pth",
          "size_mb": 199.6
        },
        {
          "name": "yolov8n-pose.pt",
          "path": "yolov8n-pose.pt",
          "size_mb": 6.5
        }
      ],
      "total_checkpoints": 2,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_02_pose_estimation"
    },
    "openpose": {
      "name": "OpenPose",
      "type": "pose_estimation",
      "step": "step_02_pose_estimation",
      "ready": true,
      "total_size_mb": 539.7,
      "priority": 4,
      "checkpoints": [
        {
          "name": "body_pose_model.pth",
          "path": "body_pose_model.pth",
          "size_mb": 199.6
        },
        {
          "name": "hand_pose_model.pth",
          "path": "hand_pose_model.pth",
          "size_mb": 140.5
        }
      ],
      "total_checkpoints": 3,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/openpose"
    },
    "cloth_segmentation": {
      "name": "Cloth Segmentation",
      "type": "cloth_segmentation",
      "step": "step_03_cloth_segmentation",
      "ready": true,
      "total_size_mb": 803.2,
      "priority": 5,
      "checkpoints": [
        {
          "name": "model.pth",
          "path": "model.pth",
          "size_mb": 168.5
        },
        {
          "name": "pytorch_model.bin",
          "path": "pytorch_model.bin",
          "size_mb": 168.4
        }
      ],
      "total_checkpoints": 2,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/cloth_segmentation"
    },
    "step_03_cloth_segmentation": {
      "name": "Step 03 Cloth Segmentation",
      "type": "cloth_segmentation",
      "step": "step_03_cloth_segmentation",
      "ready": true,
      "total_size_mb": 206.7,
      "priority": 5,
      "checkpoints": [
        {
          "name": "mobile_sam.pt",
          "path": "mobile_sam.pt",
          "size_mb": 38.8
        }
      ],
      "total_checkpoints": 1,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_03_cloth_segmentation"
    },
    "step_04_geometric_matching": {
      "name": "Step 04 Geometric Matching",
      "type": "geometric_matching",
      "step": "step_04_geometric_matching",
      "ready": true,
      "total_size_mb": 33.2,
      "priority": 6,
      "checkpoints": [
        {
          "name": "gmm_final.pth",
          "path": "gmm_final.pth",
          "size_mb": 4.1
        },
        {
          "name": "lightweight_gmm.pth",
          "path": "lightweight_gmm.pth",
          "size_mb": 4.1
        },
        {
          "name": "tps_network.pth",
          "path": "tps_network.pth",
          "size_mb": 2.1
        }
      ],
      "total_checkpoints": 4,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_04_geometric_matching"
    },
    "step_05_cloth_warping": {
      "name": "Step 05 Cloth Warping",
      "type": "cloth_warping",
      "step": "step_05_cloth_warping",
      "ready": true,
      "total_size_mb": 3279.2,
      "priority": 7,
      "checkpoints": [
        {
          "name": "tom_final.pth",
          "path": "tom_final.pth",
          "size_mb": 3279.1
        },
        {
          "name": "lightweight_warping.pth",
          "path": "lightweight_warping.pth",
          "size_mb": 0.1
        }
      ],
      "total_checkpoints": 2,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_05_cloth_warping"
    },
    "step_06_virtual_fitting": {
      "name": "Step 06 Virtual Fitting",
      "type": "virtual_tryon",
      "step": "step_06_virtual_fitting",
      "ready": true,
      "total_size_mb": 20854.2,
      "priority": 1,
      "checkpoints": [
        {
          "name": "hrviton_final.pth",
          "path": "hrviton_final.pth",
          "size_mb": 2445.7
        },
        {
          "name": "diffusion_pytorch_model.bin",
          "path": "diffusion_pytorch_model.bin",
          "size_mb": 3279.1
        },
        {
          "name": "pytorch_model.bin",
          "path": "pytorch_model.bin",
          "size_mb": 469.5
        }
      ],
      "total_checkpoints": 7,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_06_virtual_fitting"
    },
    "step_07_post_processing": {
      "name": "Step 07 Post Processing",
      "type": "auxiliary",
      "step": "step_07_post_processing",
      "ready": true,
      "total_size_mb": 63.9,
      "priority": 8,
      "checkpoints": [
        {
          "name": "RealESRGAN_x4plus.pth",
          "path": "RealESRGAN_x4plus.pth",
          "size_mb": 63.9
        }
      ],
      "total_checkpoints": 1,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_07_post_processing"
    },
    "sam": {
      "name": "SAM (Segment Anything Model)",
      "type": "auxiliary",
      "step": "auxiliary",
      "ready": true,
      "total_size_mb": 2445.7,
      "priority": 8,
      "checkpoints": [
        {
          "name": "sam_vit_h_4b8939.pth",
          "path": "sam_vit_h_4b8939.pth",
          "size_mb": 2445.7
        }
      ],
      "total_checkpoints": 1,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/sam"
    },
    "clip-vit-base-patch32": {
      "name": "CLIP ViT Base",
      "type": "text_image",
      "step": "auxiliary",
      "ready": true,
      "total_size_mb": 580.7,
      "priority": 9,
      "checkpoints": [
        {
          "name": "pytorch_model.bin",
          "path": "pytorch_model.bin",
          "size_mb": 577.2
        }
      ],
      "total_checkpoints": 1,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/clip-vit-base-patch32"
    },
    "grounding_dino": {
      "name": "Grounding DINO",
      "type": "auxiliary",
      "step": "auxiliary",
      "ready": true,
      "total_size_mb": 1318.2,
      "priority": 9,
      "checkpoints": [
        {
          "name": "pytorch_model.bin",
          "path": "pytorch_model.bin",
          "size_mb": 659.9
        }
      ],
      "total_checkpoints": 1,
      "path": "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/grounding_dino"
    }
  },
  "step_optimal_models": {
    "step_01_human_parsing": "step_01_human_parsing",
    "step_02_pose_estimation": "step_02_pose_estimation",
    "step_03_cloth_segmentation": "step_03_cloth_segmentation",
    "step_04_geometric_matching": "step_04_geometric_matching",
    "step_05_cloth_warping": "step_05_cloth_warping",
    "step_06_virtual_fitting": "step_06_virtual_fitting",
    "step_07_post_processing": "step_07_post_processing",
    "auxiliary": "sam"
  },
  "analysis_stats": {
    "total_models": 17,
    "ready_models": 17,
    "total_size_gb": 116.11240234374998,
    "largest_model": "stable-diffusion-v1-5"
  }
}