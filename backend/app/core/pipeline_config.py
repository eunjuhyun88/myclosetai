# app/core/pipeline_config.py
"""
MyCloset AI - íŒŒì´í”„ë¼ì¸ ì„¤ì • í´ë˜ìŠ¤
âœ… M3 Max 128GB ìµœì í™”
âœ… 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì„¤ì •
âœ… ë™ì  ë””ë°”ì´ìŠ¤ ê°ì§€
âœ… í’ˆì§ˆ/ì„±ëŠ¥ ë ˆë²¨ ì„¤ì •
âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”

íŒŒì¼ ìœ„ì¹˜: backend/app/core/pipeline_config.py
"""

import os
import sys
import logging
import platform
import psutil
from typing import Dict, Any, Optional, Union, List, Tuple
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì—´ê±°í˜• ì •ì˜ (Enum Classes)
# ==============================================

class DeviceType(Enum):
    """ì§€ì›ë˜ëŠ” ë””ë°”ì´ìŠ¤ íƒ€ì…"""
    AUTO = "auto"           # ìë™ ê°ì§€
    CPU = "cpu"            # CPUë§Œ ì‚¬ìš©
    CUDA = "cuda"          # NVIDIA GPU
    MPS = "mps"            # Apple Silicon (M1/M2/M3)
    OPENCL = "opencl"      # OpenCL ì§€ì› GPU

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨ (ì„±ëŠ¥ vs í’ˆì§ˆ íŠ¸ë ˆì´ë“œì˜¤í”„)"""
    FAST = "fast"          # ë¹ ë¥¸ ì²˜ë¦¬ (ë‚®ì€ í’ˆì§ˆ)
    BALANCED = "balanced"   # ê· í˜•ì¡íŒ í’ˆì§ˆ/ì„±ëŠ¥
    HIGH = "high"          # ë†’ì€ í’ˆì§ˆ (ëŠë¦° ì²˜ë¦¬)
    ULTRA = "ultra"        # ìµœê³  í’ˆì§ˆ (ë§¤ìš° ëŠë¦¼)
    MAXIMUM = "maximum"    # ìµœëŒ€ í’ˆì§ˆ (M3 Max ì „ìš©)

class PipelineMode(Enum):
    """íŒŒì´í”„ë¼ì¸ ë™ì‘ ëª¨ë“œ"""
    DEVELOPMENT = "development"  # ê°œë°œ ëª¨ë“œ (ë””ë²„ê¹…)
    PRODUCTION = "production"    # í”„ë¡œë•ì…˜ ëª¨ë“œ (ìµœì í™”)
    TESTING = "testing"         # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ê²€ì¦)
    SIMULATION = "simulation"   # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ (ë”ë¯¸)
    HYBRID = "hybrid"          # í˜¼í•© ëª¨ë“œ

class ProcessingStrategy(Enum):
    """ì²˜ë¦¬ ì „ëµ"""
    SEQUENTIAL = "sequential"   # ìˆœì°¨ ì²˜ë¦¬
    PARALLEL = "parallel"      # ë³‘ë ¬ ì²˜ë¦¬
    PIPELINE = "pipeline"      # íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬
    BATCH = "batch"           # ë°°ì¹˜ ì²˜ë¦¬

class MemoryStrategy(Enum):
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ ì „ëµ"""
    CONSERVATIVE = "conservative"  # ë³´ìˆ˜ì  (ë©”ëª¨ë¦¬ ì ˆì•½)
    BALANCED = "balanced"         # ê· í˜•ì¡íŒ
    AGGRESSIVE = "aggressive"     # ê³µê²©ì  (ì„±ëŠ¥ ìš°ì„ )
    MAXIMUM = "maximum"          # ìµœëŒ€ í™œìš© (M3 Max)

# ==============================================
# ğŸ ì‹œìŠ¤í…œ ì •ë³´ í´ë˜ìŠ¤
# ==============================================

@dataclass
class SystemInfo:
    """ì‹œìŠ¤í…œ í•˜ë“œì›¨ì–´ ì •ë³´"""
    platform: str = field(default_factory=lambda: platform.system())
    architecture: str = field(default_factory=lambda: platform.machine())
    cpu_cores: int = field(default_factory=lambda: psutil.cpu_count())
    cpu_name: str = ""
    memory_gb: float = field(default_factory=lambda: psutil.virtual_memory().total / (1024**3))
    available_memory_gb: float = field(default_factory=lambda: psutil.virtual_memory().available / (1024**3))
    is_m3_max: bool = False
    is_apple_silicon: bool = False
    gpu_available: bool = False
    gpu_memory_gb: float = 0.0
    
    def __post_init__(self):
        """ì‹œìŠ¤í…œ ì •ë³´ ìë™ ê°ì§€"""
        self._detect_cpu_info()
        self._detect_gpu_info()
        self._detect_apple_silicon()
    
    def _detect_cpu_info(self):
        """CPU ì •ë³´ ê°ì§€"""
        try:
            if self.platform == "Darwin":  # macOS
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                self.cpu_name = result.stdout.strip()
                
                # M3 Max ê°ì§€
                if "M3" in self.cpu_name and "Max" in self.cpu_name:
                    self.is_m3_max = True
                    self.memory_gb = min(self.memory_gb, 128.0)  # M3 Max ìµœëŒ€ 128GB
            
            elif self.platform == "Linux":
                with open('/proc/cpuinfo', 'r') as f:
                    for line in f:
                        if line.startswith('model name'):
                            self.cpu_name = line.split(':')[1].strip()
                            break
            
            elif self.platform == "Windows":
                import winreg
                key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, 
                                   "HARDWARE\\DESCRIPTION\\System\\CentralProcessor\\0")
                self.cpu_name = winreg.QueryValueEx(key, "ProcessorNameString")[0]
                
        except Exception as e:
            logger.warning(f"CPU ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
            self.cpu_name = "Unknown CPU"
    
    def _detect_gpu_info(self):
        """GPU ì •ë³´ ê°ì§€"""
        try:
            # PyTorch GPU ê°ì§€
            import torch
            
            if torch.cuda.is_available():
                self.gpu_available = True
                self.gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.gpu_available = True
                # Apple Siliconì˜ ê²½ìš° í†µí•© ë©”ëª¨ë¦¬ ì‚¬ìš©
                self.gpu_memory_gb = self.memory_gb * 0.7  # í†µí•© ë©”ëª¨ë¦¬ì˜ 70% ì¶”ì •
                
        except ImportError:
            logger.warning("PyTorch ì—†ìŒ - GPU ê°ì§€ ê±´ë„ˆëœ€")
        except Exception as e:
            logger.warning(f"GPU ì •ë³´ ê°ì§€ ì‹¤íŒ¨: {e}")
    
    def _detect_apple_silicon(self):
        """Apple Silicon ê°ì§€"""
        if self.platform == "Darwin" and self.architecture == "arm64":
            self.is_apple_silicon = True
            
            # M ì‹œë¦¬ì¦ˆ ì¹© ê°ì§€
            if any(chip in self.cpu_name for chip in ["M1", "M2", "M3"]):
                self.is_apple_silicon = True
                
                # M3 Max íŠ¹ë³„ ì²˜ë¦¬
                if "M3" in self.cpu_name and "Max" in self.cpu_name:
                    self.is_m3_max = True
                    self.cpu_cores = 16  # M3 Max 16ì½”ì–´
                    self.memory_gb = min(self.memory_gb, 128.0)

# ==============================================
# ğŸ¯ ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì„¤ì • í´ë˜ìŠ¤
# ==============================================

@dataclass
class PipelineConfig:
    """MyCloset AI íŒŒì´í”„ë¼ì¸ í†µí•© ì„¤ì •"""
    
    # === ê¸°ë³¸ ì„¤ì • ===
    device: Union[DeviceType, str] = DeviceType.AUTO
    quality_level: Union[QualityLevel, str] = QualityLevel.BALANCED
    mode: Union[PipelineMode, str] = PipelineMode.PRODUCTION
    processing_strategy: Union[ProcessingStrategy, str] = ProcessingStrategy.SEQUENTIAL
    memory_strategy: Union[MemoryStrategy, str] = MemoryStrategy.BALANCED
    
    # === ì‹œìŠ¤í…œ ì •ë³´ ===
    system_info: Optional[SystemInfo] = None
    
    # === ì„±ëŠ¥ ì„¤ì • ===
    batch_size: int = 1
    max_workers: int = 4
    timeout_seconds: int = 300
    max_retries: int = 3
    enable_caching: bool = True
    cache_size_mb: int = 1024
    
    # === ë©”ëª¨ë¦¬ ì„¤ì • ===
    memory_optimization: bool = True
    gpu_memory_fraction: float = 0.8
    cpu_memory_limit_gb: float = 8.0
    enable_memory_monitoring: bool = True
    memory_cleanup_threshold: float = 0.85
    
    # === ëª¨ë¸ ì„¤ì • ===
    model_precision: str = "float32"  # float16, float32, mixed
    enable_quantization: bool = False
    model_cache_enabled: bool = True
    model_cache_size: int = 10
    preload_models: bool = False
    
    # === 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì„¤ì • ===
    step_configs: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    step_timeouts: Dict[str, int] = field(default_factory=dict)
    step_enabled: Dict[str, bool] = field(default_factory=dict)
    
    # === ê³ ê¸‰ ì„¤ì • ===
    enable_profiling: bool = False
    save_intermediate_results: bool = False
    output_format: str = "png"  # png, jpg, webp
    output_quality: int = 95
    enable_progress_callback: bool = True
    
    # === ë””ë²„ê¹… ì„¤ì • ===
    debug_mode: bool = False
    verbose_logging: bool = False
    save_debug_images: bool = False
    benchmark_mode: bool = False
    
    def __post_init__(self):
        """ì„¤ì • í›„ì²˜ë¦¬ ë° ìµœì í™”"""
        # ì‹œìŠ¤í…œ ì •ë³´ ìë™ ìƒì„±
        if self.system_info is None:
            self.system_info = SystemInfo()
        
        # Enum ë³€í™˜
        self._convert_enums()
        
        # ì‹œìŠ¤í…œë³„ ìë™ ìµœì í™”
        self._auto_optimize_for_system()
        
        # 8ë‹¨ê³„ ì„¤ì • ì´ˆê¸°í™”
        self._initialize_step_configs()
        
        # ê²€ì¦
        self._validate_config()
    
    def _convert_enums(self):
        """ë¬¸ìì—´ì„ Enumìœ¼ë¡œ ë³€í™˜"""
        if isinstance(self.device, str):
            self.device = DeviceType(self.device)
        if isinstance(self.quality_level, str):
            self.quality_level = QualityLevel(self.quality_level)
        if isinstance(self.mode, str):
            self.mode = PipelineMode(self.mode)
        if isinstance(self.processing_strategy, str):
            self.processing_strategy = ProcessingStrategy(self.processing_strategy)
        if isinstance(self.memory_strategy, str):
            self.memory_strategy = MemoryStrategy(self.memory_strategy)
    
    def _auto_optimize_for_system(self):
        """ì‹œìŠ¤í…œë³„ ìë™ ìµœì í™”"""
        system = self.system_info
        
        # M3 Max ìµœì í™”
        if system.is_m3_max:
            self.device = DeviceType.MPS
            self.quality_level = QualityLevel.MAXIMUM
            self.memory_strategy = MemoryStrategy.MAXIMUM
            self.processing_strategy = ProcessingStrategy.PARALLEL
            self.batch_size = min(8, self.batch_size * 4)
            self.max_workers = 16
            self.gpu_memory_fraction = 0.95
            self.cpu_memory_limit_gb = min(64.0, system.memory_gb * 0.8)
            self.model_precision = "float16"
            self.cache_size_mb = 4096
            self.model_cache_size = 15
            logger.info("ğŸ M3 Max ìµœì í™” ì„¤ì • ì ìš©")
        
        # Apple Silicon ì¼ë°˜ ìµœì í™”
        elif system.is_apple_silicon:
            self.device = DeviceType.MPS
            self.memory_strategy = MemoryStrategy.BALANCED
            self.gpu_memory_fraction = 0.8
            self.model_precision = "float16"
            logger.info("ğŸ Apple Silicon ìµœì í™” ì„¤ì • ì ìš©")
        
        # CUDA GPU ìµœì í™”
        elif system.gpu_available and system.gpu_memory_gb > 8:
            self.device = DeviceType.CUDA
            self.processing_strategy = ProcessingStrategy.PARALLEL
            self.batch_size = min(4, self.batch_size * 2)
            self.gpu_memory_fraction = 0.9
            self.model_precision = "float16"
            logger.info("ğŸš€ CUDA GPU ìµœì í™” ì„¤ì • ì ìš©")
        
        # CPU ì „ìš© ìµœì í™”
        else:
            self.device = DeviceType.CPU
            self.quality_level = QualityLevel.FAST
            self.memory_strategy = MemoryStrategy.CONSERVATIVE
            self.batch_size = 1
            self.model_precision = "float32"
            logger.info("ğŸ’» CPU ìµœì í™” ì„¤ì • ì ìš©")
        
        # ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
        if system.memory_gb < 16:
            self.memory_strategy = MemoryStrategy.CONSERVATIVE
            self.cache_size_mb = 512
            self.model_cache_size = 3
            self.cpu_memory_limit_gb = system.memory_gb * 0.6
    
    def _initialize_step_configs(self):
        """8ë‹¨ê³„ë³„ ì„¸ë¶€ ì„¤ì • ì´ˆê¸°í™”"""
        steps = [
            "human_parsing", "pose_estimation", "cloth_segmentation",
            "geometric_matching", "cloth_warping", "virtual_fitting",
            "post_processing", "quality_assessment"
        ]
        
        # ê¸°ë³¸ ë‹¨ê³„ë³„ ì„¤ì •
        for step in steps:
            if step not in self.step_configs:
                self.step_configs[step] = self._get_default_step_config(step)
            
            if step not in self.step_timeouts:
                self.step_timeouts[step] = 60  # ê¸°ë³¸ 60ì´ˆ
            
            if step not in self.step_enabled:
                self.step_enabled[step] = True
        
        # M3 Max ìµœì í™”ëœ íƒ€ì„ì•„ì›ƒ
        if self.system_info.is_m3_max:
            for step in steps:
                self.step_timeouts[step] = min(30, self.step_timeouts[step] // 2)
    
    def _get_default_step_config(self, step: str) -> Dict[str, Any]:
        """ë‹¨ê³„ë³„ ê¸°ë³¸ ì„¤ì • ë°˜í™˜"""
        base_config = {
            "enabled": True,
            "device": self.device.value,
            "precision": self.model_precision,
            "batch_size": self.batch_size,
            "enable_caching": self.enable_caching
        }
        
        # ë‹¨ê³„ë³„ íŠ¹ìˆ˜ ì„¤ì •
        step_specific = {
            "human_parsing": {
                "model_name": "graphonomy",
                "input_size": (512, 512),
                "enable_visualization": True
            },
            "pose_estimation": {
                "model_name": "openpose",
                "confidence_threshold": 0.5,
                "enable_hand_detection": False
            },
            "cloth_segmentation": {
                "model_name": "u2net",
                "refinement_enabled": True,
                "edge_smoothing": True
            },
            "geometric_matching": {
                "model_name": "gmm",
                "matching_threshold": 0.8,
                "enable_refinement": True
            },
            "cloth_warping": {
                "model_name": "tom",
                "warp_strength": 1.0,
                "preserve_details": True
            },
            "virtual_fitting": {
                "model_name": "ootdiffusion",
                "inference_steps": 20,
                "guidance_scale": 7.5
            },
            "post_processing": {
                "enable_super_resolution": self.quality_level in [QualityLevel.HIGH, QualityLevel.ULTRA, QualityLevel.MAXIMUM],
                "enable_denoising": True,
                "sharpening_strength": 0.5
            },
            "quality_assessment": {
                "enable_ai_assessment": True,
                "assessment_model": "combined",
                "threshold_score": 0.7
            }
        }
        
        base_config.update(step_specific.get(step, {}))
        return base_config
    
    def _validate_config(self):
        """ì„¤ì • ê²€ì¦"""
        # ë©”ëª¨ë¦¬ ê²€ì¦
        if self.cpu_memory_limit_gb > self.system_info.memory_gb:
            self.cpu_memory_limit_gb = self.system_info.memory_gb * 0.8
            logger.warning(f"CPU ë©”ëª¨ë¦¬ ì œí•œì„ {self.cpu_memory_limit_gb:.1f}GBë¡œ ì¡°ì •")
        
        # ë°°ì¹˜ í¬ê¸° ê²€ì¦
        if self.batch_size > 16:
            self.batch_size = 16
            logger.warning("ë°°ì¹˜ í¬ê¸°ë¥¼ 16ìœ¼ë¡œ ì œí•œ")
        
        # ì›Œì»¤ ìˆ˜ ê²€ì¦
        if self.max_workers > self.system_info.cpu_cores:
            self.max_workers = self.system_info.cpu_cores
            logger.warning(f"ì›Œì»¤ ìˆ˜ë¥¼ {self.max_workers}ë¡œ ì œí•œ")
    
    # === ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ ===
    
    def get_device_str(self) -> str:
        """ë””ë°”ì´ìŠ¤ ë¬¸ìì—´ ë°˜í™˜"""
        return self.device.value
    
    def get_step_config(self, step_name: str) -> Dict[str, Any]:
        """íŠ¹ì • ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        return self.step_configs.get(step_name, {})
    
    def update_step_config(self, step_name: str, config: Dict[str, Any]):
        """ë‹¨ê³„ ì„¤ì • ì—…ë°ì´íŠ¸"""
        if step_name in self.step_configs:
            self.step_configs[step_name].update(config)
        else:
            self.step_configs[step_name] = config
    
    def enable_debug_mode(self):
        """ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”"""
        self.debug_mode = True
        self.verbose_logging = True
        self.save_debug_images = True
        self.save_intermediate_results = True
        self.enable_profiling = True
        logger.info("ğŸ› ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    
    def enable_production_mode(self):
        """í”„ë¡œë•ì…˜ ëª¨ë“œ ìµœì í™”"""
        self.mode = PipelineMode.PRODUCTION
        self.debug_mode = False
        self.verbose_logging = False
        self.save_debug_images = False
        self.save_intermediate_results = False
        self.enable_profiling = False
        self.memory_optimization = True
        logger.info("ğŸš€ í”„ë¡œë•ì…˜ ëª¨ë“œ í™œì„±í™”")
    
    def to_dict(self) -> Dict[str, Any]:
        """ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        config_dict = {}
        for field_name, field_value in self.__dict__.items():
            if isinstance(field_value, Enum):
                config_dict[field_name] = field_value.value
            elif isinstance(field_value, SystemInfo):
                config_dict[field_name] = field_value.__dict__
            else:
                config_dict[field_name] = field_value
        return config_dict
    
    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'PipelineConfig':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ì„¤ì • ìƒì„±"""
        return cls(**config_dict)
    
    def save_to_file(self, filepath: str):
        """ì„¤ì •ì„ íŒŒì¼ë¡œ ì €ì¥"""
        import json
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)
        logger.info(f"ì„¤ì •ì„ {filepath}ì— ì €ì¥")
    
    @classmethod
    def load_from_file(cls, filepath: str) -> 'PipelineConfig':
        """íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ"""
        import json
        with open(filepath, 'r', encoding='utf-8') as f:
            config_dict = json.load(f)
        return cls.from_dict(config_dict)

# ==============================================
# ğŸ­ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_pipeline_config(
    device: Optional[str] = None,
    quality_level: Optional[str] = None,
    mode: Optional[str] = None,
    **kwargs
) -> PipelineConfig:
    """íŒŒì´í”„ë¼ì¸ ì„¤ì • ìƒì„± íŒ©í† ë¦¬"""
    config_kwargs = kwargs.copy()
    
    if device:
        config_kwargs['device'] = device
    if quality_level:
        config_kwargs['quality_level'] = quality_level
    if mode:
        config_kwargs['mode'] = mode
    
    return PipelineConfig(**config_kwargs)

def create_development_config() -> PipelineConfig:
    """ê°œë°œìš© ì„¤ì • ìƒì„±"""
    config = PipelineConfig(
        mode=PipelineMode.DEVELOPMENT,
        quality_level=QualityLevel.FAST,
        debug_mode=True,
        verbose_logging=True,
        save_intermediate_results=True
    )
    config.enable_debug_mode()
    return config

def create_production_config() -> PipelineConfig:
    """í”„ë¡œë•ì…˜ìš© ì„¤ì • ìƒì„±"""
    config = PipelineConfig(
        mode=PipelineMode.PRODUCTION,
        memory_optimization=True,
        enable_caching=True,
        preload_models=True
    )
    config.enable_production_mode()
    return config

def create_m3_max_config() -> PipelineConfig:
    """M3 Max ìµœì í™” ì„¤ì • ìƒì„±"""
    return PipelineConfig(
        device=DeviceType.MPS,
        quality_level=QualityLevel.MAXIMUM,
        memory_strategy=MemoryStrategy.MAXIMUM,
        processing_strategy=ProcessingStrategy.PARALLEL,
        model_precision="float16",
        batch_size=8,
        max_workers=16
    )

# ==============================================
# ğŸ”§ ì „ì—­ ì„¤ì • ê´€ë¦¬
# ==============================================

_global_pipeline_config: Optional[PipelineConfig] = None

def get_global_pipeline_config() -> PipelineConfig:
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ì„¤ì • ë°˜í™˜"""
    global _global_pipeline_config
    if _global_pipeline_config is None:
        _global_pipeline_config = PipelineConfig()
    return _global_pipeline_config

def set_global_pipeline_config(config: PipelineConfig):
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì„¤ì •"""
    global _global_pipeline_config
    _global_pipeline_config = config

def reset_global_pipeline_config():
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì´ˆê¸°í™”"""
    global _global_pipeline_config
    _global_pipeline_config = None

# ==============================================
# ğŸ¯ í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì„¤ì • ë¡œë“œ
# ==============================================

def load_config_from_env() -> PipelineConfig:
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ"""
    config_kwargs = {}
    
    # ê¸°ë³¸ ì„¤ì •
    if os.getenv('MYCLOSET_DEVICE'):
        config_kwargs['device'] = os.getenv('MYCLOSET_DEVICE')
    if os.getenv('MYCLOSET_QUALITY'):
        config_kwargs['quality_level'] = os.getenv('MYCLOSET_QUALITY')
    if os.getenv('MYCLOSET_MODE'):
        config_kwargs['mode'] = os.getenv('MYCLOSET_MODE')
    
    # ì„±ëŠ¥ ì„¤ì •
    if os.getenv('MYCLOSET_BATCH_SIZE'):
        config_kwargs['batch_size'] = int(os.getenv('MYCLOSET_BATCH_SIZE'))
    if os.getenv('MYCLOSET_MAX_WORKERS'):
        config_kwargs['max_workers'] = int(os.getenv('MYCLOSET_MAX_WORKERS'))
    
    # ë©”ëª¨ë¦¬ ì„¤ì •
    if os.getenv('MYCLOSET_MEMORY_LIMIT'):
        config_kwargs['cpu_memory_limit_gb'] = float(os.getenv('MYCLOSET_MEMORY_LIMIT'))
    if os.getenv('MYCLOSET_GPU_MEMORY_FRACTION'):
        config_kwargs['gpu_memory_fraction'] = float(os.getenv('MYCLOSET_GPU_MEMORY_FRACTION'))
    
    # ë””ë²„ê·¸ ì„¤ì •
    if os.getenv('MYCLOSET_DEBUG', '').lower() in ['true', '1']:
        config_kwargs['debug_mode'] = True
    
    return PipelineConfig(**config_kwargs)

# ==============================================
# ğŸ“‹ ëª¨ë“ˆ Export
# ==============================================

__all__ = [
    # Enums
    'DeviceType', 'QualityLevel', 'PipelineMode', 'ProcessingStrategy', 'MemoryStrategy',
    
    # Classes
    'SystemInfo', 'PipelineConfig',
    
    # Factory Functions
    'create_pipeline_config', 'create_development_config', 'create_production_config', 'create_m3_max_config',
    
    # Global Config Management
    'get_global_pipeline_config', 'set_global_pipeline_config', 'reset_global_pipeline_config',
    
    # Environment Config
    'load_config_from_env'
]

# ==============================================
# ğŸ‰ ëª¨ë“ˆ ì´ˆê¸°í™”
# ==============================================

logger.info("âœ… Pipeline Config ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
logger.info(f"ğŸ ì‹œìŠ¤í…œ: {SystemInfo().platform} ({SystemInfo().architecture})")
logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬: {SystemInfo().memory_gb:.1f}GB")
if SystemInfo().is_m3_max:
    logger.info("ğŸ”¥ M3 Max ê°ì§€ë¨ - ìµœì í™” ëª¨ë“œ í™œì„±í™”")