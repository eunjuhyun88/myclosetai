"""
AI ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï Í¥ÄÎ¶¨
8Îã®Í≥Ñ Í∞ÄÏÉÅ ÌîºÌåÖ ÌååÏù¥ÌîÑÎùºÏù∏Ïùò Ï†ÑÏ≤¥ ÏÑ§Ï†ïÏùÑ Í¥ÄÎ¶¨Ìï©ÎãàÎã§.
Í∏∞Ï°¥ app/ Íµ¨Ï°∞ÏôÄ ÏôÑÏ†ÑÌûà Ìò∏ÌôòÎêòÎ©∞, M3 Max ÏµúÏ†ÅÌôîÎ•º Ìè¨Ìï®Ìï©ÎãàÎã§.
"""

import os
import json
import logging
from typing import Dict, Any, Optional, List, Union
from pathlib import Path
from functools import lru_cache
import torch

from .gpu_config import gpu_config, DEVICE, DEVICE_INFO

logger = logging.getLogger(__name__)

class PipelineConfig:
    """
    8Îã®Í≥Ñ AI ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï Í¥ÄÎ¶¨
    
    Í∏∞Ï°¥ app/ Íµ¨Ï°∞Ïóê ÎßûÏ∂∞ ÏÑ§Í≥ÑÎêú ÏôÑÏ†ÑÌïú ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï ÏãúÏä§ÌÖú
    - 8Îã®Í≥Ñ Í∞ÄÏÉÅ ÌîºÌåÖ ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÄÏõê
    - M3 Max MPS ÏµúÏ†ÅÌôî
    - ÎèôÏ†Å ÏÑ§Ï†ï Î°úÎî©
    - ÌôòÍ≤ΩÎ≥Ñ ÏÑ§Ï†ï Î∂ÑÎ¶¨
    """
    
    def __init__(self, config_path: Optional[str] = None, quality_level: str = "high"):
        """
        Args:
            config_path: ÏÑ§Ï†ï ÌååÏùº Í≤ΩÎ°ú (ÏÑ†ÌÉùÏ†Å)
            quality_level: ÌíàÏßà Î†àÎ≤® (fast, balanced, high, ultra)
        """
        self.quality_level = quality_level
        self.device = DEVICE
        self.device_info = DEVICE_INFO
        
        # Í∏∞Î≥∏ ÏÑ§Ï†ï Î°úÎìú
        self.config = self._load_default_config()
        
        # Ïô∏Î∂Ä ÏÑ§Ï†ï ÌååÏùº Î°úÎìú (ÏûàÎäî Í≤ΩÏö∞)
        if config_path and os.path.exists(config_path):
            self._load_external_config(config_path)
        
        # ÌôòÍ≤ΩÎ≥ÄÏàò Í∏∞Î∞ò Ïò§Î≤ÑÎùºÏù¥Îìú
        self._apply_environment_overrides()
        
        # ÎîîÎ∞îÏù¥Ïä§Î≥Ñ ÏµúÏ†ÅÌôî Ï†ÅÏö©
        self._apply_device_optimizations()
        
        logger.info(f"üîß ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï ÏôÑÎ£å - ÌíàÏßà: {quality_level}, ÎîîÎ∞îÏù¥Ïä§: {self.device}")
    
    def _load_default_config(self) -> Dict[str, Any]:
        """Í∏∞Î≥∏ ÏÑ§Ï†ï Î°úÎìú"""
        return {
            # Ï†ÑÏó≠ ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï
            "pipeline": {
                "name": "mycloset_virtual_fitting",
                "version": "2.0.0",
                "quality_level": self.quality_level,
                "processing_mode": "complete",  # fast, balanced, complete
                "enable_optimization": True,
                "enable_caching": True,
                "enable_parallel": True,
                "memory_optimization": True,
                "max_concurrent_requests": 4,
                "timeout_seconds": 300,
                "enable_intermediate_saving": False,
                "auto_retry": True,
                "max_retries": 3
            },
            
            # Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ ÏÑ§Ï†ï
            "image": {
                "input_size": (512, 512),
                "output_size": (512, 512),
                "max_resolution": 1024,
                "supported_formats": ["jpg", "jpeg", "png", "webp"],
                "quality": 95,
                "preprocessing": {
                    "normalize": True,
                    "resize_mode": "lanczos",
                    "center_crop": True,
                    "background_removal": True
                }
            },
            
            # 8Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ Í∞úÎ≥Ñ ÏÑ§Ï†ï
            "steps": {
                # 1Îã®Í≥Ñ: Ïù∏Ï≤¥ ÌååÏã± (Human Parsing)
                "human_parsing": {
                    "model_name": "graphonomy",
                    "model_path": "app/ai_pipeline/models/ai_models/graphonomy",
                    "num_classes": 20,
                    "confidence_threshold": 0.7,
                    "input_size": (512, 512),
                    "batch_size": 1,
                    "cache_enabled": True,
                    "preprocessing": {
                        "normalize": True,
                        "mean": [0.485, 0.456, 0.406],
                        "std": [0.229, 0.224, 0.225]
                    },
                    "postprocessing": {
                        "morphology_cleanup": True,
                        "smooth_edges": True,
                        "fill_holes": True
                    }
                },
                
                # 2Îã®Í≥Ñ: Ìè¨Ï¶à Ï∂îÏ†ï (Pose Estimation)
                "pose_estimation": {
                    "model_name": "mediapipe",
                    "model_complexity": 2,
                    "min_detection_confidence": 0.5,
                    "min_tracking_confidence": 0.5,
                    "static_image_mode": True,
                    "enable_segmentation": True,
                    "smooth_landmarks": True,
                    "keypoints_format": "openpose_18",
                    "fallback_models": ["openpose", "hrnet"],
                    "pose_validation": {
                        "min_keypoints": 8,
                        "visibility_threshold": 0.3,
                        "symmetry_check": True
                    }
                },
                
                # 3Îã®Í≥Ñ: ÏùòÎ•ò ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò (Cloth Segmentation)
                "cloth_segmentation": {
                    "model_name": "u2net",
                    "model_path": "app/ai_pipeline/models/ai_models/u2net",
                    "fallback_method": "rembg",
                    "background_removal": True,
                    "edge_refinement": True,
                    "post_processing": {
                        "morphology_enabled": True,
                        "gaussian_blur": True,
                        "edge_smoothing": True,
                        "noise_removal": True
                    },
                    "quality_assessment": {
                        "enable": True,
                        "min_quality": 0.6,
                        "auto_retry": True
                    }
                },
                
                # 4Îã®Í≥Ñ: Í∏∞ÌïòÌïôÏ†Å Îß§Ïπ≠ (Geometric Matching)
                "geometric_matching": {
                    "algorithm": "tps_hybrid",  # tps, affine, tps_hybrid
                    "num_control_points": 20,
                    "regularization": 0.001,
                    "matching_method": "hungarian",
                    "keypoint_extraction": {
                        "method": "contour_based",
                        "num_points": 50,
                        "adaptive_sampling": True
                    },
                    "validation": {
                        "min_matched_points": 4,
                        "outlier_threshold": 2.0,
                        "quality_threshold": 0.7
                    }
                },
                
                # 5Îã®Í≥Ñ: Ïò∑ ÏõåÌïë (Cloth Warping)
                "cloth_warping": {
                    "physics_enabled": True,
                    "fabric_simulation": True,
                    "deformation_strength": 0.8,
                    "wrinkle_simulation": True,
                    "fabric_properties": {
                        "cotton": {"stiffness": 0.6, "elasticity": 0.3, "thickness": 0.5},
                        "denim": {"stiffness": 0.9, "elasticity": 0.1, "thickness": 0.8},
                        "silk": {"stiffness": 0.2, "elasticity": 0.4, "thickness": 0.2},
                        "wool": {"stiffness": 0.7, "elasticity": 0.2, "thickness": 0.7},
                        "polyester": {"stiffness": 0.4, "elasticity": 0.6, "thickness": 0.3}
                    },
                    "simulation_steps": 50,
                    "convergence_threshold": 0.001
                },
                
                # 6Îã®Í≥Ñ: Í∞ÄÏÉÅ ÌîºÌåÖ (Virtual Fitting)
                "virtual_fitting": {
                    "model_name": "hr_viton",
                    "model_path": "app/ai_pipeline/models/ai_models/hr_viton",
                    "guidance_scale": 7.5,
                    "num_inference_steps": 50,
                    "strength": 0.8,
                    "eta": 0.0,
                    "composition_method": "neural_blending",
                    "fallback_method": "traditional_blending",
                    "quality_enhancement": {
                        "color_matching": True,
                        "lighting_adjustment": True,
                        "texture_preservation": True,
                        "edge_smoothing": True
                    }
                },
                
                # 7Îã®Í≥Ñ: ÌõÑÏ≤òÎ¶¨ (Post Processing)
                "post_processing": {
                    "super_resolution": {
                        "enabled": True,
                        "model": "real_esrgan",
                        "scale_factor": 2,
                        "model_path": "app/ai_pipeline/models/ai_models/real_esrgan"
                    },
                    "face_enhancement": {
                        "enabled": True,
                        "model": "gfpgan",
                        "strength": 0.8,
                        "model_path": "app/ai_pipeline/models/ai_models/gfpgan"
                    },
                    "color_correction": {
                        "enabled": True,
                        "method": "histogram_matching",
                        "strength": 0.6
                    },
                    "noise_reduction": {
                        "enabled": True,
                        "method": "bilateral_filter",
                        "strength": 0.7
                    },
                    "edge_enhancement": {
                        "enabled": True,
                        "method": "unsharp_mask",
                        "strength": 0.5
                    }
                },
                
                # 8Îã®Í≥Ñ: ÌíàÏßà ÌèâÍ∞Ä (Quality Assessment)
                "quality_assessment": {
                    "metrics": ["ssim", "lpips", "fid", "is"],
                    "quality_threshold": 0.7,
                    "comprehensive_analysis": True,
                    "generate_suggestions": True,
                    "benchmarking": {
                        "enabled": False,
                        "reference_dataset": None,
                        "save_results": False
                    }
                }
            },
            
            # Î™®Îç∏ Í≤ΩÎ°ú ÏÑ§Ï†ï
            "model_paths": {
                "base_dir": "app/ai_pipeline/models/ai_models",
                "cache_dir": "app/ai_pipeline/cache",
                "checkpoints": {
                    "graphonomy": "graphonomy/checkpoints/graphonomy.pth",
                    "hr_viton": "hr_viton/checkpoints/hr_viton.pth",
                    "u2net": "u2net/checkpoints/u2net.pth",
                    "real_esrgan": "real_esrgan/checkpoints/RealESRGAN_x4plus.pth",
                    "gfpgan": "gfpgan/checkpoints/GFPGANv1.4.pth",
                    "openpose": "openpose/checkpoints/pose_iter_440000.caffemodel"
                }
            },
            
            # ÏÑ±Îä• ÏµúÏ†ÅÌôî ÏÑ§Ï†ï
            "optimization": {
                "device": self.device,
                "mixed_precision": True,
                "gradient_checkpointing": False,
                "memory_efficient_attention": True,
                "compile_models": False,  # PyTorch 2.0 compile
                "batch_processing": {
                    "enabled": True,
                    "max_batch_size": 4,
                    "dynamic_batching": True
                },
                "caching": {
                    "enabled": True,
                    "ttl": 3600,  # 1ÏãúÍ∞Ñ
                    "max_size": "2GB",
                    "cache_intermediate": False
                }
            },
            
            # Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏÑ§Ï†ï
            "memory": {
                "max_memory_usage": "80%",
                "cleanup_interval": 300,  # 5Î∂Ñ
                "aggressive_cleanup": False,
                "model_offloading": {
                    "enabled": True,
                    "offload_to": "cpu",
                    "keep_in_memory": ["human_parsing", "pose_estimation"]
                }
            },
            
            # Î°úÍπÖ Î∞è Î™®ÎãàÌÑ∞ÎßÅ
            "logging": {
                "level": "INFO",
                "detailed_timing": True,
                "performance_metrics": True,
                "save_intermediate": False,
                "debug_mode": False
            }
        }
    
    def _load_external_config(self, config_path: str):
        """Ïô∏Î∂Ä ÏÑ§Ï†ï ÌååÏùº Î°úÎìú"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                external_config = json.load(f)
            
            # Îî• Î®∏ÏßÄ
            self._deep_merge(self.config, external_config)
            logger.info(f"‚úÖ Ïô∏Î∂Ä ÏÑ§Ï†ï Î°úÎìú ÏôÑÎ£å: {config_path}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ïô∏Î∂Ä ÏÑ§Ï†ï Î°úÎìú Ïã§Ìå®: {e}")
    
    def _apply_environment_overrides(self):
        """ÌôòÍ≤ΩÎ≥ÄÏàò Í∏∞Î∞ò ÏÑ§Ï†ï Ïò§Î≤ÑÎùºÏù¥Îìú"""
        
        # ÌíàÏßà Î†àÎ≤®
        quality = os.getenv("PIPELINE_QUALITY_LEVEL", self.quality_level)
        if quality != self.quality_level:
            self.quality_level = quality
            self.config["pipeline"]["quality_level"] = quality
            self._apply_quality_preset(quality)
        
        # ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
        device_override = os.getenv("PIPELINE_DEVICE")
        if device_override and device_override != self.device:
            self.device = device_override
            self.config["optimization"]["device"] = device_override
        
        # Î©îÎ™®Î¶¨ Ï†úÌïú
        memory_limit = os.getenv("PIPELINE_MEMORY_LIMIT")
        if memory_limit:
            self.config["memory"]["max_memory_usage"] = memory_limit
        
        # ÎèôÏãú Ï≤òÎ¶¨ Ïàò
        max_concurrent = os.getenv("PIPELINE_MAX_CONCURRENT")
        if max_concurrent:
            try:
                self.config["pipeline"]["max_concurrent_requests"] = int(max_concurrent)
            except ValueError:
                pass
        
        # ÎîîÎ≤ÑÍ∑∏ Î™®Îìú
        debug_mode = os.getenv("PIPELINE_DEBUG", "false").lower() == "true"
        self.config["logging"]["debug_mode"] = debug_mode
        if debug_mode:
            self.config["logging"]["level"] = "DEBUG"
            self.config["logging"]["save_intermediate"] = True
    
    def _apply_device_optimizations(self):
        """ÎîîÎ∞îÏù¥Ïä§Î≥Ñ ÏµúÏ†ÅÌôî Ï†ÅÏö©"""
        
        if self.device == "mps":
            # M3 Max MPS ÏµúÏ†ÅÌôî
            self.config["optimization"].update({
                "mixed_precision": True,
                "memory_efficient_attention": True,
                "compile_models": False,  # MPSÏóêÏÑúÎäî Ïª¥ÌååÏùº ÎπÑÌôúÏÑ±Ìôî
                "batch_processing": {
                    "enabled": True,
                    "max_batch_size": 2,  # MPS Î©îÎ™®Î¶¨ Ï†úÌïú
                    "dynamic_batching": False
                }
            })
            
            # Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï°∞Ï†ï (Î©îÎ™®Î¶¨ Ìö®Ïú®ÏÑ±)
            if self.quality_level in ["fast", "balanced"]:
                self.config["image"]["input_size"] = (512, 512)
                self.config["image"]["max_resolution"] = 1024
            
        elif self.device == "cuda":
            # CUDA ÏµúÏ†ÅÌôî
            self.config["optimization"].update({
                "mixed_precision": True,
                "gradient_checkpointing": True,
                "compile_models": True,
                "batch_processing": {
                    "enabled": True,
                    "max_batch_size": 8,
                    "dynamic_batching": True
                }
            })
            
        else:
            # CPU ÏµúÏ†ÅÌôî
            self.config["optimization"].update({
                "mixed_precision": False,
                "compile_models": False,
                "batch_processing": {
                    "enabled": False,
                    "max_batch_size": 1
                }
            })
            
            # CPUÏóêÏÑúÎäî Îçî ÏûëÏùÄ Î™®Îç∏ ÏÇ¨Ïö©
            self.config["steps"]["virtual_fitting"]["num_inference_steps"] = 20
            self.config["steps"]["post_processing"]["super_resolution"]["enabled"] = False
    
    def _apply_quality_preset(self, quality_level: str):
        """ÌíàÏßà Î†àÎ≤®Ïóê Îî∞Î•∏ ÌîÑÎ¶¨ÏÖã Ï†ÅÏö©"""
        
        quality_presets = {
            "fast": {
                "image_size": (256, 256),
                "inference_steps": 20,
                "super_resolution": False,
                "face_enhancement": False,
                "physics_simulation": False,
                "timeout": 60
            },
            "balanced": {
                "image_size": (512, 512),
                "inference_steps": 30,
                "super_resolution": True,
                "face_enhancement": False,
                "physics_simulation": True,
                "timeout": 120
            },
            "high": {
                "image_size": (512, 512),
                "inference_steps": 50,
                "super_resolution": True,
                "face_enhancement": True,
                "physics_simulation": True,
                "timeout": 300
            },
            "ultra": {
                "image_size": (1024, 1024),
                "inference_steps": 100,
                "super_resolution": True,
                "face_enhancement": True,
                "physics_simulation": True,
                "timeout": 600
            }
        }
        
        preset = quality_presets.get(quality_level, quality_presets["high"])
        
        # Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞
        self.config["image"]["input_size"] = preset["image_size"]
        self.config["image"]["output_size"] = preset["image_size"]
        
        # Ï∂îÎ°† Îã®Í≥Ñ
        self.config["steps"]["virtual_fitting"]["num_inference_steps"] = preset["inference_steps"]
        
        # ÌõÑÏ≤òÎ¶¨ ÏÑ§Ï†ï
        self.config["steps"]["post_processing"]["super_resolution"]["enabled"] = preset["super_resolution"]
        self.config["steps"]["post_processing"]["face_enhancement"]["enabled"] = preset["face_enhancement"]
        
        # Î¨ºÎ¶¨ ÏãúÎÆ¨Î†àÏù¥ÏÖò
        self.config["steps"]["cloth_warping"]["physics_enabled"] = preset["physics_simulation"]
        
        # ÌÉÄÏûÑÏïÑÏõÉ
        self.config["pipeline"]["timeout_seconds"] = preset["timeout"]
        
        logger.info(f"üéØ ÌíàÏßà ÌîÑÎ¶¨ÏÖã Ï†ÅÏö©: {quality_level}")
    
    def _deep_merge(self, base_dict: Dict, update_dict: Dict):
        """ÎîïÏÖîÎÑàÎ¶¨ Îî• Î®∏ÏßÄ"""
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._deep_merge(base_dict[key], value)
            else:
                base_dict[key] = value
    
    # ÏÑ§Ï†ï Ï†ëÍ∑º Î©îÏÑúÎìúÎì§
    
    def get_step_config(self, step_name: str) -> Dict[str, Any]:
        """ÌäπÏ†ï Îã®Í≥Ñ ÏÑ§Ï†ï Î∞òÌôò"""
        return self.config["steps"].get(step_name, {})
    
    def get_model_path(self, model_name: str) -> str:
        """Î™®Îç∏ ÌååÏùº Í≤ΩÎ°ú Î∞òÌôò"""
        base_dir = self.config["model_paths"]["base_dir"]
        checkpoint_path = self.config["model_paths"]["checkpoints"].get(model_name)
        
        if checkpoint_path:
            full_path = os.path.join(base_dir, checkpoint_path)
            return full_path
        else:
            # Í∏∞Î≥∏ Í≤ΩÎ°ú ÏÉùÏÑ±
            return os.path.join(base_dir, model_name)
    
    def get_optimization_config(self) -> Dict[str, Any]:
        """ÏµúÏ†ÅÌôî ÏÑ§Ï†ï Î∞òÌôò"""
        return self.config["optimization"]
    
    def get_memory_config(self) -> Dict[str, Any]:
        """Î©îÎ™®Î¶¨ ÏÑ§Ï†ï Î∞òÌôò"""
        return self.config["memory"]
    
    def get_image_config(self) -> Dict[str, Any]:
        """Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ ÏÑ§Ï†ï Î∞òÌôò"""
        return self.config["image"]
    
    def get_pipeline_config(self) -> Dict[str, Any]:
        """ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ÑÏó≠ ÏÑ§Ï†ï Î∞òÌôò"""
        return self.config["pipeline"]
    
    # ÎèôÏ†Å ÏÑ§Ï†ï Î≥ÄÍ≤Ω Î©îÏÑúÎìúÎì§
    
    def update_quality_level(self, quality_level: str):
        """ÌíàÏßà Î†àÎ≤® ÎèôÏ†Å Î≥ÄÍ≤Ω"""
        if quality_level != self.quality_level:
            self.quality_level = quality_level
            self._apply_quality_preset(quality_level)
            logger.info(f"üîÑ ÌíàÏßà Î†àÎ≤® Î≥ÄÍ≤Ω: {quality_level}")
    
    def update_device(self, device: str):
        """ÎîîÎ∞îÏù¥Ïä§ ÎèôÏ†Å Î≥ÄÍ≤Ω"""
        if device != self.device:
            self.device = device
            self.config["optimization"]["device"] = device
            self._apply_device_optimizations()
            logger.info(f"üîÑ ÎîîÎ∞îÏù¥Ïä§ Î≥ÄÍ≤Ω: {device}")
    
    def enable_debug_mode(self, enabled: bool = True):
        """ÎîîÎ≤ÑÍ∑∏ Î™®Îìú ÌÜ†Í∏Ä"""
        self.config["logging"]["debug_mode"] = enabled
        self.config["logging"]["save_intermediate"] = enabled
        if enabled:
            self.config["logging"]["level"] = "DEBUG"
        logger.info(f"üîÑ ÎîîÎ≤ÑÍ∑∏ Î™®Îìú: {'ÌôúÏÑ±Ìôî' if enabled else 'ÎπÑÌôúÏÑ±Ìôî'}")
    
    def set_memory_limit(self, limit: Union[str, float]):
        """Î©îÎ™®Î¶¨ Ï†úÌïú ÏÑ§Ï†ï"""
        self.config["memory"]["max_memory_usage"] = limit
        logger.info(f"üîÑ Î©îÎ™®Î¶¨ Ï†úÌïú ÏÑ§Ï†ï: {limit}")
    
    # Í≤ÄÏ¶ù Î∞è ÏßÑÎã® Î©îÏÑúÎìúÎì§
    
    def validate_config(self) -> Dict[str, Any]:
        """ÏÑ§Ï†ï Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨"""
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": []
        }
        
        # ÌïÑÏàò Î™®Îç∏ Í≤ΩÎ°ú ÌôïÏù∏
        for model_name, checkpoint_path in self.config["model_paths"]["checkpoints"].items():
            full_path = self.get_model_path(model_name)
            if not os.path.exists(os.path.dirname(full_path)):
                validation_result["warnings"].append(f"Î™®Îç∏ ÎîîÎ†âÌÜ†Î¶¨ ÏóÜÏùå: {full_path}")
        
        # ÎîîÎ∞îÏù¥Ïä§ Ìò∏ÌôòÏÑ± ÌôïÏù∏
        if self.device == "mps" and not torch.backends.mps.is_available():
            validation_result["errors"].append("MPSÍ∞Ä ÏöîÏ≤≠ÎêòÏóàÏßÄÎßå ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§")
            validation_result["valid"] = False
        
        if self.device == "cuda" and not torch.cuda.is_available():
            validation_result["errors"].append("CUDAÍ∞Ä ÏöîÏ≤≠ÎêòÏóàÏßÄÎßå ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§")
            validation_result["valid"] = False
        
        # Î©îÎ™®Î¶¨ ÏÑ§Ï†ï ÌôïÏù∏
        max_memory = self.config["memory"]["max_memory_usage"]
        if isinstance(max_memory, str) and max_memory.endswith("%"):
            try:
                percent = float(max_memory[:-1])
                if not (0 < percent <= 100):
                    validation_result["errors"].append(f"ÏûòÎ™ªÎêú Î©îÎ™®Î¶¨ Î∞±Î∂ÑÏú®: {max_memory}")
                    validation_result["valid"] = False
            except ValueError:
                validation_result["errors"].append(f"ÏûòÎ™ªÎêú Î©îÎ™®Î¶¨ ÌòïÏãù: {max_memory}")
                validation_result["valid"] = False
        
        return validation_result
    
    def get_system_info(self) -> Dict[str, Any]:
        """ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Î∞òÌôò"""
        return {
            "device": self.device,
            "device_info": self.device_info,
            "quality_level": self.quality_level,
            "memory_config": self.get_memory_config(),
            "optimization_config": self.get_optimization_config(),
            "torch_version": torch.__version__,
            "config_valid": self.validate_config()["valid"]
        }
    
    def export_config(self, file_path: str):
        """ÏÑ§Ï†ïÏùÑ JSON ÌååÏùºÎ°ú ÎÇ¥Î≥¥ÎÇ¥Í∏∞"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
            logger.info(f"üíæ ÏÑ§Ï†ï ÎÇ¥Î≥¥ÎÇ¥Í∏∞ ÏôÑÎ£å: {file_path}")
        except Exception as e:
            logger.error(f"‚ùå ÏÑ§Ï†ï ÎÇ¥Î≥¥ÎÇ¥Í∏∞ Ïã§Ìå®: {e}")
    
    def __repr__(self):
        return f"PipelineConfig(device={self.device}, quality={self.quality_level})"


# Ï†ÑÏó≠ ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï Ïù∏Ïä§ÌÑ¥Ïä§Îì§

@lru_cache()
def get_pipeline_config(quality_level: str = "high") -> PipelineConfig:
    """ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò (Ï∫êÏãúÎê®)"""
    return PipelineConfig(quality_level=quality_level)

@lru_cache()
def get_step_configs() -> Dict[str, Dict[str, Any]]:
    """Î™®Îì† Îã®Í≥Ñ ÏÑ§Ï†ï Î∞òÌôò (Ï∫êÏãúÎê®)"""
    config = get_pipeline_config()
    return config.config["steps"]

@lru_cache()
def get_model_paths() -> Dict[str, str]:
    """Î™®Îì† Î™®Îç∏ Í≤ΩÎ°ú Î∞òÌôò (Ï∫êÏãúÎê®)"""
    config = get_pipeline_config()
    return {
        model_name: config.get_model_path(model_name)
        for model_name in config.config["model_paths"]["checkpoints"].keys()
    }

def create_custom_config(
    quality_level: str = "high",
    device: Optional[str] = None,
    custom_settings: Optional[Dict[str, Any]] = None
) -> PipelineConfig:
    """Ïª§Ïä§ÌÖÄ ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï ÏÉùÏÑ±"""
    
    config = PipelineConfig(quality_level=quality_level)
    
    # ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
    if device:
        config.update_device(device)
    
    # Ïª§Ïä§ÌÖÄ ÏÑ§Ï†ï Ï†ÅÏö©
    if custom_settings:
        config._deep_merge(config.config, custom_settings)
    
    return config

# Ï¥àÍ∏∞Ìôî Î∞è Í≤ÄÏ¶ù
_default_config = get_pipeline_config()
_validation_result = _default_config.validate_config()

if not _validation_result["valid"]:
    for error in _validation_result["errors"]:
        logger.error(f"‚ùå ÏÑ§Ï†ï Ïò§Î•ò: {error}")
    
    # Í≤ΩÍ≥†Îäî Î°úÍπÖÎßå
    for warning in _validation_result["warnings"]:
        logger.warning(f"‚ö†Ô∏è ÏÑ§Ï†ï Í≤ΩÍ≥†: {warning}")

logger.info(f"üîß ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï Ï¥àÍ∏∞Ìôî ÏôÑÎ£å - ÎîîÎ∞îÏù¥Ïä§: {DEVICE}")

# ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Î°úÍπÖ
_system_info = _default_config.get_system_info()
logger.info(f"üíª ÏãúÏä§ÌÖú Ï†ïÎ≥¥: {_system_info['device']} ({_system_info['quality_level']})")

# Î™®Îìà Î†àÎ≤® exports
__all__ = [
    "PipelineConfig",
    "get_pipeline_config", 
    "get_step_configs",
    "get_model_paths",
    "create_custom_config"
]

# ÌôòÍ≤ΩÎ≥Ñ ÏÑ§Ï†ï Ìï®ÏàòÎì§
def configure_for_development():
    """Í∞úÎ∞ú ÌôòÍ≤Ω ÏÑ§Ï†ï"""
    config = get_pipeline_config()
    config.enable_debug_mode(True)
    config.config["pipeline"]["enable_caching"] = False
    config.config["logging"]["detailed_timing"] = True
    logger.info("üîß Í∞úÎ∞ú ÌôòÍ≤Ω ÏÑ§Ï†ï Ï†ÅÏö©")

def configure_for_production():
    """ÌîÑÎ°úÎçïÏÖò ÌôòÍ≤Ω ÏÑ§Ï†ï"""
    config = get_pipeline_config()
    config.enable_debug_mode(False)
    config.config["pipeline"]["enable_caching"] = True
    config.config["memory"]["aggressive_cleanup"] = True
    logger.info("üîß ÌîÑÎ°úÎçïÏÖò ÌôòÍ≤Ω ÏÑ§Ï†ï Ï†ÅÏö©")

def configure_for_testing():
    """ÌÖåÏä§Ìä∏ ÌôòÍ≤Ω ÏÑ§Ï†ï"""
    config = get_pipeline_config("fast")  # Îπ†Î•∏ Ï≤òÎ¶¨
    config.config["pipeline"]["max_concurrent_requests"] = 1
    config.config["pipeline"]["timeout_seconds"] = 60
    config.config["logging"]["level"] = "DEBUG"
    logger.info("üîß ÌÖåÏä§Ìä∏ ÌôòÍ≤Ω ÏÑ§Ï†ï Ï†ÅÏö©")