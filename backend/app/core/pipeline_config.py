def _load_external_config(self, config_path: str):
        """ì™¸ë¶€ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                if config_path.endswith('.json'):
                    external_config = json.load(f)
                elif config_path.endswith(('.yml', '.yaml')):
                    try:
                        import yaml
                        external_config = yaml.safe_load(f)
                    except ImportError:
                        logger.warning("âš ï¸ PyYAML not installed, skipping YAML config")
                        return
                else:
                    logger.warning(f"âš ï¸ Unsupported config file format: {config_path}")
                    return
            
            # ë”¥ ë¨¸ì§€
            self._deep_merge(self.config, external_config)
            logger.info(f"âœ… ì™¸ë¶€ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {config_path}")
            
        except FileNotFoundError:
            logger.warning(f"âš ï¸ ì„¤ì • íŒŒì¼ ì—†ìŒ: {config_path}")
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"âŒ ì™¸ë¶€ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _apply_environment_overrides(self):
        """í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ"""
        
        # í’ˆì§ˆ ë ˆë²¨
        quality = os.getenv("PIPELINE_QUALITY_LEVEL", self.quality_level)
        if quality != self.quality_level:
            self.quality_level = quality
            self.config["pipeline"]["quality_level"] = quality
            logger.info(f"ğŸ”„ í™˜ê²½ë³€ìˆ˜: í’ˆì§ˆ ë ˆë²¨ = {quality}")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        device_override = os.getenv("PIPELINE_DEVICE")
        if device_override and device_override != self.device:
            self.device = device_override
            self.config["optimization"]["device"] = device_override
            self.config["system"]["device"] = device_override
            logger.info(f"ğŸ”„ í™˜ê²½ë³€ìˆ˜: ë””ë°”ì´ìŠ¤ = {device_override}")
        
        # ë©”ëª¨ë¦¬ ì œí•œ
        memory_limit = os.getenv("PIPELINE_MEMORY_LIMIT")
        if memory_limit:
            self.config["memory"]["max_memory_usage"] = memory_limit
            logger.info(f"ğŸ”„ í™˜ê²½ë³€ìˆ˜: ë©”ëª¨ë¦¬ ì œí•œ = {memory_limit}")
        
        # ë™ì‹œ ì²˜ë¦¬ ìˆ˜
        max_concurrent = os.getenv("PIPELINE_MAX_CONCURRENT")
        if max_concurrent:
            try:
                concurrent_val = int(max_concurrent)
                self.max_concurrent_requests = concurrent_val
                self.config["pipeline"]["max_concurrent_requests"] = concurrent_val
                logger.info(f"ğŸ”„ í™˜ê²½ë³€ìˆ˜: ë™ì‹œ ì²˜ë¦¬ = {concurrent_val}")
            except ValueError:
                logger.warning(f"âš ï¸ ì˜ëª»ëœ ë™ì‹œ ì²˜ë¦¬ ê°’: {max_concurrent}")
        
        # íƒ€ì„ì•„ì›ƒ
        timeout = os.getenv("PIPELINE_TIMEOUT")
        if timeout:
            try:
                timeout_val = int(timeout)
                self.timeout_seconds = timeout_val
                self.config["pipeline"]["timeout_seconds"] = timeout_val
                logger.info(f"ğŸ”„ í™˜ê²½ë³€ìˆ˜: íƒ€ì„ì•„ì›ƒ = {timeout_val}ì´ˆ")
            except ValueError:
                logger.warning(f"âš ï¸ ì˜ëª»ëœ íƒ€ì„ì•„ì›ƒ ê°’: {timeout}")
        
        # ë””ë²„ê·¸ ëª¨ë“œ
        debug_mode = os.getenv("PIPELINE_DEBUG", "false").lower() == "true"
        if debug_mode != self.debug_mode:
            self.debug_mode = debug_mode
            self.config["logging"]["debug_mode"] = debug_mode
            self.config["logging"]["level"] = "DEBUG" if debug_mode else "INFO"
            self.config["logging"]["save_intermediate"] = debug_mode
            logger.info(f"ğŸ”„ í™˜ê²½ë³€ìˆ˜: ë””ë²„ê·¸ ëª¨ë“œ = {debug_mode}")
        
        # ìµœì í™” í™œì„±í™”/ë¹„í™œì„±í™”
        optimization_override = os.getenv("PIPELINE_OPTIMIZATION")
        if optimization_override:
            enable_opt = optimization_override.lower() == "true"
            if enable_opt != self.optimization_enabled:
                self.optimization_enabled = enable_opt
                self.config["optimization"]["optimization_enabled"] = enable_opt
                self.config["system"]["optimization_enabled"] = enable_opt
                logger.info(f"ğŸ”„ í™˜ê²½ë³€ìˆ˜: ìµœì í™” = {enable_opt}")
        
        # ìºì‹± ì„¤ì •
        caching_override = os.getenv("PIPELINE_CACHING")
        if caching_override:
            enable_cache = caching_override.lower() == "true"
            if enable_cache != self.enable_caching:
                self.enable_caching = enable_cache
                self.config["pipeline"]["enable_caching"] = enable_cache
                self.config["optimization"]["caching"]["enabled"] = enable_cache
                logger.info(f"ğŸ”„ í™˜ê²½ë³€ìˆ˜: ìºì‹± = {enable_cache}")
    
    def _apply_device_optimizations(self):
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì ìš©"""
        
        if self.device == "mps":
            # M3 Max MPS ìµœì í™”
            self.config["optimization"].update({
                "mixed_precision": self.optimization_enabled,
                "memory_efficient_attention": True,
                "compile_models": False,  # MPSì—ì„œëŠ” ì»´íŒŒì¼ ë¹„í™œì„±í™”
                "batch_processing": {
                    "enabled": self.batch_processing,
                    "max_batch_size": 2 if self.is_m3_max else 1,  # M3 Max ë©”ëª¨ë¦¬ ì œí•œ
                    "dynamic_batching": False  # ì•ˆì •ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
                }
            })
            
            # M3 Max ì „ìš© ìµœì í™”
            if self.is_m3_max:
                self.config["memory"]["max_memory_usage"] = "70%"  # ì•ˆì „ ë§ˆì§„
                self.config["optimization"]["model_offloading"]["keep_active"] = 3
                logger.info("ğŸ M3 Max MPS ìµœì í™” ì ìš©")
            
            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            if self.quality_level in ["fast", "balanced"]:
                self.config["image"]["input_size"] = (512, 512)
                self.config["image"]["max_resolution"] = 1024
            
        elif self.device == "cuda":
            # CUDA ìµœì í™”
            self.config["optimization"].update({
                "mixed_precision": self.optimization_enabled,
                "gradient_checkpointing": True,
                "compile_models": self.optimization_enabled,
                "batch_processing": {
                    "enabled": self.batch_processing,
                    "max_batch_size": 8 if self.memory_gb >= 16 else 4,
                    "dynamic_batching": self.dynamic_batching
                }
            })
            
            # CUDA ë©”ëª¨ë¦¬ ê´€ë¦¬
            self.config["memory"]["aggressive_cleanup"] = True
            self.config["memory"]["model_offloading"]["enabled"] = self.memory_gb < 24
            logger.info("ğŸ”¥ CUDA ìµœì í™” ì ìš©")
            
        else:
            # CPU ìµœì í™”
            self.config["optimization"].update({
                "mixed_precision": False,
                "compile_models": False,
                "batch_processing": {
                    "enabled": False,
                    "max_batch_size": 1
                }
            })
            
            # CPUì—ì„œëŠ” ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
            self.config["steps"]["virtual_fitting"]["num_inference_steps"] = 20
            self.config["steps"]["post_processing"]["super_resolution"]["enabled"] = False
            self.config["steps"]["post_processing"]["face_enhancement"]["enabled"] = False
            logger.info("ğŸ’» CPU ìµœì í™” ì ìš©")
    
    def _apply_quality_preset(self, quality_level: str):
        """í’ˆì§ˆ ë ˆë²¨ì— ë”°ë¥¸ í”„ë¦¬ì…‹ ì ìš©"""
        
        quality_presets = {
            "low": {
                "image_size": (256, 256),
                "inference_steps": 15,
                "super_resolution": False,
                "face_enhancement": False,
                "physics_simulation": False,
                "advanced_post_processing": False,
                "timeout": 60,
                "batch_size": 1
            },
            "medium": {
                "image_size": (384, 384),
                "inference_steps": 25,
                "super_resolution": False,
                "face_enhancement": False,
                "physics_simulation": True,
                "advanced_post_processing": False,
                "timeout": 120,
                "batch_size": 1 if self.device == 'cpu' else 2
            },
            "high": {
                "image_size": (512, 512),
                "inference_steps": 50,
                "super_resolution": self.optimization_enabled,
                "face_enhancement": self.optimization_enabled,
                "physics_simulation": True,
                "advanced_post_processing": True,
                "timeout": 300,
                "batch_size": 1 if self.device == 'cpu' else 2
            },
            "ultra": {
                "image_size": (768, 768) if self.is_m3_max else (512, 512),
                "inference_steps": 100,
                "super_resolution": self.optimization_enabled,
                "face_enhancement": self.optimization_enabled,
                "physics_simulation": True,
                "advanced_post_processing": True,
                "timeout": 600,
                "batch_size": 1
            }
        }
        
        preset = quality_presets.get(quality_level, quality_presets["high"])
        
        # ì´ë¯¸ì§€ í¬ê¸°
        self.config["image"]["input_size"] = preset["image_size"]
        self.config["image"]["output_size"] = preset["image_size"]
        
        # ì¶”ë¡  ë‹¨ê³„
        self.config["steps"]["virtual_fitting"]["num_inference_steps"] = preset["inference_steps"]
        
        # í›„ì²˜ë¦¬ ì„¤ì •
        self.config["steps"]["post_processing"]["super_resolution"]["enabled"] = preset["super_resolution"]
        self.config["steps"]["post_processing"]["face_enhancement"]["enabled"] = preset["face_enhancement"]
        self.config["steps"]["post_processing"]["detail_enhancement"]["enabled"] = preset["advanced_post_processing"]
        self.config["steps"]["post_processing"]["edge_enhancement"]["enabled"] = preset["advanced_post_processing"]
        
        # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        self.config["steps"]["cloth_warping"]["physics_enabled"] = preset["physics_simulation"]
        
        # ë°°ì¹˜ í¬ê¸°
        for step_name in ["human_parsing"]:
            if step_name in self.config["steps"]:
                self.config["steps"][step_name]["batch_size"] = preset["batch_size"]
        
        # íƒ€ì„ì•„ì›ƒ
        self.config["pipeline"]["timeout_seconds"] = preset["timeout"]
        
        logger.info(f"ğŸ¯ í’ˆì§ˆ í”„ë¦¬ì…‹ ì ìš©: {quality_level} (í•´ìƒë„: {preset['image_size']})")
    
    def _apply_custom_configurations(self):
        """ì‚¬ìš©ì ì •ì˜ ì„¤ì • ì ìš©"""
        
        # ì»¤ìŠ¤í…€ ë‹¨ê³„ ì„¤ì •
        if self.custom_step_configs:
            for step_name, step_config in self.custom_step_configs.items():
                if step_name in self.config["steps"]:
                    self._deep_merge(self.config["steps"][step_name], step_config)
                    logger.info(f"ğŸ”§ ì»¤ìŠ¤í…€ ë‹¨ê³„ ì„¤ì • ì ìš©: {step_name}")
        
        # ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ
        if self.custom_model_paths:
            self.config["model_paths"]["custom_paths"].update(self.custom_model_paths)
            logger.info(f"ğŸ“ ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ ì ìš©: {len(self.custom_model_paths)}ê°œ")
        
        # ì‹¤í—˜ì  ê¸°ëŠ¥ ì ìš©
        if self.experimental_features:
            # ì‹ ê²½ë§ ìŠ¤íƒ€ì¼ ì „ì†¡
            if self.experimental_features.get('neural_style_transfer'):
                self.config["steps"]["virtual_fitting"]["style_transfer"]["enable"] = True
                logger.info("ğŸ§ª ì‹¤í—˜ì  ê¸°ëŠ¥: ì‹ ê²½ë§ ìŠ¤íƒ€ì¼ ì „ì†¡ í™œì„±í™”")
            
            # ê³ ê¸‰ ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            if self.experimental_features.get('advanced_physics'):
                self.config["steps"]["cloth_warping"]["gravity_simulation"] = True
                self.config["steps"]["cloth_warping"]["wind_simulation"] = True
                logger.info("ğŸ§ª ì‹¤í—˜ì  ê¸°ëŠ¥: ê³ ê¸‰ ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ í™œì„±í™”")
            
            # ë‹¤ì¤‘ ì¸ë¬¼ ì§€ì›
            if self.experimental_features.get('multi_person_support'):
                self.config["steps"]["pose_estimation"]["multi_person"] = True
                self.config["steps"]["human_parsing"]["multi_person"] = True
                logger.info("ğŸ§ª ì‹¤í—˜ì  ê¸°ëŠ¥: ë‹¤ì¤‘ ì¸ë¬¼ ì§€ì› í™œì„±í™”")
    
    def _deep_merge(self, base_dict: Dict, update_dict: Dict):
        """ë”•ì…”ë„ˆë¦¬ ë”¥ ë¨¸ì§€ (ê°œì„ ëœ ë²„ì „)"""
        for key, value in update_dict.items():
            if key in base_dict:
                if isinstance(base_dict[key], dict) and isinstance(value, dict):
                    self._deep_merge(base_dict[key], value)
                elif isinstance(base_dict[key], list) and isinstance(value, list):
                    # ë¦¬ìŠ¤íŠ¸ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                    base_dict[key] = list(set(base_dict[key] + value))
                else:
                    base_dict[key] = value
            else:
                base_dict[key] = value
    
    # ===============================================================
    # âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ì„¤ì • ì ‘ê·¼ ë©”ì„œë“œë“¤ (ì™„ì „ ê°œì„ )
    # ===============================================================
    
    def get_step_config(self, step_name: str) -> Dict[str, Any]:
        """íŠ¹ì • ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        step_config = self.config["steps"].get(step_name, {})
        if not step_config:
            logger.warning(f"âš ï¸ ë‹¨ê³„ ì„¤ì • ì—†ìŒ: {step_name}")
        return step_config
    
    def get_model_path(self, model_name: str) -> str:
        """ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜ (ê°œì„ ëœ ë²„ì „)"""
        # ì»¤ìŠ¤í…€ ê²½ë¡œ ìš°ì„  í™•ì¸
        if model_name in self.config["model_paths"]["custom_paths"]:
            return self.config["model_paths"]["custom_paths"][model_name]
        
        # ê¸°ë³¸ ê²½ë¡œ í™•ì¸
        base_dir = self.config["model_paths"]["base_dir"]
        checkpoint_path = self.config["model_paths"]["checkpoints"].get(model_name)
        
        if checkpoint_path:
            full_path = os.path.join(base_dir, checkpoint_path)
            return full_path
        else:
            # ê¸°ë³¸ ê²½ë¡œ ìƒì„±
            default_path = os.path.join(base_dir, model_name)
            logger.warning(f"âš ï¸ ëª¨ë¸ ê²½ë¡œ ì¶”ì •: {model_name} -> {default_path}")
            return default_path
    
    def get_optimization_config(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ë°˜í™˜"""
        return self.config["optimization"]
    
    def get_memory_config(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì„¤ì • ë°˜í™˜"""
        return self.config["memory"]
    
    def get_image_config(self) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì • ë°˜í™˜"""
        return self.config["image"]
    
    def get_pipeline_config(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì „ì—­ ì„¤ì • ë°˜í™˜"""
        return self.config["pipeline"]
    
    def get_system_config(self) -> Dict[str, Any]:
        """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ì‹œìŠ¤í…œ ì„¤ì • ë°˜í™˜"""
        return {
            "device": self.device,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "cpu_cores": self.system_config.cpu_cores,
            "is_m3_max": self.is_m3_max,
            "platform": self.system_config.platform,
            "architecture": self.system_config.architecture,
            "optimization_enabled": self.optimization_enabled,
            "quality_level": self.quality_level,
            "torch_available": self.system_config.torch_available,
            "constructor_pattern": "optimal"
        }
    
    def get_experimental_config(self) -> Dict[str, Any]:
        """ì‹¤í—˜ì  ê¸°ëŠ¥ ì„¤ì • ë°˜í™˜"""
        return self.config["experimental"]
    
    def get_logging_config(self) -> Dict[str, Any]:
        """ë¡œê¹… ì„¤ì • ë°˜í™˜"""
        return self.config["logging"]
    
    # ===============================================================
    # âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ë™ì  ì„¤ì • ë³€ê²½ ë©”ì„œë“œë“¤ (ì™„ì „ ê°œì„ )
    # ===============================================================
    
    def update_quality_level(self, quality_level: str):
        """í’ˆì§ˆ ë ˆë²¨ ë™ì  ë³€ê²½"""
        valid_levels = ["low", "medium", "high", "ultra"]
        if quality_level not in valid_levels:
            logger.warning(f"âš ï¸ ì˜ëª»ëœ í’ˆì§ˆ ë ˆë²¨: {quality_level}, ìœ íš¨ê°’: {valid_levels}")
            return False
        
        if quality_level != self.quality_level:
            old_level = self.quality_level
            self.quality_level = quality_level
            self._apply_quality_preset(quality_level)
            logger.info(f"ğŸ”„ í’ˆì§ˆ ë ˆë²¨ ë³€ê²½: {old_level} -> {quality_level}")
            return True
        return False
    
    def update_device(self, device: str):
        """ë””ë°”ì´ìŠ¤ ë™ì  ë³€ê²½"""
        valid_devices = ["auto", "cpu", "cuda", "mps"]
        if device not in valid_devices:
            logger.warning(f"âš ï¸ ì˜ëª»ëœ ë””ë°”ì´ìŠ¤: {device}, ìœ íš¨ê°’: {valid_devices}")
            return False
        
        if device != self.device:
            old_device = self.device
            
            if device == "auto":
                self.device = self._determine_optimal_device(None, self.is_m3_max, 
                                                           self.system_config.mps_available, 
                                                           self.system_config.cuda_available)
            else:
                self.device = device
            
            self.config["optimization"]["device"] = self.device
            self.config["system"]["device"] = self.device
            self._apply_device_optimizations()
            logger.info(f"ğŸ”„ ë””ë°”ì´ìŠ¤ ë³€ê²½: {old_device} -> {self.device}")
            return True
        return False
    
    def update_memory_limit(self, memory_gb: float):
        """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ë©”ëª¨ë¦¬ ì œí•œ ë™ì  ë³€ê²½"""
        if memory_gb <= 0:
            logger.warning(f"âš ï¸ ì˜ëª»ëœ ë©”ëª¨ë¦¬ í¬ê¸°: {memory_gb}GB")
            return False
        
        old_memory = self.memory_gb
        self.memory_gb = memory_gb
        self.config["memory"]["memory_gb"] = memory_gb
        self.config["system"]["memory_gb"] = memory_gb
        self.config["memory"]["max_memory_usage"] = f"{min(80, int(memory_gb * 0.8))}%"
        
        # ë©”ëª¨ë¦¬ í¬ê¸°ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if memory_gb >= 32:
            max_batch = 4
        elif memory_gb >= 16:
            max_batch = 2
        else:
            max_batch = 1
        
        if self.device != 'cpu':
            self.config["optimization"]["batch_processing"]["max_batch_size"] = max_batch
        
        logger.info(f"ğŸ”„ ë©”ëª¨ë¦¬ ì œí•œ ë³€ê²½: {old_memory}GB -> {memory_gb}GB")
        return True
    
    def toggle_optimization(self, enabled: bool):
        """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ìµœì í™” í† ê¸€"""
        if enabled == self.optimization_enabled:
            return False
        
        self.optimization_enabled = enabled
        self.config["optimization"]["optimization_enabled"] = enabled
        self.config["system"]["optimization_enabled"] = enabled
        
        # ê´€ë ¨ ì„¤ì •ë“¤ ì—…ë°ì´íŠ¸
        self.config["optimization"]["mixed_precision"] = enabled
        self.config["steps"]["post_processing"]["super_resolution"]["enabled"] = (
            enabled and self.super_resolution_enabled
        )
        self.config["steps"]["human_parsing"]["enable_quantization"] = enabled
        
        logger.info(f"ğŸ”„ ìµœì í™” ëª¨ë“œ: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")
        return True
    
    def toggle_caching(self, enabled: bool):
        """ìºì‹± í† ê¸€"""
        if enabled == self.enable_caching:
            return False
        
        self.enable_caching = enabled
        self.config["pipeline"]["enable_caching"] = enabled
        self.config["optimization"]["caching"]["enabled"] = enabled
        
        # ë‹¨ê³„ë³„ ìºì‹± ì„¤ì • ì—…ë°ì´íŠ¸
        for step_config in self.config["steps"].values():
            if "cache_enabled" in step_config:
                step_config["cache_enabled"] = enabled
        
        logger.info(f"ğŸ”„ ìºì‹±: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")
        return True
    
    def toggle_debug_mode(self, enabled: bool):
        """ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€"""
        if enabled == self.debug_mode:
            return False
        
        self.debug_mode = enabled
        self.save_debug_info = enabled
        self.config["logging"]["debug_mode"] = enabled
        self.config["logging"]["save_debug_info"] = enabled
        self.config["logging"]["save_intermediate"] = enabled
        self.config["logging"]["level"] = "DEBUG" if enabled else "INFO"
        self.config["pipeline"]["save_debug_info"] = enabled
        
        logger.info(f"ğŸ”„ ë””ë²„ê·¸ ëª¨ë“œ: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")
        return True
    
    def update_concurrent_requests(self, max_requests: int):
        """ë™ì‹œ ìš”ì²­ ìˆ˜ ë³€ê²½"""
        if max_requests <= 0 or max_requests > 32:
            logger.warning(f"âš ï¸ ì˜ëª»ëœ ë™ì‹œ ìš”ì²­ ìˆ˜: {max_requests} (1-32 ì‚¬ì´ì—¬ì•¼ í•¨)")
            return False
        
        old_max = self.max_concurrent_requests
        self.max_concurrent_requests = max_requests
        self.config["pipeline"]["max_concurrent_requests"] = max_requests
        
        logger.info(f"ğŸ”„ ë™ì‹œ ìš”ì²­ ìˆ˜ ë³€ê²½: {old_max} -> {max_requests}")
        return True
    
    def update_timeout(self, timeout_seconds: int):
        """íƒ€ì„ì•„ì›ƒ ë³€ê²½"""
        if timeout_seconds <= 0 or timeout_seconds > 3600:
            logger.warning(f"âš ï¸ ì˜ëª»ëœ íƒ€ì„ì•„ì›ƒ: {timeout_seconds}ì´ˆ (1-3600 ì‚¬ì´ì—¬ì•¼ í•¨)")
            return False
        
        old_timeout = self.timeout_seconds
        self.timeout_seconds = timeout_seconds
        self.config["pipeline"]["timeout_seconds"] = timeout_seconds
        
        logger.info(f"ğŸ”„ íƒ€ì„ì•„ì›ƒ ë³€ê²½: {old_timeout}ì´ˆ -> {timeout_seconds}ì´ˆ")
        return True
    
    def enable_experimental_feature(self, feature_name: str, enabled: bool = True):
        """ì‹¤í—˜ì  ê¸°ëŠ¥ í™œì„±í™”/ë¹„í™œì„±í™”"""
        if feature_name not in self.experimental_features:
            self.experimental_features[feature_name] = enabled
        else:
            self.experimental_features[feature_name] = enabled
        
        self.config["experimental"]["features"][feature_name] = enabled
        self._apply_custom_configurations()  # ì‹¤í—˜ì  ê¸°ëŠ¥ ì¬ì ìš©
        
        logger.info(f"ğŸ§ª ì‹¤í—˜ì  ê¸°ëŠ¥ {feature_name}: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")
        return True
    
    def add_custom_model_path(self, model_name: str, model_path: str):
        """ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ ì¶”ê°€"""
        if not os.path.exists(model_path):
            logger.warning(f"âš ï¸ ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {model_path}")
            return False
        
        self.custom_model_paths[model_name] = model_path
        self.config["model_paths"]["custom_paths"][model_name] = model_path
        
        logger.info(f"ğŸ“ ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ ì¶”ê°€: {model_name} -> {model_path}")
        return True
    
    def update_step_config(self, step_name: str, step_config: Dict[str, Any]):
        """ë‹¨ê³„ë³„ ì„¤ì • ì—…ë°ì´íŠ¸"""
        if step_name not in self.config["steps"]:
            logger.warning(f"âš ï¸ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ê³„: {step_name}")
            return False
        
        self._deep_merge(self.config["steps"][step_name], step_config)
        logger.info(f"ğŸ”§ ë‹¨ê³„ ì„¤ì • ì—…ë°ì´íŠ¸: {step_name}")
        return True
    
    # ===============================================================
    # âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ê²€ì¦ ë° ì§„ë‹¨ ë©”ì„œë“œë“¤ (ì™„ì „ ê°œì„ )
    # ===============================================================
    
    def validate_config(self) -> Dict[str, Any]:
        """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬ (ì™„ì „ ê°œì„  ë²„ì „)"""
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "info": [],
            "constructor_pattern": "optimal",
            "validation_timestamp": self._get_timestamp()
        }
        
        # í•„ìˆ˜ ë‹¨ê³„ í™•ì¸
        required_steps = [
            "human_parsing", "pose_estimation", "cloth_segmentation",
            "geometric_matching", "cloth_warping", "virtual_fitting",
            "post_processing", "quality_assessment"
        ]
        
        for step in required_steps:
            if step not in self.config["steps"]:
                validation_result["errors"].append(f"í•„ìˆ˜ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ëˆ„ë½: {step}")
                validation_result["valid"] = False
        
        # ëª¨ë¸ ê²½ë¡œ í™•ì¸
        missing_models = []
        for model_name, checkpoint_path in self.config["model_paths"]["checkpoints"].items():
            full_path = self.get_model_path(model_name)
            model_dir = os.path.dirname(full_path)
            if not os.path.exists(model_dir):
                missing_models.append(model_name)
        
        if missing_models:
            validation_result["warnings"].append(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {', '.join(missing_models)}")
        
        # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í™•ì¸
        if self.device == "mps" and not self.system_config.mps_available:
            validation_result["errors"].append("MPSê°€ ìš”ì²­ë˜ì—ˆì§€ë§Œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            validation_result["valid"] = False
        
        if self.device == "cuda" and not self.system_config.cuda_available:
            validation_result["errors"].append("CUDAê°€ ìš”ì²­ë˜ì—ˆì§€ë§Œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            validation_result["valid"] = False
        
        # ë©”ëª¨ë¦¬ ì„¤ì • í™•ì¸
        max_memory = self.config["memory"]["max_memory_usage"]
        if isinstance(max_memory, str) and max_memory.endswith("%"):
            try:
                percent = float(max_memory[:-1])
                if not (10 <= percent <= 95):
                    validation_result["errors"].append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë²”ìœ„ ì˜¤ë¥˜: {max_memory} (10-95% ì‚¬ì´ì—¬ì•¼ í•¨)")
                    validation_result["valid"] = False
            except ValueError:
                validation_result["errors"].append(f"ì˜ëª»ëœ ë©”ëª¨ë¦¬ í˜•ì‹: {max_memory}")
                validation_result["valid"] = False
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
        if self.memory_gb < 8:
            validation_result["warnings"].append(f"ë©”ëª¨ë¦¬ ë¶€ì¡±: {self.memory_gb}GB < 8GB (ê¶Œì¥)")
        
        if self.memory_gb < 4:
            validation_result["errors"].append(f"ë©”ëª¨ë¦¬ ì‹¬ê° ë¶€ì¡±: {self.memory_gb}GB < 4GB (ìµœì†Œ)")
            validation_result["valid"] = False
        
        # í’ˆì§ˆ ì„¤ì •ê³¼ ì‹œìŠ¤í…œ ì„±ëŠ¥ í˜¸í™˜ì„±
        if self.quality_level == "ultra" and self.memory_gb < 16:
            validation_result["warnings"].append("Ultra í’ˆì§ˆì—ëŠ” 16GB ì´ìƒ ë©”ëª¨ë¦¬ ê¶Œì¥")
        
        if self.quality_level in ["high", "ultra"] and self.device == "cpu":
            validation_result["warnings"].append("High/Ultra í’ˆì§ˆì—ëŠ” GPU ì‚¬ìš© ê¶Œì¥")
        
        # ìµœì  ìƒì„±ì íŒ¨í„´ ê²€ì¦
        required_system_params = [
            "device", "device_type", "memory_gb", "is_m3_max", 
            "optimization_enabled", "quality_level"
        ]
        for param in required_system_params:
            if not hasattr(self, param):
                validation_result["errors"].append(f"í•„ìˆ˜ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ëˆ„ë½: {param}")
                validation_result["valid"] = False
        
        # ì‹¤í—˜ì  ê¸°ëŠ¥ ì¶©ëŒ í™•ì¸
        if self.experimental_features.get('multi_person_support') and self.quality_level == "ultra":
            validation_result["warnings"].append("ë‹¤ì¤‘ ì¸ë¬¼ ì§€ì›ê³¼ Ultra í’ˆì§ˆì€ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŒ")
        
        # ì •ë³´ì„± ë©”ì‹œì§€
        validation_result["info"].append(f"íŒŒì´í”„ë¼ì¸ ë²„ì „: {self.config['pipeline']['version']}")
        validation_result["info"].append(f"ìƒì„±ì íŒ¨í„´: {self.config['pipeline']['constructor_pattern']}")
        validation_result["info"].append(f"í™œì„±í™”ëœ ì‹¤í—˜ì  ê¸°ëŠ¥: {len([k for k, v in self.experimental_features.items() if v])}ê°œ")
        
        return validation_result
    
    def get_system_info(self) -> Dict[str, Any]:
        """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜ (ì™„ì „ ê°œì„  ë²„ì „)"""
        base_info = super().get_system_info()
        
        # PipelineConfig íŠ¹í™” ì •ë³´ ì¶”ê°€
        base_info.update({
            "pipeline_version": self.config["pipeline"]["version"],
            "quality_level": self.quality_level,
            "config_path": self.config_path,
            "device_info": self.device_info,
            "memory_config": self.get_memory_config(),
            "optimization_config": self.get_optimization_config(),
            "torch_version": torch.__version__ if TORCH_AVAILABLE else "N/A",
            "config_valid": self.validate_config()["valid"],
            "pipeline_mode": self.config["pipeline"]["processing_mode"],
            "constructor_pattern": "optimal",
            "total_steps": len(self.config["steps"]),
            "experimental_features_count": len([k for k, v in self.experimental_features.items() if v]),
            "custom_models_count": len(self.custom_model_paths),
            "performance_config": self.get_performance_config()
        })
        
        return base_info
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´"""
        return {
            "quality_level": self.quality_level,
            "device": self.device,
            "optimization_enabled": self.optimization_enabled,
            "expected_memory_usage": self.config["memory"]["max_memory_usage"],
            "max_concurrent_requests": self.max_concurrent_requests,
            "timeout_seconds": self.timeout_seconds,
            "caching_enabled": self.enable_caching,
            "parallel_processing": self.enable_parallel,
            "batch_processing": self.batch_processing,
            "m3_max_optimized": self.is_m3_max,
            "experimental_features_active": len([k for k, v in self.experimental_features.items() if v])
        }
    
    def export_config(self, file_path: str, include_system_info: bool = True):
        """ì„¤ì •ì„ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° (ì™„ì „ ê°œì„  ë²„ì „)"""
        try:
            export_data = {
                "config": self.config,
                "export_metadata": {
                    "export_timestamp": self._get_timestamp(),
                    "exported_from": self.class_name,
                    "constructor_pattern": "optimal",
                    "pipeline_version": self.config["pipeline"]["version"]
                }
            }
            
            if include_system_info:
                export_data.update({
                    "system_info": self.get_system_info(),
                    "performance_summary": self.get_performance_summary(),
                    "validation_result": self.validate_config()
                })
            
            file_path_obj = Path(file_path)
            file_path_obj.parent.mkdir(parents=True, exist_ok=True)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ğŸ’¾ ì„¤ì • ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì„¤ì • ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    def import_config(self, file_path: str, merge_mode: bool = True):
        """ì„¤ì •ì„ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                import_data = json.load(f)
            
            if "config" in import_data:
                imported_config = import_data["config"]
                
                if merge_mode:
                    self._deep_merge(self.config, imported_config)
                else:
                    self.config = imported_config
                
                logger.info(f"ğŸ“¥ ì„¤ì • ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {file_path} ({'ë³‘í•©' if merge_mode else 'êµì²´'} ëª¨ë“œ)")
                return True
            else:
                logger.error("âŒ ì˜ëª»ëœ ì„¤ì • íŒŒì¼ í˜•ì‹")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ì„¤ì • ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    def create_config_backup(self, backup_dir: str = "config_backups"):
        """ì„¤ì • ë°±ì—… ìƒì„±"""
        try:
            backup_path = Path(backup_dir)
            backup_path.mkdir(parents=True, exist_ok=True)
            
            timestamp = self._get_timestamp().replace(":", "-").replace(".", "-")
            backup_file = backup_path / f"pipeline_config_backup_{timestamp}.json"
            
            return self.export_config(str(backup_file), include_system_info=True)
            
        except Exception as e:
            logger.error(f"âŒ ì„¤ì • ë°±ì—… ì‹¤íŒ¨: {e}")
            return False
    
    def __repr__(self):
        return (f"PipelineConfig(device={self.device}, quality={self.quality_level}, "
                f"memory={self.memory_gb}GB, m3_max={self.is_m3_max}, "
                f"optimization={self.optimization_enabled}, constructor='optimal', "
                f"steps={len(self.config.get('steps', {}))}, "
                f"experimental_features={len([k for k, v in self.experimental_features.items() if v])})")


# ===============================================================
# âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ì „ì—­ íŒŒì´í”„ë¼ì¸ ì„¤ì • íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ===============================================================

@lru_cache()
def get_pipeline_config(
    quality_level: str = "high",
    device: Optional[str] = None,    # ğŸ”¥ ìë™ ê°ì§€
    **kwargs  # ğŸš€ í™•ì¥ì„±
) -> PipelineConfig:
    """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - íŒŒì´í”„ë¼ì¸ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ìºì‹œë¨)"""
    return PipelineConfig(
        device=device,
        quality_level=quality_level,
        **kwargs
    )

@lru_cache()
def get_step_configs() -> Dict[str, Dict[str, Any]]:
    """ëª¨ë“  ë‹¨ê³„ ì„¤ì • ë°˜í™˜ (ìºì‹œë¨)"""
    config = get_pipeline_config()
    return config.config["steps"]

@lru_cache()
def get_model_paths() -> Dict[str, str]:
    """ëª¨ë“  ëª¨ë¸ ê²½ë¡œ ë°˜í™˜ (ìºì‹œë¨)"""
    config = get_pipeline_config()
    return {
        model_name: config.get_model_path(model_name)
        for model_name in config.config["model_paths"]["checkpoints"].keys()
    }

def create_custom_config(
    quality_level: str = "high",
    device: Optional[str] = None,      # ğŸ”¥ ìë™ ê°ì§€
    custom_settings: Optional[Dict[str, Any]] = None,
    **kwargs  # ğŸš€ í™•ì¥ì„±
) -> PipelineConfig:
    """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ì»¤ìŠ¤í…€ íŒŒì´í”„ë¼ì¸ ì„¤ì • ìƒì„±"""
    
    # ì»¤ìŠ¤í…€ ì„¤ì •ì„ kwargsì— ë³‘í•©
    if custom_settings:
        kwargs.update(custom_settings)
    
    config = PipelineConfig(
        device=device,
        quality_level=quality_level,
        **kwargs
    )
    
    return config

def create_optimized_config(
    device: Optional[str] = None,
    optimization_level: str = "balanced",  # conservative, balanced, aggressive
    **kwargs
) -> PipelineConfig:
    """ìµœì í™” ë ˆë²¨ì— ë”°ë¥¸ ì„¤ì • ìƒì„±"""
    
    optimization_presets = {
        "conservative": {
            "optimization_enabled": False,
            "enable_caching": False,
            "batch_processing": False,
            "memory_optimization": False,
            "quality_level": "medium"
        },
        "balanced": {
            "optimization_enabled": True,
            "enable_caching": True,
            "batch_processing": True,
            "memory_optimization": True,
            "quality_level": "high"
        },
        "aggressive": {
            "optimization_enabled": True,
            "enable_caching": True,
            "batch_processing": True,
            "dynamic_batching": True,
            "memory_optimization": True,
            "super_resolution_enabled": True,
            "face_enhancement_enabled": True,
            "physics_simulation_enabled": True,
            "quality_level": "ultra"
        }
    }
    
    preset = optimization_presets.get(optimization_level, optimization_presets["balanced"])
    preset.update(kwargs)
    
    return PipelineConfig(device=device, **preset)

# ===============================================================
# âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥ í•¨ìˆ˜ë“¤
# ===============================================================

def create_optimal_pipeline_config(
    device: Optional[str] = None,      # ğŸ”¥ ìë™ ê°ì§€
    config: Optional[Dict[str, Any]] = None,
    **kwargs  # ğŸš€ í™•ì¥ì„±
) -> PipelineConfig:
    """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ìƒˆë¡œìš´ ìµœì  ë°©ì‹"""
    return PipelineConfig(
        device=device,
        config=config,
        **kwargs
    )

def create_legacy_pipeline_config(
    config_path: Optional[str] = None, 
    quality_level: str = "high"
) -> PipelineConfig:
    """ê¸°ì¡´ ë°©ì‹ í˜¸í™˜ (ìµœì  ìƒì„±ì íŒ¨í„´ìœ¼ë¡œ ë‚´ë¶€ ì²˜ë¦¬)"""
    return PipelineConfig(
        config_path=config_path,
        quality_level=quality_level
    )

def create_advanced_pipeline_config(
    device: Optional[str] = None,
    quality_level: str = "high",
    experimental_features: Optional[Dict[str, bool]] = None,
    custom_model_paths: Optional[Dict[str, str]] = None,
    custom_step_configs: Optional[Dict[str, Dict]] = None,
    **kwargs
) -> PipelineConfig:
    """ê³ ê¸‰ íŒŒì´í”„ë¼ì¸ ì„¤ì • ìƒì„±"""
    
    return PipelineConfig(
        device=device,
        quality_level=quality_level,
        experimental_features=experimental_features or {},
        custom_model_paths=custom_model_paths or {},
        custom_step_configs=custom_step_configs or {},
        **kwargs
    )

# ===============================================================
# í™˜ê²½ë³„ ì„¤ì • í•¨ìˆ˜ë“¤ - ìµœì  ìƒì„±ì íŒ¨í„´ (ì™„ì „ ê°œì„ )
# ===============================================================

def configure_for_development(**kwargs):
    """ê°œë°œ í™˜ê²½ ì„¤ì • - ìµœì  ìƒì„±ì íŒ¨í„´"""
    dev_config = {
        "quality_level": "medium",
        "optimization_enabled": False,
        "enable_caching": False,
        "enable_intermediate_saving": True,
        "debug_mode": True,
        "save_debug_info": True,
        "profiling_enabled": True,
        "timeout_seconds": 120,
        "max_concurrent_requests": 2,
        **kwargs
    }
    
    config = create_optimal_pipeline_config(**dev_config)
    logger.info("ğŸ”§ ê°œë°œ í™˜ê²½ ì„¤ì • ì ìš© (ìµœì  ìƒì„±ì íŒ¨í„´)")
    return config

def configure_for_production(**kwargs):
    """í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì • - ìµœì  ìƒì„±ì íŒ¨í„´"""
    prod_config = {
        "quality_level": "high",
        "optimization_enabled": True,
        "enable_caching": True,
        "memory_optimization": True,
        "debug_mode": False,
        "save_debug_info": False,
        "profiling_enabled": False,
        "timeout_seconds": 300,
        "max_concurrent_requests": 8,
        "auto_retry": True,
        "max_retries": 2,
        **kwargs
    }
    
    config = create_optimal_pipeline_config(**prod_config)
    logger.info("ğŸ”§ í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì • ì ìš© (ìµœì  ìƒì„±ì íŒ¨í„´)")
    return config

def configure_for_testing(**kwargs):
    """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • - ìµœì  ìƒì„±ì íŒ¨í„´"""
    test_config = {
        "quality_level": "low",
        "max_concurrent_requests": 1,
        "timeout_seconds": 60,
        "optimization_enabled": False,
        "enable_caching": False,
        "batch_processing": False,
        "debug_mode": True,
        "save_debug_info": True,
        **kwargs
    }
    
    config = create_optimal_pipeline_config(**test_config)
    logger.info("ğŸ”§ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì ìš© (ìµœì  ìƒì„±ì íŒ¨í„´)")
    return config

def configure_for_m3_max(**kwargs):
    """âœ… M3 Max ìµœì í™” ì„¤ì • - ìµœì  ìƒì„±ì íŒ¨í„´"""
    m3_config = {
        "device": "mps",
        "quality_level": "high",
        "memory_gb": 128.0,
        "is_m3_max": True,
        "optimization_enabled": True,
        "enable_caching": True,
        "memory_optimization": True,
        "batch_processing": True,
        "super_resolution_enabled": True,
        "face_enhancement_enabled": True,
        "physics_simulation_enabled": True,
        "advanced_post_processing": True,
        "max_concurrent_requests": 6,
        "timeout_seconds": 300,
        **kwargs
    }
    
    config = create_optimal_pipeline_config(**m3_config)
    logger.info("ğŸ”§ M3 Max ìµœì í™” ì„¤ì • ì ìš© (ìµœì  ìƒì„±ì íŒ¨í„´)")
    return config

def configure_for_low_memory(**kwargs):
    """ì €ë©”ëª¨ë¦¬ í™˜ê²½ ì„¤ì •"""
    low_mem_config = {
        "quality_level": "low",
        "memory_gb": 8.0,
        "optimization_enabled": False,
        "enable_caching": False,
        "batch_processing": False,
        "super_resolution_enabled": False,
        "face_enhancement_enabled": False,
        "physics_simulation_enabled": False,
        "max_concurrent_requests": 1,
        "timeout_seconds": 180,
        **kwargs
    }
    
    config = create_optimal_pipeline_config(**low_mem_config)
    logger.info("ğŸ”§ ì €ë©”ëª¨ë¦¬ í™˜ê²½ ì„¤ì • ì ìš©")
    return config

def configure_for_high_performance(**kwargs):
    """ê³ ì„±ëŠ¥ í™˜ê²½ ì„¤ì •"""
    high_perf_config = {
        "quality_level": "ultra",
        "optimization_enabled": True,
        "enable_caching": True,
        "batch_processing": True,
        "dynamic_batching": True,
        "memory_optimization": True,
        "super_resolution_enabled": True,
        "face_enhancement_enabled": True,
        "physics_simulation_enabled": True,
        "advanced_post_processing": True,
        "max_concurrent_requests": 12,
        "timeout_seconds": 600,
        "experimental_features": {
            "advanced_physics": True,
            "neural_style_transfer": True,
            "advanced_lighting": True
        },
        **kwargs
    }
    
    config = create_optimal_pipeline_config(**high_perf_config)
    logger.info("ğŸ”§ ê³ ì„±ëŠ¥ í™˜ê²½ ì„¤ì • ì ìš©")
    return config

# ===============================================================
# ì„¤ì • ê´€ë¦¬ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ===============================================================

def compare_configs(config1: PipelineConfig, config2: PipelineConfig) -> Dict[str, Any]:
    """ë‘ ì„¤ì • ë¹„êµ"""
    def deep_diff(dict1, dict2, path=""):
        """ë”•ì…”ë„ˆë¦¬ ê¹Šì€ ë¹„êµ"""
        differences = []
        
        all_keys = set(dict1.keys()) | set(dict2.keys())
        for key in all_keys:
            current_path = f"{path}.{key}" if path else key
            
            if key not in dict1:
                differences.append(f"+ {current_path}: {dict2[key]}")
            elif key not in dict2:
                differences.append(f"- {current_path}: {dict1[key]}")
            elif isinstance(dict1[key], dict) and isinstance(dict2[key], dict):
                differences.extend(deep_diff(dict1[key], dict2[key], current_path))
            elif dict1[key] != dict2[key]:
                differences.append(f"~ {current_path}: {dict1[key]} -> {dict2[key]}")
        
        return differences
    
    return {
        "config1_name": f"{config1.class_name}({config1.quality_level})",
        "config2_name": f"{config2.class_name}({config2.quality_level})",
        "differences": deep_diff(config1.config, config2.config),
        "system_differences": deep_diff(config1.get_system_config(), config2.get_system_config())
    }

def merge_configs(base_config: PipelineConfig, override_config: PipelineConfig) -> PipelineConfig:
    """ë‘ ì„¤ì • ë³‘í•©"""
    merged_config = PipelineConfig(
        device=override_config.device,
        quality_level=override_config.quality_level,
        optimization_enabled=override_config.optimization_enabled
    )
    
    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹œì‘
    merged_config.config = base_config.config.copy()
    
    # ì˜¤ë²„ë¼ì´ë“œ ì„¤ì • ë³‘í•©
    merged_config._deep_merge(merged_config.config, override_config.config)
    
    logger.info(f"ğŸ”€ ì„¤ì • ë³‘í•© ì™„ë£Œ: {base_config.quality_level} + {override_config.quality_level}")
    return merged_config

def create_config_profile(profile_name: str, **kwargs) -> PipelineConfig:
    """í”„ë¡œí•„ ê¸°ë°˜ ì„¤ì • ìƒì„±"""
    profiles = {
        "ultra_quality": {
            "quality_level": "ultra",
            "optimization_enabled": True,
            "super_resolution_enabled": True,
            "face_enhancement_enabled": True,
            "physics_simulation_enabled": True,
            "advanced_post_processing": True
        },
        "speed_optimized": {
            "quality_level": "low",
            "optimization_enabled": True,
            "enable_caching": True,
            "batch_processing": True,
            "timeout_seconds": 60
        },
        "memory_efficient": {
            "quality_level": "medium",
            "memory_optimization": True,
            "batch_processing": False,
            "super_resolution_enabled": False,
            "face_enhancement_enabled": False
        },
        "experimental": {
            "quality_level": "high",
            "experimental_features": {
                "neural_style_transfer": True,
                "advanced_physics": True,
                "multi_person_support": True,
                "3d_pose_estimation": True
            }
        }
    }
    
    profile_config = profiles.get(profile_name, {})
    profile_config.update(kwargs)
    
    config = create_optimal_pipeline_config(**profile_config)
    logger.info(f"ğŸ‘¤ í”„ë¡œí•„ ì„¤ì • ìƒì„±: {profile_name}")
    return config

# ===============================================================
# ì´ˆê¸°í™” ë° ê²€ì¦ (ìµœì  ìƒì„±ì íŒ¨í„´)
# ===============================================================

# ê¸°ë³¸ ì„¤ì • ìƒì„± (ìë™ ê°ì§€)
try:
    _default_config = get_pipeline_config()
    _validation_result = _default_config.validate_config()

    if not _validation_result["valid"]:
        for error in _validation_result["errors"]:
            logger.error(f"âŒ ì„¤ì • ì˜¤ë¥˜: {error}")
        
        # ê²½ê³ ëŠ” ë¡œê¹…ë§Œ
        for warning in _validation_result["warnings"]:
            logger.warning(f"âš ï¸ ì„¤ì • ê²½ê³ : {warning}")
    else:
        logger.info("âœ… ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ì„¤ì • ê²€ì¦ ì™„ë£Œ")

    logger.info(f"ğŸ”§ ìµœì  ìƒì„±ì íŒ¨í„´ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {DEVICE}")

    # ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹…
    _system_info = _default_config.get_system_info()
    logger.info(f"ğŸ’» ì‹œìŠ¤í…œ: {_system_info['device']} ({_system_info['quality_level']}) - ìµœì  ìƒì„±ì íŒ¨í„´")
    logger.info(f"ğŸ¯ ë©”ëª¨ë¦¬: {_system_info['memory_gb']}GB, M3 Max: {'âœ…' if _system_info['is_m3_max'] else 'âŒ'}")
    logger.info(f"ğŸ”§ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„: {_system_info['total_steps']}ê°œ")
    logger.info(f"ğŸ§ª ì‹¤í—˜ì  ê¸°ëŠ¥: {_system_info['experimental_features_count']}ê°œ í™œì„±í™”")

except Exception as e:
    logger.error(f"âŒ ê¸°ë³¸ ì„¤ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    # ìµœì†Œí•œì˜ í´ë°± ì„¤ì •
    _default_config = None

# ===============================================================
# ìµœì  ìƒì„±ì íŒ¨í„´ í˜¸í™˜ì„± ê²€ì¦ (ì™„ì „ ê°œì„ )
# ===============================================================

def validate_optimal_constructor_compatibility() -> Dict[str, bool]:
    """ìµœì  ìƒì„±ì íŒ¨í„´ í˜¸í™˜ì„± ê²€ì¦ (ì™„ì „ ê°œì„  ë²„ì „)"""
    try:
        # í…ŒìŠ¤íŠ¸ ì„¤ì • ìƒì„± - ìµœì  ìƒì„±ì íŒ¨í„´
        test_config = create_optimal_pipeline_config(
            device="cpu",  # ëª…ì‹œì  ì„¤ì •
            quality_level="medium",
            device_type="test",
            memory_gb=16.0,
            is_m3_max=False,
            optimization_enabled=True,
            enable_caching=True,
            batch_processing=True,
            experimental_features={"test_feature": True},
            custom_model_paths={"test_model": "/test/path"},
            custom_step_configs={"test_step": {"test_param": "test_value"}},
            custom_param="test_value"  # í™•ì¥ íŒŒë¼ë¯¸í„°
        )
        
        # í•„ìˆ˜ ì†ì„± í™•ì¸
        required_attrs = [
            'device', 'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level', 'enable_caching',
            'batch_processing', 'experimental_features', 'custom_model_paths'
        ]
        attr_check = {attr: hasattr(test_config, attr) for attr in required_attrs}
        
        # í•„ìˆ˜ ë©”ì„œë“œ í™•ì¸
        required_methods = [
            'get_step_config', 'get_model_path', 'get_system_config',
            'update_quality_level', 'update_device', 'validate_config',
            'toggle_optimization', 'toggle_caching', 'enable_experimental_feature',
            'export_config', 'get_performance_summary'
        ]
        method_check = {method: hasattr(test_config, method) and callable(getattr(test_config, method)) 
                       for method in required_methods}
        
        # í™•ì¥ íŒŒë¼ë¯¸í„° í™•ì¸
        extension_check = test_config.config.get('custom_param') == 'test_value'
        
        # ì‹¤í—˜ì  ê¸°ëŠ¥ í™•ì¸
        experimental_check = test_config.experimental_features.get('test_feature') == True
        
        # ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ í™•ì¸
        custom_model_check = test_config.custom_model_paths.get('test_model') == '/test/path'
        
        # ì„¤ì • ê²€ì¦ í™•ì¸
        validation_result = test_config.validate_config()
        validation_check = isinstance(validation_result, dict) and 'valid' in validation_result
        
        # ë™ì  ë³€ê²½ í…ŒìŠ¤íŠ¸
        dynamic_test_passed = True
        try:
            test_config.update_quality_level("low")
            test_config.toggle_optimization(False)
            test_config.toggle_caching(False)
            test_config.enable_experimental_feature("new_feature", True)
        except Exception:
            dynamic_test_passed = False
        
        return {
            'attributes': all(attr_check.values()),
            'methods': all(method_check.values()),
            'extensions': extension_check,
            'experimental_features': experimental_check,
            'custom_models': custom_model_check,
            'validation': validation_check,
            'dynamic_changes': dynamic_test_passed,
            'attr_details': attr_check,
            'method_details': method_check,
            'overall_compatible': (
                all(attr_check.values()) and 
                all(method_check.values()) and 
                extension_check and
                experimental_check and
                custom_model_check and
                validation_check and
                dynamic_test_passed
            ),
            'constructor_pattern': 'optimal',
            'test_config_info': test_config.get_system_info()
        }
        
    except Exception as e:
        logger.error(f"ìµœì  ìƒì„±ì íŒ¨í„´ í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {
            'overall_compatible': False, 
            'error': str(e), 
            'constructor_pattern': 'optimal'
        }

# ëª¨ë“ˆ ë¡œë“œ ì‹œ í˜¸í™˜ì„± ê²€ì¦
_compatibility_result = validate_optimal_constructor_compatibility()
if _compatibility_result.get('overall_compatible'):
    logger.info("âœ… ìµœì  ìƒì„±ì íŒ¨í„´ í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ (ì™„ì „ ê°œì„  ë²„ì „)")
    if _compatibility_result.get('test_config_info'):
        test_info = _compatibility_result['test_config_info']
        logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì„¤ì •: {test_info.get('pipeline_version', 'unknown')} "
                   f"({test_info.get('total_steps', 0)}ë‹¨ê³„)")
else:
    logger.warning(f"âš ï¸ í˜¸í™˜ì„± ë¬¸ì œ ë°œê²¬: {_compatibility_result}")

# ===============================================================
# ì„¤ì • í…œí”Œë¦¿ ë° ì˜ˆì œ
# ===============================================================

def get_config_templates() -> Dict[str, Dict[str, Any]]:
    """ì„¤ì • í…œí”Œë¦¿ ë°˜í™˜"""
    return {
        "minimal": {
            "quality_level": "low",
            "optimization_enabled": False,
            "enable_caching": False
        },
        "standard": {
            "quality_level": "high",
            "optimization_enabled": True,
            "enable_caching": True,
            "batch_processing": True
        },
        "professional": {
            "quality_level": "ultra",
            "optimization_enabled": True,
            "enable_caching": True,
            "batch_processing": True,
            "super_resolution_enabled": True,
            "face_enhancement_enabled": True,
            "physics_simulation_enabled": True,
            "advanced_post_processing": True
        },
        "m3_max": {
            "device": "mps",
            "quality_level": "high",
            "optimization_enabled": True,
            "memory_gb": 128.0,
            "is_m3_max": True,
            "enable_caching": True,
            "batch_processing": True,
            "super_resolution_enabled": True
        }
    }

def create_config_from_template(template_name: str, **overrides) -> PipelineConfig:
    """í…œí”Œë¦¿ì—ì„œ ì„¤ì • ìƒì„±"""
    templates = get_config_templates()
    
    if template_name not in templates:
        available = ", ".join(templates.keys())
        raise ValueError(f"Unknown template: {template_name}. Available: {available}")
    
    template_config = templates[template_name].copy()
    template_config.update(overrides)
    
    config = create_optimal_pipeline_config(**template_config)
    logger.info(f"ğŸ“‹ í…œí”Œë¦¿ ì„¤ì • ìƒì„±: {template_name}")
    return config

# ëª¨ë“ˆ ë ˆë²¨ exports (ì™„ì „ ê°œì„ )
__all__ = [
    # í•µì‹¬ í´ë˜ìŠ¤ë“¤
    "OptimalConstructorBase",         # ìµœì  ìƒì„±ì ë² ì´ìŠ¤
    "PipelineConfig",                 # ë©”ì¸ ì„¤ì • í´ë˜ìŠ¤
    "SystemConfig",                   # ì‹œìŠ¤í…œ ì„¤ì • ë°ì´í„° í´ë˜ìŠ¤
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ë³¸)
    "get_pipeline_config", 
    "get_step_configs",
    "get_model_paths",
    "create_custom_config",
    "create_optimized_config",
    
    # ìƒì„±ì íŒ¨í„´ í•¨ìˆ˜ë“¤
    "create_optimal_pipeline_config", # ìƒˆë¡œìš´ ìµœì  ë°©ì‹
    "create_legacy_pipeline_config",  # ê¸°ì¡´ í˜¸í™˜ì„±
    "create_advanced_pipeline_config", # ê³ ê¸‰ ì„¤ì •
    
    # í™˜ê²½ë³„ ì„¤ì • í•¨ìˆ˜ë“¤
    "configure_for_development",
    "configure_for_production", 
    "configure_for_testing",
    "configure_for_m3_max",
    "configure_for_low_memory",
    "configure_for_high_performance",
    
    # ì„¤ì • ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°
    "compare_configs",
    "merge_configs",
    "create_config_profile",
    "get_config_templates",
    "create_config_from_template",
    
    # ê²€ì¦ ë° í˜¸í™˜ì„±
    "validate_optimal_constructor_compatibility"
]

logger.info("ğŸ¯ ìµœì  ìƒì„±ì íŒ¨í„´ PipelineConfig ì´ˆê¸°í™” ì™„ë£Œ (ì™„ì „ ê°œì„  ë²„ì „) - ëª¨ë“  ê¸°ëŠ¥ í™œì„±í™”")
logger.info(f"ğŸ“š ë‚´ë³´ë‚´ê¸° í•¨ìˆ˜: {len(__all__)}ê°œ")
logger.info(f"ğŸ”§ GPU Config ì‚¬ìš© ê°€ëŠ¥: {'âœ…' if GPU_CONFIG_AVAILABLE else 'âŒ'}")
logger.info(f"ğŸ PyTorch ì‚¬ìš© ê°€ëŠ¥: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")# backend/app/core/pipeline_config.py
"""
ğŸ¯ MyCloset AI Backend ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ì„¤ì • ê´€ë¦¬
- ìµœì  ìƒì„±ì íŒ¨í„´ (Optimal Constructor Pattern) ì ìš©
- 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ ì„¤ì • ê´€ë¦¬
- ê¸°ì¡´ app/ êµ¬ì¡°ì™€ ì™„ì „ í˜¸í™˜
- M3 Max ìµœì í™” ë° ìë™ ê°ì§€ í¬í•¨
- ë¬´ì œí•œ í™•ì¥ì„±ê³¼ í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥

âœ… ìµœì  ìƒì„±ì íŒ¨í„´ ì¥ì :
- ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ (Noneìœ¼ë¡œ ì„¤ì • ì‹œ)
- **kwargsë¡œ ë¬´ì œí•œ í™•ì¥ì„±
- í‘œì¤€ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° í†µì¼
- í•˜ìœ„ í˜¸í™˜ì„± 100% ë³´ì¥
- íƒ€ì… ì•ˆì „ì„± ë° ê²€ì¦ ê¸°ëŠ¥
"""

import os
import json
import logging
import platform
import subprocess
from typing import Dict, Any, Optional, List, Union
from pathlib import Path
from functools import lru_cache
from abc import ABC, abstractmethod
from dataclasses import dataclass, field

# PyTorch ì•ˆì „ import
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

# ê¸°ì¡´ gpu_config import (ì•ˆì „í•œ import)
try:
    from .gpu_config import gpu_config, DEVICE, DEVICE_INFO
    GPU_CONFIG_AVAILABLE = True
except ImportError:
    logger = logging.getLogger(__name__)
    logger.warning("gpu_config import ì‹¤íŒ¨ - ê¸°ë³¸ê°’ ì‚¬ìš©")
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    DEVICE = "mps" if TORCH_AVAILABLE and torch.backends.mps.is_available() else (
        "cuda" if TORCH_AVAILABLE and torch.cuda.is_available() else "cpu"
    )
    DEVICE_INFO = {"device": DEVICE, "available": True}
    
    class DummyGPUConfig:
        def __init__(self):
            self.device = DEVICE
            self.device_type = "auto"
    
    gpu_config = DummyGPUConfig()
    GPU_CONFIG_AVAILABLE = False

logger = logging.getLogger(__name__)

# ===============================================================
# ğŸ¯ ìµœì  ìƒì„±ì ë² ì´ìŠ¤ í´ë˜ìŠ¤ (í†µí•© ê°œì„  ë²„ì „)
# ===============================================================

@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì • ë°ì´í„° í´ë˜ìŠ¤"""
    device: str = "auto"
    device_type: str = "auto"
    memory_gb: float = 16.0
    cpu_cores: int = 4
    is_m3_max: bool = False
    platform: str = "unknown"
    architecture: str = "unknown"
    torch_available: bool = False
    mps_available: bool = False
    cuda_available: bool = False

class OptimalConstructorBase(ABC):
    """
    ğŸ¯ ìµœì í™”ëœ ìƒì„±ì íŒ¨í„´ ë² ì´ìŠ¤ í´ë˜ìŠ¤
    - ë‹¨ìˆœí•¨ + í¸ì˜ì„± + í™•ì¥ì„± + ì¼ê´€ì„± í†µí•©
    - ì§€ëŠ¥ì  ìë™ ê°ì§€ì™€ ì™„ì „í•œ í™•ì¥ì„±
    """

    def __init__(
        self,
        device: Optional[str] = None,           # ğŸ”¥ í•µì‹¬: Noneìœ¼ë¡œ ìë™ ê°ì§€
        config: Optional[Dict[str, Any]] = None,
        **kwargs  # ğŸš€ í™•ì¥ì„±: ë¬´ì œí•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
    ):
        """
        âœ… ìµœì  ìƒì„±ì - ëª¨ë“  ì¥ì  ê²°í•©

        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (None=ìë™ê°ì§€, 'cpu', 'cuda', 'mps')
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            **kwargs: í™•ì¥ íŒŒë¼ë¯¸í„°ë“¤
                - device_type: str = "auto"
                - memory_gb: float = 16.0
                - is_m3_max: bool = False (ìë™ ê°ì§€)
                - optimization_enabled: bool = True
                - quality_level: str = "balanced"
                - enable_caching: bool = True
                - enable_parallel: bool = True
                - memory_optimization: bool = True
                - max_concurrent_requests: int = 4
                - timeout_seconds: int = 300
                - debug_mode: bool = False
                - ê¸°íƒ€ ëª¨ë“  í™•ì¥ íŒŒë¼ë¯¸í„°ë“¤...
        """
        # 1. ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì • ì´ˆê¸°í™”
        self.system_config = self._detect_system_config(device, **kwargs)
        
        # 2. ğŸ“‹ ê¸°ë³¸ ì†ì„± ì„¤ì •
        self.config = config or {}
        self.class_name = self.__class__.__name__
        self.logger = logging.getLogger(f"pipeline.{self.class_name}")
        
        # 3. ğŸ¯ í‘œì¤€ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ìµœì  ìƒì„±ì íŒ¨í„´)
        self.device = self.system_config.device
        self.device_type = kwargs.get('device_type', self.system_config.device_type)
        self.memory_gb = kwargs.get('memory_gb', self.system_config.memory_gb)
        self.is_m3_max = kwargs.get('is_m3_max', self.system_config.is_m3_max)
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')
        
        # 4. ğŸ”§ ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°ë“¤
        self.enable_caching = kwargs.get('enable_caching', True)
        self.enable_parallel = kwargs.get('enable_parallel', True)
        self.memory_optimization = kwargs.get('memory_optimization', True)
        self.max_concurrent_requests = kwargs.get('max_concurrent_requests', 4)
        self.timeout_seconds = kwargs.get('timeout_seconds', 300)
        self.debug_mode = kwargs.get('debug_mode', False)
        
        # 5. âš™ï¸ í´ë˜ìŠ¤ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„°ë¥¼ configì— ë³‘í•©
        self._merge_class_specific_config(kwargs)
        
        # 6. âœ… ìƒíƒœ ì´ˆê¸°í™”
        self.is_initialized = False
        self.creation_timestamp = self._get_timestamp()
        
        self.logger.info(f"ğŸ¯ {self.class_name} ìµœì  ìƒì„±ì ì´ˆê¸°í™” - "
                        f"ë””ë°”ì´ìŠ¤: {self.device}, í’ˆì§ˆ: {self.quality_level}")

    def _detect_system_config(self, preferred_device: Optional[str], **kwargs) -> SystemConfig:
        """ğŸ’¡ ì§€ëŠ¥ì  ì‹œìŠ¤í…œ ì„¤ì • ê°ì§€"""
        try:
            # ê¸°ë³¸ ì‹œìŠ¤í…œ ì •ë³´
            platform_name = platform.system()
            architecture = platform.machine()
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            try:
                import psutil
                memory_bytes = psutil.virtual_memory().total
                memory_gb = round(memory_bytes / (1024**3), 1)
                cpu_cores = psutil.cpu_count(logical=False) or 4
            except ImportError:
                memory_gb = kwargs.get('memory_gb', 16.0)
                cpu_cores = kwargs.get('cpu_cores', 4)
            
            # PyTorch ì§€ì› í™•ì¸
            torch_available = TORCH_AVAILABLE
            mps_available = torch_available and torch.backends.mps.is_available()
            cuda_available = torch_available and torch.cuda.is_available()
            
            # M3 Max ê°ì§€
            is_m3_max = self._detect_m3_max(platform_name, architecture, memory_gb, mps_available)
            
            # ìµœì  ë””ë°”ì´ìŠ¤ ê²°ì •
            device = self._determine_optimal_device(preferred_device, is_m3_max, mps_available, cuda_available)
            
            return SystemConfig(
                device=device,
                device_type=device,
                memory_gb=memory_gb,
                cpu_cores=cpu_cores,
                is_m3_max=is_m3_max,
                platform=platform_name,
                architecture=architecture,
                torch_available=torch_available,
                mps_available=mps_available,
                cuda_available=cuda_available
            )
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹œìŠ¤í…œ ê°ì§€ ì‹¤íŒ¨: {e}")
            return SystemConfig()  # ê¸°ë³¸ê°’ ì‚¬ìš©

    def _detect_m3_max(self, platform_name: str, architecture: str, memory_gb: float, mps_available: bool) -> bool:
        """ğŸ M3 Max ì •ë°€ ê°ì§€"""
        try:
            # macOS ARM64 í™•ì¸
            if platform_name != 'Darwin' or architecture != 'arm64':
                return False
            
            # ë©”ëª¨ë¦¬ í¬ê¸° í™•ì¸ (M3 MaxëŠ” ë³´í†µ 64GB ì´ìƒ)
            if memory_gb < 64:
                return False
            
            # MPS ë°±ì—”ë“œ ì§€ì› í™•ì¸
            if not mps_available:
                return False
            
            # ì¶”ê°€ ê²€ì¦: CPU ë¸Œëœë“œ í™•ì¸
            try:
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                    capture_output=True, text=True, timeout=5
                )
                if 'M3' in result.stdout:
                    return True
            except Exception:
                pass
            
            # ë©”ëª¨ë¦¬ í¬ê¸°ë¡œë§Œ íŒë‹¨ (128GB ëª¨ë¸)
            return memory_gb >= 118  # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬
            
        except Exception as e:
            self.logger.debug(f"M3 Max ê°ì§€ ì‹¤íŒ¨: {e}")
            return False

    def _determine_optimal_device(self, preferred: Optional[str], is_m3_max: bool, 
                                mps_available: bool, cuda_available: bool) -> str:
        """ğŸ’¡ ìµœì  ë””ë°”ì´ìŠ¤ ê²°ì •"""
        if preferred and preferred != "auto":
            return preferred
        
        # M3 Max ìš°ì„ 
        if is_m3_max and mps_available:
            return "mps"
        
        # CUDA ë‹¤ìŒ
        if cuda_available:
            return "cuda"
        
        # ì¼ë°˜ MPS
        if mps_available:
            return "mps"
        
        # CPU í´ë°±
        return "cpu"

    def _merge_class_specific_config(self, kwargs: Dict[str, Any]):
        """âš™ï¸ í´ë˜ìŠ¤ë³„ íŠ¹í™” ì„¤ì • ë³‘í•©"""
        # í‘œì¤€ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì œì™¸í•˜ê³  ëª¨ë“  kwargsë¥¼ configì— ë³‘í•©
        standard_params = {
            'device_type', 'memory_gb', 'is_m3_max', 'optimization_enabled', 
            'quality_level', 'enable_caching', 'enable_parallel', 'memory_optimization',
            'max_concurrent_requests', 'timeout_seconds', 'debug_mode', 'cpu_cores'
        }

        for key, value in kwargs.items():
            if key not in standard_params:
                self.config[key] = value

    def _get_timestamp(self) -> str:
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        from datetime import datetime
        return datetime.now().isoformat()

    def get_system_info(self) -> Dict[str, Any]:
        """ğŸ” ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        return {
            "class_name": self.class_name,
            "device": self.device,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "cpu_cores": self.system_config.cpu_cores,
            "is_m3_max": self.is_m3_max,
            "platform": self.system_config.platform,
            "architecture": self.system_config.architecture,
            "optimization_enabled": self.optimization_enabled,
            "quality_level": self.quality_level,
            "torch_available": self.system_config.torch_available,
            "mps_available": self.system_config.mps_available,
            "cuda_available": self.system_config.cuda_available,
            "initialized": self.is_initialized,
            "config_keys": list(self.config.keys()),
            "constructor_pattern": "optimal",
            "creation_timestamp": self.creation_timestamp
        }

    def get_performance_config(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì„¤ì • ì •ë³´ ë°˜í™˜"""
        return {
            "enable_caching": self.enable_caching,
            "enable_parallel": self.enable_parallel,
            "memory_optimization": self.memory_optimization,
            "max_concurrent_requests": self.max_concurrent_requests,
            "timeout_seconds": self.timeout_seconds,
            "optimization_enabled": self.optimization_enabled
        }

# ===============================================================
# ğŸ¯ ìµœì í™”ëœ PipelineConfig í´ë˜ìŠ¤ (ì™„ì „ ê°œì„  ë²„ì „)
# ===============================================================

class PipelineConfig(OptimalConstructorBase):
    """
    ğŸ¯ ìµœì  ìƒì„±ì íŒ¨í„´ì´ ì ìš©ëœ 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì„¤ì • ê´€ë¦¬ (ì™„ì „ ê°œì„  ë²„ì „)
    
    âœ… ìµœì  ìƒì„±ì íŒ¨í„´ ì¥ì :
    - ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ (Noneìœ¼ë¡œ ì„¤ì • ì‹œ)
    - **kwargsë¡œ ë¬´ì œí•œ í™•ì¥ì„±
    - í‘œì¤€ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° í†µì¼
    - ê¸°ì¡´ ì½”ë“œì™€ 100% í•˜ìœ„ í˜¸í™˜ì„±
    - íƒ€ì… ì•ˆì „ì„± ë° ê²€ì¦ ê¸°ëŠ¥
    
    ì™„ì „í•œ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸:
    1. Human Parsing (ì¸ì²´ íŒŒì‹±)
    2. Pose Estimation (í¬ì¦ˆ ì¶”ì •)
    3. Cloth Segmentation (ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜)
    4. Geometric Matching (ê¸°í•˜í•™ì  ë§¤ì¹­)
    5. Cloth Warping (ì˜· ì›Œí•‘)
    6. Virtual Fitting (ê°€ìƒ í”¼íŒ…)
    7. Post Processing (í›„ì²˜ë¦¬)
    8. Quality Assessment (í’ˆì§ˆ í‰ê°€)
    """
    
    def __init__(
        self, 
        device: Optional[str] = None,                    # ğŸ”¥ ìë™ ê°ì§€ë¡œ ë³€ê²½
        config: Optional[Dict[str, Any]] = None,
        config_path: Optional[str] = None,               # ê¸°ì¡´ í˜¸í™˜ì„±
        quality_level: str = "high",                     # ê¸°ì¡´ í˜¸í™˜ì„±
        **kwargs  # ğŸš€ ë¬´ì œí•œ í™•ì¥ì„±
    ):
        """
        âœ… ìµœì  ìƒì„±ì - PipelineConfig ì™„ì „ ê°œì„  ë²„ì „
        
        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (None=ìë™ê°ì§€, 'cpu', 'cuda', 'mps')
            config: ê¸°ë³¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ì¡´ í˜¸í™˜ì„±)
            quality_level: í’ˆì§ˆ ë ˆë²¨ (low, medium, high, ultra)
            **kwargs: í™•ì¥ íŒŒë¼ë¯¸í„°ë“¤
                # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
                - device_type: str = "auto"
                - memory_gb: float = 16.0
                - is_m3_max: bool = False (ìë™ ê°ì§€)
                - optimization_enabled: bool = True
                
                # ì„±ëŠ¥ íŒŒë¼ë¯¸í„°
                - enable_caching: bool = True
                - enable_parallel: bool = True
                - memory_optimization: bool = True
                - max_concurrent_requests: int = 4
                - timeout_seconds: int = 300
                
                # ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
                - enable_intermediate_saving: bool = False
                - auto_retry: bool = True
                - max_retries: int = 3
                - batch_processing: bool = True
                - dynamic_batching: bool = True
                
                # í’ˆì§ˆ íŒŒë¼ë¯¸í„°
                - super_resolution_enabled: bool = True
                - face_enhancement_enabled: bool = True
                - physics_simulation_enabled: bool = True
                - advanced_post_processing: bool = True
                
                # ê°œë°œì íŒŒë¼ë¯¸í„°
                - debug_mode: bool = False
                - save_debug_info: bool = False
                - benchmark_mode: bool = False
                - profiling_enabled: bool = False
                
                # ì‚¬ìš©ì ì •ì˜ íŒŒë¼ë¯¸í„°
                - custom_model_paths: Dict[str, str] = None
                - custom_step_configs: Dict[str, Dict] = None
                - experimental_features: Dict[str, bool] = None
        """
        
        # kwargsì—ì„œ í’ˆì§ˆ ë ˆë²¨ ë®ì–´ì“°ê¸° í™•ì¸
        if 'quality_level_override' in kwargs:
            quality_level = kwargs.pop('quality_level_override')
        
        # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™” (ìµœì  ìƒì„±ì íŒ¨í„´)
        super().__init__(
            device=device,
            config=config,
            quality_level=quality_level,
            **kwargs
        )
        
        # PipelineConfig íŠ¹í™” ì†ì„±ë“¤
        self.quality_level = quality_level
        self.config_path = config_path or kwargs.get('config_path')
        self.device_info = DEVICE_INFO if GPU_CONFIG_AVAILABLE else {"device": self.device}
        
        # í™•ì¥ íŒŒë¼ë¯¸í„°ë“¤ ì¶”ê°€ ì¶”ì¶œ
        self.enable_intermediate_saving = kwargs.get('enable_intermediate_saving', False)
        self.auto_retry = kwargs.get('auto_retry', True)
        self.max_retries = kwargs.get('max_retries', 3)
        self.batch_processing = kwargs.get('batch_processing', True)
        self.dynamic_batching = kwargs.get('dynamic_batching', True)
        
        self.super_resolution_enabled = kwargs.get('super_resolution_enabled', True)
        self.face_enhancement_enabled = kwargs.get('face_enhancement_enabled', True)
        self.physics_simulation_enabled = kwargs.get('physics_simulation_enabled', True)
        self.advanced_post_processing = kwargs.get('advanced_post_processing', True)
        
        self.save_debug_info = kwargs.get('save_debug_info', False)
        self.benchmark_mode = kwargs.get('benchmark_mode', False)
        self.profiling_enabled = kwargs.get('profiling_enabled', False)
        
        self.custom_model_paths = kwargs.get('custom_model_paths', {})
        self.custom_step_configs = kwargs.get('custom_step_configs', {})
        self.experimental_features = kwargs.get('experimental_features', {})
        
        # ê¸°ë³¸ ì„¤ì • ë¡œë“œ (ìµœì  ìƒì„±ì íŒ¨í„´ê³¼ í†µí•©)
        self.config = self._load_default_config_optimal()
        
        # ì™¸ë¶€ ì„¤ì • íŒŒì¼ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        if self.config_path and os.path.exists(self.config_path):
            self._load_external_config(self.config_path)
        
        # í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì˜¤ë²„ë¼ì´ë“œ
        self._apply_environment_overrides()
        
        # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì ìš©
        self._apply_device_optimizations()
        
        # í’ˆì§ˆ ë ˆë²¨ í”„ë¦¬ì…‹ ì ìš©
        self._apply_quality_preset(quality_level)
        
        # ì‚¬ìš©ì ì •ì˜ ì„¤ì • ì ìš©
        self._apply_custom_configurations()
        
        # ì´ˆê¸°í™” ì™„ë£Œ
        self.is_initialized = True
        
        logger.info(f"ğŸ”§ ìµœì  ìƒì„±ì íŒ¨í„´ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ")
        logger.info(f"   - í’ˆì§ˆ: {quality_level}, ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"   - ì‹œìŠ¤í…œ: {self.device_type}, ë©”ëª¨ë¦¬: {self.memory_gb}GB")
        logger.info(f"   - M3 Max: {'âœ…' if self.is_m3_max else 'âŒ'}")
        logger.info(f"   - í™•ì¥ ì„¤ì •: {len(self.config)} í•­ëª©")

    def _load_default_config_optimal(self) -> Dict[str, Any]:
        """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ê³¼ í†µí•©ëœ ì™„ì „í•œ ê¸°ë³¸ ì„¤ì • ë¡œë“œ"""
        
        return {
            # =======================================
            # ğŸ¯ ì „ì—­ íŒŒì´í”„ë¼ì¸ ì„¤ì •
            # =======================================
            "pipeline": {
                "name": "mycloset_virtual_fitting",
                "version": "4.1.0-optimal-enhanced",
                "constructor_pattern": "optimal",
                "quality_level": self.quality_level,
                "processing_mode": "complete",  # fast, balanced, complete
                "enable_optimization": self.optimization_enabled,
                "enable_caching": self.enable_caching,
                "enable_parallel": self.enable_parallel,
                "memory_optimization": self.memory_optimization,
                "max_concurrent_requests": self.max_concurrent_requests,
                "timeout_seconds": self.timeout_seconds,
                "enable_intermediate_saving": self.enable_intermediate_saving,
                "auto_retry": self.auto_retry,
                "max_retries": self.max_retries,
                "batch_processing": self.batch_processing,
                "dynamic_batching": self.dynamic_batching,
                "debug_mode": self.debug_mode,
                "save_debug_info": self.save_debug_info,
                "benchmark_mode": self.benchmark_mode,
                "profiling_enabled": self.profiling_enabled
            },
            
            # =======================================
            # ğŸ’» ì‹œìŠ¤í…œ ì •ë³´ (ìµœì  ìƒì„±ì íŒ¨í„´)
            # =======================================
            "system": {
                "device": self.device,
                "device_type": self.device_type,
                "memory_gb": self.memory_gb,
                "cpu_cores": self.system_config.cpu_cores,
                "is_m3_max": self.is_m3_max,
                "platform": self.system_config.platform,
                "architecture": self.system_config.architecture,
                "optimization_enabled": self.optimization_enabled,
                "torch_available": self.system_config.torch_available,
                "mps_available": self.system_config.mps_available,
                "cuda_available": self.system_config.cuda_available,
                "constructor_pattern": "optimal"
            },
            
            # =======================================
            # ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì •
            # =======================================
            "image": {
                "input_size": (512, 512),
                "output_size": (512, 512),
                "max_resolution": 1024,
                "supported_formats": ["jpg", "jpeg", "png", "webp", "bmp", "tiff"],
                "quality": 95,
                "color_space": "RGB",
                "bit_depth": 8,
                "preprocessing": {
                    "normalize": True,
                    "resize_mode": "lanczos",
                    "center_crop": True,
                    "background_removal": True,
                    "noise_reduction": True,
                    "contrast_enhancement": True
                },
                "postprocessing": {
                    "color_correction": True,
                    "gamma_correction": True,
                    "sharpening": True,
                    "edge_enhancement": True
                }
            },
            
            # =======================================
            # ğŸ”„ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ê°œë³„ ì„¤ì •
            # =======================================
            "steps": {
                # 1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± (Human Parsing)
                "human_parsing": {
                    "model_name": "graphonomy",
                    "model_path": "app/ai_pipeline/models/ai_models/graphonomy",
                    "fallback_models": ["detectron2", "deeplabv3"],
                    "num_classes": 20,
                    "confidence_threshold": 0.7,
                    "input_size": (512, 512),
                    "batch_size": 1 if self.device == 'cpu' else 2,
                    "cache_enabled": self.enable_caching,
                    "use_coreml": self.is_m3_max,
                    "enable_quantization": self.optimization_enabled,
                    "model_precision": "float16" if self.optimization_enabled else "float32",
                    "preprocessing": {
                        "normalize": True,
                        "mean": [0.485, 0.456, 0.406],
                        "std": [0.229, 0.224, 0.225],
                        "resize_method": "bilinear",
                        "padding_mode": "reflect"
                    },
                    "postprocessing": {
                        "morphology_cleanup": True,
                        "smooth_edges": True,
                        "fill_holes": True,
                        "remove_noise": True,
                        "edge_refinement": True
                    },
                    "validation": {
                        "min_body_area": 0.1,
                        "max_fragmentation": 0.3,
                        "require_torso": True
                    }
                },
                
                # 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • (Pose Estimation)
                "pose_estimation": {
                    "model_name": "mediapipe",
                    "fallback_models": ["openpose", "hrnet", "alphapose"],
                    "model_complexity": 2,
                    "min_detection_confidence": 0.5,
                    "min_tracking_confidence": 0.5,
                    "static_image_mode": True,
                    "enable_segmentation": True,
                    "smooth_landmarks": True,
                    "keypoints_format": "openpose_18",
                    "use_gpu": self.device != 'cpu',
                    "enable_face_landmarks": True,
                    "enable_hand_landmarks": False,
                    "pose_validation": {
                        "min_keypoints": 8,
                        "visibility_threshold": 0.3,
                        "symmetry_check": True,
                        "anatomical_constraints": True
                    },
                    "filtering": {
                        "temporal_smoothing": True,
                        "outlier_removal": True,
                        "confidence_weighting": True
                    }
                },
                
                # 3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Cloth Segmentation)
                "cloth_segmentation": {
                    "model_name": "u2net",
                    "model_path": "app/ai_pipeline/models/ai_models/u2net",
                    "fallback_models": ["rembg", "backgroundmattingv2", "modnet"],
                    "background_removal": True,
                    "edge_refinement": True,
                    "background_threshold": 0.5,
                    "trimap_generation": True,
                    "alpha_matting": True,
                    "multi_scale_processing": True,
                    "cloth_categories": {
                        "upper_body": ["shirt", "blouse", "jacket", "sweater", "hoodie"],
                        "lower_body": ["pants", "jeans", "skirt", "shorts"],
                        "full_body": ["dress", "jumpsuit", "overall"],
                        "accessories": ["hat", "scarf", "belt", "bag"]
                    },
                    "preprocessing": {
                        "contrast_enhancement": True,
                        "edge_detection": True,
                        "texture_analysis": True
                    },
                    "postprocessing": {
                        "morphology_enabled": True,
                        "gaussian_blur": True,
                        "edge_smoothing": True,
                        "noise_removal": True,
                        "hole_filling": True,
                        "boundary_refinement": True
                    },
                    "quality_assessment": {
                        "enable": True,
                        "min_quality": 0.6,
                        "auto_retry": self.auto_retry,
                        "quality_metrics": ["edge_sharpness", "mask_completeness", "boundary_smoothness"]
                    }
                },
                
                # 4ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­ (Geometric Matching)
                "geometric_matching": {
                    "algorithm": "tps_hybrid",  # tps, affine, tps_hybrid, thin_plate_spline
                    "num_control_points": 20,
                    "regularization": 0.001,
                    "matching_method": "hungarian",
                    "tps_points": 25,
                    "matching_threshold": 0.8,
                    "use_advanced_matching": True,
                    "multi_scale_matching": True,
                    "iterative_refinement": True,
                    "keypoint_extraction": {
                        "method": "contour_based",
                        "num_points": 50,
                        "adaptive_sampling": True,
                        "curvature_based": True,
                        "corner_detection": True
                    },
                    "feature_matching": {
                        "descriptor": "sift",  # sift, surf, orb, brief
                        "matcher": "flann",    # brute_force, flann
                        "cross_check": True,
                        "ratio_test": 0.7
                    },
                    "validation": {
                        "min_matched_points": 4,
                        "outlier_threshold": 2.0,
                        "quality_threshold": 0.7,
                        "geometric_consistency": True
                    },
                    "optimization": {
                        "max_iterations": 100,
                        "convergence_threshold": 1e-6,
                        "adaptive_learning": True
                    }
                },
                
                # 5ë‹¨ê³„: ì˜· ì›Œí•‘ (Cloth Warping)
                "cloth_warping": {
                    "physics_enabled": self.physics_simulation_enabled,
                    "fabric_simulation": True,
                    "deformation_strength": 0.8,
                    "wrinkle_simulation": True,
                    "gravity_simulation": False,
                    "wind_simulation": False,
                    "warping_method": "tps",  # tps, grid, mesh, physics
                    "optimization_level": "high",
                    "mesh_resolution": 64,
                    "simulation_steps": 50,
                    "convergence_threshold": 0.001,
                    "fabric_properties": {
                        "cotton": {
                            "stiffness": 0.6, 
                            "elasticity": 0.3, 
                            "thickness": 0.5,
                            "damping": 0.1,
                            "friction": 0.4
                        },
                        "denim": {
                            "stiffness": 0.9, 
                            "elasticity": 0.1, 
                            "thickness": 0.8,
                            "damping": 0.2,
                            "friction": 0.6
                        },
                        "silk": {
                            "stiffness": 0.2, 
                            "elasticity": 0.4, 
                            "thickness": 0.2,
                            "damping": 0.05,
                            "friction": 0.2
                        },
                        "wool": {
                            "stiffness": 0.7, 
                            "elasticity": 0.2, 
                            "thickness": 0.7,
                            "damping": 0.15,
                            "friction": 0.5
                        },
                        "polyester": {
                            "stiffness": 0.4, 
                            "elasticity": 0.6, 
                            "thickness": 0.3,
                            "damping": 0.08,
                            "friction": 0.3
                        }
                    },
                    "collision_detection": {
                        "enable": True,
                        "method": "continuous",
                        "resolution": "high"
                    },
                    "constraints": {
                        "anatomical": True,
                        "fabric_stretch": True,
                        "seam_preservation": True
                    }
                },
                
                # 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… (Virtual Fitting)
                "virtual_fitting": {
                    "model_name": "hr_viton",
                    "model_path": "app/ai_pipeline/models/ai_models/hr_viton",
                    "fallback_models": ["ootd", "viton_hd", "cagan"],
                    "guidance_scale": 7.5,
                    "num_inference_steps": 50,
                    "strength": 0.8,
                    "eta": 0.0,
                    "composition_method": "neural_blending",
                    "fallback_method": "traditional_blending",
                    "blending_method": "poisson",  # poisson, laplacian, multiband
                    "seamless_cloning": True,
                    "color_transfer": True,
                    "lighting_preservation": True,
                    "shadow_generation": True,
                    "reflection_handling": True,
                    "style_transfer": {
                        "enable": False,
                        "strength": 0.3,
                        "preserve_structure": True
                    },
                    "quality_enhancement": {
                        "color_matching": True,
                        "lighting_adjustment": True,
                        "texture_preservation": True,
                        "edge_smoothing": True,
                        "detail_enhancement": True,
                        "artifact_removal": True
                    },
                    "advanced_features": {
                        "multi_pose_support": True,
                        "dynamic_lighting": True,
                        "fabric_physics": self.physics_simulation_enabled,
                        "realistic_draping": True
                    }
                },
                
                # 7ë‹¨ê³„: í›„ì²˜ë¦¬ (Post Processing)
                "post_processing": {
                    "enable_super_resolution": self.super_resolution_enabled and self.optimization_enabled,
                    "enhance_faces": self.face_enhancement_enabled,
                    "color_correction": True,
                    "noise_reduction": True,
                    "detail_enhancement": self.advanced_post_processing,
                    "artifact_removal": True,
                    "super_resolution": {
                        "enabled": self.super_resolution_enabled and self.optimization_enabled,
                        "model": "real_esrgan",
                        "fallback_models": ["esrgan", "srcnn", "edsr"],
                        "scale_factor": 2,
                        "model_path": "app/ai_pipeline/models/ai_models/real_esrgan",
                        "tile_processing": True,
                        "overlap": 32,
                        "precision": "float16" if self.optimization_enabled else "float32"
                    },
                    "face_enhancement": {
                        "enabled": self.face_enhancement_enabled,
                        "model": "gfpgan",
                        "fallback_models": ["codeformer", "restoreformer"],
                        "strength": 0.8,
                        "model_path": "app/ai_pipeline/models/ai_models/gfpgan",
                        "detect_faces": True,
                        "preserve_identity": True,
                        "enhance_eyes": True,
                        "enhance_mouth": True
                    },
                    "color_correction": {
                        "enabled": True,
                        "method": "histogram_matching",
                        "strength": 0.6,
                        "preserve_skin_tone": True,
                        "white_balance": True,
                        "exposure_correction": True
                    },
                    "noise_reduction": {
                        "enabled": True,
                        "method": "bilateral_filter",
                        "strength": 0.7,
                        "preserve_edges": True,
                        "adaptive": True
                    },
                    "edge_enhancement": {
                        "enabled": self.advanced_post_processing,
                        "method": "unsharp_mask",
                        "strength": 0.5,
                        "radius": 1.0,
                        "threshold": 0.05
                    },
                    "detail_enhancement": {
                        "enabled": self.advanced_post_processing,
                        "texture_sharpening": True,
                        "clarity_boost": True,
                        "micro_contrast": True
                    },
                    "final_optimization": {
                        "compression_quality": 95,
                        "format_optimization": True,
                        "metadata_preservation": False
                    }
                },
                
                # 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (Quality Assessment)
                "quality_assessment": {
                    "enabled": True,
                    "comprehensive_analysis": True,
                    "generate_suggestions": True,
                    "save_report": self.save_debug_info,
                    "metrics": {
                        "perceptual": ["ssim", "lpips", "ms_ssim"],
                        "technical": ["psnr", "mse", "mae"],
                        "aesthetic": ["brisque", "niqe"],
                        "semantic": ["face_quality", "cloth_fit", "realism"]
                    },
                    "thresholds": {
                        "ssim_min": 0.7,
                        "lpips_max": 0.3,
                        "psnr_min": 20.0,
                        "brisque_max": 50.0,
                        "overall_min": 0.7
                    },
                    "detailed_analysis": {
                        "face_region": True,
                        "cloth_region": True,
                        "background_region": True,
                        "edge_quality": True,
                        "color_consistency": True,
                        "lighting_coherence": True
                    },
                    "benchmarking": {
                        "enabled": self.benchmark_mode,
                        "reference_dataset": None,
                        "save_results": self.benchmark_mode,
                        "performance_tracking": True
                    },
                    "auto_improvement": {
                        "enabled": True,
                        "retry_on_low_quality": self.auto_retry,
                        "parameter_adjustment": True,
                        "fallback_models": True
                    }
                }
            },
            
            # =======================================
            # ğŸ“ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
            # =======================================
            "model_paths": {
                "base_dir": "app/ai_pipeline/models/ai_models",
                "cache_dir": "app/ai_pipeline/cache",
                "checkpoints": {
                    "graphonomy": "graphonomy/checkpoints/graphonomy.pth",
                    "hr_viton": "hr_viton/checkpoints/hr_viton.pth",
                    "u2net": "u2net/checkpoints/u2net.pth",
                    "real_esrgan": "real_esrgan/checkpoints/RealESRGAN_x4plus.pth",
                    "gfpgan": "gfpgan/checkpoints/GFPGANv1.4.pth",
                    "openpose": "openpose/checkpoints/pose_iter_440000.caffemodel",
                    "mediapipe": "mediapipe/models/pose_landmarker.task",
                    "ootd": "ootd/checkpoints/ootd_diffusion.pth",
                    "viton_hd": "viton_hd/checkpoints/viton_hd.pth"
                },
                "custom_paths": self.custom_model_paths
            },
            
            # =======================================
            # âš¡ ì„±ëŠ¥ ìµœì í™” ì„¤ì •
            # =======================================
            "optimization": {
                "device": self.device,
                "device_type": self.device_type,
                "memory_gb": self.memory_gb,
                "is_m3_max": self.is_m3_max,
                "optimization_enabled": self.optimization_enabled,
                "mixed_precision": self.optimization_enabled,
                "gradient_checkpointing": False,
                "memory_efficient_attention": True,
                "compile_models": False,  # PyTorch 2.0 compile
                "constructor_pattern": "optimal",
                "model_offloading": {
                    "enabled": True,
                    "strategy": "lru",  # lru, priority, manual
                    "keep_active": 2,
                    "offload_to": "cpu"
                },
                "batch_processing": {
                    "enabled": self.batch_processing,
                    "max_batch_size": 4 if self.device != 'cpu' else 1,
                    "dynamic_batching": self.dynamic_batching,
                    "batch_timeout": 5.0
                },
                "caching": {
                    "enabled": self.enable_caching,
                    "ttl": 3600,  # 1ì‹œê°„
                    "max_size": "2GB",
                    "cache_intermediate": self.enable_intermediate_saving,
                    "compression": True,
                    "cache_strategy": "lru"
                },
                "parallel_processing": {
                    "enabled": self.enable_parallel,
                    "max_workers": min(4, self.system_config.cpu_cores),
                    "thread_pool": True,
                    "async_pipeline": True
                }
            },
            
            # =======================================
            # ğŸ’¾ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •
            # =======================================
            "memory": {
                "max_memory_usage": f"{min(80, int(self.memory_gb * 0.8))}%" if self.memory_gb else "80%",
                "memory_gb": self.memory_gb,
                "cleanup_interval": 300,  # 5ë¶„
                "aggressive_cleanup": self.memory_optimization,
                "optimization": self.memory_optimization,
                "monitoring": {
                    "enabled": True,
                    "warning_threshold": 0.85,
                    "critical_threshold": 0.95,
                    "auto_cleanup": True
                },
                "model_offloading": {
                    "enabled": True,
                    "offload_to": "cpu",
                    "keep_in_memory": ["human_parsing", "pose_estimation"] if self.memory_gb < 32 else [],
                    "intelligent_preloading": True
                },
                "garbage_collection": {
                    "aggressive": self.memory_optimization,
                    "frequency": "auto",
                    "force_after_step": True
                }
            },
            
            # =======================================
            # ğŸ“Š ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
            # =======================================
            "logging": {
                "level": "DEBUG" if self.debug_mode else "INFO",
                "detailed_timing": True,
                "performance_metrics": True,
                "save_intermediate": self.enable_intermediate_saving,
                "debug_mode": self.debug_mode,
                "save_debug_info": self.save_debug_info,
                "constructor_pattern": "optimal",
                "profiling": {
                    "enabled": self.profiling_enabled,
                    "memory_profiling": self.profiling_enabled,
                    "time_profiling": self.profiling_enabled,
                    "gpu_profiling": self.profiling_enabled and self.device != 'cpu'
                },
                "monitoring": {
                    "system_metrics": True,
                    "pipeline_metrics": True,
                    "error_tracking": True,
                    "performance_alerts": True
                }
            },
            
            # =======================================
            # ğŸ§ª ì‹¤í—˜ì  ê¸°ëŠ¥
            # =======================================
            "experimental": {
                "features": self.experimental_features,
                "neural_style_transfer": self.experimental_features.get('neural_style_transfer', False),
                "advanced_physics": self.experimental_features.get('advanced_physics', False),
                "ai_fabric_detection": self.experimental_features.get('ai_fabric_detection', False),
                "real_time_processing": self.experimental_features.get('real_time_processing', False),
                "multi_person_support": self.experimental_features.get('multi_person_support', False),
                "3d_pose_estimation": self.experimental_features.get('3d_pose_estimation', False),
                "advanced_lighting": self.experimental_features.get('advanced_lighting', False)
            }
        }