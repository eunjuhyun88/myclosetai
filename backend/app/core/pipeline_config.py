"""
ğŸ¯ ìµœì í™”ëœ AI íŒŒì´í”„ë¼ì¸ ì„¤ì • ê´€ë¦¬ - ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš©
8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ ì„¤ì •ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
ê¸°ì¡´ app/ êµ¬ì¡°ì™€ ì™„ì „íˆ í˜¸í™˜ë˜ë©°, M3 Max ìµœì í™”ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

âœ… ìµœì  ìƒì„±ì íŒ¨í„´ ì ìš©:
- ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
- **kwargsë¡œ ë¬´ì œí•œ í™•ì¥ì„±
- í‘œì¤€ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° í†µì¼
- í•˜ìœ„ í˜¸í™˜ì„± 100% ë³´ì¥
"""

import os
import json
import logging
import platform
import subprocess
from typing import Dict, Any, Optional, List, Union
from pathlib import Path
from functools import lru_cache
from abc import ABC, abstractmethod
import torch

# gpu_configëŠ” ì‹¤ì œ íŒŒì¼ì—ì„œ importí•˜ë˜, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
try:
    from .gpu_config import gpu_config, DEVICE, DEVICE_INFO
except ImportError:
    logger = logging.getLogger(__name__)
    logger.warning("gpu_config import ì‹¤íŒ¨ - ê¸°ë³¸ê°’ ì‚¬ìš©")
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    DEVICE = "mps" if torch.backends.mps.is_available() else ("cuda" if torch.cuda.is_available() else "cpu")
    DEVICE_INFO = {"device": DEVICE, "available": True}
    
    class DummyGPUConfig:
        def __init__(self):
            self.device = DEVICE
            self.device_type = "auto"
    
    gpu_config = DummyGPUConfig()

logger = logging.getLogger(__name__)

# ===============================================================
# ğŸ¯ ìµœì  ìƒì„±ì ë² ì´ìŠ¤ í´ë˜ìŠ¤ (OptimalStepConstructor í†µí•©)
# ===============================================================

class OptimalConstructorBase(ABC):
    """
    ğŸ¯ ìµœì í™”ëœ ìƒì„±ì íŒ¨í„´ ë² ì´ìŠ¤ í´ë˜ìŠ¤
    - ë‹¨ìˆœí•¨ + í¸ì˜ì„± + í™•ì¥ì„± + ì¼ê´€ì„±
    """

    def __init__(
        self,
        device: Optional[str] = None,  # ğŸ”¥ í•µì‹¬ ê°œì„ : Noneìœ¼ë¡œ ìë™ ê°ì§€
        config: Optional[Dict[str, Any]] = None,
        **kwargs  # ğŸš€ í™•ì¥ì„±: ë¬´ì œí•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
    ):
        """
        âœ… ìµœì  ìƒì„±ì - ëª¨ë“  ì¥ì  ê²°í•©

        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (None=ìë™ê°ì§€, 'cpu', 'cuda', 'mps')
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            **kwargs: í™•ì¥ íŒŒë¼ë¯¸í„°ë“¤
                - device_type: str = "auto"
                - memory_gb: float = 16.0
                - is_m3_max: bool = False
                - optimization_enabled: bool = True
                - quality_level: str = "balanced"
                - ê¸°íƒ€ íŠ¹í™” íŒŒë¼ë¯¸í„°ë“¤...
        """
        # 1. ğŸ’¡ ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._auto_detect_device(device)

        # 2. ğŸ“‹ ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.class_name = self.__class__.__name__
        self.logger = logging.getLogger(f"pipeline.{self.class_name}")

        # 3. ğŸ”§ í‘œì¤€ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ì¼ê´€ì„±)
        self.device_type = kwargs.get('device_type', 'auto')
        self.memory_gb = kwargs.get('memory_gb', 16.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')

        # 4. âš™ï¸ í´ë˜ìŠ¤ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„°ë¥¼ configì— ë³‘í•©
        self._merge_class_specific_config(kwargs)

        # 5. âœ… ìƒíƒœ ì´ˆê¸°í™”
        self.is_initialized = False

        self.logger.info(f"ğŸ¯ {self.class_name} ìµœì  ìƒì„±ì ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")

    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ğŸ’¡ ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device:
            return preferred_device

        try:
            import torch
            if torch.backends.mps.is_available():
                return 'mps'  # M3 Max ìš°ì„ 
            elif torch.cuda.is_available():
                return 'cuda'  # NVIDIA GPU
            else:
                return 'cpu'  # í´ë°±
        except ImportError:
            return 'cpu'

    def _detect_m3_max(self) -> bool:
        """ğŸ M3 Max ì¹© ìë™ ê°ì§€"""
        try:
            if platform.system() == 'Darwin':  # macOS
                # M3 Max ê°ì§€ ë¡œì§
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False

    def _merge_class_specific_config(self, kwargs: Dict[str, Any]):
        """âš™ï¸ í´ë˜ìŠ¤ë³„ íŠ¹í™” ì„¤ì • ë³‘í•©"""
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì œì™¸í•˜ê³  ëª¨ë“  kwargsë¥¼ configì— ë³‘í•©
        system_params = {
            'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level'
        }

        for key, value in kwargs.items():
            if key not in system_params:
                self.config[key] = value

    def get_system_info(self) -> Dict[str, Any]:
        """ğŸ” ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        return {
            "class_name": self.class_name,
            "device": self.device,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_enabled": self.optimization_enabled,
            "quality_level": self.quality_level,
            "initialized": self.is_initialized,
            "config_keys": list(self.config.keys()),
            "constructor_pattern": "optimal"
        }

# ===============================================================
# ğŸ¯ ìµœì í™”ëœ PipelineConfig í´ë˜ìŠ¤
# ===============================================================

class PipelineConfig(OptimalConstructorBase):
    """
    ğŸ¯ ìµœì  ìƒì„±ì íŒ¨í„´ì´ ì ìš©ëœ 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì„¤ì • ê´€ë¦¬
    
    âœ… ìµœì  ìƒì„±ì íŒ¨í„´ ì¥ì :
    - ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ (Noneìœ¼ë¡œ ì„¤ì • ì‹œ)
    - **kwargsë¡œ ë¬´ì œí•œ í™•ì¥ì„±
    - í‘œì¤€ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° í†µì¼
    - ê¸°ì¡´ ì½”ë“œì™€ 100% í•˜ìœ„ í˜¸í™˜ì„±
    
    ê¸°ì¡´ app/ êµ¬ì¡°ì— ë§ì¶° ì„¤ê³„ëœ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì‹œìŠ¤í…œ
    - 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ ì§€ì›
    - M3 Max MPS ìµœì í™”
    - ë™ì  ì„¤ì • ë¡œë”©
    - í™˜ê²½ë³„ ì„¤ì • ë¶„ë¦¬
    """
    
    def __init__(
        self, 
        device: Optional[str] = None,           # ğŸ”¥ ìë™ ê°ì§€ë¡œ ë³€ê²½
        config: Optional[Dict[str, Any]] = None,
        config_path: Optional[str] = None,      # ê¸°ì¡´ í˜¸í™˜ì„±
        quality_level: str = "high",            # ê¸°ì¡´ í˜¸í™˜ì„±
        **kwargs  # ğŸš€ í™•ì¥ì„±: device_type, memory_gb, is_m3_max, optimization_enabled ë“±
    ):
        """
        âœ… ìµœì  ìƒì„±ì - PipelineConfigìš©
        
        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (None=ìë™ê°ì§€, 'cpu', 'cuda', 'mps')
            config: ê¸°ë³¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ì¡´ í˜¸í™˜ì„±)
            quality_level: í’ˆì§ˆ ë ˆë²¨ (ê¸°ì¡´ í˜¸í™˜ì„±)
            **kwargs: í™•ì¥ íŒŒë¼ë¯¸í„°ë“¤
                - device_type: str = "auto"
                - memory_gb: float = 16.0
                - is_m3_max: bool = False (ìë™ ê°ì§€)
                - optimization_enabled: bool = True
                - quality_level_override: str = None (í’ˆì§ˆ ë ˆë²¨ ë®ì–´ì“°ê¸°)
                - enable_caching: bool = True
                - enable_parallel: bool = True
                - memory_optimization: bool = True
                - max_concurrent_requests: int = 4
                - timeout_seconds: int = 300
                - enable_intermediate_saving: bool = False
                - auto_retry: bool = True
                - max_retries: int = 3
        """
        
        # kwargsì—ì„œ í’ˆì§ˆ ë ˆë²¨ ë®ì–´ì“°ê¸° í™•ì¸
        if 'quality_level_override' in kwargs:
            quality_level = kwargs.pop('quality_level_override')
        
        # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™” (ìµœì  ìƒì„±ì íŒ¨í„´)
        super().__init__(
            device=device,
            config=config,
            quality_level=quality_level,  # kwargsì— í¬í•¨
            **kwargs
        )
        
        # PipelineConfig íŠ¹í™” ì†ì„±ë“¤
        self.quality_level = quality_level  # ëª…ì‹œì  ì„¤ì •
        self.config_path = config_path or kwargs.get('config_path')
        self.device_info = DEVICE_INFO
        
        # ê¸°ë³¸ ì„¤ì • ë¡œë“œ (ìµœì  ìƒì„±ì íŒ¨í„´ê³¼ í†µí•©)
        self.config = self._load_default_config_optimal()
        
        # ì™¸ë¶€ ì„¤ì • íŒŒì¼ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        if self.config_path and os.path.exists(self.config_path):
            self._load_external_config(self.config_path)
        
        # í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì˜¤ë²„ë¼ì´ë“œ
        self._apply_environment_overrides()
        
        # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì ìš©
        self._apply_device_optimizations()
        
        # ì´ˆê¸°í™” ì™„ë£Œ
        self.is_initialized = True
        
        logger.info(f"ğŸ”§ ìµœì  ìƒì„±ì íŒ¨í„´ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ - í’ˆì§ˆ: {quality_level}, ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"ğŸ’» ì‹œìŠ¤í…œ: {self.device_type}, ë©”ëª¨ë¦¬: {self.memory_gb}GB, M3 Max: {'âœ…' if self.is_m3_max else 'âŒ'}")

    def _load_default_config_optimal(self) -> Dict[str, Any]:
        """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ê³¼ í†µí•©ëœ ê¸°ë³¸ ì„¤ì • ë¡œë“œ"""
        
        # kwargsì—ì„œ ì„¤ì •ëœ íŒŒë¼ë¯¸í„°ë“¤ í™œìš©
        enable_caching = self.config.get('enable_caching', True)
        enable_parallel = self.config.get('enable_parallel', True)
        memory_optimization = self.config.get('memory_optimization', True)
        max_concurrent_requests = self.config.get('max_concurrent_requests', 4)
        timeout_seconds = self.config.get('timeout_seconds', 300)
        enable_intermediate_saving = self.config.get('enable_intermediate_saving', False)
        auto_retry = self.config.get('auto_retry', True)
        max_retries = self.config.get('max_retries', 3)
        
        return {
            # ì „ì—­ íŒŒì´í”„ë¼ì¸ ì„¤ì • (ìµœì  ìƒì„±ì íŒ¨í„´ í†µí•©)
            "pipeline": {
                "name": "mycloset_virtual_fitting",
                "version": "4.0.0-optimal",
                "constructor_pattern": "optimal",
                "quality_level": self.quality_level,
                "processing_mode": "complete",  # fast, balanced, complete
                "enable_optimization": self.optimization_enabled,
                "enable_caching": enable_caching,
                "enable_parallel": enable_parallel,
                "memory_optimization": memory_optimization,
                "max_concurrent_requests": max_concurrent_requests,
                "timeout_seconds": timeout_seconds,
                "enable_intermediate_saving": enable_intermediate_saving,
                "auto_retry": auto_retry,
                "max_retries": max_retries
            },
            
            # ì‹œìŠ¤í…œ ì •ë³´ (ìµœì  ìƒì„±ì íŒ¨í„´)
            "system": {
                "device": self.device,
                "device_type": self.device_type,
                "memory_gb": self.memory_gb,
                "is_m3_max": self.is_m3_max,
                "optimization_enabled": self.optimization_enabled,
                "constructor_pattern": "optimal"
            },
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì •
            "image": {
                "input_size": (512, 512),
                "output_size": (512, 512),
                "max_resolution": 1024,
                "supported_formats": ["jpg", "jpeg", "png", "webp"],
                "quality": 95,
                "preprocessing": {
                    "normalize": True,
                    "resize_mode": "lanczos",
                    "center_crop": True,
                    "background_removal": True
                }
            },
            
            # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ê°œë³„ ì„¤ì •
            "steps": {
                # 1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± (Human Parsing)
                "human_parsing": {
                    "model_name": "graphonomy",
                    "model_path": "app/ai_pipeline/models/ai_models/graphonomy",
                    "num_classes": 20,
                    "confidence_threshold": 0.7,
                    "input_size": (512, 512),
                    "batch_size": 1,
                    "cache_enabled": enable_caching,
                    "use_coreml": self.is_m3_max,
                    "enable_quantization": self.optimization_enabled,
                    "preprocessing": {
                        "normalize": True,
                        "mean": [0.485, 0.456, 0.406],
                        "std": [0.229, 0.224, 0.225]
                    },
                    "postprocessing": {
                        "morphology_cleanup": True,
                        "smooth_edges": True,
                        "fill_holes": True
                    }
                },
                
                # 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • (Pose Estimation)
                "pose_estimation": {
                    "model_name": "mediapipe",
                    "model_complexity": 2,
                    "min_detection_confidence": 0.5,
                    "min_tracking_confidence": 0.5,
                    "static_image_mode": True,
                    "enable_segmentation": True,
                    "smooth_landmarks": True,
                    "keypoints_format": "openpose_18",
                    "fallback_models": ["openpose", "hrnet"],
                    "use_gpu": self.device != 'cpu',
                    "pose_validation": {
                        "min_keypoints": 8,
                        "visibility_threshold": 0.3,
                        "symmetry_check": True
                    }
                },
                
                # 3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Cloth Segmentation)
                "cloth_segmentation": {
                    "model_name": "u2net",
                    "model_path": "app/ai_pipeline/models/ai_models/u2net",
                    "fallback_method": "rembg",
                    "background_removal": True,
                    "edge_refinement": True,
                    "background_threshold": 0.5,
                    "post_process": True,
                    "refine_edges": True,
                    "post_processing": {
                        "morphology_enabled": True,
                        "gaussian_blur": True,
                        "edge_smoothing": True,
                        "noise_removal": True
                    },
                    "quality_assessment": {
                        "enable": True,
                        "min_quality": 0.6,
                        "auto_retry": auto_retry
                    }
                },
                
                # 4ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­ (Geometric Matching)
                "geometric_matching": {
                    "algorithm": "tps_hybrid",  # tps, affine, tps_hybrid
                    "num_control_points": 20,
                    "regularization": 0.001,
                    "matching_method": "hungarian",
                    "tps_points": 25,
                    "matching_threshold": 0.8,
                    "use_advanced_matching": True,
                    "keypoint_extraction": {
                        "method": "contour_based",
                        "num_points": 50,
                        "adaptive_sampling": True
                    },
                    "validation": {
                        "min_matched_points": 4,
                        "outlier_threshold": 2.0,
                        "quality_threshold": 0.7
                    }
                },
                
                # 5ë‹¨ê³„: ì˜· ì›Œí•‘ (Cloth Warping)
                "cloth_warping": {
                    "physics_enabled": True,
                    "fabric_simulation": True,
                    "deformation_strength": 0.8,
                    "wrinkle_simulation": True,
                    "warping_method": "tps",
                    "optimization_level": "high",
                    "fabric_properties": {
                        "cotton": {"stiffness": 0.6, "elasticity": 0.3, "thickness": 0.5},
                        "denim": {"stiffness": 0.9, "elasticity": 0.1, "thickness": 0.8},
                        "silk": {"stiffness": 0.2, "elasticity": 0.4, "thickness": 0.2},
                        "wool": {"stiffness": 0.7, "elasticity": 0.2, "thickness": 0.7},
                        "polyester": {"stiffness": 0.4, "elasticity": 0.6, "thickness": 0.3}
                    },
                    "simulation_steps": 50,
                    "convergence_threshold": 0.001
                },
                
                # 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… (Virtual Fitting)
                "virtual_fitting": {
                    "model_name": "hr_viton",
                    "model_path": "app/ai_pipeline/models/ai_models/hr_viton",
                    "guidance_scale": 7.5,
                    "num_inference_steps": 50,
                    "strength": 0.8,
                    "eta": 0.0,
                    "composition_method": "neural_blending",
                    "fallback_method": "traditional_blending",
                    "blending_method": "poisson",
                    "seamless_cloning": True,
                    "color_transfer": True,
                    "quality_enhancement": {
                        "color_matching": True,
                        "lighting_adjustment": True,
                        "texture_preservation": True,
                        "edge_smoothing": True
                    }
                },
                
                # 7ë‹¨ê³„: í›„ì²˜ë¦¬ (Post Processing)
                "post_processing": {
                    "enable_super_resolution": self.optimization_enabled,
                    "enhance_faces": True,
                    "color_correction": True,
                    "noise_reduction": True,
                    "super_resolution": {
                        "enabled": self.optimization_enabled,
                        "model": "real_esrgan",
                        "scale_factor": 2,
                        "model_path": "app/ai_pipeline/models/ai_models/real_esrgan"
                    },
                    "face_enhancement": {
                        "enabled": True,
                        "model": "gfpgan",
                        "strength": 0.8,
                        "model_path": "app/ai_pipeline/models/ai_models/gfpgan"
                    },
                    "color_correction": {
                        "enabled": True,
                        "method": "histogram_matching",
                        "strength": 0.6
                    },
                    "noise_reduction": {
                        "enabled": True,
                        "method": "bilateral_filter",
                        "strength": 0.7
                    },
                    "edge_enhancement": {
                        "enabled": True,
                        "method": "unsharp_mask",
                        "strength": 0.5
                    }
                },
                
                # 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (Quality Assessment)
                "quality_assessment": {
                    "metrics": ["ssim", "lpips", "fid", "is"],
                    "quality_threshold": 0.7,
                    "comprehensive_analysis": True,
                    "generate_suggestions": True,
                    "enable_detailed_analysis": True,
                    "perceptual_metrics": True,
                    "technical_metrics": True,
                    "benchmarking": {
                        "enabled": False,
                        "reference_dataset": None,
                        "save_results": False
                    }
                }
            },
            
            # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
            "model_paths": {
                "base_dir": "app/ai_pipeline/models/ai_models",
                "cache_dir": "app/ai_pipeline/cache",
                "checkpoints": {
                    "graphonomy": "graphonomy/checkpoints/graphonomy.pth",
                    "hr_viton": "hr_viton/checkpoints/hr_viton.pth",
                    "u2net": "u2net/checkpoints/u2net.pth",
                    "real_esrgan": "real_esrgan/checkpoints/RealESRGAN_x4plus.pth",
                    "gfpgan": "gfpgan/checkpoints/GFPGANv1.4.pth",
                    "openpose": "openpose/checkpoints/pose_iter_440000.caffemodel"
                }
            },
            
            # ì„±ëŠ¥ ìµœì í™” ì„¤ì • (ìµœì  ìƒì„±ì íŒ¨í„´ í†µí•©)
            "optimization": {
                "device": self.device,
                "device_type": self.device_type,
                "memory_gb": self.memory_gb,
                "is_m3_max": self.is_m3_max,
                "optimization_enabled": self.optimization_enabled,
                "mixed_precision": self.optimization_enabled,
                "gradient_checkpointing": False,
                "memory_efficient_attention": True,
                "compile_models": False,  # PyTorch 2.0 compile
                "constructor_pattern": "optimal",
                "batch_processing": {
                    "enabled": enable_parallel,
                    "max_batch_size": 4 if self.device != 'cpu' else 1,
                    "dynamic_batching": self.optimization_enabled
                },
                "caching": {
                    "enabled": enable_caching,
                    "ttl": 3600,  # 1ì‹œê°„
                    "max_size": "2GB",
                    "cache_intermediate": enable_intermediate_saving
                }
            },
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì • (ìµœì  ìƒì„±ì íŒ¨í„´ í†µí•©)
            "memory": {
                "max_memory_usage": f"{min(80, int(self.memory_gb * 0.8))}%",
                "memory_gb": self.memory_gb,
                "cleanup_interval": 300,  # 5ë¶„
                "aggressive_cleanup": False,
                "optimization": memory_optimization,
                "model_offloading": {
                    "enabled": True,
                    "offload_to": "cpu",
                    "keep_in_memory": ["human_parsing", "pose_estimation"]
                }
            },
            
            # ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
            "logging": {
                "level": "INFO",
                "detailed_timing": True,
                "performance_metrics": True,
                "save_intermediate": enable_intermediate_saving,
                "debug_mode": False,
                "constructor_pattern": "optimal"
            }
        }
    
    def _load_external_config(self, config_path: str):
        """ì™¸ë¶€ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                external_config = json.load(f)
            
            # ë”¥ ë¨¸ì§€
            self._deep_merge(self.config, external_config)
            logger.info(f"âœ… ì™¸ë¶€ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {config_path}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì™¸ë¶€ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _apply_environment_overrides(self):
        """í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ"""
        
        # í’ˆì§ˆ ë ˆë²¨
        quality = os.getenv("PIPELINE_QUALITY_LEVEL", self.quality_level)
        if quality != self.quality_level:
            self.quality_level = quality
            self.config["pipeline"]["quality_level"] = quality
            self._apply_quality_preset(quality)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        device_override = os.getenv("PIPELINE_DEVICE")
        if device_override and device_override != self.device:
            self.device = device_override
            self.config["optimization"]["device"] = device_override
        
        # ë©”ëª¨ë¦¬ ì œí•œ
        memory_limit = os.getenv("PIPELINE_MEMORY_LIMIT")
        if memory_limit:
            self.config["memory"]["max_memory_usage"] = memory_limit
        
        # ë™ì‹œ ì²˜ë¦¬ ìˆ˜
        max_concurrent = os.getenv("PIPELINE_MAX_CONCURRENT")
        if max_concurrent:
            try:
                self.config["pipeline"]["max_concurrent_requests"] = int(max_concurrent)
            except ValueError:
                pass
        
        # ë””ë²„ê·¸ ëª¨ë“œ
        debug_mode = os.getenv("PIPELINE_DEBUG", "false").lower() == "true"
        self.config["logging"]["debug_mode"] = debug_mode
        if debug_mode:
            self.config["logging"]["level"] = "DEBUG"
            self.config["logging"]["save_intermediate"] = True
        
        # ìµœì í™” í™œì„±í™”/ë¹„í™œì„±í™”
        optimization_override = os.getenv("PIPELINE_OPTIMIZATION")
        if optimization_override:
            enable_opt = optimization_override.lower() == "true"
            self.optimization_enabled = enable_opt
            self.config["optimization"]["optimization_enabled"] = enable_opt
    
    def _apply_device_optimizations(self):
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì ìš©"""
        
        if self.device == "mps":
            # M3 Max MPS ìµœì í™”
            self.config["optimization"].update({
                "mixed_precision": self.optimization_enabled,
                "memory_efficient_attention": True,
                "compile_models": False,  # MPSì—ì„œëŠ” ì»´íŒŒì¼ ë¹„í™œì„±í™”
                "batch_processing": {
                    "enabled": True,
                    "max_batch_size": 2,  # MPS ë©”ëª¨ë¦¬ ì œí•œ
                    "dynamic_batching": False
                }
            })
            
            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            if self.quality_level in ["fast", "balanced"]:
                self.config["image"]["input_size"] = (512, 512)
                self.config["image"]["max_resolution"] = 1024
            
        elif self.device == "cuda":
            # CUDA ìµœì í™”
            self.config["optimization"].update({
                "mixed_precision": self.optimization_enabled,
                "gradient_checkpointing": True,
                "compile_models": self.optimization_enabled,
                "batch_processing": {
                    "enabled": True,
                    "max_batch_size": 8,
                    "dynamic_batching": True
                }
            })
            
        else:
            # CPU ìµœì í™”
            self.config["optimization"].update({
                "mixed_precision": False,
                "compile_models": False,
                "batch_processing": {
                    "enabled": False,
                    "max_batch_size": 1
                }
            })
            
            # CPUì—ì„œëŠ” ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
            self.config["steps"]["virtual_fitting"]["num_inference_steps"] = 20
            self.config["steps"]["post_processing"]["super_resolution"]["enabled"] = False
    
    def _apply_quality_preset(self, quality_level: str):
        """í’ˆì§ˆ ë ˆë²¨ì— ë”°ë¥¸ í”„ë¦¬ì…‹ ì ìš©"""
        
        quality_presets = {
            "fast": {
                "image_size": (256, 256),
                "inference_steps": 20,
                "super_resolution": False,
                "face_enhancement": False,
                "physics_simulation": False,
                "timeout": 60
            },
            "balanced": {
                "image_size": (512, 512),
                "inference_steps": 30,
                "super_resolution": True,
                "face_enhancement": False,
                "physics_simulation": True,
                "timeout": 120
            },
            "high": {
                "image_size": (512, 512),
                "inference_steps": 50,
                "super_resolution": True,
                "face_enhancement": True,
                "physics_simulation": True,
                "timeout": 300
            },
            "ultra": {
                "image_size": (1024, 1024),
                "inference_steps": 100,
                "super_resolution": True,
                "face_enhancement": True,
                "physics_simulation": True,
                "timeout": 600
            }
        }
        
        preset = quality_presets.get(quality_level, quality_presets["high"])
        
        # ì´ë¯¸ì§€ í¬ê¸°
        self.config["image"]["input_size"] = preset["image_size"]
        self.config["image"]["output_size"] = preset["image_size"]
        
        # ì¶”ë¡  ë‹¨ê³„
        self.config["steps"]["virtual_fitting"]["num_inference_steps"] = preset["inference_steps"]
        
        # í›„ì²˜ë¦¬ ì„¤ì •
        self.config["steps"]["post_processing"]["super_resolution"]["enabled"] = preset["super_resolution"]
        self.config["steps"]["post_processing"]["face_enhancement"]["enabled"] = preset["face_enhancement"]
        
        # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        self.config["steps"]["cloth_warping"]["physics_enabled"] = preset["physics_simulation"]
        
        # íƒ€ì„ì•„ì›ƒ
        self.config["pipeline"]["timeout_seconds"] = preset["timeout"]
        
        logger.info(f"ğŸ¯ í’ˆì§ˆ í”„ë¦¬ì…‹ ì ìš©: {quality_level}")
    
    def _deep_merge(self, base_dict: Dict, update_dict: Dict):
        """ë”•ì…”ë„ˆë¦¬ ë”¥ ë¨¸ì§€"""
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._deep_merge(base_dict[key], value)
            else:
                base_dict[key] = value
    
    # ===============================================================
    # âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ì„¤ì • ì ‘ê·¼ ë©”ì„œë“œë“¤
    # ===============================================================
    
    def get_step_config(self, step_name: str) -> Dict[str, Any]:
        """íŠ¹ì • ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        return self.config["steps"].get(step_name, {})
    
    def get_model_path(self, model_name: str) -> str:
        """ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        base_dir = self.config["model_paths"]["base_dir"]
        checkpoint_path = self.config["model_paths"]["checkpoints"].get(model_name)
        
        if checkpoint_path:
            full_path = os.path.join(base_dir, checkpoint_path)
            return full_path
        else:
            # ê¸°ë³¸ ê²½ë¡œ ìƒì„±
            return os.path.join(base_dir, model_name)
    
    def get_optimization_config(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ë°˜í™˜"""
        return self.config["optimization"]
    
    def get_memory_config(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì„¤ì • ë°˜í™˜"""
        return self.config["memory"]
    
    def get_image_config(self) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì • ë°˜í™˜"""
        return self.config["image"]
    
    def get_pipeline_config(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì „ì—­ ì„¤ì • ë°˜í™˜"""
        return self.config["pipeline"]
    
    def get_system_config(self) -> Dict[str, Any]:
        """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ì‹œìŠ¤í…œ ì„¤ì • ë°˜í™˜"""
        return {
            "device": self.device,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_enabled": self.optimization_enabled,
            "quality_level": self.quality_level,
            "constructor_pattern": "optimal"
        }
    
    # ===============================================================
    # âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ë™ì  ì„¤ì • ë³€ê²½ ë©”ì„œë“œë“¤
    # ===============================================================
    
    def update_quality_level(self, quality_level: str):
        """í’ˆì§ˆ ë ˆë²¨ ë™ì  ë³€ê²½"""
        if quality_level != self.quality_level:
            self.quality_level = quality_level
            self._apply_quality_preset(quality_level)
            logger.info(f"ğŸ”„ í’ˆì§ˆ ë ˆë²¨ ë³€ê²½: {quality_level}")
    
    def update_device(self, device: str):
        """ë””ë°”ì´ìŠ¤ ë™ì  ë³€ê²½"""
        if device != self.device:
            self.device = device
            self.config["optimization"]["device"] = device
            self.config["system"]["device"] = device
            self._apply_device_optimizations()
            logger.info(f"ğŸ”„ ë””ë°”ì´ìŠ¤ ë³€ê²½: {device}")
    
    def update_memory_limit(self, memory_gb: float):
        """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ë©”ëª¨ë¦¬ ì œí•œ ë™ì  ë³€ê²½"""
        self.memory_gb = memory_gb
        self.config["memory"]["memory_gb"] = memory_gb
        self.config["system"]["memory_gb"] = memory_gb
        self.config["memory"]["max_memory_usage"] = f"{min(80, int(memory_gb * 0.8))}%"
        logger.info(f"ğŸ”„ ë©”ëª¨ë¦¬ ì œí•œ ë³€ê²½: {memory_gb}GB")
    
    def toggle_optimization(self, enabled: bool):
        """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ìµœì í™” í† ê¸€"""
        self.optimization_enabled = enabled
        self.config["optimization"]["optimization_enabled"] = enabled
        self.config["system"]["optimization_enabled"] = enabled
        
        # ê´€ë ¨ ì„¤ì •ë“¤ ì—…ë°ì´íŠ¸
        self.config["optimization"]["mixed_precision"] = enabled
        self.config["steps"]["post_processing"]["enable_super_resolution"] = enabled
        self.config["steps"]["human_parsing"]["enable_quantization"] = enabled
        
        logger.info(f"ğŸ”„ ìµœì í™” ëª¨ë“œ: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")
    
    def enable_debug_mode(self, enabled: bool = True):
        """ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€"""
        self.config["logging"]["debug_mode"] = enabled
        self.config["logging"]["save_intermediate"] = enabled
        if enabled:
            self.config["logging"]["level"] = "DEBUG"
        logger.info(f"ğŸ”„ ë””ë²„ê·¸ ëª¨ë“œ: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")
    
    def set_memory_limit(self, limit: Union[str, float]):
        """ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì • (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        if isinstance(limit, (int, float)):
            self.update_memory_limit(float(limit))
        else:
            self.config["memory"]["max_memory_usage"] = limit
            logger.info(f"ğŸ”„ ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •: {limit}")
    
    # ===============================================================
    # âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ê²€ì¦ ë° ì§„ë‹¨ ë©”ì„œë“œë“¤
    # ===============================================================
    
    def validate_config(self) -> Dict[str, Any]:
        """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "constructor_pattern": "optimal"
        }
        
        # í•„ìˆ˜ ëª¨ë¸ ê²½ë¡œ í™•ì¸
        for model_name, checkpoint_path in self.config["model_paths"]["checkpoints"].items():
            full_path = self.get_model_path(model_name)
            if not os.path.exists(os.path.dirname(full_path)):
                validation_result["warnings"].append(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {full_path}")
        
        # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í™•ì¸
        if self.device == "mps" and not torch.backends.mps.is_available():
            validation_result["errors"].append("MPSê°€ ìš”ì²­ë˜ì—ˆì§€ë§Œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            validation_result["valid"] = False
        
        if self.device == "cuda" and not torch.cuda.is_available():
            validation_result["errors"].append("CUDAê°€ ìš”ì²­ë˜ì—ˆì§€ë§Œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            validation_result["valid"] = False
        
        # ë©”ëª¨ë¦¬ ì„¤ì • í™•ì¸
        max_memory = self.config["memory"]["max_memory_usage"]
        if isinstance(max_memory, str) and max_memory.endswith("%"):
            try:
                percent = float(max_memory[:-1])
                if not (0 < percent <= 100):
                    validation_result["errors"].append(f"ì˜ëª»ëœ ë©”ëª¨ë¦¬ ë°±ë¶„ìœ¨: {max_memory}")
                    validation_result["valid"] = False
            except ValueError:
                validation_result["errors"].append(f"ì˜ëª»ëœ ë©”ëª¨ë¦¬ í˜•ì‹: {max_memory}")
                validation_result["valid"] = False
        
        # ìµœì  ìƒì„±ì íŒ¨í„´ ê²€ì¦
        required_system_params = ["device", "device_type", "memory_gb", "is_m3_max", "optimization_enabled"]
        for param in required_system_params:
            if not hasattr(self, param):
                validation_result["errors"].append(f"í•„ìˆ˜ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ëˆ„ë½: {param}")
                validation_result["valid"] = False
        
        return validation_result
    
    def get_system_info(self) -> Dict[str, Any]:
        """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜ (ì˜¤ë²„ë¼ì´ë“œ)"""
        base_info = super().get_system_info()
        
        # PipelineConfig íŠ¹í™” ì •ë³´ ì¶”ê°€
        base_info.update({
            "pipeline_version": self.config["pipeline"]["version"],
            "quality_level": self.quality_level,
            "config_path": self.config_path,
            "device_info": self.device_info,
            "memory_config": self.get_memory_config(),
            "optimization_config": self.get_optimization_config(),
            "torch_version": torch.__version__,
            "config_valid": self.validate_config()["valid"],
            "pipeline_mode": self.config["pipeline"]["processing_mode"],
            "constructor_pattern": "optimal"
        })
        
        return base_info
    
    def export_config(self, file_path: str):
        """ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            # ì‹œìŠ¤í…œ ì •ë³´ í¬í•¨í•˜ì—¬ ë‚´ë³´ë‚´ê¸°
            export_data = {
                "config": self.config,
                "system_info": self.get_system_info(),
                "export_timestamp": json.dumps({"timestamp": "now"})  # í˜„ì¬ ì‹œê°„
            }
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ’¾ ì„¤ì • ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {file_path}")
        except Exception as e:
            logger.error(f"âŒ ì„¤ì • ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
    
    def __repr__(self):
        return (f"PipelineConfig(device={self.device}, quality={self.quality_level}, "
                f"memory={self.memory_gb}GB, m3_max={self.is_m3_max}, "
                f"optimization={self.optimization_enabled}, constructor='optimal')")


# ===============================================================
# âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ì „ì—­ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ë“¤
# ===============================================================

@lru_cache()
def get_pipeline_config(
    quality_level: str = "high",
    device: Optional[str] = None,    # ğŸ”¥ ìë™ ê°ì§€
    **kwargs  # ğŸš€ í™•ì¥ì„±
) -> PipelineConfig:
    """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - íŒŒì´í”„ë¼ì¸ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ìºì‹œë¨)"""
    return PipelineConfig(
        device=device,
        quality_level=quality_level,
        **kwargs
    )

@lru_cache()
def get_step_configs() -> Dict[str, Dict[str, Any]]:
    """ëª¨ë“  ë‹¨ê³„ ì„¤ì • ë°˜í™˜ (ìºì‹œë¨)"""
    config = get_pipeline_config()
    return config.config["steps"]

@lru_cache()
def get_model_paths() -> Dict[str, str]:
    """ëª¨ë“  ëª¨ë¸ ê²½ë¡œ ë°˜í™˜ (ìºì‹œë¨)"""
    config = get_pipeline_config()
    return {
        model_name: config.get_model_path(model_name)
        for model_name in config.config["model_paths"]["checkpoints"].keys()
    }

def create_custom_config(
    quality_level: str = "high",
    device: Optional[str] = None,      # ğŸ”¥ ìë™ ê°ì§€
    custom_settings: Optional[Dict[str, Any]] = None,
    **kwargs  # ğŸš€ í™•ì¥ì„±
) -> PipelineConfig:
    """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ì»¤ìŠ¤í…€ íŒŒì´í”„ë¼ì¸ ì„¤ì • ìƒì„±"""
    
    # ì»¤ìŠ¤í…€ ì„¤ì •ì„ kwargsì— ë³‘í•©
    if custom_settings:
        kwargs.update(custom_settings)
    
    config = PipelineConfig(
        device=device,
        quality_level=quality_level,
        **kwargs
    )
    
    return config

# ===============================================================
# âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥ í•¨ìˆ˜ë“¤
# ===============================================================

def create_optimal_pipeline_config(
    device: Optional[str] = None,      # ğŸ”¥ ìë™ ê°ì§€
    config: Optional[Dict[str, Any]] = None,
    **kwargs  # ğŸš€ í™•ì¥ì„±
) -> PipelineConfig:
    """âœ… ìµœì  ìƒì„±ì íŒ¨í„´ - ìƒˆë¡œìš´ ìµœì  ë°©ì‹"""
    return PipelineConfig(
        device=device,
        config=config,
        **kwargs
    )

def create_legacy_pipeline_config(
    config_path: Optional[str] = None, 
    quality_level: str = "high"
) -> PipelineConfig:
    """ê¸°ì¡´ ë°©ì‹ í˜¸í™˜ (ìµœì  ìƒì„±ì íŒ¨í„´ìœ¼ë¡œ ë‚´ë¶€ ì²˜ë¦¬)"""
    return PipelineConfig(
        config_path=config_path,
        quality_level=quality_level
    )

# ===============================================================
# í™˜ê²½ë³„ ì„¤ì • í•¨ìˆ˜ë“¤ - ìµœì  ìƒì„±ì íŒ¨í„´
# ===============================================================

def configure_for_development():
    """ê°œë°œ í™˜ê²½ ì„¤ì • - ìµœì  ìƒì„±ì íŒ¨í„´"""
    config = get_pipeline_config(
        quality_level="fast",
        optimization_enabled=False,
        enable_caching=False,
        enable_intermediate_saving=True
    )
    config.enable_debug_mode(True)
    logger.info("ğŸ”§ ê°œë°œ í™˜ê²½ ì„¤ì • ì ìš© (ìµœì  ìƒì„±ì íŒ¨í„´)")
    return config

def configure_for_production():
    """í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì • - ìµœì  ìƒì„±ì íŒ¨í„´"""
    config = get_pipeline_config(
        quality_level="high",
        optimization_enabled=True,
        enable_caching=True,
        memory_optimization=True
    )
    config.enable_debug_mode(False)
    logger.info("ğŸ”§ í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì • ì ìš© (ìµœì  ìƒì„±ì íŒ¨í„´)")
    return config

def configure_for_testing():
    """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • - ìµœì  ìƒì„±ì íŒ¨í„´"""
    config = get_pipeline_config(
        quality_level="fast",
        max_concurrent_requests=1,
        timeout_seconds=60,
        optimization_enabled=False
    )
    config.enable_debug_mode(True)
    logger.info("ğŸ”§ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì ìš© (ìµœì  ìƒì„±ì íŒ¨í„´)")
    return config

def configure_for_m3_max():
    """âœ… M3 Max ìµœì í™” ì„¤ì • - ìµœì  ìƒì„±ì íŒ¨í„´"""
    config = get_pipeline_config(
        device="mps",
        quality_level="high",
        memory_gb=128.0,
        is_m3_max=True,
        optimization_enabled=True,
        enable_caching=True,
        memory_optimization=True
    )
    logger.info("ğŸ”§ M3 Max ìµœì í™” ì„¤ì • ì ìš© (ìµœì  ìƒì„±ì íŒ¨í„´)")
    return config

# ===============================================================
# ì´ˆê¸°í™” ë° ê²€ì¦ (ìµœì  ìƒì„±ì íŒ¨í„´)
# ===============================================================

# ê¸°ë³¸ ì„¤ì • ìƒì„± (ìë™ ê°ì§€)
_default_config = get_pipeline_config()
_validation_result = _default_config.validate_config()

if not _validation_result["valid"]:
    for error in _validation_result["errors"]:
        logger.error(f"âŒ ì„¤ì • ì˜¤ë¥˜: {error}")
    
    # ê²½ê³ ëŠ” ë¡œê¹…ë§Œ
    for warning in _validation_result["warnings"]:
        logger.warning(f"âš ï¸ ì„¤ì • ê²½ê³ : {warning}")

logger.info(f"ğŸ”§ ìµœì  ìƒì„±ì íŒ¨í„´ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {DEVICE}")

# ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹…
_system_info = _default_config.get_system_info()
logger.info(f"ğŸ’» ì‹œìŠ¤í…œ: {_system_info['device']} ({_system_info['quality_level']}) - ìµœì  ìƒì„±ì íŒ¨í„´")
logger.info(f"ğŸ¯ ë©”ëª¨ë¦¬: {_system_info['memory_gb']}GB, M3 Max: {'âœ…' if _system_info['is_m3_max'] else 'âŒ'}")

# ===============================================================
# ìµœì  ìƒì„±ì íŒ¨í„´ í˜¸í™˜ì„± ê²€ì¦
# ===============================================================

def validate_optimal_constructor_compatibility() -> Dict[str, bool]:
    """ìµœì  ìƒì„±ì íŒ¨í„´ í˜¸í™˜ì„± ê²€ì¦"""
    try:
        # í…ŒìŠ¤íŠ¸ ì„¤ì • ìƒì„± - ìµœì  ìƒì„±ì íŒ¨í„´
        test_config = create_optimal_pipeline_config(
            device="cpu",  # ëª…ì‹œì  ì„¤ì •
            quality_level="fast",
            device_type="test",
            memory_gb=8.0,
            is_m3_max=False,
            optimization_enabled=False,
            custom_param="test_value"  # í™•ì¥ íŒŒë¼ë¯¸í„°
        )
        
        # í•„ìˆ˜ ì†ì„± í™•ì¸
        required_attrs = [
            'device', 'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level'
        ]
        attr_check = {attr: hasattr(test_config, attr) for attr in required_attrs}
        
        # í•„ìˆ˜ ë©”ì„œë“œ í™•ì¸
        required_methods = [
            'get_step_config', 'get_model_path', 'get_system_config',
            'update_quality_level', 'update_device', 'validate_config'
        ]
        method_check = {method: hasattr(test_config, method) for method in required_methods}
        
        # í™•ì¥ íŒŒë¼ë¯¸í„° í™•ì¸
        extension_check = test_config.config.get('custom_param') == 'test_value'
        
        return {
            'attributes': all(attr_check.values()),
            'methods': all(method_check.values()),
            'extensions': extension_check,
            'attr_details': attr_check,
            'method_details': method_check,
            'overall_compatible': (
                all(attr_check.values()) and 
                all(method_check.values()) and 
                extension_check
            ),
            'constructor_pattern': 'optimal'
        }
        
    except Exception as e:
        logger.error(f"ìµœì  ìƒì„±ì íŒ¨í„´ í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {
            'overall_compatible': False, 
            'error': str(e), 
            'constructor_pattern': 'optimal'
        }

# ëª¨ë“ˆ ë¡œë“œ ì‹œ í˜¸í™˜ì„± ê²€ì¦
_compatibility_result = validate_optimal_constructor_compatibility()
if _compatibility_result['overall_compatible']:
    logger.info("âœ… ìµœì  ìƒì„±ì íŒ¨í„´ í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ")
else:
    logger.warning(f"âš ï¸ í˜¸í™˜ì„± ë¬¸ì œ: {_compatibility_result}")

# ëª¨ë“ˆ ë ˆë²¨ exports
__all__ = [
    "OptimalConstructorBase",         # ìµœì  ìƒì„±ì ë² ì´ìŠ¤
    "PipelineConfig",                 # ë©”ì¸ ì„¤ì • í´ë˜ìŠ¤
    "get_pipeline_config", 
    "get_step_configs",
    "get_model_paths",
    "create_custom_config",
    "create_optimal_pipeline_config", # ìƒˆë¡œìš´ ìµœì  ë°©ì‹
    "create_legacy_pipeline_config",  # ê¸°ì¡´ í˜¸í™˜ì„±
    "configure_for_development",
    "configure_for_production", 
    "configure_for_testing",
    "configure_for_m3_max",
    "validate_optimal_constructor_compatibility"
]

logger.info("ğŸ¯ ìµœì  ìƒì„±ì íŒ¨í„´ PipelineConfig ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë“  ê¸°ëŠ¥ í™œì„±í™”")