"""
AI íŒŒì´í”„ë¼ì¸ ì„¤ì • ê´€ë¦¬
8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ ì„¤ì •ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
ê¸°ì¡´ app/ êµ¬ì¡°ì™€ ì™„ì „íˆ í˜¸í™˜ë˜ë©°, M3 Max ìµœì í™”ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
"""

import os
import json
import logging
from typing import Dict, Any, Optional, List, Union
from pathlib import Path
from functools import lru_cache
import torch

from .gpu_config import gpu_config, DEVICE, DEVICE_INFO

logger = logging.getLogger(__name__)

class PipelineConfig:
    """
    8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì„¤ì • ê´€ë¦¬
    
    ê¸°ì¡´ app/ êµ¬ì¡°ì— ë§ì¶° ì„¤ê³„ëœ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì‹œìŠ¤í…œ
    - 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ ì§€ì›
    - M3 Max MPS ìµœì í™”
    - ë™ì  ì„¤ì • ë¡œë”©
    - í™˜ê²½ë³„ ì„¤ì • ë¶„ë¦¬
    """
    
    def __init__(self, config_path: Optional[str] = None, quality_level: str = "high"):
        """
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ (ì„ íƒì )
            quality_level: í’ˆì§ˆ ë ˆë²¨ (fast, balanced, high, ultra)
        """
        self.quality_level = quality_level
        self.device = DEVICE
        self.device_info = DEVICE_INFO
        
        # ê¸°ë³¸ ì„¤ì • ë¡œë“œ
        self.config = self._load_default_config()
        
        # ì™¸ë¶€ ì„¤ì • íŒŒì¼ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        if config_path and os.path.exists(config_path):
            self._load_external_config(config_path)
        
        # í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì˜¤ë²„ë¼ì´ë“œ
        self._apply_environment_overrides()
        
        # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì ìš©
        self._apply_device_optimizations()
        
        logger.info(f"ğŸ”§ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ - í’ˆì§ˆ: {quality_level}, ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _load_default_config(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ì„¤ì • ë¡œë“œ"""
        return {
            # ì „ì—­ íŒŒì´í”„ë¼ì¸ ì„¤ì •
            "pipeline": {
                "name": "mycloset_virtual_fitting",
                "version": "2.0.0",
                "quality_level": self.quality_level,
                "processing_mode": "complete",  # fast, balanced, complete
                "enable_optimization": True,
                "enable_caching": True,
                "enable_parallel": True,
                "memory_optimization": True,
                "max_concurrent_requests": 4,
                "timeout_seconds": 300,
                "enable_intermediate_saving": False,
                "auto_retry": True,
                "max_retries": 3
            },
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì •
            "image": {
                "input_size": (512, 512),
                "output_size": (512, 512),
                "max_resolution": 1024,
                "supported_formats": ["jpg", "jpeg", "png", "webp"],
                "quality": 95,
                "preprocessing": {
                    "normalize": True,
                    "resize_mode": "lanczos",
                    "center_crop": True,
                    "background_removal": True
                }
            },
            
            # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ê°œë³„ ì„¤ì •
            "steps": {
                # 1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± (Human Parsing)
                "human_parsing": {
                    "model_name": "graphonomy",
                    "model_path": "app/ai_pipeline/models/ai_models/graphonomy",
                    "num_classes": 20,
                    "confidence_threshold": 0.7,
                    "input_size": (512, 512),
                    "batch_size": 1,
                    "cache_enabled": True,
                    "preprocessing": {
                        "normalize": True,
                        "mean": [0.485, 0.456, 0.406],
                        "std": [0.229, 0.224, 0.225]
                    },
                    "postprocessing": {
                        "morphology_cleanup": True,
                        "smooth_edges": True,
                        "fill_holes": True
                    }
                },
                
                # 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • (Pose Estimation)
                "pose_estimation": {
                    "model_name": "mediapipe",
                    "model_complexity": 2,
                    "min_detection_confidence": 0.5,
                    "min_tracking_confidence": 0.5,
                    "static_image_mode": True,
                    "enable_segmentation": True,
                    "smooth_landmarks": True,
                    "keypoints_format": "openpose_18",
                    "fallback_models": ["openpose", "hrnet"],
                    "pose_validation": {
                        "min_keypoints": 8,
                        "visibility_threshold": 0.3,
                        "symmetry_check": True
                    }
                },
                
                # 3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Cloth Segmentation)
                "cloth_segmentation": {
                    "model_name": "u2net",
                    "model_path": "app/ai_pipeline/models/ai_models/u2net",
                    "fallback_method": "rembg",
                    "background_removal": True,
                    "edge_refinement": True,
                    "post_processing": {
                        "morphology_enabled": True,
                        "gaussian_blur": True,
                        "edge_smoothing": True,
                        "noise_removal": True
                    },
                    "quality_assessment": {
                        "enable": True,
                        "min_quality": 0.6,
                        "auto_retry": True
                    }
                },
                
                # 4ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­ (Geometric Matching)
                "geometric_matching": {
                    "algorithm": "tps_hybrid",  # tps, affine, tps_hybrid
                    "num_control_points": 20,
                    "regularization": 0.001,
                    "matching_method": "hungarian",
                    "keypoint_extraction": {
                        "method": "contour_based",
                        "num_points": 50,
                        "adaptive_sampling": True
                    },
                    "validation": {
                        "min_matched_points": 4,
                        "outlier_threshold": 2.0,
                        "quality_threshold": 0.7
                    }
                },
                
                # 5ë‹¨ê³„: ì˜· ì›Œí•‘ (Cloth Warping)
                "cloth_warping": {
                    "physics_enabled": True,
                    "fabric_simulation": True,
                    "deformation_strength": 0.8,
                    "wrinkle_simulation": True,
                    "fabric_properties": {
                        "cotton": {"stiffness": 0.6, "elasticity": 0.3, "thickness": 0.5},
                        "denim": {"stiffness": 0.9, "elasticity": 0.1, "thickness": 0.8},
                        "silk": {"stiffness": 0.2, "elasticity": 0.4, "thickness": 0.2},
                        "wool": {"stiffness": 0.7, "elasticity": 0.2, "thickness": 0.7},
                        "polyester": {"stiffness": 0.4, "elasticity": 0.6, "thickness": 0.3}
                    },
                    "simulation_steps": 50,
                    "convergence_threshold": 0.001
                },
                
                # 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… (Virtual Fitting)
                "virtual_fitting": {
                    "model_name": "hr_viton",
                    "model_path": "app/ai_pipeline/models/ai_models/hr_viton",
                    "guidance_scale": 7.5,
                    "num_inference_steps": 50,
                    "strength": 0.8,
                    "eta": 0.0,
                    "composition_method": "neural_blending",
                    "fallback_method": "traditional_blending",
                    "quality_enhancement": {
                        "color_matching": True,
                        "lighting_adjustment": True,
                        "texture_preservation": True,
                        "edge_smoothing": True
                    }
                },
                
                # 7ë‹¨ê³„: í›„ì²˜ë¦¬ (Post Processing)
                "post_processing": {
                    "super_resolution": {
                        "enabled": True,
                        "model": "real_esrgan",
                        "scale_factor": 2,
                        "model_path": "app/ai_pipeline/models/ai_models/real_esrgan"
                    },
                    "face_enhancement": {
                        "enabled": True,
                        "model": "gfpgan",
                        "strength": 0.8,
                        "model_path": "app/ai_pipeline/models/ai_models/gfpgan"
                    },
                    "color_correction": {
                        "enabled": True,
                        "method": "histogram_matching",
                        "strength": 0.6
                    },
                    "noise_reduction": {
                        "enabled": True,
                        "method": "bilateral_filter",
                        "strength": 0.7
                    },
                    "edge_enhancement": {
                        "enabled": True,
                        "method": "unsharp_mask",
                        "strength": 0.5
                    }
                },
                
                # 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (Quality Assessment)
                "quality_assessment": {
                    "metrics": ["ssim", "lpips", "fid", "is"],
                    "quality_threshold": 0.7,
                    "comprehensive_analysis": True,
                    "generate_suggestions": True,
                    "benchmarking": {
                        "enabled": False,
                        "reference_dataset": None,
                        "save_results": False
                    }
                }
            },
            
            # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
            "model_paths": {
                "base_dir": "app/ai_pipeline/models/ai_models",
                "cache_dir": "app/ai_pipeline/cache",
                "checkpoints": {
                    "graphonomy": "graphonomy/checkpoints/graphonomy.pth",
                    "hr_viton": "hr_viton/checkpoints/hr_viton.pth",
                    "u2net": "u2net/checkpoints/u2net.pth",
                    "real_esrgan": "real_esrgan/checkpoints/RealESRGAN_x4plus.pth",
                    "gfpgan": "gfpgan/checkpoints/GFPGANv1.4.pth",
                    "openpose": "openpose/checkpoints/pose_iter_440000.caffemodel"
                }
            },
            
            # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
            "optimization": {
                "device": self.device,
                "mixed_precision": True,
                "gradient_checkpointing": False,
                "memory_efficient_attention": True,
                "compile_models": False,  # PyTorch 2.0 compile
                "batch_processing": {
                    "enabled": True,
                    "max_batch_size": 4,
                    "dynamic_batching": True
                },
                "caching": {
                    "enabled": True,
                    "ttl": 3600,  # 1ì‹œê°„
                    "max_size": "2GB",
                    "cache_intermediate": False
                }
            },
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •
            "memory": {
                "max_memory_usage": "80%",
                "cleanup_interval": 300,  # 5ë¶„
                "aggressive_cleanup": False,
                "model_offloading": {
                    "enabled": True,
                    "offload_to": "cpu",
                    "keep_in_memory": ["human_parsing", "pose_estimation"]
                }
            },
            
            # ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
            "logging": {
                "level": "INFO",
                "detailed_timing": True,
                "performance_metrics": True,
                "save_intermediate": False,
                "debug_mode": False
            }
        }
    
    def _load_external_config(self, config_path: str):
        """ì™¸ë¶€ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                external_config = json.load(f)
            
            # ë”¥ ë¨¸ì§€
            self._deep_merge(self.config, external_config)
            logger.info(f"âœ… ì™¸ë¶€ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {config_path}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì™¸ë¶€ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _apply_environment_overrides(self):
        """í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ"""
        
        # í’ˆì§ˆ ë ˆë²¨
        quality = os.getenv("PIPELINE_QUALITY_LEVEL", self.quality_level)
        if quality != self.quality_level:
            self.quality_level = quality
            self.config["pipeline"]["quality_level"] = quality
            self._apply_quality_preset(quality)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        device_override = os.getenv("PIPELINE_DEVICE")
        if device_override and device_override != self.device:
            self.device = device_override
            self.config["optimization"]["device"] = device_override
        
        # ë©”ëª¨ë¦¬ ì œí•œ
        memory_limit = os.getenv("PIPELINE_MEMORY_LIMIT")
        if memory_limit:
            self.config["memory"]["max_memory_usage"] = memory_limit
        
        # ë™ì‹œ ì²˜ë¦¬ ìˆ˜
        max_concurrent = os.getenv("PIPELINE_MAX_CONCURRENT")
        if max_concurrent:
            try:
                self.config["pipeline"]["max_concurrent_requests"] = int(max_concurrent)
            except ValueError:
                pass
        
        # ë””ë²„ê·¸ ëª¨ë“œ
        debug_mode = os.getenv("PIPELINE_DEBUG", "false").lower() == "true"
        self.config["logging"]["debug_mode"] = debug_mode
        if debug_mode:
            self.config["logging"]["level"] = "DEBUG"
            self.config["logging"]["save_intermediate"] = True
    
    def _apply_device_optimizations(self):
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì ìš©"""
        
        if self.device == "mps":
            # M3 Max MPS ìµœì í™”
            self.config["optimization"].update({
                "mixed_precision": True,
                "memory_efficient_attention": True,
                "compile_models": False,  # MPSì—ì„œëŠ” ì»´íŒŒì¼ ë¹„í™œì„±í™”
                "batch_processing": {
                    "enabled": True,
                    "max_batch_size": 2,  # MPS ë©”ëª¨ë¦¬ ì œí•œ
                    "dynamic_batching": False
                }
            })
            
            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            if self.quality_level in ["fast", "balanced"]:
                self.config["image"]["input_size"] = (512, 512)
                self.config["image"]["max_resolution"] = 1024
            
        elif self.device == "cuda":
            # CUDA ìµœì í™”
            self.config["optimization"].update({
                "mixed_precision": True,
                "gradient_checkpointing": True,
                "compile_models": True,
                "batch_processing": {
                    "enabled": True,
                    "max_batch_size": 8,
                    "dynamic_batching": True
                }
            })
            
        else:
            # CPU ìµœì í™”
            self.config["optimization"].update({
                "mixed_precision": False,
                "compile_models": False,
                "batch_processing": {
                    "enabled": False,
                    "max_batch_size": 1
                }
            })
            
            # CPUì—ì„œëŠ” ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
            self.config["steps"]["virtual_fitting"]["num_inference_steps"] = 20
            self.config["steps"]["post_processing"]["super_resolution"]["enabled"] = False
    
    def _apply_quality_preset(self, quality_level: str):
        """í’ˆì§ˆ ë ˆë²¨ì— ë”°ë¥¸ í”„ë¦¬ì…‹ ì ìš©"""
        
        quality_presets = {
            "fast": {
                "image_size": (256, 256),
                "inference_steps": 20,
                "super_resolution": False,
                "face_enhancement": False,
                "physics_simulation": False,
                "timeout": 60
            },
            "balanced": {
                "image_size": (512, 512),
                "inference_steps": 30,
                "super_resolution": True,
                "face_enhancement": False,
                "physics_simulation": True,
                "timeout": 120
            },
            "high": {
                "image_size": (512, 512),
                "inference_steps": 50,
                "super_resolution": True,
                "face_enhancement": True,
                "physics_simulation": True,
                "timeout": 300
            },
            "ultra": {
                "image_size": (1024, 1024),
                "inference_steps": 100,
                "super_resolution": True,
                "face_enhancement": True,
                "physics_simulation": True,
                "timeout": 600
            }
        }
        
        preset = quality_presets.get(quality_level, quality_presets["high"])
        
        # ì´ë¯¸ì§€ í¬ê¸°
        self.config["image"]["input_size"] = preset["image_size"]
        self.config["image"]["output_size"] = preset["image_size"]
        
        # ì¶”ë¡  ë‹¨ê³„
        self.config["steps"]["virtual_fitting"]["num_inference_steps"] = preset["inference_steps"]
        
        # í›„ì²˜ë¦¬ ì„¤ì •
        self.config["steps"]["post_processing"]["super_resolution"]["enabled"] = preset["super_resolution"]
        self.config["steps"]["post_processing"]["face_enhancement"]["enabled"] = preset["face_enhancement"]
        
        # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        self.config["steps"]["cloth_warping"]["physics_enabled"] = preset["physics_simulation"]
        
        # íƒ€ì„ì•„ì›ƒ
        self.config["pipeline"]["timeout_seconds"] = preset["timeout"]
        
        logger.info(f"ğŸ¯ í’ˆì§ˆ í”„ë¦¬ì…‹ ì ìš©: {quality_level}")
    
    def _deep_merge(self, base_dict: Dict, update_dict: Dict):
        """ë”•ì…”ë„ˆë¦¬ ë”¥ ë¨¸ì§€"""
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._deep_merge(base_dict[key], value)
            else:
                base_dict[key] = value
    
    # ì„¤ì • ì ‘ê·¼ ë©”ì„œë“œë“¤
    
    def get_step_config(self, step_name: str) -> Dict[str, Any]:
        """íŠ¹ì • ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        return self.config["steps"].get(step_name, {})
    
    def get_model_path(self, model_name: str) -> str:
        """ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        base_dir = self.config["model_paths"]["base_dir"]
        checkpoint_path = self.config["model_paths"]["checkpoints"].get(model_name)
        
        if checkpoint_path:
            full_path = os.path.join(base_dir, checkpoint_path)
            return full_path
        else:
            # ê¸°ë³¸ ê²½ë¡œ ìƒì„±
            return os.path.join(base_dir, model_name)
    
    def get_optimization_config(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ë°˜í™˜"""
        return self.config["optimization"]
    
    def get_memory_config(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì„¤ì • ë°˜í™˜"""
        return self.config["memory"]
    
    def get_image_config(self) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì • ë°˜í™˜"""
        return self.config["image"]
    
    def get_pipeline_config(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì „ì—­ ì„¤ì • ë°˜í™˜"""
        return self.config["pipeline"]
    
    # ë™ì  ì„¤ì • ë³€ê²½ ë©”ì„œë“œë“¤
    
    def update_quality_level(self, quality_level: str):
        """í’ˆì§ˆ ë ˆë²¨ ë™ì  ë³€ê²½"""
        if quality_level != self.quality_level:
            self.quality_level = quality_level
            self._apply_quality_preset(quality_level)
            logger.info(f"ğŸ”„ í’ˆì§ˆ ë ˆë²¨ ë³€ê²½: {quality_level}")
    
    def update_device(self, device: str):
        """ë””ë°”ì´ìŠ¤ ë™ì  ë³€ê²½"""
        if device != self.device:
            self.device = device
            self.config["optimization"]["device"] = device
            self._apply_device_optimizations()
            logger.info(f"ğŸ”„ ë””ë°”ì´ìŠ¤ ë³€ê²½: {device}")
    
    def enable_debug_mode(self, enabled: bool = True):
        """ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€"""
        self.config["logging"]["debug_mode"] = enabled
        self.config["logging"]["save_intermediate"] = enabled
        if enabled:
            self.config["logging"]["level"] = "DEBUG"
        logger.info(f"ğŸ”„ ë””ë²„ê·¸ ëª¨ë“œ: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")
    
    def set_memory_limit(self, limit: Union[str, float]):
        """ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •"""
        self.config["memory"]["max_memory_usage"] = limit
        logger.info(f"ğŸ”„ ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •: {limit}")
    
    # ê²€ì¦ ë° ì§„ë‹¨ ë©”ì„œë“œë“¤
    
    def validate_config(self) -> Dict[str, Any]:
        """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": []
        }
        
        # í•„ìˆ˜ ëª¨ë¸ ê²½ë¡œ í™•ì¸
        for model_name, checkpoint_path in self.config["model_paths"]["checkpoints"].items():
            full_path = self.get_model_path(model_name)
            if not os.path.exists(os.path.dirname(full_path)):
                validation_result["warnings"].append(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {full_path}")
        
        # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í™•ì¸
        if self.device == "mps" and not torch.backends.mps.is_available():
            validation_result["errors"].append("MPSê°€ ìš”ì²­ë˜ì—ˆì§€ë§Œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            validation_result["valid"] = False
        
        if self.device == "cuda" and not torch.cuda.is_available():
            validation_result["errors"].append("CUDAê°€ ìš”ì²­ë˜ì—ˆì§€ë§Œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            validation_result["valid"] = False
        
        # ë©”ëª¨ë¦¬ ì„¤ì • í™•ì¸
        max_memory = self.config["memory"]["max_memory_usage"]
        if isinstance(max_memory, str) and max_memory.endswith("%"):
            try:
                percent = float(max_memory[:-1])
                if not (0 < percent <= 100):
                    validation_result["errors"].append(f"ì˜ëª»ëœ ë©”ëª¨ë¦¬ ë°±ë¶„ìœ¨: {max_memory}")
                    validation_result["valid"] = False
            except ValueError:
                validation_result["errors"].append(f"ì˜ëª»ëœ ë©”ëª¨ë¦¬ í˜•ì‹: {max_memory}")
                validation_result["valid"] = False
        
        return validation_result
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        return {
            "device": self.device,
            "device_info": self.device_info,
            "quality_level": self.quality_level,
            "memory_config": self.get_memory_config(),
            "optimization_config": self.get_optimization_config(),
            "torch_version": torch.__version__,
            "config_valid": self.validate_config()["valid"]
        }
    
    def export_config(self, file_path: str):
        """ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ’¾ ì„¤ì • ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {file_path}")
        except Exception as e:
            logger.error(f"âŒ ì„¤ì • ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
    
    def __repr__(self):
        return f"PipelineConfig(device={self.device}, quality={self.quality_level})"


# ì „ì—­ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ë“¤

@lru_cache()
def get_pipeline_config(quality_level: str = "high") -> PipelineConfig:
    """íŒŒì´í”„ë¼ì¸ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ìºì‹œë¨)"""
    return PipelineConfig(quality_level=quality_level)

@lru_cache()
def get_step_configs() -> Dict[str, Dict[str, Any]]:
    """ëª¨ë“  ë‹¨ê³„ ì„¤ì • ë°˜í™˜ (ìºì‹œë¨)"""
    config = get_pipeline_config()
    return config.config["steps"]

@lru_cache()
def get_model_paths() -> Dict[str, str]:
    """ëª¨ë“  ëª¨ë¸ ê²½ë¡œ ë°˜í™˜ (ìºì‹œë¨)"""
    config = get_pipeline_config()
    return {
        model_name: config.get_model_path(model_name)
        for model_name in config.config["model_paths"]["checkpoints"].keys()
    }

def create_custom_config(
    quality_level: str = "high",
    device: Optional[str] = None,
    custom_settings: Optional[Dict[str, Any]] = None
) -> PipelineConfig:
    """ì»¤ìŠ¤í…€ íŒŒì´í”„ë¼ì¸ ì„¤ì • ìƒì„±"""
    
    config = PipelineConfig(quality_level=quality_level)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if device:
        config.update_device(device)
    
    # ì»¤ìŠ¤í…€ ì„¤ì • ì ìš©
    if custom_settings:
        config._deep_merge(config.config, custom_settings)
    
    return config

# ì´ˆê¸°í™” ë° ê²€ì¦
_default_config = get_pipeline_config()
_validation_result = _default_config.validate_config()

if not _validation_result["valid"]:
    for error in _validation_result["errors"]:
        logger.error(f"âŒ ì„¤ì • ì˜¤ë¥˜: {error}")
    
    # ê²½ê³ ëŠ” ë¡œê¹…ë§Œ
    for warning in _validation_result["warnings"]:
        logger.warning(f"âš ï¸ ì„¤ì • ê²½ê³ : {warning}")

logger.info(f"ğŸ”§ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {DEVICE}")

# ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹…
_system_info = _default_config.get_system_info()
logger.info(f"ğŸ’» ì‹œìŠ¤í…œ ì •ë³´: {_system_info['device']} ({_system_info['quality_level']})")

# ëª¨ë“ˆ ë ˆë²¨ exports
__all__ = [
    "PipelineConfig",
    "get_pipeline_config", 
    "get_step_configs",
    "get_model_paths",
    "create_custom_config"
]

# í™˜ê²½ë³„ ì„¤ì • í•¨ìˆ˜ë“¤
def configure_for_development():
    """ê°œë°œ í™˜ê²½ ì„¤ì •"""
    config = get_pipeline_config()
    config.enable_debug_mode(True)
    config.config["pipeline"]["enable_caching"] = False
    config.config["logging"]["detailed_timing"] = True
    logger.info("ğŸ”§ ê°œë°œ í™˜ê²½ ì„¤ì • ì ìš©")

def configure_for_production():
    """í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì •"""
    config = get_pipeline_config()
    config.enable_debug_mode(False)
    config.config["pipeline"]["enable_caching"] = True
    config.config["memory"]["aggressive_cleanup"] = True
    logger.info("ğŸ”§ í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì • ì ìš©")

def configure_for_testing():
    """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""
    config = get_pipeline_config("fast")  # ë¹ ë¥¸ ì²˜ë¦¬
    config.config["pipeline"]["max_concurrent_requests"] = 1
    config.config["pipeline"]["timeout_seconds"] = 60
    config.config["logging"]["level"] = "DEBUG"
    logger.info("ğŸ”§ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì ìš©")