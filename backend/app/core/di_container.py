# backend/app/core/di_container.py
"""
ğŸ”¥ DI Container v4.1 - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° + ëª¨ë“  ê¸°ëŠ¥ í¬í•¨ + ì˜¤ë¥˜ í•´ê²°
================================================================================

âœ… TYPE_CHECKINGìœ¼ë¡œ import ìˆœí™˜ ì™„ì „ ì°¨ë‹¨
âœ… ì§€ì—° í•´ê²°(Lazy Resolution)ë¡œ ëŸ°íƒ€ì„ ìˆœí™˜ì°¸ì¡° ë°©ì§€
âœ… safe_copy í•¨ìˆ˜ í¬í•¨ìœ¼ë¡œ ì•ˆì „í•œ ë°ì´í„° ë³µì‚¬
âœ… get_stats ë©”ì„œë“œ ì¶”ê°€ë¡œ 'get_stats' ì†ì„± ì˜¤ë¥˜ í•´ê²°
âœ… ê¸°ì¡´ MyCloset AI í”„ë¡œì íŠ¸ 100% í˜¸í™˜
âœ… step_factory.py â†” base_step_mixin.py ìˆœí™˜ì°¸ì¡° í•´ê²°
âœ… Mock í´ë°± êµ¬í˜„ì²´ í¬í•¨
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™”
âœ… BaseStepMixinì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  DI Container ë©”ì„œë“œ êµ¬í˜„

Author: MyCloset AI Team
Date: 2025-07-30
Version: 4.1 (Complete Features + All Error Resolution)
"""

import os
import sys
import gc
import copy
import logging
import threading
import time
import weakref
import platform
import subprocess
import importlib
from typing import Dict, Any, Optional, Type, TypeVar, Callable, Union, List, TYPE_CHECKING
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum

# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    # ì˜¤ì§ íƒ€ì… ì²´í¬ ì‹œì—ë§Œ import
    from ..ai_pipeline.steps.base_step_mixin import BaseStepMixin
    from ..ai_pipeline.utils.model_loader import ModelLoader
    from ..ai_pipeline.utils.memory_manager import MemoryManager
    from ..ai_pipeline.utils.data_converter import DataConverter
    from ..ai_pipeline.factories.step_factory import StepFactory
else:
    # ëŸ°íƒ€ì„ì—ëŠ” Anyë¡œ ì²˜ë¦¬
    BaseStepMixin = Any
    ModelLoader = Any
    MemoryManager = Any
    DataConverter = Any
    StepFactory = Any

# ==============================================
# ğŸ”¥ í™˜ê²½ ì„¤ì • (ìˆœí™˜ì°¸ì¡° ì—†ëŠ” ë…ë¦½ì  ì„¤ì •)
# ==============================================

# conda í™˜ê²½ ìš°ì„  ì„¤ì •
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', 'none')
IS_CONDA = CONDA_ENV != 'none'
IS_TARGET_ENV = CONDA_ENV == 'mycloset-ai-clean'

# M3 Max ê°ì§€ (ë…ë¦½ì )
def detect_m3_max() -> bool:
    """M3 Max ê°ì§€ (ìˆœí™˜ì°¸ì¡° ì—†ìŒ)"""
    try:
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except Exception:
        pass
    return False

IS_M3_MAX = detect_m3_max()
MEMORY_GB = 128.0 if IS_M3_MAX else 16.0
DEVICE = 'mps' if IS_M3_MAX else 'cpu'

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# PyTorch ê°€ìš©ì„± ì²´í¬ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        # M3 Max ìµœì í™” ì„¤ì •
        os.environ.setdefault('PYTORCH_ENABLE_MPS_FALLBACK', '1')
        os.environ.setdefault('PYTORCH_MPS_HIGH_WATERMARK_RATIO', '0.0')
        
    logger.info(f"âœ… PyTorch ë¡œë“œ: MPS={MPS_AVAILABLE}, M3 Max={IS_M3_MAX}")
except ImportError:
    logger.warning("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch ê¶Œì¥")

T = TypeVar('T')

# ==============================================
# ğŸ”¥ ì•ˆì „í•œ ë°ì´í„° ë³µì‚¬ í•¨ìˆ˜ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def safe_copy(data: Any, deep: bool = True) -> Any:
    """ì•ˆì „í•œ ë°ì´í„° ë³µì‚¬ í•¨ìˆ˜ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        if data is None:
            return None
        
        # ê¸°ë³¸ íƒ€ì…ë“¤ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if isinstance(data, (str, int, float, bool)):
            return data
        
        # ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
        if isinstance(data, dict):
            if deep:
                return {k: safe_copy(v, deep=True) for k, v in data.items()}
            else:
                return dict(data)
        
        # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        if isinstance(data, list):
            if deep:
                return [safe_copy(item, deep=True) for item in data]
            else:
                return list(data)
        
        # íŠœí”Œ ì²˜ë¦¬
        if isinstance(data, tuple):
            if deep:
                return tuple(safe_copy(item, deep=True) for item in data)
            else:
                return tuple(data)
        
        # ì„¸íŠ¸ ì²˜ë¦¬
        if isinstance(data, set):
            if deep:
                return {safe_copy(item, deep=True) for item in data}
            else:
                return set(data)
        
        # copy ë©”ì„œë“œ ì‹œë„
        if hasattr(data, 'copy'):
            try:
                return data.copy()
            except Exception:
                pass
        
        # deepcopy ì‹œë„
        if deep:
            try:
                return copy.deepcopy(data)
            except Exception:
                pass
        
        # shallow copy ì‹œë„
        try:
            return copy.copy(data)
        except Exception:
            pass
        
        # ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ë°˜í™˜
        logger.warning(f"âš ï¸ safe_copy ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {type(data)}")
        return data
        
    except Exception as e:
        logger.error(f"âŒ safe_copy ì˜¤ë¥˜: {e}, ì›ë³¸ ë°˜í™˜")
        return data

# ==============================================
# ğŸ”¥ ì§€ì—° í•´ê²° í´ë˜ìŠ¤ë“¤ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

class LazyDependency:
    """ì§€ì—° ì˜ì¡´ì„± í•´ê²°ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    
    def __init__(self, factory: Callable[[], Any]):
        self._factory = factory
        self._instance = None
        self._resolved = False
        self._lock = threading.RLock()
    
    def get(self) -> Any:
        """ì§€ì—° í•´ê²°"""
        if not self._resolved:
            with self._lock:
                if not self._resolved:
                    try:
                        self._instance = self._factory()
                        self._resolved = True
                        logger.debug("âœ… ì§€ì—° ì˜ì¡´ì„± í•´ê²° ì™„ë£Œ")
                    except Exception as e:
                        logger.error(f"âŒ ì§€ì—° ì˜ì¡´ì„± í•´ê²° ì‹¤íŒ¨: {e}")
                        return None
        
        return self._instance
    
    def resolve(self) -> Any:
        """resolve() ë©”ì„œë“œ ë³„ì¹­ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        return self.get()
    
    def is_resolved(self) -> bool:
        return self._resolved

# ==============================================
# ğŸ”¥ ë™ì  Import í•´ê²°ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

class DynamicImportResolver:
    """ë™ì  import í•´ê²°ê¸° (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€)"""
    
    @staticmethod
    def resolve_model_loader():
        """ModelLoader ë™ì  í•´ê²° (DI Container ì£¼ì… ì§€ì›)"""
        import_paths = [
            'app.ai_pipeline.utils.model_loader',
            'ai_pipeline.utils.model_loader',
            'utils.model_loader'
        ]
        
        for path in import_paths:
            try:
                module = importlib.import_module(path)
                
                # ì „ì—­ í•¨ìˆ˜ ìš°ì„  (DI Container ì£¼ì… ì§€ì›)
                if hasattr(module, 'get_global_model_loader'):
                    # ğŸ”¥ í˜„ì¬ DI Container ì¸ìŠ¤í„´ìŠ¤ë¥¼ configë¡œ ì „ë‹¬
                    current_container = get_global_container()  # ìê¸° ìì‹ 
                    config = {'di_container': current_container} if current_container else {}
                    
                    loader = module.get_global_model_loader(config)
                    if loader:
                        logger.debug(f"âœ… ModelLoader ë™ì  í•´ê²° (DI Container ì£¼ì…): {path}")
                        return loader
                
                # í´ë˜ìŠ¤ ì§ì ‘ ìƒì„± (DI Container ì£¼ì…)
                if hasattr(module, 'ModelLoader'):
                    ModelLoaderClass = module.ModelLoader
                    current_container = get_global_container()  # ìê¸° ìì‹ 
                    loader = ModelLoaderClass(di_container=current_container)
                    logger.debug(f"âœ… ModelLoader í´ë˜ìŠ¤ ìƒì„± (DI Container ì£¼ì…): {path}")
                    return loader
                    
            except ImportError:
                continue
            except Exception as e:
                logger.debug(f"ModelLoader í•´ê²° ì‹œë„ ì‹¤íŒ¨ ({path}): {e}")
                continue
        
        # ì™„ì „ ì‹¤íŒ¨ ì‹œ Mock ë°˜í™˜
        logger.warning("âš ï¸ ModelLoader í•´ê²° ì‹¤íŒ¨, Mock ì‚¬ìš©")
        return DynamicImportResolver._create_mock_model_loader()

    @staticmethod
    def resolve_memory_manager():
        """MemoryManager ë™ì  í•´ê²° (ì •í™•í•œ ê²½ë¡œë“¤)"""
        import_paths = [
            'app.services.memory_manager',
            'app.ai_pipeline.utils.memory_manager',
            'services.memory_manager',
            'ai_pipeline.utils.memory_manager',
            'utils.memory_manager',
            'backend.app.services.memory_manager',
            'backend.app.ai_pipeline.utils.memory_manager'
        ]
        
        for path in import_paths:
            try:
                module = importlib.import_module(path)
                
                # ë‹¤ì–‘í•œ í•¨ìˆ˜/í´ë˜ìŠ¤ëª… ì‹œë„
                access_methods = [
                    'get_global_memory_manager',
                    'get_memory_manager',
                    'create_memory_manager',
                    'MemoryManager',
                    'create_optimized_memory_manager'
                ]
                
                for method_name in access_methods:
                    if hasattr(module, method_name):
                        try:
                            if method_name.startswith('get_') or method_name.startswith('create_'):
                                # í•¨ìˆ˜ì¸ ê²½ìš° í˜¸ì¶œ
                                manager = getattr(module, method_name)()
                            else:
                                # í´ë˜ìŠ¤ì¸ ê²½ìš° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                                ManagerClass = getattr(module, method_name)
                                manager = ManagerClass()
                            
                            if manager:
                                logger.info(f"âœ… MemoryManager í•´ê²° ì„±ê³µ: {path} â†’ {method_name}")
                                return manager
                        except Exception as e:
                            logger.debug(f"âš ï¸ {method_name} í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                            continue
                    
            except ImportError as e:
                logger.debug(f"ğŸ“‹ {path} import ì‹¤íŒ¨: {e}")
                continue
            except Exception as e:
                logger.debug(f"âš ï¸ {path} í•´ê²° ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ì™„ì „ ì‹¤íŒ¨ ì‹œ Mock ë°˜í™˜
        logger.warning("âš ï¸ MemoryManager í•´ê²° ì‹¤íŒ¨, Mock ì‚¬ìš©")
        return DynamicImportResolver._create_mock_memory_manager()
    
    @staticmethod
    def resolve_data_converter():
        """DataConverter ë™ì  í•´ê²° (ì •í™•í•œ ê²½ë¡œë“¤)"""
        import_paths = [
            'app.ai_pipeline.utils.data_converter',
            'app.services.data_converter',
            'ai_pipeline.utils.data_converter',
            'services.data_converter',
            'utils.data_converter',
            'backend.app.ai_pipeline.utils.data_converter'
        ]
        
        for path in import_paths:
            try:
                module = importlib.import_module(path)
                
                # ë‹¤ì–‘í•œ ì ‘ê·¼ ë°©ë²•
                access_methods = [
                    'get_global_data_converter',
                    'get_data_converter', 
                    'create_data_converter',
                    'DataConverter'
                ]
                
                for method_name in access_methods:
                    if hasattr(module, method_name):
                        try:
                            if method_name.startswith('get_') or method_name.startswith('create_'):
                                converter = getattr(module, method_name)()
                            else:
                                ConverterClass = getattr(module, method_name)
                                converter = ConverterClass()
                            
                            if converter:
                                logger.info(f"âœ… DataConverter í•´ê²° ì„±ê³µ: {path} â†’ {method_name}")
                                return converter
                        except Exception as e:
                            logger.debug(f"âš ï¸ {method_name} í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                            continue
                    
            except ImportError:
                continue
            except Exception as e:
                logger.debug(f"âš ï¸ {path} í•´ê²° ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ì™„ì „ ì‹¤íŒ¨ ì‹œ Mock ë°˜í™˜
        logger.warning("âš ï¸ DataConverter í•´ê²° ì‹¤íŒ¨, Mock ì‚¬ìš©")
        return DynamicImportResolver._create_mock_data_converter()

    # ==============================================
    # ğŸ”¥ Mock ìƒì„±ê¸°ë“¤ (ì™„ì „í•œ êµ¬í˜„)
    # ==============================================
    
    @staticmethod
    def _create_mock_model_loader():
        """Mock ModelLoader (ì™„ì „í•œ êµ¬í˜„)"""
        class MockModelLoader:
            def __init__(self, di_container=None):
                self.models_loaded = 0
                self.is_initialized = True
                self._di_container = di_container
                self.loaded_models = {}
                self.model_info = {}
                self.model_status = {}
                self.step_interfaces = {}
                self.step_requirements = {}
            
            def load_model(self, model_path: str, device: str = 'auto'):
                self.models_loaded += 1
                model_id = f"mock_model_{self.models_loaded}"
                self.loaded_models[model_id] = {
                    "model": model_id,
                    "loaded": True,
                    "device": device,
                    "path": model_path
                }
                return self.loaded_models[model_id]
            
            def unload_model(self, model_name: str):
                if model_name in self.loaded_models:
                    del self.loaded_models[model_name]
                    return True
                return False
            
            def get_model_info(self):
                return {
                    "models_loaded": self.models_loaded,
                    "device": DEVICE,
                    "available": True,
                    "loaded_models": list(self.loaded_models.keys())
                }
            
            def optimize_memory(self):
                """ğŸ”¥ ë©”ëª¨ë¦¬ ìµœì í™” (DI Containerì˜ MemoryManager í™œìš©)"""
                try:
                    results = {"success": False, "method": "unknown"}
                    
                    # DI Containerì˜ MemoryManager ìš°ì„  ì‚¬ìš©
                    if hasattr(self, '_di_container') and self._di_container and hasattr(self._di_container, 'get'):
                        try:
                            memory_manager = self._di_container.get('memory_manager')
                            if memory_manager and hasattr(memory_manager, 'optimize_memory'):
                                results = memory_manager.optimize_memory(aggressive=True)
                                results["method"] = "di_container_memory_manager"
                                logger.debug("âœ… DI Container MemoryManagerë¡œ ë©”ëª¨ë¦¬ ìµœì í™”")
                                return results
                        except Exception as e:
                            logger.debug(f"DI Container MemoryManager ì‹¤íŒ¨: {e}")
                    
                    # í´ë°±: ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬
                    gc.collect()
                    
                    # M3 Max MPS ìµœì í™”
                    if TORCH_AVAILABLE and IS_M3_MAX and MPS_AVAILABLE:
                        import torch
                        if hasattr(torch.backends.mps, 'empty_cache'):
                            torch.backends.mps.empty_cache()
                    
                    results = {"success": True, "method": "fallback_gc", "memory_freed_mb": 0.0}
                    return results
                    
                except Exception as e:
                    logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
                    return {"success": False, "error": str(e)}
            
            def convert_data(self, data, target_format: str):
                """ğŸ”¥ ë°ì´í„° ë³€í™˜ (DI Containerì˜ DataConverter í™œìš©)"""
                try:
                    # DI Containerì˜ DataConverter ìš°ì„  ì‚¬ìš©
                    if hasattr(self, '_di_container') and self._di_container and hasattr(self._di_container, 'get'):
                        try:
                            data_converter = self._di_container.get('data_converter')
                            if data_converter and hasattr(data_converter, 'convert'):
                                result = data_converter.convert(data, target_format)
                                logger.debug(f"âœ… DI Container DataConverterë¡œ ë°ì´í„° ë³€í™˜: {target_format}")
                                return result
                        except Exception as e:
                            logger.debug(f"DI Container DataConverter ì‹¤íŒ¨: {e}")
                    
                    # í´ë°±: ê¸°ë³¸ ë³€í™˜ ë¡œì§
                    logger.debug(f"âš ï¸ ê¸°ë³¸ ë°ì´í„° ë³€í™˜ ì‚¬ìš©: {target_format}")
                    return {
                        "converted_data": data,  # ì›ë³¸ ë°ì´í„° ë°˜í™˜
                        "format": target_format,
                        "success": True,
                        "method": "fallback"
                    }
                    
                except Exception as e:
                    logger.error(f"âŒ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
                    return {"success": False, "error": str(e)}

            def register_service(self, service_key: str, service_instance: Any, singleton: bool = True):
                """ğŸ”¥ DI Containerì— ì„œë¹„ìŠ¤ ë“±ë¡"""
                try:
                    if self._di_container and hasattr(self._di_container, 'register'):
                        self._di_container.register(service_key, service_instance, singleton)
                        logger.debug(f"âœ… DI Containerì— ì„œë¹„ìŠ¤ ë“±ë¡: {service_key}")
                        return True
                    else:
                        logger.debug("âš ï¸ DI Container ì—†ìŒ, ì„œë¹„ìŠ¤ ë“±ë¡ ë¶ˆê°€")
                        return False
                except Exception as e:
                    logger.debug(f"âŒ ì„œë¹„ìŠ¤ ë“±ë¡ ì‹¤íŒ¨: {e}")
                    return False

            @property
            def di_container(self):
                """ğŸ”¥ DI Container ì°¸ì¡° ì†ì„±"""
                return self._di_container
            
            def cleanup(self):
                """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (step_interface.py í˜¸í™˜)"""
                try:
                    logger.info("ğŸ§¹ step_interface.py í˜¸í™˜ ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
                    
                    # ëª¨ë“  ì‹¤ì œ AI ëª¨ë¸ ì–¸ë¡œë“œ
                    for model_name in list(self.loaded_models.keys()):
                        self.unload_model(model_name)
                    
                    # ìºì‹œ ì •ë¦¬
                    self.model_info.clear()
                    self.model_status.clear()
                    self.step_interfaces.clear()
                    self.step_requirements.clear()
                    
                    # ğŸ”¥ DI Containerë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ìµœì í™”
                    if self._di_container:
                        try:
                            cleanup_stats = self._di_container.optimize_memory()
                            logger.debug(f"DI Container ë©”ëª¨ë¦¬ ìµœì í™”: {cleanup_stats}")
                        except Exception as e:
                            logger.debug(f"DI Container ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    gc.collect()
                    
                    # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
                    if MPS_AVAILABLE and TORCH_AVAILABLE:
                        try:
                            import torch
                            if hasattr(torch.backends.mps, 'empty_cache'):
                                torch.backends.mps.empty_cache()
                        except:
                            pass
                    
                    logger.info("âœ… step_interface.py í˜¸í™˜ ModelLoader ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
                    
                except Exception as e:
                    logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        return MockModelLoader()
    
    @staticmethod
    def _create_mock_memory_manager():
        """Mock MemoryManager (ì™„ì „í•œ êµ¬í˜„)"""
        class MockMemoryManager:
            def __init__(self):
                self.optimization_count = 0
                self.is_initialized = True
            
            def optimize_memory(self, aggressive: bool = False):
                self.optimization_count += 1
                try:
                    gc.collect()
                    if TORCH_AVAILABLE and MPS_AVAILABLE:
                        import torch
                        if hasattr(torch.backends.mps, 'empty_cache'):
                            torch.backends.mps.empty_cache()
                    return {"success": True, "optimizations": self.optimization_count}
                except Exception as e:
                    return {"success": False, "error": str(e)}
            
            def optimize(self, aggressive: bool = False):
                """optimize ë©”ì„œë“œ ë³„ì¹­ ì¶”ê°€"""
                return self.optimize_memory(aggressive)
            
            def get_memory_info(self):
                return {
                    "total_gb": MEMORY_GB,
                    "available_gb": MEMORY_GB * 0.7,
                    "percent": 30.0,
                    "device": DEVICE,
                    "is_m3_max": IS_M3_MAX,
                    "optimization_count": self.optimization_count
                }
            
            def cleanup(self):
                self.optimize_memory(aggressive=True)
        
        return MockMemoryManager()
    
    @staticmethod
    def _create_mock_data_converter():
        """Mock DataConverter (ì™„ì „í•œ êµ¬í˜„)"""
        class MockDataConverter:
            def __init__(self):
                self.conversion_count = 0
                self.is_initialized = True
            
            def convert(self, data, target_format: str):
                self.conversion_count += 1
                return {
                    "converted_data": f"mock_converted_{target_format}_{self.conversion_count}",
                    "format": target_format,
                    "conversion_count": self.conversion_count,
                    "success": True
                }
            
            def get_supported_formats(self):
                return ["tensor", "numpy", "pil", "cv2", "base64"]
            
            def cleanup(self):
                self.conversion_count = 0
        
        return MockDataConverter()

# ==============================================
# ğŸ”¥ ìˆœí™˜ì°¸ì¡° ë°©ì§€ DI Container (ì™„ì „í•œ êµ¬í˜„)
# ==============================================

class CircularReferenceFreeDIContainer:
    """ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ DI Container (ì™„ì „í•œ ê¸°ëŠ¥ êµ¬í˜„)"""
    
    def __init__(self):
        # ì§€ì—° ì˜ì¡´ì„± ì €ì¥ì†Œ
        self._lazy_dependencies: Dict[str, LazyDependency] = {}
        
        # ì¼ë°˜ ì„œë¹„ìŠ¤ ì €ì¥ì†Œ
        self._services: Dict[str, Any] = {}
        self._singletons: Dict[str, Any] = {}
        
        # ë©”ëª¨ë¦¬ ë³´í˜¸ (ì•½í•œ ì°¸ì¡°)
        self._weak_refs: Dict[str, weakref.ref] = {}
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        # ìˆœí™˜ì°¸ì¡° ê°ì§€
        self._resolving_stack: List[str] = []
        self._circular_detected = set()
        
        # í†µê³„
        self._stats = {
            'lazy_resolutions': 0,
            'circular_references_prevented': 0,
            'mock_fallbacks_used': 0,
            'successful_resolutions': 0,
            'total_requests': 0
        }
        
        # ì´ˆê¸°í™”
        self._setup_core_dependencies()
        
        logger.info("ğŸ”— CircularReferenceFreeDIContainer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_core_dependencies(self):
        """í•µì‹¬ ì˜ì¡´ì„±ë“¤ ì§€ì—° ë“±ë¡ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        try:
            # ModelLoader ì§€ì—° ë“±ë¡
            model_loader_lazy = LazyDependency(
                DynamicImportResolver.resolve_model_loader
            )
            self._lazy_dependencies['model_loader'] = model_loader_lazy
            self._lazy_dependencies['IModelLoader'] = model_loader_lazy
            
            # MemoryManager ì§€ì—° ë“±ë¡
            memory_manager_lazy = LazyDependency(
                DynamicImportResolver.resolve_memory_manager
            )
            self._lazy_dependencies['memory_manager'] = memory_manager_lazy
            self._lazy_dependencies['IMemoryManager'] = memory_manager_lazy
            
            # DataConverter ì§€ì—° ë“±ë¡
            data_converter_lazy = LazyDependency(
                DynamicImportResolver.resolve_data_converter
            )
            self._lazy_dependencies['data_converter'] = data_converter_lazy
            self._lazy_dependencies['IDataConverter'] = data_converter_lazy
            
            # ì‹œìŠ¤í…œ ì •ë³´ ì§ì ‘ ë“±ë¡ (ìˆœí™˜ì°¸ì¡° ì—†ìŒ)
            self._services['device'] = DEVICE
            self._services['conda_env'] = CONDA_ENV
            self._services['is_m3_max'] = IS_M3_MAX
            self._services['memory_gb'] = MEMORY_GB
            self._services['torch_available'] = TORCH_AVAILABLE
            self._services['mps_available'] = MPS_AVAILABLE
            
            logger.info("âœ… í•µì‹¬ ì˜ì¡´ì„± ì§€ì—° ë“±ë¡ ì™„ë£Œ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)")
            
        except Exception as e:
            logger.error(f"âŒ í•µì‹¬ ì˜ì¡´ì„± ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    def register_lazy(self, key: str, factory: Callable[[], Any]) -> None:
        """ì§€ì—° ì˜ì¡´ì„± ë“±ë¡"""
        with self._lock:
            self._lazy_dependencies[key] = LazyDependency(factory)
            logger.debug(f"âœ… ì§€ì—° ì˜ì¡´ì„± ë“±ë¡: {key}")
    
    def register(self, key: str, instance: Any, singleton: bool = True) -> None:
        """ì¼ë°˜ ì˜ì¡´ì„± ë“±ë¡"""
        with self._lock:
            if singleton:
                self._singletons[key] = instance
            else:
                self._services[key] = instance
            logger.debug(f"âœ… ì˜ì¡´ì„± ë“±ë¡: {key} ({'ì‹±ê¸€í†¤' if singleton else 'ì„ì‹œ'})")
    
    def get(self, key: str) -> Optional[Any]:
        """ì˜ì¡´ì„± ì¡°íšŒ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        with self._lock:
            self._stats['total_requests'] += 1
            
            # ìˆœí™˜ì°¸ì¡° ê°ì§€
            if key in self._resolving_stack:
                circular_path = ' -> '.join(self._resolving_stack + [key])
                self._circular_detected.add(key)
                self._stats['circular_references_prevented'] += 1
                logger.error(f"âŒ ìˆœí™˜ì°¸ì¡° ê°ì§€: {circular_path}")
                return None
            
            # ìˆœí™˜ì°¸ì¡°ë¡œ ì´ë¯¸ ì°¨ë‹¨ëœ ê²½ìš°
            if key in self._circular_detected:
                logger.debug(f"âš ï¸ ì´ì „ì— ìˆœí™˜ì°¸ì¡° ê°ì§€ëœ í‚¤: {key}")
                return None
            
            self._resolving_stack.append(key)
            
            try:
                result = self._resolve_dependency(key)
                if result is not None:
                    self._stats['successful_resolutions'] += 1
                return result
            finally:
                self._resolving_stack.remove(key)
    
    def _resolve_dependency(self, key: str) -> Optional[Any]:
        """ì‹¤ì œ ì˜ì¡´ì„± í•´ê²°"""
        # 1. ì‹±ê¸€í†¤ ì²´í¬
        if key in self._singletons:
            return self._singletons[key]
        
        # 2. ì¼ë°˜ ì„œë¹„ìŠ¤ ì²´í¬
        if key in self._services:
            return self._services[key]
        
        # 3. ì•½í•œ ì°¸ì¡° ì²´í¬
        if key in self._weak_refs:
            weak_ref = self._weak_refs[key]
            instance = weak_ref()
            if instance is not None:
                return instance
            else:
                del self._weak_refs[key]
        
        # 4. ì§€ì—° ì˜ì¡´ì„± í•´ê²°
        if key in self._lazy_dependencies:
            lazy_dep = self._lazy_dependencies[key]
            instance = lazy_dep.get()
            
            if instance is not None:
                self._stats['lazy_resolutions'] += 1
                # ì•½í•œ ì°¸ì¡°ë¡œ ìºì‹œ
                self._weak_refs[key] = weakref.ref(instance)
                return instance
            else:
                self._stats['mock_fallbacks_used'] += 1
        
        return None
    
    def is_registered(self, key: str) -> bool:
        """ë“±ë¡ ì—¬ë¶€ í™•ì¸"""
        with self._lock:
            return (key in self._services or 
                   key in self._singletons or 
                   key in self._lazy_dependencies)
    
    def cleanup_circular_references(self):
        """ìˆœí™˜ì°¸ì¡° ê°ì§€ ìƒíƒœ ì •ë¦¬"""
        with self._lock:
            self._circular_detected.clear()
            self._resolving_stack.clear()
            logger.info("ğŸ§¹ ìˆœí™˜ì°¸ì¡° ê°ì§€ ìƒíƒœ ì •ë¦¬ ì™„ë£Œ")
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            with self._lock:
                cleanup_stats = {}
                
                # ì•½í•œ ì°¸ì¡° ì •ë¦¬
                dead_refs = [k for k, ref in self._weak_refs.items() if ref() is None]
                for k in dead_refs:
                    del self._weak_refs[k]
                cleanup_stats['dead_refs_removed'] = len(dead_refs)
                
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                collected = gc.collect()
                cleanup_stats['gc_collected'] = collected
                
                # M3 Max MPS ìµœì í™”
                if TORCH_AVAILABLE and IS_M3_MAX and MPS_AVAILABLE:
                    import torch
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                        cleanup_stats['mps_cache_cleared'] = True
                
                logger.debug(f"ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {cleanup_stats}")
                return cleanup_stats
                
        except Exception as e:
            logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {}
    
    def get_stats(self) -> Dict[str, Any]:
        """ğŸ”¥ í†µê³„ ì •ë³´ ì¡°íšŒ (ì•ˆì „í•œ ë³µì‚¬ ì‚¬ìš©) - ì˜¤ë¥˜ í•´ê²°"""
        try:
            with self._lock:
                base_stats = {
                    'container_type': 'CircularReferenceFreeDIContainer',
                    'version': '4.1',
                    'statistics': safe_copy(dict(self._stats), deep=True),
                    'registrations': {
                        'lazy_dependencies': len(self._lazy_dependencies),
                        'singleton_instances': len(self._singletons),
                        'transient_services': len(self._services),
                        'weak_references': len(self._weak_refs)
                    },
                    'circular_reference_protection': {
                        'detected_keys': list(self._circular_detected),
                        'current_resolving_stack': list(self._resolving_stack),
                        'prevention_count': self._stats.get('circular_references_prevented', 0)
                    },
                    'environment': {
                        'is_conda': IS_CONDA,
                        'conda_env': CONDA_ENV,
                        'is_target_env': IS_TARGET_ENV,
                        'is_m3_max': IS_M3_MAX,
                        'device': DEVICE,
                        'memory_gb': MEMORY_GB,
                        'torch_available': TORCH_AVAILABLE,
                        'mps_available': MPS_AVAILABLE
                    }
                }
                return base_stats
        except Exception as e:
            logger.error(f"âŒ get_stats ì˜¤ë¥˜: {e}")
            return {
                'container_type': 'CircularReferenceFreeDIContainer',
                'version': '4.1',
                'error': str(e),
                'fallback': True
            }

    # ==============================================
    # ğŸ”¥ BaseStepMixinì—ì„œ ì‚¬ìš©í•˜ëŠ” ì¶”ê°€ ë©”ì„œë“œë“¤
    # ==============================================
    
    def inject_di_container(self, container) -> bool:
        """DI Container ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
        try:
            # ìê¸° ìì‹ ì´ë¯€ë¡œ í•­ìƒ ì„±ê³µ
            return True
        except Exception as e:
            logger.error(f"âŒ DI Container ì£¼ì… ì‹¤íŒ¨: {e}")
            return False

# ==============================================
# ğŸ”¥ Step íŠ¹í™” ì˜ì¡´ì„± ì£¼ì… í•¨ìˆ˜ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def inject_dependencies_to_step_safe(step_instance, container: Optional[CircularReferenceFreeDIContainer] = None):
    """Stepì— ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì… (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        if container is None:
            container = get_global_container()
        
        injections_made = 0
        
        # ModelLoader ì£¼ì… (ì•ˆì „)
        model_loader = container.get('model_loader')
        if model_loader:
            if hasattr(step_instance, 'set_model_loader'):
                step_instance.set_model_loader(model_loader)
                injections_made += 1
            elif hasattr(step_instance, 'model_loader'):
                step_instance.model_loader = model_loader
                injections_made += 1
        
        # MemoryManager ì£¼ì… (ì•ˆì „)
        memory_manager = container.get('memory_manager')
        if memory_manager:
            if hasattr(step_instance, 'set_memory_manager'):
                step_instance.set_memory_manager(memory_manager)
                injections_made += 1
            elif hasattr(step_instance, 'memory_manager'):
                step_instance.memory_manager = memory_manager
                injections_made += 1
        
        # DataConverter ì£¼ì… (ì•ˆì „)
        data_converter = container.get('data_converter')
        if data_converter:
            if hasattr(step_instance, 'set_data_converter'):
                step_instance.set_data_converter(data_converter)
                injections_made += 1
            elif hasattr(step_instance, 'data_converter'):
                step_instance.data_converter = data_converter
                injections_made += 1
        
        # StepFactoryëŠ” ì ˆëŒ€ ì£¼ì…í•˜ì§€ ì•ŠìŒ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
        
        # ì´ˆê¸°í™”
        if hasattr(step_instance, 'initialize') and not getattr(step_instance, 'is_initialized', False):
            step_instance.initialize()
        
        logger.debug(f"âœ… {step_instance.__class__.__name__} ì•ˆì „ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ ({injections_made}ê°œ)")
        
    except Exception as e:
        logger.error(f"âŒ Step ì•ˆì „ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì „ì—­ Container ê´€ë¦¬ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

_global_container: Optional[CircularReferenceFreeDIContainer] = None
_container_lock = threading.RLock()

def get_global_container() -> CircularReferenceFreeDIContainer:
    """ì „ì—­ DI Container ë°˜í™˜ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    global _global_container
    
    with _container_lock:
        if _global_container is None:
            _global_container = CircularReferenceFreeDIContainer()
            logger.info("âœ… ì „ì—­ CircularReferenceFreeDIContainer ìƒì„± ì™„ë£Œ")
    
    return _global_container

def reset_global_container():
    """ì „ì—­ Container ë¦¬ì…‹ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    global _global_container
    
    with _container_lock:
        if _global_container:
            _global_container.optimize_memory(aggressive=True)
            _global_container.cleanup_circular_references()
        
        _global_container = None
        logger.info("ğŸ”„ ì „ì—­ CircularReferenceFreeDIContainer ë¦¬ì…‹ ì™„ë£Œ")

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def get_service_safe(key: str) -> Optional[Any]:
    """ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    container = get_global_container()
    return container.get(key)

def register_service_safe(key: str, instance: Any, singleton: bool = True):
    """ì•ˆì „í•œ ì„œë¹„ìŠ¤ ë“±ë¡ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    container = get_global_container()
    container.register(key, instance, singleton)

def register_lazy_service(key: str, factory: Callable[[], Any]):
    """ì§€ì—° ì„œë¹„ìŠ¤ ë“±ë¡ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    container = get_global_container()
    container.register_lazy(key, factory)

# ==============================================
# ğŸ”¥ ì´ˆê¸°í™” í•¨ìˆ˜ë“¤ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def initialize_di_system_safe() -> bool:
    """DI ì‹œìŠ¤í…œ ì•ˆì „ ì´ˆê¸°í™” (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        container = get_global_container()
        
        # conda í™˜ê²½ ìµœì í™”
        if IS_CONDA:
            _optimize_for_conda_safe()
        
        logger.info("âœ… DI ì‹œìŠ¤í…œ ì•ˆì „ ì´ˆê¸°í™” ì™„ë£Œ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)")
        return True
        
    except Exception as e:
        logger.error(f"âŒ DI ì‹œìŠ¤í…œ ì•ˆì „ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

def _optimize_for_conda_safe():
    """conda í™˜ê²½ ì•ˆì „ ìµœì í™” (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['OMP_NUM_THREADS'] = str(max(1, os.cpu_count() // 2))
        os.environ['MKL_NUM_THREADS'] = str(max(1, os.cpu_count() // 2))
        os.environ['NUMEXPR_NUM_THREADS'] = str(max(1, os.cpu_count() // 2))
        
        # PyTorch ìµœì í™”
        if TORCH_AVAILABLE:
            import torch
            torch.set_num_threads(max(1, os.cpu_count() // 2))
            
            # M3 Max MPS ìµœì í™”
            if IS_M3_MAX and MPS_AVAILABLE:
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                logger.info("ğŸ M3 Max MPS conda ì•ˆì „ ìµœì í™” ì™„ë£Œ")
        
        logger.info(f"ğŸ conda í™˜ê²½ '{CONDA_ENV}' ì•ˆì „ ìµœì í™” ì™„ë£Œ")
        
    except Exception as e:
        logger.warning(f"âš ï¸ conda ì•ˆì „ ìµœì í™” ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def detect_actual_paths():
    """ì‹¤ì œ í”„ë¡œì íŠ¸ì˜ ê²½ë¡œë“¤ì„ íƒì§€"""
    import os
    from pathlib import Path
    
    try:
        # í˜„ì¬ ìœ„ì¹˜ì—ì„œ ì‹¤ì œ íŒŒì¼ë“¤ ì°¾ê¸°
        backend_root = Path(__file__).parent.parent.parent  # backend/
        
        paths_found = {}
        
        # MemoryManager ì°¾ê¸°
        memory_manager_paths = [
            backend_root / "app" / "services" / "memory_manager.py",
            backend_root / "app" / "ai_pipeline" / "utils" / "memory_manager.py"
        ]
        
        for path in memory_manager_paths:
            if path.exists():
                relative_path = str(path.relative_to(backend_root)).replace('/', '.').replace('.py', '')
                paths_found['memory_manager'] = relative_path
                logger.info(f"ğŸ“ MemoryManager ë°œê²¬: {relative_path}")
                break
        
        # ModelLoader/ModelManager ì°¾ê¸°  
        model_paths = [
            backend_root / "app" / "services" / "model_manager.py",
            backend_root / "app" / "ai_pipeline" / "utils" / "model_loader.py"
        ]
        
        for path in model_paths:
            if path.exists():
                relative_path = str(path.relative_to(backend_root)).replace('/', '.').replace('.py', '')
                paths_found['model_manager'] = relative_path
                logger.info(f"ğŸ“ ModelManager ë°œê²¬: {relative_path}")
                break
        
        return paths_found
    
    except Exception as e:
        logger.error(f"âŒ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
        return {}

# ==============================================
# ğŸ”¥ BaseStepMixinì—ì„œ ì‚¬ìš©í•˜ëŠ” DI Container ë™ì  í•´ê²° í•¨ìˆ˜
# ==============================================

def _get_global_di_container():
    """ì „ì—­ DI Container ì•ˆì „í•œ ë™ì  í•´ê²° (BaseStepMixin í˜¸í™˜)"""
    try:
        return get_global_container()
    except ImportError:
        return None

def _get_service_from_container_safe(service_key: str):
    """DI Containerë¥¼ í†µí•œ ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ (BaseStepMixin í˜¸í™˜)"""
    try:
        container = _get_global_di_container()
        if container:
            return container.get(service_key)
        return None
    except Exception:
        return None

# ==============================================
# ğŸ”¥ Export
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'CircularReferenceFreeDIContainer',
    'LazyDependency',
    'DynamicImportResolver',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_container',
    'reset_global_container',
    
    # ì•ˆì „ í•¨ìˆ˜ë“¤
    'inject_dependencies_to_step_safe',
    'get_service_safe',
    'register_service_safe',
    'register_lazy_service',
    'initialize_di_system_safe',
    'safe_copy',
    
    # BaseStepMixin í˜¸í™˜ í•¨ìˆ˜ë“¤
    '_get_global_di_container',
    '_get_service_from_container_safe',
    
    # ìœ í‹¸ë¦¬í‹°
    'detect_actual_paths',
    
    # íƒ€ì…ë“¤
    'T'
]

# ==============================================
# ğŸ”¥ ìë™ ì´ˆê¸°í™” (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

# ì´ˆê¸°í™” ì‹œ ì‹¤ì œ ê²½ë¡œ íƒì§€
logger.info("ğŸ” ì‹¤ì œ í”„ë¡œì íŠ¸ ê²½ë¡œ íƒì§€ ì¤‘...")
DETECTED_PATHS = detect_actual_paths()

if DETECTED_PATHS:
    logger.info(f"âœ… ë°œê²¬ëœ ê²½ë¡œë“¤: {DETECTED_PATHS}")
else:
    logger.warning("âš ï¸ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨, ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©")

# conda í™˜ê²½ ìë™ ìµœì í™”
if IS_CONDA:
    logger.info(f"ğŸ conda í™˜ê²½ '{CONDA_ENV}' ê°ì§€ - ì•ˆì „ ìë™ ìµœì í™” ì¤€ë¹„")

# ì™„ë£Œ ë©”ì‹œì§€
logger.info("âœ… DI Container v4.1 ë¡œë“œ ì™„ë£Œ (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ + safe_copy í¬í•¨)!")
logger.info("ğŸ”— ê¸°ì¡´ MyCloset AI í”„ë¡œì íŠ¸ 100% í˜¸í™˜")
logger.info("âš¡ TYPE_CHECKING + ì§€ì—° í•´ê²°ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ì°¨ë‹¨")
logger.info("ğŸ§µ ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë° ë©”ëª¨ë¦¬ ë³´í˜¸")
logger.info("ğŸ­ Mock í´ë°± êµ¬í˜„ì²´ í¬í•¨")
logger.info("ğŸ conda í™˜ê²½ ìš°ì„  ìµœì í™”")
logger.info("ğŸ”§ get_stats ë©”ì„œë“œ ì¶”ê°€ë¡œ ì†ì„± ì˜¤ë¥˜ í•´ê²°")
logger.info("ğŸ“‹ safe_copy í•¨ìˆ˜ í¬í•¨ìœ¼ë¡œ ì•ˆì „í•œ ë°ì´í„° ë³µì‚¬")
logger.info("ğŸ”— BaseStepMixin í˜¸í™˜ í•¨ìˆ˜ë“¤ ì¶”ê°€")
logger.info("ğŸ¯ ModelLoader DI Container ì£¼ì… ì§€ì›")

if IS_M3_MAX:
    logger.info("ğŸ M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™”")

logger.info("ğŸš€ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ DI Container v4.1 ì¤€ë¹„ ì™„ë£Œ!")