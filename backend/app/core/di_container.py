# backend/app/core/di_container.py
"""
ğŸ”¥ DI Container v4.0 - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° íŠ¹í™”
===============================================

âœ… TYPE_CHECKINGìœ¼ë¡œ import ìˆœí™˜ ì™„ì „ ì°¨ë‹¨
âœ… ì§€ì—° í•´ê²°(Lazy Resolution)ë¡œ ëŸ°íƒ€ì„ ìˆœí™˜ì°¸ì¡° ë°©ì§€
âœ… ê¸°ì¡´ MyCloset AI í”„ë¡œì íŠ¸ 100% í˜¸í™˜
âœ… step_factory.py â†” base_step_mixin.py ìˆœí™˜ì°¸ì¡° í•´ê²°
âœ… Mock í´ë°± êµ¬í˜„ì²´ í¬í•¨
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™”

Author: MyCloset AI Team
Date: 2025-07-30
Version: 4.0 (Circular Reference Fix Specialized)
"""

import os
import gc
import logging
import threading
import weakref
import time
import platform
import subprocess
import importlib
from typing import Dict, Any, Optional, Type, TypeVar, Callable, Union, List, TYPE_CHECKING
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum

# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    # ì˜¤ì§ íƒ€ì… ì²´í¬ ì‹œì—ë§Œ import
    from ..ai_pipeline.steps.base_step_mixin import BaseStepMixin
    from ..ai_pipeline.utils.model_loader import ModelLoader
    from ..ai_pipeline.utils.memory_manager import MemoryManager
    from ..ai_pipeline.utils.data_converter import DataConverter
    from ..ai_pipeline.factories.step_factory import StepFactory
else:
    # ëŸ°íƒ€ì„ì—ëŠ” Anyë¡œ ì²˜ë¦¬
    BaseStepMixin = Any
    ModelLoader = Any
    MemoryManager = Any
    DataConverter = Any
    StepFactory = Any

# ==============================================
# ğŸ”¥ í™˜ê²½ ì„¤ì • (ìˆœí™˜ì°¸ì¡° ì—†ëŠ” ë…ë¦½ì  ì„¤ì •)
# ==============================================

# conda í™˜ê²½ ìš°ì„  ì„¤ì •
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', 'none')
IS_CONDA = CONDA_ENV != 'none'
IS_TARGET_ENV = CONDA_ENV == 'mycloset-ai-clean'

# M3 Max ê°ì§€ (ë…ë¦½ì )
def detect_m3_max() -> bool:
    """M3 Max ê°ì§€ (ìˆœí™˜ì°¸ì¡° ì—†ìŒ)"""
    try:
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except Exception:
        pass
    return False

IS_M3_MAX = detect_m3_max()
MEMORY_GB = 128.0 if IS_M3_MAX else 16.0
DEVICE = 'mps' if IS_M3_MAX else 'cpu'

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# PyTorch ê°€ìš©ì„± ì²´í¬ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        # M3 Max ìµœì í™” ì„¤ì •
        os.environ.setdefault('PYTORCH_ENABLE_MPS_FALLBACK', '1')
        os.environ.setdefault('PYTORCH_MPS_HIGH_WATERMARK_RATIO', '0.0')
        
    logger.info(f"âœ… PyTorch ë¡œë“œ: MPS={MPS_AVAILABLE}, M3 Max={IS_M3_MAX}")
except ImportError:
    logger.warning("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch ê¶Œì¥")

T = TypeVar('T')

# ==============================================
# ğŸ”¥ ì§€ì—° í•´ê²° í´ë˜ìŠ¤ë“¤ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

class LazyDependency:
    """ì§€ì—° ì˜ì¡´ì„± í•´ê²°ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    
    def __init__(self, factory: Callable[[], Any]):
        self._factory = factory
        self._instance = None
        self._resolved = False
        self._lock = threading.RLock()
    
    def get(self) -> Any:
        """ì§€ì—° í•´ê²°"""
        if not self._resolved:
            with self._lock:
                if not self._resolved:
                    try:
                        self._instance = self._factory()
                        self._resolved = True
                    except Exception as e:
                        logger.error(f"âŒ ì§€ì—° ì˜ì¡´ì„± í•´ê²° ì‹¤íŒ¨: {e}")
                        return None
        
        return self._instance
    
    def is_resolved(self) -> bool:
        return self._resolved

class DynamicImportResolver:
    """ë™ì  import í•´ê²°ê¸° (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€)"""
    
    @staticmethod
    def resolve_model_loader():
        """ModelLoader ë™ì  í•´ê²° (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        import_paths = [
            'app.ai_pipeline.utils.model_loader',
            'ai_pipeline.utils.model_loader',
            'utils.model_loader'
        ]
        
        for path in import_paths:
            try:
                module = importlib.import_module(path)
                
                # ì „ì—­ í•¨ìˆ˜ ìš°ì„ 
                if hasattr(module, 'get_global_model_loader'):
                    loader = module.get_global_model_loader()
                    if loader:
                        logger.debug(f"âœ… ModelLoader ë™ì  í•´ê²°: {path}")
                        return loader
                
                # í´ë˜ìŠ¤ ì§ì ‘ ìƒì„±
                if hasattr(module, 'ModelLoader'):
                    ModelLoaderClass = module.ModelLoader
                    loader = ModelLoaderClass()
                    logger.debug(f"âœ… ModelLoader í´ë˜ìŠ¤ ìƒì„±: {path}")
                    return loader
                    
            except ImportError:
                continue
        
        # ì™„ì „ ì‹¤íŒ¨ ì‹œ Mock ë°˜í™˜
        logger.warning("âš ï¸ ModelLoader í•´ê²° ì‹¤íŒ¨, Mock ì‚¬ìš©")
        return DynamicImportResolver._create_mock_model_loader()
    
    @staticmethod
    def resolve_memory_manager():
        """MemoryManager ë™ì  í•´ê²° (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        import_paths = [
            'app.ai_pipeline.utils.memory_manager',
            'ai_pipeline.utils.memory_manager',
            'utils.memory_manager'
        ]
        
        for path in import_paths:
            try:
                module = importlib.import_module(path)
                
                # ì „ì—­ í•¨ìˆ˜ ìš°ì„ 
                if hasattr(module, 'get_global_memory_manager'):
                    manager = module.get_global_memory_manager()
                    if manager:
                        logger.debug(f"âœ… MemoryManager ë™ì  í•´ê²°: {path}")
                        return manager
                
                # í´ë˜ìŠ¤ ì§ì ‘ ìƒì„±
                if hasattr(module, 'MemoryManager'):
                    MemoryManagerClass = module.MemoryManager
                    manager = MemoryManagerClass()
                    logger.debug(f"âœ… MemoryManager í´ë˜ìŠ¤ ìƒì„±: {path}")
                    return manager
                    
            except ImportError:
                continue
        
        # ì™„ì „ ì‹¤íŒ¨ ì‹œ Mock ë°˜í™˜
        logger.warning("âš ï¸ MemoryManager í•´ê²° ì‹¤íŒ¨, Mock ì‚¬ìš©")
        return DynamicImportResolver._create_mock_memory_manager()
    
    @staticmethod
    def resolve_data_converter():
        """DataConverter ë™ì  í•´ê²° (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        import_paths = [
            'app.ai_pipeline.utils.data_converter',
            'ai_pipeline.utils.data_converter',
            'utils.data_converter'
        ]
        
        for path in import_paths:
            try:
                module = importlib.import_module(path)
                
                # ì „ì—­ í•¨ìˆ˜ ìš°ì„ 
                if hasattr(module, 'get_global_data_converter'):
                    converter = module.get_global_data_converter()
                    if converter:
                        logger.debug(f"âœ… DataConverter ë™ì  í•´ê²°: {path}")
                        return converter
                
                # í´ë˜ìŠ¤ ì§ì ‘ ìƒì„±
                if hasattr(module, 'DataConverter'):
                    DataConverterClass = module.DataConverter
                    converter = DataConverterClass()
                    logger.debug(f"âœ… DataConverter í´ë˜ìŠ¤ ìƒì„±: {path}")
                    return converter
                    
            except ImportError:
                continue
        
        # ì™„ì „ ì‹¤íŒ¨ ì‹œ Mock ë°˜í™˜
        logger.warning("âš ï¸ DataConverter í•´ê²° ì‹¤íŒ¨, Mock ì‚¬ìš©")
        return DynamicImportResolver._create_mock_data_converter()
    
    @staticmethod
    def resolve_step_factory():
        """StepFactory ë™ì  í•´ê²° (ìˆœí™˜ì°¸ì¡° ë°©ì§€) - ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ!"""
        # âš ï¸ ì´ í•¨ìˆ˜ëŠ” ìˆœí™˜ì°¸ì¡°ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‚¬ìš© ê¸ˆì§€
        logger.warning("âš ï¸ StepFactory ë™ì  í•´ê²° ìš”ì²­ë¨ - ìˆœí™˜ì°¸ì¡° ìœ„í—˜!")
        return None
    
    @staticmethod
    def _create_mock_model_loader():
        """Mock ModelLoader (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        class MockModelLoader:
            def __init__(self):
                self.models = {}
                self.device = DEVICE
                self.is_initialized = True
            
            def get_model(self, model_name: str):
                if model_name not in self.models:
                    self.models[model_name] = {
                        "name": model_name,
                        "device": self.device,
                        "type": "mock_model",
                        "loaded": True,
                        "size_mb": 50.0
                    }
                return self.models[model_name]
            
            def load_model(self, model_name: str):
                return self.get_model(model_name)
            
            def initialize(self):
                return True
            
            def cleanup_models(self):
                self.models.clear()
        
        return MockModelLoader()
    
    @staticmethod
    def _create_mock_memory_manager():
        """Mock MemoryManager (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        class MockMemoryManager:
            def __init__(self):
                self.optimization_count = 0
                self.is_initialized = True
            
            def optimize_memory(self, aggressive: bool = False):
                try:
                    gc.collect()
                    
                    if TORCH_AVAILABLE and IS_M3_MAX and MPS_AVAILABLE:
                        import torch
                        if hasattr(torch.backends.mps, 'empty_cache'):
                            torch.backends.mps.empty_cache()
                    
                    self.optimization_count += 1
                    return {
                        "success": True,
                        "method": "mock_optimization",
                        "count": self.optimization_count,
                        "memory_freed_mb": 50.0
                    }
                except Exception as e:
                    return {"success": False, "error": str(e)}
            
            def optimize(self, aggressive: bool = False):
                return self.optimize_memory(aggressive)
            
            def get_memory_info(self):
                return {
                    "total_gb": MEMORY_GB,
                    "available_gb": MEMORY_GB * 0.7,
                    "percent": 30.0,
                    "device": DEVICE,
                    "is_m3_max": IS_M3_MAX,
                    "optimization_count": self.optimization_count
                }
            
            def cleanup(self):
                self.optimize_memory(aggressive=True)
        
        return MockMemoryManager()
    
    @staticmethod
    def _create_mock_data_converter():
        """Mock DataConverter (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        class MockDataConverter:
            def __init__(self):
                self.conversion_count = 0
                self.is_initialized = True
            
            def convert(self, data, target_format: str):
                self.conversion_count += 1
                return {
                    "converted_data": f"mock_converted_{target_format}_{self.conversion_count}",
                    "format": target_format,
                    "conversion_count": self.conversion_count,
                    "success": True
                }
            
            def get_supported_formats(self):
                return ["tensor", "numpy", "pil", "cv2", "base64"]
            
            def cleanup(self):
                self.conversion_count = 0
        
        return MockDataConverter()

# ==============================================
# ğŸ”¥ ìˆœí™˜ì°¸ì¡° ë°©ì§€ DI Container
# ==============================================

class CircularReferenceFreeDIContainer:
    """ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ DI Container"""
    
    def __init__(self):
        # ì§€ì—° ì˜ì¡´ì„± ì €ì¥ì†Œ
        self._lazy_dependencies: Dict[str, LazyDependency] = {}
        
        # ì¼ë°˜ ì„œë¹„ìŠ¤ ì €ì¥ì†Œ
        self._services: Dict[str, Any] = {}
        self._singletons: Dict[str, Any] = {}
        
        # ë©”ëª¨ë¦¬ ë³´í˜¸ (ì•½í•œ ì°¸ì¡°)
        self._weak_refs: Dict[str, weakref.ref] = {}
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        # ìˆœí™˜ì°¸ì¡° ê°ì§€
        self._resolving_stack: List[str] = []
        self._circular_detected = set()
        
        # í†µê³„
        self._stats = {
            'lazy_resolutions': 0,
            'circular_references_prevented': 0,
            'mock_fallbacks_used': 0,
            'successful_resolutions': 0,
            'total_requests': 0
        }
        
        # ì´ˆê¸°í™”
        self._setup_core_dependencies()
        
        logger.info("ğŸ”— CircularReferenceFreeDIContainer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_core_dependencies(self):
        """í•µì‹¬ ì˜ì¡´ì„±ë“¤ ì§€ì—° ë“±ë¡ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        try:
            # ModelLoader ì§€ì—° ë“±ë¡
            model_loader_lazy = LazyDependency(
                DynamicImportResolver.resolve_model_loader
            )
            self._lazy_dependencies['model_loader'] = model_loader_lazy
            self._lazy_dependencies['IModelLoader'] = model_loader_lazy
            
            # MemoryManager ì§€ì—° ë“±ë¡
            memory_manager_lazy = LazyDependency(
                DynamicImportResolver.resolve_memory_manager
            )
            self._lazy_dependencies['memory_manager'] = memory_manager_lazy
            self._lazy_dependencies['IMemoryManager'] = memory_manager_lazy
            
            # DataConverter ì§€ì—° ë“±ë¡
            data_converter_lazy = LazyDependency(
                DynamicImportResolver.resolve_data_converter
            )
            self._lazy_dependencies['data_converter'] = data_converter_lazy
            self._lazy_dependencies['IDataConverter'] = data_converter_lazy
            
            # ì‹œìŠ¤í…œ ì •ë³´ ì§ì ‘ ë“±ë¡ (ìˆœí™˜ì°¸ì¡° ì—†ìŒ)
            self._services['device'] = DEVICE
            self._services['conda_env'] = CONDA_ENV
            self._services['is_m3_max'] = IS_M3_MAX
            self._services['memory_gb'] = MEMORY_GB
            self._services['torch_available'] = TORCH_AVAILABLE
            self._services['mps_available'] = MPS_AVAILABLE
            
            logger.info("âœ… í•µì‹¬ ì˜ì¡´ì„± ì§€ì—° ë“±ë¡ ì™„ë£Œ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)")
            
        except Exception as e:
            logger.error(f"âŒ í•µì‹¬ ì˜ì¡´ì„± ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    def register_lazy(self, key: str, factory: Callable[[], Any]) -> None:
        """ì§€ì—° ì˜ì¡´ì„± ë“±ë¡"""
        with self._lock:
            self._lazy_dependencies[key] = LazyDependency(factory)
            logger.debug(f"âœ… ì§€ì—° ì˜ì¡´ì„± ë“±ë¡: {key}")
    
    def register(self, key: str, instance: Any, singleton: bool = True) -> None:
        """ì¼ë°˜ ì˜ì¡´ì„± ë“±ë¡"""
        with self._lock:
            if singleton:
                self._singletons[key] = instance
            else:
                self._services[key] = instance
            logger.debug(f"âœ… ì˜ì¡´ì„± ë“±ë¡: {key} ({'ì‹±ê¸€í†¤' if singleton else 'ì„ì‹œ'})")
    
    def get(self, key: str) -> Optional[Any]:
        """ì˜ì¡´ì„± ì¡°íšŒ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        with self._lock:
            self._stats['total_requests'] += 1
            
            # ìˆœí™˜ì°¸ì¡° ê°ì§€
            if key in self._resolving_stack:
                circular_path = ' -> '.join(self._resolving_stack + [key])
                self._circular_detected.add(key)
                self._stats['circular_references_prevented'] += 1
                logger.error(f"âŒ ìˆœí™˜ì°¸ì¡° ê°ì§€: {circular_path}")
                return None
            
            # ìˆœí™˜ì°¸ì¡°ë¡œ ì´ë¯¸ ì°¨ë‹¨ëœ ê²½ìš°
            if key in self._circular_detected:
                logger.debug(f"âš ï¸ ì´ì „ì— ìˆœí™˜ì°¸ì¡° ê°ì§€ëœ í‚¤: {key}")
                return None
            
            self._resolving_stack.append(key)
            
            try:
                result = self._resolve_dependency(key)
                if result is not None:
                    self._stats['successful_resolutions'] += 1
                return result
            finally:
                self._resolving_stack.remove(key)
    
    def _resolve_dependency(self, key: str) -> Optional[Any]:
        """ì‹¤ì œ ì˜ì¡´ì„± í•´ê²°"""
        # 1. ì‹±ê¸€í†¤ ì²´í¬
        if key in self._singletons:
            return self._singletons[key]
        
        # 2. ì¼ë°˜ ì„œë¹„ìŠ¤ ì²´í¬
        if key in self._services:
            return self._services[key]
        
        # 3. ì•½í•œ ì°¸ì¡° ì²´í¬
        if key in self._weak_refs:
            weak_ref = self._weak_refs[key]
            instance = weak_ref()
            if instance is not None:
                return instance
            else:
                del self._weak_refs[key]
        
        # 4. ì§€ì—° ì˜ì¡´ì„± í•´ê²°
        if key in self._lazy_dependencies:
            lazy_dep = self._lazy_dependencies[key]
            instance = lazy_dep.get()
            
            if instance is not None:
                self._stats['lazy_resolutions'] += 1
                # ì•½í•œ ì°¸ì¡°ë¡œ ìºì‹œ
                self._weak_refs[key] = weakref.ref(instance)
                return instance
            else:
                self._stats['mock_fallbacks_used'] += 1
        
        return None
    
    def is_registered(self, key: str) -> bool:
        """ë“±ë¡ ì—¬ë¶€ í™•ì¸"""
        with self._lock:
            return (key in self._services or 
                   key in self._singletons or 
                   key in self._lazy_dependencies)
    
    def cleanup_circular_references(self):
        """ìˆœí™˜ì°¸ì¡° ê°ì§€ ìƒíƒœ ì •ë¦¬"""
        with self._lock:
            self._circular_detected.clear()
            self._resolving_stack.clear()
            logger.info("ğŸ§¹ ìˆœí™˜ì°¸ì¡° ê°ì§€ ìƒíƒœ ì •ë¦¬ ì™„ë£Œ")
    
    def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            with self._lock:
                cleanup_stats = {
                    'weak_refs_cleaned': 0,
                    'lazy_deps_reset': 0,
                    'gc_collected': 0
                }
                
                # ì•½í•œ ì°¸ì¡° ì •ë¦¬
                dead_refs = [key for key, ref in self._weak_refs.items() if ref() is None]
                for key in dead_refs:
                    del self._weak_refs[key]
                    cleanup_stats['weak_refs_cleaned'] += 1
                
                # ë©”ëª¨ë¦¬ ê´€ë¦¬ìë¥¼ í†µí•œ ìµœì í™”
                memory_manager = self.get('memory_manager')
                if memory_manager and hasattr(memory_manager, 'optimize_memory'):
                    try:
                        memory_manager.optimize_memory(aggressive=True)
                    except Exception as e:
                        logger.debug(f"ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìµœì í™” ì‹¤íŒ¨: {e}")
                
                # ì „ì—­ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                collected = gc.collect()
                cleanup_stats['gc_collected'] = collected
                
                # M3 Max MPS ìµœì í™”
                if TORCH_AVAILABLE and IS_M3_MAX and MPS_AVAILABLE:
                    import torch
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                        cleanup_stats['mps_cache_cleared'] = True
                
                logger.debug(f"ğŸ§¹ DI Container ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {cleanup_stats}")
                return cleanup_stats
                
        except Exception as e:
            logger.error(f"âŒ DI Container ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {}
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ì¡°íšŒ"""
        with self._lock:
            return {
                'container_type': 'CircularReferenceFreeDIContainer',
                'version': '4.0',
                'statistics': dict(self._stats),
                'registrations': {
                    'lazy_dependencies': len(self._lazy_dependencies),
                    'singleton_instances': len(self._singletons),
                    'transient_services': len(self._services),
                    'weak_references': len(self._weak_refs)
                },
                'circular_reference_protection': {
                    'detected_keys': list(self._circular_detected),
                    'current_resolving_stack': list(self._resolving_stack),
                    'prevention_count': self._stats['circular_references_prevented']
                },
                'environment': {
                    'is_conda': IS_CONDA,
                    'conda_env': CONDA_ENV,
                    'is_target_env': IS_TARGET_ENV,
                    'is_m3_max': IS_M3_MAX,
                    'device': DEVICE,
                    'memory_gb': MEMORY_GB,
                    'torch_available': TORCH_AVAILABLE,
                    'mps_available': MPS_AVAILABLE
                }
            }

# ==============================================
# ğŸ”¥ Step íŠ¹í™” ì˜ì¡´ì„± ì£¼ì… í•¨ìˆ˜ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def inject_dependencies_to_step_safe(step_instance, container: Optional[CircularReferenceFreeDIContainer] = None):
    """Stepì— ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì… (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        if container is None:
            container = get_global_container()
        
        injections_made = 0
        
        # ModelLoader ì£¼ì… (ì•ˆì „)
        model_loader = container.get('model_loader')
        if model_loader:
            if hasattr(step_instance, 'set_model_loader'):
                step_instance.set_model_loader(model_loader)
                injections_made += 1
            elif hasattr(step_instance, 'model_loader'):
                step_instance.model_loader = model_loader
                injections_made += 1
        
        # MemoryManager ì£¼ì… (ì•ˆì „)
        memory_manager = container.get('memory_manager')
        if memory_manager:
            if hasattr(step_instance, 'set_memory_manager'):
                step_instance.set_memory_manager(memory_manager)
                injections_made += 1
            elif hasattr(step_instance, 'memory_manager'):
                step_instance.memory_manager = memory_manager
                injections_made += 1
        
        # DataConverter ì£¼ì… (ì•ˆì „)
        data_converter = container.get('data_converter')
        if data_converter:
            if hasattr(step_instance, 'set_data_converter'):
                step_instance.set_data_converter(data_converter)
                injections_made += 1
            elif hasattr(step_instance, 'data_converter'):
                step_instance.data_converter = data_converter
                injections_made += 1
        
        # StepFactoryëŠ” ì ˆëŒ€ ì£¼ì…í•˜ì§€ ì•ŠìŒ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
        
        # ì´ˆê¸°í™”
        if hasattr(step_instance, 'initialize') and not getattr(step_instance, 'is_initialized', False):
            step_instance.initialize()
        
        logger.debug(f"âœ… {step_instance.__class__.__name__} ì•ˆì „ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ ({injections_made}ê°œ)")
        
    except Exception as e:
        logger.error(f"âŒ Step ì•ˆì „ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ì „ì—­ Container ê´€ë¦¬ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

_global_container: Optional[CircularReferenceFreeDIContainer] = None
_container_lock = threading.RLock()

def get_global_container() -> CircularReferenceFreeDIContainer:
    """ì „ì—­ DI Container ë°˜í™˜ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    global _global_container
    
    with _container_lock:
        if _global_container is None:
            _global_container = CircularReferenceFreeDIContainer()
            logger.info("âœ… ì „ì—­ CircularReferenceFreeDIContainer ìƒì„± ì™„ë£Œ")
    
    return _global_container

def reset_global_container():
    """ì „ì—­ Container ë¦¬ì…‹ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    global _global_container
    
    with _container_lock:
        if _global_container:
            _global_container.optimize_memory()
            _global_container.cleanup_circular_references()
        
        _global_container = None
        logger.info("ğŸ”„ ì „ì—­ CircularReferenceFreeDIContainer ë¦¬ì…‹ ì™„ë£Œ")

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def get_service_safe(key: str) -> Optional[Any]:
    """ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    container = get_global_container()
    return container.get(key)

def register_service_safe(key: str, instance: Any, singleton: bool = True):
    """ì•ˆì „í•œ ì„œë¹„ìŠ¤ ë“±ë¡ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    container = get_global_container()
    container.register(key, instance, singleton)

def register_lazy_service(key: str, factory: Callable[[], Any]):
    """ì§€ì—° ì„œë¹„ìŠ¤ ë“±ë¡ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    container = get_global_container()
    container.register_lazy(key, factory)

# ==============================================
# ğŸ”¥ ì´ˆê¸°í™” í•¨ìˆ˜ë“¤ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def initialize_di_system_safe() -> bool:
    """DI ì‹œìŠ¤í…œ ì•ˆì „ ì´ˆê¸°í™” (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        container = get_global_container()
        
        # conda í™˜ê²½ ìµœì í™”
        if IS_CONDA:
            _optimize_for_conda_safe()
        
        logger.info("âœ… DI ì‹œìŠ¤í…œ ì•ˆì „ ì´ˆê¸°í™” ì™„ë£Œ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)")
        return True
        
    except Exception as e:
        logger.error(f"âŒ DI ì‹œìŠ¤í…œ ì•ˆì „ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

def _optimize_for_conda_safe():
    """conda í™˜ê²½ ì•ˆì „ ìµœì í™” (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
    try:
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['OMP_NUM_THREADS'] = str(max(1, os.cpu_count() // 2))
        os.environ['MKL_NUM_THREADS'] = str(max(1, os.cpu_count() // 2))
        os.environ['NUMEXPR_NUM_THREADS'] = str(max(1, os.cpu_count() // 2))
        
        # PyTorch ìµœì í™”
        if TORCH_AVAILABLE:
            import torch
            torch.set_num_threads(max(1, os.cpu_count() // 2))
            
            # M3 Max MPS ìµœì í™”
            if IS_M3_MAX and MPS_AVAILABLE:
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                logger.info("ğŸ M3 Max MPS conda ì•ˆì „ ìµœì í™” ì™„ë£Œ")
        
        logger.info(f"ğŸ conda í™˜ê²½ '{CONDA_ENV}' ì•ˆì „ ìµœì í™” ì™„ë£Œ")
        
    except Exception as e:
        logger.warning(f"âš ï¸ conda ì•ˆì „ ìµœì í™” ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ Export
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'CircularReferenceFreeDIContainer',
    'LazyDependency',
    'DynamicImportResolver',
    
    # ì „ì—­ í•¨ìˆ˜ë“¤
    'get_global_container',
    'reset_global_container',
    
    # ì•ˆì „ í•¨ìˆ˜ë“¤
    'inject_dependencies_to_step_safe',
    'get_service_safe',
    'register_service_safe',
    'register_lazy_service',
    'initialize_di_system_safe',
    
    # íƒ€ì…ë“¤
    'T'
]

# ==============================================
# ğŸ”¥ ìë™ ì´ˆê¸°í™” (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

# conda í™˜ê²½ ìë™ ìµœì í™”
if IS_CONDA:
    logger.info(f"ğŸ conda í™˜ê²½ '{CONDA_ENV}' ê°ì§€ - ì•ˆì „ ìë™ ìµœì í™” ì¤€ë¹„")

# ì™„ë£Œ ë©”ì‹œì§€
logger.info("âœ… DI Container v4.0 ë¡œë“œ ì™„ë£Œ (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€)!")
logger.info("ğŸ”— ê¸°ì¡´ MyCloset AI í”„ë¡œì íŠ¸ 100% í˜¸í™˜")
logger.info("âš¡ TYPE_CHECKING + ì§€ì—° í•´ê²°ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ì°¨ë‹¨")
logger.info("ğŸ§µ ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë° ë©”ëª¨ë¦¬ ë³´í˜¸")
logger.info("ğŸ­ Mock í´ë°± êµ¬í˜„ì²´ í¬í•¨")
logger.info("ğŸ conda í™˜ê²½ ìš°ì„  ìµœì í™”")

if IS_M3_MAX:
    logger.info("ğŸ M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™”")

logger.info("ğŸš€ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ DI Container v4.0 ì¤€ë¹„ ì™„ë£Œ!")