"""
ë‹¨ìˆœí™”ëœ PipelineManager - Step í´ë˜ìŠ¤ë“¤ë§Œ ê´€ë¦¬
âœ… ê¸°ì¡´ í•¨ìˆ˜ëª…/í´ë˜ìŠ¤ëª… 100% ìœ ì§€
âœ… Step í´ë˜ìŠ¤ë“¤ì˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ë§Œ ë‹´ë‹¹
âœ… ê° Stepì´ ìì²´ì ìœ¼ë¡œ ëª¨ë¸/ë©”ëª¨ë¦¬ ê´€ë¦¬
âœ… M3 Max ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

íŒŒì¼ ìœ„ì¹˜: backend/app/ai_pipeline/pipeline_manager.py
"""

import os
import sys
import logging
import asyncio
import time
import traceback
import threading
import json
import gc
from pathlib import Path
from typing import Dict, Any, Optional, Callable, Union, List, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image, ImageEnhance, ImageFilter

# Step í´ë˜ìŠ¤ë“¤ë§Œ import (ë‹¨ë°©í–¥ ì˜ì¡´ì„±)
from app.ai_pipeline.steps.step_01_human_parsing import HumanParsingStep
from app.ai_pipeline.steps.step_02_pose_estimation import PoseEstimationStep
from app.ai_pipeline.steps.step_03_cloth_segmentation import ClothSegmentationStep
from app.ai_pipeline.steps.step_04_geometric_matching import GeometricMatchingStep
from app.ai_pipeline.steps.step_05_cloth_warping import ClothWarpingStep
from app.ai_pipeline.steps.step_06_virtual_fitting import VirtualFittingStep
from app.ai_pipeline.steps.step_07_post_processing import PostProcessingStep
from app.ai_pipeline.steps.step_08_quality_assessment import QualityAssessmentStep

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==============================================
# 1. ì—´ê±°í˜• ë° ìƒìˆ˜ ì •ì˜
# ==============================================

class PipelineMode(Enum):
    """íŒŒì´í”„ë¼ì¸ ëª¨ë“œ"""
    DEVELOPMENT = "development"
    PRODUCTION = "production"
    TESTING = "testing"
    OPTIMIZATION = "optimization"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    MAXIMUM = "maximum"

class ProcessingStatus(Enum):
    """ì²˜ë¦¬ ìƒíƒœ"""
    IDLE = "idle"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CLEANING = "cleaning"

# ==============================================
# 2. ë°ì´í„° í´ë˜ìŠ¤
# ==============================================

@dataclass
class PipelineConfig:
    """íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    device: str = "auto"
    quality_level: Union[QualityLevel, str] = QualityLevel.BALANCED
    processing_mode: Union[PipelineMode, str] = PipelineMode.PRODUCTION
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    memory_gb: float = 16.0
    is_m3_max: bool = False
    device_type: str = "auto"
    
    # ìµœì í™” ì„¤ì •
    optimization_enabled: bool = True
    enable_caching: bool = True
    parallel_processing: bool = True
    memory_optimization: bool = True
    use_fp16: bool = True
    enable_quantization: bool = False
    
    # ì²˜ë¦¬ ì„¤ì •
    batch_size: int = 1
    max_retries: int = 3
    timeout_seconds: int = 300
    save_intermediate: bool = False
    enable_progress_callback: bool = True
    
    # ê³ ê¸‰ ì„¤ì •
    model_cache_size: int = 10
    memory_threshold: float = 0.8
    gpu_memory_fraction: float = 0.9
    thread_pool_size: int = 4
    
    def __post_init__(self):
        # ë¬¸ìì—´ì„ Enumìœ¼ë¡œ ë³€í™˜
        if isinstance(self.quality_level, str):
            self.quality_level = QualityLevel(self.quality_level)
        if isinstance(self.processing_mode, str):
            self.processing_mode = PipelineMode(self.processing_mode)
        
        # M3 Max ìë™ ìµœì í™”
        if self.is_m3_max:
            self.memory_gb = max(self.memory_gb, 64.0)
            self.use_fp16 = True
            self.optimization_enabled = True
            self.batch_size = 4
            self.model_cache_size = 15
            self.gpu_memory_fraction = 0.95

@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼"""
    success: bool
    session_id: str = ""
    result_image: Optional[Image.Image] = None
    result_tensor: Optional[torch.Tensor] = None
    quality_score: float = 0.0
    quality_grade: str = "Unknown"
    processing_time: float = 0.0
    step_results: Dict[str, Any] = field(default_factory=dict)
    step_timings: Dict[str, float] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'success': self.success,
            'session_id': self.session_id,
            'quality_score': self.quality_score,
            'quality_grade': self.quality_grade,
            'processing_time': self.processing_time,
            'step_results': self.step_results,
            'step_timings': self.step_timings,
            'metadata': self.metadata,
            'error_message': self.error_message,
            'warnings': self.warnings
        }

@dataclass
class SessionData:
    """ì„¸ì…˜ ë°ì´í„°"""
    session_id: str
    start_time: float
    status: ProcessingStatus = ProcessingStatus.IDLE
    step_results: Dict[str, Any] = field(default_factory=dict)
    step_timings: Dict[str, float] = field(default_factory=dict)
    intermediate_results: Dict[str, Any] = field(default_factory=dict)
    error_log: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def add_step_result(self, step_name: str, result: Dict[str, Any], timing: float):
        """ë‹¨ê³„ ê²°ê³¼ ì¶”ê°€"""
        self.step_results[step_name] = result
        self.step_timings[step_name] = timing

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    total_sessions: int = 0
    successful_sessions: int = 0
    failed_sessions: int = 0
    average_processing_time: float = 0.0
    average_quality_score: float = 0.0
    total_processing_time: float = 0.0
    fastest_processing_time: float = float('inf')
    slowest_processing_time: float = 0.0
    
    def update(self, processing_time: float, quality_score: float, success: bool):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.total_sessions += 1
        self.total_processing_time += processing_time
        
        if success:
            self.successful_sessions += 1
            self.fastest_processing_time = min(self.fastest_processing_time, processing_time)
            self.slowest_processing_time = max(self.slowest_processing_time, processing_time)
        else:
            self.failed_sessions += 1
        
        # í‰ê·  ê³„ì‚°
        if self.total_sessions > 0:
            self.average_processing_time = self.total_processing_time / self.total_sessions
        
        if self.successful_sessions > 0:
            prev_total = self.average_quality_score * (self.successful_sessions - 1)
            self.average_quality_score = (prev_total + quality_score) / self.successful_sessions

# ==============================================
# 3. ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤ (ìµœì†Œí•œë§Œ ìœ ì§€)
# ==============================================

class SimpleDataConverter:
    """ê°„ë‹¨í•œ ë°ì´í„° ë³€í™˜ê¸° - PipelineManagerìš©"""
    
    def __init__(self, device: str = "mps"):
        self.device = device
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
    
    def preprocess_image(self, image_input: Union[str, Image.Image, np.ndarray]) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ë³€í™˜
            if isinstance(image_input, str):
                if not os.path.exists(image_input):
                    raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_input}")
                image = Image.open(image_input).convert('RGB')
            elif isinstance(image_input, Image.Image):
                image = image_input.convert('RGB')
            elif isinstance(image_input, np.ndarray):
                if image_input.dtype != np.uint8:
                    image_input = (image_input * 255).astype(np.uint8)
                image = Image.fromarray(image_input).convert('RGB')
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_input)}")
            
            # í¬ê¸° ì¡°ì •
            target_size = (512, 512)
            if image.size != target_size:
                image = image.resize(target_size, Image.Resampling.LANCZOS)
            
            # í…ì„œ ë³€í™˜
            img_array = np.array(image)
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0
            img_tensor = img_tensor.unsqueeze(0)
            
            return img_tensor
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            
            if tensor.shape[0] == 3:
                tensor = tensor.permute(1, 2, 0)
            
            tensor = torch.clamp(tensor, 0, 1)
            tensor = tensor.cpu()
            array = (tensor.numpy() * 255).astype(np.uint8)
            
            return Image.fromarray(array)
            
        except Exception as e:
            self.logger.error(f"í…ì„œ-PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (512, 512), color='black')

class SimpleMemoryManager:
    """ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ì - PipelineManagerìš©"""
    
    def __init__(self, device: str = "mps"):
        self.device = device
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            gc.collect()
            
            if self.device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            elif self.device == "mps" and torch.backends.mps.is_available():
                try:
                    torch.mps.empty_cache()
                    torch.mps.synchronize()
                except AttributeError:
                    pass
                
            self.logger.debug("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìƒì„¸ ì •ë³´"""
        try:
            usage = {}
            
            if self.device == "cuda" and torch.cuda.is_available():
                usage.update({
                    'allocated_gb': torch.cuda.memory_allocated() / 1024**3,
                    'cached_gb': torch.cuda.memory_reserved() / 1024**3,
                    'total_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3
                })
            elif self.device == "mps":
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    usage.update({
                        'used_gb': memory.used / 1024**3,
                        'available_gb': memory.available / 1024**3,
                        'total_gb': memory.total / 1024**3,
                        'percent': memory.percent
                    })
                except ImportError:
                    usage['status'] = 'psutil not available'
            
            return usage
            
        except Exception as e:
            self.logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}

# ==============================================
# 4. ë©”ì¸ PipelineManager í´ë˜ìŠ¤ (ë‹¨ìˆœí™”)
# ==============================================

class PipelineManager:
    """
    ë‹¨ìˆœí™”ëœ PipelineManager - Step í´ë˜ìŠ¤ë“¤ë§Œ ê´€ë¦¬
    
    ì—­í• :
    - Step í´ë˜ìŠ¤ë“¤ì˜ ì´ˆê¸°í™” ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
    - 8ë‹¨ê³„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    - ì„¸ì…˜ ê´€ë¦¬ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    - ê¸°ë³¸ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬
    
    ê° Stepì´ ë‹´ë‹¹:
    - ìì²´ ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬
    - ìƒì„¸í•œ ë©”ëª¨ë¦¬ ìµœì í™”
    - ì‹¤ì œ AI ì²˜ë¦¬ ë¡œì§
    """
    
    def __init__(
        self,
        config_path: Optional[str] = None,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], PipelineConfig]] = None,
        **kwargs
    ):
        """íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        
        # 1. ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._auto_detect_device(device)
        
        # 2. ì„¤ì • ì´ˆê¸°í™”
        if isinstance(config, PipelineConfig):
            self.config = config
        else:
            config_dict = self._load_config(config_path) if config_path else {}
            if config:
                config_dict.update(config if isinstance(config, dict) else {})
            config_dict.update(kwargs)
            
            self.config = PipelineConfig(
                device=self.device,
                **config_dict
            )
        
        # 3. ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€
        self.device_type = self._detect_device_type()
        self.memory_gb = self._detect_memory_gb()
        self.is_m3_max = self._detect_m3_max()
        
        # ì„¤ì • ì—…ë°ì´íŠ¸
        self.config.device_type = self.device_type
        self.config.memory_gb = self.memory_gb
        self.config.is_m3_max = self.is_m3_max
        
        # 4. ê°„ë‹¨í•œ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™”
        self.data_converter = SimpleDataConverter(self.device)
        self.memory_manager = SimpleMemoryManager(self.device)
        
        # 5. íŒŒì´í”„ë¼ì¸ ìƒíƒœ
        self.is_initialized = False
        self.current_status = ProcessingStatus.IDLE
        self.steps = {}
        self.step_order = [
            'human_parsing',
            'pose_estimation', 
            'cloth_segmentation',
            'geometric_matching',
            'cloth_warping',
            'virtual_fitting',
            'post_processing',
            'quality_assessment'
        ]
        
        # 6. ì„¸ì…˜ ê´€ë¦¬
        self.sessions: Dict[str, SessionData] = {}
        self.performance_metrics = PerformanceMetrics()
        
        # 7. ë™ì‹œì„± ê´€ë¦¬
        self._lock = threading.RLock()
        self.thread_pool = ThreadPoolExecutor(max_workers=self.config.thread_pool_size)
        
        # 8. ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        # 9. ë””ë°”ì´ìŠ¤ ìµœì í™”
        self._configure_device_optimizations()
        
        # 10. ì´ˆê¸°í™” ì™„ë£Œ ë¡œê¹…
        self.logger.info(f"âœ… PipelineManager ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {self.device} ({self.device_type})")
        self.logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬: {self.memory_gb}GB, M3 Max: {'âœ…' if self.is_m3_max else 'âŒ'}")
        self.logger.info(f"âš™ï¸ ì„¤ì •: {self.config.quality_level.value} í’ˆì§ˆ, {self.config.processing_mode.value} ëª¨ë“œ")
        
        # 11. ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™”
        self.memory_manager.cleanup_memory()
    
    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device and preferred_device != "auto":
            return preferred_device
        
        try:
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'
    
    def _detect_device_type(self) -> str:
        """ë””ë°”ì´ìŠ¤ íƒ€ì… ê°ì§€"""
        if self.device == 'mps':
            return 'apple_silicon'
        elif self.device == 'cuda':
            return 'nvidia'
        else:
            return 'cpu'
    
    def _detect_memory_gb(self) -> float:
        """ë©”ëª¨ë¦¬ ìš©ëŸ‰ ê°ì§€"""
        try:
            import psutil
            return round(psutil.virtual_memory().total / (1024**3), 1)
        except ImportError:
            return 16.0
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                    capture_output=True, text=True, timeout=5
                )
                chip_info = result.stdout.strip()
                return 'M3' in chip_info and 'Max' in chip_info
        except:
            pass
        return False
    
    def _configure_device_optimizations(self):
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •"""
        if self.device == 'mps':
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            if torch.backends.mps.is_available():
                try:
                    torch.mps.empty_cache()
                except:
                    pass
            self.logger.info("ğŸ”§ M3 Max MPS ìµœì í™” ì„¤ì • ì™„ë£Œ")
        elif self.device == 'cuda':
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            if self.config.optimization_enabled:
                torch.backends.cudnn.enabled = True
            self.logger.info("ğŸ”§ CUDA ìµœì í™” ì„¤ì • ì™„ë£Œ")
        
        # í˜¼í•© ì •ë°€ë„ ì„¤ì •
        if self.device in ['cuda', 'mps'] and self.config.use_fp16:
            self.use_amp = True
            self.logger.info("âš¡ í˜¼í•© ì •ë°€ë„ ì—°ì‚° í™œì„±í™”")
        else:
            self.use_amp = False
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if not config_path or not os.path.exists(config_path):
            return {}
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.warning(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    async def initialize(self) -> bool:
        """íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” - Step í´ë˜ìŠ¤ë“¤ë§Œ ì´ˆê¸°í™”"""
        try:
            self.logger.info("ğŸ”„ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹œì‘...")
            self.current_status = ProcessingStatus.INITIALIZING
            start_time = time.time()
            
            # 1. ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
            
            # 2. ê° Step í´ë˜ìŠ¤ ì´ˆê¸°í™”
            await self._initialize_all_steps()
            
            # 3. ì´ˆê¸°í™” ê²€ì¦
            success_rate = self._verify_initialization()
            if success_rate < 0.5:  # 50% ì´ìƒ ì„±ê³µí•´ì•¼ í•¨
                raise RuntimeError(f"ì´ˆê¸°í™” ì„±ê³µë¥  ë¶€ì¡±: {success_rate:.1%}")
            
            initialization_time = time.time() - start_time
            self.is_initialized = True
            self.current_status = ProcessingStatus.IDLE
            
            self.logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
            self.logger.info(f"â±ï¸ ì´ˆê¸°í™” ì‹œê°„: {initialization_time:.2f}ì´ˆ")
            self.logger.info(f"ğŸ“Š ì´ˆê¸°í™” ì„±ê³µë¥ : {success_rate:.1%}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            self.is_initialized = False
            self.current_status = ProcessingStatus.FAILED
            return False
    
    async def _initialize_all_steps(self):
        """ëª¨ë“  Step í´ë˜ìŠ¤ ì´ˆê¸°í™”"""
        
        # ê¸°ë³¸ ì„¤ì • - Stepë“¤ì´ ìì²´ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ ìµœì†Œí•œë§Œ ì „ë‹¬
        base_config = {
            'quality_level': self.config.quality_level.value,
            'device': self.device,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'optimization_enabled': self.config.optimization_enabled,
            'memory_optimization': self.config.memory_optimization,
            'use_fp16': self.config.use_fp16,
            'batch_size': self.config.batch_size,
            'enable_quantization': self.config.enable_quantization
        }
        
        # Step í´ë˜ìŠ¤ ë§¤í•‘
        step_classes = {
            'human_parsing': HumanParsingStep,
            'pose_estimation': PoseEstimationStep,
            'cloth_segmentation': ClothSegmentationStep,
            'geometric_matching': GeometricMatchingStep,
            'cloth_warping': ClothWarpingStep,
            'virtual_fitting': VirtualFittingStep,
            'post_processing': PostProcessingStep,
            'quality_assessment': QualityAssessmentStep
        }
        
        # ê° Step í´ë˜ìŠ¤ ì´ˆê¸°í™”
        for step_name in self.step_order:
            self.logger.info(f"ğŸ”§ {step_name} ì´ˆê¸°í™” ì¤‘...")
            
            try:
                step_class = step_classes[step_name]
                step_config = {**base_config, **self._get_step_config(step_name)}
                
                # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± - ê° Stepì´ ìì²´ì ìœ¼ë¡œ ëª¨ë“  ê²ƒì„ ê´€ë¦¬
                step_instance = step_class(**step_config)
                
                # ì´ˆê¸°í™” ì‹¤í–‰ (Step ìì²´ ëª¨ë¸ ë¡œë”© ë“±)
                if hasattr(step_instance, 'initialize'):
                    await step_instance.initialize()
                
                self.steps[step_name] = step_instance
                self.logger.info(f"âœ… {step_name} ì´ˆê¸°í™” ì™„ë£Œ")
                
            except Exception as e:
                self.logger.error(f"âŒ {step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ë‹¨ê³„ëŠ” ê±´ë„ˆë›°ê¸°
                continue
    
    def _get_step_config(self, step_name: str) -> Dict[str, Any]:
        """ë‹¨ê³„ë³„ íŠ¹í™” ì„¤ì •"""
        step_configs = {
            'human_parsing': {
                'model_name': 'graphonomy',
                'num_classes': 20,
                'input_size': (512, 512)
            },
            'pose_estimation': {
                'model_type': 'mediapipe',
                'input_size': (368, 368),
                'confidence_threshold': 0.5
            },
            'cloth_segmentation': {
                'model_name': 'u2net',
                'background_threshold': 0.5,
                'post_process': True
            },
            'geometric_matching': {
                'tps_points': 25,
                'matching_threshold': 0.8
            },
            'cloth_warping': {
                'warping_method': 'tps',
                'physics_simulation': True
            },
            'virtual_fitting': {
                'blending_method': 'poisson',
                'seamless_cloning': True
            },
            'post_processing': {
                'enable_super_resolution': self.config.optimization_enabled,
                'enhance_faces': True
            },
            'quality_assessment': {
                'enable_detailed_analysis': True,
                'perceptual_metrics': True
            }
        }
        
        return step_configs.get(step_name, {})
    
    def _verify_initialization(self) -> float:
        """ì´ˆê¸°í™” ê²€ì¦"""
        total_steps = len(self.step_order)
        initialized_steps = len(self.steps)
        
        success_rate = initialized_steps / total_steps
        self.logger.info(f"ğŸ“Š ì´ˆê¸°í™” ìƒíƒœ: {initialized_steps}/{total_steps} ({success_rate:.1%})")
        
        return success_rate
    
    async def process_complete_virtual_fitting(
        self,
        person_image: Union[str, Image.Image, np.ndarray],
        clothing_image: Union[str, Image.Image, np.ndarray],
        body_measurements: Optional[Dict[str, float]] = None,
        clothing_type: str = "shirt",
        fabric_type: str = "cotton",
        style_preferences: Optional[Dict[str, Any]] = None,
        quality_target: float = 0.8,
        progress_callback: Optional[Callable] = None,
        save_intermediate: bool = None,
        session_id: Optional[str] = None
    ) -> ProcessingResult:
        """
        ì™„ì „í•œ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ - Step í´ë˜ìŠ¤ë“¤ë§Œ ì‚¬ìš©
        """
        if not self.is_initialized:
            raise RuntimeError("íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # ì„¤ì • ì²˜ë¦¬
        if save_intermediate is None:
            save_intermediate = self.config.save_intermediate
        
        if session_id is None:
            session_id = f"vf_{int(time.time())}_{np.random.randint(1000, 9999)}"
        
        start_time = time.time()
        self.current_status = ProcessingStatus.PROCESSING
        
        try:
            self.logger.info(f"ğŸ¯ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì‹œì‘ - ì„¸ì…˜ ID: {session_id}")
            self.logger.info(f"âš™ï¸ ì„¤ì •: {clothing_type} ({fabric_type}), ëª©í‘œ í’ˆì§ˆ: {quality_target}")
            
            # 1. ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            person_tensor = self.data_converter.preprocess_image(person_image)
            clothing_tensor = self.data_converter.preprocess_image(clothing_image)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            person_tensor = person_tensor.to(self.device)
            clothing_tensor = clothing_tensor.to(self.device)
            
            # 2. ì„¸ì…˜ ë°ì´í„° ì´ˆê¸°í™”
            session_data = SessionData(
                session_id=session_id,
                start_time=start_time,
                status=ProcessingStatus.PROCESSING,
                metadata={
                    'clothing_type': clothing_type,
                    'fabric_type': fabric_type,
                    'quality_target': quality_target,
                    'style_preferences': style_preferences or {},
                    'body_measurements': body_measurements,
                    'device': self.device,
                    'quality_level': self.config.quality_level.value
                }
            )
            
            self.sessions[session_id] = session_data
            
            if progress_callback:
                await progress_callback("ì…ë ¥ ì „ì²˜ë¦¬ ì™„ë£Œ", 5)
            
            # 3. ë©”ëª¨ë¦¬ ìµœì í™”
            if self.config.memory_optimization:
                self.memory_manager.cleanup_memory()
            
            # 4. 8ë‹¨ê³„ ìˆœì°¨ ì²˜ë¦¬ - ê° Stepì´ ìì²´ì ìœ¼ë¡œ ëª¨ë“  ê²ƒì„ ì²˜ë¦¬
            step_results = {}
            current_data = person_tensor
            
            for i, step_name in enumerate(self.step_order):
                if step_name not in self.steps:
                    self.logger.warning(f"âš ï¸ {step_name} ë‹¨ê³„ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆë›°ê¸°...")
                    continue
                
                step_start = time.time()
                step = self.steps[step_name]
                
                self.logger.info(f"ğŸ“‹ {i+1}/{len(self.step_order)} ë‹¨ê³„: {step_name} ì²˜ë¦¬ ì¤‘...")
                
                try:
                    # Stepë³„ ì²˜ë¦¬ ì‹¤í–‰ - ê° Stepì´ ëª¨ë“  ê²ƒì„ ìì²´ ê´€ë¦¬
                    step_result = await self._execute_step_with_retry(
                        step, step_name, current_data, clothing_tensor,
                        body_measurements, clothing_type, fabric_type,
                        style_preferences, self.config.max_retries
                    )
                    
                    step_time = time.time() - step_start
                    step_results[step_name] = step_result
                    
                    # ì„¸ì…˜ ë°ì´í„° ì—…ë°ì´íŠ¸
                    session_data.add_step_result(step_name, step_result, step_time)
                    
                    # ê²°ê³¼ ì—…ë°ì´íŠ¸
                    if step_result.get('success', True):
                        result_data = step_result.get('result')
                        if result_data is not None:
                            current_data = result_data
                    
                    # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
                    if save_intermediate:
                        session_data.intermediate_results[step_name] = {
                            'result': current_data,
                            'metadata': step_result
                        }
                    
                    # ë¡œê¹…
                    confidence = step_result.get('confidence', 0.8)
                    quality_score = step_result.get('quality_score', confidence)
                    self.logger.info(f"âœ… {i+1}ë‹¨ê³„ ì™„ë£Œ - ì‹œê°„: {step_time:.2f}ì´ˆ, ì‹ ë¢°ë„: {confidence:.3f}, í’ˆì§ˆ: {quality_score:.3f}")
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if progress_callback:
                        progress = 5 + (i + 1) * 85 // len(self.step_order)
                        await progress_callback(f"{step_name} ì™„ë£Œ", progress)
                    
                    # ë©”ëª¨ë¦¬ ìµœì í™” (ì¤‘ê°„ ë‹¨ê³„)
                    if self.config.memory_optimization and i % 2 == 0:
                        self.memory_manager.cleanup_memory()
                    
                except Exception as e:
                    self.logger.error(f"âŒ {i+1}ë‹¨ê³„ ({step_name}) ì‹¤íŒ¨: {e}")
                    step_time = time.time() - step_start
                    step_results[step_name] = {
                        'success': False,
                        'error': str(e),
                        'processing_time': step_time,
                        'confidence': 0.0,
                        'quality_score': 0.0
                    }
                    
                    session_data.add_step_result(step_name, step_results[step_name], step_time)
                    session_data.error_log.append(f"{step_name}: {str(e)}")
                    
                    # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                    continue
            
            # 5. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            total_time = time.time() - start_time
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
            if isinstance(current_data, torch.Tensor):
                result_image = self.data_converter.tensor_to_pil(current_data)
            else:
                result_image = Image.new('RGB', (512, 512), color='gray')
            
            # ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€
            quality_score = self._assess_simple_quality(step_results)
            quality_grade = self._get_quality_grade(quality_score)
            
            # ì„±ê³µ ì—¬ë¶€ ê²°ì •
            success = quality_score >= (quality_target * 0.8)  # 80% ì´ìƒ ë‹¬ì„±
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.performance_metrics.update(total_time, quality_score, success)
            
            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            session_data.status = ProcessingStatus.COMPLETED if success else ProcessingStatus.FAILED
            
            # ì„¸ì…˜ ë°ì´í„° ì •ë¦¬
            if not save_intermediate:
                self.sessions.pop(session_id, None)
            
            if progress_callback:
                await progress_callback("ì²˜ë¦¬ ì™„ë£Œ", 100)
            
            self.current_status = ProcessingStatus.IDLE
            
            # ê²°ê³¼ ë¡œê¹…
            self.logger.info(f"ğŸ‰ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì™„ë£Œ!")
            self.logger.info(f"â±ï¸ ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
            self.logger.info(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f} ({quality_grade})")
            self.logger.info(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {'âœ…' if quality_score >= quality_target else 'âŒ'}")
            
            return ProcessingResult(
                success=success,
                session_id=session_id,
                result_image=result_image,
                result_tensor=current_data if isinstance(current_data, torch.Tensor) else None,
                quality_score=quality_score,
                quality_grade=quality_grade,
                processing_time=total_time,
                step_results=step_results,
                step_timings=session_data.step_timings,
                metadata={
                    'device': self.device,
                    'device_type': self.device_type,
                    'is_m3_max': self.is_m3_max,
                    'quality_target': quality_target,
                    'quality_target_achieved': quality_score >= quality_target,
                    'total_steps': len(self.step_order),
                    'completed_steps': len(step_results),
                    'success_rate': len([r for r in step_results.values() if r.get('success', True)]) / len(step_results) if step_results else 0,
                    'processing_config': {
                        'quality_level': self.config.quality_level.value,
                        'processing_mode': self.config.processing_mode.value,
                        'optimization_enabled': self.config.optimization_enabled,
                        'memory_optimization': self.config.memory_optimization,
                        'use_fp16': self.config.use_fp16,
                        'batch_size': self.config.batch_size
                    },
                    'performance_metrics': {
                        'total_sessions': self.performance_metrics.total_sessions,
                        'success_rate': self.performance_metrics.successful_sessions / self.performance_metrics.total_sessions if self.performance_metrics.total_sessions > 0 else 0,
                        'average_processing_time': self.performance_metrics.average_processing_time,
                        'average_quality_score': self.performance_metrics.average_quality_score
                    },
                    'memory_usage': self.memory_manager.get_memory_usage(),
                    'session_data': session_data.__dict__ if save_intermediate else None
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            
            # ì—ëŸ¬ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.performance_metrics.update(time.time() - start_time, 0.0, False)
            
            self.current_status = ProcessingStatus.FAILED
            
            return ProcessingResult(
                success=False,
                session_id=session_id,
                processing_time=time.time() - start_time,
                error_message=str(e),
                metadata={
                    'device': self.device,
                    'error_type': type(e).__name__,
                    'error_location': traceback.format_exc(),
                    'session_data': self.sessions.get(session_id).__dict__ if session_id in self.sessions else None
                }
            )
    
    async def _execute_step_with_retry(self, step, step_name: str, current_data: torch.Tensor, 
                                     clothing_tensor: torch.Tensor, body_measurements: Optional[Dict],
                                     clothing_type: str, fabric_type: str, 
                                     style_preferences: Optional[Dict], max_retries: int) -> Dict[str, Any]:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ Step ì‹¤í–‰"""
        last_error = None
        
        for attempt in range(max_retries + 1):
            try:
                if attempt > 0:
                    self.logger.info(f"ğŸ”„ {step_name} ì¬ì‹œë„ {attempt}/{max_retries}")
                    # ì¬ì‹œë„ ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
                    self.memory_manager.cleanup_memory()
                    await asyncio.sleep(0.5)  # ì ì‹œ ëŒ€ê¸°
                
                # Step ì‹¤í–‰ - Stepì´ ìì²´ì ìœ¼ë¡œ ëª¨ë“  ê²ƒì„ ì²˜ë¦¬
                result = await self._execute_step(
                    step, step_name, current_data, clothing_tensor,
                    body_measurements, clothing_type, fabric_type, style_preferences
                )
                
                # ì„±ê³µ ì‹œ ë°˜í™˜
                if result.get('success', True):
                    return result
                else:
                    last_error = result.get('error', 'Unknown error')
                    
            except Exception as e:
                last_error = str(e)
                self.logger.warning(f"âš ï¸ {step_name} ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                
                if attempt < max_retries:
                    continue
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        return {
            'success': False,
            'error': last_error,
            'confidence': 0.0,
            'quality_score': 0.0,
            'processing_time': 0.0,
            'method': 'failed_after_retries'
        }
    
    async def _execute_step(self, step, step_name: str, current_data: torch.Tensor, 
                          clothing_tensor: torch.Tensor, body_measurements: Optional[Dict],
                          clothing_type: str, fabric_type: str, 
                          style_preferences: Optional[Dict]) -> Dict[str, Any]:
        """
        ê°œë³„ Step ì‹¤í–‰ - Stepì´ ìì²´ì ìœ¼ë¡œ ëª¨ë“  ê²ƒì„ ì²˜ë¦¬
        """
        try:
            # Stepë³„ ì²˜ë¦¬ ë¡œì§ - ê° Stepì˜ process ë©”ì„œë“œ í˜¸ì¶œ
            if step_name == 'human_parsing':
                result = await step.process(current_data)
                
            elif step_name == 'pose_estimation':
                result = await step.process(current_data)
                
            elif step_name == 'cloth_segmentation':
                result = await step.process(clothing_tensor, clothing_type=clothing_type)
                
            elif step_name == 'geometric_matching':
                result = await step.process(current_data, clothing_tensor, body_measurements)
                
            elif step_name == 'cloth_warping':
                result = await step.process(current_data, clothing_tensor, body_measurements, fabric_type)
                
            elif step_name == 'virtual_fitting':
                result = await step.process(current_data, clothing_tensor, style_preferences)
                
            elif step_name == 'post_processing':
                result = await step.process(current_data)
                
            elif step_name == 'quality_assessment':
                result = await step.process(current_data, clothing_tensor)
                
            else:
                # ê¸°ë³¸ ì²˜ë¦¬
                result = await step.process(current_data)
            
            # ê²°ê³¼ ê²€ì¦ ë° í‘œì¤€í™”
            if not result or not isinstance(result, dict):
                return {
                    'success': True,
                    'result': current_data,
                    'confidence': 0.8,
                    'quality_score': 0.8,
                    'processing_time': 0.1,
                    'method': 'default'
                }
            
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            if 'confidence' not in result:
                result['confidence'] = 0.8
            if 'quality_score' not in result:
                result['quality_score'] = result.get('confidence', 0.8)
            if 'success' not in result:
                result['success'] = True
            
            return result
            
        except Exception as e:
            self.logger.error(f"Step ì‹¤í–‰ ì‹¤íŒ¨ {step_name}: {e}")
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0,
                'processing_time': 0.0,
                'method': 'error'
            }
    
    def _assess_simple_quality(self, step_results: Dict[str, Any]) -> float:
        """ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€"""
        if not step_results:
            return 0.5
        
        # Stepë³„ í’ˆì§ˆ ì ìˆ˜ ìˆ˜ì§‘
        quality_scores = []
        confidence_scores = []
        
        for step_result in step_results.values():
            if isinstance(step_result, dict):
                confidence = step_result.get('confidence', 0.8)
                quality = step_result.get('quality_score', confidence)
                
                quality_scores.append(quality)
                confidence_scores.append(confidence)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        if quality_scores:
            avg_quality = sum(quality_scores) / len(quality_scores)
            avg_confidence = sum(confidence_scores) / len(confidence_scores)
            
            # ê°€ì¤‘ í‰ê· 
            overall_score = avg_quality * 0.6 + avg_confidence * 0.4
            return min(max(overall_score, 0.0), 1.0)
        
        return 0.5
    
    def _get_quality_grade(self, quality_score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if quality_score >= 0.9:
            return "Excellent"
        elif quality_score >= 0.8:
            return "Good"
        elif quality_score >= 0.7:
            return "Fair"
        elif quality_score >= 0.6:
            return "Poor"
        else:
            return "Very Poor"
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ"""
        return {
            'initialized': self.is_initialized,
            'current_status': self.current_status.value,
            'device': self.device,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'config': {
                'quality_level': self.config.quality_level.value,
                'processing_mode': self.config.processing_mode.value,
                'optimization_enabled': self.config.optimization_enabled,
                'memory_optimization': self.config.memory_optimization,
                'use_fp16': self.config.use_fp16,
                'batch_size': self.config.batch_size
            },
            'steps_status': {
                step_name: {
                    'loaded': step_name in self.steps,
                    'type': type(self.steps[step_name]).__name__ if step_name in self.steps else None,
                    'ready': step_name in self.steps and hasattr(self.steps[step_name], 'process')
                }
                for step_name in self.step_order
            },
            'performance_metrics': {
                'total_sessions': self.performance_metrics.total_sessions,
                'successful_sessions': self.performance_metrics.successful_sessions,
                'failed_sessions': self.performance_metrics.failed_sessions,
                'success_rate': self.performance_metrics.successful_sessions / self.performance_metrics.total_sessions if self.performance_metrics.total_sessions > 0 else 0,
                'average_processing_time': self.performance_metrics.average_processing_time,
                'average_quality_score': self.performance_metrics.average_quality_score
            },
            'memory_usage': self.memory_manager.get_memory_usage(),
            'active_sessions': len(self.sessions)
        }
    
    def get_session_info(self, session_id: str) -> Optional[Dict[str, Any]]:
        """ì„¸ì…˜ ì •ë³´ ì¡°íšŒ"""
        session = self.sessions.get(session_id)
        if session:
            return session.__dict__
        return None
    
    def list_active_sessions(self) -> List[Dict[str, Any]]:
        """í™œì„± ì„¸ì…˜ ëª©ë¡"""
        return [
            {
                'session_id': session_id,
                'status': session.status.value,
                'start_time': session.start_time,
                'elapsed_time': time.time() - session.start_time,
                'completed_steps': len(session.step_results),
                'total_steps': len(self.step_order)
            }
            for session_id, session in self.sessions.items()
        ]
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´"""
        return {
            'total_sessions': self.performance_metrics.total_sessions,
            'success_rate': self.performance_metrics.successful_sessions / self.performance_metrics.total_sessions if self.performance_metrics.total_sessions > 0 else 0,
            'average_processing_time': self.performance_metrics.average_processing_time,
            'average_quality_score': self.performance_metrics.average_quality_score,
            'fastest_time': self.performance_metrics.fastest_processing_time if self.performance_metrics.fastest_processing_time != float('inf') else 0,
            'slowest_time': self.performance_metrics.slowest_processing_time,
            'total_processing_time': self.performance_metrics.total_processing_time,
            'active_sessions': len(self.sessions),
            'device_info': {
                'device': self.device,
                'device_type': self.device_type,
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb
            }
        }
    
    def clear_session_history(self, keep_recent: int = 10):
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì •ë¦¬"""
        try:
            if len(self.sessions) <= keep_recent:
                return
            
            # ìµœê·¼ ì„¸ì…˜ë“¤ë§Œ ìœ ì§€
            sorted_sessions = sorted(
                self.sessions.items(),
                key=lambda x: x[1].start_time,
                reverse=True
            )
            
            sessions_to_keep = dict(sorted_sessions[:keep_recent])
            cleared_count = len(self.sessions) - len(sessions_to_keep)
            
            self.sessions = sessions_to_keep
            
            self.logger.info(f"ğŸ§¹ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì •ë¦¬ ì™„ë£Œ: {cleared_count}ê°œ ì„¸ì…˜ ì œê±°")
            
        except Exception as e:
            self.logger.error(f"âŒ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def warmup(self):
        """íŒŒì´í”„ë¼ì¸ ì›Œë°ì—…"""
        try:
            self.logger.info("ğŸ”¥ íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… ì‹œì‘...")
            
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
            dummy_person = Image.new('RGB', (512, 512), color=(100, 150, 200))
            dummy_cloth = Image.new('RGB', (512, 512), color=(200, 100, 100))
            
            # ì›Œë°ì—… ì‹¤í–‰
            result = await self.process_complete_virtual_fitting(
                person_image=dummy_person,
                clothing_image=dummy_cloth,
                clothing_type='shirt',
                fabric_type='cotton',
                quality_target=0.6,  # ë‚®ì€ ëª©í‘œë¡œ ë¹ ë¥¸ ì²˜ë¦¬
                save_intermediate=False,
                session_id="warmup_session"
            )
            
            if result.success:
                self.logger.info(f"âœ… ì›Œë°ì—… ì™„ë£Œ - ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
                return True
            else:
                self.logger.warning(f"âš ï¸ ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜: {result.error_message}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return False
    
    async def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ì²´í¬"""
        try:
            health_status = {
                'status': 'healthy',
                'timestamp': datetime.now().isoformat(),
                'pipeline_initialized': self.is_initialized,
                'current_status': self.current_status.value,
                'device': self.device,
                'checks': {}
            }
            
            # Stepë³„ ì²´í¬
            steps_healthy = 0
            for step_name in self.step_order:
                if step_name in self.steps:
                    step = self.steps[step_name]
                    has_process = hasattr(step, 'process')
                    steps_healthy += 1 if has_process else 0
            
            health_status['checks']['steps'] = {
                'status': 'ok' if steps_healthy >= len(self.step_order) * 0.8 else 'warning',
                'healthy_steps': steps_healthy,
                'total_steps': len(self.step_order)
            }
            
            # ë©”ëª¨ë¦¬ ì²´í¬
            try:
                memory_usage = self.memory_manager.get_memory_usage()
                health_status['checks']['memory'] = {
                    'status': 'ok',
                    'usage': memory_usage
                }
            except Exception as e:
                health_status['checks']['memory'] = {
                    'status': 'error',
                    'error': str(e)
                }
            
            # ì „ì²´ ìƒíƒœ ê²°ì •
            check_statuses = [check.get('status', 'error') for check in health_status['checks'].values()]
            if 'error' in check_statuses:
                health_status['status'] = 'unhealthy'
            elif 'warning' in check_statuses:
                health_status['status'] = 'degraded'
            
            return health_status
            
        except Exception as e:
            return {
                'status': 'error',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            self.current_status = ProcessingStatus.CLEANING
            
            # 1. ê° Step ì •ë¦¬
            for step_name, step in self.steps.items():
                try:
                    if hasattr(step, 'cleanup'):
                        await step.cleanup()
                    self.logger.info(f"âœ… {step_name} ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 2. ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì •ë¦¬
            try:
                self.memory_manager.cleanup_memory()
                self.logger.info("âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 3. ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            if hasattr(self, 'thread_pool'):
                try:
                    self.thread_pool.shutdown(wait=True)
                    self.logger.info("âœ… ìŠ¤ë ˆë“œ í’€ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œ í’€ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 4. ì„¸ì…˜ ë°ì´í„° ì •ë¦¬
            try:
                self.sessions.clear()
                self.logger.info("âœ… ì„¸ì…˜ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì„¸ì…˜ ë°ì´í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 5. ìƒíƒœ ì´ˆê¸°í™”
            self.is_initialized = False
            self.current_status = ProcessingStatus.IDLE
            
            self.logger.info("âœ… íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.current_status = ProcessingStatus.FAILED

# ==============================================
# 5. í¸ì˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í•¨ìˆ˜ëª… 100% ìœ ì§€)
# ==============================================

def create_pipeline(
    device: str = "auto",
    quality_level: str = "balanced",
    processing_mode: str = "production",
    **kwargs
) -> PipelineManager:
    """íŒŒì´í”„ë¼ì¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return PipelineManager(
        device=device,
        config=PipelineConfig(
            quality_level=QualityLevel(quality_level),
            processing_mode=PipelineMode(processing_mode),
            **kwargs
        )
    )

def create_development_pipeline(**kwargs) -> PipelineManager:
    """ê°œë°œìš© íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    return create_pipeline(
        quality_level="fast",
        processing_mode="development",
        optimization_enabled=False,
        save_intermediate=True,
        enable_caching=False,
        **kwargs
    )

def create_production_pipeline(**kwargs) -> PipelineManager:
    """í”„ë¡œë•ì…˜ìš© íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    return create_pipeline(
        quality_level="high",
        processing_mode="production",
        optimization_enabled=True,
        memory_optimization=True,
        enable_caching=True,
        parallel_processing=True,
        **kwargs
    )

def create_m3_max_pipeline(**kwargs) -> PipelineManager:
    """M3 Max ìµœì í™” íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    return PipelineManager(
        device="mps",
        config=PipelineConfig(
            quality_level=QualityLevel.HIGH,
            processing_mode=PipelineMode.PRODUCTION,
            memory_gb=128.0,
            is_m3_max=True,
            optimization_enabled=True,
            use_fp16=True,
            batch_size=4,
            memory_optimization=True,
            enable_caching=True,
            parallel_processing=True,
            model_cache_size=15,
            gpu_memory_fraction=0.95,
            **kwargs
        )
    )

def create_testing_pipeline(**kwargs) -> PipelineManager:
    """í…ŒìŠ¤íŠ¸ìš© íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    return create_pipeline(
        quality_level="fast",
        processing_mode="testing",
        optimization_enabled=False,
        save_intermediate=True,
        enable_caching=False,
        max_retries=1,
        timeout_seconds=60,
        **kwargs
    )

@lru_cache(maxsize=1)
def get_global_pipeline_manager(device: str = "auto") -> PipelineManager:
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤"""
    try:
        if device == "mps" and torch.backends.mps.is_available():
            return create_m3_max_pipeline()
        else:
            return create_production_pipeline(device=device)
    except Exception as e:
        logger.error(f"ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ìƒì„± ì‹¤íŒ¨: {e}")
        return create_pipeline(device="cpu", quality_level="fast")

# ==============================================
# 6. ë°ëª¨ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# ==============================================

async def demo_simplified_pipeline():
    """ë‹¨ìˆœí™”ëœ íŒŒì´í”„ë¼ì¸ ë°ëª¨"""
    
    print("ğŸ¯ ë‹¨ìˆœí™”ëœ PipelineManager ë°ëª¨ ì‹œì‘")
    print("=" * 50)
    
    # 1. íŒŒì´í”„ë¼ì¸ ìƒì„±
    print("1ï¸âƒ£ íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")
    pipeline = create_m3_max_pipeline()
    
    # 2. ì´ˆê¸°í™”
    print("2ï¸âƒ£ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
    success = await pipeline.initialize()
    if not success:
        print("âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # 3. ìƒíƒœ í™•ì¸
    print("3ï¸âƒ£ íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸...")
    status = pipeline.get_pipeline_status()
    print(f"ğŸ“Š ì´ˆê¸°í™” ìƒíƒœ: {status['initialized']}")
    print(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {status['device']} ({status['device_type']})")
    print(f"ğŸ“‹ ë¡œë“œëœ ë‹¨ê³„: {len([s for s in status['steps_status'].values() if s['loaded']])}/{len(status['steps_status'])}")
    
    # 4. í—¬ìŠ¤ì²´í¬
    print("4ï¸âƒ£ í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰...")
    health = await pipeline.health_check()
    print(f"ğŸ¥ í—¬ìŠ¤ ìƒíƒœ: {health['status']}")
    
    # 5. ê°€ìƒ í”¼íŒ… ì‹¤í–‰
    print("5ï¸âƒ£ ê°€ìƒ í”¼íŒ… ì‹¤í–‰...")
    
    try:
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        person_image = Image.new('RGB', (512, 512), color=(100, 150, 200))
        clothing_image = Image.new('RGB', (512, 512), color=(200, 100, 100))
        
        # ì§„í–‰ë¥  ì½œë°±
        async def progress_callback(message: str, percentage: int):
            print(f"ğŸ”„ {message}: {percentage}%")
        
        # ê°€ìƒ í”¼íŒ… ì²˜ë¦¬
        result = await pipeline.process_complete_virtual_fitting(
            person_image=person_image,
            clothing_image=clothing_image,
            clothing_type='shirt',
            fabric_type='cotton',
            body_measurements={'height': 175, 'weight': 70, 'chest': 95},
            style_preferences={'fit': 'regular', 'color': 'original'},
            quality_target=0.8,
            progress_callback=progress_callback,
            save_intermediate=True
        )
        
        if result.success:
            print(f"âœ… ê°€ìƒ í”¼íŒ… ì„±ê³µ!")
            print(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {result.quality_score:.3f} ({result.quality_grade})")
            print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
            print(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {'âœ…' if result.quality_score >= 0.8 else 'âŒ'}")
            print(f"ğŸ“‹ ì™„ë£Œëœ ë‹¨ê³„: {len(result.step_results)}/{len(pipeline.step_order)}")
            
            # ë‹¨ê³„ë³„ ê²°ê³¼ ì¶œë ¥
            print("\nğŸ“‹ ë‹¨ê³„ë³„ ê²°ê³¼:")
            for step_name, step_result in result.step_results.items():
                success_icon = "âœ…" if step_result.get('success', True) else "âŒ"
                confidence = step_result.get('confidence', 0.0)
                timing = result.step_timings.get(step_name, 0.0)
                print(f"  {success_icon} {step_name}: {confidence:.3f} ({timing:.2f}s)")
            
            # ê²°ê³¼ ì €ì¥
            if result.result_image:
                result.result_image.save('demo_result.jpg')
                print("ğŸ’¾ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥: demo_result.jpg")
        else:
            print(f"âŒ ê°€ìƒ í”¼íŒ… ì‹¤íŒ¨: {result.error_message}")
    
    except Exception as e:
        print(f"ğŸ’¥ ì˜ˆì™¸ ë°œìƒ: {e}")
    
    # 6. ì„±ëŠ¥ ìš”ì•½
    print("6ï¸âƒ£ ì„±ëŠ¥ ìš”ì•½...")
    performance = pipeline.get_performance_summary()
    print(f"ğŸ“ˆ ì´ ì„¸ì…˜: {performance['total_sessions']}")
    print(f"ğŸ“Š ì„±ê³µë¥ : {performance['success_rate']:.1%}")
    print(f"â±ï¸ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {performance['average_processing_time']:.2f}ì´ˆ")
    print(f"ğŸ¯ í‰ê·  í’ˆì§ˆ ì ìˆ˜: {performance['average_quality_score']:.3f}")
    
    # 7. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    print("7ï¸âƒ£ ë¦¬ì†ŒìŠ¤ ì •ë¦¬...")
    await pipeline.cleanup()
    print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
    
    print("\nğŸ‰ ë°ëª¨ ì™„ë£Œ!")

async def performance_test():
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ”¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 30)
    
    pipeline = create_m3_max_pipeline()
    await pipeline.initialize()
    
    # ì›Œë°ì—…
    await pipeline.warmup()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_count = 3
    results = []
    
    for i in range(test_count):
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ {i+1}/{test_count}")
        
        person_image = Image.new('RGB', (512, 512), color=(i*50, 100, 150))
        clothing_image = Image.new('RGB', (512, 512), color=(150, i*30, 100))
        
        result = await pipeline.process_complete_virtual_fitting(
            person_image=person_image,
            clothing_image=clothing_image,
            quality_target=0.7
        )
        
        results.append({
            'success': result.success,
            'processing_time': result.processing_time,
            'quality_score': result.quality_score,
            'completed_steps': len(result.step_results)
        })
        
        print(f"  âœ… ì™„ë£Œ: {result.processing_time:.2f}s, í’ˆì§ˆ: {result.quality_score:.3f}")
    
    # ê²°ê³¼ ë¶„ì„
    successful_results = [r for r in results if r['success']]
    
    if successful_results:
        avg_time = sum(r['processing_time'] for r in successful_results) / len(successful_results)
        avg_quality = sum(r['quality_score'] for r in successful_results) / len(successful_results)
        
        print(f"\nğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"  ì„±ê³µë¥ : {len(successful_results)}/{test_count} ({len(successful_results)/test_count:.1%})")
        print(f"  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ")
        print(f"  í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.3f}")
        print(f"  ì´ˆë‹¹ ì²˜ë¦¬ëŸ‰: {len(successful_results)/(sum(r['processing_time'] for r in successful_results)):.2f} ì‘ì—…/ì´ˆ")
    
    await pipeline.cleanup()
    print("ğŸ§¹ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

# ==============================================
# 7. ë©”ì¸ ì‹¤í–‰
# ==============================================

if __name__ == "__main__":
    print("ğŸ½ ë‹¨ìˆœí™”ëœ PipelineManager - Step í´ë˜ìŠ¤ë“¤ë§Œ ê´€ë¦¬")
    print("=" * 80)
    print("âœ¨ Step í´ë˜ìŠ¤ë“¤ì˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ë§Œ ë‹´ë‹¹")
    print("ğŸ”§ ê° Stepì´ ìì²´ì ìœ¼ë¡œ ëª¨ë¸/ë©”ëª¨ë¦¬ ê´€ë¦¬")
    print("ğŸ“‹ ê¸°ì¡´ í•¨ìˆ˜ëª…/í´ë˜ìŠ¤ëª… 100% ìœ ì§€")
    print("ğŸš€ M3 Max ìµœì í™”")
    print("ğŸ’ª í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
    print("=" * 80)
    
    import asyncio
    
    async def main():
        # 1. ë‹¨ìˆœí™”ëœ ë°ëª¨ ì‹¤í–‰
        await demo_simplified_pipeline()
        
        print("\n" + "="*50)
        
        # 2. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        await performance_test()
    
    # ì‹¤í–‰
    asyncio.run(main())