# backend/app/ai_pipeline/pipeline_manager.py
"""
ðŸ”¥ ì™„ì „ížˆ ìƒˆë¡œìš´ PipelineManager v10.0 - ì™„ì „ ê¸°ëŠ¥ êµ¬í˜„
=======================================================

âœ… ëª¨ë“  ê¸°ì¡´ í•¨ìˆ˜/í´ëž˜ìŠ¤ëª… 100% ìœ ì§€
âœ… AI ëª¨ë¸ 229GB ì™„ì „ í™œìš© (Stepë³„ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ)
âœ… StepFactory v7.0 ì™„ì „ ì—°ë™
âœ… BaseStepMixin v18.0 ì™„ì „ í˜¸í™˜
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (ë™ì  import)
âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì˜¤ë¥˜ ì™„ì „ í•´ê²°
âœ… conda í™˜ê²½ ì™„ë²½ ìµœì í™”
âœ… M3 Max 128GB ìµœì í™”
âœ… 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ì „ ìž‘ë™
âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°± ì™„ì „ êµ¬í˜„
âœ… ì‹¤ì œ AI ì¶”ë¡  íŒŒì´í”„ë¼ì¸
âœ… DI + ì–´ëŒ‘í„° íŒ¨í„´ ì™„ì „ í†µí•©

í•µì‹¬ ì•„í‚¤í…ì²˜:
1. StepManager: Step ìƒì„±/ê´€ë¦¬ (StepFactory ì—°ë™)
2. AIModelManager: AI ëª¨ë¸ ë¡œë”©/ê´€ë¦¬ (ì‹¤ì œ íŒŒì¼ ê²½ë¡œ)
3. ProcessingEngine: 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
4. ResourceManager: ë©”ëª¨ë¦¬/ë””ë°”ì´ìŠ¤ ìµœì í™”
5. ErrorManager: ì—ëŸ¬ ì²˜ë¦¬/í´ë°±/ë³µêµ¬

ì£¼ìš” í•´ê²°ì‚¬í•­:
- object bool can't be used in 'await' expression âœ… í•´ê²°
- QualityAssessmentStep has no attribute 'is_m3_max' âœ… í•´ê²°
- StepFactory ì—°ë™ âœ… ì™„ë£Œ
- AI ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  âœ… êµ¬í˜„
"""

import os
import sys
import logging
import asyncio
import time
import threading
import json
import gc
import traceback
import hashlib
import uuid
from pathlib import Path
from typing import Dict, Any, Optional, Callable, Union, List, Tuple, Type, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from abc import ABC, abstractmethod

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image, ImageEnhance, ImageFilter

# ==============================================
# ðŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
# ==============================================
if TYPE_CHECKING:
    from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
    from app.ai_pipeline.factories.step_factory import StepFactory
    from app.ai_pipeline.utils.model_loader import ModelLoader

# ì‹œìŠ¤í…œ ì •ë³´ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ==============================================
# ðŸ”¥ ì—´ê±°í˜• ë° ë°ì´í„° í´ëž˜ìŠ¤ ì •ì˜
# ==============================================

class PipelineMode(Enum):
    """íŒŒì´í”„ë¼ì¸ ëª¨ë“œ"""
    DEVELOPMENT = "development"
    PRODUCTION = "production"
    TESTING = "testing"
    OPTIMIZATION = "optimization"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    MAXIMUM = "maximum"

class ProcessingStatus(Enum):
    """ì²˜ë¦¬ ìƒíƒœ"""
    IDLE = "idle"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CLEANING = "cleaning"

class ExecutionStrategy(Enum):
    """ì‹¤í–‰ ì „ëžµ"""
    UNIFIED_AI = "unified_ai"
    STEP_FACTORY = "step_factory"
    MODEL_LOADER = "model_loader"
    BASIC_FALLBACK = "basic_fallback"

@dataclass
class PipelineConfig:
    """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    device: str = "auto"
    quality_level: Union[QualityLevel, str] = QualityLevel.HIGH
    processing_mode: Union[PipelineMode, str] = PipelineMode.PRODUCTION
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    memory_gb: float = 128.0
    is_m3_max: bool = True
    device_type: str = "apple_silicon"
    
    # AI ëª¨ë¸ ì„¤ì •
    ai_model_enabled: bool = True
    model_preload_enabled: bool = True
    model_cache_size: int = 20
    ai_inference_timeout: int = 120
    model_fallback_enabled: bool = True
    
    # DI ì„¤ì •
    use_dependency_injection: bool = True
    auto_inject_dependencies: bool = True
    lazy_loading_enabled: bool = True
    interface_based_design: bool = True
    enable_adapter_pattern: bool = True
    enable_runtime_injection: bool = True
    
    # ì„±ëŠ¥ ìµœì í™”
    performance_mode: str = "maximum"
    memory_optimization: bool = True
    gpu_memory_fraction: float = 0.95
    use_fp16: bool = True
    enable_quantization: bool = True
    parallel_processing: bool = True
    batch_processing: bool = True
    async_processing: bool = True
    
    # ì²˜ë¦¬ ì„¤ì •
    batch_size: int = 4
    max_retries: int = 2
    timeout_seconds: int = 300
    thread_pool_size: int = 8
    max_fallback_attempts: int = 2
    fallback_timeout: int = 30
    enable_smart_fallback: bool = True
    
    def __post_init__(self):
        if isinstance(self.quality_level, str):
            self.quality_level = QualityLevel(self.quality_level)
        if isinstance(self.processing_mode, str):
            self.processing_mode = PipelineMode(self.processing_mode)
        
        # M3 Max ìžë™ ìµœì í™”
        if self._detect_m3_max():
            self.is_m3_max = True
            self.memory_gb = max(self.memory_gb, 128.0)
            self.model_cache_size = 20
            self.batch_size = 4
            self.thread_pool_size = 8
            self.gpu_memory_fraction = 0.95
            self.performance_mode = "maximum"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                    capture_output=True, text=True, timeout=5
                )
                chip_info = result.stdout.strip()
                return 'M3' in chip_info and 'Max' in chip_info
        except:
            pass
        return False

@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼"""
    success: bool
    session_id: str = ""
    result_image: Optional[Image.Image] = None
    result_tensor: Optional[torch.Tensor] = None
    quality_score: float = 0.0
    quality_grade: str = "Unknown"
    processing_time: float = 0.0
    step_results: Dict[str, Any] = field(default_factory=dict)
    step_timings: Dict[str, float] = field(default_factory=dict)
    ai_models_used: Dict[str, str] = field(default_factory=dict)
    execution_strategies: Dict[str, str] = field(default_factory=dict)
    
    # ì¶”ê°€ ì •ë³´
    dependency_injection_info: Dict[str, Any] = field(default_factory=dict)
    adapter_pattern_info: Dict[str, Any] = field(default_factory=dict)
    interface_usage_info: Dict[str, Any] = field(default_factory=dict)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)

# ==============================================
# ðŸ”¥ Step ê´€ë¦¬ìž í´ëž˜ìŠ¤
# ==============================================

class StepManager:
    """Step ìƒì„± ë° ê´€ë¦¬ (StepFactory ì—°ë™)"""
    
    def __init__(self, config: PipelineConfig, device: str, logger: logging.Logger):
        self.config = config
        self.device = device
        self.logger = logger
        self.steps = {}
        self.step_factory = None
        self.step_order = [
            'human_parsing', 'pose_estimation', 'cloth_segmentation',
            'geometric_matching', 'cloth_warping', 'virtual_fitting',
            'post_processing', 'quality_assessment'
        ]
        self.step_id_mapping = {
            1: 'human_parsing',
            2: 'pose_estimation',
            3: 'cloth_segmentation',
            4: 'geometric_matching',
            5: 'cloth_warping',
            6: 'virtual_fitting',
            7: 'post_processing',
            8: 'quality_assessment'
        }
        self.step_name_to_id = {v: k for k, v in self.step_id_mapping.items()}
        
    async def initialize(self) -> bool:
        """Step ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            self.logger.info("ðŸ”§ StepManager ì´ˆê¸°í™” ì‹œìž‘...")
            
            # StepFactory ë™ì  ë¡œë”©
            success = await self._load_step_factory()
            if not success:
                self.logger.warning("âš ï¸ StepFactory ë¡œë”© ì‹¤íŒ¨, í´ë°± ëª¨ë“œë¡œ ì§„í–‰")
                return await self._initialize_fallback_steps()
            
            # StepFactoryë¥¼ í†µí•œ Step ìƒì„±
            return await self._create_steps_via_factory()
            
        except Exception as e:
            self.logger.error(f"âŒ StepManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return await self._initialize_fallback_steps()
    
    async def _load_step_factory(self) -> bool:
        """StepFactory ë™ì  ë¡œë”©"""
        try:
            # ë™ì  importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
            import importlib
            factory_module = importlib.import_module('app.ai_pipeline.factories.step_factory')
            get_global_factory = getattr(factory_module, 'get_global_step_factory', None)
            
            if get_global_factory:
                self.step_factory = get_global_factory()
                self.logger.info("âœ… StepFactory ë¡œë”© ì™„ë£Œ")
                return True
            else:
                self.logger.warning("âš ï¸ get_global_step_factory í•¨ìˆ˜ ì—†ìŒ")
                return False
                
        except ImportError as e:
            self.logger.debug(f"StepFactory import ì‹¤íŒ¨: {e}")
            return False
        except Exception as e:
            self.logger.warning(f"âš ï¸ StepFactory ë¡œë”© ì˜¤ë¥˜: {e}")
            return False
    
    async def _create_steps_via_factory(self) -> bool:
        """StepFactoryë¥¼ í†µí•œ Step ìƒì„±"""
        try:
            if not self.step_factory:
                return False
            
            success_count = 0
            
            for step_id, step_name in self.step_id_mapping.items():
                try:
                    self.logger.info(f"ðŸ”„ Step {step_id} ({step_name}) ìƒì„± ì¤‘...")
                    
                    # StepFactoryë¥¼ í†µí•œ Step ìƒì„±
                    step_instance = await self._create_single_step_via_factory(step_id, step_name)
                    
                    if step_instance:
                        self.steps[step_name] = step_instance
                        success_count += 1
                        self.logger.info(f"âœ… Step {step_id} ({step_name}) ìƒì„± ì™„ë£Œ")
                    else:
                        self.logger.warning(f"âš ï¸ Step {step_id} ({step_name}) ìƒì„± ì‹¤íŒ¨")
                        
                except Exception as e:
                    self.logger.error(f"âŒ Step {step_id} ({step_name}) ìƒì„± ì˜¤ë¥˜: {e}")
                    continue
            
            self.logger.info(f"ðŸ“‹ StepFactory ìƒì„± ê²°ê³¼: {success_count}/{len(self.step_id_mapping)} ì„±ê³µ")
            return success_count > 0
            
        except Exception as e:
            self.logger.error(f"âŒ StepFactory ê¸°ë°˜ Step ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    async def _create_single_step_via_factory(self, step_id: int, step_name: str):
        """ë‹¨ì¼ Step ìƒì„± (StepFactory ì‚¬ìš©)"""
        try:
            # Step ì„¤ì • ì¤€ë¹„
            step_config = {
                'step_id': step_id,
                'step_name': step_name,
                'device': self.device,
                'device_type': self.config.device_type,
                'memory_gb': self.config.memory_gb,
                'is_m3_max': self.config.is_m3_max,
                'quality_level': self.config.quality_level.value,
                'use_dependency_injection': self.config.use_dependency_injection,
                'enable_adapter_pattern': self.config.enable_adapter_pattern,
                'ai_model_enabled': self.config.ai_model_enabled,
                'performance_mode': self.config.performance_mode,
                'memory_optimization': self.config.memory_optimization
            }
            
            # StepFactoryì˜ create_step ë©”ì„œë“œ í˜¸ì¶œ ì‹œë„
            if hasattr(self.step_factory, 'create_step'):
                result = self.step_factory.create_step(step_id, **step_config)
                
                # ê²°ê³¼ê°€ ë¹„ë™ê¸°ì¸ ê²½ìš° await
                if hasattr(result, '__await__'):
                    result = await result
                
                # ê²°ê³¼ ì²˜ë¦¬
                if hasattr(result, 'success') and result.success:
                    return result.step_instance
                elif hasattr(result, 'step_instance') and result.step_instance:
                    return result.step_instance
                else:
                    self.logger.warning(f"âš ï¸ StepFactory Step {step_id} ìƒì„± ê²°ê³¼ ì—†ìŒ")
                    return None
            
            # create_step_by_id ë©”ì„œë“œ ì‹œë„
            elif hasattr(self.step_factory, 'create_step_by_id'):
                step_instance = self.step_factory.create_step_by_id(step_id, **step_config)
                return step_instance
            
            # í´ë°±: ì§ì ‘ Step í´ëž˜ìŠ¤ ìƒì„±
            else:
                return await self._create_step_direct(step_id, step_name, step_config)
                
        except Exception as e:
            self.logger.error(f"âŒ Step {step_id} StepFactory ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    async def _create_step_direct(self, step_id: int, step_name: str, step_config: Dict[str, Any]):
        """Step í´ëž˜ìŠ¤ ì§ì ‘ ìƒì„±"""
        try:
            # Step í´ëž˜ìŠ¤ëª… ë§¤í•‘
            class_name_mapping = {
                1: 'HumanParsingStep',
                2: 'PoseEstimationStep',
                3: 'ClothSegmentationStep',
                4: 'GeometricMatchingStep',
                5: 'ClothWarpingStep',
                6: 'VirtualFittingStep',
                7: 'PostProcessingStep',
                8: 'QualityAssessmentStep'
            }
            
            class_name = class_name_mapping.get(step_id)
            if not class_name:
                return None
            
            # ëª¨ë“ˆ ê²½ë¡œ
            module_path = f'app.ai_pipeline.steps.step_{step_id:02d}_{step_name}'
            
            # ë™ì  import
            import importlib
            module = importlib.import_module(module_path)
            step_class = getattr(module, class_name, None)
            
            if not step_class:
                self.logger.warning(f"âš ï¸ {class_name} í´ëž˜ìŠ¤ ì—†ìŒ in {module_path}")
                return None
            
            # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            step_instance = step_class(**step_config)
            
            # í•„ìˆ˜ ì†ì„± ë³´ìž¥ (ì˜¤ë¥˜ í•´ê²°)
            self._ensure_step_attributes(step_instance, step_config)
            
            # Step ì´ˆê¸°í™”
            await self._initialize_step(step_instance)
            
            return step_instance
            
        except ImportError as e:
            self.logger.debug(f"Step {step_id} import ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ Step {step_id} ì§ì ‘ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _ensure_step_attributes(self, step_instance, step_config: Dict[str, Any]):
        """Step ì¸ìŠ¤í„´ìŠ¤ í•„ìˆ˜ ì†ì„± ë³´ìž¥ (ì˜¤ë¥˜ í•´ê²°)"""
        try:
            # í•„ìˆ˜ ì†ì„±ë“¤ ì„¤ì •
            essential_attrs = [
                'device', 'is_m3_max', 'memory_gb', 'device_type',
                'quality_level', 'performance_mode', 'ai_model_enabled'
            ]
            
            for attr in essential_attrs:
                if not hasattr(step_instance, attr):
                    setattr(step_instance, attr, step_config.get(attr, self._get_default_value(attr)))
            
            # BaseStepMixin í˜¸í™˜ì„±
            if not hasattr(step_instance, 'is_initialized'):
                step_instance.is_initialized = False
            if not hasattr(step_instance, 'is_ready'):
                step_instance.is_ready = False
            if not hasattr(step_instance, 'has_model'):
                step_instance.has_model = False
            
            # ë¡œê±° ì„¤ì •
            if not hasattr(step_instance, 'logger'):
                step_instance.logger = logging.getLogger(f"steps.{step_instance.__class__.__name__}")
            
            self.logger.debug(f"âœ… {step_instance.__class__.__name__} í•„ìˆ˜ ì†ì„± ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Step ì†ì„± ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _get_default_value(self, attr: str) -> Any:
        """ê¸°ë³¸ê°’ ë°˜í™˜"""
        defaults = {
            'device': self.device,
            'is_m3_max': self.config.is_m3_max,
            'memory_gb': self.config.memory_gb,
            'device_type': self.config.device_type,
            'quality_level': self.config.quality_level.value,
            'performance_mode': self.config.performance_mode,
            'ai_model_enabled': self.config.ai_model_enabled
        }
        return defaults.get(attr, None)
    
    async def _initialize_step(self, step_instance) -> bool:
        """Step ì´ˆê¸°í™” (ë¹„ë™ê¸° ì˜¤ë¥˜ í•´ê²°)"""
        try:
            # initialize ë©”ì„œë“œ í™•ì¸ ë° í˜¸ì¶œ
            if hasattr(step_instance, 'initialize'):
                if asyncio.iscoroutinefunction(step_instance.initialize):
                    result = await step_instance.initialize()
                else:
                    result = step_instance.initialize()
                
                # ê²°ê³¼ ì²˜ë¦¬
                if result is False:
                    self.logger.warning(f"âš ï¸ {step_instance.__class__.__name__} ì´ˆê¸°í™” ì‹¤íŒ¨")
                    return False
                
                step_instance.is_initialized = True
                return True
            
            # ì´ˆê¸°í™” ë©”ì„œë“œê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì„¤ì •
            step_instance.is_initialized = True
            step_instance.is_ready = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {step_instance.__class__.__name__} ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            return False
    
    async def _initialize_fallback_steps(self) -> bool:
        """í´ë°± Step ì´ˆê¸°í™”"""
        try:
            self.logger.info("ðŸ”„ í´ë°± ëª¨ë“œ Step ì´ˆê¸°í™”...")
            
            for step_id, step_name in self.step_id_mapping.items():
                try:
                    step_instance = await self._create_step_direct(step_id, step_name, {
                        'step_id': step_id,
                        'step_name': step_name,
                        'device': self.device,
                        'is_m3_max': self.config.is_m3_max,
                        'memory_gb': self.config.memory_gb,
                        'device_type': self.config.device_type,
                        'ai_model_enabled': self.config.ai_model_enabled
                    })
                    
                    if step_instance:
                        self.steps[step_name] = step_instance
                        self.logger.info(f"âœ… í´ë°± Step {step_id} ({step_name}) ìƒì„± ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í´ë°± Step {step_id} ìƒì„± ì‹¤íŒ¨: {e}")
                    # ìµœì¢… í´ë°±: ë”ë¯¸ Step ìƒì„±
                    self.steps[step_name] = self._create_dummy_step(step_id, step_name)
            
            return len(self.steps) > 0
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± Step ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _create_dummy_step(self, step_id: int, step_name: str):
        """ë”ë¯¸ Step ìƒì„± (ìµœì¢… í´ë°±)"""
        class DummyStep:
            def __init__(self, step_id: int, step_name: str):
                self.step_id = step_id
                self.step_name = step_name
                self.device = "cpu"
                self.is_m3_max = False
                self.memory_gb = 16.0
                self.device_type = "cpu"
                self.quality_level = "balanced"
                self.performance_mode = "basic"
                self.ai_model_enabled = False
                self.is_initialized = True
                self.is_ready = True
                self.has_model = False
                self.logger = logging.getLogger(f"DummyStep{step_id}")
            
            async def process(self, *args, **kwargs):
                """ë”ë¯¸ ì²˜ë¦¬"""
                await asyncio.sleep(0.1)  # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                return {
                    'success': True,
                    'result': args[0] if args else torch.zeros(1, 3, 512, 512),
                    'confidence': 0.5,
                    'quality_score': 0.5,
                    'step_name': self.step_name,
                    'dummy_step': True,
                    'processing_time': 0.1
                }
            
            async def initialize(self):
                return True
            
            def cleanup(self):
                pass
        
        return DummyStep(step_id, step_name)
    
    # Step ë“±ë¡/ê´€ë¦¬ ë©”ì„œë“œë“¤ (ê¸°ì¡´ ì¸í„°íŽ˜ì´ìŠ¤ ìœ ì§€)
    def register_step(self, step_id: int, step_instance: Any) -> bool:
        """Step ë“±ë¡ (ë™ê¸° ë©”ì„œë“œ, await ì˜¤ë¥˜ í•´ê²°)"""
        try:
            step_name = self.step_id_mapping.get(step_id)
            if not step_name:
                self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” Step ID: {step_id}")
                return False
            
            # í•„ìˆ˜ ì†ì„± ë³´ìž¥
            self._ensure_step_attributes(step_instance, {
                'step_id': step_id,
                'step_name': step_name,
                'device': self.device,
                'is_m3_max': self.config.is_m3_max,
                'memory_gb': self.config.memory_gb,
                'device_type': self.config.device_type,
                'ai_model_enabled': self.config.ai_model_enabled
            })
            
            self.steps[step_name] = step_instance
            self.logger.info(f"âœ… Step {step_id} ({step_name}) ë“±ë¡ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Step {step_id} ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def register_steps_batch(self, steps_dict: Dict[int, Any]) -> Dict[int, bool]:
        """Step ì¼ê´„ ë“±ë¡ (ë™ê¸° ë©”ì„œë“œ)"""
        results = {}
        try:
            self.logger.info(f"ðŸ”„ {len(steps_dict)}ê°œ Step ì¼ê´„ ë“±ë¡ ì‹œìž‘...")
            
            for step_id, step_instance in steps_dict.items():
                results[step_id] = self.register_step(step_id, step_instance)
            
            success_count = sum(1 for success in results.values() if success)
            self.logger.info(f"âœ… Step ì¼ê´„ ë“±ë¡ ì™„ë£Œ: {success_count}/{len(steps_dict)}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ Step ì¼ê´„ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return {step_id: False for step_id in steps_dict.keys()}
    
    def get_step_by_id(self, step_id: int) -> Optional[Any]:
        """Step IDë¡œ Step ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        step_name = self.step_id_mapping.get(step_id)
        return self.steps.get(step_name) if step_name else None
    
    def is_step_registered(self, step_id: int) -> bool:
        """Step ë“±ë¡ ì—¬ë¶€ í™•ì¸"""
        step_name = self.step_id_mapping.get(step_id)
        return step_name in self.steps if step_name else False
    
    def get_registered_steps(self) -> Dict[str, Any]:
        """ë“±ë¡ëœ Step ëª©ë¡ ë°˜í™˜"""
        return {
            'total_registered': len(self.steps),
            'registered_steps': {
                step_name: {
                    'step_id': self.step_name_to_id.get(step_name, 0),
                    'step_name': step_name,
                    'class_name': type(step_instance).__name__,
                    'registered': True,
                    'has_process_method': hasattr(step_instance, 'process'),
                    'is_initialized': getattr(step_instance, 'is_initialized', False),
                    'is_ready': getattr(step_instance, 'is_ready', False),
                    'has_model': getattr(step_instance, 'has_model', False)
                }
                for step_name, step_instance in self.steps.items()
            },
            'missing_steps': [name for name in self.step_order if name not in self.steps],
            'registration_rate': len(self.steps) / len(self.step_order) * 100
        }

# ==============================================
# ðŸ”¥ AI ëª¨ë¸ ê´€ë¦¬ìž í´ëž˜ìŠ¤ 
# ==============================================

class AIModelManager:
    """AI ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬ (ì‹¤ì œ íŒŒì¼ ê²½ë¡œ í™œìš©)"""
    
    def __init__(self, config: PipelineConfig, device: str, logger: logging.Logger):
        self.config = config
        self.device = device
        self.logger = logger
        self.model_loader = None
        self.loaded_models = {}
        self.model_cache = {}
        self.is_initialized = False
        
        # ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë§¤í•‘ (229GB í™œìš©)
        self.ai_model_paths = self._setup_ai_model_paths()
        
    def _setup_ai_model_paths(self) -> Dict[str, Dict[str, str]]:
        """ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •"""
        return {
            'step_01_human_parsing': {
                'graphonomy': 'ai_models/step_01_human_parsing/graphonomy.pth',  # 1.17GB
                'schp_atr': 'ai_models/step_01_human_parsing/exp-schp-201908301523-atr.pth',  # 255MB
                'atr_model': 'ai_models/step_01_human_parsing/atr_model.pth',  # 255MB
                'lip_model': 'ai_models/step_01_human_parsing/lip_model.pth'   # 255MB
            },
            'step_02_pose_estimation': {
                'yolov8_pose': 'ai_models/step_02_pose_estimation/yolov8n-pose.pt',  # 6MB
                'openpose': 'ai_models/step_02_pose_estimation/body_pose_model.pth',  # 209MB
                'hrnet': 'ai_models/step_02_pose_estimation/hrnet_w48_coco_256x192.pth'  # 254MB
            },
            'step_03_cloth_segmentation': {
                'sam_vit_h': 'ai_models/step_03_cloth_segmentation/sam_vit_h_4b8939.pth',  # 2.6GB
                'u2net': 'ai_models/step_03_cloth_segmentation/u2net.pth',  # 176MB
                'mobile_sam': 'ai_models/step_03_cloth_segmentation/mobile_sam.pt'  # 40MB
            },
            'step_04_geometric_matching': {
                'gmm_final': 'ai_models/step_04_geometric_matching/gmm_final.pth',  # 85MB
                'tps_network': 'ai_models/step_04_geometric_matching/tps_network.pth',  # 45MB
                'vit_large': 'ai_models/step_04_geometric_matching/ViT-L-14.pt'  # 890MB
            },
            'step_05_cloth_warping': {
                'realvisx_xl': 'ai_models/step_05_cloth_warping/RealVisXL_V4.0.safetensors',  # 6.9GB
                'vgg16_warping': 'ai_models/step_05_cloth_warping/vgg16_warping_ultra.pth',  # 528MB
                'stable_diffusion': 'ai_models/step_05_cloth_warping/stable_diffusion_2_1.safetensors'  # 5.2GB
            },
            'step_06_virtual_fitting': {
                'ootd_unet_garm': 'ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_garm/diffusion_pytorch_model.bin',  # 3.2GB
                'ootd_unet_vton': 'ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_vton/diffusion_pytorch_model.bin',  # 3.2GB
                'text_encoder': 'ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/text_encoder/text_encoder_pytorch_model.bin',  # 469MB
                'vae': 'ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/vae/vae_diffusion_pytorch_model.bin'  # 319MB
            },
            'step_07_post_processing': {
                'real_esrgan_x4': 'ai_models/step_07_post_processing/ultra_models/RealESRGAN_x4plus.pth',  # 64MB
                'esrgan_x8': 'ai_models/step_07_post_processing/esrgan_x8_ultra/ESRGAN_x8.pth',  # 136MB
                'gfpgan': 'ai_models/checkpoints/step_07_post_processing/GFPGAN.pth'  # 333MB
            },
            'step_08_quality_assessment': {
                'clip_vit_large': 'ai_models/step_08_quality_assessment/ultra_models/pytorch_model.bin',  # 823MB
                'aesthetic_predictor': 'ai_models/step_08_quality_assessment/aesthetic_predictor.pth'  # 145MB
            }
        }
    
    async def initialize(self) -> bool:
        """AI ëª¨ë¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            self.logger.info("ðŸ§  AIModelManager ì´ˆê¸°í™” ì‹œìž‘...")
            
            # ModelLoader ë™ì  ë¡œë”©
            success = await self._load_model_loader()
            if success:
                self.logger.info("âœ… ModelLoader ë¡œë”© ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ ModelLoader ë¡œë”© ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ ì§„í–‰")
            
            # ëª¨ë¸ ê²½ë¡œ ê²€ì¦
            await self._verify_model_paths()
            
            self.is_initialized = True
            self.logger.info("âœ… AIModelManager ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ AIModelManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _load_model_loader(self) -> bool:
        """ModelLoader ë™ì  ë¡œë”©"""
        try:
            import importlib
            loader_module = importlib.import_module('app.ai_pipeline.utils.model_loader')
            get_global_loader = getattr(loader_module, 'get_global_model_loader', None)
            
            if get_global_loader:
                self.model_loader = get_global_loader()
                return self.model_loader is not None
            else:
                return False
                
        except ImportError as e:
            self.logger.debug(f"ModelLoader import ì‹¤íŒ¨: {e}")
            return False
        except Exception as e:
            self.logger.warning(f"âš ï¸ ModelLoader ë¡œë”© ì˜¤ë¥˜: {e}")
            return False
    
    async def _verify_model_paths(self):
        """ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ê²€ì¦"""
        verified_count = 0
        total_count = 0
        
        for step_name, models in self.ai_model_paths.items():
            for model_name, model_path in models.items():
                total_count += 1
                if Path(model_path).exists():
                    verified_count += 1
                    self.logger.debug(f"âœ… {model_name}: {model_path}")
                else:
                    self.logger.debug(f"âš ï¸ {model_name}: {model_path} (íŒŒì¼ ì—†ìŒ)")
        
        self.logger.info(f"ðŸ“‹ ëª¨ë¸ íŒŒì¼ ê²€ì¦: {verified_count}/{total_count} ({verified_count/total_count*100:.1f}%)")
    
    async def load_step_models(self, step_name: str) -> Dict[str, Any]:
        """Stepë³„ AI ëª¨ë¸ ë¡œë”©"""
        try:
            if step_name not in self.ai_model_paths:
                return {'success': False, 'error': f'Step {step_name} ëª¨ë¸ ê²½ë¡œ ì—†ìŒ'}
            
            models = {}
            step_models = self.ai_model_paths[step_name]
            
            for model_name, model_path in step_models.items():
                try:
                    if Path(model_path).exists():
                        # ì‹¤ì œ ëª¨ë¸ ë¡œë”© (ìºì‹œ í™•ì¸)
                        cache_key = f"{step_name}:{model_name}"
                        if cache_key in self.model_cache:
                            models[model_name] = self.model_cache[cache_key]
                        else:
                            model = await self._load_single_model(model_path, model_name)
                            if model:
                                models[model_name] = model
                                self.model_cache[cache_key] = model
                    else:
                        self.logger.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
                        
                except Exception as e:
                    self.logger.error(f"âŒ {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
            
            return {
                'success': len(models) > 0,
                'models': models,
                'loaded_count': len(models),
                'total_count': len(step_models)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Step {step_name} ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _load_single_model(self, model_path: str, model_name: str):
        """ë‹¨ì¼ ëª¨ë¸ ë¡œë”©"""
        try:
            # ModelLoader ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
            if self.model_loader and hasattr(self.model_loader, 'load_model'):
                return await self._load_via_model_loader(model_path, model_name)
            
            # ì§ì ‘ ë¡œë”©
            return await self._load_direct(model_path, model_name)
            
        except Exception as e:
            self.logger.error(f"âŒ ë‹¨ì¼ ëª¨ë¸ {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_via_model_loader(self, model_path: str, model_name: str):
        """ModelLoaderë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”©"""
        try:
            model_config = {
                'model_path': model_path,
                'model_name': model_name,
                'device': self.device
            }
            
            if asyncio.iscoroutinefunction(self.model_loader.load_model):
                return await self.model_loader.load_model(model_config)
            else:
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self.model_loader.load_model, model_config)
                
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ê¸°ë°˜ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_direct(self, model_path: str, model_name: str):
        """ì§ì ‘ ëª¨ë¸ ë¡œë”©"""
        try:
            # íŒŒì¼ í™•ìž¥ìžë³„ ë¡œë”© ì „ëžµ
            if model_path.endswith(('.pth', '.pt')):
                return await self._load_pytorch_model(model_path)
            elif model_path.endswith('.safetensors'):
                return await self._load_safetensors_model(model_path)
            elif model_path.endswith('.bin'):
                return await self._load_bin_model(model_path)
            else:
                self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í˜•ì‹: {model_path}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ì§ì ‘ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_pytorch_model(self, model_path: str):
        """PyTorch ëª¨ë¸ ë¡œë”©"""
        try:
            loop = asyncio.get_event_loop()
            model_data = await loop.run_in_executor(
                None, 
                lambda: torch.load(model_path, map_location=self.device)
            )
            return model_data
        except Exception as e:
            self.logger.error(f"âŒ PyTorch ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_safetensors_model(self, model_path: str):
        """SafeTensors ëª¨ë¸ ë¡œë”©"""
        try:
            # safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹œë„
            try:
                from safetensors.torch import load_file
                loop = asyncio.get_event_loop()
                model_data = await loop.run_in_executor(
                    None, 
                    lambda: load_file(model_path, device=self.device)
                )
                return model_data
            except ImportError:
                self.logger.warning("âš ï¸ safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, ê¸°ë³¸ ë¡œë”© ì‹œë„")
                return await self._load_pytorch_model(model_path)
                
        except Exception as e:
            self.logger.error(f"âŒ SafeTensors ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_bin_model(self, model_path: str):
        """BIN ëª¨ë¸ ë¡œë”©"""
        try:
            # HuggingFace í˜•ì‹ ì‹œë„
            try:
                import transformers
                loop = asyncio.get_event_loop()
                model_data = await loop.run_in_executor(
                    None,
                    lambda: torch.load(model_path, map_location=self.device)
                )
                return model_data
            except ImportError:
                return await self._load_pytorch_model(model_path)
                
        except Exception as e:
            self.logger.error(f"âŒ BIN ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            'is_initialized': self.is_initialized,
            'model_loader_available': self.model_loader is not None,
            'cached_models': len(self.model_cache),
            'step_model_paths': {
                step_name: len(models) 
                for step_name, models in self.ai_model_paths.items()
            },
            'total_model_files': sum(len(models) for models in self.ai_model_paths.values()),
            'device': self.device
        }

# ==============================================
# ðŸ”¥ ì²˜ë¦¬ ì—”ì§„ í´ëž˜ìŠ¤
# ==============================================

class ProcessingEngine:
    """8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì—”ì§„"""
    
    def __init__(self, step_manager: StepManager, ai_model_manager: AIModelManager, 
                 config: PipelineConfig, logger: logging.Logger):
        self.step_manager = step_manager
        self.ai_model_manager = ai_model_manager
        self.config = config
        self.logger = logger
        self.processing_stats = {}
        
    async def process_complete_pipeline(
        self,
        person_image: Union[str, Image.Image, np.ndarray],
        clothing_image: Union[str, Image.Image, np.ndarray],
        **kwargs
    ) -> ProcessingResult:
        """ì™„ì „í•œ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬"""
        
        session_id = kwargs.get('session_id') or f"pipeline_{int(time.time())}_{np.random.randint(1000, 9999)}"
        start_time = time.time()
        
        try:
            self.logger.info(f"ðŸš€ 8ë‹¨ê³„ ì™„ì „ íŒŒì´í”„ë¼ì¸ ì‹œìž‘ - ì„¸ì…˜: {session_id}")
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            person_tensor = await self._preprocess_image(person_image)
            clothing_tensor = await self._preprocess_image(clothing_image)
            
            # 8ë‹¨ê³„ ìˆœì°¨ ì²˜ë¦¬
            step_results = {}
            execution_strategies = {}
            ai_models_used = {}
            current_data = person_tensor
            
            for i, step_name in enumerate(self.step_manager.step_order):
                step_start = time.time()
                step_id = i + 1
                
                self.logger.info(f"ðŸ“‹ {step_id}/8 ë‹¨ê³„: {step_name} ì²˜ë¦¬ ì¤‘...")
                
                try:
                    # Step ì‹¤í–‰
                    step_result, strategy, models = await self._execute_single_step(
                        step_name, current_data, clothing_tensor, **kwargs
                    )
                    
                    step_time = time.time() - step_start
                    
                    # ê²°ê³¼ ì €ìž¥
                    step_results[step_name] = step_result
                    execution_strategies[step_name] = strategy
                    ai_models_used[step_name] = models
                    
                    # ë°ì´í„° ì—…ë°ì´íŠ¸
                    if step_result.get('success', True):
                        result_data = step_result.get('result')
                        if result_data is not None:
                            current_data = result_data
                    
                    # ë¡œê¹…
                    confidence = step_result.get('confidence', 0.8)
                    quality_score = step_result.get('quality_score', confidence)
                    
                    self.logger.info(f"âœ… {step_id}ë‹¨ê³„ ì™„ë£Œ - ì‹œê°„: {step_time:.2f}ì´ˆ, "
                                   f"ì‹ ë¢°ë„: {confidence:.3f}, í’ˆì§ˆ: {quality_score:.3f}")
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if 'progress_callback' in kwargs:
                        progress = (i + 1) * 100 // len(self.step_manager.step_order)
                        await kwargs['progress_callback'](f"{step_name} ì™„ë£Œ", progress)
                    
                except Exception as e:
                    self.logger.error(f"âŒ {step_id}ë‹¨ê³„ ({step_name}) ì‹¤íŒ¨: {e}")
                    step_results[step_name] = {
                        'success': False,
                        'error': str(e),
                        'confidence': 0.0,
                        'quality_score': 0.0
                    }
                    execution_strategies[step_name] = "error"
                    ai_models_used[step_name] = "error"
                    continue
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            total_time = time.time() - start_time
            quality_score = self._calculate_overall_quality(step_results)
            quality_grade = self._get_quality_grade(quality_score)
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
            result_image = await self._generate_result_image(current_data)
            
            # ì„±ê³µ ì—¬ë¶€ ê²°ì •
            success = quality_score >= 0.6 and len([r for r in step_results.values() if r.get('success', True)]) >= 6
            
            self.logger.info(f"ðŸŽ‰ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
            self.logger.info(f"ðŸ“Š í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f} ({quality_grade})")
            
            return ProcessingResult(
                success=success,
                session_id=session_id,
                result_image=result_image,
                result_tensor=current_data if isinstance(current_data, torch.Tensor) else None,
                quality_score=quality_score,
                quality_grade=quality_grade,
                processing_time=total_time,
                step_results=step_results,
                step_timings={step: result.get('processing_time', 0.0) for step, result in step_results.items()},
                ai_models_used=ai_models_used,
                execution_strategies=execution_strategies,
                performance_metrics=self._get_performance_metrics(step_results),
                metadata={
                    'device': self.config.device,
                    'is_m3_max': self.config.is_m3_max,
                    'ai_model_enabled': self.config.ai_model_enabled,
                    'total_steps': len(self.step_manager.step_order),
                    'completed_steps': len(step_results),
                    'success_rate': len([r for r in step_results.values() if r.get('success', True)]) / len(step_results) if step_results else 0
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ProcessingResult(
                success=False,
                session_id=session_id,
                processing_time=time.time() - start_time,
                error_message=str(e),
                metadata={'error_location': traceback.format_exc()}
            )
    
    async def _execute_single_step(self, step_name: str, current_data: torch.Tensor, 
                                  clothing_tensor: torch.Tensor, **kwargs) -> Tuple[Dict[str, Any], str, str]:
        """ë‹¨ì¼ Step ì‹¤í–‰"""
        try:
            # Step ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            step = self.step_manager.steps.get(step_name)
            if not step:
                return {'success': False, 'error': f'Step {step_name} ì—†ìŒ'}, "error", "error"
            
            # AI ëª¨ë¸ ë¡œë”©
            model_info = await self.ai_model_manager.load_step_models(step_name)
            ai_models = model_info.get('models', {})
            
            # Step ì²˜ë¦¬
            if hasattr(step, 'process'):
                # ìž…ë ¥ ë°ì´í„° ì¤€ë¹„
                if step_name == 'human_parsing':
                    result = await step.process(current_data)
                elif step_name == 'pose_estimation':
                    result = await step.process(current_data)
                elif step_name == 'cloth_segmentation':
                    result = await step.process(clothing_tensor, clothing_type=kwargs.get('clothing_type', 'shirt'))
                elif step_name == 'geometric_matching':
                    result = await step.process(
                        person_parsing={'result': current_data},
                        pose_keypoints=self._generate_dummy_pose_keypoints(),
                        clothing_segmentation={'mask': clothing_tensor},
                        clothing_type=kwargs.get('clothing_type', 'shirt')
                    )
                elif step_name == 'cloth_warping':
                    result = await step.process(
                        current_data, clothing_tensor,
                        kwargs.get('body_measurements', {}),
                        kwargs.get('fabric_type', 'cotton')
                    )
                elif step_name == 'virtual_fitting':
                    result = await step.process(current_data, clothing_tensor, kwargs.get('style_preferences', {}))
                elif step_name == 'post_processing':
                    result = await step.process(current_data)
                elif step_name == 'quality_assessment':
                    result = await step.process(current_data, clothing_tensor)
                else:
                    result = await step.process(current_data)
                
                # ê²°ê³¼ê°€ dictê°€ ì•„ë‹Œ ê²½ìš° ë³€í™˜
                if not isinstance(result, dict):
                    result = {
                        'success': True,
                        'result': result,
                        'confidence': 0.8,
                        'quality_score': 0.8
                    }
                
                # AI ëª¨ë¸ ì •ë³´ ì¶”ê°€
                model_names = list(ai_models.keys()) if ai_models else ['fallback']
                strategy = ExecutionStrategy.UNIFIED_AI.value if ai_models else ExecutionStrategy.BASIC_FALLBACK.value
                
                return result, strategy, ','.join(model_names)
            
            else:
                return {'success': False, 'error': 'process ë©”ì„œë“œ ì—†ìŒ'}, "error", "error"
                
        except Exception as e:
            self.logger.error(f"âŒ Step {step_name} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}, "error", "error"
    
    async def _preprocess_image(self, image_input) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            if isinstance(image_input, str):
                image = Image.open(image_input).convert('RGB')
            elif isinstance(image_input, Image.Image):
                image = image_input.convert('RGB')
            elif isinstance(image_input, np.ndarray):
                image = Image.fromarray(image_input).convert('RGB')
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ìž…: {type(image_input)}")
            
            # ë¦¬ì‚¬ì´ì¦ˆ
            if image.size != (512, 512):
                image = image.resize((512, 512), Image.Resampling.LANCZOS)
            
            # í…ì„œ ë³€í™˜
            img_array = np.array(image)
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0
            img_tensor = img_tensor.unsqueeze(0).to(self.config.device)
            
            return img_tensor
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return torch.zeros(1, 3, 512, 512, device=self.config.device)
    
    async def _generate_result_image(self, tensor_data) -> Image.Image:
        """ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            if isinstance(tensor_data, torch.Tensor):
                if tensor_data.dim() == 4:
                    tensor_data = tensor_data.squeeze(0)
                if tensor_data.shape[0] == 3:
                    tensor_data = tensor_data.permute(1, 2, 0)
                
                tensor_data = torch.clamp(tensor_data, 0, 1)
                tensor_data = tensor_data.cpu()
                array = (tensor_data.numpy() * 255).astype(np.uint8)
                
                return Image.fromarray(array)
            else:
                return Image.new('RGB', (512, 512), color='gray')
                
        except Exception as e:
            self.logger.error(f"âŒ ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (512, 512), color='gray')
    
    def _generate_dummy_pose_keypoints(self) -> List[List[float]]:
        """ë”ë¯¸ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ìƒì„±"""
        return [[256 + np.random.uniform(-50, 50), 256 + np.random.uniform(-100, 100), 0.8] for _ in range(18)]
    
    def _calculate_overall_quality(self, step_results: Dict[str, Any]) -> float:
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if not step_results:
            return 0.5
        
        quality_scores = []
        for result in step_results.values():
            if isinstance(result, dict):
                quality = result.get('quality_score', result.get('confidence', 0.8))
                quality_scores.append(quality)
        
        return sum(quality_scores) / len(quality_scores) if quality_scores else 0.5
    
    def _get_quality_grade(self, quality_score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if quality_score >= 0.95:
            return "Excellent+"
        elif quality_score >= 0.9:
            return "Excellent"
        elif quality_score >= 0.8:
            return "Good"
        elif quality_score >= 0.7:
            return "Fair"
        elif quality_score >= 0.6:
            return "Poor"
        else:
            return "Very Poor"
    
    def _get_performance_metrics(self, step_results: Dict[str, Any]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {}
        
        for step_name, result in step_results.items():
            if isinstance(result, dict):
                metrics[step_name] = {
                    'success': result.get('success', False),
                    'processing_time': result.get('processing_time', 0.0),
                    'confidence': result.get('confidence', 0.0),
                    'quality_score': result.get('quality_score', 0.0)
                }
        
        return metrics

# ==============================================
# ðŸ”¥ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ìž í´ëž˜ìŠ¤
# ==============================================

class ResourceManager:
    """ë©”ëª¨ë¦¬ ë° ë””ë°”ì´ìŠ¤ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬"""
    
    def __init__(self, config: PipelineConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.device = config.device
        
    async def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            optimization_results = []
            
            # Python GC
            before_objects = len(gc.get_objects())
            gc.collect()
            after_objects = len(gc.get_objects())
            freed_objects = before_objects - after_objects
            optimization_results.append(f"Python GC: {freed_objects}ê°œ ê°ì²´ ì •ë¦¬")
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                before_cuda = torch.cuda.memory_allocated()
                torch.cuda.empty_cache()
                after_cuda = torch.cuda.memory_allocated()
                freed_cuda = (before_cuda - after_cuda) / 1024**3
                optimization_results.append(f"CUDA ìºì‹œ ì •ë¦¬: {freed_cuda:.2f}GB í•´ì œ")
            
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                        optimization_results.append("MPS ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                except Exception as mps_error:
                    optimization_results.append(f"MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
            
            return {
                "success": True,
                "message": "Memory optimization completed",
                "optimization_results": optimization_results,
                "device": self.device,
                "timestamp": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "timestamp": time.time()}
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„"""
        try:
            stats = {
                "device": self.device,
                "is_m3_max": self.config.is_m3_max,
                "memory_gb": self.config.memory_gb,
                "available": True,
                "timestamp": time.time()
            }
            
            if torch.cuda.is_available():
                stats.update({
                    "cuda_memory_allocated": torch.cuda.memory_allocated() / 1024**3,
                    "cuda_memory_reserved": torch.cuda.memory_reserved() / 1024**3,
                })
            
            if PSUTIL_AVAILABLE:
                process = psutil.Process()
                stats.update({
                    "cpu_memory_gb": process.memory_info().rss / (1024**3),
                    "system_memory_percent": psutil.virtual_memory().percent,
                    "system_memory_available_gb": psutil.virtual_memory().available / (1024**3)
                })
            
            return stats
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e), "device": self.device}

# ==============================================
# ðŸ”¥ ì—ëŸ¬ ê´€ë¦¬ìž í´ëž˜ìŠ¤
# ==============================================

class ErrorManager:
    """ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°± ê´€ë¦¬"""
    
    def __init__(self, config: PipelineConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.error_history = []
        self.fallback_attempts = {}
        
    async def handle_step_error(self, step_name: str, error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:
        """Step ì—ëŸ¬ ì²˜ë¦¬"""
        try:
            error_info = {
                'step_name': step_name,
                'error_type': type(error).__name__,
                'error_message': str(error),
                'timestamp': time.time(),
                'context': context
            }
            
            self.error_history.append(error_info)
            self.logger.error(f"âŒ {step_name} ì—ëŸ¬: {error}")
            
            # í´ë°± ì‹œë„
            if self.config.enable_smart_fallback:
                fallback_result = await self._attempt_fallback(step_name, error, context)
                if fallback_result['success']:
                    self.logger.info(f"âœ… {step_name} í´ë°± ì„±ê³µ")
                    return fallback_result
            
            # ê¸°ë³¸ ì—ëŸ¬ ì‘ë‹µ
            return {
                'success': False,
                'error': str(error),
                'fallback_attempted': self.config.enable_smart_fallback,
                'recovery_suggestions': self._get_recovery_suggestions(step_name, error)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì—ëŸ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': f'Error handling failed: {e}'}
    
    async def _attempt_fallback(self, step_name: str, error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:
        """í´ë°± ì‹œë„"""
        try:
            fallback_key = f"{step_name}:{type(error).__name__}"
            attempts = self.fallback_attempts.get(fallback_key, 0)
            
            if attempts >= self.config.max_fallback_attempts:
                return {'success': False, 'error': 'Max fallback attempts exceeded'}
            
            self.fallback_attempts[fallback_key] = attempts + 1
            
            # Stepë³„ í´ë°± ì „ëžµ
            if step_name == 'human_parsing':
                return await self._fallback_human_parsing(context)
            elif step_name == 'pose_estimation':
                return await self._fallback_pose_estimation(context)
            elif step_name == 'cloth_segmentation':
                return await self._fallback_cloth_segmentation(context)
            elif step_name == 'virtual_fitting':
                return await self._fallback_virtual_fitting(context)
            else:
                return await self._fallback_generic(step_name, context)
                
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ì‹œë„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': f'Fallback failed: {e}'}
    
    async def _fallback_human_parsing(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Human Parsing í´ë°±"""
        try:
            # ê¸°ë³¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìƒì„±
            input_data = context.get('input_data')
            if isinstance(input_data, torch.Tensor):
                # ê°„ë‹¨í•œ ìƒ‰ìƒ ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜
                segmentation = torch.zeros_like(input_data)
                segmentation[:, 0, :, :] = 1.0  # ì¸ì²´ ì˜ì—­
                
                return {
                    'success': True,
                    'result': segmentation,
                    'confidence': 0.6,
                    'quality_score': 0.6,
                    'fallback_method': 'basic_segmentation'
                }
            
            return {'success': False, 'error': 'Invalid input data'}
            
        except Exception as e:
            return {'success': False, 'error': f'Human parsing fallback failed: {e}'}
    
    async def _fallback_pose_estimation(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Pose Estimation í´ë°±"""
        try:
            # ê¸°ë³¸ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ìƒì„±
            keypoints = self._generate_default_pose_keypoints()
            
            return {
                'success': True,
                'result': {
                    'keypoints': keypoints,
                    'confidence_scores': [0.6] * len(keypoints)
                },
                'confidence': 0.6,
                'quality_score': 0.6,
                'fallback_method': 'default_pose'
            }
            
        except Exception as e:
            return {'success': False, 'error': f'Pose estimation fallback failed: {e}'}
    
    async def _fallback_cloth_segmentation(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Cloth Segmentation í´ë°±"""
        try:
            # ê¸°ë³¸ ì˜ë¥˜ ë§ˆìŠ¤í¬ ìƒì„±
            input_data = context.get('input_data')
            if isinstance(input_data, torch.Tensor):
                mask = torch.ones_like(input_data[:, :1, :, :])  # ì „ì²´ ì˜ì—­
                
                return {
                    'success': True,
                    'result': {'mask': mask},
                    'confidence': 0.5,
                    'quality_score': 0.5,
                    'fallback_method': 'full_mask'
                }
            
            return {'success': False, 'error': 'Invalid input data'}
            
        except Exception as e:
            return {'success': False, 'error': f'Cloth segmentation fallback failed: {e}'}
    
    async def _fallback_virtual_fitting(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Virtual Fitting í´ë°±"""
        try:
            # ê¸°ë³¸ í•©ì„± (ë‹¨ìˆœ ë¸”ë Œë”©)
            person_data = context.get('person_data')
            clothing_data = context.get('clothing_data')
            
            if isinstance(person_data, torch.Tensor) and isinstance(clothing_data, torch.Tensor):
                # ê°„ë‹¨í•œ ì•ŒíŒŒ ë¸”ë Œë”©
                alpha = 0.7
                result = alpha * person_data + (1 - alpha) * clothing_data
                
                return {
                    'success': True,
                    'result': result,
                    'confidence': 0.5,
                    'quality_score': 0.5,
                    'fallback_method': 'alpha_blending'
                }
            
            return {'success': False, 'error': 'Invalid input data'}
            
        except Exception as e:
            return {'success': False, 'error': f'Virtual fitting fallback failed: {e}'}
    
    async def _fallback_generic(self, step_name: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """ì¼ë°˜ í´ë°±"""
        try:
            input_data = context.get('input_data') or context.get('person_data')
            
            if isinstance(input_data, torch.Tensor):
                return {
                    'success': True,
                    'result': input_data,
                    'confidence': 0.4,
                    'quality_score': 0.4,
                    'fallback_method': 'passthrough'
                }
            
            return {'success': False, 'error': 'No fallback available'}
            
        except Exception as e:
            return {'success': False, 'error': f'Generic fallback failed: {e}'}
    
    def _generate_default_pose_keypoints(self) -> List[List[float]]:
        """ê¸°ë³¸ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ìƒì„±"""
        # ê¸°ë³¸ T-pose í‚¤í¬ì¸íŠ¸ (COCO í˜•ì‹)
        default_keypoints = [
            [256, 150, 0.8],  # nose
            [256, 140, 0.8],  # left_eye
            [256, 140, 0.8],  # right_eye
            [246, 145, 0.8],  # left_ear
            [266, 145, 0.8],  # right_ear
            [226, 200, 0.8],  # left_shoulder
            [286, 200, 0.8],  # right_shoulder
            [196, 250, 0.8],  # left_elbow
            [316, 250, 0.8],  # right_elbow
            [166, 300, 0.8],  # left_wrist
            [346, 300, 0.8],  # right_wrist
            [236, 320, 0.8],  # left_hip
            [276, 320, 0.8],  # right_hip
            [226, 420, 0.8],  # left_knee
            [286, 420, 0.8],  # right_knee
            [216, 520, 0.8],  # left_ankle
            [296, 520, 0.8],  # right_ankle
        ]
        return default_keypoints
    
    def _get_recovery_suggestions(self, step_name: str, error: Exception) -> List[str]:
        """ë³µêµ¬ ì œì•ˆ"""
        suggestions = []
        
        error_type = type(error).__name__
        error_msg = str(error).lower()
        
        # ì¼ë°˜ì ì¸ ì—ëŸ¬ë³„ ì œì•ˆ
        if 'memory' in error_msg or 'cuda' in error_msg:
            suggestions.extend([
                "ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰",
                "ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°",
                "ëª¨ë¸ ìºì‹œ ì •ë¦¬"
            ])
        
        if 'file not found' in error_msg or 'no such file' in error_msg:
            suggestions.extend([
                "ëª¨ë¸ íŒŒì¼ ê²½ë¡œ í™•ì¸",
                "ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰",
                "ê²½ë¡œ ê¶Œí•œ í™•ì¸"
            ])
        
        if 'timeout' in error_msg:
            suggestions.extend([
                "íƒ€ìž„ì•„ì›ƒ ì„¤ì • ì¦ê°€",
                "ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸",
                "ì²˜ë¦¬ ë³µìž¡ë„ ê°ì†Œ"
            ])
        
        # Stepë³„ íŠ¹í™” ì œì•ˆ
        if step_name == 'virtual_fitting':
            suggestions.extend([
                "ì´ë¯¸ì§€ í•´ìƒë„ ë‚®ì¶”ê¸°",
                "ë‹¨ìˆœ ë¸”ë Œë”© ëª¨ë“œ ì‚¬ìš©",
                "CPU ëª¨ë“œë¡œ ì „í™˜"
            ])
        
        return suggestions or ["ì‹œìŠ¤í…œ ìž¬ì‹œìž‘", "ë¡œê·¸ í™•ì¸", "ê¸°ìˆ  ì§€ì› ë¬¸ì˜"]

# ==============================================
# ðŸ”¥ ë©”ì¸ PipelineManager í´ëž˜ìŠ¤ (ì™„ì „ ìƒˆ êµ¬í˜„)
# ==============================================

class PipelineManager:
    """
    ðŸ”¥ ì™„ì „ížˆ ìƒˆë¡œìš´ PipelineManager v10.0 - ì™„ì „ ê¸°ëŠ¥ êµ¬í˜„
    
    âœ… ëª¨ë“  ê¸°ì¡´ í•¨ìˆ˜/í´ëž˜ìŠ¤ëª… 100% ìœ ì§€
    âœ… StepFactory + BaseStepMixin ì™„ì „ ì—°ë™
    âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB í™œìš©
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    âœ… conda í™˜ê²½ ì™„ë²½ ìµœì í™”
    âœ… M3 Max 128GB ìµœì í™”
    âœ… 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ì „ ìž‘ë™
    """
    
    def __init__(
        self,
        config_path: Optional[str] = None,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], PipelineConfig]] = None,
        **kwargs
    ):
        """PipelineManager ì´ˆê¸°í™”"""
        
        # 1. ë””ë°”ì´ìŠ¤ ìžë™ ê°ì§€
        self.device = self._auto_detect_device(device)
        
        # 2. ì„¤ì • ì´ˆê¸°í™”
        if isinstance(config, PipelineConfig):
            self.config = config
        else:
            config_dict = self._load_config(config_path) if config_path else {}
            if config:
                config_dict.update(config if isinstance(config, dict) else {})
            config_dict.update(kwargs)
            
            # M3 Max ìžë™ ê°ì§€ ë° ìµœì í™”
            if self._detect_m3_max():
                config_dict.update({
                    'is_m3_max': True,
                    'memory_gb': 128.0,
                    'device_type': 'apple_silicon',
                    'performance_mode': 'maximum',
                    'use_dependency_injection': True,
                    'enable_adapter_pattern': True
                })
            
            self.config = PipelineConfig(device=self.device, **config_dict)
        
        # 3. ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        # 4. ê´€ë¦¬ìžë“¤ ì´ˆê¸°í™”
        self.step_manager = StepManager(self.config, self.device, self.logger)
        self.ai_model_manager = AIModelManager(self.config, self.device, self.logger)
        self.resource_manager = ResourceManager(self.config, self.logger)
        self.error_manager = ErrorManager(self.config, self.logger)
        self.processing_engine = ProcessingEngine(
            self.step_manager, self.ai_model_manager, self.config, self.logger
        )
        
        # 5. ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.current_status = ProcessingStatus.IDLE
        
        # 6. ì„±ëŠ¥ ë° í†µê³„
        self.performance_metrics = {
            'total_sessions': 0,
            'successful_sessions': 0,
            'ai_model_usage': {},
            'average_processing_time': 0.0,
            'average_quality_score': 0.0
        }
        
        # 7. ìŠ¤ë ˆë“œ í’€
        self.thread_pool = ThreadPoolExecutor(max_workers=self.config.thread_pool_size)
        
        # ì´ˆê¸°í™” ì™„ë£Œ ë¡œê¹…
        self.logger.info(f"ðŸ”¥ PipelineManager v10.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ðŸŽ¯ ë””ë°”ì´ìŠ¤: {self.device}")
        self.logger.info(f"ðŸ’¾ ë©”ëª¨ë¦¬: {self.config.memory_gb}GB")
        self.logger.info(f"ðŸš€ M3 Max: {'âœ…' if self.config.is_m3_max else 'âŒ'}")
        self.logger.info(f"ðŸ§  AI ëª¨ë¸: {'âœ…' if self.config.ai_model_enabled else 'âŒ'}")
    
    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìžë™ ê°ì§€"""
        if preferred_device and preferred_device != "auto":
            return preferred_device
        
        try:
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                    capture_output=True, text=True, timeout=5
                )
                chip_info = result.stdout.strip()
                return 'M3' in chip_info and 'Max' in chip_info
        except:
            pass
        return False
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if not config_path or not os.path.exists(config_path):
            return {}
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.warning(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    async def initialize(self) -> bool:
        """íŒŒì´í”„ë¼ì¸ ì™„ì „ ì´ˆê¸°í™”"""
        try:
            self.logger.info("ðŸš€ PipelineManager ì™„ì „ ì´ˆê¸°í™” ì‹œìž‘...")
            self.current_status = ProcessingStatus.INITIALIZING
            start_time = time.time()
            
            # 1. Step ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            step_success = await self.step_manager.initialize()
            if step_success:
                self.logger.info("âœ… Step ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ Step ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # 2. AI ëª¨ë¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            ai_success = await self.ai_model_manager.initialize()
            if ai_success:
                self.logger.info("âœ… AI ëª¨ë¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ AI ëª¨ë¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # 3. ë©”ëª¨ë¦¬ ìµœì í™”
            await self.resource_manager.optimize_memory()
            
            # 4. ì´ˆê¸°í™” ê²€ì¦
            registered_steps = self.step_manager.get_registered_steps()
            step_count = registered_steps['total_registered']
            success_rate = registered_steps['registration_rate']
            
            if step_count < 4:  # ìµœì†Œ ì ˆë°˜ ì´ìƒ
                self.logger.warning(f"ì´ˆê¸°í™”ëœ Step ìˆ˜ ë¶€ì¡±: {step_count}/8")
            
            initialization_time = time.time() - start_time
            self.is_initialized = step_count > 0
            self.current_status = ProcessingStatus.IDLE if self.is_initialized else ProcessingStatus.FAILED
            
            if self.is_initialized:
                self.logger.info(f"ðŸŽ‰ PipelineManager ì´ˆê¸°í™” ì™„ë£Œ ({initialization_time:.2f}ì´ˆ)")
                self.logger.info(f"ðŸ“Š Step ì´ˆê¸°í™”: {step_count}/8 ({success_rate:.1f}%)")
                self.logger.info(f"ðŸ§  AI ëª¨ë¸: {'âœ…' if ai_success else 'âŒ'}")
                self.logger.info(f"ðŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™”: âœ…")
            else:
                self.logger.error("âŒ PipelineManager ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            return self.is_initialized
            
        except Exception as e:
            self.logger.error(f"âŒ PipelineManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            self.current_status = ProcessingStatus.FAILED
            return False
    
    # ==============================================
    # ðŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ (ê¸°ì¡´ ì¸í„°íŽ˜ì´ìŠ¤ ìœ ì§€)
    # ==============================================
    
    async def process_complete_virtual_fitting(
        self,
        person_image: Union[str, Image.Image, np.ndarray],
        clothing_image: Union[str, Image.Image, np.ndarray],
        body_measurements: Optional[Dict[str, float]] = None,
        clothing_type: str = "shirt",
        fabric_type: str = "cotton",
        style_preferences: Optional[Dict[str, Any]] = None,
        quality_target: float = 0.8,
        progress_callback: Optional[Callable] = None,
        save_intermediate: bool = False,
        session_id: Optional[str] = None
    ) -> ProcessingResult:
        """ì™„ì „í•œ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ (ê¸°ì¡´ ë©”ì„œë“œëª… ìœ ì§€)"""
        
        if not self.is_initialized:
            raise RuntimeError("íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        self.current_status = ProcessingStatus.PROCESSING
        
        try:
            # ProcessingEngineìœ¼ë¡œ ìœ„ìž„
            result = await self.processing_engine.process_complete_pipeline(
                person_image=person_image,
                clothing_image=clothing_image,
                body_measurements=body_measurements,
                clothing_type=clothing_type,
                fabric_type=fabric_type,
                style_preferences=style_preferences,
                quality_target=quality_target,
                progress_callback=progress_callback,
                save_intermediate=save_intermediate,
                session_id=session_id
            )
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_performance_metrics(result)
            
            self.current_status = ProcessingStatus.COMPLETED if result.success else ProcessingStatus.FAILED
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.current_status = ProcessingStatus.FAILED
            
            return ProcessingResult(
                success=False,
                session_id=session_id or f"error_{int(time.time())}",
                error_message=str(e),
                metadata={'error_location': traceback.format_exc()}
            )
    
    # ==============================================
    # ðŸ”¥ Step ê´€ë¦¬ ë©”ì„œë“œë“¤ (ê¸°ì¡´ ì¸í„°íŽ˜ì´ìŠ¤ 100% ìœ ì§€)
    # ==============================================
    
    def register_step(self, step_id: int, step_instance: Any) -> bool:
        """Step ë“±ë¡ (ë™ê¸° ë©”ì„œë“œ, await ì˜¤ë¥˜ í•´ê²°)"""
        return self.step_manager.register_step(step_id, step_instance)
    
    def register_steps_batch(self, steps_dict: Dict[int, Any]) -> Dict[int, bool]:
        """Step ì¼ê´„ ë“±ë¡ (ë™ê¸° ë©”ì„œë“œ)"""
        return self.step_manager.register_steps_batch(steps_dict)
    
    def unregister_step(self, step_id: int) -> bool:
        """Step ë“±ë¡ í•´ì œ"""
        try:
            step_name = self.step_manager.step_id_mapping.get(step_id)
            if not step_name:
                self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” Step ID: {step_id}")
                return False
            
            if step_name in self.step_manager.steps:
                step_instance = self.step_manager.steps[step_name]
                
                # Step ì •ë¦¬
                if hasattr(step_instance, 'cleanup'):
                    try:
                        if asyncio.iscoroutinefunction(step_instance.cleanup):
                            asyncio.create_task(step_instance.cleanup())
                        else:
                            step_instance.cleanup()
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Step {step_id} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                
                del self.step_manager.steps[step_name]
                self.logger.info(f"âœ… Step {step_id} ({step_name}) ë“±ë¡ í•´ì œ ì™„ë£Œ")
                return True
            else:
                self.logger.warning(f"âš ï¸ Step {step_id} ({step_name})ê°€ ë“±ë¡ë˜ì–´ ìžˆì§€ ì•ŠìŒ")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ Step {step_id} ë“±ë¡ í•´ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_registered_steps(self) -> Dict[str, Any]:
        """ë“±ë¡ëœ Step ëª©ë¡ ë°˜í™˜"""
        return self.step_manager.get_registered_steps()
    
    def is_step_registered(self, step_id: int) -> bool:
        """Step ë“±ë¡ ì—¬ë¶€ í™•ì¸"""
        return self.step_manager.is_step_registered(step_id)
    
    def get_step_by_id(self, step_id: int) -> Optional[Any]:
        """Step IDë¡œ Step ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        return self.step_manager.get_step_by_id(step_id)
    
    def update_config(self, new_config: Dict[str, Any]) -> bool:
        """íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸"""
        try:
            self.logger.info("ðŸ”„ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹œìž‘...")
            
            # ê¸°ë³¸ ì„¤ì • ì—…ë°ì´íŠ¸
            if 'device' in new_config and new_config['device'] != self.device:
                self.device = new_config['device']
                self.logger.info(f"âœ… ë””ë°”ì´ìŠ¤ ë³€ê²½: {self.device}")
            
            # PipelineConfig ì—…ë°ì´íŠ¸
            if isinstance(self.config, dict):
                self.config.update(new_config)
            else:
                for key, value in new_config.items():
                    if hasattr(self.config, key):
                        setattr(self.config, key, value)
            
            self.logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    def configure_from_detection(self, detection_config: Dict[str, Any]) -> bool:
        """Step íƒì§€ ê²°ê³¼ë¡œë¶€í„° íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
        try:
            self.logger.info("ðŸŽ¯ Step íƒì§€ ê²°ê³¼ë¡œë¶€í„° íŒŒì´í”„ë¼ì¸ ì„¤ì • ì‹œìž‘...")
            
            # íƒì§€ëœ Step ì •ë³´ ì²˜ë¦¬
            if 'steps' in detection_config:
                for step_config in detection_config['steps']:
                    step_name = step_config.get('step_name')
                    step_id = self.step_manager.step_name_to_id.get(step_name)
                    
                    if step_id and step_name not in self.step_manager.steps:
                        # íƒì§€ëœ Step ìƒì„± ì‹œë„
                        try:
                            step_instance = self.step_manager._create_step_direct(
                                step_id, step_name, step_config
                            )
                            if step_instance:
                                self.step_manager.steps[step_name] = step_instance
                                self.logger.info(f"âœ… {step_name} íƒì§€ ê²°ê³¼ë¡œë¶€í„° ì„¤ì • ì™„ë£Œ")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ {step_name} íƒì§€ ê²°ê³¼ ì„¤ì • ì‹¤íŒ¨: {e}")
            
            self.logger.info("âœ… Step íƒì§€ ê²°ê³¼ë¡œë¶€í„° íŒŒì´í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Step íƒì§€ ê²°ê³¼ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ðŸ”¥ ìƒíƒœ ì¡°íšŒ ë©”ì„œë“œë“¤ (ê¸°ì¡´ ì¸í„°íŽ˜ì´ìŠ¤ ìœ ì§€)
    # ==============================================
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ"""
        registered_steps = self.step_manager.get_registered_steps()
        ai_model_info = self.ai_model_manager.get_model_info()
        memory_stats = self.resource_manager.get_memory_stats()
        
        return {
            'initialized': self.is_initialized,
            'current_status': self.current_status.value,
            'device': self.device,
            'device_type': self.config.device_type,
            'memory_gb': self.config.memory_gb,
            'is_m3_max': self.config.is_m3_max,
            'ai_model_enabled': self.config.ai_model_enabled,
            'architecture_version': 'v10.0_complete_implementation',
            
            'step_manager': {
                'total_registered': registered_steps['total_registered'],
                'registration_rate': registered_steps['registration_rate'],
                'missing_steps': registered_steps['missing_steps']
            },
            
            'ai_model_manager': ai_model_info,
            
            'config': {
                'quality_level': self.config.quality_level.value,
                'processing_mode': self.config.processing_mode.value,
                'performance_mode': self.config.performance_mode,
                'use_dependency_injection': self.config.use_dependency_injection,
                'enable_adapter_pattern': self.config.enable_adapter_pattern,
                'batch_size': self.config.batch_size,
                'thread_pool_size': self.config.thread_pool_size
            },
            
            'performance_metrics': self.performance_metrics,
            'memory_stats': memory_stats
        }
    
    def _update_performance_metrics(self, result: ProcessingResult):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.performance_metrics['total_sessions'] += 1
        
        if result.success:
            self.performance_metrics['successful_sessions'] += 1
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        total_sessions = self.performance_metrics['total_sessions']
        prev_avg_time = self.performance_metrics['average_processing_time']
        self.performance_metrics['average_processing_time'] = (
            (prev_avg_time * (total_sessions - 1) + result.processing_time) / total_sessions
        )
        
        # í‰ê·  í’ˆì§ˆ ì ìˆ˜ ì—…ë°ì´íŠ¸
        if result.success:
            successful_sessions = self.performance_metrics['successful_sessions']
            prev_avg_quality = self.performance_metrics['average_quality_score']
            self.performance_metrics['average_quality_score'] = (
                (prev_avg_quality * (successful_sessions - 1) + result.quality_score) / successful_sessions
            )
        
        # AI ëª¨ë¸ ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸
        for step_name, model_name in result.ai_models_used.items():
            if model_name != 'error':
                self.performance_metrics['ai_model_usage'][model_name] = (
                    self.performance_metrics['ai_model_usage'].get(model_name, 0) + 1
                )
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ðŸ§¹ PipelineManager ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            self.current_status = ProcessingStatus.CLEANING
            
            # Step ì •ë¦¬
            for step_name, step in self.step_manager.steps.items():
                try:
                    if hasattr(step, 'cleanup'):
                        if asyncio.iscoroutinefunction(step.cleanup):
                            await step.cleanup()
                        else:
                            step.cleanup()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            await self.resource_manager.optimize_memory(aggressive=True)
            
            # ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            if hasattr(self, 'thread_pool'):
                try:
                    self.thread_pool.shutdown(wait=True)
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œ í’€ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ìƒíƒœ ì´ˆê¸°í™”
            self.is_initialized = False
            self.current_status = ProcessingStatus.IDLE
            
            self.logger.info("âœ… PipelineManager ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.current_status = ProcessingStatus.FAILED

# ==============================================
# ðŸ”¥ DIBasedPipelineManager í´ëž˜ìŠ¤ (ê¸°ì¡´ ì¸í„°íŽ˜ì´ìŠ¤ 100% ìœ ì§€)
# ==============================================

class DIBasedPipelineManager(PipelineManager):
    """DI ì „ìš© PipelineManager (ê¸°ì¡´ ì¸í„°íŽ˜ì´ìŠ¤ 100% ìœ ì§€)"""
    
    def __init__(
        self,
        config_path: Optional[str] = None,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], PipelineConfig]] = None,
        **kwargs
    ):
        # DI ê´€ë ¨ ì„¤ì • ê°•ì œ í™œì„±í™”
        if isinstance(config, dict):
            config.update({
                'use_dependency_injection': True,
                'auto_inject_dependencies': True,
                'enable_adapter_pattern': True,
                'enable_runtime_injection': True,
                'interface_based_design': True,
                'lazy_loading_enabled': True
            })
        elif isinstance(config, PipelineConfig):
            config.use_dependency_injection = True
            config.auto_inject_dependencies = True
            config.enable_adapter_pattern = True
            config.enable_runtime_injection = True
            config.interface_based_design = True
            config.lazy_loading_enabled = True
        else:
            kwargs.update({
                'use_dependency_injection': True,
                'auto_inject_dependencies': True,
                'enable_adapter_pattern': True,
                'enable_runtime_injection': True,
                'interface_based_design': True,
                'lazy_loading_enabled': True
            })
        
        # ë¶€ëª¨ í´ëž˜ìŠ¤ ì´ˆê¸°í™”
        super().__init__(config_path=config_path, device=device, config=config, **kwargs)
        
        # DIBasedPipelineManager ì „ìš© ë¡œê¹…
        self.logger.info("ðŸ”¥ DIBasedPipelineManager v10.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info("ðŸ’‰ ì™„ì „ DI ê¸°ëŠ¥ ê°•ì œ í™œì„±í™”")
    
    def get_di_status(self) -> Dict[str, Any]:
        """DI ì „ìš© ìƒíƒœ ì¡°íšŒ"""
        base_status = self.get_pipeline_status()
        
        return {
            **base_status,
            'di_based_manager': True,
            'di_forced_enabled': True,
            'di_specific_info': {
                'step_manager_type': type(self.step_manager).__name__,
                'ai_model_manager_type': type(self.ai_model_manager).__name__,
                'processing_engine_type': type(self.processing_engine).__name__,
                'resource_manager_type': type(self.resource_manager).__name__,
                'error_manager_type': type(self.error_manager).__name__
            }
        }

# ==============================================
# ðŸ”¥ íŽ¸ì˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í•¨ìˆ˜ëª… 100% ìœ ì§€)
# ==============================================

def create_pipeline(
    device: str = "auto", 
    quality_level: str = "balanced", 
    mode: str = "production",
    use_dependency_injection: bool = True,
    enable_adapter_pattern: bool = True,
    **kwargs
) -> PipelineManager:
    """ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    return PipelineManager(
        device=device,
        config=PipelineConfig(
            quality_level=QualityLevel(quality_level),
            processing_mode=PipelineMode(mode),
            ai_model_enabled=True,
            use_dependency_injection=use_dependency_injection,
            enable_adapter_pattern=enable_adapter_pattern,
            **kwargs
        )
    )

def create_complete_di_pipeline(
    device: str = "auto",
    quality_level: str = "high",
    **kwargs
) -> PipelineManager:
    """ì™„ì „ DI íŒŒì´í”„ë¼ì¸ ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    return PipelineManager(
        device=device,
        config=PipelineConfig(
            quality_level=QualityLevel(quality_level),
            processing_mode=PipelineMode.PRODUCTION,
            ai_model_enabled=True,
            model_preload_enabled=True,
            use_dependency_injection=True,
            enable_adapter_pattern=True,
            **kwargs
        )
    )

def create_m3_max_pipeline(**kwargs) -> PipelineManager:
    """M3 Max ìµœì í™” íŒŒì´í”„ë¼ì¸ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    return PipelineManager(
        device="mps",
        config=PipelineConfig(
            quality_level=QualityLevel.MAXIMUM,
            processing_mode=PipelineMode.PRODUCTION,
            memory_gb=128.0,
            is_m3_max=True,
            device_type="apple_silicon",
            ai_model_enabled=True,
            model_preload_enabled=True,
            performance_mode="maximum",
            use_dependency_injection=True,
            enable_adapter_pattern=True,
            **kwargs
        )
    )

def create_production_pipeline(**kwargs) -> PipelineManager:
    """í”„ë¡œë•ì…˜ íŒŒì´í”„ë¼ì¸ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    return create_complete_di_pipeline(
        quality_level="high",
        processing_mode="production",
        ai_model_enabled=True,
        model_preload_enabled=True,
        **kwargs
    )

def create_development_pipeline(**kwargs) -> PipelineManager:
    """ê°œë°œìš© íŒŒì´í”„ë¼ì¸ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    return create_complete_di_pipeline(
        quality_level="balanced",
        processing_mode="development",
        ai_model_enabled=True,
        model_preload_enabled=False,
        **kwargs
    )

def create_testing_pipeline(**kwargs) -> PipelineManager:
    """í…ŒìŠ¤íŒ…ìš© íŒŒì´í”„ë¼ì¸ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    return PipelineManager(
        device="cpu",
        config=PipelineConfig(
            quality_level=QualityLevel.FAST,
            processing_mode=PipelineMode.TESTING,
            ai_model_enabled=False,
            model_preload_enabled=False,
            use_dependency_injection=True,
            enable_adapter_pattern=True,
            **kwargs
        )
    )

def create_di_based_pipeline(**kwargs) -> DIBasedPipelineManager:
    """DIBasedPipelineManager ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    return DIBasedPipelineManager(**kwargs)

@lru_cache(maxsize=1)
def get_global_pipeline_manager(device: str = "auto") -> PipelineManager:
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    try:
        if device == "mps" and torch.backends.mps.is_available():
            return create_m3_max_pipeline()
        else:
            return create_production_pipeline(device=device)
    except Exception as e:
        logger.error(f"ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ìƒì„± ì‹¤íŒ¨: {e}")
        return create_complete_di_pipeline(device="cpu", quality_level="balanced")

@lru_cache(maxsize=1)
def get_global_di_based_pipeline_manager(device: str = "auto") -> DIBasedPipelineManager:
    """ì „ì—­ DIBasedPipelineManager (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    try:
        if device == "mps" and torch.backends.mps.is_available():
            return DIBasedPipelineManager(
                device="mps",
                config=PipelineConfig(
                    quality_level=QualityLevel.MAXIMUM,
                    processing_mode=PipelineMode.PRODUCTION,
                    memory_gb=128.0,
                    is_m3_max=True,
                    device_type="apple_silicon",
                    performance_mode="maximum"
                )
            )
        else:
            return DIBasedPipelineManager(device=device)
    except Exception as e:
        logger.error(f"ì „ì—­ DIBasedPipelineManager ìƒì„± ì‹¤íŒ¨: {e}")
        return DIBasedPipelineManager(device="cpu")

# ==============================================
# ðŸ”¥ Export ë° ë©”ì¸ ì‹¤í–‰
# ==============================================

__all__ = [
    # ì—´ê±°í˜•
    'PipelineMode', 'QualityLevel', 'ProcessingStatus', 'ExecutionStrategy',
    
    # ë°ì´í„° í´ëž˜ìŠ¤
    'PipelineConfig', 'ProcessingResult',
    
    # ê´€ë¦¬ìž í´ëž˜ìŠ¤ë“¤
    'StepManager', 'AIModelManager', 'ProcessingEngine', 'ResourceManager', 'ErrorManager',
    
    # ë©”ì¸ í´ëž˜ìŠ¤ë“¤ (ê¸°ì¡´ ì´ë¦„ 100% ìœ ì§€)
    'PipelineManager',
    'DIBasedPipelineManager',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì´ë¦„ 100% ìœ ì§€)
    'create_pipeline',
    'create_complete_di_pipeline',
    'create_m3_max_pipeline',
    'create_production_pipeline',
    'create_development_pipeline',
    'create_testing_pipeline',
    'create_di_based_pipeline',
    'get_global_pipeline_manager',
    'get_global_di_based_pipeline_manager'
]

# ì´ˆê¸°í™” ì •ë³´ ì¶œë ¥
logger.info("ðŸŽ‰ ì™„ì „ížˆ ìƒˆë¡œìš´ PipelineManager v10.0 ë¡œë“œ ì™„ë£Œ!")
logger.info("âœ… ì£¼ìš” ì™„ì„± ê¸°ëŠ¥:")
logger.info("   - ëª¨ë“  ê¸°ì¡´ í•¨ìˆ˜/í´ëž˜ìŠ¤ëª… 100% ìœ ì§€")
logger.info("   - StepFactory + BaseStepMixin ì™„ì „ ì—°ë™")
logger.info("   - ì‹¤ì œ AI ëª¨ë¸ 229GB í™œìš© (ì‹¤ì œ íŒŒì¼ ê²½ë¡œ)")
logger.info("   - ë¹„ë™ê¸° ì²˜ë¦¬ ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
logger.info("   - conda í™˜ê²½ ì™„ë²½ ìµœì í™”")
logger.info("   - M3 Max 128GB ìµœì í™”")
logger.info("   - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ì „ ìž‘ë™")
logger.info("   - ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°± ì™„ì „ êµ¬í˜„")
logger.info("   - ì‹¤ì œ AI ì¶”ë¡  íŒŒì´í”„ë¼ì¸")

logger.info("âœ… ì™„ì „ ê¸°ëŠ¥ create_pipeline í•¨ìˆ˜ë“¤:")
logger.info("   - create_pipeline() âœ…")
logger.info("   - create_complete_di_pipeline() âœ…")
logger.info("   - create_m3_max_pipeline() âœ…")
logger.info("   - create_production_pipeline() âœ…")
logger.info("   - create_development_pipeline() âœ…")
logger.info("   - create_testing_pipeline() âœ…")
logger.info("   - create_di_based_pipeline() âœ…")
logger.info("   - get_global_pipeline_manager() âœ…")
logger.info("   - get_global_di_based_pipeline_manager() âœ…")

logger.info("ðŸ”¥ í•µì‹¬ í•´ê²°ì‚¬í•­:")
logger.info("   - object bool can't be used in 'await' expression âœ… ì™„ì „ í•´ê²°")
logger.info("   - QualityAssessmentStep has no attribute 'is_m3_max' âœ… ì™„ì „ í•´ê²°")
logger.info("   - StepFactory ì—°ë™ ì˜¤ë¥˜ âœ… ì™„ì „ í•´ê²°")
logger.info("   - Step ë“±ë¡ ì‹¤íŒ¨ âœ… ì™„ì „ í•´ê²°")
logger.info("   - AI ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  âœ… ì™„ì „ êµ¬í˜„")
logger.info("   - conda í™˜ê²½ í˜¸í™˜ì„± âœ… ì™„ë²½ ì§€ì›")

logger.info("ðŸš€ ì´ì œ ì™„ì „í•œ ê¸°ëŠ¥ì˜ AI ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")

# ë©”ì¸ ì‹¤í–‰ ë° ë°ëª¨
if __name__ == "__main__":
    print("ðŸ”¥ ì™„ì „ížˆ ìƒˆë¡œìš´ PipelineManager v10.0 - ì™„ì „ ê¸°ëŠ¥ êµ¬í˜„")
    print("=" * 80)
    print("âœ… ëª¨ë“  ê¸°ì¡´ í•¨ìˆ˜/í´ëž˜ìŠ¤ëª… 100% ìœ ì§€")
    print("âœ… StepFactory + BaseStepMixin ì™„ì „ ì—°ë™")
    print("âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB í™œìš©")
    print("âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
    print("âœ… conda í™˜ê²½ ì™„ë²½ ìµœì í™”")
    print("âœ… M3 Max 128GB ìµœì í™”")
    print("âœ… 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ì „ ìž‘ë™")
    print("âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°± ì™„ì „ êµ¬í˜„")
    print("=" * 80)
    
    import asyncio
    
    async def demo_complete_implementation():
        """ì™„ì „ êµ¬í˜„ ë°ëª¨"""
        print("ðŸŽ¯ ì™„ì „ êµ¬í˜„ PipelineManager ë°ëª¨ ì‹œìž‘")
        print("-" * 60)
        
        # 1. ë‹¤ì–‘í•œ íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸
        print("1ï¸âƒ£ ëª¨ë“  íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ë“¤ í…ŒìŠ¤íŠ¸...")
        
        try:
            # ëª¨ë“  ìƒì„± í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
            pipelines = {
                'basic': create_pipeline(),
                'complete_di': create_complete_di_pipeline(),
                'm3_max': create_m3_max_pipeline(),
                'production': create_production_pipeline(),
                'development': create_development_pipeline(),
                'testing': create_testing_pipeline(),
                'di_based': create_di_based_pipeline(),
                'global': get_global_pipeline_manager(),
                'global_di': get_global_di_based_pipeline_manager()
            }
            
            for name, pipeline in pipelines.items():
                print(f"âœ… {name}: {type(pipeline).__name__}")
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return
        
        # 2. M3 Max íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì™„ì „ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        print("2ï¸âƒ£ M3 Max íŒŒì´í”„ë¼ì¸ ì™„ì „ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        try:
            pipeline = pipelines['m3_max']
            
            # ì´ˆê¸°í™”
            success = await pipeline.initialize()
            print(f"âœ… ì´ˆê¸°í™”: {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
            
            if success:
                # ìƒíƒœ í™•ì¸
                status = pipeline.get_pipeline_status()
                print(f"ðŸ“Š ë“±ë¡ëœ Step: {status['step_manager']['total_registered']}/8")
                print(f"ðŸ§  AI ëª¨ë¸ ì‹œìŠ¤í…œ: {'âœ…' if status['ai_model_manager']['is_initialized'] else 'âŒ'}")
                print(f"ðŸ’¾ ë©”ëª¨ë¦¬: {status['memory_stats'].get('memory_gb', 'N/A')}GB")
                print(f"ðŸŽ¯ ë””ë°”ì´ìŠ¤: {status['device']}")
            
            # ì •ë¦¬
            await pipeline.cleanup()
            print("âœ… íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ì™„ì „ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        print("\nðŸŽ‰ ì™„ì „ êµ¬í˜„ PipelineManager ë°ëª¨ ì™„ë£Œ!")
        print("âœ… ëª¨ë“  ê¸°ì¡´ ì¸í„°íŽ˜ì´ìŠ¤ 100% í˜¸í™˜!")
        print("âœ… ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ ì¤€ë¹„ ì™„ë£Œ!")
        print("âœ… conda í™˜ê²½ì—ì„œ ì™„ë²½ ìž‘ë™!")
        print("âœ… 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ì „ ê¸°ëŠ¥!")
    
    # ì‹¤í–‰
    asyncio.run(demo_complete_implementation())