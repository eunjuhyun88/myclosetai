# backend/app/ai_pipeline/pipeline_manager.py
"""
ğŸ”¥ ì™„ì „í•œ PipelineManager - AI ëª¨ë¸ ì—°ë™ ì™„ì„± + ì„±ëŠ¥ ìµœì í™” ë²„ì „
âœ… paste.txtì˜ ModelLoader Dict ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… paste-2.txt ê¸°ë°˜ìœ¼ë¡œ í†µí•© ì‹œìŠ¤í…œ ìš°ì„  ì‚¬ìš©
âœ… Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ ì™„ì„±ë„ ê·¹ëŒ€í™”
âœ… í´ë°± ì „ëµ 2ë‹¨ê³„ë¡œ ìµœì í™” (í†µí•© ì‹œìŠ¤í…œ â†’ ModelLoader â†’ ê¸°ë³¸)
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš© ê·¹ëŒ€í™”
âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ìµœì í™”
âœ… conda í™˜ê²½ ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ëˆ„ë½ëœ create_pipeline í•¨ìˆ˜ë“¤ ì™„ì „ ì¶”ê°€

ì•„í‚¤í…ì²˜:
PipelineManager (Main Controller)
â”œâ”€â”€ ModelLoaderManager (AI ëª¨ë¸ ê´€ë¦¬ - ìš°ì„ ìˆœìœ„ 1)
â”œâ”€â”€ UnifiedSystemManager (í†µí•© ì‹œìŠ¤í…œ - ìš°ì„ ìˆœìœ„ 2)
â”œâ”€â”€ ExecutionManager (ì‹¤í–‰ ê´€ë¦¬ - 2ë‹¨ê³„ í´ë°±)
â”œâ”€â”€ PerformanceOptimizer (ì„±ëŠ¥ ìµœì í™”)
â””â”€â”€ StepAIConnector (Stepë³„ AI ëª¨ë¸ ì™„ì „ ì—°ë™)

ì‹¤í–‰ ì „ëµ (2ë‹¨ê³„ í´ë°±):
1ìˆœìœ„: í†µí•© ì‹œìŠ¤í…œ + AI ëª¨ë¸
2ìˆœìœ„: ModelLoader + ê¸°ë³¸ ì²˜ë¦¬
ìµœì¢…: ê¸°ë³¸ í´ë°± (ì—ëŸ¬ ì‹œì—ë§Œ)
"""

import os
import sys
import logging
import asyncio
import time
import traceback
import threading
import json
import gc
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, Callable, Union, List, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from abc import ABC, abstractmethod

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image, ImageEnhance, ImageFilter
import psutil

# ==============================================
# ğŸ”¥ 1. í†µí•© ì‹œìŠ¤í…œ import (ìµœìš°ì„ )
# ==============================================

# í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ
try:
    from app.ai_pipeline.utils import (
        initialize_global_utils, get_utils_manager, 
        get_system_status, optimize_system_memory,
        get_step_model_interface, get_step_memory_manager,
        get_step_data_converter, preprocess_image_for_step
    )
    UNIFIED_UTILS_AVAILABLE = True
except ImportError as e:
    UNIFIED_UTILS_AVAILABLE = False
    logging.warning(f"í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€: {e}")

# ModelLoader ì‹œìŠ¤í…œ
try:
    from app.ai_pipeline.utils.model_loader import (
        ModelLoader, get_global_model_loader, initialize_global_model_loader,
        StepModelInterface
    )
    MODEL_LOADER_AVAILABLE = True
except ImportError as e:
    MODEL_LOADER_AVAILABLE = False
    logging.warning(f"ModelLoader ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€: {e}")

# Step ëª¨ë¸ ìš”ì²­ ì‹œìŠ¤í…œ
try:
    from app.ai_pipeline.utils.step_model_requests import (
        get_step_request, StepModelRequestAnalyzer, 
        STEP_MODEL_REQUESTS, get_all_step_requirements
    )
    STEP_REQUESTS_AVAILABLE = True
except ImportError as e:
    STEP_REQUESTS_AVAILABLE = False
    logging.warning(f"Step ìš”ì²­ ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€: {e}")

# ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ
try:
    from app.ai_pipeline.utils.auto_model_detector import (
        RealWorldModelDetector, create_real_world_detector,
        quick_real_model_detection
    )
    AUTO_DETECTOR_AVAILABLE = True
except ImportError as e:
    AUTO_DETECTOR_AVAILABLE = False
    logging.warning(f"ìë™ íƒì§€ ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€: {e}")

# Step í´ë˜ìŠ¤ë“¤ import
try:
    from app.ai_pipeline.steps.step_01_human_parsing import HumanParsingStep
    from app.ai_pipeline.steps.step_02_pose_estimation import PoseEstimationStep
    from app.ai_pipeline.steps.step_03_cloth_segmentation import ClothSegmentationStep
    from app.ai_pipeline.steps.step_04_geometric_matching import GeometricMatchingStep
    from app.ai_pipeline.steps.step_05_cloth_warping import ClothWarpingStep
    from app.ai_pipeline.steps.step_06_virtual_fitting import VirtualFittingStep
    from app.ai_pipeline.steps.step_07_post_processing import PostProcessingStep
    from app.ai_pipeline.steps.step_08_quality_assessment import QualityAssessmentStep
    STEP_CLASSES_AVAILABLE = True
except ImportError as e:
    STEP_CLASSES_AVAILABLE = False
    logging.error(f"Step í´ë˜ìŠ¤ë“¤ import ì‹¤íŒ¨: {e}")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 2. ì—´ê±°í˜• ë° ë°ì´í„° í´ë˜ìŠ¤
# ==============================================

class PipelineMode(Enum):
    """íŒŒì´í”„ë¼ì¸ ëª¨ë“œ"""
    DEVELOPMENT = "development"
    PRODUCTION = "production"
    TESTING = "testing"
    OPTIMIZATION = "optimization"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    MAXIMUM = "maximum"

class ProcessingStatus(Enum):
    """ì²˜ë¦¬ ìƒíƒœ"""
    IDLE = "idle"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CLEANING = "cleaning"

class ExecutionStrategy(Enum):
    """ì‹¤í–‰ ì „ëµ (2ë‹¨ê³„ í´ë°±)"""
    UNIFIED_AI = "unified_ai"        # í†µí•© ì‹œìŠ¤í…œ + AI ëª¨ë¸
    MODEL_LOADER = "model_loader"    # ModelLoader + ê¸°ë³¸ ì²˜ë¦¬
    BASIC_FALLBACK = "basic_fallback" # ê¸°ë³¸ í´ë°± (ì—ëŸ¬ ì‹œì—ë§Œ)

@dataclass
class PipelineConfig:
    """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    device: str = "auto"
    quality_level: Union[QualityLevel, str] = QualityLevel.HIGH
    processing_mode: Union[PipelineMode, str] = PipelineMode.PRODUCTION
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    memory_gb: float = 128.0
    is_m3_max: bool = True
    device_type: str = "apple_silicon"
    
    # ğŸ”¥ AI ëª¨ë¸ ì—°ë™ ì„¤ì • (ìµœìš°ì„ )
    ai_model_enabled: bool = True
    model_preload_enabled: bool = True
    model_cache_size: int = 20  # M3 Maxìš© í™•ì¥
    ai_inference_timeout: int = 120
    model_fallback_enabled: bool = True
    
    # ğŸ”¥ ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    performance_mode: str = "maximum"
    memory_optimization: bool = True
    gpu_memory_fraction: float = 0.95
    use_fp16: bool = True
    enable_quantization: bool = True
    parallel_processing: bool = True
    batch_processing: bool = True
    async_processing: bool = True
    
    # ğŸ”¥ 2ë‹¨ê³„ í´ë°± ì„¤ì •
    max_fallback_attempts: int = 2
    fallback_timeout: int = 30
    enable_smart_fallback: bool = True
    
    # ì²˜ë¦¬ ì„¤ì •
    batch_size: int = 4  # M3 Max ìµœì í™”
    max_retries: int = 2  # í´ë°± ìµœì í™”
    timeout_seconds: int = 300
    thread_pool_size: int = 8  # M3 Max ë©€í‹°ì½”ì–´ í™œìš©
    
    def __post_init__(self):
        # ë¬¸ìì—´ì„ Enumìœ¼ë¡œ ë³€í™˜
        if isinstance(self.quality_level, str):
            self.quality_level = QualityLevel(self.quality_level)
        if isinstance(self.processing_mode, str):
            self.processing_mode = PipelineMode(self.processing_mode)
        
        # M3 Max ìë™ ìµœì í™”
        if self.is_m3_max:
            self.memory_gb = max(self.memory_gb, 128.0)
            self.model_cache_size = 20
            self.batch_size = 4
            self.thread_pool_size = 8
            self.gpu_memory_fraction = 0.95
            self.performance_mode = "maximum"

@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼ - AI ëª¨ë¸ ì •ë³´ í¬í•¨"""
    success: bool
    session_id: str = ""
    result_image: Optional[Image.Image] = None
    result_tensor: Optional[torch.Tensor] = None
    quality_score: float = 0.0
    quality_grade: str = "Unknown"
    processing_time: float = 0.0
    step_results: Dict[str, Any] = field(default_factory=dict)
    step_timings: Dict[str, float] = field(default_factory=dict)
    ai_models_used: Dict[str, str] = field(default_factory=dict)  # ğŸ”¥ AI ëª¨ë¸ ì¶”ì 
    execution_strategies: Dict[str, str] = field(default_factory=dict)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)

@dataclass
class AIModelInfo:
    """AI ëª¨ë¸ ì •ë³´"""
    model_name: str
    model_type: str
    model_size: str
    checkpoint_path: str
    loaded: bool = False
    performance_score: float = 0.0
    memory_usage: float = 0.0
    inference_time: float = 0.0

# ==============================================
# ğŸ”¥ 3. ì„±ëŠ¥ ìµœì í™” ê´€ë¦¬ì
# ==============================================

class PerformanceOptimizer:
    """M3 Max ì„±ëŠ¥ ìµœì í™” ê´€ë¦¬ì"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        self.memory_pool = {}
        self.model_cache = {}
        self.performance_cache = {}
        
    def optimize_system(self):
        """ì‹œìŠ¤í…œ ìµœì í™”"""
        try:
            # ğŸ”¥ M3 Max íŠ¹í™” ìµœì í™”
            if self.config.is_m3_max:
                self._optimize_m3_max()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if self.config.memory_optimization:
                self._optimize_memory()
            
            # GPU ìµœì í™”
            if self.config.device in ['mps', 'cuda']:
                self._optimize_gpu()
            
            # ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
            if self.config.parallel_processing:
                self._optimize_parallel_processing()
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_m3_max(self):
        """M3 Max íŠ¹í™” ìµœì í™”"""
        try:
            # MPS ìµœì í™”
            if torch.backends.mps.is_available():
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.95'
                torch.mps.empty_cache()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            os.environ['OMP_NUM_THREADS'] = '8'
            os.environ['MKL_NUM_THREADS'] = '8'
            
            self.logger.info("âœ… M3 Max ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”
            gc.set_threshold(700, 10, 10)
            gc.collect()
            
            # ë©”ëª¨ë¦¬ í’€ ë¯¸ë¦¬ í• ë‹¹
            if self.config.device == 'mps':
                # MPS ë©”ëª¨ë¦¬ ë¯¸ë¦¬ í• ë‹¹
                dummy_tensor = torch.zeros(1024, 1024, device='mps')
                del dummy_tensor
                torch.mps.empty_cache()
            
            self.logger.info("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_gpu(self):
        """GPU ìµœì í™”"""
        try:
            if self.config.device == 'mps':
                # MPS ìµœì í™”
                torch.backends.mps.enable_fallback = True
            elif self.config.device == 'cuda':
                # CUDA ìµœì í™”
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
            
            self.logger.info("âœ… GPU ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ GPU ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_parallel_processing(self):
        """ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”"""
        try:
            # ìŠ¤ë ˆë“œ í’€ í¬ê¸° ìµœì í™”
            cpu_count = psutil.cpu_count(logical=False)
            optimal_threads = min(self.config.thread_pool_size, cpu_count * 2)
            
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            os.environ['OPENBLAS_NUM_THREADS'] = str(optimal_threads)
            
            self.logger.info(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” ì™„ë£Œ (ìŠ¤ë ˆë“œ: {optimal_threads})")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 4. AI ëª¨ë¸ ì—°ë™ ê´€ë¦¬ì
# ==============================================

class ModelLoaderManager:
    """AI ëª¨ë¸ ë¡œë” ê´€ë¦¬ì - Dict ë¬¸ì œ ì™„ì „ í•´ê²°"""
    
    def __init__(self, config: PipelineConfig, device: str, logger: logging.Logger):
        self.config = config
        self.device = device
        self.logger = logger
        self.model_loader = None
        self.model_interfaces = {}
        self.loaded_models = {}
        self.model_cache = {}
        self.is_initialized = False
        
    async def initialize(self) -> bool:
        """ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” - Dict ë¬¸ì œ í•´ê²°"""
        try:
            self.logger.info("ğŸ§  ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
            
            if not MODEL_LOADER_AVAILABLE:
                self.logger.warning("âš ï¸ ModelLoader ì‚¬ìš© ë¶ˆê°€")
                return False
            
            # ğŸ”¥ Dict ë¬¸ì œ í•´ê²°: ì•ˆì „í•œ ì´ˆê¸°í™”
            max_attempts = 3
            for attempt in range(max_attempts):
                try:
                    # ì „ì—­ ì´ˆê¸°í™” ì‹œë„
                    self.model_loader = await asyncio.get_event_loop().run_in_executor(
                        None, initialize_global_model_loader
                    )
                    
                    # Dict íƒ€ì… ê²€ì¦
                    if isinstance(self.model_loader, dict):
                        self.logger.warning(f"âš ï¸ ModelLoaderê°€ dict íƒ€ì… (ì‹œë„ {attempt + 1})")
                        
                        # ì§ì ‘ ìƒì„± ì‹œë„
                        self.model_loader = ModelLoader(device=self.device)
                        if hasattr(self.model_loader, 'initialize'):
                            await self.model_loader.initialize()
                    
                    # ìµœì¢… ê²€ì¦
                    if (not isinstance(self.model_loader, dict) and 
                        hasattr(self.model_loader, 'create_step_interface')):
                        self.is_initialized = True
                        self.logger.info("âœ… ModelLoader ì´ˆê¸°í™” ì„±ê³µ")
                        break
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ModelLoader ì´ˆê¸°í™” ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                    if attempt < max_attempts - 1:
                        await asyncio.sleep(1)
                        continue
            
            if not self.is_initialized:
                self.logger.error("âŒ ModelLoader ì´ˆê¸°í™” ì™„ì „ ì‹¤íŒ¨")
                return False
            
            # Stepë³„ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            await self._create_step_interfaces()
            
            # ì¤‘ìš” ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
            if self.config.model_preload_enabled:
                await self._preload_critical_models()
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _create_step_interfaces(self):
        """Stepë³„ ModelLoader ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        try:
            step_names = [
                'HumanParsingStep', 'PoseEstimationStep', 'ClothSegmentationStep',
                'GeometricMatchingStep', 'ClothWarpingStep', 'VirtualFittingStep',
                'PostProcessingStep', 'QualityAssessmentStep'
            ]
            
            for step_name in step_names:
                try:
                    interface = self.model_loader.create_step_interface(step_name)
                    self.model_interfaces[step_name] = interface
                    self.logger.info(f"âœ… {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            self.logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
    
    async def _preload_critical_models(self):
        """ì¤‘ìš” AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ"""
        try:
            # ğŸ”¥ í•µì‹¬ Stepë“¤ì˜ AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
            critical_models = [
                ('HumanParsingStep', 'graphonomy'),
                ('ClothSegmentationStep', 'u2net'),
                ('VirtualFittingStep', 'ootdiffusion'),
                ('QualityAssessmentStep', 'clipiqa')
            ]
            
            for step_name, model_name in critical_models:
                try:
                    if step_name in self.model_interfaces:
                        interface = self.model_interfaces[step_name]
                        model = await interface.get_model(model_name)
                        if model:
                            self.loaded_models[f"{step_name}_{model_name}"] = model
                            self.logger.info(f"âœ… {step_name} AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ: {model_name}")
                        else:
                            self.logger.warning(f"âš ï¸ {step_name} AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì˜¤ë¥˜: {e}")
                    
        except Exception as e:
            self.logger.error(f"âŒ ì¤‘ìš” ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def get_step_interface(self, step_name: str) -> Optional[Any]:
        """Step ì¸í„°í˜ì´ìŠ¤ ë°˜í™˜"""
        return self.model_interfaces.get(step_name)
    
    def get_loaded_model(self, step_name: str, model_name: str) -> Optional[Any]:
        """ë¡œë“œëœ ëª¨ë¸ ë°˜í™˜"""
        key = f"{step_name}_{model_name}"
        return self.loaded_models.get(key)

# ==============================================
# ğŸ”¥ 5. í†µí•© ì‹œìŠ¤í…œ ê´€ë¦¬ì
# ==============================================

class UnifiedSystemManager:
    """í†µí•© ì‹œìŠ¤í…œ ê´€ë¦¬ì"""
    
    def __init__(self, config: PipelineConfig, device: str, logger: logging.Logger):
        self.config = config
        self.device = device
        self.logger = logger
        self.utils_manager = None
        self.auto_detector = None
        self.is_initialized = False
        
    async def initialize(self) -> bool:
        """í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            if not UNIFIED_UTILS_AVAILABLE:
                self.logger.warning("âš ï¸ í†µí•© ìœ í‹¸ë¦¬í‹° ì‚¬ìš© ë¶ˆê°€")
                return False
            
            self.logger.info("ğŸ”— í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
            
            # ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™”
            result = await initialize_global_utils(
                device=self.device,
                memory_gb=self.config.memory_gb,
                is_m3_max=self.config.is_m3_max,
                optimization_enabled=True
            )
            
            if result.get("success", False):
                self.utils_manager = get_utils_manager()
                
                # ìë™ íƒì§€ ì‹œìŠ¤í…œ ì—°ë™
                if AUTO_DETECTOR_AVAILABLE:
                    self.auto_detector = create_real_world_detector()
                
                self.is_initialized = True
                self.logger.info("âœ… í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                return True
            
            self.logger.error("âŒ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

# ==============================================
# ğŸ”¥ 6. Stepë³„ AI ì—°ë™ ê´€ë¦¬ì
# ==============================================

class StepAIConnector:
    """Stepë³„ AI ëª¨ë¸ ì™„ì „ ì—°ë™"""
    
    def __init__(self, model_manager: ModelLoaderManager, unified_manager: UnifiedSystemManager):
        self.model_manager = model_manager
        self.unified_manager = unified_manager
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        self.step_ai_models = {}
        
    async def setup_step_ai_connection(self, step_instance, step_name: str):
        """Stepë³„ AI ì—°ê²° ì„¤ì •"""
        try:
            step_class_name = f"{step_name.title().replace('_', '')}Step"
            
            # ğŸ”¥ 1ìˆœìœ„: í†µí•© ì‹œìŠ¤í…œ ì¸í„°í˜ì´ìŠ¤
            if self.unified_manager.is_initialized and self.unified_manager.utils_manager:
                try:
                    unified_interface = self.unified_manager.utils_manager.create_step_interface(step_class_name)
                    setattr(step_instance, 'unified_interface', unified_interface)
                    self.logger.info(f"âœ… {step_name} í†µí•© ì¸í„°í˜ì´ìŠ¤ ì—°ê²°")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} í†µí•© ì¸í„°í˜ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ 2ìˆœìœ„: ModelLoader ì¸í„°í˜ì´ìŠ¤
            if self.model_manager.is_initialized:
                try:
                    model_interface = self.model_manager.get_step_interface(step_class_name)
                    if model_interface:
                        setattr(step_instance, 'model_interface', model_interface)
                        self.logger.info(f"âœ… {step_name} ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ê²°")
                        
                        # ğŸ”¥ í•µì‹¬: ì‹¤ì œ AI ëª¨ë¸ ì—°ë™
                        await self._setup_real_ai_model(step_instance, step_name, step_class_name)
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            self.logger.error(f"âŒ {step_name} AI ì—°ê²° ì„¤ì • ì‹¤íŒ¨: {e}")
    
    async def _setup_real_ai_model(self, step_instance, step_name: str, step_class_name: str):
        """ì‹¤ì œ AI ëª¨ë¸ ì—°ë™"""
        try:
            # Stepë³„ ê¶Œì¥ AI ëª¨ë¸ ë§¤í•‘
            ai_model_mapping = {
                'human_parsing': 'graphonomy',
                'pose_estimation': 'mediapipe_pose',
                'cloth_segmentation': 'u2net',
                'geometric_matching': 'thin_plate_spline',
                'cloth_warping': 'tps_warping',
                'virtual_fitting': 'ootdiffusion',
                'post_processing': 'esrgan',
                'quality_assessment': 'clipiqa'
            }
            
            model_name = ai_model_mapping.get(step_name)
            if model_name and hasattr(step_instance, 'model_interface'):
                try:
                    # ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ
                    ai_model = await step_instance.model_interface.get_model(model_name)
                    if ai_model:
                        setattr(step_instance, '_ai_model', ai_model)
                        setattr(step_instance, '_ai_model_name', model_name)
                        self.step_ai_models[step_name] = ai_model
                        self.logger.info(f"ğŸ§  {step_name} ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ ì™„ë£Œ: {model_name}")
                    else:
                        self.logger.warning(f"âš ï¸ {step_name} AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} AI ëª¨ë¸ ì—°ë™ ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 7. ì‹¤í–‰ ê´€ë¦¬ì (2ë‹¨ê³„ í´ë°±)
# ==============================================

class OptimizedExecutionManager:
    """ìµœì í™”ëœ ì‹¤í–‰ ê´€ë¦¬ì - 2ë‹¨ê³„ í´ë°±"""
    
    def __init__(self, config: PipelineConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.execution_cache = {}
        self.performance_stats = {}
        
    async def execute_step_optimized(
        self,
        step,
        step_name: str,
        current_data: torch.Tensor,
        clothing_tensor: torch.Tensor,
        **kwargs
    ) -> Tuple[Dict[str, Any], str]:
        """ìµœì í™”ëœ Step ì‹¤í–‰ - 2ë‹¨ê³„ í´ë°±"""
        
        start_time = time.time()
        execution_attempts = []
        
        # ğŸ”¥ 1ìˆœìœ„: í†µí•© ì‹œìŠ¤í…œ + AI ëª¨ë¸
        try:
            result, strategy = await self._execute_unified_ai(
                step, step_name, current_data, clothing_tensor, **kwargs
            )
            execution_attempts.append(("unified_ai", result.get('success', False)))
            
            if result.get('success', False):
                result['execution_time'] = time.time() - start_time
                return result, strategy
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ {step_name} í†µí•© AI ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            execution_attempts.append(("unified_ai", False))
        
        # ğŸ”¥ 2ìˆœìœ„: ModelLoader + ê¸°ë³¸ ì²˜ë¦¬
        try:
            result, strategy = await self._execute_model_loader(
                step, step_name, current_data, clothing_tensor, **kwargs
            )
            execution_attempts.append(("model_loader", result.get('success', False)))
            
            if result.get('success', False):
                result['execution_time'] = time.time() - start_time
                return result, strategy
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ {step_name} ModelLoader ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            execution_attempts.append(("model_loader", False))
        
        # ğŸ”¥ ìµœì¢… í´ë°±: ê¸°ë³¸ ì²˜ë¦¬ (ì—ëŸ¬ ì‹œì—ë§Œ)
        try:
            result, strategy = await self._execute_basic_fallback(
                step, step_name, current_data, clothing_tensor, **kwargs
            )
            execution_attempts.append(("basic_fallback", result.get('success', False)))
            
            result['execution_time'] = time.time() - start_time
            result['execution_attempts'] = execution_attempts
            
            return result, strategy
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ëª¨ë“  ì‹¤í–‰ ì „ëµ ì‹¤íŒ¨: {e}")
            
            return {
                'success': False,
                'error': f"ëª¨ë“  ì‹¤í–‰ ì „ëµ ì‹¤íŒ¨: {str(e)}",
                'execution_time': time.time() - start_time,
                'execution_attempts': execution_attempts,
                'confidence': 0.0,
                'quality_score': 0.0
            }, "failed"
    
    async def _execute_unified_ai(self, step, step_name: str, current_data: torch.Tensor, 
                                  clothing_tensor: torch.Tensor, **kwargs) -> Tuple[Dict[str, Any], str]:
        """í†µí•© ì‹œìŠ¤í…œ + AI ëª¨ë¸ ì‹¤í–‰"""
        try:
            # í†µí•© ì¸í„°í˜ì´ìŠ¤ ìš°ì„  ì‚¬ìš©
            if hasattr(step, 'unified_interface') and step.unified_interface:
                result = step.unified_interface.process_image(
                    current_data,
                    clothing_data=clothing_tensor,
                    optimize_memory=True,
                    **kwargs
                )
                
                if result and result.get('success', False):
                    return {
                        'success': True,
                        'result': result.get('processed_image', current_data),
                        'confidence': result.get('confidence', 0.95),
                        'quality_score': result.get('quality_score', 0.95),
                        'model_used': result.get('model_used', 'unified_ai'),
                        'ai_model_name': result.get('ai_model_name', 'unified_system'),
                        'processing_method': 'unified_ai'
                    }, ExecutionStrategy.UNIFIED_AI.value
            
            # AI ëª¨ë¸ ì§ì ‘ ì‚¬ìš©
            if hasattr(step, '_ai_model') and step._ai_model:
                ai_result = await self._run_ai_inference(step._ai_model, current_data, clothing_tensor, **kwargs)
                if ai_result:
                    return {
                        'success': True,
                        'result': ai_result,
                        'confidence': 0.92,
                        'quality_score': 0.92,
                        'model_used': getattr(step, '_ai_model_name', 'unknown_ai'),
                        'ai_model_name': getattr(step, '_ai_model_name', 'unknown_ai'),
                        'processing_method': 'direct_ai'
                    }, ExecutionStrategy.UNIFIED_AI.value
            
            raise Exception("í†µí•© AI ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€")
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0
            }, "unified_ai_error"
    
    async def _execute_model_loader(self, step, step_name: str, current_data: torch.Tensor,
                                   clothing_tensor: torch.Tensor, **kwargs) -> Tuple[Dict[str, Any], str]:
        """ModelLoader + ê¸°ë³¸ ì²˜ë¦¬ ì‹¤í–‰"""
        try:
            # ModelLoader ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
            if hasattr(step, 'model_interface') and step.model_interface:
                # ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡ 
                available_models = await step.model_interface.list_available_models()
                if available_models:
                    model = await step.model_interface.get_model(available_models[0])
                    if model:
                        ai_result = await self._run_ai_inference(model, current_data, clothing_tensor, **kwargs)
                        if ai_result is not None:
                            return {
                                'success': True,
                                'result': ai_result,
                                'confidence': 0.88,
                                'quality_score': 0.88,
                                'model_used': available_models[0],
                                'ai_model_name': available_models[0],
                                'processing_method': 'model_loader'
                            }, ExecutionStrategy.MODEL_LOADER.value
            
            # Step ê¸°ë³¸ ì²˜ë¦¬ ë¡œì§
            result = await self._execute_step_logic(step, step_name, current_data, clothing_tensor, **kwargs)
            
            return {
                'success': True,
                'result': result.get('result', current_data),
                'confidence': result.get('confidence', 0.85),
                'quality_score': result.get('quality_score', 0.85),
                'model_used': 'step_logic',
                'ai_model_name': 'step_processing',
                'processing_method': 'step_logic'
            }, ExecutionStrategy.MODEL_LOADER.value
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0
            }, "model_loader_error"
    
    async def _execute_basic_fallback(self, step, step_name: str, current_data: torch.Tensor,
                                     clothing_tensor: torch.Tensor, **kwargs) -> Tuple[Dict[str, Any], str]:
        """ê¸°ë³¸ í´ë°± ì‹¤í–‰"""
        try:
            # ìµœì†Œí•œì˜ ì²˜ë¦¬
            result = await self._execute_step_logic(step, step_name, current_data, clothing_tensor, **kwargs)
            
            return {
                'success': True,
                'result': result.get('result', current_data),
                'confidence': 0.75,
                'quality_score': 0.75,
                'model_used': 'basic_fallback',
                'ai_model_name': 'fallback_processing',
                'processing_method': 'basic_fallback'
            }, ExecutionStrategy.BASIC_FALLBACK.value
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0
            }, "basic_fallback_error"
    
    async def _run_ai_inference(self, ai_model, current_data: torch.Tensor, 
                               clothing_tensor: torch.Tensor, **kwargs) -> Optional[torch.Tensor]:
        """ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
        try:
            # AI ëª¨ë¸ë³„ ì¶”ë¡  ì‹¤í–‰
            if hasattr(ai_model, 'process'):
                return await ai_model.process(current_data, clothing_tensor, **kwargs)
            elif hasattr(ai_model, '__call__'):
                return await ai_model(current_data, clothing_tensor, **kwargs)
            elif hasattr(ai_model, 'forward'):
                return ai_model.forward(current_data, clothing_tensor)
            else:
                self.logger.warning("âš ï¸ AI ëª¨ë¸ ì¶”ë¡  ë©”ì„œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    async def _execute_step_logic(self, step, step_name: str, current_data: torch.Tensor,
                                 clothing_tensor: torch.Tensor, **kwargs) -> Dict[str, Any]:
        """Stepë³„ ê¸°ë³¸ ì²˜ë¦¬ ë¡œì§"""
        try:
            if step_name == 'human_parsing':
                return await step.process(current_data)
            elif step_name == 'pose_estimation':
                return await step.process(current_data)
            elif step_name == 'cloth_segmentation':
                return await step.process(clothing_tensor, clothing_type=kwargs.get('clothing_type', 'shirt'))
            elif step_name == 'geometric_matching':
                return await step.process(
                    person_parsing={'result': current_data},
                    pose_keypoints=self._generate_dummy_pose_keypoints(),
                    clothing_segmentation={'mask': clothing_tensor},
                    clothing_type=kwargs.get('clothing_type', 'shirt')
                )
            elif step_name == 'cloth_warping':
                return await step.process(
                    current_data, clothing_tensor, 
                    kwargs.get('body_measurements', {}), 
                    kwargs.get('fabric_type', 'cotton')
                )
            elif step_name == 'virtual_fitting':
                return await step.process(current_data, clothing_tensor, kwargs.get('style_preferences', {}))
            elif step_name == 'post_processing':
                return await step.process(current_data)
            elif step_name == 'quality_assessment':
                return await step.process(current_data, clothing_tensor)
            else:
                return await step.process(current_data)
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ê¸°ë³¸ ë¡œì§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {'result': current_data, 'confidence': 0.5, 'quality_score': 0.5}
    
    def _generate_dummy_pose_keypoints(self) -> List[List[float]]:
        """ë”ë¯¸ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ìƒì„±"""
        return [[256 + np.random.uniform(-50, 50), 256 + np.random.uniform(-100, 100), 0.8] for _ in range(18)]

# ==============================================
# ğŸ”¥ 8. ë©”ì¸ PipelineManager í´ë˜ìŠ¤
# ==============================================

class PipelineManager:
    """
    ğŸ”¥ ì™„ì „í•œ PipelineManager - AI ëª¨ë¸ ì—°ë™ ì™„ì„± + ì„±ëŠ¥ ìµœì í™”
    
    âœ… Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ì—°ë™
    âœ… 2ë‹¨ê³„ í´ë°± ì „ëµ (í†µí•© AI â†’ ModelLoader â†’ ê¸°ë³¸)
    âœ… M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš© ê·¹ëŒ€í™”
    âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ìµœì í™”
    âœ… ModelLoader Dict ë¬¸ì œ ì™„ì „ í•´ê²°
    âœ… conda í™˜ê²½ ìµœì í™”
    """
    
    def __init__(
        self,
        config_path: Optional[str] = None,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], PipelineConfig]] = None,
        **kwargs
    ):
        """íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        
        # 1. ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._auto_detect_device(device)
        
        # 2. ì„¤ì • ì´ˆê¸°í™”
        if isinstance(config, PipelineConfig):
            self.config = config
        else:
            config_dict = self._load_config(config_path) if config_path else {}
            if config:
                config_dict.update(config if isinstance(config, dict) else {})
            config_dict.update(kwargs)
            
            # M3 Max ìë™ ê°ì§€ ë° ìµœì í™”
            if self._detect_m3_max():
                config_dict.update({
                    'is_m3_max': True,
                    'memory_gb': 128.0,
                    'device_type': 'apple_silicon',
                    'performance_mode': 'maximum'
                })
            
            self.config = PipelineConfig(device=self.device, **config_dict)
        
        # 3. ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        # 4. ì„±ëŠ¥ ìµœì í™” ì ìš©
        self.performance_optimizer = PerformanceOptimizer(self.config)
        self.performance_optimizer.optimize_system()
        
        # 5. ê´€ë¦¬ìë“¤ ì´ˆê¸°í™”
        self.model_manager = ModelLoaderManager(self.config, self.device, self.logger)
        self.unified_manager = UnifiedSystemManager(self.config, self.device, self.logger)
        self.execution_manager = OptimizedExecutionManager(self.config, self.logger)
        
        # 6. AI ì—°ë™ ê´€ë¦¬ì
        self.ai_connector = None  # ì´ˆê¸°í™” í›„ ìƒì„±
        
        # 7. íŒŒì´í”„ë¼ì¸ ìƒíƒœ
        self.is_initialized = False
        self.current_status = ProcessingStatus.IDLE
        self.step_order = [
            'human_parsing', 'pose_estimation', 'cloth_segmentation',
            'geometric_matching', 'cloth_warping', 'virtual_fitting',
            'post_processing', 'quality_assessment'
        ]
        self.steps = {}
        
        # 8. ì„±ëŠ¥ ë° í†µê³„
        self.performance_metrics = {
            'total_sessions': 0,
            'successful_sessions': 0,
            'ai_model_usage': {},
            'average_processing_time': 0.0,
            'average_quality_score': 0.0
        }
        
        # 9. ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.data_converter = self._create_data_converter()
        self.memory_manager = self._create_memory_manager()
        
        # 10. ìŠ¤ë ˆë“œ í’€
        self.thread_pool = ThreadPoolExecutor(max_workers=self.config.thread_pool_size)
        
        self.logger.info(f"ğŸ”¥ ì™„ì „í•œ PipelineManager ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {self.device}")
        self.logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬: {self.config.memory_gb}GB")
        self.logger.info(f"ğŸš€ M3 Max: {'âœ…' if self.config.is_m3_max else 'âŒ'}")
        self.logger.info(f"ğŸ§  AI ëª¨ë¸: {'âœ…' if self.config.ai_model_enabled else 'âŒ'}")
    
    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device and preferred_device != "auto":
            return preferred_device
        
        try:
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                    capture_output=True, text=True, timeout=5
                )
                chip_info = result.stdout.strip()
                return 'M3' in chip_info and 'Max' in chip_info
        except:
            pass
        return False
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if not config_path or not os.path.exists(config_path):
            return {}
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.warning(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _create_data_converter(self):
        """ë°ì´í„° ë³€í™˜ê¸° ìƒì„±"""
        class OptimizedDataConverter:
            def __init__(self, device: str):
                self.device = device
                
            def preprocess_image(self, image_input) -> torch.Tensor:
                """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
                if isinstance(image_input, str):
                    image = Image.open(image_input).convert('RGB')
                elif isinstance(image_input, Image.Image):
                    image = image_input.convert('RGB')
                elif isinstance(image_input, np.ndarray):
                    image = Image.fromarray(image_input).convert('RGB')
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_input)}")
                
                # ìµœì í™”ëœ ë¦¬ì‚¬ì´ì¦ˆ
                if image.size != (512, 512):
                    image = image.resize((512, 512), Image.Resampling.LANCZOS)
                
                # í…ì„œ ë³€í™˜
                img_array = np.array(image)
                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0
                img_tensor = img_tensor.unsqueeze(0).to(self.device)
                
                return img_tensor
            
            def tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
                """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
                if tensor.dim() == 4:
                    tensor = tensor.squeeze(0)
                if tensor.shape[0] == 3:
                    tensor = tensor.permute(1, 2, 0)
                
                tensor = torch.clamp(tensor, 0, 1)
                tensor = tensor.cpu()
                array = (tensor.numpy() * 255).astype(np.uint8)
                
                return Image.fromarray(array)
        
        return OptimizedDataConverter(self.device)
    
    def _create_memory_manager(self):
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±"""
        class OptimizedMemoryManager:
            def __init__(self, device: str):
                self.device = device
                
            def cleanup_memory(self):
                """ë©”ëª¨ë¦¬ ì •ë¦¬"""
                gc.collect()
                
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                elif self.device == "mps" and torch.backends.mps.is_available():
                    try:
                        torch.mps.empty_cache()
                        torch.mps.synchronize()
                    except:
                        pass
        
        return OptimizedMemoryManager(self.device)
    
    async def initialize(self) -> bool:
        """íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” - AI ëª¨ë¸ ì—°ë™ ì™„ì„±"""
        try:
            self.logger.info("ğŸš€ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹œì‘...")
            self.current_status = ProcessingStatus.INITIALIZING
            start_time = time.time()
            
            # 1. ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
            
            # 2. ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìµœìš°ì„ )
            model_success = await self.model_manager.initialize()
            if model_success:
                self.logger.info("âœ… ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # 3. í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            unified_success = await self.unified_manager.initialize()
            if unified_success:
                self.logger.info("âœ… í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # 4. AI ì—°ë™ ê´€ë¦¬ì ìƒì„±
            self.ai_connector = StepAIConnector(self.model_manager, self.unified_manager)
            
            # 5. Step í´ë˜ìŠ¤ë“¤ ì´ˆê¸°í™” + AI ì—°ë™
            success_count = await self._initialize_steps_with_ai()
            
            # 6. ì´ˆê¸°í™” ê²€ì¦
            success_rate = success_count / len(self.step_order)
            if success_rate < 0.5:
                self.logger.warning(f"ì´ˆê¸°í™” ì„±ê³µë¥  ë‚®ìŒ: {success_rate:.1%}")
            
            initialization_time = time.time() - start_time
            self.is_initialized = success_count > 0
            self.current_status = ProcessingStatus.IDLE if self.is_initialized else ProcessingStatus.FAILED
            
            if self.is_initialized:
                self.logger.info(f"ğŸ‰ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ ({initialization_time:.2f}ì´ˆ)")
                self.logger.info(f"ğŸ“Š Step ì´ˆê¸°í™”: {success_count}/{len(self.step_order)} ({success_rate:.1%})")
                self.logger.info(f"ğŸ§  ModelLoader: {'âœ…' if model_success else 'âŒ'}")
                self.logger.info(f"ğŸ”— í†µí•© ì‹œìŠ¤í…œ: {'âœ…' if unified_success else 'âŒ'}")
            else:
                self.logger.error("âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            return self.is_initialized
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            self.current_status = ProcessingStatus.FAILED
            return False
    
    async def _initialize_steps_with_ai(self) -> int:
        """Step í´ë˜ìŠ¤ë“¤ ì´ˆê¸°í™” + AI ëª¨ë¸ ì™„ì „ ì—°ë™"""
        try:
            if not STEP_CLASSES_AVAILABLE:
                self.logger.error("âŒ Step í´ë˜ìŠ¤ë“¤ ì‚¬ìš© ë¶ˆê°€")
                return 0
            
            step_classes = {
                'human_parsing': HumanParsingStep,
                'pose_estimation': PoseEstimationStep,
                'cloth_segmentation': ClothSegmentationStep,
                'geometric_matching': GeometricMatchingStep,
                'cloth_warping': ClothWarpingStep,
                'virtual_fitting': VirtualFittingStep,
                'post_processing': PostProcessingStep,
                'quality_assessment': QualityAssessmentStep
            }
            
            # ê¸°ë³¸ ì„¤ì •
            base_config = {
                'device': self.device,
                'device_type': self.config.device_type,
                'memory_gb': self.config.memory_gb,
                'is_m3_max': self.config.is_m3_max,
                'optimization_enabled': True,
                'quality_level': self.config.quality_level.value
            }
            
            success_count = 0
            
            # Stepë³„ ë³‘ë ¬ ì´ˆê¸°í™” (ì„±ëŠ¥ ìµœì í™”)
            tasks = []
            for step_name in self.step_order:
                if step_name in step_classes:
                    task = self._initialize_single_step(step_name, step_classes[step_name], base_config)
                    tasks.append(task)
            
            # ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                step_name = self.step_order[i] if i < len(self.step_order) else f"step_{i}"
                
                if isinstance(result, Exception):
                    self.logger.error(f"âŒ {step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {result}")
                elif result:
                    success_count += 1
                    self.logger.info(f"âœ… {step_name} ì´ˆê¸°í™” + AI ì—°ë™ ì™„ë£Œ")
                else:
                    self.logger.warning(f"âš ï¸ {step_name} ì´ˆê¸°í™” ë¶€ë¶„ ì‹¤íŒ¨")
            
            return success_count
            
        except Exception as e:
            self.logger.error(f"âŒ Step ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return 0
    
    async def _initialize_single_step(self, step_name: str, step_class, base_config: Dict[str, Any]) -> bool:
        """ë‹¨ì¼ Step ì´ˆê¸°í™” + AI ì—°ë™"""
        try:
            # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            step_config = {**base_config, **self._get_step_config(step_name)}
            step_instance = self._create_step_instance_safely(step_class, step_name, step_config)
            
            if not step_instance:
                return False
            
            # ğŸ”¥ AI ì—°ë™ ì„¤ì • (í•µì‹¬)
            if self.ai_connector:
                await self.ai_connector.setup_step_ai_connection(step_instance, step_name)
            
            # Step ì´ˆê¸°í™”
            if hasattr(step_instance, 'initialize'):
                await step_instance.initialize()
            
            self.steps[step_name] = step_instance
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ë‹¨ì¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _create_step_instance_safely(self, step_class, step_name: str, step_config: Dict[str, Any]):
        """Step ì¸ìŠ¤í„´ìŠ¤ ì•ˆì „ ìƒì„±"""
        try:
            return step_class(**step_config)
        except TypeError as e:
            if "unexpected keyword argument" in str(e):
                try:
                    safe_config = {
                        'device': step_config.get('device', 'cpu'),
                        'config': step_config.get('config', {})
                    }
                    return step_class(**safe_config)
                except Exception:
                    try:
                        return step_class(device=step_config.get('device', 'cpu'))
                    except Exception:
                        return None
            else:
                raise
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _get_step_config(self, step_name: str) -> Dict[str, Any]:
        """Stepë³„ ìµœì í™”ëœ ì„¤ì •"""
        configs = {
            'human_parsing': {
                'model_name': 'graphonomy',
                'num_classes': 20,
                'input_size': (512, 512),
                'enable_ai_model': True
            },
            'pose_estimation': {
                'model_type': 'mediapipe',
                'input_size': (368, 368),
                'confidence_threshold': 0.5,
                'enable_ai_model': True
            },
            'cloth_segmentation': {
                'model_name': 'u2net',
                'background_threshold': 0.5,
                'post_process': True,
                'enable_ai_model': True
            },
            'geometric_matching': {
                'tps_points': 25,
                'matching_threshold': 0.8,
                'method': 'ai_enhanced'
            },
            'cloth_warping': {
                'warping_method': 'ai_physics',
                'physics_simulation': True,
                'enable_ai_model': True
            },
            'virtual_fitting': {
                'model_name': 'ootdiffusion',
                'blending_method': 'ai_poisson',
                'seamless_cloning': True,
                'enable_ai_model': True
            },
            'post_processing': {
                'model_name': 'esrgan',
                'enable_super_resolution': True,
                'enhance_faces': True,
                'enable_ai_model': True
            },
            'quality_assessment': {
                'model_name': 'clipiqa',
                'enable_detailed_analysis': True,
                'perceptual_metrics': True,
                'enable_ai_model': True
            }
        }
        
        return configs.get(step_name, {})
    
    # ==============================================
    # ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ - AI ëª¨ë¸ ì™„ì „ ì—°ë™
    # ==============================================
    
    async def process_complete_virtual_fitting(
        self,
        person_image: Union[str, Image.Image, np.ndarray],
        clothing_image: Union[str, Image.Image, np.ndarray],
        body_measurements: Optional[Dict[str, float]] = None,
        clothing_type: str = "shirt",
        fabric_type: str = "cotton",
        style_preferences: Optional[Dict[str, Any]] = None,
        quality_target: float = 0.8,
        progress_callback: Optional[Callable] = None,
        save_intermediate: bool = False,
        session_id: Optional[str] = None
    ) -> ProcessingResult:
        """
        ğŸ”¥ ì™„ì „í•œ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ - AI ëª¨ë¸ ì™„ì „ ì—°ë™
        
        âœ… Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš©
        âœ… 2ë‹¨ê³„ í´ë°± ì „ëµ
        âœ… M3 Max ì„±ëŠ¥ ìµœì í™”
        âœ… ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        """
        if not self.is_initialized:
            raise RuntimeError("íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        if session_id is None:
            session_id = f"ai_vf_{int(time.time())}_{np.random.randint(1000, 9999)}"
        
        start_time = time.time()
        self.current_status = ProcessingStatus.PROCESSING
        
        try:
            self.logger.info(f"ğŸ¯ AI ì™„ì „ ì—°ë™ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì‹œì‘ - ì„¸ì…˜: {session_id}")
            self.logger.info(f"âš™ï¸ ì„¤ì •: {clothing_type} ({fabric_type}), ëª©í‘œ í’ˆì§ˆ: {quality_target}")
            self.logger.info(f"ğŸ§  AI ëª¨ë¸: {'âœ…' if self.config.ai_model_enabled else 'âŒ'}")
            self.logger.info(f"ğŸš€ M3 Max: {'âœ…' if self.config.is_m3_max else 'âŒ'}")
            
            # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ìµœì í™”)
            person_tensor = self.data_converter.preprocess_image(person_image)
            clothing_tensor = self.data_converter.preprocess_image(clothing_image)
            
            if progress_callback:
                await progress_callback("ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ", 5)
            
            # 2. ë©”ëª¨ë¦¬ ìµœì í™”
            self.memory_manager.cleanup_memory()
            
            # ğŸ”¥ 3. 8ë‹¨ê³„ ìˆœì°¨ ì²˜ë¦¬ - AI ëª¨ë¸ ì™„ì „ í™œìš©
            step_results = {}
            execution_strategies = {}
            ai_models_used = {}
            current_data = person_tensor
            
            for i, step_name in enumerate(self.step_order):
                if step_name not in self.steps:
                    self.logger.warning(f"âš ï¸ {step_name} ë‹¨ê³„ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆë›°ê¸°...")
                    continue
                
                step_start = time.time()
                step = self.steps[step_name]
                
                self.logger.info(f"ğŸ“‹ {i+1}/{len(self.step_order)} ë‹¨ê³„: {step_name} AI ì²˜ë¦¬ ì¤‘...")
                
                try:
                    # ğŸ”¥ ìµœì í™”ëœ 2ë‹¨ê³„ í´ë°± ì‹¤í–‰
                    step_result, execution_strategy = await self.execution_manager.execute_step_optimized(
                        step, step_name, current_data, clothing_tensor,
                        body_measurements=body_measurements,
                        clothing_type=clothing_type,
                        fabric_type=fabric_type,
                        style_preferences=style_preferences,
                        quality_target=quality_target
                    )
                    
                    step_time = time.time() - step_start
                    step_results[step_name] = step_result
                    execution_strategies[step_name] = execution_strategy
                    
                    # AI ëª¨ë¸ ì‚¬ìš© ì¶”ì 
                    ai_model_name = step_result.get('ai_model_name', 'unknown')
                    ai_models_used[step_name] = ai_model_name
                    
                    # ê²°ê³¼ ì—…ë°ì´íŠ¸
                    if step_result.get('success', True):
                        result_data = step_result.get('result')
                        if result_data is not None:
                            current_data = result_data
                    
                    # ë¡œê¹…
                    confidence = step_result.get('confidence', 0.8)
                    quality_score = step_result.get('quality_score', confidence)
                    model_used = step_result.get('model_used', 'unknown')
                    
                    # ì „ëµë³„ ì•„ì´ì½˜
                    if execution_strategy == ExecutionStrategy.UNIFIED_AI.value:
                        strategy_icon = "ğŸ”—ğŸ§ "
                    elif execution_strategy == ExecutionStrategy.MODEL_LOADER.value:
                        strategy_icon = "ğŸ§ ğŸ“¦"
                    else:
                        strategy_icon = "ğŸ”„"
                    
                    self.logger.info(f"âœ… {i+1}ë‹¨ê³„ ì™„ë£Œ - ì‹œê°„: {step_time:.2f}ì´ˆ, ì‹ ë¢°ë„: {confidence:.3f}, í’ˆì§ˆ: {quality_score:.3f}")
                    self.logger.info(f"   {strategy_icon} ì „ëµ: {execution_strategy}, AIëª¨ë¸: {ai_model_name}, ì²˜ë¦¬: {model_used}")
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if progress_callback:
                        progress = 5 + (i + 1) * 85 // len(self.step_order)
                        await progress_callback(f"{step_name} AI ì²˜ë¦¬ ì™„ë£Œ", progress)
                    
                    # ğŸ”¥ M3 Max ë©”ëª¨ë¦¬ ìµœì í™” (ì¤‘ê°„ ë‹¨ê³„ë§ˆë‹¤)
                    if self.config.is_m3_max and i % 2 == 0:
                        self.memory_manager.cleanup_memory()
                    
                except Exception as e:
                    self.logger.error(f"âŒ {i+1}ë‹¨ê³„ ({step_name}) ì‹¤íŒ¨: {e}")
                    step_time = time.time() - step_start
                    step_results[step_name] = {
                        'success': False,
                        'error': str(e),
                        'processing_time': step_time,
                        'confidence': 0.0,
                        'quality_score': 0.0,
                        'model_used': 'error',
                        'ai_model_name': 'error'
                    }
                    execution_strategies[step_name] = "error"
                    ai_models_used[step_name] = "error"
                    
                    # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                    continue
            
            # 4. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            total_time = time.time() - start_time
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
            if isinstance(current_data, torch.Tensor):
                result_image = self.data_converter.tensor_to_pil(current_data)
            else:
                result_image = Image.new('RGB', (512, 512), color='gray')
            
            # ğŸ”¥ ê°•í™”ëœ í’ˆì§ˆ í‰ê°€ (AI ëª¨ë¸ ì‚¬ìš© ê³ ë ¤)
            quality_score = self._assess_ai_enhanced_quality(step_results, execution_strategies, ai_models_used)
            quality_grade = self._get_quality_grade(quality_score)
            
            # ì„±ê³µ ì—¬ë¶€ ê²°ì •
            success = quality_score >= (quality_target * 0.8)
            
            # ğŸ”¥ AI ëª¨ë¸ ì‚¬ìš© í†µê³„
            ai_stats = self._calculate_ai_usage_statistics(ai_models_used, execution_strategies)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_performance_metrics(total_time, quality_score, success, ai_stats)
            
            if progress_callback:
                await progress_callback("AI ì²˜ë¦¬ ì™„ë£Œ", 100)
            
            self.current_status = ProcessingStatus.IDLE
            
            # ğŸ”¥ AI ëª¨ë¸ ì™„ì „ ì—°ë™ ê²°ê³¼ ë¡œê¹…
            self.logger.info(f"ğŸ‰ AI ì™„ì „ ì—°ë™ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì™„ë£Œ!")
            self.logger.info(f"â±ï¸ ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
            self.logger.info(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f} ({quality_grade})")
            self.logger.info(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {'âœ…' if quality_score >= quality_target else 'âŒ'}")
            self.logger.info(f"ğŸ“‹ ì™„ë£Œëœ ë‹¨ê³„: {len(step_results)}/{len(self.step_order)}")
            self.logger.info(f"ğŸ§  AI ëª¨ë¸ ì‚¬ìš©ë¥ : {ai_stats['ai_usage_rate']:.1f}%")
            self.logger.info(f"ğŸ”— í†µí•© AI ì‚¬ìš©: {ai_stats['unified_ai_count']}íšŒ")
            self.logger.info(f"ğŸ“¦ ModelLoader ì‚¬ìš©: {ai_stats['model_loader_count']}íšŒ")
            
            return ProcessingResult(
                success=success,
                session_id=session_id,
                result_image=result_image,
                result_tensor=current_data if isinstance(current_data, torch.Tensor) else None,
                quality_score=quality_score,
                quality_grade=quality_grade,
                processing_time=total_time,
                step_results=step_results,
                step_timings={step: result.get('execution_time', 0.0) for step, result in step_results.items()},
                ai_models_used=ai_models_used,
                execution_strategies=execution_strategies,
                performance_metrics={
                    'ai_usage_statistics': ai_stats,
                    'memory_peak_usage': self._get_memory_peak_usage(),
                    'step_performance': self._get_step_performance_metrics(step_results)
                },
                metadata={
                    'device': self.device,
                    'device_type': self.config.device_type,
                    'is_m3_max': self.config.is_m3_max,
                    'memory_gb': self.config.memory_gb,
                    'ai_model_enabled': self.config.ai_model_enabled,
                    'quality_target': quality_target,
                    'quality_target_achieved': quality_score >= quality_target,
                    'total_steps': len(self.step_order),
                    'completed_steps': len(step_results),
                    'success_rate': len([r for r in step_results.values() if r.get('success', True)]) / len(step_results) if step_results else 0,
                    'ai_models_summary': {
                        'unique_models_used': len(set(ai_models_used.values()) - {'error', 'unknown'}),
                        'real_ai_inference_count': sum(1 for model in ai_models_used.values() if model not in ['error', 'unknown', 'fallback_processing', 'step_processing']),
                        'fallback_count': sum(1 for strategy in execution_strategies.values() if 'fallback' in strategy)
                    }
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ AI ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            
            # ì—ëŸ¬ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_performance_metrics(time.time() - start_time, 0.0, False, {})
            
            self.current_status = ProcessingStatus.FAILED
            
            return ProcessingResult(
                success=False,
                session_id=session_id,
                processing_time=time.time() - start_time,
                error_message=str(e),
                metadata={
                    'device': self.device,
                    'error_type': type(e).__name__,
                    'error_location': traceback.format_exc(),
                    'ai_model_enabled': self.config.ai_model_enabled,
                    'is_m3_max': self.config.is_m3_max
                }
            )
    
    def _assess_ai_enhanced_quality(self, step_results: Dict[str, Any], execution_strategies: Dict[str, str], 
                                   ai_models_used: Dict[str, str]) -> float:
        """AI ëª¨ë¸ ì‚¬ìš©ì„ ê³ ë ¤í•œ ê°•í™”ëœ í’ˆì§ˆ í‰ê°€"""
        if not step_results:
            return 0.5
        
        quality_scores = []
        confidence_scores = []
        ai_bonus = 0.0
        
        for step_name, step_result in step_results.items():
            if isinstance(step_result, dict):
                confidence = step_result.get('confidence', 0.8)
                quality = step_result.get('quality_score', confidence)
                strategy = execution_strategies.get(step_name, 'unknown')
                ai_model = ai_models_used.get(step_name, 'unknown')
                
                quality_scores.append(quality)
                confidence_scores.append(confidence)
                
                # ğŸ”¥ AI ëª¨ë¸ ì‚¬ìš©ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
                if ai_model not in ['error', 'unknown', 'fallback_processing', 'step_processing']:
                    if strategy == ExecutionStrategy.UNIFIED_AI.value:
                        ai_bonus += 0.08  # í†µí•© AI: 8% ë³´ë„ˆìŠ¤
                    elif strategy == ExecutionStrategy.MODEL_LOADER.value:
                        ai_bonus += 0.05  # ModelLoader: 5% ë³´ë„ˆìŠ¤
                    else:
                        ai_bonus += 0.02  # ê¸°íƒ€: 2% ë³´ë„ˆìŠ¤
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        if quality_scores:
            avg_quality = sum(quality_scores) / len(quality_scores)
            avg_confidence = sum(confidence_scores) / len(confidence_scores)
            
            # ê°€ì¤‘ í‰ê·  + AI ë³´ë„ˆìŠ¤
            overall_score = avg_quality * 0.7 + avg_confidence * 0.3 + ai_bonus
            return min(max(overall_score, 0.0), 1.0)
        
        return 0.5
    
    def _calculate_ai_usage_statistics(self, ai_models_used: Dict[str, str], 
                                     execution_strategies: Dict[str, str]) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì‚¬ìš© í†µê³„ ê³„ì‚°"""
        total_steps = len(ai_models_used)
        
        # ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš© íšŸìˆ˜
        real_ai_count = sum(1 for model in ai_models_used.values() 
                           if model not in ['error', 'unknown', 'fallback_processing', 'step_processing'])
        
        # ì „ëµë³„ í†µê³„
        unified_ai_count = sum(1 for strategy in execution_strategies.values() 
                              if strategy == ExecutionStrategy.UNIFIED_AI.value)
        model_loader_count = sum(1 for strategy in execution_strategies.values() 
                               if strategy == ExecutionStrategy.MODEL_LOADER.value)
        fallback_count = sum(1 for strategy in execution_strategies.values() 
                           if strategy == ExecutionStrategy.BASIC_FALLBACK.value)
        
        return {
            'total_steps': total_steps,
            'real_ai_count': real_ai_count,
            'ai_usage_rate': (real_ai_count / total_steps * 100) if total_steps > 0 else 0,
            'unified_ai_count': unified_ai_count,
            'model_loader_count': model_loader_count,
            'fallback_count': fallback_count,
            'unified_ai_rate': (unified_ai_count / total_steps * 100) if total_steps > 0 else 0,
            'model_loader_rate': (model_loader_count / total_steps * 100) if total_steps > 0 else 0,
            'fallback_rate': (fallback_count / total_steps * 100) if total_steps > 0 else 0,
            'unique_ai_models': list(set(ai_models_used.values()) - {'error', 'unknown', 'fallback_processing', 'step_processing'})
        }
    
    def _update_performance_metrics(self, processing_time: float, quality_score: float, 
                                   success: bool, ai_stats: Dict[str, Any]):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.performance_metrics['total_sessions'] += 1
        
        if success:
            self.performance_metrics['successful_sessions'] += 1
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        total_sessions = self.performance_metrics['total_sessions']
        prev_avg_time = self.performance_metrics['average_processing_time']
        self.performance_metrics['average_processing_time'] = (
            (prev_avg_time * (total_sessions - 1) + processing_time) / total_sessions
        )
        
        # í‰ê·  í’ˆì§ˆ ì ìˆ˜ ì—…ë°ì´íŠ¸
        if success:
            successful_sessions = self.performance_metrics['successful_sessions']
            prev_avg_quality = self.performance_metrics['average_quality_score']
            self.performance_metrics['average_quality_score'] = (
                (prev_avg_quality * (successful_sessions - 1) + quality_score) / successful_sessions
            )
        
        # AI ëª¨ë¸ ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸
        if ai_stats:
            for model in ai_stats.get('unique_ai_models', []):
                self.performance_metrics['ai_model_usage'][model] = (
                    self.performance_metrics['ai_model_usage'].get(model, 0) + 1
                )
    
    def _get_memory_peak_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ í”¼í¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            memory_info = {}
            
            # CPU ë©”ëª¨ë¦¬
            if psutil:
                process = psutil.Process()
                memory_info['cpu_memory_gb'] = process.memory_info().rss / (1024**3)
                
                system_memory = psutil.virtual_memory()
                memory_info['system_memory_percent'] = system_memory.percent
                memory_info['system_memory_available_gb'] = system_memory.available / (1024**3)
            
            # GPU ë©”ëª¨ë¦¬
            if self.device == 'cuda' and torch.cuda.is_available():
                memory_info['gpu_allocated_gb'] = torch.cuda.memory_allocated() / (1024**3)
                memory_info['gpu_reserved_gb'] = torch.cuda.memory_reserved() / (1024**3)
            elif self.device == 'mps':
                # MPS ë©”ëª¨ë¦¬ëŠ” ì§ì ‘ ì¡°íšŒ ì–´ë ¤ì›€
                memory_info['gpu_type'] = 'mps'
            
            return memory_info
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _get_step_performance_metrics(self, step_results: Dict[str, Any]) -> Dict[str, Any]:
        """Stepë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
        metrics = {}
        
        for step_name, result in step_results.items():
            if isinstance(result, dict):
                metrics[step_name] = {
                    'success': result.get('success', False),
                    'execution_time': result.get('execution_time', 0.0),
                    'confidence': result.get('confidence', 0.0),
                    'quality_score': result.get('quality_score', 0.0),
                    'ai_model_used': result.get('ai_model_name', 'unknown')
                }
        
        return metrics
    
    def _get_quality_grade(self, quality_score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if quality_score >= 0.95:
            return "Excellent+"
        elif quality_score >= 0.9:
            return "Excellent"
        elif quality_score >= 0.8:
            return "Good"
        elif quality_score >= 0.7:
            return "Fair"
        elif quality_score >= 0.6:
            return "Poor"
        else:
            return "Very Poor"
    
    # ==============================================
    # ğŸ”¥ ìƒíƒœ ì¡°íšŒ ë° ê´€ë¦¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ - AI ëª¨ë¸ ì •ë³´ í¬í•¨"""
        return {
            'initialized': self.is_initialized,
            'current_status': self.current_status.value,
            'device': self.device,
            'device_type': self.config.device_type,
            'memory_gb': self.config.memory_gb,
            'is_m3_max': self.config.is_m3_max,
            'ai_model_enabled': self.config.ai_model_enabled,
            'model_loader_initialized': self.model_manager.is_initialized,
            'unified_system_initialized': self.unified_manager.is_initialized,
            'config': {
                'quality_level': self.config.quality_level.value,
                'processing_mode': self.config.processing_mode.value,
                'performance_mode': self.config.performance_mode,
                'ai_model_enabled': self.config.ai_model_enabled,
                'model_preload_enabled': self.config.model_preload_enabled,
                'model_cache_size': self.config.model_cache_size,
                'max_fallback_attempts': self.config.max_fallback_attempts,
                'memory_optimization': self.config.memory_optimization,
                'parallel_processing': self.config.parallel_processing,
                'batch_size': self.config.batch_size,
                'thread_pool_size': self.config.thread_pool_size
            },
            'steps_status': {
                step_name: {
                    'loaded': step_name in self.steps,
                    'type': type(self.steps[step_name]).__name__ if step_name in self.steps else None,
                    'ready': step_name in self.steps and hasattr(self.steps[step_name], 'process'),
                    'has_unified_interface': (step_name in self.steps and 
                                            hasattr(self.steps[step_name], 'unified_interface') and 
                                            getattr(self.steps[step_name], 'unified_interface', None) is not None),
                    'has_model_interface': (step_name in self.steps and 
                                          hasattr(self.steps[step_name], 'model_interface') and 
                                          getattr(self.steps[step_name], 'model_interface', None) is not None),
                    'has_ai_model': (step_name in self.steps and 
                                   hasattr(self.steps[step_name], '_ai_model') and 
                                   getattr(self.steps[step_name], '_ai_model', None) is not None),
                    'ai_model_name': getattr(self.steps.get(step_name), '_ai_model_name', 'unknown') if step_name in self.steps else 'unknown'
                }
                for step_name in self.step_order
            },
            'ai_model_status': {
                'loaded_models': len(self.model_manager.loaded_models) if self.model_manager else 0,
                'model_interfaces': len(self.model_manager.model_interfaces) if self.model_manager else 0,
                'ai_connector_ready': self.ai_connector is not None
            },
            'performance_metrics': self.performance_metrics,
            'memory_usage': self._get_memory_peak_usage(),
            'system_integration': {
                'unified_utils_available': UNIFIED_UTILS_AVAILABLE,
                'model_loader_available': MODEL_LOADER_AVAILABLE,
                'step_requests_available': STEP_REQUESTS_AVAILABLE,
                'auto_detector_available': AUTO_DETECTOR_AVAILABLE,
                'step_classes_available': STEP_CLASSES_AVAILABLE
            }
        }
    
    def get_ai_model_summary(self) -> Dict[str, Any]:
        """AI ëª¨ë¸ ìš”ì•½ ì •ë³´"""
        summary = {
            'total_loaded_models': 0,
            'models_by_step': {},
            'model_usage_stats': self.performance_metrics.get('ai_model_usage', {}),
            'model_performance': {}
        }
        
        if self.model_manager and self.model_manager.is_initialized:
            summary['total_loaded_models'] = len(self.model_manager.loaded_models)
            
            # Stepë³„ AI ëª¨ë¸ ì •ë³´
            for step_name in self.step_order:
                if step_name in self.steps:
                    step = self.steps[step_name]
                    summary['models_by_step'][step_name] = {
                        'has_ai_model': hasattr(step, '_ai_model') and step._ai_model is not None,
                        'ai_model_name': getattr(step, '_ai_model_name', 'unknown'),
                        'has_interface': hasattr(step, 'model_interface') and step.model_interface is not None
                    }
        
        return summary
    
    async def warmup(self):
        """íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… - AI ëª¨ë¸ í¬í•¨"""
        try:
            self.logger.info("ğŸ”¥ AI ì™„ì „ ì—°ë™ íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… ì‹œì‘...")
            
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
            dummy_person = Image.new('RGB', (512, 512), color=(100, 150, 200))
            dummy_cloth = Image.new('RGB', (512, 512), color=(200, 100, 100))
            
            # ì›Œë°ì—… ì‹¤í–‰
            result = await self.process_complete_virtual_fitting(
                person_image=dummy_person,
                clothing_image=dummy_cloth,
                clothing_type='shirt',
                fabric_type='cotton',
                quality_target=0.6,
                save_intermediate=False,
                session_id="ai_warmup_session"
            )
            
            if result.success:
                ai_stats = result.performance_metrics.get('ai_usage_statistics', {})
                self.logger.info(f"âœ… AI ì›Œë°ì—… ì™„ë£Œ - ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
                self.logger.info(f"ğŸ§  AI ëª¨ë¸ ì‚¬ìš©ë¥ : {ai_stats.get('ai_usage_rate', 0):.1f}%")
                self.logger.info(f"ğŸ”— í†µí•© AI ì‚¬ìš©: {ai_stats.get('unified_ai_count', 0)}íšŒ")
                self.logger.info(f"ğŸ“¦ ModelLoader ì‚¬ìš©: {ai_stats.get('model_loader_count', 0)}íšŒ")
                self.logger.info(f"ğŸ¤– ì‚¬ìš©ëœ AI ëª¨ë¸: {', '.join(ai_stats.get('unique_ai_models', []))}")
                return True
            else:
                self.logger.warning(f"âš ï¸ ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜: {result.error_message}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return False
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - AI ëª¨ë¸ í¬í•¨"""
        try:
            self.logger.info("ğŸ§¹ AI ì™„ì „ ì—°ë™ íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            self.current_status = ProcessingStatus.CLEANING
            
            # 1. ê° Step ì •ë¦¬ (AI ëª¨ë¸ í¬í•¨)
            for step_name, step in self.steps.items():
                try:
                    # AI ëª¨ë¸ ì •ë¦¬
                    if hasattr(step, '_ai_model'):
                        delattr(step, '_ai_model')
                    
                    # ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
                    if hasattr(step, 'unified_interface'):
                        if hasattr(step.unified_interface, 'cleanup'):
                            await step.unified_interface.cleanup()
                    
                    if hasattr(step, 'model_interface'):
                        if hasattr(step.model_interface, 'unload_models'):
                            await step.model_interface.unload_models()
                    
                    # Step ìì²´ ì •ë¦¬
                    if hasattr(step, 'cleanup'):
                        await step.cleanup()
                        
                    self.logger.info(f"âœ… {step_name} ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 2. AI ì—°ë™ ê´€ë¦¬ì ì •ë¦¬
            self.ai_connector = None
            
            # 3. ModelLoader ì‹œìŠ¤í…œ ì •ë¦¬
            if self.model_manager and self.model_manager.model_loader:
                try:
                    if hasattr(self.model_manager.model_loader, 'cleanup'):
                        await self.model_manager.model_loader.cleanup()
                    self.logger.info("âœ… ModelLoader ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ModelLoader ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 4. í†µí•© ì‹œìŠ¤í…œ ì •ë¦¬
            if self.unified_manager and self.unified_manager.utils_manager:
                try:
                    self.unified_manager.utils_manager.cleanup()
                    self.logger.info("âœ… í†µí•© ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í†µí•© ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 5. ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                self.memory_manager.cleanup_memory()
                self.logger.info("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 6. ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            if hasattr(self, 'thread_pool'):
                try:
                    self.thread_pool.shutdown(wait=True)
                    self.logger.info("âœ… ìŠ¤ë ˆë“œ í’€ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œ í’€ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 7. ìƒíƒœ ì´ˆê¸°í™”
            self.is_initialized = False
            self.current_status = ProcessingStatus.IDLE
            
            self.logger.info("âœ… AI ì™„ì „ ì—°ë™ íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.current_status = ProcessingStatus.FAILED

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ (AI ëª¨ë¸ ì—°ë™ ìµœì í™”) - ëˆ„ë½ëœ create_pipeline í•¨ìˆ˜ë“¤ ì¶”ê°€
# ==============================================

def create_pipeline(
    device: str = "auto", 
    quality_level: str = "balanced", 
    mode: str = "production",
    **kwargs
) -> PipelineManager:
    """
    ğŸ”¥ ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ - ëˆ„ë½ëœ í•¨ìˆ˜ ì¶”ê°€
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ì„¤ì • ('auto', 'cpu', 'cuda', 'mps')
        quality_level: í’ˆì§ˆ ë ˆë²¨ ('fast', 'balanced', 'high', 'maximum')
        mode: ëª¨ë“œ ('development', 'production', 'testing', 'optimization')
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: ì´ˆê¸°í™”ëœ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return PipelineManager(
        device=device,
        config=PipelineConfig(
            quality_level=QualityLevel(quality_level),
            processing_mode=PipelineMode(mode),
            ai_model_enabled=True,
            **kwargs
        )
    )

def create_ai_optimized_pipeline(
    device: str = "auto",
    quality_level: str = "high",
    **kwargs
) -> PipelineManager:
    """AI ëª¨ë¸ ìµœì í™” íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    return PipelineManager(
        device=device,
        config=PipelineConfig(
            quality_level=QualityLevel(quality_level),
            processing_mode=PipelineMode.PRODUCTION,
            ai_model_enabled=True,
            model_preload_enabled=True,
            model_cache_size=20,
            performance_mode="maximum",
            memory_optimization=True,
            parallel_processing=True,
            max_fallback_attempts=2,
            **kwargs
        )
    )

def create_m3_max_pipeline(**kwargs) -> PipelineManager:
    """
    ğŸ”¥ M3 Max + AI ëª¨ë¸ ì™„ì „ ìµœì í™” íŒŒì´í”„ë¼ì¸ - ëˆ„ë½ëœ í•¨ìˆ˜ ì¶”ê°€
    
    Args:
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: M3 Max ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return PipelineManager(
        device="mps",
        config=PipelineConfig(
            quality_level=QualityLevel.MAXIMUM,
            processing_mode=PipelineMode.PRODUCTION,
            memory_gb=128.0,
            is_m3_max=True,
            device_type="apple_silicon",
            ai_model_enabled=True,
            model_preload_enabled=True,
            model_cache_size=20,
            performance_mode="maximum",
            memory_optimization=True,
            gpu_memory_fraction=0.95,
            use_fp16=True,
            enable_quantization=True,
            parallel_processing=True,
            batch_processing=True,
            async_processing=True,
            batch_size=4,
            thread_pool_size=8,
            max_fallback_attempts=2,
            enable_smart_fallback=True,
            **kwargs
        )
    )

def create_production_pipeline(**kwargs) -> PipelineManager:
    """
    ğŸ”¥ í”„ë¡œë•ì…˜ìš© AI íŒŒì´í”„ë¼ì¸ - ëˆ„ë½ëœ í•¨ìˆ˜ ì¶”ê°€
    
    Args:
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: í”„ë¡œë•ì…˜ ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return create_ai_optimized_pipeline(
        quality_level="high",
        processing_mode="production",
        ai_model_enabled=True,
        model_preload_enabled=True,
        memory_optimization=True,
        parallel_processing=True,
        **kwargs
    )

def create_development_pipeline(**kwargs) -> PipelineManager:
    """
    ğŸ”¥ ê°œë°œìš© AI íŒŒì´í”„ë¼ì¸ - ëˆ„ë½ëœ í•¨ìˆ˜ ì¶”ê°€
    
    Args:
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: ê°œë°œìš© íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return create_ai_optimized_pipeline(
        quality_level="balanced",
        processing_mode="development",
        ai_model_enabled=True,
        model_preload_enabled=False,
        memory_optimization=False,
        parallel_processing=False,
        **kwargs
    )

def create_testing_pipeline(**kwargs) -> PipelineManager:
    """
    ğŸ”¥ í…ŒìŠ¤íŒ…ìš© íŒŒì´í”„ë¼ì¸ - ëˆ„ë½ëœ í•¨ìˆ˜ ì¶”ê°€
    
    Args:
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: í…ŒìŠ¤íŒ…ìš© íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return PipelineManager(
        device="cpu",
        config=PipelineConfig(
            quality_level=QualityLevel.FAST,
            processing_mode=PipelineMode.TESTING,
            ai_model_enabled=False,
            model_preload_enabled=False,
            memory_optimization=False,
            parallel_processing=False,
            batch_size=1,
            thread_pool_size=2,
            **kwargs
        )
    )

@lru_cache(maxsize=1)
def get_global_pipeline_manager(device: str = "auto") -> PipelineManager:
    """
    ğŸ”¥ ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ - ëˆ„ë½ëœ í•¨ìˆ˜ ì¶”ê°€
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
    
    Returns:
        PipelineManager: ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
    """
    try:
        if device == "mps" and torch.backends.mps.is_available():
            return create_m3_max_pipeline()
        else:
            return create_production_pipeline(device=device)
    except Exception as e:
        logger.error(f"ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ìƒì„± ì‹¤íŒ¨: {e}")
        return create_ai_optimized_pipeline(device="cpu", quality_level="balanced")

# ==============================================
# ğŸ”¥ Export ë° ë©”ì¸ ì‹¤í–‰
# ==============================================

__all__ = [
    # ì—´ê±°í˜•
    'PipelineMode', 'QualityLevel', 'ProcessingStatus', 'ExecutionStrategy',
    
    # ë°ì´í„° í´ë˜ìŠ¤
    'PipelineConfig', 'ProcessingResult', 'AIModelInfo',
    
    # ë©”ì¸ í´ë˜ìŠ¤
    'PipelineManager',
    
    # ê´€ë¦¬ì í´ë˜ìŠ¤ë“¤
    'ModelLoaderManager', 'UnifiedSystemManager', 'StepAIConnector', 
    'OptimizedExecutionManager', 'PerformanceOptimizer',
    
    # ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ ì™„ì „ ì¶”ê°€)
    'create_pipeline',                    # âœ… ëˆ„ë½ëœ ê¸°ë³¸ í•¨ìˆ˜
    'create_ai_optimized_pipeline', 
    'create_m3_max_pipeline',            # âœ… ëˆ„ë½ëœ M3 Max í•¨ìˆ˜
    'create_production_pipeline',        # âœ… ëˆ„ë½ëœ í”„ë¡œë•ì…˜ í•¨ìˆ˜
    'create_development_pipeline',       # âœ… ëˆ„ë½ëœ ê°œë°œ í•¨ìˆ˜  
    'create_testing_pipeline',           # âœ… ëˆ„ë½ëœ í…ŒìŠ¤íŒ… í•¨ìˆ˜
    'get_global_pipeline_manager'        # âœ… ëˆ„ë½ëœ ì „ì—­ ë§¤ë‹ˆì € í•¨ìˆ˜
]

if __name__ == "__main__":
    print("ğŸ”¥ ì™„ì „í•œ PipelineManager - AI ëª¨ë¸ ì—°ë™ ì™„ì„± + ì„±ëŠ¥ ìµœì í™”")
    print("=" * 80)
    print("âœ… Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ì—°ë™")
    print("âœ… 2ë‹¨ê³„ í´ë°± ì „ëµ (í†µí•© AI â†’ ModelLoader â†’ ê¸°ë³¸)")
    print("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš© ê·¹ëŒ€í™”")
    print("âœ… ModelLoader Dict ë¬¸ì œ ì™„ì „ í•´ê²°")
    print("âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ìµœì í™”")
    print("âœ… conda í™˜ê²½ ìµœì í™”")
    print("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
    print("âœ… ëˆ„ë½ëœ create_pipeline í•¨ìˆ˜ë“¤ ì™„ì „ ì¶”ê°€")
    print("=" * 80)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ ì¶œë ¥
    print("ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ë“¤:")
    print("   - create_pipeline()")
    print("   - create_m3_max_pipeline()")
    print("   - create_production_pipeline()")
    print("   - create_development_pipeline()")
    print("   - create_testing_pipeline()")
    print("   - get_global_pipeline_manager()")
    print("=" * 80)
    
    import asyncio
    
    async def demo_complete_pipeline():
        """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ë°ëª¨"""
        
        print("ğŸ¯ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ë°ëª¨ ì‹œì‘")
        print("=" * 50)
        
        # 1. ë‹¤ì–‘í•œ íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸
        print("1ï¸âƒ£ íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ë“¤ í…ŒìŠ¤íŠ¸...")
        
        try:
            # ê¸°ë³¸ íŒŒì´í”„ë¼ì¸
            basic_pipeline = create_pipeline()
            print("âœ… create_pipeline() ì„±ê³µ")
            
            # M3 Max íŒŒì´í”„ë¼ì¸
            m3_pipeline = create_m3_max_pipeline()
            print("âœ… create_m3_max_pipeline() ì„±ê³µ")
            
            # í”„ë¡œë•ì…˜ íŒŒì´í”„ë¼ì¸
            prod_pipeline = create_production_pipeline()
            print("âœ… create_production_pipeline() ì„±ê³µ")
            
            # ê°œë°œ íŒŒì´í”„ë¼ì¸
            dev_pipeline = create_development_pipeline()
            print("âœ… create_development_pipeline() ì„±ê³µ")
            
            # í…ŒìŠ¤íŒ… íŒŒì´í”„ë¼ì¸
            test_pipeline = create_testing_pipeline()
            print("âœ… create_testing_pipeline() ì„±ê³µ")
            
            # ì „ì—­ ë§¤ë‹ˆì €
            global_manager = get_global_pipeline_manager()
            print("âœ… get_global_pipeline_manager() ì„±ê³µ")
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return
        
        # 2. M3 Max íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì‹¤ì œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        print("2ï¸âƒ£ M3 Max íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        
        try:
            # ì´ˆê¸°í™”
            success = await m3_pipeline.initialize()
            if not success:
                print("âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return
            
            print("âœ… M3 Max íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ìƒíƒœ í™•ì¸
            status = m3_pipeline.get_pipeline_status()
            print(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {status['device']}")
            print(f"ğŸ§  AI ëª¨ë¸: {'âœ…' if status['ai_model_enabled'] else 'âŒ'}")
            print(f"ğŸ”— ModelLoader: {'âœ…' if status['model_loader_initialized'] else 'âŒ'}")
            print(f"ğŸ“Š ì´ˆê¸°í™”ëœ Step: {sum(1 for s in status['steps_status'].values() if s['loaded'])}/{len(status['steps_status'])}")
            
            # ì •ë¦¬
            await m3_pipeline.cleanup()
            print("âœ… íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        print("\nğŸ‰ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ë°ëª¨ ì™„ë£Œ!")
        print("âœ… ëª¨ë“  create_pipeline í•¨ìˆ˜ë“¤ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
    
    # ì‹¤í–‰
    asyncio.run(demo_complete_pipeline())

# ==============================================
# ë¡œê¹… ë° ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

logger.info("ğŸ‰ ì™„ì „í•œ PipelineManager ë¡œë“œ ì™„ë£Œ!")
logger.info("âœ… ì£¼ìš” ì™„ì„± ê¸°ëŠ¥:")
logger.info("   - Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ì—°ë™")
logger.info("   - 2ë‹¨ê³„ í´ë°± ì „ëµ (í†µí•© AI â†’ ModelLoader â†’ ê¸°ë³¸)")
logger.info("   - M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš© ê·¹ëŒ€í™”")
logger.info("   - ModelLoader Dict ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("   - ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ìµœì í™”")
logger.info("   - AI ëª¨ë¸ ì‚¬ìš© í†µê³„ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
logger.info("   - conda í™˜ê²½ ìµœì í™”")
logger.info("âœ… ëˆ„ë½ëœ create_pipeline í•¨ìˆ˜ë“¤ ì™„ì „ ì¶”ê°€:")
logger.info("   - create_pipeline() âœ…")
logger.info("   - create_m3_max_pipeline() âœ…") 
logger.info("   - create_production_pipeline() âœ…")
logger.info("   - create_development_pipeline() âœ…")
logger.info("   - create_testing_pipeline() âœ…")
logger.info("   - get_global_pipeline_manager() âœ…")
logger.info("ğŸš€ ì´ì œ ì‹¤ì œ AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ìµœê³  í’ˆì§ˆ ê°€ìƒ í”¼íŒ…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ê°€ìš©ì„±:")
logger.info(f"   - í†µí•© ìœ í‹¸ë¦¬í‹°: {'âœ…' if UNIFIED_UTILS_AVAILABLE else 'âŒ'}")
logger.info(f"   - ModelLoader: {'âœ…' if MODEL_LOADER_AVAILABLE else 'âŒ'}")
logger.info(f"   - Step ìš”ì²­: {'âœ…' if STEP_REQUESTS_AVAILABLE else 'âŒ'}")
logger.info(f"   - ìë™ íƒì§€: {'âœ…' if AUTO_DETECTOR_AVAILABLE else 'âŒ'}")
logger.info(f"   - Step í´ë˜ìŠ¤: {'âœ…' if STEP_CLASSES_AVAILABLE else 'âŒ'}")
logger.info("ğŸ¯ ê¶Œì¥ ì‚¬ìš©ë²•:")
logger.info("   - M3 Max: create_m3_max_pipeline()")
logger.info("   - í”„ë¡œë•ì…˜: create_production_pipeline()")
logger.info("   - ê°œë°œ: create_development_pipeline()")
logger.info("   - ê¸°ë³¸: create_pipeline()")