# backend/app/ai_pipeline/pipeline_manager.py
"""
ğŸ”¥ ì™„ì „í•œ DI Container í†µí•© PipelineManager v12.0 - GitHub êµ¬ì¡° ì™„ì „ ë°˜ì˜ + DI Container ì ìš©
=================================================================================

âœ… DI Container v4.0 ì™„ì „ í†µí•© (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€)
âœ… GitHub Step íŒŒì¼ êµ¬ì¡° 100% ë°˜ì˜
âœ… RealAIStepImplementationManager v14.0 ì™„ì „ ì—°ë™
âœ… BaseStepMixin v19.3 DI Container ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì…
âœ… DetailedDataSpec ê¸°ë°˜ API â†” Step ìë™ ë³€í™˜
âœ… StepFactory v11.0 ì™„ì „ í†µí•©
âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB ì™„ì „ í™œìš©
âœ… ì™„ì „í•œ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‘ë™
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›

í•µì‹¬ ê°œì„ ì‚¬í•­:
- CircularReferenceFreeDIContainer ì™„ì „ í†µí•©
- ëª¨ë“  ì˜ì¡´ì„± ì£¼ì…ì„ DI Containerë¥¼ í†µí•´ ê´€ë¦¬
- StepFactory â†” BaseStepMixin ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
- ì‹¤ì œ GitHub Step íŒŒì¼ë“¤ê³¼ 100% í˜¸í™˜
- ëª¨ë“  ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ 100% ìœ ì§€

Author: MyCloset AI Team
Date: 2025-07-30
Version: 12.0 (Complete DI Container Integration)
"""

import os
import sys
import gc
import time
import asyncio
import logging
import threading
import traceback
import json
import uuid
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from dataclasses import dataclass, field, asdict
from enum import Enum
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache, wraps
import hashlib

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image, ImageEnhance, ImageFilter

# ğŸ”¥ DI Container ìš°ì„  ì„í¬íŠ¸ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
try:
    from app.core.di_container import (
        CircularReferenceFreeDIContainer,
        get_global_container,
        reset_global_container,
        inject_dependencies_to_step_safe,
        get_service_safe,
        register_service_safe,
        register_lazy_service,
        initialize_di_system_safe,
        ensure_global_step_compatibility,
        _add_global_step_methods
    )
    DI_CONTAINER_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("âœ… DI Container v4.0 ë¡œë”© ì™„ë£Œ")
except ImportError as e:
    DI_CONTAINER_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"âš ï¸ DI Container ë¡œë”© ì‹¤íŒ¨: {e}")

# TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
    from app.ai_pipeline.factories.step_factory import StepFactory
    from app.ai_pipeline.models.model_loader import ModelLoader
    from app.services.step_implementations import RealAIStepImplementationManager
else:
    BaseStepMixin = Any
    StepFactory = Any
    ModelLoader = Any
    RealAIStepImplementationManager = Any

# ì‹œìŠ¤í…œ ì •ë³´
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# í™˜ê²½ ê°ì§€
def detect_m3_max() -> bool:
    """M3 Max ì¹© ê°ì§€"""
    try:
        import platform
        import subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                capture_output=True, text=True, timeout=5
            )
            chip_info = result.stdout.strip()
            return 'M3' in chip_info and 'Max' in chip_info
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', 'none')
IS_CONDA = CONDA_ENV != 'none'
IS_TARGET_ENV = CONDA_ENV == 'mycloset-ai-clean'
MEMORY_GB = 128.0 if IS_M3_MAX else 16.0

# PyTorch ì„¤ì •
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        os.environ.setdefault('PYTORCH_ENABLE_MPS_FALLBACK', '1')
        os.environ.setdefault('PYTORCH_MPS_HIGH_WATERMARK_RATIO', '0.0')
    
    logger.info(f"âœ… PyTorch ë¡œë”© ì™„ë£Œ: MPS={MPS_AVAILABLE}, M3 Max={IS_M3_MAX}")
except ImportError:
    logger.warning("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch ê¶Œì¥")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)

# ==============================================
# ğŸ”¥ ì—´ê±°í˜• ë° ìƒíƒœ í´ë˜ìŠ¤ë“¤
# ==============================================

class PipelineStatus(Enum):
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ"""
    IDLE = "idle"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CLEANING = "cleaning"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    MAXIMUM = "maximum"

class ProcessingMode(Enum):
    """ì²˜ë¦¬ ëª¨ë“œ"""
    DEVELOPMENT = "development"
    PRODUCTION = "production"
    TESTING = "testing"

# ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
PipelineMode = ProcessingMode

@dataclass
class PipelineStepResult:
    """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ Step ê²°ê³¼ êµ¬ì¡° (GitHub êµ¬ì¡° ì™„ì „ ë°˜ì˜)"""
    step_id: int
    step_name: str
    success: bool
    error: Optional[str] = None
    
    # ë‹¤ìŒ Stepë“¤ë¡œ ì „ë‹¬í•  êµ¬ì²´ì  ë°ì´í„° (ì‹¤ì œ AI ê²°ê³¼)
    for_step_02: Dict[str, Any] = field(default_factory=dict)  # pose_estimation ì…ë ¥
    for_step_03: Dict[str, Any] = field(default_factory=dict)  # cloth_segmentation ì…ë ¥
    for_step_04: Dict[str, Any] = field(default_factory=dict)  # geometric_matching ì…ë ¥
    for_step_05: Dict[str, Any] = field(default_factory=dict)  # cloth_warping ì…ë ¥
    for_step_06: Dict[str, Any] = field(default_factory=dict)  # virtual_fitting ì…ë ¥
    for_step_07: Dict[str, Any] = field(default_factory=dict)  # post_processing ì…ë ¥
    for_step_08: Dict[str, Any] = field(default_factory=dict)  # quality_assessment ì…ë ¥
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ë°ì´í„° (ëˆ„ì )
    pipeline_data: Dict[str, Any] = field(default_factory=dict)
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë³´ì¡´
    previous_results: Dict[str, Any] = field(default_factory=dict)
    
    # ì›ë³¸ ì…ë ¥ ë°ì´í„°
    original_inputs: Dict[str, Any] = field(default_factory=dict)
    
    # AI ëª¨ë¸ ì²˜ë¦¬ ê²°ê³¼
    ai_results: Dict[str, Any] = field(default_factory=dict)
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)
    processing_time: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)
    
    def get_data_for_step(self, step_id: int) -> Dict[str, Any]:
        """íŠ¹ì • Stepìš© ë°ì´í„° ë°˜í™˜"""
        return getattr(self, f"for_step_{step_id:02d}", {})

@dataclass
class PipelineConfig:
    """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì„¤ì • (DI Container í†µí•©)"""
    # ê¸°ë³¸ ì„¤ì •
    device: str = "auto"
    quality_level: Union[QualityLevel, str] = QualityLevel.HIGH
    processing_mode: Union[ProcessingMode, str] = ProcessingMode.PRODUCTION
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    memory_gb: float = 128.0
    is_m3_max: bool = True
    device_type: str = "apple_silicon"
    
    # AI ëª¨ë¸ ì„¤ì •
    ai_model_enabled: bool = True
    model_preload_enabled: bool = True
    model_cache_size: int = 20
    
    # ğŸ”¥ DI Container ì„¤ì • (v4.0)
    use_dependency_injection: bool = True
    auto_inject_dependencies: bool = True
    enable_adapter_pattern: bool = True
    use_circular_reference_free_di: bool = True
    enable_lazy_dependency_resolution: bool = True
    
    # ì„±ëŠ¥ ì„¤ì •
    batch_size: int = 4
    max_retries: int = 3
    timeout_seconds: int = 300
    thread_pool_size: int = 8
    
    def __post_init__(self):
        if isinstance(self.quality_level, str):
            self.quality_level = QualityLevel(self.quality_level)
        if isinstance(self.processing_mode, str):
            self.processing_mode = ProcessingMode(self.processing_mode)
        
        # M3 Max ìë™ ìµœì í™”
        if self._detect_m3_max():
            self.is_m3_max = True
            self.memory_gb = max(self.memory_gb, 128.0)
            self.device = "mps"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        return detect_m3_max()

@dataclass
class ProcessingResult:
    """ìµœì¢… ì²˜ë¦¬ ê²°ê³¼"""
    success: bool
    session_id: str = ""
    result_image: Optional[Image.Image] = None
    result_tensor: Optional[torch.Tensor] = None
    quality_score: float = 0.0
    quality_grade: str = "Unknown"
    processing_time: float = 0.0
    
    # Stepë³„ ê²°ê³¼
    step_results: Dict[str, Any] = field(default_factory=dict)
    step_timings: Dict[str, float] = field(default_factory=dict)
    ai_models_used: Dict[str, str] = field(default_factory=dict)
    
    # íŒŒì´í”„ë¼ì¸ ì •ë³´
    pipeline_metadata: Dict[str, Any] = field(default_factory=dict)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)

# ==============================================
# ğŸ”¥ DI Container ê¸°ë°˜ Step ê´€ë¦¬ì
# ==============================================

class DIContainerStepManager:
    """DI Container ê¸°ë°˜ Step ê´€ë¦¬ì (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€)"""
    
    def __init__(self, config: PipelineConfig, device: str, logger: logging.Logger):
        self.config = config
        self.device = device
        self.logger = logger
        self.steps = {}
        
        # ğŸ”¥ DI Container í†µí•©
        if DI_CONTAINER_AVAILABLE and config.use_dependency_injection:
            self.di_container = get_global_container()
            self.use_di_container = True
            self.logger.info("âœ… DI Container ê¸°ë°˜ Step ê´€ë¦¬ì ì´ˆê¸°í™”")
        else:
            self.di_container = None
            self.use_di_container = False
            self.logger.warning("âš ï¸ DI Container ì—†ì´ Step ê´€ë¦¬ì ì´ˆê¸°í™”")
        
        # GitHub ì‹¤ì œ Step êµ¬ì¡° ë§¤í•‘
        self.step_mapping = {
            1: {
                'name': 'human_parsing',
                'class_name': 'HumanParsingStep',
                'module_path': 'app.ai_pipeline.steps.step_01_human_parsing_models.step_01_human_parsing',
                'process_method': 'process',
                'required_inputs': ['person_image'],
                'outputs': ['parsed_image', 'body_masks', 'human_regions']
            },
            2: {
                'name': 'pose_estimation',
                'class_name': 'PoseEstimationStep',
                'module_path': 'app.ai_pipeline.steps.step_02_pose_estimation_models.step_02_pose_estimation',
                'process_method': 'process',
                'required_inputs': ['image', 'parsed_image'],
                'outputs': ['keypoints_18', 'skeleton_structure', 'pose_confidence']
            },
            3: {
                'name': 'cloth_segmentation',
                'class_name': 'ClothSegmentationStep',
                'module_path': 'app.ai_pipeline.steps.step_03_cloth_segmentation_models.step_03_cloth_segmentation',
                'process_method': 'process',
                'required_inputs': ['clothing_image', 'clothing_type'],
                'outputs': ['clothing_masks', 'garment_type', 'segmentation_confidence']
            },
            4: {
                'name': 'geometric_matching',
                'class_name': 'GeometricMatchingStep',
                'module_path': 'app.ai_pipeline.steps.step_04_geometric_matching_models.step_04_geometric_matching',
                'process_method': 'process',
                'required_inputs': ['person_parsing', 'pose_keypoints', 'clothing_segmentation'],
                'outputs': ['matching_matrix', 'correspondence_points', 'geometric_confidence']
            },
            5: {
                'name': 'cloth_warping',
                'class_name': 'ClothWarpingStep',
                'module_path': 'app.ai_pipeline.steps.step_05_cloth_warping_models.step_05_cloth_warping',
                'process_method': 'process',
                'required_inputs': ['cloth_image', 'person_image', 'geometric_matching'],
                'outputs': ['warped_clothing', 'warping_field', 'warping_confidence']
            },
            6: {
                'name': 'virtual_fitting',
                'class_name': 'VirtualFittingStep',
                'module_path': 'app.ai_pipeline.steps.step_06_virtual_fitting_models.step_06_virtual_fitting',
                'process_method': 'process',
                'required_inputs': ['person_image', 'warped_clothing', 'pose_data'],
                'outputs': ['fitted_image', 'fitting_quality', 'virtual_confidence']
            },
            7: {
                'name': 'post_processing',
                'class_name': 'PostProcessingStep',
                'module_path': 'app.ai_pipeline.steps.post_processing.step_07_post_processing',
                'process_method': 'process',
                'required_inputs': ['fitted_image'],
                'outputs': ['enhanced_image', 'enhancement_quality', 'processing_details']
            },
            8: {
                'name': 'quality_assessment',
                'class_name': 'QualityAssessmentStep',
                'module_path': 'app.ai_pipeline.steps.step_08_quality_assessment_models.step_08_quality_assessment',
                'process_method': 'process',
                'required_inputs': ['final_image', 'original_images'],
                'outputs': ['quality_score', 'quality_metrics', 'assessment_details']
            }
        }
        
        # AI ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ì‹¤ì œ 229GB êµ¬ì¡°)
        self.ai_model_paths = {
            'step_01_human_parsing': {
                'graphonomy': 'ai_models/step_01_human_parsing/graphonomy.pth',
                'schp_atr': 'ai_models/step_01_human_parsing/exp-schp-201908301523-atr.pth',
                'atr_model': 'ai_models/step_01_human_parsing/atr_model.pth'
            },
            'step_02_pose_estimation': {
                'yolov8_pose': 'ai_models/step_02_pose_estimation/yolov8n-pose.pt',
                'openpose': 'ai_models/step_02_pose_estimation/body_pose_model.pth',
                'hrnet': 'ai_models/step_02_pose_estimation/hrnet_w48_coco_256x192.pth'
            },
            'step_03_cloth_segmentation': {
                'sam_vit_h': 'ai_models/step_03_cloth_segmentation/sam_vit_h_4b8939.pth',
                'u2net': 'ai_models/step_03_cloth_segmentation/u2net.pth',
                'mobile_sam': 'ai_models/step_03_cloth_segmentation/mobile_sam.pt'
            },
            'step_04_geometric_matching': {
                'gmm_final': 'ai_models/step_04_geometric_matching/gmm_final.pth',
                'tps_network': 'ai_models/step_04_geometric_matching/tps_network.pth',
                'vit_large': 'ai_models/step_04_geometric_matching/ViT-L-14.pt'
            },
            'step_05_cloth_warping': {
                'realvisx_xl': 'ai_models/step_05_cloth_warping/RealVisXL_V4.0.safetensors',
                'vgg16_warping': 'ai_models/step_05_cloth_warping/vgg16_warping_ultra.pth',
                'stable_diffusion': 'ai_models/step_05_cloth_warping/stable_diffusion_2_1.safetensors'
            },
            'step_06_virtual_fitting': {
                'ootd_unet_garm': 'ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_garm/diffusion_pytorch_model.bin',
                'ootd_unet_vton': 'ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_vton/diffusion_pytorch_model.bin',
                'text_encoder': 'ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/text_encoder/text_encoder_pytorch_model.bin',
                'vae': 'ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/vae/vae_diffusion_pytorch_model.bin'
            },
            'step_07_post_processing': {
                'real_esrgan_x4': 'ai_models/step_07_post_processing/ultra_models/RealESRGAN_x4plus.pth',
                'esrgan_x8': 'ai_models/step_07_post_processing/esrgan_x8_ultra/ESRGAN_x8.pth',
                'gfpgan': 'ai_models/checkpoints/step_07_post_processing/GFPGAN.pth'
            },
            'step_08_quality_assessment': {
                'clip_vit_large': 'ai_models/step_08_quality_assessment/ultra_models/pytorch_model.bin',
                'aesthetic_predictor': 'ai_models/step_08_quality_assessment/aesthetic_predictor.pth'
            }
        }
        
        # RealAIStepImplementationManager ì—°ë™ ì‹œë„
        self.step_implementation_manager = None
        self._load_step_implementation_manager()
    
    def _load_step_implementation_manager(self):
        """RealAIStepImplementationManager v14.0 ë™ì  ë¡œë”©"""
        try:
            # ë™ì  import
            import importlib
            impl_module = importlib.import_module('app.services.step_implementations')
            
            # ì „ì—­ í•¨ìˆ˜ ì‹œë„
            get_manager_func = getattr(impl_module, 'get_step_implementation_manager', None)
            if get_manager_func:
                self.step_implementation_manager = get_manager_func()
                if self.step_implementation_manager:
                    self.logger.info("âœ… RealAIStepImplementationManager v14.0 ì—°ë™ ì™„ë£Œ")
                    return True
            
            # í´ë˜ìŠ¤ ì§ì ‘ ìƒì„± ì‹œë„
            manager_class = getattr(impl_module, 'RealAIStepImplementationManager', None)
            if manager_class:
                self.step_implementation_manager = manager_class()
                self.logger.info("âœ… RealAIStepImplementationManager v14.0 ì§ì ‘ ìƒì„± ì™„ë£Œ")
                return True
            
        except ImportError as e:
            self.logger.debug(f"RealAIStepImplementationManager import ì‹¤íŒ¨: {e}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ RealAIStepImplementationManager ë¡œë”© ì‹¤íŒ¨: {e}")
        
        return False
    
    async def initialize(self) -> bool:
        """Step ì‹œìŠ¤í…œ ì´ˆê¸°í™” (DI Container ê¸°ë°˜)"""
        try:
            self.logger.info("ğŸ”§ DI Container ê¸°ë°˜ Step ê´€ë¦¬ì ì´ˆê¸°í™” ì‹œì‘...")
            
            # ğŸ”¥ DI Container ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            if self.use_di_container:
                success = initialize_di_system_safe()
                if success:
                    self.logger.info("âœ… DI Container ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    self.logger.warning("âš ï¸ DI Container ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # Step ìƒì„± ë°©ë²• ê²°ì •
            if self._should_use_step_factory():
                return await self._create_steps_via_step_factory()
            elif self.step_implementation_manager:
                return await self._create_steps_via_implementation_manager()
            else:
                return await self._create_steps_directly()
                
        except Exception as e:
            self.logger.error(f"âŒ DI Container ê¸°ë°˜ Step ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _should_use_step_factory(self) -> bool:
        """StepFactory ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
        try:
            import importlib
            factory_module = importlib.import_module('app.ai_pipeline.factories.step_factory')
            get_global_factory = getattr(factory_module, 'get_global_step_factory', None)
            
            if get_global_factory:
                factory = get_global_factory()
                return factory is not None
            
        except ImportError:
            pass
        except Exception as e:
            self.logger.debug(f"StepFactory í™•ì¸ ì‹¤íŒ¨: {e}")
        
        return False
    
    async def _create_steps_via_step_factory(self) -> bool:
        """StepFactoryë¥¼ í†µí•œ Step ìƒì„± (DI Container í†µí•©)"""
        try:
            import importlib
            factory_module = importlib.import_module('app.ai_pipeline.factories.step_factory')
            get_global_factory = getattr(factory_module, 'get_global_step_factory')
            step_factory = get_global_factory()
            
            if not step_factory:
                raise RuntimeError("StepFactory ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            success_count = 0
            
            for step_id, step_info in self.step_mapping.items():
                try:
                    self.logger.info(f"ğŸ”„ Step {step_id} ({step_info['name']}) StepFactoryë¡œ ìƒì„± ì¤‘...")
                    
                    # Step ì„¤ì •
                    step_config = {
                        'step_id': step_id,
                        'step_name': step_info['name'],
                        'device': self.device,
                        'is_m3_max': self.config.is_m3_max,
                        'memory_gb': self.config.memory_gb,
                        'ai_model_enabled': self.config.ai_model_enabled,
                        'use_dependency_injection': self.config.use_dependency_injection,
                        'enable_adapter_pattern': self.config.enable_adapter_pattern
                    }
                    
                    # StepFactoryë¡œ ìƒì„±
                    result = await self._create_step_with_step_factory(
                        step_factory, step_id, step_config
                    )
                    
                    if result and result.get('success', False):
                        step_instance = result.get('step_instance')
                        if step_instance:
                            # ğŸ”¥ DI Container ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì…
                            await self._inject_dependencies_via_di_container(step_instance)
                            
                            self.steps[step_info['name']] = step_instance
                            success_count += 1
                            self.logger.info(f"âœ… Step {step_id} ({step_info['name']}) StepFactory ìƒì„± ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.error(f"âŒ Step {step_id} StepFactory ìƒì„± ì˜¤ë¥˜: {e}")
                    # ì§ì ‘ ìƒì„± ì‹œë„
                    step_instance = await self._create_step_directly(step_id, step_info)
                    if step_instance:
                        self.steps[step_info['name']] = step_instance
                        success_count += 1
            
            self.logger.info(f"ğŸ“‹ StepFactory ê¸°ë°˜ Step ìƒì„± ì™„ë£Œ: {success_count}/{len(self.step_mapping)}")
            return success_count > 0
            
        except Exception as e:
            self.logger.error(f"âŒ StepFactory ê¸°ë°˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    async def _create_step_with_step_factory(self, step_factory, step_id: int, step_config: Dict[str, Any]):
        """StepFactoryë¡œ Step ìƒì„±"""
        try:
            if hasattr(step_factory, 'create_step'):
                result = step_factory.create_step(step_id, **step_config)
                
                # ë¹„ë™ê¸° ê²°ê³¼ ì²˜ë¦¬
                if hasattr(result, '__await__'):
                    result = await result
                
                # ê²°ê³¼ í˜•ì‹ í™•ì¸
                if hasattr(result, 'success'):
                    return {
                        'success': result.success,
                        'step_instance': getattr(result, 'step_instance', None),
                        'error': getattr(result, 'error_message', None)
                    }
                elif isinstance(result, dict):
                    return result
                else:
                    return {
                        'success': True,
                        'step_instance': result,
                        'error': None
                    }
            
            return {'success': False, 'error': 'create_step ë©”ì„œë“œ ì—†ìŒ'}
            
        except Exception as e:
            self.logger.error(f"âŒ StepFactory Step {step_id} ìƒì„± ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _create_steps_via_implementation_manager(self) -> bool:
        """RealAIStepImplementationManagerë¥¼ í†µí•œ Step ìƒì„±"""
        try:
            if not self.step_implementation_manager:
                return False
            
            success_count = 0
            
            for step_id, step_info in self.step_mapping.items():
                try:
                    self.logger.info(f"ğŸ”„ Step {step_id} ({step_info['name']}) ImplementationManagerë¡œ ìƒì„± ì¤‘...")
                    
                    # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (êµ¬ì²´ì  ë°©ë²•ì€ êµ¬í˜„ì— ë”°ë¼)
                    step_instance = await self._create_step_directly(step_id, step_info)
                    
                    if step_instance:
                        # RealAIStepImplementationManager ì—°ë™ ì„¤ì •
                        if hasattr(step_instance, 'set_implementation_manager'):
                            step_instance.set_implementation_manager(self.step_implementation_manager)
                        
                        # DI Container ì˜ì¡´ì„± ì£¼ì…
                        await self._inject_dependencies_via_di_container(step_instance)
                        
                        self.steps[step_info['name']] = step_instance
                        success_count += 1
                        self.logger.info(f"âœ… Step {step_id} ({step_info['name']}) ImplementationManager ì—°ë™ ì™„ë£Œ")
                
                except Exception as e:
                    self.logger.error(f"âŒ Step {step_id} ImplementationManager ìƒì„± ì˜¤ë¥˜: {e}")
            
            self.logger.info(f"ğŸ“‹ ImplementationManager ê¸°ë°˜ Step ìƒì„± ì™„ë£Œ: {success_count}/{len(self.step_mapping)}")
            return success_count > 0
            
        except Exception as e:
            self.logger.error(f"âŒ ImplementationManager ê¸°ë°˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    async def _create_steps_directly(self) -> bool:
        """Step ì§ì ‘ ìƒì„± (DI Container ê¸°ë°˜)"""
        try:
            success_count = 0
            
            for step_id, step_info in self.step_mapping.items():
                step_instance = await self._create_step_directly(step_id, step_info)
                if step_instance:
                    self.steps[step_info['name']] = step_instance
                    success_count += 1
                else:
                    # ìµœì¢… í´ë°±: ë”ë¯¸ Step ìƒì„±
                    dummy_step = self._create_dummy_step(step_id, step_info)
                    self.steps[step_info['name']] = dummy_step
                    success_count += 1
            
            return success_count > 0
            
        except Exception as e:
            self.logger.error(f"âŒ ì§ì ‘ Step ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    async def _create_step_directly(self, step_id: int, step_info: Dict[str, Any]):
        """Step ì§ì ‘ ìƒì„± (DI Container ê¸°ë°˜ ì™„ì „ í˜¸í™˜ì„± ë³´ì¥)"""
        try:
            # ë™ì  ëª¨ë“ˆ import
            import importlib
            module = importlib.import_module(step_info['module_path'])
            step_class = getattr(module, step_info['class_name'], None)
            
            if not step_class:
                return None
            
            # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            step_instance = step_class(
                step_id=step_id,
                step_name=step_info['name'],
                device=self.device,
                is_m3_max=self.config.is_m3_max,
                memory_gb=self.config.memory_gb,
                ai_model_enabled=self.config.ai_model_enabled,
                use_dependency_injection=self.config.use_dependency_injection
            )
            
            # ğŸ”¥ DI Container ê¸°ë°˜ ê¸€ë¡œë²Œ í˜¸í™˜ì„± ë³´ì¥
            if DI_CONTAINER_AVAILABLE:
                config = {
                    'device': self.device,
                    'is_m3_max': self.config.is_m3_max,
                    'memory_gb': self.config.memory_gb,
                    'device_type': self.config.device_type,
                    'ai_model_enabled': self.config.ai_model_enabled,
                    'quality_level': self.config.quality_level.value if hasattr(self.config.quality_level, 'value') else self.config.quality_level,
                    'performance_mode': 'maximum' if self.config.is_m3_max else 'balanced'
                }
                
                ensure_global_step_compatibility(step_instance, step_id, step_info['name'], config)
            
            # ğŸ”¥ DI Container ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì…
            await self._inject_dependencies_via_di_container(step_instance)
            
            # ì•ˆì „í•œ ì´ˆê¸°í™”
            await self._initialize_step_safe(step_instance)
            
            return step_instance
            
        except ImportError as e:
            self.logger.debug(f"Step {step_id} ì§ì ‘ import ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ Step {step_id} ì§ì ‘ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    async def _inject_dependencies_via_di_container(self, step_instance):
        """DI Containerë¥¼ í†µí•œ ì˜ì¡´ì„± ì£¼ì…"""
        try:
            if not self.use_di_container or not self.di_container:
                return False
            
            # ğŸ”¥ DI Container ì•ˆì „ ì˜ì¡´ì„± ì£¼ì… ì‚¬ìš©
            inject_dependencies_to_step_safe(step_instance, self.di_container)
            
            # ì¶”ê°€ ì˜ì¡´ì„±ë“¤
            injections_made = 0
            
            # device ì •ë³´ ì£¼ì…
            if not hasattr(step_instance, 'device') or step_instance.device != self.device:
                step_instance.device = self.device
                injections_made += 1
            
            # ì‹œìŠ¤í…œ ì •ë³´ ì£¼ì…
            system_info = {
                'is_m3_max': self.config.is_m3_max,
                'memory_gb': self.config.memory_gb,
                'device_type': self.config.device_type,
                'conda_env': CONDA_ENV,
                'torch_available': TORCH_AVAILABLE,
                'mps_available': MPS_AVAILABLE
            }
            
            for key, value in system_info.items():
                if not hasattr(step_instance, key):
                    setattr(step_instance, key, value)
                    injections_made += 1
            
            # AI ëª¨ë¸ ê²½ë¡œ ì£¼ì…
            step_module = step_instance.__class__.__module__
            if step_module:
                module_name = step_module.split('.')[-1]  # step_XX_name í˜•ì‹
                if module_name in self.ai_model_paths:
                    step_instance.ai_model_paths = self.ai_model_paths[module_name]
                    injections_made += 1
            
            # StepImplementationManager ì—°ë™
            if self.step_implementation_manager and hasattr(step_instance, 'set_implementation_manager'):
                step_instance.set_implementation_manager(self.step_implementation_manager)
                injections_made += 1
            
            self.logger.debug(f"âœ… {step_instance.__class__.__name__} DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ ({injections_made}ê°œ)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ DI Container ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    async def _initialize_step_safe(self, step_instance) -> bool:
        """Step ì•ˆì „ ì´ˆê¸°í™” (DI Container ê¸°ë°˜)"""
        try:
            # ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš°
            if getattr(step_instance, 'is_initialized', False):
                return True
            
            # initialize ë©”ì„œë“œê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ í˜¸ì¶œ
            if hasattr(step_instance, 'initialize'):
                initialize_method = getattr(step_instance, 'initialize')
                
                try:
                    # ë¹„ë™ê¸° í•¨ìˆ˜ì¸ì§€ í™•ì¸
                    if asyncio.iscoroutinefunction(initialize_method):
                        result = await initialize_method()
                    else:
                        result = initialize_method()
                    
                    # ê²°ê³¼ ì²˜ë¦¬ (bool íƒ€ì… ì•ˆì „ ì²˜ë¦¬)
                    if result is None:
                        result = True  # Noneì€ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                    elif not isinstance(result, bool):
                        result = bool(result)  # ë‹¤ë¥¸ íƒ€ì…ì€ boolë¡œ ë³€í™˜
                        
                    # ê²°ê³¼ ì²˜ë¦¬
                    if result:
                        step_instance.is_initialized = True
                        step_instance.is_ready = True
                        return True
                    else:
                        self.logger.warning(f"âš ï¸ {step_instance.__class__.__name__} ì´ˆê¸°í™” ê²°ê³¼ False")
                        return False
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_instance.__class__.__name__} ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
                    return False
            else:
                # initialize ë©”ì„œë“œê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ì´ˆê¸°í™”
                self.logger.debug(f"â„¹ï¸ {step_instance.__class__.__name__} initialize ë©”ì„œë“œ ì—†ìŒ - ì§ì ‘ ì´ˆê¸°í™”")
            
            # ìƒíƒœ ì„¤ì • (í•­ìƒ ì‹¤í–‰)
            step_instance.is_initialized = True
            step_instance.is_ready = True
            
            self.logger.debug(f"âœ… {step_instance.__class__.__name__} ì•ˆì „ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Step ì•ˆì „ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ì˜ˆì™¸ ë°œìƒí•´ë„ ê¸°ë³¸ ìƒíƒœëŠ” ì„¤ì •
            step_instance.is_initialized = False
            step_instance.is_ready = False
            return False
    
    def _create_dummy_step(self, step_id: int, step_info: Dict[str, Any]):
        """ë”ë¯¸ Step ìƒì„± (DI Container í˜¸í™˜)"""
        class DummyStep:
            def __init__(self, step_id: int, step_info: Dict[str, Any], step_manager):
                self.step_id = step_id
                self.step_name = step_info['name']
                self.device = step_manager.device
                self.is_m3_max = step_manager.config.is_m3_max
                self.memory_gb = step_manager.config.memory_gb
                self.device_type = step_manager.config.device_type
                self.quality_level = "balanced"
                self.performance_mode = "basic"
                self.ai_model_enabled = False
                self.is_initialized = True
                self.is_ready = True
                self.has_model = False
                self.model_loaded = False
                self.warmup_completed = False
                self.logger = logging.getLogger(f"DummyStep{step_id}")
                
                # ğŸ”¥ Stepë³„ íŠ¹í™” ì†ì„±
                if step_info['name'] == 'geometric_matching':
                    self._force_mps_device = lambda: True
                    self.geometric_config = {'use_tps': True, 'use_gmm': True}
                
                if step_info['name'] == 'quality_assessment':
                    self.is_m3_max = step_manager.config.is_m3_max
                    self.optimization_enabled = self.is_m3_max
                    self.analysis_depth = 'comprehensive'
            
            async def process(self, *args, **kwargs):
                """ë”ë¯¸ ì²˜ë¦¬ (ëª¨ë“  ì‹œê·¸ë‹ˆì²˜ í˜¸í™˜)"""
                await asyncio.sleep(0.1)  # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                
                # Stepë³„ íŠ¹í™” ê²°ê³¼
                if self.step_name == 'human_parsing':
                    return {
                        'success': True,
                        'result': args[0] if args else torch.zeros(1, 3, 512, 512),
                        'parsed_image': args[0] if args else torch.zeros(1, 3, 512, 512),
                        'body_masks': torch.zeros(1, 20, 512, 512),
                        'human_regions': ['torso', 'arms', 'legs'],
                        'confidence': 0.7,
                        'dummy': True
                    }
                elif self.step_name == 'virtual_fitting':
                    return {
                        'success': True,
                        'result': args[0] if args else torch.zeros(1, 3, 512, 512),
                        'fitted_image': args[0] if args else torch.zeros(1, 3, 512, 512),
                        'fitting_quality': 0.7,
                        'virtual_confidence': 0.7,
                        'confidence': 0.7,
                        'dummy': True
                    }
                else:
                    return {
                        'success': True,
                        'result': args[0] if args else torch.zeros(1, 3, 512, 512),
                        'confidence': 0.7,
                        'quality_score': 0.7,
                        'step_name': self.step_name,
                        'dummy': True,
                        'processing_time': 0.1
                    }
            
            def initialize(self):
                """ì´ˆê¸°í™” (ë™ê¸° ë©”ì„œë“œ)"""
                return True
            
            def cleanup(self):
                """ì •ë¦¬ (ë™ê¸° ë©”ì„œë“œ)"""
                pass
            
            def get_status(self):
                """ìƒíƒœ ë°˜í™˜ (ë™ê¸° ë©”ì„œë“œ)"""
                return {
                    'step_name': self.step_name,
                    'is_initialized': self.is_initialized,
                    'is_ready': self.is_ready,
                    'has_model': self.has_model,
                    'dummy': True
                }
        
        return DummyStep(step_id, step_info, self)
    
    def get_step_by_name(self, step_name: str):
        """ì´ë¦„ìœ¼ë¡œ Step ë°˜í™˜"""
        return self.steps.get(step_name)
    
    def get_step_by_id(self, step_id: int):
        """IDë¡œ Step ë°˜í™˜"""
        for step_info in self.step_mapping.values():
            if step_info.get('step_id') == step_id:
                return self.steps.get(step_info['name'])
        return None

# ==============================================
# ğŸ”¥ DI Container ê¸°ë°˜ ë°ì´í„° íë¦„ ì—”ì§„
# ==============================================

class DIContainerDataFlowEngine:
    """DI Container ê¸°ë°˜ ì™„ì „í•œ ë°ì´í„° íë¦„ ì—”ì§„"""
    
    def __init__(self, step_manager: DIContainerStepManager, config: PipelineConfig, logger: logging.Logger):
        self.step_manager = step_manager
        self.config = config
        self.logger = logger
        
        # DI Container í†µí•©
        if config.use_dependency_injection and DI_CONTAINER_AVAILABLE:
            self.di_container = get_global_container()
            self.use_di_container = True
        else:
            self.di_container = None
            self.use_di_container = False
        
        # ë°ì´í„° íë¦„ ê·œì¹™ (GitHub ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜)
        self.data_flow_rules = {
            1: {  # HumanParsing
                'outputs_to': {
                    2: ['parsed_image', 'body_masks'],
                    3: ['parsed_image'],
                    4: ['parsed_image', 'human_regions'],
                    6: ['parsed_image', 'body_masks']
                }
            },
            2: {  # PoseEstimation
                'outputs_to': {
                    3: ['keypoints_18', 'skeleton_structure'],
                    4: ['keypoints_18', 'pose_confidence'],
                    5: ['pose_data'],
                    6: ['keypoints_18', 'skeleton_structure']
                }
            },
            3: {  # ClothSegmentation
                'outputs_to': {
                    4: ['clothing_masks', 'garment_type'],
                    5: ['clothing_masks', 'segmentation_confidence'],
                    6: ['clothing_masks']
                }
            },
            4: {  # GeometricMatching
                'outputs_to': {
                    5: ['matching_matrix', 'correspondence_points'],
                    6: ['geometric_matching']
                }
            },
            5: {  # ClothWarping
                'outputs_to': {
                    6: ['warped_clothing', 'warping_field']
                }
            },
            6: {  # VirtualFitting
                'outputs_to': {
                    7: ['fitted_image', 'fitting_quality'],
                    8: ['fitted_image']
                }
            },
            7: {  # PostProcessing
                'outputs_to': {
                    8: ['enhanced_image', 'enhancement_quality']
                }
            }
        }
    
    def prepare_step_input(self, step_id: int, current_result: PipelineStepResult, 
                          original_inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Stepë³„ ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (DI Container ê¸°ë°˜)"""
        try:
            step_info = self.step_manager.step_mapping.get(step_id, {})
            step_name = step_info.get('name', f'step_{step_id}')
            
            # ê¸°ë³¸ ì…ë ¥ ë°ì´í„°
            input_data = {
                'session_id': original_inputs.get('session_id'),
                'step_id': step_id,
                'step_name': step_name
            }
            
            # ğŸ”¥ DI Containerë¥¼ í†µí•œ ë°ì´í„° ë³´ê°•
            if self.use_di_container:
                # ì‹œìŠ¤í…œ ì •ë³´ ì¶”ê°€
                system_data = {
                    'device': get_service_safe('device') or self.config.device,
                    'is_m3_max': get_service_safe('is_m3_max') or self.config.is_m3_max,
                    'memory_gb': get_service_safe('memory_gb') or self.config.memory_gb
                }
                input_data.update(system_data)
            
            # Stepë³„ íŠ¹í™” ì…ë ¥ ì¤€ë¹„
            if step_id == 1:  # HumanParsing
                input_data.update({
                    'image': original_inputs.get('person_image'),
                    'person_image': original_inputs.get('person_image')
                })
            
            elif step_id == 2:  # PoseEstimation
                step01_data = current_result.get_data_for_step(2)
                input_data.update({
                    'image': step01_data.get('parsed_image', original_inputs.get('person_image')),
                    'parsed_image': step01_data.get('parsed_image'),
                    'body_masks': step01_data.get('body_masks')
                })
            
            elif step_id == 3:  # ClothSegmentation
                input_data.update({
                    'image': original_inputs.get('clothing_image'),
                    'clothing_image': original_inputs.get('clothing_image'),
                    'clothing_type': original_inputs.get('clothing_type', 'shirt')
                })
            
            elif step_id == 4:  # GeometricMatching
                # ğŸ”¥ ì˜¬ë°”ë¥¸ ë°©ì‹: ê° Stepì˜ ê²°ê³¼ë¥¼ ê°œë³„ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
                step01_data = current_result.get_data_for_step(1)  # Step 1 ê²°ê³¼
                step02_data = current_result.get_data_for_step(2)  # Step 2 ê²°ê³¼  
                step03_data = current_result.get_data_for_step(3)  # Step 3 ê²°ê³¼
                
                input_data.update({
                    'person_image': original_inputs.get('person_image'),
                    'clothing_image': original_inputs.get('clothing_image'),
                    # ğŸ”¥ Step 1 ê²°ê³¼: ì¸ì²´ íŒŒì‹±
                    'person_parsing': {
                        'result': step01_data.get('parsed_image'),
                        'body_masks': step01_data.get('body_masks'),
                        'parsing_mask': step01_data.get('parsing_mask'),
                        'segments': step01_data.get('segments', {})
                    },
                    # ğŸ”¥ Step 2 ê²°ê³¼: í¬ì¦ˆ ì¶”ì •
                    'pose_keypoints': step02_data.get('keypoints_18', []),
                    'pose_data': step02_data.get('pose_data', {}),
                    'pose_confidence': step02_data.get('pose_confidence', 0.0),
                    # ğŸ”¥ Step 3 ê²°ê³¼: ì˜ë¥˜ ë¶„í• 
                    'clothing_segmentation': {
                        'mask': step03_data.get('segmentation_masks', {}),
                        'segmentation_result': step03_data.get('segmentation_masks', {}),
                        'clothing_mask': step03_data.get('segmentation_masks', {}).get('all_clothes', None)
                    },
                    'clothing_type': original_inputs.get('clothing_type', 'shirt')
                })
            
            elif step_id == 5:  # ClothWarping
                # ğŸ”¥ ì˜¬ë°”ë¥¸ ë°©ì‹: ê° Stepì˜ ê²°ê³¼ë¥¼ ê°œë³„ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
                step03_data = current_result.get_data_for_step(3)  # Step 3 ê²°ê³¼
                step04_data = current_result.get_data_for_step(4)  # Step 4 ê²°ê³¼
                
                input_data.update({
                    'cloth_image': original_inputs.get('clothing_image'),
                    'person_image': original_inputs.get('person_image'),
                    'cloth_mask': step03_data.get('segmentation_masks', {}).get('all_clothes', None),  # Step 3: ì˜ë¥˜ ë¶„í•  ë§ˆìŠ¤í¬
                    'body_measurements': original_inputs.get('body_measurements', {}),
                    'fabric_type': original_inputs.get('fabric_type', 'cotton'),
                    'geometric_matching': step04_data.get('matching_matrix'),  # Step 4: ê¸°í•˜í•™ì  ë§¤ì¹­ ê²°ê³¼
                    'matching_precision': step04_data.get('matching_precision', 'high'),
                    'transformation_matrix': step04_data.get('transformation_matrix')
                })
            
            elif step_id == 6:  # VirtualFitting
                # ğŸ”¥ ì˜¬ë°”ë¥¸ ë°©ì‹: Step 5ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê¸°
                step05_data = current_result.get_data_for_step(5)  # Step 5 ê²°ê³¼
                step02_data = current_result.get_data_for_step(2)  # Step 2 ê²°ê³¼ (í¬ì¦ˆ ë°ì´í„°)
                step03_data = current_result.get_data_for_step(3)  # Step 3 ê²°ê³¼ (ì˜ë¥˜ ë§ˆìŠ¤í¬)
                
                input_data.update({
                    'person_image': original_inputs.get('person_image'),
                    'cloth_image': step05_data.get('warped_clothing', original_inputs.get('clothing_image')),  # Step 5: ë³€í˜•ëœ ì˜ë¥˜
                    'pose_data': step02_data.get('keypoints_18', []),  # Step 2: í¬ì¦ˆ í‚¤í¬ì¸íŠ¸
                    'cloth_mask': step03_data.get('segmentation_masks', {}).get('all_clothes', None),  # Step 3: ì˜ë¥˜ ë¶„í•  ë§ˆìŠ¤í¬
                    'style_preferences': original_inputs.get('style_preferences', {}),
                    'warping_quality': step05_data.get('warping_quality', 'high'),  # Step 5: ë³€í˜• í’ˆì§ˆ
                    'transformation_matrix': step05_data.get('transformation_matrix')  # Step 5: ë³€í˜• í–‰ë ¬
                })
            
            elif step_id == 7:  # PostProcessing
                # ğŸ”¥ ì˜¬ë°”ë¥¸ ë°©ì‹: Step 6ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê¸°
                step06_data = current_result.get_data_for_step(6)  # Step 6 ê²°ê³¼
                
                input_data.update({
                    'fitted_image': step06_data.get('fitted_image'),  # Step 6: ê°€ìƒ í”¼íŒ… ê²°ê³¼ ì´ë¯¸ì§€
                    'enhancement_level': original_inputs.get('enhancement_level', 'medium'),
                    'fitting_quality': step06_data.get('fitting_quality', 'high'),  # Step 6: í”¼íŒ… í’ˆì§ˆ
                    'confidence_score': step06_data.get('confidence_score', 0.8),  # Step 6: ì‹ ë¢°ë„ ì ìˆ˜
                    'virtual_fitting_result': step06_data.get('virtual_fitting_result', {})  # Step 6: í”¼íŒ… ê²°ê³¼ ë°ì´í„°
                })
            
            elif step_id == 8:  # QualityAssessment
                # ğŸ”¥ ì˜¬ë°”ë¥¸ ë°©ì‹: Step 7ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê¸°
                step07_data = current_result.get_data_for_step(7)  # Step 7 ê²°ê³¼
                
                input_data.update({
                    'final_image': step07_data.get('enhanced_image'),  # Step 7: í›„ì²˜ë¦¬ëœ ìµœì¢… ì´ë¯¸ì§€
                    'original_images': {
                        'person': original_inputs.get('person_image'),
                        'clothing': original_inputs.get('clothing_image')
                    },
                    'analysis_depth': original_inputs.get('analysis_depth', 'comprehensive'),
                    'enhancement_quality': step07_data.get('enhancement_quality', 'high'),  # Step 7: í–¥ìƒ í’ˆì§ˆ
                    'post_processing_result': step07_data.get('post_processing_result', {}),  # Step 7: í›„ì²˜ë¦¬ ê²°ê³¼
                    'enhancement_metrics': step07_data.get('enhancement_metrics', {})  # Step 7: í–¥ìƒ ì§€í‘œ
                })
            
            return input_data
            
        except Exception as e:
            self.logger.error(f"âŒ Step {step_id} ì…ë ¥ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return {}
    
    def process_step_output(self, step_id: int, step_result: Dict[str, Any], 
                           current_result: PipelineStepResult) -> PipelineStepResult:
        """Step ì¶œë ¥ ì²˜ë¦¬ ë° ë‹¤ìŒ Step ë°ì´í„° ì¤€ë¹„ (DI Container ê¸°ë°˜) - ê°•í™”ëœ ë¡œê¹… ë° ê²€ì¦"""
        data_flow_stats = {
            'total_transfers': 0,
            'successful_transfers': 0,
            'data_loss_detected': 0,
            'memory_optimizations': 0,
            'di_container_services_used': [],
            'warnings': [],
            'errors': []
        }
        
        try:
            self.logger.info(f"ğŸ”„ Step {step_id} ì¶œë ¥ ì²˜ë¦¬ ì‹œì‘")
            
            # í˜„ì¬ Step ê²°ê³¼ ì €ì¥
            current_result.ai_results[f'step_{step_id:02d}'] = step_result
            
            # ğŸ”¥ DI Containerë¥¼ í†µí•œ ê²°ê³¼ ë³´ê°• ë° ë¶„ì„
            if self.use_di_container:
                # ë©”ëª¨ë¦¬ ìµœì í™” ì„œë¹„ìŠ¤ í˜¸ì¶œ
                memory_manager = get_service_safe('memory_manager')
                if memory_manager and hasattr(memory_manager, 'optimize_memory'):
                    try:
                        memory_manager.optimize_memory()
                        data_flow_stats['memory_optimizations'] += 1
                        data_flow_stats['di_container_services_used'].append('memory_manager')
                        self.logger.debug(f"âœ… Step {step_id} ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
                    except Exception as e:
                        self.logger.debug(f"âš ï¸ Step {step_id} ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
                        data_flow_stats['warnings'].append(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
                
                # ë°ì´í„° ì»¨ë²„í„° ì„œë¹„ìŠ¤ í™•ì¸
                data_converter = get_service_safe('data_converter')
                if data_converter:
                    data_flow_stats['di_container_services_used'].append('data_converter')
            
            # ğŸ”¥ _apply_step_data_flow ë©”ì„œë“œë¥¼ í†µí•œ ë°ì´í„° íë¦„ ì²˜ë¦¬
            step_instance = self.step_manager.get_step_by_id(step_id) if hasattr(self, 'step_manager') else None
            if step_instance:
                try:
                    enhanced_result = self._apply_step_data_flow(
                        step_result, step_instance, step_id, current_result
                    )
                    self.logger.info(f"âœ… Step {step_id} ë°ì´í„° íë¦„ ì²˜ë¦¬ ì™„ë£Œ")
                    data_flow_stats['successful_transfers'] += 1
                except Exception as flow_error:
                    self.logger.error(f"âŒ Step {step_id} ë°ì´í„° íë¦„ ì²˜ë¦¬ ì‹¤íŒ¨: {flow_error}")
                    data_flow_stats['errors'].append(f"ë°ì´í„° íë¦„ ì²˜ë¦¬ ì‹¤íŒ¨: {flow_error}")
            else:
                # ğŸ”¥ í´ë°±: ê¸°ì¡´ ë°ì´í„° íë¦„ ê·œì¹™ ì‚¬ìš©
                self.logger.warning(f"âš ï¸ Step {step_id} ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ì¡´ ê·œì¹™ ì‚¬ìš©")
                flow_rules = self.data_flow_rules.get(step_id, {})
                outputs_to = flow_rules.get('outputs_to', {})
                
                self.logger.debug(f"   - Step {step_id} ë°ì´í„° íë¦„ ê·œì¹™: {outputs_to}")
                
                for target_step, data_keys in outputs_to.items():
                    target_data = {}
                    step_data_loss_count = 0
                    
                    data_flow_stats['total_transfers'] += 1
                    
                    # ğŸ”¥ ë°ì´í„° í‚¤ë³„ ìƒì„¸ ê²€ì¦ ë° ë³µì‚¬
                    for key in data_keys:
                        if key in step_result:
                            target_data[key] = step_result[key]
                            self.logger.debug(f"     - {key} â†’ Step {target_step}: âœ…")
                        elif 'data' in step_result and key in step_result['data']:
                            target_data[key] = step_result['data'][key]
                            self.logger.debug(f"     - {key} â†’ Step {target_step}: âœ… (nested)")
                        else:
                            step_data_loss_count += 1
                            data_flow_stats['data_loss_detected'] += 1
                            data_flow_stats['warnings'].append(f"Step {target_step}: {key} í‚¤ ëˆ„ë½")
                            self.logger.warning(f"âš ï¸ Step {target_step}: {key} í‚¤ê°€ step_resultì— ì—†ìŒ")
                    
                    # ğŸ”¥ ë°ì´í„° ì†ì‹¤ ê²€ì¦
                    if step_data_loss_count > 0:
                        self.logger.warning(f"âš ï¸ Step {step_id} â†’ Step {target_step}: {step_data_loss_count}ê°œ ë°ì´í„° ì†ì‹¤")
                    else:
                        data_flow_stats['successful_transfers'] += 1
                        self.logger.debug(f"âœ… Step {step_id} â†’ Step {target_step}: ëª¨ë“  ë°ì´í„° ì „ë‹¬ ì„±ê³µ")
                    
                    # ëŒ€ìƒ Stepì˜ for_step_XX í•„ë“œì— ë°ì´í„° ì„¤ì •
                    target_field = f'for_step_{target_step:02d}'
                    if hasattr(current_result, target_field):
                        existing_data = getattr(current_result, target_field)
                        existing_data.update(target_data)
                        setattr(current_result, target_field, existing_data)
                        
                        # ğŸ”¥ ë°ì´í„° í¬ê¸° ë¡œê¹…
                        try:
                            total_size_mb = 0
                            for value in target_data.values():
                                if hasattr(value, 'nbytes'):
                                    total_size_mb += value.nbytes / (1024 * 1024)
                                elif hasattr(value, 'shape'):
                                    total_size_mb += np.prod(value.shape) * value.dtype.itemsize / (1024 * 1024)
                            
                            if total_size_mb > 50:  # 50MB ì´ìƒ
                                self.logger.info(f"ğŸ“Š Step {step_id} â†’ Step {target_step}: {total_size_mb:.2f}MB ì „ë‹¬")
                                
                        except Exception as size_error:
                            pass
                    else:
                        self.logger.error(f"âŒ Step {step_id}: {target_field} í•„ë“œê°€ ì—†ìŒ")
                        data_flow_stats['errors'].append(f"{target_field} í•„ë“œ ì—†ìŒ")
            
            # íŒŒì´í”„ë¼ì¸ ì „ì²´ ë°ì´í„° ì—…ë°ì´íŠ¸
            current_result.pipeline_data.update({
                f'step_{step_id:02d}_output': step_result,
                f'step_{step_id:02d}_completed': True,
                f'step_{step_id:02d}_data_flow_stats': data_flow_stats
            })
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            current_result.metadata[f'step_{step_id:02d}'] = {
                'completed': True,
                'processing_time': step_result.get('processing_time', 0.0),
                'success': step_result.get('success', True),
                'confidence': step_result.get('confidence', 0.8),
                'data_flow_stats': data_flow_stats
            }
            
            # ğŸ”¥ ë°ì´í„° íë¦„ í†µê³„ ë¡œê¹…
            success_rate = (data_flow_stats['successful_transfers'] / 
                          max(data_flow_stats['total_transfers'], 1)) * 100
            
            self.logger.info(f"âœ… Step {step_id} ì¶œë ¥ ì²˜ë¦¬ ì™„ë£Œ")
            self.logger.info(f"   - ë°ì´í„° ì „ë‹¬ ì„±ê³µë¥ : {success_rate:.1f}% ({data_flow_stats['successful_transfers']}/{data_flow_stats['total_transfers']})")
            self.logger.info(f"   - ë°ì´í„° ì†ì‹¤: {data_flow_stats['data_loss_detected']}ê°œ")
            self.logger.info(f"   - ë©”ëª¨ë¦¬ ìµœì í™”: {data_flow_stats['memory_optimizations']}íšŒ")
            self.logger.info(f"   - DI Container ì„œë¹„ìŠ¤: {len(data_flow_stats['di_container_services_used'])}ê°œ")
            
            # ê²½ê³  ë° ì˜¤ë¥˜ ë¡œê¹…
            for warning in data_flow_stats['warnings']:
                self.logger.warning(f"âš ï¸ Step {step_id}: {warning}")
            for error in data_flow_stats['errors']:
                self.logger.error(f"âŒ Step {step_id}: {error}")
            
            return current_result
            
        except Exception as e:
            self.logger.error(f"âŒ Step {step_id} ì¶œë ¥ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"   - ì˜¤ë¥˜ ìœ„ì¹˜: {traceback.format_exc()}")
            data_flow_stats['errors'].append(f"ì¶œë ¥ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ì˜¤ë¥˜ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì €ì¥
            if f'step_{step_id:02d}' not in current_result.metadata:
                current_result.metadata[f'step_{step_id:02d}'] = {}
            current_result.metadata[f'step_{step_id:02d}']['error'] = str(e)
            current_result.metadata[f'step_{step_id:02d}']['data_flow_stats'] = data_flow_stats
            
            return current_result

# ==============================================
# ğŸ”¥ ì™„ì „í•œ DI Container í†µí•© PipelineManager v12.0
# ==============================================

class PipelineManager:
    """
    ğŸ”¥ ì™„ì „í•œ DI Container í†µí•© PipelineManager v12.0
    
    âœ… DI Container v4.0 ì™„ì „ í†µí•© (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€)
    âœ… GitHub Step íŒŒì¼ êµ¬ì¡° 100% ë°˜ì˜
    âœ… RealAIStepImplementationManager v14.0 ì™„ì „ ì—°ë™
    âœ… BaseStepMixin v19.3 DI Container ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì…
    âœ… ì™„ì „í•œ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‘ë™
    âœ… ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ 100% í˜¸í™˜
    """
    
    def __init__(
        self,
        config_path: Optional[str] = None,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], PipelineConfig]] = None,
        **kwargs
    ):
        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._auto_detect_device(device)
        
        # ì„¤ì • ì´ˆê¸°í™”
        if isinstance(config, PipelineConfig):
            self.config = config
        else:
            config_dict = self._load_config(config_path) if config_path else {}
            if config:
                config_dict.update(config if isinstance(config, dict) else {})
            config_dict.update(kwargs)
            
            # M3 Max ìë™ ìµœì í™”
            if detect_m3_max():
                config_dict.update({
                    'is_m3_max': True,
                    'memory_gb': 128.0,
                    'device': 'mps',
                    'device_type': 'apple_silicon'
                })
            
            # ğŸ”¥ DI Container ì„¤ì • ê°•ì œ í™œì„±í™”
            config_dict.update({
                'use_dependency_injection': True,
                'auto_inject_dependencies': True,
                'enable_adapter_pattern': True,
                'use_circular_reference_free_di': True,
                'enable_lazy_dependency_resolution': True
            })
            config_dict.pop("device", None)
            self.config = PipelineConfig(device=self.device, **config_dict)
        
        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        # ğŸ”¥ DI Container í†µí•© ì´ˆê¸°í™”
        if DI_CONTAINER_AVAILABLE and self.config.use_dependency_injection:
            self.di_container = get_global_container()
            self.use_di_container = True
            self.logger.info("âœ… DI Container v4.0 í†µí•© PipelineManager ì´ˆê¸°í™”")
        else:
            self.di_container = None
            self.use_di_container = False
            self.logger.warning("âš ï¸ DI Container ì—†ì´ PipelineManager ì´ˆê¸°í™”")
        
        # ê´€ë¦¬ìë“¤ ì´ˆê¸°í™” (DI Container ê¸°ë°˜)
        self.step_manager = DIContainerStepManager(self.config, self.device, self.logger)
        self.data_flow_engine = DIContainerDataFlowEngine(self.step_manager, self.config, self.logger)
        
        # ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.current_status = PipelineStatus.IDLE
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'total_sessions': 0,
            'successful_sessions': 0,
            'average_processing_time': 0.0,
            'average_quality_score': 0.0
        }
        
        self.logger.info(f"ğŸ”¥ PipelineManager v12.0 DI Container í†µí•© ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device and preferred_device != "auto":
            return preferred_device
        
        try:
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if not config_path or not os.path.exists(config_path):
            return {}
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.warning(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    async def initialize(self) -> bool:
        """íŒŒì´í”„ë¼ì¸ ì™„ì „ ì´ˆê¸°í™” (DI Container ê¸°ë°˜)"""
        try:
            self.logger.info("ğŸš€ PipelineManager v12.0 DI Container ê¸°ë°˜ ì™„ì „ ì´ˆê¸°í™” ì‹œì‘...")
            self.current_status = PipelineStatus.INITIALIZING
            start_time = time.time()
            
            # ğŸ”¥ DI Container ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            if self.use_di_container:
                di_success = initialize_di_system_safe()
                if di_success:
                    self.logger.info("âœ… DI Container ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                    
                    # ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤ë“¤ ë“±ë¡
                    register_service_safe('device', self.device)
                    register_service_safe('is_m3_max', self.config.is_m3_max)
                    register_service_safe('memory_gb', self.config.memory_gb)
                    register_service_safe('conda_env', CONDA_ENV)
                    register_service_safe('torch_available', TORCH_AVAILABLE)
                    register_service_safe('mps_available', MPS_AVAILABLE)
                    
                else:
                    self.logger.warning("âš ï¸ DI Container ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # Step ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            step_success = await self.step_manager.initialize()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            await self._optimize_memory()
            
            # ì´ˆê¸°í™” ê²€ì¦
            step_count = len(self.step_manager.steps)
            
            initialization_time = time.time() - start_time
            self.is_initialized = step_count >= 4  # ìµœì†Œ ì ˆë°˜ ì´ìƒ
            self.current_status = PipelineStatus.IDLE if self.is_initialized else PipelineStatus.FAILED
            
            if self.is_initialized:
                self.logger.info(f"ğŸ‰ PipelineManager v12.0 DI Container ì´ˆê¸°í™” ì™„ë£Œ ({initialization_time:.2f}ì´ˆ)")
                self.logger.info(f"ğŸ“Š Step ì´ˆê¸°í™”: {step_count}/8")
                self.logger.info(f"ğŸ”— DI Container ì‚¬ìš©: {'âœ…' if self.use_di_container else 'âŒ'}")
            else:
                self.logger.error("âŒ PipelineManager v12.0 ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            return self.is_initialized
            
        except Exception as e:
            self.logger.error(f"âŒ PipelineManager DI Container ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            self.current_status = PipelineStatus.FAILED
            return False
    
    # ==============================================
    # ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ 100% ìœ ì§€)
    # ==============================================
    
    async def process_complete_virtual_fitting(
        self,
        person_image: Union[str, Image.Image, np.ndarray],
        clothing_image: Union[str, Image.Image, np.ndarray],
        body_measurements: Optional[Dict[str, float]] = None,
        clothing_type: str = "shirt",
        fabric_type: str = "cotton",
        style_preferences: Optional[Dict[str, Any]] = None,
        quality_target: float = 0.8,
        progress_callback: Optional[Callable] = None,
        save_intermediate: bool = False,
        session_id: Optional[str] = None
    ) -> ProcessingResult:
        """ì™„ì „í•œ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ (DI Container ê¸°ë°˜)"""
        
        if not self.is_initialized:
            raise RuntimeError("íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        self.current_status = PipelineStatus.PROCESSING
        
        try:
            session_id = session_id or f"session_{int(time.time())}_{uuid.uuid4().hex[:8]}"
            start_time = time.time()
            
            self.logger.info(f"ğŸš€ DI Container ê¸°ë°˜ ì™„ì „í•œ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì‹œì‘ - ì„¸ì…˜: {session_id}")
            
            # ğŸ”¥ DI Containerë¥¼ í†µí•œ ì „ì²˜ë¦¬ ì„œë¹„ìŠ¤ í™œìš©
            person_tensor = await self._preprocess_image_via_di_container(person_image)
            clothing_tensor = await self._preprocess_image_via_di_container(clothing_image)
            
            # ì›ë³¸ ì…ë ¥ ë°ì´í„°
            original_inputs = {
                'session_id': session_id,
                'person_image': person_tensor,
                'clothing_image': clothing_tensor,
                'body_measurements': body_measurements or {},
                'clothing_type': clothing_type,
                'fabric_type': fabric_type,
                'style_preferences': style_preferences or {},
                'quality_target': quality_target
            }
            
            # íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ì´ˆê¸°í™”
            pipeline_result = PipelineStepResult(
                step_id=0,
                step_name="pipeline_start",
                success=True,
                original_inputs=original_inputs,
                pipeline_data={'start_time': start_time}
            )
            
            # 8ë‹¨ê³„ ìˆœì°¨ ì²˜ë¦¬ (DI Container ê¸°ë°˜)
            step_results = {}
            step_timings = {}
            ai_models_used = {}
            
            for step_id in range(1, 9):
                step_start_time = time.time()
                step_info = self.step_manager.step_mapping.get(step_id, {})
                step_name = step_info.get('name', f'step_{step_id}')
                
                self.logger.info(f"ğŸ“‹ {step_id}/8 ë‹¨ê³„: {step_name} ì²˜ë¦¬ ì¤‘... (DI Container ê¸°ë°˜)")
                
                try:
                    # Step ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                    step_instance = self.step_manager.get_step_by_name(step_name)
                    if not step_instance:
                        raise RuntimeError(f"Step {step_name} ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    
                    # ğŸ”¥ Step ì…ë ¥ ë°ì´í„° ì¤€ë¹„
                    step_input = self.data_flow_engine.prepare_step_input(
                        step_id, pipeline_result, original_inputs
                    )
                    
                    # ğŸ”¥ DetailedDataSpec ê¸°ë°˜ ì…ë ¥ ì „ì²˜ë¦¬
                    step_input = self._apply_detailed_data_spec_processing(
                        step_instance, step_input, step_id
                    )
                    
                    # ğŸ”¥ pipeline_resultë¥¼ step_inputì— ì¶”ê°€
                    step_input['pipeline_result'] = pipeline_result
                    
                    # ğŸ”¥ RealAIStepImplementationManagerë¥¼ í†µí•œ ì²˜ë¦¬ ì‹œë„
                    step_result = await self._process_step_via_implementation_manager(
                        step_instance, step_input, step_id, step_name
                    )
                    
                    # GitHub ì‹¤ì œ process ë©”ì„œë“œ í˜¸ì¶œ (í´ë°±)
                    if not step_result or not step_result.get('success', False):
                        step_result = await self._process_step_directly(
                            step_instance, step_input, step_name
                        )
                    
                    # ê²°ê³¼ ì²˜ë¦¬
                    if not isinstance(step_result, dict):
                        step_result = {'success': True, 'result': step_result}
                    
                    # ğŸ”¥ DetailedDataSpec ê¸°ë°˜ í›„ì²˜ë¦¬
                    step_result = self._apply_postprocessing_requirements(
                        step_result, step_instance, step_id
                    )
                    
                    step_processing_time = time.time() - step_start_time
                    step_result['processing_time'] = step_processing_time
                    step_result['di_container_used'] = self.use_di_container
                    
                    # ğŸ”¥ DI Container ê¸°ë°˜ ë°ì´í„° íë¦„ ì²˜ë¦¬
                    step_result = self._apply_step_data_flow(
                        step_result, step_instance, step_id, pipeline_result
                    )
                    
                    # ğŸ”¥ data_flow_engineì€ ë©”íƒ€ë°ì´í„°ë§Œ ì—…ë°ì´íŠ¸ (ë°ì´í„°ëŠ” _apply_step_data_flowì—ì„œ ì²˜ë¦¬ë¨)
                    pipeline_result = self.data_flow_engine.process_step_output(
                        step_id, step_result, pipeline_result
                    )
                    
                    # ê²°ê³¼ ì €ì¥
                    step_results[step_name] = step_result
                    step_timings[step_name] = step_processing_time
                    ai_models_used[step_name] = step_result.get('ai_models_used', ['unknown'])
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if progress_callback:
                        progress = step_id * 100 // 8
                        try:
                            if asyncio.iscoroutinefunction(progress_callback):
                                await progress_callback(f"{step_name} ì™„ë£Œ", progress)
                            else:
                                progress_callback(f"{step_name} ì™„ë£Œ", progress)
                        except:
                            pass
                    
                    confidence = step_result.get('confidence', 0.8)
                    self.logger.info(f"âœ… {step_id}ë‹¨ê³„ ì™„ë£Œ - ì‹œê°„: {step_processing_time:.2f}ì´ˆ, ì‹ ë¢°ë„: {confidence:.3f}")
                    
                except Exception as e:
                    self.logger.error(f"âŒ {step_id}ë‹¨ê³„ ({step_name}) ì‹¤íŒ¨: {e}")
                    
                    # ì—ëŸ¬ ê²°ê³¼ ì €ì¥
                    step_results[step_name] = {
                        'success': False,
                        'error': str(e),
                        'processing_time': time.time() - step_start_time,
                        'di_container_used': self.use_di_container
                    }
                    step_timings[step_name] = time.time() - step_start_time
                    ai_models_used[step_name] = ['error']
                    
                    # ì¹˜ëª…ì  ë‹¨ê³„ì—ì„œëŠ” ì¤‘ë‹¨
                    if step_id in [6, 7]:  # VirtualFitting, PostProcessing 
                        break
                    continue
            
            # ìµœì¢… ê²°ê³¼ ìƒì„±
            total_time = time.time() - start_time
            quality_score = self._calculate_quality_score(step_results)
            quality_grade = self._get_quality_grade(quality_score)
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ì¶”ì¶œ
            result_image = self._extract_final_image(step_results)
            result_tensor = self._extract_final_tensor(step_results)
            
            # ì„±ê³µ ì—¬ë¶€ ê²°ì •
            success = quality_score >= 0.6 and len([r for r in step_results.values() if r.get('success', False)]) >= 6
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            self._update_performance_stats(success, total_time, quality_score)
            
            self.current_status = PipelineStatus.COMPLETED if success else PipelineStatus.FAILED
            
            result = ProcessingResult(
                success=success,
                session_id=session_id,
                result_image=result_image,
                result_tensor=result_tensor,
                quality_score=quality_score,
                quality_grade=quality_grade,
                processing_time=total_time,
                step_results=step_results,
                step_timings=step_timings,
                ai_models_used=ai_models_used,
                pipeline_metadata={
                    'device': self.device,
                    'is_m3_max': self.config.is_m3_max,
                    'total_steps': 8,
                    'completed_steps': len(step_results),
                    'github_structure': True,
                    'di_container_version': '4.0',
                    'di_container_used': self.use_di_container,
                    'real_ai_implementation_manager': self.step_manager.step_implementation_manager is not None
                },
                performance_metrics=self._get_performance_metrics(step_results)
            )
            
            # ğŸ”¥ ì „ì²´ ì„±ëŠ¥ ìš”ì•½ ë¡œê·¸ ì¶”ê°€
            self.logger.info(f"ğŸ‰ DI Container ê¸°ë°˜ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì™„ë£Œ!")
            self.logger.info(f"   ğŸ“Š ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.3f}ì´ˆ")
            self.logger.info(f"   ğŸ“Š í‰ê·  Step ì‹œê°„: {total_time/8:.3f}ì´ˆ")
            self.logger.info(f"   ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f}")
            self.logger.info(f"   ğŸ“Š í’ˆì§ˆ ë“±ê¸‰: {quality_grade}")
            self.logger.info(f"   ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
            self.logger.info(f"   ğŸ§  DI Container ì‚¬ìš©: {self.use_di_container}")
            
            # Stepë³„ ìƒì„¸ ì‹œê°„ ë¡œê·¸
            self.logger.info(f"   ğŸ“‹ Stepë³„ ì²˜ë¦¬ ì‹œê°„:")
            for step_name, step_time in step_timings.items():
                step_success = step_results.get(step_name, {}).get('success', False)
                status_icon = "âœ…" if step_success else "âŒ"
                self.logger.info(f"      {status_icon} {step_name}: {step_time:.3f}ì´ˆ")
            
            # ì„±ëŠ¥ í†µê³„
            successful_steps = len([r for r in step_results.values() if r.get('success', False)])
            failed_steps = len(step_results) - successful_steps
            self.logger.info(f"   ğŸ“ˆ ì„±ê³µí•œ Step: {successful_steps}/8")
            self.logger.info(f"   ğŸ“‰ ì‹¤íŒ¨í•œ Step: {failed_steps}/8")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ëŒ€ëµì )
            try:
                import psutil
                memory_usage = psutil.Process().memory_info().rss / (1024 * 1024)
                self.logger.info(f"   ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.1f}MB")
            except:
                pass
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ DI Container ê¸°ë°˜ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.current_status = PipelineStatus.FAILED
            
            return ProcessingResult(
                success=False,
                session_id=session_id or f"error_{int(time.time())}",
                processing_time=time.time() - start_time if 'start_time' in locals() else 0.0,
                error_message=str(e),
                pipeline_metadata={
                    'error_location': traceback.format_exc(),
                    'di_container_used': self.use_di_container
                }
            )
    
    # ==============================================
    # ğŸ”¥ DetailedDataSpec ê¸°ë°˜ API ë§¤í•‘ ë° ë°ì´í„° ë³€í™˜ (ë¹ ì§„ ê¸°ëŠ¥ ì¶”ê°€)
    # ==============================================
    
    def _apply_detailed_data_spec_processing(self, step_instance, input_data: Dict[str, Any], step_id: int) -> Dict[str, Any]:
        """DetailedDataSpec ê¸°ë°˜ API ë§¤í•‘ ë° ë°ì´í„° ë³€í™˜"""
        try:
            # DetailedDataSpec ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            detailed_spec = getattr(step_instance, 'detailed_data_spec', None)
            if not detailed_spec:
                return input_data
            
            processed_data = input_data.copy()
            
            # ğŸ”¥ API ì…ë ¥ ë§¤í•‘ ì ìš©
            api_input_mapping = getattr(detailed_spec, 'api_input_mapping', {})
            if api_input_mapping:
                mapped_data = {}
                for api_field, step_field in api_input_mapping.items():
                    if api_field in processed_data:
                        mapped_data[step_field] = processed_data[api_field]
                        self.logger.debug(f"API ë§¤í•‘: {api_field} â†’ {step_field}")
                
                processed_data.update(mapped_data)
            
            # ğŸ”¥ ì „ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ì ìš©
            preprocessing_required = getattr(detailed_spec, 'preprocessing_required', [])
            preprocessing_steps = getattr(detailed_spec, 'preprocessing_steps', [])
            
            if preprocessing_required or preprocessing_steps:
                processed_data = self._apply_preprocessing_requirements(
                    processed_data, preprocessing_required, preprocessing_steps, detailed_spec
                )
            
            # ğŸ”¥ ì…ë ¥ ìŠ¤í‚¤ë§ˆ ê²€ì¦
            input_shapes = getattr(detailed_spec, 'input_shapes', {})
            input_value_ranges = getattr(detailed_spec, 'input_value_ranges', {})
            
            if input_shapes or input_value_ranges:
                processed_data = self._validate_input_schema(
                    processed_data, input_shapes, input_value_ranges
                )
            
            return processed_data
            
        except Exception as e:
            self.logger.error(f"âŒ DetailedDataSpec ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return input_data
    
    def _apply_preprocessing_requirements(
        self, 
        data: Dict[str, Any], 
        requirements: List[str], 
        steps: List[str], 
        detailed_spec
    ) -> Dict[str, Any]:
        """ì „ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ì ìš©"""
        try:
            processed_data = data.copy()
            
            # ì •ê·œí™” ì„¤ì •
            normalization_mean = getattr(detailed_spec, 'normalization_mean', (0.485, 0.456, 0.406))
            normalization_std = getattr(detailed_spec, 'normalization_std', (0.229, 0.224, 0.225))
            
            for key, value in processed_data.items():
                if isinstance(value, torch.Tensor):
                    # ì´ë¯¸ì§€ í…ì„œ ì „ì²˜ë¦¬
                    if value.dim() == 4 and value.shape[1] == 3:  # [B, C, H, W]
                        if 'normalize' in requirements or 'normalize' in steps:
                            # ì •ê·œí™” ì ìš©
                            mean = torch.tensor(normalization_mean).view(1, 3, 1, 1).to(value.device)
                            std = torch.tensor(normalization_std).view(1, 3, 1, 1).to(value.device)
                            value = (value - mean) / std
                            processed_data[key] = value
                            
                        if 'resize' in requirements or any('resize' in step for step in steps):
                            # ë¦¬ì‚¬ì´ì§• (í•„ìš”ì‹œ)
                            target_size = getattr(detailed_spec, 'target_size', (512, 512))
                            if value.shape[-2:] != target_size:
                                value = F.interpolate(value, size=target_size, mode='bilinear', align_corners=False)
                                processed_data[key] = value
                
                elif isinstance(value, Image.Image):
                    # PIL ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                    if 'resize' in requirements or any('resize' in step for step in steps):
                        target_size = getattr(detailed_spec, 'target_size', (512, 512))
                        if value.size != target_size:
                            value = value.resize(target_size, Image.Resampling.LANCZOS)
                            processed_data[key] = value
            
            return processed_data
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ì ìš© ì‹¤íŒ¨: {e}")
            return data
    
    def _validate_input_schema(
        self, 
        data: Dict[str, Any], 
        input_shapes: Dict[str, Tuple], 
        input_value_ranges: Dict[str, Tuple]
    ) -> Dict[str, Any]:
        """ì…ë ¥ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë° ë³´ì •"""
        try:
            validated_data = data.copy()
            
            for key, value in validated_data.items():
                # ëª¨ì–‘ ê²€ì¦
                if key in input_shapes and isinstance(value, torch.Tensor):
                    expected_shape = input_shapes[key]
                    if value.shape[-len(expected_shape):] != expected_shape:
                        self.logger.warning(f"âš ï¸ {key} ëª¨ì–‘ ë¶ˆì¼ì¹˜: {value.shape} vs {expected_shape}")
                
                # ê°’ ë²”ìœ„ ê²€ì¦
                if key in input_value_ranges and isinstance(value, torch.Tensor):
                    min_val, max_val = input_value_ranges[key]
                    if value.min() < min_val or value.max() > max_val:
                        # ê°’ ë²”ìœ„ í´ë¦¬í•‘
                        value = torch.clamp(value, min_val, max_val)
                        validated_data[key] = value
                        self.logger.debug(f"ê°’ ë²”ìœ„ ë³´ì •: {key} â†’ [{min_val}, {max_val}]")
            
            return validated_data
            
        except Exception as e:
            self.logger.error(f"âŒ ì…ë ¥ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return data
    
    def _apply_postprocessing_requirements(
        self, 
        result: Dict[str, Any], 
        step_instance, 
        step_id: int
    ) -> Dict[str, Any]:
        """í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ì ìš©"""
        try:
            # DetailedDataSpec í›„ì²˜ë¦¬ ì •ë³´
            detailed_spec = getattr(step_instance, 'detailed_data_spec', None)
            if not detailed_spec:
                return result
            
            postprocessing_required = getattr(detailed_spec, 'postprocessing_required', [])
            postprocessing_steps = getattr(detailed_spec, 'postprocessing_steps', [])
            api_output_mapping = getattr(detailed_spec, 'api_output_mapping', {})
            
            processed_result = result.copy()
            
            # ğŸ”¥ API ì¶œë ¥ ë§¤í•‘ ì ìš©
            if api_output_mapping:
                mapped_outputs = {}
                for step_field, api_field in api_output_mapping.items():
                    if step_field in processed_result:
                        mapped_outputs[api_field] = processed_result[step_field]
                        self.logger.debug(f"ì¶œë ¥ ë§¤í•‘: {step_field} â†’ {api_field}")
                
                processed_result.update(mapped_outputs)
            
            # ğŸ”¥ í›„ì²˜ë¦¬ ë‹¨ê³„ ì ìš©
            if postprocessing_required or postprocessing_steps:
                for key, value in processed_result.items():
                    if isinstance(value, torch.Tensor):
                        # í…ì„œ í›„ì²˜ë¦¬
                        if 'denormalize' in postprocessing_required:
                            # ì •ê·œí™” í•´ì œ
                            normalization_mean = getattr(detailed_spec, 'normalization_mean', (0.485, 0.456, 0.406))
                            normalization_std = getattr(detailed_spec, 'normalization_std', (0.229, 0.224, 0.225))
                            
                            if value.dim() == 4 and value.shape[1] == 3:
                                mean = torch.tensor(normalization_mean).view(1, 3, 1, 1).to(value.device)
                                std = torch.tensor(normalization_std).view(1, 3, 1, 1).to(value.device)
                                value = value * std + mean
                                processed_result[key] = value
                        
                        if 'clamp_0_1' in postprocessing_required:
                            # 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
                            value = torch.clamp(value, 0, 1)
                            processed_result[key] = value
            
            return processed_result
            
        except Exception as e:
            self.logger.error(f"âŒ í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ì ìš© ì‹¤íŒ¨: {e}")
            return result
    
    def _apply_step_data_flow(
        self, 
        step_result: Dict[str, Any], 
        step_instance, 
        step_id: int, 
        pipeline_result: PipelineStepResult
    ) -> Dict[str, Any]:
        """Step ê°„ ë°ì´í„° íë¦„ ì²˜ë¦¬"""
        try:
            # DetailedDataSpec ë°ì´í„° íë¦„ ì •ë³´
            detailed_spec = getattr(step_instance, 'detailed_data_spec', None)
            if not detailed_spec:
                return step_result
            
            provides_to_next_step = getattr(detailed_spec, 'provides_to_next_step', {})
            step_output_schema = getattr(detailed_spec, 'step_output_schema', {})
            
            enhanced_result = step_result.copy()
            
            # ğŸ”¥ ë‹¤ìŒ Stepì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
            if provides_to_next_step:
                for next_step_name, data_mapping in provides_to_next_step.items():
                    # Step ID ì¶”ì¶œ (ì˜ˆ: "PoseEstimationStep" -> 2)
                    next_step_id = self._extract_step_id_from_name(next_step_name)
                    if next_step_id:
                        field_name = f"for_step_{next_step_id:02d}"
                        
                        # Stepë³„ ë°ì´í„° ë§¤í•‘
                        mapped_data = self._map_step_data_for_next_step(
                            step_result, step_id, next_step_id, data_mapping
                        )
                        
                        # Pipeline ê²°ê³¼ì— ì €ì¥
                        setattr(pipeline_result, field_name, mapped_data)
                        
                        self.logger.info(f"âœ… Step {step_id} â†’ Step {next_step_id} ë°ì´í„° ë§¤í•‘ ì™„ë£Œ")
            
            return enhanced_result
            
        except Exception as e:
            self.logger.error(f"âŒ Step ë°ì´í„° íë¦„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return step_result
    
    def _extract_step_id_from_name(self, step_name: str) -> Optional[int]:
        """Step ì´ë¦„ì—ì„œ ID ì¶”ì¶œ"""
        step_id_mapping = {
            'HumanParsingStep': 1,
            'PoseEstimationStep': 2,
            'ClothSegmentationStep': 3,
            'GeometricMatchingStep': 4,
            'ClothWarpingStep': 5,
            'VirtualFittingStep': 6,
            'PostProcessingStep': 7,
            'QualityAssessmentStep': 8
        }
        return step_id_mapping.get(step_name)
    
    def _map_step_data_for_next_step(
        self, 
        step_result: Dict[str, Any], 
        current_step_id: int, 
        next_step_id: int, 
        data_mapping: Dict[str, str]
    ) -> Dict[str, Any]:
        """Step ê°„ ë°ì´í„° ë§¤í•‘"""
        try:
            mapped_data = {}
            
            # Step 1 â†’ Step 2, 3, 4, 5, 6 (Human Parsing ê²°ê³¼)
            if current_step_id == 1:
                # ğŸ”¥ Step 1ì˜ ì‹¤ì œ ê²°ê³¼ êµ¬ì¡°ì— ë§ê²Œ ë§¤í•‘ - AI ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” êµ¬ì¡°
                if 'parsing_map' in step_result:
                    mapped_data['parsing_mask'] = step_result['parsing_map']
                    # Step 4, 5ì—ì„œ ê¸°ëŒ€í•˜ëŠ” êµ¬ì¡°
                    mapped_data['person_parsing'] = {
                        'parsing_map': step_result['parsing_map'],
                        'confidence': step_result.get('confidence', 0.8),
                        'result': step_result.get('result', step_result)
                    }
                if 'intermediate_results' in step_result:
                    intermediate = step_result['intermediate_results']
                    mapped_data['body_masks'] = intermediate.get('body_mask')
                    mapped_data['clothing_mask'] = intermediate.get('clothing_mask')
                    mapped_data['skin_mask'] = intermediate.get('skin_mask')
                    mapped_data['face_mask'] = intermediate.get('face_mask')
                    mapped_data['arms_mask'] = intermediate.get('arms_mask')
                    mapped_data['legs_mask'] = intermediate.get('legs_mask')
                    mapped_data['detected_body_parts'] = intermediate.get('detected_body_parts')
                    mapped_data['clothing_regions'] = intermediate.get('clothing_regions')
                if 'confidence_map' in step_result:
                    mapped_data['parsing_confidence'] = step_result['confidence_map']
                if 'detected_parts' in step_result:
                    mapped_data['detected_parts'] = step_result['detected_parts']
            
            # Step 2 â†’ Step 3, 4, 5, 6 (Pose Estimation ê²°ê³¼)
            elif current_step_id == 2:
                # ğŸ”¥ Step 2ì˜ ì‹¤ì œ ê²°ê³¼ êµ¬ì¡°ì— ë§ê²Œ ë§¤í•‘ - AI ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” êµ¬ì¡°
                if 'keypoints' in step_result:
                    mapped_data['keypoints_18'] = step_result['keypoints']  # COCO 17ê°œ + 1ê°œ = 18ê°œ
                    mapped_data['pose_keypoints'] = step_result['keypoints']  # í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
                    # Step 4, 5ì—ì„œ ê¸°ëŒ€í•˜ëŠ” êµ¬ì¡°
                    mapped_data['pose_data'] = step_result['keypoints']
                if 'intermediate_results' in step_result:
                    intermediate = step_result['intermediate_results']
                    mapped_data['keypoints_numpy'] = intermediate.get('keypoints_numpy')
                    mapped_data['confidence_scores'] = intermediate.get('confidence_scores')
                    mapped_data['joint_angles'] = intermediate.get('joint_angles_dict')
                    mapped_data['body_proportions'] = intermediate.get('body_proportions_dict')
                    mapped_data['skeleton_structure'] = intermediate.get('skeleton_structure')
                    mapped_data['landmarks'] = intermediate.get('landmarks_dict')
                    mapped_data['body_bbox'] = intermediate.get('body_bbox')
                    mapped_data['torso_bbox'] = intermediate.get('torso_bbox')
                    mapped_data['head_bbox'] = intermediate.get('head_bbox')
                    mapped_data['arms_bbox'] = intermediate.get('arms_bbox')
                    mapped_data['legs_bbox'] = intermediate.get('legs_bbox')
                    mapped_data['pose_direction'] = intermediate.get('pose_direction')
                    mapped_data['pose_stability'] = intermediate.get('pose_stability')
                    mapped_data['body_orientation'] = intermediate.get('body_orientation')
                if 'overall_confidence' in step_result:
                    mapped_data['pose_confidence'] = step_result['overall_confidence']
            
            # Step 3 â†’ Step 4, 5, 6 (Cloth Segmentation ê²°ê³¼)
            elif current_step_id == 3:
                # ğŸ”¥ Step 3ì˜ ì‹¤ì œ ê²°ê³¼ êµ¬ì¡°ì— ë§ê²Œ ë§¤í•‘ - AI ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” êµ¬ì¡°
                if 'segmentation_masks' in step_result:
                    mapped_data['segmentation_masks'] = step_result['segmentation_masks']
                    # ì£¼ìš” ë§ˆìŠ¤í¬ë“¤ ê°œë³„ ë§¤í•‘
                    masks = step_result['segmentation_masks']
                    mapped_data['all_clothes'] = masks.get('all_clothes')
                    mapped_data['upper_clothes'] = masks.get('upper_clothes')
                    mapped_data['lower_clothes'] = masks.get('lower_clothes')
                    mapped_data['dresses'] = masks.get('dresses')
                    mapped_data['accessories'] = masks.get('accessories')
                if 'cloth_mask' in step_result:
                    mapped_data['cloth_mask'] = step_result['cloth_mask']
                    # Step 4, 5ì—ì„œ ê¸°ëŒ€í•˜ëŠ” êµ¬ì¡°
                    mapped_data['clothing_segmentation'] = {
                        'cloth_mask': step_result['cloth_mask'],
                        'confidence': step_result.get('confidence', 0.8)
                    }
                if 'segmented_cloth' in step_result:
                    mapped_data['segmented_clothing'] = step_result['segmented_cloth']
                if 'confidence' in step_result:
                    mapped_data['segmentation_confidence'] = step_result['confidence']
                if 'cloth_features' in step_result:
                    mapped_data['cloth_features'] = step_result['cloth_features']
                if 'cloth_contours' in step_result:
                    mapped_data['cloth_contours'] = step_result['cloth_contours']
                if 'parsing_map' in step_result:
                    mapped_data['parsing_map'] = step_result['parsing_map']
            
            # Step 4 â†’ Step 5, 6 (Geometric Matching ê²°ê³¼)
            elif current_step_id == 4:
                # ğŸ”¥ Step 4ì˜ ì‹¤ì œ ê²°ê³¼ êµ¬ì¡°ì— ë§ê²Œ ë§¤í•‘ - AI ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” êµ¬ì¡°
                if 'matching_result' in step_result:
                    mapped_data['geometric_matching'] = step_result['matching_result']
                if 'transformation_matrix' in step_result:
                    mapped_data['transformation_matrix'] = step_result['transformation_matrix']
                    # Step 5ì—ì„œ ê¸°ëŒ€í•˜ëŠ” êµ¬ì¡°
                    mapped_data['step_4_transformation_matrix'] = step_result['transformation_matrix']
                if 'confidence' in step_result:
                    mapped_data['matching_confidence'] = step_result['confidence']
            
            # Step 5 â†’ Step 6 (Cloth Warping ê²°ê³¼)
            elif current_step_id == 5:
                if 'warped_cloth' in step_result:
                    mapped_data['warped_cloth'] = step_result['warped_cloth']
                if 'warping_grid' in step_result:
                    mapped_data['warping_grid'] = step_result['warping_grid']
                if 'confidence' in step_result:
                    mapped_data['warping_confidence'] = step_result['confidence']
            
            # ì›ë³¸ ì…ë ¥ ë°ì´í„°ë„ í¬í•¨
            if 'original_inputs' in step_result:
                mapped_data['original_inputs'] = step_result['original_inputs']
            
            # ë©”íƒ€ë°ì´í„° í¬í•¨
            if 'metadata' in step_result:
                mapped_data['metadata'] = step_result['metadata']
            
            self.logger.info(f"âœ… Step {current_step_id} â†’ Step {next_step_id} ë°ì´í„° ë§¤í•‘: {list(mapped_data.keys())}")
            return mapped_data
            
        except Exception as e:
            self.logger.error(f"âŒ Step {current_step_id} â†’ Step {next_step_id} ë°ì´í„° ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return {}
    
    # ==============================================
    # ğŸ”¥ DI Container ê¸°ë°˜ ì²˜ë¦¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    async def _preprocess_image_via_di_container(self, image_input) -> torch.Tensor:
        """DI Container ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # ğŸ”¥ DI Containerë¥¼ í†µí•œ ë°ì´í„° ë³€í™˜ê¸° í™œìš©
            data_converter = None
            if self.use_di_container:
                data_converter = get_service_safe('data_converter')
            
            if isinstance(image_input, str):
                image = Image.open(image_input).convert('RGB')
            elif isinstance(image_input, Image.Image):
                image = image_input.convert('RGB')
            elif isinstance(image_input, np.ndarray):
                image = Image.fromarray(image_input).convert('RGB')
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_input)}")
            
            # í‘œì¤€ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            if image.size != (512, 512):
                image = image.resize((512, 512), Image.Resampling.LANCZOS)
            
            # ğŸ”¥ DI Container ë°ì´í„° ë³€í™˜ê¸° ì‚¬ìš© ì‹œë„
            if data_converter and hasattr(data_converter, 'convert'):
                try:
                    convert_result = data_converter.convert(image, 'tensor')
                    if isinstance(convert_result, dict) and 'converted_data' in convert_result:
                        tensor = convert_result['converted_data']
                        if isinstance(tensor, torch.Tensor):
                            return tensor.to(self.device)
                except Exception as e:
                    self.logger.debug(f"DI Container ë°ì´í„° ë³€í™˜ê¸° ì‹¤íŒ¨, ì§ì ‘ ë³€í™˜: {e}")
            
            # ì§ì ‘ í…ì„œ ë³€í™˜
            img_array = np.array(image)
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0
            img_tensor = img_tensor.unsqueeze(0).to(self.device)
            
            return img_tensor
            
        except Exception as e:
            self.logger.error(f"âŒ DI Container ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return torch.zeros(1, 3, 512, 512, device=self.device)
    
    async def _process_step_via_implementation_manager(
        self, step_instance, step_input: Dict[str, Any], step_id: int, step_name: str
    ) -> Optional[Dict[str, Any]]:
        """RealAIStepImplementationManagerë¥¼ í†µí•œ Step ì²˜ë¦¬"""
        try:
            if not self.step_manager.step_implementation_manager:
                return None
            
            impl_manager = self.step_manager.step_implementation_manager
            
            # Step ID ê¸°ë°˜ ì²˜ë¦¬ ì‹œë„
            if hasattr(impl_manager, 'process_step_by_id'):
                result = await impl_manager.process_step_by_id(step_id, **step_input)
                if isinstance(result, dict) and result.get('success', False):
                    result['implementation_manager_used'] = True
                    return result
            
            # Step ì´ë¦„ ê¸°ë°˜ ì²˜ë¦¬ ì‹œë„
            if hasattr(impl_manager, 'process_step_by_name'):
                result = await impl_manager.process_step_by_name(step_name, step_input)
                if isinstance(result, dict) and result.get('success', False):
                    result['implementation_manager_used'] = True
                    return result
            
            return None
            
        except Exception as e:
            self.logger.debug(f"RealAIStepImplementationManager ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    async def _process_step_directly(
        self, step_instance, step_input: Dict[str, Any], step_name: str
    ) -> Dict[str, Any]:
        """Step ì§ì ‘ ì²˜ë¦¬"""
        try:
            # GitHub ì‹¤ì œ process ë©”ì„œë“œ í˜¸ì¶œ
            process_method = getattr(step_instance, 'process', None)
            if not process_method:
                raise RuntimeError(f"Step {step_name}ì— process ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # ì‹¤ì œ Step ì²˜ë¦¬ (GitHub ì‹œê·¸ë‹ˆì²˜ ë°˜ì˜)
            if asyncio.iscoroutinefunction(process_method):
                step_result = await process_method(**step_input)
            else:
                step_result = process_method(**step_input)
            
            # ê²°ê³¼ í˜•ì‹ ì •ê·œí™”
            if not isinstance(step_result, dict):
                step_result = {'success': True, 'result': step_result}
            
            step_result['direct_processing_used'] = True
            return step_result
            
        except Exception as e:
            self.logger.error(f"âŒ Step {step_name} ì§ì ‘ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'direct_processing_used': True
            }
    
    # ==============================================
    # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (DI Container ê¸°ë°˜)
    # ==============================================
    
    def _calculate_quality_score(self, step_results: Dict[str, Any]) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if not step_results:
            return 0.5
        
        scores = []
        weights = {
            'human_parsing': 0.1,
            'pose_estimation': 0.1,
            'cloth_segmentation': 0.15,
            'geometric_matching': 0.15,
            'cloth_warping': 0.15,
            'virtual_fitting': 0.25,  # ê°€ì¥ ì¤‘ìš”
            'post_processing': 0.05,
            'quality_assessment': 0.05
        }
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for step_name, result in step_results.items():
            if isinstance(result, dict) and result.get('success', False):
                weight = weights.get(step_name, 0.1)
                score = result.get('confidence', result.get('quality_score', 0.8))
                weighted_sum += weight * score
                total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.5
    
    def _get_quality_grade(self, quality_score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰"""
        if quality_score >= 0.95:
            return "Excellent+"
        elif quality_score >= 0.9:
            return "Excellent"
        elif quality_score >= 0.8:
            return "Good"
        elif quality_score >= 0.7:
            return "Fair"
        elif quality_score >= 0.6:
            return "Poor"
        else:
            return "Very Poor"
    
    def _extract_final_image(self, step_results: Dict[str, Any]) -> Optional[Image.Image]:
        """ìµœì¢… ê²°ê³¼ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        try:
            # PostProcessing ê²°ê³¼ ìš°ì„ 
            if 'post_processing' in step_results:
                post_result = step_results['post_processing']
                if 'enhanced_image' in post_result:
                    return self._tensor_to_image(post_result['enhanced_image'])
            
            # VirtualFitting ê²°ê³¼ ì°¨ì„ 
            if 'virtual_fitting' in step_results:
                fitting_result = step_results['virtual_fitting']
                if 'fitted_image' in fitting_result:
                    return self._tensor_to_image(fitting_result['fitted_image'])
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ìµœì¢… ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_final_tensor(self, step_results: Dict[str, Any]) -> Optional[torch.Tensor]:
        """ìµœì¢… ê²°ê³¼ í…ì„œ ì¶”ì¶œ"""
        try:
            if 'post_processing' in step_results:
                post_result = step_results['post_processing']
                if 'enhanced_image' in post_result:
                    result = post_result['enhanced_image']
                    if isinstance(result, torch.Tensor):
                        return result
            
            if 'virtual_fitting' in step_results:
                fitting_result = step_results['virtual_fitting']
                if 'fitted_image' in fitting_result:
                    result = fitting_result['fitted_image']
                    if isinstance(result, torch.Tensor):
                        return result
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ìµœì¢… í…ì„œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _tensor_to_image(self, tensor) -> Image.Image:
        """í…ì„œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            if isinstance(tensor, torch.Tensor):
                if tensor.dim() == 4:
                    tensor = tensor.squeeze(0)
                if tensor.shape[0] == 3:
                    tensor = tensor.permute(1, 2, 0)
                
                tensor = torch.clamp(tensor, 0, 1)
                tensor = tensor.cpu()
                array = (tensor.numpy() * 255).astype(np.uint8)
                
                return Image.fromarray(array)
            else:
                return Image.new('RGB', (512, 512), color='gray')
                
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ to ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (512, 512), color='gray')
    
    def _get_performance_metrics(self, step_results: Dict[str, Any]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {
            'total_steps': len(step_results),
            'successful_steps': len([r for r in step_results.values() if r.get('success', False)]),
            'failed_steps': len([r for r in step_results.values() if not r.get('success', True)]),
            'average_step_time': 0.0,
            'total_ai_models': 0,
            'di_container_usage': self.use_di_container,
            'implementation_manager_steps': len([r for r in step_results.values() if r.get('implementation_manager_used', False)]),
            'direct_processing_steps': len([r for r in step_results.values() if r.get('direct_processing_used', False)])
        }
        
        if step_results:
            total_time = sum(r.get('processing_time', 0.0) for r in step_results.values())
            metrics['average_step_time'] = total_time / len(step_results)
            
            # AI ëª¨ë¸ ì‚¬ìš© í†µê³„
            all_models = []
            for result in step_results.values():
                models = result.get('ai_models_used', [])
                if isinstance(models, list):
                    all_models.extend(models)
                elif isinstance(models, str):
                    all_models.append(models)
            
            metrics['total_ai_models'] = len(set(all_models))
            metrics['ai_models_list'] = list(set(all_models))
        
        return metrics
    
    def _update_performance_stats(self, success: bool, processing_time: float, quality_score: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.performance_stats['total_sessions'] += 1
        
        if success:
            self.performance_stats['successful_sessions'] += 1
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        total = self.performance_stats['total_sessions']
        prev_avg_time = self.performance_stats['average_processing_time']
        self.performance_stats['average_processing_time'] = (
            (prev_avg_time * (total - 1) + processing_time) / total
        )
        
        # í‰ê·  í’ˆì§ˆ ì ìˆ˜ ì—…ë°ì´íŠ¸ (ì„±ê³µí•œ ì„¸ì…˜ë§Œ)
        if success:
            successful = self.performance_stats['successful_sessions']
            prev_avg_quality = self.performance_stats['average_quality_score']
            self.performance_stats['average_quality_score'] = (
                (prev_avg_quality * (successful - 1) + quality_score) / successful
            )
    
    async def _optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” (DI Container ê¸°ë°˜)"""
        try:
            # ğŸ”¥ DI Container ë©”ëª¨ë¦¬ ê´€ë¦¬ì í™œìš©
            if self.use_di_container:
                memory_manager = get_service_safe('memory_manager')
                if memory_manager and hasattr(memory_manager, 'optimize_memory'):
                    memory_manager.optimize_memory(aggressive=True)
                
                # DI Container ìì²´ ìµœì í™”
                self.di_container.optimize_memory()
            
            # Python GC
            gc.collect()
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
            
            self.logger.info("ğŸ’¾ DI Container ê¸°ë°˜ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ê¸°ë°˜ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ê³ ê¸‰ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ ë©”ì„œë“œë“¤ (ë¹ ì§„ ê¸°ëŠ¥ ì¶”ê°€)
    # ==============================================
    
    def get_step_api_specification(self, step_id: int) -> Dict[str, Any]:
        """Step API ëª…ì„¸ ì¡°íšŒ"""
        try:
            step_info = self.step_manager.step_mapping.get(step_id, {})
            step_name = step_info.get('name', f'step_{step_id}')
            step_instance = self.step_manager.get_step_by_id(step_id)
            
            if not step_instance:
                return {'error': f'Step {step_id} ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ'}
            
            detailed_spec = getattr(step_instance, 'detailed_data_spec', None)
            if not detailed_spec:
                return {'error': f'Step {step_id} DetailedDataSpec ì—†ìŒ'}
            
            return {
                'step_id': step_id,
                'step_name': step_name,
                'api_input_mapping': getattr(detailed_spec, 'api_input_mapping', {}),
                'api_output_mapping': getattr(detailed_spec, 'api_output_mapping', {}),
                'input_shapes': getattr(detailed_spec, 'input_shapes', {}),
                'output_shapes': getattr(detailed_spec, 'output_shapes', {}),
                'preprocessing_required': getattr(detailed_spec, 'preprocessing_required', []),
                'postprocessing_required': getattr(detailed_spec, 'postprocessing_required', []),
                'data_flow': {
                    'accepts_from': getattr(detailed_spec, 'accepts_from_previous_step', {}),
                    'provides_to': getattr(detailed_spec, 'provides_to_next_step', {})
                }
            }
            
        except Exception as e:
            return {'error': str(e)}
    
    def validate_step_input_data(self, step_id: int, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Step ì…ë ¥ ë°ì´í„° ê²€ì¦"""
        try:
            spec = self.get_step_api_specification(step_id)
            if 'error' in spec:
                return {'valid': False, 'error': spec['error']}
            
            validation_result = {
                'valid': True,
                'issues': [],
                'warnings': []
            }
            
            # API ì…ë ¥ ë§¤í•‘ ê²€ì¦
            api_input_mapping = spec.get('api_input_mapping', {})
            if api_input_mapping:
                for api_field in api_input_mapping.keys():
                    if api_field not in input_data:
                        validation_result['warnings'].append(f'API í•„ë“œ ëˆ„ë½: {api_field}')
            
            # ì…ë ¥ ëª¨ì–‘ ê²€ì¦
            input_shapes = spec.get('input_shapes', {})
            for field, expected_shape in input_shapes.items():
                if field in input_data:
                    value = input_data[field]
                    if isinstance(value, torch.Tensor):
                        if value.shape[-len(expected_shape):] != expected_shape:
                            validation_result['issues'].append(
                                f'{field} ëª¨ì–‘ ë¶ˆì¼ì¹˜: {value.shape} vs {expected_shape}'
                            )
            
            validation_result['valid'] = len(validation_result['issues']) == 0
            return validation_result
            
        except Exception as e:
            return {'valid': False, 'error': str(e)}
    
    def get_pipeline_data_flow_analysis(self) -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ë°ì´í„° íë¦„ ë¶„ì„"""
        try:
            data_flow_analysis = {
                'steps': {},
                'connections': [],
                'data_dependencies': {},
                'validation': {'valid': True, 'issues': []}
            }
            
            for step_id in range(1, 9):
                spec = self.get_step_api_specification(step_id)
                if 'error' not in spec:
                    step_info = self.step_manager.step_mapping.get(step_id, {})
                    
                    data_flow_analysis['steps'][step_id] = {
                        'name': step_info.get('name', f'step_{step_id}'),
                        'inputs': list(spec.get('api_input_mapping', {}).keys()),
                        'outputs': list(spec.get('api_output_mapping', {}).keys()),
                        'accepts_from': spec.get('data_flow', {}).get('accepts_from', {}),
                        'provides_to': spec.get('data_flow', {}).get('provides_to', {})
                    }
                    
                    # ì—°ê²° ê´€ê³„ ë¶„ì„
                    provides_to = spec.get('data_flow', {}).get('provides_to', {})
                    for next_step_id in provides_to.keys():
                        if isinstance(next_step_id, int):
                            data_flow_analysis['connections'].append({
                                'from': step_id,
                                'to': next_step_id,
                                'data_mapping': provides_to[next_step_id]
                            })
            
            return data_flow_analysis
            
        except Exception as e:
            return {'error': str(e)}
    
    def get_step_performance_metrics(self) -> Dict[str, Any]:
        """Stepë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            metrics = {
                'overall': self.performance_stats.copy(),
                'step_specific': {},
                'di_container_metrics': {}
            }
            
            # Stepë³„ ê°œë³„ ë©”íŠ¸ë¦­
            for step_name, step_instance in self.step_manager.steps.items():
                if hasattr(step_instance, 'performance_metrics'):
                    step_metrics = getattr(step_instance, 'performance_metrics')
                    if hasattr(step_metrics, 'get_stats'):
                        metrics['step_specific'][step_name] = step_metrics.get_stats()
                    else:
                        metrics['step_specific'][step_name] = {
                            'available': False,
                            'reason': 'No performance metrics available'
                        }
            
            # DI Container ë©”íŠ¸ë¦­
            if self.use_di_container and self.di_container:
                try:
                    metrics['di_container_metrics'] = self.di_container.get_stats()
                except Exception as e:
                    metrics['di_container_metrics'] = {'error': str(e)}
            
            return metrics
            
        except Exception as e:
            return {'error': str(e)}
    
    def optimize_pipeline_performance(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ìµœì í™”"""
        try:
            optimization_results = {
                'memory_optimization': False,
                'di_container_optimization': False,
                'step_optimization': {},
                'overall_improvement': 0.0
            }
            
            start_time = time.time()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            try:
                self._optimize_memory()
                optimization_results['memory_optimization'] = True
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            
            # DI Container ìµœì í™”
            if self.use_di_container and self.di_container:
                try:
                    optimization_stats = self.di_container.optimize_memory()
                    optimization_results['di_container_optimization'] = True
                    optimization_results['di_optimization_stats'] = optimization_stats
                except Exception as e:
                    self.logger.warning(f"âš ï¸ DI Container ìµœì í™” ì‹¤íŒ¨: {e}")
            
            # Stepë³„ ìµœì í™”
            for step_name, step_instance in self.step_manager.steps.items():
                try:
                    if hasattr(step_instance, 'optimize_performance'):
                        step_optimization = step_instance.optimize_performance()
                        optimization_results['step_optimization'][step_name] = step_optimization
                    else:
                        optimization_results['step_optimization'][step_name] = {'skipped': 'No optimization method'}
                except Exception as e:
                    optimization_results['step_optimization'][step_name] = {'error': str(e)}
            
            optimization_time = time.time() - start_time
            optimization_results['optimization_time'] = optimization_time
            optimization_results['timestamp'] = datetime.now().isoformat()
            
            return optimization_results
            
        except Exception as e:
            return {'error': str(e)}
    
    def generate_pipeline_report(self) -> Dict[str, Any]:
        """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ë³´ê³ ì„œ ìƒì„±"""
        try:
            report = {
                'generated_at': datetime.now().isoformat(),
                'pipeline_version': 'v12.0_complete_di_container_integration',
                'system_info': {
                    'device': self.device,
                    'is_m3_max': self.config.is_m3_max,
                    'memory_gb': self.config.memory_gb,
                    'conda_env': CONDA_ENV,
                    'di_container_enabled': self.use_di_container
                },
                'configuration': {
                    'quality_level': self.config.quality_level.value,
                    'processing_mode': self.config.processing_mode.value,
                    'ai_model_enabled': self.config.ai_model_enabled,
                    'batch_size': self.config.batch_size
                },
                'step_status': {},
                'performance_metrics': self.get_step_performance_metrics(),
                'data_flow_analysis': self.get_pipeline_data_flow_analysis(),
                'api_specifications': {}
            }
            
            # Step ìƒíƒœ ë° API ëª…ì„¸
            for step_id in range(1, 9):
                step_info = self.step_manager.step_mapping.get(step_id, {})
                step_name = step_info.get('name', f'step_{step_id}')
                step_instance = self.step_manager.get_step_by_id(step_id)
                
                report['step_status'][step_name] = {
                    'registered': step_instance is not None,
                    'initialized': getattr(step_instance, 'is_initialized', False) if step_instance else False,
                    'ready': getattr(step_instance, 'is_ready', False) if step_instance else False,
                    'has_detailed_spec': hasattr(step_instance, 'detailed_data_spec') if step_instance else False,
                    'di_injected': getattr(step_instance, 'model_loader', None) is not None if step_instance else False
                }
                
                report['api_specifications'][step_name] = self.get_step_api_specification(step_id)
            
            # ì „ì²´ ìƒíƒœ ìš”ì•½
            total_steps = len(self.step_manager.step_mapping)
            registered_steps = len([s for s in report['step_status'].values() if s['registered']])
            initialized_steps = len([s for s in report['step_status'].values() if s['initialized']])
            
            report['summary'] = {
                'total_steps': total_steps,
                'registered_steps': registered_steps,
                'initialized_steps': initialized_steps,
                'registration_rate': (registered_steps / total_steps) * 100,
                'initialization_rate': (initialized_steps / total_steps) * 100,
                'overall_health': 'Good' if initialized_steps >= total_steps * 0.8 else 'Warning' if initialized_steps >= total_steps * 0.5 else 'Poor'
            }
            
            return report
            
        except Exception as e:
            return {
                'error': str(e),
                'generated_at': datetime.now().isoformat(),
                'pipeline_version': 'v12.0_complete_di_container_integration'
            }

    # ==============================================
    # ğŸ”¥ Step ê´€ë¦¬ ë©”ì„œë“œë“¤ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ 100% ìœ ì§€)
    # ==============================================
    
    def register_step(self, step_id: int, step_instance: Any) -> bool:
        """Step ë“±ë¡ (DI Container ê¸°ë°˜)"""
        try:
            step_info = self.step_manager.step_mapping.get(step_id)
            if not step_info:
                self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” Step ID: {step_id}")
                return False
            
            step_name = step_info['name']
            
            # ğŸ”¥ DI Container ê¸°ë°˜ ê¸€ë¡œë²Œ í˜¸í™˜ì„± ë³´ì¥
            if DI_CONTAINER_AVAILABLE:
                config = {
                    'device': self.device,
                    'is_m3_max': self.config.is_m3_max,
                    'memory_gb': self.config.memory_gb,
                    'device_type': self.config.device_type,
                    'ai_model_enabled': self.config.ai_model_enabled,
                    'quality_level': self.config.quality_level.value if hasattr(self.config.quality_level, 'value') else self.config.quality_level,
                    'performance_mode': 'maximum' if self.config.is_m3_max else 'balanced'
                }
                
                ensure_global_step_compatibility(step_instance, step_id, step_name, config)
            
            # ğŸ”¥ DI Container ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì…
            if self.use_di_container:
                inject_dependencies_to_step_safe(step_instance, self.di_container)
            
            # ì´ˆê¸°í™” (ë™ê¸° ë°©ì‹)
            try:
                if hasattr(step_instance, 'initialize'):
                    initialize_method = getattr(step_instance, 'initialize')
                    
                    if asyncio.iscoroutinefunction(initialize_method):
                        # ë¹„ë™ê¸° ë©”ì„œë“œëŠ” ë§ˆí‚¹ë§Œ í•˜ê³  ì¦‰ì‹œ ì™„ë£Œë¡œ ì²˜ë¦¬
                        step_instance._needs_async_init = True
                        step_instance.is_initialized = True
                        step_instance.is_ready = True
                    else:
                        # ë™ê¸° ë©”ì„œë“œëŠ” ì¦‰ì‹œ ì‹¤í–‰
                        try:
                            result = initialize_method()
                            if result is None or result is True or result:
                                step_instance.is_initialized = True
                                step_instance.is_ready = True
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ {step_instance.__class__.__name__} ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                            step_instance.is_initialized = True
                            step_instance.is_ready = True
                else:
                    step_instance.is_initialized = True
                    step_instance.is_ready = True
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ Step {step_id} ì´ˆê¸°í™” ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                step_instance.is_initialized = True
                step_instance.is_ready = True
            
            # Step ë“±ë¡
            self.step_manager.steps[step_name] = step_instance
            self.logger.info(f"âœ… Step {step_id} ({step_name}) DI Container ê¸°ë°˜ ë“±ë¡ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Step {step_id} DI Container ê¸°ë°˜ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def register_steps_batch(self, steps_dict: Dict[int, Any]) -> Dict[int, bool]:
        """Step ì¼ê´„ ë“±ë¡ (DI Container ê¸°ë°˜)"""
        results = {}
        for step_id, step_instance in steps_dict.items():
            results[step_id] = self.register_step(step_id, step_instance)
        return results
    
    def unregister_step(self, step_id: int) -> bool:
        """Step ë“±ë¡ í•´ì œ (DI Container ê¸°ë°˜)"""
        try:
            step_info = self.step_manager.step_mapping.get(step_id)
            if not step_info:
                return False
            
            step_name = step_info['name']
            if step_name in self.step_manager.steps:
                step_instance = self.step_manager.steps[step_name]
                
                # ì •ë¦¬ ì‘ì—…
                if hasattr(step_instance, 'cleanup'):
                    try:
                        if asyncio.iscoroutinefunction(step_instance.cleanup):
                            asyncio.create_task(step_instance.cleanup())
                        else:
                            step_instance.cleanup()
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Step {step_id} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                
                del self.step_manager.steps[step_name]
                self.logger.info(f"âœ… Step {step_id} ({step_name}) DI Container ê¸°ë°˜ ë“±ë¡ í•´ì œ ì™„ë£Œ")
                return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ Step {step_id} DI Container ê¸°ë°˜ ë“±ë¡ í•´ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_registered_steps(self) -> Dict[str, Any]:
        """ë“±ë¡ëœ Step ëª©ë¡ ë°˜í™˜"""
        registered_steps = {}
        for step_id, step_info in self.step_manager.step_mapping.items():
            step_name = step_info['name']
            step_instance = self.step_manager.steps.get(step_name)
            
            registered_steps[step_name] = {
                'step_id': step_id,
                'step_name': step_name,
                'class_name': step_info['class_name'],
                'registered': step_instance is not None,
                'has_process_method': hasattr(step_instance, 'process') if step_instance else False,
                'is_initialized': getattr(step_instance, 'is_initialized', False) if step_instance else False,
                'is_ready': getattr(step_instance, 'is_ready', False) if step_instance else False,
                'di_container_injected': getattr(step_instance, 'model_loader', None) is not None if step_instance else False
            }
        
        total_registered = len([s for s in registered_steps.values() if s['registered']])
        missing_steps = [name for name, info in registered_steps.items() if not info['registered']]
        
        return {
            'total_registered': total_registered,
            'total_expected': len(self.step_manager.step_mapping),
            'registration_rate': (total_registered / len(self.step_manager.step_mapping)) * 100,
            'registered_steps': registered_steps,
            'missing_steps': missing_steps,
            'di_container_enabled': self.use_di_container
        }
    
    def is_step_registered(self, step_id: int) -> bool:
        """Step ë“±ë¡ ì—¬ë¶€ í™•ì¸"""
        step_info = self.step_manager.step_mapping.get(step_id)
        if not step_info:
            return False
        
        step_name = step_info['name']
        return step_name in self.step_manager.steps
    
    def get_step_by_id(self, step_id: int) -> Optional[Any]:
        """Step IDë¡œ Step ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        step_info = self.step_manager.step_mapping.get(step_id)
        if not step_info:
            return None
        
        step_name = step_info['name']
        return self.step_manager.steps.get(step_name)
    
    def update_config(self, new_config: Dict[str, Any]) -> bool:
        """íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ (DI Container ê¸°ë°˜)"""
        try:
            self.logger.info("ğŸ”„ DI Container ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹œì‘...")
            
            # ê¸°ë³¸ ì„¤ì • ì—…ë°ì´íŠ¸
            if 'device' in new_config and new_config['device'] != self.device:
                self.device = new_config['device']
                # DI Containerì—ë„ ì—…ë°ì´íŠ¸
                if self.use_di_container:
                    register_service_safe('device', self.device)
                self.logger.info(f"âœ… ë””ë°”ì´ìŠ¤ ë³€ê²½: {self.device}")
            
            # PipelineConfig ì—…ë°ì´íŠ¸
            for key, value in new_config.items():
                if hasattr(self.config, key):
                    setattr(self.config, key, value)
                    # DI Containerì—ë„ ë°˜ì˜
                    if self.use_di_container and key in ['is_m3_max', 'memory_gb']:
                        register_service_safe(key, value)
            
            self.logger.info("âœ… DI Container ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ DI Container ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ ìƒíƒœ ì¡°íšŒ ë©”ì„œë“œë“¤ (DI Container ì •ë³´ í¬í•¨)
    # ==============================================
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ (DI Container ì •ë³´ í¬í•¨)"""
        registered_steps = self.get_registered_steps()
        
        # DI Container ìƒíƒœ ì •ë³´
        di_container_status = {}
        if self.use_di_container and self.di_container:
            try:
                di_container_status = self.di_container.get_stats()
            except Exception as e:
                di_container_status = {'error': str(e)}
        
        return {
            'initialized': self.is_initialized,
            'current_status': self.current_status.value,
            'device': self.device,
            'device_type': self.config.device_type,
            'memory_gb': self.config.memory_gb,
            'is_m3_max': self.config.is_m3_max,
            'ai_model_enabled': self.config.ai_model_enabled,
            'architecture_version': 'v12.0_complete_di_container_integration',
            
            # DI Container ìƒíƒœ
            'di_container': {
                'enabled': self.use_di_container,
                'available': DI_CONTAINER_AVAILABLE,
                'version': '4.0',
                'circular_reference_free': self.config.use_circular_reference_free_di,
                'lazy_resolution': self.config.enable_lazy_dependency_resolution,
                'status': di_container_status
            },
            
            'step_manager': {
                'type': 'DIContainerStepManager',
                'total_registered': registered_steps['total_registered'],
                'total_expected': registered_steps['total_expected'],
                'registration_rate': registered_steps['registration_rate'],
                'missing_steps': registered_steps['missing_steps'],
                'github_structure': True,
                'step_implementation_manager': self.step_manager.step_implementation_manager is not None
            },
            
            'config': {
                'quality_level': self.config.quality_level.value,
                'processing_mode': self.config.processing_mode.value,
                'use_dependency_injection': self.config.use_dependency_injection,
                'enable_adapter_pattern': self.config.enable_adapter_pattern,
                'batch_size': self.config.batch_size,
                'thread_pool_size': self.config.thread_pool_size
            },
            
            'performance_stats': self.performance_stats,
            
            'ai_model_paths': {
                step_name: len(models) 
                for step_name, models in self.step_manager.ai_model_paths.items()
            },
            
            'data_flow_engine': {
                'engine_type': 'DIContainerDataFlowEngine',
                'flow_rules_count': len(self.data_flow_engine.data_flow_rules),
                'supports_pipeline_data': True,
                'di_container_integrated': self.data_flow_engine.use_di_container
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (DI Container ê¸°ë°˜)"""
        try:
            self.logger.info("ğŸ§¹ PipelineManager v12.0 DI Container ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            self.current_status = PipelineStatus.CLEANING
            
            # Step ì •ë¦¬
            for step_name, step in self.step_manager.steps.items():
                try:
                    if hasattr(step, 'cleanup'):
                        if asyncio.iscoroutinefunction(step.cleanup):
                            await step.cleanup()
                        else:
                            step.cleanup()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ğŸ”¥ DI Container ì •ë¦¬
            if self.use_di_container and self.di_container:
                try:
                    self.di_container.optimize_memory()
                    self.di_container.cleanup_circular_references()
                    self.logger.info("âœ… DI Container ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ DI Container ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            await self._optimize_memory()
            
            # ìƒíƒœ ì´ˆê¸°í™”
            self.is_initialized = False
            self.current_status = PipelineStatus.IDLE
            
            self.logger.info("âœ… PipelineManager v12.0 DI Container ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ DI Container ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.current_status = PipelineStatus.FAILED

# ==============================================
# ğŸ”¥ DIBasedPipelineManager í´ë˜ìŠ¤ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ 100% ìœ ì§€)
# ==============================================

class DIBasedPipelineManager(PipelineManager):
    """DI ì „ìš© PipelineManager (DI Container v4.0 ê°•ì œ í™œì„±í™”)"""
    
    def __init__(
        self,
        config_path: Optional[str] = None,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], PipelineConfig]] = None,
        **kwargs
    ):
        # DI Container ê´€ë ¨ ì„¤ì • ê°•ì œ í™œì„±í™”
        di_config = {
            'use_dependency_injection': True,
            'auto_inject_dependencies': True,
            'enable_adapter_pattern': True,
            'use_circular_reference_free_di': True,
            'enable_lazy_dependency_resolution': True
        }
        
        if isinstance(config, dict):
            config.update(di_config)
        elif isinstance(config, PipelineConfig):
            for key, value in di_config.items():
                setattr(config, key, value)
        else:
            kwargs.update(di_config)
        
            # âœ… ì¤‘ë³µ í‚¤ ì œê±°
        if "device" in kwargs and device is not None:
            kwargs.pop("device")
        if "config" in kwargs and config is not None:
            kwargs.pop("config")

        # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        super().__init__(config_path=config_path, device=device, config=config, **kwargs)

        self.logger.info("ğŸ”¥ DIBasedPipelineManager v12.0 ì´ˆê¸°í™” ì™„ë£Œ (DI Container v4.0 ê°•ì œ í™œì„±í™”)")
    
    def get_di_status(self) -> Dict[str, Any]:
        """DI ì „ìš© ìƒíƒœ ì¡°íšŒ"""
        base_status = self.get_pipeline_status()
        
        return {
            **base_status,
            'di_based_manager': True,
            'di_forced_enabled': True,
            'github_structure_reflection': True,
            'di_container_version': '4.0',
            'circular_reference_free': True,
            'di_specific_info': {
                'step_manager_type': type(self.step_manager).__name__,
                'data_flow_engine_type': type(self.data_flow_engine).__name__,
                'di_container_type': type(self.di_container).__name__ if self.di_container else None
            }
        }

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í•¨ìˆ˜ëª… 100% ìœ ì§€ + DI Container í™œì„±í™”)
# ==============================================

def create_pipeline(
    device: str = "auto", 
    quality_level: str = "balanced", 
    mode: str = "production",
    use_dependency_injection: bool = True,
    enable_adapter_pattern: bool = True,
    **kwargs
) -> PipelineManager:
    """ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ (DI Container ê¸°ë³¸ í™œì„±í™”)"""
    return PipelineManager(
        device=device,
        config=PipelineConfig(
            quality_level=QualityLevel(quality_level),
            processing_mode=ProcessingMode(mode),
            ai_model_enabled=True,
            use_dependency_injection=use_dependency_injection,
            enable_adapter_pattern=enable_adapter_pattern,
            use_circular_reference_free_di=True,
            enable_lazy_dependency_resolution=True,
            **kwargs
        )
    )

def create_complete_di_pipeline(
    device: str = "auto",
    quality_level: str = "high",
    **kwargs
) -> PipelineManager:
    """ì™„ì „ DI Container íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    return PipelineManager(
        device=device,
        config=PipelineConfig(
            quality_level=QualityLevel(quality_level),
            processing_mode=ProcessingMode.PRODUCTION,
            ai_model_enabled=True,
            model_preload_enabled=True,
            use_dependency_injection=True,
            enable_adapter_pattern=True,
            use_circular_reference_free_di=True,
            enable_lazy_dependency_resolution=True,
            **kwargs
        )
    )

def create_m3_max_pipeline(**kwargs) -> PipelineManager:
    """M3 Max ìµœì í™” íŒŒì´í”„ë¼ì¸ (DI Container ìµœì í™”)"""
    return PipelineManager(
        device="mps",
        config=PipelineConfig(
            quality_level=QualityLevel.MAXIMUM,
            processing_mode=ProcessingMode.PRODUCTION,
            memory_gb=128.0,
            is_m3_max=True,
            device_type="apple_silicon",
            ai_model_enabled=True,
            model_preload_enabled=True,
            use_dependency_injection=True,
            enable_adapter_pattern=True,
            use_circular_reference_free_di=True,
            enable_lazy_dependency_resolution=True,
            **kwargs
        )
    )

def create_production_pipeline(**kwargs) -> PipelineManager:
    """í”„ë¡œë•ì…˜ íŒŒì´í”„ë¼ì¸ (DI Container ì™„ì „ í™œì„±í™”)"""
    return create_complete_di_pipeline(
        quality_level="high",
        processing_mode="production",
        ai_model_enabled=True,
        model_preload_enabled=True,
        **kwargs
    )

def create_development_pipeline(**kwargs) -> PipelineManager:
    """ê°œë°œìš© íŒŒì´í”„ë¼ì¸ (DI Container í™œì„±í™”)"""
    return create_complete_di_pipeline(
        quality_level="balanced",
        processing_mode="development",
        ai_model_enabled=True,
        model_preload_enabled=False,
        **kwargs
    )

def create_testing_pipeline(**kwargs) -> PipelineManager:
    """í…ŒìŠ¤íŒ…ìš© íŒŒì´í”„ë¼ì¸ (DI Container ê¸°ë³¸ í™œì„±í™”)"""
    return PipelineManager(
        device="cpu",
        config=PipelineConfig(
            quality_level=QualityLevel.FAST,
            processing_mode=ProcessingMode.TESTING,
            ai_model_enabled=False,
            model_preload_enabled=False,
            use_dependency_injection=True,
            enable_adapter_pattern=True,
            use_circular_reference_free_di=True,
            enable_lazy_dependency_resolution=True,
            **kwargs
        )
    )

def create_di_based_pipeline(**kwargs) -> DIBasedPipelineManager:
    """DIBasedPipelineManager ìƒì„± (DI Container v4.0 ê°•ì œ)"""
    return DIBasedPipelineManager(**kwargs)

@lru_cache(maxsize=1)
def get_global_pipeline_manager(device: str = "auto") -> PipelineManager:
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € (DI Container í™œì„±í™”)"""
    try:
        if device == "mps" and torch.backends.mps.is_available():
            return create_m3_max_pipeline()
        else:
            return create_production_pipeline(device=device)
    except Exception as e:
        logger.error(f"ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ìƒì„± ì‹¤íŒ¨: {e}")
        return create_complete_di_pipeline(device="cpu", quality_level="balanced")

@lru_cache(maxsize=1)
def get_global_di_based_pipeline_manager(device: str = "auto") -> DIBasedPipelineManager:
    """ì „ì—­ DIBasedPipelineManager (DI Container v4.0 ê°•ì œ)"""
    try:
        if device == "mps" and torch.backends.mps.is_available():
            return DIBasedPipelineManager(
                device="mps",
                config=PipelineConfig(
                    quality_level=QualityLevel.MAXIMUM,
                    processing_mode=ProcessingMode.PRODUCTION,
                    memory_gb=128.0,
                    is_m3_max=True,
                    device_type="apple_silicon"
                )
            )
        else:
            return DIBasedPipelineManager(device=device)
    except Exception as e:
        logger.error(f"ì „ì—­ DIBasedPipelineManager ìƒì„± ì‹¤íŒ¨: {e}")
        return DIBasedPipelineManager(device="cpu")

# ==============================================
# ğŸ”¥ Export ë° ë©”ì¸ ì‹¤í–‰
# ==============================================

__all__ = [
    # ì—´ê±°í˜•
    'PipelineStatus', 'QualityLevel', 'ProcessingMode', 'PipelineMode',
    
    # ë°ì´í„° í´ë˜ìŠ¤
    'PipelineConfig', 'PipelineStepResult', 'ProcessingResult',
    
    # DI Container ê¸°ë°˜ ê´€ë¦¬ì í´ë˜ìŠ¤ë“¤
    'DIContainerStepManager', 'DIContainerDataFlowEngine',
    
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ ì´ë¦„ 100% ìœ ì§€)
    'PipelineManager',
    'DIBasedPipelineManager',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì´ë¦„ 100% ìœ ì§€)
    'create_pipeline',
    'create_complete_di_pipeline',
    'create_m3_max_pipeline',
    'create_production_pipeline',
    'create_development_pipeline',
    'create_testing_pipeline',
    'create_di_based_pipeline',
    'get_global_pipeline_manager',
    'get_global_di_based_pipeline_manager'
]

# ==============================================
# ğŸ”¥ ì´ˆê¸°í™” ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

logger.info("ğŸ‰ ì™„ì „í•œ DI Container í†µí•© PipelineManager v12.0 ë¡œë“œ ì™„ë£Œ!")
logger.info("âœ… DI Container v4.0 ì™„ì „ í†µí•©:")
logger.info("   - CircularReferenceFreeDIContainer ì™„ì „ í†µí•©")
logger.info("   - ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ (StepFactory â†” BaseStepMixin)")
logger.info("   - ì§€ì—° ì˜ì¡´ì„± í•´ê²° (Lazy Dependency Resolution)")
logger.info("   - ë™ì  Import í•´ê²°ê¸° (Dynamic Import Resolver)")
logger.info("   - Mock í´ë°± êµ¬í˜„ì²´ í¬í•¨")

logger.info("âœ… GitHub êµ¬ì¡° 100% ë°˜ì˜:")
logger.info("   - ì‹¤ì œ Step íŒŒì¼ process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ì •í™• ë§¤í•‘")
logger.info("   - PipelineStepResult ì™„ì „í•œ ë°ì´í„° êµ¬ì¡° êµ¬í˜„")
logger.info("   - DIContainerStepManager - DI Container ê¸°ë°˜ Step ê´€ë¦¬")
logger.info("   - DIContainerDataFlowEngine - DI Container ê¸°ë°˜ ë°ì´í„° íë¦„")
logger.info("   - ì‹¤ì œ AI ëª¨ë¸ 229GB ê²½ë¡œ ë§¤í•‘")
logger.info("   - RealAIStepImplementationManager v14.0 ì™„ì „ ì—°ë™")

logger.info("âœ… ì™„ì „ í•´ê²°ëœ í•µì‹¬ ë¬¸ì œë“¤:")
logger.info("   - ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ âœ…")
logger.info("   - ì˜ì¡´ì„± ì£¼ì… ì•ˆì „ì„± ë³´ì¥ âœ…")
logger.info("   - Step íŒŒì¼ ìˆ˜ì • ì—†ì´ GitHub ì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš© âœ…")
logger.info("   - BaseStepMixin v19.3 DI Container ì™„ì „ í†µí•© âœ…")
logger.info("   - StepFactory v11.0 ì™„ì „ ì—°ë™ âœ…")
logger.info("   - DetailedDataSpec ê¸°ë°˜ API â†” Step ìë™ ë³€í™˜ âœ…")

logger.info("ğŸ”— DI Container v4.0 ì£¼ìš” ê¸°ëŠ¥:")
logger.info("   - ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ ì‹œìŠ¤í…œ")
logger.info("   - ì§€ì—° ì˜ì¡´ì„± í•´ê²° (LazyDependency)")
logger.info("   - ë™ì  Import í•´ê²°ê¸° (DynamicImportResolver)")
logger.info("   - ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë° ë©”ëª¨ë¦¬ ë³´í˜¸")
logger.info("   - Mock í´ë°± êµ¬í˜„ì²´ ìë™ ìƒì„±")
logger.info("   - M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")

logger.info("ğŸ›¡ï¸ ì•ˆì „ì„± ë³´ì¥:")
logger.info("   - ëª¨ë“  ì˜ì¡´ì„±ì„ DI Containerë¥¼ í†µí•´ ê´€ë¦¬")
logger.info("   - ìˆœí™˜ì°¸ì¡° ê°ì§€ ë° ìë™ ì°¨ë‹¨")
logger.info("   - ì•½í•œ ì°¸ì¡°(Weak Reference) ë©”ëª¨ë¦¬ ë³´í˜¸")
logger.info("   - ì˜ˆì™¸ ë°œìƒ ì‹œ ì•ˆì „í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜")

logger.info("ğŸ¯ ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ 100% í˜¸í™˜:")
logger.info("   - ëª¨ë“  í•¨ìˆ˜ëª…/í´ë˜ìŠ¤ëª… ì™„ì „ ë³´ì¡´")
logger.info("   - ê¸°ì¡´ ì‚¬ìš©ë²• ê·¸ëŒ€ë¡œ ìœ ì§€")
logger.info("   - DI Container ê¸°ëŠ¥ ìë™ í™œì„±í™”")
logger.info("   - GitHub Step íŒŒì¼ë“¤ê³¼ ì™„ë²½ ì—°ë™")

# conda í™˜ê²½ ìë™ ìµœì í™”
if IS_CONDA and DI_CONTAINER_AVAILABLE:
    try:
        initialize_di_system_safe()
        logger.info("ğŸ conda í™˜ê²½ì—ì„œ DI Container ìë™ ì´ˆê¸°í™” ì™„ë£Œ!")
    except Exception as e:
        logger.warning(f"âš ï¸ conda í™˜ê²½ DI Container ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™”
gc.collect()
if TORCH_AVAILABLE and MPS_AVAILABLE:
    try:
        if hasattr(torch.backends.mps, 'empty_cache'):
            torch.backends.mps.empty_cache()
    except:
        pass

logger.info(f"ğŸ’¾ ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {'mps' if MPS_AVAILABLE else 'cpu'})")
logger.info(f"ğŸ¤– ì´ AI ëª¨ë¸ ê²½ë¡œ: 8ê°œ Step ì¹´í…Œê³ ë¦¬ (ì‹¤ì œ 229GB í™œìš©)")

logger.info("=" * 80)
logger.info("ğŸš€ COMPLETE DI CONTAINER INTEGRATED PIPELINE MANAGER v12.0 READY! ğŸš€")
logger.info("=" * 80)

# ==============================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ ë° ë°ëª¨
# ==============================================

if __name__ == "__main__":
    print("ğŸ”¥ ì™„ì „í•œ DI Container í†µí•© PipelineManager v12.0")
    print("=" * 80)
    print("âœ… DI Container v4.0 ì™„ì „ í†µí•©")
    print("âœ… GitHub Step íŒŒì¼ êµ¬ì¡° 100% ë°˜ì˜")
    print("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
    print("âœ… ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ 100% í˜¸í™˜")
    print("=" * 80)
    
    import asyncio
    
    async def demo_di_container_integration():
        """DI Container í†µí•© ë°ëª¨"""
        print("ğŸ¯ DI Container v4.0 í†µí•© PipelineManager ë°ëª¨ ì‹œì‘")
        print("-" * 60)
        
        # 1. DI Container ê°€ìš©ì„± í™•ì¸
        print("1ï¸âƒ£ DI Container v4.0 ê°€ìš©ì„± í™•ì¸...")
        print(f"âœ… DI Container ì‚¬ìš© ê°€ëŠ¥: {'ì˜ˆ' if DI_CONTAINER_AVAILABLE else 'ì•„ë‹ˆì˜¤'}")
        
        if DI_CONTAINER_AVAILABLE:
            # DI Container ìƒíƒœ í™•ì¸
            container = get_global_container()
            stats = container.get_stats()
            print(f"ğŸ“Š DI Container íƒ€ì…: {stats['container_type']}")
            print(f"ğŸ”— DI Container ë²„ì „: {stats['version']}")
            print(f"ğŸ›¡ï¸ ìˆœí™˜ì°¸ì¡° ë°©ì§€: í™œì„±í™”")
        
        # 2. ëª¨ë“  íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ (DI Container ê¸°ë°˜)
        print("2ï¸âƒ£ DI Container ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸...")
        
        try:
            pipelines = {
                'basic': create_pipeline(),
                'complete_di': create_complete_di_pipeline(),
                'm3_max': create_m3_max_pipeline(),
                'production': create_production_pipeline(),
                'development': create_development_pipeline(),
                'testing': create_testing_pipeline(),
                'di_based': create_di_based_pipeline(),
                'global': get_global_pipeline_manager(),
                'global_di': get_global_di_based_pipeline_manager()
            }
            
            for name, pipeline in pipelines.items():
                di_enabled = getattr(pipeline.config, 'use_dependency_injection', False)
                print(f"âœ… {name}: {type(pipeline).__name__} (DI: {'âœ…' if di_enabled else 'âŒ'})")
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return
        
        # 3. DI Container í†µí•© ì™„ì „ í…ŒìŠ¤íŠ¸
        print("3ï¸âƒ£ DI Container í†µí•© ì™„ì „ í…ŒìŠ¤íŠ¸...")
        
        try:
            pipeline = pipelines['m3_max']
            
            # ì´ˆê¸°í™”
            success = await pipeline.initialize()
            print(f"âœ… ì´ˆê¸°í™”: {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
            
            if success:
                # ìƒíƒœ í™•ì¸
                status = pipeline.get_pipeline_status()
                print(f"ğŸ“Š DI Container í™œì„±í™”: {'âœ…' if status['di_container']['enabled'] else 'âŒ'}")
                print(f"ğŸ”— DI Container ë²„ì „: {status['di_container']['version']}")
                print(f"ğŸ›¡ï¸ ìˆœí™˜ì°¸ì¡° ë°©ì§€: {'âœ…' if status['di_container']['circular_reference_free'] else 'âŒ'}")
                print(f"âš¡ ì§€ì—° í•´ê²°: {'âœ…' if status['di_container']['lazy_resolution'] else 'âŒ'}")
                print(f"ğŸ“‹ Step ê´€ë¦¬ì: {status['step_manager']['type']}")
                print(f"ğŸ¯ Step ë“±ë¡: {status['step_manager']['total_registered']}/8")
                print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {status['memory_gb']}GB")
                print(f"ğŸš€ ë””ë°”ì´ìŠ¤: {status['device']}")
                
                # DI Container ìƒì„¸ ìƒíƒœ
                if 'status' in status['di_container'] and status['di_container']['status']:
                    di_stats = status['di_container']['status']
                    if 'registrations' in di_stats:
                        reg = di_stats['registrations']
                        print(f"ğŸ”— DI ë“±ë¡: ì§€ì—°={reg.get('lazy_dependencies', 0)}, ì‹±ê¸€í†¤={reg.get('singleton_instances', 0)}")
                
                # Step ë“±ë¡ ìƒì„¸ í™•ì¸
                registered_steps = pipeline.get_registered_steps()
                print(f"ğŸ“‹ Step ë“±ë¡ ìƒì„¸ (DI Container ê¸°ë°˜):")
                for step_name, step_info in registered_steps['registered_steps'].items():
                    status_emoji = "âœ…" if step_info['registered'] else "âŒ"
                    di_emoji = "ğŸ”—" if step_info.get('di_container_injected', False) else "âŒ"
                    print(f"   {status_emoji} {step_info['step_id']:02d}: {step_name} (DI: {di_emoji})")
            
            # ì •ë¦¬
            await pipeline.cleanup()
            print("âœ… íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ DI Container í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        print("\nğŸ‰ DI Container v4.0 í†µí•© PipelineManager ë°ëª¨ ì™„ë£Œ!")
        print("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€!")
        print("âœ… GitHub Step íŒŒì¼ë“¤ê³¼ ì™„ë²½ ì—°ë™!")
        print("âœ… ëª¨ë“  ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ 100% í˜¸í™˜!")
        print("âœ… BaseStepMixin v19.3 DI Container ì™„ì „ í†µí•©!")
        print("âœ… RealAIStepImplementationManager v14.0 ì—°ë™!")
        print("âœ… ì‹¤ì œ AI ëª¨ë¸ 229GB ì™„ì „ í™œìš©!")
    
    # ì‹¤í–‰
    asyncio.run(demo_di_container_integration())