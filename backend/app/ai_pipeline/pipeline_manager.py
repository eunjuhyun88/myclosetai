# backend/app/ai_pipeline/pipeline_manager.py
"""
ğŸ”¥ ì™„ì „ í†µí•© PipelineManager - ë‘ ë²„ì „ ìµœì  í•©ì„± + ëª¨ë“ˆí™”
âœ… í”„ë¡œì íŠ¸ í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ì™„ë²½ ì—°ë™ (ìš°ì„ ìˆœìœ„ 1)
âœ… StepModelInterface.get_model() ì™„ì „ ì—°ë™ (ìš°ì„ ìˆœìœ„ 2)
âœ… ìë™ íƒì§€ëœ ëª¨ë¸ê³¼ Step ìš”ì²­ ìë™ ë§¤ì¹­ ì™„ë²½ ì§€ì›
âœ… ModelLoader ì´ˆê¸°í™” ìˆœì„œ ì™„ë²½ ë³´ì¥
âœ… ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ ìœ ì§€ë³´ìˆ˜ì„± ê·¹ëŒ€í™”
âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°± ë©”ì»¤ë‹ˆì¦˜ ëŒ€í­ ê°•í™”
âœ… M3 Max 128GB + conda í™˜ê²½ ìµœì í™”
âœ… ëª¨ë“  ê¸°ì¡´ í•¨ìˆ˜/í´ë˜ìŠ¤ëª… 100% ìœ ì§€
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

ì•„í‚¤í…ì²˜:
PipelineManager (Main Controller)
â”œâ”€â”€ InitializationManager (ì´ˆê¸°í™” ê´€ë¦¬)
â”‚   â”œâ”€â”€ UnifiedSystemInitializer (í†µí•© ì‹œìŠ¤í…œ)
â”‚   â”œâ”€â”€ ModelLoaderInitializer (ModelLoader)
â”‚   â””â”€â”€ StepInitializer (Step í´ë˜ìŠ¤ë“¤)
â”œâ”€â”€ ExecutionManager (ì‹¤í–‰ ê´€ë¦¬)
â”‚   â”œâ”€â”€ UnifiedExecutor (í†µí•© ì‹œìŠ¤í…œ ìš°ì„ )
â”‚   â”œâ”€â”€ ModelLoaderExecutor (ModelLoader í´ë°±)
â”‚   â””â”€â”€ FallbackExecutor (ìµœì¢… í´ë°±)
â”œâ”€â”€ UtilityManager (ìœ í‹¸ë¦¬í‹° ê´€ë¦¬)
â”‚   â”œâ”€â”€ OptimizedDataConverter
â”‚   â”œâ”€â”€ OptimizedMemoryManager
â”‚   â””â”€â”€ PerformanceMonitor
â””â”€â”€ ConfigurationManager (ì„¤ì • ê´€ë¦¬)
    â”œâ”€â”€ DeviceOptimizer
    â”œâ”€â”€ M3MaxOptimizer
    â””â”€â”€ CondaOptimizer
"""

import os
import sys
import logging
import asyncio
import time
import traceback
import threading
import json
import gc
from pathlib import Path
from typing import Dict, Any, Optional, Callable, Union, List, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
from abc import ABC, abstractmethod

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image, ImageEnhance, ImageFilter

# ==============================================
# ğŸ”¥ 1. í”„ë¡œì íŠ¸ í†µí•© ì‹œìŠ¤í…œ import (ìµœìš°ì„ )
# ==============================================

# í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ
try:
    from app.ai_pipeline.utils import (
        initialize_global_utils, get_utils_manager, 
        get_system_status, optimize_system_memory,
        get_step_model_interface, get_step_memory_manager,
        get_step_data_converter, preprocess_image_for_step
    )
    UNIFIED_UTILS_AVAILABLE = True
except ImportError as e:
    UNIFIED_UTILS_AVAILABLE = False
    logging.warning(f"í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€: {e}")

# ModelLoader ì‹œìŠ¤í…œ
try:
    from app.ai_pipeline.utils.model_loader import (
        ModelLoader, get_global_model_loader, initialize_global_model_loader,
        StepModelInterface
    )
    MODEL_LOADER_AVAILABLE = True
except ImportError as e:
    MODEL_LOADER_AVAILABLE = False
    logging.warning(f"ModelLoader ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€: {e}")

# Step ëª¨ë¸ ìš”ì²­ ì‹œìŠ¤í…œ
try:
    from app.ai_pipeline.utils.step_model_requests import (
        get_step_request, StepModelRequestAnalyzer, 
        STEP_MODEL_REQUESTS, get_all_step_requirements
    )
    STEP_REQUESTS_AVAILABLE = True
except ImportError as e:
    STEP_REQUESTS_AVAILABLE = False
    logging.warning(f"Step ìš”ì²­ ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€: {e}")

# ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ
try:
    from app.ai_pipeline.utils.auto_model_detector import (
        RealWorldModelDetector, create_real_world_detector,
        quick_real_model_detection
    )
    AUTO_DETECTOR_AVAILABLE = True
except ImportError as e:
    AUTO_DETECTOR_AVAILABLE = False
    logging.warning(f"ìë™ íƒì§€ ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€: {e}")

# Step í´ë˜ìŠ¤ë“¤ import
try:
    from app.ai_pipeline.steps.step_01_human_parsing import HumanParsingStep
    from app.ai_pipeline.steps.step_02_pose_estimation import PoseEstimationStep
    from app.ai_pipeline.steps.step_03_cloth_segmentation import ClothSegmentationStep
    from app.ai_pipeline.steps.step_04_geometric_matching import GeometricMatchingStep
    from app.ai_pipeline.steps.step_05_cloth_warping import ClothWarpingStep
    from app.ai_pipeline.steps.step_06_virtual_fitting import VirtualFittingStep
    from app.ai_pipeline.steps.step_07_post_processing import PostProcessingStep
    from app.ai_pipeline.steps.step_08_quality_assessment import QualityAssessmentStep
    STEP_CLASSES_AVAILABLE = True
except ImportError as e:
    STEP_CLASSES_AVAILABLE = False
    logging.error(f"Step í´ë˜ìŠ¤ë“¤ import ì‹¤íŒ¨: {e}")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 2. ì—´ê±°í˜• ë° ë°ì´í„° í´ë˜ìŠ¤
# ==============================================

class PipelineMode(Enum):
    """íŒŒì´í”„ë¼ì¸ ëª¨ë“œ"""
    DEVELOPMENT = "development"
    PRODUCTION = "production"
    TESTING = "testing"
    OPTIMIZATION = "optimization"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    MAXIMUM = "maximum"

class ProcessingStatus(Enum):
    """ì²˜ë¦¬ ìƒíƒœ"""
    IDLE = "idle"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CLEANING = "cleaning"

class ExecutionMode(Enum):
    """ì‹¤í–‰ ëª¨ë“œ"""
    UNIFIED_SYSTEM = "unified_system"  # í†µí•© ì‹œìŠ¤í…œ ìš°ì„ 
    MODEL_LOADER = "model_loader"      # ModelLoader ìš°ì„ 
    FALLBACK = "fallback"              # í´ë°± ëª¨ë“œ

@dataclass
class PipelineConfig:
    """í†µí•© íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    device: str = "auto"
    quality_level: Union[QualityLevel, str] = QualityLevel.BALANCED
    processing_mode: Union[PipelineMode, str] = PipelineMode.PRODUCTION
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    memory_gb: float = 16.0
    is_m3_max: bool = False
    device_type: str = "auto"
    
    # ğŸ”¥ í†µí•© ì‹œìŠ¤í…œ ì„¤ì • (ìµœìš°ì„ )
    unified_utils_enabled: bool = True
    model_loader_enabled: bool = True
    auto_detect_models: bool = True
    preload_critical_models: bool = True
    model_cache_warmup: bool = True
    step_model_validation: bool = True
    
    # ì‹¤í–‰ ì „ëµ
    execution_mode: Union[ExecutionMode, str] = ExecutionMode.UNIFIED_SYSTEM
    fallback_enabled: bool = True
    retry_with_fallback: bool = True
    
    # ìµœì í™” ì„¤ì •
    optimization_enabled: bool = True
    enable_caching: bool = True
    parallel_processing: bool = True
    memory_optimization: bool = True
    use_fp16: bool = True
    enable_quantization: bool = False
    
    # ì²˜ë¦¬ ì„¤ì •
    batch_size: int = 1
    max_retries: int = 3
    timeout_seconds: int = 300
    save_intermediate: bool = False
    enable_progress_callback: bool = True
    
    # ê³ ê¸‰ ì„¤ì •
    model_cache_size: int = 10
    memory_threshold: float = 0.8
    gpu_memory_fraction: float = 0.9
    thread_pool_size: int = 4
    
    def __post_init__(self):
        # ë¬¸ìì—´ì„ Enumìœ¼ë¡œ ë³€í™˜
        if isinstance(self.quality_level, str):
            self.quality_level = QualityLevel(self.quality_level)
        if isinstance(self.processing_mode, str):
            self.processing_mode = PipelineMode(self.processing_mode)
        if isinstance(self.execution_mode, str):
            self.execution_mode = ExecutionMode(self.execution_mode)
        
        # M3 Max ìë™ ìµœì í™”
        if self.is_m3_max:
            self.memory_gb = max(self.memory_gb, 64.0)
            self.use_fp16 = True
            self.optimization_enabled = True
            self.batch_size = 4
            self.model_cache_size = 15
            self.gpu_memory_fraction = 0.95
            self.unified_utils_enabled = True
            self.auto_detect_models = True
            self.preload_critical_models = True
            self.model_cache_warmup = True

@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼"""
    success: bool
    session_id: str = ""
    result_image: Optional[Image.Image] = None
    result_tensor: Optional[torch.Tensor] = None
    quality_score: float = 0.0
    quality_grade: str = "Unknown"
    processing_time: float = 0.0
    step_results: Dict[str, Any] = field(default_factory=dict)
    step_timings: Dict[str, float] = field(default_factory=dict)
    execution_strategy: Dict[str, str] = field(default_factory=dict)  # ğŸ”¥ ì‹¤í–‰ ì „ëµ ì¶”ê°€
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)

@dataclass
class SessionData:
    """ì„¸ì…˜ ë°ì´í„°"""
    session_id: str
    start_time: float
    status: ProcessingStatus = ProcessingStatus.IDLE
    step_results: Dict[str, Any] = field(default_factory=dict)
    step_timings: Dict[str, float] = field(default_factory=dict)
    execution_strategies: Dict[str, str] = field(default_factory=dict)  # ğŸ”¥ ì‹¤í–‰ ì „ëµ ì¶”ê°€
    intermediate_results: Dict[str, Any] = field(default_factory=dict)
    error_log: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def add_step_result(self, step_name: str, result: Dict[str, Any], timing: float, strategy: str = "unknown"):
        """ë‹¨ê³„ ê²°ê³¼ ì¶”ê°€"""
        self.step_results[step_name] = result
        self.step_timings[step_name] = timing
        self.execution_strategies[step_name] = strategy

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    total_sessions: int = 0
    successful_sessions: int = 0
    failed_sessions: int = 0
    average_processing_time: float = 0.0
    average_quality_score: float = 0.0
    total_processing_time: float = 0.0
    fastest_processing_time: float = float('inf')
    slowest_processing_time: float = 0.0
    unified_system_usage: int = 0  # ğŸ”¥ í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš© íšŸìˆ˜
    model_loader_usage: int = 0    # ğŸ”¥ ModelLoader ì‚¬ìš© íšŸìˆ˜
    fallback_usage: int = 0        # ğŸ”¥ í´ë°± ì‚¬ìš© íšŸìˆ˜
    
    def update(self, processing_time: float, quality_score: float, success: bool, execution_strategy: str = "unknown"):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.total_sessions += 1
        self.total_processing_time += processing_time
        
        # ì‹¤í–‰ ì „ëµë³„ í†µê³„
        if execution_strategy == "unified_system":
            self.unified_system_usage += 1
        elif execution_strategy == "model_loader":
            self.model_loader_usage += 1
        elif execution_strategy == "fallback":
            self.fallback_usage += 1
        
        if success:
            self.successful_sessions += 1
            self.fastest_processing_time = min(self.fastest_processing_time, processing_time)
            self.slowest_processing_time = max(self.slowest_processing_time, processing_time)
        else:
            self.failed_sessions += 1
        
        # í‰ê·  ê³„ì‚°
        if self.total_sessions > 0:
            self.average_processing_time = self.total_processing_time / self.total_sessions
        
        if self.successful_sessions > 0:
            prev_total = self.average_quality_score * (self.successful_sessions - 1)
            self.average_quality_score = (prev_total + quality_score) / self.successful_sessions

# ==============================================
# ğŸ”¥ 3. ëª¨ë“ˆí™”ëœ ê´€ë¦¬ í´ë˜ìŠ¤ë“¤
# ==============================================

class OptimizedDataConverter:
    """ìµœì í™”ëœ ë°ì´í„° ë³€í™˜ê¸° - í†µí•© ì‹œìŠ¤í…œ ìš°ì„ """
    
    def __init__(self, device: str = "mps"):
        self.device = device
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        # ğŸ”¥ í†µí•© ì‹œìŠ¤í…œ ì—°ë™
        self.project_converter = None
        if UNIFIED_UTILS_AVAILABLE:
            try:
                self.project_converter = get_step_data_converter("PipelineManager")
                if self.project_converter:
                    self.logger.info("âœ… í†µí•© DataConverter ì—°ë™ ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ í†µí•© DataConverter ì—°ë™ ì‹¤íŒ¨: {e}")
    
    def preprocess_image(self, image_input: Union[str, Image.Image, np.ndarray]) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ - í†µí•© ì‹œìŠ¤í…œ ìš°ì„ """
        try:
            # ğŸ”¥ 1ìˆœìœ„: í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš©
            if self.project_converter:
                try:
                    result = self.project_converter.image_to_tensor(image_input)
                    if result is not None:
                        return result
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í†µí•© ì»¨ë²„í„° ì‹¤íŒ¨, í´ë°± ì‚¬ìš©: {e}")
            
            # 2ìˆœìœ„: í”„ë¡œì íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì‚¬ìš©
            if UNIFIED_UTILS_AVAILABLE:
                try:
                    result = preprocess_image_for_step(image_input, "PipelineManager")
                    if result is not None:
                        return result
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í”„ë¡œì íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨, ê¸°ë³¸ ì‚¬ìš©: {e}")
            
            # 3ìˆœìœ„: ê¸°ë³¸ ì „ì²˜ë¦¬
            return self._basic_preprocess(image_input)
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _basic_preprocess(self, image_input: Union[str, Image.Image, np.ndarray]) -> torch.Tensor:
        """ê¸°ë³¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        if isinstance(image_input, str):
            if not os.path.exists(image_input):
                raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_input}")
            image = Image.open(image_input).convert('RGB')
        elif isinstance(image_input, Image.Image):
            image = image_input.convert('RGB')
        elif isinstance(image_input, np.ndarray):
            if image_input.dtype != np.uint8:
                image_input = (image_input * 255).astype(np.uint8)
            image = Image.fromarray(image_input).convert('RGB')
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_input)}")
        
        # í¬ê¸° ì¡°ì •
        target_size = (512, 512)
        if image.size != target_size:
            image = image.resize(target_size, Image.Resampling.LANCZOS)
        
        # í…ì„œ ë³€í™˜
        img_array = np.array(image)
        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0
        img_tensor = img_tensor.unsqueeze(0)
        
        return img_tensor
    
    def tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜ - í†µí•© ì‹œìŠ¤í…œ ìš°ì„ """
        try:
            # ğŸ”¥ 1ìˆœìœ„: í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš©
            if self.project_converter:
                try:
                    result = self.project_converter.tensor_to_image(tensor, format="PIL")
                    if result is not None:
                        return result
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í†µí•© ì»¨ë²„í„° ì‹¤íŒ¨, ê¸°ë³¸ ì‚¬ìš©: {e}")
            
            # 2ìˆœìœ„: ê¸°ë³¸ ë³€í™˜
            return self._basic_tensor_to_pil(tensor)
            
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ-PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (512, 512), color='black')
    
    def _basic_tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """ê¸°ë³¸ í…ì„œ-PIL ë³€í™˜"""
        if tensor.dim() == 4:
            tensor = tensor.squeeze(0)
        
        if tensor.shape[0] == 3:
            tensor = tensor.permute(1, 2, 0)
        
        tensor = torch.clamp(tensor, 0, 1)
        tensor = tensor.cpu()
        array = (tensor.numpy() * 255).astype(np.uint8)
        
        return Image.fromarray(array)

class OptimizedMemoryManager:
    """ìµœì í™”ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬ì - í†µí•© ì‹œìŠ¤í…œ ìš°ì„ """
    
    def __init__(self, device: str = "mps"):
        self.device = device
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        # ğŸ”¥ í†µí•© ì‹œìŠ¤í…œ ì—°ë™
        self.project_memory_manager = None
        if UNIFIED_UTILS_AVAILABLE:
            try:
                self.project_memory_manager = get_step_memory_manager("PipelineManager")
                if self.project_memory_manager:
                    self.logger.info("âœ… í†µí•© MemoryManager ì—°ë™ ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ í†µí•© MemoryManager ì—°ë™ ì‹¤íŒ¨: {e}")
    
    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ - í†µí•© ì‹œìŠ¤í…œ ìš°ì„ """
        try:
            # ğŸ”¥ 1ìˆœìœ„: í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš©
            if self.project_memory_manager:
                try:
                    result = self.project_memory_manager.cleanup_memory()
                    if result.get("success", False):
                        self.logger.debug("âœ… í†µí•© ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ë¡œ ì •ë¦¬ ì™„ë£Œ")
                        return
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í†µí•© ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì‹¤íŒ¨, ê¸°ë³¸ ì‚¬ìš©: {e}")
            
            # 2ìˆœìœ„: ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìµœì í™”
            if UNIFIED_UTILS_AVAILABLE:
                try:
                    optimize_system_memory()
                    self.logger.debug("âœ… ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
                    return
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            
            # 3ìˆœìœ„: ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            self._basic_cleanup()
            
        except Exception as e:
            self.logger.warning(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _basic_cleanup(self):
        """ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        
        if self.device == "cuda" and torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        elif self.device == "mps" and torch.backends.mps.is_available():
            try:
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                if hasattr(torch.mps, 'synchronize'):
                    torch.mps.synchronize()
            except (AttributeError, RuntimeError):
                pass
    
    def get_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ - í†µí•© ì‹œìŠ¤í…œ ìš°ì„ """
        try:
            # ğŸ”¥ 1ìˆœìœ„: í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš©
            if self.project_memory_manager:
                try:
                    stats = self.project_memory_manager.get_memory_stats()
                    return {
                        'allocated_gb': stats.gpu_allocated_gb,
                        'total_gb': stats.gpu_total_gb,
                        'cpu_used_gb': stats.cpu_used_gb,
                        'cpu_total_gb': stats.cpu_total_gb,
                        'cpu_percent': stats.cpu_percent,
                        'source': 'unified_system'
                    }
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í†µí•© ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # 2ìˆœìœ„: ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë³´
            return self._basic_memory_usage()
            
        except Exception as e:
            self.logger.warning(f"âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'source': 'error'}
    
    def _basic_memory_usage(self) -> Dict[str, float]:
        """ê¸°ë³¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        usage = {'source': 'basic'}
        
        if self.device == "cuda" and torch.cuda.is_available():
            usage.update({
                'allocated_gb': torch.cuda.memory_allocated() / 1024**3,
                'cached_gb': torch.cuda.memory_reserved() / 1024**3,
                'total_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3
            })
        elif self.device == "mps":
            try:
                import psutil
                memory = psutil.virtual_memory()
                usage.update({
                    'used_gb': memory.used / 1024**3,
                    'available_gb': memory.available / 1024**3,
                    'total_gb': memory.total / 1024**3,
                    'percent': memory.percent
                })
            except ImportError:
                usage['status'] = 'psutil not available'
        
        return usage

class InitializationManager:
    """ì´ˆê¸°í™” ê´€ë¦¬ì - ëª¨ë“  ì´ˆê¸°í™” ë¡œì§ í†µí•©"""
    
    def __init__(self, config: PipelineConfig, device: str, logger: logging.Logger):
        self.config = config
        self.device = device
        self.logger = logger
        
        # ì´ˆê¸°í™” ìƒíƒœ
        self.unified_system_initialized = False
        self.model_loader_initialized = False
        self.steps_initialized = False
        
        # ì»´í¬ë„ŒíŠ¸ ì°¸ì¡°
        self.utils_manager = None
        self.model_loader = None
        self.auto_detector = None
        self.steps = {}
    
    async def initialize_all(self, step_order: List[str]) -> bool:
        """ì „ì²´ ì´ˆê¸°í™” ì‹¤í–‰"""
        try:
            self.logger.info("ğŸ”§ í†µí•© ì´ˆê¸°í™” ì‹œì‘...")
            start_time = time.time()
            
            # 1. í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìµœìš°ì„ )
            if self.config.unified_utils_enabled:
                self.unified_system_initialized = await self._initialize_unified_system()
                if self.unified_system_initialized:
                    self.logger.info("âœ… í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    self.logger.warning("âš ï¸ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨, ê³„ì† ì§„í–‰")
            
            # 2. ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            if self.config.model_loader_enabled:
                self.model_loader_initialized = await self._initialize_model_loader()
                if self.model_loader_initialized:
                    self.logger.info("âœ… ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    self.logger.warning("âš ï¸ ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨, ê³„ì† ì§„í–‰")
            
            # 3. Step í´ë˜ìŠ¤ë“¤ ì´ˆê¸°í™”
            self.steps_initialized = await self._initialize_steps(step_order)
            if self.steps_initialized:
                self.logger.info("âœ… Step í´ë˜ìŠ¤ë“¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # 4. ì¤‘ìš” ëª¨ë¸ ì‚¬ì „ ë¡œë“œ (ì˜µì…˜)
            if self.config.preload_critical_models:
                await self._preload_critical_models()
            
            # 5. ëª¨ë¸ ìºì‹œ ì›Œë°ì—… (ì˜µì…˜)
            if self.config.model_cache_warmup:
                await self._warmup_model_cache()
            
            initialization_time = time.time() - start_time
            self.logger.info(f"ğŸ‰ í†µí•© ì´ˆê¸°í™” ì™„ë£Œ ({initialization_time:.2f}ì´ˆ)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ í†µí•© ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _initialize_unified_system(self) -> bool:
        """í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            if not UNIFIED_UTILS_AVAILABLE:
                return False
            
            result = await initialize_global_utils(
                device=self.device,
                memory_gb=self.config.memory_gb,
                is_m3_max=self.config.is_m3_max,
                optimization_enabled=self.config.optimization_enabled
            )
            
            if result.get("success", False):
                self.utils_manager = get_utils_manager()
                
                # ìë™ íƒì§€ ì‹œìŠ¤í…œ ì—°ë™
                if AUTO_DETECTOR_AVAILABLE and self.config.auto_detect_models:
                    self.auto_detector = create_real_world_detector()
                
                return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _initialize_model_loader(self) -> bool:
        """ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            if not MODEL_LOADER_AVAILABLE:
                return False
            
            self.model_loader = await asyncio.get_event_loop().run_in_executor(
                None, initialize_global_model_loader
            )
            if self.model_loader is None:
                self.model_loader = get_global_model_loader()
            
            return self.model_loader is not None
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _initialize_steps(self, step_order: List[str]) -> bool:
        """Step í´ë˜ìŠ¤ë“¤ ì´ˆê¸°í™”"""
        try:
            if not STEP_CLASSES_AVAILABLE:
                return False
            
            step_classes = {
                'human_parsing': HumanParsingStep,
                'pose_estimation': PoseEstimationStep,
                'cloth_segmentation': ClothSegmentationStep,
                'geometric_matching': GeometricMatchingStep,
                'cloth_warping': ClothWarpingStep,
                'virtual_fitting': VirtualFittingStep,
                'post_processing': PostProcessingStep,
                'quality_assessment': QualityAssessmentStep
            }
            
            # ê¸°ë³¸ ì„¤ì •
            base_config = {
                'device': self.device,
                'device_type': self.config.device_type,
                'memory_gb': self.config.memory_gb,
                'is_m3_max': self.config.is_m3_max,
                'optimization_enabled': self.config.optimization_enabled,
                'quality_level': self.config.quality_level.value
            }
            
            for step_name in step_order:
                try:
                    step_class = step_classes[step_name]
                    step_config = {**base_config, **self._get_step_config(step_name)}
                    
                    # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                    step_instance = self._create_step_instance_safely(step_class, step_name, step_config)
                    
                    if step_instance:
                        # ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
                        await self._setup_step_interfaces(step_instance, step_name)
                        
                        # Step ì´ˆê¸°í™”
                        if hasattr(step_instance, 'initialize'):
                            await step_instance.initialize()
                        
                        self.steps[step_name] = step_instance
                        self.logger.info(f"âœ… {step_name} ì´ˆê¸°í™” ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.error(f"âŒ {step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    continue
            
            return len(self.steps) > 0
            
        except Exception as e:
            self.logger.error(f"âŒ Step ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _setup_step_interfaces(self, step_instance, step_name: str):
        """Step ì¸í„°í˜ì´ìŠ¤ ì„¤ì •"""
        try:
            step_class_name = f"{step_name.title().replace('_', '')}Step"
            
            # 1. í†µí•© ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
            if self.unified_system_initialized and self.utils_manager:
                try:
                    unified_interface = self.utils_manager.create_step_interface(step_class_name)
                    setattr(step_instance, 'unified_interface', unified_interface)
                    self.logger.debug(f"âœ… {step_name} í†µí•© ì¸í„°í˜ì´ìŠ¤ ì„¤ì •")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} í†µí•© ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            
            # 2. ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
            if self.model_loader_initialized and self.model_loader:
                try:
                    model_interface = self.model_loader.create_step_interface(step_class_name)
                    setattr(step_instance, 'model_interface', model_interface)
                    self.logger.debug(f"âœ… {step_name} ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì •")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _create_step_instance_safely(self, step_class, step_name: str, step_config: Dict[str, Any]):
        """Step ì¸ìŠ¤í„´ìŠ¤ ì•ˆì „ ìƒì„±"""
        try:
            return step_class(**step_config)
        except TypeError as e:
            if "unexpected keyword argument" in str(e):
                try:
                    safe_config = {
                        'device': step_config.get('device', 'cpu'),
                        'config': step_config.get('config', {})
                    }
                    return step_class(**safe_config)
                except Exception:
                    try:
                        return step_class(device=step_config.get('device', 'cpu'))
                    except Exception:
                        return None
            else:
                raise
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _get_step_config(self, step_name: str) -> Dict[str, Any]:
        """Stepë³„ ì„¤ì • ë°˜í™˜"""
        step_configs = {
            'human_parsing': {
                'model_name': 'graphonomy',
                'num_classes': 20,
                'input_size': (512, 512)
            },
            'pose_estimation': {
                'model_type': 'mediapipe',
                'input_size': (368, 368),
                'confidence_threshold': 0.5
            },
            'cloth_segmentation': {
                'model_name': 'u2net',
                'background_threshold': 0.5,
                'post_process': True
            },
            'geometric_matching': {
                'tps_points': 25,
                'matching_threshold': 0.8,
                'method': 'auto'
            },
            'cloth_warping': {
                'warping_method': 'tps',
                'physics_simulation': True
            },
            'virtual_fitting': {
                'blending_method': 'poisson',
                'seamless_cloning': True
            },
            'post_processing': {
                'enable_super_resolution': self.config.optimization_enabled,
                'enhance_faces': True
            },
            'quality_assessment': {
                'enable_detailed_analysis': True,
                'perceptual_metrics': True
            }
        }
        
        return step_configs.get(step_name, {})
    
    async def _preload_critical_models(self):
        """ì¤‘ìš” ëª¨ë¸ ì‚¬ì „ ë¡œë“œ"""
        try:
            critical_steps = ['human_parsing', 'pose_estimation', 'cloth_segmentation']
            
            for step_name in critical_steps:
                if step_name in self.steps:
                    step = self.steps[step_name]
                    
                    # ModelLoader ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
                    if hasattr(step, 'model_interface') and step.model_interface:
                        try:
                            if STEP_REQUESTS_AVAILABLE:
                                step_class_name = f"{step_name.title().replace('_', '')}Step"
                                step_req = get_step_request(step_class_name)
                                
                                if step_req:
                                    model_name = step_req.model_name
                                    await step.model_interface.get_model(model_name)
                                    self.logger.info(f"âœ… {step_name} ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ {step_name} ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")
                            
        except Exception as e:
            self.logger.error(f"âŒ ì¤‘ìš” ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def _warmup_model_cache(self):
        """ëª¨ë¸ ìºì‹œ ì›Œë°ì—…"""
        try:
            for step_name, step_instance in self.steps.items():
                try:
                    # í†µí•© ì¸í„°í˜ì´ìŠ¤ ìš°ì„ 
                    if hasattr(step_instance, 'unified_interface') and step_instance.unified_interface:
                        model = await step_instance.unified_interface.get_model()
                        if model:
                            self.logger.debug(f"âœ… {step_name} í†µí•© ìºì‹œ ì›Œë°ì—…")
                            continue
                    
                    # ModelLoader ì¸í„°í˜ì´ìŠ¤
                    if hasattr(step_instance, 'model_interface') and step_instance.model_interface:
                        available_models = await step_instance.model_interface.list_available_models()
                        if available_models:
                            await step_instance.model_interface.get_model(available_models[0])
                            self.logger.debug(f"âœ… {step_name} ModelLoader ìºì‹œ ì›Œë°ì—…")
                            
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ìºì‹œ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìºì‹œ ì›Œë°ì—… ì‹¤íŒ¨: {e}")

class ExecutionManager:
    """ì‹¤í–‰ ê´€ë¦¬ì - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì‹¤í–‰ ì „ëµ"""
    
    def __init__(self, config: PipelineConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
    
    async def execute_step(
        self,
        step,
        step_name: str,
        current_data: torch.Tensor,
        clothing_tensor: torch.Tensor,
        body_measurements: Optional[Dict],
        clothing_type: str,
        fabric_type: str,
        style_preferences: Optional[Dict],
        max_retries: int
    ) -> Tuple[Dict[str, Any], str]:
        """Step ì‹¤í–‰ - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì „ëµ"""
        
        last_error = None
        execution_strategy = "unknown"
        
        for attempt in range(max_retries + 1):
            try:
                if attempt > 0:
                    self.logger.info(f"ğŸ”„ {step_name} ì¬ì‹œë„ {attempt}/{max_retries}")
                    await asyncio.sleep(0.5)
                
                # ì‹¤í–‰ ì „ëµ ê²°ì •
                if self.config.execution_mode == ExecutionMode.UNIFIED_SYSTEM:
                    # 1ìˆœìœ„: í†µí•© ì‹œìŠ¤í…œ
                    result, strategy = await self._execute_with_unified_system(
                        step, step_name, current_data, clothing_tensor,
                        body_measurements, clothing_type, fabric_type, style_preferences
                    )
                    if result.get('success', False):
                        return result, strategy
                    
                    # í´ë°± í—ˆìš© ì‹œ ModelLoader ì‹œë„
                    if self.config.fallback_enabled:
                        result, strategy = await self._execute_with_model_loader(
                            step, step_name, current_data, clothing_tensor,
                            body_measurements, clothing_type, fabric_type, style_preferences
                        )
                        if result.get('success', False):
                            return result, strategy
                
                elif self.config.execution_mode == ExecutionMode.MODEL_LOADER:
                    # 1ìˆœìœ„: ModelLoader
                    result, strategy = await self._execute_with_model_loader(
                        step, step_name, current_data, clothing_tensor,
                        body_measurements, clothing_type, fabric_type, style_preferences
                    )
                    if result.get('success', False):
                        return result, strategy
                    
                    # í´ë°± í—ˆìš© ì‹œ í†µí•© ì‹œìŠ¤í…œ ì‹œë„
                    if self.config.fallback_enabled:
                        result, strategy = await self._execute_with_unified_system(
                            step, step_name, current_data, clothing_tensor,
                            body_measurements, clothing_type, fabric_type, style_preferences
                        )
                        if result.get('success', False):
                            return result, strategy
                
                # ìµœì¢… í´ë°±: ê¸°ë³¸ ì‹¤í–‰
                if self.config.fallback_enabled:
                    result, strategy = await self._execute_with_fallback(
                        step, step_name, current_data, clothing_tensor,
                        body_measurements, clothing_type, fabric_type, style_preferences
                    )
                    if result.get('success', False):
                        return result, strategy
                
                last_error = result.get('error', 'Unknown error')
                execution_strategy = strategy
                    
            except Exception as e:
                last_error = str(e)
                execution_strategy = "error"
                self.logger.warning(f"âš ï¸ {step_name} ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                
                if attempt < max_retries:
                    continue
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        return {
            'success': False,
            'error': last_error,
            'confidence': 0.0,
            'quality_score': 0.0,
            'processing_time': 0.0,
            'model_used': 'failed_after_retries'
        }, execution_strategy
    
    async def _execute_with_unified_system(
        self,
        step,
        step_name: str,
        current_data: torch.Tensor,
        clothing_tensor: torch.Tensor,
        body_measurements: Optional[Dict],
        clothing_type: str,
        fabric_type: str,
        style_preferences: Optional[Dict]
    ) -> Tuple[Dict[str, Any], str]:
        """í†µí•© ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ ì‹¤í–‰"""
        try:
            if hasattr(step, 'unified_interface') and step.unified_interface:
                self.logger.debug(f"ğŸ”— {step_name} í†µí•© ì‹œìŠ¤í…œ ì‹¤í–‰")
                
                result = step.unified_interface.process_image(
                    current_data,
                    clothing_data=clothing_tensor,
                    clothing_type=clothing_type,
                    fabric_type=fabric_type,
                    style_preferences=style_preferences,
                    optimize_memory=True
                )
                
                if result and result.get('success', False):
                    return {
                        'success': True,
                        'result': result.get('processed_image', current_data),
                        'confidence': result.get('confidence', 0.9),
                        'quality_score': result.get('quality_score', 0.9),
                        'processing_time': result.get('processing_time', 0.1),
                        'model_used': result.get('model_used', 'unified_system'),
                        'processing_method': 'unified_interface'
                    }, "unified_system"
            
            raise Exception("í†µí•© ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš© ë¶ˆê°€")
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0,
                'processing_time': 0.0,
                'model_used': 'unified_system_error'
            }, "unified_system_error"
    
    async def _execute_with_model_loader(
        self,
        step,
        step_name: str,
        current_data: torch.Tensor,
        clothing_tensor: torch.Tensor,
        body_measurements: Optional[Dict],
        clothing_type: str,
        fabric_type: str,
        style_preferences: Optional[Dict]
    ) -> Tuple[Dict[str, Any], str]:
        """ModelLoaderë¥¼ ì‚¬ìš©í•œ ì‹¤í–‰"""
        try:
            model_used = "fallback"
            
            # AI ëª¨ë¸ ë¡œë“œ ì‹œë„
            if hasattr(step, 'model_interface') and step.model_interface:
                if STEP_REQUESTS_AVAILABLE:
                    step_class_name = f"{step_name.title().replace('_', '')}Step"
                    step_req = get_step_request(step_class_name)
                    
                    if step_req:
                        model_name = step_req.model_name
                        ai_model = await step.model_interface.get_model(model_name)
                        
                        if ai_model:
                            model_used = model_name
                            setattr(step, '_ai_model', ai_model)
            
            # Stepë³„ ì²˜ë¦¬ ì‹¤í–‰
            result = await self._execute_step_logic(
                step, step_name, current_data, clothing_tensor,
                body_measurements, clothing_type, fabric_type, style_preferences
            )
            
            if not result or not isinstance(result, dict):
                result = {
                    'success': True,
                    'result': current_data,
                    'confidence': 0.8,
                    'quality_score': 0.8,
                    'processing_time': 0.1
                }
            
            result['model_used'] = model_used
            result['processing_method'] = 'model_loader'
            
            return result, "model_loader"
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0,
                'processing_time': 0.0,
                'model_used': 'model_loader_error'
            }, "model_loader_error"
    
    async def _execute_with_fallback(
        self,
        step,
        step_name: str,
        current_data: torch.Tensor,
        clothing_tensor: torch.Tensor,
        body_measurements: Optional[Dict],
        clothing_type: str,
        fabric_type: str,
        style_preferences: Optional[Dict]
    ) -> Tuple[Dict[str, Any], str]:
        """í´ë°± ì‹¤í–‰"""
        try:
            result = await self._execute_step_logic(
                step, step_name, current_data, clothing_tensor,
                body_measurements, clothing_type, fabric_type, style_preferences
            )
            
            if not result or not isinstance(result, dict):
                result = {
                    'success': True,
                    'result': current_data,
                    'confidence': 0.7,
                    'quality_score': 0.7,
                    'processing_time': 0.1
                }
            
            result['model_used'] = 'fallback'
            result['processing_method'] = 'fallback'
            
            return result, "fallback"
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0,
                'processing_time': 0.0,
                'model_used': 'fallback_error'
            }, "fallback_error"
    
    async def _execute_step_logic(
        self,
        step,
        step_name: str,
        current_data: torch.Tensor,
        clothing_tensor: torch.Tensor,
        body_measurements: Optional[Dict],
        clothing_type: str,
        fabric_type: str,
        style_preferences: Optional[Dict]
    ) -> Dict[str, Any]:
        """Stepë³„ ì²˜ë¦¬ ë¡œì§ ì‹¤í–‰"""
        if step_name == 'human_parsing':
            return await step.process(current_data)
        elif step_name == 'pose_estimation':
            return await step.process(current_data)
        elif step_name == 'cloth_segmentation':
            return await step.process(clothing_tensor, clothing_type=clothing_type)
        elif step_name == 'geometric_matching':
            dummy_pose_keypoints = self._generate_dummy_pose_keypoints()
            dummy_clothing_segmentation = {'mask': clothing_tensor}
            return await step.process(
                person_parsing={'result': current_data},
                pose_keypoints=dummy_pose_keypoints,
                clothing_segmentation=dummy_clothing_segmentation,
                clothing_type=clothing_type
            )
        elif step_name == 'cloth_warping':
            return await step.process(
                current_data, clothing_tensor, body_measurements or {}, fabric_type
            )
        elif step_name == 'virtual_fitting':
            return await step.process(current_data, clothing_tensor, style_preferences or {})
        elif step_name == 'post_processing':
            return await step.process(current_data)
        elif step_name == 'quality_assessment':
            return await step.process(current_data, clothing_tensor)
        else:
            return await step.process(current_data)
    
    def _generate_dummy_pose_keypoints(self) -> List[List[float]]:
        """ë”ë¯¸ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ìƒì„±"""
        dummy_keypoints = []
        for i in range(18):
            x = 256 + np.random.uniform(-50, 50)
            y = 256 + np.random.uniform(-100, 100)
            confidence = 0.8
            dummy_keypoints.append([x, y, confidence])
        return dummy_keypoints

# ==============================================
# ğŸ”¥ 4. í†µí•© PipelineManager í´ë˜ìŠ¤
# ==============================================

class PipelineManager:
    """
    ğŸ”¥ ì™„ì „ í†µí•© PipelineManager
    
    âœ… í”„ë¡œì íŠ¸ í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ìš°ì„  ì‚¬ìš©
    âœ… ModelLoader ì‹œìŠ¤í…œ ì™„ë²½ ì—°ë™
    âœ… ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì‹¤í–‰ ì „ëµ
    âœ… ëª¨ë“ˆí™”ëœ ê´€ë¦¬ êµ¬ì¡°
    âœ… ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°±
    âœ… M3 Max + conda í™˜ê²½ ìµœì í™”
    """
    
    def __init__(
        self,
        config_path: Optional[str] = None,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], PipelineConfig]] = None,
        **kwargs
    ):
        """íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        
        # 1. ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._auto_detect_device(device)
        
        # 2. ì„¤ì • ì´ˆê¸°í™”
        if isinstance(config, PipelineConfig):
            self.config = config
        else:
            config_dict = self._load_config(config_path) if config_path else {}
            if config:
                config_dict.update(config if isinstance(config, dict) else {})
            config_dict.update(kwargs)
            
            self.config = PipelineConfig(device=self.device, **config_dict)
        
        # 3. ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€ ë° ì„¤ì • ì—…ë°ì´íŠ¸
        self.device_type = self._detect_device_type()
        self.memory_gb = self._detect_memory_gb()
        self.is_m3_max = self._detect_m3_max()
        
        self.config.device_type = self.device_type
        self.config.memory_gb = self.memory_gb
        self.config.is_m3_max = self.is_m3_max
        
        # 4. ê´€ë¦¬ìë“¤ ì´ˆê¸°í™”
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        self.initialization_manager = InitializationManager(self.config, self.device, self.logger)
        self.execution_manager = ExecutionManager(self.config, self.logger)
        
        # 5. ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™”
        self.data_converter = OptimizedDataConverter(self.device)
        self.memory_manager = OptimizedMemoryManager(self.device)
        
        # 6. íŒŒì´í”„ë¼ì¸ ìƒíƒœ
        self.is_initialized = False
        self.current_status = ProcessingStatus.IDLE
        self.step_order = [
            'human_parsing', 'pose_estimation', 'cloth_segmentation',
            'geometric_matching', 'cloth_warping', 'virtual_fitting',
            'post_processing', 'quality_assessment'
        ]
        
        # 7. ì„¸ì…˜ ë° ì„±ëŠ¥ ê´€ë¦¬
        self.sessions: Dict[str, SessionData] = {}
        self.performance_metrics = PerformanceMetrics()
        
        # 8. ë™ì‹œì„± ê´€ë¦¬
        self._lock = threading.RLock()
        self.thread_pool = ThreadPoolExecutor(max_workers=self.config.thread_pool_size)
        
        # 9. ë””ë°”ì´ìŠ¤ ìµœì í™”
        self._configure_device_optimizations()
        
        # 10. ì´ˆê¸°í™” ì™„ë£Œ ë¡œê¹…
        self.logger.info(f"âœ… í†µí•© PipelineManager ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {self.device} ({self.device_type})")
        self.logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬: {self.memory_gb}GB, M3 Max: {'âœ…' if self.is_m3_max else 'âŒ'}")
        self.logger.info(f"âš™ï¸ ì‹¤í–‰ ëª¨ë“œ: {self.config.execution_mode.value}")
        self.logger.info(f"ğŸ”§ í†µí•© ì‹œìŠ¤í…œ: {'âœ…' if UNIFIED_UTILS_AVAILABLE else 'âŒ'}")
        self.logger.info(f"ğŸ”§ ModelLoader: {'âœ…' if MODEL_LOADER_AVAILABLE else 'âŒ'}")
        
        # 11. ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™”
        self.memory_manager.cleanup_memory()
    
    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device and preferred_device != "auto":
            return preferred_device
        
        try:
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'
    
    def _detect_device_type(self) -> str:
        """ë””ë°”ì´ìŠ¤ íƒ€ì… ê°ì§€"""
        if self.device == 'mps':
            return 'apple_silicon'
        elif self.device == 'cuda':
            return 'nvidia'
        else:
            return 'cpu'
    
    def _detect_memory_gb(self) -> float:
        """ë©”ëª¨ë¦¬ ìš©ëŸ‰ ê°ì§€"""
        try:
            import psutil
            return round(psutil.virtual_memory().total / (1024**3), 1)
        except ImportError:
            return 16.0
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                    capture_output=True, text=True, timeout=5
                )
                chip_info = result.stdout.strip()
                return 'M3' in chip_info and 'Max' in chip_info
        except:
            pass
        return False
    
    def _configure_device_optimizations(self):
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •"""
        if self.device == 'mps':
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            if torch.backends.mps.is_available():
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except:
                    pass
            self.logger.info("ğŸ”§ M3 Max MPS ìµœì í™” ì„¤ì • ì™„ë£Œ")
        elif self.device == 'cuda':
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            if self.config.optimization_enabled:
                torch.backends.cudnn.enabled = True
            self.logger.info("ğŸ”§ CUDA ìµœì í™” ì„¤ì • ì™„ë£Œ")
        
        if self.device in ['cuda', 'mps'] and self.config.use_fp16:
            self.use_amp = True
            self.logger.info("âš¡ í˜¼í•© ì •ë°€ë„ ì—°ì‚° í™œì„±í™”")
        else:
            self.use_amp = False
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if not config_path or not os.path.exists(config_path):
            return {}
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.warning(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    @property
    def steps(self) -> Dict[str, Any]:
        """Step ì°¸ì¡° ë°˜í™˜"""
        return self.initialization_manager.steps
    
    async def initialize(self) -> bool:
        """íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        try:
            self.logger.info("ğŸ”„ í†µí•© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹œì‘...")
            self.current_status = ProcessingStatus.INITIALIZING
            start_time = time.time()
            
            # 1. ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
            
            # 2. í†µí•© ì´ˆê¸°í™” ì‹¤í–‰
            success = await self.initialization_manager.initialize_all(self.step_order)
            
            # 3. ì´ˆê¸°í™” ê²€ì¦
            success_rate = self._verify_initialization()
            if success_rate < 0.5:
                self.logger.warning(f"ì´ˆê¸°í™” ì„±ê³µë¥  ë‚®ìŒ: {success_rate:.1%}")
            
            initialization_time = time.time() - start_time
            self.is_initialized = success
            self.current_status = ProcessingStatus.IDLE if success else ProcessingStatus.FAILED
            
            if success:
                self.logger.info(f"ğŸ‰ í†µí•© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ ({initialization_time:.2f}ì´ˆ)")
                self.logger.info(f"ğŸ“Š ì´ˆê¸°í™” ì„±ê³µë¥ : {success_rate:.1%}")
                self.logger.info(f"ğŸ”§ í†µí•© ì‹œìŠ¤í…œ: {'âœ…' if self.initialization_manager.unified_system_initialized else 'âŒ'}")
                self.logger.info(f"ğŸ”§ ModelLoader: {'âœ…' if self.initialization_manager.model_loader_initialized else 'âŒ'}")
            else:
                self.logger.error("âŒ í†µí•© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            return success
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            self.is_initialized = False
            self.current_status = ProcessingStatus.FAILED
            return False
    
    def _verify_initialization(self) -> float:
        """ì´ˆê¸°í™” ê²€ì¦"""
        total_steps = len(self.step_order)
        initialized_steps = len(self.steps)
        
        success_rate = initialized_steps / total_steps if total_steps > 0 else 0
        self.logger.info(f"ğŸ“Š ì´ˆê¸°í™” ìƒíƒœ: {initialized_steps}/{total_steps} ({success_rate:.1%})")
        
        return success_rate
    
    # ==============================================
    # ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ
    # ==============================================
    
    async def process_complete_virtual_fitting(
        self,
        person_image: Union[str, Image.Image, np.ndarray],
        clothing_image: Union[str, Image.Image, np.ndarray],
        body_measurements: Optional[Dict[str, float]] = None,
        clothing_type: str = "shirt",
        fabric_type: str = "cotton",
        style_preferences: Optional[Dict[str, Any]] = None,
        quality_target: float = 0.8,
        progress_callback: Optional[Callable] = None,
        save_intermediate: bool = None,
        session_id: Optional[str] = None
    ) -> ProcessingResult:
        """
        ğŸ”¥ ì™„ì „í•œ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ - í†µí•© ì‹œìŠ¤í…œ ìš°ì„  ì‹¤í–‰
        
        âœ… í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ìš°ì„  ì‚¬ìš©
        âœ… ModelLoader í´ë°± ì§€ì›
        âœ… ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì‹¤í–‰ ì „ëµ
        âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
        âœ… ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬
        """
        if not self.is_initialized:
            raise RuntimeError("íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # ì„¤ì • ì²˜ë¦¬
        if save_intermediate is None:
            save_intermediate = self.config.save_intermediate
        
        if session_id is None:
            session_id = f"vf_{int(time.time())}_{np.random.randint(1000, 9999)}"
        
        start_time = time.time()
        self.current_status = ProcessingStatus.PROCESSING
        
        try:
            self.logger.info(f"ğŸ¯ í†µí•© 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì‹œì‘ - ì„¸ì…˜ ID: {session_id}")
            self.logger.info(f"âš™ï¸ ì„¤ì •: {clothing_type} ({fabric_type}), ëª©í‘œ í’ˆì§ˆ: {quality_target}")
            self.logger.info(f"ğŸ”§ ì‹¤í–‰ ëª¨ë“œ: {self.config.execution_mode.value}")
            self.logger.info(f"ğŸ”§ í†µí•© ì‹œìŠ¤í…œ: {'âœ…' if self.initialization_manager.unified_system_initialized else 'âŒ'}")
            self.logger.info(f"ğŸ”§ ModelLoader: {'âœ…' if self.initialization_manager.model_loader_initialized else 'âŒ'}")
            
            # 1. ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (í†µí•© ì‹œìŠ¤í…œ ìš°ì„ )
            person_tensor = self.data_converter.preprocess_image(person_image)
            clothing_tensor = self.data_converter.preprocess_image(clothing_image)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            person_tensor = person_tensor.to(self.device)
            clothing_tensor = clothing_tensor.to(self.device)
            
            # 2. ì„¸ì…˜ ë°ì´í„° ì´ˆê¸°í™”
            session_data = SessionData(
                session_id=session_id,
                start_time=start_time,
                status=ProcessingStatus.PROCESSING,
                metadata={
                    'clothing_type': clothing_type,
                    'fabric_type': fabric_type,
                    'quality_target': quality_target,
                    'style_preferences': style_preferences or {},
                    'body_measurements': body_measurements,
                    'device': self.device,
                    'execution_mode': self.config.execution_mode.value,
                    'unified_system_enabled': self.initialization_manager.unified_system_initialized,
                    'model_loader_enabled': self.initialization_manager.model_loader_initialized
                }
            )
            
            self.sessions[session_id] = session_data
            
            if progress_callback:
                await progress_callback("ì…ë ¥ ì „ì²˜ë¦¬ ì™„ë£Œ", 5)
            
            # 3. ë©”ëª¨ë¦¬ ìµœì í™”
            if self.config.memory_optimization:
                self.memory_manager.cleanup_memory()
            
            # ğŸ”¥ 4. 8ë‹¨ê³„ ìˆœì°¨ ì²˜ë¦¬ - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì‹¤í–‰
            step_results = {}
            execution_strategies = {}
            current_data = person_tensor
            
            for i, step_name in enumerate(self.step_order):
                if step_name not in self.steps:
                    self.logger.warning(f"âš ï¸ {step_name} ë‹¨ê³„ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆë›°ê¸°...")
                    continue
                
                step_start = time.time()
                step = self.steps[step_name]
                
                self.logger.info(f"ğŸ“‹ {i+1}/{len(self.step_order)} ë‹¨ê³„: {step_name} ì²˜ë¦¬ ì¤‘...")
                
                try:
                    # ğŸ”¥ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì‹¤í–‰
                    step_result, execution_strategy = await self.execution_manager.execute_step(
                        step, step_name, current_data, clothing_tensor,
                        body_measurements, clothing_type, fabric_type,
                        style_preferences, self.config.max_retries
                    )
                    
                    step_time = time.time() - step_start
                    step_results[step_name] = step_result
                    execution_strategies[step_name] = execution_strategy
                    
                    # ì„¸ì…˜ ë°ì´í„° ì—…ë°ì´íŠ¸
                    session_data.add_step_result(step_name, step_result, step_time, execution_strategy)
                    
                    # ê²°ê³¼ ì—…ë°ì´íŠ¸
                    if step_result.get('success', True):
                        result_data = step_result.get('result')
                        if result_data is not None:
                            current_data = result_data
                    
                    # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
                    if save_intermediate:
                        session_data.intermediate_results[step_name] = {
                            'result': current_data,
                            'metadata': step_result,
                            'execution_strategy': execution_strategy
                        }
                    
                    # ë¡œê¹…
                    confidence = step_result.get('confidence', 0.8)
                    quality_score = step_result.get('quality_score', confidence)
                    model_used = step_result.get('model_used', 'unknown')
                    processing_method = step_result.get('processing_method', 'unknown')
                    
                    strategy_icon = "ğŸ”—" if execution_strategy == "unified_system" else "ğŸ§ " if execution_strategy == "model_loader" else "ğŸ”„"
                    
                    self.logger.info(f"âœ… {i+1}ë‹¨ê³„ ì™„ë£Œ - ì‹œê°„: {step_time:.2f}ì´ˆ, ì‹ ë¢°ë„: {confidence:.3f}, í’ˆì§ˆ: {quality_score:.3f}")
                    self.logger.info(f"   {strategy_icon} ì‹¤í–‰ì „ëµ: {execution_strategy}, ëª¨ë¸: {model_used}, ë°©ë²•: {processing_method}")
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if progress_callback:
                        progress = 5 + (i + 1) * 85 // len(self.step_order)
                        await progress_callback(f"{step_name} ì™„ë£Œ ({execution_strategy})", progress)
                    
                    # ë©”ëª¨ë¦¬ ìµœì í™” (ì¤‘ê°„ ë‹¨ê³„)
                    if self.config.memory_optimization and i % 2 == 0:
                        self.memory_manager.cleanup_memory()
                    
                except Exception as e:
                    self.logger.error(f"âŒ {i+1}ë‹¨ê³„ ({step_name}) ì‹¤íŒ¨: {e}")
                    step_time = time.time() - step_start
                    step_results[step_name] = {
                        'success': False,
                        'error': str(e),
                        'processing_time': step_time,
                        'confidence': 0.0,
                        'quality_score': 0.0,
                        'model_used': 'error',
                        'processing_method': 'error'
                    }
                    execution_strategies[step_name] = "error"
                    
                    session_data.add_step_result(step_name, step_results[step_name], step_time, "error")
                    session_data.error_log.append(f"{step_name}: {str(e)}")
                    
                    # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                    continue
            
            # 5. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            total_time = time.time() - start_time
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
            if isinstance(current_data, torch.Tensor):
                result_image = self.data_converter.tensor_to_pil(current_data)
            else:
                result_image = Image.new('RGB', (512, 512), color='gray')
            
            # í’ˆì§ˆ í‰ê°€ ê°•í™”
            quality_score = self._assess_enhanced_quality(step_results, execution_strategies)
            quality_grade = self._get_quality_grade(quality_score)
            
            # ì„±ê³µ ì—¬ë¶€ ê²°ì •
            success = quality_score >= (quality_target * 0.8)
            
            # ì‹¤í–‰ ì „ëµ í†µê³„
            strategy_stats = self._calculate_strategy_statistics(execution_strategies)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            dominant_strategy = max(strategy_stats.items(), key=lambda x: x[1])[0] if strategy_stats else "unknown"
            self.performance_metrics.update(total_time, quality_score, success, dominant_strategy)
            
            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            session_data.status = ProcessingStatus.COMPLETED if success else ProcessingStatus.FAILED
            
            # ì„¸ì…˜ ë°ì´í„° ì •ë¦¬
            if not save_intermediate:
                self.sessions.pop(session_id, None)
            
            if progress_callback:
                await progress_callback("ì²˜ë¦¬ ì™„ë£Œ", 100)
            
            self.current_status = ProcessingStatus.IDLE
            
            # ê²°ê³¼ ë¡œê¹…
            self.logger.info(f"ğŸ‰ í†µí•© 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì™„ë£Œ!")
            self.logger.info(f"â±ï¸ ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
            self.logger.info(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f} ({quality_grade})")
            self.logger.info(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {'âœ…' if quality_score >= quality_target else 'âŒ'}")
            self.logger.info(f"ğŸ“‹ ì™„ë£Œëœ ë‹¨ê³„: {len(step_results)}/{len(self.step_order)}")
            self.logger.info(f"ğŸ”— ì‹¤í–‰ ì „ëµ í†µê³„: {strategy_stats}")
            
            return ProcessingResult(
                success=success,
                session_id=session_id,
                result_image=result_image,
                result_tensor=current_data if isinstance(current_data, torch.Tensor) else None,
                quality_score=quality_score,
                quality_grade=quality_grade,
                processing_time=total_time,
                step_results=step_results,
                step_timings=session_data.step_timings,
                execution_strategy=execution_strategies,
                metadata={
                    'device': self.device,
                    'device_type': self.device_type,
                    'is_m3_max': self.is_m3_max,
                    'execution_mode': self.config.execution_mode.value,
                    'quality_target': quality_target,
                    'quality_target_achieved': quality_score >= quality_target,
                    'total_steps': len(self.step_order),
                    'completed_steps': len(step_results),
                    'success_rate': len([r for r in step_results.values() if r.get('success', True)]) / len(step_results) if step_results else 0,
                    'unified_system_enabled': self.initialization_manager.unified_system_initialized,
                    'model_loader_enabled': self.initialization_manager.model_loader_initialized,
                    'strategy_statistics': strategy_stats,
                    'unified_system_usage_rate': strategy_stats.get('unified_system', 0) / len(step_results) * 100 if step_results else 0,
                    'model_loader_usage_rate': strategy_stats.get('model_loader', 0) / len(step_results) * 100 if step_results else 0,
                    'fallback_usage_rate': strategy_stats.get('fallback', 0) / len(step_results) * 100 if step_results else 0,
                    'session_data': session_data.__dict__ if save_intermediate else None
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ í†µí•© ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            
            # ì—ëŸ¬ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.performance_metrics.update(time.time() - start_time, 0.0, False, "error")
            
            self.current_status = ProcessingStatus.FAILED
            
            return ProcessingResult(
                success=False,
                session_id=session_id,
                processing_time=time.time() - start_time,
                error_message=str(e),
                metadata={
                    'device': self.device,
                    'error_type': type(e).__name__,
                    'error_location': traceback.format_exc(),
                    'execution_mode': self.config.execution_mode.value,
                    'unified_system_enabled': self.initialization_manager.unified_system_initialized,
                    'model_loader_enabled': self.initialization_manager.model_loader_initialized,
                    'session_data': self.sessions.get(session_id).__dict__ if session_id in self.sessions else None
                }
            )
    
    def _assess_enhanced_quality(self, step_results: Dict[str, Any], execution_strategies: Dict[str, str]) -> float:
        """ê°•í™”ëœ í’ˆì§ˆ í‰ê°€ - ì‹¤í–‰ ì „ëµ ê³ ë ¤"""
        if not step_results:
            return 0.5
        
        quality_scores = []
        confidence_scores = []
        strategy_bonus = 0.0
        
        for step_name, step_result in step_results.items():
            if isinstance(step_result, dict):
                confidence = step_result.get('confidence', 0.8)
                quality = step_result.get('quality_score', confidence)
                strategy = execution_strategies.get(step_name, 'unknown')
                
                quality_scores.append(quality)
                confidence_scores.append(confidence)
                
                # ì‹¤í–‰ ì „ëµë³„ ë³´ë„ˆìŠ¤
                if strategy == 'unified_system':
                    strategy_bonus += 0.05  # 5% ë³´ë„ˆìŠ¤
                elif strategy == 'model_loader':
                    strategy_bonus += 0.03  # 3% ë³´ë„ˆìŠ¤
                elif strategy == 'fallback':
                    strategy_bonus += 0.01  # 1% ë³´ë„ˆìŠ¤
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        if quality_scores:
            avg_quality = sum(quality_scores) / len(quality_scores)
            avg_confidence = sum(confidence_scores) / len(confidence_scores)
            
            # ê°€ì¤‘ í‰ê·  + ì „ëµ ë³´ë„ˆìŠ¤
            overall_score = avg_quality * 0.6 + avg_confidence * 0.4 + strategy_bonus
            return min(max(overall_score, 0.0), 1.0)
        
        return 0.5
    
    def _calculate_strategy_statistics(self, execution_strategies: Dict[str, str]) -> Dict[str, int]:
        """ì‹¤í–‰ ì „ëµ í†µê³„ ê³„ì‚°"""
        stats = {}
        for strategy in execution_strategies.values():
            stats[strategy] = stats.get(strategy, 0) + 1
        return stats
    
    def _get_quality_grade(self, quality_score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if quality_score >= 0.9:
            return "Excellent"
        elif quality_score >= 0.8:
            return "Good"
        elif quality_score >= 0.7:
            return "Fair"
        elif quality_score >= 0.6:
            return "Poor"
        else:
            return "Very Poor"
    
    # ==============================================
    # ğŸ”¥ ìƒíƒœ ì¡°íšŒ ë° ê´€ë¦¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ - í†µí•© ì‹œìŠ¤í…œ ì •ë³´ í¬í•¨"""
        return {
            'initialized': self.is_initialized,
            'current_status': self.current_status.value,
            'device': self.device,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'unified_system_initialized': self.initialization_manager.unified_system_initialized,
            'model_loader_initialized': self.initialization_manager.model_loader_initialized,
            'config': {
                'quality_level': self.config.quality_level.value,
                'processing_mode': self.config.processing_mode.value,
                'execution_mode': self.config.execution_mode.value,
                'optimization_enabled': self.config.optimization_enabled,
                'memory_optimization': self.config.memory_optimization,
                'use_fp16': self.config.use_fp16,
                'batch_size': self.config.batch_size,
                'unified_utils_enabled': self.config.unified_utils_enabled,
                'model_loader_enabled': self.config.model_loader_enabled,
                'auto_detect_models': self.config.auto_detect_models,
                'preload_critical_models': self.config.preload_critical_models,
                'fallback_enabled': self.config.fallback_enabled
            },
            'steps_status': {
                step_name: {
                    'loaded': step_name in self.steps,
                    'type': type(self.steps[step_name]).__name__ if step_name in self.steps else None,
                    'ready': step_name in self.steps and hasattr(self.steps[step_name], 'process'),
                    'has_unified_interface': (step_name in self.steps and 
                                            hasattr(self.steps[step_name], 'unified_interface') and 
                                            self.steps[step_name].unified_interface is not None),
                    'has_model_interface': (step_name in self.steps and 
                                          hasattr(self.steps[step_name], 'model_interface') and 
                                          self.steps[step_name].model_interface is not None)
                }
                for step_name in self.step_order
            },
            'performance_metrics': {
                'total_sessions': self.performance_metrics.total_sessions,
                'successful_sessions': self.performance_metrics.successful_sessions,
                'failed_sessions': self.performance_metrics.failed_sessions,
                'success_rate': self.performance_metrics.successful_sessions / self.performance_metrics.total_sessions if self.performance_metrics.total_sessions > 0 else 0,
                'average_processing_time': self.performance_metrics.average_processing_time,
                'average_quality_score': self.performance_metrics.average_quality_score,
                'unified_system_usage': self.performance_metrics.unified_system_usage,
                'model_loader_usage': self.performance_metrics.model_loader_usage,
                'fallback_usage': self.performance_metrics.fallback_usage
            },
            'memory_usage': self.memory_manager.get_memory_usage(),
            'active_sessions': len(self.sessions),
            'system_integration': {
                'unified_utils_available': UNIFIED_UTILS_AVAILABLE,
                'model_loader_available': MODEL_LOADER_AVAILABLE,
                'step_requests_available': STEP_REQUESTS_AVAILABLE,
                'auto_detector_available': AUTO_DETECTOR_AVAILABLE,
                'step_classes_available': STEP_CLASSES_AVAILABLE
            }
        }
    
    def get_session_info(self, session_id: str) -> Optional[Dict[str, Any]]:
        """ì„¸ì…˜ ì •ë³´ ì¡°íšŒ"""
        session = self.sessions.get(session_id)
        if session:
            return session.__dict__
        return None
    
    def list_active_sessions(self) -> List[Dict[str, Any]]:
        """í™œì„± ì„¸ì…˜ ëª©ë¡"""
        return [
            {
                'session_id': session_id,
                'status': session.status.value,
                'start_time': session.start_time,
                'elapsed_time': time.time() - session.start_time,
                'completed_steps': len(session.step_results),
                'total_steps': len(self.step_order),
                'execution_strategies': session.execution_strategies
            }
            for session_id, session in self.sessions.items()
        ]
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´ - ì‹¤í–‰ ì „ëµ í†µê³„ í¬í•¨"""
        return {
            'total_sessions': self.performance_metrics.total_sessions,
            'success_rate': self.performance_metrics.successful_sessions / self.performance_metrics.total_sessions if self.performance_metrics.total_sessions > 0 else 0,
            'average_processing_time': self.performance_metrics.average_processing_time,
            'average_quality_score': self.performance_metrics.average_quality_score,
            'fastest_time': self.performance_metrics.fastest_processing_time if self.performance_metrics.fastest_processing_time != float('inf') else 0,
            'slowest_time': self.performance_metrics.slowest_processing_time,
            'total_processing_time': self.performance_metrics.total_processing_time,
            'active_sessions': len(self.sessions),
            'execution_strategy_stats': {
                'unified_system_usage': self.performance_metrics.unified_system_usage,
                'model_loader_usage': self.performance_metrics.model_loader_usage,
                'fallback_usage': self.performance_metrics.fallback_usage,
                'unified_system_rate': self.performance_metrics.unified_system_usage / self.performance_metrics.total_sessions * 100 if self.performance_metrics.total_sessions > 0 else 0,
                'model_loader_rate': self.performance_metrics.model_loader_usage / self.performance_metrics.total_sessions * 100 if self.performance_metrics.total_sessions > 0 else 0,
                'fallback_rate': self.performance_metrics.fallback_usage / self.performance_metrics.total_sessions * 100 if self.performance_metrics.total_sessions > 0 else 0
            },
            'device_info': {
                'device': self.device,
                'device_type': self.device_type,
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb
            }
        }
    
    def clear_session_history(self, keep_recent: int = 10):
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì •ë¦¬"""
        try:
            if len(self.sessions) <= keep_recent:
                return
            
            # ìµœê·¼ ì„¸ì…˜ë“¤ë§Œ ìœ ì§€
            sorted_sessions = sorted(
                self.sessions.items(),
                key=lambda x: x[1].start_time,
                reverse=True
            )
            
            sessions_to_keep = dict(sorted_sessions[:keep_recent])
            cleared_count = len(self.sessions) - len(sessions_to_keep)
            
            self.sessions = sessions_to_keep
            
            self.logger.info(f"ğŸ§¹ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì •ë¦¬ ì™„ë£Œ: {cleared_count}ê°œ ì„¸ì…˜ ì œê±°")
            
        except Exception as e:
            self.logger.error(f"âŒ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def warmup(self):
        """íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… - í†µí•© ì‹œìŠ¤í…œ í¬í•¨"""
        try:
            self.logger.info("ğŸ”¥ í†µí•© íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… ì‹œì‘...")
            
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
            dummy_person = Image.new('RGB', (512, 512), color=(100, 150, 200))
            dummy_cloth = Image.new('RGB', (512, 512), color=(200, 100, 100))
            
            # ì›Œë°ì—… ì‹¤í–‰
            result = await self.process_complete_virtual_fitting(
                person_image=dummy_person,
                clothing_image=dummy_cloth,
                clothing_type='shirt',
                fabric_type='cotton',
                quality_target=0.6,
                save_intermediate=False,
                session_id="warmup_session"
            )
            
            if result.success:
                self.logger.info(f"âœ… ì›Œë°ì—… ì™„ë£Œ - ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
                self.logger.info(f"ğŸ”— ì‹¤í–‰ ì „ëµ í†µê³„: {result.metadata.get('strategy_statistics', {})}")
                self.logger.info(f"ğŸ”§ í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš©ë¥ : {result.metadata.get('unified_system_usage_rate', 0):.1f}%")
                return True
            else:
                self.logger.warning(f"âš ï¸ ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜: {result.error_message}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return False
    
    async def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ì²´í¬ - í†µí•© ì‹œìŠ¤í…œ ìƒíƒœ í¬í•¨"""
        try:
            health_status = {
                'status': 'healthy',
                'timestamp': datetime.now().isoformat(),
                'pipeline_initialized': self.is_initialized,
                'current_status': self.current_status.value,
                'device': self.device,
                'unified_system_initialized': self.initialization_manager.unified_system_initialized,
                'model_loader_initialized': self.initialization_manager.model_loader_initialized,
                'checks': {}
            }
            
            # Stepë³„ ì²´í¬
            steps_healthy = 0
            steps_with_unified_interface = 0
            steps_with_model_interface = 0
            
            for step_name in self.step_order:
                if step_name in self.steps:
                    step = self.steps[step_name]
                    has_process = hasattr(step, 'process')
                    has_unified_interface = hasattr(step, 'unified_interface') and step.unified_interface
                    has_model_interface = hasattr(step, 'model_interface') and step.model_interface
                    
                    if has_process:
                        steps_healthy += 1
                    if has_unified_interface:
                        steps_with_unified_interface += 1
                    if has_model_interface:
                        steps_with_model_interface += 1
            
            health_status['checks']['steps'] = {
                'status': 'ok' if steps_healthy >= len(self.step_order) * 0.8 else 'warning',
                'healthy_steps': steps_healthy,
                'total_steps': len(self.step_order),
                'steps_with_unified_interface': steps_with_unified_interface,
                'steps_with_model_interface': steps_with_model_interface,
                'unified_interface_coverage': f"{steps_with_unified_interface}/{len(self.step_order)}",
                'model_interface_coverage': f"{steps_with_model_interface}/{len(self.step_order)}"
            }
            
            # í†µí•© ì‹œìŠ¤í…œ ì²´í¬
            health_status['checks']['unified_system'] = {
                'status': 'ok' if self.initialization_manager.unified_system_initialized else 'warning',
                'initialized': self.initialization_manager.unified_system_initialized,
                'utils_available': UNIFIED_UTILS_AVAILABLE,
                'auto_detector_available': AUTO_DETECTOR_AVAILABLE
            }
            
            # ModelLoader ì‹œìŠ¤í…œ ì²´í¬
            health_status['checks']['model_loader'] = {
                'status': 'ok' if self.initialization_manager.model_loader_initialized else 'warning',
                'initialized': self.initialization_manager.model_loader_initialized,
                'available': MODEL_LOADER_AVAILABLE,
                'step_requests_available': STEP_REQUESTS_AVAILABLE
            }
            
            # ë©”ëª¨ë¦¬ ì²´í¬
            try:
                memory_usage = self.memory_manager.get_memory_usage()
                health_status['checks']['memory'] = {
                    'status': 'ok',
                    'usage': memory_usage
                }
            except Exception as e:
                health_status['checks']['memory'] = {
                    'status': 'error',
                    'error': str(e)
                }
            
            # ì „ì²´ ìƒíƒœ ê²°ì •
            check_statuses = [check.get('status', 'error') for check in health_status['checks'].values()]
            if 'error' in check_statuses:
                health_status['status'] = 'unhealthy'
            elif 'warning' in check_statuses:
                health_status['status'] = 'degraded'
            
            return health_status
            
        except Exception as e:
            return {
                'status': 'error',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - í†µí•© ì‹œìŠ¤í…œ í¬í•¨"""
        try:
            self.logger.info("ğŸ§¹ í†µí•© íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            self.current_status = ProcessingStatus.CLEANING
            
            # 1. ê° Step ì •ë¦¬
            for step_name, step in self.steps.items():
                try:
                    # Stepì˜ í†µí•© ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
                    if hasattr(step, 'unified_interface') and step.unified_interface:
                        try:
                            if hasattr(step.unified_interface, 'cleanup'):
                                await step.unified_interface.cleanup()
                            self.logger.info(f"âœ… {step_name} í†µí•© ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ {step_name} í†µí•© ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                    
                    # Stepì˜ model_interface ì •ë¦¬
                    if hasattr(step, 'model_interface') and step.model_interface:
                        try:
                            if hasattr(step.model_interface, 'unload_models'):
                                await step.model_interface.unload_models()
                            self.logger.info(f"âœ… {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ {step_name} ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                    
                    # Step ìì²´ ì •ë¦¬
                    if hasattr(step, 'cleanup'):
                        await step.cleanup()
                    self.logger.info(f"âœ… {step_name} ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 2. í†µí•© ì‹œìŠ¤í…œ ì •ë¦¬
            if self.initialization_manager.utils_manager:
                try:
                    self.initialization_manager.utils_manager.cleanup()
                    self.logger.info("âœ… í†µí•© ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í†µí•© ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 3. ModelLoader ì‹œìŠ¤í…œ ì •ë¦¬
            if self.initialization_manager.model_loader:
                try:
                    if hasattr(self.initialization_manager.model_loader, 'cleanup'):
                        await self.initialization_manager.model_loader.cleanup()
                    self.logger.info("âœ… ModelLoader ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ModelLoader ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 4. ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì •ë¦¬
            try:
                self.memory_manager.cleanup_memory()
                self.logger.info("âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 5. ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            if hasattr(self, 'thread_pool'):
                try:
                    self.thread_pool.shutdown(wait=True)
                    self.logger.info("âœ… ìŠ¤ë ˆë“œ í’€ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œ í’€ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 6. ì„¸ì…˜ ë°ì´í„° ì •ë¦¬
            try:
                self.sessions.clear()
                self.logger.info("âœ… ì„¸ì…˜ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì„¸ì…˜ ë°ì´í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 7. ìƒíƒœ ì´ˆê¸°í™”
            self.is_initialized = False
            self.initialization_manager.unified_system_initialized = False
            self.initialization_manager.model_loader_initialized = False
            self.current_status = ProcessingStatus.IDLE
            
            self.logger.info("âœ… í†µí•© íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.current_status = ProcessingStatus.FAILED

# ==============================================
# ğŸ”¥ 5. í¸ì˜ í•¨ìˆ˜ë“¤ (ëª¨ë“  ê¸°ì¡´ í•¨ìˆ˜ëª… 100% ìœ ì§€)
# ==============================================

def create_pipeline(
    device: str = "auto",
    quality_level: str = "balanced",
    processing_mode: str = "production",
    execution_mode: str = "unified_system",
    **kwargs
) -> PipelineManager:
    """íŒŒì´í”„ë¼ì¸ ìƒì„± í¸ì˜ í•¨ìˆ˜ - í†µí•© ì‹œìŠ¤í…œ ìš°ì„ """
    return PipelineManager(
        device=device,
        config=PipelineConfig(
            quality_level=QualityLevel(quality_level),
            processing_mode=PipelineMode(processing_mode),
            execution_mode=ExecutionMode(execution_mode),
            **kwargs
        )
    )

def create_development_pipeline(**kwargs) -> PipelineManager:
    """ê°œë°œìš© íŒŒì´í”„ë¼ì¸ ìƒì„± - í†µí•© ì‹œìŠ¤í…œ í™œì„±í™”"""
    return create_pipeline(
        quality_level="fast",
        processing_mode="development",
        execution_mode="unified_system",
        optimization_enabled=False,
        save_intermediate=True,
        enable_caching=False,
        unified_utils_enabled=True,
        model_loader_enabled=True,
        auto_detect_models=True,
        preload_critical_models=False,
        fallback_enabled=True,
        **kwargs
    )

def create_production_pipeline(**kwargs) -> PipelineManager:
    """í”„ë¡œë•ì…˜ìš© íŒŒì´í”„ë¼ì¸ ìƒì„± - í†µí•© ì‹œìŠ¤í…œ ìš°ì„ , ëª¨ë“  í´ë°± í™œì„±í™”"""
    return create_pipeline(
        quality_level="high",
        processing_mode="production",
        execution_mode="unified_system",
        optimization_enabled=True,
        memory_optimization=True,
        enable_caching=True,
        parallel_processing=True,
        unified_utils_enabled=True,
        model_loader_enabled=True,
        auto_detect_models=True,
        preload_critical_models=True,
        model_cache_warmup=True,
        fallback_enabled=True,
        retry_with_fallback=True,
        **kwargs
    )

def create_m3_max_pipeline(**kwargs) -> PipelineManager:
    """M3 Max ìµœì í™” íŒŒì´í”„ë¼ì¸ ìƒì„± - í†µí•© ì‹œìŠ¤í…œ ìµœëŒ€ í™œìš©"""
    return PipelineManager(
        device="mps",
        config=PipelineConfig(
            quality_level=QualityLevel.HIGH,
            processing_mode=PipelineMode.PRODUCTION,
            execution_mode=ExecutionMode.UNIFIED_SYSTEM,
            memory_gb=128.0,
            is_m3_max=True,
            optimization_enabled=True,
            use_fp16=True,
            batch_size=4,
            memory_optimization=True,
            enable_caching=True,
            parallel_processing=True,
            model_cache_size=15,
            gpu_memory_fraction=0.95,
            unified_utils_enabled=True,
            model_loader_enabled=True,
            auto_detect_models=True,
            preload_critical_models=True,
            model_cache_warmup=True,
            step_model_validation=True,
            fallback_enabled=True,
            retry_with_fallback=True,
            **kwargs
        )
    )

def create_testing_pipeline(**kwargs) -> PipelineManager:
    """í…ŒìŠ¤íŠ¸ìš© íŒŒì´í”„ë¼ì¸ ìƒì„± - ë¹ ë¥¸ ì‹¤í–‰ ìš°ì„ """
    return create_pipeline(
        quality_level="fast",
        processing_mode="testing",
        execution_mode="model_loader",  # í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ModelLoader ìš°ì„ 
        optimization_enabled=False,
        save_intermediate=True,
        enable_caching=False,
        max_retries=1,
        timeout_seconds=60,
        unified_utils_enabled=True,
        model_loader_enabled=True,
        auto_detect_models=False,
        preload_critical_models=False,
        fallback_enabled=True,
        **kwargs
    )

def create_unified_first_pipeline(**kwargs) -> PipelineManager:
    """í†µí•© ì‹œìŠ¤í…œ ìš°ì„  íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    return create_pipeline(
        execution_mode="unified_system",
        unified_utils_enabled=True,
        model_loader_enabled=True,
        fallback_enabled=True,
        **kwargs
    )

def create_model_loader_first_pipeline(**kwargs) -> PipelineManager:
    """ModelLoader ìš°ì„  íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    return create_pipeline(
        execution_mode="model_loader",
        unified_utils_enabled=True,
        model_loader_enabled=True,
        fallback_enabled=True,
        **kwargs
    )

@lru_cache(maxsize=1)
def get_global_pipeline_manager(device: str = "auto") -> PipelineManager:
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ - í†µí•© ì‹œìŠ¤í…œ ìš°ì„ """
    try:
        if device == "mps" and torch.backends.mps.is_available():
            return create_m3_max_pipeline()
        else:
            return create_production_pipeline(device=device)
    except Exception as e:
        logger.error(f"ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ìƒì„± ì‹¤íŒ¨: {e}")
        return create_pipeline(device="cpu", quality_level="fast")

# í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ìœ ì§€)
def get_human_parsing_step():
    """ê¸°ì¡´ í˜¸í™˜ì„± - HumanParsingStep ë°˜í™˜"""
    return HumanParsingStep

def get_pose_estimation_step():
    """ê¸°ì¡´ í˜¸í™˜ì„± - PoseEstimationStep ë°˜í™˜"""
    return PoseEstimationStep

def get_cloth_segmentation_step():
    """ê¸°ì¡´ í˜¸í™˜ì„± - ClothSegmentationStep ë°˜í™˜"""
    return ClothSegmentationStep

def get_geometric_matching_step():
    """ê¸°ì¡´ í˜¸í™˜ì„± - GeometricMatchingStep ë°˜í™˜"""
    return GeometricMatchingStep

def get_cloth_warping_step():
    """ê¸°ì¡´ í˜¸í™˜ì„± - ClothWarpingStep ë°˜í™˜"""
    return ClothWarpingStep

def get_virtual_fitting_step():
    """ê¸°ì¡´ í˜¸í™˜ì„± - VirtualFittingStep ë°˜í™˜"""
    return VirtualFittingStep

def get_post_processing_step():
    """ê¸°ì¡´ í˜¸í™˜ì„± - PostProcessingStep ë°˜í™˜"""
    return PostProcessingStep

def get_quality_assessment_step():
    """ê¸°ì¡´ í˜¸í™˜ì„± - QualityAssessmentStep ë°˜í™˜"""
    return QualityAssessmentStep

# ==============================================
# ğŸ”¥ 6. ë°ëª¨ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ==============================================

async def demo_unified_pipeline():
    """ğŸ”¥ í†µí•© PipelineManager ë°ëª¨"""
    
    print("ğŸ¯ í†µí•© PipelineManager ë°ëª¨ ì‹œì‘")
    print("=" * 80)
    print("âœ… í”„ë¡œì íŠ¸ í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ìš°ì„  ì‚¬ìš©")
    print("âœ… ModelLoader ì‹œìŠ¤í…œ ì™„ë²½ ì—°ë™")
    print("âœ… ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì‹¤í–‰ ì „ëµ")
    print("âœ… ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°±")
    print("âœ… ëª¨ë“ˆí™”ëœ ê´€ë¦¬ êµ¬ì¡°")
    print("âœ… M3 Max + conda í™˜ê²½ ìµœì í™”")
    print("=" * 80)
    
    # 1. í†µí•© íŒŒì´í”„ë¼ì¸ ìƒì„±
    print("1ï¸âƒ£ í†µí•© íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")
    pipeline = create_m3_max_pipeline()
    
    # 2. ì´ˆê¸°í™”
    print("2ï¸âƒ£ í†µí•© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
    success = await pipeline.initialize()
    if not success:
        print("âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # 3. ìƒíƒœ í™•ì¸
    print("3ï¸âƒ£ í†µí•© íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸...")
    status = pipeline.get_pipeline_status()
    print(f"ğŸ“Š ì´ˆê¸°í™” ìƒíƒœ: {status['initialized']}")
    print(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {status['device']} ({status['device_type']})")
    print(f"âš™ï¸ ì‹¤í–‰ ëª¨ë“œ: {status['config']['execution_mode']}")
    print(f"ğŸ”§ í†µí•© ì‹œìŠ¤í…œ: {'âœ…' if status['unified_system_initialized'] else 'âŒ'}")
    print(f"ğŸ”§ ModelLoader: {'âœ…' if status['model_loader_initialized'] else 'âŒ'}")
    print(f"ğŸ“‹ ë¡œë“œëœ ë‹¨ê³„: {len([s for s in status['steps_status'].values() if s['loaded']])}/{len(status['steps_status'])}")
    
    # 4. Stepë³„ ì¸í„°í˜ì´ìŠ¤ ìƒíƒœ ì¶œë ¥
    print("4ï¸âƒ£ Stepë³„ ì¸í„°í˜ì´ìŠ¤ ìƒíƒœ:")
    for step_name, step_status in status['steps_status'].items():
        status_icon = "âœ…" if step_status['loaded'] else "âŒ"
        unified_icon = "ğŸ”—" if step_status.get('has_unified_interface', False) else "â­•"
        model_icon = "ğŸ§ " if step_status.get('has_model_interface', False) else "â­•"
        print(f"  {status_icon} {unified_icon} {model_icon} {step_name}")
        print(f"      í†µí•© ì¸í„°í˜ì´ìŠ¤: {'ìˆìŒ' if step_status.get('has_unified_interface', False) else 'ì—†ìŒ'}")
        print(f"      ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤: {'ìˆìŒ' if step_status.get('has_model_interface', False) else 'ì—†ìŒ'}")
    
    # 5. í—¬ìŠ¤ì²´í¬
    print("5ï¸âƒ£ í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰...")
    health = await pipeline.health_check()
    print(f"ğŸ¥ í—¬ìŠ¤ ìƒíƒœ: {health['status']}")
    print(f"ğŸ“Š ê±´ê°•í•œ Step: {health['checks']['steps']['healthy_steps']}/{health['checks']['steps']['total_steps']}")
    print(f"ğŸ”— í†µí•© ì¸í„°í˜ì´ìŠ¤ ì»¤ë²„ë¦¬ì§€: {health['checks']['steps']['unified_interface_coverage']}")
    print(f"ğŸ§  ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì»¤ë²„ë¦¬ì§€: {health['checks']['steps']['model_interface_coverage']}")
    
    # 6. í†µí•© ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ ê°€ìƒ í”¼íŒ… ì‹¤í–‰
    print("6ï¸âƒ£ í†µí•© ì‹œìŠ¤í…œ ìš°ì„  ê°€ìƒ í”¼íŒ… ì‹¤í–‰...")
    
    try:
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        person_image = Image.new('RGB', (512, 512), color=(100, 150, 200))
        clothing_image = Image.new('RGB', (512, 512), color=(200, 100, 100))
        
        # ì§„í–‰ë¥  ì½œë°±
        async def progress_callback(message: str, percentage: int):
            print(f"ğŸ”„ {message}: {percentage}%")
        
        # ê°€ìƒ í”¼íŒ… ì²˜ë¦¬
        result = await pipeline.process_complete_virtual_fitting(
            person_image=person_image,
            clothing_image=clothing_image,
            clothing_type='shirt',
            fabric_type='cotton',
            body_measurements={'height': 175, 'weight': 70, 'chest': 95},
            style_preferences={'fit': 'regular', 'color': 'original'},
            quality_target=0.8,
            progress_callback=progress_callback,
            save_intermediate=True
        )
        
        if result.success:
            print(f"âœ… í†µí•© ê°€ìƒ í”¼íŒ… ì„±ê³µ!")
            print(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {result.quality_score:.3f} ({result.quality_grade})")
            print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
            print(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {'âœ…' if result.quality_score >= 0.8 else 'âŒ'}")
            print(f"ğŸ“‹ ì™„ë£Œëœ ë‹¨ê³„: {len(result.step_results)}/{len(pipeline.step_order)}")
            
            # ì‹¤í–‰ ì „ëµ í†µê³„
            strategy_stats = result.metadata.get('strategy_statistics', {})
            print(f"ğŸ”— ì‹¤í–‰ ì „ëµ í†µê³„: {strategy_stats}")
            print(f"ğŸ”§ í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš©ë¥ : {result.metadata.get('unified_system_usage_rate', 0):.1f}%")
            print(f"ğŸ§  ModelLoader ì‚¬ìš©ë¥ : {result.metadata.get('model_loader_usage_rate', 0):.1f}%")
            print(f"ğŸ”„ í´ë°± ì‚¬ìš©ë¥ : {result.metadata.get('fallback_usage_rate', 0):.1f}%")
            
            # ë‹¨ê³„ë³„ ê²°ê³¼ ì¶œë ¥
            print("\nğŸ“‹ ë‹¨ê³„ë³„ ì‹¤í–‰ ì „ëµ ê²°ê³¼:")
            for step_name, step_result in result.step_results.items():
                success_icon = "âœ…" if step_result.get('success', True) else "âŒ"
                confidence = step_result.get('confidence', 0.0)
                timing = result.step_timings.get(step_name, 0.0)
                strategy = result.execution_strategy.get(step_name, 'unknown')
                model_used = step_result.get('model_used', 'unknown')
                
                # ì „ëµë³„ ì•„ì´ì½˜
                strategy_icon = "ğŸ”—" if strategy == "unified_system" else "ğŸ§ " if strategy == "model_loader" else "ğŸ”„"
                
                print(f"  {success_icon} {strategy_icon} {step_name}: {confidence:.3f} ({timing:.2f}s)")
                print(f"      ì „ëµ: {strategy}, ëª¨ë¸: {model_used}")
            
            # ê²°ê³¼ ì €ì¥
            if result.result_image:
                result.result_image.save('demo_unified_result.jpg')
                print("ğŸ’¾ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥: demo_unified_result.jpg")
        else:
            print(f"âŒ í†µí•© ê°€ìƒ í”¼íŒ… ì‹¤íŒ¨: {result.error_message}")
    
    except Exception as e:
        print(f"ğŸ’¥ ì˜ˆì™¸ ë°œìƒ: {e}")
    
    # 7. ì„±ëŠ¥ ìš”ì•½
    print("7ï¸âƒ£ ì„±ëŠ¥ ìš”ì•½...")
    performance = pipeline.get_performance_summary()
    print(f"ğŸ“ˆ ì´ ì„¸ì…˜: {performance['total_sessions']}")
    print(f"ğŸ“Š ì„±ê³µë¥ : {performance['success_rate']:.1%}")
    print(f"â±ï¸ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {performance['average_processing_time']:.2f}ì´ˆ")
    print(f"ğŸ¯ í‰ê·  í’ˆì§ˆ ì ìˆ˜: {performance['average_quality_score']:.3f}")
    
    # ì‹¤í–‰ ì „ëµ í†µê³„
    strategy_stats = performance['execution_strategy_stats']
    print(f"ğŸ”— í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš©: {strategy_stats['unified_system_usage']}íšŒ ({strategy_stats['unified_system_rate']:.1f}%)")
    print(f"ğŸ§  ModelLoader ì‚¬ìš©: {strategy_stats['model_loader_usage']}íšŒ ({strategy_stats['model_loader_rate']:.1f}%)")
    print(f"ğŸ”„ í´ë°± ì‚¬ìš©: {strategy_stats['fallback_usage']}íšŒ ({strategy_stats['fallback_rate']:.1f}%)")
    
    # 8. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    print("8ï¸âƒ£ ë¦¬ì†ŒìŠ¤ ì •ë¦¬...")
    await pipeline.cleanup()
    print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
    
    print("\nğŸ‰ í†µí•© PipelineManager ë°ëª¨ ì™„ë£Œ!")
    print("âœ… ëª¨ë“  í†µí•© ê¸°ëŠ¥ì´ ì„±ê³µì ìœ¼ë¡œ ì‘ë™í–ˆìŠµë‹ˆë‹¤!")
    print("ğŸ”— í†µí•© ì‹œìŠ¤í…œ ìš°ì„  ì‹¤í–‰ìœ¼ë¡œ ìµœê³  í’ˆì§ˆ ë‹¬ì„±!")

async def test_execution_strategies():
    """ì‹¤í–‰ ì „ëµ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ”¬ ì‹¤í–‰ ì „ëµ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    strategies = [
        ("unified_system", "í†µí•© ì‹œìŠ¤í…œ ìš°ì„ "),
        ("model_loader", "ModelLoader ìš°ì„ "),
        ("fallback", "í´ë°± ëª¨ë“œ")
    ]
    
    results = {}
    
    for strategy_mode, strategy_desc in strategies:
        print(f"\nğŸ¯ {strategy_desc} í…ŒìŠ¤íŠ¸...")
        
        try:
            # ì „ëµë³„ íŒŒì´í”„ë¼ì¸ ìƒì„±
            pipeline = create_pipeline(
                device="cpu",
                execution_mode=strategy_mode,
                quality_level="fast"
            )
            
            # ì´ˆê¸°í™”
            success = await pipeline.initialize()
            if not success:
                print(f"âŒ {strategy_desc} ì´ˆê¸°í™” ì‹¤íŒ¨")
                continue
            
            # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
            dummy_person = Image.new('RGB', (256, 256), color=(100, 150, 200))
            dummy_cloth = Image.new('RGB', (256, 256), color=(200, 100, 100))
            
            # ê°€ìƒ í”¼íŒ… ì‹¤í–‰
            result = await pipeline.process_complete_virtual_fitting(
                person_image=dummy_person,
                clothing_image=dummy_cloth,
                quality_target=0.6,
                save_intermediate=False,
                session_id=f"test_{strategy_mode}"
            )
            
            if result.success:
                print(f"âœ… {strategy_desc} ì„±ê³µ")
                strategy_stats = result.metadata.get('strategy_statistics', {})
                print(f"   ì‹¤í–‰ ì „ëµ í†µê³„: {strategy_stats}")
                print(f"   ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
                print(f"   í’ˆì§ˆ ì ìˆ˜: {result.quality_score:.3f}")
                
                results[strategy_mode] = {
                    'success': True,
                    'time': result.processing_time,
                    'quality': result.quality_score,
                    'strategies': strategy_stats
                }
            else:
                print(f"âŒ {strategy_desc} ì‹¤íŒ¨: {result.error_message}")
                results[strategy_mode] = {
                    'success': False,
                    'error': result.error_message
                }
            
            # ì •ë¦¬
            await pipeline.cleanup()
            
        except Exception as e:
            print(f"âŒ {strategy_desc} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results[strategy_mode] = {
                'success': False,
                'error': str(e)
            }
    
    # ê²°ê³¼ ìš”ì•½
    print("\nğŸ“Š ì‹¤í–‰ ì „ëµ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    for strategy_mode, strategy_desc in strategies:
        result = results.get(strategy_mode, {})
        if result.get('success', False):
            print(f"âœ… {strategy_desc}:")
            print(f"   ì²˜ë¦¬ ì‹œê°„: {result['time']:.2f}ì´ˆ")
            print(f"   í’ˆì§ˆ ì ìˆ˜: {result['quality']:.3f}")
            print(f"   ì „ëµ ë¶„í¬: {result['strategies']}")
        else:
            print(f"âŒ {strategy_desc}: {result.get('error', 'Unknown error')}")
    
    print("âœ… ì‹¤í–‰ ì „ëµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

# ==============================================
# ğŸ”¥ 7. Export ë° ë©”ì¸ ì‹¤í–‰
# ==============================================

# Export ëª©ë¡ (ëª¨ë“  ê¸°ì¡´ í•­ëª© + ìƒˆë¡œìš´ í•­ëª©)
__all__ = [
    # ì—´ê±°í˜•
    'PipelineMode', 'QualityLevel', 'ProcessingStatus', 'ExecutionMode',
    
    # ë°ì´í„° í´ë˜ìŠ¤
    'PipelineConfig', 'ProcessingResult', 'SessionData', 'PerformanceMetrics',
    
    # ë©”ì¸ í´ë˜ìŠ¤
    'PipelineManager',
    
    # ê´€ë¦¬ í´ë˜ìŠ¤ë“¤
    'InitializationManager', 'ExecutionManager', 'OptimizedDataConverter', 'OptimizedMemoryManager',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ëª¨ë“  ê¸°ì¡´ í•¨ìˆ˜ + ìƒˆë¡œìš´ í•¨ìˆ˜ë“¤)
    'create_pipeline', 'create_development_pipeline', 'create_production_pipeline',
    'create_m3_max_pipeline', 'create_testing_pipeline', 'get_global_pipeline_manager',
    'create_unified_first_pipeline', 'create_model_loader_first_pipeline',
    
    # í•˜ìœ„ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
    'get_human_parsing_step', 'get_pose_estimation_step', 'get_cloth_segmentation_step',
    'get_geometric_matching_step', 'get_cloth_warping_step', 'get_virtual_fitting_step',
    'get_post_processing_step', 'get_quality_assessment_step'
]

if __name__ == "__main__":
    print("ğŸ”¥ ì™„ì „ í†µí•© PipelineManager - ë‘ ë²„ì „ ìµœì  í•©ì„±")
    print("=" * 80)
    print("âœ… í”„ë¡œì íŠ¸ í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ìš°ì„  ì‚¬ìš©")
    print("âœ… ModelLoader ì‹œìŠ¤í…œ ì™„ë²½ ì—°ë™")
    print("âœ… ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì‹¤í–‰ ì „ëµ")
    print("âœ… ëª¨ë“ˆí™”ëœ ê´€ë¦¬ êµ¬ì¡°")
    print("âœ… ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°±")
    print("âœ… M3 Max + conda í™˜ê²½ ìµœì í™”")
    print("âœ… ëª¨ë“  ê¸°ì¡´ í•¨ìˆ˜/í´ë˜ìŠ¤ëª… 100% ìœ ì§€")
    print("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
    print("=" * 80)
    
    import asyncio
    
    async def main():
        # 1. í†µí•© ë°ëª¨ ì‹¤í–‰
        await demo_unified_pipeline()
        
        print("\n" + "="*50)
        
        # 2. ì‹¤í–‰ ì „ëµ í…ŒìŠ¤íŠ¸
        await test_execution_strategies()
    
    # ì‹¤í–‰
    asyncio.run(main())

# ==============================================
# ğŸ”¥ 8. ë¡œê¹… ë° ì´ˆê¸°í™” ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

logger.info("ğŸ‰ ì™„ì „ í†µí•© PipelineManager ë¡œë“œ ì™„ë£Œ!")
logger.info("âœ… ì£¼ìš” í†µí•© ê¸°ëŠ¥:")
logger.info("   - í”„ë¡œì íŠ¸ í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ìš°ì„  ì‚¬ìš©")
logger.info("   - ModelLoader ì‹œìŠ¤í…œ ì™„ë²½ ì—°ë™")
logger.info("   - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì‹¤í–‰ ì „ëµ (unified_system â†’ model_loader â†’ fallback)")
logger.info("   - ëª¨ë“ˆí™”ëœ ê´€ë¦¬ êµ¬ì¡° (InitializationManager, ExecutionManager)")
logger.info("   - ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°± ë©”ì»¤ë‹ˆì¦˜")
logger.info("   - M3 Max + conda í™˜ê²½ íŠ¹í™” ìµœì í™”")
logger.info("   - ëª¨ë“  ê¸°ì¡´ í•¨ìˆ˜/í´ë˜ìŠ¤ëª… 100% ìœ ì§€")
logger.info("   - ì‹¤í–‰ ì „ëµë³„ í†µê³„ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
logger.info("ğŸš€ ì´ì œ ìµœê³  í’ˆì§ˆì˜ í†µí•© ê°€ìƒ í”¼íŒ…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ê°€ìš©ì„±:")
logger.info(f"   - í†µí•© ìœ í‹¸ë¦¬í‹°: {'âœ…' if UNIFIED_UTILS_AVAILABLE else 'âŒ'}")
logger.info(f"   - ModelLoader: {'âœ…' if MODEL_LOADER_AVAILABLE else 'âŒ'}")
logger.info(f"   - Step ìš”ì²­: {'âœ…' if STEP_REQUESTS_AVAILABLE else 'âŒ'}")
logger.info(f"   - ìë™ íƒì§€: {'âœ…' if AUTO_DETECTOR_AVAILABLE else 'âŒ'}")
logger.info(f"   - Step í´ë˜ìŠ¤: {'âœ…' if STEP_CLASSES_AVAILABLE else 'âŒ'}")
logger.info("ğŸ¯ ê¶Œì¥ ì‚¬ìš©ë²•: create_m3_max_pipeline() ë˜ëŠ” create_production_pipeline()")