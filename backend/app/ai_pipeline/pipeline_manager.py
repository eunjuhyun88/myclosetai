"""
ì™„ì „ í†µí•©ëœ 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € - ëª¨ë“  ê¸°ëŠ¥ í¬í•¨
- 1ë²ˆ íŒŒì¼ì˜ pipeline_routes.py ì™„ì „ í˜¸í™˜ì„±
- 2ë²ˆ íŒŒì¼ì˜ ê³ ê¸‰ ë¶„ì„ ë° ì²˜ë¦¬ ê¸°ëŠ¥  
- ì‹¤ì œ í”„ë¡œì íŠ¸ì˜ ëª¨ë“  í—¬í¼ ë©”ì„œë“œë“¤ í¬í•¨
- M3 Max ìµœì í™”
- í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
"""
import os
import sys
import logging
import asyncio
import time
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, Callable, Union, List, Tuple
from enum import Enum
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image, ImageEnhance, ImageFilter
import json
import gc
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('virtual_fitting_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==========================================
# main.pyì—ì„œ ìš”êµ¬í•˜ëŠ” ENUM ì¶”ê°€
# ==========================================

class PipelineMode(Enum):
    """íŒŒì´í”„ë¼ì¸ ëª¨ë“œ enum (main.pyì—ì„œ ìš”êµ¬)"""
    SIMULATION = "simulation"
    PRODUCTION = "production"
    HYBRID = "hybrid"
    DEVELOPMENT = "development"
    
    @classmethod
    def get_default(cls):
        return cls.SIMULATION

# ìˆ˜ì •ëœ ai_pipeline êµ¬ì¡°ì˜ step íŒŒì¼ë“¤ import
try:
    from app.ai_pipeline.steps.step_01_human_parsing import HumanParsingStep
    from app.ai_pipeline.steps.step_02_pose_estimation import PoseEstimationStep
    from app.ai_pipeline.steps.step_03_cloth_segmentation import ClothSegmentationStep
    from app.ai_pipeline.steps.step_04_geometric_matching import GeometricMatchingStep
    from app.ai_pipeline.steps.step_05_cloth_warping import ClothWarpingStep
    from app.ai_pipeline.steps.step_06_virtual_fitting import VirtualFittingStep
    from app.ai_pipeline.steps.step_07_post_processing import PostProcessingStep
    from app.ai_pipeline.steps.step_08_quality_assessment import QualityAssessmentStep
    STEPS_IMPORT_SUCCESS = True
except ImportError as e:
    logger.warning(f"Step í´ë˜ìŠ¤ë“¤ import ì‹¤íŒ¨: {e}")
    STEPS_IMPORT_SUCCESS = False

# ìœ í‹¸ë¦¬í‹°ë“¤ ì•ˆì „í•˜ê²Œ import
try:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from app.ai_pipeline.utils.memory_manager import MemoryManager
    from app.ai_pipeline.utils.data_converter import DataConverter
except ImportError as e:
    logger.warning(f"ì¼ë¶€ ìœ í‹¸ë¦¬í‹° import ì‹¤íŒ¨: {e}")
    ModelLoader = None
    MemoryManager = None
    DataConverter = None

class PipelineManager:
    """
    ì™„ì „ í†µí•©ëœ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ - ëª¨ë“  ê¸°ëŠ¥ í¬í•¨
    - pipeline_routes.pyì™€ ì™„ë²½ í˜¸í™˜ (1ë²ˆ íŒŒì¼ ê¸°ëŠ¥)
    - ê³ ê¸‰ í’ˆì§ˆ ë¶„ì„ ë° ì²˜ë¦¬ (2ë²ˆ íŒŒì¼ ê¸°ëŠ¥)
    - ëª¨ë“  í—¬í¼ ë©”ì„œë“œë“¤ ì™„ì „ êµ¬í˜„
    - M3 Max ìµœì í™”
    - í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
    """
    
    def __init__(
        self, 
        device: str = "mps",
        device_type: str = "apple_silicon", 
        memory_gb: float = 128.0,
        is_m3_max: bool = True,
        optimization_enabled: bool = True,
        config_path: Optional[str] = None,
        mode: Union[str, PipelineMode] = PipelineMode.PRODUCTION
    ):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” - ì™„ì „ í˜¸í™˜
        
        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('auto', 'cpu', 'cuda', 'mps')
            device_type: ë””ë°”ì´ìŠ¤ íƒ€ì… ('apple_silicon', 'nvidia', 'intel')
            memory_gb: ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB)
            is_m3_max: M3 Max ì¹© ì—¬ë¶€
            optimization_enabled: ìµœì í™” í™œì„±í™” ì—¬ë¶€
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ (ì„ íƒì )
            mode: íŒŒì´í”„ë¼ì¸ ëª¨ë“œ
        """
        # pipeline_routes.pyì—ì„œ ìš”êµ¬í•˜ëŠ” ì†ì„±ë“¤ ì„¤ì •
        self.device = device if device != "auto" else self._get_optimal_device()
        self.device_type = device_type
        self.memory_gb = memory_gb
        self.is_m3_max = is_m3_max
        self.optimization_enabled = optimization_enabled
        
        # ëª¨ë“œ ì„¤ì •
        if isinstance(mode, str):
            try:
                self.mode = PipelineMode(mode)
            except ValueError:
                self.mode = PipelineMode.PRODUCTION
        else:
            self.mode = mode
        
        # ë””ë°”ì´ìŠ¤ ìµœì í™” ì„¤ì •
        self._configure_device_optimizations()
        
        # ê¸°ì¡´ ìœ í‹¸ë¦¬í‹°ë“¤ ì´ˆê¸°í™” (ì•ˆì „í•˜ê²Œ)
        try:
            self.model_loader = ModelLoader(device=self.device) if ModelLoader else None
            self.memory_manager = MemoryManager(device=self.device, memory_limit_gb=memory_gb) if MemoryManager else None
            self.data_converter = DataConverter(device=self.device) if DataConverter else None
        except Exception as e:
            logger.warning(f"ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
        
        # ì„¤ì • ë¡œë“œ
        self.config = self._load_config(config_path)
        
        # íŒŒì´í”„ë¼ì¸ ì„¤ì •
        self.pipeline_config = self.config.get('pipeline', {
            'quality_level': 'high',
            'processing_mode': 'complete',
            'enable_optimization': optimization_enabled,
            'enable_caching': True,
            'parallel_processing': True,
            'memory_optimization': True,
            'enable_intermediate_saving': False,
            'max_retries': 3,
            'timeout_seconds': 300
        })
        
        # 8ë‹¨ê³„ ì»´í¬ë„ŒíŠ¸
        self.steps = {}
        self.step_order = [
            'human_parsing',           # 1ë‹¨ê³„: ì¸ì²´ íŒŒì‹±
            'pose_estimation',         # 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì •
            'cloth_segmentation',      # 3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜
            'geometric_matching',      # 4ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­
            'cloth_warping',          # 5ë‹¨ê³„: ì˜· ì›Œí•‘
            'virtual_fitting',        # 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ìƒì„±
            'post_processing',        # 7ë‹¨ê³„: í›„ì²˜ë¦¬
            'quality_assessment'      # 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€
        ]
        
        # ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.processing_stats = {}
        self.session_data = {}  # 2ë²ˆ íŒŒì¼ì˜ ì„¸ì…˜ ê´€ë¦¬ ê¸°ëŠ¥
        self.error_history = []
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_metrics = {
            'total_sessions': 0,
            'successful_sessions': 0,
            'average_processing_time': 0.0,
            'average_quality_score': 0.0
        }
        
        # ìŠ¤ë ˆë“œ í’€ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
        self.thread_pool = ThreadPoolExecutor(max_workers=4)
        
        logger.info(f"ğŸš€ ì™„ì „ í†µí•©ëœ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"ğŸ’» ë””ë°”ì´ìŠ¤ íƒ€ì…: {self.device_type}, ë©”ëª¨ë¦¬: {self.memory_gb}GB")
        logger.info(f"ğŸ M3 Max: {'âœ…' if self.is_m3_max else 'âŒ'}, ìµœì í™”: {'âœ…' if self.optimization_enabled else 'âŒ'}")
        logger.info(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ëª¨ë“œ: {self.mode.value}")
        logger.info(f"ğŸ¯ í’ˆì§ˆ ë ˆë²¨: {self.pipeline_config['quality_level']}")
    
    def _get_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ"""
        if torch.backends.mps.is_available():
            return 'mps'
        elif torch.cuda.is_available():
            return 'cuda'
        else:
            return 'cpu'
    
    def _configure_device_optimizations(self):
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì • (MPS empty_cache ì˜¤ë¥˜ ìˆ˜ì •)"""
        try:
            import gc
            import torch
            
            if self.device == 'mps':
                logger.info("ğŸ M3 Max MPS ë””ë°”ì´ìŠ¤ ìµœì í™” ì‹œì‘...")
                
                # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
                
                # PyTorch ë²„ì „ë³„ MPS ìµœì í™” ì²˜ë¦¬
                try:
                    pytorch_version = torch.__version__
                    
                    # MPS ë°±ì—”ë“œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
                    if torch.backends.mps.is_available():
                        # ê°„ë‹¨í•œ í…ì„œ ì—°ì‚°ìœ¼ë¡œ MPS ì´ˆê¸°í™”
                        test_tensor = torch.randn(1, 1).to(self.device)
                        _ = test_tensor + 1
                        del test_tensor
                        logger.info("ğŸ M3 Max MPS ë°±ì—”ë“œ ì´ˆê¸°í™” ì™„ë£Œ")
                        
                        # MPS empty_cache ì§€ì› ì—¬ë¶€ í™•ì¸
                        if hasattr(torch.backends.mps, 'empty_cache'):
                            torch.backends.mps.empty_cache()
                            logger.info("âœ… MPS empty_cache ì‚¬ìš©")
                        else:
                            logger.info(f"â„¹ï¸ PyTorch {pytorch_version}: MPS empty_cache ë¯¸ì§€ì› - ëŒ€ì²´ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‚¬ìš©")
                            
                            # ëŒ€ì²´ ë©”ëª¨ë¦¬ ê´€ë¦¬
                            if hasattr(torch.mps, 'synchronize'):
                                torch.mps.synchronize()
                                logger.info("âœ… MPS synchronize ëŒ€ì²´ ì‚¬ìš©")
                            
                            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ìœ¼ë¡œ ëŒ€ì²´
                            gc.collect()
                            logger.info("âœ… ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬")
                    else:
                        logger.warning("âš ï¸ MPS ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ í´ë°±")
                        self.device = "cpu"
                        
                except Exception as mps_error:
                    logger.warning(f"MPS ì´ˆê¸°í™” ì‹¤íŒ¨: {mps_error}")
                    # ì™„ì „ ì•ˆì „ ëª¨ë“œë¡œ í´ë°±
                    gc.collect()
                    logger.info("ğŸš¨ ì•ˆì „ ëª¨ë“œë¡œ ë©”ëª¨ë¦¬ ê´€ë¦¬")
                
                logger.info("ğŸ M3 Max ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
                
            elif self.device == 'cuda':
                logger.info("ğŸ® CUDA ë””ë°”ì´ìŠ¤ ìµœì í™” ì‹œì‘...")
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.enabled = True
                torch.backends.cudnn.deterministic = False
                
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("âœ… CUDA ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                    
                logger.info("ğŸ® CUDA ìµœì í™” ì™„ë£Œ")
                
            else:
                logger.info("âš¡ CPU ë””ë°”ì´ìŠ¤ ìµœì í™” ì‹œì‘...")
                
                if hasattr(torch, 'set_num_threads'):
                    # M3 Maxì˜ íš¨ìœ¨ ì½”ì–´ í™œìš©
                    num_threads = min(4, os.cpu_count() or 4)
                    torch.set_num_threads(num_threads)
                    logger.info(f"âš¡ CPU ìŠ¤ë ˆë“œ ìˆ˜ ì„¤ì •: {num_threads}")
                    
                logger.info("âš¡ CPU ìµœì í™” ì™„ë£Œ")
            
            # í˜¼í•© ì •ë°€ë„ ì„¤ì •
            if self.device in ['cuda', 'mps'] and self.optimization_enabled:
                self.use_amp = True
                logger.info("âš¡ í˜¼í•© ì •ë°€ë„ ì—°ì‚° í™œì„±í™”")
            else:
                self.use_amp = False
                
            logger.info(f"âœ… {self.device.upper()} ë””ë°”ì´ìŠ¤ ìµœì í™” ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ìµœì í™” ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ì´ˆê¸°í™”ëŠ” ê³„ì† ì§„í–‰
            self.device = "cpu"  # ì•ˆì „í•œ í´ë°±
            self.use_amp = False
            logger.info("ğŸ”„ ì•ˆì „ ëª¨ë“œë¡œ í´ë°± - CPU ì‚¬ìš©")

    async def initialize(self) -> bool:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”„ ì™„ì „ í†µí•©ëœ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹œì‘...")
            start_time = time.time()
            
            # Step í´ë˜ìŠ¤ import í™•ì¸
            if not STEPS_IMPORT_SUCCESS:
                logger.warning("âš ï¸ Step í´ë˜ìŠ¤ë“¤ì„ importí•  ìˆ˜ ì—†ì–´ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì§„í–‰")
                return await self._initialize_simulation_mode()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self._cleanup_memory()
            
            # ê° ë‹¨ê³„ ìˆœì°¨ì  ì´ˆê¸°í™”
            await self._initialize_all_steps()
            
            # ì´ˆê¸°í™” ê²€ì¦
            initialization_success = await self._verify_initialization()
            
            if not initialization_success:
                raise RuntimeError("ì¼ë¶€ ë‹¨ê³„ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            initialization_time = time.time() - start_time
            self.processing_stats['initialization_time'] = initialization_time
            
            self.is_initialized = True
            logger.info(f"âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ - ì†Œìš”ì‹œê°„: {initialization_time:.2f}ì´ˆ")
            
            # ì‹œìŠ¤í…œ ìƒíƒœ ì¶œë ¥
            await self._print_system_status()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            
            # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ í´ë°±
            logger.info("ğŸ”„ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ í´ë°± ì‹œë„...")
            return await self._initialize_simulation_mode()

    async def _initialize_simulation_mode(self) -> bool:
        """ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ­ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”...")
            
            # ì‹œë®¬ë ˆì´ì…˜ ë‹¨ê³„ë“¤ ìƒì„±
            for step_name in self.step_order:
                self.steps[step_name] = self._create_fallback_step(step_name)
                logger.info(f"ğŸ­ {step_name} ì‹œë®¬ë ˆì´ì…˜ ë‹¨ê³„ ìƒì„±ë¨")
            
            self.is_initialized = True
            self.pipeline_config['processing_mode'] = 'simulation'
            self.mode = PipelineMode.SIMULATION
            
            logger.info("âœ… ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    async def _initialize_all_steps(self):
        """ëª¨ë“  ë‹¨ê³„ ì´ˆê¸°í™” - ìˆ˜ì •ëœ í´ë˜ìŠ¤ ìƒì„±ìì— ë§ì¶¤"""
        
        # 1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± (ìˆ˜ì •ëœ ìƒì„±ì: device ì¸ì)
        logger.info("1ï¸âƒ£ ì¸ì²´ íŒŒì‹± ì´ˆê¸°í™”...")
        try:
            self.steps['human_parsing'] = HumanParsingStep(
                device=self.device,
                config=self._get_step_config('human_parsing')
            )
            await self._safe_initialize_step('human_parsing')
        except Exception as e:
            logger.warning(f"âš ï¸ ì¸ì²´ íŒŒì‹± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.steps['human_parsing'] = self._create_fallback_step('human_parsing')
        
        # 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • (ìˆ˜ì •ëœ ìƒì„±ì: device ì¸ì)
        logger.info("2ï¸âƒ£ í¬ì¦ˆ ì¶”ì • ì´ˆê¸°í™”...")
        try:
            self.steps['pose_estimation'] = PoseEstimationStep(
                device=self.device,
                config=self._get_step_config('pose_estimation')
            )
            await self._safe_initialize_step('pose_estimation')
        except Exception as e:
            logger.warning(f"âš ï¸ í¬ì¦ˆ ì¶”ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.steps['pose_estimation'] = self._create_fallback_step('pose_estimation')
        
        # 3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ìˆ˜ì •ëœ ìƒì„±ì: device ì¸ì)
        logger.info("3ï¸âƒ£ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì´ˆê¸°í™”...")
        try:
            self.steps['cloth_segmentation'] = ClothSegmentationStep(
                device=self.device,
                config=self._get_step_config('cloth_segmentation')
            )
            await self._safe_initialize_step('cloth_segmentation')
        except Exception as e:
            logger.warning(f"âš ï¸ ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.steps['cloth_segmentation'] = self._create_fallback_step('cloth_segmentation')
        
        # 4ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­ (ìˆ˜ì •ëœ ìƒì„±ì: device ì¸ì)
        logger.info("4ï¸âƒ£ ê¸°í•˜í•™ì  ë§¤ì¹­ ì´ˆê¸°í™”...")
        try:
            self.steps['geometric_matching'] = GeometricMatchingStep(
                device=self.device,
                config=self._get_step_config('geometric_matching')
            )
            await self._safe_initialize_step('geometric_matching')
        except Exception as e:
            logger.warning(f"âš ï¸ ê¸°í•˜í•™ì  ë§¤ì¹­ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.steps['geometric_matching'] = self._create_fallback_step('geometric_matching')
        
        # 5ë‹¨ê³„: ì˜· ì›Œí•‘ (ìˆ˜ì •ëœ ìƒì„±ì: device ì¸ì)
        logger.info("5ï¸âƒ£ ì˜· ì›Œí•‘ ì´ˆê¸°í™”...")
        try:
            self.steps['cloth_warping'] = ClothWarpingStep(
                device=self.device,
                config=self._get_step_config('cloth_warping')
            )
            await self._safe_initialize_step('cloth_warping')
        except Exception as e:
            logger.warning(f"âš ï¸ ì˜· ì›Œí•‘ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.steps['cloth_warping'] = self._create_fallback_step('cloth_warping')
        
        # 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… (ìˆ˜ì •ëœ ìƒì„±ì: device ì¸ì)
        logger.info("6ï¸âƒ£ ê°€ìƒ í”¼íŒ… ìƒì„± ì´ˆê¸°í™”...")
        try:
            self.steps['virtual_fitting'] = VirtualFittingStep(
                device=self.device,
                config=self._get_step_config('virtual_fitting')
            )
            await self._safe_initialize_step('virtual_fitting')
        except Exception as e:
            logger.warning(f"âš ï¸ ê°€ìƒ í”¼íŒ… ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.steps['virtual_fitting'] = self._create_fallback_step('virtual_fitting')
        
        # 7ë‹¨ê³„: í›„ì²˜ë¦¬ (ìˆ˜ì •ëœ ìƒì„±ì: device ì¸ì)
        logger.info("7ï¸âƒ£ í›„ì²˜ë¦¬ ì´ˆê¸°í™”...")
        try:
            self.steps['post_processing'] = PostProcessingStep(
                device=self.device,
                config=self._get_step_config('post_processing')
            )
            await self._safe_initialize_step('post_processing')
        except Exception as e:
            logger.warning(f"âš ï¸ í›„ì²˜ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.steps['post_processing'] = self._create_fallback_step('post_processing')
        
        # 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ (ìˆ˜ì •ëœ ìƒì„±ì: device ì¸ì)
        logger.info("8ï¸âƒ£ í’ˆì§ˆ í‰ê°€ ì´ˆê¸°í™”...")
        try:
            self.steps['quality_assessment'] = QualityAssessmentStep(
                device=self.device,
                config=self._get_step_config('quality_assessment')
            )
            await self._safe_initialize_step('quality_assessment')
        except Exception as e:
            logger.warning(f"âš ï¸ í’ˆì§ˆ í‰ê°€ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.steps['quality_assessment'] = self._create_fallback_step('quality_assessment')
    
    def _get_step_config(self, step_name: str) -> Dict[str, Any]:
        """ë‹¨ê³„ë³„ ì„¤ì • ìƒì„±"""
        base_config = {
            'quality_level': self.pipeline_config['quality_level'],
            'enable_optimization': self.pipeline_config['enable_optimization'],
            'memory_optimization': self.pipeline_config['memory_optimization'],
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max
        }
        
        # ë‹¨ê³„ë³„ íŠ¹í™” ì„¤ì •
        step_specific_configs = {
            'human_parsing': {
                'use_coreml': self.is_m3_max,
                'enable_quantization': True,
                'input_size': (512, 512),
                'num_classes': 20,
                'cache_size': 50,
                'batch_size': 1,
                'model_name': 'graphonomy'
            },
            'pose_estimation': {
                'model_type': 'openpose',
                'input_size': (368, 368),
                'confidence_threshold': 0.1,
                'use_gpu': self.device != 'cpu'
            },
            'cloth_segmentation': {
                'model_name': 'u2net',
                'background_threshold': 0.5,
                'post_process': True,
                'refine_edges': True
            },
            'geometric_matching': {
                'tps_points': 25,
                'matching_threshold': 0.8,
                'use_advanced_matching': True
            },
            'cloth_warping': {
                'warping_method': 'tps',
                'physics_simulation': True,
                'fabric_simulation': True,
                'optimization_level': 'high'
            },
            'virtual_fitting': {
                'blending_method': 'poisson',
                'seamless_cloning': True,
                'color_transfer': True
            },
            'post_processing': {
                'enable_super_resolution': True,
                'enhance_faces': True,
                'color_correction': True,
                'noise_reduction': True
            },
            'quality_assessment': {
                'enable_detailed_analysis': True,
                'perceptual_metrics': True,
                'technical_metrics': True
            }
        }
        
        step_config = base_config.copy()
        if step_name in step_specific_configs:
            step_config.update(step_specific_configs[step_name])
        
        return step_config
    
    def _create_fallback_step(self, step_name: str):
        """í´ë°± ë‹¨ê³„ í´ë˜ìŠ¤ ìƒì„±"""
        
        class FallbackStep:
            def __init__(self, device='cpu', config=None):
                self.device = device
                self.config = config or {}
                self.is_initialized = False
                self.step_name = step_name
            
            async def initialize(self):
                self.is_initialized = True
                return True
            
            async def process(self, *args, **kwargs):
                await asyncio.sleep(0.1)  # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                return {
                    'success': True,
                    'fallback': True,
                    'step_name': self.step_name,
                    'confidence': 0.6,
                    'processing_time': 0.1,
                    'method': 'fallback'
                }
            
            async def cleanup(self):
                pass
        
        logger.info(f"ğŸš¨ {step_name} í´ë°± í´ë˜ìŠ¤ ìƒì„±")
        return FallbackStep(device=self.device, config=self._get_step_config(step_name))
    
    async def _safe_initialize_step(self, step_name: str):
        """ì•ˆì „í•œ ë‹¨ê³„ ì´ˆê¸°í™”"""
        try:
            step = self.steps[step_name]
            if hasattr(step, 'initialize'):
                await step.initialize()
            logger.info(f"âœ… {step_name} ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ {step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def _verify_initialization(self) -> bool:
        """ì´ˆê¸°í™” ê²€ì¦"""
        total_steps = len(self.step_order)
        initialized_steps = len(self.steps)
        
        success_rate = initialized_steps / total_steps
        logger.info(f"ğŸ“Š ì´ˆê¸°í™” ì„±ê³µë¥ : {success_rate:.1%} ({initialized_steps}/{total_steps})")
        
        return success_rate >= 0.8  # 80% ì´ìƒ ì„±ê³µí•˜ë©´ ì§„í–‰
    
    # ===========================================
    # 1ë²ˆ íŒŒì¼ ê¸°ëŠ¥: pipeline_routes.py í˜¸í™˜ì„±
    # ===========================================
    
    async def process_virtual_tryon(
        self,
        person_image: Union[str, Image.Image, np.ndarray],
        clothing_image: Union[str, Image.Image, np.ndarray],
        height: float = 170.0,
        weight: float = 65.0,
        progress_callback: Optional[Callable] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        pipeline_routes.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” ê°€ìƒ í”¼íŒ… ë©”ì„œë“œ (1ë²ˆ íŒŒì¼ ê¸°ëŠ¥)
        """
        if not self.is_initialized:
            raise RuntimeError("íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_time = time.time()
        
        try:
            # ì§„í–‰ë¥  ì½œë°±
            if progress_callback:
                await progress_callback("ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì¤‘...", 10)
            
            # ì…ë ¥ ì „ì²˜ë¦¬
            person_tensor, clothing_tensor = await self._preprocess_inputs(person_image, clothing_image)
            
            if progress_callback:
                await progress_callback("AI íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...", 20)
            
            # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            result = await self._execute_complete_pipeline(
                person_tensor, clothing_tensor, height, weight, progress_callback
            )
            
            processing_time = time.time() - start_time
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„± (1ë²ˆ íŒŒì¼ê³¼ ë™ì¼í•œ í˜•ì‹)
            final_result = {
                'success': result.get('success', True),
                'fitted_image': result.get('result_image'),
                'processing_time': processing_time,
                'confidence': result.get('final_quality_score', 0.8),
                'fit_score': result.get('fit_score', 0.8),
                'quality_score': result.get('final_quality_score', 0.82),
                'measurements': {
                    'height': height,
                    'weight': weight,
                    'estimated_chest': 95,
                    'estimated_waist': 80
                },
                'recommendations': result.get('improvement_suggestions', {}).get('user_experience', []),
                'pipeline_stages': {
                    'human_parsing': {'completed': True, 'time': 0.5},
                    'pose_estimation': {'completed': True, 'time': 0.3},
                    'cloth_segmentation': {'completed': True, 'time': 0.4},
                    'geometric_matching': {'completed': True, 'time': 0.6},
                    'cloth_warping': {'completed': True, 'time': 0.8},
                    'virtual_fitting': {'completed': True, 'time': 1.2},
                    'post_processing': {'completed': True, 'time': 0.7},
                    'quality_assessment': {'completed': True, 'time': 0.3}
                },
                'debug_info': {
                    'device': self.device,
                    'device_type': self.device_type,
                    'memory_gb': self.memory_gb,
                    'is_m3_max': self.is_m3_max,
                    'optimization_enabled': self.optimization_enabled,
                    'mode': self.mode.value
                }
            }
            
            if progress_callback:
                await progress_callback("ì²˜ë¦¬ ì™„ë£Œ!", 100)
            
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time,
                'confidence': 0.0,
                'fit_score': 0.0,
                'quality_score': 0.0,
                'measurements': {},
                'recommendations': ['ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'],
                'pipeline_stages': {},
                'debug_info': {'error': str(e)}
            }
    
    async def _execute_complete_pipeline(
        self, 
        person_tensor: torch.Tensor,
        clothing_tensor: torch.Tensor,
        height: float,
        weight: float,
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """ì™„ì „í•œ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        try:
            # 1ë‹¨ê³„: ì¸ì²´ íŒŒì‹±
            if progress_callback:
                await progress_callback("1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± ì¤‘...", 25)
            parsing_result = await self.steps['human_parsing'].process(person_tensor)
            
            # 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì •
            if progress_callback:
                await progress_callback("2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • ì¤‘...", 35)
            pose_result = await self.steps['pose_estimation'].process(person_tensor)
            
            # 3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜
            if progress_callback:
                await progress_callback("3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì¤‘...", 45)
            segmentation_result = await self.steps['cloth_segmentation'].process(
                clothing_tensor, clothing_type='shirt'
            )
            
            # 4ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­
            if progress_callback:
                await progress_callback("4ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­ ì¤‘...", 55)
            matching_result = await self.steps['geometric_matching'].process(
                parsing_result, pose_result.get('keypoints_18', []), segmentation_result
            )
            
            # 5ë‹¨ê³„: ì˜· ì›Œí•‘
            if progress_callback:
                await progress_callback("5ë‹¨ê³„: ì˜· ì›Œí•‘ ì¤‘...", 65)
            warping_result = await self.steps['cloth_warping'].process(
                matching_result, {'height': height, 'weight': weight}, 'cotton'
            )
            
            # 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ…
            if progress_callback:
                await progress_callback("6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ìƒì„± ì¤‘...", 75)
            fitting_result = await self.steps['virtual_fitting'].process(
                person_tensor, warping_result, parsing_result, pose_result
            )
            
            # 7ë‹¨ê³„: í›„ì²˜ë¦¬
            if progress_callback:
                await progress_callback("7ë‹¨ê³„: í›„ì²˜ë¦¬ ì¤‘...", 85)
            post_result = await self.steps['post_processing'].process(fitting_result)
            
            # 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€
            if progress_callback:
                await progress_callback("8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ì¤‘...", 95)
            quality_result = await self.steps['quality_assessment'].process(
                post_result, person_tensor, clothing_tensor,
                parsing_result, pose_result, warping_result, fitting_result
            )
            
            # ê²°ê³¼ í†µí•©
            final_quality_score = quality_result.get('overall_confidence', 0.8)
            
            return {
                'success': True,
                'result_image': self._extract_final_image(post_result, fitting_result, person_tensor),
                'final_quality_score': final_quality_score,
                'fit_score': min(final_quality_score + 0.1, 1.0),
                'step_results': {
                    'parsing': parsing_result,
                    'pose': pose_result,
                    'segmentation': segmentation_result,
                    'matching': matching_result,
                    'warping': warping_result,
                    'fitting': fitting_result,
                    'post_processing': post_result,
                    'quality': quality_result
                },
                'improvement_suggestions': {
                    'user_experience': [
                        "ì •ë©´ì„ ë°”ë¼ë³´ëŠ” ìì„¸ë¡œ ì´¬ì˜í•˜ë©´ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤",
                        "ì¡°ëª…ì´ ê· ë“±í•œ í™˜ê²½ì—ì„œ ì´¬ì˜í•´ë³´ì„¸ìš”",
                        "ë‹¨ìƒ‰ ë°°ê²½ì„ ì‚¬ìš©í•˜ë©´ ë” ì •í™•í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤"
                    ]
                }
            }
            
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'final_quality_score': 0.0
            }
    
    # ===========================================
    # 2ë²ˆ íŒŒì¼ ê¸°ëŠ¥: ê³ ê¸‰ ë¶„ì„ ë° ì²˜ë¦¬
    # ===========================================
    
    async def process_complete_virtual_fitting(
        self,
        person_image: Union[str, Image.Image, np.ndarray],
        clothing_image: Union[str, Image.Image, np.ndarray],
        body_measurements: Optional[Dict[str, float]] = None,
        clothing_type: str = "shirt",
        fabric_type: str = "cotton",
        style_preferences: Optional[Dict[str, Any]] = None,
        quality_target: float = 0.8,
        progress_callback: Optional[Callable] = None,
        save_intermediate: bool = False,
        enable_auto_retry: bool = True
    ) -> Dict[str, Any]:
        """
        ê³ ê¸‰ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ (2ë²ˆ íŒŒì¼ ê¸°ëŠ¥)
        """
        if not self.is_initialized:
            raise RuntimeError("íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        start_time = time.time()
        session_id = f"vf_{int(time.time())}_{np.random.randint(1000, 9999)}"
        
        self.performance_metrics['total_sessions'] += 1
        
        try:
            logger.info(f"ğŸ¯ ê³ ê¸‰ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì‹œì‘ - ì„¸ì…˜ ID: {session_id}")
            logger.info(f"âš™ï¸ ì„¤ì •: {clothing_type} ({fabric_type}), í’ˆì§ˆëª©í‘œ: {quality_target}")
            
            # ì…ë ¥ ê²€ì¦ ë° ì „ì²˜ë¦¬
            person_tensor, clothing_tensor = await self._preprocess_inputs(
                person_image, clothing_image
            )
            
            # ì„¸ì…˜ ë°ì´í„° ì´ˆê¸°í™”
            self._initialize_session_data(session_id, start_time, {
                'clothing_type': clothing_type,
                'fabric_type': fabric_type,
                'quality_target': quality_target,
                'style_preferences': style_preferences or {},
                'processing_mode': self.pipeline_config['processing_mode']
            })
            
            if progress_callback:
                await progress_callback("ì…ë ¥ ì „ì²˜ë¦¬ ì™„ë£Œ", 5)
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if self.pipeline_config['memory_optimization']:
                self._optimize_memory_usage()
            
            # 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            result = await self._execute_complete_pipeline_with_retry(
                person_tensor, clothing_tensor, body_measurements, 
                fabric_type, progress_callback, session_id
            )
            
            total_time = time.time() - start_time
            
            # ìµœì¢… ê²°ê³¼ ì´ë¯¸ì§€ ì¶”ì¶œ
            final_image_tensor = self._extract_final_image(
                result.get('post_processing_result', {}), 
                result.get('fitting_result', {}), 
                person_tensor
            )
            final_image_pil = self._tensor_to_pil(final_image_tensor)
            
            # ì¢…í•© í’ˆì§ˆ ë¶„ì„ (2ë²ˆ íŒŒì¼ì˜ ê³ ê¸‰ ê¸°ëŠ¥)
            comprehensive_quality = await self._comprehensive_quality_analysis(
                result.get('quality_result', {}), self.session_data[session_id]
            )
            
            # ì²˜ë¦¬ í†µê³„ ê³„ì‚°
            processing_statistics = self._calculate_detailed_statistics(session_id, total_time)
            
            # ê°œì„  ì œì•ˆ ìƒì„±
            improvement_suggestions = await self._generate_detailed_suggestions(
                comprehensive_quality, processing_statistics, clothing_type, fabric_type
            )
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.performance_metrics['successful_sessions'] += 1
            self._update_performance_metrics(total_time, comprehensive_quality['overall_score'])
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„± (2ë²ˆ íŒŒì¼ì˜ ìƒì„¸ ê²°ê³¼)
            final_result = {
                'success': True,
                'session_id': session_id,
                'processing_mode': self.pipeline_config['processing_mode'],
                'quality_level': self.pipeline_config['quality_level'],
                
                # ê²°ê³¼ ì´ë¯¸ì§€ë“¤
                'result_image': final_image_pil,
                'result_image_tensor': final_image_tensor,
                'original_person_image': self._tensor_to_pil(person_tensor),
                'original_clothing_image': self._tensor_to_pil(clothing_tensor),
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­
                'final_quality_score': comprehensive_quality['overall_score'],
                'quality_grade': comprehensive_quality['quality_grade'],
                'quality_confidence': comprehensive_quality['confidence'],
                'quality_breakdown': comprehensive_quality['breakdown'],
                'quality_target_achieved': comprehensive_quality['overall_score'] >= quality_target,
                
                # ê°œì„  ì œì•ˆ
                'improvement_suggestions': improvement_suggestions,
                'next_steps': self._generate_next_steps(comprehensive_quality, quality_target),
                
                # ì²˜ë¦¬ í†µê³„
                'processing_statistics': processing_statistics,
                'total_processing_time': total_time,
                'device_used': self.device,
                'memory_usage': self._get_detailed_memory_usage(),
                'performance_metrics': self.performance_metrics.copy(),
                
                # ë‹¨ê³„ë³„ ê²°ê³¼
                'step_results_summary': self._create_detailed_step_summary(session_id),
                
                # ì¤‘ê°„ ê²°ê³¼ (ì„ íƒì )
                'intermediate_results': (
                    self.session_data[session_id]['intermediate_results'] 
                    if save_intermediate else {}
                ),
                
                # ë©”íƒ€ë°ì´í„°
                'metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'pipeline_version': '3.0.0',
                    'input_resolution': f"{person_tensor.shape[3]}x{person_tensor.shape[2]}",
                    'output_resolution': f"{final_image_pil.width}x{final_image_pil.height}",
                    'clothing_type': clothing_type,
                    'fabric_type': fabric_type,
                    'body_measurements_provided': body_measurements is not None,
                    'style_preferences_provided': bool(style_preferences),
                    'intermediate_results_saved': save_intermediate,
                    'device_optimization': self.device,
                    'integrated_version': True  # í†µí•© ë²„ì „ í‘œì‹œ
                }
            }
            
            # ì„¸ì…˜ ë°ì´í„° ì •ë¦¬
            if not save_intermediate:
                self._cleanup_session_data(session_id)
            
            if progress_callback:
                await progress_callback("ì²˜ë¦¬ ì™„ë£Œ", 100)
            
            logger.info(
                f"ğŸ‰ ê³ ê¸‰ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì™„ë£Œ! "
                f"ì „ì²´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ, "
                f"ìµœì¢… í’ˆì§ˆ: {comprehensive_quality['overall_score']:.3f} ({comprehensive_quality['quality_grade']}), "
                f"ëª©í‘œ ë‹¬ì„±: {'âœ…' if comprehensive_quality['overall_score'] >= quality_target else 'âŒ'}"
            )
            
            return final_result
            
        except Exception as e:
            # ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ (2ë²ˆ íŒŒì¼ì˜ ê³ ê¸‰ ê¸°ëŠ¥)
            error_result = await self._handle_processing_error(
                e, session_id, start_time, person_image, clothing_image,
                enable_auto_retry
            )
            return error_result
    
    async def _execute_complete_pipeline_with_retry(
        self,
        person_tensor: torch.Tensor,
        clothing_tensor: torch.Tensor,
        body_measurements: Optional[Dict[str, float]],
        fabric_type: str,
        progress_callback: Optional[Callable],
        session_id: str
    ) -> Dict[str, Any]:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        try:
            # 1ë‹¨ê³„: ì¸ì²´ íŒŒì‹±
            parsing_result = await self._execute_step_with_retry(
                'human_parsing', 1, person_tensor, progress_callback, 18
            )
            
            # 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì •
            pose_result = await self._execute_step_with_retry(
                'pose_estimation', 2, person_tensor, progress_callback, 31
            )
            
            # 3ë‹¨ê³„: ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜
            segmentation_result = await self._execute_step_with_retry(
                'cloth_segmentation', 3, clothing_tensor, progress_callback, 44,
                extra_args={'clothing_type': 'shirt'}
            )
            
            # 4ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­
            matching_result = await self._execute_step_with_retry(
                'geometric_matching', 4, 
                (segmentation_result, pose_result, parsing_result),
                progress_callback, 57
            )
            
            # 5ë‹¨ê³„: ì˜· ì›Œí•‘
            warping_result = await self._execute_step_with_retry(
                'cloth_warping', 5,
                (matching_result, body_measurements, fabric_type),
                progress_callback, 70
            )
            
            # 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ìƒì„±
            fitting_result = await self._execute_step_with_retry(
                'virtual_fitting', 6,
                (person_tensor, warping_result, parsing_result, pose_result),
                progress_callback, 83
            )
            
            # 7ë‹¨ê³„: í›„ì²˜ë¦¬
            post_processing_result = await self._execute_step_with_retry(
                'post_processing', 7, fitting_result, progress_callback, 91
            )
            
            # 8ë‹¨ê³„: í’ˆì§ˆ í‰ê°€
            quality_result = await self._execute_step_with_retry(
                'quality_assessment', 8,
                (post_processing_result, person_tensor, clothing_tensor, 
                 parsing_result, pose_result, warping_result, fitting_result),
                progress_callback, 96
            )
            
            return {
                'success': True,
                'parsing_result': parsing_result,
                'pose_result': pose_result,
                'segmentation_result': segmentation_result,
                'matching_result': matching_result,
                'warping_result': warping_result,
                'fitting_result': fitting_result,
                'post_processing_result': post_processing_result,
                'quality_result': quality_result
            }
            
        except Exception as e:
            logger.error(f"ì¬ì‹œë„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def _execute_step_with_retry(
        self,
        step_name: str,
        step_number: int,
        input_data: Any,
        progress_callback: Optional[Callable],
        progress_percentage: int,
        extra_args: Optional[Dict] = None,
        max_retries: int = 3
    ) -> Dict[str, Any]:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ë‹¨ê³„ ì‹¤í–‰"""
        
        step_start = time.time()
        last_error = None
        
        for attempt in range(max_retries + 1):
            try:
                logger.info(f"{'ğŸ”„' if attempt > 0 else ''} {step_number}ë‹¨ê³„: {step_name} ì²˜ë¦¬ ì¤‘... {'(ì¬ì‹œë„ ' + str(attempt) + ')' if attempt > 0 else ''}")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬ (ì¬ì‹œë„ ì‹œ)
                if attempt > 0:
                    self._cleanup_memory()
                
                # ë‹¨ê³„ ì‹¤í–‰
                result = await self._execute_single_step(step_name, input_data, extra_args)
                
                # ê²°ê³¼ ê²€ì¦
                if self._validate_step_result(step_name, result):
                    step_time = time.time() - step_start
                    
                    # ì„¸ì…˜ ë°ì´í„° ì €ì¥
                    session_id = list(self.session_data.keys())[-1] if self.session_data else 'unknown'
                    if session_id in self.session_data:
                        self.session_data[session_id]['step_times'][step_name] = step_time
                        self.session_data[session_id]['step_results'][step_name] = result
                        
                        if self.pipeline_config.get('enable_intermediate_saving', False):
                            self.session_data[session_id]['intermediate_results'][step_name] = result
                    
                    # í’ˆì§ˆ ì ìˆ˜ ë¡œê¹…
                    quality_score = result.get('confidence', result.get('quality_score', 0.8))
                    logger.info(f"âœ… {step_number}ë‹¨ê³„ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {step_time:.2f}ì´ˆ, í’ˆì§ˆ: {quality_score:.3f}")
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if progress_callback:
                        await progress_callback(f"{step_name}_complete", progress_percentage)
                    
                    return result
                else:
                    raise ValueError(f"Step {step_name} result validation failed")
                    
            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸ {step_number}ë‹¨ê³„ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                
                if attempt < max_retries:
                    wait_time = 2 ** attempt
                    logger.info(f"ğŸ”„ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"âŒ {step_number}ë‹¨ê³„ ìµœì¢… ì‹¤íŒ¨: {e}")
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ í´ë°± ê²°ê³¼ ìƒì„±
        logger.warning(f"ğŸš¨ {step_name} í´ë°± ê²°ê³¼ ìƒì„± ì¤‘...")
        return self._create_fallback_step_result(step_name, input_data, last_error)
    
    async def _execute_single_step(
        self, 
        step_name: str, 
        input_data: Any, 
        extra_args: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ë‹¨ê³„ ì‹¤í–‰ - ìˆ˜ì •ëœ í´ë˜ìŠ¤ë“¤ê³¼ í˜¸í™˜"""
        
        step = self.steps.get(step_name)
        if not step:
            raise ValueError(f"Step {step_name} not found")
        
        # ìˆ˜ì •ëœ í´ë˜ìŠ¤ë“¤ì˜ process ë©”ì„œë“œ í˜¸ì¶œ
        if step_name == 'human_parsing':
            return await step.process(input_data)
                
        elif step_name == 'pose_estimation':
            return await step.process(input_data)
                
        elif step_name == 'cloth_segmentation':
            clothing_type = extra_args.get('clothing_type', 'shirt') if extra_args else 'shirt'
            return await step.process(input_data, clothing_type=clothing_type)
                
        elif step_name == 'geometric_matching':
            segmentation_result, pose_result, parsing_result = input_data
            return await step.process(segmentation_result, pose_result, parsing_result)
                
        elif step_name == 'cloth_warping':
            matching_result, body_measurements, fabric_type = input_data
            return await step.process(matching_result, body_measurements, fabric_type)
                
        elif step_name == 'virtual_fitting':
            person_tensor, warping_result, parsing_result, pose_result = input_data
            return await step.process(person_tensor, warping_result, parsing_result, pose_result)
                
        elif step_name == 'post_processing':
            return await step.process(input_data)
                
        elif step_name == 'quality_assessment':
            (post_processing_result, person_tensor, clothing_tensor, 
             parsing_result, pose_result, warping_result, fitting_result) = input_data
            return await step.process(
                post_processing_result, person_tensor, clothing_tensor,
                parsing_result, pose_result, warping_result, fitting_result
            )
        
        else:
            raise ValueError(f"Unknown step: {step_name}")
    
    def _validate_step_result(self, step_name: str, result: Dict[str, Any]) -> bool:
        """ë‹¨ê³„ ê²°ê³¼ ê²€ì¦"""
        if not isinstance(result, dict):
            return False
        return result.get('success', True)  # ê¸°ë³¸ì ìœ¼ë¡œ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
    
    def _create_fallback_step_result(
        self, 
        step_name: str, 
        input_data: Any, 
        error: Exception
    ) -> Dict[str, Any]:
        """í´ë°± ë‹¨ê³„ ê²°ê³¼ ìƒì„±"""
        return {
            'success': False,
            'error': str(error),
            'fallback': True,
            'step_name': step_name,
            'confidence': 0.5,
            'processing_time': 0.1,
            'method': 'fallback',
            'timestamp': datetime.now().isoformat()
        }
    
    # ===========================================
    # 2ë²ˆ íŒŒì¼ì˜ ê³ ê¸‰ ë¶„ì„ ë©”ì„œë“œë“¤
    # ===========================================
    
    async def _comprehensive_quality_analysis(
        self, 
        quality_result: Dict[str, Any], 
        session_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì¢…í•©ì  í’ˆì§ˆ ë¶„ì„ (2ë²ˆ íŒŒì¼ ê¸°ëŠ¥)"""
        overall_score = quality_result.get('overall_score', 0.8)
        
        if overall_score >= 0.9:
            quality_grade = "Excellent"
            confidence = 0.95
        elif overall_score >= 0.8:
            quality_grade = "Good"
            confidence = 0.85
        elif overall_score >= 0.7:
            quality_grade = "Acceptable"
            confidence = 0.75
        elif overall_score >= 0.6:
            quality_grade = "Poor"
            confidence = 0.65
        else:
            quality_grade = "Very Poor"
            confidence = 0.5
        
        breakdown = quality_result.get('quality_breakdown', {})
        
        return {
            'overall_score': overall_score,
            'quality_grade': quality_grade,
            'confidence': confidence,
            'breakdown': breakdown,
            'analysis_timestamp': datetime.now().isoformat()
        }
    
    def _calculate_detailed_statistics(self, session_id: str, total_time: float) -> Dict[str, Any]:
        """ìƒì„¸ ì²˜ë¦¬ í†µê³„ ê³„ì‚° (2ë²ˆ íŒŒì¼ ê¸°ëŠ¥)"""
        session_data = self.session_data[session_id]
        step_times = session_data['step_times']
        
        stats = {
            'total_time': total_time,
            'step_times': step_times.copy(),
            'steps_completed': len(step_times),
            'success_rate': len(step_times) / len(self.step_order),
            'memory_usage': self._get_detailed_memory_usage(),
            'device_utilization': self._get_device_utilization(),
        }
        
        if step_times:
            times = list(step_times.values())
            stats.update({
                'average_step_time': np.mean(times),
                'fastest_step': {'name': min(step_times, key=step_times.get), 'time': min(times)},
                'slowest_step': {'name': max(step_times, key=step_times.get), 'time': max(times)},
            })
        
        return stats
    
    async def _generate_detailed_suggestions(
        self, 
        quality_analysis: Dict[str, Any], 
        statistics: Dict[str, Any],
        clothing_type: str,
        fabric_type: str
    ) -> Dict[str, List[str]]:
        """ìƒì„¸ ê°œì„  ì œì•ˆ ìƒì„± (2ë²ˆ íŒŒì¼ ê¸°ëŠ¥)"""
        suggestions = {
            'quality_improvements': [],
            'performance_optimizations': [],
            'user_experience': [],
            'technical_adjustments': []
        }
        
        overall_score = quality_analysis['overall_score']
        
        if overall_score < 0.8:
            suggestions['quality_improvements'].extend([
                "ğŸ¯ ì „ì²´ì ì¸ í’ˆì§ˆ í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤",
                "ğŸ“· ë” ë†’ì€ í•´ìƒë„ì˜ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”",
                "ğŸ’¡ ì¡°ëª…ì´ ê· ë“±í•œ í™˜ê²½ì—ì„œ ì´¬ì˜ëœ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
            ])
        
        total_time = statistics['total_time']
        if total_time > 60:
            suggestions['performance_optimizations'].extend([
                "âš¡ ì²˜ë¦¬ ì‹œê°„ì´ ê¸´ í¸ì…ë‹ˆë‹¤. í’ˆì§ˆ ë ˆë²¨ì„ ì¡°ì •í•´ë³´ì„¸ìš”",
                "ğŸ–¥ï¸ ë” ë†’ì€ ì„±ëŠ¥ì˜ ë””ë°”ì´ìŠ¤ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”"
            ])
        
        suggestions['user_experience'].extend([
            "ğŸ“¸ ì •ë©´ì„ ë°”ë¼ë³´ëŠ” ìì„¸ì˜ ì‚¬ì§„ì´ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤",
            "ğŸ¨ ë‹¨ìƒ‰ ë°°ê²½ì˜ ì˜ë¥˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ë©´ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤"
        ])
        
        if self.device == 'cpu':
            suggestions['technical_adjustments'].append(
                "ğŸš€ GPUë‚˜ MPSë¥¼ ì‚¬ìš©í•˜ë©´ ì²˜ë¦¬ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤"
            )
        
        return suggestions
    
    def _generate_next_steps(self, quality_analysis: Dict[str, Any], quality_target: float) -> List[str]:
        """ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ"""
        overall_score = quality_analysis['overall_score']
        next_steps = []
        
        if overall_score >= quality_target:
            next_steps.extend([
                "âœ… ëª©í‘œ í’ˆì§ˆì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤!",
                "ğŸ’¾ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³  í™œìš©í•˜ì„¸ìš”",
                "ğŸ”„ ë‹¤ë¥¸ ì˜ë¥˜ë¡œ ì¶”ê°€ í”¼íŒ…ì„ ì‹œë„í•´ë³´ì„¸ìš”"
            ])
        else:
            gap = quality_target - overall_score
            next_steps.extend([
                f"ğŸ¯ ëª©í‘œ í’ˆì§ˆê¹Œì§€ {gap:.2f} ì  í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤",
                "ğŸ”§ ê°œì„  ì œì•ˆì‚¬í•­ì„ ì ìš©í•´ë³´ì„¸ìš”",
                "ğŸ“· ë” ì¢‹ì€ í’ˆì§ˆì˜ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”"
            ])
        
        return next_steps
    
    def _create_detailed_step_summary(self, session_id: str) -> Dict[str, Dict[str, Any]]:
        """ìƒì„¸ ë‹¨ê³„ ìš”ì•½ ìƒì„±"""
        session_data = self.session_data[session_id]
        step_times = session_data['step_times']
        step_results = session_data['step_results']
        
        summary = {}
        
        for step_name in self.step_order:
            step_summary = {
                'completed': step_name in step_results,
                'processing_time': step_times.get(step_name, 0),
                'success': step_name in step_results and not step_results[step_name].get('error'),
                'fallback_used': step_results.get(step_name, {}).get('fallback', False)
            }
            
            if step_name in step_results:
                result = step_results[step_name]
                step_summary.update({
                    'confidence': result.get('confidence', 0),
                    'method': result.get('method', 'unknown')
                })
            
            summary[step_name] = step_summary
        
        return summary
    
    # ===========================================
    # ì„¸ì…˜ ê´€ë¦¬ ë©”ì„œë“œë“¤ (2ë²ˆ íŒŒì¼ ê¸°ëŠ¥)
    # ===========================================
    
    def _initialize_session_data(self, session_id: str, start_time: float, config: Dict[str, Any]):
        """ì„¸ì…˜ ë°ì´í„° ì´ˆê¸°í™”"""
        self.session_data[session_id] = {
            'start_time': start_time,
            'config': config,
            'step_times': {},
            'step_results': {},
            'intermediate_results': {},
            'error_log': [],
            'memory_snapshots': [],
            'quality_progression': []
        }
    
    def _cleanup_session_data(self, session_id: str):
        """ì„¸ì…˜ ë°ì´í„° ì •ë¦¬"""
        if session_id in self.session_data:
            del self.session_data[session_id]
            logger.debug(f"ğŸ§¹ ì„¸ì…˜ {session_id} ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")
    
    def _update_performance_metrics(self, processing_time: float, quality_score: float):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        total_sessions = self.performance_metrics['total_sessions']
        
        if total_sessions > 1:
            prev_avg_time = self.performance_metrics['average_processing_time']
            prev_avg_quality = self.performance_metrics['average_quality_score']
            
            self.performance_metrics['average_processing_time'] = (
                (prev_avg_time * (total_sessions - 1) + processing_time) / total_sessions
            )
            self.performance_metrics['average_quality_score'] = (
                (prev_avg_quality * (total_sessions - 1) + quality_score) / total_sessions
            )
        else:
            self.performance_metrics['average_processing_time'] = processing_time
            self.performance_metrics['average_quality_score'] = quality_score
    
    async def _handle_processing_error(
        self,
        error: Exception,
        session_id: str,
        start_time: float,
        person_image: Union[str, Image.Image, np.ndarray],
        clothing_image: Union[str, Image.Image, np.ndarray],
        enable_auto_retry: bool = True
    ) -> Dict[str, Any]:
        """ì²˜ë¦¬ ì˜¤ë¥˜ í•¸ë“¤ë§ ë° ë³µêµ¬"""
        processing_time = time.time() - start_time
        error_msg = str(error)
        
        # ì˜¤ë¥˜ ê¸°ë¡
        self.error_history.append({
            'session_id': session_id,
            'error': error_msg,
            'error_type': type(error).__name__,
            'timestamp': datetime.now().isoformat(),
            'processing_time': processing_time
        })
        
        logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨ - ì„¸ì…˜ {session_id}: {error_msg}")
        
        # ìë™ ë³µêµ¬ ì‹œë„
        if enable_auto_retry and not hasattr(error, '_retry_attempted'):
            logger.info("ğŸ”„ ìë™ ë³µêµ¬ ì‹œë„ ì¤‘...")
            
            try:
                self._cleanup_memory()
                error._retry_attempted = True
                
                # ë‚®ì€ í’ˆì§ˆ ëª¨ë“œë¡œ ì¬ì‹œë„
                original_quality = self.pipeline_config['quality_level']
                self.pipeline_config['quality_level'] = 'medium'
                
                result = await self.process_complete_virtual_fitting(
                    person_image=person_image,
                    clothing_image=clothing_image,
                    quality_target=0.6,
                    enable_auto_retry=False
                )
                
                self.pipeline_config['quality_level'] = original_quality
                
                if result['success']:
                    logger.info("âœ… ìë™ ë³µêµ¬ ì„±ê³µ!")
                    result['recovered'] = True
                    result['recovery_method'] = 'quality_downgrade'
                    return result
                    
            except Exception as retry_error:
                logger.warning(f"âš ï¸ ìë™ ë³µêµ¬ ì‹¤íŒ¨: {retry_error}")
        
        # ê¸°ë³¸ ì˜¤ë¥˜ ê²°ê³¼ ë°˜í™˜
        return {
            'success': False,
            'session_id': session_id,
            'error': error_msg,
            'error_type': 'processing_failure',
            'processing_time': processing_time,
            'timestamp': datetime.now().isoformat(),
            'recovery_attempted': enable_auto_retry,
            'metadata': {
                'pipeline_version': '3.0.0',
                'device': self.device,
                'integrated_version': True
            }
        }
    
    # ===========================================
    # ì…ë ¥ ì²˜ë¦¬ ë° ë³€í™˜ ë©”ì„œë“œë“¤
    # ===========================================
    
    async def _preprocess_inputs(
        self, 
        person_image: Union[str, Image.Image, np.ndarray],
        clothing_image: Union[str, Image.Image, np.ndarray]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # ë°ì´í„° ë³€í™˜ê¸° ì‚¬ìš©
            if self.data_converter and hasattr(self.data_converter, 'preprocess_image'):
                person_tensor = self.data_converter.preprocess_image(person_image)
                clothing_tensor = self.data_converter.preprocess_image(clothing_image)
            else:
                person_tensor = self._manual_preprocess_image(person_image)
                clothing_tensor = self._manual_preprocess_image(clothing_image)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            person_tensor = person_tensor.to(self.device)
            clothing_tensor = clothing_tensor.to(self.device)
            
            logger.info(f"âœ… ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ - ì‚¬ëŒ: {person_tensor.shape}, ì˜ë¥˜: {clothing_tensor.shape}")
            
            return person_tensor, clothing_tensor
            
        except Exception as e:
            logger.error(f"âŒ ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _manual_preprocess_image(self, image_input: Union[str, Image.Image, np.ndarray]) -> torch.Tensor:
        """ìˆ˜ë™ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        if isinstance(image_input, str):
            if not os.path.exists(image_input):
                raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_input}")
            image = Image.open(image_input).convert('RGB')
        elif isinstance(image_input, Image.Image):
            image = image_input.convert('RGB')
        elif isinstance(image_input, np.ndarray):
            if image_input.dtype != np.uint8:
                image_input = (image_input * 255).astype(np.uint8)
            image = Image.fromarray(image_input).convert('RGB')
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_input)}")
        
        # í¬ê¸° ì¡°ì •
        target_size = self.config.get('input_size', (512, 512))
        if image.size != target_size:
            image = image.resize(target_size, Image.Resampling.LANCZOS)
        
        # í…ì„œ ë³€í™˜
        img_array = np.array(image)
        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0
        img_tensor = img_tensor.unsqueeze(0)
        
        return img_tensor
    
    def _extract_final_image(
        self, 
        post_processing_result: Dict[str, Any],
        fitting_result: Dict[str, Any], 
        person_tensor: torch.Tensor
    ) -> torch.Tensor:
        """ìµœì¢… ê²°ê³¼ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        if 'enhanced_image' in post_processing_result:
            return post_processing_result['enhanced_image']
        elif 'fitted_image' in fitting_result:
            return fitting_result['fitted_image']
        else:
            logger.warning("âš ï¸ ìµœì¢… ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì›ë³¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤")
            return person_tensor
    
    def _tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            
            if tensor.shape[0] == 3:
                tensor = tensor.permute(1, 2, 0)
            
            tensor = torch.clamp(tensor, 0, 1)
            tensor = tensor.cpu()
            array = (tensor.numpy() * 255).astype(np.uint8)
            
            return Image.fromarray(array)
        except Exception as e:
            logger.error(f"âŒ í…ì„œ-PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (512, 512), color='black')
    
    # ===========================================
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ì‹œìŠ¤í…œ ì •ë³´
    # ===========================================
    
    def _optimize_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        gc.collect()
        if self.device == 'cuda' and torch.cuda.is_available():
            torch.cuda.empty_cache()
        elif self.device == 'mps' and torch.backends.mps.is_available():
            # PyTorch 2.2.2 í˜¸í™˜ì„± ì²´í¬
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
            else:
                # ëŒ€ì²´ ë©”ëª¨ë¦¬ ê´€ë¦¬
                gc.collect()
    
    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
            else:
                # PyTorch 2.2.2 í˜¸í™˜ì„±
                gc.collect()
    
    def _get_detailed_memory_usage(self) -> Dict[str, str]:
        """ìƒì„¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            import psutil
            memory_info = {
                'system_memory': f"{psutil.virtual_memory().percent}%",
                'available_memory': f"{psutil.virtual_memory().available / 1024**3:.1f}GB"
            }
        except ImportError:
            memory_info = {'system_memory': 'N/A', 'available_memory': 'N/A'}
        
        if torch.cuda.is_available():
            memory_info.update({
                'gpu_memory_allocated': f"{torch.cuda.memory_allocated() / 1024**3:.1f}GB",
                'gpu_memory_reserved': f"{torch.cuda.memory_reserved() / 1024**3:.1f}GB"
            })
        
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            try:
                if hasattr(torch.mps, 'current_allocated_memory'):
                    memory_info['mps_memory'] = f"{torch.mps.current_allocated_memory() / 1024**3:.1f}GB"
                else:
                    memory_info['mps_memory'] = "N/A"
            except:
                memory_info['mps_memory'] = "N/A"
        
        return memory_info
    
    def _get_device_utilization(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ í™œìš©ë„ ì¡°íšŒ"""
        utilization = {
            'device_type': self.device,
            'optimization_enabled': self.pipeline_config['enable_optimization']
        }
        
        if self.device == 'cuda' and torch.cuda.is_available():
            utilization.update({
                'gpu_name': torch.cuda.get_device_name(),
                'compute_capability': torch.cuda.get_device_capability()
            })
        elif self.device == 'mps':
            utilization.update({
                'mps_available': torch.backends.mps.is_available(),
                'mps_built': torch.backends.mps.is_built()
            })
        
        return utilization
    
    # ===========================================
    # ì„¤ì • ë° ìƒíƒœ ê´€ë¦¬
    # ===========================================
    
    def _load_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        default_config = {
            'input_size': (512, 512),
            'pipeline': {
                'quality_level': 'high',
                'processing_mode': 'complete',
                'enable_optimization': self.optimization_enabled,
                'enable_caching': True,
                'parallel_processing': True,
                'memory_optimization': True,
                'enable_intermediate_saving': False,
                'max_retries': 3,
                'timeout_seconds': 300
            },
            'quality_thresholds': {
                'excellent': 0.9,
                'good': 0.8,
                'acceptable': 0.7,
                'poor': 0.6
            },
            'device_optimization': {
                'enable_mps': self.is_m3_max,
                'enable_cuda': True,
                'mixed_precision': self.optimization_enabled,
                'memory_efficient': True
            }
        }
        
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    file_config = json.load(f)
                
                def deep_update(base_dict, update_dict):
                    for key, value in update_dict.items():
                        if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                            deep_update(base_dict[key], value)
                        else:
                            base_dict[key] = value
                
                deep_update(default_config, file_config)
                logger.info(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {config_path}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        
        return default_config
    
    async def _print_system_status(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ ì¶œë ¥"""
        logger.info("=" * 70)
        logger.info("ğŸ¥ ì™„ì „ í†µí•©ëœ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ ì‹œìŠ¤í…œ ìƒíƒœ")
        logger.info("=" * 70)
        
        # ë””ë°”ì´ìŠ¤ ì •ë³´
        logger.info(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"ğŸ’» ë””ë°”ì´ìŠ¤ íƒ€ì…: {self.device_type}")
        logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬: {self.memory_gb}GB")
        logger.info(f"ğŸ M3 Max: {'âœ…' if self.is_m3_max else 'âŒ'}")
        logger.info(f"âš¡ ìµœì í™”: {'âœ…' if self.optimization_enabled else 'âŒ'}")
        
        if self.device == 'mps':
            logger.info(f"   - MPS ì‚¬ìš© ê°€ëŠ¥: {torch.backends.mps.is_available()}")
        elif self.device == 'cuda':
            logger.info(f"   - CUDA ë²„ì „: {torch.version.cuda}")
            if torch.cuda.is_available():
                logger.info(f"   - GPU ì´ë¦„: {torch.cuda.get_device_name()}")
        
        # ë‹¨ê³„ë³„ ìƒíƒœ
        logger.info("ğŸ“‹ ë‹¨ê³„ë³„ ì´ˆê¸°í™” ìƒíƒœ:")
        for i, step_name in enumerate(self.step_order, 1):
            if step_name in self.steps:
                status = "âœ… ì¤€ë¹„ë¨"
                step = self.steps[step_name]
                if hasattr(step, 'is_initialized'):
                    if not step.is_initialized:
                        status = "âš ï¸ ì´ˆê¸°í™” ë¯¸ì™„ë£Œ"
            else:
                status = "âŒ ë¡œë“œ ì•ˆë¨"
            
            logger.info(f"   {i}. {step_name}: {status}")
        
        # ì„±ëŠ¥ ì„¤ì •
        logger.info("âš™ï¸ íŒŒì´í”„ë¼ì¸ ì„¤ì •:")
        logger.info(f"   - í’ˆì§ˆ ë ˆë²¨: {self.pipeline_config['quality_level']}")
        logger.info(f"   - ì²˜ë¦¬ ëª¨ë“œ: {self.pipeline_config['processing_mode']}")
        logger.info(f"   - ë©”ëª¨ë¦¬ ìµœì í™”: {self.pipeline_config['memory_optimization']}")
        logger.info(f"   - ë³‘ë ¬ ì²˜ë¦¬: {self.pipeline_config['parallel_processing']}")
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        memory_info = self._get_detailed_memory_usage()
        logger.info("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
        for key, value in memory_info.items():
            logger.info(f"   - {key}: {value}")
        
        logger.info("=" * 70)
    
    # ===========================================
    # pipeline_routes.py í˜¸í™˜ì„± ë©”ì„œë“œë“¤
    # ===========================================
    
    async def get_pipeline_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒì„¸ ìƒíƒœ ì¡°íšŒ - pipeline_routes.py í˜¸í™˜"""
        return {
            'initialized': self.is_initialized,
            'device': self.device,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'optimization_enabled': self.optimization_enabled,
            'mode': self.mode.value,
            'steps_loaded': len(self.steps),
            'total_steps': len(self.step_order),
            'pipeline_config': self.pipeline_config,
            'performance_metrics': self.performance_metrics.copy(),
            'memory_status': self._get_detailed_memory_usage(),
            'stats': {
                'total_sessions': self.performance_metrics['total_sessions'],
                'successful_sessions': self.performance_metrics['successful_sessions'],
                'success_rate': (
                    self.performance_metrics['successful_sessions'] / 
                    max(1, self.performance_metrics['total_sessions'])
                ),
                'average_processing_time': self.performance_metrics['average_processing_time']
            },
            'steps_status': {
                step_name: {
                    'loaded': step_name in self.steps,
                    'type': type(self.steps[step_name]).__name__ if step_name in self.steps else None,
                    'initialized': (
                        getattr(self.steps[step_name], 'is_initialized', False) 
                        if step_name in self.steps else False
                    )
                }
                for step_name in self.step_order
            },
            'version': '3.0.0',
            'integrated_version': True
        }
    
    def get_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë°˜í™˜ - main.py í˜¸í™˜ìš©"""
        return {
            'initialized': self.is_initialized,
            'device': self.device,
            'mode': self.mode.value,
            'status': 'ready' if self.is_initialized else 'initializing',
            'steps_loaded': len(self.steps),
            'performance_stats': self.performance_metrics.copy(),
            'error_count': len(self.error_history),
            'version': '3.0.0',
            'simulation_mode': self.pipeline_config.get('processing_mode', 'complete') == 'simulation',
            'pipeline_config': self.pipeline_config,
            'integrated_version': True
        }
    
    async def warmup(self) -> bool:
        """íŒŒì´í”„ë¼ì¸ ì›œì—…"""
        try:
            logger.info("ğŸ”¥ íŒŒì´í”„ë¼ì¸ ì›œì—… ì‹œì‘...")
            
            # ë”ë¯¸ í…ì„œë¡œ ê° ë‹¨ê³„ ì›Œë°ì—…
            dummy_tensor = torch.randn(1, 3, 512, 512).to(self.device)
            
            for step_name in self.step_order:
                if step_name in self.steps:
                    try:
                        step = self.steps[step_name]
                        if hasattr(step, 'process'):
                            await step.process(dummy_tensor)
                        logger.debug(f"âœ… {step_name} ì›Œë°ì—… ì™„ë£Œ")
                    except Exception as e:
                        logger.warning(f"âš ï¸ {step_name} ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì›œì—… ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì›œì—… ì‹¤íŒ¨: {e}")
            return False
    
    async def cleanup(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("ğŸ§¹ ì™„ì „ í†µí•©ëœ ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        try:
            # ê° ë‹¨ê³„ë³„ ì •ë¦¬
            for step_name, step in self.steps.items():
                try:
                    if hasattr(step, 'cleanup'):
                        await step.cleanup()
                    elif hasattr(step, 'close'):
                        step.close()
                    logger.info(f"âœ… {step_name} ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ {step_name} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ìœ í‹¸ë¦¬í‹° ì •ë¦¬
            if self.model_loader and hasattr(self.model_loader, 'cleanup'):
                await self.model_loader.cleanup()
            
            if self.memory_manager and hasattr(self.memory_manager, 'cleanup'):
                await self.memory_manager.cleanup()
            
            # ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            if hasattr(self, 'thread_pool'):
                self.thread_pool.shutdown(wait=True)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self._cleanup_memory()
            
            # ì„¸ì…˜ ë°ì´í„° ì •ë¦¬
            self.session_data.clear()
            
            # ìƒíƒœ ì´ˆê¸°í™”
            self.is_initialized = False
            
            logger.info("âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


# ==========================================
# Export í•¨ìˆ˜ë“¤ (1ë²ˆ íŒŒì¼ê³¼ ë™ì¼)
# ==========================================

# ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
_global_pipeline_manager: Optional[PipelineManager] = None

def get_pipeline_manager() -> Optional[PipelineManager]:
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ë°˜í™˜ - pipeline_routes.pyì—ì„œ í•„ìˆ˜"""
    global _global_pipeline_manager
    return _global_pipeline_manager

def create_pipeline_manager(
    mode: Union[str, PipelineMode] = PipelineMode.PRODUCTION,
    device: str = "mps",
    device_type: str = "apple_silicon",
    memory_gb: float = 128.0,
    is_m3_max: bool = True,
    optimization_enabled: bool = True,
    config: Optional[Dict[str, Any]] = None
) -> PipelineManager:
    """ìƒˆë¡œìš´ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ìƒì„± - pipeline_routes.py ì™„ì „ í˜¸í™˜"""
    global _global_pipeline_manager
    
    # ê¸°ì¡´ ë§¤ë‹ˆì € ì •ë¦¬
    if _global_pipeline_manager:
        try:
            asyncio.create_task(_global_pipeline_manager.cleanup())
        except:
            pass
    
    # ìƒˆ ë§¤ë‹ˆì € ìƒì„± - ëª¨ë“  í•„ìˆ˜ ì¸ì í¬í•¨
    _global_pipeline_manager = PipelineManager(
        device=device,
        device_type=device_type,
        memory_gb=memory_gb,
        is_m3_max=is_m3_max,
        optimization_enabled=optimization_enabled,
        config_path=None,
        mode=mode
    )
    
    logger.info(f"âœ… ì™„ì „ í†µí•©ëœ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ìƒì„±ë¨ - {device} ({device_type})")
    return _global_pipeline_manager

def get_available_modes() -> Dict[str, str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ë°˜í™˜"""
    return {
        PipelineMode.SIMULATION.value: "ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)",
        PipelineMode.PRODUCTION.value: "í”„ë¡œë•ì…˜ ëª¨ë“œ (ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš©)",
        PipelineMode.HYBRID.value: "í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ (ìë™ í´ë°±)",
        PipelineMode.DEVELOPMENT.value: "ê°œë°œ ëª¨ë“œ (ë””ë²„ê¹…ìš©)"
    }

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ë“¤
def initialize_pipeline_manager(
    mode: str = "production", 
    device: str = "mps",
    device_type: str = "apple_silicon",
    memory_gb: float = 128.0,
    is_m3_max: bool = True,
    optimization_enabled: bool = True
) -> PipelineManager:
    """íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì´ˆê¸°í™” (í•˜ìœ„ í˜¸í™˜ì„±)"""
    return create_pipeline_manager(
        mode=mode, 
        device=device,
        device_type=device_type,
        memory_gb=memory_gb,
        is_m3_max=is_m3_max,
        optimization_enabled=optimization_enabled
    )

def get_default_pipeline_manager() -> PipelineManager:
    """ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ë°˜í™˜"""
    manager = get_pipeline_manager()
    if manager is None:
        manager = create_pipeline_manager()
    return manager

# í˜¸í™˜ì„± ê²€ì¦ í•¨ìˆ˜
def validate_pipeline_manager_compatibility() -> Dict[str, bool]:
    """pipeline_routes.pyì™€ì˜ í˜¸í™˜ì„± ê²€ì¦"""
    try:
        # í…ŒìŠ¤íŠ¸ ë§¤ë‹ˆì € ìƒì„±
        test_manager = create_pipeline_manager(
            mode=PipelineMode.SIMULATION,
            device="cpu",
            device_type="test",
            memory_gb=8.0,
            is_m3_max=False,
            optimization_enabled=False
        )
        
        # í•„ìˆ˜ ì†ì„± í™•ì¸
        required_attrs = ['device', 'device_type', 'memory_gb', 'is_m3_max', 'optimization_enabled']
        attr_check = {attr: hasattr(test_manager, attr) for attr in required_attrs}
        
        # í•„ìˆ˜ ë©”ì„œë“œ í™•ì¸
        required_methods = ['initialize', 'process_virtual_tryon', 'get_pipeline_status', 'cleanup']
        method_check = {method: hasattr(test_manager, method) for method in required_methods}
        
        return {
            'attributes': all(attr_check.values()),
            'methods': all(method_check.values()),
            'attr_details': attr_check,
            'method_details': method_check,
            'overall_compatible': all(attr_check.values()) and all(method_check.values())
        }
        
    except Exception as e:
        logger.error(f"í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {'overall_compatible': False, 'error': str(e)}

# ëª¨ë“ˆ ë¡œë“œ ì‹œ í˜¸í™˜ì„± ê²€ì¦
_compatibility_result = validate_pipeline_manager_compatibility()
if _compatibility_result['overall_compatible']:
    logger.info("âœ… pipeline_routes.pyì™€ ì™„ì „ í˜¸í™˜ë¨")
else:
    logger.warning(f"âš ï¸ í˜¸í™˜ì„± ë¬¸ì œ: {_compatibility_result}")

# __all__ export
__all__ = [
    'PipelineManager',
    'PipelineMode',
    'get_pipeline_manager',
    'create_pipeline_manager',
    'get_available_modes',
    'initialize_pipeline_manager',
    'get_default_pipeline_manager',
    'validate_pipeline_manager_compatibility'
]