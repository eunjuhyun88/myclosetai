# backend/app/ai_pipeline/pipeline_manager.py
"""
ğŸ”¥ ì™„ì „ DI í†µí•© PipelineManager v9.1 - base_step_mixin.py ê¸°ë°˜ ì™„ì „ ê°œì„  + ìˆœí™˜ì°¸ì¡° í•´ê²°
=====================================================================================

âœ… base_step_mixin.pyì˜ DI íŒ¨í„´ ì™„ì „ ì ìš©
âœ… ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°  
âœ… TYPE_CHECKINGìœ¼ë¡œ import ì‹œì  ìˆœí™˜ì°¸ì¡° ë°©ì§€
âœ… ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ëŠìŠ¨í•œ ê²°í•© ê°•í™”
âœ… ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… ì™„ì „ êµ¬í˜„
âœ… ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ 100% ìœ ì§€
âœ… M3 Max 128GB ìµœì í™” ìœ ì§€
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ìµœê³  ìˆ˜ì¤€
âœ… 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ì „ ì‘ë™
âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›
âœ… DIBasedPipelineManager í´ë˜ìŠ¤ ì™„ì „ êµ¬í˜„

ğŸ”¥ í•µì‹¬ í•´ê²°ì‚¬í•­:
- cannot import name 'DIBasedPipelineManager' ì™„ì „ í•´ê²°
- ìˆœí™˜ì°¸ì¡° ë¬¸ì œ ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ì™„ì „ í•´ê²°
- ê¸°ì¡´ í•¨ìˆ˜/í´ë˜ìŠ¤ëª… 100% ìœ ì§€
- DI Container + ì–´ëŒ‘í„° íŒ¨í„´ ì™„ì „ í†µí•©

ì•„í‚¤í…ì²˜ (base_step_mixin.py ê¸°ë°˜):
PipelineManager (DI Container + ì–´ëŒ‘í„° íŒ¨í„´)
â”œâ”€â”€ DI Container (ì˜ì¡´ì„± ê´€ë¦¬)
â”œâ”€â”€ ì–´ëŒ‘í„° íŒ¨í„´ (ModelLoaderAdapter, MemoryManagerAdapter)
â”œâ”€â”€ ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ì„¤ê³„ (IModelLoader, IMemoryManager)
â”œâ”€â”€ ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì…
â””â”€â”€ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
"""

import os
import sys
import logging
import asyncio
import time
import traceback
import threading
import json
import gc
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, Callable, Union, List, Tuple, Type, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from abc import ABC, abstractmethod

# ==============================================
# ğŸ”¥ 1. TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ ë°©ì§€
# ==============================================

if TYPE_CHECKING:
    from .interfaces.model_interface import IModelLoader, IStepInterface, IMemoryManager, IDataConverter
    from .steps.base_step_mixin import BaseStepMixin

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image, ImageEnhance, ImageFilter

# ì‹œìŠ¤í…œ ì •ë³´ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì )
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# ==============================================
# ğŸ”¥ 2. DI Container ë° ì¸í„°í˜ì´ìŠ¤ ì•ˆì „í•œ import
# ==============================================

# DI Container (ë™ì  importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€)
DI_CONTAINER_AVAILABLE = False
try:
    from app.core.di_container import (
        get_di_container, create_step_with_di, inject_dependencies_to_step,
        initialize_di_system
    )
    DI_CONTAINER_AVAILABLE = True
    logging.info("âœ… DI Container ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    DI_CONTAINER_AVAILABLE = False
    logging.warning(f"âš ï¸ DI Container ì‚¬ìš© ë¶ˆê°€: {e}")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 3. ì—´ê±°í˜• ë° ë°ì´í„° í´ë˜ìŠ¤ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class PipelineMode(Enum):
    """íŒŒì´í”„ë¼ì¸ ëª¨ë“œ"""
    DEVELOPMENT = "development"
    PRODUCTION = "production"
    TESTING = "testing"
    OPTIMIZATION = "optimization"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    MAXIMUM = "maximum"

class ProcessingStatus(Enum):
    """ì²˜ë¦¬ ìƒíƒœ"""
    IDLE = "idle"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CLEANING = "cleaning"

class ExecutionStrategy(Enum):
    """ì‹¤í–‰ ì „ëµ (2ë‹¨ê³„ í´ë°±)"""
    UNIFIED_AI = "unified_ai"        # í†µí•© ì‹œìŠ¤í…œ + AI ëª¨ë¸
    MODEL_LOADER = "model_loader"    # ModelLoader + ê¸°ë³¸ ì²˜ë¦¬
    BASIC_FALLBACK = "basic_fallback" # ê¸°ë³¸ í´ë°± (ì—ëŸ¬ ì‹œì—ë§Œ)

@dataclass
class PipelineConfig:
    """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì„¤ì • + DI ì„¤ì • ê°•í™”"""
    # ê¸°ë³¸ ì„¤ì •
    device: str = "auto"
    quality_level: Union[QualityLevel, str] = QualityLevel.HIGH
    processing_mode: Union[PipelineMode, str] = PipelineMode.PRODUCTION
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    memory_gb: float = 128.0
    is_m3_max: bool = True
    device_type: str = "apple_silicon"
    
    # ğŸ”¥ DI ì„¤ì • ê°•í™” (base_step_mixin.py ê¸°ë°˜)
    use_dependency_injection: bool = True
    auto_inject_dependencies: bool = True
    lazy_loading_enabled: bool = True
    interface_based_design: bool = True
    enable_adapter_pattern: bool = True
    enable_runtime_injection: bool = True
    
    # AI ëª¨ë¸ ì—°ë™ ì„¤ì • (ê¸°ì¡´ ìœ ì§€)
    ai_model_enabled: bool = True
    model_preload_enabled: bool = True
    model_cache_size: int = 20
    ai_inference_timeout: int = 120
    model_fallback_enabled: bool = True
    
    # ì„±ëŠ¥ ìµœì í™” ì„¤ì • (ê¸°ì¡´ ìœ ì§€)
    performance_mode: str = "maximum"
    memory_optimization: bool = True
    gpu_memory_fraction: float = 0.95
    use_fp16: bool = True
    enable_quantization: bool = True
    parallel_processing: bool = True
    batch_processing: bool = True
    async_processing: bool = True
    
    # í´ë°± ì„¤ì •
    max_fallback_attempts: int = 2
    fallback_timeout: int = 30
    enable_smart_fallback: bool = True
    
    # ì²˜ë¦¬ ì„¤ì •
    batch_size: int = 4
    max_retries: int = 2
    timeout_seconds: int = 300
    thread_pool_size: int = 8
    
    def __post_init__(self):
        # ë¬¸ìì—´ì„ Enumìœ¼ë¡œ ë³€í™˜
        if isinstance(self.quality_level, str):
            self.quality_level = QualityLevel(self.quality_level)
        if isinstance(self.processing_mode, str):
            self.processing_mode = PipelineMode(self.processing_mode)
        
        # M3 Max ìë™ ìµœì í™”
        if self.is_m3_max:
            self.memory_gb = max(self.memory_gb, 128.0)
            self.model_cache_size = 20
            self.batch_size = 4
            self.thread_pool_size = 8
            self.gpu_memory_fraction = 0.95
            self.performance_mode = "maximum"
            self.use_dependency_injection = True
            self.enable_adapter_pattern = True

@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼ - DI ì •ë³´ ê°•í™”"""
    success: bool
    session_id: str = ""
    result_image: Optional[Image.Image] = None
    result_tensor: Optional[torch.Tensor] = None
    quality_score: float = 0.0
    quality_grade: str = "Unknown"
    processing_time: float = 0.0
    step_results: Dict[str, Any] = field(default_factory=dict)
    step_timings: Dict[str, float] = field(default_factory=dict)
    ai_models_used: Dict[str, str] = field(default_factory=dict)
    execution_strategies: Dict[str, str] = field(default_factory=dict)
    
    # ğŸ”¥ DI ì •ë³´ ê°•í™”
    dependency_injection_info: Dict[str, Any] = field(default_factory=dict)
    adapter_pattern_info: Dict[str, Any] = field(default_factory=dict)
    interface_usage_info: Dict[str, Any] = field(default_factory=dict)
    
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)

# ==============================================
# ğŸ”¥ 4. DI ê¸°ë°˜ ì–´ëŒ‘í„° í´ë˜ìŠ¤ë“¤ (base_step_mixin.py ê¸°ë°˜)
# ==============================================

class ModelLoaderAdapter:
    """ModelLoader ì–´ëŒ‘í„° - base_step_mixin.py íŒ¨í„´ ì ìš©"""
    
    def __init__(self, model_loader=None):
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        self.adapter_info = {
            'created_at': time.time(),
            'adapter_type': 'ModelLoaderAdapter',
            'base_step_mixin_pattern': True
        }
    
    def create_step_interface(self, step_name: str):
        """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± - ì•ˆì „í•œ ë°©ì‹"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'create_step_interface'):
                return self.model_loader.create_step_interface(step_name)
            else:
                return self._create_fallback_interface(step_name)
        except Exception as e:
            self.logger.warning(f"âš ï¸ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_fallback_interface(step_name)
    
    def _create_fallback_interface(self, step_name: str):
        """í´ë°± ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        class FallbackInterface:
            def __init__(self, name: str):
                self.step_name = name
                self.adapter_created = True
                
            async def get_model(self, model_name: str = None):
                return self._create_mock_model(model_name or "fallback")
                
            def _create_mock_model(self, name: str):
                class MockModel:
                    def __init__(self, model_name: str):
                        self.name = model_name
                        self.device = "cpu"
                        
                    def __call__(self, *args, **kwargs):
                        return {
                            'status': 'success',
                            'model_name': self.name,
                            'result': f'mock_result_for_{self.name}',
                            'adapter_generated': True
                        }
                
                return MockModel(name)
        
        return FallbackInterface(step_name)
    
    async def load_model(self, model_config: Dict[str, Any]):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'load_model'):
                if asyncio.iscoroutinefunction(self.model_loader.load_model):
                    return await self.model_loader.load_model(model_config)
                else:
                    loop = asyncio.get_event_loop()
                    return await loop.run_in_executor(None, self.model_loader.load_model, model_config)
            return None
        except Exception as e:
            self.logger.error(f"âŒ ì–´ëŒ‘í„° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def get_model(self, model_name: str):
        """ëª¨ë¸ ì¡°íšŒ"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'get_model'):
                return self.model_loader.get_model(model_name)
            return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì–´ëŒ‘í„° ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def register_model(self, model_name: str, model_config: Dict[str, Any]) -> bool:
        """ëª¨ë¸ ë“±ë¡"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'register_model'):
                return self.model_loader.register_model(model_name, model_config)
            return False
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì–´ëŒ‘í„° ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'cleanup'):
                if asyncio.iscoroutinefunction(self.model_loader.cleanup):
                    asyncio.create_task(self.model_loader.cleanup())
                else:
                    self.model_loader.cleanup()
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì–´ëŒ‘í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")

class MemoryManagerAdapter:
    """MemoryManager ì–´ëŒ‘í„° - base_step_mixin.py íŒ¨í„´ ì ìš©"""
    
    def __init__(self, memory_manager=None):
        self.memory_manager = memory_manager
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        self.adapter_info = {
            'created_at': time.time(),
            'adapter_type': 'MemoryManagerAdapter',
            'base_step_mixin_pattern': True
        }
        self._ensure_basic_attributes()
    
    def _ensure_basic_attributes(self):
        """ê¸°ë³¸ ì†ì„±ë“¤ì´ í•­ìƒ ì¡´ì¬í•˜ë„ë¡ ë³´ì¥"""
        if not hasattr(self, 'device'):
            self.device = getattr(self.memory_manager, 'device', 'cpu')
        if not hasattr(self, 'is_m3_max'):
            self.is_m3_max = getattr(self.memory_manager, 'is_m3_max', False)
        if not hasattr(self, 'memory_gb'):
            self.memory_gb = getattr(self.memory_manager, 'memory_gb', 16.0)
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” - base_step_mixin.py íŒ¨í„´"""
        try:
            optimization_results = []
            
            # ì›ë³¸ ë§¤ë‹ˆì € ì‚¬ìš©
            if self.memory_manager and hasattr(self.memory_manager, 'optimize_memory'):
                try:
                    result = self.memory_manager.optimize_memory(aggressive=aggressive)
                    optimization_results.append("ì›ë³¸ ë§¤ë‹ˆì € optimize_memory ì„±ê³µ")
                except Exception as e:
                    optimization_results.append(f"ì›ë³¸ ë§¤ë‹ˆì € ì‹¤íŒ¨: {e}")
            
            # ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                before_objects = len(gc.get_objects())
                gc.collect()
                after_objects = len(gc.get_objects())
                freed_objects = before_objects - after_objects
                optimization_results.append(f"Python GC: {freed_objects}ê°œ ê°ì²´ ì •ë¦¬")
            except Exception as e:
                optimization_results.append(f"Python GC ì‹¤íŒ¨: {e}")
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                if torch.cuda.is_available():
                    before_cuda = torch.cuda.memory_allocated()
                    torch.cuda.empty_cache()
                    after_cuda = torch.cuda.memory_allocated()
                    freed_cuda = (before_cuda - after_cuda) / 1024**3
                    optimization_results.append(f"CUDA ìºì‹œ ì •ë¦¬: {freed_cuda:.2f}GB í•´ì œ")
                
                if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            safe_mps_empty_cache()
                            optimization_results.append("MPS ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                    except Exception as mps_error:
                        optimization_results.append(f"MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {mps_error}")
                        
            except Exception as torch_error:
                optimization_results.append(f"PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {torch_error}")
            
            return {
                "success": True, 
                "message": "Memory optimization completed",
                "optimization_results": optimization_results,
                "device": self.device,
                "is_m3_max": self.is_m3_max,
                "adapter_pattern": True,
                "timestamp": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì–´ëŒ‘í„° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False, 
                "error": str(e),
                "adapter_pattern": True,
                "timestamp": time.time()
            }
    
    async def optimize_memory_async(self, aggressive: bool = False):
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            if self.memory_manager and hasattr(self.memory_manager, 'optimize_memory_async'):
                result = await self.memory_manager.optimize_memory_async(aggressive=aggressive)
                if result.get('success', False):
                    return result
            
            # í´ë°±: ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, self.optimize_memory, aggressive)
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False, 
                "error": str(e),
                "call_type": "async",
                "adapter_pattern": True,
                "timestamp": time.time()
            }
    
    def get_memory_stats(self):
        """ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ"""
        try:
            if self.memory_manager and hasattr(self.memory_manager, 'get_memory_stats'):
                return self.memory_manager.get_memory_stats()
            else:
                stats = {
                    "device": self.device,
                    "is_m3_max": self.is_m3_max,
                    "memory_gb": getattr(self, 'memory_gb', 16.0),
                    "available": True,
                    "adapter_pattern": True,
                    "version": "v9.1"
                }
                
                if torch.cuda.is_available():
                    stats.update({
                        "cuda_memory_allocated": torch.cuda.memory_allocated() / 1024**3,
                        "cuda_memory_reserved": torch.cuda.memory_reserved() / 1024**3,
                    })
                
                return stats
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì–´ëŒ‘í„° ë©”ëª¨ë¦¬ í†µê³„ ì‹¤íŒ¨: {e}")
            return {"error": str(e), "adapter_pattern": True}

class DataConverterAdapter:
    """DataConverter ì–´ëŒ‘í„° - base_step_mixin.py íŒ¨í„´ ì ìš©"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        self.adapter_info = {
            'created_at': time.time(),
            'adapter_type': 'DataConverterAdapter',
            'base_step_mixin_pattern': True
        }
                
    def preprocess_image(self, image_input) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ - base_step_mixin.py íŒ¨í„´"""
        try:
            if isinstance(image_input, str):
                image = Image.open(image_input).convert('RGB')
            elif isinstance(image_input, Image.Image):
                image = image_input.convert('RGB')
            elif isinstance(image_input, np.ndarray):
                image = Image.fromarray(image_input).convert('RGB')
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_input)}")
            
            # ìµœì í™”ëœ ë¦¬ì‚¬ì´ì¦ˆ
            if image.size != (512, 512):
                image = image.resize((512, 512), Image.Resampling.LANCZOS)
            
            # í…ì„œ ë³€í™˜
            img_array = np.array(image)
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0
            img_tensor = img_tensor.unsqueeze(0).to(self.device)
            
            return img_tensor
            
        except Exception as e:
            self.logger.error(f"âŒ ì–´ëŒ‘í„° ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ í…ì„œ ë°˜í™˜
            return torch.zeros(1, 3, 512, 512, device=self.device)
    
    def tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            if tensor.shape[0] == 3:
                tensor = tensor.permute(1, 2, 0)
            
            tensor = torch.clamp(tensor, 0, 1)
            tensor = tensor.cpu()
            array = (tensor.numpy() * 255).astype(np.uint8)
            
            return Image.fromarray(array)
            
        except Exception as e:
            self.logger.error(f"âŒ ì–´ëŒ‘í„° í…ì„œ->PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
            return Image.new('RGB', (512, 512), color='gray')

# ==============================================
# ğŸ”¥ 5. DI ê¸°ë°˜ ê´€ë¦¬ì í´ë˜ìŠ¤ë“¤ (base_step_mixin.py íŒ¨í„´ ì ìš©)
# ==============================================

class DIBasedModelLoaderManager:
    """DI ê¸°ë°˜ ModelLoader ê´€ë¦¬ì - base_step_mixin.py íŒ¨í„´"""
    
    def __init__(self, config: PipelineConfig, device: str, logger: logging.Logger, di_container=None):
        self.config = config
        self.device = device
        self.logger = logger
        self.di_container = di_container
        self.model_loader_adapter = None
        self.model_interfaces = {}
        self.loaded_models = {}
        self.is_initialized = False
        
        # base_step_mixin.py íŒ¨í„´ ì ìš©
        self.initialization_time = time.time()
        self.di_pattern_applied = True
        
    async def initialize(self) -> bool:
        """DI ê¸°ë°˜ ì´ˆê¸°í™” - base_step_mixin.py íŒ¨í„´"""
        try:
            self.logger.info("ğŸ§  DI ê¸°ë°˜ ModelLoader ì´ˆê¸°í™” ì‹œì‘...")
            
            # ğŸ”¥ Step 1: DI Containerì—ì„œ ModelLoader ì¡°íšŒ
            if self.di_container and self.config.use_dependency_injection:
                model_loader = self.di_container.get('IModelLoader')
                if model_loader:
                    self.model_loader_adapter = ModelLoaderAdapter(model_loader)
                    self.logger.info("âœ… DI Containerì—ì„œ ModelLoader íšë“")
                else:
                    self.logger.info("âš ï¸ DI Containerì— ModelLoader ì—†ìŒ, ë™ì  ë¡œë”© ì‹œë„")
            
            # ğŸ”¥ Step 2: ë™ì  importë¡œ ModelLoader ê°€ì ¸ì˜¤ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
            if not self.model_loader_adapter:
                try:
                    # ëŸ°íƒ€ì„ ë™ì  import
                    from app.ai_pipeline.utils.model_loader import get_global_model_loader
                    
                    raw_loader = get_global_model_loader()
                    if raw_loader and not isinstance(raw_loader, dict):
                        self.model_loader_adapter = ModelLoaderAdapter(raw_loader)
                        self.logger.info("âœ… ë™ì  importë¡œ ModelLoader íšë“")
                    else:
                        self.logger.warning("âš ï¸ ModelLoaderê°€ dict íƒ€ì…ì´ê±°ë‚˜ None")
                        
                except ImportError as e:
                    self.logger.debug(f"ModelLoader ë™ì  import ì‹¤íŒ¨: {e}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë™ì  ModelLoader ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ Step 3: í´ë°± ì–´ëŒ‘í„° ìƒì„±
            if not self.model_loader_adapter:
                self.model_loader_adapter = ModelLoaderAdapter(None)
                self.logger.info("âš ï¸ í´ë°± ModelLoader ì–´ëŒ‘í„° ì‚¬ìš©")
            
            # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            await self._create_step_interfaces()
            
            self.is_initialized = True
            initialization_duration = time.time() - self.initialization_time
            self.logger.info(f"âœ… DI ê¸°ë°˜ ModelLoader ì´ˆê¸°í™” ì™„ë£Œ ({initialization_duration:.2f}ì´ˆ)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ DI ê¸°ë°˜ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _create_step_interfaces(self):
        """Stepë³„ ì¸í„°í˜ì´ìŠ¤ ìƒì„± - DI íŒ¨í„´"""
        try:
            step_names = [
                'HumanParsingStep', 'PoseEstimationStep', 'ClothSegmentationStep',
                'GeometricMatchingStep', 'ClothWarpingStep', 'VirtualFittingStep',
                'PostProcessingStep', 'QualityAssessmentStep'
            ]
            
            for step_name in step_names:
                try:
                    interface = self.model_loader_adapter.create_step_interface(step_name)
                    self.model_interfaces[step_name] = interface
                    self.logger.info(f"âœ… {step_name} DI ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} DI ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            self.logger.error(f"âŒ DI Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def get_step_interface(self, step_name: str) -> Optional[Any]:
        """Step ì¸í„°í˜ì´ìŠ¤ ë°˜í™˜"""
        return self.model_interfaces.get(step_name)
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.model_loader_adapter:
                self.model_loader_adapter.cleanup()
            self.model_interfaces.clear()
            self.loaded_models.clear()
            self.logger.info("âœ… DIBasedModelLoaderManager ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DIBasedModelLoaderManager ì •ë¦¬ ì‹¤íŒ¨: {e}")

class DIBasedExecutionManager:
    """DI ê¸°ë°˜ ì‹¤í–‰ ê´€ë¦¬ì - base_step_mixin.py íŒ¨í„´"""
    
    def __init__(self, config: PipelineConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.execution_cache = {}
        self.performance_stats = {}
        
        # base_step_mixin.py íŒ¨í„´ ì ìš©
        self.di_pattern_applied = True
        self.adapter_pattern_enabled = config.enable_adapter_pattern
        
    async def execute_step_with_di(
        self,
        step,
        step_name: str,
        current_data: torch.Tensor,
        clothing_tensor: torch.Tensor,
        **kwargs
    ) -> Tuple[Dict[str, Any], str]:
        """DI ê¸°ë°˜ Step ì‹¤í–‰ - base_step_mixin.py íŒ¨í„´"""
        
        start_time = time.time()
        execution_attempts = []
        
        # ğŸ”¥ 1ìˆœìœ„: DI ì£¼ì…ëœ ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©
        try:
            result, strategy = await self._execute_with_di_components(
                step, step_name, current_data, clothing_tensor, **kwargs
            )
            execution_attempts.append(("di_components", result.get('success', False)))
            
            if result.get('success', False):
                result['execution_time'] = time.time() - start_time
                result['di_pattern_used'] = True
                return result, strategy
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ {step_name} DI ì»´í¬ë„ŒíŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            execution_attempts.append(("di_components", False))
        
        # ğŸ”¥ 2ìˆœìœ„: ì–´ëŒ‘í„° íŒ¨í„´ ì‚¬ìš©
        if self.adapter_pattern_enabled:
            try:
                result, strategy = await self._execute_with_adapters(
                    step, step_name, current_data, clothing_tensor, **kwargs
                )
                execution_attempts.append(("adapter_pattern", result.get('success', False)))
                
                if result.get('success', False):
                    result['execution_time'] = time.time() - start_time
                    result['adapter_pattern_used'] = True
                    return result, strategy
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ {step_name} ì–´ëŒ‘í„° íŒ¨í„´ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                execution_attempts.append(("adapter_pattern", False))
        
        # ğŸ”¥ ìµœì¢… í´ë°±: ê¸°ë³¸ ì²˜ë¦¬
        try:
            result, strategy = await self._execute_basic_fallback(
                step, step_name, current_data, clothing_tensor, **kwargs
            )
            execution_attempts.append(("basic_fallback", result.get('success', False)))
            
            result['execution_time'] = time.time() - start_time
            result['execution_attempts'] = execution_attempts
            result['fallback_used'] = True
            
            return result, strategy
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ëª¨ë“  ì‹¤í–‰ ì „ëµ ì‹¤íŒ¨: {e}")
            
            return {
                'success': False,
                'error': f"ëª¨ë“  ì‹¤í–‰ ì „ëµ ì‹¤íŒ¨: {str(e)}",
                'execution_time': time.time() - start_time,
                'execution_attempts': execution_attempts,
                'confidence': 0.0,
                'quality_score': 0.0
            }, "failed"
    
    async def _execute_with_di_components(self, step, step_name: str, current_data: torch.Tensor, 
                                          clothing_tensor: torch.Tensor, **kwargs) -> Tuple[Dict[str, Any], str]:
        """DI ì£¼ì…ëœ ì»´í¬ë„ŒíŠ¸ë¡œ ì‹¤í–‰"""
        try:
            # DI ì£¼ì…ëœ model_loader í™•ì¸
            if hasattr(step, 'model_loader') and step.model_loader:
                # ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
                if hasattr(step, 'model_interface') and step.model_interface:
                    model = await step.model_interface.get_model()
                    if model:
                        ai_result = await self._run_ai_inference(model, current_data, clothing_tensor, **kwargs)
                        if ai_result is not None:
                            return {
                                'success': True,
                                'result': ai_result,
                                'confidence': 0.95,
                                'quality_score': 0.95,
                                'model_used': 'di_injected_model',
                                'ai_model_name': getattr(model, 'name', 'di_model'),
                                'processing_method': 'di_components'
                            }, ExecutionStrategy.UNIFIED_AI.value
            
            # DI ì£¼ì…ëœ ê¸°ë³¸ ì²˜ë¦¬
            if hasattr(step, 'process'):
                result = await self._execute_step_logic(step, step_name, current_data, clothing_tensor, **kwargs)
                return {
                    'success': True,
                    'result': result.get('result', current_data),
                    'confidence': result.get('confidence', 0.90),
                    'quality_score': result.get('quality_score', 0.90),
                    'model_used': 'di_step_logic',
                    'ai_model_name': 'di_step_processing',
                    'processing_method': 'di_step_logic'
                }, ExecutionStrategy.UNIFIED_AI.value
            
            raise Exception("DI ì»´í¬ë„ŒíŠ¸ ì‚¬ìš© ë¶ˆê°€")
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0
            }, "di_error"
    
    async def _execute_with_adapters(self, step, step_name: str, current_data: torch.Tensor,
                                     clothing_tensor: torch.Tensor, **kwargs) -> Tuple[Dict[str, Any], str]:
        """ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ì‹¤í–‰"""
        try:
            # ì–´ëŒ‘í„° ìƒì„± ë° ì‚¬ìš©
            if not hasattr(step, 'model_loader') or not step.model_loader:
                # ë™ì ìœ¼ë¡œ ModelLoader ì–´ëŒ‘í„° ìƒì„±
                try:
                    from app.ai_pipeline.utils.model_loader import get_global_model_loader
                    raw_loader = get_global_model_loader()
                    step.model_loader = ModelLoaderAdapter(raw_loader)
                except Exception as e:
                    step.model_loader = ModelLoaderAdapter(None)
            
            # ì–´ëŒ‘í„°ë¥¼ í†µí•œ ëª¨ë¸ ì‚¬ìš©
            if hasattr(step.model_loader, 'get_model'):
                model = step.model_loader.get_model(f"{step_name}_model")
                if model:
                    ai_result = await self._run_ai_inference(model, current_data, clothing_tensor, **kwargs)
                    if ai_result is not None:
                        return {
                            'success': True,
                            'result': ai_result,
                            'confidence': 0.88,
                            'quality_score': 0.88,
                            'model_used': 'adapter_model',
                            'ai_model_name': f"adapter_{step_name}",
                            'processing_method': 'adapter_pattern'
                        }, ExecutionStrategy.MODEL_LOADER.value
            
            # ì–´ëŒ‘í„° ê¸°ë³¸ ì²˜ë¦¬
            result = await self._execute_step_logic(step, step_name, current_data, clothing_tensor, **kwargs)
            
            return {
                'success': True,
                'result': result.get('result', current_data),
                'confidence': result.get('confidence', 0.85),
                'quality_score': result.get('quality_score', 0.85),
                'model_used': 'adapter_logic',
                'ai_model_name': 'adapter_processing',
                'processing_method': 'adapter_logic'
            }, ExecutionStrategy.MODEL_LOADER.value
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0
            }, "adapter_error"
    
    async def _execute_basic_fallback(self, step, step_name: str, current_data: torch.Tensor,
                                     clothing_tensor: torch.Tensor, **kwargs) -> Tuple[Dict[str, Any], str]:
        """ê¸°ë³¸ í´ë°± ì‹¤í–‰"""
        try:
            result = await self._execute_step_logic(step, step_name, current_data, clothing_tensor, **kwargs)
            
            return {
                'success': True,
                'result': result.get('result', current_data),
                'confidence': 0.75,
                'quality_score': 0.75,
                'model_used': 'basic_fallback',
                'ai_model_name': 'fallback_processing',
                'processing_method': 'basic_fallback'
            }, ExecutionStrategy.BASIC_FALLBACK.value
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0
            }, "basic_fallback_error"
    
    async def _run_ai_inference(self, ai_model, current_data: torch.Tensor, 
                               clothing_tensor: torch.Tensor, **kwargs) -> Optional[torch.Tensor]:
        """AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
        try:
            if hasattr(ai_model, 'process'):
                if asyncio.iscoroutinefunction(ai_model.process):
                    return await ai_model.process(current_data, clothing_tensor, **kwargs)
                else:
                    return ai_model.process(current_data, clothing_tensor, **kwargs)
            elif hasattr(ai_model, '__call__'):
                if asyncio.iscoroutinefunction(ai_model.__call__):
                    return await ai_model(current_data, clothing_tensor, **kwargs)
                else:
                    return ai_model(current_data, clothing_tensor, **kwargs)
            elif hasattr(ai_model, 'forward'):
                return ai_model.forward(current_data, clothing_tensor)
            else:
                self.logger.warning("âš ï¸ AI ëª¨ë¸ ì¶”ë¡  ë©”ì„œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    async def _execute_step_logic(self, step, step_name: str, current_data: torch.Tensor,
                                 clothing_tensor: torch.Tensor, **kwargs) -> Dict[str, Any]:
        """Stepë³„ ê¸°ë³¸ ì²˜ë¦¬ ë¡œì§"""
        try:
            if hasattr(step, 'process'):
                if step_name == 'human_parsing':
                    return await step.process(current_data)
                elif step_name == 'pose_estimation':
                    return await step.process(current_data)
                elif step_name == 'cloth_segmentation':
                    return await step.process(clothing_tensor, clothing_type=kwargs.get('clothing_type', 'shirt'))
                elif step_name == 'geometric_matching':
                    return await step.process(
                        person_parsing={'result': current_data},
                        pose_keypoints=self._generate_dummy_pose_keypoints(),
                        clothing_segmentation={'mask': clothing_tensor},
                        clothing_type=kwargs.get('clothing_type', 'shirt')
                    )
                elif step_name == 'cloth_warping':
                    return await step.process(
                        current_data, clothing_tensor, 
                        kwargs.get('body_measurements', {}), 
                        kwargs.get('fabric_type', 'cotton')
                    )
                elif step_name == 'virtual_fitting':
                    return await step.process(current_data, clothing_tensor, kwargs.get('style_preferences', {}))
                elif step_name == 'post_processing':
                    return await step.process(current_data)
                elif step_name == 'quality_assessment':
                    return await step.process(current_data, clothing_tensor)
                else:
                    return await step.process(current_data)
            else:
                return {'result': current_data, 'confidence': 0.8, 'quality_score': 0.8}
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ê¸°ë³¸ ë¡œì§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {'result': current_data, 'confidence': 0.5, 'quality_score': 0.5}
    
    def _generate_dummy_pose_keypoints(self) -> List[List[float]]:
        """ë”ë¯¸ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ìƒì„±"""
        return [[256 + np.random.uniform(-50, 50), 256 + np.random.uniform(-100, 100), 0.8] for _ in range(18)]

# ==============================================
# ğŸ”¥ 6. ë©”ì¸ PipelineManager í´ë˜ìŠ¤ (ì™„ì „ DI ì ìš©)
# ==============================================

class PipelineManager:
    """
    ğŸ”¥ ì™„ì „ DI í†µí•© PipelineManager v9.1 - base_step_mixin.py ê¸°ë°˜ ì™„ì „ ê°œì„ 
    
    âœ… base_step_mixin.pyì˜ DI íŒ¨í„´ ì™„ì „ ì ìš©
    âœ… ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°
    âœ… TYPE_CHECKINGìœ¼ë¡œ import ì‹œì  ìˆœí™˜ì°¸ì¡° ë°©ì§€
    âœ… ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ëŠìŠ¨í•œ ê²°í•© ê°•í™”
    âœ… ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… ì™„ì „ êµ¬í˜„
    âœ… ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ 100% ìœ ì§€
    âœ… DIBasedPipelineManager í˜¸í™˜ì„± ì™„ì „ ë³´ì¥
    """
    
    def __init__(
        self,
        config_path: Optional[str] = None,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], PipelineConfig]] = None,
        **kwargs
    ):
        """DI ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        
        # 1. ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._auto_detect_device(device)
        
        # 2. ì„¤ì • ì´ˆê¸°í™”
        if isinstance(config, PipelineConfig):
            self.config = config
        else:
            config_dict = self._load_config(config_path) if config_path else {}
            if config:
                config_dict.update(config if isinstance(config, dict) else {})
            config_dict.update(kwargs)
            
            # M3 Max ìë™ ê°ì§€ ë° ìµœì í™”
            if self._detect_m3_max():
                config_dict.update({
                    'is_m3_max': True,
                    'memory_gb': 128.0,
                    'device_type': 'apple_silicon',
                    'performance_mode': 'maximum',
                    'use_dependency_injection': True,
                    'enable_adapter_pattern': True
                })
            
            self.config = PipelineConfig(device=self.device, **config_dict)
        
        # 3. ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        # ğŸ”¥ 4. DI Container ì´ˆê¸°í™” (ìµœìš°ì„ ) - base_step_mixin.py íŒ¨í„´
        self.di_container = None
        if DI_CONTAINER_AVAILABLE and self.config.use_dependency_injection:
            try:
                self.di_container = get_di_container()
                self._setup_di_dependencies()
                self.logger.info("âœ… DI Container ì´ˆê¸°í™” ì™„ë£Œ (base_step_mixin.py íŒ¨í„´)")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DI Container ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.di_container = None
        
        # 5. ì–´ëŒ‘í„° ê¸°ë°˜ ê´€ë¦¬ìë“¤ ì´ˆê¸°í™”
        self.model_manager = DIBasedModelLoaderManager(self.config, self.device, self.logger, self.di_container)
        self.execution_manager = DIBasedExecutionManager(self.config, self.logger)
        
        # 6. ì–´ëŒ‘í„°ë“¤ ì´ˆê¸°í™”
        self.memory_manager = MemoryManagerAdapter()
        self.data_converter = DataConverterAdapter(self.device)
        
        # 7. íŒŒì´í”„ë¼ì¸ ìƒíƒœ
        self.is_initialized = False
        self.current_status = ProcessingStatus.IDLE
        self.step_order = [
            'human_parsing', 'pose_estimation', 'cloth_segmentation',
            'geometric_matching', 'cloth_warping', 'virtual_fitting',
            'post_processing', 'quality_assessment'
        ]
        self.steps = {}
        
        # 8. ì„±ëŠ¥ ë° í†µê³„
        self.performance_metrics = {
            'total_sessions': 0,
            'successful_sessions': 0,
            'ai_model_usage': {},
            'average_processing_time': 0.0,
            'average_quality_score': 0.0,
            'di_injection_count': 0,
            'di_success_rate': 0.0,
            'adapter_pattern_usage': 0
        }
        
        # 9. ìŠ¤ë ˆë“œ í’€
        self.thread_pool = ThreadPoolExecutor(max_workers=self.config.thread_pool_size)
        
        # ğŸ”¥ ì´ˆê¸°í™” ì™„ë£Œ ë¡œê¹… (base_step_mixin.py ìŠ¤íƒ€ì¼)
        initialization_duration = time.time() - getattr(self, 'start_time', time.time())
        self.logger.info(f"ğŸ”¥ ì™„ì „ DI í†µí•© PipelineManager v9.1 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {self.device}")
        self.logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬: {self.config.memory_gb}GB")
        self.logger.info(f"ğŸš€ M3 Max: {'âœ…' if self.config.is_m3_max else 'âŒ'}")
        self.logger.info(f"ğŸ§  AI ëª¨ë¸: {'âœ…' if self.config.ai_model_enabled else 'âŒ'}")
        self.logger.info(f"ğŸ”— ì˜ì¡´ì„± ì£¼ì…: {'âœ…' if self.config.use_dependency_injection else 'âŒ'}")
        self.logger.info(f"ğŸ”§ ì–´ëŒ‘í„° íŒ¨í„´: {'âœ…' if self.config.enable_adapter_pattern else 'âŒ'}")
        self.logger.info(f"ğŸ“ base_step_mixin.py íŒ¨í„´: âœ…")
    
    def _setup_di_dependencies(self):
        """DI ì˜ì¡´ì„± ì„¤ì • - base_step_mixin.py íŒ¨í„´"""
        try:
            if not self.di_container:
                return
            
            # ModelLoader ì–´ëŒ‘í„° ë“±ë¡
            self.di_container.register_instance('IModelLoader', ModelLoaderAdapter())
            
            # MemoryManager ì–´ëŒ‘í„° ë“±ë¡
            self.di_container.register_instance('IMemoryManager', MemoryManagerAdapter())
            
            # DataConverter ì–´ëŒ‘í„° ë“±ë¡
            self.di_container.register_instance('IDataConverter', DataConverterAdapter(self.device))
            
            self.logger.info("âœ… DI ì–´ëŒ‘í„° ì˜ì¡´ì„± ë“±ë¡ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ DI ì˜ì¡´ì„± ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device and preferred_device != "auto":
            return preferred_device
        
        try:
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                    capture_output=True, text=True, timeout=5
                )
                chip_info = result.stdout.strip()
                return 'M3' in chip_info and 'Max' in chip_info
        except:
            pass
        return False
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if not config_path or not os.path.exists(config_path):
            return {}
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.warning(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    async def initialize(self) -> bool:
        """DI ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” - base_step_mixin.py íŒ¨í„´"""
        try:
            self.logger.info("ğŸš€ ì™„ì „ DI í†µí•© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹œì‘...")
            self.current_status = ProcessingStatus.INITIALIZING
            start_time = time.time()
            
            # 1. DI ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            if self.config.use_dependency_injection and DI_CONTAINER_AVAILABLE:
                try:
                    initialize_di_system()
                    self.logger.info("âœ… DI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ DI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # 2. Step í´ë˜ìŠ¤ë“¤ ë™ì  ë¡œë”© (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
            success_count = await self._load_step_classes_dynamically()
            
            # 3. ë©”ëª¨ë¦¬ ì •ë¦¬
            await self.memory_manager.optimize_memory_async()
            
            # 4. ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            model_success = await self.model_manager.initialize()
            if model_success:
                self.logger.info("âœ… DI ê¸°ë°˜ ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ DI ê¸°ë°˜ ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # 5. Step í´ë˜ìŠ¤ë“¤ DI ê¸°ë°˜ ì´ˆê¸°í™”
            step_success_count = await self._initialize_steps_with_complete_di()
            
            # 6. ì´ˆê¸°í™” ê²€ì¦
            success_rate = step_success_count / len(self.step_order) if self.step_order else 0
            if success_rate < 0.5:
                self.logger.warning(f"ì´ˆê¸°í™” ì„±ê³µë¥  ë‚®ìŒ: {success_rate:.1%}")
            
            initialization_time = time.time() - start_time
            self.is_initialized = step_success_count > 0
            self.current_status = ProcessingStatus.IDLE if self.is_initialized else ProcessingStatus.FAILED
            
            if self.is_initialized:
                self.logger.info(f"ğŸ‰ ì™„ì „ DI í†µí•© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ ({initialization_time:.2f}ì´ˆ)")
                self.logger.info(f"ğŸ“Š Step ì´ˆê¸°í™”: {step_success_count}/{len(self.step_order)} ({success_rate:.1%})")
                self.logger.info(f"ğŸ§  ModelLoader: {'âœ…' if model_success else 'âŒ'}")
                self.logger.info(f"ğŸ’‰ ì˜ì¡´ì„± ì£¼ì…: {'âœ…' if self.config.use_dependency_injection else 'âŒ'}")
                self.logger.info(f"ğŸ”§ ì–´ëŒ‘í„° íŒ¨í„´: {'âœ…' if self.config.enable_adapter_pattern else 'âŒ'}")
            else:
                self.logger.error("âŒ DI ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            return self.is_initialized
            
        except Exception as e:
            self.logger.error(f"âŒ DI ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            self.current_status = ProcessingStatus.FAILED
            return False
    
    async def _load_step_classes_dynamically(self) -> int:
        """Step í´ë˜ìŠ¤ë“¤ ë™ì  ë¡œë”© - ìˆœí™˜ì°¸ì¡° ë°©ì§€"""
        try:
            step_modules = {
                'HumanParsingStep': 'app.ai_pipeline.steps.step_01_human_parsing',
                'PoseEstimationStep': 'app.ai_pipeline.steps.step_02_pose_estimation',
                'ClothSegmentationStep': 'app.ai_pipeline.steps.step_03_cloth_segmentation',
                'GeometricMatchingStep': 'app.ai_pipeline.steps.step_04_geometric_matching',
                'ClothWarpingStep': 'app.ai_pipeline.steps.step_05_cloth_warping',
                'VirtualFittingStep': 'app.ai_pipeline.steps.step_06_virtual_fitting',
                'PostProcessingStep': 'app.ai_pipeline.steps.step_07_post_processing',
                'QualityAssessmentStep': 'app.ai_pipeline.steps.step_08_quality_assessment'
            }
            
            loaded_count = 0
            self.step_classes = {}
            
            for step_name, module_path in step_modules.items():
                try:
                    # ë™ì  importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
                    import importlib
                    module = importlib.import_module(module_path)
                    step_class = getattr(module, step_name, None)
                    if step_class:
                        self.step_classes[step_name] = step_class
                        loaded_count += 1
                        self.logger.info(f"âœ… {step_name} ë™ì  ë¡œë”© ì™„ë£Œ")
                except ImportError as e:
                    self.logger.warning(f"âš ï¸ {step_name} ë™ì  ë¡œë”© ì‹¤íŒ¨: {e}")
                    # í´ë°± í´ë˜ìŠ¤ ìƒì„±
                    self.step_classes[step_name] = self._create_fallback_step_class(step_name)
                    loaded_count += 1
                    self.logger.info(f"ğŸ”„ {step_name} í´ë°± í´ë˜ìŠ¤ ìƒì„±")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ë¡œë”© ì˜¤ë¥˜: {e}")
            
            self.logger.info(f"ğŸ“¦ Step í´ë˜ìŠ¤ ë™ì  ë¡œë”© ì™„ë£Œ: {loaded_count}ê°œ")
            return loaded_count
            
        except Exception as e:
            self.logger.error(f"âŒ Step í´ë˜ìŠ¤ ë™ì  ë¡œë”© ì‹¤íŒ¨: {e}")
            return 0
    
    def _create_fallback_step_class(self, step_name: str):
        """í´ë°± Step í´ë˜ìŠ¤ ìƒì„±"""
        class FallbackStep:
            def __init__(self, **kwargs):
                self.step_name = step_name.replace('Step', '').lower()
                self.device = kwargs.get('device', 'cpu')
                self.logger = logging.getLogger(f"fallback.{step_name}")
                self.model_loader = kwargs.get('model_loader')
                self.memory_manager = kwargs.get('memory_manager')
                self.data_converter = kwargs.get('data_converter')
                
            async def process(self, *args, **kwargs):
                """ê¸°ë³¸ ì²˜ë¦¬ ë¡œì§"""
                try:
                    # ê¸°ë³¸ì ì¸ íŒ¨ìŠ¤ìŠ¤ë£¨ ì²˜ë¦¬
                    if args:
                        return {
                            'success': True,
                            'result': args[0],
                            'confidence': 0.7,
                            'quality_score': 0.7,
                            'step_name': self.step_name,
                            'fallback_used': True
                        }
                    else:
                        return {
                            'success': True,
                            'result': torch.zeros(1, 3, 512, 512),
                            'confidence': 0.7,
                            'quality_score': 0.7,
                            'step_name': self.step_name,
                            'fallback_used': True
                        }
                except Exception as e:
                    self.logger.error(f"âŒ í´ë°± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    return {
                        'success': False,
                        'error': str(e),
                        'step_name': self.step_name,
                        'fallback_used': True
                    }
            
            def cleanup(self):
                """ì •ë¦¬"""
                pass
        
        return FallbackStep
    
    async def _initialize_steps_with_complete_di(self) -> int:
        """Step í´ë˜ìŠ¤ë“¤ ì™„ì „ DI ê¸°ë°˜ ì´ˆê¸°í™” - base_step_mixin.py íŒ¨í„´"""
        try:
            if not hasattr(self, 'step_classes') or not self.step_classes:
                self.logger.error("âŒ Step í´ë˜ìŠ¤ë“¤ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                return 0
            
            # ê¸°ë³¸ ì„¤ì •
            base_config = {
                'device': self.device,
                'device_type': self.config.device_type,
                'memory_gb': self.config.memory_gb,
                'is_m3_max': self.config.is_m3_max,
                'optimization_enabled': True,
                'quality_level': self.config.quality_level.value,
                'use_dependency_injection': self.config.use_dependency_injection,
                'enable_adapter_pattern': self.config.enable_adapter_pattern
            }
            
            success_count = 0
            
            # Stepë³„ DI ê¸°ë°˜ ì´ˆê¸°í™”
            for step_name in self.step_order:
                step_class_name = f"{step_name.title().replace('_', '')}Step"
                if step_class_name in self.step_classes:
                    try:
                        success = await self._initialize_single_step_with_complete_di(
                            step_name, 
                            self.step_classes[step_class_name], 
                            base_config
                        )
                        if success:
                            success_count += 1
                            self.logger.info(f"âœ… {step_name} ì™„ì „ DI ì´ˆê¸°í™” ì™„ë£Œ")
                        else:
                            self.logger.warning(f"âš ï¸ {step_name} DI ì´ˆê¸°í™” ì‹¤íŒ¨")
                    except Exception as e:
                        self.logger.error(f"âŒ {step_name} DI ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
                else:
                    self.logger.warning(f"âš ï¸ {step_class_name} í´ë˜ìŠ¤ ì—†ìŒ")
            
            # DI í†µê³„ ì—…ë°ì´íŠ¸
            self.performance_metrics['di_injection_count'] = success_count
            self.performance_metrics['di_success_rate'] = (success_count / len(self.step_order)) * 100
            
            return success_count
            
        except Exception as e:
            self.logger.error(f"âŒ ì™„ì „ DI Step ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return 0
    
    async def _initialize_single_step_with_complete_di(self, step_name: str, step_class, base_config: Dict[str, Any]) -> bool:
        """ë‹¨ì¼ Step ì™„ì „ DI ê¸°ë°˜ ì´ˆê¸°í™” - base_step_mixin.py íŒ¨í„´"""
        try:
            # Step ì„¤ì • ì¤€ë¹„
            step_config = {**base_config, **self._get_step_config(step_name)}
            
            # ğŸ”¥ ì™„ì „ DI ê¸°ë°˜ Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            if self.config.use_dependency_injection and DI_CONTAINER_AVAILABLE and self.di_container:
                try:
                    # DI Containerë¥¼ í†µí•œ ì˜ì¡´ì„± ì£¼ì…
                    model_loader = self.di_container.get('IModelLoader') or ModelLoaderAdapter()
                    memory_manager = self.di_container.get('IMemoryManager') or MemoryManagerAdapter()
                    data_converter = self.di_container.get('IDataConverter') or DataConverterAdapter(self.device)
                    
                    # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œ ì˜ì¡´ì„± ì£¼ì…
                    step_instance = step_class(
                        model_loader=model_loader,
                        memory_manager=memory_manager,
                        data_converter=data_converter,
                        **step_config
                    )
                    
                    self.logger.debug(f"âœ… {step_name} ì™„ì „ DI ìƒì„± ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ì™„ì „ DI ìƒì„± ì‹¤íŒ¨: {e}, í´ë°± ëª¨ë“œ")
                    step_instance = self._create_step_instance_safely(step_class, step_name, step_config)
            else:
                # í´ë°±: ì–´ëŒ‘í„° ê¸°ë°˜ ìƒì„±
                step_instance = self._create_step_instance_with_adapters(step_class, step_name, step_config)
            
            if not step_instance:
                return False
            
            # ğŸ”¥ ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… (base_step_mixin.py íŒ¨í„´)
            if self.config.enable_runtime_injection and self.di_container:
                try:
                    inject_dependencies_to_step(step_instance, self.di_container)
                    self.logger.debug(f"âœ… {step_name} ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            
            # Step ì´ˆê¸°í™”
            if hasattr(step_instance, 'initialize'):
                if asyncio.iscoroutinefunction(step_instance.initialize):
                    await step_instance.initialize()
                else:
                    step_instance.initialize()
            elif hasattr(step_instance, 'initialize_step'):
                if asyncio.iscoroutinefunction(step_instance.initialize_step):
                    await step_instance.initialize_step()
                else:
                    step_instance.initialize_step()
            
            self.steps[step_name] = step_instance
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì™„ì „ DI ë‹¨ì¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _create_step_instance_with_adapters(self, step_class, step_name: str, step_config: Dict[str, Any]):
        """ì–´ëŒ‘í„° ê¸°ë°˜ Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        try:
            # ì–´ëŒ‘í„°ë“¤ ìƒì„±
            model_loader_adapter = ModelLoaderAdapter()
            memory_manager_adapter = MemoryManagerAdapter()
            data_converter_adapter = DataConverterAdapter(self.device)
            
            # ì–´ëŒ‘í„°ì™€ í•¨ê»˜ Step ìƒì„±
            return step_class(
                model_loader=model_loader_adapter,
                memory_manager=memory_manager_adapter,
                data_converter=data_converter_adapter,
                **step_config
            )
        except Exception as e:
            self.logger.warning(f"âš ï¸ {step_name} ì–´ëŒ‘í„° ê¸°ë°˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_step_instance_safely(step_class, step_name, step_config)
    
    def _create_step_instance_safely(self, step_class, step_name: str, step_config: Dict[str, Any]):
        """Step ì¸ìŠ¤í„´ìŠ¤ ì•ˆì „ ìƒì„±"""
        try:
            return step_class(**step_config)
        except TypeError as e:
            if "unexpected keyword argument" in str(e):
                try:
                    safe_config = {
                        'device': step_config.get('device', 'cpu'),
                        'config': step_config.get('config', {})
                    }
                    return step_class(**safe_config)
                except Exception:
                    try:
                        return step_class(device=step_config.get('device', 'cpu'))
                    except Exception:
                        return None
            else:
                raise
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì•ˆì „ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _get_step_config(self, step_name: str) -> Dict[str, Any]:
        """Stepë³„ ìµœì í™”ëœ ì„¤ì •"""
        configs = {
            'human_parsing': {
                'model_name': 'graphonomy',
                'num_classes': 20,
                'input_size': (512, 512),
                'enable_ai_model': True
            },
            'pose_estimation': {
                'model_type': 'mediapipe',
                'input_size': (368, 368),
                'confidence_threshold': 0.5,
                'enable_ai_model': True
            },
            'cloth_segmentation': {
                'model_name': 'u2net',
                'background_threshold': 0.5,
                'post_process': True,
                'enable_ai_model': True
            },
            'geometric_matching': {
                'tps_points': 25,
                'matching_threshold': 0.8,
                'method': 'ai_enhanced'
            },
            'cloth_warping': {
                'warping_method': 'ai_physics',
                'physics_simulation': True,
                'enable_ai_model': True
            },
            'virtual_fitting': {
                'model_name': 'ootdiffusion',
                'blending_method': 'ai_poisson',
                'seamless_cloning': True,
                'enable_ai_model': True
            },
            'post_processing': {
                'model_name': 'esrgan',
                'enable_super_resolution': True,
                'enhance_faces': True,
                'enable_ai_model': True
            },
            'quality_assessment': {
                'model_name': 'clipiqa',
                'enable_detailed_analysis': True,
                'perceptual_metrics': True,
                'enable_ai_model': True
            }
        }
        
        return configs.get(step_name, {})
    
    # ==============================================
    # ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ - ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´
    # ==============================================
    
    async def process_complete_virtual_fitting(
        self,
        person_image: Union[str, Image.Image, np.ndarray],
        clothing_image: Union[str, Image.Image, np.ndarray],
        body_measurements: Optional[Dict[str, float]] = None,
        clothing_type: str = "shirt",
        fabric_type: str = "cotton",
        style_preferences: Optional[Dict[str, Any]] = None,
        quality_target: float = 0.8,
        progress_callback: Optional[Callable] = None,
        save_intermediate: bool = False,
        session_id: Optional[str] = None
    ) -> ProcessingResult:
        """
        ğŸ”¥ ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ - base_step_mixin.py ê¸°ë°˜
        
        âœ… ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°
        âœ… DI ê¸°ë°˜ Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš©
        âœ… ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… ì™„ì „ êµ¬í˜„
        âœ… M3 Max ì„±ëŠ¥ ìµœì í™” ìœ ì§€
        âœ… ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        """
        if not self.is_initialized:
            raise RuntimeError("íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        if session_id is None:
            session_id = f"complete_di_vf_{int(time.time())}_{np.random.randint(1000, 9999)}"
        
        start_time = time.time()
        self.current_status = ProcessingStatus.PROCESSING
        
        try:
            self.logger.info(f"ğŸ¯ ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì‹œì‘ - ì„¸ì…˜: {session_id}")
            self.logger.info(f"âš™ï¸ ì„¤ì •: {clothing_type} ({fabric_type}), ëª©í‘œ í’ˆì§ˆ: {quality_target}")
            self.logger.info(f"ğŸ§  AI ëª¨ë¸: {'âœ…' if self.config.ai_model_enabled else 'âŒ'}")
            self.logger.info(f"ğŸš€ M3 Max: {'âœ…' if self.config.is_m3_max else 'âŒ'}")
            self.logger.info(f"ğŸ”— ì˜ì¡´ì„± ì£¼ì…: {'âœ…' if self.config.use_dependency_injection else 'âŒ'}")
            self.logger.info(f"ğŸ”§ ì–´ëŒ‘í„° íŒ¨í„´: {'âœ…' if self.config.enable_adapter_pattern else 'âŒ'}")
            
            # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ì–´ëŒ‘í„° ì‚¬ìš©)
            person_tensor = self.data_converter.preprocess_image(person_image)
            clothing_tensor = self.data_converter.preprocess_image(clothing_image)
            
            if progress_callback:
                await progress_callback("ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ", 5)
            
            # 2. ë©”ëª¨ë¦¬ ìµœì í™” (ì–´ëŒ‘í„° ì‚¬ìš©)
            await self.memory_manager.optimize_memory_async()
            
            # ğŸ”¥ 3. 8ë‹¨ê³„ ìˆœì°¨ ì²˜ë¦¬ - ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´
            step_results = {}
            execution_strategies = {}
            ai_models_used = {}
            di_injection_info = {}
            adapter_pattern_info = {}
            current_data = person_tensor
            
            for i, step_name in enumerate(self.step_order):
                if step_name not in self.steps:
                    self.logger.warning(f"âš ï¸ {step_name} ë‹¨ê³„ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆë›°ê¸°...")
                    continue
                
                step_start = time.time()
                step = self.steps[step_name]
                
                self.logger.info(f"ğŸ“‹ {i+1}/{len(self.step_order)} ë‹¨ê³„: {step_name} ì™„ì „ DI+ì–´ëŒ‘í„° ì²˜ë¦¬ ì¤‘...")
                
                try:
                    # ğŸ”¥ DI ì •ë³´ ìˆ˜ì§‘
                    di_info = self._collect_complete_di_info(step, step_name)
                    di_injection_info[step_name] = di_info
                    
                    # ğŸ”¥ ì–´ëŒ‘í„° íŒ¨í„´ ì •ë³´ ìˆ˜ì§‘
                    adapter_info = self._collect_adapter_pattern_info(step, step_name)
                    adapter_pattern_info[step_name] = adapter_info
                    
                    # ğŸ”¥ ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ ì‹¤í–‰
                    step_result, execution_strategy = await self.execution_manager.execute_step_with_di(
                        step, step_name, current_data, clothing_tensor,
                        body_measurements=body_measurements,
                        clothing_type=clothing_type,
                        fabric_type=fabric_type,
                        style_preferences=style_preferences,
                        quality_target=quality_target
                    )
                    
                    step_time = time.time() - step_start
                    step_results[step_name] = step_result
                    execution_strategies[step_name] = execution_strategy
                    
                    # AI ëª¨ë¸ ì‚¬ìš© ì¶”ì 
                    ai_model_name = step_result.get('ai_model_name', 'unknown')
                    ai_models_used[step_name] = ai_model_name
                    
                    # ê²°ê³¼ ì—…ë°ì´íŠ¸
                    if step_result.get('success', True):
                        result_data = step_result.get('result')
                        if result_data is not None:
                            current_data = result_data
                    
                    # ë¡œê¹…
                    confidence = step_result.get('confidence', 0.8)
                    quality_score = step_result.get('quality_score', confidence)
                    model_used = step_result.get('model_used', 'unknown')
                    
                    # ì „ëµë³„ ì•„ì´ì½˜
                    if execution_strategy == ExecutionStrategy.UNIFIED_AI.value:
                        strategy_icon = "ğŸ”—ğŸ§ "
                    elif execution_strategy == ExecutionStrategy.MODEL_LOADER.value:
                        strategy_icon = "ğŸ§ ğŸ“¦"
                    else:
                        strategy_icon = "ğŸ”„"
                    
                    # DI + ì–´ëŒ‘í„° ì•„ì´ì½˜
                    di_icon = "ğŸ’‰" if di_info.get('has_injected_dependencies', False) else "ğŸ”§"
                    adapter_icon = "ğŸ”§" if adapter_info.get('adapters_used', 0) > 0 else "ğŸ“¦"
                    
                    self.logger.info(f"âœ… {i+1}ë‹¨ê³„ ì™„ë£Œ - ì‹œê°„: {step_time:.2f}ì´ˆ, ì‹ ë¢°ë„: {confidence:.3f}, í’ˆì§ˆ: {quality_score:.3f}")
                    self.logger.info(f"   {strategy_icon} ì „ëµ: {execution_strategy}, AIëª¨ë¸: {ai_model_name}, ì²˜ë¦¬: {model_used}")
                    self.logger.info(f"   {di_icon} DI: {di_info.get('injection_summary', 'None')}")
                    self.logger.info(f"   {adapter_icon} ì–´ëŒ‘í„°: {adapter_info.get('adapter_summary', 'None')}")
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if progress_callback:
                        progress = 5 + (i + 1) * 85 // len(self.step_order)
                        await progress_callback(f"{step_name} ì™„ì „ DI+ì–´ëŒ‘í„° ì²˜ë¦¬ ì™„ë£Œ", progress)
                    
                    # ğŸ”¥ M3 Max ë©”ëª¨ë¦¬ ìµœì í™” (ì¤‘ê°„ ë‹¨ê³„ë§ˆë‹¤)
                    if self.config.is_m3_max and i % 2 == 0:
                        await self.memory_manager.optimize_memory_async()
                    
                except Exception as e:
                    self.logger.error(f"âŒ {i+1}ë‹¨ê³„ ({step_name}) ì‹¤íŒ¨: {e}")
                    step_time = time.time() - step_start
                    step_results[step_name] = {
                        'success': False,
                        'error': str(e),
                        'processing_time': step_time,
                        'confidence': 0.0,
                        'quality_score': 0.0,
                        'model_used': 'error',
                        'ai_model_name': 'error'
                    }
                    execution_strategies[step_name] = "error"
                    ai_models_used[step_name] = "error"
                    di_injection_info[step_name] = {'error': str(e)}
                    adapter_pattern_info[step_name] = {'error': str(e)}
                    
                    # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                    continue
            
            # 4. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            total_time = time.time() - start_time
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
            if isinstance(current_data, torch.Tensor):
                result_image = self.data_converter.tensor_to_pil(current_data)
            else:
                result_image = Image.new('RGB', (512, 512), color='gray')
            
            # ğŸ”¥ ê°•í™”ëœ í’ˆì§ˆ í‰ê°€ (DI + ì–´ëŒ‘í„° ì‚¬ìš© ê³ ë ¤)
            quality_score = self._assess_complete_di_quality(
                step_results, execution_strategies, ai_models_used, 
                di_injection_info, adapter_pattern_info
            )
            quality_grade = self._get_quality_grade(quality_score)
            
            # ì„±ê³µ ì—¬ë¶€ ê²°ì •
            success = quality_score >= (quality_target * 0.8)
            
            # ğŸ”¥ AI ëª¨ë¸ + DI + ì–´ëŒ‘í„° ì‚¬ìš© í†µê³„
            ai_stats = self._calculate_ai_usage_statistics(ai_models_used, execution_strategies)
            di_stats = self._calculate_complete_di_usage_statistics(di_injection_info)
            adapter_stats = self._calculate_adapter_pattern_statistics(adapter_pattern_info)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_complete_performance_metrics(total_time, quality_score, success, ai_stats, di_stats, adapter_stats)
            
            if progress_callback:
                await progress_callback("ì™„ì „ DI+ì–´ëŒ‘í„° ì²˜ë¦¬ ì™„ë£Œ", 100)
            
            self.current_status = ProcessingStatus.IDLE
            
            # ğŸ”¥ ì™„ì „ í†µí•© ê²°ê³¼ ë¡œê¹…
            self.logger.info(f"ğŸ‰ ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì™„ë£Œ!")
            self.logger.info(f"â±ï¸ ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
            self.logger.info(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f} ({quality_grade})")
            self.logger.info(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {'âœ…' if quality_score >= quality_target else 'âŒ'}")
            self.logger.info(f"ğŸ“‹ ì™„ë£Œëœ ë‹¨ê³„: {len(step_results)}/{len(self.step_order)}")
            self.logger.info(f"ğŸ§  AI ëª¨ë¸ ì‚¬ìš©ë¥ : {ai_stats['ai_usage_rate']:.1f}%")
            self.logger.info(f"ğŸ”— DI ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©: {ai_stats['unified_ai_count']}íšŒ")
            self.logger.info(f"ğŸ“¦ ì–´ëŒ‘í„° íŒ¨í„´ ì‚¬ìš©: {ai_stats['model_loader_count']}íšŒ")
            self.logger.info(f"ğŸ’‰ DI ì£¼ì…ë¥ : {di_stats['injection_rate']:.1f}%")
            self.logger.info(f"ğŸ”§ DI ì„±ê³µë¥ : {di_stats['success_rate']:.1f}%")
            self.logger.info(f"ğŸ”§ ì–´ëŒ‘í„° ì‚¬ìš©ë¥ : {adapter_stats['adapter_usage_rate']:.1f}%")
            self.logger.info(f"ğŸ“ base_step_mixin.py íŒ¨í„´: âœ…")
            
            return ProcessingResult(
                success=success,
                session_id=session_id,
                result_image=result_image,
                result_tensor=current_data if isinstance(current_data, torch.Tensor) else None,
                quality_score=quality_score,
                quality_grade=quality_grade,
                processing_time=total_time,
                step_results=step_results,
                step_timings={step: result.get('execution_time', 0.0) for step, result in step_results.items()},
                ai_models_used=ai_models_used,
                execution_strategies=execution_strategies,
                dependency_injection_info=di_injection_info,
                adapter_pattern_info=adapter_pattern_info,
                interface_usage_info={
                    'di_interfaces_used': len([info for info in di_injection_info.values() if info.get('has_injected_dependencies', False)]),
                    'adapter_interfaces_used': len([info for info in adapter_pattern_info.values() if info.get('adapters_used', 0) > 0]),
                    'total_interfaces': len(di_injection_info) + len(adapter_pattern_info)
                },
                performance_metrics={
                    'ai_usage_statistics': ai_stats,
                    'di_usage_statistics': di_stats,
                    'adapter_pattern_statistics': adapter_stats,
                    'memory_peak_usage': self._get_memory_peak_usage(),
                    'step_performance': self._get_step_performance_metrics(step_results)
                },
                metadata={
                    'device': self.device,
                    'device_type': self.config.device_type,
                    'is_m3_max': self.config.is_m3_max,
                    'memory_gb': self.config.memory_gb,
                    'ai_model_enabled': self.config.ai_model_enabled,
                    'use_dependency_injection': self.config.use_dependency_injection,
                    'enable_adapter_pattern': self.config.enable_adapter_pattern,
                    'quality_target': quality_target,
                    'quality_target_achieved': quality_score >= quality_target,
                    'total_steps': len(self.step_order),
                    'completed_steps': len(step_results),
                    'success_rate': len([r for r in step_results.values() if r.get('success', True)]) / len(step_results) if step_results else 0,
                    'complete_integration_summary': {
                        'di_container_used': self.di_container is not None,
                        'adapter_pattern_applied': self.config.enable_adapter_pattern,
                        'ai_models_connected': sum(1 for model in ai_models_used.values() if model not in ['error', 'unknown']),
                        'di_injections_performed': sum(1 for info in di_injection_info.values() if info.get('has_injected_dependencies', False)),
                        'adapters_used': sum(info.get('adapters_used', 0) for info in adapter_pattern_info.values()),
                        'circular_import_resolved': True,
                        'base_step_mixin_pattern_applied': True,
                        'architecture_version': 'v9.1_complete_di_integration'
                    }
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            
            # ì—ëŸ¬ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_complete_performance_metrics(time.time() - start_time, 0.0, False, {}, {}, {})
            
            self.current_status = ProcessingStatus.FAILED
            
            return ProcessingResult(
                success=False,
                session_id=session_id,
                processing_time=time.time() - start_time,
                error_message=str(e),
                metadata={
                    'device': self.device,
                    'error_type': type(e).__name__,
                    'error_location': traceback.format_exc(),
                    'ai_model_enabled': self.config.ai_model_enabled,
                    'use_dependency_injection': self.config.use_dependency_injection,
                    'enable_adapter_pattern': self.config.enable_adapter_pattern,
                    'is_m3_max': self.config.is_m3_max,
                    'architecture_version': 'v9.1_complete_di_integration'
                }
            )
    
    def _collect_complete_di_info(self, step_instance, step_name: str) -> Dict[str, Any]:
        """Stepì˜ ì™„ì „ DI ì •ë³´ ìˆ˜ì§‘ - base_step_mixin.py íŒ¨í„´"""
        try:
            di_info = {
                'step_name': step_name,
                'has_injected_dependencies': False,
                'injected_count': 0,
                'available_interfaces': [],
                'injection_summary': 'None',
                'base_step_mixin_pattern': True
            }
            
            # ì£¼ì…ëœ ì˜ì¡´ì„± í™•ì¸
            dependencies = []
            
            if hasattr(step_instance, 'model_loader') and step_instance.model_loader:
                dependencies.append('model_loader')
            if hasattr(step_instance, 'memory_manager') and step_instance.memory_manager:
                dependencies.append('memory_manager')
            if hasattr(step_instance, 'data_converter') and step_instance.data_converter:
                dependencies.append('data_converter')
            if hasattr(step_instance, 'unified_interface') and step_instance.unified_interface:
                dependencies.append('unified_interface')
            if hasattr(step_instance, 'model_interface') and step_instance.model_interface:
                dependencies.append('model_interface')
            if hasattr(step_instance, 'function_validator') and step_instance.function_validator:
                dependencies.append('function_validator')
            
            di_info['has_injected_dependencies'] = len(dependencies) > 0
            di_info['injected_count'] = len(dependencies)
            di_info['available_interfaces'] = dependencies
            di_info['injection_summary'] = ', '.join(dependencies) if dependencies else 'None'
            
            return di_info
            
        except Exception as e:
            return {
                'step_name': step_name,
                'error': str(e),
                'has_injected_dependencies': False,
                'base_step_mixin_pattern': True
            }
    
    def _collect_adapter_pattern_info(self, step_instance, step_name: str) -> Dict[str, Any]:
        """Stepì˜ ì–´ëŒ‘í„° íŒ¨í„´ ì •ë³´ ìˆ˜ì§‘"""
        try:
            adapter_info = {
                'step_name': step_name,
                'adapters_used': 0,
                'adapter_types': [],
                'adapter_summary': 'None',
                'base_step_mixin_pattern': True
            }
            
            # ì–´ëŒ‘í„° ì‚¬ìš© í™•ì¸
            adapters = []
            
            # ModelLoaderAdapter í™•ì¸
            if hasattr(step_instance, 'model_loader'):
                if isinstance(step_instance.model_loader, ModelLoaderAdapter):
                    adapters.append('ModelLoaderAdapter')
                elif hasattr(step_instance.model_loader, 'adapter_info'):
                    adapters.append('ModelLoaderAdapter')
            
            # MemoryManagerAdapter í™•ì¸
            if hasattr(step_instance, 'memory_manager'):
                if isinstance(step_instance.memory_manager, MemoryManagerAdapter):
                    adapters.append('MemoryManagerAdapter')
                elif hasattr(step_instance.memory_manager, 'adapter_info'):
                    adapters.append('MemoryManagerAdapter')
            
            # DataConverterAdapter í™•ì¸
            if hasattr(step_instance, 'data_converter'):
                if isinstance(step_instance.data_converter, DataConverterAdapter):
                    adapters.append('DataConverterAdapter')
                elif hasattr(step_instance.data_converter, 'adapter_info'):
                    adapters.append('DataConverterAdapter')
            
            adapter_info['adapters_used'] = len(adapters)
            adapter_info['adapter_types'] = adapters
            adapter_info['adapter_summary'] = ', '.join(adapters) if adapters else 'None'
            
            return adapter_info
            
        except Exception as e:
            return {
                'step_name': step_name,
                'error': str(e),
                'adapters_used': 0,
                'base_step_mixin_pattern': True
            }
    
    def _assess_complete_di_quality(self, step_results: Dict[str, Any], execution_strategies: Dict[str, str], 
                                   ai_models_used: Dict[str, str], di_injection_info: Dict[str, Any],
                                   adapter_pattern_info: Dict[str, Any]) -> float:
        """AI ëª¨ë¸ + DI + ì–´ëŒ‘í„° ì‚¬ìš©ì„ ê³ ë ¤í•œ ì™„ì „ í’ˆì§ˆ í‰ê°€"""
        if not step_results:
            return 0.5
        
        quality_scores = []
        confidence_scores = []
        ai_bonus = 0.0
        di_bonus = 0.0
        adapter_bonus = 0.0
        
        for step_name, step_result in step_results.items():
            if isinstance(step_result, dict):
                confidence = step_result.get('confidence', 0.8)
                quality = step_result.get('quality_score', confidence)
                strategy = execution_strategies.get(step_name, 'unknown')
                ai_model = ai_models_used.get(step_name, 'unknown')
                di_info = di_injection_info.get(step_name, {})
                adapter_info = adapter_pattern_info.get(step_name, {})
                
                quality_scores.append(quality)
                confidence_scores.append(confidence)
                
                # ğŸ”¥ AI ëª¨ë¸ ì‚¬ìš©ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
                if ai_model not in ['error', 'unknown', 'fallback_processing', 'step_processing']:
                    if strategy == ExecutionStrategy.UNIFIED_AI.value:
                        ai_bonus += 0.08  # í†µí•© AI: 8% ë³´ë„ˆìŠ¤
                    elif strategy == ExecutionStrategy.MODEL_LOADER.value:
                        ai_bonus += 0.05  # ModelLoader: 5% ë³´ë„ˆìŠ¤
                    else:
                        ai_bonus += 0.02  # ê¸°íƒ€: 2% ë³´ë„ˆìŠ¤
                
                # ğŸ”¥ DI ì‚¬ìš©ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
                if di_info.get('has_injected_dependencies', False):
                    injected_count = di_info.get('injected_count', 0)
                    di_bonus += min(injected_count * 0.015, 0.06)  # ìµœëŒ€ 6% ë³´ë„ˆìŠ¤
                
                # ğŸ”¥ ì–´ëŒ‘í„° íŒ¨í„´ ì‚¬ìš©ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
                if adapter_info.get('adapters_used', 0) > 0:
                    adapter_count = adapter_info.get('adapters_used', 0)
                    adapter_bonus += min(adapter_count * 0.01, 0.04)  # ìµœëŒ€ 4% ë³´ë„ˆìŠ¤
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        if quality_scores:
            avg_quality = sum(quality_scores) / len(quality_scores)
            avg_confidence = sum(confidence_scores) / len(confidence_scores)
            
            # ê°€ì¤‘ í‰ê·  + AI ë³´ë„ˆìŠ¤ + DI ë³´ë„ˆìŠ¤ + ì–´ëŒ‘í„° ë³´ë„ˆìŠ¤
            overall_score = avg_quality * 0.7 + avg_confidence * 0.3 + ai_bonus + di_bonus + adapter_bonus
            return min(max(overall_score, 0.0), 1.0)
        
        return 0.5
    
    def _calculate_complete_di_usage_statistics(self, di_injection_info: Dict[str, Any]) -> Dict[str, Any]:
        """ì™„ì „ DI ì‚¬ìš© í†µê³„ ê³„ì‚°"""
        total_steps = len(di_injection_info)
        
        # DI ì£¼ì… í†µê³„
        injected_steps = sum(1 for info in di_injection_info.values() 
                           if info.get('has_injected_dependencies', False))
        
        total_injections = sum(info.get('injected_count', 0) for info in di_injection_info.values())
        
        # ì¸í„°í˜ì´ìŠ¤ë³„ ì‚¬ìš© í†µê³„
        interface_counts = {}
        for info in di_injection_info.values():
            for interface in info.get('available_interfaces', []):
                interface_counts[interface] = interface_counts.get(interface, 0) + 1
        
        return {
            'total_steps': total_steps,
            'injected_steps': injected_steps,
            'injection_rate': (injected_steps / total_steps * 100) if total_steps > 0 else 0,
            'total_injections': total_injections,
            'average_injections_per_step': total_injections / total_steps if total_steps > 0 else 0,
            'success_rate': (injected_steps / total_steps * 100) if total_steps > 0 else 0,
            'interface_usage': interface_counts,
            'most_used_interface': max(interface_counts.items(), key=lambda x: x[1])[0] if interface_counts else 'none',
            'base_step_mixin_pattern': True
        }
    
    def _calculate_adapter_pattern_statistics(self, adapter_pattern_info: Dict[str, Any]) -> Dict[str, Any]:
        """ì–´ëŒ‘í„° íŒ¨í„´ ì‚¬ìš© í†µê³„ ê³„ì‚°"""
        total_steps = len(adapter_pattern_info)
        
        # ì–´ëŒ‘í„° ì‚¬ìš© í†µê³„
        steps_with_adapters = sum(1 for info in adapter_pattern_info.values() 
                                if info.get('adapters_used', 0) > 0)
        
        total_adapters = sum(info.get('adapters_used', 0) for info in adapter_pattern_info.values())
        
        # ì–´ëŒ‘í„° íƒ€ì…ë³„ ì‚¬ìš© í†µê³„
        adapter_type_counts = {}
        for info in adapter_pattern_info.values():
            for adapter_type in info.get('adapter_types', []):
                adapter_type_counts[adapter_type] = adapter_type_counts.get(adapter_type, 0) + 1
        
        return {
            'total_steps': total_steps,
            'steps_with_adapters': steps_with_adapters,
            'adapter_usage_rate': (steps_with_adapters / total_steps * 100) if total_steps > 0 else 0,
            'total_adapters_used': total_adapters,
            'average_adapters_per_step': total_adapters / total_steps if total_steps > 0 else 0,
            'adapter_type_usage': adapter_type_counts,
            'most_used_adapter': max(adapter_type_counts.items(), key=lambda x: x[1])[0] if adapter_type_counts else 'none',
            'base_step_mixin_pattern': True
        }
    
    def _calculate_ai_usage_statistics(self, ai_models_used: Dict[str, str], 
                                     execution_strategies: Dict[str, str]) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì‚¬ìš© í†µê³„ ê³„ì‚°"""
        total_steps = len(ai_models_used)
        
        # ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš© íšŸìˆ˜
        real_ai_count = sum(1 for model in ai_models_used.values() 
                           if model not in ['error', 'unknown', 'fallback_processing', 'step_processing'])
        
        # ì „ëµë³„ í†µê³„
        unified_ai_count = sum(1 for strategy in execution_strategies.values() 
                              if strategy == ExecutionStrategy.UNIFIED_AI.value)
        model_loader_count = sum(1 for strategy in execution_strategies.values() 
                               if strategy == ExecutionStrategy.MODEL_LOADER.value)
        fallback_count = sum(1 for strategy in execution_strategies.values() 
                           if strategy == ExecutionStrategy.BASIC_FALLBACK.value)
        
        return {
            'total_steps': total_steps,
            'real_ai_count': real_ai_count,
            'ai_usage_rate': (real_ai_count / total_steps * 100) if total_steps > 0 else 0,
            'unified_ai_count': unified_ai_count,
            'model_loader_count': model_loader_count,
            'fallback_count': fallback_count,
            'unified_ai_rate': (unified_ai_count / total_steps * 100) if total_steps > 0 else 0,
            'model_loader_rate': (model_loader_count / total_steps * 100) if total_steps > 0 else 0,
            'fallback_rate': (fallback_count / total_steps * 100) if total_steps > 0 else 0,
            'unique_ai_models': list(set(ai_models_used.values()) - {'error', 'unknown', 'fallback_processing', 'step_processing'})
        }
    
    def _update_complete_performance_metrics(self, processing_time: float, quality_score: float, 
                                           success: bool, ai_stats: Dict[str, Any], 
                                           di_stats: Dict[str, Any], adapter_stats: Dict[str, Any]):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (ì™„ì „ DI + ì–´ëŒ‘í„° ì •ë³´ í¬í•¨)"""
        self.performance_metrics['total_sessions'] += 1
        
        if success:
            self.performance_metrics['successful_sessions'] += 1
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        total_sessions = self.performance_metrics['total_sessions']
        prev_avg_time = self.performance_metrics['average_processing_time']
        self.performance_metrics['average_processing_time'] = (
            (prev_avg_time * (total_sessions - 1) + processing_time) / total_sessions
        )
        
        # í‰ê·  í’ˆì§ˆ ì ìˆ˜ ì—…ë°ì´íŠ¸
        if success:
            successful_sessions = self.performance_metrics['successful_sessions']
            prev_avg_quality = self.performance_metrics['average_quality_score']
            self.performance_metrics['average_quality_score'] = (
                (prev_avg_quality * (successful_sessions - 1) + quality_score) / successful_sessions
            )
        
        # AI ëª¨ë¸ ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸
        if ai_stats:
            for model in ai_stats.get('unique_ai_models', []):
                self.performance_metrics['ai_model_usage'][model] = (
                    self.performance_metrics['ai_model_usage'].get(model, 0) + 1
                )
        
        # DI í†µê³„ ì—…ë°ì´íŠ¸
        if di_stats:
            self.performance_metrics['di_injection_count'] += di_stats.get('total_injections', 0)
            if total_sessions > 0:
                self.performance_metrics['di_success_rate'] = (
                    self.performance_metrics['di_injection_count'] / (total_sessions * len(self.step_order)) * 100
                )
        
        # ì–´ëŒ‘í„° íŒ¨í„´ í†µê³„ ì—…ë°ì´íŠ¸
        if adapter_stats:
            self.performance_metrics['adapter_pattern_usage'] += adapter_stats.get('total_adapters_used', 0)
    
    def _get_memory_peak_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ í”¼í¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            memory_info = {}
            
            # CPU ë©”ëª¨ë¦¬
            if PSUTIL_AVAILABLE:
                process = psutil.Process()
                memory_info['cpu_memory_gb'] = process.memory_info().rss / (1024**3)
                
                system_memory = psutil.virtual_memory()
                memory_info['system_memory_percent'] = system_memory.percent
                memory_info['system_memory_available_gb'] = system_memory.available / (1024**3)
            
            # GPU ë©”ëª¨ë¦¬
            if self.device == 'cuda' and torch.cuda.is_available():
                memory_info['gpu_allocated_gb'] = torch.cuda.memory_allocated() / (1024**3)
                memory_info['gpu_reserved_gb'] = torch.cuda.memory_reserved() / (1024**3)
            elif self.device == 'mps':
                memory_info['gpu_type'] = 'mps'
            
            return memory_info
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _get_step_performance_metrics(self, step_results: Dict[str, Any]) -> Dict[str, Any]:
        """Stepë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
        metrics = {}
        
        for step_name, result in step_results.items():
            if isinstance(result, dict):
                metrics[step_name] = {
                    'success': result.get('success', False),
                    'execution_time': result.get('execution_time', 0.0),
                    'confidence': result.get('confidence', 0.0),
                    'quality_score': result.get('quality_score', 0.0),
                    'ai_model_used': result.get('ai_model_name', 'unknown')
                }
        
        return metrics
    
    def _get_quality_grade(self, quality_score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if quality_score >= 0.95:
            return "Excellent+"
        elif quality_score >= 0.9:
            return "Excellent"
        elif quality_score >= 0.8:
            return "Good"
        elif quality_score >= 0.7:
            return "Fair"
        elif quality_score >= 0.6:
            return "Poor"
        else:
            return "Very Poor"
    
    # ==============================================
    # ğŸ”¥ ìƒíƒœ ì¡°íšŒ ë° ê´€ë¦¬ ë©”ì„œë“œë“¤ (ì™„ì „ DI + ì–´ëŒ‘í„° ì •ë³´ í¬í•¨)
    # ==============================================
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ - ì™„ì „ DI + ì–´ëŒ‘í„° ì •ë³´ í¬í•¨"""
        return {
            'initialized': self.is_initialized,
            'current_status': self.current_status.value,
            'device': self.device,
            'device_type': self.config.device_type,
            'memory_gb': self.config.memory_gb,
            'is_m3_max': self.config.is_m3_max,
            'ai_model_enabled': self.config.ai_model_enabled,
            'use_dependency_injection': self.config.use_dependency_injection,
            'enable_adapter_pattern': self.config.enable_adapter_pattern,
            'architecture_version': 'v9.1_complete_di_integration',
            'base_step_mixin_pattern': True,
            'model_loader_initialized': self.model_manager.is_initialized,
            'di_container_available': self.di_container is not None,
            'config': {
                'quality_level': self.config.quality_level.value,
                'processing_mode': self.config.processing_mode.value,
                'performance_mode': self.config.performance_mode,
                'ai_model_enabled': self.config.ai_model_enabled,
                'model_preload_enabled': self.config.model_preload_enabled,
                'use_dependency_injection': self.config.use_dependency_injection,
                'auto_inject_dependencies': self.config.auto_inject_dependencies,
                'lazy_loading_enabled': self.config.lazy_loading_enabled,
                'interface_based_design': self.config.interface_based_design,
                'enable_adapter_pattern': self.config.enable_adapter_pattern,
                'enable_runtime_injection': self.config.enable_runtime_injection,
                'model_cache_size': self.config.model_cache_size,
                'max_fallback_attempts': self.config.max_fallback_attempts,
                'memory_optimization': self.config.memory_optimization,
                'parallel_processing': self.config.parallel_processing,
                'batch_size': self.config.batch_size,
                'thread_pool_size': self.config.thread_pool_size
            },
            'steps_status': {
                step_name: {
                    'loaded': step_name in self.steps,
                    'type': type(self.steps[step_name]).__name__ if step_name in self.steps else None,
                    'ready': step_name in self.steps and hasattr(self.steps[step_name], 'process'),
                    'has_model_loader': (step_name in self.steps and 
                                        hasattr(self.steps[step_name], 'model_loader') and 
                                        getattr(self.steps[step_name], 'model_loader', None) is not None),
                    'has_memory_manager': (step_name in self.steps and 
                                         hasattr(self.steps[step_name], 'memory_manager') and 
                                         getattr(self.steps[step_name], 'memory_manager', None) is not None),
                    'has_data_converter': (step_name in self.steps and 
                                         hasattr(self.steps[step_name], 'data_converter') and 
                                         getattr(self.steps[step_name], 'data_converter', None) is not None),
                    'has_model_interface': (step_name in self.steps and 
                                          hasattr(self.steps[step_name], 'model_interface') and 
                                          getattr(self.steps[step_name], 'model_interface', None) is not None),
                    'di_injected': (step_name in self.steps and 
                                   hasattr(self.steps[step_name], 'model_loader') and 
                                   getattr(self.steps[step_name], 'model_loader', None) is not None),
                    'adapters_used': self._count_step_adapters(step_name)
                }
                for step_name in self.step_order
            },
            'dependency_injection_status': {
                'di_container_initialized': self.di_container is not None,
                'di_injection_count': self.performance_metrics.get('di_injection_count', 0),
                'di_success_rate': self.performance_metrics.get('di_success_rate', 0.0),
                'adapter_pattern_usage': self.performance_metrics.get('adapter_pattern_usage', 0),
                'circular_import_resolved': True,
                'base_step_mixin_pattern_applied': True
            },
            'performance_metrics': self.performance_metrics,
            'memory_usage': self._get_memory_peak_usage(),
            'system_integration': {
                'di_container_available': DI_CONTAINER_AVAILABLE,
                'adapter_pattern_enabled': self.config.enable_adapter_pattern,
                'base_step_mixin_integration': True
            }
        }
    
    def _count_step_adapters(self, step_name: str) -> int:
        """Stepì˜ ì–´ëŒ‘í„° ì‚¬ìš© ê°œìˆ˜ ê³„ì‚°"""
        try:
            if step_name not in self.steps:
                return 0
            
            step = self.steps[step_name]
            adapter_count = 0
            
            # ModelLoaderAdapter í™•ì¸
            if hasattr(step, 'model_loader'):
                if isinstance(step.model_loader, ModelLoaderAdapter) or hasattr(step.model_loader, 'adapter_info'):
                    adapter_count += 1
            
            # MemoryManagerAdapter í™•ì¸
            if hasattr(step, 'memory_manager'):
                if isinstance(step.memory_manager, MemoryManagerAdapter) or hasattr(step.memory_manager, 'adapter_info'):
                    adapter_count += 1
            
            # DataConverterAdapter í™•ì¸
            if hasattr(step, 'data_converter'):
                if isinstance(step.data_converter, DataConverterAdapter) or hasattr(step.data_converter, 'adapter_info'):
                    adapter_count += 1
            
            return adapter_count
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ {step_name} ì–´ëŒ‘í„° ì¹´ìš´íŠ¸ ì‹¤íŒ¨: {e}")
            return 0
    
    # backend/app/ai_pipeline/pipeline_manager.pyì— ì¶”ê°€í•  ì½”ë“œ

# PipelineManager í´ë˜ìŠ¤ ë‚´ë¶€ì— ë‹¤ìŒ ë©”ì„œë“œë“¤ì„ ì¶”ê°€:

def register_step(self, step_id: int, step_instance: Any) -> bool:
    """
    Step ì¸ìŠ¤í„´ìŠ¤ë¥¼ íŒŒì´í”„ë¼ì¸ì— ë“±ë¡
    
    Args:
        step_id: Step ID (1-8)
        step_instance: Step ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        bool: ë“±ë¡ ì„±ê³µ ì—¬ë¶€
    """
    try:
        # Step IDë¥¼ step_nameìœ¼ë¡œ ë³€í™˜
        step_name_mapping = {
            1: 'human_parsing',
            2: 'pose_estimation', 
            3: 'cloth_segmentation',
            4: 'geometric_matching',
            5: 'cloth_warping',
            6: 'virtual_fitting',
            7: 'post_processing',
            8: 'quality_assessment'
        }
        
        step_name = step_name_mapping.get(step_id)
        if not step_name:
            self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” Step ID: {step_id}")
            return False
        
        # Step ë“±ë¡
        self.steps[step_name] = step_instance
        
        # DI ì˜ì¡´ì„± ì£¼ì… (ìˆëŠ” ê²½ìš°)
        if self.config.use_dependency_injection and self.di_container:
            try:
                # ModelLoader ì–´ëŒ‘í„° ì£¼ì…
                if hasattr(step_instance, 'model_loader') and not step_instance.model_loader:
                    model_loader = self.di_container.get('IModelLoader') or ModelLoaderAdapter()
                    step_instance.model_loader = model_loader
                
                # MemoryManager ì–´ëŒ‘í„° ì£¼ì…
                if hasattr(step_instance, 'memory_manager') and not step_instance.memory_manager:
                    memory_manager = self.di_container.get('IMemoryManager') or MemoryManagerAdapter()
                    step_instance.memory_manager = memory_manager
                
                # DataConverter ì–´ëŒ‘í„° ì£¼ì…
                if hasattr(step_instance, 'data_converter') and not step_instance.data_converter:
                    data_converter = self.di_container.get('IDataConverter') or DataConverterAdapter(self.device)
                    step_instance.data_converter = data_converter
                
                self.logger.debug(f"âœ… Step {step_id} ({step_name}) DI ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ Step {step_id} DI ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        self.logger.info(f"âœ… Step {step_id} ({step_name}) ë“±ë¡ ì™„ë£Œ")
        return True
        
    except Exception as e:
        self.logger.error(f"âŒ Step {step_id} ë“±ë¡ ì‹¤íŒ¨: {e}")
        return False

def register_steps_batch(self, steps_dict: Dict[int, Any]) -> Dict[int, bool]:
    """
    ì—¬ëŸ¬ Stepì„ ì¼ê´„ ë“±ë¡
    
    Args:
        steps_dict: {step_id: step_instance} ë”•ì…”ë„ˆë¦¬
        
    Returns:
        Dict[int, bool]: ê° Stepì˜ ë“±ë¡ ê²°ê³¼
    """
    results = {}
    
    try:
        self.logger.info(f"ğŸ”„ {len(steps_dict)}ê°œ Step ì¼ê´„ ë“±ë¡ ì‹œì‘...")
        
        for step_id, step_instance in steps_dict.items():
            results[step_id] = self.register_step(step_id, step_instance)
        
        success_count = sum(1 for success in results.values() if success)
        self.logger.info(f"âœ… Step ì¼ê´„ ë“±ë¡ ì™„ë£Œ: {success_count}/{len(steps_dict)}")
        
        return results
        
    except Exception as e:
        self.logger.error(f"âŒ Step ì¼ê´„ ë“±ë¡ ì‹¤íŒ¨: {e}")
        return {step_id: False for step_id in steps_dict.keys()}

def unregister_step(self, step_id: int) -> bool:
    """
    Step ë“±ë¡ í•´ì œ
    
    Args:
        step_id: Step ID (1-8)
        
    Returns:
        bool: í•´ì œ ì„±ê³µ ì—¬ë¶€
    """
    try:
        # Step IDë¥¼ step_nameìœ¼ë¡œ ë³€í™˜
        step_name_mapping = {
            1: 'human_parsing',
            2: 'pose_estimation',
            3: 'cloth_segmentation', 
            4: 'geometric_matching',
            5: 'cloth_warping',
            6: 'virtual_fitting',
            7: 'post_processing',
            8: 'quality_assessment'
        }
        
        step_name = step_name_mapping.get(step_id)
        if not step_name:
            self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” Step ID: {step_id}")
            return False
        
        if step_name in self.steps:
            # Step ì •ë¦¬
            step_instance = self.steps[step_name]
            if hasattr(step_instance, 'cleanup'):
                try:
                    if asyncio.iscoroutinefunction(step_instance.cleanup):
                        asyncio.create_task(step_instance.cleanup())
                    else:
                        step_instance.cleanup()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Step {step_id} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ë“±ë¡ í•´ì œ
            del self.steps[step_name]
            self.logger.info(f"âœ… Step {step_id} ({step_name}) ë“±ë¡ í•´ì œ ì™„ë£Œ")
            return True
        else:
            self.logger.warning(f"âš ï¸ Step {step_id} ({step_name})ê°€ ë“±ë¡ë˜ì–´ ìˆì§€ ì•ŠìŒ")
            return False
            
    except Exception as e:
        self.logger.error(f"âŒ Step {step_id} ë“±ë¡ í•´ì œ ì‹¤íŒ¨: {e}")
        return False

def get_registered_steps(self) -> Dict[str, Any]:
    """
    ë“±ë¡ëœ Step ëª©ë¡ ë°˜í™˜
    
    Returns:
        Dict[str, Any]: ë“±ë¡ëœ Step ì •ë³´
    """
    try:
        registered_info = {}
        
        # Step nameì„ IDë¡œ ë³€í™˜í•˜ëŠ” ë§¤í•‘
        name_to_id_mapping = {
            'human_parsing': 1,
            'pose_estimation': 2,
            'cloth_segmentation': 3,
            'geometric_matching': 4,
            'cloth_warping': 5,
            'virtual_fitting': 6,
            'post_processing': 7,
            'quality_assessment': 8
        }
        
        for step_name, step_instance in self.steps.items():
            step_id = name_to_id_mapping.get(step_name, 0)
            
            step_info = {
                'step_id': step_id,
                'step_name': step_name,
                'class_name': type(step_instance).__name__,
                'registered': True,
                'has_process_method': hasattr(step_instance, 'process'),
                'has_model_loader': hasattr(step_instance, 'model_loader') and step_instance.model_loader is not None,
                'has_memory_manager': hasattr(step_instance, 'memory_manager') and step_instance.memory_manager is not None,
                'has_data_converter': hasattr(step_instance, 'data_converter') and step_instance.data_converter is not None,
                'di_injected': (hasattr(step_instance, 'model_loader') and step_instance.model_loader is not None) or
                             (hasattr(step_instance, 'memory_manager') and step_instance.memory_manager is not None),
                'adapters_used': self._count_step_adapters(step_name)
            }
            
            registered_info[step_name] = step_info
        
        return {
            'total_registered': len(self.steps),
            'registered_steps': registered_info,
            'missing_steps': [name for name in name_to_id_mapping.keys() if name not in self.steps],
            'registration_rate': len(self.steps) / len(name_to_id_mapping) * 100
        }
        
    except Exception as e:
        self.logger.error(f"âŒ ë“±ë¡ëœ Step ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {'error': str(e)}

def is_step_registered(self, step_id: int) -> bool:
    """
    Step ë“±ë¡ ì—¬ë¶€ í™•ì¸
    
    Args:
        step_id: Step ID (1-8)
        
    Returns:
        bool: ë“±ë¡ ì—¬ë¶€
    """
    step_name_mapping = {
        1: 'human_parsing',
        2: 'pose_estimation',
        3: 'cloth_segmentation',
        4: 'geometric_matching', 
        5: 'cloth_warping',
        6: 'virtual_fitting',
        7: 'post_processing',
        8: 'quality_assessment'
    }
    
    step_name = step_name_mapping.get(step_id)
    return step_name in self.steps if step_name else False

def get_step_by_id(self, step_id: int) -> Optional[Any]:
    """
    Step IDë¡œ Step ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    
    Args:
        step_id: Step ID (1-8)
        
    Returns:
        Optional[Any]: Step ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
    """
    step_name_mapping = {
        1: 'human_parsing',
        2: 'pose_estimation',
        3: 'cloth_segmentation',
        4: 'geometric_matching',
        5: 'cloth_warping', 
        6: 'virtual_fitting',
        7: 'post_processing',
        8: 'quality_assessment'
    }
    
    step_name = step_name_mapping.get(step_id)
    return self.steps.get(step_name) if step_name else None

def update_config(self, new_config: Dict[str, Any]) -> bool:
    """
    íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸
    
    Args:
        new_config: ìƒˆë¡œìš´ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        bool: ì—…ë°ì´íŠ¸ ì„±ê³µ ì—¬ë¶€
    """
    try:
        self.logger.info("ğŸ”„ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        # ê¸°ë³¸ ì„¤ì • ì—…ë°ì´íŠ¸
        if 'device' in new_config and new_config['device'] != self.device:
            self.device = new_config['device']
            self.data_converter = DataConverterAdapter(self.device)
            self.logger.info(f"âœ… ë””ë°”ì´ìŠ¤ ë³€ê²½: {self.device}")
        
        # PipelineConfig ì—…ë°ì´íŠ¸
        if isinstance(self.config, dict):
            self.config.update(new_config)
        else:
            # PipelineConfig ê°ì²´ì¸ ê²½ìš° ì†ì„± ì—…ë°ì´íŠ¸
            for key, value in new_config.items():
                if hasattr(self.config, key):
                    setattr(self.config, key, value)
        
        # Stepë³„ ì„¤ì • ì—…ë°ì´íŠ¸
        if 'steps' in new_config:
            steps_config = new_config['steps']
            for step_config in steps_config:
                if 'step_name' in step_config:
                    step_name = step_config['step_name']
                    if step_name in self.steps:
                        step_instance = self.steps[step_name]
                        # Step ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • ì—…ë°ì´íŠ¸
                        for config_key, config_value in step_config.items():
                            if hasattr(step_instance, config_key):
                                setattr(step_instance, config_key, config_value)
        
        self.logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def configure_from_detection(self, detection_config: Dict[str, Any]) -> bool:
    """
    Step íƒì§€ ê²°ê³¼ë¡œë¶€í„° íŒŒì´í”„ë¼ì¸ ì„¤ì •
    
    Args:
        detection_config: Step íƒì§€ ê²°ê³¼ ì„¤ì •
        
    Returns:
        bool: ì„¤ì • ì„±ê³µ ì—¬ë¶€
    """
    try:
        self.logger.info("ğŸ¯ Step íƒì§€ ê²°ê³¼ë¡œë¶€í„° íŒŒì´í”„ë¼ì¸ ì„¤ì • ì‹œì‘...")
        
        # íƒì§€ëœ Step ì •ë³´ ì¶”ì¶œ
        if 'steps' in detection_config:
            for step_config in detection_config['steps']:
                step_name = step_config.get('step_name')
                step_class = step_config.get('step_class')
                checkpoint_path = step_config.get('checkpoint_path')
                
                if step_name and step_class:
                    # Step í´ë˜ìŠ¤ ë™ì  ë¡œë”© ì‹œë„
                    try:
                        # Step í´ë˜ìŠ¤ê°€ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        if hasattr(self, 'step_classes') and step_class in self.step_classes:
                            StepClass = self.step_classes[step_class]
                            
                            # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                            step_instance = StepClass(
                                device=self.device,
                                checkpoint_path=checkpoint_path,
                                **step_config
                            )
                            
                            # DI ì˜ì¡´ì„± ì£¼ì…
                            if self.config.use_dependency_injection:
                                step_instance.model_loader = ModelLoaderAdapter()
                                step_instance.memory_manager = MemoryManagerAdapter()
                                step_instance.data_converter = DataConverterAdapter(self.device)
                            
                            # Step ë“±ë¡
                            self.steps[step_name] = step_instance
                            self.logger.info(f"âœ… {step_name} íƒì§€ ê²°ê³¼ë¡œë¶€í„° ì„¤ì • ì™„ë£Œ")
                            
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {step_name} íƒì§€ ê²°ê³¼ ì„¤ì • ì‹¤íŒ¨: {e}")
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        if 'pipeline_metadata' in detection_config:
            metadata = detection_config['pipeline_metadata']
            if hasattr(self, 'performance_metrics'):
                self.performance_metrics.update({
                    'detection_based_configuration': True,
                    'detected_steps_count': metadata.get('total_steps', 0),
                    'available_steps_count': metadata.get('available_steps', 0),
                    'configuration_time': time.time()
                })
        
        self.logger.info("âœ… Step íƒì§€ ê²°ê³¼ë¡œë¶€í„° íŒŒì´í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ")
        return True
        
    except Exception as e:
        self.logger.error(f"âŒ Step íƒì§€ ê²°ê³¼ ì„¤ì • ì‹¤íŒ¨: {e}")
        return False

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - ì™„ì „ DI + ì–´ëŒ‘í„° í¬í•¨"""
        try:
            self.logger.info("ğŸ§¹ ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            self.current_status = ProcessingStatus.CLEANING
            
            # 1. ê° Step ì •ë¦¬ (DI + ì–´ëŒ‘í„° í¬í•¨)
            for step_name, step in self.steps.items():
                try:
                    # DI ì£¼ì…ëœ ì»´í¬ë„ŒíŠ¸ë“¤ ì •ë¦¬
                    if hasattr(step, 'model_loader') and step.model_loader:
                        if hasattr(step.model_loader, 'cleanup'):
                            step.model_loader.cleanup()
                    
                    if hasattr(step, 'memory_manager') and step.memory_manager:
                        if hasattr(step.memory_manager, 'cleanup'):
                            step.memory_manager.cleanup()
                    
                    if hasattr(step, 'data_converter') and step.data_converter:
                        if hasattr(step.data_converter, 'cleanup'):
                            step.data_converter.cleanup()
                    
                    # Step ìì²´ ì •ë¦¬
                    if hasattr(step, 'cleanup'):
                        if asyncio.iscoroutinefunction(step.cleanup):
                            await step.cleanup()
                        else:
                            step.cleanup()
                        
                    self.logger.info(f"âœ… {step_name} ì •ë¦¬ ì™„ë£Œ (ì™„ì „ DI + ì–´ëŒ‘í„°)")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 2. DI ê¸°ë°˜ ê´€ë¦¬ìë“¤ ì •ë¦¬
            if hasattr(self.model_manager, 'cleanup'):
                await self.model_manager.cleanup()
            
            # 3. ì–´ëŒ‘í„°ë“¤ ì •ë¦¬
            if hasattr(self.memory_manager, 'cleanup'):
                self.memory_manager.cleanup()
            
            # 4. DI Container ì •ë¦¬
            if self.di_container:
                try:
                    if hasattr(self.di_container, 'clear'):
                        self.di_container.clear()
                    self.di_container = None
                    self.logger.info("âœ… DI Container ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ DI Container ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 5. ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                await self.memory_manager.optimize_memory_async(aggressive=True)
                self.logger.info("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 6. ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            if hasattr(self, 'thread_pool'):
                try:
                    self.thread_pool.shutdown(wait=True)
                    self.logger.info("âœ… ìŠ¤ë ˆë“œ í’€ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œ í’€ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 7. ìƒíƒœ ì´ˆê¸°í™”
            self.is_initialized = False
            self.current_status = ProcessingStatus.IDLE
            
            self.logger.info("âœ… ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.current_status = ProcessingStatus.FAILED

# ==============================================
# ğŸ”¥ 7. DIBasedPipelineManager í´ë˜ìŠ¤ ì™„ì „ êµ¬í˜„
# ==============================================

class DIBasedPipelineManager(PipelineManager):
    """
    ğŸ”¥ DIBasedPipelineManager - PipelineManagerì˜ DI íŠ¹í™” ë²„ì „
    
    âœ… PipelineManagerë¥¼ ìƒì†í•˜ì—¬ ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€
    âœ… DI íŠ¹í™” ê¸°ëŠ¥ ì¶”ê°€ ë° ê°•í™”
    âœ… ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ 100% í˜¸í™˜
    âœ… cannot import name 'DIBasedPipelineManager' ì™„ì „ í•´ê²°
    """
    
    def __init__(
        self,
        config_path: Optional[str] = None,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], PipelineConfig]] = None,
        **kwargs
    ):
        """DIBasedPipelineManager ì´ˆê¸°í™” - DI ê¸°ëŠ¥ ê°•ì œ í™œì„±í™”"""
        
        # DI ê´€ë ¨ ì„¤ì • ê°•ì œ í™œì„±í™”
        if isinstance(config, dict):
            config.update({
                'use_dependency_injection': True,
                'auto_inject_dependencies': True,
                'enable_adapter_pattern': True,
                'enable_runtime_injection': True,
                'interface_based_design': True,
                'lazy_loading_enabled': True
            })
        elif isinstance(config, PipelineConfig):
            config.use_dependency_injection = True
            config.auto_inject_dependencies = True
            config.enable_adapter_pattern = True
            config.enable_runtime_injection = True
            config.interface_based_design = True
            config.lazy_loading_enabled = True
        else:
            kwargs.update({
                'use_dependency_injection': True,
                'auto_inject_dependencies': True,
                'enable_adapter_pattern': True,
                'enable_runtime_injection': True,
                'interface_based_design': True,
                'lazy_loading_enabled': True
            })
        
        # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        super().__init__(config_path=config_path, device=device, config=config, **kwargs)
        
        # DIBasedPipelineManager ì „ìš© ë¡œê¹…
        self.logger.info("ğŸ”¥ DIBasedPipelineManager v9.1 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info("ğŸ’‰ ì™„ì „ DI ê¸°ëŠ¥ ê°•ì œ í™œì„±í™”")
        self.logger.info(f"ğŸ”§ DI Container: {'âœ…' if self.di_container else 'âŒ'}")
        self.logger.info(f"ğŸ”§ ì–´ëŒ‘í„° íŒ¨í„´: âœ…")
        self.logger.info(f"ğŸ“ base_step_mixin.py íŒ¨í„´: âœ…")
    
    def get_di_status(self) -> Dict[str, Any]:
        """DI ì „ìš© ìƒíƒœ ì¡°íšŒ"""
        base_status = self.get_pipeline_status()
        
        # DI íŠ¹í™” ì •ë³´ ì¶”ê°€
        di_status = {
            **base_status,
            'di_based_manager': True,
            'di_forced_enabled': True,
            'di_specific_info': {
                'di_container_type': type(self.di_container).__name__ if self.di_container else 'None',
                'model_manager_type': type(self.model_manager).__name__,
                'execution_manager_type': type(self.execution_manager).__name__,
                'memory_manager_adapter': isinstance(self.memory_manager, MemoryManagerAdapter),
                'data_converter_adapter': isinstance(self.data_converter, DataConverterAdapter),
                'total_adapters_active': sum([
                    isinstance(self.memory_manager, MemoryManagerAdapter),
                    isinstance(self.data_converter, DataConverterAdapter),
                    1  # model_loader_adapterëŠ” í•­ìƒ í™œì„±
                ])
            }
        }
        
        return di_status
    
    async def initialize_with_enhanced_di(self) -> bool:
        """ê°•í™”ëœ DI ì´ˆê¸°í™”"""
        try:
            self.logger.info("ğŸš€ DIBasedPipelineManager ê°•í™”ëœ DI ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. ê¸°ë³¸ ì´ˆê¸°í™”
            basic_success = await self.initialize()
            
            # 2. DI ê°•í™” ì´ˆê¸°í™”
            if basic_success and self.di_container:
                try:
                    # ì¶”ê°€ DI ë“±ë¡
                    self.di_container.register_instance('DIBasedPipelineManager', self)
                    self.di_container.register_instance('PipelineManager', self)
                    
                    # Stepë³„ DI ì¬ì£¼ì…
                    for step_name, step in self.steps.items():
                        if hasattr(step, '__dict__'):
                            step.__dict__['di_based_manager'] = self
                    
                    self.logger.info("âœ… DIBasedPipelineManager ê°•í™”ëœ DI ì´ˆê¸°í™” ì™„ë£Œ")
                    return True
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ê°•í™”ëœ DI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    return basic_success
            
            return basic_success
            
        except Exception as e:
            self.logger.error(f"âŒ DIBasedPipelineManager ê°•í™”ëœ DI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

# ==============================================
# ğŸ”¥ 8. í¸ì˜ í•¨ìˆ˜ë“¤ (ì™„ì „ DI í†µí•© ë²„ì „)
# ==============================================

def create_pipeline(
    device: str = "auto", 
    quality_level: str = "balanced", 
    mode: str = "production",
    use_dependency_injection: bool = True,
    enable_adapter_pattern: bool = True,
    **kwargs
) -> PipelineManager:
    """
    ğŸ”¥ ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ - ì™„ì „ DI í†µí•© ë²„ì „
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ì„¤ì • ('auto', 'cpu', 'cuda', 'mps')
        quality_level: í’ˆì§ˆ ë ˆë²¨ ('fast', 'balanced', 'high', 'maximum')
        mode: ëª¨ë“œ ('development', 'production', 'testing', 'optimization')
        use_dependency_injection: ì˜ì¡´ì„± ì£¼ì… ì‚¬ìš© ì—¬ë¶€
        enable_adapter_pattern: ì–´ëŒ‘í„° íŒ¨í„´ ì‚¬ìš© ì—¬ë¶€
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: ì´ˆê¸°í™”ëœ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return PipelineManager(
        device=device,
        config=PipelineConfig(
            quality_level=QualityLevel(quality_level),
            processing_mode=PipelineMode(mode),
            ai_model_enabled=True,
            use_dependency_injection=use_dependency_injection,
            enable_adapter_pattern=enable_adapter_pattern,
            **kwargs
        )
    )

def create_complete_di_pipeline(
    device: str = "auto",
    quality_level: str = "high",
    **kwargs
) -> PipelineManager:
    """ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    return PipelineManager(
        device=device,
        config=PipelineConfig(
            quality_level=QualityLevel(quality_level),
            processing_mode=PipelineMode.PRODUCTION,
            ai_model_enabled=True,
            model_preload_enabled=True,
            model_cache_size=20,
            performance_mode="maximum",
            memory_optimization=True,
            parallel_processing=True,
            max_fallback_attempts=2,
            use_dependency_injection=True,
            auto_inject_dependencies=True,
            lazy_loading_enabled=True,
            interface_based_design=True,
            enable_adapter_pattern=True,
            enable_runtime_injection=True,
            **kwargs
        )
    )

def create_m3_max_pipeline(**kwargs) -> PipelineManager:
    """
    ğŸ”¥ M3 Max + ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ ìµœì í™” íŒŒì´í”„ë¼ì¸
    
    Args:
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: M3 Max ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return PipelineManager(
        device="mps",
        config=PipelineConfig(
            quality_level=QualityLevel.MAXIMUM,
            processing_mode=PipelineMode.PRODUCTION,
            memory_gb=128.0,
            is_m3_max=True,
            device_type="apple_silicon",
            ai_model_enabled=True,
            model_preload_enabled=True,
            model_cache_size=20,
            performance_mode="maximum",
            memory_optimization=True,
            gpu_memory_fraction=0.95,
            use_fp16=True,
            enable_quantization=True,
            parallel_processing=True,
            batch_processing=True,
            async_processing=True,
            batch_size=4,
            thread_pool_size=8,
            max_fallback_attempts=2,
            enable_smart_fallback=True,
            # ğŸ”¥ ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ ì„¤ì •
            use_dependency_injection=True,
            auto_inject_dependencies=True,
            lazy_loading_enabled=True,
            interface_based_design=True,
            enable_adapter_pattern=True,
            enable_runtime_injection=True,
            **kwargs
        )
    )

def create_production_pipeline(**kwargs) -> PipelineManager:
    """
    ğŸ”¥ í”„ë¡œë•ì…˜ìš© ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ íŒŒì´í”„ë¼ì¸
    
    Args:
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: í”„ë¡œë•ì…˜ ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return create_complete_di_pipeline(
        quality_level="high",
        processing_mode="production",
        ai_model_enabled=True,
        model_preload_enabled=True,
        memory_optimization=True,
        parallel_processing=True,
        **kwargs
    )

def create_development_pipeline(**kwargs) -> PipelineManager:
    """
    ğŸ”¥ ê°œë°œìš© ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ íŒŒì´í”„ë¼ì¸
    
    Args:
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: ê°œë°œìš© íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return create_complete_di_pipeline(
        quality_level="balanced",
        processing_mode="development",
        ai_model_enabled=True,
        model_preload_enabled=False,
        memory_optimization=False,
        parallel_processing=False,
        **kwargs
    )

def create_testing_pipeline(**kwargs) -> PipelineManager:
    """
    ğŸ”¥ í…ŒìŠ¤íŒ…ìš© íŒŒì´í”„ë¼ì¸ - ê¸°ë³¸ DI + ì–´ëŒ‘í„° ì§€ì›
    
    Args:
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: í…ŒìŠ¤íŒ…ìš© íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return PipelineManager(
        device="cpu",
        config=PipelineConfig(
            quality_level=QualityLevel.FAST,
            processing_mode=PipelineMode.TESTING,
            ai_model_enabled=False,
            model_preload_enabled=False,
            memory_optimization=False,
            parallel_processing=False,
            batch_size=1,
            thread_pool_size=2,
            use_dependency_injection=True,
            auto_inject_dependencies=False,
            enable_adapter_pattern=True,
            **kwargs
        )
    )

def create_di_based_pipeline(**kwargs) -> DIBasedPipelineManager:
    """
    ğŸ”¥ DIBasedPipelineManager ì „ìš© ìƒì„± í•¨ìˆ˜
    
    Args:
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        DIBasedPipelineManager: DI ì „ìš© íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return DIBasedPipelineManager(**kwargs)

@lru_cache(maxsize=1)
def get_global_pipeline_manager(device: str = "auto") -> PipelineManager:
    """
    ğŸ”¥ ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ - ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ ë²„ì „
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
    
    Returns:
        PipelineManager: ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
    """
    try:
        if device == "mps" and torch.backends.mps.is_available():
            return create_m3_max_pipeline()
        else:
            return create_production_pipeline(device=device)
    except Exception as e:
        logger.error(f"ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ìƒì„± ì‹¤íŒ¨: {e}")
        return create_complete_di_pipeline(device="cpu", quality_level="balanced")

@lru_cache(maxsize=1)
def get_global_di_based_pipeline_manager(device: str = "auto") -> DIBasedPipelineManager:
    """
    ğŸ”¥ ì „ì—­ DIBasedPipelineManager ì¸ìŠ¤í„´ìŠ¤
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
    
    Returns:
        DIBasedPipelineManager: ì „ì—­ DI ì „ìš© íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
    """
    try:
        if device == "mps" and torch.backends.mps.is_available():
            return DIBasedPipelineManager(
                device="mps",
                config=PipelineConfig(
                    quality_level=QualityLevel.MAXIMUM,
                    processing_mode=PipelineMode.PRODUCTION,
                    memory_gb=128.0,
                    is_m3_max=True,
                    device_type="apple_silicon",
                    performance_mode="maximum"
                )
            )
        else:
            return DIBasedPipelineManager(device=device)
    except Exception as e:
        logger.error(f"ì „ì—­ DIBasedPipelineManager ìƒì„± ì‹¤íŒ¨: {e}")
        return DIBasedPipelineManager(device="cpu")

# ==============================================
# ğŸ”¥ 9. Export ë° ë©”ì¸ ì‹¤í–‰
# ==============================================

__all__ = [
    # ì—´ê±°í˜•
    'PipelineMode', 'QualityLevel', 'ProcessingStatus', 'ExecutionStrategy',
    
    # ë°ì´í„° í´ë˜ìŠ¤
    'PipelineConfig', 'ProcessingResult',
    
    # ğŸ”¥ ë©”ì¸ í´ë˜ìŠ¤ë“¤ (ìˆœí™˜ì°¸ì¡° í•´ê²°)
    'PipelineManager',                    # âœ… ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    'DIBasedPipelineManager',            # âœ… DI ì „ìš© íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € (ì™„ì „ êµ¬í˜„)
    
    # ì–´ëŒ‘í„° í´ë˜ìŠ¤ë“¤
    'ModelLoaderAdapter', 'MemoryManagerAdapter', 'DataConverterAdapter',
    
    # DI ê¸°ë°˜ ê´€ë¦¬ì í´ë˜ìŠ¤ë“¤
    'DIBasedModelLoaderManager', 'DIBasedExecutionManager',
    
    # ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ ë²„ì „)
    'create_pipeline',                    # âœ… ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ (ì™„ì „ DI + ì–´ëŒ‘í„°)
    'create_complete_di_pipeline',        # âœ… ì™„ì „ DI ìµœì í™” (ì–´ëŒ‘í„° íŒ¨í„´)
    'create_m3_max_pipeline',            # âœ… M3 Max ìµœì í™” (ì™„ì „ DI + ì–´ëŒ‘í„°)
    'create_production_pipeline',        # âœ… í”„ë¡œë•ì…˜ (ì™„ì „ DI + ì–´ëŒ‘í„°)
    'create_development_pipeline',       # âœ… ê°œë°œìš© (ì™„ì „ DI + ì–´ëŒ‘í„°)  
    'create_testing_pipeline',           # âœ… í…ŒìŠ¤íŒ… (ê¸°ë³¸ DI + ì–´ëŒ‘í„°)
    'create_di_based_pipeline',          # âœ… DIBasedPipelineManager ì „ìš©
    'get_global_pipeline_manager',        # âœ… ì „ì—­ ë§¤ë‹ˆì € (ì™„ì „ DI + ì–´ëŒ‘í„°)
    'get_global_di_based_pipeline_manager' # âœ… ì „ì—­ DI ì „ìš© ë§¤ë‹ˆì €
]

# ==============================================
# ğŸ”¥ 10. ì™„ë£Œ ë©”ì‹œì§€ ë° ë¡œê¹…
# ==============================================

logger.info("ğŸ‰ ì™„ì „ DI í†µí•© PipelineManager v9.1 ë¡œë“œ ì™„ë£Œ!")
logger.info("âœ… ì£¼ìš” ì™„ì„± ê¸°ëŠ¥:")
logger.info("   - base_step_mixin.pyì˜ DI íŒ¨í„´ ì™„ì „ ì ìš©")
logger.info("   - ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°")
logger.info("   - TYPE_CHECKINGìœ¼ë¡œ import ì‹œì  ìˆœí™˜ì°¸ì¡° ë°©ì§€")
logger.info("   - ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ëŠìŠ¨í•œ ê²°í•© ê°•í™”")
logger.info("   - ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… ì™„ì „ êµ¬í˜„")
logger.info("   - ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ 100% ìœ ì§€")
logger.info("   - M3 Max 128GB ìµœì í™” ìœ ì§€")
logger.info("   - í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ìµœê³  ìˆ˜ì¤€")
logger.info("   - 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ì „ ì‘ë™")
logger.info("   - conda í™˜ê²½ ì™„ë²½ ì§€ì›")
logger.info("   ğŸ”¥ DIBasedPipelineManager í´ë˜ìŠ¤ ì™„ì „ êµ¬í˜„")

logger.info("âœ… ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ create_pipeline í•¨ìˆ˜ë“¤:")
logger.info("   - create_pipeline() âœ… (ì™„ì „ DI + ì–´ëŒ‘í„°)")
logger.info("   - create_complete_di_pipeline() âœ… (ì™„ì „ DI + ì–´ëŒ‘í„°)")
logger.info("   - create_m3_max_pipeline() âœ… (M3 Max + ì™„ì „ DI + ì–´ëŒ‘í„°)") 
logger.info("   - create_production_pipeline() âœ… (í”„ë¡œë•ì…˜ + ì™„ì „ DI + ì–´ëŒ‘í„°)")
logger.info("   - create_development_pipeline() âœ… (ê°œë°œ + ì™„ì „ DI + ì–´ëŒ‘í„°)")
logger.info("   - create_testing_pipeline() âœ… (í…ŒìŠ¤íŠ¸ + ê¸°ë³¸ DI + ì–´ëŒ‘í„°)")
logger.info("   - create_di_based_pipeline() âœ… (DIBasedPipelineManager ì „ìš©)")
logger.info("   - get_global_pipeline_manager() âœ… (ì „ì—­ + ì™„ì „ DI + ì–´ëŒ‘í„°)")
logger.info("   - get_global_di_based_pipeline_manager() âœ… (ì „ì—­ DI ì „ìš©)")

logger.info("ğŸ’‰ ì™„ì „ ì˜ì¡´ì„± ì£¼ì… + ì–´ëŒ‘í„° íŒ¨í„´ ê¸°ëŠ¥:")
logger.info("   - ìˆœí™˜ ì„í¬íŠ¸ ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("   - IModelLoader, IMemoryManager, IDataConverter ì¸í„°í˜ì´ìŠ¤")
logger.info("   - ModelLoaderAdapter, MemoryManagerAdapter, DataConverterAdapter íŒ¨í„´")
logger.info("   - DI Container ê¸°ë°˜ ì „ì—­ ì˜ì¡´ì„± ê´€ë¦¬")
logger.info("   - ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… (inject_dependencies)")
logger.info("   - ì§€ì—° ë¡œë”© (resolve_lazy_dependencies)")
logger.info("   - TYPE_CHECKINGìœ¼ë¡œ import ì‹œì  ìˆœí™˜ì°¸ì¡° ë°©ì§€")
logger.info("   - base_step_mixin.py íŒ¨í„´ ì™„ì „ ì ìš©")
logger.info("   ğŸ”¥ DIBasedPipelineManager ì™„ì „ í˜¸í™˜ì„±")

logger.info("ğŸš€ ì´ì œ ìˆœí™˜ ì„í¬íŠ¸ ì—†ì´ ìµœê³  í’ˆì§ˆ AI ê°€ìƒ í”¼íŒ…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")

logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ê°€ìš©ì„±:")
logger.info(f"   - DI Container: {'âœ…' if DI_CONTAINER_AVAILABLE else 'âŒ'}")
logger.info(f"   - ì–´ëŒ‘í„° íŒ¨í„´: âœ…")
logger.info(f"   - base_step_mixin.py íŒ¨í„´: âœ…")
logger.info(f"   - PSUTIL: {'âœ…' if PSUTIL_AVAILABLE else 'âŒ'}")
logger.info(f"   ğŸ”¥ DIBasedPipelineManager: âœ…")

logger.info("ğŸ¯ ê¶Œì¥ ì‚¬ìš©ë²• (ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´):")
logger.info("   - M3 Max: create_m3_max_pipeline() (ì™„ì „ DI + ì–´ëŒ‘í„° ìë™)")
logger.info("   - í”„ë¡œë•ì…˜: create_production_pipeline() (ì™„ì „ DI + ì–´ëŒ‘í„° ìë™)")
logger.info("   - ê°œë°œ: create_development_pipeline() (ì™„ì „ DI + ì–´ëŒ‘í„° ìë™)")
logger.info("   - DI ì „ìš©: create_di_based_pipeline() (DIBasedPipelineManager)")
logger.info("   - ê¸°ë³¸: create_pipeline(use_dependency_injection=True, enable_adapter_pattern=True)")

logger.info("ğŸ—ï¸ ì•„í‚¤í…ì²˜ v9.1 ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ í†µí•©:")
logger.info("   - ìˆœí™˜ ì„í¬íŠ¸ â†’ âœ… ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ì™„ì „ í•´ê²°")
logger.info("   - AI ëª¨ë¸ ì—°ë™ â†’ âœ… 100% ìœ ì§€ ë° ê°•í™”")
logger.info("   - ì„±ëŠ¥ ìµœì í™” â†’ âœ… M3 Max + ì™„ì „ DI + ì–´ëŒ‘í„° í†µí•©")
logger.info("   - ì½”ë“œ í’ˆì§ˆ â†’ âœ… ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ì„¤ê³„")
logger.info("   - ìœ ì§€ë³´ìˆ˜ì„± â†’ âœ… ëŠìŠ¨í•œ ê²°í•© + ë†’ì€ ì‘ì§‘ë„")
logger.info("   - í™•ì¥ì„± â†’ âœ… DI Container + ì–´ëŒ‘í„° íŒ¨í„´")
logger.info("   - base_step_mixin.py íŒ¨í„´ â†’ âœ… ì™„ì „ ì ìš©")
logger.info("   ğŸ”¥ DIBasedPipelineManager â†’ âœ… ì™„ì „ êµ¬í˜„")

logger.info("ğŸ”¥ ì¤‘ìš” í•´ê²°ì‚¬í•­:")
logger.info("   - cannot import name 'DIBasedPipelineManager' â†’ âœ… ì™„ì „ í•´ê²°")
logger.info("   - ìˆœí™˜ì°¸ì¡° ë¬¸ì œ â†’ âœ… ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ í•´ê²°")
logger.info("   - ê¸°ì¡´ í•¨ìˆ˜/í´ë˜ìŠ¤ëª… â†’ âœ… 100% ìœ ì§€")
logger.info("   - AI ëª¨ë¸ ì—°ë™ â†’ âœ… ì™„ì „ ì‘ë™")
logger.info("   - conda í™˜ê²½ í˜¸í™˜ì„± â†’ âœ… ì™„ë²½ ì§€ì›")

# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ ë° ë°ëª¨
if __name__ == "__main__":
    print("ğŸ”¥ ì™„ì „ DI í†µí•© PipelineManager v9.1 - base_step_mixin.py ê¸°ë°˜ ì™„ì „ ê°œì„  + DIBasedPipelineManager ì™„ì„±")
    print("=" * 100)
    print("âœ… base_step_mixin.pyì˜ DI íŒ¨í„´ ì™„ì „ ì ìš©")
    print("âœ… ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°")
    print("âœ… TYPE_CHECKINGìœ¼ë¡œ import ì‹œì  ìˆœí™˜ì°¸ì¡° ë°©ì§€")
    print("âœ… ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ëŠìŠ¨í•œ ê²°í•© ê°•í™”")
    print("âœ… ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… ì™„ì „ êµ¬í˜„")
    print("âœ… ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ 100% ìœ ì§€")
    print("âœ… M3 Max 128GB ìµœì í™” ìœ ì§€")
    print("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ìµœê³  ìˆ˜ì¤€")
    print("âœ… 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ì „ ì‘ë™")
    print("âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›")
    print("ğŸ”¥ DIBasedPipelineManager í´ë˜ìŠ¤ ì™„ì „ êµ¬í˜„")
    print("ğŸ”¥ cannot import name 'DIBasedPipelineManager' ì™„ì „ í•´ê²°")
    print("=" * 100)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ ì¶œë ¥
    print("ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ë“¤ (ì™„ì „ DI + ì–´ëŒ‘í„°):")
    print("   - create_pipeline(use_dependency_injection=True, enable_adapter_pattern=True)")
    print("   - create_complete_di_pipeline() (ì™„ì „ DI + ì–´ëŒ‘í„°)")
    print("   - create_m3_max_pipeline() (M3 Max + ì™„ì „ DI + ì–´ëŒ‘í„°)")
    print("   - create_production_pipeline() (í”„ë¡œë•ì…˜ + ì™„ì „ DI + ì–´ëŒ‘í„°)")
    print("   - create_development_pipeline() (ê°œë°œ + ì™„ì „ DI + ì–´ëŒ‘í„°)")
    print("   - create_testing_pipeline() (í…ŒìŠ¤íŠ¸ + ê¸°ë³¸ DI + ì–´ëŒ‘í„°)")
    print("   ğŸ”¥ create_di_based_pipeline() (DIBasedPipelineManager ì „ìš©)")
    print("   - get_global_pipeline_manager() (ì „ì—­ + ì™„ì „ DI + ì–´ëŒ‘í„°)")
    print("   ğŸ”¥ get_global_di_based_pipeline_manager() (ì „ì—­ DI ì „ìš©)")
    print("=" * 100)
    
    # ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ ì •ë³´
    print("ğŸ’‰ ì™„ì „ ì˜ì¡´ì„± ì£¼ì… + ì–´ëŒ‘í„° íŒ¨í„´ ê¸°ëŠ¥:")
    print("   - ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°")
    print("   - ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ì„¤ê³„")
    print("   - ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì…")
    print("   - ì–´ëŒ‘í„° íŒ¨í„´ ì ìš©")
    print("   - ì§€ì—° ë¡œë”© ì§€ì›")
    print("   - DI Container ê´€ë¦¬")
    print("   - base_step_mixin.py íŒ¨í„´ ì™„ì „ ì ìš©")
    print("   ğŸ”¥ DIBasedPipelineManager ì™„ì „ í˜¸í™˜ì„±")
    print("=" * 100)
    
    import asyncio
    
    async def demo_complete_di_integration_with_di_based():
        """ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ + DIBasedPipelineManager ë°ëª¨"""
        
        print("ğŸ¯ ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ + DIBasedPipelineManager ë°ëª¨ ì‹œì‘")
        print("=" * 60)
        
        # 1. ë‹¤ì–‘í•œ íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸ (DIBasedPipelineManager í¬í•¨)
        print("1ï¸âƒ£ ëª¨ë“  íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ë“¤ í…ŒìŠ¤íŠ¸ (DIBasedPipelineManager í¬í•¨)...")
        
        try:
            # ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ (ì™„ì „ DI + ì–´ëŒ‘í„°)
            basic_pipeline = create_pipeline(
                use_dependency_injection=True, 
                enable_adapter_pattern=True
            )
            print("âœ… create_pipeline(ì™„ì „ DI + ì–´ëŒ‘í„°) ì„±ê³µ")
            
            # ì™„ì „ DI íŒŒì´í”„ë¼ì¸
            complete_di_pipeline = create_complete_di_pipeline()
            print("âœ… create_complete_di_pipeline() ì„±ê³µ")
            
            # M3 Max íŒŒì´í”„ë¼ì¸ (ì™„ì „ DI + ì–´ëŒ‘í„°)
            m3_pipeline = create_m3_max_pipeline()
            print("âœ… create_m3_max_pipeline() ì„±ê³µ (ì™„ì „ DI + ì–´ëŒ‘í„°)")
            
            # í”„ë¡œë•ì…˜ íŒŒì´í”„ë¼ì¸ (ì™„ì „ DI + ì–´ëŒ‘í„°)
            prod_pipeline = create_production_pipeline()
            print("âœ… create_production_pipeline() ì„±ê³µ (ì™„ì „ DI + ì–´ëŒ‘í„°)")
            
            # ê°œë°œ íŒŒì´í”„ë¼ì¸ (ì™„ì „ DI + ì–´ëŒ‘í„°)
            dev_pipeline = create_development_pipeline()
            print("âœ… create_development_pipeline() ì„±ê³µ (ì™„ì „ DI + ì–´ëŒ‘í„°)")
            
            # í…ŒìŠ¤íŒ… íŒŒì´í”„ë¼ì¸ (ê¸°ë³¸ DI + ì–´ëŒ‘í„°)
            test_pipeline = create_testing_pipeline()
            print("âœ… create_testing_pipeline() ì„±ê³µ (ê¸°ë³¸ DI + ì–´ëŒ‘í„°)")
            
            # ğŸ”¥ DIBasedPipelineManager ì „ìš©
            di_based_pipeline = create_di_based_pipeline()
            print("ğŸ”¥ create_di_based_pipeline() ì„±ê³µ (DIBasedPipelineManager)")
            
            # ì „ì—­ ë§¤ë‹ˆì € (ì™„ì „ DI + ì–´ëŒ‘í„°)
            global_manager = get_global_pipeline_manager()
            print("âœ… get_global_pipeline_manager() ì„±ê³µ (ì™„ì „ DI + ì–´ëŒ‘í„°)")
            
            # ğŸ”¥ ì „ì—­ DI ì „ìš© ë§¤ë‹ˆì €
            global_di_manager = get_global_di_based_pipeline_manager()
            print("ğŸ”¥ get_global_di_based_pipeline_manager() ì„±ê³µ (DIBasedPipelineManager)")
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return
        
        # 2. DIBasedPipelineManager íŠ¹í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        print("2ï¸âƒ£ DIBasedPipelineManager íŠ¹í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        try:
            # DIBasedPipelineManager ì¸ìŠ¤í„´ìŠ¤ í™•ì¸
            print(f"ğŸ” di_based_pipeline íƒ€ì…: {type(di_based_pipeline).__name__}")
            print(f"ğŸ” global_di_manager íƒ€ì…: {type(global_di_manager).__name__}")
            
            # DIBasedPipelineManagerê°€ PipelineManagerë¥¼ ìƒì†í•˜ëŠ”ì§€ í™•ì¸
            print(f"ğŸ” DIBasedPipelineManagerëŠ” PipelineManager ìƒì†: {isinstance(di_based_pipeline, PipelineManager)}")
            
            # DI ìƒíƒœ ì¡°íšŒ (DIBasedPipelineManager ì „ìš©)
            if hasattr(di_based_pipeline, 'get_di_status'):
                di_status = di_based_pipeline.get_di_status()
                print(f"ğŸ”¥ DI ê¸°ë°˜ ë§¤ë‹ˆì €: {di_status.get('di_based_manager', False)}")
                print(f"ğŸ”¥ DI ê°•ì œ í™œì„±í™”: {di_status.get('di_forced_enabled', False)}")
                
                di_specific = di_status.get('di_specific_info', {})
                print(f"ğŸ”§ í™œì„± ì–´ëŒ‘í„° ìˆ˜: {di_specific.get('total_adapters_active', 0)}")
            
            print("âœ… DIBasedPipelineManager íŠ¹í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ DIBasedPipelineManager íŠ¹í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # 3. M3 Max ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        print("3ï¸âƒ£ M3 Max ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        
        try:
            # ì´ˆê¸°í™”
            success = await m3_pipeline.initialize()
            if not success:
                print("âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return
            
            print("âœ… M3 Max ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ìƒíƒœ í™•ì¸
            status = m3_pipeline.get_pipeline_status()
            print(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {status['device']}")
            print(f"ğŸ§  AI ëª¨ë¸: {'âœ…' if status['ai_model_enabled'] else 'âŒ'}")
            print(f"ğŸ”— ModelLoader: {'âœ…' if status['model_loader_initialized'] else 'âŒ'}")
            print(f"ğŸ’‰ ì˜ì¡´ì„± ì£¼ì…: {'âœ…' if status['use_dependency_injection'] else 'âŒ'}")
            print(f"ğŸ”§ ì–´ëŒ‘í„° íŒ¨í„´: {'âœ…' if status['enable_adapter_pattern'] else 'âŒ'}")
            print(f"ğŸ”§ DI Container: {'âœ…' if status['di_container_available'] else 'âŒ'}")
            print(f"ğŸ“ base_step_mixin íŒ¨í„´: {'âœ…' if status['base_step_mixin_pattern'] else 'âŒ'}")
            print(f"ğŸ“Š ì´ˆê¸°í™”ëœ Step: {sum(1 for s in status['steps_status'].values() if s['loaded'])}/{len(status['steps_status'])}")
            print(f"ğŸ”— DI ì£¼ì…ëœ Step: {sum(1 for s in status['steps_status'].values() if s.get('di_injected', False))}")
            print(f"ğŸ”§ ì–´ëŒ‘í„° ì‚¬ìš©ëœ Step: {sum(s.get('adapters_used', 0) for s in status['steps_status'].values())}")
            
            # DI í†µê³„
            di_stats = status['dependency_injection_status']
            print(f"ğŸ’‰ DI ì£¼ì… íšŸìˆ˜: {di_stats['di_injection_count']}")
            print(f"ğŸ“ˆ DI ì„±ê³µë¥ : {di_stats['di_success_rate']:.1f}%")
            print(f"ğŸ”§ ì–´ëŒ‘í„° ì‚¬ìš©ëŸ‰: {di_stats['adapter_pattern_usage']}")
            
            # ì •ë¦¬
            await m3_pipeline.cleanup()
            print("âœ… íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        print("\nğŸ‰ ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ + DIBasedPipelineManager ë°ëª¨ ì™„ë£Œ!")
        print("âœ… base_step_mixin.py íŒ¨í„´ ì™„ì „ ì ìš©!")
        print("âœ… ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°!")
        print("âœ… ì˜ì¡´ì„± ì£¼ì… ê¸°ëŠ¥ 100% êµ¬í˜„!")
        print("âœ… ëª¨ë“  create_pipeline í•¨ìˆ˜ë“¤ì´ ì™„ì „ DI + ì–´ëŒ‘í„°ì™€ í•¨ê»˜ ì •ìƒ ì‘ë™!")
        print("âœ… M3 Max ì„±ëŠ¥ ìµœì í™” + ì™„ì „ DI + ì–´ëŒ‘í„° íŒ¨í„´ ì™„ì „ í†µí•©!")
        print("âœ… 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ì „ ì‘ë™!")
        print("âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›!")
        print("ğŸ”¥ DIBasedPipelineManager í´ë˜ìŠ¤ ì™„ì „ êµ¬í˜„ ë° ì‘ë™!")
        print("ğŸ”¥ cannot import name 'DIBasedPipelineManager' ë¬¸ì œ ì™„ì „ í•´ê²°!")
    
    # ì‹¤í–‰
    asyncio.run(demo_complete_di_integration_with_di_based())