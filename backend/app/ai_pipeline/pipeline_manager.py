# app/ai_pipeline/pipeline_manager.py
"""
ğŸ”¥ ì™„ì „ í†µí•© PipelineManager v8.0 - ì˜ì¡´ì„± ì£¼ì… + AI ëª¨ë¸ ì—°ë™ ì™„ì„±
================================================================================

âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°
âœ… Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ì—°ë™ ìœ ì§€
âœ… 2ë‹¨ê³„ í´ë°± ì „ëµ (í†µí•© AI â†’ ModelLoader â†’ ê¸°ë³¸)
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš© ê·¹ëŒ€í™”
âœ… ModelLoader Dict ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… ê¸°ì¡´ API 100% í˜¸í™˜ì„± ìœ ì§€
âœ… conda í™˜ê²½ ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ëª¨ë“  create_pipeline í•¨ìˆ˜ë“¤ ì™„ì „ ì§€ì›

ì•„í‚¤í…ì²˜ (ì˜ì¡´ì„± ì£¼ì… ì ìš©):
PipelineManager (Main Controller + DI Container)
â”œâ”€â”€ DI Container (ì˜ì¡´ì„± ê´€ë¦¬)
â”œâ”€â”€ ModelLoaderManager (AI ëª¨ë¸ ê´€ë¦¬ - DI ê¸°ë°˜)
â”œâ”€â”€ UnifiedSystemManager (í†µí•© ì‹œìŠ¤í…œ - DI ê¸°ë°˜)
â”œâ”€â”€ ExecutionManager (ì‹¤í–‰ ê´€ë¦¬ - 2ë‹¨ê³„ í´ë°±)
â”œâ”€â”€ PerformanceOptimizer (ì„±ëŠ¥ ìµœì í™”)
â””â”€â”€ StepAIConnector (Stepë³„ AI ëª¨ë¸ ì™„ì „ ì—°ë™ - DI ê¸°ë°˜)
"""

import os
import sys
import logging
import asyncio
import time
import traceback
import threading
import json
import gc
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, Callable, Union, List, Tuple, Type, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from abc import ABC, abstractmethod

# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ë°©ì§€
if TYPE_CHECKING:
    from .interfaces.model_interface import IModelLoader, IStepInterface, IMemoryManager, IDataConverter

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image, ImageEnhance, ImageFilter
import psutil

# ==============================================
# ğŸ”¥ 1. ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ import (ìµœìš°ì„ )
# ==============================================

# DI Container
try:
    from ..core.di_container import (
        get_di_container, create_step_with_di, inject_dependencies_to_step,
        initialize_di_system
    )
    DI_CONTAINER_AVAILABLE = True
except ImportError as e:
    DI_CONTAINER_AVAILABLE = False
    logging.warning(f"DI Container ì‚¬ìš© ë¶ˆê°€: {e}")

# ì¸í„°í˜ì´ìŠ¤ë“¤
try:
    from .interfaces.model_interface import (
        IModelLoader, IStepInterface, IMemoryManager, IDataConverter
    )
    INTERFACES_AVAILABLE = True
except ImportError as e:
    INTERFACES_AVAILABLE = False
    logging.warning(f"ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš© ë¶ˆê°€: {e}")

# ==============================================
# ğŸ”¥ 2. í†µí•© ì‹œìŠ¤í…œ import (ê¸°ì¡´ ìœ ì§€)
# ==============================================

# í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ
try:
    from app.ai_pipeline.utils import (
        initialize_global_utils, get_utils_manager, 
        get_system_status, optimize_system_memory,
        get_step_model_interface, get_step_memory_manager,
        get_step_data_converter, preprocess_image_for_step
    )
    UNIFIED_UTILS_AVAILABLE = True
except ImportError as e:
    UNIFIED_UTILS_AVAILABLE = False
    logging.warning(f"í†µí•© ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€: {e}")

# ModelLoader ì‹œìŠ¤í…œ (ë™ì  importë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
MODEL_LOADER_AVAILABLE = False
try:
    # ë™ì  importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
    import importlib
    model_loader_module = importlib.import_module('app.ai_pipeline.utils.model_loader')
    ModelLoader = getattr(model_loader_module, 'ModelLoader', None)
    get_global_model_loader = getattr(model_loader_module, 'get_global_model_loader', None)
    initialize_global_model_loader = getattr(model_loader_module, 'initialize_global_model_loader', None)
    StepModelInterface = getattr(model_loader_module, 'StepModelInterface', None)
    
    if all([ModelLoader, get_global_model_loader, StepModelInterface]):
        MODEL_LOADER_AVAILABLE = True
    else:
        logging.warning("ModelLoader ì¼ë¶€ ê¸°ëŠ¥ ì‚¬ìš© ë¶ˆê°€")
except ImportError as e:
    MODEL_LOADER_AVAILABLE = False
    logging.warning(f"ModelLoader ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€: {e}")

# Step ëª¨ë¸ ìš”ì²­ ì‹œìŠ¤í…œ
try:
    from app.ai_pipeline.utils.step_model_requests import (
        get_step_request, StepModelRequestAnalyzer, 
        STEP_MODEL_REQUESTS, get_all_step_requirements
    )
    STEP_REQUESTS_AVAILABLE = True
except ImportError as e:
    STEP_REQUESTS_AVAILABLE = False
    logging.warning(f"Step ìš”ì²­ ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€: {e}")

# ìë™ ëª¨ë¸ íƒì§€ ì‹œìŠ¤í…œ
try:
    from app.ai_pipeline.utils.auto_model_detector import (
        RealWorldModelDetector, create_real_world_detector,
        quick_real_model_detection
    )
    AUTO_DETECTOR_AVAILABLE = True
except ImportError as e:
    AUTO_DETECTOR_AVAILABLE = False
    logging.warning(f"ìë™ íƒì§€ ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€: {e}")

# Step í´ë˜ìŠ¤ë“¤ (ë™ì  importë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
STEP_CLASSES_AVAILABLE = False
STEP_CLASSES = {}

def _load_step_classes():
    """Step í´ë˜ìŠ¤ë“¤ ë™ì  ë¡œë”©"""
    global STEP_CLASSES_AVAILABLE, STEP_CLASSES
    
    step_modules = {
        'HumanParsingStep': 'app.ai_pipeline.steps.step_01_human_parsing',
        'PoseEstimationStep': 'app.ai_pipeline.steps.step_02_pose_estimation',
        'ClothSegmentationStep': 'app.ai_pipeline.steps.step_03_cloth_segmentation',
        'GeometricMatchingStep': 'app.ai_pipeline.steps.step_04_geometric_matching',
        'ClothWarpingStep': 'app.ai_pipeline.steps.step_05_cloth_warping',
        'VirtualFittingStep': 'app.ai_pipeline.steps.step_06_virtual_fitting',
        'PostProcessingStep': 'app.ai_pipeline.steps.step_07_post_processing',
        'QualityAssessmentStep': 'app.ai_pipeline.steps.step_08_quality_assessment'
    }
    
    for step_name, module_path in step_modules.items():
        try:
            module = importlib.import_module(module_path)
            step_class = getattr(module, step_name, None)
            if step_class:
                STEP_CLASSES[step_name] = step_class
        except ImportError:
            logging.warning(f"{step_name} ë¡œë“œ ì‹¤íŒ¨")
    
    STEP_CLASSES_AVAILABLE = len(STEP_CLASSES) > 0

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 3. ì—´ê±°í˜• ë° ë°ì´í„° í´ë˜ìŠ¤ (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class PipelineMode(Enum):
    """íŒŒì´í”„ë¼ì¸ ëª¨ë“œ"""
    DEVELOPMENT = "development"
    PRODUCTION = "production"
    TESTING = "testing"
    OPTIMIZATION = "optimization"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    MAXIMUM = "maximum"

class ProcessingStatus(Enum):
    """ì²˜ë¦¬ ìƒíƒœ"""
    IDLE = "idle"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CLEANING = "cleaning"

class ExecutionStrategy(Enum):
    """ì‹¤í–‰ ì „ëµ (2ë‹¨ê³„ í´ë°±)"""
    UNIFIED_AI = "unified_ai"        # í†µí•© ì‹œìŠ¤í…œ + AI ëª¨ë¸
    MODEL_LOADER = "model_loader"    # ModelLoader + ê¸°ë³¸ ì²˜ë¦¬
    BASIC_FALLBACK = "basic_fallback" # ê¸°ë³¸ í´ë°± (ì—ëŸ¬ ì‹œì—ë§Œ)

@dataclass
class PipelineConfig:
    """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì„¤ì • + ì˜ì¡´ì„± ì£¼ì… ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    device: str = "auto"
    quality_level: Union[QualityLevel, str] = QualityLevel.HIGH
    processing_mode: Union[PipelineMode, str] = PipelineMode.PRODUCTION
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    memory_gb: float = 128.0
    is_m3_max: bool = True
    device_type: str = "apple_silicon"
    
    # ğŸ”¥ ì˜ì¡´ì„± ì£¼ì… ì„¤ì • (ìƒˆë¡œ ì¶”ê°€)
    use_dependency_injection: bool = True
    auto_inject_dependencies: bool = True
    lazy_loading_enabled: bool = True
    interface_based_design: bool = True
    
    # ğŸ”¥ AI ëª¨ë¸ ì—°ë™ ì„¤ì • (ê¸°ì¡´ ìœ ì§€)
    ai_model_enabled: bool = True
    model_preload_enabled: bool = True
    model_cache_size: int = 20  # M3 Maxìš© í™•ì¥
    ai_inference_timeout: int = 120
    model_fallback_enabled: bool = True
    
    # ğŸ”¥ ì„±ëŠ¥ ìµœì í™” ì„¤ì • (ê¸°ì¡´ ìœ ì§€)
    performance_mode: str = "maximum"
    memory_optimization: bool = True
    gpu_memory_fraction: float = 0.95
    use_fp16: bool = True
    enable_quantization: bool = True
    parallel_processing: bool = True
    batch_processing: bool = True
    async_processing: bool = True
    
    # ğŸ”¥ 2ë‹¨ê³„ í´ë°± ì„¤ì • (ê¸°ì¡´ ìœ ì§€)
    max_fallback_attempts: int = 2
    fallback_timeout: int = 30
    enable_smart_fallback: bool = True
    
    # ì²˜ë¦¬ ì„¤ì •
    batch_size: int = 4  # M3 Max ìµœì í™”
    max_retries: int = 2  # í´ë°± ìµœì í™”
    timeout_seconds: int = 300
    thread_pool_size: int = 8  # M3 Max ë©€í‹°ì½”ì–´ í™œìš©
    
    def __post_init__(self):
        # ë¬¸ìì—´ì„ Enumìœ¼ë¡œ ë³€í™˜
        if isinstance(self.quality_level, str):
            self.quality_level = QualityLevel(self.quality_level)
        if isinstance(self.processing_mode, str):
            self.processing_mode = PipelineMode(self.processing_mode)
        
        # M3 Max ìë™ ìµœì í™”
        if self.is_m3_max:
            self.memory_gb = max(self.memory_gb, 128.0)
            self.model_cache_size = 20
            self.batch_size = 4
            self.thread_pool_size = 8
            self.gpu_memory_fraction = 0.95
            self.performance_mode = "maximum"
            self.use_dependency_injection = True

@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼ - AI ëª¨ë¸ ì •ë³´ + DI ì •ë³´ í¬í•¨"""
    success: bool
    session_id: str = ""
    result_image: Optional[Image.Image] = None
    result_tensor: Optional[torch.Tensor] = None
    quality_score: float = 0.0
    quality_grade: str = "Unknown"
    processing_time: float = 0.0
    step_results: Dict[str, Any] = field(default_factory=dict)
    step_timings: Dict[str, float] = field(default_factory=dict)
    ai_models_used: Dict[str, str] = field(default_factory=dict)  # AI ëª¨ë¸ ì¶”ì 
    execution_strategies: Dict[str, str] = field(default_factory=dict)
    
    # ğŸ”¥ ì˜ì¡´ì„± ì£¼ì… ì •ë³´ (ìƒˆë¡œ ì¶”ê°€)
    dependency_injection_info: Dict[str, Any] = field(default_factory=dict)
    
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)

@dataclass
class AIModelInfo:
    """AI ëª¨ë¸ ì •ë³´"""
    model_name: str
    model_type: str
    model_size: str
    checkpoint_path: str
    loaded: bool = False
    performance_score: float = 0.0
    memory_usage: float = 0.0
    inference_time: float = 0.0

# ==============================================
# ğŸ”¥ 4. ì˜ì¡´ì„± ì£¼ì… ì–´ëŒ‘í„° í´ë˜ìŠ¤ë“¤
# ==============================================

class ModelLoaderAdapter:
    """ModelLoaderë¥¼ IModelLoader ì¸í„°í˜ì´ìŠ¤ì— ë§ì¶”ëŠ” ì–´ëŒ‘í„°"""
    
    def __init__(self, model_loader):
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
    
    def create_step_interface(self, step_name: str):
        """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        if hasattr(self.model_loader, 'create_step_interface'):
            return self.model_loader.create_step_interface(step_name)
        return None
    
    async def load_model(self, model_config: Dict[str, Any]):
        """ëª¨ë¸ ë¡œë“œ"""
        if hasattr(self.model_loader, 'load_model'):
            return await self.model_loader.load_model(model_config)
        return None
    
    def get_model(self, model_name: str):
        """ëª¨ë¸ ì¡°íšŒ"""
        if hasattr(self.model_loader, 'get_model'):
            return self.model_loader.get_model(model_name)
        return None
    
    def register_model(self, model_name: str, model_config: Dict[str, Any]) -> bool:
        """ëª¨ë¸ ë“±ë¡"""
        if hasattr(self.model_loader, 'register_model'):
            return self.model_loader.register_model(model_name, model_config)
        return False
    
    def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self.model_loader, 'cleanup'):
            self.model_loader.cleanup()

class MemoryManagerAdapter:
    """MemoryManagerë¥¼ IMemoryManager ì¸í„°í˜ì´ìŠ¤ì— ë§ì¶”ëŠ” ì–´ëŒ‘í„°"""
    
    def __init__(self, memory_manager):
        self.memory_manager = memory_manager
    
    def cleanup_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if hasattr(self.memory_manager, 'cleanup_memory'):
            return self.memory_manager.cleanup_memory(aggressive=aggressive)
        
        # ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        elif torch.backends.mps.is_available():
            try:
                torch.mps.empty_cache()
            except:
                pass
        
        return {'success': True, 'method': 'basic_cleanup'}
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
        if hasattr(self.memory_manager, 'get_memory_stats'):
            return self.memory_manager.get_memory_stats()
        
        # ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë³´
        stats = {'available': True}
        try:
            if psutil:
                memory = psutil.virtual_memory()
                stats.update({
                    'cpu_memory_percent': memory.percent,
                    'cpu_memory_available_gb': memory.available / (1024**3)
                })
        except:
            pass
        
        return stats

# ==============================================
# ğŸ”¥ 5. ì„±ëŠ¥ ìµœì í™” ê´€ë¦¬ì (ê¸°ì¡´ ìœ ì§€)
# ==============================================

class PerformanceOptimizer:
    """M3 Max ì„±ëŠ¥ ìµœì í™” ê´€ë¦¬ì"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        self.memory_pool = {}
        self.model_cache = {}
        self.performance_cache = {}
        
    def optimize_system(self):
        """ì‹œìŠ¤í…œ ìµœì í™”"""
        try:
            # ğŸ”¥ M3 Max íŠ¹í™” ìµœì í™”
            if self.config.is_m3_max:
                self._optimize_m3_max()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            if self.config.memory_optimization:
                self._optimize_memory()
            
            # GPU ìµœì í™”
            if self.config.device in ['mps', 'cuda']:
                self._optimize_gpu()
            
            # ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
            if self.config.parallel_processing:
                self._optimize_parallel_processing()
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_m3_max(self):
        """M3 Max íŠ¹í™” ìµœì í™”"""
        try:
            # MPS ìµœì í™”
            if torch.backends.mps.is_available():
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.95'
                torch.mps.empty_cache()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            os.environ['OMP_NUM_THREADS'] = '8'
            os.environ['MKL_NUM_THREADS'] = '8'
            
            self.logger.info("âœ… M3 Max ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”
            gc.set_threshold(700, 10, 10)
            gc.collect()
            
            # ë©”ëª¨ë¦¬ í’€ ë¯¸ë¦¬ í• ë‹¹
            if self.config.device == 'mps':
                # MPS ë©”ëª¨ë¦¬ ë¯¸ë¦¬ í• ë‹¹
                dummy_tensor = torch.zeros(1024, 1024, device='mps')
                del dummy_tensor
                torch.mps.empty_cache()
            
            self.logger.info("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_gpu(self):
        """GPU ìµœì í™”"""
        try:
            if self.config.device == 'mps':
                # MPS ìµœì í™”
                torch.backends.mps.enable_fallback = True
            elif self.config.device == 'cuda':
                # CUDA ìµœì í™”
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
            
            self.logger.info("âœ… GPU ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ GPU ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _optimize_parallel_processing(self):
        """ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”"""
        try:
            # ìŠ¤ë ˆë“œ í’€ í¬ê¸° ìµœì í™”
            cpu_count = psutil.cpu_count(logical=False)
            optimal_threads = min(self.config.thread_pool_size, cpu_count * 2)
            
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            os.environ['OPENBLAS_NUM_THREADS'] = str(optimal_threads)
            
            self.logger.info(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” ì™„ë£Œ (ìŠ¤ë ˆë“œ: {optimal_threads})")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 6. AI ëª¨ë¸ ì—°ë™ ê´€ë¦¬ì (DI ì ìš©)
# ==============================================

class ModelLoaderManager:
    """AI ëª¨ë¸ ë¡œë” ê´€ë¦¬ì - DI ì ìš© + Dict ë¬¸ì œ ì™„ì „ í•´ê²°"""
    
    def __init__(self, config: PipelineConfig, device: str, logger: logging.Logger, di_container=None):
        self.config = config
        self.device = device
        self.logger = logger
        self.di_container = di_container
        self.model_loader = None
        self.model_interfaces = {}
        self.loaded_models = {}
        self.model_cache = {}
        self.is_initialized = False
        
    async def initialize(self) -> bool:
        """ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” - DI ê¸°ë°˜"""
        try:
            self.logger.info("ğŸ§  ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘ (DI)...")
            
            if not MODEL_LOADER_AVAILABLE:
                self.logger.warning("âš ï¸ ModelLoader ì‚¬ìš© ë¶ˆê°€")
                return False
            
            # ğŸ”¥ DI Containerì—ì„œ ModelLoader ì¡°íšŒ
            if self.di_container and self.config.use_dependency_injection:
                self.model_loader = self.di_container.get('IModelLoader')
                if self.model_loader:
                    self.logger.info("âœ… DI Containerì—ì„œ ModelLoader íšë“")
                else:
                    self.logger.info("âš ï¸ DI Containerì— ModelLoader ì—†ìŒ, ì§ì ‘ ìƒì„±")
            
            # ì§ì ‘ ìƒì„± (í´ë°±)
            if not self.model_loader:
                max_attempts = 3
                for attempt in range(max_attempts):
                    try:
                        # ì „ì—­ ì´ˆê¸°í™” ì‹œë„
                        if initialize_global_model_loader:
                            raw_loader = await asyncio.get_event_loop().run_in_executor(
                                None, initialize_global_model_loader
                            )
                        else:
                            raw_loader = ModelLoader(device=self.device)
                        
                        # Dict íƒ€ì… ê²€ì¦
                        if isinstance(raw_loader, dict):
                            self.logger.warning(f"âš ï¸ ModelLoaderê°€ dict íƒ€ì… (ì‹œë„ {attempt + 1})")
                            
                            # ì§ì ‘ ìƒì„± ì‹œë„
                            raw_loader = ModelLoader(device=self.device)
                            if hasattr(raw_loader, 'initialize'):
                                await raw_loader.initialize()
                        
                        # ì–´ëŒ‘í„°ë¡œ ë˜í•‘
                        if (not isinstance(raw_loader, dict) and 
                            hasattr(raw_loader, 'create_step_interface')):
                            self.model_loader = ModelLoaderAdapter(raw_loader)
                            self.is_initialized = True
                            self.logger.info("âœ… ModelLoader ì´ˆê¸°í™” ì„±ê³µ (ì–´ëŒ‘í„°)")
                            break
                            
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ì´ˆê¸°í™” ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                        if attempt < max_attempts - 1:
                            await asyncio.sleep(1)
                            continue
            else:
                # DI Containerì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°
                self.is_initialized = True
            
            if not self.is_initialized:
                self.logger.error("âŒ ModelLoader ì´ˆê¸°í™” ì™„ì „ ì‹¤íŒ¨")
                return False
            
            # DI Containerì— ë“±ë¡
            if self.di_container and self.model_loader:
                self.di_container.register_instance('IModelLoader', self.model_loader)
            
            # Stepë³„ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            await self._create_step_interfaces()
            
            # ì¤‘ìš” ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
            if self.config.model_preload_enabled:
                await self._preload_critical_models()
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _create_step_interfaces(self):
        """Stepë³„ ModelLoader ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        try:
            step_names = [
                'HumanParsingStep', 'PoseEstimationStep', 'ClothSegmentationStep',
                'GeometricMatchingStep', 'ClothWarpingStep', 'VirtualFittingStep',
                'PostProcessingStep', 'QualityAssessmentStep'
            ]
            
            for step_name in step_names:
                try:
                    interface = self.model_loader.create_step_interface(step_name)
                    self.model_interfaces[step_name] = interface
                    self.logger.info(f"âœ… {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            self.logger.error(f"âŒ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
    
    async def _preload_critical_models(self):
        """ì¤‘ìš” AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ"""
        try:
            # ğŸ”¥ í•µì‹¬ Stepë“¤ì˜ AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
            critical_models = [
                ('HumanParsingStep', 'graphonomy'),
                ('ClothSegmentationStep', 'u2net'),
                ('VirtualFittingStep', 'ootdiffusion'),
                ('QualityAssessmentStep', 'clipiqa')
            ]
            
            for step_name, model_name in critical_models:
                try:
                    if step_name in self.model_interfaces:
                        interface = self.model_interfaces[step_name]
                        if hasattr(interface, 'get_model'):
                            model = await interface.get_model(model_name)
                            if model:
                                self.loaded_models[f"{step_name}_{model_name}"] = model
                                self.logger.info(f"âœ… {step_name} AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ: {model_name}")
                            else:
                                self.logger.warning(f"âš ï¸ {step_name} AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì˜¤ë¥˜: {e}")
                    
        except Exception as e:
            self.logger.error(f"âŒ ì¤‘ìš” ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def get_step_interface(self, step_name: str) -> Optional[Any]:
        """Step ì¸í„°í˜ì´ìŠ¤ ë°˜í™˜"""
        return self.model_interfaces.get(step_name)
    
    def get_loaded_model(self, step_name: str, model_name: str) -> Optional[Any]:
        """ë¡œë“œëœ ëª¨ë¸ ë°˜í™˜"""
        key = f"{step_name}_{model_name}"
        return self.loaded_models.get(key)

# ==============================================
# ğŸ”¥ 7. í†µí•© ì‹œìŠ¤í…œ ê´€ë¦¬ì (DI ì ìš©)
# ==============================================

class UnifiedSystemManager:
    """í†µí•© ì‹œìŠ¤í…œ ê´€ë¦¬ì - DI ì ìš©"""
    
    def __init__(self, config: PipelineConfig, device: str, logger: logging.Logger, di_container=None):
        self.config = config
        self.device = device
        self.logger = logger
        self.di_container = di_container
        self.utils_manager = None
        self.auto_detector = None
        self.is_initialized = False
        
    async def initialize(self) -> bool:
        """í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” - DI ê¸°ë°˜"""
        try:
            if not UNIFIED_UTILS_AVAILABLE:
                self.logger.warning("âš ï¸ í†µí•© ìœ í‹¸ë¦¬í‹° ì‚¬ìš© ë¶ˆê°€")
                return False
            
            self.logger.info("ğŸ”— í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘ (DI)...")
            
            # DI Containerì—ì„œ ì¡°íšŒ
            if self.di_container and self.config.use_dependency_injection:
                self.utils_manager = self.di_container.get('utils_manager')
                if self.utils_manager:
                    self.logger.info("âœ… DI Containerì—ì„œ UtilsManager íšë“")
            
            # ì§ì ‘ ì´ˆê¸°í™” (í´ë°±)
            if not self.utils_manager:
                # ì „ì—­ ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™”
                result = await initialize_global_utils(
                    device=self.device,
                    memory_gb=self.config.memory_gb,
                    is_m3_max=self.config.is_m3_max,
                    optimization_enabled=True
                )
                
                if result.get("success", False):
                    self.utils_manager = get_utils_manager()
                    
                    # DI Containerì— ë“±ë¡
                    if self.di_container:
                        self.di_container.register_instance('utils_manager', self.utils_manager)
            
            if self.utils_manager:
                # ìë™ íƒì§€ ì‹œìŠ¤í…œ ì—°ë™
                if AUTO_DETECTOR_AVAILABLE:
                    self.auto_detector = create_real_world_detector()
                
                self.is_initialized = True
                self.logger.info("âœ… í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (DI)")
                return True
            
            self.logger.error("âŒ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

# ==============================================
# ğŸ”¥ 8. Stepë³„ AI ì—°ë™ ê´€ë¦¬ì (DI ì ìš©)
# ==============================================

class StepAIConnector:
    """Stepë³„ AI ëª¨ë¸ ì™„ì „ ì—°ë™ - DI ì ìš©"""
    
    def __init__(self, model_manager: ModelLoaderManager, unified_manager: UnifiedSystemManager, di_container=None):
        self.model_manager = model_manager
        self.unified_manager = unified_manager
        self.di_container = di_container
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        self.step_ai_models = {}
        
    async def setup_step_ai_connection(self, step_instance, step_name: str):
        """Stepë³„ AI ì—°ê²° ì„¤ì • - DI ê¸°ë°˜"""
        try:
            step_class_name = f"{step_name.title().replace('_', '')}Step"
            
            # ğŸ”¥ DI Containerë¥¼ í†µí•œ ì˜ì¡´ì„± ì£¼ì…
            if self.di_container and hasattr(step_instance, 'inject_dependencies'):
                model_loader = self.di_container.get('IModelLoader')
                memory_manager = self.di_container.get('IMemoryManager')
                data_converter = self.di_container.get('IDataConverter')
                
                step_instance.inject_dependencies(
                    model_loader=model_loader,
                    memory_manager=memory_manager,
                    data_converter=data_converter
                )
                self.logger.info(f"âœ… {step_name} DI ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            
            # ğŸ”¥ 1ìˆœìœ„: í†µí•© ì‹œìŠ¤í…œ ì¸í„°í˜ì´ìŠ¤
            if self.unified_manager.is_initialized and self.unified_manager.utils_manager:
                try:
                    unified_interface = self.unified_manager.utils_manager.create_step_interface(step_class_name)
                    setattr(step_instance, 'unified_interface', unified_interface)
                    self.logger.info(f"âœ… {step_name} í†µí•© ì¸í„°í˜ì´ìŠ¤ ì—°ê²°")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} í†µí•© ì¸í„°í˜ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ 2ìˆœìœ„: ModelLoader ì¸í„°í˜ì´ìŠ¤
            if self.model_manager.is_initialized:
                try:
                    model_interface = self.model_manager.get_step_interface(step_class_name)
                    if model_interface:
                        setattr(step_instance, 'model_interface', model_interface)
                        self.logger.info(f"âœ… {step_name} ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ê²°")
                        
                        # ğŸ”¥ í•µì‹¬: ì‹¤ì œ AI ëª¨ë¸ ì—°ë™
                        await self._setup_real_ai_model(step_instance, step_name, step_class_name)
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ ì§€ì—° ì˜ì¡´ì„± í•´ê²°
            if hasattr(step_instance, 'resolve_lazy_dependencies'):
                step_instance.resolve_lazy_dependencies()
                    
        except Exception as e:
            self.logger.error(f"âŒ {step_name} AI ì—°ê²° ì„¤ì • ì‹¤íŒ¨: {e}")
    
    async def _setup_real_ai_model(self, step_instance, step_name: str, step_class_name: str):
        """ì‹¤ì œ AI ëª¨ë¸ ì—°ë™"""
        try:
            # Stepë³„ ê¶Œì¥ AI ëª¨ë¸ ë§¤í•‘
            ai_model_mapping = {
                'human_parsing': 'graphonomy',
                'pose_estimation': 'mediapipe_pose',
                'cloth_segmentation': 'u2net',
                'geometric_matching': 'thin_plate_spline',
                'cloth_warping': 'tps_warping',
                'virtual_fitting': 'ootdiffusion',
                'post_processing': 'esrgan',
                'quality_assessment': 'clipiqa'
            }
            
            model_name = ai_model_mapping.get(step_name)
            if model_name and hasattr(step_instance, 'model_interface'):
                try:
                    # ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ
                    interface = getattr(step_instance, 'model_interface')
                    if hasattr(interface, 'get_model'):
                        ai_model = await interface.get_model(model_name)
                        if ai_model:
                            setattr(step_instance, '_ai_model', ai_model)
                            setattr(step_instance, '_ai_model_name', model_name)
                            self.step_ai_models[step_name] = ai_model
                            self.logger.info(f"ğŸ§  {step_name} ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ ì™„ë£Œ: {model_name}")
                        else:
                            self.logger.warning(f"âš ï¸ {step_name} AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                            
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} AI ëª¨ë¸ ì—°ë™ ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 9. ì‹¤í–‰ ê´€ë¦¬ì (ê¸°ì¡´ ìœ ì§€ + DI ì§€ì›)
# ==============================================

class OptimizedExecutionManager:
    """ìµœì í™”ëœ ì‹¤í–‰ ê´€ë¦¬ì - 2ë‹¨ê³„ í´ë°± + DI ì§€ì›"""
    
    def __init__(self, config: PipelineConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.execution_cache = {}
        self.performance_stats = {}
        
    async def execute_step_optimized(
        self,
        step,
        step_name: str,
        current_data: torch.Tensor,
        clothing_tensor: torch.Tensor,
        **kwargs
    ) -> Tuple[Dict[str, Any], str]:
        """ìµœì í™”ëœ Step ì‹¤í–‰ - 2ë‹¨ê³„ í´ë°± + DI ì§€ì›"""
        
        start_time = time.time()
        execution_attempts = []
        
        # ğŸ”¥ 1ìˆœìœ„: í†µí•© ì‹œìŠ¤í…œ + AI ëª¨ë¸
        try:
            result, strategy = await self._execute_unified_ai(
                step, step_name, current_data, clothing_tensor, **kwargs
            )
            execution_attempts.append(("unified_ai", result.get('success', False)))
            
            if result.get('success', False):
                result['execution_time'] = time.time() - start_time
                return result, strategy
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ {step_name} í†µí•© AI ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            execution_attempts.append(("unified_ai", False))
        
        # ğŸ”¥ 2ìˆœìœ„: ModelLoader + ê¸°ë³¸ ì²˜ë¦¬
        try:
            result, strategy = await self._execute_model_loader(
                step, step_name, current_data, clothing_tensor, **kwargs
            )
            execution_attempts.append(("model_loader", result.get('success', False)))
            
            if result.get('success', False):
                result['execution_time'] = time.time() - start_time
                return result, strategy
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ {step_name} ModelLoader ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            execution_attempts.append(("model_loader", False))
        
        # ğŸ”¥ ìµœì¢… í´ë°±: ê¸°ë³¸ ì²˜ë¦¬ (ì—ëŸ¬ ì‹œì—ë§Œ)
        try:
            result, strategy = await self._execute_basic_fallback(
                step, step_name, current_data, clothing_tensor, **kwargs
            )
            execution_attempts.append(("basic_fallback", result.get('success', False)))
            
            result['execution_time'] = time.time() - start_time
            result['execution_attempts'] = execution_attempts
            
            return result, strategy
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ëª¨ë“  ì‹¤í–‰ ì „ëµ ì‹¤íŒ¨: {e}")
            
            return {
                'success': False,
                'error': f"ëª¨ë“  ì‹¤í–‰ ì „ëµ ì‹¤íŒ¨: {str(e)}",
                'execution_time': time.time() - start_time,
                'execution_attempts': execution_attempts,
                'confidence': 0.0,
                'quality_score': 0.0
            }, "failed"
    
    async def _execute_unified_ai(self, step, step_name: str, current_data: torch.Tensor, 
                                  clothing_tensor: torch.Tensor, **kwargs) -> Tuple[Dict[str, Any], str]:
        """í†µí•© ì‹œìŠ¤í…œ + AI ëª¨ë¸ ì‹¤í–‰"""
        try:
            # í†µí•© ì¸í„°í˜ì´ìŠ¤ ìš°ì„  ì‚¬ìš©
            if hasattr(step, 'unified_interface') and step.unified_interface:
                if hasattr(step.unified_interface, 'process_image'):
                    result = step.unified_interface.process_image(
                        current_data,
                        clothing_data=clothing_tensor,
                        optimize_memory=True,
                        **kwargs
                    )
                    
                    if result and result.get('success', False):
                        return {
                            'success': True,
                            'result': result.get('processed_image', current_data),
                            'confidence': result.get('confidence', 0.95),
                            'quality_score': result.get('quality_score', 0.95),
                            'model_used': result.get('model_used', 'unified_ai'),
                            'ai_model_name': result.get('ai_model_name', 'unified_system'),
                            'processing_method': 'unified_ai'
                        }, ExecutionStrategy.UNIFIED_AI.value
            
            # AI ëª¨ë¸ ì§ì ‘ ì‚¬ìš©
            if hasattr(step, '_ai_model') and step._ai_model:
                ai_result = await self._run_ai_inference(step._ai_model, current_data, clothing_tensor, **kwargs)
                if ai_result is not None:
                    return {
                        'success': True,
                        'result': ai_result,
                        'confidence': 0.92,
                        'quality_score': 0.92,
                        'model_used': getattr(step, '_ai_model_name', 'unknown_ai'),
                        'ai_model_name': getattr(step, '_ai_model_name', 'unknown_ai'),
                        'processing_method': 'direct_ai'
                    }, ExecutionStrategy.UNIFIED_AI.value
            
            raise Exception("í†µí•© AI ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€")
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0
            }, "unified_ai_error"
    
    async def _execute_model_loader(self, step, step_name: str, current_data: torch.Tensor,
                                   clothing_tensor: torch.Tensor, **kwargs) -> Tuple[Dict[str, Any], str]:
        """ModelLoader + ê¸°ë³¸ ì²˜ë¦¬ ì‹¤í–‰"""
        try:
            # ModelLoader ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
            if hasattr(step, 'model_interface') and step.model_interface:
                # ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡ 
                if hasattr(step.model_interface, 'list_available_models'):
                    available_models = await step.model_interface.list_available_models()
                    if available_models:
                        if hasattr(step.model_interface, 'get_model'):
                            model = await step.model_interface.get_model(available_models[0])
                            if model:
                                ai_result = await self._run_ai_inference(model, current_data, clothing_tensor, **kwargs)
                                if ai_result is not None:
                                    return {
                                        'success': True,
                                        'result': ai_result,
                                        'confidence': 0.88,
                                        'quality_score': 0.88,
                                        'model_used': available_models[0],
                                        'ai_model_name': available_models[0],
                                        'processing_method': 'model_loader'
                                    }, ExecutionStrategy.MODEL_LOADER.value
            
            # Step ê¸°ë³¸ ì²˜ë¦¬ ë¡œì§
            result = await self._execute_step_logic(step, step_name, current_data, clothing_tensor, **kwargs)
            
            return {
                'success': True,
                'result': result.get('result', current_data),
                'confidence': result.get('confidence', 0.85),
                'quality_score': result.get('quality_score', 0.85),
                'model_used': 'step_logic',
                'ai_model_name': 'step_processing',
                'processing_method': 'step_logic'
            }, ExecutionStrategy.MODEL_LOADER.value
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0
            }, "model_loader_error"
    
    async def _execute_basic_fallback(self, step, step_name: str, current_data: torch.Tensor,
                                     clothing_tensor: torch.Tensor, **kwargs) -> Tuple[Dict[str, Any], str]:
        """ê¸°ë³¸ í´ë°± ì‹¤í–‰"""
        try:
            # ìµœì†Œí•œì˜ ì²˜ë¦¬
            result = await self._execute_step_logic(step, step_name, current_data, clothing_tensor, **kwargs)
            
            return {
                'success': True,
                'result': result.get('result', current_data),
                'confidence': 0.75,
                'quality_score': 0.75,
                'model_used': 'basic_fallback',
                'ai_model_name': 'fallback_processing',
                'processing_method': 'basic_fallback'
            }, ExecutionStrategy.BASIC_FALLBACK.value
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0
            }, "basic_fallback_error"
    
    async def _run_ai_inference(self, ai_model, current_data: torch.Tensor, 
                               clothing_tensor: torch.Tensor, **kwargs) -> Optional[torch.Tensor]:
        """ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
        try:
            # AI ëª¨ë¸ë³„ ì¶”ë¡  ì‹¤í–‰
            if hasattr(ai_model, 'process'):
                if asyncio.iscoroutinefunction(ai_model.process):
                    return await ai_model.process(current_data, clothing_tensor, **kwargs)
                else:
                    return ai_model.process(current_data, clothing_tensor, **kwargs)
            elif hasattr(ai_model, '__call__'):
                if asyncio.iscoroutinefunction(ai_model.__call__):
                    return await ai_model(current_data, clothing_tensor, **kwargs)
                else:
                    return ai_model(current_data, clothing_tensor, **kwargs)
            elif hasattr(ai_model, 'forward'):
                return ai_model.forward(current_data, clothing_tensor)
            else:
                self.logger.warning("âš ï¸ AI ëª¨ë¸ ì¶”ë¡  ë©”ì„œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    async def _execute_step_logic(self, step, step_name: str, current_data: torch.Tensor,
                                 clothing_tensor: torch.Tensor, **kwargs) -> Dict[str, Any]:
        """Stepë³„ ê¸°ë³¸ ì²˜ë¦¬ ë¡œì§"""
        try:
            if hasattr(step, 'process'):
                if step_name == 'human_parsing':
                    return await step.process(current_data)
                elif step_name == 'pose_estimation':
                    return await step.process(current_data)
                elif step_name == 'cloth_segmentation':
                    return await step.process(clothing_tensor, clothing_type=kwargs.get('clothing_type', 'shirt'))
                elif step_name == 'geometric_matching':
                    return await step.process(
                        person_parsing={'result': current_data},
                        pose_keypoints=self._generate_dummy_pose_keypoints(),
                        clothing_segmentation={'mask': clothing_tensor},
                        clothing_type=kwargs.get('clothing_type', 'shirt')
                    )
                elif step_name == 'cloth_warping':
                    return await step.process(
                        current_data, clothing_tensor, 
                        kwargs.get('body_measurements', {}), 
                        kwargs.get('fabric_type', 'cotton')
                    )
                elif step_name == 'virtual_fitting':
                    return await step.process(current_data, clothing_tensor, kwargs.get('style_preferences', {}))
                elif step_name == 'post_processing':
                    return await step.process(current_data)
                elif step_name == 'quality_assessment':
                    return await step.process(current_data, clothing_tensor)
                else:
                    return await step.process(current_data)
            else:
                # ê¸°ë³¸ í†µê³¼ ì²˜ë¦¬
                return {'result': current_data, 'confidence': 0.8, 'quality_score': 0.8}
                
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ê¸°ë³¸ ë¡œì§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {'result': current_data, 'confidence': 0.5, 'quality_score': 0.5}
    
    def _generate_dummy_pose_keypoints(self) -> List[List[float]]:
        """ë”ë¯¸ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ìƒì„±"""
        return [[256 + np.random.uniform(-50, 50), 256 + np.random.uniform(-100, 100), 0.8] for _ in range(18)]

# ==============================================
# ğŸ”¥ 10. ë©”ì¸ PipelineManager í´ë˜ìŠ¤ (ì™„ì „ í†µí•©)
# ==============================================

class PipelineManager:
    """
    ğŸ”¥ ì™„ì „ í†µí•© PipelineManager v8.0 - ì˜ì¡´ì„± ì£¼ì… + AI ëª¨ë¸ ì—°ë™ ì™„ì„±
    
    âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°
    âœ… Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ì—°ë™ ìœ ì§€
    âœ… 2ë‹¨ê³„ í´ë°± ì „ëµ (í†µí•© AI â†’ ModelLoader â†’ ê¸°ë³¸)
    âœ… M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš© ê·¹ëŒ€í™”
    âœ… ê¸°ì¡´ API 100% í˜¸í™˜ì„± ìœ ì§€
    âœ… DI Container ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬
    """
    
    def __init__(
        self,
        config_path: Optional[str] = None,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], PipelineConfig]] = None,
        **kwargs
    ):
        """íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì´ˆê¸°í™” - DI í†µí•©"""
        
        # 1. ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._auto_detect_device(device)
        
        # 2. ì„¤ì • ì´ˆê¸°í™”
        if isinstance(config, PipelineConfig):
            self.config = config
        else:
            config_dict = self._load_config(config_path) if config_path else {}
            if config:
                config_dict.update(config if isinstance(config, dict) else {})
            config_dict.update(kwargs)
            
            # M3 Max ìë™ ê°ì§€ ë° ìµœì í™”
            if self._detect_m3_max():
                config_dict.update({
                    'is_m3_max': True,
                    'memory_gb': 128.0,
                    'device_type': 'apple_silicon',
                    'performance_mode': 'maximum',
                    'use_dependency_injection': True
                })
            
            self.config = PipelineConfig(device=self.device, **config_dict)
        
        # 3. ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        # ğŸ”¥ 4. DI Container ì´ˆê¸°í™” (ìµœìš°ì„ )
        self.di_container = None
        if DI_CONTAINER_AVAILABLE and self.config.use_dependency_injection:
            try:
                self.di_container = get_di_container()
                self._setup_default_dependencies()
                self.logger.info("âœ… DI Container ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DI Container ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.di_container = None
        
        # 5. ì„±ëŠ¥ ìµœì í™” ì ìš©
        self.performance_optimizer = PerformanceOptimizer(self.config)
        self.performance_optimizer.optimize_system()
        
        # 6. ê´€ë¦¬ìë“¤ ì´ˆê¸°í™” (DI ì ìš©)
        self.model_manager = ModelLoaderManager(self.config, self.device, self.logger, self.di_container)
        self.unified_manager = UnifiedSystemManager(self.config, self.device, self.logger, self.di_container)
        self.execution_manager = OptimizedExecutionManager(self.config, self.logger)
        
        # 7. AI ì—°ë™ ê´€ë¦¬ì
        self.ai_connector = None  # ì´ˆê¸°í™” í›„ ìƒì„±
        
        # 8. íŒŒì´í”„ë¼ì¸ ìƒíƒœ
        self.is_initialized = False
        self.current_status = ProcessingStatus.IDLE
        self.step_order = [
            'human_parsing', 'pose_estimation', 'cloth_segmentation',
            'geometric_matching', 'cloth_warping', 'virtual_fitting',
            'post_processing', 'quality_assessment'
        ]
        self.steps = {}
        
        # 9. ì„±ëŠ¥ ë° í†µê³„
        self.performance_metrics = {
            'total_sessions': 0,
            'successful_sessions': 0,
            'ai_model_usage': {},
            'average_processing_time': 0.0,
            'average_quality_score': 0.0,
            'di_injection_count': 0,
            'di_success_rate': 0.0
        }
        
        # 10. ë©”ëª¨ë¦¬ ê´€ë¦¬ (DI ì ìš©)
        self.data_converter = self._create_data_converter()
        self.memory_manager = self._create_memory_manager()
        
        # 11. ìŠ¤ë ˆë“œ í’€
        self.thread_pool = ThreadPoolExecutor(max_workers=self.config.thread_pool_size)
        
        self.logger.info(f"ğŸ”¥ ì™„ì „ í†µí•© PipelineManager v8.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {self.device}")
        self.logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬: {self.config.memory_gb}GB")
        self.logger.info(f"ğŸš€ M3 Max: {'âœ…' if self.config.is_m3_max else 'âŒ'}")
        self.logger.info(f"ğŸ§  AI ëª¨ë¸: {'âœ…' if self.config.ai_model_enabled else 'âŒ'}")
        self.logger.info(f"ğŸ”— ì˜ì¡´ì„± ì£¼ì…: {'âœ…' if self.config.use_dependency_injection else 'âŒ'}")
    
    def _setup_default_dependencies(self):
        """ê¸°ë³¸ ì˜ì¡´ì„±ë“¤ DI Containerì— ë“±ë¡"""
        try:
            if not self.di_container:
                return
            
            # ë°ì´í„° ë³€í™˜ê¸° ë“±ë¡
            data_converter = self._create_data_converter()
            self.di_container.register_instance('IDataConverter', data_converter)
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë“±ë¡
            memory_manager = self._create_memory_manager()
            memory_adapter = MemoryManagerAdapter(memory_manager)
            self.di_container.register_instance('IMemoryManager', memory_adapter)
            
            self.logger.info("âœ… ê¸°ë³¸ ì˜ì¡´ì„± DI Container ë“±ë¡ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ì˜ì¡´ì„± ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device and preferred_device != "auto":
            return preferred_device
        
        try:
            if torch.backends.mps.is_available():
                return 'mps'
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        except:
            return 'cpu'
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ì¹© ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                    capture_output=True, text=True, timeout=5
                )
                chip_info = result.stdout.strip()
                return 'M3' in chip_info and 'Max' in chip_info
        except:
            pass
        return False
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if not config_path or not os.path.exists(config_path):
            return {}
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.warning(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _create_data_converter(self):
        """ë°ì´í„° ë³€í™˜ê¸° ìƒì„±"""
        class OptimizedDataConverter:
            def __init__(self, device: str):
                self.device = device
                
            def preprocess_image(self, image_input) -> torch.Tensor:
                """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
                if isinstance(image_input, str):
                    image = Image.open(image_input).convert('RGB')
                elif isinstance(image_input, Image.Image):
                    image = image_input.convert('RGB')
                elif isinstance(image_input, np.ndarray):
                    image = Image.fromarray(image_input).convert('RGB')
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_input)}")
                
                # ìµœì í™”ëœ ë¦¬ì‚¬ì´ì¦ˆ
                if image.size != (512, 512):
                    image = image.resize((512, 512), Image.Resampling.LANCZOS)
                
                # í…ì„œ ë³€í™˜
                img_array = np.array(image)
                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0
                img_tensor = img_tensor.unsqueeze(0).to(self.device)
                
                return img_tensor
            
            def tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
                """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
                if tensor.dim() == 4:
                    tensor = tensor.squeeze(0)
                if tensor.shape[0] == 3:
                    tensor = tensor.permute(1, 2, 0)
                
                tensor = torch.clamp(tensor, 0, 1)
                tensor = tensor.cpu()
                array = (tensor.numpy() * 255).astype(np.uint8)
                
                return Image.fromarray(array)
        
        return OptimizedDataConverter(self.device)
    
    def _create_memory_manager(self):
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±"""
        class OptimizedMemoryManager:
            def __init__(self, device: str):
                self.device = device
                
            def cleanup_memory(self):
                """ë©”ëª¨ë¦¬ ì •ë¦¬"""
                gc.collect()
                
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                elif self.device == "mps" and torch.backends.mps.is_available():
                    try:
                        torch.mps.empty_cache()
                        torch.mps.synchronize()
                    except:
                        pass
        
        return OptimizedMemoryManager(self.device)
    
    async def initialize(self) -> bool:
        """íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” - DI + AI ëª¨ë¸ ì—°ë™ ì™„ì„±"""
        try:
            self.logger.info("ğŸš€ ì™„ì „ í†µí•© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹œì‘ (DI + AI)...")
            self.current_status = ProcessingStatus.INITIALIZING
            start_time = time.time()
            
            # 1. DI ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            if self.config.use_dependency_injection and DI_CONTAINER_AVAILABLE:
                try:
                    initialize_di_system()
                    self.logger.info("âœ… DI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ DI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # 2. Step í´ë˜ìŠ¤ë“¤ ë™ì  ë¡œë”©
            _load_step_classes()
            if not STEP_CLASSES_AVAILABLE:
                self.logger.warning("âš ï¸ Step í´ë˜ìŠ¤ë“¤ ë¡œë”© ì‹¤íŒ¨")
            
            # 3. ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory()
            
            # 4. ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìµœìš°ì„ )
            model_success = await self.model_manager.initialize()
            if model_success:
                self.logger.info("âœ… ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ ModelLoader ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # 5. í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            unified_success = await self.unified_manager.initialize()
            if unified_success:
                self.logger.info("âœ… í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # 6. AI ì—°ë™ ê´€ë¦¬ì ìƒì„±
            self.ai_connector = StepAIConnector(
                self.model_manager, 
                self.unified_manager, 
                self.di_container
            )
            
            # 7. Step í´ë˜ìŠ¤ë“¤ ì´ˆê¸°í™” + AI ì—°ë™ + DI
            success_count = await self._initialize_steps_with_di_and_ai()
            
            # 8. ì´ˆê¸°í™” ê²€ì¦
            success_rate = success_count / len(self.step_order)
            if success_rate < 0.5:
                self.logger.warning(f"ì´ˆê¸°í™” ì„±ê³µë¥  ë‚®ìŒ: {success_rate:.1%}")
            
            initialization_time = time.time() - start_time
            self.is_initialized = success_count > 0
            self.current_status = ProcessingStatus.IDLE if self.is_initialized else ProcessingStatus.FAILED
            
            if self.is_initialized:
                self.logger.info(f"ğŸ‰ ì™„ì „ í†µí•© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ ({initialization_time:.2f}ì´ˆ)")
                self.logger.info(f"ğŸ“Š Step ì´ˆê¸°í™”: {success_count}/{len(self.step_order)} ({success_rate:.1%})")
                self.logger.info(f"ğŸ§  ModelLoader: {'âœ…' if model_success else 'âŒ'}")
                self.logger.info(f"ğŸ”— í†µí•© ì‹œìŠ¤í…œ: {'âœ…' if unified_success else 'âŒ'}")
                self.logger.info(f"ğŸ’‰ ì˜ì¡´ì„± ì£¼ì…: {'âœ…' if self.config.use_dependency_injection else 'âŒ'}")
            else:
                self.logger.error("âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            return self.is_initialized
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            self.current_status = ProcessingStatus.FAILED
            return False
    
    async def _initialize_steps_with_di_and_ai(self) -> int:
        """Step í´ë˜ìŠ¤ë“¤ ì´ˆê¸°í™” + DI + AI ëª¨ë¸ ì™„ì „ ì—°ë™"""
        try:
            if not STEP_CLASSES_AVAILABLE:
                self.logger.error("âŒ Step í´ë˜ìŠ¤ë“¤ ì‚¬ìš© ë¶ˆê°€")
                return 0
            
            # ê¸°ë³¸ ì„¤ì •
            base_config = {
                'device': self.device,
                'device_type': self.config.device_type,
                'memory_gb': self.config.memory_gb,
                'is_m3_max': self.config.is_m3_max,
                'optimization_enabled': True,
                'quality_level': self.config.quality_level.value
            }
            
            success_count = 0
            
            # Stepë³„ ë³‘ë ¬ ì´ˆê¸°í™” (ì„±ëŠ¥ ìµœì í™”)
            tasks = []
            for step_name in self.step_order:
                step_class_name = f"{step_name.title().replace('_', '')}Step"
                if step_class_name in STEP_CLASSES:
                    task = self._initialize_single_step_with_di(
                        step_name, 
                        STEP_CLASSES[step_class_name], 
                        base_config
                    )
                    tasks.append(task)
            
            # ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                step_name = self.step_order[i] if i < len(self.step_order) else f"step_{i}"
                
                if isinstance(result, Exception):
                    self.logger.error(f"âŒ {step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {result}")
                elif result:
                    success_count += 1
                    self.logger.info(f"âœ… {step_name} ì´ˆê¸°í™” + DI + AI ì—°ë™ ì™„ë£Œ")
                else:
                    self.logger.warning(f"âš ï¸ {step_name} ì´ˆê¸°í™” ë¶€ë¶„ ì‹¤íŒ¨")
            
            # DI í†µê³„ ì—…ë°ì´íŠ¸
            self.performance_metrics['di_injection_count'] = success_count
            self.performance_metrics['di_success_rate'] = (success_count / len(self.step_order)) * 100
            
            return success_count
            
        except Exception as e:
            self.logger.error(f"âŒ Step ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return 0
    
    async def _initialize_single_step_with_di(self, step_name: str, step_class, base_config: Dict[str, Any]) -> bool:
        """ë‹¨ì¼ Step ì´ˆê¸°í™” + DI + AI ì—°ë™"""
        try:
            # Step ì„¤ì • ì¤€ë¹„
            step_config = {**base_config, **self._get_step_config(step_name)}
            
            # ğŸ”¥ DIë¥¼ ì‚¬ìš©í•œ Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            if self.config.use_dependency_injection and DI_CONTAINER_AVAILABLE and self.di_container:
                try:
                    step_instance = create_step_with_di(step_class, **step_config)
                    self.logger.debug(f"âœ… {step_name} DI ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} DI ìƒì„± ì‹¤íŒ¨: {e}, í´ë°± ëª¨ë“œ")
                    step_instance = self._create_step_instance_safely(step_class, step_name, step_config)
            else:
                # ì¼ë°˜ ìƒì„± (í´ë°±)
                step_instance = self._create_step_instance_safely(step_class, step_name, step_config)
            
            if not step_instance:
                return False
            
            # ğŸ”¥ AI ì—°ë™ ì„¤ì • (í•µì‹¬)
            if self.ai_connector:
                await self.ai_connector.setup_step_ai_connection(step_instance, step_name)
            
            # ğŸ”¥ ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… (DI Containerê°€ ìˆì„ ê²½ìš°)
            if self.di_container and hasattr(step_instance, 'inject_dependencies'):
                inject_dependencies_to_step(step_instance, self.di_container)
            
            # Step ì´ˆê¸°í™”
            if hasattr(step_instance, 'initialize'):
                if asyncio.iscoroutinefunction(step_instance.initialize):
                    await step_instance.initialize()
                else:
                    step_instance.initialize()
            elif hasattr(step_instance, 'initialize_step'):
                if asyncio.iscoroutinefunction(step_instance.initialize_step):
                    await step_instance.initialize_step()
                else:
                    step_instance.initialize_step()
            
            self.steps[step_name] = step_instance
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ë‹¨ì¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _create_step_instance_safely(self, step_class, step_name: str, step_config: Dict[str, Any]):
        """Step ì¸ìŠ¤í„´ìŠ¤ ì•ˆì „ ìƒì„±"""
        try:
            return step_class(**step_config)
        except TypeError as e:
            if "unexpected keyword argument" in str(e):
                try:
                    safe_config = {
                        'device': step_config.get('device', 'cpu'),
                        'config': step_config.get('config', {})
                    }
                    return step_class(**safe_config)
                except Exception:
                    try:
                        return step_class(device=step_config.get('device', 'cpu'))
                    except Exception:
                        return None
            else:
                raise
        except Exception as e:
            self.logger.error(f"âŒ {step_name} ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _get_step_config(self, step_name: str) -> Dict[str, Any]:
        """Stepë³„ ìµœì í™”ëœ ì„¤ì •"""
        configs = {
            'human_parsing': {
                'model_name': 'graphonomy',
                'num_classes': 20,
                'input_size': (512, 512),
                'enable_ai_model': True
            },
            'pose_estimation': {
                'model_type': 'mediapipe',
                'input_size': (368, 368),
                'confidence_threshold': 0.5,
                'enable_ai_model': True
            },
            'cloth_segmentation': {
                'model_name': 'u2net',
                'background_threshold': 0.5,
                'post_process': True,
                'enable_ai_model': True
            },
            'geometric_matching': {
                'tps_points': 25,
                'matching_threshold': 0.8,
                'method': 'ai_enhanced'
            },
            'cloth_warping': {
                'warping_method': 'ai_physics',
                'physics_simulation': True,
                'enable_ai_model': True
            },
            'virtual_fitting': {
                'model_name': 'ootdiffusion',
                'blending_method': 'ai_poisson',
                'seamless_cloning': True,
                'enable_ai_model': True
            },
            'post_processing': {
                'model_name': 'esrgan',
                'enable_super_resolution': True,
                'enhance_faces': True,
                'enable_ai_model': True
            },
            'quality_assessment': {
                'model_name': 'clipiqa',
                'enable_detailed_analysis': True,
                'perceptual_metrics': True,
                'enable_ai_model': True
            }
        }
        
        return configs.get(step_name, {})
    
    # ==============================================
    # ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ - DI + AI ëª¨ë¸ ì™„ì „ ì—°ë™
    # ==============================================
    
    async def process_complete_virtual_fitting(
        self,
        person_image: Union[str, Image.Image, np.ndarray],
        clothing_image: Union[str, Image.Image, np.ndarray],
        body_measurements: Optional[Dict[str, float]] = None,
        clothing_type: str = "shirt",
        fabric_type: str = "cotton",
        style_preferences: Optional[Dict[str, Any]] = None,
        quality_target: float = 0.8,
        progress_callback: Optional[Callable] = None,
        save_intermediate: bool = False,
        session_id: Optional[str] = None
    ) -> ProcessingResult:
        """
        ğŸ”¥ ì™„ì „ í†µí•© 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ - DI + AI ëª¨ë¸ ì™„ì „ ì—°ë™
        
        âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ í•´ê²°
        âœ… Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš©
        âœ… 2ë‹¨ê³„ í´ë°± ì „ëµ
        âœ… M3 Max ì„±ëŠ¥ ìµœì í™”
        âœ… ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        """
        if not self.is_initialized:
            raise RuntimeError("íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        if session_id is None:
            session_id = f"di_ai_vf_{int(time.time())}_{np.random.randint(1000, 9999)}"
        
        start_time = time.time()
        self.current_status = ProcessingStatus.PROCESSING
        
        try:
            self.logger.info(f"ğŸ¯ ì™„ì „ í†µí•© 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì‹œì‘ - ì„¸ì…˜: {session_id}")
            self.logger.info(f"âš™ï¸ ì„¤ì •: {clothing_type} ({fabric_type}), ëª©í‘œ í’ˆì§ˆ: {quality_target}")
            self.logger.info(f"ğŸ§  AI ëª¨ë¸: {'âœ…' if self.config.ai_model_enabled else 'âŒ'}")
            self.logger.info(f"ğŸš€ M3 Max: {'âœ…' if self.config.is_m3_max else 'âŒ'}")
            self.logger.info(f"ğŸ”— ì˜ì¡´ì„± ì£¼ì…: {'âœ…' if self.config.use_dependency_injection else 'âŒ'}")
            
            # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ìµœì í™”)
            person_tensor = self.data_converter.preprocess_image(person_image)
            clothing_tensor = self.data_converter.preprocess_image(clothing_image)
            
            if progress_callback:
                await progress_callback("ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ", 5)
            
            # 2. ë©”ëª¨ë¦¬ ìµœì í™”
            self.memory_manager.cleanup_memory()
            
            # ğŸ”¥ 3. 8ë‹¨ê³„ ìˆœì°¨ ì²˜ë¦¬ - DI + AI ëª¨ë¸ ì™„ì „ í™œìš©
            step_results = {}
            execution_strategies = {}
            ai_models_used = {}
            di_injection_info = {}
            current_data = person_tensor
            
            for i, step_name in enumerate(self.step_order):
                if step_name not in self.steps:
                    self.logger.warning(f"âš ï¸ {step_name} ë‹¨ê³„ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆë›°ê¸°...")
                    continue
                
                step_start = time.time()
                step = self.steps[step_name]
                
                self.logger.info(f"ğŸ“‹ {i+1}/{len(self.step_order)} ë‹¨ê³„: {step_name} DI+AI ì²˜ë¦¬ ì¤‘...")
                
                try:
                    # ğŸ”¥ DI ì •ë³´ ìˆ˜ì§‘
                    di_info = self._collect_di_info(step, step_name)
                    di_injection_info[step_name] = di_info
                    
                    # ğŸ”¥ ìµœì í™”ëœ 2ë‹¨ê³„ í´ë°± ì‹¤í–‰
                    step_result, execution_strategy = await self.execution_manager.execute_step_optimized(
                        step, step_name, current_data, clothing_tensor,
                        body_measurements=body_measurements,
                        clothing_type=clothing_type,
                        fabric_type=fabric_type,
                        style_preferences=style_preferences,
                        quality_target=quality_target
                    )
                    
                    step_time = time.time() - step_start
                    step_results[step_name] = step_result
                    execution_strategies[step_name] = execution_strategy
                    
                    # AI ëª¨ë¸ ì‚¬ìš© ì¶”ì 
                    ai_model_name = step_result.get('ai_model_name', 'unknown')
                    ai_models_used[step_name] = ai_model_name
                    
                    # ê²°ê³¼ ì—…ë°ì´íŠ¸
                    if step_result.get('success', True):
                        result_data = step_result.get('result')
                        if result_data is not None:
                            current_data = result_data
                    
                    # ë¡œê¹…
                    confidence = step_result.get('confidence', 0.8)
                    quality_score = step_result.get('quality_score', confidence)
                    model_used = step_result.get('model_used', 'unknown')
                    
                    # ì „ëµë³„ ì•„ì´ì½˜
                    if execution_strategy == ExecutionStrategy.UNIFIED_AI.value:
                        strategy_icon = "ğŸ”—ğŸ§ "
                    elif execution_strategy == ExecutionStrategy.MODEL_LOADER.value:
                        strategy_icon = "ğŸ§ ğŸ“¦"
                    else:
                        strategy_icon = "ğŸ”„"
                    
                    # DI ì•„ì´ì½˜
                    di_icon = "ğŸ’‰" if di_info.get('has_injected_dependencies', False) else "ğŸ”§"
                    
                    self.logger.info(f"âœ… {i+1}ë‹¨ê³„ ì™„ë£Œ - ì‹œê°„: {step_time:.2f}ì´ˆ, ì‹ ë¢°ë„: {confidence:.3f}, í’ˆì§ˆ: {quality_score:.3f}")
                    self.logger.info(f"   {strategy_icon} ì „ëµ: {execution_strategy}, AIëª¨ë¸: {ai_model_name}, ì²˜ë¦¬: {model_used}")
                    self.logger.info(f"   {di_icon} DI: {di_info.get('injection_summary', 'None')}")
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if progress_callback:
                        progress = 5 + (i + 1) * 85 // len(self.step_order)
                        await progress_callback(f"{step_name} DI+AI ì²˜ë¦¬ ì™„ë£Œ", progress)
                    
                    # ğŸ”¥ M3 Max ë©”ëª¨ë¦¬ ìµœì í™” (ì¤‘ê°„ ë‹¨ê³„ë§ˆë‹¤)
                    if self.config.is_m3_max and i % 2 == 0:
                        self.memory_manager.cleanup_memory()
                    
                except Exception as e:
                    self.logger.error(f"âŒ {i+1}ë‹¨ê³„ ({step_name}) ì‹¤íŒ¨: {e}")
                    step_time = time.time() - step_start
                    step_results[step_name] = {
                        'success': False,
                        'error': str(e),
                        'processing_time': step_time,
                        'confidence': 0.0,
                        'quality_score': 0.0,
                        'model_used': 'error',
                        'ai_model_name': 'error'
                    }
                    execution_strategies[step_name] = "error"
                    ai_models_used[step_name] = "error"
                    di_injection_info[step_name] = {'error': str(e)}
                    
                    # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                    continue
            
            # 4. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            total_time = time.time() - start_time
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
            if isinstance(current_data, torch.Tensor):
                result_image = self.data_converter.tensor_to_pil(current_data)
            else:
                result_image = Image.new('RGB', (512, 512), color='gray')
            
            # ğŸ”¥ ê°•í™”ëœ í’ˆì§ˆ í‰ê°€ (AI ëª¨ë¸ + DI ì‚¬ìš© ê³ ë ¤)
            quality_score = self._assess_enhanced_quality(step_results, execution_strategies, ai_models_used, di_injection_info)
            quality_grade = self._get_quality_grade(quality_score)
            
            # ì„±ê³µ ì—¬ë¶€ ê²°ì •
            success = quality_score >= (quality_target * 0.8)
            
            # ğŸ”¥ AI ëª¨ë¸ + DI ì‚¬ìš© í†µê³„
            ai_stats = self._calculate_ai_usage_statistics(ai_models_used, execution_strategies)
            di_stats = self._calculate_di_usage_statistics(di_injection_info)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_performance_metrics(total_time, quality_score, success, ai_stats, di_stats)
            
            if progress_callback:
                await progress_callback("DI+AI ì²˜ë¦¬ ì™„ë£Œ", 100)
            
            self.current_status = ProcessingStatus.IDLE
            
            # ğŸ”¥ ì™„ì „ í†µí•© ê²°ê³¼ ë¡œê¹…
            self.logger.info(f"ğŸ‰ ì™„ì „ í†µí•© 8ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì™„ë£Œ!")
            self.logger.info(f"â±ï¸ ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
            self.logger.info(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f} ({quality_grade})")
            self.logger.info(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {'âœ…' if quality_score >= quality_target else 'âŒ'}")
            self.logger.info(f"ğŸ“‹ ì™„ë£Œëœ ë‹¨ê³„: {len(step_results)}/{len(self.step_order)}")
            self.logger.info(f"ğŸ§  AI ëª¨ë¸ ì‚¬ìš©ë¥ : {ai_stats['ai_usage_rate']:.1f}%")
            self.logger.info(f"ğŸ”— í†µí•© AI ì‚¬ìš©: {ai_stats['unified_ai_count']}íšŒ")
            self.logger.info(f"ğŸ“¦ ModelLoader ì‚¬ìš©: {ai_stats['model_loader_count']}íšŒ")
            self.logger.info(f"ğŸ’‰ DI ì£¼ì…ë¥ : {di_stats['injection_rate']:.1f}%")
            self.logger.info(f"ğŸ”§ DI ì„±ê³µë¥ : {di_stats['success_rate']:.1f}%")
            
            return ProcessingResult(
                success=success,
                session_id=session_id,
                result_image=result_image,
                result_tensor=current_data if isinstance(current_data, torch.Tensor) else None,
                quality_score=quality_score,
                quality_grade=quality_grade,
                processing_time=total_time,
                step_results=step_results,
                step_timings={step: result.get('execution_time', 0.0) for step, result in step_results.items()},
                ai_models_used=ai_models_used,
                execution_strategies=execution_strategies,
                dependency_injection_info=di_injection_info,
                performance_metrics={
                    'ai_usage_statistics': ai_stats,
                    'di_usage_statistics': di_stats,
                    'memory_peak_usage': self._get_memory_peak_usage(),
                    'step_performance': self._get_step_performance_metrics(step_results)
                },
                metadata={
                    'device': self.device,
                    'device_type': self.config.device_type,
                    'is_m3_max': self.config.is_m3_max,
                    'memory_gb': self.config.memory_gb,
                    'ai_model_enabled': self.config.ai_model_enabled,
                    'use_dependency_injection': self.config.use_dependency_injection,
                    'quality_target': quality_target,
                    'quality_target_achieved': quality_score >= quality_target,
                    'total_steps': len(self.step_order),
                    'completed_steps': len(step_results),
                    'success_rate': len([r for r in step_results.values() if r.get('success', True)]) / len(step_results) if step_results else 0,
                    'integration_summary': {
                        'di_container_used': self.di_container is not None,
                        'ai_models_connected': sum(1 for model in ai_models_used.values() if model not in ['error', 'unknown']),
                        'di_injections_performed': sum(1 for info in di_injection_info.values() if info.get('has_injected_dependencies', False)),
                        'circular_import_resolved': True,
                        'architecture_version': 'v8.0_complete_integration'
                    }
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ ì™„ì „ í†µí•© ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            
            # ì—ëŸ¬ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_performance_metrics(time.time() - start_time, 0.0, False, {}, {})
            
            self.current_status = ProcessingStatus.FAILED
            
            return ProcessingResult(
                success=False,
                session_id=session_id,
                processing_time=time.time() - start_time,
                error_message=str(e),
                metadata={
                    'device': self.device,
                    'error_type': type(e).__name__,
                    'error_location': traceback.format_exc(),
                    'ai_model_enabled': self.config.ai_model_enabled,
                    'use_dependency_injection': self.config.use_dependency_injection,
                    'is_m3_max': self.config.is_m3_max,
                    'architecture_version': 'v8.0_complete_integration'
                }
            )
    
    def _collect_di_info(self, step_instance, step_name: str) -> Dict[str, Any]:
        """Stepì˜ DI ì •ë³´ ìˆ˜ì§‘"""
        try:
            di_info = {
                'step_name': step_name,
                'has_injected_dependencies': False,
                'injected_count': 0,
                'available_interfaces': [],
                'injection_summary': 'None'
            }
            
            # ì£¼ì…ëœ ì˜ì¡´ì„± í™•ì¸
            dependencies = []
            
            if hasattr(step_instance, 'model_loader') and step_instance.model_loader:
                dependencies.append('model_loader')
            if hasattr(step_instance, 'memory_manager') and step_instance.memory_manager:
                dependencies.append('memory_manager')
            if hasattr(step_instance, 'data_converter') and step_instance.data_converter:
                dependencies.append('data_converter')
            if hasattr(step_instance, 'unified_interface') and step_instance.unified_interface:
                dependencies.append('unified_interface')
            if hasattr(step_instance, 'model_interface') and step_instance.model_interface:
                dependencies.append('model_interface')
            
            di_info['has_injected_dependencies'] = len(dependencies) > 0
            di_info['injected_count'] = len(dependencies)
            di_info['available_interfaces'] = dependencies
            di_info['injection_summary'] = ', '.join(dependencies) if dependencies else 'None'
            
            return di_info
            
        except Exception as e:
            return {
                'step_name': step_name,
                'error': str(e),
                'has_injected_dependencies': False
            }
    
    def _assess_enhanced_quality(self, step_results: Dict[str, Any], execution_strategies: Dict[str, str], 
                               ai_models_used: Dict[str, str], di_injection_info: Dict[str, Any]) -> float:
        """AI ëª¨ë¸ + DI ì‚¬ìš©ì„ ê³ ë ¤í•œ ê°•í™”ëœ í’ˆì§ˆ í‰ê°€"""
        if not step_results:
            return 0.5
        
        quality_scores = []
        confidence_scores = []
        ai_bonus = 0.0
        di_bonus = 0.0
        
        for step_name, step_result in step_results.items():
            if isinstance(step_result, dict):
                confidence = step_result.get('confidence', 0.8)
                quality = step_result.get('quality_score', confidence)
                strategy = execution_strategies.get(step_name, 'unknown')
                ai_model = ai_models_used.get(step_name, 'unknown')
                di_info = di_injection_info.get(step_name, {})
                
                quality_scores.append(quality)
                confidence_scores.append(confidence)
                
                # ğŸ”¥ AI ëª¨ë¸ ì‚¬ìš©ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
                if ai_model not in ['error', 'unknown', 'fallback_processing', 'step_processing']:
                    if strategy == ExecutionStrategy.UNIFIED_AI.value:
                        ai_bonus += 0.08  # í†µí•© AI: 8% ë³´ë„ˆìŠ¤
                    elif strategy == ExecutionStrategy.MODEL_LOADER.value:
                        ai_bonus += 0.05  # ModelLoader: 5% ë³´ë„ˆìŠ¤
                    else:
                        ai_bonus += 0.02  # ê¸°íƒ€: 2% ë³´ë„ˆìŠ¤
                
                # ğŸ”¥ DI ì‚¬ìš©ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
                if di_info.get('has_injected_dependencies', False):
                    injected_count = di_info.get('injected_count', 0)
                    di_bonus += min(injected_count * 0.015, 0.06)  # ìµœëŒ€ 6% ë³´ë„ˆìŠ¤
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        if quality_scores:
            avg_quality = sum(quality_scores) / len(quality_scores)
            avg_confidence = sum(confidence_scores) / len(confidence_scores)
            
            # ê°€ì¤‘ í‰ê·  + AI ë³´ë„ˆìŠ¤ + DI ë³´ë„ˆìŠ¤
            overall_score = avg_quality * 0.7 + avg_confidence * 0.3 + ai_bonus + di_bonus
            return min(max(overall_score, 0.0), 1.0)
        
        return 0.5
    
    def _calculate_ai_usage_statistics(self, ai_models_used: Dict[str, str], 
                                     execution_strategies: Dict[str, str]) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì‚¬ìš© í†µê³„ ê³„ì‚°"""
        total_steps = len(ai_models_used)
        
        # ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš© íšŸìˆ˜
        real_ai_count = sum(1 for model in ai_models_used.values() 
                           if model not in ['error', 'unknown', 'fallback_processing', 'step_processing'])
        
        # ì „ëµë³„ í†µê³„
        unified_ai_count = sum(1 for strategy in execution_strategies.values() 
                              if strategy == ExecutionStrategy.UNIFIED_AI.value)
        model_loader_count = sum(1 for strategy in execution_strategies.values() 
                               if strategy == ExecutionStrategy.MODEL_LOADER.value)
        fallback_count = sum(1 for strategy in execution_strategies.values() 
                           if strategy == ExecutionStrategy.BASIC_FALLBACK.value)
        
        return {
            'total_steps': total_steps,
            'real_ai_count': real_ai_count,
            'ai_usage_rate': (real_ai_count / total_steps * 100) if total_steps > 0 else 0,
            'unified_ai_count': unified_ai_count,
            'model_loader_count': model_loader_count,
            'fallback_count': fallback_count,
            'unified_ai_rate': (unified_ai_count / total_steps * 100) if total_steps > 0 else 0,
            'model_loader_rate': (model_loader_count / total_steps * 100) if total_steps > 0 else 0,
            'fallback_rate': (fallback_count / total_steps * 100) if total_steps > 0 else 0,
            'unique_ai_models': list(set(ai_models_used.values()) - {'error', 'unknown', 'fallback_processing', 'step_processing'})
        }
    
    def _calculate_di_usage_statistics(self, di_injection_info: Dict[str, Any]) -> Dict[str, Any]:
        """DI ì‚¬ìš© í†µê³„ ê³„ì‚°"""
        total_steps = len(di_injection_info)
        
        # DI ì£¼ì… í†µê³„
        injected_steps = sum(1 for info in di_injection_info.values() 
                           if info.get('has_injected_dependencies', False))
        
        total_injections = sum(info.get('injected_count', 0) for info in di_injection_info.values())
        
        # ì¸í„°í˜ì´ìŠ¤ë³„ ì‚¬ìš© í†µê³„
        interface_counts = {}
        for info in di_injection_info.values():
            for interface in info.get('available_interfaces', []):
                interface_counts[interface] = interface_counts.get(interface, 0) + 1
        
        return {
            'total_steps': total_steps,
            'injected_steps': injected_steps,
            'injection_rate': (injected_steps / total_steps * 100) if total_steps > 0 else 0,
            'total_injections': total_injections,
            'average_injections_per_step': total_injections / total_steps if total_steps > 0 else 0,
            'success_rate': (injected_steps / total_steps * 100) if total_steps > 0 else 0,
            'interface_usage': interface_counts,
            'most_used_interface': max(interface_counts.items(), key=lambda x: x[1])[0] if interface_counts else 'none'
        }
    
    def _update_performance_metrics(self, processing_time: float, quality_score: float, 
                                   success: bool, ai_stats: Dict[str, Any], di_stats: Dict[str, Any]):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (DI ì •ë³´ í¬í•¨)"""
        self.performance_metrics['total_sessions'] += 1
        
        if success:
            self.performance_metrics['successful_sessions'] += 1
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        total_sessions = self.performance_metrics['total_sessions']
        prev_avg_time = self.performance_metrics['average_processing_time']
        self.performance_metrics['average_processing_time'] = (
            (prev_avg_time * (total_sessions - 1) + processing_time) / total_sessions
        )
        
        # í‰ê·  í’ˆì§ˆ ì ìˆ˜ ì—…ë°ì´íŠ¸
        if success:
            successful_sessions = self.performance_metrics['successful_sessions']
            prev_avg_quality = self.performance_metrics['average_quality_score']
            self.performance_metrics['average_quality_score'] = (
                (prev_avg_quality * (successful_sessions - 1) + quality_score) / successful_sessions
            )
        
        # AI ëª¨ë¸ ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸
        if ai_stats:
            for model in ai_stats.get('unique_ai_models', []):
                self.performance_metrics['ai_model_usage'][model] = (
                    self.performance_metrics['ai_model_usage'].get(model, 0) + 1
                )
        
        # DI í†µê³„ ì—…ë°ì´íŠ¸
        if di_stats:
            self.performance_metrics['di_injection_count'] += di_stats.get('total_injections', 0)
            if total_sessions > 0:
                self.performance_metrics['di_success_rate'] = (
                    self.performance_metrics['di_injection_count'] / (total_sessions * len(self.step_order)) * 100
                )
    
    def _get_memory_peak_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ í”¼í¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            memory_info = {}
            
            # CPU ë©”ëª¨ë¦¬
            if psutil:
                process = psutil.Process()
                memory_info['cpu_memory_gb'] = process.memory_info().rss / (1024**3)
                
                system_memory = psutil.virtual_memory()
                memory_info['system_memory_percent'] = system_memory.percent
                memory_info['system_memory_available_gb'] = system_memory.available / (1024**3)
            
            # GPU ë©”ëª¨ë¦¬
            if self.device == 'cuda' and torch.cuda.is_available():
                memory_info['gpu_allocated_gb'] = torch.cuda.memory_allocated() / (1024**3)
                memory_info['gpu_reserved_gb'] = torch.cuda.memory_reserved() / (1024**3)
            elif self.device == 'mps':
                # MPS ë©”ëª¨ë¦¬ëŠ” ì§ì ‘ ì¡°íšŒ ì–´ë ¤ì›€
                memory_info['gpu_type'] = 'mps'
            
            return memory_info
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _get_step_performance_metrics(self, step_results: Dict[str, Any]) -> Dict[str, Any]:
        """Stepë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
        metrics = {}
        
        for step_name, result in step_results.items():
            if isinstance(result, dict):
                metrics[step_name] = {
                    'success': result.get('success', False),
                    'execution_time': result.get('execution_time', 0.0),
                    'confidence': result.get('confidence', 0.0),
                    'quality_score': result.get('quality_score', 0.0),
                    'ai_model_used': result.get('ai_model_name', 'unknown')
                }
        
        return metrics
    
    def _get_quality_grade(self, quality_score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if quality_score >= 0.95:
            return "Excellent+"
        elif quality_score >= 0.9:
            return "Excellent"
        elif quality_score >= 0.8:
            return "Good"
        elif quality_score >= 0.7:
            return "Fair"
        elif quality_score >= 0.6:
            return "Poor"
        else:
            return "Very Poor"
    
    # ==============================================
    # ğŸ”¥ ìƒíƒœ ì¡°íšŒ ë° ê´€ë¦¬ ë©”ì„œë“œë“¤ (DI ì •ë³´ í¬í•¨)
    # ==============================================
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ - AI ëª¨ë¸ + DI ì •ë³´ í¬í•¨"""
        return {
            'initialized': self.is_initialized,
            'current_status': self.current_status.value,
            'device': self.device,
            'device_type': self.config.device_type,
            'memory_gb': self.config.memory_gb,
            'is_m3_max': self.config.is_m3_max,
            'ai_model_enabled': self.config.ai_model_enabled,
            'use_dependency_injection': self.config.use_dependency_injection,
            'architecture_version': 'v8.0_complete_integration',
            'model_loader_initialized': self.model_manager.is_initialized,
            'unified_system_initialized': self.unified_manager.is_initialized,
            'di_container_available': self.di_container is not None,
            'config': {
                'quality_level': self.config.quality_level.value,
                'processing_mode': self.config.processing_mode.value,
                'performance_mode': self.config.performance_mode,
                'ai_model_enabled': self.config.ai_model_enabled,
                'model_preload_enabled': self.config.model_preload_enabled,
                'use_dependency_injection': self.config.use_dependency_injection,
                'auto_inject_dependencies': self.config.auto_inject_dependencies,
                'lazy_loading_enabled': self.config.lazy_loading_enabled,
                'interface_based_design': self.config.interface_based_design,
                'model_cache_size': self.config.model_cache_size,
                'max_fallback_attempts': self.config.max_fallback_attempts,
                'memory_optimization': self.config.memory_optimization,
                'parallel_processing': self.config.parallel_processing,
                'batch_size': self.config.batch_size,
                'thread_pool_size': self.config.thread_pool_size
            },
            'steps_status': {
                step_name: {
                    'loaded': step_name in self.steps,
                    'type': type(self.steps[step_name]).__name__ if step_name in self.steps else None,
                    'ready': step_name in self.steps and hasattr(self.steps[step_name], 'process'),
                    'has_unified_interface': (step_name in self.steps and 
                                            hasattr(self.steps[step_name], 'unified_interface') and 
                                            getattr(self.steps[step_name], 'unified_interface', None) is not None),
                    'has_model_interface': (step_name in self.steps and 
                                          hasattr(self.steps[step_name], 'model_interface') and 
                                          getattr(self.steps[step_name], 'model_interface', None) is not None),
                    'has_ai_model': (step_name in self.steps and 
                                   hasattr(self.steps[step_name], '_ai_model') and 
                                   getattr(self.steps[step_name], '_ai_model', None) is not None),
                    'ai_model_name': getattr(self.steps.get(step_name), '_ai_model_name', 'unknown') if step_name in self.steps else 'unknown',
                    'di_injected': (step_name in self.steps and 
                                   hasattr(self.steps[step_name], 'model_loader') and 
                                   getattr(self.steps[step_name], 'model_loader', None) is not None)
                }
                for step_name in self.step_order
            },
            'ai_model_status': {
                'loaded_models': len(self.model_manager.loaded_models) if self.model_manager else 0,
                'model_interfaces': len(self.model_manager.model_interfaces) if self.model_manager else 0,
                'ai_connector_ready': self.ai_connector is not None
            },
            'dependency_injection_status': {
                'di_container_initialized': self.di_container is not None,
                'di_injection_count': self.performance_metrics.get('di_injection_count', 0),
                'di_success_rate': self.performance_metrics.get('di_success_rate', 0.0),
                'interfaces_available': INTERFACES_AVAILABLE,
                'circular_import_resolved': True
            },
            'performance_metrics': self.performance_metrics,
            'memory_usage': self._get_memory_peak_usage(),
            'system_integration': {
                'unified_utils_available': UNIFIED_UTILS_AVAILABLE,
                'model_loader_available': MODEL_LOADER_AVAILABLE,
                'step_requests_available': STEP_REQUESTS_AVAILABLE,
                'auto_detector_available': AUTO_DETECTOR_AVAILABLE,
                'step_classes_available': STEP_CLASSES_AVAILABLE,
                'di_container_available': DI_CONTAINER_AVAILABLE,
                'interfaces_available': INTERFACES_AVAILABLE
            }
        }
    
    def get_ai_model_summary(self) -> Dict[str, Any]:
        """AI ëª¨ë¸ ìš”ì•½ ì •ë³´ (DI ì •ë³´ í¬í•¨)"""
        summary = {
            'total_loaded_models': 0,
            'models_by_step': {},
            'model_usage_stats': self.performance_metrics.get('ai_model_usage', {}),
            'model_performance': {},
            'di_integration': {
                'steps_with_di': 0,
                'total_injections': 0,
                'injection_success_rate': 0.0
            }
        }
        
        if self.model_manager and self.model_manager.is_initialized:
            summary['total_loaded_models'] = len(self.model_manager.loaded_models)
        
        # Stepë³„ AI ëª¨ë¸ + DI ì •ë³´
        steps_with_di = 0
        total_injections = 0
        
        for step_name in self.step_order:
            if step_name in self.steps:
                step = self.steps[step_name]
                
                # AI ëª¨ë¸ ì •ë³´
                has_ai_model = hasattr(step, '_ai_model') and step._ai_model is not None
                ai_model_name = getattr(step, '_ai_model_name', 'unknown')
                has_interface = hasattr(step, 'model_interface') and step.model_interface is not None
                
                # DI ì •ë³´
                has_model_loader = hasattr(step, 'model_loader') and step.model_loader is not None
                has_memory_manager = hasattr(step, 'memory_manager') and step.memory_manager is not None
                has_data_converter = hasattr(step, 'data_converter') and step.data_converter is not None
                
                di_count = sum([has_model_loader, has_memory_manager, has_data_converter])
                if di_count > 0:
                    steps_with_di += 1
                    total_injections += di_count
                
                summary['models_by_step'][step_name] = {
                    'has_ai_model': has_ai_model,
                    'ai_model_name': ai_model_name,
                    'has_interface': has_interface,
                    'di_injections': {
                        'model_loader': has_model_loader,
                        'memory_manager': has_memory_manager,
                        'data_converter': has_data_converter,
                        'total_count': di_count
                    }
                }
        
        # DI í†µê³„ ì—…ë°ì´íŠ¸
        summary['di_integration']['steps_with_di'] = steps_with_di
        summary['di_integration']['total_injections'] = total_injections
        if len(self.step_order) > 0:
            summary['di_integration']['injection_success_rate'] = (steps_with_di / len(self.step_order)) * 100
        
        return summary
    
    async def warmup(self):
        """íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… - DI + AI ëª¨ë¸ í¬í•¨"""
        try:
            self.logger.info("ğŸ”¥ ì™„ì „ í†µí•© íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… ì‹œì‘ (DI + AI)...")
            
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
            dummy_person = Image.new('RGB', (512, 512), color=(100, 150, 200))
            dummy_cloth = Image.new('RGB', (512, 512), color=(200, 100, 100))
            
            # ì›Œë°ì—… ì‹¤í–‰
            result = await self.process_complete_virtual_fitting(
                person_image=dummy_person,
                clothing_image=dummy_cloth,
                clothing_type='shirt',
                fabric_type='cotton',
                quality_target=0.6,
                save_intermediate=False,
                session_id="complete_warmup_session"
            )
            
            if result.success:
                ai_stats = result.performance_metrics.get('ai_usage_statistics', {})
                di_stats = result.performance_metrics.get('di_usage_statistics', {})
                
                self.logger.info(f"âœ… ì™„ì „ í†µí•© ì›Œë°ì—… ì™„ë£Œ - ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
                self.logger.info(f"ğŸ§  AI ëª¨ë¸ ì‚¬ìš©ë¥ : {ai_stats.get('ai_usage_rate', 0):.1f}%")
                self.logger.info(f"ğŸ”— í†µí•© AI ì‚¬ìš©: {ai_stats.get('unified_ai_count', 0)}íšŒ")
                self.logger.info(f"ğŸ“¦ ModelLoader ì‚¬ìš©: {ai_stats.get('model_loader_count', 0)}íšŒ")
                self.logger.info(f"ğŸ’‰ DI ì£¼ì…ë¥ : {di_stats.get('injection_rate', 0):.1f}%")
                self.logger.info(f"ğŸ”§ DI ì„±ê³µë¥ : {di_stats.get('success_rate', 0):.1f}%")
                self.logger.info(f"ğŸ¤– ì‚¬ìš©ëœ AI ëª¨ë¸: {', '.join(ai_stats.get('unique_ai_models', []))}")
                return True
            else:
                self.logger.warning(f"âš ï¸ ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜: {result.error_message}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return False
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - DI + AI ëª¨ë¸ í¬í•¨"""
        try:
            self.logger.info("ğŸ§¹ ì™„ì „ í†µí•© íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ (DI + AI)...")
            self.current_status = ProcessingStatus.CLEANING
            
            # 1. ê° Step ì •ë¦¬ (AI ëª¨ë¸ + DI í¬í•¨)
            for step_name, step in self.steps.items():
                try:
                    # AI ëª¨ë¸ ì •ë¦¬
                    if hasattr(step, '_ai_model'):
                        delattr(step, '_ai_model')
                    
                    # DI ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
                    if hasattr(step, 'model_loader'):
                        delattr(step, 'model_loader')
                    if hasattr(step, 'memory_manager'):
                        delattr(step, 'memory_manager')
                    if hasattr(step, 'data_converter'):
                        delattr(step, 'data_converter')
                    
                    # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
                    if hasattr(step, 'unified_interface'):
                        if hasattr(step.unified_interface, 'cleanup'):
                            await step.unified_interface.cleanup()
                    
                    if hasattr(step, 'model_interface'):
                        if hasattr(step.model_interface, 'unload_models'):
                            await step.model_interface.unload_models()
                    
                    # Step ìì²´ ì •ë¦¬
                    if hasattr(step, 'cleanup'):
                        if asyncio.iscoroutinefunction(step.cleanup):
                            await step.cleanup()
                        else:
                            step.cleanup()
                        
                    self.logger.info(f"âœ… {step_name} ì •ë¦¬ ì™„ë£Œ (DI + AI)")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {step_name} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 2. AI ì—°ë™ ê´€ë¦¬ì ì •ë¦¬
            self.ai_connector = None
            
            # 3. ModelLoader ì‹œìŠ¤í…œ ì •ë¦¬
            if self.model_manager and self.model_manager.model_loader:
                try:
                    if hasattr(self.model_manager.model_loader, 'cleanup'):
                        await self.model_manager.model_loader.cleanup()
                    self.logger.info("âœ… ModelLoader ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ModelLoader ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 4. í†µí•© ì‹œìŠ¤í…œ ì •ë¦¬
            if self.unified_manager and self.unified_manager.utils_manager:
                try:
                    if hasattr(self.unified_manager.utils_manager, 'cleanup'):
                        self.unified_manager.utils_manager.cleanup()
                    self.logger.info("âœ… í†µí•© ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í†µí•© ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 5. DI Container ì •ë¦¬
            if self.di_container:
                try:
                    if hasattr(self.di_container, 'clear'):
                        self.di_container.clear()
                    self.di_container = None
                    self.logger.info("âœ… DI Container ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ DI Container ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 6. ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                self.memory_manager.cleanup_memory()
                self.logger.info("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 7. ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            if hasattr(self, 'thread_pool'):
                try:
                    self.thread_pool.shutdown(wait=True)
                    self.logger.info("âœ… ìŠ¤ë ˆë“œ í’€ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œ í’€ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 8. ìƒíƒœ ì´ˆê¸°í™”
            self.is_initialized = False
            self.current_status = ProcessingStatus.IDLE
            
            self.logger.info("âœ… ì™„ì „ í†µí•© íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ (DI + AI)")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.current_status = ProcessingStatus.FAILED

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ (ì™„ì „ í†µí•© ë²„ì „)
# ==============================================

def create_pipeline(
    device: str = "auto", 
    quality_level: str = "balanced", 
    mode: str = "production",
    use_dependency_injection: bool = True,
    **kwargs
) -> PipelineManager:
    """
    ğŸ”¥ ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ - ì™„ì „ í†µí•© ë²„ì „
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ì„¤ì • ('auto', 'cpu', 'cuda', 'mps')
        quality_level: í’ˆì§ˆ ë ˆë²¨ ('fast', 'balanced', 'high', 'maximum')
        mode: ëª¨ë“œ ('development', 'production', 'testing', 'optimization')
        use_dependency_injection: ì˜ì¡´ì„± ì£¼ì… ì‚¬ìš© ì—¬ë¶€
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: ì´ˆê¸°í™”ëœ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return PipelineManager(
        device=device,
        config=PipelineConfig(
            quality_level=QualityLevel(quality_level),
            processing_mode=PipelineMode(mode),
            ai_model_enabled=True,
            use_dependency_injection=use_dependency_injection,
            **kwargs
        )
    )

def create_ai_optimized_pipeline(
    device: str = "auto",
    quality_level: str = "high",
    use_dependency_injection: bool = True,
    **kwargs
) -> PipelineManager:
    """AI ëª¨ë¸ ìµœì í™” íŒŒì´í”„ë¼ì¸ ìƒì„± - DI ì§€ì›"""
    return PipelineManager(
        device=device,
        config=PipelineConfig(
            quality_level=QualityLevel(quality_level),
            processing_mode=PipelineMode.PRODUCTION,
            ai_model_enabled=True,
            model_preload_enabled=True,
            model_cache_size=20,
            performance_mode="maximum",
            memory_optimization=True,
            parallel_processing=True,
            max_fallback_attempts=2,
            use_dependency_injection=use_dependency_injection,
            auto_inject_dependencies=True,
            lazy_loading_enabled=True,
            interface_based_design=True,
            **kwargs
        )
    )

def create_m3_max_pipeline(**kwargs) -> PipelineManager:
    """
    ğŸ”¥ M3 Max + AI ëª¨ë¸ + DI ì™„ì „ ìµœì í™” íŒŒì´í”„ë¼ì¸
    
    Args:
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: M3 Max ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return PipelineManager(
        device="mps",
        config=PipelineConfig(
            quality_level=QualityLevel.MAXIMUM,
            processing_mode=PipelineMode.PRODUCTION,
            memory_gb=128.0,
            is_m3_max=True,
            device_type="apple_silicon",
            ai_model_enabled=True,
            model_preload_enabled=True,
            model_cache_size=20,
            performance_mode="maximum",
            memory_optimization=True,
            gpu_memory_fraction=0.95,
            use_fp16=True,
            enable_quantization=True,
            parallel_processing=True,
            batch_processing=True,
            async_processing=True,
            batch_size=4,
            thread_pool_size=8,
            max_fallback_attempts=2,
            enable_smart_fallback=True,
            # ğŸ”¥ DI ì„¤ì •
            use_dependency_injection=True,
            auto_inject_dependencies=True,
            lazy_loading_enabled=True,
            interface_based_design=True,
            **kwargs
        )
    )

def create_production_pipeline(**kwargs) -> PipelineManager:
    """
    ğŸ”¥ í”„ë¡œë•ì…˜ìš© ì™„ì „ í†µí•© íŒŒì´í”„ë¼ì¸
    
    Args:
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: í”„ë¡œë•ì…˜ ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return create_ai_optimized_pipeline(
        quality_level="high",
        processing_mode="production",
        ai_model_enabled=True,
        model_preload_enabled=True,
        memory_optimization=True,
        parallel_processing=True,
        use_dependency_injection=True,
        **kwargs
    )

def create_development_pipeline(**kwargs) -> PipelineManager:
    """
    ğŸ”¥ ê°œë°œìš© ì™„ì „ í†µí•© íŒŒì´í”„ë¼ì¸
    
    Args:
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: ê°œë°œìš© íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return create_ai_optimized_pipeline(
        quality_level="balanced",
        processing_mode="development",
        ai_model_enabled=True,
        model_preload_enabled=False,
        memory_optimization=False,
        parallel_processing=False,
        use_dependency_injection=True,
        **kwargs
    )

def create_testing_pipeline(**kwargs) -> PipelineManager:
    """
    ğŸ”¥ í…ŒìŠ¤íŒ…ìš© íŒŒì´í”„ë¼ì¸ - ê¸°ë³¸ DI ì§€ì›
    
    Args:
        **kwargs: ì¶”ê°€ ì„¤ì • íŒŒë¼ë¯¸í„°
    
    Returns:
        PipelineManager: í…ŒìŠ¤íŒ…ìš© íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €
    """
    return PipelineManager(
        device="cpu",
        config=PipelineConfig(
            quality_level=QualityLevel.FAST,
            processing_mode=PipelineMode.TESTING,
            ai_model_enabled=False,
            model_preload_enabled=False,
            memory_optimization=False,
            parallel_processing=False,
            batch_size=1,
            thread_pool_size=2,
            use_dependency_injection=True,
            auto_inject_dependencies=False,
            **kwargs
        )
    )

@lru_cache(maxsize=1)
def get_global_pipeline_manager(device: str = "auto") -> PipelineManager:
    """
    ğŸ”¥ ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ - ì™„ì „ í†µí•© ë²„ì „
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
    
    Returns:
        PipelineManager: ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
    """
    try:
        if device == "mps" and torch.backends.mps.is_available():
            return create_m3_max_pipeline()
        else:
            return create_production_pipeline(device=device)
    except Exception as e:
        logger.error(f"ì „ì—­ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ìƒì„± ì‹¤íŒ¨: {e}")
        return create_ai_optimized_pipeline(device="cpu", quality_level="balanced")

# ==============================================
# ğŸ”¥ Export ë° ë©”ì¸ ì‹¤í–‰
# ==============================================

__all__ = [
    # ì—´ê±°í˜•
    'PipelineMode', 'QualityLevel', 'ProcessingStatus', 'ExecutionStrategy',
    
    # ë°ì´í„° í´ë˜ìŠ¤
    'PipelineConfig', 'ProcessingResult', 'AIModelInfo',
    
    # ë©”ì¸ í´ë˜ìŠ¤
    'PipelineManager',
    
    # ê´€ë¦¬ì í´ë˜ìŠ¤ë“¤
    'ModelLoaderManager', 'UnifiedSystemManager', 'StepAIConnector', 
    'OptimizedExecutionManager', 'PerformanceOptimizer',
    
    # ì–´ëŒ‘í„° í´ë˜ìŠ¤ë“¤
    'ModelLoaderAdapter', 'MemoryManagerAdapter',
    
    # ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (ì™„ì „ í†µí•© ë²„ì „)
    'create_pipeline',                    # âœ… ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ (DI ì§€ì›)
    'create_ai_optimized_pipeline',       # âœ… AI ìµœì í™” (DI ì§€ì›)
    'create_m3_max_pipeline',            # âœ… M3 Max ìµœì í™” (DI ì§€ì›)
    'create_production_pipeline',        # âœ… í”„ë¡œë•ì…˜ (DI ì§€ì›)
    'create_development_pipeline',       # âœ… ê°œë°œìš© (DI ì§€ì›)  
    'create_testing_pipeline',           # âœ… í…ŒìŠ¤íŒ… (DI ì§€ì›)
    'get_global_pipeline_manager'        # âœ… ì „ì—­ ë§¤ë‹ˆì € (DI ì§€ì›)
]

if __name__ == "__main__":
    print("ğŸ”¥ ì™„ì „ í†µí•© PipelineManager v8.0 - ì˜ì¡´ì„± ì£¼ì… + AI ëª¨ë¸ ì—°ë™ ì™„ì„±")
    print("=" * 80)
    print("âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°")
    print("âœ… Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ì—°ë™ ìœ ì§€")
    print("âœ… 2ë‹¨ê³„ í´ë°± ì „ëµ (í†µí•© AI â†’ ModelLoader â†’ ê¸°ë³¸)")
    print("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš© ê·¹ëŒ€í™”")
    print("âœ… ê¸°ì¡´ API 100% í˜¸í™˜ì„± ìœ ì§€")
    print("âœ… DI Container ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬")
    print("âœ… ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ëŠìŠ¨í•œ ê²°í•©")
    print("âœ… ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… ì§€ì›")
    print("âœ… conda í™˜ê²½ ìµœì í™”")
    print("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
    print("=" * 80)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ ì¶œë ¥
    print("ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ë“¤ (DI ì§€ì›):")
    print("   - create_pipeline(use_dependency_injection=True)")
    print("   - create_m3_max_pipeline() (ìë™ DI í™œì„±í™”)")
    print("   - create_production_pipeline() (ìë™ DI í™œì„±í™”)")
    print("   - create_development_pipeline() (ìë™ DI í™œì„±í™”)")
    print("   - create_testing_pipeline() (ê¸°ë³¸ DI ì§€ì›)")
    print("   - get_global_pipeline_manager() (ìë™ DI)")
    print("=" * 80)
    
    # ì˜ì¡´ì„± ì£¼ì… ì •ë³´
    print("ğŸ’‰ ì˜ì¡´ì„± ì£¼ì… ê¸°ëŠ¥:")
    print("   - ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°")
    print("   - ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ì„¤ê³„")
    print("   - ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì…")
    print("   - ì§€ì—° ë¡œë”© ì§€ì›")
    print("   - DI Container ê´€ë¦¬")
    print("=" * 80)
    
    import asyncio
    
    async def demo_complete_integration():
        """ì™„ì „ í†µí•© íŒŒì´í”„ë¼ì¸ ë°ëª¨"""
        
        print("ğŸ¯ ì™„ì „ í†µí•© íŒŒì´í”„ë¼ì¸ ë°ëª¨ ì‹œì‘ (DI + AI)")
        print("=" * 50)
        
        # 1. ë‹¤ì–‘í•œ íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸
        print("1ï¸âƒ£ DI ì§€ì› íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ë“¤ í…ŒìŠ¤íŠ¸...")
        
        try:
            # ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ (DI í™œì„±í™”)
            basic_pipeline = create_pipeline(use_dependency_injection=True)
            print("âœ… create_pipeline(use_dependency_injection=True) ì„±ê³µ")
            
            # M3 Max íŒŒì´í”„ë¼ì¸ (ìë™ DI)
            m3_pipeline = create_m3_max_pipeline()
            print("âœ… create_m3_max_pipeline() ì„±ê³µ (ìë™ DI)")
            
            # í”„ë¡œë•ì…˜ íŒŒì´í”„ë¼ì¸ (ìë™ DI)
            prod_pipeline = create_production_pipeline()
            print("âœ… create_production_pipeline() ì„±ê³µ (ìë™ DI)")
            
            # ê°œë°œ íŒŒì´í”„ë¼ì¸ (ìë™ DI)
            dev_pipeline = create_development_pipeline()
            print("âœ… create_development_pipeline() ì„±ê³µ (ìë™ DI)")
            
            # í…ŒìŠ¤íŒ… íŒŒì´í”„ë¼ì¸ (ê¸°ë³¸ DI)
            test_pipeline = create_testing_pipeline()
            print("âœ… create_testing_pipeline() ì„±ê³µ (ê¸°ë³¸ DI)")
            
            # ì „ì—­ ë§¤ë‹ˆì € (ìë™ DI)
            global_manager = get_global_pipeline_manager()
            print("âœ… get_global_pipeline_manager() ì„±ê³µ (ìë™ DI)")
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return
        
        # 2. M3 Max íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì™„ì „ í†µí•© ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        print("2ï¸âƒ£ M3 Max ì™„ì „ í†µí•© íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (DI + AI)...")
        
        try:
            # ì´ˆê¸°í™”
            success = await m3_pipeline.initialize()
            if not success:
                print("âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return
            
            print("âœ… M3 Max ì™„ì „ í†µí•© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ìƒíƒœ í™•ì¸
            status = m3_pipeline.get_pipeline_status()
            print(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {status['device']}")
            print(f"ğŸ§  AI ëª¨ë¸: {'âœ…' if status['ai_model_enabled'] else 'âŒ'}")
            print(f"ğŸ”— ModelLoader: {'âœ…' if status['model_loader_initialized'] else 'âŒ'}")
            print(f"ğŸ’‰ ì˜ì¡´ì„± ì£¼ì…: {'âœ…' if status['use_dependency_injection'] else 'âŒ'}")
            print(f"ğŸ”§ DI Container: {'âœ…' if status['di_container_available'] else 'âŒ'}")
            print(f"ğŸ“Š ì´ˆê¸°í™”ëœ Step: {sum(1 for s in status['steps_status'].values() if s['loaded'])}/{len(status['steps_status'])}")
            print(f"ğŸ”— DI ì£¼ì…ëœ Step: {sum(1 for s in status['steps_status'].values() if s.get('di_injected', False))}")
            
            # AI ëª¨ë¸ ìš”ì•½
            ai_summary = m3_pipeline.get_ai_model_summary()
            print(f"ğŸ¤– ë¡œë“œëœ AI ëª¨ë¸: {ai_summary['total_loaded_models']}")
            print(f"ğŸ’‰ DIê°€ ì ìš©ëœ Step: {ai_summary['di_integration']['steps_with_di']}")
            print(f"ğŸ“ˆ DI ì„±ê³µë¥ : {ai_summary['di_integration']['injection_success_rate']:.1f}%")
            
            # ì •ë¦¬
            await m3_pipeline.cleanup()
            print("âœ… íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        print("\nğŸ‰ ì™„ì „ í†µí•© íŒŒì´í”„ë¼ì¸ ë°ëª¨ ì™„ë£Œ!")
        print("âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°!")
        print("âœ… AI ëª¨ë¸ ì—°ë™ ê¸°ëŠ¥ 100% ìœ ì§€!")
        print("âœ… ëª¨ë“  create_pipeline í•¨ìˆ˜ë“¤ì´ DIì™€ í•¨ê»˜ ì •ìƒ ì‘ë™!")
        print("âœ… M3 Max ì„±ëŠ¥ ìµœì í™” + DI + AI ì™„ì „ í†µí•©!")
    
    # ì‹¤í–‰
    asyncio.run(demo_complete_integration())

# ==============================================
# ë¡œê¹… ë° ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

logger.info("ğŸ‰ ì™„ì „ í†µí•© PipelineManager v8.0 ë¡œë“œ ì™„ë£Œ!")
logger.info("âœ… ì£¼ìš” ì™„ì„± ê¸°ëŠ¥:")
logger.info("   - ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°")
logger.info("   - Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ì™„ì „ ì—°ë™ ìœ ì§€")
logger.info("   - 2ë‹¨ê³„ í´ë°± ì „ëµ (í†µí•© AI â†’ ModelLoader â†’ ê¸°ë³¸)")
logger.info("   - M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš© ê·¹ëŒ€í™”")
logger.info("   - ê¸°ì¡´ API 100% í˜¸í™˜ì„± ìœ ì§€")
logger.info("   - DI Container ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬")
logger.info("   - ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ëŠìŠ¨í•œ ê²°í•©")
logger.info("   - ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… ì§€ì›")
logger.info("   - ì§€ì—° ë¡œë”© ë° ë™ì  import")
logger.info("   - AI ëª¨ë¸ ì‚¬ìš© í†µê³„ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
logger.info("   - DI ì‚¬ìš© í†µê³„ ë° ì£¼ì… ì„±ê³µë¥  ì¶”ì ")
logger.info("   - conda í™˜ê²½ ìµœì í™”")

logger.info("âœ… ì™„ì „ í†µí•©ëœ create_pipeline í•¨ìˆ˜ë“¤:")
logger.info("   - create_pipeline() âœ… (DI ì§€ì›)")
logger.info("   - create_m3_max_pipeline() âœ… (ìë™ DI)") 
logger.info("   - create_production_pipeline() âœ… (ìë™ DI)")
logger.info("   - create_development_pipeline() âœ… (ìë™ DI)")
logger.info("   - create_testing_pipeline() âœ… (ê¸°ë³¸ DI)")
logger.info("   - get_global_pipeline_manager() âœ… (ìë™ DI)")

logger.info("ğŸ’‰ ì˜ì¡´ì„± ì£¼ì… í†µí•© ê¸°ëŠ¥:")
logger.info("   - ìˆœí™˜ ì„í¬íŠ¸ ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("   - IModelLoader, IMemoryManager, IDataConverter ì¸í„°í˜ì´ìŠ¤")
logger.info("   - ModelLoaderAdapter, MemoryManagerAdapter ì–´ëŒ‘í„° íŒ¨í„´")
logger.info("   - DI Container ê¸°ë°˜ ì „ì—­ ì˜ì¡´ì„± ê´€ë¦¬")
logger.info("   - ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… (inject_dependencies)")
logger.info("   - ì§€ì—° ë¡œë”© (resolve_lazy_dependencies)")
logger.info("   - TYPE_CHECKINGìœ¼ë¡œ import ì‹œì  ìˆœí™˜ì°¸ì¡° ë°©ì§€")

logger.info("ğŸš€ ì´ì œ ìˆœí™˜ ì„í¬íŠ¸ ì—†ì´ ìµœê³  í’ˆì§ˆ AI ê°€ìƒ í”¼íŒ…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")

logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ê°€ìš©ì„±:")
logger.info(f"   - í†µí•© ìœ í‹¸ë¦¬í‹°: {'âœ…' if UNIFIED_UTILS_AVAILABLE else 'âŒ'}")
logger.info(f"   - ModelLoader: {'âœ…' if MODEL_LOADER_AVAILABLE else 'âŒ'}")
logger.info(f"   - Step ìš”ì²­: {'âœ…' if STEP_REQUESTS_AVAILABLE else 'âŒ'}")
logger.info(f"   - ìë™ íƒì§€: {'âœ…' if AUTO_DETECTOR_AVAILABLE else 'âŒ'}")
logger.info(f"   - Step í´ë˜ìŠ¤: {'âœ…' if STEP_CLASSES_AVAILABLE else 'âŒ'}")
logger.info(f"   - DI Container: {'âœ…' if DI_CONTAINER_AVAILABLE else 'âŒ'}")
logger.info(f"   - ì¸í„°í˜ì´ìŠ¤: {'âœ…' if INTERFACES_AVAILABLE else 'âŒ'}")

logger.info("ğŸ¯ ê¶Œì¥ ì‚¬ìš©ë²• (ì™„ì „ í†µí•©):")
logger.info("   - M3 Max: create_m3_max_pipeline() (DI + AI ìë™)")
logger.info("   - í”„ë¡œë•ì…˜: create_production_pipeline() (DI + AI ìë™)")
logger.info("   - ê°œë°œ: create_development_pipeline() (DI + AI ìë™)")
logger.info("   - ê¸°ë³¸: create_pipeline(use_dependency_injection=True)")

logger.info("ğŸ—ï¸ ì•„í‚¤í…ì²˜ v8.0 ì™„ì „ í†µí•©:")
logger.info("   - ìˆœí™˜ ì„í¬íŠ¸ â†’ âœ… ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ì™„ì „ í•´ê²°")
logger.info("   - AI ëª¨ë¸ ì—°ë™ â†’ âœ… 100% ìœ ì§€ ë° ê°•í™”")
logger.info("   - ì„±ëŠ¥ ìµœì í™” â†’ âœ… M3 Max + DI + AI í†µí•©")
logger.info("   - ì½”ë“œ í’ˆì§ˆ â†’ âœ… ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ì„¤ê³„")
logger.info("   - ìœ ì§€ë³´ìˆ˜ì„± â†’ âœ… ëŠìŠ¨í•œ ê²°í•© + ë†’ì€ ì‘ì§‘ë„")
logger.info("   - í™•ì¥ì„± â†’ âœ… DI Container + ì–´ëŒ‘í„° íŒ¨í„´")