# backend/app/ai_pipeline/interfaces/step_interface.py
"""
ğŸ”¥ Step Interface v3.1 - Import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
===============================================

âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
âœ… GitHub ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡° ë°˜ì˜  
âœ… TYPE_CHECKING ì‚¬ìš©ìœ¼ë¡œ import ì˜¤ë¥˜ í•´ê²°
âœ… BaseStepMixin ì™„ì „ í˜¸í™˜
âœ… register_model_requirement ì™„ì „ êµ¬í˜„
âœ… list_available_models í¬ê¸°ìˆœ ì •ë ¬
"""

import os
import gc
import sys
import time
import logging
import asyncio
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Callable, Type, Tuple, Set, TYPE_CHECKING
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from enum import Enum
from functools import wraps, lru_cache

# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from ...utils.model_loader import ModelLoader
    from ...utils.memory_manager import MemoryManager
    from ...utils.data_converter import DataConverter

logger = logging.getLogger(__name__)

# =============================================================================
# ğŸ”¥ í™˜ê²½ ì„¤ì • ë° ì‹œìŠ¤í…œ ì •ë³´
# =============================================================================

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean'
}

# ì‹œìŠ¤í…œ ì •ë³´
IS_M3_MAX = False
MEMORY_GB = 16.0

try:
    import platform
    import subprocess
    if platform.system() == 'Darwin':
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.returncode == 0:
            MEMORY_GB = round(int(memory_result.stdout.strip()) / (1024**3), 1)
except Exception:
    pass

# MPS ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
MPS_AVAILABLE = False
try:
    import torch
    MPS_AVAILABLE = (
        IS_M3_MAX and 
        hasattr(torch.backends, 'mps') and 
        torch.backends.mps.is_available()
    )
except ImportError:
    pass

# =============================================================================
# ğŸ”¥ ì—´ê±°í˜• ë° ìƒìˆ˜ ì •ì˜
# =============================================================================

class StepType(Enum):
    """Step íƒ€ì… ì •ì˜"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class StepPriority(Enum):
    """Step ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1      # Virtual Fitting (14GB), Human Parsing (4GB)
    HIGH = 2          # Cloth Warping (7GB), Quality Assessment (7GB)
    MEDIUM = 3        # Cloth Segmentation (5.5GB), Pose Estimation (3.4GB)
    LOW = 4           # Post Processing (1.3GB), Geometric Matching (1.3GB)

class DeviceType(Enum):
    """ë””ë°”ì´ìŠ¤ íƒ€ì…"""
    AUTO = "auto"
    CPU = "cpu"
    CUDA = "cuda"
    MPS = "mps"

class ProcessingStatus(Enum):
    """ì²˜ë¦¬ ìƒíƒœ"""
    NOT_STARTED = "not_started"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    COMPLETED = "completed"
    ERROR = "error"
    CANCELLED = "cancelled"

# =============================================================================
# ğŸ”¥ BaseStepMixinConfig - conda_env ë§¤ê°œë³€ìˆ˜ ì™„ì „ ì§€ì›
# =============================================================================

@dataclass
class BaseStepMixinConfig:
    """BaseStepMixin ì„¤ì • êµ¬ì¡°"""
    # ê¸°ë³¸ Step ì •ë³´
    step_name: str = "BaseStep"
    step_id: int = 0
    class_name: str = "BaseStepMixin"
    
    # ë””ë°”ì´ìŠ¤ ë° ì„±ëŠ¥ ì„¤ì •
    device: str = "auto"
    use_fp16: bool = False
    batch_size: int = 1
    confidence_threshold: float = 0.5
    
    # ìë™í™” ì„¤ì •
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True
    auto_inject_dependencies: bool = True
    optimization_enabled: bool = True
    strict_mode: bool = False
    
    # ì˜ì¡´ì„± ìš”êµ¬ì‚¬í•­
    require_model_loader: bool = True
    require_memory_manager: bool = True
    require_data_converter: bool = True
    require_di_container: bool = False
    require_unified_dependency_manager: bool = True
    
    # AI ëª¨ë¸ ì„¤ì •
    ai_models: List[str] = field(default_factory=list)
    model_size_gb: float = 1.0
    
    # ğŸ”¥ í™˜ê²½ ìµœì í™” ì„¤ì • (conda_env ë§¤ê°œë³€ìˆ˜ ì¶”ê°€)
    conda_optimized: bool = True
    m3_max_optimized: bool = True
    conda_env: Optional[str] = None
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ì„¤ì • ë³´ì •"""
        # conda_env ìë™ ì„¤ì •
        if self.conda_env is None:
            self.conda_env = CONDA_INFO['conda_env']
        
        # mycloset-ai-clean í™˜ê²½ íŠ¹ë³„ ìµœì í™”
        if self.conda_env == 'mycloset-ai-clean':
            self.conda_optimized = True
            self.optimization_enabled = True
            self.auto_memory_cleanup = True
            
            if IS_M3_MAX:
                self.m3_max_optimized = True
                if self.device == "auto" and MPS_AVAILABLE:
                    self.device = "mps"
                if self.batch_size == 1 and MEMORY_GB >= 64:
                    self.batch_size = 2
        
        # AI ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì •ê·œí™”
        if not isinstance(self.ai_models, list):
            self.ai_models = []

    def validate(self) -> Tuple[bool, List[str]]:
        """ì„¤ì • ê²€ì¦"""
        errors = []
        
        if not self.step_name:
            errors.append("step_nameì´ ë¹„ì–´ìˆìŒ")
        
        if self.step_id < 0:
            errors.append("step_idëŠ” 0 ì´ìƒì´ì–´ì•¼ í•¨")
        
        if self.batch_size <= 0:
            errors.append("batch_sizeëŠ” 1 ì´ìƒì´ì–´ì•¼ í•¨")
        
        if not 0.0 <= self.confidence_threshold <= 1.0:
            errors.append("confidence_thresholdëŠ” 0.0-1.0 ë²”ìœ„ì—¬ì•¼ í•¨")
        
        if self.model_size_gb < 0:
            errors.append("model_size_gbëŠ” 0 ì´ìƒì´ì–´ì•¼ í•¨")
        
        valid_devices = {"auto", "cpu", "cuda", "mps"}
        if self.device not in valid_devices:
            errors.append(f"deviceëŠ” {valid_devices} ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•¨")
        
        if self.conda_optimized and self.conda_env == 'none':
            errors.append("conda_optimizedê°€ Trueì¸ë° conda í™˜ê²½ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ")
        
        return len(errors) == 0, errors

# =============================================================================
# ğŸ”¥ StepCreationResult
# =============================================================================

@dataclass
class StepCreationResult:
    """Step ìƒì„± ê²°ê³¼"""
    success: bool
    step_instance: Optional[Any] = None
    step_name: str = ""
    step_id: int = 0
    device: str = "cpu"
    creation_time: float = field(default_factory=time.time)
    error_message: Optional[str] = None
    dependencies_injected: Dict[str, bool] = field(default_factory=dict)
    initialization_success: bool = False
    memory_usage_mb: float = 0.0
    conda_env: str = field(default_factory=lambda: CONDA_INFO['conda_env'])
    metadata: Dict[str, Any] = field(default_factory=dict)

# =============================================================================
# ğŸ”¥ ê³ ê¸‰ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ
# =============================================================================

class AdvancedMemoryManager:
    """ê³ ê¸‰ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ (M3 Max ìµœì í™”)"""
    
    def __init__(self, max_memory_gb: float = None):
        self.logger = logging.getLogger(f"{__name__}.AdvancedMemoryManager")
        
        if max_memory_gb is None:
            self.max_memory_gb = MEMORY_GB * 0.8 if IS_M3_MAX else 8.0
        else:
            self.max_memory_gb = max_memory_gb
        
        self.current_memory_gb = 0.0
        self.memory_pool = {}
        self.allocation_history = []
        self._lock = threading.RLock()
        
        self.is_m3_max = IS_M3_MAX
        self.mps_enabled = MPS_AVAILABLE
        
        self.peak_memory_gb = 0.0
        self.allocation_count = 0
        self.deallocation_count = 0
        
        self.logger.info(f"ğŸ§  ê³ ê¸‰ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”: {self.max_memory_gb:.1f}GB (M3 Max: {self.is_m3_max})")
    
    def allocate_memory(self, size_gb: float, owner: str) -> bool:
        """ë©”ëª¨ë¦¬ í• ë‹¹"""
        with self._lock:
            if self.current_memory_gb + size_gb <= self.max_memory_gb:
                self.current_memory_gb += size_gb
                self.memory_pool[owner] = size_gb
                self.allocation_history.append({
                    'action': 'allocate',
                    'size_gb': size_gb,
                    'owner': owner,
                    'timestamp': time.time(),
                    'total_after': self.current_memory_gb
                })
                
                self.allocation_count += 1
                self.peak_memory_gb = max(self.peak_memory_gb, self.current_memory_gb)
                
                self.logger.debug(f"âœ… ë©”ëª¨ë¦¬ í• ë‹¹: {size_gb:.1f}GB â†’ {owner} (ì´: {self.current_memory_gb:.1f}GB)")
                return True
            else:
                self.logger.warning(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±: {size_gb:.1f}GB ìš”ì²­, {self.max_memory_gb - self.current_memory_gb:.1f}GB ì‚¬ìš© ê°€ëŠ¥")
                return False
    
    def deallocate_memory(self, owner: str) -> float:
        """ë©”ëª¨ë¦¬ í•´ì œ"""
        with self._lock:
            if owner in self.memory_pool:
                size_gb = self.memory_pool[owner]
                del self.memory_pool[owner]
                self.current_memory_gb -= size_gb
                
                self.allocation_history.append({
                    'action': 'deallocate',
                    'size_gb': size_gb,
                    'owner': owner,
                    'timestamp': time.time(),
                    'total_after': self.current_memory_gb
                })
                
                self.deallocation_count += 1
                
                self.logger.debug(f"âœ… ë©”ëª¨ë¦¬ í•´ì œ: {size_gb:.1f}GB â† {owner} (ì´: {self.current_memory_gb:.1f}GB)")
                return size_gb
            return 0.0
    
    def optimize_for_m3_max(self):
        """M3 Max ì „ìš© ë©”ëª¨ë¦¬ ìµœì í™”"""
        if not self.is_m3_max:
            return
        
        try:
            if self.mps_enabled:
                import torch
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                    self.logger.debug("ğŸ MPS ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            
            gc.collect()
            
            if MEMORY_GB >= 64:
                self.max_memory_gb = min(MEMORY_GB * 0.9, 100.0)
                self.logger.info(f"ğŸ M3 Max ë©”ëª¨ë¦¬ í’€ í™•ì¥: {self.max_memory_gb:.1f}GB")
            
        except Exception as e:
            self.logger.error(f"âŒ M3 Max ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„ ë°˜í™˜"""
        with self._lock:
            return {
                'current_gb': self.current_memory_gb,
                'max_gb': self.max_memory_gb,
                'peak_gb': self.peak_memory_gb,
                'available_gb': self.max_memory_gb - self.current_memory_gb,
                'usage_percent': (self.current_memory_gb / self.max_memory_gb) * 100,
                'allocations': self.allocation_count,
                'deallocations': self.deallocation_count,
                'active_pools': len(self.memory_pool),
                'is_m3_max': self.is_m3_max,
                'mps_enabled': self.mps_enabled,
                'memory_pool': self.memory_pool.copy(),
                'total_system_gb': MEMORY_GB
            }

# =============================================================================
# ğŸ”¥ StepInterface êµ¬í˜„ 
# =============================================================================

class StepInterface:
    """
    ğŸ”— Step Interface v3.1 - import ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    
    âœ… ìˆœí™˜ì°¸ì¡° ë°©ì§€
    âœ… ë™ì  import ì‚¬ìš©
    âœ… BaseStepMixin ì™„ì „ í˜¸í™˜
    âœ… register_model_requirement ì™„ì „ êµ¬í˜„
    âœ… list_available_models í¬ê¸°ìˆœ ì •ë ¬
    """
    
    def __init__(self, step_name: str, model_loader: Optional['ModelLoader'] = None):
        self.step_name = step_name
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # ëª¨ë¸ ê´€ë¦¬
        self._model_registry: Dict[str, Dict[str, Any]] = {}
        self._model_cache: Dict[str, Any] = {}
        self._model_requirements: Dict[str, Any] = {}
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.memory_manager = AdvancedMemoryManager()
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        
        # í†µê³„
        self.statistics = {
            'models_registered': 0,
            'models_loaded': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'loading_failures': 0,
            'creation_time': time.time()
        }
        
        self.logger.info(f"ğŸ”— {step_name} StepInterface v3.1 ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """
        ğŸ”¥ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - BaseStepMixin ì™„ì „ í˜¸í™˜ êµ¬í˜„
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            model_type: ëª¨ë¸ íƒ€ì…
            **kwargs: ì¶”ê°€ ì„¤ì •
            
        Returns:
            bool: ë“±ë¡ ì„±ê³µ ì—¬ë¶€
        """
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
                
                # ìš”êµ¬ì‚¬í•­ ì •ë³´ ìƒì„±
                requirement = {
                    'model_name': model_name,
                    'model_type': model_type,
                    'step_name': self.step_name,
                    'device': kwargs.get('device', 'auto'),
                    'precision': kwargs.get('precision', 'fp16'),
                    'input_size': kwargs.get('input_size', (512, 512)),
                    'num_classes': kwargs.get('num_classes'),
                    'priority': kwargs.get('priority', 5),
                    'min_memory_mb': kwargs.get('min_memory_mb', 100.0),
                    'max_memory_mb': kwargs.get('max_memory_mb', 8192.0),
                    'conda_env': kwargs.get('conda_env', CONDA_INFO['conda_env']),
                    'registered_at': time.time(),
                    'metadata': kwargs.get('metadata', {})
                }
                
                # ìš”êµ¬ì‚¬í•­ ì €ì¥
                self._model_requirements[model_name] = requirement
                
                # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
                self._model_registry[model_name] = {
                    'name': model_name,
                    'type': model_type,
                    'step_class': self.step_name,
                    'loaded': False,
                    'size_mb': requirement['max_memory_mb'],
                    'device': requirement['device'],
                    'status': 'registered',
                    'requirement': requirement,
                    'registered_at': requirement['registered_at']
                }
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.statistics['models_registered'] += 1
                
                # ModelLoaderì— ì „ë‹¬ (ê°€ëŠ¥í•œ ê²½ìš°)
                if self.model_loader and hasattr(self.model_loader, 'register_model_requirement'):
                    try:
                        self.model_loader.register_model_requirement(
                            model_name=model_name,
                            model_type=model_type,
                            step_name=self.step_name,
                            **kwargs
                        )
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ìš”êµ¬ì‚¬í•­ ì „ë‹¬ ì‹¤íŒ¨: {e}")
                
                self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {model_name}")
                return True
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    def list_available_models(
        self, 
        step_class: Optional[str] = None,
        model_type: Optional[str] = None,
        include_unloaded: bool = True,
        sort_by: str = "size"
    ) -> List[Dict[str, Any]]:
        """
        ğŸ”¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ - BaseStepMixin ì™„ì „ í˜¸í™˜
        
        Args:
            step_class: Step í´ë˜ìŠ¤ í•„í„°
            model_type: ëª¨ë¸ íƒ€ì… í•„í„°
            include_unloaded: ë¡œë“œë˜ì§€ ì•Šì€ ëª¨ë¸ í¬í•¨ ì—¬ë¶€
            sort_by: ì •ë ¬ ê¸°ì¤€ (size, name, priority)
            
        Returns:
            List[Dict[str, Any]]: ëª¨ë¸ ëª©ë¡ (ì •ë ¬ë¨)
        """
        try:
            with self._lock:
                models = []
                
                # ë“±ë¡ëœ ëª¨ë¸ë“¤ì—ì„œ ëª©ë¡ ìƒì„±
                for model_name, registry_entry in self._model_registry.items():
                    # í•„í„°ë§
                    if step_class and registry_entry['step_class'] != step_class:
                        continue
                    if model_type and registry_entry['type'] != model_type:
                        continue
                    if not include_unloaded and not registry_entry['loaded']:
                        continue
                    
                    # ëª¨ë¸ ì •ë³´ êµ¬ì„±
                    requirement = registry_entry.get('requirement', {})
                    
                    model_info = {
                        'name': model_name,
                        'path': f"ai_models/step_{requirement.get('step_name', self.step_name).lower()}/{model_name}",
                        'size_mb': registry_entry['size_mb'],
                        'model_type': registry_entry['type'],
                        'step_class': registry_entry['step_class'],
                        'loaded': registry_entry['loaded'],
                        'device': registry_entry['device'],
                        'status': registry_entry['status'],
                        'priority': requirement.get('priority', 5),
                        'metadata': {
                            'step_name': self.step_name,
                            'input_size': requirement.get('input_size', (512, 512)),
                            'num_classes': requirement.get('num_classes'),
                            'precision': requirement.get('precision', 'fp16'),
                            'conda_env': requirement.get('conda_env', CONDA_INFO['conda_env']),
                            'registered_at': requirement.get('registered_at', 0),
                            'github_structure_compliant': True,
                            **requirement.get('metadata', {})
                        }
                    }
                    models.append(model_info)
                
                # ModelLoaderì—ì„œ ì¶”ê°€ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ê°€ëŠ¥í•œ ê²½ìš°)
                if self.model_loader and hasattr(self.model_loader, 'list_available_models'):
                    try:
                        additional_models = self.model_loader.list_available_models(
                            step_class=step_class or self.step_name,
                            model_type=model_type
                        )
                        
                        # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                        existing_names = {m['name'] for m in models}
                        for model in additional_models:
                            if model['name'] not in existing_names:
                                model_info = {
                                    'name': model['name'],
                                    'path': model.get('path', f"loader_models/{model['name']}"),
                                    'size_mb': model.get('size_mb', 0.0),
                                    'model_type': model.get('model_type', 'unknown'),
                                    'step_class': model.get('step_class', self.step_name),
                                    'loaded': model.get('loaded', False),
                                    'device': model.get('device', 'auto'),
                                    'status': 'loaded' if model.get('loaded', False) else 'available',
                                    'priority': 5,
                                    'metadata': {
                                        'step_name': self.step_name,
                                        'source': 'model_loader',
                                        'github_structure_compliant': False,
                                        **model.get('metadata', {})
                                    }
                                }
                                models.append(model_info)
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                
                # ì •ë ¬ ìˆ˜í–‰
                if sort_by == "size":
                    models.sort(key=lambda x: x['size_mb'], reverse=True)  # í° ê²ƒë¶€í„°
                elif sort_by == "name":
                    models.sort(key=lambda x: x['name'])
                elif sort_by == "priority":
                    models.sort(key=lambda x: x['priority'])  # ì‘ì€ ê°’ì´ ë†’ì€ ìš°ì„ ìˆœìœ„
                else:
                    # ê¸°ë³¸ê°’: í¬ê¸°ìˆœ ì •ë ¬
                    models.sort(key=lambda x: x['size_mb'], reverse=True)
                
                self.logger.debug(f"ğŸ“‹ ëª¨ë¸ ëª©ë¡ ë°˜í™˜: {len(models)}ê°œ")
                return models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë™ê¸°) - BaseStepMixin í˜¸í™˜"""
        try:
            with self._lock:
                # ìºì‹œ í™•ì¸
                if model_name in self._model_cache:
                    self.statistics['cache_hits'] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return self._model_cache[model_name]
                
                # ModelLoaderë¥¼ í†µí•œ ë¡œë”©
                if self.model_loader and hasattr(self.model_loader, 'load_model'):
                    model = self.model_loader.load_model(model_name, **kwargs)
                    
                    if model is not None:
                        # ìºì‹œì— ì €ì¥
                        self._model_cache[model_name] = model
                        
                        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸
                        if model_name in self._model_registry:
                            self._model_registry[model_name]['loaded'] = True
                            self._model_registry[model_name]['status'] = 'loaded'
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.statistics['models_loaded'] += 1
                        
                        self.logger.info(f"âœ… ë™ê¸° ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                        return model
                
                # ë¡œë”© ì‹¤íŒ¨
                self.statistics['cache_misses'] += 1
                self.statistics['loading_failures'] += 1
                self.logger.warning(f"âš ï¸ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            return None
    
    async def get_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë¹„ë™ê¸°) - BaseStepMixin í˜¸í™˜"""
        try:
            with self._lock:
                # ìºì‹œ í™•ì¸
                if model_name in self._model_cache:
                    self.statistics['cache_hits'] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return self._model_cache[model_name]
                
                # ModelLoaderë¥¼ í†µí•œ ë¡œë”©
                if self.model_loader:
                    if hasattr(self.model_loader, 'load_model_async'):
                        model = await self.model_loader.load_model_async(model_name, **kwargs)
                    elif hasattr(self.model_loader, 'load_model'):
                        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                        loop = asyncio.get_event_loop()
                        model = await loop.run_in_executor(
                            None, 
                            lambda: self.model_loader.load_model(model_name, **kwargs)
                        )
                    else:
                        model = None
                    
                    if model is not None:
                        # ìºì‹œì— ì €ì¥
                        self._model_cache[model_name] = model
                        
                        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸
                        if model_name in self._model_registry:
                            self._model_registry[model_name]['loaded'] = True
                            self._model_registry[model_name]['status'] = 'loaded'
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.statistics['models_loaded'] += 1
                        
                        self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                        return model
                
                # ë¡œë”© ì‹¤íŒ¨
                self.statistics['cache_misses'] += 1
                self.statistics['loading_failures'] += 1
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            return None
    
    def get_model_status(self, model_name: Optional[str] = None) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ - BaseStepMixin í˜¸í™˜"""
        try:
            with self._lock:
                if model_name:
                    # íŠ¹ì • ëª¨ë¸ ìƒíƒœ
                    if model_name in self._model_registry:
                        return self._model_registry[model_name].copy()
                    else:
                        return {
                            'name': model_name,
                            'status': 'not_registered',
                            'loaded': False,
                            'error': 'Model not found in registry'
                        }
                else:
                    # ì „ì²´ ìƒíƒœ
                    memory_stats = self.memory_manager.get_memory_stats()
                    
                    return {
                        'step_name': self.step_name,
                        'models': dict(self._model_registry),
                        'total_registered': len(self._model_registry),
                        'total_loaded': len(self._model_cache),
                        'statistics': self.statistics.copy(),
                        'memory_stats': memory_stats,
                        'environment': {
                            'conda_env': CONDA_INFO['conda_env'],
                            'is_target_env': CONDA_INFO['is_target_env'],
                            'is_m3_max': IS_M3_MAX,
                            'memory_gb': MEMORY_GB
                        },
                        'version': '3.1'
                    }
        except Exception as e:
            return {'error': str(e)}
    
    def clear_cache(self) -> bool:
        """ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™”"""
        try:
            with self._lock:
                # ë©”ëª¨ë¦¬ í•´ì œ
                for model_name in self._model_cache:
                    self.memory_manager.deallocate_memory(model_name)
                
                # ìºì‹œ ì´ˆê¸°í™”
                self._model_cache.clear()
                
                # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
                for model_name in self._model_registry:
                    self._model_registry[model_name]['loaded'] = False
                    self._model_registry[model_name]['status'] = 'registered'
                
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                gc.collect()
                
                self.logger.info("ğŸ§¹ ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.clear_cache()
            self._model_requirements.clear()
            self._model_registry.clear()
            self.memory_manager = AdvancedMemoryManager()
            self.logger.info(f"âœ… {self.step_name} Interface ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ Interface ì •ë¦¬ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ GitHub êµ¬ì¡° ê¸°ë°˜ Step ë§¤í•‘ í´ë˜ìŠ¤
# =============================================================================

class BaseStepMixinMapping:
    """GitHub êµ¬ì¡° ê¸°ë°˜ BaseStepMixin ë§¤í•‘"""
    
    # GitHub ì‹¤ì œ íŒŒì¼ êµ¬ì¡°ì— ë§ëŠ” Step ì„¤ì •ë“¤
    STEP_CONFIGS = {
        StepType.HUMAN_PARSING: BaseStepMixinConfig(
            step_name="HumanParsingStep",
            step_id=1,
            class_name="HumanParsingStep",
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.5,
            ai_models=["graphonomy.pth", "atr_model.pth", "lip_model.pth"],
            model_size_gb=4.0,
            conda_optimized=True,
            m3_max_optimized=True,
            conda_env=CONDA_INFO['conda_env']
        ),
        
        StepType.VIRTUAL_FITTING: BaseStepMixinConfig(
            step_name="VirtualFittingStep",
            step_id=6,
            class_name="VirtualFittingStep",
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.8,
            ai_models=["stable-diffusion-v1-5", "controlnet", "vae"],
            model_size_gb=14.0,  # í•µì‹¬ 14GB ëª¨ë¸
            conda_optimized=True,
            m3_max_optimized=True,
            conda_env=CONDA_INFO['conda_env']
        )
    }
    
    @classmethod
    def get_config(cls, step_type: StepType) -> BaseStepMixinConfig:
        """Step íƒ€ì…ë³„ ì„¤ì • ë°˜í™˜"""
        return cls.STEP_CONFIGS.get(step_type, BaseStepMixinConfig())

# =============================================================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# =============================================================================

def create_step_interface(
    step_name: str, 
    model_loader: Optional['ModelLoader'] = None,
    max_memory_gb: float = None
) -> StepInterface:
    """Step Interface ìƒì„± (GitHub êµ¬ì¡° í˜¸í™˜)"""
    try:
        interface = StepInterface(step_name, model_loader)
        
        # M3 Max í™˜ê²½ì— ë§ëŠ” ë©”ëª¨ë¦¬ ì„¤ì •
        if max_memory_gb is None:
            max_memory_gb = MEMORY_GB * 0.8 if IS_M3_MAX else 8.0
        
        interface.memory_manager = AdvancedMemoryManager(max_memory_gb)
        
        logger.info(f"âœ… Step Interface ìƒì„± ì™„ë£Œ: {step_name} ({max_memory_gb:.1f}GB)")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ Step Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        # í´ë°± ì¸í„°í˜ì´ìŠ¤
        return StepInterface(step_name, None)

def create_optimized_step_interface(
    step_name: str,
    model_loader: Optional['ModelLoader'] = None
) -> StepInterface:
    """ìµœì í™”ëœ Step Interface ìƒì„± (conda + M3 Max ëŒ€ì‘)"""
    try:
        # conda + M3 Max ì¡°í•© ìµœì í™” ì„¤ì •
        if CONDA_INFO['is_target_env'] and IS_M3_MAX:
            max_memory_gb = MEMORY_GB * 0.9  # 90% ì‚¬ìš©
        elif IS_M3_MAX:
            max_memory_gb = MEMORY_GB * 0.8  # 80% ì‚¬ìš©
        elif CONDA_INFO['is_target_env']:
            max_memory_gb = 12.0  # 12GB
        else:
            max_memory_gb = 8.0   # 8GB
        
        interface = create_step_interface(
            step_name=step_name,
            model_loader=model_loader,
            max_memory_gb=max_memory_gb
        )
        
        logger.info(f"âœ… ìµœì í™”ëœ Interface: {step_name} (conda: {CONDA_INFO['is_target_env']}, M3: {IS_M3_MAX})")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ ìµœì í™”ëœ Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        return create_step_interface(step_name, model_loader)

# =============================================================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def get_environment_info() -> Dict[str, Any]:
    """í™˜ê²½ ì •ë³´ ì¡°íšŒ"""
    return {
        'conda_info': CONDA_INFO,
        'system_info': {
            'is_m3_max': IS_M3_MAX,
            'memory_gb': MEMORY_GB,
            'mps_available': MPS_AVAILABLE,
            'platform': platform.system(),
            'machine': platform.machine()
        },
        'optimization_status': {
            'conda_optimized': CONDA_INFO['is_target_env'],
            'm3_max_optimized': IS_M3_MAX,
            'ultra_optimization_available': CONDA_INFO['is_target_env'] and IS_M3_MAX
        }
    }

def optimize_environment():
    """í™˜ê²½ ìµœì í™” ì‹¤í–‰"""
    try:
        optimizations = []
        
        # conda í™˜ê²½ ìµœì í™”
        if CONDA_INFO['is_target_env']:
            optimizations.append("conda í™˜ê²½ ìµœì í™”")
        
        # M3 Max ìµœì í™”
        if IS_M3_MAX:
            optimizations.append("M3 Max ìµœì í™”")
            
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            if MPS_AVAILABLE:
                try:
                    import torch
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                    optimizations.append("MPS ë©”ëª¨ë¦¬ ì •ë¦¬")
                except:
                    pass
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        optimizations.append("ê°€ë¹„ì§€ ì»¬ë ‰ì…˜")
        
        logger.info(f"âœ… í™˜ê²½ ìµœì í™” ì™„ë£Œ: {', '.join(optimizations)}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
        return False

# =============================================================================
# ğŸ”¥ Export
# =============================================================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'StepInterface',
    'AdvancedMemoryManager',
    'BaseStepMixinMapping',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'BaseStepMixinConfig',
    'StepCreationResult',
    'StepType',
    'StepPriority',
    'DeviceType', 
    'ProcessingStatus',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_step_interface',
    'create_optimized_step_interface',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_environment_info',
    'optimize_environment',
    
    # ìƒìˆ˜ë“¤
    'CONDA_INFO',
    'IS_M3_MAX',
    'MEMORY_GB',
    'MPS_AVAILABLE'
]

# =============================================================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ
# =============================================================================

# conda í™˜ê²½ ìë™ ìµœì í™”
if CONDA_INFO['is_target_env']:
    optimize_environment()
    logger.info("ğŸ conda í™˜ê²½ ìë™ ìµœì í™” ì™„ë£Œ!")

# M3 Max ìµœì í™”
if IS_M3_MAX:
    try:
        if MPS_AVAILABLE:
            import torch
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
        gc.collect()
        logger.info("ğŸ M3 Max ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ!")
    except:
        pass

logger.info("ğŸ”¥ StepInterface v3.1 - Import ì˜¤ë¥˜ ì™„ì „ í•´ê²° ì™„ë£Œ!")
logger.info(f"ğŸ”§ í˜„ì¬ í™˜ê²½: conda={CONDA_INFO['conda_env']}, M3 Max={IS_M3_MAX}, ë©”ëª¨ë¦¬={MEMORY_GB:.1f}GB")