# backend/app/ai_pipeline/interface/step_interface.py
"""
ğŸ”¥ StepModelInterface v2.0 - ì™„ì „ í˜¸í™˜ì„± + ìˆœí™˜ì°¸ì¡° í•´ê²°
======================================================

âœ… BaseStepMixin ì™„ì „ í˜¸í™˜ì„± ë³´ì¥
âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
âœ… register_model_requirement ë©”ì„œë“œ ì™„ì „ êµ¬í˜„
âœ… list_available_models í¬ê¸°ìˆœ ì •ë ¬
âœ… í–¥ìƒëœ ì—ëŸ¬ ì²˜ë¦¬ ë° ì•ˆì •ì„±
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”

Author: MyCloset AI Team
Date: 2025-07-24
Version: 2.0 (Complete Compatibility)
"""

import logging
import threading
import asyncio
import time
import gc
import weakref
from typing import Dict, Any, Optional, List, Union, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path

# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
if TYPE_CHECKING:
    from ..utils.model_loader import ModelLoader

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class ModelStatus(Enum):
    """ëª¨ë¸ ìƒíƒœ"""
    NOT_LOADED = "not_loaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"
    VALIDATING = "validating"

class CachePolicy(Enum):
    """ìºì‹œ ì •ì±…"""
    MEMORY_FIRST = "memory_first"
    DISK_FIRST = "disk_first"
    NO_CACHE = "no_cache"
    HYBRID = "hybrid"

@dataclass
class ModelRequirement:
    """ëª¨ë¸ ìš”êµ¬ì‚¬í•­"""
    model_name: str
    model_type: str = "BaseModel"
    device: str = "auto"
    precision: str = "fp16"
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    priority: int = 5
    min_memory_mb: float = 100.0
    max_memory_mb: float = 8192.0
    cache_policy: CachePolicy = CachePolicy.MEMORY_FIRST
    metadata: Dict[str, Any] = field(default_factory=dict)
    registered_at: float = field(default_factory=time.time)

@dataclass
class ModelCacheEntry:
    """ëª¨ë¸ ìºì‹œ ì—”íŠ¸ë¦¬"""
    model: Any
    status: ModelStatus
    load_time: float
    last_access: float
    access_count: int
    memory_mb: float
    device: str
    step_name: str
    requirement: Optional[ModelRequirement] = None
    validation_passed: bool = True
    error_count: int = 0
    last_error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class InterfaceStatistics:
    """ì¸í„°í˜ì´ìŠ¤ í†µê³„"""
    models_registered: int = 0
    models_loaded: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    loading_failures: int = 0
    total_memory_mb: float = 0.0
    average_load_time: float = 0.0
    creation_time: float = field(default_factory=time.time)
    last_activity: float = field(default_factory=time.time)

# ==============================================
# ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ì
# ==============================================

class ModelMemoryManager:
    """ëª¨ë¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, max_memory_mb: float = 4096.0):
        self.max_memory_mb = max_memory_mb
        self.current_memory_mb = 0.0
        self.logger = logging.getLogger("ModelMemoryManager")
        self._lock = threading.RLock()
    
    def can_load_model(self, required_memory_mb: float) -> bool:
        """ëª¨ë¸ ë¡œë”© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        with self._lock:
            return (self.current_memory_mb + required_memory_mb) <= self.max_memory_mb
    
    def reserve_memory(self, memory_mb: float) -> bool:
        """ë©”ëª¨ë¦¬ ì˜ˆì•½"""
        with self._lock:
            if self.can_load_model(memory_mb):
                self.current_memory_mb += memory_mb
                return True
            return False
    
    def release_memory(self, memory_mb: float):
        """ë©”ëª¨ë¦¬ í•´ì œ"""
        with self._lock:
            self.current_memory_mb = max(0.0, self.current_memory_mb - memory_mb)
    
    def force_cleanup(self) -> float:
        """ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            released_memory = self.current_memory_mb
            
            # Python GC ì‹¤í–‰
            gc.collect()
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ (ì•ˆì „í•˜ê²Œ)
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
            except:
                pass
            
            self.current_memory_mb = 0.0
            self.logger.info(f"ğŸ§¹ ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬: {released_memory:.1f}MB í•´ì œ")
            return released_memory
            
        except Exception as e:
            self.logger.error(f"âŒ ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def get_memory_info(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        with self._lock:
            return {
                "current_mb": self.current_memory_mb,
                "max_mb": self.max_memory_mb,
                "available_mb": self.max_memory_mb - self.current_memory_mb,
                "usage_percent": (self.current_memory_mb / self.max_memory_mb) * 100
            }

# ==============================================
# ğŸ”¥ StepModelInterface v2.0 - ì™„ì „ í˜¸í™˜ì„±
# ==============================================

class StepModelInterface:
    """
    ğŸ”— Stepìš© ModelLoader ì¸í„°í˜ì´ìŠ¤ v2.0 - ì™„ì „ í˜¸í™˜ì„±
    
    âœ… BaseStepMixin ì™„ì „ í˜¸í™˜ì„± ë³´ì¥
    âœ… register_model_requirement ì™„ì „ êµ¬í˜„
    âœ… list_available_models í¬ê¸°ìˆœ ì •ë ¬
    âœ… í–¥ìƒëœ ìºì‹± ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
    âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
    """
    
    def __init__(self, step_name: str, model_loader: Optional['ModelLoader'] = None):
        self.step_name = step_name
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # ëª¨ë¸ ê´€ë¦¬
        self._model_cache: Dict[str, ModelCacheEntry] = {}
        self._model_requirements: Dict[str, ModelRequirement] = {}
        self._model_status: Dict[str, ModelStatus] = {}
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        max_memory = 8192.0 if self._is_m3_max() else 4096.0
        self.memory_manager = ModelMemoryManager(max_memory)
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        self._async_lock = asyncio.Lock()
        
        # í†µê³„ ë° ìƒíƒœ
        self.statistics = InterfaceStatistics()
        
        # ì„¤ì •
        self.auto_cleanup = True
        self.cache_policy = CachePolicy.MEMORY_FIRST
        self.max_cache_entries = 20
        
        # ì•½í•œ ì°¸ì¡°ë¡œ ëª¨ë¸ ì¶”ì  (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
        self._weak_model_refs: Dict[str, weakref.ref] = {}
        
        self.logger.info(f"ğŸ”— {step_name} StepInterface v2.0 ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _is_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    # ==============================================
    # ğŸ”¥ í•µì‹¬ ë©”ì„œë“œ: register_model_requirement
    # ==============================================
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """
        ğŸ”¥ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - BaseStepMixin ì™„ì „ í˜¸í™˜ êµ¬í˜„
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            model_type: ëª¨ë¸ íƒ€ì…
            **kwargs: ì¶”ê°€ ì„¤ì • (device, precision, input_size ë“±)
            
        Returns:
            bool: ë“±ë¡ ì„±ê³µ ì—¬ë¶€
        """
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹œì‘: {model_name} ({model_type})")
                
                # ModelRequirement ìƒì„±
                requirement = ModelRequirement(
                    model_name=model_name,
                    model_type=model_type,
                    device=kwargs.get("device", "auto"),
                    precision=kwargs.get("precision", "fp16"),
                    input_size=kwargs.get("input_size", (512, 512)),
                    num_classes=kwargs.get("num_classes"),
                    priority=kwargs.get("priority", 5),
                    min_memory_mb=kwargs.get("min_memory_mb", 100.0),
                    max_memory_mb=kwargs.get("max_memory_mb", 8192.0),
                    cache_policy=kwargs.get("cache_policy", CachePolicy.MEMORY_FIRST),
                    metadata={
                        "step_name": self.step_name,
                        "registered_by": "register_model_requirement",
                        **kwargs.get("metadata", {})
                    }
                )
                
                # ìš”êµ¬ì‚¬í•­ ì €ì¥
                self._model_requirements[model_name] = requirement
                self._model_status[model_name] = ModelStatus.NOT_LOADED
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.statistics.models_registered += 1
                self.statistics.last_activity = time.time()
                
                # ModelLoaderì— ì „ë‹¬ (ê°€ëŠ¥í•œ ê²½ìš°)
                if self.model_loader and hasattr(self.model_loader, 'register_model_requirement'):
                    try:
                        loader_success = self.model_loader.register_model_requirement(
                            model_name=model_name,
                            model_type=model_type,
                            step_name=self.step_name,
                            **kwargs
                        )
                        if loader_success:
                            self.logger.debug(f"âœ… ModelLoaderì— ìš”êµ¬ì‚¬í•­ ì „ë‹¬ ì„±ê³µ: {model_name}")
                        else:
                            self.logger.warning(f"âš ï¸ ModelLoader ìš”êµ¬ì‚¬í•­ ì „ë‹¬ ì‹¤íŒ¨: {model_name}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ìš”êµ¬ì‚¬í•­ ì „ë‹¬ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # ModelLoaderì— ì„¤ì • ë“±ë¡ ì‹œë„ (register_model_config)
                if self.model_loader and hasattr(self.model_loader, 'register_model_config'):
                    try:
                        config = {
                            "model_type": model_type,
                            "model_class": model_type,
                            "device": requirement.device,
                            "precision": requirement.precision,
                            "input_size": requirement.input_size,
                            "num_classes": requirement.num_classes,
                            "metadata": requirement.metadata
                        }
                        self.model_loader.register_model_config(model_name, config)
                        self.logger.debug(f"âœ… ModelLoader ì„¤ì • ë“±ë¡ ì„±ê³µ: {model_name}")
                    except Exception as e:
                        self.logger.debug(f"ModelLoader ì„¤ì • ë“±ë¡ ì‹¤íŒ¨: {e}")
                
                self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {model_name} ({model_type})")
                return True
                
        except Exception as e:
            self.statistics.loading_failures += 1
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    async def register_model_requirement_async(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """ë¹„ë™ê¸° ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        async with self._async_lock:
            # ë™ê¸° ë©”ì„œë“œë¥¼ executorì—ì„œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                None,
                self.register_model_requirement,
                model_name,
                model_type,
                **kwargs
            )
    
    # ==============================================
    # ğŸ”¥ í•µì‹¬ ë©”ì„œë“œ: list_available_models
    # ==============================================
    
    def list_available_models(
        self, 
        step_class: Optional[str] = None,
        model_type: Optional[str] = None,
        include_unloaded: bool = True,
        sort_by: str = "size"  # size, name, priority, load_time
    ) -> List[Dict[str, Any]]:
        """
        ğŸ”¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ - BaseStepMixin ì™„ì „ í˜¸í™˜
        
        Args:
            step_class: Step í´ë˜ìŠ¤ í•„í„°
            model_type: ëª¨ë¸ íƒ€ì… í•„í„°
            include_unloaded: ë¡œë“œë˜ì§€ ì•Šì€ ëª¨ë¸ í¬í•¨ ì—¬ë¶€
            sort_by: ì •ë ¬ ê¸°ì¤€
            
        Returns:
            List[Dict[str, Any]]: ëª¨ë¸ ëª©ë¡ (í¬ê¸°ìˆœ ì •ë ¬)
        """
        try:
            models = []
            
            with self._lock:
                # 1. ë“±ë¡ëœ ìš”êµ¬ì‚¬í•­ì—ì„œ ëª¨ë¸ ëª©ë¡ ìƒì„±
                for model_name, requirement in self._model_requirements.items():
                    # í•„í„°ë§
                    if step_class and step_class != self.step_name:
                        continue
                    if model_type and requirement.model_type != model_type:
                        continue
                    
                    # ìºì‹œì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    cache_entry = self._model_cache.get(model_name)
                    is_loaded = cache_entry is not None and cache_entry.status == ModelStatus.LOADED
                    
                    # ë¡œë“œë˜ì§€ ì•Šì€ ëª¨ë¸ ì œì™¸ (ì„¤ì •ì— ë”°ë¼)
                    if not include_unloaded and not is_loaded:
                        continue
                    
                    memory_mb = cache_entry.memory_mb if cache_entry else requirement.min_memory_mb
                    device = cache_entry.device if cache_entry else requirement.device
                    
                    model_info = {
                        "name": model_name,
                        "path": f"step_models/{self.step_name}/{model_name}",
                        "size_mb": memory_mb,
                        "model_type": requirement.model_type,
                        "step_class": self.step_name,
                        "loaded": is_loaded,
                        "device": device,
                        "status": self._model_status.get(model_name, ModelStatus.NOT_LOADED).value,
                        "priority": requirement.priority,
                        "metadata": {
                            "step_name": self.step_name,
                            "input_size": requirement.input_size,
                            "num_classes": requirement.num_classes,
                            "precision": requirement.precision,
                            "cache_policy": requirement.cache_policy.value,
                            "access_count": cache_entry.access_count if cache_entry else 0,
                            "last_access": cache_entry.last_access if cache_entry else 0,
                            "load_time": cache_entry.load_time if cache_entry else 0,
                            "error_count": cache_entry.error_count if cache_entry else 0,
                            "validation_passed": cache_entry.validation_passed if cache_entry else True,
                            **requirement.metadata
                        }
                    }
                    models.append(model_info)
                
                # 2. ModelLoaderì—ì„œ ì¶”ê°€ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ê°€ëŠ¥í•œ ê²½ìš°)
                if self.model_loader and hasattr(self.model_loader, 'list_available_models'):
                    try:
                        additional_models = self.model_loader.list_available_models(
                            step_class=self.step_name,
                            model_type=model_type
                        )
                        
                        # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                        existing_names = {m["name"] for m in models}
                        for model in additional_models:
                            if model["name"] not in existing_names:
                                # ModelLoaderì˜ ëª¨ë¸ ì •ë³´ë¥¼ StepInterface í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                model_info = {
                                    "name": model["name"],
                                    "path": model.get("path", f"loader_models/{model['name']}"),
                                    "size_mb": model.get("size_mb", 0.0),
                                    "model_type": model.get("model_type", "unknown"),
                                    "step_class": model.get("step_class", self.step_name),
                                    "loaded": model.get("loaded", False),
                                    "device": model.get("device", "auto"),
                                    "status": "loaded" if model.get("loaded", False) else "not_loaded",
                                    "priority": 5,  # ê¸°ë³¸ ìš°ì„ ìˆœìœ„
                                    "metadata": {
                                        "step_name": self.step_name,
                                        "source": "model_loader",
                                        "access_count": 0,
                                        "last_access": 0,
                                        "load_time": 0,
                                        "error_count": 0,
                                        "validation_passed": True,
                                        **model.get("metadata", {})
                                    }
                                }
                                models.append(model_info)
                                
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                
                # 3. ì •ë ¬ ìˆ˜í–‰
                if sort_by == "size":
                    models.sort(key=lambda x: x["size_mb"], reverse=True)  # í° ê²ƒë¶€í„°
                elif sort_by == "name":
                    models.sort(key=lambda x: x["name"])
                elif sort_by == "priority":
                    models.sort(key=lambda x: x["priority"])  # ì‘ì€ ê°’ì´ ë†’ì€ ìš°ì„ ìˆœìœ„
                elif sort_by == "load_time":
                    models.sort(key=lambda x: x["metadata"].get("load_time", 0), reverse=True)
                else:
                    # ê¸°ë³¸ê°’: í¬ê¸°ìˆœ ì •ë ¬
                    models.sort(key=lambda x: x["size_mb"], reverse=True)
                
                self.logger.debug(f"ğŸ“‹ ëª¨ë¸ ëª©ë¡ ë°˜í™˜: {len(models)}ê°œ (step={step_class}, type={model_type}, sort={sort_by})")
                return models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    # ==============================================
    # ğŸ”¥ ëª¨ë¸ ë¡œë”© ë©”ì„œë“œë“¤
    # ==============================================
    
    async def get_model_async(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ - BaseStepMixin í˜¸í™˜"""
        async with self._async_lock:
            try:
                self.statistics.last_activity = time.time()
                
                # ìºì‹œ í™•ì¸
                if model_name in self._model_cache:
                    cache_entry = self._model_cache[model_name]
                    if cache_entry.status == ModelStatus.LOADED and cache_entry.model is not None:
                        cache_entry.last_access = time.time()
                        cache_entry.access_count += 1
                        self.statistics.cache_hits += 1
                        self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                        return cache_entry.model
                    elif cache_entry.status == ModelStatus.ERROR:
                        self.logger.warning(f"âš ï¸ ì´ì „ì— ë¡œë”© ì‹¤íŒ¨í•œ ëª¨ë¸: {model_name}")
                        return None
                
                # ë¡œë”© ìƒíƒœ ì„¤ì •
                self._model_status[model_name] = ModelStatus.LOADING
                
                # ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ í™•ì¸
                requirement = self._model_requirements.get(model_name)
                if requirement:
                    if not self.memory_manager.can_load_model(requirement.max_memory_mb):
                        if self.auto_cleanup:
                            self.logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ìë™ ì •ë¦¬ ì‹¤í–‰: {model_name}")
                            self._cleanup_least_used_models()
                            
                        if not self.memory_manager.can_load_model(requirement.max_memory_mb):
                            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ ë¡œë”© ë¶ˆê°€: {model_name}")
                            self._model_status[model_name] = ModelStatus.ERROR
                            return None
                
                # ModelLoaderë¥¼ í†µí•œ ì•ˆì „í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                model = await self._safe_load_model(model_name, **kwargs)
                
                if model is not None:
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
                    memory_usage = self._estimate_model_memory(model)
                    
                    # ë©”ëª¨ë¦¬ ì˜ˆì•½
                    self.memory_manager.reserve_memory(memory_usage)
                    
                    # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                    cache_entry = ModelCacheEntry(
                        model=model,
                        status=ModelStatus.LOADED,
                        load_time=time.time(),
                        last_access=time.time(),
                        access_count=1,
                        memory_mb=memory_usage,
                        device=getattr(model, 'device', 'cpu') if hasattr(model, 'device') else 'cpu',
                        step_name=self.step_name,
                        requirement=requirement,
                        validation_passed=True,
                        error_count=0,
                        metadata={
                            "loading_method": "async",
                            "kwargs": kwargs
                        }
                    )
                    
                    # ìºì‹œ ê´€ë¦¬
                    self._manage_cache_size()
                    
                    # ìºì‹œì— ì €ì¥
                    with self._lock:
                        self._model_cache[model_name] = cache_entry
                        self._model_status[model_name] = ModelStatus.LOADED
                        
                        # ì•½í•œ ì°¸ì¡° ì €ì¥ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
                        self._weak_model_refs[model_name] = weakref.ref(model)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.statistics.models_loaded += 1
                    self.statistics.total_memory_mb += memory_usage
                    self._update_average_load_time(cache_entry.load_time)
                    
                    self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name} ({memory_usage:.1f}MB)")
                    return model
                
                # ë¡œë”© ì‹¤íŒ¨
                self._model_status[model_name] = ModelStatus.ERROR
                self.statistics.loading_failures += 1
                self.statistics.cache_misses += 1
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
            except Exception as e:
                self._model_status[model_name] = ModelStatus.ERROR
                self.statistics.loading_failures += 1
                self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
                return None
    
    def get_model_sync(self, model_name: str, **kwargs) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ë¡œë“œ - BaseStepMixin í˜¸í™˜"""
        try:
            self.statistics.last_activity = time.time()
            
            # ìºì‹œ í™•ì¸
            with self._lock:
                if model_name in self._model_cache:
                    cache_entry = self._model_cache[model_name]
                    if cache_entry.status == ModelStatus.LOADED and cache_entry.model is not None:
                        cache_entry.last_access = time.time()
                        cache_entry.access_count += 1
                        self.statistics.cache_hits += 1
                        self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                        return cache_entry.model
                    elif cache_entry.status == ModelStatus.ERROR:
                        self.logger.warning(f"âš ï¸ ì´ì „ì— ë¡œë”© ì‹¤íŒ¨í•œ ëª¨ë¸: {model_name}")
                        return None
            
            # ë¡œë”© ìƒíƒœ ì„¤ì •
            self._model_status[model_name] = ModelStatus.LOADING
            
            # ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ í™•ì¸
            requirement = self._model_requirements.get(model_name)
            if requirement:
                if not self.memory_manager.can_load_model(requirement.max_memory_mb):
                    if self.auto_cleanup:
                        self.logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ìë™ ì •ë¦¬ ì‹¤í–‰: {model_name}")
                        self._cleanup_least_used_models()
                        
                    if not self.memory_manager.can_load_model(requirement.max_memory_mb):
                        self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ ë¡œë”© ë¶ˆê°€: {model_name}")
                        self._model_status[model_name] = ModelStatus.ERROR
                        return None
            
            # ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            model = None
            if self.model_loader and hasattr(self.model_loader, 'load_model'):
                try:
                    model = self.model_loader.load_model(model_name, **kwargs)
                except Exception as e:
                    self.logger.error(f"âŒ ModelLoaderë¥¼ í†µí•œ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            if model is not None:
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
                memory_usage = self._estimate_model_memory(model)
                
                # ë©”ëª¨ë¦¬ ì˜ˆì•½
                self.memory_manager.reserve_memory(memory_usage)
                
                # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                cache_entry = ModelCacheEntry(
                    model=model,
                    status=ModelStatus.LOADED,
                    load_time=time.time(),
                    last_access=time.time(),
                    access_count=1,
                    memory_mb=memory_usage,
                    device=getattr(model, 'device', 'cpu') if hasattr(model, 'device') else 'cpu',
                    step_name=self.step_name,
                    requirement=requirement,
                    validation_passed=True,
                    error_count=0,
                    metadata={
                        "loading_method": "sync",
                        "kwargs": kwargs
                    }
                )
                
                # ìºì‹œ ê´€ë¦¬
                self._manage_cache_size()
                
                # ìºì‹œì— ì €ì¥
                with self._lock:
                    self._model_cache[model_name] = cache_entry
                    self._model_status[model_name] = ModelStatus.LOADED
                    
                    # ì•½í•œ ì°¸ì¡° ì €ì¥
                    self._weak_model_refs[model_name] = weakref.ref(model)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.statistics.models_loaded += 1
                self.statistics.total_memory_mb += memory_usage
                self._update_average_load_time(cache_entry.load_time)
                
                self.logger.info(f"âœ… ë™ê¸° ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name} ({memory_usage:.1f}MB)")
                return model
            
            # ë¡œë”© ì‹¤íŒ¨
            self._model_status[model_name] = ModelStatus.ERROR
            self.statistics.loading_failures += 1
            self.statistics.cache_misses += 1
            self.logger.warning(f"âš ï¸ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
            return None
            
        except Exception as e:
            self._model_status[model_name] = ModelStatus.ERROR
            self.statistics.loading_failures += 1
            self.logger.error(f"âŒ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            return None
    
    async def _safe_load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ì•ˆì „í•œ ëª¨ë¸ ë¡œë”©"""
        try:
            if self.model_loader:
                if hasattr(self.model_loader, 'load_model_async'):
                    return await self.model_loader.load_model_async(model_name, **kwargs)
                elif hasattr(self.model_loader, 'load_model'):
                    # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                    loop = asyncio.get_event_loop()
                    return await loop.run_in_executor(
                        None, 
                        lambda: self.model_loader.load_model(model_name, **kwargs)
                    )
            
            self.logger.error(f"âŒ ModelLoaderê°€ ì—†ê±°ë‚˜ ë¡œë”© ë©”ì„œë“œ ì—†ìŒ")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì•ˆì „í•œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    # BaseStepMixin í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
    async def get_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë¹„ë™ê¸° ìš°ì„ ) - BaseStepMixin í˜¸í™˜"""
        return await self.get_model_async(model_name, **kwargs)
    
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë™ê¸°) - BaseStepMixin í˜¸í™˜"""
        return self.get_model_sync(model_name, **kwargs)
    
    # ==============================================
    # ğŸ”¥ ëª¨ë¸ ìƒíƒœ ë° ê´€ë¦¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_model_status(self, model_name: Optional[str] = None) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ - BaseStepMixin í˜¸í™˜"""
        try:
            if model_name:
                # íŠ¹ì • ëª¨ë¸ ìƒíƒœ
                with self._lock:
                    if model_name in self._model_cache:
                        cache_entry = self._model_cache[model_name]
                        return {
                            "name": model_name,
                            "status": cache_entry.status.value,
                            "loaded": cache_entry.status == ModelStatus.LOADED,
                            "device": cache_entry.device,
                            "memory_mb": cache_entry.memory_mb,
                            "load_time": cache_entry.load_time,
                            "last_access": cache_entry.last_access,
                            "access_count": cache_entry.access_count,
                            "validation_passed": cache_entry.validation_passed,
                            "error_count": cache_entry.error_count,
                            "last_error": cache_entry.last_error,
                            "metadata": cache_entry.metadata
                        }
                    else:
                        status = self._model_status.get(model_name, ModelStatus.NOT_LOADED)
                        requirement = self._model_requirements.get(model_name)
                        return {
                            "name": model_name,
                            "status": status.value,
                            "loaded": False,
                            "device": requirement.device if requirement else "unknown",
                            "memory_mb": 0,
                            "load_time": 0,
                            "last_access": 0,
                            "access_count": 0,
                            "validation_passed": True,
                            "error_count": 0,
                            "last_error": None,
                            "metadata": {}
                        }
            else:
                # ì „ì²´ ìƒíƒœ
                with self._lock:
                    models_status = {}
                    all_model_names = set(self._model_requirements.keys()) | set(self._model_cache.keys())
                    
                    for name in all_model_names:
                        models_status[name] = self.get_model_status(name)
                    
                    memory_info = self.memory_manager.get_memory_info()
                    
                    return {
                        "step_name": self.step_name,
                        "models": models_status,
                        "total_models": len(self._model_requirements),
                        "loaded_models": len([
                            entry for entry in self._model_cache.values() 
                            if entry.status == ModelStatus.LOADED
                        ]),
                        "cache_entries": len(self._model_cache),
                        "memory_info": memory_info,
                        "statistics": {
                            "models_registered": self.statistics.models_registered,
                            "models_loaded": self.statistics.models_loaded,
                            "cache_hits": self.statistics.cache_hits,
                            "cache_misses": self.statistics.cache_misses,
                            "loading_failures": self.statistics.loading_failures,
                            "average_load_time": self.statistics.average_load_time,
                            "total_memory_mb": self.statistics.total_memory_mb
                        },
                        "creation_time": self.statistics.creation_time,
                        "last_activity": self.statistics.last_activity,
                        "version": "2.0"
                    }
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def unload_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                if model_name in self._model_cache:
                    cache_entry = self._model_cache[model_name]
                    
                    # ë©”ëª¨ë¦¬ í•´ì œ
                    self.memory_manager.release_memory(cache_entry.memory_mb)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.statistics.total_memory_mb -= cache_entry.memory_mb
                    
                    # ìºì‹œì—ì„œ ì œê±°
                    del self._model_cache[model_name]
                    
                    # ì•½í•œ ì°¸ì¡° ì œê±°
                    if model_name in self._weak_model_refs:
                        del self._weak_model_refs[model_name]
                    
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    self._model_status[model_name] = ModelStatus.NOT_LOADED
                    
                    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                    gc.collect()
                    
                    self.logger.info(f"âœ… ëª¨ë¸ ì–¸ë¡œë“œ: {model_name} ({cache_entry.memory_mb:.1f}MB í•´ì œ)")
                    return True
                else:
                    self.logger.warning(f"âš ï¸ ì–¸ë¡œë“œí•  ëª¨ë¸ì´ ìºì‹œì— ì—†ìŒ: {model_name}")
                    return False
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    def clear_cache(self, force: bool = False) -> bool:
        """ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™”"""
        try:
            with self._lock:
                if not force and len(self._model_cache) > 0:
                    # ê°•ì œê°€ ì•„ë‹Œ ê²½ìš° í™•ì¸
                    self.logger.warning(f"âš ï¸ {len(self._model_cache)}ê°œ ëª¨ë¸ì´ ìºì‹œì— ìˆìŒ")
                
                # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
                unloaded_count = 0
                for model_name in list(self._model_cache.keys()):
                    if self.unload_model(model_name):
                        unloaded_count += 1
                
                # ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
                released_memory = self.memory_manager.force_cleanup()
                
                # ìƒíƒœ ì´ˆê¸°í™”
                for model_name in self._model_status:
                    self._model_status[model_name] = ModelStatus.NOT_LOADED
                
                # í†µê³„ ë¦¬ì…‹ (ì¼ë¶€)
                self.statistics.total_memory_mb = 0.0
                self.statistics.last_activity = time.time()
                
                self.logger.info(f"ğŸ§¹ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ: {unloaded_count}ê°œ ëª¨ë¸ ì–¸ë¡œë“œ, {released_memory:.1f}MB í•´ì œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ ë‚´ë¶€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def _estimate_model_memory(self, model) -> float:
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (MB)"""
        try:
            if model is None:
                return 0.0
                
            # PyTorch ëª¨ë¸
            if hasattr(model, 'parameters'):
                total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                return total_params * 4 / (1024 * 1024)  # float32 ê¸°ì¤€
            
            # state_dict
            elif isinstance(model, dict):
                total_size = 0
                for tensor in model.values():
                    if hasattr(tensor, 'numel'):
                        total_size += tensor.numel() * 4  # float32 ê¸°ì¤€
                return total_size / (1024 * 1024)
            
            # ê¸°ë³¸ ì¶”ì •ì¹˜
            else:
                return 100.0
                
        except Exception:
            return 100.0  # ê¸°ë³¸ê°’
    
    def _update_average_load_time(self, load_time: float):
        """í‰ê·  ë¡œë”© ì‹œê°„ ì—…ë°ì´íŠ¸"""
        try:
            current_avg = self.statistics.average_load_time
            loaded_count = self.statistics.models_loaded
            
            if loaded_count <= 1:
                self.statistics.average_load_time = load_time
            else:
                # ì´ë™ í‰ê·  ê³„ì‚°
                self.statistics.average_load_time = (
                    (current_avg * (loaded_count - 1) + load_time) / loaded_count
                )
        except Exception:
            pass
    
    def _manage_cache_size(self):
        """ìºì‹œ í¬ê¸° ê´€ë¦¬"""
        try:
            with self._lock:
                if len(self._model_cache) >= self.max_cache_entries:
                    self.logger.info(f"ğŸ“¦ ìºì‹œ í¬ê¸° ì´ˆê³¼ ({len(self._model_cache)}/{self.max_cache_entries}), ì •ë¦¬ ì‹¤í–‰")
                    self._cleanup_least_used_models(1)
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ í¬ê¸° ê´€ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _cleanup_least_used_models(self, count: int = 1):
        """ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ ëª¨ë¸ë“¤ ì •ë¦¬"""
        try:
            with self._lock:
                if len(self._model_cache) <= count:
                    return
                
                # ì ‘ê·¼ ì‹œê°„ê³¼ íšŸìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                sorted_models = sorted(
                    self._model_cache.items(),
                    key=lambda x: (x[1].last_access, x[1].access_count)
                )
                
                # ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ ëª¨ë¸ë“¤ ì–¸ë¡œë“œ
                for i in range(min(count, len(sorted_models))):
                    model_name, cache_entry = sorted_models[i]
                    self.logger.info(f"ğŸ§¹ ì‚¬ìš© ë¹ˆë„ ë‚®ì€ ëª¨ë¸ ì •ë¦¬: {model_name} (ì ‘ê·¼: {cache_entry.access_count}íšŒ)")
                    self.unload_model(model_name)
                    
        except Exception as e:
            self.logger.error(f"âŒ ìµœì†Œ ì‚¬ìš© ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _cleanup_dead_references(self):
        """ì£½ì€ ì•½í•œ ì°¸ì¡° ì •ë¦¬"""
        try:
            with self._lock:
                dead_refs = []
                for model_name, weak_ref in self._weak_model_refs.items():
                    if weak_ref() is None:
                        dead_refs.append(model_name)
                
                for model_name in dead_refs:
                    del self._weak_model_refs[model_name]
                    if model_name in self._model_cache:
                        cache_entry = self._model_cache[model_name]
                        self.memory_manager.release_memory(cache_entry.memory_mb)
                        del self._model_cache[model_name]
                        self._model_status[model_name] = ModelStatus.NOT_LOADED
                
                if dead_refs:
                    self.logger.info(f"ğŸ§¹ ì£½ì€ ì°¸ì¡° ì •ë¦¬: {len(dead_refs)}ê°œ")
                    
        except Exception as e:
            self.logger.error(f"âŒ ì£½ì€ ì°¸ì¡° ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ê³ ê¸‰ ê¸°ëŠ¥ë“¤
    # ==============================================
    
    def get_requirements(self) -> Dict[str, ModelRequirement]:
        """ë“±ë¡ëœ ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
        with self._lock:
            return self._model_requirements.copy()
    
    def get_cache_info(self) -> Dict[str, Any]:
        """ìºì‹œ ì •ë³´ ë°˜í™˜"""
        try:
            with self._lock:
                memory_info = self.memory_manager.get_memory_info()
                
                return {
                    "cache_size": len(self._model_cache),
                    "max_cache_size": self.max_cache_entries,
                    "weak_references": len(self._weak_model_refs),
                    "memory_info": memory_info,
                    "cache_policy": self.cache_policy.value,
                    "auto_cleanup": self.auto_cleanup,
                    "loaded_models": [
                        {
                            "name": name,
                            "memory_mb": entry.memory_mb,
                            "access_count": entry.access_count,
                            "last_access": entry.last_access
                        }
                        for name, entry in self._model_cache.items()
                        if entry.status == ModelStatus.LOADED
                    ]
                }
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def get_statistics(self) -> Dict[str, Any]:
        """ìƒì„¸ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            with self._lock:
                return {
                    "models_registered": self.statistics.models_registered,
                    "models_loaded": self.statistics.models_loaded,
                    "cache_hits": self.statistics.cache_hits,
                    "cache_misses": self.statistics.cache_misses,
                    "loading_failures": self.statistics.loading_failures,
                    "total_memory_mb": self.statistics.total_memory_mb,
                    "average_load_time": self.statistics.average_load_time,
                    "creation_time": self.statistics.creation_time,
                    "last_activity": self.statistics.last_activity,
                    "uptime_seconds": time.time() - self.statistics.creation_time,
                    "cache_hit_rate": (
                        self.statistics.cache_hits / 
                        max(1, self.statistics.cache_hits + self.statistics.cache_misses)
                    ) * 100,
                    "success_rate": (
                        self.statistics.models_loaded / 
                        max(1, self.statistics.models_loaded + self.statistics.loading_failures)
                    ) * 100,
                    "memory_efficiency": self.memory_manager.get_memory_info()
                }
        except Exception as e:
            self.logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def optimize_cache(self):
        """ìºì‹œ ìµœì í™” ì‹¤í–‰"""
        try:
            self.logger.info("ğŸ”§ ìºì‹œ ìµœì í™” ì‹œì‘...")
            
            # ì£½ì€ ì°¸ì¡° ì •ë¦¬
            self._cleanup_dead_references()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì€ ê²½ìš° ì •ë¦¬
            memory_info = self.memory_manager.get_memory_info()
            if memory_info["usage_percent"] > 80:
                cleanup_count = max(1, len(self._model_cache) // 4)
                self._cleanup_least_used_models(cleanup_count)
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.logger.info("âœ… ìºì‹œ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ì •ë¦¬ ë©”ì„œë“œ
    # ==============================================
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info(f"ğŸ§¹ {self.step_name} Interface ì •ë¦¬ ì‹œì‘...")
            
            # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
            self.clear_cache(force=True)
            
            # ìš”êµ¬ì‚¬í•­ ì •ë¦¬
            self._model_requirements.clear()
            self._model_status.clear()
            
            # ì•½í•œ ì°¸ì¡° ì •ë¦¬
            self._weak_model_refs.clear()
            
            self.logger.info(f"âœ… {self.step_name} Interface ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Interface ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup()
        except:
            pass

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_step_model_interface(
    step_name: str, 
    model_loader: Optional['ModelLoader'] = None,
    max_cache_entries: int = 20,
    auto_cleanup: bool = True,
    cache_policy: CachePolicy = CachePolicy.MEMORY_FIRST
) -> StepModelInterface:
    """Step Model Interface ìƒì„±"""
    try:
        interface = StepModelInterface(step_name, model_loader)
        
        # ì„¤ì • ì ìš©
        interface.max_cache_entries = max_cache_entries
        interface.auto_cleanup = auto_cleanup
        interface.cache_policy = cache_policy
        
        logger.info(f"âœ… Step Interface ìƒì„± ì™„ë£Œ: {step_name}")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ Step Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        # í´ë°± ì¸í„°í˜ì´ìŠ¤
        return StepModelInterface(step_name, None)

def create_optimized_step_interface(
    step_name: str,
    model_loader: Optional['ModelLoader'] = None,
    memory_limit_mb: float = None
) -> StepModelInterface:
    """ìµœì í™”ëœ Step Interface ìƒì„± (M3 Max ëŒ€ì‘)"""
    try:
        # M3 Max ê°ì§€
        is_m3_max = False
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                is_m3_max = 'M3' in result.stdout
        except:
            pass
        
        # M3 Maxì— ë§ëŠ” ì„¤ì •
        if is_m3_max:
            max_cache = 30
            memory_limit = memory_limit_mb or 16384.0  # 16GB
            auto_cleanup = True
            cache_policy = CachePolicy.HYBRID
        else:
            max_cache = 15
            memory_limit = memory_limit_mb or 4096.0   # 4GB
            auto_cleanup = True
            cache_policy = CachePolicy.MEMORY_FIRST
        
        interface = create_step_model_interface(
            step_name=step_name,
            model_loader=model_loader,
            max_cache_entries=max_cache,
            auto_cleanup=auto_cleanup,
            cache_policy=cache_policy
        )
        
        # ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
        interface.memory_manager.max_memory_mb = memory_limit
        
        logger.info(f"âœ… ìµœì í™”ëœ Step Interface ìƒì„±: {step_name} (M3 Max: {is_m3_max})")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ ìµœì í™”ëœ Step Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        return create_step_model_interface(step_name, model_loader)

# ==============================================
# ğŸ”¥ Export
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'StepModelInterface',
    'ModelMemoryManager',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'ModelRequirement',
    'ModelCacheEntry',
    'InterfaceStatistics',
    'ModelStatus',
    'CachePolicy',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_step_model_interface',
    'create_optimized_step_interface'
]

# ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ
logger.info("=" * 80)
logger.info("ğŸ”— StepModelInterface v2.0 - ì™„ì „ í˜¸í™˜ì„± + ìˆœí™˜ì°¸ì¡° í•´ê²°")
logger.info("=" * 80)
logger.info("âœ… BaseStepMixin ì™„ì „ í˜¸í™˜ì„± ë³´ì¥")
logger.info("âœ… register_model_requirement ì™„ì „ êµ¬í˜„")
logger.info("âœ… list_available_models í¬ê¸°ìˆœ ì •ë ¬")
logger.info("âœ… í–¥ìƒëœ ìºì‹± ë° ë©”ëª¨ë¦¬ ê´€ë¦¬")
logger.info("âœ… ì•½í•œ ì°¸ì¡° ê¸°ë°˜ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
logger.info("=" * 80)