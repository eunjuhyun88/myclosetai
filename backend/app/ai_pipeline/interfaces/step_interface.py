# backend/app/ai_pipeline/interface/step_interface.py
"""
ğŸ”¥ Step Model Interface - BaseStepMixin í˜¸í™˜ ì™„ì „ êµ¬í˜„
=====================================================
âœ… register_model_requirement ë©”ì„œë“œ ì¶”ê°€
âœ… ë¹„ë™ê¸° ë©”ì„œë“œ ì™„ì „ êµ¬í˜„
âœ… list_available_models ë©”ì„œë“œ í¬í•¨
âœ… conda í™˜ê²½ ìµœì í™”
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ í™œìš©
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€

Author: MyCloset AI Team  
Date: 2025-07-24
Version: 1.0 (Complete Implementation)
"""

import logging
import threading
import asyncio
import time
from typing import Dict, Any, Optional, List, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import weakref
import gc

logger = logging.getLogger(__name__)

# =============================================================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° ì •ì˜
# =============================================================================

class ModelStatus(Enum):
    """ëª¨ë¸ ìƒíƒœ"""
    NOT_LOADED = "not_loaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"

@dataclass
class ModelRequirement:
    """ëª¨ë¸ ìš”êµ¬ì‚¬í•­"""
    model_name: str
    model_type: str = "BaseModel"
    device: str = "auto"
    precision: str = "fp16"
    input_size: tuple = (512, 512)
    num_classes: Optional[int] = None
    priority: int = 5
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ModelCacheEntry:
    """ëª¨ë¸ ìºì‹œ ì—”íŠ¸ë¦¬"""
    model: Any
    status: ModelStatus
    load_time: float
    last_access: float
    access_count: int
    memory_mb: float
    device: str
    metadata: Dict[str, Any] = field(default_factory=dict)

# =============================================================================
# ğŸ”¥ StepModelInterface ì™„ì „ êµ¬í˜„
# =============================================================================

class StepModelInterface:
    """
    ğŸ”— Stepìš© ModelLoader ì¸í„°í˜ì´ìŠ¤ (BaseStepMixin ì™„ì „ í˜¸í™˜)
    âœ… register_model_requirement ë©”ì„œë“œ êµ¬í˜„ (í•µì‹¬!)
    âœ… list_available_models ë©”ì„œë“œ êµ¬í˜„
    âœ… ë¹„ë™ê¸° ë©”ì„œë“œ ì™„ì „ ì§€ì›
    âœ… conda í™˜ê²½ ìµœì í™”
    """
    
    def __init__(self, step_name: str, model_loader=None):
        self.step_name = step_name
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # ëª¨ë¸ ê´€ë¦¬
        self._model_cache: Dict[str, ModelCacheEntry] = {}
        self._model_requirements: Dict[str, ModelRequirement] = {}
        self._model_status: Dict[str, ModelStatus] = {}
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        self._async_lock = asyncio.Lock()
        
        # ì„±ëŠ¥ ì¶”ì 
        self._stats = {
            "models_loaded": 0,
            "cache_hits": 0,
            "requirements_registered": 0,
            "errors": 0
        }
        
        # ì‹œìŠ¤í…œ ì •ë³´
        self._creation_time = time.time()
        self._last_cleanup = time.time()
        
        self.logger.info(f"ğŸ”— {step_name} Step Interface ì´ˆê¸°í™” ì™„ë£Œ")
    
    # =============================================================================
    # ğŸ”¥ í•µì‹¬ ë©”ì„œë“œ: register_model_requirement (ì˜¤ë¥˜ í•´ê²°!)
    # =============================================================================
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """
        ğŸ”¥ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - BaseStepMixinì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ
        âœ… QualityAssessmentStep ì˜¤ë¥˜ í•´ê²°
        """
        try:
            with self._lock:
                requirement = ModelRequirement(
                    model_name=model_name,
                    model_type=model_type,
                    device=kwargs.get("device", "auto"),
                    precision=kwargs.get("precision", "fp16"),
                    input_size=kwargs.get("input_size", (512, 512)),
                    num_classes=kwargs.get("num_classes"),
                    priority=kwargs.get("priority", 5),
                    metadata=kwargs.get("metadata", {})
                )
                
                self._model_requirements[model_name] = requirement
                self._model_status[model_name] = ModelStatus.NOT_LOADED
                self._stats["requirements_registered"] += 1
                
                # ModelLoaderì— ì „ë‹¬ (ê°€ëŠ¥í•œ ê²½ìš°)
                if self.model_loader and hasattr(self.model_loader, 'register_model_config'):
                    try:
                        config = {
                            "model_type": model_type,
                            "model_class": model_type,
                            "device": requirement.device,
                            "precision": requirement.precision,
                            "input_size": requirement.input_size,
                            "num_classes": requirement.num_classes,
                            "metadata": requirement.metadata
                        }
                        self.model_loader.register_model_config(model_name, config)
                        self.logger.debug(f"âœ… ModelLoaderì— ì „ë‹¬: {model_name}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ì „ë‹¬ ì‹¤íŒ¨: {e}")
                
                self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
                return True
                
        except Exception as e:
            self._stats["errors"] += 1
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    async def register_model_requirement_async(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """ë¹„ë™ê¸° ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        async with self._async_lock:
            return self.register_model_requirement(model_name, model_type, **kwargs)
    
    # =============================================================================
    # ğŸ”¥ í•µì‹¬ ë©”ì„œë“œ: list_available_models (BaseStepMixin í•„ìˆ˜)
    # =============================================================================
    
    def list_available_models(
        self, 
        step_class: Optional[str] = None,
        model_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        ğŸ”¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ - BaseStepMixinì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ
        âœ… í¬ê¸°ìˆœ ì •ë ¬ (í° ê²ƒë¶€í„°)
        """
        try:
            models = []
            
            # 1. ë“±ë¡ëœ ìš”êµ¬ì‚¬í•­ì—ì„œ ëª¨ë¸ ëª©ë¡ ìƒì„±
            with self._lock:
                for model_name, requirement in self._model_requirements.items():
                    # í•„í„°ë§
                    if step_class and step_class != self.step_name:
                        continue
                    if model_type and requirement.model_type != model_type:
                        continue
                    
                    # ìºì‹œì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    cache_entry = self._model_cache.get(model_name)
                    is_loaded = cache_entry is not None
                    memory_mb = cache_entry.memory_mb if cache_entry else 0.0
                    
                    model_info = {
                        "name": model_name,
                        "path": f"step_models/{self.step_name}/{model_name}",
                        "size_mb": memory_mb,
                        "model_type": requirement.model_type,
                        "step_class": self.step_name,
                        "loaded": is_loaded,
                        "device": requirement.device,
                        "status": self._model_status.get(model_name, ModelStatus.NOT_LOADED).value,
                        "priority": requirement.priority,
                        "metadata": {
                            "step_name": self.step_name,
                            "input_size": requirement.input_size,
                            "num_classes": requirement.num_classes,
                            "precision": requirement.precision,
                            "access_count": cache_entry.access_count if cache_entry else 0,
                            "last_access": cache_entry.last_access if cache_entry else 0,
                            **requirement.metadata
                        }
                    }
                    models.append(model_info)
            
            # 2. ModelLoaderì—ì„œ ì¶”ê°€ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ê°€ëŠ¥í•œ ê²½ìš°)
            if self.model_loader and hasattr(self.model_loader, 'list_available_models'):
                try:
                    additional_models = self.model_loader.list_available_models(
                        step_class=self.step_name,
                        model_type=model_type
                    )
                    
                    # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                    existing_names = {m["name"] for m in models}
                    for model in additional_models:
                        if model["name"] not in existing_names:
                            models.append(model)
                            
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ModelLoader ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # 3. í¬ê¸°ìˆœ ì •ë ¬ (í° ê²ƒë¶€í„°)
            models.sort(key=lambda x: x["size_mb"], reverse=True)
            
            self.logger.debug(f"ğŸ“‹ ëª¨ë¸ ëª©ë¡ ë°˜í™˜: {len(models)}ê°œ (step={step_class}, type={model_type})")
            return models
            
        except Exception as e:
            self._stats["errors"] += 1
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    # =============================================================================
    # ğŸ”¥ ëª¨ë¸ ë¡œë”© ë©”ì„œë“œë“¤
    # =============================================================================
    
    async def get_model_async(self, model_name: str) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
        async with self._async_lock:
            try:
                # ìºì‹œ í™•ì¸
                if model_name in self._model_cache:
                    cache_entry = self._model_cache[model_name]
                    cache_entry.last_access = time.time()
                    cache_entry.access_count += 1
                    self._stats["cache_hits"] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return cache_entry.model
                
                # ModelLoaderë¥¼ í†µí•œ ë¡œë”©
                model = None
                if self.model_loader:
                    if hasattr(self.model_loader, 'load_model_async'):
                        model = await self.model_loader.load_model_async(model_name)
                    elif hasattr(self.model_loader, 'load_model'):
                        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                        loop = asyncio.get_event_loop()
                        model = await loop.run_in_executor(
                            None, 
                            self.model_loader.load_model, 
                            model_name
                        )
                
                if model is not None:
                    # ìºì‹œì— ì €ì¥
                    cache_entry = ModelCacheEntry(
                        model=model,
                        status=ModelStatus.LOADED,
                        load_time=time.time(),
                        last_access=time.time(),
                        access_count=1,
                        memory_mb=self._estimate_model_size(model),
                        device=getattr(model, 'device', 'cpu'),
                        metadata={"source": "model_loader"}
                    )
                    
                    self._model_cache[model_name] = cache_entry
                    self._model_status[model_name] = ModelStatus.LOADED
                    self._stats["models_loaded"] += 1
                    
                    self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                    return model
                
                self._model_status[model_name] = ModelStatus.ERROR
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
            except Exception as e:
                self._stats["errors"] += 1
                self._model_status[model_name] = ModelStatus.ERROR
                self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
                return None
    
    def get_model_sync(self, model_name: str) -> Optional[Any]:
        """ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
        try:
            # ìºì‹œ í™•ì¸
            with self._lock:
                if model_name in self._model_cache:
                    cache_entry = self._model_cache[model_name]
                    cache_entry.last_access = time.time()
                    cache_entry.access_count += 1
                    self._stats["cache_hits"] += 1
                    return cache_entry.model
            
            # ModelLoaderë¥¼ í†µí•œ ë¡œë”©
            model = None
            if self.model_loader and hasattr(self.model_loader, 'load_model'):
                model = self.model_loader.load_model(model_name)
            
            if model is not None:
                with self._lock:
                    cache_entry = ModelCacheEntry(
                        model=model,
                        status=ModelStatus.LOADED,
                        load_time=time.time(),
                        last_access=time.time(),
                        access_count=1,
                        memory_mb=self._estimate_model_size(model),
                        device=getattr(model, 'device', 'cpu'),
                        metadata={"source": "model_loader"}
                    )
                    
                    self._model_cache[model_name] = cache_entry
                    self._model_status[model_name] = ModelStatus.LOADED
                    self._stats["models_loaded"] += 1
                
                return model
            
            self._model_status[model_name] = ModelStatus.ERROR
            return None
            
        except Exception as e:
            self._stats["errors"] += 1
            self._model_status[model_name] = ModelStatus.ERROR
            self.logger.error(f"âŒ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            return None
    
    # ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
    async def get_model(self, model_name: str) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë¹„ë™ê¸° ìš°ì„ )"""
        return await self.get_model_async(model_name)
    
    def load_model(self, model_name: str) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë™ê¸°)"""
        return self.get_model_sync(model_name)
    
    # =============================================================================
    # ğŸ”¥ ëª¨ë¸ ìƒíƒœ ê´€ë¦¬ ë©”ì„œë“œë“¤
    # =============================================================================
    
    def get_model_status(self, model_name: Optional[str] = None) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            if model_name:
                # íŠ¹ì • ëª¨ë¸ ìƒíƒœ
                with self._lock:
                    if model_name in self._model_cache:
                        cache_entry = self._model_cache[model_name]
                        return {
                            "name": model_name,
                            "status": self._model_status.get(model_name, ModelStatus.NOT_LOADED).value,
                            "loaded": True,
                            "device": cache_entry.device,
                            "memory_mb": cache_entry.memory_mb,
                            "load_time": cache_entry.load_time,
                            "last_access": cache_entry.last_access,
                            "access_count": cache_entry.access_count,
                            "metadata": cache_entry.metadata
                        }
                    else:
                        return {
                            "name": model_name,
                            "status": self._model_status.get(model_name, ModelStatus.NOT_LOADED).value,
                            "loaded": False,
                            "device": None,
                            "memory_mb": 0,
                            "load_time": 0,
                            "last_access": 0,
                            "access_count": 0,
                            "metadata": {}
                        }
            else:
                # ì „ì²´ ìƒíƒœ
                with self._lock:
                    models_status = {}
                    for name in set(list(self._model_requirements.keys()) + list(self._model_cache.keys())):
                        models_status[name] = self.get_model_status(name)
                    
                    return {
                        "step_name": self.step_name,
                        "models": models_status,
                        "total_models": len(self._model_requirements),
                        "loaded_models": len(self._model_cache),
                        "total_memory_mb": sum(entry.memory_mb for entry in self._model_cache.values()),
                        "stats": self._stats.copy(),
                        "creation_time": self._creation_time,
                        "last_cleanup": self._last_cleanup
                    }
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def unload_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            with self._lock:
                if model_name in self._model_cache:
                    del self._model_cache[model_name]
                    self._model_status[model_name] = ModelStatus.NOT_LOADED
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    gc.collect()
                    
                    self.logger.info(f"âœ… ëª¨ë¸ ì–¸ë¡œë“œ: {model_name}")
                    return True
                return False
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    def clear_cache(self) -> bool:
        """ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™”"""
        try:
            with self._lock:
                self._model_cache.clear()
                for name in self._model_status:
                    self._model_status[name] = ModelStatus.NOT_LOADED
                
                self._last_cleanup = time.time()
                gc.collect()
                
                self.logger.info("ğŸ§¹ ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
                return True
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    # =============================================================================
    # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # =============================================================================
    
    def _estimate_model_size(self, model) -> float:
        """ëª¨ë¸ í¬ê¸° ì¶”ì • (MB)"""
        try:
            if hasattr(model, 'parameters'):
                # PyTorch ëª¨ë¸
                total_params = sum(p.numel() for p in model.parameters())
                return total_params * 4 / (1024 * 1024)  # float32 ê¸°ì¤€
            elif isinstance(model, dict):
                # State dict
                total_size = 0
                for tensor in model.values():
                    if hasattr(tensor, 'numel'):
                        total_size += tensor.numel() * 4  # float32 ê¸°ì¤€
                return total_size / (1024 * 1024)
            else:
                return 100.0  # ê¸°ë³¸ê°’
        except Exception:
            return 100.0  # ê¸°ë³¸ê°’
    
    def get_requirements(self) -> Dict[str, ModelRequirement]:
        """ë“±ë¡ëœ ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
        with self._lock:
            return self._model_requirements.copy()
    
    def get_statistics(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        with self._lock:
            uptime = time.time() - self._creation_time
            return {
                **self._stats,
                "uptime_seconds": uptime,
                "cache_size": len(self._model_cache),
                "requirements_count": len(self._model_requirements),
                "hit_rate": self._stats["cache_hits"] / max(1, self._stats["models_loaded"]),
                "memory_usage_mb": sum(entry.memory_mb for entry in self._model_cache.values())
            }
    
    # =============================================================================
    # ğŸ”¥ ì •ë¦¬ ë©”ì„œë“œ
    # =============================================================================
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.clear_cache()
            self._model_requirements.clear()
            self._model_status.clear()
            self.logger.info(f"ğŸ§¹ {self.step_name} Interface ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ Interface ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup()
        except:
            pass

# =============================================================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# =============================================================================

def create_step_model_interface(
    step_name: str, 
    model_loader=None
) -> StepModelInterface:
    """Step Model Interface ìƒì„±"""
    try:
        interface = StepModelInterface(step_name, model_loader)
        logger.info(f"âœ… Step Interface ìƒì„± ì™„ë£Œ: {step_name}")
        return interface
    except Exception as e:
        logger.error(f"âŒ Step Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        # í´ë°± ì¸í„°í˜ì´ìŠ¤
        return StepModelInterface(step_name, None)

# =============================================================================
# ğŸ”¥ Export
# =============================================================================

__all__ = [
    'StepModelInterface',
    'ModelRequirement',
    'ModelCacheEntry',
    'ModelStatus',
    'create_step_model_interface'
]

logger.info("âœ… Step Interface ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ”¥ register_model_requirement ë©”ì„œë“œ êµ¬í˜„ ì™„ë£Œ")
logger.info("âœ… BaseStepMixin ì™„ì „ í˜¸í™˜ì„± í™•ë³´")