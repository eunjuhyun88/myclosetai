# backend/app/ai_pipeline/interfaces/step_interface.py
"""
ğŸ”¥ Step Interface v3.0 - GitHub êµ¬ì¡° ê¸°ë°˜ ì™„ì „ ìˆ˜ì •íŒ
=========================================================

âœ… BaseStepMixinConfig conda_env ë§¤ê°œë³€ìˆ˜ ì˜¤ë¥˜ ì™„ì „ í•´ê²°
âœ… GitHub ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡° 100% ë°˜ì˜
âœ… StepFactory v9.0 ì™„ì „ í˜¸í™˜
âœ… BaseStepMixin v18.0 í‘œì¤€ ì¤€ìˆ˜
âœ… conda í™˜ê²½ mycloset-ai-clean ìš°ì„  ìµœì í™”
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

Author: MyCloset AI Team
Date: 2025-07-27
Version: 3.0 (Complete GitHub Structure Fix)
"""

import os
import gc
import sys
import time
import logging
import asyncio
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Callable, Type, Tuple, Set, TYPE_CHECKING
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from enum import Enum
from functools import wraps, lru_cache
from contextlib import asynccontextmanager

# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from ..utils.model_loader import ModelLoader, StepModelInterface
    from ..factories.step_factory import StepFactory
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
    from ..core.di_container import DIContainer

logger = logging.getLogger(__name__)

# =============================================================================
# ğŸ”¥ í™˜ê²½ ì„¤ì • ë° ì‹œìŠ¤í…œ ì •ë³´ (GitHub êµ¬ì¡° ê¸°ë°˜)
# =============================================================================

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean'
}

# ì‹œìŠ¤í…œ ì •ë³´
IS_M3_MAX = False
MEMORY_GB = 16.0

try:
    import platform
    import subprocess
    if platform.system() == 'Darwin':
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.returncode == 0:
            MEMORY_GB = round(int(memory_result.stdout.strip()) / (1024**3), 1)
except Exception:
    pass

# MPS ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
MPS_AVAILABLE = False
try:
    import torch
    MPS_AVAILABLE = (
        IS_M3_MAX and 
        hasattr(torch.backends, 'mps') and 
        torch.backends.mps.is_available()
    )
except ImportError:
    pass

# =============================================================================
# ğŸ”¥ ì—´ê±°í˜• ë° ìƒìˆ˜ ì •ì˜
# =============================================================================

class StepType(Enum):
    """Step íƒ€ì… ì •ì˜ (GitHub êµ¬ì¡° ê¸°ë°˜)"""
    HUMAN_PARSING = "human_parsing"
    POSE_ESTIMATION = "pose_estimation"
    CLOTH_SEGMENTATION = "cloth_segmentation"
    GEOMETRIC_MATCHING = "geometric_matching"
    CLOTH_WARPING = "cloth_warping"
    VIRTUAL_FITTING = "virtual_fitting"
    POST_PROCESSING = "post_processing"
    QUALITY_ASSESSMENT = "quality_assessment"

class StepPriority(Enum):
    """Step ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1      # Virtual Fitting (14GB), Human Parsing (4GB)
    HIGH = 2          # Cloth Warping (7GB), Quality Assessment (7GB)
    MEDIUM = 3        # Cloth Segmentation (5.5GB), Pose Estimation (3.4GB)
    LOW = 4           # Post Processing (1.3GB), Geometric Matching (1.3GB)

class DeviceType(Enum):
    """ë””ë°”ì´ìŠ¤ íƒ€ì…"""
    AUTO = "auto"
    CPU = "cpu"
    CUDA = "cuda"
    MPS = "mps"

class ProcessingStatus(Enum):
    """ì²˜ë¦¬ ìƒíƒœ"""
    NOT_STARTED = "not_started"
    INITIALIZING = "initializing"
    PROCESSING = "processing"
    COMPLETED = "completed"
    ERROR = "error"
    CANCELLED = "cancelled"

# =============================================================================
# ğŸ”¥ BaseStepMixinConfig - conda_env ë§¤ê°œë³€ìˆ˜ ì¶”ê°€ (ì˜¤ë¥˜ í•´ê²°)
# =============================================================================

@dataclass
class BaseStepMixinConfig:
    """
    ğŸ”¥ BaseStepMixin ì„¤ì • êµ¬ì¡° - conda_env ë§¤ê°œë³€ìˆ˜ ì™„ì „ ì§€ì›
    
    í•µì‹¬ ìˆ˜ì •ì‚¬í•­:
    âœ… conda_env ë§¤ê°œë³€ìˆ˜ ì¶”ê°€ë¡œ ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    âœ… GitHub í”„ë¡œì íŠ¸ êµ¬ì¡° ê¸°ë°˜ ê¸°ë³¸ê°’ ì„¤ì •
    âœ… mycloset-ai-clean í™˜ê²½ ìë™ ê°ì§€ ë° ìµœì í™”
    âœ… M3 Max í•˜ë“œì›¨ì–´ ìë™ ìµœì í™”
    âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
    """
    # ê¸°ë³¸ Step ì •ë³´
    step_name: str = "BaseStep"
    step_id: int = 0
    class_name: str = "BaseStepMixin"
    
    # ë””ë°”ì´ìŠ¤ ë° ì„±ëŠ¥ ì„¤ì •
    device: str = "auto"
    use_fp16: bool = False
    batch_size: int = 1
    confidence_threshold: float = 0.5
    
    # ìë™í™” ì„¤ì •
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True
    auto_inject_dependencies: bool = True
    optimization_enabled: bool = True
    strict_mode: bool = False
    
    # ì˜ì¡´ì„± ìš”êµ¬ì‚¬í•­
    require_model_loader: bool = True
    require_memory_manager: bool = True
    require_data_converter: bool = True
    require_di_container: bool = False
    require_unified_dependency_manager: bool = True
    
    # AI ëª¨ë¸ ì„¤ì •
    ai_models: List[str] = field(default_factory=list)
    model_size_gb: float = 1.0
    
    # ğŸ”¥ í™˜ê²½ ìµœì í™” ì„¤ì • (conda_env ë§¤ê°œë³€ìˆ˜ ì¶”ê°€)
    conda_optimized: bool = True
    m3_max_optimized: bool = True
    conda_env: Optional[str] = None  # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: conda_env ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ì„¤ì • ë³´ì • (conda_env ìë™ ì„¤ì •)"""
        # ğŸ”¥ conda_env ìë™ ì„¤ì • (ì „ë‹¬ë˜ì§€ ì•Šì€ ê²½ìš°)
        if self.conda_env is None:
            self.conda_env = CONDA_INFO['conda_env']
        
        # ğŸ”¥ mycloset-ai-clean í™˜ê²½ íŠ¹ë³„ ìµœì í™”
        if self.conda_env == 'mycloset-ai-clean':
            self.conda_optimized = True
            self.optimization_enabled = True
            self.auto_memory_cleanup = True
            
            # M3 Max + mycloset-ai-clean ì¡°í•© ìš¸íŠ¸ë¼ ìµœì í™”
            if IS_M3_MAX:
                self.m3_max_optimized = True
                if self.device == "auto" and MPS_AVAILABLE:
                    self.device = "mps"
                if self.batch_size == 1 and MEMORY_GB >= 64:
                    self.batch_size = 2
        
        # M3 Max í™˜ê²½ì—ì„œ ìµœì í™”
        if self.m3_max_optimized and IS_M3_MAX:
            if self.device == "auto" and MPS_AVAILABLE:
                self.device = "mps"
            if self.batch_size == 1 and MEMORY_GB >= 64:
                self.batch_size = 2
        
        # AI ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì •ê·œí™”
        if not isinstance(self.ai_models, list):
            self.ai_models = []

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'class_name': self.class_name,
            'device': self.device,
            'use_fp16': self.use_fp16,
            'batch_size': self.batch_size,
            'confidence_threshold': self.confidence_threshold,
            'auto_memory_cleanup': self.auto_memory_cleanup,
            'auto_warmup': self.auto_warmup,
            'auto_inject_dependencies': self.auto_inject_dependencies,
            'optimization_enabled': self.optimization_enabled,
            'strict_mode': self.strict_mode,
            'require_model_loader': self.require_model_loader,
            'require_memory_manager': self.require_memory_manager,
            'require_data_converter': self.require_data_converter,
            'require_di_container': self.require_di_container,
            'require_unified_dependency_manager': self.require_unified_dependency_manager,
            'ai_models': self.ai_models.copy(),
            'model_size_gb': self.model_size_gb,
            'conda_optimized': self.conda_optimized,
            'm3_max_optimized': self.m3_max_optimized,
            'conda_env': self.conda_env
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'BaseStepMixinConfig':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ìƒì„±"""
        return cls(**data)
    
    def validate(self) -> Tuple[bool, List[str]]:
        """ì„¤ì • ê²€ì¦"""
        errors = []
        
        # ê¸°ë³¸ ê²€ì¦
        if not self.step_name:
            errors.append("step_nameì´ ë¹„ì–´ìˆìŒ")
        
        if self.step_id < 0:
            errors.append("step_idëŠ” 0 ì´ìƒì´ì–´ì•¼ í•¨")
        
        if self.batch_size <= 0:
            errors.append("batch_sizeëŠ” 1 ì´ìƒì´ì–´ì•¼ í•¨")
        
        if not 0.0 <= self.confidence_threshold <= 1.0:
            errors.append("confidence_thresholdëŠ” 0.0-1.0 ë²”ìœ„ì—¬ì•¼ í•¨")
        
        if self.model_size_gb < 0:
            errors.append("model_size_gbëŠ” 0 ì´ìƒì´ì–´ì•¼ í•¨")
        
        # ë””ë°”ì´ìŠ¤ ê²€ì¦
        valid_devices = {"auto", "cpu", "cuda", "mps"}
        if self.device not in valid_devices:
            errors.append(f"deviceëŠ” {valid_devices} ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•¨")
        
        # conda í™˜ê²½ ê²€ì¦
        if self.conda_optimized and self.conda_env == 'none':
            errors.append("conda_optimizedê°€ Trueì¸ë° conda í™˜ê²½ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ")
        
        return len(errors) == 0, errors

# =============================================================================
# ğŸ”¥ Step ìƒì„± ê²°ê³¼ ë°ì´í„° êµ¬ì¡°
# =============================================================================

@dataclass
class StepCreationResult:
    """Step ìƒì„± ê²°ê³¼"""
    success: bool
    step_instance: Optional[Any] = None
    step_name: str = ""
    step_id: int = 0
    device: str = "cpu"
    creation_time: float = field(default_factory=time.time)
    error_message: Optional[str] = None
    dependencies_injected: Dict[str, bool] = field(default_factory=dict)
    initialization_success: bool = False
    memory_usage_mb: float = 0.0
    conda_env: str = field(default_factory=lambda: CONDA_INFO['conda_env'])
    metadata: Dict[str, Any] = field(default_factory=dict)

# =============================================================================
# ğŸ”¥ GitHub êµ¬ì¡° ê¸°ë°˜ Step ë§¤í•‘ í´ë˜ìŠ¤
# =============================================================================

class BaseStepMixinMapping:
    """GitHub êµ¬ì¡° ê¸°ë°˜ BaseStepMixin ë§¤í•‘"""
    
    # GitHub ì‹¤ì œ íŒŒì¼ êµ¬ì¡°ì— ë§ëŠ” Step ì„¤ì •ë“¤
    STEP_CONFIGS = {
        StepType.HUMAN_PARSING: BaseStepMixinConfig(
            step_name="HumanParsingStep",
            step_id=1,
            class_name="HumanParsingStep",
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.5,
            ai_models=["graphonomy.pth", "atr_model.pth", "lip_model.pth"],
            model_size_gb=4.0,
            conda_optimized=True,
            m3_max_optimized=True,
            conda_env=CONDA_INFO['conda_env']
        ),
        
        StepType.POSE_ESTIMATION: BaseStepMixinConfig(
            step_name="PoseEstimationStep",
            step_id=2,
            class_name="PoseEstimationStep",
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.5,
            ai_models=["pose_model.pth", "openpose_model.pth"],
            model_size_gb=3.4,
            conda_optimized=True,
            m3_max_optimized=True,
            conda_env=CONDA_INFO['conda_env']
        ),
        
        StepType.CLOTH_SEGMENTATION: BaseStepMixinConfig(
            step_name="ClothSegmentationStep",
            step_id=3,
            class_name="ClothSegmentationStep",
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.5,
            ai_models=["sam_vit_h_4b8939.pth", "cloth_segmentation.pth"],
            model_size_gb=5.5,
            conda_optimized=True,
            m3_max_optimized=True,
            conda_env=CONDA_INFO['conda_env']
        ),
        
        StepType.GEOMETRIC_MATCHING: BaseStepMixinConfig(
            step_name="GeometricMatchingStep",
            step_id=4,
            class_name="GeometricMatchingStep",
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.7,
            ai_models=["geometric_matching.pth", "tps_model.pth"],
            model_size_gb=1.3,
            conda_optimized=True,
            m3_max_optimized=True,
            conda_env=CONDA_INFO['conda_env']
        ),
        
        StepType.CLOTH_WARPING: BaseStepMixinConfig(
            step_name="ClothWarpingStep",
            step_id=5,
            class_name="ClothWarpingStep",
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.6,
            ai_models=["cloth_warping.pth", "flow_estimation.pth"],
            model_size_gb=7.0,
            conda_optimized=True,
            m3_max_optimized=True,
            conda_env=CONDA_INFO['conda_env']
        ),
        
        StepType.VIRTUAL_FITTING: BaseStepMixinConfig(
            step_name="VirtualFittingStep",
            step_id=6,
            class_name="VirtualFittingStep",
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.8,
            ai_models=["stable-diffusion-v1-5", "controlnet", "vae"],
            model_size_gb=14.0,  # ğŸ”¥ í•µì‹¬ 14GB ëª¨ë¸
            conda_optimized=True,
            m3_max_optimized=True,
            conda_env=CONDA_INFO['conda_env']
        ),
        
        StepType.POST_PROCESSING: BaseStepMixinConfig(
            step_name="PostProcessingStep",
            step_id=7,
            class_name="PostProcessingStep",
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.5,
            ai_models=["super_resolution.pth", "enhancement.pth"],
            model_size_gb=1.3,
            conda_optimized=True,
            m3_max_optimized=True,
            conda_env=CONDA_INFO['conda_env']
        ),
        
        StepType.QUALITY_ASSESSMENT: BaseStepMixinConfig(
            step_name="QualityAssessmentStep",
            step_id=8,
            class_name="QualityAssessmentStep",
            device="auto",
            use_fp16=True,
            batch_size=1,
            confidence_threshold=0.7,
            ai_models=["open_clip_pytorch_model.bin", "ViT-L-14.pt"],
            model_size_gb=7.0,
            conda_optimized=True,
            m3_max_optimized=True,
            conda_env=CONDA_INFO['conda_env']
        )
    }
    
    @classmethod
    def get_config(cls, step_type: StepType) -> BaseStepMixinConfig:
        """Step íƒ€ì…ë³„ ì„¤ì • ë°˜í™˜"""
        return cls.STEP_CONFIGS.get(step_type, BaseStepMixinConfig())
    
    @classmethod
    def get_config_by_name(cls, step_name: str) -> Optional[BaseStepMixinConfig]:
        """Step ì´ë¦„ìœ¼ë¡œ ì„¤ì • ë°˜í™˜"""
        for config in cls.STEP_CONFIGS.values():
            if config.step_name == step_name or config.class_name == step_name:
                return config
        return None
    
    @classmethod
    def get_config_by_id(cls, step_id: int) -> Optional[BaseStepMixinConfig]:
        """Step IDë¡œ ì„¤ì • ë°˜í™˜"""
        for config in cls.STEP_CONFIGS.values():
            if config.step_id == step_id:
                return config
        return None
    
    @classmethod
    def create_custom_config(cls, base_config: BaseStepMixinConfig, **overrides) -> BaseStepMixinConfig:
        """ê¸°ì¡´ ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ì»¤ìŠ¤í…€ ì„¤ì • ìƒì„±"""
        config_dict = {
            'step_name': base_config.step_name,
            'step_id': base_config.step_id,
            'class_name': base_config.class_name,
            'device': base_config.device,
            'use_fp16': base_config.use_fp16,
            'batch_size': base_config.batch_size,
            'confidence_threshold': base_config.confidence_threshold,
            'auto_memory_cleanup': base_config.auto_memory_cleanup,
            'auto_warmup': base_config.auto_warmup,
            'auto_inject_dependencies': base_config.auto_inject_dependencies,
            'optimization_enabled': base_config.optimization_enabled,
            'strict_mode': base_config.strict_mode,
            'require_model_loader': base_config.require_model_loader,
            'require_memory_manager': base_config.require_memory_manager,
            'require_data_converter': base_config.require_data_converter,
            'require_di_container': base_config.require_di_container,
            'require_unified_dependency_manager': base_config.require_unified_dependency_manager,
            'ai_models': base_config.ai_models.copy(),
            'model_size_gb': base_config.model_size_gb,
            'conda_optimized': base_config.conda_optimized,
            'm3_max_optimized': base_config.m3_max_optimized,
            'conda_env': base_config.conda_env  # ğŸ”¥ conda_env í¬í•¨
        }
        config_dict.update(overrides)
        return BaseStepMixinConfig(**config_dict)

# =============================================================================
# ğŸ”¥ ì˜ì¡´ì„± ìƒíƒœ ê´€ë¦¬
# =============================================================================

@dataclass
class DependencyStatus:
    """ì˜ì¡´ì„± ìƒíƒœ"""
    model_loader: bool = False
    step_interface: bool = False
    memory_manager: bool = False
    data_converter: bool = False
    di_container: bool = False
    base_initialized: bool = False
    custom_initialized: bool = False
    dependencies_validated: bool = False
    conda_optimized: bool = False
    m3_max_optimized: bool = False
    injection_attempts: Dict[str, int] = field(default_factory=dict)
    injection_errors: Dict[str, List[str]] = field(default_factory=dict)
    last_injection_time: Optional[float] = None

# =============================================================================
# ğŸ”¥ ê³ ê¸‰ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ
# =============================================================================

class AdvancedMemoryManager:
    """ê³ ê¸‰ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ (M3 Max ìµœì í™”)"""
    
    def __init__(self, max_memory_gb: float = None):
        self.logger = logging.getLogger(f"{__name__}.AdvancedMemoryManager")
        
        # M3 Max ìë™ ê°ì§€ ë° ë©”ëª¨ë¦¬ ì„¤ì •
        if max_memory_gb is None:
            self.max_memory_gb = MEMORY_GB * 0.8 if IS_M3_MAX else 8.0
        else:
            self.max_memory_gb = max_memory_gb
        
        self.current_memory_gb = 0.0
        self.memory_pool = {}
        self.allocation_history = []
        self._lock = threading.RLock()
        
        # M3 Max íŠ¹í™” ì„¤ì •
        self.is_m3_max = IS_M3_MAX
        self.mps_enabled = MPS_AVAILABLE
        
        # ë©”ëª¨ë¦¬ ì¶”ì 
        self.peak_memory_gb = 0.0
        self.allocation_count = 0
        self.deallocation_count = 0
        
        self.logger.info(f"ğŸ§  ê³ ê¸‰ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”: {self.max_memory_gb:.1f}GB (M3 Max: {self.is_m3_max})")
    
    def allocate_memory(self, size_gb: float, owner: str) -> bool:
        """ë©”ëª¨ë¦¬ í• ë‹¹"""
        with self._lock:
            if self.current_memory_gb + size_gb <= self.max_memory_gb:
                self.current_memory_gb += size_gb
                self.memory_pool[owner] = size_gb
                self.allocation_history.append({
                    'action': 'allocate',
                    'size_gb': size_gb,
                    'owner': owner,
                    'timestamp': time.time(),
                    'total_after': self.current_memory_gb
                })
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.allocation_count += 1
                self.peak_memory_gb = max(self.peak_memory_gb, self.current_memory_gb)
                
                self.logger.debug(f"âœ… ë©”ëª¨ë¦¬ í• ë‹¹: {size_gb:.1f}GB â†’ {owner} (ì´: {self.current_memory_gb:.1f}GB)")
                return True
            else:
                self.logger.warning(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±: {size_gb:.1f}GB ìš”ì²­, {self.max_memory_gb - self.current_memory_gb:.1f}GB ì‚¬ìš© ê°€ëŠ¥")
                return False
    
    def deallocate_memory(self, owner: str) -> float:
        """ë©”ëª¨ë¦¬ í•´ì œ"""
        with self._lock:
            if owner in self.memory_pool:
                size_gb = self.memory_pool[owner]
                del self.memory_pool[owner]
                self.current_memory_gb -= size_gb
                
                self.allocation_history.append({
                    'action': 'deallocate',
                    'size_gb': size_gb,
                    'owner': owner,
                    'timestamp': time.time(),
                    'total_after': self.current_memory_gb
                })
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.deallocation_count += 1
                
                self.logger.debug(f"âœ… ë©”ëª¨ë¦¬ í•´ì œ: {size_gb:.1f}GB â† {owner} (ì´: {self.current_memory_gb:.1f}GB)")
                return size_gb
            return 0.0
    
    def optimize_for_m3_max(self):
        """M3 Max ì „ìš© ë©”ëª¨ë¦¬ ìµœì í™”"""
        if not self.is_m3_max:
            return
        
        try:
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.mps_enabled:
                import torch
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                    self.logger.debug("ğŸ MPS ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            
            # Python GC ì‹¤í–‰
            gc.collect()
            
            # í†µí•© ë©”ëª¨ë¦¬ ìµœì í™” (M3 Max íŠ¹í™”)
            if MEMORY_GB >= 64:  # 64GB ì´ìƒì¼ ë•Œë§Œ
                # ë©”ëª¨ë¦¬ í’€ í¬ê¸° ì¦ê°€
                self.max_memory_gb = min(MEMORY_GB * 0.9, 100.0)
                self.logger.info(f"ğŸ M3 Max ë©”ëª¨ë¦¬ í’€ í™•ì¥: {self.max_memory_gb:.1f}GB")
            
        except Exception as e:
            self.logger.error(f"âŒ M3 Max ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„ ë°˜í™˜"""
        with self._lock:
            return {
                'current_gb': self.current_memory_gb,
                'max_gb': self.max_memory_gb,
                'peak_gb': self.peak_memory_gb,
                'available_gb': self.max_memory_gb - self.current_memory_gb,
                'usage_percent': (self.current_memory_gb / self.max_memory_gb) * 100,
                'allocations': self.allocation_count,
                'deallocations': self.deallocation_count,
                'active_pools': len(self.memory_pool),
                'is_m3_max': self.is_m3_max,
                'mps_enabled': self.mps_enabled,
                'memory_pool': self.memory_pool.copy(),
                'total_system_gb': MEMORY_GB
            }

# =============================================================================
# ğŸ”¥ í–¥ìƒëœ ì˜ì¡´ì„± ê´€ë¦¬ì 
# =============================================================================

class EnhancedDependencyManager:
    """í–¥ìƒëœ ì˜ì¡´ì„± ê´€ë¦¬ì (BaseStepMixin v18.0 í˜¸í™˜)"""
    
    def __init__(self, step_name: str):
        self.step_name = step_name
        self.logger = logging.getLogger(f"{__name__}.DependencyManager.{step_name}")
        
        # ì˜ì¡´ì„± ìƒíƒœ
        self.dependency_status = DependencyStatus()
        
        # ì˜ì¡´ì„± ì €ì¥ì†Œ
        self.dependencies: Dict[str, Any] = {}
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì
        self.memory_manager = AdvancedMemoryManager()
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        
        # ìë™ ì£¼ì… í”Œë˜ê·¸
        self._auto_injection_attempted = False
        
        self.logger.debug(f"ğŸ”§ ì˜ì¡´ì„± ê´€ë¦¬ì ì´ˆê¸°í™”: {step_name}")
    
    def inject_dependency(self, name: str, dependency: Any, required: bool = False) -> bool:
        """ì˜ì¡´ì„± ì£¼ì…"""
        try:
            with self._lock:
                if dependency is not None:
                    self.dependencies[name] = dependency
                    
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    if name == 'model_loader':
                        self.dependency_status.model_loader = True
                    elif name == 'memory_manager':
                        self.dependency_status.memory_manager = True
                    elif name == 'data_converter':
                        self.dependency_status.data_converter = True
                    elif name == 'di_container':
                        self.dependency_status.di_container = True
                    
                    # ì£¼ì… í†µê³„ ì—…ë°ì´íŠ¸
                    if name not in self.dependency_status.injection_attempts:
                        self.dependency_status.injection_attempts[name] = 0
                    self.dependency_status.injection_attempts[name] += 1
                    self.dependency_status.last_injection_time = time.time()
                    
                    self.logger.debug(f"âœ… ì˜ì¡´ì„± ì£¼ì… ì„±ê³µ: {name}")
                    return True
                else:
                    if required:
                        error_msg = f"í•„ìˆ˜ ì˜ì¡´ì„± {name}ì´ Noneì„"
                        self.logger.error(f"âŒ {error_msg}")
                        
                        # ì˜¤ë¥˜ ê¸°ë¡
                        if name not in self.dependency_status.injection_errors:
                            self.dependency_status.injection_errors[name] = []
                        self.dependency_status.injection_errors[name].append(error_msg)
                        
                        return False
                    else:
                        self.logger.warning(f"âš ï¸ ì„ íƒì  ì˜ì¡´ì„± {name}ì´ None (í—ˆìš©ë¨)")
                        return True
        
        except Exception as e:
            error_msg = f"ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {name} - {e}"
            self.logger.error(f"âŒ {error_msg}")
            
            # ì˜¤ë¥˜ ê¸°ë¡
            if name not in self.dependency_status.injection_errors:
                self.dependency_status.injection_errors[name] = []
            self.dependency_status.injection_errors[name].append(error_msg)
            
            return False
    
    def get_dependency(self, name: str) -> Optional[Any]:
        """ì˜ì¡´ì„± ì¡°íšŒ"""
        with self._lock:
            return self.dependencies.get(name)
    
    def has_dependency(self, name: str) -> bool:
        """ì˜ì¡´ì„± ì¡´ì¬ í™•ì¸"""
        with self._lock:
            return name in self.dependencies and self.dependencies[name] is not None
    
    def auto_inject_dependencies(self) -> bool:
        """ìë™ ì˜ì¡´ì„± ì£¼ì…"""
        if self._auto_injection_attempted:
            self.logger.debug("ìë™ ì˜ì¡´ì„± ì£¼ì… ì´ë¯¸ ì‹œë„ë¨")
            return True
        
        try:
            self._auto_injection_attempted = True
            self.logger.info(f"ğŸ”„ {self.step_name} ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
            
            # conda í™˜ê²½ ìµœì í™”
            if CONDA_INFO['is_target_env']:
                self.dependency_status.conda_optimized = True
                self.logger.debug("âœ… conda í™˜ê²½ ìµœì í™” í™œì„±í™”")
            
            # M3 Max ìµœì í™”
            if IS_M3_MAX:
                self.dependency_status.m3_max_optimized = True
                self.memory_manager.optimize_for_m3_max()
                self.logger.debug("âœ… M3 Max ìµœì í™” í™œì„±í™”")
            
            # ê¸°ë³¸ ì´ˆê¸°í™” ì™„ë£Œ
            self.dependency_status.base_initialized = True
            
            self.logger.info(f"âœ… {self.step_name} ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def validate_dependencies(self, config: BaseStepMixinConfig) -> Tuple[bool, List[str]]:
        """ì˜ì¡´ì„± ê²€ì¦"""
        errors = []
        
        with self._lock:
            # í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸
            if config.require_model_loader and not self.dependency_status.model_loader:
                errors.append("ModelLoaderê°€ í•„ìš”í•˜ì§€ë§Œ ì£¼ì…ë˜ì§€ ì•ŠìŒ")
            
            if config.require_memory_manager and not self.dependency_status.memory_manager:
                errors.append("MemoryManagerê°€ í•„ìš”í•˜ì§€ë§Œ ì£¼ì…ë˜ì§€ ì•ŠìŒ")
            
            if config.require_data_converter and not self.dependency_status.data_converter:
                errors.append("DataConverterê°€ í•„ìš”í•˜ì§€ë§Œ ì£¼ì…ë˜ì§€ ì•ŠìŒ")
            
            if config.require_di_container and not self.dependency_status.di_container:
                errors.append("DIContainerê°€ í•„ìš”í•˜ì§€ë§Œ ì£¼ì…ë˜ì§€ ì•ŠìŒ")
            
            # conda í™˜ê²½ ê²€ì¦
            if config.conda_optimized and not self.dependency_status.conda_optimized:
                errors.append("conda ìµœì í™”ê°€ í•„ìš”í•˜ì§€ë§Œ í™œì„±í™”ë˜ì§€ ì•ŠìŒ")
            
            # M3 Max ê²€ì¦
            if config.m3_max_optimized and IS_M3_MAX and not self.dependency_status.m3_max_optimized:
                errors.append("M3 Max ìµœì í™”ê°€ í•„ìš”í•˜ì§€ë§Œ í™œì„±í™”ë˜ì§€ ì•ŠìŒ")
        
        self.dependency_status.dependencies_validated = len(errors) == 0
        return len(errors) == 0, errors
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            with self._lock:
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                for name in list(self.dependencies.keys()):
                    self.memory_manager.deallocate_memory(name)
                
                # ì˜ì¡´ì„± ì œê±°
                self.dependencies.clear()
                
                # ìƒíƒœ ë¦¬ì…‹
                self.dependency_status = DependencyStatus()
                
                self.logger.debug(f"ğŸ§¹ {self.step_name} ì˜ì¡´ì„± ê´€ë¦¬ì ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± ê´€ë¦¬ì ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ê´€ë¦¬ì ìƒíƒœ ì¡°íšŒ"""
        with self._lock:
            memory_stats = self.memory_manager.get_memory_stats()
            
            return {
                'step_name': self.step_name,
                'dependency_status': {
                    'model_loader': self.dependency_status.model_loader,
                    'step_interface': self.dependency_status.step_interface,
                    'memory_manager': self.dependency_status.memory_manager,
                    'data_converter': self.dependency_status.data_converter,
                    'di_container': self.dependency_status.di_container,
                    'base_initialized': self.dependency_status.base_initialized,
                    'custom_initialized': self.dependency_status.custom_initialized,
                    'dependencies_validated': self.dependency_status.dependencies_validated,
                    'conda_optimized': self.dependency_status.conda_optimized,
                    'm3_max_optimized': self.dependency_status.m3_max_optimized
                },
                'environment': {
                    'conda_env': CONDA_INFO['conda_env'],
                    'is_target_env': CONDA_INFO['is_target_env'],
                    'is_m3_max': IS_M3_MAX,
                    'memory_gb': MEMORY_GB,
                    'mps_available': MPS_AVAILABLE
                },
                'injection_history': {
                    'auto_injection_attempted': self._auto_injection_attempted,
                    'injection_attempts': dict(self.dependency_status.injection_attempts),
                    'injection_errors': dict(self.dependency_status.injection_errors),
                    'last_injection_time': self.dependency_status.last_injection_time
                },
                'dependencies_available': list(self.dependencies.keys()),
                'dependencies_count': len(self.dependencies),
                'memory_stats': memory_stats
            }

# =============================================================================
# ğŸ”¥ StepModelInterface v3.0 - ì™„ì „ í˜¸í™˜ì„±
# =============================================================================

class StepModelInterface:
    """
    ğŸ”— Stepìš© ModelLoader ì¸í„°í˜ì´ìŠ¤ v3.0 - GitHub êµ¬ì¡° ì™„ì „ í˜¸í™˜
    
    âœ… BaseStepMixin ì™„ì „ í˜¸í™˜ì„± ë³´ì¥
    âœ… register_model_requirement ì™„ì „ êµ¬í˜„
    âœ… list_available_models í¬ê¸°ìˆœ ì •ë ¬
    âœ… conda í™˜ê²½ mycloset-ai-clean ìš°ì„  ìµœì í™”
    âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
    âœ… GitHub ì‹¤ì œ êµ¬ì¡° ë°˜ì˜
    """
    
    def __init__(self, step_name: str, model_loader: Optional['ModelLoader'] = None):
        self.step_name = step_name
        self.model_loader = model_loader
        self.logger = logging.getLogger(f"StepInterface.{step_name}")
        
        # ëª¨ë¸ ê´€ë¦¬
        self._model_registry: Dict[str, Dict[str, Any]] = {}
        self._model_cache: Dict[str, Any] = {}
        self._model_requirements: Dict[str, Any] = {}
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.memory_manager = AdvancedMemoryManager()
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        
        # í†µê³„
        self.statistics = {
            'models_registered': 0,
            'models_loaded': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'loading_failures': 0,
            'creation_time': time.time()
        }
        
        self.logger.info(f"ğŸ”— {step_name} StepInterface v3.0 ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_model_requirement(
        self, 
        model_name: str, 
        model_type: str = "BaseModel",
        **kwargs
    ) -> bool:
        """
        ğŸ”¥ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ - BaseStepMixin ì™„ì „ í˜¸í™˜ êµ¬í˜„
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            model_type: ëª¨ë¸ íƒ€ì…
            **kwargs: ì¶”ê°€ ì„¤ì •
            
        Returns:
            bool: ë“±ë¡ ì„±ê³µ ì—¬ë¶€
        """
        try:
            with self._lock:
                self.logger.info(f"ğŸ“ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡: {model_name} ({model_type})")
                
                # ìš”êµ¬ì‚¬í•­ ì •ë³´ ìƒì„±
                requirement = {
                    'model_name': model_name,
                    'model_type': model_type,
                    'step_name': self.step_name,
                    'device': kwargs.get('device', 'auto'),
                    'precision': kwargs.get('precision', 'fp16'),
                    'input_size': kwargs.get('input_size', (512, 512)),
                    'num_classes': kwargs.get('num_classes'),
                    'priority': kwargs.get('priority', 5),
                    'min_memory_mb': kwargs.get('min_memory_mb', 100.0),
                    'max_memory_mb': kwargs.get('max_memory_mb', 8192.0),
                    'conda_env': kwargs.get('conda_env', CONDA_INFO['conda_env']),
                    'registered_at': time.time(),
                    'metadata': kwargs.get('metadata', {})
                }
                
                # ìš”êµ¬ì‚¬í•­ ì €ì¥
                self._model_requirements[model_name] = requirement
                
                # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
                self._model_registry[model_name] = {
                    'name': model_name,
                    'type': model_type,
                    'step_class': self.step_name,
                    'loaded': False,
                    'size_mb': requirement['max_memory_mb'],
                    'device': requirement['device'],
                    'status': 'registered',
                    'requirement': requirement,
                    'registered_at': requirement['registered_at']
                }
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.statistics['models_registered'] += 1
                
                # ModelLoaderì— ì „ë‹¬ (ê°€ëŠ¥í•œ ê²½ìš°)
                if self.model_loader and hasattr(self.model_loader, 'register_model_requirement'):
                    try:
                        self.model_loader.register_model_requirement(
                            model_name=model_name,
                            model_type=model_type,
                            step_name=self.step_name,
                            **kwargs
                        )
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ìš”êµ¬ì‚¬í•­ ì „ë‹¬ ì‹¤íŒ¨: {e}")
                
                self.logger.info(f"âœ… ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ: {model_name}")
                return True
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {model_name} - {e}")
            return False
    
    def list_available_models(
        self, 
        step_class: Optional[str] = None,
        model_type: Optional[str] = None,
        include_unloaded: bool = True,
        sort_by: str = "size"
    ) -> List[Dict[str, Any]]:
        """
        ğŸ”¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ - BaseStepMixin ì™„ì „ í˜¸í™˜
        
        Args:
            step_class: Step í´ë˜ìŠ¤ í•„í„°
            model_type: ëª¨ë¸ íƒ€ì… í•„í„°
            include_unloaded: ë¡œë“œë˜ì§€ ì•Šì€ ëª¨ë¸ í¬í•¨ ì—¬ë¶€
            sort_by: ì •ë ¬ ê¸°ì¤€ (size, name, priority)
            
        Returns:
            List[Dict[str, Any]]: ëª¨ë¸ ëª©ë¡ (ì •ë ¬ë¨)
        """
        try:
            with self._lock:
                models = []
                
                # ë“±ë¡ëœ ëª¨ë¸ë“¤ì—ì„œ ëª©ë¡ ìƒì„±
                for model_name, registry_entry in self._model_registry.items():
                    # í•„í„°ë§
                    if step_class and registry_entry['step_class'] != step_class:
                        continue
                    if model_type and registry_entry['type'] != model_type:
                        continue
                    if not include_unloaded and not registry_entry['loaded']:
                        continue
                    
                    # ëª¨ë¸ ì •ë³´ êµ¬ì„±
                    requirement = registry_entry.get('requirement', {})
                    
                    model_info = {
                        'name': model_name,
                        'path': f"ai_models/step_{requirement.get('step_name', self.step_name).lower()}/{model_name}",
                        'size_mb': registry_entry['size_mb'],
                        'model_type': registry_entry['type'],
                        'step_class': registry_entry['step_class'],
                        'loaded': registry_entry['loaded'],
                        'device': registry_entry['device'],
                        'status': registry_entry['status'],
                        'priority': requirement.get('priority', 5),
                        'metadata': {
                            'step_name': self.step_name,
                            'input_size': requirement.get('input_size', (512, 512)),
                            'num_classes': requirement.get('num_classes'),
                            'precision': requirement.get('precision', 'fp16'),
                            'conda_env': requirement.get('conda_env', CONDA_INFO['conda_env']),
                            'registered_at': requirement.get('registered_at', 0),
                            'github_structure_compliant': True,
                            **requirement.get('metadata', {})
                        }
                    }
                    models.append(model_info)
                
                # ModelLoaderì—ì„œ ì¶”ê°€ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ê°€ëŠ¥í•œ ê²½ìš°)
                if self.model_loader and hasattr(self.model_loader, 'list_available_models'):
                    try:
                        additional_models = self.model_loader.list_available_models(
                            step_class=step_class or self.step_name,
                            model_type=model_type
                        )
                        
                        # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                        existing_names = {m['name'] for m in models}
                        for model in additional_models:
                            if model['name'] not in existing_names:
                                model_info = {
                                    'name': model['name'],
                                    'path': model.get('path', f"loader_models/{model['name']}"),
                                    'size_mb': model.get('size_mb', 0.0),
                                    'model_type': model.get('model_type', 'unknown'),
                                    'step_class': model.get('step_class', self.step_name),
                                    'loaded': model.get('loaded', False),
                                    'device': model.get('device', 'auto'),
                                    'status': 'loaded' if model.get('loaded', False) else 'available',
                                    'priority': 5,
                                    'metadata': {
                                        'step_name': self.step_name,
                                        'source': 'model_loader',
                                        'github_structure_compliant': False,
                                        **model.get('metadata', {})
                                    }
                                }
                                models.append(model_info)
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ModelLoader ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                
                # ì •ë ¬ ìˆ˜í–‰
                if sort_by == "size":
                    models.sort(key=lambda x: x['size_mb'], reverse=True)  # í° ê²ƒë¶€í„°
                elif sort_by == "name":
                    models.sort(key=lambda x: x['name'])
                elif sort_by == "priority":
                    models.sort(key=lambda x: x['priority'])  # ì‘ì€ ê°’ì´ ë†’ì€ ìš°ì„ ìˆœìœ„
                else:
                    # ê¸°ë³¸ê°’: í¬ê¸°ìˆœ ì •ë ¬
                    models.sort(key=lambda x: x['size_mb'], reverse=True)
                
                self.logger.debug(f"ğŸ“‹ ëª¨ë¸ ëª©ë¡ ë°˜í™˜: {len(models)}ê°œ")
                return models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def get_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë¹„ë™ê¸°) - BaseStepMixin í˜¸í™˜"""
        try:
            with self._lock:
                # ìºì‹œ í™•ì¸
                if model_name in self._model_cache:
                    self.statistics['cache_hits'] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return self._model_cache[model_name]
                
                # ModelLoaderë¥¼ í†µí•œ ë¡œë”©
                if self.model_loader:
                    if hasattr(self.model_loader, 'load_model_async'):
                        model = await self.model_loader.load_model_async(model_name, **kwargs)
                    elif hasattr(self.model_loader, 'load_model'):
                        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                        loop = asyncio.get_event_loop()
                        model = await loop.run_in_executor(
                            None, 
                            lambda: self.model_loader.load_model(model_name, **kwargs)
                        )
                    else:
                        model = None
                    
                    if model is not None:
                        # ìºì‹œì— ì €ì¥
                        self._model_cache[model_name] = model
                        
                        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸
                        if model_name in self._model_registry:
                            self._model_registry[model_name]['loaded'] = True
                            self._model_registry[model_name]['status'] = 'loaded'
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.statistics['models_loaded'] += 1
                        
                        self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                        return model
                
                # ë¡œë”© ì‹¤íŒ¨
                self.statistics['cache_misses'] += 1
                self.statistics['loading_failures'] += 1
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            return None
    
    def get_model_sync(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ (ë™ê¸°) - BaseStepMixin í˜¸í™˜"""
        try:
            with self._lock:
                # ìºì‹œ í™•ì¸
                if model_name in self._model_cache:
                    self.statistics['cache_hits'] += 1
                    self.logger.debug(f"â™»ï¸ ìºì‹œëœ ëª¨ë¸ ë°˜í™˜: {model_name}")
                    return self._model_cache[model_name]
                
                # ModelLoaderë¥¼ í†µí•œ ë¡œë”©
                if self.model_loader and hasattr(self.model_loader, 'load_model'):
                    model = self.model_loader.load_model(model_name, **kwargs)
                    
                    if model is not None:
                        # ìºì‹œì— ì €ì¥
                        self._model_cache[model_name] = model
                        
                        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸
                        if model_name in self._model_registry:
                            self._model_registry[model_name]['loaded'] = True
                            self._model_registry[model_name]['status'] = 'loaded'
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.statistics['models_loaded'] += 1
                        
                        self.logger.info(f"âœ… ë™ê¸° ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                        return model
                
                # ë¡œë”© ì‹¤íŒ¨
                self.statistics['cache_misses'] += 1
                self.statistics['loading_failures'] += 1
                self.logger.warning(f"âš ï¸ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}")
                return None
                
        except Exception as e:
            self.statistics['loading_failures'] += 1
            self.logger.error(f"âŒ ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            return None
    
    # BaseStepMixin í˜¸í™˜ì„ ìœ„í•œ ë³„ì¹­
    def load_model(self, model_name: str, **kwargs) -> Optional[Any]:
        """ëª¨ë¸ ë¡œë“œ - BaseStepMixin í˜¸í™˜ ë³„ì¹­"""
        return self.get_model_sync(model_name, **kwargs)
    
    def get_model_status(self, model_name: Optional[str] = None) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ - BaseStepMixin í˜¸í™˜"""
        try:
            with self._lock:
                if model_name:
                    # íŠ¹ì • ëª¨ë¸ ìƒíƒœ
                    if model_name in self._model_registry:
                        return self._model_registry[model_name].copy()
                    else:
                        return {
                            'name': model_name,
                            'status': 'not_registered',
                            'loaded': False,
                            'error': 'Model not found in registry'
                        }
                else:
                    # ì „ì²´ ìƒíƒœ
                    memory_stats = self.memory_manager.get_memory_stats()
                    
                    return {
                        'step_name': self.step_name,
                        'models': dict(self._model_registry),
                        'total_registered': len(self._model_registry),
                        'total_loaded': len(self._model_cache),
                        'statistics': self.statistics.copy(),
                        'memory_stats': memory_stats,
                        'environment': {
                            'conda_env': CONDA_INFO['conda_env'],
                            'is_target_env': CONDA_INFO['is_target_env'],
                            'is_m3_max': IS_M3_MAX,
                            'memory_gb': MEMORY_GB
                        },
                        'version': '3.0'
                    }
        except Exception as e:
            return {'error': str(e)}
    
    def clear_cache(self) -> bool:
        """ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™”"""
        try:
            with self._lock:
                # ë©”ëª¨ë¦¬ í•´ì œ
                for model_name in self._model_cache:
                    self.memory_manager.deallocate_memory(model_name)
                
                # ìºì‹œ ì´ˆê¸°í™”
                self._model_cache.clear()
                
                # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
                for model_name in self._model_registry:
                    self._model_registry[model_name]['loaded'] = False
                    self._model_registry[model_name]['status'] = 'registered'
                
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                gc.collect()
                
                self.logger.info("ğŸ§¹ ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.clear_cache()
            self._model_requirements.clear()
            self._model_registry.clear()
            self.memory_manager = AdvancedMemoryManager()
            self.logger.info(f"âœ… {self.step_name} Interface ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ Interface ì •ë¦¬ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# =============================================================================

def create_step_model_interface(
    step_name: str, 
    model_loader: Optional['ModelLoader'] = None,
    max_memory_gb: float = None
) -> StepModelInterface:
    """Step Model Interface ìƒì„± (GitHub êµ¬ì¡° í˜¸í™˜)"""
    try:
        interface = StepModelInterface(step_name, model_loader)
        
        # M3 Max í™˜ê²½ì— ë§ëŠ” ë©”ëª¨ë¦¬ ì„¤ì •
        if max_memory_gb is None:
            max_memory_gb = MEMORY_GB * 0.8 if IS_M3_MAX else 8.0
        
        interface.memory_manager = AdvancedMemoryManager(max_memory_gb)
        
        logger.info(f"âœ… Step Interface ìƒì„± ì™„ë£Œ: {step_name} ({max_memory_gb:.1f}GB)")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ Step Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        # í´ë°± ì¸í„°í˜ì´ìŠ¤
        return StepModelInterface(step_name, None)

def create_optimized_step_interface(
    step_name: str,
    model_loader: Optional['ModelLoader'] = None
) -> StepModelInterface:
    """ìµœì í™”ëœ Step Interface ìƒì„± (conda + M3 Max ëŒ€ì‘)"""
    try:
        # conda + M3 Max ì¡°í•© ìµœì í™” ì„¤ì •
        if CONDA_INFO['is_target_env'] and IS_M3_MAX:
            max_memory_gb = MEMORY_GB * 0.9  # 90% ì‚¬ìš©
        elif IS_M3_MAX:
            max_memory_gb = MEMORY_GB * 0.8  # 80% ì‚¬ìš©
        elif CONDA_INFO['is_target_env']:
            max_memory_gb = 12.0  # 12GB
        else:
            max_memory_gb = 8.0   # 8GB
        
        interface = create_step_model_interface(
            step_name=step_name,
            model_loader=model_loader,
            max_memory_gb=max_memory_gb
        )
        
        logger.info(f"âœ… ìµœì í™”ëœ Interface: {step_name} (conda: {CONDA_INFO['is_target_env']}, M3: {IS_M3_MAX})")
        return interface
        
    except Exception as e:
        logger.error(f"âŒ ìµœì í™”ëœ Interface ìƒì„± ì‹¤íŒ¨: {step_name} - {e}")
        return create_step_model_interface(step_name, model_loader)

# =============================================================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def get_environment_info() -> Dict[str, Any]:
    """í™˜ê²½ ì •ë³´ ì¡°íšŒ"""
    return {
        'conda_info': CONDA_INFO,
        'system_info': {
            'is_m3_max': IS_M3_MAX,
            'memory_gb': MEMORY_GB,
            'mps_available': MPS_AVAILABLE,
            'platform': platform.system(),
            'machine': platform.machine()
        },
        'optimization_status': {
            'conda_optimized': CONDA_INFO['is_target_env'],
            'm3_max_optimized': IS_M3_MAX,
            'ultra_optimization_available': CONDA_INFO['is_target_env'] and IS_M3_MAX
        }
    }

def optimize_environment():
    """í™˜ê²½ ìµœì í™” ì‹¤í–‰"""
    try:
        optimizations = []
        
        # conda í™˜ê²½ ìµœì í™”
        if CONDA_INFO['is_target_env']:
            optimizations.append("conda í™˜ê²½ ìµœì í™”")
        
        # M3 Max ìµœì í™”
        if IS_M3_MAX:
            optimizations.append("M3 Max ìµœì í™”")
            
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            if MPS_AVAILABLE:
                try:
                    import torch
                    if hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                    optimizations.append("MPS ë©”ëª¨ë¦¬ ì •ë¦¬")
                except:
                    pass
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        optimizations.append("ê°€ë¹„ì§€ ì»¬ë ‰ì…˜")
        
        logger.info(f"âœ… í™˜ê²½ ìµœì í™” ì™„ë£Œ: {', '.join(optimizations)}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
        return False

# =============================================================================
# ğŸ”¥ Export
# =============================================================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'StepModelInterface',
    'AdvancedMemoryManager',
    'EnhancedDependencyManager',
    'BaseStepMixinMapping',
    
    # ë°ì´í„° êµ¬ì¡°ë“¤
    'BaseStepMixinConfig',
    'StepCreationResult',
    'DependencyStatus',
    'StepType',
    'StepPriority',
    'DeviceType', 
    'ProcessingStatus',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_step_model_interface',
    'create_optimized_step_interface',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_environment_info',
    'optimize_environment',
    
    # ìƒìˆ˜ë“¤
    'CONDA_INFO',
    'IS_M3_MAX',
    'MEMORY_GB',
    'MPS_AVAILABLE'
]

# =============================================================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì™„ë£Œ ë©”ì‹œì§€
# =============================================================================

# conda í™˜ê²½ ìë™ ìµœì í™”
if CONDA_INFO['is_target_env']:
    optimize_environment()
    logger.info("ğŸ conda í™˜ê²½ ìë™ ìµœì í™” ì™„ë£Œ!")
else:
    logger.warning(f"âš ï¸ conda í™˜ê²½ì„ í™•ì¸í•˜ì„¸ìš”: conda activate mycloset-ai-clean")

# M3 Max ìµœì í™”
if IS_M3_MAX:
    try:
        # MPS ì´ˆê¸° ì„¤ì •
        if MPS_AVAILABLE:
            import torch
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
        gc.collect()
        logger.info("ğŸ M3 Max ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ!")
    except:
        pass

logger.info("=" * 80)
logger.info("ğŸ”¥ Step Interface v3.0 - GitHub êµ¬ì¡° ê¸°ë°˜ ì™„ì „ ìˆ˜ì •íŒ")
logger.info("=" * 80)
logger.info("âœ… BaseStepMixinConfig conda_env ë§¤ê°œë³€ìˆ˜ ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
logger.info("âœ… GitHub ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡° 100% ë°˜ì˜")
logger.info("âœ… StepFactory v9.0 ì™„ì „ í˜¸í™˜")
logger.info("âœ… BaseStepMixin v18.0 í‘œì¤€ ì¤€ìˆ˜")
logger.info("âœ… conda í™˜ê²½ mycloset-ai-clean ìš°ì„  ìµœì í™”")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
logger.info("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")

logger.info(f"ğŸ”§ í˜„ì¬ í™˜ê²½:")
logger.info(f"   - conda í™˜ê²½: {CONDA_INFO['conda_env']} ({'âœ… ìµœì í™”ë¨' if CONDA_INFO['is_target_env'] else 'âš ï¸ ê¶Œì¥: mycloset-ai-clean'})")
logger.info(f"   - M3 Max: {'âœ…' if IS_M3_MAX else 'âŒ'}")
logger.info(f"   - ë©”ëª¨ë¦¬: {MEMORY_GB:.1f}GB")
logger.info(f"   - MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")

logger.info("ğŸ¯ ì§€ì› Step í´ë˜ìŠ¤ (GitHub êµ¬ì¡° ê¸°ë°˜):")
for step_type in StepType:
    config = BaseStepMixinMapping.get_config(step_type)
    logger.info(f"   - {config.class_name} (Step {config.step_id:02d}) - {config.model_size_gb}GB")

logger.info("ğŸ”¥ í•µì‹¬ ìˆ˜ì •ì‚¬í•­:")
logger.info("   â€¢ BaseStepMixinConfigì— conda_env ë§¤ê°œë³€ìˆ˜ ì¶”ê°€")
logger.info("   â€¢ GitHub ì‹¤ì œ íŒŒì¼ êµ¬ì¡° 100% ë°˜ì˜")
logger.info("   â€¢ BaseStepMixin v18.0 í‘œì¤€ ì™„ì „ ì¤€ìˆ˜")
logger.info("   â€¢ StepFactory v9.0 ì™„ì „ í˜¸í™˜")
logger.info("   â€¢ mycloset-ai-clean í™˜ê²½ ìš°ì„  ìµœì í™”")
logger.info("   â€¢ M3 Max 128GB ë©”ëª¨ë¦¬ ì™„ì „ í™œìš©")
logger.info("   â€¢ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
logger.info("   â€¢ í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë³´ì¥")

logger.info("ğŸš€ ì£¼ìš” í´ë˜ìŠ¤:")
logger.info("   - BaseStepMixinConfig: conda_env ë§¤ê°œë³€ìˆ˜ ì™„ì „ ì§€ì›")
logger.info("   - StepModelInterface: register_model_requirement ì™„ì „ êµ¬í˜„")
logger.info("   - AdvancedMemoryManager: M3 Max 128GB ìµœì í™”")
logger.info("   - EnhancedDependencyManager: ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ì§€ì›")
logger.info("   - BaseStepMixinMapping: GitHub êµ¬ì¡° ê¸°ë°˜ Step ë§¤í•‘")

logger.info("=" * 80)
logger.info("ğŸ‰ Step Interface v3.0 ì™„ì „ ì¤€ë¹„ ì™„ë£Œ!")
logger.info("ğŸ‰ ì´ì œ BaseStepMixinConfig conda_env ì˜¤ë¥˜ê°€ ì™„ì „íˆ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
logger.info("=" * 80)
