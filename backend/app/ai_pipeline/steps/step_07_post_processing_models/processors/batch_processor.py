"""
Batch Processor
ë°°ì¹˜ ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
"""

import torch
import torch.nn as nn
from typing import Dict, Any, List, Optional, Union, Tuple
import numpy as np
from PIL import Image
import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
import gc

# í”„ë¡œì íŠ¸ ë¡œê¹… ì„¤ì • import
import logging

logger = logging.getLogger(__name__)

@dataclass
class BatchResult:
    """ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    input_images: List[torch.Tensor]
    output_images: List[torch.Tensor]
    processing_times: List[float]
    success_flags: List[bool]
    error_messages: List[Optional[str]]
    batch_size: int
    total_processing_time: float

class BatchProcessor:
    """
    ë°°ì¹˜ ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, device: Optional[torch.device] = None):
        """
        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        """
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
        self.batch_config = {
            'max_batch_size': 8,
            'enable_parallel_processing': True,
            'max_workers': 4,
            'memory_limit_mb': 2048,  # 2GB
            'timeout_seconds': 300,  # 5ë¶„
            'enable_progress_tracking': True,
            'fallback_batch_size': 2
        }
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'total_batches': 0,
            'total_images': 0,
            'successful_batches': 0,
            'failed_batches': 0,
            'average_batch_time': 0.0,
            'total_processing_time': 0.0,
            'memory_usage_history': []
        }
        
        logger.info(f"BatchProcessor initialized on device: {self.device}")
    
    def process_batch(self, input_images: List[torch.Tensor], 
                     processing_function: callable,
                     processing_config: Optional[Dict[str, Any]] = None) -> BatchResult:
        """
        ë°°ì¹˜ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤í–‰
        
        Args:
            input_images: ì…ë ¥ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            processing_function: ì²˜ë¦¬ í•¨ìˆ˜
            processing_config: ì²˜ë¦¬ ì„¤ì •
            
        Returns:
            ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ - {len(input_images)}ê°œ ì´ë¯¸ì§€")
            
            # ì„¤ì • ë³‘í•©
            if processing_config is None:
                processing_config = {}
            
            config = {**self.batch_config, **processing_config}
            
            # ë°°ì¹˜ í¬ê¸° ê²°ì •
            optimal_batch_size = self._determine_optimal_batch_size(input_images, config)
            
            # ë°°ì¹˜ ë¶„í• 
            batches = self._split_into_batches(input_images, optimal_batch_size)
            
            # ë°°ì¹˜ë³„ ì²˜ë¦¬
            all_results = []
            batch_times = []
            success_flags = []
            error_messages = []
            
            for i, batch in enumerate(batches):
                logger.info(f"ë°°ì¹˜ {i+1}/{len(batches)} ì²˜ë¦¬ ì¤‘... (í¬ê¸°: {len(batch)})")
                
                try:
                    batch_start_time = time.time()
                    
                    if config['enable_parallel_processing']:
                        batch_result = self._process_batch_parallel(batch, processing_function, config)
                    else:
                        batch_result = self._process_batch_sequential(batch, processing_function, config)
                    
                    batch_time = time.time() - batch_start_time
                    
                    all_results.extend(batch_result)
                    batch_times.extend([batch_time] * len(batch))
                    success_flags.extend([True] * len(batch))
                    error_messages.extend([None] * len(batch))
                    
                    logger.info(f"ë°°ì¹˜ {i+1} ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {batch_time:.3f}s)")
                    
                except Exception as e:
                    batch_time = time.time() - batch_start_time
                    error_msg = f"ë°°ì¹˜ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
                    
                    # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
                    all_results.extend(batch)
                    batch_times.extend([batch_time] * len(batch))
                    success_flags.extend([False] * len(batch))
                    error_messages.extend([error_msg] * len(batch))
                    
                    logger.error(error_msg)
            
            # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            total_processing_time = time.time() - start_time
            
            # ê²°ê³¼ ìƒì„±
            result = BatchResult(
                input_images=input_images,
                output_images=all_results,
                processing_times=batch_times,
                success_flags=success_flags,
                error_messages=error_messages,
                batch_size=optimal_batch_size,
                total_processing_time=total_processing_time
            )
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            self._update_performance_stats(len(batches), len(input_images), 
                                         total_processing_time, True)
            
            logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ (ì´ ì†Œìš”ì‹œê°„: {total_processing_time:.3f}s)")
            return result
            
        except Exception as e:
            total_processing_time = time.time() - start_time
            self._update_performance_stats(0, len(input_images), total_processing_time, False)
            
            logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
            return BatchResult(
                input_images=input_images,
                output_images=input_images,
                processing_times=[total_processing_time] * len(input_images),
                success_flags=[False] * len(input_images),
                error_messages=[str(e)] * len(input_images),
                batch_size=1,
                total_processing_time=total_processing_time
            )
    
    def _determine_optimal_batch_size(self, images: List[torch.Tensor], 
                                     config: Dict[str, Any]) -> int:
        """ìµœì  ë°°ì¹˜ í¬ê¸° ê²°ì •"""
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            total_memory = sum(self._estimate_image_memory(img) for img in images)
            available_memory = config['memory_limit_mb'] * 1024 * 1024  # MB to bytes
            
            # ì•ˆì „ ë§ˆì§„ (80%)
            safe_memory = available_memory * 0.8
            
            if total_memory <= safe_memory:
                # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬ ê°€ëŠ¥
                optimal_size = len(images)
            else:
                # ë©”ëª¨ë¦¬ì— ë§ëŠ” ë°°ì¹˜ í¬ê¸° ê³„ì‚°
                avg_memory_per_image = total_memory / len(images)
                optimal_size = int(safe_memory / avg_memory_per_image)
            
            # ì„¤ì •ëœ ìµœëŒ€ ë°°ì¹˜ í¬ê¸° ì œí•œ
            optimal_size = min(optimal_size, config['max_batch_size'])
            
            # ìµœì†Œ ë°°ì¹˜ í¬ê¸° ë³´ì¥
            optimal_size = max(optimal_size, 1)
            
            logger.info(f"ìµœì  ë°°ì¹˜ í¬ê¸° ê²°ì •: {optimal_size}")
            return optimal_size
            
        except Exception as e:
            logger.error(f"ìµœì  ë°°ì¹˜ í¬ê¸° ê²°ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return config['fallback_batch_size']
    
    def _estimate_image_memory(self, image: torch.Tensor) -> int:
        """ì´ë¯¸ì§€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        try:
            # í…ì„œ í¬ê¸° ê³„ì‚°
            element_size = image.element_size()  # bytes per element
            total_elements = image.numel()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (bytes)
            memory_usage = element_size * total_elements
            
            return memory_usage
            
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 1024 * 1024  # ê¸°ë³¸ê°’: 1MB
    
    def _split_into_batches(self, images: List[torch.Tensor], 
                            batch_size: int) -> List[List[torch.Tensor]]:
        """ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• """
        try:
            batches = []
            for i in range(0, len(images), batch_size):
                batch = images[i:i + batch_size]
                batches.append(batch)
            
            return batches
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return [images]  # ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ
    
    def _process_batch_sequential(self, batch: List[torch.Tensor], 
                                 processing_function: callable,
                                 config: Dict[str, Any]) -> List[torch.Tensor]:
        """ìˆœì°¨ì  ë°°ì¹˜ ì²˜ë¦¬"""
        try:
            results = []
            
            for i, image in enumerate(batch):
                if config['enable_progress_tracking']:
                    logger.debug(f"ì´ë¯¸ì§€ {i+1}/{len(batch)} ì²˜ë¦¬ ì¤‘...")
                
                try:
                    # ì´ë¯¸ì§€ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    device_image = image.to(self.device)
                    
                    # ì²˜ë¦¬ í•¨ìˆ˜ ì‹¤í–‰
                    result = processing_function(device_image)
                    
                    # ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™
                    if result.device != torch.device('cpu'):
                        result = result.cpu()
                    
                    results.append(result)
                    
                except Exception as e:
                    logger.error(f"ì´ë¯¸ì§€ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    results.append(image)  # ì›ë³¸ ë°˜í™˜
            
            return results
            
        except Exception as e:
            logger.error(f"ìˆœì°¨ì  ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return batch  # ì›ë³¸ ë°˜í™˜
    
    def _process_batch_parallel(self, batch: List[torch.Tensor], 
                               processing_function: callable,
                               config: Dict[str, Any]) -> List[torch.Tensor]:
        """ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬"""
        try:
            max_workers = min(config['max_workers'], len(batch))
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ê° ì´ë¯¸ì§€ì— ëŒ€í•œ ì²˜ë¦¬ ì‘ì—… ì œì¶œ
                future_to_index = {
                    executor.submit(self._process_single_image, image, processing_function): i
                    for i, image in enumerate(batch)
                }
                
                # ê²°ê³¼ ìˆ˜ì§‘
                results = [None] * len(batch)
                
                for future in as_completed(future_to_index, timeout=config['timeout_seconds']):
                    index = future_to_index[future]
                    
                    try:
                        result = future.result()
                        results[index] = result
                    except Exception as e:
                        logger.error(f"ì´ë¯¸ì§€ {index+1} ë³‘ë ¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        results[index] = batch[index]  # ì›ë³¸ ë°˜í™˜
                
                # None ê°’ ì²˜ë¦¬
                for i, result in enumerate(results):
                    if result is None:
                        results[i] = batch[i]  # ì›ë³¸ ë°˜í™˜
                
                return results
                
        except Exception as e:
            logger.error(f"ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self._process_batch_sequential(batch, processing_function, config)
    
    def _process_single_image(self, image: torch.Tensor, 
                             processing_function: callable) -> torch.Tensor:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            device_image = image.to(self.device)
            
            # ì²˜ë¦¬ í•¨ìˆ˜ ì‹¤í–‰
            result = processing_function(device_image)
            
            # ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™
            if result.device != torch.device('cpu'):
                result = result.cpu()
            
            return result
            
        except Exception as e:
            logger.error(f"ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return image  # ì›ë³¸ ë°˜í™˜
    
    def _update_performance_stats(self, num_batches: int, num_images: int, 
                                 processing_time: float, success: bool):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.performance_stats['total_batches'] += num_batches
            self.performance_stats['total_images'] += num_images
            self.performance_stats['total_processing_time'] += processing_time
            
            if success:
                self.performance_stats['successful_batches'] += num_batches
            else:
                self.performance_stats['failed_batches'] += num_batches
            
            # í‰ê·  ë°°ì¹˜ ì‹œê°„ ì—…ë°ì´íŠ¸
            total_successful = self.performance_stats['successful_batches']
            if total_successful > 0:
                self.performance_stats['average_batch_time'] = \
                    self.performance_stats['total_processing_time'] / total_successful
                    
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        try:
            stats = self.performance_stats.copy()
            
            # ì¶”ê°€ í†µê³„ ê³„ì‚°
            if stats['total_batches'] > 0:
                stats['success_rate'] = stats['successful_batches'] / stats['total_batches']
                stats['failure_rate'] = stats['failed_batches'] / stats['total_batches']
            else:
                stats['success_rate'] = 0.0
                stats['failure_rate'] = 0.0
            
            if stats['total_images'] > 0:
                stats['average_images_per_batch'] = stats['total_images'] / stats['total_batches']
            else:
                stats['average_images_per_batch'] = 0.0
            
            return stats
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}
    
    def reset_statistics(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        try:
            self.performance_stats = {
                'total_batches': 0,
                'total_images': 0,
                'successful_batches': 0,
                'failed_batches': 0,
                'average_batch_time': 0.0,
                'total_processing_time': 0.0,
                'memory_usage_history': []
            }
            logger.info("ë°°ì¹˜ ì²˜ë¦¬ í†µê³„ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"í†µê³„ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def set_batch_config(self, **kwargs):
        """ë°°ì¹˜ ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.batch_config.update(kwargs)
        logger.info("ë°°ì¹˜ ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    def get_batch_config(self) -> Dict[str, Any]:
        """ë°°ì¹˜ ì„¤ì • ë°˜í™˜"""
        return self.batch_config.copy()
    
    def get_memory_usage(self) -> Dict[str, Any]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        try:
            if torch.cuda.is_available():
                memory_info = {
                    'device': 'CUDA',
                    'allocated_mb': torch.cuda.memory_allocated() / (1024 * 1024),
                    'cached_mb': torch.cuda.memory_reserved() / (1024 * 1024),
                    'total_mb': torch.cuda.get_device_properties(0).total_memory / (1024 * 1024)
                }
            else:
                memory_info = {
                    'device': 'CPU',
                    'allocated_mb': 0.0,
                    'cached_mb': 0.0,
                    'total_mb': 0.0
                }
            
            return memory_info
            
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {'error': str(e)}

class PostProcessingBatchProcessor(nn.Module):
    """í›„ì²˜ë¦¬ ë°°ì¹˜ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__()
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        
        # MPS ë””ë°”ì´ìŠ¤ í™•ì¸
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        self.logger.info(f"ğŸ¯ Post Processing ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
        self.batch_config = {
            'max_batch_size': 8,
            'enable_parallel_processing': True,
            'max_workers': 4,
            'memory_limit_mb': 2048,
            'timeout_seconds': 300,
            'enable_progress_tracking': True,
            'fallback_batch_size': 2
        }
        
        # ì„¤ì • ë³‘í•©
        self.batch_config.update(self.config)
        
        # ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.batch_processor = BatchProcessor(device=self.device)
        self.batch_processor.set_batch_config(**self.batch_config)
        
        # í›„ì²˜ë¦¬ ë„¤íŠ¸ì›Œí¬
        self.post_processing_net = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 3, 3, padding=1),
            nn.Tanh()
        ).to(self.device)
        
        self.logger.info("âœ… Post Processing ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def forward(self, batch_images: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        ë°°ì¹˜ ì´ë¯¸ì§€ í›„ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            batch_images: í›„ì²˜ë¦¬í•  ì´ë¯¸ì§€ ë°°ì¹˜ (B, C, H, W)
            
        Returns:
            í›„ì²˜ë¦¬ëœ ë°°ì¹˜ ê²°ê³¼
        """
        batch_size, channels, height, width = batch_images.shape
        
        # ì…ë ¥ ê²€ì¦
        if channels != 3:
            raise ValueError(f"Expected 3 channels, got {channels}")
        
        # ì…ë ¥ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        batch_images = batch_images.to(self.device)
        
        # í›„ì²˜ë¦¬ ë„¤íŠ¸ì›Œí¬ ì ìš©
        processed_batch = self.post_processing_net(batch_images)
        
        # ê²°ê³¼ ë°˜í™˜
        result = {
            'processed_batch': processed_batch,
            'batch_size': batch_size,
            'input_size': (height, width),
            'device': str(self.device)
        }
        
        return result
    
    def process_batch(self, batch_images: List[torch.Tensor]) -> Dict[str, Any]:
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ í›„ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            batch_images: í›„ì²˜ë¦¬í•  ì´ë¯¸ì§€ ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼
        """
        try:
            # ë°°ì¹˜ ì²˜ë¦¬ ìˆ˜í–‰
            result = self.batch_processor.process_batch(
                batch_images, 
                self._process_single_image,
                self.batch_config
            )
            
            return {
                'status': 'success',
                'batch_result': result,
                'config': self.batch_config
            }
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'message': 'ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
            }
    
    def _process_single_image(self, image: torch.Tensor) -> torch.Tensor:
        """ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ í›„ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            # ë‹¨ì¼ ì´ë¯¸ì§€ í›„ì²˜ë¦¬
            processed = self.forward(image.unsqueeze(0))
            return processed['processed_batch'].squeeze(0)
            
        except Exception as e:
            self.logger.error(f"ë‹¨ì¼ ì´ë¯¸ì§€ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def get_batch_stats(self) -> Dict[str, Any]:
        """ë°°ì¹˜ ì²˜ë¦¬ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            batch_stats = self.batch_processor.get_performance_stats()
            memory_usage = self.batch_processor.get_memory_usage()
            
            return {
                **batch_stats,
                'memory_usage': memory_usage,
                'device': str(self.device),
                'config': self.batch_config
            }
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            # ë°°ì¹˜ ì²˜ë¦¬ê¸° ì •ë¦¬
            self.batch_processor.reset_statistics()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.logger.info("Post Processing ë°°ì¹˜ í”„ë¡œì„¸ì„œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì„¤ì •
    config = {
        'max_batch_size': 4,
        'enable_parallel_processing': True,
        'max_workers': 2,
        'memory_limit_mb': 1024,
        'timeout_seconds': 60,
        'enable_progress_tracking': True,
        'fallback_batch_size': 1
    }
    
    # Post Processing ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    batch_processor = PostProcessingBatchProcessor(config)
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    batch_size = 4
    channels = 3
    height = 256
    width = 256
    
    test_batch = torch.randn(batch_size, channels, height, width)
    
    # ë°°ì¹˜ í›„ì²˜ë¦¬ ìˆ˜í–‰
    with torch.no_grad():
        result = batch_processor(test_batch)
        print(f"ë°°ì¹˜ í›„ì²˜ë¦¬ ê²°ê³¼: {result['processed_batch'].shape}")
    
    # ë°°ì¹˜ ì²˜ë¦¬ í†µê³„
    stats = batch_processor.get_batch_stats()
    print(f"ë°°ì¹˜ ì²˜ë¦¬ í†µê³„: {stats}")
    
    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    batch_processor.cleanup()
