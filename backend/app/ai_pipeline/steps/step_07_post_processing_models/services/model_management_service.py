"""
Model Management Service
ëª¨ë¸ì˜ ìƒëª…ì£¼ê¸°, ë²„ì „ ê´€ë¦¬, ë°°í¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
"""

import torch
import torch.nn as nn
from typing import Dict, Any, List, Optional, Union, Tuple
import os
import json
import shutil
import hashlib
import time
from datetime import datetime
from dataclasses import dataclass, asdict
import logging
import pickle
from pathlib import Path

# ë¡œê¹… ì„¤ì •
import logging

logger = logging.getLogger(__name__)

@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    model_name: str
    model_type: str
    version: str
    checkpoint_path: str
    model_size: int  # bytes
    creation_date: str
    last_modified: str
    checksum: str
    device: str
    status: str  # 'active', 'inactive', 'deprecated'
    performance_metrics: Dict[str, float]
    dependencies: List[str]
    description: str

@dataclass
class DeploymentConfig:
    """ë°°í¬ ì„¤ì •ì„ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    target_device: str
    optimization_level: str  # 'none', 'basic', 'advanced'
    quantization: bool
    pruning: bool
    batch_size: int
    memory_limit: int  # MB
    timeout: int  # seconds
    fallback_model: Optional[str]

class ModelManagementService:
    """
    ëª¨ë¸ì˜ ìƒëª…ì£¼ê¸°, ë²„ì „ ê´€ë¦¬, ë°°í¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
    """
    
    def __init__(self, base_path: str = "models", device: Optional[torch.device] = None):
        """
        Args:
            base_path: ëª¨ë¸ ì €ì¥ ê¸°ë³¸ ê²½ë¡œ
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        """
        self.base_path = Path(base_path)
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ëª¨ë¸ ì €ì¥ì†Œ ê²½ë¡œë“¤
        self.models_dir = self.base_path / "active"
        self.archive_dir = self.base_path / "archive"
        self.backup_dir = self.base_path / "backup"
        self.temp_dir = self.base_path / "temp"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_directories()
        
        # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ íŒŒì¼
        self.registry_file = self.base_path / "model_registry.json"
        self.model_registry = self._load_registry()
        
        # ëª¨ë¸ ê´€ë¦¬ ì„¤ì •
        self.management_config = {
            'max_models_per_type': 5,
            'auto_backup': True,
            'backup_interval': 24 * 60 * 60,  # 24ì‹œê°„
            'cleanup_old_versions': True,
            'max_archive_size': 10 * 1024 * 1024 * 1024,  # 10GB
            'checksum_verification': True
        }
        
        logger.info(f"ModelManagementService initialized at {self.base_path}")
    
    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„±"""
        try:
            for directory in [self.models_dir, self.archive_dir, self.backup_dir, self.temp_dir]:
                directory.mkdir(parents=True, exist_ok=True)
            logger.info("ëª¨ë¸ ê´€ë¦¬ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ë””ë ‰í† ë¦¬ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _load_registry(self) -> Dict[str, ModelInfo]:
        """ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë¡œë“œ"""
        try:
            if self.registry_file.exists():
                with open(self.registry_file, 'r', encoding='utf-8') as f:
                    registry_data = json.load(f)
                    return {k: ModelInfo(**v) for k, v in registry_data.items()}
            return {}
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}
    
    def _save_registry(self):
        """ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì €ì¥"""
        try:
            registry_data = {k: asdict(v) for k, v in self.model_registry.items()}
            with open(self.registry_file, 'w', encoding='utf-8') as f:
                json.dump(registry_data, f, indent=2, ensure_ascii=False)
            logger.debug("ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def register_model(self, model: nn.Module, model_name: str, model_type: str, 
                      checkpoint_path: str, version: str = "1.0.0", 
                      description: str = "") -> str:
        """
        ìƒˆë¡œìš´ ëª¨ë¸ì„ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
        
        Args:
            model: ë“±ë¡í•  ëª¨ë¸
            model_name: ëª¨ë¸ ì´ë¦„
            model_type: ëª¨ë¸ íƒ€ì…
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
            version: ëª¨ë¸ ë²„ì „
            description: ëª¨ë¸ ì„¤ëª…
            
        Returns:
            ë“±ë¡ëœ ëª¨ë¸ì˜ ê³ ìœ  ID
        """
        try:
            # ëª¨ë¸ ID ìƒì„±
            model_id = f"{model_type}_{model_name}_{version}"
            
            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ì¦
            if not os.path.exists(checkpoint_path):
                raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
            
            # ëª¨ë¸ í¬ê¸° ê³„ì‚°
            model_size = os.path.getsize(checkpoint_path)
            
            # ì²´í¬ì„¬ ê³„ì‚°
            checksum = self._calculate_checksum(checkpoint_path)
            
            # í˜„ì¬ ì‹œê°„
            current_time = datetime.now().isoformat()
            
            # ëª¨ë¸ ì •ë³´ ìƒì„±
            model_info = ModelInfo(
                model_name=model_name,
                model_type=model_type,
                version=version,
                checkpoint_path=checkpoint_path,
                model_size=model_size,
                creation_date=current_time,
                last_modified=current_time,
                checksum=checksum,
                device=str(self.device),
                status='active',
                performance_metrics={},
                dependencies=self._get_model_dependencies(model),
                description=description
            )
            
            # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ì¶”ê°€
            self.model_registry[model_id] = model_info
            
            # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì €ì¥
            self._save_registry()
            
            logger.info(f"ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model_id}")
            return model_id
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë“±ë¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def _calculate_checksum(self, file_path: str) -> str:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚°"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            logger.error(f"ì²´í¬ì„¬ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""
    
    def _get_model_dependencies(self, model: nn.Module) -> List[str]:
        """ëª¨ë¸ì˜ ì˜ì¡´ì„± ì •ë³´ ì¶”ì¶œ"""
        try:
            dependencies = []
            
            # PyTorch ë²„ì „
            dependencies.append(f"torch=={torch.__version__}")
            
            # CUDA ë²„ì „ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                dependencies.append(f"cuda=={torch.version.cuda}")
            
            # ëª¨ë¸ì˜ íŠ¹ë³„í•œ ì˜ì¡´ì„±ë“¤
            if hasattr(model, 'dependencies'):
                dependencies.extend(model.dependencies)
            
            return dependencies
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì˜ì¡´ì„± ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def get_model_info(self, model_id: str) -> Optional[ModelInfo]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        try:
            return self.model_registry.get(model_id)
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def list_models(self, model_type: Optional[str] = None, status: Optional[str] = None) -> List[ModelInfo]:
        """ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            models = list(self.model_registry.values())
            
            if model_type:
                models = [m for m in models if m.model_type == model_type]
            
            if status:
                models = [m for m in models if m.status == status]
            
            return sorted(models, key=lambda x: x.last_modified, reverse=True)
        except Exception as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def update_model_status(self, model_id: str, status: str) -> bool:
        """ëª¨ë¸ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            if model_id not in self.model_registry:
                logger.warning(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_id}")
                return False
            
            self.model_registry[model_id].status = status
            self.model_registry[model_id].last_modified = datetime.now().isoformat()
            
            self._save_registry()
            logger.info(f"ëª¨ë¸ ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {model_id} -> {status}")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def backup_model(self, model_id: str) -> bool:
        """ëª¨ë¸ ë°±ì—…"""
        try:
            if model_id not in self.model_registry:
                logger.warning(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_id}")
                return False
            
            model_info = self.model_registry[model_id]
            
            # ë°±ì—… íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_filename = f"{model_id}_{timestamp}.pth"
            backup_path = self.backup_dir / backup_filename
            
            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë³µì‚¬
            shutil.copy2(model_info.checkpoint_path, backup_path)
            
            # ë°±ì—… ì •ë³´ ì €ì¥
            backup_info = {
                'model_id': model_id,
                'backup_path': str(backup_path),
                'backup_date': timestamp,
                'original_checksum': model_info.checksum,
                'backup_checksum': self._calculate_checksum(str(backup_path))
            }
            
            backup_info_path = self.backup_dir / f"{model_id}_{timestamp}_info.json"
            with open(backup_info_path, 'w', encoding='utf-8') as f:
                json.dump(backup_info, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ëª¨ë¸ ë°±ì—… ì™„ë£Œ: {model_id} -> {backup_path}")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë°±ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def restore_model(self, model_id: str, backup_timestamp: str) -> bool:
        """ëª¨ë¸ ë³µì›"""
        try:
            if model_id not in self.model_registry:
                logger.warning(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_id}")
                return False
            
            # ë°±ì—… íŒŒì¼ ì°¾ê¸°
            backup_pattern = f"{model_id}_{backup_timestamp}"
            backup_files = list(self.backup_dir.glob(f"{backup_pattern}*.pth"))
            
            if not backup_files:
                logger.warning(f"ë°±ì—… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {backup_pattern}")
                return False
            
            backup_path = backup_files[0]
            
            # ë°±ì—… ì •ë³´ íŒŒì¼ ì°¾ê¸°
            info_files = list(self.backup_dir.glob(f"{backup_pattern}*_info.json"))
            if info_files:
                with open(info_files[0], 'r', encoding='utf-8') as f:
                    backup_info = json.load(f)
                
                # ì²´í¬ì„¬ ê²€ì¦
                if self.management_config['checksum_verification']:
                    current_checksum = self._calculate_checksum(str(backup_path))
                    if current_checksum != backup_info['backup_checksum']:
                        logger.error("ë°±ì—… íŒŒì¼ ì²´í¬ì„¬ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                        return False
            
            # í˜„ì¬ ì²´í¬í¬ì¸íŠ¸ ë°±ì—…
            model_info = self.model_registry[model_id]
            current_backup = f"{model_info.checkpoint_path}.backup"
            shutil.copy2(model_info.checkpoint_path, current_backup)
            
            # ë°±ì—…ì—ì„œ ë³µì›
            shutil.copy2(backup_path, model_info.checkpoint_path)
            
            # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸
            model_info.last_modified = datetime.now().isoformat()
            model_info.checksum = self._calculate_checksum(model_info.checkpoint_path)
            
            self._save_registry()
            
            logger.info(f"ëª¨ë¸ ë³µì› ì™„ë£Œ: {model_id} from {backup_path}")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë³µì› ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def deploy_model(self, model_id: str, deployment_config: DeploymentConfig) -> bool:
        """ëª¨ë¸ ë°°í¬"""
        try:
            if model_id not in self.model_registry:
                logger.warning(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_id}")
                return False
            
            model_info = self.model_registry[model_id]
            
            # ë°°í¬ ë””ë ‰í† ë¦¬ ìƒì„±
            deploy_dir = self.temp_dir / f"deploy_{model_id}_{int(time.time())}"
            deploy_dir.mkdir(parents=True, exist_ok=True)
            
            # ëª¨ë¸ ë¡œë“œ ë° ìµœì í™”
            checkpoint = torch.load(model_info.checkpoint_path, map_location='cpu')
            model = checkpoint.get('model', checkpoint)
            
            # ìµœì í™” ì ìš©
            if deployment_config.quantization:
                model = self._quantize_model(model)
            
            if deployment_config.pruning:
                model = self._prune_model(model)
            
            # ìµœì í™”ëœ ëª¨ë¸ ì €ì¥
            optimized_path = deploy_dir / "optimized_model.pth"
            torch.save({
                'model': model,
                'deployment_config': asdict(deployment_config),
                'original_model_id': model_id,
                'deployment_date': datetime.now().isoformat()
            }, optimized_path)
            
            # ë°°í¬ ì •ë³´ ì €ì¥
            deployment_info = {
                'model_id': model_id,
                'deployment_config': asdict(deployment_config),
                'deployment_path': str(optimized_path),
                'deployment_date': datetime.now().isoformat(),
                'status': 'deployed'
            }
            
            deployment_info_path = deploy_dir / "deployment_info.json"
            with open(deployment_info_path, 'w', encoding='utf-8') as f:
                json.dump(deployment_info, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ëª¨ë¸ ë°°í¬ ì™„ë£Œ: {model_id} -> {deploy_dir}")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë°°í¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def _quantize_model(self, model: nn.Module) -> nn.Module:
        """ëª¨ë¸ ì–‘ìí™” (ê°„ë‹¨í•œ êµ¬í˜„)"""
        try:
            # ë™ì  ì–‘ìí™” ì ìš©
            quantized_model = torch.quantization.quantize_dynamic(
                model, {nn.Linear, nn.Conv2d}, dtype=torch.qint8
            )
            return quantized_model
        except Exception as e:
            logger.warning(f"ëª¨ë¸ ì–‘ìí™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ì›ë³¸ ëª¨ë¸ ë°˜í™˜: {e}")
            return model
    
    def _prune_model(self, model: nn.Module) -> nn.Module:
        """ëª¨ë¸ í”„ë£¨ë‹ (ê°„ë‹¨í•œ êµ¬í˜„)"""
        try:
            # ê°„ë‹¨í•œ ê°€ì¤‘ì¹˜ í”„ë£¨ë‹
            for name, module in model.named_modules():
                if isinstance(module, nn.Conv2d):
                    # ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’ì´ ì‘ì€ ê²ƒë“¤ì„ 0ìœ¼ë¡œ ì„¤ì •
                    weight = module.weight.data
                    threshold = torch.quantile(torch.abs(weight), 0.1)  # í•˜ìœ„ 10% ì œê±°
                    mask = torch.abs(weight) > threshold
                    module.weight.data = weight * mask.float()
            
            return model
        except Exception as e:
            logger.warning(f"ëª¨ë¸ í”„ë£¨ë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ì›ë³¸ ëª¨ë¸ ë°˜í™˜: {e}")
            return model
    
    def cleanup_old_versions(self, model_type: str, keep_count: int = 3) -> int:
        """ì˜¤ë˜ëœ ëª¨ë¸ ë²„ì „ ì •ë¦¬"""
        try:
            if not self.management_config['cleanup_old_versions']:
                return 0
            
            # í•´ë‹¹ íƒ€ì…ì˜ ëª¨ë¸ë“¤ ì¡°íšŒ
            models = [m for m in self.model_registry.values() if m.model_type == model_type]
            
            if len(models) <= keep_count:
                return 0
            
            # ìˆ˜ì •ì¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  ì˜¤ë˜ëœ ê²ƒë“¤ ì œê±°
            models.sort(key=lambda x: x.last_modified)
            models_to_remove = models[:-keep_count]
            
            removed_count = 0
            for model in models_to_remove:
                model_id = f"{model.model_type}_{model.model_name}_{model.version}"
                
                # ì•„ì¹´ì´ë¸Œë¡œ ì´ë™
                archive_path = self.archive_dir / f"{model_id}.pth"
                if os.path.exists(model.checkpoint_path):
                    shutil.move(model.checkpoint_path, archive_path)
                
                # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ì œê±°
                del self.model_registry[model_id]
                removed_count += 1
            
            self._save_registry()
            logger.info(f"ì˜¤ë˜ëœ ëª¨ë¸ ë²„ì „ {removed_count}ê°œ ì •ë¦¬ ì™„ë£Œ")
            return removed_count
            
        except Exception as e:
            logger.error(f"ì˜¤ë˜ëœ ëª¨ë¸ ë²„ì „ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 0
    
    def get_model_statistics(self) -> Dict[str, Any]:
        """ëª¨ë¸ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            total_models = len(self.model_registry)
            active_models = len([m for m in self.model_registry.values() if m.status == 'active'])
            inactive_models = len([m for m in self.model_registry.values() if m.status == 'inactive'])
            deprecated_models = len([m for m in self.model_registry.values() if m.status == 'deprecated'])
            
            total_size = sum(m.model_size for m in self.model_registry.values())
            
            # íƒ€ì…ë³„ í†µê³„
            type_stats = {}
            for model in self.model_registry.values():
                if model.model_type not in type_stats:
                    type_stats[model.model_type] = 0
                type_stats[model.model_type] += 1
            
            return {
                'total_models': total_models,
                'active_models': active_models,
                'inactive_models': inactive_models,
                'deprecated_models': deprecated_models,
                'total_size_bytes': total_size,
                'total_size_mb': total_size / (1024 * 1024),
                'type_distribution': type_stats
            }
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ í†µê³„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}
    
    def validate_model_integrity(self, model_id: str) -> bool:
        """ëª¨ë¸ ë¬´ê²°ì„± ê²€ì¦"""
        try:
            if model_id not in self.model_registry:
                return False
            
            model_info = self.model_registry[model_id]
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(model_info.checkpoint_path):
                logger.error(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_info.checkpoint_path}")
                return False
            
            # ì²´í¬ì„¬ ê²€ì¦
            if self.management_config['checksum_verification']:
                current_checksum = self._calculate_checksum(model_info.checkpoint_path)
                if current_checksum != model_info.checksum:
                    logger.error(f"ì²´í¬ì„¬ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_id}")
                    return False
            
            # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
            try:
                checkpoint = torch.load(model_info.checkpoint_path, map_location='cpu')
                logger.info(f"ëª¨ë¸ ë¬´ê²°ì„± ê²€ì¦ ì„±ê³µ: {model_id}")
                return True
            except Exception as e:
                logger.error(f"ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {model_id}, ì˜¤ë¥˜: {e}")
                return False
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¬´ê²°ì„± ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

class PostProcessingModelManagementService:
    """í›„ì²˜ë¦¬ ëª¨ë¸ ê´€ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        
        # MPS ë””ë°”ì´ìŠ¤ í™•ì¸
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        self.logger.info(f"ğŸ¯ Post Processing ëª¨ë¸ ê´€ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # ê¸°ë³¸ ëª¨ë¸ ê´€ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        base_path = self.config.get('base_path', 'post_processing_models')
        self.model_service = ModelManagementService(base_path=base_path, device=self.device)
        
        # í›„ì²˜ë¦¬ ëª¨ë¸ ì„¤ì •
        self.post_processing_config = {
            'model_types': ['quality_enhancer', 'artifact_remover', 'resolution_enhancer', 'color_corrector', 'final_optimizer'],
            'auto_update': True,
            'version_control': True,
            'performance_tracking': True
        }
        
        # ì„¤ì • ë³‘í•©
        self.post_processing_config.update(self.config)
        
        # í›„ì²˜ë¦¬ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬
        self.post_processing_models = {}
        
        self.logger.info("âœ… Post Processing ëª¨ë¸ ê´€ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def register_post_processing_model(self, model_name: str, model_type: str, 
                                     checkpoint_path: str, version: str = "1.0.0") -> str:
        """
        í›„ì²˜ë¦¬ ëª¨ë¸ì„ ë“±ë¡í•©ë‹ˆë‹¤.
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            model_type: ëª¨ë¸ íƒ€ì…
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            version: ëª¨ë¸ ë²„ì „
            
        Returns:
            ë“±ë¡ëœ ëª¨ë¸ ID
        """
        try:
            if model_type not in self.post_processing_config['model_types']:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
            
            # ëª¨ë¸ ì •ë³´ ìƒì„±
            model_info = ModelInfo(
                model_name=model_name,
                model_type=model_type,
                version=version,
                checkpoint_path=checkpoint_path,
                model_size=os.path.getsize(checkpoint_path),
                creation_date=datetime.now().isoformat(),
                last_modified=datetime.now().isoformat(),
                checksum=self.model_service._calculate_checksum(checkpoint_path),
                device=str(self.device),
                status='active',
                performance_metrics={},
                dependencies=[],
                description=f"Post Processing {model_type} model"
            )
            
            # ëª¨ë¸ ë“±ë¡
            model_id = self.model_service.register_model(model_info)
            
            # í›„ì²˜ë¦¬ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ì¶”ê°€
            self.post_processing_models[model_id] = model_info
            
            self.logger.info(f"í›„ì²˜ë¦¬ ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model_id}")
            return model_id
            
        except Exception as e:
            self.logger.error(f"í›„ì²˜ë¦¬ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")
            raise
    
    def get_post_processing_model(self, model_type: str, version: str = None) -> Optional[ModelInfo]:
        """
        í›„ì²˜ë¦¬ ëª¨ë¸ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
        
        Args:
            model_type: ëª¨ë¸ íƒ€ì…
            version: ëª¨ë¸ ë²„ì „ (Noneì´ë©´ ìµœì‹  ë²„ì „)
            
        Returns:
            ëª¨ë¸ ì •ë³´ ë˜ëŠ” None
        """
        try:
            # í•´ë‹¹ íƒ€ì…ì˜ ëª¨ë¸ë“¤ ì¡°íšŒ
            models = [m for m in self.post_processing_models.values() if m.model_type == model_type]
            
            if not models:
                return None
            
            if version is None:
                # ìµœì‹  ë²„ì „ ë°˜í™˜
                latest_model = max(models, key=lambda x: x.version)
                return latest_model
            else:
                # íŠ¹ì • ë²„ì „ ë°˜í™˜
                for model in models:
                    if model.version == version:
                        return model
                return None
                
        except Exception as e:
            self.logger.error(f"í›„ì²˜ë¦¬ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def update_post_processing_model(self, model_id: str, new_checkpoint_path: str, 
                                   new_version: str = None) -> bool:
        """
        í›„ì²˜ë¦¬ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            model_id: ëª¨ë¸ ID
            new_checkpoint_path: ìƒˆë¡œìš´ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            new_version: ìƒˆë¡œìš´ ë²„ì „
            
        Returns:
            ì—…ë°ì´íŠ¸ ì„±ê³µ ì—¬ë¶€
        """
        try:
            if model_id not in self.post_processing_models:
                raise ValueError(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_id}")
            
            # ê¸°ì¡´ ëª¨ë¸ ì •ë³´
            old_model = self.post_processing_models[model_id]
            
            # ìƒˆ ë²„ì „ ê²°ì •
            if new_version is None:
                # ê¸°ì¡´ ë²„ì „ì—ì„œ íŒ¨ì¹˜ ë²„ì „ ì¦ê°€
                version_parts = old_model.version.split('.')
                if len(version_parts) >= 3:
                    version_parts[2] = str(int(version_parts[2]) + 1)
                    new_version = '.'.join(version_parts)
                else:
                    new_version = old_model.version + ".1"
            
            # ìƒˆ ëª¨ë¸ ì •ë³´ ìƒì„±
            new_model_info = ModelInfo(
                model_name=old_model.model_name,
                model_type=old_model.model_type,
                version=new_version,
                checkpoint_path=new_checkpoint_path,
                model_size=os.path.getsize(new_checkpoint_path),
                creation_date=datetime.now().isoformat(),
                last_modified=datetime.now().isoformat(),
                checksum=self.model_service._calculate_checksum(new_checkpoint_path),
                device=str(self.device),
                status='active',
                performance_metrics=old_model.performance_metrics.copy(),
                dependencies=old_model.dependencies.copy(),
                description=old_model.description
            )
            
            # ê¸°ì¡´ ëª¨ë¸ ë¹„í™œì„±í™”
            old_model.status = 'inactive'
            
            # ìƒˆ ëª¨ë¸ ë“±ë¡
            new_model_id = self.register_post_processing_model(
                new_model_info.model_name,
                new_model_info.model_type,
                new_model_info.checkpoint_path,
                new_model_info.version
            )
            
            self.logger.info(f"í›„ì²˜ë¦¬ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {model_id} -> {new_model_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"í›„ì²˜ë¦¬ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    def get_post_processing_model_stats(self) -> Dict[str, Any]:
        """í›„ì²˜ë¦¬ ëª¨ë¸ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ë³¸ ëª¨ë¸ í†µê³„
            base_stats = self.model_service.get_model_statistics()
            
            # í›„ì²˜ë¦¬ ëª¨ë¸ í†µê³„
            post_processing_stats = {
                'total_post_processing_models': len(self.post_processing_models),
                'models_by_type': {},
                'active_models': len([m for m in self.post_processing_models.values() if m.status == 'active']),
                'inactive_models': len([m for m in self.post_processing_models.values() if m.status == 'inactive'])
            }
            
            # íƒ€ì…ë³„ ëª¨ë¸ ìˆ˜ ê³„ì‚°
            for model in self.post_processing_models.values():
                if model.model_type not in post_processing_stats['models_by_type']:
                    post_processing_stats['models_by_type'][model.model_type] = 0
                post_processing_stats['models_by_type'][model.model_type] += 1
            
            return {
                **base_stats,
                'post_processing': post_processing_stats,
                'device': str(self.device),
                'config': self.post_processing_config
            }
            
        except Exception as e:
            self.logger.error(f"í›„ì²˜ë¦¬ ëª¨ë¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            # ëª¨ë¸ ì„œë¹„ìŠ¤ ì •ë¦¬
            self.model_service.cleanup()
            
            # í›„ì²˜ë¦¬ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì •ë¦¬
            self.post_processing_models.clear()
            
            self.logger.info("Post Processing ëª¨ë¸ ê´€ë¦¬ ì„œë¹„ìŠ¤ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì„¤ì •
    config = {
        'base_path': 'post_processing_models',
        'auto_update': True,
        'version_control': True,
        'performance_tracking': True
    }
    
    # Post Processing ëª¨ë¸ ê´€ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    model_service = PostProcessingModelManagementService(config)
    
    # ëª¨ë¸ ë“±ë¡ ì˜ˆì‹œ
    try:
        model_id = model_service.register_post_processing_model(
            model_name="quality_enhancer_v1",
            model_type="quality_enhancer",
            checkpoint_path="/path/to/checkpoint.pth",
            version="1.0.0"
        )
        print(f"ëª¨ë¸ ë“±ë¡ ì„±ê³µ: {model_id}")
        
        # ëª¨ë¸ ì¡°íšŒ
        model_info = model_service.get_post_processing_model("quality_enhancer")
        if model_info:
            print(f"ëª¨ë¸ ì •ë³´: {model_info.model_name} v{model_info.version}")
        
        # ëª¨ë¸ í†µê³„
        stats = model_service.get_post_processing_model_stats()
        print(f"ëª¨ë¸ í†µê³„: {stats}")
        
    except Exception as e:
        print(f"ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")
    
    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    model_service.cleanup()
