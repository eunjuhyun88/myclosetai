#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Post Processing Monitoring Service
==================================================

ğŸ¯ í›„ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤
âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
âœ… ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
âœ… í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
âœ… M3 Max ìµœì í™”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
import time
import psutil
import gc
import os
import threading
from collections import deque

logger = logging.getLogger(__name__)

@dataclass
class MonitoringServiceConfig:
    """ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ ì„¤ì •"""
    enable_performance_monitoring: bool = True
    enable_memory_monitoring: bool = True
    enable_quality_monitoring: bool = True
    enable_real_time_monitoring: bool = True
    monitoring_interval: float = 1.0  # ì´ˆ
    history_size: int = 100
    use_mps: bool = True

class PostProcessingPerformanceMonitor(nn.Module):
    """í›„ì²˜ë¦¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°"""
    
    def __init__(self, input_channels: int = 3):
        super().__init__()
        self.input_channels = input_channels
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ë„¤íŠ¸ì›Œí¬
        self.monitoring_net = nn.Sequential(
            nn.Conv2d(input_channels, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, input_channels, 3, padding=1),
            nn.Tanh()
        )
        
    def forward(self, x):
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        monitored = self.monitoring_net(x)
        return monitored

class PostProcessingMemoryMonitor(nn.Module):
    """í›„ì²˜ë¦¬ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°"""
    
    def __init__(self, input_channels: int = 3):
        super().__init__()
        self.input_channels = input_channels
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ë„¤íŠ¸ì›Œí¬
        self.monitoring_net = nn.Sequential(
            nn.Conv2d(input_channels, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, input_channels, 3, padding=1),
            nn.Tanh()
        )
        
    def forward(self, x):
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
        monitored = self.monitoring_net(x)
        return monitored

class PostProcessingQualityMonitor(nn.Module):
    """í›„ì²˜ë¦¬ í’ˆì§ˆ ëª¨ë‹ˆí„°"""
    
    def __init__(self, input_channels: int = 3):
        super().__init__()
        self.input_channels = input_channels
        
        # í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ë„¤íŠ¸ì›Œí¬
        self.monitoring_net = nn.Sequential(
            nn.Conv2d(input_channels, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, input_channels, 3, padding=1),
            nn.Tanh()
        )
        
    def forward(self, x):
        # í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
        monitored = self.monitoring_net(x)
        return monitored

class PostProcessingMonitoringService:
    """í›„ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤"""
    
    def __init__(self, config: MonitoringServiceConfig = None):
        self.config = config or MonitoringServiceConfig()
        self.logger = logging.getLogger(__name__)
        
        # MPS ë””ë°”ì´ìŠ¤ í™•ì¸
        self.device = torch.device("mps" if torch.backends.mps.is_available() and self.config.use_mps else "cpu")
        self.logger.info(f"ğŸ¯ Post Processing ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°
        if self.config.enable_performance_monitoring:
            self.performance_monitor = PostProcessingPerformanceMonitor(3).to(self.device)
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°
        if self.config.enable_memory_monitoring:
            self.memory_monitor = PostProcessingMemoryMonitor(3).to(self.device)
        
        # í’ˆì§ˆ ëª¨ë‹ˆí„°
        if self.config.enable_quality_monitoring:
            self.quality_monitor = PostProcessingQualityMonitor(3).to(self.device)
        
        # ëª¨ë‹ˆí„°ë§ ë°ì´í„° íˆìŠ¤í† ë¦¬
        self.monitoring_history = {
            'performance': deque(maxlen=self.config.history_size),
            'memory': deque(maxlen=self.config.history_size),
            'quality': deque(maxlen=self.config.history_size),
            'timestamps': deque(maxlen=self.config.history_size)
        }
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ
        self.monitoring_thread = None
        self.is_monitoring = False
        
        # ëª¨ë‹ˆí„°ë§ í†µê³„
        self.monitoring_stats = {
            'total_monitoring_cycles': 0,
            'performance_monitoring_cycles': 0,
            'memory_monitoring_cycles': 0,
            'quality_monitoring_cycles': 0,
            'total_monitoring_time': 0.0
        }
        
        self.logger.info("âœ… Post Processing ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def start_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤."""
        if self.is_monitoring:
            self.logger.warning("ëª¨ë‹ˆí„°ë§ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return
        
        if not self.config.enable_real_time_monitoring:
            self.logger.warning("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        self.is_monitoring = True
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitoring_thread.start()
        self.logger.info("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ ì¤‘ì§€í•©ë‹ˆë‹¤."""
        if not self.is_monitoring:
            return
        
        self.is_monitoring = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5.0)
        self.logger.info("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.is_monitoring:
            try:
                # ëª¨ë‹ˆí„°ë§ ìˆ˜í–‰
                self._perform_monitoring_cycle()
                
                # ëŒ€ê¸°
                time.sleep(self.config.monitoring_interval)
                
            except Exception as e:
                self.logger.error(f"ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(self.config.monitoring_interval)
    
    def _perform_monitoring_cycle(self):
        """ëª¨ë‹ˆí„°ë§ ì‚¬ì´í´ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            start_time = time.time()
            timestamp = time.time()
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            if self.config.enable_performance_monitoring:
                performance_data = self._monitor_performance()
                self.monitoring_history['performance'].append(performance_data)
                self.monitoring_stats['performance_monitoring_cycles'] += 1
            
            # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
            if self.config.enable_memory_monitoring:
                memory_data = self._monitor_memory()
                self.monitoring_history['memory'].append(memory_data)
                self.monitoring_stats['memory_monitoring_cycles'] += 1
            
            # í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
            if self.config.enable_quality_monitoring:
                quality_data = self._monitor_quality()
                self.monitoring_history['quality'].append(quality_data)
                self.monitoring_stats['quality_monitoring_cycles'] += 1
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            self.monitoring_history['timestamps'].append(timestamp)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            cycle_time = time.time() - start_time
            self.monitoring_stats['total_monitoring_cycles'] += 1
            self.monitoring_stats['total_monitoring_time'] += cycle_time
            
            self.logger.debug(f"ëª¨ë‹ˆí„°ë§ ì‚¬ì´í´ ì™„ë£Œ (ì‹œê°„: {cycle_time:.4f}ì´ˆ)")
            
        except Exception as e:
            self.logger.error(f"ëª¨ë‹ˆí„°ë§ ì‚¬ì´í´ ì‹¤íŒ¨: {e}")
    
    def _monitor_performance(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤."""
        try:
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent(interval=0.1)
            
            # í”„ë¡œì„¸ìŠ¤ë³„ CPU ì‚¬ìš©ë¥ 
            process = psutil.Process()
            process_cpu_percent = process.cpu_percent()
            
            # ì²˜ë¦¬ ì‹œê°„ í†µê³„
            avg_processing_time = 0.0
            if self.monitoring_history['performance']:
                recent_times = [data.get('processing_time', 0) for data in list(self.monitoring_history['performance'])[-10:]]
                avg_processing_time = sum(recent_times) / len(recent_times) if recent_times else 0.0
            
            return {
                'cpu_percent': cpu_percent,
                'process_cpu_percent': process_cpu_percent,
                'avg_processing_time': avg_processing_time,
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")
            return {
                'cpu_percent': 0.0,
                'process_cpu_percent': 0.0,
                'avg_processing_time': 0.0,
                'timestamp': time.time(),
                'error': str(e)
            }
    
    def _monitor_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ë¥¼ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤."""
        try:
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
            system_memory = psutil.virtual_memory()
            
            # í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬
            process = psutil.Process()
            process_memory = process.memory_info()
            
            # PyTorch ë©”ëª¨ë¦¬ (GPUê°€ ìˆëŠ” ê²½ìš°)
            torch_memory = {}
            if torch.cuda.is_available():
                torch_memory = {
                    'cuda_allocated': torch.cuda.memory_allocated() / 1024**3,  # GB
                    'cuda_reserved': torch.cuda.memory_reserved() / 1024**3,    # GB
                    'cuda_max_allocated': torch.cuda.max_memory_allocated() / 1024**3  # GB
                }
            
            return {
                'system_memory_percent': system_memory.percent,
                'system_memory_available_gb': system_memory.available / 1024**3,
                'system_memory_used_gb': system_memory.used / 1024**3,
                'process_memory_rss_gb': process_memory.rss / 1024**3,
                'process_memory_vms_gb': process_memory.vms / 1024**3,
                'torch_memory': torch_memory,
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")
            return {
                'system_memory_percent': 0.0,
                'system_memory_available_gb': 0.0,
                'system_memory_used_gb': 0.0,
                'process_memory_rss_gb': 0.0,
                'process_memory_vms_gb': 0.0,
                'torch_memory': {},
                'timestamp': time.time(),
                'error': str(e)
            }
    
    def _monitor_quality(self) -> Dict[str, Any]:
        """í’ˆì§ˆì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤."""
        try:
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ (ì˜ˆì‹œ)
            quality_score = 0.8  # ì‹¤ì œë¡œëŠ” ê³„ì‚°ëœ ê°’
            artifact_level = 0.1
            sharpness_score = 0.9
            
            return {
                'quality_score': quality_score,
                'artifact_level': artifact_level,
                'sharpness_score': sharpness_score,
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")
            return {
                'quality_score': 0.0,
                'artifact_level': 0.0,
                'sharpness_score': 0.0,
                'timestamp': time.time(),
                'error': str(e)
            }
    
    def get_monitoring_data(self, data_type: str = 'all', limit: int = None) -> Dict[str, Any]:
        """ëª¨ë‹ˆí„°ë§ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            if data_type == 'all':
                data = {
                    'performance': list(self.monitoring_history['performance']),
                    'memory': list(self.monitoring_history['memory']),
                    'quality': list(self.monitoring_history['quality']),
                    'timestamps': list(self.monitoring_history['timestamps'])
                }
            else:
                data = {
                    data_type: list(self.monitoring_history.get(data_type, [])),
                    'timestamps': list(self.monitoring_history['timestamps'])
                }
            
            # ì œí•œ ì ìš©
            if limit:
                for key in data:
                    if isinstance(data[key], list):
                        data[key] = data[key][-limit:]
            
            return data
            
        except Exception as e:
            self.logger.error(f"ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def get_monitoring_stats(self) -> Dict[str, Any]:
        """ëª¨ë‹ˆí„°ë§ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            **self.monitoring_stats,
            'service_config': self.config.__dict__,
            'device': str(self.device),
            'is_monitoring': self.is_monitoring,
            'history_size': len(self.monitoring_history['timestamps'])
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            self.stop_monitoring()
            
            # íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
            for key in self.monitoring_history:
                self.monitoring_history[key].clear()
            
            # í†µê³„ ì´ˆê¸°í™”
            self.monitoring_stats = {
                'total_monitoring_cycles': 0,
                'performance_monitoring_cycles': 0,
                'memory_monitoring_cycles': 0,
                'quality_monitoring_cycles': 0,
                'total_monitoring_time': 0.0
            }
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.logger.info("ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì„¤ì •
    config = MonitoringServiceConfig(
        enable_performance_monitoring=True,
        enable_memory_monitoring=True,
        enable_quality_monitoring=True,
        enable_real_time_monitoring=True,
        monitoring_interval=1.0,
        history_size=100,
        use_mps=True
    )
    
    # ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    monitoring_service = PostProcessingMonitoringService(config)
    
    # ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitoring_service.start_monitoring()
    
    # ì ì‹œ ëŒ€ê¸°
    time.sleep(3)
    
    # ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì¡°íšŒ
    data = monitoring_service.get_monitoring_data('all', limit=5)
    print(f"ëª¨ë‹ˆí„°ë§ ë°ì´í„°: {len(data.get('timestamps', []))}ê°œ")
    
    # ëª¨ë‹ˆí„°ë§ í†µê³„
    stats = monitoring_service.get_monitoring_stats()
    print(f"ëª¨ë‹ˆí„°ë§ í†µê³„: {stats}")
    
    # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
    monitoring_service.stop_monitoring()
    
    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    monitoring_service.cleanup()
