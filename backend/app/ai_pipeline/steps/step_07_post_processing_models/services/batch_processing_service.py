#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Post Processing Batch Processing Service
=======================================================

ğŸ¯ í›„ì²˜ë¦¬ ë°°ì¹˜ ì²˜ë¦¬ ì„œë¹„ìŠ¤
âœ… ë°°ì¹˜ ì²˜ë¦¬ ê´€ë¦¬
âœ… ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
âœ… M3 Max ìµœì í™”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
import time
import psutil
import gc

logger = logging.getLogger(__name__)

@dataclass
class BatchProcessingServiceConfig:
    """ë°°ì¹˜ ì²˜ë¦¬ ì„œë¹„ìŠ¤ ì„¤ì •"""
    batch_size: int = 4
    enable_parallel_processing: bool = True
    enable_memory_optimization: bool = True
    processing_timeout: float = 30.0
    use_mps: bool = True

class PostProcessingBatchProcessingService:
    """í›„ì²˜ë¦¬ ë°°ì¹˜ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self, config: BatchProcessingServiceConfig = None):
        self.config = config or BatchProcessingServiceConfig()
        self.logger = logging.getLogger(__name__)
        
        # MPS ë””ë°”ì´ìŠ¤ í™•ì¸
        self.device = torch.device("mps" if torch.backends.mps.is_available() and self.config.use_mps else "cpu")
        self.logger.info(f"ğŸ¯ Post Processing ë°°ì¹˜ ì²˜ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # ë°°ì¹˜ ì²˜ë¦¬ í†µê³„
        self.processing_stats = {
            'total_batches': 0,
            'total_images': 0,
            'total_processing_time': 0.0,
            'average_processing_time': 0.0
        }
        
        self.logger.info("âœ… Post Processing ë°°ì¹˜ ì²˜ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def process_batch(self, batch_images: List[torch.Tensor]) -> Dict[str, Any]:
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ í›„ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            batch_images: í›„ì²˜ë¦¬í•  ì´ë¯¸ì§€ ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼
        """
        if not batch_images:
            return {
                'status': 'error',
                'message': 'ë°°ì¹˜ ì´ë¯¸ì§€ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.'
            }
        
        try:
            start_time = time.time()
            
            # ë°°ì¹˜ í¬ê¸° í™•ì¸
            batch_size = len(batch_images)
            self.logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
            
            # ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼
            batch_results = []
            
            # ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
            if self.config.enable_parallel_processing and batch_size > 1:
                # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ í…ì„œ ìƒì„±
                batch_tensor = torch.stack(batch_images).to(self.device)
                self.logger.debug("ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ í™œì„±í™”")
                
                # ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ (ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œ)
                processed_batch = self._process_batch_tensor(batch_tensor)
                
                # ê²°ê³¼ ë¶„ë¦¬
                for i in range(batch_size):
                    batch_results.append({
                        'status': 'success',
                        'processed_image': processed_batch[i],
                        'batch_index': i,
                        'processing_time': 0.0  # ë³‘ë ¬ ì²˜ë¦¬ì´ë¯€ë¡œ ê°œë³„ ì‹œê°„ ì¸¡ì • ë¶ˆê°€
                    })
            else:
                # ìˆœì°¨ ì²˜ë¦¬
                for i, image in enumerate(batch_images):
                    try:
                        image_start_time = time.time()
                        
                        # ê°œë³„ ì´ë¯¸ì§€ ì²˜ë¦¬
                        processed_image = self._process_single_image(image)
                        
                        image_processing_time = time.time() - image_start_time
                        
                        batch_results.append({
                            'status': 'success',
                            'processed_image': processed_image,
                            'batch_index': i,
                            'processing_time': image_processing_time
                        })
                        
                        self.logger.debug(f"ì´ë¯¸ì§€ {i} ì²˜ë¦¬ ì™„ë£Œ (ì‹œê°„: {image_processing_time:.4f}ì´ˆ)")
                        
                    except Exception as e:
                        self.logger.error(f"ì´ë¯¸ì§€ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        batch_results.append({
                            'status': 'error',
                            'error': str(e),
                            'batch_index': i,
                            'processing_time': 0.0
                        })
            
            # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            total_processing_time = time.time() - start_time
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_processing_stats(batch_size, total_processing_time)
            
            result = {
                'status': 'success',
                'batch_results': batch_results,
                'batch_size': batch_size,
                'total_processing_time': total_processing_time,
                'average_processing_time': total_processing_time / batch_size,
                'parallel_processing_used': self.config.enable_parallel_processing and batch_size > 1
            }
            
            self.logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ (ì´ ì‹œê°„: {total_processing_time:.4f}ì´ˆ, í‰ê· : {result['average_processing_time']:.4f}ì´ˆ)")
            return result
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'message': 'ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
            }
    
    def _process_batch_tensor(self, batch_tensor: torch.Tensor) -> torch.Tensor:
        """ë°°ì¹˜ í…ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ì›ë³¸ ë°˜í™˜
        # ì‹¤ì œë¡œëŠ” í›„ì²˜ë¦¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬
        return batch_tensor
    
    def _process_single_image(self, image: torch.Tensor) -> torch.Tensor:
        """ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ì›ë³¸ ë°˜í™˜
        # ì‹¤ì œë¡œëŠ” í›„ì²˜ë¦¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬
        return image.to(self.device)
    
    def _update_processing_stats(self, batch_size: int, processing_time: float):
        """ì²˜ë¦¬ í†µê³„ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        self.processing_stats['total_batches'] += 1
        self.processing_stats['total_images'] += batch_size
        self.processing_stats['total_processing_time'] += processing_time
        self.processing_stats['average_processing_time'] = (
            self.processing_stats['total_processing_time'] / 
            self.processing_stats['total_batches']
        )
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            **self.processing_stats,
            'service_config': self.config.__dict__,
            'device': str(self.device)
        }
    
    def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # PyTorch ìºì‹œ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´
            memory = psutil.virtual_memory()
            
            result = {
                'status': 'success',
                'memory_optimization': 'completed',
                'system_memory_percent': memory.percent,
                'system_memory_available_gb': memory.available / 1024**3
            }
            
            self.logger.info("ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            return result
            
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'message': 'ë©”ëª¨ë¦¬ ìµœì í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
            }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            # í†µê³„ ì´ˆê¸°í™”
            self.processing_stats = {
                'total_batches': 0,
                'total_images': 0,
                'total_processing_time': 0.0,
                'average_processing_time': 0.0
            }
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.logger.info("ë°°ì¹˜ ì²˜ë¦¬ ì„œë¹„ìŠ¤ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì„¤ì •
    config = BatchProcessingServiceConfig(
        batch_size=4,
        enable_parallel_processing=True,
        enable_memory_optimization=True,
        processing_timeout=30.0,
        use_mps=True
    )
    
    # ë°°ì¹˜ ì²˜ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    batch_service = PostProcessingBatchProcessingService(config)
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    batch_size = 4
    channels = 3
    height = 256
    width = 256
    
    test_batch = [torch.randn(channels, height, width) for _ in range(batch_size)]
    
    # ë°°ì¹˜ ì²˜ë¦¬ ìˆ˜í–‰
    result = batch_service.process_batch(test_batch)
    print(f"ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼: {result['status']}")
    
    # ì²˜ë¦¬ í†µê³„
    stats = batch_service.get_processing_stats()
    print(f"ì²˜ë¦¬ í†µê³„: {stats}")
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    optimization = batch_service.optimize_memory()
    print(f"ë©”ëª¨ë¦¬ ìµœì í™”: {optimization['status']}")
    
    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    batch_service.cleanup()
