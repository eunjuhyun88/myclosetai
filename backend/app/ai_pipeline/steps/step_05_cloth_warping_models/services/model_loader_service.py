#!/usr/bin/env python3
"""
π”¥ MyCloset AI - Cloth Warping Model Loader Service
==================================================

π― μλ¥ μ›ν•‘ λ¨λΈ λ΅λ” μ„λΉ„μ¤
β… λ¨λΈ λ΅λ”© λ° κ΄€λ¦¬
β… μ²΄ν¬ν¬μΈνΈ κ΄€λ¦¬
β… λ¨λΈ λ²„μ „ κ΄€λ¦¬
β… M3 Max μµμ ν™”
"""

import logging
import torch
import os
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
import json

logger = logging.getLogger(__name__)

@dataclass
class ModelLoaderConfig:
    """λ¨λΈ λ΅λ” μ„¤μ •"""
    models_directory: str = "models"
    checkpoints_directory: str = "checkpoints"
    enable_model_caching: bool = True
    enable_auto_download: bool = False
    use_mps: bool = True

class ClothWarpingModelLoaderService:
    """μλ¥ μ›ν•‘ λ¨λΈ λ΅λ” μ„λΉ„μ¤"""
    
    def __init__(self, config: ModelLoaderConfig = None):
        self.config = config or ModelLoaderConfig()
        self.logger = logging.getLogger(__name__)
        self.logger.info("π― Cloth Warping λ¨λΈ λ΅λ” μ„λΉ„μ¤ μ΄κΈ°ν™”")
        
        # λ¨λΈ μΊμ‹
        self.model_cache = {}
        self.loaded_models = {}
        
        # MPS λ””λ°”μ΄μ¤ ν™•μΈ
        self.device = torch.device("mps" if torch.backends.mps.is_available() and self.config.use_mps else "cpu")
        
        # λ””λ ‰ν† λ¦¬ ν™•μΈ λ° μƒμ„±
        self._ensure_directories()
        
        self.logger.info("β… Cloth Warping λ¨λΈ λ΅λ” μ„λΉ„μ¤ μ΄κΈ°ν™” μ™„λ£")
    
    def _ensure_directories(self):
        """ν•„μ”ν• λ””λ ‰ν† λ¦¬λ“¤μ„ μƒμ„±ν•©λ‹λ‹¤."""
        directories = [self.config.models_directory, self.config.checkpoints_directory]
        
        for directory in directories:
            if not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                self.logger.info(f"λ””λ ‰ν† λ¦¬ μƒμ„±: {directory}")
    
    def load_model(self, model_name: str, model_path: str = None) -> Optional[torch.nn.Module]:
        """λ¨λΈμ„ λ΅λ“ν•©λ‹λ‹¤."""
        try:
            # μΊμ‹μ—μ„ ν™•μΈ
            if self.config.enable_model_caching and model_name in self.model_cache:
                self.logger.info(f"μΊμ‹μ—μ„ λ¨λΈ λ΅λ“: {model_name}")
                return self.model_cache[model_name]
            
            # λ¨λΈ κ²½λ΅ κ²°μ •
            if model_path is None:
                model_path = os.path.join(self.config.models_directory, f"{model_name}.pth")
            
            # λ¨λΈ λ΅λ“
            if os.path.exists(model_path):
                model = torch.load(model_path, map_location=self.device)
                model.to(self.device)
                
                # μΊμ‹μ— μ €μ¥
                if self.config.enable_model_caching:
                    self.model_cache[model_name] = model
                
                self.loaded_models[model_name] = {
                    'path': model_path,
                    'device': str(self.device),
                    'loaded_at': torch.cuda.Event() if self.device.type == 'cuda' else None
                }
                
                self.logger.info(f"λ¨λΈ λ΅λ“ μ™„λ£: {model_name}")
                return model
            else:
                self.logger.error(f"λ¨λΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {model_path}")
                return None
                
        except Exception as e:
            self.logger.error(f"λ¨λΈ λ΅λ“ μ‹¤ν¨: {model_name}, μ¤λ¥: {e}")
            return None
    
    def save_model(self, model: torch.nn.Module, model_name: str, save_path: str = None) -> bool:
        """λ¨λΈμ„ μ €μ¥ν•©λ‹λ‹¤."""
        try:
            # μ €μ¥ κ²½λ΅ κ²°μ •
            if save_path is None:
                save_path = os.path.join(self.config.models_directory, f"{model_name}.pth")
            
            # λ¨λΈ μ €μ¥
            torch.save(model, save_path)
            
            self.logger.info(f"λ¨λΈ μ €μ¥ μ™„λ£: {model_name} -> {save_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"λ¨λΈ μ €μ¥ μ‹¤ν¨: {model_name}, μ¤λ¥: {e}")
            return False
    
    def load_checkpoint(self, checkpoint_name: str, checkpoint_path: str = None) -> Optional[Dict[str, Any]]:
        """μ²΄ν¬ν¬μΈνΈλ¥Ό λ΅λ“ν•©λ‹λ‹¤."""
        try:
            # μ²΄ν¬ν¬μΈνΈ κ²½λ΅ κ²°μ •
            if checkpoint_path is None:
                checkpoint_path = os.path.join(self.config.checkpoints_directory, f"{checkpoint_name}.pth")
            
            # μ²΄ν¬ν¬μΈνΈ λ΅λ“
            if os.path.exists(checkpoint_path):
                checkpoint = torch.load(checkpoint_path, map_location=self.device)
                self.logger.info(f"μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ™„λ£: {checkpoint_name}")
                return checkpoint
            else:
                self.logger.error(f"μ²΄ν¬ν¬μΈνΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {checkpoint_path}")
                return None
                
        except Exception as e:
            self.logger.error(f"μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ‹¤ν¨: {checkpoint_name}, μ¤λ¥: {e}")
            return None
    
    def save_checkpoint(self, checkpoint_data: Dict[str, Any], checkpoint_name: str, save_path: str = None) -> bool:
        """μ²΄ν¬ν¬μΈνΈλ¥Ό μ €μ¥ν•©λ‹λ‹¤."""
        try:
            # μ €μ¥ κ²½λ΅ κ²°μ •
            if save_path is None:
                save_path = os.path.join(self.config.checkpoints_directory, f"{checkpoint_name}.pth")
            
            # μ²΄ν¬ν¬μΈνΈ μ €μ¥
            torch.save(checkpoint_data, save_path)
            
            self.logger.info(f"μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ™„λ£: {checkpoint_name} -> {save_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ‹¤ν¨: {checkpoint_name}, μ¤λ¥: {e}")
            return False
    
    def get_available_models(self) -> List[str]:
        """μ‚¬μ© κ°€λ¥ν• λ¨λΈ λ©λ΅μ„ λ°ν™ν•©λ‹λ‹¤."""
        try:
            models = []
            if os.path.exists(self.config.models_directory):
                for file in os.listdir(self.config.models_directory):
                    if file.endswith('.pth'):
                        models.append(file[:-4])  # .pth ν™•μ¥μ μ κ±°
            return models
        except Exception as e:
            self.logger.error(f"λ¨λΈ λ©λ΅ μ΅°ν μ‹¤ν¨: {e}")
            return []
    
    def get_available_checkpoints(self) -> List[str]:
        """μ‚¬μ© κ°€λ¥ν• μ²΄ν¬ν¬μΈνΈ λ©λ΅μ„ λ°ν™ν•©λ‹λ‹¤."""
        try:
            checkpoints = []
            if os.path.exists(self.config.checkpoints_directory):
                for file in os.listdir(self.config.checkpoints_directory):
                    if file.endswith('.pth'):
                        checkpoints.append(file[:-4])  # .pth ν™•μ¥μ μ κ±°
            return checkpoints
        except Exception as e:
            self.logger.error(f"μ²΄ν¬ν¬μΈνΈ λ©λ΅ μ΅°ν μ‹¤ν¨: {e}")
            return []
    
    def clear_cache(self) -> bool:
        """λ¨λΈ μΊμ‹λ¥Ό μ •λ¦¬ν•©λ‹λ‹¤."""
        try:
            cache_size = len(self.model_cache)
            self.model_cache.clear()
            self.logger.info(f"λ¨λΈ μΊμ‹ μ •λ¦¬ μ™„λ£: {cache_size}κ° λ¨λΈ μ κ±°")
            return True
        except Exception as e:
            self.logger.error(f"λ¨λΈ μΊμ‹ μ •λ¦¬ μ‹¤ν¨: {e}")
            return False
    
    def get_model_info(self, model_name: str) -> Optional[Dict[str, Any]]:
        """λ¨λΈ μ •λ³΄λ¥Ό λ°ν™ν•©λ‹λ‹¤."""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name]
        return None
    
    def get_service_stats(self) -> Dict[str, Any]:
        """μ„λΉ„μ¤ ν†µκ³„λ¥Ό λ°ν™ν•©λ‹λ‹¤."""
        return {
            'loaded_models': len(self.loaded_models),
            'cached_models': len(self.model_cache),
            'available_models': self.get_available_models(),
            'available_checkpoints': self.get_available_checkpoints(),
            'device': str(self.device),
            'config': self.config.__dict__
        }

# μ‚¬μ© μμ‹
if __name__ == "__main__":
    # μ„¤μ •
    config = ModelLoaderConfig(
        models_directory="models",
        checkpoints_directory="checkpoints",
        enable_model_caching=True,
        enable_auto_download=False,
        use_mps=True
    )
    
    # λ¨λΈ λ΅λ” μ„λΉ„μ¤ μ΄κΈ°ν™”
    model_loader = ClothWarpingModelLoaderService(config)
    
    # μ‚¬μ© κ°€λ¥ν• λ¨λΈ λ° μ²΄ν¬ν¬μΈνΈ ν™•μΈ
    available_models = model_loader.get_available_models()
    available_checkpoints = model_loader.get_available_checkpoints()
    
    print(f"μ‚¬μ© κ°€λ¥ν• λ¨λΈ: {available_models}")
    print(f"μ‚¬μ© κ°€λ¥ν• μ²΄ν¬ν¬μΈνΈ: {available_checkpoints}")
    
    # μ„λΉ„μ¤ ν†µκ³„
    stats = model_loader.get_service_stats()
    print(f"μ„λΉ„μ¤ ν†µκ³„: {stats}")
    
    # ν…μ¤νΈ λ¨λΈ μƒμ„± λ° μ €μ¥
    test_model = torch.nn.Sequential(
        torch.nn.Linear(10, 20),
        torch.nn.ReLU(),
        torch.nn.Linear(20, 1)
    )
    
    # λ¨λΈ μ €μ¥
    save_success = model_loader.save_model(test_model, "test_model")
    print(f"ν…μ¤νΈ λ¨λΈ μ €μ¥: {'μ„±κ³µ' if save_success else 'μ‹¤ν¨'}")
    
    # λ¨λΈ λ΅λ“
    loaded_model = model_loader.load_model("test_model")
    print(f"ν…μ¤νΈ λ¨λΈ λ΅λ“: {'μ„±κ³µ' if loaded_model is not None else 'μ‹¤ν¨'}")
    
    # μ²΄ν¬ν¬μΈνΈ μ €μ¥
    checkpoint_data = {
        'model_state_dict': test_model.state_dict(),
        'epoch': 100,
        'loss': 0.01
    }
    
    save_checkpoint_success = model_loader.save_checkpoint(checkpoint_data, "test_checkpoint")
    print(f"ν…μ¤νΈ μ²΄ν¬ν¬μΈνΈ μ €μ¥: {'μ„±κ³µ' if save_checkpoint_success else 'μ‹¤ν¨'}")
    
    # μ²΄ν¬ν¬μΈνΈ λ΅λ“
    loaded_checkpoint = model_loader.load_checkpoint("test_checkpoint")
    print(f"ν…μ¤νΈ μ²΄ν¬ν¬μΈνΈ λ΅λ“: {'μ„±κ³µ' if loaded_checkpoint is not None else 'μ‹¤ν¨'}")
