#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Cloth Warping Memory Service
=============================================

ğŸ¯ ì˜ë¥˜ ì›Œí•‘ ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤
âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
âœ… ë©”ëª¨ë¦¬ ìµœì í™”
âœ… ìºì‹œ ê´€ë¦¬
âœ… M3 Max ìµœì í™”
"""

import logging
import psutil
import torch
import gc
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
import time

logger = logging.getLogger(__name__)

@dataclass
class MemoryServiceConfig:
    """ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ ì„¤ì •"""
    enable_memory_monitoring: bool = True
    enable_memory_optimization: bool = True
    enable_cache_management: bool = True
    memory_threshold: float = 0.8  # 80% ì‚¬ìš© ì‹œ ê²½ê³ 
    cache_size_limit: int = 1024 * 1024 * 1024  # 1GB
    use_mps: bool = True

class ClothWarpingMemoryService:
    """ì˜ë¥˜ ì›Œí•‘ ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self, config: MemoryServiceConfig = None):
        self.config = config or MemoryServiceConfig()
        self.logger = logging.getLogger(__name__)
        self.logger.info("ğŸ¯ Cloth Warping ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.memory_usage_history = []
        self.cache_objects = {}
        self.last_cleanup_time = time.time()
        
        # MPS ë””ë°”ì´ìŠ¤ í™•ì¸
        self.device = torch.device("mps" if torch.backends.mps.is_available() and self.config.use_mps else "cpu")
        
        self.logger.info("âœ… Cloth Warping ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_system_memory_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            memory = psutil.virtual_memory()
            return {
                'total': memory.total,
                'available': memory.available,
                'used': memory.used,
                'percent': memory.percent,
                'free': memory.free
            }
        except Exception as e:
            self.logger.error(f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def get_torch_memory_info(self) -> Dict[str, Any]:
        """PyTorch ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            if self.device.type == 'mps':
                # MPS ë””ë°”ì´ìŠ¤ì˜ ê²½ìš°
                return {
                    'device': str(self.device),
                    'memory_allocated': torch.mps.current_allocated_memory(),
                    'memory_reserved': torch.mps.driver_allocated_memory(),
                    'memory_cached': 0  # MPSëŠ” ìºì‹œ ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ
                }
            elif self.device.type == 'cuda':
                # CUDA ë””ë°”ì´ìŠ¤ì˜ ê²½ìš°
                return {
                    'device': str(self.device),
                    'memory_allocated': torch.cuda.memory_allocated(self.device),
                    'memory_reserved': torch.cuda.memory_reserved(self.device),
                    'memory_cached': torch.cuda.memory_cached(self.device)
                }
            else:
                # CPUì˜ ê²½ìš°
                return {
                    'device': str(self.device),
                    'memory_allocated': 0,
                    'memory_reserved': 0,
                    'memory_cached': 0
                }
        except Exception as e:
            self.logger.error(f"PyTorch ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def get_memory_usage_summary(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìš”ì•½ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        system_memory = self.get_system_memory_info()
        torch_memory = self.get_torch_memory_info()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
        memory_info = {
            'timestamp': time.time(),
            'system_memory': system_memory,
            'torch_memory': torch_memory,
            'cache_size': len(self.cache_objects)
        }
        
        self.memory_usage_history.append(memory_info)
        
        # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
        if len(self.memory_usage_history) > 100:
            self.memory_usage_history = self.memory_usage_history[-100:]
        
        return memory_info
    
    def check_memory_threshold(self) -> bool:
        """ë©”ëª¨ë¦¬ ì„ê³„ê°’ì„ í™•ì¸í•©ë‹ˆë‹¤."""
        memory_info = self.get_memory_usage_summary()
        system_memory = memory_info.get('system_memory', {})
        
        if 'percent' in system_memory:
            return system_memory['percent'] > (self.config.memory_threshold * 100)
        
        return False
    
    def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤."""
        if not self.config.enable_memory_optimization:
            return {'status': 'disabled'}
        
        optimization_results = {}
        
        try:
            # PyTorch ìºì‹œ ì •ë¦¬
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
                optimization_results['torch_cache_cleared'] = True
            
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            optimization_results['garbage_collected'] = True
            
            # ìºì‹œ ì •ë¦¬
            if self.config.enable_cache_management:
                cache_cleared = self.clear_cache()
                optimization_results['cache_cleared'] = cache_cleared
            
            # ë©”ëª¨ë¦¬ ì •ë³´ ì—…ë°ì´íŠ¸
            memory_info = self.get_memory_usage_summary()
            optimization_results['memory_info'] = memory_info
            
            self.logger.info("ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            optimization_results['status'] = 'success'
            
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            optimization_results['status'] = 'error'
            optimization_results['error'] = str(e)
        
        return optimization_results
    
    def add_to_cache(self, key: str, value: Any, size: int = 0) -> bool:
        """ìºì‹œì— ê°ì²´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        if not self.config.enable_cache_management:
            return False
        
        try:
            # ìºì‹œ í¬ê¸° í™•ì¸
            current_cache_size = sum(obj.get('size', 0) for obj in self.cache_objects.values())
            
            if current_cache_size + size > self.config.cache_size_limit:
                # ìºì‹œê°€ ê°€ë“ ì°¬ ê²½ìš° ì˜¤ë˜ëœ í•­ëª© ì œê±°
                self._cleanup_cache()
            
            # ìºì‹œì— ì¶”ê°€
            self.cache_objects[key] = {
                'value': value,
                'size': size,
                'timestamp': time.time()
            }
            
            self.logger.debug(f"ìºì‹œì— ì¶”ê°€ë¨: {key} (í¬ê¸°: {size})")
            return True
            
        except Exception as e:
            self.logger.error(f"ìºì‹œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def get_from_cache(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°ì²´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        if not self.config.enable_cache_management:
            return None
        
        try:
            if key in self.cache_objects:
                cache_item = self.cache_objects[key]
                cache_item['last_accessed'] = time.time()
                return cache_item['value']
            return None
            
        except Exception as e:
            self.logger.error(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def clear_cache(self) -> bool:
        """ìºì‹œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        try:
            cleared_count = len(self.cache_objects)
            self.cache_objects.clear()
            self.last_cleanup_time = time.time()
            
            self.logger.info(f"ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {cleared_count}ê°œ í•­ëª© ì œê±°")
            return True
            
        except Exception as e:
            self.logger.error(f"ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def _cleanup_cache(self):
        """ìºì‹œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤ (ë‚´ë¶€ ë©”ì„œë“œ)."""
        try:
            # ì˜¤ë˜ëœ í•­ëª©ë“¤ì„ ì œê±°
            current_time = time.time()
            keys_to_remove = []
            
            for key, item in self.cache_objects.items():
                # 1ì‹œê°„ ì´ìƒ ëœ í•­ëª© ì œê±°
                if current_time - item['timestamp'] > 3600:
                    keys_to_remove.append(key)
            
            for key in keys_to_remove:
                del self.cache_objects[key]
            
            if keys_to_remove:
                self.logger.debug(f"ìºì‹œ ìë™ ì •ë¦¬: {len(keys_to_remove)}ê°œ í•­ëª© ì œê±°")
                
        except Exception as e:
            self.logger.error(f"ìºì‹œ ìë™ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_memory_statistics(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        current_memory = self.get_memory_usage_summary()
        
        # íˆìŠ¤í† ë¦¬ ë¶„ì„
        if self.memory_usage_history:
            memory_percentages = [item['system_memory'].get('percent', 0) for item in self.memory_usage_history]
            avg_memory_usage = sum(memory_percentages) / len(memory_percentages)
            max_memory_usage = max(memory_percentages)
            min_memory_usage = min(memory_percentages)
        else:
            avg_memory_usage = max_memory_usage = min_memory_usage = 0
        
        return {
            'current_memory': current_memory,
            'memory_history': {
                'total_records': len(self.memory_usage_history),
                'average_usage_percent': avg_memory_usage,
                'max_usage_percent': max_memory_usage,
                'min_usage_percent': min_memory_usage
            },
            'cache_info': {
                'total_objects': len(self.cache_objects),
                'cache_size_limit': self.config.cache_size_limit,
                'last_cleanup': self.last_cleanup_time
            },
            'config': self.config.__dict__
        }
    
    def monitor_memory_continuously(self, interval: float = 5.0, duration: float = 60.0):
        """ì§€ì†ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ë¥¼ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤."""
        if not self.config.enable_memory_monitoring:
            self.logger.warning("ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        start_time = time.time()
        self.logger.info(f"ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ê°„ê²©: {interval}ì´ˆ, ì§€ì†ì‹œê°„: {duration}ì´ˆ)")
        
        try:
            while time.time() - start_time < duration:
                memory_info = self.get_memory_usage_summary()
                system_memory = memory_info.get('system_memory', {})
                
                if 'percent' in system_memory:
                    usage_percent = system_memory['percent']
                    self.logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {usage_percent:.1f}%")
                    
                    # ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ê²½ê³ 
                    if usage_percent > (self.config.memory_threshold * 100):
                        self.logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {usage_percent:.1f}%")
                        
                        # ìë™ ìµœì í™”
                        if self.config.enable_memory_optimization:
                            self.optimize_memory()
                
                time.sleep(interval)
                
        except KeyboardInterrupt:
            self.logger.info("ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        self.logger.info("ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì„¤ì •
    config = MemoryServiceConfig(
        enable_memory_monitoring=True,
        enable_memory_optimization=True,
        enable_cache_management=True,
        memory_threshold=0.8,
        cache_size_limit=1024 * 1024 * 1024,  # 1GB
        use_mps=True
    )
    
    # ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    memory_service = ClothWarpingMemoryService(config)
    
    # ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ
    memory_info = memory_service.get_memory_usage_summary()
    print(f"í˜„ì¬ ë©”ëª¨ë¦¬ ì •ë³´: {memory_info}")
    
    # ìºì‹œì— í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ê°€
    test_data = torch.randn(100, 100)
    memory_service.add_to_cache('test_tensor', test_data, size=100 * 100 * 4)  # 4 bytes per float32
    
    # ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ
    retrieved_data = memory_service.get_from_cache('test_tensor')
    print(f"ìºì‹œì—ì„œ ì¡°íšŒëœ ë°ì´í„°: {retrieved_data is not None}")
    
    # ë©”ëª¨ë¦¬ í†µê³„
    stats = memory_service.get_memory_statistics()
    print(f"ë©”ëª¨ë¦¬ í†µê³„: {stats}")
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    optimization_result = memory_service.optimize_memory()
    print(f"ë©”ëª¨ë¦¬ ìµœì í™” ê²°ê³¼: {optimization_result}")
