#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Cloth Warping High Resolution Processor
========================================================

ğŸ¯ ì˜ë¥˜ ì›Œí•‘ ê³ í•´ìƒë„ ì²˜ë¦¬ê¸°
âœ… ê³ í•´ìƒë„ ì›Œí•‘ ì²˜ë¦¬
âœ… ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì›Œí•‘
âœ… í•´ìƒë„ë³„ ìµœì í™”
âœ… M3 Max ìµœì í™”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
import cv2
from PIL import Image

logger = logging.getLogger(__name__)

@dataclass
class HighResolutionConfig:
    """ê³ í•´ìƒë„ ì²˜ë¦¬ ì„¤ì •"""
    input_resolutions: List[Tuple[int, int]] = None
    target_resolution: Tuple[int, int] = (1024, 1024)
    enable_multi_scale_warping: bool = True
    enable_super_resolution: bool = True
    enable_attention: bool = True
    enable_adaptive_warping: bool = True
    use_mps: bool = True
    memory_efficient: bool = True

class MultiScaleWarpingNetwork(nn.Module):
    """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì›Œí•‘ ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, input_channels: int = 6):  # 3 for cloth + 3 for person
        super().__init__()
        self.input_channels = input_channels
        
        # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì›Œí•‘ì„ ìœ„í•œ í”¼ë¼ë¯¸ë“œ êµ¬ì¡°
        self.scale1 = nn.Sequential(
            nn.Conv2d(input_channels, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU()
        )
        
        self.scale2 = nn.Sequential(
            nn.Conv2d(64, 128, 3, padding=1, stride=2),
            nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU()
        )
        
        self.scale3 = nn.Sequential(
            nn.Conv2d(128, 256, 3, padding=1, stride=2),
            nn.ReLU(),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU()
        )
        
        # ìŠ¤ì¼€ì¼ ê°„ ì—°ê²°ì„ ìœ„í•œ ì—…ìƒ˜í”Œë§
        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        self.upsample4 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)
        
        # ì›Œí•‘ ë³€í˜• í•„ë“œ ìƒì„±
        self.warping_field = nn.Sequential(
            nn.Conv2d(448, 256, 3, padding=1),  # 64 + 128 + 256
            nn.ReLU(),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 2, 3, padding=1),  # 2 channels for x, y offsets
            nn.Tanh()
        )
        
    def forward(self, x):
        # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œ
        scale1_features = self.scale1(x)  # ì›ë³¸ í•´ìƒë„
        scale2_features = self.scale2(scale1_features)  # 1/2 í•´ìƒë„
        scale3_features = self.scale3(scale2_features)  # 1/4 í•´ìƒë„
        
        # ìŠ¤ì¼€ì¼ ê°„ ì—°ê²°
        scale2_upsampled = self.upsample2(scale2_features)
        scale3_upsampled = self.upsample4(scale3_features)
        
        # íŠ¹ì§• ìœµí•©
        fused_features = torch.cat([
            scale1_features,
            scale2_upsampled,
            scale3_upsampled
        ], dim=1)
        
        # ì›Œí•‘ ë³€í˜• í•„ë“œ ìƒì„±
        warping_field = self.warping_field(fused_features)
        
        return warping_field

class SuperResolutionWarpingNetwork(nn.Module):
    """ì´ˆí•´ìƒë„ ì›Œí•‘ ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, input_channels: int = 6, scale_factor: int = 2):
        super().__init__()
        self.input_channels = input_channels
        self.scale_factor = scale_factor
        
        # ì´ˆí•´ìƒë„ ì›Œí•‘ì„ ìœ„í•œ ë„¤íŠ¸ì›Œí¬
        self.feature_extraction = nn.Sequential(
            nn.Conv2d(input_channels, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU()
        )
        
        # Pixel Shuffleì„ ì‚¬ìš©í•œ ì—…ìƒ˜í”Œë§
        self.pixel_shuffle = nn.PixelShuffle(scale_factor)
        
        # ì›Œí•‘ ë³€í˜• í•„ë“œ ìƒì„±
        self.warping_field = nn.Sequential(
            nn.Conv2d(input_channels, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 2, 3, padding=1),  # 2 channels for x, y offsets
            nn.Tanh()
        )
        
    def forward(self, x):
        # íŠ¹ì§• ì¶”ì¶œ
        features = self.feature_extraction(x)
        
        # Pixel Shuffle ì—…ìƒ˜í”Œë§
        upsampled = self.pixel_shuffle(features)
        
        # ì›Œí•‘ ë³€í˜• í•„ë“œ ìƒì„±
        warping_field = self.warping_field(x)
        
        return upsampled, warping_field

class AdaptiveWarpingNetwork(nn.Module):
    """ì ì‘í˜• ì›Œí•‘ ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, input_channels: int = 6):
        super().__init__()
        self.input_channels = input_channels
        
        # ì ì‘í˜• ì›Œí•‘ì„ ìœ„í•œ ë„¤íŠ¸ì›Œí¬
        self.adaptive_net = nn.Sequential(
            nn.Conv2d(input_channels, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 2, 3, padding=1),  # 2 channels for x, y offsets
            nn.Tanh()
        )
        
        # ì ì‘í˜• ê°€ì¤‘ì¹˜ ìƒì„±
        self.adaptive_weight = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(input_channels, 32, 1),
            nn.ReLU(),
            nn.Conv2d(32, 1, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # ì ì‘í˜• ì›Œí•‘
        warping_field = self.adaptive_net(x)
        
        # ì ì‘í˜• ê°€ì¤‘ì¹˜
        weight = self.adaptive_weight(x)
        
        return warping_field, weight

class AttentionWarpingModule(nn.Module):
    """ì–´í…ì…˜ ì›Œí•‘ ëª¨ë“ˆ"""
    
    def __init__(self, channels: int):
        super().__init__()
        self.channels = channels
        
        # ê³µê°„ ì–´í…ì…˜
        self.spatial_attention = nn.Sequential(
            nn.Conv2d(channels, 1, 7, padding=3),
            nn.Sigmoid()
        )
        
        # ì±„ë„ ì–´í…ì…˜
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, max(channels // 16, 1), 1),
            nn.ReLU(),
            nn.Conv2d(max(channels // 16, 1), channels, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # ê³µê°„ ì–´í…ì…˜
        spatial_weights = self.spatial_attention(x)
        spatial_attended = x * spatial_weights
        
        # ì±„ë„ ì–´í…ì…˜
        channel_weights = self.channel_attention(x)
        channel_attended = spatial_attended * channel_weights
        
        return channel_attended

class ClothWarpingHighResolutionProcessor(nn.Module):
    """ì˜ë¥˜ ì›Œí•‘ ê³ í•´ìƒë„ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, config: HighResolutionConfig = None):
        super().__init__()
        self.config = config or HighResolutionConfig()
        self.logger = logging.getLogger(__name__)
        
        # MPS ë””ë°”ì´ìŠ¤ í™•ì¸
        self.device = torch.device("mps" if torch.backends.mps.is_available() and self.config.use_mps else "cpu")
        self.logger.info(f"ğŸ¯ Cloth Warping ê³ í•´ìƒë„ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì›Œí•‘ ë„¤íŠ¸ì›Œí¬
        if self.config.enable_multi_scale_warping:
            self.multi_scale_warping = MultiScaleWarpingNetwork(6).to(self.device)
        
        # ì´ˆí•´ìƒë„ ì›Œí•‘ ë„¤íŠ¸ì›Œí¬
        if self.config.enable_super_resolution:
            self.super_resolution_warping = SuperResolutionWarpingNetwork(6, 2).to(self.device)
        
        # ì ì‘í˜• ì›Œí•‘ ë„¤íŠ¸ì›Œí¬
        if self.config.enable_adaptive_warping:
            self.adaptive_warping = AdaptiveWarpingNetwork(6).to(self.device)
        
        # ì–´í…ì…˜ ëª¨ë“ˆ
        if self.config.enable_attention:
            self.attention_module = AttentionWarpingModule(6).to(self.device)
        
        # ìµœì¢… ì›Œí•‘ ì ìš©
        self.final_warping = nn.Sequential(
            nn.Conv2d(6, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 3, 3, padding=1),
            nn.Tanh()
        ).to(self.device)
        
        self.logger.info("âœ… Cloth Warping ê³ í•´ìƒë„ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def forward(self, cloth_image: torch.Tensor, 
                person_image: torch.Tensor,
                target_size: Optional[Tuple[int, int]] = None) -> Dict[str, torch.Tensor]:
        """
        ì˜ë¥˜ë¥¼ ê³ í•´ìƒë„ë¡œ ì›Œí•‘í•©ë‹ˆë‹¤.
        
        Args:
            cloth_image: ì˜ë¥˜ ì´ë¯¸ì§€ (B, C, H, W)
            person_image: ì‚¬ëŒ ì´ë¯¸ì§€ (B, C, H, W)
            target_size: ëª©í‘œ í•´ìƒë„ (H, W)
            
        Returns:
            ì›Œí•‘ëœ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        batch_size, channels, height, width = cloth_image.shape
        
        # ëª©í‘œ í•´ìƒë„ ì„¤ì •
        if target_size is None:
            target_size = self.config.target_resolution
        
        # ì…ë ¥ ê²°í•©
        combined_input = torch.cat([cloth_image, person_image], dim=1)
        
        # ì–´í…ì…˜ ì ìš©
        if self.config.enable_attention:
            attended_input = self.attention_module(combined_input)
            self.logger.debug("ì–´í…ì…˜ ì ìš© ì™„ë£Œ")
        else:
            attended_input = combined_input
        
        # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì›Œí•‘
        if self.config.enable_multi_scale_warping:
            multi_scale_field = self.multi_scale_warping(attended_input)
            self.logger.debug("ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì›Œí•‘ ì™„ë£Œ")
        else:
            multi_scale_field = torch.zeros(batch_size, 2, height, width, device=self.device)
        
        # ì´ˆí•´ìƒë„ ì›Œí•‘
        if self.config.enable_super_resolution:
            upsampled, super_res_field = self.super_resolution_warping(attended_input)
            self.logger.debug("ì´ˆí•´ìƒë„ ì›Œí•‘ ì™„ë£Œ")
        else:
            upsampled = attended_input
            super_res_field = torch.zeros(batch_size, 2, height, width, device=self.device)
        
        # ì ì‘í˜• ì›Œí•‘
        if self.config.enable_adaptive_warping:
            adaptive_field, adaptive_weight = self.adaptive_warping(attended_input)
            self.logger.debug("ì ì‘í˜• ì›Œí•‘ ì™„ë£Œ")
        else:
            adaptive_field = torch.zeros(batch_size, 2, height, width, device=self.device)
            adaptive_weight = torch.ones(batch_size, 1, 1, 1, device=self.device)
        
        # ì›Œí•‘ í•„ë“œ ê²°í•©
        final_warping_field = (
            multi_scale_field * 0.4 + 
            super_res_field * 0.3 + 
            adaptive_field * 0.3
        )
        
        # ì›Œí•‘ ì ìš©
        warped_cloth = self.apply_warping(cloth_image, final_warping_field)
        
        # ìµœì¢… ì¶œë ¥ ì¡°ì •
        output = self.final_warping(torch.cat([warped_cloth, person_image], dim=1))
        
        # ëª©í‘œ í•´ìƒë„ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        if output.shape[-2:] != target_size:
            output = F.interpolate(
                output, 
                size=target_size, 
                mode='bilinear', 
                align_corners=False
            )
        
        # ê²°ê³¼ ë°˜í™˜
        result = {
            'warped_cloth': output,
            'warping_field': final_warping_field,
            'multi_scale_field': multi_scale_field,
            'super_res_field': super_res_field,
            'adaptive_field': adaptive_field,
            'adaptive_weight': adaptive_weight,
            'target_size': target_size,
            'input_size': (height, width)
        }
        
        return result
    
    def apply_warping(self, image: torch.Tensor, warping_field: torch.Tensor) -> torch.Tensor:
        """ì›Œí•‘ í•„ë“œë¥¼ ì ìš©í•©ë‹ˆë‹¤."""
        batch_size, channels, height, width = image.shape
        
        # ê·¸ë¦¬ë“œ ìƒì„±
        grid_y, grid_x = torch.meshgrid(
            torch.linspace(-1, 1, height, device=image.device),
            torch.linspace(-1, 1, width, device=image.device),
            indexing='ij'
        )
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        grid_x = grid_x.unsqueeze(0).expand(batch_size, -1, -1)
        grid_y = grid_y.unsqueeze(0).expand(batch_size, -1, -1)
        
        # ì›Œí•‘ í•„ë“œ ì ìš©
        grid_x = grid_x + warping_field[:, 0, :, :] * 0.1
        grid_y = grid_y + warping_field[:, 1, :, :] * 0.1
        
        # ê·¸ë¦¬ë“œ ì •ê·œí™”
        grid = torch.stack([grid_x, grid_y], dim=-1)
        
        # ì›Œí•‘ ì ìš©
        warped = F.grid_sample(image, grid, mode='bilinear', align_corners=False)
        
        return warped
    
    def process_batch(self, batch_cloth: List[torch.Tensor], 
                     batch_person: List[torch.Tensor],
                     target_sizes: Optional[List[Tuple[int, int]]] = None) -> List[Dict[str, torch.Tensor]]:
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê³ í•´ìƒë„ ì›Œí•‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            batch_cloth: ì˜ë¥˜ ì´ë¯¸ì§€ ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
            batch_person: ì‚¬ëŒ ì´ë¯¸ì§€ ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
            target_sizes: ëª©í‘œ í•´ìƒë„ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì›Œí•‘ëœ ê²°ê³¼ ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for i, (cloth, person) in enumerate(zip(batch_cloth, batch_person)):
            try:
                target_size = target_sizes[i] if target_sizes else None
                result = self.forward(cloth, person, target_size)
                results.append(result)
                self.logger.debug(f"ë°°ì¹˜ {i} ê³ í•´ìƒë„ ì›Œí•‘ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"ë°°ì¹˜ {i} ê³ í•´ìƒë„ ì›Œí•‘ ì‹¤íŒ¨: {e}")
                # ì—ëŸ¬ ë°œìƒ ì‹œ ì›ë³¸ ì˜ë¥˜ ì´ë¯¸ì§€ ë°˜í™˜
                results.append({
                    'warped_cloth': cloth,
                    'warping_field': torch.zeros(1, 2, cloth.shape[-2], cloth.shape[-1], device=self.device),
                    'multi_scale_field': torch.zeros(1, 2, cloth.shape[-2], cloth.shape[-1], device=self.device),
                    'super_res_field': torch.zeros(1, 2, cloth.shape[-2], cloth.shape[-1], device=self.device),
                    'adaptive_field': torch.zeros(1, 2, cloth.shape[-2], cloth.shape[-1], device=self.device),
                    'adaptive_weight': torch.ones(1, 1, 1, 1, device=self.device),
                    'target_size': cloth.shape[-2:],
                    'input_size': cloth.shape[-2:]
                })
        
        return results
    
    def get_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self.config.memory_efficient:
            return {"memory_efficient": False}
        
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            "memory_efficient": True,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "device": str(self.device)
        }
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            'multi_scale_warping_enabled': self.config.enable_multi_scale_warping,
            'super_resolution_enabled': self.config.enable_super_resolution,
            'attention_enabled': self.config.enable_attention,
            'adaptive_warping_enabled': self.config.enable_adaptive_warping,
            'target_resolution': self.config.target_resolution,
            'memory_efficient': self.config.memory_efficient,
            'device': str(self.device),
            'config': self.config.__dict__
        }

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì„¤ì •
    config = HighResolutionConfig(
        input_resolutions=[(256, 256), (512, 512)],
        target_resolution=(1024, 1024),
        enable_multi_scale_warping=True,
        enable_super_resolution=True,
        enable_attention=True,
        enable_adaptive_warping=True,
        use_mps=True,
        memory_efficient=True
    )
    
    # ê³ í•´ìƒë„ ì›Œí•‘ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    hr_processor = ClothWarpingHighResolutionProcessor(config)
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    batch_size = 2
    channels = 3
    height = 256
    width = 256
    
    test_cloth = torch.randn(batch_size, channels, height, width)
    test_person = torch.randn(batch_size, channels, height, width)
    
    # ê³ í•´ìƒë„ ì›Œí•‘ ì²˜ë¦¬ ìˆ˜í–‰
    with torch.no_grad():
        result = hr_processor(test_cloth, test_person)
        
        print("âœ… ê³ í•´ìƒë„ ì›Œí•‘ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ì˜ë¥˜ ì´ë¯¸ì§€ í˜•íƒœ: {test_cloth.shape}")
        print(f"ì‚¬ëŒ ì´ë¯¸ì§€ í˜•íƒœ: {test_person.shape}")
        print(f"ì›Œí•‘ëœ ê²°ê³¼ í˜•íƒœ: {result['warped_cloth'].shape}")
        print(f"ëª©í‘œ í•´ìƒë„: {result['target_size']}")
        print(f"ì²˜ë¦¬ í†µê³„: {hr_processor.get_processing_stats()}")
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {hr_processor.get_memory_usage()}")
