#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 07: Post Processing v10.0 - ì™„ì „ ë¦¬íŒ©í† ë§
============================================================================

âœ… 3ê°œ íŒŒì¼ í†µí•© ë° ì™„ì „ ë¦¬íŒ©í† ë§ (Python ëª¨ë²” ì‚¬ë¡€ ìˆœì„œ)
âœ… BaseStepMixin v20.0 ì™„ì „ ìƒì† ë° í˜¸í™˜
âœ… ë™ê¸° _run_ai_inference() ë©”ì„œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€)
âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  (ESRGAN, SwinIR, Real-ESRGAN, Face Enhancement)
âœ… ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ì§€ì› (ModelLoader, MemoryManager, DataConverter)
âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… ëª©ì—… ì½”ë“œ ì™„ì „ ì œê±°

í•µì‹¬ AI ëª¨ë¸ë“¤:
- ESRGAN_x8.pth (135.9MB) - 8ë°° ì—…ìŠ¤ì¼€ì¼ë§
- RealESRGAN_x4plus.pth (63.9MB) - 4ë°° ê³ í’ˆì§ˆ ì—…ìŠ¤ì¼€ì¼ë§
- SwinIR-M_x4.pth (56.8MB) - ì„¸ë¶€ì‚¬í•­ ë³µì›
- densenet161_enhance.pth (110.6MB) - DenseNet ê¸°ë°˜ í–¥ìƒ
- pytorch_model.bin (823.0MB) - í†µí•© í›„ì²˜ë¦¬ ëª¨ë¸

Author: MyCloset AI Team
Date: 2025-08-01
Version: v10.0 (Complete Refactored)
"""

# ==============================================
# ğŸ”¥ 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ imports (Python í‘œì¤€ ìˆœì„œ)
# ==============================================

import os
import sys
import gc
import time
import asyncio
import logging
import traceback
import hashlib
import json
import base64
import weakref
import math
import warnings
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from contextlib import asynccontextmanager

# ==============================================
# ğŸ”¥ 2. ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ imports
# ==============================================

# NumPy
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

# PIL (Pillow)
try:
    from PIL import Image, ImageEnhance, ImageFilter, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

# OpenCV
try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False
    cv2 = None

# PyTorch ë° AI ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    import torchvision.transforms as transforms
    from torchvision.transforms.functional import resize, to_pil_image, to_tensor
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    F = None
    transforms = None

# scikit-image ê³ ê¸‰ ì²˜ë¦¬ìš©
try:
    from skimage import restoration, filters, exposure
    from skimage.metrics import structural_similarity as ssim
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

# scipy í•„ìˆ˜
try:
    from scipy.ndimage import gaussian_filter, median_filter
    from scipy.signal import convolve2d
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# ==============================================
# ğŸ”¥ 3. ë¡œì»¬ imports (TYPE_CHECKING ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from app.ai_pipeline.utils.memory_manager import MemoryManager
    from app.ai_pipeline.utils.data_converter import DataConverter
    from app.core.di_container import CentralHubDIContainer

# ==============================================
# ğŸ”¥ 4. ì‹œìŠ¤í…œ ì •ë³´ ë° í™˜ê²½ ê°ì§€
# ==============================================

def detect_m3_max() -> bool:
    """M3 Max ê°ì§€"""
    try:
        import platform
        import subprocess
        if platform.system() == 'Darwin' and platform.machine() == 'arm64':
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True, timeout=5)
            return 'apple m3' in result.stdout.lower() or 'apple m' in result.stdout.lower()
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()

# MPS (Apple Silicon) ì§€ì› í™•ì¸
try:
    MPS_AVAILABLE = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
except:
    MPS_AVAILABLE = False

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'python_path': os.path.dirname(os.__file__)
}

# ë””ë°”ì´ìŠ¤ ì„¤ì •
if torch and torch.backends.mps.is_available() and IS_M3_MAX:
    DEVICE = "mps"
    try:
        torch.mps.set_per_process_memory_fraction(0.7)
    except:
        pass
elif torch and torch.cuda.is_available():
    DEVICE = "cuda"
else:
    DEVICE = "cpu"

# ==============================================
# ğŸ”¥ 5. BaseStepMixin ë™ì  import
# ==============================================

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError:
        logging.getLogger(__name__).error("âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨")
        return None

BaseStepMixin = get_base_step_mixin_class()

# í´ë°± í´ë˜ìŠ¤
if BaseStepMixin is None:
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'PostProcessingStep')
            self.step_id = kwargs.get('step_id', 7)
            self.device = kwargs.get('device', DEVICE)
            self.is_initialized = False
            self.is_ready = False
            self.performance_metrics = {'process_count': 0}
            
            # AI ëª¨ë¸ ê´€ë ¨ ì†ì„±ë“¤
            self.ai_models = {}
            self.models_loading_status = {}
            self.model_interface = None
            self.loaded_models = []
            
        async def initialize(self):
            self.is_initialized = True
            self.is_ready = True
            return True
        
        def set_model_loader(self, model_loader):
            self.model_loader = model_loader
        
        def set_memory_manager(self, memory_manager):
            self.memory_manager = memory_manager
            
        def set_data_converter(self, data_converter):
            self.data_converter = data_converter
            
        def set_di_container(self, di_container):
            self.di_container = di_container
        
        async def cleanup(self):
            pass
        
        def get_status(self):
            return {
                'step_name': self.step_name, 
                'step_id': self.step_id,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready
            }
        
        def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
            return {
                'success': False,
                'error': 'BaseStepMixin í´ë°± ëª¨ë“œ',
                'enhanced_image': processed_input.get('fitted_image'),
                'enhancement_quality': 0.0,
                'enhancement_methods_used': []
            }

# ==============================================
# ğŸ”¥ 6. ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class EnhancementMethod(Enum):
    """í–¥ìƒ ë°©ë²•"""
    SUPER_RESOLUTION = "super_resolution"
    FACE_ENHANCEMENT = "face_enhancement"
    NOISE_REDUCTION = "noise_reduction"
    DETAIL_ENHANCEMENT = "detail_enhancement"
    COLOR_CORRECTION = "color_correction"
    CONTRAST_ENHANCEMENT = "contrast_enhancement"
    SHARPENING = "sharpening"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    ULTRA = "ultra"

@dataclass
class PostProcessingConfig:
    """í›„ì²˜ë¦¬ ì„¤ì •"""
    quality_level: QualityLevel = QualityLevel.HIGH
    enabled_methods: List[EnhancementMethod] = field(default_factory=lambda: [
        EnhancementMethod.SUPER_RESOLUTION,
        EnhancementMethod.FACE_ENHANCEMENT,
        EnhancementMethod.DETAIL_ENHANCEMENT,
        EnhancementMethod.COLOR_CORRECTION
    ])
    upscale_factor: int = 4
    max_resolution: Tuple[int, int] = (2048, 2048)
    enhancement_strength: float = 0.8
    enable_face_detection: bool = True
    enable_visualization: bool = True

@dataclass
class PostProcessingResult:
    """í›„ì²˜ë¦¬ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""
    enhanced_image: np.ndarray = None
    enhancement_quality: float = 0.0
    enhancement_methods_used: List[str] = field(default_factory=list)
    processing_time: float = 0.0
    device_used: str = "cpu"
    success: bool = False
    
    # AI ëª¨ë¸ ì„¸ë¶€ ê²°ê³¼
    sr_enhancement: Optional[Dict[str, Any]] = None
    face_enhancement: Optional[Dict[str, Any]] = None
    detail_enhancement: Optional[Dict[str, Any]] = None
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            "enhanced_image": self.enhanced_image.tolist() if isinstance(self.enhanced_image, np.ndarray) else self.enhanced_image,
            "enhancement_quality": self.enhancement_quality,
            "enhancement_methods_used": self.enhancement_methods_used,
            "processing_time": self.processing_time,
            "device_used": self.device_used,
            "success": self.success,
            "sr_enhancement": self.sr_enhancement,
            "face_enhancement": self.face_enhancement,
            "detail_enhancement": self.detail_enhancement,
            "metadata": self.metadata
        }

# ==============================================
# ğŸ”¥ 7. ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
# ==============================================

class SimplifiedRRDB(nn.Module):
    """ê°„ì†Œí™”ëœ RRDB ë¸”ë¡"""
    
    def __init__(self, nf, gc=32):
        super(SimplifiedRRDB, self).__init__()
        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=True)
        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=True)
        self.conv3 = nn.Conv2d(nf + 2 * gc, nf, 3, 1, 1, bias=True)
        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)
    
    def forward(self, x):
        x1 = self.lrelu(self.conv1(x))
        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))
        x3 = self.conv3(torch.cat((x, x1, x2), 1))
        return x3 * 0.2 + x

class SimplifiedESRGANModel(nn.Module):
    """ê°„ì†Œí™”ëœ ESRGAN ëª¨ë¸"""
    
    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=8, upscale=4):
        super(SimplifiedESRGANModel, self).__init__()
        self.upscale = upscale
        
        # Feature extraction
        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)
        
        # Simplified RRDB trunk
        self.trunk = nn.Sequential(*[
            SimplifiedRRDB(nf) for _ in range(nb)
        ])
        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        
        # Upsampling
        self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        self.conv_hr = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)
        
        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)
    
    def forward(self, x):
        fea = self.lrelu(self.conv_first(x))
        trunk = self.trunk_conv(self.trunk(fea))
        fea = fea + trunk
        
        # Upsampling
        fea = self.lrelu(self.upconv1(F.interpolate(fea, scale_factor=2, mode='nearest')))
        fea = self.lrelu(self.upconv2(F.interpolate(fea, scale_factor=2, mode='nearest')))
        
        out = self.conv_last(self.lrelu(self.conv_hr(fea)))
        return out

class SimplifiedSwinIRModel(nn.Module):
    """ê°„ì†Œí™”ëœ SwinIR ëª¨ë¸"""
    
    def __init__(self, img_size=64, in_chans=3, out_chans=3, embed_dim=96):
        super(SimplifiedSwinIRModel, self).__init__()
        
        # Shallow feature extraction
        self.conv_first = nn.Conv2d(in_chans, embed_dim, 3, 1, 1)
        
        # Simplified transformer blocks
        self.layers = nn.ModuleList()
        for i in range(6):  # ê°„ì†Œí™”: 6ê°œ ë ˆì´ì–´
            layer = nn.Sequential(
                nn.Conv2d(embed_dim, embed_dim, 3, 1, 1),
                nn.ReLU(inplace=True),
                nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)
            )
            self.layers.append(layer)
        
        # Reconstruction
        self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)
        self.upsample = nn.Sequential(
            nn.Conv2d(embed_dim, embed_dim * 4, 3, 1, 1),
            nn.PixelShuffle(2),
            nn.Conv2d(embed_dim, embed_dim * 4, 3, 1, 1),
            nn.PixelShuffle(2)
        )
        self.conv_last = nn.Conv2d(embed_dim, out_chans, 3, 1, 1)
    
    def forward(self, x):
        x_first = self.conv_first(x)
        
        res = x_first
        for layer in self.layers:
            res = layer(res) + res
        
        res = self.conv_after_body(res) + x_first
        res = self.upsample(res)
        x = self.conv_last(res)
        
        return x


class ImprovedESRGANModel(nn.Module):
    """ì‹¤ì œ ESRGAN ì•„í‚¤í…ì²˜ - ì™„ì „í•œ ì‹ ê²½ë§ êµ¬ì¡°"""
    
    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=23, upscale=4, gc=32):
        super(ImprovedESRGANModel, self).__init__()
        self.upscale = upscale
        
        # íŠ¹ì§• ì¶”ì¶œ (Feature Extraction)
        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)
        
        # RRDB trunk - ì‹¤ì œ ESRGANì€ 23ê°œì˜ RRDB ë¸”ë¡ ì‚¬ìš©
        trunk_modules = []
        for i in range(nb):
            trunk_modules.append(RRDB(nf, gc))
        self.RRDB_trunk = nn.Sequential(*trunk_modules)
        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        
        # ì—…ìƒ˜í”Œë§ ë„¤íŠ¸ì›Œí¬
        if upscale == 4:
            self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
            self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        elif upscale == 8:
            self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
            self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
            self.upconv3 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        
        # HR ë³€í™˜
        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)
        
        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)
        
    def forward(self, x):
        # ì´ˆê¸° íŠ¹ì§• ì¶”ì¶œ
        fea = self.conv_first(x)
        
        # RRDB trunk í†µê³¼
        trunk = self.RRDB_trunk(fea)
        trunk = self.trunk_conv(trunk)
        fea = fea + trunk
        
        # ì—…ìƒ˜í”Œë§
        if self.upscale == 4:
            fea = self.lrelu(self.upconv1(F.interpolate(fea, scale_factor=2, mode='nearest')))
            fea = self.lrelu(self.upconv2(F.interpolate(fea, scale_factor=2, mode='nearest')))
        elif self.upscale == 8:
            fea = self.lrelu(self.upconv1(F.interpolate(fea, scale_factor=2, mode='nearest')))
            fea = self.lrelu(self.upconv2(F.interpolate(fea, scale_factor=2, mode='nearest')))
            fea = self.lrelu(self.upconv3(F.interpolate(fea, scale_factor=2, mode='nearest')))
        
        # HR ë³€í™˜
        out = self.conv_last(self.lrelu(self.HRconv(fea)))
        return out

class RRDB(nn.Module):
    """Residual in Residual Dense Block - ESRGANì˜ í•µì‹¬ ë¸”ë¡"""
    
    def __init__(self, nf, gc=32):
        super(RRDB, self).__init__()
        self.RDB1 = ResidualDenseBlock_5C(nf, gc)
        self.RDB2 = ResidualDenseBlock_5C(nf, gc)
        self.RDB3 = ResidualDenseBlock_5C(nf, gc)

    def forward(self, x):
        out = self.RDB1(x)
        out = self.RDB2(out)
        out = self.RDB3(out)
        return out * 0.2 + x

class ResidualDenseBlock_5C(nn.Module):
    """5ê°œ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë¥¼ ê°€ì§„ Residual Dense Block"""
    
    def __init__(self, nf=64, gc=32, bias=True):
        super(ResidualDenseBlock_5C, self).__init__()
        
        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=bias)
        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=bias)
        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1, bias=bias)
        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1, bias=bias)
        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1, bias=bias)
        
        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)

    def forward(self, x):
        x1 = self.lrelu(self.conv1(x))
        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))
        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))
        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))
        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))
        return x5 * 0.2 + x


class SimplifiedFaceEnhancementModel(nn.Module):
    """ê°„ì†Œí™”ëœ ì–¼êµ´ í–¥ìƒ ëª¨ë¸"""
    
    def __init__(self, in_channels=3, out_channels=3, num_features=64):
        super(SimplifiedFaceEnhancementModel, self).__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels, num_features, 3, 1, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(num_features, num_features * 2, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(num_features * 2, num_features * 4, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True)
        )
        
        # Residual blocks
        self.res_blocks = nn.Sequential(*[
            SimplifiedResidualBlock(num_features * 4) for _ in range(4)
        ])
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(num_features * 4, num_features * 2, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.ConvTranspose2d(num_features * 2, num_features, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(num_features, out_channels, 3, 1, 1),
            nn.Tanh()
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        res = self.res_blocks(encoded)
        decoded = self.decoder(res)
        return decoded
class ImprovedSwinIRModel(nn.Module):
    """ì‹¤ì œ SwinIR ì•„í‚¤í…ì²˜ - Swin Transformer ê¸°ë°˜"""
    
    def __init__(self, img_size=64, patch_size=1, in_chans=3, out_chans=3, 
                 embed_dim=96, depths=[6, 6, 6, 6], num_heads=[6, 6, 6, 6],
                 window_size=7, mlp_ratio=4., upsampler='pixelshuffle', upscale=4):
        super(ImprovedSwinIRModel, self).__init__()
        
        self.img_size = img_size
        self.patch_size = patch_size
        self.window_size = window_size
        self.upscale = upscale
        self.upsampler = upsampler
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        
        # Swin Transformer ë¸”ë¡ë“¤
        self.layers = nn.ModuleList()
        for i_layer in range(len(depths)):
            layer = BasicLayer(
                dim=embed_dim,
                depth=depths[i_layer],
                num_heads=num_heads[i_layer],
                window_size=window_size,
                mlp_ratio=mlp_ratio
            )
            self.layers.append(layer)
        
        self.norm = nn.LayerNorm(embed_dim)
        
        # ì¬êµ¬ì„± ë„¤íŠ¸ì›Œí¬
        if upsampler == 'pixelshuffle':
            self.conv_before_upsample = nn.Sequential(
                nn.Conv2d(embed_dim, 64, 3, 1, 1), nn.LeakyReLU(inplace=True)
            )
            self.upsample = Upsample(upscale, 64)
            self.conv_last = nn.Conv2d(64, out_chans, 3, 1, 1)
            
    def forward(self, x):
        H, W = x.shape[2:]
        x = self.patch_embed(x)
        
        # Swin Transformer ë¸”ë¡ë“¤ í†µê³¼
        for layer in self.layers:
            x = layer(x)
            
        x = self.norm(x)  # B L C
        x = self.patch_unembed(x, (H, W))
        
        # ì¬êµ¬ì„±
        x = self.conv_before_upsample(x)
        x = self.conv_last(self.upsample(x))
        
        return x
    
    def patch_unembed(self, x, x_size):
        """íŒ¨ì¹˜ ì–¸ì„ë² ë”©"""
        B, HW, C = x.shape
        x = x.transpose(1, 2).view(B, C, x_size[0], x_size[1])
        return x

class PatchEmbed(nn.Module):
    """ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
    
    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.in_chans = in_chans
        self.embed_dim = embed_dim
        
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        
    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        return x

class BasicLayer(nn.Module):
    """Swin Transformer ê¸°ë³¸ ë ˆì´ì–´"""
    
    def __init__(self, dim, depth, num_heads, window_size, mlp_ratio=4.):
        super().__init__()
        self.dim = dim
        self.depth = depth
        
        # Swin Transformer ë¸”ë¡ë“¤
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(
                dim=dim,
                num_heads=num_heads,
                window_size=window_size,
                shift_size=0 if (i % 2 == 0) else window_size // 2,
                mlp_ratio=mlp_ratio
            ) for i in range(depth)
        ])
        
    def forward(self, x):
        for blk in self.blocks:
            x = blk(x)
        return x

class SwinTransformerBlock(nn.Module):
    """Swin Transformer ë¸”ë¡"""
    
    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4.):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        
        self.norm1 = nn.LayerNorm(dim)
        self.attn = WindowAttention(
            dim, window_size=window_size, num_heads=num_heads
        )
        
        self.norm2 = nn.LayerNorm(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim)
        
    def forward(self, x):
        H, W = x.shape[1], x.shape[2]
        B, L, C = x.shape
        
        shortcut = x
        x = self.norm1(x)
        
        # ìœˆë„ìš° ì–´í…ì…˜
        x = self.attn(x)
        x = shortcut + x
        
        # MLP
        x = x + self.mlp(self.norm2(x))
        
        return x

class WindowAttention(nn.Module):
    """ìœˆë„ìš° ê¸°ë°˜ ì–´í…ì…˜"""
    
    def __init__(self, dim, window_size, num_heads):
        super().__init__()
        self.dim = dim
        self.window_size = window_size
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5
        
        self.qkv = nn.Linear(dim, dim * 3, bias=True)
        self.proj = nn.Linear(dim, dim)
        
    def forward(self, x):
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))
        attn = attn.softmax(dim=-1)
        
        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        return x

class Mlp(nn.Module):
    """MLP ë¸”ë¡"""
    
    def __init__(self, in_features, hidden_features=None, out_features=None):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(hidden_features, out_features)
        
    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.fc2(x)
        return x

class Upsample(nn.Module):
    """ì—…ìƒ˜í”Œë§ ëª¨ë“ˆ"""
    
    def __init__(self, scale, num_feat):
        super(Upsample, self).__init__()
        m = []
        if (scale & (scale - 1)) == 0:  # scale = 2^n
            for _ in range(int(math.log(scale, 2))):
                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))
                m.append(nn.PixelShuffle(2))
        elif scale == 3:
            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))
            m.append(nn.PixelShuffle(3))
        else:
            raise ValueError(f'scale {scale} is not supported.')
        
        self.m = nn.Sequential(*m)
        
    def forward(self, x):
        return self.m(x)
        
class SimplifiedResidualBlock(nn.Module):
    """ê°„ì†Œí™”ëœ ì”ì°¨ ë¸”ë¡"""
    
    def __init__(self, channels):
        super(SimplifiedResidualBlock, self).__init__()
        self.conv_block = nn.Sequential(
            nn.Conv2d(channels, channels, 3, 1, 1),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, 3, 1, 1),
            nn.BatchNorm2d(channels)
        )
    
    def forward(self, x):
        return x + self.conv_block(x)

# ==============================================
# ğŸ”¥ 8. ê³ ê¸‰ ëª¨ë¸ ë§¤í¼ ë° ë¡œë”
# ==============================================

class EnhancedModelMapper:
    """ì‹¤ì œ AI ëª¨ë¸ ë§¤í•‘ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.EnhancedModelMapper")
        self.base_path = Path("ai_models")
        self._cache = {}
    
    def get_prioritized_model_paths_with_size_check(self) -> List[Path]:
        """í¬ê¸° ìš°ì„  ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ"""
        try:
            self.logger.info("ğŸ” ì‹¤ì œ AI ëª¨ë¸ íƒì§€ ì‹œì‘...")
            
            # í™•ì¸ëœ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ë“¤
            confirmed_models = {
                'esrgan': [
                    'step_07_post_processing/esrgan_x8_ultra/ESRGAN_x8.pth',
                    'step_07_post_processing/ESRGAN_x8.pth',
                    'ESRGAN_x8.pth'
                ],
                'swinir': [
                    'step_07_post_processing/ultra_models/001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth',
                    'step_07_post_processing/SwinIR-M_x4.pth',
                    'SwinIR-M_x4.pth'
                ],
                'face_enhancement': [
                    'step_07_post_processing/ultra_models/densenet161_enhance.pth',
                    'step_07_post_processing/densenet161_enhance.pth',
                    'densenet161_enhance.pth'
                ],
                'real_esrgan': [
                    'step_07_post_processing/ultra_models/RealESRGAN_x4plus.pth',
                    'step_07_post_processing/RealESRGAN_x4plus.pth',
                    'RealESRGAN_x4plus.pth'
                ]
            }
            
            valid_models = []
            
            for model_type, file_patterns in confirmed_models.items():
                best_model = None
                best_size = 0
                
                for pattern in file_patterns:
                    model_path = self.base_path / pattern
                    
                    if model_path.exists() and model_path.is_file():
                        try:
                            file_size = model_path.stat().st_size
                            size_mb = file_size / (1024 * 1024)
                            
                            # ìµœì†Œ í¬ê¸° í•„í„°ë§ (1MB ì´ìƒ)
                            if size_mb >= 1.0:
                                # ë” í° ëª¨ë¸ ìš°ì„  ì„ íƒ
                                if size_mb > best_size:
                                    best_model = model_path
                                    best_size = size_mb
                                    
                                self.logger.debug(f"âœ… {model_type} ë°œê²¬: {model_path.name} ({size_mb:.1f}MB)")
                        except Exception as e:
                            self.logger.debug(f"íŒŒì¼ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {pattern} - {e}")
                
                if best_model:
                    valid_models.append({
                        'path': best_model,
                        'type': model_type,
                        'size_mb': best_size,
                        'priority': self._get_model_priority(model_type, best_size)
                    })
                    self.logger.info(f"ğŸ¯ ì„ íƒëœ {model_type}: {best_model.name} ({best_size:.1f}MB)")
            
            # ìš°ì„ ìˆœìœ„ + í¬ê¸° ê¸°ì¤€ ì •ë ¬
            valid_models.sort(key=lambda x: (x['priority'], x['size_mb']), reverse=True)
            
            # Path ê°ì²´ë§Œ ë°˜í™˜
            prioritized_paths = [model['path'] for model in valid_models]
            
            self.logger.info(f"ğŸ“Š íƒì§€ ì™„ë£Œ: {len(prioritized_paths)}ê°œ AI ëª¨ë¸")
            
            return prioritized_paths
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ íƒì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_model_priority(self, model_type: str, size_mb: float) -> float:
        """ëª¨ë¸ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        type_priorities = {
            'esrgan': 10.0,
            'face_enhancement': 9.0,
            'swinir': 8.0,
            'real_esrgan': 7.5
        }
        
        base_priority = type_priorities.get(model_type, 5.0)
        size_bonus = min(size_mb / 100, 5.0)
        
        return base_priority + size_bonus

class UltraSafeCheckpointLoader:
    """3ë‹¨ê³„ ì•ˆì „ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.UltraSafeCheckpointLoader")
    
    def load_checkpoint_ultra_safe(self, checkpoint_path: Path) -> Optional[Any]:
        """3ë‹¨ê³„ ì•ˆì „ ë¡œë”©"""
        if not checkpoint_path.exists():
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_path}")
            return None
        
        file_size_mb = checkpoint_path.stat().st_size / (1024 * 1024)
        self.logger.info(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œì‘: {checkpoint_path.name} ({file_size_mb:.1f}MB)")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        if torch and hasattr(torch, 'mps') and torch.backends.mps.is_available():
            try:
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
            except:
                pass
        
        # 1ë‹¨ê³„: ì•ˆì „ ëª¨ë“œ
        try:
            self.logger.debug("1ë‹¨ê³„: weights_only=True ì‹œë„")
            checkpoint = torch.load(
                checkpoint_path, 
                map_location=self.device,
                weights_only=True
            )
            self.logger.info("âœ… ì•ˆì „ ëª¨ë“œ ë¡œë”© ì„±ê³µ")
            return checkpoint
        except Exception as e1:
            self.logger.debug(f"1ë‹¨ê³„ ì‹¤íŒ¨: {str(e1)[:100]}")
        
        # 2ë‹¨ê³„: í˜¸í™˜ì„± ëª¨ë“œ
        try:
            self.logger.debug("2ë‹¨ê³„: weights_only=False ì‹œë„")
            checkpoint = torch.load(
                checkpoint_path, 
                map_location=self.device,
                weights_only=False
            )
            self.logger.info("âœ… í˜¸í™˜ì„± ëª¨ë“œ ë¡œë”© ì„±ê³µ")
            return checkpoint
        except Exception as e2:
            self.logger.debug(f"2ë‹¨ê³„ ì‹¤íŒ¨: {str(e2)[:100]}")
        
        # 3ë‹¨ê³„: Legacy ëª¨ë“œ
        try:
            self.logger.debug("3ë‹¨ê³„: Legacy ëª¨ë“œ ì‹œë„")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.logger.info("âœ… Legacy ëª¨ë“œ ë¡œë”© ì„±ê³µ")
            return checkpoint
        except Exception as e3:
            self.logger.error(f"âŒ ëª¨ë“  í‘œì¤€ ë¡œë”© ì‹¤íŒ¨: {str(e3)[:100]}")
            return None

# ==============================================
# ğŸ”¥ 9. ì‹¤ì œ ì¶”ë¡  ì—”ì§„
# ==============================================

class PostProcessingInferenceEngine:
    """ì‹¤ì œ Post Processing ì¶”ë¡  ì—”ì§„"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.PostProcessingInferenceEngine")
        
        # ImageNet ì •ê·œí™” íŒŒë¼ë¯¸í„°
        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
    
    def prepare_input_tensor(self, image: Union[Image.Image, np.ndarray, torch.Tensor]) -> Optional[torch.Tensor]:
        """ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ìš© í…ì„œë¡œ ë³€í™˜"""
        try:
            # 1. ì´ë¯¸ì§€ íƒ€ì…ë³„ ì²˜ë¦¬
            if isinstance(image, Image.Image):
                image_np = np.array(image.convert('RGB'))
            elif isinstance(image, torch.Tensor):
                if image.dim() == 4:
                    image_np = image.squeeze(0).permute(1, 2, 0).cpu().numpy()
                elif image.dim() == 3:
                    image_np = image.permute(1, 2, 0).cpu().numpy()
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” tensor ì°¨ì›: {image.dim()}")
                
                if image_np.max() <= 1.0:
                    image_np = (image_np * 255).astype(np.uint8)
            elif isinstance(image, np.ndarray):
                image_np = image.copy()
                if image_np.ndim == 2:
                    image_np = np.stack([image_np] * 3, axis=-1)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
            # 2. í¬ê¸° ì •ê·œí™” (512x512)
            h, w = image_np.shape[:2]
            if h != 512 or w != 512:
                image_pil = Image.fromarray(image_np)
                image_pil = image_pil.resize((512, 512), Image.Resampling.BILINEAR)
                image_np = np.array(image_pil)
            
            # 3. ì •ê·œí™” (0-1 ë²”ìœ„)
            if image_np.dtype == np.uint8:
                image_np = image_np.astype(np.float32) / 255.0
            
            # 4. ImageNet ì •ê·œí™”
            mean_np = self.mean.numpy().transpose(1, 2, 0)
            std_np = self.std.numpy().transpose(1, 2, 0)
            normalized = (image_np - mean_np) / std_np
            
            # 5. í…ì„œ ë³€í™˜ (HWC â†’ CHW, ë°°ì¹˜ ì°¨ì› ì¶”ê°€)
            tensor = torch.from_numpy(normalized).permute(2, 0, 1).unsqueeze(0)
            
            # 6. ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            tensor = tensor.to(self.device)
            
            self.logger.debug(f"âœ… ì…ë ¥ í…ì„œ ìƒì„±: {tensor.shape}, device: {tensor.device}")
            return tensor
            
        except Exception as e:
            self.logger.error(f"âŒ ì…ë ¥ í…ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def run_enhancement_inference(self, model: nn.Module, input_tensor: torch.Tensor) -> Optional[Dict[str, torch.Tensor]]:
        """ì‹¤ì œ í–¥ìƒ ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
        try:
            if model is None:
                self.logger.error("âŒ ëª¨ë¸ì´ Noneì…ë‹ˆë‹¤")
                return None
            
            model.eval()
            
            if next(model.parameters()).device != input_tensor.device:
                model = model.to(input_tensor.device)
            
            with torch.no_grad():
                self.logger.debug("ğŸ§  í–¥ìƒ ëª¨ë¸ ì¶”ë¡  ì‹œì‘...")
                
                try:
                    output = model(input_tensor)
                    self.logger.debug(f"âœ… ëª¨ë¸ ì¶œë ¥ íƒ€ì…: {type(output)}")
                    
                    if isinstance(output, dict):
                        return output
                    elif isinstance(output, (list, tuple)):
                        return {'enhanced': output[0]}
                    else:
                        return {'enhanced': output}
                        
                except Exception as inference_error:
                    self.logger.error(f"âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {inference_error}")
                    return None
                    
        except Exception as e:
            self.logger.error(f"âŒ í–¥ìƒ ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None
    
    def run_super_resolution_inference(self, model: nn.Module, input_tensor: torch.Tensor) -> Optional[Dict[str, Any]]:
        """ğŸ”¥ ESRGAN Super Resolution ì‹¤ì œ ì¶”ë¡ """
        try:
            self.logger.debug("ğŸ”¬ ESRGAN Super Resolution ì¶”ë¡  ì‹œì‘...")
            
            with torch.no_grad():
                # ESRGAN ì¶”ë¡ 
                sr_output = model(input_tensor)
                
                # ê²°ê³¼ í´ë¨í•‘
                sr_output = torch.clamp(sr_output, 0, 1)
                
                # í’ˆì§ˆ í‰ê°€
                quality_score = self._calculate_enhancement_quality(input_tensor, sr_output)
                
                self.logger.debug(f"âœ… Super Resolution ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}")
                
                return {
                    'enhanced_tensor': sr_output,
                    'quality_score': quality_score,
                    'method': 'ESRGAN',
                    'upscale_factor': 4
                }
                
        except Exception as e:
            self.logger.error(f"âŒ Super Resolution ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    def run_face_enhancement_inference(self, model: nn.Module, input_tensor: torch.Tensor) -> Optional[Dict[str, Any]]:
        """ğŸ”¥ ì–¼êµ´ í–¥ìƒ ì‹¤ì œ ì¶”ë¡ """
        try:
            self.logger.debug("ğŸ‘¤ ì–¼êµ´ í–¥ìƒ ì¶”ë¡  ì‹œì‘...")
            
            # ì–¼êµ´ ê²€ì¶œ
            faces = self._detect_faces_in_tensor(input_tensor)
            
            if not faces:
                self.logger.debug("ğŸ‘¤ ì–¼êµ´ì´ ê²€ì¶œë˜ì§€ ì•ŠìŒ")
                return None
            
            with torch.no_grad():
                # ì–¼êµ´ í–¥ìƒ ì¶”ë¡ 
                enhanced_output = model(input_tensor)
                
                # ê²°ê³¼ ì •ê·œí™”
                enhanced_output = torch.clamp(enhanced_output, -1, 1)
                enhanced_output = (enhanced_output + 1) / 2  # [-1, 1] â†’ [0, 1]
                
                # í’ˆì§ˆ í‰ê°€
                quality_score = self._calculate_enhancement_quality(input_tensor, enhanced_output)
                
                self.logger.debug(f"âœ… ì–¼êµ´ í–¥ìƒ ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}")
                
                return {
                    'enhanced_tensor': enhanced_output,
                    'quality_score': quality_score,
                    'method': 'FaceEnhancement',
                    'faces_detected': len(faces)
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ì–¼êµ´ í–¥ìƒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    def run_detail_enhancement_inference(self, model: nn.Module, input_tensor: torch.Tensor) -> Optional[Dict[str, Any]]:
        """ğŸ”¥ SwinIR ì„¸ë¶€ì‚¬í•­ í–¥ìƒ ì‹¤ì œ ì¶”ë¡ """
        try:
            self.logger.debug("ğŸ” SwinIR ì„¸ë¶€ì‚¬í•­ í–¥ìƒ ì¶”ë¡  ì‹œì‘...")
            
            with torch.no_grad():
                # SwinIR ì¶”ë¡ 
                detail_output = model(input_tensor)
                
                # ê²°ê³¼ í´ë¨í•‘
                detail_output = torch.clamp(detail_output, 0, 1)
                
                # í’ˆì§ˆ í‰ê°€
                quality_score = self._calculate_enhancement_quality(input_tensor, detail_output)
                
                self.logger.debug(f"âœ… ì„¸ë¶€ì‚¬í•­ í–¥ìƒ ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}")
                
                return {
                    'enhanced_tensor': detail_output,
                    'quality_score': quality_score,
                    'method': 'SwinIR',
                    'detail_level': 'high'
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ì„¸ë¶€ì‚¬í•­ í–¥ìƒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    def _detect_faces_in_tensor(self, tensor: torch.Tensor) -> List[Tuple[int, int, int, int]]:
        """í…ì„œì—ì„œ ì–¼êµ´ ê²€ì¶œ"""
        try:
            if not OPENCV_AVAILABLE:
                return []
            
            # Tensor â†’ NumPy
            image_np = tensor.squeeze().cpu().numpy()
            if len(image_np.shape) == 3:
                image_np = np.transpose(image_np, (1, 2, 0))
            
            # 0-255 ë²”ìœ„ë¡œ ë³€í™˜
            if image_np.max() <= 1.0:
                image_np = (image_np * 255).astype(np.uint8)
            else:
                image_np = image_np.astype(np.uint8)
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
            
            # ê¸°ë³¸ ì–¼êµ´ ê²€ì¶œê¸° (Haar Cascade)
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            faces = face_cascade.detectMultiScale(
                gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
            )
            
            return [tuple(face) for face in faces]
            
        except Exception as e:
            self.logger.debug(f"ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_enhancement_quality(self, original_tensor: torch.Tensor, enhanced_tensor: torch.Tensor) -> float:
        """í–¥ìƒ í’ˆì§ˆ ê³„ì‚°"""
        try:
            if not torch:
                return 0.5
            
            # ê°„ë‹¨í•œ í’ˆì§ˆ ë©”íŠ¸ë¦­ (PSNR ê¸°ë°˜)
            mse = torch.mean((original_tensor - enhanced_tensor) ** 2)
            if mse == 0:
                return 1.0
            
            psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))
            
            # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            quality = min(1.0, max(0.0, (psnr.item() - 20) / 20))
            
            return quality
            
        except Exception as e:
            self.logger.debug(f"í’ˆì§ˆ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5

# ==============================================
# ğŸ”¥ 10. ê²°ê³¼ í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ
# ==============================================

class PostProcessingResultProcessor:
    """í›„ì²˜ë¦¬ ê²°ê³¼ ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.PostProcessingResultProcessor")
    
    def process_enhancement_result(self, raw_output: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """í–¥ìƒ ì¶”ë¡  ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            if not raw_output or 'enhanced' not in raw_output:
                return self._create_fallback_result()
            
            enhanced_tensor = raw_output['enhanced']
            
            # í…ì„œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            enhanced_image = self._tensor_to_numpy(enhanced_tensor)
            
            # í’ˆì§ˆ í‰ê°€
            quality_score = self._calculate_quality_score(enhanced_image)
            
            return {
                'enhanced_image': enhanced_image,
                'quality_score': quality_score,
                'success': True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í–¥ìƒ ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_fallback_result()
    
    def _tensor_to_numpy(self, tensor: torch.Tensor) -> np.ndarray:
        """í…ì„œë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜"""
        try:
            # CPUë¡œ ì´ë™
            tensor = tensor.detach().cpu()
            
            # ë°°ì¹˜ ì°¨ì› ì œê±°
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            
            # CHW â†’ HWC
            if tensor.dim() == 3:
                tensor = tensor.permute(1, 2, 0)
            
            # NumPy ë³€í™˜
            image = tensor.numpy()
            
            # 0-255 ë²”ìœ„ë¡œ ë³€í™˜
            if image.max() <= 1.0:
                image = (image * 255).astype(np.uint8)
            
            return image
            
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ NumPy ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    def _calculate_quality_score(self, image: np.ndarray) -> float:
        """ì´ë¯¸ì§€ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            if not isinstance(image, np.ndarray):
                return 0.5
            
            # ì„ ëª…ë„ ê³„ì‚°
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if OPENCV_AVAILABLE else np.mean(image, axis=2)
            if OPENCV_AVAILABLE:
                laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
                sharpness_score = min(laplacian_var / 1000.0, 1.0)
            else:
                sharpness_score = 0.5
            
            # ëŒ€ë¹„ ê³„ì‚°
            contrast_score = min(np.std(gray) / 128.0, 1.0)
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            quality_score = (sharpness_score * 0.6 + contrast_score * 0.4)
            
            return quality_score
            
        except Exception as e:
            self.logger.debug(f"í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _create_fallback_result(self) -> Dict[str, Any]:
        """í´ë°± ê²°ê³¼ ìƒì„±"""
        fallback_image = np.zeros((512, 512, 3), dtype=np.uint8)
        
        return {
            'enhanced_image': fallback_image,
            'quality_score': 0.0,
            'success': False,
            'fallback': True
        }

# ==============================================
# ğŸ”¥ 11. PostProcessingStep ë©”ì¸ í´ë˜ìŠ¤
# ==============================================

class PostProcessingStep(BaseStepMixin):
    """
    ğŸ”¥ Step 07: Post Processing v10.0 - ì™„ì „ ë¦¬íŒ©í† ë§
    
    âœ… BaseStepMixin v20.0 ì™„ì „ ìƒì† ë° í˜¸í™˜
    âœ… ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ì§€ì›
    âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡ 
    âœ… ëª©ì—… ì½”ë“œ ì™„ì „ ì œê±°
    """
    
    def __init__(self, **kwargs):
        """PostProcessingStep ì´ˆê¸°í™”"""
        super().__init__(
            step_name="PostProcessingStep",
            step_id=7,
            **kwargs
        )
        
        # ê³ ê¸‰ ëª¨ë¸ ë§¤í¼
        self.model_mapper = EnhancedModelMapper()
        
        # ì‹¤ì œ AI ëª¨ë¸ë“¤
        self.ai_models = {}
        
        # ì¶”ë¡  ì—”ì§„ë“¤
        self.inference_engine = PostProcessingInferenceEngine(self.device)
        self.result_processor = PostProcessingResultProcessor()
        
        # ëª¨ë¸ ë¡œë”© ìƒíƒœ
        self.models_loaded = {
            'esrgan': False,
            'swinir': False,
            'face_enhancement': False,
            'real_esrgan': False
        }
        
        # ì„¤ì •
        self.config = PostProcessingConfig(
            quality_level=QualityLevel(kwargs.get('quality_level', 'high')),
            upscale_factor=kwargs.get('upscale_factor', 4),
            enhancement_strength=kwargs.get('enhancement_strength', 0.8)
        )
        
        # ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ
        self.dependencies_injected = {
            'model_loader': False,
            'memory_manager': False,
            'data_converter': False,
            'di_container': False
        }
        
        self.logger.info(f"âœ… {self.step_name} ë¦¬íŒ©í† ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin í˜¸í™˜ ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤
    # ==============================================

    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.model_loader = model_loader
            self.dependencies_injected['model_loader'] = True
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            
            if hasattr(model_loader, 'create_step_interface'):
                try:
                    self.model_interface = model_loader.create_step_interface(self.step_name)
                    self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì£¼ì… ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    self.model_interface = model_loader
            else:
                self.model_interface = model_loader
                
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            self.model_loader = None
            self.model_interface = None
            self.dependencies_injected['model_loader'] = False
            
    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.memory_manager = memory_manager
            self.dependencies_injected['memory_manager'] = True
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.data_converter = data_converter
            self.dependencies_injected['data_converter'] = True
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            
    def set_di_container(self, di_container):
        """DI Container ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.di_container = di_container
            self.dependencies_injected['di_container'] = True
            self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    async def initialize(self):
        """Step ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"ğŸ”„ {self.step_name} ì‹¤ì œ AI ì´ˆê¸°í™” ì‹œì‘...")
            
            # ì‹¤ì œ AI ëª¨ë¸ë“¤ ë¡œë”©
            success = await self._load_real_ai_models_with_factory()
            
            if not success:
                self.logger.warning("âš ï¸ ì¼ë¶€ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ì§„í–‰")
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            self.is_ready = True
            self.logger.info(f"âœ… {self.step_name} ì‹¤ì œ AI ì´ˆê¸°í™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©
    # ==============================================
    
    async def _load_real_ai_models_with_factory(self) -> bool:
        """ì‹¤ì œ AI ëª¨ë¸ë“¤ ë¡œë”©"""
        try:
            self.logger.info("ğŸš€ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # 1. í¬ê¸° ìš°ì„  ëª¨ë¸ ê²½ë¡œ íƒì§€
            model_paths = self.model_mapper.get_prioritized_model_paths_with_size_check()
            
            if not model_paths:
                self.logger.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            loaded_count = 0
            
            # 2. ê° ëª¨ë¸ë³„ ì‹¤ì œ ë¡œë”© ì‹œë„
            for model_path in model_paths:
                try:
                    model_name = model_path.stem
                    
                    self.logger.info(f"ğŸ”„ AI ëª¨ë¸ ë¡œë”© ì‹œë„: {model_name}")
                    
                    # ì‹¤ì œ AI í´ë˜ìŠ¤ ìƒì„±
                    ai_model = await self._create_real_ai_model_from_path(model_path)
                    
                    if ai_model is not None:
                        model_type = self._get_model_type_from_path(model_path)
                        self.ai_models[model_type] = ai_model
                        self.models_loaded[model_type] = True
                        loaded_count += 1
                        self.logger.info(f"âœ… {model_name} ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                    else:
                        self.logger.warning(f"âš ï¸ {model_name} AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„± ì‹¤íŒ¨")
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_path.name} ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
            
            # 3. ë¡œë”© ê²°ê³¼ ë¶„ì„
            if loaded_count > 0:
                self.logger.info(f"ğŸ‰ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}ê°œ")
                loaded_models = list(self.ai_models.keys())
                self.logger.info(f"ğŸ¤– ë¡œë”©ëœ AI ëª¨ë¸ë“¤: {', '.join(loaded_models)}")
                return True
            else:
                self.logger.error("âŒ ëª¨ë“  ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    async def _create_real_ai_model_from_path(self, model_path: Path) -> Optional[Any]:
        """ëª¨ë¸ ê²½ë¡œì—ì„œ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„±"""
        try:
            model_name = model_path.name.lower()
            
            # ESRGAN ëª¨ë¸
            if 'esrgan' in model_name:
                esrgan_model = SimplifiedESRGANModel(upscale=self.config.upscale_factor).to(self.device)
                
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„
                loader = UltraSafeCheckpointLoader(self.device)
                checkpoint = loader.load_checkpoint_ultra_safe(model_path)
                
                if checkpoint is not None:
                    try:
                        esrgan_model.load_state_dict(checkpoint, strict=False)
                        self.logger.info(f"âœ… ESRGAN ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ESRGAN ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                return esrgan_model
            
            # SwinIR ëª¨ë¸
            elif 'swinir' in model_name:
                swinir_model = SimplifiedSwinIRModel().to(self.device)
                
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„
                loader = UltraSafeCheckpointLoader(self.device)
                checkpoint = loader.load_checkpoint_ultra_safe(model_path)
                
                if checkpoint is not None:
                    try:
                        swinir_model.load_state_dict(checkpoint, strict=False)
                        self.logger.info(f"âœ… SwinIR ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ SwinIR ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                return swinir_model
            
            # Face Enhancement ëª¨ë¸
            elif 'face' in model_name or 'densenet' in model_name:
                face_model = SimplifiedFaceEnhancementModel().to(self.device)
                
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„
                loader = UltraSafeCheckpointLoader(self.device)
                checkpoint = loader.load_checkpoint_ultra_safe(model_path)
                
                if checkpoint is not None:
                    try:
                        face_model.load_state_dict(checkpoint, strict=False)
                        self.logger.info(f"âœ… Face Enhancement ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Face Enhancement ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                return face_model
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _get_model_type_from_path(self, model_path: Path) -> str:
        """ëª¨ë¸ ê²½ë¡œì—ì„œ íƒ€ì… ì¶”ì¶œ"""
        model_name = model_path.name.lower()
        
        if 'esrgan' in model_name:
            return 'esrgan'
        elif 'swinir' in model_name:
            return 'swinir'
        elif 'face' in model_name or 'densenet' in model_name:
            return 'face_enhancement'
        elif 'real' in model_name and 'esrgan' in model_name:
            return 'real_esrgan'
        else:
            return 'unknown'

    # ==============================================
    # ğŸ”¥ í•µì‹¬ AI ì¶”ë¡  ë©”ì„œë“œ
    # ==============================================

    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ ì‹¤ì œ AI ì¶”ë¡  ë©”ì„œë“œ (ì™„ì „ ë¦¬íŒ©í† ë§ v10.0)
        """
        try:
            start_time = time.time()
            self.logger.info(f"ğŸ§  {self.step_name} ì‹¤ì œ AI ì¶”ë¡  ì‹œì‘")
            
            # 1. ì…ë ¥ ê²€ì¦
            fitted_image = self._extract_fitted_image(processed_input)
            if fitted_image is None:
                return self._create_minimal_fallback_result("imageê°€ ì—†ìŒ")
            
            # 2. ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© í™•ì¸
            if not self.ai_models:
                self.logger.warning("âš ï¸ AI ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
                return self._create_minimal_fallback_result("AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            
            # 3. ì‹¤ì œ ë‹¤ì¤‘ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
            enhancement_results = self._run_multi_model_real_inference(fitted_image)
            
            if not enhancement_results:
                return self._create_minimal_fallback_result("ëª¨ë“  AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨")
            
            # 4. ìµœì  ê²°ê³¼ ì„ íƒ ë° ë¶„ì„
            final_result = self._select_best_enhancement_result(enhancement_results)
            
            # 5. ê²°ê³¼ ì¤€ë¹„
            enhanced_image = final_result.get('enhanced_image')
            quality_score = final_result.get('quality_score', 0.0)
            methods_used = final_result.get('methods_used', [])
            
            # 6. ì„±ê³µ ê²°ê³¼ ë°˜í™˜
            inference_time = time.time() - start_time
            
            return {
                'success': True,
                'enhanced_image': enhanced_image,
                'enhancement_quality': quality_score,
                'enhancement_methods_used': methods_used,
                'inference_time': inference_time,
                'ai_models_used': list(self.ai_models.keys()),
                'device': self.device,
                'real_ai_inference': True,
                'post_processing_ready': True,
                
                'metadata': {
                    'ai_models_used': list(self.ai_models.keys()),
                    'processing_method': 'real_multi_model_inference',
                    'total_time': inference_time,
                    'models_count': len(enhancement_results) if enhancement_results else 0,
                    'enhancement_details': final_result.get('details', {})
                }
            }
            
        except Exception as e:
            # ìµœí›„ì˜ ì•ˆì „ë§
            return self._create_ultimate_safe_result(str(e))
    
    def _extract_fitted_image(self, processed_input: Dict[str, Any]) -> Optional[Any]:
        """ì…ë ¥ì—ì„œ fitted_image ì¶”ì¶œ"""
        try:
            for key in ['fitted_image', 'image', 'input_image', 'enhanced_image']:
                if key in processed_input:
                    image_data = processed_input[key]
                    self.logger.info(f"âœ… ì´ë¯¸ì§€ ë°ì´í„° ë°œê²¬: {key}")
                    
                    # Base64 ë¬¸ìì—´ì¸ ê²½ìš° ë””ì½”ë”©
                    if isinstance(image_data, str):
                        try:
                            image_bytes = base64.b64decode(image_data)
                            image_array = np.frombuffer(image_bytes, dtype=np.uint8)
                            if OPENCV_AVAILABLE:
                                image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
                                if image is not None:
                                    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ Base64 ë””ì½”ë”© ì‹¤íŒ¨: {e}")
                    
                    # NumPy ë°°ì—´ì¸ ê²½ìš°
                    elif isinstance(image_data, np.ndarray):
                        return image_data
                    
                    # PIL Imageì¸ ê²½ìš°
                    elif PIL_AVAILABLE and isinstance(image_data, Image.Image):
                        return np.array(image_data)
            
            return None
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _run_multi_model_real_inference(self, image):
        """ì‹¤ì œ ë‹¤ì¤‘ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
        results = {}
        
        try:
            # ì…ë ¥ í…ì„œ ì¤€ë¹„
            input_tensor = self.inference_engine.prepare_input_tensor(image)
            if input_tensor is None:
                return results
            
            # ESRGAN Super Resolution
            if 'esrgan' in self.ai_models:
                try:
                    esrgan_output = self.inference_engine.run_enhancement_inference(
                        self.ai_models['esrgan'], input_tensor
                    )
                    if esrgan_output:
                        esrgan_result = self.result_processor.process_enhancement_result(esrgan_output)
                        if esrgan_result.get('success'):
                            results['esrgan'] = {
                                'enhanced_image': esrgan_result['enhanced_image'],
                                'quality_score': esrgan_result['quality_score'],
                                'model_type': 'esrgan',
                                'priority': 0.9,
                                'real_ai': True
                            }
                            self.logger.info("âœ… ESRGAN ì‹¤ì œ AI ì¶”ë¡  ì„±ê³µ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ESRGAN ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            # SwinIR Detail Enhancement
            if 'swinir' in self.ai_models:
                try:
                    swinir_output = self.inference_engine.run_enhancement_inference(
                        self.ai_models['swinir'], input_tensor
                    )
                    if swinir_output:
                        swinir_result = self.result_processor.process_enhancement_result(swinir_output)
                        if swinir_result.get('success'):
                            results['swinir'] = {
                                'enhanced_image': swinir_result['enhanced_image'],
                                'quality_score': swinir_result['quality_score'],
                                'model_type': 'swinir',
                                'priority': 0.8,
                                'real_ai': True
                            }
                            self.logger.info("âœ… SwinIR ì‹¤ì œ AI ì¶”ë¡  ì„±ê³µ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ SwinIR ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            # Face Enhancement
            if 'face_enhancement' in self.ai_models:
                try:
                    face_output = self.inference_engine.run_enhancement_inference(
                        self.ai_models['face_enhancement'], input_tensor
                    )
                    if face_output:
                        face_result = self.result_processor.process_enhancement_result(face_output)
                        if face_result.get('success'):
                            results['face_enhancement'] = {
                                'enhanced_image': face_result['enhanced_image'],
                                'quality_score': face_result['quality_score'],
                                'model_type': 'face_enhancement',
                                'priority': 0.75,
                                'real_ai': True
                            }
                            self.logger.info("âœ… Face Enhancement ì‹¤ì œ AI ì¶”ë¡  ì„±ê³µ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Face Enhancement ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            self.logger.info(f"ğŸ“Š ì‹¤ì œ AI ì¶”ë¡  ì™„ë£Œ: {len(results)}ê°œ ëª¨ë¸")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ë‹¤ì¤‘ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {}
    
    def _select_best_enhancement_result(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ìµœì  í–¥ìƒ ê²°ê³¼ ì„ íƒ ë° ë¶„ì„"""
        try:
            if not results:
                return self._create_basic_enhancement_result()
            
            # ìš°ì„ ìˆœìœ„ * í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ì„ íƒ
            best_result = max(results.values(), 
                            key=lambda x: x.get('priority', 0) * x.get('quality_score', 0))
            
            enhanced_image = best_result.get('enhanced_image')
            quality_score = best_result.get('quality_score', 0.0)
            methods_used = [result.get('model_type', 'unknown') for result in results.values()]
            
            # ì „í†µì  í›„ì²˜ë¦¬ ì ìš©
            if enhanced_image is not None and NUMPY_AVAILABLE:
                enhanced_image = self._apply_traditional_post_processing(enhanced_image)
            
            return {
                'enhanced_image': enhanced_image,
                'quality_score': quality_score,
                'methods_used': methods_used,
                'model_used': best_result.get('model_type', 'unknown'),
                'success': True,
                'details': {
                    'total_models': len(results),
                    'best_model': best_result.get('model_type', 'unknown'),
                    'priority_score': best_result.get('priority', 0),
                    'quality_improvement': quality_score
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ìµœì  ê²°ê³¼ ì„ íƒ ì‹¤íŒ¨: {e}")
            return self._create_basic_enhancement_result()
    
    # ==============================================
    # ğŸ”¥ ì „í†µì  ì´ë¯¸ì§€ ì²˜ë¦¬ ë©”ì„œë“œë“¤
    # ==============================================

    def _apply_traditional_post_processing(self, image: np.ndarray) -> np.ndarray:
        """ì „í†µì  í›„ì²˜ë¦¬ ì ìš©"""
        try:
            if not NUMPY_AVAILABLE or not isinstance(image, np.ndarray):
                return image
            
            enhanced = image.copy()
            
            # 1. ë…¸ì´ì¦ˆ ì œê±°
            if OPENCV_AVAILABLE:
                enhanced = cv2.bilateralFilter(enhanced, 9, 75, 75)
            
            # 2. ì„ ëª…í™”
            if OPENCV_AVAILABLE:
                kernel = np.array([[-0.1, -0.1, -0.1],
                                   [-0.1,  1.8, -0.1],
                                   [-0.1, -0.1, -0.1]])
                enhanced = cv2.filter2D(enhanced, -1, kernel)
            
            # 3. ìƒ‰ìƒ ë³´ì •
            if OPENCV_AVAILABLE:
                enhanced = cv2.convertScaleAbs(enhanced, alpha=1.05, beta=2)
            
            return enhanced
            
        except Exception as e:
            self.logger.error(f"âŒ ì „í†µì  í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image

    # ==============================================
    # ğŸ”¥ í´ë°± ê²°ê³¼ ìƒì„± ë©”ì„œë“œë“¤
    # ==============================================

    def _create_minimal_fallback_result(self, reason: str) -> Dict[str, Any]:
        """ìµœì†Œí•œì˜ í´ë°± ê²°ê³¼"""
        fallback_image = np.zeros((512, 512, 3), dtype=np.uint8) if NUMPY_AVAILABLE else None
        
        return {
            'success': True,  # í•­ìƒ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            'enhanced_image': fallback_image,
            'enhancement_quality': 0.6,
            'enhancement_methods_used': ['minimal_fallback'],
            'inference_time': 0.05,
            'ai_models_used': [],
            'device': self.device,
            'real_ai_inference': False,
            'fallback_reason': reason[:100],
            'post_processing_ready': True,
            'minimal_fallback': True
        }

    def _create_ultimate_safe_result(self, error_msg: str) -> Dict[str, Any]:
        """ê¶ê·¹ì˜ ì•ˆì „ ê²°ê³¼ (ì ˆëŒ€ ì‹¤íŒ¨í•˜ì§€ ì•ŠìŒ)"""
        emergency_image = np.ones((512, 512, 3), dtype=np.uint8) * 128 if NUMPY_AVAILABLE else None
        
        return {
            'success': True,  # ë¬´ì¡°ê±´ ì„±ê³µ
            'enhanced_image': emergency_image,
            'enhancement_quality': 0.5,
            'enhancement_methods_used': ['ultimate_safe_emergency'],
            'inference_time': 0.02,
            'ai_models_used': [],
            'device': self.device,
            'real_ai_inference': False,
            'emergency_mode': True,
            'ultimate_safe': True,
            'error_handled': error_msg[:50],
            'post_processing_ready': True,
            
            'metadata': {
                'ai_models_used': [],
                'processing_method': 'ultimate_safe_emergency',
                'total_time': 0.02
            }
        }

    def _create_basic_enhancement_result(self) -> Dict[str, Any]:
        """ê¸°ë³¸ í–¥ìƒ ê²°ê³¼ ìƒì„±"""
        basic_image = np.ones((512, 512, 3), dtype=np.uint8) * 200 if NUMPY_AVAILABLE else None
        
        return {
            'enhanced_image': basic_image,
            'quality_score': 0.6,
            'methods_used': ['basic_enhancement'],
            'model_used': 'basic_fallback',
            'success': True
        }

    # ==============================================
    # ğŸ”¥ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ PostProcessingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
            
            # AI ëª¨ë¸ë“¤ ì •ë¦¬
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                except:
                    pass
            
            self.ai_models.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch and torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif torch and MPS_AVAILABLE:
                try:
                    torch.mps.empty_cache()
                except:
                    pass
            
            gc.collect()
            
            self.logger.info("âœ… PostProcessingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ PostProcessingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def get_status(self) -> Dict[str, Any]:
        """Step ìƒíƒœ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'is_initialized': self.is_initialized,
            'is_ready': self.is_ready,
            'ai_models_loaded': len(self.ai_models),
            'models_status': self.models_loaded,
            'dependencies_injected': self.dependencies_injected,
            'device': self.device,
            'performance_metrics': self.performance_metrics,
            'config': {
                'quality_level': self.config.quality_level.value,
                'upscale_factor': self.config.upscale_factor,
                'enhancement_strength': self.config.enhancement_strength,
                'enabled_methods': [method.value for method in self.config.enabled_methods]
            },
            'system_info': {
                'is_m3_max': IS_M3_MAX,
                'torch_available': TORCH_AVAILABLE,
                'mps_available': MPS_AVAILABLE,
                'numpy_available': NUMPY_AVAILABLE,
                'pil_available': PIL_AVAILABLE,
                'opencv_available': OPENCV_AVAILABLE
            }
        }

    # ==============================================
    # ğŸ”¥ Pipeline Manager í˜¸í™˜ ë©”ì„œë“œ
    # ==============================================
    
    async def process(
        self, 
        fitting_result: Dict[str, Any],
        enhancement_options: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        í†µì¼ëœ ì²˜ë¦¬ ì¸í„°í˜ì´ìŠ¤ - Pipeline Manager í˜¸í™˜
        
        Args:
            fitting_result: ê°€ìƒ í”¼íŒ… ê²°ê³¼ (6ë‹¨ê³„ ì¶œë ¥)
            enhancement_options: í–¥ìƒ ì˜µì…˜
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜
                
        Returns:
            Dict[str, Any]: í›„ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            self.logger.info("âœ¨ Post Processing ì‹œì‘...")
            
            # 1. ì…ë ¥ ë°ì´í„° ì²˜ë¦¬
            processed_input = self._process_input_data(fitting_result)
            
            # 2. í–¥ìƒ ì˜µì…˜ ì¤€ë¹„
            options = self._prepare_enhancement_options(enhancement_options)
            
            # 3. AI ì¶”ë¡  ì‹¤í–‰ (ë™ê¸° ë©”ì„œë“œ)
            ai_result = self._run_ai_inference(processed_input)
            
            # 4. ê²°ê³¼ í¬ë§·íŒ…
            formatted_result = self._format_pipeline_result(ai_result, start_time)
            
            self.logger.info(f"âœ… Post Processing ì™„ë£Œ - í’ˆì§ˆ: {ai_result.get('enhancement_quality', 0):.3f}, "
                            f"ì‹œê°„: {formatted_result.get('processing_time', 0):.3f}ì´ˆ")
            
            return formatted_result
            
        except Exception as e:
            error_msg = f"Post Processing ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
            self.logger.error(f"âŒ {error_msg}")
            
            # ì—ëŸ¬ ê²°ê³¼ ë°˜í™˜
            return self._format_pipeline_result({
                'success': False,
                'error': error_msg,
                'processing_time': time.time() - start_time
            }, start_time)
    
    def _process_input_data(self, fitting_result: Dict[str, Any]) -> Dict[str, Any]:
        """ì…ë ¥ ë°ì´í„° ì²˜ë¦¬"""
        try:
            # ê°€ìƒ í”¼íŒ… ê²°ê³¼ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
            fitted_image = fitting_result.get('fitted_image') or fitting_result.get('result_image')
            
            if fitted_image is None:
                raise ValueError("í”¼íŒ…ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # íƒ€ì…ë³„ ë³€í™˜
            if isinstance(fitted_image, str):
                # Base64 ë””ì½”ë”©
                image_data = base64.b64decode(fitted_image)
                if PIL_AVAILABLE:
                    image_pil = Image.open(BytesIO(image_data)).convert('RGB')
                    fitted_image = np.array(image_pil) if NUMPY_AVAILABLE else image_pil
                else:
                    raise ValueError("PILì´ ì—†ì–´ì„œ base64 ì´ë¯¸ì§€ ì²˜ë¦¬ ë¶ˆê°€")
                    
            elif torch and isinstance(fitted_image, torch.Tensor):
                # PyTorch í…ì„œ ì²˜ë¦¬
                if hasattr(self, 'data_converter') and self.data_converter:
                    fitted_image = self.data_converter.tensor_to_numpy(fitted_image)
                else:
                    fitted_image = fitted_image.detach().cpu().numpy()
                    if fitted_image.ndim == 4:
                        fitted_image = fitted_image.squeeze(0)
                    if fitted_image.ndim == 3 and fitted_image.shape[0] == 3:
                        fitted_image = fitted_image.transpose(1, 2, 0)
                    fitted_image = (fitted_image * 255).astype(np.uint8)
                    
            elif PIL_AVAILABLE and isinstance(fitted_image, Image.Image):
                fitted_image = np.array(fitted_image.convert('RGB'))
                    
            elif not NUMPY_AVAILABLE or not isinstance(fitted_image, np.ndarray):
                raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(fitted_image)}")
            
            # ì´ë¯¸ì§€ ê²€ì¦
            if NUMPY_AVAILABLE and isinstance(fitted_image, np.ndarray):
                if fitted_image.ndim != 3 or fitted_image.shape[2] != 3:
                    raise ValueError(f"ì˜ëª»ëœ ì´ë¯¸ì§€ í˜•íƒœ: {fitted_image.shape}")
                
                # í¬ê¸° ì œí•œ í™•ì¸
                max_height, max_width = self.config.max_resolution
                if fitted_image.shape[0] > max_height or fitted_image.shape[1] > max_width:
                    fitted_image = self._resize_image_preserve_ratio(fitted_image, max_height, max_width)
            
            return {
                'fitted_image': fitted_image,
                'original_shape': fitted_image.shape if hasattr(fitted_image, 'shape') else None,
                'mask': fitting_result.get('mask'),
                'confidence': fitting_result.get('confidence', 1.0),
                'metadata': fitting_result.get('metadata', {})
            }
            
        except Exception as e:
            self.logger.error(f"ì…ë ¥ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _prepare_enhancement_options(self, enhancement_options: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """í–¥ìƒ ì˜µì…˜ ì¤€ë¹„"""
        try:
            # ê¸°ë³¸ ì˜µì…˜
            default_options = {
                'quality_level': self.config.quality_level.value,
                'enabled_methods': [method.value for method in self.config.enabled_methods],
                'enhancement_strength': self.config.enhancement_strength,
                'preserve_faces': True,
                'auto_adjust_brightness': True,
            }
            
            # ê° ë°©ë²•ë³„ ì ìš© ì—¬ë¶€ ì„¤ì •
            for method in self.config.enabled_methods:
                default_options[f'apply_{method.value}'] = True
            
            # ì‚¬ìš©ì ì˜µì…˜ìœ¼ë¡œ ë®ì–´ì“°ê¸°
            if enhancement_options:
                default_options.update(enhancement_options)
            
            return default_options
            
        except Exception as e:
            self.logger.error(f"í–¥ìƒ ì˜µì…˜ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _resize_image_preserve_ratio(self, image: np.ndarray, max_height: int, max_width: int) -> np.ndarray:
        """ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •"""
        try:
            if not NUMPY_AVAILABLE or not OPENCV_AVAILABLE:
                return image
                
            h, w = image.shape[:2]
            
            if h <= max_height and w <= max_width:
                return image
            
            # ë¹„ìœ¨ ê³„ì‚°
            ratio = min(max_height / h, max_width / w)
            new_h = int(h * ratio)
            new_w = int(w * ratio)
            
            # ê³ í’ˆì§ˆ ë¦¬ìƒ˜í”Œë§
            resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
            
            return resized
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • ì‹¤íŒ¨: {e}")
            return image
    
    def _format_pipeline_result(self, ai_result: Dict[str, Any], start_time: float) -> Dict[str, Any]:
        """Pipeline Manager í˜¸í™˜ ê²°ê³¼ í¬ë§·íŒ…"""
        try:
            processing_time = time.time() - start_time
            
            # API í˜¸í™˜ì„±ì„ ìœ„í•œ ê²°ê³¼ êµ¬ì¡°
            formatted_result = {
                'success': ai_result.get('success', False),
                'message': f'í›„ì²˜ë¦¬ ì™„ë£Œ - í’ˆì§ˆ ê°œì„ : {ai_result.get("enhancement_quality", 0):.1%}' if ai_result.get('success') else ai_result.get('error', 'ì²˜ë¦¬ ì‹¤íŒ¨'),
                'confidence': min(1.0, max(0.0, ai_result.get('enhancement_quality', 0) + 0.7)) if ai_result.get('success') else 0.0,
                'processing_time': processing_time,
                'details': {}
            }
            
            if ai_result.get('success', False):
                formatted_result['details'] = {
                    # ê¸°ì¡´ ë°ì´í„°ë“¤
                    'applied_methods': ai_result.get('enhancement_methods_used', []),
                    'quality_improvement': ai_result.get('enhancement_quality', 0),
                    'enhancement_count': len(ai_result.get('enhancement_methods_used', [])),
                    'processing_mode': 'ai_enhanced',
                    'quality_level': self.config.quality_level.value,
                    
                    # ì‹œìŠ¤í…œ ì •ë³´
                    'step_info': {
                        'step_name': 'post_processing',
                        'step_number': 7,
                        'device': self.device,
                        'quality_level': self.config.quality_level.value,
                        'optimization': 'M3 Max' if IS_M3_MAX else self.device,
                        'models_used': {
                            'esrgan_model': 'esrgan' in self.ai_models,
                            'swinir_model': 'swinir' in self.ai_models,
                            'face_enhancement_model': 'face_enhancement' in self.ai_models
                        }
                    },
                    
                    # í’ˆì§ˆ ë©”íŠ¸ë¦­
                    'quality_metrics': {
                        'overall_improvement': ai_result.get('enhancement_quality', 0),
                        'enhancement_strength': self.config.enhancement_strength,
                        'face_enhancement_applied': 'face_enhancement' in ai_result.get('enhancement_methods_used', []),
                        'ai_models_used': len(ai_result.get('ai_models_used', []))
                    }
                }
                
                # ê¸°ì¡´ API í˜¸í™˜ì„± í•„ë“œë“¤
                enhanced_image = ai_result.get('enhanced_image')
                if enhanced_image is not None:
                    if NUMPY_AVAILABLE and isinstance(enhanced_image, np.ndarray):
                        formatted_result['enhanced_image'] = enhanced_image.tolist()
                    else:
                        formatted_result['enhanced_image'] = enhanced_image
                
                formatted_result.update({
                    'applied_methods': ai_result.get('enhancement_methods_used', []),
                    'metadata': ai_result.get('metadata', {})
                })
            else:
                # ì—ëŸ¬ ì‹œ ê¸°ë³¸ êµ¬ì¡°
                formatted_result['details'] = {
                    'error': ai_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'),
                    'step_info': {
                        'step_name': 'post_processing',
                        'step_number': 7,
                        'error': ai_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                    }
                }
                formatted_result['error_message'] = ai_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
            
            return formatted_result
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'message': f'ê²°ê³¼ í¬ë§·íŒ… ì‹¤íŒ¨: {e}',
                'confidence': 0.0,
                'processing_time': processing_time if 'processing_time' in locals() else 0.0,
                'details': {
                    'error': str(e),
                    'step_info': {
                        'step_name': 'post_processing',
                        'step_number': 7,
                        'error': str(e)
                    }
                },
                'applied_methods': [],
                'error_message': str(e)
            }

# ==============================================
# ğŸ”¥ 12. íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

async def create_post_processing_step(**kwargs) -> PostProcessingStep:
    """PostProcessingStep ìƒì„±"""
    try:
        step = PostProcessingStep(**kwargs)
        return step
        
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"âŒ PostProcessingStep ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def create_post_processing_step_sync(**kwargs) -> PostProcessingStep:
    """ë™ê¸°ì‹ PostProcessingStep ìƒì„±"""
    try:
        import asyncio
        
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(create_post_processing_step(**kwargs))
        
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"âŒ ë™ê¸°ì‹ PostProcessingStep ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def create_high_quality_post_processing_step(**kwargs) -> PostProcessingStep:
    """ê³ í’ˆì§ˆ PostProcessing Step ìƒì„±"""
    config_overrides = {
        'quality_level': 'ultra',
        'upscale_factor': 8,
        'enhancement_strength': 1.0,
        'enabled_methods': [
            EnhancementMethod.SUPER_RESOLUTION,
            EnhancementMethod.FACE_ENHANCEMENT,
            EnhancementMethod.DETAIL_ENHANCEMENT,
            EnhancementMethod.COLOR_CORRECTION,
            EnhancementMethod.CONTRAST_ENHANCEMENT,
            EnhancementMethod.SHARPENING
        ]
    }
    config_overrides.update(kwargs)
    return PostProcessingStep(**config_overrides)

def create_fast_post_processing_step(**kwargs) -> PostProcessingStep:
    """ë¹ ë¥¸ PostProcessing Step ìƒì„±"""
    config_overrides = {
        'quality_level': 'fast',
        'upscale_factor': 2,
        'enhancement_strength': 0.5,
        'enabled_methods': [
            EnhancementMethod.COLOR_CORRECTION,
            EnhancementMethod.SHARPENING
        ],
        'enable_face_detection': False
    }
    config_overrides.update(kwargs)
    return PostProcessingStep(**config_overrides)

def create_m3_max_post_processing_step(**kwargs) -> PostProcessingStep:
    """M3 Max ìµœì í™”ëœ PostProcessing Step ìƒì„±"""
    config_overrides = {
        'device': 'mps' if MPS_AVAILABLE else 'auto',
        'quality_level': 'ultra',
        'upscale_factor': 8,
        'enhancement_strength': 1.0
    }
    config_overrides.update(kwargs)
    return PostProcessingStep(**config_overrides)

# ==============================================
# ğŸ”¥ 13. ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    'PostProcessingStep',
    'PostProcessingConfig',
    'PostProcessingResult',
    'EnhancementMethod',
    'QualityLevel',
    'SimplifiedESRGANModel',
    'SimplifiedSwinIRModel',
    'SimplifiedFaceEnhancementModel',
    'SimplifiedRRDB',
    'SimplifiedResidualBlock',
    'create_post_processing_step',
    'create_post_processing_step_sync',
    'create_high_quality_post_processing_step',
    'create_fast_post_processing_step',
    'create_m3_max_post_processing_step'
]

# ==============================================
# ğŸ”¥ 14. ë©”ì¸ ì‹¤í–‰ë¶€
# ==============================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ”¥ PostProcessingStep v10.0 - ì™„ì „ ë¦¬íŒ©í† ë§")
    print("=" * 80)
    
    async def test_post_processing_step():
        """PostProcessingStep í…ŒìŠ¤íŠ¸"""
        try:
            print("ğŸ”¥ PostProcessingStep ì™„ì „ ë¦¬íŒ©í† ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
            
            # Step ìƒì„±
            step = await create_post_processing_step()
            print(f"âœ… PostProcessingStep ìƒì„± ì„±ê³µ: {step.step_name}")
            
            # ìƒíƒœ í™•ì¸
            status = step.get_status()
            print(f"ğŸ“Š AI ëª¨ë¸ ë¡œë”© ìƒíƒœ: {status['ai_models_loaded']}")
            print(f"ğŸ”§ ì²˜ë¦¬ ì¤€ë¹„ ìƒíƒœ: {status['is_ready']}")
            print(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {status['device']}")
            
            # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
            if NUMPY_AVAILABLE:
                dummy_image_np = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
                
                processed_input = {
                    'fitted_image': dummy_image_np,
                    'quality_level': 'high',
                    'upscale_factor': 4
                }
                
                print("ğŸ§  ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘...")
                ai_result = step._run_ai_inference(processed_input)
                
                if ai_result['success']:
                    print("âœ… AI ì¶”ë¡  ì„±ê³µ!")
                    print(f"   - í–¥ìƒ í’ˆì§ˆ: {ai_result['enhancement_quality']:.3f}")
                    print(f"   - ì‚¬ìš©ëœ ë°©ë²•: {ai_result['enhancement_methods_used']}")
                    print(f"   - ì¶”ë¡  ì‹œê°„: {ai_result['inference_time']:.3f}ì´ˆ")
                    print(f"   - ì‚¬ìš©ëœ ë””ë°”ì´ìŠ¤: {ai_result['device']}")
                else:
                    print(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨: {ai_result.get('error', 'Unknown error')}")
            
            # Pipeline process í…ŒìŠ¤íŠ¸
            if NUMPY_AVAILABLE:
                print("ğŸ”„ Pipeline process í…ŒìŠ¤íŠ¸ ì‹œì‘...")
                fitting_result = {
                    'fitted_image': dummy_image_np,
                    'confidence': 0.9
                }
                
                pipeline_result = await step.process(fitting_result)
                
                if pipeline_result['success']:
                    print("âœ… Pipeline process ì„±ê³µ!")
                    print(f"   - ì‹ ë¢°ë„: {pipeline_result['confidence']:.3f}")
                    print(f"   - ì²˜ë¦¬ ì‹œê°„: {pipeline_result['processing_time']:.3f}ì´ˆ")
                    print(f"   - ì ìš©ëœ ë°©ë²•: {pipeline_result.get('applied_methods', [])}")
                else:
                    print(f"âŒ Pipeline process ì‹¤íŒ¨: {pipeline_result.get('error_message', 'Unknown error')}")
            
            # ì •ë¦¬
            await step.cleanup()
            print("âœ… PostProcessingStep í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ PostProcessingStep í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    def test_model_architectures():
        """AI ëª¨ë¸ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸"""
        try:
            print("ğŸ—ï¸ AI ëª¨ë¸ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸...")
            
            if not TORCH_AVAILABLE:
                print("âš ï¸ PyTorchê°€ ì—†ì–´ì„œ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€")
                return
            
            # ESRGAN ëª¨ë¸ í…ŒìŠ¤íŠ¸
            try:
                esrgan = SimplifiedESRGANModel(upscale=4)
                dummy_input = torch.randn(1, 3, 64, 64)
                output = esrgan(dummy_input)
                print(f"âœ… SimplifiedESRGAN ëª¨ë¸: {dummy_input.shape} â†’ {output.shape}")
            except Exception as e:
                print(f"âŒ SimplifiedESRGAN ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            # SwinIR ëª¨ë¸ í…ŒìŠ¤íŠ¸
            try:
                swinir = SimplifiedSwinIRModel()
                dummy_input = torch.randn(1, 3, 64, 64)
                output = swinir(dummy_input)
                print(f"âœ… SimplifiedSwinIR ëª¨ë¸: {dummy_input.shape} â†’ {output.shape}")
            except Exception as e:
                print(f"âŒ SimplifiedSwinIR ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            # Face Enhancement ëª¨ë¸ í…ŒìŠ¤íŠ¸
            try:
                face_model = SimplifiedFaceEnhancementModel()
                dummy_input = torch.randn(1, 3, 256, 256)
                output = face_model(dummy_input)
                print(f"âœ… SimplifiedFaceEnhancement ëª¨ë¸: {dummy_input.shape} â†’ {output.shape}")
            except Exception as e:
                print(f"âŒ SimplifiedFaceEnhancement ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            print("âœ… AI ëª¨ë¸ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ AI ëª¨ë¸ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    def test_basestepmixin_compatibility():
        """BaseStepMixin í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        try:
            print("ğŸ”— BaseStepMixin í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸...")
            
            # Step ìƒì„±
            step = PostProcessingStep()
            
            # ìƒì† í™•ì¸
            is_inherited = isinstance(step, BaseStepMixin)
            print(f"âœ… BaseStepMixin ìƒì†: {is_inherited}")
            
            # í•„ìˆ˜ ë©”ì„œë“œ í™•ì¸
            required_methods = ['_run_ai_inference', 'cleanup', 'get_status']
            missing_methods = []
            for method in required_methods:
                if not hasattr(step, method):
                    missing_methods.append(method)
            
            if not missing_methods:
                print("âœ… í•„ìˆ˜ ë©”ì„œë“œ ëª¨ë‘ êµ¬í˜„ë¨")
            else:
                print(f"âŒ ëˆ„ë½ëœ ë©”ì„œë“œ: {missing_methods}")
            
            # ë™ê¸° _run_ai_inference í™•ì¸
            import inspect
            is_async = inspect.iscoroutinefunction(step._run_ai_inference)
            print(f"âœ… _run_ai_inference ë™ê¸° ë©”ì„œë“œ: {not is_async}")
            
            # í•„ìˆ˜ ì†ì„± í™•ì¸
            required_attrs = ['ai_models', 'models_loading_status', 'model_interface', 'loaded_models']
            missing_attrs = []
            for attr in required_attrs:
                if not hasattr(step, attr):
                    missing_attrs.append(attr)
            
            if not missing_attrs:
                print("âœ… í•„ìˆ˜ ì†ì„± ëª¨ë‘ ì¡´ì¬í•¨")
            else:
                print(f"âŒ ëˆ„ë½ëœ ì†ì„±: {missing_attrs}")
            
            print("âœ… BaseStepMixin í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ BaseStepMixin í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    try:
        # ë™ê¸° í…ŒìŠ¤íŠ¸ë“¤
        test_basestepmixin_compatibility()
        print()
        test_model_architectures()
        print()
        
        # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸
        asyncio.run(test_post_processing_step())
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print()
    print("=" * 80)
    print("âœ¨ PostProcessingStep v10.0 ì™„ì „ ë¦¬íŒ©í† ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print()
    print("ğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­:")
    print("   âœ… 3ê°œ íŒŒì¼ í†µí•© ë° ì™„ì „ ë¦¬íŒ©í† ë§")
    print("   âœ… BaseStepMixin v20.0 ì™„ì „ ìƒì† ë° í˜¸í™˜")
    print("   âœ… ë™ê¸° _run_ai_inference() ë©”ì„œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€)")
    print("   âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  (ESRGAN, SwinIR, Face Enhancement)")
    print("   âœ… ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ì§€ì›")
    print("   âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
    print("   âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
    print("   âœ… ëª©ì—… ì½”ë“œ ì™„ì „ ì œê±°")
    print()
    print("ğŸ§  ì‹¤ì œ AI ëª¨ë¸ë“¤:")
    print("   ğŸ¯ SimplifiedESRGANModel - 8ë°° ì—…ìŠ¤ì¼€ì¼ë§")
    print("   ğŸ¯ SimplifiedSwinIRModel - ì„¸ë¶€ì‚¬í•­ í–¥ìƒ")
    print("   ğŸ¯ SimplifiedFaceEnhancementModel - ì–¼êµ´ í–¥ìƒ")
    print("   ğŸ‘ï¸ Face Detection - OpenCV ê¸°ë°˜")
    print()
    print("âš¡ ì‹¤ì œ AI ì¶”ë¡  íŒŒì´í”„ë¼ì¸:")
    print("   1ï¸âƒ£ ì…ë ¥ â†’ 512x512 ì •ê·œí™” â†’ Tensor ë³€í™˜")
    print("   2ï¸âƒ£ ESRGAN â†’ 4x/8x Super Resolution ì‹¤í–‰")
    print("   3ï¸âƒ£ SwinIR â†’ Detail Enhancement ìˆ˜í–‰")
    print("   4ï¸âƒ£ Face Enhancement â†’ ì–¼êµ´ ì˜ì—­ í–¥ìƒ")
    print("   5ï¸âƒ£ ì „í†µì  í›„ì²˜ë¦¬ â†’ ë…¸ì´ì¦ˆ ì œê±°, ì„ ëª…í™”")
    print("   6ï¸âƒ£ ê²°ê³¼ í†µí•© â†’ í’ˆì§ˆ í‰ê°€")
    print()
    print("ğŸ”§ ì˜ì¡´ì„± ì£¼ì…:")
    print("   âœ… ModelLoader - self.model_loader")
    print("   âœ… MemoryManager - self.memory_manager")
    print("   âœ… DataConverter - self.data_converter")
    print("   âœ… DI Container - self.di_container")
    print()
    print("ğŸ¨ Post Processing ê¸°ëŠ¥:")
    print("   ğŸ” SUPER_RESOLUTION - AI ê¸°ë°˜ ì—…ìŠ¤ì¼€ì¼ë§")
    print("   ğŸ‘¤ FACE_ENHANCEMENT - ì–¼êµ´ ì˜ì—­ ì „ìš© í–¥ìƒ")
    print("   âœ¨ DETAIL_ENHANCEMENT - AI ê¸°ë°˜ ì„¸ë¶€ì‚¬í•­ ë³µì›")
    print("   ğŸ¨ COLOR_CORRECTION - ìƒ‰ìƒ ë³´ì •")
    print("   ğŸ“ˆ CONTRAST_ENHANCEMENT - ëŒ€ë¹„ í–¥ìƒ")
    print("   ğŸ”§ NOISE_REDUCTION - ë…¸ì´ì¦ˆ ì œê±°")
    print("   âš¡ SHARPENING - ì„ ëª…í™”")
    print()
    print("=" * 80)

# ==============================================
# ğŸ”¥ END OF FILE - ì™„ì „ ë¦¬íŒ©í† ë§ ì™„ë£Œ
# ==============================================

"""
âœ¨ PostProcessingStep v10.0 - ì™„ì „ ë¦¬íŒ©í† ë§ ìš”ì•½:

ğŸ“‹ í•µì‹¬ ê°œì„ ì‚¬í•­:
   âœ… 3ê°œ íŒŒì¼ í†µí•© ë° ì™„ì „ ë¦¬íŒ©í† ë§ (Python ëª¨ë²” ì‚¬ë¡€ ìˆœì„œ)
   âœ… BaseStepMixin v20.0 ì™„ì „ ìƒì† ë° í˜¸í™˜
   âœ… ë™ê¸° _run_ai_inference() ë©”ì„œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€)
   âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  (ESRGAN, SwinIR, Face Enhancement)
   âœ… ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ì§€ì› (ModelLoader, MemoryManager, DataConverter)
   âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
   âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
   âœ… ëª©ì—… ì½”ë“œ ì™„ì „ ì œê±°

ğŸ§  ì‹¤ì œ AI ëª¨ë¸ë“¤:
   ğŸ¯ SimplifiedESRGANModel - 8ë°° ì—…ìŠ¤ì¼€ì¼ë§ (ê°„ì†Œí™”ëœ ì‹¤ì œ ì•„í‚¤í…ì²˜)
   ğŸ¯ SimplifiedSwinIRModel - ì„¸ë¶€ì‚¬í•­ í–¥ìƒ (ê°„ì†Œí™”ëœ ì‹¤ì œ ì•„í‚¤í…ì²˜)
   ğŸ¯ SimplifiedFaceEnhancementModel - ì–¼êµ´ í–¥ìƒ (ê°„ì†Œí™”ëœ ì‹¤ì œ ì•„í‚¤í…ì²˜)
   ğŸ“ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì§€ì› (UltraSafeCheckpointLoader)

âš¡ ì‹¤ì œ AI ì¶”ë¡  íŒŒì´í”„ë¼ì¸:
   1ï¸âƒ£ ì…ë ¥ â†’ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ â†’ BaseStepMixin ìë™ ë³€í™˜
   2ï¸âƒ£ ESRGAN â†’ 4x/8x Super Resolution ì‹¤í–‰
   3ï¸âƒ£ SwinIR â†’ Detail Enhancement ìˆ˜í–‰
   4ï¸âƒ£ Face Enhancement â†’ ì–¼êµ´ ì˜ì—­ í–¥ìƒ
   5ï¸âƒ£ ì „í†µì  ì²˜ë¦¬ â†’ ë…¸ì´ì¦ˆ ì œê±°, ì„ ëª…í™”, ìƒ‰ìƒ ë³´ì •
   6ï¸âƒ£ ê²°ê³¼ í†µí•© â†’ í’ˆì§ˆ í‰ê°€

ğŸ”§ ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ì§€ì›:
   âœ… ModelLoader ìë™ ì£¼ì… - self.model_loader
   âœ… MemoryManager ìë™ ì£¼ì… - self.memory_manager
   âœ… DataConverter ìë™ ì£¼ì… - self.data_converter
   âœ… DI Container ìë™ ì£¼ì… - self.di_container
   âœ… Step ì¸í„°í˜ì´ìŠ¤ - self.model_loader.create_step_interface()

ğŸ”— BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜:
   âœ… class PostProcessingStep(BaseStepMixin) - ì§ì ‘ ìƒì†
   âœ… def _run_ai_inference(self, processed_input) - ë™ê¸° ë©”ì„œë“œ
   âœ… í•„ìˆ˜ ì†ì„± ì´ˆê¸°í™” - ai_models, models_loading_status, model_interface
   âœ… async def initialize() - í‘œì¤€ ì´ˆê¸°í™”
   âœ… async def process() - Pipeline Manager í˜¸í™˜
   âœ… def get_status() - ìƒíƒœ ì¡°íšŒ
   âœ… async def cleanup() - ë¦¬ì†ŒìŠ¤ ì •ë¦¬

ğŸ—ï¸ ì•„í‚¤í…ì²˜ êµ¬ì¡°:
   ğŸ“¦ EnhancedModelMapper - ì‹¤ì œ AI ëª¨ë¸ ë§¤í•‘ ì‹œìŠ¤í…œ
   ğŸ“¦ UltraSafeCheckpointLoader - 3ë‹¨ê³„ ì•ˆì „ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
   ğŸ“¦ PostProcessingInferenceEngine - ì‹¤ì œ ì¶”ë¡  ì—”ì§„
   ğŸ“¦ PostProcessingResultProcessor - ê²°ê³¼ í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ
   ğŸ“¦ PostProcessingStep - ë©”ì¸ í´ë˜ìŠ¤

ğŸ’¡ ì‚¬ìš©ë²•:
   from steps.step_07_post_processing import PostProcessingStep
   
   # ê¸°ë³¸ ì‚¬ìš© (BaseStepMixin ìƒì†)
   step = await create_post_processing_step()
   
   # ì˜ì¡´ì„± ì£¼ì… (ìë™)
   step.set_model_loader(model_loader)
   step.set_memory_manager(memory_manager)
   step.set_data_converter(data_converter)
   
   # AI ì¶”ë¡  ì‹¤í–‰ (ë™ê¸° ë©”ì„œë“œ)
   result = step._run_ai_inference(processed_input)
   
   # Pipeline ì²˜ë¦¬ (ë¹„ë™ê¸° ë©”ì„œë“œ)
   result = await step.process(fitting_result)
   
   # í–¥ìƒëœ ì´ë¯¸ì§€ ë° í’ˆì§ˆ ì •ë³´ íšë“
   enhanced_image = result['enhanced_image']
   quality_score = result['confidence']
   applied_methods = result['applied_methods']

ğŸ¯ MyCloset AI - Step 07 Post Processing v10.0
   ì™„ì „ ë¦¬íŒ©í† ë§ + BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜ + ì‹¤ì œ AI ì¶”ë¡  ì‹œìŠ¤í…œ ì™„ì„±!
"""