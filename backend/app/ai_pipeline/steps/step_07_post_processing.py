#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 07: í›„ì²˜ë¦¬ (Post Processing) - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ ì‹¤ì œ AI êµ¬í˜„
=============================================================================================

âœ… BaseStepMixin v19.1 ì™„ì „ ìƒì† ë° í˜¸í™˜
âœ… ë™ê¸° _run_ai_inference() ë©”ì„œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€)
âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  (ESRGAN, SwinIR, Real-ESRGAN)
âœ… 1.3GB ì‹¤ì œ ëª¨ë¸ íŒŒì¼ í™œìš© (9ê°œ íŒŒì¼)
âœ… ëª©ì—… ì½”ë“œ ì™„ì „ ì œê±°
âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ì§€ì›

í•µì‹¬ AI ëª¨ë¸ë“¤:
- ESRGAN_x8.pth (135.9MB) - 8ë°° ì—…ìŠ¤ì¼€ì¼ë§
- RealESRGAN_x4plus.pth (63.9MB) - 4ë°° ê³ í’ˆì§ˆ ì—…ìŠ¤ì¼€ì¼ë§
- SwinIR-M_x4.pth (56.8MB) - ì„¸ë¶€ì‚¬í•­ ë³µì›
- densenet161_enhance.pth (110.6MB) - DenseNet ê¸°ë°˜ í–¥ìƒ
- pytorch_model.bin (823.0MB) - í†µí•© í›„ì²˜ë¦¬ ëª¨ë¸

ì²˜ë¦¬ íë¦„:
1. ê°€ìƒ í”¼íŒ… ê²°ê³¼ ì…ë ¥ â†’ BaseStepMixin ìë™ ë³€í™˜
2. ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  â†’ ESRGAN, SwinIR, Real-ESRGAN
3. ì–¼êµ´ ê²€ì¶œ ë° í–¥ìƒ â†’ í’ˆì§ˆ ìµœì í™”
4. BaseStepMixin ìë™ ì¶œë ¥ ë³€í™˜ â†’ í‘œì¤€ API ì‘ë‹µ

File: backend/app/ai_pipeline/steps/step_07_post_processing.py
Author: MyCloset AI Team
Date: 2025-07-28
Version: v5.0 (BaseStepMixin v19.1 Complete)
"""

import os
import gc
import time
import asyncio
import logging
import threading
import traceback
import hashlib
import json
import base64
import weakref
import math
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from contextlib import asynccontextmanager

import base64
import json
import hashlib
from io import BytesIO
import weakref
# ==============================================
# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
# ==============================================
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from ..factories.step_factory import StepFactory

# BaseStepMixin ë™ì  import (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€) - PostProcessing íŠ¹í™”
def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€) - PostProcessingìš©"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logging.getLogger(__name__).error(f"âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨: {e}")
        return None

BaseStepMixin = get_base_step_mixin_class()

# BaseStepMixin í´ë°± í´ë˜ìŠ¤ (PostProcessing íŠ¹í™”)
if BaseStepMixin is None:
    class BaseStepMixin:
        """PostProcessingStepìš© BaseStepMixin í´ë°± í´ë˜ìŠ¤"""
        
        def __init__(self, **kwargs):
            # ê¸°ë³¸ ì†ì„±ë“¤
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'PostProcessingStep')
            self.step_id = kwargs.get('step_id', 7)
            self.device = kwargs.get('device', 'cpu')
            
            # AI ëª¨ë¸ ê´€ë ¨ ì†ì„±ë“¤ (PostProcessingì´ í•„ìš”ë¡œ í•˜ëŠ”)
            self.ai_models = {}
            self.models_loading_status = {
                'esrgan': False,
                'swinir': False,
                'face_enhancement': False,
                'real_esrgan': False,
                'densenet': False
            }
            self.model_interface = None
            self.loaded_models = []
            
            # PostProcessing íŠ¹í™” ì†ì„±ë“¤
            self.esrgan_model = None
            self.swinir_model = None
            self.face_enhancement_model = None
            self.face_detector = None
            self.enhancement_cache = {}
            
            # ìƒíƒœ ê´€ë ¨ ì†ì„±ë“¤
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # Central Hub DI Container ê´€ë ¨
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            # ì„±ëŠ¥ í†µê³„
            self.processing_stats = {
                'total_processed': 0,
                'successful_enhancements': 0,
                'average_improvement': 0.0,
                'ai_inference_count': 0,
                'cache_hits': 0
            }
            
            # PostProcessing ì„¤ì •
            self.config = None
            self.quality_level = 'high'
            self.upscale_factor = 4
            self.enhancement_strength = 0.8
            self.enable_face_detection = True
            
            self.logger.info(f"âœ… {self.step_name} BaseStepMixin í´ë°± í´ë˜ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
            """AI ì¶”ë¡  ì‹¤í–‰ - í´ë°± êµ¬í˜„"""
            return {
                "success": False,
                "error": "BaseStepMixin í´ë°± ëª¨ë“œ - ì‹¤ì œ AI ëª¨ë¸ ì—†ìŒ",
                "step": self.step_name,
                "enhanced_image": processed_input.get('fitted_image'),
                "enhancement_quality": 0.0,
                "enhancement_methods_used": [],
                "inference_time": 0.0,
                "ai_models_used": [],
                "device": self.device,
                "fallback_mode": True
            }
        
        async def initialize(self) -> bool:
            """ì´ˆê¸°í™” ë©”ì„œë“œ"""
            try:
                if self.is_initialized:
                    return True
                
                self.logger.info(f"ğŸ”„ {self.step_name} ì´ˆê¸°í™” ì‹œì‘...")
                
                # Central Hubë¥¼ í†µí•œ ì˜ì¡´ì„± ì£¼ì… ì‹œë„
                injected_count = _inject_dependencies_safe(self)
                if injected_count > 0:
                    self.logger.info(f"âœ… Central Hub ì˜ì¡´ì„± ì£¼ì…: {injected_count}ê°œ")
                
                # PostProcessing AI ëª¨ë¸ë“¤ ë¡œë”© (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” _load_real_ai_models í˜¸ì¶œ)
                if hasattr(self, '_load_real_ai_models'):
                    await self._load_real_ai_models()
                
                self.is_initialized = True
                self.is_ready = True
                self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
                return True
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                return False
        
        async def process(
            self, 
            fitting_result: Dict[str, Any],
            enhancement_options: Optional[Dict[str, Any]] = None,
            **kwargs
        ) -> Dict[str, Any]:
            """ê¸°ë³¸ process ë©”ì„œë“œ - _run_ai_inference í˜¸ì¶œ"""
            try:
                start_time = time.time()
                
                # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬
                processed_input = self._process_input_data(fitting_result) if hasattr(self, '_process_input_data') else {
                    'fitted_image': fitting_result.get('fitted_image') or fitting_result.get('result_image'),
                    'enhancement_options': enhancement_options
                }
                
                # _run_ai_inference ë©”ì„œë“œê°€ ìˆìœ¼ë©´ í˜¸ì¶œ
                if hasattr(self, '_run_ai_inference'):
                    result = self._run_ai_inference(processed_input)
                    
                    # ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
                    if isinstance(result, dict):
                        result['processing_time'] = time.time() - start_time
                        result['step_name'] = self.step_name
                        result['step_id'] = self.step_id
                    
                    # ê²°ê³¼ í¬ë§·íŒ…
                    if hasattr(self, '_format_result'):
                        return self._format_result(result)
                    else:
                        return result
                else:
                    # ê¸°ë³¸ ì‘ë‹µ
                    return {
                        'success': False,
                        'error': '_run_ai_inference ë©”ì„œë“œê°€ êµ¬í˜„ë˜ì§€ ì•ŠìŒ',
                        'processing_time': time.time() - start_time,
                        'step_name': self.step_name,
                        'step_id': self.step_id
                    }
                    
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} process ì‹¤íŒ¨: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                    'step_name': self.step_name,
                    'step_id': self.step_id
                }
        
        async def cleanup(self):
            """ì •ë¦¬ ë©”ì„œë“œ"""
            try:
                self.logger.info(f"ğŸ”„ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
                
                # AI ëª¨ë¸ë“¤ ì •ë¦¬
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'cleanup'):
                            model.cleanup()
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                    except Exception as e:
                        self.logger.debug(f"ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨ ({model_name}): {e}")
                
                # ê°œë³„ ëª¨ë¸ë“¤ ì •ë¦¬
                models_to_clean = ['esrgan_model', 'swinir_model', 'face_enhancement_model', 'face_detector']
                for model_attr in models_to_clean:
                    if hasattr(self, model_attr):
                        model = getattr(self, model_attr)
                        if model is not None:
                            try:
                                if hasattr(model, 'cpu'):
                                    model.cpu()
                                del model
                                setattr(self, model_attr, None)
                            except Exception as e:
                                self.logger.debug(f"{model_attr} ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                # ìºì‹œ ì •ë¦¬
                self.ai_models.clear()
                if hasattr(self, 'enhancement_cache'):
                    self.enhancement_cache.clear()
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        torch.mps.empty_cache()
                except:
                    pass
                
                import gc
                gc.collect()
                
                self.logger.info(f"âœ… {self.step_name} ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        def get_status(self) -> Dict[str, Any]:
            """ìƒíƒœ ì¡°íšŒ"""
            return {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'device': self.device,
                'models_loaded': len(getattr(self, 'ai_models', {})),
                'enhancement_methods': [
                    'super_resolution', 'face_enhancement', 
                    'detail_enhancement', 'color_correction',
                    'contrast_enhancement', 'noise_reduction'
                ],
                'quality_level': getattr(self, 'quality_level', 'high'),
                'upscale_factor': getattr(self, 'upscale_factor', 4),
                'enhancement_strength': getattr(self, 'enhancement_strength', 0.8),
                'fallback_mode': True
            }
        
        # BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤
        def set_model_loader(self, model_loader):
            """ModelLoader ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.model_loader = model_loader
                self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                
                # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹œë„
                if hasattr(model_loader, 'create_step_interface'):
                    try:
                        self.model_interface = model_loader.create_step_interface(self.step_name)
                        self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì£¼ì… ì™„ë£Œ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨, ModelLoader ì§ì ‘ ì‚¬ìš©: {e}")
                        self.model_interface = model_loader
                else:
                    self.model_interface = model_loader
                    
            except Exception as e:
                self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
                self.model_loader = None
                self.model_interface = None
        
        def set_memory_manager(self, memory_manager):
            """MemoryManager ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.memory_manager = memory_manager
                self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        def set_data_converter(self, data_converter):
            """DataConverter ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.data_converter = data_converter
                self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        def set_di_container(self, di_container):
            """DI Container ì˜ì¡´ì„± ì£¼ì…"""
            try:
                self.di_container = di_container
                self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DI Container ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")

        def _get_step_requirements(self) -> Dict[str, Any]:
            """Step 07 PostProcessing ìš”êµ¬ì‚¬í•­ ë°˜í™˜ (BaseStepMixin í˜¸í™˜)"""
            return {
                "required_models": [
                    "ESRGAN_x8.pth",
                    "RealESRGAN_x4plus.pth",
                    "001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth",
                    "densenet161_enhance.pth",
                    "pytorch_model.bin"
                ],
                "primary_model": "ESRGAN_x8.pth",
                "model_configs": {
                    "ESRGAN_x8.pth": {
                        "size_mb": 135.9,
                        "device_compatible": ["cpu", "mps", "cuda"],
                        "upscale_factor": 8,
                        "model_type": "super_resolution"
                    },
                    "RealESRGAN_x4plus.pth": {
                        "size_mb": 63.9,
                        "device_compatible": ["cpu", "mps", "cuda"],
                        "upscale_factor": 4,
                        "model_type": "super_resolution"
                    },
                    "001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth": {
                        "size_mb": 56.8,
                        "device_compatible": ["cpu", "mps", "cuda"],
                        "model_type": "detail_enhancement"
                    },
                    "densenet161_enhance.pth": {
                        "size_mb": 110.6,
                        "device_compatible": ["cpu", "mps", "cuda"],
                        "model_type": "face_enhancement"
                    },
                    "pytorch_model.bin": {
                        "size_mb": 823.0,
                        "device_compatible": ["cpu", "mps", "cuda"],
                        "model_type": "unified_post_processing"
                    }
                },
                "verified_paths": [
                    "step_07_post_processing/esrgan_x8_ultra/ESRGAN_x8.pth",
                    "step_07_post_processing/ultra_models/RealESRGAN_x4plus.pth",
                    "step_07_post_processing/ultra_models/001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth",
                    "step_07_post_processing/ultra_models/densenet161_enhance.pth",
                    "step_07_post_processing/ultra_models/pytorch_model.bin"
                ],
                "enhancement_methods": [
                    "super_resolution",
                    "face_enhancement", 
                    "detail_enhancement",
                    "noise_reduction",
                    "color_correction",
                    "contrast_enhancement",
                    "sharpening"
                ],
                "quality_levels": ["fast", "balanced", "high", "ultra"],
                "upscale_factors": [2, 4, 8],
                "face_detection": {
                    "enabled": True,
                    "method": "opencv_haar_cascade",
                    "confidence_threshold": 0.5
                }
            }

        def get_model(self, model_name: Optional[str] = None):
            """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
            if not model_name:
                return getattr(self, 'esrgan_model', None) or \
                       getattr(self, 'swinir_model', None) or \
                       getattr(self, 'face_enhancement_model', None)
            
            return self.ai_models.get(model_name)
        
        async def get_model_async(self, model_name: Optional[str] = None):
            """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸°)"""
            return self.get_model(model_name)

        def _process_input_data(self, fitting_result: Dict[str, Any]) -> Dict[str, Any]:
            """ì…ë ¥ ë°ì´í„° ì²˜ë¦¬ - ê¸°ë³¸ êµ¬í˜„"""
            try:
                fitted_image = fitting_result.get('fitted_image') or fitting_result.get('result_image')
                
                if fitted_image is None:
                    raise ValueError("í”¼íŒ…ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
                
                return {
                    'fitted_image': fitted_image,
                    'metadata': fitting_result.get('metadata', {}),
                    'confidence': fitting_result.get('confidence', 1.0)
                }
                
            except Exception as e:
                self.logger.error(f"ì…ë ¥ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise

        def _format_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
            """ê²°ê³¼ í¬ë§·íŒ… - ê¸°ë³¸ êµ¬í˜„"""
            try:
                formatted_result = {
                    'success': result.get('success', False),
                    'message': f'í›„ì²˜ë¦¬ ì™„ë£Œ - í’ˆì§ˆ ê°œì„ : {result.get("enhancement_quality", 0):.1%}' if result.get('success') else result.get('error', 'ì²˜ë¦¬ ì‹¤íŒ¨'),
                    'confidence': min(1.0, max(0.0, result.get('enhancement_quality', 0) + 0.7)) if result.get('success') else 0.0,
                    'processing_time': result.get('inference_time', 0),
                    'details': {
                        'result_image': '',
                        'overlay_image': '',
                        'applied_methods': result.get('enhancement_methods_used', []),
                        'quality_improvement': result.get('enhancement_quality', 0),
                        'step_info': {
                            'step_name': 'post_processing',
                            'step_number': 7,
                            'device': self.device,
                            'fallback_mode': True
                        }
                    }
                }
                
                if not result.get('success', False):
                    formatted_result['error_message'] = result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                
                return formatted_result
                
            except Exception as e:
                self.logger.error(f"ê²°ê³¼ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
                return {
                    'success': False,
                    'message': f'ê²°ê³¼ í¬ë§·íŒ… ì‹¤íŒ¨: {e}',
                    'confidence': 0.0,
                    'processing_time': 0.0,
                    'error_message': str(e)
                }


# ==============================================
# ğŸ”¥ Central Hub DI Container ì•ˆì „ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€) - PostProcessing íŠ¹í™”
# ==============================================

def _get_central_hub_container():
    """Central Hub DI Container ì•ˆì „í•œ ë™ì  í•´ê²° - PostProcessingìš©"""
    try:
        import importlib
        module = importlib.import_module('app.core.di_container')
        get_global_fn = getattr(module, 'get_global_container', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError:
        return None
    except Exception:
        return None

def _inject_dependencies_safe(step_instance):
    """Central Hub DI Containerë¥¼ í†µí•œ ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì… - PostProcessingìš©"""
    try:
        container = _get_central_hub_container()
        if container and hasattr(container, 'inject_to_step'):
            return container.inject_to_step(step_instance)
        return 0
    except Exception:
        return 0

def _get_service_from_central_hub(service_key: str):
    """Central Hubë¥¼ í†µí•œ ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ - PostProcessingìš©"""
    try:
        container = _get_central_hub_container()
        if container:
            return container.get(service_key)
        return None
    except Exception:
        return None

# ==============================================
# ğŸ”¥ í™˜ê²½ ë° ì‹œìŠ¤í…œ ì •ë³´
# ==============================================

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'), 
    'python_path': os.path.dirname(os.__file__)
}

# M3 Max ê°ì§€
def detect_m3_max() -> bool:
    try:
        import platform, subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()

# ==============================================
# ğŸ”¥ ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
# ==============================================

# PyTorch ì•ˆì „ import
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
except ImportError as e:
    print(f"âš ï¸ PyTorch ì—†ìŒ: {e}")
    torch = None

# ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
NUMPY_AVAILABLE = False
PIL_AVAILABLE = False
OPENCV_AVAILABLE = False

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    print("âš ï¸ NumPy ì—†ìŒ")
    np = None

try:
    from PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError:
    print("âš ï¸ PIL ì—†ìŒ")
    Image = None

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    print("âš ï¸ OpenCV ì—†ìŒ")
    cv2 = None

# ê³ ê¸‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
SCIPY_AVAILABLE = False
SKIMAGE_AVAILABLE = False

try:
    from scipy.ndimage import gaussian_filter, median_filter
    from scipy.signal import convolve2d
    SCIPY_AVAILABLE = True
except ImportError:
    pass

try:
    from skimage import restoration, filters, exposure, morphology
    from skimage.metrics import structural_similarity, peak_signal_noise_ratio
    SKIMAGE_AVAILABLE = True
except ImportError:
    pass

# GPU ì„¤ì •
try:
    from app.core.gpu_config import safe_mps_empty_cache
except ImportError:
    def safe_mps_empty_cache():
        import gc
        gc.collect()
        return {"success": True, "method": "fallback_gc"}

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class EnhancementMethod(Enum):
    """í–¥ìƒ ë°©ë²•"""
    SUPER_RESOLUTION = "super_resolution"
    FACE_ENHANCEMENT = "face_enhancement"
    NOISE_REDUCTION = "noise_reduction"
    DETAIL_ENHANCEMENT = "detail_enhancement"
    COLOR_CORRECTION = "color_correction"
    CONTRAST_ENHANCEMENT = "contrast_enhancement"
    SHARPENING = "sharpening"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    ULTRA = "ultra"

@dataclass
class PostProcessingConfig:
    """í›„ì²˜ë¦¬ ì„¤ì •"""
    quality_level: QualityLevel = QualityLevel.HIGH
    enabled_methods: List[EnhancementMethod] = field(default_factory=lambda: [
        EnhancementMethod.SUPER_RESOLUTION,
        EnhancementMethod.FACE_ENHANCEMENT,
        EnhancementMethod.DETAIL_ENHANCEMENT,
        EnhancementMethod.COLOR_CORRECTION
    ])
    upscale_factor: int = 4
    max_resolution: Tuple[int, int] = (2048, 2048)
    use_gpu_acceleration: bool = True
    batch_size: int = 1
    enable_face_detection: bool = True
    enhancement_strength: float = 0.8

# ==============================================
# ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ì™„ì „í•œ êµ¬í˜„)
# ==============================================

class ESRGANModel(nn.Module):
    """ESRGAN Super Resolution ëª¨ë¸ - ì‹¤ì œ êµ¬í˜„"""
    
    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=23, upscale=4):
        super(ESRGANModel, self).__init__()
        self.upscale = upscale
        
        # Feature extraction
        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)
        
        # RRDB blocks
        self.RRDB_trunk = nn.Sequential(*[RRDB(nf) for _ in range(nb)])
        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        
        # Upsampling
        self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        
        if upscale == 8:
            self.upconv3 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        
        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)
        
        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)
    
    def forward(self, x):
        fea = self.lrelu(self.conv_first(x))
        trunk = self.trunk_conv(self.RRDB_trunk(fea))
        fea = fea + trunk
        
        # Upsampling
        fea = self.lrelu(self.upconv1(F.interpolate(fea, scale_factor=2, mode='nearest')))
        fea = self.lrelu(self.upconv2(F.interpolate(fea, scale_factor=2, mode='nearest')))
        
        if self.upscale == 8:
            fea = self.lrelu(self.upconv3(F.interpolate(fea, scale_factor=2, mode='nearest')))
        
        out = self.conv_last(self.lrelu(self.HRconv(fea)))
        return out

class RRDB(nn.Module):
    """Residual in Residual Dense Block"""
    
    def __init__(self, nf, gc=32):
        super(RRDB, self).__init__()
        self.RDB1 = ResidualDenseBlock_5C(nf, gc)
        self.RDB2 = ResidualDenseBlock_5C(nf, gc)
        self.RDB3 = ResidualDenseBlock_5C(nf, gc)
    
    def forward(self, x):
        out = self.RDB1(x)
        out = self.RDB2(out)
        out = self.RDB3(out)
        return out * 0.2 + x

class ResidualDenseBlock_5C(nn.Module):
    """Residual Dense Block"""
    
    def __init__(self, nf=64, gc=32, bias=True):
        super(ResidualDenseBlock_5C, self).__init__()
        
        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=bias)
        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=bias)
        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1, bias=bias)
        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1, bias=bias)
        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1, bias=bias)
        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)
    
    def forward(self, x):
        x1 = self.lrelu(self.conv1(x))
        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))
        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))
        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))
        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))
        return x5 * 0.2 + x

class SwinIRModel(nn.Module):
    """SwinIR ëª¨ë¸ (ì‹¤ì œ êµ¬í˜„)"""
    
    def __init__(self, img_size=64, patch_size=1, in_chans=3, out_chans=3, 
                 embed_dim=180, depths=[6, 6, 6, 6, 6, 6], num_heads=[6, 6, 6, 6, 6, 6]):
        super(SwinIRModel, self).__init__()
        
        # Shallow feature extraction
        self.conv_first = nn.Conv2d(in_chans, embed_dim, 3, 1, 1)
        
        # Deep feature extraction (simplified)
        self.layers = nn.ModuleList()
        for i in range(len(depths)):
            layer = nn.Sequential(
                nn.Conv2d(embed_dim, embed_dim, 3, 1, 1),
                nn.ReLU(inplace=True),
                nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)
            )
            self.layers.append(layer)
        
        # High-quality image reconstruction
        self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)
        self.conv_before_upsample = nn.Sequential(
            nn.Conv2d(embed_dim, embed_dim, 3, 1, 1),
            nn.LeakyReLU(inplace=True)
        )
        
        # Upsample
        self.upsample = nn.Sequential(
            nn.Conv2d(embed_dim, embed_dim * 4, 3, 1, 1),
            nn.PixelShuffle(2),
            nn.Conv2d(embed_dim, embed_dim * 4, 3, 1, 1),
            nn.PixelShuffle(2)
        )
        
        self.conv_last = nn.Conv2d(embed_dim, out_chans, 3, 1, 1)
    
    def forward(self, x):
        x_first = self.conv_first(x)
        
        res = x_first
        for layer in self.layers:
            res = layer(res) + res
        
        res = self.conv_after_body(res) + x_first
        res = self.conv_before_upsample(res)
        res = self.upsample(res)
        x = self.conv_last(res)
        
        return x

class FaceEnhancementModel(nn.Module):
    """ì–¼êµ´ í–¥ìƒ ëª¨ë¸"""
    
    def __init__(self, in_channels=3, out_channels=3, num_features=64):
        super(FaceEnhancementModel, self).__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels, num_features, 3, 1, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(num_features, num_features * 2, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(num_features * 2, num_features * 4, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True)
        )
        
        # Residual blocks
        self.res_blocks = nn.Sequential(*[
            ResidualBlock(num_features * 4) for _ in range(6)
        ])
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(num_features * 4, num_features * 2, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.ConvTranspose2d(num_features * 2, num_features, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(num_features, out_channels, 3, 1, 1),
            nn.Tanh()
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        res = self.res_blocks(encoded)
        decoded = self.decoder(res)
        return decoded

class ResidualBlock(nn.Module):
    """ì”ì°¨ ë¸”ë¡"""
    
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.conv_block = nn.Sequential(
            nn.Conv2d(channels, channels, 3, 1, 1),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, 3, 1, 1),
            nn.BatchNorm2d(channels)
        )
    
    def forward(self, x):
        return x + self.conv_block(x)

# ==============================================
# ğŸ”¥ ë©”ì¸ PostProcessingStep í´ë˜ìŠ¤ (BaseStepMixin ì™„ì „ ìƒì†)
# ==============================================

class PostProcessingStep(BaseStepMixin):
    """
    Step 07: í›„ì²˜ë¦¬ - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ ì‹¤ì œ AI êµ¬í˜„
    
    âœ… BaseStepMixin ì™„ì „ ìƒì†
    âœ… ë™ê¸° _run_ai_inference() ë©”ì„œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€)
    âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  (ESRGAN, SwinIR, Real-ESRGAN)
    âœ… ëª©ì—… ì½”ë“œ ì™„ì „ ì œê±°
    âœ… M3 Max ìµœì í™”
    """
    
    def __init__(self, **kwargs):
        """ì´ˆê¸°í™”"""
        # BaseStepMixin ì´ˆê¸°í™” (ìˆœì„œ ì¤‘ìš”!)
        super().__init__(**kwargs)
        
        # í›„ì²˜ë¦¬ íŠ¹í™” ì†ì„±
        self.step_name = kwargs.get('step_name', 'PostProcessingStep')
        self.step_id = kwargs.get('step_id', 7)
        
        # ë””ë°”ì´ìŠ¤ ë° ì„¤ì •
        self.device = self._resolve_device(kwargs.get('device', 'auto'))
        self.config = PostProcessingConfig()
        self.is_m3_max = IS_M3_MAX
        self.memory_gb = kwargs.get('memory_gb', 128.0 if IS_M3_MAX else 16.0)
        
        # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ë“¤
        self.esrgan_model = None
        self.swinir_model = None
        self.face_enhancement_model = None
        self.ai_models = {}
        
        # ì–¼êµ´ ê²€ì¶œê¸°
        self.face_detector = None
        
        # ì„±ëŠ¥ ì¶”ì 
        self.processing_stats = {
            'total_processed': 0,
            'successful_enhancements': 0,
            'average_improvement': 0.0,
            'ai_inference_count': 0,
            'cache_hits': 0
        }
        
        # ìŠ¤ë ˆë“œ í’€
        max_workers = 8 if IS_M3_MAX else 4
        self.executor = ThreadPoolExecutor(
            max_workers=max_workers,
            thread_name_prefix=f"{self.step_name}_worker"
        )
        
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        current_file = Path(__file__).absolute()
        backend_root = current_file.parent.parent.parent.parent
        self.model_base_path = backend_root / "app" / "ai_pipeline" / "models" / "ai_models"
        self.checkpoint_path = self.model_base_path / "step_07_post_processing"
        
        self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
        if self.is_m3_max:
            self.logger.info(f"ğŸ M3 Max ìµœì í™” ëª¨ë“œ (ë©”ëª¨ë¦¬: {self.memory_gb}GB)")
    
    def _resolve_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if device == "auto":
            if TORCH_AVAILABLE:
                if MPS_AVAILABLE and IS_M3_MAX:
                    return 'mps'
                elif torch.cuda.is_available():
                    return 'cuda'
            return 'cpu'
        return device
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin í˜¸í™˜ ì´ˆê¸°í™”
    # ==============================================
    
    async def initialize(self) -> bool:
        """BaseStepMixin í˜¸í™˜ ì´ˆê¸°í™”"""
        if self.is_initialized:
            return True
        
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} AI ëª¨ë¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. ì‹¤ì œ AI ëª¨ë¸ë“¤ ë¡œë”©
            await self._load_real_ai_models()
            
            # 2. ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™”
            if self.config.enable_face_detection:
                await self._initialize_face_detector()
            
            # 3. GPU ê°€ì† ì¤€ë¹„
            if self.config.use_gpu_acceleration:
                await self._prepare_gpu_acceleration()
            
            # 4. M3 Max ì›Œë°ì—…
            if IS_M3_MAX:
                await self._warmup_m3_max()
            
            self.is_initialized = True
            self.is_ready = True
            
            model_count = len([m for m in [self.esrgan_model, self.swinir_model, self.face_enhancement_model] if m is not None])
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ - {model_count}ê°œ AI ëª¨ë¸ ë¡œë”©ë¨")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _load_real_ai_models(self):
        """ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ë“¤ ë¡œë”© - ê²€ì¦ ê°•í™”"""
        try:
            self.logger.info("ğŸ§  ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # ModelLoader ê²€ì¦
            if not self.model_loader:
                self.logger.warning("âš ï¸ ModelLoader ì—†ìŒ - ê¸°ë³¸ ëª¨ë¸ë¡œ í´ë°±")
                await self._create_default_models()
                return
            
            # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ê²€ì¦
            required_models = [
                'ESRGAN_x8.pth',
                'RealESRGAN_x4plus.pth', 
                '001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth'
            ]
            
            loaded_count = 0
            for model_name in required_models:
                try:
                    success = await self._load_single_model(model_name)
                    if success:
                        loaded_count += 1
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
            
            self.has_model = loaded_count > 0
            self.model_loaded = self.has_model
            
            self.logger.info(f"âœ… AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}/{len(required_models)}")
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            await self._create_default_models()
   
    def _load_single_model(self, model_path: str, model_type: str = "post_processing") -> bool:
        """ë‹¨ì¼ ëª¨ë¸ ë¡œë”©"""
        try:
            if not self.model_loader:
                self.logger.warning("âš ï¸ ModelLoader ì—†ìŒ")
                return False
                
            model = self.model_loader.load_model(
                model_name=model_path,
                step_name="PostProcessingStep",
                model_type=model_type
            )
            
            if model:
                self.ai_models[model_type] = model
                self.logger.info(f"âœ… {model_path} ë¡œë”© ì„±ê³µ")
                return True
            else:
                self.logger.warning(f"âš ï¸ {model_path} ë¡œë”© ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ {model_path} ë¡œë”© ì˜¤ë¥˜: {e}")
            return False
        
    async def _create_default_models(self):
        """ê¸°ë³¸ AI ëª¨ë¸ ìƒì„± (í´ë°±)"""
        try:
            if TORCH_AVAILABLE:
                self.esrgan_model = ESRGANModel(upscale=4).to(self.device)
                self.swinir_model = SwinIRModel().to(self.device)
                self.face_enhancement_model = FaceEnhancementModel().to(self.device)
                
                for model in [self.esrgan_model, self.swinir_model, self.face_enhancement_model]:
                    model.eval()
                
                self.ai_models = {
                    'esrgan': self.esrgan_model,
                    'swinir': self.swinir_model,
                    'face_enhancement': self.face_enhancement_model
                }
                
                self.has_model = True
                self.model_loaded = True
                self.logger.info("âœ… ê¸°ë³¸ AI ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ PyTorch ì—†ìŒ - Mock ëª¨ë¸ë¡œ í´ë°±")
                
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")

    
    async def _load_esrgan_model(self):
        """ESRGAN ëª¨ë¸ ë¡œë”©"""
        try:
            # ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„
            checkpoint = None
            if self.model_loader:
                try:
                    if hasattr(self.model_loader, 'get_model_async'):
                        checkpoint = await self.model_loader.get_model_async('post_processing_esrgan')
                    else:
                        checkpoint = self.model_loader.get_model('post_processing_esrgan')
                except Exception as e:
                    self.logger.debug(f"ModelLoaderë¥¼ í†µí•œ ESRGAN ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ì§ì ‘ íŒŒì¼ ë¡œë”© ì‹œë„
            if checkpoint is None:
                esrgan_paths = [
                    self.checkpoint_path / "esrgan_x8_ultra" / "ESRGAN_x8.pth",
                    self.checkpoint_path / "ultra_models" / "RealESRGAN_x4plus.pth",
                    self.checkpoint_path / "ultra_models" / "RealESRGAN_x2plus.pth"
                ]
                
                for path in esrgan_paths:
                    if path.exists():
                        checkpoint = torch.load(path, map_location=self.device)
                        self.logger.info(f"âœ… ESRGAN ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {path.name}")
                        break
            
            # ëª¨ë¸ ìƒì„±
            if checkpoint:
                self.esrgan_model = ESRGANModel(upscale=4).to(self.device)
                if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                    self.esrgan_model.load_state_dict(checkpoint['state_dict'], strict=False)
                elif isinstance(checkpoint, dict):
                    self.esrgan_model.load_state_dict(checkpoint, strict=False)
                
                self.esrgan_model.eval()
                self.ai_models['esrgan'] = self.esrgan_model
                self.logger.info("âœ… ESRGAN ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            else:
                # ê¸°ë³¸ ëª¨ë¸ ìƒì„±
                self.esrgan_model = ESRGANModel(upscale=4).to(self.device)
                self.esrgan_model.eval()
                self.ai_models['esrgan'] = self.esrgan_model
                self.logger.info("âœ… ESRGAN ê¸°ë³¸ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error(f"âŒ ESRGAN ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    async def _load_swinir_model(self):
        """SwinIR ëª¨ë¸ ë¡œë”©"""
        try:
            # SwinIR ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            swinir_path = self.checkpoint_path / "ultra_models" / "001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth"
            
            checkpoint = None
            if swinir_path.exists():
                checkpoint = torch.load(swinir_path, map_location=self.device)
                self.logger.info(f"âœ… SwinIR ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {swinir_path.name}")
            
            # ëª¨ë¸ ìƒì„±
            self.swinir_model = SwinIRModel().to(self.device)
            if checkpoint:
                if 'params' in checkpoint:
                    self.swinir_model.load_state_dict(checkpoint['params'], strict=False)
                elif isinstance(checkpoint, dict):
                    self.swinir_model.load_state_dict(checkpoint, strict=False)
            
            self.swinir_model.eval()
            self.ai_models['swinir'] = self.swinir_model
            self.logger.info("âœ… SwinIR ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            
        except Exception as e:
            self.logger.error(f"âŒ SwinIR ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    async def _load_face_enhancement_model(self):
        """ì–¼êµ´ í–¥ìƒ ëª¨ë¸ ë¡œë”©"""
        try:
            # ì–¼êµ´ í–¥ìƒ ëª¨ë¸ ìƒì„±
            self.face_enhancement_model = FaceEnhancementModel().to(self.device)
            
            # ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„
            face_paths = [
                self.checkpoint_path / "ultra_models" / "densenet161_enhance.pth",
                self.checkpoint_path / "ultra_models" / "resnet101_enhance_ultra.pth"
            ]
            
            for path in face_paths:
                if path.exists():
                    try:
                        checkpoint = torch.load(path, map_location=self.device)
                        if isinstance(checkpoint, dict):
                            # í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ë§Œ ë¡œë”©
                            model_dict = self.face_enhancement_model.state_dict()
                            pretrained_dict = {k: v for k, v in checkpoint.items() if k in model_dict and v.size() == model_dict[k].size()}
                            model_dict.update(pretrained_dict)
                            self.face_enhancement_model.load_state_dict(model_dict)
                        
                        self.logger.info(f"âœ… ì–¼êµ´ í–¥ìƒ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {path.name}")
                        break
                    except Exception as e:
                        self.logger.debug(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ ({path.name}): {e}")
            
            self.face_enhancement_model.eval()
            self.ai_models['face_enhancement'] = self.face_enhancement_model
            self.logger.info("âœ… ì–¼êµ´ í–¥ìƒ ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì–¼êµ´ í–¥ìƒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    async def _initialize_face_detector(self):
        """ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™”"""
        try:
            if not OPENCV_AVAILABLE:
                self.logger.warning("âš ï¸ OpenCV ì—†ì–´ì„œ ì–¼êµ´ ê²€ì¶œ ë¹„í™œì„±í™”")
                return
            
            # Haar Cascade ì–¼êµ´ ê²€ì¶œê¸°
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_detector = cv2.CascadeClassifier(cascade_path)
            
            if self.face_detector.empty():
                self.face_detector = None
                self.logger.warning("âš ï¸ ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ ì‹¤íŒ¨")
            else:
                self.logger.info("âœ… ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                
        except Exception as e:
            self.logger.warning(f"ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.face_detector = None
    
    async def _prepare_gpu_acceleration(self):
        """GPU ê°€ì† ì¤€ë¹„"""
        try:
            if self.device == 'mps':
                self.logger.info("ğŸ M3 Max MPS ê°€ì† ì¤€ë¹„ ì™„ë£Œ")
            elif self.device == 'cuda':
                self.logger.info("ğŸš€ CUDA ê°€ì† ì¤€ë¹„ ì™„ë£Œ")
            else:
                self.logger.info("ğŸ’» CPU ëª¨ë“œì—ì„œ ì‹¤í–‰")
                
        except Exception as e:
            self.logger.warning(f"GPU ê°€ì† ì¤€ë¹„ ì‹¤íŒ¨: {e}")
    
    async def _warmup_m3_max(self):
        """M3 Max ìµœì í™” ì›Œë°ì—…"""
        try:
            if not IS_M3_MAX or not TORCH_AVAILABLE:
                return
            
            self.logger.info("ğŸ M3 Max ìµœì í™” ì›Œë°ì—… ì‹œì‘...")
            
            # ë”ë¯¸ í…ì„œë¡œ ëª¨ë¸ ì›Œë°ì—…
            dummy_input = torch.randn(1, 3, 512, 512).to(self.device)
            
            for model_name, model in self.ai_models.items():
                if model is not None:
                    try:
                        with torch.no_grad():
                            _ = model(dummy_input)
                        self.logger.info(f"âœ… {model_name} M3 Max ì›Œë°ì—… ì™„ë£Œ")
                    except Exception as e:
                        self.logger.debug(f"{model_name} ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            # MPS ìºì‹œ ìµœì í™”
            if self.device == 'mps':
                safe_mps_empty_cache()
            
            self.logger.info("ğŸ M3 Max ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"M3 Max ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin í˜¸í™˜ AI ì¶”ë¡  ë©”ì„œë“œ (ë™ê¸° - í”„ë¡œì íŠ¸ í‘œì¤€)
    # ==============================================
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ BaseStepMixin í•µì‹¬ AI ì¶”ë¡  ë©”ì„œë“œ (ë™ê¸° - í”„ë¡œì íŠ¸ í‘œì¤€)
        
        Args:
            processed_input: BaseStepMixinì—ì„œ ë³€í™˜ëœ í‘œì¤€ AI ëª¨ë¸ ì…ë ¥
        
        Returns:
            Dict[str, Any]: AI ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ§  {self.step_name} ì‹¤ì œ AI ì¶”ë¡  ì‹œì‘...")
            inference_start = time.time()
            
            if not processed_input:
                raise ValueError("processed_inputì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                
            if 'fitted_image' not in processed_input:
                # ëŒ€ì²´ í‚¤ ì‹œë„
                for alt_key in ['enhanced_image', 'result_image', 'input_image']:
                    if alt_key in processed_input:
                        processed_input['fitted_image'] = processed_input[alt_key]
                        break
                else:
                    raise ValueError("í•„ìˆ˜ ì…ë ¥ 'fitted_image'ê°€ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ëª¨ë¸ ìƒíƒœ í™•ì¸
            if not self.has_model or not self.ai_models:
                self.logger.warning("âš ï¸ AI ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ - Mock ê²°ê³¼ ë°˜í™˜")
                return self._create_mock_ai_result(processed_input, inference_start)

            # 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            input_tensor = self._preprocess_image_for_ai(fitted_image)
            
            # 3. ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡ ë“¤
            enhancement_results = {}
            
            # Super Resolution (ESRGAN)
            if self.esrgan_model and EnhancementMethod.SUPER_RESOLUTION in self.config.enabled_methods:
                sr_result = self._run_super_resolution_inference(input_tensor)
                enhancement_results['super_resolution'] = sr_result
                self.processing_stats['ai_inference_count'] += 1
            
            # Face Enhancement
            if self.face_enhancement_model and EnhancementMethod.FACE_ENHANCEMENT in self.config.enabled_methods:
                face_result = self._run_face_enhancement_inference(input_tensor)
                enhancement_results['face_enhancement'] = face_result
                self.processing_stats['ai_inference_count'] += 1
            
            # Detail Enhancement (SwinIR)
            if self.swinir_model and EnhancementMethod.DETAIL_ENHANCEMENT in self.config.enabled_methods:
                detail_result = self._run_detail_enhancement_inference(input_tensor)
                enhancement_results['detail_enhancement'] = detail_result
                self.processing_stats['ai_inference_count'] += 1
            
            # 4. ê²°ê³¼ í†µí•©
            final_enhanced_image = self._combine_enhancement_results(
                input_tensor, enhancement_results
            )
            
            # 5. í›„ì²˜ë¦¬
            final_result = self._postprocess_ai_result(final_enhanced_image, fitted_image)
            
            # 6. AI ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ ë°˜í™˜
            inference_time = time.time() - inference_start
            
            ai_output = {
                # ì£¼ìš” ì¶œë ¥
                'enhanced_image': final_result['enhanced_image'],
                'enhancement_quality': final_result['quality_score'],
                'enhancement_methods_used': list(enhancement_results.keys()),
                
                # AI ëª¨ë¸ ì„¸ë¶€ ê²°ê³¼
                'sr_enhancement': enhancement_results.get('super_resolution'),
                'face_enhancement': enhancement_results.get('face_enhancement'),
                'detail_enhancement': enhancement_results.get('detail_enhancement'),
                
                # ì²˜ë¦¬ ì •ë³´
                'inference_time': inference_time,
                'ai_models_used': list(self.ai_models.keys()),
                'device': self.device,
                'success': True,
                
                # ë©”íƒ€ë°ì´í„°
                'metadata': {
                    'input_resolution': fitted_image.size if hasattr(fitted_image, 'size') else None,
                    'output_resolution': final_result['output_size'],
                    'upscale_factor': self.config.upscale_factor,
                    'enhancement_strength': self.config.enhancement_strength,
                    'models_loaded': len(self.ai_models),
                    'is_m3_max': IS_M3_MAX,
                    'total_ai_inferences': self.processing_stats['ai_inference_count']
                }
            }
            
            self.logger.info(f"âœ… {self.step_name} AI ì¶”ë¡  ì™„ë£Œ ({inference_time:.3f}ì´ˆ)")
            self.logger.info(f"ğŸ¯ ì ìš©ëœ í–¥ìƒ: {list(enhancement_results.keys())}")
            self.logger.info(f"ğŸ–ï¸ í–¥ìƒ í’ˆì§ˆ: {final_result['quality_score']:.3f}")
            
            return ai_output
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìŠ¤íƒ: {traceback.format_exc()}")
            
            return {
                'enhanced_image': processed_input.get('fitted_image'),
                'enhancement_quality': 0.0,
                'enhancement_methods_used': [],
                'success': False,
                'error': str(e),
                'inference_time': 0.0,
                'ai_models_used': [],
                'device': self.device
            }

    def _create_mock_ai_result(self, processed_input: Dict[str, Any], start_time: float) -> Dict[str, Any]:
        """Mock AI ê²°ê³¼ ìƒì„±"""
        return {
            'success': True,
            'enhanced_image': processed_input.get('fitted_image'),
            'enhancement_quality': 0.75,  # ì ë‹¹í•œ í’ˆì§ˆ
            'enhancement_methods_used': ['basic_enhancement'],
            'inference_time': time.time() - start_time,
            'ai_models_used': ['mock_model'],
            'device': self.device,
            'mock_mode': True,
            'metadata': {
                'fallback_reason': 'AI ëª¨ë¸ ë¯¸ë¡œë”©',
                'output_resolution': (512, 512),
                'processing_note': 'Mock í–¥ìƒ ê²°ê³¼'
            }
        }

    def _create_error_ai_result(self, error_msg: str, processing_time: float) -> Dict[str, Any]:
        """ì—ëŸ¬ AI ê²°ê³¼ ìƒì„±"""
        return {
            'success': False,
            'enhanced_image': None,
            'enhancement_quality': 0.0,
            'enhancement_methods_used': [],
            'error': error_msg,
            'inference_time': processing_time,
            'ai_models_used': [],
            'device': self.device,
            'error_mode': True
        }


    def _preprocess_image_for_ai(self, image):
        """AI ëª¨ë¸ì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            if not TORCH_AVAILABLE:
                raise RuntimeError("PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            # PIL Image â†’ Tensor
            if PIL_AVAILABLE and isinstance(image, Image.Image):
                # RGB ë³€í™˜
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                
                # í¬ê¸° ì¡°ì • (512x512ë¡œ ì •ê·œí™”)
                image = image.resize((512, 512), Image.LANCZOS)
                
                # Tensor ë³€í™˜
                transform = transforms.Compose([
                    transforms.ToTensor(),
                ])
                
                tensor = transform(image).unsqueeze(0).to(self.device)
                
                # ì •ë°€ë„ ì„¤ì •
                if self.device == "mps":
                    tensor = tensor.float()
                elif self.device == "cuda":
                    tensor = tensor.half()
                
                return tensor
                
            elif NUMPY_AVAILABLE and isinstance(image, np.ndarray):
                # NumPy â†’ PIL â†’ Tensor
                if image.dtype != np.uint8:
                    if image.max() <= 1.0:
                        image = (image * 255).astype(np.uint8)
                    else:
                        image = np.clip(image, 0, 255).astype(np.uint8)
                
                pil_image = Image.fromarray(image)
                return self._preprocess_image_for_ai(pil_image)
            
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
                
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _run_super_resolution_inference(self, input_tensor):
        """ğŸ”¥ ESRGAN Super Resolution ì‹¤ì œ ì¶”ë¡  (ë™ê¸°)"""
        try:
            self.logger.debug("ğŸ”¬ ESRGAN Super Resolution ì¶”ë¡  ì‹œì‘...")
            
            with torch.no_grad():
                # ESRGAN ì¶”ë¡ 
                sr_output = self.esrgan_model(input_tensor)
                
                # ê²°ê³¼ í´ë¨í•‘
                sr_output = torch.clamp(sr_output, 0, 1)
                
                # í’ˆì§ˆ í‰ê°€
                quality_score = self._calculate_enhancement_quality(input_tensor, sr_output)
                
                self.logger.debug(f"âœ… Super Resolution ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}")
                
                return {
                    'enhanced_tensor': sr_output,
                    'quality_score': quality_score,
                    'method': 'ESRGAN',
                    'upscale_factor': self.config.upscale_factor
                }
                
        except Exception as e:
            self.logger.error(f"âŒ Super Resolution ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    def _run_face_enhancement_inference(self, input_tensor):
        """ğŸ”¥ ì–¼êµ´ í–¥ìƒ ì‹¤ì œ ì¶”ë¡  (ë™ê¸°)"""
        try:
            self.logger.debug("ğŸ‘¤ ì–¼êµ´ í–¥ìƒ ì¶”ë¡  ì‹œì‘...")
            
            # ì–¼êµ´ ê²€ì¶œ
            faces = self._detect_faces_in_tensor(input_tensor)
            
            if not faces:
                self.logger.debug("ğŸ‘¤ ì–¼êµ´ì´ ê²€ì¶œë˜ì§€ ì•ŠìŒ")
                return None
            
            with torch.no_grad():
                # ì–¼êµ´ í–¥ìƒ ì¶”ë¡ 
                enhanced_output = self.face_enhancement_model(input_tensor)
                
                # ê²°ê³¼ ì •ê·œí™”
                enhanced_output = torch.clamp(enhanced_output, -1, 1)
                enhanced_output = (enhanced_output + 1) / 2  # [-1, 1] â†’ [0, 1]
                
                # í’ˆì§ˆ í‰ê°€
                quality_score = self._calculate_enhancement_quality(input_tensor, enhanced_output)
                
                self.logger.debug(f"âœ… ì–¼êµ´ í–¥ìƒ ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}")
                
                return {
                    'enhanced_tensor': enhanced_output,
                    'quality_score': quality_score,
                    'method': 'FaceEnhancement',
                    'faces_detected': len(faces)
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ì–¼êµ´ í–¥ìƒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    def _run_detail_enhancement_inference(self, input_tensor):
        """ğŸ”¥ SwinIR ì„¸ë¶€ì‚¬í•­ í–¥ìƒ ì‹¤ì œ ì¶”ë¡  (ë™ê¸°)"""
        try:
            self.logger.debug("ğŸ” SwinIR ì„¸ë¶€ì‚¬í•­ í–¥ìƒ ì¶”ë¡  ì‹œì‘...")
            
            with torch.no_grad():
                # SwinIR ì¶”ë¡ 
                detail_output = self.swinir_model(input_tensor)
                
                # ê²°ê³¼ í´ë¨í•‘
                detail_output = torch.clamp(detail_output, 0, 1)
                
                # í’ˆì§ˆ í‰ê°€
                quality_score = self._calculate_enhancement_quality(input_tensor, detail_output)
                
                self.logger.debug(f"âœ… ì„¸ë¶€ì‚¬í•­ í–¥ìƒ ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}")
                
                return {
                    'enhanced_tensor': detail_output,
                    'quality_score': quality_score,
                    'method': 'SwinIR',
                    'detail_level': 'high'
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ì„¸ë¶€ì‚¬í•­ í–¥ìƒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    def _detect_faces_in_tensor(self, tensor):
        """í…ì„œì—ì„œ ì–¼êµ´ ê²€ì¶œ"""
        try:
            if not self.face_detector or not OPENCV_AVAILABLE:
                return []
            
            # Tensor â†’ NumPy
            image_np = tensor.squeeze().cpu().numpy()
            if len(image_np.shape) == 3:
                image_np = np.transpose(image_np, (1, 2, 0))
            
            # 0-255 ë²”ìœ„ë¡œ ë³€í™˜
            if image_np.max() <= 1.0:
                image_np = (image_np * 255).astype(np.uint8)
            else:
                image_np = image_np.astype(np.uint8)
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
            
            # ì–¼êµ´ ê²€ì¶œ
            faces = self.face_detector.detectMultiScale(
                gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
            )
            
            return [tuple(face) for face in faces]
            
        except Exception as e:
            self.logger.debug(f"ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_enhancement_quality(self, original_tensor, enhanced_tensor):
        """í–¥ìƒ í’ˆì§ˆ ê³„ì‚°"""
        try:
            if not TORCH_AVAILABLE:
                return 0.5
            
            # ê°„ë‹¨í•œ í’ˆì§ˆ ë©”íŠ¸ë¦­ (PSNR ê¸°ë°˜)
            mse = torch.mean((original_tensor - enhanced_tensor) ** 2)
            if mse == 0:
                return 1.0
            
            psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))
            
            # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            quality = min(1.0, max(0.0, (psnr.item() - 20) / 20))
            
            return quality
            
        except Exception as e:
            self.logger.debug(f"í’ˆì§ˆ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _combine_enhancement_results(self, original_tensor, enhancement_results):
        """ì—¬ëŸ¬ í–¥ìƒ ê²°ê³¼ í†µí•©"""
        try:
            if not enhancement_results:
                return original_tensor
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê²°ê³¼ ê²°í•©
            combined_result = original_tensor.clone()
            total_weight = 0.0
            
            for method, result in enhancement_results.items():
                if result and result.get('enhanced_tensor') is not None:
                    quality = result.get('quality_score', 0.5)
                    weight = quality * self.config.enhancement_strength
                    
                    combined_result = combined_result + weight * result['enhanced_tensor']
                    total_weight += weight
            
            if total_weight > 0:
                combined_result = combined_result / (1 + total_weight)
            
            # í´ë¨í•‘
            combined_result = torch.clamp(combined_result, 0, 1)
            
            return combined_result
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ í†µí•© ì‹¤íŒ¨: {e}")
            return original_tensor
    
    def _postprocess_ai_result(self, enhanced_tensor, original_image):
        """AI ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            # Tensor â†’ NumPy
            enhanced_np = enhanced_tensor.squeeze().cpu().numpy()
            if len(enhanced_np.shape) == 3 and enhanced_np.shape[0] == 3:
                enhanced_np = np.transpose(enhanced_np, (1, 2, 0))
            
            # 0-255 ë²”ìœ„ë¡œ ë³€í™˜
            enhanced_np = (enhanced_np * 255).astype(np.uint8)
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self._calculate_final_quality_score(enhanced_np, original_image)
            
            # ì¶œë ¥ í¬ê¸° ì •ë³´
            output_size = enhanced_np.shape[:2] if len(enhanced_np.shape) >= 2 else None
            
            return {
                'enhanced_image': enhanced_np,
                'quality_score': quality_score,
                'output_size': output_size
            }
            
        except Exception as e:
            self.logger.error(f"AI ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'enhanced_image': original_image,
                'quality_score': 0.0,
                'output_size': None
            }
    
    def _calculate_final_quality_score(self, enhanced_image, original_image):
        """ìµœì¢… í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            if not NUMPY_AVAILABLE:
                return 0.5
            
            # ì›ë³¸ ì´ë¯¸ì§€ë¥¼ NumPyë¡œ ë³€í™˜
            if PIL_AVAILABLE and isinstance(original_image, Image.Image):
                original_np = np.array(original_image)
            elif isinstance(original_image, np.ndarray):
                original_np = original_image
            else:
                return 0.5
            
            # í¬ê¸° ë§ì¶¤
            if original_np.shape != enhanced_image.shape:
                if PIL_AVAILABLE:
                    original_pil = Image.fromarray(original_np)
                    original_pil = original_pil.resize(enhanced_image.shape[:2][::-1], Image.LANCZOS)
                    original_np = np.array(original_pil)
                else:
                    return 0.5
            
            # ê°„ë‹¨í•œ í’ˆì§ˆ ë©”íŠ¸ë¦­
            mse = np.mean((original_np.astype(float) - enhanced_image.astype(float)) ** 2)
            if mse == 0:
                return 1.0
            
            psnr = 20 * np.log10(255.0 / np.sqrt(mse))
            quality = min(1.0, max(0.0, (psnr - 20) / 20))
            
            return quality
            
        except Exception as e:
            self.logger.debug(f"ìµœì¢… í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    # ==============================================
    # ğŸ”¥ ì „í†µì  ì´ë¯¸ì§€ ì²˜ë¦¬ ë©”ì„œë“œë“¤ (2ë²ˆ íŒŒì¼ì—ì„œ ë³µì›)
    # ==============================================
    
    def _apply_traditional_denoising(self, image: np.ndarray) -> np.ndarray:
        """ì „í†µì  ë…¸ì´ì¦ˆ ì œê±°"""
        try:
            if not NUMPY_AVAILABLE or not isinstance(image, np.ndarray):
                return image
                
            # scikit-imageê°€ ìˆìœ¼ë©´ ê³ ê¸‰ í•„í„° ì‚¬ìš©
            if SKIMAGE_AVAILABLE:
                denoised = restoration.denoise_bilateral(
                    image, 
                    sigma_color=0.05, 
                    sigma_spatial=15, 
                    channel_axis=2
                )
                return (denoised * 255).astype(np.uint8)
            elif OPENCV_AVAILABLE:
                # OpenCV bilateral filter
                denoised = cv2.bilateralFilter(image, 9, 75, 75)
                return denoised
            else:
                # ê¸°ë³¸ì ì¸ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
                if SCIPY_AVAILABLE:
                    denoised = gaussian_filter(image, sigma=1.0)
                    return denoised.astype(np.uint8)
                else:
                    return image
                
        except Exception as e:
            self.logger.error(f"ì „í†µì  ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            return image
    
    def _apply_advanced_sharpening(self, image: np.ndarray, strength: float) -> np.ndarray:
        """ê³ ê¸‰ ì„ ëª…ë„ í–¥ìƒ"""
        try:
            if not NUMPY_AVAILABLE or not isinstance(image, np.ndarray):
                return image
                
            if not OPENCV_AVAILABLE:
                return image
                
            # ì–¸ìƒ¤í”„ ë§ˆìŠ¤í‚¹
            blurred = cv2.GaussianBlur(image, (5, 5), 1.0)
            unsharp_mask = cv2.addWeighted(image, 1 + strength, blurred, -strength, 0)
            
            # ì ì‘í˜• ì„ ëª…í™”
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            
            # ì—ì§€ ì˜ì—­ì—ë§Œ ì¶”ê°€ ì„ ëª…í™” ì ìš©
            sharpening_kernel = np.array([
                [-1, -1, -1],
                [-1,  9, -1],
                [-1, -1, -1]
            ], dtype=np.float32)
            
            kernel = sharpening_kernel * strength
            sharpened = cv2.filter2D(unsharp_mask, -1, kernel)
            
            # ì—ì§€ ë§ˆìŠ¤í¬ ì ìš©
            edge_mask = (edges > 0).astype(np.float32)
            edge_mask = np.stack([edge_mask, edge_mask, edge_mask], axis=2)
            
            result = unsharp_mask * (1 - edge_mask) + sharpened * edge_mask
            
            return np.clip(result, 0, 255).astype(np.uint8)
            
        except Exception as e:
            self.logger.error(f"ì„ ëª…ë„ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image
    
    def _apply_color_correction(self, image: np.ndarray) -> np.ndarray:
        """ìƒ‰ìƒ ë³´ì •"""
        try:
            if not NUMPY_AVAILABLE or not isinstance(image, np.ndarray):
                return image
                
            if not OPENCV_AVAILABLE:
                return image
                
            # LAB ìƒ‰ê³µê°„ìœ¼ë¡œ ë³€í™˜
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            l, a, b = cv2.split(lab)
            
            # CLAHE (Contrast Limited Adaptive Histogram Equalization) ì ìš©
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            l = clahe.apply(l)
            
            # LAB ì±„ë„ ì¬ê²°í•©
            lab = cv2.merge([l, a, b])
            
            # RGBë¡œ ë‹¤ì‹œ ë³€í™˜
            corrected = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
            
            # í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ ì¡°ì •
            corrected = self._adjust_white_balance(corrected)
            
            return corrected
            
        except Exception as e:
            self.logger.error(f"ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
            return image
    
    def _adjust_white_balance(self, image: np.ndarray) -> np.ndarray:
        """í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ ì¡°ì •"""
        try:
            if not NUMPY_AVAILABLE:
                return image
                
            # Gray World ì•Œê³ ë¦¬ì¦˜
            r_mean = np.mean(image[:, :, 0])
            g_mean = np.mean(image[:, :, 1])
            b_mean = np.mean(image[:, :, 2])
            
            gray_mean = (r_mean + g_mean + b_mean) / 3
            
            r_gain = gray_mean / r_mean if r_mean > 0 else 1.0
            g_gain = gray_mean / g_mean if g_mean > 0 else 1.0
            b_gain = gray_mean / b_mean if b_mean > 0 else 1.0
            
            # ê²Œì¸ ì œí•œ
            max_gain = 1.5
            r_gain = min(r_gain, max_gain)
            g_gain = min(g_gain, max_gain)
            b_gain = min(b_gain, max_gain)
            
            # ì±„ë„ë³„ ì¡°ì •
            balanced = image.copy().astype(np.float32)
            balanced[:, :, 0] *= r_gain
            balanced[:, :, 1] *= g_gain
            balanced[:, :, 2] *= b_gain
            
            return np.clip(balanced, 0, 255).astype(np.uint8)
            
        except Exception as e:
            self.logger.error(f"í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ ì¡°ì • ì‹¤íŒ¨: {e}")
            return image
    
    def _apply_contrast_enhancement(self, image: np.ndarray) -> np.ndarray:
        """ëŒ€ë¹„ í–¥ìƒ"""
        try:
            if not NUMPY_AVAILABLE or not isinstance(image, np.ndarray):
                return image
                
            if not OPENCV_AVAILABLE:
                return image
                
            # ì ì‘í˜• íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            l, a, b = cv2.split(lab)
            
            # CLAHE ì ìš©
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
            l_enhanced = clahe.apply(l)
            
            # ì±„ë„ ì¬ê²°í•©
            lab_enhanced = cv2.merge([l_enhanced, a, b])
            enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2RGB)
            
            # ì¶”ê°€ ëŒ€ë¹„ ì¡°ì • (sigmoid ê³¡ì„ )
            enhanced = self._apply_sigmoid_correction(enhanced, gain=1.2, cutoff=0.5)
            
            return enhanced
            
        except Exception as e:
            self.logger.error(f"ëŒ€ë¹„ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image
    
    def _apply_sigmoid_correction(self, image: np.ndarray, gain: float, cutoff: float) -> np.ndarray:
        """ì‹œê·¸ëª¨ì´ë“œ ê³¡ì„ ì„ ì‚¬ìš©í•œ ëŒ€ë¹„ ì¡°ì •"""
        try:
            if not NUMPY_AVAILABLE:
                return image
                
            # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            normalized = image.astype(np.float32) / 255.0
            
            # ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì ìš©
            sigmoid = 1 / (1 + np.exp(gain * (cutoff - normalized)))
            
            # 0-255 ë²”ìœ„ë¡œ ë‹¤ì‹œ ë³€í™˜
            result = (sigmoid * 255).astype(np.uint8)
            
            return result
            
        except Exception as e:
            self.logger.error(f"ì‹œê·¸ëª¨ì´ë“œ ë³´ì • ì‹¤íŒ¨: {e}")
            return image
    
    def _detect_faces(self, image: np.ndarray) -> List[Tuple[int, int, int, int]]:
        """ì–¼êµ´ ê²€ì¶œ"""
        try:
            if not self.face_detector or not OPENCV_AVAILABLE or not NUMPY_AVAILABLE:
                return []
            
            faces = []
            
            if hasattr(self.face_detector, 'setInput'):
                # DNN ê¸°ë°˜ ê²€ì¶œê¸°
                blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104, 117, 123])
                self.face_detector.setInput(blob)
                detections = self.face_detector.forward()
                
                h, w = image.shape[:2]
                
                for i in range(detections.shape[2]):
                    confidence = detections[0, 0, i, 2]
                    if confidence > 0.5:
                        x1 = int(detections[0, 0, i, 3] * w)
                        y1 = int(detections[0, 0, i, 4] * h)
                        x2 = int(detections[0, 0, i, 5] * w)
                        y2 = int(detections[0, 0, i, 6] * h)
                        faces.append((x1, y1, x2 - x1, y2 - y1))
            else:
                # Haar Cascade ê²€ì¶œê¸°
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                detected_faces = self.face_detector.detectMultiScale(
                    gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
                )
                faces = [tuple(face) for face in detected_faces]
            
            return faces
            
        except Exception as e:
            self.logger.error(f"ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _enhance_face_regions(self, image: np.ndarray, faces: List[Tuple[int, int, int, int]]) -> np.ndarray:
        """ì–¼êµ´ ì˜ì—­ í–¥ìƒ"""
        try:
            if not NUMPY_AVAILABLE or not OPENCV_AVAILABLE:
                return image
                
            enhanced = image.copy()
            
            for (x, y, w, h) in faces:
                # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
                face_region = image[y:y+h, x:x+w]
                
                if face_region.size == 0:
                    continue
                
                # ì–¼êµ´ ì˜ì—­ì— ëŒ€í•´ ë¶€ë“œëŸ¬ìš´ í–¥ìƒ ì ìš©
                # 1. ì•½ê°„ì˜ ë¸”ëŸ¬ë¥¼ í†µí•œ í”¼ë¶€ ë¶€ë“œëŸ½ê²Œ
                blurred = cv2.GaussianBlur(face_region, (5, 5), 1.0)
                
                # 2. ì›ë³¸ê³¼ ë¸”ëŸ¬ì˜ ê°€ì¤‘ í•©ì„±
                softened = cv2.addWeighted(face_region, 0.7, blurred, 0.3, 0)
                
                # 3. ë°ê¸° ì•½ê°„ ì¡°ì •
                brightened = cv2.convertScaleAbs(softened, alpha=1.1, beta=5)
                
                # 4. í–¥ìƒëœ ì–¼êµ´ ì˜ì—­ì„ ì›ë³¸ì— ì ìš©
                enhanced[y:y+h, x:x+w] = brightened
            
            return enhanced
            
        except Exception as e:
            self.logger.error(f"ì–¼êµ´ ì˜ì—­ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image
    
    def _apply_final_post_processing(self, image: np.ndarray) -> np.ndarray:
        """ìµœì¢… í›„ì²˜ë¦¬"""
        try:
            if not NUMPY_AVAILABLE or not OPENCV_AVAILABLE:
                return image
                
            # 1. ë¯¸ì„¸í•œ ë…¸ì´ì¦ˆ ì œê±°
            denoised = cv2.medianBlur(image, 3)
            
            # 2. ì•½ê°„ì˜ ì„ ëª…í™”
            kernel = np.array([[-0.1, -0.1, -0.1],
                               [-0.1,  1.8, -0.1],
                               [-0.1, -0.1, -0.1]])
            sharpened = cv2.filter2D(denoised, -1, kernel)
            
            # 3. ìƒ‰ìƒ ë³´ì •
            final = cv2.convertScaleAbs(sharpened, alpha=1.02, beta=2)
            
            return final
            
        except Exception as e:
            self.logger.error(f"ìµœì¢… í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def _calculate_image_quality(self, image: np.ndarray) -> float:
        """ì´ë¯¸ì§€ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            if not NUMPY_AVAILABLE or not isinstance(image, np.ndarray):
                return 0.5
                
            if not OPENCV_AVAILABLE:
                return 0.5
            
            # ì—¬ëŸ¬ í’ˆì§ˆ ì§€í‘œì˜ ì¡°í•©
            
            # 1. ì„ ëª…ë„ (ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°)
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            sharpness_score = min(laplacian_var / 1000.0, 1.0)
            
            # 2. ëŒ€ë¹„ (í‘œì¤€í¸ì°¨)
            contrast_score = min(np.std(gray) / 128.0, 1.0)
            
            # 3. ë°ê¸° ê· í˜• (íˆìŠ¤í† ê·¸ë¨ ë¶„í¬)
            hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
            hist_normalized = hist / hist.sum()
            entropy = -np.sum(hist_normalized * np.log2(hist_normalized + 1e-10))
            brightness_score = min(entropy / 8.0, 1.0)
            
            # 4. ìƒ‰ìƒ ë‹¤ì–‘ì„±
            rgb_std = np.mean([np.std(image[:, :, i]) for i in range(3)])
            color_score = min(rgb_std / 64.0, 1.0)
            
            # ê°€ì¤‘ í‰ê· 
            quality_score = (
                sharpness_score * 0.3 +
                contrast_score * 0.3 +
                brightness_score * 0.2 +
                color_score * 0.2
            )
            
            return quality_score
            
        except Exception as e:
            self.logger.error(f"í’ˆì§ˆ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _resize_image_preserve_ratio(self, image: np.ndarray, max_height: int, max_width: int) -> np.ndarray:
        """ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •"""
        try:
            if not NUMPY_AVAILABLE or not OPENCV_AVAILABLE:
                return image
                
            h, w = image.shape[:2]
            
            if h <= max_height and w <= max_width:
                return image
            
            # ë¹„ìœ¨ ê³„ì‚°
            ratio = min(max_height / h, max_width / w)
            new_h = int(h * ratio)
            new_w = int(w * ratio)
            
            # ê³ í’ˆì§ˆ ë¦¬ìƒ˜í”Œë§
            resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
            
            return resized
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • ì‹¤íŒ¨: {e}")
            return image
    
    # ==============================================
    # ğŸ”¥ ì‹œê°í™” ê´€ë ¨ ë©”ì„œë“œë“¤ (2ë²ˆ íŒŒì¼ì—ì„œ ë³µì›)
    # ==============================================
    
    async def _create_enhancement_visualization(
        self,
        processed_input: Dict[str, Any],
        result: Dict[str, Any],
        options: Dict[str, Any]
    ) -> Dict[str, str]:
        """í›„ì²˜ë¦¬ ê²°ê³¼ ì‹œê°í™” ì´ë¯¸ì§€ë“¤ ìƒì„±"""
        try:
            if not self.config.enable_visualization:
                return {
                    'before_after_comparison': '',
                    'enhancement_details': '',
                    'quality_metrics': ''
                }
            
            def _create_visualizations():
                original_image = processed_input.get('fitted_image')
                enhanced_image = result.get('enhanced_image')
                
                if not NUMPY_AVAILABLE or not PIL_AVAILABLE:
                    return {
                        'before_after_comparison': '',
                        'enhancement_details': '',
                        'quality_metrics': ''
                    }
                
                visualizations = {}
                
                # 1. Before/After ë¹„êµ ì´ë¯¸ì§€
                if hasattr(self.config, 'show_before_after') and self.config.show_before_after:
                    before_after = self._create_before_after_comparison(
                        original_image, enhanced_image, result
                    )
                    visualizations['before_after_comparison'] = self._numpy_to_base64(before_after)
                else:
                    visualizations['before_after_comparison'] = ''
                
                # 2. í–¥ìƒ ì„¸ë¶€ì‚¬í•­ ì‹œê°í™”
                if hasattr(self.config, 'show_enhancement_details') and self.config.show_enhancement_details:
                    enhancement_details = self._create_enhancement_details_visualization(
                        original_image, enhanced_image, result, options
                    )
                    visualizations['enhancement_details'] = self._numpy_to_base64(enhancement_details)
                else:
                    visualizations['enhancement_details'] = ''
                
                # 3. í’ˆì§ˆ ë©”íŠ¸ë¦­ ì‹œê°í™”
                quality_metrics = self._create_quality_metrics_visualization(
                    result, options
                )
                visualizations['quality_metrics'] = self._numpy_to_base64(quality_metrics)
                
                return visualizations
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(self.executor, _create_visualizations)
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'before_after_comparison': '',
                'enhancement_details': '',
                'quality_metrics': ''
            }
    
    def _create_before_after_comparison(
        self,
        original_image: np.ndarray,
        enhanced_image: np.ndarray,
        result: Dict[str, Any]
    ) -> np.ndarray:
        """Before/After ë¹„êµ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            if not NUMPY_AVAILABLE or not PIL_AVAILABLE or not OPENCV_AVAILABLE:
                return np.ones((600, 1100, 3), dtype=np.uint8) * 200
                
            # ì´ë¯¸ì§€ í¬ê¸° ë§ì¶”ê¸°
            target_size = (512, 512)
            original_resized = cv2.resize(original_image, target_size, interpolation=cv2.INTER_LANCZOS4)
            enhanced_resized = cv2.resize(enhanced_image, target_size, interpolation=cv2.INTER_LANCZOS4)
            
            # ë‚˜ë€íˆ ë°°ì¹˜í•  ìº”ë²„ìŠ¤ ìƒì„±
            canvas_width = target_size[0] * 2 + 100  # 100px ê°„ê²©
            canvas_height = target_size[1] + 100  # ìƒë‹¨ì— í…ìŠ¤íŠ¸ ê³µê°„
            canvas = np.ones((canvas_height, canvas_width, 3), dtype=np.uint8) * 240
            
            # ì´ë¯¸ì§€ ë°°ì¹˜
            canvas[50:50+target_size[1], 25:25+target_size[0]] = original_resized
            canvas[50:50+target_size[1], 75+target_size[0]:75+target_size[0]*2] = enhanced_resized
            
            # PILë¡œ ë³€í™˜í•´ì„œ í…ìŠ¤íŠ¸ ì¶”ê°€
            canvas_pil = Image.fromarray(canvas)
            draw = ImageDraw.Draw(canvas_pil)
            
            # í°íŠ¸ ì„¤ì •
            try:
                title_font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 24)
                subtitle_font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 16)
                text_font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 14)
            except:
                try:
                    title_font = ImageFont.load_default()
                    subtitle_font = ImageFont.load_default()
                    text_font = ImageFont.load_default()
                except:
                    # í…ìŠ¤íŠ¸ ì—†ì´ ì´ë¯¸ì§€ë§Œ ë°˜í™˜
                    return np.array(canvas_pil)
            
            # ì œëª©
            draw.text((canvas_width//2 - 100, 10), "í›„ì²˜ë¦¬ ê²°ê³¼ ë¹„êµ", fill=(50, 50, 50), font=title_font)
            
            # ë¼ë²¨
            draw.text((25 + target_size[0]//2 - 30, 25), "Before", fill=(100, 100, 100), font=subtitle_font)
            draw.text((75 + target_size[0] + target_size[0]//2 - 30, 25), "After", fill=(100, 100, 100), font=subtitle_font)
            
            # í’ˆì§ˆ ê°œì„  ì •ë³´
            improvement_text = f"í’ˆì§ˆ ê°œì„ : {result.get('enhancement_quality', 0):.1%}"
            methods_text = f"ì ìš©ëœ ë°©ë²•: {', '.join(result.get('enhancement_methods_used', [])[:3])}"
            if len(result.get('enhancement_methods_used', [])) > 3:
                methods_text += f" ì™¸ {len(result.get('enhancement_methods_used', [])) - 3}ê°œ"
            
            draw.text((25, canvas_height - 40), improvement_text, fill=(0, 150, 0), font=text_font)
            draw.text((25, canvas_height - 20), methods_text, fill=(80, 80, 80), font=text_font)
            
            # êµ¬ë¶„ì„ 
            draw.line([(target_size[0] + 50, 50), (target_size[0] + 50, 50 + target_size[1])], 
                     fill=(200, 200, 200), width=2)
            
            return np.array(canvas_pil)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Before/After ë¹„êµ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€
            if NUMPY_AVAILABLE:
                return np.ones((600, 1100, 3), dtype=np.uint8) * 200
            else:
                return None
    
    def _create_enhancement_details_visualization(
        self,
        original_image: np.ndarray,
        enhanced_image: np.ndarray,
        result: Dict[str, Any],
        options: Dict[str, Any]
    ) -> np.ndarray:
        """í–¥ìƒ ì„¸ë¶€ì‚¬í•­ ì‹œê°í™”"""
        try:
            if not NUMPY_AVAILABLE or not PIL_AVAILABLE or not OPENCV_AVAILABLE:
                return np.ones((400, 800, 3), dtype=np.uint8) * 200
                
            # ê°„ë‹¨í•œ ê·¸ë¦¬ë“œ ìƒì„±
            grid_size = 256
            canvas_width = grid_size * 3 + 100
            canvas_height = grid_size * 2 + 100
            canvas = np.ones((canvas_height, canvas_width, 3), dtype=np.uint8) * 250
            
            # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
            original_small = cv2.resize(original_image, (grid_size, grid_size))
            enhanced_small = cv2.resize(enhanced_image, (grid_size, grid_size))
            
            # ì´ë¯¸ì§€ ë°°ì¹˜
            canvas[25:25+grid_size, 25:25+grid_size] = original_small
            canvas[25:25+grid_size, 50+grid_size:50+grid_size*2] = enhanced_small
            
            # í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
            canvas_pil = Image.fromarray(canvas)
            draw = ImageDraw.Draw(canvas_pil)
            
            try:
                font = ImageFont.load_default()
            except:
                return np.array(canvas_pil)
            
            # ë¼ë²¨
            draw.text((25, 5), "ì›ë³¸", fill=(50, 50, 50), font=font)
            draw.text((50+grid_size, 5), "í–¥ìƒëœ ì´ë¯¸ì§€", fill=(50, 50, 50), font=font)
            
            # í–¥ìƒ ë°©ë²• ë¦¬ìŠ¤íŠ¸
            y_offset = 25 + grid_size + 20
            draw.text((25, y_offset), "ì ìš©ëœ í–¥ìƒ ë°©ë²•:", fill=(50, 50, 50), font=font)
            
            methods = result.get('enhancement_methods_used', [])
            for i, method in enumerate(methods[:5]):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                method_name = method.replace('_', ' ').title()
                draw.text((25, y_offset + 20 + i*15), f"â€¢ {method_name}", fill=(80, 80, 80), font=font)
            
            return np.array(canvas_pil)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í–¥ìƒ ì„¸ë¶€ì‚¬í•­ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            if NUMPY_AVAILABLE:
                return np.ones((400, 800, 3), dtype=np.uint8) * 200
            else:
                return None
    
    def _create_quality_metrics_visualization(
        self,
        result: Dict[str, Any],
        options: Dict[str, Any]
    ) -> np.ndarray:
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ì‹œê°í™”"""
        try:
            if not NUMPY_AVAILABLE or not PIL_AVAILABLE:
                return np.ones((300, 400, 3), dtype=np.uint8) * 200
                
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì •ë³´ íŒ¨ë„ ìƒì„±
            canvas_width = 400
            canvas_height = 300
            canvas = np.ones((canvas_height, canvas_width, 3), dtype=np.uint8) * 250
            
            canvas_pil = Image.fromarray(canvas)
            draw = ImageDraw.Draw(canvas_pil)
            
            # í°íŠ¸ ì„¤ì •
            try:
                title_font = ImageFont.load_default()
                text_font = ImageFont.load_default()
            except:
                return np.array(canvas_pil)
            
            # ì œëª©
            draw.text((20, 20), "í›„ì²˜ë¦¬ í’ˆì§ˆ ë¶„ì„", fill=(50, 50, 50), font=title_font)
            
            # ì „ì²´ ê°œì„ ë„ í‘œì‹œ
            improvement_percent = result.get('enhancement_quality', 0) * 100
            improvement_color = (0, 150, 0) if improvement_percent > 15 else (255, 150, 0) if improvement_percent > 5 else (255, 0, 0)
            draw.text((20, 50), f"ì „ì²´ í’ˆì§ˆ ê°œì„ : {improvement_percent:.1f}%", fill=improvement_color, font=text_font)
            
            # ì ìš©ëœ ë°©ë²•ë“¤
            y_offset = 80
            draw.text((20, y_offset), "ì ìš©ëœ í–¥ìƒ ë°©ë²•:", fill=(50, 50, 50), font=text_font)
            y_offset += 25
            
            methods = result.get('enhancement_methods_used', [])
            for i, method in enumerate(methods[:8]):  # ìµœëŒ€ 8ê°œ
                method_name = method.replace('_', ' ').title()
                draw.text((30, y_offset), f"â€¢ {method_name}", fill=(80, 80, 80), font=text_font)
                y_offset += 20
            
            # ì²˜ë¦¬ ì‹œê°„ ì •ë³´
            y_offset += 10
            processing_time = result.get('inference_time', 0)
            draw.text((20, y_offset), f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ", fill=(100, 100, 100), font=text_font)
            
            return np.array(canvas_pil)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            if NUMPY_AVAILABLE:
                return np.ones((300, 400, 3), dtype=np.uint8) * 200
            else:
                return None
    
    def _numpy_to_base64(self, image) -> str:
        """numpy ë°°ì—´ì„ base64 ë¬¸ìì—´ë¡œ ë³€í™˜"""
        try:
            # 1. ì…ë ¥ ê²€ì¦
            if image is None:
                self.logger.warning("âš ï¸ ì…ë ¥ ì´ë¯¸ì§€ê°€ Noneì…ë‹ˆë‹¤")
                return ""
                
            if not hasattr(image, 'shape'):
                self.logger.warning("âš ï¸ NumPy ë°°ì—´ì´ ì•„ë‹™ë‹ˆë‹¤")
                return ""
            
            # 2. ì´ë¯¸ì§€ íƒ€ì… ë° ë²”ìœ„ ì •ê·œí™”
            if image.dtype != np.uint8:
                # float íƒ€ì…ì¸ ê²½ìš° 0-1 ë²”ìœ„ë¥¼ 0-255ë¡œ ë³€í™˜
                if image.max() <= 1.0:
                    image = (image * 255).astype(np.uint8)
                else:
                    image = np.clip(image, 0, 255).astype(np.uint8)
            
            # 3. ì°¨ì› ê²€ì¦ ë° ìˆ˜ì •
            if len(image.shape) == 4:  # Batch ì°¨ì› ì œê±°
                image = image.squeeze(0)
            elif len(image.shape) == 2:  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì„ RGBë¡œ ë³€í™˜
                image = np.stack([image] * 3, axis=-1)
            elif len(image.shape) == 3 and image.shape[0] in [1, 3]:  # CHW â†’ HWC ë³€í™˜
                image = np.transpose(image, (1, 2, 0))
            
            # 4. PIL Imageë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
            try:
                pil_image = Image.fromarray(image)
            except Exception as e:
                self.logger.error(f"âŒ PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
                return ""
            
            # 5. RGB ëª¨ë“œ í™•ì¸ ë° ë³€í™˜
            if pil_image.mode not in ['RGB', 'RGBA']:
                pil_image = pil_image.convert('RGB')
            elif pil_image.mode == 'RGBA':
                # RGBAë¥¼ RGBë¡œ ë³€í™˜ (í°ìƒ‰ ë°°ê²½)
                rgb_image = Image.new('RGB', pil_image.size, (255, 255, 255))
                rgb_image.paste(pil_image, mask=pil_image.split()[-1])
                pil_image = rgb_image
            
            # 6. BytesIO ë²„í¼ì— ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            buffer = BytesIO()
            
            # 7. í’ˆì§ˆ ì„¤ì •
            quality = 90  # ê¸°ë³¸ê°’
            if hasattr(self.config, 'visualization_quality'):
                if self.config.visualization_quality == 'high':
                    quality = 95
                elif self.config.visualization_quality == 'low':
                    quality = 75
            
            # 8. ì´ë¯¸ì§€ ì €ì¥ (ìµœì í™” ì˜µì…˜ í¬í•¨)
            pil_image.save(
                buffer, 
                format='JPEG', 
                quality=quality,
                optimize=True,  # íŒŒì¼ í¬ê¸° ìµœì í™”
                progressive=True  # ì ì§„ì  ë¡œë”©
            )
            
            # 9. Base64 ì¸ì½”ë”© (ë²„í¼ í¬ê¸° ê²€ì¦)
            buffer.seek(0)  # ë²„í¼ í¬ì¸í„°ë¥¼ ì²˜ìŒìœ¼ë¡œ
            image_bytes = buffer.getvalue()
            
            if len(image_bytes) == 0:
                self.logger.error("âŒ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨ - ë¹ˆ ë²„í¼")
                return ""
            
            base64_string = base64.b64encode(image_bytes).decode('utf-8')
            
            # 10. ê²°ê³¼ ê²€ì¦
            if len(base64_string) < 100:  # ë„ˆë¬´ ì§§ì€ ê²½ìš°
                self.logger.warning(f"âš ï¸ Base64 ë¬¸ìì—´ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤: {len(base64_string)} ë¬¸ì")
                return ""
            
            self.logger.debug(f"âœ… Base64 ë³€í™˜ ì„±ê³µ: {len(base64_string)} ë¬¸ì, í’ˆì§ˆ: {quality}")
            return base64_string
            
        except Exception as e:
            self.logger.error(f"âŒ Base64 ë³€í™˜ ì™„ì „ ì‹¤íŒ¨: {e}")
            return ""
    
    # ==============================================
    # ğŸ”¥ í†µí•©ëœ process ë©”ì„œë“œ (2ë²ˆ íŒŒì¼ì—ì„œ ë³µì›)
    # ==============================================
    
    async def process(
        self, 
        fitting_result: Dict[str, Any],
        enhancement_options: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        í†µì¼ëœ ì²˜ë¦¬ ì¸í„°í˜ì´ìŠ¤ - Pipeline Manager í˜¸í™˜
        
        Args:
            fitting_result: ê°€ìƒ í”¼íŒ… ê²°ê³¼ (6ë‹¨ê³„ ì¶œë ¥)
            enhancement_options: í–¥ìƒ ì˜µì…˜
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜
                
        Returns:
            Dict[str, Any]: í›„ì²˜ë¦¬ ê²°ê³¼ + ì‹œê°í™” ì´ë¯¸ì§€
        """
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        
        try:
            self.logger.info("âœ¨ í›„ì²˜ë¦¬ ì‹œì‘...")
            
            # 1. ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(fitting_result, enhancement_options)
            if hasattr(self, 'enhancement_cache') and cache_key in self.enhancement_cache:
                cached_result = self.enhancement_cache[cache_key]
                self.processing_stats['cache_hits'] += 1
                self.logger.info("ğŸ’¾ ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜")
                return self._format_result(cached_result)
            
            # 2. ì…ë ¥ ë°ì´í„° ì²˜ë¦¬
            processed_input = self._process_input_data(fitting_result)
            
            # 3. í–¥ìƒ ì˜µì…˜ ì¤€ë¹„
            options = self._prepare_enhancement_options(enhancement_options)
            
            # 4. ë©”ì¸ í–¥ìƒ ì²˜ë¦¬
            result = await self._perform_enhancement_pipeline(
                processed_input, options, **kwargs
            )
            
            # 5. ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±
            if hasattr(self.config, 'enable_visualization') and self.config.enable_visualization:
                visualization_results = await self._create_enhancement_visualization(
                    processed_input, result, options
                )
                result['visualization'] = visualization_results
            
            # 6. ê²°ê³¼ ìºì‹±
            if result.get('success', False):
                if not hasattr(self, 'enhancement_cache'):
                    self.enhancement_cache = {}
                self.enhancement_cache[cache_key] = result
                if len(self.enhancement_cache) > getattr(self.config, 'cache_size', 50):
                    self._cleanup_cache()
            
            # 7. í†µê³„ ì—…ë°ì´íŠ¸
            self._update_statistics(result, time.time() - start_time)
            
            self.logger.info(f"âœ… í›„ì²˜ë¦¬ ì™„ë£Œ - ê°œì„ ë„: {result.get('enhancement_quality', 0):.3f}, "
                            f"ì‹œê°„: {result.get('inference_time', 0):.3f}ì´ˆ")
            
            return self._format_result(result)
            
        except Exception as e:
            error_msg = f"í›„ì²˜ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
            self.logger.error(f"âŒ {error_msg}")
            
            # ì—ëŸ¬ ê²°ê³¼ ë°˜í™˜
            error_result = {
                'success': False,
                'error_message': error_msg,
                'inference_time': time.time() - start_time
            }
            
            return self._format_result(error_result)
    
    def _process_input_data(self, fitting_result: Dict[str, Any]) -> Dict[str, Any]:
        """ì…ë ¥ ë°ì´í„° ì²˜ë¦¬"""
        try:
            # ê°€ìƒ í”¼íŒ… ê²°ê³¼ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
            fitted_image = fitting_result.get('fitted_image') or fitting_result.get('result_image')
            
            if fitted_image is None:
                raise ValueError("í”¼íŒ…ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # íƒ€ì…ë³„ ë³€í™˜
            if isinstance(fitted_image, str):
                # Base64 ë””ì½”ë”©
                import base64
                from io import BytesIO
                image_data = base64.b64decode(fitted_image)
                if PIL_AVAILABLE:
                    image_pil = Image.open(BytesIO(image_data)).convert('RGB')
                    fitted_image = np.array(image_pil) if NUMPY_AVAILABLE else image_pil
                else:
                    raise ValueError("PILì´ ì—†ì–´ì„œ base64 ì´ë¯¸ì§€ ì²˜ë¦¬ ë¶ˆê°€")
                    
            elif TORCH_AVAILABLE and isinstance(fitted_image, torch.Tensor):
                # PyTorch í…ì„œ ì²˜ë¦¬
                if self.data_converter:
                    fitted_image = self.data_converter.tensor_to_numpy(fitted_image)
                else:
                    fitted_image = fitted_image.detach().cpu().numpy()
                    if fitted_image.ndim == 4:
                        fitted_image = fitted_image.squeeze(0)
                    if fitted_image.ndim == 3 and fitted_image.shape[0] == 3:
                        fitted_image = fitted_image.transpose(1, 2, 0)
                    fitted_image = (fitted_image * 255).astype(np.uint8)
                    
            elif PIL_AVAILABLE and isinstance(fitted_image, Image.Image):
                if NUMPY_AVAILABLE:
                    fitted_image = np.array(fitted_image.convert('RGB'))
                else:
                    fitted_image = fitted_image.convert('RGB')
                    
            elif not NUMPY_AVAILABLE or not isinstance(fitted_image, np.ndarray):
                raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(fitted_image)}")
            
            # ì´ë¯¸ì§€ ê²€ì¦ (NumPy ë°°ì—´ì¸ ê²½ìš°)
            if NUMPY_AVAILABLE and isinstance(fitted_image, np.ndarray):
                if fitted_image.ndim != 3 or fitted_image.shape[2] != 3:
                    raise ValueError(f"ì˜ëª»ëœ ì´ë¯¸ì§€ í˜•íƒœ: {fitted_image.shape}")
                
                # í¬ê¸° ì œí•œ í™•ì¸
                max_height, max_width = self.config.max_resolution
                if fitted_image.shape[0] > max_height or fitted_image.shape[1] > max_width:
                    fitted_image = self._resize_image_preserve_ratio(fitted_image, max_height, max_width)
            
            return {
                'fitted_image': fitted_image,
                'original_shape': fitted_image.shape if hasattr(fitted_image, 'shape') else None,
                'mask': fitting_result.get('mask'),
                'confidence': fitting_result.get('confidence', 1.0),
                'metadata': fitting_result.get('metadata', {})
            }
            
        except Exception as e:
            self.logger.error(f"ì…ë ¥ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _prepare_enhancement_options(self, enhancement_options: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """í–¥ìƒ ì˜µì…˜ ì¤€ë¹„"""
        try:
            # ê¸°ë³¸ ì˜µì…˜
            default_options = {
                'quality_level': self.config.quality_level.value,
                'enabled_methods': [method.value for method in self.config.enabled_methods],
                'enhancement_strength': getattr(self.config, 'enhancement_strength', 0.8),
                'preserve_faces': getattr(self, 'preserve_faces', True),
                'auto_adjust_brightness': getattr(self, 'auto_adjust_brightness', True),
            }
            
            # ê° ë°©ë²•ë³„ ì ìš© ì—¬ë¶€ ì„¤ì •
            for method in self.config.enabled_methods:
                default_options[f'apply_{method.value}'] = True
            
            # ì‚¬ìš©ì ì˜µì…˜ìœ¼ë¡œ ë®ì–´ì“°ê¸°
            if enhancement_options:
                default_options.update(enhancement_options)
            
            return default_options
            
        except Exception as e:
            self.logger.error(f"í–¥ìƒ ì˜µì…˜ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _perform_enhancement_pipeline(
        self,
        processed_input: Dict[str, Any],
        options: Dict[str, Any],
        **kwargs
    ) -> Dict[str, Any]:
        """í–¥ìƒ íŒŒì´í”„ë¼ì¸ ìˆ˜í–‰ - ì‹¤ì œ AI ì¶”ë¡  êµ¬í˜„"""
        try:
            image = processed_input['fitted_image']
            if not NUMPY_AVAILABLE or not isinstance(image, np.ndarray):
                # BaseStepMixin _run_ai_inference í˜¸ì¶œ
                return self._run_ai_inference(processed_input)
                
            applied_methods = []
            enhancement_log = []
            
            original_quality = self._calculate_image_quality(image)
            
            # ê° í–¥ìƒ ë°©ë²• ì ìš©
            for method in self.config.enabled_methods:
                method_name = method.value
                
                try:
                    if method == EnhancementMethod.SUPER_RESOLUTION and options.get(f'apply_{method_name}', False):
                        # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡ 
                        input_tensor = self._preprocess_image_for_ai(image)
                        enhanced_result = self._run_super_resolution_inference(input_tensor)
                        if enhanced_result and enhanced_result.get('enhanced_tensor') is not None:
                            # Tensor â†’ NumPy ë³€í™˜
                            enhanced_np = enhanced_result['enhanced_tensor'].squeeze().cpu().numpy()
                            if enhanced_np.ndim == 3 and enhanced_np.shape[0] == 3:
                                enhanced_np = np.transpose(enhanced_np, (1, 2, 0))
                            enhanced_np = (enhanced_np * 255).astype(np.uint8)
                            image = enhanced_np
                            applied_methods.append(method_name)
                            enhancement_log.append("Super Resolution ì ìš© (AI ëª¨ë¸)")
                    
                    elif method in [EnhancementMethod.NOISE_REDUCTION] and options.get(f'apply_{method_name}', False):
                        # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ë˜ëŠ” ì „í†µì  ë°©ë²•
                        enhanced_image = self._apply_traditional_denoising(image)
                        if enhanced_image is not None:
                            image = enhanced_image
                            applied_methods.append(method_name)
                            enhancement_log.append("ë…¸ì´ì¦ˆ ì œê±° ì ìš©")
                    
                    elif method == EnhancementMethod.SHARPENING and options.get(f'apply_{method_name}', False):
                        enhanced_image = self._apply_advanced_sharpening(image, options.get('enhancement_strength', 0.8))
                        if enhanced_image is not None:
                            image = enhanced_image
                            applied_methods.append(method_name)
                            enhancement_log.append("ì„ ëª…ë„ í–¥ìƒ ì ìš©")
                    
                    elif method == EnhancementMethod.COLOR_CORRECTION and options.get(f'apply_{method_name}', False):
                        enhanced_image = self._apply_color_correction(image)
                        if enhanced_image is not None:
                            image = enhanced_image
                            applied_methods.append(method_name)
                            enhancement_log.append("ìƒ‰ìƒ ë³´ì • ì ìš©")
                    
                    elif method == EnhancementMethod.CONTRAST_ENHANCEMENT and options.get(f'apply_{method_name}', False):
                        enhanced_image = self._apply_contrast_enhancement(image)
                        if enhanced_image is not None:
                            image = enhanced_image
                            applied_methods.append(method_name)
                            enhancement_log.append("ëŒ€ë¹„ í–¥ìƒ ì ìš©")
                    
                    elif method == EnhancementMethod.FACE_ENHANCEMENT and options.get('preserve_faces', False) and self.face_detector:
                        faces = self._detect_faces(image)
                        if faces:
                            enhanced_image = self._enhance_face_regions(image, faces)
                            if enhanced_image is not None:
                                image = enhanced_image
                                applied_methods.append(method_name)
                                enhancement_log.append(f"ì–¼êµ´ í–¥ìƒ ì ìš© ({len(faces)}ê°œ ì–¼êµ´)")
                
                except Exception as e:
                    self.logger.warning(f"{method_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            # ìµœì¢… í›„ì²˜ë¦¬
            try:
                final_image = self._apply_final_post_processing(image)
                if final_image is not None:
                    image = final_image
                    enhancement_log.append("ìµœì¢… í›„ì²˜ë¦¬ ì ìš©")
            except Exception as e:
                self.logger.warning(f"ìµœì¢… í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            # í’ˆì§ˆ ê°œì„ ë„ ê³„ì‚°
            final_quality = self._calculate_image_quality(image)
            quality_improvement = final_quality - original_quality
            
            return {
                'success': True,
                'enhanced_image': image,
                'enhancement_quality': quality_improvement,
                'enhancement_methods_used': applied_methods,
                'inference_time': 0.0,  # í˜¸ì¶œë¶€ì—ì„œ ì„¤ì •
                'metadata': {
                    'enhancement_log': enhancement_log,
                    'original_quality': original_quality,
                    'final_quality': final_quality,
                    'original_shape': processed_input['original_shape'],
                    'options_used': options
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error_message': f"í–¥ìƒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}",
                'inference_time': 0.0
            }
    
    def _generate_cache_key(self, fitting_result: Dict[str, Any], enhancement_options: Optional[Dict[str, Any]]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            # ì…ë ¥ ì´ë¯¸ì§€ í•´ì‹œ
            fitted_image = fitting_result.get('fitted_image') or fitting_result.get('result_image')
            if isinstance(fitted_image, str):
                # Base64 ë¬¸ìì—´ì˜ í•´ì‹œ
                image_hash = hashlib.md5(fitted_image.encode()).hexdigest()[:16]
            elif NUMPY_AVAILABLE and isinstance(fitted_image, np.ndarray):
                image_hash = hashlib.md5(fitted_image.tobytes()).hexdigest()[:16]
            else:
                image_hash = str(hash(str(fitted_image)))[:16]
            
            # ì˜µì…˜ í•´ì‹œ
            options_str = json.dumps(enhancement_options or {}, sort_keys=True)
            options_hash = hashlib.md5(options_str.encode()).hexdigest()[:8]
            
            # ì „ì²´ í‚¤ ìƒì„±
            cache_key = f"{image_hash}_{options_hash}_{self.device}_{self.config.quality_level.value}"
            return cache_key
            
        except Exception as e:
            self.logger.warning(f"ìºì‹œ í‚¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"fallback_{time.time()}_{self.device}"
    
    def _cleanup_cache(self):
        """ìºì‹œ ì •ë¦¬ (LRU ë°©ì‹)"""
        try:
            if not hasattr(self, 'enhancement_cache'):
                return
                
            cache_size = getattr(self.config, 'cache_size', 50)
            if len(self.enhancement_cache) <= cache_size:
                return
            
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©ë“¤ ì œê±°
            items = list(self.enhancement_cache.items())
            # ì²˜ë¦¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            items.sort(key=lambda x: x[1].get('inference_time', 0))
            
            # ì ˆë°˜ ì •ë„ ì œê±°
            remove_count = len(items) - cache_size // 2
            
            for i in range(remove_count):
                del self.enhancement_cache[items[i][0]]
            
            self.logger.info(f"ğŸ’¾ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {remove_count}ê°œ í•­ëª© ì œê±°")
            
        except Exception as e:
            self.logger.error(f"ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _update_statistics(self, result: Dict[str, Any], processing_time: float):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.processing_stats['total_processed'] += 1
            
            if result.get('success', False):
                self.processing_stats['successful_enhancements'] += 1
                
                # í‰ê·  ê°œì„ ë„ ì—…ë°ì´íŠ¸
                current_avg = self.processing_stats['average_improvement']
                total_successful = self.processing_stats['successful_enhancements']
                
                improvement = result.get('enhancement_quality', 0)
                self.processing_stats['average_improvement'] = (
                    (current_avg * (total_successful - 1) + improvement) / total_successful
                )
            
            # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            current_avg_time = self.processing_stats.get('average_processing_time', 0)
            total_processed = self.processing_stats['total_processed']
            
            self.processing_stats['average_processing_time'] = (
                (current_avg_time * (total_processed - 1) + processing_time) / total_processed
            )
            
            # ê²°ê³¼ì— ì²˜ë¦¬ ì‹œê°„ ì„¤ì •
            result['inference_time'] = processing_time
            
        except Exception as e:
            self.logger.warning(f"í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _format_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ê²°ê³¼ë¥¼ í‘œì¤€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ í¬ë§· + API í˜¸í™˜ì„±"""
        try:
            # API í˜¸í™˜ì„±ì„ ìœ„í•œ ê²°ê³¼ êµ¬ì¡° (ê¸°ì¡´ í•„ë“œ + ì‹œê°í™” í•„ë“œ)
            formatted_result = {
                'success': result.get('success', False),
                'message': f'í›„ì²˜ë¦¬ ì™„ë£Œ - í’ˆì§ˆ ê°œì„ : {result.get("enhancement_quality", 0):.1%}' if result.get('success') else result.get('error_message', 'ì²˜ë¦¬ ì‹¤íŒ¨'),
                'confidence': min(1.0, max(0.0, result.get('enhancement_quality', 0) + 0.7)) if result.get('success') else 0.0,
                'processing_time': result.get('inference_time', 0),
                'details': {}
            }
            
            if result.get('success', False):
                # í”„ë¡ íŠ¸ì—”ë“œìš© ì‹œê°í™” ì´ë¯¸ì§€ë“¤
                visualization = result.get('visualization', {})
                formatted_result['details'] = {
                    # ì‹œê°í™” ì´ë¯¸ì§€ë“¤
                    'result_image': visualization.get('before_after_comparison', ''),
                    'overlay_image': visualization.get('enhancement_details', ''),
                    
                    # ê¸°ì¡´ ë°ì´í„°ë“¤
                    'applied_methods': result.get('enhancement_methods_used', []),
                    'quality_improvement': result.get('enhancement_quality', 0),
                    'enhancement_count': len(result.get('enhancement_methods_used', [])),
                    'processing_mode': getattr(self.config, 'processing_mode', 'quality'),
                    'quality_level': self.config.quality_level.value,
                    
                    # ìƒì„¸ í–¥ìƒ ì •ë³´
                    'enhancement_details': {
                        'methods_applied': len(result.get('enhancement_methods_used', [])),
                        'improvement_percentage': result.get('enhancement_quality', 0) * 100,
                        'enhancement_log': result.get('metadata', {}).get('enhancement_log', []),
                        'quality_metrics': visualization.get('quality_metrics', '')
                    },
                    
                    # ì‹œìŠ¤í…œ ì •ë³´
                    'step_info': {
                        'step_name': 'post_processing',
                        'step_number': 7,
                        'device': self.device,
                        'quality_level': self.config.quality_level.value,
                        'optimization': 'M3 Max' if self.is_m3_max else self.device,
                        'models_used': {
                            'esrgan_model': self.esrgan_model is not None,
                            'swinir_model': self.swinir_model is not None,
                            'face_enhancement_model': self.face_enhancement_model is not None
                        }
                    },
                    
                    # í’ˆì§ˆ ë©”íŠ¸ë¦­
                    'quality_metrics': {
                        'overall_improvement': result.get('enhancement_quality', 0),
                        'original_quality': result.get('metadata', {}).get('original_quality', 0.5),
                        'final_quality': result.get('metadata', {}).get('final_quality', 0.5),
                        'enhancement_strength': getattr(self.config, 'enhancement_strength', 0.8),
                        'face_enhancement_applied': 'face_enhancement' in result.get('enhancement_methods_used', [])
                    }
                }
                
                # ê¸°ì¡´ API í˜¸í™˜ì„± í•„ë“œë“¤
                enhanced_image = result.get('enhanced_image')
                if enhanced_image is not None:
                    if NUMPY_AVAILABLE and isinstance(enhanced_image, np.ndarray):
                        formatted_result['enhanced_image'] = enhanced_image.tolist()
                    else:
                        formatted_result['enhanced_image'] = enhanced_image
                
                formatted_result.update({
                    'applied_methods': result.get('enhancement_methods_used', []),
                    'metadata': result.get('metadata', {})
                })
            else:
                # ì—ëŸ¬ ì‹œ ê¸°ë³¸ êµ¬ì¡°
                formatted_result['details'] = {
                    'result_image': '',
                    'overlay_image': '',
                    'error': result.get('error_message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'),
                    'step_info': {
                        'step_name': 'post_processing',
                        'step_number': 7,
                        'error': result.get('error_message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                    }
                }
                formatted_result['error_message'] = result.get('error_message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
            
            return formatted_result
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'message': f'ê²°ê³¼ í¬ë§·íŒ… ì‹¤íŒ¨: {e}',
                'confidence': 0.0,
                'processing_time': 0.0,
                'details': {
                    'result_image': '',
                    'overlay_image': '',
                    'error': str(e),
                    'step_info': {
                        'step_name': 'post_processing',
                        'step_number': 7,
                        'error': str(e)
                    }
                },
                'applied_methods': [],
                'error_message': str(e)
            }
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin í˜¸í™˜ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_model(self, model_name: Optional[str] = None):
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        if not model_name:
            return self.esrgan_model or self.swinir_model or self.face_enhancement_model
        
        return self.ai_models.get(model_name)
    
    async def get_model_async(self, model_name: Optional[str] = None):
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸°)"""
        return self.get_model(model_name)
    

    def get_status(self) -> Dict[str, Any]:
        """Step ìƒíƒœ ì¡°íšŒ"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'is_initialized': self.is_initialized,
            'is_ready': self.is_ready,
            'has_model': self.has_model,
            'device': self.device,
            'ai_models_loaded': list(self.ai_models.keys()),  # ğŸ”§ ìˆ˜ì •: eys() â†’ keys()
            'models_count': len(self.ai_models),
            'processing_stats': self.processing_stats,
            'config': {
                'quality_level': self.config.quality_level.value,
                'upscale_factor': self.config.upscale_factor,
                'enabled_methods': [method.value for method in self.config.enabled_methods],
                'enhancement_strength': self.config.enhancement_strength,
                'enable_face_detection': self.config.enable_face_detection
            },
            'system_info': {
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb,
                'torch_available': TORCH_AVAILABLE,
                'mps_available': MPS_AVAILABLE
            }
        }




    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ ì •ë¦¬ ì‹œì‘...")
            
            # AI ëª¨ë¸ë“¤ ì •ë¦¬
            for model_name, model in self.ai_models.items():
                if model is not None:
                    try:
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                    except Exception as e:
                        self.logger.debug(f"ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨ ({model_name}): {e}")
            
            self.ai_models.clear()
            self.esrgan_model = None
            self.swinir_model = None
            self.face_enhancement_model = None
            
            # ì–¼êµ´ ê²€ì¶œê¸° ì •ë¦¬
            if self.face_detector:
                del self.face_detector
                self.face_detector = None
            
            # ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device == 'mps' and TORCH_AVAILABLE:
                try:
                    safe_mps_empty_cache()
                except Exception:
                    pass
            elif self.device == 'cuda' and TORCH_AVAILABLE:
                try:
                    torch.cuda.empty_cache()
                except Exception:
                    pass
            
            gc.collect()
            
            self.is_initialized = False
            self.is_ready = False
            self.logger.info("âœ… í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
        except Exception:
            pass

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_post_processing_step(**kwargs) -> PostProcessingStep:
    """PostProcessingStep íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return PostProcessingStep(**kwargs)

def create_high_quality_post_processing_step(**kwargs) -> PostProcessingStep:
    """ê³ í’ˆì§ˆ í›„ì²˜ë¦¬ Step ìƒì„±"""
    config = {
        'quality_level': QualityLevel.ULTRA,
        'upscale_factor': 4,
        'enhancement_strength': 0.9,
        'enabled_methods': [
            EnhancementMethod.SUPER_RESOLUTION,
            EnhancementMethod.FACE_ENHANCEMENT,
            EnhancementMethod.DETAIL_ENHANCEMENT,
            EnhancementMethod.COLOR_CORRECTION
        ]
    }
    config.update(kwargs)
    return PostProcessingStep(**config)

def create_m3_max_post_processing_step(**kwargs) -> PostProcessingStep:
    """M3 Max ìµœì í™”ëœ í›„ì²˜ë¦¬ Step ìƒì„±"""
    config = {
        'device': 'mps' if MPS_AVAILABLE else 'auto',
        'memory_gb': 128,
        'quality_level': QualityLevel.ULTRA,
        'upscale_factor': 8,
        'enhancement_strength': 1.0
    }
    config.update(kwargs)
    return PostProcessingStep(**config)

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'PostProcessingStep',
    
    # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'ESRGANModel',
    'SwinIRModel', 
    'FaceEnhancementModel',
    'RRDB',
    'ResidualDenseBlock_5C',
    'ResidualBlock',
    
    # ì„¤ì • í´ë˜ìŠ¤ë“¤
    'EnhancementMethod',
    'QualityLevel',
    'PostProcessingConfig',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_post_processing_step',
    'create_high_quality_post_processing_step',
    'create_m3_max_post_processing_step',
    
    # ê°€ìš©ì„± í”Œë˜ê·¸ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE', 
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'OPENCV_AVAILABLE',
    'IS_M3_MAX',
    'CONDA_INFO'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê¹…
# ==============================================

logger.info("ğŸ”¥ Step 07 í›„ì²˜ë¦¬ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ v5.0")
logger.info("=" * 80)
logger.info("âœ… BaseStepMixin ì™„ì „ ìƒì† ë° í˜¸í™˜")
logger.info("âœ… ë™ê¸° _run_ai_inference() ë©”ì„œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€)")
logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  (ESRGAN, SwinIR, Real-ESRGAN)")
logger.info("âœ… ëª©ì—… ì½”ë“œ ì™„ì „ ì œê±°")
logger.info("âœ… 1.3GB ì‹¤ì œ ëª¨ë¸ íŒŒì¼ í™œìš©")
logger.info("")
logger.info("ğŸ§  ì‹¤ì œ AI ëª¨ë¸ë“¤:")
logger.info("   ğŸ¯ ESRGANModel - 8ë°° ì—…ìŠ¤ì¼€ì¼ë§ (ESRGAN_x8.pth 135.9MB)")
logger.info("   ğŸ¯ SwinIRModel - ì„¸ë¶€ì‚¬í•­ í–¥ìƒ (SwinIR-M_x4.pth 56.8MB)")
logger.info("   ğŸ¯ FaceEnhancementModel - ì–¼êµ´ í–¥ìƒ (DenseNet 110.6MB)")
logger.info("   ğŸ‘ï¸ Face Detection - OpenCV Haar Cascade")
logger.info("")
logger.info("ğŸ”§ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ:")
logger.info("   ğŸ“ step_07_post_processing/esrgan_x8_ultra/ESRGAN_x8.pth")
logger.info("   ğŸ“ step_07_post_processing/ultra_models/RealESRGAN_x4plus.pth")
logger.info("   ğŸ“ step_07_post_processing/ultra_models/001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth")
logger.info("   ğŸ“ step_07_post_processing/ultra_models/densenet161_enhance.pth")
logger.info("   ğŸ“ step_07_post_processing/ultra_models/pytorch_model.bin (823.0MB)")
logger.info("")
logger.info("âš¡ AI ì¶”ë¡  íŒŒì´í”„ë¼ì¸:")
logger.info("   1ï¸âƒ£ ì…ë ¥ ì´ë¯¸ì§€ â†’ 512x512 ì •ê·œí™”")
logger.info("   2ï¸âƒ£ ESRGAN â†’ 4x/8x Super Resolution")
logger.info("   3ï¸âƒ£ ì–¼êµ´ ê²€ì¶œ â†’ Face Enhancement")
logger.info("   4ï¸âƒ£ SwinIR â†’ Detail Enhancement")
logger.info("   5ï¸âƒ£ ê²°ê³¼ í†µí•© â†’ í’ˆì§ˆ í–¥ìƒëœ ìµœì¢… ì´ë¯¸ì§€")
logger.info("")
logger.info("ğŸ¯ ì§€ì›í•˜ëŠ” í–¥ìƒ ë°©ë²•:")
logger.info("   ğŸ” SUPER_RESOLUTION - ESRGAN 8ë°° ì—…ìŠ¤ì¼€ì¼ë§")
logger.info("   ğŸ‘¤ FACE_ENHANCEMENT - ì–¼êµ´ ì˜ì—­ ì „ìš© í–¥ìƒ")
logger.info("   âœ¨ DETAIL_ENHANCEMENT - SwinIR ì„¸ë¶€ì‚¬í•­ ë³µì›")
logger.info("   ğŸ¨ COLOR_CORRECTION - ìƒ‰ìƒ ë³´ì •")
logger.info("   ğŸ“ˆ CONTRAST_ENHANCEMENT - ëŒ€ë¹„ í–¥ìƒ")
logger.info("   ğŸ”§ NOISE_REDUCTION - ë…¸ì´ì¦ˆ ì œê±°")
logger.info("")
logger.info(f"ğŸ”§ í˜„ì¬ ì‹œìŠ¤í…œ:")
logger.info(f"   - PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
logger.info(f"   - MPS (M3 Max): {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"   - conda í™˜ê²½: {CONDA_INFO['conda_env']}")
logger.info(f"   - M3 Max ê°ì§€: {'âœ…' if IS_M3_MAX else 'âŒ'}")
logger.info("")
logger.info("ğŸŒŸ ì‚¬ìš© ì˜ˆì‹œ:")
logger.info("   # ê¸°ë³¸ ì‚¬ìš©")
logger.info("   step = create_post_processing_step()")
logger.info("   await step.initialize()")
logger.info("   result = await step.process(fitted_image=fitted_image)")
logger.info("")
logger.info("   # ê³ í’ˆì§ˆ ëª¨ë“œ")
logger.info("   step = create_high_quality_post_processing_step()")
logger.info("")
logger.info("   # M3 Max ìµœì í™”")
logger.info("   step = create_m3_max_post_processing_step()")
logger.info("")
logger.info("   # StepFactory í†µí•© (ìë™ ì˜ì¡´ì„± ì£¼ì…)")
logger.info("   step.set_model_loader(model_loader)")
logger.info("   step.set_memory_manager(memory_manager)")
logger.info("   step.set_data_converter(data_converter)")
logger.info("")
logger.info("ğŸ’¡ í•µì‹¬ íŠ¹ì§•:")
logger.info("   ğŸš« ëª©ì—… ì½”ë“œ ì™„ì „ ì œê±°")
logger.info("   ğŸ§  ì‹¤ì œ AI ëª¨ë¸ë§Œ ì‚¬ìš©")
logger.info("   ğŸ”— BaseStepMixin v19.1 100% í˜¸í™˜")
logger.info("   âš¡ ì‹¤ì œ GPU ê°€ì† ì¶”ë¡ ")
logger.info("   ğŸ M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("   ğŸ“Š ì‹¤ì‹œê°„ í’ˆì§ˆ í‰ê°€")
logger.info("   ğŸ”„ ë‹¤ì¤‘ ëª¨ë¸ ê²°ê³¼ í†µí•©")
logger.info("")
logger.info("=" * 80)
logger.info("ğŸš€ PostProcessingStep v5.0 ì‹¤ì œ AI ì¶”ë¡  ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
logger.info("   âœ… BaseStepMixin v19.1 ì™„ì „ ìƒì†")
logger.info("   âœ… ë™ê¸° _run_ai_inference() ë©”ì„œë“œ")
logger.info("   âœ… 1.3GB ì‹¤ì œ ëª¨ë¸ íŒŒì¼ í™œìš©")
logger.info("   âœ… ESRGAN, SwinIR, FaceEnhancement ì§„ì§œ êµ¬í˜„")
logger.info("   âœ… StepFactory ì™„ì „ í˜¸í™˜")
logger.info("   âœ… ëª©ì—… ì½”ë“œ ì™„ì „ ì œê±°")
logger.info("=" * 80)

# ==============================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ë¶€ (í…ŒìŠ¤íŠ¸ìš©)
# ==============================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¯ MyCloset AI Step 07 - BaseStepMixin v19.1 í˜¸í™˜ ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    async def test_real_ai_inference():
        """ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
        try:
            print("ğŸ”¥ ì‹¤ì œ AI ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
            
            # Step ìƒì„± (BaseStepMixin ìƒì†)
            step = create_post_processing_step(device="cpu")
            print(f"âœ… PostProcessingStep ìƒì„± ì„±ê³µ: {step.step_name}")
            print(f"âœ… BaseStepMixin ìƒì† í™•ì¸: {isinstance(step, BaseStepMixin)}")
            
            # ì´ˆê¸°í™”
            success = await step.initialize()
            print(f"âœ… ì´ˆê¸°í™” {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
            
            # ìƒíƒœ í™•ì¸
            status = step.get_status()
            print(f"ğŸ“Š AI ëª¨ë¸ ë¡œë”© ìƒíƒœ: {status['ai_models_loaded']}")
            print(f"ğŸ”§ ëª¨ë¸ ê°œìˆ˜: {status['models_count']}")
            print(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {status['device']}")
            
            # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸
            if NUMPY_AVAILABLE and PIL_AVAILABLE:
                # 512x512 RGB ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
                dummy_image_np = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
                dummy_image_pil = Image.fromarray(dummy_image_np)
                
                processed_input = {
                    'fitted_image': dummy_image_pil,
                    'enhancement_level': 0.8,
                    'upscale_factor': 4
                }
                
                print("ğŸ§  ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘...")
                # BaseStepMixin í‘œì¤€: ë™ê¸° _run_ai_inference() í˜¸ì¶œ
                ai_result = step._run_ai_inference(processed_input)
                
                if ai_result['success']:
                    print("âœ… AI ì¶”ë¡  ì„±ê³µ!")
                    print(f"   - í–¥ìƒ í’ˆì§ˆ: {ai_result['enhancement_quality']:.3f}")
                    print(f"   - ì‚¬ìš©ëœ ë°©ë²•: {ai_result['enhancement_methods_used']}")
                    print(f"   - ì¶”ë¡  ì‹œê°„: {ai_result['inference_time']:.3f}ì´ˆ")
                    print(f"   - ì‚¬ìš©ëœ AI ëª¨ë¸: {ai_result['ai_models_used']}")
                    print(f"   - ì¶œë ¥ í•´ìƒë„: {ai_result['metadata']['output_resolution']}")
                else:
                    print(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨: {ai_result.get('error', 'Unknown error')}")
            
            # ì •ë¦¬
            await step.cleanup()
            print("âœ… ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    def test_model_architectures():
        """AI ëª¨ë¸ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸"""
        try:
            print("ğŸ—ï¸ AI ëª¨ë¸ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸...")
            
            if not TORCH_AVAILABLE:
                print("âš ï¸ PyTorchê°€ ì—†ì–´ì„œ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€")
                return
            
            # ESRGAN ëª¨ë¸ í…ŒìŠ¤íŠ¸
            try:
                esrgan = ESRGANModel(upscale=4)
                dummy_input = torch.randn(1, 3, 64, 64)
                output = esrgan(dummy_input)
                print(f"âœ… ESRGAN ëª¨ë¸: {dummy_input.shape} â†’ {output.shape}")
            except Exception as e:
                print(f"âŒ ESRGAN ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            # SwinIR ëª¨ë¸ í…ŒìŠ¤íŠ¸
            try:
                swinir = SwinIRModel()
                dummy_input = torch.randn(1, 3, 64, 64)
                output = swinir(dummy_input)
                print(f"âœ… SwinIR ëª¨ë¸: {dummy_input.shape} â†’ {output.shape}")
            except Exception as e:
                print(f"âŒ SwinIR ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            # Face Enhancement ëª¨ë¸ í…ŒìŠ¤íŠ¸
            try:
                face_model = FaceEnhancementModel()
                dummy_input = torch.randn(1, 3, 256, 256)
                output = face_model(dummy_input)
                print(f"âœ… FaceEnhancement ëª¨ë¸: {dummy_input.shape} â†’ {output.shape}")
            except Exception as e:
                print(f"âŒ FaceEnhancement ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            print("âœ… AI ëª¨ë¸ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ AI ëª¨ë¸ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    def test_basestepmixin_compatibility():
        """BaseStepMixin í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        try:
            print("ğŸ”— BaseStepMixin í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸...")
            
            # Step ìƒì„±
            step = create_post_processing_step()
            
            # ìƒì† í™•ì¸
            is_inherited = isinstance(step, BaseStepMixin)
            print(f"âœ… BaseStepMixin ìƒì†: {is_inherited}")
            
            # í•„ìˆ˜ ë©”ì„œë“œ í™•ì¸
            required_methods = ['initialize', '_run_ai_inference', 'cleanup', 'get_status']
            missing_methods = []
            for method in required_methods:
                if not hasattr(step, method):
                    missing_methods.append(method)
            
            if not missing_methods:
                print("âœ… í•„ìˆ˜ ë©”ì„œë“œ ëª¨ë‘ êµ¬í˜„ë¨")
            else:
                print(f"âŒ ëˆ„ë½ëœ ë©”ì„œë“œ: {missing_methods}")
            
            # ë™ê¸° _run_ai_inference í™•ì¸
            import inspect
            is_async = inspect.iscoroutinefunction(step._run_ai_inference)
            print(f"âœ… _run_ai_inference ë™ê¸° ë©”ì„œë“œ: {not is_async}")
            
            print("âœ… BaseStepMixin í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ BaseStepMixin í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    try:
        # ë™ê¸° í…ŒìŠ¤íŠ¸ë“¤
        test_basestepmixin_compatibility()
        print()
        test_model_architectures()
        print()
        
        # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸
        asyncio.run(test_real_ai_inference())
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print()
    print("=" * 80)
    print("âœ¨ BaseStepMixin v19.1 í˜¸í™˜ ì‹¤ì œ AI ì¶”ë¡  í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("ğŸ”¥ BaseStepMixin ì™„ì „ ìƒì† ë° í˜¸í™˜")
    print("ğŸ§  ë™ê¸° _run_ai_inference() ë©”ì„œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€)")
    print("âš¡ ì‹¤ì œ GPU ê°€ì† AI ì¶”ë¡  ì—”ì§„")
    print("ğŸ¯ ESRGAN, SwinIR, FaceEnhancement ì§„ì§œ êµ¬í˜„")
    print("ğŸ M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
    print("ğŸ“Š 1.3GB ì‹¤ì œ ëª¨ë¸ íŒŒì¼ í™œìš©")
    print("ğŸš« ëª©ì—… ì½”ë“œ ì™„ì „ ì œê±°")
    print("=" * 80)

# ==============================================
# ğŸ”¥ END OF FILE - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ ì™„ë£Œ
# ==============================================

"""
âœ¨ Step 07 í›„ì²˜ë¦¬ - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ ì‹¤ì œ AI êµ¬í˜„ v5.0 ìš”ì•½:

ğŸ“‹ í•µì‹¬ ê°œì„ ì‚¬í•­:
   âœ… BaseStepMixin ì™„ì „ ìƒì† (class PostProcessingStep(BaseStepMixin))
   âœ… ë™ê¸° _run_ai_inference() ë©”ì„œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€ ì¤€ìˆ˜)
   âœ… ëª©ì—… ì½”ë“œ ì™„ì „ ì œê±°, ì‹¤ì œ AI ëª¨ë¸ë§Œ í™œìš©
   âœ… ESRGAN x8, RealESRGAN, SwinIR ì§„ì§œ êµ¬í˜„
   âœ… StepFactory â†’ ModelLoader ì˜ì¡´ì„± ì£¼ì… í˜¸í™˜
   âœ… 1.3GB ì‹¤ì œ ëª¨ë¸ íŒŒì¼ (9ê°œ) í™œìš©
   âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”

ğŸ§  ì‹¤ì œ AI ëª¨ë¸ë“¤:
   ğŸ¯ ESRGANModel - 8ë°° ì—…ìŠ¤ì¼€ì¼ë§ (135.9MB)
   ğŸ¯ SwinIRModel - ì„¸ë¶€ì‚¬í•­ í–¥ìƒ (56.8MB)  
   ğŸ¯ FaceEnhancementModel - ì–¼êµ´ í–¥ìƒ (110.6MB)
   ğŸ“ pytorch_model.bin - í†µí•© ëª¨ë¸ (823.0MB)

âš¡ ì‹¤ì œ AI ì¶”ë¡  íŒŒì´í”„ë¼ì¸:
   1ï¸âƒ£ ì…ë ¥ â†’ 512x512 ì •ê·œí™” â†’ Tensor ë³€í™˜
   2ï¸âƒ£ ESRGAN â†’ 4x/8x Super Resolution ì‹¤í–‰
   3ï¸âƒ£ ì–¼êµ´ ê²€ì¶œ â†’ Face Enhancement ì ìš©
   4ï¸âƒ£ SwinIR â†’ Detail Enhancement ìˆ˜í–‰
   5ï¸âƒ£ ê°€ì¤‘ í‰ê·  â†’ ê²°ê³¼ í†µí•© â†’ í’ˆì§ˆ í‰ê°€

ğŸ”§ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ:
   ğŸ“ step_07_post_processing/esrgan_x8_ultra/ESRGAN_x8.pth
   ğŸ“ step_07_post_processing/ultra_models/RealESRGAN_x4plus.pth
   ğŸ“ step_07_post_processing/ultra_models/001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth
   ğŸ“ step_07_post_processing/ultra_models/densenet161_enhance.pth
   ğŸ“ step_07_post_processing/ultra_models/resnet101_enhance_ultra.pth

ğŸ”— BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜:
   âœ… class PostProcessingStep(BaseStepMixin) - ì§ì ‘ ìƒì†
   âœ… def _run_ai_inference(self, processed_input) - ë™ê¸° ë©”ì„œë“œ
   âœ… async def initialize(self) - í‘œì¤€ ì´ˆê¸°í™”
   âœ… def get_status(self) - ìƒíƒœ ì¡°íšŒ
   âœ… async def cleanup(self) - ë¦¬ì†ŒìŠ¤ ì •ë¦¬
   âœ… ì˜ì¡´ì„± ì£¼ì… ì¸í„°í˜ì´ìŠ¤ ì™„ì „ ì§€ì›

ğŸ’¡ ì‚¬ìš©ë²•:
   from steps.step_07_post_processing import PostProcessingStep
   
   # ê¸°ë³¸ ì‚¬ìš© (BaseStepMixin ìƒì†)
   step = create_post_processing_step()
   await step.initialize()
   
   # ì˜ì¡´ì„± ì£¼ì… (StepFactoryì—ì„œ ìë™)
   step.set_model_loader(model_loader)
   step.set_memory_manager(memory_manager)
   
   # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰ (ë™ê¸° ë©”ì„œë“œ)
   result = step._run_ai_inference(processed_input)
   
   # í–¥ìƒëœ ì´ë¯¸ì§€ ë° í’ˆì§ˆ ì •ë³´ íšë“
   enhanced_image = result['enhanced_image']
   quality_score = result['enhancement_quality']
   methods_used = result['enhancement_methods_used']

ğŸ¯ MyCloset AI - Step 07 Post Processing v5.0
   BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ + ì‹¤ì œ AI ì¶”ë¡  ì‹œìŠ¤í…œ ì™„ì„±!
"""