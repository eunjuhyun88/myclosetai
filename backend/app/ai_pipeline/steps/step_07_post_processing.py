# app/ai_pipeline/steps/step_07_post_processing.py
"""
7ë‹¨ê³„: í›„ì²˜ë¦¬ (Post Processing) - í’ˆì§ˆ í–¥ìƒ
MyCloset AI ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ì˜ ìµœì¢… ë‹¨ê³„ - model_loader ìˆ˜ì • ë²„ì „

ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
- Real-ESRGAN: Super Resolution (2x, 4x í•´ìƒë„ í–¥ìƒ)
- GFPGAN: ì–¼êµ´ í’ˆì§ˆ í–¥ìƒ ë° ë³µì›
- CodeFormer: ì „ì²´ì ì¸ ì´ë¯¸ì§€ ë³µì›  
- ìƒ‰ìƒ ë³´ì •, ë…¸ì´ì¦ˆ ì œê±°, ì—£ì§€ í–¥ìƒ
- M3 Max Metal Performance Shaders ìµœì í™”
- ì‹¤ì‹œê°„ í’ˆì§ˆ í‰ê°€ ë° í”¼ë“œë°±

ğŸš€ M3 Max ìµœì í™”:
- 128GB RAM ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬
- 14ì½”ì–´ CPU ë³‘ë ¬ ì²˜ë¦¬
- Metal GPU ê°€ì†
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  íƒ€ì¼ ì²˜ë¦¬
"""

import os
import logging
import time
import asyncio
from typing import Dict, Any, Optional, Tuple, List, Union
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
from PIL import Image, ImageFilter, ImageEnhance
from concurrent.futures import ThreadPoolExecutor
import json

logger = logging.getLogger(__name__)

class PostProcessingStep:
    """
    Step 7: í›„ì²˜ë¦¬ í’ˆì§ˆ í–¥ìƒ 
    ê¸°ì¡´ ai_pipeline êµ¬ì¡°ì— ë§ì¶˜ í†µí•© í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, device: str = "mps", config: Dict[str, Any] = None):
        """
        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (mps, cuda, cpu)
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config or {}
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (M3 Max ìµœì í™”)
        self.device = self._get_optimal_device(device)
        
        # model_loaderëŠ” ë‚´ë¶€ì—ì„œ ìƒì„±í•˜ê±°ë‚˜ ì „ì—­ì—ì„œ ê°€ì ¸ì˜´
        try:
            from app.ai_pipeline.utils.model_loader import get_global_model_loader
            self.model_loader = get_global_model_loader()
        except ImportError:
            logger.warning("ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨ - ê¸°ë³¸ ì²˜ë¦¬ë¡œ ì§„í–‰")
            self.model_loader = None
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ì´ˆê¸°í™”  
        try:
            from app.ai_pipeline.utils.memory_manager import MemoryManager
            self.memory_manager = MemoryManager()
        except ImportError:
            logger.warning("MemoryManager ì´ˆê¸°í™” ì‹¤íŒ¨")
            self.memory_manager = None
        
        # í›„ì²˜ë¦¬ ì„¤ì •
        self.enhancement_config = self.config.get('post_processing', {
            'super_resolution': True,    # Real-ESRGAN
            'face_enhancement': True,    # GFPGAN
            'image_restoration': True,   # CodeFormer
            'color_correction': True,    # ìƒ‰ìƒ ë³´ì •
            'noise_reduction': True,     # ë…¸ì´ì¦ˆ ì œê±°
            'edge_enhancement': True,    # ì—£ì§€ í–¥ìƒ
            'quality_level': 'high'      # í’ˆì§ˆ ìˆ˜ì¤€
        })
        
        # M3 Max ìµœì í™” ì„¤ì •
        self.use_mps = self.device == 'mps' and torch.backends.mps.is_available()
        self.batch_size = self.config.get('batch_size', 1)
        self.tile_size = self.config.get('tile_size', 512)
        
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë“¤
        self.real_esrgan = None
        self.gfpgan = None
        self.codeformer = None
        
        # ì „í†µì  ì²˜ë¦¬ ë„êµ¬ë“¤
        self.color_enhancer = None
        self.noise_reducer = None
        self.edge_enhancer = None
        
        # ì´ˆê¸°í™” ìƒíƒœ
        self.is_initialized = False
        
        # ì„±ëŠ¥ í†µê³„
        self.processing_stats = {
            'total_images': 0,
            'average_time': 0.0,
            'enhancement_success_rate': 0.0
        }
        
        logger.info(f"ğŸ¨ Step 7 í›„ì²˜ë¦¬ ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _get_optimal_device(self, preferred_device: str) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if preferred_device == 'auto':
            if torch.backends.mps.is_available():
                return 'mps'  # M3 Max Metal Performance Shaders
            elif torch.cuda.is_available():
                return 'cuda'
            else:
                return 'cpu'
        return preferred_device
    
    async def initialize(self) -> bool:
        """í›„ì²˜ë¦¬ ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”„ Step 7 í›„ì²˜ë¦¬ ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            initialization_tasks = []
            
            # AI ëª¨ë¸ ë¹„ë™ê¸° ì´ˆê¸°í™”
            if self.enhancement_config.get('super_resolution', True):
                initialization_tasks.append(self._init_real_esrgan())
            
            if self.enhancement_config.get('face_enhancement', True):
                initialization_tasks.append(self._init_gfpgan())
            
            if self.enhancement_config.get('image_restoration', True):
                initialization_tasks.append(self._init_codeformer())
            
            # ì „í†µì  ë„êµ¬ ì´ˆê¸°í™”
            initialization_tasks.extend([
                self._init_color_enhancer(),
                self._init_noise_reducer(),
                self._init_edge_enhancer()
            ])
            
            # ë³‘ë ¬ ì´ˆê¸°í™” ì‹¤í–‰
            results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
            
            # ì´ˆê¸°í™” ê²°ê³¼ í™•ì¸
            success_count = sum(1 for result in results if result is True)
            total_count = len(results)
            
            if success_count >= total_count // 2:  # ì ˆë°˜ ì´ìƒ ì„±ê³µì‹œ OK
                self.is_initialized = True
                logger.info(f"âœ… Step 7 í›„ì²˜ë¦¬ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({success_count}/{total_count})")
                return True
            else:
                logger.error(f"âŒ Step 7 í›„ì²˜ë¦¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({success_count}/{total_count})")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Step 7 í›„ì²˜ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    async def _init_real_esrgan(self) -> bool:
        """Real-ESRGAN Super Resolution ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            model_path = self._get_model_path('real_esrgan', 'RealESRGAN_x4plus.pth')
            
            if os.path.exists(model_path) and self.model_loader:
                self.real_esrgan = await self.model_loader.load_model(
                    'real_esrgan', 
                    model_path, 
                    device=self.device
                )
                logger.info("âœ… Real-ESRGAN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                # í´ë°±: ê¸°ë³¸ ì—…ìŠ¤ì¼€ì¼ë§
                self.real_esrgan = BasicUpscaler()
                logger.info("ğŸ“„ Real-ESRGAN í´ë°± ëª¨ë“œ ì‚¬ìš©")
                return True
                
        except Exception as e:
            logger.warning(f"Real-ESRGAN ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.real_esrgan = BasicUpscaler()
            return True
    
    async def _init_gfpgan(self) -> bool:
        """GFPGAN ì–¼êµ´ í–¥ìƒ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            model_path = self._get_model_path('gfpgan', 'GFPGANv1.4.pth')
            
            if os.path.exists(model_path) and self.model_loader:
                self.gfpgan = await self.model_loader.load_model(
                    'gfpgan', 
                    model_path, 
                    device=self.device
                )
                logger.info("âœ… GFPGAN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                # í´ë°±: ê¸°ë³¸ ì–¼êµ´ í–¥ìƒ
                self.gfpgan = BasicFaceEnhancer()
                logger.info("ğŸ“„ GFPGAN í´ë°± ëª¨ë“œ ì‚¬ìš©")
                return True
                
        except Exception as e:
            logger.warning(f"GFPGAN ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.gfpgan = BasicFaceEnhancer()
            return True
    
    async def _init_codeformer(self) -> bool:
        """CodeFormer ì´ë¯¸ì§€ ë³µì› ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            model_path = self._get_model_path('codeformer', 'codeformer.pth')
            
            if os.path.exists(model_path) and self.model_loader:
                self.codeformer = await self.model_loader.load_model(
                    'codeformer', 
                    model_path, 
                    device=self.device
                )
                logger.info("âœ… CodeFormer ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€ ë³µì›
                self.codeformer = BasicImageRestorer()
                logger.info("ğŸ“„ CodeFormer í´ë°± ëª¨ë“œ ì‚¬ìš©")
                return True
                
        except Exception as e:
            logger.warning(f"CodeFormer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.codeformer = BasicImageRestorer()
            return True
    
    async def _init_color_enhancer(self) -> bool:
        """ìƒ‰ìƒ í–¥ìƒê¸° ì´ˆê¸°í™”"""
        try:
            self.color_enhancer = ColorEnhancer()
            return True
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ í–¥ìƒê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _init_noise_reducer(self) -> bool:
        """ë…¸ì´ì¦ˆ ì œê±°ê¸° ì´ˆê¸°í™”"""
        try:
            self.noise_reducer = NoiseReducer()
            return True
        except Exception as e:
            logger.warning(f"ë…¸ì´ì¦ˆ ì œê±°ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _init_edge_enhancer(self) -> bool:
        """ì—£ì§€ í–¥ìƒê¸° ì´ˆê¸°í™”"""
        try:
            self.edge_enhancer = EdgeEnhancer()
            return True
        except Exception as e:
            logger.warning(f"ì—£ì§€ í–¥ìƒê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _get_model_path(self, model_type: str, filename: str) -> str:
        """ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        base_path = self.config.get('models_base_path', 'app/ai_pipeline/models/ai_models')
        return os.path.join(base_path, model_type, filename)
    
    async def process(
        self, 
        input_image: Union[np.ndarray, torch.Tensor, str],
        enhancement_options: Optional[Dict[str, Any]] = None,
        quality_target: float = 0.8
    ) -> Dict[str, Any]:
        """
        í›„ì²˜ë¦¬ ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜
        
        Args:
            input_image: ì…ë ¥ ì´ë¯¸ì§€
            enhancement_options: í–¥ìƒ ì˜µì…˜
            quality_target: ëª©í‘œ í’ˆì§ˆ (0.0-1.0)
            
        Returns:
            í›„ì²˜ë¦¬ ê²°ê³¼
        """
        if not self.is_initialized:
            raise RuntimeError("í›„ì²˜ë¦¬ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_time = time.time()
        
        try:
            # 1. ì…ë ¥ ì „ì²˜ë¦¬
            image_tensor = await self._preprocess_input(input_image)
            original_shape = image_tensor.shape
            
            logger.info(f"ğŸ¨ í›„ì²˜ë¦¬ ì‹œì‘ - í¬ê¸°: {original_shape}")
            
            # 2. í–¥ìƒ ì˜µì…˜ ì„¤ì •
            options = {**self.enhancement_config, **(enhancement_options or {})}
            
            # 3. ìˆœì°¨ì  í–¥ìƒ ì²˜ë¦¬
            enhanced_image = image_tensor.clone()
            processing_log = []
            
            # Super Resolution (í•´ìƒë„ í–¥ìƒ)
            if options.get('super_resolution', True) and self.real_esrgan:
                logger.info("ğŸ” Super Resolution ì ìš© ì¤‘...")
                enhanced_image, sr_metrics = await self._apply_super_resolution(enhanced_image)
                processing_log.append({'step': 'super_resolution', 'metrics': sr_metrics})
            
            # Face Enhancement (ì–¼êµ´ í–¥ìƒ)
            if options.get('face_enhancement', True) and self.gfpgan:
                logger.info("ğŸ‘¤ ì–¼êµ´ í–¥ìƒ ì ìš© ì¤‘...")
                enhanced_image, face_metrics = await self._apply_face_enhancement(enhanced_image)
                processing_log.append({'step': 'face_enhancement', 'metrics': face_metrics})
            
            # Image Restoration (ì „ì²´ ë³µì›)
            if options.get('image_restoration', True) and self.codeformer:
                logger.info("ğŸ”§ ì´ë¯¸ì§€ ë³µì› ì ìš© ì¤‘...")
                enhanced_image, restoration_metrics = await self._apply_image_restoration(enhanced_image)
                processing_log.append({'step': 'image_restoration', 'metrics': restoration_metrics})
            
            # Color Correction (ìƒ‰ìƒ ë³´ì •)
            if options.get('color_correction', True) and self.color_enhancer:
                logger.info("ğŸŒˆ ìƒ‰ìƒ ë³´ì • ì ìš© ì¤‘...")
                enhanced_image, color_metrics = await self._apply_color_correction(enhanced_image)
                processing_log.append({'step': 'color_correction', 'metrics': color_metrics})
            
            # Noise Reduction (ë…¸ì´ì¦ˆ ì œê±°)
            if options.get('noise_reduction', True) and self.noise_reducer:
                logger.info("ğŸ”‡ ë…¸ì´ì¦ˆ ì œê±° ì ìš© ì¤‘...")
                enhanced_image, noise_metrics = await self._apply_noise_reduction(enhanced_image)
                processing_log.append({'step': 'noise_reduction', 'metrics': noise_metrics})
            
            # Edge Enhancement (ì—£ì§€ í–¥ìƒ)
            if options.get('edge_enhancement', True) and self.edge_enhancer:
                logger.info("ğŸ“ ì—£ì§€ í–¥ìƒ ì ìš© ì¤‘...")
                enhanced_image, edge_metrics = await self._apply_edge_enhancement(enhanced_image)
                processing_log.append({'step': 'edge_enhancement', 'metrics': edge_metrics})
            
            # 4. í›„ì²˜ë¦¬ ë° í’ˆì§ˆ í‰ê°€
            final_image = await self._postprocess_output(enhanced_image)
            quality_score = await self._evaluate_enhancement_quality(
                original=image_tensor, 
                enhanced=final_image
            )
            
            # 5. ê²°ê³¼ êµ¬ì„±
            processing_time = time.time() - start_time
            
            result = {
                'success': True,
                'enhanced_image': final_image,
                'original_shape': original_shape,
                'final_shape': final_image.shape,
                'quality_score': quality_score,
                'processing_time': processing_time,
                'enhancement_log': processing_log,
                'applied_enhancements': [log['step'] for log in processing_log],
                'target_achieved': quality_score >= quality_target,
                'device_used': self.device,
                'config_used': options
            }
            
            # 6. í†µê³„ ì—…ë°ì´íŠ¸
            self._update_processing_stats(processing_time, quality_score)
            
            logger.info(f"âœ… í›„ì²˜ë¦¬ ì™„ë£Œ - í’ˆì§ˆ: {quality_score:.3f}, ì‹œê°„: {processing_time:.2f}ì´ˆ")
            
            return result
            
        except Exception as e:
            error_msg = f"í›„ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
            logger.error(error_msg)
            
            return {
                'success': False,
                'error': error_msg,
                'processing_time': time.time() - start_time
            }
    
    async def _preprocess_input(self, input_image: Union[np.ndarray, torch.Tensor, str]) -> torch.Tensor:
        """ì…ë ¥ ì „ì²˜ë¦¬"""
        try:
            if isinstance(input_image, str):
                # Base64 ë””ì½”ë”©
                import base64
                import io
                from PIL import Image
                
                if input_image.startswith('data:image'):
                    header, data = input_image.split(',', 1)
                    image_data = base64.b64decode(data)
                else:
                    image_data = base64.b64decode(input_image)
                
                pil_image = Image.open(io.BytesIO(image_data))
                image_np = np.array(pil_image)
                
            elif isinstance(input_image, np.ndarray):
                image_np = input_image.copy()
                
            elif isinstance(input_image, torch.Tensor):
                return input_image.to(self.device)
                
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {type(input_image)}")
            
            # NumPyë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜
            if len(image_np.shape) == 3 and image_np.shape[2] == 3:
                # HWC -> CHW
                tensor = torch.from_numpy(image_np.transpose(2, 0, 1)).float()
                
                # ì •ê·œí™” (0-255 -> 0-1)
                if tensor.max() > 1.0:
                    tensor = tensor / 255.0
                
                # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                if len(tensor.shape) == 3:
                    tensor = tensor.unsqueeze(0)
                
                return tensor.to(self.device)
            
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•íƒœ: {image_np.shape}")
                
        except Exception as e:
            logger.error(f"ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë”ë¯¸ í…ì„œ ë°˜í™˜
            return torch.zeros(1, 3, 512, 512, device=self.device)
    
    async def _apply_super_resolution(self, image: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """Super Resolution ì ìš©"""
        try:
            start_time = time.time()
            
            if hasattr(self.real_esrgan, 'enhance'):
                # ì‹¤ì œ Real-ESRGAN ëª¨ë¸
                enhanced = await asyncio.to_thread(self.real_esrgan.enhance, image)
            else:
                # í´ë°±: ê¸°ë³¸ ì—…ìŠ¤ì¼€ì¼ë§
                enhanced = await asyncio.to_thread(self.real_esrgan.upscale, image)
            
            processing_time = time.time() - start_time
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = {
                'processing_time': processing_time,
                'scale_factor': enhanced.shape[-1] / image.shape[-1],
                'improvement_score': self._calculate_sharpness_improvement(image, enhanced)
            }
            
            return enhanced, metrics
            
        except Exception as e:
            logger.warning(f"Super Resolution ì‹¤íŒ¨: {e}")
            return image, {'error': str(e)}
    
    async def _apply_face_enhancement(self, image: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """ì–¼êµ´ í–¥ìƒ ì ìš©"""
        try:
            start_time = time.time()
            
            if hasattr(self.gfpgan, 'enhance'):
                enhanced = await asyncio.to_thread(self.gfpgan.enhance, image)
            else:
                enhanced = await asyncio.to_thread(self.gfpgan.process, image)
            
            processing_time = time.time() - start_time
            
            metrics = {
                'processing_time': processing_time,
                'face_regions_processed': self._count_face_regions(image),
                'enhancement_strength': 0.7  # ê¸°ë³¸ê°’
            }
            
            return enhanced, metrics
            
        except Exception as e:
            logger.warning(f"ì–¼êµ´ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image, {'error': str(e)}
    
    async def _apply_image_restoration(self, image: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """ì´ë¯¸ì§€ ë³µì› ì ìš©"""
        try:
            start_time = time.time()
            
            if hasattr(self.codeformer, 'restore'):
                enhanced = await asyncio.to_thread(self.codeformer.restore, image)
            else:
                enhanced = await asyncio.to_thread(self.codeformer.process, image)
            
            processing_time = time.time() - start_time
            
            metrics = {
                'processing_time': processing_time,
                'artifacts_removed': self._estimate_artifacts_removed(image, enhanced),
                'detail_preservation': self._calculate_detail_preservation(image, enhanced)
            }
            
            return enhanced, metrics
            
        except Exception as e:
            logger.warning(f"ì´ë¯¸ì§€ ë³µì› ì‹¤íŒ¨: {e}")
            return image, {'error': str(e)}
    
    async def _apply_color_correction(self, image: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """ìƒ‰ìƒ ë³´ì • ì ìš©"""
        try:
            start_time = time.time()
            
            enhanced = await asyncio.to_thread(self.color_enhancer.correct_colors, image)
            
            processing_time = time.time() - start_time
            
            metrics = {
                'processing_time': processing_time,
                'color_balance_improvement': self._calculate_color_balance_improvement(image, enhanced),
                'saturation_adjustment': self._calculate_saturation_change(image, enhanced)
            }
            
            return enhanced, metrics
            
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
            return image, {'error': str(e)}
    
    async def _apply_noise_reduction(self, image: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """ë…¸ì´ì¦ˆ ì œê±° ì ìš©"""
        try:
            start_time = time.time()
            
            enhanced = await asyncio.to_thread(self.noise_reducer.reduce_noise, image)
            
            processing_time = time.time() - start_time
            
            metrics = {
                'processing_time': processing_time,
                'noise_reduction_amount': self._calculate_noise_reduction(image, enhanced),
                'detail_preservation': self._calculate_detail_preservation(image, enhanced)
            }
            
            return enhanced, metrics
            
        except Exception as e:
            logger.warning(f"ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            return image, {'error': str(e)}
    
    async def _apply_edge_enhancement(self, image: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """ì—£ì§€ í–¥ìƒ ì ìš©"""
        try:
            start_time = time.time()
            
            enhanced = await asyncio.to_thread(self.edge_enhancer.enhance_edges, image)
            
            processing_time = time.time() - start_time
            
            metrics = {
                'processing_time': processing_time,
                'edge_strength_improvement': self._calculate_edge_improvement(image, enhanced),
                'sharpness_gain': self._calculate_sharpness_improvement(image, enhanced)
            }
            
            return enhanced, metrics
            
        except Exception as e:
            logger.warning(f"ì—£ì§€ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image, {'error': str(e)}
    
    async def _postprocess_output(self, image: torch.Tensor) -> torch.Tensor:
        """ì¶œë ¥ í›„ì²˜ë¦¬"""
        try:
            # í…ì„œ ì •ê·œí™” ë° í´ë¦¬í•‘
            image = torch.clamp(image, 0.0, 1.0)
            
            # ìµœì¢… í’ˆì§ˆ ì¡°ì •
            if self.enhancement_config.get('final_adjustment', True):
                image = self._apply_final_adjustments(image)
            
            return image
            
        except Exception as e:
            logger.warning(f"ì¶œë ¥ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def _apply_final_adjustments(self, image: torch.Tensor) -> torch.Tensor:
        """ìµœì¢… ì¡°ì • ì ìš©"""
        try:
            # ì•½ê°„ì˜ ì„ ëª…ë„ í–¥ìƒ
            if self.enhancement_config.get('final_sharpening', True):
                image = self._apply_unsharp_mask(image, strength=0.2)
            
            # ìƒ‰ìƒ ë¯¸ì„¸ ì¡°ì •
            if self.enhancement_config.get('final_color_boost', True):
                image = self._boost_colors(image, factor=0.1)
            
            return image
            
        except Exception as e:
            logger.warning(f"ìµœì¢… ì¡°ì • ì‹¤íŒ¨: {e}")
            return image
    
    def _apply_unsharp_mask(self, image: torch.Tensor, strength: float = 0.2) -> torch.Tensor:
        """ì–¸ìƒµ ë§ˆìŠ¤í¬ ì ìš©"""
        try:
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
            blurred = F.conv2d(
                image,
                self._get_gaussian_kernel(5, 1.0).to(image.device),
                padding=2,
                groups=image.shape[1]
            )
            
            # ì–¸ìƒµ ë§ˆìŠ¤í¬
            unsharp = image + strength * (image - blurred)
            
            return torch.clamp(unsharp, 0.0, 1.0)
            
        except Exception as e:
            logger.warning(f"ì–¸ìƒµ ë§ˆìŠ¤í¬ ì‹¤íŒ¨: {e}")
            return image
    
    def _boost_colors(self, image: torch.Tensor, factor: float = 0.1) -> torch.Tensor:
        """ìƒ‰ìƒ ë¶€ìŠ¤íŠ¸"""
        try:
            # RGBë¥¼ HSVë¡œ ë³€í™˜ (ê·¼ì‚¬)
            # ë‹¨ìˆœí™”ëœ ì±„ë„ ì¦ê°€
            mean_brightness = torch.mean(image, dim=1, keepdim=True)
            color_deviation = image - mean_brightness
            
            # ì±„ë„ ì¦ê°€
            boosted = image + factor * color_deviation
            
            return torch.clamp(boosted, 0.0, 1.0)
            
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ë¶€ìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return image
    
    def _get_gaussian_kernel(self, size: int, sigma: float) -> torch.Tensor:
        """ê°€ìš°ì‹œì•ˆ ì»¤ë„ ìƒì„±"""
        coords = torch.arange(size, dtype=torch.float32)
        coords -= size // 2
        
        g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))
        g /= g.sum()
        
        # 2D ì»¤ë„
        kernel = g[:, None] * g[None, :]
        
        # 3ì±„ë„ìš©ìœ¼ë¡œ í™•ì¥
        kernel = kernel.expand(3, 1, size, size)
        
        return kernel
    
    async def _evaluate_enhancement_quality(self, original: torch.Tensor, enhanced: torch.Tensor) -> float:
        """í–¥ìƒ í’ˆì§ˆ í‰ê°€"""
        try:
            # ì—¬ëŸ¬ í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¡°í•©
            
            # 1. ì„ ëª…ë„ ê°œì„ 
            sharpness_gain = self._calculate_sharpness_improvement(original, enhanced)
            
            # 2. ë””í…Œì¼ ë³´ì¡´
            detail_preservation = self._calculate_detail_preservation(original, enhanced)
            
            # 3. ìƒ‰ìƒ ìì—°ìŠ¤ëŸ¬ì›€
            color_naturalness = self._calculate_color_naturalness(enhanced)
            
            # 4. ì•„í‹°íŒ©íŠ¸ ìˆ˜ì¤€
            artifact_level = self._estimate_artifact_level(enhanced)
            
            # ì¢…í•© ì ìˆ˜
            quality_score = (
                sharpness_gain * 0.3 +
                detail_preservation * 0.25 +
                color_naturalness * 0.25 +
                (1.0 - artifact_level) * 0.2
            )
            
            return max(0.0, min(1.0, quality_score))
            
        except Exception as e:
            logger.warning(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_sharpness_improvement(self, original: torch.Tensor, enhanced: torch.Tensor) -> float:
        """ì„ ëª…ë„ ê°œì„  ê³„ì‚°"""
        try:
            # ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°ìœ¼ë¡œ ì„ ëª…ë„ ì¸¡ì •
            laplacian_kernel = torch.tensor([[[0, -1, 0], [-1, 4, -1], [0, -1, 0]]], 
                                          dtype=torch.float32, device=original.device)
            
            orig_sharpness = self._calculate_laplacian_variance(original, laplacian_kernel)
            enhanced_sharpness = self._calculate_laplacian_variance(enhanced, laplacian_kernel)
            
            if orig_sharpness > 0:
                improvement = (enhanced_sharpness - orig_sharpness) / orig_sharpness
                return max(0.0, min(1.0, improvement + 0.5))  # 0.5 ê¸°ì¤€ì 
            
            return 0.5
            
        except Exception as e:
            logger.warning(f"ì„ ëª…ë„ ê°œì„  ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_laplacian_variance(self, image: torch.Tensor, kernel: torch.Tensor) -> float:
        """ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚° ê³„ì‚°"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if image.shape[1] == 3:
                gray = 0.299 * image[:, 0:1] + 0.587 * image[:, 1:2] + 0.114 * image[:, 2:3]
            else:
                gray = image
            
            # ë¼í”Œë¼ì‹œì•ˆ ì ìš©
            laplacian = F.conv2d(gray, kernel.unsqueeze(0), padding=1)
            
            return float(torch.var(laplacian))
            
        except Exception as e:
            logger.warning(f"ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚° ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_detail_preservation(self, original: torch.Tensor, enhanced: torch.Tensor) -> float:
        """ë””í…Œì¼ ë³´ì¡´ë„ ê³„ì‚°"""
        try:
            # ê³ ì£¼íŒŒ ì„±ë¶„ ë¹„êµ
            orig_details = self._extract_high_frequency(original)
            enhanced_details = self._extract_high_frequency(enhanced)
            
            # ìƒê´€ê´€ê³„ ê³„ì‚°
            correlation = F.cosine_similarity(
                orig_details.flatten(), 
                enhanced_details.flatten(), 
                dim=0
            )
            
            return float((correlation + 1.0) / 2.0)  # -1~1ì„ 0~1ë¡œ ë³€í™˜
            
        except Exception as e:
            logger.warning(f"ë””í…Œì¼ ë³´ì¡´ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _extract_high_frequency(self, image: torch.Tensor) -> torch.Tensor:
        """ê³ ì£¼íŒŒ ì„±ë¶„ ì¶”ì¶œ"""
        try:
            # ê³ ì£¼íŒŒ í•„í„° (ë¼í”Œë¼ì‹œì•ˆ)
            kernel = torch.tensor([[[0, -1, 0], [-1, 4, -1], [0, -1, 0]]], 
                                dtype=torch.float32, device=image.device)
            
            if image.shape[1] == 3:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
                gray = 0.299 * image[:, 0:1] + 0.587 * image[:, 1:2] + 0.114 * image[:, 2:3]
            else:
                gray = image
            
            high_freq = F.conv2d(gray, kernel.unsqueeze(0), padding=1)
            
            return high_freq
            
        except Exception as e:
            logger.warning(f"ê³ ì£¼íŒŒ ì„±ë¶„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return torch.zeros_like(image[:, 0:1])
    
    def _calculate_color_naturalness(self, image: torch.Tensor) -> float:
        """ìƒ‰ìƒ ìì—°ìŠ¤ëŸ¬ì›€ ê³„ì‚°"""
        try:
            # RGB ê°’ ë¶„í¬ ë¶„ì„
            r_mean = torch.mean(image[:, 0])
            g_mean = torch.mean(image[:, 1])
            b_mean = torch.mean(image[:, 2])
            
            # ìƒ‰ìƒ ê· í˜• ê²€ì‚¬ (ìì—°ìŠ¤ëŸ¬ìš´ ì´ë¯¸ì§€ëŠ” ì ì ˆí•œ ê· í˜•ì„ ê°€ì§)
            color_balance = 1.0 - torch.std(torch.tensor([r_mean, g_mean, b_mean]))
            
            # ì±„ë„ ê²€ì‚¬ (ê³¼ë„í•œ ì±„ë„ëŠ” ë¶€ìì—°ìŠ¤ëŸ¬ì›€)
            saturation = torch.std(image, dim=1).mean()
            saturation_score = 1.0 - torch.clamp(saturation - 0.2, 0, 1)
            
            # ì¡°í•©
            naturalness = (color_balance * 0.6 + saturation_score * 0.4)
            
            return float(torch.clamp(naturalness, 0.0, 1.0))
            
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ìì—°ìŠ¤ëŸ¬ì›€ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _estimate_artifact_level(self, image: torch.Tensor) -> float:
        """ì•„í‹°íŒ©íŠ¸ ìˆ˜ì¤€ ì¶”ì •"""
        try:
            # ë¸”ë¡œí‚¹ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            blocking_score = self._detect_blocking_artifacts(image)
            
            # ë§ì‰ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ
            ringing_score = self._detect_ringing_artifacts(image)
            
            # ì „ì²´ ì•„í‹°íŒ©íŠ¸ ìˆ˜ì¤€
            artifact_level = (blocking_score + ringing_score) / 2.0
            
            return float(torch.clamp(artifact_level, 0.0, 1.0))
            
        except Exception as e:
            logger.warning(f"ì•„í‹°íŒ©íŠ¸ ìˆ˜ì¤€ ì¶”ì • ì‹¤íŒ¨: {e}")
            return 0.3
    
    def _detect_blocking_artifacts(self, image: torch.Tensor) -> torch.Tensor:
        """ë¸”ë¡œí‚¹ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ"""
        try:
            # 8x8 ë¸”ë¡ ê²½ê³„ ë¶ˆì—°ì†ì„± ê²€ì‚¬
            b, c, h, w = image.shape
            
            # ìˆ˜ì§ ê²½ê³„ ê²€ì‚¬
            vertical_diff = torch.abs(image[:, :, :, 8::8] - image[:, :, :, 7::8])
            
            # ìˆ˜í‰ ê²½ê³„ ê²€ì‚¬  
            horizontal_diff = torch.abs(image[:, :, 8::8, :] - image[:, :, 7::8, :])
            
            # í‰ê·  ë¶ˆì—°ì†ì„±
            blocking_level = (torch.mean(vertical_diff) + torch.mean(horizontal_diff)) / 2.0
            
            return blocking_level
            
        except Exception as e:
            logger.warning(f"ë¸”ë¡œí‚¹ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return torch.tensor(0.0)
    
    def _detect_ringing_artifacts(self, image: torch.Tensor) -> torch.Tensor:
        """ë§ì‰ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ"""
        try:
            # ë¼í”Œë¼ì‹œì•ˆìœ¼ë¡œ ì—£ì§€ ì£¼ë³€ ì§„ë™ ê²€ì¶œ
            laplacian_kernel = torch.tensor([[[0, -1, 0], [-1, 4, -1], [0, -1, 0]]], 
                                          dtype=torch.float32, device=image.device)
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if image.shape[1] == 3:
                gray = 0.299 * image[:, 0:1] + 0.587 * image[:, 1:2] + 0.114 * image[:, 2:3]
            else:
                gray = image
            
            # ë¼í”Œë¼ì‹œì•ˆ ì ìš©
            laplacian = F.conv2d(gray, laplacian_kernel.unsqueeze(0), padding=1)
            
            # ë§ì‰ì€ ë¼í”Œë¼ì‹œì•ˆì˜ ê³¼ë„í•œ ë³€ë™ìœ¼ë¡œ ë‚˜íƒ€ë‚¨
            ringing_level = torch.std(laplacian)
            
            return ringing_level / 10.0  # ì •ê·œí™”
            
        except Exception as e:
            logger.warning(f"ë§ì‰ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return torch.tensor(0.0)
    
    def _count_face_regions(self, image: torch.Tensor) -> int:
        """ì–¼êµ´ ì˜ì—­ ì¹´ìš´íŠ¸ (ê°„ë‹¨ ë²„ì „)"""
        try:
            # ê°„ë‹¨í•œ ì–¼êµ´ ì˜ì—­ ì¶”ì •
            # ì‹¤ì œë¡œëŠ” ì–¼êµ´ ê²€ì¶œê¸°ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
            return 1  # ê¸°ë³¸ê°’
            
        except Exception as e:
            logger.warning(f"ì–¼êµ´ ì˜ì—­ ì¹´ìš´íŠ¸ ì‹¤íŒ¨: {e}")
            return 0
    
    def _estimate_artifacts_removed(self, original: torch.Tensor, enhanced: torch.Tensor) -> float:
        """ì œê±°ëœ ì•„í‹°íŒ©íŠ¸ ì¶”ì •"""
        try:
            orig_artifacts = self._estimate_artifact_level(original)
            enhanced_artifacts = self._estimate_artifact_level(enhanced)
            
            artifacts_removed = float(orig_artifacts - enhanced_artifacts)
            
            return max(0.0, artifacts_removed)
            
        except Exception as e:
            logger.warning(f"ì•„í‹°íŒ©íŠ¸ ì œê±° ì¶”ì • ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_color_balance_improvement(self, original: torch.Tensor, enhanced: torch.Tensor) -> float:
        """ìƒ‰ìƒ ê· í˜• ê°œì„  ê³„ì‚°"""
        try:
            orig_balance = self._calculate_color_naturalness(original)
            enhanced_balance = self._calculate_color_naturalness(enhanced)
            
            improvement = enhanced_balance - orig_balance
            
            return float(max(0.0, improvement))
            
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ê· í˜• ê°œì„  ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_saturation_change(self, original: torch.Tensor, enhanced: torch.Tensor) -> float:
        """ì±„ë„ ë³€í™” ê³„ì‚°"""
        try:
            orig_saturation = torch.std(original, dim=1).mean()
            enhanced_saturation = torch.std(enhanced, dim=1).mean()
            
            saturation_change = float(enhanced_saturation - orig_saturation)
            
            return saturation_change
            
        except Exception as e:
            logger.warning(f"ì±„ë„ ë³€í™” ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_noise_reduction(self, original: torch.Tensor, enhanced: torch.Tensor) -> float:
        """ë…¸ì´ì¦ˆ ê°ì†ŒëŸ‰ ê³„ì‚°"""
        try:
            # ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆ ë¹„êµ
            orig_noise = torch.std(self._extract_high_frequency(original))
            enhanced_noise = torch.std(self._extract_high_frequency(enhanced))
            
            noise_reduction = float(orig_noise - enhanced_noise)
            
            return max(0.0, noise_reduction)
            
        except Exception as e:
            logger.warning(f"ë…¸ì´ì¦ˆ ê°ì†ŒëŸ‰ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_edge_improvement(self, original: torch.Tensor, enhanced: torch.Tensor) -> float:
        """ì—£ì§€ ê°œì„  ê³„ì‚°"""
        try:
            # ì—£ì§€ ê°•ë„ ë¹„êµ
            orig_edges = self._calculate_edge_strength(original)
            enhanced_edges = self._calculate_edge_strength(enhanced)
            
            edge_improvement = float(enhanced_edges - orig_edges)
            
            return max(0.0, edge_improvement)
            
        except Exception as e:
            logger.warning(f"ì—£ì§€ ê°œì„  ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_edge_strength(self, image: torch.Tensor) -> torch.Tensor:
        """ì—£ì§€ ê°•ë„ ê³„ì‚°"""
        try:
            # Sobel í•„í„°
            sobel_x = torch.tensor([[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]], 
                                 dtype=torch.float32, device=image.device)
            sobel_y = torch.tensor([[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]], 
                                 dtype=torch.float32, device=image.device)
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if image.shape[1] == 3:
                gray = 0.299 * image[:, 0:1] + 0.587 * image[:, 1:2] + 0.114 * image[:, 2:3]
            else:
                gray = image
            
            # Sobel ì ìš©
            edge_x = F.conv2d(gray, sobel_x.unsqueeze(0), padding=1)
            edge_y = F.conv2d(gray, sobel_y.unsqueeze(0), padding=1)
            
            # ì—£ì§€ í¬ê¸°
            edge_magnitude = torch.sqrt(edge_x ** 2 + edge_y ** 2)
            
            return torch.mean(edge_magnitude)
            
        except Exception as e:
            logger.warning(f"ì—£ì§€ ê°•ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return torch.tensor(0.0)
    
    def _update_processing_stats(self, processing_time: float, quality_score: float):
        """ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.processing_stats['total_images'] += 1
            
            # í‰ê·  ì‹œê°„ ì—…ë°ì´íŠ¸
            total_images = self.processing_stats['total_images']
            old_avg_time = self.processing_stats['average_time']
            self.processing_stats['average_time'] = (
                (old_avg_time * (total_images - 1) + processing_time) / total_images
            )
            
            # ì„±ê³µë¥  ì—…ë°ì´íŠ¸ (í’ˆì§ˆ ì ìˆ˜ 0.6 ì´ìƒì„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼)
            success_count = self.processing_stats.get('success_count', 0)
            if quality_score >= 0.6:
                success_count += 1
            
            self.processing_stats['success_count'] = success_count
            self.processing_stats['enhancement_success_rate'] = success_count / total_images
            
        except Exception as e:
            logger.warning(f"í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    async def get_processing_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        return {
            **self.processing_stats,
            'device': self.device,
            'models_loaded': {
                'real_esrgan': self.real_esrgan is not None,
                'gfpgan': self.gfpgan is not None,
                'codeformer': self.codeformer is not None,
                'color_enhancer': self.color_enhancer is not None,
                'noise_reducer': self.noise_reducer is not None,
                'edge_enhancer': self.edge_enhancer is not None
            },
            'is_initialized': self.is_initialized
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ëª¨ë¸ ì •ë¦¬
            if self.real_esrgan and hasattr(self.real_esrgan, 'cleanup'):
                await self.real_esrgan.cleanup()
            
            if self.gfpgan and hasattr(self.gfpgan, 'cleanup'):
                await self.gfpgan.cleanup()
            
            if self.codeformer and hasattr(self.codeformer, 'cleanup'):
                await self.codeformer.cleanup()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.memory_manager:
                await self.memory_manager.cleanup()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.use_mps:
                torch.mps.empty_cache()
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self.is_initialized = False
            logger.info("âœ… Step 7 í›„ì²˜ë¦¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


# === í´ë°± í´ë˜ìŠ¤ë“¤ ===

class BasicUpscaler:
    """ê¸°ë³¸ ì—…ìŠ¤ì¼€ì¼ëŸ¬ (Real-ESRGAN í´ë°±)"""
    
    def __init__(self, scale_factor: int = 2):
        self.scale_factor = scale_factor
    
    def upscale(self, image: torch.Tensor) -> torch.Tensor:
        """ê¸°ë³¸ ë°”ì´íë¹… ì—…ìŠ¤ì¼€ì¼ë§"""
        try:
            b, c, h, w = image.shape
            new_h, new_w = h * self.scale_factor, w * self.scale_factor
            
            upscaled = F.interpolate(
                image, 
                size=(new_h, new_w), 
                mode='bicubic', 
                align_corners=False
            )
            
            return upscaled
            
        except Exception as e:
            logger.warning(f"ê¸°ë³¸ ì—…ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨: {e}")
            return image


class BasicFaceEnhancer:
    """ê¸°ë³¸ ì–¼êµ´ í–¥ìƒê¸° (GFPGAN í´ë°±)"""
    
    def process(self, image: torch.Tensor) -> torch.Tensor:
        """ê¸°ë³¸ ì–¼êµ´ í–¥ìƒ"""
        try:
            # ê°„ë‹¨í•œ ì„ ëª…í™” ì ìš©
            kernel = torch.tensor([[[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]]], 
                                dtype=torch.float32, device=image.device) / 8.0
            
            if image.shape[1] == 3:
                enhanced = F.conv2d(image, kernel.repeat(3, 1, 1, 1), padding=1, groups=3)
            else:
                enhanced = F.conv2d(image, kernel, padding=1)
            
            return torch.clamp(enhanced, 0.0, 1.0)
            
        except Exception as e:
            logger.warning(f"ê¸°ë³¸ ì–¼êµ´ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image


class BasicImageRestorer:
    """ê¸°ë³¸ ì´ë¯¸ì§€ ë³µì›ê¸° (CodeFormer í´ë°±)"""
    
    def process(self, image: torch.Tensor) -> torch.Tensor:
        """ê¸°ë³¸ ì´ë¯¸ì§€ ë³µì›"""
        try:
            # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì œê±°
            kernel_size = 3
            sigma = 0.5
            
            # ê°€ìš°ì‹œì•ˆ ì»¤ë„ ìƒì„±
            coords = torch.arange(kernel_size, dtype=torch.float32, device=image.device)
            coords -= kernel_size // 2
            g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))
            g /= g.sum()
            
            kernel = g[:, None] * g[None, :]
            kernel = kernel.expand(image.shape[1], 1, kernel_size, kernel_size)
            
            # ì ìš©
            restored = F.conv2d(image, kernel, padding=kernel_size//2, groups=image.shape[1])
            
            return restored
            
        except Exception as e:
            logger.warning(f"ê¸°ë³¸ ì´ë¯¸ì§€ ë³µì› ì‹¤íŒ¨: {e}")
            return image


class ColorEnhancer:
    """ìƒ‰ìƒ í–¥ìƒê¸°"""
    
    def correct_colors(self, image: torch.Tensor) -> torch.Tensor:
        """ìƒ‰ìƒ ë³´ì •"""
        try:
            # ê°„ë‹¨í•œ ìƒ‰ìƒ ê· í˜• ì¡°ì •
            r_mean = torch.mean(image[:, 0])
            g_mean = torch.mean(image[:, 1])
            b_mean = torch.mean(image[:, 2])
            
            overall_mean = (r_mean + g_mean + b_mean) / 3.0
            
            # ê° ì±„ë„ ì¡°ì •
            r_factor = overall_mean / (r_mean + 1e-8)
            g_factor = overall_mean / (g_mean + 1e-8)
            b_factor = overall_mean / (b_mean + 1e-8)
            
            # ë¶€ë“œëŸ¬ìš´ ì¡°ì • (ë„ˆë¬´ ê¸‰ê²©í•˜ì§€ ì•Šê²Œ)
            r_factor = 1.0 + 0.1 * (r_factor - 1.0)
            g_factor = 1.0 + 0.1 * (g_factor - 1.0)
            b_factor = 1.0 + 0.1 * (b_factor - 1.0)
            
            corrected = image.clone()
            corrected[:, 0] *= r_factor
            corrected[:, 1] *= g_factor
            corrected[:, 2] *= b_factor
            
            return torch.clamp(corrected, 0.0, 1.0)
            
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
            return image


class NoiseReducer:
    """ë…¸ì´ì¦ˆ ì œê±°ê¸°"""
    
    def reduce_noise(self, image: torch.Tensor) -> torch.Tensor:
        """ë…¸ì´ì¦ˆ ì œê±°"""
        try:
            # ì–‘ë°©í–¥ í•„í„° ê·¼ì‚¬ (ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬)
            kernel_size = 5
            sigma = 1.0
            
            # ê°€ìš°ì‹œì•ˆ ì»¤ë„ ìƒì„±
            coords = torch.arange(kernel_size, dtype=torch.float32, device=image.device)
            coords -= kernel_size // 2
            g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))
            g /= g.sum()
            
            kernel = g[:, None] * g[None, :]
            kernel = kernel.expand(image.shape[1], 1, kernel_size, kernel_size)
            
            # ë…¸ì´ì¦ˆ ì œê±°
            denoised = F.conv2d(image, kernel, padding=kernel_size//2, groups=image.shape[1])
            
            # ì›ë³¸ê³¼ ë¸”ë Œë”© (ë””í…Œì¼ ë³´ì¡´)
            blending_factor = 0.7
            result = blending_factor * denoised + (1 - blending_factor) * image
            
            return result
            
        except Exception as e:
            logger.warning(f"ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            return image


class EdgeEnhancer:
    """ì—£ì§€ í–¥ìƒê¸°"""
    
    def enhance_edges(self, image: torch.Tensor) -> torch.Tensor:
        """ì—£ì§€ í–¥ìƒ"""
        try:
            # ì–¸ìƒµ ë§ˆìŠ¤í¬ ì ìš©
            # 1. ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
            kernel_size = 5
            sigma = 1.0
            
            coords = torch.arange(kernel_size, dtype=torch.float32, device=image.device)
            coords -= kernel_size // 2
            g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))
            g /= g.sum()
            
            kernel = g[:, None] * g[None, :]
            kernel = kernel.expand(image.shape[1], 1, kernel_size, kernel_size)
            
            blurred = F.conv2d(image, kernel, padding=kernel_size//2, groups=image.shape[1])
            
            # 2. ì–¸ìƒµ ë§ˆìŠ¤í¬
            strength = 0.3
            enhanced = image + strength * (image - blurred)
            
            return torch.clamp(enhanced, 0.0, 1.0)
            
        except Exception as e:
            logger.warning(f"ì—£ì§€ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image


# === í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ===
async def test_post_processing():
    """í›„ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ¨ Step 7 í›„ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # 1. í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    post_processor = PostProcessingStep(
        device='cpu',  # ë˜ëŠ” 'mps'
        config={
            'post_processing': {
                'super_resolution': True,
                'face_enhancement': True,
                'image_restoration': True,
                'color_correction': True,
                'noise_reduction': True,
                'edge_enhancement': True,
                'quality_level': 'high'
            }
        }
    )
    
    # 2. ì´ˆê¸°í™”
    success = await post_processor.initialize()
    if not success:
        print("âŒ í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    print("âœ… í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # 3. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
    print("ğŸ“ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
    test_image = torch.randn(1, 3, 256, 256)  # ë”ë¯¸ ì´ë¯¸ì§€
    
    # 4. í›„ì²˜ë¦¬ ì‹¤í–‰
    print("ğŸš€ í›„ì²˜ë¦¬ ì‹œì‘...")
    
    result = await post_processor.process(
        input_image=test_image,
        enhancement_options={
            'super_resolution': True,
            'face_enhancement': False,  # ì–¼êµ´ì´ ì—†ëŠ” ë”ë¯¸ ì´ë¯¸ì§€
            'color_correction': True
        },
        quality_target=0.8
    )
    
    # 5. ê²°ê³¼ ì¶œë ¥
    if result['success']:
        print("\n" + "="*50)
        print("ğŸ¨ í›„ì²˜ë¦¬ ê²°ê³¼")
        print("="*50)
        
        print(f"âœ… ì²˜ë¦¬ ì„±ê³µ!")
        print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
        print(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {result['quality_score']:.3f}")
        print(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {'ì˜ˆ' if result['target_achieved'] else 'ì•„ë‹ˆì˜¤'}")
        print(f"ğŸ”§ ì‚¬ìš©ëœ ë””ë°”ì´ìŠ¤: {result['device_used']}")
        
        print(f"\nğŸ”„ ì ìš©ëœ í–¥ìƒ:")
        for enhancement in result['applied_enhancements']:
            print(f"  â€¢ {enhancement.replace('_', ' ').title()}")
        
        print(f"\nğŸ“‹ ì„¸ë¶€ ë¡œê·¸:")
        for i, log_entry in enumerate(result['enhancement_log'], 1):
            step = log_entry['step'].replace('_', ' ').title()
            metrics = log_entry['metrics']
            
            print(f"  {i}. {step}:")
            if 'processing_time' in metrics:
                print(f"     - ì²˜ë¦¬ ì‹œê°„: {metrics['processing_time']:.3f}ì´ˆ")
            
            for key, value in metrics.items():
                if key != 'processing_time' and not key.startswith('error'):
                    if isinstance(value, (int, float)):
                        print(f"     - {key.replace('_', ' ').title()}: {value:.3f}")
                    else:
                        print(f"     - {key.replace('_', ' ').title()}: {value}")
        
        # 6. í†µê³„ ì •ë³´
        stats = await post_processor.get_processing_stats()
        print(f"\nğŸ“Š ì²˜ë¦¬ í†µê³„:")
        print(f"  â€¢ ì´ ì²˜ë¦¬ ì´ë¯¸ì§€: {stats['total_images']}")
        print(f"  â€¢ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['average_time']:.2f}ì´ˆ")
        print(f"  â€¢ í–¥ìƒ ì„±ê³µë¥ : {stats['enhancement_success_rate']:.1%}")
        
        print(f"\nğŸ¤– ë¡œë“œëœ ëª¨ë¸:")
        for model_name, loaded in stats['models_loaded'].items():
            status = "âœ…" if loaded else "âŒ"
            print(f"  {status} {model_name.replace('_', ' ').title()}")
        
    else:
        print(f"âŒ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
    
    # 7. ì •ë¦¬
    await post_processor.cleanup()
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    print("ğŸ¨ ì‹¤ì œ í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_post_processing())