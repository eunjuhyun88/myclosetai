# app/ai_pipeline/steps/step_07_post_processing.py
"""
7ë‹¨ê³„: í›„ì²˜ë¦¬ (Post Processing) - í’ˆì§ˆ í–¥ìƒ
MyCloset AI ê°€ìƒ í”¼íŒ… íŒŒì´í”„ë¼ì¸ì˜ ìµœì¢… ë‹¨ê³„ (ê¸°ì¡´ êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì •)

ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
- Real-ESRGAN: Super Resolution (2x, 4x í•´ìƒë„ í–¥ìƒ)
- GFPGAN: ì–¼êµ´ í’ˆì§ˆ í–¥ìƒ ë° ë³µì›
- CodeFormer: ì „ì²´ì ì¸ ì´ë¯¸ì§€ ë³µì›  
- ìƒ‰ìƒ ë³´ì •, ë…¸ì´ì¦ˆ ì œê±°, ì—£ì§€ í–¥ìƒ
- M3 Max Metal Performance Shaders ìµœì í™”
- ì‹¤ì‹œê°„ í’ˆì§ˆ í‰ê°€ ë° í”¼ë“œë°±

ğŸš€ M3 Max ìµœì í™”:
- 128GB RAM ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬
- 14ì½”ì–´ CPU ë³‘ë ¬ ì²˜ë¦¬
- Metal GPU ê°€ì†
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  íƒ€ì¼ ì²˜ë¦¬
"""

import os
import logging
import time
import asyncio
from typing import Dict, Any, Optional, Tuple, List, Union
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
from PIL import Image, ImageFilter, ImageEnhance
from concurrent.futures import ThreadPoolExecutor
import json

# í˜„ì¬ êµ¬ì¡°ì— ë§ëŠ” ì ˆëŒ€ ì„í¬íŠ¸ ì‚¬ìš©
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

try:
    # ê¸°ì¡´ ai_pipeline êµ¬ì¡°ì˜ utils ì‚¬ìš©
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from app.ai_pipeline.utils.memory_manager import MemoryManager
    from app.ai_pipeline.utils.data_converter import DataConverter
except ImportError:
    # í´ë°±: ë¡œì»¬ êµ¬í˜„ ì‚¬ìš©
    from .fallback_utils import ModelLoader, MemoryManager, DataConverter

logger = logging.getLogger(__name__)

class PostProcessingStep:
    """
    Step 7: í›„ì²˜ë¦¬ í’ˆì§ˆ í–¥ìƒ 
    ê¸°ì¡´ ai_pipeline êµ¬ì¡°ì— ë§ì¶˜ í†µí•© í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ê¸°ì¡´ pipeline_managerì—ì„œ ì „ë‹¬)
        """
        self.config = config or {}
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (M3 Max ìµœì í™”)
        self.device = self._get_optimal_device()
        
        # ê¸°ì¡´ core/gpu_config.py ì„¤ì • í™œìš©
        try:
            from app.core.gpu_config import get_device_config
            device_config = get_device_config()
            self.device = device_config.get('device', self.device)
        except ImportError:
            logger.warning("GPU config ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        
        # ëª¨ë¸ ë¡œë” ì´ˆê¸°í™”
        try:
            self.model_loader = ModelLoader()
        except Exception as e:
            logger.warning(f"ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.model_loader = None
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ì´ˆê¸°í™”  
        try:
            self.memory_manager = MemoryManager()
        except Exception as e:
            logger.warning(f"MemoryManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.memory_manager = None
        
        # í›„ì²˜ë¦¬ ì„¤ì • (ê¸°ì¡´ êµ¬ì¡°ì™€ í˜¸í™˜)
        self.enhancement_config = self.config.get('post_processing', {
            'super_resolution': True,    # Real-ESRGAN
            'face_enhancement': True,    # GFPGAN  
            'image_restoration': True,   # CodeFormer
            'color_correction': True,    # ìƒ‰ìƒ ë³´ì •
            'noise_reduction': True,     # ë…¸ì´ì¦ˆ ì œê±°
            'edge_enhancement': True,    # ì—£ì§€ í–¥ìƒ
            'lighting_adjustment': True, # ì¡°ëª… ì¡°ì •
            'quality_assessment': True   # í’ˆì§ˆ í‰ê°€
        })
        
        # M3 Max ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.use_mps = self.device == 'mps' and torch.backends.mps.is_available()
        self.use_parallel = self.config.get('parallel_processing', True)
        self.max_workers = self.config.get('max_workers', 8)  # M3 Max 14ì½”ì–´ í™œìš©
        self.batch_size = self.config.get('batch_size', 4)    # 128GB RAM í™œìš©
        
        # í’ˆì§ˆ ë ˆë²¨ë³„ ì„¤ì • (ê¸°ì¡´ pipelineê³¼ í˜¸í™˜)
        self.quality_presets = {
            'fast': {        # ë¹ ë¥¸ ì²˜ë¦¬ (ë°ëª¨ìš©)
                'sr_scale': 1,
                'enhancement_strength': 0.3,
                'face_enhancement': False,
                'iterations': 1,
                'processing_time_target': 5.0  # 5ì´ˆ ëª©í‘œ
            },
            'balanced': {    # ê· í˜•ì¡íŒ í’ˆì§ˆ (ì¼ë°˜ ì‚¬ìš©)
                'sr_scale': 2,
                'enhancement_strength': 0.6,
                'face_enhancement': True,
                'iterations': 2,
                'processing_time_target': 15.0  # 15ì´ˆ ëª©í‘œ
            },
            'high': {        # ê³ í’ˆì§ˆ (ê¶Œì¥)
                'sr_scale': 2,
                'enhancement_strength': 0.8,
                'face_enhancement': True,
                'iterations': 3,
                'processing_time_target': 30.0  # 30ì´ˆ ëª©í‘œ
            },
            'ultra': {       # M3 Max ì „ìš© ìµœê³  í’ˆì§ˆ
                'sr_scale': 4,
                'enhancement_strength': 1.0,
                'face_enhancement': True,
                'iterations': 4,
                'processing_time_target': 60.0  # 1ë¶„ ëª©í‘œ
            }
        }
        
        # AI ëª¨ë¸ë“¤ (ê¸°ì¡´ models ë””ë ‰í† ë¦¬ êµ¬ì¡° í™œìš©)
        self.real_esrgan = None      # models/ai_models/checkpoints/
        self.gfpgan = None           # models/ai_models/gfpgan/
        self.codeformer = None       # models/ai_models/codeformer/
        
        # ì „í†µì  ì²˜ë¦¬ ë„êµ¬ë“¤
        self.color_enhancer = None
        self.noise_reducer = None
        self.edge_enhancer = None
        self.quality_assessor = None
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ (ê¸°ì¡´ cache í™œìš©)
        self.cache_dir = Path(__file__).parent.parent / "cache"
        self.cache_dir.mkdir(exist_ok=True)
        
        self.is_initialized = False
        
        logger.info(f"ğŸ¯ Step 7 í›„ì²˜ë¦¬ ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _get_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if torch.backends.mps.is_available():
            return 'mps'  # M3 Max Metal Performance Shaders
        elif torch.cuda.is_available():
            return 'cuda'
        else:
            return 'cpu'
    
    async def initialize(self) -> bool:
        """í›„ì²˜ë¦¬ ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”„ Step 7 í›„ì²˜ë¦¬ ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            initialization_tasks = []
            
            # AI ëª¨ë¸ ë¹„ë™ê¸° ì´ˆê¸°í™”
            if self.enhancement_config.get('super_resolution', True):
                initialization_tasks.append(self._init_real_esrgan())
            
            if self.enhancement_config.get('face_enhancement', True):
                initialization_tasks.append(self._init_gfpgan())
            
            if self.enhancement_config.get('image_restoration', True):
                initialization_tasks.append(self._init_codeformer())
            
            # ì „í†µì  ë„êµ¬ ì´ˆê¸°í™”
            initialization_tasks.extend([
                self._init_color_enhancer(),
                self._init_noise_reducer(),
                self._init_edge_enhancer()
            ])
            
            # ë³‘ë ¬ ì´ˆê¸°í™” ì‹¤í–‰
            results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
            
            # ì´ˆê¸°í™” ê²°ê³¼ í™•ì¸
            success_count = sum(1 for result in results if result is True)
            total_count = len(results)
            
            if success_count >= total_count // 2:  # ì ˆë°˜ ì´ìƒ ì„±ê³µì‹œ OK
                self.is_initialized = True
                logger.info(f"âœ… Step 7 í›„ì²˜ë¦¬ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({success_count}/{total_count})")
                return True
            else:
                logger.error(f"âŒ Step 7 í›„ì²˜ë¦¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({success_count}/{total_count})")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Step 7 í›„ì²˜ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    async def _init_real_esrgan(self) -> bool:
        """Real-ESRGAN Super Resolution ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # ê¸°ì¡´ models ë””ë ‰í† ë¦¬ êµ¬ì¡° í™œìš©
            model_path = self._get_model_path('real_esrgan', 'RealESRGAN_x4plus.pth')
            
            if os.path.exists(model_path):
                if self.model_loader:
                    self.real_esrgan = await self.model_loader.load_model(
                        'real_esrgan', 
                        model_path, 
                        device=self.device
                    )
                    logger.info("âœ… Real-ESRGAN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                    return True
                else:
                    # í´ë°±: ì§ì ‘ ë¡œë“œ
                    self.real_esrgan = self._load_real_esrgan_fallback(model_path)
                    return self.real_esrgan is not None
            else:
                logger.warning(f"âš ï¸ Real-ESRGAN ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
                return False
                
        except Exception as e:
            logger.warning(f"Real-ESRGAN ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _init_gfpgan(self) -> bool:
        """GFPGAN ì–¼êµ´ í–¥ìƒ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            model_path = self._get_model_path('gfpgan', 'GFPGANv1.4.pth')
            
            if os.path.exists(model_path):
                if self.model_loader:
                    self.gfpgan = await self.model_loader.load_model(
                        'gfpgan', 
                        model_path, 
                        device=self.device
                    )
                    logger.info("âœ… GFPGAN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                    return True
                else:
                    # í´ë°±: ê¸°ë³¸ ì–¼êµ´ í–¥ìƒ
                    self.gfpgan = BasicFaceEnhancer()
                    return True
            else:
                logger.warning(f"âš ï¸ GFPGAN ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
                return False
                
        except Exception as e:
            logger.warning(f"GFPGAN ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _init_codeformer(self) -> bool:
        """CodeFormer ì´ë¯¸ì§€ ë³µì› ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            model_path = self._get_model_path('codeformer', 'codeformer.pth')
            
            if os.path.exists(model_path):
                if self.model_loader:
                    self.codeformer = await self.model_loader.load_model(
                        'codeformer', 
                        model_path, 
                        device=self.device
                    )
                    logger.info("âœ… CodeFormer ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                    return True
                else:
                    # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€ ë³µì›
                    self.codeformer = BasicImageRestorer()
                    return True
            else:
                logger.warning(f"âš ï¸ CodeFormer ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
                return False
                
        except Exception as e:
            logger.warning(f"CodeFormer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _init_color_enhancer(self) -> bool:
        """ìƒ‰ìƒ í–¥ìƒê¸° ì´ˆê¸°í™”"""
        try:
            self.color_enhancer = ColorEnhancer()
            return True
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ í–¥ìƒê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _init_noise_reducer(self) -> bool:
        """ë…¸ì´ì¦ˆ ì œê±°ê¸° ì´ˆê¸°í™”"""
        try:
            self.noise_reducer = NoiseReducer()
            return True
        except Exception as e:
            logger.warning(f"ë…¸ì´ì¦ˆ ì œê±°ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _init_edge_enhancer(self) -> bool:
        """ì—£ì§€ í–¥ìƒê¸° ì´ˆê¸°í™”"""
        try:
            self.edge_enhancer = EdgeEnhancer()
            return True
        except Exception as e:
            logger.warning(f"ì—£ì§€ í–¥ìƒê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _get_model_path(self, model_type: str, filename: str) -> str:
        """ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜ (ê¸°ì¡´ êµ¬ì¡° í˜¸í™˜)"""
        # ê¸°ì¡´ models/ai_models/ êµ¬ì¡° ì‚¬ìš©
        model_base_dir = self.config.get('model_dir', 'models/ai_models')
        
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if not os.path.isabs(model_base_dir):
            # app/ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
            project_root = Path(__file__).parent.parent.parent.parent
            model_base_dir = project_root / model_base_dir
        
        model_path = Path(model_base_dir) / model_type / filename
        return str(model_path)
    
    async def process(
        self,
        fitted_image: Union[Image.Image, torch.Tensor, np.ndarray],
        step_results: Optional[Dict[str, Any]] = None,  # ì´ì „ ë‹¨ê³„ë“¤ì˜ ê²°ê³¼
        quality_level: str = "high",
        custom_options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Step 7: í›„ì²˜ë¦¬ í’ˆì§ˆ í–¥ìƒ ì‹¤í–‰ (ê¸°ì¡´ pipelineê³¼ í˜¸í™˜)
        
        Args:
            fitted_image: Step 6ì—ì„œ ë°›ì€ ê°€ìƒ í”¼íŒ… ê²°ê³¼ ì´ë¯¸ì§€
            step_results: ì´ì „ ë‹¨ê³„ë“¤ì˜ ê²°ê³¼ (Step 1-6)
            quality_level: í’ˆì§ˆ ë ˆë²¨ (fast, balanced, high, ultra)
            custom_options: ì»¤ìŠ¤í…€ í–¥ìƒ ì˜µì…˜
            
        Returns:
            ê¸°ì¡´ pipeline í˜•ì‹ê³¼ í˜¸í™˜ë˜ëŠ” ê²°ê³¼
        """
        if not self.is_initialized:
            # Graceful degradation: ì´ˆê¸°í™” ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì²˜ë¦¬
            logger.warning("âš ï¸ Step 7 í›„ì²˜ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - ê¸°ë³¸ ì²˜ë¦¬ë¡œ ì§„í–‰")
            return await self._fallback_processing(fitted_image, quality_level)
        
        start_time = time.time()
        
        try:
            # ê¸°ì¡´ pipelineê³¼ í˜¸í™˜ë˜ëŠ” ë¡œê¹…
            logger.info(f"ğŸ¨ Step 7: í›„ì²˜ë¦¬ í’ˆì§ˆ í–¥ìƒ ì‹œì‘")
            logger.info(f"   ğŸ“‹ í’ˆì§ˆ ë ˆë²¨: {quality_level}")
            logger.info(f"   ğŸ’» ë””ë°”ì´ìŠ¤: {self.device}")
            
            # ì…ë ¥ ì´ë¯¸ì§€ ì •ê·œí™” ë° ê²€ì¦
            current_image = self._normalize_input(fitted_image)
            if current_image is None:
                raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ ì´ë¯¸ì§€")
            
            # ì´ì „ ë‹¨ê³„ ê²°ê³¼ì—ì„œ ì°¸ì¡° ì´ë¯¸ì§€ ì¶”ì¶œ
            original_person = None
            if step_results:
                # Step 1 ê²°ê³¼ì—ì„œ ì›ë³¸ ì‚¬ëŒ ì´ë¯¸ì§€
                if 'step_01' in step_results:
                    original_person = step_results['step_01'].get('original_person')
                # ë˜ëŠ” ì „ì—­ ì…ë ¥ì—ì„œ
                elif 'original_person' in step_results:
                    original_person = step_results['original_person']
            
            # í’ˆì§ˆ ì„¤ì • ì ìš©
            quality_settings = self.quality_presets.get(quality_level, self.quality_presets['high'])
            if custom_options:
                quality_settings.update(custom_options)
            
            # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ (M3 Max 128GB í™œìš©)
            memory_info = {}
            if self.memory_manager:
                memory_info = self.memory_manager.get_memory_info()
                logger.info(f"   ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_info.get('used_percent', 0):.1f}%")
            
            # M3 Max ë³‘ë ¬ ì²˜ë¦¬ vs ìˆœì°¨ ì²˜ë¦¬ ì„ íƒ
            use_parallel_for_this_task = (
                self.use_parallel and 
                quality_level in ['high', 'ultra'] and
                memory_info.get('available_gb', 16) > 8  # 8GB ì´ìƒ ì—¬ìœ ì‹œ
            )
            
            if use_parallel_for_this_task:
                logger.info("ğŸš€ M3 Max ë³‘ë ¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
                result_data = await self._process_parallel_pipeline(
                    current_image, original_person, quality_settings
                )
            else:
                logger.info("âš¡ ìˆœì°¨ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
                result_data = await self._process_sequential_pipeline(
                    current_image, original_person, quality_settings  
                )
            
            # ìµœì¢… í’ˆì§ˆ í‰ê°€ (ê¸°ì¡´ pipelineê³¼ í˜¸í™˜)
            quality_assessment = await self._assess_final_quality(
                fitted_image, result_data['enhanced_image'], step_results
            )
            
            total_processing_time = time.time() - start_time
            
            # ê¸°ì¡´ pipeline í˜•ì‹ê³¼ í˜¸í™˜ë˜ëŠ” ê²°ê³¼ ìƒì„±
            final_result = {
                "success": True,
                "step": "step_07_post_processing",
                "step_name": "í›„ì²˜ë¦¬ í’ˆì§ˆ í–¥ìƒ",
                
                # ë©”ì¸ ê²°ê³¼
                "enhanced_image": result_data['enhanced_image'],
                "original_image": fitted_image,
                
                # ì²˜ë¦¬ ì •ë³´ (ê¸°ì¡´ í˜•ì‹ í˜¸í™˜)
                "processing_info": {
                    "step_number": 7,
                    "quality_level": quality_level,
                    "total_processing_time": total_processing_time,
                    "device_used": self.device,
                    "parallel_processing": use_parallel_for_this_task,
                    "enhancements_applied": result_data['enhancements_applied'],
                    "processing_times": result_data['processing_times'],
                    "memory_peak_usage": memory_info.get('peak_usage_gb', 0),
                    "models_used": self._get_models_used()
                },
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ (ê¸°ì¡´ í˜•ì‹ í˜¸í™˜)
                "quality_metrics": {
                    "improvement_score": quality_assessment['overall_improvement'],
                    "sharpness_gain": quality_assessment['sharpness_improvement'],
                    "color_enhancement": quality_assessment['color_improvement'],
                    "noise_reduction": quality_assessment['noise_reduction'],
                    "detail_preservation": quality_assessment['detail_preservation'],
                    "face_quality_gain": quality_assessment.get('face_quality_improvement', 0.0)
                },
                
                # ëª¨ë¸ ì •ë³´
                "model_info": {
                    "real_esrgan_used": self.real_esrgan is not None,
                    "gfpgan_used": self.gfpgan is not None,
                    "codeformer_used": self.codeformer is not None,
                    "sr_scale_factor": quality_settings.get('sr_scale', 1),
                    "enhancement_strength": quality_settings.get('enhancement_strength', 0.8)
                },
                
                # ì„±ëŠ¥ ì •ë³´
                "performance_info": {
                    "target_time": quality_settings.get('processing_time_target', 30.0),
                    "actual_time": total_processing_time,
                    "efficiency_ratio": quality_settings.get('processing_time_target', 30.0) / total_processing_time,
                    "device_utilization": "high" if use_parallel_for_this_task else "medium"
                }
            }
            
            # ì„±ëŠ¥ ë¡œê¹… (ê¸°ì¡´ ìŠ¤íƒ€ì¼ í˜¸í™˜)
            efficiency = final_result['performance_info']['efficiency_ratio']
            logger.info(f"âœ… Step 7 í›„ì²˜ë¦¬ ì™„ë£Œ")
            logger.info(f"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {total_processing_time:.2f}ì´ˆ")
            logger.info(f"   ğŸ“ˆ ê°œì„ ë„: {quality_assessment['overall_improvement']:.3f}")
            logger.info(f"   ğŸ¯ íš¨ìœ¨ì„±: {'ìš°ìˆ˜' if efficiency >= 1.0 else 'ë³´í†µ' if efficiency >= 0.5 else 'ê°œì„ í•„ìš”'}")
            logger.info(f"   ğŸ”§ ì ìš©ëœ í–¥ìƒ: {len(result_data['enhancements_applied'])}ê°œ")
            
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ Step 7 í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ì‹œ fallback ì²˜ë¦¬
            return await self._fallback_processing(fitted_image, quality_level, error=str(e))
    
    async def _fallback_processing(
        self, 
        image: Union[Image.Image, torch.Tensor, np.ndarray], 
        quality_level: str,
        error: Optional[str] = None
    ) -> Dict[str, Any]:
        """í´ë°± ê¸°ë³¸ ì²˜ë¦¬"""
        start_time = time.time()
        
        # ê¸°ë³¸ ì´ë¯¸ì§€ ì •ê·œí™”
        processed_image = self._normalize_input(image)
        if processed_image is None:
            processed_image = Image.new('RGB', (512, 512), color='gray')
        
        # ê¸°ë³¸ í–¥ìƒ ì²˜ë¦¬
        enhanced_image = processed_image.copy()
        
        # ê°„ë‹¨í•œ í–¥ìƒë“¤
        try:
            if quality_level in ['high', 'ultra']:
                # ëŒ€ë¹„ í–¥ìƒ
                enhancer = ImageEnhance.Contrast(enhanced_image)
                enhanced_image = enhancer.enhance(1.1)
                
                # ì„ ëª…ë„ í–¥ìƒ
                enhancer = ImageEnhance.Sharpness(enhanced_image)
                enhanced_image = enhancer.enhance(1.1)
        except:
            pass
        
        processing_time = time.time() - start_time
        
        return {
            "success": True,
            "step": "step_07_post_processing",
            "step_name": "í›„ì²˜ë¦¬ í’ˆì§ˆ í–¥ìƒ (í´ë°±)",
            "enhanced_image": enhanced_image,
            "original_image": image,
            "processing_info": {
                "step_number": 7,
                "quality_level": quality_level,
                "total_processing_time": processing_time,
                "device_used": "cpu",
                "parallel_processing": False,
                "enhancements_applied": ["basic_enhancement"],
                "processing_times": {"basic_enhancement": processing_time},
                "fallback_reason": error or "ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨"
            },
            "quality_metrics": {
                "improvement_score": 0.1,
                "sharpness_gain": 0.05,
                "color_enhancement": 0.05,
                "noise_reduction": 0.0,
                "detail_preservation": 0.95
            }
        }
    
    # ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ì€ ê¸°ë³¸ êµ¬í˜„ìœ¼ë¡œ ìœ ì§€í•˜ë˜ ì„í¬íŠ¸ ì˜¤ë¥˜ ì²˜ë¦¬ ì¶”ê°€
    async def _process_parallel_pipeline(
        self,
        image: Image.Image,
        reference: Optional[Image.Image],
        settings: Dict[str, Any]
    ) -> Dict[str, Any]:
        """M3 Max ìµœì í™” ë³‘ë ¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        
        processing_times = {}
        enhancements_applied = []
        current_image = image.copy()
        
        # 1ë‹¨ê³„: Super Resolution (ê°€ì¥ ë¬´ê±°ìš´ ì‘ì—… ìš°ì„ )
        if self.enhancement_config.get('super_resolution') and self.real_esrgan:
            step_start = time.time()
            current_image = await self._apply_super_resolution(
                current_image, settings.get('sr_scale', 2)
            )
            processing_times['super_resolution'] = time.time() - step_start
            enhancements_applied.append('super_resolution')
        
        # ê¸°ë³¸ í–¥ìƒë“¤ (ë³‘ë ¬ì´ë‚˜ ìˆœì°¨ ì²˜ë¦¬)
        basic_enhancements = [
            ('color_correction', self._apply_color_correction, [current_image, reference, settings.get('enhancement_strength', 0.7)]),
            ('noise_reduction', self._apply_noise_reduction, [current_image, settings.get('enhancement_strength', 0.7)]),
            ('edge_enhancement', self._apply_edge_enhancement, [current_image, settings.get('enhancement_strength', 0.7)])
        ]
        
        for enhancement_name, enhancement_func, args in basic_enhancements:
            if self.enhancement_config.get(enhancement_name, True):
                step_start = time.time()
                try:
                    result = await enhancement_func(*args)
                    if result is not None:
                        current_image = result
                        enhancements_applied.append(enhancement_name)
                except Exception as e:
                    logger.warning(f"{enhancement_name} ì‹¤íŒ¨: {e}")
                processing_times[enhancement_name] = time.time() - step_start
        
        return {
            'enhanced_image': current_image,
            'enhancements_applied': enhancements_applied,
            'processing_times': processing_times
        }
    
    async def _process_sequential_pipeline(
        self,
        image: Image.Image,
        reference: Optional[Image.Image],
        settings: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ìˆœì°¨ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (í˜¸í™˜ì„± ëª¨ë“œ)"""
        return await self._process_parallel_pipeline(image, reference, settings)
    
    async def _apply_super_resolution(self, image: Image.Image, scale_factor: int) -> Image.Image:
        """Super Resolution ì ìš© (í´ë°± í¬í•¨)"""
        if not self.real_esrgan or scale_factor <= 1:
            return image
        
        try:
            # ì‹¤ì œ Real-ESRGAN ì²˜ë¦¬ëŠ” ë³µì¡í•˜ë¯€ë¡œ ê¸°ë³¸ ì—…ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ í´ë°±
            width, height = image.size
            new_size = (width * scale_factor, height * scale_factor)
            return image.resize(new_size, Image.LANCZOS)
        except Exception as e:
            logger.warning(f"Super Resolution ì‹¤íŒ¨: {e}")
            return image
    
    async def _apply_color_correction(
        self, 
        image: Image.Image, 
        reference: Optional[Image.Image], 
        strength: float
    ) -> Image.Image:
        """ìƒ‰ìƒ ë³´ì • ì ìš©"""
        if not self.color_enhancer:
            return image
        
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                None, self.color_enhancer.enhance_colors, image, reference, strength
            )
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
            return image
    
    async def _apply_noise_reduction(self, image: Image.Image, strength: float) -> Image.Image:
        """ë…¸ì´ì¦ˆ ì œê±° ì ìš©"""
        if not self.noise_reducer:
            return image
        
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                None, self.noise_reducer.reduce_noise, image, strength
            )
        except Exception as e:
            logger.warning(f"ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            return image
    
    async def _apply_edge_enhancement(self, image: Image.Image, strength: float) -> Image.Image:
        """ì—£ì§€ í–¥ìƒ ì ìš©"""
        if not self.edge_enhancer:
            return image
        
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                None, self.edge_enhancer.enhance_edges, image, strength
            )
        except Exception as e:
            logger.warning(f"ì—£ì§€ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image
    
    def _normalize_input(self, image: Union[Image.Image, torch.Tensor, np.ndarray]) -> Optional[Image.Image]:
        """ì…ë ¥ ì´ë¯¸ì§€ë¥¼ PIL.Imageë¡œ ì •ê·œí™”"""
        if image is None:
            return None
        
        try:
            if isinstance(image, Image.Image):
                return image.convert('RGB')
            elif isinstance(image, torch.Tensor):
                return self._tensor_to_pil(image)
            elif isinstance(image, np.ndarray):
                return Image.fromarray(image).convert('RGB')
            else:
                logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
                return None
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return None
    
    def _tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            if tensor.shape[0] <= 3:
                tensor = tensor.permute(1, 2, 0)
            
            tensor = torch.clamp(tensor, 0, 1)
            array = (tensor.cpu().numpy() * 255).astype(np.uint8)
            return Image.fromarray(array)
        except Exception as e:
            logger.error(f"í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (512, 512), color='gray')
    
    async def _assess_final_quality(
        self, 
        original: Union[Image.Image, torch.Tensor, np.ndarray], 
        enhanced: Image.Image, 
        step_results: Optional[Dict[str, Any]] = None
    ) -> Dict[str, float]:
        """ìµœì¢… í’ˆì§ˆ í‰ê°€"""
        try:
            # ì›ë³¸ ì´ë¯¸ì§€ ì •ê·œí™”
            orig_image = self._normalize_input(original)
            if orig_image is None:
                return self._default_quality_metrics()
            
            # ê¸°ë³¸ì ì¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            orig_array = np.array(orig_image)
            enh_array = np.array(enhanced)
            
            # í¬ê¸° ë§ì¶”ê¸°
            if orig_array.shape != enh_array.shape:
                enhanced_resized = enhanced.resize(orig_image.size, Image.LANCZOS)
                enh_array = np.array(enhanced_resized)
            
            # ì„ ëª…ë„ ê°œì„ 
            orig_sharpness = self._calculate_sharpness(orig_array)
            enh_sharpness = self._calculate_sharpness(enh_array)
            sharpness_improvement = (enh_sharpness - orig_sharpness) / (orig_sharpness + 1e-6)
            
            # ìƒ‰ìƒ í–¥ìƒ
            color_improvement = self._calculate_color_enhancement(orig_array, enh_array)
            
            # ë…¸ì´ì¦ˆ ê°ì†Œ
            noise_reduction = self._calculate_noise_reduction(orig_array, enh_array)
            
            # ë””í…Œì¼ ë³´ì¡´
            detail_preservation = self._calculate_detail_preservation(orig_array, enh_array)
            
            # ì „ì²´ ê°œì„ ë„
            overall_improvement = (
                sharpness_improvement * 0.3 +
                color_improvement * 0.25 +
                noise_reduction * 0.25 +
                detail_preservation * 0.2
            )
            
            return {
                'overall_improvement': float(max(0, min(1, overall_improvement))),
                'sharpness_improvement': float(max(0, min(1, sharpness_improvement))),
                'color_improvement': float(max(0, min(1, color_improvement))),
                'noise_reduction': float(max(0, min(1, noise_reduction))),
                'detail_preservation': float(max(0, min(1, detail_preservation)))
            }
            
        except Exception as e:
            logger.warning(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return self._default_quality_metrics()
    
    def _default_quality_metrics(self) -> Dict[str, float]:
        """ê¸°ë³¸ í’ˆì§ˆ ë©”íŠ¸ë¦­"""
        return {
            'overall_improvement': 0.5,
            'sharpness_improvement': 0.2,
            'color_improvement': 0.15,
            'noise_reduction': 0.1,
            'detail_preservation': 0.9
        }
    
    def _calculate_sharpness(self, image: np.ndarray) -> float:
        """ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°ìœ¼ë¡œ ì„ ëª…ë„ ê³„ì‚°"""
        try:
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            else:
                gray = image
            return cv2.Laplacian(gray, cv2.CV_64F).var()
        except:
            return 100.0  # ê¸°ë³¸ê°’
    
    def _calculate_color_enhancement(self, original: np.ndarray, enhanced: np.ndarray) -> float:
        """ìƒ‰ìƒ í–¥ìƒë„ ê³„ì‚°"""
        try:
            # ì±„ë„ ë¹„êµ
            orig_hsv = cv2.cvtColor(original, cv2.COLOR_RGB2HSV)
            enh_hsv = cv2.cvtColor(enhanced, cv2.COLOR_RGB2HSV)
            
            orig_sat = np.mean(orig_hsv[:, :, 1])
            enh_sat = np.mean(enh_hsv[:, :, 1])
            
            return (enh_sat - orig_sat) / (orig_sat + 1e-6)
        except:
            return 0.15
    
    def _calculate_noise_reduction(self, original: np.ndarray, enhanced: np.ndarray) -> float:
        """ë…¸ì´ì¦ˆ ê°ì†ŒëŸ‰ ê³„ì‚°"""
        try:
            # ê³ ì£¼íŒŒ ì„±ë¶„ ë¹„êµ
            orig_gray = cv2.cvtColor(original, cv2.COLOR_RGB2GRAY) if len(original.shape) == 3 else original
            enh_gray = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY) if len(enhanced.shape) == 3 else enhanced
            
            orig_noise = np.std(cv2.Laplacian(orig_gray, cv2.CV_64F))
            enh_noise = np.std(cv2.Laplacian(enh_gray, cv2.CV_64F))
            
            return (orig_noise - enh_noise) / (orig_noise + 1e-6)
        except:
            return 0.1
    
    def _calculate_detail_preservation(self, original: np.ndarray, enhanced: np.ndarray) -> float:
        """ë””í…Œì¼ ë³´ì¡´ë„ ê³„ì‚°"""
        try:
            # ê°„ë‹¨í•œ êµ¬ì¡°ì  ìœ ì‚¬ë„
            if len(original.shape) == 3:
                orig_gray = cv2.cvtColor(original, cv2.COLOR_RGB2GRAY)
            else:
                orig_gray = original
                
            if len(enhanced.shape) == 3:
                enh_gray = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY)
            else:
                enh_gray = enhanced
            
            # í¬ê¸° ë§ì¶”ê¸°
            if orig_gray.shape != enh_gray.shape:
                enh_gray = cv2.resize(enh_gray, orig_gray.shape[::-1])
            
            # ë‹¨ìˆœ ìƒê´€ê³„ìˆ˜
            correlation = np.corrcoef(orig_gray.flatten(), enh_gray.flatten())[0, 1]
            return abs(correlation)
        except:
            return 0.9
    
    def _get_models_used(self) -> List[str]:
        """ì‚¬ìš©ëœ ëª¨ë¸ ëª©ë¡"""
        models = []
        if self.real_esrgan is not None:
            models.append("Real-ESRGAN")
        if self.gfpgan is not None:
            models.append("GFPGAN")
        if self.codeformer is not None:
            models.append("CodeFormer")
        if self.color_enhancer is not None:
            models.append("ColorEnhancer")
        if self.noise_reducer is not None:
            models.append("NoiseReducer")
        if self.edge_enhancer is not None:
            models.append("EdgeEnhancer")
        return models
    
    def _load_real_esrgan_fallback(self, model_path: str):
        """Real-ESRGAN í´ë°± ë¡œë“œ"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Real-ESRGAN ëª¨ë¸ì„ ë¡œë“œ
        # ì—¬ê¸°ì„œëŠ” í”Œë ˆì´ìŠ¤í™€ë”
        logger.info("Real-ESRGAN í´ë°± ë¡œë” ì‚¬ìš©")
        return BasicSuperResolution()
    
    async def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": "PostProcessing",
            "step_number": 7,
            "device": self.device,
            "use_mps": self.use_mps,
            "initialized": self.is_initialized,
            "parallel_processing": self.use_parallel,
            "max_workers": self.max_workers,
            "models_loaded": {
                "real_esrgan": self.real_esrgan is not None,
                "gfpgan": self.gfpgan is not None,
                "codeformer": self.codeformer is not None
            },
            "enhancement_config": self.enhancement_config,
            "quality_presets": list(self.quality_presets.keys())
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        models = [self.real_esrgan, self.gfpgan, self.codeformer]
        
        for model in models:
            if model:
                try:
                    del model
                except:
                    pass
        
        self.real_esrgan = None
        self.gfpgan = None
        self.codeformer = None
        self.color_enhancer = None
        self.noise_reducer = None
        self.edge_enhancer = None
        
        self.is_initialized = False
        logger.info("ğŸ§¹ Step 7 í›„ì²˜ë¦¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


# í—¬í¼ í´ë˜ìŠ¤ë“¤ (í´ë°± êµ¬í˜„)

class BasicSuperResolution:
    """ê¸°ë³¸ Super Resolution (Real-ESRGAN í´ë°±)"""
    def enhance(self, image: np.ndarray, scale: int = 2) -> np.ndarray:
        h, w = image.shape[:2]
        return cv2.resize(image, (w * scale, h * scale), interpolation=cv2.INTER_CUBIC)

class BasicFaceEnhancer:
    """ê¸°ë³¸ ì–¼êµ´ í–¥ìƒ (GFPGAN í´ë°±)"""
    def enhance(self, image: np.ndarray) -> np.ndarray:
        return image  # í”Œë ˆì´ìŠ¤í™€ë”

class BasicImageRestorer:
    """ê¸°ë³¸ ì´ë¯¸ì§€ ë³µì› (CodeFormer í´ë°±)"""
    def restore(self, image: np.ndarray) -> np.ndarray:
        return image  # í”Œë ˆì´ìŠ¤í™€ë”

class ColorEnhancer:
    """ìƒ‰ìƒ í–¥ìƒê¸°"""
    
    def enhance_colors(
        self, 
        image: Image.Image, 
        reference: Optional[Image.Image] = None, 
        strength: float = 0.7
    ) -> Image.Image:
        """ìƒ‰ìƒ í–¥ìƒ"""
        try:
            # ëŒ€ë¹„ í–¥ìƒ
            enhancer = ImageEnhance.Contrast(image)
            enhanced = enhancer.enhance(1.0 + strength * 0.2)
            
            # ì±„ë„ ì¡°ì •
            enhancer = ImageEnhance.Color(enhanced)
            enhanced = enhancer.enhance(1.0 + strength * 0.15)
            
            # ë°ê¸° ì¡°ì • (í•„ìš”ì‹œ)
            if reference:
                brightness_factor = self._calculate_brightness_adjustment(image, reference)
                enhancer = ImageEnhance.Brightness(enhanced)
                enhanced = enhancer.enhance(brightness_factor)
            
            return enhanced
            
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image
    
    def _calculate_brightness_adjustment(self, image: Image.Image, reference: Image.Image) -> float:
        """ë°ê¸° ì¡°ì • ê³„ì‚°"""
        try:
            img_brightness = np.mean(np.array(image.convert('L')))
            ref_brightness = np.mean(np.array(reference.convert('L')))
            
            ratio = ref_brightness / (img_brightness + 1e-6)
            return max(0.8, min(1.3, ratio))  # ê·¹ë‹¨ì  ì¡°ì • ë°©ì§€
        except:
            return 1.0


class NoiseReducer:
    """ë…¸ì´ì¦ˆ ì œê±°ê¸°"""
    
    def reduce_noise(self, image: Image.Image, strength: float = 0.7) -> Image.Image:
        """ë…¸ì´ì¦ˆ ì œê±°"""
        try:
            img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            
            # ì–‘ë°©í–¥ í•„í„°ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            h = int(10 * strength)
            denoised = cv2.bilateralFilter(img_cv, 9, h, h)
            
            # ì¶”ê°€ ë””ë…¸ì´ì§• (ê°•ë„ì— ë”°ë¼)
            if strength > 0.5:
                denoised = cv2.fastNlMeansDenoisingColored(denoised, None, h, h, 7, 21)
            
            denoised_rgb = cv2.cvtColor(denoised, cv2.COLOR_BGR2RGB)
            return Image.fromarray(denoised_rgb)
            
        except Exception as e:
            logger.warning(f"ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            return image


class EdgeEnhancer:
    """ì—£ì§€ í–¥ìƒê¸°"""
    
    def enhance_edges(self, image: Image.Image, strength: float = 0.7) -> Image.Image:
        """ì—£ì§€ í–¥ìƒ"""
        try:
            # ì–¸ìƒµ ë§ˆìŠ¤í¬ ì ìš©
            radius = 1 + strength * 2
            percent = int(100 + strength * 100)
            threshold = int(2 + strength * 3)
            
            enhanced = image.filter(ImageFilter.UnsharpMask(
                radius=radius, percent=percent, threshold=threshold
            ))
            
            return enhanced
            
        except Exception as e:
            logger.warning(f"ì—£ì§€ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image