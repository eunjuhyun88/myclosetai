"""
ğŸ”¥ Quality Assessment Service
============================

í’ˆì§ˆ í‰ê°€ ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
ë…¼ë¬¸ ê¸°ë°˜ì˜ AI ëª¨ë¸ êµ¬ì¡°ì— ë§ì¶° êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
from typing import Dict, Any, Optional, Union, List, Tuple
import numpy as np
from PIL import Image
import logging
import time
from datetime import datetime

# í”„ë¡œì íŠ¸ ë¡œê¹… ì„¤ì • import
try:
    from backend.app.core.logging_config import get_logger
    logger = get_logger(__name__)
except ImportError:
    logger = logging.getLogger(__name__)

class QualityAssessmentService:
    """
    í’ˆì§ˆ í‰ê°€ ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
    """

    def __init__(self, model_loader=None, processor=None, inference_engine=None):
        """
        Args:
            model_loader: ëª¨ë¸ ë¡œë” ì¸ìŠ¤í„´ìŠ¤
            processor: ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
            inference_engine: ì¶”ë¡  ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
        """
        self.model_loader = model_loader
        self.processor = processor
        self.inference_engine = inference_engine
        
        # ì„œë¹„ìŠ¤ ì„¤ì •
        self.service_config = {
            'default_model': 'qualitynet',
            'batch_size': 32,
            'enable_caching': True,
            'quality_thresholds': {
                'excellent': 0.8,
                'good': 0.6,
                'fair': 0.4,
                'poor': 0.0
            }
        }
        
        # ìºì‹œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
        self._quality_cache = {}
        self._max_cache_size = 1000
        
        logger.info("âœ… QualityAssessmentService initialized")

    def assess_single_image(self, image: Union[np.ndarray, Image.Image, torch.Tensor],
                          model_type: str = None,
                          **kwargs) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
            model_type: ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì…
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            start_time = time.time()
            
            # ëª¨ë¸ íƒ€ì… ì„¤ì •
            if model_type is None:
                model_type = self.service_config['default_model']
            
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(image, model_type)
            if self.service_config['enable_caching'] and cache_key in self._quality_cache:
                logger.info("âœ… Quality assessment result found in cache")
                return self._quality_cache[cache_key]
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if self.processor:
                processed_image = self.processor.preprocess_for_quality_assessment(
                    image, target_size=(224, 224), normalize=True
                )
            else:
                processed_image = image
            
            # í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
            if self.inference_engine:
                quality_result = self.inference_engine.assess_image_quality(
                    processed_image, model_type, **kwargs
                )
            else:
                # ê¸°ë³¸ í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ ë©”íŠ¸ë¦­)
                quality_result = self._basic_quality_assessment(processed_image)
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            result = self._postprocess_quality_result(quality_result, model_type)
            result['processing_time'] = time.time() - start_time
            result['timestamp'] = datetime.now().isoformat()
            
            # ìºì‹œì— ì €ì¥
            if self.service_config['enable_caching']:
                self._add_to_cache(cache_key, result)
            
            logger.info(f"âœ… Single image quality assessment completed: {result['quality_grade']}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Single image quality assessment failed: {e}")
            return {
                'error': str(e),
                'quality_score': 0.0,
                'quality_grade': 'Error',
                'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                'timestamp': datetime.now().isoformat()
            }

    def assess_batch_images(self, images: List[Union[np.ndarray, Image.Image, torch.Tensor]],
                          model_type: str = None,
                          **kwargs) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ í’ˆì§ˆì„ ì¼ê´„ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            images: ì…ë ¥ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            model_type: ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì…
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            start_time = time.time()
            batch_size = self.service_config['batch_size']
            results = []
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, len(images), batch_size):
                batch_images = images[i:i + batch_size]
                batch_results = []
                
                for j, image in enumerate(batch_images):
                    try:
                        result = self.assess_single_image(image, model_type, **kwargs)
                        result['batch_index'] = i + j
                        batch_results.append(result)
                    except Exception as e:
                        logger.error(f"âŒ Failed to assess image {i + j}: {e}")
                        batch_results.append({
                            'batch_index': i + j,
                            'error': str(e),
                            'quality_score': 0.0,
                            'quality_grade': 'Error',
                            'timestamp': datetime.now().isoformat()
                        })
                
                results.extend(batch_results)
                
                # ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ìƒí™© ë¡œê¹…
                logger.info(f"âœ… Batch {i//batch_size + 1} completed: {len(batch_results)} images")
            
            # ì „ì²´ í†µê³„ ê³„ì‚°
            total_time = time.time() - start_time
            self._log_batch_statistics(results, total_time)
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Batch quality assessment failed: {e}")
            return []

    def _basic_quality_assessment(self, image: torch.Tensor) -> Dict[str, Any]:
        """
        ê¸°ë³¸ í’ˆì§ˆ í‰ê°€ (ì¶”ë¡  ì—”ì§„ì´ ì—†ì„ ë•Œ ì‚¬ìš©)
        """
        try:
            # ê°„ë‹¨í•œ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            if len(image.shape) == 4:
                image = image.squeeze(0)
            
            # ë°ê¸°
            brightness = image.mean().item()
            
            # ëŒ€ë¹„
            contrast = image.std().item()
            
            # ì„ ëª…ë„ (ê°„ë‹¨í•œ ì—ì§€ ê²€ì¶œ)
            if image.shape[0] == 3:  # RGB
                gray = 0.299 * image[0] + 0.587 * image[1] + 0.114 * image[2]
            else:
                gray = image[0]
            
            # Sobel í•„í„°ë¡œ ì—ì§€ ê²€ì¶œ
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                                 dtype=torch.float32, device=image.device)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                                 dtype=torch.float32, device=image.device)
            
            edge_x = torch.conv2d(gray.unsqueeze(0).unsqueeze(0), 
                                sobel_x.unsqueeze(0).unsqueeze(0), padding=1)
            edge_y = torch.conv2d(gray.unsqueeze(0).unsqueeze(0), 
                                sobel_y.unsqueeze(0).unsqueeze(0), padding=1)
            
            edge_magnitude = torch.sqrt(edge_x**2 + edge_y**2)
            sharpness = edge_magnitude.mean().item()
            
            # ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-1 ë²”ìœ„)
            quality_score = min(1.0, (brightness * 0.3 + contrast * 0.3 + sharpness * 0.4))
            
            return {
                'quality_score': quality_score,
                'brightness': brightness,
                'contrast': contrast,
                'sharpness': sharpness,
                'model_type': 'basic'
            }
            
        except Exception as e:
            logger.error(f"âŒ Basic quality assessment failed: {e}")
            return {
                'quality_score': 0.0,
                'model_type': 'basic',
                'error': str(e)
            }

    def _postprocess_quality_result(self, result: Dict[str, Any], 
                                  model_type: str) -> Dict[str, Any]:
        """
        í’ˆì§ˆ í‰ê°€ ê²°ê³¼ë¥¼ í›„ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        try:
            # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
            quality_score = result.get('quality_score', 0.0)
            thresholds = self.service_config['quality_thresholds']
            
            if quality_score >= thresholds['excellent']:
                quality_grade = 'Excellent'
            elif quality_score >= thresholds['good']:
                quality_grade = 'Good'
            elif quality_score >= thresholds['fair']:
                quality_grade = 'Fair'
            else:
                quality_grade = 'Poor'
            
            # ê²°ê³¼ì— í’ˆì§ˆ ë“±ê¸‰ ì¶”ê°€
            result['quality_grade'] = quality_grade
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ì¶”ê°€ (ëª¨ë¸ íƒ€ì…ë³„)
            confidence_scores = {
                'qualitynet': 0.95,
                'brisque': 0.90,
                'niqe': 0.88,
                'piqe': 0.92,
                'basic': 0.70
            }
            
            result['confidence'] = confidence_scores.get(model_type, 0.80)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Result postprocessing failed: {e}")
            result['quality_grade'] = 'Unknown'
            result['confidence'] = 0.0
            return result

    def _generate_cache_key(self, image: Union[np.ndarray, Image.Image, torch.Tensor],
                           model_type: str) -> str:
        """
        ìºì‹œ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        try:
            # ì´ë¯¸ì§€ í•´ì‹œ ìƒì„± (ê°„ë‹¨í•œ ë°©ë²•)
            if isinstance(image, np.ndarray):
                # numpy arrayì˜ í‰ê· ê°’ê³¼ í‘œì¤€í¸ì°¨ë¡œ í•´ì‹œ ìƒì„±
                hash_value = f"{image.mean():.6f}_{image.std():.6f}_{image.shape}"
            elif isinstance(image, Image.Image):
                # PIL Imageì˜ í¬ê¸°ì™€ ëª¨ë“œë¡œ í•´ì‹œ ìƒì„±
                hash_value = f"{image.size}_{image.mode}"
            elif isinstance(image, torch.Tensor):
                # torch tensorì˜ í†µê³„ë¡œ í•´ì‹œ ìƒì„±
                hash_value = f"{image.mean().item():.6f}_{image.std().item():.6f}_{image.shape}"
            else:
                hash_value = str(hash(str(image)))
            
            return f"{model_type}_{hash_value}"
            
        except Exception as e:
            logger.warning(f"âš ï¸ Cache key generation failed: {e}")
            return f"{model_type}_{hash(str(image))}"

    def _add_to_cache(self, key: str, result: Dict[str, Any]):
        """
        ê²°ê³¼ë¥¼ ìºì‹œì— ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        try:
            # ìºì‹œ í¬ê¸° ì œí•œ í™•ì¸
            if len(self._quality_cache) >= self._max_cache_size:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = next(iter(self._quality_cache))
                del self._quality_cache[oldest_key]
                logger.debug("ğŸ—‘ï¸ Oldest cache entry removed")
            
            self._quality_cache[key] = result
            logger.debug(f"ğŸ’¾ Result cached: {key}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Cache addition failed: {e}")

    def _log_batch_statistics(self, results: List[Dict[str, Any]], total_time: float):
        """
        ë°°ì¹˜ ì²˜ë¦¬ í†µê³„ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.
        """
        try:
            if not results:
                return
            
            # ì„±ê³µí•œ í‰ê°€ ìˆ˜
            successful_results = [r for r in results if 'error' not in r]
            error_count = len(results) - len(successful_results)
            
            # í’ˆì§ˆ ë“±ê¸‰ë³„ ë¶„í¬
            grade_counts = {}
            for result in successful_results:
                grade = result.get('quality_grade', 'Unknown')
                grade_counts[grade] = grade_counts.get(grade, 0) + 1
            
            # í‰ê·  í’ˆì§ˆ ì ìˆ˜
            quality_scores = [r.get('quality_score', 0.0) for r in successful_results if 'quality_score' in r]
            avg_quality = np.mean(quality_scores) if quality_scores else 0.0
            
            # í†µê³„ ë¡œê¹…
            logger.info(f"ğŸ“Š Batch Statistics:")
            logger.info(f"   Total images: {len(results)}")
            logger.info(f"   Successful: {len(successful_results)}")
            logger.info(f"   Errors: {error_count}")
            logger.info(f"   Average quality score: {avg_quality:.3f}")
            logger.info(f"   Quality grades: {grade_counts}")
            logger.info(f"   Total processing time: {total_time:.2f}s")
            logger.info(f"   Average time per image: {total_time/len(results):.3f}s")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Statistics logging failed: {e}")

    def get_service_info(self) -> Dict[str, Any]:
        """
        ì„œë¹„ìŠ¤ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        return {
            'service_name': 'QualityAssessmentService',
            'version': '1.0.0',
            'supported_models': self.inference_engine.supported_models if self.inference_engine else [],
            'cache_enabled': self.service_config['enable_caching'],
            'cache_size': len(self._quality_cache),
            'max_cache_size': self._max_cache_size,
            'default_model': self.service_config['default_model'],
            'quality_thresholds': self.service_config['quality_thresholds']
        }

    def clear_cache(self):
        """
        ìºì‹œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
        """
        try:
            cache_size = len(self._quality_cache)
            self._quality_cache.clear()
            logger.info(f"ğŸ—‘ï¸ Cache cleared: {cache_size} entries removed")
        except Exception as e:
            logger.error(f"âŒ Cache clearing failed: {e}")

    def update_service_config(self, **kwargs):
        """
        ì„œë¹„ìŠ¤ ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        """
        try:
            for key, value in kwargs.items():
                if key in self.service_config:
                    self.service_config[key] = value
                    logger.info(f"âœ… Service config updated: {key} = {value}")
                else:
                    logger.warning(f"âš ï¸ Unknown config key: {key}")
        except Exception as e:
            logger.error(f"âŒ Service config update failed: {e}")
