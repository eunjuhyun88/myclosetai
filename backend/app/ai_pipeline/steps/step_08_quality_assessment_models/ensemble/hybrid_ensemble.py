#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Quality Assessment Hybrid Ensemble System
==========================================================

ğŸ¯ ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”ì„ í†µí•œ ì •í™•ë„ í–¥ìƒ
âœ… 8ê°œ Quality Assessment ëª¨ë¸ í†µí•©
âœ… M3 Max ìµœì í™”
âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
âœ… í’ˆì§ˆ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

@dataclass
class EnsembleConfig:
    """ì•™ìƒë¸” ì„¤ì •"""
    method: str = "quality_weighted"  # voting, weighted, quality, simple_average
    quality_threshold: float = 0.7
    confidence_threshold: float = 0.5
    max_models: int = 8
    use_mps: bool = True
    memory_efficient: bool = True

class QualityAssessmentEnsembleSystem(nn.Module):
    """
    ğŸ”¥ Quality Assessment ì•™ìƒë¸” ì‹œìŠ¤í…œ
    
    ë‹¤ì¤‘ ëª¨ë¸ì˜ ì¶œë ¥ì„ í†µí•©í•˜ì—¬ ìµœì¢… í’ˆì§ˆ í‰ê°€ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: EnsembleConfig = None):
        super().__init__()
        self.config = config or EnsembleConfig()
        self.logger = logging.getLogger(__name__)
        
        # MPS ë””ë°”ì´ìŠ¤ í™•ì¸
        self.device = torch.device("mps" if torch.backends.mps.is_available() and self.config.use_mps else "cpu")
        self.logger.info(f"ğŸ¯ Quality Assessment ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.ensemble_weights = nn.Parameter(torch.ones(8) / 8)  # 8ê°œ ëª¨ë¸
        
        # í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­
        self.quality_metrics = {
            "confidence": 0.0,
            "spatial_consistency": 0.0,
            "assessment_accuracy": 0.0,
            "evaluation_consistency": 0.0
        }
        
        self.logger.info("âœ… Quality Assessment ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def forward(self, model_outputs: List[torch.Tensor], 
                confidences: List[float] = None,
                quality_scores: List[float] = None) -> torch.Tensor:
        """
        ì•™ìƒë¸” ì¶”ë¡  ìˆ˜í–‰
        
        Args:
            model_outputs: ê° ëª¨ë¸ì˜ ì¶œë ¥ (List[torch.Tensor])
            confidences: ê° ëª¨ë¸ì˜ ì‹ ë¢°ë„ (List[float])
            quality_scores: ê° ëª¨ë¸ì˜ í’ˆì§ˆ ì ìˆ˜ (List[float])
        
        Returns:
            ì•™ìƒë¸”ëœ í’ˆì§ˆ í‰ê°€ ê²°ê³¼
        """
        if not model_outputs:
            raise ValueError("ëª¨ë¸ ì¶œë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        # ì…ë ¥ ê²€ì¦
        num_models = len(model_outputs)
        if num_models > self.config.max_models:
            self.logger.warning(f"ëª¨ë¸ ìˆ˜ê°€ ìµœëŒ€ í—ˆìš©ì¹˜ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤: {num_models} > {self.config.max_models}")
            model_outputs = model_outputs[:self.config.max_models]
        
        # ë””ë°”ì´ìŠ¤ ì´ë™
        model_outputs = [output.to(self.device) if isinstance(output, torch.Tensor) else torch.tensor(output, device=self.device) 
                        for output in model_outputs]
        
        # ì•™ìƒë¸” ë°©ë²•ì— ë”°ë¥¸ í†µí•©
        if self.config.method == "voting":
            ensemble_output = self._voting_ensemble(model_outputs, confidences)
        elif self.config.method == "weighted":
            ensemble_output = self._weighted_ensemble(model_outputs, confidences)
        elif self.config.method == "quality":
            ensemble_output = self._quality_weighted_ensemble(model_outputs, quality_scores)
        elif self.config.method == "simple_average":
            ensemble_output = self._simple_average_ensemble(model_outputs)
        else:
            self.logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì•™ìƒë¸” ë°©ë²•: {self.config.method}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            ensemble_output = self._quality_weighted_ensemble(model_outputs, quality_scores)
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self._update_quality_metrics(ensemble_output, model_outputs)
        
        return ensemble_output
    
    def _voting_ensemble(self, model_outputs: List[torch.Tensor], 
                         confidences: List[float] = None) -> torch.Tensor:
        """íˆ¬í‘œ ê¸°ë°˜ ì•™ìƒë¸”"""
        self.logger.debug("ğŸ¯ íˆ¬í‘œ ê¸°ë°˜ ì•™ìƒë¸” ìˆ˜í–‰")
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        if confidences:
            weights = torch.tensor(confidences, device=self.device)
            weights = F.softmax(weights, dim=0)
        else:
            weights = torch.ones(len(model_outputs), device=self.device) / len(model_outputs)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        ensemble_output = torch.zeros_like(model_outputs[0])
        for i, output in enumerate(model_outputs):
            ensemble_output += weights[i] * output
        
        return ensemble_output
    
    def _weighted_ensemble(self, model_outputs: List[torch.Tensor], 
                          confidences: List[float] = None) -> torch.Tensor:
        """ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì•™ìƒë¸”"""
        self.logger.debug("ğŸ¯ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì•™ìƒë¸” ìˆ˜í–‰")
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜
        if confidences:
            weights = torch.tensor(confidences, device=self.device)
            weights = F.softmax(weights, dim=0)
        else:
            weights = self.ensemble_weights[:len(model_outputs)]
            weights = F.softmax(weights, dim=0)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        ensemble_output = torch.zeros_like(model_outputs[0])
        for i, output in enumerate(model_outputs):
            ensemble_output += weights[i] * output
        
        return ensemble_output
    
    def _quality_weighted_ensemble(self, model_outputs: List[torch.Tensor], 
                                 quality_scores: List[float] = None) -> torch.Tensor:
        """í’ˆì§ˆ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì•™ìƒë¸”"""
        self.logger.debug("ğŸ¯ í’ˆì§ˆ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì•™ìƒë¸” ìˆ˜í–‰")
        
        # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜
        if quality_scores:
            weights = torch.tensor(quality_scores, device=self.device)
            weights = F.softmax(weights, dim=0)
        else:
            # ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜ (ì‹ ë¢°ë„ ê¸°ë°˜)
            weights = torch.ones(len(model_outputs), device=self.device) / len(model_outputs)
        
        # í’ˆì§ˆ ì„ê³„ê°’ ì ìš©
        quality_mask = weights > self.config.quality_threshold
        if quality_mask.sum() > 0:
            weights = weights * quality_mask.float()
            weights = F.softmax(weights, dim=0)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        ensemble_output = torch.zeros_like(model_outputs[0])
        for i, output in enumerate(model_outputs):
            ensemble_output += weights[i] * output
        
        return ensemble_output
    
    def _simple_average_ensemble(self, model_outputs: List[torch.Tensor]) -> torch.Tensor:
        """ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸”"""
        self.logger.debug("ğŸ¯ ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸” ìˆ˜í–‰")
        
        # ëª¨ë“  ëª¨ë¸ ì¶œë ¥ì˜ í‰ê· 
        ensemble_output = torch.stack(model_outputs).mean(dim=0)
        return ensemble_output
    
    def _update_quality_metrics(self, ensemble_output: torch.Tensor, 
                              model_outputs: List[torch.Tensor]):
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        # ì‹ ë¢°ë„ ê³„ì‚°
        if ensemble_output.dim() > 0:
            self.quality_metrics["confidence"] = float(ensemble_output.mean().item())
        
        # ê³µê°„ ì¼ê´€ì„± ê³„ì‚°
        if len(model_outputs) > 1:
            spatial_consistency = self._calculate_spatial_consistency(model_outputs)
            self.quality_metrics["spatial_consistency"] = spatial_consistency
        
        # í‰ê°€ ì •í™•ë„ ê³„ì‚°
        assessment_accuracy = self._calculate_assessment_accuracy(ensemble_output)
        self.quality_metrics["assessment_accuracy"] = assessment_accuracy
        
        # í‰ê°€ ì¼ê´€ì„± ê³„ì‚°
        evaluation_consistency = self._calculate_evaluation_consistency(ensemble_output)
        self.quality_metrics["evaluation_consistency"] = evaluation_consistency
    
    def _calculate_spatial_consistency(self, model_outputs: List[torch.Tensor]) -> float:
        """ê³µê°„ ì¼ê´€ì„± ê³„ì‚°"""
        if len(model_outputs) < 2:
            return 0.0
        
        # ê° ëª¨ë¸ ì¶œë ¥ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i in range(len(model_outputs)):
            for j in range(i + 1, len(model_outputs)):
                sim = F.cosine_similarity(
                    model_outputs[i].flatten(), 
                    model_outputs[j].flatten(), 
                    dim=0
                )
                similarities.append(sim.item())
        
        return float(np.mean(similarities)) if similarities else 0.0
    
    def _calculate_assessment_accuracy(self, output: torch.Tensor) -> float:
        """í‰ê°€ ì •í™•ë„ ê³„ì‚°"""
        if output.dim() == 0:
            return 0.0
        
        # í’ˆì§ˆ í‰ê°€ ê²°ê³¼ì˜ ì •í™•ë„ í‰ê°€
        if output.dim() > 1:
            # 2D ì´ìƒì¸ ê²½ìš° í‰ê· ê°’ ì‚¬ìš©
            output_flat = output.mean(dim=0) if output.dim() > 1 else output
        else:
            output_flat = output
        
        # í‰ê°€ ì •í™•ë„ (entropy ê¸°ë°˜)
        if output_flat.numel() > 1:
            probs = F.softmax(output_flat, dim=0)
            entropy = -torch.sum(probs * torch.log(probs + 1e-8))
            max_entropy = torch.log(torch.tensor(float(output_flat.numel())))
            accuracy_score = float(entropy / max_entropy)
        else:
            accuracy_score = 0.0
        
        return accuracy_score
    
    def _calculate_evaluation_consistency(self, output: torch.Tensor) -> float:
        """í‰ê°€ ì¼ê´€ì„± ê³„ì‚°"""
        if output.dim() == 0:
            return 0.0
        
        # í’ˆì§ˆ í‰ê°€ ê²°ê³¼ì˜ ì¼ê´€ì„± í‰ê°€
        if output.dim() > 2:
            # 2D ì´ìƒì¸ ê²½ìš° í‰ê°€ ì¼ê´€ì„± ê³„ì‚°
            batch_size = output.size(0)
            consistency_scores = []
            
            for b in range(batch_size):
                # í‰ê°€ ì¼ê´€ì„± ê³„ì‚°
                if output.size(1) > 1:  # ì±„ë„ì´ ìˆëŠ” ê²½ìš°
                    assessment_result = output[b].mean(dim=0)  # ì±„ë„ í‰ê· 
                else:
                    assessment_result = output[b]
                
                # í‰ê°€ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
                if assessment_result.dim() == 2:
                    # 2D í…ì„œë¥¼ numpyë¡œ ë³€í™˜
                    result_np = assessment_result.detach().cpu().numpy()
                    
                    # í‰ê°€ ì¼ê´€ì„± ê³„ì‚° (ì˜ˆ: í‰ê°€ ì ìˆ˜ì˜ ë¶„í¬ ê· ë“±ì„±)
                    # 1. í‰ê°€ ì ìˆ˜ì˜ ë¶„í¬ ê· ë“±ì„±
                    score_hist = np.histogram(result_np, bins=20)[0]
                    score_entropy = -np.sum(score_hist * np.log(score_hist + 1e-8))
                    max_score_entropy = np.log(20)
                    distribution_score = score_entropy / max_score_entropy
                    
                    # 2. í‰ê°€ ì ìˆ˜ì˜ í‘œì¤€í¸ì°¨ (ì ë‹¹í•œ ë¶„ì‚°ì´ ì¢‹ìŒ)
                    score_std = np.std(result_np)
                    variance_score = 1.0 / (1.0 + score_std)
                    
                    # ì¢…í•© ì¼ê´€ì„± ì ìˆ˜
                    consistency_score = (distribution_score * 0.6 + variance_score * 0.4)
                    consistency_scores.append(consistency_score)
            
            return float(np.mean(consistency_scores)) if consistency_scores else 0.0
        else:
            return 0.0
    
    def get_quality_metrics(self) -> Dict[str, float]:
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        return self.quality_metrics.copy()
    
    def update_ensemble_weights(self, new_weights: torch.Tensor):
        """ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        if new_weights.shape == self.ensemble_weights.shape:
            with torch.no_grad():
                self.ensemble_weights.copy_(new_weights)
            self.logger.info("âœ… ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        else:
            self.logger.warning(f"ê°€ì¤‘ì¹˜ ì°¨ì› ë¶ˆì¼ì¹˜: {new_weights.shape} vs {self.ensemble_weights.shape}")
    
    def get_ensemble_info(self) -> Dict[str, Union[str, int, float]]:
        """ì•™ìƒë¸” ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        return {
            "method": self.config.method,
            "num_models": self.config.max_models,
            "device": str(self.device),
            "quality_threshold": self.config.quality_threshold,
            "confidence_threshold": self.config.confidence_threshold,
            "memory_efficient": self.config.memory_efficient,
            "current_weights": self.ensemble_weights.detach().cpu().numpy().tolist()
        }

# ì•™ìƒë¸” ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_quality_assessment_ensemble(config: EnsembleConfig = None) -> QualityAssessmentEnsembleSystem:
    """Quality Assessment ì•™ìƒë¸” ì‹œìŠ¤í…œ ìƒì„±"""
    return QualityAssessmentEnsembleSystem(config)

# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì•™ìƒë¸” ì‹œìŠ¤í…œ ìƒì„±
def create_default_ensemble() -> QualityAssessmentEnsembleSystem:
    """ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì•™ìƒë¸” ì‹œìŠ¤í…œ ìƒì„±"""
    config = EnsembleConfig(
        method="quality_weighted",
        quality_threshold=0.7,
        confidence_threshold=0.5,
        max_models=8,
        use_mps=True,
        memory_efficient=True
    )
    return QualityAssessmentEnsembleSystem(config)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    # ì•™ìƒë¸” ì‹œìŠ¤í…œ ìƒì„±
    ensemble = create_default_ensemble()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size, num_metrics = 2, 10  # 10ê°œ í’ˆì§ˆ ë©”íŠ¸ë¦­
    test_outputs = [
        torch.randn(batch_size, num_metrics) for _ in range(4)
    ]
    
    # ì•™ìƒë¸” ìˆ˜í–‰
    result = ensemble(test_outputs)
    print(f"ì•™ìƒë¸” ê²°ê³¼ í˜•íƒœ: {result.shape}")
    print(f"í’ˆì§ˆ ë©”íŠ¸ë¦­: {ensemble.get_quality_metrics()}")
    print(f"ì•™ìƒë¸” ì •ë³´: {ensemble.get_ensemble_info()}")
