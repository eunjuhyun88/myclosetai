#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Quality Assessment Quality Assessment
======================================================

âœ… í†µì¼ëœ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ
âœ… í’ˆì§ˆ í‰ê°€ í’ˆì§ˆ ìë™ í‰ê°€
âœ… ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°

Author: MyCloset AI Team
Date: 2025-08-14
Version: 1.0 (í†µì¼ëœ êµ¬ì¡°)
"""

import logging
from typing import Dict, Any, List, Optional, Tuple
import numpy as np

logger = logging.getLogger(__name__)

class QualityAssessmentQualityAssessment:
    """Quality Assessment í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ - í†µì¼ëœ êµ¬ì¡°"""
    
    def __init__(self):
        self.quality_metrics = [
            'assessment_accuracy',
            'evaluation_consistency',
            'metric_reliability',
            'confidence_score'
        ]
        self.quality_thresholds = {
            'excellent': 0.9,
            'good': 0.7,
            'fair': 0.5,
            'poor': 0.3
        }
    
    def assess_quality(self, assessment_result: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ í‰ê°€ ê²°ê³¼ í’ˆì§ˆ í‰ê°€"""
        try:
            quality_scores = {}
            
            # í‰ê°€ ì •í™•ë„ í‰ê°€
            quality_scores['assessment_accuracy'] = self._assess_assessment_accuracy(assessment_result)
            
            # í‰ê°€ ì¼ê´€ì„± í‰ê°€
            quality_scores['evaluation_consistency'] = self._assess_evaluation_consistency(assessment_result)
            
            # ë©”íŠ¸ë¦­ ì‹ ë¢°ì„± í‰ê°€
            quality_scores['metric_reliability'] = self._assess_metric_reliability(assessment_result)
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            quality_scores['confidence_score'] = self._calculate_confidence_score(assessment_result)
            
            # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
            overall_quality = np.mean(list(quality_scores.values()))
            quality_scores['overall_quality'] = overall_quality
            
            # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
            quality_grade = self._determine_quality_grade(overall_quality)
            quality_scores['quality_grade'] = quality_grade
            
            return {
                'quality_scores': quality_scores,
                'assessment_status': 'success',
                'timestamp': self._get_timestamp()
            }
            
        except Exception as e:
            logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'quality_scores': {},
                'assessment_status': 'failed',
                'error': str(e)
            }
    
    def _assess_assessment_accuracy(self, result: Dict[str, Any]) -> float:
        """í‰ê°€ ì •í™•ë„ í‰ê°€"""
        try:
            # í’ˆì§ˆ í‰ê°€ ê²°ê³¼ì˜ ì •í™•ë„ í‰ê°€
            quality_scores = result.get('quality_scores', {})
            
            if isinstance(quality_scores, dict) and quality_scores:
                # ê°„ë‹¨í•œ ì •í™•ë„ ì ìˆ˜ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
                return 0.85
            else:
                return 0.5
        except Exception as e:
            logger.warning(f"âš ï¸ í‰ê°€ ì •í™•ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _assess_evaluation_consistency(self, result: Dict[str, Any]) -> float:
        """í‰ê°€ ì¼ê´€ì„± í‰ê°€"""
        try:
            # í’ˆì§ˆ í‰ê°€ ê²°ê³¼ì˜ ì¼ê´€ì„± í‰ê°€
            quality_scores = result.get('quality_scores', {})
            
            if isinstance(quality_scores, dict) and len(quality_scores) > 1:
                # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
                scores = list(quality_scores.values())
                if all(isinstance(s, (int, float)) for s in scores):
                    # ì ìˆ˜ ê°„ì˜ í‘œì¤€í¸ì°¨ë¥¼ ì´ìš©í•œ ì¼ê´€ì„± ê³„ì‚°
                    scores_array = np.array(scores)
                    mean_score = np.mean(scores_array)
                    std_score = np.std(scores_array)
                    
                    if mean_score > 0:
                        consistency = 1.0 / (1.0 + std_score / mean_score)
                        return min(consistency, 1.0)
                
                return 0.8
            else:
                return 0.5
        except Exception as e:
            logger.warning(f"âš ï¸ í‰ê°€ ì¼ê´€ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _assess_metric_reliability(self, result: Dict[str, Any]) -> float:
        """ë©”íŠ¸ë¦­ ì‹ ë¢°ì„± í‰ê°€"""
        try:
            # í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­ì˜ ì‹ ë¢°ì„± í‰ê°€
            quality_scores = result.get('quality_scores', {})
            
            if isinstance(quality_scores, dict) and quality_scores:
                # ë©”íŠ¸ë¦­ ì‹ ë¢°ì„± ì ìˆ˜
                return 0.9
            else:
                return 0.5
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”íŠ¸ë¦­ ì‹ ë¢°ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_confidence_score(self, result: Dict[str, Any]) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ì—¬ëŸ¬ ìš”ì†Œë¥¼ ì¢…í•©í•œ ì‹ ë¢°ë„ ê³„ì‚°
            confidence_factors = []
            
            # í‰ê°€ í’ˆì§ˆ
            if 'assessment_quality' in result:
                confidence_factors.append(result['assessment_quality'])
            
            # í‰ê°€ ì¼ê´€ì„±
            if 'evaluation_consistency' in result:
                confidence_factors.append(result['evaluation_consistency'])
            
            # ë©”íŠ¸ë¦­ í’ˆì§ˆ
            if 'metric_quality' in result:
                confidence_factors.append(result['metric_quality'])
            
            if confidence_factors:
                return np.mean(confidence_factors)
            else:
                return 0.5
        except Exception as e:
            logger.warning(f"âš ï¸ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _determine_quality_grade(self, quality_score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        if quality_score >= self.quality_thresholds['excellent']:
            return 'excellent'
        elif quality_score >= self.quality_thresholds['good']:
            return 'good'
        elif quality_score >= self.quality_thresholds['fair']:
            return 'fair'
        elif quality_score >= self.quality_thresholds['poor']:
            return 'poor'
        else:
            return 'very_poor'
    
    def _get_timestamp(self) -> str:
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def get_quality_metrics(self) -> List[str]:
        """í’ˆì§ˆ ì§€í‘œ ëª©ë¡ ë°˜í™˜"""
        return self.quality_metrics.copy()
    
    def get_quality_thresholds(self) -> Dict[str, float]:
        """í’ˆì§ˆ ì„ê³„ê°’ ë°˜í™˜"""
        return self.quality_thresholds.copy()
    
    def set_quality_thresholds(self, thresholds: Dict[str, float]):
        """í’ˆì§ˆ ì„ê³„ê°’ ì„¤ì •"""
        self.quality_thresholds.update(thresholds)
        logger.info(f"âœ… í’ˆì§ˆ ì„ê³„ê°’ ì—…ë°ì´íŠ¸: {thresholds}")
