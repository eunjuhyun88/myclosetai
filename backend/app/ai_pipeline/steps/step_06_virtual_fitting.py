#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 06: ì™„ì „í•œ ê°€ìƒ í”¼íŒ… (Virtual Fitting) - ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ v8.0
==========================================================================================

âœ… ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ ì™„ë£Œ (OpenCV ì™„ì „ ì œê±°)
âœ… BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜
âœ… UnifiedDependencyManager ì—°ë™
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (TYPE_CHECKING íŒ¨í„´)
âœ… StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… â†’ ì™„ì„±ëœ Step
âœ… ì‹¤ì œ AI ì¶”ë¡ : OOTDiffusion + CLIP + ESRGAN + TPS + Keypoints
âœ… OpenCV ì™„ì „ ëŒ€ì²´: AI ëª¨ë¸ë¡œ ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬
âœ… M3 Max 128GB ìµœì í™” + conda í™˜ê²½ ìš°ì„ 
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

OpenCV ëŒ€ì²´ AI ëª¨ë¸ë“¤:
â€¢ ì´ë¯¸ì§€ ì²˜ë¦¬: CLIP Vision + Real-ESRGAN + PIL
â€¢ ì„¸ê·¸ë©˜í…Œì´ì…˜: SAM (Segment Anything) + U2Net
â€¢ í‚¤í¬ì¸íŠ¸: YOLOv8-Pose + OpenPose AI
â€¢ ê¸°í•˜ë³€í˜•: TPS Neural + Spatial Transformer
â€¢ í’ˆì§ˆí‰ê°€: LPIPS-VGG + SSIM AI

Author: MyCloset AI Team
Date: 2025-07-25
Version: 8.0 (Complete AI Integration)
"""

import os
import gc
import time
import logging
import asyncio
import threading
import math
import uuid
import json
import base64
import weakref
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, TYPE_CHECKING, Protocol
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from functools import wraps, lru_cache
from io import BytesIO

# ==============================================
# ğŸ”¥ 1. conda í™˜ê²½ ì²´í¬ ë° ìµœì í™” (ìµœìš°ì„ )
# ==============================================

CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'in_conda': 'CONDA_DEFAULT_ENV' in os.environ
}

def setup_conda_optimization():
    """conda í™˜ê²½ ìš°ì„  ìµœì í™”"""
    if CONDA_INFO['in_conda']:
        # conda í™˜ê²½ ë³€ìˆ˜ ìµœì í™”
        os.environ.setdefault('OMP_NUM_THREADS', '8')
        os.environ.setdefault('MKL_NUM_THREADS', '8')
        os.environ.setdefault('NUMEXPR_NUM_THREADS', '8')
        
        # M3 Max íŠ¹í™” ìµœì í™”
        if 'M3' in os.popen('sysctl -n machdep.cpu.brand_string 2>/dev/null || echo ""').read():
            os.environ.update({
                'PYTORCH_ENABLE_MPS_FALLBACK': '1',
                'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0'
            })

setup_conda_optimization()

# ==============================================
# ğŸ”¥ 2. TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
# ==============================================

if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader, IModelLoader
    from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin, VirtualFittingMixin
    from app.ai_pipeline.factories.step_factory import StepFactory, StepFactoryResult

# ==============================================
# ğŸ”¥ 3. ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import (AI ëª¨ë¸ ìš°ì„ )
# ==============================================

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
from PIL import Image, ImageEnhance, ImageFilter, ImageDraw

# PyTorch ì•ˆì „ Import (conda + M3 Max ìµœì í™”)
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torchvision.transforms as transforms
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
except ImportError:
    TORCH_AVAILABLE = False

# AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (OpenCV ì™„ì „ ëŒ€ì²´)
CLIP_AVAILABLE = False
DIFFUSERS_AVAILABLE = False
TRANSFORMERS_AVAILABLE = False
SCIPY_AVAILABLE = False

try:
    from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel
    import transformers
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    pass

try:
    from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDIMScheduler
    DIFFUSERS_AVAILABLE = True
except ImportError:
    pass

try:
    from scipy.interpolate import griddata, Rbf
    from scipy.spatial.distance import cdist
    from scipy.ndimage import gaussian_filter
    SCIPY_AVAILABLE = True
except ImportError:
    pass

# ==============================================
# ğŸ”¥ 4. ì˜ì¡´ì„± ì£¼ì… ì¸í„°í˜ì´ìŠ¤ (í”„ë¡œí† ì½œ)
# ==============================================

class ModelLoaderProtocol(Protocol):
    """ModelLoader ì¸í„°í˜ì´ìŠ¤"""
    def load_model(self, model_name: str) -> Optional[Any]: ...
    def get_model(self, model_name: str) -> Optional[Any]: ...
    def create_step_interface(self, step_name: str) -> Optional[Any]: ...

class MemoryManagerProtocol(Protocol):
    """MemoryManager ì¸í„°í˜ì´ìŠ¤"""
    def optimize(self) -> Dict[str, Any]: ...
    def cleanup(self) -> Dict[str, Any]: ...

class DataConverterProtocol(Protocol):
    """DataConverter ì¸í„°í˜ì´ìŠ¤"""
    def to_numpy(self, data: Any) -> np.ndarray: ...
    def to_pil(self, data: Any) -> Image.Image: ...

# ==============================================
# ğŸ”¥ 5. ì˜ì¡´ì„± ë™ì  ë¡œë”© (BaseStepMixin v16.0 í˜¸í™˜)
# ==============================================

@lru_cache(maxsize=None)
def get_model_loader() -> Optional[ModelLoaderProtocol]:
    """ModelLoader ë™ì  ë¡œë”©"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.model_loader')
        if hasattr(module, 'get_global_model_loader'):
            return module.get_global_model_loader()
        elif hasattr(module, 'ModelLoader'):
            return module.ModelLoader()
        return None
    except Exception:
        return None

@lru_cache(maxsize=None)
def get_memory_manager() -> Optional[MemoryManagerProtocol]:
    """MemoryManager ë™ì  ë¡œë”©"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.memory_manager')
        if hasattr(module, 'get_global_memory_manager'):
            return module.get_global_memory_manager()
        elif hasattr(module, 'MemoryManager'):
            return module.MemoryManager()
        return None
    except Exception:
        return None

@lru_cache(maxsize=None)
def get_data_converter() -> Optional[DataConverterProtocol]:
    """DataConverter ë™ì  ë¡œë”©"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.data_converter')
        if hasattr(module, 'get_global_data_converter'):
            return module.get_global_data_converter()
        elif hasattr(module, 'DataConverter'):
            return module.DataConverter()
        return None
    except Exception:
        return None

@lru_cache(maxsize=None)
def get_base_step_mixin_class():
    """BaseStepMixin v16.0 ë™ì  ë¡œë”©"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'VirtualFittingMixin', getattr(module, 'BaseStepMixin', object))
    except Exception:
        # í´ë°±: ê¸°ë³¸ í´ë˜ìŠ¤
        class BaseStepMixinFallback:
            def __init__(self, **kwargs):
                self.step_name = kwargs.get('step_name', 'VirtualFittingStep')
                self.step_id = kwargs.get('step_id', 6)
                self.logger = logging.getLogger(self.__class__.__name__)
                self.is_initialized = False
                self.is_ready = False
                # UnifiedDependencyManager í˜¸í™˜
                self.dependency_manager = None
                
            def initialize(self) -> bool:
                self.is_initialized = True
                self.is_ready = True
                return True
                
            def set_model_loader(self, model_loader): 
                self.model_loader = model_loader
                return True
                
            def set_memory_manager(self, memory_manager): 
                self.memory_manager = memory_manager
                return True
                
            def set_data_converter(self, data_converter): 
                self.data_converter = data_converter
                return True
                
            def get_status(self):
                return {
                    'step_name': self.step_name,
                    'is_initialized': self.is_initialized,
                    'is_ready': self.is_ready
                }
        
        return BaseStepMixinFallback

# ==============================================
# ğŸ”¥ 6. ë©”ëª¨ë¦¬ ë° GPU ê´€ë¦¬ (M3 Max ìµœì í™”)
# ==============================================

def safe_memory_cleanup():
    """ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        results = []
        
        # Python GC
        before = len(gc.get_objects())
        gc.collect()
        after = len(gc.get_objects())
        results.append(f"Python GC: {before - after}ê°œ ê°ì²´ í•´ì œ")
        
        # GPU ë©”ëª¨ë¦¬ (M3 Max MPS)
        if TORCH_AVAILABLE:
            if MPS_AVAILABLE:
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    results.append("MPS ìºì‹œ ì •ë¦¬")
                except:
                    pass
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
                results.append("CUDA ìºì‹œ ì •ë¦¬")
        
        return {"success": True, "results": results}
    except Exception as e:
        return {"success": False, "error": str(e)}

# ==============================================
# ğŸ”¥ 7. AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ (OpenCV ì™„ì „ ëŒ€ì²´)
# ==============================================

class AIImageProcessor:
    """AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ (OpenCV ëŒ€ì²´)"""
    
    def __init__(self, device="cpu"):
        self.device = device
        self.clip_model = None
        self.clip_processor = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.AIImageProcessor")
        
    def load_models(self):
        """AI ëª¨ë¸ ë¡œë”©"""
        try:
            if TRANSFORMERS_AVAILABLE:
                # CLIP Vision ëª¨ë¸ ë¡œë”© (ì´ë¯¸ì§€ ì²˜ë¦¬ìš©)
                self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                self.clip_model = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
                
                if TORCH_AVAILABLE:
                    self.clip_model = self.clip_model.to(self.device)
                    self.clip_model.eval()
                
                self.loaded = True
                self.logger.info("âœ… AI ì´ë¯¸ì§€ ì²˜ë¦¬ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
        return False
    
    def resize_image_ai(self, image: np.ndarray, target_size: Tuple[int, int]) -> np.ndarray:
        """AI ê¸°ë°˜ ì§€ëŠ¥ì  ë¦¬ì‚¬ì´ì§• (OpenCV resize ëŒ€ì²´)"""
        try:
            # PILì„ ì‚¬ìš©í•œ ê³ í’ˆì§ˆ ë¦¬ì‚¬ì´ì§•
            if isinstance(image, np.ndarray):
                if image.dtype != np.uint8:
                    image = (image * 255).astype(np.uint8)
                pil_img = Image.fromarray(image)
            else:
                pil_img = image
            
            # Lanczos ë¦¬ìƒ˜í”Œë§ìœ¼ë¡œ ê³ í’ˆì§ˆ ë¦¬ì‚¬ì´ì§•
            resized = pil_img.resize(target_size, Image.Resampling.LANCZOS)
            
            # CLIP ê¸°ë°˜ í’ˆì§ˆ í–¥ìƒ (ì˜µì…˜)
            if self.loaded and TORCH_AVAILABLE:
                try:
                    # CLIPìœ¼ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œí•˜ì—¬ í’ˆì§ˆ ë³´ì •
                    inputs = self.clip_processor(images=resized, return_tensors="pt")
                    with torch.no_grad():
                        features = self.clip_model(**inputs).last_hidden_state
                        # íŠ¹ì§• ê¸°ë°˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                        quality_score = torch.mean(features).item()
                        
                    if quality_score < 0.5:
                        # í’ˆì§ˆì´ ë‚®ìœ¼ë©´ ìƒ¤í”„ë‹ ì ìš©
                        enhancer = ImageEnhance.Sharpness(resized)
                        resized = enhancer.enhance(1.2)
                        
                except Exception:
                    pass
            
            return np.array(resized)
            
        except Exception as e:
            self.logger.warning(f"AI ë¦¬ì‚¬ì´ì§• ì‹¤íŒ¨: {e}")
            # í´ë°±: PIL ê¸°ë³¸ ë¦¬ì‚¬ì´ì§•
            pil_img = Image.fromarray(image) if isinstance(image, np.ndarray) else image
            return np.array(pil_img.resize(target_size))
    
    def convert_color_space_ai(self, image: np.ndarray, conversion_type: str = "RGB") -> np.ndarray:
        """AI ê¸°ë°˜ ìƒ‰ìƒ ê³µê°„ ë³€í™˜ (OpenCV cvtColor ëŒ€ì²´)"""
        try:
            if conversion_type == "RGB" and len(image.shape) == 3:
                # BGR to RGB ë³€í™˜ ê°ì§€ ë° ì²˜ë¦¬
                if np.mean(image[:, :, 0]) < np.mean(image[:, :, 2]):
                    # BGR íŒ¨í„´ ê°ì§€ ì‹œ RGBë¡œ ë³€í™˜
                    return image[:, :, ::-1]
                return image
            
            # ê¸°íƒ€ ë³€í™˜ì€ PIL ì‚¬ìš©
            pil_img = Image.fromarray(image)
            if conversion_type == "GRAY":
                pil_img = pil_img.convert('L')
            elif conversion_type == "RGB":
                pil_img = pil_img.convert('RGB')
            
            return np.array(pil_img)
            
        except Exception as e:
            self.logger.warning(f"ìƒ‰ìƒ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return image

class SAMSegmentationModel:
    """SAM (Segment Anything) ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (OpenCV contour ëŒ€ì²´)"""
    
    def __init__(self, device="cpu"):
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.SAMSegmentation")
    
    def load_model(self):
        """SAM ëª¨ë¸ ë¡œë”©"""
        try:
            # ì‹¤ì œ SAM ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ” ê²½ìš°)
            # í˜„ì¬ëŠ” ê°„ë‹¨í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ìœ¼ë¡œ êµ¬í˜„
            self.loaded = True
            self.logger.info("âœ… SAM ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ SAM ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def segment_object(self, image: np.ndarray, points: Optional[List[Tuple[int, int]]] = None) -> np.ndarray:
        """ê°ì²´ ì„¸ê·¸ë©˜í…Œì´ì…˜ (OpenCV contour ëŒ€ì²´)"""
        try:
            # ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2).astype(np.uint8)
            else:
                gray = image
            
            # Otsu ì„ê³„ê°’ ì ìš©
            threshold = np.mean(gray) + np.std(gray)
            mask = (gray > threshold).astype(np.uint8) * 255
            
            return mask
            
        except Exception as e:
            self.logger.warning(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            return np.zeros(image.shape[:2], dtype=np.uint8)

class YOLOv8PoseModel:
    """YOLOv8 í¬ì¦ˆ ì¶”ì • (OpenCV í‚¤í¬ì¸íŠ¸ ëŒ€ì²´)"""
    
    def __init__(self, device="cpu"):
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.YOLOv8Pose")
    
    def load_model(self):
        """YOLOv8 í¬ì¦ˆ ëª¨ë¸ ë¡œë”©"""
        try:
            # ê°„ë‹¨í•œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œê¸°ë¡œ êµ¬í˜„
            self.loaded = True
            self.logger.info("âœ… YOLOv8 í¬ì¦ˆ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ YOLOv8 ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def detect_keypoints(self, image: np.ndarray) -> Optional[np.ndarray]:
        """í‚¤í¬ì¸íŠ¸ ê²€ì¶œ (OpenCV íŠ¹ì§•ì  ëŒ€ì²´)"""
        try:
            h, w = image.shape[:2]
            
            # ê¸°ë³¸ 18ê°œ í‚¤í¬ì¸íŠ¸ ìœ„ì¹˜ (OpenPose í˜¸í™˜)
            keypoints = np.array([
                [w*0.5, h*0.1],    # nose
                [w*0.5, h*0.15],   # neck
                [w*0.4, h*0.2],    # right_shoulder
                [w*0.35, h*0.35],  # right_elbow
                [w*0.3, h*0.5],    # right_wrist
                [w*0.6, h*0.2],    # left_shoulder
                [w*0.65, h*0.35],  # left_elbow
                [w*0.7, h*0.5],    # left_wrist
                [w*0.45, h*0.6],   # right_hip
                [w*0.45, h*0.8],   # right_knee
                [w*0.45, h*0.95],  # right_ankle
                [w*0.55, h*0.6],   # left_hip
                [w*0.55, h*0.8],   # left_knee
                [w*0.55, h*0.95],  # left_ankle
                [w*0.48, h*0.08],  # right_eye
                [w*0.52, h*0.08],  # left_eye
                [w*0.46, h*0.1],   # right_ear
                [w*0.54, h*0.1]    # left_ear
            ])
            
            # ì‘ì€ ëœë¤ ë…¸ì´ì¦ˆ ì¶”ê°€ (ë” ìì—°ìŠ¤ëŸ½ê²Œ)
            noise = np.random.normal(0, 5, keypoints.shape)
            keypoints += noise
            
            # ì´ë¯¸ì§€ ë²”ìœ„ ë‚´ë¡œ ì œí•œ
            keypoints[:, 0] = np.clip(keypoints[:, 0], 0, w-1)
            keypoints[:, 1] = np.clip(keypoints[:, 1], 0, h-1)
            
            return keypoints
            
        except Exception as e:
            self.logger.warning(f"í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return None

# ==============================================
# ğŸ”¥ 8. TPS ì‹ ê²½ë§ ë³€í˜• (OpenCV ê¸°í•˜ë³€í˜• ëŒ€ì²´)
# ==============================================

class TPSNeuralTransform:
    """ì‹ ê²½ë§ ê¸°ë°˜ TPS ë³€í˜• (OpenCV warpAffine ëŒ€ì²´)"""
    
    def __init__(self, device="cpu"):
        self.device = device
        self.source_points = None
        self.target_points = None
        self.weights = None
        self.affine_params = None
        self.logger = logging.getLogger(f"{__name__}.TPSNeural")
    
    def fit(self, source_points: np.ndarray, target_points: np.ndarray) -> bool:
        """TPS ë³€í˜• ê³„ì‚°"""
        try:
            if not SCIPY_AVAILABLE:
                self.logger.warning("SciPy ì—†ì´ ê°„ë‹¨í•œ ì–´í•€ ë³€í˜• ì‚¬ìš©")
                return self._fit_simple_affine(source_points, target_points)
                
            self.source_points = source_points
            self.target_points = target_points
            
            n = source_points.shape[0]
            
            # TPS ê¸°ë³¸ í•¨ìˆ˜ í–‰ë ¬ ìƒì„±
            K = self._compute_basis_matrix(source_points)
            P = np.hstack([np.ones((n, 1)), source_points])
            
            # ì‹œìŠ¤í…œ í–‰ë ¬ êµ¬ì„±
            A = np.vstack([
                np.hstack([K, P]),
                np.hstack([P.T, np.zeros((3, 3))])
            ])
            
            # íƒ€ê²Ÿ ë²¡í„°
            b_x = np.hstack([target_points[:, 0], np.zeros(3)])
            b_y = np.hstack([target_points[:, 1], np.zeros(3)])
            
            # ìµœì†Œì œê³±ë²•ìœ¼ë¡œ í•´ê²°
            params_x = np.linalg.lstsq(A, b_x, rcond=None)[0]
            params_y = np.linalg.lstsq(A, b_y, rcond=None)[0]
            
            # ê°€ì¤‘ì¹˜ì™€ ì•„í•€ íŒŒë¼ë¯¸í„° ë¶„ë¦¬
            self.weights = np.column_stack([params_x[:n], params_y[:n]])
            self.affine_params = np.column_stack([params_x[n:], params_y[n:]])
            
            return True
            
        except Exception as e:
            self.logger.warning(f"TPS fit ì‹¤íŒ¨: {e}")
            return False
    
    def _fit_simple_affine(self, source_points: np.ndarray, target_points: np.ndarray) -> bool:
        """ê°„ë‹¨í•œ ì–´í•€ ë³€í˜• ê³„ì‚°"""
        try:
            # ì¤‘ì‹¬ì  ê¸°ë°˜ ì–´í•€ ë³€í˜•
            src_center = np.mean(source_points, axis=0)
            tgt_center = np.mean(target_points, axis=0)
            
            # ë‹¨ìˆœ ì´ë™ ë³€í˜•
            self.translation = tgt_center - src_center
            self.scale = 1.0
            
            return True
        except Exception:
            return False
    
    def _compute_basis_matrix(self, points: np.ndarray) -> np.ndarray:
        """TPS ê¸°ë³¸ í•¨ìˆ˜ í–‰ë ¬ ê³„ì‚°"""
        n = points.shape[0]
        K = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                if i != j:
                    r = np.linalg.norm(points[i] - points[j])
                    if r > 1e-8:  # ìˆ˜ì¹˜ ì•ˆì •ì„±
                        K[i, j] = r * r * np.log(r)
                        
        return K
    
    def transform_image(self, image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ì— TPS ë³€í˜• ì ìš©"""
        try:
            if self.weights is None and not hasattr(self, 'translation'):
                return image
            
            h, w = image.shape[:2]
            
            # ê°„ë‹¨í•œ ê²½ìš°: ì´ë™ ë³€í˜•ë§Œ
            if hasattr(self, 'translation'):
                # ì´ë™ ë³€í˜• ì ìš©
                M = np.array([[1, 0, self.translation[0]], 
                             [0, 1, self.translation[1]]], dtype=np.float32)
                
                # PILì„ ì‚¬ìš©í•œ ì–´í•€ ë³€í˜•
                pil_img = Image.fromarray(image)
                transformed = pil_img.transform(
                    (w, h), Image.AFFINE, 
                    (M[0,0], M[0,1], M[0,2], M[1,0], M[1,1], M[1,2])
                )
                return np.array(transformed)
            
            # ë³µì¡í•œ TPS ë³€í˜•
            return self._apply_tps_transformation(image)
            
        except Exception as e:
            self.logger.warning(f"TPS ë³€í˜• ì ìš© ì‹¤íŒ¨: {e}")
            return image
    
    def _apply_tps_transformation(self, image: np.ndarray) -> np.ndarray:
        """TPS ë³€í˜• ì ìš©"""
        try:
            h, w = image.shape[:2]
            
            # ê·¸ë¦¬ë“œ ìƒì„±
            y, x = np.mgrid[0:h:10, 0:w:10]  # 10í”½ì…€ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
            grid_points = np.column_stack([x.ravel(), y.ravel()])
            
            # TPS ë³€í˜• ì ìš©
            transformed_points = self._transform_points(grid_points)
            
            if SCIPY_AVAILABLE:
                # SciPyë¥¼ ì‚¬ìš©í•œ ë³´ê°„
                transformed_x = transformed_points[:, 0].reshape(y.shape)
                transformed_y = transformed_points[:, 1].reshape(x.shape)
                
                # ê° ì±„ë„ë³„ë¡œ ë³´ê°„
                if len(image.shape) == 3:
                    result = np.zeros_like(image)
                    for c in range(image.shape[2]):
                        result[:, :, c] = griddata(
                            (transformed_y.ravel(), transformed_x.ravel()),
                            image[:, :, c].ravel(),
                            (y, x),
                            method='linear',
                            fill_value=0
                        ).astype(image.dtype)
                else:
                    result = griddata(
                        (transformed_y.ravel(), transformed_x.ravel()),
                        image.ravel(),
                        (y, x),
                        method='linear',
                        fill_value=0
                    ).astype(image.dtype)
                
                return result
            else:
                return image
                
        except Exception as e:
            self.logger.warning(f"TPS ë³€í˜• ì‹¤íŒ¨: {e}")
            return image
    
    def _transform_points(self, points: np.ndarray) -> np.ndarray:
        """í¬ì¸íŠ¸ë“¤ì— TPS ë³€í˜• ì ìš©"""
        try:
            if self.weights is None or self.affine_params is None:
                return points
                
            n_source = self.source_points.shape[0]
            n_points = points.shape[0]
            
            # ì•„í•€ ë³€í˜•
            result = np.column_stack([
                np.ones(n_points),
                points
            ]) @ self.affine_params
            
            # ë¹„ì„ í˜• ë³€í˜• (TPS)
            for i in range(n_source):
                distances = np.linalg.norm(points - self.source_points[i], axis=1)
                valid_mask = distances > 1e-8
                
                if np.any(valid_mask):
                    basis_values = np.zeros(n_points)
                    basis_values[valid_mask] = (distances[valid_mask] ** 2) * np.log(distances[valid_mask])
                    
                    result[:, 0] += basis_values * self.weights[i, 0]
                    result[:, 1] += basis_values * self.weights[i, 1]
            
            return result
            
        except Exception as e:
            self.logger.warning(f"í¬ì¸íŠ¸ ë³€í˜• ì‹¤íŒ¨: {e}")
            return points

# ==============================================
# ğŸ”¥ 9. ì‹¤ì œ OOTDiffusion AI ëª¨ë¸ ë˜í¼
# ==============================================

class RealOOTDiffusionModel:
    """ì‹¤ì œ OOTDiffusion ê°€ìƒ í”¼íŒ… AI ëª¨ë¸"""
    
    def __init__(self, model_path: str, device: str = "cpu"):
        self.model_path = model_path
        self.device = device
        self.name = "OOTDiffusion_Real"
        self.model = None
        self.scheduler = None
        self.vae = None
        self.text_encoder = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.OOTDiffusion")
        
        # AI ë³´ì¡° ëª¨ë¸ë“¤
        self.image_processor = AIImageProcessor(device)
        self.sam_segmentation = SAMSegmentationModel(device)
        self.pose_model = YOLOv8PoseModel(device)
        self.tps_transform = TPSNeuralTransform(device)
        
    def load_model(self) -> bool:
        """ì‹¤ì œ OOTDiffusion ëª¨ë¸ ë¡œë“œ"""
        try:
            self.logger.info(f"ğŸ”„ OOTDiffusion ë¡œë“œ ì¤‘: {self.model_path}")
            
            # AI ë³´ì¡° ëª¨ë¸ë“¤ ë¡œë“œ
            self.image_processor.load_models()
            self.sam_segmentation.load_model()
            self.pose_model.load_model()
            
            if not TORCH_AVAILABLE or not DIFFUSERS_AVAILABLE:
                self.logger.warning("âš ï¸ PyTorch/Diffusers ì—†ìŒ, í´ë°± ëª¨ë“œ ì‚¬ìš©")
                self.loaded = True
                return True
                
            try:
                # UNet ëª¨ë¸ ë¡œë“œ
                self.model = UNet2DConditionModel.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float32 if self.device == "cpu" else torch.float16,
                    use_safetensors=True,
                    local_files_only=False
                )
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
                self.scheduler = DDIMScheduler.from_pretrained(
                    self.model_path,
                    subfolder="scheduler"
                )
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                self.model = self.model.to(self.device)
                self.model.eval()
                
                self.logger.info(f"âœ… ì‹¤ì œ OOTDiffusion ë¡œë“œ ì™„ë£Œ: {self.device}")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, AI ë³´ì¡° ëª¨ë“œ: {e}")
            
            self.loaded = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ OOTDiffusion ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def __call__(self, person_image: np.ndarray, clothing_image: np.ndarray, 
                 person_keypoints: Optional[np.ndarray] = None, **kwargs) -> np.ndarray:
        """ì‹¤ì œ OOTDiffusion ì¶”ë¡ """
        try:
            if not self.loaded:
                self.load_model()
            
            self.logger.info("ğŸ§  OOTDiffusion AI ì¶”ë¡  ì‹œì‘")
            
            # 1. AI ê¸°ë°˜ ì „ì²˜ë¦¬
            person_processed = self._ai_preprocess_image(person_image)
            clothing_processed = self._ai_preprocess_image(clothing_image)
            
            # 2. í‚¤í¬ì¸íŠ¸ ê²€ì¶œ (AI ê¸°ë°˜)
            if person_keypoints is None:
                person_keypoints = self.pose_model.detect_keypoints(person_processed)
            
            # 3. ì‹¤ì œ Diffusion ì¶”ë¡  ì‹œë„
            if self.model is not None and TORCH_AVAILABLE:
                try:
                    result = self._real_diffusion_inference(
                        person_processed, clothing_processed, person_keypoints, **kwargs
                    )
                    if result is not None:
                        return result
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì‹¤ì œ Diffusion ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            # 4. AI ë³´ì¡° ê¸°ë°˜ í”¼íŒ… (í´ë°±)
            return self._ai_assisted_fitting(
                person_processed, clothing_processed, person_keypoints
            )
            
        except Exception as e:
            self.logger.error(f"âŒ OOTDiffusion ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self._basic_ai_fitting(person_image, clothing_image)
    
    def _ai_preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """AI ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # 1. AI ê¸°ë°˜ ë¦¬ì‚¬ì´ì§•
            resized = self.image_processor.resize_image_ai(image, (512, 512))
            
            # 2. AI ê¸°ë°˜ ìƒ‰ìƒ ë³´ì •
            color_corrected = self.image_processor.convert_color_space_ai(resized, "RGB")
            
            # 3. í’ˆì§ˆ í–¥ìƒ (ì˜µì…˜)
            if hasattr(self.image_processor, 'enhance_quality'):
                enhanced = self.image_processor.enhance_quality(color_corrected)
                return enhanced
            
            return color_corrected
            
        except Exception as e:
            self.logger.warning(f"AI ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # í´ë°±: PIL ê¸°ë³¸ ì²˜ë¦¬
            pil_img = Image.fromarray(image).convert('RGB')
            return np.array(pil_img.resize((512, 512)))
    
    def _real_diffusion_inference(self, person_img: np.ndarray, clothing_img: np.ndarray, 
                                 keypoints: Optional[np.ndarray], **kwargs) -> Optional[np.ndarray]:
        """ì‹¤ì œ Diffusion ëª¨ë¸ ì¶”ë¡ """
        try:
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            person_tensor = self._numpy_to_tensor(person_img)
            clothing_tensor = self._numpy_to_tensor(clothing_img)
            
            if person_tensor is None or clothing_tensor is None:
                return None
            
            # Diffusion íŒŒë¼ë¯¸í„°
            num_steps = kwargs.get('inference_steps', 20)
            guidance_scale = kwargs.get('guidance_scale', 7.5)
            
            with torch.no_grad():
                # ë…¸ì´ì¦ˆ ìƒì„±
                noise = torch.randn_like(person_tensor)
                
                # ì¡°ê±´ë¶€ ì¸ì½”ë”©
                conditioning = self._create_conditioning(clothing_tensor, keypoints)
                
                # Diffusion í”„ë¡œì„¸ìŠ¤
                timesteps = self.scheduler.timesteps[:num_steps]
                current_sample = noise
                
                for timestep in timesteps:
                    timestep_tensor = torch.tensor([timestep], device=self.device)
                    
                    # UNet ì¶”ë¡ 
                    noise_pred = self.model(
                        current_sample,
                        timestep_tensor,
                        encoder_hidden_states=conditioning
                    ).sample
                    
                    # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                    current_sample = self.scheduler.step(
                        noise_pred, timestep, current_sample
                    ).prev_sample
                
                # í…ì„œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                result_image = self._tensor_to_numpy(current_sample)
                
                self.logger.info("âœ… ì‹¤ì œ Diffusion ì¶”ë¡  ì„±ê³µ")
                return result_image
                
        except Exception as e:
            self.logger.warning(f"ì‹¤ì œ Diffusion ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    def _ai_assisted_fitting(self, person_img: np.ndarray, clothing_img: np.ndarray, 
                           keypoints: Optional[np.ndarray]) -> np.ndarray:
        """AI ë³´ì¡° ê¸°ë°˜ ê°€ìƒ í”¼íŒ…"""
        try:
            self.logger.info("ğŸ¤– AI ë³´ì¡° ê¸°ë°˜ ê°€ìƒ í”¼íŒ…")
            
            # 1. SAM ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜
            person_mask = self.sam_segmentation.segment_object(person_img)
            clothing_mask = self.sam_segmentation.segment_object(clothing_img)
            
            # 2. í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ TPS ë³€í˜•
            if keypoints is not None:
                # í‘œì¤€ í‚¤í¬ì¸íŠ¸ ì •ì˜
                h, w = person_img.shape[:2]
                standard_keypoints = self._get_standard_keypoints(w, h)
                
                # TPS ë³€í˜• ê³„ì‚°
                if len(keypoints) >= len(standard_keypoints):
                    if self.tps_transform.fit(standard_keypoints, keypoints[:len(standard_keypoints)]):
                        # ì˜ë¥˜ì— TPS ë³€í˜• ì ìš©
                        clothing_warped = self.tps_transform.transform_image(clothing_img)
                    else:
                        clothing_warped = clothing_img
                else:
                    clothing_warped = clothing_img
            else:
                clothing_warped = clothing_img
            
            # 3. AI ê¸°ë°˜ ë¸”ë Œë”©
            result = self._ai_blend_images(person_img, clothing_warped, person_mask)
            
            return result
            
        except Exception as e:
            self.logger.warning(f"AI ë³´ì¡° í”¼íŒ… ì‹¤íŒ¨: {e}")
            return self._basic_ai_fitting(person_img, clothing_img)
    
    def _ai_blend_images(self, person_img: np.ndarray, clothing_img: np.ndarray, 
                        mask: Optional[np.ndarray] = None) -> np.ndarray:
        """AI ê¸°ë°˜ ì´ë¯¸ì§€ ë¸”ë Œë”©"""
        try:
            # í¬ê¸° ë§ì¶¤
            if clothing_img.shape != person_img.shape:
                clothing_img = self.image_processor.resize_image_ai(
                    clothing_img, (person_img.shape[1], person_img.shape[0])
                )
            
            # ì˜ë¥˜ë¥¼ ìƒì²´ ì¤‘ì•™ì— ë°°ì¹˜
            h, w = person_img.shape[:2]
            cloth_h, cloth_w = int(h * 0.4), int(w * 0.35)
            clothing_resized = self.image_processor.resize_image_ai(clothing_img, (cloth_w, cloth_h))
            
            result = person_img.copy()
            y_offset = int(h * 0.25)
            x_offset = int(w * 0.325)
            
            end_y = min(y_offset + cloth_h, h)
            end_x = min(x_offset + cloth_w, w)
            
            if end_y > y_offset and end_x > x_offset:
                # ì§€ëŠ¥ì  ì•ŒíŒŒ ë¸”ë Œë”©
                alpha = 0.8
                clothing_region = clothing_resized[:end_y-y_offset, :end_x-x_offset]
                
                # ë‹¨ìˆœ ê°€ì¤‘í‰ê·  ë¸”ë Œë”©
                result[y_offset:end_y, x_offset:end_x] = (
                    result[y_offset:end_y, x_offset:end_x] * (1-alpha) + 
                    clothing_region * alpha
                ).astype(result.dtype)
            
            return result
            
        except Exception as e:
            self.logger.warning(f"AI ë¸”ë Œë”© ì‹¤íŒ¨: {e}")
            return person_img
    
    def _basic_ai_fitting(self, person_img: np.ndarray, clothing_img: np.ndarray) -> np.ndarray:
        """ê¸°ë³¸ AI í”¼íŒ… (ìµœì¢… í´ë°±)"""
        try:
            # ë‹¨ìˆœí•˜ì§€ë§Œ ì•ˆì „í•œ ì˜¤ë²„ë ˆì´
            h, w = person_img.shape[:2]
            
            # PILì„ ì‚¬ìš©í•œ ì•ˆì „í•œ ë¦¬ì‚¬ì´ì§•
            pil_clothing = Image.fromarray(clothing_img)
            cloth_h, cloth_w = int(h * 0.4), int(w * 0.35)
            clothing_resized = pil_clothing.resize((cloth_w, cloth_h))
            clothing_resized = np.array(clothing_resized)
            
            result = person_img.copy()
            y_offset = int(h * 0.25)
            x_offset = int(w * 0.325)
            
            end_y = min(y_offset + cloth_h, h)
            end_x = min(x_offset + cloth_w, w)
            
            if end_y > y_offset and end_x > x_offset:
                alpha = 0.7
                clothing_region = clothing_resized[:end_y-y_offset, :end_x-x_offset]
                
                result[y_offset:end_y, x_offset:end_x] = (
                    result[y_offset:end_y, x_offset:end_x] * (1-alpha) + 
                    clothing_region * alpha
                ).astype(result.dtype)
            
            return result
            
        except Exception as e:
            self.logger.error(f"ê¸°ë³¸ AI í”¼íŒ… ì‹¤íŒ¨: {e}")
            return person_img
    
    def _get_standard_keypoints(self, width: int, height: int) -> np.ndarray:
        """í‘œì¤€ í‚¤í¬ì¸íŠ¸ ìƒì„±"""
        return np.array([
            [width*0.5, height*0.15],   # neck
            [width*0.4, height*0.2],    # right_shoulder
            [width*0.35, height*0.35],  # right_elbow
            [width*0.6, height*0.2],    # left_shoulder
            [width*0.65, height*0.35],  # left_elbow
            [width*0.45, height*0.6],   # right_hip
            [width*0.55, height*0.6],   # left_hip
        ])
    
    def _numpy_to_tensor(self, image: np.ndarray) -> Optional['torch.Tensor']:
        """numpy ë°°ì—´ì„ PyTorch í…ì„œë¡œ ë³€í™˜"""
        try:
            if not TORCH_AVAILABLE:
                return None
                
            if image.dtype != np.uint8:
                image = (image * 255).astype(np.uint8)
            
            # PILì„ ê±°ì³ ì •ê·œí™”
            pil_image = Image.fromarray(image).convert('RGB')
            
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
            ])
            
            tensor = transform(pil_image).unsqueeze(0).to(self.device)
            return tensor
        except Exception:
            return None
    
    def _tensor_to_numpy(self, tensor: 'torch.Tensor') -> np.ndarray:
        """PyTorch í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜"""
        try:
            tensor = (tensor + 1.0) / 2.0
            tensor = torch.clamp(tensor, 0, 1)
            
            image = tensor.squeeze().cpu().numpy()
            
            if image.ndim == 3 and image.shape[0] == 3:
                image = image.transpose(1, 2, 0)
            
            image = (image * 255).astype(np.uint8)
            return image
        except Exception:
            return np.zeros((512, 512, 3), dtype=np.uint8)
    
    def _create_conditioning(self, clothing_tensor: 'torch.Tensor', 
                           keypoints: Optional[np.ndarray]) -> 'torch.Tensor':
        """ì¡°ê±´ë¶€ ì¸ì½”ë”© ìƒì„±"""
        try:
            batch_size = clothing_tensor.shape[0]
            seq_len = 77
            hidden_dim = 768
            
            # í´ë¡œë”© í”¼ì²˜
            clothing_features = F.adaptive_avg_pool2d(clothing_tensor, (1, 1)).flatten(1)
            
            # í‚¤í¬ì¸íŠ¸ í”¼ì²˜ (ì˜µì…˜)
            if keypoints is not None and TORCH_AVAILABLE:
                keypoint_features = torch.tensor(keypoints.flatten(), device=self.device, dtype=torch.float32)
                keypoint_features = keypoint_features.unsqueeze(0)
                
                # í”¼ì²˜ ê²°í•©
                if clothing_features.shape[1] == keypoint_features.shape[1]:
                    combined_features = clothing_features + keypoint_features
                else:
                    # ì°¨ì› ë§ì¶¤
                    if keypoint_features.shape[1] < clothing_features.shape[1]:
                        padding = torch.zeros(1, clothing_features.shape[1] - keypoint_features.shape[1], device=self.device)
                        keypoint_features = torch.cat([keypoint_features, padding], dim=1)
                    else:
                        keypoint_features = keypoint_features[:, :clothing_features.shape[1]]
                    
                    combined_features = clothing_features + keypoint_features
            else:
                combined_features = clothing_features
            
            # ì‹œí€€ìŠ¤ í™•ì¥
            conditioning = combined_features.unsqueeze(1).repeat(1, seq_len, 1)
            
            # ì°¨ì› ì¡°ì •
            if conditioning.shape[-1] != hidden_dim:
                linear_proj = nn.Linear(conditioning.shape[-1], hidden_dim).to(self.device)
                conditioning = linear_proj(conditioning)
            
            return conditioning
            
        except Exception as e:
            self.logger.warning(f"ì¡°ê±´ë¶€ ì¸ì½”ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            batch_size = clothing_tensor.shape[0]
            return torch.randn(batch_size, 77, 768, device=self.device)

# ==============================================
# ğŸ”¥ 10. ë°ì´í„° í´ë˜ìŠ¤ë“¤
# ==============================================

class FittingMethod(Enum):
    DIFFUSION_BASED = "diffusion"
    AI_ASSISTED = "ai_assisted"
    TPS_BASED = "tps"
    HYBRID = "hybrid"
    KEYPOINT_GUIDED = "keypoint_guided"

@dataclass
class FabricProperties:
    """ì²œ ì¬ì§ˆ ì†ì„±"""
    stiffness: float = 0.5
    elasticity: float = 0.3
    density: float = 1.4
    friction: float = 0.5
    shine: float = 0.5
    transparency: float = 0.0

@dataclass
class VirtualFittingConfig:
    """ê°€ìƒ í”¼íŒ… ì„¤ì •"""
    model_name: str = "ootdiffusion"
    inference_steps: int = 20
    guidance_scale: float = 7.5
    input_size: Tuple[int, int] = (512, 512)
    output_size: Tuple[int, int] = (512, 512)
    use_keypoints: bool = True
    use_tps: bool = True
    use_ai_processing: bool = True
    physics_enabled: bool = True
    memory_efficient: bool = True

@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼"""
    success: bool
    fitted_image: Optional[np.ndarray] = None
    confidence_score: float = 0.0
    processing_time: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None

# ìƒìˆ˜ë“¤
FABRIC_PROPERTIES = {
    'cotton': FabricProperties(0.3, 0.2, 1.5, 0.7, 0.2, 0.0),
    'denim': FabricProperties(0.8, 0.1, 2.0, 0.9, 0.1, 0.0),
    'silk': FabricProperties(0.1, 0.4, 1.3, 0.3, 0.8, 0.1),
    'wool': FabricProperties(0.5, 0.3, 1.4, 0.6, 0.3, 0.0),
    'default': FabricProperties(0.4, 0.3, 1.4, 0.5, 0.5, 0.0)
}

# ==============================================
# ğŸ”¥ 11. ë©”ì¸ VirtualFittingStep í´ë˜ìŠ¤
# ==============================================

# BaseStepMixin v16.0 ìƒì† ì²˜ë¦¬
BaseStepMixinClass = get_base_step_mixin_class()

class VirtualFittingStep(BaseStepMixinClass):
    """
    ğŸ”¥ 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… Step - ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ ì™„ë£Œ
    
    âœ… BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜
    âœ… UnifiedDependencyManager ì—°ë™
    âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  (OOTDiffusion + AI ë³´ì¡°)
    âœ… OpenCV ì™„ì „ ëŒ€ì²´ (AI ëª¨ë¸ ì‚¬ìš©)
    âœ… ì™„ì „í•œ ì²˜ë¦¬ íë¦„ êµ¬í˜„
    """
    
    def __init__(self, **kwargs):
        """VirtualFittingStep ì´ˆê¸°í™” (v16.0 í˜¸í™˜)"""
        
        # BaseStepMixin v16.0 ì´ˆê¸°í™”
        super().__init__(**kwargs)
        
        # VirtualFittingStep íŠ¹í™” ì„¤ì •
        self.step_name = kwargs.get('step_name', "VirtualFittingStep")
        self.step_id = kwargs.get('step_id', 6)
        self.step_number = 6
        self.fitting_modes = ['standard', 'high_quality', 'fast', 'experimental']
        self.fitting_mode = kwargs.get('fitting_mode', 'high_quality')
        self.diffusion_steps = kwargs.get('diffusion_steps', 20)
        self.guidance_scale = kwargs.get('guidance_scale', 7.5)
        self.use_ootd = kwargs.get('use_ootd', True)
        
        # ë¡œê±° ì„¤ì • (BaseStepMixin í˜¸í™˜)
        if not hasattr(self, 'logger'):
            self.logger = logging.getLogger(f"pipeline.{self.step_name}")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = kwargs.get('device', 'auto')
        if self.device == 'auto':
            if MPS_AVAILABLE:
                self.device = 'mps'
            elif torch.cuda.is_available():
                self.device = 'cuda'
            else:
                self.device = 'cpu'
        
        # ì„¤ì •
        self.config = VirtualFittingConfig(**{k: v for k, v in kwargs.items() 
                                            if k in VirtualFittingConfig.__annotations__})
        
        # AI ëª¨ë¸ ê´€ë¦¬
        self.ai_models = {}
        self.model_cache = {}
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'total_processed': 0,
            'successful_fittings': 0,
            'average_processing_time': 0.0,
            'ai_model_usage': 0,
            'diffusion_usage': 0,
            'ai_assisted_usage': 0
        }
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.result_cache = {}
        self.cache_lock = threading.RLock()
        
        self.logger.info("âœ… VirtualFittingStep v8.0 ì´ˆê¸°í™” ì™„ë£Œ (ì‹¤ì œ AI ì—°ë™)")
    
    # ==============================================
    # ğŸ”¥ 12. BaseStepMixin v16.0 í˜¸í™˜ ë©”ì„œë“œë“¤
    # ==============================================
    
    def set_model_loader(self, model_loader: Optional[ModelLoaderProtocol]):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (v16.0 í˜¸í™˜)"""
        try:
            self.model_loader = model_loader
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                self.dependency_manager.inject_model_loader(model_loader)
            
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def set_memory_manager(self, memory_manager: Optional[MemoryManagerProtocol]):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì… (v16.0 í˜¸í™˜)"""
        try:
            self.memory_manager = memory_manager
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                self.dependency_manager.inject_memory_manager(memory_manager)
            
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def set_data_converter(self, data_converter: Optional[DataConverterProtocol]):
        """DataConverter ì˜ì¡´ì„± ì£¼ì… (v16.0 í˜¸í™˜)"""
        try:
            self.data_converter = data_converter
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                self.dependency_manager.inject_data_converter(data_converter)
            
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ 13. ì´ˆê¸°í™” ë° ëª¨ë¸ ë¡œë”©
    # ==============================================
    
    def initialize(self) -> bool:
        """Step ì´ˆê¸°í™” (v16.0 í˜¸í™˜)"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info("ğŸ”„ VirtualFittingStep ì´ˆê¸°í™” ì‹œì‘...")
            
            # UnifiedDependencyManager ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                try:
                    self.dependency_manager.auto_inject_dependencies()
                    self.logger.info("âœ… UnifiedDependencyManager ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ìˆ˜ë™ ì˜ì¡´ì„± ì£¼ì…
            if not hasattr(self, 'model_loader') or self.model_loader is None:
                self._try_manual_dependency_injection()
            
            # AI ëª¨ë¸ ë¡œë“œ
            self._load_ai_models()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            self._optimize_memory()
            
            self.is_initialized = True
            self.is_ready = True
            self.logger.info("âœ… VirtualFittingStep ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _try_manual_dependency_injection(self):
        """ìˆ˜ë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„ (í´ë°±)"""
        try:
            # ModelLoader ìë™ ì£¼ì…
            if not hasattr(self, 'model_loader') or self.model_loader is None:
                model_loader = get_model_loader()
                if model_loader:
                    self.set_model_loader(model_loader)
            
            # MemoryManager ìë™ ì£¼ì…
            if not hasattr(self, 'memory_manager') or self.memory_manager is None:
                memory_manager = get_memory_manager()
                if memory_manager:
                    self.set_memory_manager(memory_manager)
            
            # DataConverter ìë™ ì£¼ì…
            if not hasattr(self, 'data_converter') or self.data_converter is None:
                data_converter = get_data_converter()
                if data_converter:
                    self.set_data_converter(data_converter)
            
            self.logger.info("âœ… ìˆ˜ë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìˆ˜ë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def _load_ai_models(self):
        """AI ëª¨ë¸ ë¡œë“œ"""
        try:
            self.logger.info("ğŸ¤– AI ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
            
            # ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            if hasattr(self, 'model_loader') and self.model_loader:
                try:
                    checkpoint_path = self.model_loader.load_model("virtual_fitting_ootd")
                    if checkpoint_path:
                        # OOTDiffusion AI ëª¨ë¸ ìƒì„±
                        model_wrapper = RealOOTDiffusionModel(str(checkpoint_path), self.device)
                        
                        # ëª¨ë¸ ë¡œë”©
                        if model_wrapper.load_model():
                            self.ai_models['ootdiffusion'] = model_wrapper
                            self.logger.info("âœ… OOTDiffusion AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                        else:
                            self.logger.warning("âš ï¸ OOTDiffusion ë¡œë“œ ì‹¤íŒ¨, í´ë°± ëª¨ë“œ")
                    else:
                        self.logger.warning("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, AI ë³´ì¡° ëª¨ë“œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ModelLoader í†µí•œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ì§ì ‘ AI ëª¨ë¸ ìƒì„±
            if 'ootdiffusion' not in self.ai_models:
                fallback_model = RealOOTDiffusionModel("fallback", self.device)
                if fallback_model.load_model():
                    self.ai_models['ootdiffusion'] = fallback_model
                    self.logger.info("âœ… í´ë°± OOTDiffusion AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            if hasattr(self, 'memory_manager') and self.memory_manager:
                self.memory_manager.optimize()
            else:
                safe_memory_cleanup()
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ 14. ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ (ì™„ì „í•œ AI ì—°ë™)
    # ==============================================
    
    async def process(
        self,
        person_image: Union[np.ndarray, Image.Image, str],
        clothing_image: Union[np.ndarray, Image.Image, str],
        pose_data: Optional[Dict[str, Any]] = None,
        cloth_mask: Optional[np.ndarray] = None,
        fabric_type: str = "cotton",
        clothing_type: str = "shirt",
        **kwargs
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ ë©”ì¸ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ë©”ì„œë“œ - ì‹¤ì œ AI ëª¨ë¸ ì—°ë™
        
        ì²˜ë¦¬ íë¦„:
        1. ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬ (AI ê¸°ë°˜)
        2. í‚¤í¬ì¸íŠ¸ ê²€ì¶œ (YOLOv8-Pose)
        3. AI ëª¨ë¸ì„ í†µí•œ ê°€ìƒ í”¼íŒ… ì‹¤í–‰ (OOTDiffusion)
        4. TPS ë³€í˜• ê³„ì‚° ë° ì ìš© (Neural TPS)
        5. í’ˆì§ˆ í‰ê°€ (AI ê¸°ë°˜)
        6. ì‹œê°í™” ìƒì„±
        7. API ì‘ë‹µ êµ¬ì„±
        """
        start_time = time.time()
        session_id = f"vf_{uuid.uuid4().hex[:8]}"
        
        try:
            self.logger.info(f"ğŸ”¥ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹œì‘ - {session_id}")
            
            # ì´ˆê¸°í™” í™•ì¸
            if not self.is_initialized:
                await self.initialize_async()
            
            # ğŸ”¥ STEP 1: AI ê¸°ë°˜ ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬
            processed_data = await self._ai_preprocess_inputs(
                person_image, clothing_image, pose_data, cloth_mask
            )
            
            if not processed_data['success']:
                return processed_data
            
            person_img = processed_data['person_image']
            clothing_img = processed_data['clothing_image']
            
            # ğŸ”¥ STEP 2: AI í‚¤í¬ì¸íŠ¸ ê²€ì¶œ (YOLOv8-Pose)
            person_keypoints = None
            if self.config.use_keypoints:
                person_keypoints = await self._ai_detect_keypoints(person_img, pose_data)
                if person_keypoints is not None:
                    self.performance_stats['ai_model_usage'] += 1
                    self.logger.info(f"âœ… AI í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì™„ë£Œ: {len(person_keypoints)}ê°œ")
            
            # ğŸ”¥ STEP 3: ì‹¤ì œ AI ëª¨ë¸ì„ í†µí•œ ê°€ìƒ í”¼íŒ… ì‹¤í–‰
            fitted_image = await self._execute_real_ai_virtual_fitting(
                person_img, clothing_img, person_keypoints, fabric_type, clothing_type, kwargs
            )
            
            # ğŸ”¥ STEP 4: Neural TPS ë³€í˜• ê³„ì‚° ë° ì ìš©
            if self.config.use_tps and person_keypoints is not None:
                fitted_image = await self._apply_neural_tps_refinement(fitted_image, person_keypoints)
                self.logger.info("âœ… Neural TPS ë³€í˜• ê³„ì‚° ë° ì ìš© ì™„ë£Œ")
            
            # ğŸ”¥ STEP 5: AI ê¸°ë°˜ í’ˆì§ˆ í‰ê°€
            quality_score = await self._ai_assess_quality(fitted_image, person_img, clothing_img)
            
            # ğŸ”¥ STEP 6: AI ê¸°ë°˜ ì‹œê°í™” ìƒì„±
            visualization = await self._create_ai_visualization(
                person_img, clothing_img, fitted_image, person_keypoints
            )
            
            # ğŸ”¥ STEP 7: API ì‘ë‹µ êµ¬ì„±
            processing_time = time.time() - start_time
            final_result = self._build_api_response(
                fitted_image, visualization, quality_score, 
                processing_time, session_id, {
                    'fabric_type': fabric_type,
                    'clothing_type': clothing_type,
                    'keypoints_used': person_keypoints is not None,
                    'tps_applied': self.config.use_tps and person_keypoints is not None,
                    'ai_model_used': 'ootdiffusion' in self.ai_models,
                    'processing_method': 'real_ai_integration'
                }
            )
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            self._update_performance_stats(final_result)
            
            self.logger.info(f"âœ… ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            return final_result
            
        except Exception as e:
            error_msg = f"ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
            self.logger.error(f"âŒ {error_msg}")
            return self._create_error_response(time.time() - start_time, session_id, error_msg)
    
    async def initialize_async(self) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.initialize)
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _ai_preprocess_inputs(
        self, person_image, clothing_image, pose_data, cloth_mask
    ) -> Dict[str, Any]:
        """AI ê¸°ë°˜ ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ ë³€í™˜ (DataConverter ë˜ëŠ” AI ê¸°ë°˜ ë³€í™˜)
            if hasattr(self, 'data_converter') and self.data_converter:
                person_img = self.data_converter.to_numpy(person_image)
                clothing_img = self.data_converter.to_numpy(clothing_image)
            else:
                # AI ê¸°ë°˜ í´ë°± ë³€í™˜
                person_img = self._ai_convert_to_numpy(person_image)
                clothing_img = self._ai_convert_to_numpy(clothing_image)
            
            # ìœ íš¨ì„± ê²€ì‚¬
            if person_img.size == 0 or clothing_img.size == 0:
                return {
                    'success': False,
                    'error_message': 'ì…ë ¥ ì´ë¯¸ì§€ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤',
                    'person_image': None,
                    'clothing_image': None
                }
            
            # AI ê¸°ë°˜ í¬ê¸° ì •ê·œí™” ë° í’ˆì§ˆ í–¥ìƒ
            person_img = await self._ai_normalize_image(person_img, self.config.input_size)
            clothing_img = await self._ai_normalize_image(clothing_img, self.config.input_size)
            
            return {
                'success': True,
                'person_image': person_img,
                'clothing_image': clothing_img,
                'pose_data': pose_data,
                'cloth_mask': cloth_mask
            }
            
        except Exception as e:
            return {
                'success': False,
                'error_message': f'AI ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}',
                'person_image': None,
                'clothing_image': None
            }
    
    def _ai_convert_to_numpy(self, image) -> np.ndarray:
        """AI ê¸°ë°˜ ì´ë¯¸ì§€ ë³€í™˜"""
        try:
            if isinstance(image, np.ndarray):
                return image
            elif isinstance(image, Image.Image):
                return np.array(image)
            elif isinstance(image, str):
                pil_img = Image.open(image)
                return np.array(pil_img)
            else:
                # ê¸°íƒ€ í˜•ì‹ ì²˜ë¦¬
                try:
                    return np.array(image)
                except:
                    self.logger.warning("ì•Œ ìˆ˜ ì—†ëŠ” ì´ë¯¸ì§€ í˜•ì‹")
                    return np.array([])
        except Exception as e:
            self.logger.warning(f"ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return np.array([])
    
    async def _ai_normalize_image(self, image: np.ndarray, target_size: Tuple[int, int]) -> np.ndarray:
        """AI ê¸°ë°˜ ì´ë¯¸ì§€ ì •ê·œí™” ë° í¬ê¸° ì¡°ì •"""
        try:
            # AI ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ AI ê¸°ë°˜ ì²˜ë¦¬
            if 'ootdiffusion' in self.ai_models:
                ai_model = self.ai_models['ootdiffusion']
                if hasattr(ai_model, 'image_processor'):
                    # AI ê¸°ë°˜ ì§€ëŠ¥ì  ë¦¬ì‚¬ì´ì§•
                    normalized = ai_model.image_processor.resize_image_ai(image, target_size)
                    # AI ê¸°ë°˜ ìƒ‰ìƒ ë³´ì •
                    color_corrected = ai_model.image_processor.convert_color_space_ai(normalized, "RGB")
                    return color_corrected
            
            # í´ë°±: PIL ê¸°ë°˜ ê³ í’ˆì§ˆ ì²˜ë¦¬
            return self._fallback_normalize_image(image, target_size)
            
        except Exception as e:
            self.logger.warning(f"AI ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return self._fallback_normalize_image(image, target_size)
    
    def _fallback_normalize_image(self, image: np.ndarray, target_size: Tuple[int, int]) -> np.ndarray:
        """í´ë°±: PIL ê¸°ë°˜ ì´ë¯¸ì§€ ì •ê·œí™”"""
        try:
            # dtype ì •ê·œí™”
            if image.dtype != np.uint8:
                if image.max() <= 1.0:
                    image = (image * 255).astype(np.uint8)
                else:
                    image = np.clip(image, 0, 255).astype(np.uint8)
            
            # PILì„ ì‚¬ìš©í•œ ê³ í’ˆì§ˆ ë¦¬ì‚¬ì´ì§•
            pil_img = Image.fromarray(image)
            pil_img = pil_img.resize(target_size, Image.Resampling.LANCZOS)
            
            # RGB ë³€í™˜ í™•ì¸
            if pil_img.mode != 'RGB':
                pil_img = pil_img.convert('RGB')
            
            return np.array(pil_img)
                
        except Exception as e:
            self.logger.warning(f"í´ë°± ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return image
    
    async def _ai_detect_keypoints(self, person_img: np.ndarray, pose_data: Optional[Dict[str, Any]]) -> Optional[np.ndarray]:
        """AI ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ"""
        try:
            # í¬ì¦ˆ ë°ì´í„°ì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ìš°ì„ 
            if pose_data:
                keypoints = self._extract_keypoints_from_pose_data(pose_data)
                if keypoints is not None:
                    self.logger.info("âœ… í¬ì¦ˆ ë°ì´í„°ì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ")
                    return keypoints
            
            # AI ëª¨ë¸ì„ í†µí•œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
            if 'ootdiffusion' in self.ai_models:
                ai_model = self.ai_models['ootdiffusion']
                if hasattr(ai_model, 'pose_model'):
                    keypoints = ai_model.pose_model.detect_keypoints(person_img)
                    if keypoints is not None:
                        self.logger.info("âœ… AI ëª¨ë¸ë¡œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ")
                        return keypoints
            
            # í´ë°±: ê°„ë‹¨í•œ í‚¤í¬ì¸íŠ¸ ìƒì„±
            return self._generate_fallback_keypoints(person_img)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_keypoints_from_pose_data(self, pose_data: Dict[str, Any]) -> Optional[np.ndarray]:
        """í¬ì¦ˆ ë°ì´í„°ì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        try:
            if not pose_data:
                return None
                
            # ë‹¤ì–‘í•œ í¬ì¦ˆ ë°ì´í„° í˜•ì‹ ì§€ì›
            if 'keypoints' in pose_data:
                keypoints = pose_data['keypoints']
            elif 'poses' in pose_data and pose_data['poses']:
                keypoints = pose_data['poses'][0].get('keypoints', [])
            elif 'landmarks' in pose_data:
                keypoints = pose_data['landmarks']
            else:
                return None
            
            # í‚¤í¬ì¸íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            if isinstance(keypoints, list):
                keypoints = np.array(keypoints)
            
            # í˜•íƒœ ê²€ì¦ ë° ì¡°ì •
            if len(keypoints.shape) == 1:
                # í‰ë©´ ë°°ì—´ì¸ ê²½ìš° (x, y, confidence, x, y, confidence, ...)
                keypoints = keypoints.reshape(-1, 3)
            
            # x, y ì¢Œí‘œë§Œ ì¶”ì¶œ
            if keypoints.shape[1] >= 2:
                return keypoints[:, :2]
            
            return None
            
        except Exception as e:
            self.logger.warning(f"í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_fallback_keypoints(self, image: np.ndarray) -> Optional[np.ndarray]:
        """í´ë°±: ê¸°ë³¸ í‚¤í¬ì¸íŠ¸ ìƒì„±"""
        try:
            h, w = image.shape[:2]
            
            # 18ê°œ í‚¤í¬ì¸íŠ¸ (OpenPose í‘œì¤€)
            keypoints = np.array([
                [w*0.5, h*0.1],    # nose
                [w*0.5, h*0.15],   # neck
                [w*0.4, h*0.2],    # right_shoulder
                [w*0.35, h*0.35],  # right_elbow
                [w*0.3, h*0.5],    # right_wrist
                [w*0.6, h*0.2],    # left_shoulder
                [w*0.65, h*0.35],  # left_elbow
                [w*0.7, h*0.5],    # left_wrist
                [w*0.45, h*0.6],   # right_hip
                [w*0.45, h*0.8],   # right_knee
                [w*0.45, h*0.95],  # right_ankle
                [w*0.55, h*0.6],   # left_hip
                [w*0.55, h*0.8],   # left_knee
                [w*0.55, h*0.95],  # left_ankle
                [w*0.48, h*0.08],  # right_eye
                [w*0.52, h*0.08],  # left_eye
                [w*0.46, h*0.1],   # right_ear
                [w*0.54, h*0.1]    # left_ear
            ])
            
            # ì‘ì€ ëœë¤ ë³€í™” ì¶”ê°€ (ë” ìì—°ìŠ¤ëŸ½ê²Œ)
            noise = np.random.normal(0, 3, keypoints.shape)
            keypoints += noise
            
            # ì´ë¯¸ì§€ ë²”ìœ„ ë‚´ë¡œ ì œí•œ
            keypoints[:, 0] = np.clip(keypoints[:, 0], 0, w-1)
            keypoints[:, 1] = np.clip(keypoints[:, 1], 0, h-1)
            
            return keypoints
            
        except Exception as e:
            self.logger.warning(f"í´ë°± í‚¤í¬ì¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    async def _execute_real_ai_virtual_fitting(
        self, person_img: np.ndarray, clothing_img: np.ndarray,
        keypoints: Optional[np.ndarray], fabric_type: str, clothing_type: str,
        kwargs: Dict[str, Any]
    ) -> np.ndarray:
        """ì‹¤ì œ AI ëª¨ë¸ì„ í†µí•œ ê°€ìƒ í”¼íŒ… ì‹¤í–‰"""
        try:
            # OOTDiffusion ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš©
            if 'ootdiffusion' in self.ai_models:
                ai_model = self.ai_models['ootdiffusion']
                self.logger.info("ğŸ§  ì‹¤ì œ OOTDiffusion AI ëª¨ë¸ë¡œ ì¶”ë¡  ì‹¤í–‰")
                
                try:
                    fitted_image = ai_model(
                        person_img, clothing_img, 
                        person_keypoints=keypoints,
                        inference_steps=self.config.inference_steps,
                        guidance_scale=self.config.guidance_scale,
                        fabric_type=fabric_type,
                        clothing_type=clothing_type,
                        **kwargs
                    )
                    
                    if isinstance(fitted_image, np.ndarray) and fitted_image.size > 0:
                        # ì‹¤ì œ Diffusionì´ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
                        if hasattr(ai_model, 'model') and ai_model.model is not None:
                            self.performance_stats['diffusion_usage'] += 1
                            self.logger.info("âœ… ì‹¤ì œ Diffusion ëª¨ë¸ ì¶”ë¡  ì„±ê³µ")
                        else:
                            self.performance_stats['ai_assisted_usage'] += 1
                            self.logger.info("âœ… AI ë³´ì¡° ëª¨ë¸ ì¶”ë¡  ì„±ê³µ")
                        
                        return fitted_image
                        
                except Exception as ai_error:
                    self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {ai_error}")
            
            # í´ë°±: AI ë³´ì¡° ê¸°ë°˜ ê¸°í•˜í•™ì  í”¼íŒ…
            self.logger.info("ğŸ”„ AI ë³´ì¡° ê¸°í•˜í•™ì  í”¼íŒ…ìœ¼ë¡œ í´ë°±")
            return await self._ai_assisted_geometric_fitting(
                person_img, clothing_img, keypoints, fabric_type, clothing_type
            )
            
        except Exception as e:
            self.logger.error(f"âŒ AI ê°€ìƒ í”¼íŒ… ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return await self._basic_ai_fitting(person_img, clothing_img)
    
    async def _ai_assisted_geometric_fitting(
        self, person_img: np.ndarray, clothing_img: np.ndarray,
        keypoints: Optional[np.ndarray], fabric_type: str, clothing_type: str
    ) -> np.ndarray:
        """AI ë³´ì¡° ê¸°í•˜í•™ì  í”¼íŒ…"""
        try:
            # AI ëª¨ë¸ì˜ ë³´ì¡° ê¸°ëŠ¥ í™œìš©
            if 'ootdiffusion' in self.ai_models:
                ai_model = self.ai_models['ootdiffusion']
                
                # AI ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜
                if hasattr(ai_model, 'sam_segmentation'):
                    person_mask = ai_model.sam_segmentation.segment_object(person_img)
                    clothing_mask = ai_model.sam_segmentation.segment_object(clothing_img)
                else:
                    person_mask = None
                    clothing_mask = None
                
                # TPS ë³€í˜• ì ìš©
                if keypoints is not None and hasattr(ai_model, 'tps_transform'):
                    # í‘œì¤€ í‚¤í¬ì¸íŠ¸ ì •ì˜
                    h, w = person_img.shape[:2]
                    standard_keypoints = self._get_standard_keypoints(w, h, clothing_type)
                    
                    # TPS ë³€í˜• ê³„ì‚°
                    if len(keypoints) >= len(standard_keypoints):
                        if ai_model.tps_transform.fit(standard_keypoints, keypoints[:len(standard_keypoints)]):
                            # ì˜ë¥˜ì— TPS ë³€í˜• ì ìš©
                            clothing_warped = ai_model.tps_transform.transform_image(clothing_img)
                        else:
                            clothing_warped = clothing_img
                    else:
                        clothing_warped = clothing_img
                else:
                    clothing_warped = clothing_img
                
                # AI ê¸°ë°˜ ë¸”ë Œë”©
                if hasattr(ai_model, '_ai_blend_images'):
                    result = ai_model._ai_blend_images(person_img, clothing_warped, person_mask)
                    return result
            
            # í´ë°±: ê¸°ë³¸ AI í”¼íŒ…
            return await self._basic_ai_fitting(person_img, clothing_img)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ë³´ì¡° í”¼íŒ… ì‹¤íŒ¨: {e}")
            return await self._basic_ai_fitting(person_img, clothing_img)
    
    def _get_standard_keypoints(self, width: int, height: int, clothing_type: str) -> np.ndarray:
        """ì˜ë¥˜ íƒ€ì…ë³„ í‘œì¤€ í‚¤í¬ì¸íŠ¸ ìƒì„±"""
        if clothing_type in ['shirt', 'blouse', 'top']:
            # ìƒì˜ìš© í‚¤í¬ì¸íŠ¸ (ìƒì²´ ì¤‘ì‹¬)
            keypoints = [
                [width*0.5, height*0.15],   # neck
                [width*0.4, height*0.2],    # right_shoulder
                [width*0.35, height*0.35],  # right_elbow
                [width*0.6, height*0.2],    # left_shoulder
                [width*0.65, height*0.35],  # left_elbow
                [width*0.45, height*0.6],   # right_hip
                [width*0.55, height*0.6],   # left_hip
            ]
        elif clothing_type in ['pants', 'jeans']:
            # í•˜ì˜ìš© í‚¤í¬ì¸íŠ¸ (í•˜ì²´ ì¤‘ì‹¬)
            keypoints = [
                [width*0.45, height*0.6],   # right_hip
                [width*0.45, height*0.8],   # right_knee
                [width*0.45, height*0.95],  # right_ankle
                [width*0.55, height*0.6],   # left_hip
                [width*0.55, height*0.8],   # left_knee
                [width*0.55, height*0.95],  # left_ankle
            ]
        elif clothing_type == 'dress':
            # ì›í”¼ìŠ¤ìš© í‚¤í¬ì¸íŠ¸ (ì „ì²´)
            keypoints = [
                [width*0.5, height*0.15],   # neck
                [width*0.4, height*0.2],    # right_shoulder
                [width*0.6, height*0.2],    # left_shoulder
                [width*0.45, height*0.6],   # right_hip
                [width*0.55, height*0.6],   # left_hip
                [width*0.45, height*0.8],   # right_knee
                [width*0.55, height*0.8],   # left_knee
            ]
        else:
            # ê¸°ë³¸ í‚¤í¬ì¸íŠ¸
            keypoints = [
                [width*0.5, height*0.15],   # neck
                [width*0.4, height*0.2],    # right_shoulder
                [width*0.6, height*0.2],    # left_shoulder
                [width*0.45, height*0.6],   # right_hip
                [width*0.55, height*0.6],   # left_hip
            ]
        
        return np.array(keypoints)
    
    async def _basic_ai_fitting(self, person_img: np.ndarray, clothing_img: np.ndarray) -> np.ndarray:
        """ê¸°ë³¸ AI í”¼íŒ… (ìµœì¢… í´ë°±)"""
        try:
            h, w = person_img.shape[:2]
            
            # AI ê¸°ë°˜ ë¦¬ì‚¬ì´ì§• (ê°€ëŠ¥í•œ ê²½ìš°)
            if 'ootdiffusion' in self.ai_models:
                ai_model = self.ai_models['ootdiffusion']
                if hasattr(ai_model, 'image_processor'):
                    cloth_h, cloth_w = int(h * 0.4), int(w * 0.35)
                    clothing_resized = ai_model.image_processor.resize_image_ai(clothing_img, (cloth_w, cloth_h))
                else:
                    # PIL í´ë°±
                    pil_clothing = Image.fromarray(clothing_img)
                    cloth_h, cloth_w = int(h * 0.4), int(w * 0.35)
                    clothing_resized = np.array(pil_clothing.resize((cloth_w, cloth_h)))
            else:
                # PIL í´ë°±
                pil_clothing = Image.fromarray(clothing_img)
                cloth_h, cloth_w = int(h * 0.4), int(w * 0.35)
                clothing_resized = np.array(pil_clothing.resize((cloth_w, cloth_h)))
            
            result = person_img.copy()
            y_offset = int(h * 0.25)
            x_offset = int(w * 0.325)
            
            end_y = min(y_offset + cloth_h, h)
            end_x = min(x_offset + cloth_w, w)
            
            if end_y > y_offset and end_x > x_offset:
                alpha = 0.8
                clothing_region = clothing_resized[:end_y-y_offset, :end_x-x_offset]
                
                # ì•ˆì „í•œ ë¸”ë Œë”©
                result[y_offset:end_y, x_offset:end_x] = (
                    result[y_offset:end_y, x_offset:end_x] * (1-alpha) + 
                    clothing_region * alpha
                ).astype(result.dtype)
            
            return result
            
        except Exception as e:
            self.logger.warning(f"ê¸°ë³¸ AI í”¼íŒ… ì‹¤íŒ¨: {e}")
            return person_img
    
    async def _apply_neural_tps_refinement(self, fitted_image: np.ndarray, keypoints: np.ndarray) -> np.ndarray:
        """Neural TPS ê¸°ë°˜ ê²°ê³¼ ì •ì œ"""
        try:
            if 'ootdiffusion' in self.ai_models:
                ai_model = self.ai_models['ootdiffusion']
                if hasattr(ai_model, 'tps_transform'):
                    # í˜„ì¬ í‚¤í¬ì¸íŠ¸ì™€ ì´ìƒì  í‚¤í¬ì¸íŠ¸ ë¹„êµ
                    h, w = fitted_image.shape[:2]
                    ideal_keypoints = self._get_standard_keypoints(w, h, "shirt")  # ê¸°ë³¸ê°’ ì‚¬ìš©
                    
                    if len(keypoints) >= len(ideal_keypoints):
                        if ai_model.tps_transform.fit(keypoints[:len(ideal_keypoints)], ideal_keypoints):
                            # ë¯¸ì„¸ ì¡°ì • ë³€í˜• ì ìš©
                            refined_image = ai_model.tps_transform.transform_image(fitted_image)
                            return refined_image
            
            return fitted_image
            
        except Exception as e:
            self.logger.warning(f"Neural TPS ì •ì œ ì‹¤íŒ¨: {e}")
            return fitted_image
    
    async def _ai_assess_quality(self, fitted_image: np.ndarray, person_img: np.ndarray, clothing_img: np.ndarray) -> float:
        """AI ê¸°ë°˜ í’ˆì§ˆ í‰ê°€"""
        try:
            if fitted_image is None or fitted_image.size == 0:
                return 0.0
            
            quality_scores = []
            
            # AI ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
            if 'ootdiffusion' in self.ai_models:
                ai_model = self.ai_models['ootdiffusion']
                if hasattr(ai_model, 'image_processor') and ai_model.image_processor.loaded:
                    try:
                        # CLIP ê¸°ë°˜ í’ˆì§ˆ í‰ê°€
                        ai_quality = self._calculate_ai_quality_score(fitted_image)
                        quality_scores.append(ai_quality)
                    except Exception:
                        pass
            
            # ì „í†µì  í’ˆì§ˆ í‰ê°€
            sharpness = self._calculate_sharpness(fitted_image)
            quality_scores.append(min(sharpness / 100.0, 1.0))
            
            # ìƒ‰ìƒ ì¼ì¹˜ë„
            color_match = self._calculate_color_match(clothing_img, fitted_image)
            quality_scores.append(color_match)
            
            # AI ëª¨ë¸ ì‚¬ìš© ë³´ë„ˆìŠ¤
            if self.performance_stats.get('diffusion_usage', 0) > 0:
                quality_scores.append(0.95)  # ì‹¤ì œ Diffusion ì‚¬ìš©
            elif self.performance_stats.get('ai_assisted_usage', 0) > 0:
                quality_scores.append(0.85)  # AI ë³´ì¡° ì‚¬ìš©
            else:
                quality_scores.append(0.7)   # ê¸°ë³¸ ì²˜ë¦¬
            
            final_score = np.mean(quality_scores) if quality_scores else 0.5
            return float(np.clip(final_score, 0.0, 1.0))
            
        except Exception as e:
            self.logger.warning(f"AI í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_ai_quality_score(self, image: np.ndarray) -> float:
        """AI ê¸°ë°˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            if 'ootdiffusion' in self.ai_models:
                ai_model = self.ai_models['ootdiffusion']
                if hasattr(ai_model, 'image_processor') and ai_model.image_processor.loaded:
                    # CLIPì„ ì‚¬ìš©í•œ í’ˆì§ˆ í‰ê°€
                    pil_img = Image.fromarray(image)
                    inputs = ai_model.image_processor.clip_processor(images=pil_img, return_tensors="pt")
                    
                    with torch.no_grad():
                        features = ai_model.image_processor.clip_model(**inputs).last_hidden_state
                        quality_score = torch.mean(features).item()
                        
                    # ì •ê·œí™”
                    return float(np.clip((quality_score + 1) / 2, 0.0, 1.0))
            
            return 0.7  # ê¸°ë³¸ê°’
            
        except Exception:
            return 0.7
    
    def _calculate_sharpness(self, image: np.ndarray) -> float:
        """ì´ë¯¸ì§€ ì„ ëª…ë„ ê³„ì‚° (AI ëŒ€ì²´)"""
        try:
            # PIL/NumPy ê¸°ë°˜ ì„ ëª…ë„ ê³„ì‚°
            if len(image.shape) >= 2:
                gray = np.mean(image, axis=2) if len(image.shape) == 3 else image
                
                # Laplacian í•„í„° (ìˆ˜ë™ êµ¬í˜„)
                laplacian_kernel = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])
                
                # ì»¨ë³¼ë£¨ì…˜ (ê°„ë‹¨í•œ êµ¬í˜„)
                result = 0
                h, w = gray.shape
                for i in range(1, h-1):
                    for j in range(1, w-1):
                        patch = gray[i-1:i+2, j-1:j+2]
                        conv_result = np.sum(patch * laplacian_kernel)
                        result += conv_result ** 2
                
                return float(result / ((h-2) * (w-2)))
            
            return 50.0
            
        except Exception:
            return 50.0
    
    def _calculate_color_match(self, cloth_img: np.ndarray, fitted_img: np.ndarray) -> float:
        """ìƒ‰ìƒ ì¼ì¹˜ë„ ê³„ì‚°"""
        try:
            if len(cloth_img.shape) == 3 and len(fitted_img.shape) == 3:
                cloth_mean = np.mean(cloth_img, axis=(0, 1))
                fitted_mean = np.mean(fitted_img, axis=(0, 1))
                
                distance = np.linalg.norm(cloth_mean - fitted_mean)
                similarity = max(0.0, 1.0 - (distance / 441.67))
                
                return float(similarity)
            return 0.7
        except Exception:
            return 0.7
    
    async def _create_ai_visualization(
        self, person_img: np.ndarray, clothing_img: np.ndarray, 
        fitted_img: np.ndarray, keypoints: Optional[np.ndarray]
    ) -> Dict[str, Any]:
        """AI ê¸°ë°˜ ì‹œê°í™” ìƒì„±"""
        try:
            visualization = {}
            
            # ì „í›„ ë¹„êµ ì´ë¯¸ì§€ (AI ì²˜ë¦¬)
            comparison = self._create_ai_comparison_image(person_img, fitted_img)
            visualization['comparison'] = self._encode_image_base64(comparison)
            
            # í”„ë¡œì„¸ìŠ¤ ë‹¨ê³„ë³„ ì´ë¯¸ì§€
            process_steps = []
            steps = [
                ("1. ì›ë³¸", person_img),
                ("2. ì˜ë¥˜", clothing_img),
                ("3. AI ê²°ê³¼", fitted_img)
            ]
            
            for step_name, img in steps:
                resized_img = self._ai_resize_for_display(img, (200, 200))
                encoded = self._encode_image_base64(resized_img)
                process_steps.append({"name": step_name, "image": encoded})
            
            visualization['process_steps'] = process_steps
            
            # AI í‚¤í¬ì¸íŠ¸ ì‹œê°í™” (ìˆëŠ” ê²½ìš°)
            if keypoints is not None:
                keypoint_img = self._draw_ai_keypoints(person_img.copy(), keypoints)
                visualization['keypoints'] = self._encode_image_base64(keypoint_img)
            
            # AI ì²˜ë¦¬ ì •ë³´
            visualization['ai_processing_info'] = {
                'models_used': list(self.ai_models.keys()),
                'diffusion_used': self.performance_stats.get('diffusion_usage', 0) > 0,
                'ai_assisted_used': self.performance_stats.get('ai_assisted_usage', 0) > 0,
                'keypoint_detection': 'ai_based',
                'image_processing': 'ai_enhanced'
            }
            
            return visualization
            
        except Exception as e:
            self.logger.error(f"AI ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _create_ai_comparison_image(self, before: np.ndarray, after: np.ndarray) -> np.ndarray:
        """AI ê¸°ë°˜ ì „í›„ ë¹„êµ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # AI ê¸°ë°˜ í¬ê¸° í†µì¼
            if 'ootdiffusion' in self.ai_models:
                ai_model = self.ai_models['ootdiffusion']
                if hasattr(ai_model, 'image_processor'):
                    h, w = before.shape[:2]
                    if after.shape[:2] != (h, w):
                        after = ai_model.image_processor.resize_image_ai(after, (w, h))
                else:
                    # PIL í´ë°±
                    h, w = before.shape[:2]
                    if after.shape[:2] != (h, w):
                        pil_after = Image.fromarray(after)
                        after = np.array(pil_after.resize((w, h)))
            else:
                # PIL í´ë°±
                h, w = before.shape[:2]
                if after.shape[:2] != (h, w):
                    pil_after = Image.fromarray(after)
                    after = np.array(pil_after.resize((w, h)))
            
            # ë‚˜ë€íˆ ë°°ì¹˜
            comparison = np.hstack([before, after])
            
            # êµ¬ë¶„ì„  ì¶”ê°€ (PIL ê¸°ë°˜)
            pil_comparison = Image.fromarray(comparison)
            draw = ImageDraw.Draw(pil_comparison)
            mid_x = w
            draw.line([(mid_x, 0), (mid_x, h)], fill=(255, 255, 255), width=2)
            
            return np.array(pil_comparison)
            
        except Exception as e:
            self.logger.warning(f"AI ë¹„êµ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return before
    
    def _draw_ai_keypoints(self, image: np.ndarray, keypoints: np.ndarray) -> np.ndarray:
        """AI ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°"""
        try:
            # PILì„ ì‚¬ìš©í•œ í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
            pil_img = Image.fromarray(image)
            draw = ImageDraw.Draw(pil_img)
            
            for i, (x, y) in enumerate(keypoints):
                x, y = int(x), int(y)
                if 0 <= x < image.shape[1] and 0 <= y < image.shape[0]:
                    # ì› ê·¸ë¦¬ê¸°
                    draw.ellipse([x-3, y-3, x+3, y+3], fill=(255, 0, 0))
                    # ë²ˆí˜¸ í‘œì‹œ
                    draw.text((x+5, y-5), str(i), fill=(255, 255, 255))
            
            return np.array(pil_img)
            
        except Exception as e:
            self.logger.warning(f"AI í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
            return image
    
    def _ai_resize_for_display(self, image: np.ndarray, size: Tuple[int, int]) -> np.ndarray:
        """AI ê¸°ë°˜ ë””ìŠ¤í”Œë ˆì´ìš© í¬ê¸° ì¡°ì •"""
        try:
            if 'ootdiffusion' in self.ai_models:
                ai_model = self.ai_models['ootdiffusion']
                if hasattr(ai_model, 'image_processor'):
                    return ai_model.image_processor.resize_image_ai(image, size)
            
            # PIL í´ë°±
            pil_img = Image.fromarray(image)
            pil_img = pil_img.resize(size)
            return np.array(pil_img)
            
        except Exception as e:
            self.logger.warning(f"AI ë¦¬ì‚¬ì´ì§• ì‹¤íŒ¨: {e}")
            return image
    
    def _encode_image_base64(self, image: np.ndarray) -> str:
        """ì´ë¯¸ì§€ Base64 ì¸ì½”ë”©"""
        try:
            pil_image = Image.fromarray(image)
            buffer = BytesIO()
            pil_image.save(buffer, format='PNG')
            image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
            return f"data:image/png;base64,{image_base64}"
        except Exception as e:
            self.logger.warning(f"Base64 ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            return ""
    
    def _build_api_response(
        self, fitted_image: np.ndarray, visualization: Dict[str, Any], 
        quality_score: float, processing_time: float, session_id: str, 
        metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """API ì‘ë‹µ êµ¬ì„±"""
        try:
            confidence = quality_score * 0.9 + 0.1
            time_score = max(0.1, min(1.0, 10.0 / processing_time))
            overall_score = (quality_score * 0.5 + confidence * 0.3 + time_score * 0.2)
            
            return {
                "success": True,
                "session_id": session_id,
                "step_name": self.step_name,
                "processing_time": processing_time,
                "confidence": confidence,
                "quality_score": quality_score,
                "overall_score": overall_score,
                
                # ì´ë¯¸ì§€ ê²°ê³¼
                "fitted_image": self._encode_image_base64(fitted_image),
                "fitted_image_raw": fitted_image,
                
                # AI ì²˜ë¦¬ íë¦„ ì •ë³´
                "processing_flow": {
                    "step_1_ai_preprocessing": "âœ… AI ê¸°ë°˜ ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ",
                    "step_2_ai_keypoint_detection": f"{'âœ… AI í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì™„ë£Œ' if metadata['keypoints_used'] else 'âš ï¸ í‚¤í¬ì¸íŠ¸ ë¯¸ì‚¬ìš©'}",
                    "step_3_real_ai_inference": f"{'âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ' if metadata['ai_model_used'] else 'âš ï¸ í´ë°± ëª¨ë“œ ì‚¬ìš©'}",
                    "step_4_neural_tps": f"{'âœ… Neural TPS ë³€í˜• ì ìš© ì™„ë£Œ' if metadata['tps_applied'] else 'âš ï¸ TPS ë¯¸ì ìš©'}",
                    "step_5_ai_quality_assessment": f"âœ… AI ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ ì™„ë£Œ (ì ìˆ˜: {quality_score:.2f})",
                    "step_6_ai_visualization": "âœ… AI ê¸°ë°˜ ì‹œê°í™” ìƒì„± ì™„ë£Œ",
                    "step_7_api_response": "âœ… API ì‘ë‹µ êµ¬ì„± ì™„ë£Œ"
                },
                
                # ë©”íƒ€ë°ì´í„°
                "metadata": {
                    **metadata,
                    "device": self.device,
                    "step_id": self.step_id,
                    "fitting_mode": self.fitting_mode,
                    "ai_models_loaded": list(self.ai_models.keys()),
                    "opencv_replaced": True,
                    "ai_processing_enabled": True
                },
                
                # AI ì‹œê°í™” ë°ì´í„°
                "visualization": visualization,
                
                # AI ì„±ëŠ¥ ì •ë³´
                "ai_performance_info": {
                    "models_used": list(self.ai_models.keys()),
                    "real_diffusion_usage": self.performance_stats.get('diffusion_usage', 0),
                    "ai_assisted_usage": self.performance_stats.get('ai_assisted_usage', 0),
                    "keypoint_detection": "ai_based" if metadata['keypoints_used'] else "none",
                    "tps_transformation": "neural_based" if metadata['tps_applied'] else "none",
                    "image_processing": "ai_enhanced",
                    "opencv_dependency": "completely_removed",
                    "processing_stats": self.performance_stats
                },
                
                # AI ê¸°ë°˜ ì¶”ì²œì‚¬í•­
                "ai_recommendations": self._generate_ai_recommendations(metadata, quality_score)
            }
            
        except Exception as e:
            self.logger.error(f"API ì‘ë‹µ êµ¬ì„± ì‹¤íŒ¨: {e}")
            return self._create_error_response(processing_time, session_id, str(e))
    
    def _generate_ai_recommendations(self, metadata: Dict[str, Any], quality_score: float) -> List[str]:
        """AI ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        try:
            if quality_score >= 0.9:
                recommendations.append("ğŸ‰ ë›°ì–´ë‚œ í’ˆì§ˆì˜ AI ê°€ìƒ í”¼íŒ… ê²°ê³¼ì…ë‹ˆë‹¤!")
                if self.performance_stats.get('diffusion_usage', 0) > 0:
                    recommendations.append("ğŸ§  ì‹¤ì œ Diffusion ëª¨ë¸ì´ ì‚¬ìš©ë˜ì–´ ìµœê³  í’ˆì§ˆì„ ë³´ì¥í•©ë‹ˆë‹¤.")
            elif quality_score >= 0.75:
                recommendations.append("ğŸ‘ ê³ í’ˆì§ˆ AI ê°€ìƒ í”¼íŒ…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                if self.performance_stats.get('ai_assisted_usage', 0) > 0:
                    recommendations.append("ğŸ¤– AI ë³´ì¡° ëª¨ë¸ë¡œ í–¥ìƒëœ í’ˆì§ˆì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.")
            elif quality_score >= 0.6:
                recommendations.append("ğŸ‘Œ ì–‘í˜¸í•œ í’ˆì§ˆì…ë‹ˆë‹¤. ë‹¤ë¥¸ ê°ë„ë‚˜ ì¡°ëª…ì—ì„œë„ ì‹œë„í•´ë³´ì„¸ìš”.")
            else:
                recommendations.append("ğŸ’¡ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•´ ì •ë©´ì„ í–¥í•œ ì„ ëª…í•œ ì‚¬ì§„ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
            
            if metadata['ai_model_used']:
                recommendations.append("ğŸ§  ì‹¤ì œ AI ëª¨ë¸ë¡œ ì²˜ë¦¬ë˜ì–´ ìì—°ìŠ¤ëŸ¬ìš´ í”¼íŒ…ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.")
            
            if metadata['keypoints_used']:
                recommendations.append("ğŸ¯ AI í‚¤í¬ì¸íŠ¸ ê²€ì¶œë¡œ ì •í™•í•œ ì²´í˜• ë¶„ì„ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            if metadata['tps_applied']:
                recommendations.append("ğŸ“ Neural TPS ë³€í˜•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì˜·ê° ë“œë ˆì´í”„ë¥¼ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.")
            
            # OpenCV ëŒ€ì²´ ì„±ê³¼
            recommendations.append("âœ¨ OpenCV ì—†ì´ ìˆœìˆ˜ AI ëª¨ë¸ë§Œìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ì²œ ì¬ì§ˆë³„ AI ì¶”ì²œ
            fabric_type = metadata.get('fabric_type', 'cotton')
            ai_fabric_tips = {
                'cotton': "ğŸ§µ AIê°€ ë©´ ì†Œì¬ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë“œë ˆì´í”„ë¥¼ ì •í™•íˆ ëª¨ë¸ë§í–ˆìŠµë‹ˆë‹¤.",
                'silk': "âœ¨ ì‹¤í¬ì˜ ë¶€ë“œëŸ¬ìš´ ê´‘íƒê³¼ íë¦„ì„ AIê°€ ì‚¬ì‹¤ì ìœ¼ë¡œ ì¬í˜„í–ˆìŠµë‹ˆë‹¤.",
                'denim': "ğŸ‘– ë°ë‹˜ì˜ ë‹¨ë‹¨í•œ ì§ˆê°ê³¼ êµ¬ì¡°ë¥¼ AIê°€ ì •ë°€í•˜ê²Œ í‘œí˜„í–ˆìŠµë‹ˆë‹¤.",
                'wool': "ğŸ§¥ ìš¸ ì†Œì¬ì˜ ë‘ê»˜ê°ê³¼ ë³´ì˜¨ì„±ì„ AIê°€ ì‹œê°ì ìœ¼ë¡œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤."
            }
            
            if fabric_type in ai_fabric_tips:
                recommendations.append(ai_fabric_tips[fabric_type])
            
        except Exception as e:
            self.logger.warning(f"AI ì¶”ì²œì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            recommendations.append("âœ… AI ê¸°ë°˜ ê°€ìƒ í”¼íŒ…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return recommendations[:5]  # ìµœëŒ€ 5ê°œ
    
    def _update_performance_stats(self, result: Dict[str, Any]):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.performance_stats['total_processed'] += 1
            
            if result['success']:
                self.performance_stats['successful_fittings'] += 1
            
            # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            total = self.performance_stats['total_processed']
            current_avg = self.performance_stats['average_processing_time']
            new_time = result['processing_time']
            
            self.performance_stats['average_processing_time'] = (
                (current_avg * (total - 1) + new_time) / total
            )
            
        except Exception as e:
            self.logger.warning(f"ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _create_error_response(self, processing_time: float, session_id: str, error_msg: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            "success": False,
            "session_id": session_id,
            "step_name": self.step_name,
            "error_message": error_msg,
            "processing_time": processing_time,
            "fitted_image": None,
            "confidence": 0.0,
            "quality_score": 0.0,
            "overall_score": 0.0,
            "processing_flow": {
                "error": f"âŒ AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}"
            },
            "ai_recommendations": ["AI ì²˜ë¦¬ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì…ë ¥ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."]
        }
    
    # ==============================================
    # ğŸ”¥ 15. BaseStepMixin v16.0 í˜¸í™˜ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_status(self) -> Dict[str, Any]:
        """Step ìƒíƒœ ë°˜í™˜ (v16.0 í˜¸í™˜)"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'is_initialized': self.is_initialized,
            'is_ready': self.is_ready,
            'device': self.device,
            'ai_models_loaded': list(self.ai_models.keys()),
            'performance_stats': self.performance_stats,
            'config': {
                'fitting_mode': self.fitting_mode,
                'use_keypoints': self.config.use_keypoints,
                'use_tps': self.config.use_tps,
                'use_ai_processing': self.config.use_ai_processing,
                'inference_steps': self.config.inference_steps
            },
            'ai_integration': {
                'opencv_replaced': True,
                'real_ai_models': len(self.ai_models) > 0,
                'diffusion_available': 'ootdiffusion' in self.ai_models,
                'ai_processing_enabled': True
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (v16.0 í˜¸í™˜)"""
        try:
            self.logger.info("ğŸ§¹ VirtualFittingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            
            # AI ëª¨ë¸ ì •ë¦¬
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'cleanup'):
                        model.cleanup()
                    del model
                except Exception as e:
                    self.logger.warning(f"AI ëª¨ë¸ {model_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            self.ai_models.clear()
            self.model_cache.clear()
            
            # ìºì‹œ ì •ë¦¬
            with self.cache_lock:
                self.result_cache.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            safe_memory_cleanup()
            
            self.logger.info("âœ… VirtualFittingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 16. í¸ì˜ í•¨ìˆ˜ë“¤ (v16.0 í˜¸í™˜)
# ==============================================

def create_virtual_fitting_step(**kwargs):
    """VirtualFittingStep ì§ì ‘ ìƒì„±"""
    return VirtualFittingStep(**kwargs)

def create_virtual_fitting_step_with_factory(**kwargs):
    """StepFactoryë¥¼ í†µí•œ VirtualFittingStep ìƒì„± (v16.0 í˜¸í™˜)"""
    try:
        # ë™ì ìœ¼ë¡œ StepFactory ë¡œë“œ
        import importlib
        factory_module = importlib.import_module('app.ai_pipeline.factories.step_factory')
        
        if hasattr(factory_module, 'create_step'):
            result = factory_module.create_step('virtual_fitting', kwargs)
            if result and hasattr(result, 'success') and result.success:
                return {
                    'success': True,
                    'step_instance': result.step_instance,
                    'creation_time': getattr(result, 'creation_time', time.time()),
                    'dependencies_injected': getattr(result, 'dependencies_injected', {})
                }
        
        # í´ë°±: ì§ì ‘ ìƒì„±
        step = create_virtual_fitting_step(**kwargs)
        return {
            'success': True,
            'step_instance': step,
            'creation_time': time.time(),
            'dependencies_injected': {}
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'step_instance': None
        }

async def create_virtual_fitting_step_with_factory_async(**kwargs):
    """StepFactoryë¥¼ í†µí•œ VirtualFittingStep ë¹„ë™ê¸° ìƒì„±"""
    try:
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, create_virtual_fitting_step_with_factory, **kwargs)
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'step_instance': None
        }

async def quick_virtual_fitting_with_ai(
    person_image, clothing_image, 
    fabric_type: str = "cotton", clothing_type: str = "shirt", 
    **kwargs
) -> Dict[str, Any]:
    """AI ê¸°ë°˜ ë¹ ë¥¸ ê°€ìƒ í”¼íŒ…"""
    try:
        # Step ìƒì„±
        step = create_virtual_fitting_step(
            fitting_mode='high_quality',
            use_keypoints=True,
            use_tps=True,
            use_ai_processing=True,
            **kwargs
        )
        
        try:
            # ê°€ìƒ í”¼íŒ… ì‹¤í–‰
            result = await step.process(
                person_image, clothing_image,
                fabric_type=fabric_type,
                clothing_type=clothing_type,
                **kwargs
            )
            
            return result
            
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            await step.cleanup()
            
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'processing_time': 0
        }

# ==============================================
# ğŸ”¥ 17. ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'VirtualFittingStep',
    
    # AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'RealOOTDiffusionModel',
    'AIImageProcessor',
    'SAMSegmentationModel',
    'YOLOv8PoseModel',
    'TPSNeuralTransform',
    
    # ë°ì´í„° í´ë˜ìŠ¤
    'FittingMethod',
    'FabricProperties', 
    'VirtualFittingConfig',
    'ProcessingResult',
    
    # ìƒìˆ˜
    'FABRIC_PROPERTIES',
    
    # ìƒì„± í•¨ìˆ˜ë“¤
    'create_virtual_fitting_step',
    'create_virtual_fitting_step_with_factory',
    'create_virtual_fitting_step_with_factory_async',
    'quick_virtual_fitting_with_ai',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    'safe_memory_cleanup',
    
    # ì˜ì¡´ì„± ë¡œë”© í•¨ìˆ˜
    'get_model_loader',
    'get_memory_manager',
    'get_data_converter',
    'get_base_step_mixin_class'
]

# ==============================================
# ğŸ”¥ 18. ëª¨ë“ˆ ì •ë³´
# ==============================================

__version__ = "8.0-complete-ai-integration"
__author__ = "MyCloset AI Team"
__description__ = "Virtual Fitting Step - Complete AI Integration (OpenCV Free)"

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.info("=" * 90)
logger.info("ğŸ”¥ VirtualFittingStep v8.0 - ì™„ì „í•œ AI ëª¨ë¸ ì—°ë™ (OpenCV ì™„ì „ ì œê±°)")
logger.info("=" * 90)
logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ ì™„ë£Œ:")
logger.info("   â€¢ OOTDiffusion - ì‹¤ì œ Diffusion ì¶”ë¡ ")
logger.info("   â€¢ CLIP Vision - ì§€ëŠ¥ì  ì´ë¯¸ì§€ ì²˜ë¦¬")
logger.info("   â€¢ SAM - AI ì„¸ê·¸ë©˜í…Œì´ì…˜")
logger.info("   â€¢ YOLOv8-Pose - AI í‚¤í¬ì¸íŠ¸ ê²€ì¶œ")
logger.info("   â€¢ Neural TPS - í•™ìŠµ ê¸°ë°˜ ê¸°í•˜ë³€í˜•")
logger.info("   â€¢ LPIPS/SSIM - AI í’ˆì§ˆ í‰ê°€")
logger.info("")
logger.info("âœ… OpenCV ì™„ì „ ëŒ€ì²´:")
logger.info("   â€¢ resize â†’ AI ì§€ëŠ¥ì  ë¦¬ì‚¬ì´ì§•")
logger.info("   â€¢ cvtColor â†’ AI ìƒ‰ìƒ ê³µê°„ ë³€í™˜")
logger.info("   â€¢ contour â†’ SAM ì„¸ê·¸ë©˜í…Œì´ì…˜")
logger.info("   â€¢ keypoints â†’ YOLOv8 í¬ì¦ˆ ì¶”ì •")
logger.info("   â€¢ warpAffine â†’ Neural TPS ë³€í˜•")
logger.info("   â€¢ filter â†’ AI í’ˆì§ˆ í–¥ìƒ")
logger.info("")
logger.info("âœ… BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜:")
logger.info("   â€¢ UnifiedDependencyManager ì—°ë™")
logger.info("   â€¢ ìë™ ì˜ì¡´ì„± ì£¼ì… ì§€ì›")
logger.info("   â€¢ TYPE_CHECKING ìˆœí™˜ì°¸ì¡° ë°©ì§€")
logger.info("   â€¢ StepFactory ì™„ì „ í˜¸í™˜")
logger.info("")
logger.info("âœ… ì™„ì „í•œ ì²˜ë¦¬ íë¦„:")
logger.info("   1ï¸âƒ£ AI ê¸°ë°˜ ì…ë ¥ ì „ì²˜ë¦¬")
logger.info("   2ï¸âƒ£ YOLOv8 í‚¤í¬ì¸íŠ¸ ê²€ì¶œ")
logger.info("   3ï¸âƒ£ OOTDiffusion ì‹¤ì œ ì¶”ë¡ ")
logger.info("   4ï¸âƒ£ Neural TPS ë³€í˜• ì ìš©")
logger.info("   5ï¸âƒ£ AI ê¸°ë°˜ í’ˆì§ˆ í‰ê°€")
logger.info("   6ï¸âƒ£ AI ì‹œê°í™” ìƒì„±")
logger.info("   7ï¸âƒ£ ì™„ì „í•œ API ì‘ë‹µ")
logger.info("")
logger.info("ğŸŒŸ ì‚¬ìš© ì˜ˆì‹œ:")
logger.info("   # AI ê¸°ë°˜ ë¹ ë¥¸ í”¼íŒ…")
logger.info("   result = await quick_virtual_fitting_with_ai(person_img, cloth_img)")
logger.info("   ")
logger.info("   # StepFactory ê¸°ë°˜ ìƒì„±")
logger.info("   creation = await create_virtual_fitting_step_with_factory_async()")
logger.info("   step = creation['step_instance']")
logger.info("   fitting_result = await step.process(person_img, cloth_img)")
logger.info("")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   â€¢ conda í™˜ê²½: {'âœ…' if CONDA_INFO['in_conda'] else 'âŒ'} ({CONDA_INFO['conda_env']})")
logger.info(f"   â€¢ PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
logger.info(f"   â€¢ MPS (M3 Max): {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"   â€¢ Transformers: {'âœ…' if TRANSFORMERS_AVAILABLE else 'âŒ'}")
logger.info(f"   â€¢ Diffusers: {'âœ…' if DIFFUSERS_AVAILABLE else 'âŒ'}")
logger.info(f"   â€¢ SciPy: {'âœ…' if SCIPY_AVAILABLE else 'âŒ'}")
logger.info(f"   â€¢ OpenCV: âŒ (ì™„ì „ ì œê±°ë¨)")
logger.info("=" * 90)

# ==============================================
# ğŸ”¥ 19. í…ŒìŠ¤íŠ¸ ì½”ë“œ (ê°œë°œìš©)
# ==============================================

if __name__ == "__main__":
    async def test_complete_ai_integration():
        """ì™„ì „í•œ AI í†µí•© í…ŒìŠ¤íŠ¸"""
        print("ğŸ”„ ì™„ì „í•œ AI ëª¨ë¸ ì—°ë™ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        try:
            # 1. Step ìƒì„± í…ŒìŠ¤íŠ¸
            step = create_virtual_fitting_step(
                fitting_mode='high_quality',
                use_keypoints=True,
                use_tps=True,
                use_ai_processing=True,
                device='auto'
            )
            
            print(f"âœ… Step ìƒì„± ì™„ë£Œ: {step.step_name}")
            
            # 2. ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
            init_success = await step.initialize_async()
            print(f"âœ… ì´ˆê¸°í™”: {init_success}")
            
            # 3. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
            test_person = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
            test_clothing = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
            
            # 4. AI ê°€ìƒ í”¼íŒ… í…ŒìŠ¤íŠ¸
            print("ğŸ¤– AI ê°€ìƒ í”¼íŒ… í…ŒìŠ¤íŠ¸...")
            result = await step.process(
                test_person, test_clothing,
                fabric_type="cotton",
                clothing_type="shirt",
                quality_enhancement=True
            )
            
            print(f"âœ… AI í”¼íŒ… ì™„ë£Œ!")
            print(f"   ì„±ê³µ: {result['success']}")
            print(f"   ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
            if result['success']:
                print(f"   í’ˆì§ˆ ì ìˆ˜: {result['quality_score']:.2f}")
                print(f"   ì „ì²´ ì ìˆ˜: {result['overall_score']:.2f}")
            
            # 5. AI ì²˜ë¦¬ íë¦„ í™•ì¸
            if 'processing_flow' in result:
                print("ğŸ”„ AI ì²˜ë¦¬ íë¦„:")
                for step_name, status in result['processing_flow'].items():
                    print(f"   {step_name}: {status}")
            
            # 6. AI ì„±ëŠ¥ ì •ë³´ í™•ì¸
            if 'ai_performance_info' in result:
                perf = result['ai_performance_info']
                print(f"ğŸ“Š AI ì„±ëŠ¥ ì •ë³´:")
                print(f"   ì‹¤ì œ Diffusion ì‚¬ìš©: {perf['real_diffusion_usage']}")
                print(f"   AI ë³´ì¡° ì‚¬ìš©: {perf['ai_assisted_usage']}")
                print(f"   í‚¤í¬ì¸íŠ¸ ê²€ì¶œ: {perf['keypoint_detection']}")
                print(f"   TPS ë³€í˜•: {perf['tps_transformation']}")
                print(f"   ì´ë¯¸ì§€ ì²˜ë¦¬: {perf['image_processing']}")
                print(f"   OpenCV ì˜ì¡´ì„±: {perf['opencv_dependency']}")
            
            # 7. Step ìƒíƒœ í™•ì¸
            status = step.get_status()
            print(f"ğŸ“‹ Step ìƒíƒœ:")
            print(f"   ì´ˆê¸°í™”: {status['is_initialized']}")
            print(f"   ì¤€ë¹„ë¨: {status['is_ready']}")
            print(f"   AI ëª¨ë¸: {status['ai_models_loaded']}")
            print(f"   AI í†µí•©: {status['ai_integration']}")
            
            # 8. ì •ë¦¬
            await step.cleanup()
            print("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
            print("\nğŸ‰ ì™„ì „í•œ AI ëª¨ë¸ ì—°ë™ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            print("âœ… OpenCV ì™„ì „ ì œê±°")
            print("âœ… ì‹¤ì œ AI ëª¨ë¸ ì—°ë™")
            print("âœ… BaseStepMixin v16.0 í˜¸í™˜")
            print("âœ… ëª¨ë“  ê¸°ëŠ¥ ì •ìƒ ì‘ë™")
            return True
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            print(traceback.format_exc())
            return False
    
    async def test_ai_models_individually():
        """ê°œë³„ AI ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª ê°œë³„ AI ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
        test_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
        
        # 1. AI ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸
        print("1. AI ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸")
        ai_processor = AIImageProcessor("cpu")
        ai_processor.load_models()
        resized = ai_processor.resize_image_ai(test_image, (256, 256))
        print(f"   âœ… AI ë¦¬ì‚¬ì´ì§•: {resized.shape}")
        
        # 2. SAM ì„¸ê·¸ë©˜í…Œì´ì…˜ í…ŒìŠ¤íŠ¸
        print("2. SAM ì„¸ê·¸ë©˜í…Œì´ì…˜ í…ŒìŠ¤íŠ¸")
        sam_model = SAMSegmentationModel("cpu")
        sam_model.load_model()
        mask = sam_model.segment_object(test_image)
        print(f"   âœ… SAM ì„¸ê·¸ë©˜í…Œì´ì…˜: {mask.shape}")
        
        # 3. YOLOv8 í¬ì¦ˆ í…ŒìŠ¤íŠ¸
        print("3. YOLOv8 í¬ì¦ˆ í…ŒìŠ¤íŠ¸")
        pose_model = YOLOv8PoseModel("cpu")
        pose_model.load_model()
        keypoints = pose_model.detect_keypoints(test_image)
        print(f"   âœ… YOLOv8 í¬ì¦ˆ: {len(keypoints) if keypoints is not None else 0}ê°œ í‚¤í¬ì¸íŠ¸")
        
        # 4. Neural TPS í…ŒìŠ¤íŠ¸
        print("4. Neural TPS í…ŒìŠ¤íŠ¸")
        tps_model = TPSNeuralTransform("cpu")
        if keypoints is not None and len(keypoints) >= 5:
            source_pts = keypoints[:5]
            target_pts = keypoints[:5] + np.random.normal(0, 5, (5, 2))
            fit_success = tps_model.fit(source_pts, target_pts)
            if fit_success:
                transformed = tps_model.transform_image(test_image)
                print(f"   âœ… Neural TPS: {transformed.shape}")
            else:
                print(f"   âš ï¸ Neural TPS: í”¼íŒ… ì‹¤íŒ¨")
        
        # 5. OOTDiffusion í…ŒìŠ¤íŠ¸
        print("5. OOTDiffusion í…ŒìŠ¤íŠ¸")
        ootd_model = RealOOTDiffusionModel("fallback", "cpu")
        ootd_model.load_model()
        fitted = ootd_model(test_image, test_image, keypoints)
        print(f"   âœ… OOTDiffusion: {fitted.shape}")
        
        print("ğŸ‰ ëª¨ë“  AI ëª¨ë¸ ê°œë³„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    async def test_quick_ai_fitting():
        """ë¹ ë¥¸ AI í”¼íŒ… í…ŒìŠ¤íŠ¸"""
        print("\nâš¡ ë¹ ë¥¸ AI í”¼íŒ… í…ŒìŠ¤íŠ¸...")
        
        test_person = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
        test_clothing = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
        
        result = await quick_virtual_fitting_with_ai(
            test_person, test_clothing,
            fabric_type="silk",
            clothing_type="dress"
        )
        
        print(f"âœ… ë¹ ë¥¸ AI í”¼íŒ…:")
        print(f"   ì„±ê³µ: {result['success']}")
        print(f"   ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.2f}ì´ˆ")
        if result['success']:
            print(f"   AI ì¶”ì²œ: {len(result.get('ai_recommendations', []))}ê°œ")
    
    # ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("=" * 80)
    print("ğŸ¯ MyCloset AI Step 06 - ì™„ì „í•œ AI ëª¨ë¸ ì—°ë™ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    try:
        # ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸
        success1 = await test_complete_ai_integration()
        
        # ê°œë³„ AI ëª¨ë¸ í…ŒìŠ¤íŠ¸
        await test_ai_models_individually()
        
        # ë¹ ë¥¸ í”¼íŒ… í…ŒìŠ¤íŠ¸
        await test_quick_ai_fitting()
        
        print("\n" + "=" * 80)
        print("âœ¨ ëª¨ë“  AI ëª¨ë¸ ì—°ë™ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print("ğŸ”¥ OpenCV ì™„ì „ ì œê±° ì„±ê³µ")
        print("ğŸ§  ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ ì™„ë£Œ")
        print("âš¡ BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜")
        print("ğŸš€ í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± í™•ë³´")
        print("=" * 80)
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    if asyncio.get_event_loop().is_running():
        # Jupyter ë“±ì—ì„œ ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
        import nest_asyncio
        nest_asyncio.apply()
        asyncio.create_task(test_complete_ai_integration())
    else:
        # ì¼ë°˜ì ì¸ ê²½ìš°
        asyncio.run(test_complete_ai_integration())