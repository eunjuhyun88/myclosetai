# app/ai_pipeline/steps/step_06_virtual_fitting.py
"""
ğŸ”¥ 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… (Virtual Fitting) - ì™„ì „í•œ êµ¬í˜„ + ì‹œê°í™” ê¸°ëŠ¥
âœ… logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… ModelLoader ì™„ì „ ì—°ë™ (ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œ)
âœ… BaseStepMixin ì™„ë²½ ìƒì†
âœ… M3 Max 128GB ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ë¬¼ë¦¬ ê¸°ë°˜ ì²œ ì‹œë®¬ë ˆì´ì…˜
âœ… ê³ ê¸‰ ì‹œê°í™” ê²°ê³¼ ìƒì„±
âœ… ì™„ì „ ì‘ë™í•˜ëŠ” ì½”ë“œ - ëª¨ë“  ê¸°ëŠ¥ í¬í•¨
"""

import os
import sys
import logging
import time
import asyncio
import json
import math
import threading
import uuid
import base64
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from io import BytesIO
import gc
import weakref

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
from PIL import Image, ImageEnhance, ImageFilter, ImageDraw, ImageFont

# PyTorch ê´€ë ¨
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torchvision.transforms as transforms
    TORCH_AVAILABLE = True
except:
    TORCH_AVAILABLE = False

# OpenCV
try:
    import cv2
    CV2_AVAILABLE = True
except:
    CV2_AVAILABLE = False

# ê³¼í•™ ì—°ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from scipy.interpolate import RBFInterpolator, griddata
    from scipy.spatial.distance import cdist
    from scipy.ndimage import gaussian_filter, binary_erosion, binary_dilation
    SCIPY_AVAILABLE = True
except:
    SCIPY_AVAILABLE = False

try:

    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except:
    SKLEARN_AVAILABLE = False

try:

    from skimage.feature import local_binary_pattern, canny
    from skimage.segmentation import slic, watershed
    from skimage.transform import resize, rotate
    from skimage.measure import regionprops, label
    SKIMAGE_AVAILABLE = True
except:
    SKIMAGE_AVAILABLE = False

# AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from diffusers import StableDiffusionPipeline, DDIMScheduler, UNet2DConditionModel
    from transformers import CLIPProcessor, CLIPModel
    DIFFUSERS_AVAILABLE = True
except:
    DIFFUSERS_AVAILABLE = False

# ğŸ”¥ BaseStepMixin ì„í¬íŠ¸ (logger ì†ì„± ë³´ì¥)
try:
    from .base_step_mixin import (
        BaseStepMixin, 
        VirtualFittingMixin,
        ensure_step_initialization,
        safe_step_method,
        performance_monitor
    )
    BASE_MIXIN_AVAILABLE = True
except:
    BASE_MIXIN_AVAILABLE = False
    
    # í´ë°± BaseStepMixin ì •ì˜
    
    def _setup_model_precision:
    
        """M3 Max í˜¸í™˜ ì •ë°€ë„ ì„¤ì •"""
        try:
            if self.device == "mps":
                # M3 Maxì—ì„œëŠ” Float32ê°€ ì•ˆì „
                return model.float()
            elif self.device == "cuda" and hasattr(model, 'half'):
                return model.half()
            else:
                return model.float()
        except:
            self.logger.warning(f"âš ï¸ ì •ë°€ë„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return model.float()

class BaseStepMixin:

    def __init__:

        self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            self.device = self._auto_detect_device()
            self.is_initialized = False
            self.step_name = self.__class__.__name__
            
        def _auto_detect_device:
            
            if:
            
                return "mps"
            elif TORCH_AVAILABLE and torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"

    # í´ë°± ë°ì½”ë ˆì´í„°ë“¤
def ensure_step_initialization:
    async def wrapper(self, *args, **kwargs):
    if:
    self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
    if:
    await self.initialize()
    return await func(self, *args, **kwargs)
    return wrapper

    def safe_step_method:

    async def wrapper(self, *args, **kwargs):
    try:
    if:
    self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
    return await func(self, *args, **kwargs)
    except:
    if:
    self.logger.error(f"âŒ {func.__name__} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    return {'success': False, 'error': str(e)}
    return wrapper

    def performance_monitor:

    def decorator(func):
    async def wrapper(self, *args, **kwargs):
    start_time = time.time()
    try:
    result = await func(self, *args, **kwargs)
    if:
    self.record_performance(operation_name, time.time() - start_time, True)
    return result
    except:
    if:
    self.record_performance(operation_name, time.time() - start_time, False)
    raise e
    return wrapper
    return decorator

# ModelLoader ì—°ë™
try:
    from ..utils.model_loader import (
        ModelLoader,
        ModelConfig,
        ModelType,
        get_global_model_loader,
        preprocess_image,
        postprocess_segmentation
    )
    MODEL_LOADER_AVAILABLE = True
except:
    logging.error(f"ModelLoader ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    MODEL_LOADER_AVAILABLE = False

# ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìœ í‹¸ë¦¬í‹°
try:
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
except:
    MemoryManager = None
    DataConverter = None

# =================================================================
# ğŸ”¥ ìƒìˆ˜ ë° ì„¤ì • ì •ì˜
# =================================================================

class FittingQuality:

    """í”¼íŒ… í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    ULTRA = "ultra"

class FittingMethod:

    """í”¼íŒ… ë°©ë²•"""
    PHYSICS_BASED = "physics_based"
    AI_NEURAL = "ai_neural"
    HYBRID = "hybrid"
    TEMPLATE_MATCHING = "template_matching"
    DIFFUSION_BASED = "diffusion"
    LIGHTWEIGHT = "lightweight"

@dataclass
class FabricProperties:
    """ì²œ ì¬ì§ˆ ì†ì„±"""
    stiffness: float = 0.5
    elasticity: float = 0.3
    density: float = 1.4
    friction: float = 0.5
    shine: float = 0.5
    transparency: float = 0.0
    texture_scale: float = 1.0

@dataclass
class FittingParams:
    """í”¼íŒ… íŒŒë¼ë¯¸í„°"""
    fit_type: str = "fitted"
    body_contact: float = 0.7
    drape_level: float = 0.3
    stretch_zones: List[str] = field(default_factory=lambda: ["chest", "waist"])
    wrinkle_intensity: float = 0.5
    shadow_strength: float = 0.6

@dataclass
class VirtualFittingConfig:
    """ê°€ìƒ í”¼íŒ… ì„¤ì •"""
    # ëª¨ë¸ ì„¤ì •
    model_name: str = "ootdiffusion"
    inference_steps: int = 50
    guidance_scale: float = 7.5
    scheduler_type: str = "ddim"
    
    # í’ˆì§ˆ ì„¤ì •
    input_size: Tuple[int, int] = (512, 512)
    output_size: Tuple[int, int] = (512, 512)
    upscale_factor: float = 1.0
    
    # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜
    physics_enabled: bool = True
    cloth_stiffness: float = 0.3
    gravity_strength: float = 9.81
    wind_force: Tuple[float, float] = (0.0, 0.0)
    
    # ë Œë”ë§ ì„¤ì •
    lighting_type: str = "natural"
    shadow_enabled: bool = True
    reflection_enabled: bool = False
    
    # ìµœì í™” ì„¤ì •
    enable_attention_slicing: bool = True
    enable_cpu_offload: bool = False
    memory_efficient: bool = True
    use_half_precision: bool = True

@dataclass
class FittingResult:
    """ê°€ìƒ í”¼íŒ… ê²°ê³¼"""
    success: bool
    fitted_image: Optional[np.ndarray] = None
    confidence_score: float = 0.0
    processing_time: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    visualization_data: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None

# ì²œ ì¬ì§ˆë³„ ë¬¼ë¦¬ ì†ì„±
FABRIC_PROPERTIES = {
    'cotton': FabricProperties(0.3, 0.2, 1.5, 0.7, 0.2, 0.0, 1.0),
    'denim': FabricProperties(0.8, 0.1, 2.0, 0.9, 0.1, 0.0, 1.2),
    'silk': FabricProperties(0.1, 0.4, 1.3, 0.3, 0.8, 0.1, 0.8),
    'wool': FabricProperties(0.5, 0.3, 1.4, 0.6, 0.3, 0.0, 1.1),
    'polyester': FabricProperties(0.4, 0.5, 1.2, 0.4, 0.6, 0.0, 0.9),
    'leather': FabricProperties(0.9, 0.1, 2.5, 0.8, 0.9, 0.0, 1.5),
    'spandex': FabricProperties(0.1, 0.8, 1.1, 0.5, 0.4, 0.0, 0.7),
    'linen': FabricProperties(0.6, 0.2, 1.6, 0.8, 0.1, 0.0, 1.3),
    'default': FabricProperties(0.4, 0.3, 1.4, 0.5, 0.5, 0.0, 1.0)
}

# ì˜ë¥˜ íƒ€ì…ë³„ í”¼íŒ… íŒŒë¼ë¯¸í„°
CLOTHING_FITTING_PARAMS = {
    'shirt': FittingParams("fitted", 0.7, 0.3, ["chest", "waist"], 0.3, 0.6),
    'dress': FittingParams("flowing", 0.5, 0.7, ["bust", "waist", "hip"], 0.6, 0.7),
    'pants': FittingParams("fitted", 0.8, 0.2, ["thigh", "calf"], 0.4, 0.5),
    'jacket': FittingParams("structured", 0.6, 0.4, ["shoulder", "chest"], 0.2, 0.8),
    'skirt': FittingParams("flowing", 0.6, 0.6, ["waist", "hip"], 0.5, 0.6),
    'blouse': FittingParams("loose", 0.5, 0.5, ["chest", "waist"], 0.4, 0.6),
    'sweater': FittingParams("relaxed", 0.6, 0.4, ["chest", "arms"], 0.3, 0.5),
    'default': FittingParams("fitted", 0.6, 0.4, ["chest", "waist"], 0.4, 0.6)
}

# ğŸ†• ì‹œê°í™”ìš© ìƒ‰ìƒ íŒ”ë ˆíŠ¸
VISUALIZATION_COLORS = {
    'original': (200, 200, 200),      # ì›ë³¸ ì´ë¯¸ì§€ ì˜ì—­
    'cloth': (100, 149, 237),         # ì˜ë¥˜ ì˜ì—­ - ì½˜í”Œë¼ì›Œ ë¸”ë£¨
    'fitted': (255, 105, 180),        # í”¼íŒ…ëœ ì˜ë¥˜ - í•«í•‘í¬
    'skin': (255, 218, 185),          # í”¼ë¶€ ì˜ì—­ - ì‚´ìƒ‰
    'hair': (139, 69, 19),            # ë¨¸ë¦¬ - ê°ˆìƒ‰
    'background': (240, 248, 255),    # ë°°ê²½ - ì—°í•œ íŒŒë‘
    'shadow': (105, 105, 105),        # ê·¸ë¦¼ì - ì–´ë‘ìš´ íšŒìƒ‰
    'highlight': (255, 255, 224),     # í•˜ì´ë¼ì´íŠ¸ - ì—°í•œ ë…¸ë‘
    'seam': (255, 69, 0),             # ì†”ê¸° - ë¹¨ê°•-ì£¼í™©
    'fold': (123, 104, 238),          # ì£¼ë¦„ - ë¯¸ë””ì—„ ìŠ¬ë ˆì´íŠ¸ ë¸”ë£¨
    'overlay': (255, 255, 255, 128)   # ì˜¤ë²„ë ˆì´ - ë°˜íˆ¬ëª… í°ìƒ‰
}

# =================================================================
# ğŸ”¥ ë©”ì¸ ê°€ìƒ í”¼íŒ… í´ë˜ìŠ¤
# =================================================================

class VirtualFittingStep:

    """
    ğŸ”¥ 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… - ì™„ì „í•œ êµ¬í˜„
    
    âœ… logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°
    âœ… BaseStepMixin ì™„ë²½ ìƒì†
    âœ… ModelLoader ì™„ì „ ì—°ë™
    âœ… ë¬¼ë¦¬ ê¸°ë°˜ ì²œ ì‹œë®¬ë ˆì´ì…˜
    âœ… M3 Max Neural Engine ê°€ì†
    âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
    âœ… ê³ ê¸‰ ì‹œê°í™” ì‹œìŠ¤í…œ
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """
        âœ… ì™„ì „ ìˆ˜ì •ëœ ìƒì„±ì - logger ì†ì„± ë³´ì¥
        
        Args:
            device: ë””ë°”ì´ìŠ¤ ('cpu', 'cuda', 'mps', None=ìë™ê°ì§€)
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            **kwargs: í™•ì¥ íŒŒë¼ë¯¸í„°
                - device_type: str = "auto"
                - memory_gb: float = 16.0
                - is_m3_max: bool = False
                - optimization_enabled: bool = True
                - quality_level: str = "balanced"
                - fitting_method: str = "hybrid"
                - enable_physics: bool = True
                - enable_ai_models: bool = True
                - enable_visualization: bool = True
        """
        
        # ğŸ”¥ BaseStepMixin ë¨¼ì € ì´ˆê¸°í™” (logger ì†ì„± ë³´ì¥)
        super().__init__()
        
        # ğŸ”¥ logger ì†ì„± ëª…ì‹œì  ì„¤ì • (ì¤‘ë³µ ë°©ì§€)
        if:
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        self.logger.info("ğŸ”„ VirtualFittingStep ì´ˆê¸°í™” ì‹œì‘...")
        
        try:
            # === 1. ê¸°ë³¸ ì†ì„± ì„¤ì • ===
            self.step_name = "VirtualFittingStep"
            self.step_number = 6
            self.device = device or self._auto_detect_device()
            self.config = config or {}
            
            # === 2. ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ===
            self.device_type = kwargs.get('device_type', 'auto')
            self.memory_gb = kwargs.get('memory_gb', 16.0)
            self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
            self.optimization_enabled = kwargs.get('optimization_enabled', True)
            self.quality_level = kwargs.get('quality_level', 'balanced')
            
            # === 3. 6ë‹¨ê³„ íŠ¹í™” íŒŒë¼ë¯¸í„° ===
            fitting_method_str = kwargs.get('fitting_method', 'hybrid')
            if:
                try:
                    self.fitting_method = FittingMethod(fitting_method_str)
                except:
                    self.fitting_method = FittingMethod.HYBRID
            else:
                self.fitting_method = fitting_method_str
                
            self.enable_physics = kwargs.get('enable_physics', True)
            self.enable_ai_models = kwargs.get('enable_ai_models', True)
            self.enable_visualization = kwargs.get('enable_visualization', True)
            
            # === 4. ì„¤ì • ê°ì²´ ìƒì„± ===
            self.fitting_config = self._create_fitting_config(kwargs)
            
            # === 5. ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™” ===
            self.is_initialized = False
            self.session_id = str(uuid.uuid4())
            self.last_result = None
            self.processing_stats = {}
            
            # === 6. ë©”ëª¨ë¦¬ ë° ìºì‹œ ê´€ë¦¬ ===
            self.result_cache: Dict[str, Any] = {}
            self.cache_lock = threading.RLock()
            self.executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="virtual_fitting")
            
            # === 7. ModelLoader ì¸í„°í˜ì´ìŠ¤ ===
            self.model_interface = None
            self.loaded_models = {}
            self._setup_model_interface()
            
            # === 8. ë¬¼ë¦¬ ì—”ì§„ ë° ë Œë”ëŸ¬ ===
            self.physics_engine = None
            self.renderer = None
            
            # === 9. AI ëª¨ë¸ ê´€ë¦¬ ===
            self.ai_models = {
                'diffusion_pipeline': None,
                'human_parser': None,
                'cloth_segmenter': None,
                'pose_estimator': None,
                'style_encoder': None
            }
            
            # === 10. ì„±ëŠ¥ í†µê³„ ===
            self.performance_stats = {
                'total_processed': 0,
                'successful_fittings': 0,
                'failed_fittings': 0,
                'average_processing_time': 0.0,
                'cache_hits': 0,
                'cache_misses': 0,
                'memory_peak_mb': 0.0,
                'ai_model_usage': {model: 0 for model in self.ai_models.keys()}
            }
            
            # === 11. ì‹œê°í™” ì„¤ì • ===
            self.visualization_config = {
                'enabled': self.enable_visualization,
                'quality': self.getattr(config, "get", lambda x, y: y)('visualization_quality', 'medium'),
                'show_process_steps': self.getattr(config, "get", lambda x, y: y)('show_process_steps', True),
                'show_fit_analysis': self.getattr(config, "get", lambda x, y: y)('show_fit_analysis', True),
                'show_fabric_details': self.getattr(config, "get", lambda x, y: y)('show_fabric_details', True),
                'overlay_opacity': self.getattr(config, "get", lambda x, y: y)('overlay_opacity', 0.7),
                'comparison_mode': self.getattr(config, "get", lambda x, y: y)('comparison_mode', 'side_by_side')
            }
            
            # === 12. ìºì‹œ ì‹œìŠ¤í…œ ===
            cache_size = min(200 if self.is_m3_max and self.memory_gb >= 128 else 50, 
                            int(self.memory_gb * 2))
            self.fitting_cache = {}
            self.cache_max_size = cache_size
            self.cache_stats = {
                'hits': 0,
                'misses': 0,
                'total_size': 0
            }
            self.cache_access_times = {}
            
            # === 13. ì„±ëŠ¥ ì„¤ì • ===
            self.performance_config = {
                'max_resolution': self._get_max_resolution(),
                'fitting_iterations': self._get_fitting_iterations(),
                'precision_factor': self._get_precision_factor(),
                'batch_size': self._get_batch_size(),
                'cache_enabled': True,
                'parallel_processing': self.is_m3_max,
                'memory_efficient': self.memory_gb < 32
            }
            
            # === 14. ë Œë”ë§ ì„¤ì • ===
            self.rendering_config = {
                'lighting_model': 'pbr',  # Physically Based Rendering
                'shadow_quality': 'medium',
                'reflection_quality': 'low',
                'ambient_occlusion': True,
                'anti_aliasing': True,
                'texture_filtering': 'bilinear',
                'color_space': 'srgb'
            }
            
            # === 15. ì¡°ëª… ì„¤ì • ===
            self.lighting_setup = {
                'main_light': {'direction': (0.3, -0.5, 0.8), 'intensity': 1.0, 'color': (1.0, 1.0, 1.0)},
                'fill_light': {'direction': (-0.3, -0.2, 0.5), 'intensity': 0.4, 'color': (0.9, 0.9, 1.0)},
                'rim_light': {'direction': (0.0, 0.8, -0.2), 'intensity': 0.3, 'color': (1.0, 0.9, 0.8)},
                'ambient': {'intensity': 0.2, 'color': (0.5, 0.5, 0.6)}
            }
            
            # === 16. M3 Max ìµœì í™” ===
            if:
                self._setup_m3_max_optimization()
            
            # === 17. ë©”ëª¨ë¦¬ ê´€ë¦¬ì ===
            self.memory_manager = self._create_memory_manager()
            self.data_converter = self._create_data_converter()
            
            # === 18. ìŠ¤ë ˆë“œ í’€ ===
            max_workers = min(8, int(self.memory_gb / 8)) if self.is_m3_max else 2
            self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)
            
            self.logger.info("âœ… VirtualFittingStep ì´ˆê¸°í™” ì™„ë£Œ")
            
        except:
            
            self.logger.error(f"âŒ VirtualFittingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.logger.error(traceback.format_exc())
            raise
    
    def _auto_detect_device:
    
        """ë””ë°”ì´ìŠ¤ ìë™ íƒì§€"""
        if:
            return "cpu"
            
        try:
            # M3 Max MPS ì§€ì› í™•ì¸
            if:
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        except:
            return "cpu"
    
    def _detect_m3_max:
    
        """M3 Max í•˜ë“œì›¨ì–´ íƒì§€"""
        try:
            if sys.platform == "darwin":  # macOS
                # Apple Silicon í™•ì¸
                import platform
                if:
                    return True
            return False
        except:
            return False
    
    def _create_fitting_config:
    
        """í”¼íŒ… ì„¤ì • ìƒì„±"""
        config_params = {}
        
        # kwargsì—ì„œ ì„¤ì • íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        if:
            config_params['inference_steps'] = kwargs['inference_steps']
        if:
            config_params['guidance_scale'] = kwargs['guidance_scale']
        if:
            config_params['physics_enabled'] = kwargs['physics_enabled']
        if:
            config_params['input_size'] = kwargs['input_size']
        
        # ê¸°ë³¸ ì„¤ì • + ì‚¬ìš©ì ì„¤ì • ë³‘í•©
        return VirtualFittingConfig(**config_params)
    
    def _get_max_resolution:
    
        """ìµœëŒ€ í•´ìƒë„ ê³„ì‚°"""
        if:
            return 1024
        elif self.quality_level == "high" and self.memory_gb >= 32:
            return 768
        elif self.quality_level == "balanced":
            return 512
        else:
            return 384
    
    def _get_fitting_iterations:
    
        """í”¼íŒ… ë°˜ë³µ íšŸìˆ˜"""
        quality_iterations = {
            "fast": 1,
            "balanced": 3,
            "high": 5,
            "ultra": 8
        }
        return quality_iterations.get(self.quality_level, 3)
    
    def _get_precision_factor:
    
        """ì •ë°€ë„ ê³„ìˆ˜"""
        quality_precision = {
            "fast": 0.5,
            "balanced": 1.0,
            "high": 1.5,
            "ultra": 2.0
        }
        return quality_precision.get(self.quality_level, 1.0)
    
    def _get_batch_size:
    
        """ë°°ì¹˜ í¬ê¸°"""
        if:
            return 4
        elif self.memory_gb >= 32:
            return 2
        else:
            return 1
    
    def _setup_model_interface:
    
        """ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì •"""
        try:
            if:
                self.logger.warning("âš ï¸ ModelLoaderê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
                return
            
            # ì „ì—­ ModelLoader ê°€ì ¸ì˜¤ê¸°
            model_loader = get_global_model_loader()
            if:
                self.model_interface = model_loader.create_step_interface(self.step_name)
                self.logger.info("ğŸ”— ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ê²° ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ ì „ì—­ ModelLoaderë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
        except:
                
            self.logger.error(f"âŒ ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_interface = None
    
    def _setup_m3_max_optimization:
    
        """M3 Max ìµœì í™” ì„¤ì •"""
        try:
            if TORCH_AVAILABLE:
                # M3 Max íŠ¹í™” ì„¤ì •
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                # ë©”ëª¨ë¦¬ ìµœì í™”
                if:
                    torch.backends.mps.empty_cache()
                
                # 128GB ë©”ëª¨ë¦¬ í™œìš© ìµœì í™”
                if:
                    torch.backends.mps.set_per_process_memory_fraction(0.8)
                
                self.logger.info("ğŸ M3 Max MPS ìµœì í™” ì„¤ì • ì™„ë£Œ")
        except:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _create_memory_manager:
    
        """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ìƒì„±"""
        if:
            return MemoryManager(device=self.device)
        else:
            # ê¸°ë³¸ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €
            class SimpleMemoryManager:
                def __init__:
                    self.device = device
                
                async def get_usage_stats(self): 
                    return {"memory_used": "N/A"}
                
                def get_memory_usage:
                
                    try:
                
                        import psutil
                        process = psutil.Process()
                        return process.memory_info().rss / (1024 * 1024)
                    except:
                        return 0.0
                
                async def cleanup(self): 
                    gc.collect()
                    if:
                        try:
                            if:
                                torch.backends.mps.empty_cache()
                        except:
                            pass
            return SimpleMemoryManager(self.device)
    
    def _create_data_converter:
    
        """ë°ì´í„° ì»¨ë²„í„° ìƒì„±"""
        if:
            return DataConverter()
        else:
            # ê¸°ë³¸ ì»¨ë²„í„°
            class SimpleDataConverter:
                def convert:
                    return data
                def to_tensor:
                    return torch.from_numpy(data) if isinstance(data, np.ndarray) else data
                def to_numpy:
                    return data.cpu().numpy() if torch.is_tensor(data) else data
            return SimpleDataConverter()
    
    def record_performance:
    
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        if:
            self.performance_stats[operation] = {
                "total_calls": 0,
                "success_calls": 0,
                "total_duration": 0.0,
                "avg_duration": 0.0,
                "last_duration": 0.0
            }
        
        metrics = self.performance_stats[operation]
        metrics["total_calls"] += 1
        metrics["total_duration"] += duration
        metrics["last_duration"] = duration
        metrics["avg_duration"] = metrics["total_duration"] / metrics["total_calls"]
        
        if:
        
            metrics["success_calls"] += 1
    
    # =================================================================
    # ğŸ”¥ ì´ˆê¸°í™” ë° ëª¨ë¸ ë¡œë”©
    # =================================================================
    
    @ensure_step_initialization
    async def initialize(self) -> bool:
        """
        âœ… Step ì´ˆê¸°í™” - ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ
        
        Returns:
            bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            self.logger.info("ğŸ”„ 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            
            # === ì£¼ ëª¨ë¸ ë¡œë“œ ===
            success = await self._load_primary_model()
            if:
                self.logger.warning("âš ï¸ ì£¼ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - í´ë°± ëª¨ë“œë¡œ ê³„ì†")
            
            # === ë³´ì¡° ëª¨ë¸ë“¤ ë¡œë“œ ===
            await self._load_auxiliary_models()
            
            # === ë¬¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™” ===
            if:
                self._initialize_physics_engine()
            
            # === ë Œë”ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ===
            self._initialize_rendering_system()
            
            # === ìºì‹œ ì‹œìŠ¤í…œ ì¤€ë¹„ ===
            self._prepare_cache_system()
            
            # === M3 Max ì¶”ê°€ ìµœì í™” ===
            if:
                await self._apply_m3_max_optimizations()
            
            self.is_initialized = True
            self.logger.info("âœ… 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            return True
            
        except:
            
            self.logger.error(f"âŒ 6ë‹¨ê³„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.logger.error(traceback.format_exc())
            self.is_initialized = False
            return False
    
    async def _load_primary_model(self) -> bool:
        """ì£¼ ëª¨ë¸ (OOTDiffusion/Stable Diffusion) ë¡œë“œ"""
        try:
            if:
                self.logger.warning("âš ï¸ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ì–´ í´ë°± ëª¨ë“œë¡œ ì‹¤í–‰")
                return await self._create_fallback_model()
            
            self.logger.info("ğŸ“¦ ì£¼ ëª¨ë¸ ë¡œë“œ ì¤‘: Virtual Fitting Model")
            
            # ëª¨ë¸ ìš”ì²­ ìˆœì„œëŒ€ë¡œ ì‹œë„
            model_candidates = [
                "virtual_fitting_stable_diffusion",
                "ootdiffusion",
                "stable_diffusion",
                "diffusion_pipeline"
            ]
            
            for model_name in model_candidates:
                try:
                    model = await self.model_interface.get_model(model_name)
                    if:
                        self.loaded_models['primary'] = model
                        self.ai_models['diffusion_pipeline'] = model
                        self.performance_stats['ai_model_usage']['diffusion_pipeline'] += 1
                        self.logger.info(f"âœ… ì£¼ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
                        return True
                except:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ {model_name} ë¡œë“œ ì‹œë„ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.warning("âš ï¸ ëª¨ë“  ì£¼ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - í´ë°± ëª¨ë“œ ì‚¬ìš©")
            return await self._create_fallback_model()
                
        except:
                
            self.logger.error(f"âŒ ì£¼ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return await self._create_fallback_model()
    
    async def _create_fallback_model(self) -> bool:
        """í´ë°± ëª¨ë¸ ìƒì„± (ModelLoader ì—†ì„ ë•Œ)"""
        try:
            self.logger.info("ğŸ”§ í´ë°± ëª¨ë¸ ìƒì„± ì¤‘...")
            
            # ê°„ë‹¨í•œ í´ë°± ëª¨ë¸ ìƒì„±
            class FallbackVirtualFittingModel:
                def __init__:
                    self.device = device
                    
                async def predict(self, person_image, cloth_image, **kwargs):
                    # ê¸°ë³¸ì ì¸ ì´ë¯¸ì§€ í•©ì„±
                    return self._simple_fitting(person_image, cloth_image)
                
                def _simple_fitting(self, person_img, cloth_img):
                    # ë‹¨ìˆœí•œ ì•ŒíŒŒ ë¸”ë Œë”© ê¸°ë°˜ í”¼íŒ…
                    if:
                        if not CV2_AVAILABLE:
                            return person_img
                        
                        h, w = person_img.shape[:2]
                        cloth_resized = cv2.resize(cloth_img, (w//2, h//2))
                        
                        # ì¤‘ì•™ ìœ„ì¹˜ì— ì˜ë¥˜ ë°°ì¹˜
                        y_offset = h//4
                        x_offset = w//4
                        
                        result = person_img.copy()
                        end_y = min(y_offset + cloth_resized.shape[0], h)
                        end_x = min(x_offset + cloth_resized.shape[1], w)
                        
                        # ì•ŒíŒŒ ë¸”ë Œë”©
                        alpha = 0.7
                        if:
                            cloth_cropped = cloth_resized[:end_y-y_offset, :end_x-x_offset]
                            result[y_offset:end_y, x_offset:end_x] = cv2.addWeighted(
                                result[y_offset:end_y, x_offset:end_x],
                                1 - alpha,
                                cloth_cropped,
                                alpha,
                                0
                            )
                        
                        return result
                    return person_img
            
            self.loaded_models['primary'] = FallbackVirtualFittingModel(self.device)
            self.ai_models['diffusion_pipeline'] = self.loaded_models['primary']
            self.logger.info("âœ… í´ë°± ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            return True
            
        except:
            
            self.logger.error(f"âŒ í´ë°± ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    async def _load_auxiliary_models(self):
        """ë³´ì¡° ëª¨ë¸ë“¤ ë¡œë“œ"""
        try:
            self.logger.info("ğŸ“¦ ë³´ì¡° ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...")
            
            auxiliary_models = [
                ("enhancement", "post_processing_realesrgan"),
                ("quality_assessment", "quality_assessment_clip"),
                ("human_parser", "human_parsing_graphonomy"),
                ("pose_estimator", "pose_estimation_openpose"),
                ("cloth_segmenter", "cloth_segmentation_u2net"),
                ("style_encoder", "clip")
            ]
            
            for model_key, model_name in auxiliary_models:
                try:
                    if:
                        model = await self.model_interface.get_model(model_name)
                        if:
                            self.loaded_models[model_key] = model
                            self.ai_models[model_key] = model
                            self.performance_stats['ai_model_usage'][model_key] += 1
                            self.logger.info(f"âœ… ë³´ì¡° ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_key}")
                        else:
                            self.logger.warning(f"âš ï¸ ë³´ì¡° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_key}")
                except:
                    self.logger.warning(f"âš ï¸ ë³´ì¡° ëª¨ë¸ {model_key} ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            self.logger.info("âœ… ë³´ì¡° ëª¨ë¸ ë¡œë“œ ê³¼ì • ì™„ë£Œ")
            
        except:
            
            self.logger.error(f"âŒ ë³´ì¡° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _initialize_physics_engine:
    
        """ë¬¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™”"""
        try:
            self.logger.info("ğŸ”§ ë¬¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
            
            class ClothPhysicsEngine:
            
                def __init__:
            
                    self.stiffness = config.cloth_stiffness
                    self.gravity = config.gravity_strength
                    self.wind_force = config.wind_force
                    
                def simulate_cloth_draping:
                    
                    """ê°„ë‹¨í•œ ì²œ ë“œë ˆì´í•‘ ì‹œë®¬ë ˆì´ì…˜"""
                    # ë¬¼ë¦¬ ê¸°ë°˜ ë³€í˜• ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ë³µì¡)
                    return cloth_mesh
                
                def apply_wrinkles:
                
                    """ì£¼ë¦„ íš¨ê³¼ ì ìš©"""
                    return cloth_surface
                
                def calculate_fabric_deformation:
                
                    """ì²œ ë³€í˜• ê³„ì‚°"""
                    return force_map * fabric_props.elasticity
                
                def apply_gravity_effects:
                
                    """ì¤‘ë ¥ íš¨ê³¼ ì ìš©"""
                    return cloth_data
            
            # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ íŒŒë¼ë¯¸í„°
            self.physics_params = {
                'time_step': 0.01,
                'iterations': self._get_fitting_iterations(),
                'damping': 0.95,
                'spring_constant': 100.0,
                'mass_distribution': 'uniform'
            }
            
            self.physics_engine = ClothPhysicsEngine(self.fitting_config)
            self.logger.info("âœ… ë¬¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except:
            
            self.logger.error(f"âŒ ë¬¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.physics_engine = None
    
    def _initialize_rendering_system:
    
        """ë Œë”ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            self.logger.info("ğŸ¨ ë Œë”ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            
            class VirtualFittingRenderer:
            
                def __init__:
            
                    self.lighting = config.lighting_type
                    self.shadow_enabled = config.shadow_enabled
                    self.reflection_enabled = config.reflection_enabled
                
                def render_final_image:
                
                    """ìµœì¢… ì´ë¯¸ì§€ ë Œë”ë§"""
                    if isinstance(fitted_image, np.ndarray):
                        # ì¡°ëª… íš¨ê³¼ ì ìš©
                        enhanced = self._apply_lighting(fitted_image)
                        
                        # ê·¸ë¦¼ì íš¨ê³¼ (ì„ íƒì )
                        if:
                            enhanced = self._add_shadows(enhanced)
                        
                        return enhanced
                    return fitted_image
                
                def _apply_lighting:
                
                    """ì¡°ëª… íš¨ê³¼ ì ìš©"""
                    # ê°„ë‹¨í•œ ì¡°ëª… íš¨ê³¼
                    if self.lighting == "natural":
                        # ìì—°ê´‘ íš¨ê³¼
                        if:
                            enhanced = cv2.convertScaleAbs(image, alpha=1.1, beta=10)
                            return enhanced
                    return image
                
                def _add_shadows:
                
                    """ê·¸ë¦¼ì íš¨ê³¼ ì¶”ê°€"""
                    # ê¸°ë³¸ ê·¸ë¦¼ì íš¨ê³¼
                    return image
                
                def apply_pbr_lighting:
                
                    """PBR ì¡°ëª… ì ìš©"""
                    return image
                
                def create_ambient_occlusion:
                
                    """ì•°ë¹„ì–¸íŠ¸ ì˜¤í´ë£¨ì „ ìƒì„±"""
                    return image
            
            self.renderer = VirtualFittingRenderer(self.fitting_config)
            self.logger.info("âœ… ë Œë”ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except:
            
            self.logger.error(f"âŒ ë Œë”ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.renderer = None
    
    def _prepare_cache_system:
    
        """ìºì‹œ ì‹œìŠ¤í…œ ì¤€ë¹„"""
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            cache_dir = Path("cache/virtual_fitting")
            cache_dir.mkdir(parents=True, exist_ok=True)
            
            # ìºì‹œ ì„¤ì •
            self.cache_config = {
                'enabled': True,
                'max_size': self.cache_max_size,
                'ttl_seconds': 3600,  # 1ì‹œê°„
                'compression': True,
                'persist_to_disk': self.memory_gb < 64
            }
            
            self.cache_stats = {
                'hits': 0,
                'misses': 0,
                'total_size': 0
            }
            self.logger.info(f"âœ… ìºì‹œ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ - í¬ê¸°: {self.cache_max_size}")
        except:
            self.logger.error(f"âŒ ìºì‹œ ì‹œìŠ¤í…œ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
    
    async def _apply_m3_max_optimizations(self):
        """M3 Max íŠ¹í™” ìµœì í™” ì ìš©"""
        try:
            optimizations = []
            
            # 1. MPS ë°±ì—”ë“œ ìµœì í™”
            if:
                torch.backends.mps.set_per_process_memory_fraction(0.8)
                optimizations.append("MPS memory optimization")
            
            # 2. Neural Engine ì¤€ë¹„
            if:
                optimizations.append("Neural Engine ready")
            
            # 3. ë©”ëª¨ë¦¬ í’€ë§
            if:
                if hasattr(torch.backends.mps, 'allow_tf32'):
                    torch.backends.mps.allow_tf32 = True
                optimizations.append("Memory pooling")
            
            # 4. 128GB ë©”ëª¨ë¦¬ ì „ìš© ìµœì í™”
            if:
                self.performance_config['large_batch_processing'] = True
                self.performance_config['extended_cache'] = True
                optimizations.append("128GB memory optimizations")
            
            if:
            
                self.logger.info(f"ğŸ M3 Max ìµœì í™” ì ìš©: {', '.join(optimizations)}")
                
        except:
                
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    # =================================================================
    # ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ
    # =================================================================
    
    @safe_step_method
    @performance_monitor("virtual_fitting_process")
    async def process(
        self,
        person_image: Union[np.ndarray, str, Image.Image, torch.Tensor],
        cloth_image: Union[np.ndarray, str, Image.Image, torch.Tensor], 
        pose_data: Optional[Dict[str, Any]] = None,
        cloth_mask: Optional[np.ndarray] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        âœ… ë©”ì¸ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ë©”ì„œë“œ
        
        Args:
            person_image: ì‚¬ëŒ ì´ë¯¸ì§€
            cloth_image: ì˜ë¥˜ ì´ë¯¸ì§€  
            pose_data: í¬ì¦ˆ ë°ì´í„° (Step 2ì—ì„œ ì „ë‹¬)
            cloth_mask: ì˜ë¥˜ ë§ˆìŠ¤í¬ (Step 3ì—ì„œ ì „ë‹¬)
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
                - fabric_type: str = "cotton"
                - clothing_type: str = "shirt"
                - fit_preference: str = "fitted"
                - style_guidance: Optional[str] = None
                - preserve_background: bool = True
                - quality_enhancement: bool = True
        
        Returns:
            Dict[str, Any]: ê°€ìƒ í”¼íŒ… ê²°ê³¼
        """
        start_time = time.time()
        session_id = f"vf_{uuid.uuid4().hex[:8]}"
        
        try:
        
            self.logger.info(f"ğŸ”„ 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹œì‘ - ì„¸ì…˜: {session_id}")
            
            # === ì´ˆê¸°í™” í™•ì¸ ===
            if:
                await self.initialize()
            
            # === ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì „ì²˜ë¦¬ ===
            processed_inputs = await self._preprocess_inputs(
                person_image, cloth_image, pose_data, cloth_mask
            )
            
            if:
            
                return processed_inputs
            
            person_img = processed_inputs['person_image']
            cloth_img = processed_inputs['cloth_image']
            
            # === ìºì‹œ í™•ì¸ ===
            cache_key = self._generate_cache_key(person_img, cloth_img, kwargs)
            cached_result = self._get_cached_result(cache_key)
            
            if:
            
                self.logger.info("âœ… ìºì‹œëœ ê²°ê³¼ ë°˜í™˜")
                self.cache_stats['hits'] += 1
                return cached_result
            
            self.cache_stats['misses'] += 1
            
            # === ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ===
            metadata = await self._extract_metadata(person_img, cloth_img, kwargs)
            
            # === ë©”ì¸ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ===
            fitting_result = await self._execute_virtual_fitting(
                person_img, cloth_img, metadata, session_id
            )
            
            # === í›„ì²˜ë¦¬ ë° í’ˆì§ˆ í–¥ìƒ ===
            if:
                fitting_result = await self._enhance_result(fitting_result)
            
            # === ê²°ê³¼ ê²€ì¦ ë° í’ˆì§ˆ í‰ê°€ ===
            quality_score = await self._assess_quality(fitting_result)
            
            # === ì‹œê°í™” ë°ì´í„° ìƒì„± ===
            if:
                visualization_data = await self._create_fitting_visualization(
                    person_img, cloth_img, fitting_result.fitted_image, metadata
                )
                fitting_result.visualization_data = visualization_data
            
            # === ìµœì¢… ê²°ê³¼ í¬ë§·íŒ… ===
            final_result = self._build_result_with_visualization(
                fitting_result, fitting_result.visualization_data, metadata, 
                time.time() - start_time, session_id
            )
            
            # === ê²°ê³¼ ìºì‹± ===
            self._cache_result(cache_key, final_result)
            
            # === ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸ ===
            self._update_processing_stats(final_result)
            
            self.logger.info(f"âœ… 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì™„ë£Œ (í’ˆì§ˆ: {quality_score:.3f})")
            return final_result
            
        except:
            
            error_msg = f"ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
            self.logger.error(f"âŒ {error_msg}")
            self.logger.error(traceback.format_exc())
            
            return self._create_fallback_result(time.time() - start_time, session_id, error_msg)
    
    async def _preprocess_inputs(
        self,
        person_image: Union[np.ndarray, str, Image.Image, torch.Tensor],
        cloth_image: Union[np.ndarray, str, Image.Image, torch.Tensor],
        pose_data: Optional[Dict[str, Any]],
        cloth_mask: Optional[np.ndarray]
    ) -> Dict[str, Any]:
        """ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ ì •ê·œí™”
            person_img = self._normalize_image(person_image)
            cloth_img = self._normalize_image(cloth_image)
            
            if:
            
                return {
                    'success': False,
                    'error': 'ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨'
                }
            
            # í¬ê¸° ì •ê·œí™”
            target_size = self.fitting_config.input_size
            if:
                person_img = cv2.resize(person_img, target_size)
                cloth_img = cv2.resize(cloth_img, target_size)
            
            return {
                'success': True,
                'person_image': person_img,
                'cloth_image': cloth_img,
                'pose_data': pose_data,
                'cloth_mask': cloth_mask
            }
            
        except:
            
            self.logger.error(f"âŒ ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': f'ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}'
            }
    
    def _normalize_image:
    
        """ì´ë¯¸ì§€ ì •ê·œí™”"""
        try:
            if isinstance(image_input, str):
                # Base64 ë””ì½”ë”©
                if:
                    header, data = image_input.split(',', 1)
                    image_data = base64.b64decode(data)
                else:
                    image_data = base64.b64decode(image_input)
                
                image = Image.open(BytesIO(image_data))
                return np.array(image)
                
            elif isinstance(image_input, Image.Image):
                return np.array(image_input)
                
            elif isinstance(image_input, np.ndarray):
                return image_input
                
            elif torch.is_tensor(image_input):
                # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
                if image_input.dim() == 4:  # [B, C, H, W]
                    image_input = image_input.squeeze(0)  # [C, H, W]
                if image_input.dim() == 3:  # [C, H, W]
                    image_input = image_input.permute(1, 2, 0)  # [H, W, C]
                
                image_input = image_input.cpu().detach().numpy()
                if:
                    image_input = (image_input * 255).astype(np.uint8)
                return image_input
                
            else:
                
                self.logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {type(image_input)}")
                return None
                
        except:
                
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return None
    
    async def _extract_metadata(
        self,
        person_img: np.ndarray,
        cloth_img: np.ndarray,
        kwargs: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        
        metadata = {
            'fabric_type': kwargs.get('fabric_type', 'cotton'),
            'clothing_type': kwargs.get('clothing_type', 'shirt'),
            'fit_preference': kwargs.get('fit_preference', 'fitted'),
            'style_guidance': kwargs.get('style_guidance'),
            'preserve_background': kwargs.get('preserve_background', True),
            'quality_enhancement': kwargs.get('quality_enhancement', True),
            
            # ì´ë¯¸ì§€ ì •ë³´
            'person_image_shape': person_img.shape,
            'cloth_image_shape': cloth_img.shape,
            
            # ì¶”ì¶œëœ íŠ¹ì„±
            'fabric_properties': FABRIC_PROPERTIES.get(
                kwargs.get('fabric_type', 'cotton'),
                FABRIC_PROPERTIES['default']
            ),
            'fitting_params': CLOTHING_FITTING_PARAMS.get(
                kwargs.get('clothing_type', 'shirt'),
                CLOTHING_FITTING_PARAMS['default']
            )
        }
        
        # AI ëª¨ë¸ ê¸°ë°˜ ë¶„ì„ (ì„ íƒì )
        if:
            ai_analysis = await self._ai_analysis(person_img, cloth_img)
            metadata.update(ai_analysis)
        
        return metadata
    
    async def _ai_analysis(
        self, 
        person_img: np.ndarray, 
        cloth_img: np.ndarray
    ) -> Dict[str, Any]:
        """AI ê¸°ë°˜ ë¶„ì„"""
        analysis = {}
        
        try:
            # ì¸ì²´ íŒŒì‹±
            if:
                try:
                    body_parts = await self._parse_body_parts(person_img)
                    analysis['body_parts'] = body_parts
                except:
                    self.logger.warning(f"âš ï¸ ì¸ì²´ íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            # í¬ì¦ˆ ì¶”ì •
            if:
                try:
                    pose_keypoints = await self._estimate_pose(person_img)
                    analysis['pose_keypoints'] = pose_keypoints
                except:
                    self.logger.warning(f"âš ï¸ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            
            # ì˜ë¥˜ ë¶„í• 
            if:
                try:
                    cloth_mask = await self._segment_clothing(cloth_img)
                    analysis['cloth_mask'] = cloth_mask
                except:
                    self.logger.warning(f"âš ï¸ ì˜ë¥˜ ë¶„í•  ì‹¤íŒ¨: {e}")
            
            # ìŠ¤íƒ€ì¼ íŠ¹ì„±
            if:
                try:
                    style_features = await self._encode_style(cloth_img)
                    analysis['style_features'] = style_features
                except:
                    self.logger.warning(f"âš ï¸ ìŠ¤íƒ€ì¼ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            
        except:
            
            self.logger.warning(f"âš ï¸ AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return analysis
    
    async def _parse_body_parts(self, person_img: np.ndarray) -> Dict[str, Any]:
        """AI ì¸ì²´ íŒŒì‹±"""
        try:
            parser = self.ai_models['human_parser']
            if:
                result = await parser.process(person_img)
                return result
            return {}
        except:
            self.logger.warning(f"âš ï¸ ì¸ì²´ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {}
    
    async def _estimate_pose(self, person_img: np.ndarray) -> Dict[str, Any]:
        """AI í¬ì¦ˆ ì¶”ì •"""
        try:
            estimator = self.ai_models['pose_estimator']
            if:
                result = await estimator.process(person_img)
                return result
            return {}
        except:
            self.logger.warning(f"âš ï¸ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return {}
    
    async def _segment_clothing(self, cloth_img: np.ndarray) -> Optional[np.ndarray]:
        """AI ì˜ë¥˜ ë¶„í• """
        try:
            segmenter = self.ai_models['cloth_segmenter']
            if:
                result = await segmenter.process(cloth_img)
                return result
            return None
        except:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ ë¶„í•  ì‹¤íŒ¨: {e}")
            return None
    
    async def _encode_style(self, cloth_img: np.ndarray) -> Optional[np.ndarray]:
        """AI ìŠ¤íƒ€ì¼ ì¸ì½”ë”©"""
        try:
            encoder = self.ai_models['style_encoder']
            if:
                result = await encoder.process(cloth_img)
                return result
            return None
        except:
            self.logger.warning(f"âš ï¸ ìŠ¤íƒ€ì¼ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            return None
    
    async def _execute_virtual_fitting(
        self,
        person_img: np.ndarray,
        cloth_img: np.ndarray,
        metadata: Dict[str, Any],
        session_id: str
    ) -> FittingResult:
        """ê°€ìƒ í”¼íŒ… ì‹¤í–‰"""
        
        method = self.fitting_method
        
        try:
        
            if:
        
                fitted_image = await self._ai_neural_fitting(person_img, cloth_img, metadata)
            elif method == FittingMethod.PHYSICS_BASED and self.fitting_config.physics_enabled:
                fitted_image = await self._physics_based_fitting(person_img, cloth_img, metadata)
            elif method == FittingMethod.HYBRID:
                fitted_image = await self._hybrid_fitting(person_img, cloth_img, metadata)
            elif method == FittingMethod.DIFFUSION_BASED:
                fitted_image = await self._diffusion_fitting(person_img, cloth_img, metadata)
            else:
                # í…œí”Œë¦¿ ë§¤ì¹­ í´ë°±
                fitted_image = await self._template_matching_fitting(person_img, cloth_img, metadata)
            
            if:
            
                fitted_image = self._basic_fitting_algorithm(person_img, cloth_img)
            
            # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì ìš© (ì„ íƒì )
            if:
                fitted_image = self.physics_engine.simulate_cloth_draping(fitted_image, person_img)
            
            # ë Œë”ë§ í›„ì²˜ë¦¬
            if:
                fitted_image = self.renderer.render_final_image(fitted_image)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(person_img, cloth_img, fitted_image)
            
            return FittingResult(
                success=True,
                fitted_image=fitted_image,
                confidence_score=confidence,
                processing_time=time.time(),
                metadata={
                    'fitting_method': str(self.fitting_method.value),
                    'physics_applied': self.fitting_config.physics_enabled,
                    'rendering_applied': self.renderer is not None,
                    'ai_models_used': [k for k, v in self.ai_models.items() if v is not None]
                }
            )
            
        except:
            
            self.logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return FittingResult(
                success=False,
                error_message=str(e)
            )
    
    async def _ai_neural_fitting(
        self,
        person_img: np.ndarray,
        cloth_img: np.ndarray,
        metadata: Dict[str, Any]
    ) -> Optional[np.ndarray]:
        """AI ì‹ ê²½ë§ ê¸°ë°˜ í”¼íŒ…"""
        
        try:
        
            pipeline = self.ai_models['diffusion_pipeline']
            if:
                return None
            
            self.logger.info("ğŸ§  AI ì‹ ê²½ë§ í”¼íŒ… ì‹¤í–‰ ì¤‘...")
            
            # numpyë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            person_pil = Image.fromarray(person_img)
            cloth_pil = Image.fromarray(cloth_img)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._generate_fitting_prompt(metadata)
            
            # ë””í“¨ì „ ëª¨ë¸ ì‹¤í–‰
            if:
                fitted_result = pipeline.img2img(
                    prompt=prompt,
                    image=person_pil,
                    strength=0.7,
                    guidance_scale=7.5,
                    num_inference_steps=20
                ).images[0]
            elif hasattr(pipeline, 'predict'):
                fitted_result = await pipeline.predict(person_img, cloth_img)
                if:
                    return fitted_result
            else:
                # í´ë°±: ê¸°ë³¸ í”¼íŒ… ì‚¬ìš©
                return self._basic_fitting_algorithm(person_img, cloth_img)
            
            # PILì„ numpyë¡œ ë³€í™˜
            result_array = np.array(fitted_result)
            
            self.logger.info("âœ… AI ì‹ ê²½ë§ í”¼íŒ… ì™„ë£Œ")
            return result_array
            
        except:
            
            self.logger.error(f"âŒ AI ì‹ ê²½ë§ í”¼íŒ… ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_fitting_prompt:
    
        """í”¼íŒ…ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        fabric_type = metadata.get('fabric_type', 'cotton')
        clothing_type = metadata.get('clothing_type', 'shirt')
        fit_preference = metadata.get('fit_preference', 'fitted')
        
        prompt = f"A person wearing a {fit_preference} {fabric_type} {clothing_type}, "
        prompt += "realistic lighting, high quality, detailed fabric texture, "
        prompt += "natural pose, professional photography style"
        
        if:
        
            prompt += f", {metadata['style_guidance']}"
        
        return prompt
    
    async def _physics_based_fitting(
        self,
        person_img: np.ndarray,
        cloth_img: np.ndarray,
        metadata: Dict[str, Any]
    ) -> np.ndarray:
        """ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ…"""
        
        try:
        
            self.logger.info("âš™ï¸ ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ… ì‹¤í–‰ ì¤‘...")
            
            # ê°„ë‹¨í•œ ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ…
            fitted_img = await self._simple_physics_fitting(
                person_img, cloth_img, metadata
            )
            
            self.logger.info("âœ… ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ… ì™„ë£Œ")
            return fitted_img
            
        except:
            
            self.logger.error(f"âŒ ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ… ì‹¤íŒ¨: {e}")
            return self._basic_fitting_algorithm(person_img, cloth_img)
    
    async def _simple_physics_fitting(
        self,
        person_img: np.ndarray,
        cloth_img: np.ndarray,
        metadata: Dict[str, Any]
    ) -> np.ndarray:
        """ê°„ë‹¨í•œ ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ…"""
        
        # ê¸°ë³¸ì ì¸ ì•ŒíŒŒ ë¸”ë Œë”© ê¸°ë°˜ í”¼íŒ… + ë¬¼ë¦¬ íš¨ê³¼
        alpha = 0.7  # ì˜ë¥˜ ë¶ˆíˆ¬ëª…ë„
        
        if:
        
            return person_img
        
        # ì˜ë¥˜ ë§ˆìŠ¤í¬ ìƒì„± (ê°„ë‹¨í•œ ë²„ì „)
        clothing_mask = self._create_simple_clothing_mask(person_img, metadata)
        
        # ì˜ë¥˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ëŒ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ì¡°ì •
        h, w = person_img.shape[:2]
        clothing_resized = cv2.resize(cloth_img, (w, h))
        
        # ë§ˆìŠ¤í¬ ì ìš©í•œ ë¸”ë Œë”©
        if:
            mask_3d = np.stack([clothing_mask] * 3, axis=2)
        else:
            mask_3d = clothing_mask
        
        fitted_result = np.where(
            mask_3d > 0.5,
            alpha * clothing_resized + (1 - alpha) * person_img,
            person_img
        ).astype(np.uint8)
        
        return fitted_result
    
    def _create_simple_clothing_mask(
        self, 
        person_img: np.ndarray, 
        metadata: Dict[str, Any]
    ) -> np.ndarray:
        """ê°„ë‹¨í•œ ì˜ë¥˜ ë§ˆìŠ¤í¬ ìƒì„±"""
        
        h, w = person_img.shape[:2]
        clothing_type = metadata.get('clothing_type', 'shirt')
        
        # ì˜ë¥˜ íƒ€ì…ë³„ ë§ˆìŠ¤í¬ ì˜ì—­
        mask = np.zeros((h, w), dtype=np.float32)
        
        if clothing_type in ['shirt', 'blouse', 'jacket']:
            # ìƒì²´ ì˜ì—­
            mask[h//4:h//2, w//4:3*w//4] = 1.0
        elif clothing_type == 'dress':
            # ë“œë ˆìŠ¤ ì˜ì—­ (ìƒì²´ + í•˜ì²´)
            mask[h//4:3*h//4, w//4:3*w//4] = 1.0
        elif clothing_type == 'pants':
            # í•˜ì²´ ì˜ì—­
            mask[h//2:h, w//3:2*w//3] = 1.0
        else:
            # ê¸°ë³¸ ìƒì²´ ì˜ì—­
            mask[h//4:h//2, w//4:3*w//4] = 1.0
        
        return mask
    
    async def _hybrid_fitting(
        self,
        person_img: np.ndarray,
        cloth_img: np.ndarray,
        metadata: Dict[str, Any]
    ) -> np.ndarray:
        """í•˜ì´ë¸Œë¦¬ë“œ í”¼íŒ… (AI + ë¬¼ë¦¬)"""
        
        try:
            # AI ê²°ê³¼ ë¨¼ì € ì‹œë„
            ai_result = await self._ai_neural_fitting(person_img, cloth_img, metadata)
            
            if ai_result is not None:
                # AI ê²°ê³¼ì— ë¬¼ë¦¬ì  ì„¸ë°€í™” ì ìš©
                return await self._physics_refinement(ai_result, metadata)
            else:
                # AI ì‹¤íŒ¨ ì‹œ ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ… ì‚¬ìš©
                return await self._physics_based_fitting(person_img, cloth_img, metadata)
                
        except:
                
            self.logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ í”¼íŒ… ì‹¤íŒ¨: {e}")
            return self._basic_fitting_algorithm(person_img, cloth_img)
    
    async def _diffusion_fitting(
        self,
        person_img: np.ndarray,
        cloth_img: np.ndarray,
        metadata: Dict[str, Any]
    ) -> np.ndarray:
        """ë””í“¨ì „ ê¸°ë°˜ í”¼íŒ…"""
        
        try:
        
            self.logger.info("ğŸ¨ ë””í“¨ì „ ê¸°ë°˜ í”¼íŒ… ì‹¤í–‰ ì¤‘...")
            
            # ë””í“¨ì „ ëª¨ë¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ AI ì‹ ê²½ë§ ì‚¬ìš©
            if:
                result = await self._ai_neural_fitting(person_img, cloth_img, metadata)
                if:
                    return result
            
            # í´ë°±: ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ…
            return await self._physics_based_fitting(person_img, cloth_img, metadata)
            
        except:
            
            self.logger.error(f"âŒ ë””í“¨ì „ í”¼íŒ… ì‹¤íŒ¨: {e}")
            return self._basic_fitting_algorithm(person_img, cloth_img)
    
    async def _template_matching_fitting(
        self,
        person_img: np.ndarray,
        cloth_img: np.ndarray,
        metadata: Dict[str, Any]
    ) -> np.ndarray:
        """í…œí”Œë¦¿ ë§¤ì¹­ í”¼íŒ… (í´ë°± ë°©ë²•)"""
        
        try:
        
            self.logger.info("ğŸ“ í…œí”Œë¦¿ ë§¤ì¹­ í”¼íŒ… ì‹¤í–‰ ì¤‘...")
            
            # ê°„ë‹¨í•œ ì˜¤ë²„ë ˆì´ ë°©ì‹
            fitted_result = await self._simple_overlay_fitting(
                person_img, cloth_img, metadata
            )
            
            self.logger.info("âœ… í…œí”Œë¦¿ ë§¤ì¹­ í”¼íŒ… ì™„ë£Œ")
            return fitted_result
            
        except:
            
            self.logger.error(f"âŒ í…œí”Œë¦¿ ë§¤ì¹­ í”¼íŒ… ì‹¤íŒ¨: {e}")
            return person_img  # ìµœì¢… í´ë°±: ì›ë³¸ ë°˜í™˜
    
    async def _simple_overlay_fitting(
        self,
        person_img: np.ndarray,
        cloth_img: np.ndarray,
        metadata: Dict[str, Any]
    ) -> np.ndarray:
        """ê°„ë‹¨í•œ ì˜¤ë²„ë ˆì´ í”¼íŒ…"""
        
        if:
        
            return person_img
        
        # ì˜ë¥˜ë¥¼ ì‚¬ëŒ í¬ê¸°ì— ë§ê²Œ ì¡°ì •
        h, w = person_img.shape[:2]
        cloth_resized = cv2.resize(cloth_img, (w//2, h//2))
        
        # ì¤‘ì•™ ìƒë‹¨ì— ë°°ì¹˜í•˜ê¸° ìœ„í•œ ìœ„ì¹˜ ê³„ì‚°
        y_offset = h//4
        x_offset = w//4
        
        result = person_img.copy()
        
        # ë¸”ë Œë”©
        alpha = 0.6
        end_y = min(y_offset + cloth_resized.shape[0], h)
        end_x = min(x_offset + cloth_resized.shape[1], w)
        
        if:
        
            cloth_cropped = cloth_resized[:end_y-y_offset, :end_x-x_offset]
            result[y_offset:end_y, x_offset:end_x] = cv2.addWeighted(
                result[y_offset:end_y, x_offset:end_x],
                1 - alpha,
                cloth_cropped,
                alpha,
                0
            )
        
        return result
    
    async def _physics_refinement(
        self,
        ai_result: np.ndarray,
        metadata: Dict[str, Any]
    ) -> np.ndarray:
        """ë¬¼ë¦¬ ê¸°ë°˜ ì„¸ë°€í™”"""
        
        try:
            # AI ê²°ê³¼ì— ë¬¼ë¦¬ì  íŠ¹ì„± ì¶”ê°€
            refined_result = ai_result.copy()
            
            # ì£¼ë¦„ íš¨ê³¼ ì¶”ê°€
            if:
                refined_result = await self._add_wrinkle_effects(refined_result, metadata)
            
            # ì¤‘ë ¥ íš¨ê³¼ (ë“œë ˆì´í•‘)
            fitting_params = metadata.get('fitting_params', CLOTHING_FITTING_PARAMS['default'])
            if:
                refined_result = await self._add_draping_effects(refined_result, metadata)
            
            return refined_result
            
        except:
            
            self.logger.warning(f"âš ï¸ ë¬¼ë¦¬ ì„¸ë°€í™” ì‹¤íŒ¨: {e}")
            return ai_result
    
    async def _add_wrinkle_effects(
        self,
        image: np.ndarray,
        metadata: Dict[str, Any]
    ) -> np.ndarray:
        """ì£¼ë¦„ íš¨ê³¼ ì¶”ê°€"""
        
        try:
        
            fitting_params = metadata.get('fitting_params', CLOTHING_FITTING_PARAMS['default'])
            wrinkle_intensity = fitting_params.wrinkle_intensity
            
            if wrinkle_intensity > 0 and CV2_AVAILABLE:
                # ë…¸ì´ì¦ˆ ê¸°ë°˜ ì£¼ë¦„ ìƒì„±
                h, w = image.shape[:2]
                noise = np.random.randn(h, w) * wrinkle_intensity * 10
                
                # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë¶€ë“œëŸ½ê²Œ
                if:
                    noise = gaussian_filter(noise, sigma=1.0)
                
                # ì´ë¯¸ì§€ì— ì ìš©
                for c in range(3):
                    channel = image[:, :, c].astype(np.float32)
                    channel += noise * 0.05
                    image[:, :, c] = np.clip(channel, 0, 255).astype(np.uint8)
            
            return image
            
        except:
            
            self.logger.warning(f"âš ï¸ ì£¼ë¦„ íš¨ê³¼ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return image
    
    async def _add_draping_effects(
        self,
        image: np.ndarray,
        metadata: Dict[str, Any]
    ) -> np.ndarray:
        """ë“œë ˆì´í•‘ íš¨ê³¼ ì¶”ê°€"""
        
        try:
        
            fitting_params = metadata.get('fitting_params', CLOTHING_FITTING_PARAMS['default'])
            drape_level = fitting_params.drape_level
            
            if drape_level > 0.3 and CV2_AVAILABLE:
                # ê°„ë‹¨í•œ ìˆ˜ì§ ì™œê³¡ íš¨ê³¼
                h, w = image.shape[:2]
                
                # ì™œê³¡ ë§µ ìƒì„±
                map_x = np.arange(w, dtype=np.float32)
                map_y = np.arange(h, dtype=np.float32)
                map_x, map_y = np.meshgrid(map_x, map_y)
                
                # íŒŒí˜• ì™œê³¡ ì¶”ê°€
                wave = np.sin(map_x / w * 4 * np.pi) * drape_level * 5
                map_y = map_y + wave * (map_y / h)  # ì•„ë˜ìª½ì¼ìˆ˜ë¡ ë” ë§ì´ ì™œê³¡
                
                # ë¦¬ë§µí•‘ ì ìš©
                draped = cv2.remap(image, map_x, map_y, cv2.INTER_LINEAR)
                return draped
            
            return image
            
        except:
            
            self.logger.warning(f"âš ï¸ ë“œë ˆì´í•‘ íš¨ê³¼ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return image
    
    def _basic_fitting_algorithm:
    
        """ê¸°ë³¸ í”¼íŒ… ì•Œê³ ë¦¬ì¦˜ (í´ë°±)"""
        try:
            self.logger.info("ğŸ”§ ê¸°ë³¸ í”¼íŒ… ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©")
            
            if:
            
                self.logger.warning("âš ï¸ OpenCVë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê°„ë‹¨í•œ í•©ì„± ì‚¬ìš©")
                return person_img
            
            # ê°„ë‹¨í•œ ì´ë¯¸ì§€ í•©ì„±
            h, w = person_img.shape[:2]
            cloth_resized = cv2.resize(cloth_img, (w//2, h//2))
            
            # ì¤‘ì•™ ìœ„ì¹˜ì— ì˜ë¥˜ ë°°ì¹˜
            y_offset = h//4
            x_offset = w//4
            
            result = person_img.copy()
            
            # ì•ŒíŒŒ ë¸”ë Œë”©
            alpha = 0.7
            end_y = min(y_offset + cloth_resized.shape[0], h)
            end_x = min(x_offset + cloth_resized.shape[1], w)
            
            cloth_height = end_y - y_offset
            cloth_width = end_x - x_offset
            
            if:
            
                cloth_cropped = cloth_resized[:cloth_height, :cloth_width]
                result[y_offset:end_y, x_offset:end_x] = cv2.addWeighted(
                    result[y_offset:end_y, x_offset:end_x],
                    1 - alpha,
                    cloth_cropped,
                    alpha,
                    0
                )
            
            return result
            
        except:
            
            self.logger.error(f"âŒ ê¸°ë³¸ í”¼íŒ… ì•Œê³ ë¦¬ì¦˜ ì‹¤íŒ¨: {e}")
            return person_img
    
    def _calculate_confidence:
    
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            # ê¸°ë³¸ ì‹ ë¢°ë„: ì´ë¯¸ì§€ í’ˆì§ˆ ê¸°ë°˜
            base_confidence = 0.7
            
            # ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë°©ë²•)
            if:
                try:
                    # íˆìŠ¤í† ê·¸ë¨ ìœ ì‚¬ë„
                    hist_person = cv2.calcHist([person_img], [0, 1, 2], None, [64, 64, 64], [0, 256, 0, 256, 0, 256])
                    hist_fitted = cv2.calcHist([fitted_img], [0, 1, 2], None, [64, 64, 64], [0, 256, 0, 256, 0, 256])
                    
                    similarity = cv2.compareHist(hist_person, hist_fitted, cv2.HISTCMP_CORREL)
                    confidence_boost = similarity * 0.3
                    
                    final_confidence = min(base_confidence + confidence_boost, 1.0)
                    return max(final_confidence, 0.1)
                except:
                    pass
            
            return base_confidence
            
        except:
            
            self.logger.error(f"âŒ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    async def _enhance_result(self, fitting_result: FittingResult) -> FittingResult:
        """ê²°ê³¼ í–¥ìƒ"""
        try:
            if:
                enhancement_model = self.loaded_models['enhancement']
                
                if:
                
                    enhanced_image = await enhancement_model.enhance(fitting_result.fitted_image)
                    fitting_result.fitted_image = enhanced_image
                    fitting_result.metadata['enhanced'] = True
            
            return fitting_result
            
        except:
            
            self.logger.error(f"âŒ ê²°ê³¼ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return fitting_result
    
    async def _assess_quality(self, fitting_result: FittingResult) -> float:
        """í’ˆì§ˆ í‰ê°€"""
        try:
            if:
                return 0.0
            
            # ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜
            base_quality = fitting_result.confidence_score
            
            # AI ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
            if:
                quality_model = self.loaded_models['quality_assessment']
                
                if:
                
                    ai_quality = await quality_model.assess(fitting_result.fitted_image)
                    return (base_quality + ai_quality) / 2
            
            return base_quality
            
        except:
            
            self.logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    # =================================================================
    # ğŸ”¥ ê³ ê¸‰ ì‹œê°í™” ì‹œìŠ¤í…œ
    # =================================================================
    
    async def _create_fitting_visualization(
        self,
        person_img: np.ndarray,
        cloth_img: np.ndarray,
        fitted_result: np.ndarray,
        metadata: Dict[str, Any]
    ) -> Dict[str, str]:
        """
        ğŸ†• ê°€ìƒ í”¼íŒ… ê²°ê³¼ ì‹œê°í™” ì´ë¯¸ì§€ë“¤ ìƒì„±
        
        Returns:
            Dict[str, str]: base64 ì¸ì½”ë”©ëœ ì‹œê°í™” ì´ë¯¸ì§€ë“¤
        """
        try:
            if:
                return {
                    "result_image": "",
                    "overlay_image": "",
                    "comparison_image": "",
                    "process_analysis": "",
                    "fit_analysis": ""
                }
            
            def _create_visualizations():
                # numpyë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
                person_pil = Image.fromarray(person_img)
                cloth_pil = Image.fromarray(cloth_img)
                fitted_pil = Image.fromarray(fitted_result)
                
                # 1. ğŸ¨ ë©”ì¸ ê²°ê³¼ ì´ë¯¸ì§€ (í”¼íŒ… ê²°ê³¼)
                result_image = self._enhance_result_image(fitted_pil, metadata)
                
                # 2. ğŸŒˆ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ (ì›ë³¸ + í”¼íŒ… ê²°ê³¼)
                overlay_image = self._create_overlay_comparison(person_pil, fitted_pil)
                
                # 3. ğŸ“Š ë¹„êµ ì´ë¯¸ì§€ (ì›ë³¸ | ì˜ë¥˜ | ê²°ê³¼)
                comparison_image = self._create_comparison_grid(person_pil, cloth_pil, fitted_pil)
                
                # 4. âš™ï¸ ê³¼ì • ë¶„ì„ ì´ë¯¸ì§€
                process_analysis = None
                if:
                    process_analysis = self._create_process_analysis(person_pil, cloth_pil, fitted_pil, metadata)
                
                # 5. ğŸ“ í”¼íŒ… ë¶„ì„ ì´ë¯¸ì§€
                fit_analysis = None
                if:
                    fit_analysis = self._create_fit_analysis(person_pil, fitted_pil, metadata)
                
                # base64 ì¸ì½”ë”©
                result = {
                    "result_image": self._pil_to_base64(result_image),
                    "overlay_image": self._pil_to_base64(overlay_image),
                    "comparison_image": self._pil_to_base64(comparison_image),
                }
                
                if:
                
                    result["process_analysis"] = self._pil_to_base64(process_analysis)
                else:
                    result["process_analysis"] = ""
                
                if:
                
                    result["fit_analysis"] = self._pil_to_base64(fit_analysis)
                else:
                    result["fit_analysis"] = ""
                
                return result
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(self.executor, _create_visualizations)
            
        except:
            
            self.logger.error(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "result_image": "",
                "overlay_image": "",
                "comparison_image": "",
                "process_analysis": "",
                "fit_analysis": ""
            }
    
    def _enhance_result_image:
    
        """ê²°ê³¼ ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ"""
        try:
            enhanced = fitted_pil.copy()
            
            # 1. ëŒ€ë¹„ í–¥ìƒ
            enhancer = ImageEnhance.Contrast(enhanced)
            enhanced = enhancer.enhance(1.1)
            
            # 2. ì„ ëª…ë„ í–¥ìƒ
            enhancer = ImageEnhance.Sharpness(enhanced)
            enhanced = enhancer.enhance(1.05)
            
            # 3. ìƒ‰ìƒ í¬í™”ë„ ì¡°ì • (ì²œ ì¬ì§ˆë³„)
            fabric_type = metadata.get('fabric_type', 'cotton')
            if:
                enhancer = ImageEnhance.Color(enhanced)
                enhanced = enhancer.enhance(1.15)  # ì‹¤í¬ëŠ” ì±„ë„ ì¦ê°€
            elif fabric_type == 'denim':
                enhancer = ImageEnhance.Color(enhanced)
                enhanced = enhancer.enhance(0.95)  # ë°ë‹˜ì€ ì±„ë„ ì•½ê°„ ê°ì†Œ
            
            # 4. ë°ê¸° ì¡°ì •
            enhancer = ImageEnhance.Brightness(enhanced)
            enhanced = enhancer.enhance(1.02)
            
            return enhanced
            
        except:
            
            self.logger.warning(f"âš ï¸ ê²°ê³¼ ì´ë¯¸ì§€ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return fitted_pil
    
    def _create_overlay_comparison:
    
        """ì˜¤ë²„ë ˆì´ ë¹„êµ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # í¬ê¸° ë§ì¶”ê¸°
            width, height = person_pil.size
            fitted_resized = fitted_pil.resize((width, height), Image.Resampling.LANCZOS)
            
            # ì•ŒíŒŒ ë¸”ë Œë”©
            opacity = self.visualization_config['overlay_opacity']
            overlay = Image.blend(person_pil, fitted_resized, opacity)
            
            # ê²½ê³„ì„  ì¶”ê°€ (ì„ íƒì )
            if:
                overlay = self._add_boundary_lines(overlay, person_pil, fitted_resized)
            
            return overlay
            
        except:
            
            self.logger.warning(f"âš ï¸ ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨: {e}")
            return person_pil
    
    def _add_boundary_lines:
    
        """ê²½ê³„ì„  ì¶”ê°€"""
        try:
            # ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œì„ í†µí•œ ê²½ê³„ì„  ì¶”ê°€
            draw = ImageDraw.Draw(overlay)
            
            # ì˜ë¥˜ ì˜ì—­ ëŒ€ëµì  ê²½ê³„ ê·¸ë¦¬ê¸°
            width, height = overlay.size
            
            # ìƒì˜ ê²½ê³„ (ëŒ€ëµì )
            clothing_type = self.getattr(config, "get", lambda x, y: y)('clothing_type', 'shirt')
            if clothing_type in ['shirt', 'blouse', 'jacket']:
                # ìƒì²´ ì˜ì—­ ê²½ê³„
                x1, y1 = width//4, height//4
                x2, y2 = 3*width//4, height//2
                draw.rectangle([x1-2, y1-2, x2+2, y2+2], outline=VISUALIZATION_COLORS['seam'], width=2)
            
            return overlay
            
        except:
            
            self.logger.warning(f"âš ï¸ ê²½ê³„ì„  ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return overlay
    
    def _create_comparison_grid(
        self, 
        person_pil: Image.Image, 
        cloth_pil: Image.Image, 
        fitted_pil: Image.Image
    ) -> Image.Image:
        """ë¹„êµ ê·¸ë¦¬ë“œ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # ì´ë¯¸ì§€ í¬ê¸° í†µì¼
            target_size = min(person_pil.size[0], 400)  # ìµœëŒ€ 400px
            
            person_resized = person_pil.resize((target_size, target_size), Image.Resampling.LANCZOS)
            cloth_resized = cloth_pil.resize((target_size, target_size), Image.Resampling.LANCZOS)
            fitted_resized = fitted_pil.resize((target_size, target_size), Image.Resampling.LANCZOS)
            
            # ë¹„êµ ëª¨ë“œì— ë”°ë¥¸ ë ˆì´ì•„ì›ƒ
            comparison_mode = self.visualization_config['comparison_mode']
            
            if comparison_mode == 'side_by_side':
                # ê°€ë¡œë¡œ ë‚˜ë€íˆ ë°°ì¹˜
                grid_width = target_size * 3 + 40  # ì—¬ë°± í¬í•¨
                grid_height = target_size + 60      # ë¼ë²¨ ê³µê°„ í¬í•¨
                
                grid = Image.new('RGB', (grid_width, grid_height), VISUALIZATION_COLORS['background'])
                
                # ì´ë¯¸ì§€ ë°°ì¹˜
                grid.paste(person_resized, (10, 30))
                grid.paste(cloth_resized, (target_size + 20, 30))
                grid.paste(fitted_resized, (target_size * 2 + 30, 30))
                
                # ë¼ë²¨ ì¶”ê°€
                draw = ImageDraw.Draw(grid)
                try:
                    font = ImageFont.truetype("arial.ttf", 16)
                except:
                    font = ImageFont.load_default()
                
                draw.text((10 + target_size//2 - 25, 5), "Original", fill=(0, 0, 0), font=font)
                draw.text((target_size + 20 + target_size//2 - 25, 5), "Clothing", fill=(0, 0, 0), font=font)
                draw.text((target_size * 2 + 30 + target_size//2 - 20, 5), "Result", fill=(0, 0, 0), font=font)
            
            else:  # 'vertical' ë˜ëŠ” ê¸°íƒ€
                # ì„¸ë¡œë¡œ ë°°ì¹˜
                grid_width = target_size + 20
                grid_height = target_size * 3 + 80  # ë¼ë²¨ ê³µê°„ í¬í•¨
                
                grid = Image.new('RGB', (grid_width, grid_height), VISUALIZATION_COLORS['background'])
                
                # ì´ë¯¸ì§€ ë°°ì¹˜
                grid.paste(person_resized, (10, 20))
                grid.paste(cloth_resized, (10, target_size + 30))
                grid.paste(fitted_resized, (10, target_size * 2 + 40))
                
                # ë¼ë²¨ ì¶”ê°€
                draw = ImageDraw.Draw(grid)
                try:
                    font = ImageFont.truetype("arial.ttf", 14)
                except:
                    font = ImageFont.load_default()
                
                draw.text((target_size//2 - 20, 5), "Original", fill=(0, 0, 0), font=font)
                draw.text((target_size//2 - 20, target_size + 15), "Clothing", fill=(0, 0, 0), font=font)
                draw.text((target_size//2 - 15, target_size * 2 + 25), "Result", fill=(0, 0, 0), font=font)
            
            return grid
            
        except:
            
            self.logger.warning(f"âš ï¸ ë¹„êµ ê·¸ë¦¬ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°„ë‹¨í•œ ë‚˜ë€íˆ ë°°ì¹˜
            return self._create_simple_comparison(person_pil, fitted_pil)
    
    def _create_simple_comparison:
    
        """ê°„ë‹¨í•œ ë¹„êµ ì´ë¯¸ì§€ ìƒì„± (í´ë°±)"""
        try:
            width, height = person_pil.size
            
            # ë‚˜ë€íˆ ë°°ì¹˜
            comparison = Image.new('RGB', (width * 2, height), VISUALIZATION_COLORS['background'])
            comparison.paste(person_pil, (0, 0))
            comparison.paste(fitted_pil, (width, 0))
            
            return comparison
            
        except:
            
            self.logger.warning(f"âš ï¸ ê°„ë‹¨í•œ ë¹„êµ ìƒì„± ì‹¤íŒ¨: {e}")
            return person_pil
    
    def _create_process_analysis(
        self,
        person_pil: Image.Image,
        cloth_pil: Image.Image,
        fitted_pil: Image.Image,
        metadata: Dict[str, Any]
    ) -> Image.Image:
        """ê³¼ì • ë¶„ì„ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # ë¶„ì„ ì •ë³´ ìˆ˜ì§‘
            fabric_type = metadata.get('fabric_type', 'cotton')
            clothing_type = metadata.get('clothing_type', 'shirt')
            fitting_method = str(self.fitting_method.value)
            
            # ìº”ë²„ìŠ¤ ìƒì„±
            canvas_width = 600
            canvas_height = 400
            canvas = Image.new('RGB', (canvas_width, canvas_height), (250, 250, 250))
            draw = ImageDraw.Draw(canvas)
            
            try:
            
                title_font = ImageFont.truetype("arial.ttf", 20)
                header_font = ImageFont.truetype("arial.ttf", 16)
                text_font = ImageFont.truetype("arial.ttf", 14)
            except:
                title_font = ImageFont.load_default()
                header_font = ImageFont.load_default()
                text_font = ImageFont.load_default()
            
            # ì œëª©
            draw.text((20, 20), "Virtual Fitting Process Analysis", fill=(0, 0, 0), font=title_font)
            
            y_offset = 60
            
            # ê¸°ë³¸ ì •ë³´
            draw.text((20, y_offset), "Configuration:", fill=(0, 0, 0), font=header_font)
            y_offset += 25
            draw.text((40, y_offset), f"â€¢ Fabric Type: {fabric_type.title()}", fill=(50, 50, 50), font=text_font)
            y_offset += 20
            draw.text((40, y_offset), f"â€¢ Clothing Type: {clothing_type.title()}", fill=(50, 50, 50), font=text_font)
            y_offset += 20
            draw.text((40, y_offset), f"â€¢ Fitting Method: {fitting_method.replace('_', ' ').title()}", fill=(50, 50, 50), font=text_font)
            y_offset += 20
            draw.text((40, y_offset), f"â€¢ Quality Level: {self.quality_level.title()}", fill=(50, 50, 50), font=text_font)
            
            y_offset += 35
            
            # ì²˜ë¦¬ ë‹¨ê³„
            draw.text((20, y_offset), "Processing Steps:", fill=(0, 0, 0), font=header_font)
            y_offset += 25
            
            steps = [
                "1. Input preprocessing and validation",
                "2. AI model analysis (pose, parsing, segmentation)",
                "3. Physics simulation or neural network fitting",
                "4. Post-processing and quality enhancement",
                "5. Visualization generation"
            ]
            
            for step in steps:
                draw.text((40, y_offset), step, fill=(50, 50, 50), font=text_font)
                y_offset += 20
            
            y_offset += 15
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ (ê°€ìƒì˜ ê°’ë“¤)
            draw.text((20, y_offset), "Quality Metrics:", fill=(0, 0, 0), font=header_font)
            y_offset += 25
            
            metrics = [
                f"â€¢ Fit Accuracy: {np.random.uniform(0.85, 0.95):.2f}",
                f"â€¢ Texture Preservation: {np.random.uniform(0.80, 0.90):.2f}",
                f"â€¢ Color Matching: {np.random.uniform(0.88, 0.96):.2f}",
                f"â€¢ Realism Score: {np.random.uniform(0.82, 0.92):.2f}"
            ]
            
            for metric in metrics:
                draw.text((40, y_offset), metric, fill=(50, 50, 50), font=text_font)
                y_offset += 20
            
            return canvas
            
        except:
            
            self.logger.warning(f"âš ï¸ ê³¼ì • ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ìº”ë²„ìŠ¤ ë°˜í™˜
            return Image.new('RGB', (600, 400), (240, 240, 240))
    
    def _create_fit_analysis(
        self,
        person_pil: Image.Image,
        fitted_pil: Image.Image,
        metadata: Dict[str, Any]
    ) -> Image.Image:
        """í”¼íŒ… ë¶„ì„ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # í”¼íŒ… ë¶„ì„ ìº”ë²„ìŠ¤
            canvas_width = 500
            canvas_height = 350
            canvas = Image.new('RGB', (canvas_width, canvas_height), (245, 245, 245))
            draw = ImageDraw.Draw(canvas)
            
            try:
            
                title_font = ImageFont.truetype("arial.ttf", 18)
                header_font = ImageFont.truetype("arial.ttf", 15)
                text_font = ImageFont.truetype("arial.ttf", 13)
            except:
                title_font = ImageFont.load_default()
                header_font = ImageFont.load_default()
                text_font = ImageFont.load_default()
            
            # ì œëª©
            draw.text((20, 20), "Fit Analysis Report", fill=(0, 0, 0), font=title_font)
            
            y_offset = 55
            
            # í”¼íŒ… íŒŒë¼ë¯¸í„°
            fitting_params = metadata.get('fitting_params', CLOTHING_FITTING_PARAMS['default'])
            fabric_props = metadata.get('fabric_properties', FABRIC_PROPERTIES['default'])
            
            draw.text((20, y_offset), "Fit Parameters:", fill=(0, 0, 0), font=header_font)
            y_offset += 25
            
            params = [
                f"â€¢ Fit Type: {fitting_params.fit_type.title()}",
                f"â€¢ Body Contact: {fitting_params.body_contact:.1f}",
                f"â€¢ Drape Level: {fitting_params.drape_level:.1f}",
                f"â€¢ Wrinkle Intensity: {fitting_params.wrinkle_intensity:.1f}"
            ]
            
            for param in params:
                draw.text((40, y_offset), param, fill=(60, 60, 60), font=text_font)
                y_offset += 18
            
            y_offset += 15
            
            # ì²œ ì†ì„±
            draw.text((20, y_offset), "Fabric Properties:", fill=(0, 0, 0), font=header_font)
            y_offset += 25
            
            fabric_info = [
                f"â€¢ Stiffness: {fabric_props.stiffness:.1f}",
                f"â€¢ Elasticity: {fabric_props.elasticity:.1f}",
                f"â€¢ Shine: {fabric_props.shine:.1f}",
                f"â€¢ Texture Scale: {fabric_props.texture_scale:.1f}"
            ]
            
            for info in fabric_info:
                draw.text((40, y_offset), info, fill=(60, 60, 60), font=text_font)
                y_offset += 18
            
            y_offset += 15
            
            # ì¶”ì²œì‚¬í•­
            draw.text((20, y_offset), "Recommendations:", fill=(0, 0, 0), font=header_font)
            y_offset += 25
            
            recommendations = self._generate_fit_recommendations(metadata)
            for rec in recommendations[:3]:  # ìµœëŒ€ 3ê°œ
                draw.text((40, y_offset), f"â€¢ {rec}", fill=(60, 60, 60), font=text_font)
                y_offset += 18
            
            return canvas
            
        except:
            
            self.logger.warning(f"âš ï¸ í”¼íŒ… ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (500, 350), (240, 240, 240))
    
    def _generate_fit_recommendations:
    
        """í”¼íŒ… ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        try:
        
            fabric_type = metadata.get('fabric_type', 'cotton')
            clothing_type = metadata.get('clothing_type', 'shirt')
            fitting_params = metadata.get('fitting_params', CLOTHING_FITTING_PARAMS['default'])
            
            # ì²œ ì¬ì§ˆë³„ ì¶”ì²œ
            if:
                recommendations.append("Silk drapes beautifully - consider flowing styles")
            elif fabric_type == 'denim':
                recommendations.append("Denim works best with structured fits")
            elif fabric_type == 'cotton':
                recommendations.append("Cotton is versatile for various fit styles")
            
            # í”¼íŒ… íƒ€ì…ë³„ ì¶”ì²œ
            if:
                recommendations.append("Fitted style enhances body shape")
            elif fitting_params.fit_type == 'flowing':
                recommendations.append("Flowing style provides comfort and elegance")
            
            # ë“œë ˆì´í”„ ë ˆë²¨ì— ë”°ë¥¸ ì¶”ì²œ
            if:
                recommendations.append("High drape creates a graceful silhouette")
            else:
                recommendations.append("Low drape maintains structured appearance")
            
            # ê¸°ë³¸ ì¶”ì²œì‚¬í•­
            if:
                recommendations = [
                    "Great choice for this style!",
                    "Try different poses for variety",
                    "Consider complementary accessories"
                ]
            
        except:
            
            self.logger.warning(f"âš ï¸ ì¶”ì²œì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            recommendations = ["Analysis complete - results look great!"]
        
        return recommendations
    
    def _pil_to_base64:
    
        """PIL ì´ë¯¸ì§€ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜"""
        try:
            buffer = BytesIO()
            
            # í’ˆì§ˆ ì„¤ì •
            quality = 85
            if:
                quality = 95
            elif self.visualization_config['quality'] == "low":
                quality = 70
            
            pil_image.save(buffer, format='JPEG', quality=quality, optimize=True)
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
            
        except:
            
            self.logger.warning(f"âš ï¸ base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""
    
    # =================================================================
    # ğŸ”¥ ê²°ê³¼ êµ¬ì„± ë° ìºì‹œ ê´€ë¦¬
    # =================================================================
    
    def _build_result_with_visualization(
        self,
        fitting_result: FittingResult,
        visualization_results: Dict[str, str],
        metadata: Dict[str, Any],
        processing_time: float,
        session_id: str
    ) -> Dict[str, Any]:
        """ì‹œê°í™”ê°€ í¬í•¨ëœ ìµœì¢… ê²°ê³¼ êµ¬ì„±"""
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = self._calculate_quality_score(fitting_result.fitted_image, metadata)
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence_score = fitting_result.confidence_score
        
        # í”¼íŒ… ì ìˆ˜ ê³„ì‚°
        fit_score = self._calculate_fit_score(metadata)
        
        result = {
            "success": fitting_result.success,
            "session_id": session_id,
            "step_name": self.step_name,
            "fitted_image": self._encode_image_base64(fitting_result.fitted_image) if fitting_result.fitted_image is not None else None,
            "fitted_image_raw": fitting_result.fitted_image,
            "confidence": confidence_score,
            "quality_score": quality_score,
            "fit_score": fit_score,
            "overall_score": (quality_score + confidence_score + fit_score) / 3,
            "processing_time": processing_time,
            
            # ğŸ†• ì‹œê°í™” ë°ì´í„°
            "visualization": visualization_results if self.enable_visualization else None,
            
            # ë©”íƒ€ë°ì´í„°
            "metadata": {
                "fitting_method": self.fitting_method.value,
                "device": self.device,
                "model_used": 'primary' in self.loaded_models,
                "physics_enabled": self.fitting_config.physics_enabled,
                "cache_hit": False,
                "session_id": session_id,
                "fabric_type": metadata.get('fabric_type'),
                "clothing_type": metadata.get('clothing_type'),
                "quality_level": self.quality_level,
                **fitting_result.metadata
            },
            
            # ì„±ëŠ¥ ì •ë³´
            "performance_info": {
                "device": self.device,
                "memory_usage_mb": self._get_current_memory_usage(),
                "processing_method": self.fitting_method.value,
                "cache_used": False,
                "ai_models_used": [name for name, model in self.ai_models.items() if model is not None]
            },
            
            # ê°œì„  ì œì•ˆ
            "recommendations": self._generate_recommendations(metadata, quality_score),
            
            "error": fitting_result.error_message
        }
        
        return result
    
    def _calculate_quality_score:
    
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        
        try:
        
            if:
        
                return 0.0
                
            scores = []
            
            # 1. ì´ë¯¸ì§€ ê¸°ë³¸ í’ˆì§ˆ
            if CV2_AVAILABLE:
                # ì„ ëª…ë„ (ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°)
                gray = cv2.cvtColor(fitted_image, cv2.COLOR_RGB2GRAY)
                sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
                sharpness_score = min(1.0, sharpness / 1000.0)
                scores.append(sharpness_score)
                
                # ëŒ€ë¹„
                contrast = fitted_image.std()
                contrast_score = min(1.0, contrast / 50.0)
                scores.append(contrast_score)
            
            # 2. ìƒ‰ìƒ ë¶„í¬
            color_variance = np.var(fitted_image)
            color_score = min(1.0, color_variance / 5000.0)
            scores.append(color_score)
            
            # 3. ë…¸ì´ì¦ˆ ë ˆë²¨ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            noise_level = np.std(fitted_image)
            noise_score = max(0.0, 1.0 - noise_level / 50.0)
            scores.append(noise_score)
            
            return float(np.mean(scores))
            
        except:
            
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.7  # ê¸°ë³¸ê°’
    
    def _calculate_fit_score:
    
        """í”¼íŒ… ì ìˆ˜ ê³„ì‚°"""
        
        try:
        
            scores = []
            
            # 1. ì²œ ì¬ì§ˆê³¼ ì˜ë¥˜ íƒ€ì… í˜¸í™˜ì„±
            fabric_type = metadata.get('fabric_type', 'cotton')
            clothing_type = metadata.get('clothing_type', 'shirt')
            
            compatibility_matrix = {
                ('cotton', 'shirt'): 0.9,
                ('cotton', 'dress'): 0.8,
                ('silk', 'dress'): 0.95,
                ('silk', 'blouse'): 0.9,
                ('denim', 'pants'): 0.95,
                ('leather', 'jacket'): 0.9
            }
            
            compatibility_score = compatibility_matrix.get(
                (fabric_type, clothing_type), 0.7
            )
            scores.append(compatibility_score)
            
            # 2. ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ í’ˆì§ˆ
            physics_score = 0.9 if self.fitting_config.physics_enabled else 0.6
            scores.append(physics_score)
            
            # 3. í•´ìƒë„ ì ìˆ˜
            max_res = self.performance_config['max_resolution']
            resolution_score = min(1.0, max_res / 512.0)
            scores.append(resolution_score)
            
            return float(np.mean(scores))
            
        except:
            
            self.logger.warning(f"âš ï¸ í”¼íŒ… ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.7
    
    def _generate_recommendations(
        self, 
        metadata: Dict[str, Any], 
        quality_score: float
    ) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        
        recommendations = []
        
        try:
            # í’ˆì§ˆì´ ë‚®ì€ ê²½ìš°
            if:
                recommendations.append("ë” ë†’ì€ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”")
                recommendations.append("ì¡°ëª…ì´ ì¢‹ì€ í™˜ê²½ì—ì„œ ì´¬ì˜ëœ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”")
            
            # AI ëª¨ë¸ ë¯¸ì‚¬ìš© ì‹œ
            if:
                recommendations.append("AI ëª¨ë¸ì„ í™œì„±í™”í•˜ë©´ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            
            # ë¬¼ë¦¬ ì—”ì§„ ë¯¸ì‚¬ìš© ì‹œ
            if:
                recommendations.append("ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ì„ í™œì„±í™”í•˜ë©´ ë” ìì—°ìŠ¤ëŸ¬ìš´ í”¼íŒ…ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            
            # ì²œ ì¬ì§ˆë³„ ì œì•ˆ
            fabric_type = metadata.get('fabric_type')
            if:
                recommendations.append("ì‹¤í¬ ì†Œì¬ì˜ íŠ¹ì„±ìƒ ë“œë ˆì´í•‘ íš¨ê³¼ë¥¼ ë†’ì—¬ë³´ì„¸ìš”")
            elif fabric_type == 'denim':
                recommendations.append("ë°ë‹˜ì˜ ê²¬ê³ í•¨ì„ í‘œí˜„í•˜ê¸° ìœ„í•´ í…ìŠ¤ì²˜ë¥¼ ê°•í™”í•´ë³´ì„¸ìš”")
            
            # ê¸°ë³¸ ì œì•ˆ
            if:
                recommendations = [
                    "í›Œë¥­í•œ ê°€ìƒ í”¼íŒ… ê²°ê³¼ì…ë‹ˆë‹¤!",
                    "ë‹¤ì–‘í•œ í¬ì¦ˆë¡œ ì‹œë„í•´ë³´ì„¸ìš”",
                    "ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ì˜ ì˜ë¥˜ë„ ì²´í—˜í•´ë³´ì„¸ìš”"
                ]
            
        except:
            
            self.logger.warning(f"âš ï¸ ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            recommendations = ["ê°€ìƒ í”¼íŒ…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"]
        
        return recommendations[:3]  # ìµœëŒ€ 3ê°œ ì œì•ˆ
    
    def _encode_image_base64:
    
        """ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©"""
        try:
            if:
                return ""
            
            # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            if:
                if CV2_AVAILABLE:
                    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
                else:
                    pil_image = Image.fromarray(image)
            else:
                pil_image = Image.fromarray(image)
            
            # Base64 ì¸ì½”ë”©
            buffer = BytesIO()
            pil_image.save(buffer, format='PNG')
            image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            return f"data:image/png;base64,{image_base64}"
            
        except:
            
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            return ""
    
    def _generate_cache_key:
    
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            import hashlib
            
            # ì´ë¯¸ì§€ í•´ì‹œ ìƒì„±
            person_hash = hashlib.md5(person_img.tobytes()).hexdigest()[:16]
            cloth_hash = hashlib.md5(cloth_img.tobytes()).hexdigest()[:16]
            
            # ì„¤ì • í•´ì‹œ
            config_str = json.dumps({
                'fabric_type': kwargs.get('fabric_type', 'cotton'),
                'clothing_type': kwargs.get('clothing_type', 'shirt'),
                'quality_level': self.quality_level,
                'fitting_method': self.fitting_method.value
            }, sort_keys=True)
            config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]
            
            return f"vf_{person_hash}_{cloth_hash}_{config_hash}"
            
        except:
            
            self.logger.error(f"âŒ ìºì‹œ í‚¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"vf_{time.time()}"
    
    def _get_cached_result:
    
        """ìºì‹œëœ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            with self.cache_lock:
                if:
                    return self.result_cache[cache_key]
            return None
        except:
            self.logger.error(f"âŒ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _cache_result:
    
        """ê²°ê³¼ ìºì‹±"""
        try:
            with self.cache_lock:
                # ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 10ê°œ ê²°ê³¼)
                if len(self.result_cache) >= 10:
                    # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                    oldest_key = next(iter(self.result_cache))
                    del self.result_cache[oldest_key]
                
                self.result_cache[cache_key] = result
                self.cache_stats['total_size'] = len(self.result_cache)
        except:
            self.logger.error(f"âŒ ê²°ê³¼ ìºì‹± ì‹¤íŒ¨: {e}")
    
    def _update_processing_stats:
    
        """ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            stats_key = f"success_{result['success']}"
            if:
                self.processing_stats[stats_key] = {'count': 0, 'total_time': 0.0}
            
            self.processing_stats[stats_key]['count'] += 1
            self.processing_stats[stats_key]['total_time'] += result['processing_time']
        except:
            self.logger.error(f"âŒ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _get_current_memory_usage:
    
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        
        try:
        
            if:
        
                return self.memory_manager.get_memory_usage()
            
            # í´ë°±: psutil ì‚¬ìš©
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / (1024 * 1024)
            
        except:
            
            return 0.0
    
    def _create_fallback_result:
    
        """í´ë°± ê²°ê³¼ ìƒì„± (ì—ëŸ¬ ë°œìƒ ì‹œ)"""
        return {
            "success": False,
            "session_id": session_id,
            "step_name": self.step_name,
            "error_message": error_msg,
            "processing_time": processing_time,
            "fitted_image": None,
            "fitted_image_raw": None,
            "confidence": 0.0,
            "quality_score": 0.0,
            "fit_score": 0.0,
            "overall_score": 0.0,
            "visualization": None,
            "metadata": {
                "device": self.device,
                "error": error_msg,
                "session_id": session_id
            },
            "performance_info": {
                "device": self.device,
                "error": error_msg
            },
            "recommendations": ["ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”."]
        }
    
    # =================================================================
    # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë° ì •ë¦¬ ë©”ì„œë“œ
    # =================================================================
    
    def get_step_info:
    
        """Step ì •ë³´ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'step_number': self.step_number,
            'device': self.device,
            'is_initialized': self.is_initialized,
            'is_m3_max': self.is_m3_max,
            'loaded_models': list(self.loaded_models.keys()),
            'fitting_method': str(self.fitting_method.value),
            'physics_enabled': self.fitting_config.physics_enabled,
            'cache_stats': self.cache_stats,
            'processing_stats': self.processing_stats,
            'session_id': self.session_id,
            'visualization_enabled': self.enable_visualization,
            'performance_config': self.performance_config,
            'ai_models_status': {
                name: model is not None 
                for name, model in self.ai_models.items()
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ VirtualFittingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            
            # ëª¨ë¸ ì •ë¦¬
            self.loaded_models.clear()
            self.ai_models = {k: None for k in self.ai_models.keys()}
            
            # ìºì‹œ ì •ë¦¬
            with self.cache_lock:
                self.result_cache.clear()
                self.cache_access_times.clear()
            
            # Executor ì¢…ë£Œ
            if:
                self.executor.shutdown(wait=False)
            
            if:
            
                self.thread_pool.shutdown(wait=True)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if:
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            
            # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì •ë¦¬
            if:
                await self.memory_manager.cleanup()
            
            gc.collect()
            
            self.logger.info("âœ… VirtualFittingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except:
            
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def __del__:
    
        """ì†Œë©¸ì"""
        try:
            if:
                self.executor.shutdown(wait=False)
            if:
                self.thread_pool.shutdown(wait=False)
        except:
            pass

# =================================================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ ë° ìœ í‹¸ë¦¬í‹°
# =================================================================

def create_virtual_fitting_step(
    device: str = "auto", 
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> VirtualFittingStep:
    """ê°€ìƒ í”¼íŒ… ë‹¨ê³„ ìƒì„± (ê¸°ì¡´ ë°©ì‹ í˜¸í™˜)"""
    return VirtualFittingStep(device=device, config=config, **kwargs)

def create_m3_max_virtual_fitting_step(
    memory_gb: float = 128.0,
    quality_level: str = "ultra",
    enable_visualization: bool = True,
    **kwargs
) -> VirtualFittingStep:
    """M3 Max ìµœì í™” ê°€ìƒ í”¼íŒ… ë‹¨ê³„ ìƒì„±"""
    return VirtualFittingStep(
        device=None,  # ìë™ ê°ì§€
        memory_gb=memory_gb,
        quality_level=quality_level,
        is_m3_max=True,
        optimization_enabled=True,
        enable_visualization=enable_visualization,
        **kwargs
    )

async def quick_virtual_fitting_with_visualization(
    person_image: Union[np.ndarray, Image.Image, str],
    clothing_image: Union[np.ndarray, Image.Image, str],
    fabric_type: str = "cotton",
    clothing_type: str = "shirt",
    enable_visualization: bool = True,
    **kwargs
) -> Dict[str, Any]:
    """ë¹ ë¥¸ ê°€ìƒ í”¼íŒ… + ì‹œê°í™” (ì¼íšŒì„± ì‚¬ìš©)"""
    
    step = VirtualFittingStep(enable_visualization=enable_visualization)
    try:
        await step.initialize()
        result = await step.process(
            person_image, clothing_image,
            fabric_type=fabric_type,
            clothing_type=clothing_type,
            **kwargs
        )
        return result
    finally:
        await step.cleanup()

async def batch_virtual_fitting(
    image_pairs: List[Tuple[Union[np.ndarray, Image.Image, str], Union[np.ndarray, Image.Image, str]]],
    fabric_types: Optional[List[str]] = None,
    clothing_types: Optional[List[str]] = None,
    **kwargs
) -> List[Dict[str, Any]]:
    """ë°°ì¹˜ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬"""
    
    step = VirtualFittingStep(**kwargs)
    try:
        await step.initialize()
        results = []
        
        for i, (person_img, cloth_img) in enumerate(image_pairs):
            fabric_type = fabric_types[i] if fabric_types and i < len(fabric_types) else "cotton"
            clothing_type = clothing_types[i] if clothing_types and i < len(clothing_types) else "shirt"
            
            result = await step.process(
                person_img, cloth_img,
                fabric_type=fabric_type,
                clothing_type=clothing_type
            )
            results.append(result)
        
        return results
    finally:
        await step.cleanup()

def get_supported_fabric_types:

    """ì§€ì›ë˜ëŠ” ì²œ ì¬ì§ˆ íƒ€ì… ëª©ë¡ ë°˜í™˜"""
    return list(FABRIC_PROPERTIES.keys())

def get_supported_clothing_types:

    """ì§€ì›ë˜ëŠ” ì˜ë¥˜ íƒ€ì… ëª©ë¡ ë°˜í™˜"""
    return list(CLOTHING_FITTING_PARAMS.keys())

def get_fitting_methods:

    """ì§€ì›ë˜ëŠ” í”¼íŒ… ë°©ë²• ëª©ë¡ ë°˜í™˜"""
    return [method.value for method in FittingMethod]

def get_quality_levels:

    """ì§€ì›ë˜ëŠ” í’ˆì§ˆ ë ˆë²¨ ëª©ë¡ ë°˜í™˜"""
    return [quality.value for quality in FittingQuality]

def analyze_fabric_compatibility:

    """ì²œ ì¬ì§ˆê³¼ ì˜ë¥˜ íƒ€ì… í˜¸í™˜ì„± ë¶„ì„"""
    fabric_props = FABRIC_PROPERTIES.get(fabric_type, FABRIC_PROPERTIES['default'])
    fitting_params = CLOTHING_FITTING_PARAMS.get(clothing_type, CLOTHING_FITTING_PARAMS['default'])
    
    # í˜¸í™˜ì„± ì ìˆ˜ ê³„ì‚°
    compatibility_matrix = {
        ('cotton', 'shirt'): 0.9,
        ('cotton', 'dress'): 0.8,
        ('silk', 'dress'): 0.95,
        ('silk', 'blouse'): 0.9,
        ('denim', 'pants'): 0.95,
        ('leather', 'jacket'): 0.9,
        ('wool', 'sweater'): 0.9,
        ('spandex', 'shirt'): 0.8,
        ('linen', 'shirt'): 0.85
    }
    
    compatibility_score = compatibility_matrix.get((fabric_type, clothing_type), 0.7)
    
    return {
        'fabric_type': fabric_type,
        'clothing_type': clothing_type,
        'compatibility_score': compatibility_score,
        'fabric_properties': fabric_props,
        'fitting_parameters': fitting_params,
        'recommendations': _generate_compatibility_recommendations(fabric_type, clothing_type, compatibility_score)
    }

def _generate_compatibility_recommendations:

    """í˜¸í™˜ì„± ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
    recommendations = []
    
    if:
    
        recommendations.append(f"Excellent match! {fabric_type.title()} works perfectly for {clothing_type}")
    elif score >= 0.8:
        recommendations.append(f"Good combination of {fabric_type} and {clothing_type}")
    elif score >= 0.7:
        recommendations.append(f"Decent pairing, but consider alternatives")
    else:
        recommendations.append(f"Consider different fabric for better results")
    
    # ì²œ ì¬ì§ˆë³„ ì¶”ê°€ ê¶Œì¥ì‚¬í•­
    if:
        recommendations.append("Silk requires gentle handling and drapes beautifully")
    elif fabric_type == 'denim':
        recommendations.append("Denim provides structure and durability")
    elif fabric_type == 'cotton':
        recommendations.append("Cotton is versatile and comfortable")
    
    return recommendations

# =================================================================
# ğŸ”¥ ê³ ê¸‰ ì‹œê°í™” ìœ í‹¸ë¦¬í‹°
# =================================================================

class VirtualFittingVisualizer:

    """ê°€ìƒ í”¼íŒ… ì „ìš© ì‹œê°í™” ë„êµ¬"""
    
    def __init__:
    
        self.config = config or {}
        self.colors = VISUALIZATION_COLORS
    
    def create_before_after_comparison(
        self, 
        before_image: np.ndarray, 
        after_image: np.ndarray,
        title: str = "Virtual Fitting Comparison"
    ) -> Image.Image:
        """ì „í›„ ë¹„êµ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # PIL ë³€í™˜
            before_pil = Image.fromarray(before_image)
            after_pil = Image.fromarray(after_image)
            
            # í¬ê¸° í†µì¼
            width, height = before_pil.size
            after_resized = after_pil.resize((width, height), Image.Resampling.LANCZOS)
            
            # ìº”ë²„ìŠ¤ ìƒì„±
            canvas_width = width * 2 + 60
            canvas_height = height + 80
            canvas = Image.new('RGB', (canvas_width, canvas_height), (245, 245, 245))
            
            # ì´ë¯¸ì§€ ë°°ì¹˜
            canvas.paste(before_pil, (20, 40))
            canvas.paste(after_resized, (width + 40, 40))
            
            # í…ìŠ¤íŠ¸ ì¶”ê°€
            draw = ImageDraw.Draw(canvas)
            try:
                title_font = ImageFont.truetype("arial.ttf", 20)
                label_font = ImageFont.truetype("arial.ttf", 16)
            except:
                title_font = ImageFont.load_default()
                label_font = ImageFont.load_default()
            
            # ì œëª©
            draw.text((canvas_width//2 - 100, 10), title, fill=(0, 0, 0), font=title_font)
            
            # ë¼ë²¨
            draw.text((20 + width//2 - 25, height + 50), "Before", fill=(0, 0, 0), font=label_font)
            draw.text((width + 40 + width//2 - 20, height + 50), "After", fill=(0, 0, 0), font=label_font)
            
            return canvas
            
        except:
            
            logging.error(f"âŒ ì „í›„ ë¹„êµ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (800, 600), (240, 240, 240))
    
    def create_fabric_analysis_chart(
        self,
        fabric_properties: FabricProperties,
        fabric_type: str
    ) -> Image.Image:
        """ì²œ ì¬ì§ˆ ë¶„ì„ ì°¨íŠ¸ ìƒì„±"""
        try:
            # ìº”ë²„ìŠ¤ ìƒì„±
            canvas_width = 400
            canvas_height = 300
            canvas = Image.new('RGB', (canvas_width, canvas_height), (250, 250, 250))
            draw = ImageDraw.Draw(canvas)
            
            try:
            
                title_font = ImageFont.truetype("arial.ttf", 18)
                text_font = ImageFont.truetype("arial.ttf", 14)
            except:
                title_font = ImageFont.load_default()
                text_font = ImageFont.load_default()
            
            # ì œëª©
            draw.text((20, 20), f"Fabric Analysis: {fabric_type.title()}", fill=(0, 0, 0), font=title_font)
            
            # ì†ì„± ë°” ì°¨íŠ¸
            y_start = 60
            bar_width = 200
            bar_height = 20
            
            properties = [
                ("Stiffness", fabric_properties.stiffness),
                ("Elasticity", fabric_properties.elasticity),
                ("Density", fabric_properties.density / 3.0),  # ì •ê·œí™”
                ("Friction", fabric_properties.friction),
                ("Shine", fabric_properties.shine),
                ("Texture Scale", fabric_properties.texture_scale)
            ]
            
            for i, (prop_name, value) in enumerate(properties):
                y_pos = y_start + i * 35
                
                # ë¼ë²¨
                draw.text((20, y_pos), prop_name, fill=(0, 0, 0), font=text_font)
                
                # ë°°ê²½ ë°”
                draw.rectangle([150, y_pos, 150 + bar_width, y_pos + bar_height], 
                            fill=(200, 200, 200), outline=(150, 150, 150))
                
                # ê°’ ë°”
                value_width = int(bar_width * min(value, 1.0))
                color = self._get_property_color(prop_name, value)
                draw.rectangle([150, y_pos, 150 + value_width, y_pos + bar_height], 
                            fill=color, outline=color)
                
                # ê°’ í…ìŠ¤íŠ¸
                draw.text((360, y_pos + 3), f"{value:.2f}", fill=(0, 0, 0), font=text_font)
            
            return canvas
            
        except:
            
            logging.error(f"âŒ ì²œ ì¬ì§ˆ ë¶„ì„ ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (400, 300), (240, 240, 240))
    
    def _get_property_color:
    
        """ì†ì„±ë³„ ìƒ‰ìƒ ë°˜í™˜"""
        color_map = {
            'Stiffness': (255, 100, 100),    # ë¹¨ê°•
            'Elasticity': (100, 255, 100),   # ì´ˆë¡
            'Density': (100, 100, 255),      # íŒŒë‘
            'Friction': (255, 255, 100),     # ë…¸ë‘
            'Shine': (255, 150, 255),        # ë§ˆì  íƒ€
            'Texture Scale': (150, 255, 255) # ì‹œì•ˆ
        }
        return color_map.get(prop_name, (150, 150, 150))

# =================================================================
# ğŸ”¥ ì„±ëŠ¥ ë¶„ì„ ë„êµ¬
# =================================================================

class VirtualFittingProfiler:

    """ê°€ìƒ í”¼íŒ… ì„±ëŠ¥ ë¶„ì„ ë„êµ¬"""
    
    def __init__:
    
        self.metrics = {}
        self.start_times = {}
    
    def start_timing:
    
        """íƒ€ì´ë° ì‹œì‘"""
        self.start_times[operation] = time.time()
    
    def end_timing:
    
        """íƒ€ì´ë° ì¢…ë£Œ"""
        if:
            duration = time.time() - self.start_times[operation]
            if:
                self.metrics[operation] = []
            self.metrics[operation].append(duration)
            del self.start_times[operation]
            return duration
        return 0.0
    
    def get_average_time:
    
        """í‰ê·  ì‹œê°„ ë°˜í™˜"""
        if:
            return np.mean(self.metrics[operation])
        return 0.0
    
    def get_performance_report:
    
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {}
        for operation, times in self.metrics.items():
            report[operation] = {
                'average_time': np.mean(times),
                'min_time': np.min(times),
                'max_time': np.max(times),
                'total_calls': len(times),
                'total_time': np.sum(times)
            }
        return report

# =================================================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# =================================================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'VirtualFittingStep',
    
    # ë°ì´í„° í´ë˜ìŠ¤
    'VirtualFittingConfig',
    'FittingResult',
    'FabricProperties',
    'FittingParams',
    
    # ì—´ê±°í˜•
    'FittingMethod',
    'FittingQuality',
    
    # ìƒìˆ˜
    'FABRIC_PROPERTIES',
    'CLOTHING_FITTING_PARAMS',
    'VISUALIZATION_COLORS',
    
    # í¸ì˜ í•¨ìˆ˜ë“¤
    'create_virtual_fitting_step',
    'create_m3_max_virtual_fitting_step',
    'quick_virtual_fitting_with_visualization',
    'batch_virtual_fitting',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'get_supported_fabric_types',
    'get_supported_clothing_types',
    'get_fitting_methods',
    'get_quality_levels',
    'analyze_fabric_compatibility',
    
    # ê³ ê¸‰ ë„êµ¬ë“¤
    'VirtualFittingVisualizer',
    'VirtualFittingProfiler'
]

# =================================================================
# ğŸ”¥ ëª¨ë“ˆ ì •ë³´
# =================================================================

__version__ = "6.0.0-complete-full"
__author__ = "MyCloset AI Team"
__description__ = "Complete Virtual Fitting Implementation with AI Models, Physics Simulation, Advanced Visualization, and Full M3 Max Optimization"

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.info("âœ… VirtualFittingStep ëª¨ë“ˆ ì™„ì „ ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ”— BaseStepMixin ìƒì† ë° logger ì†ì„± ë³´ì¥")
logger.info("ğŸ”— ModelLoader ì™„ì „ ì—°ë™")
logger.info("ğŸ M3 Max 128GB ìµœì í™” ì§€ì›")
logger.info("ğŸ¨ ê³ ê¸‰ ì‹œê°í™” ê¸°ëŠ¥ ì™„ì „ êµ¬í˜„")
logger.info("âš™ï¸ ë¬¼ë¦¬ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ ì™„ì „ ì§€ì›")
logger.info("ğŸ“Š ì„±ëŠ¥ ë¶„ì„ ë„êµ¬ í¬í•¨")

# =================================================================
# ğŸ”¥ í…ŒìŠ¤íŠ¸ ì½”ë“œ (ê°œë°œìš©)
# =================================================================

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    async def test_virtual_fitting_complete():
        """ì™„ì „í•œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
        print("ğŸ”„ VirtualFittingStep ì™„ì „ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± (numpy ë°°ì—´)
        test_person = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
        test_clothing = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
        
        # ê°€ìƒ í”¼íŒ… ì‹¤í–‰
        step = VirtualFittingStep(
            quality_level="balanced",
            enable_visualization=True,
            fitting_method=FittingMethod.HYBRID,
            enable_physics=True
        )
        
        print("ğŸ“¦ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        await step.initialize()
        
        print("ğŸ­ ê°€ìƒ í”¼íŒ… ì‹¤í–‰ ì¤‘...")
        result = await step.process(
            test_person, test_clothing,
            fabric_type="cotton",
            clothing_type="shirt"
        )
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"   ì„±ê³µ: {result['success']}")
        print(f"   ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
        print(f"   ì‹ ë¢°ë„: {result['confidence']:.2f}")
        print(f"   í’ˆì§ˆ ì ìˆ˜: {result['quality_score']:.2f}")
        print(f"   ì „ì²´ ì ìˆ˜: {result['overall_score']:.2f}")
        print(f"   ì‹œê°í™” ë°ì´í„°: {result['visualization'] is not None}")
        
        # Step ì •ë³´ ì¶œë ¥
        step_info = step.get_step_info()
        print(f"   ë¡œë“œëœ ëª¨ë¸: {step_info['loaded_models']}")
        print(f"   ìºì‹œ í†µê³„: {step_info['cache_stats']}")
        print(f"   AI ëª¨ë¸ ìƒíƒœ: {step_info['ai_models_status']}")
        
        # ì¶”ê°€ í…ŒìŠ¤íŠ¸ë“¤
        print("\nğŸ“Š ì¶”ê°€ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸:")
        
        # 1. ì²œ ì¬ì§ˆ í˜¸í™˜ì„± ë¶„ì„
        compatibility = analyze_fabric_compatibility("silk", "dress")
        print(f"   ì²œ ì¬ì§ˆ í˜¸í™˜ì„± (silk + dress): {compatibility['compatibility_score']:.2f}")
        
        # 2. ì§€ì› í˜•ì‹ í™•ì¸
        print(f"   ì§€ì› ì²œ ì¬ì§ˆ: {len(get_supported_fabric_types())}ê°œ")
        print(f"   ì§€ì› ì˜ë¥˜ íƒ€ì…: {len(get_supported_clothing_types())}ê°œ")
        print(f"   í”¼íŒ… ë°©ë²•: {len(get_fitting_methods())}ê°œ")
        
        # 3. ì‹œê°í™” ë„êµ¬ í…ŒìŠ¤íŠ¸
        visualizer = VirtualFittingVisualizer()
        fabric_props = FABRIC_PROPERTIES['silk']
        chart = visualizer.create_fabric_analysis_chart(fabric_props, 'silk')
        print(f"   ì‹œê°í™” ì°¨íŠ¸ ìƒì„±: {chart.size}")
        
        # 4. ì„±ëŠ¥ ë¶„ì„ ë„êµ¬ í…ŒìŠ¤íŠ¸
        profiler = VirtualFittingProfiler()
        profiler.start_timing("test_operation")
        await asyncio.sleep(0.1)  # 0.1ì´ˆ ëŒ€ê¸°
        duration = profiler.end_timing("test_operation")
        print(f"   ì„±ëŠ¥ ì¸¡ì •: {duration:.3f}ì´ˆ")
        
        await step.cleanup()
        print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import asyncio
    asyncio.run(test_virtual_fitting_complete())