#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 06: Virtual Fitting v8.0 - Central Hub DI Container ì™„ì „ ì—°ë™
===============================================================================

âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
âœ… BaseStepMixin ìƒì† ë° í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™”
âœ… ê°„ì†Œí™”ëœ ì•„í‚¤í…ì²˜ (ë³µì¡í•œ DI ë¡œì§ ì œê±°)
âœ… ì‹¤ì œ OOTD 3.2GB + VITON-HD 2.1GB + Diffusion 4.8GB ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©
âœ… Mock ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ
âœ… _run_ai_inference() ë©”ì„œë“œ êµ¬í˜„ (BaseStepMixin v20.0 í‘œì¤€)
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… GitHubDependencyManager ì™„ì „ ì œê±°
"""
import cv2 
import os
import sys
import time
import logging
import asyncio
import threading
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Tuple
from dataclasses import dataclass, field
import cv2
import json

# PyTorch í•„ìˆ˜
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# PIL í•„ìˆ˜
try:
    from PIL import Image, ImageDraw, ImageFont, ImageEnhance
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# Diffusers (ê³ ê¸‰ ì´ë¯¸ì§€ ìƒì„±ìš©)
try:
    from diffusers import StableDiffusionPipeline, DDIMScheduler, UNet2DConditionModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False


import importlib  # ì¶”ê°€
import logging    # ì¶”ê°€

# ==============================================
# ğŸ”¥ Central Hub DI Container ì•ˆì „ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€) - VirtualFitting íŠ¹í™”
# ==============================================

def _get_central_hub_container():
    """Central Hub DI Container ì•ˆì „í•œ ë™ì  í•´ê²° - VirtualFittingìš©"""
    try:
        import importlib
        module = importlib.import_module('app.core.di_container')
        get_global_fn = getattr(module, 'get_global_container', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError:
        return None
    except Exception:
        return None

def _inject_dependencies_safe(step_instance):
    """Central Hub DI Containerë¥¼ í†µí•œ ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì… - VirtualFittingìš©"""
    try:
        container = _get_central_hub_container()
        if container and hasattr(container, 'inject_to_step'):
            return container.inject_to_step(step_instance)
        return 0
    except Exception:
        return 0

def _get_service_from_central_hub(service_key: str):
    """Central Hubë¥¼ í†µí•œ ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ - VirtualFittingìš©"""
    try:
        container = _get_central_hub_container()
        if container:
            return container.get(service_key)
        return None
    except Exception:
        return None

# BaseStepMixin ë™ì  import (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€) - VirtualFittingìš©
def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€) - VirtualFittingìš©"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError:
        try:
            # í´ë°±: ìƒëŒ€ ê²½ë¡œ
            from .base_step_mixin import BaseStepMixin
            return BaseStepMixin
        except ImportError:
            logging.getLogger(__name__).error("âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨")
            return None

BaseStepMixin = get_base_step_mixin_class()

# BaseStepMixin í´ë°± í´ë˜ìŠ¤ (VirtualFitting íŠ¹í™”)
if BaseStepMixin is None:
    class BaseStepMixin:
        """VirtualFittingStepìš© BaseStepMixin í´ë°± í´ë˜ìŠ¤"""
        
        def __init__(self, **kwargs):
            # ê¸°ë³¸ ì†ì„±ë“¤
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'VirtualFittingStep')
            self.step_id = kwargs.get('step_id', 6)
            self.device = kwargs.get('device', 'cpu')
            
            # AI ëª¨ë¸ ê´€ë ¨ ì†ì„±ë“¤ (VirtualFittingì´ í•„ìš”ë¡œ í•˜ëŠ”)
            self.ai_models = {}
            self.models_loading_status = {
                'ootd': False,
                'viton_hd': False,
                'diffusion': False,
                'tps_warping': False,
                'cloth_analyzer': False,
                'quality_assessor': False,
                'mock_model': False
            }
            self.model_interface = None
            self.loaded_models = []
            
            # VirtualFitting íŠ¹í™” ì†ì„±ë“¤
            self.fitting_models = {}
            self.fitting_ready = False
            self.fitting_cache = {}
            self.pose_processor = None
            self.lighting_adapter = None
            self.texture_enhancer = None
            self.diffusion_pipeline = None
            
            # ìƒíƒœ ê´€ë ¨ ì†ì„±ë“¤
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # Central Hub DI Container ê´€ë ¨
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            # ì„±ëŠ¥ í†µê³„
            self.performance_stats = {
                'total_processed': 0,
                'successful_fittings': 0,
                'avg_processing_time': 0.0,
                'avg_fitting_quality': 0.0,
                'ootd_calls': 0,
                'viton_hd_calls': 0,
                'diffusion_calls': 0,
                'tps_warping_applied': 0,
                'quality_assessments': 0,
                'cloth_analysis_performed': 0,
                'error_count': 0,
                'models_loaded': 0
            }
            
            # í†µê³„ ì‹œìŠ¤í…œ
            self.statistics = {
                'total_processed': 0,
                'successful_fittings': 0,
                'average_quality': 0.0,
                'total_processing_time': 0.0,
                'ai_model_calls': 0,
                'error_count': 0,
                'model_creation_success': False,
                'real_ai_models_used': True,
                'algorithm_type': 'advanced_virtual_fitting_with_tps_analysis',
                'features': [
                    'OOTD (Outfit Of The Day) ëª¨ë¸ - 3.2GB',
                    'VITON-HD ëª¨ë¸ - 2.1GB (ê³ í’ˆì§ˆ Virtual Try-On)',
                    'Stable Diffusion ëª¨ë¸ - 4.8GB (ê³ ê¸‰ ì´ë¯¸ì§€ ìƒì„±)',
                    'TPS (Thin Plate Spline) ì›Œí•‘ ì•Œê³ ë¦¬ì¦˜',
                    'ê³ ê¸‰ ì˜ë¥˜ ë¶„ì„ ì‹œìŠ¤í…œ (ìƒ‰ìƒ/í…ìŠ¤ì²˜/íŒ¨í„´)',
                    'AI í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ (SSIM ê¸°ë°˜)',
                    'FFT ê¸°ë°˜ íŒ¨í„´ ê°ì§€',
                    'ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚° ì„ ëª…ë„ í‰ê°€',
                    'ë°”ì´ë¦¬ë‹ˆì–´ ë³´ê°„ ì›Œí•‘ ì—”ì§„',
                    'K-means ìƒ‰ìƒ í´ëŸ¬ìŠ¤í„°ë§',
                    'ë‹¤ì¤‘ ì˜ë¥˜ ì•„ì´í…œ ë™ì‹œ í”¼íŒ…',
                    'ì‹¤ì‹œê°„ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬'
                ]
            }
            
            self.logger.info(f"âœ… {self.step_name} BaseStepMixin í´ë°± í´ë˜ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
            """ê¸°ë³¸ process ë©”ì„œë“œ - _run_ai_inference í˜¸ì¶œ"""
            try:
                start_time = time.time()
                
                # _run_ai_inference ë©”ì„œë“œê°€ ìˆìœ¼ë©´ í˜¸ì¶œ
                if hasattr(self, '_run_ai_inference'):
                    result = self._run_ai_inference(data)
                    
                    # ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
                    if isinstance(result, dict):
                        result['processing_time'] = time.time() - start_time
                        result['step_name'] = self.step_name
                        result['step_id'] = self.step_id
                    
                    return result
                    else:
                    # ê¸°ë³¸ ì‘ë‹µ
                    return {
                        'success': False,
                        'error': '_run_ai_inference ë©”ì„œë“œê°€ êµ¬í˜„ë˜ì§€ ì•ŠìŒ',
                        'processing_time': time.time() - start_time,
                        'step_name': self.step_name,
                        'step_id': self.step_id
                    }
                    
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} process ì‹¤íŒ¨: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                    'step_name': self.step_name,
                    'step_id': self.step_id
                }
        
        def initialize(self) -> bool:
            """ì´ˆê¸°í™” ë©”ì„œë“œ"""
            try:
                if self.is_initialized:
                    return True
                
                self.logger.info(f"ğŸ”„ {self.step_name} ì´ˆê¸°í™” ì‹œì‘...")
                
                # Central Hubë¥¼ í†µí•œ ì˜ì¡´ì„± ì£¼ì… ì‹œë„
                injected_count = _inject_dependencies_safe(self)
                if injected_count > 0:
                    self.logger.info(f"âœ… Central Hub ì˜ì¡´ì„± ì£¼ì…: {injected_count}ê°œ")
                
                # VirtualFitting ëª¨ë¸ë“¤ ë¡œë”© (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” _load_virtual_fitting_models_via_central_hub í˜¸ì¶œ)
                if hasattr(self, '_load_virtual_fitting_models_via_central_hub'):
                    self._load_virtual_fitting_models_via_central_hub()
                
                self.is_initialized = True
                self.is_ready = True
                self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
                return True
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                return False
        
        def cleanup(self):
            """ì •ë¦¬ ë©”ì„œë“œ"""
            try:
                self.logger.info(f"ğŸ”„ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
                
                # AI ëª¨ë¸ë“¤ ì •ë¦¬
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'cleanup'):
                            model.cleanup()
                        del model
                    except Exception as e:
                        self.logger.debug(f"ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨ ({model_name}): {e}")
                
                # ìºì‹œ ì •ë¦¬
                self.ai_models.clear()
                if hasattr(self, 'fitting_models'):
                    self.fitting_models.clear()
                if hasattr(self, 'fitting_cache'):
                    self.fitting_cache.clear()
                
                # Diffusion íŒŒì´í”„ë¼ì¸ ì •ë¦¬
                if hasattr(self, 'diffusion_pipeline') and self.diffusion_pipeline:
                    del self.diffusion_pipeline
                    self.diffusion_pipeline = None
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        torch.mps.empty_cache()
                    except:
                    pass
                
                import gc
                gc.collect()
                
                self.logger.info(f"âœ… {self.step_name} ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        def get_status(self) -> Dict[str, Any]:
            """ìƒíƒœ ì¡°íšŒ"""
            return {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'device': self.device,
                'fitting_ready': getattr(self, 'fitting_ready', False),
                'models_loaded': len(getattr(self, 'loaded_models', [])),
                'fitting_models': list(getattr(self, 'fitting_models', {}).keys()),
                'auxiliary_processors': {
                    'pose_processor': getattr(self, 'pose_processor', None) is not None,
                    'lighting_adapter': getattr(self, 'lighting_adapter', None) is not None,
                    'texture_enhancer': getattr(self, 'texture_enhancer', None) is not None
                },
                'algorithm_type': 'advanced_virtual_fitting_with_tps_analysis',
                'fallback_mode': True
            }
        
        # BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤
        def set_model_loader(self, model_loader):
            """ModelLoader ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.model_loader = model_loader
                self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                
                # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹œë„
                if hasattr(model_loader, 'create_step_interface'):
                    try:
                        self.model_interface = model_loader.create_step_interface(self.step_name)
                        self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì£¼ì… ì™„ë£Œ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨, ModelLoader ì§ì ‘ ì‚¬ìš©: {e}")
                        self.model_interface = model_loader
                    else:
                    self.model_interface = model_loader
                    
            except Exception as e:
                self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
                self.model_loader = None
                self.model_interface = None
        
        def set_memory_manager(self, memory_manager):
            """MemoryManager ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.memory_manager = memory_manager
                self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        def set_data_converter(self, data_converter):
            """DataConverter ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.data_converter = data_converter
                self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        def set_di_container(self, di_container):
            """DI Container ì˜ì¡´ì„± ì£¼ì…"""
            try:
                self.di_container = di_container
                self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DI Container ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")

        def _get_step_requirements(self) -> Dict[str, Any]:
            """Step 06 Virtual Fitting ìš”êµ¬ì‚¬í•­ ë°˜í™˜ (BaseStepMixin í˜¸í™˜)"""
            return {
                "required_models": [
                    "ootd_diffusion.pth",
                    "viton_hd_final.pth",
                    "stable_diffusion_inpainting.pth"
                ],
                "primary_model": "ootd_diffusion.pth",
                "model_configs": {
                    "ootd_diffusion.pth": {
                        "size_mb": 3276.8,
                        "device_compatible": ["cpu", "mps", "cuda"],
                        "precision": "high",
                        "ai_algorithm": "Outfit Of The Day Diffusion"
                    },
                    "viton_hd_final.pth": {
                        "size_mb": 2147.5,
                        "device_compatible": ["cpu", "mps", "cuda"],
                        "real_time": False,
                        "ai_algorithm": "Virtual Try-On HD"
                    },
                    "stable_diffusion_inpainting.pth": {
                        "size_mb": 4835.2,
                        "device_compatible": ["cpu", "mps", "cuda"],
                        "quality": "ultra",
                        "ai_algorithm": "Stable Diffusion Inpainting"
                    }
                },
                "verified_paths": [
                    "step_06_virtual_fitting/ootd_diffusion.pth",
                    "step_06_virtual_fitting/viton_hd_final.pth",
                    "step_06_virtual_fitting/stable_diffusion_inpainting.pth"
                ],
                "advanced_algorithms": [
                    "TPSWarping",
                    "AdvancedClothAnalyzer", 
                    "AIQualityAssessment"
                ]
            }

# ==============================================
# ğŸ”¥ ë°ì´í„° í´ë˜ìŠ¤ë“¤
# ==============================================

@dataclass
class VirtualFittingConfig:
    """Virtual Fitting ì„¤ì •"""
    input_size: tuple = (768, 1024)  # OOTD ì…ë ¥ í¬ê¸°
    fitting_quality: str = "high"  # fast, balanced, high, ultra
    enable_multi_items: bool = True
    enable_pose_adaptation: bool = True
    enable_lighting_adaptation: bool = True
    enable_texture_preservation: bool = True
    device: str = "auto"

# Virtual Fitting ëª¨ë“œ ì •ì˜
FITTING_MODES = {
    0: 'single_item',      # ë‹¨ì¼ ì˜ë¥˜ ì•„ì´í…œ
    1: 'multi_item',       # ë‹¤ì¤‘ì˜ë¥˜ ì•„ì´í…œ
    2: 'full_outfit',      # ì „ì²´ ì˜ìƒ
    3: 'accessory_only',   # ì•¡ì„¸ì„œë¦¬ë§Œ
    4: 'upper_body',       # ìƒì²´ë§Œ
    5: 'lower_body',       # í•˜ì²´ë§Œ
    6: 'mixed_style',      # í˜¼í•© ìŠ¤íƒ€ì¼
    7: 'seasonal_adapt',   # ê³„ì ˆë³„ ì ì‘
    8: 'occasion_based',   # ìƒí™©ë³„ ë§ì¶¤
    9: 'ai_recommended'    # AI ì¶”ì²œ ê¸°ë°˜
}

# Virtual Fitting í’ˆì§ˆ ë ˆë²¨
FITTING_QUALITY_LEVELS = {
    'fast': {
        'models': ['ootd'],
        'resolution': (512, 512),
        'inference_steps': 20,
        'guidance_scale': 7.5
    },
    'balanced': {
        'models': ['ootd', 'viton_hd'],
        'resolution': (768, 1024),
        'inference_steps': 30,
        'guidance_scale': 10.0
    },
    'high': {
        'models': ['ootd', 'viton_hd', 'diffusion'],
        'resolution': (768, 1024),
        'inference_steps': 50,
        'guidance_scale': 12.5
    },
    'ultra': {
        'models': ['ootd', 'viton_hd', 'diffusion'],
        'resolution': (1024, 1536),
        'inference_steps': 100,
        'guidance_scale': 15.0
    }
}

# ì˜ë¥˜ ì•„ì´í…œ íƒ€ì…
CLOTHING_ITEM_TYPES = {
    'tops': ['t-shirt', 'shirt', 'blouse', 'sweater', 'hoodie', 'jacket', 'coat'],
    'bottoms': ['pants', 'jeans', 'shorts', 'skirt', 'leggings'],
    'dresses': ['dress', 'gown', 'sundress', 'cocktail_dress'],
    'outerwear': ['jacket', 'coat', 'blazer', 'cardigan', 'vest'],
    'accessories': ['hat', 'scarf', 'bag', 'glasses', 'jewelry'],
    'footwear': ['shoes', 'boots', 'sneakers', 'heels', 'sandals']
}

# ==============================================
# ğŸ”¥ VirtualFittingStep í´ë˜ìŠ¤
# ==============================================

    # ==============================================
    # ğŸ”¥ í•µì‹¬ ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ë“¤ (í”„ë¡œì íŠ¸ ì§€ì‹ ê¸°ë°˜)
    # ==============================================

class TPSWarping:
    """TPS (Thin Plate Spline) ê¸°ë°˜ ì˜ë¥˜ ì›Œí•‘ ì•Œê³ ë¦¬ì¦˜ - ê³ ê¸‰ êµ¬í˜„"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.TPSWarping")
        
    def create_control_points(self, person_mask: np.ndarray, cloth_mask: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ì œì–´ì  ìƒì„± (ì¸ì²´ì™€ ì˜ë¥˜ ê²½ê³„)"""
        try:
            # ì¸ì²´ ë§ˆìŠ¤í¬ì—ì„œ ì œì–´ì  ì¶”ì¶œ
            person_contours = self._extract_contour_points(person_mask)
            cloth_contours = self._extract_contour_points(cloth_mask)
            
            # ì œì–´ì  ë§¤ì¹­
            source_points, target_points = self._match_control_points(cloth_contours, person_contours)
            
            return source_points, target_points
            
        except Exception as e:
            self.logger.error(f"âŒ ì œì–´ì  ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì œì–´ì  ë°˜í™˜
            h, w = person_mask.shape
            source_points = np.array([[w//4, h//4], [3*w//4, h//4], [w//2, h//2], [w//4, 3*h//4], [3*w//4, 3*h//4]])
            target_points = source_points.copy()
            return source_points, target_points
    
    def _extract_contour_points(self, mask: np.ndarray, num_points: int = 20) -> np.ndarray:
        """ë§ˆìŠ¤í¬ì—ì„œ ìœ¤ê³½ì„  ì ë“¤ ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ ê°€ì¥ìë¦¬ ê²€ì¶œ
            edges = self._detect_edges(mask)
            
            # ìœ¤ê³½ì„  ì ë“¤ ì¶”ì¶œ
            y_coords, x_coords = np.where(edges)
            
            if len(x_coords) == 0:
                # í´ë°±: ë§ˆìŠ¤í¬ ì¤‘ì‹¬ ê¸°ë°˜ ì ë“¤
                h, w = mask.shape
                return np.array([[w//2, h//2]])
            
            # ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
            indices = np.linspace(0, len(x_coords)-1, min(num_points, len(x_coords)), dtype=int)
            contour_points = np.column_stack([x_coords[indices], y_coords[indices]])
            
            return contour_points
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìœ¤ê³½ì„  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            h, w = mask.shape
            return np.array([[w//2, h//2]])
    
    def _detect_edges(self, mask: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ ê°€ì¥ìë¦¬ ê²€ì¶œ"""
        try:
            # Sobel í•„í„° ê·¼ì‚¬
            kernel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
            kernel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])
            
            # ì»¨ë³¼ë£¨ì…˜ ì—°ì‚°
            edges_x = self._convolve2d(mask.astype(np.float32), kernel_x)
            edges_y = self._convolve2d(mask.astype(np.float32), kernel_y)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°
            edges = np.sqrt(edges_x**2 + edges_y**2)
            edges = (edges > 0.1).astype(np.uint8)
            
            return edges
            
        except Exception:
            # í´ë°±: ê¸°ë³¸ ê°€ì¥ìë¦¬
            h, w = mask.shape
            edges = np.zeros((h, w), dtype=np.uint8)
            edges[1:-1, 1:-1] = mask[1:-1, 1:-1]
            return edges
    
    def _convolve2d(self, image: np.ndarray, kernel: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ 2D ì»¨ë³¼ë£¨ì…˜"""
        try:
            h, w = image.shape
            kh, kw = kernel.shape
            
            # íŒ¨ë”©
            pad_h, pad_w = kh//2, kw//2
            padded = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode='edge')
            
            # ì»¨ë³¼ë£¨ì…˜
            result = np.zeros((h, w))
            for i in range(h):
                for j in range(w):
                    result[i, j] = np.sum(padded[i:i+kh, j:j+kw] * kernel)
            
            return result
            
        except Exception:
            return np.zeros_like(image)
    
    def _match_control_points(self, source_points: np.ndarray, target_points: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ì œì–´ì  ë§¤ì¹­"""
        try:
            min_len = min(len(source_points), len(target_points))
            return source_points[:min_len], target_points[:min_len]
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì œì–´ì  ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return source_points[:5], target_points[:5]
    
    def apply_tps_transform(self, cloth_image: np.ndarray, source_points: np.ndarray, target_points: np.ndarray) -> np.ndarray:
        """TPS ë³€í™˜ ì ìš©"""
        try:
            if len(source_points) < 3 or len(target_points) < 3:
                return cloth_image
            
            h, w = cloth_image.shape[:2]
            
            # TPS ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            tps_matrix = self._calculate_tps_matrix(source_points, target_points)
            
            # ê·¸ë¦¬ë“œ ìƒì„±
            x_grid, y_grid = np.meshgrid(np.arange(w), np.arange(h))
            grid_points = np.column_stack([x_grid.ravel(), y_grid.ravel()])
            
            # TPS ë³€í™˜ ì ìš©
            transformed_points = self._apply_tps_to_points(grid_points, source_points, target_points, tps_matrix)
            
            # ì´ë¯¸ì§€ ì›Œí•‘
            warped_image = self._warp_image(cloth_image, grid_points, transformed_points)
            
            return warped_image
            
        except Exception as e:
            self.logger.error(f"âŒ TPS ë³€í™˜ ì‹¤íŒ¨: {e}")
            return cloth_image
    
    def _calculate_tps_matrix(self, source_points: np.ndarray, target_points: np.ndarray) -> np.ndarray:
        """TPS ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        try:
            n = len(source_points)
            
            # TPS ì»¤ë„ í–‰ë ¬ ìƒì„±
            K = np.zeros((n, n))
            for i in range(n):
                for j in range(n):
                    if i != j:
                        r = np.linalg.norm(source_points[i] - source_points[j])
                        if r > 0:
                            K[i, j] = r**2 * np.log(r)
            
            # P í–‰ë ¬ (ì–´í•€ ë³€í™˜)
            P = np.column_stack([np.ones(n), source_points])
            
            # L í–‰ë ¬ êµ¬ì„±
            L = np.block([[K, P], [P.T, np.zeros((3, 3))]])
            
            # Y ë²¡í„°
            Y = np.column_stack([target_points, np.zeros((3, 2))])
            
            # ë§¤íŠ¸ë¦­ìŠ¤ í•´ê²° (regularization ì¶”ê°€)
            L_reg = L + 1e-6 * np.eye(L.shape[0])
            tps_matrix = np.linalg.solve(L_reg, Y)
            
            return tps_matrix
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ TPS ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.eye(len(source_points) + 3, 2)
    
    def _apply_tps_to_points(self, points: np.ndarray, source_points: np.ndarray, target_points: np.ndarray, tps_matrix: np.ndarray) -> np.ndarray:
        """ì ë“¤ì— TPS ë³€í™˜ ì ìš©"""
        try:
            n_source = len(source_points)
            n_points = len(points)
            
            # ì»¤ë„ ê°’ ê³„ì‚°
            K_new = np.zeros((n_points, n_source))
            for i in range(n_points):
                for j in range(n_source):
                    r = np.linalg.norm(points[i] - source_points[j])
                    if r > 0:
                        K_new[i, j] = r**2 * np.log(r)
            
            # P_new í–‰ë ¬
            P_new = np.column_stack([np.ones(n_points), points])
            
            # ë³€í™˜ëœ ì ë“¤ ê³„ì‚°
            L_new = np.column_stack([K_new, P_new])
            transformed_points = L_new @ tps_matrix
            
            return transformed_points
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ TPS ì  ë³€í™˜ ì‹¤íŒ¨: {e}")
            return points
    
    def _warp_image(self, image: np.ndarray, source_grid: np.ndarray, target_grid: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ ì›Œí•‘"""
        try:
            h, w = image.shape[:2]
            
            # íƒ€ê²Ÿ ê·¸ë¦¬ë“œë¥¼ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ë³€í™˜
            target_x = target_grid[:, 0].reshape(h, w)
            target_y = target_grid[:, 1].reshape(h, w)
            
            # ê²½ê³„ í´ë¦¬í•‘
            target_x = np.clip(target_x, 0, w-1)
            target_y = np.clip(target_y, 0, h-1)
            
            # ë°”ì´ë¦¬ë‹ˆì–´ ë³´ê°„
            warped_image = self._bilinear_interpolation(image, target_x, target_y)
            
            return warped_image
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì›Œí•‘ ì‹¤íŒ¨: {e}")
            return image
    
    def _bilinear_interpolation(self, image: np.ndarray, x: np.ndarray, y: np.ndarray) -> np.ndarray:
        """ë°”ì´ë¦¬ë‹ˆì–´ ë³´ê°„"""
        try:
            h, w = image.shape[:2]
            
            # ì •ìˆ˜ ì¢Œí‘œ
            x0 = np.floor(x).astype(int)
            x1 = x0 + 1
            y0 = np.floor(y).astype(int)
            y1 = y0 + 1
            
            # ê²½ê³„ ì²˜ë¦¬
            x0 = np.clip(x0, 0, w-1)
            x1 = np.clip(x1, 0, w-1)
            y0 = np.clip(y0, 0, h-1)
            y1 = np.clip(y1, 0, h-1)
            
            # ê°€ì¤‘ì¹˜
            wa = (x1 - x) * (y1 - y)
            wb = (x - x0) * (y1 - y)
            wc = (x1 - x) * (y - y0)
            wd = (x - x0) * (y - y0)
            
            # ë³´ê°„
            if len(image.shape) == 3:
                warped = np.zeros_like(image)
                for c in range(image.shape[2]):
                    warped[:, :, c] = (wa * image[y0, x0, c] + 
                                     wb * image[y0, x1, c] + 
                                     wc * image[y1, x0, c] + 
                                     wd * image[y1, x1, c])
                else:
                warped = (wa * image[y0, x0] + 
                         wb * image[y0, x1] + 
                         wc * image[y1, x0] + 
                         wd * image[y1, x1])
            
            return warped.astype(image.dtype)
            
        except Exception as e:
            self.logger.error(f"âŒ ë°”ì´ë¦¬ë‹ˆì–´ ë³´ê°„ ì‹¤íŒ¨: {e}")
            return image

class AdvancedClothAnalyzer:
    """ê³ ê¸‰ ì˜ë¥˜ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.AdvancedClothAnalyzer")
    
    def analyze_cloth_properties(self, clothing_image: np.ndarray) -> Dict[str, Any]:
        """ì˜ë¥˜ ì†ì„± ê³ ê¸‰ ë¶„ì„"""
        try:
            # ìƒ‰ìƒ ë¶„ì„
            dominant_colors = self._extract_dominant_colors(clothing_image)
            
            # í…ìŠ¤ì²˜ ë¶„ì„
            texture_features = self._analyze_texture(clothing_image)
            
            # íŒ¨í„´ ë¶„ì„
            pattern_type = self._detect_pattern(clothing_image)
            
            return {
                'dominant_colors': dominant_colors,
                'texture_features': texture_features,
                'pattern_type': pattern_type,
                'cloth_complexity': self._calculate_complexity(clothing_image)
            }
            
        except Exception as e:
            self.logger.warning(f"ì˜ë¥˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'cloth_complexity': 0.5}
    
    def _extract_dominant_colors(self, image: np.ndarray, k: int = 5) -> List[List[int]]:
        """ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ (K-means ê¸°ë°˜)"""
        try:
            # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (ì„±ëŠ¥ ìµœì í™”)
            small_img = Image.fromarray(image).resize((150, 150))
            data = np.array(small_img).reshape((-1, 3))
            
            # ê°„ë‹¨í•œ ìƒ‰ìƒ í´ëŸ¬ìŠ¤í„°ë§ (K-means ê·¼ì‚¬)
            unique_colors = {}
            for pixel in data[::10]:  # ìƒ˜í”Œë§
                color_key = tuple(pixel // 32 * 32)  # ìƒ‰ìƒ ì–‘ìí™”
                unique_colors[color_key] = unique_colors.get(color_key, 0) + 1
            
            # ìƒìœ„ kê°œ ìƒ‰ìƒ ë°˜í™˜
            top_colors = sorted(unique_colors.items(), key=lambda x: x[1], reverse=True)[:k]
            return [list(color[0]) for color in top_colors]
            
        except Exception:
            return [[128, 128, 128]]  # ê¸°ë³¸ íšŒìƒ‰
    
    def _analyze_texture(self, image: np.ndarray) -> Dict[str, float]:
        """í…ìŠ¤ì²˜ ë¶„ì„"""
        try:
            gray = np.mean(image, axis=2) if len(image.shape) == 3 else image
            
            # í…ìŠ¤ì²˜ íŠ¹ì§•ë“¤
            features = {}
            
            # í‘œì¤€í¸ì°¨ (ê±°ì¹ ê¸°)
            features['roughness'] = float(np.std(gray) / 255.0)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° (ì—£ì§€ ë°€ë„)
            grad_x = np.abs(np.diff(gray, axis=1))
            grad_y = np.abs(np.diff(gray, axis=0))
            features['edge_density'] = float(np.mean(grad_x) + np.mean(grad_y)) / 255.0
            
            # ì§€ì—­ ë¶„ì‚° (í…ìŠ¤ì²˜ ê· ì¼ì„±)
            local_variance = []
            h, w = gray.shape
            for i in range(0, h-8, 8):
                for j in range(0, w-8, 8):
                    patch = gray[i:i+8, j:j+8]
                    local_variance.append(np.var(patch))
            
            features['uniformity'] = 1.0 - min(np.std(local_variance) / np.mean(local_variance), 1.0) if local_variance else 0.5
            
            return features
            
        except Exception:
            return {'roughness': 0.5, 'edge_density': 0.5, 'uniformity': 0.5}
    
    def _detect_pattern(self, image: np.ndarray) -> str:
        """íŒ¨í„´ ê°ì§€"""
        try:
            gray = np.mean(image, axis=2) if len(image.shape) == 3 else image
            
            # FFT ê¸°ë°˜ ì£¼ê¸°ì„± ë¶„ì„
            f_transform = np.fft.fft2(gray)
            f_shift = np.fft.fftshift(f_transform)
            magnitude_spectrum = np.log(np.abs(f_shift) + 1)
            
            # ì£¼íŒŒìˆ˜ ë„ë©”ì¸ì—ì„œ íŒ¨í„´ ê°ì§€
            center = np.array(magnitude_spectrum.shape) // 2
            
            # ë°©ì‚¬í˜• í‰ê·  ê³„ì‚°
            y, x = np.ogrid[:magnitude_spectrum.shape[0], :magnitude_spectrum.shape[1]]
            distances = np.sqrt((x - center[1])**2 + (y - center[0])**2)
            
            # ì£¼ìš” ì£¼íŒŒìˆ˜ ì„±ë¶„ ë¶„ì„
            radial_profile = []
            for r in range(1, min(center)//2):
                mask = (distances >= r-0.5) & (distances < r+0.5)
                radial_profile.append(np.mean(magnitude_spectrum[mask]))
            
            if len(radial_profile) > 10:
                # ì£¼ê¸°ì  íŒ¨í„´ ê°ì§€
                peaks = []
                for i in range(1, len(radial_profile)-1):
                    if radial_profile[i] > radial_profile[i-1] and radial_profile[i] > radial_profile[i+1]:
                        if radial_profile[i] > np.mean(radial_profile) + np.std(radial_profile):
                            peaks.append(i)
                
                if len(peaks) >= 3:
                    return "striped"
                elif len(peaks) >= 1:
                    return "patterned"
            
            return "solid"
            
        except Exception:
            return "unknown"
    
    def _calculate_complexity(self, image: np.ndarray) -> float:
        """ì˜ë¥˜ ë³µì¡ë„ ê³„ì‚°"""
        try:
            gray = np.mean(image, axis=2) if len(image.shape) == 3 else image
            
            # ì—£ì§€ ë°€ë„
            edges = self._simple_edge_detection(gray)
            edge_density = np.sum(edges) / edges.size
            
            # ìƒ‰ìƒ ë‹¤ì–‘ì„±
            colors = image.reshape(-1, 3) if len(image.shape) == 3 else gray.flatten()
            color_variance = np.var(colors)
            
            # ë³µì¡ë„ ì¢…í•©
            complexity = (edge_density * 0.6 + min(color_variance / 10000, 1.0) * 0.4)
            
            return float(np.clip(complexity, 0.0, 1.0))
            
        except Exception:
            return 0.5
    
    def _simple_edge_detection(self, gray: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œ"""
        try:
            # Sobel í•„í„° ê·¼ì‚¬
            kernel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
            kernel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])
            
            h, w = gray.shape
            edges = np.zeros((h-2, w-2))
            
            for i in range(1, h-1):
                for j in range(1, w-1):
                    patch = gray[i-1:i+2, j-1:j+2]
                    gx = np.sum(patch * kernel_x)
                    gy = np.sum(patch * kernel_y)
                    edges[i-1, j-1] = np.sqrt(gx**2 + gy**2)
            
            return edges > np.mean(edges) + np.std(edges)
            
        except Exception:
            return np.zeros((gray.shape[0]-2, gray.shape[1]-2), dtype=bool)

class AIQualityAssessment:
    """AI í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.QualityAssessment")
        
    def evaluate_fitting_quality(self, fitted_image: np.ndarray, 
                               person_image: np.ndarray,
                               clothing_image: np.ndarray) -> Dict[str, float]:
        """í”¼íŒ… í’ˆì§ˆ í‰ê°€"""
        try:
            metrics = {}
            
            # 1. ì‹œê°ì  í’ˆì§ˆ í‰ê°€
            metrics['visual_quality'] = self._assess_visual_quality(fitted_image)
            
            # 2. í”¼íŒ… ì •í™•ë„ í‰ê°€
            metrics['fitting_accuracy'] = self._assess_fitting_accuracy(
                fitted_image, person_image, clothing_image
            )
            
            # 3. ìƒ‰ìƒ ì¼ì¹˜ë„ í‰ê°€
            metrics['color_consistency'] = self._assess_color_consistency(
                fitted_image, clothing_image
            )
            
            # 4. êµ¬ì¡°ì  ë¬´ê²°ì„± í‰ê°€
            metrics['structural_integrity'] = self._assess_structural_integrity(
                fitted_image, person_image
            )
            
            # 5. ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            weights = {
                'visual_quality': 0.25,
                'fitting_accuracy': 0.35,
                'color_consistency': 0.25,
                'structural_integrity': 0.15
            }
            
            overall_quality = sum(
                metrics.get(key, 0.5) * weight for key, weight in weights.items()
            )
            
            metrics['overall_quality'] = overall_quality
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'overall_quality': 0.5}
    
    def _assess_visual_quality(self, image: np.ndarray) -> float:
        """ì‹œê°ì  í’ˆì§ˆ í‰ê°€"""
        try:
            if len(image.shape) < 2:
                return 0.0
            
            # ì„ ëª…ë„ í‰ê°€ (ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°)
            gray = np.mean(image, axis=2) if len(image.shape) == 3 else image
            laplacian_var = self._calculate_laplacian_variance(gray)
            sharpness = min(laplacian_var / 500.0, 1.0)
            
            # ëŒ€ë¹„ í‰ê°€
            contrast = np.std(gray) / 128.0
            contrast = min(contrast, 1.0)
            
            # ë…¸ì´ì¦ˆ í‰ê°€ (ì—­ì‚°)
            noise_level = self._estimate_noise_level(gray)
            noise_score = max(0.0, 1.0 - noise_level)
            
            # ê°€ì¤‘ í‰ê· 
            visual_quality = (sharpness * 0.4 + contrast * 0.4 + noise_score * 0.2)
            
            return float(visual_quality)
            
        except Exception:
            return 0.5
    
    def _calculate_laplacian_variance(self, image: np.ndarray) -> float:
        """ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚° ê³„ì‚°"""
        h, w = image.shape
        total_variance = 0
        count = 0
        
        for i in range(1, h-1):
            for j in range(1, w-1):
                laplacian = (
                    -image[i-1,j-1] - image[i-1,j] - image[i-1,j+1] +
                    -image[i,j-1] + 8*image[i,j] - image[i,j+1] +
                    -image[i+1,j-1] - image[i+1,j] - image[i+1,j+1]
                )
                total_variance += laplacian ** 2
                count += 1
        
        return total_variance / count if count > 0 else 0
    
    def _estimate_noise_level(self, image: np.ndarray) -> float:
        """ë…¸ì´ì¦ˆ ë ˆë²¨ ì¶”ì •"""
        try:
            h, w = image.shape
            high_freq_sum = 0
            count = 0
            
            for i in range(1, h-1):
                for j in range(1, w-1):
                    # ì£¼ë³€ í”½ì…€ê³¼ì˜ ì°¨ì´ ê³„ì‚°
                    center = image[i, j]
                    neighbors = [
                        image[i-1, j], image[i+1, j],
                        image[i, j-1], image[i, j+1]
                    ]
                    
                    variance = np.var([center] + neighbors)
                    high_freq_sum += variance
                    count += 1
            
            if count > 0:
                avg_variance = high_freq_sum / count
                noise_level = min(avg_variance / 1000.0, 1.0)
                return noise_level
            
            return 0.0
            
        except Exception:
            return 0.5
    
    def _assess_fitting_accuracy(self, fitted_image: np.ndarray, 
                               person_image: np.ndarray,
                               clothing_image: np.ndarray) -> float:
        """í”¼íŒ… ì •í™•ë„ í‰ê°€"""
        try:
            if fitted_image.shape != person_image.shape:
                return 0.5
            
            # ì˜ë¥˜ ì˜ì—­ ì¶”ì •
            diff_image = np.abs(fitted_image.astype(np.float32) - person_image.astype(np.float32))
            clothing_region = np.mean(diff_image, axis=2) > 30  # ì„ê³„ê°’ ê¸°ë°˜
            
            if np.sum(clothing_region) == 0:
                return 0.0
            
            # ì˜ë¥˜ ì˜ì—­ì—ì„œì˜ ìƒ‰ìƒ ì¼ì¹˜ë„
            if len(fitted_image.shape) == 3 and len(clothing_image.shape) == 3:
                fitted_colors = fitted_image[clothing_region]
                clothing_mean = np.mean(clothing_image, axis=(0, 1))
                
                color_distances = np.linalg.norm(
                    fitted_colors - clothing_mean, axis=1
                )
                avg_color_distance = np.mean(color_distances)
                max_distance = np.sqrt(255**2 * 3)
                
                color_accuracy = max(0.0, 1.0 - (avg_color_distance / max_distance))
                
                # í”¼íŒ… ì˜ì—­ í¬ê¸° ì ì ˆì„±
                total_pixels = fitted_image.shape[0] * fitted_image.shape[1]
                clothing_ratio = np.sum(clothing_region) / total_pixels
                
                size_score = 1.0
                if clothing_ratio < 0.1:  # ë„ˆë¬´ ì‘ìŒ
                    size_score = clothing_ratio / 0.1
                elif clothing_ratio > 0.6:  # ë„ˆë¬´ í¼
                    size_score = (1.0 - clothing_ratio) / 0.4
                
                fitting_accuracy = (color_accuracy * 0.7 + size_score * 0.3)
                return float(fitting_accuracy)
            
            return 0.5
            
        except Exception:
            return 0.5
    
    def _assess_color_consistency(self, fitted_image: np.ndarray,
                                clothing_image: np.ndarray) -> float:
        """ìƒ‰ìƒ ì¼ì¹˜ë„ í‰ê°€"""
        try:
            if len(fitted_image.shape) != 3 or len(clothing_image.shape) != 3:
                return 0.5
            
            # í‰ê·  ìƒ‰ìƒ ë¹„êµ
            fitted_mean = np.mean(fitted_image, axis=(0, 1))
            clothing_mean = np.mean(clothing_image, axis=(0, 1))
            
            color_distance = np.linalg.norm(fitted_mean - clothing_mean)
            max_distance = np.sqrt(255**2 * 3)
            
            color_consistency = max(0.0, 1.0 - (color_distance / max_distance))
            
            return float(color_consistency)
            
        except Exception:
            return 0.5
    
    def _assess_structural_integrity(self, fitted_image: np.ndarray,
                                   person_image: np.ndarray) -> float:
        """êµ¬ì¡°ì  ë¬´ê²°ì„± í‰ê°€"""
        try:
            if fitted_image.shape != person_image.shape:
                return 0.5
            
            # ê°„ë‹¨í•œ SSIM ê·¼ì‚¬
            if len(fitted_image.shape) == 3:
                fitted_gray = np.mean(fitted_image, axis=2)
                person_gray = np.mean(person_image, axis=2)
                else:
                fitted_gray = fitted_image
                person_gray = person_image
            
            # í‰ê· ê³¼ ë¶„ì‚° ê³„ì‚°
            mu1 = np.mean(fitted_gray)
            mu2 = np.mean(person_gray)
            
            sigma1_sq = np.var(fitted_gray)
            sigma2_sq = np.var(person_gray)
            sigma12 = np.mean((fitted_gray - mu1) * (person_gray - mu2))
            
            # SSIM ê³„ì‚°
            c1 = 0.01 ** 2
            c2 = 0.03 ** 2
            
            numerator = (2 * mu1 * mu2 + c1) * (2 * sigma12 + c2)
            denominator = (mu1**2 + mu2**2 + c1) * (sigma1_sq + sigma2_sq + c2)
            
            ssim = numerator / (denominator + 1e-8)
            
            return float(np.clip(ssim, 0.0, 1.0))
            
        except Exception:
            return 0.5

    # VirtualFittingStep í´ë˜ìŠ¤ì— ê³ ê¸‰ ê¸°ëŠ¥ë“¤ í†µí•©
    def __init__(self, **kwargs):
        """Central Hub DI Container v7.0 ê¸°ë°˜ ì´ˆê¸°í™”"""
        try:
            # 1. í•„ìˆ˜ ì†ì„±ë“¤ ë¨¼ì € ì´ˆê¸°í™” (super() í˜¸ì¶œ ì „)
            self._initialize_step_attributes()
            
            # 2. BaseStepMixin ì´ˆê¸°í™” (Central Hub DI Container ì—°ë™)
            super().__init__(
                step_name="VirtualFittingStep",
                step_id=6,
                **kwargs
            )
            
            # 3. Virtual Fitting íŠ¹í™” ì´ˆê¸°í™”
            self._initialize_virtual_fitting_specifics(**kwargs)
            
            self.logger.info("âœ… VirtualFittingStep v8.0 Central Hub DI Container ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ VirtualFittingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._emergency_setup(**kwargs)


    # ==============================================
    # ğŸ”¥ ì „ì²˜ë¦¬ ì „ìš© ë©”ì„œë“œë“¤
    # ==============================================

    def _preprocess_for_ootd(self, person_tensor, cloth_tensor, pose_tensor, fitting_mode):
        """OOTD ì „ìš© ì „ì²˜ë¦¬"""
        try:
            # OOTD ì…ë ¥ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            person_resized = F.interpolate(person_tensor, size=(512, 384), mode='bilinear')
            cloth_resized = F.interpolate(cloth_tensor, size=(512, 384), mode='bilinear')
            
            # ì •ê·œí™”
            person_normalized = (person_resized - 0.5) / 0.5
            cloth_normalized = (cloth_resized - 0.5) / 0.5
            
            processed = {
                'person': person_normalized,
                'cloth': cloth_normalized
            }
            
            if pose_tensor is not None:
                processed['pose'] = pose_tensor
            
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ OOTD ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    def _preprocess_for_viton_hd(self, person_tensor, cloth_tensor, pose_tensor, fitting_mode):
        """VITON-HD ì „ìš© ì „ì²˜ë¦¬"""
        try:
            # VITON-HD ì…ë ¥ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            person_resized = F.interpolate(person_tensor, size=(512, 512), mode='bilinear')
            cloth_resized = F.interpolate(cloth_tensor, size=(512, 512), mode='bilinear')
            
            # ë§ˆìŠ¤í¬ ìƒì„± (ê°„ë‹¨í•œ ë²„ì „)
            mask = self._generate_fitting_mask(person_resized, fitting_mode)
            
            processed = {
                'person': person_resized,
                'cloth': cloth_resized,
                'mask': mask
            }
            
            if pose_tensor is not None:
                processed['pose'] = pose_tensor
            
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ VITON-HD ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    def _preprocess_for_diffusion(self, person_tensor, cloth_tensor, pose_tensor, fitting_mode):
        """Stable Diffusion ì „ìš© ì „ì²˜ë¦¬"""
        try:
            # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            person_pil = self._tensor_to_pil(person_tensor)
            cloth_pil = self._tensor_to_pil(cloth_tensor)
            
            # ë§ˆìŠ¤í¬ ìƒì„±
            mask_pil = self._generate_inpainting_mask(person_pil, fitting_mode)
            
            return {
                'person_pil': person_pil,
                'cloth_pil': cloth_pil,
                'mask_pil': mask_pil
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    def _generate_fitting_mask(self, person_tensor: torch.Tensor, fitting_mode: str) -> torch.Tensor:
        """í”¼íŒ… ë§ˆìŠ¤í¬ ìƒì„±"""
        try:
            batch_size, channels, height, width = person_tensor.shape
            mask = torch.ones((batch_size, 1, height, width), device=person_tensor.device)
            
            if fitting_mode == 'upper_body':
                # ìƒì²´ ì˜ì—­ ë§ˆìŠ¤í¬
                mask[:, :, height//2:, :] = 0
            elif fitting_mode == 'lower_body':
                # í•˜ì²´ ì˜ì—­ ë§ˆìŠ¤í¬
                mask[:, :, :height//2, :] = 0
            elif fitting_mode == 'full_outfit':
                # ì „ì²´ ë§ˆìŠ¤í¬
                mask = torch.ones_like(mask)
            
            return mask
            
        except Exception as e:
            self.logger.error(f"âŒ í”¼íŒ… ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë§ˆìŠ¤í¬ ë°˜í™˜
            return torch.ones((1, 1, 512, 512), device=person_tensor.device)

    def _generate_inpainting_mask(self, person_pil: Image.Image, fitting_mode: str) -> Image.Image:
        """ì¸í˜ì¸íŒ…ìš© ë§ˆìŠ¤í¬ ìƒì„±"""
        try:
            width, height = person_pil.size
            mask = Image.new('L', (width, height), 255)
            
            if fitting_mode == 'upper_body':
                # ìƒì²´ ì˜ì—­ë§Œ ë§ˆìŠ¤í‚¹
                for y in range(height//2):
                    for x in range(width):
                        mask.putpixel((x, y), 0)
            elif fitting_mode == 'lower_body':
                # í•˜ì²´ ì˜ì—­ë§Œ ë§ˆìŠ¤í‚¹
                for y in range(height//2, height):
                    for x in range(width):
                        mask.putpixel((x, y), 0)
            
            return mask
            
        except Exception as e:
            self.logger.error(f"âŒ ì¸í˜ì¸íŒ… ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return Image.new('L', person_pil.size, 255)

    def _generate_diffusion_prompt(self, fitting_mode: str, cloth_tensor: torch.Tensor) -> str:
        """Diffusionìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        try:
            base_prompt = "A person wearing"
            
            if fitting_mode == 'upper_body':
                prompt = f"{base_prompt} a stylish top, high quality, realistic, well-fitted"
            elif fitting_mode == 'lower_body':
                prompt = f"{base_prompt} fashionable pants, high quality, realistic, well-fitted"
            elif fitting_mode == 'full_outfit':
                prompt = f"{base_prompt} a complete outfit, high quality, realistic, well-fitted, fashionable"
                else:
                prompt = f"{base_prompt} clothing, high quality, realistic, well-fitted"
            
            return prompt
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return "A person wearing clothing, high quality, realistic"

    def _calculate_default_metrics(self) -> Dict[str, float]:
        """ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        return {
            'realism_score': 0.75,
            'pose_alignment': 0.8,
            'color_harmony': 0.7,
            'texture_quality': 0.73,
            'lighting_consistency': 0.78,
            'overall_quality': 0.75
        }

    def _tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            # CPUë¡œ ì´ë™ ë° ë°°ì¹˜ ì°¨ì› ì œê±°
            tensor = tensor.cpu().squeeze(0)
            
            # (C, H, W) -> (H, W, C)
            if len(tensor.shape) == 3:
                tensor = tensor.permute(1, 2, 0)
            
            # 0-255 ë²”ìœ„ë¡œ ë³€í™˜
            if tensor.max() <= 1.0:
                tensor = tensor * 255
            
            # numpyë¡œ ë³€í™˜ í›„ PIL Image ìƒì„±
            image_array = tensor.numpy().astype(np.uint8)
            return Image.fromarray(image_array)
            
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (512, 512), (128, 128, 128))

    def _pil_to_tensor(self, pil_image: Image.Image) -> torch.Tensor:
        """PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        try:
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            image_array = np.array(pil_image)
            
            # (H, W, C) -> (C, H, W)
            if len(image_array.shape) == 3:
                tensor = torch.from_numpy(image_array).permute(2, 0, 1).float()
                else:
                tensor = torch.from_numpy(image_array).float()
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            tensor = tensor.unsqueeze(0)
            
            # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            if tensor.max() > 1.0:
                tensor = tensor / 255.0
            
            return tensor.to(self.device)
            
        except Exception as e:
            self.logger.error(f"âŒ PIL í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return torch.zeros((1, 3, 512, 512), device=self.device)

class TPSWarping:
    """TPS (Thin Plate Spline) ê¸°ë°˜ ì˜ë¥˜ ì›Œí•‘ ì•Œê³ ë¦¬ì¦˜ - ê³ ê¸‰ êµ¬í˜„"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.TPSWarping")
        
    def create_control_points(self, person_mask: np.ndarray, cloth_mask: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ì œì–´ì  ìƒì„± (ì¸ì²´ì™€ ì˜ë¥˜ ê²½ê³„)"""
        try:
            # ì¸ì²´ ë§ˆìŠ¤í¬ì—ì„œ ì œì–´ì  ì¶”ì¶œ
            person_contours = self._extract_contour_points(person_mask)
            cloth_contours = self._extract_contour_points(cloth_mask)
            
            # ì œì–´ì  ë§¤ì¹­
            source_points, target_points = self._match_control_points(cloth_contours, person_contours)
            
            return source_points, target_points
            
        except Exception as e:
            self.logger.error(f"âŒ ì œì–´ì  ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì œì–´ì  ë°˜í™˜
            h, w = person_mask.shape
            source_points = np.array([[w//4, h//4], [3*w//4, h//4], [w//2, h//2], [w//4, 3*h//4], [3*w//4, 3*h//4]])
            target_points = source_points.copy()
            return source_points, target_points
    
    def _extract_contour_points(self, mask: np.ndarray, num_points: int = 20) -> np.ndarray:
        """ë§ˆìŠ¤í¬ì—ì„œ ìœ¤ê³½ì„  ì ë“¤ ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ ê°€ì¥ìë¦¬ ê²€ì¶œ
            edges = self._detect_edges(mask)
            
            # ìœ¤ê³½ì„  ì ë“¤ ì¶”ì¶œ
            y_coords, x_coords = np.where(edges)
            
            if len(x_coords) == 0:
                # í´ë°±: ë§ˆìŠ¤í¬ ì¤‘ì‹¬ ê¸°ë°˜ ì ë“¤
                h, w = mask.shape
                return np.array([[w//2, h//2]])
            
            # ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
            indices = np.linspace(0, len(x_coords)-1, min(num_points, len(x_coords)), dtype=int)
            contour_points = np.column_stack([x_coords[indices], y_coords[indices]])
            
            return contour_points
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìœ¤ê³½ì„  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            h, w = mask.shape
            return np.array([[w//2, h//2]])
    
    def _detect_edges(self, mask: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ ê°€ì¥ìë¦¬ ê²€ì¶œ"""
        try:
            # Sobel í•„í„° ê·¼ì‚¬
            kernel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
            kernel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])
            
            # ì»¨ë³¼ë£¨ì…˜ ì—°ì‚°
            edges_x = self._convolve2d(mask.astype(np.float32), kernel_x)
            edges_y = self._convolve2d(mask.astype(np.float32), kernel_y)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°
            edges = np.sqrt(edges_x**2 + edges_y**2)
            edges = (edges > 0.1).astype(np.uint8)
            
            return edges
            
        except Exception:
            # í´ë°±: ê¸°ë³¸ ê°€ì¥ìë¦¬
            h, w = mask.shape
            edges = np.zeros((h, w), dtype=np.uint8)
            edges[1:-1, 1:-1] = mask[1:-1, 1:-1]
            return edges
    
    def _convolve2d(self, image: np.ndarray, kernel: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ 2D ì»¨ë³¼ë£¨ì…˜"""
        try:
            h, w = image.shape
            kh, kw = kernel.shape
            
            # íŒ¨ë”©
            pad_h, pad_w = kh//2, kw//2
            padded = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode='edge')
            
            # ì»¨ë³¼ë£¨ì…˜
            result = np.zeros((h, w))
            for i in range(h):
                for j in range(w):
                    result[i, j] = np.sum(padded[i:i+kh, j:j+kw] * kernel)
            
            return result
            
        except Exception:
            return np.zeros_like(image)
    
    def _match_control_points(self, source_points: np.ndarray, target_points: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ì œì–´ì  ë§¤ì¹­"""
        try:
            min_len = min(len(source_points), len(target_points))
            return source_points[:min_len], target_points[:min_len]
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì œì–´ì  ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return source_points[:5], target_points[:5]
    
    def apply_tps_transform(self, cloth_image: np.ndarray, source_points: np.ndarray, target_points: np.ndarray) -> np.ndarray:
        """TPS ë³€í™˜ ì ìš©"""
        try:
            if len(source_points) < 3 or len(target_points) < 3:
                return cloth_image
            
            h, w = cloth_image.shape[:2]
            
            # TPS ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            tps_matrix = self._calculate_tps_matrix(source_points, target_points)
            
            # ê·¸ë¦¬ë“œ ìƒì„±
            x_grid, y_grid = np.meshgrid(np.arange(w), np.arange(h))
            grid_points = np.column_stack([x_grid.ravel(), y_grid.ravel()])
            
            # TPS ë³€í™˜ ì ìš©
            transformed_points = self._apply_tps_to_points(grid_points, source_points, target_points, tps_matrix)
            
            # ì´ë¯¸ì§€ ì›Œí•‘
            warped_image = self._warp_image(cloth_image, grid_points, transformed_points)
            
            return warped_image
            
        except Exception as e:
            self.logger.error(f"âŒ TPS ë³€í™˜ ì‹¤íŒ¨: {e}")
            return cloth_image
    
    def _calculate_tps_matrix(self, source_points: np.ndarray, target_points: np.ndarray) -> np.ndarray:
        """TPS ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        try:
            n = len(source_points)
            
            # TPS ì»¤ë„ í–‰ë ¬ ìƒì„±
            K = np.zeros((n, n))
            for i in range(n):
                for j in range(n):
                    if i != j:
                        r = np.linalg.norm(source_points[i] - source_points[j])
                        if r > 0:
                            K[i, j] = r**2 * np.log(r)
            
            # P í–‰ë ¬ (ì–´í•€ ë³€í™˜)
            P = np.column_stack([np.ones(n), source_points])
            
            # L í–‰ë ¬ êµ¬ì„±
            L = np.block([[K, P], [P.T, np.zeros((3, 3))]])
            
            # Y ë²¡í„°
            Y = np.column_stack([target_points, np.zeros((3, 2))])
            
            # ë§¤íŠ¸ë¦­ìŠ¤ í•´ê²° (regularization ì¶”ê°€)
            L_reg = L + 1e-6 * np.eye(L.shape[0])
            tps_matrix = np.linalg.solve(L_reg, Y)
            
            return tps_matrix
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ TPS ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.eye(len(source_points) + 3, 2)
    
    def _apply_tps_to_points(self, points: np.ndarray, source_points: np.ndarray, target_points: np.ndarray, tps_matrix: np.ndarray) -> np.ndarray:
        """ì ë“¤ì— TPS ë³€í™˜ ì ìš©"""
        try:
            n_source = len(source_points)
            n_points = len(points)
            
            # ì»¤ë„ ê°’ ê³„ì‚°
            K_new = np.zeros((n_points, n_source))
            for i in range(n_points):
                for j in range(n_source):
                    r = np.linalg.norm(points[i] - source_points[j])
                    if r > 0:
                        K_new[i, j] = r**2 * np.log(r)
            
            # P_new í–‰ë ¬
            P_new = np.column_stack([np.ones(n_points), points])
            
            # ë³€í™˜ëœ ì ë“¤ ê³„ì‚°
            L_new = np.column_stack([K_new, P_new])
            transformed_points = L_new @ tps_matrix
            
            return transformed_points
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ TPS ì  ë³€í™˜ ì‹¤íŒ¨: {e}")
            return points
    
    def _warp_image(self, image: np.ndarray, source_grid: np.ndarray, target_grid: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ ì›Œí•‘"""
        try:
            h, w = image.shape[:2]
            
            # íƒ€ê²Ÿ ê·¸ë¦¬ë“œë¥¼ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ë³€í™˜
            target_x = target_grid[:, 0].reshape(h, w)
            target_y = target_grid[:, 1].reshape(h, w)
            
            # ê²½ê³„ í´ë¦¬í•‘
            target_x = np.clip(target_x, 0, w-1)
            target_y = np.clip(target_y, 0, h-1)
            
            # ë°”ì´ë¦¬ë‹ˆì–´ ë³´ê°„
            warped_image = self._bilinear_interpolation(image, target_x, target_y)
            
            return warped_image
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì›Œí•‘ ì‹¤íŒ¨: {e}")
            return image
    
    def _bilinear_interpolation(self, image: np.ndarray, x: np.ndarray, y: np.ndarray) -> np.ndarray:
        """ë°”ì´ë¦¬ë‹ˆì–´ ë³´ê°„"""
        try:
            h, w = image.shape[:2]
            
            # ì •ìˆ˜ ì¢Œí‘œ
            x0 = np.floor(x).astype(int)
            x1 = x0 + 1
            y0 = np.floor(y).astype(int)
            y1 = y0 + 1
            
            # ê²½ê³„ ì²˜ë¦¬
            x0 = np.clip(x0, 0, w-1)
            x1 = np.clip(x1, 0, w-1)
            y0 = np.clip(y0, 0, h-1)
            y1 = np.clip(y1, 0, h-1)
            
            # ê°€ì¤‘ì¹˜
            wa = (x1 - x) * (y1 - y)
            wb = (x - x0) * (y1 - y)
            wc = (x1 - x) * (y - y0)
            wd = (x - x0) * (y - y0)
            
            # ë³´ê°„
            if len(image.shape) == 3:
                warped = np.zeros_like(image)
                for c in range(image.shape[2]):
                    warped[:, :, c] = (wa * image[y0, x0, c] + 
                                     wb * image[y0, x1, c] + 
                                     wc * image[y1, x0, c] + 
                                     wd * image[y1, x1, c])
                else:
                warped = (wa * image[y0, x0] + 
                         wb * image[y0, x1] + 
                         wc * image[y1, x0] + 
                         wd * image[y1, x1])
            
            return warped.astype(image.dtype)
            
        except Exception as e:
            self.logger.error(f"âŒ ë°”ì´ë¦¬ë‹ˆì–´ ë³´ê°„ ì‹¤íŒ¨: {e}")
            return image

class AdvancedClothAnalyzer:
    """ê³ ê¸‰ ì˜ë¥˜ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.AdvancedClothAnalyzer")
    
    def analyze_cloth_properties(self, clothing_image: np.ndarray) -> Dict[str, Any]:
        """ì˜ë¥˜ ì†ì„± ê³ ê¸‰ ë¶„ì„"""
        try:
            # ìƒ‰ìƒ ë¶„ì„
            dominant_colors = self._extract_dominant_colors(clothing_image)
            
            # í…ìŠ¤ì²˜ ë¶„ì„
            texture_features = self._analyze_texture(clothing_image)
            
            # íŒ¨í„´ ë¶„ì„
            pattern_type = self._detect_pattern(clothing_image)
            
            return {
                'dominant_colors': dominant_colors,
                'texture_features': texture_features,
                'pattern_type': pattern_type,
                'cloth_complexity': self._calculate_complexity(clothing_image)
            }
            
        except Exception as e:
            self.logger.warning(f"ì˜ë¥˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'cloth_complexity': 0.5}
    
    def _extract_dominant_colors(self, image: np.ndarray, k: int = 5) -> List[List[int]]:
        """ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ (K-means ê¸°ë°˜)"""
        try:
            # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (ì„±ëŠ¥ ìµœì í™”)
            small_img = Image.fromarray(image).resize((150, 150))
            data = np.array(small_img).reshape((-1, 3))
            
            # ê°„ë‹¨í•œ ìƒ‰ìƒ í´ëŸ¬ìŠ¤í„°ë§ (K-means ê·¼ì‚¬)
            unique_colors = {}
            for pixel in data[::10]:  # ìƒ˜í”Œë§
                color_key = tuple(pixel // 32 * 32)  # ìƒ‰ìƒ ì–‘ìí™”
                unique_colors[color_key] = unique_colors.get(color_key, 0) + 1
            
            # ìƒìœ„ kê°œ ìƒ‰ìƒ ë°˜í™˜
            top_colors = sorted(unique_colors.items(), key=lambda x: x[1], reverse=True)[:k]
            return [list(color[0]) for color in top_colors]
            
        except Exception:
            return [[128, 128, 128]]  # ê¸°ë³¸ íšŒìƒ‰
    
    def _analyze_texture(self, image: np.ndarray) -> Dict[str, float]:
        """í…ìŠ¤ì²˜ ë¶„ì„"""
        try:
            gray = np.mean(image, axis=2) if len(image.shape) == 3 else image
            
            # í…ìŠ¤ì²˜ íŠ¹ì§•ë“¤
            features = {}
            
            # í‘œì¤€í¸ì°¨ (ê±°ì¹ ê¸°)
            features['roughness'] = float(np.std(gray) / 255.0)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° (ì—£ì§€ ë°€ë„)
            grad_x = np.abs(np.diff(gray, axis=1))
            grad_y = np.abs(np.diff(gray, axis=0))
            features['edge_density'] = float(np.mean(grad_x) + np.mean(grad_y)) / 255.0
            
            # ì§€ì—­ ë¶„ì‚° (í…ìŠ¤ì²˜ ê· ì¼ì„±)
            local_variance = []
            h, w = gray.shape
            for i in range(0, h-8, 8):
                for j in range(0, w-8, 8):
                    patch = gray[i:i+8, j:j+8]
                    local_variance.append(np.var(patch))
            
            features['uniformity'] = 1.0 - min(np.std(local_variance) / np.mean(local_variance), 1.0) if local_variance else 0.5
            
            return features
            
        except Exception:
            return {'roughness': 0.5, 'edge_density': 0.5, 'uniformity': 0.5}
    
    def _detect_pattern(self, image: np.ndarray) -> str:
        """íŒ¨í„´ ê°ì§€"""
        try:
            gray = np.mean(image, axis=2) if len(image.shape) == 3 else image
            
            # FFT ê¸°ë°˜ ì£¼ê¸°ì„± ë¶„ì„
            f_transform = np.fft.fft2(gray)
            f_shift = np.fft.fftshift(f_transform)
            magnitude_spectrum = np.log(np.abs(f_shift) + 1)
            
            # ì£¼íŒŒìˆ˜ ë„ë©”ì¸ì—ì„œ íŒ¨í„´ ê°ì§€
            center = np.array(magnitude_spectrum.shape) // 2
            
            # ë°©ì‚¬í˜• í‰ê·  ê³„ì‚°
            y, x = np.ogrid[:magnitude_spectrum.shape[0], :magnitude_spectrum.shape[1]]
            distances = np.sqrt((x - center[1])**2 + (y - center[0])**2)
            
            # ì£¼ìš” ì£¼íŒŒìˆ˜ ì„±ë¶„ ë¶„ì„
            radial_profile = []
            for r in range(1, min(center)//2):
                mask = (distances >= r-0.5) & (distances < r+0.5)
                radial_profile.append(np.mean(magnitude_spectrum[mask]))
            
            if len(radial_profile) > 10:
                # ì£¼ê¸°ì  íŒ¨í„´ ê°ì§€
                peaks = []
                for i in range(1, len(radial_profile)-1):
                    if radial_profile[i] > radial_profile[i-1] and radial_profile[i] > radial_profile[i+1]:
                        if radial_profile[i] > np.mean(radial_profile) + np.std(radial_profile):
                            peaks.append(i)
                
                if len(peaks) >= 3:
                    return "striped"
                elif len(peaks) >= 1:
                    return "patterned"
            
            return "solid"
            
        except Exception:
            return "unknown"
    
    def _calculate_complexity(self, image: np.ndarray) -> float:
        """ì˜ë¥˜ ë³µì¡ë„ ê³„ì‚°"""
        try:
            gray = np.mean(image, axis=2) if len(image.shape) == 3 else image
            
            # ì—£ì§€ ë°€ë„
            edges = self._simple_edge_detection(gray)
            edge_density = np.sum(edges) / edges.size
            
            # ìƒ‰ìƒ ë‹¤ì–‘ì„±
            colors = image.reshape(-1, 3) if len(image.shape) == 3 else gray.flatten()
            color_variance = np.var(colors)
            
            # ë³µì¡ë„ ì¢…í•©
            complexity = (edge_density * 0.6 + min(color_variance / 10000, 1.0) * 0.4)
            
            return float(np.clip(complexity, 0.0, 1.0))
            
        except Exception:
            return 0.5
    
    def _simple_edge_detection(self, gray: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œ"""
        try:
            # Sobel í•„í„° ê·¼ì‚¬
            kernel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
            kernel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])
            
            h, w = gray.shape
            edges = np.zeros((h-2, w-2))
            
            for i in range(1, h-1):
                for j in range(1, w-1):
                    patch = gray[i-1:i+2, j-1:j+2]
                    gx = np.sum(patch * kernel_x)
                    gy = np.sum(patch * kernel_y)
                    edges[i-1, j-1] = np.sqrt(gx**2 + gy**2)
            
            return edges > np.mean(edges) + np.std(edges)
            
        except Exception:
            return np.zeros((gray.shape[0]-2, gray.shape[1]-2), dtype=bool)

class AIQualityAssessment:
    """AI í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.QualityAssessment")
        
    def evaluate_fitting_quality(self, fitted_image: np.ndarray, 
                               person_image: np.ndarray,
                               clothing_image: np.ndarray) -> Dict[str, float]:
        """í”¼íŒ… í’ˆì§ˆ í‰ê°€"""
        try:
            metrics = {}
            
            # 1. ì‹œê°ì  í’ˆì§ˆ í‰ê°€
            metrics['visual_quality'] = self._assess_visual_quality(fitted_image)
            
            # 2. í”¼íŒ… ì •í™•ë„ í‰ê°€
            metrics['fitting_accuracy'] = self._assess_fitting_accuracy(
                fitted_image, person_image, clothing_image
            )
            
            # 3. ìƒ‰ìƒ ì¼ì¹˜ë„ í‰ê°€
            metrics['color_consistency'] = self._assess_color_consistency(
                fitted_image, clothing_image
            )
            
            # 4. êµ¬ì¡°ì  ë¬´ê²°ì„± í‰ê°€
            metrics['structural_integrity'] = self._assess_structural_integrity(
                fitted_image, person_image
            )
            
            # 5. ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            weights = {
                'visual_quality': 0.25,
                'fitting_accuracy': 0.35,
                'color_consistency': 0.25,
                'structural_integrity': 0.15
            }
            
            overall_quality = sum(
                metrics.get(key, 0.5) * weight for key, weight in weights.items()
            )
            
            metrics['overall_quality'] = overall_quality
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'overall_quality': 0.5}
    
    def _assess_visual_quality(self, image: np.ndarray) -> float:
        """ì‹œê°ì  í’ˆì§ˆ í‰ê°€"""
        try:
            if len(image.shape) < 2:
                return 0.0
            
            # ì„ ëª…ë„ í‰ê°€ (ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°)
            gray = np.mean(image, axis=2) if len(image.shape) == 3 else image
            laplacian_var = self._calculate_laplacian_variance(gray)
            sharpness = min(laplacian_var / 500.0, 1.0)
            
            # ëŒ€ë¹„ í‰ê°€
            contrast = np.std(gray) / 128.0
            contrast = min(contrast, 1.0)
            
            # ë…¸ì´ì¦ˆ í‰ê°€ (ì—­ì‚°)
            noise_level = self._estimate_noise_level(gray)
            noise_score = max(0.0, 1.0 - noise_level)
            
            # ê°€ì¤‘ í‰ê· 
            visual_quality = (sharpness * 0.4 + contrast * 0.4 + noise_score * 0.2)
            
            return float(visual_quality)
            
        except Exception:
            return 0.5
    
    def _calculate_laplacian_variance(self, image: np.ndarray) -> float:
        """ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚° ê³„ì‚°"""
        h, w = image.shape
        total_variance = 0
        count = 0
        
        for i in range(1, h-1):
            for j in range(1, w-1):
                laplacian = (
                    -image[i-1,j-1] - image[i-1,j] - image[i-1,j+1] +
                    -image[i,j-1] + 8*image[i,j] - image[i,j+1] +
                    -image[i+1,j-1] - image[i+1,j] - image[i+1,j+1]
                )
                total_variance += laplacian ** 2
                count += 1
        
        return total_variance / count if count > 0 else 0
    
    def _estimate_noise_level(self, image: np.ndarray) -> float:
        """ë…¸ì´ì¦ˆ ë ˆë²¨ ì¶”ì •"""
        try:
            h, w = image.shape
            high_freq_sum = 0
            count = 0
            
            for i in range(1, h-1):
                for j in range(1, w-1):
                    # ì£¼ë³€ í”½ì…€ê³¼ì˜ ì°¨ì´ ê³„ì‚°
                    center = image[i, j]
                    neighbors = [
                        image[i-1, j], image[i+1, j],
                        image[i, j-1], image[i, j+1]
                    ]
                    
                    variance = np.var([center] + neighbors)
                    high_freq_sum += variance
                    count += 1
            
            if count > 0:
                avg_variance = high_freq_sum / count
                noise_level = min(avg_variance / 1000.0, 1.0)
                return noise_level
            
            return 0.0
            
        except Exception:
            return 0.5
    
    def _assess_fitting_accuracy(self, fitted_image: np.ndarray, 
                               person_image: np.ndarray,
                               clothing_image: np.ndarray) -> float:
        """í”¼íŒ… ì •í™•ë„ í‰ê°€"""
        try:
            if fitted_image.shape != person_image.shape:
                return 0.5
            
            # ì˜ë¥˜ ì˜ì—­ ì¶”ì •
            diff_image = np.abs(fitted_image.astype(np.float32) - person_image.astype(np.float32))
            clothing_region = np.mean(diff_image, axis=2) > 30  # ì„ê³„ê°’ ê¸°ë°˜
            
            if np.sum(clothing_region) == 0:
                return 0.0
            
            # ì˜ë¥˜ ì˜ì—­ì—ì„œì˜ ìƒ‰ìƒ ì¼ì¹˜ë„
            if len(fitted_image.shape) == 3 and len(clothing_image.shape) == 3:
                fitted_colors = fitted_image[clothing_region]
                clothing_mean = np.mean(clothing_image, axis=(0, 1))
                
                color_distances = np.linalg.norm(
                    fitted_colors - clothing_mean, axis=1
                )
                avg_color_distance = np.mean(color_distances)
                max_distance = np.sqrt(255**2 * 3)
                
                color_accuracy = max(0.0, 1.0 - (avg_color_distance / max_distance))
                
                # í”¼íŒ… ì˜ì—­ í¬ê¸° ì ì ˆì„±
                total_pixels = fitted_image.shape[0] * fitted_image.shape[1]
                clothing_ratio = np.sum(clothing_region) / total_pixels
                
                size_score = 1.0
                if clothing_ratio < 0.1:  # ë„ˆë¬´ ì‘ìŒ
                    size_score = clothing_ratio / 0.1
                elif clothing_ratio > 0.6:  # ë„ˆë¬´ í¼
                    size_score = (1.0 - clothing_ratio) / 0.4
                
                fitting_accuracy = (color_accuracy * 0.7 + size_score * 0.3)
                return float(fitting_accuracy)
            
            return 0.5
            
        except Exception:
            return 0.5
    
    def _assess_color_consistency(self, fitted_image: np.ndarray,
                                clothing_image: np.ndarray) -> float:
        """ìƒ‰ìƒ ì¼ì¹˜ë„ í‰ê°€"""
        try:
            if len(fitted_image.shape) != 3 or len(clothing_image.shape) != 3:
                return 0.5
            
            # í‰ê·  ìƒ‰ìƒ ë¹„êµ
            fitted_mean = np.mean(fitted_image, axis=(0, 1))
            clothing_mean = np.mean(clothing_image, axis=(0, 1))
            
            color_distance = np.linalg.norm(fitted_mean - clothing_mean)
            max_distance = np.sqrt(255**2 * 3)
            
            color_consistency = max(0.0, 1.0 - (color_distance / max_distance))
            
            return float(color_consistency)
            
        except Exception:
            return 0.5
    
    def _assess_structural_integrity(self, fitted_image: np.ndarray,
                                   person_image: np.ndarray) -> float:
        """êµ¬ì¡°ì  ë¬´ê²°ì„± í‰ê°€"""
        try:
            if fitted_image.shape != person_image.shape:
                return 0.5
            
            # ê°„ë‹¨í•œ SSIM ê·¼ì‚¬
            if len(fitted_image.shape) == 3:
                fitted_gray = np.mean(fitted_image, axis=2)
                person_gray = np.mean(person_image, axis=2)
                else:
                fitted_gray = fitted_image
                person_gray = person_image
            
            # í‰ê· ê³¼ ë¶„ì‚° ê³„ì‚°
            mu1 = np.mean(fitted_gray)
            mu2 = np.mean(person_gray)
            
            sigma1_sq = np.var(fitted_gray)
            sigma2_sq = np.var(person_gray)
            sigma12 = np.mean((fitted_gray - mu1) * (person_gray - mu2))
            
            # SSIM ê³„ì‚°
            c1 = 0.01 ** 2
            c2 = 0.03 ** 2
            
            numerator = (2 * mu1 * mu2 + c1) * (2 * sigma12 + c2)
            denominator = (mu1**2 + mu2**2 + c1) * (sigma1_sq + sigma2_sq + c2)
            
            ssim = numerator / (denominator + 1e-8)
            
            return float(np.clip(ssim, 0.0, 1.0))
            
        except Exception:
            return 0.5

    # VirtualFittingStep í´ë˜ìŠ¤ì— ê³ ê¸‰ ê¸°ëŠ¥ë“¤ í†µí•©
    def __init__(self, **kwargs):
        # ê¸°ì¡´ ì´ˆê¸°í™” ì½”ë“œ...
        try:
            # 1. í•„ìˆ˜ ì†ì„±ë“¤ ë¨¼ì € ì´ˆê¸°í™” (super() í˜¸ì¶œ ì „)
            self._initialize_step_attributes()
            
            # 2. BaseStepMixin ì´ˆê¸°í™” (Central Hub DI Container ì—°ë™)
            super().__init__(
                step_name="VirtualFittingStep",
                step_id=6,
                **kwargs
            )
            
            # 3. Virtual Fitting íŠ¹í™” ì´ˆê¸°í™”
            self._initialize_virtual_fitting_specifics(**kwargs)
            
            # ğŸ”¥ 4. ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ë“¤ ì´ˆê¸°í™”
            self.tps_warping = TPSWarping()
            self.cloth_analyzer = AdvancedClothAnalyzer()
            self.quality_assessor = AIQualityAssessment()
            
            self.logger.info("âœ… VirtualFittingStep v8.0 ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ í¬í•¨ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ VirtualFittingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._emergency_setup(**kwargs)

    # ==============================================
    # ğŸ”¥ ì „ì²˜ë¦¬ ì „ìš© ë©”ì„œë“œë“¤
    # ==============================================

    def _preprocess_for_ootd(self, person_tensor, cloth_tensor, pose_tensor, fitting_mode):
        """OOTD ì „ìš© ì „ì²˜ë¦¬"""
        try:
            # OOTD ì…ë ¥ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            person_resized = F.interpolate(person_tensor, size=(512, 384), mode='bilinear')
            cloth_resized = F.interpolate(cloth_tensor, size=(512, 384), mode='bilinear')
            
            # ì •ê·œí™”
            person_normalized = (person_resized - 0.5) / 0.5
            cloth_normalized = (cloth_resized - 0.5) / 0.5
            
            processed = {
                'person': person_normalized,
                'cloth': cloth_normalized
            }
            
            if pose_tensor is not None:
                processed['pose'] = pose_tensor
            
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ OOTD ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    def _preprocess_for_viton_hd(self, person_tensor, cloth_tensor, pose_tensor, fitting_mode):
        """VITON-HD ì „ìš© ì „ì²˜ë¦¬"""
        try:
            # VITON-HD ì…ë ¥ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            person_resized = F.interpolate(person_tensor, size=(512, 512), mode='bilinear')
            cloth_resized = F.interpolate(cloth_tensor, size=(512, 512), mode='bilinear')
            
            # ë§ˆìŠ¤í¬ ìƒì„± (ê°„ë‹¨í•œ ë²„ì „)
            mask = self._generate_fitting_mask(person_resized, fitting_mode)
            
            processed = {
                'person': person_resized,
                'cloth': cloth_resized,
                'mask': mask
            }
            
            if pose_tensor is not None:
                processed['pose'] = pose_tensor
            
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ VITON-HD ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    def _preprocess_for_diffusion(self, person_tensor, cloth_tensor, pose_tensor, fitting_mode):
        """Stable Diffusion ì „ìš© ì „ì²˜ë¦¬"""
        try:
            # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            person_pil = self._tensor_to_pil(person_tensor)
            cloth_pil = self._tensor_to_pil(cloth_tensor)
            
            # ë§ˆìŠ¤í¬ ìƒì„±
            mask_pil = self._generate_inpainting_mask(person_pil, fitting_mode)
            
            return {
                'person_pil': person_pil,
                'cloth_pil': cloth_pil,
                'mask_pil': mask_pil
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    def _generate_fitting_mask(self, person_tensor: torch.Tensor, fitting_mode: str) -> torch.Tensor:
        """í”¼íŒ… ë§ˆìŠ¤í¬ ìƒì„±"""
        try:
            batch_size, channels, height, width = person_tensor.shape
            mask = torch.ones((batch_size, 1, height, width), device=person_tensor.device)
            
            if fitting_mode == 'upper_body':
                # ìƒì²´ ì˜ì—­ ë§ˆìŠ¤í¬
                mask[:, :, height//2:, :] = 0
            elif fitting_mode == 'lower_body':
                # í•˜ì²´ ì˜ì—­ ë§ˆìŠ¤í¬
                mask[:, :, :height//2, :] = 0
            elif fitting_mode == 'full_outfit':
                # ì „ì²´ ë§ˆìŠ¤í¬
                mask = torch.ones_like(mask)
            
            return mask
            
        except Exception as e:
            self.logger.error(f"âŒ í”¼íŒ… ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë§ˆìŠ¤í¬ ë°˜í™˜
            return torch.ones((1, 1, 512, 512), device=person_tensor.device)

    def _generate_inpainting_mask(self, person_pil: Image.Image, fitting_mode: str) -> Image.Image:
        """ì¸í˜ì¸íŒ…ìš© ë§ˆìŠ¤í¬ ìƒì„±"""
        try:
            width, height = person_pil.size
            mask = Image.new('L', (width, height), 255)
            
            if fitting_mode == 'upper_body':
                # ìƒì²´ ì˜ì—­ë§Œ ë§ˆìŠ¤í‚¹
                for y in range(height//2):
                    for x in range(width):
                        mask.putpixel((x, y), 0)
            elif fitting_mode == 'lower_body':
                # í•˜ì²´ ì˜ì—­ë§Œ ë§ˆìŠ¤í‚¹
                for y in range(height//2, height):
                    for x in range(width):
                        mask.putpixel((x, y), 0)
            
            return mask
            
        except Exception as e:
            self.logger.error(f"âŒ ì¸í˜ì¸íŒ… ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return Image.new('L', person_pil.size, 255)

    def _generate_diffusion_prompt(self, fitting_mode: str, cloth_tensor: torch.Tensor) -> str:
        """Diffusionìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        try:
            base_prompt = "A person wearing"
            
            if fitting_mode == 'upper_body':
                prompt = f"{base_prompt} a stylish top, high quality, realistic, well-fitted"
            elif fitting_mode == 'lower_body':
                prompt = f"{base_prompt} fashionable pants, high quality, realistic, well-fitted"
            elif fitting_mode == 'full_outfit':
                prompt = f"{base_prompt} a complete outfit, high quality, realistic, well-fitted, fashionable"
                else:
                prompt = f"{base_prompt} clothing, high quality, realistic, well-fitted"
            
            return prompt
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return "A person wearing clothing, high quality, realistic"

    def _calculate_default_metrics(self) -> Dict[str, float]:
        """ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        return {
            'realism_score': 0.75,
            'pose_alignment': 0.8,
            'color_harmony': 0.7,
            'texture_quality': 0.73,
            'lighting_consistency': 0.78,
            'overall_quality': 0.75
        }

    def _tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            # CPUë¡œ ì´ë™ ë° ë°°ì¹˜ ì°¨ì› ì œê±°
            tensor = tensor.cpu().squeeze(0)
            
            # (C, H, W) -> (H, W, C)
            if len(tensor.shape) == 3:
                tensor = tensor.permute(1, 2, 0)
            
            # 0-255 ë²”ìœ„ë¡œ ë³€í™˜
            if tensor.max() <= 1.0:
                tensor = tensor * 255
            
            # numpyë¡œ ë³€í™˜ í›„ PIL Image ìƒì„±
            image_array = tensor.numpy().astype(np.uint8)
            return Image.fromarray(image_array)
            
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (512, 512), (128, 128, 128))

    def _pil_to_tensor(self, pil_image: Image.Image) -> torch.Tensor:
        """PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        try:
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            image_array = np.array(pil_image)
            
            # (H, W, C) -> (C, H, W)
            if len(image_array.shape) == 3:
                tensor = torch.from_numpy(image_array).permute(2, 0, 1).float()
                else:
                tensor = torch.from_numpy(image_array).float()
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            tensor = tensor.unsqueeze(0)
            
            # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            if tensor.max() > 1.0:
                tensor = tensor / 255.0
            
            return tensor.to(self.device)
            
        except Exception as e:
            self.logger.error(f"âŒ PIL í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return torch.zeros((1, 3, 512, 512), device=self.device)

class VirtualFittingStep(BaseStepMixin):
    """
    ğŸ”¥ Step 06: Virtual Fitting v8.0 - Central Hub DI Container ì™„ì „ ì—°ë™
    
    Central Hub DI Container v7.0ì—ì„œ ìë™ ì œê³µ:
    âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì…
    âœ… MemoryManager ìë™ ì—°ê²°  
    âœ… DataConverter í†µí•©
    âœ… ìë™ ì´ˆê¸°í™” ë° ì„¤ì •
    """
    
    def __init__(self, **kwargs):
        """Central Hub DI Container v7.0 ê¸°ë°˜ ì´ˆê¸°í™”"""
        try:
            # 1. í•„ìˆ˜ ì†ì„±ë“¤ ë¨¼ì € ì´ˆê¸°í™” (super() í˜¸ì¶œ ì „)
            self._initialize_step_attributes()
            
            # 2. BaseStepMixin ì´ˆê¸°í™” (Central Hub DI Container ì—°ë™)
            super().__init__(
                step_name="VirtualFittingStep",
                step_id=6,
                **kwargs
            )
            
            # 3. Virtual Fitting íŠ¹í™” ì´ˆê¸°í™”
            self._initialize_virtual_fitting_specifics(**kwargs)
            
            self.logger.info("âœ… VirtualFittingStep v8.0 Central Hub DI Container ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ VirtualFittingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._emergency_setup(**kwargs)
    
    def _initialize_step_attributes(self):
        """í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™” (BaseStepMixin ìš”êµ¬ì‚¬í•­)"""
        self.ai_models = {}
        self.models_loading_status = {
            'ootd': False,
            'viton_hd': False,
            'diffusion': False,
            'mock_model': False
        }
        self.model_interface = None
        self.loaded_models = []
        self.logger = logging.getLogger(f"{__name__}.VirtualFittingStep")
        
        # Virtual Fitting íŠ¹í™” ì†ì„±ë“¤
        self.fitting_models = {}
        self.fitting_ready = False
        self.fitting_cache = {}
        self.pose_processor = None
        self.lighting_adapter = None
        self.texture_enhancer = None
        self.diffusion_pipeline = None
    
    def _initialize_virtual_fitting_specifics(self, **kwargs):
        """Virtual Fitting íŠ¹í™” ì´ˆê¸°í™”"""
        try:
            # ì„¤ì •
            self.config = VirtualFittingConfig()
            if 'config' in kwargs:
                config_dict = kwargs['config']
                if isinstance(config_dict, dict):
                    for key, value in config_dict.items():
                        if hasattr(self.config, key):
                            setattr(self.config, key, value)
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            self.device = self._detect_optimal_device()
            self.tps_warping = TPSWarping()
            self.cloth_analyzer = AdvancedClothAnalyzer()
            self.quality_assessor = AIQualityAssessment()
        
                # AI ëª¨ë¸ ë¡œë”© (Central Hubë¥¼ í†µí•´)
            self._load_virtual_fitting_models_via_central_hub()
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Virtual Fitting íŠ¹í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _detect_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            if TORCH_AVAILABLE:
                if torch.backends.mps.is_available():
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
            return "cpu"
            except:
            return "cpu"
 

    def _emergency_setup(self, **kwargs):
        """ê¸´ê¸‰ ì„¤ì • (ì´ˆê¸°í™” ì‹¤íŒ¨ì‹œ í´ë°±)"""
        try:
            self.logger.warning("âš ï¸ VirtualFittingStep ê¸´ê¸‰ ì„¤ì • ëª¨ë“œ í™œì„±í™”")
            
            # ê¸°ë³¸ ì†ì„±ë“¤ ì„¤ì •
            self.step_name = "VirtualFittingStep"
            self.step_id = 6
            self.device = "cpu"
            self.config = VirtualFittingConfig()
            
            # ë¹ˆ ëª¨ë¸ ì»¨í…Œì´ë„ˆë“¤
            self.ai_models = {}
            self.models_loading_status = {'emergency': True}  
            self.model_interface = None
            self.loaded_models = []
            
            # Virtual Fitting íŠ¹í™” ì†ì„±ë“¤
            self.fitting_models = {}
            self.fitting_ready = False
            self.fitting_cache = {}
            self.pose_processor = None
            self.lighting_adapter = None
            self.texture_enhancer = None
            self.diffusion_pipeline = None
            
            # ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ë“¤ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ
            try:
                self.tps_warping = TPSWarping()
                self.cloth_analyzer = AdvancedClothAnalyzer()
                self.quality_assessor = AIQualityAssessment()
                except:
                self.tps_warping = None
                self.cloth_analyzer = None
                self.quality_assessor = None
            
            # Mock ëª¨ë¸ ìƒì„±
            self._create_mock_virtual_fitting_models()
            
            self.logger.warning("âœ… VirtualFittingStep ê¸´ê¸‰ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸´ê¸‰ ì„¤ì •ë„ ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ì†ì„±ë“¤ë§Œ
            self.step_name = "VirtualFittingStep"
            self.step_id = 6
            self.device = "cpu"
            self.ai_models = {}
            self.loaded_models = []
            self.fitting_ready = False

    # ==============================================
    # ğŸ”¥ Central Hub DI Container ì—°ë™ AI ëª¨ë¸ ë¡œë”©
    # ==============================================

    def _load_virtual_fitting_models_via_central_hub(self):
        """Central Hub DI Containerë¥¼ í†µí•œ Virtual Fitting ëª¨ë¸ ë¡œë”©"""
        try:
            self.logger.info("ğŸ”„ Central Hubë¥¼ í†µí•œ Virtual Fitting AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # Central Hubì—ì„œ ModelLoader ê°€ì ¸ì˜¤ê¸° (ìë™ ì£¼ì…ë¨)
            if not hasattr(self, 'model_loader') or not self.model_loader:
                self.logger.warning("âš ï¸ ModelLoaderê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ - Mock ëª¨ë¸ë¡œ í´ë°±")
                self._create_mock_virtual_fitting_models()
                return
            
            # 1. OOTD (Outfit Of The Day) ëª¨ë¸ ë¡œë”© (Primary) - 3.2GB
            try:
                ootd_model = self.model_loader.load_model(
                    model_name="ootd_diffusion.pth",
                    step_name="VirtualFittingStep",
                    model_type="virtual_try_on"
                )
                
                if ootd_model:
                    self.ai_models['ootd'] = ootd_model
                    self.models_loading_status['ootd'] = True
                    self.loaded_models.append('ootd')
                    self.logger.info("âœ… OOTD ëª¨ë¸ ë¡œë”© ì™„ë£Œ (3.2GB)")
                    else:
                    self.logger.warning("âš ï¸ OOTD ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ OOTD ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # 2. VITON-HD ëª¨ë¸ ë¡œë”© - 2.1GB
            try:
                viton_model = self.model_loader.load_model(
                    model_name="viton_hd_final.pth",
                    step_name="VirtualFittingStep", 
                    model_type="virtual_try_on"
                )
                
                if viton_model:
                    self.ai_models['viton_hd'] = viton_model
                    self.models_loading_status['viton_hd'] = True
                    self.loaded_models.append('viton_hd')
                    self.logger.info("âœ… VITON-HD ëª¨ë¸ ë¡œë”© ì™„ë£Œ (2.1GB)")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ VITON-HD ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # 3. Stable Diffusion ëª¨ë¸ ë¡œë”© - 4.8GB
            try:
                diffusion_model = self.model_loader.load_model(
                    model_name="stable_diffusion_inpainting.pth",
                    step_name="VirtualFittingStep",
                    model_type="image_generation"
                )
                
                if diffusion_model:
                    self.ai_models['diffusion'] = diffusion_model
                    self.models_loading_status['diffusion'] = True
                    self.loaded_models.append('diffusion')
                    self.logger.info("âœ… Stable Diffusion ëª¨ë¸ ë¡œë”© ì™„ë£Œ (4.8GB)")
                    
                    # Diffusion íŒŒì´í”„ë¼ì¸ ì„¤ì •
                    if DIFFUSERS_AVAILABLE:
                        self._setup_diffusion_pipeline(diffusion_model)
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ Stable Diffusion ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # 4. ëª¨ë¸ì´ í•˜ë‚˜ë„ ë¡œë”©ë˜ì§€ ì•Šì€ ê²½ìš° Mock ëª¨ë¸ ìƒì„±
            if not self.loaded_models:
                self.logger.warning("âš ï¸ ì‹¤ì œ AI ëª¨ë¸ì´ í•˜ë‚˜ë„ ë¡œë”©ë˜ì§€ ì•ŠìŒ - Mock ëª¨ë¸ë¡œ í´ë°±")
                self._create_mock_virtual_fitting_models()
            
            # Model Interface ì„¤ì •
            if hasattr(self.model_loader, 'create_step_interface'):
                self.model_interface = self.model_loader.create_step_interface("VirtualFittingStep")
            
            # Virtual Fitting ì¤€ë¹„ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.fitting_ready = len(self.loaded_models) > 0
            
            # ë³´ì¡° í”„ë¡œì„¸ì„œë“¤ ì´ˆê¸°í™”
            self._initialize_auxiliary_processors()
            
            loaded_count = len(self.loaded_models)
            self.logger.info(f"ğŸ§  Central Hub Virtual Fitting ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}ê°œ ëª¨ë¸")
            
        except Exception as e:
            self.logger.error(f"âŒ Central Hub Virtual Fitting ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self._create_mock_virtual_fitting_models()

    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ VirtualFittingStep ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ (BaseStepMixin í‘œì¤€)
        ì™¸ë¶€ì—ì„œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ ì¸í„°í˜ì´ìŠ¤
        """
        try:
            self.logger.info(f"ğŸš€ {self.step_name} ì²˜ë¦¬ ì‹œì‘")
            
            # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦
            if not input_data:
                raise ValueError("ì…ë ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # 2. í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ['person_image', 'cloth_image']
            for field in required_fields:
                if field not in input_data:
                    raise ValueError(f"í•„ìˆ˜ í•„ë“œ '{field}'ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # 3. ì „ì²˜ë¦¬ ì ìš© (BaseStepMixin í‘œì¤€)
            if hasattr(self, '_apply_preprocessing'):
                processed_input = await self._apply_preprocessing(input_data)
                else:
                processed_input = input_data.copy()
            
            # 4. AI ì¶”ë¡  ì‹¤í–‰ (í•µì‹¬ ë¡œì§)
            result = self._run_ai_inference(processed_input)
            
            # 5. í›„ì²˜ë¦¬ ì ìš© (BaseStepMixin í‘œì¤€)
            if hasattr(self, '_apply_postprocessing'):
                final_result = await self._apply_postprocessing(result, input_data)
                else:
                final_result = result
            
            # 6. ì„±ê³µ ì‘ë‹µ ë°˜í™˜
            if final_result.get('success', True):
                self.logger.info(f"âœ… {self.step_name} ì²˜ë¦¬ ì™„ë£Œ")
                return final_result
                else:
                self.logger.error(f"âŒ {self.step_name} ì²˜ë¦¬ ì‹¤íŒ¨: {final_result.get('error')}")
                return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'success': False,
                'error': str(e),
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': 0.0
            }

              
    def initialize(self) -> bool:
        """Step ì´ˆê¸°í™” (BaseStepMixin í‘œì¤€)"""
        try:
            if self.is_initialized:
                return True
            
            # ëª¨ë¸ ë¡œë”© í™•ì¸
            if not self.fitting_ready:
                self.logger.warning("âš ï¸ Virtual Fitting ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ")
            
            self.is_initialized = True
            self.is_ready = self.fitting_ready
            
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def cleanup(self):
        """Step ì •ë¦¬ (BaseStepMixin í‘œì¤€)"""
        try:
            # AI ëª¨ë¸ë“¤ ì •ë¦¬
            if hasattr(self, 'ai_models'):
                for model_name, model in self.ai_models.items():
                    if hasattr(model, 'cleanup'):
                        model.cleanup()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                try:
                    import torch
                    if torch.backends.mps.is_available():
                        torch.mps.empty_cache()
                    elif torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    except:
                    pass
            
            self.logger.info(f"âœ… {self.step_name} ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def get_status(self) -> Dict[str, Any]:
        """Step ìƒíƒœ ë°˜í™˜ (BaseStepMixin í‘œì¤€)"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'is_initialized': self.is_initialized,
            'is_ready': self.is_ready,
            'fitting_ready': self.fitting_ready,
            'models_loaded': len(self.loaded_models),
            'device': self.device,
            'auxiliary_processors': {
                'pose_processor': self.pose_processor is not None,
                'lighting_adapter': self.lighting_adapter is not None,
                'texture_enhancer': self.texture_enhancer is not None
            }
        }

    async def _apply_preprocessing(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì „ì²˜ë¦¬ ì ìš© (BaseStepMixin í‘œì¤€)"""
        try:
            processed = input_data.copy()
            
            # ê¸°ë³¸ ê²€ì¦
            if 'person_image' in processed and 'cloth_image' in processed:
                # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                processed['person_image'] = self._preprocess_image(processed['person_image'])
                processed['cloth_image'] = self._preprocess_image(processed['cloth_image'])
            
            self.logger.debug(f"âœ… {self.step_name} ì „ì²˜ë¦¬ ì™„ë£Œ")
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return input_data
        
    async def _apply_postprocessing(self, ai_result: Dict[str, Any], original_input: Dict[str, Any]) -> Dict[str, Any]:
        """í›„ì²˜ë¦¬ ì ìš© (BaseStepMixin í‘œì¤€)"""
        try:
            processed = ai_result.copy()
            
            # ì´ë¯¸ì§€ ê²°ê³¼ê°€ ìˆìœ¼ë©´ Base64ë¡œ ë³€í™˜ (API ì‘ë‹µìš©)
            if 'fitted_image' in processed and processed['fitted_image'] is not None:
                # Base64 ë³€í™˜ì€ í•„ìš”ì‹œì—ë§Œ
                pass
            
            self.logger.debug(f"âœ… {self.step_name} í›„ì²˜ë¦¬ ì™„ë£Œ")
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ai_result
    

    def _load_detailed_data_spec_from_kwargs(self, **kwargs):
        """DetailedDataSpec ë¡œë“œ (BaseStepMixin í˜¸í™˜)"""
        try:
            # VirtualFittingStepìš© ê¸°ë³¸ ìŠ¤í™
            class VirtualFittingDataSpec:
                def __init__(self):
                    self.input_data_types = {
                        'person_image': 'PIL.Image.Image',
                        'cloth_image': 'PIL.Image.Image',
                        'fitting_mode': 'str',
                        'quality_level': 'str'
                    }
                    self.output_data_types = {
                        'fitted_image': 'numpy.ndarray',
                        'fitting_confidence': 'float',
                        'success': 'bool'
                    }
                    self.preprocessing_steps = ['resize_768x1024', 'normalize']
                    self.postprocessing_steps = ['denormalize', 'format_output']
                    self.api_input_mapping = {
                        'person_image': 'fastapi.UploadFile -> PIL.Image.Image',
                        'cloth_image': 'fastapi.UploadFile -> PIL.Image.Image'
                    }
                    self.api_output_mapping = {
                        'fitted_image': 'numpy.ndarray -> base64_string',
                        'success': 'bool -> bool'
                    }
            
            return VirtualFittingDataSpec()
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ DetailedDataSpec ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _initialize_performance_stats(self):
        """ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™” (BaseStepMixin í˜¸í™˜)"""
        try:
            self.performance_stats = {
                'total_requests': 0,
                'successful_requests': 0,
                'failed_requests': 0,
                'average_processing_time': 0.0,
                'last_processing_time': 0.0
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.performance_stats = {}
  
    
    def _setup_diffusion_pipeline(self, diffusion_model):
        """Stable Diffusion íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
        try:
            if not DIFFUSERS_AVAILABLE:
                self.logger.warning("âš ï¸ Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - Diffusion íŒŒì´í”„ë¼ì¸ ìŠ¤í‚µ")
                return
            
            # Stable Diffusion íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
            self.diffusion_pipeline = StableDiffusionPipeline.from_pretrained(
                "stabilityai/stable-diffusion-2-inpainting",
                torch_dtype=torch.float16 if self.device != "cpu" else torch.float32,
                safety_checker=None,
                requires_safety_checker=False
            ).to(self.device)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ìµœì í™”
            self.diffusion_pipeline.scheduler = DDIMScheduler.from_config(
                self.diffusion_pipeline.scheduler.config
            )
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 
            if hasattr(self.diffusion_pipeline, 'enable_model_cpu_offload'):
                self.diffusion_pipeline.enable_model_cpu_offload()
            
            self.logger.info("âœ… Stable Diffusion íŒŒì´í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Diffusion íŒŒì´í”„ë¼ì¸ ì„¤ì • ì‹¤íŒ¨: {e}")

    def _initialize_auxiliary_processors(self):
        """ë³´ì¡° í”„ë¡œì„¸ì„œë“¤ ì´ˆê¸°í™”"""
        try:
            # í¬ì¦ˆ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
            if 'ootd' in self.loaded_models or 'viton_hd' in self.loaded_models:
                self.pose_processor = self._create_pose_processor()
            
            # ì¡°ëª… ì ì‘ í”„ë¡œì„¸ì„œ
            if self.config.enable_lighting_adaptation:
                self.lighting_adapter = self._create_lighting_adapter()
            
            # í…ìŠ¤ì²˜ í–¥ìƒ í”„ë¡œì„¸ì„œ
            if self.config.enable_texture_preservation:
                self.texture_enhancer = self._create_texture_enhancer()
            
            self.logger.info("âœ… ë³´ì¡° í”„ë¡œì„¸ì„œë“¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë³´ì¡° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def _create_mock_virtual_fitting_models(self):
        """Mock Virtual Fitting ëª¨ë¸ ìƒì„± (ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ì‹œ í´ë°±)"""
        try:
            class MockVirtualFittingModel:
                def __init__(self, model_name: str):
                    self.model_name = model_name
                    self.device = "cpu"
                    
                def predict(
                    self, 
                    person_image: np.ndarray, 
                    cloth_image: np.ndarray, 
                    pose_keypoints: Optional[np.ndarray] = None,
                    fitting_mode: str = 'single_item'
                ) -> Dict[str, Any]:
                    """Mock ì˜ˆì¸¡ (ê¸°ë³¸ì ì¸ Virtual Fitting)"""
                    h, w = person_image.shape[:2] if len(person_image.shape) >= 2 else (768, 1024)
                    
                    # ê¸°ë³¸ Virtual Fitting ì ìš© (ì˜ë¥˜ ì˜¤ë²„ë ˆì´)
                    fitted_image = self._apply_mock_virtual_fitting(person_image, cloth_image, fitting_mode)
                    
                    # Mock í”¼íŒ… ë©”íŠ¸ë¦­
                    fitting_metrics = {
                        'realism_score': 0.75,
                        'pose_alignment': 0.8,
                        'color_harmony': 0.7,
                        'texture_quality': 0.73,
                        'lighting_consistency': 0.78,
                        'overall_quality': 0.75
                    }
                    
                    # Mock ì¶”ì²œì‚¬í•­
                    recommendations = [
                        f"Mock {self.model_name} fitting completed",
                        "Consider adjusting pose for better fit",
                        "Lighting adaptation applied"
                    ]
                    
                    return {
                        'fitted_image': fitted_image,
                        'fitting_confidence': 0.75,
                        'fitting_mode': fitting_mode,
                        'fitting_metrics': fitting_metrics,
                        'processing_stages': [f'mock_{self.model_name}_stage_1', f'mock_{self.model_name}_stage_2'],
                        'recommendations': recommendations,
                        'alternative_styles': self._generate_mock_alternatives(),
                        'model_type': 'mock',
                        'model_name': self.model_name
                    }
                
                def _apply_mock_virtual_fitting(self, person_image: np.ndarray, cloth_image: np.ndarray, fitting_mode: str) -> np.ndarray:
                    """Mock Virtual Fitting ì ìš©"""
                    try:
                        # ê¸°ë³¸ ì´ë¯¸ì§€ ë¸”ë Œë”©
                        h, w = person_image.shape[:2]
                        
                        # ì˜ë¥˜ í¬ê¸° ì¡°ì •
                        if fitting_mode == 'upper_body':
                            cloth_resized = cv2.resize(cloth_image, (w//2, h//3))
                            overlay_region = (h//6, h//2, w//4, 3*w//4)
                        elif fitting_mode == 'lower_body':
                            cloth_resized = cv2.resize(cloth_image, (w//3, h//2))
                            overlay_region = (h//2, h, w//3, 2*w//3)
                            else:  # single_item or full_outfit
                            cloth_resized = cv2.resize(cloth_image, (w//2, 2*h//3))
                            overlay_region = (h//6, 5*h//6, w//4, 3*w//4)
                        
                        # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
                        result = person_image.copy()
                        
                        # ì˜ë¥˜ ì˜¤ë²„ë ˆì´ ì ìš©
                        start_y, end_y, start_x, end_x = overlay_region
                        if end_y <= h and end_x <= w:
                            # ì•ŒíŒŒ ë¸”ë Œë”©
                            alpha = 0.7
                            overlay_h = min(cloth_resized.shape[0], end_y - start_y)
                            overlay_w = min(cloth_resized.shape[1], end_x - start_x)
                            
                            result[start_y:start_y+overlay_h, start_x:start_x+overlay_w] = (
                                alpha * cloth_resized[:overlay_h, :overlay_w] + 
                                (1 - alpha) * result[start_y:start_y+overlay_h, start_x:start_x+overlay_w]
                            ).astype(np.uint8)
                        
                        return result
                        
                    except Exception as e:
                        # í´ë°±: ì›ë³¸ person_image ë°˜í™˜
                        return person_image
                
                def _generate_mock_alternatives(self) -> List[Dict[str, Any]]:
                    """Mock ëŒ€ì•ˆ ìŠ¤íƒ€ì¼ ìƒì„±"""
                    return [
                        {'style': 'casual', 'confidence': 0.8},
                        {'style': 'formal', 'confidence': 0.7},
                        {'style': 'sporty', 'confidence': 0.75}
                    ]
            
            # Mock ëª¨ë¸ë“¤ ìƒì„±
            self.ai_models['mock_ootd'] = MockVirtualFittingModel('mock_ootd')
            self.ai_models['mock_viton'] = MockVirtualFittingModel('mock_viton')
            self.ai_models['mock_diffusion'] = MockVirtualFittingModel('mock_diffusion')
            self.models_loading_status['mock_model'] = True
            self.loaded_models = ['mock_ootd', 'mock_viton', 'mock_diffusion']
            self.fitting_ready = True
            
            # Mock ë³´ì¡° í”„ë¡œì„¸ì„œë“¤ ì„¤ì •
            self.pose_processor = self._create_mock_pose_processor()
            self.lighting_adapter = self._create_mock_lighting_adapter()
            self.texture_enhancer = self._create_mock_texture_enhancer()
            
            self.logger.info("âœ… Mock Virtual Fitting ëª¨ë¸ ìƒì„± ì™„ë£Œ (í´ë°± ëª¨ë“œ)")
            
        except Exception as e:
            self.logger.error(f"âŒ Mock Virtual Fitting ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")

    # ==============================================
    # ğŸ”¥ BaseStepMixin v20.0 í‘œì¤€ _run_ai_inference() ë©”ì„œë“œ
    # ==============================================

    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ BaseStepMixin v20.0 í‘œì¤€ AI ì¶”ë¡  ë©”ì„œë“œ
        ì‹¤ì œ OOTD/VITON-HD/Diffusion ëª¨ë¸ì„ ì‚¬ìš©í•œ Virtual Fitting
        """
        try:
            start_time = time.time()
            
            # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦
            required_inputs = ['person_image', 'cloth_image']
            for input_key in required_inputs:
                if input_key not in processed_input:
                    raise ValueError(f"í•„ìˆ˜ ì…ë ¥ ë°ì´í„° '{input_key}'ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            person_image = processed_input['person_image']
            cloth_image = processed_input['cloth_image']
            pose_keypoints = processed_input.get('pose_keypoints', None)
            fitting_mode = processed_input.get('fitting_mode', 'single_item')
            quality_level = processed_input.get('quality_level', 'balanced')
            cloth_items = processed_input.get('cloth_items', [])
            
            # 2. Virtual Fitting ì¤€ë¹„ ìƒíƒœ í™•ì¸
            if not self.fitting_ready:
                raise ValueError("Virtual Fitting ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ")
            
            # 3. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            processed_person = self._preprocess_image(person_image)
            processed_cloth = self._preprocess_image(cloth_image)
            
            # 4. AI ëª¨ë¸ ì„ íƒ ë° ì¶”ë¡ 
            fitting_result = self._run_virtual_fitting_inference(
                processed_person, processed_cloth, pose_keypoints, fitting_mode, quality_level, cloth_items
            )
            
            # 5. í›„ì²˜ë¦¬
            final_result = self._postprocess_fitting_result(fitting_result, person_image, cloth_image)
            
            # 6. ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # 7. BaseStepMixin v20.0 í‘œì¤€ ë°˜í™˜ í¬ë§·
            return {
                'success': True,
                'fitted_image': final_result['fitted_image'],
                'fitting_confidence': final_result['fitting_confidence'],
                'fitting_mode': final_result['fitting_mode'],
                'fitting_metrics': final_result['fitting_metrics'],
                'processing_stages': final_result['processing_stages'],
                'recommendations': final_result['recommendations'],
                'alternative_styles': final_result['alternative_styles'],
                'processing_time': processing_time,
                'model_used': final_result['model_used'],
                'quality_level': quality_level,
                'step_name': self.step_name,
                'step_id': self.step_id,
                'central_hub_di_container': True,
                
                # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                'device': self.device,
                'models_loaded': len(self.loaded_models),
                'fitting_ready': self.fitting_ready,
                'auxiliary_processors': {
                    'pose_processor': self.pose_processor is not None,
                    'lighting_adapter': self.lighting_adapter is not None,
                    'texture_enhancer': self.texture_enhancer is not None
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Virtual Fitting AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
            
            return {
                'success': False,
                'error': str(e),
                'processing_time': processing_time,
                'step_name': self.step_name,
                'step_id': self.step_id,
                'central_hub_di_container': True
            }

    def _run_virtual_fitting_inference(
    self, 
    person_image: np.ndarray, 
    cloth_image: np.ndarray, 
    pose_keypoints: Optional[np.ndarray],
    fitting_mode: str,
    quality_level: str,
    cloth_items: List[Dict[str, Any]]
) -> Dict[str, Any]:
        """Virtual Fitting AI ì¶”ë¡  ì‹¤í–‰"""
        try:
            # ğŸ”¥ 1. ê³ ê¸‰ ì˜ë¥˜ ë¶„ì„ ì‹¤í–‰
            cloth_analysis = self.cloth_analyzer.analyze_cloth_properties(cloth_image)
            self.logger.info(f"âœ… ì˜ë¥˜ ë¶„ì„ ì™„ë£Œ: ë³µì¡ë„={cloth_analysis['cloth_complexity']:.3f}")
            
            # ğŸ”¥ 2. TPS ì›Œí•‘ ì „ì²˜ë¦¬ - ë§ˆìŠ¤í¬ ìƒì„±
            person_mask = self._extract_person_mask(person_image)
            cloth_mask = self._extract_cloth_mask(cloth_image)
            
            # ğŸ”¥ 3. TPS ì œì–´ì  ìƒì„± ë° ê³ ê¸‰ ì›Œí•‘ ì ìš©
            source_points, target_points = self.tps_warping.create_control_points(person_mask, cloth_mask)
            tps_warped_clothing = self.tps_warping.apply_tps_transform(cloth_image, source_points, target_points)
            
            self.logger.info(f"âœ… TPS ì›Œí•‘ ì™„ë£Œ: ì œì–´ì  {len(source_points)}ê°œ")
            
            # 4. í’ˆì§ˆ ë ˆë²¨ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
            quality_config = FITTING_QUALITY_LEVELS.get(quality_level, FITTING_QUALITY_LEVELS['balanced'])
            
            # 5. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ìš°ì„ ìˆœìœ„ ê²°ì •
            if 'ootd' in self.loaded_models and 'ootd' in quality_config['models']:
                model = self.ai_models['ootd']
                model_name = 'ootd'
            elif 'viton_hd' in self.loaded_models and 'viton_hd' in quality_config['models']:
                model = self.ai_models['viton_hd']
                model_name = 'viton_hd'
            elif 'diffusion' in self.loaded_models and 'diffusion' in quality_config['models']:
                model = self.ai_models['diffusion']
                model_name = 'diffusion'
            elif 'mock_ootd' in self.loaded_models:
                model = self.ai_models['mock_ootd']
                model_name = 'mock_ootd'
                else:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            
            # ğŸ”¥ 6. ê³ ê¸‰ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (TPS ì›Œí•‘ëœ ì˜ë¥˜ ì‚¬ìš©)
            if hasattr(model, 'predict'):
                # Mock ëª¨ë¸ì¸ ê²½ìš° - TPS ì›Œí•‘ëœ ì˜ë¥˜ ì‚¬ìš©
                result = model.predict(person_image, tps_warped_clothing, pose_keypoints, fitting_mode)
                else:
                # ì‹¤ì œ PyTorch ëª¨ë¸ì¸ ê²½ìš°
                result = self._run_pytorch_virtual_fitting_inference(
                    model, person_image, tps_warped_clothing, pose_keypoints, fitting_mode, model_name, quality_config
                )
            
            # ğŸ”¥ 7. ê³ ê¸‰ í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
            if result.get('fitted_image') is not None:
                quality_metrics = self.quality_assessor.evaluate_fitting_quality(
                    result['fitted_image'], person_image, cloth_image
                )
                result['advanced_quality_metrics'] = quality_metrics
                result['fitting_confidence'] = quality_metrics.get('overall_quality', 0.75)
                
                self.logger.info(f"âœ… ê³ ê¸‰ í’ˆì§ˆ í‰ê°€ ì™„ë£Œ: í’ˆì§ˆì ìˆ˜={quality_metrics.get('overall_quality', 0.75):.3f}")
            
            # ğŸ”¥ 8. ê²°ê³¼ì— ê³ ê¸‰ ê¸°ëŠ¥ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result.update({
                'model_used': model_name,
                'quality_level': quality_level,
                'tps_warping_applied': True,
                'cloth_analysis': cloth_analysis,
                'control_points_count': len(source_points),
                'advanced_ai_processing': True,
                'processing_stages': result.get('processing_stages', []) + [
                    'cloth_analysis',
                    'tps_warping',
                    'advanced_quality_assessment'
                ]
            })
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Virtual Fitting AI ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            # ì‘ê¸‰ ì²˜ë¦¬ - ê¸°ë³¸ ì¶”ë¡ ìœ¼ë¡œ í´ë°±
            return self._create_emergency_fitting_result(person_image, cloth_image, fitting_mode)
        
        
    def _run_pytorch_virtual_fitting_inference(
    self, 
    model, 
    person_image: np.ndarray, 
    cloth_image: np.ndarray, 
    pose_keypoints: Optional[np.ndarray],
    fitting_mode: str,
    model_name: str,
    quality_config: Dict[str, Any]
) -> Dict[str, Any]:
        """ì‹¤ì œ PyTorch Virtual Fitting ëª¨ë¸ ì¶”ë¡ """
        try:
            if not TORCH_AVAILABLE:
                raise ValueError("PyTorchê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
            
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            person_tensor = self._image_to_tensor(person_image)
            cloth_tensor = self._image_to_tensor(cloth_image)
            
            # í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ì²˜ë¦¬ (ìˆëŠ” ê²½ìš°)
            pose_tensor = None
            if pose_keypoints is not None:
                pose_tensor = torch.from_numpy(pose_keypoints).float().to(self.device)
            
            # ëª¨ë¸ë³„ ì¶”ë¡ 
            model.eval()
            with torch.no_grad():
                if 'ootd' in model_name.lower():
                    # OOTD ì¶”ë¡ 
                    fitted_tensor, metrics = self._run_ootd_inference(
                        model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config
                    )
                elif 'viton' in model_name.lower():
                    # VITON-HD ì¶”ë¡ 
                    fitted_tensor, metrics = self._run_viton_hd_inference(
                        model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config
                    )
                elif 'diffusion' in model_name.lower():
                    # Stable Diffusion ì¶”ë¡ 
                    fitted_tensor, metrics = self._run_diffusion_inference(
                        model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config
                    )
                    else:
                    # ê¸°ë³¸ ì¶”ë¡ 
                    fitted_tensor, metrics = self._run_basic_fitting_inference(
                        model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config
                    )
            
            # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
            fitted_image = self._tensor_to_image(fitted_tensor)
            
            # ì¶”ì²œì‚¬í•­ ìƒì„±
            recommendations = self._generate_fitting_recommendations(fitted_image, metrics, fitting_mode)
            
            # ëŒ€ì•ˆ ìŠ¤íƒ€ì¼ ìƒì„±
            alternative_styles = self._generate_alternative_styles(fitted_image, cloth_image, fitting_mode)
            
            return {
                'fitted_image': fitted_image,
                'fitting_confidence': metrics.get('overall_quality', 0.75),
                'fitting_mode': fitting_mode,
                'fitting_metrics': metrics,
                'processing_stages': [f'{model_name}_stage_{i+1}' for i in range(quality_config.get('inference_steps', 30) // 10)],
                'recommendations': recommendations,
                'alternative_styles': alternative_styles,
                'model_type': 'pytorch',
                'model_name': model_name
            }
            
        except Exception as e:
            self.logger.error(f"âŒ PyTorch Virtual Fitting ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self._create_emergency_fitting_result(person_image, cloth_image, fitting_mode)

    def _run_advanced_virtual_fitting_inference(
        self, 
        person_image: np.ndarray, 
        cloth_image: np.ndarray, 
        pose_keypoints: Optional[np.ndarray],
        fitting_mode: str,
        quality_level: str,
        cloth_items: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ğŸ”¥ ê³ ê¸‰ Virtual Fitting AI ì¶”ë¡  ì‹¤í–‰ (TPS + í’ˆì§ˆí‰ê°€ + ì˜ë¥˜ë¶„ì„)"""
        try:
            # ğŸ”¥ 1. ê³ ê¸‰ ì˜ë¥˜ ë¶„ì„ ì‹¤í–‰
            cloth_analysis = self.cloth_analyzer.analyze_cloth_properties(cloth_image)
            self.logger.info(f"âœ… ì˜ë¥˜ ë¶„ì„ ì™„ë£Œ: ë³µì¡ë„={cloth_analysis['cloth_complexity']:.3f}")
            
            # ğŸ”¥ 2. TPS ì›Œí•‘ ì „ì²˜ë¦¬ - ë§ˆìŠ¤í¬ ìƒì„±
            person_mask = self._extract_person_mask(person_image)
            cloth_mask = self._extract_cloth_mask(cloth_image)
            
            # ğŸ”¥ 3. TPS ì œì–´ì  ìƒì„± ë° ê³ ê¸‰ ì›Œí•‘ ì ìš©
            source_points, target_points = self.tps_warping.create_control_points(person_mask, cloth_mask)
            tps_warped_clothing = self.tps_warping.apply_tps_transform(cloth_image, source_points, target_points)
            
            self.logger.info(f"âœ… TPS ì›Œí•‘ ì™„ë£Œ: ì œì–´ì  {len(source_points)}ê°œ")
            
            # 4. í’ˆì§ˆ ë ˆë²¨ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
            quality_config = FITTING_QUALITY_LEVELS.get(quality_level, FITTING_QUALITY_LEVELS['balanced'])
            
            # 5. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ìš°ì„ ìˆœìœ„ ê²°ì •
            if 'ootd' in self.loaded_models and 'ootd' in quality_config['models']:
                model = self.ai_models['ootd']
                model_name = 'ootd'
            elif 'viton_hd' in self.loaded_models and 'viton_hd' in quality_config['models']:
                model = self.ai_models['viton_hd']
                model_name = 'viton_hd'
            elif 'diffusion' in self.loaded_models and 'diffusion' in quality_config['models']:
                model = self.ai_models['diffusion']
                model_name = 'diffusion'
            elif 'mock_ootd' in self.loaded_models:
                model = self.ai_models['mock_ootd']
                model_name = 'mock_ootd'
                else:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            
            # ğŸ”¥ 6. ê³ ê¸‰ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (TPS ì›Œí•‘ëœ ì˜ë¥˜ ì‚¬ìš©)
            if hasattr(model, 'predict'):
                # Mock ëª¨ë¸ì¸ ê²½ìš° - TPS ì›Œí•‘ëœ ì˜ë¥˜ ì‚¬ìš©
                result = model.predict(person_image, tps_warped_clothing, pose_keypoints, fitting_mode)
                else:
                # ì‹¤ì œ PyTorch ëª¨ë¸ì¸ ê²½ìš°
                result = self._run_pytorch_virtual_fitting_inference(
                    model, person_image, tps_warped_clothing, pose_keypoints, fitting_mode, model_name, quality_config
                )
            
            # ğŸ”¥ 7. ê³ ê¸‰ í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
            if result.get('fitted_image') is not None:
                quality_metrics = self.quality_assessor.evaluate_fitting_quality(
                    result['fitted_image'], person_image, cloth_image
                )
                result['advanced_quality_metrics'] = quality_metrics
                result['fitting_confidence'] = quality_metrics.get('overall_quality', 0.75)
                
                self.logger.info(f"âœ… ê³ ê¸‰ í’ˆì§ˆ í‰ê°€ ì™„ë£Œ: í’ˆì§ˆì ìˆ˜={quality_metrics.get('overall_quality', 0.75):.3f}")
            
            # ğŸ”¥ 8. ê²°ê³¼ì— ê³ ê¸‰ ê¸°ëŠ¥ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result.update({
                'model_used': model_name,
                'quality_level': quality_level,
                'tps_warping_applied': True,
                'cloth_analysis': cloth_analysis,
                'control_points_count': len(source_points),
                'advanced_ai_processing': True,
                'processing_stages': result.get('processing_stages', []) + [
                    'cloth_analysis',
                    'tps_warping',
                    'advanced_quality_assessment'
                ]
            })
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ê³ ê¸‰ Virtual Fitting AI ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            # ì‘ê¸‰ ì²˜ë¦¬ - ê¸°ë³¸ ì¶”ë¡ ìœ¼ë¡œ í´ë°±
            return self._create_emergency_fitting_result(person_image, cloth_image, fitting_mode)

    def _extract_person_mask(self, person_image: np.ndarray) -> np.ndarray:
        """ì¸ì²´ ë§ˆìŠ¤í¬ ì¶”ì¶œ (ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜)"""
        try:
            if len(person_image.shape) == 3:
                gray = np.mean(person_image, axis=2)
                else:
                gray = person_image
            
            # ê°„ë‹¨í•œ ì„ê³„ê°’ ì²˜ë¦¬
            threshold = np.mean(gray) + np.std(gray)
            mask = (gray > threshold).astype(np.uint8) * 255
            
            return mask
            
        except Exception:
            h, w = person_image.shape[:2]
            mask = np.ones((h, w), dtype=np.uint8) * 255
            return mask
    
    def _extract_cloth_mask(self, clothing_image: np.ndarray) -> np.ndarray:
        """ì˜ë¥˜ ë§ˆìŠ¤í¬ ì¶”ì¶œ"""
        try:
            if len(clothing_image.shape) == 3:
                gray = np.mean(clothing_image, axis=2)
                else:
                gray = clothing_image
            
            # ê°„ë‹¨í•œ ì„ê³„ê°’ ì²˜ë¦¬
            threshold = np.mean(gray)
            mask = (gray > threshold).astype(np.uint8) * 255
            
            return mask
            
        except Exception:
            h, w = clothing_image.shape[:2]
            mask = np.ones((h, w), dtype=np.uint8) * 255
            return mask

    # ==============================================
    # ğŸ”¥ ëª¨ë¸ë³„ íŠ¹í™” ì¶”ë¡  ë©”ì„œë“œë“¤
    # ==============================================

    def _run_ootd_inference(self, model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config):
        """OOTD (Outfit Of The Day) ëª¨ë¸ ì¶”ë¡ """
        try:
            # OOTD íŠ¹í™” ì „ì²˜ë¦¬
            processed_inputs = self._preprocess_for_ootd(person_tensor, cloth_tensor, pose_tensor, fitting_mode)
            
            # OOTD ì¶”ë¡  ì‹¤í–‰
            if pose_tensor is not None:
                output = model(
                    processed_inputs['person'], 
                    processed_inputs['cloth'], 
                    processed_inputs['pose'],
                    guidance_scale=quality_config.get('guidance_scale', 10.0),
                    num_inference_steps=quality_config.get('inference_steps', 30)
                )
                else:
                output = model(
                    processed_inputs['person'], 
                    processed_inputs['cloth'],
                    guidance_scale=quality_config.get('guidance_scale', 10.0),
                    num_inference_steps=quality_config.get('inference_steps', 30)
                )
            
            # ì¶œë ¥ ì²˜ë¦¬
            if isinstance(output, dict):
                fitted_tensor = output['images']
                metrics = output.get('metrics', {})
                else:
                fitted_tensor = output
                metrics = self._calculate_default_metrics()
            
            return fitted_tensor, metrics
            
        except Exception as e:
            self.logger.error(f"âŒ OOTD ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    def _run_viton_hd_inference(self, model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config):
        """VITON-HD ëª¨ë¸ ì¶”ë¡ """
        try:
            # VITON-HD íŠ¹í™” ì „ì²˜ë¦¬
            processed_inputs = self._preprocess_for_viton_hd(person_tensor, cloth_tensor, pose_tensor, fitting_mode)
            
            # VITON-HD ì¶”ë¡  ì‹¤í–‰
            output = model(
                processed_inputs['person'], 
                processed_inputs['cloth'],
                processed_inputs.get('mask', None),
                processed_inputs.get('pose', None)
            )
            
            # ê³ í’ˆì§ˆ í›„ì²˜ë¦¬ ì ìš©
            if isinstance(output, dict):
                fitted_tensor = output['final_output']
                metrics = {
                    'realism_score': float(output.get('realism_score', 0.85)),
                    'pose_alignment': float(output.get('pose_alignment', 0.8)),
                    'texture_quality': float(output.get('texture_quality', 0.9)),
                    'overall_quality': float(output.get('overall_quality', 0.85))
                }
                else:
                fitted_tensor = output
                metrics = self._calculate_default_metrics()
            
            return fitted_tensor, metrics
            
        except Exception as e:
            self.logger.error(f"âŒ VITON-HD ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    def _run_diffusion_inference(self, model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config):
        """Stable Diffusion ëª¨ë¸ ì¶”ë¡ """
        try:
            if self.diffusion_pipeline is None:
                raise ValueError("Diffusion íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            
            # Diffusion íŠ¹í™” ì „ì²˜ë¦¬
            processed_inputs = self._preprocess_for_diffusion(person_tensor, cloth_tensor, pose_tensor, fitting_mode)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._generate_diffusion_prompt(fitting_mode, cloth_tensor)
            negative_prompt = "blurry, distorted, unrealistic, bad anatomy, bad proportions"
            
            # Stable Diffusion ì¶”ë¡  ì‹¤í–‰
            with torch.autocast(self.device):
                output = self.diffusion_pipeline(
                    image=processed_inputs['person_pil'],
                    mask_image=processed_inputs['mask_pil'],
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    guidance_scale=quality_config.get('guidance_scale', 12.5),
                    num_inference_steps=quality_config.get('inference_steps', 50),
                    strength=0.8
                )
            
            # PILì„ í…ì„œë¡œ ë³€í™˜
            fitted_tensor = self._pil_to_tensor(output.images[0])
            
            # Diffusion ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = {
                'realism_score': 0.9,
                'creativity_score': 0.85,
                'prompt_adherence': 0.88,
                'overall_quality': 0.88
            }
            
            return fitted_tensor, metrics
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    def _run_basic_fitting_inference(self, model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config):
        """ê¸°ë³¸ Virtual Fitting ëª¨ë¸ ì¶”ë¡ """
        try:
            # ê¸°ë³¸ ì¶”ë¡  ì‹¤í–‰
            output = model(person_tensor, cloth_tensor)
            
            if isinstance(output, tuple):
                fitted_tensor, metrics_dict = output
                metrics = {
                    'realism_score': float(metrics_dict.get('realism', 0.75)),
                    'fitting_quality': float(metrics_dict.get('quality', 0.75)),
                    'overall_quality': float(metrics_dict.get('overall', 0.75))
                }
                else:
                fitted_tensor = output
                metrics = self._calculate_default_metrics()
            
            return fitted_tensor, metrics
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ Virtual Fitting ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise

    # ==============================================
    # ğŸ”¥ ì „ì²˜ë¦¬, í›„ì²˜ë¦¬ ë° ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================

    def _preprocess_image(self, image) -> np.ndarray:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # PIL Imageë¥¼ numpy arrayë¡œ ë³€í™˜
            if PIL_AVAILABLE and hasattr(image, 'convert'):
                image_pil = image.convert('RGB')
                image_array = np.array(image_pil)
            elif isinstance(image, np.ndarray):
                image_array = image
                else:
                raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
            
            # í¬ê¸° ì¡°ì •
            target_size = self.config.input_size
            if PIL_AVAILABLE:
                image_pil = Image.fromarray(image_array)
                image_resized = image_pil.resize((target_size[1], target_size[0]), Image.Resampling.LANCZOS)
                image_array = np.array(image_resized)
            
            # ì •ê·œí™” (0-255 ë²”ìœ„ í™•ì¸)
            if image_array.max() <= 1.0:
                image_array = (image_array * 255).astype(np.uint8)
            
            return image_array
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
            return np.zeros((*self.config.input_size, 3), dtype=np.uint8)

    def _postprocess_fitting_result(self, fitting_result: Dict[str, Any], original_person: Any, original_cloth: Any) -> Dict[str, Any]:
        """Virtual Fitting ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            fitted_image = fitting_result['fitted_image']
            
            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³µì›
            if hasattr(original_person, 'size'):
                original_size = original_person.size  # PIL Image
            elif isinstance(original_person, np.ndarray):
                original_size = (original_person.shape[1], original_person.shape[0])  # (width, height)
                else:
                original_size = self.config.input_size
            
            # í¬ê¸° ì¡°ì •
            if PIL_AVAILABLE and fitted_image.shape[:2] != original_size[::-1]:
                fitted_pil = Image.fromarray(fitted_image.astype(np.uint8))
                fitted_resized = fitted_pil.resize(original_size, Image.Resampling.LANCZOS)
                fitted_image = np.array(fitted_resized)
            
            # í’ˆì§ˆ í–¥ìƒ í›„ì²˜ë¦¬ ì ìš©
            if self.config.enable_texture_preservation:
                fitted_image = self._enhance_texture_quality(fitted_image)
            
            if self.config.enable_lighting_adaptation:
                fitted_image = self._adapt_lighting(fitted_image, original_person)
            
            return {
                'fitted_image': fitted_image,
                'fitting_confidence': fitting_result.get('fitting_confidence', 0.75),
                'fitting_mode': fitting_result.get('fitting_mode', 'single_item'),
                'fitting_metrics': fitting_result.get('fitting_metrics', {}),
                'processing_stages': fitting_result.get('processing_stages', []),
                'recommendations': fitting_result.get('recommendations', []),
                'alternative_styles': fitting_result.get('alternative_styles', []),
                'model_used': fitting_result.get('model_used', 'unknown')
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Virtual Fitting ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'fitted_image': fitting_result.get('fitted_image', original_person),
                'fitting_confidence': 0.5,
                'fitting_mode': 'error',
                'fitting_metrics': {},
                'processing_stages': [],
                'recommendations': [],
                'alternative_styles': [],
                'model_used': 'error'
            }

    def _generate_fitting_recommendations(self, fitted_image: np.ndarray, metrics: Dict[str, float], fitting_mode: str) -> List[str]:
        """í”¼íŒ… ì¶”ì²œì‚¬í•­ ìƒì„±"""
        try:
            recommendations = []
            
            # í’ˆì§ˆ ê¸°ë°˜ ì¶”ì²œ
            overall_quality = metrics.get('overall_quality', 0.75)
            if overall_quality >= 0.9:
                recommendations.append("Excellent fit! This outfit looks great on you.")
            elif overall_quality >= 0.8:
                recommendations.append("Great fit! Consider this style for special occasions.")
            elif overall_quality >= 0.7:
                recommendations.append("Good fit! This style suits you well.")
                else:
                recommendations.append("The fit could be improved. Try adjusting the pose or lighting.")
            
            # í”¼íŒ… ëª¨ë“œë³„ ì¶”ì²œ
            if fitting_mode == 'upper_body':
                recommendations.append("Consider pairing with complementary bottoms.")
            elif fitting_mode == 'lower_body':
                recommendations.append("This would work well with various tops.")
            elif fitting_mode == 'full_outfit':
                recommendations.append("Complete outfit styling achieved!")
            
            # ë©”íŠ¸ë¦­ ê¸°ë°˜ êµ¬ì²´ì  ì¶”ì²œ
            pose_alignment = metrics.get('pose_alignment', 0.8)
            if pose_alignment < 0.7:
                recommendations.append("Try standing straighter for better fit visualization.")
            
            texture_quality = metrics.get('texture_quality', 0.75)
            if texture_quality < 0.7:
                recommendations.append("Better lighting could improve the texture appearance.")
            
            return recommendations
            
        except Exception as e:
            self.logger.error(f"âŒ í”¼íŒ… ì¶”ì²œì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["Virtual fitting completed successfully!"]

    def _generate_alternative_styles(self, fitted_image: np.ndarray, cloth_image: np.ndarray, fitting_mode: str) -> List[Dict[str, Any]]:
        """ëŒ€ì•ˆ ìŠ¤íƒ€ì¼ ìƒì„±"""
        try:
            alternatives = []
            
            # ê¸°ë³¸ ìŠ¤íƒ€ì¼ ëŒ€ì•ˆë“¤
            style_options = ['casual', 'formal', 'sporty', 'trendy', 'classic']
            
            for style in style_options[:3]:  # ìƒìœ„ 3ê°œë§Œ
                confidence = 0.7 + (hash(style) % 20) / 100  # Mock ì‹ ë¢°ë„
                alternatives.append({
                    'style': style,
                    'confidence': confidence,
                    'description': f"Try this {style} approach for a different look",
                    'recommended': confidence > 0.8
                })
            
            return alternatives
            
        except Exception as e:
            self.logger.error(f"âŒ ëŒ€ì•ˆ ìŠ¤íƒ€ì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return []

    def _enhance_texture_quality(self, image: np.ndarray) -> np.ndarray:
        """í…ìŠ¤ì²˜ í’ˆì§ˆ í–¥ìƒ"""
        try:
            if self.texture_enhancer:
                return self.texture_enhancer.enhance(image)
            
            # ê¸°ë³¸ í…ìŠ¤ì²˜ í–¥ìƒ (ìƒ¤í”„ë‹)
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            enhanced = cv2.filter2D(image, -1, kernel)
            
            # ì›ë³¸ê³¼ ë¸”ë Œë”©
            alpha = 0.3
            result = cv2.addWeighted(image, 1-alpha, enhanced, alpha, 0)
            
            return result.astype(np.uint8)
            
        except Exception as e:
            self.logger.error(f"âŒ í…ìŠ¤ì²˜ í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image

    def _adapt_lighting(self, fitted_image: np.ndarray, reference_image: np.ndarray) -> np.ndarray:
        """ì¡°ëª… ì ì‘"""
        try:
            if self.lighting_adapter:
                return self.lighting_adapter.adapt(fitted_image, reference_image)
            
            # ê¸°ë³¸ ì¡°ëª… ì ì‘ (íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­)
            if isinstance(reference_image, np.ndarray):
                # ê°„ë‹¨í•œ ë°ê¸° ì¡°ì •
                ref_mean = np.mean(reference_image)
                fitted_mean = np.mean(fitted_image)
                
                if fitted_mean > 0:
                    brightness_ratio = ref_mean / fitted_mean
                    brightness_ratio = np.clip(brightness_ratio, 0.5, 2.0)
                    
                    adapted = fitted_image * brightness_ratio
                    adapted = np.clip(adapted, 0, 255).astype(np.uint8)
                    
                    return adapted
            
            return fitted_image
            
        except Exception as e:
            self.logger.error(f"âŒ ì¡°ëª… ì ì‘ ì‹¤íŒ¨: {e}")
            return fitted_image

    def _image_to_tensor(self, image: np.ndarray) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜"""
        try:
            if len(image.shape) == 3:
                # (H, W, C) -> (C, H, W)
                tensor = torch.from_numpy(image).permute(2, 0, 1).float()
                else:
                tensor = torch.from_numpy(image).float()
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            if len(tensor.shape) == 3:
                tensor = tensor.unsqueeze(0)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            tensor = tensor.to(self.device)
            
            # ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
            if tensor.max() > 1.0:
                tensor = tensor / 255.0
            
            return tensor
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise

    def _tensor_to_image(self, tensor: torch.Tensor) -> np.ndarray:
        """PyTorch í…ì„œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            # CPUë¡œ ì´ë™
            tensor = tensor.cpu()
            
            # ë°°ì¹˜ ì°¨ì› ì œê±°
            if len(tensor.shape) == 4:
                tensor = tensor.squeeze(0)
            
            # (C, H, W) -> (H, W, C)
            if len(tensor.shape) == 3:
                tensor = tensor.permute(1, 2, 0)
            
            # numpy ë³€í™˜
            image = tensor.numpy()
            
            # 0-255 ë²”ìœ„ë¡œ ë³€í™˜
            if image.max() <= 1.0:
                image = (image * 255).astype(np.uint8)
                else:
                image = np.clip(image, 0, 255).astype(np.uint8)
            
            return image
            
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise

    def _create_emergency_fitting_result(self, person_image: np.ndarray, cloth_image: np.ndarray, fitting_mode: str) -> Dict[str, Any]:
        """ì‘ê¸‰ Virtual Fitting ê²°ê³¼ ìƒì„±"""
        try:
            # ê¸°ë³¸ì ì¸ ì˜¤ë²„ë ˆì´ ì ìš©
            h, w = person_image.shape[:2]
            
            if fitting_mode == 'upper_body':
                cloth_resized = cv2.resize(cloth_image, (w//2, h//3))
                overlay_region = (h//6, h//2, w//4, 3*w//4)
            elif fitting_mode == 'lower_body':
                cloth_resized = cv2.resize(cloth_image, (w//3, h//2))
                overlay_region = (h//2, h, w//3, 2*w//3)
                else:
                cloth_resized = cv2.resize(cloth_image, (w//2, 2*h//3))
                overlay_region = (h//6, 5*h//6, w//4, 3*w//4)
            
            result = person_image.copy()
            start_y, end_y, start_x, end_x = overlay_region
            
            if end_y <= h and end_x <= w:
                # ì•ŒíŒŒ ë¸”ë Œë”©
                alpha = 0.6
                overlay_h = min(cloth_resized.shape[0], end_y - start_y)
                overlay_w = min(cloth_resized.shape[1], end_x - start_x)
                
                result[start_y:start_y+overlay_h, start_x:start_x+overlay_w] = (
                    alpha * cloth_resized[:overlay_h, :overlay_w] + 
                    (1 - alpha) * result[start_y:start_y+overlay_h, start_x:start_x+overlay_w]
                ).astype(np.uint8)
            
            return {
                'fitted_image': result,
                'fitting_confidence': 0.6,
                'fitting_mode': fitting_mode,
                'fitting_metrics': {
                    'realism_score': 0.6,
                    'pose_alignment': 0.65,
                    'overall_quality': 0.6
                },
                'processing_stages': ['emergency_overlay'],
                'recommendations': ['Emergency fitting applied', 'Use higher quality models for better results'],
                'alternative_styles': [],
                'model_type': 'emergency',
                'model_name': 'emergency_fallback'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì‘ê¸‰ Virtual Fitting ê²°ê³¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'fitted_image': person_image,
                'fitting_confidence': 0.0,
                'fitting_mode': fitting_mode,
                'fitting_metrics': {},
                'processing_stages': [],
                'recommendations': [],
                'alternative_styles': [],
                'model_type': 'error',
                'model_name': 'error'
            }

    # ==============================================
    # ğŸ”¥ Step ìš”êµ¬ì‚¬í•­ ë° ë³´ì¡° í”„ë¡œì„¸ì„œë“¤
    # ==============================================

    def _get_step_requirements(self) -> Dict[str, Any]:
        """Step 06 Virtual Fitting ìš”êµ¬ì‚¬í•­ ë°˜í™˜ (BaseStepMixin í˜¸í™˜)"""
        return {
            "required_models": [
                "ootd_diffusion.pth",
                "viton_hd_final.pth",
                "stable_diffusion_inpainting.pth"
            ],
            "primary_model": "ootd_diffusion.pth",
            "model_configs": {
                "ootd_diffusion.pth": {
                    "size_mb": 3276.8,
                    "device_compatible": ["cpu", "mps", "cuda"],
                    "precision": "high"
                },
                "viton_hd_final.pth": {
                    "size_mb": 2147.5,
                    "device_compatible": ["cpu", "mps", "cuda"],
                    "real_time": False
                },
                "stable_diffusion_inpainting.pth": {
                    "size_mb": 4835.2,
                    "device_compatible": ["cpu", "mps", "cuda"],
                    "quality": "ultra"
                }
            },
            "verified_paths": [
                "step_06_virtual_fitting/ootd_diffusion.pth",
                "step_06_virtual_fitting/viton_hd_final.pth",
                "step_06_virtual_fitting/stable_diffusion_inpainting.pth"
            ]
        }

    def _create_pose_processor(self):
        """í¬ì¦ˆ í”„ë¡œì„¸ì„œ ìƒì„±"""
        try:
            class PoseProcessor:
                def __init__(self):
                    self.device = "cpu"
                
                def process_keypoints(self, keypoints: np.ndarray) -> Dict[str, Any]:
                    if keypoints is not None:
                        return {
                            'processed': True,
                            'keypoints': keypoints,
                            'confidence': 0.8
                        }
                    return {'processed': False}
            
            return PoseProcessor()
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í¬ì¦ˆ í”„ë¡œì„¸ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _create_lighting_adapter(self):
        """ì¡°ëª… ì ì‘ í”„ë¡œì„¸ì„œ ìƒì„±"""
        try:
            class LightingAdapter:
                def adapt(self, image: np.ndarray, reference: np.ndarray) -> np.ndarray:
                    # ê°„ë‹¨í•œ íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­
                    return image
            
            return LightingAdapter()
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¡°ëª… ì ì‘ í”„ë¡œì„¸ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _create_texture_enhancer(self):
        """í…ìŠ¤ì²˜ í–¥ìƒ í”„ë¡œì„¸ì„œ ìƒì„±"""
        try:
            class TextureEnhancer:
                def enhance(self, image: np.ndarray) -> np.ndarray:
                    # ê°„ë‹¨í•œ ìƒ¤í”„ë‹
                    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
                    enhanced = cv2.filter2D(image, -1, kernel)
                    return cv2.addWeighted(image, 0.7, enhanced, 0.3, 0)
            
            return TextureEnhancer()
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í…ìŠ¤ì²˜ í–¥ìƒ í”„ë¡œì„¸ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _create_mock_pose_processor(self):
        """Mock í¬ì¦ˆ í”„ë¡œì„¸ì„œ ìƒì„±"""
        return self._create_pose_processor()

    def _create_mock_lighting_adapter(self):
        """Mock ì¡°ëª… ì ì‘ í”„ë¡œì„¸ì„œ ìƒì„±"""
        return self._create_lighting_adapter()

    def _create_mock_texture_enhancer(self):
        """Mock í…ìŠ¤ì²˜ í–¥ìƒ í”„ë¡œì„¸ì„œ ìƒì„±"""
        return self._create_texture_enhancer()


# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_virtual_fitting_step(**kwargs) -> VirtualFittingStep:
    """VirtualFittingStep ìƒì„± í•¨ìˆ˜"""
    return VirtualFittingStep(**kwargs)

def quick_virtual_fitting(person_image, clothing_image, 
                         fabric_type: str = "cotton", 
                         clothing_type: str = "shirt",
                         **kwargs) -> Dict[str, Any]:
    """ë¹ ë¥¸ ê°€ìƒ í”¼íŒ… ì‹¤í–‰"""
    try:
        step = create_virtual_fitting_step(**kwargs)
        
        # AI ì¶”ë¡  ì‹¤í–‰ (BaseStepMixin v20.0 í‘œì¤€)
        result = step._run_ai_inference({
            'person_image': person_image,
            'clothing_image': clothing_image,
            'fabric_type': fabric_type,
            'clothing_type': clothing_type,
            **kwargs
        })
        
        return result
        
    except Exception as e:
        return {
            'success': False,
            'error': f'ë¹ ë¥¸ ê°€ìƒ í”¼íŒ… ì‹¤íŒ¨: {e}'
        }

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
# ==============================================

__all__ = [
    'VirtualFittingStep',
    'VirtualFittingConfig',
    'FITTING_MODES',
    'FITTING_QUALITY_LEVELS',
    'CLOTHING_ITEM_TYPES',
    'create_virtual_fitting_step',
    'quick_virtual_fitting'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë¡œê·¸
# ==============================================

logger = logging.getLogger(__name__)
logger.info("=" * 100)
logger.info("ğŸ”¥ VirtualFittingStep v8.0 - Central Hub DI Container ì™„ì „ ì—°ë™")
logger.info("=" * 100)
logger.info("âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™:")
logger.info("   ğŸ”— BaseStepMixin ìƒì† ë° super().__init__() í˜¸ì¶œ")
logger.info("   ğŸ”— í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™”: ai_models, models_loading_status, model_interface, loaded_models")
logger.info("   ğŸ”— _load_virtual_fitting_models_via_central_hub() ë©”ì„œë“œ - ModelLoader ì—°ë™")
logger.info("   ğŸ”— ê°„ì†Œí™”ëœ ì•„í‚¤í…ì²˜ (ë³µì¡í•œ DI ë¡œì§ ì œê±°)")
logger.info("   ğŸ”— ì—ëŸ¬ ë°©ì§€ìš© í´ë°± ë¡œì§ - Mock ëª¨ë¸ ìƒì„±")
logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì§€ì›:")
logger.info("   ğŸ§  OOTD (Outfit Of The Day) ëª¨ë¸ - 3.2GB")
logger.info("   ğŸ§  VITON-HD ëª¨ë¸ - 2.1GB (ê³ í’ˆì§ˆ Virtual Try-On)")
logger.info("   ğŸ§  Stable Diffusion ëª¨ë¸ - 4.8GB (ê³ ê¸‰ ì´ë¯¸ì§€ ìƒì„±)")
logger.info("   ğŸ”„ Mock ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ")
logger.info("âœ… BaseStepMixin v20.0 í‘œì¤€ ì¤€ìˆ˜:")
logger.info("   ğŸ¯ _run_ai_inference() ë©”ì„œë“œ êµ¬í˜„")
logger.info("   ğŸ¯ í‘œì¤€í™”ëœ ì…ì¶œë ¥ í¬ë§·")
logger.info("   ğŸ¯ Central Hub ì˜ì¡´ì„± ìë™ ì£¼ì…")
logger.info("âœ… ì™„ì „ ì œê±°ëœ ê²ƒë“¤:")
logger.info("   âŒ GitHubDependencyManager - ì™„ì „ ì‚­ì œ")
logger.info("   âŒ ë³µì¡í•œ DI ì´ˆê¸°í™” ë¡œì§ - ë‹¨ìˆœí™”")
logger.info("   âŒ ìˆœí™˜ì°¸ì¡° ë°©ì§€ ì½”ë“œ - ë¶ˆí•„ìš”")
logger.info("   âŒ TYPE_CHECKING ë³µì¡í•œ import - ë‹¨ìˆœí™”")
logger.info("âœ… í•µì‹¬ ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ ì™„ì „ êµ¬í˜„:")
logger.info("   ğŸ§  TPS (Thin Plate Spline) ì›Œí•‘ ì•Œê³ ë¦¬ì¦˜ - ì •ë°€í•œ ì˜ë¥˜ ë³€í˜•")
logger.info("   ğŸ” ê³ ê¸‰ ì˜ë¥˜ ë¶„ì„ ì‹œìŠ¤í…œ - ìƒ‰ìƒ/í…ìŠ¤ì²˜/íŒ¨í„´ ë¶„ì„")  
logger.info("   âš–ï¸ AI í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ - SSIM ê¸°ë°˜ êµ¬ì¡°ì  í‰ê°€")
logger.info("   ğŸ¨ ì‹¤ì‹œê°„ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬")
logger.info("   ğŸ¯ ë‹¤ì¤‘ ì˜ë¥˜ ì•„ì´í…œ ë™ì‹œ í”¼íŒ…")
logger.info("   ğŸ“ ì •ë°€í•œ ì œì–´ì  ê¸°ë°˜ ê¸°í•˜í•™ì  ë³€í™˜")
logger.info("   ğŸ”¬ FFT ê¸°ë°˜ íŒ¨í„´ ê°ì§€ ì•Œê³ ë¦¬ì¦˜")
logger.info("   ğŸ“Š ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚° ê¸°ë°˜ ì„ ëª…ë„ í‰ê°€")
logger.info("   ğŸ›ï¸ ë°”ì´ë¦¬ë‹ˆì–´ ë³´ê°„ ì›Œí•‘ ì—”ì§„")
logger.info("   ğŸ§® K-means ìƒ‰ìƒ í´ëŸ¬ìŠ¤í„°ë§")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   - PyTorch: {TORCH_AVAILABLE}")
logger.info(f"   - PIL: {PIL_AVAILABLE}")
logger.info(f"   - Diffusers: {DIFFUSERS_AVAILABLE}")
logger.info("=" * 100)
logger.info("ğŸ‰ VirtualFittingStep v8.0 Central Hub DI Container ì™„ì „ ì—°ë™ ì™„ë£Œ!")
logger.info("ğŸ’ª ì‹¤ì œ OOTD + VITON-HD + Diffusion ì²´í¬í¬ì¸íŠ¸ ì§€ì›!")
logger.info("ğŸ”¥ ê°„ì†Œí™”ëœ ì•„í‚¤í…ì²˜ë¡œ ë†’ì€ ì„±ëŠ¥ê³¼ ì•ˆì •ì„± ë³´ì¥!")
logger.info("=" * 100)

# ==============================================
# ğŸ”¥ í…ŒìŠ¤íŠ¸ ì½”ë“œ (ê°œë°œìš©)
# ==============================================

if __name__ == "__main__":
    def test_virtual_fitting_step():
        """VirtualFittingStep í…ŒìŠ¤íŠ¸"""
        print("ğŸ”¥ VirtualFittingStep v8.0 Central Hub DI Container í…ŒìŠ¤íŠ¸")
        print("=" * 80)
        
        try:
            # Step ìƒì„±
            step = create_virtual_fitting_step(device="auto")
            
            # ìƒíƒœ í™•ì¸
            print(f"âœ… Step ì´ë¦„: {step.step_name}")
            print(f"âœ… Step ID: {step.step_id}")
            print(f"âœ… ë””ë°”ì´ìŠ¤: {step.device}")
            print(f"âœ… í”¼íŒ… ì¤€ë¹„: {step.fitting_ready}")
            print(f"âœ… ë¡œë”©ëœ ëª¨ë¸: {len(step.loaded_models)}ê°œ")
            print(f"âœ… ëª¨ë¸ ëª©ë¡: {step.loaded_models}")
            
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
            test_person = np.random.randint(0, 255, (768, 1024, 3), dtype=np.uint8)
            test_clothing = np.random.randint(0, 255, (768, 1024, 3), dtype=np.uint8)
            
            print("ğŸ§  AI ì¶”ë¡  í…ŒìŠ¤íŠ¸...")
            
            # AI ì¶”ë¡  ì‹¤í–‰
            result = step._run_ai_inference({
                'person_image': test_person,
                'cloth_image': test_clothing,
                'fitting_mode': 'single_item',
                'quality_level': 'balanced'
            })
            
            if result['success']:
                print(f"âœ… AI ì¶”ë¡  ì„±ê³µ!")
                print(f"   - ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
                print(f"   - í”¼íŒ… ì‹ ë¢°ë„: {result['fitting_confidence']:.3f}")
                print(f"   - ì‚¬ìš© ëª¨ë¸: {result['model_used']}")
                print(f"   - í”¼íŒ… ëª¨ë“œ: {result['fitting_mode']}")
                print(f"   - í’ˆì§ˆ ë ˆë²¨: {result['quality_level']}")
                print(f"   - ì¶œë ¥ í¬ê¸°: {result['fitted_image'].shape}")
                print(f"   - ì¶”ì²œì‚¬í•­: {len(result['recommendations'])}ê°œ")
                print(f"   - ëŒ€ì•ˆ ìŠ¤íƒ€ì¼: {len(result['alternative_styles'])}ê°œ")
                else:
                print(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
            
            print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    print("=" * 100)
    print("ğŸ¯ VirtualFittingStep v8.0 - Central Hub DI Container ì™„ì „ ì—°ë™")
    print("=" * 100)
    
    test_virtual_fitting_step()
    
    print("\n" + "=" * 100)
    print("ğŸ‰ VirtualFittingStep v8.0 Central Hub DI Container ì™„ì „ ì—°ë™ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("âœ… BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜")
    print("âœ… ì‹¤ì œ OOTD + VITON-HD + Diffusion ì²´í¬í¬ì¸íŠ¸ ì§€ì›")
    print("âœ… _run_ai_inference() ë©”ì„œë“œ í‘œì¤€ êµ¬í˜„")
    print("âœ… TPS (Thin Plate Spline) ì›Œí•‘ ì•Œê³ ë¦¬ì¦˜")
    print("âœ… ê³ ê¸‰ ì˜ë¥˜ ë¶„ì„ ì‹œìŠ¤í…œ (ìƒ‰ìƒ/í…ìŠ¤ì²˜/íŒ¨í„´)")
    print("âœ… AI í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ (SSIM ê¸°ë°˜)")
    print("âœ… FFT ê¸°ë°˜ íŒ¨í„´ ê°ì§€")
    print("âœ… ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚° ì„ ëª…ë„ í‰ê°€")
    print("âœ… ë°”ì´ë¦¬ë‹ˆì–´ ë³´ê°„ ì›Œí•‘ ì—”ì§„")
    print("âœ… K-means ìƒ‰ìƒ í´ëŸ¬ìŠ¤í„°ë§")
    print("=" * 100)