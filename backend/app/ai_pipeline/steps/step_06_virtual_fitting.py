"""
6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… (Virtual Fitting) - Pipeline Manager ì™„ì „ í˜¸í™˜ ë²„ì „
M3 Max 128GB í™˜ê²½ ìµœì í™” ë° í†µí•© ì¸í„°í˜ì´ìŠ¤
"""
import os
import time
import asyncio
import logging
from typing import Dict, Any, Optional, Union, List, Tuple
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
from PIL import Image, ImageEnhance, ImageFilter
import base64
import io

# ì„ íƒì  import (ì—†ì–´ë„ ì‘ë™)
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False
    mp = None

try:
    from scipy.spatial.distance import cdist
    from scipy.interpolate import Rbf
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# ê¸°ì¡´ app êµ¬ì¡° import (ì•ˆì „í•˜ê²Œ)
try:
    from app.core.config import get_settings
    settings = get_settings()
except ImportError:
    settings = None

try:
    from app.utils.image_utils import save_temp_image, load_image
except ImportError:
    save_temp_image = None
    load_image = None

try:
    from app.ai_pipeline.utils.memory_manager import optimize_memory_usage
except ImportError:
    def optimize_memory_usage():
        pass

logger = logging.getLogger(__name__)

class VirtualFittingStep:
    """
    6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… - Pipeline Manager ì™„ì „ í˜¸í™˜ ë²„ì „
    
    Pipeline Managerê°€ ìš”êµ¬í•˜ëŠ” í‘œì¤€ ì¸í„°í˜ì´ìŠ¤:
    - __init__(device: str, config: Dict[str, Any])
    - async initialize() -> bool
    - process(...) -> Dict[str, Any]
    - async cleanup()
    """
    
    def __init__(self, device: str = "mps", config: Dict[str, Any] = None):
        """
        Pipeline Manager í˜¸í™˜ ì´ˆê¸°í™”
        
        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ("mps", "cuda", "cpu")
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.device = self._validate_device(device)
        self.config = config or {}
        
        # ê°€ìƒ í”¼íŒ… ì„¤ì •
        self.fitting_config = self.config.get('virtual_fitting', {
            'composition_method': 'neural_blend',  # neural_blend, traditional_blend, advanced_blend
            'quality_level': 'high',  # fast, medium, high
            'enable_pose_guidance': True,
            'enable_texture_enhancement': True,
            'blend_strength': 0.8,
            'edge_smoothing': True
        })
        
        # M3 Max ìµœì í™” ì„¤ì •
        self.optimization_config = {
            'use_mps': self.device == 'mps',
            'memory_efficient': True,
            'batch_size': 1,
            'fp16_enabled': self.device == 'mps',  # M3 Maxì—ì„œ fp16 í™œìš©
            'enable_caching': True
        }
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤
        self.pose_analyzer = None
        self.composition_engine = None
        self.quality_enhancer = None
        self.texture_processor = None
        
        # ìƒíƒœ ë³€ìˆ˜
        self.is_initialized = False
        self.initialization_error = None
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'total_processed': 0,
            'average_time': 0.0,
            'success_rate': 1.0,
            'average_quality': 0.85
        }
        
        logger.info(f"ğŸ¯ VirtualFittingStep ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _validate_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ìœ íš¨ì„± ê²€ì‚¬ ë° ìµœì í™”"""
        if device == 'mps' and torch.backends.mps.is_available():
            logger.info("âœ… Apple Silicon MPS ë°±ì—”ë“œ í™œì„±í™”")
            return 'mps'
        elif device == 'cuda' and torch.cuda.is_available():
            logger.info("âœ… CUDA ë°±ì—”ë“œ í™œì„±í™”")
            return 'cuda'
        else:
            logger.info("âš ï¸ CPU ë°±ì—”ë“œ ì‚¬ìš©")
            return 'cpu'
    
    async def initialize(self) -> bool:
        """
        ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        Pipeline Managerê°€ í˜¸ì¶œí•˜ëŠ” í‘œì¤€ ë©”ì„œë“œ
        """
        try:
            logger.info("ğŸ”„ ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. í¬ì¦ˆ ë¶„ì„ê¸° ì´ˆê¸°í™”
            self.pose_analyzer = PoseAnalyzer(
                device=self.device,
                enabled=self.fitting_config['enable_pose_guidance']
            )
            await self.pose_analyzer.initialize()
            
            # 2. í•©ì„± ì—”ì§„ ì´ˆê¸°í™”
            self.composition_engine = CompositionEngine(
                device=self.device,
                method=self.fitting_config['composition_method'],
                quality_level=self.fitting_config['quality_level']
            )
            await self.composition_engine.initialize()
            
            # 3. í’ˆì§ˆ í–¥ìƒê¸° ì´ˆê¸°í™”
            self.quality_enhancer = QualityEnhancer(
                device=self.device,
                enable_texture=self.fitting_config['enable_texture_enhancement']
            )
            
            # 4. í…ìŠ¤ì²˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
            self.texture_processor = TextureProcessor(
                device=self.device,
                optimization_level=self.optimization_config
            )
            
            # 5. ë©”ëª¨ë¦¬ ìµœì í™”
            self._optimize_memory()
            
            self.is_initialized = True
            logger.info("âœ… ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            error_msg = f"ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"
            logger.error(f"âŒ {error_msg}")
            self.initialization_error = error_msg
            self.is_initialized = False
            return False
    
    def process(
        self,
        person_image: torch.Tensor,
        warped_clothing: torch.Tensor,
        clothing_mask: torch.Tensor,
        pose_keypoints: Optional[Dict[str, Any]] = None,
        user_preferences: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ - Pipeline Manager í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤
        
        Args:
            person_image: ì¸ë¬¼ ì´ë¯¸ì§€ í…ì„œ [1, 3, H, W]
            warped_clothing: ë³€í˜•ëœ ì˜ë¥˜ ì´ë¯¸ì§€ í…ì„œ [1, 3, H, W]  
            clothing_mask: ì˜ë¥˜ ë§ˆìŠ¤í¬ í…ì„œ [1, 1, H, W]
            pose_keypoints: í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ì •ë³´ (ì„ íƒì )
            user_preferences: ì‚¬ìš©ì ì„ í˜¸ë„ (ì„ íƒì )
        
        Returns:
            Pipeline Manager í˜¸í™˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_initialized:
            return self._create_error_result(
                f"ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ: {self.initialization_error}"
            )
        
        start_time = time.time()
        
        try:
            logger.info("ğŸ¨ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹œì‘...")
            
            # 1. ì…ë ¥ ê²€ì¦ ë° ì „ì²˜ë¦¬
            person_np = self._tensor_to_numpy(person_image)
            clothing_np = self._tensor_to_numpy(warped_clothing)
            mask_np = self._tensor_to_numpy(clothing_mask, is_mask=True)
            
            # 2. í¬ì¦ˆ ê¸°ë°˜ ê°€ì´ë˜ìŠ¤ ìƒì„±
            pose_guidance = self._generate_pose_guidance(
                person_np, pose_keypoints
            )
            
            # 3. ë©”ì¸ í•©ì„± ì²˜ë¦¬
            composition_result = self._perform_composition(
                person_np, clothing_np, mask_np, pose_guidance
            )
            
            # 4. í’ˆì§ˆ í–¥ìƒ í›„ì²˜ë¦¬
            enhanced_result = self._enhance_quality(
                composition_result, person_np, clothing_np
            )
            
            # 5. ìµœì¢… ê²°ê³¼ ê²€ì¦ ë° í’ˆì§ˆ í‰ê°€
            quality_metrics = self._evaluate_result_quality(
                enhanced_result, person_np, clothing_np
            )
            
            processing_time = time.time() - start_time
            
            # 6. Pipeline Manager í˜¸í™˜ ê²°ê³¼ êµ¬ì„±
            result = self._build_pipeline_result(
                enhanced_result, quality_metrics, processing_time,
                user_preferences
            )
            
            # 7. ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            self._update_performance_stats(processing_time, quality_metrics['overall_quality'])
            
            logger.info(f"âœ… ê°€ìƒ í”¼íŒ… ì™„ë£Œ - {processing_time:.3f}ì´ˆ, í’ˆì§ˆ: {quality_metrics['overall_quality']:.3f}")
            return result
            
        except Exception as e:
            error_msg = f"ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
            logger.error(f"âŒ {error_msg}")
            processing_time = time.time() - start_time
            return self._create_error_result(error_msg, processing_time)
    
    def _generate_pose_guidance(
        self, 
        person_image: np.ndarray, 
        pose_keypoints: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """í¬ì¦ˆ ê¸°ë°˜ ê°€ì´ë˜ìŠ¤ ìƒì„±"""
        
        if not self.fitting_config['enable_pose_guidance'] or not self.pose_analyzer:
            return {'enabled': False}
        
        try:
            if pose_keypoints:
                # ê¸°ì¡´ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ì‚¬ìš©
                guidance = self.pose_analyzer.process_existing_keypoints(pose_keypoints)
            else:
                # ìƒˆë¡œìš´ í¬ì¦ˆ ë¶„ì„
                guidance = self.pose_analyzer.analyze_pose(person_image)
            
            return {
                'enabled': True,
                'body_regions': guidance.get('body_regions', {}),
                'attention_map': guidance.get('attention_map'),
                'pose_confidence': guidance.get('confidence', 0.8)
            }
            
        except Exception as e:
            logger.warning(f"í¬ì¦ˆ ê°€ì´ë˜ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'enabled': False, 'error': str(e)}
    
    def _perform_composition(
        self,
        person_image: np.ndarray,
        clothing_image: np.ndarray,
        clothing_mask: np.ndarray,
        pose_guidance: Dict[str, Any]
    ) -> np.ndarray:
        """ë©”ì¸ í•©ì„± ì²˜ë¦¬"""
        
        method = self.fitting_config['composition_method']
        
        if method == 'neural_blend' and self.composition_engine.neural_compositor:
            return self._neural_composition(
                person_image, clothing_image, clothing_mask, pose_guidance
            )
        elif method == 'advanced_blend':
            return self._advanced_composition(
                person_image, clothing_image, clothing_mask, pose_guidance
            )
        else:
            return self._traditional_composition(
                person_image, clothing_image, clothing_mask, pose_guidance
            )
    
    def _neural_composition(
        self,
        person_image: np.ndarray,
        clothing_image: np.ndarray,
        clothing_mask: np.ndarray,
        pose_guidance: Dict[str, Any]
    ) -> np.ndarray:
        """ì‹ ê²½ë§ ê¸°ë°˜ í•©ì„±"""
        
        try:
            # í…ì„œë¡œ ë³€í™˜
            person_tensor = self._numpy_to_tensor(person_image)
            clothing_tensor = self._numpy_to_tensor(clothing_image)
            mask_tensor = self._numpy_to_tensor(clothing_mask, is_mask=True)
            
            # ì‹ ê²½ë§ í•©ì„± ì‹¤í–‰
            with torch.no_grad():
                if pose_guidance.get('enabled'):
                    attention_map = pose_guidance.get('attention_map')
                    if attention_map is not None:
                        attention_tensor = self._numpy_to_tensor(attention_map, is_mask=True)
                        result_tensor = self.composition_engine.compose_with_attention(
                            person_tensor, clothing_tensor, mask_tensor, attention_tensor
                        )
                    else:
                        result_tensor = self.composition_engine.compose(
                            person_tensor, clothing_tensor, mask_tensor
                        )
                else:
                    result_tensor = self.composition_engine.compose(
                        person_tensor, clothing_tensor, mask_tensor
                    )
            
            return self._tensor_to_numpy(result_tensor)
            
        except Exception as e:
            logger.warning(f"ì‹ ê²½ë§ í•©ì„± ì‹¤íŒ¨, ì „í†µì  ë°©ë²• ì‚¬ìš©: {e}")
            return self._traditional_composition(
                person_image, clothing_image, clothing_mask, pose_guidance
            )
    
    def _advanced_composition(
        self,
        person_image: np.ndarray,
        clothing_image: np.ndarray,
        clothing_mask: np.ndarray,
        pose_guidance: Dict[str, Any]
    ) -> np.ndarray:
        """ê³ ê¸‰ í•©ì„± (í¬ì•„ì†¡ ë¸”ë Œë”© + ê·¸ë¼ë””ì–¸íŠ¸ ë„ë©”ì¸)"""
        
        try:
            # 1. í¬ì¦ˆ ê°€ì´ë˜ìŠ¤ë¥¼ ì´ìš©í•œ ë¸”ë Œë”© ì˜ì—­ ì •ì œ
            if pose_guidance.get('enabled'):
                refined_mask = self._refine_mask_with_pose(
                    clothing_mask, pose_guidance
                )
            else:
                refined_mask = clothing_mask
            
            # 2. ë©€í‹° ë ˆë²¨ ë¸”ë Œë”©
            result = self._multi_level_blending(
                person_image, clothing_image, refined_mask
            )
            
            # 3. ê·¸ë¼ë””ì–¸íŠ¸ ë„ë©”ì¸ ìµœì í™”
            if self.fitting_config['quality_level'] == 'high':
                result = self._gradient_domain_optimization(result, person_image)
            
            return result
            
        except Exception as e:
            logger.warning(f"ê³ ê¸‰ í•©ì„± ì‹¤íŒ¨, ì „í†µì  ë°©ë²• ì‚¬ìš©: {e}")
            return self._traditional_composition(
                person_image, clothing_image, clothing_mask, pose_guidance
            )
    
    def _traditional_composition(
        self,
        person_image: np.ndarray,
        clothing_image: np.ndarray,
        clothing_mask: np.ndarray,
        pose_guidance: Dict[str, Any]
    ) -> np.ndarray:
        """ì „í†µì  ì•ŒíŒŒ ë¸”ë Œë”© í•©ì„±"""
        
        try:
            # ë§ˆìŠ¤í¬ ì •ê·œí™” ë° ë¶€ë“œëŸ½ê²Œ ì²˜ë¦¬
            mask_float = clothing_mask.astype(np.float32) / 255.0
            
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ê²½ê³„ ë¶€ë“œëŸ½ê²Œ
            blur_kernel_size = 15 if self.fitting_config['edge_smoothing'] else 5
            for i in range(3):
                mask_float = cv2.GaussianBlur(mask_float, (blur_kernel_size, blur_kernel_size), 3)
            
            # 3ì±„ë„ë¡œ í™•ì¥
            if len(mask_float.shape) == 2:
                mask_float = np.stack([mask_float] * 3, axis=2)
            
            # ë¸”ë Œë”© ê°•ë„ ì ìš©
            blend_strength = self.fitting_config['blend_strength']
            mask_float = mask_float * blend_strength
            
            # ì•ŒíŒŒ ë¸”ë Œë”©
            person_float = person_image.astype(np.float32)
            clothing_float = clothing_image.astype(np.float32)
            
            blended = person_float * (1 - mask_float) + clothing_float * mask_float
            
            return np.clip(blended, 0, 255).astype(np.uint8)
            
        except Exception as e:
            logger.error(f"ì „í†µì  í•©ì„± ì‹¤íŒ¨: {e}")
            return person_image  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    def _enhance_quality(
        self,
        composed_image: np.ndarray,
        person_image: np.ndarray,
        clothing_image: np.ndarray
    ) -> np.ndarray:
        """í’ˆì§ˆ í–¥ìƒ í›„ì²˜ë¦¬"""
        
        if not self.quality_enhancer:
            return composed_image
        
        try:
            enhanced = self.quality_enhancer.enhance(composed_image, person_image)
            
            if self.fitting_config['enable_texture_enhancement']:
                enhanced = self.texture_processor.enhance_texture(enhanced, clothing_image)
            
            return enhanced
            
        except Exception as e:
            logger.warning(f"í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return composed_image
    
    def _evaluate_result_quality(
        self,
        result_image: np.ndarray,
        person_image: np.ndarray,
        clothing_image: np.ndarray
    ) -> Dict[str, float]:
        """ê²°ê³¼ í’ˆì§ˆ í‰ê°€"""
        
        try:
            metrics = {}
            
            # 1. êµ¬ì¡°ì  ìœ ì‚¬ë„ (SSIM ê°„ì†Œí™” ë²„ì „)
            metrics['structural_similarity'] = self._calculate_simple_ssim(result_image, person_image)
            
            # 2. ìƒ‰ìƒ ì¼ê´€ì„±
            metrics['color_consistency'] = self._evaluate_color_harmony(result_image, person_image)
            
            # 3. ê²½ê³„ ìì—°ìŠ¤ëŸ¬ì›€
            metrics['edge_naturalness'] = self._evaluate_edge_quality(result_image)
            
            # 4. ì˜ë¥˜ ë³´ì¡´ë„
            metrics['clothing_preservation'] = self._evaluate_clothing_preservation(result_image, clothing_image)
            
            # 5. ì „ì²´ í’ˆì§ˆ (ê°€ì¤‘ í‰ê· )
            metrics['overall_quality'] = (
                metrics['structural_similarity'] * 0.3 +
                metrics['color_consistency'] * 0.25 +
                metrics['edge_naturalness'] * 0.25 +
                metrics['clothing_preservation'] * 0.2
            )
            
            return metrics
            
        except Exception as e:
            logger.warning(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'overall_quality': 0.75,
                'structural_similarity': 0.8,
                'color_consistency': 0.7,
                'edge_naturalness': 0.8,
                'clothing_preservation': 0.7
            }
    
    def _build_pipeline_result(
        self,
        fitted_image: np.ndarray,
        quality_metrics: Dict[str, float],
        processing_time: float,
        user_preferences: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """Pipeline Manager í˜¸í™˜ ê²°ê³¼ êµ¬ì„±"""
        
        # í…ì„œë¡œ ë³€í™˜
        fitted_tensor = self._numpy_to_tensor(fitted_image)
        
        return {
            "success": True,
            "fitted_image": fitted_tensor,
            "fitted_image_numpy": fitted_image,
            "fitted_image_pil": Image.fromarray(cv2.cvtColor(fitted_image, cv2.COLOR_BGR2RGB)),
            "quality_metrics": quality_metrics,
            "confidence": quality_metrics['overall_quality'],
            "fitting_info": {
                "composition_method": self.fitting_config['composition_method'],
                "quality_level": self.fitting_config['quality_level'],
                "processing_time": processing_time,
                "device": self.device,
                "pose_guidance_used": self.fitting_config['enable_pose_guidance'],
                "texture_enhancement_used": self.fitting_config['enable_texture_enhancement'],
                "optimization": "M3_Max_MPS" if self.device == 'mps' else self.device.upper()
            },
            "recommendations": self._generate_recommendations(quality_metrics, user_preferences)
        }
    
    def _generate_recommendations(
        self, 
        quality_metrics: Dict[str, float], 
        user_preferences: Optional[Dict] = None
    ) -> List[str]:
        """ì‚¬ìš©ì ì¶”ì²œ ìƒì„±"""
        
        recommendations = []
        
        overall_quality = quality_metrics.get('overall_quality', 0.75)
        edge_quality = quality_metrics.get('edge_naturalness', 0.75)
        color_consistency = quality_metrics.get('color_consistency', 0.75)
        
        if overall_quality > 0.85:
            recommendations.append("ì™„ë²½í•œ í•ì…ë‹ˆë‹¤! ì´ ìŠ¤íƒ€ì¼ì´ ë‹¹ì‹ ì—ê²Œ ì˜ ì–´ìš¸ë ¤ìš”.")
        elif overall_quality > 0.7:
            recommendations.append("ì¢‹ì€ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ ìŠ¤íƒ€ì¼ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
        else:
            recommendations.append("ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•´ ì •ë©´ì„ í–¥í•œ ì „ì‹  ì‚¬ì§„ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
        
        if edge_quality < 0.7:
            recommendations.append("ë” ìì—°ìŠ¤ëŸ¬ìš´ ê²½ê³„ë¥¼ ìœ„í•´ ì¡°ëª…ì´ ê· ì¼í•œ í™˜ê²½ì—ì„œ ì´¬ì˜í•´ë³´ì„¸ìš”.")
        
        if color_consistency < 0.7:
            recommendations.append("ìƒ‰ìƒ ì¡°í™”ë¥¼ ìœ„í•´ ë¹„ìŠ·í•œ í†¤ì˜ ì˜ë¥˜ë¥¼ ì„ íƒí•´ë³´ì„¸ìš”.")
        
        return recommendations[:3]  # ìµœëŒ€ 3ê°œ
    
    def _create_error_result(self, error_message: str, processing_time: float = 0.0) -> Dict[str, Any]:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            "success": False,
            "error": error_message,
            "fitted_image": None,
            "fitted_image_numpy": None,
            "fitted_image_pil": None,
            "quality_metrics": {},
            "confidence": 0.0,
            "fitting_info": {
                "processing_time": processing_time,
                "device": self.device,
                "error_details": error_message
            },
            "recommendations": ["ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."]
        }
    
    # =================================================================
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # =================================================================
    
    def _tensor_to_numpy(self, tensor: torch.Tensor, is_mask: bool = False) -> np.ndarray:
        """í…ì„œë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜"""
        try:
            # GPUì—ì„œ CPUë¡œ ì´ë™
            if tensor.is_cuda or (hasattr(tensor, 'is_mps') and tensor.is_mps):
                tensor = tensor.cpu()
            
            # ë°°ì¹˜ ì°¨ì› ì œê±°
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            
            if is_mask:
                if tensor.dim() == 3:
                    tensor = tensor.squeeze(0)
                array = tensor.numpy().astype(np.uint8)
                if array.max() <= 1.0:
                    array = array * 255
            else:
                if tensor.dim() == 3 and tensor.size(0) == 3:
                    tensor = tensor.permute(1, 2, 0)
                array = tensor.numpy()
                if array.max() <= 1.0:
                    array = array * 255
                array = array.astype(np.uint8)
            
            return array
            
        except Exception as e:
            logger.error(f"í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    def _numpy_to_tensor(self, array: np.ndarray, is_mask: bool = False) -> torch.Tensor:
        """NumPy ë°°ì—´ì„ í…ì„œë¡œ ë³€í™˜"""
        try:
            if is_mask:
                if len(array.shape) == 2:
                    array = array[np.newaxis, :]
                tensor = torch.from_numpy(array.astype(np.float32) / 255.0)
                tensor = tensor.unsqueeze(0)
            else:
                if len(array.shape) == 3 and array.shape[2] == 3:
                    array = array.transpose(2, 0, 1)
                tensor = torch.from_numpy(array.astype(np.float32) / 255.0)
                tensor = tensor.unsqueeze(0)
            
            return tensor.to(self.device)
            
        except Exception as e:
            logger.warning(f"í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return torch.zeros(1, 3, 256, 256).to(self.device)
    
    def _optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        optimize_memory_usage()
        
        if self.device == 'mps':
            try:
                torch.mps.empty_cache()
            except:
                pass
        elif self.device == 'cuda':
            torch.cuda.empty_cache()
    
    def _calculate_simple_ssim(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """ê°„ì†Œí™”ëœ SSIM ê³„ì‚°"""
        try:
            gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY) if len(img1.shape) == 3 else img1
            gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY) if len(img2.shape) == 3 else img2
            
            # ê°„ì†Œí™”ëœ SSIM êµ¬í˜„
            mu1 = cv2.GaussianBlur(gray1.astype(np.float32), (11, 11), 1.5)
            mu2 = cv2.GaussianBlur(gray2.astype(np.float32), (11, 11), 1.5)
            
            mu1_sq = mu1 * mu1
            mu2_sq = mu2 * mu2
            mu1_mu2 = mu1 * mu2
            
            C1 = (0.01 * 255) ** 2
            numerator = 2 * mu1_mu2 + C1
            denominator = mu1_sq + mu2_sq + C1
            
            ssim_map = numerator / denominator
            return float(np.mean(ssim_map))
            
        except Exception as e:
            logger.warning(f"SSIM ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.8
    
    def _evaluate_color_harmony(self, result: np.ndarray, reference: np.ndarray) -> float:
        """ìƒ‰ìƒ ì¡°í™”ë„ í‰ê°€"""
        try:
            result_mean = np.mean(result, axis=(0, 1))
            ref_mean = np.mean(reference, axis=(0, 1))
            
            color_diff = np.linalg.norm(result_mean - ref_mean)
            harmony = max(0.0, 1.0 - color_diff / 255.0)
            
            return harmony
        except:
            return 0.75
    
    def _evaluate_edge_quality(self, image: np.ndarray) -> float:
        """ê²½ê³„ í’ˆì§ˆ í‰ê°€"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            
            # ê²½ê³„ì˜ ì—°ì†ì„± í‰ê°€
            edge_density = np.sum(edges > 0) / edges.size
            quality = min(1.0, edge_density * 5)  # ì •ê·œí™”
            
            return quality
        except:
            return 0.75
    
    def _evaluate_clothing_preservation(self, result: np.ndarray, clothing: np.ndarray) -> float:
        """ì˜ë¥˜ íŠ¹ì„± ë³´ì¡´ë„ í‰ê°€"""
        try:
            # ìƒ‰ìƒ ë¶„í¬ ë¹„êµ
            result_hist = cv2.calcHist([result], [0, 1, 2], None, [50, 50, 50], [0, 256, 0, 256, 0, 256])
            clothing_hist = cv2.calcHist([clothing], [0, 1, 2], None, [50, 50, 50], [0, 256, 0, 256, 0, 256])
            
            correlation = cv2.compareHist(result_hist, clothing_hist, cv2.HISTCMP_CORREL)
            return max(0.0, correlation)
        except:
            return 0.7
    
    def _refine_mask_with_pose(self, mask: np.ndarray, pose_guidance: Dict) -> np.ndarray:
        """í¬ì¦ˆ ê°€ì´ë˜ìŠ¤ë¡œ ë§ˆìŠ¤í¬ ì •ì œ"""
        # ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”
        return mask
    
    def _multi_level_blending(self, person: np.ndarray, clothing: np.ndarray, mask: np.ndarray) -> np.ndarray:
        """ë©€í‹° ë ˆë²¨ ë¸”ë Œë”©"""
        # ê°„ì†Œí™”ëœ êµ¬í˜„
        mask_float = mask.astype(np.float32) / 255.0
        if len(mask_float.shape) == 2:
            mask_float = np.stack([mask_float] * 3, axis=2)
        
        return (person.astype(np.float32) * (1 - mask_float) + 
                clothing.astype(np.float32) * mask_float).astype(np.uint8)
    
    def _gradient_domain_optimization(self, image: np.ndarray, reference: np.ndarray) -> np.ndarray:
        """ê·¸ë¼ë””ì–¸íŠ¸ ë„ë©”ì¸ ìµœì í™”"""
        # ê°„ì†Œí™”ëœ êµ¬í˜„
        return image
    
    def _update_performance_stats(self, processing_time: float, quality: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.performance_stats['total_processed'] += 1
            total = self.performance_stats['total_processed']
            
            # í‰ê·  ì‹œê°„ ì—…ë°ì´íŠ¸
            current_avg = self.performance_stats['average_time']
            self.performance_stats['average_time'] = (current_avg * (total - 1) + processing_time) / total
            
            # í‰ê·  í’ˆì§ˆ ì—…ë°ì´íŠ¸
            current_quality = self.performance_stats['average_quality']
            self.performance_stats['average_quality'] = (current_quality * (total - 1) + quality) / total
            
            # ì„±ê³µë¥  ì—…ë°ì´íŠ¸
            if quality > 0.5:
                success_count = int(self.performance_stats['success_rate'] * (total - 1)) + 1
                self.performance_stats['success_rate'] = success_count / total
            
        except Exception as e:
            logger.warning(f"í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    async def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜ - Pipeline Manager í˜¸í™˜"""
        return {
            "step_name": "VirtualFitting",
            "version": "3.0",
            "device": self.device,
            "initialized": self.is_initialized,
            "initialization_error": self.initialization_error,
            "config": self.fitting_config,
            "optimization": self.optimization_config,
            "performance_stats": self.performance_stats,
            "capabilities": {
                "neural_composition": bool(self.composition_engine),
                "pose_guidance": bool(self.pose_analyzer),
                "quality_enhancement": bool(self.quality_enhancer),
                "texture_processing": bool(self.texture_processor)
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - Pipeline Manager í˜¸í™˜"""
        try:
            logger.info("ğŸ§¹ ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬...")
            
            if self.pose_analyzer:
                await self.pose_analyzer.cleanup()
                self.pose_analyzer = None
            
            if self.composition_engine:
                await self.composition_engine.cleanup()
                self.composition_engine = None
            
            if self.quality_enhancer:
                del self.quality_enhancer
                self.quality_enhancer = None
            
            if self.texture_processor:
                del self.texture_processor
                self.texture_processor = None
            
            self._optimize_memory()
            self.is_initialized = False
            
            logger.info("âœ… ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


# =================================================================
# ë³´ì¡° í´ë˜ìŠ¤ë“¤
# =================================================================

class PoseAnalyzer:
    """í¬ì¦ˆ ë¶„ì„ê¸°"""
    
    def __init__(self, device: str = 'cpu', enabled: bool = True):
        self.device = device
        self.enabled = enabled and MEDIAPIPE_AVAILABLE
        self.pose_model = None
    
    async def initialize(self) -> bool:
        if not self.enabled:
            return True
        
        try:
            self.pose_model = mp.solutions.pose.Pose(
                static_image_mode=True,
                model_complexity=1,
                enable_segmentation=False,
                min_detection_confidence=0.5
            )
            return True
        except Exception as e:
            logger.warning(f"í¬ì¦ˆ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.enabled = False
            return False
    
    def analyze_pose(self, image: np.ndarray) -> Dict[str, Any]:
        if not self.enabled:
            return {'confidence': 0.5}
        
        try:
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            results = self.pose_model.process(rgb_image)
            
            if results.pose_landmarks:
                return {
                    'confidence': 0.8,
                    'body_regions': self._extract_body_regions(results.pose_landmarks),
                    'attention_map': self._generate_attention_map(image.shape[:2], results.pose_landmarks)
                }
            else:
                return {'confidence': 0.3}
        except Exception as e:
            logger.warning(f"í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'confidence': 0.5}
    
    def process_existing_keypoints(self, keypoints: Dict) -> Dict[str, Any]:
        return {'confidence': 0.8, 'body_regions': keypoints}
    
    def _extract_body_regions(self, landmarks) -> Dict[str, Any]:
        return {'torso': True, 'arms': True, 'legs': True}
    
    def _generate_attention_map(self, shape: Tuple[int, int], landmarks) -> np.ndarray:
        h, w = shape
        attention = np.ones((h, w), dtype=np.float32) * 0.5
        return attention
    
    async def cleanup(self):
        if self.pose_model:
            self.pose_model.close()


class CompositionEngine:
    """í•©ì„± ì—”ì§„"""
    
    def __init__(self, device: str = 'cpu', method: str = 'neural_blend', quality_level: str = 'medium'):
        self.device = device
        self.method = method
        self.quality_level = quality_level
        self.neural_compositor = None
    
    async def initialize(self) -> bool:
        try:
            if self.method == 'neural_blend':
                self.neural_compositor = SimpleNeuralCompositor(self.device)
                await self.neural_compositor.initialize()
            return True
        except Exception as e:
            logger.warning(f"í•©ì„± ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def compose(self, person: torch.Tensor, clothing: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        if self.neural_compositor:
            return self.neural_compositor.compose(person, clothing, mask)
        else:
            # ê¸°ë³¸ ë¸”ë Œë”©
            return person * (1 - mask) + clothing * mask
    
    def compose_with_attention(self, person: torch.Tensor, clothing: torch.Tensor, 
                             mask: torch.Tensor, attention: torch.Tensor) -> torch.Tensor:
        combined_mask = mask * attention
        return self.compose(person, clothing, combined_mask)
    
    async def cleanup(self):
        if self.neural_compositor:
            del self.neural_compositor
            self.neural_compositor = None


class SimpleNeuralCompositor(nn.Module):
    """ê°„ë‹¨í•œ ì‹ ê²½ë§ í•©ì„±ê¸°"""
    
    def __init__(self, device: str = 'cpu'):
        super().__init__()
        self.device = device
        
        # ê°„ë‹¨í•œ CNN ë ˆì´ì–´ë“¤
        self.conv1 = nn.Conv2d(7, 32, 3, padding=1)  # person(3) + clothing(3) + mask(1)
        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, 3, 3, padding=1)
        self.relu = nn.ReLU(inplace=True)
        self.sigmoid = nn.Sigmoid()
    
    async def initialize(self):
        self.to(self.device)
        self.eval()
    
    def compose(self, person: torch.Tensor, clothing: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        try:
            # ì…ë ¥ ê²°í•©
            inputs = torch.cat([person, clothing, mask], dim=1)
            
            # CNN ì²˜ë¦¬
            x = self.relu(self.conv1(inputs))
            x = self.relu(self.conv2(x))
            x = self.sigmoid(self.conv3(x))
            
            return x
        except Exception as e:
            logger.warning(f"ì‹ ê²½ë§ í•©ì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°„ë‹¨í•œ ë¸”ë Œë”©
            return person * (1 - mask) + clothing * mask


class QualityEnhancer:
    """í’ˆì§ˆ í–¥ìƒê¸°"""
    
    def __init__(self, device: str = 'cpu', enable_texture: bool = True):
        self.device = device
        self.enable_texture = enable_texture
    
    def enhance(self, image: np.ndarray, reference: np.ndarray) -> np.ndarray:
        try:
            # ê¸°ë³¸ ë…¸ì´ì¦ˆ ì œê±°
            enhanced = cv2.bilateralFilter(image, 9, 75, 75)
            
            # ì„ ëª…í™”
            kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]]) * 0.1
            enhanced = cv2.filter2D(enhanced, -1, kernel)
            
            return enhanced
        except Exception as e:
            logger.warning(f"í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image


class TextureProcessor:
    """í…ìŠ¤ì²˜ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, device: str = 'cpu', optimization_level: Dict = None):
        self.device = device
        self.optimization_level = optimization_level or {}
    
    def enhance_texture(self, image: np.ndarray, clothing_reference: np.ndarray) -> np.ndarray:
        try:
            # ê°„ë‹¨í•œ í…ìŠ¤ì²˜ í–¥ìƒ
            return cv2.addWeighted(image, 0.9, clothing_reference, 0.1, 0)
        except Exception as e:
            logger.warning(f"í…ìŠ¤ì²˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image