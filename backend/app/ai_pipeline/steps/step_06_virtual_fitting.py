# app/ai_pipeline/steps/step_06_virtual_fitting.py
"""
ğŸ”¥ 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… (Virtual Fitting) - ì™„ì „í•œ DIíŒ¨í„´ + StepFactory ê¸°ë°˜
====================================================================================

âœ… StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… â†’ ì™„ì„±ëœ Step
âœ… ì™„ì „í•œ ì²˜ë¦¬ íë¦„:
   1. StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì…
   2. ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„± â†’ ê°€ì¤‘ì¹˜ ë¡œë”©
   3. í‚¤í¬ì¸íŠ¸ ê²€ì¶œ â†’ TPS ë³€í˜• ê³„ì‚° â†’ ê¸°í•˜í•™ì  ë³€í˜• ì ìš©
   4. í’ˆì§ˆ í‰ê°€ â†’ ì‹œê°í™” ìƒì„± â†’ API ì‘ë‹µ
âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… BaseStepMixin ìƒì† + VirtualFittingMixin íŠ¹í™”
âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  (OOTDiffusion, IDM-VTON)
âœ… M3 Max 128GB ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

Author: MyCloset AI Team
Date: 2025-07-22
Version: 6.3.0 (Complete DI Pattern + StepFactory Based)
"""

import os
import gc
import time
import logging
import asyncio
import traceback
import threading
import math
import uuid
import json
import base64
import weakref
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from functools import wraps
from io import BytesIO

# ==============================================
# ğŸ”¥ 1. TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
# ==============================================

if TYPE_CHECKING:
    # íƒ€ì… ì²´í‚¹ ì‹œì—ë§Œ import (ëŸ°íƒ€ì„ì—ëŠ” import ì•ˆë¨)
    from ..utils.model_loader import ModelLoader, IModelLoader
    from ..steps.base_step_mixin import BaseStepMixin, VirtualFittingMixin
    from ..factories.step_factory import StepFactory, StepFactoryResult

# ==============================================
# ğŸ”¥ 2. ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import
# ==============================================

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
from PIL import Image, ImageEnhance, ImageFilter, ImageDraw

# PyTorch ì•ˆì „ Import (M3 Max ìµœì í™”)
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torchvision.transforms as transforms
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        
except ImportError:
    TORCH_AVAILABLE = False

# OpenCV ì•ˆì „ Import
CV2_AVAILABLE = False
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

# ê³¼í•™ ì—°ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬
SCIPY_AVAILABLE = False
SKLEARN_AVAILABLE = False
DIFFUSERS_AVAILABLE = False

try:
    from scipy.interpolate import griddata, Rbf
    from scipy.spatial.distance import cdist
    from scipy.ndimage import gaussian_filter
    SCIPY_AVAILABLE = True
except ImportError:
    pass

try:
    from sklearn.cluster import KMeans
    SKLEARN_AVAILABLE = True
except ImportError:
    pass

try:
    from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDIMScheduler
    from transformers import CLIPProcessor, CLIPModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    pass

# ==============================================
# ğŸ”¥ 3. ë™ì  import í•¨ìˆ˜ë“¤ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def get_step_factory_dynamic():
    """StepFactory ë™ì  ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        factory_module = importlib.import_module('app.ai_pipeline.factories.step_factory')
        get_global_factory = getattr(factory_module, 'get_global_step_factory', None)
        if get_global_factory:
            return get_global_factory()
        
        StepFactoryClass = getattr(factory_module, 'StepFactory', None)
        if StepFactoryClass:
            return StepFactoryClass()
        return None
    except Exception as e:
        logging.getLogger(__name__).debug(f"StepFactory ë™ì  ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def get_virtual_fitting_mixin_dynamic():
    """VirtualFittingMixin ë™ì  ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        mixin_module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        VirtualFittingMixinClass = getattr(mixin_module, 'VirtualFittingMixin', None)
        if VirtualFittingMixinClass:
            return VirtualFittingMixinClass
        
        BaseStepMixinClass = getattr(mixin_module, 'BaseStepMixin', None)
        return BaseStepMixinClass
    except Exception as e:
        logging.getLogger(__name__).debug(f"VirtualFittingMixin ë™ì  ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ 4. ë©”ëª¨ë¦¬ ë° GPU ê´€ë¦¬
# ==============================================

def safe_memory_cleanup():
    """ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        results = []
        
        # Python GC
        before = len(gc.get_objects())
        gc.collect()
        after = len(gc.get_objects())
        results.append(f"Python GC: {before - after}ê°œ ê°ì²´ í•´ì œ")
        
        # GPU ë©”ëª¨ë¦¬
        if TORCH_AVAILABLE:
            if MPS_AVAILABLE:
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                    results.append("MPS ìºì‹œ ì •ë¦¬")
                except:
                    pass
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
                results.append("CUDA ìºì‹œ ì •ë¦¬")
        
        return {"success": True, "results": results}
    except Exception as e:
        return {"success": False, "error": str(e)}

# ==============================================
# ğŸ”¥ 5. í‚¤í¬ì¸íŠ¸ ë° TPS ë³€í˜• ìœ í‹¸ë¦¬í‹°
# ==============================================

class TPSTransform:
    """Thin Plate Spline ë³€í˜• êµ¬í˜„"""
    
    def __init__(self):
        self.source_points = None
        self.target_points = None
        self.weights = None
        self.affine_params = None
    
    def fit(self, source_points: np.ndarray, target_points: np.ndarray):
        """TPS ë³€í˜• ê³„ì‚°"""
        try:
            if not SCIPY_AVAILABLE:
                return False
                
            self.source_points = source_points
            self.target_points = target_points
            
            n = source_points.shape[0]
            
            # TPS ê¸°ë³¸ í•¨ìˆ˜ í–‰ë ¬ ìƒì„±
            K = self._compute_basis_matrix(source_points)
            P = np.hstack([np.ones((n, 1)), source_points])
            
            # ì‹œìŠ¤í…œ í–‰ë ¬ êµ¬ì„±
            A = np.vstack([
                np.hstack([K, P]),
                np.hstack([P.T, np.zeros((3, 3))])
            ])
            
            # íƒ€ê²Ÿ ë²¡í„°
            b_x = np.hstack([target_points[:, 0], np.zeros(3)])
            b_y = np.hstack([target_points[:, 1], np.zeros(3)])
            
            # ìµœì†Œì œê³±ë²•ìœ¼ë¡œ í•´ê²°
            params_x = np.linalg.lstsq(A, b_x, rcond=None)[0]
            params_y = np.linalg.lstsq(A, b_y, rcond=None)[0]
            
            # ê°€ì¤‘ì¹˜ì™€ ì•„í•€ íŒŒë¼ë¯¸í„° ë¶„ë¦¬
            self.weights = np.column_stack([params_x[:n], params_y[:n]])
            self.affine_params = np.column_stack([params_x[n:], params_y[n:]])
            
            return True
            
        except Exception as e:
            logging.error(f"TPS fit ì‹¤íŒ¨: {e}")
            return False
    
    def _compute_basis_matrix(self, points: np.ndarray) -> np.ndarray:
        """TPS ê¸°ë³¸ í•¨ìˆ˜ í–‰ë ¬ ê³„ì‚°"""
        n = points.shape[0]
        K = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                if i != j:
                    r = np.linalg.norm(points[i] - points[j])
                    if r > 0:
                        K[i, j] = r * r * np.log(r)
                        
        return K
    
    def transform(self, points: np.ndarray) -> np.ndarray:
        """í¬ì¸íŠ¸ë“¤ì„ TPS ë³€í˜• ì ìš©"""
        try:
            if self.weights is None or self.affine_params is None:
                return points
                
            n_source = self.source_points.shape[0]
            n_points = points.shape[0]
            
            # ì•„í•€ ë³€í˜•
            result = np.column_stack([
                np.ones(n_points),
                points
            ]) @ self.affine_params
            
            # ë¹„ì„ í˜• ë³€í˜• (TPS)
            for i in range(n_source):
                distances = np.linalg.norm(points - self.source_points[i], axis=1)
                valid_mask = distances > 0
                
                if np.any(valid_mask):
                    basis_values = np.zeros(n_points)
                    basis_values[valid_mask] = (distances[valid_mask] ** 2) * np.log(distances[valid_mask])
                    
                    result[:, 0] += basis_values * self.weights[i, 0]
                    result[:, 1] += basis_values * self.weights[i, 1]
            
            return result
            
        except Exception as e:
            logging.error(f"TPS transform ì‹¤íŒ¨: {e}")
            return points

def extract_keypoints_from_pose_data(pose_data: Dict[str, Any]) -> Optional[np.ndarray]:
    """í¬ì¦ˆ ë°ì´í„°ì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
    try:
        if not pose_data:
            return None
            
        # ë‹¤ì–‘í•œ í¬ì¦ˆ ë°ì´í„° í˜•ì‹ ì§€ì›
        if 'keypoints' in pose_data:
            keypoints = pose_data['keypoints']
        elif 'poses' in pose_data and pose_data['poses']:
            keypoints = pose_data['poses'][0].get('keypoints', [])
        elif 'landmarks' in pose_data:
            keypoints = pose_data['landmarks']
        else:
            return None
        
        # í‚¤í¬ì¸íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        if isinstance(keypoints, list):
            keypoints = np.array(keypoints)
        
        # í˜•íƒœ ê²€ì¦ ë° ì¡°ì •
        if len(keypoints.shape) == 1:
            # í‰ë©´ ë°°ì—´ì¸ ê²½ìš° (x, y, confidence, x, y, confidence, ...)
            keypoints = keypoints.reshape(-1, 3)
        
        # x, y ì¢Œí‘œë§Œ ì¶”ì¶œ
        if keypoints.shape[1] >= 2:
            return keypoints[:, :2]
        
        return None
        
    except Exception as e:
        logging.error(f"í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None

def detect_body_keypoints(image: np.ndarray) -> Optional[np.ndarray]:
    """ì´ë¯¸ì§€ì—ì„œ ì‹ ì²´ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ (í´ë°±)"""
    try:
        if not CV2_AVAILABLE:
            return None
            
        # ê°„ë‹¨í•œ íŠ¹ì§•ì  ê²€ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• ì‚¬ìš©)
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image
        
        # ì½”ë„ˆ ê²€ì¶œ
        corners = cv2.goodFeaturesToTrack(
            gray,
            maxCorners=18,  # OpenPose í‚¤í¬ì¸íŠ¸ ìˆ˜
            qualityLevel=0.01,
            minDistance=10
        )
        
        if corners is not None:
            keypoints = corners.reshape(-1, 2)
            
            # 18ê°œ í‚¤í¬ì¸íŠ¸ë¡œ ë§ì¶”ê¸°
            if len(keypoints) < 18:
                # ë¶€ì¡±í•œ í‚¤í¬ì¸íŠ¸ëŠ” ë³´ê°„ìœ¼ë¡œ ì±„ì›€
                needed = 18 - len(keypoints)
                for _ in range(needed):
                    if len(keypoints) > 1:
                        # ê¸°ì¡´ í‚¤í¬ì¸íŠ¸ë“¤ì˜ í‰ê·  ì£¼ë³€ì— ì¶”ê°€
                        center = np.mean(keypoints, axis=0)
                        noise = np.random.normal(0, 10, 2)
                        new_point = center + noise
                        keypoints = np.vstack([keypoints, new_point])
                    else:
                        # ì´ë¯¸ì§€ ì¤‘ì‹¬ì— ì¶”ê°€
                        center = np.array([image.shape[1]//2, image.shape[0]//2])
                        keypoints = np.vstack([keypoints, center])
            elif len(keypoints) > 18:
                # ë„ˆë¬´ ë§ìœ¼ë©´ ì²˜ìŒ 18ê°œë§Œ ì‚¬ìš©
                keypoints = keypoints[:18]
            
            return keypoints
        
        return None
        
    except Exception as e:
        logging.error(f"í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ 6. ì‹¤ì œ AI ëª¨ë¸ ë˜í¼ë“¤
# ==============================================

class OOTDiffusionWrapper:
    """ì‹¤ì œ OOTDiffusion ê°€ìƒ í”¼íŒ… ëª¨ë¸"""
    
    def __init__(self, model_path: str, device: str = "cpu"):
        self.model_path = model_path
        self.device = device
        self.name = "OOTDiffusion_Real"
        self.model = None
        self.scheduler = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.OOTDiffusion")
        
    def load_model(self) -> bool:
        """ì‹¤ì œ ëª¨ë¸ ë¡œë“œ"""
        try:
            if not TORCH_AVAILABLE or not DIFFUSERS_AVAILABLE:
                return False
                
            self.logger.info(f"OOTDiffusion ë¡œë“œ ì¤‘: {self.model_path}")
            
            # UNet ëª¨ë¸ ë¡œë“œ
            self.model = UNet2DConditionModel.from_pretrained(
                self.model_path,
                torch_dtype=torch.float32 if self.device == "cpu" else torch.float16,
                use_safetensors=True,
                local_files_only=True
            )
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
            try:
                self.scheduler = DDIMScheduler.from_pretrained(
                    self.model_path,
                    subfolder="scheduler"
                )
            except:
                # ê¸°ë³¸ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
                self.scheduler = DDIMScheduler(
                    num_train_timesteps=1000,
                    beta_start=0.00085,
                    beta_end=0.012,
                    beta_schedule="scaled_linear",
                    clip_sample=False
                )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model = self.model.to(self.device)
            self.model.eval()
            
            self.loaded = True
            self.logger.info(f"âœ… OOTDiffusion ë¡œë“œ ì™„ë£Œ: {self.device}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ OOTDiffusion ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def __call__(self, person_image: np.ndarray, clothing_image: np.ndarray, 
                 person_keypoints: Optional[np.ndarray] = None, **kwargs) -> np.ndarray:
        """ì‹¤ì œ OOTDiffusion ì¶”ë¡ """
        try:
            if not self.loaded and not self.load_model():
                return self._fallback_fitting(person_image, clothing_image, person_keypoints)
            
            # í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë³€í˜• ì ìš©
            if person_keypoints is not None:
                person_image = self._apply_keypoint_transformation(person_image, person_keypoints)
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            person_tensor = self._preprocess_image(person_image)
            clothing_tensor = self._preprocess_image(clothing_image)
            
            if person_tensor is None or clothing_tensor is None:
                return self._fallback_fitting(person_image, clothing_image, person_keypoints)
            
            # ì‹¤ì œ Diffusion ì¶”ë¡ 
            with torch.no_grad():
                num_steps = kwargs.get('inference_steps', 20)
                guidance_scale = kwargs.get('guidance_scale', 7.5)
                
                # ë…¸ì´ì¦ˆ ìƒì„±
                noise = torch.randn_like(person_tensor)
                
                # ì¡°ê±´ë¶€ ì¸ì½”ë”©
                conditioning = self._create_conditioning(clothing_tensor, person_keypoints)
                
                # Diffusion í”„ë¡œì„¸ìŠ¤
                timesteps = self.scheduler.timesteps[:num_steps]
                current_sample = noise
                
                for timestep in timesteps:
                    timestep_tensor = torch.tensor([timestep], device=self.device)
                    
                    # UNet ì¶”ë¡ 
                    noise_pred = self.model(
                        current_sample,
                        timestep_tensor,
                        encoder_hidden_states=conditioning
                    ).sample
                    
                    # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                    current_sample = self.scheduler.step(
                        noise_pred, timestep, current_sample
                    ).prev_sample
                
                # ê²°ê³¼ ë³€í™˜
                result_image = self._tensor_to_image(current_sample)
                
                # TPS í›„ì²˜ë¦¬
                if person_keypoints is not None:
                    result_image = self._apply_tps_refinement(result_image, person_keypoints)
                
                self.logger.info(f"âœ… OOTDiffusion ì‹¤ì œ ì¶”ë¡  ì™„ë£Œ")
                return result_image
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ OOTDiffusion ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self._fallback_fitting(person_image, clothing_image, person_keypoints)
    
    def _apply_keypoint_transformation(self, image: np.ndarray, keypoints: np.ndarray) -> np.ndarray:
        """í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ì´ë¯¸ì§€ ë³€í˜•"""
        try:
            if not CV2_AVAILABLE or keypoints is None:
                return image
            
            h, w = image.shape[:2]
            
            # í‘œì¤€ í‚¤í¬ì¸íŠ¸ ìœ„ì¹˜ (ì •ê·œí™”ëœ ì¢Œí‘œ)
            standard_keypoints = np.array([
                [0.5, 0.1],    # nose
                [0.5, 0.15],   # neck
                [0.4, 0.2],    # right_shoulder
                [0.35, 0.35],  # right_elbow
                [0.3, 0.5],    # right_wrist
                [0.6, 0.2],    # left_shoulder
                [0.65, 0.35],  # left_elbow
                [0.7, 0.5],    # left_wrist
                [0.45, 0.6],   # right_hip
                [0.45, 0.8],   # right_knee
                [0.45, 0.95],  # right_ankle
                [0.55, 0.6],   # left_hip
                [0.55, 0.8],   # left_knee
                [0.55, 0.95],  # left_ankle
                [0.48, 0.08],  # right_eye
                [0.52, 0.08],  # left_eye
                [0.46, 0.1],   # right_ear
                [0.54, 0.1]    # left_ear
            ])
            
            # ì‹¤ì œ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³€í™˜
            standard_keypoints[:, 0] *= w
            standard_keypoints[:, 1] *= h
            
            # í‚¤í¬ì¸íŠ¸ ìˆ˜ ë§ì¶”ê¸°
            if len(keypoints) != len(standard_keypoints):
                if len(keypoints) < len(standard_keypoints):
                    # ë¶€ì¡±í•œ í‚¤í¬ì¸íŠ¸ëŠ” í‘œì¤€ê°’ìœ¼ë¡œ ì±„ì›€
                    padded = standard_keypoints.copy()
                    padded[:len(keypoints)] = keypoints
                    keypoints = padded
                else:
                    # ë„ˆë¬´ ë§ìœ¼ë©´ ì²˜ìŒ 18ê°œë§Œ ì‚¬ìš©
                    keypoints = keypoints[:len(standard_keypoints)]
            
            # TPS ë³€í˜• ì ìš©
            tps = TPSTransform()
            if tps.fit(standard_keypoints, keypoints):
                # ì´ë¯¸ì§€ ê·¸ë¦¬ë“œ ìƒì„±
                y, x = np.mgrid[0:h:10, 0:w:10]  # 10í”½ì…€ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
                grid_points = np.column_stack([x.ravel(), y.ravel()])
                
                # TPS ë³€í˜• ì ìš©
                transformed_points = tps.transform(grid_points)
                
                # ë³€í˜•ëœ ê·¸ë¦¬ë“œë¡œ ì´ë¯¸ì§€ ì›Œí•‘
                if SCIPY_AVAILABLE:
                    transformed_x = transformed_points[:, 0].reshape(y.shape)
                    transformed_y = transformed_points[:, 1].reshape(x.shape)
                    
                    # ê° ì±„ë„ë³„ë¡œ ë³´ê°„
                    if len(image.shape) == 3:
                        result = np.zeros_like(image)
                        for c in range(image.shape[2]):
                            result[:, :, c] = griddata(
                                (transformed_y.ravel(), transformed_x.ravel()),
                                image[:, :, c].ravel(),
                                (y, x),
                                method='linear',
                                fill_value=0
                            ).astype(image.dtype)
                    else:
                        result = griddata(
                            (transformed_y.ravel(), transformed_x.ravel()),
                            image.ravel(),
                            (y, x),
                            method='linear',
                            fill_value=0
                        ).astype(image.dtype)
                    
                    return result
            
            return image
            
        except Exception as e:
            self.logger.warning(f"í‚¤í¬ì¸íŠ¸ ë³€í˜• ì‹¤íŒ¨: {e}")
            return image
    
    def _apply_tps_refinement(self, image: np.ndarray, keypoints: np.ndarray) -> np.ndarray:
        """TPS ê¸°ë°˜ ê²°ê³¼ ì •ì œ"""
        try:
            # ê²°ê³¼ ì´ë¯¸ì§€ì— ëŒ€í•´ ì¶”ê°€ì ì¸ í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ì •ì œ
            return self._apply_keypoint_transformation(image, keypoints)
        except Exception as e:
            self.logger.warning(f"TPS ì •ì œ ì‹¤íŒ¨: {e}")
            return image
    
    def _preprocess_image(self, image: np.ndarray) -> Optional[torch.Tensor]:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            if image.dtype != np.uint8:
                image = (image * 255).astype(np.uint8)
            
            pil_image = Image.fromarray(image).convert('RGB')
            pil_image = pil_image.resize((512, 512))
            
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
            ])
            
            tensor = transform(pil_image).unsqueeze(0).to(self.device)
            return tensor
        except Exception:
            return None
    
    def _create_conditioning(self, clothing_tensor: torch.Tensor, keypoints: Optional[np.ndarray]) -> torch.Tensor:
        """ì¡°ê±´ë¶€ ì¸ì½”ë”© ìƒì„±"""
        try:
            batch_size = clothing_tensor.shape[0]
            seq_len = 77
            hidden_dim = 768
            
            # í´ë¡œë”© í”¼ì²˜
            clothing_features = F.adaptive_avg_pool2d(clothing_tensor, (1, 1)).flatten(1)
            
            # í‚¤í¬ì¸íŠ¸ í”¼ì²˜ (ì˜µì…˜)
            if keypoints is not None and TORCH_AVAILABLE:
                keypoint_features = torch.tensor(keypoints.flatten(), device=self.device, dtype=torch.float32)
                keypoint_features = keypoint_features.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                
                # í”¼ì²˜ ê²°í•©
                if clothing_features.shape[1] == keypoint_features.shape[1]:
                    combined_features = clothing_features + keypoint_features
                else:
                    # ì°¨ì› ë§ì¶¤
                    if keypoint_features.shape[1] < clothing_features.shape[1]:
                        padding = torch.zeros(1, clothing_features.shape[1] - keypoint_features.shape[1], device=self.device)
                        keypoint_features = torch.cat([keypoint_features, padding], dim=1)
                    else:
                        keypoint_features = keypoint_features[:, :clothing_features.shape[1]]
                    
                    combined_features = clothing_features + keypoint_features
            else:
                combined_features = clothing_features
            
            # ì‹œí€€ìŠ¤ í™•ì¥
            conditioning = combined_features.unsqueeze(1).repeat(1, seq_len, 1)
            
            # ì°¨ì› ì¡°ì •
            if conditioning.shape[-1] != hidden_dim:
                linear_proj = nn.Linear(conditioning.shape[-1], hidden_dim).to(self.device)
                conditioning = linear_proj(conditioning)
            
            return conditioning
            
        except Exception as e:
            self.logger.warning(f"ì¡°ê±´ë¶€ ì¸ì½”ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            batch_size = clothing_tensor.shape[0]
            return torch.randn(batch_size, 77, 768, device=self.device)
    
    def _tensor_to_image(self, tensor: torch.Tensor) -> np.ndarray:
        """í…ì„œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            tensor = (tensor + 1.0) / 2.0
            tensor = torch.clamp(tensor, 0, 1)
            
            image = tensor.squeeze().cpu().numpy()
            
            if image.ndim == 3 and image.shape[0] == 3:
                image = image.transpose(1, 2, 0)
            
            image = (image * 255).astype(np.uint8)
            return image
        except Exception:
            return np.zeros((512, 512, 3), dtype=np.uint8)
    
    def _fallback_fitting(self, person_image: np.ndarray, clothing_image: np.ndarray, 
                         keypoints: Optional[np.ndarray] = None) -> np.ndarray:
        """í´ë°±: í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ê¸°í•˜í•™ì  í”¼íŒ…"""
        try:
            # í‚¤í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ í™œìš©
            if keypoints is not None:
                person_transformed = self._apply_keypoint_transformation(person_image, keypoints)
            else:
                person_transformed = person_image
            
            # ê¸°ë³¸ ì˜¤ë²„ë ˆì´ ì ìš©
            return self._basic_overlay(person_transformed, clothing_image)
            
        except Exception:
            return person_image
    
    def _basic_overlay(self, person_img: np.ndarray, cloth_img: np.ndarray) -> np.ndarray:
        """ê¸°ë³¸ ì˜¤ë²„ë ˆì´"""
        try:
            if not CV2_AVAILABLE:
                return person_img
                
            h, w = person_img.shape[:2]
            cloth_h, cloth_w = int(h * 0.4), int(w * 0.35)
            clothing_resized = cv2.resize(cloth_img, (cloth_w, cloth_h))
            
            y_offset = int(h * 0.25)
            x_offset = int(w * 0.325)
            
            result = person_img.copy()
            end_y = min(y_offset + cloth_h, h)
            end_x = min(x_offset + cloth_w, w)
            
            if end_y > y_offset and end_x > x_offset:
                alpha = 0.8
                clothing_region = clothing_resized[:end_y-y_offset, :end_x-x_offset]
                
                result[y_offset:end_y, x_offset:end_x] = cv2.addWeighted(
                    result[y_offset:end_y, x_offset:end_x], 1-alpha,
                    clothing_region, alpha, 0
                )
            
            return result
        except Exception:
            return person_img

# ==============================================
# ğŸ”¥ 7. ë°ì´í„° í´ë˜ìŠ¤ë“¤
# ==============================================

class FittingMethod(Enum):
    DIFFUSION_BASED = "diffusion"
    TPS_BASED = "tps"
    HYBRID = "hybrid"
    KEYPOINT_GUIDED = "keypoint_guided"

@dataclass
class FabricProperties:
    """ì²œ ì¬ì§ˆ ì†ì„±"""
    stiffness: float = 0.5
    elasticity: float = 0.3
    density: float = 1.4
    friction: float = 0.5
    shine: float = 0.5
    transparency: float = 0.0

@dataclass
class VirtualFittingConfig:
    """ê°€ìƒ í”¼íŒ… ì„¤ì •"""
    model_name: str = "ootdiffusion"
    inference_steps: int = 20
    guidance_scale: float = 7.5
    input_size: Tuple[int, int] = (512, 512)
    output_size: Tuple[int, int] = (512, 512)
    use_keypoints: bool = True
    use_tps: bool = True
    physics_enabled: bool = True
    memory_efficient: bool = True

@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼"""
    success: bool
    fitted_image: Optional[np.ndarray] = None
    confidence_score: float = 0.0
    processing_time: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None

# ìƒìˆ˜ë“¤
FABRIC_PROPERTIES = {
    'cotton': FabricProperties(0.3, 0.2, 1.5, 0.7, 0.2, 0.0),
    'denim': FabricProperties(0.8, 0.1, 2.0, 0.9, 0.1, 0.0),
    'silk': FabricProperties(0.1, 0.4, 1.3, 0.3, 0.8, 0.1),
    'wool': FabricProperties(0.5, 0.3, 1.4, 0.6, 0.3, 0.0),
    'default': FabricProperties(0.4, 0.3, 1.4, 0.5, 0.5, 0.0)
}

# ==============================================
# ğŸ”¥ 8. ë©”ì¸ VirtualFittingStep í´ë˜ìŠ¤ (BaseStepMixin ìƒì†)
# ==============================================

class VirtualFittingStep:
    """
    ğŸ”¥ 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… Step - ì™„ì „í•œ DIíŒ¨í„´ + StepFactory ê¸°ë°˜
    
    âœ… StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… â†’ ì™„ì„±ëœ Step
    âœ… VirtualFittingMixin ìƒì† + íŠ¹í™” ê¸°ëŠ¥
    âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  (OOTDiffusion + í‚¤í¬ì¸íŠ¸ + TPS)
    âœ… ì™„ì „í•œ ì²˜ë¦¬ íë¦„ êµ¬í˜„
    """
    
    def __init__(self, **kwargs):
        """VirtualFittingStep ì´ˆê¸°í™” (BaseStepMixin íŒ¨í„´)"""
        
        # VirtualFittingMixin íŠ¹í™” ì„¤ì •
        self.step_name = "VirtualFittingStep"
        self.step_id = 6
        self.step_number = 6
        self.fitting_modes = ['standard', 'high_quality', 'fast', 'experimental']
        self.fitting_mode = kwargs.get('fitting_mode', 'high_quality')
        self.diffusion_steps = kwargs.get('diffusion_steps', 20)
        self.guidance_scale = kwargs.get('guidance_scale', 7.5)
        self.use_ootd = kwargs.get('use_ootd', True)
        
        # BaseStepMixin í•µì‹¬ ì†ì„±ë“¤
        self.logger = logging.getLogger(f"pipeline.{self.step_name}")
        self.device = kwargs.get('device', 'auto')
        self.is_initialized = False
        self.is_ready = False
        
        # ğŸ”¥ DI íŒ¨í„´ í•µì‹¬: ì˜ì¡´ì„± ì£¼ì… ëŒ€ê¸° ì†ì„±ë“¤
        self.model_loader = None
        self.memory_manager = None
        self.data_converter = None
        self.di_container = None
        self.step_factory = None
        self.step_interface = None
        
        # ì„¤ì •
        self.config = VirtualFittingConfig(**{k: v for k, v in kwargs.items() 
                                            if k in VirtualFittingConfig.__annotations__})
        
        # AI ëª¨ë¸ ê´€ë¦¬
        self.ai_models = {}
        self.model_cache = {}
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'total_processed': 0,
            'successful_fittings': 0,
            'average_processing_time': 0.0,
            'keypoint_usage': 0,
            'tps_usage': 0,
            'ai_model_usage': 0
        }
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.result_cache = {}
        self.cache_lock = threading.RLock()
        
        self.logger.info("âœ… VirtualFittingStep ì´ˆê¸°í™” ì™„ë£Œ (DI íŒ¨í„´)")
    
    # ==============================================
    # ğŸ”¥ 9. BaseStepMixin íŒ¨í„´ ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.model_loader = model_loader
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.memory_manager = memory_manager
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.data_converter = data_converter
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def set_di_container(self, di_container):
        """DI Container ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.di_container = di_container
            self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def set_step_factory(self, step_factory):
        """StepFactory ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.step_factory = step_factory
            self.logger.info("âœ… StepFactory ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ StepFactory ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def set_step_interface(self, step_interface):
        """Step ì¸í„°í˜ì´ìŠ¤ ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.step_interface = step_interface
            self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ Step ì¸í„°í˜ì´ìŠ¤ ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ 10. BaseStepMixin í•µì‹¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    def initialize(self) -> bool:
        """Step ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info("ğŸ”„ VirtualFittingStep ì´ˆê¸°í™” ì‹œì‘...")
            
            # AI ëª¨ë¸ ë¡œë“œ
            self._load_ai_models()
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            self._optimize_memory()
            
            self.is_initialized = True
            self.is_ready = True
            self.logger.info("âœ… VirtualFittingStep ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_async(self) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.initialize)
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _load_ai_models(self):
        """AI ëª¨ë¸ ë¡œë“œ"""
        try:
            # ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            if self.model_loader and hasattr(self.model_loader, 'load_model'):
                checkpoint = self.model_loader.load_model("virtual_fitting_ootd")
                if checkpoint:
                    # AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„±
                    device = "mps" if MPS_AVAILABLE else "cpu"
                    model_wrapper = OOTDiffusionWrapper(checkpoint, device)
                    
                    # ê°€ì¤‘ì¹˜ ë¡œë”©
                    if model_wrapper.load_model():
                        self.ai_models['ootdiffusion'] = model_wrapper
                        self.logger.info("âœ… OOTDiffusion AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                        return
            
            self.logger.warning("âš ï¸ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - í´ë°± ëª¨ë“œ ì‚¬ìš©")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            if self.memory_manager and hasattr(self.memory_manager, 'optimize'):
                self.memory_manager.optimize()
            else:
                safe_memory_cleanup()
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (BaseStepMixin í˜¸í™˜)"""
        try:
            if model_name in self.ai_models:
                return self.ai_models[model_name]
            
            if self.model_loader and hasattr(self.model_loader, 'get_model'):
                return self.model_loader.get_model(model_name or "default")
            
            return None
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.get_model, model_name)
        except Exception:
            return None
    
    # ==============================================
    # ğŸ”¥ 11. ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ (ì™„ì „í•œ ì²˜ë¦¬ íë¦„)
    # ==============================================
    
    async def process(
        self,
        person_image: Union[np.ndarray, Image.Image, str],
        clothing_image: Union[np.ndarray, Image.Image, str],
        pose_data: Optional[Dict[str, Any]] = None,
        cloth_mask: Optional[np.ndarray] = None,
        fabric_type: str = "cotton",
        clothing_type: str = "shirt",
        **kwargs
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ ë©”ì¸ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ë©”ì„œë“œ
        ì™„ì „í•œ ì²˜ë¦¬ íë¦„:
        1. StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… âœ…
        2. ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„± â†’ ê°€ì¤‘ì¹˜ ë¡œë”© âœ…
        3. í‚¤í¬ì¸íŠ¸ ê²€ì¶œ â†’ TPS ë³€í˜• ê³„ì‚° â†’ ê¸°í•˜í•™ì  ë³€í˜• ì ìš© âœ…
        4. í’ˆì§ˆ í‰ê°€ â†’ ì‹œê°í™” ìƒì„± â†’ API ì‘ë‹µ âœ…
        """
        start_time = time.time()
        session_id = f"vf_{uuid.uuid4().hex[:8]}"
        
        try:
            self.logger.info(f"ğŸ”¥ 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹œì‘ - {session_id}")
            
            # ì´ˆê¸°í™” í™•ì¸
            if not self.is_initialized:
                self.initialize()
            
            # ğŸ”¥ STEP 1: ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬
            processed_data = await self._preprocess_inputs(
                person_image, clothing_image, pose_data, cloth_mask
            )
            
            if not processed_data['success']:
                return processed_data
            
            person_img = processed_data['person_image']
            clothing_img = processed_data['clothing_image']
            
            # ğŸ”¥ STEP 2: í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
            person_keypoints = None
            if self.config.use_keypoints:
                person_keypoints = await self._detect_keypoints(person_img, pose_data)
                if person_keypoints is not None:
                    self.performance_stats['keypoint_usage'] += 1
                    self.logger.info(f"âœ… í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì™„ë£Œ: {len(person_keypoints)}ê°œ")
            
            # ğŸ”¥ STEP 3: AI ëª¨ë¸ì„ í†µí•œ ê°€ìƒ í”¼íŒ… ì‹¤í–‰
            fitted_image = await self._execute_ai_virtual_fitting(
                person_img, clothing_img, person_keypoints, fabric_type, clothing_type, kwargs
            )
            
            # ğŸ”¥ STEP 4: TPS ë³€í˜• ê³„ì‚° ë° ì ìš©
            if self.config.use_tps and person_keypoints is not None:
                fitted_image = await self._apply_tps_refinement(fitted_image, person_keypoints)
                self.performance_stats['tps_usage'] += 1
                self.logger.info("âœ… TPS ë³€í˜• ê³„ì‚° ë° ì ìš© ì™„ë£Œ")
            
            # ğŸ”¥ STEP 5: í’ˆì§ˆ í‰ê°€
            quality_score = await self._assess_quality(fitted_image, person_img, clothing_img)
            
            # ğŸ”¥ STEP 6: ì‹œê°í™” ìƒì„±
            visualization = await self._create_visualization(
                person_img, clothing_img, fitted_image, person_keypoints
            )
            
            # ğŸ”¥ STEP 7: API ì‘ë‹µ êµ¬ì„±
            processing_time = time.time() - start_time
            final_result = self._build_api_response(
                fitted_image, visualization, quality_score, 
                processing_time, session_id, {
                    'fabric_type': fabric_type,
                    'clothing_type': clothing_type,
                    'keypoints_used': person_keypoints is not None,
                    'tps_applied': self.config.use_tps and person_keypoints is not None,
                    'ai_model_used': 'ootdiffusion' in self.ai_models
                }
            )
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            self._update_performance_stats(final_result)
            
            self.logger.info(f"âœ… ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            return final_result
            
        except Exception as e:
            error_msg = f"ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
            self.logger.error(f"âŒ {error_msg}")
            return self._create_error_response(time.time() - start_time, session_id, error_msg)
    
    async def _preprocess_inputs(
        self, person_image, clothing_image, pose_data, cloth_mask
    ) -> Dict[str, Any]:
        """ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ ë³€í™˜ (DataConverter ì‚¬ìš© ë˜ëŠ” í´ë°±)
            if self.data_converter and hasattr(self.data_converter, 'to_numpy'):
                person_img = self.data_converter.to_numpy(person_image)
                clothing_img = self.data_converter.to_numpy(clothing_image)
            else:
                # í´ë°±: ì§ì ‘ ë³€í™˜
                person_img = self._convert_to_numpy(person_image)
                clothing_img = self._convert_to_numpy(clothing_image)
            
            # ìœ íš¨ì„± ê²€ì‚¬
            if person_img.size == 0 or clothing_img.size == 0:
                return {
                    'success': False,
                    'error_message': 'ì…ë ¥ ì´ë¯¸ì§€ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤',
                    'person_image': None,
                    'clothing_image': None
                }
            
            # í¬ê¸° ì •ê·œí™”
            person_img = self._normalize_image(person_img, self.config.input_size)
            clothing_img = self._normalize_image(clothing_img, self.config.input_size)
            
            return {
                'success': True,
                'person_image': person_img,
                'clothing_image': clothing_img,
                'pose_data': pose_data,
                'cloth_mask': cloth_mask
            }
            
        except Exception as e:
            return {
                'success': False,
                'error_message': f'ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}',
                'person_image': None,
                'clothing_image': None
            }
    
    def _convert_to_numpy(self, image) -> np.ndarray:
        """ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜"""
        try:
            if isinstance(image, np.ndarray):
                return image
            elif isinstance(image, Image.Image):
                return np.array(image)
            elif isinstance(image, str):
                pil_img = Image.open(image)
                return np.array(pil_img)
            else:
                return np.array(image)
        except Exception:
            return np.array([])
    
    def _normalize_image(self, image: np.ndarray, target_size: Tuple[int, int]) -> np.ndarray:
        """ì´ë¯¸ì§€ ì •ê·œí™” ë° í¬ê¸° ì¡°ì •"""
        try:
            # dtype ì •ê·œí™”
            if image.dtype != np.uint8:
                if image.max() <= 1.0:
                    image = (image * 255).astype(np.uint8)
                else:
                    image = np.clip(image, 0, 255).astype(np.uint8)
            
            # í¬ê¸° ì¡°ì •
            if CV2_AVAILABLE:
                resized = cv2.resize(image, target_size, interpolation=cv2.INTER_LANCZOS4)
                # BGR -> RGB ë³€í™˜ ì²´í¬
                if len(resized.shape) == 3 and np.mean(resized[:, :, 0]) < np.mean(resized[:, :, 2]):
                    resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
                return resized
            else:
                pil_img = Image.fromarray(image)
                pil_img = pil_img.resize(target_size, Image.Resampling.LANCZOS)
                return np.array(pil_img)
                
        except Exception:
            return image
    
    async def _detect_keypoints(self, person_img: np.ndarray, pose_data: Optional[Dict[str, Any]]) -> Optional[np.ndarray]:
        """í‚¤í¬ì¸íŠ¸ ê²€ì¶œ"""
        try:
            # í¬ì¦ˆ ë°ì´í„°ì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ìš°ì„ 
            if pose_data:
                keypoints = extract_keypoints_from_pose_data(pose_data)
                if keypoints is not None:
                    self.logger.info("âœ… í¬ì¦ˆ ë°ì´í„°ì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ")
                    return keypoints
            
            # ì´ë¯¸ì§€ì—ì„œ ì§ì ‘ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
            keypoints = detect_body_keypoints(person_img)
            if keypoints is not None:
                self.logger.info("âœ… ì´ë¯¸ì§€ì—ì„œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ")
                return keypoints
            
            self.logger.warning("âš ï¸ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _execute_ai_virtual_fitting(
        self, person_img: np.ndarray, clothing_img: np.ndarray,
        keypoints: Optional[np.ndarray], fabric_type: str, clothing_type: str,
        kwargs: Dict[str, Any]
    ) -> np.ndarray:
        """AI ëª¨ë¸ì„ í†µí•œ ê°€ìƒ í”¼íŒ… ì‹¤í–‰"""
        try:
            # OOTDiffusion ëª¨ë¸ ì‚¬ìš©
            if 'ootdiffusion' in self.ai_models:
                ai_model = self.ai_models['ootdiffusion']
                self.logger.info("ğŸ§  OOTDiffusion AI ëª¨ë¸ë¡œ ì¶”ë¡  ì‹¤í–‰")
                
                try:
                    fitted_image = ai_model(
                        person_img, clothing_img, 
                        person_keypoints=keypoints,
                        inference_steps=self.config.inference_steps,
                        guidance_scale=self.config.guidance_scale,
                        **kwargs
                    )
                    
                    if isinstance(fitted_image, np.ndarray) and fitted_image.size > 0:
                        self.performance_stats['ai_model_usage'] += 1
                        self.logger.info("âœ… AI ëª¨ë¸ ì¶”ë¡  ì„±ê³µ")
                        return fitted_image
                        
                except Exception as ai_error:
                    self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {ai_error}")
            
            # í´ë°±: í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ê¸°í•˜í•™ì  í”¼íŒ…
            self.logger.info("ğŸ”„ í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ê¸°í•˜í•™ì  í”¼íŒ…ìœ¼ë¡œ í´ë°±")
            return await self._keypoint_based_geometric_fitting(
                person_img, clothing_img, keypoints, fabric_type, clothing_type
            )
            
        except Exception as e:
            self.logger.error(f"âŒ AI ê°€ìƒ í”¼íŒ… ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return await self._basic_geometric_fitting(person_img, clothing_img)
    
    async def _keypoint_based_geometric_fitting(
        self, person_img: np.ndarray, clothing_img: np.ndarray,
        keypoints: Optional[np.ndarray], fabric_type: str, clothing_type: str
    ) -> np.ndarray:
        """í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ê¸°í•˜í•™ì  í”¼íŒ…"""
        try:
            # í‚¤í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ TPS ë³€í˜• ì ìš©
            if keypoints is not None and SCIPY_AVAILABLE:
                # í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë³€í˜•
                tps = TPSTransform()
                
                # í‘œì¤€ í‚¤í¬ì¸íŠ¸ ì •ì˜
                h, w = person_img.shape[:2]
                standard_keypoints = self._get_standard_keypoints(w, h, clothing_type)
                
                if len(keypoints) >= len(standard_keypoints):
                    # TPS ë³€í˜• ê³„ì‚°
                    if tps.fit(standard_keypoints, keypoints[:len(standard_keypoints)]):
                        # ì˜ë¥˜ ì´ë¯¸ì§€ì— ë³€í˜• ì ìš©
                        clothing_transformed = self._apply_tps_to_image(clothing_img, tps, person_img.shape)
                        
                        # ë³€í˜•ëœ ì˜ë¥˜ì™€ ì‚¬ëŒ ì´ë¯¸ì§€ ë¸”ë Œë”©
                        return self._blend_images(person_img, clothing_transformed, fabric_type)
            
            # í´ë°±: ê¸°ë³¸ ì˜¤ë²„ë ˆì´
            return await self._basic_geometric_fitting(person_img, clothing_img)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ í”¼íŒ… ì‹¤íŒ¨: {e}")
            return await self._basic_geometric_fitting(person_img, clothing_img)
    
    def _get_standard_keypoints(self, width: int, height: int, clothing_type: str) -> np.ndarray:
        """ì˜ë¥˜ íƒ€ì…ë³„ í‘œì¤€ í‚¤í¬ì¸íŠ¸ ìƒì„±"""
        if clothing_type in ['shirt', 'blouse', 'top']:
            # ìƒì˜ìš© í‚¤í¬ì¸íŠ¸ (ìƒì²´ ì¤‘ì‹¬)
            keypoints = [
                [width*0.5, height*0.15],   # neck
                [width*0.4, height*0.2],    # right_shoulder
                [width*0.35, height*0.35],  # right_elbow
                [width*0.6, height*0.2],    # left_shoulder
                [width*0.65, height*0.35],  # left_elbow
                [width*0.45, height*0.6],   # right_hip
                [width*0.55, height*0.6],   # left_hip
            ]
        elif clothing_type in ['pants', 'jeans']:
            # í•˜ì˜ìš© í‚¤í¬ì¸íŠ¸ (í•˜ì²´ ì¤‘ì‹¬)
            keypoints = [
                [width*0.45, height*0.6],   # right_hip
                [width*0.45, height*0.8],   # right_knee
                [width*0.45, height*0.95],  # right_ankle
                [width*0.55, height*0.6],   # left_hip
                [width*0.55, height*0.8],   # left_knee
                [width*0.55, height*0.95],  # left_ankle
            ]
        elif clothing_type == 'dress':
            # ì›í”¼ìŠ¤ìš© í‚¤í¬ì¸íŠ¸ (ì „ì²´)
            keypoints = [
                [width*0.5, height*0.15],   # neck
                [width*0.4, height*0.2],    # right_shoulder
                [width*0.6, height*0.2],    # left_shoulder
                [width*0.45, height*0.6],   # right_hip
                [width*0.55, height*0.6],   # left_hip
                [width*0.45, height*0.8],   # right_knee
                [width*0.55, height*0.8],   # left_knee
            ]
        else:
            # ê¸°ë³¸ í‚¤í¬ì¸íŠ¸
            keypoints = [
                [width*0.5, height*0.15],   # neck
                [width*0.4, height*0.2],    # right_shoulder
                [width*0.6, height*0.2],    # left_shoulder
                [width*0.45, height*0.6],   # right_hip
                [width*0.55, height*0.6],   # left_hip
            ]
        
        return np.array(keypoints)
    
    def _apply_tps_to_image(self, image: np.ndarray, tps: TPSTransform, target_shape: Tuple[int, int]) -> np.ndarray:
        """TPS ë³€í˜•ì„ ì´ë¯¸ì§€ì— ì ìš©"""
        try:
            h, w = target_shape[:2]
            
            # ì´ë¯¸ì§€ ê·¸ë¦¬ë“œ ìƒì„±
            y_coords, x_coords = np.mgrid[0:h:5, 0:w:5]  # 5í”½ì…€ ê°„ê²©
            grid_points = np.column_stack([x_coords.ravel(), y_coords.ravel()])
            
            # TPS ë³€í˜• ì ìš©
            transformed_points = tps.transform(grid_points)
            
            # ë³€í˜•ëœ ì¢Œí‘œë¡œ ì´ë¯¸ì§€ ì›Œí•‘
            if SCIPY_AVAILABLE:
                transformed_x = transformed_points[:, 0].reshape(x_coords.shape)
                transformed_y = transformed_points[:, 1].reshape(y_coords.shape)
                
                # ì´ë¯¸ì§€ í¬ê¸°ë¥¼ íƒ€ê²Ÿì— ë§ì¶¤
                image_resized = cv2.resize(image, (w, h)) if CV2_AVAILABLE else image
                
                # ê° ì±„ë„ë³„ë¡œ ë³´ê°„
                if len(image_resized.shape) == 3:
                    result = np.zeros((h, w, image_resized.shape[2]), dtype=image_resized.dtype)
                    for c in range(image_resized.shape[2]):
                        result[:, :, c] = griddata(
                            (transformed_y.ravel(), transformed_x.ravel()),
                            image_resized[:, :, c].ravel(),
                            (y_coords, x_coords),
                            method='linear',
                            fill_value=0
                        ).astype(image_resized.dtype)
                else:
                    result = griddata(
                        (transformed_y.ravel(), transformed_x.ravel()),
                        image_resized.ravel(),
                        (y_coords, x_coords),
                        method='linear',
                        fill_value=0
                    ).astype(image_resized.dtype)
                
                return result
            
            return image
            
        except Exception as e:
            self.logger.warning(f"TPS ì´ë¯¸ì§€ ì ìš© ì‹¤íŒ¨: {e}")
            return image
    
    def _blend_images(self, person_img: np.ndarray, clothing_img: np.ndarray, fabric_type: str) -> np.ndarray:
        """ì´ë¯¸ì§€ ë¸”ë Œë”©"""
        try:
            # ì²œ ì¬ì§ˆì— ë”°ë¥¸ ë¸”ë Œë”© íŒŒë¼ë¯¸í„°
            fabric_props = FABRIC_PROPERTIES.get(fabric_type, FABRIC_PROPERTIES['default'])
            alpha = 0.7 + fabric_props.transparency * 0.2
            alpha = np.clip(alpha, 0.5, 0.9)
            
            # í¬ê¸° ë§ì¶¤
            if clothing_img.shape != person_img.shape:
                clothing_img = cv2.resize(clothing_img, (person_img.shape[1], person_img.shape[0])) if CV2_AVAILABLE else clothing_img
            
            # ë¸”ë Œë”©
            if CV2_AVAILABLE:
                result = cv2.addWeighted(person_img, 1-alpha, clothing_img, alpha, 0)
            else:
                result = (person_img * (1-alpha) + clothing_img * alpha).astype(person_img.dtype)
            
            return result
            
        except Exception as e:
            self.logger.warning(f"ì´ë¯¸ì§€ ë¸”ë Œë”© ì‹¤íŒ¨: {e}")
            return person_img
    
    async def _basic_geometric_fitting(self, person_img: np.ndarray, clothing_img: np.ndarray) -> np.ndarray:
        """ê¸°ë³¸ ê¸°í•˜í•™ì  í”¼íŒ…"""
        try:
            if not CV2_AVAILABLE:
                return person_img
            
            h, w = person_img.shape[:2]
            
            # ì˜ë¥˜ë¥¼ ìƒì²´ ì¤‘ì•™ì— ë°°ì¹˜
            cloth_h, cloth_w = int(h * 0.4), int(w * 0.35)
            clothing_resized = cv2.resize(clothing_img, (cloth_w, cloth_h))
            
            y_offset = int(h * 0.25)
            x_offset = int(w * 0.325)
            
            result = person_img.copy()
            end_y = min(y_offset + cloth_h, h)
            end_x = min(x_offset + cloth_w, w)
            
            if end_y > y_offset and end_x > x_offset:
                alpha = 0.8
                clothing_region = clothing_resized[:end_y-y_offset, :end_x-x_offset]
                
                result[y_offset:end_y, x_offset:end_x] = cv2.addWeighted(
                    result[y_offset:end_y, x_offset:end_x], 1-alpha,
                    clothing_region, alpha, 0
                )
            
            return result
            
        except Exception as e:
            self.logger.warning(f"ê¸°ë³¸ í”¼íŒ… ì‹¤íŒ¨: {e}")
            return person_img
    
    async def _apply_tps_refinement(self, fitted_image: np.ndarray, keypoints: np.ndarray) -> np.ndarray:
        """TPS ê¸°ë°˜ ê²°ê³¼ ì •ì œ"""
        try:
            if not SCIPY_AVAILABLE:
                return fitted_image
            
            # í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë¯¸ì„¸ ì¡°ì •
            h, w = fitted_image.shape[:2]
            
            # í˜„ì¬ í‚¤í¬ì¸íŠ¸ì™€ ì´ìƒì  í‚¤í¬ì¸íŠ¸ ë¹„êµ
            ideal_keypoints = self._get_standard_keypoints(w, h, "shirt")  # ê¸°ë³¸ê°’ ì‚¬ìš©
            
            if len(keypoints) >= len(ideal_keypoints):
                tps = TPSTransform()
                if tps.fit(keypoints[:len(ideal_keypoints)], ideal_keypoints):
                    # ë¯¸ì„¸ ì¡°ì • ë³€í˜• ì ìš©
                    refined_image = self._apply_tps_to_image(fitted_image, tps, fitted_image.shape)
                    return refined_image
            
            return fitted_image
            
        except Exception as e:
            self.logger.warning(f"TPS ì •ì œ ì‹¤íŒ¨: {e}")
            return fitted_image
    
    async def _assess_quality(self, fitted_image: np.ndarray, person_img: np.ndarray, clothing_img: np.ndarray) -> float:
        """í’ˆì§ˆ í‰ê°€"""
        try:
            if fitted_image is None or fitted_image.size == 0:
                return 0.0
            
            quality_scores = []
            
            # ì´ë¯¸ì§€ ì„ ëª…ë„
            sharpness = self._calculate_sharpness(fitted_image)
            quality_scores.append(min(sharpness / 100.0, 1.0))
            
            # ìƒ‰ìƒ ì¼ì¹˜ë„
            color_match = self._calculate_color_match(clothing_img, fitted_image)
            quality_scores.append(color_match)
            
            # í‚¤í¬ì¸íŠ¸ ì‚¬ìš© ë³´ë„ˆìŠ¤
            if self.performance_stats.get('keypoint_usage', 0) > 0:
                quality_scores.append(0.8)
            
            # AI ëª¨ë¸ ì‚¬ìš© ë³´ë„ˆìŠ¤
            if self.performance_stats.get('ai_model_usage', 0) > 0:
                quality_scores.append(0.9)
            else:
                quality_scores.append(0.7)
            
            final_score = np.mean(quality_scores) if quality_scores else 0.5
            return float(np.clip(final_score, 0.0, 1.0))
            
        except Exception as e:
            self.logger.warning(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_sharpness(self, image: np.ndarray) -> float:
        """ì´ë¯¸ì§€ ì„ ëª…ë„ ê³„ì‚°"""
        try:
            if CV2_AVAILABLE and len(image.shape) >= 2:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image
                laplacian = cv2.Laplacian(gray, cv2.CV_64F)
                return float(np.var(laplacian))
            return 50.0
        except Exception:
            return 50.0
    
    def _calculate_color_match(self, cloth_img: np.ndarray, fitted_img: np.ndarray) -> float:
        """ìƒ‰ìƒ ì¼ì¹˜ë„ ê³„ì‚°"""
        try:
            if len(cloth_img.shape) == 3 and len(fitted_img.shape) == 3:
                cloth_mean = np.mean(cloth_img, axis=(0, 1))
                fitted_mean = np.mean(fitted_img, axis=(0, 1))
                
                distance = np.linalg.norm(cloth_mean - fitted_mean)
                similarity = max(0.0, 1.0 - (distance / 441.67))
                
                return float(similarity)
            return 0.7
        except Exception:
            return 0.7
    
    async def _create_visualization(
        self, person_img: np.ndarray, clothing_img: np.ndarray, 
        fitted_img: np.ndarray, keypoints: Optional[np.ndarray]
    ) -> Dict[str, Any]:
        """ì‹œê°í™” ìƒì„±"""
        try:
            visualization = {}
            
            # ì „í›„ ë¹„êµ ì´ë¯¸ì§€
            comparison = self._create_comparison_image(person_img, fitted_img)
            visualization['comparison'] = self._encode_image_base64(comparison)
            
            # í”„ë¡œì„¸ìŠ¤ ë‹¨ê³„ë³„ ì´ë¯¸ì§€
            process_steps = []
            steps = [
                ("1. ì›ë³¸", person_img),
                ("2. ì˜ë¥˜", clothing_img),
                ("3. ê²°ê³¼", fitted_img)
            ]
            
            for step_name, img in steps:
                encoded = self._encode_image_base64(self._resize_for_display(img, (200, 200)))
                process_steps.append({"name": step_name, "image": encoded})
            
            visualization['process_steps'] = process_steps
            
            # í‚¤í¬ì¸íŠ¸ ì‹œê°í™” (ìˆëŠ” ê²½ìš°)
            if keypoints is not None:
                keypoint_img = self._draw_keypoints(person_img.copy(), keypoints)
                visualization['keypoints'] = self._encode_image_base64(keypoint_img)
            
            return visualization
            
        except Exception as e:
            self.logger.error(f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _create_comparison_image(self, before: np.ndarray, after: np.ndarray) -> np.ndarray:
        """ì „í›„ ë¹„êµ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # í¬ê¸° í†µì¼
            h, w = before.shape[:2]
            if after.shape[:2] != (h, w):
                after = cv2.resize(after, (w, h)) if CV2_AVAILABLE else after
            
            # ë‚˜ë€íˆ ë°°ì¹˜
            comparison = np.hstack([before, after])
            
            # êµ¬ë¶„ì„  ì¶”ê°€
            if CV2_AVAILABLE and len(comparison.shape) == 3:
                mid_x = w
                cv2.line(comparison, (mid_x, 0), (mid_x, h), (255, 255, 255), 2)
            
            return comparison
        except Exception:
            return before
    
    def _draw_keypoints(self, image: np.ndarray, keypoints: np.ndarray) -> np.ndarray:
        """í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°"""
        try:
            if not CV2_AVAILABLE:
                return image
            
            result = image.copy()
            for i, (x, y) in enumerate(keypoints):
                x, y = int(x), int(y)
                if 0 <= x < result.shape[1] and 0 <= y < result.shape[0]:
                    cv2.circle(result, (x, y), 3, (255, 0, 0), -1)
                    cv2.putText(result, str(i), (x+5, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)
            
            return result
        except Exception:
            return image
    
    def _resize_for_display(self, image: np.ndarray, size: Tuple[int, int]) -> np.ndarray:
        """ë””ìŠ¤í”Œë ˆì´ìš© í¬ê¸° ì¡°ì •"""
        try:
            if CV2_AVAILABLE:
                return cv2.resize(image, size)
            else:
                pil_img = Image.fromarray(image)
                pil_img = pil_img.resize(size)
                return np.array(pil_img)
        except Exception:
            return image
    
    def _encode_image_base64(self, image: np.ndarray) -> str:
        """ì´ë¯¸ì§€ Base64 ì¸ì½”ë”©"""
        try:
            pil_image = Image.fromarray(image)
            buffer = BytesIO()
            pil_image.save(buffer, format='PNG')
            image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
            return f"data:image/png;base64,{image_base64}"
        except Exception:
            return ""
    
    def _build_api_response(
        self, fitted_image: np.ndarray, visualization: Dict[str, Any], 
        quality_score: float, processing_time: float, session_id: str, 
        metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """API ì‘ë‹µ êµ¬ì„±"""
        try:
            confidence = quality_score * 0.9 + 0.1
            time_score = max(0.1, min(1.0, 10.0 / processing_time))
            overall_score = (quality_score * 0.5 + confidence * 0.3 + time_score * 0.2)
            
            return {
                "success": True,
                "session_id": session_id,
                "step_name": self.step_name,
                "processing_time": processing_time,
                "confidence": confidence,
                "quality_score": quality_score,
                "overall_score": overall_score,
                
                # ì´ë¯¸ì§€ ê²°ê³¼
                "fitted_image": self._encode_image_base64(fitted_image),
                "fitted_image_raw": fitted_image,
                
                # ì²˜ë¦¬ íë¦„ ì •ë³´
                "processing_flow": {
                    "step_1_preprocessing": "âœ… ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ",
                    "step_2_keypoint_detection": f"{'âœ… í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì™„ë£Œ' if metadata['keypoints_used'] else 'âš ï¸ í‚¤í¬ì¸íŠ¸ ë¯¸ì‚¬ìš©'}",
                    "step_3_ai_inference": f"{'âœ… AI ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ' if metadata['ai_model_used'] else 'âš ï¸ í´ë°± ëª¨ë“œ ì‚¬ìš©'}",
                    "step_4_tps_transformation": f"{'âœ… TPS ë³€í˜• ì ìš© ì™„ë£Œ' if metadata['tps_applied'] else 'âš ï¸ TPS ë¯¸ì ìš©'}",
                    "step_5_quality_assessment": f"âœ… í’ˆì§ˆ í‰ê°€ ì™„ë£Œ (ì ìˆ˜: {quality_score:.2f})",
                    "step_6_visualization": "âœ… ì‹œê°í™” ìƒì„± ì™„ë£Œ",
                    "step_7_api_response": "âœ… API ì‘ë‹µ êµ¬ì„± ì™„ë£Œ"
                },
                
                # ë©”íƒ€ë°ì´í„°
                "metadata": {
                    **metadata,
                    "device": self.device,
                    "step_id": self.step_id,
                    "fitting_mode": self.fitting_mode
                },
                
                # ì‹œê°í™” ë°ì´í„°
                "visualization": visualization,
                
                # ì„±ëŠ¥ ì •ë³´
                "performance_info": {
                    "models_used": list(self.ai_models.keys()),
                    "keypoint_detection": metadata['keypoints_used'],
                    "tps_transformation": metadata['tps_applied'],
                    "ai_model_inference": metadata['ai_model_used'],
                    "processing_stats": self.performance_stats
                },
                
                # ì¶”ì²œì‚¬í•­
                "recommendations": self._generate_recommendations(metadata, quality_score)
            }
            
        except Exception as e:
            self.logger.error(f"API ì‘ë‹µ êµ¬ì„± ì‹¤íŒ¨: {e}")
            return self._create_error_response(processing_time, session_id, str(e))
    
    def _generate_recommendations(self, metadata: Dict[str, Any], quality_score: float) -> List[str]:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        try:
            if quality_score >= 0.8:
                recommendations.append("ğŸ‰ í›Œë¥­í•œ í’ˆì§ˆì˜ ê°€ìƒ í”¼íŒ… ê²°ê³¼ì…ë‹ˆë‹¤!")
            elif quality_score >= 0.6:
                recommendations.append("ğŸ‘ ì–‘í˜¸í•œ í’ˆì§ˆì…ë‹ˆë‹¤. ë‹¤ë¥¸ ê°ë„ë‚˜ ì¡°ëª…ì—ì„œë„ ì‹œë„í•´ë³´ì„¸ìš”.")
            else:
                recommendations.append("ğŸ’¡ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•´ ì •ë©´ì„ í–¥í•œ ì„ ëª…í•œ ì‚¬ì§„ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
            
            if metadata['ai_model_used']:
                recommendations.append("ğŸ§  ì‹¤ì œ AI ëª¨ë¸(OOTDiffusion)ë¡œ ì²˜ë¦¬ë˜ì–´ ë†’ì€ í’ˆì§ˆì„ ë³´ì¥í•©ë‹ˆë‹¤.")
            
            if metadata['keypoints_used']:
                recommendations.append("ğŸ¯ í‚¤í¬ì¸íŠ¸ ê²€ì¶œì´ ì ìš©ë˜ì–´ ë” ì •í™•í•œ í”¼íŒ…ì´ ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤.")
            
            if metadata['tps_applied']:
                recommendations.append("ğŸ“ TPS ë³€í˜•ì´ ì ìš©ë˜ì–´ ìì—°ìŠ¤ëŸ¬ìš´ ì°©ìš©ê°ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.")
            
            # ì²œ ì¬ì§ˆë³„ ì¶”ì²œ
            fabric_type = metadata.get('fabric_type', 'cotton')
            fabric_tips = {
                'cotton': "ë©´ ì†Œì¬ëŠ” í¸ì•ˆí•˜ê³  í†µê¸°ì„±ì´ ì¢‹ìŠµë‹ˆë‹¤ ğŸ‘•",
                'silk': "ì‹¤í¬ëŠ” ìš°ì•„í•˜ê³  ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ëŠë‚Œì„ ì¤ë‹ˆë‹¤ âœ¨",
                'denim': "ë°ë‹˜ì€ ìºì£¼ì–¼í•œ ìŠ¤íƒ€ì¼ë§ì— ì™„ë²½í•©ë‹ˆë‹¤ ğŸ‘–",
                'wool': "ìš¸ ì†Œì¬ëŠ” ë³´ì˜¨ì„±ì´ ë›°ì–´ë‚©ë‹ˆë‹¤ ğŸ§¥"
            }
            
            if fabric_type in fabric_tips:
                recommendations.append(fabric_tips[fabric_type])
            
        except Exception as e:
            self.logger.warning(f"ì¶”ì²œì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            recommendations.append("âœ… ê°€ìƒ í”¼íŒ…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return recommendations[:4]  # ìµœëŒ€ 4ê°œ
    
    def _update_performance_stats(self, result: Dict[str, Any]):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.performance_stats['total_processed'] += 1
            
            if result['success']:
                self.performance_stats['successful_fittings'] += 1
            
            # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            total = self.performance_stats['total_processed']
            current_avg = self.performance_stats['average_processing_time']
            new_time = result['processing_time']
            
            self.performance_stats['average_processing_time'] = (
                (current_avg * (total - 1) + new_time) / total
            )
            
        except Exception as e:
            self.logger.warning(f"ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _create_error_response(self, processing_time: float, session_id: str, error_msg: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            "success": False,
            "session_id": session_id,
            "step_name": self.step_name,
            "error_message": error_msg,
            "processing_time": processing_time,
            "fitted_image": None,
            "confidence": 0.0,
            "quality_score": 0.0,
            "overall_score": 0.0,
            "processing_flow": {
                "error": f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}"
            },
            "recommendations": ["ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì…ë ¥ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."]
        }
    
    # ==============================================
    # ğŸ”¥ 12. BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_status(self) -> Dict[str, Any]:
        """Step ìƒíƒœ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'is_initialized': self.is_initialized,
            'is_ready': self.is_ready,
            'device': self.device,
            'ai_models_loaded': list(self.ai_models.keys()),
            'performance_stats': self.performance_stats,
            'dependencies': {
                'model_loader': self.model_loader is not None,
                'memory_manager': self.memory_manager is not None,
                'data_converter': self.data_converter is not None,
                'di_container': self.di_container is not None,
                'step_factory': self.step_factory is not None,
                'step_interface': self.step_interface is not None
            },
            'config': {
                'fitting_mode': self.fitting_mode,
                'use_keypoints': self.config.use_keypoints,
                'use_tps': self.config.use_tps,
                'inference_steps': self.config.inference_steps
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ VirtualFittingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            
            # AI ëª¨ë¸ ì •ë¦¬
            self.ai_models.clear()
            self.model_cache.clear()
            
            # ìºì‹œ ì •ë¦¬
            with self.cache_lock:
                self.result_cache.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            safe_memory_cleanup()
            
            self.logger.info("âœ… VirtualFittingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 13. StepFactory ê¸°ë°˜ ìƒì„± í•¨ìˆ˜ë“¤
# ==============================================

def create_virtual_fitting_step_with_factory(**kwargs) -> Dict[str, Any]:
    """StepFactoryë¥¼ í†µí•œ VirtualFittingStep ìƒì„±"""
    try:
        # StepFactory ê°€ì ¸ì˜¤ê¸°
        step_factory = get_step_factory_dynamic()
        if not step_factory:
            return {
                'success': False,
                'error': 'StepFactoryë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤',
                'step_instance': None
            }
        
        # VirtualFittingStep ìƒì„± ìš”ì²­
        result = step_factory.create_step('virtual_fitting', kwargs)
        
        if result and hasattr(result, 'success') and result.success:
            return {
                'success': True,
                'step_instance': result.step_instance,
                'model_loader': result.model_loader,
                'creation_time': result.creation_time,
                'dependencies_injected': result.dependencies_injected
            }
        else:
            error_msg = getattr(result, 'error_message', 'Unknown error') if result else 'No result'
            return {
                'success': False,
                'error': error_msg,
                'step_instance': None
            }
            
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'step_instance': None
        }

async def create_virtual_fitting_step_with_factory_async(**kwargs) -> Dict[str, Any]:
    """StepFactoryë¥¼ í†µí•œ VirtualFittingStep ë¹„ë™ê¸° ìƒì„±"""
    try:
        step_factory = get_step_factory_dynamic()
        if not step_factory:
            return {
                'success': False,
                'error': 'StepFactoryë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤',
                'step_instance': None
            }
        
        # ë¹„ë™ê¸° ìƒì„±
        result = await step_factory.create_step_async('virtual_fitting', kwargs)
        
        if result and hasattr(result, 'success') and result.success:
            return {
                'success': True,
                'step_instance': result.step_instance,
                'model_loader': result.model_loader,
                'creation_time': result.creation_time,
                'dependencies_injected': result.dependencies_injected
            }
        else:
            error_msg = getattr(result, 'error_message', 'Unknown error') if result else 'No result'
            return {
                'success': False,
                'error': error_msg,
                'step_instance': None
            }
            
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'step_instance': None
        }

# ê¸°ì¡´ ë°©ì‹ í˜¸í™˜ì„ ìœ„í•œ ì§ì ‘ ìƒì„±
def create_virtual_fitting_step(**kwargs):
    """ì§ì ‘ ìƒì„± (ê¸°ì¡´ ë°©ì‹ í˜¸í™˜)"""
    return VirtualFittingStep(**kwargs)

# ==============================================
# ğŸ”¥ 14. í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

async def quick_virtual_fitting_with_factory(
    person_image, clothing_image, 
    fabric_type: str = "cotton", clothing_type: str = "shirt", 
    **kwargs
) -> Dict[str, Any]:
    """StepFactory ê¸°ë°˜ ë¹ ë¥¸ ê°€ìƒ í”¼íŒ…"""
    try:
        # StepFactoryë¡œ Step ìƒì„±
        creation_result = await create_virtual_fitting_step_with_factory_async(
            fitting_mode='high_quality',
            use_keypoints=True,
            use_tps=True,
            **kwargs
        )
        
        if not creation_result['success']:
            return {
                'success': False,
                'error': f"Step ìƒì„± ì‹¤íŒ¨: {creation_result['error']}",
                'processing_time': 0
            }
        
        step = creation_result['step_instance']
        
        try:
            # ê°€ìƒ í”¼íŒ… ì‹¤í–‰
            result = await step.process(
                person_image, clothing_image,
                fabric_type=fabric_type,
                clothing_type=clothing_type,
                **kwargs
            )
            
            return result
            
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            await step.cleanup()
            
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'processing_time': 0
        }

# ==============================================
# ğŸ”¥ 15. ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'VirtualFittingStep',
    
    # AI ëª¨ë¸ ë˜í¼
    'OOTDiffusionWrapper',
    
    # ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤
    'TPSTransform',
    
    # ë°ì´í„° í´ë˜ìŠ¤
    'FittingMethod',
    'FabricProperties', 
    'VirtualFittingConfig',
    'ProcessingResult',
    
    # ìƒìˆ˜
    'FABRIC_PROPERTIES',
    
    # ìƒì„± í•¨ìˆ˜ë“¤
    'create_virtual_fitting_step_with_factory',
    'create_virtual_fitting_step_with_factory_async',
    'create_virtual_fitting_step',
    'quick_virtual_fitting_with_factory',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    'extract_keypoints_from_pose_data',
    'detect_body_keypoints',
    'safe_memory_cleanup',
    'get_step_factory_dynamic',
    'get_virtual_fitting_mixin_dynamic'
]

# ==============================================
# ğŸ”¥ 16. ëª¨ë“ˆ ì •ë³´
# ==============================================

__version__ = "6.3.0-complete-di-stepfactory"
__author__ = "MyCloset AI Team"
__description__ = "Virtual Fitting Step - Complete DI Pattern with StepFactory"

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.info("=" * 90)
logger.info("ğŸ”¥ VirtualFittingStep v6.3.0 - ì™„ì „í•œ DIíŒ¨í„´ + StepFactory ê¸°ë°˜")
logger.info("=" * 90)
logger.info("âœ… StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… â†’ ì™„ì„±ëœ Step")
logger.info("âœ… ì™„ì „í•œ ì²˜ë¦¬ íë¦„:")
logger.info("   1ï¸âƒ£ StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì…")
logger.info("   2ï¸âƒ£ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„± â†’ ê°€ì¤‘ì¹˜ ë¡œë”©")
logger.info("   3ï¸âƒ£ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ â†’ TPS ë³€í˜• ê³„ì‚° â†’ ê¸°í•˜í•™ì  ë³€í˜• ì ìš©")
logger.info("   4ï¸âƒ£ í’ˆì§ˆ í‰ê°€ â†’ ì‹œê°í™” ìƒì„± â†’ API ì‘ë‹µ")
logger.info("âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("âœ… BaseStepMixin ìƒì† + VirtualFittingMixin íŠ¹í™”")
logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  (OOTDiffusion + í‚¤í¬ì¸íŠ¸ + TPS)")
logger.info("âœ… M3 Max 128GB ìµœì í™”")
logger.info("âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
logger.info("")
logger.info("ğŸ§  ì§€ì› AI ëª¨ë¸:")
logger.info("   â€¢ OOTDiffusion - ì‹¤ì œ Diffusion ì¶”ë¡  + í‚¤í¬ì¸íŠ¸ ê°€ì´ë“œ")
logger.info("   â€¢ TPS Transform - Thin Plate Spline ê¸°í•˜í•™ì  ë³€í˜•")
logger.info("   â€¢ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ - OpenPose í˜¸í™˜ 18ê°œ í‚¤í¬ì¸íŠ¸")
logger.info("")
logger.info("ğŸ”— DI íŒ¨í„´ ì˜ì¡´ì„±:")
logger.info("   â€¢ ModelLoader - ì²´í¬í¬ì¸íŠ¸ ë¡œë”©")
logger.info("   â€¢ MemoryManager - ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("   â€¢ DataConverter - ë°ì´í„° ë³€í™˜")
logger.info("   â€¢ DI Container - ì˜ì¡´ì„± ì»¨í…Œì´ë„ˆ")
logger.info("   â€¢ StepFactory - Step ìƒì„± íŒ©í† ë¦¬")
logger.info("   â€¢ StepInterface - Step ì¸í„°í˜ì´ìŠ¤")
logger.info("")
logger.info("ğŸŒŸ ì‚¬ìš© ì˜ˆì‹œ:")
logger.info("   # StepFactory ê¸°ë°˜ ìƒì„±")
logger.info("   result = await create_virtual_fitting_step_with_factory_async()")
logger.info("   step = result['step_instance']")
logger.info("   ")
logger.info("   # ê°€ìƒ í”¼íŒ… ì‹¤í–‰")
logger.info("   fitting_result = await step.process(person_img, cloth_img)")
logger.info("   ")
logger.info("   # ë¹ ë¥¸ ì‚¬ìš©")
logger.info("   result = await quick_virtual_fitting_with_factory(person_img, cloth_img)")
logger.info("")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
logger.info(f"   â€¢ PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
logger.info(f"   â€¢ MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"   â€¢ OpenCV: {'âœ…' if CV2_AVAILABLE else 'âŒ'}")
logger.info(f"   â€¢ SciPy: {'âœ…' if SCIPY_AVAILABLE else 'âŒ'}")
logger.info(f"   â€¢ Diffusers: {'âœ…' if DIFFUSERS_AVAILABLE else 'âŒ'}")
logger.info("=" * 90)

# ==============================================
# ğŸ”¥ 17. í…ŒìŠ¤íŠ¸ ì½”ë“œ (ê°œë°œìš©)
# ==============================================

if __name__ == "__main__":
    async def test_complete_di_stepfactory():
        """ì™„ì „í•œ DIíŒ¨í„´ + StepFactory í…ŒìŠ¤íŠ¸"""
        print("ğŸ”„ ì™„ì „í•œ DIíŒ¨í„´ + StepFactory í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        try:
            # 1. StepFactoryë¥¼ í†µí•œ ìƒì„± í…ŒìŠ¤íŠ¸
            creation_result = await create_virtual_fitting_step_with_factory_async(
                fitting_mode='high_quality',
                use_keypoints=True,
                use_tps=True,
                device='auto'
            )
            
            print(f"âœ… StepFactory ìƒì„± ê²°ê³¼: {creation_result['success']}")
            if not creation_result['success']:
                print(f"âŒ ìƒì„± ì‹¤íŒ¨: {creation_result['error']}")
                return False
            
            step = creation_result['step_instance']
            print(f"âœ… Step ì¸ìŠ¤í„´ìŠ¤: {step.step_name}")
            print(f"âœ… ì˜ì¡´ì„± ì£¼ì…: {creation_result['dependencies_injected']}")
            
            # 2. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
            test_person = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
            test_clothing = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
            
            # 3. ì™„ì „í•œ ì²˜ë¦¬ íë¦„ í…ŒìŠ¤íŠ¸
            print("ğŸ­ ì™„ì „í•œ ì²˜ë¦¬ íë¦„ í…ŒìŠ¤íŠ¸...")
            result = await step.process(
                test_person, test_clothing,
                fabric_type="cotton",
                clothing_type="shirt",
                quality_enhancement=True
            )
            
            print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"   ì„±ê³µ: {result['success']}")
            print(f"   ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
            print(f"   í’ˆì§ˆ ì ìˆ˜: {result['quality_score']:.2f}")
            print(f"   ì „ì²´ ì ìˆ˜: {result['overall_score']:.2f}")
            
            # 4. ì²˜ë¦¬ íë¦„ í™•ì¸
            if 'processing_flow' in result:
                print("ğŸ”„ ì²˜ë¦¬ íë¦„:")
                for step_name, status in result['processing_flow'].items():
                    print(f"   {step_name}: {status}")
            
            # 5. ì„±ëŠ¥ ì •ë³´ í™•ì¸
            if 'performance_info' in result:
                perf = result['performance_info']
                print(f"ğŸ“Š ì„±ëŠ¥ ì •ë³´:")
                print(f"   í‚¤í¬ì¸íŠ¸ ì‚¬ìš©: {perf['keypoint_detection']}")
                print(f"   TPS ë³€í˜•: {perf['tps_transformation']}")
                print(f"   AI ëª¨ë¸: {perf['ai_model_inference']}")
            
            # 6. Step ìƒíƒœ í™•ì¸
            status = step.get_status()
            print(f"ğŸ“‹ Step ìƒíƒœ:")
            print(f"   ì´ˆê¸°í™”: {status['is_initialized']}")
            print(f"   ì¤€ë¹„ë¨: {status['is_ready']}")
            print(f"   AI ëª¨ë¸: {status['ai_models_loaded']}")
            print(f"   ì˜ì¡´ì„±: {sum(status['dependencies'].values())}/6")
            
            # 7. ì •ë¦¬
            await step.cleanup()
            print("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
            print("\nğŸ‰ ì™„ì „í•œ DIíŒ¨í„´ + StepFactory í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            return True
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            print(traceback.format_exc())
            return False
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import asyncio
    asyncio.run(test_complete_di_stepfactory())