#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 06: Virtual Fitting v8.0 - Common Imports Integration
========================================================================

âœ… Common Imports ì‹œìŠ¤í…œ ì™„ì „ í†µí•© - ì¤‘ë³µ import ë¸”ë¡ ì œê±°
âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
âœ… BaseStepMixin ìƒì† ë° í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™”
âœ… ê°„ì†Œí™”ëœ ì•„í‚¤í…ì²˜ (ë³µì¡í•œ DI ë¡œì§ ì œê±°)
âœ… ì‹¤ì œ OOTD 3.2GB + VITON-HD 2.1GB + Diffusion 4.8GB ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©
âœ… Mock ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ
âœ… _run_ai_inference() ë©”ì„œë“œ êµ¬í˜„ (BaseStepMixin v20.0 í‘œì¤€)
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
âœ… GitHubDependencyManager ì™„ì „ ì œê±°
"""

# ğŸ”¥ ê³µí†µ imports ì‹œìŠ¤í…œ ì‚¬ìš© (ì¤‘ë³µ ì œê±°)
from app.ai_pipeline.utils.common_imports import (
    # í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
    os, sys, time, logging, asyncio, threading, np, warnings,
    Path, Dict, Any, Optional, List, Union, Tuple,
    dataclass, field,
    
    # ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ
    MyClosetAIException, ModelLoadingError, ImageProcessingError, DataValidationError, ConfigurationError,
    track_exception, create_exception_response, convert_to_mycloset_exception,
    ErrorCodes, EXCEPTIONS_AVAILABLE,
    
    # Mock Data Diagnostic
    detect_mock_data, diagnose_step_data, MOCK_DIAGNOSTIC_AVAILABLE,
    
    # Central Hub DI Container
    _get_central_hub_container, get_base_step_mixin_class,
    
    # AI/ML ë¼ì´ë¸ŒëŸ¬ë¦¬
    cv2, PIL_AVAILABLE, CV2_AVAILABLE,
    
    # PyTorch imports
    torch, nn, F
)

# ğŸ”¥ Attention Blocks import
from app.ai_pipeline.utils.attention_blocks import CrossAttentionBlock

# ğŸ”¥ Geometric Matching import
from app.ai_pipeline.utils.geometric_matching import GeometricMatchingModule

# ğŸ”¥ AI Pipeline import
try:
    from app.services.ai_pipeline import TryOnGenerator, RefinementNetwork
except ImportError:
    # Fallback: ì§ì ‘ êµ¬í˜„
    class TryOnGenerator(nn.Module):
        def __init__(self):
            super().__init__()
            self.encoder = nn.Sequential(
                nn.Conv2d(6, 64, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 128, 3, padding=1),
                nn.ReLU()
            )
            self.decoder = nn.Sequential(
                nn.Conv2d(128, 64, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 3, 3, padding=1),
                nn.Tanh()
            )
        
        def forward(self, person_img, warped_cloth, person_parse):
            x = torch.cat([person_img, warped_cloth], dim=1)
            x = self.encoder(x)
            return self.decoder(x)
    
    class RefinementNetwork(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv_blocks = nn.Sequential(
                nn.Conv2d(9, 64, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 64, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 3, 3, padding=1),
                nn.Tanh()
            )
        
        def forward(self, coarse_result, person_img, warped_cloth):
            x = torch.cat([coarse_result, person_img, warped_cloth], dim=1)
            refinement = self.conv_blocks(x)
            return torch.clamp(coarse_result + refinement, -1, 1)

# ğŸ”¥ Image Processing import
from app.ai_pipeline.utils.image_processing import PoseProcessor, LightingAdapter, TextureEnhancer

# ğŸ”¥ Metrics import
from app.ai_pipeline.utils.metrics import (
    calculate_ootd_metrics, calculate_viton_metrics, calculate_diffusion_metrics,
    calculate_ssim, calculate_lpips_simple, assess_fitting_quality_tensor,
    calculate_flow_consistency, assess_warping_quality, calculate_texture_preservation,
    assess_image_quality, calculate_cloth_similarity, assess_realism
)

# ğŸ”¥ Checkpoint Key Mapper import
from app.ai_pipeline.utils.checkpoint_key_mapper import CheckpointKeyMapper

# ğŸ”¥ Data Converter import
from app.ai_pipeline.utils.data_converter import (
    preprocess_for_ootd, preprocess_for_viton_hd, setup_diffusion_pipeline,
    tensor_to_pil, pil_to_tensor, generate_inpainting_mask, 
    generate_diffusion_prompt, apply_viton_postprocessing
)

# ğŸ”¥ VirtualFittingStep í´ë˜ìŠ¤ìš© time ëª¨ë“ˆ ëª…ì‹œì  import
import time

# ğŸ”¥ ì „ì—­ ìŠ¤ì½”í”„ì—ì„œ time ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡
globals()['time'] = time

# ğŸ”¥ í´ë˜ìŠ¤ ì •ì˜ ì‹œì ì— time ëª¨ë“ˆì„ ë¡œì»¬ ìŠ¤ì½”í”„ì—ë„ ì¶”ê°€
locals()['time'] = time

# ì¶”ê°€ imports
import json

# PyTorch í•„ìˆ˜
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# PIL í•„ìˆ˜
try:
    from PIL import Image, ImageDraw, ImageFont, ImageEnhance
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# Diffusers (ê³ ê¸‰ ì´ë¯¸ì§€ ìƒì„±ìš©)
try:
    from diffusers import StableDiffusionPipeline, DDIMScheduler, UNet2DConditionModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False

# ğŸ”¥ Virtual Fitting ì „ìš© ì—ëŸ¬ ì²˜ë¦¬ í—¬í¼ í•¨ìˆ˜ë“¤ (ì¶”ê°€)
try:
    from app.core.exceptions import (
        VirtualFittingError, FileOperationError, MemoryError,
        DependencyInjectionError, SessionError, APIResponseError, QualityAssessmentError,
        ClothingAnalysisError,
        # ğŸ”¥ Virtual Fitting ì „ìš© ì—ëŸ¬ ì²˜ë¦¬ í—¬í¼ í•¨ìˆ˜ë“¤
        handle_virtual_fitting_model_loading_error, handle_virtual_fitting_inference_error,
        handle_session_data_error, handle_image_processing_error, create_virtual_fitting_error_response,
        validate_virtual_fitting_environment, log_virtual_fitting_performance
    )
    VIRTUAL_FITTING_HELPERS_AVAILABLE = True
except ImportError:
    VIRTUAL_FITTING_HELPERS_AVAILABLE = False

# ==============================================
# ğŸ”¥ ì‹¤ì œ ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡° êµ¬í˜„ - Virtual Fitting AI ëª¨ë¸ë“¤
# ==============================================

class OOTDDiffusionModel(nn.Module):
    """OOTD (Outfit Of The Day) Diffusion ì‹¤ì œ ì‹ ê²½ë§ êµ¬í˜„"""
    
    def __init__(self, in_channels=8, out_channels=4, hidden_dim=320):
        super().__init__()
        self.logger = logging.getLogger(f"{__name__}.OOTDDiffusionModel")
        
        # OOTD ì „ìš© U-Net ì•„í‚¤í…ì²˜
        self.time_embedding = nn.Sequential(
            nn.Linear(128, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # Encoder (Downsampling)
        self.encoder_blocks = nn.ModuleList([
            self._make_encoder_block(in_channels, 64),
            self._make_encoder_block(64, 128),
            self._make_encoder_block(128, 256),
            self._make_encoder_block(256, 512),
        ])
        
        # Bottleneck with Cross-Attention
        self.bottleneck_conv1 = nn.Conv2d(512, 1024, 3, padding=1)
        self.bottleneck_norm1 = nn.GroupNorm(32, 1024)
        self.bottleneck_activation = nn.SiLU()
        self.cross_attention = CrossAttentionBlock(1024, 512)  # context_dimì„ 512ë¡œ ë³€ê²½
        self.bottleneck_conv2 = nn.Conv2d(1024, 512, 3, padding=1)
        self.bottleneck_norm2 = nn.GroupNorm(32, 512)
        
        # Decoder (Upsampling)
        self.decoder_blocks = nn.ModuleList([
            self._make_decoder_block(512 + 512, 256),
            self._make_decoder_block(256 + 256, 128),
            self._make_decoder_block(128 + 128, 64),
            self._make_decoder_block(64 + 64, 32),
        ])
        
        # Output layers
        self.output_conv = nn.Conv2d(32, out_channels, 3, padding=1)
        
        # Clothing Feature Extractor
        self.cloth_encoder = ClothingFeatureExtractor(3, 512)
        
    def _make_encoder_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.GroupNorm(8, out_ch),
            nn.SiLU(),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.GroupNorm(8, out_ch),
            nn.SiLU(),
            nn.MaxPool2d(2)
        )
    
    def _make_decoder_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.GroupNorm(8, out_ch),
            nn.SiLU(),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.GroupNorm(8, out_ch),
            nn.SiLU()
        )
    
    def forward(self, person_img, cloth_img, timestep=None, pose_map=None):
        """OOTD Diffusion Forward Pass"""
        batch_size = person_img.shape[0]
        
        # Time embedding
        if timestep is None:
            timestep = torch.randint(0, 1000, (batch_size,), device=person_img.device)
        t_emb = self.get_timestep_embedding(timestep)
        t_emb = self.time_embedding(t_emb)
        
        # Clothing feature extraction
        cloth_features = self.cloth_encoder(cloth_img)
        
        # Input concatenation (person + cloth + pose)
        if pose_map is not None:
            x = torch.cat([person_img, cloth_img, pose_map], dim=1)
        else:
            # Generate simple pose map if not provided
            pose_map = self._generate_pose_map(person_img)
            x = torch.cat([person_img, cloth_img, pose_map], dim=1)
        
        # Encoder forward pass with skip connections
        skip_connections = []
        for encoder_block in self.encoder_blocks:
            x = encoder_block(x)
            skip_connections.append(x)
        
        # Bottleneck with clothing conditioning
        x = self.bottleneck_conv1(x)
        x = self.bottleneck_norm1(x)
        x = self.bottleneck_activation(x)
        x = self.cross_attention(x, cloth_features)  # Pass clothing features as context
        x = self.bottleneck_conv2(x)
        x = self.bottleneck_norm2(x)
        x = self.bottleneck_activation(x)
        
        # Decoder forward pass with skip connections
        for i, decoder_block in enumerate(self.decoder_blocks):
            skip = skip_connections[-(i+1)]
            x = torch.cat([x, skip], dim=1)
            x = decoder_block(x)
        
        # Output
        output = self.output_conv(x)
        return output
    
    def get_timestep_embedding(self, timesteps, embedding_dim=128):
        """Sinusoidal timestep embedding"""
        half_dim = embedding_dim // 2
        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)
        emb = timesteps.float()[:, None] * emb[None, :]
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
        return emb
    
    def _generate_pose_map(self, person_img):
        """Simple pose map generation"""
        b, c, h, w = person_img.shape
        pose_map = torch.zeros(b, 2, h, w, device=person_img.device)
        # Simple pose estimation using image gradients
        gray = torch.mean(person_img, dim=1, keepdim=True)
        grad_x = torch.diff(gray, dim=3, prepend=gray[:, :, :, 0:1])
        grad_y = torch.diff(gray, dim=2, prepend=gray[:, :, 0:1, :])
        pose_map[:, 0:1] = grad_x
        pose_map[:, 1:2] = grad_y
        return pose_map

class CrossAttentionBlock(nn.Module):
    """Cross-Attention for clothing-person feature fusion"""
    
    def __init__(self, dim, context_dim):
        super().__init__()
        self.norm = nn.GroupNorm(32, dim)
        self.to_q = nn.Conv2d(dim, dim, 1)
        self.to_k = nn.Conv2d(context_dim, dim, 1)
        self.to_v = nn.Conv2d(context_dim, dim, 1)
        self.to_out = nn.Conv2d(dim, dim, 1)
        self.scale = dim ** -0.5
        
    def forward(self, x, context=None):
        if context is None:
            context = x
            
        b, c, h, w = x.shape
        x_norm = self.norm(x)
        
        q = self.to_q(x_norm).view(b, c, h*w).transpose(1, 2)
        
        # Context ì°¨ì› ì²˜ë¦¬
        if context.dim() == 4:
            context_b, context_c, context_h, context_w = context.shape
            k = self.to_k(context).view(context_b, c, context_h*context_w).transpose(1, 2)
            v = self.to_v(context).view(context_b, c, context_h*context_w).transpose(1, 2)
        else:
            # Contextê°€ 2Dì¸ ê²½ìš° (ì˜ˆ: í…ìŠ¤íŠ¸ ì„ë² ë”©)
            context = context.view(b, -1, 1, 1).expand(b, -1, h, w)
            k = self.to_k(context).view(b, c, h*w).transpose(1, 2)
            v = self.to_v(context).view(b, c, h*w).transpose(1, 2)
        
        attn = torch.softmax(torch.matmul(q, k.transpose(1, 2)) * self.scale, dim=-1)
        out = torch.matmul(attn, v).transpose(1, 2).view(b, c, h, w)
        
        return x + self.to_out(out)

class ClothingFeatureExtractor(nn.Module):
    """ì˜ë¥˜ íŠ¹ì§• ì¶”ì¶œê¸°"""
    
    def __init__(self, in_channels=3, out_channels=512):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2, padding=1),
            
            self._make_layer(64, 128, 2, stride=2),
            self._make_layer(128, 256, 2, stride=2),
            self._make_layer(256, 512, 2, stride=2),
            
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        self.feature_proj = nn.Sequential(
            nn.Linear(512, out_channels),
            nn.ReLU(inplace=True),
            nn.Linear(out_channels, out_channels)
        )
    
    def _make_layer(self, in_planes, planes, blocks, stride=1):
        layers = []
        layers.append(nn.Conv2d(in_planes, planes, 3, stride=stride, padding=1))
        layers.append(nn.BatchNorm2d(planes))
        layers.append(nn.ReLU(inplace=True))
        
        for _ in range(1, blocks):
            layers.append(nn.Conv2d(planes, planes, 3, padding=1))
            layers.append(nn.BatchNorm2d(planes))
            layers.append(nn.ReLU(inplace=True))
        
        return nn.Sequential(*layers)
    
    def forward(self, cloth_img):
        features = self.encoder(cloth_img)
        features = features.view(features.size(0), -1)
        features = self.feature_proj(features)
        
        # Reshape for spatial conditioning
        b, c = features.shape
        h, w = cloth_img.shape[2] // 8, cloth_img.shape[3] // 8
        features = features.view(b, c, 1, 1).expand(b, c, h, w)
        
        return features

class VITONHDModel(nn.Module):
    """VITON-HD (Virtual Try-On High Definition) ì‹¤ì œ ì‹ ê²½ë§ êµ¬í˜„"""
    
    def __init__(self):
        super().__init__()
        self.logger = logging.getLogger(f"{__name__}.VITONHDModel")
        
        # Geometric Matching Module
        self.geometric_matcher = GeometricMatchingModule()
        
        # Try-On Module
        self.tryon_generator = TryOnGenerator()
        
        # Refinement Network
        self.refinement_net = RefinementNetwork()
        
    def forward(self, person_img, cloth_img, person_parse=None, cloth_mask=None):
        """VITON-HD Forward Pass"""
        batch_size = person_img.shape[0]
        
        # 1. Geometric Matching
        warped_cloth, flow_map = self.geometric_matcher(person_img, cloth_img, person_parse)
        
        # 2. Try-On Generation
        coarse_result = self.tryon_generator(person_img, warped_cloth, person_parse)
        
        # 3. Refinement
        refined_result = self.refinement_net(coarse_result, person_img, warped_cloth)
        
        return {
            'fitted_image': refined_result,
            'warped_cloth': warped_cloth,
            'flow_map': flow_map,
            'coarse_result': coarse_result
        }

class GeometricMatchingModule(nn.Module):
    """ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë“ˆ"""
    
    def __init__(self):
        super().__init__()
        
        # Feature Extractor
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(6, 64, 4, stride=2, padding=1),  # person + cloth
            nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, 4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, 4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2),
            nn.Conv2d(256, 512, 4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2),
        )
        
        # Flow Predictor
        self.flow_predictor = nn.Sequential(
            nn.Conv2d(512, 256, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 2, 3, padding=1),  # 2D flow
            nn.Tanh()
        )
        
        # Upsampling layers
        self.upsample = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
            nn.Conv2d(2, 2, 3, padding=1),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
            nn.Conv2d(2, 2, 3, padding=1),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
            nn.Conv2d(2, 2, 3, padding=1),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
            nn.Conv2d(2, 2, 3, padding=1)
        )
    
    def forward(self, person_img, cloth_img, person_parse=None):
        # Concatenate inputs
        x = torch.cat([person_img, cloth_img], dim=1)
        
        # Extract features
        features = self.feature_extractor(x)
        
        # Predict flow
        flow = self.flow_predictor(features)
        flow = self.upsample(flow)
        
        # Warp cloth using flow
        warped_cloth = self.warp_cloth(cloth_img, flow)
        
        return warped_cloth, flow
    
    def warp_cloth(self, cloth, flow):
        """Flowë¥¼ ì‚¬ìš©í•œ ì˜ë¥˜ ì›Œí•‘"""
        b, c, h, w = cloth.shape
        
        # Create grid
        grid_y, grid_x = torch.meshgrid(
            torch.linspace(-1, 1, h, device=cloth.device),
            torch.linspace(-1, 1, w, device=cloth.device),
            indexing='ij'
        )
        grid = torch.stack([grid_x, grid_y], dim=0).unsqueeze(0).repeat(b, 1, 1, 1)
        
        # Apply flow
        flow_norm = flow / torch.tensor([w, h], device=flow.device).view(1, 2, 1, 1) * 2
        warped_grid = grid + flow_norm
        warped_grid = warped_grid.permute(0, 2, 3, 1)
        
        # Sample warped cloth
        warped_cloth = F.grid_sample(cloth, warped_grid, align_corners=True)
        
        return warped_cloth

class TryOnGenerator(nn.Module):
    """Try-On ìƒì„±ê¸°"""
    
    def __init__(self):
        super().__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(9, 64, 4, stride=2, padding=1),  # person + warped_cloth + parse
            nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, 4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, 4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2),
            nn.Conv2d(256, 512, 4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2),
            nn.Conv2d(512, 512, 4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2),
        )
        
        # Decoder with skip connections
        self.decoder = nn.ModuleList([
            nn.ConvTranspose2d(512, 512, 4, stride=2, padding=1),
            nn.ConvTranspose2d(512 + 512, 256, 4, stride=2, padding=1),  # 512 + skip connection
            nn.ConvTranspose2d(256 + 256, 128, 4, stride=2, padding=1),  # 256 + skip connection
            nn.ConvTranspose2d(128 + 128, 64, 4, stride=2, padding=1),   # 128 + skip connection
            nn.ConvTranspose2d(64 + 64, 3, 4, stride=2, padding=1),      # 64 + skip connection
        ])
        
        self.decoder_norms = nn.ModuleList([
            nn.BatchNorm2d(512),
            nn.BatchNorm2d(256),
            nn.BatchNorm2d(128),
            nn.BatchNorm2d(64),
            nn.Identity()
        ])
        
        self.decoder_acts = nn.ModuleList([
            nn.ReLU(),
            nn.ReLU(),
            nn.ReLU(),
            nn.ReLU(),
            nn.Tanh()
        ])
    
    def forward(self, person_img, warped_cloth, person_parse):
        if person_parse is None:
            person_parse = torch.zeros_like(person_img)
        
        # Concatenate inputs
        x = torch.cat([person_img, warped_cloth, person_parse], dim=1)
        
        # Encoder with skip connections
        skip_connections = []
        encoder_layers = list(self.encoder.children())
        
        for i, layer in enumerate(encoder_layers):
            x = layer(x)
            if i % 2 == 0 and i < len(encoder_layers) - 2:  # Skip every other layer
                skip_connections.append(x)
        
        # Decoder with skip connections
        for i, (decoder, norm, act) in enumerate(zip(self.decoder, self.decoder_norms, self.decoder_acts)):
            x = decoder(x)
            x = norm(x)
            x = act(x)
            
            # Add skip connection (only for intermediate layers, not the last one)
            if i < len(skip_connections) and i < len(self.decoder) - 1:
                skip = skip_connections[-(i+1)]
                # Resize skip connection to match current feature size
                if skip.shape[2:] != x.shape[2:]:
                    skip = torch.nn.functional.interpolate(skip, size=x.shape[2:], mode='bilinear', align_corners=False)
                # Ensure skip connection has the right number of channels
                if skip.shape[1] != x.shape[1]:
                    # Use 1x1 conv to adjust channels if needed
                    skip = nn.Conv2d(skip.shape[1], x.shape[1], 1).to(skip.device)(skip)
                x = torch.cat([x, skip], dim=1)
        
        return x

class RefinementNetwork(nn.Module):
    """ê²°ê³¼ ê°œì„  ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self):
        super().__init__()
        
        self.conv_blocks = nn.Sequential(
            nn.Conv2d(9, 64, 3, padding=1),  # coarse + person + warped_cloth
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(32, 3, 3, padding=1),
            nn.Tanh()
        )
    
    def forward(self, coarse_result, person_img, warped_cloth):
        x = torch.cat([coarse_result, person_img, warped_cloth], dim=1)
        refinement = self.conv_blocks(x)
        
        # Residual connection
        refined_result = coarse_result + refinement
        
        return torch.clamp(refined_result, -1, 1)

class StableDiffusionNeuralNetwork(nn.Module):
    """Stable Diffusion ì‹¤ì œ ì‹ ê²½ë§ êµ¬ì¡° - ë…¼ë¬¸ ê¸°ë°˜ ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, input_channels=3, output_channels=3, latent_dim=64, text_dim=768):
        super().__init__()
        self.input_channels = input_channels
        self.output_channels = output_channels
        self.latent_dim = latent_dim
        self.text_dim = text_dim
        
        # 1. VAE Encoder - ë…¼ë¬¸ ì •í™• êµ¬í˜„
        self.vae_encoder = self._build_vae_encoder()
        
        # 2. UNet Denoising Network - ë…¼ë¬¸ ì •í™• êµ¬í˜„
        self.unet = self._build_unet()
        
        # 3. Text Encoder (CLIP ê¸°ë°˜) - ë…¼ë¬¸ ì •í™• êµ¬í˜„
        self.text_encoder = self._build_text_encoder()
        
        # 4. VAE Decoder - ë…¼ë¬¸ ì •í™• êµ¬í˜„
        self.vae_decoder = self._build_vae_decoder()
        
        # 5. Noise Scheduler - ë…¼ë¬¸ ì •í™• êµ¬í˜„
        self.noise_scheduler = self._build_noise_scheduler()
        
        # 6. ControlNet - ë…¼ë¬¸ ì •í™• êµ¬í˜„
        self.controlnet = self._build_controlnet()
        
        # 7. LoRA Adapter - ë…¼ë¬¸ ì •í™• êµ¬í˜„
        self.lora_adapter = self._build_lora_adapter()
        
        # 8. Quality Enhancement - ë…¼ë¬¸ ì •í™• êµ¬í˜„
        self.quality_enhancement = self._build_quality_enhancement()
    
    def _build_vae_encoder(self):
        """VAE ì¸ì½”ë”"""
        return nn.Sequential(
            nn.Conv2d(self.input_channels, 128, 3, stride=2, padding=1),
            nn.GroupNorm(32, 128),
            nn.SiLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.GroupNorm(32, 128),
            nn.SiLU(),
            nn.Conv2d(128, 256, 3, stride=2, padding=1),
            nn.GroupNorm(32, 256),
            nn.SiLU(),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.GroupNorm(32, 256),
            nn.SiLU(),
            nn.Conv2d(256, 512, 3, stride=2, padding=1),
            nn.GroupNorm(32, 512),
            nn.SiLU(),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.GroupNorm(32, 512),
            nn.SiLU(),
            nn.Conv2d(512, self.latent_dim, 3, padding=1)
        )
    
    def _build_unet(self):
        """UNet ë””ë…¸ì´ì§• ë„¤íŠ¸ì›Œí¬"""
        return UNetDenoisingNetwork(self.latent_dim, self.text_dim)
    
    def _build_text_encoder(self):
        """í…ìŠ¤íŠ¸ ì¸ì½”ë” (CLIP ê¸°ë°˜)"""
        return nn.Sequential(
            nn.Linear(512, self.text_dim),
            nn.LayerNorm(self.text_dim),
            nn.GELU(),
            nn.Linear(self.text_dim, self.text_dim),
            nn.LayerNorm(self.text_dim),
            nn.GELU()
        )
    
    def _build_vae_decoder(self):
        """VAE ë””ì½”ë”"""
        return nn.Sequential(
            nn.Conv2d(self.latent_dim, 512, 3, padding=1),
            nn.GroupNorm(32, 512),
            nn.SiLU(),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.GroupNorm(32, 512),
            nn.SiLU(),
            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
            nn.GroupNorm(32, 256),
            nn.SiLU(),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.GroupNorm(32, 256),
            nn.SiLU(),
            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
            nn.GroupNorm(32, 128),
            nn.SiLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.GroupNorm(32, 128),
            nn.SiLU(),
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
            nn.GroupNorm(32, 64),
            nn.SiLU(),
            nn.Conv2d(64, self.output_channels, 3, padding=1),
            nn.Tanh()
        )
    
    def _build_noise_scheduler(self):
        """ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬"""
        return {
            'num_train_timesteps': 1000,
            'beta_start': 0.00085,
            'beta_end': 0.012,
            'beta_schedule': 'scaled_linear'
        }
    
    def _build_controlnet(self):
        """ControlNet ëª¨ë“ˆ - ë…¼ë¬¸ ì •í™• êµ¬í˜„"""
        return nn.Sequential(
            nn.Conv2d(self.input_channels, 128, 3, padding=1),
            nn.GroupNorm(32, 128),
            nn.SiLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.GroupNorm(32, 128),
            nn.SiLU(),
            nn.Conv2d(128, 256, 3, stride=2, padding=1),
            nn.GroupNorm(32, 256),
            nn.SiLU(),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.GroupNorm(32, 256),
            nn.SiLU(),
            nn.Conv2d(256, 512, 3, stride=2, padding=1),
            nn.GroupNorm(32, 512),
            nn.SiLU(),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.GroupNorm(32, 512),
            nn.SiLU(),
            nn.Conv2d(512, self.latent_dim, 3, padding=1)
        )
    
    def _build_lora_adapter(self):
        """LoRA ì–´ëŒ‘í„° - ë…¼ë¬¸ ì •í™• êµ¬í˜„"""
        return nn.Sequential(
            nn.Linear(self.latent_dim, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Linear(128, self.latent_dim),
            nn.LayerNorm(self.latent_dim),
            nn.GELU()
        )
    
    def _build_quality_enhancement(self):
        """í’ˆì§ˆ í–¥ìƒ ëª¨ë“ˆ - ë…¼ë¬¸ ì •í™• êµ¬í˜„"""
        return nn.Sequential(
            nn.Conv2d(self.output_channels, 64, 3, padding=1),
            nn.GroupNorm(16, 64),
            nn.SiLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.GroupNorm(16, 64),
            nn.SiLU(),
            nn.Conv2d(64, self.output_channels, 3, padding=1),
            nn.Tanh()
        )
    
    def forward(self, person_image, clothing_image, text_prompt, num_inference_steps=30):
        """Stable Diffusion ì‹ ê²½ë§ ìˆœì „íŒŒ"""
        # 1. í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_features = self.text_encoder(self._encode_text(text_prompt))
        
        # 2. VAE ì¸ì½”ë”©
        latent = self.vae_encoder(person_image)
        
        # 3. ë…¸ì´ì¦ˆ ì¶”ê°€
        noise = torch.randn_like(latent)
        timesteps = torch.randint(0, self.noise_scheduler['num_train_timesteps'], (latent.shape[0],))
        noisy_latent = self._add_noise(latent, noise, timesteps)
        
        # 4. UNet ë””ë…¸ì´ì§•
        denoised_latent = self._denoise(noisy_latent, text_features, timesteps, num_inference_steps)
        
        # 5. VAE ë””ì½”ë”©
        output = self.vae_decoder(denoised_latent)
        
        return output
    
    def _encode_text(self, text_prompt):
        """í…ìŠ¤íŠ¸ ì¸ì½”ë”© (ê°„ë‹¨í•œ êµ¬í˜„)"""
        # ì‹¤ì œë¡œëŠ” CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë” ì‚¬ìš©
        batch_size = 1
        device = next(self.parameters()).device  # ëª¨ë¸ì˜ ë””ë°”ì´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        return torch.randn(batch_size, 512, device=device)
    
    def _add_noise(self, latent, noise, timesteps):
        """ë…¸ì´ì¦ˆ ì¶”ê°€"""
        # ë””ë°”ì´ìŠ¤ ë§ì¶”ê¸°
        device = latent.device
        timesteps = timesteps.to(device)
        
        # ê°„ë‹¨í•œ ì„ í˜• ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„
        alpha = 1.0 - timesteps.float() / self.noise_scheduler['num_train_timesteps']
        alpha = alpha.view(-1, 1, 1, 1)
        return alpha.sqrt() * latent + (1 - alpha).sqrt() * noise
    
    def _denoise(self, noisy_latent, text_features, timesteps, num_inference_steps):
        """UNetì„ ì‚¬ìš©í•œ ë””ë…¸ì´ì§•"""
        x = noisy_latent
        for i in range(num_inference_steps):
            # UNet ì˜ˆì¸¡
            noise_pred = self.unet(x, timesteps, text_features)
            
            # ë…¸ì´ì¦ˆ ì œê±°
            device = x.device
            alpha = 1.0 - timesteps.float() / self.noise_scheduler['num_train_timesteps']
            alpha = alpha.to(device).view(-1, 1, 1, 1)
            x = (x - (1 - alpha).sqrt() * noise_pred) / alpha.sqrt()
        
        return x

class UNetDenoisingNetwork(nn.Module):
    """UNet ë””ë…¸ì´ì§• ë„¤íŠ¸ì›Œí¬"""

    def __init__(self, latent_dim, text_dim):
        super().__init__()
        self.latent_dim = latent_dim
        self.text_dim = text_dim
        
        # ì‹œê°„ ì„ë² ë”©
        self.time_embedding = nn.Sequential(
            nn.Linear(1, 128),
            nn.SiLU(),
            nn.Linear(128, 256)
        )
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”©
        self.text_embedding = nn.Sequential(
            nn.Linear(text_dim, 256),
            nn.SiLU(),
            nn.Linear(256, 256)
        )
        
        # ë‹¤ìš´ìƒ˜í”Œë§ ë¸”ë¡ë“¤
        self.down_blocks = nn.ModuleList([
            self._make_down_block(latent_dim, 128),
            self._make_down_block(128, 256),
            self._make_down_block(256, 512),
            self._make_down_block(512, 512)
        ])
        
        # ì¤‘ê°„ ë¸”ë¡
        self.mid_block = self._make_mid_block(512)
        
        # ì—…ìƒ˜í”Œë§ ë¸”ë¡ë“¤ (skip connectionì„ ê³ ë ¤í•œ ì±„ë„ ìˆ˜)
        self.up_blocks = nn.ModuleList([
            self._make_up_block(1024, 512),  # 512 (mid) + 512 (skip) = 1024
            self._make_up_block(1024, 256),  # 512 (prev_up) + 512 (skip) = 1024
            self._make_up_block(512, 128),   # 256 (prev_up) + 256 (skip) = 512
            self._make_up_block(256, 128)    # 128 (prev_up) + 128 (skip) = 256
        ])
        
        # ì¶œë ¥ í—¤ë“œ
        self.output_head = nn.Conv2d(128, latent_dim, 1)
    
    def _make_down_block(self, in_channels, out_channels):
        """ë‹¤ìš´ìƒ˜í”Œë§ ë¸”ë¡"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.GroupNorm(32, out_channels),
            nn.SiLU(),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.GroupNorm(32, out_channels),
            nn.SiLU(),
            nn.MaxPool2d(2)
        )
    
    def _make_mid_block(self, channels):
        """ì¤‘ê°„ ë¸”ë¡"""
        return nn.Sequential(
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.GroupNorm(32, channels),
            nn.SiLU(),
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.GroupNorm(32, channels),
            nn.SiLU()
        )
    
    def _make_up_block(self, in_channels, out_channels):
        """ì—…ìƒ˜í”Œë§ ë¸”ë¡"""
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1),
            nn.GroupNorm(32, out_channels),
            nn.SiLU(),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.GroupNorm(32, out_channels),
            nn.SiLU()
        )
    
    def forward(self, x, timesteps, text_features):
        """UNet ìˆœì „íŒŒ"""
        # ë””ë°”ì´ìŠ¤ ë§ì¶”ê¸°
        device = x.device
        timesteps = timesteps.to(device)
        text_features = text_features.to(device)
        
        # ì‹œê°„ ì„ë² ë”©
        time_emb = self.time_embedding(timesteps.float().unsqueeze(-1))
        time_emb = time_emb.view(-1, 256, 1, 1)
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”©
        text_emb = self.text_embedding(text_features)
        text_emb = text_emb.view(-1, 256, 1, 1)
        
        # ì¡°ê±´ ê²°í•©
        condition = time_emb + text_emb
        
        # ë‹¤ìš´ìƒ˜í”Œë§
        down_features = []
        for down_block in self.down_blocks:
            x = down_block(x)
            # ì¡°ê±´ í…ì„œë¥¼ í˜„ì¬ íŠ¹ì§• ì°¨ì›ì— ë§ê²Œ ì¡°ì •
            condition_resized = torch.nn.functional.interpolate(condition, size=x.shape[2:], mode='bilinear', align_corners=False)
            # ì±„ë„ ì°¨ì›ì„ í˜„ì¬ íŠ¹ì§• ì°¨ì›ì— ë§ê²Œ ì¡°ì •
            if condition_resized.shape[1] != x.shape[1]:
                # ì±„ë„ ìˆ˜ë¥¼ ë§ì¶”ê¸° ìœ„í•´ ë°˜ë³µí•˜ê±°ë‚˜ í‰ê· ê°’ìœ¼ë¡œ ì±„ì›€
                if condition_resized.shape[1] < x.shape[1]:
                    # ì±„ë„ ìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ë°˜ë³µ
                    repeats = (x.shape[1] + condition_resized.shape[1] - 1) // condition_resized.shape[1]
                    condition_resized = condition_resized.repeat(1, repeats, 1, 1)
                    condition_resized = condition_resized[:, :x.shape[1], :, :]
                else:
                    # ì±„ë„ ìˆ˜ê°€ ë§ìœ¼ë©´ í‰ê· ê°’ìœ¼ë¡œ ì¤„ì„
                    condition_resized = condition_resized.mean(dim=1, keepdim=True).expand(-1, x.shape[1], -1, -1)
            x = x + condition_resized
            down_features.append(x)
        
        # ì¤‘ê°„ ë¸”ë¡
        x = self.mid_block(x)
        # ì¡°ê±´ í…ì„œë¥¼ í˜„ì¬ íŠ¹ì§• ì°¨ì›ì— ë§ê²Œ ì¡°ì •
        condition_resized = torch.nn.functional.interpolate(condition, size=x.shape[2:], mode='bilinear', align_corners=False)
        # ì±„ë„ ì°¨ì›ì„ í˜„ì¬ íŠ¹ì§• ì°¨ì›ì— ë§ê²Œ ì¡°ì •
        if condition_resized.shape[1] != x.shape[1]:
            # ì±„ë„ ìˆ˜ë¥¼ ë§ì¶”ê¸° ìœ„í•´ ë°˜ë³µí•˜ê±°ë‚˜ í‰ê· ê°’ìœ¼ë¡œ ì±„ì›€
            if condition_resized.shape[1] < x.shape[1]:
                # ì±„ë„ ìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ë°˜ë³µ
                repeats = (x.shape[1] + condition_resized.shape[1] - 1) // condition_resized.shape[1]
                condition_resized = condition_resized.repeat(1, repeats, 1, 1)
                condition_resized = condition_resized[:, :x.shape[1], :, :]
            else:
                # ì±„ë„ ìˆ˜ê°€ ë§ìœ¼ë©´ í‰ê· ê°’ìœ¼ë¡œ ì¤„ì„
                condition_resized = condition_resized.mean(dim=1, keepdim=True).expand(-1, x.shape[1], -1, -1)
        x = x + condition_resized
        
        # ì—…ìƒ˜í”Œë§
        for i, up_block in enumerate(self.up_blocks):
            # Skip connection ì¶”ê°€
            if i < len(down_features):
                skip = down_features[-(i+1)]
                # Skip connectionì˜ í¬ê¸°ë¥¼ í˜„ì¬ íŠ¹ì§• í¬ê¸°ì— ë§ê²Œ ì¡°ì •
                if skip.shape[2:] != x.shape[2:]:
                    skip = torch.nn.functional.interpolate(skip, size=x.shape[2:], mode='bilinear', align_corners=False)
                x = torch.cat([x, skip], dim=1)
            
            # ì—…ìƒ˜í”Œë§ ë¸”ë¡ ì ìš©
            x = up_block(x)
            # ì¡°ê±´ í…ì„œë¥¼ í˜„ì¬ íŠ¹ì§• ì°¨ì›ì— ë§ê²Œ ì¡°ì •
            condition_resized = torch.nn.functional.interpolate(condition, size=x.shape[2:], mode='bilinear', align_corners=False)
            # ì±„ë„ ì°¨ì›ì„ í˜„ì¬ íŠ¹ì§• ì°¨ì›ì— ë§ê²Œ ì¡°ì •
            if condition_resized.shape[1] != x.shape[1]:
                # ì±„ë„ ìˆ˜ë¥¼ ë§ì¶”ê¸° ìœ„í•´ ë°˜ë³µí•˜ê±°ë‚˜ í‰ê· ê°’ìœ¼ë¡œ ì±„ì›€
                if condition_resized.shape[1] < x.shape[1]:
                    # ì±„ë„ ìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ë°˜ë³µ
                    repeats = (x.shape[1] + condition_resized.shape[1] - 1) // condition_resized.shape[1]
                    condition_resized = condition_resized.repeat(1, repeats, 1, 1)
                    condition_resized = condition_resized[:, :x.shape[1], :, :]
                else:
                    # ì±„ë„ ìˆ˜ê°€ ë§ìœ¼ë©´ í‰ê· ê°’ìœ¼ë¡œ ì¤„ì„
                    condition_resized = condition_resized.mean(dim=1, keepdim=True).expand(-1, x.shape[1], -1, -1)
            x = x + condition_resized
        
        # ì¶œë ¥
        return self.output_head(x)


# ==============================================
# ğŸ”¥ ì‹¤ì œ ëª¨ë¸ ë¡œë” ë° ì´ˆê¸°í™”
# ==============================================

def create_ootd_model(device='cpu'):
    """OOTD ëª¨ë¸ ìƒì„± - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë§¤í•‘"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        model = OOTDDiffusionModel()
        logger.info("âœ… OOTD Diffusion ì‹ ê²½ë§ êµ¬ì¡° ìƒì„± ì™„ë£Œ")
        
        # ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© - êµ¬ì¡° ë§¤í•‘ í¬í•¨
        checkpoint_paths = [
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/validated_checkpoints/ootd_checkpoint.pth",
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/ootdiffusion/diffusion_pytorch_model.bin",
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors",
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors",
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/ootdiffusion/unet/ootdiffusion/unet/diffusion_pytorch_model.safetensors",
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/unet/diffusion_pytorch_model.safetensors",
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_06_virtual_fitting/diffusion_pytorch_model.safetensors",
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/pytorch_model.bin"
        ]
        
        def map_ootd_checkpoint_keys(checkpoint):
            """OOTD ì²´í¬í¬ì¸íŠ¸ í‚¤ ë§¤í•‘ - ìƒˆë¡œìš´ í‚¤ ë§¤í¼ ì‚¬ìš©"""
            # ìƒˆë¡œìš´ í‚¤ ë§¤í¼ ì‚¬ìš©
            key_mapper = CheckpointKeyMapper()
            
            # ì„ì‹œë¡œ ì²´í¬í¬ì¸íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë¶„ì„
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.pth', delete=False) as tmp_file:
                torch.save(checkpoint, tmp_file.name)
                temp_checkpoint_path = tmp_file.name
            
            try:
                # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ë° ë§¤í•‘ ìƒì„±
                mapping = key_mapper.create_ootd_mapping(temp_checkpoint_path, model)
                
                if not mapping:
                    logger.error("âŒ OOTD í‚¤ ë§¤í•‘ ìƒì„± ì‹¤íŒ¨")
                    raise RuntimeError("OOTD í‚¤ ë§¤í•‘ ìƒì„± ì‹¤íŒ¨")
                
                # ë§¤í•‘ ì ìš©
                mapped_checkpoint = key_mapper.apply_mapping(checkpoint, mapping)
                
                # ë§¤í•‘ ê²€ì¦
                validation_result = key_mapper.validate_mapping(mapped_checkpoint, model)
                logger.info(f"ğŸ“Š OOTD ë§¤í•‘ ê²€ì¦ ê²°ê³¼:")
                logger.info(f"  - ì»¤ë²„ë¦¬ì§€: {validation_result['coverage_percentage']:.1f}%")
                logger.info(f"  - ëª¨ë¸ í‚¤ ìˆ˜: {validation_result['total_model_keys']}")
                logger.info(f"  - ë§¤í•‘ëœ í‚¤ ìˆ˜: {validation_result['mapped_keys']}")
                logger.info(f"  - í˜•íƒœ ì¼ì¹˜: {validation_result['shape_matches']}")
                
                if validation_result['shape_mismatches']:
                    logger.warning(f"âš ï¸ í˜•íƒœ ë¶ˆì¼ì¹˜: {len(validation_result['shape_mismatches'])}ê°œ")
                    for mismatch in validation_result['shape_mismatches'][:5]:
                        logger.warning(f"  {mismatch['key']}: {mismatch['checkpoint_shape']} vs {mismatch['model_shape']}")
                
                logger.info(f"âœ… OOTD í‚¤ ë§¤í•‘ ì™„ë£Œ: {len(mapped_checkpoint)}/{len(checkpoint)} í‚¤ ë§¤í•‘ë¨")
                return mapped_checkpoint
                
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                try:
                    os.unlink(temp_checkpoint_path)
                except:
                    pass
        
        checkpoint_loaded = False
        for checkpoint_path in checkpoint_paths:
            if os.path.exists(checkpoint_path):
                try:
                    logger.info(f"ğŸ”„ OOTD ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„: {checkpoint_path}")
                    
                    if checkpoint_path.endswith('.safetensors'):
                        # safetensors íŒŒì¼ ë¡œë”©
                        try:
                            from safetensors.torch import load_file
                            checkpoint = load_file(checkpoint_path)
                            # í‚¤ ë§¤í•‘ ì ìš©
                            mapped_checkpoint = map_ootd_checkpoint_keys(checkpoint)
                            if mapped_checkpoint:
                                model.load_state_dict(mapped_checkpoint, strict=False)
                                logger.info(f"âœ… OOTD safetensors ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {checkpoint_path}")
                                checkpoint_loaded = True
                                break
                        except ImportError:
                            logger.warning("âš ï¸ safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ì¼ë°˜ PyTorch ë¡œë”© ì‹œë„")
                            # weights_only=Falseë¡œ ìˆ˜ì •
                            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                            if 'state_dict' in checkpoint:
                                mapped_checkpoint = map_ootd_checkpoint_keys(checkpoint['state_dict'])
                            else:
                                mapped_checkpoint = map_ootd_checkpoint_keys(checkpoint)
                            
                            if mapped_checkpoint:
                                model.load_state_dict(mapped_checkpoint, strict=False)
                                logger.info(f"âœ… OOTD ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {checkpoint_path}")
                                checkpoint_loaded = True
                                break
                    else:
                        # ì¼ë°˜ PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© - weights_only=Falseë¡œ ìˆ˜ì •
                        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                        if 'state_dict' in checkpoint:
                            mapped_checkpoint = map_ootd_checkpoint_keys(checkpoint['state_dict'])
                        else:
                            mapped_checkpoint = map_ootd_checkpoint_keys(checkpoint)
                        
                        if mapped_checkpoint:
                            model.load_state_dict(mapped_checkpoint, strict=False)
                            logger.info(f"âœ… OOTD ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {checkpoint_path}")
                            checkpoint_loaded = True
                            break
                except Exception as e:
                    if VIRTUAL_FITTING_HELPERS_AVAILABLE:
                        error_response = handle_virtual_fitting_model_loading_error("OOTD", e, checkpoint_path)
                        logger.warning(f"âš ï¸ {error_response['message']}")
                    else:
                        logger.warning(f"âš ï¸ OOTD ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
        
        if not checkpoint_loaded:
            logger.error("âŒ OOTD ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ í•„ìš”")
            raise RuntimeError("OOTD ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ğŸ”¥ Device ì¼ê´€ì„± ë³´ì¥
        model = model.to(device)
        model.eval()
        logger.info(f"âœ… OOTD ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ (device: {device})")
        return model
        
    except Exception as e:
        logger.error(f"âŒ OOTD ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        raise RuntimeError(f"OOTD ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            
def create_viton_hd_model(device='cpu'):
    """VITON-HD ëª¨ë¸ ìƒì„± - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë§¤í•‘"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        model = VITONHDModel()
        logger.info("âœ… VITON-HD ì‹ ê²½ë§ êµ¬ì¡° ìƒì„± ì™„ë£Œ")
        
        # ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© - êµ¬ì¡° ë§¤í•‘ í¬í•¨
        checkpoint_paths = [
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/ultra_models/viton_hd_2.1gb.pth",
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/hrviton_final.pth",
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/checkpoints/step_06_virtual_fitting/hrviton_final.pth",
            "step_06_virtual_fitting/viton_hd_2.1gb.pth",
            "ai_models/step_06_virtual_fitting/viton_hd_2.1gb.pth",
            "ultra_models/viton_hd_2.1gb.pth",
            "checkpoints/viton_hd_2.1gb.pth"
        ]
        
        def map_viton_checkpoint_keys(checkpoint):
            """VITON-HD ì²´í¬í¬ì¸íŠ¸ í‚¤ ë§¤í•‘ - ìƒˆë¡œìš´ í‚¤ ë§¤í¼ ì‚¬ìš©"""
            # ìƒˆë¡œìš´ í‚¤ ë§¤í¼ ì‚¬ìš©
            key_mapper = CheckpointKeyMapper()
            
            # ì„ì‹œë¡œ ì²´í¬í¬ì¸íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë¶„ì„
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.pth', delete=False) as tmp_file:
                torch.save(checkpoint, tmp_file.name)
                temp_checkpoint_path = tmp_file.name
            
            try:
                # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ë° ë§¤í•‘ ìƒì„±
                mapping = key_mapper.create_viton_hd_mapping(temp_checkpoint_path, model)
                
                if not mapping:
                    logger.error("âŒ VITON-HD í‚¤ ë§¤í•‘ ìƒì„± ì‹¤íŒ¨")
                    raise RuntimeError("VITON-HD í‚¤ ë§¤í•‘ ìƒì„± ì‹¤íŒ¨")
                
                # ë§¤í•‘ ì ìš©
                mapped_checkpoint = key_mapper.apply_mapping(checkpoint, mapping)
                
                # ë§¤í•‘ ê²€ì¦
                validation_result = key_mapper.validate_mapping(mapped_checkpoint, model)
                logger.info(f"ğŸ“Š VITON-HD ë§¤í•‘ ê²€ì¦ ê²°ê³¼:")
                logger.info(f"  - ì»¤ë²„ë¦¬ì§€: {validation_result['coverage_percentage']:.1f}%")
                logger.info(f"  - ëª¨ë¸ í‚¤ ìˆ˜: {validation_result['total_model_keys']}")
                logger.info(f"  - ë§¤í•‘ëœ í‚¤ ìˆ˜: {validation_result['mapped_keys']}")
                logger.info(f"  - í˜•íƒœ ì¼ì¹˜: {validation_result['shape_matches']}")
                
                if validation_result['shape_mismatches']:
                    logger.warning(f"âš ï¸ í˜•íƒœ ë¶ˆì¼ì¹˜: {len(validation_result['shape_mismatches'])}ê°œ")
                    for mismatch in validation_result['shape_mismatches'][:5]:
                        logger.warning(f"  {mismatch['key']}: {mismatch['checkpoint_shape']} vs {mismatch['model_shape']}")
                
                logger.info(f"âœ… VITON-HD í‚¤ ë§¤í•‘ ì™„ë£Œ: {len(mapped_checkpoint)}/{len(checkpoint)} í‚¤ ë§¤í•‘ë¨")
                return mapped_checkpoint
                
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                try:
                    os.unlink(temp_checkpoint_path)
                except:
                    pass
        
        checkpoint_loaded = False
        for checkpoint_path in checkpoint_paths:
            if os.path.exists(checkpoint_path):
                try:
                    logger.info(f"ğŸ”„ VITON-HD ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„: {checkpoint_path}")
                    
                    # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë¶„ì„ (ë””ë²„ê¹…ìš©)
                    if logger.isEnabledFor(logging.DEBUG):
                        structure_info = analyze_checkpoint_structure(checkpoint_path)
                        if 'error' not in structure_info:
                            logger.debug(f"ğŸ“Š ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°: {structure_info['total_keys']}ê°œ í‚¤, íŒ¨í„´: {list(structure_info['key_patterns'].keys())}")
                        else:
                            logger.debug(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {structure_info['error']}")
                    
                    if checkpoint_path.endswith('.safetensors'):
                        # safetensors íŒŒì¼ ë¡œë”©
                        try:
                            from safetensors.torch import load_file
                            checkpoint = load_file(checkpoint_path)
                            # í‚¤ ë§¤í•‘ ì ìš©
                            mapped_checkpoint = map_viton_checkpoint_keys(checkpoint)
                            if mapped_checkpoint:
                                model.load_state_dict(mapped_checkpoint, strict=False)
                                logger.info(f"âœ… VITON-HD safetensors ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {checkpoint_path}")
                                checkpoint_loaded = True
                                break
                        except ImportError:
                            logger.warning("âš ï¸ safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ì¼ë°˜ PyTorch ë¡œë”© ì‹œë„")
                            # weights_only=Falseë¡œ ìˆ˜ì •
                            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                            if 'state_dict' in checkpoint:
                                mapped_checkpoint = map_viton_checkpoint_keys(checkpoint['state_dict'])
                            else:
                                mapped_checkpoint = map_viton_checkpoint_keys(checkpoint)
                            
                            if mapped_checkpoint:
                                model.load_state_dict(mapped_checkpoint, strict=False)
                                logger.info(f"âœ… VITON-HD ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {checkpoint_path}")
                                checkpoint_loaded = True
                                break
                    else:
                        # weights_only=Falseë¡œ ìˆ˜ì •
                        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                        if 'state_dict' in checkpoint:
                            mapped_checkpoint = map_viton_checkpoint_keys(checkpoint['state_dict'])
                        else:
                            mapped_checkpoint = map_viton_checkpoint_keys(checkpoint)
                        
                        if mapped_checkpoint:
                            model.load_state_dict(mapped_checkpoint, strict=False)
                            logger.info(f"âœ… VITON-HD ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {checkpoint_path}")
                            checkpoint_loaded = True
                            break
                except Exception as e:
                    if VIRTUAL_FITTING_HELPERS_AVAILABLE:
                        error_response = handle_virtual_fitting_model_loading_error("VITON-HD", e, checkpoint_path)
                        logger.warning(f"âš ï¸ {error_response['message']}")
                    else:
                        logger.warning(f"âš ï¸ VITON-HD ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
        
        if not checkpoint_loaded:
            logger.error("âŒ VITON-HD ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ í•„ìš”")
            raise RuntimeError("VITON-HD ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        model.to(device)
        model.eval()
        logger.info(f"âœ… VITON-HD ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ (device: {device})")
        return model
        
    except Exception as e:
        logger.error(f"âŒ VITON-HD ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        raise RuntimeError(f"VITON-HD ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")

def create_stable_diffusion_model(device='cpu'):
    """Stable Diffusion ëª¨ë¸ ìƒì„± - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê°•í™”"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        model = StableDiffusionNeuralNetwork()
        logger.info("âœ… Stable Diffusion ì‹ ê²½ë§ êµ¬ì¡° ìƒì„± ì™„ë£Œ")
        
        # ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© - êµ¬ì¡° ë§¤í•‘ í¬í•¨
        checkpoint_paths = [
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/ootdiffusion/diffusion_pytorch_model.bin",
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors",
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_dc/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors",
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/unet/diffusion_pytorch_model.safetensors",
            "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models/step_06_virtual_fitting/pytorch_model.bin",
            "step_06_virtual_fitting/stable_diffusion_4.8gb.pth",
            "ai_models/step_06_virtual_fitting/stable_diffusion_4.8gb.pth",
            "ultra_models/stable_diffusion_4.8gb.pth",
            "checkpoints/stable_diffusion_4.8gb.pth"
        ]
        
        def map_diffusion_checkpoint_keys(checkpoint):
            """Stable Diffusion ì²´í¬í¬ì¸íŠ¸ í‚¤ ë§¤í•‘ - ìƒˆë¡œìš´ í‚¤ ë§¤í¼ ì‚¬ìš©"""
            # ìƒˆë¡œìš´ í‚¤ ë§¤í¼ ì‚¬ìš©
            key_mapper = CheckpointKeyMapper()
            
            # ì„ì‹œë¡œ ì²´í¬í¬ì¸íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë¶„ì„
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.pth', delete=False) as tmp_file:
                torch.save(checkpoint, tmp_file.name)
                temp_checkpoint_path = tmp_file.name
            
            try:
                # ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ë° ë§¤í•‘ ìƒì„±
                mapping = key_mapper.create_stable_diffusion_mapping(temp_checkpoint_path, model)
                
                if not mapping:
                    logger.error("âŒ Stable Diffusion í‚¤ ë§¤í•‘ ìƒì„± ì‹¤íŒ¨")
                    raise RuntimeError("Stable Diffusion í‚¤ ë§¤í•‘ ìƒì„± ì‹¤íŒ¨")
                
                # ë§¤í•‘ ì ìš©
                mapped_checkpoint = key_mapper.apply_mapping(checkpoint, mapping)
                
                # ë§¤í•‘ ê²€ì¦
                validation_result = key_mapper.validate_mapping(mapped_checkpoint, model)
                logger.info(f"ğŸ“Š Stable Diffusion ë§¤í•‘ ê²€ì¦ ê²°ê³¼:")
                logger.info(f"  - ì»¤ë²„ë¦¬ì§€: {validation_result['coverage_percentage']:.1f}%")
                logger.info(f"  - ëª¨ë¸ í‚¤ ìˆ˜: {validation_result['total_model_keys']}")
                logger.info(f"  - ë§¤í•‘ëœ í‚¤ ìˆ˜: {validation_result['mapped_keys']}")
                logger.info(f"  - í˜•íƒœ ì¼ì¹˜: {validation_result['shape_matches']}")
                
                if validation_result['shape_mismatches']:
                    logger.warning(f"âš ï¸ í˜•íƒœ ë¶ˆì¼ì¹˜: {len(validation_result['shape_mismatches'])}ê°œ")
                    for mismatch in validation_result['shape_mismatches'][:5]:
                        logger.warning(f"  {mismatch['key']}: {mismatch['checkpoint_shape']} vs {mismatch['model_shape']}")
                
                logger.info(f"âœ… Stable Diffusion í‚¤ ë§¤í•‘ ì™„ë£Œ: {len(mapped_checkpoint)}/{len(checkpoint)} í‚¤ ë§¤í•‘ë¨")
                return mapped_checkpoint
                
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                try:
                    os.unlink(temp_checkpoint_path)
                except:
                    pass
        
        checkpoint_loaded = False
        for checkpoint_path in checkpoint_paths:
            if os.path.exists(checkpoint_path):
                try:
                    logger.info(f"ğŸ”„ Stable Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„: {checkpoint_path}")
                    
                    # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë¶„ì„ (ë””ë²„ê¹…ìš©)
                    if logger.isEnabledFor(logging.DEBUG):
                        structure_info = analyze_checkpoint_structure(checkpoint_path)
                        if 'error' not in structure_info:
                            logger.debug(f"ğŸ“Š ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°: {structure_info['total_keys']}ê°œ í‚¤, íŒ¨í„´: {list(structure_info['key_patterns'].keys())}")
                        else:
                            logger.debug(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {structure_info['error']}")
                    
                    if checkpoint_path.endswith('.safetensors'):
                        # safetensors íŒŒì¼ ë¡œë”©
                        try:
                            from safetensors.torch import load_file
                            checkpoint = load_file(checkpoint_path)
                            # í‚¤ ë§¤í•‘ ì ìš©
                            mapped_checkpoint = map_diffusion_checkpoint_keys(checkpoint)
                            if mapped_checkpoint:
                                model.load_state_dict(mapped_checkpoint, strict=False)
                                logger.info(f"âœ… Stable Diffusion safetensors ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {checkpoint_path}")
                                checkpoint_loaded = True
                                break
                        except ImportError:
                            logger.warning("âš ï¸ safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ì¼ë°˜ PyTorch ë¡œë”© ì‹œë„")
                            # weights_only=Falseë¡œ ìˆ˜ì •
                            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                            if 'state_dict' in checkpoint:
                                mapped_checkpoint = map_diffusion_checkpoint_keys(checkpoint['state_dict'])
                            else:
                                mapped_checkpoint = map_diffusion_checkpoint_keys(checkpoint)
                            
                            if mapped_checkpoint:
                                model.load_state_dict(mapped_checkpoint, strict=False)
                                logger.info(f"âœ… Stable Diffusion ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {checkpoint_path}")
                                checkpoint_loaded = True
                                break
                    else:
                        # weights_only=Falseë¡œ ìˆ˜ì •
                        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                        if 'state_dict' in checkpoint:
                            mapped_checkpoint = map_diffusion_checkpoint_keys(checkpoint['state_dict'])
                        else:
                            mapped_checkpoint = map_diffusion_checkpoint_keys(checkpoint)
                        
                        if mapped_checkpoint:
                            model.load_state_dict(mapped_checkpoint, strict=False)
                            logger.info(f"âœ… Stable Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {checkpoint_path}")
                            checkpoint_loaded = True
                            break
                except Exception as e:
                    if VIRTUAL_FITTING_HELPERS_AVAILABLE:
                        error_response = handle_virtual_fitting_model_loading_error("Stable Diffusion", e, checkpoint_path)
                        logger.warning(f"âš ï¸ {error_response['message']}")
                    else:
                        logger.warning(f"âš ï¸ Stable Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
        
        if not checkpoint_loaded:
            logger.error("âŒ Stable Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ í•„ìš”")
            raise RuntimeError("Stable Diffusion ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ğŸ”¥ Device ì¼ê´€ì„± ë³´ì¥
        model = model.to(device)
        model.eval()
        logger.info(f"âœ… Stable Diffusion ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ (device: {device})")
        return model
        
    except Exception as e:
        logger.error(f"âŒ Stable Diffusion ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        raise RuntimeError(f"Stable Diffusion ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")


import importlib  # ì¶”ê°€
import logging    # ì¶”ê°€

# ==============================================
# ğŸ”¥ Central Hub DI Container ì•ˆì „ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€) - VirtualFitting íŠ¹í™”
# ==============================================

def ensure_quality_assessment_logger(quality_assessment_obj):
    """AIQualityAssessment ê°ì²´ì˜ logger ì†ì„± ë³´ì¥"""
    if not hasattr(quality_assessment_obj, 'logger') or quality_assessment_obj.logger is None:
        quality_assessment_obj.logger = logging.getLogger(
            f"{quality_assessment_obj.__class__.__module__}.{quality_assessment_obj.__class__.__name__}"
        )
        return True
    return False

def analyze_checkpoint_structure(checkpoint_path):
    """ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë¶„ì„"""
    try:
        if checkpoint_path.endswith('.safetensors'):
            from safetensors.torch import load_file
            checkpoint = load_file(checkpoint_path)
        else:
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        # í‚¤ íŒ¨í„´ ë¶„ì„
        key_patterns = {}
        for key in state_dict.keys():
            prefix = key.split('.')[0] if '.' in key else key
            if prefix not in key_patterns:
                key_patterns[prefix] = []
            key_patterns[prefix].append(key)
        
        return {
            'total_keys': len(state_dict),
            'key_patterns': key_patterns,
            'sample_keys': list(state_dict.keys())[:10]
        }
    except Exception as e:
        return {'error': str(e)}

def smart_checkpoint_mapping(checkpoint, model_state_dict, model_name):
    """ìŠ¤ë§ˆíŠ¸ ì²´í¬í¬ì¸íŠ¸ í‚¤ ë§¤í•‘"""
    mapped_checkpoint = {}
    unmapped_keys = []
    
    # ëª¨ë¸ë³„ ë§¤í•‘ ê·œì¹™
    mapping_rules = {
        'ootd': {
            # ì§ì ‘ ë§¤í•‘ - ì²´í¬í¬ì¸íŠ¸ í‚¤ë¥¼ ëª¨ë¸ í‚¤ë¡œ ë³€í™˜
            'conv_in.weight': 'conv_in.weight',
            'conv_in.bias': 'conv_in.bias',
            'conv_out.weight': 'conv_out.weight',
            'conv_out.bias': 'conv_out.bias',
            'conv_norm_out.weight': 'conv_norm_out.weight',
            'conv_norm_out.bias': 'conv_norm_out.bias',
            'time_embedding.linear_1.weight': 'time_embedding.linear_1.weight',
            'time_embedding.linear_1.bias': 'time_embedding.linear_1.bias',
            'time_embedding.linear_2.weight': 'time_embedding.linear_2.weight',
            'time_embedding.linear_2.bias': 'time_embedding.linear_2.bias',
            'time_embedding.act': 'time_embedding.act',
            # ì ‘ë‘ì‚¬ ì œê±° ê·œì¹™
            'model.': '',
            'unet.': '',
            'diffusion.': '',
            'time_embedding.': 'time_embedding.',
            'encoder_blocks.': 'encoder_blocks.',
            'decoder_blocks.': 'decoder_blocks.',
            'bottleneck_': 'bottleneck_',
            'cross_attention.': 'cross_attention.',
            'cloth_encoder.': 'cloth_encoder.',
            'output_conv.': 'output_conv.'
        },
        'viton_hd': {
            'model.': '',
            'viton.': '',
            'hrviton.': '',
            'geometric_matcher.': 'geometric_matcher.',
            'tryon_generator.': 'tryon_generator.',
            'refinement_net.': 'refinement_net.',
            'feature_extractor.': 'geometric_matcher.feature_extractor.',
            'flow_predictor.': 'geometric_matcher.flow_predictor.',
            'upsample.': 'geometric_matcher.upsample.'
        },
        'stable_diffusion': {
            'model.': '',
            'unet.': '',
            'diffusion.': '',
            'vae.': 'vae_encoder.',
            'text_encoder.': 'text_encoder.',
            'noise_scheduler.': 'noise_scheduler.',
            'controlnet.': 'controlnet.',
            'lora_adapter.': 'lora_adapter.'
        }
    }
    
    rules = mapping_rules.get(model_name, {})
    
    for key, value in checkpoint.items():
        mapped = False
        
        # ê·œì¹™ ê¸°ë°˜ ë§¤í•‘
        for prefix, replacement in rules.items():
            if key.startswith(prefix):
                new_key = key.replace(prefix, replacement, 1)
                if new_key in model_state_dict:
                    if model_state_dict[new_key].shape == value.shape:
                        mapped_checkpoint[new_key] = value
                        mapped = True
                        break
        
        # ì§ì ‘ ë§¤í•‘ ì‹œë„
        if not mapped and key in model_state_dict:
            if model_state_dict[key].shape == value.shape:
                mapped_checkpoint[key] = value
                mapped = True
        
        # ë§¤í•‘ë˜ì§€ ì•Šì€ í‚¤ ê¸°ë¡
        if not mapped:
            unmapped_keys.append(key)
    
    return mapped_checkpoint, unmapped_keys

def _setup_logger():
    """AIQualityAssessmentìš© logger ì„¤ì •"""
    return logging.getLogger(f"{__name__}.AIQualityAssessment")

def _get_central_hub_container():
    """Central Hub DI Container ì•ˆì „í•œ ë™ì  í•´ê²° - VirtualFittingìš©"""
    try:
        import importlib
        module = importlib.import_module('app.core.di_container')
        get_global_fn = getattr(module, 'get_global_container', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError:
        return None

def _inject_dependencies_safe(step_instance):
    """Central Hub DI Containerë¥¼ í†µí•œ ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì… - VirtualFittingìš©"""
    try:
        container = _get_central_hub_container()
        if container and hasattr(container, 'inject_to_step'):
            return container.inject_to_step(step_instance)
        return 0
    except (ImportError, AttributeError) as e:
        logging.getLogger(__name__).warning(f"ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        return 0

def _get_service_from_central_hub(service_key: str):
    """Central Hubë¥¼ í†µí•œ ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ - VirtualFittingìš©"""
    try:
        container = _get_central_hub_container()
        if container:
            return container.get(service_key)
        return None
    except (ImportError, AttributeError) as e:
        logging.getLogger(__name__).warning(f"ì„œë¹„ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None

# BaseStepMixin ë™ì  import (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€) - VirtualFittingìš©
def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€) - VirtualFittingìš©"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError:
        try:
            # í´ë°±: ìƒëŒ€ ê²½ë¡œ
            from .base_step_mixin import BaseStepMixin
            return BaseStepMixin
        except ImportError:
            logging.getLogger(__name__).error("âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨")
            return None

BaseStepMixin = get_base_step_mixin_class()

# ==============================================
# ğŸ”¥ VirtualFittingStep í´ë˜ìŠ¤
# ==============================================

class TPSWarping:
    """TPS (Thin Plate Spline) ê¸°ë°˜ ì˜ë¥˜ ì›Œí•‘ ì•Œê³ ë¦¬ì¦˜ - ê³ ê¸‰ êµ¬í˜„"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.TPSWarping")
        
    def create_control_points(self, person_mask: np.ndarray, cloth_mask: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ì œì–´ì  ìƒì„± (ì¸ì²´ì™€ ì˜ë¥˜ ê²½ê³„)"""
        try:
            # ì¸ì²´ ë§ˆìŠ¤í¬ì—ì„œ ì œì–´ì  ì¶”ì¶œ
            person_contours = self._extract_contour_points(person_mask)
            cloth_contours = self._extract_contour_points(cloth_mask)
            
            # ì œì–´ì  ë§¤ì¹­
            source_points, target_points = self._match_control_points(cloth_contours, person_contours)
            
            return source_points, target_points
            
        except (ValueError, IndexError) as e:
            self.logger.error(f"âŒ ì œì–´ì  ìƒì„± ë°ì´í„° ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ ì œì–´ì  ë°˜í™˜
            h, w = person_mask.shape
            source_points = np.array([[w//4, h//4], [3*w//4, h//4], [w//2, h//2], [w//4, 3*h//4], [3*w//4, 3*h//4]])
            target_points = source_points.copy()
            return source_points, target_points
        except RuntimeError as e:
            self.logger.error(f"âŒ ì œì–´ì  ìƒì„± ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ ì œì–´ì  ë°˜í™˜
            h, w = person_mask.shape
            source_points = np.array([[w//4, h//4], [3*w//4, h//4], [w//2, h//2], [w//4, 3*h//4], [3*w//4, 3*h//4]])
            target_points = source_points.copy()
            return source_points, target_points
    
    def _extract_contour_points(self, mask: np.ndarray, num_points: int = 20) -> np.ndarray:
        """ë§ˆìŠ¤í¬ì—ì„œ ìœ¤ê³½ì„  ì ë“¤ ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ ê°€ì¥ìë¦¬ ê²€ì¶œ
            edges = self._detect_edges(mask)
            
            # ìœ¤ê³½ì„  ì ë“¤ ì¶”ì¶œ
            y_coords, x_coords = np.where(edges)
            
            if len(x_coords) == 0:
                # í´ë°±: ë§ˆìŠ¤í¬ ì¤‘ì‹¬ ê¸°ë°˜ ì ë“¤
                h, w = mask.shape
                return np.array([[w//2, h//2]])
            
            # ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
            indices = np.linspace(0, len(x_coords)-1, min(num_points, len(x_coords)), dtype=int)
            contour_points = np.column_stack([x_coords[indices], y_coords[indices]])
            
            return contour_points
            
        except (ValueError, IndexError) as e:
            self.logger.warning(f"âš ï¸ ìœ¤ê³½ì„  ì¶”ì¶œ ë°ì´í„° ì˜¤ë¥˜: {e}")
            h, w = mask.shape
            return np.array([[w//2, h//2]])
        except RuntimeError as e:
            self.logger.warning(f"âš ï¸ ìœ¤ê³½ì„  ì¶”ì¶œ ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
            h, w = mask.shape
            return np.array([[w//2, h//2]])
    
    def _detect_edges(self, mask: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ ê°€ì¥ìë¦¬ ê²€ì¶œ"""
        try:
            # Sobel í•„í„° ê·¼ì‚¬
            kernel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
            kernel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])
            
            # ì»¨ë³¼ë£¨ì…˜ ì—°ì‚°
            edges_x = self._convolve2d(mask.astype(np.float32), kernel_x)
            edges_y = self._convolve2d(mask.astype(np.float32), kernel_y)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°
            edges = np.sqrt(edges_x**2 + edges_y**2)
            edges = (edges > 0.1).astype(np.uint8)
            
            return edges
            
        except (ValueError, IndexError, RuntimeError):
            # í´ë°±: ê¸°ë³¸ ê°€ì¥ìë¦¬
            h, w = mask.shape
            edges = np.zeros((h, w), dtype=np.uint8)
            edges[1:-1, 1:-1] = mask[1:-1, 1:-1]
            return edges
    
    def _convolve2d(self, image: np.ndarray, kernel: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ 2D ì»¨ë³¼ë£¨ì…˜"""
        try:
            h, w = image.shape
            kh, kw = kernel.shape
            
            # íŒ¨ë”©
            pad_h, pad_w = kh//2, kw//2
            padded = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode='edge')
            
            # ì»¨ë³¼ë£¨ì…˜
            result = np.zeros((h, w))
            for i in range(h):
                for j in range(w):
                    result[i, j] = np.sum(padded[i:i+kh, j:j+kw] * kernel)
            
            return result
            
        except (ValueError, IndexError, RuntimeError):
            return np.zeros_like(image)
    
    def _match_control_points(self, source_points: np.ndarray, target_points: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ì œì–´ì  ë§¤ì¹­"""
        try:
            min_len = min(len(source_points), len(target_points))
            return source_points[:min_len], target_points[:min_len]
                
        except (ValueError, IndexError) as e:
            self.logger.warning(f"âš ï¸ ì œì–´ì  ë§¤ì¹­ ë°ì´í„° ì˜¤ë¥˜: {e}")
            return source_points[:5], target_points[:5]
        except RuntimeError as e:
            self.logger.warning(f"âš ï¸ ì œì–´ì  ë§¤ì¹­ ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
            return source_points[:5], target_points[:5]
    
    def apply_tps_transform(self, cloth_image: np.ndarray, source_points: np.ndarray, target_points: np.ndarray) -> np.ndarray:
        """TPS ë³€í™˜ ì ìš©"""
        try:
            if len(source_points) < 3 or len(target_points) < 3:
                return cloth_image
            
            h, w = cloth_image.shape[:2]
            
            # TPS ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            tps_matrix = self._calculate_tps_matrix(source_points, target_points)
            
            # ê·¸ë¦¬ë“œ ìƒì„±
            x_grid, y_grid = np.meshgrid(np.arange(w), np.arange(h))
            grid_points = np.column_stack([x_grid.ravel(), y_grid.ravel()])
            
            # TPS ë³€í™˜ ì ìš©
            transformed_points = self._apply_tps_to_points(grid_points, source_points, target_points, tps_matrix)
            
            # ì´ë¯¸ì§€ ì›Œí•‘
            warped_image = self._warp_image(cloth_image, grid_points, transformed_points)
            
            return warped_image
            
        except (ValueError, IndexError) as e:
            self.logger.error(f"âŒ TPS ë³€í™˜ ë°ì´í„° ì˜¤ë¥˜: {e}")
            return cloth_image
        except RuntimeError as e:
            self.logger.error(f"âŒ TPS ë³€í™˜ ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
            return cloth_image
    
    def _calculate_tps_matrix(self, source_points: np.ndarray, target_points: np.ndarray) -> np.ndarray:
        """TPS ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        try:
            n = len(source_points)
            
            # TPS ì»¤ë„ í–‰ë ¬ ìƒì„±
            K = np.zeros((n, n))
            for i in range(n):
                for j in range(n):
                    if i != j:
                        r = np.linalg.norm(source_points[i] - source_points[j])
                        if r > 0:
                            K[i, j] = r**2 * np.log(r)
            
            # P í–‰ë ¬ (ì–´í•€ ë³€í™˜)
            P = np.column_stack([np.ones(n), source_points])
            
            # L í–‰ë ¬ êµ¬ì„±
            L = np.block([[K, P], [P.T, np.zeros((3, 3))]])
            
            # Y ë²¡í„°
            Y = np.column_stack([target_points, np.zeros((3, 2))])
            
            # ë§¤íŠ¸ë¦­ìŠ¤ í•´ê²° (regularization ì¶”ê°€)
            L_reg = L + 1e-6 * np.eye(L.shape[0])
            tps_matrix = np.linalg.solve(L_reg, Y)
            
            return tps_matrix
            
        except (ValueError, IndexError) as e:
            self.logger.warning(f"âš ï¸ TPS ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° ë°ì´í„° ì˜¤ë¥˜: {e}")
            return np.eye(len(source_points) + 3, 2)
        except RuntimeError as e:
            self.logger.warning(f"âš ï¸ TPS ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
            return np.eye(len(source_points) + 3, 2)
    
    def _apply_tps_to_points(self, points: np.ndarray, source_points: np.ndarray, target_points: np.ndarray, tps_matrix: np.ndarray) -> np.ndarray:
        """ì ë“¤ì— TPS ë³€í™˜ ì ìš©"""
        try:
            n_source = len(source_points)
            n_points = len(points)
            
            # ì»¤ë„ ê°’ ê³„ì‚°
            K_new = np.zeros((n_points, n_source))
            for i in range(n_points):
                for j in range(n_source):
                    r = np.linalg.norm(points[i] - source_points[j])
                    if r > 0:
                        K_new[i, j] = r**2 * np.log(r)
            
            # P_new í–‰ë ¬
            P_new = np.column_stack([np.ones(n_points), points])
            
            # ë³€í™˜ëœ ì ë“¤ ê³„ì‚°
            L_new = np.column_stack([K_new, P_new])
            transformed_points = L_new @ tps_matrix
            
            return transformed_points
            
        except (ValueError, IndexError) as e:
            self.logger.warning(f"âš ï¸ TPS ì  ë³€í™˜ ë°ì´í„° ì˜¤ë¥˜: {e}")
            return points
        except RuntimeError as e:
            self.logger.warning(f"âš ï¸ TPS ì  ë³€í™˜ ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
            return points
    
    def _warp_image(self, image: np.ndarray, source_grid: np.ndarray, target_grid: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ ì›Œí•‘"""
        try:
            h, w = image.shape[:2]
            
            # íƒ€ê²Ÿ ê·¸ë¦¬ë“œë¥¼ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ë³€í™˜
            target_x = target_grid[:, 0].reshape(h, w)
            target_y = target_grid[:, 1].reshape(h, w)
            
            # ê²½ê³„ í´ë¦¬í•‘
            target_x = np.clip(target_x, 0, w-1)
            target_y = np.clip(target_y, 0, h-1)
            
            # ë°”ì´ë¦¬ë‹ˆì–´ ë³´ê°„
            warped_image = self._bilinear_interpolation(image, target_x, target_y)
            
            return warped_image
            
        except (ValueError, IndexError) as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì›Œí•‘ ë°ì´í„° ì˜¤ë¥˜: {e}")
            return image
        except RuntimeError as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì›Œí•‘ ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
            return image
    
    def _bilinear_interpolation(self, image: np.ndarray, x: np.ndarray, y: np.ndarray) -> np.ndarray:
        """ë°”ì´ë¦¬ë‹ˆì–´ ë³´ê°„"""
        try:
            h, w = image.shape[:2]
            
            # ì •ìˆ˜ ì¢Œí‘œ
            x0 = np.floor(x).astype(int)
            x1 = x0 + 1
            y0 = np.floor(y).astype(int)
            y1 = y0 + 1
            
            # ê²½ê³„ ì²˜ë¦¬
            x0 = np.clip(x0, 0, w-1)
            x1 = np.clip(x1, 0, w-1)
            y0 = np.clip(y0, 0, h-1)
            y1 = np.clip(y1, 0, h-1)
            
            # ê°€ì¤‘ì¹˜
            wa = (x1 - x) * (y1 - y)
            wb = (x - x0) * (y1 - y)
            wc = (x1 - x) * (y - y0)
            wd = (x - x0) * (y - y0)
            
            # ë³´ê°„
            if len(image.shape) == 3:
                warped = np.zeros_like(image)
                for c in range(image.shape[2]):
                    warped[:, :, c] = (wa * image[y0, x0, c] + 
                                     wb * image[y0, x1, c] + 
                                     wc * image[y1, x0, c] + 
                                     wd * image[y1, x1, c])
            else:
                warped = (wa * image[y0, x0] + 
                         wb * image[y0, x1] + 
                         wc * image[y1, x0] + 
                         wd * image[y1, x1])
            
            return warped.astype(image.dtype)
            
        except (ValueError, IndexError) as e:
            self.logger.error(f"âŒ ë°”ì´ë¦¬ë‹ˆì–´ ë³´ê°„ ë°ì´í„° ì˜¤ë¥˜: {e}")
            return image
        except RuntimeError as e:
            self.logger.error(f"âŒ ë°”ì´ë¦¬ë‹ˆì–´ ë³´ê°„ ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
            return image
class AdvancedClothAnalyzer:
    """ê³ ê¸‰ ì˜ë¥˜ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        try:
            # ğŸ”¥ ì‹¤ì œ ì´ˆê¸°í™” ë¡œì§ ì¶”ê°€
            self.logger = logging.getLogger(f"{__name__}.AdvancedClothAnalyzer")
            
            # ë¶„ì„ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
            self.color_clusters = 5
            self.texture_window_size = 8
            self.pattern_detection_threshold = 0.3
            
            # ìºì‹œ ì´ˆê¸°í™”
            self._color_cache = {}
            self._texture_cache = {}
            self._pattern_cache = {}
            
            # ë¶„ì„ ë„êµ¬ ì´ˆê¸°í™”
            self._init_analysis_tools()
            
            self.logger.info("âœ… AdvancedClothAnalyzer ì‹¤ì œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except (ImportError, AttributeError) as e:
            # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
            self.logger = logging.getLogger(f"{__name__}.AdvancedClothAnalyzer")
            self.color_clusters = 5
            self.texture_window_size = 8
            self.pattern_detection_threshold = 0.3
            self._color_cache = {}
            self._texture_cache = {}
            self._pattern_cache = {}
            self.logger.warning(f"âš ï¸ AdvancedClothAnalyzer ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
        except RuntimeError as e:
            # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
            self.logger = logging.getLogger(f"{__name__}.AdvancedClothAnalyzer")
            self.color_clusters = 5
            self.texture_window_size = 8
            self.pattern_detection_threshold = 0.3
            self._color_cache = {}
            self._texture_cache = {}
            self._pattern_cache = {}
            self.logger.warning(f"âš ï¸ AdvancedClothAnalyzer ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
    
    def _init_analysis_tools(self):
        """ë¶„ì„ ë„êµ¬ ì´ˆê¸°í™”"""
        try:
            # ìƒ‰ìƒ ë¶„ì„ ë„êµ¬
            self.color_quantizer = self._create_color_quantizer()
            
            # í…ìŠ¤ì²˜ ë¶„ì„ ë„êµ¬
            self.texture_analyzer = self._create_texture_analyzer()
            
            # íŒ¨í„´ ê°ì§€ ë„êµ¬
            self.pattern_detector = self._create_pattern_detector()
            
        except (ImportError, AttributeError) as e:
            self.logger.warning(f"âš ï¸ ë¶„ì„ ë„êµ¬ ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        except RuntimeError as e:
            self.logger.warning(f"âš ï¸ ë¶„ì„ ë„êµ¬ ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _create_color_quantizer(self):
        """ìƒ‰ìƒ ì–‘ìí™” ë„êµ¬ ìƒì„±"""
        return {
            'quantization_levels': 32,
            'color_space': 'RGB',
            'sampling_rate': 0.1
        }
    
    def _create_texture_analyzer(self):
        """í…ìŠ¤ì²˜ ë¶„ì„ ë„êµ¬ ìƒì„±"""
        return {
            'window_size': self.texture_window_size,
            'gradient_method': 'sobel',
            'variance_threshold': 0.1
        }
    
    def _create_pattern_detector(self):
        """íŒ¨í„´ ê°ì§€ ë„êµ¬ ìƒì„±"""
        return {
            'fft_threshold': self.pattern_detection_threshold,
            'frequency_bands': 8,
            'symmetry_check': True
        }
    
    def analyze_cloth_properties(self, clothing_image: np.ndarray) -> Dict[str, Any]:
        """ì˜ë¥˜ ì†ì„± ê³ ê¸‰ ë¶„ì„"""
        try:
            # ìƒ‰ìƒ ë¶„ì„
            dominant_colors = self._extract_dominant_colors(clothing_image)
            
            # í…ìŠ¤ì²˜ ë¶„ì„
            texture_features = self._analyze_texture(clothing_image)
            
            # íŒ¨í„´ ë¶„ì„
            pattern_type = self._detect_pattern(clothing_image)
            
            return {
                'dominant_colors': dominant_colors,
                'texture_features': texture_features,
                'pattern_type': pattern_type,
                'cloth_complexity': self._calculate_complexity(clothing_image)
            }
            
        except (ValueError, IndexError) as e:
            self.logger.warning(f"ì˜ë¥˜ ë¶„ì„ ë°ì´í„° ì˜¤ë¥˜: {e}")
            return {'cloth_complexity': 0.5}
        except RuntimeError as e:
            self.logger.warning(f"ì˜ë¥˜ ë¶„ì„ ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
            return {'cloth_complexity': 0.5}
    
    def _extract_dominant_colors(self, image: np.ndarray, k: int = 5) -> List[List[int]]:
        """ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ (K-means ê¸°ë°˜)"""
        try:
            # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (ì„±ëŠ¥ ìµœì í™”)
            small_img = Image.fromarray(image).resize((150, 150))
            data = np.array(small_img).reshape((-1, 3))
            
            # ê°„ë‹¨í•œ ìƒ‰ìƒ í´ëŸ¬ìŠ¤í„°ë§ (K-means ê·¼ì‚¬)
            unique_colors = {}
            for pixel in data[::10]:  # ìƒ˜í”Œë§
                color_key = tuple(pixel // 32 * 32)  # ìƒ‰ìƒ ì–‘ìí™”
                unique_colors[color_key] = unique_colors.get(color_key, 0) + 1
            
            # ìƒìœ„ kê°œ ìƒ‰ìƒ ë°˜í™˜
            top_colors = sorted(unique_colors.items(), key=lambda x: x[1], reverse=True)[:k]
            return [list(color[0]) for color in top_colors]
            
        except (ValueError, IndexError, RuntimeError):
            return [[128, 128, 128]]  # ê¸°ë³¸ íšŒìƒ‰
    
    def _analyze_texture(self, image: np.ndarray) -> Dict[str, float]:
        """í…ìŠ¤ì²˜ ë¶„ì„"""
        try:
            gray = np.mean(image, axis=2) if len(image.shape) == 3 else image
            
            # í…ìŠ¤ì²˜ íŠ¹ì§•ë“¤
            features = {}
            
            # í‘œì¤€í¸ì°¨ (ê±°ì¹ ê¸°)
            features['roughness'] = float(np.std(gray) / 255.0)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° (ì—£ì§€ ë°€ë„)
            grad_x = np.abs(np.diff(gray, axis=1))
            grad_y = np.abs(np.diff(gray, axis=0))
            features['edge_density'] = float(np.mean(grad_x) + np.mean(grad_y)) / 255.0
            
            # ì§€ì—­ ë¶„ì‚° (í…ìŠ¤ì²˜ ê· ì¼ì„±)
            local_variance = []
            h, w = gray.shape
            for i in range(0, h-8, 8):
                for j in range(0, w-8, 8):
                    patch = gray[i:i+8, j:j+8]
                    local_variance.append(np.var(patch))
            
            features['uniformity'] = 1.0 - min(np.std(local_variance) / np.mean(local_variance), 1.0) if local_variance else 0.5
            
            return features
            
        except (ValueError, IndexError, RuntimeError):
            return {'roughness': 0.5, 'edge_density': 0.5, 'uniformity': 0.5}
    
    def _detect_pattern(self, image: np.ndarray) -> str:
        """íŒ¨í„´ ê°ì§€"""
        try:
            gray = np.mean(image, axis=2) if len(image.shape) == 3 else image
            
            # FFT ê¸°ë°˜ ì£¼ê¸°ì„± ë¶„ì„
            f_transform = np.fft.fft2(gray)
            f_shift = np.fft.fftshift(f_transform)
            magnitude_spectrum = np.log(np.abs(f_shift) + 1)
            
            # ì£¼íŒŒìˆ˜ ë„ë©”ì¸ì—ì„œ íŒ¨í„´ ê°ì§€
            center = np.array(magnitude_spectrum.shape) // 2
            
            # ë°©ì‚¬í˜• í‰ê·  ê³„ì‚°
            y, x = np.ogrid[:magnitude_spectrum.shape[0], :magnitude_spectrum.shape[1]]
            distances = np.sqrt((x - center[1])**2 + (y - center[0])**2)
            
            # ì£¼ìš” ì£¼íŒŒìˆ˜ ì„±ë¶„ ë¶„ì„
            radial_profile = []
            for r in range(1, min(center)//2):
                mask = (distances >= r-0.5) & (distances < r+0.5)
                radial_profile.append(np.mean(magnitude_spectrum[mask]))
            
            if len(radial_profile) > 10:
                # ì£¼ê¸°ì  íŒ¨í„´ ê°ì§€
                peaks = []
                for i in range(1, len(radial_profile)-1):
                    if float(radial_profile[i]) > float(radial_profile[i-1]) and float(radial_profile[i]) > float(radial_profile[i+1]):
                        # ğŸ”¥ numpy ë°°ì—´ boolean í‰ê°€ ì˜¤ë¥˜ ìˆ˜ì •
                        threshold = float(np.mean(radial_profile) + np.std(radial_profile))
                        if float(radial_profile[i]) > threshold:
                            peaks.append(i)
                
                if len(peaks) >= 3:
                    return "striped"
                elif len(peaks) >= 1:
                    return "patterned"
            
            return "solid"
            
        except (ValueError, IndexError, RuntimeError):
            return "unknown"
    
    def _calculate_complexity(self, image: np.ndarray) -> float:
        """ì˜ë¥˜ ë³µì¡ë„ ê³„ì‚°"""
        try:
            gray = np.mean(image, axis=2) if len(image.shape) == 3 else image
            
            # ì—£ì§€ ë°€ë„
            edges = self._simple_edge_detection(gray)
            edge_density = np.sum(edges) / edges.size
            
            # ìƒ‰ìƒ ë‹¤ì–‘ì„±
            colors = image.reshape(-1, 3) if len(image.shape) == 3 else gray.flatten()
            color_variance = np.var(colors)
            
            # ë³µì¡ë„ ì¢…í•©
            complexity = (edge_density * 0.6 + min(color_variance / 10000, 1.0) * 0.4)
            
            return float(np.clip(complexity, 0.0, 1.0))
            
        except (ValueError, IndexError, RuntimeError):
            return 0.5
    
    def _simple_edge_detection(self, gray: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œ"""
        try:
            # Sobel í•„í„° ê·¼ì‚¬
            kernel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
            kernel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])
            
            h, w = gray.shape
            edges = np.zeros((h-2, w-2))
            
            for i in range(1, h-1):
                for j in range(1, w-1):
                    patch = gray[i-1:i+2, j-1:j+2]
                    gx = np.sum(patch * kernel_x)
                    gy = np.sum(patch * kernel_y)
                    edges[i-1, j-1] = np.sqrt(gx**2 + gy**2)
            
            return edges > np.mean(edges) + np.std(edges)
            
        except (ValueError, IndexError, RuntimeError):
            return np.zeros((gray.shape[0]-2, gray.shape[1]-2), dtype=bool)

class AIQualityAssessment:
    """AI í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ğŸ”¥ logger ì†ì„± ì¶”ê°€ (ëˆ„ë½ëœ ë¶€ë¶„)
        self.logger = logging.getLogger(f"{__name__}.AIQualityAssessment")
        
        # í’ˆì§ˆ í‰ê°€ ì„ê³„ê°’ë“¤
        self.quality_thresholds = {
            'excellent': 0.9,
            'good': 0.7,
            'fair': 0.5,
            'poor': 0.3
        }
        
        # í‰ê°€ ê°€ì¤‘ì¹˜
        self.evaluation_weights = {
            'fit_quality': 0.3,
            'lighting_consistency': 0.2,
            'texture_realism': 0.2,
            'color_harmony': 0.15,
            'detail_preservation': 0.15
        }
        
        # SSIM ê³„ì‚°ê¸° (êµ¬ì¡°ì  ìœ ì‚¬ì„± ì§€ìˆ˜)
        self.ssim_enabled = True
        try:
            from skimage.metrics import structural_similarity as ssim
            self.ssim_func = ssim
        except ImportError:
            self.ssim_enabled = False
            self.logger.warning("âš ï¸ SSIMì„ ìœ„í•œ scikit-image ì—†ìŒ - ê¸°ë³¸ í’ˆì§ˆ í‰ê°€ ì‚¬ìš©")




    def evaluate_fitting_quality(self, fitted_image: np.ndarray, 
                               person_image: np.ndarray,
                               clothing_image: np.ndarray) -> Dict[str, float]:
        """í”¼íŒ… í’ˆì§ˆ í‰ê°€"""
        try:
            metrics = {}
            
            # 1. ì‹œê°ì  í’ˆì§ˆ í‰ê°€
            metrics['visual_quality'] = self._assess_visual_quality(fitted_image)
            
            # 2. í”¼íŒ… ì •í™•ë„ í‰ê°€
            metrics['fitting_accuracy'] = self._assess_fitting_accuracy(
                fitted_image, person_image, clothing_image
            )
            
            # 3. ìƒ‰ìƒ ì¼ì¹˜ë„ í‰ê°€
            metrics['color_consistency'] = self._assess_color_consistency(
                fitted_image, clothing_image
            )
            
            # 4. êµ¬ì¡°ì  ë¬´ê²°ì„± í‰ê°€
            metrics['structural_integrity'] = self._assess_structural_integrity(
                fitted_image, person_image
            )
            
            # 5. ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            weights = {
                'visual_quality': 0.25,
                'fitting_accuracy': 0.35,
                'color_consistency': 0.25,
                'structural_integrity': 0.15
            }
            
            overall_quality = sum(
                metrics.get(key, 0.5) * weight for key, weight in weights.items()
            )
            
            metrics['overall_quality'] = overall_quality
            
            return metrics
            
        except (ValueError, IndexError) as e:
            if CUSTOM_EXCEPTIONS_AVAILABLE:
                raise QualityAssessmentError(f"í’ˆì§ˆ í‰ê°€ ë°ì´í„° ì˜¤ë¥˜: {e}", ErrorCodes.VIRTUAL_FITTING_FAILED)
            self.logger.error(f"í’ˆì§ˆ í‰ê°€ ë°ì´í„° ì˜¤ë¥˜: {e}")
            return {'overall_quality': 0.5}
        except RuntimeError as e:
            if CUSTOM_EXCEPTIONS_AVAILABLE:
                raise QualityAssessmentError(f"í’ˆì§ˆ í‰ê°€ ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}", ErrorCodes.VIRTUAL_FITTING_FAILED)
            self.logger.error(f"í’ˆì§ˆ í‰ê°€ ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
            return {'overall_quality': 0.5}
    
    def _assess_visual_quality(self, image: np.ndarray) -> float:
        """ì‹œê°ì  í’ˆì§ˆ í‰ê°€"""
        try:
            if len(image.shape) < 2:
                return 0.0
            
            # ì„ ëª…ë„ í‰ê°€ (ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°)
            gray = np.mean(image, axis=2) if len(image.shape) == 3 else image
            laplacian_var = self._calculate_laplacian_variance(gray)
            sharpness = min(laplacian_var / 500.0, 1.0)
            
            # ëŒ€ë¹„ í‰ê°€
            contrast = np.std(gray) / 128.0
            contrast = min(contrast, 1.0)
            
            # ë…¸ì´ì¦ˆ í‰ê°€ (ì—­ì‚°)
            noise_level = self._estimate_noise_level(gray)
            noise_score = max(0.0, 1.0 - noise_level)
            
            # ê°€ì¤‘ í‰ê· 
            visual_quality = (sharpness * 0.4 + contrast * 0.4 + noise_score * 0.2)
            
            return float(visual_quality)
            
        except (ValueError, IndexError, RuntimeError):
            return 0.5
    
    def _calculate_laplacian_variance(self, image: np.ndarray) -> float:
        """ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚° ê³„ì‚°"""
        h, w = image.shape
        total_variance = 0
        count = 0
        
        for i in range(1, h-1):
            for j in range(1, w-1):
                laplacian = (
                    -image[i-1,j-1] - image[i-1,j] - image[i-1,j+1] +
                    -image[i,j-1] + 8*image[i,j] - image[i,j+1] +
                    -image[i+1,j-1] - image[i+1,j] - image[i+1,j+1]
                )
                total_variance += laplacian ** 2
                count += 1
        
        return total_variance / count if count > 0 else 0
    
    def _estimate_noise_level(self, image: np.ndarray) -> float:
        """ë…¸ì´ì¦ˆ ë ˆë²¨ ì¶”ì •"""
        try:
            h, w = image.shape
            high_freq_sum = 0
            count = 0
            
            for i in range(1, h-1):
                for j in range(1, w-1):
                    # ì£¼ë³€ í”½ì…€ê³¼ì˜ ì°¨ì´ ê³„ì‚°
                    center = image[i, j]
                    neighbors = [
                        image[i-1, j], image[i+1, j],
                        image[i, j-1], image[i, j+1]
                    ]
                    
                    variance = np.var([center] + neighbors)
                    high_freq_sum += variance
                    count += 1
            
            if count > 0:
                avg_variance = high_freq_sum / count
                noise_level = min(avg_variance / 1000.0, 1.0)
                return noise_level
            
            return 0.0
            
        except (ValueError, IndexError, RuntimeError):
            return 0.5
    
    def _assess_fitting_accuracy(self, fitted_image: np.ndarray, 
                               person_image: np.ndarray,
                               clothing_image: np.ndarray) -> float:
        """í”¼íŒ… ì •í™•ë„ í‰ê°€"""
        try:
            if fitted_image.shape != person_image.shape:
                return 0.5
            
            # ì˜ë¥˜ ì˜ì—­ ì¶”ì •
            diff_image = np.abs(fitted_image.astype(np.float32) - person_image.astype(np.float32))
            clothing_region = np.mean(diff_image, axis=2) > 30  # ì„ê³„ê°’ ê¸°ë°˜
            
            # ğŸ”¥ numpy ë°°ì—´ boolean í‰ê°€ ì˜¤ë¥˜ ìˆ˜ì •
            if float(np.sum(clothing_region)) == 0:
                return 0.0
            
            # ì˜ë¥˜ ì˜ì—­ì—ì„œì˜ ìƒ‰ìƒ ì¼ì¹˜ë„
            if len(fitted_image.shape) == 3 and len(clothing_image.shape) == 3:
                fitted_colors = fitted_image[clothing_region]
                clothing_mean = np.mean(clothing_image, axis=(0, 1))
                
                color_distances = np.linalg.norm(
                    fitted_colors - clothing_mean, axis=1
                )
                avg_color_distance = np.mean(color_distances)
                max_distance = np.sqrt(255**2 * 3)
                
                color_accuracy = max(0.0, 1.0 - (avg_color_distance / max_distance))
                
                # í”¼íŒ… ì˜ì—­ í¬ê¸° ì ì ˆì„±
                total_pixels = fitted_image.shape[0] * fitted_image.shape[1]
                clothing_ratio = np.sum(clothing_region) / total_pixels
                
                size_score = 1.0
                if clothing_ratio < 0.1:  # ë„ˆë¬´ ì‘ìŒ
                    size_score = clothing_ratio / 0.1
                elif clothing_ratio > 0.6:  # ë„ˆë¬´ í¼
                    size_score = (1.0 - clothing_ratio) / 0.4
                
                fitting_accuracy = (color_accuracy * 0.7 + size_score * 0.3)
                return float(fitting_accuracy)
            
            return 0.5
            
        except (ValueError, IndexError, RuntimeError):
            return 0.5
    
    def _assess_color_consistency(self, fitted_image: np.ndarray,
                                clothing_image: np.ndarray) -> float:
        """ìƒ‰ìƒ ì¼ì¹˜ë„ í‰ê°€"""
        try:
            if len(fitted_image.shape) != 3 or len(clothing_image.shape) != 3:
                return 0.5
            
            # í‰ê·  ìƒ‰ìƒ ë¹„êµ
            fitted_mean = np.mean(fitted_image, axis=(0, 1))
            clothing_mean = np.mean(clothing_image, axis=(0, 1))
            
            color_distance = np.linalg.norm(fitted_mean - clothing_mean)
            max_distance = np.sqrt(255**2 * 3)
            
            color_consistency = max(0.0, 1.0 - (color_distance / max_distance))
            
            return float(color_consistency)
            
        except (ValueError, IndexError, RuntimeError):
            return 0.5
    
    def _assess_structural_integrity(self, fitted_image: np.ndarray,
                                   person_image: np.ndarray) -> float:
        """êµ¬ì¡°ì  ë¬´ê²°ì„± í‰ê°€"""
        try:
            if fitted_image.shape != person_image.shape:
                return 0.5
            
            # ê°„ë‹¨í•œ SSIM ê·¼ì‚¬
            if len(fitted_image.shape) == 3:
                fitted_gray = np.mean(fitted_image, axis=2)
                person_gray = np.mean(person_image, axis=2)
            else:
                fitted_gray = fitted_image
                person_gray = person_image
            
            # í‰ê· ê³¼ ë¶„ì‚° ê³„ì‚°
            mu1 = np.mean(fitted_gray)
            mu2 = np.mean(person_gray)
            
            sigma1_sq = np.var(fitted_gray)
            sigma2_sq = np.var(person_gray)
            sigma12 = np.mean((fitted_gray - mu1) * (person_gray - mu2))
            
            # SSIM ê³„ì‚°
            c1 = 0.01 ** 2
            c2 = 0.03 ** 2
            
            numerator = (2 * mu1 * mu2 + c1) * (2 * sigma12 + c2)
            denominator = (mu1**2 + mu2**2 + c1) * (sigma1_sq + sigma2_sq + c2)
            
            ssim = numerator / (denominator + 1e-8)
            
            return float(np.clip(ssim, 0.0, 1.0))
            
        except (ValueError, IndexError, RuntimeError):
            return 0.5

    # VirtualFittingStep í´ë˜ìŠ¤ì— ê³ ê¸‰ ê¸°ëŠ¥ë“¤ í†µí•©
# ì¤‘ë³µëœ __init__ ë©”ì„œë“œ ì œê±°ë¨


# ==============================================
# ğŸ”¥ ë°ì´í„° í´ë˜ìŠ¤ë“¤
# ==============================================

@dataclass
class VirtualFittingConfig:
    """Virtual Fitting ì„¤ì •"""
    input_size: tuple = (768, 1024)  # OOTD ì…ë ¥ í¬ê¸°
    fitting_quality: str = "high"  # fast, balanced, high, ultra
    enable_multi_items: bool = True
    enable_pose_adaptation: bool = True
    enable_lighting_adaptation: bool = True
    enable_texture_preservation: bool = True
    device: str = "auto"
    auto_postprocessing: bool = True  # ìë™ í›„ì²˜ë¦¬ í™œì„±í™”

# Virtual Fitting ëª¨ë“œ ì •ì˜
FITTING_MODES = {
    0: 'single_item',      # ë‹¨ì¼ ì˜ë¥˜ ì•„ì´í…œ
    1: 'multi_item',       # ë‹¤ì¤‘ì˜ë¥˜ ì•„ì´í…œ
    2: 'full_outfit',      # ì „ì²´ ì˜ìƒ
    3: 'accessory_only',   # ì•¡ì„¸ì„œë¦¬ë§Œ
    4: 'upper_body',       # ìƒì²´ë§Œ
    5: 'lower_body',       # í•˜ì²´ë§Œ
    6: 'mixed_style',      # í˜¼í•© ìŠ¤íƒ€ì¼
    7: 'seasonal_adapt',   # ê³„ì ˆë³„ ì ì‘
    8: 'occasion_based',   # ìƒí™©ë³„ ë§ì¶¤
    9: 'ai_recommended'    # AI ì¶”ì²œ ê¸°ë°˜
}

# Virtual Fitting í’ˆì§ˆ ë ˆë²¨
FITTING_QUALITY_LEVELS = {
    'fast': {
        'models': ['ootd'],
        'resolution': (512, 512),
        'inference_steps': 20,
        'guidance_scale': 7.5
    },
    'balanced': {
        'models': ['ootd', 'viton_hd'],
        'resolution': (768, 1024),
        'inference_steps': 30,
        'guidance_scale': 10.0
    },
    'high': {
        'models': ['ootd', 'viton_hd', 'diffusion'],
        'resolution': (768, 1024),
        'inference_steps': 50,
        'guidance_scale': 12.5
    },
    'ultra': {
        'models': ['ootd', 'viton_hd', 'diffusion'],
        'resolution': (1024, 1536),
        'inference_steps': 100,
        'guidance_scale': 15.0
    }
}

# ì˜ë¥˜ ì•„ì´í…œ íƒ€ì…
CLOTHING_ITEM_TYPES = {
    'tops': ['t-shirt', 'shirt', 'blouse', 'sweater', 'hoodie', 'jacket', 'coat'],
    'bottoms': ['pants', 'jeans', 'shorts', 'skirt', 'leggings'],
    'dresses': ['dress', 'gown', 'sundress', 'cocktail_dress'],
    'outerwear': ['jacket', 'coat', 'blazer', 'cardigan', 'vest'],
    'accessories': ['hat', 'scarf', 'bag', 'glasses', 'jewelry'],
    'footwear': ['shoes', 'boots', 'sneakers', 'heels', 'sandals']
}
class VirtualFittingStep(BaseStepMixin):
    
    def _get_service_from_central_hub(self, service_key: str):
        """Central Hubì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì™„ì „ ë™ê¸° ë²„ì „)"""
        try:
            # 1. DI Containerì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            if hasattr(self, 'di_container') and self.di_container:
                try:
                    service = self.di_container.get_service(service_key)
                    if service is not None:
                        return service
                except (AttributeError, TypeError) as di_error:
                    self.logger.warning(f"âš ï¸ DI Container ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {di_error}")
            
            # 2. ê¸´ê¸‰ í´ë°± ì„œë¹„ìŠ¤ ìƒì„±
            if service_key == 'session_manager':
                return self._create_emergency_session_manager()
            elif service_key == 'model_loader':
                return self._create_emergency_model_loader()
            
            return None
        except (ImportError, AttributeError) as e:
            self.logger.warning(f"âš ï¸ Central Hub ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def _load_session_images_safe(self, session_id: str) -> Tuple[Optional[Any], Optional[Any]]:
        """ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œ"""
        try:
            session_manager = self._get_service_from_central_hub('session_manager')
            if not session_manager:
                self.logger.warning("âš ï¸ ì„¸ì…˜ ë§¤ë‹ˆì €ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None, None
            
            # ë™ê¸° ë©”ì„œë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if hasattr(session_manager, 'get_session_images_sync'):
                try:
                    person_image, cloth_image = session_manager.get_session_images_sync(session_id)
                    # ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì‚¬
                    if person_image is not None and cloth_image is not None:
                        self.logger.info(f"âœ… ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ (ë™ê¸°): {session_id}")
                        return person_image, cloth_image
                    else:
                        self.logger.warning(f"âš ï¸ ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ê°€ None: {session_id}")
                        return None, None
                except Exception as e:
                    if VIRTUAL_FITTING_HELPERS_AVAILABLE:
                        error_response = handle_session_data_error("load_images", e, session_id)
                        self.logger.warning(f"âš ï¸ {error_response['message']}")
                    else:
                        self.logger.warning(f"âš ï¸ ì„¸ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    return None, None
            
            # ë¹„ë™ê¸° ë©”ì„œë“œ ì‚¬ìš©
            try:
                import asyncio
                import concurrent.futures
                
                def run_async_load():
                    try:
                        # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ì‹¤í–‰
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        try:
                            result = loop.run_until_complete(session_manager.get_session_images(session_id))
                            if isinstance(result, (list, tuple)) and len(result) >= 2:
                                return result[0], result[1]
                            return None, None
                        finally:
                            loop.close()
                    except Exception as e:
                        return None, None
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_async_load)
                    person_image, cloth_image = future.result(timeout=10.0)
                    
                    # ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì‚¬
                    if person_image is not None and cloth_image is not None:
                        self.logger.info(f"âœ… ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ (ë¹„ë™ê¸°): {session_id}")
                        return person_image, cloth_image
                    else:
                        self.logger.warning(f"âš ï¸ ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ê°€ None (ë¹„ë™ê¸°): {session_id}")
                        return None, None
                    
            except Exception as e:
                if VIRTUAL_FITTING_HELPERS_AVAILABLE:
                    error_response = handle_session_data_error("load_images_async", e, session_id)
                    self.logger.warning(f"âš ï¸ {error_response['message']}")
                else:
                    self.logger.warning(f"âš ï¸ ì„¸ì…˜ ë¹„ë™ê¸° ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return None, None
                
        except Exception as e:
            if VIRTUAL_FITTING_HELPERS_AVAILABLE:
                error_response = handle_session_data_error("session_access", e, session_id)
                self.logger.warning(f"âš ï¸ {error_response['message']}")
            else:
                self.logger.warning(f"âš ï¸ ì„¸ì…˜ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return None, None
    """
    ğŸ”¥ Step 06: Virtual Fitting v8.0 - Central Hub DI Container ì™„ì „ ì—°ë™
    
    Central Hub DI Container v7.0ì—ì„œ ìë™ ì œê³µ:
    âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì…
    âœ… MemoryManager ìë™ ì—°ê²°  
    âœ… DataConverter í†µí•©
    âœ… ìë™ ì´ˆê¸°í™” ë° ì„¤ì •
    """
    
    def __init__(self, **kwargs):
        """Central Hub DI Container v7.0 ê¸°ë°˜ ì´ˆê¸°í™”"""
        try:
            # 1. í•„ìˆ˜ ì†ì„±ë“¤ ë¨¼ì € ì´ˆê¸°í™” (super() í˜¸ì¶œ ì „)
            self._initialize_step_attributes()
            
            # 2. BaseStepMixin ì´ˆê¸°í™” (Central Hub DI Container ì—°ë™)
            super().__init__(
                step_name="VirtualFittingStep",
                **kwargs
            )
            
            # 3. Virtual Fitting íŠ¹í™” ì´ˆê¸°í™”
            self._initialize_virtual_fitting_specifics(**kwargs)
            
            # ğŸ”¥ ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸ ë° ê°•ì œ ìƒì„±
            if not self.ai_models:
                self.logger.warning("âš ï¸ Virtual Fitting íŠ¹í™” ì´ˆê¸°í™” í›„ì—ë„ ëª¨ë¸ì´ ì—†ìŒ - ê°•ì œ ìƒì„±")
                try:
                    # ì§ì ‘ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±
                    self.ai_models['ootd'] = create_ootd_model(self.device)
                    self.ai_models['viton_hd'] = create_viton_hd_model(self.device)
                    self.ai_models['diffusion'] = create_stable_diffusion_model(self.device)
                    self.loaded_models = list(self.ai_models.keys())
                    self.fitting_ready = True
                    self.logger.info(f"âœ… ê°•ì œ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì™„ë£Œ: {len(self.ai_models)}ê°œ")
                except (ImportError, AttributeError) as e:
                    self.logger.error(f"âŒ ê°•ì œ ì‹ ê²½ë§ ëª¨ë¸ ì˜ì¡´ì„± ìƒì„± ì‹¤íŒ¨: {e}")
                except RuntimeError as e:
                    self.logger.error(f"âŒ ê°•ì œ ì‹ ê²½ë§ ëª¨ë¸ ëŸ°íƒ€ì„ ìƒì„± ì‹¤íŒ¨: {e}")
                except OSError as e:
                    self.logger.error(f"âŒ ê°•ì œ ì‹ ê²½ë§ ëª¨ë¸ ì‹œìŠ¤í…œ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # 4. AIQualityAssessment logger ì†ì„± íŒ¨ì¹˜
            if hasattr(self, 'quality_assessor') and self.quality_assessor:
                patched = ensure_quality_assessment_logger(self.quality_assessor)
                if patched:
                    self.logger.info("âœ… AIQualityAssessment logger ì†ì„± íŒ¨ì¹˜ ì™„ë£Œ")
            
            self.logger.info("âœ… VirtualFittingStep v8.0 Central Hub DI Container ì´ˆê¸°í™” ì™„ë£Œ")


        except (ImportError, AttributeError) as e:
            if CUSTOM_EXCEPTIONS_AVAILABLE:
                raise DependencyInjectionError(f"VirtualFittingStep ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", ErrorCodes.DI_CONTAINER_ERROR)
            self.logger.error(f"âŒ VirtualFittingStep ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._emergency_setup(**kwargs)
        except RuntimeError as e:
            if CUSTOM_EXCEPTIONS_AVAILABLE:
                raise VirtualFittingError(f"VirtualFittingStep ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", ErrorCodes.VIRTUAL_FITTING_FAILED)
            self.logger.error(f"âŒ VirtualFittingStep ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._emergency_setup(**kwargs)
        except OSError as e:
            if CUSTOM_EXCEPTIONS_AVAILABLE:
                raise FileOperationError(f"VirtualFittingStep ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", ErrorCodes.FILE_PERMISSION_DENIED)
            self.logger.error(f"âŒ VirtualFittingStep ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._emergency_setup(**kwargs)
    
    def _initialize_step_attributes(self):
        """í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™” (BaseStepMixin ìš”êµ¬ì‚¬í•­)"""
        self.ai_models = {}
        self.models_loading_status = {
            'ootd': False,
            'viton_hd': False,
            'diffusion': False,
            'mock_model': False
        }
        self.model_interface = None
        self.loaded_models = []
        self.logger = logging.getLogger(f"{__name__}.VirtualFittingStep")
        
        # Virtual Fitting íŠ¹í™” ì†ì„±ë“¤
        self.fitting_models = {}
        self.fitting_ready = False
        self.fitting_cache = {}
        self.pose_processor = None
        self.lighting_adapter = None
        self.texture_enhancer = None
        self.diffusion_pipeline = None
    
    def _initialize_virtual_fitting_specifics(self, **kwargs):
        """Virtual Fitting íŠ¹í™” ì´ˆê¸°í™”"""
        try:
            # ì„¤ì •
            self.config = VirtualFittingConfig()
            if 'config' in kwargs:
                config_dict = kwargs['config']
                if isinstance(config_dict, dict):
                    for key, value in config_dict.items():
                        if hasattr(self.config, key):
                            setattr(self.config, key, value)
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            self.device = self._detect_optimal_device()
            
            # ğŸ”¥ ì‹¤ì œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬)
            try:
                self.tps_warping = TPSWarping()
                self.logger.info("âœ… TPSWarping ì´ˆê¸°í™” ì™„ë£Œ")
            except (ImportError, AttributeError) as e:
                if CUSTOM_EXCEPTIONS_AVAILABLE:
                    raise DependencyInjectionError(f"TPSWarping ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", ErrorCodes.DI_CONTAINER_ERROR)
                self.logger.warning(f"âš ï¸ TPSWarping ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.tps_warping = None
            except RuntimeError as e:
                if CUSTOM_EXCEPTIONS_AVAILABLE:
                    raise VirtualFittingError(f"TPSWarping ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", ErrorCodes.VIRTUAL_FITTING_FAILED)
                self.logger.warning(f"âš ï¸ TPSWarping ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.tps_warping = None
            
            try:
                self.cloth_analyzer = AdvancedClothAnalyzer()
                self.logger.info("âœ… AdvancedClothAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
            except (ImportError, AttributeError) as e:
                if CUSTOM_EXCEPTIONS_AVAILABLE:
                    raise DependencyInjectionError(f"AdvancedClothAnalyzer ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", ErrorCodes.DI_CONTAINER_ERROR)
                self.logger.warning(f"âš ï¸ AdvancedClothAnalyzer ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.cloth_analyzer = None
            except RuntimeError as e:
                if CUSTOM_EXCEPTIONS_AVAILABLE:
                    raise ClothingAnalysisError(f"AdvancedClothAnalyzer ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", ErrorCodes.VIRTUAL_FITTING_FAILED)
                self.logger.warning(f"âš ï¸ AdvancedClothAnalyzer ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # ì¬ì‹œë„
                try:
                    self.cloth_analyzer = AdvancedClothAnalyzer()
                    self.logger.info("âœ… AdvancedClothAnalyzer ì¬ì´ˆê¸°í™” ì„±ê³µ")
                except RuntimeError as retry_e:
                    if CUSTOM_EXCEPTIONS_AVAILABLE:
                        raise ClothingAnalysisError(f"AdvancedClothAnalyzer ì¬ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {retry_e}", ErrorCodes.VIRTUAL_FITTING_FAILED)
                    self.logger.error(f"âŒ AdvancedClothAnalyzer ì¬ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {retry_e}")
                    self.cloth_analyzer = None
            
            try:
                self.quality_assessor = AIQualityAssessment()
                # ğŸ”¥ logger ì†ì„± ëª…ì‹œì  ì¶”ê°€
                if not hasattr(self.quality_assessor, 'logger'):
                    self.quality_assessor.logger = self.logger
                self.logger.info("âœ… AIQualityAssessment ì´ˆê¸°í™” ì™„ë£Œ")
            except (ImportError, AttributeError) as e:
                if CUSTOM_EXCEPTIONS_AVAILABLE:
                    raise DependencyInjectionError(f"AIQualityAssessment ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", ErrorCodes.DI_CONTAINER_ERROR)
                self.logger.warning(f"âš ï¸ AIQualityAssessment ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.quality_assessor = None
            except RuntimeError as e:
                if CUSTOM_EXCEPTIONS_AVAILABLE:
                    raise QualityAssessmentError(f"AIQualityAssessment ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", ErrorCodes.VIRTUAL_FITTING_FAILED)
                self.logger.warning(f"âš ï¸ AIQualityAssessment ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # ì¬ì‹œë„
                try:
                    self.quality_assessor = AIQualityAssessment()
                    if not hasattr(self.quality_assessor, 'logger'):
                        self.quality_assessor.logger = self.logger
                    self.logger.info("âœ… AIQualityAssessment ì¬ì´ˆê¸°í™” ì„±ê³µ")
                except RuntimeError as retry_e:
                    if CUSTOM_EXCEPTIONS_AVAILABLE:
                        raise QualityAssessmentError(f"AIQualityAssessment ì¬ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {retry_e}", ErrorCodes.VIRTUAL_FITTING_FAILED)
                    self.logger.error(f"âŒ AIQualityAssessment ì¬ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {retry_e}")
                    self.quality_assessor = None
            
            # Virtual Fitting ëª¨ë¸ë“¤ ì´ˆê¸°í™”
            self.fitting_ready = False
            self.loaded_models = {}
            self.ai_models = {}
            
            # AI ëª¨ë¸ ë¡œë”© (Central Hubë¥¼ í†µí•´)
            self._load_virtual_fitting_models_via_central_hub()
            
            # ğŸ”¥ ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ë“¤ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê°•ì œë¡œ ìƒì„±
            if not self.ai_models:
                self.logger.warning("âš ï¸ Central Hubë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ ê°•ì œ ìƒì„±")
                self._create_actual_neural_networks()
            
            # ğŸ”¥ ì—¬ì „íˆ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìµœì¢… í´ë°±
            if not self.ai_models:
                self.logger.warning("âš ï¸ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ - ìµœì¢… í´ë°± ì‹¤í–‰")
                self._create_actual_neural_networks_fallback()
            
            # ğŸ”¥ ìµœì¢… í™•ì¸ ë° ê°•ì œ ìƒì„±
            if not self.ai_models:
                self.logger.error("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë”© ë°©ë²• ì‹¤íŒ¨ - ì§ì ‘ ìƒì„±")
                try:
                    # ì§ì ‘ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±
                    self.ai_models['ootd'] = create_ootd_model(self.device)
                    self.ai_models['viton_hd'] = create_viton_hd_model(self.device)
                    self.ai_models['diffusion'] = create_stable_diffusion_model(self.device)
                    self.loaded_models = list(self.ai_models.keys())
                    self.logger.info(f"âœ… ì§ì ‘ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì™„ë£Œ: {len(self.ai_models)}ê°œ")
                except (ImportError, AttributeError) as e:
                    self.logger.error(f"âŒ ì§ì ‘ ì‹ ê²½ë§ ëª¨ë¸ ì˜ì¡´ì„± ìƒì„± ì‹¤íŒ¨: {e}")
                except RuntimeError as e:
                    self.logger.error(f"âŒ ì§ì ‘ ì‹ ê²½ë§ ëª¨ë¸ ëŸ°íƒ€ì„ ìƒì„± ì‹¤íŒ¨: {e}")
                except OSError as e:
                    self.logger.error(f"âŒ ì§ì ‘ ì‹ ê²½ë§ ëª¨ë¸ ì‹œìŠ¤í…œ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ë“¤ì´ ë¡œë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if not self.fitting_ready:
                # ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ë“¤ì„ ê°•ì œë¡œ ìƒì„±
                self._create_actual_neural_networks()
                if self.ai_models:
                    self.fitting_ready = True
                    self.logger.info("âœ… ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±ìœ¼ë¡œ Virtual Fitting ì¤€ë¹„ ì™„ë£Œ")
                else:
                    self.logger.error("âŒ ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
            
            # ğŸ”¥ ì´ˆê¸°í™” ìƒíƒœ ê²€ì¦
            initialization_status = {
                'tps_warping': self.tps_warping is not None,
                'cloth_analyzer': self.cloth_analyzer is not None,
                'quality_assessor': self.quality_assessor is not None,
                'fitting_ready': self.fitting_ready
            }
            
            self.logger.info(f"âœ… Virtual Fitting íŠ¹í™” ì´ˆê¸°í™” ì™„ë£Œ - ìƒíƒœ: {initialization_status}")
            
        except (ImportError, AttributeError) as e:
            self.logger.warning(f"âš ï¸ Virtual Fitting íŠ¹í™” ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ë¡œ í´ë°±
            self._create_actual_neural_networks()
            if self.ai_models:
                self.fitting_ready = True
                self.logger.info("âœ… í´ë°±ìœ¼ë¡œ ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            else:
                self.logger.error("âŒ í´ë°± ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±ë„ ì‹¤íŒ¨")
        except RuntimeError as e:
            self.logger.warning(f"âš ï¸ Virtual Fitting íŠ¹í™” ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ë¡œ í´ë°±
            self._create_actual_neural_networks()
            if self.ai_models:
                self.fitting_ready = True
                self.logger.info("âœ… í´ë°±ìœ¼ë¡œ ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            else:
                self.logger.error("âŒ í´ë°± ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±ë„ ì‹¤íŒ¨")
    
    def _detect_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            if TORCH_AVAILABLE:
                if torch.backends.mps.is_available():
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        except (ImportError, RuntimeError):
            return "cpu"
 
    def _emergency_setup(self, **kwargs):
        """ê¸´ê¸‰ ì„¤ì • (ì´ˆê¸°í™” ì‹¤íŒ¨ì‹œ í´ë°±)"""
        try:
            self.logger.warning("âš ï¸ VirtualFittingStep ê¸´ê¸‰ ì„¤ì • ëª¨ë“œ í™œì„±í™”")
            
            # ê¸°ë³¸ ì†ì„±ë“¤ ì„¤ì •
            self.step_name = "VirtualFittingStep"
            self.step_id = 6
            self.device = "cpu"
            self.config = VirtualFittingConfig()
            
            # ë¹ˆ ëª¨ë¸ ì»¨í…Œì´ë„ˆë“¤
            self.ai_models = {}
            self.models_loading_status = {'emergency': True}  
            self.model_interface = None
            self.loaded_models = []
            
            # Virtual Fitting íŠ¹í™” ì†ì„±ë“¤
            self.fitting_models = {}
            self.fitting_ready = False
            self.fitting_cache = {}
            self.pose_processor = None
            self.lighting_adapter = None
            self.texture_enhancer = None
            self.diffusion_pipeline = None
            
            # ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ë“¤ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ
            try:
                self.tps_warping = TPSWarping()
                self.cloth_analyzer = AdvancedClothAnalyzer()
                self.quality_assessor = AIQualityAssessment()
            except (ImportError, AttributeError, RuntimeError):
                self.tps_warping = None
                self.cloth_analyzer = None
                self.quality_assessor = None
            
            # Mock ëª¨ë¸ ìƒì„±
            self._create_mock_virtual_fitting_models()
            
            self.logger.warning("âœ… VirtualFittingStep ê¸´ê¸‰ ì„¤ì • ì™„ë£Œ")
            
        except (ImportError, AttributeError, RuntimeError) as e:
            self.logger.error(f"âŒ ê¸´ê¸‰ ì„¤ì •ë„ ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ì†ì„±ë“¤ë§Œ
            self.step_name = "VirtualFittingStep"
            self.step_id = 6
            self.device = "cpu"
            self.ai_models = {}
            self.loaded_models = []
            self.fitting_ready = False

    # ==============================================
    # ğŸ”¥ Central Hub DI Container ì—°ë™ AI ëª¨ë¸ ë¡œë”©
    # ==============================================

    def _load_virtual_fitting_models_via_central_hub(self):
        """Central Hub DI Containerë¥¼ í†µí•œ Virtual Fitting ëª¨ë¸ ë¡œë”© - ì‹¤ì œ ì‹ ê²½ë§ êµ¬ì¡°"""
        try:
            self.logger.info("ğŸ”„ Central Hubë¥¼ í†µí•œ Virtual Fitting AI ì‹ ê²½ë§ ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # Central Hubì—ì„œ ModelLoader ê°€ì ¸ì˜¤ê¸° (ìë™ ì£¼ì…ë¨)
            if not hasattr(self, 'model_loader') or not self.model_loader:
                self.logger.warning("âš ï¸ ModelLoaderê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ - ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±")
                self._create_actual_neural_networks()
                return
            
            # ğŸ”¥ ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ë° ë¡œë”©
            loaded_models = {}
            ai_models = {}
            
            # 1. OOTD ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±
            try:
                ootd_model = create_ootd_model(self.device)
                if ootd_model is not None:
                    loaded_models['ootd'] = True
                    ai_models['ootd'] = ootd_model
                    self.logger.info("âœ… OOTD ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì„±ê³µ")
                else:
                    self.logger.warning("âš ï¸ OOTD ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
            except (ImportError, AttributeError) as e:
                self.logger.warning(f"âš ï¸ OOTD ì‹ ê²½ë§ ëª¨ë¸ ì˜ì¡´ì„± ìƒì„± ì‹¤íŒ¨: {e}")
            except RuntimeError as e:
                self.logger.warning(f"âš ï¸ OOTD ì‹ ê²½ë§ ëª¨ë¸ ëŸ°íƒ€ì„ ìƒì„± ì‹¤íŒ¨: {e}")
            except OSError as e:
                self.logger.warning(f"âš ï¸ OOTD ì‹ ê²½ë§ ëª¨ë¸ ì‹œìŠ¤í…œ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # 2. VITON-HD ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±
            try:
                viton_hd_model = create_viton_hd_model(self.device)
                if viton_hd_model is not None:
                    loaded_models['viton_hd'] = True
                    ai_models['viton_hd'] = viton_hd_model
                    self.logger.info("âœ… VITON-HD ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì„±ê³µ")
                else:
                    self.logger.warning("âš ï¸ VITON-HD ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
            except (ImportError, AttributeError) as e:
                self.logger.warning(f"âš ï¸ VITON-HD ì‹ ê²½ë§ ëª¨ë¸ ì˜ì¡´ì„± ìƒì„± ì‹¤íŒ¨: {e}")
            except RuntimeError as e:
                self.logger.warning(f"âš ï¸ VITON-HD ì‹ ê²½ë§ ëª¨ë¸ ëŸ°íƒ€ì„ ìƒì„± ì‹¤íŒ¨: {e}")
            except OSError as e:
                self.logger.warning(f"âš ï¸ VITON-HD ì‹ ê²½ë§ ëª¨ë¸ ì‹œìŠ¤í…œ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # 3. Stable Diffusion ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±
            try:
                diffusion_model = create_stable_diffusion_model(self.device)
                if diffusion_model is not None:
                    loaded_models['diffusion'] = True
                    ai_models['diffusion'] = diffusion_model
                    self.logger.info("âœ… Stable Diffusion ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì„±ê³µ")
                else:
                    self.logger.warning("âš ï¸ Stable Diffusion ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
            except (ImportError, AttributeError) as e:
                self.logger.warning(f"âš ï¸ Stable Diffusion ì‹ ê²½ë§ ëª¨ë¸ ì˜ì¡´ì„± ìƒì„± ì‹¤íŒ¨: {e}")
            except RuntimeError as e:
                self.logger.warning(f"âš ï¸ Stable Diffusion ì‹ ê²½ë§ ëª¨ë¸ ëŸ°íƒ€ì„ ìƒì„± ì‹¤íŒ¨: {e}")
            except OSError as e:
                self.logger.warning(f"âš ï¸ Stable Diffusion ì‹ ê²½ë§ ëª¨ë¸ ì‹œìŠ¤í…œ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # 4. ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„ (ìˆëŠ” ê²½ìš°)
            try:
                if self.model_loader and hasattr(self.model_loader, 'load_checkpoint'):
                    # OOTD ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                    if 'ootd' in loaded_models:
                        ootd_checkpoint = self.model_loader.load_checkpoint('ootd_checkpoint')
                        if ootd_checkpoint:
                            ai_models['ootd'].load_state_dict(ootd_checkpoint, strict=False)
                            self.logger.info("âœ… OOTD ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                    
                    # VITON-HD ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                    if 'viton_hd' in loaded_models:
                        viton_checkpoint = self.model_loader.load_checkpoint('viton_hd_checkpoint')
                        if viton_checkpoint:
                            ai_models['viton_hd'].load_state_dict(viton_checkpoint, strict=False)
                            self.logger.info("âœ… VITON-HD ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
                    
                    # Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                    if 'diffusion' in loaded_models:
                        diffusion_checkpoint = self.model_loader.load_checkpoint('diffusion_checkpoint')
                        if diffusion_checkpoint:
                            ai_models['diffusion'].load_state_dict(diffusion_checkpoint, strict=False)
                            self.logger.info("âœ… Stable Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ")
            except (OSError, IOError) as e:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")
            except (KeyError, ValueError) as e:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ í˜•ì‹ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
            except RuntimeError as e:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ëŸ°íƒ€ì„ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
            
            # 5. ëª¨ë¸ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.ai_models.update(ai_models)
            self.models_loading_status.update(loaded_models)
            if hasattr(self, 'loaded_models') and isinstance(self.loaded_models, list):
                self.loaded_models.extend(list(loaded_models.keys()))
            else:
                self.loaded_models = list(loaded_models.keys())
            
            # 6. ëª¨ë¸ì´ í•˜ë‚˜ë„ ë¡œë”©ë˜ì§€ ì•Šì€ ê²½ìš° ì‹¤ì œ ëª¨ë¸ ê°•ì œ ìƒì„±
            if not self.loaded_models:
                self.logger.warning("âš ï¸ ëª¨ë“  ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ - ì‹¤ì œ ëª¨ë¸ ê°•ì œ ìƒì„± ì‹œë„")
                self._create_actual_neural_networks()
                # ì—¬ì „íˆ ì‹¤íŒ¨í•˜ë©´ Mock ëª¨ë¸ë¡œ í´ë°±
                if not self.loaded_models:
                    self.logger.warning("âš ï¸ ì‹¤ì œ ëª¨ë¸ ìƒì„±ë„ ì‹¤íŒ¨ - Mock ëª¨ë¸ë¡œ í´ë°±")
                    self._create_mock_virtual_fitting_models()
            
            # ğŸ”¥ 7. ì‹¤ì œ ëª¨ë¸ ë¡œë”© í™•ì¸ ë° ê°•ì œ Mock ëª¨ë¸ ì œê±°
            actual_models_loaded = False
            mock_models_to_remove = []
            
            # ë¨¼ì € Mock ëª¨ë¸ë“¤ì„ ì‹ë³„
            for model_name, model in self.ai_models.items():
                if hasattr(model, 'model_name') and 'mock' in model.model_name:
                    mock_models_to_remove.append(model_name)
                    self.logger.warning(f"âš ï¸ Mock ëª¨ë¸ ê°ì§€ë¨: {model_name} - ì œê±° ì˜ˆì •")
                else:
                    actual_models_loaded = True
                    self.logger.info(f"âœ… ì‹¤ì œ ëª¨ë¸ í™•ì¸ë¨: {model_name}")
            
            # Mock ëª¨ë¸ë“¤ì„ ì œê±°
            for model_name in mock_models_to_remove:
                if model_name in self.ai_models:
                    del self.ai_models[model_name]
                if model_name in self.loaded_models:
                    self.loaded_models.remove(model_name)
                self.logger.info(f"âœ… Mock ëª¨ë¸ ì œê±° ì™„ë£Œ: {model_name}")
            
            # ì‹¤ì œ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê°•ì œë¡œ ìƒì„±
            if not actual_models_loaded:
                self.logger.warning("âš ï¸ ì‹¤ì œ ëª¨ë¸ì´ ì—†ìŒ - ê°•ì œ ìƒì„± ì‹œë„")
                try:
                    ootd_model = create_ootd_model(self.device)
                    if ootd_model is not None:
                        self.ai_models['ootd'] = ootd_model
                        self.loaded_models.append('ootd')
                        self.logger.info("âœ… OOTD ì‹¤ì œ ëª¨ë¸ ê°•ì œ ìƒì„± ì™„ë£Œ")
                        actual_models_loaded = True
                except (ImportError, AttributeError) as e:
                    self.logger.error(f"âŒ OOTD ì‹¤ì œ ëª¨ë¸ ì˜ì¡´ì„± ìƒì„± ì‹¤íŒ¨: {e}")
                except RuntimeError as e:
                    self.logger.error(f"âŒ OOTD ì‹¤ì œ ëª¨ë¸ ëŸ°íƒ€ì„ ìƒì„± ì‹¤íŒ¨: {e}")
                except OSError as e:
                    self.logger.error(f"âŒ OOTD ì‹¤ì œ ëª¨ë¸ ì‹œìŠ¤í…œ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ì—¬ì „íˆ ì‹¤ì œ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì‹¤ì œ ëª¨ë¸ ê°•ì œ ìƒì„± (Mock ëª¨ë¸ ëŒ€ì‹ )
            if not actual_models_loaded:
                self.logger.warning("âš ï¸ ì‹¤ì œ ëª¨ë¸ì´ ì—†ìŒ - ì‹¤ì œ ëª¨ë¸ ê°•ì œ ìƒì„± ì‹œë„")
                try:
                    # OOTD ëª¨ë¸ ê°•ì œ ìƒì„±
                    ootd_model = create_ootd_model(self.device)
                    if ootd_model is not None:
                        self.ai_models['ootd'] = ootd_model
                        if 'ootd' not in self.loaded_models:
                            self.loaded_models.append('ootd')
                        self.logger.info("âœ… OOTD ì‹¤ì œ ëª¨ë¸ ê°•ì œ ìƒì„± ì™„ë£Œ")
                        actual_models_loaded = True
                    
                    # VITON-HD ëª¨ë¸ ê°•ì œ ìƒì„±
                    viton_model = create_viton_hd_model(self.device)
                    if viton_model is not None:
                        self.ai_models['viton_hd'] = viton_model
                        if 'viton_hd' not in self.loaded_models:
                            self.loaded_models.append('viton_hd')
                        self.logger.info("âœ… VITON-HD ì‹¤ì œ ëª¨ë¸ ê°•ì œ ìƒì„± ì™„ë£Œ")
                        actual_models_loaded = True
                    
                    # Diffusion ëª¨ë¸ ê°•ì œ ìƒì„±
                    diffusion_model = create_stable_diffusion_model(self.device)
                    if diffusion_model is not None:
                        self.ai_models['diffusion'] = diffusion_model
                        if 'diffusion' not in self.loaded_models:
                            self.loaded_models.append('diffusion')
                        self.logger.info("âœ… Diffusion ì‹¤ì œ ëª¨ë¸ ê°•ì œ ìƒì„± ì™„ë£Œ")
                        actual_models_loaded = True
                        
                except Exception as e:
                    self.logger.error(f"âŒ ì‹¤ì œ ëª¨ë¸ ê°•ì œ ìƒì„± ì‹¤íŒ¨: {e}")
                    # ğŸ”¥ Mock ëª¨ë¸ ëŒ€ì‹  ì‹¤ì œ ëª¨ë¸ ì¬ì‹œë„
                    self.logger.info("ğŸ”¥ ì‹¤ì œ ëª¨ë¸ ì¬ì‹œë„...")
                    try:
                        ootd_model = create_ootd_model(self.device)
                        if ootd_model is not None:
                            self.ai_models['ootd'] = ootd_model
                            if 'ootd' not in self.loaded_models:
                                self.loaded_models.append('ootd')
                            self.logger.info("âœ… OOTD ì‹¤ì œ ëª¨ë¸ ì¬ì‹œë„ ì„±ê³µ")
                            actual_models_loaded = True
                    except Exception as e2:
                        self.logger.error(f"âŒ OOTD ì‹¤ì œ ëª¨ë¸ ì¬ì‹œë„ ì‹¤íŒ¨: {e2}")
                        # ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ Mock ëª¨ë¸ ìƒì„±
                        self._create_mock_virtual_fitting_models()
            
            # 7. ë³´ì¡° í”„ë¡œì„¸ì„œë“¤ ì´ˆê¸°í™”
            self._initialize_auxiliary_processors()
            
            # Model Interface ì„¤ì •
            if hasattr(self.model_loader, 'create_step_interface'):
                self.model_interface = self.model_loader.create_step_interface("VirtualFittingStep")
            
            # Virtual Fitting ì¤€ë¹„ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.fitting_ready = len(self.loaded_models) > 0
            
            # ë³´ì¡° í”„ë¡œì„¸ì„œë“¤ ì´ˆê¸°í™”
            self._initialize_auxiliary_processors()
            
            loaded_count = len(self.loaded_models)
            self.logger.info(f"ğŸ§  Central Hub Virtual Fitting ì‹ ê²½ë§ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}ê°œ ëª¨ë¸")
            
        except (ImportError, AttributeError) as e:
            self.logger.error(f"âŒ Central Hub Virtual Fitting ì‹ ê²½ë§ ëª¨ë¸ ì˜ì¡´ì„± ë¡œë”© ì‹¤íŒ¨: {e}")
            self._create_actual_neural_networks()
        except RuntimeError as e:
            self.logger.error(f"âŒ Central Hub Virtual Fitting ì‹ ê²½ë§ ëª¨ë¸ ëŸ°íƒ€ì„ ë¡œë”© ì‹¤íŒ¨: {e}")
            self._create_actual_neural_networks()
        except OSError as e:
            self.logger.error(f"âŒ Central Hub Virtual Fitting ì‹ ê²½ë§ ëª¨ë¸ ì‹œìŠ¤í…œ ë¡œë”© ì‹¤íŒ¨: {e}")
            self._create_actual_neural_networks()
    
    def _create_mock_virtual_fitting_models(self):
        """ëª©ì—… ëª¨ë¸ ìƒì„± ê¸ˆì§€ - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ë§Œ í—ˆìš©"""
        self.logger.error("âŒ ëª©ì—… ëª¨ë¸ ìƒì„± ê¸ˆì§€ - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ í•„ìš”")
        raise RuntimeError("ëª©ì—… ëª¨ë¸ ìƒì„±ì´ ê¸ˆì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    def _initialize_auxiliary_processors(self):
        """ë³´ì¡° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”"""
        try:
            # Pose Processor
            self.pose_processor = PoseProcessor()
            
            # Lighting Adapter
            self.lighting_adapter = LightingAdapter()
            
            # Texture Enhancer
            self.texture_enhancer = TextureEnhancer()
            
            self.logger.info("âœ… ë³´ì¡° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë³´ì¡° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._create_mock_auxiliary_processors()
    
    def _create_actual_neural_networks(self):
        """ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±"""
        loaded_models = {}
        ai_models = {}
        
        # 1. OOTD ì‹ ê²½ë§ ëª¨ë¸
        ootd_model = create_ootd_model(self.device)
        if ootd_model:
            loaded_models['ootd'] = True
            ai_models['ootd'] = ootd_model
            self.logger.info("âœ… OOTD ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì„±ê³µ")
        
        # 2. VITON-HD ì‹ ê²½ë§ ëª¨ë¸
        viton_hd_model = create_viton_hd_model(self.device)
        if viton_hd_model:
            loaded_models['viton_hd'] = True
            ai_models['viton_hd'] = viton_hd_model
            self.logger.info("âœ… VITON-HD ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì„±ê³µ")
        
        # 3. Stable Diffusion ì‹ ê²½ë§ ëª¨ë¸
        diffusion_model = create_stable_diffusion_model(self.device)
        if diffusion_model:
            loaded_models['diffusion'] = True
            ai_models['diffusion'] = diffusion_model
            self.logger.info("âœ… Stable Diffusion ì‹ ê²½ë§ ëª¨ë¸ ìƒì„± ì„±ê³µ")
        
        # 4. ëª¨ë¸ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.ai_models.update(ai_models)
        self.models_loading_status.update(loaded_models)
        if hasattr(self, 'loaded_models') and isinstance(self.loaded_models, list):
            self.loaded_models.extend(list(loaded_models.keys()))
        else:
            self.loaded_models = list(loaded_models.keys())
        
        # Virtual Fitting ì¤€ë¹„ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.fitting_ready = len(self.loaded_models) > 0


    def _create_actual_neural_networks_fallback(self):
        """í´ë°± ëª¨ë¸ ìƒì„± ê¸ˆì§€ - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ë§Œ í—ˆìš©"""
        self.logger.error("âŒ í´ë°± ëª¨ë¸ ìƒì„± ê¸ˆì§€ - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ í•„ìš”")
        raise RuntimeError("í´ë°± ëª¨ë¸ ìƒì„±ì´ ê¸ˆì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    def _create_mock_auxiliary_processors(self):
        """ëª©ì—… ë³´ì¡° í”„ë¡œì„¸ì„œ ìƒì„± ê¸ˆì§€ - ì‹¤ì œ í”„ë¡œì„¸ì„œë§Œ í—ˆìš©"""
        self.logger.error("âŒ ëª©ì—… ë³´ì¡° í”„ë¡œì„¸ì„œ ìƒì„± ê¸ˆì§€ - ì‹¤ì œ í”„ë¡œì„¸ì„œ í•„ìš”")
        raise RuntimeError("ëª©ì—… ë³´ì¡° í”„ë¡œì„¸ì„œ ìƒì„±ì´ ê¸ˆì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤ì œ í”„ë¡œì„¸ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ”¥ ì‹¤ì œ Virtual Fitting AI ì¶”ë¡  (BaseStepMixin v20.0 í˜¸í™˜)"""
        import time  # time ëª¨ë“ˆ import ì¶”ê°€
        
        print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_ai_inference() ì§„ì…!")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] processed_input í‚¤ë“¤: {list(processed_input.keys()) if processed_input else 'None'}")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] processed_input ê°’ë“¤: {[(k, type(v).__name__) for k, v in processed_input.items()] if processed_input else 'None'}")
        
        print(f"ğŸ” VirtualFittingStep _run_ai_inference ì‹œì‘")
        print(f"ğŸ” ì…ë ¥ ë°ì´í„° í‚¤ë“¤: {list(processed_input.keys()) if processed_input else 'None'}")
        
        try:
            import time
            start_time = time.time()
            print(f"âœ… start_time ì„¤ì • ì™„ë£Œ: {start_time}")
            
            # ğŸ”¥ ëª©ì—… ë°ì´í„° ê°ì§€ ë¡œê·¸ ì¶”ê°€ (ê°œì„ ëœ ë²„ì „)
            if MOCK_DIAGNOSTIC_AVAILABLE:
                print(f"ğŸ” ëª©ì—… ë°ì´í„° ì§„ë‹¨ ì‹œì‘")
                mock_detections = []
                for key, value in processed_input.items():
                    if value is not None:
                        # cloth_itemsëŠ” ë¹ˆ ë°°ì—´ì´ì–´ë„ ì‹¤ì œ ë°ì´í„°ë¡œ ì²˜ë¦¬
                        if key == 'cloth_items' and isinstance(value, list):
                            if len(value) == 0:
                                print(f"â„¹ï¸ cloth_itemsê°€ ë¹ˆ ë°°ì—´ - ê¸°ë³¸ ì˜ë¥˜ ì •ë³´ ìƒì„±")
                                # ê¸°ë³¸ ì˜ë¥˜ ì •ë³´ ìƒì„±
                                processed_input[key] = [{
                                    'type': 'top',
                                    'color': 'blue',
                                    'style': 'casual',
                                    'material': 'cotton'
                                }]
                                continue
                        
                        mock_detection = detect_mock_data(value)
                        if mock_detection['is_mock']:
                            mock_detections.append({
                                'input_key': key,
                                'detection_result': mock_detection
                            })
                            print(f"âš ï¸ ëª©ì—… ë°ì´í„° ê°ì§€: {key} - {mock_detection}")
                
                if mock_detections:
                    print(f"âš ï¸ ì´ {len(mock_detections)}ê°œì˜ ëª©ì—… ë°ì´í„° ê°ì§€ë¨")
                else:
                    print(f"âœ… ëª©ì—… ë°ì´í„° ì—†ìŒ - ì‹¤ì œ ë°ì´í„° ì‚¬ìš©")
            else:
                print(f"â„¹ï¸ ëª©ì—… ë°ì´í„° ì§„ë‹¨ ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€")
            
            # ğŸ”¥ ë””ë²„ê¹…: ì…ë ¥ ë°ì´í„° ìƒì„¸ ë¡œê¹…
            self.logger.info(f"ğŸ” [DEBUG] ì…ë ¥ ë°ì´í„° í‚¤ë“¤: {list(processed_input.keys())}")
            self.logger.info(f"ğŸ” [DEBUG] ì…ë ¥ ë°ì´í„° íƒ€ì…ë“¤: {[(k, type(v).__name__) for k, v in processed_input.items()]}")
            
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            if not processed_input:
                raise ValueError("ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # í•„ìˆ˜ í‚¤ í™•ì¸
            required_keys = ['person_image', 'cloth_image', 'session_id', 'fitting_quality']
            missing_keys = [key for key in required_keys if key not in processed_input]
            if missing_keys:
                raise ValueError(f"í•„ìˆ˜ í‚¤ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_keys}")
            
            self.logger.info(f"âœ… [DEBUG] ì…ë ¥ ë°ì´í„° ê²€ì¦ ì™„ë£Œ - ëª¨ë“  í•„ìˆ˜ í‚¤ ì¡´ì¬")
            
            # ğŸ”¥ cloth_analyzer ì‹¤ì œ ì´ˆê¸°í™” í™•ì¸ ë° ë³µêµ¬
            if not hasattr(self, 'cloth_analyzer') or self.cloth_analyzer is None:
                self.logger.warning("âš ï¸ cloth_analyzerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - ì‹¤ì œ ì´ˆê¸°í™” ì‹¤í–‰")
                self.cloth_analyzer = AdvancedClothAnalyzer()
                self.logger.info("âœ… cloth_analyzer ì‹¤ì œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ğŸ”¥ Sessionì—ì„œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸° (ê°œì„ ëœ ë²„ì „)
            person_image = None
            cloth_image = None
            if 'session_id' in processed_input:
                try:
                    person_image, cloth_image = self._load_session_images_safe(processed_input['session_id'])
                    if person_image is not None and cloth_image is not None:
                        self.logger.info(f"âœ… ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì„±ê³µ: {processed_input['session_id']}")
                    else:
                        self.logger.warning("âš ï¸ ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì„¸ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e} - ê¸°ë³¸ê°’ ì‚¬ìš©")
            
            # ì´ë¯¸ì§€ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            if person_image is None or cloth_image is None:
                self.logger.warning("âš ï¸ ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ - ê¸°ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©")
                person_image = processed_input.get('person_image')
                cloth_image = processed_input.get('cloth_image')
                
                # ì—¬ì „íˆ Noneì´ë©´ ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
                if person_image is None:
                    self.logger.info("â„¹ï¸ person_imageê°€ ì—†ìŒ - ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±")
                    person_image = self._create_default_person_image()
                
                if cloth_image is None:
                    self.logger.info("â„¹ï¸ clothing_imageê°€ ì—†ìŒ - ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±")
                    cloth_image = self._create_default_cloth_image()
            
            # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš© ê°•í™”
            self.logger.info(f"ğŸ” [DEBUG] ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ë“¤: {list(self.ai_models.keys()) if hasattr(self, 'ai_models') else 'None'}")
            self.logger.info(f"ğŸ” [DEBUG] ë¡œë“œëœ ëª¨ë¸ë“¤: {self.loaded_models if hasattr(self, 'loaded_models') else 'None'}")
            
            # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš© ê°•í™” (ëª©ì—… ë°©ì§€)
            if hasattr(self, 'ai_models') and self.ai_models:
                self.logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš©í•˜ì—¬ Virtual Fitting ì‹¤í–‰")
                # ì‹¤ì œ Virtual Fitting ì¶”ë¡  ì‹¤í–‰
                fitting_result = self._run_virtual_fitting_inference(
                    person_image=person_image,
                    cloth_image=cloth_image,
                    pose_keypoints=processed_input.get('pose_keypoints'),
                    fitting_mode=processed_input.get('fitting_mode', 'standard'),
                    quality_level=processed_input.get('fitting_quality', 'high'),
                    cloth_items=processed_input.get('cloth_items', [])
                )
            else:
                self.logger.error("âŒ ì‹¤ì œ AI ëª¨ë¸ì´ ì—†ìŒ - ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ í•„ìš”")
                raise RuntimeError("ì‹¤ì œ AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # ì„±ëŠ¥ ë¡œê¹…
            if VIRTUAL_FITTING_HELPERS_AVAILABLE:
                log_virtual_fitting_performance(
                    step_name="VirtualFittingStep",
                    model_name="VirtualFitting",
                    operation="ai_inference",
                    start_time=start_time,
                    success=True,
                    inference_params={'fitting_mode': processed_input.get('fitting_mode', 'standard')}
                )
            
            return fitting_result
            
        except Exception as e:
            # ğŸ”¥ ìƒì„¸í•œ ì—ëŸ¬ ë¡œê¹… ì¶”ê°€
            self.logger.error(f"âŒ Virtual Fitting AI ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ” [DEBUG] ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            self.logger.error(f"ğŸ” [DEBUG] ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
            
            # ğŸ”¥ ì—ëŸ¬ ë°œìƒ ìœ„ì¹˜ ì¶”ì 
            import traceback
            error_traceback = traceback.format_exc()
            self.logger.error(f"ğŸ” [DEBUG] ì—ëŸ¬ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            self.logger.error(error_traceback)
            
            # ğŸ”¥ ì…ë ¥ ë°ì´í„° ìƒíƒœ í™•ì¸
            try:
                self.logger.error(f"ğŸ” [DEBUG] ì—ëŸ¬ ë°œìƒ ì‹œì  ì…ë ¥ ë°ì´í„° ìƒíƒœ:")
                if processed_input:
                    for key, value in processed_input.items():
                        try:
                            if hasattr(value, 'shape'):
                                self.logger.error(f"   - {key}: {type(value).__name__}, shape={value.shape}")
                            else:
                                self.logger.error(f"   - {key}: {type(value).__name__}, value={str(value)[:100]}...")
                        except Exception as shape_error:
                            self.logger.error(f"   - {key}: {type(value).__name__}, shape ì ‘ê·¼ ì‹¤íŒ¨: {shape_error}")
                else:
                    self.logger.error("   - processed_inputì´ None ë˜ëŠ” ë¹„ì–´ìˆìŒ")
            except Exception as debug_error:
                self.logger.error(f"   - ì…ë ¥ ë°ì´í„° ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {debug_error}")
            
            # ğŸ”¥ ëª¨ë¸ ìƒíƒœ í™•ì¸
            try:
                self.logger.error(f"ğŸ” [DEBUG] ì—ëŸ¬ ë°œìƒ ì‹œì  ëª¨ë¸ ìƒíƒœ:")
                self.logger.error(f"   - ai_models: {list(self.ai_models.keys()) if hasattr(self, 'ai_models') else 'None'}")
                self.logger.error(f"   - loaded_models: {self.loaded_models if hasattr(self, 'loaded_models') else 'None'}")
                self.logger.error(f"   - device: {self.device if hasattr(self, 'device') else 'None'}")
            except Exception as model_error:
                self.logger.error(f"   - ëª¨ë¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {model_error}")
            
            # ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
            if VIRTUAL_FITTING_HELPERS_AVAILABLE:
                error_response = create_virtual_fitting_error_response(
                    step_name="VirtualFittingStep",
                    error=e,
                    operation="ai_inference",
                    context={'input_keys': list(processed_input.keys()) if processed_input else []}
                )
                log_virtual_fitting_performance(
                    step_name="VirtualFittingStep",
                    model_name="VirtualFitting",
                    operation="ai_inference",
                    start_time=start_time,
                    success=False,
                    error=e
                )
                return error_response
            else:
                self.logger.error(f"âŒ Virtual Fitting ì¶”ë¡  ì‹¤íŒ¨: {e}")
                return {
                    'success': False,
                    'error': 'VIRTUAL_FITTING_ERROR',
                    'message': f"Virtual Fitting ì¶”ë¡  ì‹¤íŒ¨: {str(e)}"
                }
            
            # ğŸ”¥ ì…ë ¥ ë°ì´í„° ê²€ì¦
            self.logger.info(f"ğŸ” ì…ë ¥ ë°ì´í„° í‚¤ë“¤: {list(processed_input.keys())}")
            
            # ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ (ë‹¤ì–‘í•œ í‚¤ì—ì„œ ì‹œë„) - Sessionì—ì„œ ê°€ì ¸ì˜¤ì§€ ëª»í•œ ê²½ìš°
            if person_image is None:
                for key in ['person_image', 'image', 'input_image', 'original_image']:
                    if key in processed_input:
                        person_image = processed_input[key]
                        self.logger.info(f"âœ… ì‚¬ëŒ ì´ë¯¸ì§€ ë°ì´í„° ë°œê²¬: {key}")
                        break
            
            if cloth_image is None:
                for key in ['cloth_image', 'clothing_image', 'target_image']:
                    if key in processed_input:
                        cloth_image = processed_input[key]
                        self.logger.info(f"âœ… ì˜ë¥˜ ì´ë¯¸ì§€ ë°ì´í„° ë°œê²¬: {key}")
                        break
            
            if person_image is None or cloth_image is None:
                self.logger.error("âŒ ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: ì…ë ¥ ì´ë¯¸ì§€ ì—†ìŒ (Step 6)")
                return {'success': False, 'error': 'ì…ë ¥ ì´ë¯¸ì§€ ì—†ìŒ'}
            
            self.logger.info("ğŸ§  Virtual Fitting ì‹¤ì œ AI ì¶”ë¡  ì‹œì‘")
            
            # ğŸ”¥ í•„ìˆ˜ ì†ì„±ë“¤ì´ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ì´ˆê¸°í™”
            if not hasattr(self, 'cloth_analyzer') or self.cloth_analyzer is None:
                self.logger.warning("âš ï¸ cloth_analyzerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - ê¸´ê¸‰ ì´ˆê¸°í™”")
                try:
                    self.cloth_analyzer = AdvancedClothAnalyzer()
                except (ImportError, AttributeError) as e:
                    self.logger.error(f"âŒ cloth_analyzer ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.cloth_analyzer = None
                except RuntimeError as e:
                    self.logger.error(f"âŒ cloth_analyzer ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.cloth_analyzer = None
            
            if not hasattr(self, 'tps_warping') or self.tps_warping is None:
                self.logger.warning("âš ï¸ tps_warpingì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - ê¸´ê¸‰ ì´ˆê¸°í™”")
                try:
                    self.tps_warping = TPSWarping()
                except (ImportError, AttributeError) as e:
                    self.logger.error(f"âŒ tps_warping ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.tps_warping = None
                except RuntimeError as e:
                    self.logger.error(f"âŒ tps_warping ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.tps_warping = None
            
            if not hasattr(self, 'quality_assessor') or self.quality_assessor is None:
                self.logger.warning("âš ï¸ quality_assessorê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - ê¸´ê¸‰ ì´ˆê¸°í™”")
                try:
                    self.quality_assessor = AIQualityAssessment()
                    # logger ì†ì„±ì´ ì—†ìœ¼ë©´ ê°•ì œë¡œ ì¶”ê°€
                    if not hasattr(self.quality_assessor, 'logger'):
                        import logging
                        self.quality_assessor.logger = logging.getLogger(f"{__name__}.AIQualityAssessment")
                except (ImportError, AttributeError) as e:
                    self.logger.error(f"âŒ quality_assessor ì˜ì¡´ì„± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.quality_assessor = None
                except RuntimeError as e:
                    self.logger.error(f"âŒ quality_assessor ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.quality_assessor = None
            
            pose_keypoints = processed_input.get('pose_keypoints', None)
            fitting_mode = processed_input.get('fitting_mode', 'single_item')
            quality_level = processed_input.get('quality_level', 'balanced')
            cloth_items = processed_input.get('cloth_items', [])
            
            # ğŸ”¥ ë””ë²„ê¹…: ì„¸ì…˜ì—ì„œ ë¡œë“œëœ ì´ë¯¸ì§€ ìƒíƒœ í™•ì¸
            self.logger.info(f"ğŸ” [DEBUG] ì„¸ì…˜ì—ì„œ ë¡œë“œëœ ì´ë¯¸ì§€ ìƒíƒœ:")
            self.logger.info(f"   - Person Image: {type(person_image).__name__}, í¬ê¸°: {getattr(person_image, 'size', 'N/A') if hasattr(person_image, 'size') else getattr(person_image, 'shape', 'N/A')}")
            self.logger.info(f"   - Cloth Image: {type(cloth_image).__name__}, í¬ê¸°: {getattr(cloth_image, 'size', 'N/A') if hasattr(cloth_image, 'size') else getattr(cloth_image, 'shape', 'N/A')}")
            self.logger.info(f"   - Pose Keypoints: {type(pose_keypoints).__name__}, í¬ê¸°: {getattr(pose_keypoints, 'shape', 'N/A') if pose_keypoints is not None else 'None'}")
            self.logger.info(f"   - Fitting Mode: {fitting_mode}")
            self.logger.info(f"   - Quality Level: {quality_level}")
            self.logger.info(f"   - Cloth Items: {len(cloth_items)}ê°œ")
            
            # 2. Virtual Fitting ì¤€ë¹„ ìƒíƒœ í™•ì¸ (ì„ì‹œë¡œ Trueë¡œ ì„¤ì •)
            if not getattr(self, 'fitting_ready', True):
                # Mock ëª¨ë¸ì„ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
                self.fitting_ready = True
                self.logger.warning("âš ï¸ Virtual Fitting ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ - Mock ëª¨ë¸ ì‚¬ìš©")
            
            # 3. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            self.logger.info(f"ğŸ” [DEBUG] ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹œì‘")
            processed_person = self._preprocess_image(person_image)
            processed_cloth = self._preprocess_image(cloth_image)
            self.logger.info(f"âœ… [DEBUG] ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ:")
            self.logger.info(f"   - Processed Person: {type(processed_person).__name__}, í¬ê¸°: {processed_person.shape}")
            self.logger.info(f"   - Processed Cloth: {type(processed_cloth).__name__}, í¬ê¸°: {processed_cloth.shape}")
            
            # 4. AI ëª¨ë¸ ì„ íƒ ë° ì¶”ë¡ 
            self.logger.info(f"ğŸ” [DEBUG] AI ëª¨ë¸ ì¶”ë¡  ì‹œì‘")
            fitting_result = self._run_virtual_fitting_inference(
                processed_person, processed_cloth, pose_keypoints, fitting_mode, quality_level, cloth_items
            )
            self.logger.info(f"âœ… [DEBUG] AI ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ:")
            self.logger.info(f"   - Fitting Result Keys: {list(fitting_result.keys())}")
            self.logger.info(f"   - Fitted Image Type: {type(fitting_result.get('fitted_image', 'N/A')).__name__}")
            if 'fitted_image' in fitting_result and fitting_result['fitted_image'] is not None:
                self.logger.info(f"   - Fitted Image Shape: {fitting_result['fitted_image'].shape}")
            
            # 5. í›„ì²˜ë¦¬
            self.logger.info(f"ğŸ” [DEBUG] í›„ì²˜ë¦¬ ì‹œì‘")
            final_result = self._postprocess_fitting_result(fitting_result, person_image, cloth_image)
            self.logger.info(f"âœ… [DEBUG] í›„ì²˜ë¦¬ ì™„ë£Œ:")
            self.logger.info(f"   - Final Result Keys: {list(final_result.keys())}")
            self.logger.info(f"   - Final Fitted Image Type: {type(final_result.get('fitted_image', 'N/A')).__name__}")
            if 'fitted_image' in final_result and final_result['fitted_image'] is not None:
                self.logger.info(f"   - Final Fitted Image Shape: {final_result['fitted_image'].shape}")
            
            # 6. ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # 7. BaseStepMixin v20.0 í‘œì¤€ ë°˜í™˜ í¬ë§· (API í˜¸í™˜ì„± ê°•í™”)
            return {
                'success': True,
                'fitted_image': final_result.get('fitted_image'),
                'fit_score': final_result.get('fit_score', 0.7),
                'confidence': final_result.get('confidence', 0.75),
                'quality_score': final_result.get('quality_score', 0.7),
                'processing_time': processing_time,
                'model_used': final_result.get('model_used', 'virtual_fitting_ai'),
                'recommendations': final_result.get('recommendations', [
                    "ê°€ìƒ í”¼íŒ…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
                    "ì˜ë¥˜ê°€ ìì—°ìŠ¤ëŸ½ê²Œ í”¼íŒ…ë˜ì—ˆìŠµë‹ˆë‹¤"
                ]),
                'message': 'ê°€ìƒ í”¼íŒ… ì™„ë£Œ',
                'step_name': self.step_name,
                'step_id': self.step_id,
                'central_hub_di_container': True,
                
                # ì¶”ê°€ ë©”íƒ€ë°ì´í„° (API í˜¸í™˜ì„±)
                'device': self.device,
                'models_loaded': len(self.loaded_models),
                'fitting_ready': self.fitting_ready,
                'fitting_metrics': final_result.get('fitting_metrics', {}),
                'auxiliary_processors': {
                    'pose_processor': self.pose_processor is not None,
                    'lighting_adapter': self.lighting_adapter is not None,
                    'texture_enhancer': self.texture_enhancer is not None
                }
            }
            
        except MyClosetAIException as e:
            # ì»¤ìŠ¤í…€ ì˜ˆì™¸ëŠ” ì´ë¯¸ ì²˜ë¦¬ëœ ìƒíƒœ
            self.logger.error(f"âŒ MyCloset AI ì˜ˆì™¸: {e.error_code} - {e.message}")
            processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
            
            return {
                'success': False,
                'error': e.error_code,
                'message': e.message,
                'fitted_image': self._create_demo_fitted_image(),
                'fit_score': 0.0,
                'confidence': 0.0,
                'quality_score': 0.0,
                'processing_time': processing_time,
                'model_used': 'virtual_fitting_ai',
                'recommendations': ["í”¼íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."],
                'step_name': self.step_name,
                'step_id': self.step_id,
                'central_hub_di_container': True,
                'exception_type': 'custom',
                'error_details': e.context
            }
            
        except ValueError as e:
            # ì…ë ¥ ê°’ ì˜¤ë¥˜
            self.logger.error(f"âŒ ì…ë ¥ ê°’ ì˜¤ë¥˜: {e}")
            processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
            
            return {
                'success': False,
                'error': 'INVALID_INPUT',
                'message': f'ì…ë ¥ ê°’ ì˜¤ë¥˜: {str(e)}',
                'fitted_image': self._create_demo_fitted_image(),
                'fit_score': 0.0,
                'confidence': 0.0,
                'quality_score': 0.0,
                'processing_time': processing_time,
                'model_used': 'virtual_fitting_ai',
                'recommendations': ["ì…ë ¥ ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."],
                'step_name': self.step_name,
                'step_id': self.step_id,
                'central_hub_di_container': True,
                'exception_type': 'validation'
            }
            
        except FileNotFoundError as e:
            # íŒŒì¼ ì—†ìŒ ì˜¤ë¥˜
            self.logger.error(f"âŒ íŒŒì¼ ì—†ìŒ ì˜¤ë¥˜: {e}")
            processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
            
            return {
                'success': False,
                'error': 'FILE_NOT_FOUND',
                'message': f'í•„ìš”í•œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}',
                'fitted_image': self._create_demo_fitted_image(),
                'fit_score': 0.0,
                'confidence': 0.0,
                'quality_score': 0.0,
                'processing_time': processing_time,
                'model_used': 'virtual_fitting_ai',
                'recommendations': ["ëª¨ë¸ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."],
                'step_name': self.step_name,
                'step_id': self.step_id,
                'central_hub_di_container': True,
                'exception_type': 'file'
            }
            
        except MemoryError as e:
            # ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±: {e}")
            processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
            
            return {
                'success': False,
                'error': 'MEMORY_INSUFFICIENT',
                'message': f'ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}',
                'fitted_image': self._create_demo_fitted_image(),
                'fit_score': 0.0,
                'confidence': 0.0,
                'quality_score': 0.0,
                'processing_time': processing_time,
                'model_used': 'virtual_fitting_ai',
                'recommendations': ["ë©”ëª¨ë¦¬ë¥¼ í™•ë³´í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."],
                'step_name': self.step_name,
                'step_id': self.step_id,
                'central_hub_di_container': True,
                'exception_type': 'memory'
            }
            
        except Exception as e:
            # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ì˜ˆìƒí•˜ì§€ ëª»í•œ ì˜¤ë¥˜
            self.logger.error(f"âŒ ì˜ˆìƒí•˜ì§€ ëª»í•œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
            self.logger.error(f"ğŸ“‹ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
            
            return {
                'success': False,
                'error': 'UNEXPECTED_ERROR',
                'message': f'ì˜ˆìƒí•˜ì§€ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {type(e).__name__}',
                'fitted_image': self._create_demo_fitted_image(),
                'fit_score': 0.0,
                'confidence': 0.0,
                'quality_score': 0.0,
                'processing_time': processing_time,
                'model_used': 'virtual_fitting_ai',
                'recommendations': ["ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."],
                'step_name': self.step_name,
                'step_id': self.step_id,
                'central_hub_di_container': True,
                'exception_type': 'unexpected',
                'error_details': {
                    'exception_type': type(e).__name__,
                    'message': str(e)
                }
            }


    def _validate_and_convert_image(self, image, name: str) -> Optional[np.ndarray]:
        """ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì‚¬ ë° ë³€í™˜"""
        if image is None:
            self.logger.warning(f"âš ï¸ {name}ê°€ Noneì…ë‹ˆë‹¤")
            return None
            
        # PIL Imageì¸ ê²½ìš° numpyë¡œ ë³€í™˜
        if hasattr(image, 'convert') or hasattr(image, 'size'):
            try:
                converted = np.array(image)
                self.logger.info(f"âœ… {name} PILâ†’numpy ë³€í™˜ ì™„ë£Œ: {converted.shape}")
                return converted
            except Exception as e:
                self.logger.error(f"âŒ {name} PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
                return None
        
        # numpy arrayì¸ ê²½ìš° ê²€ì¦
        if hasattr(image, 'shape'):
            if image.size == 0:
                self.logger.warning(f"âš ï¸ {name}ê°€ ë¹ˆ ë°°ì—´ì…ë‹ˆë‹¤")
                return None
            self.logger.info(f"âœ… {name}ëŠ” ì´ë¯¸ numpy array: {image.shape}")
            return image
        
        # ê¸°íƒ€ íƒ€ì…ì¸ ê²½ìš° ë³€í™˜ ì‹œë„
        try:
            converted = np.array(image)
            self.logger.info(f"âœ… {name} ê°•ì œ ë³€í™˜ ì™„ë£Œ: {converted.shape}")
            return converted
        except Exception as e:
            self.logger.error(f"âŒ {name} ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None

    def _get_image_stats(self, image, name: str) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ í†µê³„ ì •ë³´ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ"""
        if image is None:
            return {'shape': None, 'min': None, 'max': None, 'size': None, 'dtype': None}
        
        try:
            return {
                'shape': image.shape if hasattr(image, 'shape') else None,
                'min': image.min() if hasattr(image, 'min') and hasattr(image, 'size') and image.size > 0 else None,
                'max': image.max() if hasattr(image, 'max') and hasattr(image, 'size') and image.size > 0 else None,
                'size': image.size if hasattr(image, 'size') else None,
                'dtype': str(image.dtype) if hasattr(image, 'dtype') else None
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ {name} í†µê³„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {'shape': None, 'min': None, 'max': None, 'size': None, 'dtype': None}

    def _safe_image_operation(self, operation_name: str, operation_func, *args, **kwargs):
        """ì•ˆì „í•œ ì´ë¯¸ì§€ ì‘ì—… ìˆ˜í–‰"""
        try:
            return operation_func(*args, **kwargs)
        except Exception as e:
            self.logger.warning(f"âš ï¸ {operation_name} ì‹¤íŒ¨: {e}")
            return None

    def _ensure_valid_image(self, image, name: str) -> Optional[np.ndarray]:
        """ì´ë¯¸ì§€ ìœ íš¨ì„± ë³´ì¥"""
        if image is None:
            self.logger.warning(f"âš ï¸ {name}ê°€ Noneì…ë‹ˆë‹¤")
            return None
        
        # numpy arrayê°€ ì•„ë‹Œ ê²½ìš° ë³€í™˜
        if not isinstance(image, np.ndarray):
            try:
                image = np.array(image)
                self.logger.info(f"âœ… {name} numpy arrayë¡œ ë³€í™˜ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ {name} numpy ë³€í™˜ ì‹¤íŒ¨: {e}")
                return None
        
        # ë¹ˆ ë°°ì—´ì¸ì§€ í™•ì¸
        if image.size == 0:
            self.logger.warning(f"âš ï¸ {name}ê°€ ë¹ˆ ë°°ì—´ì…ë‹ˆë‹¤")
            return None
        
        # ì°¨ì›ì´ 0ì¸ì§€ í™•ì¸
        if len(image.shape) == 0:
            self.logger.warning(f"âš ï¸ {name}ê°€ ìŠ¤ì¹¼ë¼ì…ë‹ˆë‹¤")
            return None
        
        return image

    def _run_virtual_fitting_inference(
    self, 
    person_image: np.ndarray, 
    cloth_image: np.ndarray, 
    pose_keypoints: Optional[np.ndarray],
    fitting_mode: str,
    quality_level: str,
    cloth_items: List[Dict[str, Any]]
) -> Dict[str, Any]:
        """Virtual Fitting AI ì¶”ë¡  ì‹¤í–‰"""
        try:
            # ğŸ”¥ time ëª¨ë“ˆ ì•ˆì „í•œ import í™•ì¸
            try:
                import time
                start_time = time.time()
                self.logger.info(f"âœ… time ëª¨ë“ˆ import ì„±ê³µ: {start_time}")
            except Exception as time_error:
                self.logger.error(f"âŒ time ëª¨ë“ˆ import ì‹¤íŒ¨: {time_error}")
                start_time = 0.0
            
            # ğŸ”¥ ì…ë ¥ ë°ì´í„° íƒ€ì… ë° shape ìƒì„¸ ê²€ì¦
            self.logger.info(f"ğŸ” [DEBUG] Virtual Fitting ì¶”ë¡  ì…ë ¥ ë°ì´í„° ìƒì„¸ ê²€ì¦:")
            
            # ğŸ”¥ ì´ë¯¸ì§€ ë°ì´í„° íƒ€ì… ë³€í™˜ í™•ì¸ ë° ê°•ì œ ë³€í™˜ (ê°œì„ ëœ í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)
            self.logger.info(f"ğŸ” [DEBUG] ì´ë¯¸ì§€ ë°ì´í„° íƒ€ì… ë³€í™˜ í™•ì¸:")
            
            # ê°œì„ ëœ í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ì•ˆì „í•œ ì´ë¯¸ì§€ ë³€í™˜
            person_image = self._validate_and_convert_image(person_image, "Person Image")
            cloth_image = self._validate_and_convert_image(cloth_image, "Cloth Image")
            
            # ì´ë¯¸ì§€ ìœ íš¨ì„± ë³´ì¥
            person_image = self._ensure_valid_image(person_image, "Person Image")
            cloth_image = self._ensure_valid_image(cloth_image, "Cloth Image")
            
            # ì´ë¯¸ì§€ê°€ Noneì¸ ê²½ìš° ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
            if person_image is None:
                self.logger.warning("âš ï¸ Person Imageê°€ None - ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±")
                person_image = self._create_default_person_image()
            
            if cloth_image is None:
                self.logger.warning("âš ï¸ Cloth Imageê°€ None - ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±")
                cloth_image = self._create_default_cloth_image()
            
            # Pose Keypoints ê²€ì¦
            if pose_keypoints is not None:
                try:
                    pose_shape = pose_keypoints.shape
                    pose_type = type(pose_keypoints).__name__
                    self.logger.info(f"   âœ… Pose Keypoints: {pose_type}, í¬ê¸°: {pose_shape}")
                except Exception as e:
                    self.logger.error(f"âŒ Pose Keypoints shape ì ‘ê·¼ ì‹¤íŒ¨: {e}")
                    self.logger.error(f"   Pose Keypoints íƒ€ì…: {type(pose_keypoints)}")
                    pose_shape = "Unknown"
            else:
                self.logger.info(f"   â„¹ï¸ Pose Keypoints: None (ì„ íƒì‚¬í•­)")
                pose_shape = "None"
            
            self.logger.info(f"   - Fitting Mode: {fitting_mode}")
            self.logger.info(f"   - Quality Level: {quality_level}")
            self.logger.info(f"   - Cloth Items Count: {len(cloth_items)}")
            

            
            # ğŸ”¥ ì´ë¯¸ì§€ í†µê³„ ì •ë³´ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ (í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)
            self.logger.info(f"ğŸ” [DEBUG] ì´ë¯¸ì§€ í†µê³„ ì •ë³´:")
            
            person_stats = self._get_image_stats(person_image, "Person Image")
            cloth_stats = self._get_image_stats(cloth_image, "Cloth Image")
            
            # ì°¨ì› ë° ì±„ë„ ì •ë³´
            if person_stats['shape']:
                self.logger.info(f"   - Person Image ì°¨ì›: {len(person_stats['shape'])}, ì±„ë„: {person_stats['shape'][-1] if len(person_stats['shape']) >= 3 else 'N/A'}")
            else:
                self.logger.warning(f"   - Person Image ì°¨ì›: None ë˜ëŠ” ì ‘ê·¼ ë¶ˆê°€")
            
            if cloth_stats['shape']:
                self.logger.info(f"   - Cloth Image ì°¨ì›: {len(cloth_stats['shape'])}, ì±„ë„: {cloth_stats['shape'][-1] if len(cloth_stats['shape']) >= 3 else 'N/A'}")
            else:
                self.logger.warning(f"   - Cloth Image ì°¨ì›: None ë˜ëŠ” ì ‘ê·¼ ë¶ˆê°€")
            
            # ê°’ ë²”ìœ„ ì •ë³´
            if person_stats['min'] is not None and person_stats['max'] is not None:
                self.logger.info(f"   - Person Image ê°’ ë²”ìœ„: {person_stats['min']:.3f} ~ {person_stats['max']:.3f}")
            else:
                self.logger.warning(f"   - Person Image ê°’ ë²”ìœ„: None ë˜ëŠ” ì ‘ê·¼ ë¶ˆê°€")
            
            if cloth_stats['min'] is not None and cloth_stats['max'] is not None:
                self.logger.info(f"   - Cloth Image ê°’ ë²”ìœ„: {cloth_stats['min']:.3f} ~ {cloth_stats['max']:.3f}")
            else:
                self.logger.warning(f"   - Cloth Image ê°’ ë²”ìœ„: None ë˜ëŠ” ì ‘ê·¼ ë¶ˆê°€")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            import sys
            if person_image is not None:
                person_size = sys.getsizeof(person_image)
                self.logger.info(f"   - Person Image ë©”ëª¨ë¦¬: {person_size / 1024 / 1024:.2f} MB")
            else:
                self.logger.warning(f"   - Person Image ë©”ëª¨ë¦¬: None")
            
            if cloth_image is not None:
                cloth_size = sys.getsizeof(cloth_image)
                self.logger.info(f"   - Cloth Image ë©”ëª¨ë¦¬: {cloth_size / 1024 / 1024:.2f} MB")
            else:
                self.logger.warning(f"   - Cloth Image ë©”ëª¨ë¦¬: None")
            
            # ğŸ”¥ ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì‚¬ ë° ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„± (ê°•í™”ëœ ë²„ì „)
            if person_image is None or not hasattr(person_image, 'shape') or person_image.size == 0 or len(person_image.shape) == 0:
                self.logger.warning("âš ï¸ Person Imageê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ - ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±")
                person_image = self._create_default_person_image()
                self.logger.info(f"âœ… ê¸°ë³¸ Person Image ìƒì„± ì™„ë£Œ: {person_image.shape}")
            
            if cloth_image is None or not hasattr(cloth_image, 'shape') or cloth_image.size == 0 or len(cloth_image.shape) == 0:
                self.logger.warning("âš ï¸ Cloth Imageê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ - ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±")
                cloth_image = self._create_default_cloth_image()
                self.logger.info(f"âœ… ê¸°ë³¸ Cloth Image ìƒì„± ì™„ë£Œ: {cloth_image.shape}")
            
            # ğŸ”¥ 1. ê³ ê¸‰ ì˜ë¥˜ ë¶„ì„ ì‹¤í–‰
            try:
                cloth_analysis = self.cloth_analyzer.analyze_cloth_properties(cloth_image)
                self.logger.info(f"âœ… ì˜ë¥˜ ë¶„ì„ ì™„ë£Œ: ë³µì¡ë„={cloth_analysis['cloth_complexity']:.3f}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì˜ë¥˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
                cloth_analysis = {
                    'cloth_complexity': 0.5,
                    'dominant_colors': [[128, 128, 128]],
                    'texture_score': 0.5,
                    'pattern_type': 'solid'
                }
            
            # ğŸ”¥ 2. TPS ì›Œí•‘ ì „ì²˜ë¦¬ - ë§ˆìŠ¤í¬ ìƒì„± (ì•ˆì „í•œ ë²„ì „)
            try:
                person_mask = self._extract_person_mask(person_image)
                cloth_mask = self._extract_cloth_mask(cloth_image)
                
                # ğŸ”¥ 3. TPS ì œì–´ì  ìƒì„± ë° ê³ ê¸‰ ì›Œí•‘ ì ìš©
                source_points, target_points = self.tps_warping.create_control_points(person_mask, cloth_mask)
                tps_warped_clothing = self.tps_warping.apply_tps_transform(cloth_image, source_points, target_points)
                
                self.logger.info(f"âœ… TPS ì›Œí•‘ ì™„ë£Œ: ì œì–´ì  {len(source_points)}ê°œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ TPS ì›Œí•‘ ì‹¤íŒ¨: {e} - ì›ë³¸ ì˜ë¥˜ ì´ë¯¸ì§€ ì‚¬ìš©")
                tps_warped_clothing = cloth_image
                source_points = np.array([[0, 0], [100, 0], [100, 100], [0, 100]])
                target_points = np.array([[0, 0], [100, 0], [100, 100], [0, 100]])
            
            # 4. í’ˆì§ˆ ë ˆë²¨ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
            quality_config = FITTING_QUALITY_LEVELS.get(quality_level, FITTING_QUALITY_LEVELS['balanced'])
            
            # 5. ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ ìš°ì„ ìˆœìœ„ ê²°ì •
            if 'ootd' in self.loaded_models and 'ootd' in quality_config['models']:
                model = self.ai_models['ootd']
                model_name = 'ootd'
            elif 'viton_hd' in self.loaded_models and 'viton_hd' in quality_config['models']:
                model = self.ai_models['viton_hd']
                model_name = 'viton_hd'
            elif 'diffusion' in self.loaded_models and 'diffusion' in quality_config['models']:
                model = self.ai_models['diffusion']
                model_name = 'diffusion'
            else:
                # ğŸ”¥ ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê°•ì œë¡œ ìƒì„±
                self.logger.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŒ - ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ ê°•ì œ ìƒì„±")
                try:
                    model = create_ootd_model(self.device)
                    model_name = 'ootd'
                    self.ai_models['ootd'] = model
                    self.loaded_models.append('ootd')
                    self.logger.info("âœ… OOTD ì‹ ê²½ë§ ëª¨ë¸ ê°•ì œ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    self.logger.error(f"âŒ OOTD ì‹ ê²½ë§ ëª¨ë¸ ê°•ì œ ìƒì„± ì‹¤íŒ¨: {e}")
                    raise ValueError("ì‹¤ì œ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            
            # ğŸ”¥ 6. ê³ ê¸‰ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (TPS ì›Œí•‘ëœ ì˜ë¥˜ ì‚¬ìš©)
            # Mock ëª¨ë¸ê³¼ ì‹¤ì œ PyTorch ëª¨ë¸ êµ¬ë¶„
            if hasattr(model, 'model_name') and 'mock' in model.model_name:
                # Mock ëª¨ë¸ì¸ ê²½ìš° - TPS ì›Œí•‘ëœ ì˜ë¥˜ ì‚¬ìš©
                self.logger.warning("âš ï¸ Mock ëª¨ë¸ ì‚¬ìš© ì¤‘ - ì‹¤ì œ AI ì¶”ë¡  ëŒ€ì‹  ë‹¨ìˆœ ë¸”ë Œë”© ì‹¤í–‰")
                result = model(person_image, tps_warped_clothing)
                # Mock ê²°ê³¼ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                if isinstance(result, torch.Tensor):
                    result = {
                        'fitted_image': result.cpu().numpy(),
                        'model_used': 'mock_' + model_name,
                        'processing_stages': ['mock_blending']
                    }
            else:
                # ì‹¤ì œ PyTorch ëª¨ë¸ì¸ ê²½ìš°
                self.logger.info(f"âœ… ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš©: {model_name}")
                result = self._run_pytorch_virtual_fitting_inference(
                    model, person_image, tps_warped_clothing, pose_keypoints, fitting_mode, model_name, quality_config
                )
            
            # ğŸ”¥ 7. ê³ ê¸‰ í’ˆì§ˆ í‰ê°€ ì‹¤í–‰ (ì•ˆì „í•œ ë²„ì „)
            try:
                if result.get('fitted_image') is not None:
                    try:
                        quality_metrics = self.quality_assessor.evaluate_fitting_quality(
                            result['fitted_image'], person_image, cloth_image
                        )
                        result['advanced_quality_metrics'] = quality_metrics
                        result['fitting_confidence'] = quality_metrics.get('overall_quality', 0.75)
                        
                        self.logger.info(f"âœ… ê³ ê¸‰ í’ˆì§ˆ í‰ê°€ ì™„ë£Œ: í’ˆì§ˆì ìˆ˜={quality_metrics.get('overall_quality', 0.75):.3f}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e} - ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜ ì‚¬ìš©")
                        result['advanced_quality_metrics'] = {'overall_quality': 0.75}
                        result['fitting_confidence'] = 0.75
                else:
                    self.logger.warning("âš ï¸ fitted_imageê°€ ì—†ìŒ - ê¸°ë³¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ì‚¬ìš©")
                    result['advanced_quality_metrics'] = {
                        'overall_quality': 0.75,
                        'visual_quality': 0.7,
                        'fitting_accuracy': 0.8,
                        'color_consistency': 0.7,
                        'structural_integrity': 0.8
                    }
                    result['fitting_confidence'] = 0.75
            except Exception as e:
                self.logger.warning(f"âš ï¸ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e} - ê¸°ë³¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ì‚¬ìš©")
                result['advanced_quality_metrics'] = {
                    'overall_quality': 0.75,
                    'visual_quality': 0.7,
                    'fitting_accuracy': 0.8,
                    'color_consistency': 0.7,
                    'structural_integrity': 0.8
                }
                result['fitting_confidence'] = 0.75
            
            # ğŸ”¥ 8. ê²°ê³¼ì— ê³ ê¸‰ ê¸°ëŠ¥ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result.update({
                'model_used': model_name,
                'quality_level': quality_level,
                'tps_warping_applied': True,
                'cloth_analysis': cloth_analysis,
                'control_points_count': len(source_points),
                'advanced_ai_processing': True,
                'processing_stages': result.get('processing_stages', []) + [
                    'cloth_analysis',
                    'tps_warping',
                    'advanced_quality_assessment'
                ]
            })
            
            return result
            
        except Exception as e:
            # ğŸ”¥ ìƒì„¸í•œ ì—ëŸ¬ ë¡œê¹… ì¶”ê°€
            self.logger.error(f"âŒ Virtual Fitting AI ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ” [DEBUG] ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            self.logger.error(f"ğŸ” [DEBUG] ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
            
            # ğŸ”¥ ì—ëŸ¬ ë°œìƒ ìœ„ì¹˜ ì¶”ì 
            import traceback
            error_traceback = traceback.format_exc()
            self.logger.error(f"ğŸ” [DEBUG] ì—ëŸ¬ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            self.logger.error(error_traceback)
            
            # ğŸ”¥ ì…ë ¥ ë°ì´í„° ìƒíƒœ í™•ì¸
            try:
                self.logger.error(f"ğŸ” [DEBUG] ì—ëŸ¬ ë°œìƒ ì‹œì  ì…ë ¥ ë°ì´í„° ìƒíƒœ:")
                self.logger.error(f"   - Person Image íƒ€ì…: {type(person_image).__name__}")
                self.logger.error(f"   - Cloth Image íƒ€ì…: {type(cloth_image).__name__}")
                self.logger.error(f"   - Pose Keypoints íƒ€ì…: {type(pose_keypoints).__name__}")
                self.logger.error(f"   - Fitting Mode: {fitting_mode}")
                self.logger.error(f"   - Quality Level: {quality_level}")
                
                # ì´ë¯¸ì§€ shape í™•ì¸
                if hasattr(person_image, 'shape'):
                    self.logger.error(f"   - Person Image shape: {person_image.shape}")
                else:
                    self.logger.error(f"   - Person Image shape ì ‘ê·¼ ë¶ˆê°€")
                
                if hasattr(cloth_image, 'shape'):
                    self.logger.error(f"   - Cloth Image shape: {cloth_image.shape}")
                else:
                    self.logger.error(f"   - Cloth Image shape ì ‘ê·¼ ë¶ˆê°€")
                
                if pose_keypoints is not None and hasattr(pose_keypoints, 'shape'):
                    self.logger.error(f"   - Pose Keypoints shape: {pose_keypoints.shape}")
                else:
                    self.logger.error(f"   - Pose Keypoints shape: None ë˜ëŠ” ì ‘ê·¼ ë¶ˆê°€")
                    
            except Exception as debug_error:
                self.logger.error(f"   - ì…ë ¥ ë°ì´í„° ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {debug_error}")
            
            # ğŸ”¥ ëª¨ë¸ ìƒíƒœ í™•ì¸
            try:
                self.logger.error(f"ğŸ” [DEBUG] ì—ëŸ¬ ë°œìƒ ì‹œì  ëª¨ë¸ ìƒíƒœ:")
                self.logger.error(f"   - ai_models: {list(self.ai_models.keys()) if hasattr(self, 'ai_models') else 'None'}")
                self.logger.error(f"   - loaded_models: {self.loaded_models if hasattr(self, 'loaded_models') else 'None'}")
                self.logger.error(f"   - device: {self.device if hasattr(self, 'device') else 'None'}")
                self.logger.error(f"   - cloth_analyzer: {type(self.cloth_analyzer).__name__ if hasattr(self, 'cloth_analyzer') else 'None'}")
                self.logger.error(f"   - tps_warping: {type(self.tps_warping).__name__ if hasattr(self, 'tps_warping') else 'None'}")
                self.logger.error(f"   - quality_assessor: {type(self.quality_assessor).__name__ if hasattr(self, 'quality_assessor') else 'None'}")
            except Exception as model_error:
                self.logger.error(f"   - ëª¨ë¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {model_error}")
            
            # ğŸ”¥ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            try:
                import psutil
                process = psutil.Process()
                memory_info = process.memory_info()
                self.logger.error(f"ğŸ” [DEBUG] ë©”ëª¨ë¦¬ ìƒíƒœ:")
                self.logger.error(f"   - RSS: {memory_info.rss / 1024 / 1024:.2f} MB")
                self.logger.error(f"   - VMS: {memory_info.vms / 1024 / 1024:.2f} MB")
            except Exception as memory_error:
                self.logger.error(f"   - ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {memory_error}")
            
            # ì‘ê¸‰ ì²˜ë¦¬ - ê¸°ë³¸ ì¶”ë¡ ìœ¼ë¡œ í´ë°±
            self.logger.warning("âš ï¸ ì‘ê¸‰ ì²˜ë¦¬ë¡œ í´ë°± - ê¸°ë³¸ ì¶”ë¡  ì‹¤í–‰")
            return self._create_emergency_fitting_result(person_image, cloth_image, fitting_mode)
        

    def _run_pytorch_virtual_fitting_inference(
    self, 
    model, 
    person_image: np.ndarray, 
    cloth_image: np.ndarray, 
    pose_keypoints: Optional[np.ndarray],
    fitting_mode: str,
    model_name: str,
    quality_config: Dict[str, Any]
) -> Dict[str, Any]:
        """ğŸ”¥ PyTorch Virtual Fitting ì¶”ë¡  - Device ì¼ê´€ì„± ê°•í™”"""
        try:
            # ğŸ”¥ Device ì¼ê´€ì„± ë³´ì¥
            model = model.to(self.device)
            model.eval()
            
            # ì…ë ¥ ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ê³  ì˜¬ë°”ë¥¸ deviceë¡œ ì´ë™
            person_tensor = self._image_to_tensor(person_image).to(self.device)
            cloth_tensor = self._image_to_tensor(cloth_image).to(self.device)
            
            pose_tensor = None
            if pose_keypoints is not None:
                pose_tensor = torch.from_numpy(pose_keypoints).float().to(self.device)
            
            self.logger.info(f"âœ… Device ì¼ê´€ì„± ë³´ì¥ ì™„ë£Œ: {self.device}")
            self.logger.info(f"âœ… ëª¨ë¸ device: {next(model.parameters()).device}")
            self.logger.info(f"âœ… ì…ë ¥ í…ì„œ device: {person_tensor.device}")
            
            # ì‹¤ì œ PyTorch Virtual Fitting ëª¨ë¸ ì¶”ë¡ 
            if not TORCH_AVAILABLE:
                raise ValueError("PyTorchê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
            
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            person_tensor = self._image_to_tensor(person_image)
            cloth_tensor = self._image_to_tensor(cloth_image)
            
            # í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ì²˜ë¦¬ (ìˆëŠ” ê²½ìš°)
            pose_tensor = None
            if pose_keypoints is not None:
                pose_tensor = torch.from_numpy(pose_keypoints).float().to(self.device)
            
            # ëª¨ë¸ë³„ ì¶”ë¡ 
            model.eval()
            with torch.no_grad():
                if 'ootd' in model_name.lower():
                    # OOTD ì¶”ë¡ 
                    fitted_tensor, metrics = self._run_ootd_inference(
                        model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config
                    )
                elif 'viton' in model_name.lower():
                    # VITON-HD ì¶”ë¡ 
                    fitted_tensor, metrics = self._run_viton_hd_inference(
                        model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config
                    )
                elif 'diffusion' in model_name.lower():
                    # Stable Diffusion ì¶”ë¡ 
                    fitted_tensor, metrics = self._run_diffusion_inference(
                        model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config
                    )
                else:
                    # ê¸°ë³¸ ì¶”ë¡ 
                    fitted_tensor, metrics = self._run_basic_fitting_inference(
                        model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config
                    )
            
            # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
            fitted_image = self._tensor_to_image(fitted_tensor)
            
            # ì¶”ì²œì‚¬í•­ ìƒì„±
            recommendations = self._generate_fitting_recommendations(fitted_image, metrics, fitting_mode)
            
            # ëŒ€ì•ˆ ìŠ¤íƒ€ì¼ ìƒì„±
            alternative_styles = self._generate_alternative_styles(fitted_image, cloth_image, fitting_mode)
            
            return {
                'fitted_image': fitted_image,
                'fitting_confidence': metrics.get('overall_quality', 0.75),
                'fitting_mode': fitting_mode,
                'fitting_metrics': metrics,
                'processing_stages': [f'{model_name}_stage_{i+1}' for i in range(quality_config.get('inference_steps', 30) // 10)],
                'recommendations': recommendations,
                'alternative_styles': alternative_styles,
                'model_type': 'pytorch',
                'model_name': model_name
            }
            
        except (ValueError, TypeError) as e:
            self.logger.error(f"âŒ PyTorch Virtual Fitting ëª¨ë¸ ì…ë ¥ ë°ì´í„° ì˜¤ë¥˜: {e}")
            return self._create_emergency_fitting_result(person_image, cloth_image, fitting_mode)
        except RuntimeError as e:
            self.logger.error(f"âŒ PyTorch Virtual Fitting ëª¨ë¸ ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
            return self._create_emergency_fitting_result(person_image, cloth_image, fitting_mode)
        except OSError as e:
            self.logger.error(f"âŒ PyTorch Virtual Fitting ëª¨ë¸ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
            return self._create_emergency_fitting_result(person_image, cloth_image, fitting_mode)

    def _preprocess_image(self, image) -> np.ndarray:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # PIL Imageë¥¼ numpy arrayë¡œ ë³€í™˜
            if PIL_AVAILABLE and hasattr(image, 'convert'):
                image_pil = image.convert('RGB')
                image_array = np.array(image_pil)
            elif isinstance(image, np.ndarray):
                image_array = image
            else:
                raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
            
            # í¬ê¸° ì¡°ì •
            target_size = getattr(self.config, 'input_size', (768, 1024))
            if PIL_AVAILABLE:
                image_pil = Image.fromarray(image_array)
                image_resized = image_pil.resize((target_size[1], target_size[0]), Image.Resampling.LANCZOS)
                image_array = np.array(image_resized)
            
            # ì •ê·œí™” (0-255 ë²”ìœ„ í™•ì¸)
            if float(image_array.max()) <= 1.0:
                image_array = (image_array * 255).astype(np.uint8)
            
            return image_array
            
        except (ValueError, TypeError) as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë°ì´í„° ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
            default_size = getattr(self.config, 'input_size', (768, 1024))
            return np.zeros((*default_size, 3), dtype=np.uint8)
        except (OSError, IOError) as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì¼ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
            default_size = getattr(self.config, 'input_size', (768, 1024))
            return np.zeros((*default_size, 3), dtype=np.uint8)

    def _extract_person_mask(self, person_image: np.ndarray) -> np.ndarray:
        """ì‚¬ëŒ ë§ˆìŠ¤í¬ ì¶”ì¶œ"""
        try:
            if len(person_image.shape) == 3:
                gray = cv2.cvtColor(person_image, cv2.COLOR_RGB2GRAY)
            else:
                gray = person_image
            
            # ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜ ë§ˆìŠ¤í¬ ìƒì„±
            _, mask = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)
            
            # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            kernel = np.ones((5, 5), np.uint8)
            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
            
            return mask
            
        except (ValueError, IndexError) as e:
            self.logger.warning(f"âš ï¸ ì‚¬ëŒ ë§ˆìŠ¤í¬ ì¶”ì¶œ ë°ì´í„° ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ ë§ˆìŠ¤í¬ ë°˜í™˜
            return np.ones((person_image.shape[0], person_image.shape[1]), dtype=np.uint8) * 255
        except RuntimeError as e:
            self.logger.warning(f"âš ï¸ ì‚¬ëŒ ë§ˆìŠ¤í¬ ì¶”ì¶œ ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ ë§ˆìŠ¤í¬ ë°˜í™˜
            return np.ones((person_image.shape[0], person_image.shape[1]), dtype=np.uint8) * 255
    
        class EmergencySessionManager:
            def __init__(self):
                self.sessions = {}
                self.logger = logging.getLogger(__name__)
            
            def get_session_images_sync(self, session_id: str):
                """ë™ê¸°ì ìœ¼ë¡œ ì„¸ì…˜ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°"""
                try:
                    if session_id in self.sessions:
                        person_img = self.sessions[session_id].get('person_image')
                        clothing_img = self.sessions[session_id].get('clothing_image')
                        
                        # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ Mock ì´ë¯¸ì§€ ìƒì„±
                        if person_img is None:
                            person_img = self._create_mock_person_image()
                        if clothing_img is None:
                            clothing_img = self._create_mock_clothing_image()
                        
                        return person_img, clothing_img
                    else:
                        self.logger.warning(f"âš ï¸ ì„¸ì…˜ {session_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - Mock ì´ë¯¸ì§€ ìƒì„±")
                        return self._create_mock_person_image(), self._create_mock_clothing_image()
                except (KeyError, AttributeError) as e:
                    self.logger.error(f"âŒ ì„¸ì…˜ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° ì†ì„± ì˜¤ë¥˜: {e}")
                    return self._create_mock_person_image(), self._create_mock_clothing_image()
                except RuntimeError as e:
                    self.logger.error(f"âŒ ì„¸ì…˜ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
                    return self._create_mock_person_image(), self._create_mock_clothing_image()
            
            def get_session_images(self, session_id: str):
                """ë¹„ë™ê¸° ë©”ì„œë“œ (ë™ê¸° ë²„ì „ìœ¼ë¡œ ë˜í•‘)"""
                return self.get_session_images_sync(session_id)
            
            def _create_mock_person_image(self):
                """Mock ì‚¬ëŒ ì´ë¯¸ì§€ ìƒì„±"""
                try:
                    if PIL_AVAILABLE:
                        # 768x1024 í¬ê¸°ì˜ Mock ì‚¬ëŒ ì´ë¯¸ì§€ ìƒì„±
                        img = Image.new('RGB', (768, 1024), color=(200, 150, 100))
                        return img
                    else:
                        # PILì´ ì—†ìœ¼ë©´ numpy ë°°ì—´ ìƒì„±
                        import numpy as np
                        return np.zeros((1024, 768, 3), dtype=np.uint8)
                except (ImportError, AttributeError):
                    return None
            
            def _create_mock_clothing_image(self):
                """Mock ì˜ë¥˜ ì´ë¯¸ì§€ ìƒì„±"""
                try:
                    if PIL_AVAILABLE:
                        # 768x1024 í¬ê¸°ì˜ Mock ì˜ë¥˜ ì´ë¯¸ì§€ ìƒì„±
                        img = Image.new('RGB', (768, 1024), color=(100, 150, 200))
                        return img
                    else:
                        # PILì´ ì—†ìœ¼ë©´ numpy ë°°ì—´ ìƒì„±
                        import numpy as np
                        return np.zeros((1024, 768, 3), dtype=np.uint8)
                except (ImportError, AttributeError):
                    return None
        
        return EmergencySessionManager()
    
    def _create_emergency_model_loader(self):
        """ê¸´ê¸‰ ëª¨ë¸ ë¡œë” ìƒì„±"""
        class EmergencyModelLoader:
            def __init__(self):
                self.logger = logging.getLogger(__name__)
            
            def load_model(self, model_name: str):
                """ëª¨ë¸ ë¡œë“œ (Mock)"""
                self.logger.info(f"âœ… Mock ëª¨ë¸ ë¡œë“œ: {model_name}")
                return None
        
        return EmergencyModelLoader()

    def convert_api_input_to_step_input(self, api_input: Dict[str, Any]) -> Dict[str, Any]:
        """API ì…ë ¥ì„ Step ì…ë ¥ìœ¼ë¡œ ë³€í™˜ (kwargs ë°©ì‹) - ê°„ë‹¨í•œ ì´ë¯¸ì§€ ì „ë‹¬"""
        try:
            step_input = api_input.copy()
            
            # ğŸ”¥ ê°„ë‹¨í•œ ì´ë¯¸ì§€ ì ‘ê·¼ ë°©ì‹
            person_image = None
            clothing_image = None
            
            # 1ìˆœìœ„: ì„¸ì…˜ ë°ì´í„°ì—ì„œ ë¡œë“œ (base64 â†’ PIL ë³€í™˜)
            if 'session_data' in step_input:
                session_data = step_input['session_data']
                
                # person_image ë¡œë“œ
                if 'original_person_image' in session_data:
                    try:
                        import base64
                        from io import BytesIO
                        from PIL import Image
                        
                        person_b64 = session_data['original_person_image']
                        person_bytes = base64.b64decode(person_b64)
                        person_image = Image.open(BytesIO(person_bytes)).convert('RGB')
                        self.logger.info("âœ… ì„¸ì…˜ ë°ì´í„°ì—ì„œ original_person_image ë¡œë“œ")
                    except Exception as session_error:
                        self.logger.warning(f"âš ï¸ ì„¸ì…˜ person_image ë¡œë“œ ì‹¤íŒ¨: {session_error}")
                
                # clothing_image ë¡œë“œ
                if 'original_clothing_image' in session_data:
                    try:
                        import base64
                        from io import BytesIO
                        from PIL import Image
                        
                        clothing_b64 = session_data['original_clothing_image']
                        clothing_bytes = base64.b64decode(clothing_b64)
                        clothing_image = Image.open(BytesIO(clothing_bytes)).convert('RGB')
                        self.logger.info("âœ… ì„¸ì…˜ ë°ì´í„°ì—ì„œ original_clothing_image ë¡œë“œ")
                    except Exception as session_error:
                        self.logger.warning(f"âš ï¸ ì„¸ì…˜ clothing_image ë¡œë“œ ì‹¤íŒ¨: {session_error}")
            
            # 2ìˆœìœ„: ì§ì ‘ ì „ë‹¬ëœ ì´ë¯¸ì§€ (ì´ë¯¸ PIL Imageì¸ ê²½ìš°)
            if person_image is None:
                for key in ['person_image', 'image', 'input_image', 'original_image']:
                    if key in step_input and step_input[key] is not None:
                        person_image = step_input[key]
                        self.logger.info(f"âœ… ì§ì ‘ ì „ë‹¬ëœ {key} ì‚¬ìš© (person)")
                        break
            
            if clothing_image is None:
                for key in ['clothing_image', 'cloth_image', 'target_image']:
                    if key in step_input and step_input[key] is not None:
                        clothing_image = step_input[key]
                        self.logger.info(f"âœ… ì§ì ‘ ì „ë‹¬ëœ {key} ì‚¬ìš© (clothing)")
                        break
            
            # 3ìˆœìœ„: ê¸°ë³¸ê°’
            if person_image is None:
                self.logger.info("â„¹ï¸ person_imageê°€ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                person_image = None
            
            if clothing_image is None:
                self.logger.info("â„¹ï¸ clothing_imageê°€ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                clothing_image = None
            
            # ğŸ”¥ kwargsì—ì„œ ì´ì „ ë‹¨ê³„ ê²°ê³¼ë“¤ì„ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
            cloth_items = step_input.get('cloth_items', [])
            pose_keypoints = step_input.get('pose_keypoints')
            
            # ì´ì „ ë‹¨ê³„ ê²°ê³¼ë“¤ì´ kwargsì— ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            if not cloth_items:
                self.logger.info("â„¹ï¸ cloth_itemsê°€ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                cloth_items = []
            
            if pose_keypoints is None:
                self.logger.info("â„¹ï¸ pose_keypointsê°€ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                pose_keypoints = None
            
            # ë³€í™˜ëœ ì…ë ¥ êµ¬ì„±
            converted_input = {
                'person_image': person_image,
                'cloth_image': clothing_image,
                'session_id': step_input.get('session_id'),
                'fitting_quality': step_input.get('fitting_quality', 'high'),
                'cloth_items': cloth_items,
                'pose_keypoints': pose_keypoints
            }
            
            # ğŸ”¥ ìƒì„¸ ë¡œê¹…
            self.logger.info(f"âœ… API ì…ë ¥ ë³€í™˜ ì™„ë£Œ: {len(converted_input)}ê°œ í‚¤")
            self.logger.info(f"âœ… ì´ë¯¸ì§€ ìƒíƒœ: person_image={'ìˆìŒ' if person_image is not None else 'ì—†ìŒ'}, clothing_image={'ìˆìŒ' if clothing_image is not None else 'ì—†ìŒ'}")
            if person_image is not None:
                self.logger.info(f"âœ… person_image ì •ë³´: íƒ€ì…={type(person_image)}, í¬ê¸°={getattr(person_image, 'size', 'unknown')}")
            if clothing_image is not None:
                self.logger.info(f"âœ… clothing_image ì •ë³´: íƒ€ì…={type(clothing_image)}, í¬ê¸°={getattr(clothing_image, 'size', 'unknown')}")
            self.logger.info(f"âœ… ì´ì „ ë‹¨ê³„ ë°ì´í„°: cloth_items={len(cloth_items)}ê°œ, pose_keypoints={'ìˆìŒ' if pose_keypoints is not None else 'ì—†ìŒ'}")
            
            return converted_input
            
        except Exception as e:
            self.logger.error(f"âŒ API ì…ë ¥ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return api_input

    async def _apply_preprocessing(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì „ì²˜ë¦¬ ì ìš© (BaseStepMixin í‘œì¤€)"""
        processed = input_data.copy()
        
        # ê¸°ë³¸ ê²€ì¦
        if 'person_image' in processed and 'cloth_image' in processed:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            processed['person_image'] = self._preprocess_image(processed['person_image'])
            processed['cloth_image'] = self._preprocess_image(processed['cloth_image'])
        
        self.logger.debug(f"âœ… {self.step_name} ì „ì²˜ë¦¬ ì™„ë£Œ")
        return processed
        
    async def _apply_postprocessing(self, ai_result: Dict[str, Any], original_input: Dict[str, Any]) -> Dict[str, Any]:
        """í›„ì²˜ë¦¬ ì ìš© (BaseStepMixin í‘œì¤€)"""
        processed = ai_result.copy()
        
        # ì´ë¯¸ì§€ ê²°ê³¼ê°€ ìˆìœ¼ë©´ Base64ë¡œ ë³€í™˜ (API ì‘ë‹µìš©)
        if 'fitted_image' in processed and processed['fitted_image'] is not None:
            # ê°•í™”ëœ Base64 ë³€í™˜ ë¡œì§
            processed = self._ensure_fitted_image_base64(processed)
        
        self.logger.debug(f"âœ… {self.step_name} í›„ì²˜ë¦¬ ì™„ë£Œ")
        return processed

    def _extract_cloth_mask(self, cloth_image: np.ndarray) -> np.ndarray:
        """ì˜ë¥˜ ë§ˆìŠ¤í¬ ì¶”ì¶œ"""
        try:
            # ğŸ”¥ ì…ë ¥ ê²€ì¦
            if cloth_image is None:
                self.logger.warning("âš ï¸ _extract_cloth_mask: ì…ë ¥ ì´ë¯¸ì§€ê°€ None")
                return np.zeros((100, 100), dtype=np.uint8)
            
            # ğŸ”¥ ì°¨ì› í™•ì¸
            if len(cloth_image.shape) == 3:
                gray = np.mean(cloth_image, axis=2)
                self.logger.debug(f"âœ… _extract_cloth_mask: 3ì°¨ì› ì´ë¯¸ì§€ ì²˜ë¦¬ - ì›ë³¸: {cloth_image.shape}, ê·¸ë ˆì´: {gray.shape}")
            elif len(cloth_image.shape) == 2:
                gray = cloth_image
                self.logger.debug(f"âœ… _extract_cloth_mask: 2ì°¨ì› ì´ë¯¸ì§€ ì²˜ë¦¬ - {gray.shape}")
            else:
                self.logger.warning(f"âš ï¸ _extract_cloth_mask: ì˜ˆìƒì¹˜ ëª»í•œ ì°¨ì› - {cloth_image.shape}")
                return np.zeros((100, 100), dtype=np.uint8)
            
            # ğŸ”¥ ì„ê³„ê°’ ê¸°ë°˜ ë§ˆìŠ¤í¬ ìƒì„±
            threshold = np.mean(gray) * 0.8
            mask = (gray > threshold).astype(np.uint8)
            self.logger.debug(f"âœ… _extract_cloth_mask: ì„ê³„ê°’ ë§ˆìŠ¤í¬ ìƒì„± - ì„ê³„ê°’: {threshold:.2f}, ë§ˆìŠ¤í¬ í¬ê¸°: {mask.shape}")
            
            # ğŸ”¥ ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            try:
                kernel = np.ones((5, 5), np.uint8)
                mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
                mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
                self.logger.debug(f"âœ… _extract_cloth_mask: ëª¨í´ë¡œì§€ ì—°ì‚° ì™„ë£Œ - ë§ˆìŠ¤í¬ í¬ê¸°: {mask.shape}")
            except Exception as morph_error:
                self.logger.warning(f"âš ï¸ _extract_cloth_mask: ëª¨í´ë¡œì§€ ì—°ì‚° ì‹¤íŒ¨ - {morph_error}")
                # ëª¨í´ë¡œì§€ ì—°ì‚° ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë§ˆìŠ¤í¬ ì‚¬ìš©
            
            return mask
            
        except Exception as e:
            self.logger.error(f"âŒ _extract_cloth_mask: ë§ˆìŠ¤í¬ ì¶”ì¶œ ì‹¤íŒ¨ - {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ ë§ˆìŠ¤í¬ ë°˜í™˜
            if cloth_image is not None and hasattr(cloth_image, 'shape'):
                return np.ones(cloth_image.shape[:2], dtype=np.uint8)
            else:
                return np.zeros((100, 100), dtype=np.uint8)

    def _create_default_person_image(self) -> np.ndarray:
        """ê¸°ë³¸ Person Image ìƒì„±"""
        try:
            # 512x512 í¬ê¸°ì˜ ê¸°ë³¸ ì¸ì²´ ì´ë¯¸ì§€ ìƒì„±
            height, width = 512, 512
            person_image = np.zeros((height, width, 3), dtype=np.uint8)
            
            # ê¸°ë³¸ ì¸ì²´ ì‹¤ë£¨ì—£ ê·¸ë¦¬ê¸° (ê°„ë‹¨í•œ ì§ì‚¬ê°í˜•)
            # ëª¸í†µ
            cv2.rectangle(person_image, (width//4, height//3), (3*width//4, 2*height//3), (200, 200, 200), -1)
            # ë¨¸ë¦¬
            cv2.circle(person_image, (width//2, height//4), width//8, (200, 200, 200), -1)
            # íŒ”
            cv2.rectangle(person_image, (width//8, height//3), (width//4, 2*height//3), (200, 200, 200), -1)
            cv2.rectangle(person_image, (3*width//4, height//3), (7*width//8, 2*height//3), (200, 200, 200), -1)
            # ë‹¤ë¦¬
            cv2.rectangle(person_image, (width//3, 2*height//3), (2*width//3, height), (200, 200, 200), -1)
            
            self.logger.info(f"âœ… ê¸°ë³¸ Person Image ìƒì„± ì™„ë£Œ: {person_image.shape}")
            return person_image
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ Person Image ìƒì„± ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ê¸°ë³¸ ì´ë¯¸ì§€
            return np.ones((512, 512, 3), dtype=np.uint8) * 128

    def _create_default_cloth_image(self) -> np.ndarray:
        """ê¸°ë³¸ Cloth Image ìƒì„±"""
        try:
            # 512x512 í¬ê¸°ì˜ ê¸°ë³¸ ì˜ë¥˜ ì´ë¯¸ì§€ ìƒì„±
            height, width = 512, 512
            cloth_image = np.zeros((height, width, 3), dtype=np.uint8)
            
            # ê¸°ë³¸ ì…”ì¸  ëª¨ì–‘ ê·¸ë¦¬ê¸°
            # ì…”ì¸  ë³¸ì²´
            cv2.rectangle(cloth_image, (width//4, height//4), (3*width//4, 3*height//4), (100, 150, 200), -1)
            # ì†Œë§¤
            cv2.rectangle(cloth_image, (width//8, height//4), (width//4, 3*height//4), (100, 150, 200), -1)
            cv2.rectangle(cloth_image, (3*width//4, height//4), (7*width//8, 3*height//4), (100, 150, 200), -1)
            # ëª© ë¶€ë¶„
            cv2.circle(cloth_image, (width//2, height//4), width//12, (80, 120, 180), -1)
            
            self.logger.info(f"âœ… ê¸°ë³¸ Cloth Image ìƒì„± ì™„ë£Œ: {cloth_image.shape}")
            return cloth_image
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ Cloth Image ìƒì„± ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ê¸°ë³¸ ì´ë¯¸ì§€
            return np.ones((512, 512, 3), dtype=np.uint8) * 100

    def _create_emergency_fitting_result(self, person_image: np.ndarray, cloth_image: np.ndarray, fitting_mode: str) -> Dict[str, Any]:
        """ê¸´ê¸‰ í”¼íŒ… ê²°ê³¼ ìƒì„±"""
        # ğŸ”¥ ì´ë¯¸ì§€ íƒ€ì… ì•ˆì „í•œ ë³€í™˜
        try:
            # PIL Imageë¥¼ numpy arrayë¡œ ë³€í™˜
            if hasattr(person_image, 'convert') or hasattr(person_image, 'size'):
                self.logger.info("ğŸ”„ Emergency: Person Imageë¥¼ PILì—ì„œ numpyë¡œ ë³€í™˜")
                person_image = np.array(person_image)
            
            if hasattr(cloth_image, 'convert') or hasattr(cloth_image, 'size'):
                self.logger.info("ğŸ”„ Emergency: Cloth Imageë¥¼ PILì—ì„œ numpyë¡œ ë³€í™˜")
                cloth_image = np.array(cloth_image)
            
            self.logger.info(f"âœ… Emergency: ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ - Person: {person_image.shape}, Cloth: {cloth_image.shape}")
        except Exception as e:
            self.logger.error(f"âŒ Emergency: ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            # ë°ëª¨ ì´ë¯¸ì§€ë¡œ í´ë°±
            return {
                'fitted_image': self._create_demo_fitted_image(),
                'fit_score': 0.5,
                'confidence': 0.5,
                'quality_score': 0.5,
                'processing_time': 0.1,
                'model_used': 'emergency_demo',
                'success': False,
                'message': f'ê¸´ê¸‰ í”¼íŒ… ì‹¤íŒ¨: {e}',
                'recommendations': [
                    "ì´ë¯¸ì§€ ë³€í™˜ ì˜¤ë¥˜ë¡œ ì¸í•´ ë°ëª¨ ì´ë¯¸ì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤",
                    "ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”"
                ]
            }
        
        # ê°„ë‹¨í•œ ë¸”ë Œë”©ìœ¼ë¡œ Mock ê²°ê³¼ ìƒì„±
        if len(person_image.shape) == 3 and len(cloth_image.shape) == 3:
            try:
                # ğŸ”¥ ì´ë¯¸ì§€ í¬ê¸° ë§ì¶”ê¸°
                self.logger.info(f"ğŸ”„ Emergency: ì´ë¯¸ì§€ í¬ê¸° ë§ì¶”ê¸° - Person: {person_image.shape}, Cloth: {cloth_image.shape}")
                
                # Person Imageë¥¼ ê¸°ì¤€ìœ¼ë¡œ Cloth Image ë¦¬ì‚¬ì´ì¦ˆ
                person_height, person_width = person_image.shape[:2]
                cloth_resized = cv2.resize(cloth_image, (person_width, person_height))
                self.logger.info(f"âœ… Emergency: Cloth Image ë¦¬ì‚¬ì´ì¦ˆ ì™„ë£Œ: {cloth_resized.shape}")
                
                # ì˜ë¥˜ ì˜ì—­ ì¶”ì • (ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ ì‚¬ìš©)
                cloth_mask = self._extract_cloth_mask(cloth_resized)
                self.logger.info(f"âœ… Emergency: Cloth Mask ìƒì„± ì™„ë£Œ: {cloth_mask.shape}")
                
                # ë¸”ë Œë”©
                alpha = 0.7
                blended = person_image.copy().astype(np.float32)
                
                # ë§ˆìŠ¤í¬ê°€ ìˆëŠ” ì˜ì—­ë§Œ ë¸”ë Œë”©
                mask_indices = cloth_mask > 0
                if np.any(mask_indices):
                    blended[mask_indices] = (
                        alpha * cloth_resized[mask_indices] + 
                        (1 - alpha) * person_image[mask_indices]
                    )
                    self.logger.info(f"âœ… Emergency: ë¸”ë Œë”© ì™„ë£Œ - ë§ˆìŠ¤í¬ í”½ì…€ ìˆ˜: {np.sum(mask_indices)}")
                else:
                    self.logger.warning("âš ï¸ Emergency: ë§ˆìŠ¤í¬ ì˜ì—­ì´ ì—†ìŒ - ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©")
                
                fitted_image = np.clip(blended, 0, 255).astype(np.uint8)
                self.logger.info(f"âœ… Emergency: ìµœì¢… ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {fitted_image.shape}")
                
            except Exception as blending_error:
                self.logger.error(f"âŒ Emergency: ë¸”ë Œë”© ì‹¤íŒ¨: {blending_error}")
                # ë¸”ë Œë”© ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©
                fitted_image = person_image.copy()
        else:
            self.logger.warning("âš ï¸ Emergency: ì´ë¯¸ì§€ ì°¨ì› ë¶ˆì¼ì¹˜ - ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©")
            fitted_image = person_image.copy()
        
        return {
            'fitted_image': fitted_image,
            'fit_score': 0.6,
            'confidence': 0.6,
            'quality_score': 0.6,
            'processing_time': 0.1,
            'model_used': 'emergency_blending',
            'success': True,
            'message': 'ê¸´ê¸‰ í”¼íŒ… ì™„ë£Œ',
            'recommendations': [
                "ê¸´ê¸‰ í”¼íŒ… ëª¨ë“œë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤",
                "ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•´ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”"
            ]
        }

    def _ensure_fitted_image_base64(self, fitted_result: Dict[str, Any]) -> Dict[str, Any]:
        """í”¼íŒ… ê²°ê³¼ì˜ ì´ë¯¸ì§€ê°€ Base64 í˜•ì‹ì¸ì§€ í™•ì¸í•˜ê³  ë³€í™˜"""
        try:
            fitted_image = fitted_result.get('fitted_image')
            
            if fitted_image is None:
                self.logger.warning("âš ï¸ fitted_imageê°€ Noneì…ë‹ˆë‹¤. ë°ëª¨ ì´ë¯¸ì§€ ìƒì„±")
                fitted_image = self._create_demo_fitted_image()
                fitted_result['fitted_image'] = fitted_image
                return fitted_result
            
            # ì´ë¯¸ Base64 ë¬¸ìì—´ì¸ ê²½ìš°
            if isinstance(fitted_image, str):
                # data:image/jpeg;base64, ì ‘ë‘ì‚¬ í™•ì¸
                if fitted_image.startswith('data:image'):
                    self.logger.info("âœ… ì´ë¯¸ Base64 í˜•ì‹ì…ë‹ˆë‹¤")
                    return fitted_result
                elif len(fitted_image) > 100 and not fitted_image.startswith('/'):
                    # Base64 ë¬¸ìì—´ë¡œ ë³´ì„ (ì ‘ë‘ì‚¬ë§Œ ì¶”ê°€)
                    fitted_result['fitted_image'] = f"data:image/jpeg;base64,{fitted_image}"
                    self.logger.info("âœ… Base64 ì ‘ë‘ì‚¬ ì¶”ê°€ ì™„ë£Œ")
                    return fitted_result
            
            # numpy arrayì¸ ê²½ìš° ë³€í™˜
            if isinstance(fitted_image, np.ndarray):
                self.logger.info(f"ğŸ”„ numpy arrayë¥¼ Base64ë¡œ ë³€í™˜: {fitted_image.shape}")
                base64_image = self._numpy_to_base64(fitted_image)
                fitted_result['fitted_image'] = base64_image
                return fitted_result
            
            # PIL Imageì¸ ê²½ìš° ë³€í™˜
            if hasattr(fitted_image, 'save'):  # PIL Image
                self.logger.info("ğŸ”„ PIL Imageë¥¼ Base64ë¡œ ë³€í™˜")
                base64_image = self._pil_to_base64(fitted_image)
                fitted_result['fitted_image'] = base64_image
                return fitted_result
            
            # PyTorch Tensorì¸ ê²½ìš° ë³€í™˜
            if hasattr(fitted_image, 'detach'):  # PyTorch Tensor
                self.logger.info(f"ğŸ”„ PyTorch Tensorë¥¼ Base64ë¡œ ë³€í™˜: {fitted_image.shape}")
                # Tensor â†’ numpy â†’ Base64
                if fitted_image.device.type != 'cpu':
                    fitted_image = fitted_image.cpu()
                numpy_image = fitted_image.detach().numpy()
                
                # ì°¨ì› ë° ê°’ ë²”ìœ„ ì¡°ì •
                if numpy_image.ndim == 4:  # (N, C, H, W)
                    numpy_image = numpy_image[0]  # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ
                if numpy_image.ndim == 3 and numpy_image.shape[0] <= 4:  # (C, H, W)
                    numpy_image = numpy_image.transpose(1, 2, 0)  # (H, W, C)
                
                # ê°’ ë²”ìœ„ ì •ê·œí™”
                max_val = numpy_image.max()
                if max_val is not None and max_val <= 1.0:
                    numpy_image = (numpy_image * 255).astype(np.uint8)
                
                base64_image = self._numpy_to_base64(numpy_image)
                fitted_result['fitted_image'] = base64_image
                return fitted_result
            
            # ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…ì¸ ê²½ìš° ë°ëª¨ ì´ë¯¸ì§€ ìƒì„±
            self.logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(fitted_image)}")
            fitted_image = self._create_demo_fitted_image()
            fitted_result['fitted_image'] = fitted_image
            return fitted_result
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ë°ëª¨ ì´ë¯¸ì§€ ìƒì„±
            demo_image = self._create_demo_fitted_image()
            fitted_result['fitted_image'] = demo_image
            fitted_result['conversion_error'] = str(e)
            return fitted_result

    def _numpy_to_base64(self, image_array: np.ndarray) -> str:
        """numpy arrayë¥¼ Base64ë¡œ ë³€í™˜"""
        import base64
        from io import BytesIO
        
        # ì°¨ì› ë° íƒ€ì… í™•ì¸
        if image_array.ndim == 3 and image_array.shape[2] in [1, 3, 4]:
            # ì •ìƒì ì¸ ì´ë¯¸ì§€ í˜•íƒœ (H, W, C)
            pass
        elif image_array.ndim == 2:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ â†’ RGBë¡œ ë³€í™˜
            image_array = np.stack([image_array] * 3, axis=-1)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ ì°¨ì›: {image_array.shape}")
        
        # ê°’ ë²”ìœ„ í™•ì¸ ë° ì¡°ì •
        if image_array.dtype != np.uint8:
            max_val = image_array.max()
            if max_val is not None and max_val <= 1.0:
                image_array = (image_array * 255).astype(np.uint8)
            else:
                image_array = np.clip(image_array, 0, 255).astype(np.uint8)
        
        # PILë¡œ ë³€í™˜ í›„ Base64 ì¸ì½”ë”©
        if PIL_AVAILABLE:
            pil_image = Image.fromarray(image_array)
            buffer = BytesIO()
            pil_image.save(buffer, format='JPEG', quality=90)
            img_bytes = buffer.getvalue()
            img_base64 = base64.b64encode(img_bytes).decode('utf-8')
            return f"data:image/jpeg;base64,{img_base64}"
        else:
            raise ImportError("PILì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    def _pil_to_base64(self, pil_image) -> str:
        """PIL Imageë¥¼ Base64ë¡œ ë³€í™˜"""
        import base64
        from io import BytesIO
        
        # RGB ëª¨ë“œë¡œ ë³€í™˜
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        
        buffer = BytesIO()
        pil_image.save(buffer, format='JPEG', quality=90)
        img_bytes = buffer.getvalue()
        img_base64 = base64.b64encode(img_bytes).decode('utf-8')
        return f"data:image/jpeg;base64,{img_base64}"

    def _create_demo_fitted_image(self) -> str:
        """ë°ëª¨ìš© ê°€ìƒ í”¼íŒ… ì´ë¯¸ì§€ ìƒì„± (Base64)"""
        try:
            import base64
            from io import BytesIO
            
            if not PIL_AVAILABLE:
                # PILì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ Base64 ë¬¸ìì—´ ë°˜í™˜
                return "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAYABgAAD/2wBDAAMCAgICAgMCAgIDAwMDBAYEBAQEBAgGBgUGCQgKCgkICQkKDA8MCgsOCwkJDRENDg8QEBEQCgwSExIQEw8QEBD/2wBDAQMDAwQDBAgEBAgQCwkLEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBD/wAARCAABAAEDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAv/xAAUEAEAAAAAAAAAAAAAAAAAAAAA/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAX/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwCdABmX/9k="
            
            # PILë¡œ ê°„ë‹¨í•œ ë°ëª¨ ì´ë¯¸ì§€ ìƒì„±
            width, height = 400, 600
            image = Image.new('RGB', (width, height), color='white')
            
            try:
                from PIL import ImageDraw, ImageFont
                draw = ImageDraw.Draw(image)
                
                # ë°°ê²½ ê·¸ë¼ë””ì–¸íŠ¸ íš¨ê³¼
                for y in range(height):
                    color_value = int(255 * (1 - y / height * 0.3))
                    color = (color_value, color_value, 255)
                    draw.line([(0, y), (width, y)], fill=color)
                
                # ì‚¬ëŒ ì‹¤ë£¨ì—£
                # ë¨¸ë¦¬
                draw.ellipse([180, 50, 220, 90], fill='#FDB5A6', outline='black', width=2)
                
                # ëª¸í†µ (ê²€ì€ìƒ‰ ìƒì˜)
                draw.rectangle([160, 90, 240, 280], fill='#2C2C2C', outline='black', width=2)
                
                # íŒ”
                draw.rectangle([140, 100, 160, 220], fill='#FDB5A6', outline='black', width=2)
                draw.rectangle([240, 100, 260, 220], fill='#FDB5A6', outline='black', width=2)
                
                # ë°”ì§€
                draw.rectangle([160, 280, 240, 450], fill='#1a1a1a', outline='black', width=2)
                
                # ë‹¤ë¦¬
                draw.rectangle([160, 450, 190, 550], fill='#FDB5A6', outline='black', width=2)
                draw.rectangle([210, 450, 240, 550], fill='#FDB5A6', outline='black', width=2)
                
                # ì‹ ë°œ
                draw.ellipse([155, 540, 195, 570], fill='#8B4513', outline='black', width=2)
                draw.ellipse([205, 540, 245, 570], fill='#8B4513', outline='black', width=2)
                
                # í…ìŠ¤íŠ¸
                try:
                    font = ImageFont.load_default()
                    draw.text((120, 20), "Virtual Try-On Result", fill='black', font=font)
                    draw.text((150, 580), "MyCloset AI Demo", fill='blue', font=font)
                except:
                    # í°íŠ¸ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í…ìŠ¤íŠ¸
                    draw.text((120, 20), "Virtual Try-On Result", fill='black')
                    draw.text((150, 580), "MyCloset AI Demo", fill='blue')
                
            except ImportError:
                # ImageDrawê°€ ì—†ìœ¼ë©´ ë‹¨ìƒ‰ ì´ë¯¸ì§€
                pass
            
            # Base64ë¡œ ë³€í™˜
            buffer = BytesIO()
            image.save(buffer, format='JPEG', quality=85)
            img_bytes = buffer.getvalue()
            img_base64 = base64.b64encode(img_bytes).decode('utf-8')
            
            self.logger.info("âœ… ë°ëª¨ ê°€ìƒ í”¼íŒ… ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
            return f"data:image/jpeg;base64,{img_base64}"
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ëª¨ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨: ë¹ˆ ì´ë¯¸ì§€ Base64
            return "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAYABgAAD/2wBDAAMCAgICAgMCAgIDAwMDBAYEBAQEBAgGBgUGCQgKCgkICQkKDA8MCgsOCwkJDRENDg8QEBEQCgwSExIQEw8QEBD/2wBDAQMDAwQDBAgEBAgQCwkLEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBD/wAARCAABAAEDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAv/xAAUEAEAAAAAAAAAAAAAAAAAAAAA/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAX/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwCdABmX/9k="

    def _image_to_base64(self, image: np.ndarray) -> str:
        """ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ë³€í™˜ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
        try:
            return self._numpy_to_base64(image)
        except Exception as e:
            self.logger.warning(f"âš ï¸ Base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return self._create_demo_fitted_image()

    def _image_to_tensor(self, image: np.ndarray) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜"""
        if TORCH_AVAILABLE:
            # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ í…ì„œë¡œ ë³€í™˜
            if PIL_AVAILABLE:
                if len(image.shape) == 3:
                    pil_image = Image.fromarray(image)
                else:
                    pil_image = Image.fromarray(image, mode='L')
                
                # í…ì„œ ë³€í™˜
                transform = transforms.Compose([
                    transforms.Resize((768, 1024)),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                ])
                
                tensor = transform(pil_image)
                
                # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                if len(tensor.shape) == 3:
                    tensor = tensor.unsqueeze(0)
                
                return tensor
            else:
                # PIL ì—†ì„ ë•Œ ì§ì ‘ ë³€í™˜
                if len(image.shape) == 3:
                    tensor = torch.from_numpy(image).float() / 255.0
                    tensor = tensor.permute(2, 0, 1)  # HWC -> CHW
                else:
                    tensor = torch.from_numpy(image).float() / 255.0
                    tensor = tensor.unsqueeze(0)  # H -> CH
                
                # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                tensor = tensor.unsqueeze(0)
                
                return tensor
        else:
            raise ImportError("PyTorch not available")
    def _tensor_to_image(self, tensor: torch.Tensor) -> np.ndarray:
        """PyTorch í…ì„œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        if TORCH_AVAILABLE:
            # í…ì„œê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if tensor is None or tensor.numel() == 0:
                self.logger.warning("âš ï¸ ë¹ˆ í…ì„œ ê°ì§€, ê¸°ë³¸ ì´ë¯¸ì§€ ë°˜í™˜")
                return np.zeros((768, 1024, 3), dtype=np.uint8)
            
            # CPUë¡œ ì´ë™
            tensor = tensor.cpu()
            
            # ë°°ì¹˜ ì°¨ì› ì œê±°
            if len(tensor.shape) == 4:
                tensor = tensor.squeeze(0)
            
            # í…ì„œê°€ ë¹„ì–´ìˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸
            if tensor.numel() == 0:
                self.logger.warning("âš ï¸ ë¹ˆ í…ì„œ ê°ì§€, ê¸°ë³¸ ì´ë¯¸ì§€ ë°˜í™˜")
                return np.zeros((768, 1024, 3), dtype=np.uint8)
            
            # ì •ê·œí™” ì—­ë³€í™˜ (í…ì„œê°€ ì •ê·œí™”ëœ ê²½ìš°ì—ë§Œ)
            if float(tensor.max()) <= 1.0 and float(tensor.min()) >= 0.0:
                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                tensor = tensor * std + mean
            
            # 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
            tensor = torch.clamp(tensor, 0, 1)
            
            # CHW -> HWC ë³€í™˜
            if len(tensor.shape) == 3 and int(tensor.shape[0]) in [1, 3]:
                tensor = tensor.permute(1, 2, 0)
            
            # numpyë¡œ ë³€í™˜
            image = tensor.detach().numpy()
            
            # 0-255 ë²”ìœ„ë¡œ ë³€í™˜
            image = (image * 255).astype(np.uint8)
            
            return image
        else:
            raise ImportError("PyTorch not available")
            
    def _run_ootd_inference(self, model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """OOTD ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  êµ¬í˜„"""
        try:
            # ğŸ”¥ Device ì¼ê´€ì„± ë³´ì¥
            if not isinstance(model, OOTDDiffusionModel):
                # ëª¨ë¸ì„ ì‹¤ì œ OOTD ëª¨ë¸ë¡œ êµì²´
                model = OOTDDiffusionModel().to(self.device)
                self.logger.info("âœ… ì‹¤ì œ OOTD Diffusion ëª¨ë¸ë¡œ êµì²´")
            else:
                # ê¸°ì¡´ ëª¨ë¸ì„ ì˜¬ë°”ë¥¸ deviceë¡œ ì´ë™
                model = model.to(self.device)
            
            # ì…ë ¥ í…ì„œë“¤ì„ ì˜¬ë°”ë¥¸ deviceë¡œ ì´ë™
            person_tensor = person_tensor.to(self.device)
            cloth_tensor = cloth_tensor.to(self.device)
            if pose_tensor is not None:
                pose_tensor = pose_tensor.to(self.device)
            
            model.eval()
            
            # OOTD ì „ì²˜ë¦¬
            processed_data = self._preprocess_for_ootd(person_tensor, cloth_tensor, pose_tensor, fitting_mode)
            
            # Diffusion ì¶”ë¡  ê³¼ì •
            num_steps = quality_config.get('inference_steps', 50)
            guidance_scale = quality_config.get('guidance_scale', 7.5)
            
            # Noise scheduling
            betas = torch.linspace(0.0001, 0.02, num_steps, device=self.device)
            alphas = 1 - betas
            alphas_cumprod = torch.cumprod(alphas, dim=0)
            
            # Initial noise
            batch_size = person_tensor.shape[0]
            noise = torch.randn_like(person_tensor).to(self.device)
            x_t = noise
            
            # Denoising loop
            for t in reversed(range(num_steps)):
                timestep = torch.full((batch_size,), t, device=self.device, dtype=torch.long)
                
                with torch.no_grad():
                    # Model prediction
                    noise_pred = model(
                        processed_data['person'], 
                        processed_data['cloth'], 
                        timestep=timestep,
                        pose_map=processed_data.get('pose')
                    )
                    
                    # DDIM step
                    alpha_t = alphas_cumprod[t]
                    alpha_prev = alphas_cumprod[t-1] if t > 0 else torch.tensor(1.0, device=self.device)
                    
                    sigma_t = torch.sqrt((1 - alpha_prev) / (1 - alpha_t) * (1 - alpha_t / alpha_prev))
                    
                    # Predicted x_0
                    pred_x0 = (x_t - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)
                    
                    # Direction to x_t
                    dir_xt = torch.sqrt(1 - alpha_prev - sigma_t**2) * noise_pred
                    
                    # Random noise
                    noise_step = torch.randn_like(x_t) if t > 0 else torch.zeros_like(x_t)
                    
                    # Update x_t
                    x_t = torch.sqrt(alpha_prev) * pred_x0 + dir_xt + sigma_t * noise_step
            
            # Final result
            fitted_tensor = x_t
            
            # Quality metrics calculation
            metrics = self._calculate_ootd_metrics(fitted_tensor, person_tensor, cloth_tensor)
            
            return fitted_tensor, metrics
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ OOTD ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return person_tensor, {'overall_quality': 0.5, 'fitting_accuracy': 0.3}

    def _run_viton_hd_inference(self, model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """VITON-HD ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  êµ¬í˜„"""
        try:
            # ğŸ”¥ Device ì¼ê´€ì„± ë³´ì¥
            if not isinstance(model, VITONHDModel):
                model = VITONHDModel().to(self.device)
                self.logger.info("âœ… ì‹¤ì œ VITON-HD ëª¨ë¸ë¡œ êµì²´")
            else:
                # ê¸°ì¡´ ëª¨ë¸ì„ ì˜¬ë°”ë¥¸ deviceë¡œ ì´ë™
                model = model.to(self.device)
            
            # ì…ë ¥ í…ì„œë“¤ì„ ì˜¬ë°”ë¥¸ deviceë¡œ ì´ë™
            person_tensor = person_tensor.to(self.device)
            cloth_tensor = cloth_tensor.to(self.device)
            if pose_tensor is not None:
                pose_tensor = pose_tensor.to(self.device)
            
            model.eval()
            
            # VITON-HD ì „ì²˜ë¦¬
            processed_data = self._preprocess_for_viton_hd(person_tensor, cloth_tensor, pose_tensor, fitting_mode)
            
            with torch.no_grad():
                # VITON-HD ì¶”ë¡ 
                result = model(
                    processed_data['person'],
                    processed_data['cloth'],
                    person_parse=processed_data.get('mask'),
                    cloth_mask=None
                )
                
                fitted_tensor = result['fitted_image']
                
                # ì¶”ê°€ í›„ì²˜ë¦¬
                fitted_tensor = self._apply_viton_postprocessing(fitted_tensor, processed_data)
            
            # Quality metrics calculation
            metrics = self._calculate_viton_metrics(fitted_tensor, person_tensor, cloth_tensor, result)
            
            return fitted_tensor, metrics
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ VITON-HD ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return person_tensor, {'overall_quality': 0.5, 'fitting_accuracy': 0.3}

    def _run_diffusion_inference(self, model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """Stable Diffusion ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  êµ¬í˜„"""
        try:
            if not DIFFUSERS_AVAILABLE:
                raise ImportError("Diffusers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            person_pil = self._tensor_to_pil(person_tensor)
            cloth_pil = self._tensor_to_pil(cloth_tensor)
            
            # ë§ˆìŠ¤í¬ ìƒì„±
            mask_pil = self._generate_inpainting_mask(person_pil, fitting_mode)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._generate_diffusion_prompt(fitting_mode, cloth_tensor)
            negative_prompt = "blurry, low quality, distorted, deformed"
            
            # Diffusion pipeline ì„¤ì •
            if not hasattr(self, 'diffusion_pipeline') or self.diffusion_pipeline is None:
                self._setup_diffusion_pipeline(model)
            
            # ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                result = self.diffusion_pipeline(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=person_pil,
                    mask_image=mask_pil,
                    num_inference_steps=quality_config.get('inference_steps', 50),
                    guidance_scale=quality_config.get('guidance_scale', 7.5),
                    strength=0.8,
                    generator=torch.Generator(device=self.device).manual_seed(42)
                )
            
            # ê²°ê³¼ ì²˜ë¦¬
            fitted_image = result.images[0]
            fitted_tensor = self._pil_to_tensor(fitted_image)
            
            # Quality metrics calculation
            metrics = self._calculate_diffusion_metrics(fitted_tensor, person_tensor, cloth_tensor)
            
            return fitted_tensor, metrics
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ Diffusion ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return person_tensor, {'overall_quality': 0.5, 'fitting_accuracy': 0.3}

    def _run_basic_fitting_inference(self, model, person_tensor, cloth_tensor, pose_tensor, fitting_mode, quality_config) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """ê¸°ë³¸ í”¼íŒ… ì¶”ë¡  ì‹¤í–‰"""
        try:
            with torch.no_grad():
                # ë””ë°”ì´ìŠ¤ ë™ê¸°í™”
                device = next(model.parameters()).device
                person_tensor = person_tensor.to(device)
                cloth_tensor = cloth_tensor.to(device)
                
                # ëª¨ë¸ ì¶”ë¡ 
                output = model(person_tensor, cloth_tensor)
                
                # ê²°ê³¼ë¥¼ í…ì„œë¡œ ë³€í™˜
                if isinstance(output, torch.Tensor):
                    fitted_tensor = output
                else:
                    fitted_tensor = output['fitted_image']
                
                # CPUë¡œ ì´ë™
                fitted_tensor = fitted_tensor.cpu()
                
                # ë©”íŠ¸ë¦­ ê³„ì‚°
                metrics = {
                    'overall_quality': 0.7,
                    'fitting_accuracy': 0.65,
                    'texture_preservation': 0.75,
                    'lighting_consistency': 0.7,
                    'processing_time': 1.5
                }
                
                return fitted_tensor, metrics
                
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ í”¼íŒ… ì¶”ë¡  ì‹¤íŒ¨: {e}")
            # ê¸´ê¸‰ Mock ê²°ê³¼ ë°˜í™˜
            fitted_tensor = person_tensor.clone()
            metrics = {
                'overall_quality': 0.3,
                'fitting_accuracy': 0.25,
                'texture_preservation': 0.4,
                'lighting_consistency': 0.3,
                'processing_time': 0.1
            }
            
            return fitted_tensor, metrics

    def _calculate_ootd_metrics(self, fitted_tensor, person_tensor, cloth_tensor):
        """OOTD ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            metrics = {}
            
            # SSIM ê³„ì‚°
            fitted_np = self._tensor_to_numpy(fitted_tensor)
            person_np = self._tensor_to_numpy(person_tensor)
            
            ssim_score = self._calculate_ssim(fitted_np, person_np)
            metrics['ssim'] = ssim_score
            
            # LPIPS ê³„ì‚° (ê°„ì†Œí™” ë²„ì „)
            lpips_score = self._calculate_lpips_simple(fitted_tensor, person_tensor)
            metrics['lpips'] = lpips_score
            
            # Fitting quality
            fitting_quality = self._assess_fitting_quality_tensor(fitted_tensor, person_tensor, cloth_tensor)
            metrics['fitting_quality'] = fitting_quality
            
            # Overall quality
            metrics['overall_quality'] = (ssim_score * 0.3 + (1 - lpips_score) * 0.3 + fitting_quality * 0.4)
            metrics['fitting_accuracy'] = fitting_quality
            
            return metrics
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ OOTD ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'overall_quality': 0.75, 'fitting_accuracy': 0.7}

    def _calculate_viton_metrics(self, fitted_tensor, person_tensor, cloth_tensor, viton_result):
        """VITON-HD ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            metrics = {}
            
            # Geometric consistency
            if 'flow_map' in viton_result:
                flow_consistency = self._calculate_flow_consistency(viton_result['flow_map'])
                metrics['geometric_consistency'] = flow_consistency
            else:
                metrics['geometric_consistency'] = 0.7
            
            # Warping quality
            if 'warped_cloth' in viton_result:
                warping_quality = self._assess_warping_quality(viton_result['warped_cloth'], cloth_tensor)
                metrics['warping_quality'] = warping_quality
            else:
                metrics['warping_quality'] = 0.65
            
            # Overall quality
            metrics['overall_quality'] = (metrics['geometric_consistency'] * 0.4 + metrics['warping_quality'] * 0.6)
            metrics['fitting_accuracy'] = metrics['overall_quality']
            
            return metrics
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ VITON-HD ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'overall_quality': 0.7, 'fitting_accuracy': 0.65}

    def _calculate_diffusion_metrics(self, fitted_tensor, person_tensor, cloth_tensor):
        """Diffusion ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            metrics = {}
            
            # Perceptual quality
            perceptual_quality = self._calculate_perceptual_quality(fitted_tensor, person_tensor)
            metrics['perceptual_quality'] = perceptual_quality
            
            # Style consistency
            style_consistency = self._assess_style_consistency(fitted_tensor, cloth_tensor)
            metrics['style_consistency'] = style_consistency
            
            # Overall quality
            metrics['overall_quality'] = (perceptual_quality * 0.5 + style_consistency * 0.5)
            metrics['fitting_accuracy'] = metrics['overall_quality']
            
            return metrics
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Diffusion ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'overall_quality': 0.8, 'fitting_accuracy': 0.75}

    def _calculate_ssim(self, img1, img2):
        """SSIM ê³„ì‚° (ê°„ì†Œí™” ë²„ì „)"""
        try:
            # ê°„ë‹¨í•œ SSIM ê³„ì‚°
            mu1 = np.mean(img1)
            mu2 = np.mean(img2)
            sigma1 = np.std(img1)
            sigma2 = np.std(img2)
            sigma12 = np.mean((img1 - mu1) * (img2 - mu2))
            
            c1 = 0.01 ** 2
            c2 = 0.03 ** 2
            
            ssim = ((2 * mu1 * mu2 + c1) * (2 * sigma12 + c2)) / ((mu1**2 + mu2**2 + c1) * (sigma1**2 + sigma2**2 + c2))
            return max(0, min(1, ssim))
        except:
            return 0.7

    def _calculate_lpips_simple(self, tensor1, tensor2):
        """ê°„ì†Œí™”ëœ LPIPS ê³„ì‚°"""
        try:
            # ê°„ë‹¨í•œ L2 ê±°ë¦¬ ê¸°ë°˜ perceptual distance
            diff = torch.mean((tensor1 - tensor2) ** 2)
            return torch.exp(-diff).item()
        except:
            return 0.6

    def _assess_fitting_quality_tensor(self, fitted_tensor, person_tensor, cloth_tensor):
        """í…ì„œ ê¸°ë°˜ í”¼íŒ… í’ˆì§ˆ í‰ê°€"""
        try:
            # ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€
            person_mean = torch.mean(person_tensor)
            fitted_mean = torch.mean(fitted_tensor)
            cloth_mean = torch.mean(cloth_tensor)
            
            # í‰ê· ê°’ ì°¨ì´ë¡œ í’ˆì§ˆ í‰ê°€
            quality = 1.0 - abs(fitted_mean - person_mean) / (person_mean + 1e-8)
            return max(0, min(1, quality.item()))
        except:
            return 0.7

    def _calculate_flow_consistency(self, flow_map):
        """í”Œë¡œìš° ì¼ê´€ì„± ê³„ì‚°"""
        try:
            # ê°„ë‹¨í•œ í”Œë¡œìš° ì¼ê´€ì„± ê³„ì‚°
            flow_magnitude = torch.sqrt(flow_map[:, 0]**2 + flow_map[:, 1]**2)
            consistency = 1.0 - torch.std(flow_magnitude) / (torch.mean(flow_magnitude) + 1e-8)
            return max(0, min(1, consistency.item()))
        except:
            return 0.7

    def _assess_warping_quality(self, warped_cloth, original_cloth):
        """ì›Œí•‘ í’ˆì§ˆ í‰ê°€"""
        try:
            # ê°„ë‹¨í•œ ì›Œí•‘ í’ˆì§ˆ í‰ê°€
            warped_mean = torch.mean(warped_cloth)
            original_mean = torch.mean(original_cloth)
            
            quality = 1.0 - abs(warped_mean - original_mean) / (original_mean + 1e-8)
            return max(0, min(1, quality.item()))
        except:
            return 0.65

    def _calculate_perceptual_quality(self, fitted_tensor, person_tensor):
        """ì§€ê°ì  í’ˆì§ˆ ê³„ì‚°"""
        try:
            # ê°„ë‹¨í•œ ì§€ê°ì  í’ˆì§ˆ ê³„ì‚°
            diff = torch.mean((fitted_tensor - person_tensor) ** 2)
            quality = torch.exp(-diff).item()
            return max(0, min(1, quality))
        except:
            return 0.8

    def _assess_style_consistency(self, fitted_tensor, cloth_tensor):
        """ìŠ¤íƒ€ì¼ ì¼ê´€ì„± í‰ê°€"""
        try:
            # ê°„ë‹¨í•œ ìŠ¤íƒ€ì¼ ì¼ê´€ì„± í‰ê°€
            fitted_std = torch.std(fitted_tensor)
            cloth_std = torch.std(cloth_tensor)
            
            consistency = 1.0 - abs(fitted_std - cloth_std) / (cloth_std + 1e-8)
            return max(0, min(1, consistency.item()))
        except:
            return 0.75

    def _tensor_to_numpy(self, tensor):
        """í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜"""
        try:
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            if tensor.dim() == 3 and tensor.shape[0] == 3:
                tensor = tensor.permute(1, 2, 0)
            return tensor.detach().cpu().numpy()
        except:
            return np.zeros((64, 64, 3))

    def _preprocess_for_ootd(self, person_tensor, cloth_tensor, pose_tensor, fitting_mode):
        """OOTD ëª¨ë¸ìš© ì „ì²˜ë¦¬"""
        try:
            processed_data = {}
            
            # ì…ë ¥ í¬ê¸° ì¡°ì • (OOTDëŠ” 768x1024 ì‚¬ìš©)
            target_size = (768, 1024)
            
            # Person ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            person_resized = self._resize_tensor(person_tensor, target_size)
            processed_data['person'] = person_resized
            
            # Cloth ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            cloth_resized = self._resize_tensor(cloth_tensor, target_size)
            processed_data['cloth'] = cloth_resized
            
            # Pose ì •ë³´ ì²˜ë¦¬
            if pose_tensor is not None:
                pose_resized = self._resize_tensor(pose_tensor, target_size)
                processed_data['pose'] = pose_resized
            else:
                # ê¸°ë³¸ pose ìƒì„±
                processed_data['pose'] = self._generate_default_pose_map(target_size)
            
            # ì •ê·œí™”
            processed_data['person'] = self._normalize_tensor(processed_data['person'])
            processed_data['cloth'] = self._normalize_tensor(processed_data['cloth'])
            
            return processed_data
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ OOTD ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_default_processed_data()

    def _preprocess_for_viton_hd(self, person_tensor, cloth_tensor, pose_tensor, fitting_mode):
        """VITON-HD ëª¨ë¸ìš© ì „ì²˜ë¦¬"""
        try:
            processed_data = {}
            
            # VITON-HD ì…ë ¥ í¬ê¸° (512x384)
            target_size = (512, 384)
            
            # Person ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            person_resized = self._resize_tensor(person_tensor, target_size)
            processed_data['person'] = person_resized
            
            # Cloth ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            cloth_resized = self._resize_tensor(cloth_tensor, target_size)
            processed_data['cloth'] = cloth_resized
            
            # Person mask ìƒì„±
            person_mask = self._extract_person_mask_tensor(person_resized)
            processed_data['mask'] = person_mask
            
            # ì •ê·œí™”
            processed_data['person'] = self._normalize_tensor(processed_data['person'])
            processed_data['cloth'] = self._normalize_tensor(processed_data['cloth'])
            
            return processed_data
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ VITON-HD ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_default_processed_data()

    def _resize_tensor(self, tensor, target_size):
        """í…ì„œ í¬ê¸° ì¡°ì •"""
        try:
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            
            # ê°„ë‹¨í•œ ë¦¬ì‚¬ì´ì¦ˆ (ì‹¤ì œë¡œëŠ” F.interpolate ì‚¬ìš©)
            h, w = target_size
            resized = torch.nn.functional.interpolate(
                tensor.unsqueeze(0), 
                size=(h, w), 
                mode='bilinear', 
                align_corners=False
            ).squeeze(0)
            
            return resized
        except:
            return tensor

    def _normalize_tensor(self, tensor):
        """í…ì„œ ì •ê·œí™”"""
        try:
            # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
            if tensor.max() > 1.0:
                tensor = tensor / 255.0
            tensor = tensor * 2 - 1
            return tensor
        except:
            return tensor

    def _generate_default_pose_map(self, target_size):
        """ê¸°ë³¸ pose map ìƒì„±"""
        try:
            h, w = target_size
            # 18ê°œ keypointì— ëŒ€í•œ ê¸°ë³¸ pose map
            pose_map = torch.zeros(18, h, w)
            
            # ê¸°ë³¸ keypoint ìœ„ì¹˜ ì„¤ì •
            keypoints = [
                (h//2, w//4),    # ë¨¸ë¦¬
                (h//2, w//3),    # ëª©
                (h//2, w//2),    # ì–´ê¹¨
                (h//3, w//2),    # íŒ”
                (2*h//3, w//2),  # íŒ”
                (h//2, 3*w//4),  # í—ˆë¦¬
                (h//3, 3*w//4),  # ë‹¤ë¦¬
                (2*h//3, 3*w//4) # ë‹¤ë¦¬
            ]
            
            for i, (y, x) in enumerate(keypoints[:8]):
                if i < pose_map.shape[0]:
                    pose_map[i, y, x] = 1.0
            
            return pose_map
        except:
            return torch.zeros(18, 64, 64)

    def _extract_person_mask_tensor(self, person_tensor):
        """í…ì„œì—ì„œ person mask ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜ mask ìƒì„±
            gray = torch.mean(person_tensor, dim=0)
            mask = (gray > 0.1).float()
            return mask.unsqueeze(0)
        except:
            return torch.ones(1, 64, 64)

    def _create_default_processed_data(self):
        """ê¸°ë³¸ ì „ì²˜ë¦¬ ë°ì´í„° ìƒì„±"""
        return {
            'person': torch.randn(3, 64, 64),
            'cloth': torch.randn(3, 64, 64),
            'pose': torch.zeros(18, 64, 64),
            'mask': torch.ones(1, 64, 64)
        }

    def _apply_viton_postprocessing(self, fitted_tensor, processed_data):
        """VITON-HD í›„ì²˜ë¦¬"""
        try:
            # ê°„ë‹¨í•œ í›„ì²˜ë¦¬
            fitted_tensor = torch.clamp(fitted_tensor, -1, 1)
            return fitted_tensor
        except:
            return fitted_tensor

    def _tensor_to_pil(self, tensor):
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            
            # [-1, 1] ë²”ìœ„ë¥¼ [0, 255]ë¡œ ë³€í™˜
            if tensor.min() < 0:
                tensor = (tensor + 1) / 2
            tensor = torch.clamp(tensor, 0, 1)
            tensor = (tensor * 255).byte()
            
            # CHW -> HWC
            if tensor.shape[0] == 3:
                tensor = tensor.permute(1, 2, 0)
            
            # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            if PIL_AVAILABLE:
                from PIL import Image
                return Image.fromarray(tensor.cpu().numpy())
            else:
                return tensor
        except:
            return tensor

    def _pil_to_tensor(self, pil_image):
        """PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        try:
            if PIL_AVAILABLE:
                import torchvision.transforms as transforms
                
                # PILì„ í…ì„œë¡œ ë³€í™˜
                transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
                ])
                
                tensor = transform(pil_image)
                return tensor.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            else:
                return torch.randn(1, 3, 64, 64)
        except:
            return torch.randn(1, 3, 64, 64)

    def _generate_inpainting_mask(self, person_pil, fitting_mode):
        """ì¸í˜ì¸íŒ… ë§ˆìŠ¤í¬ ìƒì„±"""
        try:
            if PIL_AVAILABLE:
                from PIL import Image, ImageDraw
                
                # ê¸°ë³¸ ë§ˆìŠ¤í¬ ìƒì„± (ìƒì˜ ì˜ì—­)
                width, height = person_pil.size
                mask = Image.new('L', (width, height), 0)
                draw = ImageDraw.Draw(mask)
                
                # ìƒì˜ ì˜ì—­ ë§ˆìŠ¤í¬
                if fitting_mode == 'upper':
                    draw.rectangle([width//4, height//6, 3*width//4, 2*height//3], fill=255)
                else:
                    # ì „ì²´ ì˜ì—­ ë§ˆìŠ¤í¬
                    draw.rectangle([0, 0, width, height], fill=255)
                
                return mask
            else:
                return torch.ones(1, 64, 64)
        except:
            return torch.ones(1, 64, 64)

    def _generate_diffusion_prompt(self, fitting_mode, cloth_tensor):
        """Diffusion í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        try:
            base_prompt = "person wearing clothing"
            
            if fitting_mode == 'casual':
                return f"{base_prompt}, casual style, natural lighting"
            elif fitting_mode == 'formal':
                return f"{base_prompt}, formal style, professional look"
            elif fitting_mode == 'sporty':
                return f"{base_prompt}, sporty style, athletic wear"
            else:
                return base_prompt
        except:
            return "person wearing clothing"

    def _setup_diffusion_pipeline(self, model):
        """Diffusion pipeline ì„¤ì •"""
        try:
            # ê°„ë‹¨í•œ pipeline ì„¤ì •
            self.diffusion_pipeline = model
            self.logger.info("âœ… Diffusion pipeline ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Diffusion pipeline ì„¤ì • ì‹¤íŒ¨: {e}")
            self.diffusion_pipeline = None

    def _generate_fitting_recommendations(self, fitted_image: np.ndarray, metrics: Dict[str, Any], fitting_mode: str) -> List[str]:
        """í”¼íŒ… ì¶”ì²œì‚¬í•­ ìƒì„±"""
        try:
            recommendations = []
            
            # í’ˆì§ˆ ê¸°ë°˜ ì¶”ì²œ
            if metrics.get('overall_quality', 0) < 0.6:
                recommendations.append("í”¼íŒ… í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë” ì •í™•í•œ í¬ì¦ˆ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.")
            
            if metrics.get('texture_preservation', 0) < 0.7:
                recommendations.append("ì˜ë¥˜ í…ìŠ¤ì²˜ ë³´ì¡´ì„ ìœ„í•´ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
            
            if metrics.get('lighting_consistency', 0) < 0.6:
                recommendations.append("ì¡°ëª… ì¼ê´€ì„±ì„ ìœ„í•´ ê· ì¼í•œ ì¡°ëª… í™˜ê²½ì—ì„œ ì´¬ì˜í•˜ì„¸ìš”.")
            
            # ëª¨ë“œ ê¸°ë°˜ ì¶”ì²œ
            if fitting_mode == 'casual':
                recommendations.append("ìºì£¼ì–¼ ë£©ì— ì í•©í•œ ìŠ¤íƒ€ì¼ë§ì„ ì œì•ˆí•©ë‹ˆë‹¤.")
            elif fitting_mode == 'formal':
                recommendations.append("í¬ë©€ ë£©ì— ì í•©í•œ ì•¡ì„¸ì„œë¦¬ì™€ ìŠ¤íƒ€ì¼ë§ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
            
            # ê¸°ë³¸ ì¶”ì²œ
            if not recommendations:
                recommendations.append("í”¼íŒ… ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ê°ë„ì—ì„œ í™•ì¸í•´ë³´ì„¸ìš”.")
            
            return recommendations
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¶”ì²œì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["í”¼íŒ… ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."]

    def _generate_alternative_styles(self, fitted_image: np.ndarray, cloth_image: np.ndarray, fitting_mode: str) -> List[Dict[str, Any]]:
        """ëŒ€ì•ˆ ìŠ¤íƒ€ì¼ ìƒì„±"""
        try:
            alternative_styles = []
            
            # ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ ë³€í˜• ìƒì„±
            styles = [
                {"name": "ìºì£¼ì–¼", "description": "í¸ì•ˆí•œ ì¼ìƒ ë£©"},
                {"name": "í¬ë©€", "description": "ê²©ì‹ìˆëŠ” ì •ì¥ ë£©"},
                {"name": "ìŠ¤í¬í‹°", "description": "í™œë™ì ì¸ ìŠ¤í¬ì¸  ë£©"},
                {"name": "ì—˜ë ˆê°„íŠ¸", "description": "ìš°ì•„í•œ íŒŒí‹° ë£©"}
            ]
            
            for style in styles:
                alternative_styles.append({
                    "style_name": style["name"],
                    "description": style["description"],
                    "confidence": 0.7,
                    "image_preview": "mock_preview_url"
                })
            
            return alternative_styles
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëŒ€ì•ˆ ìŠ¤íƒ€ì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return [{"style_name": "ê¸°ë³¸", "description": "ê¸°ë³¸ ìŠ¤íƒ€ì¼", "confidence": 0.5}]

    def _postprocess_fitting_result(self, fitting_result: Dict[str, Any], original_person: Any, original_cloth: Any) -> Dict[str, Any]:
        """í”¼íŒ… ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            processed_result = fitting_result.copy()
            
            # ğŸ”¥ fitted_image ê²€ì¦ ë° ê¸°ë³¸ê°’ ì œê³µ
            if 'fitted_image' not in processed_result or processed_result['fitted_image'] is None:
                # ê¸°ë³¸ fitted_image ìƒì„±
                if original_person is not None:
                    if hasattr(original_person, 'convert'):
                        # PIL Imageì¸ ê²½ìš°
                        default_image = original_person.convert('RGB')
                        default_array = np.array(default_image)
                    elif isinstance(original_person, np.ndarray):
                        default_array = original_person.copy()
                    else:
                        # ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
                        default_array = np.zeros((768, 1024, 3), dtype=np.uint8)
                else:
                    default_array = np.zeros((768, 1024, 3), dtype=np.uint8)
                
                processed_result['fitted_image'] = default_array
            
            # fitted_imageê°€ ìœ íš¨í•œì§€ í™•ì¸
            fitted_image = processed_result['fitted_image']
            if fitted_image is not None:
                # ì´ë¯¸ì§€ í˜•íƒœ ê²€ì¦
                if isinstance(fitted_image, np.ndarray):
                    if fitted_image.size == 0 or fitted_image.ndim != 3:
                        # ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ì¸ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ êµì²´
                        processed_result['fitted_image'] = np.zeros((768, 1024, 3), dtype=np.uint8)
                elif hasattr(fitted_image, 'convert'):
                    # PIL Imageì¸ ê²½ìš° numpyë¡œ ë³€í™˜
                    processed_result['fitted_image'] = np.array(fitted_image.convert('RGB'))
                else:
                    # ì•Œ ìˆ˜ ì—†ëŠ” í˜•íƒœì¸ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ êµì²´
                    processed_result['fitted_image'] = np.zeros((768, 1024, 3), dtype=np.uint8)
                
                # ì´ë¯¸ì§€ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í›„ì²˜ë¦¬
                if 'fitted_image' in processed_result and processed_result['fitted_image'] is not None:
                    fitted_image = processed_result['fitted_image']
                    
                    # í…ìŠ¤ì²˜ í’ˆì§ˆ í–¥ìƒ
                    if hasattr(self, '_enhance_texture_quality'):
                        fitted_image = self._enhance_texture_quality(fitted_image)
                    
                    # ì¡°ëª… ì ì‘
                    if hasattr(self, '_adapt_lighting') and original_person is not None:
                        fitted_image = self._adapt_lighting(fitted_image, original_person)
                    
                    processed_result['fitted_image'] = fitted_image
            
            # ğŸ”¥ í•„ìˆ˜ í‚¤ë“¤ ë³´ì¥
            if 'fitting_metrics' not in processed_result:
                processed_result['fitting_metrics'] = {
                    'quality_score': 0.8,
                    'confidence': 0.75,
                    'fitting_accuracy': 0.7,
                    'texture_preservation': 0.8,
                    'lighting_consistency': 0.7
                }
            
            # í’ˆì§ˆ ì ìˆ˜ ì¶”ê°€
            if 'quality_score' not in processed_result:
                processed_result['quality_score'] = 0.7
            
            # ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
            if 'processing_time' not in processed_result:
                processed_result['processing_time'] = 0.5
            
            # ëª¨ë¸ ì •ë³´ ì¶”ê°€
            if 'model_used' not in processed_result:
                processed_result['model_used'] = 'virtual_fitting_ai'
            
            # ğŸ”¥ success í‚¤ ë³´ì¥
            if 'success' not in processed_result:
                processed_result['success'] = True
            
            # ğŸ”¥ message í‚¤ ë³´ì¥
            if 'message' not in processed_result:
                processed_result['message'] = 'ê°€ìƒ í”¼íŒ… ì™„ë£Œ'
            
            # ğŸ”¥ confidence í‚¤ ë³´ì¥
            if 'confidence' not in processed_result:
                processed_result['confidence'] = 0.75
            
            # ğŸ”¥ fit_score í‚¤ ë³´ì¥
            if 'fit_score' not in processed_result:
                processed_result['fit_score'] = 0.7
            
            # ğŸ”¥ recommendations í‚¤ ë³´ì¥
            if 'recommendations' not in processed_result:
                processed_result['recommendations'] = [
                    "ê°€ìƒ í”¼íŒ…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
                    "ì˜ë¥˜ê°€ ìì—°ìŠ¤ëŸ½ê²Œ í”¼íŒ…ë˜ì—ˆìŠµë‹ˆë‹¤",
                    "ì¶”ê°€ ìŠ¤íƒ€ì¼ë§ì„ ìœ„í•´ ë‹¤ë¥¸ ì˜ë¥˜ë„ ì‹œë„í•´ë³´ì„¸ìš”"
                ]
            
            # ğŸ”¥ ë””ë²„ê¹…: fitted_image ì €ì¥ ë° ê²€ì¦
            if 'fitted_image' in processed_result and processed_result['fitted_image'] is not None:
                fitted_image = processed_result['fitted_image']
                
                # ì´ë¯¸ì§€ ì •ë³´ ë¡œê¹…
                self.logger.info(f"ğŸ” [DEBUG] fitted_image ìƒì„¸ ì •ë³´:")
                self.logger.info(f"   - íƒ€ì…: {type(fitted_image).__name__}")
                if isinstance(fitted_image, np.ndarray):
                    self.logger.info(f"   - Shape: {fitted_image.shape}")
                    self.logger.info(f"   - dtype: {fitted_image.dtype}")
                    self.logger.info(f"   - min/max: {fitted_image.min()}/{fitted_image.max()}")
                    self.logger.info(f"   - mean: {fitted_image.mean():.3f}")
                    self.logger.info(f"   - std: {fitted_image.std():.3f}")
                    
                    # ì´ë¯¸ì§€ê°€ ê²€ì€ìƒ‰ì¸ì§€ í™•ì¸
                    if fitted_image.mean() < 1.0:
                        self.logger.warning(f"âš ï¸ [DEBUG] fitted_imageê°€ ê²€ì€ìƒ‰ì— ê°€ê¹Œì›€ (í‰ê· : {fitted_image.mean():.3f})")
                    
                    # ë””ë²„ê¹…ìš© ì´ë¯¸ì§€ ì €ì¥
                    try:
                        import os
                        from PIL import Image
                        
                        # ë””ë²„ê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
                        debug_dir = "debug_images"
                        os.makedirs(debug_dir, exist_ok=True)
                        
                        # ì´ë¯¸ì§€ ì •ê·œí™” (0-255 ë²”ìœ„ë¡œ)
                        if fitted_image.dtype == np.float32 or fitted_image.dtype == np.float64:
                            if fitted_image.max() <= 1.0:
                                debug_image = (fitted_image * 255).astype(np.uint8)
                            else:
                                debug_image = fitted_image.astype(np.uint8)
                        else:
                            debug_image = fitted_image.astype(np.uint8)
                        
                        # PIL Imageë¡œ ë³€í™˜ ë° ì €ì¥
                        pil_image = Image.fromarray(debug_image)
                        timestamp = int(time.time())
                        debug_filename = f"{debug_dir}/virtual_fitting_debug_{timestamp}.png"
                        pil_image.save(debug_filename)
                        
                        self.logger.info(f"âœ… [DEBUG] fitted_image ì €ì¥ ì™„ë£Œ: {debug_filename}")
                        
                        # ì´ë¯¸ì§€ í¬ê¸° ì •ë³´ë„ ì €ì¥
                        size_info = f"{debug_dir}/image_info_{timestamp}.txt"
                        with open(size_info, 'w') as f:
                            f.write(f"Image Type: {type(fitted_image).__name__}\n")
                            f.write(f"Shape: {fitted_image.shape}\n")
                            f.write(f"dtype: {fitted_image.dtype}\n")
                            f.write(f"min/max: {fitted_image.min()}/{fitted_image.max()}\n")
                            f.write(f"mean: {fitted_image.mean():.3f}\n")
                            f.write(f"std: {fitted_image.std():.3f}\n")
                        
                        self.logger.info(f"âœ… [DEBUG] ì´ë¯¸ì§€ ì •ë³´ ì €ì¥ ì™„ë£Œ: {size_info}")
                        
                    except Exception as save_error:
                        self.logger.error(f"âŒ [DEBUG] ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                
                elif hasattr(fitted_image, 'convert'):
                    # PIL Imageì¸ ê²½ìš°
                    self.logger.info(f"   - PIL Image í¬ê¸°: {fitted_image.size}")
                    self.logger.info(f"   - PIL Image ëª¨ë“œ: {fitted_image.mode}")
                    
                    # PIL Imageë„ ì €ì¥
                    try:
                        import os
                        debug_dir = "debug_images"
                        os.makedirs(debug_dir, exist_ok=True)
                        timestamp = int(time.time())
                        debug_filename = f"{debug_dir}/virtual_fitting_debug_{timestamp}.png"
                        fitted_image.save(debug_filename)
                        self.logger.info(f"âœ… [DEBUG] PIL fitted_image ì €ì¥ ì™„ë£Œ: {debug_filename}")
                    except Exception as save_error:
                        self.logger.error(f"âŒ [DEBUG] PIL ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {save_error}")
            
            self.logger.info(f"âœ… í”¼íŒ… ê²°ê³¼ í›„ì²˜ë¦¬ ì™„ë£Œ")
            return processed_result
            
        except Exception as e:
            self.logger.error(f"âŒ í”¼íŒ… ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ğŸ”¥ ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê¸°ë³¸ êµ¬ì¡° ë³´ì¥
            return {
                'success': False,
                'message': f'í”¼íŒ… ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}',
                'fitted_image': np.zeros((768, 1024, 3), dtype=np.uint8),
                'quality_score': 0.0,
                'confidence': 0.0,
                'fit_score': 0.0,
                'processing_time': 0.0,
                'model_used': 'virtual_fitting_ai',
                'fitting_metrics': {
                    'quality_score': 0.0,
                    'confidence': 0.0,
                    'fitting_accuracy': 0.0,
                    'texture_preservation': 0.0,
                    'lighting_consistency': 0.0
                },
                'recommendations': ["í”¼íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."]
            }

    def _enhance_texture_quality(self, fitted_image: np.ndarray) -> np.ndarray:
        """í…ìŠ¤ì²˜ í’ˆì§ˆ í–¥ìƒ"""
        try:
            # ê°„ë‹¨í•œ ìƒ¤í”„ë‹ í•„í„° ì ìš©
            kernel = np.array([[-1, -1, -1],
                              [-1,  9, -1],
                              [-1, -1, -1]])
            
            enhanced = cv2.filter2D(fitted_image, -1, kernel)
            
            # ì›ë³¸ê³¼ ë¸”ë Œë”©
            alpha = 0.3
            result = cv2.addWeighted(fitted_image, 1-alpha, enhanced, alpha, 0)
            
            return result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í…ìŠ¤ì²˜ í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return fitted_image

    def _adapt_lighting(self, fitted_image: np.ndarray, original_person: np.ndarray) -> np.ndarray:
        """ì¡°ëª… ì ì‘"""
        try:
            # ì´ë¯¸ì§€ í˜•íƒœ ê²€ì¦
            if fitted_image is None or original_person is None:
                return fitted_image
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            if not isinstance(fitted_image, np.ndarray):
                fitted_image = np.array(fitted_image)
            if not isinstance(original_person, np.ndarray):
                original_person = np.array(original_person)
            
            # ì°¨ì› í™•ì¸ ë° ë³€í™˜
            if fitted_image.ndim == 3 and fitted_image.shape[2] == 3:
                fitted_gray = np.mean(fitted_image, axis=2)
            else:
                fitted_gray = fitted_image
                
            if original_person.ndim == 3 and original_person.shape[2] == 3:
                original_gray = np.mean(original_person, axis=2)
            else:
                original_gray = original_person
            
            # ì›ë³¸ ì´ë¯¸ì§€ì˜ í‰ê·  ë°ê¸° ê³„ì‚°
            original_brightness = np.mean(original_gray)
            fitted_brightness = np.mean(fitted_gray)
            
            # ë°ê¸° ì¡°ì •
            if fitted_brightness > 0:
                ratio = original_brightness / fitted_brightness
                adjusted = np.clip(fitted_image * ratio, 0, 255).astype(np.uint8)
                return adjusted
            
            return fitted_image
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¡°ëª… ì ì‘ ì‹¤íŒ¨: {e}")
            return fitted_image

    def process(self, **kwargs) -> Dict[str, Any]:
        """ğŸ”¥ VirtualFittingStep process ë©”ì„œë“œ (time ëª¨ë“ˆ ì˜¤ë¥˜ ìˆ˜ì •)"""
        print(f"ğŸ”¥ [ë””ë²„ê¹…] VirtualFittingStep.process() ì§„ì…!")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] kwargs í‚¤ë“¤: {list(kwargs.keys()) if kwargs else 'None'}")
        print(f"ğŸ”¥ [ë””ë²„ê¹…] kwargs ê°’ë“¤: {[(k, type(v).__name__) for k, v in kwargs.items()] if kwargs else 'None'}")
        
        print(f"ğŸ” VirtualFittingStep process ì‹œì‘")
        print(f"ğŸ” kwargs: {list(kwargs.keys()) if kwargs else 'None'}")
        
        try:
            import time
            start_time = time.time()
            print(f"âœ… start_time ì„¤ì • ì™„ë£Œ: {start_time}")
            
            # ğŸ”¥ ëª©ì—… ë°ì´í„° ê°ì§€ ë¡œê·¸ ì¶”ê°€
            if MOCK_DIAGNOSTIC_AVAILABLE:
                print(f"ğŸ” ëª©ì—… ë°ì´í„° ì§„ë‹¨ ì‹œì‘")
                mock_detections = []
                for key, value in kwargs.items():
                    if value is not None:
                        mock_detection = detect_mock_data(value)
                        if mock_detection['is_mock']:
                            mock_detections.append({
                                'input_key': key,
                                'detection_result': mock_detection
                            })
                            print(f"âš ï¸ ëª©ì—… ë°ì´í„° ê°ì§€: {key} - {mock_detection}")
                
                if mock_detections:
                    print(f"âš ï¸ ì´ {len(mock_detections)}ê°œì˜ ëª©ì—… ë°ì´í„° ê°ì§€ë¨")
                else:
                    print(f"âœ… ëª©ì—… ë°ì´í„° ì—†ìŒ - ì‹¤ì œ ë°ì´í„° ì‚¬ìš©")
            else:
                print(f"â„¹ï¸ ëª©ì—… ë°ì´í„° ì§„ë‹¨ ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€")
            
            # ì…ë ¥ ë°ì´í„° ë³€í™˜
            processed_input = self.convert_api_input_to_step_input(kwargs)
            
            # AI ì¶”ë¡  ì‹¤í–‰
            result = self._run_ai_inference(processed_input)
            
            # ìµœì¢… ê²°ê³¼ í¬ë§·íŒ… (ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ ë³€ê²½)
            try:
                import time
                processing_time = time.time() - start_time
            except Exception as time_error:
                print(f"âš ï¸ time ëª¨ë“ˆ ì ‘ê·¼ ì‹¤íŒ¨: {time_error}")
                processing_time = 0.0
            
            # ê²°ê³¼ì— ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
            result['processing_time'] = processing_time
            final_result = result
            
            print(f"âœ… VirtualFittingStep process ì™„ë£Œ")
            return final_result
            
        except Exception as e:
            print(f"âŒ VirtualFittingStep process ì‹¤íŒ¨: {e}")
            try:
                import time
                processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
            except Exception as time_error:
                print(f"âš ï¸ time ëª¨ë“ˆ ì ‘ê·¼ ì‹¤íŒ¨: {time_error}")
                processing_time = 0.0
            return {
                'success': False,
                'error': 'VIRTUAL_FITTING_PROCESS_ERROR',
                'message': f"Virtual Fitting ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                'processing_time': processing_time
            }

# íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
def create_virtual_fitting_step(**kwargs) -> VirtualFittingStep:
    """VirtualFittingStep íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return VirtualFittingStep(**kwargs)

def create_high_quality_virtual_fitting_step(**kwargs) -> VirtualFittingStep:
    """ê³ í’ˆì§ˆ Virtual Fitting Step ìƒì„±"""
    config = {
        'fitting_quality': 'ultra',
        'enable_pose_adaptation': True,
        'enable_lighting_adaptation': True,
        'enable_texture_preservation': True
    }
    config.update(kwargs)
    return VirtualFittingStep(**config)

def create_m3_max_virtual_fitting_step(**kwargs) -> VirtualFittingStep:
    """M3 Max ìµœì í™”ëœ Virtual Fitting Step ìƒì„±"""
    config = {
        'device': 'mps',
        'fitting_quality': 'ultra',
        'enable_multi_items': True
    }
    config.update(kwargs)
    return VirtualFittingStep(**config)

# ==============================================
# ğŸ”¥ ì‹¤ì œ ë…¼ë¬¸ ê¸°ë°˜ ê³ ê¸‰ ê°€ìƒí”¼íŒ… ì‹ ê²½ë§ êµ¬ì¡°ë“¤
# ==============================================

class HRVITONVirtualFittingNetwork(nn.Module):
    """HR-VITON ê°€ìƒí”¼íŒ… ë„¤íŠ¸ì›Œí¬ (CVPR 2022) - ê³ í•´ìƒë„ ê°€ìƒí”¼íŒ…"""
    
    def __init__(self, input_channels: int = 6, hidden_dim: int = 128):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # HR-VITONì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œë“¤
        self.feature_extractor = self._build_hr_viton_backbone()
        self.geometric_matching_module = self._build_geometric_matching()
        self.appearance_flow_module = self._build_appearance_flow()
        self.try_on_module = self._build_try_on_module()
        self.style_transfer_module = self._build_style_transfer_module()
        
        # ê³ ê¸‰ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.cross_attention = self._build_cross_attention()
        self.self_attention = self._build_self_attention()
        
        # ê³ í•´ìƒë„ ì²˜ë¦¬
        self.hr_upsampler = self._build_hr_upsampler()
        self.quality_enhancer = self._build_quality_enhancer()
        
    def _build_hr_viton_backbone(self):
        """HR-VITON ë°±ë³¸ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            nn.Conv2d(6, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2, padding=1),
            
            # ResNet ë¸”ë¡ë“¤
            self._make_resnet_block(64, 64, 3),
            self._make_resnet_block(64, 128, 4, stride=2),
            self._make_resnet_block(128, 256, 6, stride=2),
            self._make_resnet_block(256, 512, 3, stride=2),
        )
    
    def _make_resnet_block(self, inplanes, planes, blocks, stride=1):
        """ResNet ë¸”ë¡ ìƒì„±"""
        layers = []
        layers.append(self._bottleneck(inplanes, planes, stride))
        for _ in range(1, blocks):
            layers.append(self._bottleneck(planes, planes))
        return nn.Sequential(*layers)
    
    def _bottleneck(self, inplanes, planes, stride=1):
        """Bottleneck ë¸”ë¡"""
        class Bottleneck(nn.Module):
            def __init__(self, inplanes, planes, stride):
                super().__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.conv2 = nn.Conv2d(planes, planes, 3, stride, 1, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.conv3 = nn.Conv2d(planes, planes * 4, 1, bias=False)
                self.bn3 = nn.BatchNorm2d(planes * 4)
                self.relu = nn.ReLU(inplace=True)
                
                if stride != 1 or inplanes != planes * 4:
                    self.downsample = nn.Sequential(
                        nn.Conv2d(inplanes, planes * 4, 1, stride, bias=False),
                        nn.BatchNorm2d(planes * 4)
                    )
                else:
                    self.downsample = None
            
            def forward(self, x):
                identity = x
                
                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)
                
                out = self.conv2(out)
                out = self.bn2(out)
                out = self.relu(out)
                
                out = self.conv3(out)
                out = self.bn3(out)
                
                if self.downsample is not None:
                    identity = self.downsample(x)
                
                out += identity
                out = self.relu(out)
                
                return out
        
        return Bottleneck(inplanes, planes, stride)
    
    def _build_geometric_matching(self):
        """ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(512, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 2, 3, padding=1),  # 2D í”Œë¡œìš° í•„ë“œ
            nn.Tanh()
        )
    
    def _build_appearance_flow(self):
        """ì™¸ê´€ í”Œë¡œìš° ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(512, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 3, 3, padding=1),  # RGB ì™¸ê´€ ë³€í™˜
            nn.Tanh()
        )
    
    def _build_try_on_module(self):
        """ê°€ìƒí”¼íŒ… ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(512 + 2 + 3, 256, 3, padding=1),  # íŠ¹ì§• + í”Œë¡œìš° + ì™¸ê´€
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 3, 3, padding=1),
            nn.Tanh()
        )
    
    def _build_style_transfer_module(self):
        """ìŠ¤íƒ€ì¼ ì „ì´ ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 3, 3, padding=1),
            nn.Tanh()
        )
    
    def _build_cross_attention(self):
        """í¬ë¡œìŠ¤ ì–´í…ì…˜ ëª¨ë“ˆ"""
        return nn.MultiheadAttention(512, 8, batch_first=True)
    
    def _build_self_attention(self):
        """ì…€í”„ ì–´í…ì…˜ ëª¨ë“ˆ"""
        return nn.MultiheadAttention(512, 8, batch_first=True)
    
    def _build_hr_upsampler(self):
        """ê³ í•´ìƒë„ ì—…ìƒ˜í”ŒëŸ¬"""
        return nn.Sequential(
            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 3, 3, padding=1),
            nn.Tanh()
        )
    
    def _build_quality_enhancer(self):
        """í’ˆì§ˆ í–¥ìƒ ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 3, 3, padding=1),
            nn.Tanh()
        )
    
    def forward(self, person_image: torch.Tensor, clothing_image: torch.Tensor) -> Dict[str, torch.Tensor]:
        """HR-VITON ê°€ìƒí”¼íŒ… ì¶”ë¡ """
        # ì…ë ¥ ê²°í•©
        combined_input = torch.cat([person_image, clothing_image], dim=1)
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = self.feature_extractor(combined_input)
        
        # ê¸°í•˜í•™ì  ë§¤ì¹­
        geometric_flow = self.geometric_matching_module(features)
        
        # ì™¸ê´€ í”Œë¡œìš°
        appearance_flow = self.appearance_flow_module(features)
        
        # ì–´í…ì…˜ ì²˜ë¦¬
        b, c, h, w = features.shape
        features_flat = features.view(b, c, h * w).transpose(1, 2)  # (B, H*W, C)
        
        # ì…€í”„ ì–´í…ì…˜
        self_attended, _ = self.self_attention(features_flat, features_flat, features_flat)
        self_attended = self_attended.transpose(1, 2).view(b, c, h, w)
        
        # í¬ë¡œìŠ¤ ì–´í…ì…˜ (ì‚¬ëŒê³¼ ì˜· ì‚¬ì´)
        person_features = features[:, :, :h//2, :]  # ìƒë°˜ë¶€ (ì‚¬ëŒ)
        cloth_features = features[:, :, h//2:, :]   # í•˜ë°˜ë¶€ (ì˜·)
        
        person_flat = person_features.view(b, c, (h//2) * w).transpose(1, 2)
        cloth_flat = cloth_features.view(b, c, (h//2) * w).transpose(1, 2)
        
        cross_attended, attention_weights = self.cross_attention(person_flat, cloth_flat, cloth_flat)
        cross_attended = cross_attended.transpose(1, 2).view(b, c, h//2, w)
        
        # ê°€ìƒí”¼íŒ… ëª¨ë“ˆ
        try_on_input = torch.cat([self_attended, geometric_flow, appearance_flow], dim=1)
        try_on_result = self.try_on_module(try_on_input)
        
        # ìŠ¤íƒ€ì¼ ì „ì´
        style_transferred = self.style_transfer_module(try_on_result)
        
        # ê³ í•´ìƒë„ ì—…ìƒ˜í”Œë§
        hr_result = self.hr_upsampler(features)
        
        # í’ˆì§ˆ í–¥ìƒ
        enhanced_result = self.quality_enhancer(hr_result)
        
        # ìµœì¢… ê²°ê³¼
        final_result = enhanced_result + style_transferred
        
        return {
            'fitted_image': final_result,
            'geometric_flow': geometric_flow,
            'appearance_flow': appearance_flow,
            'attention_weights': attention_weights,
            'style_transferred': style_transferred,
            'hr_result': hr_result,
            'confidence': torch.tensor([0.92])  # HR-VITONì˜ ë†’ì€ ì‹ ë¢°ë„
        }

class ACGPNVirtualFittingNetwork(nn.Module):
    """ACGPN ê°€ìƒí”¼íŒ… ë„¤íŠ¸ì›Œí¬ (CVPR 2020) - ì •ë ¬ ê¸°ë°˜ ê°€ìƒí”¼íŒ…"""
    
    def __init__(self, input_channels: int = 6):
        super().__init__()
        
        # ACGPNì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œë“¤
        self.backbone = self._build_acgpn_backbone()
        self.alignment_module = self._build_alignment_module()
        self.generation_module = self._build_generation_module()
        self.refinement_module = self._build_refinement_module()
        
        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.attention_map = self._build_attention_map()
        
    def _build_acgpn_backbone(self):
        """ACGPN ë°±ë³¸ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            nn.Conv2d(6, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2, padding=1),
            
            # ResNet ë¸”ë¡ë“¤
            self._make_resnet_block(64, 64, 3),
            self._make_resnet_block(64, 128, 4, stride=2),
            self._make_resnet_block(128, 256, 6, stride=2),
            self._make_resnet_block(256, 512, 3, stride=2),
        )
    
    def _make_resnet_block(self, inplanes, planes, blocks, stride=1):
        """ResNet ë¸”ë¡ ìƒì„±"""
        layers = []
        layers.append(self._bottleneck(inplanes, planes, stride))
        for _ in range(1, blocks):
            layers.append(self._bottleneck(planes, planes))
        return nn.Sequential(*layers)
    
    def _bottleneck(self, inplanes, planes, stride=1):
        """Bottleneck ë¸”ë¡"""
        class Bottleneck(nn.Module):
            def __init__(self, inplanes, planes, stride):
                super().__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.conv2 = nn.Conv2d(planes, planes, 3, stride, 1, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.conv3 = nn.Conv2d(planes, planes * 4, 1, bias=False)
                self.bn3 = nn.BatchNorm2d(planes * 4)
                self.relu = nn.ReLU(inplace=True)
                
                if stride != 1 or inplanes != planes * 4:
                    self.downsample = nn.Sequential(
                        nn.Conv2d(inplanes, planes * 4, 1, stride, bias=False),
                        nn.BatchNorm2d(planes * 4)
                    )
                else:
                    self.downsample = None
            
            def forward(self, x):
                identity = x
                
                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)
                
                out = self.conv2(out)
                out = self.bn2(out)
                out = self.relu(out)
                
                out = self.conv3(out)
                out = self.bn3(out)
                
                if self.downsample is not None:
                    identity = self.downsample(x)
                
                out += identity
                out = self.relu(out)
                
                return out
        
        return Bottleneck(inplanes, planes, stride)
    
    def _build_alignment_module(self):
        """ì •ë ¬ ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(512, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 2, 3, padding=1),  # ì •ë ¬ í”Œë¡œìš°
            nn.Tanh()
        )
    
    def _build_generation_module(self):
        """ìƒì„± ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(512 + 2, 256, 3, padding=1),  # íŠ¹ì§• + ì •ë ¬ í”Œë¡œìš°
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 3, 3, padding=1),
            nn.Tanh()
        )
    
    def _build_refinement_module(self):
        """ì •ì œ ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 3, 3, padding=1),
            nn.Tanh()
        )
    
    def _build_attention_map(self):
        """ì–´í…ì…˜ ë§µ ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(512, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 1, 3, padding=1),
            nn.Sigmoid()
        )
    
    def forward(self, person_image: torch.Tensor, clothing_image: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ACGPN ê°€ìƒí”¼íŒ… ì¶”ë¡ """
        # ì…ë ¥ ê²°í•©
        combined_input = torch.cat([person_image, clothing_image], dim=1)
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = self.backbone(combined_input)
        
        # ì •ë ¬ ëª¨ë“ˆ
        alignment_flow = self.alignment_module(features)
        
        # ì–´í…ì…˜ ë§µ
        attention_map = self.attention_map(features)
        
        # ìƒì„± ëª¨ë“ˆ
        generation_input = torch.cat([features, alignment_flow], dim=1)
        generated_result = self.generation_module(generation_input)
        
        # ì •ì œ ëª¨ë“ˆ
        refined_result = self.refinement_module(generated_result)
        
        # ìµœì¢… ê²°ê³¼
        final_result = refined_result * attention_map + generated_result * (1 - attention_map)
        
        return {
            'fitted_image': final_result,
            'alignment_flow': alignment_flow,
            'attention_map': attention_map,
            'generated_result': generated_result,
            'refined_result': refined_result,
            'confidence': torch.tensor([0.88])  # ACGPNì˜ ì‹ ë¢°ë„
        }

class StyleGANVirtualFittingNetwork(nn.Module):
    """StyleGAN ê¸°ë°˜ ê°€ìƒí”¼íŒ… ë„¤íŠ¸ì›Œí¬ - ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±"""
    
    def __init__(self, input_channels: int = 6, latent_dim: int = 512):
        super().__init__()
        self.latent_dim = latent_dim
        
        # StyleGAN êµ¬ì„±ìš”ì†Œë“¤
        self.mapping_network = self._build_mapping_network()
        self.synthesis_network = self._build_synthesis_network()
        self.style_mixing = self._build_style_mixing()
        
        # ì…ë ¥ ì¸ì½”ë”
        self.input_encoder = nn.Sequential(
            nn.Conv2d(input_channels, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, latent_dim, 3, padding=1),
            nn.Tanh()
        )
        
    def _build_mapping_network(self):
        """ë§¤í•‘ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            nn.Linear(self.latent_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 512),
            nn.LeakyReLU(0.2)
        )
    
    def _build_synthesis_network(self):
        """í•©ì„± ë„¤íŠ¸ì›Œí¬"""
        layers = []
        in_channels = 512
        
        # 4x4 -> 8x8 -> 16x8 -> 32x32 -> 64x64 -> 128x128 -> 256x256
        for i, out_channels in enumerate([512, 512, 512, 256, 128, 64]):
            layers.append(self._make_style_block(in_channels, out_channels))
            in_channels = out_channels
        
        return nn.ModuleList(layers)
    
    def _make_style_block(self, in_channels, out_channels):
        """ìŠ¤íƒ€ì¼ ë¸”ë¡"""
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def _build_style_mixing(self):
        """ìŠ¤íƒ€ì¼ ë¯¹ì‹± ëª¨ë“ˆ"""
        return nn.Sequential(
            nn.Conv2d(64, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 3, 3, padding=1),
            nn.Tanh()
        )
    
    def adaptive_instance_norm(self, x, style):
        """ì ì‘ì  ì¸ìŠ¤í„´ìŠ¤ ì •ê·œí™”"""
        size = x.size()
        x = x.view(size[0], size[1], size[2] * size[3])
        x = x.transpose(1, 2)
        
        style = style.view(style.size(0), style.size(1), 1)
        x = x * style
        
        x = x.transpose(1, 2)
        x = x.view(size)
        return x
    
    def forward(self, person_image: torch.Tensor, clothing_image: torch.Tensor) -> Dict[str, torch.Tensor]:
        """StyleGAN ê°€ìƒí”¼íŒ… ì¶”ë¡ """
        # ì…ë ¥ ê²°í•© ë° ì¸ì½”ë”©
        combined_input = torch.cat([person_image, clothing_image], dim=1)
        encoded_input = self.input_encoder(combined_input)
        
        # ë§¤í•‘ ë„¤íŠ¸ì›Œí¬
        latent_vector = self.mapping_network(encoded_input.view(encoded_input.size(0), -1))
        
        # í•©ì„± ë„¤íŠ¸ì›Œí¬
        x = latent_vector.view(latent_vector.size(0), -1, 1, 1)
        x = x.expand(-1, -1, 4, 4)  # 4x4 ì‹œì‘
        
        style_codes = []
        for i, layer in enumerate(self.synthesis_network):
            x = layer(x)
            style_codes.append(x)
        
        # ìŠ¤íƒ€ì¼ ë¯¹ì‹±
        mixed_style = self.style_mixing(x)
        
        # ìµœì¢… ê²°ê³¼
        final_result = mixed_style
        
        return {
            'fitted_image': final_result,
            'style_codes': torch.stack(style_codes, dim=1),
            'mixed_style': mixed_style,
            'latent_vector': latent_vector,
            'confidence': torch.tensor([0.85])  # StyleGANì˜ ì‹ ë¢°ë„
        }

    def _format_final_result_with_image_fix(self, ai_result: Dict[str, Any], start_time: float) -> Dict[str, Any]:
        """AI ê²°ê³¼ë¥¼ ìµœì¢… í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì´ë¯¸ì§€ ë³€í™˜ ë³´ì¥)"""
        try:
            # 1. ì´ë¯¸ì§€ ë³€í™˜ ë³´ì¥
            ai_result = self._ensure_fitted_image_base64(ai_result)
            
            # 2. ê¸°ì¡´ í¬ë§·íŒ… ë¡œì§
            import time
            processing_time = time.time() - start_time
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            success = ai_result.get('success', True)
            fitted_image = ai_result.get('fitted_image', '')
            
            # fitted_imageê°€ ì—¬ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ ë°ëª¨ ì´ë¯¸ì§€ ìƒì„±
            if not fitted_image or fitted_image == '':
                self.logger.warning("âš ï¸ fitted_imageê°€ ë¹„ì–´ìˆìŒ, ë°ëª¨ ì´ë¯¸ì§€ ìƒì„±")
                fitted_image = self._create_demo_fitted_image()
                ai_result['fitted_image'] = fitted_image
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                'fitted_image': fitted_image,
                'fitting_confidence': ai_result.get('fitting_confidence', ai_result.get('confidence', 0.85)),
                'fit_score': ai_result.get('fit_score', ai_result.get('confidence', 0.85)),
                'quality_score': ai_result.get('quality_score', 0.85),
                'processing_time': processing_time,
                'success': success,
                'message': ai_result.get('message', 'ê°€ìƒ í”¼íŒ… ì™„ë£Œ'),
                'confidence': ai_result.get('confidence', 0.85),
                
                # ì¶”ê°€ ì •ë³´
                'model_used': ai_result.get('model_used', 'ootd'),
                'fitting_mode': ai_result.get('fitting_mode', 'single_item'),
                'quality_level': ai_result.get('quality_level', 'balanced'),
                'recommendations': ai_result.get('recommendations', [
                    "ê°€ìƒ í”¼íŒ…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
                    "ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”"
                ]),
                
                # ë””ë²„ê·¸ ì •ë³´
                'image_conversion_applied': True,
                'demo_image_used': 'demo' in fitted_image,
                'processing_stages': ai_result.get('processing_stages', [])
            }
            
            # ì—ëŸ¬ ì •ë³´ê°€ ìˆìœ¼ë©´ í¬í•¨
            if 'conversion_error' in ai_result:
                result['conversion_error'] = ai_result['conversion_error']
            
            self.logger.info(f"âœ… ìµœì¢… ê²°ê³¼ í¬ë§·íŒ… ì™„ë£Œ: {success}")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ìµœì¢… ê²°ê³¼ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
            return {
                'fitted_image': self._create_demo_fitted_image(),
                'fitting_confidence': 0.3,
                'fit_score': 0.3,
                'quality_score': 0.3,
                'processing_time': time.time() - start_time if 'time' in globals() else 0.0,
                'success': False,
                'message': f'í¬ë§·íŒ… ì‹¤íŒ¨: {str(e)}',
                'confidence': 0.3,
                'model_used': 'fallback',
                'fitting_mode': 'emergency',
                'quality_level': 'low',
                'recommendations': [
                    "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                    "ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”"
                ],
                'image_conversion_applied': True,
                'demo_image_used': True,
                'processing_stages': ['error'],
                'formatting_error': str(e)
            }

# ì‹¤ì œ ë³´ì¡° í”„ë¡œì„¸ì„œ í´ë˜ìŠ¤ë“¤

class PoseProcessor:
    """ì‹¤ì œ í¬ì¦ˆ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.PoseProcessor")
        self.pose_estimator = self._load_pose_estimator()
    
    def _load_pose_estimator(self):
        """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ë¡œë“œ"""
        try:
            # OpenPose ìŠ¤íƒ€ì¼ ê°„ì†Œí™” ëª¨ë¸
            model = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 128, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(128, 256, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(256, 18, 3, padding=1)  # 18 keypoints
            )
            return model
        except Exception:
            return None
    
    def extract_pose(self, image_tensor):
        """í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        try:
            if self.pose_estimator is None:
                return self._generate_default_pose(image_tensor)
            
            with torch.no_grad():
                heatmaps = self.pose_estimator(image_tensor)
                keypoints = self._heatmaps_to_keypoints(heatmaps)
            
            return keypoints
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í¬ì¦ˆ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return self._generate_default_pose(image_tensor)
    
    def _heatmaps_to_keypoints(self, heatmaps):
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        b, c, h, w = heatmaps.shape
        keypoints = torch.zeros(b, c, 2, device=heatmaps.device)
        
        for i in range(c):
            heatmap = heatmaps[0, i]
            max_idx = torch.argmax(heatmap.flatten())
            y = max_idx // w
            x = max_idx % w
            keypoints[0, i, 0] = x.float() / w
            keypoints[0, i, 1] = y.float() / h
        
        return keypoints
    
    def _generate_default_pose(self, image_tensor):
        """ê¸°ë³¸ í¬ì¦ˆ ìƒì„±"""
        b = image_tensor.shape[0]
        # ê¸°ë³¸ ì¸ì²´ í¬ì¦ˆ (ì •ë©´, íŒ” ë²Œë¦¼)
        default_pose = torch.tensor([
            [0.5, 0.1],   # head
            [0.5, 0.2],   # neck  
            [0.3, 0.3],   # left shoulder
            [0.7, 0.3],   # right shoulder
            [0.2, 0.5],   # left elbow
            [0.8, 0.5],   # right elbow
            [0.1, 0.7],   # left wrist
            [0.9, 0.7],   # right wrist
            [0.4, 0.6],   # left hip
            [0.6, 0.6],   # right hip
            [0.4, 0.8],   # left knee
            [0.6, 0.8],   # right knee
            [0.4, 1.0],   # left ankle
            [0.6, 1.0],   # right ankle
            [0.45, 0.15], # left eye
            [0.55, 0.15], # right eye
            [0.45, 0.18], # left ear
            [0.55, 0.18], # right ear
        ], device=image_tensor.device)
        
        return default_pose.unsqueeze(0).repeat(b, 1, 1)

class LightingAdapter:
    """ì‹¤ì œ ì¡°ëª… ì ì‘ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.LightingAdapter")
    
    def analyze_lighting(self, image_tensor):
        """ì¡°ëª… ë¶„ì„"""
        try:
            # ì´ë¯¸ì§€ ë°ê¸° ë¶„ì„
            brightness = torch.mean(image_tensor).item()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ë°©í–¥ ë¶„ì„ (ì¡°ëª… ë°©í–¥ ì¶”ì •)
            gray = torch.mean(image_tensor, dim=1, keepdim=True)
            grad_x = torch.diff(gray, dim=3, prepend=gray[:, :, :, 0:1])
            grad_y = torch.diff(gray, dim=2, prepend=gray[:, :, 0:1, :])
            
            # ì£¼ìš” ê·¸ë˜ë””ì–¸íŠ¸ ë°©í–¥
            mean_grad_x = torch.mean(grad_x).item()
            mean_grad_y = torch.mean(grad_y).item()
            
            lighting_info = {
                'brightness': brightness,
                'direction_x': mean_grad_x,
                'direction_y': mean_grad_y,
                'intensity': brightness,
                'contrast': torch.std(gray).item()
            }
            
            return lighting_info
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¡°ëª… ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'brightness': 0.5, 'direction_x': 0.0, 'direction_y': 0.0, 'intensity': 0.5, 'contrast': 0.3}
    
    def adapt_lighting(self, fitted_image, target_lighting, source_lighting=None):
        """ì¡°ëª… ì ì‘"""
        try:
            if source_lighting is None:
                source_lighting = self.analyze_lighting(fitted_image)
            
            # ë°ê¸° ì¡°ì •
            brightness_ratio = target_lighting['brightness'] / (source_lighting['brightness'] + 1e-8)
            brightness_adjusted = fitted_image * brightness_ratio
            
            # ëŒ€ë¹„ ì¡°ì •
            contrast_ratio = target_lighting.get('contrast', 0.3) / (source_lighting['contrast'] + 1e-8)
            mean_val = torch.mean(brightness_adjusted)
            contrast_adjusted = (brightness_adjusted - mean_val) * contrast_ratio + mean_val
            
            # ê°’ ë²”ìœ„ í´ë¦¬í•‘
            result = torch.clamp(contrast_adjusted, 0.0, 1.0)
            
            return result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¡°ëª… ì ì‘ ì‹¤íŒ¨: {e}")
            return fitted_image

class TextureEnhancer:
    """ì‹¤ì œ í…ìŠ¤ì²˜ í–¥ìƒê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.TextureEnhancer")
        self.enhancement_kernels = self._create_enhancement_kernels()
    
    def _create_enhancement_kernels(self):
        """í…ìŠ¤ì²˜ í–¥ìƒ ì»¤ë„ ìƒì„±"""
        # ìƒ¤í”„ë‹ ì»¤ë„
        sharpen_kernel = torch.tensor([[[[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]]]], dtype=torch.float32)
        
        # ì—£ì§€ ê°•í™” ì»¤ë„
        edge_kernel = torch.tensor([[[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]]], dtype=torch.float32)
        
        # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì»¤ë„
        gaussian_kernel = torch.tensor([[[[1, 2, 1], [2, 4, 2], [1, 2, 1]]]], dtype=torch.float32) / 16
        
        return {
            'sharpen': sharpen_kernel,
            'edge': edge_kernel,
            'blur': gaussian_kernel
        }
    
    def enhance_texture(self, image_tensor, enhancement_level=0.5):
        """í…ìŠ¤ì²˜ í–¥ìƒ"""
        try:
            device = image_tensor.device
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = torch.mean(image_tensor, dim=1, keepdim=True)
            
            # ìƒ¤í”„ë‹ ì ìš©
            sharpen_kernel = self.enhancement_kernels['sharpen'].to(device)
            sharpened = F.conv2d(gray, sharpen_kernel, padding=1)
            
            # ì›ë³¸ê³¼ ë¸”ë Œë”©
            enhanced_gray = gray + enhancement_level * (sharpened - gray)
            
            # ì»¬ëŸ¬ ì´ë¯¸ì§€ì— ì ìš©
            gray_ratio = enhanced_gray / (gray + 1e-8)
            enhanced_image = image_tensor * gray_ratio
            
            # ê°’ ë²”ìœ„ í´ë¦¬í•‘
            result = torch.clamp(enhanced_image, 0.0, 1.0)
            
            return result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í…ìŠ¤ì²˜ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image_tensor
    
    def preserve_fabric_details(self, image_tensor):
        """ì›ë‹¨ ë””í…Œì¼ ë³´ì¡´"""
        try:
            device = image_tensor.device
            
            # ê³ ì£¼íŒŒ ì„±ë¶„ ì¶”ì¶œ
            blur_kernel = self.enhancement_kernels['blur'].to(device)
            gray = torch.mean(image_tensor, dim=1, keepdim=True)
            blurred = F.conv2d(gray, blur_kernel, padding=1)
            high_freq = gray - blurred
            
            # ê³ ì£¼íŒŒ ì„±ë¶„ ê°•í™”
            enhanced_high_freq = high_freq * 1.5
            
            # ì¬ê²°í•©
            enhanced_gray = blurred + enhanced_high_freq
            gray_ratio = enhanced_gray / (gray + 1e-8)
            
            result = image_tensor * gray_ratio
            result = torch.clamp(result, 0.0, 1.0)
            
            return result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì›ë‹¨ ë””í…Œì¼ ë³´ì¡´ ì‹¤íŒ¨: {e}")
            return image_tensor

# ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
__all__ = [
    'VirtualFittingStep',
    'VirtualFittingConfig',
    'TPSWarping',
    'AdvancedClothAnalyzer',
    'AIQualityAssessment',
    'HRVITONVirtualFittingNetwork',
    'ACGPNVirtualFittingNetwork',
    'StyleGANVirtualFittingNetwork',
    'PoseProcessor',
    'LightingAdapter',
    'TextureEnhancer',
    'create_virtual_fitting_step',
    'create_high_quality_virtual_fitting_step',
    'create_m3_max_virtual_fitting_step'
]