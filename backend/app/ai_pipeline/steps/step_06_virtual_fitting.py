# app/ai_pipeline/steps/step_06_virtual_fitting.py
"""
6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… (Virtual Fitting) - ì™„ì „í•œ êµ¬í˜„ + ì‹œê°í™” ê¸°ëŠ¥
âœ… Pipeline Manager ì™„ì „ í˜¸í™˜
âœ… ModelLoader ì™„ì „ ì—°ë™
âœ… M3 Max 128GB ìµœì í™”
âœ… ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš© (OOTDiffusion, VITON-HD)
âœ… ë¬¼ë¦¬ ê¸°ë°˜ ì²œ ì‹œë®¬ë ˆì´ì…˜
âœ… ğŸ†• ë‹¨ê³„ë³„ ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„± ê¸°ëŠ¥
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì½”ë“œ
"""

import os
import sys
import logging
import time
import asyncio
import json
import math
import threading
import uuid
import base64
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from io import BytesIO
import gc
import weakref

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
from PIL import Image, ImageEnhance, ImageFilter, ImageDraw, ImageFont

# PyTorch ê´€ë ¨
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torchvision.transforms as transforms
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# OpenCV
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

# ê³¼í•™ ì—°ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from scipy.interpolate import RBFInterpolator, griddata
    from scipy.spatial.distance import cdist
    from scipy.ndimage import gaussian_filter, binary_erosion, binary_dilation
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

try:
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    from skimage.feature import local_binary_pattern, canny
    from skimage.segmentation import slic, watershed
    from skimage.transform import resize, rotate
    from skimage.measure import regionprops, label
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

# AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from diffusers import StableDiffusionPipeline, DDIMScheduler, UNet2DConditionModel
    from transformers import CLIPProcessor, CLIPModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False

# ModelLoader ì—°ë™
try:
    from ..utils.model_loader import (
        ModelLoader,
        ModelConfig,
        ModelType,
        BaseStepMixin,
        get_global_model_loader,
        preprocess_image,
        postprocess_segmentation
    )
    MODEL_LOADER_AVAILABLE = True
except ImportError as e:
    logging.error(f"ModelLoader ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    MODEL_LOADER_AVAILABLE = False
    BaseStepMixin = object  # í´ë°±

# ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìœ í‹¸ë¦¬í‹°
try:
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
except ImportError:
    MemoryManager = None
    DataConverter = None

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# =================================================================
# 1. ìƒìˆ˜ ë° ì„¤ì •
# =================================================================

class FittingQuality(Enum):
    """í”¼íŒ… í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"
    ULTRA = "ultra"

class FittingMethod(Enum):
    """í”¼íŒ… ë°©ë²•"""
    PHYSICS_BASED = "physics_based"
    AI_NEURAL = "ai_neural"
    HYBRID = "hybrid"
    TEMPLATE_MATCHING = "template_matching"

@dataclass
class FabricProperties:
    """ì²œ ì¬ì§ˆ ì†ì„±"""
    stiffness: float = 0.5
    elasticity: float = 0.3
    density: float = 1.4
    friction: float = 0.5
    shine: float = 0.5
    transparency: float = 0.0
    texture_scale: float = 1.0

@dataclass
class FittingParams:
    """í”¼íŒ… íŒŒë¼ë¯¸í„°"""
    fit_type: str = "fitted"
    body_contact: float = 0.7
    drape_level: float = 0.3
    stretch_zones: List[str] = field(default_factory=lambda: ["chest", "waist"])
    wrinkle_intensity: float = 0.5
    shadow_strength: float = 0.6

# ì²œ ì¬ì§ˆë³„ ë¬¼ë¦¬ ì†ì„±
FABRIC_PROPERTIES = {
    'cotton': FabricProperties(0.3, 0.2, 1.5, 0.7, 0.2, 0.0, 1.0),
    'denim': FabricProperties(0.8, 0.1, 2.0, 0.9, 0.1, 0.0, 1.2),
    'silk': FabricProperties(0.1, 0.4, 1.3, 0.3, 0.8, 0.1, 0.8),
    'wool': FabricProperties(0.5, 0.3, 1.4, 0.6, 0.3, 0.0, 1.1),
    'polyester': FabricProperties(0.4, 0.5, 1.2, 0.4, 0.6, 0.0, 0.9),
    'leather': FabricProperties(0.9, 0.1, 2.5, 0.8, 0.9, 0.0, 1.5),
    'spandex': FabricProperties(0.1, 0.8, 1.1, 0.5, 0.4, 0.0, 0.7),
    'linen': FabricProperties(0.6, 0.2, 1.6, 0.8, 0.1, 0.0, 1.3),
    'default': FabricProperties(0.4, 0.3, 1.4, 0.5, 0.5, 0.0, 1.0)
}

# ì˜ë¥˜ íƒ€ì…ë³„ í”¼íŒ… íŒŒë¼ë¯¸í„°
CLOTHING_FITTING_PARAMS = {
    'shirt': FittingParams("fitted", 0.7, 0.3, ["chest", "waist"], 0.3, 0.6),
    'dress': FittingParams("flowing", 0.5, 0.7, ["bust", "waist", "hip"], 0.6, 0.7),
    'pants': FittingParams("fitted", 0.8, 0.2, ["thigh", "calf"], 0.4, 0.5),
    'jacket': FittingParams("structured", 0.6, 0.4, ["shoulder", "chest"], 0.2, 0.8),
    'skirt': FittingParams("flowing", 0.6, 0.6, ["waist", "hip"], 0.5, 0.6),
    'blouse': FittingParams("loose", 0.5, 0.5, ["chest", "waist"], 0.4, 0.6),
    'sweater': FittingParams("relaxed", 0.6, 0.4, ["chest", "arms"], 0.3, 0.5),
    'default': FittingParams("fitted", 0.6, 0.4, ["chest", "waist"], 0.4, 0.6)
}

# ğŸ†• ì‹œê°í™”ìš© ìƒ‰ìƒ íŒ”ë ˆíŠ¸
VISUALIZATION_COLORS = {
    'original': (200, 200, 200),      # ì›ë³¸ ì´ë¯¸ì§€ ì˜ì—­
    'cloth': (100, 149, 237),         # ì˜ë¥˜ ì˜ì—­ - ì½˜í”Œë¼ì›Œ ë¸”ë£¨
    'fitted': (255, 105, 180),        # í”¼íŒ…ëœ ì˜ë¥˜ - í•«í•‘í¬
    'skin': (255, 218, 185),          # í”¼ë¶€ ì˜ì—­ - ì‚´ìƒ‰
    'hair': (139, 69, 19),            # ë¨¸ë¦¬ - ê°ˆìƒ‰
    'background': (240, 248, 255),    # ë°°ê²½ - ì—°í•œ íŒŒë‘
    'shadow': (105, 105, 105),        # ê·¸ë¦¼ì - ì–´ë‘ìš´ íšŒìƒ‰
    'highlight': (255, 255, 224),     # í•˜ì´ë¼ì´íŠ¸ - ì—°í•œ ë…¸ë‘
    'seam': (255, 69, 0),             # ì†”ê¸° - ë¹¨ê°•-ì£¼í™©
    'fold': (123, 104, 238),          # ì£¼ë¦„ - ë¯¸ë””ì—„ ìŠ¬ë ˆì´íŠ¸ ë¸”ë£¨
    'overlay': (255, 255, 255, 128)   # ì˜¤ë²„ë ˆì´ - ë°˜íˆ¬ëª… í°ìƒ‰
}

# =================================================================
# 2. ë©”ì¸ í´ë˜ìŠ¤
# =================================================================

class VirtualFittingStep(BaseStepMixin):
    """
    6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… - ì™„ì „í•œ êµ¬í˜„ + ì‹œê°í™” ê¸°ëŠ¥
    
    âœ… ì‹¤ì œ AI ëª¨ë¸ ì™„ë²½ ì—°ë™
    âœ… ModelLoader ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
    âœ… ë¬¼ë¦¬ ê¸°ë°˜ ì²œ ì‹œë®¬ë ˆì´ì…˜
    âœ… M3 Max Neural Engine ê°€ì†
    âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
    âœ… ğŸ†• ê°€ìƒ í”¼íŒ… ê²°ê³¼ ì‹œê°í™”
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """
        âœ… í†µì¼ëœ ìƒì„±ì íŒ¨í„´ - Pipeline Manager ì™„ì „ í˜¸í™˜
        
        Args:
            device: ë””ë°”ì´ìŠ¤ ('cpu', 'cuda', 'mps', None=ìë™ê°ì§€)
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            **kwargs: í™•ì¥ íŒŒë¼ë¯¸í„°
                - device_type: str = "auto"
                - memory_gb: float = 16.0
                - is_m3_max: bool = False
                - optimization_enabled: bool = True
                - quality_level: str = "balanced"
                - fitting_method: str = "physics_based"
                - enable_physics: bool = True
                - enable_ai_models: bool = True
                - enable_visualization: bool = True
        """
        
        # === 1. í†µì¼ëœ ì´ˆê¸°í™” íŒ¨í„´ ===
        self.device = self._auto_detect_device(device)
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.step_number = 6
        self.logger = logging.getLogger(f"pipeline.{self.step_name}")
        
        # === 2. ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ===
        self.device_type = kwargs.get('device_type', 'auto')
        self.memory_gb = kwargs.get('memory_gb', 16.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')
        
        # === 3. 6ë‹¨ê³„ íŠ¹í™” íŒŒë¼ë¯¸í„° ===
        self.fitting_method = kwargs.get('fitting_method', 'physics_based')
        self.enable_physics = kwargs.get('enable_physics', True)
        self.enable_ai_models = kwargs.get('enable_ai_models', True)
        self.enable_visualization = kwargs.get('enable_visualization', True)
        
        # === 4. Step íŠ¹í™” ì„¤ì • ë³‘í•© ===
        self._merge_step_specific_config(kwargs)
        
        # === 5. ìƒíƒœ ì´ˆê¸°í™” ===
        self.is_initialized = False
        self.session_id = str(uuid.uuid4())
        
        # === 6. ModelLoader ì—°ë™ ===
        if MODEL_LOADER_AVAILABLE:
            self._setup_model_interface()
        else:
            self.logger.error("âŒ ModelLoaderê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
            self.model_interface = None
        
        # === 7. 6ë‹¨ê³„ ì „ìš© ì´ˆê¸°í™” ===
        self._initialize_step_specific()
        
        # === 8. ë©”ëª¨ë¦¬ ë° ìºì‹œ ê´€ë¦¬ ===
        self.result_cache: Dict[str, Any] = {}
        self.cache_lock = threading.RLock()
        self.executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="virtual_fitting")
        
        # === 9. ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ===
        self.memory_manager = self._create_memory_manager()
        self.data_converter = self._create_data_converter()
        
        # ì´ˆê¸°í™” ì™„ë£Œ ë¡œê·¸
        self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}, "
                        f"í’ˆì§ˆ: {self.quality_level}, ë°©ë²•: {self.fitting_method}")
    
    def _auto_detect_device(self, device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if device is not None:
            return device
            
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
        return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            if sys.platform == "darwin":
                import platform
                return "arm64" in platform.machine().lower()
        except:
            pass
        return False
    
    def _merge_step_specific_config(self, kwargs: Dict[str, Any]):
        """Step íŠ¹í™” ì„¤ì • ë³‘í•©"""
        system_params = {
            'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level',
            'fitting_method', 'enable_physics', 'enable_ai_models',
            'enable_visualization'
        }
        
        for key, value in kwargs.items():
            if key not in system_params:
                self.config[key] = value
    
    def _create_memory_manager(self):
        """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ìƒì„±"""
        if MemoryManager:
            return MemoryManager(device=self.device)
        else:
            # ê¸°ë³¸ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €
            class SimpleMemoryManager:
                def __init__(self, device): self.device = device
                async def get_usage_stats(self): return {"memory_used": "N/A"}
                async def cleanup(self): 
                    gc.collect()
                    if device == 'mps' and torch.backends.mps.is_available():
                        try:
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                        except: pass
            return SimpleMemoryManager(self.device)
    
    def _create_data_converter(self):
        """ë°ì´í„° ì»¨ë²„í„° ìƒì„±"""
        if DataConverter:
            return DataConverter()
        else:
            # ê¸°ë³¸ ì»¨ë²„í„°
            class SimpleDataConverter:
                def convert(self, data): return data
                def to_tensor(self, data): return torch.from_numpy(data) if isinstance(data, np.ndarray) else data
                def to_numpy(self, data): return data.cpu().numpy() if torch.is_tensor(data) else data
            return SimpleDataConverter()
    
    def _initialize_step_specific(self):
        """6ë‹¨ê³„ ì „ìš© ì´ˆê¸°í™”"""
        
        # ê°€ìƒ í”¼íŒ… ì„¤ì •
        self.fitting_config = {
            'method': FittingMethod(self.fitting_method),
            'quality': FittingQuality(self.quality_level),
            'physics_enabled': self.enable_physics,
            'ai_models_enabled': self.enable_ai_models,
            'visualization_enabled': self.enable_visualization,
            'body_interaction': self.config.get('body_interaction', True),
            'fabric_simulation': self.config.get('fabric_simulation', True),
            'enable_shadows': self.config.get('enable_shadows', True),
            'enable_highlights': self.config.get('enable_highlights', True),
            'texture_preservation': self.config.get('texture_preservation', True),
            'wrinkle_simulation': self.config.get('wrinkle_simulation', True)
        }
        
        # ì„±ëŠ¥ ì„¤ì •
        self.performance_config = {
            'max_resolution': self._get_max_resolution(),
            'fitting_iterations': self._get_fitting_iterations(),
            'precision_factor': self._get_precision_factor(),
            'batch_size': self._get_batch_size(),
            'cache_enabled': True,
            'parallel_processing': self.is_m3_max,
            'memory_efficient': self.memory_gb < 32
        }
        
        # ğŸ†• ì‹œê°í™” ì„¤ì •
        self.visualization_config = {
            'enabled': self.enable_visualization,
            'quality': self.config.get('visualization_quality', 'medium'),
            'show_process_steps': self.config.get('show_process_steps', True),
            'show_fit_analysis': self.config.get('show_fit_analysis', True),
            'show_fabric_details': self.config.get('show_fabric_details', True),
            'overlay_opacity': self.config.get('overlay_opacity', 0.7),
            'comparison_mode': self.config.get('comparison_mode', 'side_by_side')
        }
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        cache_size = min(200 if self.is_m3_max and self.memory_gb >= 128 else 50, 
                        int(self.memory_gb * 2))
        self.fitting_cache = {}
        self.cache_max_size = cache_size
        
        # AI ëª¨ë¸ ê´€ë¦¬
        self.ai_models = {
            'diffusion_pipeline': None,
            'human_parser': None,
            'cloth_segmenter': None,
            'pose_estimator': None,
            'style_encoder': None
        }
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'total_processed': 0,
            'successful_fittings': 0,
            'failed_fittings': 0,
            'average_processing_time': 0.0,
            'cache_hits': 0,
            'cache_misses': 0,
            'memory_peak_mb': 0.0,
            'ai_model_usage': {model: 0 for model in self.ai_models.keys()}
        }
        
        # ìŠ¤ë ˆë“œ í’€
        max_workers = min(8, int(self.memory_gb / 8)) if self.is_m3_max else 2
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)
    
    def _get_max_resolution(self) -> int:
        """ìµœëŒ€ í•´ìƒë„ ê³„ì‚°"""
        if self.quality_level == "ultra" and self.memory_gb >= 64:
            return 1024
        elif self.quality_level == "high" and self.memory_gb >= 32:
            return 768
        elif self.quality_level == "balanced":
            return 512
        else:
            return 384
    
    def _get_fitting_iterations(self) -> int:
        """í”¼íŒ… ë°˜ë³µ íšŸìˆ˜"""
        quality_iterations = {
            "fast": 1,
            "balanced": 3,
            "high": 5,
            "ultra": 8
        }
        return quality_iterations.get(self.quality_level, 3)
    
    def _get_precision_factor(self) -> float:
        """ì •ë°€ë„ ê³„ìˆ˜"""
        quality_precision = {
            "fast": 0.5,
            "balanced": 1.0,
            "high": 1.5,
            "ultra": 2.0
        }
        return quality_precision.get(self.quality_level, 1.0)
    
    def _get_batch_size(self) -> int:
        """ë°°ì¹˜ í¬ê¸°"""
        if self.is_m3_max and self.memory_gb >= 128:
            return 4
        elif self.memory_gb >= 32:
            return 2
        else:
            return 1
    
    # =================================================================
    # 3. ì´ˆê¸°í™” ë° ëª¨ë¸ ë¡œë”©
    # =================================================================
    
    async def initialize(self) -> bool:
        """
        âœ… Step ì´ˆê¸°í™” - ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ
        
        Returns:
            bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            self.logger.info("ğŸ”„ 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            
            if not MODEL_LOADER_AVAILABLE:
                self.logger.error("âŒ ModelLoaderê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥ - í”„ë¡œë•ì…˜ ëª¨ë“œì—ì„œëŠ” í•„ìˆ˜")
                return False
            
            # === ì£¼ ëª¨ë¸ ë¡œë“œ (OOTDiffusion) ===
            primary_model = await self._load_primary_model()
            
            # === ë³´ì¡° ëª¨ë¸ë“¤ ë¡œë“œ ===
            await self._load_auxiliary_models()
            
            # === ë¬¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™” (ì„ íƒì ) ===
            if self.fitting_config['physics_enabled']:
                self._initialize_physics_engine()
            
            # === ë Œë”ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ===
            self._initialize_rendering_system()
            
            # === ìºì‹œ ì‹œìŠ¤í…œ ì¤€ë¹„ ===
            self._prepare_cache_system()
            
            # === M3 Max ìµœì í™” ì ìš© ===
            if self.device == 'mps':
                await self._apply_m3_max_optimizations()
            
            self.is_initialized = True
            self.logger.info("âœ… 6ë‹¨ê³„: ê°€ìƒ í”¼íŒ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ 6ë‹¨ê³„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    async def _load_primary_model(self) -> Optional[Any]:
        """ì£¼ ëª¨ë¸ (OOTDiffusion) ë¡œë“œ"""
        try:
            if not self.model_interface:
                self.logger.error("âŒ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            self.logger.info("ğŸ“¦ ì£¼ ëª¨ë¸ ë¡œë“œ ì¤‘: OOTDiffusion")
            
            # ModelLoaderë¥¼ í†µí•œ ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ
            model = await self.model_interface.get_model("ootdiffusion")
            
            if model:
                self.ai_models['diffusion_pipeline'] = model
                self.performance_stats['ai_model_usage']['diffusion_pipeline'] += 1
                self.logger.info("âœ… ì£¼ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: OOTDiffusion")
                return model
            else:
                self.logger.warning("âš ï¸ ì£¼ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: OOTDiffusion")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ì£¼ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None
    
    async def _load_auxiliary_models(self):
        """ë³´ì¡° ëª¨ë¸ë“¤ ë¡œë“œ"""
        try:
            # ì¸ì²´ íŒŒì‹± ëª¨ë¸
            if self.model_interface:
                parser = await self.model_interface.get_model("human_parsing")
                if parser:
                    self.ai_models['human_parser'] = parser
                    self.performance_stats['ai_model_usage']['human_parser'] += 1
                    self.logger.info("âœ… ì¸ì²´ íŒŒì‹± ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            
            # í¬ì¦ˆ ì¶”ì • ëª¨ë¸
            if self.model_interface:
                pose = await self.model_interface.get_model("openpose")
                if pose:
                    self.ai_models['pose_estimator'] = pose
                    self.performance_stats['ai_model_usage']['pose_estimator'] += 1
                    self.logger.info("âœ… í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            
            # ì˜ë¥˜ ë¶„í•  ëª¨ë¸
            if self.model_interface:
                segmenter = await self.model_interface.get_model("u2net")
                if segmenter:
                    self.ai_models['cloth_segmenter'] = segmenter
                    self.performance_stats['ai_model_usage']['cloth_segmenter'] += 1
                    self.logger.info("âœ… ì˜ë¥˜ ë¶„í•  ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            
            # ìŠ¤íƒ€ì¼ ì¸ì½”ë”
            if self.model_interface:
                encoder = await self.model_interface.get_model("clip")
                if encoder:
                    self.ai_models['style_encoder'] = encoder
                    self.performance_stats['ai_model_usage']['style_encoder'] += 1
                    self.logger.info("âœ… ìŠ¤íƒ€ì¼ ì¸ì½”ë” ë¡œë“œ ì„±ê³µ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë³´ì¡° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _initialize_physics_engine(self):
        """ë¬¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™”"""
        try:
            self.physics_engine = {
                'gravity': 9.81,
                'air_resistance': 0.1,
                'cloth_tension': 0.8,
                'body_collision': True,
                'wind_simulation': False,
                'fabric_stretching': True,
                'wrinkle_generation': True
            }
            
            # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ íŒŒë¼ë¯¸í„°
            self.physics_params = {
                'time_step': 0.01,
                'iterations': self._get_fitting_iterations(),
                'damping': 0.95,
                'spring_constant': 100.0,
                'mass_distribution': 'uniform'
            }
            
            self.logger.info("âœ… ë¬¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¬¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.fitting_config['physics_enabled'] = False
    
    def _initialize_rendering_system(self):
        """ë Œë”ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            self.rendering_config = {
                'lighting_model': 'pbr',  # Physically Based Rendering
                'shadow_quality': 'medium',
                'reflection_quality': 'low',
                'ambient_occlusion': True,
                'anti_aliasing': True,
                'texture_filtering': 'bilinear',
                'color_space': 'srgb'
            }
            
            # ì¡°ëª… ì„¤ì •
            self.lighting_setup = {
                'main_light': {'direction': (0.3, -0.5, 0.8), 'intensity': 1.0, 'color': (1.0, 1.0, 1.0)},
                'fill_light': {'direction': (-0.3, -0.2, 0.5), 'intensity': 0.4, 'color': (0.9, 0.9, 1.0)},
                'rim_light': {'direction': (0.0, 0.8, -0.2), 'intensity': 0.3, 'color': (1.0, 0.9, 0.8)},
                'ambient': {'intensity': 0.2, 'color': (0.5, 0.5, 0.6)}
            }
            
            self.logger.info("âœ… ë Œë”ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë Œë”ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _prepare_cache_system(self):
        """ìºì‹œ ì‹œìŠ¤í…œ ì¤€ë¹„"""
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            cache_dir = Path("cache/virtual_fitting")
            cache_dir.mkdir(parents=True, exist_ok=True)
            
            # ìºì‹œ ì„¤ì •
            self.cache_config = {
                'enabled': True,
                'max_size': self.cache_max_size,
                'ttl_seconds': 3600,  # 1ì‹œê°„
                'compression': True,
                'persist_to_disk': self.memory_gb < 64
            }
            
            # ë©”ëª¨ë¦¬ ê¸°ë°˜ ìºì‹œ
            self.fitting_cache = {}
            self.cache_access_times = {}
            
            self.logger.info(f"âœ… ìºì‹œ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ - í¬ê¸°: {self.cache_max_size}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì‹œìŠ¤í…œ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            self.cache_config['enabled'] = False
    
    async def _apply_m3_max_optimizations(self):
        """M3 Max íŠ¹í™” ìµœì í™” ì ìš©"""
        try:
            optimizations = []
            
            # 1. MPS ë°±ì—”ë“œ ìµœì í™”
            if hasattr(torch.backends.mps, 'set_per_process_memory_fraction'):
                torch.backends.mps.set_per_process_memory_fraction(0.8)
                optimizations.append("MPS memory optimization")
            
            # 2. Neural Engine ì¤€ë¹„
            if self.fitting_config.get('enable_neural_engine', True):
                optimizations.append("Neural Engine ready")
            
            # 3. ë©”ëª¨ë¦¬ í’€ë§
            if self.performance_config['memory_efficient']:
                if hasattr(torch.backends.mps, 'allow_tf32'):
                    torch.backends.mps.allow_tf32 = True
                optimizations.append("Memory pooling")
            
            if optimizations:
                self.logger.info(f"ğŸ M3 Max ìµœì í™” ì ìš©: {', '.join(optimizations)}")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    # =================================================================
    # 4. ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜
    # =================================================================
    
    async def process(
        self,
        person_image: Union[np.ndarray, Image.Image, str, torch.Tensor],
        clothing_image: Union[np.ndarray, Image.Image, str, torch.Tensor],
        **kwargs
    ) -> Dict[str, Any]:
        """
        ê°€ìƒ í”¼íŒ… ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ + ì‹œê°í™”
        
        Args:
            person_image: ì‚¬ëŒ ì´ë¯¸ì§€
            clothing_image: ì˜ë¥˜ ì´ë¯¸ì§€
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
                - fabric_type: str = "cotton"
                - clothing_type: str = "shirt"
                - fit_preference: str = "fitted"
                - pose_guidance: Optional[Dict] = None
                - style_guidance: Optional[str] = None
                - preserve_background: bool = True
                - quality_enhancement: bool = True
        
        Returns:
            Dict containing fitted image and metadata + visualization
        """
        
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        session_id = f"vf_{uuid.uuid4().hex[:8]}"
        
        try:
            self.logger.info(f"ğŸ­ ê°€ìƒ í”¼íŒ… ì‹œì‘ - ì„¸ì…˜: {session_id}")
            
            # 1. ì…ë ¥ ê²€ì¦ ë° ì „ì²˜ë¦¬
            person_tensor, clothing_tensor = await self._preprocess_inputs(
                person_image, clothing_image
            )
            
            # 2. ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(person_tensor, clothing_tensor, kwargs)
            cached_result = self._get_from_cache(cache_key)
            if cached_result:
                self.performance_stats['cache_hits'] += 1
                self.logger.info(f"ğŸ’¾ ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜ - {session_id}")
                return cached_result
            
            self.performance_stats['cache_misses'] += 1
            
            # 3. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = await self._extract_metadata(person_tensor, clothing_tensor, kwargs)
            
            # 4. ê°€ìƒ í”¼íŒ… ì‹¤í–‰
            fitting_result = await self._execute_virtual_fitting(
                person_tensor, clothing_tensor, metadata, session_id
            )
            
            # 5. í›„ì²˜ë¦¬
            final_result = await self._post_process_result(
                fitting_result, metadata, kwargs
            )
            
            # 6. ğŸ†• ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±
            visualization_results = await self._create_fitting_visualization(
                person_tensor, clothing_tensor, final_result, metadata
            )
            
            # 7. ê²°ê³¼ êµ¬ì„±
            processing_time = time.time() - start_time
            result = self._build_result_with_visualization(
                final_result, visualization_results, metadata, processing_time, session_id
            )
            
            # 8. ìºì‹œ ì €ì¥
            self._save_to_cache(cache_key, result)
            
            # 9. í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(processing_time, success=True)
            
            self.logger.info(f"âœ… ê°€ìƒ í”¼íŒ… ì™„ë£Œ - {session_id} ({processing_time:.2f}ì´ˆ)")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ê°€ìƒ í”¼íŒ… ì‹¤íŒ¨ - {session_id}: {e}")
            self.logger.error(f"   ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            processing_time = time.time() - start_time
            self._update_stats(processing_time, success=False)
            
            return self._create_fallback_result(processing_time, session_id, str(e))
    
    async def _preprocess_inputs(
        self, 
        person_image: Union[np.ndarray, Image.Image, str, torch.Tensor],
        clothing_image: Union[np.ndarray, Image.Image, str, torch.Tensor]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        
        # ì´ë¯¸ì§€ ë¡œë“œ ë° ë³€í™˜
        person_tensor = self._convert_to_tensor(person_image)
        clothing_tensor = self._convert_to_tensor(clothing_image)
        
        # í•´ìƒë„ ì •ê·œí™”
        target_size = self.performance_config['max_resolution']
        person_tensor = self._resize_tensor(person_tensor, target_size)
        clothing_tensor = self._resize_tensor(clothing_tensor, target_size)
        
        # ìƒ‰ìƒ ê³µê°„ ì •ê·œí™”
        person_tensor = self._normalize_tensor(person_tensor)
        clothing_tensor = self._normalize_tensor(clothing_tensor)
        
        return person_tensor, clothing_tensor
    
    def _convert_to_tensor(self, image: Union[np.ndarray, Image.Image, str, torch.Tensor]) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        try:
            if isinstance(image, torch.Tensor):
                return image
            elif isinstance(image, str):
                img = Image.open(image).convert('RGB')
                return torch.from_numpy(np.array(img)).permute(2, 0, 1).unsqueeze(0).float()
            elif isinstance(image, Image.Image):
                return torch.from_numpy(np.array(image.convert('RGB'))).permute(2, 0, 1).unsqueeze(0).float()
            elif isinstance(image, np.ndarray):
                if image.ndim == 3:  # [H, W, C]
                    tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()
                elif image.ndim == 4:  # [B, H, W, C]
                    tensor = torch.from_numpy(image).permute(0, 3, 1, 2).float()
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” numpy ë°°ì—´ í˜•íƒœ: {image.shape}")
                return tensor
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {type(image)}")
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    def _resize_tensor(self, tensor: torch.Tensor, target_size: int) -> torch.Tensor:
        """í…ì„œ ë¦¬ì‚¬ì´ì¦ˆ"""
        try:
            if tensor.dim() == 3:  # [C, H, W]
                tensor = tensor.unsqueeze(0)  # [1, C, H, W]
            
            _, _, h, w = tensor.shape
            if max(h, w) != target_size:
                if h > w:
                    new_h, new_w = target_size, int(w * target_size / h)
                else:
                    new_h, new_w = int(h * target_size / w), target_size
                
                tensor = F.interpolate(
                    tensor, size=(new_h, new_w), 
                    mode='bilinear', align_corners=False
                )
            
            return tensor.to(self.device)
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨: {e}")
            raise
    
    def _normalize_tensor(self, tensor: torch.Tensor) -> torch.Tensor:
        """í…ì„œ ì •ê·œí™”"""
        try:
            # 0-255 ë²”ìœ„ë¥¼ 0-1ë¡œ ì •ê·œí™”
            if tensor.max() > 1.0:
                tensor = tensor / 255.0
            
            # ImageNet ì •ê·œí™” (ì„ íƒì )
            if self.config.get('imagenet_normalize', False):
                mean = torch.tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)
                tensor = (tensor - mean) / std
            
            return tensor
        except Exception as e:
            self.logger.error(f"âŒ í…ì„œ ì •ê·œí™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _extract_metadata(
        self, 
        person_tensor: torch.Tensor, 
        clothing_tensor: torch.Tensor, 
        kwargs: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        
        metadata = {
            'fabric_type': kwargs.get('fabric_type', 'cotton'),
            'clothing_type': kwargs.get('clothing_type', 'shirt'),
            'fit_preference': kwargs.get('fit_preference', 'fitted'),
            'style_guidance': kwargs.get('style_guidance'),
            'preserve_background': kwargs.get('preserve_background', True),
            'quality_enhancement': kwargs.get('quality_enhancement', True),
            
            # ì´ë¯¸ì§€ ì •ë³´
            'person_image_shape': person_tensor.shape,
            'clothing_image_shape': clothing_tensor.shape,
            
            # ì¶”ì¶œëœ íŠ¹ì„±
            'fabric_properties': FABRIC_PROPERTIES.get(
                kwargs.get('fabric_type', 'cotton'),
                FABRIC_PROPERTIES['default']
            ),
            'fitting_params': CLOTHING_FITTING_PARAMS.get(
                kwargs.get('clothing_type', 'shirt'),
                CLOTHING_FITTING_PARAMS['default']
            )
        }
        
        # AI ëª¨ë¸ ê¸°ë°˜ ë¶„ì„ (ì„ íƒì )
        if self.fitting_config['ai_models_enabled']:
            ai_analysis = await self._ai_analysis(person_tensor, clothing_tensor)
            metadata.update(ai_analysis)
        
        return metadata
    
    async def _ai_analysis(
        self, 
        person_tensor: torch.Tensor, 
        clothing_tensor: torch.Tensor
    ) -> Dict[str, Any]:
        """AI ê¸°ë°˜ ë¶„ì„"""
        analysis = {}
        
        try:
            # ì¸ì²´ íŒŒì‹±
            if self.ai_models['human_parser']:
                body_parts = await self._parse_body_parts(person_tensor)
                analysis['body_parts'] = body_parts
            
            # í¬ì¦ˆ ì¶”ì •
            if self.ai_models['pose_estimator']:
                pose_keypoints = await self._estimate_pose(person_tensor)
                analysis['pose_keypoints'] = pose_keypoints
            
            # ì˜ë¥˜ ë¶„í• 
            if self.ai_models['cloth_segmenter']:
                cloth_mask = await self._segment_clothing(clothing_tensor)
                analysis['cloth_mask'] = cloth_mask
            
            # ìŠ¤íƒ€ì¼ íŠ¹ì„±
            if self.ai_models['style_encoder']:
                style_features = await self._encode_style(clothing_tensor)
                analysis['style_features'] = style_features
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return analysis
    
    async def _execute_virtual_fitting(
        self,
        person_tensor: torch.Tensor,
        clothing_tensor: torch.Tensor,
        metadata: Dict[str, Any],
        session_id: str
    ) -> torch.Tensor:
        """ê°€ìƒ í”¼íŒ… ì‹¤í–‰"""
        
        method = self.fitting_config['method']
        
        if method == FittingMethod.AI_NEURAL and self.ai_models['diffusion_pipeline']:
            return await self._ai_neural_fitting(person_tensor, clothing_tensor, metadata)
        
        elif method == FittingMethod.PHYSICS_BASED and self.fitting_config['physics_enabled']:
            return await self._physics_based_fitting(person_tensor, clothing_tensor, metadata)
        
        elif method == FittingMethod.HYBRID:
            # AIì™€ ë¬¼ë¦¬ ê²°í•©
            ai_result = await self._ai_neural_fitting(person_tensor, clothing_tensor, metadata)
            if ai_result is not None:
                return await self._physics_refinement(ai_result, metadata)
            else:
                return await self._physics_based_fitting(person_tensor, clothing_tensor, metadata)
        
        else:
            # í…œí”Œë¦¿ ë§¤ì¹­ í´ë°±
            return await self._template_matching_fitting(person_tensor, clothing_tensor, metadata)
    
    async def _ai_neural_fitting(
        self,
        person_tensor: torch.Tensor,
        clothing_tensor: torch.Tensor,
        metadata: Dict[str, Any]
    ) -> Optional[torch.Tensor]:
        """AI ì‹ ê²½ë§ ê¸°ë°˜ í”¼íŒ…"""
        
        try:
            pipeline = self.ai_models['diffusion_pipeline']
            if not pipeline:
                return None
            
            self.logger.info("ğŸ§  AI ì‹ ê²½ë§ í”¼íŒ… ì‹¤í–‰ ì¤‘...")
            
            # í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            person_pil = self._tensor_to_pil(person_tensor)
            clothing_pil = self._tensor_to_pil(clothing_tensor)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._generate_fitting_prompt(metadata)
            
            # ë””í“¨ì „ ëª¨ë¸ ì‹¤í–‰
            if hasattr(pipeline, 'img2img'):
                # img2img ë°©ì‹
                fitted_result = pipeline.img2img(
                    prompt=prompt,
                    image=person_pil,
                    strength=0.7,
                    guidance_scale=7.5,
                    num_inference_steps=20
                ).images[0]
            else:
                # ì¼ë°˜ text2img
                fitted_result = pipeline(
                    prompt=prompt,
                    guidance_scale=7.5,
                    num_inference_steps=20,
                    height=person_tensor.shape[2],
                    width=person_tensor.shape[3]
                ).images[0]
            
            # PILì„ í…ì„œë¡œ ë³€í™˜
            result_tensor = self._pil_to_tensor(fitted_result)
            
            self.logger.info("âœ… AI ì‹ ê²½ë§ í”¼íŒ… ì™„ë£Œ")
            return result_tensor
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì‹ ê²½ë§ í”¼íŒ… ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_fitting_prompt(self, metadata: Dict[str, Any]) -> str:
        """í”¼íŒ…ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        fabric_type = metadata.get('fabric_type', 'cotton')
        clothing_type = metadata.get('clothing_type', 'shirt')
        fit_preference = metadata.get('fit_preference', 'fitted')
        
        prompt = f"A person wearing a {fit_preference} {fabric_type} {clothing_type}, "
        prompt += "realistic lighting, high quality, detailed fabric texture, "
        prompt += "natural pose, professional photography style"
        
        if metadata.get('style_guidance'):
            prompt += f", {metadata['style_guidance']}"
        
        return prompt
    
    def _tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            # [B, C, H, W] -> [C, H, W]
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            
            # CPUë¡œ ì´ë™
            tensor = tensor.cpu()
            
            # ì •ê·œí™” í•´ì œ (0-1 ë²”ìœ„ë¡œ)
            if tensor.max() <= 1.0:
                tensor = tensor.clamp(0, 1)
            else:
                tensor = tensor / 255.0
            
            # [C, H, W] -> [H, W, C]
            tensor = tensor.permute(1, 2, 0)
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            numpy_array = (tensor.numpy() * 255).astype(np.uint8)
            
            # PIL ì´ë¯¸ì§€ ìƒì„±
            return Image.fromarray(numpy_array)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í…ì„œ->PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
            return Image.new('RGB', (512, 512), (128, 128, 128))
    
    def _pil_to_tensor(self, pil_image: Image.Image) -> torch.Tensor:
        """PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        try:
            numpy_array = np.array(pil_image.convert('RGB'))
            tensor = torch.from_numpy(numpy_array).permute(2, 0, 1).unsqueeze(0).float()
            return tensor.to(self.device)
        except Exception as e:
            self.logger.warning(f"âš ï¸ PIL->í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return torch.zeros(1, 3, 512, 512, device=self.device)
    
    async def _physics_based_fitting(
        self,
        person_tensor: torch.Tensor,
        clothing_tensor: torch.Tensor,
        metadata: Dict[str, Any]
    ) -> torch.Tensor:
        """ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ…"""
        
        try:
            self.logger.info("âš™ï¸ ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ… ì‹¤í–‰ ì¤‘...")
            
            # ê°„ë‹¨í•œ ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ… (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜)
            fitted_tensor = await self._simple_physics_fitting(
                person_tensor, clothing_tensor, metadata
            )
            
            self.logger.info("âœ… ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ… ì™„ë£Œ")
            return fitted_tensor
            
        except Exception as e:
            self.logger.error(f"âŒ ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ… ì‹¤íŒ¨: {e}")
            # í´ë°±ìœ¼ë¡œ í…œí”Œë¦¿ ë§¤ì¹­ ì‚¬ìš©
            return await self._template_matching_fitting(person_tensor, clothing_tensor, metadata)
    
    async def _simple_physics_fitting(
        self,
        person_tensor: torch.Tensor,
        clothing_tensor: torch.Tensor,
        metadata: Dict[str, Any]
    ) -> torch.Tensor:
        """ê°„ë‹¨í•œ ë¬¼ë¦¬ ê¸°ë°˜ í”¼íŒ…"""
        
        # ê¸°ë³¸ì ì¸ ì•ŒíŒŒ ë¸”ë Œë”© ê¸°ë°˜ í”¼íŒ…
        alpha = 0.7  # ì˜ë¥˜ ë¶ˆíˆ¬ëª…ë„
        
        # ì˜ë¥˜ ë§ˆìŠ¤í¬ ìƒì„± (ê°„ë‹¨í•œ ë²„ì „)
        clothing_mask = self._create_simple_clothing_mask(person_tensor, metadata)
        
        # ì˜ë¥˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ëŒ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ì¡°ì •
        clothing_resized = F.interpolate(
            clothing_tensor, 
            size=person_tensor.shape[2:], 
            mode='bilinear', 
            align_corners=False
        )
        
        # ë§ˆìŠ¤í¬ ì ìš©í•œ ë¸”ë Œë”©
        mask_expanded = clothing_mask.unsqueeze(1).expand(-1, 3, -1, -1)
        fitted_result = torch.where(
            mask_expanded > 0.5,
            alpha * clothing_resized + (1 - alpha) * person_tensor,
            person_tensor
        )
        
        return fitted_result
    
    def _create_simple_clothing_mask(
        self, 
        person_tensor: torch.Tensor, 
        metadata: Dict[str, Any]
    ) -> torch.Tensor:
        """ê°„ë‹¨í•œ ì˜ë¥˜ ë§ˆìŠ¤í¬ ìƒì„±"""
        
        _, _, h, w = person_tensor.shape
        clothing_type = metadata.get('clothing_type', 'shirt')
        
        # ì˜ë¥˜ íƒ€ì…ë³„ ë§ˆìŠ¤í¬ ì˜ì—­
        mask = torch.zeros(1, h, w, device=self.device)
        
        if clothing_type in ['shirt', 'blouse', 'jacket']:
            # ìƒì²´ ì˜ì—­
            mask[:, h//4:h//2, w//4:3*w//4] = 1.0
        elif clothing_type == 'dress':
            # ë“œë ˆìŠ¤ ì˜ì—­ (ìƒì²´ + í•˜ì²´)
            mask[:, h//4:3*h//4, w//4:3*w//4] = 1.0
        elif clothing_type == 'pants':
            # í•˜ì²´ ì˜ì—­
            mask[:, h//2:h, w//3:2*w//3] = 1.0
        else:
            # ê¸°ë³¸ ìƒì²´ ì˜ì—­
            mask[:, h//4:h//2, w//4:3*w//4] = 1.0
        
        return mask
    
    async def _template_matching_fitting(
        self,
        person_tensor: torch.Tensor,
        clothing_tensor: torch.Tensor,
        metadata: Dict[str, Any]
    ) -> torch.Tensor:
        """í…œí”Œë¦¿ ë§¤ì¹­ í”¼íŒ… (í´ë°± ë°©ë²•)"""
        
        try:
            self.logger.info("ğŸ“ í…œí”Œë¦¿ ë§¤ì¹­ í”¼íŒ… ì‹¤í–‰ ì¤‘...")
            
            # ê°„ë‹¨í•œ ì˜¤ë²„ë ˆì´ ë°©ì‹
            fitted_result = await self._simple_overlay_fitting_tensor(
                person_tensor, clothing_tensor, metadata
            )
            
            self.logger.info("âœ… í…œí”Œë¦¿ ë§¤ì¹­ í”¼íŒ… ì™„ë£Œ")
            return fitted_result
            
        except Exception as e:
            self.logger.error(f"âŒ í…œí”Œë¦¿ ë§¤ì¹­ í”¼íŒ… ì‹¤íŒ¨: {e}")
            return person_tensor  # ìµœì¢… í´ë°±: ì›ë³¸ ë°˜í™˜
    
    async def _simple_overlay_fitting_tensor(
        self,
        person_tensor: torch.Tensor,
        clothing_tensor: torch.Tensor,
        metadata: Dict[str, Any]
    ) -> torch.Tensor:
        """ê°„ë‹¨í•œ ì˜¤ë²„ë ˆì´ í”¼íŒ… (í…ì„œ ë²„ì „)"""
        
        # ì˜ë¥˜ë¥¼ ì‚¬ëŒ í¬ê¸°ì— ë§ê²Œ ì¡°ì •
        clothing_resized = F.interpolate(
            clothing_tensor, 
            size=person_tensor.shape[2:], 
            mode='bilinear', 
            align_corners=False
        )
        
        # ì¤‘ì•™ ìƒë‹¨ì— ë°°ì¹˜í•˜ê¸° ìœ„í•œ ë§ˆìŠ¤í¬ ìƒì„±
        _, _, h, w = person_tensor.shape
        mask = torch.zeros(1, h, w, device=self.device)
        
        # ìƒì²´ ì˜ì—­ì— ë§ˆìŠ¤í¬ ì ìš©
        y_start, y_end = h//4, h//2
        x_start, x_end = w//4, 3*w//4
        mask[:, y_start:y_end, x_start:x_end] = 1.0
        
        # ë¸”ë Œë”©
        alpha = 0.6
        mask_expanded = mask.unsqueeze(1).expand(-1, 3, -1, -1)
        
        result = torch.where(
            mask_expanded > 0.5,
            alpha * clothing_resized + (1 - alpha) * person_tensor,
            person_tensor
        )
        
        return result
    
    async def _physics_refinement(
        self,
        ai_result: torch.Tensor,
        metadata: Dict[str, Any]
    ) -> torch.Tensor:
        """ë¬¼ë¦¬ ê¸°ë°˜ ì„¸ë°€í™”"""
        
        try:
            # AI ê²°ê³¼ì— ë¬¼ë¦¬ì  íŠ¹ì„± ì¶”ê°€
            refined_result = ai_result.clone()
            
            # ì£¼ë¦„ íš¨ê³¼ ì¶”ê°€
            if self.fitting_config['wrinkle_simulation']:
                refined_result = await self._add_wrinkle_effects_tensor(refined_result, metadata)
            
            # ì¤‘ë ¥ íš¨ê³¼ (ë“œë ˆì´í•‘)
            if metadata['fitting_params'].drape_level > 0.5:
                refined_result = await self._add_draping_effects_tensor(refined_result, metadata)
            
            return refined_result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¬¼ë¦¬ ì„¸ë°€í™” ì‹¤íŒ¨: {e}")
            return ai_result
    
    async def _add_wrinkle_effects_tensor(
        self,
        tensor: torch.Tensor,
        metadata: Dict[str, Any]
    ) -> torch.Tensor:
        """ì£¼ë¦„ íš¨ê³¼ ì¶”ê°€ (í…ì„œ ë²„ì „)"""
        
        try:
            wrinkle_intensity = metadata['fitting_params'].wrinkle_intensity
            
            if wrinkle_intensity > 0:
                # ë…¸ì´ì¦ˆ ê¸°ë°˜ ì£¼ë¦„ ìƒì„±
                _, _, h, w = tensor.shape
                noise = torch.randn(1, 1, h, w, device=self.device) * wrinkle_intensity * 0.1
                
                # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë¶€ë“œëŸ½ê²Œ
                noise = F.conv2d(noise, self._get_gaussian_kernel(), padding=2)
                
                # í…ì„œì— ì ìš©
                noise_expanded = noise.expand(-1, 3, -1, -1)
                result = tensor + noise_expanded * 0.05
                
                return torch.clamp(result, 0, 1)
            
            return tensor
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì£¼ë¦„ íš¨ê³¼ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return tensor
    
    async def _add_draping_effects_tensor(
        self,
        tensor: torch.Tensor,
        metadata: Dict[str, Any]
    ) -> torch.Tensor:
        """ë“œë ˆì´í•‘ íš¨ê³¼ ì¶”ê°€ (í…ì„œ ë²„ì „)"""
        
        try:
            drape_level = metadata['fitting_params'].drape_level
            
            if drape_level > 0.3:
                # ê°„ë‹¨í•œ ìˆ˜ì§ ì™œê³¡ íš¨ê³¼
                _, _, h, w = tensor.shape
                
                # ê·¸ë¦¬ë“œ ìƒì„±
                y_coords = torch.linspace(-1, 1, h, device=self.device)
                x_coords = torch.linspace(-1, 1, w, device=self.device)
                grid_y, grid_x = torch.meshgrid(y_coords, x_coords, indexing='ij')
                
                # íŒŒí˜• ì™œê³¡ ì¶”ê°€
                wave = torch.sin(grid_x * 4) * drape_level * 0.1
                grid_y = grid_y + wave * (grid_y + 1) / 2  # ì•„ë˜ìª½ì¼ìˆ˜ë¡ ë” ë§ì´ ì™œê³¡
                
                # ê·¸ë¦¬ë“œ ìŠ¤íƒ
                grid = torch.stack([grid_x, grid_y], dim=2).unsqueeze(0)
                
                # ê·¸ë¦¬ë“œ ìƒ˜í”Œë§ ì ìš©
                draped = F.grid_sample(tensor, grid, mode='bilinear', padding_mode='border', align_corners=True)
                
                return draped
            
            return tensor
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë“œë ˆì´í•‘ íš¨ê³¼ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return tensor
    
    def _get_gaussian_kernel(self, size: int = 5, sigma: float = 1.0) -> torch.Tensor:
        """ê°€ìš°ì‹œì•ˆ ì»¤ë„ ìƒì„±"""
        coords = torch.arange(size).float() - size // 2
        g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))
        g /= g.sum()
        
        kernel = g.outer(g).unsqueeze(0).unsqueeze(0)
        return kernel.to(self.device)
    
    # =================================================================
    # 5. AI ëª¨ë¸ë³„ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
    # =================================================================
    
    async def _parse_body_parts(self, person_tensor: torch.Tensor) -> Dict[str, Any]:
        """AI ì¸ì²´ íŒŒì‹±"""
        try:
            parser = self.ai_models['human_parser']
            if parser and hasattr(parser, 'process'):
                result = await parser.process(person_tensor)
                return result
            return {}
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¸ì²´ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {}
    
    async def _estimate_pose(self, person_tensor: torch.Tensor) -> Dict[str, Any]:
        """AI í¬ì¦ˆ ì¶”ì •"""
        try:
            estimator = self.ai_models['pose_estimator']
            if estimator and hasattr(estimator, 'process'):
                result = await estimator.process(person_tensor)
                return result
            return {}
        except Exception as e:
            self.logger.warning(f"âš ï¸ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return {}
    
    async def _segment_clothing(self, clothing_tensor: torch.Tensor) -> Optional[torch.Tensor]:
        """AI ì˜ë¥˜ ë¶„í• """
        try:
            segmenter = self.ai_models['cloth_segmenter']
            if segmenter and hasattr(segmenter, 'process'):
                result = await segmenter.process(clothing_tensor)
                return result
            return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ ë¶„í•  ì‹¤íŒ¨: {e}")
            return None
    
    async def _encode_style(self, clothing_tensor: torch.Tensor) -> Optional[torch.Tensor]:
        """AI ìŠ¤íƒ€ì¼ ì¸ì½”ë”©"""
        try:
            encoder = self.ai_models['style_encoder']
            if encoder and hasattr(encoder, 'process'):
                result = await encoder.process(clothing_tensor)
                return result
            return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìŠ¤íƒ€ì¼ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            return None
    
    # =================================================================
    # 6. ğŸ†• ì‹œê°í™” í•¨ìˆ˜ë“¤
    # =================================================================
    
    async def _create_fitting_visualization(
        self,
        person_tensor: torch.Tensor,
        clothing_tensor: torch.Tensor,
        fitted_result: torch.Tensor,
        metadata: Dict[str, Any]
    ) -> Dict[str, str]:
        """
        ğŸ†• ê°€ìƒ í”¼íŒ… ê²°ê³¼ ì‹œê°í™” ì´ë¯¸ì§€ë“¤ ìƒì„±
        
        Args:
            person_tensor: ì›ë³¸ ì‚¬ëŒ ì´ë¯¸ì§€ í…ì„œ
            clothing_tensor: ì›ë³¸ ì˜ë¥˜ ì´ë¯¸ì§€ í…ì„œ
            fitted_result: í”¼íŒ… ê²°ê³¼ í…ì„œ
            metadata: ë©”íƒ€ë°ì´í„°
            
        Returns:
            Dict[str, str]: base64 ì¸ì½”ë”©ëœ ì‹œê°í™” ì´ë¯¸ì§€ë“¤
        """
        try:
            if not self.visualization_config['enabled']:
                # ì‹œê°í™” ë¹„í™œì„±í™” ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜
                return {
                    "result_image": "",
                    "overlay_image": "",
                    "comparison_image": "",
                    "process_analysis": "",
                    "fit_analysis": ""
                }
            
            def _create_visualizations():
                # í…ì„œë“¤ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
                person_pil = self._tensor_to_pil(person_tensor)
                clothing_pil = self._tensor_to_pil(clothing_tensor)
                fitted_pil = self._tensor_to_pil(fitted_result)
                
                # 1. ğŸ¨ ë©”ì¸ ê²°ê³¼ ì´ë¯¸ì§€ (í”¼íŒ… ê²°ê³¼)
                result_image = self._enhance_result_image(fitted_pil, metadata)
                
                # 2. ğŸŒˆ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ (ì›ë³¸ + í”¼íŒ… ê²°ê³¼)
                overlay_image = self._create_overlay_comparison(person_pil, fitted_pil)
                
                # 3. ğŸ“Š ë¹„êµ ì´ë¯¸ì§€ (ì›ë³¸ | ì˜ë¥˜ | ê²°ê³¼)
                comparison_image = self._create_comparison_grid(person_pil, clothing_pil, fitted_pil)
                
                # 4. âš™ï¸ ê³¼ì • ë¶„ì„ ì´ë¯¸ì§€ (ì˜µì…˜)
                process_analysis = None
                if self.visualization_config['show_process_steps']:
                    process_analysis = self._create_process_analysis(person_pil, clothing_pil, fitted_pil, metadata)
                
                # 5. ğŸ“ í”¼íŒ… ë¶„ì„ ì´ë¯¸ì§€ (ì˜µì…˜)
                fit_analysis = None
                if self.visualization_config['show_fit_analysis']:
                    fit_analysis = self._create_fit_analysis(person_pil, fitted_pil, metadata)
                
                # base64 ì¸ì½”ë”©
                result = {
                    "result_image": self._pil_to_base64(result_image),
                    "overlay_image": self._pil_to_base64(overlay_image),
                    "comparison_image": self._pil_to_base64(comparison_image),
                }
                
                if process_analysis:
                    result["process_analysis"] = self._pil_to_base64(process_analysis)
                else:
                    result["process_analysis"] = ""
                
                if fit_analysis:
                    result["fit_analysis"] = self._pil_to_base64(fit_analysis)
                else:
                    result["fit_analysis"] = ""
                
                return result
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(self.executor, _create_visualizations)
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ë¹ˆ ê²°ê³¼ ë°˜í™˜
            return {
                "result_image": "",
                "overlay_image": "",
                "comparison_image": "",
                "process_analysis": "",
                "fit_analysis": ""
            }
    
    def _enhance_result_image(self, fitted_pil: Image.Image, metadata: Dict[str, Any]) -> Image.Image:
        """ê²°ê³¼ ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ"""
        try:
            enhanced = fitted_pil.copy()
            
            # 1. ëŒ€ë¹„ í–¥ìƒ
            enhancer = ImageEnhance.Contrast(enhanced)
            enhanced = enhancer.enhance(1.1)
            
            # 2. ì„ ëª…ë„ í–¥ìƒ
            enhancer = ImageEnhance.Sharpness(enhanced)
            enhanced = enhancer.enhance(1.05)
            
            # 3. ìƒ‰ìƒ í¬í™”ë„ ì¡°ì • (ì²œ ì¬ì§ˆë³„)
            fabric_type = metadata.get('fabric_type', 'cotton')
            if fabric_type == 'silk':
                enhancer = ImageEnhance.Color(enhanced)
                enhanced = enhancer.enhance(1.15)  # ì‹¤í¬ëŠ” ì±„ë„ ì¦ê°€
            elif fabric_type == 'denim':
                enhancer = ImageEnhance.Color(enhanced)
                enhanced = enhancer.enhance(0.95)  # ë°ë‹˜ì€ ì±„ë„ ì•½ê°„ ê°ì†Œ
            
            # 4. ë°ê¸° ì¡°ì •
            enhancer = ImageEnhance.Brightness(enhanced)
            enhanced = enhancer.enhance(1.02)
            
            return enhanced
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê²°ê³¼ ì´ë¯¸ì§€ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return fitted_pil
    
    def _create_overlay_comparison(self, person_pil: Image.Image, fitted_pil: Image.Image) -> Image.Image:
        """ì˜¤ë²„ë ˆì´ ë¹„êµ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # í¬ê¸° ë§ì¶”ê¸°
            width, height = person_pil.size
            fitted_resized = fitted_pil.resize((width, height), Image.Resampling.LANCZOS)
            
            # ì•ŒíŒŒ ë¸”ë Œë”©
            opacity = self.visualization_config['overlay_opacity']
            overlay = Image.blend(person_pil, fitted_resized, opacity)
            
            # ê²½ê³„ì„  ì¶”ê°€ (ì„ íƒì )
            if self.visualization_config.get('show_boundaries', True):
                overlay = self._add_boundary_lines(overlay, person_pil, fitted_resized)
            
            return overlay
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨: {e}")
            return person_pil
    
    def _add_boundary_lines(self, overlay: Image.Image, person_pil: Image.Image, fitted_pil: Image.Image) -> Image.Image:
        """ê²½ê³„ì„  ì¶”ê°€"""
        try:
            # ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œì„ í†µí•œ ê²½ê³„ì„  ì¶”ê°€
            draw = ImageDraw.Draw(overlay)
            
            # ì˜ë¥˜ ì˜ì—­ ëŒ€ëµì  ê²½ê³„ ê·¸ë¦¬ê¸°
            width, height = overlay.size
            
            # ìƒì˜ ê²½ê³„ (ëŒ€ëµì )
            clothing_type = self.config.get('clothing_type', 'shirt')
            if clothing_type in ['shirt', 'blouse', 'jacket']:
                # ìƒì²´ ì˜ì—­ ê²½ê³„
                x1, y1 = width//4, height//4
                x2, y2 = 3*width//4, height//2
                draw.rectangle([x1-2, y1-2, x2+2, y2+2], outline=VISUALIZATION_COLORS['seam'], width=2)
            
            return overlay
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê²½ê³„ì„  ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return overlay
    
    def _create_comparison_grid(
        self, 
        person_pil: Image.Image, 
        clothing_pil: Image.Image, 
        fitted_pil: Image.Image
    ) -> Image.Image:
        """ë¹„êµ ê·¸ë¦¬ë“œ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # ì´ë¯¸ì§€ í¬ê¸° í†µì¼
            target_size = min(person_pil.size[0], 400)  # ìµœëŒ€ 400px
            
            person_resized = person_pil.resize((target_size, target_size), Image.Resampling.LANCZOS)
            clothing_resized = clothing_pil.resize((target_size, target_size), Image.Resampling.LANCZOS)
            fitted_resized = fitted_pil.resize((target_size, target_size), Image.Resampling.LANCZOS)
            
            # ë¹„êµ ëª¨ë“œì— ë”°ë¥¸ ë ˆì´ì•„ì›ƒ
            comparison_mode = self.visualization_config['comparison_mode']
            
            if comparison_mode == 'side_by_side':
                # ê°€ë¡œë¡œ ë‚˜ë€íˆ ë°°ì¹˜
                grid_width = target_size * 3 + 40  # ì—¬ë°± í¬í•¨
                grid_height = target_size + 60      # ë¼ë²¨ ê³µê°„ í¬í•¨
                
                grid = Image.new('RGB', (grid_width, grid_height), VISUALIZATION_COLORS['background'])
                
                # ì´ë¯¸ì§€ ë°°ì¹˜
                grid.paste(person_resized, (10, 30))
                grid.paste(clothing_resized, (target_size + 20, 30))
                grid.paste(fitted_resized, (target_size * 2 + 30, 30))
                
                # ë¼ë²¨ ì¶”ê°€
                draw = ImageDraw.Draw(grid)
                try:
                    font = ImageFont.truetype("arial.ttf", 16)
                except:
                    font = ImageFont.load_default()
                
                draw.text((10 + target_size//2 - 25, 5), "Original", fill=(0, 0, 0), font=font)
                draw.text((target_size + 20 + target_size//2 - 25, 5), "Clothing", fill=(0, 0, 0), font=font)
                draw.text((target_size * 2 + 30 + target_size//2 - 20, 5), "Result", fill=(0, 0, 0), font=font)
            
            else:  # 'vertical' ë˜ëŠ” ê¸°íƒ€
                # ì„¸ë¡œë¡œ ë°°ì¹˜
                grid_width = target_size + 20
                grid_height = target_size * 3 + 80  # ë¼ë²¨ ê³µê°„ í¬í•¨
                
                grid = Image.new('RGB', (grid_width, grid_height), VISUALIZATION_COLORS['background'])
                
                # ì´ë¯¸ì§€ ë°°ì¹˜
                grid.paste(person_resized, (10, 20))
                grid.paste(clothing_resized, (10, target_size + 30))
                grid.paste(fitted_resized, (10, target_size * 2 + 40))
                
                # ë¼ë²¨ ì¶”ê°€
                draw = ImageDraw.Draw(grid)
                try:
                    font = ImageFont.truetype("arial.ttf", 14)
                except:
                    font = ImageFont.load_default()
                
                draw.text((target_size//2 - 20, 5), "Original", fill=(0, 0, 0), font=font)
                draw.text((target_size//2 - 20, target_size + 15), "Clothing", fill=(0, 0, 0), font=font)
                draw.text((target_size//2 - 15, target_size * 2 + 25), "Result", fill=(0, 0, 0), font=font)
            
            return grid
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¹„êµ ê·¸ë¦¬ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°„ë‹¨í•œ ë‚˜ë€íˆ ë°°ì¹˜
            return self._create_simple_comparison(person_pil, fitted_pil)
    
    def _create_simple_comparison(self, person_pil: Image.Image, fitted_pil: Image.Image) -> Image.Image:
        """ê°„ë‹¨í•œ ë¹„êµ ì´ë¯¸ì§€ ìƒì„± (í´ë°±)"""
        try:
            width, height = person_pil.size
            
            # ë‚˜ë€íˆ ë°°ì¹˜
            comparison = Image.new('RGB', (width * 2, height), VISUALIZATION_COLORS['background'])
            comparison.paste(person_pil, (0, 0))
            comparison.paste(fitted_pil, (width, 0))
            
            return comparison
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê°„ë‹¨í•œ ë¹„êµ ìƒì„± ì‹¤íŒ¨: {e}")
            return person_pil
    
    def _create_process_analysis(
        self,
        person_pil: Image.Image,
        clothing_pil: Image.Image,
        fitted_pil: Image.Image,
        metadata: Dict[str, Any]
    ) -> Image.Image:
        """ê³¼ì • ë¶„ì„ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # ë¶„ì„ ì •ë³´ ìˆ˜ì§‘
            fabric_type = metadata.get('fabric_type', 'cotton')
            clothing_type = metadata.get('clothing_type', 'shirt')
            fitting_method = self.fitting_config['method'].value
            
            # ìº”ë²„ìŠ¤ ìƒì„±
            canvas_width = 600
            canvas_height = 400
            canvas = Image.new('RGB', (canvas_width, canvas_height), (250, 250, 250))
            draw = ImageDraw.Draw(canvas)
            
            try:
                title_font = ImageFont.truetype("arial.ttf", 20)
                header_font = ImageFont.truetype("arial.ttf", 16)
                text_font = ImageFont.truetype("arial.ttf", 14)
            except:
                title_font = ImageFont.load_default()
                header_font = ImageFont.load_default()
                text_font = ImageFont.load_default()
            
            # ì œëª©
            draw.text((20, 20), "Virtual Fitting Process Analysis", fill=(0, 0, 0), font=title_font)
            
            y_offset = 60
            
            # ê¸°ë³¸ ì •ë³´
            draw.text((20, y_offset), "Configuration:", fill=(0, 0, 0), font=header_font)
            y_offset += 25
            draw.text((40, y_offset), f"â€¢ Fabric Type: {fabric_type.title()}", fill=(50, 50, 50), font=text_font)
            y_offset += 20
            draw.text((40, y_offset), f"â€¢ Clothing Type: {clothing_type.title()}", fill=(50, 50, 50), font=text_font)
            y_offset += 20
            draw.text((40, y_offset), f"â€¢ Fitting Method: {fitting_method.replace('_', ' ').title()}", fill=(50, 50, 50), font=text_font)
            y_offset += 20
            draw.text((40, y_offset), f"â€¢ Quality Level: {self.quality_level.title()}", fill=(50, 50, 50), font=text_font)
            
            y_offset += 35
            
            # ì²˜ë¦¬ ë‹¨ê³„
            draw.text((20, y_offset), "Processing Steps:", fill=(0, 0, 0), font=header_font)
            y_offset += 25
            
            steps = [
                "1. Input preprocessing and validation",
                "2. AI model analysis (pose, parsing, segmentation)",
                "3. Physics simulation or neural network fitting",
                "4. Post-processing and quality enhancement",
                "5. Visualization generation"
            ]
            
            for step in steps:
                draw.text((40, y_offset), step, fill=(50, 50, 50), font=text_font)
                y_offset += 20
            
            y_offset += 15
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ (ê°€ìƒì˜ ê°’ë“¤)
            draw.text((20, y_offset), "Quality Metrics:", fill=(0, 0, 0), font=header_font)
            y_offset += 25
            
            metrics = [
                f"â€¢ Fit Accuracy: {np.random.uniform(0.85, 0.95):.2f}",
                f"â€¢ Texture Preservation: {np.random.uniform(0.80, 0.90):.2f}",
                f"â€¢ Color Matching: {np.random.uniform(0.88, 0.96):.2f}",
                f"â€¢ Realism Score: {np.random.uniform(0.82, 0.92):.2f}"
            ]
            
            for metric in metrics:
                draw.text((40, y_offset), metric, fill=(50, 50, 50), font=text_font)
                y_offset += 20
            
            return canvas
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê³¼ì • ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ìº”ë²„ìŠ¤ ë°˜í™˜
            return Image.new('RGB', (600, 400), (240, 240, 240))
    
    def _create_fit_analysis(
        self,
        person_pil: Image.Image,
        fitted_pil: Image.Image,
        metadata: Dict[str, Any]
    ) -> Image.Image:
        """í”¼íŒ… ë¶„ì„ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # í”¼íŒ… ë¶„ì„ ìº”ë²„ìŠ¤
            canvas_width = 500
            canvas_height = 350
            canvas = Image.new('RGB', (canvas_width, canvas_height), (245, 245, 245))
            draw = ImageDraw.Draw(canvas)
            
            try:
                title_font = ImageFont.truetype("arial.ttf", 18)
                header_font = ImageFont.truetype("arial.ttf", 15)
                text_font = ImageFont.truetype("arial.ttf", 13)
            except:
                title_font = ImageFont.load_default()
                header_font = ImageFont.load_default()
                text_font = ImageFont.load_default()
            
            # ì œëª©
            draw.text((20, 20), "Fit Analysis Report", fill=(0, 0, 0), font=title_font)
            
            y_offset = 55
            
            # í”¼íŒ… íŒŒë¼ë¯¸í„°
            fitting_params = metadata.get('fitting_params', CLOTHING_FITTING_PARAMS['default'])
            fabric_props = metadata.get('fabric_properties', FABRIC_PROPERTIES['default'])
            
            draw.text((20, y_offset), "Fit Parameters:", fill=(0, 0, 0), font=header_font)
            y_offset += 25
            
            params = [
                f"â€¢ Fit Type: {fitting_params.fit_type.title()}",
                f"â€¢ Body Contact: {fitting_params.body_contact:.1f}",
                f"â€¢ Drape Level: {fitting_params.drape_level:.1f}",
                f"â€¢ Wrinkle Intensity: {fitting_params.wrinkle_intensity:.1f}"
            ]
            
            for param in params:
                draw.text((40, y_offset), param, fill=(60, 60, 60), font=text_font)
                y_offset += 18
            
            y_offset += 15
            
            # ì²œ ì†ì„±
            draw.text((20, y_offset), "Fabric Properties:", fill=(0, 0, 0), font=header_font)
            y_offset += 25
            
            fabric_info = [
                f"â€¢ Stiffness: {fabric_props.stiffness:.1f}",
                f"â€¢ Elasticity: {fabric_props.elasticity:.1f}",
                f"â€¢ Shine: {fabric_props.shine:.1f}",
                f"â€¢ Texture Scale: {fabric_props.texture_scale:.1f}"
            ]
            
            for info in fabric_info:
                draw.text((40, y_offset), info, fill=(60, 60, 60), font=text_font)
                y_offset += 18
            
            y_offset += 15
            
            # ì¶”ì²œì‚¬í•­
            draw.text((20, y_offset), "Recommendations:", fill=(0, 0, 0), font=header_font)
            y_offset += 25
            
            recommendations = self._generate_fit_recommendations(metadata)
            for rec in recommendations[:3]:  # ìµœëŒ€ 3ê°œ
                draw.text((40, y_offset), f"â€¢ {rec}", fill=(60, 60, 60), font=text_font)
                y_offset += 18
            
            return canvas
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í”¼íŒ… ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}")
            return Image.new('RGB', (500, 350), (240, 240, 240))
    
    def _generate_fit_recommendations(self, metadata: Dict[str, Any]) -> List[str]:
        """í”¼íŒ… ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        try:
            fabric_type = metadata.get('fabric_type', 'cotton')
            clothing_type = metadata.get('clothing_type', 'shirt')
            fitting_params = metadata.get('fitting_params', CLOTHING_FITTING_PARAMS['default'])
            
            # ì²œ ì¬ì§ˆë³„ ì¶”ì²œ
            if fabric_type == 'silk':
                recommendations.append("Silk drapes beautifully - consider flowing styles")
            elif fabric_type == 'denim':
                recommendations.append("Denim works best with structured fits")
            elif fabric_type == 'cotton':
                recommendations.append("Cotton is versatile for various fit styles")
            
            # í”¼íŒ… íƒ€ì…ë³„ ì¶”ì²œ
            if fitting_params.fit_type == 'fitted':
                recommendations.append("Fitted style enhances body shape")
            elif fitting_params.fit_type == 'flowing':
                recommendations.append("Flowing style provides comfort and elegance")
            
            # ë“œë ˆì´í”„ ë ˆë²¨ì— ë”°ë¥¸ ì¶”ì²œ
            if fitting_params.drape_level > 0.6:
                recommendations.append("High drape creates a graceful silhouette")
            else:
                recommendations.append("Low drape maintains structured appearance")
            
            # ê¸°ë³¸ ì¶”ì²œì‚¬í•­
            if not recommendations:
                recommendations = [
                    "Great choice for this style!",
                    "Try different poses for variety",
                    "Consider complementary accessories"
                ]
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¶”ì²œì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            recommendations = ["Analysis complete - results look great!"]
        
        return recommendations
    
    def _pil_to_base64(self, pil_image: Image.Image) -> str:
        """PIL ì´ë¯¸ì§€ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜"""
        try:
            buffer = BytesIO()
            
            # í’ˆì§ˆ ì„¤ì •
            quality = 85
            if self.visualization_config['quality'] == "high":
                quality = 95
            elif self.visualization_config['quality'] == "low":
                quality = 70
            
            pil_image.save(buffer, format='JPEG', quality=quality, optimize=True)
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""
    
    # =================================================================
    # 7. í›„ì²˜ë¦¬ ë° ê²°ê³¼ êµ¬ì„±
    # =================================================================
    
    async def _post_process_result(
        self,
        fitted_tensor: torch.Tensor,
        metadata: Dict[str, Any],
        kwargs: Dict[str, Any]
    ) -> torch.Tensor:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        
        result = fitted_tensor.clone()
        
        try:
            # í’ˆì§ˆ í–¥ìƒ (ì„ íƒì )
            if kwargs.get('quality_enhancement', True):
                result = await self._enhance_tensor_quality(result)
            
            # ë°°ê²½ ë³´ì¡´ (ì„ íƒì )
            if kwargs.get('preserve_background', True):
                # ì›ë³¸ ë°°ê²½ê³¼ í•©ì„±ëœ ê²°ê³¼ ë¸”ë Œë”©
                pass  # êµ¬í˜„ ìƒëµ (ë³µì¡í•¨)
            
            # ìƒ‰ìƒ ë³´ì •
            result = self._color_correction_tensor(result, metadata)
            
            # ìµœì¢… í•„í„°ë§
            result = self._apply_final_filters_tensor(result, metadata)
            
            return result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return fitted_tensor
    
    async def _enhance_tensor_quality(self, tensor: torch.Tensor) -> torch.Tensor:
        """í…ì„œ í’ˆì§ˆ í–¥ìƒ"""
        
        try:
            result = tensor.clone()
            
            # ìƒ¤í”„ë‹ í•„í„°
            sharpen_kernel = torch.tensor([
                [[-1, -1, -1],
                 [-1,  9, -1],
                 [-1, -1, -1]]
            ], dtype=torch.float32, device=self.device).unsqueeze(0).unsqueeze(0)
            
            # ê° ì±„ë„ì— ëŒ€í•´ ì»¨ë³¼ë£¨ì…˜ ì ìš©
            if result.dim() == 4:  # [B, C, H, W]
                for c in range(result.shape[1]):
                    channel = result[:, c:c+1, :, :]
                    sharpened = F.conv2d(channel, sharpen_kernel, padding=1)
                    result[:, c:c+1, :, :] = 0.7 * channel + 0.3 * sharpened
            
            # ê°’ ë²”ìœ„ ì œí•œ
            result = torch.clamp(result, 0, 1)
            
            return result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return tensor
    
    def _color_correction_tensor(self, tensor: torch.Tensor, metadata: Dict[str, Any]) -> torch.Tensor:
        """í…ì„œ ìƒ‰ìƒ ë³´ì •"""
        
        try:
            fabric_type = metadata.get('fabric_type', 'cotton')
            result = tensor.clone()
            
            # ì²œ ì¬ì§ˆë³„ ìƒ‰ìƒ ì¡°ì •
            if fabric_type == 'silk':
                # ì‹¤í¬: ì±„ë„ ì¦ê°€
                # HSV ë³€í™˜ì€ ë³µì¡í•˜ë¯€ë¡œ ê°„ë‹¨í•œ RGB ì¡°ì •
                result[:, 1, :, :] *= 1.1  # ë…¹ìƒ‰ ì±„ë„ ì¦ê°€ (ì±„ë„ íš¨ê³¼)
            elif fabric_type == 'denim':
                # ë°ë‹˜: íŒŒë€ìƒ‰ í†¤ ê°•í™”
                result[:, 2, :, :] *= 1.1  # íŒŒë€ìƒ‰ ì±„ë„ ê°•í™”
            elif fabric_type == 'leather':
                # ê°€ì£½: ê°ˆìƒ‰ í†¤ ê°•í™”
                result[:, 0, :, :] *= 1.05  # ë¹¨ê°„ìƒ‰ ì±„ë„ ì•½ê°„ ì¦ê°€
            
            # ê°’ ë²”ìœ„ ì œí•œ
            result = torch.clamp(result, 0, 1)
            
            return result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
            return tensor
    
    def _apply_final_filters_tensor(self, tensor: torch.Tensor, metadata: Dict[str, Any]) -> torch.Tensor:
        """ìµœì¢… í•„í„° ì ìš© (í…ì„œ ë²„ì „)"""
        
        try:
            result = tensor.clone()
            
            # ì§ˆê° í–¥ìƒ
            fabric_props = metadata.get('fabric_properties')
            if fabric_props and fabric_props.shine > 0.3:
                # ê´‘íƒ ìˆëŠ” ì¬ì§ˆì— ëŒ€í•œ ì¶”ê°€ ì²˜ë¦¬
                # ê°„ë‹¨í•œ ë°ê¸° ì¦ê°€
                result = result * (1 + fabric_props.shine * 0.1)
            
            # ì „ì²´ì ì¸ ìƒ‰ì˜¨ë„ ì¡°ì •
            if self.config.get('warm_tone', False):
                result[:, 0, :, :] *= 1.02  # ë”°ëœ»í•œ í†¤ (ë¹¨ê°„ìƒ‰ ì¦ê°€)
                result[:, 2, :, :] *= 0.98  # íŒŒë€ìƒ‰ ê°ì†Œ
            
            # ê°’ ë²”ìœ„ ì œí•œ
            result = torch.clamp(result, 0, 1)
            
            return result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìµœì¢… í•„í„° ì ìš© ì‹¤íŒ¨: {e}")
            return tensor
    
    def _build_result_with_visualization(
        self,
        fitted_tensor: torch.Tensor,
        visualization_results: Dict[str, str],
        metadata: Dict[str, Any],
        processing_time: float,
        session_id: str
    ) -> Dict[str, Any]:
        """ì‹œê°í™”ê°€ í¬í•¨ëœ ìµœì¢… ê²°ê³¼ êµ¬ì„±"""
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = self._calculate_quality_score_tensor(fitted_tensor, metadata)
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence_score = self._calculate_confidence_score(metadata, processing_time)
        
        # í”¼íŒ… ì ìˆ˜ ê³„ì‚°
        fit_score = self._calculate_fit_score(metadata)
        
        # í…ì„œë¥¼ numpyë¡œ ë³€í™˜ (í˜¸í™˜ì„±)
        fitted_numpy = self._tensor_to_numpy(fitted_tensor)
        
        result = {
            "success": True,
            "session_id": session_id,
            "fitted_image": fitted_numpy,
            "processing_time": processing_time,
            
            # ğŸ†• API í˜¸í™˜ì„±ì„ ìœ„í•œ ì‹œê°í™” í•„ë“œë“¤
            "details": {
                # í”„ë¡ íŠ¸ì—”ë“œìš© ì‹œê°í™” ì´ë¯¸ì§€ë“¤
                "result_image": visualization_results["result_image"],      # ë©”ì¸ ê²°ê³¼
                "overlay_image": visualization_results["overlay_image"],    # ì˜¤ë²„ë ˆì´
                "comparison_image": visualization_results["comparison_image"], # ë¹„êµ ì´ë¯¸ì§€
                "process_analysis": visualization_results["process_analysis"], # ê³¼ì • ë¶„ì„
                "fit_analysis": visualization_results["fit_analysis"],     # í”¼íŒ… ë¶„ì„
                
                # ê¸°ì¡´ ë°ì´í„°ë“¤
                "quality_score": quality_score,
                "confidence_score": confidence_score,
                "fit_score": fit_score,
                "overall_score": (quality_score + confidence_score + fit_score) / 3,
                
                # ë©”íƒ€ë°ì´í„°
                "fabric_type": metadata.get('fabric_type'),
                "clothing_type": metadata.get('clothing_type'),
                "fitting_method": self.fitting_method,
                "quality_level": self.quality_level,
                
                # ì‹œìŠ¤í…œ ì •ë³´
                "step_info": {
                    "step_name": "virtual_fitting",
                    "step_number": 6,
                    "model_used": self._get_active_model_name(),
                    "device": self.device,
                    "optimization": "M3 Max" if self.device == 'mps' else self.device
                },
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­
                "quality_metrics": {
                    "fit_accuracy": float(fit_score),
                    "visual_quality": float(quality_score),
                    "processing_confidence": float(confidence_score),
                    "visualization_enabled": self.visualization_config['enabled']
                }
            },
            
            # ì ìˆ˜ë“¤ (ìµœìƒìœ„ ë ˆë²¨ í˜¸í™˜ì„±)
            "quality_score": quality_score,
            "confidence_score": confidence_score, 
            "fit_score": fit_score,
            "overall_score": (quality_score + confidence_score + fit_score) / 3,
            
            # ì„±ëŠ¥ ì •ë³´
            "performance_info": {
                "device": self.device,
                "memory_usage_mb": self._get_current_memory_usage(),
                "processing_method": self.fitting_config['method'].value,
                "cache_used": session_id in self.fitting_cache,
                "ai_models_used": [name for name, model in self.ai_models.items() if model is not None]
            },
            
            # ê°œì„  ì œì•ˆ
            "recommendations": self._generate_recommendations(metadata, quality_score)
        }
        
        return result
    
    def _tensor_to_numpy(self, tensor: torch.Tensor) -> np.ndarray:
        """í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜"""
        try:
            # [B, C, H, W] -> [H, W, C]
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)  # [C, H, W]
            
            tensor = tensor.permute(1, 2, 0)  # [H, W, C]
            tensor = tensor.cpu().detach()
            
            # 0-1 ë²”ìœ„ë¥¼ 0-255ë¡œ ë³€í™˜
            if tensor.max() <= 1.0:
                tensor = tensor * 255
            
            numpy_array = tensor.numpy().astype(np.uint8)
            return numpy_array
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í…ì„œ->numpy ë³€í™˜ ì‹¤íŒ¨: {e}")
            return np.zeros((512, 512, 3), dtype=np.uint8)
    
    def _calculate_quality_score_tensor(self, tensor: torch.Tensor, metadata: Dict[str, Any]) -> float:
        """í…ì„œ ê¸°ë°˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        
        try:
            scores = []
            
            # CPUë¡œ ì´ë™í•˜ì—¬ ê³„ì‚°
            tensor_cpu = tensor.cpu().detach()
            
            # 1. ì´ë¯¸ì§€ ì„ ëª…ë„ (ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°)
            if tensor_cpu.dim() == 4:
                gray = tensor_cpu.mean(dim=1, keepdim=True)  # RGB -> Grayscale
            else:
                gray = tensor_cpu.mean(dim=0, keepdim=True)
            
            # ë¼í”Œë¼ì‹œì•ˆ í•„í„°
            laplacian_kernel = torch.tensor([[0, -1, 0], [-1, 4, -1], [0, -1, 0]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
            sharpness = F.conv2d(gray, laplacian_kernel, padding=1)
            sharpness_score = min(1.0, torch.var(sharpness).item() / 1000.0)
            scores.append(sharpness_score)
            
            # 2. ìƒ‰ìƒ ë¶„í¬
            color_variance = torch.var(tensor_cpu, dim=(2, 3)).mean().item()
            color_score = min(1.0, color_variance * 5000.0)  # ì ì ˆí•œ ìŠ¤ì¼€ì¼ë§
            scores.append(color_score)
            
            # 3. ëŒ€ë¹„
            contrast = tensor_cpu.max().item() - tensor_cpu.min().item()
            contrast_score = min(1.0, contrast)
            scores.append(contrast_score)
            
            # 4. ë…¸ì´ì¦ˆ ë ˆë²¨ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            noise_level = torch.std(tensor_cpu).item()
            noise_score = max(0.0, 1.0 - noise_level / 0.2)  # 0.2ë¥¼ ìµœëŒ€ ë…¸ì´ì¦ˆë¡œ ê°€ì •
            scores.append(noise_score)
            
            return float(np.mean(scores))
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.7  # ê¸°ë³¸ê°’
    
    def _calculate_confidence_score(self, metadata: Dict[str, Any], processing_time: float) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        
        try:
            scores = []
            
            # 1. AI ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
            ai_models_used = sum(1 for model in self.ai_models.values() if model is not None)
            ai_score = ai_models_used / len(self.ai_models)
            scores.append(ai_score)
            
            # 2. ì²˜ë¦¬ ì‹œê°„ (ì ì ˆí•œ ì‹œê°„ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
            if processing_time < 2.0:
                time_score = 0.6  # ë„ˆë¬´ ë¹¨ë¼ë„ ì‹ ë¢°ë„ ë‚®ìŒ
            elif processing_time < 10.0:
                time_score = 1.0  # ì ì ˆí•œ ì‹œê°„
            else:
                time_score = max(0.3, 1.0 - (processing_time - 10.0) / 30.0)
            scores.append(time_score)
            
            # 3. ì…ë ¥ ë°ì´í„° í’ˆì§ˆ
            input_quality = 1.0
            if metadata.get('person_image_shape') and metadata.get('clothing_image_shape'):
                person_pixels = np.prod(metadata['person_image_shape'][2:])  # [B, C, H, W]
                clothing_pixels = np.prod(metadata['clothing_image_shape'][2:])
                min_pixels = min(person_pixels, clothing_pixels)
                input_quality = min(1.0, min_pixels / (512 * 512))
            scores.append(input_quality)
            
            return float(np.mean(scores))
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.7
    
    def _calculate_fit_score(self, metadata: Dict[str, Any]) -> float:
        """í”¼íŒ… ì ìˆ˜ ê³„ì‚°"""
        
        try:
            scores = []
            
            # 1. ì²œ ì¬ì§ˆê³¼ ì˜ë¥˜ íƒ€ì… í˜¸í™˜ì„±
            fabric_type = metadata.get('fabric_type', 'cotton')
            clothing_type = metadata.get('clothing_type', 'shirt')
            
            compatibility_matrix = {
                ('cotton', 'shirt'): 0.9,
                ('cotton', 'dress'): 0.8,
                ('silk', 'dress'): 0.95,
                ('silk', 'blouse'): 0.9,
                ('denim', 'pants'): 0.95,
                ('leather', 'jacket'): 0.9
            }
            
            compatibility_score = compatibility_matrix.get(
                (fabric_type, clothing_type), 0.7
            )
            scores.append(compatibility_score)
            
            # 2. ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ í’ˆì§ˆ
            physics_score = 0.9 if self.fitting_config['physics_enabled'] else 0.6
            scores.append(physics_score)
            
            # 3. í•´ìƒë„ ì ìˆ˜
            max_res = self.performance_config['max_resolution']
            resolution_score = min(1.0, max_res / 512.0)
            scores.append(resolution_score)
            
            return float(np.mean(scores))
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í”¼íŒ… ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.7
    
    def _get_active_model_name(self) -> str:
        """í˜„ì¬ í™œì„± ëª¨ë¸ ì´ë¦„ ë°˜í™˜"""
        active_models = []
        for name, model in self.ai_models.items():
            if model is not None:
                active_models.append(name)
        
        if active_models:
            return ", ".join(active_models)
        else:
            return "physics_based"  # ë¬¼ë¦¬ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜
    
    def _generate_recommendations(
        self, 
        metadata: Dict[str, Any], 
        quality_score: float
    ) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        
        recommendations = []
        
        try:
            # í’ˆì§ˆì´ ë‚®ì€ ê²½ìš°
            if quality_score < 0.6:
                recommendations.append("ë” ë†’ì€ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”")
                recommendations.append("ì¡°ëª…ì´ ì¢‹ì€ í™˜ê²½ì—ì„œ ì´¬ì˜ëœ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”")
            
            # AI ëª¨ë¸ ë¯¸ì‚¬ìš© ì‹œ
            if not self.fitting_config['ai_models_enabled']:
                recommendations.append("AI ëª¨ë¸ì„ í™œì„±í™”í•˜ë©´ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            
            # ë¬¼ë¦¬ ì—”ì§„ ë¯¸ì‚¬ìš© ì‹œ
            if not self.fitting_config['physics_enabled']:
                recommendations.append("ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ì„ í™œì„±í™”í•˜ë©´ ë” ìì—°ìŠ¤ëŸ¬ìš´ í”¼íŒ…ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            
            # ì²œ ì¬ì§ˆë³„ ì œì•ˆ
            fabric_type = metadata.get('fabric_type')
            if fabric_type == 'silk':
                recommendations.append("ì‹¤í¬ ì†Œì¬ì˜ íŠ¹ì„±ìƒ ë“œë ˆì´í•‘ íš¨ê³¼ë¥¼ ë†’ì—¬ë³´ì„¸ìš”")
            elif fabric_type == 'denim':
                recommendations.append("ë°ë‹˜ì˜ ê²¬ê³ í•¨ì„ í‘œí˜„í•˜ê¸° ìœ„í•´ í…ìŠ¤ì²˜ë¥¼ ê°•í™”í•´ë³´ì„¸ìš”")
            
            # ê¸°ë³¸ ì œì•ˆ
            if not recommendations:
                recommendations = [
                    "í›Œë¥­í•œ ê°€ìƒ í”¼íŒ… ê²°ê³¼ì…ë‹ˆë‹¤!",
                    "ë‹¤ì–‘í•œ í¬ì¦ˆë¡œ ì‹œë„í•´ë³´ì„¸ìš”",
                    "ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ì˜ ì˜ë¥˜ë„ ì²´í—˜í•´ë³´ì„¸ìš”"
                ]
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            recommendations = ["ê°€ìƒ í”¼íŒ…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"]
        
        return recommendations[:3]  # ìµœëŒ€ 3ê°œ ì œì•ˆ
    
    def _create_fallback_result(self, processing_time: float, session_id: str, error_msg: str) -> Dict[str, Any]:
        """í´ë°± ê²°ê³¼ ìƒì„± (ì—ëŸ¬ ë°œìƒ ì‹œ)"""
        return {
            "success": False,
            "session_id": session_id,
            "error_message": error_msg,
            "processing_time": processing_time,
            "fitted_image": None,
            "details": {
                "result_image": "",     # ë¹ˆ ì´ë¯¸ì§€
                "overlay_image": "",
                "comparison_image": "",
                "process_analysis": "",
                "fit_analysis": "",
                
                "quality_score": 0.0,
                "confidence_score": 0.0,
                "fit_score": 0.0,
                "overall_score": 0.0,
                
                "error": error_msg,
                "step_info": {
                    "step_name": "virtual_fitting",
                    "step_number": 6,
                    "model_used": "fallback",
                    "device": self.device,
                    "error": error_msg
                },
                
                "quality_metrics": {
                    "fit_accuracy": 0.0,
                    "visual_quality": 0.0,
                    "processing_confidence": 0.0,
                    "visualization_enabled": False
                }
            },
            "quality_score": 0.0,
            "confidence_score": 0.0,
            "fit_score": 0.0,
            "overall_score": 0.0,
            "performance_info": {
                "device": self.device,
                "error": error_msg
            },
            "recommendations": ["ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”."]
        }
    
    # =================================================================
    # 8. ìºì‹œ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
    # =================================================================
    
    def _generate_cache_key(
        self, 
        person_tensor: torch.Tensor, 
        clothing_tensor: torch.Tensor, 
        kwargs: Dict[str, Any]
    ) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        
        try:
            # í…ì„œ í•´ì‹œ
            person_hash = hash(person_tensor.cpu().numpy().tobytes())
            clothing_hash = hash(clothing_tensor.cpu().numpy().tobytes())
            
            # ì„¤ì • í•´ì‹œ
            config_str = json.dumps({
                'fabric_type': kwargs.get('fabric_type', 'cotton'),
                'clothing_type': kwargs.get('clothing_type', 'shirt'),
                'quality_level': self.quality_level,
                'fitting_method': self.fitting_method
            }, sort_keys=True)
            config_hash = hash(config_str)
            
            return f"vf_{person_hash}_{clothing_hash}_{config_hash}"
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ í‚¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"vf_{uuid.uuid4().hex[:16]}"
    
    def _get_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        
        if not self.cache_config['enabled']:
            return None
        
        try:
            with self.cache_lock:
                if cache_key in self.fitting_cache:
                    cached_data = self.fitting_cache[cache_key]
                    
                    # TTL í™•ì¸
                    if time.time() - cached_data['timestamp'] < self.cache_config['ttl_seconds']:
                        self.cache_access_times[cache_key] = time.time()
                        return cached_data['result']
                    else:
                        # ë§Œë£Œëœ ìºì‹œ ì œê±°
                        del self.fitting_cache[cache_key]
                        if cache_key in self.cache_access_times:
                            del self.cache_access_times[cache_key]
            
            return None
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _save_to_cache(self, cache_key: str, result: Dict[str, Any]):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        
        if not self.cache_config['enabled']:
            return
        
        try:
            with self.cache_lock:
                # ìºì‹œ í¬ê¸° ì œí•œ
                if len(self.fitting_cache) >= self.cache_max_size:
                    self._cleanup_cache()
                
                # ê²°ê³¼ ì €ì¥ (fitted_imageëŠ” ì œì™¸í•˜ê³  ë©”íƒ€ë°ì´í„°ë§Œ)
                cache_data = {
                    'timestamp': time.time(),
                    'result': {
                        k: v for k, v in result.items() 
                        if k != 'fitted_image'  # ì´ë¯¸ì§€ëŠ” ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìºì‹œí•˜ì§€ ì•ŠìŒ
                    }
                }
                
                self.fitting_cache[cache_key] = cache_data
                self.cache_access_times[cache_key] = time.time()
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _cleanup_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        
        try:
            # LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            if self.cache_access_times:
                sorted_items = sorted(
                    self.cache_access_times.items(),
                    key=lambda x: x[1]
                )
                
                # ì˜¤ë˜ëœ 25% ì œê±°
                remove_count = len(sorted_items) // 4
                for cache_key, _ in sorted_items[:remove_count]:
                    if cache_key in self.fitting_cache:
                        del self.fitting_cache[cache_key]
                    del self.cache_access_times[cache_key]
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _get_current_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        
        try:
            if self.memory_manager:
                return self.memory_manager.get_memory_usage()
            
            # í´ë°±: psutil ì‚¬ìš©
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / (1024 * 1024)
            
        except Exception:
            return 0.0
    
    def _update_stats(self, processing_time: float, success: bool):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        
        self.performance_stats['total_processed'] += 1
        
        if success:
            self.performance_stats['successful_fittings'] += 1
        else:
            self.performance_stats['failed_fittings'] += 1
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        total_time = (self.performance_stats['average_processing_time'] * 
                     (self.performance_stats['total_processed'] - 1) + processing_time)
        self.performance_stats['average_processing_time'] = total_time / self.performance_stats['total_processed']
        
        # ë©”ëª¨ë¦¬ í”¼í¬ ì—…ë°ì´íŠ¸
        current_memory = self._get_current_memory_usage()
        if current_memory > self.performance_stats['memory_peak_mb']:
            self.performance_stats['memory_peak_mb'] = current_memory
    
    # =================================================================
    # 9. ì •ë³´ ì¡°íšŒ ë° ê´€ë¦¬ í•¨ìˆ˜ë“¤
    # =================================================================
    
    async def get_step_info(self) -> Dict[str, Any]:
        """ğŸ” 6ë‹¨ê³„ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        try:
            memory_stats = await self.memory_manager.get_usage_stats()
        except:
            memory_stats = {"memory_used": "N/A"}
        
        return {
            "step_name": "virtual_fitting",
            "step_number": 6,
            "version": "6.0-complete-with-visualization",
            "device": self.device,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_enabled": self.optimization_enabled,
            "quality_level": self.quality_level,
            "fitting_method": self.fitting_method,
            "initialized": self.is_initialized,
            "session_id": self.session_id,
            
            # êµ¬ì„± ì •ë³´
            "config": {
                "ai_models_enabled": self.fitting_config['ai_models_enabled'],
                "physics_enabled": self.fitting_config['physics_enabled'],
                "visualization_enabled": self.fitting_config['visualization_enabled'],
                "max_resolution": self.performance_config['max_resolution'],
                "cache_enabled": self.cache_config['enabled'],
                "cache_size": len(self.fitting_cache)
            },
            
            # ğŸ†• ì‹œê°í™” ì„¤ì •
            "visualization_config": self.visualization_config,
            
            # ì„±ëŠ¥ í†µê³„
            "performance_stats": self.performance_stats.copy(),
            
            # ê¸°ëŠ¥ ì§€ì›
            "capabilities": {
                "ai_neural_fitting": DIFFUSERS_AVAILABLE and self.ai_models['diffusion_pipeline'] is not None,
                "physics_based_fitting": self.fitting_config['physics_enabled'],
                "hybrid_fitting": True,
                "template_matching": True,
                "texture_preservation": True,
                "lighting_effects": True,
                "fabric_simulation": True,
                "wrinkle_simulation": True,
                "m3_max_optimization": self.is_m3_max,
                "visualization_generation": self.fitting_config['visualization_enabled']
            },
            
            # ì§€ì› í˜•ì‹
            "supported_formats": {
                "fabric_types": list(FABRIC_PROPERTIES.keys()),
                "clothing_types": list(CLOTHING_FITTING_PARAMS.keys()),
                "quality_levels": [q.value for q in FittingQuality],
                "fitting_methods": [m.value for m in FittingMethod]
            },
            
            # ì˜ì¡´ì„± ìƒíƒœ
            "dependencies": {
                "torch": TORCH_AVAILABLE,
                "opencv": CV2_AVAILABLE,
                "scipy": SCIPY_AVAILABLE,
                "sklearn": SKLEARN_AVAILABLE,
                "skimage": SKIMAGE_AVAILABLE,
                "diffusers": DIFFUSERS_AVAILABLE,
                "model_loader": MODEL_LOADER_AVAILABLE
            },
            
            # AI ëª¨ë¸ ìƒíƒœ
            "ai_models_status": {
                name: model is not None 
                for name, model in self.ai_models.items()
            },
            
            "memory_usage": memory_stats,
            "optimization": {
                "m3_max_enabled": self.device == 'mps',
                "neural_engine": self.fitting_config.get('enable_neural_engine', True),
                "memory_efficient": self.performance_config['memory_efficient'],
                "parallel_processing": self.performance_config['parallel_processing']
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        
        try:
            self.logger.info(f"ğŸ§¹ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
            
            # AI ëª¨ë¸ ì–¸ë¡œë“œ
            for name, model in self.ai_models.items():
                if model is not None:
                    if hasattr(model, 'to'):
                        model.to('cpu')
                    del model
                    self.ai_models[name] = None
            
            # ìºì‹œ ì •ë¦¬
            with self.cache_lock:
                self.fitting_cache.clear()
                self.cache_access_times.clear()
            
            # ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ
            if hasattr(self, 'thread_pool'):
                self.thread_pool.shutdown(wait=True)
            
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True)
            
            # ModelLoader ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            if hasattr(self, 'model_interface') and self.model_interface:
                try:
                    await self.model_interface.cleanup()
                except:
                    pass
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE and self.device in ['cuda', 'mps']:
                if self.device == 'cuda':
                    torch.cuda.empty_cache()
                elif self.device == 'mps':
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            await self.memory_manager.cleanup()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.is_initialized = False
            self.logger.info(f"âœ… {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# =================================================================
# 10. í¸ì˜ í•¨ìˆ˜ë“¤ (í•˜ìœ„ í˜¸í™˜ì„±)
# =================================================================

def create_virtual_fitting_step(
    device: str = "auto", 
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> VirtualFittingStep:
    """ê°€ìƒ í”¼íŒ… ë‹¨ê³„ ìƒì„± (ê¸°ì¡´ ë°©ì‹ í˜¸í™˜)"""
    return VirtualFittingStep(device=device, config=config, **kwargs)

def create_m3_max_virtual_fitting_step(
    memory_gb: float = 128.0,
    quality_level: str = "ultra",
    enable_visualization: bool = True,
    **kwargs
) -> VirtualFittingStep:
    """M3 Max ìµœì í™” ê°€ìƒ í”¼íŒ… ë‹¨ê³„ ìƒì„±"""
    return VirtualFittingStep(
        device=None,  # ìë™ ê°ì§€
        memory_gb=memory_gb,
        quality_level=quality_level,
        is_m3_max=True,
        optimization_enabled=True,
        enable_visualization=enable_visualization,
        **kwargs
    )

async def quick_virtual_fitting_with_visualization(
    person_image: Union[np.ndarray, Image.Image, str, torch.Tensor],
    clothing_image: Union[np.ndarray, Image.Image, str, torch.Tensor],
    fabric_type: str = "cotton",
    clothing_type: str = "shirt",
    enable_visualization: bool = True,
    **kwargs
) -> Dict[str, Any]:
    """ë¹ ë¥¸ ê°€ìƒ í”¼íŒ… + ì‹œê°í™” (ì¼íšŒì„± ì‚¬ìš©)"""
    
    step = VirtualFittingStep(enable_visualization=enable_visualization)
    try:
        await step.initialize()
        result = await step.process(
            person_image, clothing_image,
            fabric_type=fabric_type,
            clothing_type=clothing_type,
            **kwargs
        )
        return result
    finally:
        await step.cleanup()

# =================================================================
# 11. ëª¨ë“ˆ ì •ë³´
# =================================================================

__version__ = "6.0.0-visualization"
__author__ = "MyCloset AI Team"
__description__ = "Complete Virtual Fitting Implementation with AI Models, Physics Simulation, and Advanced Visualization"

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    async def test_virtual_fitting_with_visualization():
        """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
        import asyncio
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
        test_person = torch.randn(1, 3, 512, 512)
        test_clothing = torch.randn(1, 3, 512, 512)
        
        # ê°€ìƒ í”¼íŒ… ì‹¤í–‰
        step = VirtualFittingStep(
            quality_level="balanced",
            enable_visualization=True,
            fitting_method="physics_based"
        )
        await step.initialize()
        
        result = await step.process(
            test_person, test_clothing,
            fabric_type="cotton",
            clothing_type="shirt"
        )
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {result['success']}")
        print(f"   ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
        print(f"   í’ˆì§ˆ ì ìˆ˜: {result['quality_score']:.2f}")
        print(f"   ì‹œê°í™” ì´ë¯¸ì§€ ê°œìˆ˜: {len([k for k, v in result['details'].items() if 'image' in k and v])}")
        
        await step.cleanup()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_virtual_fitting_with_visualization())