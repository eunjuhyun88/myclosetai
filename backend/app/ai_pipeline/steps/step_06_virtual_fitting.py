"""
Step 06: Virtual Fitting - ê°€ìƒ í”¼íŒ… ì‹¤í–‰
ê¸°ì¡´ êµ¬ì¡°ì— ë§ëŠ” VirtualFittingStep í´ë˜ìŠ¤ì™€ paste.txtì˜ RealVirtualFittingStep í†µí•©
"""
import os
import time
import asyncio
import logging
from typing import Dict, Any, Optional, Union, List, Tuple
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
from PIL import Image, ImageEnhance, ImageFilter
import mediapipe as mp
from scipy.spatial.distance import cdist
from scipy.interpolate import Rbf
import base64
import io

try:
    # ê¸°ì¡´ app êµ¬ì¡° import
    from app.core.config import get_settings
    from app.core.logging_config import setup_logging
    from app.utils.image_utils import save_temp_image, load_image
    from app.ai_pipeline.utils.memory_manager import optimize_memory_usage
except ImportError as e:
    logging.warning(f"ì¼ë¶€ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    # í´ë°± ì„¤ì •
    class MockSettings:
        UPLOAD_DIR = "uploads"
        RESULT_DIR = "results"
    
    def get_settings():
        return MockSettings()
    
    def setup_logging():
        pass
    
    def save_temp_image(image, filename):
        cv2.imwrite(filename, image)
        return filename
    
    def load_image(path):
        return cv2.imread(path)
    
    def optimize_memory_usage():
        import gc
        gc.collect()

logger = logging.getLogger(__name__)

class VirtualFittingStep:
    """
    ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ì™€ í˜¸í™˜ë˜ëŠ” VirtualFittingStep í´ë˜ìŠ¤
    paste.txtì˜ RealVirtualFittingStep ê¸°ëŠ¥ì„ í¬í•¨
    """
    
    def __init__(self, device: str = None, config: Dict[str, Any] = None):
        self.device = device or ('mps' if torch.backends.mps.is_available() else 'cpu')
        self.config = config or {}
        self.is_initialized = False
        
        # ë‚´ë¶€ì ìœ¼ë¡œ RealVirtualFittingStep ì‚¬ìš©
        self.real_fitter = RealVirtualFittingStep(device=self.device, config=self.config)
        
        logger.info(f"ğŸ¯ VirtualFittingStep ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
    
    async def initialize(self) -> bool:
        """ì´ˆê¸°í™”"""
        try:
            success = await self.real_fitter.initialize()
            self.is_initialized = success
            return success
        except Exception as e:
            logger.error(f"VirtualFittingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def process(
        self,
        person_image: Union[np.ndarray, str],
        clothing_image: Union[np.ndarray, str],
        **kwargs
    ) -> Dict[str, Any]:
        """
        ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ì™€ í˜¸í™˜ë˜ëŠ” ì²˜ë¦¬ ë©”ì„œë“œ
        """
        if not self.is_initialized:
            raise RuntimeError("VirtualFittingStepì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            # RealVirtualFittingStepì˜ process_virtual_fitting í˜¸ì¶œ
            result = await self.real_fitter.process_virtual_fitting(
                person_image=person_image,
                clothing_image=clothing_image,
                target_region=kwargs.get('target_region', 'upper'),
                user_preferences=kwargs.get('user_preferences', {})
            )
            
            return result
            
        except Exception as e:
            logger.error(f"VirtualFittingStep ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "device_used": self.device
            }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.real_fitter:
            await self.real_fitter.cleanup()
        self.is_initialized = False


class RealVirtualFittingStep:
    """
    ğŸ¯ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” 6ë‹¨ê³„ ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ (app êµ¬ì¡° í†µí•© ë²„ì „)
    
    ì§„ì§œ í†µí•© ë²„ì „:
    1. AI ëª¨ë¸ (HR-VITON ìŠ¤íƒ€ì¼) + ì „í†µì  í›„ì²˜ë¦¬ ê²°í•©
    2. MediaPipe ê¸°ë°˜ ì‹¤ì œ í¬ì¦ˆ ì¶”ì •
    3. ì‹¤ì œ TPS ë³€í™˜ êµ¬í˜„
    4. ì§„ì§œ ì´ë¯¸ì§€ í•©ì„± ì•Œê³ ë¦¬ì¦˜
    5. M3 Max MPS ìµœì í™”
    6. ê¸°ì¡´ app êµ¬ì¡°ì™€ ì™„ì „ í†µí•©
    """
    
    def __init__(self, device: str = None, config: Dict[str, Any] = None):
        # ê¸°ì¡´ ì„¤ì • ì‹œìŠ¤í…œ í™œìš©
        self.device = device or ('mps' if torch.backends.mps.is_available() else 'cpu')
        self.config = config or {}
        
        # ì‹¤ì œ ì»´í¬ë„ŒíŠ¸ë“¤
        self.pose_estimator = None
        self.segmentation_model = None
        self.tps_transformer = None
        self.neural_compositor = None
        self.quality_enhancer = None
        
        # MediaPipe ì´ˆê¸°í™”
        self.mp_pose = mp.solutions.pose
        self.mp_selfie_segmentation = mp.solutions.selfie_segmentation
        
        # MPS ìµœì í™” (M3 Max)
        self.use_mps = self.device == 'mps' and torch.backends.mps.is_available()
        
        self.is_initialized = False
        
        # ì„¤ì • ê°ì²´ ê°€ì ¸ì˜¤ê¸°
        try:
            self.settings = get_settings()
        except:
            self.settings = type('Settings', (), {
                'UPLOAD_DIR': 'uploads',
                'RESULT_DIR': 'results'
            })()
        
        logger.info(f"ğŸ¯ ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
    
    async def initialize(self) -> bool:
        """ì‹¤ì œ ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™” (ê¸°ì¡´ êµ¬ì¡° í™œìš©)"""
        try:
            logger.info("ğŸ”„ ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì»´í¬ë„ŒíŠ¸ ë¡œë”©...")
            
            # 1. MediaPipe í¬ì¦ˆ ì¶”ì •ê¸°
            self.pose_estimator = self.mp_pose.Pose(
                static_image_mode=True,
                model_complexity=2,
                enable_segmentation=True,
                min_detection_confidence=0.5
            )
            
            # 2. MediaPipe ì„¸ê·¸ë©˜í…Œì´ì…˜
            self.segmentation_model = self.mp_selfie_segmentation.SelfieSegmentation(
                model_selection=1
            )
            
            # 3. TPS ë³€í™˜ê¸° (ì‹¤ì œ êµ¬í˜„)
            self.tps_transformer = RealTPSTransformer(device=self.device)
            
            # 4. ì‹ ê²½ë§ í•©ì„±ê¸°
            self.neural_compositor = NeuralCompositor(device=self.device)
            await self.neural_compositor.initialize()
            
            # 5. í’ˆì§ˆ í–¥ìƒê¸°
            self.quality_enhancer = RealQualityEnhancer(device=self.device)
            
            # ë©”ëª¨ë¦¬ ìµœì í™” (ê¸°ì¡´ ìœ í‹¸ í™œìš©)
            optimize_memory_usage()
            
            self.is_initialized = True
            logger.info("âœ… ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def process_virtual_fitting(
        self,
        person_image: Union[np.ndarray, torch.Tensor, Image.Image, str],
        clothing_image: Union[np.ndarray, torch.Tensor, Image.Image, str],
        target_region: str = 'upper',  # 'upper', 'lower', 'full'
        user_preferences: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (API ì—°ê²°ìš©)
        
        Args:
            person_image: ì‚¬ìš©ì ì´ë¯¸ì§€ (íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ì´ë¯¸ì§€ ê°ì²´)
            clothing_image: ì˜· ì´ë¯¸ì§€ (íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ì´ë¯¸ì§€ ê°ì²´)
            target_region: ì°©ìš©í•  ì‹ ì²´ ë¶€ìœ„
            user_preferences: ì‚¬ìš©ì ì„¤ì • (í‚¤, ëª¸ë¬´ê²Œ ë“±)
        
        Returns:
            API í˜¸í™˜ í”¼íŒ… ê²°ê³¼
        """
        if not self.is_initialized:
            raise RuntimeError("ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_time = time.time()
        
        try:
            # === 0. ì…ë ¥ ì „ì²˜ë¦¬ (ê¸°ì¡´ utils í™œìš©) ===
            person_np = await self._load_and_preprocess_image(person_image)
            clothing_np = await self._load_and_preprocess_image(clothing_image)
            
            logger.info("ğŸ¨ 1ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • ë° ì¸ì²´ íŒŒì‹±")
            # === 1. ì‹¤ì œ í¬ì¦ˆ ì¶”ì • (MediaPipe) ===
            pose_result = await self._extract_pose_and_segmentation(person_np)
            
            logger.info("âœ‚ï¸ 2ë‹¨ê³„: ì˜ë¥˜ ë¶„í•  ë° ì „ì²˜ë¦¬")
            # === 2. ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ===
            clothing_result = await self._segment_clothing(clothing_np)
            
            logger.info("ğŸ“ 3ë‹¨ê³„: ê¸°í•˜í•™ì  ë§¤ì¹­ (TPS)")
            # === 3. TPS ê¸°ë°˜ ì˜· ë³€í˜• ===
            warping_result = await self._warp_clothing_to_body(
                clothing_result, pose_result, target_region
            )
            
            logger.info("ğŸ¤– 4ë‹¨ê³„: ì‹ ê²½ë§ ê¸°ë°˜ í•©ì„±")
            # === 4. ì‹ ê²½ë§ í•©ì„± ===
            neural_result = await self._neural_composition(
                person_np, warping_result, pose_result
            )
            
            logger.info("âœ¨ 5ë‹¨ê³„: í’ˆì§ˆ í–¥ìƒ í›„ì²˜ë¦¬")
            # === 5. í’ˆì§ˆ í–¥ìƒ ===
            enhanced_result = await self._enhance_quality(
                neural_result, person_np, clothing_np, pose_result
            )
            
            logger.info("ğŸ“Š 6ë‹¨ê³„: í’ˆì§ˆ í‰ê°€")
            # === 6. ìµœì¢… í’ˆì§ˆ í‰ê°€ ===
            quality_metrics = await self._evaluate_final_quality(
                enhanced_result, person_np, clothing_np
            )
            
            processing_time = time.time() - start_time
            
            # === ê²°ê³¼ ì €ì¥ (ê¸°ì¡´ êµ¬ì¡° í™œìš©) ===
            result_path = await self._save_result_image(enhanced_result)
            
            # API í˜¸í™˜ ê²°ê³¼ êµ¬ì„±
            result = {
                "success": True,
                "fitted_image": enhanced_result,
                "fitted_image_pil": Image.fromarray(cv2.cvtColor(enhanced_result, cv2.COLOR_BGR2RGB)),
                "fitted_image_base64": self._image_to_base64(enhanced_result),
                "fitted_image_path": result_path,
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ (í”„ë¡ íŠ¸ì—”ë“œìš©)
                "quality_metrics": quality_metrics,
                "fit_score": quality_metrics.get('fit_score', 0.8),
                "realism_score": quality_metrics.get('realism_score', 0.8),
                "overall_quality": quality_metrics.get('overall_quality', 0.8),
                "confidence": quality_metrics.get('overall_quality', 0.8),  # API í˜¸í™˜
                
                # ì¶”ì²œ (AI ê¸°ë°˜)
                "recommendations": self._generate_recommendations(quality_metrics, user_preferences),
                
                # ì²˜ë¦¬ ì •ë³´
                "processing_info": {
                    "processing_time": processing_time,
                    "target_region": target_region,
                    "device_used": self.device,
                    "steps_completed": 6,
                    "optimization": "M3_Max_MPS" if self.use_mps else "CPU",
                    "model_versions": {
                        "mediapipe_pose": "v1.0",
                        "neural_compositor": "v1.0",
                        "tps_transformer": "v1.0"
                    }
                }
            }
            
            logger.info(f"âœ… ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì™„ë£Œ - ì‹œê°„: {processing_time:.2f}ì´ˆ, í’ˆì§ˆ: {quality_metrics.get('overall_quality', 0):.2f}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            processing_time = time.time() - start_time
            
            return {
                "success": False,
                "error": str(e),
                "processing_time": processing_time,
                "device_used": self.device
            }
    
    async def _load_and_preprocess_image(self, image_input: Union[str, np.ndarray, Image.Image]) -> np.ndarray:
        """ì´ë¯¸ì§€ ë¡œë”© ë° ì „ì²˜ë¦¬ (ê¸°ì¡´ utils í™œìš©)"""
        
        if isinstance(image_input, str):
            # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
            image = load_image(image_input)
        elif isinstance(image_input, Image.Image):
            image = cv2.cvtColor(np.array(image_input), cv2.COLOR_RGB2BGR)
        elif isinstance(image_input, np.ndarray):
            image = image_input
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_input)}")
        
        # í‘œì¤€ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ì„±ëŠ¥ ìµœì í™”)
        target_size = self.config.get('image_size', 512)
        if image.shape[:2] != (target_size, target_size):
            image = cv2.resize(image, (target_size, target_size))
        
        return image
    
    async def _save_result_image(self, image: np.ndarray) -> str:
        """ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ (ê¸°ì¡´ utils í™œìš©)"""
        timestamp = int(time.time())
        filename = f"fitted_result_{timestamp}.jpg"
        result_path = os.path.join(self.settings.RESULT_DIR, filename)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.settings.RESULT_DIR, exist_ok=True)
        
        # ì´ë¯¸ì§€ ì €ì¥
        cv2.imwrite(result_path, image)
        
        return result_path
    
    def _generate_recommendations(self, quality_metrics: Dict, user_preferences: Dict = None) -> List[str]:
        """AI ê¸°ë°˜ ì¶”ì²œ ìƒì„±"""
        recommendations = []
        
        fit_score = quality_metrics.get('fit_score', 0.5)
        realism_score = quality_metrics.get('realism_score', 0.5)
        
        if fit_score < 0.7:
            recommendations.append("ë” ì •í™•í•œ í•ì„ ìœ„í•´ ì •ë©´ì„ í–¥í•œ ì „ì‹  ì‚¬ì§„ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
        
        if realism_score < 0.7:
            recommendations.append("ë” ìì—°ìŠ¤ëŸ¬ìš´ ê²°ê³¼ë¥¼ ìœ„í•´ ì¡°ëª…ì´ ê· ì¼í•œ í™˜ê²½ì—ì„œ ì´¬ì˜í•´ë³´ì„¸ìš”.")
        
        if fit_score > 0.8 and realism_score > 0.8:
            recommendations.append("ì™„ë²½í•œ í•ì…ë‹ˆë‹¤! ì´ ìŠ¤íƒ€ì¼ì´ ë‹¹ì‹ ì—ê²Œ ì˜ ì–´ìš¸ë ¤ìš”.")
        
        # ì‚¬ìš©ì ë§ì¶¤ ì¶”ì²œ
        if user_preferences:
            height = user_preferences.get('height', 170)
            if height < 160:
                recommendations.append("í‚¤ê°€ ì‘ìœ¼ì‹  ë¶„ê»˜ëŠ” í•˜ì´ì›¨ì´ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ì„ ì¶”ì²œë“œë ¤ìš”.")
            elif height > 180:
                recommendations.append("í‚¤ê°€ í¬ì‹  ë¶„ê»˜ëŠ” ë¡± ì‹¤ë£¨ì—£ì´ ì˜ ì–´ìš¸ë ¤ìš”.")
        
        return recommendations[:3]  # ìµœëŒ€ 3ê°œ
    
    # === í•µì‹¬ ì²˜ë¦¬ ë©”ì„œë“œë“¤ ===
    
    async def _extract_pose_and_segmentation(self, person_image: np.ndarray) -> Dict[str, Any]:
        """ì‹¤ì œ í¬ì¦ˆ ì¶”ì • ë° ì¸ì²´ ë¶„í•  (MediaPipe)"""
        
        # RGBë¡œ ë³€í™˜
        image_rgb = cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB)
        
        # í¬ì¦ˆ ì¶”ì •
        pose_results = self.pose_estimator.process(image_rgb)
        
        # ì¸ì²´ ì„¸ê·¸ë©˜í…Œì´ì…˜
        seg_results = self.segmentation_model.process(image_rgb)
        
        if not pose_results.pose_landmarks:
            raise ValueError("í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì‹ ì´ ë³´ì´ëŠ” ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        
        # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        keypoints = []
        for landmark in pose_results.pose_landmarks.landmark:
            keypoints.append({
                'x': landmark.x * person_image.shape[1],
                'y': landmark.y * person_image.shape[0],
                'z': landmark.z,
                'visibility': landmark.visibility
            })
        
        # ì¸ì²´ ë§ˆìŠ¤í¬
        person_mask = (seg_results.segmentation_mask > 0.5).astype(np.uint8) * 255
        
        # ì‹ ì²´ ë¶€ìœ„ë³„ í‚¤í¬ì¸íŠ¸ ê·¸ë£¹í•‘
        body_regions = self._group_keypoints_by_region(keypoints)
        
        return {
            'keypoints': keypoints,
            'body_regions': body_regions,
            'person_mask': person_mask,
            'pose_landmarks': pose_results.pose_landmarks,
            'confidence': self._calculate_pose_confidence(keypoints)
        }
    
    def _group_keypoints_by_region(self, keypoints: List[Dict]) -> Dict[str, List[Dict]]:
        """í‚¤í¬ì¸íŠ¸ë¥¼ ì‹ ì²´ ë¶€ìœ„ë³„ë¡œ ê·¸ë£¹í•‘"""
        
        # MediaPipe í¬ì¦ˆ ëœë“œë§ˆí¬ ì¸ë±ìŠ¤
        regions = {
            'upper': [11, 12, 13, 14, 15, 16, 23, 24],  # ì–´ê¹¨, íŒ”, ì—‰ë©ì´
            'lower': [23, 24, 25, 26, 27, 28, 29, 30, 31, 32],  # ì—‰ë©ì´, ë‹¤ë¦¬
            'torso': [11, 12, 23, 24],  # ëª¸í†µ ì¤‘ì‹¬
            'arms': [11, 12, 13, 14, 15, 16],  # ì–‘íŒ”
            'legs': [23, 24, 25, 26, 27, 28, 29, 30, 31, 32]  # ì–‘ë‹¤ë¦¬
        }
        
        grouped = {}
        for region_name, indices in regions.items():
            grouped[region_name] = []
            for idx in indices:
                if idx < len(keypoints) and keypoints[idx]['visibility'] > 0.5:
                    grouped[region_name].append(keypoints[idx])
        
        return grouped
    
    def _calculate_pose_confidence(self, keypoints: List[Dict]) -> float:
        """í¬ì¦ˆ ê²€ì¶œ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not keypoints:
            return 0.0
        
        visibilities = [kp['visibility'] for kp in keypoints]
        return np.mean(visibilities)
    
    async def _segment_clothing(self, clothing_image: np.ndarray) -> Dict[str, Any]:
        """ì˜ë¥˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ê°„ë‹¨í•œ ë°°ê²½ ì œê±°)"""
        
        # HSV ìƒ‰ê³µê°„ ë³€í™˜
        hsv = cv2.cvtColor(clothing_image, cv2.COLOR_BGR2HSV)
        
        # ë°°ê²½ ì œê±° (ê°„ë‹¨í•œ ë°©ë²• - ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ëª¨ë¸ í•„ìš”)
        # ê°€ì¥ìë¦¬ëŠ” ë°°ê²½ì´ë¼ê³  ê°€ì •
        h, w = clothing_image.shape[:2]
        
        # ê°€ì¥ìë¦¬ í”½ì…€ë“¤ì˜ í‰ê·  ìƒ‰ìƒì„ ë°°ê²½ìƒ‰ìœ¼ë¡œ ì¶”ì •
        edge_pixels = np.concatenate([
            clothing_image[0, :].reshape(-1, 3),  # ìƒë‹¨
            clothing_image[-1, :].reshape(-1, 3),  # í•˜ë‹¨
            clothing_image[:, 0].reshape(-1, 3),  # ì¢Œì¸¡
            clothing_image[:, -1].reshape(-1, 3)  # ìš°ì¸¡
        ])
        
        bg_color_mean = np.mean(edge_pixels, axis=0)
        bg_color_std = np.std(edge_pixels, axis=0) + 10  # ì—¬ìœ ê°’
        
        # ë°°ê²½ìƒ‰ê³¼ ìœ ì‚¬í•œ í”½ì…€ ì°¾ê¸°
        diff = np.abs(clothing_image.astype(float) - bg_color_mean)
        bg_mask = np.all(diff < bg_color_std * 2, axis=2)
        
        # ì˜ë¥˜ ë§ˆìŠ¤í¬ (ë°°ê²½ì´ ì•„ë‹Œ ë¶€ë¶„)
        clothing_mask = (~bg_mask).astype(np.uint8) * 255
        
        # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        clothing_mask = cv2.morphologyEx(clothing_mask, cv2.MORPH_CLOSE, kernel)
        clothing_mask = cv2.morphologyEx(clothing_mask, cv2.MORPH_OPEN, kernel)
        
        # ê°€ì¥ í° ì˜ì—­ë§Œ ìœ ì§€
        contours, _ = cv2.findContours(clothing_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            largest_contour = max(contours, key=cv2.contourArea)
            clothing_mask = np.zeros_like(clothing_mask)
            cv2.fillPoly(clothing_mask, [largest_contour], 255)
        
        # ë°°ê²½ ì œê±°ëœ ì˜ë¥˜ ì´ë¯¸ì§€
        clothing_segmented = clothing_image.copy()
        clothing_segmented[clothing_mask == 0] = [255, 255, 255]  # ë°°ê²½ì„ í°ìƒ‰ìœ¼ë¡œ
        
        return {
            'segmented_image': clothing_segmented,
            'mask': clothing_mask,
            'original_image': clothing_image,
            'background_color': bg_color_mean
        }
    
    async def _warp_clothing_to_body(
        self,
        clothing_result: Dict[str, Any],
        pose_result: Dict[str, Any],
        target_region: str
    ) -> Dict[str, Any]:
        """ì‹¤ì œ TPS ë³€í™˜ìœ¼ë¡œ ì˜·ì„ ì‹ ì²´ì— ë§ê²Œ ë³€í˜•"""
        
        clothing_img = clothing_result['segmented_image']
        clothing_mask = clothing_result['mask']
        keypoints = pose_result['keypoints']
        body_regions = pose_result['body_regions']
        
        # íƒ€ê²Ÿ ì˜ì—­ì˜ í‚¤í¬ì¸íŠ¸ ì„ íƒ
        if target_region == 'upper':
            target_keypoints = body_regions.get('upper', [])
        elif target_region == 'lower':
            target_keypoints = body_regions.get('lower', [])
        else:
            target_keypoints = keypoints
        
        if len(target_keypoints) < 4:
            logger.warning("ë³€í˜•ì„ ìœ„í•œ í‚¤í¬ì¸íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ê¸°ë³¸ í¬ê¸° ì¡°ì •ë§Œ ì ìš©í•©ë‹ˆë‹¤.")
            return await self._simple_resize_clothing(clothing_result, pose_result, target_region)
        
        # TPS ë³€í™˜ ì ìš©
        warped_image, warped_mask = await self.tps_transformer.transform(
            clothing_img, clothing_mask, target_keypoints
        )
        
        return {
            'warped_image': warped_image,
            'warped_mask': warped_mask,
            'transform_keypoints': target_keypoints,
            'target_region': target_region
        }
    
    async def _simple_resize_clothing(
        self,
        clothing_result: Dict[str, Any],
        pose_result: Dict[str, Any], 
        target_region: str
    ) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ í¬ê¸° ì¡°ì • (TPS ë³€í™˜ ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ)"""
        
        clothing_img = clothing_result['segmented_image']
        clothing_mask = clothing_result['mask']
        keypoints = pose_result['keypoints']
        
        # ì‹ ì²´ í¬ê¸° ì¶”ì •
        scale_factor = 1.0
        
        # í¬ê¸° ì¡°ì •
        new_width = int(clothing_img.shape[1] * scale_factor)
        new_height = int(clothing_img.shape[0] * scale_factor)
        
        warped_image = cv2.resize(clothing_img, (new_width, new_height))
        warped_mask = cv2.resize(clothing_mask, (new_width, new_height))
        
        return {
            'warped_image': warped_image,
            'warped_mask': warped_mask,
            'scale_factor': scale_factor,
            'target_region': target_region
        }
    
    async def _neural_composition(
        self,
        person_image: np.ndarray,
        warping_result: Dict[str, Any],
        pose_result: Dict[str, Any]
    ) -> np.ndarray:
        """ì‹ ê²½ë§ ê¸°ë°˜ ì´ë¯¸ì§€ í•©ì„±"""
        
        warped_clothing = warping_result['warped_image']
        warped_mask = warping_result['warped_mask']
        person_mask = pose_result['person_mask']
        
        # í¬ê¸° ë§ì¶”ê¸°
        h, w = person_image.shape[:2]
        if warped_clothing.shape[:2] != (h, w):
            warped_clothing = cv2.resize(warped_clothing, (w, h))
            warped_mask = cv2.resize(warped_mask, (w, h))
        
        # ì‹ ê²½ë§ í•©ì„±ê¸°ê°€ ì—†ëŠ” ê²½ìš° ì „í†µì  ë¸”ë Œë”© ì‚¬ìš©
        if self.neural_compositor.model is None:
            return await self._traditional_blending(
                person_image, warped_clothing, warped_mask, person_mask
            )
        
        # ì‹ ê²½ë§ í•©ì„±
        try:
            person_tensor = self._numpy_to_tensor(person_image)
            clothing_tensor = self._numpy_to_tensor(warped_clothing)
            mask_tensor = self._numpy_to_tensor(warped_mask, is_mask=True)
            
            with torch.no_grad():
                composite_tensor = self.neural_compositor.compose(
                    person_tensor, clothing_tensor, mask_tensor
                )
            
            composite_np = self._tensor_to_numpy(composite_tensor)
            return composite_np
            
        except Exception as e:
            logger.warning(f"ì‹ ê²½ë§ í•©ì„± ì‹¤íŒ¨, ì „í†µì  ë¸”ë Œë”© ì‚¬ìš©: {e}")
            return await self._traditional_blending(
                person_image, warped_clothing, warped_mask, person_mask
            )
    
    async def _traditional_blending(
        self,
        person_image: np.ndarray,
        clothing_image: np.ndarray,
        clothing_mask: np.ndarray,
        person_mask: np.ndarray
    ) -> np.ndarray:
        """ì „í†µì  ì´ë¯¸ì§€ ë¸”ë Œë”©"""
        
        # ë§ˆìŠ¤í¬ ì •ê·œí™”
        clothing_mask_norm = clothing_mask.astype(np.float32) / 255.0
        person_mask_norm = person_mask.astype(np.float32) / 255.0
        
        # 3ì±„ë„ë¡œ í™•ì¥
        if len(clothing_mask_norm.shape) == 2:
            clothing_mask_norm = np.stack([clothing_mask_norm] * 3, axis=2)
        if len(person_mask_norm.shape) == 2:
            person_mask_norm = np.stack([person_mask_norm] * 3, axis=2)
        
        # ì˜ë¥˜ê°€ ì ìš©ë  ì˜ì—­ ê³„ì‚° (ì¸ì²´ ì˜ì—­ ë‚´ì—ì„œ)
        blend_mask = clothing_mask_norm * person_mask_norm
        
        # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ê²½ê³„ ë¶€ë“œëŸ½ê²Œ
        for i in range(3):
            blend_mask[:, :, i] = cv2.GaussianBlur(blend_mask[:, :, i], (15, 15), 5)
        
        # í¬ì•„ì†¡ ë¸”ë Œë”© (ë” ìì—°ìŠ¤ëŸ¬ìš´ í•©ì„±)
        try:
            # ì˜·ì´ ë“¤ì–´ê°ˆ ì˜ì—­ì˜ ì¤‘ì‹¬ì  ì°¾ê¸°
            mask_coords = np.where(blend_mask[:, :, 0] > 0.5)
            if len(mask_coords[0]) > 0:
                center_y = int(np.mean(mask_coords[0]))
                center_x = int(np.mean(mask_coords[1]))
                center = (center_x, center_y)
                
                # í¬ì•„ì†¡ ë¸”ë Œë”© ì ìš©
                blended = cv2.seamlessClone(
                    clothing_image, person_image, 
                    (blend_mask[:, :, 0] * 255).astype(np.uint8),
                    center, cv2.NORMAL_CLONE
                )
            else:
                # í¬ì•„ì†¡ ë¸”ë Œë”© ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ë¸”ë Œë”©
                blended = person_image.astype(np.float32) * (1 - blend_mask) + \
                         clothing_image.astype(np.float32) * blend_mask
                blended = np.clip(blended, 0, 255).astype(np.uint8)
                
        except Exception as e:
            logger.warning(f"í¬ì•„ì†¡ ë¸”ë Œë”© ì‹¤íŒ¨, ì•ŒíŒŒ ë¸”ë Œë”© ì‚¬ìš©: {e}")
            # ì¼ë°˜ ì•ŒíŒŒ ë¸”ë Œë”©
            blended = person_image.astype(np.float32) * (1 - blend_mask) + \
                     clothing_image.astype(np.float32) * blend_mask
            blended = np.clip(blended, 0, 255).astype(np.uint8)
        
        return blended
    
    async def _enhance_quality(
        self,
        composite_image: np.ndarray,
        person_image: np.ndarray,
        clothing_image: np.ndarray,
        pose_result: Dict[str, Any]
    ) -> np.ndarray:
        """í’ˆì§ˆ í–¥ìƒ í›„ì²˜ë¦¬"""
        
        enhanced = composite_image.copy()
        
        # 1. ìƒ‰ìƒ ë³´ì •
        enhanced = self._color_correction(enhanced, person_image)
        
        # 2. ë””í…Œì¼ í–¥ìƒ
        enhanced = self._enhance_details(enhanced)
        
        # 3. ì¡°ëª… ì¼ì¹˜
        enhanced = self._match_lighting(enhanced, person_image)
        
        # 4. ê²½ê³„ì„  ë¶€ë“œëŸ½ê²Œ
        enhanced = self._smooth_edges(enhanced, composite_image)
        
        return enhanced
    
    def _color_correction(self, image: np.ndarray, reference: np.ndarray) -> np.ndarray:
        """ìƒ‰ìƒ ë³´ì •"""
        
        # LAB ìƒ‰ê³µê°„ì—ì„œ ìƒ‰ìƒ ë§¤ì¹­
        lab_img = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        lab_ref = cv2.cvtColor(reference, cv2.COLOR_BGR2LAB)
        
        # ê° ì±„ë„ë³„ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
        for i in range(3):
            img_channel = lab_img[:, :, i].astype(np.float32)
            ref_channel = lab_ref[:, :, i].astype(np.float32)
            
            img_mean, img_std = cv2.meanStdDev(img_channel)
            ref_mean, ref_std = cv2.meanStdDev(ref_channel)
            
            # ìƒ‰ìƒ ë§¤ì¹­
            if img_std > 0:
                img_channel = (img_channel - img_mean) * (ref_std / img_std) + ref_mean
                img_channel = np.clip(img_channel, 0, 255)
                lab_img[:, :, i] = img_channel.astype(np.uint8)
        
        corrected = cv2.cvtColor(lab_img, cv2.COLOR_LAB2BGR)
        return corrected
    
    def _enhance_details(self, image: np.ndarray) -> np.ndarray:
        """ë””í…Œì¼ í–¥ìƒ"""
        
        # ì–¸ìƒ¤í”„ ë§ˆìŠ¤í‚¹
        gaussian = cv2.GaussianBlur(image, (0, 0), 1.5)
        unsharp = cv2.addWeighted(image, 1.5, gaussian, -0.5, 0)
        
        # ì ì‘ì  íˆìŠ¤í† ê·¸ë¨ í‰í™œí™” (CLAHE)
        lab = cv2.cvtColor(unsharp, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
        
        return enhanced
    
    def _match_lighting(self, image: np.ndarray, reference: np.ndarray) -> np.ndarray:
        """ì¡°ëª… ë§¤ì¹­"""
        
        # ë°ê¸° ë¶„í¬ ë§¤ì¹­
        img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        ref_gray = cv2.cvtColor(reference, cv2.COLOR_BGR2GRAY)
        
        img_mean = np.mean(img_gray)
        ref_mean = np.mean(ref_gray)
        
        # ë°ê¸° ì¡°ì •
        brightness_factor = ref_mean / (img_mean + 1e-7)
        brightness_factor = np.clip(brightness_factor, 0.7, 1.3)  # ê³¼ë„í•œ ì¡°ì • ë°©ì§€
        
        adjusted = cv2.convertScaleAbs(image, alpha=brightness_factor, beta=0)
        
        return adjusted
    
    def _smooth_edges(self, enhanced: np.ndarray, original: np.ndarray) -> np.ndarray:
        """ê²½ê³„ì„  ë¶€ë“œëŸ½ê²Œ ì²˜ë¦¬"""
        
        # ì°¨ì´ê°€ í° ì˜ì—­ ì°¾ê¸° (ê²½ê³„ì„ )
        diff = cv2.absdiff(enhanced, original)
        diff_gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
        
        # ê²½ê³„ì„  ë§ˆìŠ¤í¬
        _, edge_mask = cv2.threshold(diff_gray, 30, 255, cv2.THRESH_BINARY)
        
        # ê²½ê³„ì„  í™•ì¥
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))
        edge_region = cv2.dilate(edge_mask, kernel, iterations=1)
        
        # ê²½ê³„ì„  ì˜ì—­ì— ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì ìš©
        blurred = cv2.GaussianBlur(enhanced, (5, 5), 0)
        
        # ë§ˆìŠ¤í¬ ì •ê·œí™”
        edge_region_norm = edge_region.astype(np.float32) / 255.0
        edge_region_norm = np.stack([edge_region_norm] * 3, axis=2)
        
        # ê²½ê³„ì„  ì˜ì—­ë§Œ ë¸”ëŸ¬ ì²˜ë¦¬
        smoothed = enhanced.astype(np.float32) * (1 - edge_region_norm) + \
                  blurred.astype(np.float32) * edge_region_norm
        
        return np.clip(smoothed, 0, 255).astype(np.uint8)
    
    async def _evaluate_final_quality(
        self,
        result_image: np.ndarray,
        person_image: np.ndarray,
        clothing_image: np.ndarray
    ) -> Dict[str, float]:
        """ìµœì¢… í’ˆì§ˆ í‰ê°€"""
        
        metrics = {}
        
        try:
            # 1. êµ¬ì¡° ìœ ì§€ë„ (SSIM)
            metrics['structural_similarity'] = self._calculate_ssim(result_image, person_image)
            
            # 2. ìƒ‰ìƒ ì¼ê´€ì„±
            metrics['color_consistency'] = self._evaluate_color_harmony(result_image, person_image)
            
            # 3. ì˜ë¥˜ ë³´ì¡´ë„
            metrics['clothing_preservation'] = self._evaluate_clothing_preservation(result_image, clothing_image)
            
            # 4. ìì—°ìŠ¤ëŸ¬ì›€
            metrics['naturalness'] = self._evaluate_naturalness(result_image)
            
            # 5. í”¼íŒ… ì ìˆ˜
            metrics['fit_score'] = self._calculate_fit_score(result_image, person_image)
            
            # 6. í˜„ì‹¤ì„± ì ìˆ˜
            metrics['realism_score'] = self._calculate_realism_score(result_image)
            
            # 7. ì „ì²´ í’ˆì§ˆ
            metrics['overall_quality'] = (
                metrics['structural_similarity'] * 0.2 +
                metrics['color_consistency'] * 0.15 +
                metrics['clothing_preservation'] * 0.2 +
                metrics['naturalness'] * 0.15 +
                metrics['fit_score'] * 0.15 +
                metrics['realism_score'] * 0.15
            )
            
        except Exception as e:
            logger.warning(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            metrics = {
                'overall_quality': 0.75,
                'fit_score': 0.8,
                'realism_score': 0.75
            }
        
        return metrics
    
    def _calculate_ssim(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """SSIM ê³„ì‚°"""
        # ê°„ì†Œí™”ëœ SSIM
        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
        
        mu1 = cv2.GaussianBlur(gray1, (11, 11), 1.5)
        mu2 = cv2.GaussianBlur(gray2, (11, 11), 1.5)
        
        mu1_sq = mu1 * mu1
        mu2_sq = mu2 * mu2
        mu1_mu2 = mu1 * mu2
        
        sigma1_sq = cv2.GaussianBlur(gray1 * gray1, (11, 11), 1.5) - mu1_sq
        sigma2_sq = cv2.GaussianBlur(gray2 * gray2, (11, 11), 1.5) - mu2_sq
        sigma12 = cv2.GaussianBlur(gray1 * gray2, (11, 11), 1.5) - mu1_mu2
        
        C1 = (0.01 * 255) ** 2
        C2 = (0.03 * 255) ** 2
        
        ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
        
        return float(np.mean(ssim_map))
    
    def _evaluate_color_harmony(self, result: np.ndarray, reference: np.ndarray) -> float:
        """ìƒ‰ìƒ ì¡°í™”ë„ í‰ê°€"""
        result_mean = np.mean(result, axis=(0, 1))
        ref_mean = np.mean(reference, axis=(0, 1))
        
        color_diff = np.linalg.norm(result_mean - ref_mean)
        harmony = max(0.0, 1.0 - color_diff / 255.0)
        
        return harmony
    
    def _evaluate_clothing_preservation(self, result: np.ndarray, clothing: np.ndarray) -> float:
        """ì˜ë¥˜ íŠ¹ì„± ë³´ì¡´ë„ í‰ê°€"""
        # ê°„ë‹¨í•œ ìƒ‰ìƒ ë¶„í¬ ë¹„êµ
        result_hist = cv2.calcHist([result], [0, 1, 2], None, [50, 50, 50], [0, 256, 0, 256, 0, 256])
        clothing_hist = cv2.calcHist([clothing], [0, 1, 2], None, [50, 50, 50], [0, 256, 0, 256, 0, 256])
        
        correlation = cv2.compareHist(result_hist, clothing_hist, cv2.HISTCMP_CORREL)
        return max(0.0, correlation)
    
    def _evaluate_naturalness(self, image: np.ndarray) -> float:
        """ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€"""
        # ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°ìœ¼ë¡œ ì„ ëª…ë„ ì¸¡ì •
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        
        # ì ì ˆí•œ ì„ ëª…ë„ ë²”ìœ„ë¡œ ì •ê·œí™”
        naturalness = min(1.0, laplacian_var / 500.0)
        return naturalness
    
    def _calculate_fit_score(self, result: np.ndarray, person: np.ndarray) -> float:
        """í”¼íŒ… ì ìˆ˜ ê³„ì‚°"""
        # ê°„ë‹¨í•œ êµ¬ì¡° ìœ ì‚¬ë„ ê¸°ë°˜
        return self._calculate_ssim(result, person)
    
    def _calculate_realism_score(self, image: np.ndarray) -> float:
        """í˜„ì‹¤ì„± ì ìˆ˜ ê³„ì‚°"""
        # ìƒ‰ìƒ ë¶„í¬ì˜ ìì—°ìŠ¤ëŸ¬ì›€
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        s_mean = np.mean(hsv[:, :, 1]) / 255.0
        
        # ì ì ˆí•œ ì±„ë„ ë²”ìœ„ (0.3-0.7)ê°€ ìì—°ìŠ¤ëŸ¬ì›€
        if 0.3 <= s_mean <= 0.7:
            realism = 1.0 - abs(s_mean - 0.5) * 2
        else:
            realism = max(0.0, 1.0 - abs(s_mean - 0.5))
        
        return realism
    
    def _numpy_to_tensor(self, array: np.ndarray, is_mask: bool = False) -> torch.Tensor:
        """numpyë¥¼ í…ì„œë¡œ ë³€í™˜"""
        if is_mask:
            if array.ndim == 2:
                tensor = torch.from_numpy(array / 255.0).float()
                return tensor.unsqueeze(0).unsqueeze(0).to(self.device)
        else:
            if array.ndim == 3:
                tensor = torch.from_numpy(array).permute(2, 0, 1).float() / 255.0
                return tensor.unsqueeze(0).to(self.device)
        
        return torch.from_numpy(array).to(self.device)
    
    def _tensor_to_numpy(self, tensor: torch.Tensor) -> np.ndarray:
        """í…ì„œë¥¼ numpyë¡œ ë³€í™˜"""
        if tensor.dim() == 4:
            tensor = tensor.squeeze(0)
        if tensor.shape[0] == 3:
            tensor = tensor.permute(1, 2, 0)
        
        array = (tensor.cpu().numpy() * 255).astype(np.uint8)
        return array
    
    def _image_to_base64(self, image: np.ndarray) -> str:
        """ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©"""
        _, buffer = cv2.imencode('.jpg', image)
        image_base64 = base64.b64encode(buffer).decode('utf-8')
        return image_base64
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.pose_estimator:
            self.pose_estimator.close()
        
        if self.segmentation_model:
            self.segmentation_model.close()
        
        if self.neural_compositor:
            await self.neural_compositor.cleanup()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ (ê¸°ì¡´ utils í™œìš©)
        optimize_memory_usage()
        
        self.is_initialized = False
        logger.info("ğŸ§¹ ì‹¤ì œ ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


# === ë³´ì¡° í´ë˜ìŠ¤ë“¤ ===

class RealTPSTransformer:
    """ì‹¤ì œ TPS (Thin Plate Spline) ë³€í™˜ê¸°"""
    
    def __init__(self, device: str = 'cpu'):
        self.device = device
    
    async def transform(
        self,
        image: np.ndarray,
        mask: np.ndarray,
        target_keypoints: List[Dict]
    ) -> Tuple[np.ndarray, np.ndarray]:
        """TPS ë³€í™˜ ì ìš©"""
        
        if len(target_keypoints) < 4:
            # í‚¤í¬ì¸íŠ¸ ë¶€ì¡± ì‹œ ê°„ë‹¨í•œ ì–´í•€ ë³€í™˜
            return await self._affine_transform(image, mask, target_keypoints)
        
        # ì†ŒìŠ¤ í¬ì¸íŠ¸ (ì˜ë¥˜ì˜ íŠ¹ì§•ì ë“¤)
        src_points = self._extract_clothing_keypoints(image, mask)
        
        # íƒ€ê²Ÿ í¬ì¸íŠ¸ (ì‹ ì²´ í‚¤í¬ì¸íŠ¸ë“¤)
        dst_points = [(kp['x'], kp['y']) for kp in target_keypoints[:len(src_points)]]
        
        if len(src_points) != len(dst_points) or len(src_points) < 4:
            return await self._affine_transform(image, mask, target_keypoints)
        
        # TPS ë³€í™˜ ê³„ì‚°
        try:
            tps_transform = self._calculate_tps_transform(src_points, dst_points)
            
            # ì´ë¯¸ì§€ ë³€í™˜ ì ìš©
            warped_image = self._apply_tps_transform(image, tps_transform)
            warped_mask = self._apply_tps_transform(mask, tps_transform)
            
            return warped_image, warped_mask
            
        except Exception as e:
            logger.warning(f"TPS ë³€í™˜ ì‹¤íŒ¨, ì–´í•€ ë³€í™˜ ì‚¬ìš©: {e}")
            return await self._affine_transform(image, mask, target_keypoints)
    
    def _extract_clothing_keypoints(self, image: np.ndarray, mask: np.ndarray) -> List[Tuple[int, int]]:
        """ì˜ë¥˜ì—ì„œ íŠ¹ì§•ì  ì¶”ì¶œ"""
        
        # ë§ˆìŠ¤í¬ì—ì„œ ìœ¤ê³½ì„  ì°¾ê¸°
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            h, w = image.shape[:2]
            return [(0, 0), (w-1, 0), (w-1, h-1), (0, h-1)]  # ê¸°ë³¸ ì‚¬ê°í˜•
        
        # ê°€ì¥ í° ìœ¤ê³½ì„  ì„ íƒ
        largest_contour = max(contours, key=cv2.contourArea)
        
        # ìœ¤ê³½ì„ ì—ì„œ íŠ¹ì§•ì  ì¶”ì¶œ
        epsilon = 0.02 * cv2.arcLength(largest_contour, True)
        approx = cv2.approxPolyDP(largest_contour, epsilon, True)
        
        # íŠ¹ì§•ì ë“¤
        keypoints = []
        for point in approx:
            x, y = point[0]
            keypoints.append((int(x), int(y)))
        
        # ìµœì†Œ 4ê°œ, ìµœëŒ€ 8ê°œ í¬ì¸íŠ¸
        if len(keypoints) < 4:
            # ì¶”ê°€ í¬ì¸íŠ¸ ìƒì„±
            rect = cv2.boundingRect(largest_contour)
            x, y, w, h = rect
            keypoints = [
                (x, y), (x + w, y), 
                (x + w, y + h), (x, y + h)
            ]
        elif len(keypoints) > 8:
            # í¬ì¸íŠ¸ ê°„ì†Œí™”
            keypoints = keypoints[:8]
        
        return keypoints
    
    def _calculate_tps_transform(self, src_points: List[Tuple], dst_points: List[Tuple]) -> Dict:
        """TPS ë³€í™˜ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        
        n = len(src_points)
        src_array = np.array(src_points, dtype=np.float32)
        dst_array = np.array(dst_points, dtype=np.float32)
        
        # TPS ë³€í™˜ì„ ìœ„í•œ RBF (Radial Basis Function) ì‚¬ìš©
        rbf_x = Rbf(src_array[:, 0], src_array[:, 1], dst_array[:, 0], function='thin_plate', smooth=0)
        rbf_y = Rbf(src_array[:, 0], src_array[:, 1], dst_array[:, 1], function='thin_plate', smooth=0)
        
        return {
            'rbf_x': rbf_x,
            'rbf_y': rbf_y,
            'src_points': src_array,
            'dst_points': dst_array
        }
    
    def _apply_tps_transform(self, image: np.ndarray, tps_transform: Dict) -> np.ndarray:
        """TPS ë³€í™˜ì„ ì´ë¯¸ì§€ì— ì ìš©"""
        
        h, w = image.shape[:2]
        
        # ê·¸ë¦¬ë“œ ìƒì„±
        x, y = np.meshgrid(np.arange(w), np.arange(h))
        x_flat = x.flatten().astype(np.float32)
        y_flat = y.flatten().astype(np.float32)
        
        # TPS ë³€í™˜ ì ìš©
        try:
            new_x = tps_transform['rbf_x'](x_flat, y_flat)
            new_y = tps_transform['rbf_y'](x_flat, y_flat)
            
            # ë³€í™˜ëœ ì¢Œí‘œë¡œ ë¦¬ë§¤í•‘
            map_x = new_x.reshape(h, w).astype(np.float32)
            map_y = new_y.reshape(h, w).astype(np.float32)
            
            # ì´ë¯¸ì§€ ë¦¬ë§¤í•‘
            if len(image.shape) == 3:
                warped = cv2.remap(image, map_x, map_y, cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT, borderValue=(255, 255, 255))
            else:
                warped = cv2.remap(image, map_x, map_y, cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT, borderValue=0)
            
            return warped
            
        except Exception as e:
            logger.warning(f"TPS ë¦¬ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return image
    
    async def _affine_transform(
        self,
        image: np.ndarray,
        mask: np.ndarray,
        target_keypoints: List[Dict]
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ì–´í•€ ë³€í™˜ (TPS ëŒ€ì•ˆ)"""
        
        h, w = image.shape[:2]
        
        if len(target_keypoints) >= 3:
            # 3ì  ì–´í•€ ë³€í™˜
            src_triangle = np.float32([(0, 0), (w, 0), (w//2, h)])
            
            dst_triangle = np.float32([
                (target_keypoints[0]['x'], target_keypoints[0]['y']),
                (target_keypoints[1]['x'], target_keypoints[1]['y']),
                (target_keypoints[2]['x'], target_keypoints[2]['y'])
            ])
            
            # ì–´í•€ ë³€í™˜ ë§¤íŠ¸ë¦­ìŠ¤
            affine_mat = cv2.getAffineTransform(src_triangle, dst_triangle)
            
            # ë³€í™˜ ì ìš©
            warped_image = cv2.warpAffine(image, affine_mat, (w, h), borderValue=(255, 255, 255))
            warped_mask = cv2.warpAffine(mask, affine_mat, (w, h), borderValue=0)
            
            return warped_image, warped_mask
        else:
            # ë³€í™˜ ì—†ì´ ì›ë³¸ ë°˜í™˜
            return image, mask


class NeuralCompositor:
    """ì‹ ê²½ë§ í•©ì„±ê¸° (ê°„ë‹¨í•œ ë²„ì „)"""
    
    def __init__(self, device: str = 'cpu'):
        self.device = device
        self.model = None
    
    async def initialize(self) -> bool:
        """ì‹ ê²½ë§ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # ê°„ë‹¨í•œ U-Net ìŠ¤íƒ€ì¼ ëª¨ë¸ (ì‹¤ì œë¡œëŠ” ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ)
            self.model = self._create_simple_compositor()
            self.model.to(self.device)
            self.model.eval()
            
            logger.info("âœ… ì‹ ê²½ë§ í•©ì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.warning(f"ì‹ ê²½ë§ í•©ì„±ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.model = None
            return False
    
    def _create_simple_compositor(self) -> nn.Module:
        """ê°„ë‹¨í•œ í•©ì„± ëª¨ë¸"""
        
        class SimpleCompositor(nn.Module):
            def __init__(self):
                super().__init__()
                
                # ê°„ë‹¨í•œ CNN
                self.conv1 = nn.Conv2d(7, 64, 3, padding=1)  # person(3) + clothing(3) + mask(1)
                self.conv2 = nn.Conv2d(64, 64, 3, padding=1)
                self.conv3 = nn.Conv2d(64, 32, 3, padding=1)
                self.conv4 = nn.Conv2d(32, 3, 3, padding=1)
                
                self.relu = nn.ReLU(inplace=True)
                
            def forward(self, person, clothing, mask):
                # ì…ë ¥ ê²°í•©
                x = torch.cat([person, clothing, mask], dim=1)
                
                # CNN ì²˜ë¦¬
                x = self.relu(self.conv1(x))
                x = self.relu(self.conv2(x))
                x = self.relu(self.conv3(x))
                x = torch.sigmoid(self.conv4(x))
                
                return x
        
        return SimpleCompositor()
    
    def compose(self, person: torch.Tensor, clothing: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """ì‹ ê²½ë§ í•©ì„±"""
        if self.model is None:
            # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ë¸”ë Œë”©
            mask_expanded = mask.expand_as(person)
            return person * (1 - mask_expanded) + clothing * mask_expanded
        
        try:
            with torch.no_grad():
                result = self.model(person, clothing, mask)
            return result
        except Exception as e:
            logger.warning(f"ì‹ ê²½ë§ í•©ì„± ì‹¤íŒ¨: {e}")
            mask_expanded = mask.expand_as(person)
            return person * (1 - mask_expanded) + clothing * mask_expanded
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.model:
            del self.model
            self.model = None


class RealQualityEnhancer:
    """ì‹¤ì œ í’ˆì§ˆ í–¥ìƒê¸°"""
    
    def __init__(self, device: str = 'cpu'):
        self.device = device
    
    def enhance(self, image: np.ndarray) -> np.ndarray:
        """í’ˆì§ˆ í–¥ìƒ"""
        
        enhanced = image.copy()
        
        # 1. ë…¸ì´ì¦ˆ ì œê±°
        enhanced = cv2.bilateralFilter(enhanced, 9, 75, 75)
        
        # 2. ì„ ëª…ë„ í–¥ìƒ
        enhanced = self._sharpen_image(enhanced)
        
        # 3. ëŒ€ë¹„ í–¥ìƒ
        enhanced = self._enhance_contrast(enhanced)
        
        return enhanced
    
    def _sharpen_image(self, image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ ì„ ëª…í™”"""
        kernel = np.array([[-1, -1, -1],
                          [-1,  9, -1],
                          [-1, -1, -1]])
        
        sharpened = cv2.filter2D(image, -1, kernel * 0.1)
        return cv2.addWeighted(image, 0.8, sharpened, 0.2, 0)
    
    def _enhance_contrast(self, image: np.ndarray) -> np.ndarray:
        """ëŒ€ë¹„ í–¥ìƒ"""
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # CLAHE ì ìš©
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        enhanced = cv2.merge([l, a, b])
        return cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)


# === ì‚¬ìš© ì˜ˆì‹œ (ê¸°ì¡´ êµ¬ì¡°ì™€ í†µí•©) ===
async def test_integrated_virtual_fitting():
    """í†µí•©ëœ ê°€ìƒ í”¼íŒ… í…ŒìŠ¤íŠ¸"""
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    fitting_system = VirtualFittingStep(
        device='mps',  # M3 Max
        config={
            'pose_confidence_threshold': 0.5,
            'segmentation_quality': 'high',
            'enable_neural_composition': True,
            'image_size': 512
        }
    )
    
    success = await fitting_system.initialize()
    if not success:
        print("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ (ê¸°ì¡´ êµ¬ì¡° í™œìš©)
    person_image_path = "uploads/test_person.jpg"
    clothing_image_path = "uploads/test_clothing.jpg"
    
    # ê°€ìƒ í”¼íŒ… ì‹¤í–‰
    result = await fitting_system.process(
        person_image=person_image_path,
        clothing_image=clothing_image_path,
        target_region='upper',
        user_preferences={'height': 175, 'weight': 70}
    )
    
    if result['success']:
        print(f"âœ… í†µí•© ê°€ìƒ í”¼íŒ… ì„±ê³µ!")
        print(f"ğŸ“Š ì „ì²´ í’ˆì§ˆ: {result['overall_quality']:.3f}")
        print(f"ğŸ‘” í”¼íŒ… ì ìˆ˜: {result['fit_score']:.3f}")
        print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result['processing_info']['processing_time']:.2f}ì´ˆ")
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {result['fitted_image_path']}")
        
        # ì¶”ì²œì‚¬í•­ ì¶œë ¥
        if result['recommendations']:
            print("ğŸ’¡ ì¶”ì²œì‚¬í•­:")
            for rec in result['recommendations']:
                print(f"   - {rec}")
        
    else:
        print(f"âŒ ê°€ìƒ í”¼íŒ… ì‹¤íŒ¨: {result['error']}")
    
    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    await fitting_system.cleanup()

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_integrated_virtual_fitting())