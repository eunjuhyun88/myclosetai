"""
ğŸ”¥ Inference Engines
===================

ì¸ì²´ íŒŒì‹± ì¶”ë¡  ì—”ì§„ë“¤

Author: MyCloset AI Team
Date: 2025-08-07
Version: 1.0
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Any, Optional, Tuple
import logging
import sys
import io


class InferenceEngine:
    """ì¶”ë¡  ì—”ì§„ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, device: str = 'cpu'):
        self.device = device
        self.logger = logging.getLogger(__name__)
    
    def _safe_tensor_to_scalar(self, tensor_value):
        """í…ì„œë¥¼ ìŠ¤ì¹¼ë¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
        if isinstance(tensor_value, torch.Tensor):
            if tensor_value.numel() == 1:
                return tensor_value.item()
            else:
                return tensor_value.mean().item()
        elif isinstance(tensor_value, (int, float)):
            return float(tensor_value)
        else:
            return 0.8  # ê¸°ë³¸ê°’
    
    def _extract_actual_model(self, model) -> Optional[nn.Module]:
        """ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ (í‘œì¤€í™”)"""
        try:
            if hasattr(model, 'model_instance') and model.model_instance is not None:
                return model.model_instance
            elif hasattr(model, 'get_model_instance'):
                return model.get_model_instance()
            elif callable(model):
                return model
            else:
                return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_standard_output(self, device) -> Dict[str, Any]:
        """í‘œì¤€ ì¶œë ¥ ìƒì„±"""
        return {
            'parsing_pred': torch.zeros((1, 20, 512, 512), device=device),  # ì¼ê´€ëœ í‚¤ ì´ë¦„ ì‚¬ìš©
            'parsing_output': torch.zeros((1, 20, 512, 512), device=device),
            'confidence': 0.5,
            'edge_output': None
        }
    
    def _extract_parsing_from_output(self, output, device) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """ëª¨ë¸ ì¶œë ¥ì—ì„œ íŒŒì‹± ê²°ê³¼ ì¶”ì¶œ (ê·¼ë³¸ì  í•´ê²°)"""
        try:
            # ğŸ”¥ 1ë‹¨ê³„: ì¶œë ¥ íƒ€ì… ê²€ì¦ ë° ì •ê·œí™”
            if output is None:
                self.logger.warning("âš ï¸ AI ëª¨ë¸ ì¶œë ¥ì´ Noneì…ë‹ˆë‹¤.")
                return torch.zeros((1, 20, 512, 512), device=device), None
            
            # ğŸ”¥ 2ë‹¨ê³„: ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ì¶œë ¥ ì²˜ë¦¬
            if isinstance(output, dict):
                self.logger.debug(f"ğŸ”¥ ë”•ì…”ë„ˆë¦¬ ì¶œë ¥ í‚¤ë“¤: {list(output.keys())}")
                
                # ê°€ëŠ¥í•œ í‚¤ë“¤ì—ì„œ íŒŒì‹± ê²°ê³¼ ì°¾ê¸°
                parsing_keys = ['parsing', 'parsing_pred', 'output', 'parsing_output', 'logits', 'pred', 'prediction']
                parsing_tensor = None
                confidence_tensor = None
                
                for key in parsing_keys:
                    if key in output and output[key] is not None:
                        if isinstance(output[key], torch.Tensor):
                            parsing_tensor = output[key]
                            self.logger.debug(f"âœ… íŒŒì‹± í…ì„œ ë°œê²¬: {key} - {parsing_tensor.shape}")
                            break
                        elif isinstance(output[key], (list, tuple)) and len(output[key]) > 0:
                            if isinstance(output[key][0], torch.Tensor):
                                parsing_tensor = output[key][0]
                                self.logger.debug(f"âœ… íŒŒì‹± í…ì„œ ë°œê²¬ (ë¦¬ìŠ¤íŠ¸): {key} - {parsing_tensor.shape}")
                                break
                
                # ì‹ ë¢°ë„ í…ì„œ ì°¾ê¸°
                confidence_keys = ['confidence', 'conf', 'prob', 'probability']
                for key in confidence_keys:
                    if key in output and output[key] is not None:
                        if isinstance(output[key], torch.Tensor):
                            confidence_tensor = output[key]
                            self.logger.debug(f"âœ… ì‹ ë¢°ë„ í…ì„œ ë°œê²¬: {key} - {confidence_tensor.shape}")
                            break
                
                # ğŸ”¥ 3ë‹¨ê³„: í…ì„œê°€ ì—†ëŠ” ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                if parsing_tensor is None:
                    first_value = next(iter(output.values()))
                    if isinstance(first_value, torch.Tensor):
                        parsing_tensor = first_value
                        self.logger.debug(f"âœ… ì²« ë²ˆì§¸ ê°’ì—ì„œ íŒŒì‹± í…ì„œ ì¶”ì¶œ: {parsing_tensor.shape}")
                    elif isinstance(first_value, (list, tuple)) and len(first_value) > 0:
                        if isinstance(first_value[0], torch.Tensor):
                            parsing_tensor = first_value[0]
                            self.logger.debug(f"âœ… ì²« ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ íŒŒì‹± í…ì„œ ì¶”ì¶œ: {parsing_tensor.shape}")
                
                if parsing_tensor is None:
                    raise ValueError("ë”•ì…”ë„ˆë¦¬ì—ì„œ íŒŒì‹± í…ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                return parsing_tensor, confidence_tensor
            
            # ğŸ”¥ 4ë‹¨ê³„: ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ì¶œë ¥ ì²˜ë¦¬
            elif isinstance(output, (list, tuple)):
                self.logger.debug(f"ğŸ”¥ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ ê¸¸ì´: {len(output)}")
                
                if len(output) == 0:
                    raise ValueError("ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ì…ë‹ˆë‹¤.")
                
                # ì²« ë²ˆì§¸ ìš”ì†Œê°€ í…ì„œì¸ì§€ í™•ì¸
                first_element = output[0]
                if isinstance(first_element, torch.Tensor):
                    parsing_tensor = first_element
                    self.logger.debug(f"âœ… ë¦¬ìŠ¤íŠ¸ ì²« ë²ˆì§¸ ìš”ì†Œì—ì„œ íŒŒì‹± í…ì„œ ì¶”ì¶œ: {parsing_tensor.shape}")
                    
                    # ë‘ ë²ˆì§¸ ìš”ì†Œê°€ ì‹ ë¢°ë„ í…ì„œì¸ì§€ í™•ì¸
                    confidence_tensor = None
                    if len(output) > 1 and isinstance(output[1], torch.Tensor):
                        confidence_tensor = output[1]
                        self.logger.debug(f"âœ… ë¦¬ìŠ¤íŠ¸ ë‘ ë²ˆì§¸ ìš”ì†Œì—ì„œ ì‹ ë¢°ë„ í…ì„œ ì¶”ì¶œ: {confidence_tensor.shape}")
                    
                    return parsing_tensor, confidence_tensor
                else:
                    self.logger.warning(f"âš ï¸ ë¦¬ìŠ¤íŠ¸ ì²« ë²ˆì§¸ ìš”ì†Œê°€ í…ì„œê°€ ì•„ë‹˜: {type(first_element)}")
                    # ë”•ì…”ë„ˆë¦¬ë¡œ ì²˜ë¦¬
                    if isinstance(first_element, dict):
                        return self._extract_parsing_from_output(first_element, device)
                    else:
                        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¶œë ¥ íƒ€ì…: {type(first_element)}")
            
            # ğŸ”¥ 5ë‹¨ê³„: ì§ì ‘ í…ì„œ ì¶œë ¥ ì²˜ë¦¬
            elif isinstance(output, torch.Tensor):
                self.logger.debug(f"âœ… ì§ì ‘ í…ì„œ ì¶œë ¥: {output.shape}")
                # ì›ë³¸ í…ì„œ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì°¨ì› ë³€í™˜ì€ í˜¸ì¶œí•˜ëŠ” ê³³ì—ì„œ ì²˜ë¦¬)
                return output, None
            
            # ğŸ”¥ 6ë‹¨ê³„: ê¸°íƒ€ íƒ€ì… ì²˜ë¦¬
            else:
                self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¶œë ¥ íƒ€ì…: {type(output)}")
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¶œë ¥ íƒ€ì…: {type(output)}")
                
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì‹± ì¶œë ¥ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return torch.zeros((1, 20, 512, 512), device=device), None
    
    def _standardize_channels(self, tensor: torch.Tensor, target_channels: int = 20) -> torch.Tensor:
        """ì±„ë„ ìˆ˜ í‘œì¤€í™” (ê·¼ë³¸ì  í•´ê²°)"""
        try:
            # ğŸ”¥ ì…ë ¥ ê²€ì¦
            if tensor is None:
                self.logger.warning("âš ï¸ í…ì„œê°€ Noneì…ë‹ˆë‹¤.")
                return torch.zeros((1, target_channels, 512, 512), device='cpu', dtype=torch.float32)
            
            # ğŸ”¥ ì°¨ì› ê²€ì¦
            if len(tensor.shape) != 4:
                self.logger.warning(f"âš ï¸ í…ì„œ ì°¨ì›ì´ 4ê°€ ì•„ë‹˜: {tensor.shape}")
                if len(tensor.shape) == 3:
                    # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                    tensor = tensor.unsqueeze(0)
                elif len(tensor.shape) == 2:
                    # ë°°ì¹˜ì™€ ì±„ë„ ì°¨ì› ì¶”ê°€
                    tensor = tensor.unsqueeze(0).unsqueeze(0)
                else:
                    return torch.zeros((1, target_channels, 512, 512), device=tensor.device, dtype=tensor.dtype)
            
            # ğŸ”¥ ì±„ë„ ìˆ˜ í‘œì¤€í™”
            if tensor.shape[1] == target_channels:
                return tensor
            elif tensor.shape[1] > target_channels:
                # ğŸ”¥ ì±„ë„ ìˆ˜ê°€ ë§ìœ¼ë©´ ì•ìª½ ì±„ë„ë§Œ ì‚¬ìš©
                return tensor[:, :target_channels, :, :]
            else:
                # ğŸ”¥ ì±„ë„ ìˆ˜ê°€ ì ìœ¼ë©´ íŒ¨ë”©
                padding = torch.zeros(
                    tensor.shape[0], 
                    target_channels - tensor.shape[1], 
                    tensor.shape[2], 
                    tensor.shape[3],
                    device=tensor.device,
                    dtype=tensor.dtype
                )
                return torch.cat([tensor, padding], dim=1)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì±„ë„ ìˆ˜ í‘œì¤€í™” ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return torch.zeros((1, target_channels, 512, 512), device='cpu', dtype=torch.float32)


class GraphonomyInferenceEngine(InferenceEngine):
    """Graphonomy ì¶”ë¡  ì—”ì§„"""
    
    def run_inference(self, input_tensor: torch.Tensor, model: nn.Module) -> Dict[str, Any]:
        """Graphonomy ì•™ìƒë¸” ì¶”ë¡  - ê·¼ë³¸ì  í•´ê²°"""
        try:
            # ğŸ”¥ 1. ëª¨ë¸ ê²€ì¦ ë° í‘œì¤€í™”
            if model is None:
                self.logger.warning("âš ï¸ Graphonomy ëª¨ë¸ì´ Noneì…ë‹ˆë‹¤")
                return self._create_standard_output(input_tensor.device)
            
            # ğŸ”¥ 2. ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ (í‘œì¤€í™”)
            actual_model = self._extract_actual_model(model)
            if actual_model is None:
                return self._create_standard_output(input_tensor.device)
            
            # ğŸ”¥ 3. MPS íƒ€ì… ì¼ì¹˜ (ê·¼ë³¸ì  í•´ê²°)
            device = input_tensor.device
            dtype = torch.float32  # ëª¨ë“  í…ì„œë¥¼ float32ë¡œ í†µì¼
            
            # ëª¨ë¸ì„ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ì™€ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            actual_model = actual_model.to(device, dtype=dtype)
            input_tensor = input_tensor.to(device, dtype=dtype)
            
            # ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ë™ì¼í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            for param in actual_model.parameters():
                param.data = param.data.to(dtype)
            
            # ğŸ”¥ 4. ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (ì•ˆì „í•œ ë°©ì‹)
            try:
                with torch.no_grad():
                    # í…ì„œ í¬ë§· ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì™„ì „í•œ ë¡œê¹… ë¹„í™œì„±í™”
                    original_level = logging.getLogger().level
                    logging.getLogger().setLevel(logging.CRITICAL)
                    
                    # stdout/stderr ë¦¬ë‹¤ì´ë ‰ì…˜ìœ¼ë¡œ í…ì„œ í¬ë§· ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨
                    original_stdout = sys.stdout
                    original_stderr = sys.stderr
                    sys.stdout = io.StringIO()
                    sys.stderr = io.StringIO()
                    
                    try:
                        output = actual_model(input_tensor)
                    finally:
                        # ì¶œë ¥ ë³µì›
                        sys.stdout = original_stdout
                        sys.stderr = original_stderr
                        logging.getLogger().setLevel(original_level)
                    
            except Exception as inference_error:
                self.logger.warning(f"âš ï¸ Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {inference_error}")
                return self._create_standard_output(device)
            
            # ğŸ”¥ 5. ì¶œë ¥ í‘œì¤€í™” (ê·¼ë³¸ì  í•´ê²°)
            parsing_output, _ = self._extract_parsing_from_output(output, input_tensor.device)
            
            # ğŸ”¥ 6. ì±„ë„ ìˆ˜ í‘œì¤€í™” (20ê°œë¡œ í†µì¼)
            parsing_output = self._standardize_channels(parsing_output, target_channels=20)
            
            # ğŸ”¥ 7. ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(parsing_output)
            
            return {
                'parsing_pred': parsing_output,  # ì¼ê´€ëœ í‚¤ ì´ë¦„ ì‚¬ìš©
                'parsing_output': parsing_output,
                'confidence': confidence
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Graphonomy ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
            return self._create_standard_output(input_tensor.device)
    
    def _calculate_confidence(self, parsing_probs, parsing_logits=None, edge_output=None, mode='advanced'):
        """ì‹ ë¢°ë„ ê³„ì‚° (ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜)"""
        try:
            if parsing_probs is None:
                return 0.5
            
            # ğŸ”¥ ê³ ê¸‰ ì‹ ë¢°ë„ ê³„ì‚°
            if mode == 'advanced':
                # 1. ê¸°ë³¸ ì‹ ë¢°ë„ (ì†Œí”„íŠ¸ë§¥ìŠ¤ í™•ë¥  ê¸°ë°˜)
                if parsing_probs.dim() == 4:
                    # 4ì°¨ì› í…ì„œ: [batch, channels, height, width]
                    softmax_probs = F.softmax(parsing_probs, dim=1)
                    max_probs = torch.max(softmax_probs, dim=1)[0]  # [batch, height, width]
                    base_confidence = torch.mean(max_probs).item()
                else:
                    # 2ì°¨ì› ë˜ëŠ” 3ì°¨ì› í…ì„œ
                    base_confidence = 0.8
                
                # 2. ì—£ì§€ ì‹ ë¢°ë„ (ì—£ì§€ ì¶œë ¥ì´ ìˆëŠ” ê²½ìš°)
                edge_confidence = 1.0
                if edge_output is not None:
                    if edge_output.dim() == 4:
                        edge_confidence = torch.mean(torch.sigmoid(edge_output)).item()
                    else:
                        edge_confidence = 0.9
                
                # 3. ê³µê°„ ì¼ê´€ì„± ì‹ ë¢°ë„
                spatial_confidence = self._calculate_spatial_consistency(parsing_probs)
                
                # 4. ì¢…í•© ì‹ ë¢°ë„ ê³„ì‚°
                final_confidence = (base_confidence * 0.5 + 
                                  edge_confidence * 0.3 + 
                                  spatial_confidence * 0.2)
                
                return max(0.1, min(1.0, final_confidence))
            
            else:
                # ğŸ”¥ ê¸°ë³¸ ì‹ ë¢°ë„ ê³„ì‚°
                if parsing_probs.dim() == 4:
                    softmax_probs = F.softmax(parsing_probs, dim=1)
                    max_probs = torch.max(softmax_probs, dim=1)[0]
                    confidence = torch.mean(max_probs).item()
                else:
                    confidence = 0.8
                
                return max(0.1, min(1.0, confidence))
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.8
    
    def _calculate_spatial_consistency(self, parsing_pred):
        """ê³µê°„ ì¼ê´€ì„± ê³„ì‚°"""
        try:
            if parsing_pred.dim() == 4:
                # 4ì°¨ì› í…ì„œì˜ ê²½ìš°
                softmax_probs = F.softmax(parsing_pred, dim=1)
                max_probs = torch.max(softmax_probs, dim=1)[0]
                
                # ê³µê°„ì  ì¼ê´€ì„± ê³„ì‚° (ì´ì›ƒ í”½ì…€ ê°„ì˜ ìœ ì‚¬ì„±)
                consistency = torch.mean(max_probs).item()
                return consistency
            else:
                return 0.8
        except:
            return 0.8


class HRNetInferenceEngine(InferenceEngine):
    """HRNet ì¶”ë¡  ì—”ì§„"""
    
    def run_inference(self, input_tensor: torch.Tensor, model: nn.Module) -> Dict[str, Any]:
        """HRNet ì•™ìƒë¸” ì¶”ë¡  - ê·¼ë³¸ì  í•´ê²°"""
        try:
            # ğŸ”¥ 1. ëª¨ë¸ ê²€ì¦ ë° í‘œì¤€í™”
            if model is None:
                self.logger.warning("âš ï¸ HRNet ëª¨ë¸ì´ Noneì…ë‹ˆë‹¤")
                return self._create_standard_output(input_tensor.device)
            
            # ğŸ”¥ 2. ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ (í‘œì¤€í™”)
            actual_model = self._extract_actual_model(model)
            if actual_model is None:
                return self._create_standard_output(input_tensor.device)
            
            # ğŸ”¥ 3. MPS íƒ€ì… ì¼ì¹˜ (ê·¼ë³¸ì  í•´ê²°)
            device = input_tensor.device
            dtype = torch.float32  # ëª¨ë“  í…ì„œë¥¼ float32ë¡œ í†µì¼
            
            # ëª¨ë¸ì„ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ì™€ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            actual_model = actual_model.to(device, dtype=dtype)
            input_tensor = input_tensor.to(device, dtype=dtype)
            
            # ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ë™ì¼í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            for param in actual_model.parameters():
                param.data = param.data.to(dtype)
            
            # ğŸ”¥ 4. ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (ì•ˆì „í•œ ë°©ì‹)
            try:
                with torch.no_grad():
                    # í…ì„œ í¬ë§· ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì™„ì „í•œ ë¡œê¹… ë¹„í™œì„±í™”
                    import logging
                    import sys
                    import io
                    
                    # ëª¨ë“  ë¡œê¹… ë¹„í™œì„±í™”
                    original_level = logging.getLogger().level
                    logging.getLogger().setLevel(logging.CRITICAL)
                    
                    # stdout/stderr ë¦¬ë‹¤ì´ë ‰ì…˜ìœ¼ë¡œ í…ì„œ í¬ë§· ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨
                    original_stdout = sys.stdout
                    original_stderr = sys.stderr
                    sys.stdout = io.StringIO()
                    sys.stderr = io.StringIO()
                    
                    try:
                        output = actual_model(input_tensor)
                    finally:
                        # ì¶œë ¥ ë³µì›
                        sys.stdout = original_stdout
                        sys.stderr = original_stderr
                        logging.getLogger().setLevel(original_level)
                    
            except Exception as inference_error:
                self.logger.warning(f"âš ï¸ HRNet ì¶”ë¡  ì‹¤íŒ¨: {inference_error}")
                return self._create_standard_output(input_tensor.device)
            
            # ğŸ”¥ 5. ì¶œë ¥ í‘œì¤€í™” (ê·¼ë³¸ì  í•´ê²°)
            parsing_output, _ = self._extract_parsing_from_output(output, input_tensor.device)
            
            # ğŸ”¥ 6. ì±„ë„ ìˆ˜ í‘œì¤€í™” (20ê°œë¡œ í†µì¼)
            parsing_output = self._standardize_channels(parsing_output, target_channels=20)
            
            # ğŸ”¥ 7. ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(parsing_output)
            
            return {
                'parsing_pred': parsing_output,  # ì¼ê´€ëœ í‚¤ ì´ë¦„ ì‚¬ìš©
                'parsing_output': parsing_output,
                'confidence': confidence
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ HRNet ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
            return self._create_standard_output(input_tensor.device)
    
    def _calculate_confidence(self, parsing_probs, parsing_logits=None, edge_output=None, mode='advanced'):
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            if parsing_probs is None:
                return 0.5
            
            if parsing_probs.dim() == 4:
                softmax_probs = F.softmax(parsing_probs, dim=1)
                max_probs = torch.max(softmax_probs, dim=1)[0]
                confidence = torch.mean(max_probs).item()
            else:
                confidence = 0.8
            
            return max(0.1, min(1.0, confidence))
        except:
            return 0.8


class DeepLabV3PlusInferenceEngine(InferenceEngine):
    """DeepLabV3+ ì¶”ë¡  ì—”ì§„"""
    
    def run_inference(self, input_tensor: torch.Tensor, model: nn.Module) -> Dict[str, Any]:
        """DeepLabV3+ ì•™ìƒë¸” ì¶”ë¡ """
        try:
            # RealAIModelì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ
            if hasattr(model, 'model_instance') and model.model_instance is not None:
                actual_model = model.model_instance
                self.logger.info("âœ… DeepLabV3+ - RealAIModelì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ ì„±ê³µ")
            elif hasattr(model, 'get_model_instance'):
                actual_model = model.get_model_instance()
                self.logger.info("âœ… DeepLabV3+ - get_model_instance()ë¡œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ ì„±ê³µ")
            else:
                actual_model = model
                self.logger.info("âš ï¸ DeepLabV3+ - ì§ì ‘ ëª¨ë¸ ì‚¬ìš© (RealAIModel ì•„ë‹˜)")
            
            # ëª¨ë¸ì„ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ì™€ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (MPS íƒ€ì… ì¼ì¹˜)
            device = input_tensor.device
            dtype = torch.float32  # ëª¨ë“  í…ì„œë¥¼ float32ë¡œ í†µì¼
            
            if hasattr(actual_model, 'to'):
                actual_model = actual_model.to(device, dtype=dtype)
                self.logger.info(f"âœ… DeepLabV3+ ëª¨ë¸ì„ {device} ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (float32)")
            
            # ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ë™ì¼í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            for param in actual_model.parameters():
                param.data = param.data.to(dtype)
            
            # ëª¨ë¸ì´ callableí•œì§€ í™•ì¸
            if not callable(actual_model):
                self.logger.warning("âš ï¸ DeepLabV3+ ëª¨ë¸ì´ callableí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                # ì‹¤ì œ ëª¨ë¸ì´ ì•„ë‹Œ ê²½ìš° ì˜¤ë¥˜ ë°œìƒ
                raise ValueError("DeepLabV3+ ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # í…ì„œ í¬ë§· ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì™„ì „í•œ ë¡œê¹… ë¹„í™œì„±í™”
            import logging
            import sys
            import io
            
            # ëª¨ë“  ë¡œê¹… ë¹„í™œì„±í™”
            original_level = logging.getLogger().level
            logging.getLogger().setLevel(logging.CRITICAL)
            
            # stdout/stderr ë¦¬ë‹¤ì´ë ‰ì…˜ìœ¼ë¡œ í…ì„œ í¬ë§· ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨
            original_stdout = sys.stdout
            original_stderr = sys.stderr
            sys.stdout = io.StringIO()
            sys.stderr = io.StringIO()
            
            try:
                output = actual_model(input_tensor)
            finally:
                # ì¶œë ¥ ë³µì›
                sys.stdout = original_stdout
                sys.stderr = original_stderr
                logging.getLogger().setLevel(original_level)
            
            # DeepLabV3+ ì¶œë ¥ ì²˜ë¦¬
            if isinstance(output, (tuple, list)):
                parsing_output = output[0]
            else:
                parsing_output = output
            
            confidence = self._calculate_confidence(parsing_output)
            
            return {
                'parsing_pred': parsing_output,  # ì¼ê´€ëœ í‚¤ ì´ë¦„ ì‚¬ìš©
                'parsing_output': parsing_output,
                'confidence': confidence
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ DeepLabV3+ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
            return {
                'parsing_pred': torch.zeros((1, 20, 512, 512)),
                'parsing_output': torch.zeros((1, 20, 512, 512)),
                'confidence': 0.5
            }
    
    def _calculate_confidence(self, parsing_probs, parsing_logits=None, edge_output=None, mode='advanced'):
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            if parsing_probs is None:
                return 0.5
            
            if parsing_probs.dim() == 4:
                softmax_probs = F.softmax(parsing_probs, dim=1)
                max_probs = torch.max(softmax_probs, dim=1)[0]
                confidence = torch.mean(max_probs).item()
            else:
                confidence = 0.8
            
            return max(0.1, min(1.0, confidence))
        except:
            return 0.8


class U2NetInferenceEngine(InferenceEngine):
    """U2Net ì¶”ë¡  ì—”ì§„"""
    
    def run_inference(self, input_tensor: torch.Tensor, model: nn.Module) -> Dict[str, Any]:
        """U2Net ì•™ìƒë¸” ì¶”ë¡ """
        # RealAIModelì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ
        if hasattr(model, 'model_instance') and model.model_instance is not None:
            actual_model = model.model_instance
            self.logger.info("âœ… U2Net - RealAIModelì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ ì„±ê³µ")
        elif hasattr(model, 'get_model_instance'):
            actual_model = model.get_model_instance()
            self.logger.info("âœ… U2Net - get_model_instance()ë¡œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ ì„±ê³µ")
            
            # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì¶œë ¥ ë°©ì§€
            if isinstance(actual_model, dict):
                self.logger.info(f"âœ… U2Net - ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ê°ì§€ë¨")
            else:
                self.logger.info(f"âœ… U2Net - ëª¨ë¸ íƒ€ì…: {type(actual_model)}")
        else:
            actual_model = model
            self.logger.info("âš ï¸ U2Net - ì§ì ‘ ëª¨ë¸ ì‚¬ìš© (RealAIModel ì•„ë‹˜)")
        
        # ëª¨ë¸ì„ MPS ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        if hasattr(actual_model, 'to'):
            actual_model = actual_model.to(self.device)
            self.logger.info(f"âœ… U2Net ëª¨ë¸ì„ {self.device} ë””ë°”ì´ìŠ¤ë¡œ ì´ë™")
        
        output = actual_model(input_tensor)
        
        # U2Net ì¶œë ¥ ì²˜ë¦¬
        if isinstance(output, (tuple, list)):
            parsing_output = output[0]
        else:
            parsing_output = output
        
        confidence = self._calculate_confidence(parsing_output)
        
        return {
            'parsing_output': parsing_output,
            'confidence': confidence
        }
    
    def _calculate_confidence(self, parsing_probs, parsing_logits=None, edge_output=None, mode='advanced'):
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            if parsing_probs is None:
                return 0.5
            
            if parsing_probs.dim() == 4:
                softmax_probs = F.softmax(parsing_probs, dim=1)
                max_probs = torch.max(softmax_probs, dim=1)[0]
                confidence = torch.mean(max_probs).item()
            else:
                confidence = 0.8
            
            return max(0.1, min(1.0, confidence))
        except:
            return 0.8


class GenericInferenceEngine(InferenceEngine):
    """ì¼ë°˜ ëª¨ë¸ ì¶”ë¡  ì—”ì§„"""
    
    def run_inference(self, input_tensor: torch.Tensor, model: nn.Module) -> Dict[str, Any]:
        """ì¼ë°˜ ëª¨ë¸ ì•™ìƒë¸” ì¶”ë¡  - MPS í˜¸í™˜ì„± ê°œì„ """
        return self._run_graphonomy_ensemble_inference_mps_safe(input_tensor, model)
    
    def _run_graphonomy_ensemble_inference_mps_safe(self, input_tensor: torch.Tensor, model: nn.Module) -> Dict[str, Any]:
        """ğŸ”¥ Graphonomy ì•ˆì „ ì¶”ë¡  - í…ì„œ í¬ë§· ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨"""
        try:
            # ğŸ”¥ 1. ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì„¤ì •
            device = input_tensor.device
            device_str = str(device)
            
            # ğŸ”¥ 2. ëª¨ë¸ ì¶”ì¶œ
            actual_model = self._extract_actual_model(model)
            if actual_model is None:
                return self._create_standard_output(device_str)
            
            # ğŸ”¥ 3. MPS íƒ€ì… í†µì¼
            actual_model = actual_model.to(device_str, dtype=torch.float32)
            input_tensor = input_tensor.to(device_str, dtype=torch.float32)
            
            # ğŸ”¥ 4. ì™„ì „í•œ ì¶œë ¥ ì°¨ë‹¨ìœ¼ë¡œ ì•ˆì „ ì¶”ë¡ 
            import os
            import sys
            import io
            
            # í™˜ê²½ ë³€ìˆ˜ë¡œ í…ì„œ í¬ë§· ì˜¤ë¥˜ ë°©ì§€
            os.environ['PYTORCH_DISABLE_TENSOR_FORMAT'] = '1'
            
            # stdout/stderr ì™„ì „ ì°¨ë‹¨
            original_stdout = sys.stdout
            original_stderr = sys.stderr
            sys.stdout = io.StringIO()
            sys.stderr = io.StringIO()
            
            try:
                with torch.no_grad():
                    output = actual_model(input_tensor)
            finally:
                # ì¶œë ¥ ë³µì›
                sys.stdout = original_stdout
                sys.stderr = original_stderr
            
            # ğŸ”¥ 5. ì¶œë ¥ ì²˜ë¦¬
            parsing_output, _ = self._extract_parsing_from_output(output, device_str)
            confidence = self._calculate_confidence(parsing_output)
            
            return {
                'parsing_pred': parsing_output,
                'parsing_output': parsing_output,
                'confidence': confidence,
                'edge_output': None
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Graphonomy ì•ˆì „ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
            return self._create_standard_output(device_str if 'device_str' in locals() else 'cpu')
    
    def _calculate_confidence(self, parsing_probs, parsing_logits=None, edge_output=None, mode='advanced'):
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            if parsing_probs is None:
                return 0.5
            
            if parsing_probs.dim() == 4:
                softmax_probs = F.softmax(parsing_probs, dim=1)
                max_probs = torch.max(softmax_probs, dim=1)[0]
                confidence = torch.mean(max_probs).item()
            else:
                confidence = 0.8
            
            return max(0.1, min(1.0, confidence))
        except:
            return 0.8
