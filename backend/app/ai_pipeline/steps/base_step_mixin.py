# backend/app/ai_pipeline/steps/base_step_mixin.py
"""
ğŸ”¥ BaseStepMixin v12.0 - ì™„ì „í•œ ë¦¬íŒ©í† ë§ (í•µì‹¬ ê¸°ëŠ¥ ì§‘ì¤‘)
================================================================

âœ… 5ë‹¨ê³„ ê°„ë‹¨í•œ ì´ˆê¸°í™” (17ë‹¨ê³„ â†’ 5ë‹¨ê³„)
âœ… ëª¨ë“  í•„ìˆ˜ ë©”ì„œë“œ ì™„ì „ êµ¬í˜„
âœ… ModelLoader ì—°ë™ (89.8GB ì²´í¬í¬ì¸íŠ¸ í™œìš©)
âœ… ì˜ì¡´ì„± ì£¼ì… (DI Container) ì§€ì›
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²° (coroutine ê²½ê³  ì—†ìŒ)
âœ… Stepë³„ íŠ¹í™” Mixin (8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸)
âœ… ê¹”ë”í•œ ì•„í‚¤í…ì²˜ (Clean Architecture)
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

í•µì‹¬ ê¸°ëŠ¥:
- AI ëª¨ë¸ ì—°ë™ (get_model, get_model_async)
- ë©”ëª¨ë¦¬ ìµœì í™” (optimize_memory)
- ì›Œë°ì—… ì‹œìŠ¤í…œ (warmup, warmup_async)
- ìƒíƒœ ê´€ë¦¬ (get_status, get_performance_summary)
- ë¹„ë™ê¸° ì§€ì› (ëª¨ë“  ì£¼ìš” ë©”ì„œë“œ)
- ì˜ì¡´ì„± ì£¼ì… (ModelLoader, MemoryManager ë“±)

Author: MyCloset AI Team
Date: 2025-07-22
Version: 12.0 (Complete Refactoring)
"""

# ==============================================
# ğŸ”¥ 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import
# ==============================================
import os
import gc
import time
import asyncio
import logging
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple, TYPE_CHECKING
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from functools import wraps
import platform
import subprocess
import psutil
from datetime import datetime
from enum import Enum

# ==============================================
# ğŸ”¥ 2. conda í™˜ê²½ ìš°ì„  ì²´í¬
# ==============================================
import sys

CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'python_path': sys.executable
}

if CONDA_INFO['conda_env'] != 'none':
    print(f"âœ… conda í™˜ê²½ ê°ì§€: {CONDA_INFO['conda_env']}")
else:
    print("âš ï¸ conda í™˜ê²½ ê¶Œì¥: conda activate mycloset-ai")

# ==============================================
# ğŸ”¥ 3. ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import
# ==============================================

# GPU ì„¤ì • ì•ˆì „ import
try:
    from app.core.gpu_config import safe_mps_empty_cache
except ImportError:
    def safe_mps_empty_cache():
        gc.collect()
        return {"success": True, "method": "fallback_gc"}

# PyTorch ì•ˆì „ Import (MPS í´ë°± ì„¤ì •)
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    TORCH_AVAILABLE = True
    print(f"ğŸ”¥ PyTorch {torch.__version__} ë¡œë“œ ì™„ë£Œ")
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        print("ğŸ M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
    
except ImportError:
    print("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch torchvision torchaudio -c pytorch")

# NumPy ì•ˆì „ Import (2.x í˜¸í™˜ì„±)
NUMPY_AVAILABLE = False
try:
    import numpy as np
    numpy_version = np.__version__
    major_version = int(numpy_version.split('.')[0])
    
    if major_version >= 2:
        print(f"âš ï¸ NumPy {numpy_version} - conda install numpy=1.24.3 ê¶Œì¥")
        try:
            np.set_printoptions(legacy='1.25')
        except:
            pass
    
    NUMPY_AVAILABLE = True
    print(f"ğŸ“Š NumPy {numpy_version} ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ NumPy ì—†ìŒ - conda install numpy")

# PIL ì•ˆì „ Import
PIL_AVAILABLE = False
try:
    from PIL import Image, ImageEnhance, ImageFilter
    PIL_AVAILABLE = True
    print("ğŸ–¼ï¸ PIL ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ PIL ì—†ìŒ - conda install pillow")

# ==============================================
# ğŸ”¥ 4. TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ë°©ì§€
# ==============================================
if TYPE_CHECKING:
    from ..interfaces.model_interface import IModelLoader, IStepInterface
    from ..interfaces.memory_interface import IMemoryManager
    from ..interfaces.data_interface import IDataConverter
    from ...core.di_container import DIContainer

# ==============================================
# ğŸ”¥ 5. ê°„ë‹¨í•œ ì„¤ì • í´ë˜ìŠ¤
# ==============================================
@dataclass
class StepConfig:
    """ê°„ë‹¨í•œ Step ì„¤ì • í´ë˜ìŠ¤"""
    step_name: str = "BaseStep"
    step_number: int = 0
    step_type: str = "base"
    device: str = "auto"
    use_fp16: bool = True
    batch_size: int = 1
    input_size: Tuple[int, int] = (512, 512)
    output_size: Tuple[int, int] = (512, 512)
    confidence_threshold: float = 0.8
    optimization_enabled: bool = True
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True

    def update(self, **kwargs):
        """ì„¤ì • ì—…ë°ì´íŠ¸"""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)

# ==============================================
# ğŸ”¥ 6. ì˜ì¡´ì„± ì£¼ì… ë„ìš°ë¯¸
# ==============================================
class DIHelper:
    """ì˜ì¡´ì„± ì£¼ì… ë„ìš°ë¯¸"""
    
    @staticmethod
    def get_di_container():
        """DI Container ê°€ì ¸ì˜¤ê¸°"""
        try:
            from ...core.di_container import get_di_container
            return get_di_container()
        except ImportError:
            return None
        except Exception:
            return None
    
    @staticmethod
    def inject_dependencies(instance) -> Dict[str, bool]:
        """ì˜ì¡´ì„± ì£¼ì… ì‹¤í–‰"""
        results = {}
        
        try:
            container = DIHelper.get_di_container()
            
            # ModelLoader ì£¼ì…
            try:
                if container:
                    model_loader = container.get('IModelLoader')
                    if model_loader:
                        instance.model_loader = model_loader
                        results['model_loader'] = True
                    else:
                        # í´ë°±: ì§ì ‘ import
                        from ..utils.model_loader import get_global_model_loader
                        instance.model_loader = get_global_model_loader()
                        results['model_loader'] = instance.model_loader is not None
                else:
                    from ..utils.model_loader import get_global_model_loader
                    instance.model_loader = get_global_model_loader()
                    results['model_loader'] = instance.model_loader is not None
            except Exception:
                instance.model_loader = None
                results['model_loader'] = False
            
            # MemoryManager ì£¼ì…
            try:
                if container:
                    memory_manager = container.get('IMemoryManager')
                    instance.memory_manager = memory_manager
                    results['memory_manager'] = memory_manager is not None
                else:
                    instance.memory_manager = None
                    results['memory_manager'] = False
            except Exception:
                instance.memory_manager = None
                results['memory_manager'] = False
            
            # DataConverter ì£¼ì…
            try:
                if container:
                    data_converter = container.get('IDataConverter')
                    instance.data_converter = data_converter
                    results['data_converter'] = data_converter is not None
                else:
                    instance.data_converter = None
                    results['data_converter'] = False
            except Exception:
                instance.data_converter = None
                results['data_converter'] = False
            
            return results
            
        except Exception as e:
            logging.warning(f"âš ï¸ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return {
                'model_loader': False,
                'memory_manager': False,
                'data_converter': False
            }

# ==============================================
# ğŸ”¥ 7. ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ
# ==============================================
class MemoryOptimizer:
    """ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìµœì í™”"""
    
    def __init__(self, device: str = "mps"):
        self.device = device
        self.is_m3_max = self._detect_m3_max()
        self.logger = logging.getLogger(f"{__name__}.MemoryOptimizer")
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def optimize(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰"""
        try:
            start_time = time.time()
            results = []
            
            # Python GC
            before_objects = len(gc.get_objects())
            gc.collect()
            after_objects = len(gc.get_objects())
            freed = before_objects - after_objects
            results.append(f"Python GC: {freed}ê°œ ê°ì²´ í•´ì œ")
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    results.append("CUDA ìºì‹œ ì •ë¦¬")
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        safe_mps_empty_cache()
                        results.append("MPS ìºì‹œ ì •ë¦¬")
                    except Exception as e:
                        results.append(f"MPS ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # M3 Max íŠ¹ë³„ ìµœì í™”
            if self.is_m3_max and aggressive:
                for _ in range(3):
                    gc.collect()
                results.append("M3 Max í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”")
            
            duration = time.time() - start_time
            
            return {
                "success": True,
                "duration": duration,
                "results": results,
                "device": self.device,
                "is_m3_max": self.is_m3_max,
                "timestamp": time.time()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "timestamp": time.time()
            }
    
    async def optimize_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, lambda: self.optimize(aggressive))
            return result
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "timestamp": time.time()
            }

# ==============================================
# ğŸ”¥ 8. ë¹„ë™ê¸° ì•ˆì „ ë˜í¼
# ==============================================
def safe_async_wrapper(func):
    """ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ë˜í•‘"""
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        try:
            # ì´ë²¤íŠ¸ ë£¨í”„ í™•ì¸
            try:
                loop = asyncio.get_running_loop()
                # ì´ë²¤íŠ¸ ë£¨í”„ ë‚´ì—ì„œëŠ” ë™ê¸° ì‹¤í–‰
                return self._sync_fallback(func.__name__, *args, **kwargs)
            except RuntimeError:
                # ì´ë²¤íŠ¸ ë£¨í”„ ë°–ì—ì„œëŠ” ë¹„ë™ê¸° ì‹¤í–‰
                return asyncio.run(func(self, *args, **kwargs))
        
        except Exception as e:
            logger = getattr(self, 'logger', logging.getLogger(self.__class__.__name__))
            logger.warning(f"âš ï¸ {func.__name__} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return self._sync_fallback(func.__name__, *args, **kwargs)
    
    return wrapper

# ==============================================
# ğŸ”¥ 9. ë©”ì¸ BaseStepMixin í´ë˜ìŠ¤
# ==============================================
class BaseStepMixin:
    """
    ğŸ”¥ BaseStepMixin v12.0 - ì™„ì „í•œ ë¦¬íŒ©í† ë§
    
    âœ… 5ë‹¨ê³„ ê°„ë‹¨í•œ ì´ˆê¸°í™”
    âœ… ëª¨ë“  í•„ìˆ˜ ë©”ì„œë“œ êµ¬í˜„
    âœ… ModelLoader ì—°ë™
    âœ… ì˜ì¡´ì„± ì£¼ì… ì§€ì›
    âœ… M3 Max ìµœì í™”
    âœ… conda í™˜ê²½ ìš°ì„ 
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²°
    """
    
    # í´ë˜ìŠ¤ ë³€ìˆ˜
    _class_registry = weakref.WeakSet()
    _initialization_lock = threading.RLock()
    
    def __init__(self, **kwargs):
        """5ë‹¨ê³„ ê°„ë‹¨í•œ ì´ˆê¸°í™”"""
        
        with BaseStepMixin._initialization_lock:
            try:
                # STEP 1: Logger ì„¤ì •
                self._setup_logger(kwargs)
                
                # STEP 2: ê¸°ë³¸ ì„¤ì •
                self._setup_config(kwargs)
                
                # STEP 3: ë””ë°”ì´ìŠ¤ ë° ì‹œìŠ¤í…œ ì„¤ì •
                self._setup_device_and_system()
                
                # STEP 4: ì˜ì¡´ì„± ì£¼ì…
                self._setup_dependencies()
                
                # STEP 5: ì™„ë£Œ
                self._finalize_initialization()
                
                self.logger.info(f"âœ… {self.config.step_name} BaseStepMixin v12.0 ì´ˆê¸°í™” ì™„ë£Œ")
                
            except Exception as e:
                self._emergency_initialization(e)
                if hasattr(self, 'logger'):
                    self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ì´ˆê¸°í™” ë©”ì„œë“œë“¤ (5ë‹¨ê³„)
    # ==============================================
    
    def _setup_logger(self, kwargs: Dict[str, Any]):
        """STEP 1: Logger ì„¤ì •"""
        try:
            step_name = kwargs.get('step_name', self.__class__.__name__)
            logger_name = f"pipeline.steps.{step_name}"
            
            self.logger = logging.getLogger(logger_name)
            
            if not self.logger.handlers:
                formatter = logging.Formatter(
                    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
                )
                handler = logging.StreamHandler()
                handler.setFormatter(formatter)
                self.logger.addHandler(handler)
                self.logger.setLevel(logging.INFO)
            
        except Exception as e:
            print(f"âŒ logger ì„¤ì • ì‹¤íŒ¨: {e}")
            self.logger = logging.getLogger("emergency_logger")
    
    def _setup_config(self, kwargs: Dict[str, Any]):
        """STEP 2: ê¸°ë³¸ ì„¤ì •"""
        try:
            # ê¸°ë³¸ ì„¤ì • ìƒì„±
            self.config = StepConfig()
            self.config.update(**kwargs)
            
            # ê¸°ë³¸ ì†ì„±ë“¤
            self.step_name = self.config.step_name
            self.step_number = self.config.step_number
            self.step_type = self.config.step_type
            
            # ìƒíƒœ í”Œë˜ê·¸ë“¤
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # ì²˜ë¦¬ ê´€ë ¨
            self.total_processing_count = 0
            self.error_count = 0
            self.last_error = None
            self.last_processing_time = None
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            self.performance_metrics = {
                'process_count': 0,
                'total_process_time': 0.0,
                'average_process_time': 0.0,
                'operations': {},
                'error_history': []
            }
            
            # ìƒíƒœ
            self.state = {
                'status': 'initializing',
                'last_update': time.time(),
                'metrics': {},
                'errors': [],
                'warnings': []
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _setup_device_and_system(self):
        """STEP 3: ë””ë°”ì´ìŠ¤ ë° ì‹œìŠ¤í…œ ì„¤ì •"""
        try:
            # ë””ë°”ì´ìŠ¤ ê°ì§€
            if self.config.device == "auto":
                self.device = self._detect_optimal_device()
            else:
                self.device = self.config.device
            
            # M3 Max ê°ì§€
            self.is_m3_max = self._detect_m3_max()
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            self.memory_gb = self._get_memory_info()
            
            # M3 Max íŠ¹í™” ì„¤ì •
            if self.is_m3_max:
                if self.memory_gb >= 64:
                    self.max_model_size_gb = min(40, self.memory_gb * 0.3)
                else:
                    self.max_model_size_gb = min(20, self.memory_gb * 0.25)
                self.logger.info(f"ğŸ M3 Max {self.memory_gb}GB, ìµœëŒ€ ëª¨ë¸: {self.max_model_size_gb}GB")
            else:
                self.max_model_size_gb = min(16, self.memory_gb * 0.2)
            
            # ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ
            self.memory_optimizer = MemoryOptimizer(self.device)
            
            # ìµœì í™” ì„¤ì •
            self.use_fp16 = self.config.use_fp16 and self.device != 'cpu'
            self.precision = 'fp16' if self.use_fp16 else 'fp32'
            
            # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™”
            if self.device == "mps" and MPS_AVAILABLE:
                self._setup_mps_optimizations()
            elif self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                self._setup_cuda_optimizations()
            
        except Exception as e:
            self.logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.device = "cpu"
            self.is_m3_max = False
            self.memory_gb = 16.0
            self.memory_optimizer = MemoryOptimizer("cpu")
    
    def _setup_dependencies(self):
        """STEP 4: ì˜ì¡´ì„± ì£¼ì…"""
        try:
            # ì˜ì¡´ì„± ì£¼ì… ì‹¤í–‰
            injection_results = DIHelper.inject_dependencies(self)
            
            # ê²°ê³¼ í™•ì¸
            successful_deps = [dep for dep, success in injection_results.items() if success]
            failed_deps = [dep for dep, success in injection_results.items() if not success]
            
            if successful_deps:
                self.logger.info(f"âœ… ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {', '.join(successful_deps)}")
            
            if failed_deps:
                self.logger.warning(f"âš ï¸ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {', '.join(failed_deps)}")
            
            # Step Interface ìƒì„±
            if hasattr(self, 'model_loader') and self.model_loader:
                try:
                    if hasattr(self.model_loader, 'create_step_interface'):
                        self.step_interface = self.model_loader.create_step_interface(self.step_name)
                        if self.step_interface:
                            self.logger.info("âœ… Step Interface ìƒì„± ì„±ê³µ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Step Interface ìƒì„± ì‹¤íŒ¨: {e}")
                    self.step_interface = None
            else:
                self.step_interface = None
            
            # DI ìƒíƒœ ì„¤ì •
            self.di_available = sum(1 for success in injection_results.values() if success) > 0
            
            # ëª¨ë¸ ê´€ë ¨ ì†ì„± ì´ˆê¸°í™”
            self._ai_model = None
            self._ai_model_name = None
            self.loaded_models = {}
            self.model_cache = {}
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            self.step_interface = None
            self.di_available = False
    
    def _finalize_initialization(self):
        """STEP 5: ìµœì¢… ì™„ë£Œ"""
        try:
            # í´ë˜ìŠ¤ ë“±ë¡
            BaseStepMixin._class_registry.add(self)
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            self.state['status'] = 'initialized'
            self.state['last_update'] = time.time()
            self.is_initialized = True
            
            # ìë™ ì›Œë°ì—… (ì„¤ì •ëœ ê²½ìš°)
            if self.config.auto_warmup:
                try:
                    warmup_result = self.warmup()
                    if warmup_result.get('success', False):
                        self.warmup_completed = True
                        self.is_ready = True
                        self.logger.info("ğŸ”¥ ìë™ ì›Œë°ì—… ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìë™ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.error(f"âŒ ìµœì¢… ì™„ë£Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _emergency_initialization(self, original_error: Exception = None):
        """ê¸´ê¸‰ ì´ˆê¸°í™”"""
        try:
            self.step_name = getattr(self, 'step_name', self.__class__.__name__)
            self.device = "cpu"
            self.is_m3_max = False
            self.memory_gb = 16.0
            self.error_count = 1
            self.last_error = str(original_error) if original_error else "Emergency initialization"
            
            # ê¸°ë³¸ ì„¤ì •
            self.config = StepConfig()
            self.config.step_name = self.step_name
            
            # ìƒíƒœ í”Œë˜ê·¸ë“¤
            self.is_initialized = False
            self.is_ready = False
            self.di_available = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # ê¸°ë³¸ ë©”íŠ¸ë¦­
            self.performance_metrics = {'process_count': 0, 'error_history': [str(original_error)] if original_error else []}
            self.state = {'status': 'emergency', 'last_update': time.time(), 'errors': [f"Emergency: {original_error}"] if original_error else []}
            
            # ì˜ì¡´ì„±ë“¤ì„ Noneìœ¼ë¡œ ì´ˆê¸°í™”
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            self.step_interface = None
            self.memory_optimizer = MemoryOptimizer("cpu")
            
            # ëª¨ë¸ ê´€ë ¨
            self._ai_model = None
            self._ai_model_name = None
            self.loaded_models = {}
            self.model_cache = {}
            
            # Logger í™•ì¸
            if not hasattr(self, 'logger') or self.logger is None:
                self.logger = logging.getLogger("emergency_logger")
            
            if hasattr(self, 'logger'):
                self.logger.error(f"ğŸš¨ {self.step_name} ê¸´ê¸‰ ì´ˆê¸°í™” ì‹¤í–‰")
                if original_error:
                    self.logger.error(f"ğŸš¨ ì›ë³¸ ì˜¤ë¥˜: {original_error}")
            
        except Exception as e:
            print(f"ğŸš¨ ê¸´ê¸‰ ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ë””ë°”ì´ìŠ¤ ê´€ë ¨ ë©”ì„œë“œë“¤
    # ==============================================
    
    def _detect_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            if TORCH_AVAILABLE:
                if MPS_AVAILABLE:
                    return "mps"
                elif hasattr(torch, 'cuda') and torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        except:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def _get_memory_info(self) -> float:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        try:
            memory = psutil.virtual_memory()
            return memory.total / 1024**3
        except:
            return 16.0
    
    def _setup_mps_optimizations(self):
        """MPS ìµœì í™” ì„¤ì •"""
        try:
            self.mps_optimizations = {
                'fallback_enabled': True,
                'memory_fraction': 0.8,
                'precision': self.precision
            }
            self.logger.debug("ğŸ MPS ìµœì í™” ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ MPS ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _setup_cuda_optimizations(self):
        """CUDA ìµœì í™” ì„¤ì •"""
        try:
            self.cuda_optimizations = {
                'memory_fraction': 0.9,
                'allow_tf32': True,
                'benchmark': True
            }
            
            if TORCH_AVAILABLE:
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
            
            self.logger.debug("ğŸš€ CUDA ìµœì í™” ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ CUDA ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ í•µì‹¬ ë©”ì„œë“œë“¤ (í•„ìˆ˜)
    # ==============================================
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ë™ê¸°) - í•„ìˆ˜ ë©”ì„œë“œ"""
        try:
            # ìºì‹œ í™•ì¸
            cache_key = model_name or "default"
            if cache_key in self.model_cache:
                return self.model_cache[cache_key]
            
            model = None
            
            # ModelLoaderë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ
            if hasattr(self, 'model_loader') and self.model_loader:
                try:
                    model = self.model_loader.get_model(model_name or "default")
                except Exception as e:
                    self.logger.debug(f"ModelLoader.get_model ì‹¤íŒ¨: {e}")
            
            # Step ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ
            if model is None and hasattr(self, 'step_interface') and self.step_interface:
                try:
                    model = self.step_interface.get_model(model_name)
                except Exception as e:
                    self.logger.debug(f"step_interface.get_model ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ì§ì ‘ import
            if model is None:
                try:
                    from ..utils.model_loader import get_global_model_loader
                    loader = get_global_model_loader()
                    if loader:
                        model = loader.get_model(model_name or "default")
                except Exception as e:
                    self.logger.debug(f"í´ë°± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ìºì‹œì— ì €ì¥
            if model is not None:
                self.model_cache[cache_key] = model
                self.has_model = True
                self.model_loaded = True
                self.logger.debug(f"âœ… ëª¨ë¸ ìºì‹œ ì €ì¥: {cache_key}")
            
            return model
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸°) - í•„ìˆ˜ ë©”ì„œë“œ"""
        try:
            # ìºì‹œ í™•ì¸
            cache_key = model_name or "default"
            if cache_key in self.model_cache:
                return self.model_cache[cache_key]
            
            model = None
            
            # ModelLoaderë¥¼ í†µí•œ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ
            if hasattr(self, 'model_loader') and self.model_loader:
                try:
                    if hasattr(self.model_loader, 'get_model_async'):
                        model = await self.model_loader.get_model_async(model_name or "default")
                    else:
                        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                        loop = asyncio.get_event_loop()
                        model = await loop.run_in_executor(
                            None, 
                            lambda: self.model_loader.get_model(model_name or "default")
                        )
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° ModelLoader ì‹¤íŒ¨: {e}")
            
            # Step ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ
            if model is None and hasattr(self, 'step_interface') and self.step_interface:
                try:
                    if hasattr(self.step_interface, 'get_model_async'):
                        model = await self.step_interface.get_model_async(model_name)
                    else:
                        loop = asyncio.get_event_loop()
                        model = await loop.run_in_executor(
                            None, 
                            lambda: self.step_interface.get_model(model_name)
                        )
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° step_interface ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ì§ì ‘ import (ë¹„ë™ê¸°)
            if model is None:
                try:
                    from ..utils.model_loader import get_global_model_loader
                    loader = get_global_model_loader()
                    if loader:
                        if hasattr(loader, 'get_model_async'):
                            model = await loader.get_model_async(model_name or "default")
                        else:
                            loop = asyncio.get_event_loop()
                            model = await loop.run_in_executor(
                                None, 
                                lambda: loader.get_model(model_name or "default")
                            )
                except Exception as e:
                    self.logger.debug(f"í´ë°± ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ìºì‹œì— ì €ì¥
            if model is not None:
                self.model_cache[cache_key] = model
                self.has_model = True
                self.model_loaded = True
                self.logger.debug(f"âœ… ë¹„ë™ê¸° ëª¨ë¸ ìºì‹œ ì €ì¥: {cache_key}")
            
            return model
                
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (ë™ê¸°) - í•„ìˆ˜ ë©”ì„œë“œ"""
        try:
            # DI MemoryManager ì‚¬ìš©
            if hasattr(self, 'memory_manager') and self.memory_manager:
                try:
                    result = self.memory_manager.optimize_memory(aggressive=aggressive)
                    if result.get('success', False):
                        return result
                except Exception as e:
                    self.logger.debug(f"DI MemoryManager ì‹¤íŒ¨: {e}")
            
            # ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” ì‚¬ìš©
            if hasattr(self, 'memory_optimizer') and self.memory_optimizer:
                return self.memory_optimizer.optimize(aggressive=aggressive)
            
            # ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            before_objects = len(gc.get_objects())
            gc.collect()
            after_objects = len(gc.get_objects())
            
            return {
                "success": True,
                "message": "ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ",
                "objects_freed": before_objects - after_objects,
                "timestamp": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def optimize_memory_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (ë¹„ë™ê¸°) - í•„ìˆ˜ ë©”ì„œë“œ"""
        try:
            # DI MemoryManager ì‚¬ìš©
            if hasattr(self, 'memory_manager') and self.memory_manager:
                try:
                    if hasattr(self.memory_manager, 'optimize_memory_async'):
                        result = await self.memory_manager.optimize_memory_async(aggressive=aggressive)
                        if result.get('success', False):
                            return result
                    else:
                        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                        loop = asyncio.get_event_loop()
                        result = await loop.run_in_executor(
                            None, 
                            lambda: self.memory_manager.optimize_memory(aggressive=aggressive)
                        )
                        if result.get('success', False):
                            return result
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° DI MemoryManager ì‹¤íŒ¨: {e}")
            
            # ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” ì‚¬ìš©
            if hasattr(self, 'memory_optimizer') and self.memory_optimizer:
                return await self.memory_optimizer.optimize_async(aggressive=aggressive)
            
            # í´ë°±: ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, lambda: self.optimize_memory(aggressive))
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def warmup(self) -> Dict[str, Any]:
        """ì›Œë°ì—… ì‹¤í–‰ (ë™ê¸°) - í•„ìˆ˜ ë©”ì„œë“œ"""
        try:
            if self.warmup_completed:
                return {
                    'success': True,
                    'message': 'ì´ë¯¸ ì›Œë°ì—… ì™„ë£Œë¨',
                    'cached': True
                }
            
            self.logger.info(f"ğŸ”¥ {self.step_name} ì›Œë°ì—… ì‹œì‘...")
            
            start_time = time.time()
            results = []
            
            # 1. ë©”ëª¨ë¦¬ ì›Œë°ì—…
            try:
                memory_result = self.optimize_memory()
                if memory_result.get('success', False):
                    results.append('memory_success')
                else:
                    results.append('memory_failed')
            except Exception as e:
                self.logger.debug(f"ë©”ëª¨ë¦¬ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
                results.append('memory_failed')
            
            # 2. ëª¨ë¸ ì›Œë°ì—…
            try:
                if hasattr(self, 'model_loader') and self.model_loader:
                    test_model = self.get_model("warmup_test")
                    if test_model:
                        results.append('model_success')
                    else:
                        results.append('model_skipped')
                else:
                    results.append('model_skipped')
            except Exception as e:
                self.logger.debug(f"ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
                results.append('model_failed')
            
            # 3. ë””ë°”ì´ìŠ¤ ì›Œë°ì—…
            try:
                if TORCH_AVAILABLE:
                    test_tensor = torch.randn(10, 10)
                    if self.device != 'cpu':
                        test_tensor = test_tensor.to(self.device)
                    result = torch.matmul(test_tensor, test_tensor.t())
                    results.append('device_success')
                else:
                    results.append('device_skipped')
            except Exception as e:
                self.logger.debug(f"ë””ë°”ì´ìŠ¤ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
                results.append('device_failed')
            
            # 4. Stepë³„ íŠ¹í™” ì›Œë°ì—…
            try:
                if hasattr(self, '_step_specific_warmup'):
                    self._step_specific_warmup()
                    results.append('step_specific_success')
                else:
                    results.append('step_specific_skipped')
            except Exception as e:
                self.logger.debug(f"Stepë³„ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
                results.append('step_specific_failed')
            
            duration = time.time() - start_time
            success_count = sum(1 for r in results if 'success' in r)
            total_count = len(results)
            overall_success = success_count > 0
            
            if overall_success:
                self.warmup_completed = True
                self.is_ready = True
            
            self.logger.info(f"ğŸ”¥ ì›Œë°ì—… ì™„ë£Œ: {success_count}/{total_count} ì„±ê³µ ({duration:.2f}ì´ˆ)")
            
            return {
                "success": overall_success,
                "duration": duration,
                "results": results,
                "success_count": success_count,
                "total_count": total_count,
                "step_name": self.step_name
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    @safe_async_wrapper
    async def warmup_async(self) -> Dict[str, Any]:
        """ì›Œë°ì—… ì‹¤í–‰ (ë¹„ë™ê¸°) - í•„ìˆ˜ ë©”ì„œë“œ"""
        try:
            if self.warmup_completed:
                return {
                    'success': True,
                    'message': 'ì´ë¯¸ ì›Œë°ì—… ì™„ë£Œë¨',
                    'cached': True
                }
            
            self.logger.info(f"ğŸ”¥ {self.step_name} ë¹„ë™ê¸° ì›Œë°ì—… ì‹œì‘...")
            
            start_time = time.time()
            results = []
            
            # 1. ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì›Œë°ì—…
            try:
                memory_result = await self.optimize_memory_async()
                if memory_result.get('success', False):
                    results.append('memory_async_success')
                else:
                    results.append('memory_async_failed')
            except Exception as e:
                self.logger.debug(f"ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
                results.append('memory_async_failed')
            
            # 2. ë¹„ë™ê¸° ëª¨ë¸ ì›Œë°ì—…
            try:
                if hasattr(self, 'model_loader') and self.model_loader:
                    test_model = await self.get_model_async("warmup_test")
                    if test_model:
                        results.append('model_async_success')
                    else:
                        results.append('model_async_skipped')
                else:
                    results.append('model_async_skipped')
            except Exception as e:
                self.logger.debug(f"ë¹„ë™ê¸° ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
                results.append('model_async_failed')
            
            # 3. ë¹„ë™ê¸° ë””ë°”ì´ìŠ¤ ì›Œë°ì—…
            try:
                if TORCH_AVAILABLE:
                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(None, self._device_warmup_sync)
                    results.append('device_async_success')
                else:
                    results.append('device_async_skipped')
            except Exception as e:
                self.logger.debug(f"ë¹„ë™ê¸° ë””ë°”ì´ìŠ¤ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
                results.append('device_async_failed')
            
            # 4. ë¹„ë™ê¸° Stepë³„ íŠ¹í™” ì›Œë°ì—…
            try:
                if hasattr(self, '_step_specific_warmup_async'):
                    await self._step_specific_warmup_async()
                    results.append('step_specific_async_success')
                elif hasattr(self, '_step_specific_warmup'):
                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(None, self._step_specific_warmup)
                    results.append('step_specific_async_success')
                else:
                    results.append('step_specific_async_skipped')
            except Exception as e:
                self.logger.debug(f"ë¹„ë™ê¸° Stepë³„ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
                results.append('step_specific_async_failed')
            
            duration = time.time() - start_time
            success_count = sum(1 for r in results if 'success' in r)
            total_count = len(results)
            overall_success = success_count > 0
            
            if overall_success:
                self.warmup_completed = True
                self.is_ready = True
            
            self.logger.info(f"ğŸ”¥ ë¹„ë™ê¸° ì›Œë°ì—… ì™„ë£Œ: {success_count}/{total_count} ì„±ê³µ ({duration:.2f}ì´ˆ)")
            
            return {
                "success": overall_success,
                "duration": duration,
                "results": results,
                "success_count": success_count,
                "total_count": total_count,
                "step_name": self.step_name,
                "async": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "async": True}
    
    def _sync_fallback(self, method_name: str, *args, **kwargs) -> Dict[str, Any]:
        """ë™ê¸° í´ë°± ì²˜ë¦¬"""
        try:
            if method_name == "warmup_async":
                return self.warmup()
            else:
                return {
                    "success": True,
                    "method": f"sync_fallback_{method_name}",
                    "message": f"{method_name} ë™ê¸° í´ë°± ì‹¤í–‰ ì™„ë£Œ"
                }
        except Exception as e:
            return {
                "success": False,
                "method": f"sync_fallback_{method_name}",
                "error": str(e)
            }
    
    def _device_warmup_sync(self):
        """ë™ê¸° ë””ë°”ì´ìŠ¤ ì›Œë°ì—…"""
        try:
            test_tensor = torch.randn(10, 10)
            if self.device != 'cpu':
                test_tensor = test_tensor.to(self.device)
            result = torch.matmul(test_tensor, test_tensor.t())
            return True
        except:
            return False
    
    def _step_specific_warmup(self):
        """Stepë³„ íŠ¹í™” ì›Œë°ì—… (ê¸°ë³¸ êµ¬í˜„)"""
        pass
    
    async def _step_specific_warmup_async(self):
        """Stepë³„ íŠ¹í™” ì›Œë°ì—… (ë¹„ë™ê¸° ê¸°ë³¸ êµ¬í˜„)"""
        pass
    
    @safe_async_wrapper
    async def warmup_step(self) -> Dict[str, Any]:
        """Step ì›Œë°ì—… (BaseStepMixin í˜¸í™˜ìš©) - í•„ìˆ˜ ë©”ì„œë“œ"""
        return await self.warmup_async()
    
    def get_status(self) -> Dict[str, Any]:
        """Step ìƒíƒœ ì¡°íšŒ - í•„ìˆ˜ ë©”ì„œë“œ"""
        try:
            return {
                'step_name': self.step_name,
                'step_type': self.step_type,
                'step_number': self.step_number,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'has_model': self.has_model,
                'model_loaded': self.model_loaded,
                'warmup_completed': self.warmup_completed,
                'device': self.device,
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb,
                'di_available': self.di_available,
                'error_count': self.error_count,
                'last_error': self.last_error,
                'total_processing_count': self.total_processing_count,
                'last_processing_time': self.last_processing_time,
                'dependencies': {
                    'model_loader': hasattr(self, 'model_loader') and self.model_loader is not None,
                    'memory_manager': hasattr(self, 'memory_manager') and self.memory_manager is not None,
                    'data_converter': hasattr(self, 'data_converter') and self.data_converter is not None,
                    'step_interface': hasattr(self, 'step_interface') and self.step_interface is not None,
                },
                'performance_metrics': self.performance_metrics,
                'state': self.state,
                'config': {
                    'device': self.config.device,
                    'use_fp16': self.config.use_fp16,
                    'batch_size': self.config.batch_size,
                    'input_size': self.config.input_size,
                    'output_size': self.config.output_size,
                    'confidence_threshold': self.config.confidence_threshold,
                    'optimization_enabled': self.config.optimization_enabled,
                    'auto_memory_cleanup': self.config.auto_memory_cleanup,
                    'auto_warmup': self.config.auto_warmup
                },
                'conda_info': CONDA_INFO,
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'step_name': getattr(self, 'step_name', 'unknown'),
                'error': str(e),
                'timestamp': time.time()
            }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ - í•„ìˆ˜ ë©”ì„œë“œ"""
        try:
            return {
                'total_processing_count': self.total_processing_count,
                'last_processing_time': self.last_processing_time,
                'error_count': self.error_count,
                'success_rate': self._calculate_success_rate(),
                'operations': self.performance_metrics.get('operations', {}),
                'average_process_time': self.performance_metrics.get('average_process_time', 0.0),
                'total_process_time': self.performance_metrics.get('total_process_time', 0.0)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_success_rate(self) -> float:
        """ì„±ê³µë¥  ê³„ì‚°"""
        try:
            total = self.total_processing_count
            errors = self.error_count
            if total > 0:
                return (total - errors) / total
            return 0.0
        except:
            return 0.0
    
    def initialize(self) -> bool:
        """ì´ˆê¸°í™” ë©”ì„œë“œ (BaseStepMixin í˜¸í™˜ìš©) - í•„ìˆ˜ ë©”ì„œë“œ"""
        try:
            if self.is_initialized:
                return True
            
            # ì¶”ê°€ ì´ˆê¸°í™” ë¡œì§ì´ í•„ìš”í•œ ê²½ìš° ì—¬ê¸°ì— êµ¬í˜„
            self.is_initialized = True
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_async(self) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™” ë©”ì„œë“œ - í•„ìˆ˜ ë©”ì„œë“œ"""
        try:
            if self.is_initialized:
                return True
            
            # ë¹„ë™ê¸° ì´ˆê¸°í™” ë¡œì§
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, self.initialize)
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    @safe_async_wrapper
    async def cleanup(self) -> Dict[str, Any]:
        """ì •ë¦¬ (ë¹„ë™ê¸°) - í•„ìˆ˜ ë©”ì„œë“œ"""
        try:
            self.logger.info(f"ğŸ§¹ {self.step_name} ì •ë¦¬ ì‹œì‘...")
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            if hasattr(self, 'model_cache'):
                self.model_cache.clear()
            
            if hasattr(self, 'loaded_models'):
                self.loaded_models.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_result = await self.optimize_memory_async(aggressive=True)
            
            # ìƒíƒœ ë¦¬ì…‹
            self.is_ready = False
            self.warmup_completed = False
            
            self.logger.info(f"âœ… {self.step_name} ì •ë¦¬ ì™„ë£Œ")
            
            return {
                "success": True,
                "cleanup_result": cleanup_result,
                "step_name": self.step_name
            }
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def cleanup_models(self):
        """ëª¨ë¸ ì •ë¦¬ - í•„ìˆ˜ ë©”ì„œë“œ"""
        try:
            # Step ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            if hasattr(self, 'step_interface') and self.step_interface:
                if hasattr(self.step_interface, 'cleanup'):
                    self.step_interface.cleanup()
                    
            # ModelLoader ì •ë¦¬
            if hasattr(self, 'model_loader') and self.model_loader:
                if hasattr(self.model_loader, 'cleanup'):
                    self.model_loader.cleanup()
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            if hasattr(self, 'model_cache'):
                self.model_cache.clear()
            
            if hasattr(self, 'loaded_models'):
                self.loaded_models.clear()
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "mps" and MPS_AVAILABLE:
                    try:
                        safe_mps_empty_cache()
                    except Exception:
                        pass
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
                
                gc.collect()
            
            self.has_model = False
            self.model_loaded = False
            self.logger.info(f"ğŸ§¹ {self.step_name} ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ==============================================
    # ğŸ”¥ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def record_processing(self, duration: float, success: bool = True):
        """ì²˜ë¦¬ ê¸°ë¡"""
        try:
            self.total_processing_count += 1
            self.last_processing_time = time.time()
            
            if not success:
                self.error_count += 1
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.performance_metrics['process_count'] = self.total_processing_count
            self.performance_metrics['total_process_time'] += duration
            self.performance_metrics['average_process_time'] = (
                self.performance_metrics['total_process_time'] / self.total_processing_count
            )
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            self.state['last_update'] = time.time()
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²˜ë¦¬ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    def get_di_status(self) -> Dict[str, Any]:
        """DI ìƒíƒœ í™•ì¸"""
        try:
            return {
                'di_available': self.di_available,
                'model_loader': hasattr(self, 'model_loader') and self.model_loader is not None,
                'memory_manager': hasattr(self, 'memory_manager') and self.memory_manager is not None,
                'data_converter': hasattr(self, 'data_converter') and self.data_converter is not None,
                'step_interface': hasattr(self, 'step_interface') and self.step_interface is not None,
            }
        except Exception as e:
            return {'error': str(e)}
    
    def reinject_dependencies(self) -> Dict[str, bool]:
        """ì˜ì¡´ì„± ì¬ì£¼ì…"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} ì˜ì¡´ì„± ì¬ì£¼ì…...")
            return DIHelper.inject_dependencies(self)
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± ì¬ì£¼ì… ì‹¤íŒ¨: {e}")
            return {
                'model_loader': False,
                'memory_manager': False,
                'data_converter': False
            }
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            # ë™ê¸° ì •ë¦¬ë§Œ ìˆ˜í–‰ (coroutine ê²½ê³  ë°©ì§€)
            if hasattr(self, 'model_cache'):
                self.model_cache.clear()
        except:
            pass

# ==============================================
# ğŸ”¥ 10. Stepë³„ íŠ¹í™” Mixinë“¤ (8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸)
# ==============================================

class HumanParsingMixin(BaseStepMixin):
    """Step 1: Human Parsing íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'HumanParsingStep')
        kwargs.setdefault('step_number', 1)
        kwargs.setdefault('step_type', 'human_parsing')
        super().__init__(**kwargs)
        
        self.num_classes = 20
        self.parsing_categories = [
            'background', 'hat', 'hair', 'glove', 'sunglasses', 'upperclothes',
            'dress', 'coat', 'socks', 'pants', 'jumpsuits', 'scarf', 'skirt',
            'face', 'left_arm', 'right_arm', 'left_leg', 'right_leg', 'left_shoe', 'right_shoe'
        ]
        
        self.logger.info(f"ğŸ” Human Parsing Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.num_classes}ê°œ ì¹´í…Œê³ ë¦¬")
    
    def _step_specific_warmup(self):
        """Human Parsing íŠ¹í™” ì›Œë°ì—…"""
        self.logger.debug("ğŸ”¥ Human Parsing íŠ¹í™” ì›Œë°ì—…")
    
    async def _step_specific_warmup_async(self):
        """Human Parsing íŠ¹í™” ì›Œë°ì—… (ë¹„ë™ê¸°)"""
        self.logger.debug("ğŸ”¥ Human Parsing ë¹„ë™ê¸° íŠ¹í™” ì›Œë°ì—…")
        await asyncio.sleep(0.001)

class PoseEstimationMixin(BaseStepMixin):
    """Step 2: Pose Estimation íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'PoseEstimationStep')
        kwargs.setdefault('step_number', 2)
        kwargs.setdefault('step_type', 'pose_estimation')
        super().__init__(**kwargs)
        
        self.num_keypoints = 18
        self.keypoint_names = [
            'nose', 'neck', 'right_shoulder', 'right_elbow', 'right_wrist',
            'left_shoulder', 'left_elbow', 'left_wrist', 'right_hip', 'right_knee',
            'right_ankle', 'left_hip', 'left_knee', 'left_ankle', 'right_eye',
            'left_eye', 'right_ear', 'left_ear'
        ]
        
        self.logger.info(f"ğŸ¤¸ Pose Estimation Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.num_keypoints}ê°œ í‚¤í¬ì¸íŠ¸")
    
    def _step_specific_warmup(self):
        """Pose Estimation íŠ¹í™” ì›Œë°ì—…"""
        self.logger.debug("ğŸ”¥ Pose Estimation íŠ¹í™” ì›Œë°ì—…")
    
    async def _step_specific_warmup_async(self):
        """Pose Estimation íŠ¹í™” ì›Œë°ì—… (ë¹„ë™ê¸°)"""
        self.logger.debug("ğŸ”¥ Pose Estimation ë¹„ë™ê¸° íŠ¹í™” ì›Œë°ì—…")
        await asyncio.sleep(0.001)

class ClothSegmentationMixin(BaseStepMixin):
    """Step 3: Cloth Segmentation íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'ClothSegmentationStep')
        kwargs.setdefault('step_number', 3)
        kwargs.setdefault('step_type', 'cloth_segmentation')
        super().__init__(**kwargs)
        
        self.segmentation_methods = ['traditional', 'u2net', 'deeplab', 'auto', 'hybrid']
        self.segmentation_method = kwargs.get('segmentation_method', 'u2net')
        
        self.logger.info(f"ğŸ‘• Cloth Segmentation Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.segmentation_method} ë°©ë²•")
    
    def _step_specific_warmup(self):
        """Cloth Segmentation íŠ¹í™” ì›Œë°ì—…"""
        self.logger.debug("ğŸ”¥ Cloth Segmentation íŠ¹í™” ì›Œë°ì—…")
    
    async def _step_specific_warmup_async(self):
        """Cloth Segmentation íŠ¹í™” ì›Œë°ì—… (ë¹„ë™ê¸°)"""
        self.logger.debug("ğŸ”¥ Cloth Segmentation ë¹„ë™ê¸° íŠ¹í™” ì›Œë°ì—…")
        await asyncio.sleep(0.001)

class GeometricMatchingMixin(BaseStepMixin):
    """Step 4: Geometric Matching íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'GeometricMatchingStep')
        kwargs.setdefault('step_number', 4)
        kwargs.setdefault('step_type', 'geometric_matching')
        super().__init__(**kwargs)
        
        self.matching_methods = ['thin_plate_spline', 'affine', 'perspective', 'flow_based']
        self.matching_method = kwargs.get('matching_method', 'thin_plate_spline')
        self.grid_size = kwargs.get('grid_size', (5, 5))
        
        self.logger.info(f"ğŸ“ Geometric Matching Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.matching_method} ë°©ë²•")
    
    def _step_specific_warmup(self):
        """Geometric Matching íŠ¹í™” ì›Œë°ì—…"""
        self.logger.debug("ğŸ”¥ Geometric Matching íŠ¹í™” ì›Œë°ì—…")
    
    async def _step_specific_warmup_async(self):
        """Geometric Matching íŠ¹í™” ì›Œë°ì—… (ë¹„ë™ê¸°)"""
        self.logger.debug("ğŸ”¥ Geometric Matching ë¹„ë™ê¸° íŠ¹í™” ì›Œë°ì—…")
        await asyncio.sleep(0.001)

class ClothWarpingMixin(BaseStepMixin):
    """Step 5: Cloth Warping íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'ClothWarpingStep')
        kwargs.setdefault('step_number', 5)
        kwargs.setdefault('step_type', 'cloth_warping')
        super().__init__(**kwargs)
        
        self.warping_stages = ['preprocessing', 'geometric_transformation', 'texture_mapping', 'postprocessing']
        self.warping_quality = kwargs.get('warping_quality', 'high')
        self.preserve_texture = kwargs.get('preserve_texture', True)
        
        self.logger.info(f"ğŸ”„ Cloth Warping Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.warping_quality} í’ˆì§ˆ")
    
    def _step_specific_warmup(self):
        """Cloth Warping íŠ¹í™” ì›Œë°ì—…"""
        self.logger.debug("ğŸ”¥ Cloth Warping íŠ¹í™” ì›Œë°ì—…")
    
    async def _step_specific_warmup_async(self):
        """Cloth Warping íŠ¹í™” ì›Œë°ì—… (ë¹„ë™ê¸°)"""
        self.logger.debug("ğŸ”¥ Cloth Warping ë¹„ë™ê¸° íŠ¹í™” ì›Œë°ì—…")
        await asyncio.sleep(0.001)

class VirtualFittingMixin(BaseStepMixin):
    """Step 6: Virtual Fitting íŠ¹í™” Mixin (í•µì‹¬ ë‹¨ê³„)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'VirtualFittingStep')
        kwargs.setdefault('step_number', 6)
        kwargs.setdefault('step_type', 'virtual_fitting')
        super().__init__(**kwargs)
        
        self.fitting_modes = ['standard', 'high_quality', 'fast', 'experimental']
        self.fitting_mode = kwargs.get('fitting_mode', 'high_quality')
        self.diffusion_steps = kwargs.get('diffusion_steps', 50)
        self.guidance_scale = kwargs.get('guidance_scale', 7.5)
        self.use_ootd = kwargs.get('use_ootd', True)
        
        self.logger.info(f"ğŸ‘— Virtual Fitting Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.fitting_mode} ëª¨ë“œ")
    
    def _step_specific_warmup(self):
        """Virtual Fitting íŠ¹í™” ì›Œë°ì—… (í•µì‹¬)"""
        self.logger.debug("ğŸ”¥ Virtual Fitting íŠ¹í™” ì›Œë°ì—…")
    
    async def _step_specific_warmup_async(self):
        """Virtual Fitting íŠ¹í™” ì›Œë°ì—… (ë¹„ë™ê¸°, í•µì‹¬)"""
        self.logger.debug("ğŸ”¥ Virtual Fitting ë¹„ë™ê¸° íŠ¹í™” ì›Œë°ì—…")
        await asyncio.sleep(0.001)

class PostProcessingMixin(BaseStepMixin):
    """Step 7: Post Processing íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'PostProcessingStep')
        kwargs.setdefault('step_number', 7)
        kwargs.setdefault('step_type', 'post_processing')
        super().__init__(**kwargs)
        
        self.processing_methods = ['super_resolution', 'denoising', 'color_correction', 'sharpening']
        self.enhancement_level = kwargs.get('enhancement_level', 'medium')
        self.super_resolution_factor = kwargs.get('super_resolution_factor', 2.0)
        
        self.logger.info(f"âœ¨ Post Processing Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.enhancement_level} í–¥ìƒ ìˆ˜ì¤€")
    
    def _step_specific_warmup(self):
        """Post Processing íŠ¹í™” ì›Œë°ì—…"""
        self.logger.debug("ğŸ”¥ Post Processing íŠ¹í™” ì›Œë°ì—…")
    
    async def _step_specific_warmup_async(self):
        """Post Processing íŠ¹í™” ì›Œë°ì—… (ë¹„ë™ê¸°)"""
        self.logger.debug("ğŸ”¥ Post Processing ë¹„ë™ê¸° íŠ¹í™” ì›Œë°ì—…")
        await asyncio.sleep(0.001)

class QualityAssessmentMixin(BaseStepMixin):
    """Step 8: Quality Assessment íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'QualityAssessmentStep')
        kwargs.setdefault('step_number', 8)
        kwargs.setdefault('step_type', 'quality_assessment')
        super().__init__(**kwargs)
        
        self.assessment_criteria = ['perceptual_quality', 'technical_quality', 'aesthetic_quality', 'overall_quality']
        self.quality_threshold = kwargs.get('quality_threshold', 0.7)
        self.use_clip_score = kwargs.get('use_clip_score', True)
        
        self.logger.info(f"ğŸ† Quality Assessment Mixin ì´ˆê¸°í™” ì™„ë£Œ - ì„ê³„ê°’: {self.quality_threshold}")
    
    def _step_specific_warmup(self):
        """Quality Assessment íŠ¹í™” ì›Œë°ì—…"""
        self.logger.debug("ğŸ”¥ Quality Assessment íŠ¹í™” ì›Œë°ì—…")
    
    async def _step_specific_warmup_async(self):
        """Quality Assessment íŠ¹í™” ì›Œë°ì—… (ë¹„ë™ê¸°)"""
        self.logger.debug("ğŸ”¥ Quality Assessment ë¹„ë™ê¸° íŠ¹í™” ì›Œë°ì—…")
        await asyncio.sleep(0.001)

# ==============================================
# ğŸ”¥ 11. ë°ì½”ë ˆì´í„°ë“¤
# ==============================================

def safe_step_method(func: Callable) -> Callable:
    """Step ë©”ì„œë“œ ì•ˆì „ ì‹¤í–‰ ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        try:
            # logger ì†ì„± í™•ì¸
            if not hasattr(self, 'logger') or self.logger is None:
                self.logger = logging.getLogger(self.__class__.__name__)
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            start_time = time.time()
            
            result = func(self, *args, **kwargs)
            
            # ì„±ëŠ¥ ê¸°ë¡
            duration = time.time() - start_time
            if hasattr(self, 'record_processing'):
                self.record_processing(duration, True)
            
            return result
            
        except Exception as e:
            # ì—ëŸ¬ ê¸°ë¡
            duration = time.time() - start_time if 'start_time' in locals() else 0
            if hasattr(self, 'record_processing'):
                self.record_processing(duration, False)
            
            # ì—ëŸ¬ ì¹´ìš´íŠ¸ ì¦ê°€
            if hasattr(self, 'error_count'):
                self.error_count += 1
            
            # ë§ˆì§€ë§‰ ì—ëŸ¬ ì €ì¥
            if hasattr(self, 'last_error'):
                self.last_error = str(e)
            
            # ë¡œê¹…
            if hasattr(self, 'logger'):
                self.logger.error(f"âŒ {func.__name__} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                self.logger.debug(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            return {
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__,
                'step_name': getattr(self, 'step_name', self.__class__.__name__),
                'method_name': func.__name__,
                'duration': duration,
                'timestamp': time.time()
            }
    
    return wrapper

def async_safe_step_method(func: Callable) -> Callable:
    """ì•ˆì „í•œ ë¹„ë™ê¸° Step ë©”ì„œë“œ ì‹¤í–‰ ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        try:
            # logger ì†ì„± í™•ì¸
            if not hasattr(self, 'logger') or self.logger is None:
                self.logger = logging.getLogger(self.__class__.__name__)
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            start_time = time.time()
            
            # ë¹„ë™ê¸° í•¨ìˆ˜ì¸ì§€ í™•ì¸í•˜ê³  ì ì ˆíˆ í˜¸ì¶œ
            if asyncio.iscoroutinefunction(func):
                result = await func(self, *args, **kwargs)
            else:
                # ë™ê¸° í•¨ìˆ˜ì¸ ê²½ìš° executorë¡œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, lambda: func(self, *args, **kwargs))
            
            # ì„±ëŠ¥ ê¸°ë¡
            duration = time.time() - start_time
            if hasattr(self, 'record_processing'):
                self.record_processing(duration, True)
            
            return result
            
        except Exception as e:
            # ì—ëŸ¬ ê¸°ë¡
            duration = time.time() - start_time if 'start_time' in locals() else 0
            if hasattr(self, 'record_processing'):
                self.record_processing(duration, False)
            
            # ì—ëŸ¬ ì¹´ìš´íŠ¸ ì¦ê°€
            if hasattr(self, 'error_count'):
                self.error_count += 1
            
            # ë§ˆì§€ë§‰ ì—ëŸ¬ ì €ì¥
            if hasattr(self, 'last_error'):
                self.last_error = str(e)
            
            # ë¡œê¹…
            if hasattr(self, 'logger'):
                self.logger.error(f"âŒ {func.__name__} ë¹„ë™ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                self.logger.debug(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            return {
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__,
                'step_name': getattr(self, 'step_name', self.__class__.__name__),
                'method_name': func.__name__,
                'async': True,
                'duration': duration,
                'timestamp': time.time()
            }
    
    return wrapper

def performance_monitor(operation_name: str) -> Callable:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(self, *args, **kwargs):
            start_time = time.time()
            success = True
            try:
                result = func(self, *args, **kwargs)
                return result
            except Exception as e:
                success = False
                raise e
            finally:
                duration = time.time() - start_time
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ì— ê¸°ë¡
                if hasattr(self, 'performance_metrics'):
                    if 'operations' not in self.performance_metrics:
                        self.performance_metrics['operations'] = {}
                    
                    if operation_name not in self.performance_metrics['operations']:
                        self.performance_metrics['operations'][operation_name] = {
                            'count': 0,
                            'total_time': 0.0,
                            'success_count': 0,
                            'failure_count': 0,
                            'avg_time': 0.0
                        }
                    
                    op_metrics = self.performance_metrics['operations'][operation_name]
                    op_metrics['count'] += 1
                    op_metrics['total_time'] += duration
                    op_metrics['avg_time'] = op_metrics['total_time'] / op_metrics['count']
                    
                    if success:
                        op_metrics['success_count'] += 1
                    else:
                        op_metrics['failure_count'] += 1
                        
        return wrapper
    return decorator

def async_performance_monitor(operation_name: str) -> Callable:
    """ë¹„ë™ê¸° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            start_time = time.time()
            success = True
            try:
                # ë¹„ë™ê¸° í•¨ìˆ˜ì¸ì§€ í™•ì¸í•˜ê³  ì ì ˆíˆ í˜¸ì¶œ
                if asyncio.iscoroutinefunction(func):
                    result = await func(self, *args, **kwargs)
                else:
                    # ë™ê¸° í•¨ìˆ˜ì¸ ê²½ìš° executorë¡œ ì‹¤í–‰
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(None, lambda: func(self, *args, **kwargs))
                return result
            except Exception as e:
                success = False
                raise e
            finally:
                duration = time.time() - start_time
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ì— ê¸°ë¡
                if hasattr(self, 'performance_metrics'):
                    if 'operations' not in self.performance_metrics:
                        self.performance_metrics['operations'] = {}
                    
                    async_op_name = f"{operation_name}_async"
                    if async_op_name not in self.performance_metrics['operations']:
                        self.performance_metrics['operations'][async_op_name] = {
                            'count': 0,
                            'total_time': 0.0,
                            'success_count': 0,
                            'failure_count': 0,
                            'avg_time': 0.0
                        }
                    
                    op_metrics = self.performance_metrics['operations'][async_op_name]
                    op_metrics['count'] += 1
                    op_metrics['total_time'] += duration
                    op_metrics['avg_time'] = op_metrics['total_time'] / op_metrics['count']
                    
                    if success:
                        op_metrics['success_count'] += 1
                    else:
                        op_metrics['failure_count'] += 1
                        
        return wrapper
    return decorator

def memory_optimize_after(func: Callable) -> Callable:
    """ë©”ì„œë“œ ì‹¤í–‰ í›„ ìë™ ë©”ëª¨ë¦¬ ìµœì í™”"""
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        try:
            result = func(self, *args, **kwargs)
            
            # ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ (ì„¤ì •ëœ ê²½ìš°)
            if getattr(self, 'config', None) and getattr(self.config, 'auto_memory_cleanup', False):
                try:
                    self.optimize_memory()
                except Exception as e:
                    if hasattr(self, 'logger'):
                        self.logger.debug(f"ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            return result
            
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒì‹œì—ë„ ë©”ëª¨ë¦¬ ì •ë¦¬
            if getattr(self, 'config', None) and getattr(self.config, 'auto_memory_cleanup', False):
                try:
                    self.optimize_memory(aggressive=True)
                except:
                    pass
            raise e
    
    return wrapper

def async_memory_optimize_after(func: Callable) -> Callable:
    """ë¹„ë™ê¸° ë©”ì„œë“œ ì‹¤í–‰ í›„ ìë™ ë©”ëª¨ë¦¬ ìµœì í™”"""
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        try:
            # ë¹„ë™ê¸° í•¨ìˆ˜ì¸ì§€ í™•ì¸í•˜ê³  ì ì ˆíˆ í˜¸ì¶œ
            if asyncio.iscoroutinefunction(func):
                result = await func(self, *args, **kwargs)
            else:
                # ë™ê¸° í•¨ìˆ˜ì¸ ê²½ìš° executorë¡œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, lambda: func(self, *args, **kwargs))
            
            # ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ (ì„¤ì •ëœ ê²½ìš°)
            if getattr(self, 'config', None) and getattr(self.config, 'auto_memory_cleanup', False):
                try:
                    await self.optimize_memory_async()
                except Exception as e:
                    if hasattr(self, 'logger'):
                        self.logger.debug(f"ìë™ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            return result
            
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒì‹œì—ë„ ë©”ëª¨ë¦¬ ì •ë¦¬
            if getattr(self, 'config', None) and getattr(self.config, 'auto_memory_cleanup', False):
                try:
                    await self.optimize_memory_async(aggressive=True)
                except:
                    pass
            raise e
    
    return wrapper

# ==============================================
# ğŸ”¥ 12. ë¹„ë™ê¸° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

async def ensure_coroutine(func_or_coro, *args, **kwargs) -> Any:
    """í•¨ìˆ˜ë‚˜ ì½”ë£¨í‹´ì„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•˜ëŠ” ìœ í‹¸ë¦¬í‹°"""
    try:
        if asyncio.iscoroutinefunction(func_or_coro):
            return await func_or_coro(*args, **kwargs)
        elif callable(func_or_coro):
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, lambda: func_or_coro(*args, **kwargs))
        elif asyncio.iscoroutine(func_or_coro):
            return await func_or_coro
        else:
            return func_or_coro
    except Exception as e:
        logging.error(f"âŒ ensure_coroutine ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return None

def is_coroutine_function_safe(func) -> bool:
    """ì•ˆì „í•œ ì½”ë£¨í‹´ í•¨ìˆ˜ ê²€ì‚¬"""
    try:
        return asyncio.iscoroutinefunction(func)
    except:
        return False

def is_coroutine_safe(obj) -> bool:
    """ì•ˆì „í•œ ì½”ë£¨í‹´ ê°ì²´ ê²€ì‚¬"""
    try:
        return asyncio.iscoroutine(obj)
    except:
        return False

async def run_with_timeout(coro_or_func, timeout: float = 30.0, *args, **kwargs) -> Any:
    """íƒ€ì„ì•„ì›ƒì„ ì ìš©í•œ ì•ˆì „í•œ ì‹¤í–‰"""
    try:
        if asyncio.iscoroutinefunction(coro_or_func):
            return await asyncio.wait_for(coro_or_func(*args, **kwargs), timeout=timeout)
        elif callable(coro_or_func):
            loop = asyncio.get_event_loop()
            return await asyncio.wait_for(
                loop.run_in_executor(None, lambda: coro_or_func(*args, **kwargs)), 
                timeout=timeout
            )
        elif asyncio.iscoroutine(coro_or_func):
            return await asyncio.wait_for(coro_or_func, timeout=timeout)
        else:
            return coro_or_func
    except asyncio.TimeoutError:
        logging.warning(f"âš ï¸ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ): {coro_or_func}")
        return None
    except Exception as e:
        logging.error(f"âŒ run_with_timeout ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ 13. í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_step_mixin(step_name: str, step_number: int, **kwargs) -> BaseStepMixin:
    """BaseStepMixin ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    kwargs.update({
        'step_name': step_name,
        'step_number': step_number
    })
    return BaseStepMixin(**kwargs)

def create_human_parsing_step(**kwargs) -> HumanParsingMixin:
    """Human Parsing Step ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return HumanParsingMixin(**kwargs)

def create_pose_estimation_step(**kwargs) -> PoseEstimationMixin:
    """Pose Estimation Step ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return PoseEstimationMixin(**kwargs)

def create_cloth_segmentation_step(**kwargs) -> ClothSegmentationMixin:
    """Cloth Segmentation Step ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return ClothSegmentationMixin(**kwargs)

def create_geometric_matching_step(**kwargs) -> GeometricMatchingMixin:
    """Geometric Matching Step ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return GeometricMatchingMixin(**kwargs)

def create_cloth_warping_step(**kwargs) -> ClothWarpingMixin:
    """Cloth Warping Step ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return ClothWarpingMixin(**kwargs)

def create_virtual_fitting_step(**kwargs) -> VirtualFittingMixin:
    """Virtual Fitting Step ìƒì„± í¸ì˜ í•¨ìˆ˜ (í•µì‹¬)"""
    return VirtualFittingMixin(**kwargs)

def create_post_processing_step(**kwargs) -> PostProcessingMixin:
    """Post Processing Step ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return PostProcessingMixin(**kwargs)

def create_quality_assessment_step(**kwargs) -> QualityAssessmentMixin:
    """Quality Assessment Step ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return QualityAssessmentMixin(**kwargs)

def create_m3_max_optimized_step(step_type: str, **kwargs) -> BaseStepMixin:
    """M3 Max ìµœì í™”ëœ Step ìƒì„±"""
    kwargs.update({
        'device': 'mps',
        'optimization_enabled': True,
        'auto_memory_cleanup': True,
        'use_fp16': True
    })
    
    step_creators = {
        'human_parsing': create_human_parsing_step,
        'pose_estimation': create_pose_estimation_step,
        'cloth_segmentation': create_cloth_segmentation_step,
        'geometric_matching': create_geometric_matching_step,
        'cloth_warping': create_cloth_warping_step,
        'virtual_fitting': create_virtual_fitting_step,
        'post_processing': create_post_processing_step,
        'quality_assessment': create_quality_assessment_step,
    }
    
    creator = step_creators.get(step_type, create_step_mixin)
    return creator(**kwargs)

# ==============================================
# ğŸ”¥ 14. ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'BaseStepMixin',
    'StepConfig',
    'MemoryOptimizer',
    'DIHelper',
    
    # Stepë³„ íŠ¹í™” Mixinë“¤ (8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸)
    'HumanParsingMixin',
    'PoseEstimationMixin', 
    'ClothSegmentationMixin',
    'GeometricMatchingMixin',
    'ClothWarpingMixin',
    'VirtualFittingMixin',
    'PostProcessingMixin',
    'QualityAssessmentMixin',
    
    # ë°ì½”ë ˆì´í„°ë“¤ (ë™ê¸°/ë¹„ë™ê¸°)
    'safe_step_method',
    'async_safe_step_method',
    'performance_monitor',
    'async_performance_monitor',
    'memory_optimize_after',
    'async_memory_optimize_after',
    
    # ë¹„ë™ê¸° ìœ í‹¸ë¦¬í‹°ë“¤
    'ensure_coroutine',
    'is_coroutine_function_safe',
    'is_coroutine_safe',
    'run_with_timeout',
    'safe_async_wrapper',
    
    # í¸ì˜ í•¨ìˆ˜ë“¤
    'create_step_mixin',
    'create_human_parsing_step',
    'create_pose_estimation_step',
    'create_cloth_segmentation_step',
    'create_geometric_matching_step',
    'create_cloth_warping_step',
    'create_virtual_fitting_step',
    'create_post_processing_step',
    'create_quality_assessment_step',
    'create_m3_max_optimized_step',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'CONDA_INFO'
]

# ==============================================
# ğŸ”¥ 15. ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

print("=" * 80)
print("âœ… BaseStepMixin v12.0 - ì™„ì „í•œ ë¦¬íŒ©í† ë§ ë¡œë“œ ì™„ë£Œ")
print("=" * 80)
print("ğŸ”¥ ê°œì„ ì‚¬í•­:")
print("   âœ… 5ë‹¨ê³„ ê°„ë‹¨í•œ ì´ˆê¸°í™” (17ë‹¨ê³„ â†’ 5ë‹¨ê³„)")
print("   âœ… ëª¨ë“  í•„ìˆ˜ ë©”ì„œë“œ ì™„ì „ êµ¬í˜„")
print("   âœ… ModelLoader ì—°ë™ (89.8GB ì²´í¬í¬ì¸íŠ¸ í™œìš©)")
print("   âœ… ì˜ì¡´ì„± ì£¼ì… (DI Container) ì§€ì›")
print("   âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
print("   âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›")
print("   âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²° (coroutine ê²½ê³  ì—†ìŒ)")
print("   âœ… Stepë³„ íŠ¹í™” Mixin (8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸)")
print("   âœ… ê¹”ë”í•œ ì•„í‚¤í…ì²˜ (Clean Architecture)")
print("   âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
print("")
print("ğŸš€ í•µì‹¬ ê¸°ëŠ¥ë“¤:")
print("   ğŸ¤– AI ëª¨ë¸ ì—°ë™: get_model(), get_model_async()")
print("   ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™”: optimize_memory(), optimize_memory_async()")
print("   ğŸ”¥ ì›Œë°ì—… ì‹œìŠ¤í…œ: warmup(), warmup_async(), warmup_step()")
print("   ğŸ“Š ìƒíƒœ ê´€ë¦¬: get_status(), get_performance_summary()")
print("   ğŸ”§ ì´ˆê¸°í™”: initialize(), initialize_async()")
print("   ğŸ§¹ ì •ë¦¬: cleanup(), cleanup_models()")
print("   ğŸ”„ ì˜ì¡´ì„± ì£¼ì…: reinject_dependencies(), get_di_status()")
print("")
print("ğŸ¯ 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ Stepë³„ Mixin:")
print("   1ï¸âƒ£ HumanParsingMixin - ì‹ ì²´ ì˜ì—­ ë¶„í• ")
print("   2ï¸âƒ£ PoseEstimationMixin - í¬ì¦ˆ ê°ì§€")
print("   3ï¸âƒ£ ClothSegmentationMixin - ì˜ë¥˜ ë¶„í• ")
print("   4ï¸âƒ£ GeometricMatchingMixin - ê¸°í•˜í•™ì  ë§¤ì¹­")
print("   5ï¸âƒ£ ClothWarpingMixin - ì˜ë¥˜ ë³€í˜•")
print("   6ï¸âƒ£ VirtualFittingMixin - ê°€ìƒ í”¼íŒ… (í•µì‹¬)")
print("   7ï¸âƒ£ PostProcessingMixin - í›„ì²˜ë¦¬")
print("   8ï¸âƒ£ QualityAssessmentMixin - í’ˆì§ˆ í‰ê°€")
print("")
print(f"ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ:")
print(f"   - conda í™˜ê²½: {CONDA_INFO['conda_env']}")
print(f"   - PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
print(f"   - MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
print(f"   - NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'}")
print(f"   - PIL: {'âœ…' if PIL_AVAILABLE else 'âŒ'}")
print("")
print("ğŸŒŸ ì‚¬ìš© ì˜ˆì‹œ:")
print("   # ê¸°ë³¸ ì‚¬ìš©")
print("   step = BaseStepMixin(step_name='MyStep')")
print("   await step.warmup_async()")
print("   model = await step.get_model_async('model_name')")
print("   ")
print("   # Virtual Fitting (í•µì‹¬ ë‹¨ê³„)")
print("   vf_step = VirtualFittingMixin(fitting_mode='high_quality')")
print("   await vf_step.warmup_async()")
print("   ")
print("   # M3 Max ìµœì í™”")
print("   step = create_m3_max_optimized_step('virtual_fitting')")
print("")
print("=" * 80)