# backend/app/ai_pipeline/steps/base_step_mixin.py
"""
ğŸ”¥ BaseStepMixin v14.0 - v13.0 í˜¸í™˜ ë²„ì „ (DI + ê¸°ì¡´ í•¨ìˆ˜ëª…)
================================================================

âœ… DI Container ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì… (ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°!)
âœ… v13.0ì˜ ëª¨ë“  í•¨ìˆ˜ëª…/í´ë˜ìŠ¤ëª…ê³¼ 100% í˜¸í™˜
âœ… 2ë‹¨ê³„ ì´ˆê¸°í™”: ê¸°ë³¸ ìƒì„± â†’ ìë™ ì˜ì¡´ì„± ì£¼ì… 
âœ… ëª¨ë“  Step íŒŒì¼ì´ ìš”êµ¬í•˜ëŠ” ê¸°ëŠ¥ ì™„ì „ ì œê³µ
âœ… ModelLoader ì—°ë™ (DI Containerë¥¼ í†µí•œ ì§€ì—° ë¡œë”©)
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²°
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

í•µì‹¬ ì•„í‚¤í…ì²˜ (v13.0 í˜¸í™˜ + DI ê°•í™”):
- BaseStepMixinì´ ë” ì´ìƒ ModelLoaderë¥¼ ì§ì ‘ import í•˜ì§€ ì•ŠìŒ!
- DI Containerë¥¼ í†µí•œ ì§€ì—° ë¡œë”©ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ì°¨ë‹¨
- v13.0ì˜ set_model_loader() ë“± ë©”ì„œë“œ ê·¸ëŒ€ë¡œ ìœ ì§€
- ë‚´ë¶€ì ìœ¼ë¡œëŠ” inject_dependencies() ìë™ í˜¸ì¶œ

Author: MyCloset AI Team  
Date: 2025-07-22
Version: 14.0 (v13.0 Compatible + DI Enhanced)
"""

# ==============================================
# ğŸ”¥ 1. í•„ìˆ˜ importë§Œ (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€!)
# ==============================================
import os
import gc
import time
import asyncio
import logging
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple
from abc import ABC, abstractmethod
from dataclasses import dataclass
from functools import wraps
import platform
import subprocess
import psutil
from datetime import datetime

# ==============================================
# ğŸ”¥ 2. conda í™˜ê²½ ìš°ì„  ì²´í¬
# ==============================================
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'python_path': os.path.dirname(os.__file__)
}

if CONDA_INFO['conda_env'] != 'none':
    print(f"âœ… conda í™˜ê²½ ê°ì§€: {CONDA_INFO['conda_env']}")
else:
    print("âš ï¸ conda í™˜ê²½ ê¶Œì¥: conda activate mycloset-ai")

# ==============================================
# ğŸ”¥ 3. ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import (ìˆœí™˜ì°¸ì¡° ì—†ìŒ)
# ==============================================

# PyTorch ì•ˆì „ Import
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        print("ğŸ M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
    
except ImportError:
    print("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch torchvision torchaudio -c pytorch")

# NumPy ì•ˆì „ Import
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    print("âš ï¸ NumPy ì—†ìŒ - conda install numpy")

# PIL ì•ˆì „ Import
PIL_AVAILABLE = False
try:
    from PIL import Image, ImageEnhance, ImageFilter
    PIL_AVAILABLE = True
except ImportError:
    print("âš ï¸ PIL ì—†ìŒ - conda install pillow")

# DI Container Import (í•µì‹¬! ìˆœí™˜ì°¸ì¡° ì—†ìŒ)
DI_CONTAINER_AVAILABLE = False
try:
    from ...core.di_container import get_di_container, IDependencyContainer
    DI_CONTAINER_AVAILABLE = True
    print("âœ… DI Container ì—°ë™ ì„±ê³µ!")
except ImportError:
    print("âš ï¸ DI Container import ì‹¤íŒ¨")
    # í´ë°± ì¸í„°í˜ì´ìŠ¤
    class IDependencyContainer:
        def get(self, key: str): return None
    
    def get_di_container() -> IDependencyContainer:
        return IDependencyContainer()

# ==============================================
# ğŸ”¥ 4. v13.0 í˜¸í™˜ ì„¤ì • í´ë˜ìŠ¤
# ==============================================
@dataclass
class StepConfig:
    """ê°„ë‹¨í•œ Step ì„¤ì • (v13.0 í˜¸í™˜)"""
    step_name: str = "BaseStep"
    step_id: int = 0
    device: str = "auto"
    use_fp16: bool = True
    batch_size: int = 1
    confidence_threshold: float = 0.8
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True

# ==============================================
# ğŸ”¥ 5. SimpleMemoryOptimizer (v13.0 í˜¸í™˜ ì´ë¦„)
# ==============================================
class SimpleMemoryOptimizer:
    """
    ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìµœì í™” (v13.0 í˜¸í™˜ ì´ë¦„)
    ë‚´ë¶€ì ìœ¼ë¡œ DI Container í™œìš©
    """
    
    def __init__(self, device: str = "mps"):
        self.device = device
        self.is_m3_max = self._detect_m3_max()
        self.di_container = None
        
        # DI Container ì—°ê²° ì‹œë„ (ì„ íƒì )
        if DI_CONTAINER_AVAILABLE:
            try:
                self.di_container = get_di_container()
            except Exception as e:
                print(f"âš ï¸ DI Container ì—°ê²° ì‹¤íŒ¨: {e}")
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def optimize(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰ (DI ê¸°ë°˜ ê°•í™”)"""
        try:
            results = []
            
            # Python GC
            before = len(gc.get_objects())
            gc.collect()
            after = len(gc.get_objects())
            results.append(f"Python GC: {before - after}ê°œ ê°ì²´ í•´ì œ")
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    results.append("CUDA ìºì‹œ ì •ë¦¬")
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        results.append("MPS ìºì‹œ ì •ë¦¬")
                    except Exception:
                        results.append("MPS ìºì‹œ ì •ë¦¬ ì‹œë„")
            
            # ğŸ”¥ DI ê¸°ë°˜ ì¶”ê°€ ìµœì í™” (v14.0 ê°•í™”)
            if self.di_container:
                try:
                    memory_manager = self.di_container.get('MemoryManager')
                    if memory_manager and hasattr(memory_manager, 'optimize'):
                        additional_result = memory_manager.optimize(aggressive=aggressive)
                        results.append(f"DI MemoryManager: {additional_result.get('message', 'OK')}")
                except Exception as e:
                    results.append(f"DI ìµœì í™” ì‹¤íŒ¨: {str(e)}")
            
            # M3 Max íŠ¹ë³„ ìµœì í™”
            if self.is_m3_max and aggressive:
                for _ in range(3):
                    gc.collect()
                results.append("M3 Max í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”")
            
            return {
                "success": True,
                "results": results,
                "device": self.device,
                "is_m3_max": self.is_m3_max,
                "di_enhanced": self.di_container is not None
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def optimize_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” (DI ê°•í™”)"""
        try:
            # DI ê¸°ë°˜ MemoryManager ë¹„ë™ê¸° ì‚¬ìš©
            if self.di_container:
                try:
                    memory_manager = self.di_container.get('MemoryManager')
                    if memory_manager and hasattr(memory_manager, 'optimize_async'):
                        result = await memory_manager.optimize_async(aggressive=aggressive)
                        result["di_enhanced"] = True
                        return result
                except Exception as e:
                    pass  # í´ë°±ìœ¼ë¡œ ì§„í–‰
            
            # í´ë°±: ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, lambda: self.optimize(aggressive))
            
        except Exception as e:
            return {"success": False, "error": str(e)}

# ==============================================
# ğŸ”¥ 6. BaseStepMixin v14.0 - v13.0 í˜¸í™˜ ë²„ì „
# ==============================================
class BaseStepMixin:
    """
    ğŸ”¥ BaseStepMixin v14.0 - v13.0 ì™„ë²½ í˜¸í™˜ + DI ê°•í™”
    
    âœ… v13.0ì˜ ëª¨ë“  ë©”ì„œë“œëª…ê³¼ 100% í˜¸í™˜
    âœ… DI Container ê¸°ë°˜ ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
    âœ… ìë™ ì˜ì¡´ì„± ì£¼ì… (ì‚¬ìš©ìê°€ ëª¨ë¥´ê²Œ)
    âœ… M3 Max ìµœì í™” ìœ ì§€
    âœ… conda í™˜ê²½ ìš°ì„ 
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ì§€ì›
    """
    
    def __init__(self, **kwargs):
        """3ë‹¨ê³„ ì´ˆê¸°í™” + ìë™ DI ì£¼ì…"""
        try:
            # STEP 1: ê¸°ë³¸ ì„¤ì • (v13.0ê³¼ ë™ì¼)
            self._setup_basic(**kwargs)
            
            # STEP 2: ì‹œìŠ¤í…œ ì„¤ì • (v13.0ê³¼ ë™ì¼)
            self._setup_system()
            
            # ğŸ”¥ STEP 2.5: ìë™ DI ì£¼ì… (v14.0 ì¶”ê°€, ì‚¬ìš©ìì—ê²Œ íˆ¬ëª…)
            self._auto_inject_dependencies()
            
            # STEP 3: ì™„ë£Œ (v13.0ê³¼ ë™ì¼)
            self._finalize()
            
            self.logger.info(f"âœ… {self.step_name} BaseStepMixin v14.0 (v13.0 í˜¸í™˜) ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self._emergency_setup(e)
    
    def _setup_basic(self, **kwargs):
        """STEP 1: ê¸°ë³¸ ì„¤ì • (v13.0ê³¼ ë™ì¼)"""
        # ì„¤ì •
        self.config = StepConfig()
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
        
        # ê¸°ë³¸ ì†ì„±
        self.step_name = kwargs.get('step_name', self.__class__.__name__)
        self.step_id = kwargs.get('step_id', 0)
        
        # Logger ì„¤ì •
        self.logger = logging.getLogger(f"pipeline.steps.{self.step_name}")
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
        
        # ìƒíƒœ í”Œë˜ê·¸ë“¤ (v13.0ê³¼ ë™ì¼)
        self.is_initialized = False
        self.is_ready = False
        self.has_model = False
        self.model_loaded = False
        self.warmup_completed = False
        
        # ì˜ì¡´ì„± ì£¼ì…ì„ ìœ„í•œ ì†ì„±ë“¤ (v13.0ê³¼ ë™ì¼)
        self.model_loader = None
        self.memory_manager = None
        self.data_converter = None
        
        # ğŸ”¥ DI ê´€ë ¨ ë‚´ë¶€ ì†ì„± (ì‚¬ìš©ìì—ê²ŒëŠ” ìˆ¨ê¹€)
        self._di_container = None
        self._dependencies_injected = False
        self._injection_attempts = 0
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_metrics = {
            'process_count': 0,
            'total_process_time': 0.0,
            'average_process_time': 0.0,
            'error_history': [],
            'di_injection_time': 0.0  # DI ë©”íŠ¸ë¦­
        }
        
        # ì—ëŸ¬ ì¶”ì 
        self.error_count = 0
        self.last_error = None
        self.total_processing_count = 0
        self.last_processing_time = None
    
    def _setup_system(self):
        """STEP 2: ì‹œìŠ¤í…œ ì„¤ì • (v13.0ê³¼ ë™ì¼)"""
        # ë””ë°”ì´ìŠ¤ ê°ì§€
        if self.config.device == "auto":
            self.device = self._detect_optimal_device()
        else:
            self.device = self.config.device
        
        # M3 Max ê°ì§€
        self.is_m3_max = self._detect_m3_max()
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        self.memory_gb = self._get_memory_info()
        
        # ğŸ”¥ DI ê°•í™”ëœ ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ (ì´ë¦„ì€ v13.0 í˜¸í™˜)
        self.memory_optimizer = SimpleMemoryOptimizer(self.device)
        
        # ëª¨ë¸ ìºì‹œ
        self.model_cache = {}
        self.loaded_models = {}
        
        # í˜„ì¬ ëª¨ë¸
        self._ai_model = None
        self._ai_model_name = None
    
    def _auto_inject_dependencies(self):
        """ğŸ”¥ ìë™ ì˜ì¡´ì„± ì£¼ì… (ì‚¬ìš©ìì—ê²Œ íˆ¬ëª…)"""
        if not DI_CONTAINER_AVAILABLE:
            return
        
        try:
            start_time = time.time()
            self._injection_attempts += 1
            
            # DI Container ì—°ê²°
            self._di_container = get_di_container()
            if not self._di_container:
                return
            
            # ìë™ ì˜ì¡´ì„± ì£¼ì…
            injection_count = 0
            
            # ModelLoader ìë™ ì£¼ì…
            if not self.model_loader:
                try:
                    model_loader = self._di_container.get('ModelLoader') or self._di_container.get('IModelLoader')
                    if model_loader:
                        self.model_loader = model_loader
                        injection_count += 1
                        self.logger.debug("âœ… ModelLoader ìë™ ì£¼ì…")
                except Exception:
                    pass
            
            # MemoryManager ìë™ ì£¼ì…
            if not self.memory_manager:
                try:
                    memory_manager = self._di_container.get('MemoryManager') or self._di_container.get('IMemoryManager')
                    if memory_manager:
                        self.memory_manager = memory_manager
                        injection_count += 1
                        self.logger.debug("âœ… MemoryManager ìë™ ì£¼ì…")
                except Exception:
                    pass
            
            # DataConverter ìë™ ì£¼ì…
            if not self.data_converter:
                try:
                    data_converter = self._di_container.get('DataConverter') or self._di_container.get('IDataConverter')
                    if data_converter:
                        self.data_converter = data_converter
                        injection_count += 1
                        self.logger.debug("âœ… DataConverter ìë™ ì£¼ì…")
                except Exception:
                    pass
            
            # ì£¼ì… ì™„ë£Œ ì²˜ë¦¬
            if injection_count > 0:
                self._dependencies_injected = True
                self.has_model = True
                self.model_loaded = True
                
                # ë©”íŠ¸ë¦­ ê¸°ë¡
                injection_time = time.time() - start_time
                self.performance_metrics['di_injection_time'] = injection_time
                
                self.logger.debug(f"ğŸ‰ ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {injection_count}ê°œ ({injection_time:.3f}s)")
            
        except Exception as e:
            self.logger.debug(f"ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def _finalize(self):
        """STEP 3: ì™„ë£Œ (v13.0ê³¼ ë™ì¼)"""
        self.is_initialized = True
        
        # ìë™ ì›Œë°ì—… (ì„¤ì •ëœ ê²½ìš°)
        if self.config.auto_warmup:
            try:
                warmup_result = self.warmup()
                if warmup_result.get('success', False):
                    self.warmup_completed = True
                    self.is_ready = True
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìë™ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    def _emergency_setup(self, error: Exception):
        """ê¸´ê¸‰ ì„¤ì • (v13.0ê³¼ ë™ì¼)"""
        self.step_name = getattr(self, 'step_name', self.__class__.__name__)
        self.logger = logging.getLogger("emergency")
        self.device = "cpu"
        self.is_initialized = False
        self._dependencies_injected = False
        self.error_count = 1
        self.last_error = str(error)
        print(f"ğŸš¨ {self.step_name} ê¸´ê¸‰ ì´ˆê¸°í™”: {error}")
    
    # ==============================================
    # ğŸ”¥ 7. v13.0 í˜¸í™˜ ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (v13.0 í˜¸í™˜)"""
        self.model_loader = model_loader
        self.logger.info("âœ… ModelLoader ì£¼ì… ì™„ë£Œ")
        if model_loader:
            self.has_model = True
            self.model_loaded = True
    
    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì… (v13.0 í˜¸í™˜)"""
        self.memory_manager = memory_manager
        self.logger.info("âœ… MemoryManager ì£¼ì… ì™„ë£Œ")
    
    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì… (v13.0 í˜¸í™˜)"""
        self.data_converter = data_converter
        self.logger.info("âœ… DataConverter ì£¼ì… ì™„ë£Œ")
    
    # ğŸ”¥ ì¶”ê°€: DI Container í˜¸í™˜ ë©”ì„œë“œë“¤ (ë‚´ë¶€ ì‚¬ìš©)
    def _inject_dependencies_internal(self, **dependencies):
        """ë‚´ë¶€ ì˜ì¡´ì„± ì£¼ì… (DI Container í˜¸í™˜)"""
        for name, dependency in dependencies.items():
            if dependency is not None:
                setattr(self, name, dependency)
                self.logger.debug(f"âœ… {name} ë‚´ë¶€ ì£¼ì… ì™„ë£Œ")
    
    # ==============================================
    # ğŸ”¥ 8. ì‹œìŠ¤í…œ ê°ì§€ ë©”ì„œë“œë“¤ (v13.0ê³¼ ë™ì¼)
    # ==============================================
    
    def _detect_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            if TORCH_AVAILABLE:
                if MPS_AVAILABLE:
                    return "mps"
                elif hasattr(torch, 'cuda') and torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        except:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def _get_memory_info(self) -> float:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        try:
            memory = psutil.virtual_memory()
            return memory.total / 1024**3
        except:
            return 16.0
    
    # ==============================================
    # ğŸ”¥ 9. Stepë“¤ì´ ìš”êµ¬í•˜ëŠ” í•µì‹¬ ë©”ì„œë“œë“¤ (v13.0 í˜¸í™˜ + DI ê°•í™”)
    # ==============================================
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (v13.0 í˜¸í™˜ + DI ê°•í™”)"""
        try:
            # ìºì‹œ í™•ì¸
            cache_key = model_name or "default"
            if cache_key in self.model_cache:
                return self.model_cache[cache_key]
            
            # ğŸ”¥ DI ê¸°ë°˜ ModelLoader ìš°ì„  ì‚¬ìš©
            if self.model_loader:
                try:
                    model = None
                    if hasattr(self.model_loader, 'get_model'):
                        model = self.model_loader.get_model(model_name or "default")
                    elif hasattr(self.model_loader, 'load_model'):
                        model = self.model_loader.load_model(model_name or "default")
                    
                    if model:
                        self.model_cache[cache_key] = model
                        self.has_model = True
                        self.model_loaded = True
                        self._ai_model = model
                        self._ai_model_name = model_name
                        return model
                except Exception as e:
                    self.logger.debug(f"DI ModelLoader ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ë™ì  import (v13.0 ë°©ì‹)
            try:
                import importlib
                loader_module = importlib.import_module('app.ai_pipeline.utils.model_loader')
                get_global_loader = getattr(loader_module, 'get_global_model_loader', None)
                if get_global_loader:
                    loader = get_global_loader()
                    if loader:
                        model = loader.get_model(model_name or "default")
                        if model:
                            self.model_cache[cache_key] = model
                            self.has_model = True
                            self.model_loaded = True
                            return model
            except Exception as e:
                self.logger.debug(f"í´ë°± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            return None
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸°, v13.0 í˜¸í™˜ + DI ê°•í™”)"""
        try:
            # ìºì‹œ í™•ì¸
            cache_key = model_name or "default"
            if cache_key in self.model_cache:
                return self.model_cache[cache_key]
            
            # DI ê¸°ë°˜ ë¹„ë™ê¸° ModelLoader ì‚¬ìš©
            if self.model_loader:
                try:
                    model = None
                    if hasattr(self.model_loader, 'get_model_async'):
                        model = await self.model_loader.get_model_async(model_name or "default")
                    else:
                        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                        loop = asyncio.get_event_loop()
                        model = await loop.run_in_executor(
                            None, 
                            lambda: self.model_loader.get_model(model_name or "default") if hasattr(self.model_loader, 'get_model') else None
                        )
                    
                    if model:
                        self.model_cache[cache_key] = model
                        self.has_model = True
                        self.model_loaded = True
                        return model
                        
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° DI ModelLoader ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, lambda: self.get_model(model_name))
                
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (v13.0 í˜¸í™˜ + DI ê°•í™”)"""
        try:
            # DI ê¸°ë°˜ MemoryManager ìš°ì„  ì‚¬ìš©
            if self.memory_manager:
                try:
                    if hasattr(self.memory_manager, 'optimize_memory'):
                        result = self.memory_manager.optimize_memory(aggressive=aggressive)
                        result["di_enhanced"] = True
                        return result
                    elif hasattr(self.memory_manager, 'optimize'):
                        result = self.memory_manager.optimize(aggressive=aggressive)
                        result["di_enhanced"] = True
                        return result
                except Exception as e:
                    self.logger.debug(f"DI MemoryManager ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ë‚´ì¥ DI ê°•í™”ëœ ë©”ëª¨ë¦¬ ìµœì í™” ì‚¬ìš©
            return self.memory_optimizer.optimize(aggressive=aggressive)
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def optimize_memory_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (ë¹„ë™ê¸°, v13.0 í˜¸í™˜ + DI ê°•í™”)"""
        try:
            # DI ê¸°ë°˜ MemoryManager ë¹„ë™ê¸° ì‚¬ìš©
            if self.memory_manager:
                try:
                    if hasattr(self.memory_manager, 'optimize_memory_async'):
                        result = await self.memory_manager.optimize_memory_async(aggressive=aggressive)
                        result["di_enhanced"] = True
                        return result
                    elif hasattr(self.memory_manager, 'optimize_async'):
                        result = await self.memory_manager.optimize_async(aggressive=aggressive)
                        result["di_enhanced"] = True
                        return result
                    else:
                        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                        loop = asyncio.get_event_loop()
                        result = await loop.run_in_executor(
                            None, 
                            lambda: self.memory_manager.optimize_memory(aggressive=aggressive) if hasattr(self.memory_manager, 'optimize_memory') else {"success": False}
                        )
                        result["di_enhanced"] = True
                        return result
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° DI MemoryManager ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            return await self.memory_optimizer.optimize_async(aggressive=aggressive)
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def warmup(self) -> Dict[str, Any]:
        """ì›Œë°ì—… ì‹¤í–‰ (v13.0 í˜¸í™˜ + DI ê°•í™”)"""
        try:
            if self.warmup_completed:
                return {'success': True, 'message': 'ì´ë¯¸ ì›Œë°ì—… ì™„ë£Œë¨', 'cached': True}
            
            self.logger.info(f"ğŸ”¥ {self.step_name} ì›Œë°ì—… ì‹œì‘...")
            start_time = time.time()
            results = []
            
            # 1. ë©”ëª¨ë¦¬ ì›Œë°ì—… (DI ê°•í™”)
            try:
                memory_result = self.optimize_memory()
                results.append('memory_success' if memory_result.get('success') else 'memory_failed')
            except:
                results.append('memory_failed')
            
            # 2. ëª¨ë¸ ì›Œë°ì—… (DI ê¸°ë°˜)
            try:
                if self.model_loader:
                    test_model = self.get_model("warmup_test")
                    results.append('model_success' if test_model else 'model_skipped')
                else:
                    results.append('model_skipped')
            except:
                results.append('model_failed')
            
            # 3. ë””ë°”ì´ìŠ¤ ì›Œë°ì—…
            try:
                if TORCH_AVAILABLE:
                    test_tensor = torch.randn(10, 10)
                    if self.device != 'cpu':
                        test_tensor = test_tensor.to(self.device)
                    _ = torch.matmul(test_tensor, test_tensor.t())
                    results.append('device_success')
                else:
                    results.append('device_skipped')
            except:
                results.append('device_failed')
            
            duration = time.time() - start_time
            success_count = sum(1 for r in results if 'success' in r)
            overall_success = success_count > 0
            
            if overall_success:
                self.warmup_completed = True
                self.is_ready = True
            
            self.logger.info(f"ğŸ”¥ ì›Œë°ì—… ì™„ë£Œ: {success_count}/{len(results)} ì„±ê³µ ({duration:.2f}ì´ˆ)")
            
            return {
                "success": overall_success,
                "duration": duration,
                "results": results,
                "success_count": success_count,
                "total_count": len(results),
                "di_enhanced": self._dependencies_injected
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def warmup_async(self) -> Dict[str, Any]:
        """ì›Œë°ì—… ì‹¤í–‰ (ë¹„ë™ê¸°, v13.0 í˜¸í™˜ + DI ê°•í™”)"""
        try:
            if self.warmup_completed:
                return {'success': True, 'message': 'ì´ë¯¸ ì›Œë°ì—… ì™„ë£Œë¨', 'cached': True}
            
            self.logger.info(f"ğŸ”¥ {self.step_name} ë¹„ë™ê¸° ì›Œë°ì—… ì‹œì‘...")
            start_time = time.time()
            results = []
            
            # 1. ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì›Œë°ì—… (DI ê°•í™”)
            try:
                memory_result = await self.optimize_memory_async()
                results.append('memory_async_success' if memory_result.get('success') else 'memory_async_failed')
            except:
                results.append('memory_async_failed')
            
            # 2. ë¹„ë™ê¸° ëª¨ë¸ ì›Œë°ì—… (DI ê¸°ë°˜)
            try:
                if self.model_loader:
                    test_model = await self.get_model_async("warmup_test")
                    results.append('model_async_success' if test_model else 'model_async_skipped')
                else:
                    results.append('model_async_skipped')
            except:
                results.append('model_async_failed')
            
            # 3. ë¹„ë™ê¸° ë””ë°”ì´ìŠ¤ ì›Œë°ì—…
            try:
                if TORCH_AVAILABLE:
                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(None, self._device_warmup_sync)
                    results.append('device_async_success')
                else:
                    results.append('device_async_skipped')
            except:
                results.append('device_async_failed')
            
            duration = time.time() - start_time
            success_count = sum(1 for r in results if 'success' in r)
            overall_success = success_count > 0
            
            if overall_success:
                self.warmup_completed = True
                self.is_ready = True
            
            self.logger.info(f"ğŸ”¥ ë¹„ë™ê¸° ì›Œë°ì—… ì™„ë£Œ: {success_count}/{len(results)} ì„±ê³µ ({duration:.2f}ì´ˆ)")
            
            return {
                "success": overall_success,
                "duration": duration,
                "results": results,
                "success_count": success_count,
                "total_count": len(results),
                "async": True,
                "di_enhanced": self._dependencies_injected
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "async": True}
    
    def _device_warmup_sync(self):
        """ë™ê¸° ë””ë°”ì´ìŠ¤ ì›Œë°ì—…"""
        try:
            test_tensor = torch.randn(10, 10)
            if self.device != 'cpu':
                test_tensor = test_tensor.to(self.device)
            _ = torch.matmul(test_tensor, test_tensor.t())
            return True
        except:
            return False
    
    # BaseStepMixin í˜¸í™˜ìš© ë³„ì¹­ (v13.0 í˜¸í™˜)
    async def warmup_step(self) -> Dict[str, Any]:
        """Step ì›Œë°ì—… (BaseStepMixin í˜¸í™˜ìš©)"""
        return await self.warmup_async()
    
    def initialize(self) -> bool:
        """ì´ˆê¸°í™” ë©”ì„œë“œ (v13.0 í˜¸í™˜)"""
        try:
            if self.is_initialized:
                return True
            
            self.is_initialized = True
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_async(self) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™” ë©”ì„œë“œ (v13.0 í˜¸í™˜)"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.initialize)
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def cleanup(self) -> Dict[str, Any]:
        """ì •ë¦¬ (v13.0 í˜¸í™˜ + DI ê°•í™”)"""
        try:
            self.logger.info(f"ğŸ§¹ {self.step_name} ì •ë¦¬ ì‹œì‘...")
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            self.model_cache.clear()
            self.loaded_models.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (DI ê°•í™”)
            cleanup_result = await self.optimize_memory_async(aggressive=True)
            
            # ìƒíƒœ ë¦¬ì…‹
            self.is_ready = False
            self.warmup_completed = False
            
            # ì˜ì¡´ì„± ì •ë¦¬ (ì°¸ì¡°ë§Œ ì œê±°, DI ContainerëŠ” ìœ ì§€)
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            
            self.logger.info(f"âœ… {self.step_name} ì •ë¦¬ ì™„ë£Œ")
            
            return {
                "success": True,
                "cleanup_result": cleanup_result,
                "step_name": self.step_name,
                "di_enhanced": self._dependencies_injected
            }
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def cleanup_models(self):
        """ëª¨ë¸ ì •ë¦¬ (v13.0 í˜¸í™˜)"""
        try:
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            self.model_cache.clear()
            self.loaded_models.clear()
            
            # í˜„ì¬ ëª¨ë¸ ì´ˆê¸°í™”
            self._ai_model = None
            self._ai_model_name = None
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                    except:
                        pass
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
                
                gc.collect()
            
            self.has_model = False
            self.model_loaded = False
            self.logger.info(f"ğŸ§¹ {self.step_name} ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """Step ìƒíƒœ ì¡°íšŒ (v13.0 í˜¸í™˜ + DI ì •ë³´ ì¶”ê°€)"""
        try:
            return {
                'step_name': self.step_name,
                'step_id': getattr(self, 'step_id', 0),
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'has_model': self.has_model,
                'model_loaded': self.model_loaded,
                'warmup_completed': self.warmup_completed,
                'device': self.device,
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb,
                'error_count': self.error_count,
                'last_error': self.last_error,
                'total_processing_count': self.total_processing_count,
                # v13.0 í˜¸í™˜ ì˜ì¡´ì„± ì •ë³´
                'dependencies': {
                    'model_loader': self.model_loader is not None,
                    'memory_manager': self.memory_manager is not None,
                    'data_converter': self.data_converter is not None,
                },
                # ğŸ”¥ DI ì •ë³´ ì¶”ê°€ (í•˜ì§€ë§Œ í˜¸í™˜ì„± ìœ ì§€)
                'di_enhanced': self._dependencies_injected,
                'di_injection_attempts': self._injection_attempts,
                'performance_metrics': self.performance_metrics,
                'conda_info': CONDA_INFO,
                'timestamp': time.time(),
                'version': '14.0-v13-compatible'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'step_name': getattr(self, 'step_name', 'unknown'),
                'error': str(e),
                'version': '14.0-v13-compatible',
                'timestamp': time.time()
            }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ (v13.0 í˜¸í™˜ + DI ë©”íŠ¸ë¦­ ì¶”ê°€)"""
        try:
            return {
                'total_processing_count': self.total_processing_count,
                'last_processing_time': self.last_processing_time,
                'error_count': self.error_count,
                'success_rate': self._calculate_success_rate(),
                'average_process_time': self.performance_metrics.get('average_process_time', 0.0),
                'total_process_time': self.performance_metrics.get('total_process_time', 0.0),
                # ğŸ”¥ DI ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ê°€
                'di_injection_time': self.performance_metrics.get('di_injection_time', 0.0),
                'di_enhanced': self._dependencies_injected,
                'version': '14.0-v13-compatible'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'version': '14.0-v13-compatible', 'error': str(e)}
    
    def _calculate_success_rate(self) -> float:
        """ì„±ê³µë¥  ê³„ì‚° (v13.0ê³¼ ë™ì¼)"""
        try:
            total = self.total_processing_count
            errors = self.error_count
            if total > 0:
                return (total - errors) / total
            return 0.0
        except:
            return 0.0
    
    # ==============================================
    # ğŸ”¥ 10. ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (v13.0 í˜¸í™˜)
    # ==============================================
    
    def record_processing(self, duration: float, success: bool = True):
        """ì²˜ë¦¬ ê¸°ë¡ (v13.0 í˜¸í™˜)"""
        try:
            self.total_processing_count += 1
            self.last_processing_time = time.time()
            
            if not success:
                self.error_count += 1
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.performance_metrics['process_count'] = self.total_processing_count
            self.performance_metrics['total_process_time'] += duration
            self.performance_metrics['average_process_time'] = (
                self.performance_metrics['total_process_time'] / self.total_processing_count
            )
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²˜ë¦¬ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì (ì•ˆì „í•œ ì •ë¦¬, v13.0 í˜¸í™˜)"""
        try:
            if hasattr(self, 'model_cache'):
                self.model_cache.clear()
            # DI ContainerëŠ” ì •ë¦¬í•˜ì§€ ì•ŠìŒ (ì „ì—­ ê´€ë¦¬)
        except:
            pass

# ==============================================
# ğŸ”¥ 11. Stepë³„ íŠ¹í™” Mixinë“¤ (v13.0ê³¼ ë™ì¼)
# ==============================================

class HumanParsingMixin(BaseStepMixin):
    """Step 1: Human Parsing íŠ¹í™” Mixin (v13.0 í˜¸í™˜)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'HumanParsingStep')
        kwargs.setdefault('step_id', 1)
        super().__init__(**kwargs)
        
        self.num_classes = 20
        self.parsing_categories = [
            'background', 'hat', 'hair', 'glove', 'sunglasses', 'upperclothes',
            'dress', 'coat', 'socks', 'pants', 'jumpsuits', 'scarf', 'skirt',
            'face', 'left_arm', 'right_arm', 'left_leg', 'right_leg', 'left_shoe', 'right_shoe'
        ]

class PoseEstimationMixin(BaseStepMixin):
    """Step 2: Pose Estimation íŠ¹í™” Mixin (v13.0 í˜¸í™˜)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'PoseEstimationStep')
        kwargs.setdefault('step_id', 2)
        super().__init__(**kwargs)
        
        self.num_keypoints = 18
        self.keypoint_names = [
            'nose', 'neck', 'right_shoulder', 'right_elbow', 'right_wrist',
            'left_shoulder', 'left_elbow', 'left_wrist', 'right_hip', 'right_knee',
            'right_ankle', 'left_hip', 'left_knee', 'left_ankle', 'right_eye',
            'left_eye', 'right_ear', 'left_ear'
        ]

class ClothSegmentationMixin(BaseStepMixin):
    """Step 3: Cloth Segmentation íŠ¹í™” Mixin (v13.0 í˜¸í™˜)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'ClothSegmentationStep')
        kwargs.setdefault('step_id', 3)
        super().__init__(**kwargs)
        
        self.segmentation_methods = ['traditional', 'u2net', 'deeplab', 'auto', 'hybrid']
        self.segmentation_method = kwargs.get('segmentation_method', 'u2net')

class GeometricMatchingMixin(BaseStepMixin):
    """Step 4: Geometric Matching íŠ¹í™” Mixin (v13.0 í˜¸í™˜)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'GeometricMatchingStep')
        kwargs.setdefault('step_id', 4)
        super().__init__(**kwargs)
        
        self.matching_methods = ['thin_plate_spline', 'affine', 'perspective', 'flow_based']
        self.matching_method = kwargs.get('matching_method', 'thin_plate_spline')
        self.grid_size = kwargs.get('grid_size', (5, 5))

class ClothWarpingMixin(BaseStepMixin):
    """Step 5: Cloth Warping íŠ¹í™” Mixin (v13.0 í˜¸í™˜)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'ClothWarpingStep')
        kwargs.setdefault('step_id', 5)
        super().__init__(**kwargs)
        
        self.warping_stages = ['preprocessing', 'geometric_transformation', 'texture_mapping', 'postprocessing']
        self.warping_quality = kwargs.get('warping_quality', 'high')
        self.preserve_texture = kwargs.get('preserve_texture', True)

class VirtualFittingMixin(BaseStepMixin):
    """Step 6: Virtual Fitting íŠ¹í™” Mixin (v13.0 í˜¸í™˜) - í•µì‹¬ ë‹¨ê³„"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'VirtualFittingStep')
        kwargs.setdefault('step_id', 6)
        super().__init__(**kwargs)
        
        self.fitting_modes = ['standard', 'high_quality', 'fast', 'experimental']
        self.fitting_mode = kwargs.get('fitting_mode', 'high_quality')
        self.diffusion_steps = kwargs.get('diffusion_steps', 50)
        self.guidance_scale = kwargs.get('guidance_scale', 7.5)
        self.use_ootd = kwargs.get('use_ootd', True)

class PostProcessingMixin(BaseStepMixin):
    """Step 7: Post Processing íŠ¹í™” Mixin (v13.0 í˜¸í™˜)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'PostProcessingStep')
        kwargs.setdefault('step_id', 7)
        super().__init__(**kwargs)
        
        self.processing_methods = ['super_resolution', 'denoising', 'color_correction', 'sharpening']
        self.enhancement_level = kwargs.get('enhancement_level', 'medium')
        self.super_resolution_factor = kwargs.get('super_resolution_factor', 2.0)

class QualityAssessmentMixin(BaseStepMixin):
    """Step 8: Quality Assessment íŠ¹í™” Mixin (v13.0 í˜¸í™˜)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'QualityAssessmentStep')
        kwargs.setdefault('step_id', 8)
        super().__init__(**kwargs)
        
        self.assessment_criteria = ['perceptual_quality', 'technical_quality', 'aesthetic_quality', 'overall_quality']
        self.quality_threshold = kwargs.get('quality_threshold', 0.7)
        self.use_clip_score = kwargs.get('use_clip_score', True)

# ==============================================
# ğŸ”¥ 12. v13.0 í˜¸í™˜ í¸ì˜ í•¨ìˆ˜ë“¤ (DI ì ‘ë¯¸ì‚¬ ì œê±°)
# ==============================================

def create_step_mixin(step_name: str, step_id: int, **kwargs) -> BaseStepMixin:
    """BaseStepMixin ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (v13.0 í˜¸í™˜)"""
    kwargs.update({'step_name': step_name, 'step_id': step_id})
    return BaseStepMixin(**kwargs)

def create_human_parsing_step(**kwargs) -> HumanParsingMixin:
    """Human Parsing Step ìƒì„± (v13.0 í˜¸í™˜)"""
    return HumanParsingMixin(**kwargs)

def create_pose_estimation_step(**kwargs) -> PoseEstimationMixin:
    """Pose Estimation Step ìƒì„± (v13.0 í˜¸í™˜)"""
    return PoseEstimationMixin(**kwargs)

def create_cloth_segmentation_step(**kwargs) -> ClothSegmentationMixin:
    """Cloth Segmentation Step ìƒì„± (v13.0 í˜¸í™˜)"""
    return ClothSegmentationMixin(**kwargs)

def create_geometric_matching_step(**kwargs) -> GeometricMatchingMixin:
    """Geometric Matching Step ìƒì„± (v13.0 í˜¸í™˜)"""
    return GeometricMatchingMixin(**kwargs)

def create_cloth_warping_step(**kwargs) -> ClothWarpingMixin:
    """Cloth Warping Step ìƒì„± (v13.0 í˜¸í™˜)"""
    return ClothWarpingMixin(**kwargs)

def create_virtual_fitting_step(**kwargs) -> VirtualFittingMixin:
    """Virtual Fitting Step ìƒì„± (v13.0 í˜¸í™˜) - í•µì‹¬"""
    return VirtualFittingMixin(**kwargs)

def create_post_processing_step(**kwargs) -> PostProcessingMixin:
    """Post Processing Step ìƒì„± (v13.0 í˜¸í™˜)"""
    return PostProcessingMixin(**kwargs)

def create_quality_assessment_step(**kwargs) -> QualityAssessmentMixin:
    """Quality Assessment Step ìƒì„± (v13.0 í˜¸í™˜)"""
    return QualityAssessmentMixin(**kwargs)

def create_m3_max_optimized_step(step_type: str, **kwargs) -> BaseStepMixin:
    """M3 Max ìµœì í™”ëœ Step ìƒì„± (v13.0 í˜¸í™˜)"""
    kwargs.update({
        'device': 'mps',
        'auto_memory_cleanup': True,
        'use_fp16': True
    })
    
    step_creators = {
        'human_parsing': create_human_parsing_step,
        'pose_estimation': create_pose_estimation_step,
        'cloth_segmentation': create_cloth_segmentation_step,
        'geometric_matching': create_geometric_matching_step,
        'cloth_warping': create_cloth_warping_step,
        'virtual_fitting': create_virtual_fitting_step,
        'post_processing': create_post_processing_step,
        'quality_assessment': create_quality_assessment_step,
    }
    
    creator = step_creators.get(step_type, create_step_mixin)
    return creator(**kwargs)

# ğŸ”¥ ì¶”ê°€: DI ê°•í™” í¸ì˜ í•¨ìˆ˜ë“¤ (ë‚´ë¶€ ì‚¬ìš©)
def _create_step_with_auto_di(step_class: Type[BaseStepMixin], **kwargs) -> BaseStepMixin:
    """DI ìë™ ì£¼ì…ì„ ì‚¬ìš©í•˜ì—¬ Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë‚´ë¶€ í•¨ìˆ˜)"""
    try:
        # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìë™ DI ì£¼ì…ë¨)
        step_instance = step_class(**kwargs)
        
        # ì¶”ê°€ DI ì£¼ì…ì´ í•„ìš”í•œ ê²½ìš°
        if DI_CONTAINER_AVAILABLE:
            try:
                di_container = get_di_container()
                if di_container and not step_instance._dependencies_injected:
                    # ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€ ì˜ì¡´ì„± ì£¼ì… ì‹œë„
                    additional_deps = {}
                    
                    for dep_name in ['ModelLoader', 'MemoryManager', 'DataConverter']:
                        dep = di_container.get(dep_name)
                        if dep:
                            additional_deps[dep_name.lower()] = dep
                    
                    if additional_deps:
                        step_instance._inject_dependencies_internal(**additional_deps)
                        
            except Exception as e:
                step_instance.logger.debug(f"ì¶”ê°€ DI ì£¼ì… ì‹¤íŒ¨: {e}")
        
        return step_instance
        
    except Exception as e:
        print(f"âŒ {step_class.__name__} DI ìƒì„± ì‹¤íŒ¨: {e}")
        # í´ë°±: ì¼ë°˜ ìƒì„±
        return step_class(**kwargs)

# ==============================================
# ğŸ”¥ 13. v13.0 í˜¸í™˜ ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
# ==============================================

__all__ = [
    # ğŸ”¥ ë©”ì¸ í´ë˜ìŠ¤ë“¤ (v13.0 í˜¸í™˜)
    'BaseStepMixin',
    'StepConfig',
    'SimpleMemoryOptimizer',  # v13.0 í˜¸í™˜ ì´ë¦„
    
    # ğŸ”¥ Stepë³„ íŠ¹í™” Mixinë“¤ (v13.0ê³¼ ë™ì¼)
    'HumanParsingMixin',
    'PoseEstimationMixin', 
    'ClothSegmentationMixin',
    'GeometricMatchingMixin',
    'ClothWarpingMixin',
    'VirtualFittingMixin',
    'PostProcessingMixin',
    'QualityAssessmentMixin',
    
    # ğŸ”¥ v13.0 í˜¸í™˜ í¸ì˜ í•¨ìˆ˜ë“¤ (_di ì ‘ë¯¸ì‚¬ ì œê±°)
    'create_step_mixin',
    'create_human_parsing_step',
    'create_pose_estimation_step',
    'create_cloth_segmentation_step',
    'create_geometric_matching_step',
    'create_cloth_warping_step',
    'create_virtual_fitting_step',
    'create_post_processing_step',
    'create_quality_assessment_step',
    'create_m3_max_optimized_step',
    
    # ìƒìˆ˜ë“¤ (v13.0ê³¼ ë™ì¼)
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'CONDA_INFO'
]

# ==============================================
# ğŸ”¥ 14. v13.0 í˜¸í™˜ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

print("=" * 80)
print("ğŸ‰ BaseStepMixin v14.0 - v13.0 ì™„ë²½ í˜¸í™˜ + DI ê°•í™” ë²„ì „!")
print("=" * 80)
print("ğŸ”¥ v13.0 ì™„ë²½ í˜¸í™˜ì„±:")
print("   âœ… ëª¨ë“  í•¨ìˆ˜ëª…/í´ë˜ìŠ¤ëª… 100% í˜¸í™˜ (SimpleMemoryOptimizer ë“±)")
print("   âœ… set_model_loader(), set_memory_manager() ë©”ì„œë“œ ìœ ì§€")
print("   âœ… ê¸°ì¡´ í¸ì˜ í•¨ìˆ˜ë“¤ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥")
print("   âœ… ëª¨ë“  Step íŒŒì¼ì´ ìˆ˜ì • ì—†ì´ ì‘ë™")
print("")
print("ğŸ”¥ DI ê°•í™” ê¸°ëŠ¥ (ë°±ê·¸ë¼ìš´ë“œ):")
print("   âœ… DI Container ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì… (ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°!)")
print("   âœ… ìë™ ì˜ì¡´ì„± ì£¼ì… (ì‚¬ìš©ìì—ê²Œ íˆ¬ëª…)")
print("   âœ… ì§€ì—° ë¡œë”©ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì°¨ë‹¨")
print("   âœ… ModelLoader ì§ì ‘ import ì™„ì „ ì œê±°")
print("   âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
print("   âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›")
print("   âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²°")
print("   âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
print("")
print("ğŸš€ v13.0 í˜¸í™˜ ë©”ì„œë“œë“¤ (DI ê°•í™”):")
print("   ğŸ¤– ëª¨ë¸ ì—°ë™: get_model(), get_model_async() (DI ê¸°ë°˜)")
print("   ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™”: optimize_memory(), optimize_memory_async() (DI ê°•í™”)")
print("   ğŸ”¥ ì›Œë°ì—…: warmup(), warmup_async(), warmup_step() (DI ê¸°ë°˜)")
print("   ğŸ“Š ìƒíƒœ ê´€ë¦¬: get_status(), get_performance_summary() (DI ì •ë³´ ì¶”ê°€)")
print("   ğŸ”§ ì´ˆê¸°í™”: initialize(), initialize_async()")
print("   ğŸ§¹ ì •ë¦¬: cleanup(), cleanup_models()")
print("   ğŸ“ ê¸°ë¡: record_processing()")
print("")
print("ğŸ¯ v13.0 í˜¸í™˜ ì˜ì¡´ì„± ì£¼ì…:")
print("   ğŸ’‰ set_model_loader() - ModelLoader ì£¼ì… (v13.0 í˜¸í™˜)")
print("   ğŸ’‰ set_memory_manager() - MemoryManager ì£¼ì… (v13.0 í˜¸í™˜)")
print("   ğŸ’‰ set_data_converter() - DataConverter ì£¼ì… (v13.0 í˜¸í™˜)")
print("")
print("ğŸ¯ 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ Stepë³„ Mixin (v13.0 í˜¸í™˜):")
print("   1ï¸âƒ£ HumanParsingMixin - ì‹ ì²´ ì˜ì—­ ë¶„í• ")
print("   2ï¸âƒ£ PoseEstimationMixin - í¬ì¦ˆ ê°ì§€")
print("   3ï¸âƒ£ ClothSegmentationMixin - ì˜ë¥˜ ë¶„í• ")
print("   4ï¸âƒ£ GeometricMatchingMixin - ê¸°í•˜í•™ì  ë§¤ì¹­")
print("   5ï¸âƒ£ ClothWarpingMixin - ì˜ë¥˜ ë³€í˜•")
print("   6ï¸âƒ£ VirtualFittingMixin - ê°€ìƒ í”¼íŒ… (í•µì‹¬)")
print("   7ï¸âƒ£ PostProcessingMixin - í›„ì²˜ë¦¬")
print("   8ï¸âƒ£ QualityAssessmentMixin - í’ˆì§ˆ í‰ê°€")
print("")
print(f"ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
print(f"   conda í™˜ê²½: {CONDA_INFO['conda_env']}")
print(f"   PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
print(f"   MPS (M3 Max): {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
print(f"   DI Container: {'âœ…' if DI_CONTAINER_AVAILABLE else 'âŒ'}")
print("")
print("ğŸ‰ v13.0ê³¼ 100% í˜¸í™˜ë˜ë©´ì„œ DI Container ìˆœí™˜ì°¸ì¡° ë¬¸ì œ ì™„ì „ í•´ê²°!")
print("ğŸ‰ ê¸°ì¡´ Step íŒŒì¼ë“¤ì´ ìˆ˜ì • ì—†ì´ ê·¸ëŒ€ë¡œ ì‘ë™í•˜ë©° ì„±ëŠ¥ì€ ë”ìš± í–¥ìƒ!")
print("=" * 80)