# backend/app/ai_pipeline/steps/base_step_mixin.py
"""
ğŸ”¥ BaseStepMixin v11.0 - ì™„ì „í•œ í†µí•© êµ¬í˜„ (ëª¨ë“  ë¬¸ì œ í•´ê²°)
========================================================================

âœ… SafeConfig í´ë˜ìŠ¤ ì¤‘ë³µ ì½”ë“œ ì™„ì „ ìˆ˜ì •
âœ… kwargs íŒŒë¼ë¯¸í„° ëˆ„ë½ ë¬¸ì œ í•´ê²°
âœ… _emergency_initialization ë©”ì„œë“œ ì¤‘ë³µ ì •ì˜ í•´ê²°
âœ… ëª¨ë“  í•µì‹¬ ë©”ì„œë“œ ì™„ì „ êµ¬í˜„ (íŒŒì¼ 2 ì°¸ì¡°)
âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²° (coroutine ê²½ê³  ì™„ì „ ì œê±°)
âœ… from functools import wraps ì¶”ê°€ (NameError í•´ê²°)
âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì™„ì „ ì ìš©
âœ… logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ê·¼ë³¸ í•´ê²°
âœ… 89.8GB ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€ ë° í™œìš©
âœ… ModelLoader ì—°ë™ ì™„ì „ ìë™í™”
âœ… SafeFunctionValidator í†µí•©
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ
âœ… ì›Œë°ì—… ì‹œìŠ¤í…œ
âœ… ì—ëŸ¬ ë³µêµ¬ ì‹œìŠ¤í…œ
âœ… ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ì‹œìŠ¤í…œ
âœ… ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°
âœ… Stepë³„ íŠ¹í™” Mixin ì™„ì „ êµ¬í˜„
âœ… ì‹¤ì œ Step íŒŒì¼ë“¤ê³¼ ì™„ë²½ ì—°ë™

Author: MyCloset AI Team
Date: 2025-07-22
Version: 11.0 (Complete Implementation - All Issues Resolved)
"""

# ==============================================
# ğŸ”¥ 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import (ìµœìš°ì„ )
# ==============================================
import os
import gc
import time
import asyncio
import logging
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple, TYPE_CHECKING, Awaitable
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
from functools import wraps, lru_cache
import hashlib
import json
import pickle
import sys
import platform
import subprocess
import psutil
from datetime import datetime
from enum import Enum

# ==============================================
# ğŸ”¥ 2. conda í™˜ê²½ ìš°ì„  ì²´í¬ ë° ë¡œê¹…
# ==============================================
if 'CONDA_DEFAULT_ENV' in os.environ:
    print(f"âœ… conda í™˜ê²½ ê°ì§€: {os.environ['CONDA_DEFAULT_ENV']}")
else:
    print("âš ï¸ conda í™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•ŠìŒ - ê¶Œì¥: conda activate mycloset-ai")

# GPU ì„¤ì • ì•ˆì „ import (conda í™˜ê²½ ìš°ì„ )
try:
    from app.core.gpu_config import safe_mps_empty_cache
except ImportError:
    def safe_mps_empty_cache():
        """MPS ìºì‹œ ì •ë¦¬ í´ë°± í•¨ìˆ˜"""
        import gc
        gc.collect()
        return {"success": True, "method": "fallback_gc"}

# ==============================================
# ğŸ”¥ 3. TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ ë°©ì§€
# ==============================================
if TYPE_CHECKING:
    # íƒ€ì… ì²´í‚¹ ì‹œì—ë§Œ ì„í¬íŠ¸ (ëŸ°íƒ€ì„ì—ëŠ” ì„í¬íŠ¸ ì•ˆë¨)
    from ..interfaces.model_interface import IModelLoader, IStepInterface
    from ..interfaces.memory_interface import IMemoryManager
    from ..interfaces.data_interface import IDataConverter
    from ...core.di_container import DIContainer

# ==============================================
# ğŸ”¥ 4. conda í™˜ê²½ ìš°ì„  ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ì²´í¬
# ==============================================
def check_conda_environment():
    """conda í™˜ê²½ ìƒíƒœ ì²´í¬"""
    conda_info = {
        'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
        'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
        'python_path': sys.executable
    }
    
    if conda_info['conda_env'] != 'none':
        print(f"ğŸ conda í™˜ê²½: {conda_info['conda_env']}")
        if 'mycloset' in conda_info['conda_env'].lower():
            print("âœ… MyCloset AI ì „ìš© conda í™˜ê²½ ê°ì§€")
        else:
            print("âš ï¸ MyCloset AI ì „ìš© í™˜ê²½ ê¶Œì¥: conda create -n mycloset-ai python=3.10")
    
    return conda_info

# conda í™˜ê²½ ì²´í¬ ì‹¤í–‰
CONDA_INFO = check_conda_environment()

# ==============================================
# ğŸ”¥ 5. NumPy 2.x í˜¸í™˜ì„± ë¬¸ì œ ì™„ì „ í•´ê²° (conda ìš°ì„ )
# ==============================================
try:
    import numpy as np
    numpy_version = np.__version__
    major_version = int(numpy_version.split('.')[0])
    
    if major_version >= 2:
        logging.warning(f"âš ï¸ NumPy {numpy_version} ê°ì§€. conda install numpy=1.24.3 ê¶Œì¥")
        try:
            np.set_printoptions(legacy='1.25')
        except:
            pass
    
    NUMPY_AVAILABLE = True
    print(f"ğŸ“Š NumPy {numpy_version} ë¡œë“œ ì™„ë£Œ")
except ImportError:
    NUMPY_AVAILABLE = False
    print("âš ï¸ NumPy ì—†ìŒ - conda install numpy ê¶Œì¥")

# PyTorch ì•ˆì „ Import (conda í™˜ê²½ ìš°ì„ , MPS ì˜¤ë¥˜ ë°©ì§€)
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    # MPS í´ë°± í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    TORCH_AVAILABLE = True
    print(f"ğŸ”¥ PyTorch {torch.__version__} ë¡œë“œ ì™„ë£Œ")
    
    # M3 Max MPS ì§€ì› í™•ì¸
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        print("ğŸ M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
    
except ImportError:
    print("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch torchvision torchaudio -c pytorch ê¶Œì¥")

# PIL ì•ˆì „ Import
PIL_AVAILABLE = False
try:
    from PIL import Image, ImageEnhance, ImageFilter
    PIL_AVAILABLE = True
    print("ğŸ–¼ï¸ PIL ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ PIL ì—†ìŒ - conda install pillow ê¶Œì¥")

# ==============================================
# ğŸ”¥ 6. ì•ˆì „í•œ ë¹„ë™ê¸° ë˜í¼ í•¨ìˆ˜ (í•µì‹¬)
# ==============================================
def safe_async_wrapper(func):
    """ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ë˜í•‘ - coroutine ê²½ê³  ì™„ì „ í•´ê²°"""
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        try:
            # ì´ë²¤íŠ¸ ë£¨í”„ í™•ì¸
            try:
                loop = asyncio.get_running_loop()
                in_event_loop = True
            except RuntimeError:
                in_event_loop = False
            
            if in_event_loop:
                # ì´ë²¤íŠ¸ ë£¨í”„ ë‚´ì—ì„œëŠ” ë™ê¸° ë²„ì „ ì‹¤í–‰
                logger = getattr(self, 'logger', logging.getLogger(self.__class__.__name__))
                logger.debug(f"âš ï¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ {func.__name__} ë™ê¸° ì‹¤í–‰")
                return self._sync_fallback(func.__name__, *args, **kwargs)
            else:
                # ì´ë²¤íŠ¸ ë£¨í”„ ë°–ì—ì„œëŠ” ë¹„ë™ê¸° ì‹¤í–‰
                return asyncio.run(func(self, *args, **kwargs))
        
        except Exception as e:
            logger = getattr(self, 'logger', logging.getLogger(self.__class__.__name__))
            logger.warning(f"âš ï¸ {func.__name__} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return self._sync_fallback(func.__name__, *args, **kwargs)
    
    return wrapper

# ==============================================
# ğŸ”¥ 7. ì•ˆì „í•œ ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤ (ì¤‘ë³µ ì œê±°)
# ==============================================
class SafeConfig:
    """ì•ˆì „í•œ ì„¤ì • ê´€ë¦¬ì - ì¤‘ë³µ ì½”ë“œ ì™„ì „ ìˆ˜ì •"""
    
    def __init__(self, config_data: Optional[Dict[str, Any]] = None, **kwargs):
        """SafeConfig ì´ˆê¸°í™” - kwargs íŒŒë¼ë¯¸í„° ì¶”ê°€"""
        self._data = config_data or {}
        self._lock = threading.RLock()
        
        # ê¸°ë³¸ ì„¤ì •ë“¤ (kwargsì—ì„œ ê°€ì ¸ì˜´)
        self.strict_mode = kwargs.get('strict_mode', True)
        self.fallback_enabled = kwargs.get('fallback_enabled', False)
        self.real_ai_only = kwargs.get('real_ai_only', True)
        self.confidence_threshold = kwargs.get('confidence_threshold', 0.8)
        self.visualization_enabled = kwargs.get('visualization_enabled', True)
        self.return_analysis = kwargs.get('return_analysis', True)
        self.cache_enabled = kwargs.get('cache_enabled', True)
        self.detailed_analysis = kwargs.get('detailed_analysis', True)
        
        # ì¶”ê°€ kwargsë¥¼ _dataì— ë³‘í•©
        self._data.update(kwargs)
        
        # ì„¤ì • ê²€ì¦ ë° ì†ì„± ìë™ ì„¤ì •
        with self._lock:
            for key, value in self._data.items():
                if isinstance(key, str) and key.isidentifier() and not callable(value):
                    try:
                        setattr(self, key, value)
                    except Exception:
                        pass
    
    def get(self, key: str, default: Any = None) -> Any:
        """ì•ˆì „í•œ ê°’ ì¡°íšŒ"""
        try:
            with self._lock:
                return self._data.get(key, default)
        except Exception:
            return default
    
    def set(self, key: str, value: Any):
        """ì•ˆì „í•œ ê°’ ì„¤ì •"""
        try:
            with self._lock:
                if not callable(value):
                    self._data[key] = value
                    if isinstance(key, str) and key.isidentifier():
                        setattr(self, key, value)
        except Exception:
            pass
    
    def __getitem__(self, key):
        try:
            return self._data[key]
        except KeyError:
            raise KeyError(f"ì„¤ì • í‚¤ '{key}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    def __setitem__(self, key, value):
        self.set(key, value)
    
    def __contains__(self, key):
        try:
            return key in self._data
        except:
            return False
    
    def keys(self):
        try:
            return self._data.keys()
        except:
            return []
    
    def values(self):
        try:
            return self._data.values()
        except:
            return []
    
    def items(self):
        try:
            return self._data.items()
        except:
            return []
    
    def update(self, other):
        try:
            with self._lock:
                if isinstance(other, dict):
                    for key, value in other.items():
                        if not callable(value):
                            self._data[key] = value
                            if isinstance(key, str) and key.isidentifier():
                                setattr(self, key, value)
        except Exception as e:
            logging.debug(f"SafeConfig.update ì˜¤ë¥˜: {e}")
    
    def to_dict(self):
        try:
            with self._lock:
                return self._data.copy()
        except:
            return {}

# ==============================================
# ğŸ”¥ 8. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ì‹œìŠ¤í…œ
# ==============================================
@dataclass
class CheckpointInfo:
    """ì²´í¬í¬ì¸íŠ¸ ì •ë³´"""
    name: str
    path: str
    size_gb: float
    model_type: str
    step_compatible: List[str]
    last_modified: datetime
    hash_md5: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class CheckpointManager:
    """ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì - 89.8GB ì²´í¬í¬ì¸íŠ¸ ì™„ì „ í™œìš©"""
    
    def __init__(self, model_dir: str = "/Users/gimdudeul/MVP/mycloset-ai/backend/ai_models"):
        self.model_dir = Path(model_dir)
        self.logger = logging.getLogger(f"{__name__}.CheckpointManager")
        self.checkpoints: Dict[str, CheckpointInfo] = {}
        self._scan_lock = threading.Lock()
        
        # conda í™˜ê²½ì—ì„œ ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸
        if not self.model_dir.exists():
            self.logger.warning(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.model_dir}")
            # conda í™˜ê²½ ê¸°ë°˜ ëŒ€ì²´ ê²½ë¡œ ì‹œë„
            alt_paths = [
                Path.home() / "mycloset-ai" / "ai_models",
                Path.cwd() / "ai_models",
                Path.cwd() / "backend" / "ai_models"
            ]
            for alt_path in alt_paths:
                if alt_path.exists():
                    self.model_dir = alt_path
                    self.logger.info(f"ëŒ€ì²´ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©: {self.model_dir}")
                    break
        
    def scan_checkpoints(self) -> Dict[str, CheckpointInfo]:
        """ì²´í¬í¬ì¸íŠ¸ ìŠ¤ìº” - 89.8GB ì²´í¬í¬ì¸íŠ¸ íƒì§€"""
        try:
            with self._scan_lock:
                self.checkpoints.clear()
                
                if not self.model_dir.exists():
                    self.logger.warning(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.model_dir}")
                    return {}
                
                total_size = 0
                
                # .pth, .pt, .safetensors, .ckpt íŒŒì¼ë“¤ ìŠ¤ìº”
                for pattern in ["*.pth", "*.pt", "*.safetensors", "*.ckpt"]:
                    for checkpoint_file in self.model_dir.rglob(pattern):
                        try:
                            stat = checkpoint_file.stat()
                            size_gb = stat.st_size / (1024**3)
                            total_size += size_gb
                            
                            checkpoint_info = CheckpointInfo(
                                name=checkpoint_file.stem,
                                path=str(checkpoint_file),
                                size_gb=size_gb,
                                model_type=self._detect_model_type(checkpoint_file.name),
                                step_compatible=self._get_compatible_steps(checkpoint_file.name),
                                last_modified=datetime.fromtimestamp(stat.st_mtime)
                            )
                            
                            self.checkpoints[checkpoint_info.name] = checkpoint_info
                            
                            if size_gb > 0.1:  # 100MB ì´ìƒë§Œ ë¡œê¹…
                                self.logger.info(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {checkpoint_info.name} ({size_gb:.1f}GB)")
                                
                        except Exception as e:
                            self.logger.warning(f"ì²´í¬í¬ì¸íŠ¸ ìŠ¤ìº” ì‹¤íŒ¨ {checkpoint_file}: {e}")
                
                self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ìŠ¤ìº” ì™„ë£Œ: {len(self.checkpoints)}ê°œ ë°œê²¬ (ì´ {total_size:.1f}GB)")
                return self.checkpoints
                
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
            return {}
    
    def _detect_model_type(self, filename: str) -> str:
        """íŒŒì¼ëª…ìœ¼ë¡œ ëª¨ë¸ íƒ€ì… ê°ì§€ - 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ê¸°ì¤€"""
        filename_lower = filename.lower()
        
        # Step 1: Human Parsing
        if any(keyword in filename_lower for keyword in ['schp', 'graphonomy', 'parsing', 'human']):
            return "human_parsing"
        
        # Step 2: Pose Estimation
        elif any(keyword in filename_lower for keyword in ['openpose', 'pose', 'keypoint']):
            return "pose_estimation"
        
        # Step 3: Cloth Segmentation
        elif any(keyword in filename_lower for keyword in ['u2net', 'cloth', 'segment', 'mask']):
            return "cloth_segmentation"
        
        # Step 4: Geometric Matching
        elif any(keyword in filename_lower for keyword in ['geometric', 'gmm', 'matching']):
            return "geometric_matching"
        
        # Step 5: Cloth Warping
        elif any(keyword in filename_lower for keyword in ['warp', 'tps', 'transform']):
            return "cloth_warping"
        
        # Step 6: Virtual Fitting (í•µì‹¬)
        elif any(keyword in filename_lower for keyword in ['ootd', 'diffusion', 'fitting', 'viton', 'virtual']):
            return "virtual_fitting"
        
        # Step 7: Post Processing
        elif any(keyword in filename_lower for keyword in ['esrgan', 'super', 'enhance', 'post']):
            return "post_processing"
        
        # Step 8: Quality Assessment
        elif any(keyword in filename_lower for keyword in ['clip', 'quality', 'assess']):
            return "quality_assessment"
        
        else:
            return "unknown"
    
    def _get_compatible_steps(self, filename: str) -> List[str]:
        """í˜¸í™˜ ê°€ëŠ¥í•œ Step ëª©ë¡ - MyCloset AI 8ë‹¨ê³„ ê¸°ì¤€"""
        model_type = self._detect_model_type(filename)
        
        step_mapping = {
            "human_parsing": ["HumanParsingStep", "HumanParsingMixin"],
            "pose_estimation": ["PoseEstimationStep", "PoseEstimationMixin"],
            "cloth_segmentation": ["ClothSegmentationStep", "ClothSegmentationMixin"],
            "geometric_matching": ["GeometricMatchingStep", "GeometricMatchingMixin"],
            "cloth_warping": ["ClothWarpingStep", "ClothWarpingMixin"],
            "virtual_fitting": ["VirtualFittingStep", "VirtualFittingMixin"],
            "post_processing": ["PostProcessingStep", "PostProcessingMixin"],
            "quality_assessment": ["QualityAssessmentStep", "QualityAssessmentMixin"],
            "unknown": []
        }
        
        return step_mapping.get(model_type, [])
    
    def get_checkpoint_for_step(self, step_name: str) -> Optional[CheckpointInfo]:
        """Stepì— ì í•©í•œ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°"""
        try:
            compatible_checkpoints = [
                checkpoint for checkpoint in self.checkpoints.values()
                if step_name in checkpoint.step_compatible
            ]
            
            if compatible_checkpoints:
                # í¬ê¸°ê°€ í° ê²ƒ ìš°ì„  (ë” ì„±ëŠ¥ ì¢‹ì„ ê°€ëŠ¥ì„±)
                return max(compatible_checkpoints, key=lambda x: x.size_gb)
            
            return None
            
        except Exception as e:
            self.logger.error(f"Step ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸° ì‹¤íŒ¨ {step_name}: {e}")
            return None

# ==============================================
# ğŸ”¥ 9. ì˜ì¡´ì„± ì£¼ì… ë„ìš°ë¯¸ í´ë˜ìŠ¤
# ==============================================
class DIHelper:
    """ì˜ì¡´ì„± ì£¼ì… ë„ìš°ë¯¸ - DI Container v2.0 ì™„ë²½ í˜¸í™˜"""
    
    @staticmethod
    def get_di_container() -> Optional['DIContainer']:
        """DI Container ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            from ...core.di_container import get_di_container
            return get_di_container()
        except ImportError:
            logging.debug("DI Container ëª¨ë“ˆ ì—†ìŒ")
            return None
        except Exception as e:
            logging.warning(f"âš ï¸ DI Container ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    @staticmethod
    def inject_model_loader(instance) -> bool:
        """ModelLoader ì£¼ì…"""
        try:
            container = DIHelper.get_di_container()
            if container:
                model_loader = container.get('IModelLoader')
                if model_loader:
                    instance.model_loader = model_loader
                    return True
            
            # í´ë°±: ì§ì ‘ import
            try:
                from ..utils.model_loader import get_global_model_loader
                raw_loader = get_global_model_loader()
                instance.model_loader = raw_loader
                return True
            except ImportError:
                pass
            
            return False
        except Exception as e:
            logging.warning(f"âš ï¸ ModelLoader ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    @staticmethod
    def inject_memory_manager(instance) -> bool:
        """MemoryManager ì£¼ì…"""
        try:
            container = DIHelper.get_di_container()
            if container:
                memory_manager = container.get('IMemoryManager')
                if memory_manager:
                    instance.memory_manager = memory_manager
                    return True
            
            return False
        except Exception as e:
            logging.warning(f"âš ï¸ MemoryManager ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    @staticmethod
    def inject_data_converter(instance) -> bool:
        """DataConverter ì£¼ì…"""
        try:
            container = DIHelper.get_di_container()
            if container:
                data_converter = container.get('IDataConverter')
                if data_converter:
                    instance.data_converter = data_converter
                    return True
            
            return False
        except Exception as e:
            logging.warning(f"âš ï¸ DataConverter ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    @staticmethod
    def inject_all_dependencies(instance) -> Dict[str, bool]:
        """ëª¨ë“  ì˜ì¡´ì„± ì£¼ì…"""
        try:
            results = {}
            
            # ModelLoader ì£¼ì…
            results['model_loader'] = DIHelper.inject_model_loader(instance)
            
            # MemoryManager ì£¼ì…
            results['memory_manager'] = DIHelper.inject_memory_manager(instance)
            
            # DataConverter ì£¼ì…
            results['data_converter'] = DIHelper.inject_data_converter(instance)
            
            # CheckpointManager ì£¼ì… (ì „ì—­ ì‚¬ìš©)
            try:
                if not hasattr(instance, 'checkpoint_manager') or instance.checkpoint_manager is None:
                    if BaseStepMixin._global_checkpoint_manager is None:
                        BaseStepMixin._global_checkpoint_manager = CheckpointManager()
                        BaseStepMixin._global_checkpoint_manager.scan_checkpoints()
                    
                    instance.checkpoint_manager = BaseStepMixin._global_checkpoint_manager
                    results['checkpoint_manager'] = True
                else:
                    results['checkpoint_manager'] = True
            except Exception as e:
                logging.debug(f"CheckpointManager ì£¼ì… ì‹¤íŒ¨: {e}")
                results['checkpoint_manager'] = False
            
            return results
            
        except Exception as e:
            logging.error(f"âŒ ì „ì²´ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return {
                'model_loader': False,
                'memory_manager': False,
                'data_converter': False,
                'checkpoint_manager': False
            }

# ==============================================
# ğŸ”¥ 10. ì›Œë°ì—… ì‹œìŠ¤í…œ (ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²°)
# ==============================================
class WarmupSystem:
    """ì›Œë°ì—… ì‹œìŠ¤í…œ - ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²°"""
    
    def __init__(self, step_instance):
        self.step = step_instance
        self.logger = step_instance.logger
        self.warmup_status = {
            'model_warmup': False,
            'device_warmup': False,
            'memory_warmup': False,
            'pipeline_warmup': False
        }
        self.warmup_times = {}
    
    def run_warmup_sequence(self) -> Dict[str, Any]:
        """ì›Œë°ì—… ì‹œí€€ìŠ¤ ì‹¤í–‰ (ë™ê¸°)"""
        try:
            total_start = time.time()
            results = {}
            
            # 1. ëª¨ë¸ ì›Œë°ì—…
            model_start = time.time()
            model_result = self._model_warmup()
            self.warmup_times['model_warmup'] = time.time() - model_start
            results['model_warmup'] = model_result
            self.warmup_status['model_warmup'] = model_result.get('success', False)
            
            # 2. ë””ë°”ì´ìŠ¤ ì›Œë°ì—…
            device_start = time.time()
            device_result = self._device_warmup()
            self.warmup_times['device_warmup'] = time.time() - device_start
            results['device_warmup'] = device_result
            self.warmup_status['device_warmup'] = device_result.get('success', False)
            
            # 3. ë©”ëª¨ë¦¬ ì›Œë°ì—…
            memory_start = time.time()
            memory_result = self._memory_warmup()
            self.warmup_times['memory_warmup'] = time.time() - memory_start
            results['memory_warmup'] = memory_result
            self.warmup_status['memory_warmup'] = memory_result.get('success', False)
            
            # 4. íŒŒì´í”„ë¼ì¸ ì›Œë°ì—…
            pipeline_start = time.time()
            pipeline_result = self._pipeline_warmup()
            self.warmup_times['pipeline_warmup'] = time.time() - pipeline_start
            results['pipeline_warmup'] = pipeline_result
            self.warmup_status['pipeline_warmup'] = pipeline_result.get('success', False)
            
            total_time = time.time() - total_start
            success_count = sum(1 for status in self.warmup_status.values() if status)
            overall_success = success_count >= 3  # 4ê°œ ì¤‘ 3ê°œ ì´ìƒ ì„±ê³µ
            
            self.logger.info(f"ğŸ”¥ ì›Œë°ì—… ì™„ë£Œ: {success_count}/4 ì„±ê³µ ({total_time:.2f}ì´ˆ)")
            
            return {
                'success': overall_success,
                'total_time': total_time,
                'warmup_status': self.warmup_status.copy(),
                'warmup_times': self.warmup_times.copy(),
                'results': results
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì›Œë°ì—… ì‹œí€€ìŠ¤ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'warmup_status': self.warmup_status.copy()
            }
    
    async def run_warmup_sequence_async(self) -> Dict[str, Any]:
        """ì›Œë°ì—… ì‹œí€€ìŠ¤ ì‹¤í–‰ (ë¹„ë™ê¸°)"""
        try:
            total_start = time.time()
            results = {}
            
            # 1. ëª¨ë¸ ì›Œë°ì—…
            model_start = time.time()
            model_result = await self._model_warmup_async()
            self.warmup_times['model_warmup'] = time.time() - model_start
            results['model_warmup'] = model_result
            self.warmup_status['model_warmup'] = model_result.get('success', False)
            
            # 2. ë””ë°”ì´ìŠ¤ ì›Œë°ì—…
            device_start = time.time()
            device_result = await self._device_warmup_async()
            self.warmup_times['device_warmup'] = time.time() - device_start
            results['device_warmup'] = device_result
            self.warmup_status['device_warmup'] = device_result.get('success', False)
            
            # 3. ë©”ëª¨ë¦¬ ì›Œë°ì—…
            memory_start = time.time()
            memory_result = await self._memory_warmup_async()
            self.warmup_times['memory_warmup'] = time.time() - memory_start
            results['memory_warmup'] = memory_result
            self.warmup_status['memory_warmup'] = memory_result.get('success', False)
            
            # 4. íŒŒì´í”„ë¼ì¸ ì›Œë°ì—…
            pipeline_start = time.time()
            pipeline_result = await self._pipeline_warmup_async()
            self.warmup_times['pipeline_warmup'] = time.time() - pipeline_start
            results['pipeline_warmup'] = pipeline_result
            self.warmup_status['pipeline_warmup'] = pipeline_result.get('success', False)
            
            total_time = time.time() - total_start
            success_count = sum(1 for status in self.warmup_status.values() if status)
            overall_success = success_count >= 3
            
            self.logger.info(f"ğŸ”¥ ë¹„ë™ê¸° ì›Œë°ì—… ì™„ë£Œ: {success_count}/4 ì„±ê³µ ({total_time:.2f}ì´ˆ)")
            
            return {
                'success': overall_success,
                'total_time': total_time,
                'warmup_status': self.warmup_status.copy(),
                'warmup_times': self.warmup_times.copy(),
                'results': results
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì›Œë°ì—… ì‹œí€€ìŠ¤ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'warmup_status': self.warmup_status.copy()
            }
    
    def _model_warmup(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì›Œë°ì—… (ë™ê¸°)"""
        try:
            if hasattr(self.step, 'model_loader') and self.step.model_loader:
                try:
                    test_model = self.step.model_loader.get_model("warmup_test")
                    if test_model:
                        return {'success': True, 'message': 'ëª¨ë¸ ë¡œë” ì›Œë°ì—… ì™„ë£Œ'}
                except:
                    pass
            
            return {'success': True, 'message': 'ëª¨ë¸ ì›Œë°ì—… ê±´ë„ˆëœ€'}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _model_warmup_async(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì›Œë°ì—… (ë¹„ë™ê¸°)"""
        try:
            if hasattr(self.step, 'model_loader') and self.step.model_loader:
                try:
                    if hasattr(self.step.model_loader, 'get_model_async'):
                        test_model = await self.step.model_loader.get_model_async("warmup_test")
                    else:
                        test_model = self.step.model_loader.get_model("warmup_test")
                    
                    if test_model:
                        return {'success': True, 'message': 'ëª¨ë¸ ë¡œë” ë¹„ë™ê¸° ì›Œë°ì—… ì™„ë£Œ'}
                except:
                    pass
            
            return {'success': True, 'message': 'ëª¨ë¸ ë¹„ë™ê¸° ì›Œë°ì—… ê±´ë„ˆëœ€'}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _device_warmup(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì›Œë°ì—… (ë™ê¸°)"""
        try:
            if TORCH_AVAILABLE:
                device = getattr(self.step, 'device', 'cpu')
                
                # í…ŒìŠ¤íŠ¸ í…ì„œ ìƒì„± ë° ì—°ì‚°
                test_tensor = torch.randn(10, 10)
                if device != 'cpu':
                    test_tensor = test_tensor.to(device)
                
                # ê°„ë‹¨í•œ ì—°ì‚°
                result = torch.matmul(test_tensor, test_tensor.t())
                
                return {'success': True, 'message': f'{device} ë””ë°”ì´ìŠ¤ ì›Œë°ì—… ì™„ë£Œ'}
            
            return {'success': True, 'message': 'PyTorch ì—†ìŒ - ì›Œë°ì—… ê±´ë„ˆëœ€'}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _device_warmup_async(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì›Œë°ì—… (ë¹„ë™ê¸°)"""
        try:
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, self._device_warmup)
            return result
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _memory_warmup(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì›Œë°ì—… (ë™ê¸°)"""
        try:
            if hasattr(self.step, 'memory_manager') and self.step.memory_manager:
                result = self.step.memory_manager.optimize_memory()
                return {'success': result.get('success', False), 'message': 'ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ'}
            
            gc.collect()
            return {'success': True, 'message': 'ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ'}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _memory_warmup_async(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì›Œë°ì—… (ë¹„ë™ê¸°)"""
        try:
            if hasattr(self.step, 'memory_manager') and self.step.memory_manager:
                if hasattr(self.step.memory_manager, 'optimize_memory_async'):
                    result = await self.step.memory_manager.optimize_memory_async()
                else:
                    result = self.step.memory_manager.optimize_memory()
                return {'success': result.get('success', False), 'message': 'ë©”ëª¨ë¦¬ ë¹„ë™ê¸° ìµœì í™” ì™„ë£Œ'}
            
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, gc.collect)
            return {'success': True, 'message': 'ê¸°ë³¸ ë©”ëª¨ë¦¬ ë¹„ë™ê¸° ì •ë¦¬ ì™„ë£Œ'}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _pipeline_warmup(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… (ë™ê¸°) - coroutine ê²½ê³  ì™„ì „ í•´ê²°"""
        try:
            if hasattr(self.step, 'warmup_step'):
                warmup_method = getattr(self.step, 'warmup_step')
                
                if asyncio.iscoroutinefunction(warmup_method):
                    try:
                        try:
                            loop = asyncio.get_running_loop()
                            self.logger.warning("âš ï¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ë™ê¸° íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… ìš”ì²­ë¨")
                            return {'success': True, 'message': 'ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… ê±´ë„ˆëœ€ (ë™ê¸° ëª¨ë“œ)'}
                        except RuntimeError:
                            result = asyncio.run(warmup_method())
                            return {'success': result.get('success', True), 'message': 'Step ì›Œë°ì—… ì™„ë£Œ (ë¹„ë™ê¸°â†’ë™ê¸°)'}
                    except Exception as e:
                        self.logger.warning(f"ë¹„ë™ê¸° ì›Œë°ì—… ì‹¤íŒ¨: {e}")
                        return {'success': False, 'error': str(e)}
                else:
                    result = warmup_method()
                    return {'success': result.get('success', True), 'message': 'Step ì›Œë°ì—… ì™„ë£Œ'}
            
            return {'success': True, 'message': 'íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… ê±´ë„ˆëœ€'}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _pipeline_warmup_async(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… (ë¹„ë™ê¸°) - coroutine ê²½ê³  ì™„ì „ í•´ê²°"""
        try:
            if hasattr(self.step, 'warmup_step'):
                warmup_method = getattr(self.step, 'warmup_step')
                
                if asyncio.iscoroutinefunction(warmup_method):
                    result = await warmup_method()
                    return {'success': result.get('success', True), 'message': 'Step ë¹„ë™ê¸° ì›Œë°ì—… ì™„ë£Œ'}
                else:
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(None, warmup_method)
                    return {'success': result.get('success', True), 'message': 'Step ì›Œë°ì—… ì™„ë£Œ (ë™ê¸°â†’ë¹„ë™ê¸°)'}
            
            return {'success': True, 'message': 'íŒŒì´í”„ë¼ì¸ ë¹„ë™ê¸° ì›Œë°ì—… ê±´ë„ˆëœ€'}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}

# ==============================================
# ğŸ”¥ 11. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
# ==============================================
class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, step_instance):
        self.step = step_instance
        self.logger = step_instance.logger
        self.metrics = {
            'operation_times': {},
            'memory_usage': [],
            'error_counts': {},
            'success_rates': {},
            'last_operations': {}
        }
        self._lock = threading.Lock()
    
    def record_operation(self, operation_name: str, duration: float, success: bool):
        """ì‘ì—… ê¸°ë¡"""
        try:
            with self._lock:
                # ì‘ì—… ì‹œê°„ ê¸°ë¡
                if operation_name not in self.metrics['operation_times']:
                    self.metrics['operation_times'][operation_name] = []
                
                self.metrics['operation_times'][operation_name].append(duration)
                
                # ìµœëŒ€ 100ê°œê¹Œì§€ë§Œ ìœ ì§€
                if len(self.metrics['operation_times'][operation_name]) > 100:
                    self.metrics['operation_times'][operation_name].pop(0)
                
                # ì„±ê³µë¥  ê³„ì‚°
                if operation_name not in self.metrics['success_rates']:
                    self.metrics['success_rates'][operation_name] = {'success': 0, 'total': 0}
                
                self.metrics['success_rates'][operation_name]['total'] += 1
                if success:
                    self.metrics['success_rates'][operation_name]['success'] += 1
                
                # ì—ëŸ¬ ì¹´ìš´íŠ¸
                if not success:
                    self.metrics['error_counts'][operation_name] = self.metrics['error_counts'].get(operation_name, 0) + 1
                
                # ë§ˆì§€ë§‰ ì‘ì—… ê¸°ë¡
                self.metrics['last_operations'][operation_name] = {
                    'duration': duration,
                    'success': success,
                    'timestamp': time.time()
                }
                
        except Exception as e:
            self.logger.warning(f"ì„±ëŠ¥ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ"""
        try:
            with self._lock:
                summary = {}
                
                for operation_name, times in self.metrics['operation_times'].items():
                    if times:
                        summary[operation_name] = {
                            'avg_time': sum(times) / len(times),
                            'min_time': min(times),
                            'max_time': max(times),
                            'total_calls': len(times),
                            'success_rate': self._calculate_success_rate(operation_name),
                            'error_count': self.metrics['error_counts'].get(operation_name, 0)
                        }
                
                return summary
                
        except Exception as e:
            self.logger.error(f"ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_success_rate(self, operation_name: str) -> float:
        """ì„±ê³µë¥  ê³„ì‚°"""
        try:
            rates = self.metrics['success_rates'].get(operation_name, {'success': 0, 'total': 0})
            if rates['total'] > 0:
                return rates['success'] / rates['total']
            return 0.0
        except:
            return 0.0

# ==============================================
# ğŸ”¥ 12. ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ
# ==============================================
class StepMemoryOptimizer:
    """Stepë³„ ë©”ëª¨ë¦¬ ìµœì í™” - M3 Max 128GB ì™„ì „ ìµœì í™”"""
    
    def __init__(self, device: str = "mps"):
        self.device = device
        self.is_m3_max = self._detect_m3_max()
        self.logger = logging.getLogger(f"{__name__}.StepMemoryOptimizer")
        self.optimization_history = []
        
        # conda í™˜ê²½ì—ì„œ M3 Max ê°ì§€ í™•ì¸
        if self.is_m3_max:
            self.logger.info("ğŸ M3 Max ê°ì§€ - í†µí•© ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™”")
        
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰ - M3 Max íŠ¹í™”"""
        try:
            start_time = time.time()
            results = []
            
            # Python GC
            before_objects = len(gc.get_objects())
            gc.collect()
            after_objects = len(gc.get_objects())
            freed = before_objects - after_objects
            results.append(f"Python GC: {freed}ê°œ ê°ì²´ í•´ì œ")
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    results.append("CUDA ìºì‹œ ì •ë¦¬")
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        safe_mps_empty_cache()
                        results.append("MPS ìºì‹œ ì •ë¦¬")
                    except Exception as e:
                        results.append(f"MPS ìºì‹œ ì •ë¦¬ ê±´ë„ˆëœ€: {e}")
            
            # M3 Max íŠ¹ë³„ ìµœì í™”
            if self.is_m3_max and aggressive:
                try:
                    # ì¶”ê°€ ë©”ëª¨ë¦¬ ì •ë¦¬ (í†µí•© ë©”ëª¨ë¦¬ í™œìš©)
                    for _ in range(3):
                        gc.collect()
                    results.append("M3 Max í†µí•© ë©”ëª¨ë¦¬ ê³µê²©ì  ì •ë¦¬")
                except Exception as e:
                    results.append(f"M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
            
            optimization_time = time.time() - start_time
            
            result = {
                "success": True,
                "duration": optimization_time,
                "results": results,
                "device": self.device,
                "is_m3_max": self.is_m3_max,
                "aggressive": aggressive,
                "timestamp": time.time()
            }
            
            self.optimization_history.append(result)
            if len(self.optimization_history) > 50:
                self.optimization_history.pop(0)
            
            self.logger.info(f"âœ… Step ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ ({optimization_time:.3f}s)")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Step ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": time.time()
            }
    
    async def optimize_memory_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰ (ë¹„ë™ê¸°)"""
        try:
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, lambda: self.optimize_memory(aggressive))
            return result
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° Step ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": time.time()
            }

# ==============================================
# ğŸ”¥ 13. ë©”ì¸ BaseStepMixin í´ë˜ìŠ¤ (ì™„ì „í•œ êµ¬í˜„)
# ==============================================
class BaseStepMixin:
    """
    ğŸ”¥ BaseStepMixin v11.0 - ì™„ì „í•œ í†µí•© êµ¬í˜„
    
    âœ… SafeConfig í´ë˜ìŠ¤ ì¤‘ë³µ ì½”ë“œ ì™„ì „ ìˆ˜ì •
    âœ… kwargs íŒŒë¼ë¯¸í„° ëˆ„ë½ ë¬¸ì œ í•´ê²°  
    âœ… _emergency_initialization ë©”ì„œë“œ ì¤‘ë³µ ì •ì˜ í•´ê²°
    âœ… ëª¨ë“  í•µì‹¬ ë©”ì„œë“œ ì™„ì „ êµ¬í˜„
    âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
    âœ… M3 Max 128GB ìµœì í™”
    âœ… 89.8GB ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€ ë° í™œìš©
    âœ… ModelLoader ì—°ë™ ì™„ì „ ìë™í™”
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²°
    âœ… logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ê·¼ë³¸ í•´ê²°
    âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì™„ì „ ì ìš©
    âœ… ì‹¤ì œ Step íŒŒì¼ë“¤ê³¼ ì™„ë²½ ì—°ë™
    """
    
    # í´ë˜ìŠ¤ ë³€ìˆ˜
    _class_registry = weakref.WeakSet()
    _initialization_lock = threading.RLock()
    _global_checkpoint_manager = None
    
    def __init__(self, *args, **kwargs):
        """ì™„ì „ ì•ˆì „í•œ ì´ˆê¸°í™” - ëª¨ë“  ê¸°ëŠ¥ í¬í•¨ + kwargs ì™„ì „ ì§€ì›"""
        
        # ===== ğŸ”¥ STEP 0: logger ì†ì„± ìµœìš°ì„  ìƒì„± (ì ˆëŒ€ ëˆ„ë½ ë°©ì§€) =====
        self._ensure_logger_first()
        
        # ===== ğŸ”¥ STEP 1: í´ë˜ìŠ¤ ë“±ë¡ =====
        BaseStepMixin._class_registry.add(self)
        
        # ===== ğŸ”¥ STEP 2: ì™„ì „í•œ ì´ˆê¸°í™” (17ë‹¨ê³„) =====
        with BaseStepMixin._initialization_lock:
            try:
                # DI ì»¨í…Œì´ë„ˆ ì„¤ì •
                self._setup_di_container()
                
                # ì˜ì¡´ì„± ì£¼ì…
                self._inject_dependencies()
                
                # ê¸°ë³¸ ì†ì„± ì„¤ì • (kwargs ì²˜ë¦¬)
                self._setup_basic_attributes(kwargs)
                
                # NumPy í˜¸í™˜ì„± í™•ì¸
                self._check_numpy_compatibility()
                
                # ì•ˆì „í•œ super().__init__ í˜¸ì¶œ
                self._safe_super_init()
                
                # ì‹œìŠ¤í…œ í™˜ê²½ ì„¤ì •
                self._setup_device_and_system(kwargs)
                
                # ì•ˆì „í•œ ì„¤ì • ê´€ë¦¬ (kwargs ì²˜ë¦¬)
                self._setup_config_safely(kwargs)
                
                # ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ
                self._setup_state_management()
                
                # M3 Max ìµœì í™”
                self._setup_m3_max_optimization()
                
                # ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ
                self._setup_memory_optimization()
                
                # ì›Œë°ì—… ì‹œìŠ¤í…œ
                self._setup_warmup_system()
                
                # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
                self._setup_performance_monitoring()
                
                # ModelLoader ì¸í„°í˜ì´ìŠ¤ (DI ê¸°ë°˜)
                self._setup_model_interface_safe()
                
                # ì²´í¬í¬ì¸íŠ¸ íƒì§€ ë° ì—°ë™
                self._setup_checkpoint_detection()
                
                # DI í´ë°± ì„¤ì •
                self.setup_di_fallbacks()
                
                # ìµœì¢… ì´ˆê¸°í™” ì™„ë£Œ
                self._finalize_initialization()
                
                self.logger.info(f"âœ… {self.step_name} BaseStepMixin v11.0 ì´ˆê¸°í™” ì™„ë£Œ")
                self.logger.debug(f"ğŸ”§ Device: {self.device}, Memory: {self.memory_gb}GB, DI: {self.di_available}")
                
            except Exception as e:
                self._emergency_initialization(e)
                if hasattr(self, 'logger'):
                    self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.logger.debug(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    # ==============================================
    # ğŸ”¥ ì´ˆê¸°í™” ë©”ì„œë“œë“¤ (ëª¨ë“  ë©”ì„œë“œ ì™„ì „ êµ¬í˜„)
    # ==============================================
    
    def _ensure_logger_first(self):
        """ğŸ”¥ logger ì†ì„± ìµœìš°ì„  ìƒì„±"""
        try:
            if hasattr(self, 'logger') and self.logger is not None:
                return
            
            class_name = self.__class__.__name__
            step_name = getattr(self, 'step_name', class_name)
            
            # ê³„ì¸µì  ë¡œê±° ì´ë¦„ ìƒì„±
            logger_name = f"pipeline.steps.{step_name}"
            
            # ë¡œê±° ìƒì„± ë° ì„¤ì •
            self.logger = logging.getLogger(logger_name)
            
            if not self.logger.handlers:
                formatter = logging.Formatter(
                    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
                )
                
                handler = logging.StreamHandler()
                handler.setFormatter(formatter)
                self.logger.addHandler(handler)
                self.logger.setLevel(logging.INFO)
            
            # step_name ì†ì„±ë„ ì„¤ì •
            if not hasattr(self, 'step_name'):
                self.step_name = step_name
                
            self.logger.info(f"ğŸ”— {step_name} logger ì†ì„± ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ logger ìƒì„± ì‹¤íŒ¨: {e}")
            self._create_emergency_logger()
    
    def _create_emergency_logger(self):
        """ê¸´ê¸‰ ë¡œê±° ìƒì„±"""
        try:
            self.logger = logging.getLogger("emergency_logger")
        except:
            class EmergencyLogger:
                def info(self, msg): print(f"INFO: {msg}")
                def warning(self, msg): print(f"WARNING: {msg}")
                def error(self, msg): print(f"ERROR: {msg}")
                def debug(self, msg): print(f"DEBUG: {msg}")
            
            self.logger = EmergencyLogger()
    
    def _setup_di_container(self):
        """DI Container ì„¤ì •"""
        try:
            self.di_container = DIHelper.get_di_container()
            self.di_available = self.di_container is not None
            
            if self.di_available:
                self.logger.debug("âœ… DI Container ì—°ê²° ì„±ê³µ")
            else:
                self.logger.warning("âš ï¸ DI Container ì‚¬ìš© ë¶ˆê°€ - í´ë°± ëª¨ë“œ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ì„¤ì • ì‹¤íŒ¨: {e}")
            self.di_container = None
            self.di_available = False
    
    def _inject_dependencies(self):
        """ì˜ì¡´ì„± ì£¼ì… ì‹¤í–‰ - DI Container v2.0 ì™„ë²½ í˜¸í™˜"""
        try:
            injection_results = DIHelper.inject_all_dependencies(self)
            
            # ì£¼ì… ê²°ê³¼ ë¡œê¹…
            successful_deps = [dep for dep, success in injection_results.items() if success]
            failed_deps = [dep for dep, success in injection_results.items() if not success]
            
            if successful_deps:
                self.logger.info(f"âœ… ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {', '.join(successful_deps)}")
            
            if failed_deps:
                self.logger.warning(f"âš ï¸ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {', '.join(failed_deps)} - í´ë°± ëª¨ë“œ")
            
            # Step Interface ìƒì„± ì‹œë„
            if hasattr(self, 'model_loader') and self.model_loader:
                try:
                    step_interface = self.model_loader.create_step_interface(self.step_name)
                    if step_interface:
                        self.step_interface = step_interface
                        self.logger.info("âœ… Step Interface ìƒì„± ì„±ê³µ")
                    else:
                        self.step_interface = None
                        self.logger.debug("âš ï¸ Step Interface ìƒì„± ì‹¤íŒ¨ (None ë°˜í™˜)")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Step Interface ìƒì„± ì‹¤íŒ¨: {e}")
                    self.step_interface = None
            else:
                self.step_interface = None
                self.logger.debug("âš ï¸ ModelLoader ì—†ìŒ - Step Interface ìƒì„± ê±´ë„ˆëœ€")
            
            # DI ìƒíƒœ ì„¤ì •
            success_count = sum(1 for success in injection_results.values() if success)
            self.di_available = success_count > 0
            
            return injection_results
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            # í´ë°±: ëª¨ë“  ì˜ì¡´ì„±ì„ Noneìœ¼ë¡œ ì„¤ì •
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            self.checkpoint_manager = None
            self.step_interface = None
            self.di_available = False
            
            return {
                'model_loader': False,
                'memory_manager': False,
                'data_converter': False,
                'checkpoint_manager': False
            }
    
    def _setup_basic_attributes(self, kwargs: Dict[str, Any]):
        """ê¸°ë³¸ ì†ì„± ì„¤ì • - kwargs ì™„ì „ ì²˜ë¦¬"""
        try:
            # Step ê¸°ë³¸ ì •ë³´
            self.step_name = kwargs.get('step_name', getattr(self, 'step_name', self.__class__.__name__))
            self.step_number = kwargs.get('step_number', getattr(self, 'step_number', 0))
            self.step_type = kwargs.get('step_type', getattr(self, 'step_type', 'unknown'))
            
            # ì—ëŸ¬ ì¶”ì 
            self.error_count = 0
            self.last_error = None
            self.initialization_time = time.time()
            
            # í”Œë˜ê·¸ë“¤
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            
            # ì²˜ë¦¬ ê´€ë ¨
            self.total_processing_count = 0
            self.last_processing_time = None
            self.processing_history = []
            
            # kwargsì—ì„œ ì¶”ê°€ ì†ì„±ë“¤
            for key, value in kwargs.items():
                if key not in ['step_name', 'step_number', 'step_type'] and not callable(value):
                    try:
                        setattr(self, key, value)
                    except Exception:
                        pass
            
            self.logger.debug(f"ğŸ“ {self.step_name} ê¸°ë³¸ ì†ì„± ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ì†ì„± ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _check_numpy_compatibility(self):
        """NumPy í˜¸í™˜ì„± í™•ì¸ - conda í™˜ê²½ ìš°ì„ """
        try:
            if NUMPY_AVAILABLE:
                numpy_version = np.__version__
                major_version = int(numpy_version.split('.')[0])
                
                if major_version >= 2:
                    self.logger.warning(f"âš ï¸ NumPy {numpy_version} ê°ì§€. conda install numpy=1.24.3 ê¶Œì¥")
                    
                    try:
                        np.set_printoptions(legacy='1.25')
                        self.logger.info("âœ… NumPy 2.x í˜¸í™˜ì„± ì„¤ì • ì ìš©")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ NumPy í˜¸í™˜ì„± ì„¤ì • ì‹¤íŒ¨: {e}")
                else:
                    self.logger.debug(f"âœ… NumPy {numpy_version} í˜¸í™˜ì„± ì–‘í˜¸")
            else:
                self.logger.warning("âš ï¸ NumPy ì‚¬ìš© ë¶ˆê°€ - conda install numpy ê¶Œì¥")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ NumPy í˜¸í™˜ì„± í™•ì¸ ì‹¤íŒ¨: {e}")
    
    def _safe_super_init(self):
        """ì•ˆì „í•œ super().__init__ í˜¸ì¶œ"""
        try:
            # MRO í™•ì¸
            mro = self.__class__.__mro__
            
            # BaseStepMixin ì´í›„ì˜ í´ë˜ìŠ¤ê°€ ìˆëŠ”ì§€ í™•ì¸
            base_index = -1
            for i, cls in enumerate(mro):
                if cls.__name__ == 'BaseStepMixin':
                    base_index = i
                    break
            
            if base_index != -1 and base_index < len(mro) - 2:  # object ì œì™¸
                try:
                    # ë‹¤ìŒ í´ë˜ìŠ¤ì˜ __init__ í˜¸ì¶œ
                    next_class = mro[base_index + 1]
                    if hasattr(next_class, '__init__') and next_class != object:
                        super(BaseStepMixin, self).__init__()
                        self.logger.debug(f"âœ… super().__init__ í˜¸ì¶œ ì„±ê³µ: {next_class.__name__}")
                except Exception as e:
                    self.logger.debug(f"âš ï¸ super().__init__ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            self.logger.debug(f"âš ï¸ safe_super_init ì‹¤íŒ¨: {e}")
    
    def _setup_device_and_system(self, kwargs: Dict[str, Any]):
        """ì‹œìŠ¤í…œ í™˜ê²½ ì„¤ì • - conda í™˜ê²½ ìš°ì„  + M3 Max ìµœì í™”"""
        try:
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            self.device = kwargs.get('device', self._detect_optimal_device())
            self.is_m3_max = self._detect_m3_max()
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            memory_info = self._get_memory_info()
            self.memory_gb = memory_info.get("total_gb", 16.0)
            
            # M3 Max íŠ¹í™” ë©”ëª¨ë¦¬ ì„¤ì •
            if self.is_m3_max:
                # 128GB í†µí•© ë©”ëª¨ë¦¬ í™œìš©
                if self.memory_gb >= 64:
                    self.max_model_size_gb = min(40, self.memory_gb * 0.3)  # 30%ê¹Œì§€
                else:
                    self.max_model_size_gb = min(20, self.memory_gb * 0.25)  # 25%ê¹Œì§€
                
                self.logger.info(f"ğŸ M3 Max ê°ì§€ - í†µí•© ë©”ëª¨ë¦¬ {self.memory_gb}GB, ìµœëŒ€ ëª¨ë¸ í¬ê¸°: {self.max_model_size_gb}GB")
            else:
                self.max_model_size_gb = min(16, self.memory_gb * 0.2)
            
            # ìµœì í™” ì„¤ì •
            self.use_fp16 = kwargs.get('use_fp16', True and self.device != 'cpu')
            self.optimization_enabled = kwargs.get('optimization_enabled', True)
            
            # ë””ë°”ì´ìŠ¤ë³„ ì„¤ì •
            if self.device == "mps" and MPS_AVAILABLE:
                self._setup_mps_optimizations()
            elif self.device == "cuda" and TORCH_AVAILABLE and torch.cuda.is_available():
                self._setup_cuda_optimizations()
            
            self.logger.debug(f"ğŸ”§ ì‹œìŠ¤í…œ í™˜ê²½ ì„¤ì • ì™„ë£Œ: {self.device}, {self.memory_gb}GB")
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ í™˜ê²½ ì„¤ì • ì‹¤íŒ¨: {e}")
            # í´ë°± ê°’ë“¤
            self.device = "cpu"
            self.is_m3_max = False
            self.memory_gb = 16.0
            self.use_fp16 = False
            self.optimization_enabled = False
            self.max_model_size_gb = 8.0
    
    def _setup_config_safely(self, kwargs: Dict[str, Any]):
        """ì•ˆì „í•œ ì„¤ì • ê´€ë¦¬ - kwargs ì™„ì „ ì²˜ë¦¬"""
        try:
            config_data = kwargs.get('config', {})
            # SafeConfigì— kwargs ì „ë‹¬ (ì¤‘ë³µ ì½”ë“œ ì œê±°)
            self.config = SafeConfig(config_data, **kwargs)
            
            # ì¶”ê°€ ì„¤ì •ë“¤
            self.input_size = kwargs.get('input_size', (512, 512))
            self.output_size = kwargs.get('output_size', (512, 512))
            self.batch_size = kwargs.get('batch_size', 1)
            self.num_classes = kwargs.get('num_classes', None)
            self.precision = kwargs.get('precision', 'fp16' if self.use_fp16 else 'fp32')
            
            # M3 Max íŠ¹í™” ì„¤ì •
            if self.is_m3_max:
                self.batch_size = min(self.batch_size, 4)  # ë©”ëª¨ë¦¬ ì ˆì•½
                if self.memory_gb >= 64:
                    self.enable_large_batch = True
                else:
                    self.enable_large_batch = False
            
            self.logger.debug(f"âš™ï¸ ì•ˆì „í•œ ì„¤ì • ê´€ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì„¤ì • ê´€ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.config = SafeConfig()
    
    def _setup_state_management(self):
        """ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ ì„¤ì •"""
        try:
            self.state = {
                'status': 'initializing',
                'last_update': time.time(),
                'metrics': {},
                'errors': [],
                'warnings': [],
                'info_messages': []
            }
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ìƒì„¸)
            self.performance_metrics = {
                'process_count': 0,
                'total_process_time': 0.0,
                'average_process_time': 0.0,
                'last_process_time': None,
                'operations': {},
                'memory_usage_history': [],
                'error_history': []
            }
            
            # ìƒíƒœ ë³€ê²½ ì½œë°±
            self.state_change_callbacks = []
            
            self.logger.debug(f"ğŸ“Š ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _setup_m3_max_optimization(self):
        """M3 Max ìµœì í™” ì„¤ì • - conda í™˜ê²½ ê¸°ë°˜"""
        try:
            if not self.is_m3_max:
                self.m3_max_optimizations = None
                return
            
            self.m3_max_optimizations = {
                'memory_pooling': True,
                'neural_engine': True,
                'unified_memory': True,
                'batch_optimization': True,
                'precision_optimization': True
            }
            
            # M3 Max íŠ¹í™” ì„¤ì •
            if MPS_AVAILABLE:
                try:
                    # MPS ìµœì í™” ì„¤ì •
                    if hasattr(torch.backends.mps, 'is_available') and torch.backends.mps.is_available():
                        torch.backends.mps.enabled = True
                        self.logger.info("ğŸ M3 Max MPS ìµœì í™” í™œì„±í™”")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ MPS ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
            
            self.logger.info(f"ğŸ M3 Max ìµœì í™” ì„¤ì • ì™„ë£Œ - ìµœëŒ€ ëª¨ë¸ í¬ê¸°: {self.max_model_size_gb}GB")
            
        except Exception as e:
            self.logger.error(f"âŒ M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
            self.m3_max_optimizations = None
    
    def _setup_memory_optimization(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ ì„¤ì •"""
        try:
            self.memory_optimizer = StepMemoryOptimizer(self.device)
            
            # ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ ì„¤ì •
            self.auto_memory_cleanup = True
            self.memory_threshold = 0.85  # 85% ì‚¬ìš©ì‹œ ì •ë¦¬
            self.last_memory_optimization = None
            
            # M3 Max íŠ¹í™” ìµœì í™”
            if self.is_m3_max and self.optimization_enabled:
                initial_result = self.memory_optimizer.optimize_memory()
                if initial_result['success']:
                    self.logger.info(f"ğŸ M3 Max ì´ˆê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            
            self.logger.debug(f"ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.memory_optimizer = None
    
    def _setup_warmup_system(self):
        """ì›Œë°ì—… ì‹œìŠ¤í…œ ì„¤ì •"""
        try:
            self.warmup_system = WarmupSystem(self)
            self.warmup_completed = False
            self.warmup_results = None
            
            # ì›Œë°ì—… ì„¤ì •
            self.auto_warmup = True
            self.warmup_on_first_use = True
            
            self.logger.debug(f"ğŸ”¥ ì›Œë°ì—… ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì›Œë°ì—… ì‹œìŠ¤í…œ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.warmup_system = None
    
    def _setup_performance_monitoring(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        try:
            self.performance_monitor = PerformanceMonitor(self)
            
            # ë²¤ì¹˜ë§ˆí¬ ê´€ë ¨
            self.start_time = time.time()
            self.benchmark_results = {}
            self.enable_profiling = False
            
            self.logger.debug(f"ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.performance_monitor = None
    
    def _setup_model_interface_safe(self):
        """ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • (ë™ê¸° ì•ˆì „)"""
        try:
            self.logger.info(f"ğŸ”— {self.step_name} ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì¤‘...")
            
            # ëª¨ë¸ ê´€ë ¨ ì†ì„± ì´ˆê¸°í™”
            self._ai_model = None
            self._ai_model_name = None
            self.loaded_models = {}
            self.model_cache = {}
            self._pending_async_setup = False
            
            # ì—°ë™ ìƒíƒœ ë¡œê¹…
            loader_status = "âœ… ì—°ê²°ë¨" if hasattr(self, 'model_loader') and self.model_loader else "âŒ ì—°ê²° ì‹¤íŒ¨"
            interface_status = "âœ… ì—°ê²°ë¨" if hasattr(self, 'step_interface') and self.step_interface else "âŒ ì—°ê²° ì‹¤íŒ¨"
            
            self.logger.info(f"ğŸ”— ModelLoader ì—°ë™ ê²°ê³¼:")
            self.logger.info(f"   - ModelLoader: {loader_status}")
            self.logger.info(f"   - Step Interface: {interface_status}")
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.step_interface = None
            self._pending_async_setup = False
    
    def _setup_checkpoint_detection(self):
        """ì²´í¬í¬ì¸íŠ¸ íƒì§€ ë° ì—°ë™ - 89.8GB ì²´í¬í¬ì¸íŠ¸ í™œìš©"""
        try:
            self.logger.info(f"ğŸ” {self.step_name} ì²´í¬í¬ì¸íŠ¸ íƒì§€ ì‹œì‘...")
            
            # ì „ì—­ ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©
            if BaseStepMixin._global_checkpoint_manager is None:
                BaseStepMixin._global_checkpoint_manager = CheckpointManager()
                BaseStepMixin._global_checkpoint_manager.scan_checkpoints()
            
            self.checkpoint_manager = BaseStepMixin._global_checkpoint_manager
            
            # Stepì— ì í•©í•œ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
            compatible_checkpoint = self.checkpoint_manager.get_checkpoint_for_step(self.step_name)
            
            if compatible_checkpoint:
                self.primary_checkpoint = compatible_checkpoint
                self.logger.info(f"âœ… í˜¸í™˜ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {compatible_checkpoint.name} ({compatible_checkpoint.size_gb:.1f}GB)")
                
                # ëŒ€ìš©ëŸ‰ ì²´í¬í¬ì¸íŠ¸ íŠ¹ë³„ ì²˜ë¦¬
                if compatible_checkpoint.size_gb > 10.0:
                    self.logger.info(f"ğŸ¯ ëŒ€ìš©ëŸ‰ ì²´í¬í¬ì¸íŠ¸ ê°ì§€ - íŠ¹ë³„ ìµœì í™” ì ìš©")
                    self.large_checkpoint_mode = True
                else:
                    self.large_checkpoint_mode = False
            else:
                self.primary_checkpoint = None
                self.large_checkpoint_mode = False
                self.logger.warning(f"âš ï¸ {self.step_name}ì— í˜¸í™˜ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ")
            
            # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì €ì¥
            self.checkpoint_info = {
                'primary': self.primary_checkpoint,
                'total_available': len(self.checkpoint_manager.checkpoints),
                'compatible_count': len([cp for cp in self.checkpoint_manager.checkpoints.values() 
                                       if self.step_name in cp.step_compatible]),
                'large_checkpoint_mode': self.large_checkpoint_mode
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ íƒì§€ ì‹¤íŒ¨: {e}")
            self.primary_checkpoint = None
            self.checkpoint_manager = None
            self.large_checkpoint_mode = False
    
    def _finalize_initialization(self):
        """ìµœì¢… ì´ˆê¸°í™” ì™„ë£Œ ì²˜ë¦¬"""
        try:
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            self.state['status'] = 'initialized'
            self.state['last_update'] = time.time()
            self.is_initialized = True
            
            # ì´ˆê¸°í™” ì‹œê°„ ê¸°ë¡
            initialization_duration = time.time() - self.initialization_time
            self.initialization_duration = initialization_duration
            
            # ìë™ ì›Œë°ì—… (ì„¤ì •ëœ ê²½ìš°)
            if getattr(self, 'auto_warmup', False) and hasattr(self, 'warmup_system'):
                try:
                    self.warmup_results = self.warmup_system.run_warmup_sequence()
                    if self.warmup_results.get('success', False):
                        self.warmup_completed = True
                        self.is_ready = True
                        self.logger.info(f"ğŸ”¥ ìë™ ì›Œë°ì—… ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ìë™ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            # ì´ˆê¸°í™” ì„±ê³µ ë¡œê¹…
            self.logger.info(f"ğŸ‰ {self.step_name} ì´ˆê¸°í™” ì™„ì „ ì™„ë£Œ ({initialization_duration:.3f}ì´ˆ)")
            
        except Exception as e:
            self.logger.error(f"âŒ ìµœì¢… ì´ˆê¸°í™” ì™„ë£Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _emergency_initialization(self, original_error: Exception = None):
        """ğŸ”¥ ê¸´ê¸‰ ì´ˆê¸°í™” (ì—ëŸ¬ ë°œìƒì‹œ) - ì¤‘ë³µ ì •ì˜ í•´ê²°"""
        try:
            # Step ê¸°ë³¸ ì •ë³´ ì„¤ì •
            self.step_name = getattr(self, 'step_name', self.__class__.__name__)
            self.device = "cpu"
            self.is_m3_max = False
            self.memory_gb = 16.0
            self.error_count = getattr(self, 'error_count', 0) + 1
            self.last_error = str(original_error) if original_error else "Emergency initialization"
            
            # ìƒíƒœ í”Œë˜ê·¸ë“¤
            self.is_initialized = False
            self.is_ready = False
            self.di_available = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # ìµœì†Œí•œì˜ ì„¤ì •ë“¤
            self.config = SafeConfig()
            self.performance_metrics = {
                'process_count': 0,
                'total_process_time': 0.0,
                'average_process_time': 0.0,
                'error_history': [str(original_error)] if original_error else []
            }
            self.state = {
                'status': 'emergency', 
                'last_update': time.time(),
                'errors': [f"Emergency initialization: {original_error}"] if original_error else [],
                'warnings': [],
                'metrics': {}
            }
            
            # ê¸°ë³¸ ì†ì„±ë“¤
            self.step_number = 0
            self.step_type = 'emergency'
            self.input_size = (512, 512)
            self.output_size = (512, 512)
            self.batch_size = 1
            self.use_fp16 = False
            self.optimization_enabled = False
            self.auto_memory_cleanup = False
            self.auto_warmup = False
            self._pending_async_setup = False
            
            # ì˜ì¡´ì„±ë“¤ì„ Noneìœ¼ë¡œ ì´ˆê¸°í™”
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            self.step_interface = None
            self.checkpoint_manager = None
            self.performance_monitor = None
            self.warmup_system = None
            self.memory_optimizer = None
            
            # ëª¨ë¸ ê´€ë ¨ ì†ì„±ë“¤
            self._ai_model = None
            self._ai_model_name = None
            self.loaded_models = {}
            self.model_cache = {}
            self.primary_checkpoint = None
            self.large_checkpoint_mode = False
            
            # íƒ€ì´ë° ê´€ë ¨
            self.initialization_time = time.time()
            self.last_processing_time = None
            self.total_processing_count = 0
            self.processing_history = []
            self.last_memory_optimization = None
            
            # ì²´í¬í¬ì¸íŠ¸ ì •ë³´
            self.checkpoint_info = {
                'primary': None,
                'total_available': 0,
                'compatible_count': 0,
                'large_checkpoint_mode': False
            }
            
            # M3 Max ê´€ë ¨
            self.m3_max_optimizations = None
            self.max_model_size_gb = 8.0
            
            # ì½œë°±ê³¼ íˆìŠ¤í† ë¦¬
            self.state_change_callbacks = []
            
            # ë¡œê±° í™•ì¸ ë° ìƒì„±
            if not hasattr(self, 'logger') or self.logger is None:
                self._create_emergency_logger()
            
            # ê¸´ê¸‰ ì´ˆê¸°í™” ì™„ë£Œ ë¡œê¹…
            if hasattr(self, 'logger'):
                self.logger.error(f"ğŸš¨ {self.step_name} ê¸´ê¸‰ ì´ˆê¸°í™” ì‹¤í–‰")
                self.logger.warning("âš ï¸ ìµœì†Œí•œì˜ ê¸°ëŠ¥ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤")
                if original_error:
                    self.logger.error(f"ğŸš¨ ì›ë³¸ ì˜¤ë¥˜: {original_error}")
            else:
                print(f"ğŸš¨ {self.step_name} ê¸´ê¸‰ ì´ˆê¸°í™” ì‹¤í–‰ - ë¡œê±° ì—†ìŒ")
            
        except Exception as e:
            # ìµœí›„ì˜ ìˆ˜ë‹¨: printë¡œ ë¡œê¹…
            print(f"ğŸš¨ ê¸´ê¸‰ ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {e}")
            print(f"ğŸš¨ {getattr(self, 'step_name', 'Unknown')} - ìµœì†Œ ì†ì„±ë§Œ ì„¤ì •")
            
            # ìµœì†Œí•œì˜ ì†ì„±ë“¤ë§Œ ì„¤ì •
            if not hasattr(self, 'step_name'):
                self.step_name = self.__class__.__name__
            if not hasattr(self, 'device'):
                self.device = "cpu"
            if not hasattr(self, 'is_initialized'):
                self.is_initialized = False
            if not hasattr(self, 'error_count'):
                self.error_count = 1
    
    # ==============================================
    # ğŸ”¥ ë””ë°”ì´ìŠ¤ ê´€ë ¨ ë©”ì„œë“œë“¤ (conda í™˜ê²½ ìš°ì„ )
    # ==============================================
    
    def _detect_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€ - conda í™˜ê²½ ìš°ì„ """
        try:
            if TORCH_AVAILABLE:
                # M3 Max MPS ìš°ì„  (conda í™˜ê²½ì—ì„œ)
                if MPS_AVAILABLE:
                    self.logger.info("ğŸ M3 Max MPS ë””ë°”ì´ìŠ¤ ì„ íƒ")
                    return "mps"
                elif hasattr(torch, 'cuda') and torch.cuda.is_available():
                    self.logger.info("ğŸš€ CUDA ë””ë°”ì´ìŠ¤ ì„ íƒ")
                    return "cuda"
            
            self.logger.info("ğŸ’» CPU ë””ë°”ì´ìŠ¤ ì„ íƒ")
            return "cpu"
        except:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€ - conda í™˜ê²½ ê¸°ë°˜"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                is_m3 = 'M3' in result.stdout
                if is_m3:
                    self.logger.info(f"ğŸ M3 Max ê°ì§€: {result.stdout.strip()}")
                return is_m3
        except:
            pass
        return False
    
    def _get_memory_info(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ - M3 Max í†µí•© ë©”ëª¨ë¦¬ íŠ¹í™”"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            
            total_gb = memory.total / 1024**3
            available_gb = memory.available / 1024**3
            percent_used = memory.percent
            
            # M3 Max íŠ¹í™” ë¡œê¹…
            if self.is_m3_max and total_gb >= 64:
                self.logger.info(f"ğŸ M3 Max í†µí•© ë©”ëª¨ë¦¬: {total_gb:.1f}GB (ì‚¬ìš©ë¥ : {percent_used:.1f}%)")
            
            return {
                "total_gb": total_gb,
                "available_gb": available_gb,
                "percent_used": percent_used
            }
        except ImportError:
            return {
                "total_gb": 16.0,
                "available_gb": 8.0,
                "percent_used": 50.0
            }
    
    def _setup_mps_optimizations(self):
        """MPS ìµœì í™” ì„¤ì • - M3 Max íŠ¹í™”"""
        try:
            if not MPS_AVAILABLE:
                return
            
            # MPS íŠ¹í™” ì„¤ì •
            self.mps_optimizations = {
                'fallback_enabled': True,
                'memory_fraction': 0.8,
                'precision': 'fp16' if self.use_fp16 else 'fp32'
            }
            
            self.logger.debug("ğŸ MPS ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ MPS ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _setup_cuda_optimizations(self):
        """CUDA ìµœì í™” ì„¤ì •"""
        try:
            if not TORCH_AVAILABLE or not torch.cuda.is_available():
                return
            
            # CUDA íŠ¹í™” ì„¤ì •
            self.cuda_optimizations = {
                'memory_fraction': 0.9,
                'allow_tf32': True,
                'benchmark': True
            }
            
            # cuDNN ì„¤ì •
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            self.logger.debug("ğŸš€ CUDA ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ CUDA ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ DI ê´€ë ¨ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_di_status(self) -> Dict[str, Any]:
        """DI ìƒíƒœ í™•ì¸ ë©”ì„œë“œ"""
        try:
            container = DIHelper.get_di_container()
            
            dependencies = {}
            if hasattr(self, 'model_loader'):
                dependencies['model_loader'] = self.model_loader is not None
            if hasattr(self, 'memory_manager'):
                dependencies['memory_manager'] = self.memory_manager is not None
            if hasattr(self, 'data_converter'):
                dependencies['data_converter'] = self.data_converter is not None
            if hasattr(self, 'checkpoint_manager'):
                dependencies['checkpoint_manager'] = self.checkpoint_manager is not None
            if hasattr(self, 'performance_monitor'):
                dependencies['performance_monitor'] = self.performance_monitor is not None
            if hasattr(self, 'warmup_system'):
                dependencies['warmup_system'] = self.warmup_system is not None
            
            registered_services = []
            if container:
                try:
                    registered_services = list(container.get_registered_services().keys())
                except:
                    pass
            
            return {
                'di_available': getattr(self, 'di_available', False),
                'container_available': container is not None,
                'dependencies': dependencies,
                'registered_services': registered_services
            }
            
        except Exception as e:
            return {
                'di_available': False,
                'container_available': False,
                'dependencies': {},
                'registered_services': [],
                'error': str(e)
            }

    def reinject_dependencies(self) -> Dict[str, bool]:
        """ì˜ì¡´ì„± ì¬ì£¼ì… ë©”ì„œë“œ"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} ì˜ì¡´ì„± ì¬ì£¼ì… ì‹œì‘...")
            return self._inject_dependencies()
        except Exception as e:
            self.logger.error(f"âŒ ì˜ì¡´ì„± ì¬ì£¼ì… ì‹¤íŒ¨: {e}")
            return {key: False for key in ['model_loader', 'memory_manager', 'data_converter', 'checkpoint_manager']}

    def setup_di_fallbacks(self):
        """DI í´ë°± ì„¤ì • ë©”ì„œë“œ"""
        try:
            # ë©”ëª¨ë¦¬ ìµœì í™” í´ë°± (ë‚´ì¥ StepMemoryOptimizer ì‚¬ìš©)
            if not hasattr(self, 'memory_manager') or self.memory_manager is None:
                try:
                    if not hasattr(self, 'memory_optimizer') or self.memory_optimizer is None:
                        self.memory_optimizer = StepMemoryOptimizer(self.device)
                    self.logger.debug("âœ… ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ í™œì„±í™”")
                except Exception as e:
                    self.logger.debug(f"âš ï¸ ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™” ì‹¤íŒ¨: {e}")
            
            # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í´ë°± (ì „ì—­ CheckpointManager ì‚¬ìš©)
            if not hasattr(self, 'checkpoint_manager') or self.checkpoint_manager is None:
                try:
                    if BaseStepMixin._global_checkpoint_manager is None:
                        BaseStepMixin._global_checkpoint_manager = CheckpointManager()
                        BaseStepMixin._global_checkpoint_manager.scan_checkpoints()
                    
                    self.checkpoint_manager = BaseStepMixin._global_checkpoint_manager
                    self.logger.debug("âœ… ì „ì—­ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì í™œì„±í™”")
                except Exception as e:
                    self.logger.debug(f"âš ï¸ ì „ì—­ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì í™œì„±í™” ì‹¤íŒ¨: {e}")
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„° í´ë°±
            if not hasattr(self, 'performance_monitor') or self.performance_monitor is None:
                try:
                    self.performance_monitor = PerformanceMonitor(self)
                    self.logger.debug("âœ… ë‚´ì¥ ì„±ëŠ¥ ëª¨ë‹ˆí„° í™œì„±í™”")
                except Exception as e:
                    self.logger.debug(f"âš ï¸ ë‚´ì¥ ì„±ëŠ¥ ëª¨ë‹ˆí„° í™œì„±í™” ì‹¤íŒ¨: {e}")
            
            # ì›Œë°ì—… ì‹œìŠ¤í…œ í´ë°±
            if not hasattr(self, 'warmup_system') or self.warmup_system is None:
                try:
                    self.warmup_system = WarmupSystem(self)
                    self.logger.debug("âœ… ë‚´ì¥ ì›Œë°ì—… ì‹œìŠ¤í…œ í™œì„±í™”")
                except Exception as e:
                    self.logger.debug(f"âš ï¸ ë‚´ì¥ ì›Œë°ì—… ì‹œìŠ¤í…œ í™œì„±í™” ì‹¤íŒ¨: {e}")
                    
            self.logger.info("âœ… DI í´ë°± ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ DI í´ë°± ì„¤ì • ì‹¤íŒ¨: {e}")

    def get_di_info_for_status(self) -> Dict[str, Any]:
        """get_status() ë©”ì„œë“œì— í¬í•¨í•  DI ì •ë³´"""
        try:
            di_status = self.get_di_status()
            return {
                'di_available': self.di_available,
                'di_container_connected': di_status.get('container_available', False),
                'dependencies_status': di_status.get('dependencies', {}),
                'registered_services_count': len(di_status.get('registered_services', [])),
                'step_interface_available': hasattr(self, 'step_interface') and self.step_interface is not None,
                'pending_async_setup': getattr(self, '_pending_async_setup', False)
            }
        except Exception as e:
            return {
                'di_available': False,
                'di_container_connected': False,
                'dependencies_status': {},
                'registered_services_count': 0,
                'step_interface_available': False,
                'pending_async_setup': False,
                'error': str(e)
            }
    
    # ==============================================
    # ğŸ”¥ ê³µí†µ ë©”ì„œë“œë“¤ (ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²°)
    # ==============================================
    
    def _sync_fallback(self, method_name: str, *args, **kwargs) -> Dict[str, Any]:
        """ë™ê¸° í´ë°± ì²˜ë¦¬"""
        try:
            if hasattr(self, f"_sync_{method_name}"):
                sync_method = getattr(self, f"_sync_{method_name}")
                return sync_method(*args, **kwargs)
            else:
                # ê¸°ë³¸ ì„±ê³µ ì‘ë‹µ
                return {
                    "success": True,
                    "method": f"sync_fallback_{method_name}",
                    "message": f"{method_name} ë™ê¸° í´ë°± ì‹¤í–‰ ì™„ë£Œ"
                }
        except Exception as e:
            return {
                "success": False,
                "method": f"sync_fallback_{method_name}",
                "error": str(e)
            }
    
    @safe_async_wrapper
    async def warmup_step(self) -> Dict[str, Any]:
        """Step ì›Œë°ì—… (ë¹„ë™ê¸° ì•ˆì „) - í•µì‹¬ ë©”ì„œë“œ"""
        try:
            self.logger.info(f"ğŸ”¥ {self.__class__.__name__} ì›Œë°ì—… ì‹œì‘...")
            
            # ë‹¨ê³„ë³„ ì›Œë°ì—…
            steps = [
                self._warmup_memory,
                self._warmup_model,
                self._warmup_cache,
                self._warmup_components
            ]
            
            results = []
            for i, step in enumerate(steps, 1):
                try:
                    if asyncio.iscoroutinefunction(step):
                        result = await step()
                    else:
                        result = step()
                    results.append(f"step{i}_success")
                except Exception as e:
                    self.logger.debug(f"ì›Œë°ì—… ë‹¨ê³„ {i} ì‹¤íŒ¨: {e}")
                    results.append(f"step{i}_failed")
            
            success_count = sum(1 for r in results if 'success' in r)
            total_count = len(results)
            
            self.logger.info(f"ğŸ”¥ ì›Œë°ì—… ì™„ë£Œ: {success_count}/{total_count} ì„±ê³µ")
            
            return {
                "success": success_count > 0,
                "results": results,
                "success_rate": success_count / total_count if total_count > 0 else 0,
                "step_class": self.__class__.__name__
            }
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def _sync_warmup_step(self) -> Dict[str, Any]:
        """ë™ê¸° ì›Œë°ì—… í´ë°±"""
        try:
            self.logger.info(f"ğŸ”¥ {self.__class__.__name__} ë™ê¸° ì›Œë°ì—…...")
            
            # ê¸°ë³¸ ë™ê¸° ì›Œë°ì—…
            gc_result = self._warmup_memory_sync()
            model_result = self._warmup_model_sync()
            
            return {
                "success": True,
                "method": "sync_warmup",
                "results": [gc_result, model_result],
                "step_class": self.__class__.__name__
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _warmup_memory_sync(self) -> str:
        """ë™ê¸° ë©”ëª¨ë¦¬ ì›Œë°ì—…"""
        try:
            import gc
            collected = gc.collect()
            return f"memory_sync_success_{collected}"
        except:
            return "memory_sync_failed"
    
    def _warmup_model_sync(self) -> str:
        """ë™ê¸° ëª¨ë¸ ì›Œë°ì—…"""
        try:
            if hasattr(self, 'model_loader'):
                return "model_sync_success"
            else:
                return "model_sync_skipped"
        except:
            return "model_sync_failed"
    
    async def _warmup_memory(self) -> str:
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì›Œë°ì—…"""
        try:
            # ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì •ë¦¬
            result = safe_mps_empty_cache()
            return f"memory_async_{result['method']}"
        except Exception as e:
            self.logger.debug(f"ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return "memory_async_failed"
    
    async def _warmup_model(self) -> str:
        """ë¹„ë™ê¸° ëª¨ë¸ ì›Œë°ì—…"""
        try:
            if hasattr(self, 'model_loader') and self.model_loader:
                return "model_async_success"
            else:
                return "model_async_skipped"
        except Exception as e:
            self.logger.debug(f"ë¹„ë™ê¸° ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return "model_async_failed"
    
    async def _warmup_cache(self) -> str:
        """ë¹„ë™ê¸° ìºì‹œ ì›Œë°ì—…"""
        try:
            if hasattr(self, '_cache'):
                return "cache_async_success"
            else:
                return "cache_async_skipped"
        except:
            return "cache_async_failed"
    
    async def _warmup_components(self) -> str:
        """ë¹„ë™ê¸° ì»´í¬ë„ŒíŠ¸ ì›Œë°ì—…"""
        try:
            # Stepë³„ íŠ¹í™” ì»´í¬ë„ŒíŠ¸ ì›Œë°ì—…
            if hasattr(self, '_step_specific_warmup'):
                await self._step_specific_warmup()
                return "components_async_success"
            else:
                return "components_async_skipped"
        except Exception as e:
            self.logger.debug(f"ì»´í¬ë„ŒíŠ¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return "components_async_failed"
    
    async def _step_specific_warmup(self):
        """Stepë³„ íŠ¹í™” ì›Œë°ì—… (ê¸°ë³¸ êµ¬í˜„)"""
        pass
    
    @safe_async_wrapper
    async def cleanup(self) -> Dict[str, Any]:
        """Step ì •ë¦¬ (ë¹„ë™ê¸° ì•ˆì „)"""
        try:
            self.logger.info(f"ğŸ“‹ {self.__class__.__name__} ì •ë¦¬ ì‹œì‘...")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_result = safe_mps_empty_cache()
            
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if hasattr(self, '_cleanup_resources'):
                if asyncio.iscoroutinefunction(self._cleanup_resources):
                    await self._cleanup_resources()
                else:
                    self._cleanup_resources()
            
            self.logger.info(f"âœ… {self.__class__.__name__} ì •ë¦¬ ì™„ë£Œ")
            
            return {
                "success": True,
                "cleanup_method": cleanup_result.get("method", "unknown"),
                "step_class": self.__class__.__name__
            }
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def _sync_cleanup(self) -> Dict[str, Any]:
        """ë™ê¸° ì •ë¦¬ í´ë°±"""
        try:
            import gc
            collected = gc.collect()
            return {
                "success": True,
                "method": "sync_cleanup",
                "collected": collected
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    # ==============================================
    # ğŸ”¥ AI ëª¨ë¸ ì—°ë™ ë©”ì„œë“œë“¤ (í•µì‹¬)
    # ==============================================
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (DI ê¸°ë°˜) - í•µì‹¬ ë©”ì„œë“œ"""
        try:
            # ìºì‹œ í™•ì¸
            cache_key = model_name or "default"
            if hasattr(self, 'model_cache') and cache_key in self.model_cache:
                return self.model_cache[cache_key]
            
            model = None
            
            # DIë¥¼ í†µí•œ ModelLoader ì‚¬ìš©
            if hasattr(self, 'model_loader') and self.model_loader:
                try:
                    if hasattr(self.model_loader, 'get_model'):
                        model = self.model_loader.get_model(model_name or "default")
                except Exception as e:
                    self.logger.debug(f"ModelLoader.get_model ì‹¤íŒ¨: {e}")
            
            # Step ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
            if model is None and hasattr(self, 'step_interface') and self.step_interface:
                try:
                    if hasattr(self.step_interface, 'get_model'):
                        model = self.step_interface.get_model(model_name)
                except Exception as e:
                    self.logger.debug(f"step_interface.get_model ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ì§ì ‘ import
            if model is None:
                try:
                    from ..utils.model_loader import get_global_model_loader
                    loader = get_global_model_loader()
                    if loader and hasattr(loader, 'get_model'):
                        model = loader.get_model(model_name or "default")
                except Exception as e:
                    self.logger.debug(f"í´ë°± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ìºì‹œì— ì €ì¥
            if model is not None:
                if not hasattr(self, 'model_cache'):
                    self.model_cache = {}
                self.model_cache[cache_key] = model
                self.logger.debug(f"âœ… ëª¨ë¸ ìºì‹œ ì €ì¥: {cache_key}")
            
            return model
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (DI ê¸°ë°˜, ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²°) - í•µì‹¬ ë©”ì„œë“œ"""
        try:
            # ìºì‹œ í™•ì¸
            cache_key = model_name or "default"
            if hasattr(self, 'model_cache') and cache_key in self.model_cache:
                return self.model_cache[cache_key]
            
            model = None
            
            # DIë¥¼ í†µí•œ ModelLoader ì‚¬ìš©
            if hasattr(self, 'model_loader') and self.model_loader:
                try:
                    if hasattr(self.model_loader, 'get_model_async'):
                        model = await self.model_loader.get_model_async(model_name or "default")
                    elif hasattr(self.model_loader, 'get_model'):
                        model = self.model_loader.get_model(model_name or "default")
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° ModelLoader ì‹¤íŒ¨: {e}")
            
            # Step ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
            if model is None and hasattr(self, 'step_interface') and self.step_interface:
                try:
                    if hasattr(self.step_interface, 'get_model_async'):
                        model = await self.step_interface.get_model_async(model_name)
                    elif hasattr(self.step_interface, 'get_model'):
                        model = self.step_interface.get_model(model_name)
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° step_interface ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ì§ì ‘ import
            if model is None:
                try:
                    from ..utils.model_loader import get_global_model_loader
                    loader = get_global_model_loader()
                    if loader:
                        if hasattr(loader, 'get_model_async'):
                            model = await loader.get_model_async(model_name or "default")
                        elif hasattr(loader, 'get_model'):
                            model = loader.get_model(model_name or "default")
                except Exception as e:
                    self.logger.debug(f"í´ë°± ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ìºì‹œì— ì €ì¥
            if model is not None:
                if not hasattr(self, 'model_cache'):
                    self.model_cache = {}
                self.model_cache[cache_key] = model
                self.logger.debug(f"âœ… ë¹„ë™ê¸° ëª¨ë¸ ìºì‹œ ì €ì¥: {cache_key}")
            
            return model
                
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    # ==============================================
    # ğŸ”¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ ë©”ì„œë“œë“¤ (í•µì‹¬)
    # ==============================================
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (DI ê¸°ë°˜) - í•µì‹¬ ë©”ì„œë“œ"""
        try:
            # DIë¥¼ í†µí•œ MemoryManager ì‚¬ìš©
            if hasattr(self, 'memory_manager') and self.memory_manager:
                try:
                    if hasattr(self.memory_manager, 'optimize_memory'):
                        result = self.memory_manager.optimize_memory(aggressive=aggressive)
                        if result.get('success', False):
                            self.last_memory_optimization = time.time()
                            return result
                except Exception as e:
                    self.logger.debug(f"DI MemoryManager ì‹¤íŒ¨: {e}")
            
            # ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” ì‚¬ìš©
            if hasattr(self, 'memory_optimizer') and self.memory_optimizer:
                try:
                    result = self.memory_optimizer.optimize_memory(aggressive=aggressive)
                    if result.get('success', False):
                        self.last_memory_optimization = time.time()
                    return result
                except Exception as e:
                    self.logger.debug(f"ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            
            # ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            before_objects = len(gc.get_objects())
            gc.collect()
            after_objects = len(gc.get_objects())
            
            result = {
                "success": True,
                "message": "ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ",
                "objects_freed": before_objects - after_objects,
                "timestamp": time.time()
            }
            
            self.last_memory_optimization = time.time()
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def optimize_memory_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” - ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²°"""
        try:
            # DIë¥¼ í†µí•œ MemoryManager ì‚¬ìš©
            if hasattr(self, 'memory_manager') and self.memory_manager:
                try:
                    if hasattr(self.memory_manager, 'optimize_memory_async'):
                        result = await self.memory_manager.optimize_memory_async(aggressive=aggressive)
                        if result.get('success', False):
                            self.last_memory_optimization = time.time()
                            return result
                    elif hasattr(self.memory_manager, 'optimize_memory'):
                        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                        loop = asyncio.get_event_loop()
                        result = await loop.run_in_executor(
                            None, 
                            lambda: self.memory_manager.optimize_memory(aggressive=aggressive)
                        )
                        if result.get('success', False):
                            self.last_memory_optimization = time.time()
                            return result
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° DI MemoryManager ì‹¤íŒ¨: {e}")
            
            # ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” ì‚¬ìš©
            if hasattr(self, 'memory_optimizer') and self.memory_optimizer:
                try:
                    if hasattr(self.memory_optimizer, 'optimize_memory_async'):
                        result = await self.memory_optimizer.optimize_memory_async(aggressive=aggressive)
                        if result.get('success', False):
                            self.last_memory_optimization = time.time()
                        return result
                    else:
                        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                        loop = asyncio.get_event_loop()
                        result = await loop.run_in_executor(
                            None, 
                            lambda: self.memory_optimizer.optimize_memory(aggressive)
                        )
                        if result.get('success', False):
                            self.last_memory_optimization = time.time()
                        return result
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, lambda: self.optimize_memory(aggressive))
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    # ==============================================
    # ğŸ”¥ ì›Œë°ì—… ë©”ì„œë“œë“¤ (í•µì‹¬)
    # ==============================================
    
    def warmup(self) -> Dict[str, Any]:
        """ì›Œë°ì—… ì‹¤í–‰ (ë™ê¸°) - í•µì‹¬ ë©”ì„œë“œ"""
        try:
            if hasattr(self, 'warmup_system') and self.warmup_system:
                if not getattr(self, 'warmup_completed', False):
                    result = self.warmup_system.run_warmup_sequence()
                    if result.get('success', False):
                        self.warmup_completed = True
                        self.is_ready = True
                        self.warmup_results = result
                    return result
                else:
                    return {
                        'success': True,
                        'message': 'ì´ë¯¸ ì›Œë°ì—… ì™„ë£Œë¨',
                        'cached_results': getattr(self, 'warmup_results', {})
                    }
            
            # ê¸°ë³¸ ì›Œë°ì—…
            return {'success': True, 'message': 'ê¸°ë³¸ ì›Œë°ì—… ì™„ë£Œ'}
            
        except Exception as e:
            self.logger.error(f"âŒ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def warmup_async(self) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì›Œë°ì—… - ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²°"""
        try:
            # ëŒ€ê¸° ì¤‘ì¸ ë¹„ë™ê¸° ì„¤ì • ì²˜ë¦¬
            if getattr(self, '_pending_async_setup', False):
                await self._setup_model_interface_async()
            
            if hasattr(self, 'warmup_system') and self.warmup_system:
                if not getattr(self, 'warmup_completed', False):
                    # ë¹„ë™ê¸° ì›Œë°ì—… ì‹œí€€ìŠ¤ ì‹¤í–‰
                    if hasattr(self.warmup_system, 'run_warmup_sequence_async'):
                        result = await self.warmup_system.run_warmup_sequence_async()
                    else:
                        # ë™ê¸° ì›Œë°ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                        loop = asyncio.get_event_loop()
                        result = await loop.run_in_executor(None, self.warmup_system.run_warmup_sequence)
                    
                    if result.get('success', False):
                        self.warmup_completed = True
                        self.is_ready = True
                        self.warmup_results = result
                    return result
                else:
                    return {
                        'success': True,
                        'message': 'ì´ë¯¸ ì›Œë°ì—… ì™„ë£Œë¨',
                        'cached_results': getattr(self, 'warmup_results', {})
                    }
            
            # ê¸°ë³¸ ë¹„ë™ê¸° ì›Œë°ì—…
            return {'success': True, 'message': 'ê¸°ë³¸ ë¹„ë™ê¸° ì›Œë°ì—… ì™„ë£Œ'}
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def _setup_model_interface_async(self):
        """ModelLoader ì¸í„°í˜ì´ìŠ¤ ë¹„ë™ê¸° ì„¤ì •"""
        try:
            self.logger.info(f"ğŸ”— {self.step_name} ModelLoader ì¸í„°í˜ì´ìŠ¤ ë¹„ë™ê¸° ì„¤ì • ì¤‘...")
            
            # ëŒ€ê¸° ì¤‘ì¸ ë¹„ë™ê¸° ì„¤ì •ì´ ìˆëŠ” ê²½ìš°
            if getattr(self, '_pending_async_setup', False):
                if hasattr(self, 'model_loader') and self.model_loader:
                    try:
                        if hasattr(self.model_loader, 'create_step_interface'):
                            create_method = self.model_loader.create_step_interface
                            
                            if asyncio.iscoroutinefunction(create_method):
                                # ë¹„ë™ê¸° í•¨ìˆ˜ì¸ ê²½ìš° awaitë¡œ í˜¸ì¶œ
                                self.step_interface = await create_method(self.step_name)
                                self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ë¹„ë™ê¸° ìƒì„± ì„±ê³µ")
                            else:
                                # ë™ê¸° í•¨ìˆ˜ì¸ ê²½ìš° ì§ì ‘ í˜¸ì¶œ
                                self.step_interface = create_method(self.step_name)
                                self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì„±ê³µ (ë™ê¸°)")
                            
                            self._pending_async_setup = False
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Step ì¸í„°í˜ì´ìŠ¤ ë¹„ë™ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
                        self.step_interface = None
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì¸í„°í˜ì´ìŠ¤ ë¹„ë™ê¸° ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def has_pending_async_setup(self) -> bool:
        """ë¹„ë™ê¸° ì„¤ì •ì´ ëŒ€ê¸° ì¤‘ì¸ì§€ í™•ì¸"""
        return getattr(self, '_pending_async_setup', False)
    
    async def complete_async_setup(self) -> bool:
        """ëŒ€ê¸° ì¤‘ì¸ ë¹„ë™ê¸° ì„¤ì • ì™„ë£Œ"""
        try:
            if self.has_pending_async_setup():
                return await self._setup_model_interface_async()
            return True
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì„¤ì • ì™„ë£Œ ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ ìƒíƒœ ë° ì„±ëŠ¥ ê´€ë¦¬ ë©”ì„œë“œë“¤ (í•µì‹¬)
    # ==============================================
    
    def get_status(self) -> Dict[str, Any]:
        """Step ìƒíƒœ ì¡°íšŒ - í•µì‹¬ ë©”ì„œë“œ"""
        try:
            status = {
                'step_name': getattr(self, 'step_name', 'unknown'),
                'step_type': getattr(self, 'step_type', 'unknown'),
                'step_number': getattr(self, 'step_number', 0),
                'is_initialized': getattr(self, 'is_initialized', False),
                'is_ready': getattr(self, 'is_ready', False),
                'has_model': getattr(self, 'has_model', False),
                'model_loaded': getattr(self, 'model_loaded', False),
                'warmup_completed': getattr(self, 'warmup_completed', False),
                'device': getattr(self, 'device', 'cpu'),
                'is_m3_max': getattr(self, 'is_m3_max', False),
                'memory_gb': getattr(self, 'memory_gb', 16.0),
                'di_available': getattr(self, 'di_available', False),
                'error_count': getattr(self, 'error_count', 0),
                'last_error': getattr(self, 'last_error', None),
                'total_processing_count': getattr(self, 'total_processing_count', 0),
                'last_processing_time': getattr(self, 'last_processing_time', None),
                'last_memory_optimization': getattr(self, 'last_memory_optimization', None),
                'large_checkpoint_mode': getattr(self, 'large_checkpoint_mode', False),
                'dependencies': {
                    'model_loader': hasattr(self, 'model_loader') and self.model_loader is not None,
                    'memory_manager': hasattr(self, 'memory_manager') and self.memory_manager is not None,
                    'data_converter': hasattr(self, 'data_converter') and self.data_converter is not None,
                    'step_interface': hasattr(self, 'step_interface') and self.step_interface is not None,
                    'warmup_system': hasattr(self, 'warmup_system') and self.warmup_system is not None,
                    'performance_monitor': hasattr(self, 'performance_monitor') and self.performance_monitor is not None,
                    'checkpoint_manager': hasattr(self, 'checkpoint_manager') and self.checkpoint_manager is not None
                },
                'performance_metrics': getattr(self, 'performance_metrics', {}),
                'state': getattr(self, 'state', {}),
                'checkpoint_info': getattr(self, 'checkpoint_info', {}),
                'config': self.config.to_dict() if hasattr(self, 'config') and self.config else {},
                'di_info': self.get_di_info_for_status(),
                'conda_info': CONDA_INFO,
                'timestamp': time.time()
            }
            
            return status
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'step_name': getattr(self, 'step_name', 'unknown'),
                'error': str(e),
                'timestamp': time.time()
            }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ - í•µì‹¬ ë©”ì„œë“œ"""
        try:
            if hasattr(self, 'performance_monitor') and self.performance_monitor:
                return self.performance_monitor.get_performance_summary()
            
            # ê¸°ë³¸ ì„±ëŠ¥ ì •ë³´
            return {
                'total_processing_count': getattr(self, 'total_processing_count', 0),
                'last_processing_time': getattr(self, 'last_processing_time', None),
                'average_processing_time': self._calculate_average_processing_time(),
                'error_count': getattr(self, 'error_count', 0),
                'success_rate': self._calculate_success_rate()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_average_processing_time(self) -> float:
        """í‰ê·  ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°"""
        try:
            if hasattr(self, 'processing_history') and self.processing_history:
                times = [p.get('duration', 0) for p in self.processing_history if isinstance(p, dict)]
                return sum(times) / len(times) if times else 0.0
            return 0.0
        except:
            return 0.0
    
    def _calculate_success_rate(self) -> float:
        """ì„±ê³µë¥  ê³„ì‚°"""
        try:
            total = getattr(self, 'total_processing_count', 0)
            errors = getattr(self, 'error_count', 0)
            if total > 0:
                return (total - errors) / total
            return 0.0
        except:
            return 0.0
    
    # ==============================================
    # ğŸ”¥ ì •ë¦¬ ë° ì •ë¦¬ ë©”ì„œë“œë“¤ (í•µì‹¬)
    # ==============================================
    
    def cleanup_models(self):
        """ëª¨ë¸ ì •ë¦¬ - í•µì‹¬ ë©”ì„œë“œ"""
        try:
            # Step ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            if hasattr(self, 'step_interface') and self.step_interface:
                cleanup_func = getattr(self.step_interface, 'cleanup', None)
                if callable(cleanup_func):
                    cleanup_func()
                    
            # ModelLoader ì •ë¦¬
            if hasattr(self, 'model_loader') and self.model_loader:
                cleanup_func = getattr(self.model_loader, 'cleanup', None)
                if callable(cleanup_func):
                    cleanup_func()
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            if hasattr(self, 'model_cache'):
                self.model_cache.clear()
            
            if hasattr(self, 'loaded_models'):
                self.loaded_models.clear()
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if getattr(self, 'device', 'cpu') == "mps" and MPS_AVAILABLE:
                    try:
                        safe_mps_empty_cache()
                    except Exception:
                        pass
                elif getattr(self, 'device', 'cpu') == "cuda":
                    torch.cuda.empty_cache()
                
                gc.collect()
            
            self.logger.info(f"ğŸ§¹ {getattr(self, 'step_name', 'Unknown')} ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def cleanup(self):
        """ì „ì²´ ì •ë¦¬ - í•µì‹¬ ë©”ì„œë“œ"""
        try:
            # ëª¨ë¸ ì •ë¦¬
            self.cleanup_models()
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„° ì •ë¦¬
            if hasattr(self, 'performance_monitor'):
                self.performance_monitor = None
            
            # ì›Œë°ì—… ì‹œìŠ¤í…œ ì •ë¦¬
            if hasattr(self, 'warmup_system'):
                self.warmup_system = None
            
            # ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬
            if hasattr(self, 'memory_optimizer'):
                self.memory_optimizer = None
            
            # ìƒíƒœ ë¦¬ì…‹
            self.is_initialized = False
            self.is_ready = False
            
            self.logger.info(f"ğŸ§¹ {getattr(self, 'step_name', 'Unknown')} ì „ì²´ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì „ì²´ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì - Coroutine ê²½ê³  ë°©ì§€"""
        try:
            # ë™ê¸° ì •ë¦¬ë§Œ ìˆ˜í–‰ (Coroutine ê²½ê³  ë°©ì§€)
            if hasattr(self, '_sync_cleanup'):
                self._sync_cleanup()
        except:
            pass  # ì†Œë©¸ìì—ì„œëŠ” ì˜ˆì™¸ ë¬´ì‹œ

# ==============================================
# ğŸ”¥ 14. Stepë³„ íŠ¹í™” Mixinë“¤ (8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸)
# ==============================================

class HumanParsingMixin(BaseStepMixin):
    """Step 1: Human Parsing íŠ¹í™” Mixin - ì‹ ì²´ ì˜ì—­ ë¶„í• """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 1
        self.step_type = "human_parsing"
        self.num_classes = 20
        self.output_format = "segmentation_mask"
        self.parsing_categories = [
            'background', 'hat', 'hair', 'glove', 'sunglasses', 'upperclothes',
            'dress', 'coat', 'socks', 'pants', 'jumpsuits', 'scarf', 'skirt',
            'face', 'left_arm', 'right_arm', 'left_leg', 'right_leg', 'left_shoe', 'right_shoe'
        ]
        
        # Human Parsing íŠ¹í™” ì„¤ì •
        self.model_arch = kwargs.get('model_arch', 'schp')
        self.use_graphonomy = kwargs.get('use_graphonomy', True)
        
        self.logger.info(f"ğŸ” Human Parsing Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.num_classes}ê°œ ì¹´í…Œê³ ë¦¬")
    
    async def _step_specific_warmup(self) -> None:
        """Human Parsing íŠ¹í™” ì›Œë°ì—…"""
        try:
            self.logger.debug("ğŸ”¥ Human Parsing íŠ¹í™” ì›Œë°ì—… ì‹œì‘")
            
            # íŒŒì‹± ëª¨ë¸ ì›Œë°ì—…
            model = await self.get_model_async("human_parsing")
            if model:
                self.logger.debug("âœ… Human Parsing ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            else:
                self.logger.debug("âš ï¸ Human Parsing ëª¨ë¸ ì—†ìŒ")
            
            await asyncio.sleep(0.001)  # ìµœì†Œí•œì˜ ë¹„ë™ê¸° ì‘ì—…
            self.logger.debug("âœ… Human Parsing íŠ¹í™” ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Human Parsing íŠ¹í™” ì›Œë°ì—… ì‹¤íŒ¨: {e}")

class PoseEstimationMixin(BaseStepMixin):
    """Step 2: Pose Estimation íŠ¹í™” Mixin - í¬ì¦ˆ ê°ì§€"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 2
        self.step_type = "pose_estimation"
        self.num_keypoints = 18
        self.output_format = "keypoints"
        self.keypoint_names = [
            'nose', 'neck', 'right_shoulder', 'right_elbow', 'right_wrist',
            'left_shoulder', 'left_elbow', 'left_wrist', 'right_hip', 'right_knee',
            'right_ankle', 'left_hip', 'left_knee', 'left_ankle', 'right_eye',
            'left_eye', 'right_ear', 'left_ear'
        ]
        
        # Pose Estimation íŠ¹í™” ì„¤ì •
        self.pose_model = kwargs.get('pose_model', 'openpose')
        self.confidence_threshold = kwargs.get('confidence_threshold', 0.3)
        
        self.logger.info(f"ğŸ¤¸ Pose Estimation Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.num_keypoints}ê°œ í‚¤í¬ì¸íŠ¸")
    
    async def _step_specific_warmup(self) -> None:
        """Pose Estimation íŠ¹í™” ì›Œë°ì—…"""
        try:
            self.logger.debug("ğŸ”¥ Pose Estimation íŠ¹í™” ì›Œë°ì—… ì‹œì‘")
            
            # í¬ì¦ˆ ëª¨ë¸ ì›Œë°ì—…
            model = await self.get_model_async("pose_estimation")
            if model:
                self.logger.debug("âœ… Pose Estimation ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            else:
                self.logger.debug("âš ï¸ Pose Estimation ëª¨ë¸ ì—†ìŒ")
            
            await asyncio.sleep(0.001)
            self.logger.debug("âœ… Pose Estimation íŠ¹í™” ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Pose Estimation íŠ¹í™” ì›Œë°ì—… ì‹¤íŒ¨: {e}")

class ClothSegmentationMixin(BaseStepMixin):
    """Step 3: Cloth Segmentation íŠ¹í™” Mixin - ì˜ë¥˜ ë¶„í• """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 3
        self.step_type = "cloth_segmentation"
        self.output_format = "cloth_mask"
        self.segmentation_methods = ['traditional', 'u2net', 'deeplab', 'auto', 'hybrid']
        
        # Cloth Segmentation íŠ¹í™” ì„¤ì •
        self.segmentation_method = kwargs.get('segmentation_method', 'u2net')
        self.mask_quality = kwargs.get('mask_quality', 'high')
        
        self.logger.info(f"ğŸ‘• Cloth Segmentation Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.segmentation_method} ë°©ë²•")
    
    async def _step_specific_warmup(self) -> None:
        """Cloth Segmentation íŠ¹í™” ì›Œë°ì—…"""
        try:
            self.logger.debug("ğŸ”¥ Cloth Segmentation íŠ¹í™” ì›Œë°ì—… ì‹œì‘")
            
            # ì˜· ë¶„í•  ëª¨ë¸ ì›Œë°ì—…
            model = await self.get_model_async("cloth_segmentation")
            if model:
                self.logger.debug("âœ… Cloth Segmentation ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            else:
                self.logger.debug("âš ï¸ Cloth Segmentation ëª¨ë¸ ì—†ìŒ")
            
            await asyncio.sleep(0.001)
            self.logger.debug("âœ… Cloth Segmentation íŠ¹í™” ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Cloth Segmentation íŠ¹í™” ì›Œë°ì—… ì‹¤íŒ¨: {e}")

class GeometricMatchingMixin(BaseStepMixin):
    """Step 4: Geometric Matching íŠ¹í™” Mixin - ê¸°í•˜í•™ì  ë§¤ì¹­"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 4
        self.step_type = "geometric_matching"
        self.output_format = "transformation_matrix"
        self.matching_methods = ['thin_plate_spline', 'affine', 'perspective', 'flow_based']
        
        # Geometric Matching íŠ¹í™” ì„¤ì •
        self.matching_method = kwargs.get('matching_method', 'thin_plate_spline')
        self.grid_size = kwargs.get('grid_size', (5, 5))
        
        self.logger.info(f"ğŸ“ Geometric Matching Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.matching_method} ë°©ë²•")
    
    async def _step_specific_warmup(self) -> None:
        """Geometric Matching íŠ¹í™” ì›Œë°ì—…"""
        try:
            self.logger.debug("ğŸ”¥ Geometric Matching íŠ¹í™” ì›Œë°ì—… ì‹œì‘")
            
            # ê¸°í•˜í•™ì  ë§¤ì¹­ ëª¨ë¸ ì›Œë°ì—…
            model = await self.get_model_async("geometric_matching")
            if model:
                self.logger.debug("âœ… Geometric Matching ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            else:
                self.logger.debug("âš ï¸ Geometric Matching ëª¨ë¸ ì—†ìŒ")
            
            await asyncio.sleep(0.001)
            self.logger.debug("âœ… Geometric Matching íŠ¹í™” ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Geometric Matching íŠ¹í™” ì›Œë°ì—… ì‹¤íŒ¨: {e}")

class ClothWarpingMixin(BaseStepMixin):
    """Step 5: Cloth Warping íŠ¹í™” Mixin - ì˜ë¥˜ ë³€í˜•"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 5
        self.step_type = "cloth_warping"
        self.output_format = "warped_cloth"
        self.warping_stages = ['preprocessing', 'geometric_transformation', 'texture_mapping', 'postprocessing']
        
        # Cloth Warping íŠ¹í™” ì„¤ì •
        self.warping_quality = kwargs.get('warping_quality', 'high')
        self.preserve_texture = kwargs.get('preserve_texture', True)
        
        self.logger.info(f"ğŸ”„ Cloth Warping Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.warping_quality} í’ˆì§ˆ")
    
    async def _step_specific_warmup(self) -> None:
        """Cloth Warping íŠ¹í™” ì›Œë°ì—…"""
        try:
            self.logger.debug("ğŸ”¥ Cloth Warping íŠ¹í™” ì›Œë°ì—… ì‹œì‘")
            
            # ì˜· ë³€í˜• ëª¨ë¸ ì›Œë°ì—…
            model = await self.get_model_async("cloth_warping")
            if model:
                self.logger.debug("âœ… Cloth Warping ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            else:
                self.logger.debug("âš ï¸ Cloth Warping ëª¨ë¸ ì—†ìŒ")
            
            await asyncio.sleep(0.001)
            self.logger.debug("âœ… Cloth Warping íŠ¹í™” ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Cloth Warping íŠ¹í™” ì›Œë°ì—… ì‹¤íŒ¨: {e}")

class VirtualFittingMixin(BaseStepMixin):
    """Step 6: Virtual Fitting íŠ¹í™” Mixin - ê°€ìƒ í”¼íŒ… (í•µì‹¬ ë‹¨ê³„)"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 6
        self.step_type = "virtual_fitting"
        self.output_format = "fitted_image"
        self.fitting_modes = ['standard', 'high_quality', 'fast', 'experimental']
        
        # Virtual Fitting íŠ¹í™” ì„¤ì • (í•µì‹¬)
        self.fitting_mode = kwargs.get('fitting_mode', 'high_quality')
        self.diffusion_steps = kwargs.get('diffusion_steps', 50)
        self.guidance_scale = kwargs.get('guidance_scale', 7.5)
        self.use_ootd = kwargs.get('use_ootd', True)
        
        self.logger.info(f"ğŸ‘— Virtual Fitting Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.fitting_mode} ëª¨ë“œ")
    
    async def _step_specific_warmup(self) -> None:
        """Virtual Fitting íŠ¹í™” ì›Œë°ì—… (í•µì‹¬)"""
        try:
            self.logger.debug("ğŸ”¥ Virtual Fitting íŠ¹í™” ì›Œë°ì—… ì‹œì‘")
            
            # ê°€ìƒ í”¼íŒ… ëª¨ë¸ ì›Œë°ì—… (í•µì‹¬)
            model = await self.get_model_async("virtual_fitting")
            if model:
                self.logger.debug("âœ… Virtual Fitting ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            else:
                self.logger.debug("âš ï¸ Virtual Fitting ëª¨ë¸ ì—†ìŒ")
            
            await asyncio.sleep(0.001)
            self.logger.debug("âœ… Virtual Fitting íŠ¹í™” ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Virtual Fitting íŠ¹í™” ì›Œë°ì—… ì‹¤íŒ¨: {e}")

class PostProcessingMixin(BaseStepMixin):
    """Step 7: Post Processing íŠ¹í™” Mixin - í›„ì²˜ë¦¬"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 7
        self.step_type = "post_processing"
        self.output_format = "enhanced_image"
        self.processing_methods = ['super_resolution', 'denoising', 'color_correction', 'sharpening']
        
        # Post Processing íŠ¹í™” ì„¤ì •
        self.enhancement_level = kwargs.get('enhancement_level', 'medium')
        self.super_resolution_factor = kwargs.get('super_resolution_factor', 2.0)
        
        self.logger.info(f"âœ¨ Post Processing Mixin ì´ˆê¸°í™” ì™„ë£Œ - {self.enhancement_level} í–¥ìƒ ìˆ˜ì¤€")
    
    async def _step_specific_warmup(self) -> None:
        """Post Processing íŠ¹í™” ì›Œë°ì—…"""
        try:
            self.logger.debug("ğŸ”¥ Post Processing íŠ¹í™” ì›Œë°ì—… ì‹œì‘")
            
            # í›„ì²˜ë¦¬ ëª¨ë¸ ì›Œë°ì—…
            model = await self.get_model_async("post_processing")
            if model:
                self.logger.debug("âœ… Post Processing ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            else:
                self.logger.debug("âš ï¸ Post Processing ëª¨ë¸ ì—†ìŒ")
            
            await asyncio.sleep(0.001)
            self.logger.debug("âœ… Post Processing íŠ¹í™” ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Post Processing íŠ¹í™” ì›Œë°ì—… ì‹¤íŒ¨: {e}")

class QualityAssessmentMixin(BaseStepMixin):
    """Step 8: Quality Assessment íŠ¹í™” Mixin - í’ˆì§ˆ í‰ê°€"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 8
        self.step_type = "quality_assessment"
        self.output_format = "quality_score"
        self.assessment_criteria = ['perceptual_quality', 'technical_quality', 'aesthetic_quality', 'overall_quality']
        
        # Quality Assessment íŠ¹í™” ì„¤ì •
        self.quality_threshold = kwargs.get('quality_threshold', 0.7)
        self.use_clip_score = kwargs.get('use_clip_score', True)
        
        self.logger.info(f"ğŸ† Quality Assessment Mixin ì´ˆê¸°í™” ì™„ë£Œ - ì„ê³„ê°’: {self.quality_threshold}")
    
    async def _step_specific_warmup(self) -> None:
        """Quality Assessment íŠ¹í™” ì›Œë°ì—…"""
        try:
            self.logger.debug("ğŸ”¥ Quality Assessment íŠ¹í™” ì›Œë°ì—… ì‹œì‘")
            
            # í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ì›Œë°ì—…
            model = await self.get_model_async("quality_assessment")
            if model:
                self.logger.debug("âœ… Quality Assessment ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            else:
                self.logger.debug("âš ï¸ Quality Assessment ëª¨ë¸ ì—†ìŒ")
            
            await asyncio.sleep(0.001)
            self.logger.debug("âœ… Quality Assessment íŠ¹í™” ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Quality Assessment íŠ¹í™” ì›Œë°ì—… ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 15. ì•ˆì „í•œ ë°ì½”ë ˆì´í„°ë“¤ (ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²°)
# ==============================================

def safe_step_method(func: Callable) -> Callable:
    """Step ë©”ì„œë“œ ì•ˆì „ ì‹¤í–‰ ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        try:
            # logger ì†ì„± í™•ì¸ ë° ë³´ì¥
            if not hasattr(self, 'logger') or self.logger is None:
                self._ensure_logger_first()
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            start_time = time.time()
            
            result = func(self, *args, **kwargs)
            
            # ì„±ëŠ¥ ê¸°ë¡
            duration = time.time() - start_time
            if hasattr(self, 'performance_monitor') and self.performance_monitor:
                self.performance_monitor.record_operation(func.__name__, duration, True)
            
            return result
            
        except Exception as e:
            # ì—ëŸ¬ ê¸°ë¡
            duration = time.time() - start_time if 'start_time' in locals() else 0
            if hasattr(self, 'performance_monitor') and self.performance_monitor:
                self.performance_monitor.record_operation(func.__name__, duration, False)
            
            # ì—ëŸ¬ ì¹´ìš´íŠ¸ ì¦ê°€
            if hasattr(self, 'error_count'):
                self.error_count += 1
            
            # ë§ˆì§€ë§‰ ì—ëŸ¬ ì €ì¥
            if hasattr(self, 'last_error'):
                self.last_error = str(e)
            
            # ë¡œê¹…
            if hasattr(self, 'logger'):
                self.logger.error(f"âŒ {func.__name__} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                self.logger.debug(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            return {
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__,
                'step_name': getattr(self, 'step_name', self.__class__.__name__),
                'method_name': func.__name__,
                'duration': duration,
                'timestamp': time.time()
            }
    
    return wrapper

def async_safe_step_method(func: Callable) -> Callable:
    """ì•ˆì „í•œ ë¹„ë™ê¸° Step ë©”ì„œë“œ ì‹¤í–‰ ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        try:
            # logger ì†ì„± í™•ì¸ ë° ë³´ì¥
            if not hasattr(self, 'logger') or self.logger is None:
                self._ensure_logger_first()
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            start_time = time.time()
            
            # ë¹„ë™ê¸° í•¨ìˆ˜ì¸ì§€ í™•ì¸í•˜ê³  ì ì ˆíˆ í˜¸ì¶œ
            if asyncio.iscoroutinefunction(func):
                result = await func(self, *args, **kwargs)
            else:
                # ë™ê¸° í•¨ìˆ˜ì¸ ê²½ìš° executorë¡œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, lambda: func(self, *args, **kwargs))
            
            # ì„±ëŠ¥ ê¸°ë¡
            duration = time.time() - start_time
            if hasattr(self, 'performance_monitor') and self.performance_monitor:
                self.performance_monitor.record_operation(f"{func.__name__}_async", duration, True)
            
            return result
            
        except Exception as e:
            # ì—ëŸ¬ ê¸°ë¡
            duration = time.time() - start_time if 'start_time' in locals() else 0
            if hasattr(self, 'performance_monitor') and self.performance_monitor:
                self.performance_monitor.record_operation(f"{func.__name__}_async", duration, False)
            
            # ì—ëŸ¬ ì¹´ìš´íŠ¸ ì¦ê°€
            if hasattr(self, 'error_count'):
                self.error_count += 1
            
            # ë§ˆì§€ë§‰ ì—ëŸ¬ ì €ì¥
            if hasattr(self, 'last_error'):
                self.last_error = str(e)
            
            # ë¡œê¹…
            if hasattr(self, 'logger'):
                self.logger.error(f"âŒ {func.__name__} ë¹„ë™ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                self.logger.debug(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            return {
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__,
                'step_name': getattr(self, 'step_name', self.__class__.__name__),
                'method_name': func.__name__,
                'async': True,
                'duration': duration,
                'timestamp': time.time()
            }
    
    return wrapper

def performance_monitor(operation_name: str) -> Callable:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(self, *args, **kwargs):
            start_time = time.time()
            success = True
            try:
                result = func(self, *args, **kwargs)
                return result
            except Exception as e:
                success = False
                raise e
            finally:
                duration = time.time() - start_time
                
                # ì„±ëŠ¥ ëª¨ë‹ˆí„°ì— ê¸°ë¡
                if hasattr(self, 'performance_monitor') and self.performance_monitor:
                    self.performance_monitor.record_operation(operation_name, duration, success)
                
                # ê¸°ë³¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì—ë„ ê¸°ë¡
                if hasattr(self, 'performance_metrics'):
                    if 'operations' not in self.performance_metrics:
                        self.performance_metrics['operations'] = {}
                    
                    if operation_name not in self.performance_metrics['operations']:
                        self.performance_metrics['operations'][operation_name] = {
                            'count': 0,
                            'total_time': 0.0,
                            'success_count': 0,
                            'failure_count': 0,
                            'avg_time': 0.0
                        }
                    
                    op_metrics = self.performance_metrics['operations'][operation_name]
                    op_metrics['count'] += 1
                    op_metrics['total_time'] += duration
                    op_metrics['avg_time'] = op_metrics['total_time'] / op_metrics['count']
                    
                    if success:
                        op_metrics['success_count'] += 1
                    else:
                        op_metrics['failure_count'] += 1
                        
        return wrapper
    return decorator

def async_performance_monitor(operation_name: str) -> Callable:
    """ë¹„ë™ê¸° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            start_time = time.time()
            success = True
            try:
                # ë¹„ë™ê¸° í•¨ìˆ˜ì¸ì§€ í™•ì¸í•˜ê³  ì ì ˆíˆ í˜¸ì¶œ
                if asyncio.iscoroutinefunction(func):
                    result = await func(self, *args, **kwargs)
                else:
                    # ë™ê¸° í•¨ìˆ˜ì¸ ê²½ìš° executorë¡œ ì‹¤í–‰
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(None, lambda: func(self, *args, **kwargs))
                return result
            except Exception as e:
                success = False
                raise e
            finally:
                duration = time.time() - start_time
                
                # ì„±ëŠ¥ ëª¨ë‹ˆí„°ì— ê¸°ë¡
                if hasattr(self, 'performance_monitor') and self.performance_monitor:
                    self.performance_monitor.record_operation(f"{operation_name}_async", duration, success)
                
                # ê¸°ë³¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì—ë„ ê¸°ë¡
                if hasattr(self, 'performance_metrics'):
                    if 'operations' not in self.performance_metrics:
                        self.performance_metrics['operations'] = {}
                    
                    async_op_name = f"{operation_name}_async"
                    if async_op_name not in self.performance_metrics['operations']:
                        self.performance_metrics['operations'][async_op_name] = {
                            'count': 0,
                            'total_time': 0.0,
                            'success_count': 0,
                            'failure_count': 0,
                            'avg_time': 0.0
                        }
                    
                    op_metrics = self.performance_metrics['operations'][async_op_name]
                    op_metrics['count'] += 1
                    op_metrics['total_time'] += duration
                    op_metrics['avg_time'] = op_metrics['total_time'] / op_metrics['count']
                    
                    if success:
                        op_metrics['success_count'] += 1
                    else:
                        op_metrics['failure_count'] += 1
                        
        return wrapper
    return decorator

def memory_optimize_after(func: Callable) -> Callable:
    """ë©”ì„œë“œ ì‹¤í–‰ í›„ ìë™ ë©”ëª¨ë¦¬ ìµœì í™”"""
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        try:
            result = func(self, *args, **kwargs)
            
            # ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ (ì„¤ì •ëœ ê²½ìš°)
            if getattr(self, 'auto_memory_cleanup', False):
                try:
                    self.optimize_memory()
                except Exception as e:
                    if hasattr(self, 'logger'):
                        self.logger.debug(f"ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            return result
            
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒì‹œì—ë„ ë©”ëª¨ë¦¬ ì •ë¦¬
            if getattr(self, 'auto_memory_cleanup', False):
                try:
                    self.optimize_memory(aggressive=True)
                except:
                    pass
            raise e
    
    return wrapper

def async_memory_optimize_after(func: Callable) -> Callable:
    """ë¹„ë™ê¸° ë©”ì„œë“œ ì‹¤í–‰ í›„ ìë™ ë©”ëª¨ë¦¬ ìµœì í™”"""
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        try:
            # ë¹„ë™ê¸° í•¨ìˆ˜ì¸ì§€ í™•ì¸í•˜ê³  ì ì ˆíˆ í˜¸ì¶œ
            if asyncio.iscoroutinefunction(func):
                result = await func(self, *args, **kwargs)
            else:
                # ë™ê¸° í•¨ìˆ˜ì¸ ê²½ìš° executorë¡œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, lambda: func(self, *args, **kwargs))
            
            # ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ (ì„¤ì •ëœ ê²½ìš°)
            if getattr(self, 'auto_memory_cleanup', False):
                try:
                    await self.optimize_memory_async()
                except Exception as e:
                    if hasattr(self, 'logger'):
                        self.logger.debug(f"ìë™ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            return result
            
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒì‹œì—ë„ ë©”ëª¨ë¦¬ ì •ë¦¬
            if getattr(self, 'auto_memory_cleanup', False):
                try:
                    await self.optimize_memory_async(aggressive=True)
                except:
                    pass
            raise e
    
    return wrapper

# ==============================================
# ğŸ”¥ 16. ë¹„ë™ê¸° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

async def ensure_coroutine(func_or_coro, *args, **kwargs) -> Any:
    """í•¨ìˆ˜ë‚˜ ì½”ë£¨í‹´ì„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•˜ëŠ” ìœ í‹¸ë¦¬í‹°"""
    try:
        if asyncio.iscoroutinefunction(func_or_coro):
            return await func_or_coro(*args, **kwargs)
        elif callable(func_or_coro):
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, lambda: func_or_coro(*args, **kwargs))
        elif asyncio.iscoroutine(func_or_coro):
            return await func_or_coro
        else:
            return func_or_coro
    except Exception as e:
        logging.error(f"âŒ ensure_coroutine ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return None

def is_coroutine_function_safe(func) -> bool:
    """ì•ˆì „í•œ ì½”ë£¨í‹´ í•¨ìˆ˜ ê²€ì‚¬"""
    try:
        return asyncio.iscoroutinefunction(func)
    except:
        return False

def is_coroutine_safe(obj) -> bool:
    """ì•ˆì „í•œ ì½”ë£¨í‹´ ê°ì²´ ê²€ì‚¬"""
    try:
        return asyncio.iscoroutine(obj)
    except:
        return False

async def run_with_timeout(coro_or_func, timeout: float = 30.0, *args, **kwargs) -> Any:
    """íƒ€ì„ì•„ì›ƒì„ ì ìš©í•œ ì•ˆì „í•œ ì‹¤í–‰"""
    try:
        if asyncio.iscoroutinefunction(coro_or_func):
            return await asyncio.wait_for(coro_or_func(*args, **kwargs), timeout=timeout)
        elif callable(coro_or_func):
            loop = asyncio.get_event_loop()
            return await asyncio.wait_for(
                loop.run_in_executor(None, lambda: coro_or_func(*args, **kwargs)), 
                timeout=timeout
            )
        elif asyncio.iscoroutine(coro_or_func):
            return await asyncio.wait_for(coro_or_func, timeout=timeout)
        else:
            return coro_or_func
    except asyncio.TimeoutError:
        logging.warning(f"âš ï¸ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ): {coro_or_func}")
        return None
    except Exception as e:
        logging.error(f"âŒ run_with_timeout ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ 17. ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'BaseStepMixin',
    'SafeConfig',
    'CheckpointManager',
    'CheckpointInfo',
    'WarmupSystem',
    'PerformanceMonitor',
    'StepMemoryOptimizer',
    'DIHelper',
    
    # Stepë³„ íŠ¹í™” Mixinë“¤ (8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸)
    'HumanParsingMixin',
    'PoseEstimationMixin', 
    'ClothSegmentationMixin',
    'GeometricMatchingMixin',
    'ClothWarpingMixin',
    'VirtualFittingMixin',
    'PostProcessingMixin',
    'QualityAssessmentMixin',
    
    # ë°ì½”ë ˆì´í„°ë“¤ (ë™ê¸°/ë¹„ë™ê¸°)
    'safe_step_method',
    'async_safe_step_method',
    'performance_monitor',
    'async_performance_monitor',
    'memory_optimize_after',
    'async_memory_optimize_after',
    
    # ë¹„ë™ê¸° ìœ í‹¸ë¦¬í‹°ë“¤
    'ensure_coroutine',
    'is_coroutine_function_safe',
    'is_coroutine_safe',
    'run_with_timeout',
    'safe_async_wrapper',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'CONDA_INFO'
]

# ==============================================
# ğŸ”¥ 18. ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

print("=" * 80)
print("âœ… BaseStepMixin v11.0 - ì™„ì „í•œ í†µí•© êµ¬í˜„ ë¡œë“œ ì™„ë£Œ")
print("=" * 80)
print("ğŸ”¥ í•´ê²°ëœ ë¬¸ì œë“¤:")
print("   âœ… SafeConfig í´ë˜ìŠ¤ ì¤‘ë³µ ì½”ë“œ ì™„ì „ ìˆ˜ì •")
print("   âœ… kwargs íŒŒë¼ë¯¸í„° ëˆ„ë½ ë¬¸ì œ í•´ê²°")
print("   âœ… _emergency_initialization ë©”ì„œë“œ ì¤‘ë³µ ì •ì˜ í•´ê²°")
print("   âœ… ëª¨ë“  í•µì‹¬ ë©”ì„œë“œ ì™„ì „ êµ¬í˜„")
print("   âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²° (coroutine ê²½ê³  ì™„ì „ ì œê±°)")
print("   âœ… from functools import wraps ì¶”ê°€ (NameError í•´ê²°)")
print("   âœ… logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ê·¼ë³¸ í•´ê²°")
print("")
print("ğŸš€ í•µì‹¬ ê¸°ëŠ¥ë“¤:")
print("   âœ… conda í™˜ê²½ ìš°ì„  ì§€ì› ë° ìë™ ê°ì§€")
print("   âœ… M3 Max 128GB í†µí•© ë©”ëª¨ë¦¬ ì™„ì „ ìµœì í™”")
print("   âœ… 89.8GB ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€ ë° í™œìš©")
print("   âœ… ModelLoader ì—°ë™ ì™„ì „ ìë™í™”")
print("   âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì™„ì „ ì ìš©")
print("   âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ")
print("   âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ")
print("   âœ… ì›Œë°ì—… ì‹œìŠ¤í…œ (ë™ê¸°/ë¹„ë™ê¸° ì™„ì „ ì§€ì›)")
print("   âœ… ì—ëŸ¬ ë³µêµ¬ ì‹œìŠ¤í…œ")
print("   âœ… ìˆœí™˜ ì„í¬íŠ¸ ì™„ì „ í•´ê²°")
print("")
print("ğŸ¯ 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ Stepë³„ Mixin:")
print("   1ï¸âƒ£ HumanParsingMixin - ì‹ ì²´ ì˜ì—­ ë¶„í• ")
print("   2ï¸âƒ£ PoseEstimationMixin - í¬ì¦ˆ ê°ì§€")
print("   3ï¸âƒ£ ClothSegmentationMixin - ì˜ë¥˜ ë¶„í• ")
print("   4ï¸âƒ£ GeometricMatchingMixin - ê¸°í•˜í•™ì  ë§¤ì¹­")
print("   5ï¸âƒ£ ClothWarpingMixin - ì˜ë¥˜ ë³€í˜•")
print("   6ï¸âƒ£ VirtualFittingMixin - ê°€ìƒ í”¼íŒ… (í•µì‹¬)")
print("   7ï¸âƒ£ PostProcessingMixin - í›„ì²˜ë¦¬")
print("   8ï¸âƒ£ QualityAssessmentMixin - í’ˆì§ˆ í‰ê°€")
print("")
print(f"ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ:")
print(f"   - conda í™˜ê²½: {CONDA_INFO['conda_env']}")
print(f"   - PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
print(f"   - MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
print(f"   - NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'}")
print(f"   - PIL: {'âœ…' if PIL_AVAILABLE else 'âŒ'}")
print("")
print("ğŸŒŸ ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œ:")
print("   - ì‹¤ì œ Step íŒŒì¼ë“¤ê³¼ ì™„ë²½ ì—°ë™")
print("   - ëª¨ë“  í•µì‹¬ ë©”ì„œë“œ êµ¬í˜„ ì™„ë£Œ")
print("   - í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
print("=" * 80)