# app/ai_pipeline/steps/base_step_mixin.py
"""
ğŸ”¥ MyCloset AI - BaseStepMixin v3.0 - ì™„ì „ í†µí•© ë²„ì „
=====================================

âœ… ë‘ íŒŒì¼ì˜ ì¥ì ì„ ëª¨ë‘ í†µí•©
âœ… object.__init__() íŒŒë¼ë¯¸í„° ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… device ì†ì„± ì˜¤ë¥˜ ì™„ì „ í•´ê²°
âœ… ModelLoader ì¸í„°í˜ì´ìŠ¤ ì™„ë²½ ì—°ë™
âœ… M3 Max 128GB ìµœì í™” ì§€ì›
âœ… ë‹¤ì¤‘ ìƒì† ì•ˆì „í•œ ì²˜ë¦¬
âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›

Author: MyCloset AI Team
Date: 2025-07-18
Version: 3.0 (í†µí•© ì™„ì„± ë²„ì „)
"""

import os
import gc
import time
import asyncio
import logging
import threading
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor

# ì•ˆì „í•œ PyTorch import
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
    
    # M3 Max MPS ì§€ì› í™•ì¸
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        DEFAULT_DEVICE = "mps"
    else:
        MPS_AVAILABLE = False
        DEFAULT_DEVICE = "cpu"
        
except ImportError as e:
    TORCH_AVAILABLE = False
    MPS_AVAILABLE = False
    DEFAULT_DEVICE = "cpu"
    torch = None

# ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì „í•œ import
try:
    import cv2
    import numpy as np
    from PIL import Image
    CV_AVAILABLE = True
    NUMPY_AVAILABLE = True
    PIL_AVAILABLE = True
except ImportError:
    CV_AVAILABLE = False
    NUMPY_AVAILABLE = False
    PIL_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì™„ì „ í†µí•©ëœ BaseStepMixin v3.0
# ==============================================

class BaseStepMixin:
    """
    ğŸ”¥ ì™„ì „ í†µí•©ëœ BaseStepMixin v3.0
    
    ëª¨ë“  Step í´ë˜ìŠ¤ê°€ ìƒì†ë°›ëŠ” ê¸°ë³¸ Mixin í´ë˜ìŠ¤
    âœ… ë‘ íŒŒì¼ì˜ ì¥ì ì„ ëª¨ë‘ í†µí•©
    âœ… ëª¨ë“  ì´ˆê¸°í™” ë¬¸ì œ ì™„ì „ í•´ê²°
    âœ… conda í™˜ê²½ ì™„ë²½ ì§€ì›
    âœ… M3 Max 128GB ìµœì í™”
    """
    
    def __init__(self, *args, **kwargs):
        """
        ğŸ”¥ ì™„ì „ ì•ˆì „í•œ ì´ˆê¸°í™” - ëª¨ë“  ì˜¤ë¥˜ í•´ê²°
        
        ë‹¤ì¤‘ ìƒì† ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ë©°, object.__init__() íŒŒë¼ë¯¸í„° ë¬¸ì œ í•´ê²°
        """
        
        # ğŸ”¥ Step 1: ë‹¤ì¤‘ ìƒì† ì•ˆì „í•œ super() í˜¸ì¶œ
        try:
            # kwargsì—ì„œ BaseStepMixinì´ ëª¨ë¥´ëŠ” íŒŒë¼ë¯¸í„°ë“¤ í•„í„°ë§
            base_kwargs = {}
            
            # ì•Œë ¤ì§„ íŒŒë¼ë¯¸í„°ë“¤ ì •ì˜
            known_params = {
                'device', 'quality_level', 'device_type', 'memory_gb', 
                'is_m3_max', 'optimization_enabled', 'batch_size',
                'config', 'cache_dir', 'preferred_device'
            }
            
            # ë‹¤ë¥¸ í´ë˜ìŠ¤ë“¤ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ë“¤ì€ ìœ ì§€
            for key, value in kwargs.items():
                if key not in known_params:
                    base_kwargs[key] = value
            
            # ì•ˆì „í•œ super() í˜¸ì¶œ - íŒŒë¼ë¯¸í„° ì—†ì´ í˜¸ì¶œ
            super().__init__()
            
        except TypeError:
            # super().__init__()ì´ íŒŒë¼ë¯¸í„°ë¥¼ ë°›ì§€ ì•ŠëŠ” ê²½ìš° (object í´ë˜ìŠ¤)
            # ì´ ê²½ìš°ëŠ” ì •ìƒì´ë¯€ë¡œ ë¬´ì‹œ
            pass
        
        # ğŸ”¥ Step 2: ê¸°ë³¸ ì†ì„±ë“¤ ë¨¼ì € ì„¤ì • (logger ì†ì„± ëˆ„ë½ ë¬¸ì œ í•´ê²°)
        self.step_name = getattr(self, 'step_name', self.__class__.__name__)
        
        # logger ì†ì„± ë°˜ë“œì‹œ ë¨¼ì € ì„¤ì •
        if not hasattr(self, 'logger'):
            self.logger = logging.getLogger(f"pipeline.{self.step_name}")
            self.logger.info(f"ğŸ”§ {self.step_name} logger ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ğŸ”¥ Step 3: device ì†ì„± ì•ˆì „í•˜ê²Œ ì„¤ì • (device ì†ì„± ì˜¤ë¥˜ í•´ê²°)
        self.device = self._safe_device_setup(kwargs)
        self.device_type = kwargs.get('device_type', self._detect_device_type())
        
        # ğŸ”¥ Step 4: ì‹œìŠ¤í…œ ì •ë³´ ì„¤ì •
        self.memory_gb = kwargs.get('memory_gb', self._detect_memory())
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # ğŸ”¥ Step 5: í’ˆì§ˆ ì„¤ì •
        self.quality_level = kwargs.get('quality_level', 'balanced')
        self.batch_size = kwargs.get('batch_size', self._calculate_optimal_batch_size())
        
        # ğŸ”¥ Step 6: ì„¤ì • ì²˜ë¦¬ (ë”•ì…”ë„ˆë¦¬/ê°ì²´ ëª¨ë‘ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        self.config = self._create_safe_config(kwargs.get('config', {}))
        
        # ğŸ”¥ Step 7: ìƒíƒœ ê´€ë¦¬ ì´ˆê¸°í™”
        self.is_initialized = False
        self.model_interface = None
        self.model_loader = None
        self.performance_metrics = {}
        self.error_count = 0
        self.last_error = None
        self.last_processing_time = 0.0
        self.total_processing_count = 0
        
        # ğŸ”¥ Step 8: ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.cache_dir = Path(kwargs.get('cache_dir', './cache'))
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ğŸ”¥ Step 9: M3 Max ìµœì í™” ì„¤ì •
        self._setup_m3_max_optimization()
        
        # ğŸ”¥ Step 10: ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • (ì§€ì—° ë¡œë”©)
        self._setup_model_interface_safe()
        
        # ğŸ”¥ Step 11: PyTorch ìµœì í™” ì„¤ì •
        self._setup_pytorch_optimization()
        
        # ğŸ”¥ ì´ˆê¸°í™” ì™„ë£Œ ë¡œê¹…
        self.logger.info(f"âœ… {self.step_name} BaseStepMixin v3.0 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ”§ Device: {self.device} ({self.device_type})")
        self.logger.info(f"ğŸ“Š Memory: {self.memory_gb}GB, M3 Max: {'âœ…' if self.is_m3_max else 'âŒ'}")
        self.logger.info(f"âš™ï¸ Quality: {self.quality_level}, Batch: {self.batch_size}")
    
    def _safe_device_setup(self, kwargs: Dict[str, Any]) -> str:
        """ğŸ”§ ì•ˆì „í•œ ë””ë°”ì´ìŠ¤ ì„¤ì • - ëª¨ë“  Step í´ë˜ìŠ¤ì™€ í˜¸í™˜"""
        try:
            # kwargsì—ì„œ device íŒŒë¼ë¯¸í„° í™•ì¸
            device_from_kwargs = kwargs.get('device') or kwargs.get('preferred_device')
            
            if device_from_kwargs and device_from_kwargs != "auto":
                return device_from_kwargs
            
            # ìë™ íƒì§€
            return self._auto_detect_device()
            
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return DEFAULT_DEVICE
    
    def _auto_detect_device(self) -> str:
        """ğŸ” ë””ë°”ì´ìŠ¤ ìë™ íƒì§€ - M3 Max ìµœì í™”"""
        try:
            if not TORCH_AVAILABLE:
                return "cpu"
            
            # M3 Max MPS ì§€ì› í™•ì¸ (ìµœìš°ì„ )
            if MPS_AVAILABLE:
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
                
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ íƒì§€ ì‹¤íŒ¨: {e}")
            return "cpu"
    
    def _detect_device_type(self) -> str:
        """ë””ë°”ì´ìŠ¤ íƒ€ì… íƒì§€"""
        try:
            import platform
            system = platform.system()
            machine = platform.machine()
            
            if system == "Darwin" and ("arm64" in machine or "M3" in str(platform.processor())):
                return "apple_silicon"
            elif self.device == "cuda":
                return "nvidia_gpu"
            else:
                return "generic_cpu"
        except:
            return "unknown"
    
    def _detect_memory(self) -> float:
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ íƒì§€"""
        try:
            import psutil
            return round(psutil.virtual_memory().total / (1024**3), 1)
        except:
            return 16.0  # ê¸°ë³¸ê°’
    
    def _detect_m3_max(self) -> bool:
        """M3 Max íƒì§€"""
        try:
            import platform
            processor = str(platform.processor())
            return "M3" in processor or (self.device == "mps" and self.memory_gb > 64)
        except:
            return False
    
    def _calculate_optimal_batch_size(self) -> int:
        """ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        try:
            if self.is_m3_max and self.memory_gb >= 128:
                return 8  # M3 Max 128GB
            elif self.memory_gb >= 64:
                return 4  # 64GB+
            elif self.memory_gb >= 32:
                return 2  # 32GB+
            else:
                return 1  # 16GB ì´í•˜
        except:
            return 1
    
    def _create_safe_config(self, config_data: Any) -> 'SafeConfig':
        """ì•ˆì „í•œ ì„¤ì • ê°ì²´ ìƒì„±"""
        
        class SafeConfig:
            """ì•ˆì „í•œ ì„¤ì • í´ë˜ìŠ¤ - ë”•ì…”ë„ˆë¦¬ì™€ ê°ì²´ ëª¨ë‘ ì§€ì›"""
            
            def __init__(self, data: Any):
                if hasattr(data, '__dict__'):
                    # ì„¤ì • ê°ì²´ì¸ ê²½ìš°
                    self._data = data.__dict__.copy()
                    for key, value in self._data.items():
                        setattr(self, key, value)
                elif isinstance(data, dict):
                    # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
                    self._data = data.copy()
                    for key, value in self._data.items():
                        setattr(self, key, value)
                else:
                    # ê¸°íƒ€ ê²½ìš°
                    self._data = {}
            
            def get(self, key: str, default=None):
                """ë”•ì…”ë„ˆë¦¬ì²˜ëŸ¼ get ë©”ì„œë“œ ì§€ì›"""
                return self._data.get(key, default)
            
            def __getitem__(self, key):
                return self._data[key]
            
            def __setitem__(self, key, value):
                self._data[key] = value
                setattr(self, key, value)
            
            def __contains__(self, key):
                return key in self._data
            
            def keys(self):
                return self._data.keys()
            
            def values(self):
                return self._data.values()
            
            def items(self):
                return self._data.items()
            
            def update(self, other):
                if isinstance(other, dict):
                    self._data.update(other)
                    for key, value in other.items():
                        setattr(self, key, value)
        
        return SafeConfig(config_data)
    
    def _setup_m3_max_optimization(self):
        """ğŸ M3 Max ìµœì í™” ì„¤ì •"""
        try:
            if self.device == "mps" and TORCH_AVAILABLE:
                # M3 Max íŠ¹í™” í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                # OMP ìŠ¤ë ˆë“œ ìˆ˜ ì„¤ì • (M3 Max 16ì½”ì–´ í™œìš©)
                if self.is_m3_max:
                    os.environ['OMP_NUM_THREADS'] = '16'
                
                # MPS ìºì‹œ ì •ë¦¬
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                
                self.logger.info("ğŸ M3 Max MPS ìµœì í™” ì„¤ì • ì™„ë£Œ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _setup_pytorch_optimization(self):
        """PyTorch ìµœì í™” ì„¤ì •"""
        try:
            if not TORCH_AVAILABLE:
                return
            
            # ë°ì´í„° íƒ€ì… ì„¤ì •
            if self.device == "mps" and self.optimization_enabled:
                self.dtype = torch.float16  # MPSì—ì„œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
            else:
                self.dtype = torch.float32
            
            # autograd ìµœì í™”
            torch.backends.cudnn.benchmark = True if self.device == "cuda" else False
            
            self.logger.debug(f"ğŸ”§ PyTorch ìµœì í™” ì„¤ì •: dtype={self.dtype}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ PyTorch ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
            self.dtype = torch.float32 if TORCH_AVAILABLE else None
    
    def _setup_model_interface_safe(self):
        """ğŸ”— ModelLoader ì¸í„°í˜ì´ìŠ¤ ì•ˆì „í•œ ì„¤ì •"""
        try:
            # ìˆœí™˜ import ë°©ì§€ë¥¼ ìœ„í•œ ëŠ¦ì€ import
            from ..utils.model_loader import get_global_model_loader
            
            model_loader = get_global_model_loader()
            if model_loader:
                self.model_interface = model_loader.create_step_interface(self.step_name)
                self.model_loader = model_loader
                self.logger.info(f"ğŸ”— {self.step_name} ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ê²° ì™„ë£Œ")
            else:
                self.logger.warning(f"âš ï¸ {self.step_name} ì „ì—­ ModelLoaderë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                self.model_interface = None
                self.model_loader = None
                
        except ImportError as e:
            self.logger.warning(f"âš ï¸ ModelLoader ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
            self.model_interface = None
            self.model_loader = None
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_interface = None
            self.model_loader = None
    
    async def initialize_step(self) -> bool:
        """ğŸš€ Step ì™„ì „ ì´ˆê¸°í™”"""
        try:
            self.logger.info(f"ğŸš€ {self.step_name} ì´ˆê¸°í™” ì‹œì‘...")
            
            # ê¸°ë³¸ ì´ˆê¸°í™” í™•ì¸
            if not hasattr(self, 'logger'):
                self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            
            if not hasattr(self, 'device'):
                self.device = self._auto_detect_device()
            
            # ModelLoader ì¸í„°í˜ì´ìŠ¤ ì¬ì„¤ì • (í•„ìš”ì‹œ)
            if not hasattr(self, 'model_interface') or self.model_interface is None:
                self._setup_model_interface_safe()
            
            # ì»¤ìŠ¤í…€ ì´ˆê¸°í™” í˜¸ì¶œ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
            if hasattr(self, '_custom_initialize'):
                await self._custom_initialize()
            
            self.is_initialized = True
            self.logger.info(f"âœ… {self.step_name} ì™„ì „ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            self.last_error = str(e)
            self.error_count += 1
            return False
    
    async def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ğŸ¤– ëª¨ë¸ ë¡œë“œ (ì•ˆì „í•œ í´ë°± í¬í•¨)"""
        try:
            # ì¸í„°í˜ì´ìŠ¤ ì„¤ì • í™•ì¸
            if self.model_interface is None:
                await self.setup_model_interface()
            
            if self.model_interface is not None:
                if model_name:
                    return await self.model_interface.get_model(model_name)
                else:
                    # ê¶Œì¥ ëª¨ë¸ ìë™ ë¡œë“œ
                    return await self.model_interface.get_recommended_model()
            else:
                # í´ë°±: ë”ë¯¸ ëª¨ë¸ ë°˜í™˜
                self.logger.warning(f"âš ï¸ ModelLoader ì—†ìŒ, ë”ë¯¸ ëª¨ë¸ ì‚¬ìš©")
                return self._create_dummy_model(model_name or "default")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}, ë”ë¯¸ ëª¨ë¸ ì‚¬ìš©")
            self.last_error = str(e)
            self.error_count += 1
            return self._create_dummy_model(model_name or "default")
    
    async def setup_model_interface(self) -> bool:
        """ğŸ”— ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • (ì§€ì—° ë¡œë”©)"""
        try:
            if self.model_interface is not None:
                return True
            
            # ModelLoader ê°€ì ¸ì˜¤ê¸°
            try:
                from ..utils.model_loader import get_global_model_loader
                self.model_loader = get_global_model_loader()
                
                if self.model_loader:
                    self.model_interface = self.model_loader.create_step_interface(self.step_name)
                    self.logger.info(f"ğŸ”— {self.step_name} ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
                    return True
                else:
                    self.logger.warning(f"âš ï¸ ì „ì—­ ModelLoaderë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return False
                
            except ImportError as e:
                self.logger.warning(f"âš ï¸ ModelLoader ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def _create_dummy_model(self, model_name: str) -> 'DummyModel':
        """ë”ë¯¸ ëª¨ë¸ ìƒì„±"""
        
        class DummyModel:
            """ë”ë¯¸ AI ëª¨ë¸ - í…ŒìŠ¤íŠ¸ ë° í´ë°±ìš©"""
            
            def __init__(self, name: str, device: str, step_name: str):
                self.name = name
                self.device = device
                self.step_name = step_name
                self.is_dummy = True
                self.is_loaded = True
            
            def __call__(self, *args, **kwargs):
                return self.forward(*args, **kwargs)
            
            def forward(self, *args, **kwargs):
                """ë”ë¯¸ ì²˜ë¦¬ - ì…ë ¥ í¬ê¸° ìœ ì§€í•˜ë©° ì˜ë¯¸ìˆëŠ” ì¶œë ¥ ìƒì„±"""
                if TORCH_AVAILABLE and args and isinstance(args[0], torch.Tensor):
                    input_tensor = args[0]
                    # ì…ë ¥ê³¼ ê°™ì€ í¬ê¸°ì˜ ë”ë¯¸ ì¶œë ¥ (ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€)
                    output = torch.randn_like(input_tensor) * 0.1
                    if input_tensor.dtype == torch.uint8:
                        output = (output * 255).clamp(0, 255).to(torch.uint8)
                    else:
                        output = output.clamp(0, 1)
                    return output
                elif NUMPY_AVAILABLE:
                    # NumPy ë°°ì—´ ì¶œë ¥
                    return np.random.randn(1, 3, 512, 512).astype(np.float32)
                else:
                    return None
            
            def to(self, device):
                self.device = str(device)
                return self
            
            def eval(self):
                return self
            
            def cpu(self):
                return self.to('cpu')
            
            def cuda(self):
                return self.to('cuda')
            
            def parameters(self):
                return []
        
        return DummyModel(model_name, self.device, self.step_name)
    
    def preprocess_input(self, input_data: Any, target_size: tuple = (512, 512)) -> Any:
        """ğŸ”„ ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬ - ê°•í™”ëœ ë²„ì „"""
        try:
            if input_data is None:
                return None
            
            # PIL ì´ë¯¸ì§€ ì²˜ë¦¬
            if PIL_AVAILABLE and isinstance(input_data, Image.Image):
                image = input_data.convert('RGB')
                original_size = image.size
                
                if image.size != target_size:
                    image = image.resize(target_size, Image.Resampling.LANCZOS)
                
                # í…ì„œ ë³€í™˜
                if TORCH_AVAILABLE and NUMPY_AVAILABLE:
                    img_array = np.array(image).astype(np.float32) / 255.0
                    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
                    
                    # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    if self.device != "cpu":
                        img_tensor = img_tensor.to(self.device)
                    
                    return img_tensor
                else:
                    return image
            
            # PyTorch í…ì„œ ì²˜ë¦¬
            elif TORCH_AVAILABLE and isinstance(input_data, torch.Tensor):
                # ë””ë°”ì´ìŠ¤ ì´ë™
                if input_data.device.type != self.device:
                    input_data = input_data.to(self.device)
                
                # ë°ì´í„° íƒ€ì… ìµœì í™”
                if hasattr(self, 'dtype') and input_data.dtype != self.dtype:
                    if self.dtype in [torch.float16, torch.float32]:
                        input_data = input_data.to(self.dtype)
                
                return input_data
            
            # NumPy ë°°ì—´ ì²˜ë¦¬
            elif NUMPY_AVAILABLE and isinstance(input_data, np.ndarray):
                if TORCH_AVAILABLE:
                    tensor = torch.from_numpy(input_data).to(self.device)
                    return tensor
                else:
                    return input_data
            
            # ê¸°íƒ€ ë°ì´í„°
            else:
                return input_data
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return input_data
    
    def postprocess_output(self, output_data: Any, original_size: Optional[tuple] = None) -> Any:
        """ğŸ”„ ì¶œë ¥ ë°ì´í„° í›„ì²˜ë¦¬ - ê°•í™”ëœ ë²„ì „"""
        try:
            if output_data is None:
                return None
            
            # PyTorch í…ì„œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            if TORCH_AVAILABLE and isinstance(output_data, torch.Tensor):
                # ë°°ì¹˜ ì°¨ì› ì œê±°
                if output_data.dim() == 4 and output_data.shape[0] == 1:
                    output_data = output_data.squeeze(0)
                
                # ì±„ë„ ìˆœì„œ ì¡°ì • (CHW -> HWC)
                if output_data.dim() == 3 and output_data.shape[0] <= 4:
                    output_data = output_data.permute(1, 2, 0)
                
                # CPUë¡œ ì´ë™ ë° ì •ê·œí™”
                output_data = output_data.cpu().detach()
                
                # ê°’ ë²”ìœ„ ì •ê·œí™”
                if output_data.dtype in [torch.float16, torch.float32]:
                    output_data = torch.clamp(output_data, 0, 1)
                
                # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
                if PIL_AVAILABLE and NUMPY_AVAILABLE:
                    if output_data.dtype in [torch.float16, torch.float32]:
                        array = (output_data.numpy() * 255).astype(np.uint8)
                    else:
                        array = output_data.numpy().astype(np.uint8)
                    
                    # ë‹¨ì¼ ì±„ë„ì¸ ê²½ìš° RGBë¡œ ë³€í™˜
                    if array.ndim == 2:
                        array = np.stack([array] * 3, axis=-1)
                    elif array.shape[-1] == 1:
                        array = np.repeat(array, 3, axis=-1)
                    
                    image = Image.fromarray(array)
                    
                    # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                    if original_size and image.size != original_size:
                        image = image.resize(original_size, Image.Resampling.LANCZOS)
                    
                    return image
                else:
                    return output_data
            
            # ê¸°íƒ€ ë°ì´í„°
            else:
                return output_data
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¶œë ¥ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return output_data
    
    async def safe_process(self, *args, **kwargs) -> Dict[str, Any]:
        """ğŸ›¡ï¸ ì•ˆì „í•œ ì²˜ë¦¬ ë˜í¼ - ê°•í™”ëœ ë²„ì „"""
        start_time = time.time()
        
        try:
            # ì´ˆê¸°í™” í™•ì¸
            if not getattr(self, 'is_initialized', False):
                await self.initialize_step()
            
            # ì‹¤ì œ ì²˜ë¦¬ ë©”ì„œë“œ í˜¸ì¶œ
            if hasattr(self, 'process'):
                result = await self.process(*args, **kwargs)
            elif hasattr(self, '_process'):
                result = await self._process(*args, **kwargs)
            else:
                result = await self._default_process(*args, **kwargs)
            
            processing_time = time.time() - start_time
            self.last_processing_time = processing_time
            self.total_processing_count += 1
            
            # ê²°ê³¼ í‘œì¤€í™”
            if not isinstance(result, dict):
                result = {
                    'success': True,
                    'result': result,
                    'confidence': 0.8,
                    'quality_score': 0.8,
                    'processing_time': processing_time,
                    'method': 'default'
                }
            else:
                result.setdefault('success', True)
                result.setdefault('confidence', 0.8)
                result.setdefault('quality_score', result.get('confidence', 0.8))
                result.setdefault('processing_time', processing_time)
                result.setdefault('method', 'processed')
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
            self.record_performance("process", processing_time, True)
            
            self.logger.debug(f"âœ… {self.step_name} ì²˜ë¦¬ ì™„ë£Œ: {processing_time:.3f}ì´ˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            
            self.error_count += 1
            processing_time = time.time() - start_time
            self.last_error = str(e)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡ (ì‹¤íŒ¨)
            self.record_performance("process", processing_time, False)
            
            return {
                'success': False,
                'error': str(e),
                'confidence': 0.0,
                'quality_score': 0.0,
                'processing_time': processing_time,
                'method': 'error',
                'step_name': self.step_name
            }
    
    async def _default_process(self, *args, **kwargs) -> Dict[str, Any]:
        """ê¸°ë³¸ ì²˜ë¦¬ ë©”ì„œë“œ (í•˜ìœ„í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ)"""
        self.logger.info(f"ğŸ”„ {self.step_name} ê¸°ë³¸ ì²˜ë¦¬ ì‹¤í–‰")
        
        return {
            'success': True,
            'result': args[0] if args else None,
            'confidence': 0.8,
            'quality_score': 0.8,
            'processing_time': 0.1,
            'method': 'default',
            'message': f'{self.step_name} ê¸°ë³¸ ì²˜ë¦¬ ì™„ë£Œ',
            'step_name': self.step_name
        }
    
    def record_performance(self, operation: str, duration: float, success: bool = True):
        """ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        if not hasattr(self, 'performance_metrics'):
            self.performance_metrics = {}
            
        if operation not in self.performance_metrics:
            self.performance_metrics[operation] = {
                "total_calls": 0,
                "success_calls": 0,
                "total_duration": 0.0,
                "avg_duration": 0.0,
                "last_duration": 0.0,
                "min_duration": float('inf'),
                "max_duration": 0.0
            }
        
        metrics = self.performance_metrics[operation]
        metrics["total_calls"] += 1
        metrics["total_duration"] += duration
        metrics["last_duration"] = duration
        metrics["avg_duration"] = metrics["total_duration"] / metrics["total_calls"]
        
        # ìµœì†Œ/ìµœëŒ€ ì‹œê°„ ì—…ë°ì´íŠ¸
        metrics["min_duration"] = min(metrics["min_duration"], duration)
        metrics["max_duration"] = max(metrics["max_duration"], duration)
        
        if success:
            metrics["success_calls"] += 1
    
    def get_step_info(self) -> Dict[str, Any]:
        """ğŸ“‹ Step ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'step_number': getattr(self, 'step_number', 0),
            'step_type': getattr(self, 'step_type', 'unknown'),
            'device': self.device,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'optimization_enabled': self.optimization_enabled,
            'quality_level': self.quality_level,
            'batch_size': self.batch_size,
            'is_initialized': self.is_initialized,
            'has_model_interface': self.model_interface is not None,
            'has_model_loader': self.model_loader is not None,
            'last_processing_time': self.last_processing_time,
            'total_processing_count': self.total_processing_count,
            'error_count': self.error_count,
            'last_error': self.last_error,
            'cache_dir': str(self.cache_dir),
            'config_keys': list(self.config.keys()) if hasattr(self.config, 'keys') else [],
            'performance_metrics': self.performance_metrics,
            'torch_available': TORCH_AVAILABLE,
            'mps_available': MPS_AVAILABLE,
            'dtype': str(getattr(self, 'dtype', 'None'))
        }
    
    def cleanup_models(self):
        """ğŸ§¹ ëª¨ë¸ ì •ë¦¬"""
        try:
            if hasattr(self, 'model_interface') and self.model_interface:
                self.model_interface.unload_models()
                self.logger.info(f"ğŸ§¹ {self.step_name} ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "mps" and hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
                
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                gc.collect()
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ {self.step_name} ëª¨ë¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.cleanup_models()
        except:
            pass

# ==============================================
# ğŸ”¥ Stepë³„ íŠ¹í™” Mixinë“¤ - ì™„ì „í•œ 8ë‹¨ê³„
# ==============================================

class HumanParsingMixin(BaseStepMixin):
    """Step 1: Human Parsing íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 1
        self.step_type = "human_parsing"
        self.num_classes = 20
        self.output_format = "segmentation_mask"

class PoseEstimationMixin(BaseStepMixin):
    """Step 2: Pose Estimation íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 2
        self.step_type = "pose_estimation"
        self.num_keypoints = 18
        self.output_format = "keypoints_heatmap"

class ClothSegmentationMixin(BaseStepMixin):
    """Step 3: Cloth Segmentation íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 3
        self.step_type = "cloth_segmentation"
        self.binary_output = True
        self.output_format = "binary_mask"

class GeometricMatchingMixin(BaseStepMixin):
    """Step 4: Geometric Matching íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 4
        self.step_type = "geometric_matching"
        self.num_control_points = 25
        self.output_format = "transformation_matrix"

class ClothWarpingMixin(BaseStepMixin):
    """Step 5: Cloth Warping íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 5
        self.step_type = "cloth_warping"
        self.enable_physics = True
        self.output_format = "warped_cloth"

class VirtualFittingMixin(BaseStepMixin):
    """Step 6: Virtual Fitting íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 6
        self.step_type = "virtual_fitting"
        self.diffusion_steps = 50
        self.output_format = "rgb_image"

class PostProcessingMixin(BaseStepMixin):
    """Step 7: Post Processing íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 7
        self.step_type = "post_processing"
        self.upscale_factor = 2
        self.output_format = "enhanced_image"

class QualityAssessmentMixin(BaseStepMixin):
    """Step 8: Quality Assessment íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 8
        self.step_type = "quality_assessment"
        self.quality_threshold = 0.7
        self.output_format = "quality_scores"

# ==============================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° ë°ì½”ë ˆì´í„° ë° í—¬í¼ í•¨ìˆ˜ë“¤
# ==============================================

def ensure_step_initialization(func: Callable) -> Callable:
    """Step í´ë˜ìŠ¤ ì´ˆê¸°í™” ë³´ì¥ ë°ì½”ë ˆì´í„°"""
    async def wrapper(self, *args, **kwargs):
        # logger ì†ì„± í™•ì¸ ë° ì„¤ì •
        if not hasattr(self, 'logger'):
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        # BaseStepMixin ì´ˆê¸°í™” í™•ì¸
        if not hasattr(self, 'is_initialized') or not self.is_initialized:
            await self.initialize_step()
        
        return await func(self, *args, **kwargs)
    return wrapper

def safe_step_method(func: Callable) -> Callable:
    """Step ë©”ì„œë“œ ì•ˆì „ ì‹¤í–‰ ë°ì½”ë ˆì´í„°"""
    async def wrapper(self, *args, **kwargs):
        try:
            # logger í™•ì¸
            if not hasattr(self, 'logger'):
                self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            
            return await func(self, *args, **kwargs)
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.error(f"âŒ {func.__name__} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                if hasattr(self, 'error_count'):
                    self.error_count += 1
                if hasattr(self, 'last_error'):
                    self.last_error = str(e)
            
            # ê¸°ë³¸ ì—ëŸ¬ ì‘ë‹µ ë°˜í™˜
            return {
                'success': False,
                'error': str(e),
                'step_name': getattr(self, 'step_name', self.__class__.__name__),
                'method_name': func.__name__
            }
    return wrapper

def performance_monitor(operation_name: str) -> Callable:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable) -> Callable:
        async def wrapper(self, *args, **kwargs):
            start_time = time.time()
            success = True
            try:
                result = await func(self, *args, **kwargs)
                return result
            except Exception as e:
                success = False
                raise e
            finally:
                duration = time.time() - start_time
                if hasattr(self, 'record_performance'):
                    self.record_performance(operation_name, duration, success)
        return wrapper
    return decorator

def memory_optimize(func: Callable) -> Callable:
    """ë©”ëª¨ë¦¬ ìµœì í™” ë°ì½”ë ˆì´í„°"""
    async def wrapper(self, *args, **kwargs):
        try:
            result = await func(self, *args, **kwargs)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if hasattr(self, 'device'):
                    if self.device == "mps" and hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                    elif self.device == "cuda":
                        torch.cuda.empty_cache()
                
                gc.collect()
            
            return result
        except Exception as e:
            # ì˜¤ë¥˜ ì‹œì—ë„ ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                gc.collect()
            raise e
    return wrapper

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    # ê¸°ë³¸ Mixin
    'BaseStepMixin',
    
    # Stepë³„ íŠ¹í™” Mixinë“¤
    'HumanParsingMixin',
    'PoseEstimationMixin', 
    'ClothSegmentationMixin',
    'GeometricMatchingMixin',
    'ClothWarpingMixin',
    'VirtualFittingMixin',
    'PostProcessingMixin',
    'QualityAssessmentMixin',
    
    # ìœ í‹¸ë¦¬í‹° ë°ì½”ë ˆì´í„°
    'ensure_step_initialization',
    'safe_step_method',
    'performance_monitor',
    'memory_optimize',
    
    # ìƒìˆ˜
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'CV_AVAILABLE',
    'DEFAULT_DEVICE'
]

# ë¡œê±° ì„¤ì •
logger.info("âœ… BaseStepMixin v3.0 ì™„ì „ í†µí•© ë²„ì „ ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ”— ModelLoader ì¸í„°í˜ì´ìŠ¤ ì™„ë²½ ì—°ë™")
logger.info("ğŸ M3 Max 128GB ìµœì í™” ì§€ì›")
logger.info("ğŸ conda í™˜ê²½ ì™„ë²½ ì§€ì›")
logger.info(f"ğŸ”§ PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}, MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"ğŸ¯ ê¸°ë³¸ ë””ë°”ì´ìŠ¤: {DEFAULT_DEVICE}")