# app/ai_pipeline/steps/base_step_mixin.py
"""
üî• BaseStepMixin v6.0 - 89.8GB Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ïó∞Îèô ÏôÑÏÑ± + logger ÏÜçÏÑ± ÎàÑÎùΩ ÏôÑÏ†Ñ Ìï¥Í≤∞
================================================================================

‚úÖ Í∏∞Ï°¥ Ìï®Ïàò/ÌÅ¥ÎûòÏä§Î™Ö 100% Ïú†ÏßÄ
‚úÖ logger ÏÜçÏÑ± ÎàÑÎùΩ Î¨∏Ï†ú Í∑ºÎ≥∏ Ìï¥Í≤∞
‚úÖ _setup_model_interface() Î©îÏÑúÎìú ÏôÑÏ†Ñ ÏàòÏ†ï
‚úÖ 89.8GB Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ Î∞è ÌôúÏö©
‚úÖ Dict Callable Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞
‚úÖ ModelLoader Ïó∞Îèô ÏôÑÏ†Ñ ÏûêÎèôÌôî
‚úÖ M3 Max 128GB ÏµúÏ†ÅÌôî
‚úÖ SafeFunctionValidator ÌÜµÌï©
‚úÖ MRO ÏïàÏ†ÑÏÑ± 100% Î≥¥Ïû•
‚úÖ ClothSegmentationStep await Ïò§Î•ò Ìï¥Í≤∞
"""

import os
import gc
import time
import asyncio
import logging
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
from functools import wraps

# ==============================================
# üî• NumPy 2.x Ìò∏ÌôòÏÑ± Î¨∏Ï†ú ÏôÑÏ†Ñ Ìï¥Í≤∞ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)
# ==============================================

try:
    import numpy as np
    numpy_version = np.__version__
    major_version = int(numpy_version.split('.')[0])
    
    if major_version >= 2:
        try:
            np.set_printoptions(legacy='1.25')
        except:
            pass
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

# ÏïàÏ†ÑÌïú PyTorch import (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)
try:
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        DEFAULT_DEVICE = "mps"
    else:
        MPS_AVAILABLE = False
        DEFAULT_DEVICE = "cpu"
        
except ImportError:
    TORCH_AVAILABLE = False
    MPS_AVAILABLE = False
    DEFAULT_DEVICE = "cpu"
    torch = None

# Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ ÎùºÏù¥Î∏åÎü¨Î¶¨ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)
try:
    import cv2
    from PIL import Image
    CV_AVAILABLE = True
    PIL_AVAILABLE = True
except ImportError:
    CV_AVAILABLE = False
    PIL_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# üî• SafeConfig ÌÅ¥ÎûòÏä§ (Í∏∞Ï°¥ ÎÇ¥Ïö© 100% Ïú†ÏßÄ)
# ==============================================

class SafeConfig:
    """üîß ÏôÑÏ†Ñ ÏïàÏ†ÑÌïú ÏÑ§Ï†ï ÌÅ¥ÎûòÏä§ v3.0 (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    
    def __init__(self, data: Any = None):
        """ÏôÑÏ†Ñ ÏïàÏ†ÑÌïú Ï¥àÍ∏∞Ìôî"""
        self._data = {}
        self._original_data = data
        self._lock = threading.RLock()
        
        try:
            with self._lock:
                if data is None:
                    self._data = {}
                elif isinstance(data, dict):
                    self._data = self._safe_dict_copy(data)
                elif hasattr(data, '__dict__'):
                    self._data = self._safe_object_to_dict(data)
                elif callable(data):
                    logger.warning("‚ö†Ô∏è callable ÏÑ§Ï†ï Í∞ùÏ≤¥ Í∞êÏßÄ, Îπà ÏÑ§Ï†ïÏúºÎ°ú Ï≤òÎ¶¨")
                    self._data = {}
                else:
                    if hasattr(data, '__iter__') and not isinstance(data, (str, bytes)):
                        try:
                            self._data = dict(data)
                        except:
                            self._data = {}
                    else:
                        self._data = {}
                
                self._set_attributes_safely()
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è SafeConfig Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}, Îπà ÏÑ§Ï†ï ÏÇ¨Ïö©")
            self._data = {}
    
    def _safe_dict_copy(self, data: dict) -> dict:
        """ÎîïÏÖîÎÑàÎ¶¨ ÏïàÏ†Ñ Î≥µÏÇ¨"""
        safe_dict = {}
        for key, value in data.items():
            try:
                if not callable(value):
                    safe_dict[key] = value
            except:
                pass
        return safe_dict
    
    def _safe_object_to_dict(self, obj: Any) -> dict:
        """Í∞ùÏ≤¥Î•º ÎîïÏÖîÎÑàÎ¶¨Î°ú ÏïàÏ†Ñ Î≥ÄÌôò"""
        safe_dict = {}
        
        if hasattr(obj, '__dict__'):
            for key, value in obj.__dict__.items():
                try:
                    if not key.startswith('_') and not callable(value):
                        safe_dict[key] = value
                except:
                    pass
        
        for attr_name in dir(obj):
            if not attr_name.startswith('_'):
                try:
                    attr_value = getattr(obj, attr_name)
                    if not callable(attr_value):
                        safe_dict[attr_name] = attr_value
                except:
                    pass
        
        return safe_dict
    
    def _set_attributes_safely(self):
        """ÏÜçÏÑ±Îì§ÏùÑ ÏïàÏ†ÑÌïòÍ≤å ÏÑ§Ï†ï"""
        for key, value in self._data.items():
            try:
                if isinstance(key, str) and key.isidentifier() and not hasattr(self, key):
                    setattr(self, key, value)
            except:
                pass
    
    def get(self, key: str, default=None):
        """ÏôÑÏ†Ñ ÏïàÏ†ÑÌïú get Î©îÏÑúÎìú"""
        try:
            with self._lock:
                return self._data.get(key, default)
        except Exception as e:
            logger.debug(f"SafeConfig.get Ïò§Î•ò: {e}")
            return default
    
    def __getitem__(self, key):
        return self.get(key, None)
    
    def __setitem__(self, key, value):
        try:
            with self._lock:
                self._data[key] = value
                if isinstance(key, str) and key.isidentifier():
                    setattr(self, key, value)
        except Exception as e:
            logger.debug(f"SafeConfig.__setitem__ Ïò§Î•ò: {e}")
    
    def __contains__(self, key):
        try:
            return key in self._data
        except:
            return False
    
    def keys(self):
        try:
            return self._data.keys()
        except:
            return []
    
    def values(self):
        try:
            return self._data.values()
        except:
            return []
    
    def items(self):
        try:
            return self._data.items()
        except:
            return []
    
    def update(self, other):
        try:
            with self._lock:
                if isinstance(other, dict):
                    for key, value in other.items():
                        if not callable(value):
                            self._data[key] = value
                            if isinstance(key, str) and key.isidentifier():
                                setattr(self, key, value)
        except Exception as e:
            logger.debug(f"SafeConfig.update Ïò§Î•ò: {e}")
    
    def __str__(self):
        return str(self._data)
    
    def __repr__(self):
        return f"SafeConfig({self._data})"
    
    def __bool__(self):
        return bool(self._data)
    
    def __len__(self):
        return len(self._data)

# ==============================================
# üî• BaseStepMixin v6.0 - ÌïµÏã¨ ÏàòÏ†ïÏÇ¨Ìï≠
# ==============================================

class BaseStepMixin:
    """
    üî• BaseStepMixin v6.0 - logger ÏÜçÏÑ± ÎàÑÎùΩ ÏôÑÏ†Ñ Ìï¥Í≤∞ + 89.8GB Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ïó∞Îèô ÏôÑÏÑ±
    
    ‚úÖ logger ÏÜçÏÑ± ÎàÑÎùΩ Î¨∏Ï†ú Í∑ºÎ≥∏ Ìï¥Í≤∞ (ÏµúÏö∞ÏÑ† Ï≤òÎ¶¨)
    ‚úÖ Í∏∞Ï°¥ Ìï®Ïàò/ÌÅ¥ÎûòÏä§Î™Ö 100% Ïú†ÏßÄ 
    ‚úÖ _setup_model_interface() ÏôÑÏ†Ñ ÏàòÏ†ï
    ‚úÖ 89.8GB Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ
    ‚úÖ SafeFunctionValidator ÌÜµÌï©
    ‚úÖ ModelLoader Ïó∞Îèô ÏôÑÏ†Ñ ÏûêÎèôÌôî
    ‚úÖ ClothSegmentationStep await Ïò§Î•ò Ìï¥Í≤∞
    """
    
    # ÌÅ¥ÎûòÏä§ Î≥ÄÏàò (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)
    _class_registry = weakref.WeakSet()
    _initialization_lock = threading.RLock()
    
    def __init__(self, *args, **kwargs):
        """
        üî• ÏôÑÏ†Ñ ÏïàÏ†ÑÌïú Ï¥àÍ∏∞Ìôî - logger ÏÜçÏÑ± ÎàÑÎùΩ Î¨∏Ï†ú Í∑ºÎ≥∏ Ìï¥Í≤∞
        ‚úÖ logger ÏÜçÏÑ±ÏùÑ Í∞ÄÏû• Î®ºÏ†Ä ÏÉùÏÑ±ÌïòÏó¨ ÎàÑÎùΩ Î∞©ÏßÄ
        ‚úÖ Í∏∞Ï°¥ Ï¥àÍ∏∞Ìôî ÏàúÏÑú Ïú†ÏßÄ
        """
        
        # ===== üî• STEP 0: logger ÏÜçÏÑ± ÏµúÏö∞ÏÑ† ÏÉùÏÑ± (Ï†àÎåÄ ÎàÑÎùΩ Î∞©ÏßÄ) =====
        self._ensure_logger_first()
        
        BaseStepMixin._class_registry.add(self)
        
        with BaseStepMixin._initialization_lock:
            try:
                # Í∏∞Ï°¥ Ï¥àÍ∏∞Ìôî ÏàúÏÑú Ïú†ÏßÄ (loggerÎäî Ïù¥ÎØ∏ ÏÉùÏÑ±Îê®)
                self._check_numpy_compatibility()
                self._setup_basic_attributes(kwargs)
                self._safe_super_init()
                self._setup_device_and_system(kwargs)
                self._setup_config_safely(kwargs)
                self._setup_state_management()
                self._setup_m3_max_optimization()
                self._setup_memory_optimization()
                self._setup_warmup_system()
                self._setup_performance_monitoring()
                
                # üî• ÌïµÏã¨ Ï∂îÍ∞Ä: ModelLoader Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏûêÎèô ÏÑ§Ï†ï
                self._setup_model_interface()
                
                # üî• ÌïµÏã¨ Ï∂îÍ∞Ä: 89.8GB Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ Î∞è Ïó∞Îèô
                self._setup_checkpoint_detection()
                
                self.logger.info(f"‚úÖ {self.step_name} BaseStepMixin v6.0 Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
                self.logger.debug(f"üîß Device: {self.device}, Memory: {self.memory_gb}GB")
                
            except Exception as e:
                self._emergency_initialization()
                if hasattr(self, 'logger'):
                    self.logger.error(f"‚ùå Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
                    self.logger.debug(f"üìã ÏÉÅÏÑ∏ Ïò§Î•ò: {traceback.format_exc()}")
    
    # ==============================================
    # üî• STEP 0: logger ÏÜçÏÑ± ÏµúÏö∞ÏÑ† Î≥¥Ïû• (Ïã†Í∑ú Ï∂îÍ∞Ä)
    # ==============================================
    
    def _ensure_logger_first(self):
        """
        üî• logger ÏÜçÏÑ± ÏµúÏö∞ÏÑ† ÏÉùÏÑ± - Î™®Îì† Step ÌÅ¥ÎûòÏä§ÏóêÏÑú logger ÎàÑÎùΩ Î∞©ÏßÄ
        
        ‚úÖ Í∞ÄÏû• Î®ºÏ†Ä Ïã§ÌñâÎêòÏñ¥ logger ÏÜçÏÑ± Î≥¥Ïû•
        ‚úÖ Step Ïù¥Î¶Ñ Í∏∞Î∞ò Í≥ÑÏ∏µÏ†Å Î°úÍ±∞ ÏÉùÏÑ±
        ‚úÖ Î™®Îì† Ìï∏Îì§Îü¨ Î∞è Ìè¨Îß§ÌÑ∞ ÏÑ§Ï†ï
        ‚úÖ ÏôÑÏ†ÑÌïú ÏóêÎü¨ Î∞©ÏßÄ Ï≤òÎ¶¨
        """
        try:
            # logger ÏÜçÏÑ±Ïù¥ Ïù¥ÎØ∏ ÏûàÎäîÏßÄ ÌôïÏù∏
            if hasattr(self, 'logger') and self.logger is not None:
                return
            
            # Step Ïù¥Î¶Ñ Í≤∞Ï†ï (Ïö∞ÏÑ†ÏàúÏúÑ: step_name > ÌÅ¥ÎûòÏä§Î™Ö)
            class_name = self.__class__.__name__
            step_name = getattr(self, 'step_name', class_name)
            
            # Í≥ÑÏ∏µÏ†Å Î°úÍ±∞ Ïù¥Î¶Ñ ÏÉùÏÑ±
            logger_name = f"pipeline.{step_name}"
            
            # Î°úÍ±∞ ÏÉùÏÑ±
            self.logger = logging.getLogger(logger_name)
            self.logger.setLevel(logging.INFO)
            
            # Ìï∏Îì§Îü¨Í∞Ä ÏóÜÏúºÎ©¥ Í∏∞Î≥∏ Ìï∏Îì§Îü¨ Ï∂îÍ∞Ä
            if not self.logger.handlers:
                handler = logging.StreamHandler()
                formatter = logging.Formatter(
                    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
                )
                handler.setFormatter(formatter)
                self.logger.addHandler(handler)
            
            # Ï¥àÍ∏∞ Î°úÍ∑∏ Î©îÏãúÏßÄ
            self.logger.info(f"üîß {step_name} logger Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            
        except Exception as e:
            # ÏµúÌõÑÏùò ÏàòÎã®: Í∏∞Î≥∏ Î°úÍ±∞ÎùºÎèÑ ÏÉùÏÑ±
            try:
                self.logger = logging.getLogger(__name__)
                self.logger.error(f"‚ùå logger Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            except:
                # printÎ°úÎùºÎèÑ Ïò§Î•ò ÌëúÏãú
                print(f"‚ùå CRITICAL: logger Ï¥àÍ∏∞Ìôî ÏôÑÏ†Ñ Ïã§Ìå®: {e}")
    
    # ==============================================
    # üî• Í∏∞Ï°¥ Î©îÏÑúÎìúÎì§ (logger Í¥ÄÎ†® ÏàòÏ†ï)
    # ==============================================
    
    def _check_numpy_compatibility(self):
        """NumPy 2.x Ìò∏ÌôòÏÑ± Ï≤¥ÌÅ¨ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        if NUMPY_AVAILABLE and int(np.__version__.split('.')[0]) >= 2:
            self.logger.warning(f"‚ö†Ô∏è NumPy {np.__version__} (2.x) Í∞êÏßÄÎê®")
    
    def _setup_basic_attributes(self, kwargs: Dict[str, Any]):
        """Í∏∞Î≥∏ ÏÜçÏÑ±Îì§ ÏÑ§Ï†ï (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            self.step_name = getattr(self, 'step_name', self.__class__.__name__)
            self.step_number = getattr(self, 'step_number', 0)
            self.step_type = getattr(self, 'step_type', 'unknown')
            
            self.is_initialized = False
            self.initialization_error = None
            self.error_count = 0
            self.last_error = None
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Í∏∞Î≥∏ ÏÜçÏÑ± ÏÑ§Ï†ï Ïã§Ìå®: {e}")
    
    def _safe_super_init(self):
        """ÏïàÏ†ÑÌïú super() Ìò∏Ï∂ú (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            mro = type(self).__mro__
            
            if len(mro) > 2 and mro[-2] != BaseStepMixin:
                try:
                    super().__init__()
                except TypeError as te:
                    if "positional argument" in str(te):
                        try:
                            super().__init__({})
                        except:
                            pass
                    else:
                        pass
                
        except Exception as e:
            self.logger.debug(f"super() Ìò∏Ï∂ú Í±¥ÎÑàÎúÄ: {e}")
    
    def _setup_device_and_system(self, kwargs: Dict[str, Any]):
        """ÎîîÎ∞îÏù¥Ïä§ Î∞è ÏãúÏä§ÌÖú ÏÑ§Ï†ï (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            self.device = self._safe_device_detection(kwargs)
            self.device_type = self._detect_device_type()
            self.memory_gb = kwargs.get('memory_gb', self._detect_memory())
            self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
            self.optimization_enabled = kwargs.get('optimization_enabled', True)
            self.quality_level = kwargs.get('quality_level', 'balanced')
            self.batch_size = kwargs.get('batch_size', self._calculate_optimal_batch_size())
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï Ïã§Ìå®: {e}")
            # Ìè¥Î∞± ÏÑ§Ï†ï
            self.device = DEFAULT_DEVICE
            self.device_type = "unknown"
            self.memory_gb = 16.0
            self.is_m3_max = False
            self.optimization_enabled = False
            self.quality_level = 'balanced'
            self.batch_size = 1
    
    def _safe_device_detection(self, kwargs: Dict[str, Any]) -> str:
        """ÏïàÏ†ÑÌïú ÎîîÎ∞îÏù¥Ïä§ ÌÉêÏßÄ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            device_candidates = [
                kwargs.get('device'),
                kwargs.get('preferred_device'), 
                kwargs.get('target_device'),
                getattr(self, 'device', None)
            ]
            
            for device in device_candidates:
                if device and device != "auto":
                    return device
            
            return self._auto_detect_device()
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÎîîÎ∞îÏù¥Ïä§ ÌÉêÏßÄ Ïã§Ìå®: {e}")
            return DEFAULT_DEVICE
    
    def _auto_detect_device(self, preferred_device: Optional[str] = None, device: Optional[str] = None) -> str:
        """ÌÜµÏùºÎêú ÎîîÎ∞îÏù¥Ïä§ ÏûêÎèô ÌÉêÏßÄ Î©îÏÑúÎìú (Í∏∞Ï°¥ ÏãúÍ∑∏ÎãàÏ≤ò Ïú†ÏßÄ)"""
        try:
            target_device = preferred_device or device
            
            if target_device and target_device != "auto":
                return target_device
                
            if not TORCH_AVAILABLE:
                return "cpu"
            
            if MPS_AVAILABLE:
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
                
        except Exception:
            return "cpu"
    
    def _detect_device_type(self) -> str:
        try:
            if self.device == "mps":
                return "apple_silicon"
            elif self.device == "cuda":
                return "nvidia_gpu"
            else:
                return "generic_cpu"
        except:
            return "unknown"
    
    def _detect_memory(self) -> float:
        try:
            import psutil
            return round(psutil.virtual_memory().total / (1024**3), 1)
        except:
            return 16.0
    
    def _detect_m3_max(self) -> bool:
        try:
            import platform
            processor = str(platform.processor())
            return ("M3" in processor or 
                   (self.device == "mps" and self.memory_gb > 64))
        except:
            return False
    
    def _calculate_optimal_batch_size(self) -> int:
        try:
            if self.is_m3_max and self.memory_gb >= 128:
                return 8
            elif self.memory_gb >= 64:
                return 4
            elif self.memory_gb >= 32:
                return 2
            else:
                return 1
        except:
            return 1
    
    def _setup_config_safely(self, kwargs: Dict[str, Any]):
        """ÏÑ§Ï†ï Í∞ùÏ≤¥ ÏïàÏ†Ñ Ï≤òÎ¶¨ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            raw_config = kwargs.get('config', {})
            self.config = SafeConfig(raw_config)
            
            safe_kwargs = {}
            for key, value in kwargs.items():
                try:
                    if key != 'config' and not callable(value):
                        safe_kwargs[key] = value
                except:
                    pass
            
            if safe_kwargs:
                self.config.update(safe_kwargs)
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÏÑ§Ï†ï Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
            self.config = SafeConfig({})
    
    def _setup_state_management(self):
        """ÏÉÅÌÉú Í¥ÄÎ¶¨ Ï¥àÍ∏∞Ìôî (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            # üî• Ïó¨Í∏∞ÏÑú model_interfaceÎ•º NoneÏúºÎ°ú Ï¥àÍ∏∞Ìôî (ÎÇòÏ§ëÏóê _setup_model_interfaceÏóêÏÑú ÏÑ§Ï†ï)
            self.model_interface = None
            self.model_loader = None
            
            cache_dir = getattr(self.config, 'cache_dir', './cache')
            self.cache_dir = Path(cache_dir)
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÏÉÅÌÉú Í¥ÄÎ¶¨ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    
    def _setup_m3_max_optimization(self):
        """M3 Max ÏµúÏ†ÅÌôî ÏÑ§Ï†ï (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            if self.device == "mps" and TORCH_AVAILABLE:
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                if hasattr(torch.backends.mps, 'enable_fallback'):
                    torch.backends.mps.enable_fallback = True
                
                if self.is_m3_max:
                    os.environ['OMP_NUM_THREADS'] = '16'
                
                self.dtype = torch.float32
                
                self.logger.info("üçé M3 Max ÏµúÏ†ÅÌôî ÏÑ§Ï†ï ÏôÑÎ£å")
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è M3 Max ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
    
    def _setup_memory_optimization(self):
        """Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÏÑ§Ï†ï (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            if TORCH_AVAILABLE:
                torch.backends.cudnn.benchmark = (self.device == "cuda")
                
                if hasattr(torch.backends, 'cuda') and self.device == "cuda":
                    torch.backends.cuda.matmul.allow_tf32 = True
                    torch.backends.cudnn.allow_tf32 = True
            
            gc.set_threshold(700, 10, 10)
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÏÑ§Ï†ï Ïã§Ìå®: {e}")
    
    def _setup_warmup_system(self):
        """ÏõåÎ∞çÏóÖ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            self.warmup_functions = {
                'model_warmup': self._safe_model_warmup,
                'device_warmup': self._safe_device_warmup,
                'memory_warmup': self._safe_memory_warmup,
                'pipeline_warmup': self._safe_pipeline_warmup
            }
            
            self.warmup_config = SafeConfig({
                'enabled': True,
                'timeout': 30.0,
                'retry_count': 3,
                'warm_cache': True
            })
            
            for name, func in self.warmup_functions.items():
                if not callable(func):
                    self.logger.error(f"‚ùå {name}Ïù¥ callableÏù¥ ÏïÑÎãò: {type(func)}")
                    self.warmup_functions[name] = self._create_dummy_warmup(name)
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÏõåÎ∞çÏóÖ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            self.warmup_functions = {}
            self.warmup_config = SafeConfig({})
    
    def _create_dummy_warmup(self, name: str) -> Callable:
        """ÏïàÏ†ÑÌïú ÎçîÎØ∏ ÏõåÎ∞çÏóÖ Ìï®Ïàò ÏÉùÏÑ± (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        async def dummy_warmup():
            self.logger.debug(f"üîß ÎçîÎØ∏ ÏõåÎ∞çÏóÖ Ïã§Ìñâ: {name}")
            return True
        return dummy_warmup
    
    def _setup_performance_monitoring(self):
        """ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            self.performance_metrics = {
                'total_calls': 0,
                'successful_calls': 0,
                'failed_calls': 0,
                'average_duration': 0.0,
                'min_duration': float('inf'),
                'max_duration': 0.0,
                'last_call_duration': 0.0,
                'total_duration': 0.0
            }
            
            self.last_processing_time = 0.0
            self.total_processing_count = 0
            self.performance_history = []
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            self.performance_metrics = {}
            self.last_processing_time = 0.0
            self.total_processing_count = 0
    
    def _emergency_initialization(self):
        """ÏùëÍ∏â Ï¥àÍ∏∞Ìôî (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            if not hasattr(self, 'logger'):
                self.logger = logging.getLogger(__name__)
            if not hasattr(self, 'step_name'):
                self.step_name = self.__class__.__name__
            if not hasattr(self, 'device'):
                self.device = "cpu"
            if not hasattr(self, 'config'):
                self.config = SafeConfig({})
            if not hasattr(self, 'is_initialized'):
                self.is_initialized = False
            if not hasattr(self, 'performance_metrics'):
                self.performance_metrics = {}
            
            self.logger.warning("‚ö†Ô∏è ÏùëÍ∏â Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            
        except:
            pass
    
    # ==============================================
    # üî• ÌïµÏã¨ Ïã†Í∑ú Î©îÏÑúÎìú 1: ModelLoader Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÑ§Ï†ï
    # ==============================================
    
    def _setup_model_interface(self):
        """
        üî• ModelLoader Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏûêÎèô ÏÑ§Ï†ï - ÌïµÏã¨ Í∞úÏÑ†
        
        ‚úÖ SafeFunctionValidatorÎ°ú Î™®Îì† Ìò∏Ï∂ú ÏïàÏ†ÑÏÑ± Î≥¥Ïû•
        ‚úÖ get_global_model_loader() ÏïàÏ†ÑÌïú Ìò∏Ï∂ú
        ‚úÖ create_step_interface() Dict Callable Ïò§Î•ò ÏôÑÏ†Ñ Ìï¥Í≤∞
        ‚úÖ ÏóêÎü¨ Î∞úÏÉùÏãú ÏïàÏ†ÑÌïú Ìè¥Î∞± Ï≤òÎ¶¨
        """
        try:
            self.logger.info(f"üîó {self.step_name} ModelLoader Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÑ§Ï†ï Ï§ë...")
            
            # Step 1: SafeFunctionValidator Ï¥àÍ∏∞Ìôî
            try:
                from app.ai_pipeline.utils.model_loader import SafeFunctionValidator
                self.function_validator = SafeFunctionValidator()
                validator_available = True
            except ImportError as e:
                self.logger.warning(f"SafeFunctionValidator import Ïã§Ìå®: {e}")
                validator_available = False
                # Ìè¥Î∞± validator ÏÉùÏÑ±
                self.function_validator = self._create_fallback_validator()
            
            # Step 2: ModelLoader ÏïàÏ†ÑÌïú Í∞ÄÏ†∏Ïò§Í∏∞
            model_loader = None
            
            # Î∞©Î≤ï 1: Ï†ÑÏó≠ ModelLoader
            try:
                from app.ai_pipeline.utils.model_loader import get_global_model_loader
                
                if validator_available:
                    success, result, message = self.function_validator.safe_call(get_global_model_loader)
                    if success:
                        model_loader = result
                        self.logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader ÌöçÎìù ÏÑ±Í≥µ")
                    else:
                        self.logger.warning(f"‚ö†Ô∏è get_global_model_loader Ìò∏Ï∂ú Ïã§Ìå®: {message}")
                else:
                    # Ìè¥Î∞±: ÏßÅÏ†ë Ìò∏Ï∂ú
                    model_loader = get_global_model_loader()
                    self.logger.info("‚úÖ Ï†ÑÏó≠ ModelLoader ÏßÅÏ†ë ÌöçÎìù")
                    
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Ï†ÑÏó≠ ModelLoader Í∞ÄÏ†∏Ïò§Í∏∞ Ïã§Ìå®: {e}")
            
            # Î∞©Î≤ï 2: DI Container (ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Í≤ΩÏö∞)
            if model_loader is None:
                try:
                    from app.core.di_container import get_di_container
                    di_container = get_di_container()
                    model_loader = di_container.get('model_loader')
                    if model_loader:
                        self.logger.info("‚úÖ DI ContainerÏóêÏÑú ModelLoader ÌöçÎìù")
                except Exception as e:
                    self.logger.debug(f"DI Container Ï°∞Ìöå Ïã§Ìå®: {e}")
            
            # Step 3: Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±
            if model_loader and hasattr(model_loader, 'create_step_interface'):
                try:
                    create_method = getattr(model_loader, 'create_step_interface')
                    
                    # üî• SafeFunctionValidatorÎ°ú ÏïàÏ†ÑÌïú Ìò∏Ï∂ú
                    if validator_available:
                        success, interface, message = self.function_validator.safe_call(
                            create_method, self.step_name
                        )
                        if success:
                            self.model_interface = interface
                            self.logger.info(f"‚úÖ {self.step_name} Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± ÏôÑÎ£å")
                        else:
                            self.logger.warning(f"‚ö†Ô∏è create_step_interface Ìò∏Ï∂ú Ïã§Ìå®: {message}")
                            self.model_interface = None
                    else:
                        # Ìè¥Î∞±: ÏßÅÏ†ë Ìò∏Ï∂ú
                        self.model_interface = create_method(self.step_name)
                        self.logger.info(f"‚úÖ {self.step_name} Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏßÅÏ†ë ÏÉùÏÑ± ÏôÑÎ£å")
                        
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
                    self.model_interface = None
            else:
                self.logger.warning("‚ö†Ô∏è ModelLoaderÏóê create_step_interface Î©îÏÑúÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§")
                self.model_interface = None
            
            # Step 4: ModelLoader Ïù∏Ïä§ÌÑ¥Ïä§ Ï†ÄÏû•
            self.model_loader = model_loader
            
            # Step 5: Ïó∞Îèô ÏÉÅÌÉú Î°úÍπÖ
            interface_status = "‚úÖ Ïó∞Í≤∞Îê®" if self.model_interface else "‚ùå Ïó∞Í≤∞ Ïã§Ìå®"
            loader_status = "‚úÖ Î°úÎìúÎê®" if self.model_loader else "‚ùå Î°úÎìú Ïã§Ìå®"
            
            self.logger.info(f"üîó ModelLoader Ïó∞Îèô Í≤∞Í≥º:")
            self.logger.info(f"   - ModelLoader: {loader_status}")
            self.logger.info(f"   - Step Interface: {interface_status}")
            self.logger.info(f"   - SafeFunctionValidator: {'‚úÖ ÏÇ¨Ïö©' if validator_available else '‚ùå Ìè¥Î∞±'}")
            
        except Exception as e:
            self.logger.error(f"‚ùå ModelLoader Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÑ§Ï†ï Ïã§Ìå®: {e}")
            self.logger.debug(f"üìã ÏÉÅÏÑ∏ Ïò§Î•ò: {traceback.format_exc()}")
            
            # ÏôÑÏ†Ñ Ìè¥Î∞± ÏÑ§Ï†ï
            self.model_interface = None
            self.model_loader = None
            self.function_validator = self._create_fallback_validator()
    
    def _create_fallback_validator(self):
        """Ìè¥Î∞± SafeFunctionValidator ÏÉùÏÑ±"""
        class FallbackValidator:
            @staticmethod
            def safe_call(obj, *args, **kwargs):
                try:
                    return True, obj(*args, **kwargs), "Success"
                except Exception as e:
                    return False, None, str(e)
            
            @staticmethod
            async def safe_async_call(obj, *args, **kwargs):
                try:
                    if asyncio.iscoroutinefunction(obj):
                        result = await obj(*args, **kwargs)
                    else:
                        result = obj(*args, **kwargs)
                    return True, result, "Success"
                except Exception as e:
                    return False, None, str(e)
        
        return FallbackValidator()
    
    # ==============================================
    # üî• ÌïµÏã¨ Ïã†Í∑ú Î©îÏÑúÎìú 2: 89.8GB Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÉêÏßÄ Î∞è Ïó∞Îèô
    # ==============================================
    
    def _setup_checkpoint_detection(self):
        """
        üî• 89.8GB Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô ÌÉêÏßÄ Î∞è Ïó∞Îèô
        
        ‚úÖ RealWorldModelDetector ÏÇ¨Ïö©
        ‚úÖ StepÎ≥Ñ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô Îß§Ìïë
        ‚úÖ ModelLoaderÏóê ÌÉêÏßÄ Í≤∞Í≥º ÏûêÎèô Îì±Î°ù
        ‚úÖ Ïã§Ï†ú PyTorch Í≤ÄÏ¶ù Ìè¨Ìï®
        """
        try:
            self.logger.info(f"üîç {self.step_name} Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÉêÏßÄ ÏãúÏûë...")
            
            # Step 1: RealWorldModelDetector Î°úÎìú
            try:
                from app.ai_pipeline.utils.auto_model_detector import (
                    RealWorldModelDetector, 
                    AdvancedModelLoaderAdapter,
                    create_real_world_detector
                )
                detector_available = True
            except ImportError as e:
                self.logger.warning(f"RealWorldModelDetector import Ïã§Ìå®: {e}")
                detector_available = False
                return
            
            # Step 2: ÌÉêÏßÄÍ∏∞ ÏÉùÏÑ± Î∞è Ïã§Ìñâ
            try:
                detector = create_real_world_detector(
                    enable_pytorch_validation=True,
                    max_workers=2  # Îπ†Î•∏ ÌÉêÏßÄÎ•º ÏúÑÌï¥ Ï†úÌïú
                )
                
                # StepÎ≥Ñ ÌïÑÌÑ∞ÎßÅÏúºÎ°ú ÌÉêÏßÄ
                step_model_filter = self._get_step_model_filter()
                
                detected_models = detector.detect_all_models(
                    model_type_filter=step_model_filter,
                    min_confidence=0.3,
                    force_rescan=False  # Ï∫êÏãú ÏÇ¨Ïö©
                )
                
                if detected_models:
                    self.logger.info(f"‚úÖ {len(detected_models)}Í∞ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÉêÏßÄ ÏôÑÎ£å")
                    
                    # StepÎ≥Ñ Î™®Îç∏ Ï∞æÍ∏∞
                    step_models = self._find_models_for_step(detected_models)
                    if step_models:
                        self.logger.info(f"üéØ {self.step_name}Ïö© Î™®Îç∏ {len(step_models)}Í∞ú Î∞úÍ≤¨:")
                        for model_name, model_info in step_models.items():
                            size_gb = model_info.file_size_mb / 1024
                            validation = "‚úÖÍ≤ÄÏ¶ùÎê®" if model_info.pytorch_valid else "‚ùìÎØ∏Í≤ÄÏ¶ù"
                            self.logger.info(f"   - {model_name}: {size_gb:.1f}GB {validation}")
                    
                    # Step 3: ModelLoaderÏóê ÏûêÎèô Îì±Î°ù
                    if self.model_loader and step_models:
                        self._register_detected_models(step_models)
                        
                else:
                    self.logger.warning("‚ö†Ô∏è ÌÉêÏßÄÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§")
                    
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÉêÏßÄ Ïã§Ìñâ Ïã§Ìå®: {e}")
                
        except Exception as e:
            self.logger.error(f"‚ùå Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÉêÏßÄ ÏÑ§Ï†ï Ïã§Ìå®: {e}")
    
    def _get_step_model_filter(self) -> List[str]:
        """StepÎ≥Ñ Î™®Îç∏ ÌÉÄÏûÖ ÌïÑÌÑ∞ Î∞òÌôò"""
        step_filters = {
            "HumanParsingStep": ["human_parsing"],
            "PoseEstimationStep": ["pose_estimation"],
            "ClothSegmentationStep": ["cloth_segmentation"],
            "GeometricMatchingStep": ["geometric_matching"],
            "ClothWarpingStep": ["cloth_warping"],
            "VirtualFittingStep": ["virtual_fitting"],
            "PostProcessingStep": ["post_processing"],
            "QualityAssessmentStep": ["quality_assessment"]
        }
        
        return step_filters.get(self.step_name, [])
    
    def _find_models_for_step(self, detected_models: Dict) -> Dict:
        """StepÎ≥Ñ Í¥ÄÎ†® Î™®Îç∏ Ï∞æÍ∏∞"""
        step_models = {}
        
        for model_name, model_info in detected_models.items():
            # Step Ïù¥Î¶Ñ Îß§Ïπ≠
            if model_info.step_name == self.step_name:
                step_models[model_name] = model_info
            # Î™®Îç∏ ÌÉÄÏûÖ Îß§Ïπ≠
            elif any(filter_type in model_info.category.value 
                    for filter_type in self._get_step_model_filter()):
                step_models[model_name] = model_info
        
        return step_models
    
    def _register_detected_models(self, step_models: Dict):
        """ÌÉêÏßÄÎêú Î™®Îç∏Îì§ÏùÑ ModelLoaderÏóê Îì±Î°ù"""
        try:
            if not self.model_loader or not hasattr(self.model_loader, 'register_model'):
                self.logger.warning("‚ö†Ô∏è ModelLoaderÏóê register_model Î©îÏÑúÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§")
                return
            
            registered_count = 0
            
            for model_name, model_info in step_models.items():
                try:
                    # Î™®Îç∏ ÏÑ§Ï†ï ÎîïÏÖîÎÑàÎ¶¨ ÏÉùÏÑ±
                    model_config = {
                        'name': model_name,
                        'type': model_info.category.value,
                        'checkpoint_path': str(model_info.path),
                        'device': self.device,
                        'pytorch_validated': model_info.pytorch_valid,
                        'parameter_count': model_info.parameter_count,
                        'file_size_mb': model_info.file_size_mb,
                        'confidence_score': model_info.confidence_score,
                        'step_name': self.step_name,
                        'auto_detected': True
                    }
                    
                    # ÏïàÏ†ÑÌïú Îì±Î°ù
                    if hasattr(self, 'function_validator'):
                        success, result, message = self.function_validator.safe_call(
                            self.model_loader.register_model, model_name, model_config
                        )
                        if success:
                            registered_count += 1
                            self.logger.debug(f"‚úÖ Î™®Îç∏ Îì±Î°ù ÏÑ±Í≥µ: {model_name}")
                        else:
                            self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ Îì±Î°ù Ïã§Ìå® {model_name}: {message}")
                    else:
                        # ÏßÅÏ†ë Îì±Î°ù
                        self.model_loader.register_model(model_name, model_config)
                        registered_count += 1
                        
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ Îì±Î°ù Ï§ë Ïò§Î•ò {model_name}: {e}")
            
            if registered_count > 0:
                self.logger.info(f"‚úÖ {registered_count}Í∞ú Î™®Îç∏ ModelLoaderÏóê Îì±Î°ù ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Îì±Î°ù Ïã§Ìå®: {e}")
    
    # ==============================================
    # üî• Í∏∞Ï°¥ ÏõåÎ∞çÏóÖ Î©îÏÑúÎìúÎì§ (ÎÇ¥Ïö© Ïú†ÏßÄ)
    # ==============================================
    
    async def _safe_model_warmup(self) -> bool:
        """ÏïàÏ†ÑÌïú Î™®Îç∏ ÏõåÎ∞çÏóÖ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            self.logger.debug(f"üî• {self.step_name} Î™®Îç∏ ÏõåÎ∞çÏóÖ...")
            
            if TORCH_AVAILABLE and self.device == "mps":
                warmup_tensor = torch.randn(1, 3, 224, 224, 
                                          device=self.device, dtype=self.dtype)
                _ = warmup_tensor * 2.0
                del warmup_tensor
                
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            
            return True
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Î™®Îç∏ ÏõåÎ∞çÏóÖ Ïã§Ìå®: {e}")
            return False
    
    async def _safe_device_warmup(self) -> bool:
        """ÏïàÏ†ÑÌïú ÎîîÎ∞îÏù¥Ïä§ ÏõåÎ∞çÏóÖ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            if TORCH_AVAILABLE:
                test_tensor = torch.tensor([1.0], device=self.device, dtype=self.dtype)
                result = test_tensor + 1.0
                del test_tensor, result
            
            return True
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÎîîÎ∞îÏù¥Ïä§ ÏõåÎ∞çÏóÖ Ïã§Ìå®: {e}")
            return False
    
    async def _safe_memory_warmup(self) -> bool:
        """ÏïàÏ†ÑÌïú Î©îÎ™®Î¶¨ ÏõåÎ∞çÏóÖ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            gc.collect()
            
            if TORCH_AVAILABLE and self.device == "mps":
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            
            return True
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Î©îÎ™®Î¶¨ ÏõåÎ∞çÏóÖ Ïã§Ìå®: {e}")
            return False
    
    async def _safe_pipeline_warmup(self) -> bool:
        """ÏïàÏ†ÑÌïú ÌååÏù¥ÌîÑÎùºÏù∏ ÏõåÎ∞çÏóÖ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            if not hasattr(self, 'config') or not self.config:
                self.config = SafeConfig({})
            
            return True
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÌååÏù¥ÌîÑÎùºÏù∏ ÏõåÎ∞çÏóÖ Ïã§Ìå®: {e}")
            return False
    
    # ==============================================
    # üî• Í∏∞Ï°¥ Ï£ºÏöî Î©îÏÑúÎìúÎì§ (ÎÇ¥Ïö© Ïú†ÏßÄ)
    # ==============================================
    
    async def initialize_step(self) -> bool:
        """Step ÏôÑÏ†Ñ Ï¥àÍ∏∞Ìôî (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            self.logger.info(f"üöÄ {self.step_name} Ï¥àÍ∏∞Ìôî ÏãúÏûë...")
            
            self._verify_essential_attributes()
            await self._execute_safe_warmup()
            
            if hasattr(self, '_custom_initialize') and callable(self._custom_initialize):
                await self._custom_initialize()
            
            self.is_initialized = True
            self.logger.info(f"‚úÖ {self.step_name} Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå {self.step_name} Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            self.last_error = str(e)
            self.error_count += 1
            return False
    
    def _verify_essential_attributes(self):
        """ÌïÑÏàò ÏÜçÏÑ±Îì§ Í≤ÄÏ¶ù (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        essential_attrs = ['logger', 'step_name', 'device', 'config']
        
        for attr in essential_attrs:
            if not hasattr(self, attr):
                if attr == 'logger':
                    self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
                elif attr == 'step_name':
                    self.step_name = self.__class__.__name__
                elif attr == 'device':
                    self.device = DEFAULT_DEVICE
                elif attr == 'config':
                    self.config = SafeConfig({})
    
    async def _execute_safe_warmup(self):
        """ÏïàÏ†ÑÌïú ÏõåÎ∞çÏóÖ Ïã§Ìñâ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            if not hasattr(self, 'warmup_functions') or not self.warmup_functions:
                return
            
            for warmup_name, warmup_func in self.warmup_functions.items():
                try:
                    if callable(warmup_func):
                        await warmup_func()
                    else:
                        self.logger.warning(f"‚ö†Ô∏è {warmup_name}Ïù¥ callableÏù¥ ÏïÑÎãò")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è {warmup_name} Ïã§Ìå®: {e}")
                    
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÏõåÎ∞çÏóÖ Ïã§Ìñâ Ïã§Ìå®: {e}")
    
    def record_performance(self, operation_name: str, duration: float, success: bool = True):
        """ÏÑ±Îä• Î©îÌä∏Î¶≠ Í∏∞Î°ù (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            self.performance_metrics['total_calls'] += 1
            self.performance_metrics['total_duration'] += duration
            self.performance_metrics['last_call_duration'] = duration
            
            if success:
                self.performance_metrics['successful_calls'] += 1
            else:
                self.performance_metrics['failed_calls'] += 1
            
            self.performance_metrics['min_duration'] = min(
                self.performance_metrics['min_duration'], duration
            )
            self.performance_metrics['max_duration'] = max(
                self.performance_metrics['max_duration'], duration
            )
            
            if self.performance_metrics['total_calls'] > 0:
                self.performance_metrics['average_duration'] = (
                    self.performance_metrics['total_duration'] / 
                    self.performance_metrics['total_calls']
                )
            
            self.performance_history.append({
                'operation': operation_name,
                'duration': duration,
                'success': success,
                'timestamp': time.time()
            })
            
            if len(self.performance_history) > 100:
                self.performance_history.pop(0)
                
        except Exception as e:
            self.logger.debug(f"ÏÑ±Îä• Í∏∞Î°ù Ïã§Ìå®: {e}")
    
    def get_step_info(self) -> Dict[str, Any]:
        """Step ÏÉÅÌÉú Ï†ïÎ≥¥ Î∞òÌôò (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ + Ï∂îÍ∞Ä Ï†ïÎ≥¥)"""
        try:
            base_info = {
                'step_name': getattr(self, 'step_name', 'unknown'),
                'step_number': getattr(self, 'step_number', 0),
                'step_type': getattr(self, 'step_type', 'unknown'),
                'device': getattr(self, 'device', 'unknown'),
                'device_type': getattr(self, 'device_type', 'unknown'),
                'memory_gb': getattr(self, 'memory_gb', 0.0),
                'is_m3_max': getattr(self, 'is_m3_max', False),
                'optimization_enabled': getattr(self, 'optimization_enabled', False),
                'quality_level': getattr(self, 'quality_level', 'unknown'),
                'batch_size': getattr(self, 'batch_size', 1),
                'is_initialized': getattr(self, 'is_initialized', False),
                'has_model_interface': getattr(self, 'model_interface', None) is not None,
                'last_processing_time': getattr(self, 'last_processing_time', 0.0),
                'total_processing_count': getattr(self, 'total_processing_count', 0),
                'error_count': getattr(self, 'error_count', 0),
                'last_error': getattr(self, 'last_error', None),
                'torch_available': TORCH_AVAILABLE,
                'mps_available': MPS_AVAILABLE,
                'numpy_version': np.__version__ if NUMPY_AVAILABLE else 'N/A',
                'config_type': type(getattr(self, 'config', None)).__name__,
                'performance_metrics': getattr(self, 'performance_metrics', {})
            }
            
            # üî• ÏÉàÎ°úÏö¥ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            base_info.update({
                'has_model_loader': getattr(self, 'model_loader', None) is not None,
                'has_function_validator': getattr(self, 'function_validator', None) is not None,
                'checkpoint_detection_enabled': True,  # v6.0ÏóêÏÑú Ìï≠ÏÉÅ ÌôúÏÑ±Ìôî
                'model_interface_type': type(getattr(self, 'model_interface', None)).__name__ if getattr(self, 'model_interface', None) else 'None'
            })
            
            return base_info
            
        except Exception as e:
            return {
                'error': f"Ï†ïÎ≥¥ ÏàòÏßë Ïã§Ìå®: {e}",
                'step_name': getattr(self, 'step_name', 'unknown')
            }
    
    def cleanup_models(self):
        """Î™®Îç∏ Ï†ïÎ¶¨ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            if hasattr(self, 'model_interface') and self.model_interface:
                cleanup_func = getattr(self.model_interface, 'unload_models', None)
                if callable(cleanup_func):
                    cleanup_func()
                
            if TORCH_AVAILABLE:
                if self.device == "mps" and hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
                
                gc.collect()
            
            self.logger.info(f"üßπ {self.step_name} Ï†ïÎ¶¨ ÏôÑÎ£å")
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Ï†ïÎ¶¨ Ï§ë Ïò§Î•ò: {e}")
    
    def __del__(self):
        """ÏÜåÎ©∏Ïûê (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
        try:
            self.cleanup_models()
        except:
            pass

# ==============================================
# üî• Í∏∞Ï°¥ Îç∞ÏΩîÎ†àÏù¥ÌÑ∞Îì§ (100% Ïú†ÏßÄ)
# ==============================================

def ensure_step_initialization(func: Callable) -> Callable:
    """Step Ï¥àÍ∏∞Ìôî Î≥¥Ïû• Îç∞ÏΩîÎ†àÏù¥ÌÑ∞ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        if not hasattr(self, 'logger'):
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        if not getattr(self, 'is_initialized', False):
            await self.initialize_step()
        
        return await func(self, *args, **kwargs)
    return wrapper

def safe_step_method(func: Callable) -> Callable:
    """Step Î©îÏÑúÎìú ÏïàÏ†Ñ Ïã§Ìñâ Îç∞ÏΩîÎ†àÏù¥ÌÑ∞ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        try:
            return await func(self, *args, **kwargs)
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.error(f"‚ùå {func.__name__} Ïã§Ìñâ Ïã§Ìå®: {e}")
            if hasattr(self, 'error_count'):
                self.error_count += 1
            if hasattr(self, 'last_error'):
                self.last_error = str(e)
            
            return {
                'success': False,
                'error': str(e),
                'step_name': getattr(self, 'step_name', self.__class__.__name__),
                'method_name': func.__name__
            }
    return wrapper

def performance_monitor(operation_name: str) -> Callable:
    """ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ Îç∞ÏΩîÎ†àÏù¥ÌÑ∞ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            start_time = time.time()
            success = True
            try:
                result = await func(self, *args, **kwargs)
                return result
            except Exception as e:
                success = False
                raise e
            finally:
                duration = time.time() - start_time
                
                if hasattr(self, 'record_performance'):
                    self.record_performance(operation_name, duration, success)
                
                if hasattr(self, 'last_processing_time'):
                    self.last_processing_time = duration
                if hasattr(self, 'total_processing_count'):
                    self.total_processing_count += 1
                    
        return wrapper
    return decorator

def memory_optimize(func: Callable) -> Callable:
    """Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî Îç∞ÏΩîÎ†àÏù¥ÌÑ∞ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        try:
            result = await func(self, *args, **kwargs)
            
            if TORCH_AVAILABLE and hasattr(self, 'device'):
                if self.device == "mps" and hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
                gc.collect()
            
            return result
        except Exception as e:
            if TORCH_AVAILABLE:
                gc.collect()
            raise e
    return wrapper

def step_timing(func: Callable) -> Callable:
    """Step Ïã§Ìñâ ÏãúÍ∞Ñ Ï∏°Ï†ï Îç∞ÏΩîÎ†àÏù¥ÌÑ∞ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        start_time = time.time()
        try:
            result = await func(self, *args, **kwargs)
            
            processing_time = time.time() - start_time
            if hasattr(self, 'last_processing_time'):
                self.last_processing_time = processing_time
            if hasattr(self, 'total_processing_count'):
                self.total_processing_count += 1
                
            return result
        except Exception as e:
            processing_time = time.time() - start_time
            if hasattr(self, 'last_processing_time'):
                self.last_processing_time = processing_time
            raise e
    return wrapper

def error_handler(func: Callable) -> Callable:
    """ÏóêÎü¨ Ï≤òÎ¶¨ Îç∞ÏΩîÎ†àÏù¥ÌÑ∞ (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        try:
            return await func(self, *args, **kwargs)
        except Exception as e:
            if hasattr(self, 'error_count'):
                self.error_count += 1
            if hasattr(self, 'last_error'):
                self.last_error = str(e)
            if hasattr(self, 'logger'):
                self.logger.error(f"‚ùå {func.__name__} Ïã§Ìñâ Ïã§Ìå®: {e}")
                self.logger.debug(f"üìã ÏÉÅÏÑ∏ Ïò§Î•ò: {traceback.format_exc()}")
            
            return {
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__,
                'step_name': getattr(self, 'step_name', self.__class__.__name__),
                'method_name': func.__name__,
                'timestamp': time.time()
            }
    return wrapper

# ==============================================
# üî• Í∏∞Ï°¥ StepÎ≥Ñ ÌäπÌôî MixinÎì§ (100% Ïú†ÏßÄ)
# ==============================================

class HumanParsingMixin(BaseStepMixin):
    """Step 1: Human Parsing ÌäπÌôî Mixin (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 1
        self.step_type = "human_parsing"
        self.num_classes = 20
        self.output_format = "segmentation_mask"

class PoseEstimationMixin(BaseStepMixin):
    """Step 2: Pose Estimation ÌäπÌôî Mixin (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 2
        self.step_type = "pose_estimation"
        self.num_keypoints = 18
        self.output_format = "keypoints"

class ClothSegmentationMixin(BaseStepMixin):
    """Step 3: Cloth Segmentation ÌäπÌôî Mixin (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 3
        self.step_type = "cloth_segmentation"
        self.output_format = "cloth_mask"

class GeometricMatchingMixin(BaseStepMixin):
    """Step 4: Geometric Matching ÌäπÌôî Mixin (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 4
        self.step_type = "geometric_matching"
        self.num_control_points = 25
        self.output_format = "transformation_matrix"

class ClothWarpingMixin(BaseStepMixin):
    """Step 5: Cloth Warping ÌäπÌôî Mixin (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 5
        self.step_type = "cloth_warping"
        self.enable_physics = True
        self.output_format = "warped_cloth"

class VirtualFittingMixin(BaseStepMixin):
    """Step 6: Virtual Fitting ÌäπÌôî Mixin (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 6
        self.step_type = "virtual_fitting"
        self.diffusion_steps = 50
        self.output_format = "rgb_image"

class PostProcessingMixin(BaseStepMixin):
    """Step 7: Post Processing ÌäπÌôî Mixin (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 7
        self.step_type = "post_processing"
        self.upscale_factor = 2
        self.output_format = "enhanced_image"

class QualityAssessmentMixin(BaseStepMixin):
    """Step 8: Quality Assessment ÌäπÌôî Mixin (Í∏∞Ï°¥ ÎÇ¥Ïö© Ïú†ÏßÄ)"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 8
        self.step_type = "quality_assessment"
        self.quality_threshold = 0.7
        self.output_format = "quality_scores"
        
        self.assessment_modes = ['perceptual', 'technical', 'aesthetic', 'fitting']
        self.quality_aspects = ['sharpness', 'color', 'fitting', 'realism', 'artifacts']
        self.scoring_weights = {
            'perceptual': 0.4,
            'technical': 0.3,
            'aesthetic': 0.2,
            'fitting': 0.1
        }