# backend/app/ai_pipeline/steps/base_step_mixin.py
"""
ðŸ”¥ BaseStepMixin v19.1 - DetailedDataSpec ì™„ì „ í†µí•© (GitHub í”„ë¡œì íŠ¸ 100% í˜¸í™˜)
================================================================================

âœ… step_model_requirements.py DetailedDataSpec ì™„ì „ í™œìš©
âœ… API â†” AI ëª¨ë¸ ê°„ ë°ì´í„° ë³€í™˜ í‘œì¤€í™” ì™„ë£Œ
âœ… Step ê°„ ë°ì´í„° íë¦„ ìžë™ ì²˜ë¦¬
âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ìžë™ ì ìš©
âœ… GitHub í”„ë¡œì íŠ¸ Step í´ëž˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜
âœ… process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ì™„ì „ í‘œì¤€í™”
âœ… validate_dependencies() ì˜¤ë²„ë¡œë“œ ì§€ì›
âœ… StepFactory v11.0ê³¼ ì™„ì „ í˜¸í™˜
âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™” (mycloset-ai-clean)
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›

í•µì‹¬ ê°œì„ ì‚¬í•­:
1. ðŸŽ¯ DetailedDataSpec ì •ë³´ ì €ìž¥ ë° ê´€ë¦¬
2. ðŸ”„ í‘œì¤€í™”ëœ process ë©”ì„œë“œ ìž¬ì„¤ê³„ (ìž…ë ¥ë³€í™˜ â†’ AIë¡œì§ â†’ ì¶œë ¥ë³€í™˜)
3. ðŸ” ìž…ë ¥ ë°ì´í„° ë³€í™˜ ì‹œìŠ¤í…œ (API/Stepê°„ â†’ AIëª¨ë¸ í˜•ì‹)
4. âš™ï¸ ì „ì²˜ë¦¬ ìžë™ ì ìš© (preprocessing_steps ê¸°ë°˜)
5. ðŸ“¤ ì¶œë ¥ ë°ì´í„° ë³€í™˜ ì‹œìŠ¤í…œ (AIëª¨ë¸ â†’ API + Stepê°„ í˜•ì‹)
6. ðŸ”§ í›„ì²˜ë¦¬ ìžë™ ì ìš© (postprocessing_steps ê¸°ë°˜)
7. âœ… ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ (íƒ€ìž…, í˜•íƒœ, ë²”ìœ„ ê²€ì¦)
8. ðŸ› ï¸ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (base64 ë³€í™˜, ì—ëŸ¬ ì²˜ë¦¬ ë“±)

Author: MyCloset AI Team
Date: 2025-07-27
Version: 19.1 (DetailedDataSpec Full Integration)
"""

import os
import gc
import time
import asyncio
import logging
import threading
import traceback
import weakref
import subprocess
import platform
import inspect
import base64
from io import BytesIO
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, Type, TYPE_CHECKING, Awaitable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from functools import wraps
from contextlib import asynccontextmanager
from enum import Enum

# ðŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from ..utils.model_loader import ModelLoader, StepModelInterface
    from ..factories.step_factory import StepFactory
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
    from ..core.di_container import DIContainer

# ==============================================
# ðŸ”¥ í™˜ê²½ ì„¤ì • ë° ì‹œìŠ¤í…œ ì •ë³´
# ==============================================

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean'
}

# ì‹œìŠ¤í…œ ì •ë³´
IS_M3_MAX = False
MEMORY_GB = 16.0

try:
    if platform.system() == 'Darwin':
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.stdout.strip():
            MEMORY_GB = int(memory_result.stdout.strip()) / 1024**3
except:
    pass

# ==============================================
# ðŸ”¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì „ import
# ==============================================

# PyTorch ì•ˆì „ import
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
except ImportError:
    torch = None

# PIL ì•ˆì „ import
PIL_AVAILABLE = False
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    Image = None

# NumPy ì•ˆì „ import
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None

# OpenCV ì•ˆì „ import
CV2_AVAILABLE = False
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    cv2 = None

# ==============================================
# ðŸ”¥ GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì¸í„°íŽ˜ì´ìŠ¤ (v19.1)
# ==============================================

class ProcessMethodSignature(Enum):
    """GitHub í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” process ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ íŒ¨í„´"""
    STANDARD = "async def process(self, **kwargs) -> Dict[str, Any]"
    INPUT_DATA = "async def process(self, input_data: Any) -> Dict[str, Any]"
    PIPELINE = "async def process_pipeline(self, input_data: Dict[str, Any]) -> Dict[str, Any]"
    LEGACY = "def process(self, *args, **kwargs) -> Dict[str, Any]"

class DependencyValidationFormat(Enum):
    """ì˜ì¡´ì„± ê²€ì¦ ë°˜í™˜ í˜•ì‹"""
    BOOLEAN_DICT = "dict_bool"  # GeometricMatchingStep í˜•ì‹: {'model_loader': True, ...}
    DETAILED_DICT = "dict_detailed"  # BaseStepMixin v18.0 í˜•ì‹: {'success': True, 'details': {...}}
    AUTO_DETECT = "auto"  # í˜¸ì¶œìžì— ë”°ë¼ ìžë™ ì„ íƒ

class DataConversionMethod(Enum):
    """ë°ì´í„° ë³€í™˜ ë°©ë²•"""
    AUTOMATIC = "auto"      # DetailedDataSpec ê¸°ë°˜ ìžë™ ë³€í™˜
    MANUAL = "manual"       # í•˜ìœ„ í´ëž˜ìŠ¤ì—ì„œ ìˆ˜ë™ ë³€í™˜
    HYBRID = "hybrid"       # ìžë™ + ìˆ˜ë™ ì¡°í•©

# ==============================================
# ðŸ”¥ ì„¤ì • ë° ìƒíƒœ í´ëž˜ìŠ¤ (v19.1 DetailedDataSpec ì§€ì›)
# ==============================================

@dataclass
class DetailedDataSpecConfig:
    """DetailedDataSpec ì„¤ì • ê´€ë¦¬"""
    # ìž…ë ¥ ì‚¬ì–‘
    input_data_types: List[str] = field(default_factory=list)
    input_shapes: Dict[str, Tuple[int, ...]] = field(default_factory=dict)
    input_value_ranges: Dict[str, Tuple[float, float]] = field(default_factory=dict)
    preprocessing_required: List[str] = field(default_factory=list)
    
    # ì¶œë ¥ ì‚¬ì–‘  
    output_data_types: List[str] = field(default_factory=list)
    output_shapes: Dict[str, Tuple[int, ...]] = field(default_factory=dict)
    output_value_ranges: Dict[str, Tuple[float, float]] = field(default_factory=dict)
    postprocessing_required: List[str] = field(default_factory=list)
    
    # API í˜¸í™˜ì„±
    api_input_mapping: Dict[str, str] = field(default_factory=dict)
    api_output_mapping: Dict[str, str] = field(default_factory=dict)
    
    # Step ê°„ ì—°ë™
    step_input_schema: Dict[str, Any] = field(default_factory=dict)
    step_output_schema: Dict[str, Any] = field(default_factory=dict)
    
    # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­
    normalization_mean: Tuple[float, ...] = field(default_factory=lambda: (0.485, 0.456, 0.406))
    normalization_std: Tuple[float, ...] = field(default_factory=lambda: (0.229, 0.224, 0.225))
    preprocessing_steps: List[str] = field(default_factory=list)
    postprocessing_steps: List[str] = field(default_factory=list)
    
    # Step ê°„ ë°ì´í„° ì „ë‹¬ ìŠ¤í‚¤ë§ˆ
    accepts_from_previous_step: Dict[str, Dict[str, str]] = field(default_factory=dict)
    provides_to_next_step: Dict[str, Dict[str, str]] = field(default_factory=dict)

@dataclass
class GitHubStepConfig:
    """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ Step ì„¤ì • (v19.1)"""
    step_name: str = "BaseStep"
    step_id: int = 0
    device: str = "auto"
    use_fp16: bool = True
    batch_size: int = 1
    confidence_threshold: float = 0.8
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True
    optimization_enabled: bool = True
    quality_level: str = "balanced"
    strict_mode: bool = False
    
    # ì˜ì¡´ì„± ì„¤ì •
    auto_inject_dependencies: bool = True
    require_model_loader: bool = True
    require_memory_manager: bool = False
    require_data_converter: bool = False
    dependency_timeout: float = 30.0
    dependency_retry_count: int = 3
    
    # GitHub í”„ë¡œì íŠ¸ íŠ¹ë³„ ì„¤ì •
    process_method_signature: ProcessMethodSignature = ProcessMethodSignature.STANDARD
    dependency_validation_format: DependencyValidationFormat = DependencyValidationFormat.AUTO_DETECT
    github_compatibility_mode: bool = True
    real_ai_pipeline_support: bool = True
    
    # DetailedDataSpec ì„¤ì • (v19.1 ì‹ ê·œ)
    enable_detailed_data_spec: bool = True
    data_conversion_method: DataConversionMethod = DataConversionMethod.AUTOMATIC
    strict_data_validation: bool = True
    auto_preprocessing: bool = True
    auto_postprocessing: bool = True
    
    # í™˜ê²½ ìµœì í™”
    conda_optimized: bool = False
    conda_env: str = "none"
    m3_max_optimized: bool = False
    memory_gb: float = 16.0
    use_unified_memory: bool = False

@dataclass
class GitHubDependencyStatus:
    """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì˜ì¡´ì„± ìƒíƒœ (v19.1)"""
    model_loader: bool = False
    step_interface: bool = False
    memory_manager: bool = False
    data_converter: bool = False
    di_container: bool = False
    base_initialized: bool = False
    custom_initialized: bool = False
    dependencies_validated: bool = False
    
    # GitHub íŠ¹ë³„ ìƒíƒœ
    github_compatible: bool = False
    process_method_validated: bool = False
    real_ai_models_loaded: bool = False
    
    # DetailedDataSpec ìƒíƒœ (v19.1 ì‹ ê·œ)
    detailed_data_spec_loaded: bool = False
    data_conversion_ready: bool = False
    preprocessing_configured: bool = False
    postprocessing_configured: bool = False
    api_mapping_configured: bool = False
    step_flow_configured: bool = False
    
    # í™˜ê²½ ìƒíƒœ
    conda_optimized: bool = False
    m3_max_optimized: bool = False
    
    # ì£¼ìž… ì‹œë„ ì¶”ì 
    injection_attempts: Dict[str, int] = field(default_factory=dict)
    injection_errors: Dict[str, List[str]] = field(default_factory=dict)
    last_injection_time: float = field(default_factory=time.time)

@dataclass
class GitHubPerformanceMetrics:
    """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ (v19.1)"""
    process_count: int = 0
    total_process_time: float = 0.0
    average_process_time: float = 0.0
    error_count: int = 0
    success_count: int = 0
    cache_hits: int = 0
    
    # ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­
    peak_memory_usage_mb: float = 0.0
    average_memory_usage_mb: float = 0.0
    memory_optimizations: int = 0
    
    # AI ëª¨ë¸ ë©”íŠ¸ë¦­
    models_loaded: int = 0
    total_model_size_gb: float = 0.0
    inference_count: int = 0
    
    # ì˜ì¡´ì„± ë©”íŠ¸ë¦­
    dependencies_injected: int = 0
    injection_failures: int = 0
    average_injection_time: float = 0.0
    
    # GitHub íŠ¹ë³„ ë©”íŠ¸ë¦­
    github_process_calls: int = 0
    real_ai_inferences: int = 0
    pipeline_success_rate: float = 0.0
    
    # DetailedDataSpec ë©”íŠ¸ë¦­ (v19.1 ì‹ ê·œ)
    data_conversions: int = 0
    preprocessing_operations: int = 0
    postprocessing_operations: int = 0
    api_conversions: int = 0
    step_data_transfers: int = 0
    validation_failures: int = 0

# ==============================================
# ðŸ”¥ GitHub í˜¸í™˜ ì˜ì¡´ì„± ê´€ë¦¬ìž v19.1 (ì¶•ì•½ ë²„ì „)
# ==============================================

class GitHubDependencyManager:
    """GitHub í”„ë¡œì íŠ¸ ì™„ì „ í˜¸í™˜ ì˜ì¡´ì„± ê´€ë¦¬ìž v19.1"""
    
    def __init__(self, step_name: str):
        self.step_name = step_name
        self.logger = logging.getLogger(f"GitHubDependencyManager.{step_name}")
        
        # ì˜ì¡´ì„± ì €ìž¥
        self.dependencies: Dict[str, Any] = {}
        self.dependency_status = GitHubDependencyStatus()
        
        # í™˜ê²½ ì •ë³´
        self.conda_info = CONDA_INFO
        self.is_m3_max = IS_M3_MAX
        self.memory_gb = MEMORY_GB
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        
        # í™˜ê²½ ìµœì í™” ì„¤ì •
        self._setup_environment_optimization()
    
    def _setup_environment_optimization(self):
        """í™˜ê²½ ìµœì í™” ì„¤ì •"""
        try:
            if self.conda_info['is_target_env']:
                self.dependency_status.conda_optimized = True
                self.logger.debug(f"âœ… conda í™˜ê²½ ìµœì í™” í™œì„±í™”: {self.conda_info['conda_env']}")
            
            if self.is_m3_max:
                self.dependency_status.m3_max_optimized = True
                self.logger.debug(f"âœ… M3 Max ìµœì í™” í™œì„±í™”: {self.memory_gb:.1f}GB")
                
        except Exception as e:
            self.logger.debug(f"í™˜ê²½ ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def inject_model_loader(self, model_loader) -> bool:
        """GitHub í˜¸í™˜ ModelLoader ì˜ì¡´ì„± ì£¼ìž…"""
        try:
            with self._lock:
                self.dependencies['model_loader'] = model_loader
                self.dependency_status.model_loader = True
                self.logger.info(f"âœ… {self.step_name} ModelLoader ì˜ì¡´ì„± ì£¼ìž… ì™„ë£Œ")
                return True
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ModelLoader ì£¼ìž… ì‹¤íŒ¨: {e}")
            return False
    
    def inject_memory_manager(self, memory_manager) -> bool:
        """GitHub í˜¸í™˜ MemoryManager ì˜ì¡´ì„± ì£¼ìž…"""
        try:
            with self._lock:
                self.dependencies['memory_manager'] = memory_manager
                self.dependency_status.memory_manager = True
                return True
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} MemoryManager ì£¼ìž… ì‹¤íŒ¨: {e}")
            return False
    
    def get_dependency(self, name: str) -> Optional[Any]:
        """ì˜ì¡´ì„± ì¡°íšŒ"""
        with self._lock:
            return self.dependencies.get(name)
    
    def validate_dependencies_github_format(self, format_type: DependencyValidationFormat = None) -> Union[Dict[str, bool], Dict[str, Any]]:
        """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì˜ì¡´ì„± ê²€ì¦"""
        try:
            if format_type == DependencyValidationFormat.BOOLEAN_DICT:
                return {
                    'model_loader': self.dependency_status.model_loader,
                    'step_interface': self.dependency_status.step_interface,
                    'memory_manager': self.dependency_status.memory_manager,
                    'data_converter': self.dependency_status.data_converter
                }
            else:
                return {
                    'success': self.dependency_status.model_loader,
                    'details': {
                        'model_loader': self.dependency_status.model_loader,
                        'github_compatible': self.dependency_status.github_compatible,
                        'detailed_data_spec_ready': self.dependency_status.detailed_data_spec_loaded
                    }
                }
        except Exception as e:
            self.logger.error(f"âŒ GitHub ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'model_loader': False} if format_type == DependencyValidationFormat.BOOLEAN_DICT else {'success': False}

# ==============================================
# ðŸ”¥ BaseStepMixin v19.1 - DetailedDataSpec ì™„ì „ í†µí•©
# ==============================================

class BaseStepMixin:
    """
    ðŸ”¥ BaseStepMixin v19.1 - DetailedDataSpec ì™„ì „ í†µí•©
    
    í•µì‹¬ ê°œì„ ì‚¬í•­:
    âœ… DetailedDataSpec ì •ë³´ ì €ìž¥ ë° ê´€ë¦¬
    âœ… í‘œì¤€í™”ëœ process ë©”ì„œë“œ ìž¬ì„¤ê³„
    âœ… API â†” AI ëª¨ë¸ ê°„ ë°ì´í„° ë³€í™˜ í‘œì¤€í™”
    âœ… Step ê°„ ë°ì´í„° íë¦„ ìžë™ ì²˜ë¦¬
    âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ìžë™ ì ìš©
    âœ… GitHub í”„ë¡œì íŠ¸ Step í´ëž˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜
    """
    
    def __init__(self, **kwargs):
        """DetailedDataSpec ì™„ì „ í†µí•© ì´ˆê¸°í™” (v19.1)"""
        try:
            # ê¸°ë³¸ ì„¤ì •
            self.config = self._create_github_config(**kwargs)
            self.step_name = kwargs.get('step_name', self.__class__.__name__)
            self.step_id = kwargs.get('step_id', 0)
            
            # Logger ì„¤ì •
            self.logger = logging.getLogger(f"steps.{self.step_name}")
            if not self.logger.handlers:
                handler = logging.StreamHandler()
                formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
                handler.setFormatter(formatter)
                self.logger.addHandler(handler)
                self.logger.setLevel(logging.INFO)
            
            # ðŸ”¥ DetailedDataSpec ì •ë³´ ì €ìž¥ (StepFactoryì—ì„œ ì£¼ìž…ë°›ìŒ)
            self.detailed_data_spec = self._load_detailed_data_spec_from_kwargs(**kwargs)
            
            # ðŸ”¥ GitHub í˜¸í™˜ ì˜ì¡´ì„± ê´€ë¦¬ìž
            self.dependency_manager = GitHubDependencyManager(self.step_name)
            
            # GitHub í‘œì¤€ ìƒíƒœ í”Œëž˜ê·¸ë“¤
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # ì‹œìŠ¤í…œ ì •ë³´
            self.device = self._resolve_device(self.config.device)
            self.is_m3_max = IS_M3_MAX
            self.memory_gb = MEMORY_GB
            self.conda_info = CONDA_INFO
            
            # GitHub í˜¸í™˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­
            self.performance_metrics = GitHubPerformanceMetrics()
            
            # GitHub í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±ë“¤
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
            
            # GitHub íŠ¹ë³„ ì†ì„±ë“¤
            self.github_compatible = True
            self.real_ai_pipeline_ready = False
            self.process_method_signature = self.config.process_method_signature
            
            # DetailedDataSpec ìƒíƒœ (v19.1 ì‹ ê·œ)
            self.data_conversion_ready = self._validate_data_conversion_readiness()
            
            # í™˜ê²½ ìµœì í™” ì ìš©
            self._apply_github_environment_optimization()
            
            # ìžë™ ì˜ì¡´ì„± ì£¼ìž… (ì„¤ì •ëœ ê²½ìš°)
            if self.config.auto_inject_dependencies:
                try:
                    self.dependency_manager.auto_inject_dependencies()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {self.step_name} ìžë™ ì˜ì¡´ì„± ì£¼ìž… ì‹¤íŒ¨: {e}")
            
            self.logger.info(f"âœ… {self.step_name} BaseStepMixin v19.1 DetailedDataSpec í†µí•© ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self._github_emergency_setup(e)
    
    def _load_detailed_data_spec_from_kwargs(self, **kwargs) -> DetailedDataSpecConfig:
        """StepFactoryì—ì„œ ì£¼ìž…ë°›ì€ DetailedDataSpec ì •ë³´ ë¡œë”©"""
        return DetailedDataSpecConfig(
            # ìž…ë ¥ ì‚¬ì–‘
            input_data_types=kwargs.get('input_data_types', []),
            input_shapes=kwargs.get('input_shapes', {}),
            input_value_ranges=kwargs.get('input_value_ranges', {}),
            preprocessing_required=kwargs.get('preprocessing_required', []),
            
            # ì¶œë ¥ ì‚¬ì–‘
            output_data_types=kwargs.get('output_data_types', []),
            output_shapes=kwargs.get('output_shapes', {}),
            output_value_ranges=kwargs.get('output_value_ranges', {}),
            postprocessing_required=kwargs.get('postprocessing_required', []),
            
            # API í˜¸í™˜ì„±
            api_input_mapping=kwargs.get('api_input_mapping', {}),
            api_output_mapping=kwargs.get('api_output_mapping', {}),
            
            # Step ê°„ ì—°ë™
            step_input_schema=kwargs.get('step_input_schema', {}),
            step_output_schema=kwargs.get('step_output_schema', {}),
            
            # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­
            normalization_mean=kwargs.get('normalization_mean', (0.485, 0.456, 0.406)),
            normalization_std=kwargs.get('normalization_std', (0.229, 0.224, 0.225)),
            preprocessing_steps=kwargs.get('preprocessing_steps', []),
            postprocessing_steps=kwargs.get('postprocessing_steps', []),
            
            # Step ê°„ ë°ì´í„° ì „ë‹¬ ìŠ¤í‚¤ë§ˆ
            accepts_from_previous_step=kwargs.get('accepts_from_previous_step', {}),
            provides_to_next_step=kwargs.get('provides_to_next_step', {})
        )
    
    def _validate_data_conversion_readiness(self) -> bool:
        """ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ìƒíƒœ ê²€ì¦"""
        try:
            # ìµœì†Œ ìš”êµ¬ì‚¬í•­ í™•ì¸
            has_api_mapping = bool(self.detailed_data_spec.api_input_mapping and 
                                 self.detailed_data_spec.api_output_mapping)
            
            has_preprocessing = bool(self.detailed_data_spec.preprocessing_steps)
            has_postprocessing = bool(self.detailed_data_spec.postprocessing_steps)
            
            # ë°ì´í„° íƒ€ìž… ì •ë³´ í™•ì¸
            has_input_types = bool(self.detailed_data_spec.input_data_types)
            has_output_types = bool(self.detailed_data_spec.output_data_types)
            
            readiness = has_api_mapping and has_input_types and has_output_types
            
            if readiness:
                self.dependency_manager.dependency_status.detailed_data_spec_loaded = True
                self.dependency_manager.dependency_status.data_conversion_ready = True
                self.logger.debug(f"âœ… {self.step_name} DetailedDataSpec ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ì™„ë£Œ")
            else:
                self.logger.warning(f"âš ï¸ {self.step_name} DetailedDataSpec ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ë¯¸ì™„ë£Œ")
            
            return readiness
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ìƒíƒœ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ðŸ”¥ í‘œì¤€í™”ëœ process ë©”ì„œë“œ (v19.1 í•µì‹¬)
    # ==============================================
    
    async def process(self, **kwargs) -> Dict[str, Any]:
        """
        ðŸ”¥ ì™„ì „ížˆ ìž¬ì„¤ê³„ëœ í‘œì¤€í™” process ë©”ì„œë“œ (v19.1)
        
        ëª¨ë“  ë°ì´í„° ë³€í™˜ì„ BaseStepMixinì—ì„œ í‘œì¤€í™” ì²˜ë¦¬í•˜ê³ ,
        ì‹¤ì œ Step í´ëž˜ìŠ¤ë“¤ì€ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„í•˜ë©´ ë¨
        """
        try:
            start_time = time.time()
            self.performance_metrics.github_process_calls += 1
            
            self.logger.debug(f"ðŸ”„ {self.step_name} process ì‹œìž‘ (ìž…ë ¥: {list(kwargs.keys())})")
            
            # 1. ìž…ë ¥ ë°ì´í„° ë³€í™˜ (API/Step ê°„ â†’ AI ëª¨ë¸)
            converted_input = await self._convert_input_to_model_format(kwargs)
            
            # 2. í•˜ìœ„ í´ëž˜ìŠ¤ì˜ ìˆœìˆ˜ AI ë¡œì§ ì‹¤í–‰
            ai_result = await self._run_ai_inference(converted_input)
            
            # 3. ì¶œë ¥ ë°ì´í„° ë³€í™˜ (AI ëª¨ë¸ â†’ API + Step ê°„)
            standardized_output = await self._convert_output_to_standard_format(ai_result)
            
            # 4. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self._update_performance_metrics(processing_time, True)
            
            self.logger.debug(f"âœ… {self.step_name} process ì™„ë£Œ ({processing_time:.3f}ì´ˆ)")
            
            return standardized_output
            
        except Exception as e:
            processing_time = time.time() - start_time
            self._update_performance_metrics(processing_time, False)
            self.logger.error(f"âŒ {self.step_name} process ì‹¤íŒ¨ ({processing_time:.3f}ì´ˆ): {e}")
            return self._create_error_response(str(e))
    
    @abstractmethod
    async def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        ðŸ”¥ í•˜ìœ„ í´ëž˜ìŠ¤ì—ì„œ êµ¬í˜„í•  ìˆœìˆ˜ AI ë¡œì§
        
        Args:
            processed_input: BaseStepMixinì—ì„œ ë³€í™˜ëœ í‘œì¤€ AI ëª¨ë¸ ìž…ë ¥
        
        Returns:
            AI ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ ê²°ê³¼
        """
        pass
    
    # ==============================================
    # ðŸ”¥ ìž…ë ¥ ë°ì´í„° ë³€í™˜ ì‹œìŠ¤í…œ (v19.1)
    # ==============================================
    
    async def _convert_input_to_model_format(self, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """ðŸ”¥ API/Step ê°„ ë°ì´í„° â†’ AI ëª¨ë¸ ìž…ë ¥ í˜•ì‹ ë³€í™˜"""
        try:
            converted = {}
            self.performance_metrics.data_conversions += 1
            
            self.logger.debug(f"ðŸ”„ {self.step_name} ìž…ë ¥ ë°ì´í„° ë³€í™˜ ì‹œìž‘...")
            
            # 1. API ìž…ë ¥ ë§¤í•‘ ì²˜ë¦¬ (UploadFile â†’ PIL.Image ë“±)
            for model_param, api_type in self.detailed_data_spec.api_input_mapping.items():
                if model_param in kwargs:
                    converted[model_param] = await self._convert_api_input_type(
                        kwargs[model_param], api_type, model_param
                    )
                    self.performance_metrics.api_conversions += 1
            
            # 2. Step ê°„ ë°ì´í„° ì²˜ë¦¬ (ì´ì „ Step ê²°ê³¼ í™œìš©)
            for step_name, step_data in kwargs.items():
                if step_name.startswith('from_step_'):
                    step_id = step_name.replace('from_step_', '')
                    if step_id in self.detailed_data_spec.accepts_from_previous_step:
                        step_schema = self.detailed_data_spec.accepts_from_previous_step[step_id]
                        converted.update(self._map_step_input_data(step_data, step_schema))
                        self.performance_metrics.step_data_transfers += 1
            
            # 3. ëˆ„ë½ëœ í•„ìˆ˜ ìž…ë ¥ ë°ì´í„° í™•ì¸
            for param_name in self.detailed_data_spec.api_input_mapping.keys():
                if param_name not in converted and param_name in kwargs:
                    # ì§ì ‘ ë§¤í•‘ ì‹œë„
                    converted[param_name] = kwargs[param_name]
            
            # 4. ì „ì²˜ë¦¬ ì ìš©
            if self.config.auto_preprocessing and self.detailed_data_spec.preprocessing_steps:
                converted = await self._apply_preprocessing(converted)
                self.performance_metrics.preprocessing_operations += 1
            
            # 5. ë°ì´í„° íƒ€ìž… ë° í˜•íƒœ ê²€ì¦
            if self.config.strict_data_validation:
                validated_input = self._validate_input_data(converted)
            else:
                validated_input = converted
            
            self.logger.debug(f"âœ… {self.step_name} ìž…ë ¥ ë°ì´í„° ë³€í™˜ ì™„ë£Œ (ê²°ê³¼: {list(validated_input.keys())})")
            
            return validated_input
            
        except Exception as e:
            self.performance_metrics.validation_failures += 1
            self.logger.error(f"âŒ {self.step_name} ìž…ë ¥ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    async def _convert_api_input_type(self, value: Any, api_type: str, param_name: str) -> Any:
        """API íƒ€ìž…ë³„ ë³€í™˜ ì²˜ë¦¬"""
        try:
            if api_type == "UploadFile":
                if hasattr(value, 'file'):
                    # FastAPI UploadFile
                    content = await value.read() if hasattr(value, 'read') else value.file.read()
                    return Image.open(BytesIO(content)) if PIL_AVAILABLE else content
                elif hasattr(value, 'read'):
                    # íŒŒì¼ ê°ì²´
                    content = value.read()
                    return Image.open(BytesIO(content)) if PIL_AVAILABLE else content
                
            elif api_type == "base64_string":
                if isinstance(value, str):
                    try:
                        image_data = base64.b64decode(value)
                        return Image.open(BytesIO(image_data)) if PIL_AVAILABLE else image_data
                    except Exception:
                        return value
                        
            elif api_type in ["str", "Optional[str]"]:
                return str(value) if value is not None else None
                
            elif api_type in ["int", "Optional[int]"]:
                return int(value) if value is not None else None
                
            elif api_type in ["float", "Optional[float]"]:
                return float(value) if value is not None else None
                
            elif api_type in ["List[float]", "List[int]"]:
                if isinstance(value, (list, tuple)):
                    return [float(x) if "float" in api_type else int(x) for x in value]
                    
            # ê¸°ë³¸ê°’: ì›ë³¸ ë°˜í™˜
            return value
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ {self.step_name} API íƒ€ìž… ë³€í™˜ ì‹¤íŒ¨ ({param_name}: {api_type}): {e}")
            return value
    
    def _map_step_input_data(self, step_data: Dict[str, Any], step_schema: Dict[str, str]) -> Dict[str, Any]:
        """Step ê°„ ë°ì´í„° ë§¤í•‘"""
        mapped_data = {}
        
        for data_key, data_type in step_schema.items():
            if data_key in step_data:
                value = step_data[data_key]
                
                # ë°ì´í„° íƒ€ìž…ì— ë§žê²Œ ë³€í™˜
                if data_type == "np.ndarray" and NUMPY_AVAILABLE:
                    if TORCH_AVAILABLE and torch.is_tensor(value):
                        mapped_data[data_key] = value.cpu().numpy()
                    else:
                        mapped_data[data_key] = np.array(value) if not isinstance(value, np.ndarray) else value
                        
                elif data_type == "torch.Tensor" and TORCH_AVAILABLE:
                    if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                        mapped_data[data_key] = torch.from_numpy(value)
                    else:
                        mapped_data[data_key] = value
                        
                elif data_type == "PIL.Image" and PIL_AVAILABLE:
                    if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                        mapped_data[data_key] = Image.fromarray(value.astype(np.uint8))
                    else:
                        mapped_data[data_key] = value
                        
                else:
                    mapped_data[data_key] = value
        
        return mapped_data
    
    # ==============================================
    # ðŸ”¥ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ (v19.1)
    # ==============================================
    
    async def _apply_preprocessing(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ðŸ”¥ DetailedDataSpec ê¸°ë°˜ ì „ì²˜ë¦¬ ìžë™ ì ìš©"""
        try:
            processed = input_data.copy()
            
            self.logger.debug(f"ðŸ”„ {self.step_name} ì „ì²˜ë¦¬ ì ìš©: {self.detailed_data_spec.preprocessing_steps}")
            
            for step_name in self.detailed_data_spec.preprocessing_steps:
                if step_name == "resize_512x512":
                    processed = self._resize_images(processed, (512, 512))
                elif step_name == "resize_768x1024":
                    processed = self._resize_images(processed, (768, 1024))
                elif step_name == "resize_256x192":
                    processed = self._resize_images(processed, (256, 192))
                elif step_name == "resize_224x224":
                    processed = self._resize_images(processed, (224, 224))
                elif step_name == "resize_368x368":
                    processed = self._resize_images(processed, (368, 368))
                elif step_name == "resize_1024x1024":
                    processed = self._resize_images(processed, (1024, 1024))
                    
                elif step_name == "normalize_imagenet":
                    processed = self._normalize_imagenet(processed)
                elif step_name == "normalize_clip":
                    processed = self._normalize_clip(processed)
                elif step_name == "normalize_diffusion" or step_name == "normalize_centered":
                    processed = self._normalize_diffusion(processed)
                    
                elif step_name == "to_tensor":
                    processed = self._convert_to_tensor(processed)
                    
                elif step_name == "prepare_sam_prompts":
                    processed = self._prepare_sam_prompts(processed)
                elif step_name == "prepare_diffusion_input":
                    processed = self._prepare_diffusion_input(processed)
                elif step_name == "prepare_ootd_inputs":
                    processed = self._prepare_ootd_inputs(processed)
                elif step_name == "extract_pose_features":
                    processed = self._extract_pose_features(processed)
                elif step_name == "prepare_sr_input":
                    processed = self._prepare_sr_input(processed)
                    
                else:
                    self.logger.debug(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì „ì²˜ë¦¬ ë‹¨ê³„: {step_name}")
            
            self.logger.debug(f"âœ… {self.step_name} ì „ì²˜ë¦¬ ì™„ë£Œ")
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return input_data
    
    def _resize_images(self, data: Dict[str, Any], target_size: Tuple[int, int]) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ì²˜ë¦¬"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if PIL_AVAILABLE and isinstance(value, Image.Image):
                    result[key] = value.resize(target_size, Image.LANCZOS)
                    
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray) and len(value.shape) >= 2:
                    if CV2_AVAILABLE:
                        if len(value.shape) == 3:
                            result[key] = cv2.resize(value, target_size)
                        elif len(value.shape) == 2:
                            result[key] = cv2.resize(value, target_size)
                    else:
                        # PIL í´ë°±
                        if PIL_AVAILABLE:
                            if len(value.shape) == 3:
                                img = Image.fromarray(value.astype(np.uint8))
                                result[key] = np.array(img.resize(target_size, Image.LANCZOS))
                            elif len(value.shape) == 2:
                                img = Image.fromarray(value.astype(np.uint8), mode='L')
                                result[key] = np.array(img.resize(target_size, Image.LANCZOS))
                                
            except Exception as e:
                self.logger.debug(f"ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _normalize_imagenet(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ImageNet ì •ê·œí™” (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"""
        result = data.copy()
        mean = np.array(self.detailed_data_spec.normalization_mean)
        std = np.array(self.detailed_data_spec.normalization_std)
        
        for key, value in data.items():
            try:
                if PIL_AVAILABLE and isinstance(value, Image.Image):
                    # PIL Image â†’ NumPy
                    array = np.array(value).astype(np.float32) / 255.0
                    if len(array.shape) == 3 and array.shape[2] == 3:
                        normalized = (array - mean) / std
                        result[key] = normalized
                        
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if value.dtype != np.float32:
                        value = value.astype(np.float32)
                    
                    if value.max() > 1.0:
                        value = value / 255.0
                    
                    if len(value.shape) == 3 and value.shape[2] == 3:
                        normalized = (value - mean) / std
                        result[key] = normalized
                        
            except Exception as e:
                self.logger.debug(f"ImageNet ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _normalize_clip(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """CLIP ì •ê·œí™” (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])"""
        result = data.copy()
        clip_mean = np.array([0.48145466, 0.4578275, 0.40821073])
        clip_std = np.array([0.26862954, 0.26130258, 0.27577711])
        
        for key, value in data.items():
            try:
                if PIL_AVAILABLE and isinstance(value, Image.Image):
                    array = np.array(value).astype(np.float32) / 255.0
                    if len(array.shape) == 3 and array.shape[2] == 3:
                        normalized = (array - clip_mean) / clip_std
                        result[key] = normalized
                        
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if value.dtype != np.float32:
                        value = value.astype(np.float32)
                    
                    if value.max() > 1.0:
                        value = value / 255.0
                    
                    if len(value.shape) == 3 and value.shape[2] == 3:
                        normalized = (value - clip_mean) / clip_std
                        result[key] = normalized
                        
            except Exception as e:
                self.logger.debug(f"CLIP ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _normalize_diffusion(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Diffusion ì •ê·œí™” (mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) â†’ [-1, 1] ë²”ìœ„"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if PIL_AVAILABLE and isinstance(value, Image.Image):
                    array = np.array(value).astype(np.float32) / 255.0
                    normalized = 2.0 * array - 1.0  # [0, 1] â†’ [-1, 1]
                    result[key] = normalized
                    
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if value.dtype != np.float32:
                        value = value.astype(np.float32)
                    
                    if value.max() > 1.0:
                        value = value / 255.0
                    
                    normalized = 2.0 * value - 1.0  # [0, 1] â†’ [-1, 1]
                    result[key] = normalized
                    
            except Exception as e:
                self.logger.debug(f"Diffusion ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _convert_to_tensor(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """PyTorch í…ì„œ ë³€í™˜"""
        if not TORCH_AVAILABLE:
            return data
        
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    # HWC â†’ CHW ë³€í™˜ (ì´ë¯¸ì§€ì¸ ê²½ìš°)
                    if len(value.shape) == 3 and value.shape[2] in [1, 3, 4]:
                        value = np.transpose(value, (2, 0, 1))
                    result[key] = torch.from_numpy(value).float()
                    
                elif PIL_AVAILABLE and isinstance(value, Image.Image):
                    array = np.array(value)
                    if len(array.shape) == 3:
                        array = np.transpose(array, (2, 0, 1))
                    result[key] = torch.from_numpy(array).float()
                    
                elif isinstance(value, (list, tuple)):
                    result[key] = torch.tensor(value).float()
                    
            except Exception as e:
                self.logger.debug(f"í…ì„œ ë³€í™˜ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _prepare_sam_prompts(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """SAM í”„ë¡¬í”„íŠ¸ ì¤€ë¹„"""
        result = data.copy()
        
        # SAM ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ í¬ì¸íŠ¸ ë° ë¼ë²¨ ì¤€ë¹„
        if 'prompt_points' not in result and 'image' in result:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í¬ì¸íŠ¸ (ì´ë¯¸ì§€ ì¤‘ì•™)
            if PIL_AVAILABLE and isinstance(result['image'], Image.Image):
                w, h = result['image'].size
                result['prompt_points'] = np.array([[w//2, h//2]])
                result['prompt_labels'] = np.array([1])
            elif NUMPY_AVAILABLE and isinstance(result['image'], np.ndarray):
                if len(result['image'].shape) >= 2:
                    h, w = result['image'].shape[:2]
                    result['prompt_points'] = np.array([[w//2, h//2]])
                    result['prompt_labels'] = np.array([1])
        
        return result
    
    def _prepare_diffusion_input(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Diffusion ëª¨ë¸ ìž…ë ¥ ì¤€ë¹„"""
        result = data.copy()
        
        # Diffusion ëª¨ë¸ìš© ì¡°ê±´ ì¤€ë¹„
        if 'guidance_scale' not in result:
            result['guidance_scale'] = 7.5
        
        if 'num_inference_steps' not in result:
            result['num_inference_steps'] = 20
        
        if 'strength' not in result:
            result['strength'] = 0.8
        
        return result
    
    def _prepare_ootd_inputs(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """OOTD Diffusion ìž…ë ¥ ì¤€ë¹„"""
        result = data.copy()
        
        # OOTD íŠ¹ë³„ ì„¤ì •
        if 'fitting_mode' not in result:
            result['fitting_mode'] = 'hd'  # 'hd' or 'dc'
        
        return result
    
    def _extract_pose_features(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í¬ì¦ˆ íŠ¹ì§• ì¶”ì¶œ"""
        # í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ì „ì²˜ë¦¬
        return data
    
    def _prepare_sr_input(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Super Resolution ìž…ë ¥ ì¤€ë¹„"""
        result = data.copy()
        
        # íƒ€ì¼ë§ ì •ë³´ ì¤€ë¹„
        if 'tile_size' not in result:
            result['tile_size'] = 512
        
        if 'overlap' not in result:
            result['overlap'] = 64
        
        return result
    
    # ==============================================
    # ðŸ”¥ ì¶œë ¥ ë°ì´í„° ë³€í™˜ ì‹œìŠ¤í…œ (v19.1)
    # ==============================================
    
    async def _convert_output_to_standard_format(self, ai_result: Dict[str, Any]) -> Dict[str, Any]:
        """ðŸ”¥ AI ëª¨ë¸ ì¶œë ¥ â†’ í‘œì¤€ í˜•ì‹ (API + Step ê°„) ë³€í™˜"""
        try:
            self.logger.debug(f"ðŸ”„ {self.step_name} ì¶œë ¥ ë°ì´í„° ë³€í™˜ ì‹œìž‘...")
            
            # 1. í›„ì²˜ë¦¬ ì ìš©
            processed_result = await self._apply_postprocessing(ai_result)
            
            # 2. API ì‘ë‹µ í˜•ì‹ ë³€í™˜
            api_response = self._convert_to_api_format(processed_result)
            
            # 3. ë‹¤ìŒ Stepë“¤ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
            next_step_data = self._prepare_next_step_data(processed_result)
            
            # 4. í‘œì¤€ ì‘ë‹µ êµ¬ì¡° ìƒì„±
            standard_response = {
                'success': True,
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': getattr(self, '_last_processing_time', 0.0),
                
                # API ì‘ë‹µ ë°ì´í„°
                **api_response,
                
                # ë‹¤ìŒ Stepë“¤ì„ ìœ„í•œ ë°ì´í„°
                'next_step_data': next_step_data,
                
                # ë©”íƒ€ë°ì´í„°
                'metadata': {
                    'input_shapes': {k: self._get_shape_info(v) for k, v in ai_result.items()},
                    'output_shapes': self.detailed_data_spec.output_shapes,
                    'device': self.device,
                    'github_compatible': True,
                    'detailed_data_spec_applied': True,
                    'data_conversion_version': 'v19.1'
                }
            }
            
            self.logger.debug(f"âœ… {self.step_name} ì¶œë ¥ ë°ì´í„° ë³€í™˜ ì™„ë£Œ")
            return standard_response
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì¶œë ¥ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
            return self._create_error_response(str(e))
    
    def _get_shape_info(self, value: Any) -> Optional[Tuple]:
        """ê°’ì˜ í˜•íƒœ ì •ë³´ ì¶”ì¶œ"""
        try:
            if hasattr(value, 'shape'):
                return tuple(value.shape)
            elif isinstance(value, (list, tuple)):
                return (len(value),)
            else:
                return None
        except:
            return None
    
    async def _apply_postprocessing(self, ai_result: Dict[str, Any]) -> Dict[str, Any]:
        """ðŸ”¥ DetailedDataSpec ê¸°ë°˜ í›„ì²˜ë¦¬ ìžë™ ì ìš©"""
        try:
            if not self.config.auto_postprocessing:
                return ai_result
            
            processed = ai_result.copy()
            
            self.logger.debug(f"ðŸ”„ {self.step_name} í›„ì²˜ë¦¬ ì ìš©: {self.detailed_data_spec.postprocessing_steps}")
            
            for step_name in self.detailed_data_spec.postprocessing_steps:
                if step_name == "softmax":
                    processed = self._apply_softmax(processed)
                elif step_name == "argmax":
                    processed = self._apply_argmax(processed)
                elif step_name == "resize_original":
                    processed = self._resize_to_original(processed)
                elif step_name == "to_numpy":
                    processed = self._convert_to_numpy(processed)
                elif step_name == "threshold_0.5":
                    processed = self._apply_threshold(processed, 0.5)
                elif step_name == "nms":
                    processed = self._apply_nms(processed)
                elif step_name == "denormalize_diffusion" or step_name == "denormalize_centered":
                    processed = self._denormalize_diffusion(processed)
                elif step_name == "denormalize":
                    processed = self._denormalize_imagenet(processed)
                elif step_name == "clip_values" or step_name == "clip_0_1":
                    processed = self._clip_values(processed, 0.0, 1.0)
                elif step_name == "apply_mask" or step_name == "apply_warping_mask":
                    processed = self._apply_mask(processed)
                elif step_name == "morphology_clean":
                    processed = self._morphology_operations(processed)
                elif step_name == "extract_keypoints":
                    processed = self._extract_keypoints(processed)
                elif step_name == "scale_coords":
                    processed = self._scale_coordinates(processed)
                elif step_name == "filter_confidence":
                    processed = self._filter_by_confidence(processed)
                elif step_name == "enhance_details":
                    processed = self._enhance_details(processed)
                elif step_name == "final_compositing":
                    processed = self._final_compositing(processed)
                elif step_name == "generate_quality_report":
                    processed = self._generate_quality_report(processed)
                else:
                    self.logger.debug(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” í›„ì²˜ë¦¬ ë‹¨ê³„: {step_name}")
            
            self.performance_metrics.postprocessing_operations += 1
            self.logger.debug(f"âœ… {self.step_name} í›„ì²˜ë¦¬ ì™„ë£Œ")
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ai_result
    
    def _apply_softmax(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Softmax ì ìš©"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = torch.softmax(value, dim=-1)
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    exp_vals = np.exp(value - np.max(value, axis=-1, keepdims=True))
                    result[key] = exp_vals / np.sum(exp_vals, axis=-1, keepdims=True)
            except Exception as e:
                self.logger.debug(f"Softmax ì ìš© ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _apply_argmax(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Argmax ì ìš©"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = torch.argmax(value, dim=-1)
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = np.argmax(value, axis=-1)
            except Exception as e:
                self.logger.debug(f"Argmax ì ìš© ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _convert_to_numpy(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """NumPy ë³€í™˜"""
        if not NUMPY_AVAILABLE:
            return data
        
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = value.detach().cpu().numpy()
                elif not isinstance(value, np.ndarray):
                    if isinstance(value, (list, tuple)):
                        result[key] = np.array(value)
            except Exception as e:
                self.logger.debug(f"NumPy ë³€í™˜ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _apply_threshold(self, data: Dict[str, Any], threshold: float) -> Dict[str, Any]:
        """ìž„ê³„ê°’ ì ìš©"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = (value > threshold).astype(np.float32)
                elif TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = (value > threshold).float()
            except Exception as e:
                self.logger.debug(f"ìž„ê³„ê°’ ì ìš© ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _denormalize_diffusion(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Diffusion ì—­ì •ê·œí™” ([-1, 1] â†’ [0, 1])"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = (value + 1.0) / 2.0
                elif TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = (value + 1.0) / 2.0
            except Exception as e:
                self.logger.debug(f"Diffusion ì—­ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _denormalize_imagenet(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ImageNet ì—­ì •ê·œí™”"""
        result = data.copy()
        mean = np.array(self.detailed_data_spec.normalization_mean)
        std = np.array(self.detailed_data_spec.normalization_std)
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if len(value.shape) == 3 and value.shape[2] == 3:
                        denormalized = value * std + mean
                        result[key] = np.clip(denormalized, 0, 1)
            except Exception as e:
                self.logger.debug(f"ImageNet ì—­ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _clip_values(self, data: Dict[str, Any], min_val: float, max_val: float) -> Dict[str, Any]:
        """ê°’ ë²”ìœ„ í´ë¦¬í•‘"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = np.clip(value, min_val, max_val)
                elif TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = torch.clamp(value, min_val, max_val)
            except Exception as e:
                self.logger.debug(f"ê°’ í´ë¦¬í•‘ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _apply_mask(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë§ˆìŠ¤í¬ ì ìš©"""
        result = data.copy()
        
        # ë§ˆìŠ¤í¬ê°€ ìžˆëŠ” ê²½ìš° ì ìš©
        if 'mask' in data and 'image' in data:
            try:
                mask = data['mask']
                image = data['image']
                
                if NUMPY_AVAILABLE:
                    if isinstance(mask, np.ndarray) and isinstance(image, np.ndarray):
                        result['masked_image'] = image * mask
                        
            except Exception as e:
                self.logger.debug(f"ë§ˆìŠ¤í¬ ì ìš© ì‹¤íŒ¨: {e}")
        
        return result
    
    def _morphology_operations(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í˜•íƒœí•™ì  ì—°ì‚° (ë…¸ì´ì¦ˆ ì œê±°)"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if CV2_AVAILABLE and NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if len(value.shape) == 2:  # 2D ë§ˆìŠ¤í¬
                        # ì—´ê¸°ì™€ ë‹«ê¸° ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
                        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                        opened = cv2.morphologyEx(value.astype(np.uint8), cv2.MORPH_OPEN, kernel)
                        closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)
                        result[key] = closed.astype(np.float32)
                        
            except Exception as e:
                self.logger.debug(f"í˜•íƒœí•™ì  ì—°ì‚° ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _extract_keypoints(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        result = data.copy()
        
        # OpenPose ìŠ¤íƒ€ì¼ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        if 'heatmaps' in data:
            try:
                heatmaps = data['heatmaps']
                if NUMPY_AVAILABLE and isinstance(heatmaps, np.ndarray):
                    keypoints = []
                    for i in range(heatmaps.shape[0]):  # ê° í‚¤í¬ì¸íŠ¸ë³„
                        heatmap = heatmaps[i]
                        y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
                        confidence = heatmap[y, x]
                        keypoints.append([x, y, confidence])
                    result['keypoints'] = np.array(keypoints)
                    
            except Exception as e:
                self.logger.debug(f"í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return result
    
    def _scale_coordinates(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§"""
        result = data.copy()
        
        # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§
        if 'keypoints' in data and 'original_size' in data:
            try:
                keypoints = data['keypoints']
                original_size = data['original_size']
                
                if isinstance(keypoints, np.ndarray) and len(keypoints.shape) == 2:
                    # í˜„ìž¬ í¬ê¸°ì—ì„œ ì›ë³¸ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§
                    scale_x = original_size[0] / self.detailed_data_spec.input_shapes.get('image', (512, 512))[1]
                    scale_y = original_size[1] / self.detailed_data_spec.input_shapes.get('image', (512, 512))[0]
                    
                    scaled_keypoints = keypoints.copy()
                    scaled_keypoints[:, 0] *= scale_x
                    scaled_keypoints[:, 1] *= scale_y
                    result['scaled_keypoints'] = scaled_keypoints
                    
            except Exception as e:
                self.logger.debug(f"ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨: {e}")
        
        return result
    
    def _filter_by_confidence(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§"""
        result = data.copy()
        
        confidence_threshold = self.config.confidence_threshold
        
        for key, value in data.items():
            try:
                if key.endswith('_confidence') or key.endswith('_scores'):
                    if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                        valid_mask = value > confidence_threshold
                        result[f'{key}_filtered'] = value[valid_mask]
                        
                        # í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë„ í•„í„°ë§
                        base_key = key.replace('_confidence', '').replace('_scores', '')
                        if base_key in data:
                            base_data = data[base_key]
                            if isinstance(base_data, np.ndarray) and len(base_data) == len(value):
                                result[f'{base_key}_filtered'] = base_data[valid_mask]
                                
            except Exception as e:
                self.logger.debug(f"ì‹ ë¢°ë„ í•„í„°ë§ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _enhance_details(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì„¸ë¶€ì‚¬í•­ í–¥ìƒ (Super Resolution í›„ì²˜ë¦¬)"""
        result = data.copy()
        
        # ê°„ë‹¨í•œ ìƒ¤í”„ë‹ í•„í„° ì ìš©
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray) and len(value.shape) >= 2:
                    if CV2_AVAILABLE and len(value.shape) == 3:
                        # ì–¸ìƒ¤í”„ ë§ˆìŠ¤í‚¹
                        blurred = cv2.GaussianBlur(value, (3, 3), 1.0)
                        sharpened = cv2.addWeighted(value, 1.5, blurred, -0.5, 0)
                        result[f'{key}_enhanced'] = np.clip(sharpened, 0, 1)
                        
            except Exception as e:
                self.logger.debug(f"ì„¸ë¶€ì‚¬í•­ í–¥ìƒ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _final_compositing(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ìµœì¢… í•©ì„±"""
        result = data.copy()
        
        # ì—¬ëŸ¬ ë ˆì´ì–´ê°€ ìžˆëŠ” ê²½ìš° í•©ì„±
        if 'person_image' in data and 'clothing_image' in data and 'mask' in data:
            try:
                person = data['person_image']
                clothing = data['clothing_image']
                mask = data['mask']
                
                if all(isinstance(x, np.ndarray) for x in [person, clothing, mask]):
                    # ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•œ ë¸”ë Œë”©
                    composited = person * (1 - mask) + clothing * mask
                    result['final_composited'] = composited
                    
            except Exception as e:
                self.logger.debug(f"ìµœì¢… í•©ì„± ì‹¤íŒ¨: {e}")
        
        return result
    
    def _generate_quality_report(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        result = data.copy()
        
        quality_metrics = {
            'overall_quality': 0.0,
            'detail_preservation': 0.0,
            'color_consistency': 0.0,
            'artifact_level': 0.0,
            'recommendations': []
        }
        
        try:
            # ê°„ë‹¨í•œ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            if 'final_result' in data:
                final_result = data['final_result']
                if NUMPY_AVAILABLE and isinstance(final_result, np.ndarray):
                    # ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜
                    mean_intensity = np.mean(final_result)
                    std_intensity = np.std(final_result)
                    
                    # ì •ê·œí™”ëœ ì ìˆ˜
                    quality_metrics['overall_quality'] = min(1.0, (mean_intensity + std_intensity) / 2.0)
                    quality_metrics['detail_preservation'] = min(1.0, std_intensity * 2.0)
                    quality_metrics['color_consistency'] = 1.0 - abs(0.5 - mean_intensity)
                    
                    # ê¶Œìž¥ì‚¬í•­
                    if quality_metrics['overall_quality'] < 0.7:
                        quality_metrics['recommendations'].append('ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„  í•„ìš”')
                    if quality_metrics['detail_preservation'] < 0.5:
                        quality_metrics['recommendations'].append('ì„¸ë¶€ì‚¬í•­ ë³´ì¡´ ê°œì„  í•„ìš”')
            
            result['quality_assessment'] = quality_metrics
            
        except Exception as e:
            self.logger.debug(f"í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            result['quality_assessment'] = quality_metrics
        
        return result
    
    def _apply_nms(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Non-Maximum Suppression ì ìš©"""
        result = data.copy()
        
        # ê²€ì¶œ ê²°ê³¼ê°€ ìžˆëŠ” ê²½ìš° NMS ì ìš©
        if 'detections' in data and 'scores' in data:
            try:
                # ê°„ë‹¨í•œ NMS êµ¬í˜„ (ì‹¤ì œë¡œëŠ” ë” ë³µìž¡í•œ ì•Œê³ ë¦¬ì¦˜ í•„ìš”)
                detections = data['detections']
                scores = data['scores']
                
                if NUMPY_AVAILABLE and isinstance(detections, np.ndarray) and isinstance(scores, np.ndarray):
                    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
                    sorted_indices = np.argsort(scores)[::-1]
                    
                    # ìƒìœ„ ê²°ê³¼ë§Œ ìœ ì§€ (ê°„ë‹¨í•œ êµ¬í˜„)
                    top_k = min(10, len(sorted_indices))
                    result['detections_nms'] = detections[sorted_indices[:top_k]]
                    result['scores_nms'] = scores[sorted_indices[:top_k]]
                    
            except Exception as e:
                self.logger.debug(f"NMS ì ìš© ì‹¤íŒ¨: {e}")
        
        return result
    
    def _resize_to_original(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ"""
        result = data.copy()
        
        if 'original_size' in data:
            original_size = data['original_size']
            
            for key, value in data.items():
                try:
                    if key != 'original_size' and isinstance(value, np.ndarray) and len(value.shape) >= 2:
                        if CV2_AVAILABLE:
                            if len(value.shape) == 3:
                                resized = cv2.resize(value, tuple(original_size))
                            elif len(value.shape) == 2:
                                resized = cv2.resize(value, tuple(original_size))
                            else:
                                continue
                            result[f'{key}_original_size'] = resized
                            
                except Exception as e:
                    self.logger.debug(f"ì›ë³¸ í¬ê¸° ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _convert_to_api_format(self, processed_result: Dict[str, Any]) -> Dict[str, Any]:
        """AI ê²°ê³¼ â†’ API ì‘ë‹µ í˜•ì‹ ë³€í™˜"""
        api_response = {}
        
        try:
            for api_field, api_type in self.detailed_data_spec.api_output_mapping.items():
                if api_field in processed_result:
                    value = processed_result[api_field]
                    
                    if api_type == "base64_string":
                        api_response[api_field] = self._array_to_base64(value)
                    elif api_type == "List[Dict]":
                        api_response[api_field] = self._convert_to_list_dict(value)
                    elif api_type == "List[Dict[str, float]]":
                        api_response[api_field] = self._convert_keypoints_to_dict_list(value)
                    elif api_type == "float":
                        api_response[api_field] = float(value) if value is not None else 0.0
                    elif api_type == "List[float]":
                        if isinstance(value, (list, tuple)):
                            api_response[api_field] = [float(x) for x in value]
                        elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                            api_response[api_field] = value.flatten().tolist()
                        else:
                            api_response[api_field] = [float(value)] if value is not None else []
                    elif api_type == "Dict[str, float]":
                        if isinstance(value, dict):
                            api_response[api_field] = {k: float(v) for k, v in value.items()}
                        else:
                            api_response[api_field] = {}
                    elif api_type == "List[str]":
                        if isinstance(value, (list, tuple)):
                            api_response[api_field] = [str(x) for x in value]
                        else:
                            api_response[api_field] = [str(value)] if value is not None else []
                    else:
                        api_response[api_field] = value
            
            # ê¸°ë³¸ API ì‘ë‹µì´ ì—†ëŠ” ê²½ìš° ëŒ€ì²´ ë§¤í•‘ ì‹œë„
            if not api_response:
                api_response = self._create_fallback_api_response(processed_result)
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} API í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨: {e}")
            api_response = self._create_fallback_api_response(processed_result)
        
        return api_response
    
    def _prepare_next_step_data(self, processed_result: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¤ìŒ Stepë“¤ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        next_step_data = {}
        
        try:
            for next_step, data_schema in self.detailed_data_spec.provides_to_next_step.items():
                step_data = {}
                
                for data_key, data_type in data_schema.items():
                    if data_key in processed_result:
                        value = processed_result[data_key]
                        
                        # ë°ì´í„° íƒ€ìž…ì— ë§žê²Œ ë³€í™˜
                        if data_type == "np.ndarray" and NUMPY_AVAILABLE:
                            if TORCH_AVAILABLE and torch.is_tensor(value):
                                step_data[data_key] = value.detach().cpu().numpy()
                            elif not isinstance(value, np.ndarray):
                                step_data[data_key] = np.array(value)
                            else:
                                step_data[data_key] = value
                                
                        elif data_type == "torch.Tensor" and TORCH_AVAILABLE:
                            if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                                step_data[data_key] = torch.from_numpy(value)
                            elif not torch.is_tensor(value):
                                step_data[data_key] = torch.tensor(value)
                            else:
                                step_data[data_key] = value
                                
                        elif data_type == "List[float]":
                            if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                                step_data[data_key] = value.flatten().tolist()
                            elif isinstance(value, (list, tuple)):
                                step_data[data_key] = [float(x) for x in value]
                            else:
                                step_data[data_key] = [float(value)] if value is not None else []
                                
                        elif data_type == "List[Tuple[float, float]]":
                            if NUMPY_AVAILABLE and isinstance(value, np.ndarray) and len(value.shape) == 2:
                                step_data[data_key] = [(float(row[0]), float(row[1])) for row in value]
                            else:
                                step_data[data_key] = value
                                
                        elif data_type == "Dict[str, Any]":
                            step_data[data_key] = value if isinstance(value, dict) else {'data': value}
                            
                        else:
                            step_data[data_key] = value
                
                if step_data:
                    next_step_data[next_step] = step_data
                    self.performance_metrics.step_data_transfers += 1
        
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ë‹¤ìŒ Step ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
        
        return next_step_data
    
    # ==============================================
    # ðŸ”¥ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ (v19.1)
    # ==============================================
    
    def _validate_input_data(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ìž…ë ¥ ë°ì´í„° ê²€ì¦"""
        validated = input_data.copy()
        
        try:
            for key, value in input_data.items():
                # ë°ì´í„° íƒ€ìž… ê²€ì¦
                if key in self.detailed_data_spec.input_shapes:
                    expected_shape = self.detailed_data_spec.input_shapes[key]
                    if hasattr(value, 'shape'):
                        actual_shape = value.shape
                        # í˜•íƒœ ê²€ì¦ (ë°°ì¹˜ ì°¨ì› ì œì™¸)
                        if len(actual_shape) > len(expected_shape):
                            if actual_shape[1:] != tuple(expected_shape):
                                self.logger.warning(f"âš ï¸ {self.step_name} Shape mismatch for {key}: expected {expected_shape}, got {actual_shape[1:]}")
                        elif actual_shape != tuple(expected_shape):
                            self.logger.warning(f"âš ï¸ {self.step_name} Shape mismatch for {key}: expected {expected_shape}, got {actual_shape}")
                
                # ê°’ ë²”ìœ„ ê²€ì¦
                if key in self.detailed_data_spec.input_value_ranges:
                    min_val, max_val = self.detailed_data_spec.input_value_ranges[key]
                    if hasattr(value, 'min') and hasattr(value, 'max'):
                        actual_min, actual_max = float(value.min()), float(value.max())
                        if actual_min < min_val or actual_max > max_val:
                            self.logger.warning(f"âš ï¸ {self.step_name} Value range warning for {key}: range [{actual_min:.3f}, {actual_max:.3f}], expected [{min_val}, {max_val}]")
                            
                            # ìžë™ í´ë¦¬í•‘ (ì„¤ì •ëœ ê²½ìš°)
                            if self.config.strict_data_validation:
                                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                                    validated[key] = np.clip(value, min_val, max_val)
                                elif TORCH_AVAILABLE and torch.is_tensor(value):
                                    validated[key] = torch.clamp(value, min_val, max_val)
                
                # ë°ì´í„° íƒ€ìž… ê²€ì¦
                expected_types = self.detailed_data_spec.input_data_types
                if expected_types:
                    value_type = type(value).__name__
                    if PIL_AVAILABLE and isinstance(value, Image.Image):
                        value_type = "PIL.Image"
                    elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                        value_type = "np.ndarray"
                    elif TORCH_AVAILABLE and torch.is_tensor(value):
                        value_type = "torch.Tensor"
                    
                    if value_type not in expected_types:
                        self.logger.debug(f"ðŸ”„ {self.step_name} Type mismatch for {key}: got {value_type}, expected one of {expected_types}")
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ìž…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            self.performance_metrics.validation_failures += 1
        
        return validated
    
    # ==============================================
    # ðŸ”¥ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (v19.1)
    # ==============================================
    
    def _array_to_base64(self, array: Any) -> str:
        """NumPy ë°°ì—´/í…ì„œ â†’ Base64 ë¬¸ìžì—´ ë³€í™˜"""
        try:
            # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
            if TORCH_AVAILABLE and torch.is_tensor(array):
                array = array.detach().cpu().numpy()
            
            if not NUMPY_AVAILABLE or not isinstance(array, np.ndarray):
                return ""
            
            # ê°’ ë²”ìœ„ ì •ê·œí™”
            if array.dtype != np.uint8:
                if array.max() <= 1.0:
                    array = (array * 255).astype(np.uint8)
                else:
                    array = np.clip(array, 0, 255).astype(np.uint8)
            
            # PIL Imageë¡œ ë³€í™˜
            if PIL_AVAILABLE:
                if len(array.shape) == 3:
                    # CHW â†’ HWC ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
                    if array.shape[0] in [1, 3, 4] and array.shape[0] < array.shape[1]:
                        array = np.transpose(array, (1, 2, 0))
                    
                    if array.shape[2] == 1:
                        array = array.squeeze(2)
                        image = Image.fromarray(array, mode='L')
                    elif array.shape[2] == 3:
                        image = Image.fromarray(array, mode='RGB')
                    elif array.shape[2] == 4:
                        image = Image.fromarray(array, mode='RGBA')
                    else:
                        # ì²« ë²ˆì§¸ ì±„ë„ë§Œ ì‚¬ìš©
                        image = Image.fromarray(array[:, :, 0], mode='L')
                        
                elif len(array.shape) == 2:
                    image = Image.fromarray(array, mode='L')
                else:
                    raise ValueError(f"Unsupported array shape: {array.shape}")
                
                # Base64 ì¸ì½”ë”©
                buffer = BytesIO()
                image.save(buffer, format='PNG')
                buffer.seek(0)
                
                return base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            return ""
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} Base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""
    
    def _convert_to_list_dict(self, value: Any) -> List[Dict]:
        """ê°’ì„ List[Dict] í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            if isinstance(value, (list, tuple)):
                if all(isinstance(item, dict) for item in value):
                    return list(value)
                else:
                    return [{'value': item, 'index': i} for i, item in enumerate(value)]
            
            elif isinstance(value, dict):
                return [value]
            
            elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                if len(value.shape) == 1:
                    return [{'value': float(item), 'index': i} for i, item in enumerate(value)]
                elif len(value.shape) == 2:
                    return [{'row': i, 'data': row.tolist()} for i, row in enumerate(value)]
                else:
                    return [{'data': value.tolist()}]
            
            else:
                return [{'value': value}]
                
        except Exception as e:
            self.logger.debug(f"List[Dict] ë³€í™˜ ì‹¤íŒ¨: {e}")
            return [{'value': str(value)}]
    
    def _convert_keypoints_to_dict_list(self, keypoints: Any) -> List[Dict[str, float]]:
        """í‚¤í¬ì¸íŠ¸ë¥¼ List[Dict[str, float]] í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            if NUMPY_AVAILABLE and isinstance(keypoints, np.ndarray):
                if len(keypoints.shape) == 2 and keypoints.shape[1] >= 2:
                    result = []
                    for i, point in enumerate(keypoints):
                        point_dict = {
                            'x': float(point[0]),
                            'y': float(point[1])
                        }
                        if keypoints.shape[1] > 2:
                            point_dict['confidence'] = float(point[2])
                        if keypoints.shape[1] > 3:
                            point_dict['visibility'] = float(point[3])
                        
                        point_dict['index'] = i
                        result.append(point_dict)
                    
                    return result
            
            elif isinstance(keypoints, (list, tuple)):
                result = []
                for i, point in enumerate(keypoints):
                    if isinstance(point, (list, tuple)) and len(point) >= 2:
                        point_dict = {
                            'x': float(point[0]),
                            'y': float(point[1]),
                            'index': i
                        }
                        if len(point) > 2:
                            point_dict['confidence'] = float(point[2])
                        result.append(point_dict)
                
                return result
            
            return []
            
        except Exception as e:
            self.logger.debug(f"í‚¤í¬ì¸íŠ¸ Dict ë³€í™˜ ì‹¤íŒ¨: {e}")
            return []
    
    def _create_fallback_api_response(self, processed_result: Dict[str, Any]) -> Dict[str, Any]:
        """í´ë°± API ì‘ë‹µ ìƒì„±"""
        fallback_response = {}
        
        try:
            # ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í‚¤ë“¤ì— ëŒ€í•œ ê¸°ë³¸ ë§¤í•‘
            common_mappings = {
                'parsing_mask': 'base64_string',
                'segmentation_mask': 'base64_string',
                'fitted_image': 'base64_string',
                'enhanced_image': 'base64_string',
                'final_result': 'base64_string',
                'result_image': 'base64_string',
                'output_image': 'base64_string',
                
                'keypoints': 'List[Dict[str, float]]',
                'pose_keypoints': 'List[Dict[str, float]]',
                
                'confidence': 'float',
                'quality_score': 'float',
                'confidence_scores': 'List[float]',
                
                'quality_assessment': 'Dict[str, float]',
                'processing_metadata': 'Dict[str, Any]'
            }
            
            for key, value in processed_result.items():
                if key in common_mappings:
                    api_type = common_mappings[key]
                    
                    if api_type == 'base64_string':
                        fallback_response[key] = self._array_to_base64(value)
                    elif api_type == 'List[Dict[str, float]]':
                        fallback_response[key] = self._convert_keypoints_to_dict_list(value)
                    elif api_type == 'float':
                        fallback_response[key] = float(value) if value is not None else 0.0
                    elif api_type == 'List[float]':
                        if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                            fallback_response[key] = value.flatten().tolist()
                        elif isinstance(value, (list, tuple)):
                            fallback_response[key] = [float(x) for x in value]
                    else:
                        fallback_response[key] = value
            
            # ê¸°ë³¸ ì‘ë‹µì´ ì—†ëŠ” ê²½ìš° ì²« ë²ˆì§¸ ì´ë¯¸ì§€í˜• ë°ì´í„°ë¥¼ resultë¡œ ì„¤ì •
            if not fallback_response:
                for key, value in processed_result.items():
                    if NUMPY_AVAILABLE and isinstance(value, np.ndarray) and len(value.shape) >= 2:
                        fallback_response['result'] = self._array_to_base64(value)
                        break
                    elif PIL_AVAILABLE and isinstance(value, Image.Image):
                        fallback_response['result'] = self._array_to_base64(np.array(value))
                        break
            
        except Exception as e:
            self.logger.debug(f"í´ë°± API ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return fallback_response
    
    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """í‘œì¤€ ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'step_name': self.step_name,
            'step_id': self.step_id,
            'github_compatible': True,
            'detailed_data_spec_applied': False,
            'processing_time': 0.0,
            'timestamp': time.time()
        }
    
    # ==============================================
    # ðŸ”¥ ê¸°ì¡´ GitHub í˜¸í™˜ ë©”ì„œë“œë“¤ (v19.1 ìœ ì§€)
    # ==============================================
    
    def _create_github_config(self, **kwargs) -> GitHubStepConfig:
        """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì„¤ì • ìƒì„±"""
        config = GitHubStepConfig()
        for key, value in kwargs.items():
            if hasattr(config, key):
                setattr(config, key, value)
        
        # GitHub í”„ë¡œì íŠ¸ íŠ¹ë³„ ì„¤ì •
        config.github_compatibility_mode = True
        config.real_ai_pipeline_support = True
        config.enable_detailed_data_spec = True
        
        # í™˜ê²½ë³„ ì„¤ì • ì ìš©
        if CONDA_INFO['is_target_env']:
            config.conda_optimized = True
            config.conda_env = CONDA_INFO['conda_env']
        
        if IS_M3_MAX:
            config.m3_max_optimized = True
            config.memory_gb = MEMORY_GB
            config.use_unified_memory = True
        
        return config
    
    def _apply_github_environment_optimization(self):
        """GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ìµœì í™”"""
        try:
            # M3 Max GitHub ìµœì í™”
            if self.is_m3_max:
                if self.device == "auto" and MPS_AVAILABLE:
                    self.device = "mps"
                
                if self.config.batch_size == 1 and self.memory_gb >= 64:
                    self.config.batch_size = 2
                
                self.config.auto_memory_cleanup = True
                self.logger.debug(f"âœ… GitHub M3 Max ìµœì í™”: {self.memory_gb:.1f}GB, device={self.device}")
            
            # GitHub conda í™˜ê²½ ìµœì í™”
            if self.conda_info['is_target_env']:
                self.config.optimization_enabled = True
                self.real_ai_pipeline_ready = True
                self.logger.debug(f"âœ… GitHub conda í™˜ê²½ ìµœì í™”: {self.conda_info['conda_env']}")
            
        except Exception as e:
            self.logger.debug(f"GitHub í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _github_emergency_setup(self, error: Exception):
        """GitHub í˜¸í™˜ ê¸´ê¸‰ ì„¤ì •"""
        self.step_name = getattr(self, 'step_name', self.__class__.__name__)
        self.logger = logging.getLogger("github_emergency")
        self.device = "cpu"
        self.is_initialized = False
        self.github_compatible = False
        self.performance_metrics = GitHubPerformanceMetrics()
        self.detailed_data_spec = DetailedDataSpecConfig()
        self.logger.error(f"ðŸš¨ {self.step_name} GitHub ê¸´ê¸‰ ì´ˆê¸°í™”: {error}")
    
    def _resolve_device(self, device: str) -> str:
        """GitHub ë””ë°”ì´ìŠ¤ í•´ê²° (í™˜ê²½ ìµœì í™”)"""
        if device == "auto":
            # GitHub M3 Max ìš°ì„  ì²˜ë¦¬
            if IS_M3_MAX and MPS_AVAILABLE:
                return "mps"
            
            if TORCH_AVAILABLE:
                if torch.cuda.is_available():
                    return "cuda"
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    return "mps"
            return "cpu"
        return device
    
    def _update_performance_metrics(self, processing_time: float, success: bool):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        try:
            self.performance_metrics.process_count += 1
            self.performance_metrics.total_process_time += processing_time
            self.performance_metrics.average_process_time = (
                self.performance_metrics.total_process_time / 
                self.performance_metrics.process_count
            )
            
            if success:
                self.performance_metrics.success_count += 1
            else:
                self.performance_metrics.error_count += 1
            
            # ìµœê·¼ ì²˜ë¦¬ ì‹œê°„ ì €ìž¥
            self._last_processing_time = processing_time
            
            # GitHub íŒŒì´í”„ë¼ì¸ ì„±ê³µë¥  ê³„ì‚°
            if self.performance_metrics.github_process_calls > 0:
                success_rate = (
                    self.performance_metrics.success_count /
                    self.performance_metrics.process_count * 100
                )
                self.performance_metrics.pipeline_success_rate = success_rate
                
        except Exception as e:
            self.logger.debug(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ðŸ”¥ GitHub í˜¸í™˜ ì˜ì¡´ì„± ì£¼ìž… ì¸í„°íŽ˜ì´ìŠ¤
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """GitHub í‘œì¤€ ModelLoader ì˜ì¡´ì„± ì£¼ìž…"""
        try:
            success = self.dependency_manager.inject_model_loader(model_loader)
            if success:
                self.model_loader = model_loader
                self.has_model = True
                self.model_loaded = True
                self.real_ai_pipeline_ready = True
                self.performance_metrics.dependencies_injected += 1
                self.logger.info(f"âœ… {self.step_name} GitHub ModelLoader ì˜ì¡´ì„± ì£¼ìž… ì™„ë£Œ")
        except Exception as e:
            self.performance_metrics.injection_failures += 1
            self.logger.error(f"âŒ {self.step_name} GitHub ModelLoader ì˜ì¡´ì„± ì£¼ìž… ì˜¤ë¥˜: {e}")
    
    def set_memory_manager(self, memory_manager):
        """GitHub í‘œì¤€ MemoryManager ì˜ì¡´ì„± ì£¼ìž…"""
        try:
            success = self.dependency_manager.inject_memory_manager(memory_manager)
            if success:
                self.memory_manager = memory_manager
                self.performance_metrics.dependencies_injected += 1
        except Exception as e:
            self.performance_metrics.injection_failures += 1
            self.logger.warning(f"âš ï¸ {self.step_name} GitHub MemoryManager ì˜ì¡´ì„± ì£¼ìž… ì˜¤ë¥˜: {e}")
    
    # ==============================================
    # ðŸ”¥ GitHub í˜¸í™˜ ì˜ì¡´ì„± ê²€ì¦
    # ==============================================
    
    def validate_dependencies(self, format_type: DependencyValidationFormat = None) -> Union[Dict[str, bool], Dict[str, Any]]:
        """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì˜ì¡´ì„± ê²€ì¦ (v19.1)"""
        try:
            return self.dependency_manager.validate_dependencies_github_format(format_type)
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} GitHub ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            
            if format_type == DependencyValidationFormat.BOOLEAN_DICT:
                return {'model_loader': False, 'step_interface': False, 'memory_manager': False, 'data_converter': False}
            else:
                return {'success': False, 'error': str(e), 'github_compatible': False}
    
    def validate_dependencies_boolean(self) -> Dict[str, bool]:
        """GitHub Step í´ëž˜ìŠ¤ í˜¸í™˜ (GeometricMatchingStep ë“±)"""
        return self.validate_dependencies(DependencyValidationFormat.BOOLEAN_DICT)
    
    def validate_dependencies_detailed(self) -> Dict[str, Any]:
        """StepFactory í˜¸í™˜ (ìƒì„¸ ì •ë³´)"""
        return self.validate_dependencies(DependencyValidationFormat.DETAILED_DICT)
    
    # ==============================================
    # ðŸ”¥ GitHub í‘œì¤€ ì´ˆê¸°í™” ë° ìƒíƒœ ê´€ë¦¬
    # ==============================================
    
    def initialize(self) -> bool:
        """GitHub í‘œì¤€ ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"ðŸ”„ {self.step_name} GitHub í‘œì¤€ ì´ˆê¸°í™” ì‹œìž‘...")
            
            # DetailedDataSpec ê²€ì¦
            if not self.data_conversion_ready:
                self.logger.warning(f"âš ï¸ {self.step_name} DetailedDataSpec ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ë¯¸ì™„ë£Œ")
            
            # ì´ˆê¸°í™” ìƒíƒœ ì„¤ì •
            self.dependency_manager.dependency_status.base_initialized = True
            self.dependency_manager.dependency_status.github_compatible = True
            self.is_initialized = True
            
            self.logger.info(f"âœ… {self.step_name} GitHub í‘œì¤€ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} GitHub ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.performance_metrics.error_count += 1
            return False
    
    def get_status(self) -> Dict[str, Any]:
        """GitHub í†µí•© ìƒíƒœ ì¡°íšŒ (v19.1)"""
        try:
            return {
                'step_info': {
                    'step_name': self.step_name,
                    'step_id': self.step_id,
                    'version': 'BaseStepMixin v19.1 DetailedDataSpec Integration'
                },
                'github_status_flags': {
                    'is_initialized': self.is_initialized,
                    'is_ready': self.is_ready,
                    'has_model': self.has_model,
                    'model_loaded': self.model_loaded,
                    'github_compatible': self.github_compatible,
                    'real_ai_pipeline_ready': self.real_ai_pipeline_ready,
                    'data_conversion_ready': self.data_conversion_ready
                },
                'detailed_data_spec_status': {
                    'spec_loaded': self.dependency_manager.dependency_status.detailed_data_spec_loaded,
                    'data_conversion_ready': self.dependency_manager.dependency_status.data_conversion_ready,
                    'preprocessing_configured': bool(self.detailed_data_spec.preprocessing_steps),
                    'postprocessing_configured': bool(self.detailed_data_spec.postprocessing_steps),
                    'api_mapping_configured': bool(self.detailed_data_spec.api_input_mapping and self.detailed_data_spec.api_output_mapping),
                    'step_flow_configured': bool(self.detailed_data_spec.provides_to_next_step or self.detailed_data_spec.accepts_from_previous_step)
                },
                'github_performance': {
                    'data_conversions': self.performance_metrics.data_conversions,
                    'preprocessing_operations': self.performance_metrics.preprocessing_operations,
                    'postprocessing_operations': self.performance_metrics.postprocessing_operations,
                    'api_conversions': self.performance_metrics.api_conversions,
                    'step_data_transfers': self.performance_metrics.step_data_transfers,
                    'validation_failures': self.performance_metrics.validation_failures
                },
                'timestamp': time.time()
            }
        except Exception as e:
            self.logger.error(f"âŒ GitHub ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'version': 'BaseStepMixin v19.1 DetailedDataSpec Integration'}

# ==============================================
# ðŸ”¥ Export
# ==============================================

__all__ = [
    # ë©”ì¸ í´ëž˜ìŠ¤
    'BaseStepMixin',
    'GitHubDependencyManager',
    
    # ì„¤ì • ë° ìƒíƒœ í´ëž˜ìŠ¤ë“¤
    'DetailedDataSpecConfig',
    'GitHubStepConfig',
    'GitHubDependencyStatus', 
    'GitHubPerformanceMetrics',
    
    # GitHub ì—´ê±°í˜•ë“¤
    'ProcessMethodSignature',
    'DependencyValidationFormat',
    'DataConversionMethod',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'CV2_AVAILABLE',
    'CONDA_INFO',
    'IS_M3_MAX',
    'MEMORY_GB'
]

# ==============================================
# ðŸ”¥ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë¡œê·¸
# ==============================================

logger = logging.getLogger(__name__)
logger.info("=" * 100)
logger.info("ðŸ”¥ BaseStepMixin v19.1 - DetailedDataSpec ì™„ì „ í†µí•©")
logger.info("=" * 100)
logger.info("âœ… step_model_requirements.py DetailedDataSpec ì™„ì „ í™œìš©")
logger.info("âœ… API â†” AI ëª¨ë¸ ê°„ ë°ì´í„° ë³€í™˜ í‘œì¤€í™” ì™„ë£Œ")
logger.info("âœ… Step ê°„ ë°ì´í„° íë¦„ ìžë™ ì²˜ë¦¬")
logger.info("âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ìžë™ ì ìš©")
logger.info("âœ… GitHub í”„ë¡œì íŠ¸ Step í´ëž˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜")
logger.info("âœ… process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ì™„ì „ í‘œì¤€í™”")
logger.info("âœ… ì‹¤ì œ Step í´ëž˜ìŠ¤ë“¤ì€ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„í•˜ë©´ ë¨")
logger.info("âœ… validate_dependencies() ì˜¤ë²„ë¡œë“œ ì§€ì›")
logger.info("âœ… StepFactory v11.0ê³¼ ì™„ì „ í˜¸í™˜")
logger.info("âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™” (mycloset-ai-clean)")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")

logger.info("ðŸ”§ DetailedDataSpec í†µí•© ê¸°ëŠ¥:")
logger.info("   ðŸ“‹ ìž…ì¶œë ¥ ë°ì´í„° íƒ€ìž…, í˜•íƒœ, ë²”ìœ„ ìžë™ ê²€ì¦")
logger.info("   ðŸ”— API ìž…ì¶œë ¥ ë§¤í•‘ ìžë™ ë³€í™˜")
logger.info("   ðŸ”„ Step ê°„ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ìžë™ ì²˜ë¦¬")
logger.info("   âš™ï¸ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ë‹¨ê³„ ìžë™ ì ìš©")
logger.info("   ðŸ“Š ë°ì´í„° íë¦„ ìžë™ ê´€ë¦¬")

logger.info("ðŸŽ¯ ì§€ì›í•˜ëŠ” ì „ì²˜ë¦¬:")
logger.info("   - ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (512x512, 768x1024, 256x192, 224x224, 368x368, 1024x1024)")
logger.info("   - ì •ê·œí™” (ImageNet, CLIP, Diffusion)")
logger.info("   - í…ì„œ ë³€í™˜ (HWC â†’ CHW)")
logger.info("   - SAM í”„ë¡¬í”„íŠ¸ ì¤€ë¹„")
logger.info("   - Diffusion ìž…ë ¥ ì¤€ë¹„")

logger.info("ðŸŽ¯ ì§€ì›í•˜ëŠ” í›„ì²˜ë¦¬:")
logger.info("   - Softmax, Argmax ì ìš©")
logger.info("   - ìž„ê³„ê°’ ì ìš©, NMS")
logger.info("   - ì—­ì •ê·œí™” (ImageNet, Diffusion)")
logger.info("   - í˜•íƒœí•™ì  ì—°ì‚°, í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ")
logger.info("   - ì„¸ë¶€ì‚¬í•­ í–¥ìƒ, ìµœì¢… í•©ì„±")

logger.info(f"ðŸ”§ í˜„ìž¬ conda í™˜ê²½: {CONDA_INFO['conda_env']} ({'âœ… ìµœì í™”ë¨' if CONDA_INFO['is_target_env'] else 'âš ï¸ ê¶Œìž¥: mycloset-ai-clean'})")
logger.info(f"ðŸ–¥ï¸  í˜„ìž¬ ì‹œìŠ¤í…œ: M3 Max={IS_M3_MAX}, ë©”ëª¨ë¦¬={MEMORY_GB:.1f}GB")
logger.info(f"ðŸš€ GitHub AI íŒŒì´í”„ë¼ì¸ ì¤€ë¹„: {TORCH_AVAILABLE and (MPS_AVAILABLE or (torch.cuda.is_available() if TORCH_AVAILABLE else False))}")
logger.info("=" * 100)
logger.info("ðŸŽ‰ BaseStepMixin v19.1 ì™„ì „ ì¤€ë¹„ ì™„ë£Œ!")
logger.info("ðŸ’¡ ì´ì œ ì‹¤ì œ Step í´ëž˜ìŠ¤ë“¤ì€ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„í•˜ë©´ ë©ë‹ˆë‹¤!")
logger.info("ðŸ’¡ ëª¨ë“  ë°ì´í„° ë³€í™˜ì´ BaseStepMixinì—ì„œ ìžë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤!")
logger.info("=" * 100)