# backend/app/ai_pipeline/steps/base_step_mixin.py
"""
ğŸ”¥ BaseStepMixin v19.0 - ì „ë©´ ê°œì„  ì™„ì „íŒ (GitHub í”„ë¡œì íŠ¸ 100% í˜¸í™˜)
================================================================

âœ… GitHub í”„ë¡œì íŠ¸ Step í´ë˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜
âœ… process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ì™„ì „ í‘œì¤€í™”
âœ… validate_dependencies() ë°˜í™˜ í˜•ì‹ í†µì¼
âœ… StepFactory v9.0ê³¼ ì™„ì „ í˜¸í™˜
âœ… ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ ì „ë©´ ê°œì„ 
âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™” (mycloset-ai-clean)
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë° ì„±ëŠ¥

í•µì‹¬ ê°œì„ ì‚¬í•­:
1. ğŸ¯ GitHub Step í´ë˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤
2. ğŸ”„ process() ë©”ì„œë“œ í‘œì¤€ ì‹œê·¸ë‹ˆì²˜: async def process(self, **kwargs) -> Dict[str, Any]
3. ğŸ” validate_dependencies() ì˜¤ë²„ë¡œë“œ (legacy + new format ì§€ì›)
4. ğŸ—ï¸ ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ ì „ë©´ ì¬ì„¤ê³„
5. ğŸš€ ì‹¤ì œ AI ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›
6. ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì§„ë‹¨ ë„êµ¬ ê°•í™”
7. ğŸ›¡ï¸ ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ì‹œìŠ¤í…œ ê°œì„ 

Author: MyCloset AI Team
Date: 2025-07-27
Version: 19.0 (GitHub Project Full Compatibility)
"""

import os
import gc
import time
import asyncio
import logging
import threading
import traceback
import weakref
import subprocess
import platform
import inspect
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, Type, TYPE_CHECKING, Awaitable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from functools import wraps
from contextlib import asynccontextmanager
from enum import Enum

# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from ..utils.model_loader import ModelLoader, StepModelInterface
    from ..factories.step_factory import StepFactory
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
    from ..core.di_container import DIContainer

# ==============================================
# ğŸ”¥ í™˜ê²½ ì„¤ì • ë° ì‹œìŠ¤í…œ ì •ë³´
# ==============================================

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean'
}

# ì‹œìŠ¤í…œ ì •ë³´
IS_M3_MAX = False
MEMORY_GB = 16.0

try:
    if platform.system() == 'Darwin':
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.stdout.strip():
            MEMORY_GB = int(memory_result.stdout.strip()) / 1024**3
except:
    pass

# ==============================================
# ğŸ”¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì „ import
# ==============================================

# PyTorch ì•ˆì „ import
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
except ImportError:
    torch = None

# PIL ì•ˆì „ import
PIL_AVAILABLE = False
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    Image = None

# NumPy ì•ˆì „ import
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None

# ==============================================
# ğŸ”¥ GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ (v19.0 ì‹ ê·œ)
# ==============================================

class ProcessMethodSignature(Enum):
    """GitHub í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” process ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ íŒ¨í„´"""
    STANDARD = "async def process(self, **kwargs) -> Dict[str, Any]"
    INPUT_DATA = "async def process(self, input_data: Any) -> Dict[str, Any]"
    PIPELINE = "async def process_pipeline(self, input_data: Dict[str, Any]) -> Dict[str, Any]"
    LEGACY = "def process(self, *args, **kwargs) -> Dict[str, Any]"

class DependencyValidationFormat(Enum):
    """ì˜ì¡´ì„± ê²€ì¦ ë°˜í™˜ í˜•ì‹"""
    BOOLEAN_DICT = "dict_bool"  # GeometricMatchingStep í˜•ì‹: {'model_loader': True, ...}
    DETAILED_DICT = "dict_detailed"  # BaseStepMixin v18.0 í˜•ì‹: {'success': True, 'details': {...}}
    AUTO_DETECT = "auto"  # í˜¸ì¶œìì— ë”°ë¼ ìë™ ì„ íƒ

# ==============================================
# ğŸ”¥ GitHub í˜¸í™˜ ì˜ì¡´ì„± ì£¼ì… ì¸í„°í˜ì´ìŠ¤ (v19.0 ê°•í™”)
# ==============================================

class IGitHubModelProvider(ABC):
    """GitHub í”„ë¡œì íŠ¸ ModelLoader ì¸í„°í˜ì´ìŠ¤ (v19.0)"""
    
    @abstractmethod
    def get_model(self, model_name: str) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        pass
    
    @abstractmethod
    async def get_model_async(self, model_name: str) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        pass
    
    @abstractmethod
    def is_model_available(self, model_name: str) -> bool:
        """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        pass
    
    @abstractmethod
    def load_model(self, model_name: str, **kwargs) -> bool:
        """ëª¨ë¸ ë¡œë”©"""
        pass
    
    @abstractmethod
    def create_step_interface(self, step_name: str) -> Optional['StepModelInterface']:
        """Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± (GitHub í‘œì¤€)"""
        pass

class IGitHubMemoryManager(ABC):
    """GitHub í”„ë¡œì íŠ¸ MemoryManager ì¸í„°í˜ì´ìŠ¤ (v19.0)"""
    
    @abstractmethod
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        pass
    
    @abstractmethod
    async def optimize_memory_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™”"""
        pass
    
    @abstractmethod
    def get_memory_info(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        pass

class IGitHubDataConverter(ABC):
    """GitHub í”„ë¡œì íŠ¸ DataConverter ì¸í„°í˜ì´ìŠ¤ (v19.0)"""
    
    @abstractmethod
    def convert_data(self, data: Any, target_format: str) -> Any:
        """ë°ì´í„° ë³€í™˜"""
        pass
    
    @abstractmethod
    def validate_data(self, data: Any, expected_format: str) -> bool:
        """ë°ì´í„° ê²€ì¦"""
        pass

# ==============================================
# ğŸ”¥ ì„¤ì • ë° ìƒíƒœ í´ë˜ìŠ¤ (v19.0 GitHub í˜¸í™˜)
# ==============================================

@dataclass
class GitHubStepConfig:
    """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ Step ì„¤ì • (v19.0)"""
    step_name: str = "BaseStep"
    step_id: int = 0
    device: str = "auto"
    use_fp16: bool = True
    batch_size: int = 1
    confidence_threshold: float = 0.8
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True
    optimization_enabled: bool = True
    quality_level: str = "balanced"
    strict_mode: bool = False
    
    # ì˜ì¡´ì„± ì„¤ì • (v19.0 GitHub í˜¸í™˜ ê°•í™”)
    auto_inject_dependencies: bool = True
    require_model_loader: bool = True
    require_memory_manager: bool = False
    require_data_converter: bool = False
    dependency_timeout: float = 30.0
    dependency_retry_count: int = 3
    
    # GitHub í”„ë¡œì íŠ¸ íŠ¹ë³„ ì„¤ì • (v19.0 ì‹ ê·œ)
    process_method_signature: ProcessMethodSignature = ProcessMethodSignature.STANDARD
    dependency_validation_format: DependencyValidationFormat = DependencyValidationFormat.AUTO_DETECT
    github_compatibility_mode: bool = True
    real_ai_pipeline_support: bool = True
    
    # í™˜ê²½ ìµœì í™”
    conda_optimized: bool = False
    conda_env: str = "none"
    m3_max_optimized: bool = False
    memory_gb: float = 16.0
    use_unified_memory: bool = False

@dataclass
class GitHubDependencyStatus:
    """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì˜ì¡´ì„± ìƒíƒœ (v19.0)"""
    model_loader: bool = False
    step_interface: bool = False
    memory_manager: bool = False
    data_converter: bool = False
    di_container: bool = False
    base_initialized: bool = False
    custom_initialized: bool = False
    dependencies_validated: bool = False
    
    # GitHub íŠ¹ë³„ ìƒíƒœ (v19.0 ì‹ ê·œ)
    github_compatible: bool = False
    process_method_validated: bool = False
    real_ai_models_loaded: bool = False
    
    # í™˜ê²½ ìƒíƒœ
    conda_optimized: bool = False
    m3_max_optimized: bool = False
    
    # ì£¼ì… ì‹œë„ ì¶”ì 
    injection_attempts: Dict[str, int] = field(default_factory=dict)
    injection_errors: Dict[str, List[str]] = field(default_factory=dict)
    last_injection_time: float = field(default_factory=time.time)

@dataclass
class GitHubPerformanceMetrics:
    """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ (v19.0)"""
    process_count: int = 0
    total_process_time: float = 0.0
    average_process_time: float = 0.0
    error_count: int = 0
    success_count: int = 0
    cache_hits: int = 0
    
    # ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­
    peak_memory_usage_mb: float = 0.0
    average_memory_usage_mb: float = 0.0
    memory_optimizations: int = 0
    
    # AI ëª¨ë¸ ë©”íŠ¸ë¦­
    models_loaded: int = 0
    total_model_size_gb: float = 0.0
    inference_count: int = 0
    
    # ì˜ì¡´ì„± ë©”íŠ¸ë¦­ (v19.0 ê°•í™”)
    dependencies_injected: int = 0
    injection_failures: int = 0
    average_injection_time: float = 0.0
    
    # GitHub íŠ¹ë³„ ë©”íŠ¸ë¦­ (v19.0 ì‹ ê·œ)
    github_process_calls: int = 0
    real_ai_inferences: int = 0
    pipeline_success_rate: float = 0.0

# ==============================================
# ğŸ”¥ GitHub í˜¸í™˜ ì˜ì¡´ì„± ê´€ë¦¬ì v19.0
# ==============================================

class GitHubDependencyManager:
    """GitHub í”„ë¡œì íŠ¸ ì™„ì „ í˜¸í™˜ ì˜ì¡´ì„± ê´€ë¦¬ì v19.0"""
    
    def __init__(self, step_name: str):
        self.step_name = step_name
        self.logger = logging.getLogger(f"GitHubDependencyManager.{step_name}")
        
        # ì˜ì¡´ì„± ì €ì¥
        self.dependencies: Dict[str, Any] = {}
        self.dependency_status = GitHubDependencyStatus()
        
        # í™˜ê²½ ì •ë³´
        self.conda_info = CONDA_INFO
        self.is_m3_max = IS_M3_MAX
        self.memory_gb = MEMORY_GB
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        
        # GitHub í˜¸í™˜ì„± ì¶”ì  (v19.0 ì‹ ê·œ)
        self._github_compatibility_checked = False
        self._process_method_signature = None
        self._dependency_validation_format = DependencyValidationFormat.AUTO_DETECT
        self._auto_injection_attempted = False  # ìë™ ì£¼ì… ì‹œë„ í”Œë˜ê·¸
        
        # í™˜ê²½ ìµœì í™” ì„¤ì •
        self._setup_environment_optimization()
    
    def _setup_environment_optimization(self):
        """í™˜ê²½ ìµœì í™” ì„¤ì •"""
        try:
            # conda í™˜ê²½ ìµœì í™”
            if self.conda_info['is_target_env']:
                self.dependency_status.conda_optimized = True
                self.logger.debug(f"âœ… conda í™˜ê²½ ìµœì í™” í™œì„±í™”: {self.conda_info['conda_env']}")
            
            # M3 Max ìµœì í™”
            if self.is_m3_max:
                self.dependency_status.m3_max_optimized = True
                self.logger.debug(f"âœ… M3 Max ìµœì í™” í™œì„±í™”: {self.memory_gb:.1f}GB")
                
        except Exception as e:
            self.logger.debug(f"í™˜ê²½ ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def inject_model_loader(self, model_loader: 'ModelLoader') -> bool:
        """GitHub í˜¸í™˜ ModelLoader ì˜ì¡´ì„± ì£¼ì… (v19.0 ì™„ì „ ìˆ˜ì •)"""
        injection_start = time.time()
        
        try:
            with self._lock:
                self.logger.info(f"ğŸ”„ {self.step_name} GitHub í˜¸í™˜ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
                
                # 1. ModelLoader ì €ì¥
                self.dependencies['model_loader'] = model_loader
                
                # 2. GitHub í˜¸í™˜ì„± ê²€ì¦
                if not self._validate_github_model_loader(model_loader):
                    self.logger.warning("âš ï¸ ModelLoaderê°€ GitHub í‘œì¤€ì„ ì™„ì „íˆ ì¤€ìˆ˜í•˜ì§€ ì•ŠìŒ (ê³„ì† ì§„í–‰)")
                
                # 3. ğŸ”¥ StepModelInterface ìƒì„± (GitHub í‘œì¤€)
                step_interface = self._create_github_step_interface(model_loader)
                if step_interface:
                    self.dependencies['step_interface'] = step_interface
                    self.dependency_status.step_interface = True
                    self.logger.info(f"âœ… {self.step_name} GitHub StepModelInterface ìƒì„± ì™„ë£Œ")
                
                # 4. í™˜ê²½ ìµœì í™” ì ìš©
                self._apply_github_model_loader_optimization(model_loader)
                
                # 5. ìƒíƒœ ì—…ë°ì´íŠ¸
                self.dependency_status.model_loader = True
                self.dependency_status.github_compatible = True
                self.dependency_status.last_injection_time = time.time()
                
                injection_time = time.time() - injection_start
                self.logger.info(f"âœ… {self.step_name} GitHub ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ ({injection_time:.3f}ì´ˆ)")
                
                return True
                
        except Exception as e:
            injection_time = time.time() - injection_start
            self.logger.error(f"âŒ {self.step_name} GitHub ModelLoader ì£¼ì… ì‹¤íŒ¨ ({injection_time:.3f}ì´ˆ): {e}")
            return False
    
    def _validate_github_model_loader(self, model_loader: 'ModelLoader') -> bool:
        """GitHub í‘œì¤€ ModelLoader ê²€ì¦"""
        try:
            # GitHub í•„ìˆ˜ ë©”ì„œë“œ í™•ì¸
            github_required_methods = [
                'load_model', 'is_initialized', 'create_step_interface',
                'get_model_sync', 'get_model_async'  # v19.0 ì¶”ê°€
            ]
            
            for method in github_required_methods:
                if not hasattr(model_loader, method):
                    self.logger.debug(f"âš ï¸ GitHub í‘œì¤€ ë©”ì„œë“œ ëˆ„ë½: {method}")
                    return False
            
            # GitHub íŠ¹ë³„ ì†ì„± í™•ì¸
            if hasattr(model_loader, 'github_compatible'):
                if not getattr(model_loader, 'github_compatible', False):
                    self.logger.debug("âš ï¸ ModelLoaderê°€ GitHub í˜¸í™˜ ëª¨ë“œê°€ ì•„ë‹˜")
                    return False
            
            self.logger.debug(f"âœ… {self.step_name} GitHub ModelLoader ê²€ì¦ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ModelLoader ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def _create_github_step_interface(self, model_loader: 'ModelLoader') -> Optional['StepModelInterface']:
        """GitHub í‘œì¤€ StepModelInterface ìƒì„±"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} GitHub StepModelInterface ìƒì„± ì‹œì‘...")
            
            # GitHub í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            if hasattr(model_loader, 'create_step_interface'):
                interface = model_loader.create_step_interface(self.step_name)
                
                if interface and self._validate_github_step_interface(interface):
                    self.logger.info(f"âœ… {self.step_name} GitHub StepModelInterface ìƒì„± ë° ê²€ì¦ ì™„ë£Œ")
                    return interface
            
            # GitHub í´ë°± ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            return self._create_github_fallback_interface(model_loader)
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} GitHub StepModelInterface ìƒì„± ì˜¤ë¥˜: {e}")
            return self._create_github_fallback_interface(model_loader)
    
    def _validate_github_step_interface(self, interface: 'StepModelInterface') -> bool:
        """GitHub í‘œì¤€ StepModelInterface ê²€ì¦"""
        try:
            # GitHub í•„ìˆ˜ ë©”ì„œë“œ í™•ì¸
            github_required_methods = [
                'get_model_sync', 'get_model_async', 'register_model_requirement',
                'is_model_available', 'load_model_for_step'  # v19.0 ì¶”ê°€
            ]
            
            for method in github_required_methods:
                if not hasattr(interface, method):
                    self.logger.debug(f"âš ï¸ GitHub StepModelInterface ë©”ì„œë“œ ëˆ„ë½: {method}")
            
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ GitHub StepModelInterface ê²€ì¦ ì˜¤ë¥˜: {e}")
            return False
    
    def _create_github_fallback_interface(self, model_loader: 'ModelLoader') -> Optional['StepModelInterface']:
        """GitHub í´ë°± StepModelInterface ìƒì„±"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} GitHub í´ë°± StepModelInterface ìƒì„±...")
            
            # GitHub í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ ë™ì  ìƒì„±
            class GitHubStepModelInterface:
                def __init__(self, step_name: str, model_loader):
                    self.step_name = step_name
                    self.model_loader = model_loader
                    self.github_compatible = True
                
                def get_model_sync(self, model_name: str = "default") -> Optional[Any]:
                    if hasattr(self.model_loader, 'load_model'):
                        return self.model_loader.load_model(model_name)
                    return None
                
                async def get_model_async(self, model_name: str = "default") -> Optional[Any]:
                    if hasattr(self.model_loader, 'load_model_async'):
                        return await self.model_loader.load_model_async(model_name)
                    return self.get_model_sync(model_name)
                
                def register_model_requirement(self, model_name: str, **kwargs) -> bool:
                    return True
                
                def is_model_available(self, model_name: str) -> bool:
                    return True
                
                def load_model_for_step(self, model_name: str) -> bool:
                    return self.get_model_sync(model_name) is not None
            
            interface = GitHubStepModelInterface(self.step_name, model_loader)
            self.logger.info(f"âœ… {self.step_name} GitHub í´ë°± StepModelInterface ìƒì„± ì™„ë£Œ")
            return interface
                
        except Exception as e:
            self.logger.error(f"âŒ GitHub í´ë°± StepModelInterface ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _apply_github_model_loader_optimization(self, model_loader: 'ModelLoader'):
        """GitHub ModelLoader í™˜ê²½ ìµœì í™”"""
        try:
            # GitHub íŠ¹ë³„ í™˜ê²½ ì„¤ì •
            if hasattr(model_loader, 'configure_github_environment'):
                github_config = {
                    'conda_env': self.conda_info['conda_env'],
                    'is_m3_max': self.is_m3_max,
                    'memory_gb': self.memory_gb,
                    'github_mode': True,
                    'real_ai_pipeline': True
                }
                model_loader.configure_github_environment(github_config)
                self.logger.debug(f"âœ… {self.step_name} GitHub ModelLoader í™˜ê²½ ìµœì í™” ì ìš©")
                
        except Exception as e:
            self.logger.debug(f"GitHub ModelLoader í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def validate_dependencies_github_format(self, format_type: DependencyValidationFormat = None) -> Union[Dict[str, bool], Dict[str, Any]]:
        """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì˜ì¡´ì„± ê²€ì¦ (v19.0 í•µì‹¬ ê¸°ëŠ¥)"""
        try:
            with self._lock:
                # ìë™ ê°ì§€ ë˜ëŠ” ì§€ì •ëœ í˜•ì‹ ì‚¬ìš©
                if format_type is None:
                    format_type = self._dependency_validation_format
                
                if format_type == DependencyValidationFormat.AUTO_DETECT:
                    # í˜¸ì¶œ ìŠ¤íƒ ë¶„ì„ìœ¼ë¡œ í˜•ì‹ ê²°ì •
                    format_type = self._detect_validation_format_from_caller()
                
                if format_type == DependencyValidationFormat.BOOLEAN_DICT:
                    # GeometricMatchingStep í˜•ì‹ (GitHub í‘œì¤€)
                    return self._validate_dependencies_boolean_format()
                else:
                    # BaseStepMixin v18.0 í˜•ì‹ (ìƒì„¸ ì •ë³´)
                    return self._validate_dependencies_detailed_format()
                    
        except Exception as e:
            self.logger.error(f"âŒ GitHub ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            if format_type == DependencyValidationFormat.BOOLEAN_DICT:
                return {'error': True}
            else:
                return {'success': False, 'error': str(e)}
    
    def _detect_validation_format_from_caller(self) -> DependencyValidationFormat:
        """í˜¸ì¶œì ë¶„ì„ìœ¼ë¡œ ê²€ì¦ í˜•ì‹ ìë™ ê°ì§€"""
        try:
            frame = inspect.currentframe()
            for _ in range(5):  # ìµœëŒ€ 5ë‹¨ê³„ê¹Œì§€ ì¶”ì 
                frame = frame.f_back
                if frame is None:
                    break
                
                caller_name = frame.f_code.co_name
                caller_file = frame.f_code.co_filename
                
                # GitHub Step í´ë˜ìŠ¤ì—ì„œ í˜¸ì¶œëœ ê²½ìš°
                if 'step_' in caller_file.lower() and any(name in caller_name.lower() for name in ['geometric', 'parsing', 'pose', 'cloth']):
                    return DependencyValidationFormat.BOOLEAN_DICT
                
                # StepFactoryì—ì„œ í˜¸ì¶œëœ ê²½ìš°
                if 'factory' in caller_file.lower() or 'validate' in caller_name.lower():
                    return DependencyValidationFormat.DETAILED_DICT
            
            # ê¸°ë³¸ê°’
            return DependencyValidationFormat.BOOLEAN_DICT
            
        except Exception:
            return DependencyValidationFormat.BOOLEAN_DICT
    
    def _validate_dependencies_boolean_format(self) -> Dict[str, bool]:
        """GitHub Step í´ë˜ìŠ¤ í˜¸í™˜ í˜•ì‹ (boolean dict)"""
        try:
            validation_results = {}
            
            for dep_name, dep_obj in self.dependencies.items():
                if dep_obj is not None:
                    if dep_name == 'model_loader':
                        validation_results[dep_name] = hasattr(dep_obj, 'load_model')
                    elif dep_name == 'step_interface':
                        validation_results[dep_name] = hasattr(dep_obj, 'get_model_sync')
                    elif dep_name == 'memory_manager':
                        validation_results[dep_name] = hasattr(dep_obj, 'optimize_memory')
                    elif dep_name == 'data_converter':
                        validation_results[dep_name] = hasattr(dep_obj, 'convert_data')
                    elif dep_name == 'di_container':
                        validation_results[dep_name] = True
                    else:
                        validation_results[dep_name] = True
                else:
                    validation_results[dep_name] = False
            
            # GitHub í‘œì¤€ ì˜ì¡´ì„±ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
            default_deps = ['model_loader', 'step_interface', 'memory_manager', 'data_converter']
            for dep in default_deps:
                if dep not in validation_results:
                    validation_results[dep] = False
            
            return validation_results
            
        except Exception as e:
            self.logger.error(f"âŒ Boolean í˜•ì‹ ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'model_loader': False, 'step_interface': False, 'memory_manager': False, 'data_converter': False}
    
    def _validate_dependencies_detailed_format(self) -> Dict[str, Any]:
        """BaseStepMixin v18.0 í˜¸í™˜ í˜•ì‹ (ìƒì„¸ ì •ë³´)"""
        try:
            validation_results = {
                "success": True,
                "total_dependencies": len(self.dependencies),
                "validated_dependencies": 0,
                "failed_dependencies": 0,
                "required_missing": [],
                "optional_missing": [],
                "validation_errors": [],
                "details": {},
                "github_compatible": self.dependency_status.github_compatible,  # v19.0 ì¶”ê°€
                "real_ai_ready": self.dependency_status.real_ai_models_loaded  # v19.0 ì¶”ê°€
            }
            
            for dep_name, dep_obj in self.dependencies.items():
                if dep_obj is not None:
                    if dep_name == 'model_loader':
                        is_valid = hasattr(dep_obj, 'load_model') and hasattr(dep_obj, 'create_step_interface')
                    elif dep_name == 'step_interface':
                        is_valid = hasattr(dep_obj, 'get_model_sync') and hasattr(dep_obj, 'get_model_async')
                    elif dep_name == 'memory_manager':
                        is_valid = hasattr(dep_obj, 'optimize_memory')
                    elif dep_name == 'data_converter':
                        is_valid = hasattr(dep_obj, 'convert_data')
                    else:
                        is_valid = True
                    
                    if is_valid:
                        validation_results["validated_dependencies"] += 1
                        validation_results["details"][dep_name] = {"success": True, "valid": True}
                    else:
                        validation_results["failed_dependencies"] += 1
                        validation_results["details"][dep_name] = {"success": False, "error": "í•„ìˆ˜ ë©”ì„œë“œ ëˆ„ë½"}
                        validation_results["validation_errors"].append(f"{dep_name}: í•„ìˆ˜ ë©”ì„œë“œ ëˆ„ë½")
                else:
                    validation_results["failed_dependencies"] += 1
                    validation_results["details"][dep_name] = {"success": False, "error": "ì˜ì¡´ì„± ì—†ìŒ"}
                    validation_results["required_missing"].append(dep_name)
            
            validation_results["success"] = len(validation_results["required_missing"]) == 0
            return validation_results
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "github_compatible": False,
                "real_ai_ready": False
            }
    
    # ë‚˜ë¨¸ì§€ ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤ (v18.0ê³¼ ë™ì¼í•˜ì§€ë§Œ GitHub ìµœì í™” ì¶”ê°€)
    def inject_memory_manager(self, memory_manager: 'MemoryManager') -> bool:
        """GitHub í˜¸í™˜ MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        try:
            with self._lock:
                self.dependencies['memory_manager'] = memory_manager
                self.dependency_status.memory_manager = True
                
                # GitHub M3 Max íŠ¹ë³„ ì„¤ì •
                if self.is_m3_max and hasattr(memory_manager, 'configure_github_m3_max'):
                    memory_manager.configure_github_m3_max(self.memory_gb)
                
                return True
        except Exception as e:
            self.logger.error(f"âŒ GitHub MemoryManager ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def inject_data_converter(self, data_converter: 'DataConverter') -> bool:
        """GitHub í˜¸í™˜ DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        try:
            with self._lock:
                self.dependencies['data_converter'] = data_converter
                self.dependency_status.data_converter = True
                return True
        except Exception as e:
            self.logger.error(f"âŒ GitHub DataConverter ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def get_dependency(self, name: str) -> Optional[Any]:
        """ì˜ì¡´ì„± ì¡°íšŒ"""
        with self._lock:
            return self.dependencies.get(name)
    
    def auto_inject_dependencies(self) -> bool:
        """ìë™ ì˜ì¡´ì„± ì£¼ì… (GitHub í™˜ê²½ ìµœì í™”)"""
        if self._auto_injection_attempted:
            return True
        
        self._auto_injection_attempted = True
        success_count = 0
        
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} GitHub ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
            
            # ModelLoader ìë™ ì£¼ì…
            if not self.dependency_status.model_loader:
                model_loader = self._get_global_model_loader()
                if model_loader:
                    if self.inject_model_loader(model_loader):
                        success_count += 1
            
            # MemoryManager ìë™ ì£¼ì…
            if not self.dependency_status.memory_manager:
                memory_manager = self._get_global_memory_manager()
                if memory_manager:
                    if self.inject_memory_manager(memory_manager):
                        success_count += 1
            
            self.logger.info(f"ğŸ”„ {self.step_name} GitHub ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {success_count}ê°œ")
            return success_count > 0
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ {self.step_name} GitHub ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def _get_global_model_loader(self) -> Optional['ModelLoader']:
        """ModelLoader ë™ì  import (GitHub í™˜ê²½ ìµœì í™”)"""
        try:
            import importlib
            module = importlib.import_module('app.ai_pipeline.utils.model_loader')
            get_global = getattr(module, 'get_global_model_loader', None)
            if get_global:
                # GitHub í™˜ê²½ ìµœì í™” ì„¤ì •
                config = {
                    'conda_env': self.conda_info['conda_env'],
                    'is_m3_max': self.is_m3_max,
                    'memory_gb': self.memory_gb,
                    'enable_conda_optimization': self.conda_info['is_target_env'],
                    'github_mode': True
                }
                return get_global(config)
        except Exception as e:
            self.logger.debug(f"GitHub ModelLoader ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
        return None
    
    def _get_global_memory_manager(self) -> Optional['MemoryManager']:
        """MemoryManager ë™ì  import (GitHub M3 Max ìµœì í™”)"""
        try:
            import importlib
            module = importlib.import_module('app.ai_pipeline.utils.memory_manager')
            get_global = getattr(module, 'get_global_memory_manager', None)
            if get_global:
                return get_global()
        except Exception as e:
            self.logger.debug(f"GitHub MemoryManager ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
        return None
    
    def get_github_status(self) -> Dict[str, Any]:
        """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ìƒíƒœ ì¡°íšŒ (v19.0)"""
        return {
            'step_name': self.step_name,
            'github_compatibility': {
                'compatible': self.dependency_status.github_compatible,
                'process_method_validated': self.dependency_status.process_method_validated,
                'real_ai_models_loaded': self.dependency_status.real_ai_models_loaded,
                'signature_format': self._process_method_signature.value if self._process_method_signature else 'unknown',
                'validation_format': self._dependency_validation_format.value
            },
            'dependency_status': {
                'model_loader': self.dependency_status.model_loader,
                'step_interface': self.dependency_status.step_interface,
                'memory_manager': self.dependency_status.memory_manager,
                'data_converter': self.dependency_status.data_converter
            },
            'environment': {
                'conda_optimized': self.dependency_status.conda_optimized,
                'm3_max_optimized': self.dependency_status.m3_max_optimized,
                'conda_env': self.conda_info['conda_env'],
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb
            }
        }

# ==============================================
# ğŸ”¥ BaseStepMixin v19.0 - GitHub í”„ë¡œì íŠ¸ ì™„ì „ í˜¸í™˜
# ==============================================

class BaseStepMixin:
    """
    ğŸ”¥ BaseStepMixin v19.0 - GitHub í”„ë¡œì íŠ¸ ì™„ì „ í˜¸í™˜
    
    í•µì‹¬ ê°œì„ ì‚¬í•­:
    âœ… GitHub Step í´ë˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜
    âœ… process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ì™„ì „ í‘œì¤€í™”  
    âœ… validate_dependencies() ì˜¤ë²„ë¡œë“œ ì§€ì›
    âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›
    âœ… StepFactory v9.0ê³¼ ì™„ì „ í˜¸í™˜
    âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™”
    âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
    """
    
    def __init__(self, **kwargs):
        """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì´ˆê¸°í™” (v19.0)"""
        try:
            # ê¸°ë³¸ ì„¤ì • (GitHub í˜¸í™˜)
            self.config = self._create_github_config(**kwargs)
            self.step_name = kwargs.get('step_name', self.__class__.__name__)
            self.step_id = kwargs.get('step_id', 0)
            
            # Logger ì„¤ì •
            self.logger = logging.getLogger(f"steps.{self.step_name}")
            if not self.logger.handlers:
                handler = logging.StreamHandler()
                formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
                handler.setFormatter(formatter)
                self.logger.addHandler(handler)
                self.logger.setLevel(logging.INFO)
            
            # ğŸ”¥ GitHub í˜¸í™˜ ì˜ì¡´ì„± ê´€ë¦¬ì (v19.0)
            self.dependency_manager = GitHubDependencyManager(self.step_name)
            
            # GitHub í‘œì¤€ ìƒíƒœ í”Œë˜ê·¸ë“¤
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # ì‹œìŠ¤í…œ ì •ë³´ (í™˜ê²½ ìµœì í™”)
            self.device = self._resolve_device(self.config.device)
            self.is_m3_max = IS_M3_MAX
            self.memory_gb = MEMORY_GB
            self.conda_info = CONDA_INFO
            
            # GitHub í˜¸í™˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ (v19.0)
            self.performance_metrics = GitHubPerformanceMetrics()
            
            # GitHub í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±ë“¤
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            # GitHub íŠ¹ë³„ ì†ì„±ë“¤ (v19.0 ì‹ ê·œ)
            self.github_compatible = True
            self.real_ai_pipeline_ready = False
            self.process_method_signature = self.config.process_method_signature
            
            # í™˜ê²½ ìµœì í™” ì„¤ì • ì ìš©
            self._apply_github_environment_optimization()
            
            # ìë™ ì˜ì¡´ì„± ì£¼ì… (ì„¤ì •ëœ ê²½ìš°)
            if self.config.auto_inject_dependencies:
                try:
                    self.dependency_manager.auto_inject_dependencies()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {self.step_name} ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            
            self.logger.info(f"âœ… {self.step_name} BaseStepMixin v19.1 GitHub í˜¸í™˜ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self._github_emergency_setup(e)
    
    def _create_github_config(self, **kwargs) -> GitHubStepConfig:
        """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì„¤ì • ìƒì„±"""
        config = GitHubStepConfig()
        for key, value in kwargs.items():
            if hasattr(config, key):
                setattr(config, key, value)
        
        # GitHub í”„ë¡œì íŠ¸ íŠ¹ë³„ ì„¤ì •
        config.github_compatibility_mode = True
        config.real_ai_pipeline_support = True
        
        # í™˜ê²½ë³„ ì„¤ì • ì ìš©
        if CONDA_INFO['is_target_env']:
            config.conda_optimized = True
            config.conda_env = CONDA_INFO['conda_env']
        
        if IS_M3_MAX:
            config.m3_max_optimized = True
            config.memory_gb = MEMORY_GB
            config.use_unified_memory = True
        
        return config
    
    def _apply_github_environment_optimization(self):
        """GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ìµœì í™”"""
        try:
            # M3 Max GitHub ìµœì í™”
            if self.is_m3_max:
                if self.device == "auto" and MPS_AVAILABLE:
                    self.device = "mps"
                
                if self.config.batch_size == 1 and self.memory_gb >= 64:
                    self.config.batch_size = 2
                
                self.config.auto_memory_cleanup = True
                self.logger.debug(f"âœ… GitHub M3 Max ìµœì í™”: {self.memory_gb:.1f}GB, device={self.device}")
            
            # GitHub conda í™˜ê²½ ìµœì í™”
            if self.conda_info['is_target_env']:
                self.config.optimization_enabled = True
                self.real_ai_pipeline_ready = True
                self.logger.debug(f"âœ… GitHub conda í™˜ê²½ ìµœì í™”: {self.conda_info['conda_env']}")
            
        except Exception as e:
            self.logger.debug(f"GitHub í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _github_emergency_setup(self, error: Exception):
        """GitHub í˜¸í™˜ ê¸´ê¸‰ ì„¤ì •"""
        self.step_name = getattr(self, 'step_name', self.__class__.__name__)
        self.logger = logging.getLogger("github_emergency")
        self.device = "cpu"
        self.is_initialized = False
        self.github_compatible = False
        self.performance_metrics = GitHubPerformanceMetrics()
        self.logger.error(f"ğŸš¨ {self.step_name} GitHub ê¸´ê¸‰ ì´ˆê¸°í™”: {error}")
    
    # ==============================================
    # ğŸ”¥ GitHub í‘œì¤€í™”ëœ ì˜ì¡´ì„± ì£¼ì… ì¸í„°í˜ì´ìŠ¤ (v19.0)
    # ==============================================
    
    def set_model_loader(self, model_loader: 'ModelLoader'):
        """GitHub í‘œì¤€ ModelLoader ì˜ì¡´ì„± ì£¼ì… (v19.0)"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} GitHub ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
            
            # GitHub ì˜ì¡´ì„± ê´€ë¦¬ìë¥¼ í†µí•œ ì£¼ì…
            success = self.dependency_manager.inject_model_loader(model_loader)
            
            if success:
                # GitHub í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„± ì„¤ì •
                self.model_loader = model_loader
                self.model_interface = self.dependency_manager.get_dependency('step_interface')
                
                # GitHub í‘œì¤€ ìƒíƒœ í”Œë˜ê·¸ ì—…ë°ì´íŠ¸
                self.has_model = True
                self.model_loaded = True
                self.real_ai_pipeline_ready = True
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                self.performance_metrics.dependencies_injected += 1
                
                self.logger.info(f"âœ… {self.step_name} GitHub ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            else:
                self.logger.error(f"âŒ {self.step_name} GitHub ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨")
                if self.config.strict_mode:
                    raise RuntimeError(f"GitHub Strict Mode: ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨")
                
        except Exception as e:
            self.performance_metrics.injection_failures += 1
            self.logger.error(f"âŒ {self.step_name} GitHub ModelLoader ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜: {e}")
            if self.config.strict_mode:
                raise
    
    def set_memory_manager(self, memory_manager: 'MemoryManager'):
        """GitHub í‘œì¤€ MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        try:
            success = self.dependency_manager.inject_memory_manager(memory_manager)
            if success:
                self.memory_manager = memory_manager
                self.performance_metrics.dependencies_injected += 1
                self.logger.debug(f"âœ… {self.step_name} GitHub MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.performance_metrics.injection_failures += 1
            self.logger.warning(f"âš ï¸ {self.step_name} GitHub MemoryManager ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜: {e}")
    
    def set_data_converter(self, data_converter: 'DataConverter'):
        """GitHub í‘œì¤€ DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        try:
            success = self.dependency_manager.inject_data_converter(data_converter)
            if success:
                self.data_converter = data_converter
                self.performance_metrics.dependencies_injected += 1
                self.logger.debug(f"âœ… {self.step_name} GitHub DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.performance_metrics.injection_failures += 1
            self.logger.warning(f"âš ï¸ {self.step_name} GitHub DataConverter ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜: {e}")
    
    def set_di_container(self, di_container: 'DIContainer'):
        """GitHub í‘œì¤€ DI Container ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.di_container = di_container
            self.performance_metrics.dependencies_injected += 1
            self.logger.debug(f"âœ… {self.step_name} GitHub DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.performance_metrics.injection_failures += 1
            self.logger.warning(f"âš ï¸ {self.step_name} GitHub DI Container ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜: {e}")
    
    # ==============================================
    # ğŸ”¥ GitHub í˜¸í™˜ ì˜ì¡´ì„± ê²€ì¦ (v19.0 í•µì‹¬ ê¸°ëŠ¥)
    # ==============================================
    
    def validate_dependencies(self, format_type: DependencyValidationFormat = None) -> Union[Dict[str, bool], Dict[str, Any]]:
        """
        GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì˜ì¡´ì„± ê²€ì¦ (v19.0 í•µì‹¬)
        
        ë°˜í™˜ í˜•ì‹:
        - DependencyValidationFormat.BOOLEAN_DICT: {'model_loader': True, 'step_interface': False, ...}
        - DependencyValidationFormat.DETAILED_DICT: {'success': True, 'details': {...}, ...}
        - DependencyValidationFormat.AUTO_DETECT: í˜¸ì¶œìì— ë”°ë¼ ìë™ ì„ íƒ
        """
        try:
            return self.dependency_manager.validate_dependencies_github_format(format_type)
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} GitHub ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            
            # ì—ëŸ¬ ì‹œ ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
            if format_type == DependencyValidationFormat.BOOLEAN_DICT:
                return {'model_loader': False, 'step_interface': False, 'memory_manager': False, 'data_converter': False}
            else:
                return {'success': False, 'error': str(e), 'github_compatible': False}
    
    # GitHub Step í´ë˜ìŠ¤ í˜¸í™˜ì„ ìœ„í•œ ë³„ì¹­ ë©”ì„œë“œ
    def validate_dependencies_boolean(self) -> Dict[str, bool]:
        """GitHub Step í´ë˜ìŠ¤ í˜¸í™˜ (GeometricMatchingStep ë“±)"""
        return self.validate_dependencies(DependencyValidationFormat.BOOLEAN_DICT)
    
    def validate_dependencies_detailed(self) -> Dict[str, Any]:
        """StepFactory í˜¸í™˜ (ìƒì„¸ ì •ë³´)"""
        return self.validate_dependencies(DependencyValidationFormat.DETAILED_DICT)
    
    # ==============================================
    # ğŸ”¥ GitHub í‘œì¤€ process ë©”ì„œë“œ ì§€ì› (v19.1 ë°ì´í„° ì „ë‹¬ ìµœì í™”)
    # ==============================================
    
    async def process(self, **kwargs) -> Dict[str, Any]:
        """
        GitHub í”„ë¡œì íŠ¸ í‘œì¤€ process ë©”ì„œë“œ (v19.1 ë°ì´í„° ì „ë‹¬ ìµœì í™”)
        
        ì´ ë©”ì„œë“œëŠ” ëª¨ë“  kwargsë¥¼ ë°›ì•„ì„œ ì‹¤ì œ Stepì˜ process ë©”ì„œë“œë¡œ 
        ì˜¬ë°”ë¥¸ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì „ë‹¬í•©ë‹ˆë‹¤.
        """
        try:
            # GitHub í†µê³„ ì—…ë°ì´íŠ¸
            self.performance_metrics.github_process_calls += 1
            
            # í•˜ìœ„ í´ë˜ìŠ¤ì˜ ì‹¤ì œ process ë©”ì„œë“œ ì°¾ê¸°
            actual_process_method = self._find_actual_process_method()
            
            if actual_process_method and actual_process_method != self.process:
                # ì‹¤ì œ process ë©”ì„œë“œê°€ ìˆëŠ” ê²½ìš°, ì‹œê·¸ë‹ˆì²˜ì— ë§ê²Œ ë°ì´í„° ë³€í™˜
                converted_args, converted_kwargs = self._convert_process_arguments(
                    actual_process_method, **kwargs
                )
                
                # ì‹¤ì œ Stepì˜ process ë©”ì„œë“œ í˜¸ì¶œ
                if asyncio.iscoroutinefunction(actual_process_method):
                    return await actual_process_method(*converted_args, **converted_kwargs)
                else:
                    return actual_process_method(*converted_args, **converted_kwargs)
            
            # ê¸°ë³¸ ì²˜ë¦¬ ë¡œì§ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì¬ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš°)
            self.logger.warning(f"âš ï¸ {self.step_name} process ë©”ì„œë“œê°€ ì¬ì •ì˜ë˜ì§€ ì•ŠìŒ")
            
            return {
                'success': True,
                'message': f'{self.step_name} ê¸°ë³¸ ì²˜ë¦¬ ì™„ë£Œ',
                'step_name': self.step_name,
                'step_id': self.step_id,
                'github_compatible': self.github_compatible,
                'real_ai_ready': self.real_ai_pipeline_ready,
                'inputs_received': list(kwargs.keys()),
                'note': 'BaseStepMixin ê¸°ë³¸ êµ¬í˜„ - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì¬ì •ì˜ í•„ìš”'
            }
            
        except Exception as e:
            self.performance_metrics.error_count += 1
            self.logger.error(f"âŒ {self.step_name} GitHub process ì‹¤íŒ¨: {e}")
            
            return {
                'success': False,
                'error': str(e),
                'step_name': self.step_name,
                'github_compatible': self.github_compatible
            }
    
    def _find_actual_process_method(self):
        """ì‹¤ì œ Stepì˜ process ë©”ì„œë“œ ì°¾ê¸° (BaseStepMixinì˜ process ì œì™¸)"""
        try:
            # í´ë˜ìŠ¤ hierarchyì—ì„œ ì‹¤ì œ êµ¬í˜„ëœ process ë©”ì„œë“œ ì°¾ê¸°
            for cls in self.__class__.__mro__:
                if cls == BaseStepMixin:
                    continue  # BaseStepMixinì˜ processëŠ” ì œì™¸
                
                if 'process' in cls.__dict__:
                    actual_method = getattr(self, 'process')
                    
                    # ë©”ì„œë“œê°€ BaseStepMixinì˜ ê²ƒì´ ì•„ë‹Œì§€ í™•ì¸
                    if actual_method.__func__ != BaseStepMixin.process.__func__:
                        return actual_method
            
            return None
            
        except Exception as e:
            self.logger.debug(f"ì‹¤ì œ process ë©”ì„œë“œ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def _convert_process_arguments(self, actual_process_method, **kwargs):
        """
        kwargsë¥¼ ì‹¤ì œ Stepì˜ process ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ì— ë§ê²Œ ë³€í™˜
        
        ì˜ˆì‹œ:
        - GeometricMatchingStep.process(person_image, clothing_image, **kwargs)
        - ClothSegmentationStep.process(input_data, clothing_type=None, **kwargs)
        """
        try:
            import inspect
            
            # ì‹¤ì œ ë©”ì„œë“œì˜ ì‹œê·¸ë‹ˆì²˜ ë¶„ì„
            sig = inspect.signature(actual_process_method)
            params = list(sig.parameters.keys())
            
            converted_args = []
            converted_kwargs = kwargs.copy()
            
            # self íŒŒë¼ë¯¸í„° ì œì™¸
            if 'self' in params:
                params.remove('self')
            
            # ìœ„ì¹˜ ì¸ìë“¤ ë³€í™˜
            for param_name in params:
                param = sig.parameters[param_name]
                
                # **kwargs íŒŒë¼ë¯¸í„°ëŠ” ê±´ë„ˆë›°ê¸°
                if param.kind == inspect.Parameter.VAR_KEYWORD:
                    continue
                
                # *args íŒŒë¼ë¯¸í„°ëŠ” ê±´ë„ˆë›°ê¸°  
                if param.kind == inspect.Parameter.VAR_POSITIONAL:
                    continue
                
                # ìœ„ì¹˜ ì¸ì ë˜ëŠ” ê¸°ë³¸ê°’ì´ ì—†ëŠ” ê²½ìš°
                if param.default == inspect.Parameter.empty:
                    # kwargsì—ì„œ í•´ë‹¹ ì¸ì ì°¾ì•„ì„œ ìœ„ì¹˜ ì¸ìë¡œ ë³€í™˜
                    if param_name in converted_kwargs:
                        converted_args.append(converted_kwargs.pop(param_name))
                    else:
                        # ì¼ë°˜ì ì¸ ì´ë¦„ ë§¤í•‘ ì‹œë„
                        mapped_value = self._map_common_parameter_names(param_name, converted_kwargs)
                        if mapped_value is not None:
                            converted_args.append(mapped_value)
                        else:
                            self.logger.warning(f"âš ï¸ í•„ìˆ˜ íŒŒë¼ë¯¸í„° {param_name}ì„ kwargsì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            self.logger.debug(f"âœ… ì¸ì ë³€í™˜ ì™„ë£Œ: args={len(converted_args)}, kwargs={list(converted_kwargs.keys())}")
            return converted_args, converted_kwargs
            
        except Exception as e:
            self.logger.error(f"âŒ ì¸ì ë³€í™˜ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ì‹œ ëª¨ë“  ë°ì´í„°ë¥¼ kwargsë¡œ ì „ë‹¬
            return [], kwargs
    
    def _map_common_parameter_names(self, param_name: str, kwargs: Dict[str, Any]):
        """ì¼ë°˜ì ì¸ íŒŒë¼ë¯¸í„° ì´ë¦„ ë§¤í•‘"""
        try:
            # ì¼ë°˜ì ì¸ ì´ë¦„ ë§¤í•‘ ê·œì¹™
            name_mappings = {
                'person_image': ['person_image', 'image', 'input_image', 'user_image'],
                'clothing_image': ['clothing_image', 'cloth_image', 'garment_image', 'item_image'], 
                'input_data': ['input_data', 'data', 'image', 'person_image'],
                'image': ['image', 'input_image', 'person_image', 'input_data'],
                'fitted_image': ['fitted_image', 'image', 'result_image'],
                'final_image': ['final_image', 'image', 'result_image'],
                'measurements': ['measurements', 'body_measurements', 'user_measurements'],
                'session_id': ['session_id', 'sessionId'],
                'clothing_type': ['clothing_type', 'cloth_type', 'garment_type'],
                'quality_level': ['quality_level', 'quality']
            }
            
            # ë§¤í•‘ ê·œì¹™ì— ë”°ë¼ ê°’ ì°¾ê¸°
            possible_names = name_mappings.get(param_name, [param_name])
            
            for name in possible_names:
                if name in kwargs:
                    value = kwargs.pop(name)
                    self.logger.debug(f"âœ… íŒŒë¼ë¯¸í„° ë§¤í•‘: {param_name} <- {name}")
                    return value
            
            return None
            
        except Exception as e:
            self.logger.debug(f"íŒŒë¼ë¯¸í„° ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return None
    
    # GitHub í˜¸í™˜ì„ ìœ„í•œ ì¶”ê°€ process ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ë“¤
    async def process_pipeline(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """GitHub íŒŒì´í”„ë¼ì¸ ëª¨ë“œ process ë©”ì„œë“œ"""
        return await self.process(**input_data)
    
    def process_sync(self, **kwargs) -> Dict[str, Any]:
        """GitHub ë™ê¸° process ë©”ì„œë“œ (ë ˆê±°ì‹œ í˜¸í™˜)"""
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” íƒœìŠ¤í¬ ìƒì„±
                task = asyncio.create_task(self.process(**kwargs))
                return {'success': False, 'error': 'async_required', 'task': task}
            else:
                return loop.run_until_complete(self.process(**kwargs))
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    # ==============================================
    # ğŸ”¥ í•µì‹¬ ê¸°ëŠ¥ ë©”ì„œë“œë“¤ (v19.0 GitHub ìµœì í™”)
    # ==============================================
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """GitHub í˜¸í™˜ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            start_time = time.time()
            
            # GitHub Step Interface ìš°ì„  ì‚¬ìš©
            step_interface = self.dependency_manager.get_dependency('step_interface')
            if step_interface and hasattr(step_interface, 'get_model_sync'):
                model = step_interface.get_model_sync(model_name or "default")
                if model:
                    self.performance_metrics.cache_hits += 1
                    self.performance_metrics.real_ai_inferences += 1
                    return model
            
            # GitHub ModelLoader ì§ì ‘ ì‚¬ìš©
            model_loader = self.dependency_manager.get_dependency('model_loader')
            if model_loader and hasattr(model_loader, 'load_model'):
                model = model_loader.load_model(model_name or "default")
                if model:
                    self.performance_metrics.models_loaded += 1
                    self.performance_metrics.real_ai_inferences += 1
                    return model
            
            self.logger.warning("âš ï¸ GitHub ëª¨ë¸ ì œê³µìê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            self.performance_metrics.error_count += 1
            return None
        finally:
            process_time = time.time() - start_time
            self._update_github_performance_metrics(process_time)
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[Any]:
        """GitHub í˜¸í™˜ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # GitHub Step Interface ìš°ì„  ì‚¬ìš©
            step_interface = self.dependency_manager.get_dependency('step_interface')
            if step_interface and hasattr(step_interface, 'get_model_async'):
                model = await step_interface.get_model_async(model_name or "default")
                if model:
                    self.performance_metrics.real_ai_inferences += 1
                    return model
            
            # ë™ê¸° ë©”ì„œë“œ í´ë°±
            return self.get_model(model_name)
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """GitHub í˜¸í™˜ ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            start_time = time.time()
            
            # GitHub MemoryManager ìš°ì„  ì‚¬ìš©
            memory_manager = self.dependency_manager.get_dependency('memory_manager')
            if memory_manager and hasattr(memory_manager, 'optimize_memory'):
                result = memory_manager.optimize_memory(aggressive=aggressive)
                self.performance_metrics.memory_optimizations += 1
                return result
            
            # GitHub ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™”
            result = self._github_builtin_memory_optimize(aggressive)
            self.performance_metrics.memory_optimizations += 1
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "github_mode": True}
        finally:
            optimization_time = time.time() - start_time
            self.logger.debug(f"ğŸ§¹ GitHub ë©”ëª¨ë¦¬ ìµœì í™” ì†Œìš” ì‹œê°„: {optimization_time:.3f}ì´ˆ")
    
    # ==============================================
    # ğŸ”¥ GitHub í‘œì¤€í™”ëœ ì´ˆê¸°í™” ë° ì›Œë°ì—… (v19.0)
    # ==============================================
    
    def initialize(self) -> bool:
        """GitHub í‘œì¤€ ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"ğŸ”„ {self.step_name} GitHub í‘œì¤€ ì´ˆê¸°í™” ì‹œì‘...")
            
            # GitHub ì˜ì¡´ì„± í™•ì¸
            if not self._check_github_required_dependencies():
                if self.config.strict_mode:
                    raise RuntimeError("GitHub í•„ìˆ˜ ì˜ì¡´ì„±ì´ ì£¼ì…ë˜ì§€ ì•ŠìŒ")
                else:
                    self.logger.warning("âš ï¸ GitHub ì¼ë¶€ ì˜ì¡´ì„±ì´ ëˆ„ë½ë¨")
            
            # GitHub process ë©”ì„œë“œ ê²€ì¦
            self._validate_github_process_method()
            
            # GitHub í™˜ê²½ë³„ ì´ˆê¸°í™”
            self._github_environment_specific_initialization()
            
            # ì´ˆê¸°í™” ìƒíƒœ ì„¤ì •
            self.dependency_manager.dependency_status.base_initialized = True
            self.dependency_manager.dependency_status.github_compatible = True
            self.is_initialized = True
            
            self.logger.info(f"âœ… {self.step_name} GitHub í‘œì¤€ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} GitHub ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.performance_metrics.error_count += 1
            return False
    
    def _check_github_required_dependencies(self) -> bool:
        """GitHub í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸"""
        required_deps = []
        
        if self.config.require_model_loader:
            required_deps.append('model_loader')
        if self.config.require_memory_manager:
            required_deps.append('memory_manager')
        if self.config.require_data_converter:
            required_deps.append('data_converter')
        
        validation_result = self.validate_dependencies(DependencyValidationFormat.BOOLEAN_DICT)
        
        for dep in required_deps:
            if not validation_result.get(dep, False):
                return False
        
        return True
    
    def _validate_github_process_method(self):
        """GitHub process ë©”ì„œë“œ ê²€ì¦"""
        try:
            process_method = getattr(self, 'process', None)
            if not process_method:
                self.logger.warning("âš ï¸ GitHub process ë©”ì„œë“œê°€ ì—†ìŒ")
                return
            
            # ì‹œê·¸ë‹ˆì²˜ ê²€ì¦
            sig = inspect.signature(process_method)
            params = list(sig.parameters.keys())
            
            # GitHub í‘œì¤€ ì‹œê·¸ë‹ˆì²˜ í™•ì¸
            if 'self' in params and len(params) >= 1:
                self.dependency_manager.dependency_status.process_method_validated = True
                self.logger.debug("âœ… GitHub process ë©”ì„œë“œ ê²€ì¦ ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ GitHub process ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ë¹„í‘œì¤€")
                
        except Exception as e:
            self.logger.debug(f"GitHub process ë©”ì„œë“œ ê²€ì¦ ì‹¤íŒ¨: {e}")
    
    def _github_environment_specific_initialization(self):
        """GitHub í™˜ê²½ë³„ íŠ¹ë³„ ì´ˆê¸°í™”"""
        try:
            # GitHub M3 Max íŠ¹ë³„ ì´ˆê¸°í™”
            if self.is_m3_max:
                if TORCH_AVAILABLE and self.device == "mps":
                    try:
                        test_tensor = torch.randn(10, 10, device=self.device)
                        _ = torch.matmul(test_tensor, test_tensor.t())
                        self.logger.debug("âœ… GitHub M3 Max MPS ì›Œë°ì—… ì™„ë£Œ")
                        self.real_ai_pipeline_ready = True
                    except Exception as mps_error:
                        self.logger.debug(f"GitHub M3 Max MPS ì›Œë°ì—… ì‹¤íŒ¨: {mps_error}")
            
            # GitHub conda í™˜ê²½ íŠ¹ë³„ ì´ˆê¸°í™”
            if self.conda_info['is_target_env']:
                os.environ['PYTHONPATH'] = self.conda_info['conda_prefix'] + '/lib/python3.11/site-packages'
                self.real_ai_pipeline_ready = True
                self.logger.debug("âœ… GitHub conda í™˜ê²½ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.debug(f"GitHub í™˜ê²½ë³„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def initialize_async(self) -> bool:
        """GitHub ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.initialize)
        except Exception as e:
            self.logger.error(f"âŒ GitHub ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def warmup(self) -> Dict[str, Any]:
        """GitHub í‘œì¤€ ì›Œë°ì—… (v19.0)"""
        try:
            if self.warmup_completed:
                return {'success': True, 'message': 'GitHub ì›Œë°ì—… ì´ë¯¸ ì™„ë£Œë¨', 'cached': True}
            
            self.logger.info(f"ğŸ”¥ {self.step_name} GitHub í‘œì¤€ ì›Œë°ì—… ì‹œì‘...")
            start_time = time.time()
            results = []
            
            # 1. GitHub ì˜ì¡´ì„± ì›Œë°ì—…
            try:
                github_status = self.dependency_manager.get_github_status()
                if github_status.get('github_compatibility', {}).get('compatible', False):
                    results.append('github_dependency_success')
                else:
                    results.append('github_dependency_failed')
            except:
                results.append('github_dependency_failed')
            
            # 2. GitHub ë©”ëª¨ë¦¬ ì›Œë°ì—…
            try:
                memory_result = self.optimize_memory(aggressive=False)
                results.append('github_memory_success' if memory_result.get('success') else 'github_memory_failed')
            except:
                results.append('github_memory_failed')
            
            # 3. GitHub AI ëª¨ë¸ ì›Œë°ì—…
            try:
                test_model = self.get_model("github_warmup_test")
                results.append('github_model_success' if test_model else 'github_model_skipped')
            except:
                results.append('github_model_failed')
            
            # 4. GitHub ë””ë°”ì´ìŠ¤ ì›Œë°ì—…
            results.append(self._github_device_warmup())
            
            # 5. GitHub í™˜ê²½ë³„ íŠ¹ë³„ ì›Œë°ì—…
            if self.is_m3_max:
                results.append(self._github_m3_max_warmup())
            
            if self.conda_info['is_target_env']:
                results.append(self._github_conda_warmup())
            
            # 6. GitHub process ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
            results.append(self._github_process_warmup())
            
            duration = time.time() - start_time
            success_count = sum(1 for r in results if 'success' in r)
            overall_success = success_count > 0
            
            if overall_success:
                self.warmup_completed = True
                self.is_ready = True
                self.real_ai_pipeline_ready = True
            
            self.logger.info(f"ğŸ”¥ GitHub ì›Œë°ì—… ì™„ë£Œ: {success_count}/{len(results)} ì„±ê³µ ({duration:.2f}ì´ˆ)")
            
            return {
                "success": overall_success,
                "duration": duration,
                "results": results,
                "success_count": success_count,
                "total_count": len(results),
                "github_environment": {
                    "is_m3_max": self.is_m3_max,
                    "conda_optimized": self.conda_info['is_target_env'],
                    "device": self.device,
                    "real_ai_ready": self.real_ai_pipeline_ready
                },
                "github_status": self.dependency_manager.get_github_status()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "github_mode": True}
    
    def _github_device_warmup(self) -> str:
        """GitHub ë””ë°”ì´ìŠ¤ ì›Œë°ì—…"""
        try:
            if TORCH_AVAILABLE:
                test_tensor = torch.randn(100, 100)
                if self.device != 'cpu':
                    test_tensor = test_tensor.to(self.device)
                _ = torch.matmul(test_tensor, test_tensor.t())
                return 'github_device_success'
            else:
                return 'github_device_skipped'
        except:
            return 'github_device_failed'
    
    def _github_m3_max_warmup(self) -> str:
        """GitHub M3 Max íŠ¹ë³„ ì›Œë°ì—…"""
        try:
            if TORCH_AVAILABLE and MPS_AVAILABLE:
                # GitHub ì‹¤ì œ AI ëª¨ë¸ í¬ê¸° í…ŒìŠ¤íŠ¸
                large_tensor = torch.randn(2000, 2000, device='mps')
                _ = torch.matmul(large_tensor, large_tensor.t())
                del large_tensor
                return 'github_m3_max_success'
            return 'github_m3_max_skipped'
        except:
            return 'github_m3_max_failed'
    
    def _github_conda_warmup(self) -> str:
        """GitHub conda í™˜ê²½ ì›Œë°ì—…"""
        try:
            import sys
            conda_paths = [p for p in sys.path if 'conda' in p.lower() and 'mycloset-ai-clean' in p]
            if conda_paths:
                return 'github_conda_success'
            return 'github_conda_skipped'
        except:
            return 'github_conda_failed'
    
    def _github_process_warmup(self) -> str:
        """GitHub process ë©”ì„œë“œ ì›Œë°ì—…"""
        try:
            # process ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
            if hasattr(self, 'process') and callable(getattr(self, 'process')):
                # ì‹œê·¸ë‹ˆì²˜ í™•ì¸
                sig = inspect.signature(self.process)
                if 'kwargs' in str(sig) or len(sig.parameters) >= 1:
                    return 'github_process_success'
            return 'github_process_failed'
        except:
            return 'github_process_failed'
    
    async def warmup_async(self) -> Dict[str, Any]:
        """GitHub ë¹„ë™ê¸° ì›Œë°ì—…"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.warmup)
        except Exception as e:
            self.logger.error(f"âŒ GitHub ë¹„ë™ê¸° ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "github_mode": True}
    
    # ==============================================
    # ğŸ”¥ GitHub ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë° ëª¨ë‹ˆí„°ë§ (v19.0)
    # ==============================================
    
    def _update_github_performance_metrics(self, process_time: float):
        """GitHub ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (v19.0)"""
        try:
            self.performance_metrics.process_count += 1
            self.performance_metrics.total_process_time += process_time
            self.performance_metrics.average_process_time = (
                self.performance_metrics.total_process_time / 
                self.performance_metrics.process_count
            )
            
            # GitHub íŒŒì´í”„ë¼ì¸ ì„±ê³µë¥  ê³„ì‚°
            if self.performance_metrics.github_process_calls > 0:
                success_rate = (
                    (self.performance_metrics.github_process_calls - self.performance_metrics.error_count) /
                    self.performance_metrics.github_process_calls * 100
                )
                self.performance_metrics.pipeline_success_rate = success_rate
            
            # GitHub M3 Max ë©”ëª¨ë¦¬ ìµœì í™”
            if self.is_m3_max:
                try:
                    import psutil
                    memory_info = psutil.virtual_memory()
                    current_usage = memory_info.used / 1024**2  # MB
                    
                    if current_usage > self.performance_metrics.peak_memory_usage_mb:
                        self.performance_metrics.peak_memory_usage_mb = current_usage
                    
                    # GitHub ì´ë™ í‰ê· 
                    if self.performance_metrics.average_memory_usage_mb == 0:
                        self.performance_metrics.average_memory_usage_mb = current_usage
                    else:
                        self.performance_metrics.average_memory_usage_mb = (
                            self.performance_metrics.average_memory_usage_mb * 0.9 + 
                            current_usage * 0.1
                        )
                except:
                    pass
                    
        except Exception as e:
            self.logger.debug(f"GitHub ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def get_github_performance_metrics(self) -> Dict[str, Any]:
        """GitHub ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ (v19.0)"""
        try:
            return {
                'github_process_metrics': {
                    'github_process_calls': self.performance_metrics.github_process_calls,
                    'real_ai_inferences': self.performance_metrics.real_ai_inferences,
                    'pipeline_success_rate': round(self.performance_metrics.pipeline_success_rate, 2),
                    'process_count': self.performance_metrics.process_count,
                    'total_process_time': round(self.performance_metrics.total_process_time, 3),
                    'average_process_time': round(self.performance_metrics.average_process_time, 3),
                    'success_count': self.performance_metrics.success_count,
                    'error_count': self.performance_metrics.error_count,
                    'cache_hits': self.performance_metrics.cache_hits
                },
                'github_memory_metrics': {
                    'peak_memory_usage_mb': round(self.performance_metrics.peak_memory_usage_mb, 2),
                    'average_memory_usage_mb': round(self.performance_metrics.average_memory_usage_mb, 2),
                    'memory_optimizations': self.performance_metrics.memory_optimizations
                },
                'github_ai_model_metrics': {
                    'models_loaded': self.performance_metrics.models_loaded,
                    'total_model_size_gb': round(self.performance_metrics.total_model_size_gb, 2),
                    'inference_count': self.performance_metrics.inference_count
                },
                'github_dependency_metrics': {
                    'dependencies_injected': self.performance_metrics.dependencies_injected,
                    'injection_failures': self.performance_metrics.injection_failures,
                    'average_injection_time': round(self.performance_metrics.average_injection_time, 3),
                    'injection_success_rate': round(
                        (self.performance_metrics.dependencies_injected / 
                         max(1, self.performance_metrics.dependencies_injected + self.performance_metrics.injection_failures)) * 100, 2
                    )
                },
                'github_environment_metrics': {
                    'device': self.device,
                    'is_m3_max': self.is_m3_max,
                    'memory_gb': self.memory_gb,
                    'conda_optimized': self.conda_info['is_target_env'],
                    'real_ai_pipeline_ready': self.real_ai_pipeline_ready,
                    'github_compatible': self.github_compatible
                }
            }
        except Exception as e:
            self.logger.error(f"âŒ GitHub ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'github_mode': True}
    
    # ==============================================
    # ğŸ”¥ GitHub ìƒíƒœ ë° ì •ë¦¬ ë©”ì„œë“œë“¤ (v19.0)
    # ==============================================
    
    def get_status(self) -> Dict[str, Any]:
        """GitHub í†µí•© ìƒíƒœ ì¡°íšŒ (v19.0)"""
        try:
            return {
                'step_info': {
                    'step_name': self.step_name,
                    'step_id': self.step_id,
                    'version': 'BaseStepMixin v19.1 GitHub Compatible'
                },
                'github_status_flags': {
                    'is_initialized': self.is_initialized,
                    'is_ready': self.is_ready,
                    'has_model': self.has_model,
                    'model_loaded': self.model_loaded,
                    'warmup_completed': self.warmup_completed,
                    'github_compatible': self.github_compatible,
                    'real_ai_pipeline_ready': self.real_ai_pipeline_ready
                },
                'github_system_info': {
                    'device': self.device,
                    'is_m3_max': self.is_m3_max,
                    'memory_gb': self.memory_gb,
                    'conda_info': self.conda_info
                },
                'github_dependencies': self.dependency_manager.get_github_status(),
                'github_performance': self.get_github_performance_metrics(),
                'github_config': {
                    'device': self.config.device,
                    'use_fp16': self.config.use_fp16,
                    'batch_size': self.config.batch_size,
                    'confidence_threshold': self.config.confidence_threshold,
                    'auto_memory_cleanup': self.config.auto_memory_cleanup,
                    'auto_warmup': self.config.auto_warmup,
                    'optimization_enabled': self.config.optimization_enabled,
                    'strict_mode': self.config.strict_mode,
                    'github_compatibility_mode': self.config.github_compatibility_mode,
                    'real_ai_pipeline_support': self.config.real_ai_pipeline_support,
                    'process_method_signature': self.config.process_method_signature.value,
                    'dependency_validation_format': self.config.dependency_validation_format.value
                },
                'timestamp': time.time()
            }
        except Exception as e:
            self.logger.error(f"âŒ GitHub ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'version': 'BaseStepMixin v19.0 GitHub Compatible'}
    
    async def cleanup(self) -> Dict[str, Any]:
        """GitHub í‘œì¤€í™”ëœ ì •ë¦¬ (v19.0)"""
        try:
            self.logger.info(f"ğŸ§¹ {self.step_name} GitHub í‘œì¤€ ì •ë¦¬ ì‹œì‘...")
            
            # GitHub ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
            final_github_metrics = self.get_github_performance_metrics()
            
            # GitHub ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_result = await self.optimize_memory_async(aggressive=True)
            
            # GitHub ìƒíƒœ ë¦¬ì…‹
            self.is_ready = False
            self.warmup_completed = False
            self.has_model = False
            self.model_loaded = False
            self.real_ai_pipeline_ready = False
            
            # GitHub ì˜ì¡´ì„± í•´ì œ
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            # GitHub ì˜ì¡´ì„± ê´€ë¦¬ì ì •ë¦¬
            github_dependency_status = self.dependency_manager.get_github_status()
            
            # GitHub M3 Max íŠ¹ë³„ ì •ë¦¬
            if self.is_m3_max:
                for _ in range(5):
                    gc.collect()
                if TORCH_AVAILABLE and MPS_AVAILABLE:
                    try:
                        torch.mps.empty_cache()
                        self.logger.debug("âœ… GitHub M3 Max MPS ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                    except:
                        pass
            
            # GitHub CUDA ì •ë¦¬ (í˜¸í™˜ì„±)
            if TORCH_AVAILABLE and self.device == "cuda":
                try:
                    torch.cuda.empty_cache()
                    self.logger.debug("âœ… GitHub CUDA ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                except:
                    pass
            
            self.logger.info(f"âœ… {self.step_name} GitHub í‘œì¤€ ì •ë¦¬ ì™„ë£Œ")
            
            return {
                "success": True,
                "cleanup_result": cleanup_result,
                "final_github_metrics": final_github_metrics,
                "github_dependency_status": github_dependency_status,
                "step_name": self.step_name,
                "version": "BaseStepMixin v19.0 GitHub Compatible",
                "github_environment": {
                    "is_m3_max": self.is_m3_max,
                    "conda_optimized": self.conda_info['is_target_env'],
                    "real_ai_pipeline_ready": self.real_ai_pipeline_ready
                }
            }
        except Exception as e:
            self.logger.error(f"âŒ GitHub ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "github_mode": True}
    
    # ==============================================
    # ğŸ”¥ GitHub ë‚´ë¶€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (v19.0)
    # ==============================================
    
    def _resolve_device(self, device: str) -> str:
        """GitHub ë””ë°”ì´ìŠ¤ í•´ê²° (í™˜ê²½ ìµœì í™”)"""
        if device == "auto":
            # GitHub M3 Max ìš°ì„  ì²˜ë¦¬
            if IS_M3_MAX and MPS_AVAILABLE:
                return "mps"
            
            if TORCH_AVAILABLE:
                if torch.cuda.is_available():
                    return "cuda"
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    return "mps"
            return "cpu"
        return device
    
    def _github_builtin_memory_optimize(self, aggressive: bool = False) -> Dict[str, Any]:
        """GitHub ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            results = []
            start_time = time.time()
            
            # GitHub Python GC
            before = len(gc.get_objects()) if hasattr(gc, 'get_objects') else 0
            gc.collect()
            after = len(gc.get_objects()) if hasattr(gc, 'get_objects') else 0
            results.append(f"GitHub Python GC: {before - after}ê°œ ê°ì²´ í•´ì œ")
            
            # GitHub PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    if aggressive:
                        torch.cuda.ipc_collect()
                    results.append("GitHub CUDA ìºì‹œ ì •ë¦¬")
                
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        results.append("GitHub MPS ìºì‹œ ì •ë¦¬")
                    except:
                        results.append("GitHub MPS ìºì‹œ ì •ë¦¬ ì‹œë„")
            
            # GitHub M3 Max íŠ¹ë³„ ìµœì í™”
            if self.is_m3_max and aggressive:
                # GitHub í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”
                for _ in range(3):
                    gc.collect()
                results.append("GitHub M3 Max í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”")
            
            # GitHub conda í™˜ê²½ ìµœì í™”
            if self.conda_info['is_target_env'] and aggressive:
                # GitHub conda ìºì‹œ ì •ë¦¬
                results.append("GitHub conda í™˜ê²½ ìµœì í™”")
            
            # GitHub ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            memory_info = {}
            try:
                import psutil
                vm = psutil.virtual_memory()
                memory_info = {
                    'total_gb': round(vm.total / 1024**3, 2),
                    'available_gb': round(vm.available / 1024**3, 2),
                    'used_percent': vm.percent
                }
            except:
                memory_info = {'error': 'GitHub psutil_not_available'}
            
            duration = time.time() - start_time
            
            return {
                "success": True,
                "results": results,
                "duration": round(duration, 3),
                "device": self.device,
                "github_environment": {
                    "is_m3_max": self.is_m3_max,
                    "conda_optimized": self.conda_info['is_target_env'],
                    "memory_gb": self.memory_gb,
                    "real_ai_ready": self.real_ai_pipeline_ready
                },
                "memory_info": memory_info,
                "source": "github_builtin_optimized"
            }
            
        except Exception as e:
            return {
                "success": False, 
                "error": str(e), 
                "source": "github_builtin_optimized",
                "github_environment": {"is_m3_max": self.is_m3_max}
            }
    
    # ==============================================
    # ğŸ”¥ GitHub ì§„ë‹¨ ë° ë””ë²„ê¹… ë©”ì„œë“œë“¤ (v19.0)
    # ==============================================
    
    def diagnose(self) -> Dict[str, Any]:
        """GitHub Step ì§„ë‹¨ (v19.0)"""
        try:
            self.logger.info(f"ğŸ” {self.step_name} GitHub ì§„ë‹¨ ì‹œì‘...")
            
            diagnosis = {
                'timestamp': time.time(),
                'step_name': self.step_name,
                'version': 'BaseStepMixin v19.0 GitHub Compatible',
                'github_status': self.get_status(),
                'github_issues': [],
                'github_recommendations': [],
                'github_health_score': 100
            }
            
            # GitHub ì˜ì¡´ì„± ì§„ë‹¨
            github_dependency_status = self.dependency_manager.get_github_status()
            
            if not github_dependency_status['dependency_status']['model_loader']:
                diagnosis['github_issues'].append('GitHub ModelLoaderê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ')
                diagnosis['github_recommendations'].append('GitHub ModelLoader ì˜ì¡´ì„± ì£¼ì… í•„ìš”')
                diagnosis['github_health_score'] -= 30
            
            if not github_dependency_status['dependency_status']['step_interface']:
                diagnosis['github_issues'].append('GitHub StepModelInterfaceê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ')
                diagnosis['github_recommendations'].append('GitHub ModelLoaderì˜ create_step_interface í™•ì¸ í•„ìš”')
                diagnosis['github_health_score'] -= 25
            
            # GitHub í˜¸í™˜ì„± ì§„ë‹¨
            if not self.github_compatible:
                diagnosis['github_issues'].append('GitHub í˜¸í™˜ì„± ëª¨ë“œê°€ ë¹„í™œì„±í™”ë¨')
                diagnosis['github_recommendations'].append('GitHub í˜¸í™˜ì„± ëª¨ë“œ í™œì„±í™” í•„ìš”')
                diagnosis['github_health_score'] -= 20
            
            if not self.real_ai_pipeline_ready:
                diagnosis['github_issues'].append('GitHub ì‹¤ì œ AI íŒŒì´í”„ë¼ì¸ì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ')
                diagnosis['github_recommendations'].append('ì‹¤ì œ AI ëª¨ë¸ ë° í™˜ê²½ ì„¤ì • í™•ì¸ í•„ìš”')
                diagnosis['github_health_score'] -= 15
            
            # GitHub process ë©”ì„œë“œ ì§„ë‹¨
            if not hasattr(self, 'process') or not callable(getattr(self, 'process')):
                diagnosis['github_issues'].append('GitHub í‘œì¤€ process ë©”ì„œë“œê°€ ì—†ìŒ')
                diagnosis['github_recommendations'].append('async def process(self, **kwargs) -> Dict[str, Any] êµ¬í˜„ í•„ìš”')
                diagnosis['github_health_score'] -= 35
            
            # GitHub í™˜ê²½ ì§„ë‹¨
            if not self.conda_info['is_target_env']:
                diagnosis['github_issues'].append(f"GitHub ê¶Œì¥ conda í™˜ê²½ì´ ì•„ë‹˜: {self.conda_info['conda_env']}")
                diagnosis['github_recommendations'].append('mycloset-ai-clean conda í™˜ê²½ ì‚¬ìš© ê¶Œì¥')
                diagnosis['github_health_score'] -= 10
            
            # GitHub M3 Max ì§„ë‹¨
            if self.is_m3_max and self.device != "mps":
                diagnosis['github_issues'].append('GitHub M3 Maxì—ì„œ MPSë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ')
                diagnosis['github_recommendations'].append('M3 Maxì—ì„œ MPS ë””ë°”ì´ìŠ¤ ì‚¬ìš© ê¶Œì¥')
                diagnosis['github_health_score'] -= 15
            
            # GitHub ì„±ëŠ¥ ì§„ë‹¨
            github_performance = self.get_github_performance_metrics()
            if github_performance.get('github_process_metrics', {}).get('error_count', 0) > 0:
                error_count = github_performance['github_process_metrics']['error_count']
                process_count = github_performance['github_process_metrics']['process_count']
                if process_count > 0:
                    error_rate = error_count / process_count * 100
                    if error_rate > 10:
                        diagnosis['github_issues'].append(f"GitHub ë†’ì€ ì—ëŸ¬ìœ¨: {error_rate:.1f}%")
                        diagnosis['github_recommendations'].append('GitHub ì—ëŸ¬ ì›ì¸ ë¶„ì„ ë° í•´ê²° í•„ìš”')
                        diagnosis['github_health_score'] -= 25
            
            # GitHub ìµœì¢… ê±´ê°•ë„ ë³´ì •
            diagnosis['github_health_score'] = max(0, diagnosis['github_health_score'])
            
            if diagnosis['github_health_score'] >= 90:
                diagnosis['github_health_status'] = 'excellent'
            elif diagnosis['github_health_score'] >= 70:
                diagnosis['github_health_status'] = 'good'
            elif diagnosis['github_health_score'] >= 50:
                diagnosis['github_health_status'] = 'fair'
            else:
                diagnosis['github_health_status'] = 'poor'
            
            self.logger.info(f"ğŸ” {self.step_name} GitHub ì§„ë‹¨ ì™„ë£Œ (ê±´ê°•ë„: {diagnosis['github_health_score']}%)")
            
            return diagnosis
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} GitHub ì§„ë‹¨ ì‹¤íŒ¨: {e}")
            return {
                'error': str(e),
                'step_name': self.step_name,
                'version': 'BaseStepMixin v19.0 GitHub Compatible',
                'github_health_score': 0,
                'github_health_status': 'error'
            }
    
    def benchmark(self, iterations: int = 10) -> Dict[str, Any]:
        """GitHub ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (v19.0)"""
        try:
            self.logger.info(f"ğŸ“Š {self.step_name} GitHub ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ)...")
            
            benchmark_results = {
                'iterations': iterations,
                'step_name': self.step_name,
                'device': self.device,
                'github_environment': {
                    'is_m3_max': self.is_m3_max,
                    'memory_gb': self.memory_gb,
                    'conda_optimized': self.conda_info['is_target_env'],
                    'real_ai_ready': self.real_ai_pipeline_ready,
                    'github_compatible': self.github_compatible
                },
                'github_timings': [],
                'github_memory_usage': [],
                'github_dependency_timings': [],
                'github_process_timings': [],
                'github_errors': 0
            }
            
            for i in range(iterations):
                try:
                    start_time = time.time()
                    
                    # GitHub ê¸°ë³¸ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
                    if TORCH_AVAILABLE:
                        test_tensor = torch.randn(512, 512, device=self.device)
                        result = torch.matmul(test_tensor, test_tensor.t())
                        del test_tensor, result
                    
                    # GitHub ì˜ì¡´ì„± ì ‘ê·¼ ë²¤ì¹˜ë§ˆí¬
                    dependency_start = time.time()
                    model_loader = self.dependency_manager.get_dependency('model_loader')
                    step_interface = self.dependency_manager.get_dependency('step_interface')
                    dependency_time = time.time() - dependency_start
                    benchmark_results['github_dependency_timings'].append(dependency_time)
                    
                    # GitHub process ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
                    process_start = time.time()
                    if hasattr(self, 'process'):
                        # process ë©”ì„œë“œ ì¡´ì¬ í™•ì¸ë§Œ (ì‹¤ì œ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ)
                        pass
                    process_time = time.time() - process_start
                    benchmark_results['github_process_timings'].append(process_time)
                    
                    # GitHub ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸
                    memory_result = self.optimize_memory()
                    
                    timing = time.time() - start_time
                    benchmark_results['github_timings'].append(timing)
                    
                    # GitHub ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
                    try:
                        import psutil
                        memory_usage = psutil.virtual_memory().percent
                        benchmark_results['github_memory_usage'].append(memory_usage)
                    except:
                        benchmark_results['github_memory_usage'].append(0)
                    
                except Exception as e:
                    benchmark_results['github_errors'] += 1
                    self.logger.debug(f"GitHub ë²¤ì¹˜ë§ˆí¬ {i+1} ì‹¤íŒ¨: {e}")
            
            # GitHub í†µê³„ ê³„ì‚°
            if benchmark_results['github_timings']:
                benchmark_results['github_statistics'] = {
                    'min_time': min(benchmark_results['github_timings']),
                    'max_time': max(benchmark_results['github_timings']),
                    'avg_time': sum(benchmark_results['github_timings']) / len(benchmark_results['github_timings']),
                    'total_time': sum(benchmark_results['github_timings'])
                }
            
            if benchmark_results['github_dependency_timings']:
                benchmark_results['github_dependency_statistics'] = {
                    'min_dependency_time': min(benchmark_results['github_dependency_timings']),
                    'max_dependency_time': max(benchmark_results['github_dependency_timings']),
                    'avg_dependency_time': sum(benchmark_results['github_dependency_timings']) / len(benchmark_results['github_dependency_timings'])
                }
            
            if benchmark_results['github_memory_usage']:
                benchmark_results['github_memory_statistics'] = {
                    'min_memory': min(benchmark_results['github_memory_usage']),
                    'max_memory': max(benchmark_results['github_memory_usage']),
                    'avg_memory': sum(benchmark_results['github_memory_usage']) / len(benchmark_results['github_memory_usage'])
                }
            
            benchmark_results['github_success_rate'] = (
                (iterations - benchmark_results['github_errors']) / iterations * 100
            )
            
            self.logger.info(f"ğŸ“Š {self.step_name} GitHub ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ (ì„±ê³µë¥ : {benchmark_results['github_success_rate']:.1f}%)")
            
            return benchmark_results
            
        except Exception as e:
            self.logger.error(f"âŒ GitHub ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'step_name': self.step_name, 'github_mode': True}

# ==============================================
# ğŸ”¥ GitHub í¸ì˜ í•¨ìˆ˜ë“¤ (BaseStepMixin v19.0 ì „ìš©)
# ==============================================

def create_github_base_step_mixin(**kwargs) -> BaseStepMixin:
    """GitHub í˜¸í™˜ BaseStepMixin ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    kwargs.setdefault('github_compatibility_mode', True)
    kwargs.setdefault('real_ai_pipeline_support', True)
    return BaseStepMixin(**kwargs)

def validate_github_step_environment() -> Dict[str, Any]:
    """GitHub Step í™˜ê²½ ê²€ì¦ (v19.0)"""
    try:
        validation = {
            'timestamp': time.time(),
            'github_environment_status': {},
            'github_recommendations': [],
            'github_overall_score': 100
        }
        
        # GitHub conda í™˜ê²½ ê²€ì¦
        validation['github_environment_status']['conda'] = {
            'current_env': CONDA_INFO['conda_env'],
            'is_target_env': CONDA_INFO['is_target_env'],
            'valid': CONDA_INFO['is_target_env']
        }
        
        if not CONDA_INFO['is_target_env']:
            validation['github_recommendations'].append('GitHub í‘œì¤€ mycloset-ai-clean conda í™˜ê²½ ì‚¬ìš© ê¶Œì¥')
            validation['github_overall_score'] -= 20
        
        # GitHub í•˜ë“œì›¨ì–´ ê²€ì¦
        validation['github_environment_status']['hardware'] = {
            'is_m3_max': IS_M3_MAX,
            'memory_gb': MEMORY_GB,
            'sufficient_memory': MEMORY_GB >= 16.0,
            'github_optimized': IS_M3_MAX and MEMORY_GB >= 64.0
        }
        
        if MEMORY_GB < 16.0:
            validation['github_recommendations'].append('GitHub AI íŒŒì´í”„ë¼ì¸ìš© 16GB ì´ìƒ ë©”ëª¨ë¦¬ ê¶Œì¥')
            validation['github_overall_score'] -= 30
        
        # GitHub PyTorch ê²€ì¦
        validation['github_environment_status']['pytorch'] = {
            'available': TORCH_AVAILABLE,
            'mps_available': MPS_AVAILABLE,
            'cuda_available': TORCH_AVAILABLE and torch.cuda.is_available() if TORCH_AVAILABLE else False,
            'github_ready': TORCH_AVAILABLE and (MPS_AVAILABLE or torch.cuda.is_available()) if TORCH_AVAILABLE else False
        }
        
        if not TORCH_AVAILABLE:
            validation['github_recommendations'].append('GitHub AI íŒŒì´í”„ë¼ì¸ìš© PyTorch ì„¤ì¹˜ í•„ìš”')
            validation['github_overall_score'] -= 40
        
        # GitHub ê¸°íƒ€ íŒ¨í‚¤ì§€ ê²€ì¦
        validation['github_environment_status']['packages'] = {
            'pil_available': PIL_AVAILABLE,
            'numpy_available': NUMPY_AVAILABLE,
            'github_dependencies_ready': PIL_AVAILABLE and NUMPY_AVAILABLE
        }
        
        # GitHub ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ ê²€ì¦
        try:
            import importlib
            model_loader_module = importlib.import_module('app.ai_pipeline.utils.model_loader')
            step_interface_module = importlib.import_module('app.ai_pipeline.interface.step_interface')
            validation['github_environment_status']['dependency_system'] = {
                'model_loader_available': hasattr(model_loader_module, 'get_global_model_loader'),
                'step_interface_available': hasattr(step_interface_module, 'StepModelInterface'),
                'github_compatible': True
            }
        except ImportError:
            validation['github_environment_status']['dependency_system'] = {
                'model_loader_available': False,
                'step_interface_available': False,
                'github_compatible': False
            }
            validation['github_recommendations'].append('GitHub ì˜ì¡´ì„± ì‹œìŠ¤í…œ ëª¨ë“ˆ í™•ì¸ í•„ìš”')
            validation['github_overall_score'] -= 25
        
        validation['github_overall_score'] = max(0, validation['github_overall_score'])
        
        return validation
        
    except Exception as e:
        return {'error': str(e), 'github_overall_score': 0}

def get_github_environment_info() -> Dict[str, Any]:
    """GitHub í™˜ê²½ ì •ë³´ ì¡°íšŒ (v19.0)"""
    return {
        'version': 'BaseStepMixin v19.0 GitHub Compatible',
        'github_conda_info': CONDA_INFO,
        'github_hardware': {
            'is_m3_max': IS_M3_MAX,
            'memory_gb': MEMORY_GB,
            'platform': platform.system(),
            'github_optimized': IS_M3_MAX and MEMORY_GB >= 64.0
        },
        'github_libraries': {
            'torch_available': TORCH_AVAILABLE,
            'mps_available': MPS_AVAILABLE,
            'pil_available': PIL_AVAILABLE,
            'numpy_available': NUMPY_AVAILABLE,
            'github_ai_ready': TORCH_AVAILABLE and (MPS_AVAILABLE or (torch.cuda.is_available() if TORCH_AVAILABLE else False))
        },
        'github_device_info': {
            'recommended_device': 'mps' if IS_M3_MAX and MPS_AVAILABLE else 'cuda' if TORCH_AVAILABLE and torch.cuda.is_available() else 'cpu',
            'github_performance_mode': IS_M3_MAX and MPS_AVAILABLE
        },
        'github_dependency_system': {
            'enhanced_dependency_manager': True,
            'step_model_interface_support': True,
            'auto_injection_support': True,
            'validation_support': True,
            'github_compatibility': True,
            'process_method_validation': True,
            'real_ai_pipeline_support': True
        },
        'github_features': {
            'dual_validation_format': True,
            'auto_format_detection': True,
            'github_step_compatibility': True,
            'real_ai_model_support': True,
            'm3_max_optimization': IS_M3_MAX,
            'conda_optimization': CONDA_INFO['is_target_env']
        }
    }

# ==============================================
# ğŸ”¥ Export
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'BaseStepMixin',
    'GitHubDependencyManager',
    
    # ì„¤ì • ë° ìƒíƒœ í´ë˜ìŠ¤ë“¤
    'GitHubStepConfig',
    'GitHubDependencyStatus',
    'GitHubPerformanceMetrics',
    
    # GitHub í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ë“¤
    'IGitHubModelProvider',
    'IGitHubMemoryManager',
    'IGitHubDataConverter',
    
    # GitHub ì—´ê±°í˜•ë“¤
    'ProcessMethodSignature',
    'DependencyValidationFormat',
    
    # í¸ì˜ í•¨ìˆ˜ë“¤
    'create_github_base_step_mixin',
    'validate_github_step_environment',
    'get_github_environment_info',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'CONDA_INFO',
    'IS_M3_MAX',
    'MEMORY_GB'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë¡œê·¸
# ==============================================

logger = logging.getLogger(__name__)
logger.info("=" * 80)
logger.info("ğŸ”¥ BaseStepMixin v19.1 - GitHub í”„ë¡œì íŠ¸ ì™„ì „ í˜¸í™˜ (ë°ì´í„° ì „ë‹¬ ìµœì í™”)")
logger.info("=" * 80)
logger.info("âœ… GitHub Step í´ë˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜")
logger.info("âœ… process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ì™„ì „ í‘œì¤€í™”")
logger.info("âœ… ë°ì´í„° ì „ë‹¬ ìë™ ë³€í™˜ ì‹œìŠ¤í…œ (v19.1 ì‹ ê·œ)")
logger.info("âœ… validate_dependencies() ì˜¤ë²„ë¡œë“œ ì§€ì› (dual format)")
logger.info("âœ… StepFactory v9.0ê³¼ ì™„ì „ í˜¸í™˜")
logger.info("âœ… ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ ì „ë©´ ì¬ì„¤ê³„")
logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì™„ì „ ì§€ì›")
logger.info("âœ… GitHub M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("âœ… GitHub conda í™˜ê²½ ìš°ì„  ìµœì í™” (mycloset-ai-clean)")
logger.info("âœ… GitHubDependencyManager ì™„ì „ ìƒˆë¡œìš´ ì„¤ê³„")
logger.info("âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì§„ë‹¨ ë„êµ¬ ê°•í™”")
logger.info("âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ì‹œìŠ¤í…œ ê°œì„ ")
logger.info("âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
logger.info("âœ… ğŸ”¥ ìë™ ì¸ì ë³€í™˜ ì‹œìŠ¤í…œ (v19.1)")
logger.info("=" * 80)
logger.info(f"ğŸ”§ í˜„ì¬ conda í™˜ê²½: {CONDA_INFO['conda_env']} (GitHub ìµœì í™”: {CONDA_INFO['is_target_env']})")
logger.info(f"ğŸ–¥ï¸  í˜„ì¬ ì‹œìŠ¤í…œ: M3 Max={IS_M3_MAX}, ë©”ëª¨ë¦¬={MEMORY_GB:.1f}GB")
logger.info(f"ğŸš€ GitHub AI íŒŒì´í”„ë¼ì¸ ì¤€ë¹„: {TORCH_AVAILABLE and (MPS_AVAILABLE or (torch.cuda.is_available() if TORCH_AVAILABLE else False))}")
logger.info("=" * 80)