# backend/app/ai_pipeline/steps/base_step_mixin.py
"""
ğŸ”¥ BaseStepMixin v18.0 - ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ìˆ˜ì • (StepFactory v7.0 í˜¸í™˜)
================================================================

âœ… StepFactory v7.0ê³¼ ì™„ì „ í˜¸í™˜
âœ… ì˜ì¡´ì„± ì£¼ì… ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… ModelLoader â†’ StepModelInterface ì—°ê²° ì•ˆì •í™”
âœ… ì´ˆê¸°í™” ìˆœì„œ ìµœì í™”
âœ… ì—ëŸ¬ ì²˜ë¦¬ ê°•í™” ë° ì•ˆì „í•œ í´ë°±
âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™” (mycloset-ai-clean)
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

í•µì‹¬ ìˆ˜ì •ì‚¬í•­:
1. ì˜ì¡´ì„± ì£¼ì… ì¸í„°í˜ì´ìŠ¤ ì™„ì „ í‘œì¤€í™”
2. ModelLoaderì™€ StepModelInterface ì—°ê²° ë¡œì§ ê°œì„ 
3. ì´ˆê¸°í™” ìˆœì„œ ìµœì í™” (ì˜ì¡´ì„± â†’ ì´ˆê¸°í™” â†’ ê²€ì¦)
4. ì•ˆì „í•œ ì—ëŸ¬ ì²˜ë¦¬ ë° ìƒíƒœ ê´€ë¦¬

Author: MyCloset AI Team
Date: 2025-07-25
Version: 18.0 (Dependency Injection Complete Fix)
"""

import os
import gc
import time
import asyncio
import logging
import threading
import traceback
import weakref
import subprocess
import platform
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, Type, TYPE_CHECKING
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from functools import wraps
from contextlib import asynccontextmanager

# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from ..utils.model_loader import ModelLoader, StepModelInterface
    from ..factories.step_factory import StepFactory
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
    from ..core.di_container import DIContainer

# ==============================================
# ğŸ”¥ í™˜ê²½ ì„¤ì • ë° ì‹œìŠ¤í…œ ì •ë³´
# ==============================================

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean'
}

# ì‹œìŠ¤í…œ ì •ë³´
IS_M3_MAX = False
MEMORY_GB = 16.0

try:
    if platform.system() == 'Darwin':
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.stdout.strip():
            MEMORY_GB = int(memory_result.stdout.strip()) / 1024**3
except:
    pass

# ==============================================
# ğŸ”¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì „ import
# ==============================================

# PyTorch ì•ˆì „ import
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
except ImportError:
    torch = None

# PIL ì•ˆì „ import
PIL_AVAILABLE = False
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    Image = None

# NumPy ì•ˆì „ import
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None

# ==============================================
# ğŸ”¥ ì˜ì¡´ì„± ì£¼ì… ì¸í„°í˜ì´ìŠ¤ (í‘œì¤€í™”)
# ==============================================

class IModelProvider(ABC):
    """ëª¨ë¸ ì œê³µì ì¸í„°í˜ì´ìŠ¤ (í‘œì¤€í™”)"""
    
    @abstractmethod
    def get_model(self, model_name: str) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        pass
    
    @abstractmethod
    async def get_model_async(self, model_name: str) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        pass
    
    @abstractmethod
    def is_model_available(self, model_name: str) -> bool:
        """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        pass
    
    @abstractmethod
    def load_model(self, model_name: str, **kwargs) -> bool:
        """ëª¨ë¸ ë¡œë”©"""
        pass
    
    @abstractmethod
    def unload_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë”©"""
        pass

class IMemoryManager(ABC):
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸í„°í˜ì´ìŠ¤ (í‘œì¤€í™”)"""
    
    @abstractmethod
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        pass
    
    @abstractmethod
    async def optimize_memory_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™”"""
        pass
    
    @abstractmethod
    def get_memory_info(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        pass

class IDataConverter(ABC):
    """ë°ì´í„° ë³€í™˜ê¸° ì¸í„°í˜ì´ìŠ¤ (í‘œì¤€í™”)"""
    
    @abstractmethod
    def convert_data(self, data: Any, target_format: str) -> Any:
        """ë°ì´í„° ë³€í™˜"""
        pass
    
    @abstractmethod
    def validate_data(self, data: Any, expected_format: str) -> bool:
        """ë°ì´í„° ê²€ì¦"""
        pass

# ==============================================
# ğŸ”¥ ì„¤ì • ë° ìƒíƒœ í´ë˜ìŠ¤
# ==============================================

@dataclass
class StepConfig:
    """í†µí•© Step ì„¤ì • (v18.0)"""
    step_name: str = "BaseStep"
    step_id: int = 0
    device: str = "auto"
    use_fp16: bool = True
    batch_size: int = 1
    confidence_threshold: float = 0.8
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True
    optimization_enabled: bool = True
    quality_level: str = "balanced"
    strict_mode: bool = False
    
    # ì˜ì¡´ì„± ì„¤ì • (v18.0 ê°•í™”)
    auto_inject_dependencies: bool = True
    require_model_loader: bool = True
    require_memory_manager: bool = False
    require_data_converter: bool = False
    dependency_timeout: float = 30.0  # ì˜ì¡´ì„± ì£¼ì… íƒ€ì„ì•„ì›ƒ
    dependency_retry_count: int = 3   # ì¬ì‹œë„ íšŸìˆ˜
    
    # í™˜ê²½ ìµœì í™”
    conda_optimized: bool = False
    conda_env: str = "none"
    m3_max_optimized: bool = False
    memory_gb: float = 16.0
    use_unified_memory: bool = False

@dataclass
class DependencyStatus:
    """ì˜ì¡´ì„± ìƒíƒœ ì¶”ì  (v18.0 ê°•í™”)"""
    model_loader: bool = False
    step_interface: bool = False
    memory_manager: bool = False
    data_converter: bool = False
    di_container: bool = False
    base_initialized: bool = False
    custom_initialized: bool = False
    dependencies_validated: bool = False
    
    # í™˜ê²½ ìƒíƒœ
    conda_optimized: bool = False
    m3_max_optimized: bool = False
    
    # ì£¼ì… ì‹œë„ ì¶”ì 
    injection_attempts: Dict[str, int] = field(default_factory=dict)
    injection_errors: Dict[str, List[str]] = field(default_factory=dict)
    last_injection_time: float = field(default_factory=time.time)

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ (v18.0)"""
    process_count: int = 0
    total_process_time: float = 0.0
    average_process_time: float = 0.0
    error_count: int = 0
    success_count: int = 0
    cache_hits: int = 0
    
    # ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­
    peak_memory_usage_mb: float = 0.0
    average_memory_usage_mb: float = 0.0
    memory_optimizations: int = 0
    
    # AI ëª¨ë¸ ë©”íŠ¸ë¦­
    models_loaded: int = 0
    total_model_size_gb: float = 0.0
    inference_count: int = 0
    
    # ì˜ì¡´ì„± ë©”íŠ¸ë¦­ (v18.0 ì¶”ê°€)
    dependencies_injected: int = 0
    injection_failures: int = 0
    average_injection_time: float = 0.0

# ==============================================
# ğŸ”¥ ê°•í™”ëœ ì˜ì¡´ì„± ê´€ë¦¬ì v18.0
# ==============================================

class EnhancedDependencyManager:
    """ê°•í™”ëœ ì˜ì¡´ì„± ê´€ë¦¬ì v18.0 (ì™„ì „ ìˆ˜ì •)"""
    
    def __init__(self, step_name: str):
        self.step_name = step_name
        self.logger = logging.getLogger(f"DependencyManager.{step_name}")
        
        # ì˜ì¡´ì„± ì €ì¥
        self.dependencies: Dict[str, Any] = {}
        self.dependency_status = DependencyStatus()
        
        # í™˜ê²½ ì •ë³´
        self.conda_info = CONDA_INFO
        self.is_m3_max = IS_M3_MAX
        self.memory_gb = MEMORY_GB
        
        # ë™ê¸°í™”
        self._lock = threading.RLock()
        
        # ì£¼ì… ìƒíƒœ ì¶”ì 
        self._injection_history: Dict[str, List[Dict[str, Any]]] = {}
        self._auto_injection_attempted = False
        
        # í™˜ê²½ ìµœì í™” ì„¤ì •
        self._setup_environment_optimization()
    
    def _setup_environment_optimization(self):
        """í™˜ê²½ ìµœì í™” ì„¤ì •"""
        try:
            # conda í™˜ê²½ ìµœì í™”
            if self.conda_info['is_target_env']:
                self.dependency_status.conda_optimized = True
                self.logger.debug(f"âœ… conda í™˜ê²½ ìµœì í™” í™œì„±í™”: {self.conda_info['conda_env']}")
            
            # M3 Max ìµœì í™”
            if self.is_m3_max:
                self.dependency_status.m3_max_optimized = True
                self.logger.debug(f"âœ… M3 Max ìµœì í™” í™œì„±í™”: {self.memory_gb:.1f}GB")
                
        except Exception as e:
            self.logger.debug(f"í™˜ê²½ ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def inject_model_loader(self, model_loader: 'ModelLoader') -> bool:
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (v18.0 ì™„ì „ ìˆ˜ì •)"""
        injection_start = time.time()
        
        try:
            with self._lock:
                self.logger.info(f"ğŸ”„ {self.step_name} ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
                
                # ì´ì „ ì£¼ì… ê¸°ë¡
                self._record_injection_attempt('model_loader')
                
                # 1. ModelLoader ì €ì¥
                self.dependencies['model_loader'] = model_loader
                
                # 2. ModelLoader ìœ íš¨ì„± ê²€ì¦
                if not self._validate_model_loader(model_loader):
                    raise ValueError("ModelLoader ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨")
                
                # 3. ğŸ”¥ StepModelInterface ìƒì„± (í•µì‹¬ ìˆ˜ì •)
                step_interface = self._create_step_interface(model_loader)
                if step_interface:
                    self.dependencies['step_interface'] = step_interface
                    self.dependency_status.step_interface = True
                    self.logger.info(f"âœ… {self.step_name} StepModelInterface ìƒì„± ì™„ë£Œ")
                else:
                    self.logger.warning(f"âš ï¸ {self.step_name} StepModelInterface ìƒì„± ì‹¤íŒ¨")
                
                # 4. í™˜ê²½ ìµœì í™” ì ìš©
                self._apply_model_loader_optimization(model_loader)
                
                # 5. ìƒíƒœ ì—…ë°ì´íŠ¸
                self.dependency_status.model_loader = True
                self.dependency_status.last_injection_time = time.time()
                
                injection_time = time.time() - injection_start
                self.logger.info(f"âœ… {self.step_name} ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ ({injection_time:.3f}ì´ˆ)")
                
                return True
                
        except Exception as e:
            injection_time = time.time() - injection_start
            self._record_injection_error('model_loader', str(e))
            self.logger.error(f"âŒ {self.step_name} ModelLoader ì£¼ì… ì‹¤íŒ¨ ({injection_time:.3f}ì´ˆ): {e}")
            return False
    
    def _validate_model_loader(self, model_loader: 'ModelLoader') -> bool:
        """ModelLoader ìœ íš¨ì„± ê²€ì¦"""
        try:
            # ê¸°ë³¸ ì†ì„± í™•ì¸
            required_attrs = ['load_model', 'is_initialized']
            for attr in required_attrs:
                if not hasattr(model_loader, attr):
                    self.logger.warning(f"âš ï¸ ModelLoaderì— {attr} ì†ì„± ì—†ìŒ")
                    
            # create_step_interface ë©”ì„œë“œ í™•ì¸ (í•µì‹¬)
            if not hasattr(model_loader, 'create_step_interface'):
                self.logger.warning("âš ï¸ ModelLoaderì— create_step_interface ë©”ì„œë“œ ì—†ìŒ")
                return False
            
            # ì´ˆê¸°í™” ìƒíƒœ í™•ì¸
            if hasattr(model_loader, 'is_initialized') and callable(model_loader.is_initialized):
                if not model_loader.is_initialized():
                    self.logger.warning("âš ï¸ ModelLoaderê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                    # ì´ˆê¸°í™” ì‹œë„
                    if hasattr(model_loader, 'initialize'):
                        try:
                            success = model_loader.initialize()
                            if not success:
                                self.logger.error("âŒ ModelLoader ì´ˆê¸°í™” ì‹¤íŒ¨")
                                return False
                        except Exception as init_error:
                            self.logger.error(f"âŒ ModelLoader ì´ˆê¸°í™” ì˜¤ë¥˜: {init_error}")
                            return False
            
            self.logger.debug(f"âœ… {self.step_name} ModelLoader ìœ íš¨ì„± ê²€ì¦ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def _create_step_interface(self, model_loader: 'ModelLoader') -> Optional['StepModelInterface']:
        """StepModelInterface ìƒì„± (v18.0 í•µì‹¬ ìˆ˜ì •)"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} StepModelInterface ìƒì„± ì‹œì‘...")
            
            # 1. create_step_interface ë©”ì„œë“œ í˜¸ì¶œ
            if hasattr(model_loader, 'create_step_interface'):
                interface = model_loader.create_step_interface(self.step_name)
                
                if interface:
                    # 2. ì¸í„°í˜ì´ìŠ¤ ìœ íš¨ì„± ê²€ì¦
                    if self._validate_step_interface(interface):
                        self.logger.info(f"âœ… {self.step_name} StepModelInterface ìƒì„± ë° ê²€ì¦ ì™„ë£Œ")
                        return interface
                    else:
                        self.logger.warning(f"âš ï¸ {self.step_name} StepModelInterface ê²€ì¦ ì‹¤íŒ¨")
                        return interface  # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ì¸í„°í˜ì´ìŠ¤ëŠ” ë°˜í™˜
                else:
                    self.logger.error(f"âŒ {self.step_name} StepModelInterface ìƒì„± ì‹¤íŒ¨")
                    
            else:
                self.logger.error(f"âŒ ModelLoaderì— create_step_interface ë©”ì„œë“œ ì—†ìŒ")
            
            # 3. í´ë°± ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹œë„
            return self._create_fallback_interface(model_loader)
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} StepModelInterface ìƒì„± ì˜¤ë¥˜: {e}")
            return self._create_fallback_interface(model_loader)
    
    def _validate_step_interface(self, interface: 'StepModelInterface') -> bool:
        """StepModelInterface ìœ íš¨ì„± ê²€ì¦"""
        try:
            # í•„ìˆ˜ ë©”ì„œë“œ í™•ì¸
            required_methods = ['get_model_sync', 'get_model_async', 'register_model_requirement']
            for method in required_methods:
                if not hasattr(interface, method):
                    self.logger.warning(f"âš ï¸ StepModelInterfaceì— {method} ë©”ì„œë“œ ì—†ìŒ")
            
            # step_name ì†ì„± í™•ì¸
            if hasattr(interface, 'step_name'):
                if interface.step_name != self.step_name:
                    self.logger.warning(f"âš ï¸ StepModelInterface step_name ë¶ˆì¼ì¹˜: {interface.step_name} != {self.step_name}")
            
            self.logger.debug(f"âœ… {self.step_name} StepModelInterface ê²€ì¦ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ StepModelInterface ê²€ì¦ ì˜¤ë¥˜: {e}")
            return False
    
    def _create_fallback_interface(self, model_loader: 'ModelLoader') -> Optional['StepModelInterface']:
        """í´ë°± StepModelInterface ìƒì„±"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} í´ë°± StepModelInterface ìƒì„± ì‹œë„...")
            
            # StepModelInterface ë™ì  import
            import importlib
            interface_module = importlib.import_module('app.ai_pipeline.interface.step_interface')
            StepModelInterface = getattr(interface_module, 'StepModelInterface', None)
            
            if StepModelInterface:
                interface = StepModelInterface(self.step_name, model_loader)
                self.logger.info(f"âœ… {self.step_name} í´ë°± StepModelInterface ìƒì„± ì™„ë£Œ")
                return interface
            else:
                self.logger.error("âŒ StepModelInterface í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± StepModelInterface ìƒì„± ì‹¤íŒ¨: {e}")
        
        return None
    
    def _apply_model_loader_optimization(self, model_loader: 'ModelLoader'):
        """ModelLoader í™˜ê²½ ìµœì í™” ì ìš©"""
        try:
            # í™˜ê²½ ì„¤ì • ì ìš©
            if hasattr(model_loader, 'configure_environment'):
                env_config = {
                    'conda_env': self.conda_info['conda_env'],
                    'is_m3_max': self.is_m3_max,
                    'memory_gb': self.memory_gb,
                    'conda_optimized': self.conda_info['is_target_env']
                }
                model_loader.configure_environment(env_config)
                self.logger.debug(f"âœ… {self.step_name} ModelLoader í™˜ê²½ ìµœì í™” ì ìš©")
                
        except Exception as e:
            self.logger.debug(f"ModelLoader í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def inject_memory_manager(self, memory_manager: 'MemoryManager') -> bool:
        """MemoryManager ì˜ì¡´ì„± ì£¼ì… (M3 Max ìµœì í™”)"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ”„ {self.step_name} MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
                
                self._record_injection_attempt('memory_manager')
                
                self.dependencies['memory_manager'] = memory_manager
                self.dependency_status.memory_manager = True
                
                # M3 Max íŠ¹ë³„ ì„¤ì •
                if self.is_m3_max and hasattr(memory_manager, 'configure_m3_max'):
                    memory_manager.configure_m3_max(self.memory_gb)
                    self.logger.debug("âœ… M3 Max ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì™„ë£Œ")
                
                self.logger.info(f"âœ… {self.step_name} MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                return True
                
        except Exception as e:
            self._record_injection_error('memory_manager', str(e))
            self.logger.error(f"âŒ {self.step_name} MemoryManager ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def inject_data_converter(self, data_converter: 'DataConverter') -> bool:
        """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ”„ {self.step_name} DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
                
                self._record_injection_attempt('data_converter')
                
                self.dependencies['data_converter'] = data_converter
                self.dependency_status.data_converter = True
                
                self.logger.info(f"âœ… {self.step_name} DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                return True
                
        except Exception as e:
            self._record_injection_error('data_converter', str(e))
            self.logger.error(f"âŒ {self.step_name} DataConverter ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def inject_di_container(self, di_container: 'DIContainer') -> bool:
        """DI Container ì˜ì¡´ì„± ì£¼ì…"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ”„ {self.step_name} DI Container ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
                
                self._record_injection_attempt('di_container')
                
                self.dependencies['di_container'] = di_container
                self.dependency_status.di_container = True
                
                self.logger.info(f"âœ… {self.step_name} DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                return True
                
        except Exception as e:
            self._record_injection_error('di_container', str(e))
            self.logger.error(f"âŒ {self.step_name} DI Container ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def get_dependency(self, name: str) -> Optional[Any]:
        """ì˜ì¡´ì„± ì¡°íšŒ"""
        with self._lock:
            return self.dependencies.get(name)
    
    def check_required_dependencies(self, config: StepConfig) -> bool:
        """í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸"""
        if config.require_model_loader and not self.dependency_status.model_loader:
            return False
        if config.require_memory_manager and not self.dependency_status.memory_manager:
            return False
        if config.require_data_converter and not self.dependency_status.data_converter:
            return False
        return True
    
    def validate_all_dependencies(self) -> Dict[str, bool]:
        """ëª¨ë“  ì˜ì¡´ì„± ìœ íš¨ì„± ê²€ì¦"""
        validation_results = {}
        
        try:
            with self._lock:
                for dep_name, dep_obj in self.dependencies.items():
                    try:
                        if dep_obj is not None:
                            # ê¸°ë³¸ì ì¸ callable ê²€ì‚¬
                            if dep_name == 'model_loader':
                                validation_results[dep_name] = hasattr(dep_obj, 'load_model')
                            elif dep_name == 'step_interface':
                                validation_results[dep_name] = hasattr(dep_obj, 'get_model_sync')
                            elif dep_name == 'memory_manager':
                                validation_results[dep_name] = hasattr(dep_obj, 'optimize_memory')
                            elif dep_name == 'data_converter':
                                validation_results[dep_name] = hasattr(dep_obj, 'convert_data')
                            else:
                                validation_results[dep_name] = True
                        else:
                            validation_results[dep_name] = False
                    except Exception as e:
                        self.logger.debug(f"ì˜ì¡´ì„± {dep_name} ê²€ì¦ ì˜¤ë¥˜: {e}")
                        validation_results[dep_name] = False
                
                self.dependency_status.dependencies_validated = True
                return validation_results
                
        except Exception as e:
            self.logger.error(f"ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {}
    
    def auto_inject_dependencies(self) -> bool:
        """ìë™ ì˜ì¡´ì„± ì£¼ì… (í™˜ê²½ ìµœì í™”)"""
        if self._auto_injection_attempted:
            return True
        
        self._auto_injection_attempted = True
        success_count = 0
        
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
            
            # ModelLoader ìë™ ì£¼ì…
            if not self.dependency_status.model_loader:
                model_loader = self._get_global_model_loader()
                if model_loader:
                    if self.inject_model_loader(model_loader):
                        success_count += 1
            
            # MemoryManager ìë™ ì£¼ì…
            if not self.dependency_status.memory_manager:
                memory_manager = self._get_global_memory_manager()
                if memory_manager:
                    if self.inject_memory_manager(memory_manager):
                        success_count += 1
            
            self.logger.info(f"ğŸ”„ {self.step_name} ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {success_count}ê°œ (í™˜ê²½ ìµœì í™”)")
            return success_count > 0
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ {self.step_name} ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def _get_global_model_loader(self) -> Optional['ModelLoader']:
        """ModelLoader ë™ì  import (í™˜ê²½ ìµœì í™”)"""
        try:
            import importlib
            module = importlib.import_module('app.ai_pipeline.utils.model_loader')
            get_global = getattr(module, 'get_global_model_loader', None)
            if get_global:
                # í™˜ê²½ ìµœì í™” ì„¤ì •
                config = {
                    'conda_env': self.conda_info['conda_env'],
                    'is_m3_max': self.is_m3_max,
                    'memory_gb': self.memory_gb,
                    'enable_conda_optimization': self.conda_info['is_target_env']
                }
                return get_global(config)
        except Exception as e:
            self.logger.debug(f"ModelLoader ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
        return None
    
    def _get_global_memory_manager(self) -> Optional['MemoryManager']:
        """MemoryManager ë™ì  import (M3 Max ìµœì í™”)"""
        try:
            import importlib
            module = importlib.import_module('app.ai_pipeline.utils.memory_manager')
            get_global = getattr(module, 'get_global_memory_manager', None)
            if get_global:
                return get_global()
        except Exception as e:
            self.logger.debug(f"MemoryManager ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")
        return None
    
    def _record_injection_attempt(self, dependency_name: str):
        """ì˜ì¡´ì„± ì£¼ì… ì‹œë„ ê¸°ë¡"""
        if dependency_name not in self.dependency_status.injection_attempts:
            self.dependency_status.injection_attempts[dependency_name] = 0
        self.dependency_status.injection_attempts[dependency_name] += 1
    
    def _record_injection_error(self, dependency_name: str, error_message: str):
        """ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜ ê¸°ë¡"""
        if dependency_name not in self.dependency_status.injection_errors:
            self.dependency_status.injection_errors[dependency_name] = []
        self.dependency_status.injection_errors[dependency_name].append(error_message)
    def _record_injection_error(self, dependency_name: str, error_message: str):
        """ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜ ê¸°ë¡"""
        if dependency_name not in self.dependency_status.injection_errors:
            self.dependency_status.injection_errors[dependency_name] = []
        self.dependency_status.injection_errors[dependency_name].append(error_message)
    
    # ğŸ”¥ ì—¬ê¸°ì— validate_dependencies ë©”ì„œë“œ ì¶”ê°€
    def validate_dependencies(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ê²€ì¦ ë©”ì„œë“œ (GeometricMatchingStep í˜¸í™˜)"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ”„ {self.step_name} ì˜ì¡´ì„± ê²€ì¦ ì‹œì‘...")
                
                validation_results = {
                    "success": True,
                    "total_dependencies": len(self.dependencies),
                    "validated_dependencies": 0,
                    "failed_dependencies": 0,
                    "required_missing": [],
                    "optional_missing": [],
                    "validation_errors": [],
                    "details": {}
                }
                
                # ê° ì˜ì¡´ì„± ê²€ì¦
                for dep_name, dep_obj in self.dependencies.items():
                    if dep_obj is not None:
                        # ì˜ì¡´ì„±ë³„ ê²€ì¦
                        if dep_name == 'model_loader':
                            is_valid = hasattr(dep_obj, 'load_model') and hasattr(dep_obj, 'create_step_interface')
                        elif dep_name == 'step_interface':
                            is_valid = hasattr(dep_obj, 'get_model_sync') and hasattr(dep_obj, 'get_model_async')
                        elif dep_name == 'memory_manager':
                            is_valid = hasattr(dep_obj, 'optimize_memory') or hasattr(dep_obj, 'optimize')
                        else:
                            is_valid = True
                        
                        if is_valid:
                            validation_results["validated_dependencies"] += 1
                            validation_results["details"][dep_name] = {"success": True, "valid": True}
                        else:
                            validation_results["failed_dependencies"] += 1
                            validation_results["details"][dep_name] = {"success": False, "error": "í•„ìˆ˜ ë©”ì„œë“œ ëˆ„ë½"}
                            validation_results["validation_errors"].append(f"{dep_name}: í•„ìˆ˜ ë©”ì„œë“œ ëˆ„ë½")
                    else:
                        validation_results["failed_dependencies"] += 1
                        validation_results["details"][dep_name] = {"success": False, "error": "ì˜ì¡´ì„± ì—†ìŒ"}
                        validation_results["required_missing"].append(dep_name)
                
                # ì „ì²´ ê²€ì¦ ê²°ê³¼
                validation_results["success"] = len(validation_results["required_missing"]) == 0
                
                if validation_results["success"]:
                    self.logger.info(f"âœ… {self.step_name} ì˜ì¡´ì„± ê²€ì¦ ì„±ê³µ: {validation_results['validated_dependencies']}/{validation_results['total_dependencies']}")
                else:
                    self.logger.warning(f"âš ï¸ {self.step_name} ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {len(validation_results['required_missing'])}ê°œ ëˆ„ë½")
                
                return validation_results
                
        except Exception as e:
            error_msg = f"ì˜ì¡´ì„± ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}"
            self.logger.error(f"âŒ {error_msg}")
            return {
                "success": False,
                "error": error_msg,
                "validation_errors": [error_msg],
                "total_dependencies": 0,
                "validated_dependencies": 0,
                "failed_dependencies": 0,
                "required_missing": [],
                "optional_missing": [],
                "details": {}
            }
    
    def get_status(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ê´€ë¦¬ì ìƒíƒœ ì¡°íšŒ (v18.0 ê°•í™”)"""
        return {
            'step_name': self.step_name,
            'dependency_status': {
                'model_loader': self.dependency_status.model_loader,
                'step_interface': self.dependency_status.step_interface,
                'memory_manager': self.dependency_status.memory_manager,
                'data_converter': self.dependency_status.data_converter,
                'di_container': self.dependency_status.di_container,
                'base_initialized': self.dependency_status.base_initialized,
                'custom_initialized': self.dependency_status.custom_initialized,
                'dependencies_validated': self.dependency_status.dependencies_validated
            },
            'environment': {
                'conda_optimized': self.dependency_status.conda_optimized,
                'm3_max_optimized': self.dependency_status.m3_max_optimized,
                'conda_env': self.conda_info['conda_env'],
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb
            },
            'injection_history': {
                'auto_injection_attempted': self._auto_injection_attempted,
                'injection_attempts': dict(self.dependency_status.injection_attempts),
                'injection_errors': dict(self.dependency_status.injection_errors),
                'last_injection_time': self.dependency_status.last_injection_time
            },
            'dependencies_available': list(self.dependencies.keys()),
            'dependencies_count': len(self.dependencies)
        }

# ==============================================
# ğŸ”¥ BaseStepMixin v18.0 - ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ìˆ˜ì •
# ==============================================

class BaseStepMixin:
    """
    ğŸ”¥ BaseStepMixin v18.0 - ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ìˆ˜ì • (StepFactory v7.0 í˜¸í™˜)
    
    í•µì‹¬ ìˆ˜ì •ì‚¬í•­:
    âœ… StepFactory v7.0ê³¼ ì™„ì „ í˜¸í™˜
    âœ… ì˜ì¡´ì„± ì£¼ì… ë¬¸ì œ ì™„ì „ í•´ê²°
    âœ… ModelLoader â†’ StepModelInterface ì—°ê²° ì•ˆì •í™”
    âœ… ì´ˆê¸°í™” ìˆœì„œ ìµœì í™”
    âœ… ì—ëŸ¬ ì²˜ë¦¬ ê°•í™” ë° ì•ˆì „í•œ í´ë°±
    """
    
    def __init__(self, **kwargs):
        """í†µí•© ì´ˆê¸°í™” (v18.0 ì˜ì¡´ì„± ì£¼ì… ìˆ˜ì •)"""
        try:
            # ê¸°ë³¸ ì„¤ì •
            self.config = self._create_config(**kwargs)
            self.step_name = kwargs.get('step_name', self.__class__.__name__)
            self.step_id = kwargs.get('step_id', 0)
            
            # Logger ì„¤ì •
            self.logger = logging.getLogger(f"steps.{self.step_name}")
            if not self.logger.handlers:
                handler = logging.StreamHandler()
                formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
                handler.setFormatter(formatter)
                self.logger.addHandler(handler)
                self.logger.setLevel(logging.INFO)
            
            # ğŸ”¥ ê°•í™”ëœ ì˜ì¡´ì„± ê´€ë¦¬ì (v18.0)
            self.dependency_manager = EnhancedDependencyManager(self.step_name)
            
            # ìƒíƒœ í”Œë˜ê·¸ë“¤
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # ì‹œìŠ¤í…œ ì •ë³´ (í™˜ê²½ ìµœì í™”)
            self.device = self._resolve_device(self.config.device)
            self.is_m3_max = IS_M3_MAX
            self.memory_gb = MEMORY_GB
            self.conda_info = CONDA_INFO
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ (v18.0 ê°•í™”)
            self.performance_metrics = PerformanceMetrics()
            
            # í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±ë“¤ (StepFactory í˜¸í™˜)
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            # í™˜ê²½ ìµœì í™” ì„¤ì • ì ìš©
            self._apply_environment_optimization()
            
            # ìë™ ì˜ì¡´ì„± ì£¼ì… (ì„¤ì •ëœ ê²½ìš°)
            if self.config.auto_inject_dependencies:
                self.dependency_manager.auto_inject_dependencies()
            
            self.logger.info(f"âœ… {self.step_name} BaseStepMixin v18.0 ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self._emergency_setup(e)
    
    def _create_config(self, **kwargs) -> StepConfig:
        """ì„¤ì • ìƒì„± (í™˜ê²½ ìµœì í™”)"""
        config = StepConfig()
        for key, value in kwargs.items():
            if hasattr(config, key):
                setattr(config, key, value)
        
        # í™˜ê²½ë³„ ì„¤ì • ì ìš©
        if CONDA_INFO['is_target_env']:
            config.conda_optimized = True
            config.conda_env = CONDA_INFO['conda_env']
        
        if IS_M3_MAX:
            config.m3_max_optimized = True
            config.memory_gb = MEMORY_GB
            config.use_unified_memory = True
        
        return config
    
    def _apply_environment_optimization(self):
        """í™˜ê²½ ìµœì í™” ì ìš©"""
        try:
            # M3 Max ìµœì í™”
            if self.is_m3_max:
                # MPS ë””ë°”ì´ìŠ¤ ìš°ì„  ì„¤ì •
                if self.device == "auto" and MPS_AVAILABLE:
                    self.device = "mps"
                
                # ë°°ì¹˜ í¬ê¸° ì¡°ì •
                if self.config.batch_size == 1 and self.memory_gb >= 64:
                    self.config.batch_size = 2
                
                # ë©”ëª¨ë¦¬ ìµœì í™” ê°•í™”
                self.config.auto_memory_cleanup = True
                
                self.logger.debug(f"âœ… M3 Max ìµœì í™” ì ìš©: {self.memory_gb:.1f}GB, device={self.device}")
            
            # conda í™˜ê²½ ìµœì í™”
            if self.conda_info['is_target_env']:
                self.config.optimization_enabled = True
                self.logger.debug(f"âœ… conda í™˜ê²½ ìµœì í™” ì ìš©: {self.conda_info['conda_env']}")
            
        except Exception as e:
            self.logger.debug(f"í™˜ê²½ ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
    
    def _emergency_setup(self, error: Exception):
        """ê¸´ê¸‰ ì„¤ì •"""
        self.step_name = getattr(self, 'step_name', self.__class__.__name__)
        self.logger = logging.getLogger("emergency")
        self.device = "cpu"
        self.is_initialized = False
        self.performance_metrics = PerformanceMetrics()
        self.logger.error(f"ğŸš¨ {self.step_name} ê¸´ê¸‰ ì´ˆê¸°í™”: {error}")
    
    # ==============================================
    # ğŸ”¥ í‘œì¤€í™”ëœ ì˜ì¡´ì„± ì£¼ì… ì¸í„°í˜ì´ìŠ¤ (v18.0 ìˆ˜ì •)
    # ==============================================
    
    def set_model_loader(self, model_loader: 'ModelLoader'):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (v18.0 ì™„ì „ ìˆ˜ì •)"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
            
            # 1. ì˜ì¡´ì„± ê´€ë¦¬ìë¥¼ í†µí•œ ì£¼ì…
            success = self.dependency_manager.inject_model_loader(model_loader)
            
            if success:
                # 2. í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„± ì„¤ì •
                self.model_loader = model_loader
                self.model_interface = self.dependency_manager.get_dependency('step_interface')
                
                # 3. ìƒíƒœ í”Œë˜ê·¸ ì—…ë°ì´íŠ¸
                self.has_model = True
                self.model_loaded = True
                
                # 4. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                self.performance_metrics.dependencies_injected += 1
                
                self.logger.info(f"âœ… {self.step_name} ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            else:
                self.logger.error(f"âŒ {self.step_name} ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨")
                if self.config.strict_mode:
                    raise RuntimeError(f"Strict Mode: ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨")
                
        except Exception as e:
            self.performance_metrics.injection_failures += 1
            self.logger.error(f"âŒ {self.step_name} ModelLoader ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜: {e}")
            if self.config.strict_mode:
                raise
    
    def set_memory_manager(self, memory_manager: 'MemoryManager'):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì… (v18.0 ìˆ˜ì •)"""
        try:
            success = self.dependency_manager.inject_memory_manager(memory_manager)
            if success:
                self.memory_manager = memory_manager
                self.performance_metrics.dependencies_injected += 1
                self.logger.debug(f"âœ… {self.step_name} MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            else:
                self.performance_metrics.injection_failures += 1
                
        except Exception as e:
            self.performance_metrics.injection_failures += 1
            self.logger.warning(f"âš ï¸ {self.step_name} MemoryManager ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜: {e}")
    
    def set_data_converter(self, data_converter: 'DataConverter'):
        """DataConverter ì˜ì¡´ì„± ì£¼ì… (v18.0 ìˆ˜ì •)"""
        try:
            success = self.dependency_manager.inject_data_converter(data_converter)
            if success:
                self.data_converter = data_converter
                self.performance_metrics.dependencies_injected += 1
                self.logger.debug(f"âœ… {self.step_name} DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            else:
                self.performance_metrics.injection_failures += 1
                
        except Exception as e:
            self.performance_metrics.injection_failures += 1
            self.logger.warning(f"âš ï¸ {self.step_name} DataConverter ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜: {e}")
    
    def set_di_container(self, di_container: 'DIContainer'):
        """DI Container ì˜ì¡´ì„± ì£¼ì… (v18.0 ìˆ˜ì •)"""
        try:
            success = self.dependency_manager.inject_di_container(di_container)
            if success:
                self.di_container = di_container
                self.performance_metrics.dependencies_injected += 1
                self.logger.debug(f"âœ… {self.step_name} DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            else:
                self.performance_metrics.injection_failures += 1
                
        except Exception as e:
            self.performance_metrics.injection_failures += 1
            self.logger.warning(f"âš ï¸ {self.step_name} DI Container ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜: {e}")
    
    # ==============================================
    # ğŸ”¥ í•µì‹¬ ê¸°ëŠ¥ ë©”ì„œë“œë“¤ (v18.0 ê°œì„ )
    # ==============================================
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (v18.0 ê°œì„ ëœ í†µí•© ì¸í„°í˜ì´ìŠ¤)"""
        try:
            start_time = time.time()
            
            # 1. Step Interface ìš°ì„  ì‚¬ìš© (v18.0 ìˆ˜ì •)
            step_interface = self.dependency_manager.get_dependency('step_interface')
            if step_interface and hasattr(step_interface, 'get_model_sync'):
                model = step_interface.get_model_sync(model_name or "default")
                if model:
                    self.performance_metrics.cache_hits += 1
                    return model
            
            # 2. ModelLoader ì§ì ‘ ì‚¬ìš©
            model_loader = self.dependency_manager.get_dependency('model_loader')
            if model_loader and hasattr(model_loader, 'load_model'):
                model = model_loader.load_model(model_name or "default")
                if model:
                    self.performance_metrics.models_loaded += 1
                    return model
            
            self.logger.warning("âš ï¸ ëª¨ë¸ ì œê³µìê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            self.performance_metrics.error_count += 1
            return None
        finally:
            process_time = time.time() - start_time
            self._update_performance_metrics(process_time)
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (v18.0 ê°œì„ ëœ í†µí•© ì¸í„°í˜ì´ìŠ¤)"""
        try:
            # Step Interface ìš°ì„  ì‚¬ìš©
            step_interface = self.dependency_manager.get_dependency('step_interface')
            if step_interface and hasattr(step_interface, 'get_model_async'):
                return await step_interface.get_model_async(model_name or "default")
            
            # ModelLoader ì§ì ‘ ì‚¬ìš©
            model_loader = self.dependency_manager.get_dependency('model_loader')
            if model_loader and hasattr(model_loader, 'load_model_async'):
                return await model_loader.load_model_async(model_name or "default")
            elif model_loader and hasattr(model_loader, 'load_model'):
                # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, model_loader.load_model, model_name or "default")
            
            self.logger.warning("âš ï¸ ëª¨ë¸ ì œê³µìê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (v18.0 ê°œì„ )"""
        try:
            start_time = time.time()
            
            # ì£¼ì…ëœ MemoryManager ìš°ì„  ì‚¬ìš©
            memory_manager = self.dependency_manager.get_dependency('memory_manager')
            if memory_manager and hasattr(memory_manager, 'optimize_memory'):
                result = memory_manager.optimize_memory(aggressive=aggressive)
                self.performance_metrics.memory_optimizations += 1
                return result
            
            # ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” (í™˜ê²½ë³„)
            result = self._builtin_memory_optimize(aggressive)
            self.performance_metrics.memory_optimizations += 1
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
        finally:
            optimization_time = time.time() - start_time
            self.logger.debug(f"ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™” ì†Œìš” ì‹œê°„: {optimization_time:.3f}ì´ˆ")
    
    async def optimize_memory_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            # ì£¼ì…ëœ MemoryManager ìš°ì„  ì‚¬ìš©
            memory_manager = self.dependency_manager.get_dependency('memory_manager')
            if memory_manager and hasattr(memory_manager, 'optimize_memory_async'):
                return await memory_manager.optimize_memory_async(aggressive=aggressive)
            
            # ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self._builtin_memory_optimize, aggressive)
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    # ==============================================
    # ğŸ”¥ í‘œì¤€í™”ëœ ì´ˆê¸°í™” ë° ì›Œë°ì—… (v18.0)
    # ==============================================
    
    def initialize(self) -> bool:
        """í‘œì¤€í™”ëœ ì´ˆê¸°í™” (v18.0 ì˜ì¡´ì„± ê²€ì¦ ê°•í™”)"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"ğŸ”„ {self.step_name} í‘œì¤€í™”ëœ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸
            if not self.dependency_manager.check_required_dependencies(self.config):
                if self.config.strict_mode:
                    raise RuntimeError("í•„ìˆ˜ ì˜ì¡´ì„±ì´ ì£¼ì…ë˜ì§€ ì•ŠìŒ")
                else:
                    self.logger.warning("âš ï¸ ì¼ë¶€ ì˜ì¡´ì„±ì´ ëˆ„ë½ë¨")
            
            # 2. ì˜ì¡´ì„± ìœ íš¨ì„± ê²€ì¦ (v18.0 ì¶”ê°€)
            validation_results = self.dependency_manager.validate_all_dependencies()
            if validation_results:
                failed_deps = [dep for dep, valid in validation_results.items() if not valid]
                if failed_deps:
                    self.logger.warning(f"âš ï¸ ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {failed_deps}")
            
            # 3. í™˜ê²½ë³„ ì´ˆê¸°í™”
            self._environment_specific_initialization()
            
            # 4. ì´ˆê¸°í™” ìƒíƒœ ì„¤ì •
            self.dependency_manager.dependency_status.base_initialized = True
            self.is_initialized = True
            
            self.logger.info(f"âœ… {self.step_name} í‘œì¤€í™”ëœ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.performance_metrics.error_count += 1
            return False
    
    def _environment_specific_initialization(self):
        """í™˜ê²½ë³„ íŠ¹ë³„ ì´ˆê¸°í™”"""
        try:
            # M3 Max íŠ¹ë³„ ì´ˆê¸°í™”
            if self.is_m3_max:
                # PyTorch MPS ì›Œë°ì—…
                if TORCH_AVAILABLE and self.device == "mps":
                    try:
                        test_tensor = torch.randn(10, 10, device=self.device)
                        _ = torch.matmul(test_tensor, test_tensor.t())
                        self.logger.debug("âœ… M3 Max MPS ì›Œë°ì—… ì™„ë£Œ")
                    except Exception as mps_error:
                        self.logger.debug(f"M3 Max MPS ì›Œë°ì—… ì‹¤íŒ¨: {mps_error}")
            
            # conda í™˜ê²½ íŠ¹ë³„ ì´ˆê¸°í™”
            if self.conda_info['is_target_env']:
                # í™˜ê²½ ë³€ìˆ˜ ìµœì í™”
                os.environ['PYTHONPATH'] = self.conda_info['conda_prefix'] + '/lib/python3.11/site-packages'
                self.logger.debug("âœ… conda í™˜ê²½ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.debug(f"í™˜ê²½ë³„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def initialize_async(self) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.initialize)
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def warmup(self) -> Dict[str, Any]:
        """í‘œì¤€í™”ëœ ì›Œë°ì—… (v18.0 ê°œì„ )"""
        try:
            if self.warmup_completed:
                return {'success': True, 'message': 'ì´ë¯¸ ì›Œë°ì—… ì™„ë£Œë¨', 'cached': True}
            
            self.logger.info(f"ğŸ”¥ {self.step_name} í‘œì¤€í™”ëœ ì›Œë°ì—… ì‹œì‘...")
            start_time = time.time()
            results = []
            
            # 1. ì˜ì¡´ì„± ì›Œë°ì—… (v18.0 ì¶”ê°€)
            try:
                dependency_status = self.dependency_manager.get_status()
                if dependency_status.get('dependencies_count', 0) > 0:
                    results.append('dependency_success')
                else:
                    results.append('dependency_failed')
            except:
                results.append('dependency_failed')
            
            # 2. ë©”ëª¨ë¦¬ ì›Œë°ì—… (í™˜ê²½ë³„)
            try:
                memory_result = self.optimize_memory(aggressive=False)
                results.append('memory_success' if memory_result.get('success') else 'memory_failed')
            except:
                results.append('memory_failed')
            
            # 3. ëª¨ë¸ ì›Œë°ì—…
            try:
                test_model = self.get_model("warmup_test")
                results.append('model_success' if test_model else 'model_skipped')
            except:
                results.append('model_failed')
            
            # 4. ë””ë°”ì´ìŠ¤ ì›Œë°ì—… (í™˜ê²½ë³„)
            results.append(self._device_warmup())
            
            # 5. í™˜ê²½ë³„ íŠ¹ë³„ ì›Œë°ì—…
            if self.is_m3_max:
                results.append(self._m3_max_warmup())
            
            if self.conda_info['is_target_env']:
                results.append(self._conda_warmup())
            
            duration = time.time() - start_time
            success_count = sum(1 for r in results if 'success' in r)
            overall_success = success_count > 0
            
            if overall_success:
                self.warmup_completed = True
                self.is_ready = True
            
            self.logger.info(f"ğŸ”¥ í‘œì¤€í™”ëœ ì›Œë°ì—… ì™„ë£Œ: {success_count}/{len(results)} ì„±ê³µ ({duration:.2f}ì´ˆ)")
            
            return {
                "success": overall_success,
                "duration": duration,
                "results": results,
                "success_count": success_count,
                "total_count": len(results),
                "environment": {
                    "is_m3_max": self.is_m3_max,
                    "conda_optimized": self.conda_info['is_target_env'],
                    "device": self.device
                },
                "dependency_status": self.dependency_manager.get_status()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def _device_warmup(self) -> str:
        """ë””ë°”ì´ìŠ¤ ì›Œë°ì—…"""
        try:
            if TORCH_AVAILABLE:
                test_tensor = torch.randn(100, 100)
                if self.device != 'cpu':
                    test_tensor = test_tensor.to(self.device)
                _ = torch.matmul(test_tensor, test_tensor.t())
                return 'device_success'
            else:
                return 'device_skipped'
        except:
            return 'device_failed'
    
    def _m3_max_warmup(self) -> str:
        """M3 Max íŠ¹ë³„ ì›Œë°ì—…"""
        try:
            if TORCH_AVAILABLE and MPS_AVAILABLE:
                # í° í…ì„œë¡œ í†µí•© ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
                large_tensor = torch.randn(1000, 1000, device='mps')
                _ = torch.matmul(large_tensor, large_tensor.t())
                del large_tensor
                return 'm3_max_success'
            return 'm3_max_skipped'
        except:
            return 'm3_max_failed'
    
    def _conda_warmup(self) -> str:
        """conda í™˜ê²½ ì›Œë°ì—…"""
        try:
            # íŒ¨í‚¤ì§€ ê²½ë¡œ í™•ì¸
            import sys
            conda_paths = [p for p in sys.path if 'conda' in p.lower()]
            if conda_paths:
                return 'conda_success'
            return 'conda_skipped'
        except:
            return 'conda_failed'
    
    async def warmup_async(self) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì›Œë°ì—…"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.warmup)
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    # ==============================================
    # ğŸ”¥ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë° ëª¨ë‹ˆí„°ë§ (v18.0 ê°•í™”)
    # ==============================================
    
    def _update_performance_metrics(self, process_time: float):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (v18.0 ê°•í™”)"""
        try:
            self.performance_metrics.process_count += 1
            self.performance_metrics.total_process_time += process_time
            self.performance_metrics.average_process_time = (
                self.performance_metrics.total_process_time / 
                self.performance_metrics.process_count
            )
            
            # ì˜ì¡´ì„± ì£¼ì… í‰ê·  ì‹œê°„ ê³„ì‚° (v18.0 ì¶”ê°€)
            if self.performance_metrics.dependencies_injected > 0:
                self.performance_metrics.average_injection_time = (
                    self.performance_metrics.total_process_time / 
                    max(1, self.performance_metrics.dependencies_injected)
                )
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸ (M3 Max íŠ¹ë³„ ì²˜ë¦¬)
            if self.is_m3_max:
                try:
                    import psutil
                    memory_info = psutil.virtual_memory()
                    current_usage = memory_info.used / 1024**2  # MB
                    
                    if current_usage > self.performance_metrics.peak_memory_usage_mb:
                        self.performance_metrics.peak_memory_usage_mb = current_usage
                    
                    # ì´ë™ í‰ê·  ê³„ì‚°
                    if self.performance_metrics.average_memory_usage_mb == 0:
                        self.performance_metrics.average_memory_usage_mb = current_usage
                    else:
                        self.performance_metrics.average_memory_usage_mb = (
                            self.performance_metrics.average_memory_usage_mb * 0.9 + 
                            current_usage * 0.1
                        )
                except:
                    pass
                    
        except Exception as e:
            self.logger.debug(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ (v18.0 ê°•í™”)"""
        try:
            return {
                'process_metrics': {
                    'process_count': self.performance_metrics.process_count,
                    'total_process_time': round(self.performance_metrics.total_process_time, 3),
                    'average_process_time': round(self.performance_metrics.average_process_time, 3),
                    'success_count': self.performance_metrics.success_count,
                    'error_count': self.performance_metrics.error_count,
                    'cache_hits': self.performance_metrics.cache_hits
                },
                'memory_metrics': {
                    'peak_memory_usage_mb': round(self.performance_metrics.peak_memory_usage_mb, 2),
                    'average_memory_usage_mb': round(self.performance_metrics.average_memory_usage_mb, 2),
                    'memory_optimizations': self.performance_metrics.memory_optimizations
                },
                'ai_model_metrics': {
                    'models_loaded': self.performance_metrics.models_loaded,
                    'total_model_size_gb': round(self.performance_metrics.total_model_size_gb, 2),
                    'inference_count': self.performance_metrics.inference_count
                },
                'dependency_metrics': {  # v18.0 ì¶”ê°€
                    'dependencies_injected': self.performance_metrics.dependencies_injected,
                    'injection_failures': self.performance_metrics.injection_failures,
                    'average_injection_time': round(self.performance_metrics.average_injection_time, 3),
                    'injection_success_rate': round(
                        (self.performance_metrics.dependencies_injected / 
                         max(1, self.performance_metrics.dependencies_injected + self.performance_metrics.injection_failures)) * 100, 2
                    )
                },
                'environment_metrics': {
                    'device': self.device,
                    'is_m3_max': self.is_m3_max,
                    'memory_gb': self.memory_gb,
                    'conda_optimized': self.conda_info['is_target_env']
                }
            }
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    # ==============================================
    # ğŸ”¥ ìƒíƒœ ë° ì •ë¦¬ ë©”ì„œë“œë“¤ (v18.0 ê°•í™”)
    # ==============================================
    
    def get_status(self) -> Dict[str, Any]:
        """í†µí•© ìƒíƒœ ì¡°íšŒ (v18.0 ì˜ì¡´ì„± ì •ë³´ ê°•í™”)"""
        try:
            return {
                'step_info': {
                    'step_name': self.step_name,
                    'step_id': self.step_id,
                    'version': 'BaseStepMixin v18.0'
                },
                'status_flags': {
                    'is_initialized': self.is_initialized,
                    'is_ready': self.is_ready,
                    'has_model': self.has_model,
                    'model_loaded': self.model_loaded,
                    'warmup_completed': self.warmup_completed
                },
                'system_info': {
                    'device': self.device,
                    'is_m3_max': self.is_m3_max,
                    'memory_gb': self.memory_gb,
                    'conda_info': self.conda_info
                },
                'dependencies': self.dependency_manager.get_status(),  # v18.0 ê°•í™”ëœ ì˜ì¡´ì„± ì •ë³´
                'performance': self.get_performance_metrics(),
                'config': {
                    'device': self.config.device,
                    'use_fp16': self.config.use_fp16,
                    'batch_size': self.config.batch_size,
                    'confidence_threshold': self.config.confidence_threshold,
                    'auto_memory_cleanup': self.config.auto_memory_cleanup,
                    'auto_warmup': self.config.auto_warmup,
                    'optimization_enabled': self.config.optimization_enabled,
                    'strict_mode': self.config.strict_mode,
                    'conda_optimized': self.config.conda_optimized,
                    'm3_max_optimized': self.config.m3_max_optimized,
                    'auto_inject_dependencies': self.config.auto_inject_dependencies,
                    'dependency_timeout': self.config.dependency_timeout,
                    'dependency_retry_count': self.config.dependency_retry_count
                },
                'timestamp': time.time()
            }
        except Exception as e:
            self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'version': 'BaseStepMixin v18.0'}
    
    async def cleanup(self) -> Dict[str, Any]:
        """í‘œì¤€í™”ëœ ì •ë¦¬ (v18.0 ì˜ì¡´ì„± ì •ë¦¬ ê°•í™”)"""
        try:
            self.logger.info(f"ğŸ§¹ {self.step_name} í‘œì¤€í™”ëœ ì •ë¦¬ ì‹œì‘...")
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
            final_metrics = self.get_performance_metrics()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (í™˜ê²½ë³„ ìµœì í™”)
            cleanup_result = await self.optimize_memory_async(aggressive=True)
            
            # ìƒíƒœ ë¦¬ì…‹
            self.is_ready = False
            self.warmup_completed = False
            self.has_model = False
            self.model_loaded = False
            
            # ì˜ì¡´ì„± í•´ì œ (v18.0 ê°•í™”) - ì°¸ì¡°ë§Œ ì œê±°
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            # ì˜ì¡´ì„± ê´€ë¦¬ì ì •ë¦¬
            dependency_status = self.dependency_manager.get_status()
            
            # í™˜ê²½ë³„ íŠ¹ë³„ ì •ë¦¬
            if self.is_m3_max:
                # M3 Max í†µí•© ë©”ëª¨ë¦¬ ì •ë¦¬
                for _ in range(5):
                    gc.collect()
                if TORCH_AVAILABLE and MPS_AVAILABLE:
                    try:
                        torch.mps.empty_cache()
                    except:
                        pass
            
            if TORCH_AVAILABLE and self.device == "cuda":
                try:
                    torch.cuda.empty_cache()
                except:
                    pass
            
            self.logger.info(f"âœ… {self.step_name} í‘œì¤€í™”ëœ ì •ë¦¬ ì™„ë£Œ")
            
            return {
                "success": True,
                "cleanup_result": cleanup_result,
                "final_metrics": final_metrics,
                "dependency_status": dependency_status,
                "step_name": self.step_name,
                "version": "BaseStepMixin v18.0",
                "environment": {
                    "is_m3_max": self.is_m3_max,
                    "conda_optimized": self.conda_info['is_target_env']
                }
            }
        except Exception as e:
            self.logger.error(f"âŒ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    # ==============================================
    # ğŸ”¥ ë‚´ë¶€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (v18.0)
    # ==============================================
    
    def _resolve_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ í•´ê²° (í™˜ê²½ ìµœì í™”)"""
        if device == "auto":
            # M3 Max ìš°ì„  ì²˜ë¦¬
            if IS_M3_MAX and MPS_AVAILABLE:
                return "mps"
            
            if TORCH_AVAILABLE:
                if torch.cuda.is_available():
                    return "cuda"
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    return "mps"
            return "cpu"
        return device
    
    def _builtin_memory_optimize(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” (í™˜ê²½ë³„)"""
        try:
            results = []
            start_time = time.time()
            
            # Python GC
            before = len(gc.get_objects()) if hasattr(gc, 'get_objects') else 0
            gc.collect()
            after = len(gc.get_objects()) if hasattr(gc, 'get_objects') else 0
            results.append(f"Python GC: {before - after}ê°œ ê°ì²´ í•´ì œ")
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ (í™˜ê²½ë³„)
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    if aggressive:
                        torch.cuda.ipc_collect()
                    results.append("CUDA ìºì‹œ ì •ë¦¬")
                
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        results.append("MPS ìºì‹œ ì •ë¦¬")
                    except:
                        results.append("MPS ìºì‹œ ì •ë¦¬ ì‹œë„")
            
            # M3 Max íŠ¹ë³„ ìµœì í™”
            if self.is_m3_max and aggressive:
                # í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”
                for _ in range(5):
                    gc.collect()
                
                # ë©”ëª¨ë¦¬ ì••ì¶• ì‹œë„
                try:
                    import mmap
                    # ë©”ëª¨ë¦¬ ë§¤í•‘ ìµœì í™”
                    results.append("M3 Max í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”")
                except:
                    results.append("M3 Max ìµœì í™” ì‹œë„")
            
            # conda í™˜ê²½ ìµœì í™”
            if self.conda_info['is_target_env'] and aggressive:
                # conda ìºì‹œ ì •ë¦¬
                try:
                    import tempfile
                    import shutil
                    temp_dir = tempfile.gettempdir()
                    conda_temp = os.path.join(temp_dir, 'conda_*')
                    # ì„ì‹œ íŒŒì¼ ì •ë¦¬ëŠ” ì•ˆì „í•˜ê²Œ
                    results.append("conda ì„ì‹œ íŒŒì¼ ìµœì í™”")
                except:
                    results.append("conda ìµœì í™” ì‹œë„")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            memory_info = {}
            try:
                import psutil
                vm = psutil.virtual_memory()
                memory_info = {
                    'total_gb': round(vm.total / 1024**3, 2),
                    'available_gb': round(vm.available / 1024**3, 2),
                    'used_percent': vm.percent
                }
            except:
                memory_info = {'error': 'psutil_not_available'}
            
            duration = time.time() - start_time
            
            return {
                "success": True,
                "results": results,
                "duration": round(duration, 3),
                "device": self.device,
                "environment": {
                    "is_m3_max": self.is_m3_max,
                    "conda_optimized": self.conda_info['is_target_env'],
                    "memory_gb": self.memory_gb
                },
                "memory_info": memory_info,
                "source": "builtin_optimized"
            }
            
        except Exception as e:
            return {
                "success": False, 
                "error": str(e), 
                "source": "builtin_optimized",
                "environment": {"is_m3_max": self.is_m3_max}
            }
    
    # ==============================================
    # ğŸ”¥ ì§„ë‹¨ ë° ë””ë²„ê¹… ë©”ì„œë“œë“¤ (v18.0 ê°•í™”)
    # ==============================================
    
    def diagnose(self) -> Dict[str, Any]:
        """Step ì§„ë‹¨ (v18.0 ì˜ì¡´ì„± ì§„ë‹¨ ê°•í™”)"""
        try:
            self.logger.info(f"ğŸ” {self.step_name} ì§„ë‹¨ ì‹œì‘...")
            
            diagnosis = {
                'timestamp': time.time(),
                'step_name': self.step_name,
                'version': 'BaseStepMixin v18.0',
                'status': self.get_status(),
                'issues': [],
                'recommendations': [],
                'health_score': 100
            }
            
            # ì˜ì¡´ì„± ì§„ë‹¨ (v18.0 ê°•í™”)
            dependency_status = self.dependency_manager.get_status()
            
            if not dependency_status['dependency_status']['model_loader']:
                diagnosis['issues'].append('ModelLoaderê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ')
                diagnosis['recommendations'].append('ModelLoader ì˜ì¡´ì„± ì£¼ì… í•„ìš”')
                diagnosis['health_score'] -= 30
            
            if not dependency_status['dependency_status']['step_interface']:
                diagnosis['issues'].append('StepModelInterfaceê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ')
                diagnosis['recommendations'].append('ModelLoaderì˜ create_step_interface í™•ì¸ í•„ìš”')
                diagnosis['health_score'] -= 25
            
            # ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜ ì²´í¬ (v18.0 ì¶”ê°€)
            injection_errors = dependency_status.get('injection_history', {}).get('injection_errors', {})
            if injection_errors:
                for dep_name, errors in injection_errors.items():
                    diagnosis['issues'].append(f'{dep_name} ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜: {len(errors)}íšŒ')
                    diagnosis['recommendations'].append(f'{dep_name} ì˜ì¡´ì„± ì£¼ì… ë¬¸ì œ í•´ê²° í•„ìš”')
                    diagnosis['health_score'] -= 15
            
            # í™˜ê²½ ì§„ë‹¨
            if not self.conda_info['is_target_env']:
                diagnosis['issues'].append(f"ê¶Œì¥ conda í™˜ê²½ì´ ì•„ë‹˜: {self.conda_info['conda_env']}")
                diagnosis['recommendations'].append('mycloset-ai-clean í™˜ê²½ ì‚¬ìš© ê¶Œì¥')
                diagnosis['health_score'] -= 10
            
            # ë©”ëª¨ë¦¬ ì§„ë‹¨
            if self.memory_gb < 16:
                diagnosis['issues'].append(f"ë©”ëª¨ë¦¬ ë¶€ì¡±: {self.memory_gb:.1f}GB")
                diagnosis['recommendations'].append('16GB ì´ìƒ ë©”ëª¨ë¦¬ ê¶Œì¥')
                diagnosis['health_score'] -= 20
            
            # ë””ë°”ì´ìŠ¤ ì§„ë‹¨
            if self.device == "cpu" and (TORCH_AVAILABLE and (torch.cuda.is_available() or MPS_AVAILABLE)):
                diagnosis['issues'].append('GPU ê°€ì†ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ')
                diagnosis['recommendations'].append('GPU/MPS ë””ë°”ì´ìŠ¤ ì‚¬ìš© ê¶Œì¥')
                diagnosis['health_score'] -= 15
            
            # ì„±ëŠ¥ ì§„ë‹¨ (v18.0 ê°•í™”)
            performance_metrics = self.get_performance_metrics()
            if performance_metrics.get('process_metrics', {}).get('error_count', 0) > 0:
                error_count = performance_metrics['process_metrics']['error_count']
                process_count = performance_metrics['process_metrics']['process_count']
                if process_count > 0:
                    error_rate = error_count / process_count * 100
                    if error_rate > 10:
                        diagnosis['issues'].append(f"ë†’ì€ ì—ëŸ¬ìœ¨: {error_rate:.1f}%")
                        diagnosis['recommendations'].append('ì—ëŸ¬ ì›ì¸ ë¶„ì„ ë° í•´ê²° í•„ìš”')
                        diagnosis['health_score'] -= 25
            
            # ì˜ì¡´ì„± ì£¼ì… ì„±ê³µë¥  ì§„ë‹¨ (v18.0 ì¶”ê°€)
            dependency_metrics = performance_metrics.get('dependency_metrics', {})
            injection_success_rate = dependency_metrics.get('injection_success_rate', 100)
            if injection_success_rate < 80:
                diagnosis['issues'].append(f"ë‚®ì€ ì˜ì¡´ì„± ì£¼ì… ì„±ê³µë¥ : {injection_success_rate:.1f}%")
                diagnosis['recommendations'].append('ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ ì ê²€ í•„ìš”')
                diagnosis['health_score'] -= 20
            
            # ìµœì¢… ê±´ê°•ë„ ë³´ì •
            diagnosis['health_score'] = max(0, diagnosis['health_score'])
            
            if diagnosis['health_score'] >= 80:
                diagnosis['health_status'] = 'excellent'
            elif diagnosis['health_score'] >= 60:
                diagnosis['health_status'] = 'good'
            elif diagnosis['health_score'] >= 40:
                diagnosis['health_status'] = 'fair'
            else:
                diagnosis['health_status'] = 'poor'
            
            self.logger.info(f"ğŸ” {self.step_name} ì§„ë‹¨ ì™„ë£Œ (ê±´ê°•ë„: {diagnosis['health_score']}%)")
            
            return diagnosis
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì§„ë‹¨ ì‹¤íŒ¨: {e}")
            return {
                'error': str(e),
                'step_name': self.step_name,
                'version': 'BaseStepMixin v18.0',
                'health_score': 0,
                'health_status': 'error'
            }
    
    def benchmark(self, iterations: int = 10) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (v18.0 ì˜ì¡´ì„± ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€)"""
        try:
            self.logger.info(f"ğŸ“Š {self.step_name} ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ)...")
            
            benchmark_results = {
                'iterations': iterations,
                'step_name': self.step_name,
                'device': self.device,
                'environment': {
                    'is_m3_max': self.is_m3_max,
                    'memory_gb': self.memory_gb,
                    'conda_optimized': self.conda_info['is_target_env']
                },
                'timings': [],
                'memory_usage': [],
                'dependency_timings': [],  # v18.0 ì¶”ê°€
                'errors': 0
            }
            
            for i in range(iterations):
                try:
                    start_time = time.time()
                    
                    # ê¸°ë³¸ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
                    if TORCH_AVAILABLE:
                        test_tensor = torch.randn(512, 512, device=self.device)
                        result = torch.matmul(test_tensor, test_tensor.t())
                        del test_tensor, result
                    
                    # ì˜ì¡´ì„± ì ‘ê·¼ ë²¤ì¹˜ë§ˆí¬ (v18.0 ì¶”ê°€)
                    dependency_start = time.time()
                    model_loader = self.dependency_manager.get_dependency('model_loader')
                    step_interface = self.dependency_manager.get_dependency('step_interface')
                    dependency_time = time.time() - dependency_start
                    benchmark_results['dependency_timings'].append(dependency_time)
                    
                    # ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸
                    memory_result = self.optimize_memory()
                    
                    timing = time.time() - start_time
                    benchmark_results['timings'].append(timing)
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
                    try:
                        import psutil
                        memory_usage = psutil.virtual_memory().percent
                        benchmark_results['memory_usage'].append(memory_usage)
                    except:
                        benchmark_results['memory_usage'].append(0)
                    
                except Exception as e:
                    benchmark_results['errors'] += 1
                    self.logger.debug(f"ë²¤ì¹˜ë§ˆí¬ {i+1} ì‹¤íŒ¨: {e}")
            
            # í†µê³„ ê³„ì‚°
            if benchmark_results['timings']:
                benchmark_results['statistics'] = {
                    'min_time': min(benchmark_results['timings']),
                    'max_time': max(benchmark_results['timings']),
                    'avg_time': sum(benchmark_results['timings']) / len(benchmark_results['timings']),
                    'total_time': sum(benchmark_results['timings'])
                }
            
            if benchmark_results['dependency_timings']:  # v18.0 ì¶”ê°€
                benchmark_results['dependency_statistics'] = {
                    'min_dependency_time': min(benchmark_results['dependency_timings']),
                    'max_dependency_time': max(benchmark_results['dependency_timings']),
                    'avg_dependency_time': sum(benchmark_results['dependency_timings']) / len(benchmark_results['dependency_timings'])
                }
            
            if benchmark_results['memory_usage']:
                benchmark_results['memory_statistics'] = {
                    'min_memory': min(benchmark_results['memory_usage']),
                    'max_memory': max(benchmark_results['memory_usage']),
                    'avg_memory': sum(benchmark_results['memory_usage']) / len(benchmark_results['memory_usage'])
                }
            
            benchmark_results['success_rate'] = (
                (iterations - benchmark_results['errors']) / iterations * 100
            )
            
            self.logger.info(f"ğŸ“Š {self.step_name} ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ (ì„±ê³µë¥ : {benchmark_results['success_rate']:.1f}%)")
            
            return benchmark_results
            
        except Exception as e:
            self.logger.error(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'step_name': self.step_name}

# ==============================================
# ğŸ”¥ í¸ì˜ í•¨ìˆ˜ë“¤ (BaseStepMixin v18.0 ì „ìš©)
# ==============================================

def create_base_step_mixin(**kwargs) -> BaseStepMixin:
    """BaseStepMixin ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return BaseStepMixin(**kwargs)

def validate_step_environment() -> Dict[str, Any]:
    """Step í™˜ê²½ ê²€ì¦ (v18.0 ê°•í™”)"""
    try:
        validation = {
            'timestamp': time.time(),
            'environment_status': {},
            'recommendations': [],
            'overall_score': 100
        }
        
        # conda í™˜ê²½ ê²€ì¦
        validation['environment_status']['conda'] = {
            'current_env': CONDA_INFO['conda_env'],
            'is_target_env': CONDA_INFO['is_target_env'],
            'valid': CONDA_INFO['is_target_env']
        }
        
        if not CONDA_INFO['is_target_env']:
            validation['recommendations'].append('mycloset-ai-clean conda í™˜ê²½ ì‚¬ìš© ê¶Œì¥')
            validation['overall_score'] -= 20
        
        # í•˜ë“œì›¨ì–´ ê²€ì¦
        validation['environment_status']['hardware'] = {
            'is_m3_max': IS_M3_MAX,
            'memory_gb': MEMORY_GB,
            'sufficient_memory': MEMORY_GB >= 16.0
        }
        
        if MEMORY_GB < 16.0:
            validation['recommendations'].append('16GB ì´ìƒ ë©”ëª¨ë¦¬ ê¶Œì¥')
            validation['overall_score'] -= 30
        
        # PyTorch ê²€ì¦
        validation['environment_status']['pytorch'] = {
            'available': TORCH_AVAILABLE,
            'mps_available': MPS_AVAILABLE,
            'cuda_available': TORCH_AVAILABLE and torch.cuda.is_available() if TORCH_AVAILABLE else False
        }
        
        if not TORCH_AVAILABLE:
            validation['recommendations'].append('PyTorch ì„¤ì¹˜ í•„ìš”')
            validation['overall_score'] -= 40
        
        # ê¸°íƒ€ íŒ¨í‚¤ì§€ ê²€ì¦
        validation['environment_status']['packages'] = {
            'pil_available': PIL_AVAILABLE,
            'numpy_available': NUMPY_AVAILABLE
        }
        
        # ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ ê²€ì¦ (v18.0 ì¶”ê°€)
        try:
            import importlib
            model_loader_module = importlib.import_module('app.ai_pipeline.utils.model_loader')
            step_interface_module = importlib.import_module('app.ai_pipeline.interface.step_interface')
            validation['environment_status']['dependency_system'] = {
                'model_loader_available': hasattr(model_loader_module, 'get_global_model_loader'),
                'step_interface_available': hasattr(step_interface_module, 'StepModelInterface')
            }
        except ImportError:
            validation['environment_status']['dependency_system'] = {
                'model_loader_available': False,
                'step_interface_available': False
            }
            validation['recommendations'].append('ì˜ì¡´ì„± ì‹œìŠ¤í…œ ëª¨ë“ˆ í™•ì¸ í•„ìš”')
            validation['overall_score'] -= 25
        
        validation['overall_score'] = max(0, validation['overall_score'])
        
        return validation
        
    except Exception as e:
        return {'error': str(e), 'overall_score': 0}

def get_environment_info() -> Dict[str, Any]:
    """í™˜ê²½ ì •ë³´ ì¡°íšŒ (v18.0 ê°•í™”)"""
    return {
        'version': 'BaseStepMixin v18.0',
        'conda_info': CONDA_INFO,
        'hardware': {
            'is_m3_max': IS_M3_MAX,
            'memory_gb': MEMORY_GB,
            'platform': platform.system()
        },
        'libraries': {
            'torch_available': TORCH_AVAILABLE,
            'mps_available': MPS_AVAILABLE,
            'pil_available': PIL_AVAILABLE,
            'numpy_available': NUMPY_AVAILABLE
        },
        'device_info': {
            'recommended_device': 'mps' if IS_M3_MAX and MPS_AVAILABLE else 'cuda' if TORCH_AVAILABLE and torch.cuda.is_available() else 'cpu'
        },
        'dependency_system': {
            'enhanced_dependency_manager': True,
            'step_model_interface_support': True,
            'auto_injection_support': True,
            'validation_support': True
        }
    }

# ==============================================
# ğŸ”¥ Export
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'BaseStepMixin',
    'EnhancedDependencyManager',
    
    # ì„¤ì • ë° ìƒíƒœ í´ë˜ìŠ¤ë“¤
    'StepConfig',
    'DependencyStatus',
    'PerformanceMetrics',
    
    # ì¸í„°í˜ì´ìŠ¤ë“¤
    'IModelProvider',
    'IMemoryManager',
    'IDataConverter',
    
    # í¸ì˜ í•¨ìˆ˜ë“¤
    'create_base_step_mixin',
    'validate_step_environment',
    'get_environment_info',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'CONDA_INFO',
    'IS_M3_MAX',
    'MEMORY_GB'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë¡œê·¸
# ==============================================

logger = logging.getLogger(__name__)
logger.info("=" * 80)
logger.info("ğŸ”¥ BaseStepMixin v18.0 - ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ìˆ˜ì • (StepFactory v7.0 í˜¸í™˜)")
logger.info("=" * 80)
logger.info("âœ… StepFactory v7.0ê³¼ ì™„ì „ í˜¸í™˜")
logger.info("âœ… ì˜ì¡´ì„± ì£¼ì… ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("âœ… ModelLoader â†’ StepModelInterface ì—°ê²° ì•ˆì •í™”")
logger.info("âœ… ì´ˆê¸°í™” ìˆœì„œ ìµœì í™” (ì˜ì¡´ì„± â†’ ì´ˆê¸°í™” â†’ ê²€ì¦)")
logger.info("âœ… ì—ëŸ¬ ì²˜ë¦¬ ê°•í™” ë° ì•ˆì „í•œ í´ë°±")
logger.info("âœ… EnhancedDependencyManager ë„ì…")
logger.info("âœ… ì˜ì¡´ì„± ìœ íš¨ì„± ê²€ì¦ ì‹œìŠ¤í…œ")
logger.info("âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê°•í™” (ì˜ì¡´ì„± ë©”íŠ¸ë¦­ ì¶”ê°€)")
logger.info("âœ… ì§„ë‹¨ ë° ë²¤ì¹˜ë§ˆí¬ ë„êµ¬ ê°•í™”")
logger.info("âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
logger.info("âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™” (mycloset-ai-clean)")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("=" * 80)
logger.info(f"ğŸ”§ í˜„ì¬ conda í™˜ê²½: {CONDA_INFO['conda_env']} (ìµœì í™”: {CONDA_INFO['is_target_env']})")
logger.info(f"ğŸ–¥ï¸  í˜„ì¬ ì‹œìŠ¤í…œ: M3 Max={IS_M3_MAX}, ë©”ëª¨ë¦¬={MEMORY_GB:.1f}GB")
logger.info("=" * 80)