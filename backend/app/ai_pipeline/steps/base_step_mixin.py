# backend/app/ai_pipeline/steps/base_step_mixin.py
"""
ðŸ”¥ BaseStepMixin v19.2 - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° + ëª¨ë“  ê¸°ëŠ¥ í¬í•¨
==============================================================

âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (TYPE_CHECKING + ì§€ì—° import)
âœ… EmbeddedDependencyManager ë‚´ìž¥ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì°¨ë‹¨
âœ… step_model_requirements.py DetailedDataSpec ì™„ì „ í™œìš©
âœ… API â†” AI ëª¨ë¸ ê°„ ë°ì´í„° ë³€í™˜ í‘œì¤€í™” ì™„ë£Œ
âœ… Step ê°„ ë°ì´í„° íë¦„ ìžë™ ì²˜ë¦¬
âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ìžë™ ì ìš©
âœ… GitHub í”„ë¡œì íŠ¸ Step í´ëž˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜
âœ… ëª¨ë“  ê¸°ëŠ¥ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ ìˆœí™˜ì°¸ì¡°ë§Œ í•´ê²°
âœ… v19.1ì˜ ëª¨ë“  ê³ ê¸‰ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ê¸°ëŠ¥ ì™„ì „ ë³´ì¡´
âœ… GitHubDependencyManager ì™„ì „ ë³µì› ë° ê°œì„ 
âœ… ì™„ì „í•œ í›„ì²˜ë¦¬ ì‹œìŠ¤í…œ (15ê°œ í›„ì²˜ë¦¬ ë‹¨ê³„)
âœ… ê³ ê¸‰ ë°ì´í„° ë³€í™˜ ë° ê²€ì¦ ì‹œìŠ¤í…œ

Author: MyCloset AI Team
Date: 2025-07-30
Version: 19.2 (Circular Reference Fix + Complete Features)
"""

import os
import gc
import time
import asyncio
import logging
import threading
import traceback
import weakref
import subprocess
import platform
import inspect
import base64
from io import BytesIO
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, Type, TYPE_CHECKING, Awaitable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from functools import wraps
from contextlib import asynccontextmanager
from enum import Enum

# ðŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from ..utils.model_loader import ModelLoader, StepModelInterface
    from ..factories.step_factory import StepFactory
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
    from ..core.di_container import DIContainer

# ==============================================
# ðŸ”¥ í™˜ê²½ ì„¤ì • ë° ì‹œìŠ¤í…œ ì •ë³´
# ==============================================

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean'
}

# ì‹œìŠ¤í…œ ì •ë³´
IS_M3_MAX = False
MEMORY_GB = 16.0

try:
    if platform.system() == 'Darwin':
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.stdout.strip():
            MEMORY_GB = int(memory_result.stdout.strip()) / 1024**3
except:
    pass

# ==============================================
# ðŸ”¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì „ import
# ==============================================

# PyTorch ì•ˆì „ import
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
except ImportError:
    torch = None

# PIL ì•ˆì „ import
PIL_AVAILABLE = False
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    Image = None

# NumPy ì•ˆì „ import
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None

# OpenCV ì•ˆì „ import
CV2_AVAILABLE = False
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    cv2 = None

# ==============================================
# ðŸ”¥ GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì¸í„°íŽ˜ì´ìŠ¤ (v19.2)
# ==============================================

class ProcessMethodSignature(Enum):
    """GitHub í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” process ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ íŒ¨í„´"""
    STANDARD = "async def process(self, **kwargs) -> Dict[str, Any]"
    INPUT_DATA = "async def process(self, input_data: Any) -> Dict[str, Any]"
    PIPELINE = "async def process_pipeline(self, input_data: Dict[str, Any]) -> Dict[str, Any]"
    LEGACY = "def process(self, *args, **kwargs) -> Dict[str, Any]"

class DependencyValidationFormat(Enum):
    """ì˜ì¡´ì„± ê²€ì¦ ë°˜í™˜ í˜•ì‹"""
    BOOLEAN_DICT = "dict_bool"  # GeometricMatchingStep í˜•ì‹: {'model_loader': True, ...}
    DETAILED_DICT = "dict_detailed"  # BaseStepMixin v18.0 í˜•ì‹: {'success': True, 'details': {...}}
    AUTO_DETECT = "auto"  # í˜¸ì¶œìžì— ë”°ë¼ ìžë™ ì„ íƒ

class DataConversionMethod(Enum):
    """ë°ì´í„° ë³€í™˜ ë°©ë²•"""
    AUTOMATIC = "auto"      # DetailedDataSpec ê¸°ë°˜ ìžë™ ë³€í™˜
    MANUAL = "manual"       # í•˜ìœ„ í´ëž˜ìŠ¤ì—ì„œ ìˆ˜ë™ ë³€í™˜
    HYBRID = "hybrid"       # ìžë™ + ìˆ˜ë™ ì¡°í•©

# ==============================================
# ðŸ”¥ ì„¤ì • ë° ìƒíƒœ í´ëž˜ìŠ¤ (v19.2 ìˆœí™˜ì°¸ì¡° í•´ê²°)
# ==============================================

@dataclass
class DetailedDataSpecConfig:
    """DetailedDataSpec ì„¤ì • ê´€ë¦¬"""
    # ìž…ë ¥ ì‚¬ì–‘
    input_data_types: List[str] = field(default_factory=list)
    input_shapes: Dict[str, Tuple[int, ...]] = field(default_factory=dict)
    input_value_ranges: Dict[str, Tuple[float, float]] = field(default_factory=dict)
    preprocessing_required: List[str] = field(default_factory=list)
    
    # ì¶œë ¥ ì‚¬ì–‘  
    output_data_types: List[str] = field(default_factory=list)
    output_shapes: Dict[str, Tuple[int, ...]] = field(default_factory=dict)
    output_value_ranges: Dict[str, Tuple[float, float]] = field(default_factory=dict)
    postprocessing_required: List[str] = field(default_factory=list)
    
    # API í˜¸í™˜ì„±
    api_input_mapping: Dict[str, str] = field(default_factory=dict)
    api_output_mapping: Dict[str, str] = field(default_factory=dict)
    
    # Step ê°„ ì—°ë™
    step_input_schema: Dict[str, Any] = field(default_factory=dict)
    step_output_schema: Dict[str, Any] = field(default_factory=dict)
    
    # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­
    normalization_mean: Tuple[float, ...] = field(default_factory=lambda: (0.485, 0.456, 0.406))
    normalization_std: Tuple[float, ...] = field(default_factory=lambda: (0.229, 0.224, 0.225))
    preprocessing_steps: List[str] = field(default_factory=list)
    postprocessing_steps: List[str] = field(default_factory=list)
    
    # Step ê°„ ë°ì´í„° ì „ë‹¬ ìŠ¤í‚¤ë§ˆ
    accepts_from_previous_step: Dict[str, Dict[str, str]] = field(default_factory=dict)
    provides_to_next_step: Dict[str, Dict[str, str]] = field(default_factory=dict)

@dataclass
class GitHubStepConfig:
    """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ Step ì„¤ì • (v19.2)"""
    step_name: str = "BaseStep"
    step_id: int = 0
    device: str = "auto"
    use_fp16: bool = True
    batch_size: int = 1
    confidence_threshold: float = 0.8
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True
    optimization_enabled: bool = True
    quality_level: str = "balanced"
    strict_mode: bool = False
    
    # ì˜ì¡´ì„± ì„¤ì •
    auto_inject_dependencies: bool = True
    require_model_loader: bool = True
    require_memory_manager: bool = False
    require_data_converter: bool = False
    dependency_timeout: float = 30.0
    dependency_retry_count: int = 3
    
    # GitHub í”„ë¡œì íŠ¸ íŠ¹ë³„ ì„¤ì •
    process_method_signature: ProcessMethodSignature = ProcessMethodSignature.STANDARD
    dependency_validation_format: DependencyValidationFormat = DependencyValidationFormat.AUTO_DETECT
    github_compatibility_mode: bool = True
    real_ai_pipeline_support: bool = True
    
    # DetailedDataSpec ì„¤ì • (v19.2 ì‹ ê·œ)
    enable_detailed_data_spec: bool = True
    data_conversion_method: DataConversionMethod = DataConversionMethod.AUTOMATIC
    strict_data_validation: bool = True
    auto_preprocessing: bool = True
    auto_postprocessing: bool = True
    
    # í™˜ê²½ ìµœì í™”
    conda_optimized: bool = False
    conda_env: str = "none"
    m3_max_optimized: bool = False
    memory_gb: float = 16.0
    use_unified_memory: bool = False

@dataclass
class GitHubDependencyStatus:
    """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì˜ì¡´ì„± ìƒíƒœ (v19.2 - ìˆœí™˜ì°¸ì¡° í•´ê²°)"""
    model_loader: bool = False
    step_interface: bool = False
    memory_manager: bool = False
    data_converter: bool = False
    di_container: bool = False
    base_initialized: bool = False
    custom_initialized: bool = False
    dependencies_validated: bool = False
    
    # GitHub íŠ¹ë³„ ìƒíƒœ
    github_compatible: bool = False
    process_method_validated: bool = False
    real_ai_models_loaded: bool = False
    
    # DetailedDataSpec ìƒíƒœ (v19.2 ì‹ ê·œ)
    detailed_data_spec_loaded: bool = False
    data_conversion_ready: bool = False
    preprocessing_configured: bool = False
    postprocessing_configured: bool = False
    api_mapping_configured: bool = False
    step_flow_configured: bool = False
    
    # í™˜ê²½ ìƒíƒœ
    conda_optimized: bool = False
    m3_max_optimized: bool = False
    
    # ì£¼ìž… ì‹œë„ ì¶”ì 
    injection_attempts: Dict[str, int] = field(default_factory=dict)
    injection_errors: Dict[str, List[str]] = field(default_factory=dict)
    last_injection_time: float = field(default_factory=time.time)

@dataclass
class GitHubPerformanceMetrics:
    """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ (v19.2)"""
    process_count: int = 0
    total_process_time: float = 0.0
    average_process_time: float = 0.0
    error_count: int = 0
    success_count: int = 0
    cache_hits: int = 0
    
    # ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­
    peak_memory_usage_mb: float = 0.0
    average_memory_usage_mb: float = 0.0
    memory_optimizations: int = 0
    
    # AI ëª¨ë¸ ë©”íŠ¸ë¦­
    models_loaded: int = 0
    total_model_size_gb: float = 0.0
    inference_count: int = 0
    
    # ì˜ì¡´ì„± ë©”íŠ¸ë¦­
    dependencies_injected: int = 0
    injection_failures: int = 0
    average_injection_time: float = 0.0
    
    # GitHub íŠ¹ë³„ ë©”íŠ¸ë¦­
    github_process_calls: int = 0
    real_ai_inferences: int = 0
    pipeline_success_rate: float = 0.0
    
    # DetailedDataSpec ë©”íŠ¸ë¦­ (v19.2 ì‹ ê·œ)
    data_conversions: int = 0
    preprocessing_operations: int = 0
    postprocessing_operations: int = 0
    api_conversions: int = 0
    step_data_transfers: int = 0
    validation_failures: int = 0

# ==============================================
# ðŸ”¥ GitHubDependencyManager - ì™„ì „ ë³µì› + ìˆœí™˜ì°¸ì¡° í•´ê²°
# ==============================================

class GitHubDependencyManager:
    """ðŸ”¥ GitHub í”„ë¡œì íŠ¸ ì™„ì „ í˜¸í™˜ ì˜ì¡´ì„± ê´€ë¦¬ìž v19.2 - ìˆœí™˜ì°¸ì¡° í•´ê²°"""
    
    def __init__(self, step_name: str, **kwargs):
        """ì™„ì „ ìˆ˜ì •ëœ ì´ˆê¸°í™” ë©”ì„œë“œ"""
        self.step_name = step_name
        self.logger = logging.getLogger(f"GitHubDependencyManager.{step_name}")
        
        # ðŸ”¥ í•µì‹¬ ì†ì„±ë“¤ (ì˜¤ë¥˜ í•´ê²°)
        self.step_instance = None
        self.injected_dependencies = {}
        self.dependencies = {}
        self.injection_attempts = {}
        self.injection_errors = {}
        
        # ðŸ”¥ dependency_status ì†ì„± ì¶”ê°€ (ì˜¤ë¥˜ í•´ê²°!)
        self.dependency_status = GitHubDependencyStatus()
        
        # ì‹œê°„ ì¶”ì 
        self.last_injection_time = time.time()
        
        # ì„ íƒì  ë§¤ê°œë³€ìˆ˜ ì²˜ë¦¬
        self.memory_gb = kwargs.get('memory_gb', 16)
        self.quality_level = kwargs.get('quality_level', 'balanced')
        self.auto_inject_enabled = kwargs.get('auto_inject_dependencies', True)
        self.dependency_timeout = kwargs.get('dependency_timeout', 30.0)
        self.dependency_retry_count = kwargs.get('dependency_retry_count', 3)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.dependencies_injected = 0
        self.injection_failures = 0
        self.validation_attempts = 0
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        self.logger.debug(f"âœ… GitHubDependencyManager v19.2 ì´ˆê¸°í™” ì™„ë£Œ: {step_name}")
    
    def set_step_instance(self, step_instance):
        """Step ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • - ì™„ì „ ìˆ˜ì •"""
        try:
            with self._lock:
                self.step_instance = step_instance
                self.logger.debug(f"âœ… {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • ì™„ë£Œ")
                return True
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def auto_inject_dependencies(self) -> bool:
        """ðŸ”¥ ì™„ì „ ìˆ˜ì •ëœ ìžë™ ì˜ì¡´ì„± ì£¼ìž… ë©”ì„œë“œ"""
        try:
            with self._lock:
                self.logger.info(f"ðŸ”„ {self.step_name} GitHubDependencyManager ìžë™ ì˜ì¡´ì„± ì£¼ìž… ì‹œìž‘...")
                
                if not self.step_instance:
                    self.logger.warning(f"âš ï¸ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    return False
                
                success_count = 0
                total_dependencies = 0
                
                # ModelLoader ìžë™ ì£¼ìž…
                if not hasattr(self.step_instance, 'model_loader') or self.step_instance.model_loader is None:
                    total_dependencies += 1
                    try:
                        model_loader = self._resolve_model_loader()
                        if model_loader:
                            self.step_instance.model_loader = model_loader
                            self.injected_dependencies['model_loader'] = model_loader
                            self.dependency_status.model_loader = True
                            success_count += 1
                            self.dependencies_injected += 1
                            self.logger.info(f"âœ… {self.step_name} ModelLoader ìžë™ ì£¼ìž… ì„±ê³µ")
                        else:
                            self.logger.warning(f"âš ï¸ {self.step_name} ModelLoader í•´ê²° ì‹¤íŒ¨ - ì‹¤ì œ ì˜ì¡´ì„± í•„ìš”")
                            self.injection_failures += 1
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {self.step_name} ModelLoader ìžë™ ì£¼ìž… ì‹¤íŒ¨: {e}")
                        self.injection_failures += 1
                
                # MemoryManager ìžë™ ì£¼ìž…  
                if not hasattr(self.step_instance, 'memory_manager') or self.step_instance.memory_manager is None:
                    total_dependencies += 1
                    try:
                        memory_manager = self._resolve_memory_manager()
                        if memory_manager:
                            self.step_instance.memory_manager = memory_manager
                            self.injected_dependencies['memory_manager'] = memory_manager
                            self.dependency_status.memory_manager = True
                            success_count += 1
                            self.dependencies_injected += 1
                            self.logger.info(f"âœ… {self.step_name} MemoryManager ìžë™ ì£¼ìž… ì„±ê³µ")
                        else:
                            self.logger.warning(f"âš ï¸ {self.step_name} MemoryManager í•´ê²° ì‹¤íŒ¨ - ì‹¤ì œ ì˜ì¡´ì„± í•„ìš”")
                            self.injection_failures += 1
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {self.step_name} MemoryManager ìžë™ ì£¼ìž… ì‹¤íŒ¨: {e}")
                        self.injection_failures += 1
                
                # DataConverter ìžë™ ì£¼ìž…
                if not hasattr(self.step_instance, 'data_converter') or self.step_instance.data_converter is None:
                    total_dependencies += 1
                    try:
                        data_converter = self._resolve_data_converter()
                        if data_converter:
                            self.step_instance.data_converter = data_converter
                            self.injected_dependencies['data_converter'] = data_converter
                            self.dependency_status.data_converter = True
                            success_count += 1
                            self.dependencies_injected += 1
                            self.logger.info(f"âœ… {self.step_name} DataConverter ìžë™ ì£¼ìž… ì„±ê³µ")
                        else:
                            self.logger.warning(f"âš ï¸ {self.step_name} DataConverter í•´ê²° ì‹¤íŒ¨ - ì‹¤ì œ ì˜ì¡´ì„± í•„ìš”")
                            self.injection_failures += 1
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {self.step_name} DataConverter ìžë™ ì£¼ìž… ì‹¤íŒ¨: {e}")
                        self.injection_failures += 1
                
                # ì„±ê³µ ì—¬ë¶€ íŒë‹¨ (ì‹¤ì œ ì˜ì¡´ì„± ê¸°ë°˜)
                if total_dependencies == 0:
                    self.logger.info(f"âœ… {self.step_name} ëª¨ë“  ì˜ì¡´ì„±ì´ ì´ë¯¸ ì£¼ìž…ë˜ì–´ ìžˆìŒ")
                    return True
                
                success_rate = success_count / total_dependencies if total_dependencies > 0 else 1.0
                
                # ì‹¤ì œ ì˜ì¡´ì„±ë§Œ í—ˆìš© - ì„±ê³µí•œ ê²ƒë§Œ ì‚¬ìš©
                if success_count > 0:
                    self.logger.info(f"âœ… {self.step_name} ì‹¤ì œ ì˜ì¡´ì„± ì£¼ìž… ì™„ë£Œ: {success_count}/{total_dependencies} ({success_rate*100:.1f}%)")
                    self.dependency_status.base_initialized = True
                    self.dependency_status.github_compatible = True
                    return True
                else:
                    self.logger.warning(f"âš ï¸ {self.step_name} ì‹¤ì œ ì˜ì¡´ì„± ì£¼ìž… ì‹¤íŒ¨: {success_count}/{total_dependencies} - Mock í´ë°± ì—†ìŒ")
                    return False
                    
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ìžë™ ì˜ì¡´ì„± ì£¼ìž… ì¤‘ ì˜¤ë¥˜: {e}")
            self.injection_failures += 1
            return False
    
    def _resolve_model_loader(self):
        """ModelLoader í•´ê²° (ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        try:
            # ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
            try:
                import importlib
                module = importlib.import_module('app.ai_pipeline.utils.model_loader')
                if hasattr(module, 'get_global_model_loader'):
                    loader = module.get_global_model_loader()
                    if loader and not isinstance(loader, bool):
                        return loader
            except ImportError:
                self.logger.debug(f"{self.step_name} ModelLoader ëª¨ë“ˆ import ì‹¤íŒ¨")
                return None
            
            self.logger.debug(f"{self.step_name} ModelLoader í•´ê²° ì‹¤íŒ¨ - ì‹¤ì œ ì˜ì¡´ì„± í•„ìš”")
            return None
            
        except Exception as e:
            self.logger.debug(f"{self.step_name} ModelLoader í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def _resolve_memory_manager(self):
        """MemoryManager í•´ê²° (ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        try:
            # ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
            try:
                import importlib
                module = importlib.import_module('app.ai_pipeline.utils.memory_manager')
                if hasattr(module, 'get_global_memory_manager'):
                    manager = module.get_global_memory_manager()
                    if manager:
                        return manager
            except ImportError:
                self.logger.debug(f"{self.step_name} MemoryManager ëª¨ë“ˆ import ì‹¤íŒ¨")
                return None
            
            self.logger.debug(f"{self.step_name} MemoryManager í•´ê²° ì‹¤íŒ¨ - ì‹¤ì œ ì˜ì¡´ì„± í•„ìš”")
            return None
            
        except Exception as e:
            self.logger.debug(f"{self.step_name} MemoryManager í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def _resolve_data_converter(self):
        """DataConverter í•´ê²° (ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        try:
            # ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
            try:
                import importlib
                module = importlib.import_module('app.ai_pipeline.utils.data_converter')
                if hasattr(module, 'get_global_data_converter'):
                    converter = module.get_global_data_converter()
                    if converter:
                        return converter
            except ImportError:
                self.logger.debug(f"{self.step_name} DataConverter ëª¨ë“ˆ import ì‹¤íŒ¨")
                return None
            
            self.logger.debug(f"{self.step_name} DataConverter í•´ê²° ì‹¤íŒ¨ - ì‹¤ì œ ì˜ì¡´ì„± í•„ìš”")
            return None
            
        except Exception as e:
            self.logger.debug(f"{self.step_name} DataConverter í•´ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def inject_model_loader(self, model_loader):
        """ModelLoader ì£¼ìž… - ì™„ì „ ìˆ˜ì •"""
        try:
            with self._lock:
                if not self.step_instance:
                    self.logger.warning(f"âš ï¸ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    return False
                
                # ìœ íš¨ì„± ê²€ì¦
                if model_loader is None:
                    self.logger.warning(f"âš ï¸ {self.step_name} ModelLoaderê°€ Noneìž…ë‹ˆë‹¤")
                    return False
                
                if isinstance(model_loader, bool):
                    self.logger.error(f"âŒ {self.step_name} ModelLoaderëŠ” boolì´ ì•„ë‹Œ ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤")
                    return False
                
                # ì£¼ìž… ì‹¤í–‰
                self.step_instance.model_loader = model_loader
                self.injected_dependencies['model_loader'] = model_loader
                self.dependency_status.model_loader = True
                self.dependency_status.base_initialized = True
                self.dependencies_injected += 1
                
                self.logger.info(f"âœ… {self.step_name} ModelLoader ì£¼ìž… ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ModelLoader ì£¼ìž… ì‹¤íŒ¨: {e}")
            self.injection_failures += 1
            return False
    
    def inject_memory_manager(self, memory_manager):
        """MemoryManager ì£¼ìž… - ì™„ì „ ìˆ˜ì •"""
        try:
            with self._lock:
                if not self.step_instance:
                    self.logger.warning(f"âš ï¸ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    return False
                
                if memory_manager is None:
                    self.logger.warning(f"âš ï¸ {self.step_name} MemoryManagerê°€ Noneìž…ë‹ˆë‹¤")
                    return False
                
                # ì£¼ìž… ì‹¤í–‰
                self.step_instance.memory_manager = memory_manager
                self.injected_dependencies['memory_manager'] = memory_manager
                self.dependency_status.memory_manager = True
                self.dependencies_injected += 1
                
                self.logger.info(f"âœ… {self.step_name} MemoryManager ì£¼ìž… ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} MemoryManager ì£¼ìž… ì‹¤íŒ¨: {e}")
            self.injection_failures += 1
            return False
    
    def inject_data_converter(self, data_converter):
        """DataConverter ì£¼ìž… - ì™„ì „ ìˆ˜ì •"""
        try:
            with self._lock:
                if not self.step_instance:
                    self.logger.warning(f"âš ï¸ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    return False
                
                if data_converter is None:
                    self.logger.warning(f"âš ï¸ {self.step_name} DataConverterê°€ Noneìž…ë‹ˆë‹¤")
                    return False
                
                # ì£¼ìž… ì‹¤í–‰
                self.step_instance.data_converter = data_converter
                self.injected_dependencies['data_converter'] = data_converter
                self.dependency_status.data_converter = True
                self.dependencies_injected += 1
                
                self.logger.info(f"âœ… {self.step_name} DataConverter ì£¼ìž… ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} DataConverter ì£¼ìž… ì‹¤íŒ¨: {e}")
            self.injection_failures += 1
            return False
    
    def inject_di_container(self, di_container):
        """DI Container ì˜ì¡´ì„± ì£¼ìž… - ì™„ì „ ìˆ˜ì •"""
        try:
            with self._lock:
                if di_container is None:
                    self.logger.warning(f"âš ï¸ {self.step_name} DI Containerê°€ Noneìž…ë‹ˆë‹¤")
                    return False
                
                # DI ContainerëŠ” step_instanceê°€ ì•„ë‹Œ dependenciesì— ì €ìž¥
                self.dependencies['di_container'] = di_container
                self.injected_dependencies['di_container'] = di_container
                self.dependency_status.di_container = True
                self.dependencies_injected += 1
                
                self.logger.info(f"âœ… {self.step_name} DI Container ì£¼ìž… ì™„ë£Œ")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} DI Container ì£¼ìž… ì‹¤íŒ¨: {e}")
            self.injection_failures += 1
            return False
    
    def validate_dependencies_github_format(self, format_type=None):
        """GitHub í˜•ì‹ ì˜ì¡´ì„± ê²€ì¦ - ì™„ì „ ìˆ˜ì •"""
        try:
            with self._lock:
                self.validation_attempts += 1
                
                # Step ì¸ìŠ¤í„´ìŠ¤ í™•ì¸
                if not self.step_instance:
                    dependencies = {
                        'model_loader': False,
                        'memory_manager': False,
                        'data_converter': False,
                        'step_interface': False,
                    }
                else:
                    # ì‹¤ì œ ì˜ì¡´ì„± ìƒíƒœ í™•ì¸
                    dependencies = {
                        'model_loader': hasattr(self.step_instance, 'model_loader') and self.step_instance.model_loader is not None,
                        'memory_manager': hasattr(self.step_instance, 'memory_manager') and self.step_instance.memory_manager is not None,
                        'data_converter': hasattr(self.step_instance, 'data_converter') and self.step_instance.data_converter is not None,
                        'step_interface': True,  # ê¸°ë³¸ê°’ (Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì¡´ìž¬í•˜ë©´ ì¸í„°íŽ˜ì´ìŠ¤ OK)
                    }
                
                # DI ContainerëŠ” ë³„ë„ í™•ì¸
                dependencies['di_container'] = 'di_container' in self.dependencies and self.dependencies['di_container'] is not None
                
                # ë°˜í™˜ í˜•ì‹ ê²°ì •
                if format_type:
                    # format_typeì´ ë¬¸ìžì—´ì¸ ê²½ìš°
                    if isinstance(format_type, str) and format_type.upper() == 'BOOLEAN_DICT':
                        return dependencies
                    # format_typeì´ enumì¸ ê²½ìš°
                    elif hasattr(format_type, 'value') and format_type.value == 'dict_bool':
                        return dependencies
                    elif hasattr(format_type, 'value') and format_type.value == 'boolean_dict':
                        return dependencies
                
                # ê¸°ë³¸ê°’: ìƒì„¸ ì •ë³´ ë°˜í™˜
                return {
                    'success': all(dep for key, dep in dependencies.items() if key != 'di_container'),  # DI Container ì œì™¸í•˜ê³  íŒë‹¨
                    'dependencies': dependencies,
                    'github_compatible': True,
                    'injected_count': len(self.injected_dependencies),
                    'step_name': self.step_name,
                    'dependency_status': {
                        'model_loader': self.dependency_status.model_loader,
                        'memory_manager': self.dependency_status.memory_manager,
                        'data_converter': self.dependency_status.data_converter,
                        'di_container': self.dependency_status.di_container,
                        'base_initialized': self.dependency_status.base_initialized,
                        'github_compatible': self.dependency_status.github_compatible
                    },
                    'metrics': {
                        'injected': self.dependencies_injected,
                        'failures': self.injection_failures,
                        'validation_attempts': self.validation_attempts
                    },
                    'timestamp': time.time()
                }
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} GitHub ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'github_compatible': False,
                'step_name': self.step_name
            }
    
    def get_dependency_status(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ìƒíƒœ ì¡°íšŒ - ì™„ì „ ìˆ˜ì •"""
        try:
            with self._lock:
                return {
                    'step_name': self.step_name,
                    'step_instance_set': self.step_instance is not None,
                    'injected_dependencies': list(self.injected_dependencies.keys()),
                    'dependency_status': {
                        'model_loader': self.dependency_status.model_loader,
                        'memory_manager': self.dependency_status.memory_manager,
                        'data_converter': self.dependency_status.data_converter,
                        'di_container': self.dependency_status.di_container,
                        'base_initialized': self.dependency_status.base_initialized,
                        'github_compatible': self.dependency_status.github_compatible,
                        'detailed_data_spec_loaded': self.dependency_status.detailed_data_spec_loaded,
                        'data_conversion_ready': self.dependency_status.data_conversion_ready
                    },
                    'metrics': {
                        'dependencies_injected': self.dependencies_injected,
                        'injection_failures': self.injection_failures,
                        'validation_attempts': self.validation_attempts,
                        'last_injection_time': self.last_injection_time
                    },
                    'timestamp': time.time()
                }
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì˜ì¡´ì„± ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'step_name': self.step_name,
                'error': str(e),
                'timestamp': time.time()
            }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - ì™„ì „ ìˆ˜ì •"""
        try:
            with self._lock:
                self.logger.info(f"ðŸ”„ {self.step_name} GitHubDependencyManager ì •ë¦¬ ì‹œìž‘...")
                
                # ì£¼ìž…ëœ ì˜ì¡´ì„±ë“¤ ì •ë¦¬
                for dep_name, dep_instance in self.injected_dependencies.items():
                    try:
                        if hasattr(dep_instance, 'cleanup'):
                            dep_instance.cleanup()
                        elif hasattr(dep_instance, 'close'):
                            dep_instance.close()
                    except Exception as e:
                        self.logger.debug(f"ì˜ì¡´ì„± ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ({dep_name}): {e}")
                
                # ìƒíƒœ ì´ˆê¸°í™”
                self.injected_dependencies.clear()
                self.dependencies.clear()
                self.step_instance = None
                
                # dependency_status ì´ˆê¸°í™”
                self.dependency_status = GitHubDependencyStatus()
                
                self.logger.info(f"âœ… {self.step_name} GitHubDependencyManager ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} GitHubDependencyManager ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """ì†Œë©¸ìž - ì•ˆì „í•œ ì •ë¦¬"""
        try:
            self.cleanup()
        except:
            pass  # ì†Œë©¸ìžì—ì„œëŠ” ì˜ˆì™¸ ë¬´ì‹œ

# ==============================================
# ðŸ”¥ BaseStepMixin v19.2 - ì™„ì „í•œ ê¸°ëŠ¥ + ìˆœí™˜ì°¸ì¡° í•´ê²°
# ==============================================

class BaseStepMixin:
    """
    ðŸ”¥ BaseStepMixin v19.2 - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° + ëª¨ë“  ê¸°ëŠ¥ í¬í•¨
    
    í•µì‹¬ ê°œì„ ì‚¬í•­:
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (GitHubDependencyManager ë‚´ìž¥)
    âœ… DetailedDataSpec ì •ë³´ ì €ìž¥ ë° ê´€ë¦¬
    âœ… í‘œì¤€í™”ëœ process ë©”ì„œë“œ ìž¬ì„¤ê³„
    âœ… API â†” AI ëª¨ë¸ ê°„ ë°ì´í„° ë³€í™˜ í‘œì¤€í™”
    âœ… Step ê°„ ë°ì´í„° íë¦„ ìžë™ ì²˜ë¦¬
    âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ìžë™ ì ìš©
    âœ… GitHub í”„ë¡œì íŠ¸ Step í´ëž˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜
    """
    def __init__(self, **kwargs):
        """ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° ì´ˆê¸°í™” (v19.2)"""
        try:
            # ê¸°ë³¸ ì„¤ì •
            self.config = self._create_github_config(**kwargs)
            self.step_name = kwargs.get('step_name', self.__class__.__name__)
            self.step_id = kwargs.get('step_id', 0)
            
            # Logger ì„¤ì • (ì œì¼ ë¨¼ì €)
            self.logger = logging.getLogger(f"steps.{self.step_name}")
            if not self.logger.handlers:
                handler = logging.StreamHandler()
                formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
                handler.setFormatter(formatter)
                self.logger.addHandler(handler)
                self.logger.setLevel(logging.INFO)

            # ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™”
            self._initialize_performance_stats()

            # ðŸ”¥ DetailedDataSpec ì •ë³´ ì €ìž¥
            self.detailed_data_spec = self._load_detailed_data_spec_from_kwargs(**kwargs)
            
            # ðŸ”¥ ë‚´ìž¥ ì˜ì¡´ì„± ê´€ë¦¬ìž (ìˆœí™˜ì°¸ì¡° í•´ê²°)
            self.dependency_manager = GitHubDependencyManager(self.step_name)
            self.dependency_manager.set_step_instance(self)
            
            # ë‚˜ë¨¸ì§€ ì´ˆê¸°í™”
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # ì‹œìŠ¤í…œ ì •ë³´
            self.device = self._resolve_device(self.config.device)
            self.is_m3_max = IS_M3_MAX
            self.memory_gb = MEMORY_GB
            self.conda_info = CONDA_INFO
            
            # GitHub í˜¸í™˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­
            self.performance_metrics = GitHubPerformanceMetrics()
            
            # GitHub í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±ë“¤
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
            
            # GitHub íŠ¹ë³„ ì†ì„±ë“¤
            self.github_compatible = True
            self.real_ai_pipeline_ready = False
            self.process_method_signature = self.config.process_method_signature
            
            # ðŸ”¥ DetailedDataSpec ìƒíƒœ
            self.data_conversion_ready = self._validate_data_conversion_readiness()
            
            # í™˜ê²½ ìµœì í™” ì ìš©
            self._apply_github_environment_optimization()
            
            # ìžë™ ì˜ì¡´ì„± ì£¼ìž… (ì„¤ì •ëœ ê²½ìš°)
            if self.config.auto_inject_dependencies:
                try:
                    self.dependency_manager.auto_inject_dependencies()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {self.step_name} ìžë™ ì˜ì¡´ì„± ì£¼ìž… ì‹¤íŒ¨: {e}")
            
            self.logger.info(f"âœ… {self.step_name} BaseStepMixin v19.2 ìˆœí™˜ì°¸ì¡° í•´ê²° ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self._github_emergency_setup(e)

    def _load_detailed_data_spec_from_kwargs(self, **kwargs) -> DetailedDataSpecConfig:
        """StepFactoryì—ì„œ ì£¼ìž…ë°›ì€ DetailedDataSpec ì •ë³´ ë¡œë”©"""
        return DetailedDataSpecConfig(
            # ìž…ë ¥ ì‚¬ì–‘
            input_data_types=kwargs.get('input_data_types', []),
            input_shapes=kwargs.get('input_shapes', {}),
            input_value_ranges=kwargs.get('input_value_ranges', {}),
            preprocessing_required=kwargs.get('preprocessing_required', []),
            
            # ì¶œë ¥ ì‚¬ì–‘
            output_data_types=kwargs.get('output_data_types', []),
            output_shapes=kwargs.get('output_shapes', {}),
            output_value_ranges=kwargs.get('output_value_ranges', {}),
            postprocessing_required=kwargs.get('postprocessing_required', []),
            
            # API í˜¸í™˜ì„±
            api_input_mapping=kwargs.get('api_input_mapping', {}),
            api_output_mapping=kwargs.get('api_output_mapping', {}),
            
            # Step ê°„ ì—°ë™
            step_input_schema=kwargs.get('step_input_schema', {}),
            step_output_schema=kwargs.get('step_output_schema', {}),
            
            # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­
            normalization_mean=kwargs.get('normalization_mean', (0.485, 0.456, 0.406)),
            normalization_std=kwargs.get('normalization_std', (0.229, 0.224, 0.225)),
            preprocessing_steps=kwargs.get('preprocessing_steps', []),
            postprocessing_steps=kwargs.get('postprocessing_steps', []),
            
            # Step ê°„ ë°ì´í„° ì „ë‹¬ ìŠ¤í‚¤ë§ˆ
            accepts_from_previous_step=kwargs.get('accepts_from_previous_step', {}),
            provides_to_next_step=kwargs.get('provides_to_next_step', {})
        )

    def _validate_data_conversion_readiness(self) -> bool:
        """ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ìƒíƒœ ê²€ì¦ (ì›Œë‹ ë°©ì§€)"""
        try:
            # DetailedDataSpec ì¡´ìž¬ í™•ì¸ ë° ìžë™ ìƒì„±
            if not hasattr(self, 'detailed_data_spec') or not self.detailed_data_spec:
                self._create_emergency_detailed_data_spec()
                self.logger.debug(f"âœ… {self.step_name} DetailedDataSpec ê¸°ë³¸ê°’ ìžë™ ìƒì„±")
            
            # í•„ìˆ˜ í•„ë“œ ì¡´ìž¬ í™•ì¸ ë° ìžë™ ë³´ì™„
            missing_fields = []
            required_fields = ['input_data_types', 'output_data_types', 'api_input_mapping', 'api_output_mapping']
            
            for field in required_fields:
                if not hasattr(self.detailed_data_spec, field):
                    missing_fields.append(field)
                else:
                    value = getattr(self.detailed_data_spec, field)
                    if not value:
                        missing_fields.append(field)
            
            # ëˆ„ë½ëœ í•„ë“œ ìžë™ ë³´ì™„
            if missing_fields:
                self._fill_missing_fields(missing_fields)
                self.logger.debug(f"{self.step_name} DetailedDataSpec í•„ë“œ ë³´ì™„: {missing_fields}")
            
            # dependency_manager ìƒíƒœ ì—…ë°ì´íŠ¸
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                self.dependency_manager.dependency_status.detailed_data_spec_loaded = True
                self.dependency_manager.dependency_status.data_conversion_ready = True
            
            self.logger.debug(f"âœ… {self.step_name} DetailedDataSpec ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ìƒíƒœ ê²€ì¦ ì‹¤íŒ¨: {e}")
            try:
                self._create_emergency_detailed_data_spec()
                self.logger.debug(f"ðŸ”„ {self.step_name} DetailedDataSpec ì˜ˆì™¸ ë³µêµ¬ ì™„ë£Œ")
            except:
                pass
            return True

    def _initialize_performance_stats(self):
        """ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™”"""
        try:
            self.performance_stats = {
                'total_processed': 0,
                'avg_processing_time': 0.0,
                'error_count': 0,
                'success_rate': 1.0,
                'memory_usage_mb': 0.0,
                'models_loaded': 0,
                'cache_hits': 0,
                'ai_inference_count': 0,
                'torch_errors': 0
            }
            
            self.total_processing_count = 0
            self.error_count = 0
            self.last_processing_time = 0.0
            
            self.logger.debug(f"âœ… {self.step_name} ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.performance_stats = {}
            self.total_processing_count = 0
            self.error_count = 0
            self.last_processing_time = 0.0

    def _create_emergency_detailed_data_spec(self):
        """ì‘ê¸‰ DetailedDataSpec ìƒì„±"""
        try:
            if not hasattr(self, 'detailed_data_spec') or not self.detailed_data_spec:
                class EmergencyDataSpec:
                    def __init__(self):
                        self.input_data_types = {
                            'person_image': 'PIL.Image.Image',
                            'clothing_image': 'PIL.Image.Image',
                            'data': 'Any'
                        }
                        self.output_data_types = {
                            'result': 'numpy.ndarray',
                            'success': 'bool',
                            'processing_time': 'float'
                        }
                        self.api_input_mapping = {
                            'person_image': 'fastapi.UploadFile -> PIL.Image.Image',
                            'clothing_image': 'fastapi.UploadFile -> PIL.Image.Image'
                        }
                        self.api_output_mapping = {
                            'result': 'numpy.ndarray -> base64_string',
                            'success': 'bool -> bool'
                        }
                        self.preprocessing_steps = ['validate_input', 'resize_image']
                        self.postprocessing_steps = ['format_output']
                        self.accepts_from_previous_step = {}
                        self.provides_to_next_step = {}
                
                self.detailed_data_spec = EmergencyDataSpec()
                
        except Exception as e:
            self.logger.error(f"ì‘ê¸‰ DetailedDataSpec ìƒì„± ì‹¤íŒ¨: {e}")

    def _fill_missing_fields(self, missing_fields):
        """ëˆ„ë½ëœ DetailedDataSpec í•„ë“œ ì±„ìš°ê¸°"""
        try:
            default_values = {
                'input_data_types': {
                    'person_image': 'PIL.Image.Image',
                    'clothing_image': 'PIL.Image.Image',
                    'data': 'Any'
                },
                'output_data_types': {
                    'result': 'numpy.ndarray',
                    'success': 'bool',
                    'processing_time': 'float'
                },
                'api_input_mapping': {
                    'person_image': 'fastapi.UploadFile -> PIL.Image.Image',
                    'clothing_image': 'fastapi.UploadFile -> PIL.Image.Image'
                },
                'api_output_mapping': {
                    'result': 'numpy.ndarray -> base64_string',
                    'success': 'bool -> bool'
                },
                'preprocessing_steps': ['validate_input', 'resize_image'],
                'postprocessing_steps': ['format_output'],
                'accepts_from_previous_step': {},
                'provides_to_next_step': {}
            }
            
            for field in missing_fields:
                if field in default_values:
                    if not hasattr(self.detailed_data_spec, field):
                        setattr(self.detailed_data_spec, field, default_values[field])
                    elif not getattr(self.detailed_data_spec, field):
                        setattr(self.detailed_data_spec, field, default_values[field])
            
        except Exception as e:
            self.logger.error(f"DetailedDataSpec í•„ë“œ ë³´ì™„ ì‹¤íŒ¨: {e}")

    # ==============================================
    # ðŸ”¥ í‘œì¤€í™”ëœ process ë©”ì„œë“œ (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    async def process(self, **kwargs) -> Dict[str, Any]:
        """ì™„ì „ížˆ ìž¬ì„¤ê³„ëœ í‘œì¤€í™” process ë©”ì„œë“œ"""
        try:
            start_time = time.time()
            self.performance_metrics.github_process_calls += 1
            
            self.logger.debug(f"ðŸ”„ {self.step_name} process ì‹œìž‘ (ìž…ë ¥: {list(kwargs.keys())})")
            
            # 1. ìž…ë ¥ ë°ì´í„° ë³€í™˜ (API/Step ê°„ â†’ AI ëª¨ë¸)
            converted_input = await self._convert_input_to_model_format(kwargs)
            
            # 2. í•˜ìœ„ í´ëž˜ìŠ¤ì˜ ìˆœìˆ˜ AI ë¡œì§ ì‹¤í–‰
            ai_result = self._run_ai_inference(converted_input)
            
            # 3. ì¶œë ¥ ë°ì´í„° ë³€í™˜ (AI ëª¨ë¸ â†’ API + Step ê°„)
            standardized_output = await self._convert_output_to_standard_format(ai_result)
            
            # 4. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self._update_performance_metrics(processing_time, True)
            
            self.logger.debug(f"âœ… {self.step_name} process ì™„ë£Œ ({processing_time:.3f}ì´ˆ)")
            
            return standardized_output
            
        except Exception as e:
            processing_time = time.time() - start_time
            self._update_performance_metrics(processing_time, False)
            self.logger.error(f"âŒ {self.step_name} process ì‹¤íŒ¨ ({processing_time:.3f}ì´ˆ): {e}")
            return self._create_error_response(str(e))

    @abstractmethod
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """í•˜ìœ„ í´ëž˜ìŠ¤ì—ì„œ êµ¬í˜„í•  ìˆœìˆ˜ AI ë¡œì§ (ë™ê¸° ë©”ì„œë“œ)"""
        pass

    # ==============================================
    # ðŸ”¥ ìž…ë ¥ ë°ì´í„° ë³€í™˜ ì‹œìŠ¤í…œ (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    async def _convert_input_to_model_format(self, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """API/Step ê°„ ë°ì´í„° â†’ AI ëª¨ë¸ ìž…ë ¥ í˜•ì‹ ë³€í™˜"""
        try:
            converted = {}
            self.performance_metrics.data_conversions += 1
            
            self.logger.debug(f"ðŸ”„ {self.step_name} ìž…ë ¥ ë°ì´í„° ë³€í™˜ ì‹œìž‘...")
            
            # 1. API ìž…ë ¥ ë§¤í•‘ ì²˜ë¦¬
            for model_param, api_type in self.detailed_data_spec.api_input_mapping.items():
                if model_param in kwargs:
                    converted[model_param] = await self._convert_api_input_type(
                        kwargs[model_param], api_type, model_param
                    )
                    self.performance_metrics.api_conversions += 1
            
            # 2. Step ê°„ ë°ì´í„° ì²˜ë¦¬
            for step_name, step_data in kwargs.items():
                if step_name.startswith('from_step_'):
                    step_id = step_name.replace('from_step_', '')
                    if step_id in self.detailed_data_spec.accepts_from_previous_step:
                        step_schema = self.detailed_data_spec.accepts_from_previous_step[step_id]
                        converted.update(self._map_step_input_data(step_data, step_schema))
                        self.performance_metrics.step_data_transfers += 1
            
            # 3. ëˆ„ë½ëœ í•„ìˆ˜ ìž…ë ¥ ë°ì´í„° í™•ì¸
            for param_name in self.detailed_data_spec.api_input_mapping.keys():
                if param_name not in converted and param_name in kwargs:
                    converted[param_name] = kwargs[param_name]
            
            # 4. ì „ì²˜ë¦¬ ì ìš©
            if self.config.auto_preprocessing and self.detailed_data_spec.preprocessing_steps:
                converted = await self._apply_preprocessing(converted)
                self.performance_metrics.preprocessing_operations += 1
            
            # 5. ë°ì´í„° ê²€ì¦
            if self.config.strict_data_validation:
                validated_input = self._validate_input_data(converted)
            else:
                validated_input = converted
            
            self.logger.debug(f"âœ… {self.step_name} ìž…ë ¥ ë°ì´í„° ë³€í™˜ ì™„ë£Œ")
            return validated_input
            
        except Exception as e:
            self.performance_metrics.validation_failures += 1
            self.logger.error(f"âŒ {self.step_name} ìž…ë ¥ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    async def _convert_api_input_type(self, value: Any, api_type: str, param_name: str) -> Any:
        """API íƒ€ìž…ë³„ ë³€í™˜ ì²˜ë¦¬"""
        try:
            if api_type == "UploadFile":
                if hasattr(value, 'file'):
                    content = await value.read() if hasattr(value, 'read') else value.file.read()
                    return Image.open(BytesIO(content)) if PIL_AVAILABLE else content
                elif hasattr(value, 'read'):
                    content = value.read()
                    return Image.open(BytesIO(content)) if PIL_AVAILABLE else content
                
            elif api_type == "base64_string":
                if isinstance(value, str):
                    try:
                        image_data = base64.b64decode(value)
                        return Image.open(BytesIO(image_data)) if PIL_AVAILABLE else image_data
                    except Exception:
                        return value
                        
            elif api_type in ["str", "Optional[str]"]:
                return str(value) if value is not None else None
                
            elif api_type in ["int", "Optional[int]"]:
                return int(value) if value is not None else None
                
            elif api_type in ["float", "Optional[float]"]:
                return float(value) if value is not None else None
                
            return value
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ {self.step_name} API íƒ€ìž… ë³€í™˜ ì‹¤íŒ¨ ({param_name}: {api_type}): {e}")
            return value
    
    def _map_step_input_data(self, step_data: Dict[str, Any], step_schema: Dict[str, str]) -> Dict[str, Any]:
        """Step ê°„ ë°ì´í„° ë§¤í•‘"""
        mapped_data = {}
        
        for data_key, data_type in step_schema.items():
            if data_key in step_data:
                value = step_data[data_key]
                
                if data_type == "np.ndarray" and NUMPY_AVAILABLE:
                    if TORCH_AVAILABLE and torch.is_tensor(value):
                        mapped_data[data_key] = value.cpu().numpy()
                    else:
                        mapped_data[data_key] = np.array(value) if not isinstance(value, np.ndarray) else value
                        
                elif data_type == "torch.Tensor" and TORCH_AVAILABLE:
                    if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                        mapped_data[data_key] = torch.from_numpy(value)
                    else:
                        mapped_data[data_key] = value
                        
                elif data_type == "PIL.Image" and PIL_AVAILABLE:
                    if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                        mapped_data[data_key] = Image.fromarray(value.astype(np.uint8))
                    else:
                        mapped_data[data_key] = value
                        
                else:
                    mapped_data[data_key] = value
        
        return mapped_data
    
    # ==============================================
    # ðŸ”¥ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    async def _apply_preprocessing(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """DetailedDataSpec ê¸°ë°˜ ì „ì²˜ë¦¬ ìžë™ ì ìš©"""
        try:
            processed = input_data.copy()
            
            self.logger.debug(f"ðŸ”„ {self.step_name} ì „ì²˜ë¦¬ ì ìš©: {self.detailed_data_spec.preprocessing_steps}")
            
            for step_name in self.detailed_data_spec.preprocessing_steps:
                if step_name == "resize_512x512":
                    processed = self._resize_images(processed, (512, 512))
                elif step_name == "resize_768x1024":
                    processed = self._resize_images(processed, (768, 1024))
                elif step_name == "resize_256x192":
                    processed = self._resize_images(processed, (256, 192))
                elif step_name == "resize_224x224":
                    processed = self._resize_images(processed, (224, 224))
                elif step_name == "resize_368x368":
                    processed = self._resize_images(processed, (368, 368))
                elif step_name == "resize_1024x1024":
                    processed = self._resize_images(processed, (1024, 1024))
                elif step_name == "normalize_imagenet":
                    processed = self._normalize_imagenet(processed)
                elif step_name == "normalize_clip":
                    processed = self._normalize_clip(processed)
                elif step_name == "normalize_diffusion":
                    processed = self._normalize_diffusion(processed)
                elif step_name == "to_tensor":
                    processed = self._convert_to_tensor(processed)
                elif step_name == "prepare_sam_prompts":
                    processed = self._prepare_sam_prompts(processed)
                elif step_name == "prepare_diffusion_input":
                    processed = self._prepare_diffusion_input(processed)
                elif step_name == "prepare_ootd_inputs":
                    processed = self._prepare_ootd_inputs(processed)
                elif step_name == "extract_pose_features":
                    processed = self._extract_pose_features(processed)
                elif step_name == "prepare_sr_input":
                    processed = self._prepare_sr_input(processed)
                else:
                    self.logger.debug(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì „ì²˜ë¦¬ ë‹¨ê³„: {step_name}")
            
            self.logger.debug(f"âœ… {self.step_name} ì „ì²˜ë¦¬ ì™„ë£Œ")
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return input_data
    
    def _resize_images(self, data: Dict[str, Any], target_size: Tuple[int, int]) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ì²˜ë¦¬"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if PIL_AVAILABLE and isinstance(value, Image.Image):
                    result[key] = value.resize(target_size, Image.LANCZOS)
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray) and len(value.shape) >= 2:
                    if CV2_AVAILABLE:
                        if len(value.shape) == 3:
                            result[key] = cv2.resize(value, target_size)
                        elif len(value.shape) == 2:
                            result[key] = cv2.resize(value, target_size)
                    else:
                        # PIL í´ë°±
                        if PIL_AVAILABLE:
                            if len(value.shape) == 3:
                                img = Image.fromarray(value.astype(np.uint8))
                                result[key] = np.array(img.resize(target_size, Image.LANCZOS))
                            elif len(value.shape) == 2:
                                img = Image.fromarray(value.astype(np.uint8), mode='L')
                                result[key] = np.array(img.resize(target_size, Image.LANCZOS))
            except Exception as e:
                self.logger.debug(f"ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _normalize_imagenet(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ImageNet ì •ê·œí™”"""
        result = data.copy()
        mean = np.array(self.detailed_data_spec.normalization_mean)
        std = np.array(self.detailed_data_spec.normalization_std)
        
        for key, value in data.items():
            try:
                if PIL_AVAILABLE and isinstance(value, Image.Image):
                    array = np.array(value).astype(np.float32) / 255.0
                    if len(array.shape) == 3 and array.shape[2] == 3:
                        normalized = (array - mean) / std
                        result[key] = normalized
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if value.dtype != np.float32:
                        value = value.astype(np.float32)
                    if value.max() > 1.0:
                        value = value / 255.0
                    if len(value.shape) == 3 and value.shape[2] == 3:
                        normalized = (value - mean) / std
                        result[key] = normalized
            except Exception as e:
                self.logger.debug(f"ImageNet ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _normalize_clip(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """CLIP ì •ê·œí™”"""
        result = data.copy()
        clip_mean = np.array([0.48145466, 0.4578275, 0.40821073])
        clip_std = np.array([0.26862954, 0.26130258, 0.27577711])
        
        for key, value in data.items():
            try:
                if PIL_AVAILABLE and isinstance(value, Image.Image):
                    array = np.array(value).astype(np.float32) / 255.0
                    if len(array.shape) == 3 and array.shape[2] == 3:
                        normalized = (array - clip_mean) / clip_std
                        result[key] = normalized
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if value.dtype != np.float32:
                        value = value.astype(np.float32)
                    if value.max() > 1.0:
                        value = value / 255.0
                    if len(value.shape) == 3 and value.shape[2] == 3:
                        normalized = (value - clip_mean) / clip_std
                        result[key] = normalized
            except Exception as e:
                self.logger.debug(f"CLIP ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _normalize_diffusion(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Diffusion ì •ê·œí™”"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if PIL_AVAILABLE and isinstance(value, Image.Image):
                    array = np.array(value).astype(np.float32) / 255.0
                    normalized = 2.0 * array - 1.0
                    result[key] = normalized
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if value.dtype != np.float32:
                        value = value.astype(np.float32)
                    if value.max() > 1.0:
                        value = value / 255.0
                    normalized = 2.0 * value - 1.0
                    result[key] = normalized
            except Exception as e:
                self.logger.debug(f"Diffusion ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _convert_to_tensor(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """PyTorch í…ì„œ ë³€í™˜"""
        if not TORCH_AVAILABLE:
            return data
        
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if len(value.shape) == 3 and value.shape[2] in [1, 3, 4]:
                        value = np.transpose(value, (2, 0, 1))
                    result[key] = torch.from_numpy(value).float()
                elif PIL_AVAILABLE and isinstance(value, Image.Image):
                    array = np.array(value)
                    if len(array.shape) == 3:
                        array = np.transpose(array, (2, 0, 1))
                    result[key] = torch.from_numpy(array).float()
            except Exception as e:
                self.logger.debug(f"í…ì„œ ë³€í™˜ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _prepare_sam_prompts(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """SAM í”„ë¡¬í”„íŠ¸ ì¤€ë¹„"""
        result = data.copy()
        
        if 'prompt_points' not in result and 'image' in result:
            if PIL_AVAILABLE and isinstance(result['image'], Image.Image):
                w, h = result['image'].size
                result['prompt_points'] = np.array([[w//2, h//2]])
                result['prompt_labels'] = np.array([1])
        
        return result
    
    def _prepare_diffusion_input(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Diffusion ëª¨ë¸ ìž…ë ¥ ì¤€ë¹„"""
        result = data.copy()
        
        if 'guidance_scale' not in result:
            result['guidance_scale'] = 7.5
        if 'num_inference_steps' not in result:
            result['num_inference_steps'] = 20
        if 'strength' not in result:
            result['strength'] = 0.8
        
        return result
    
    def _prepare_ootd_inputs(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """OOTD Diffusion ìž…ë ¥ ì¤€ë¹„"""
        result = data.copy()
        
        if 'fitting_mode' not in result:
            result['fitting_mode'] = 'hd'
        
        return result
    
    def _extract_pose_features(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í¬ì¦ˆ íŠ¹ì§• ì¶”ì¶œ"""
        return data
    
    def _prepare_sr_input(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Super Resolution ìž…ë ¥ ì¤€ë¹„"""
        result = data.copy()
        
        if 'tile_size' not in result:
            result['tile_size'] = 512
        if 'overlap' not in result:
            result['overlap'] = 64
        
        return result
    
    # ==============================================
    # ðŸ”¥ ì¶œë ¥ ë°ì´í„° ë³€í™˜ ì‹œìŠ¤í…œ (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    async def _convert_output_to_standard_format(self, ai_result: Dict[str, Any]) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì¶œë ¥ â†’ í‘œì¤€ í˜•ì‹ ë³€í™˜"""
        try:
            self.logger.debug(f"ðŸ”„ {self.step_name} ì¶œë ¥ ë°ì´í„° ë³€í™˜ ì‹œìž‘...")
            
            # 1. í›„ì²˜ë¦¬ ì ìš©
            processed_result = await self._apply_postprocessing(ai_result)
            
            # 2. API ì‘ë‹µ í˜•ì‹ ë³€í™˜
            api_response = self._convert_to_api_format(processed_result)
            
            # 3. ë‹¤ìŒ Stepë“¤ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
            next_step_data = self._prepare_next_step_data(processed_result)
            
            # 4. í‘œì¤€ ì‘ë‹µ êµ¬ì¡° ìƒì„±
            standard_response = {
                'success': True,
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': getattr(self, '_last_processing_time', 0.0),
                
                # API ì‘ë‹µ ë°ì´í„°
                **api_response,
                
                # ë‹¤ìŒ Stepë“¤ì„ ìœ„í•œ ë°ì´í„°
                'next_step_data': next_step_data,
                
                # ë©”íƒ€ë°ì´í„°
                'metadata': {
                    'input_shapes': {k: self._get_shape_info(v) for k, v in ai_result.items()},
                    'output_shapes': self.detailed_data_spec.output_shapes,
                    'device': self.device,
                    'github_compatible': True,
                    'detailed_data_spec_applied': True,
                    'data_conversion_version': 'v19.2'
                }
            }
            
            self.logger.debug(f"âœ… {self.step_name} ì¶œë ¥ ë°ì´í„° ë³€í™˜ ì™„ë£Œ")
            return standard_response
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì¶œë ¥ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
            return self._create_error_response(str(e))
    
    def _get_shape_info(self, value: Any) -> Optional[Tuple]:
        """ê°’ì˜ í˜•íƒœ ì •ë³´ ì¶”ì¶œ"""
        try:
            if hasattr(value, 'shape'):
                return tuple(value.shape)
            elif isinstance(value, (list, tuple)):
                return (len(value),)
            else:
                return None
        except:
            return None
    
    async def _apply_postprocessing(self, ai_result: Dict[str, Any]) -> Dict[str, Any]:
        """DetailedDataSpec ê¸°ë°˜ í›„ì²˜ë¦¬ ìžë™ ì ìš©"""
        try:
            if not self.config.auto_postprocessing:
                return ai_result
            
            processed = ai_result.copy()
            
            self.logger.debug(f"ðŸ”„ {self.step_name} í›„ì²˜ë¦¬ ì ìš©: {self.detailed_data_spec.postprocessing_steps}")
            
            for step_name in self.detailed_data_spec.postprocessing_steps:
                if step_name == "softmax":
                    processed = self._apply_softmax(processed)
                elif step_name == "argmax":
                    processed = self._apply_argmax(processed)
                elif step_name == "resize_original":
                    processed = self._resize_to_original(processed)
                elif step_name == "to_numpy":
                    processed = self._convert_to_numpy(processed)
                elif step_name == "threshold_0.5":
                    processed = self._apply_threshold(processed, 0.5)
                elif step_name == "nms":
                    processed = self._apply_nms(processed)
                elif step_name == "denormalize_diffusion" or step_name == "denormalize_centered":
                    processed = self._denormalize_diffusion(processed)
                elif step_name == "denormalize":
                    processed = self._denormalize_imagenet(processed)
                elif step_name == "clip_values" or step_name == "clip_0_1":
                    processed = self._clip_values(processed, 0.0, 1.0)
                elif step_name == "apply_mask" or step_name == "apply_warping_mask":
                    processed = self._apply_mask(processed)
                elif step_name == "morphology_clean":
                    processed = self._morphology_operations(processed)
                elif step_name == "extract_keypoints":
                    processed = self._extract_keypoints(processed)
                elif step_name == "scale_coords":
                    processed = self._scale_coordinates(processed)
                elif step_name == "filter_confidence":
                    processed = self._filter_by_confidence(processed)
                elif step_name == "enhance_details":
                    processed = self._enhance_details(processed)
                elif step_name == "final_compositing":
                    processed = self._final_compositing(processed)
                elif step_name == "generate_quality_report":
                    processed = self._generate_quality_report(processed)
                else:
                    self.logger.debug(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” í›„ì²˜ë¦¬ ë‹¨ê³„: {step_name}")
            
            self.performance_metrics.postprocessing_operations += 1
            self.logger.debug(f"âœ… {self.step_name} í›„ì²˜ë¦¬ ì™„ë£Œ")
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ai_result
    
    def _apply_softmax(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Softmax ì ìš©"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = torch.softmax(value, dim=-1)
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    exp_vals = np.exp(value - np.max(value, axis=-1, keepdims=True))
                    result[key] = exp_vals / np.sum(exp_vals, axis=-1, keepdims=True)
            except Exception as e:
                self.logger.debug(f"Softmax ì ìš© ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _apply_argmax(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Argmax ì ìš©"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = torch.argmax(value, dim=-1)
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = np.argmax(value, axis=-1)
            except Exception as e:
                self.logger.debug(f"Argmax ì ìš© ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _convert_to_numpy(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """NumPy ë³€í™˜"""
        if not NUMPY_AVAILABLE:
            return data
        
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = value.detach().cpu().numpy()
                elif not isinstance(value, np.ndarray):
                    if isinstance(value, (list, tuple)):
                        result[key] = np.array(value)
            except Exception as e:
                self.logger.debug(f"NumPy ë³€í™˜ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _apply_threshold(self, data: Dict[str, Any], threshold: float) -> Dict[str, Any]:
        """ìž„ê³„ê°’ ì ìš©"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = (value > threshold).astype(np.float32)
                elif TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = (value > threshold).float()
            except Exception as e:
                self.logger.debug(f"ìž„ê³„ê°’ ì ìš© ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _denormalize_diffusion(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Diffusion ì—­ì •ê·œí™” ([-1, 1] â†’ [0, 1])"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = (value + 1.0) / 2.0
                elif TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = (value + 1.0) / 2.0
            except Exception as e:
                self.logger.debug(f"Diffusion ì—­ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _denormalize_imagenet(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ImageNet ì—­ì •ê·œí™”"""
        result = data.copy()
        mean = np.array(self.detailed_data_spec.normalization_mean)
        std = np.array(self.detailed_data_spec.normalization_std)
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if len(value.shape) == 3 and value.shape[2] == 3:
                        denormalized = value * std + mean
                        result[key] = np.clip(denormalized, 0, 1)
            except Exception as e:
                self.logger.debug(f"ImageNet ì—­ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _clip_values(self, data: Dict[str, Any], min_val: float, max_val: float) -> Dict[str, Any]:
        """ê°’ ë²”ìœ„ í´ë¦¬í•‘"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = np.clip(value, min_val, max_val)
                elif TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = torch.clamp(value, min_val, max_val)
            except Exception as e:
                self.logger.debug(f"ê°’ í´ë¦¬í•‘ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _apply_mask(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë§ˆìŠ¤í¬ ì ìš©"""
        result = data.copy()
        
        if 'mask' in data and 'image' in data:
            try:
                mask = data['mask']
                image = data['image']
                
                if NUMPY_AVAILABLE:
                    if isinstance(mask, np.ndarray) and isinstance(image, np.ndarray):
                        result['masked_image'] = image * mask
                        
            except Exception as e:
                self.logger.debug(f"ë§ˆìŠ¤í¬ ì ìš© ì‹¤íŒ¨: {e}")
        
        return result
    
    def _morphology_operations(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í˜•íƒœí•™ì  ì—°ì‚° (ë…¸ì´ì¦ˆ ì œê±°)"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if CV2_AVAILABLE and NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if len(value.shape) == 2:
                        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                        opened = cv2.morphologyEx(value.astype(np.uint8), cv2.MORPH_OPEN, kernel)
                        closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)
                        result[key] = closed.astype(np.float32)
                        
            except Exception as e:
                self.logger.debug(f"í˜•íƒœí•™ì  ì—°ì‚° ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _extract_keypoints(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        result = data.copy()
        
        if 'heatmaps' in data:
            try:
                heatmaps = data['heatmaps']
                if NUMPY_AVAILABLE and isinstance(heatmaps, np.ndarray):
                    keypoints = []
                    for i in range(heatmaps.shape[0]):
                        heatmap = heatmaps[i]
                        y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
                        confidence = heatmap[y, x]
                        keypoints.append([x, y, confidence])
                    result['keypoints'] = np.array(keypoints)
                    
            except Exception as e:
                self.logger.debug(f"í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return result
    
    def _scale_coordinates(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§"""
        result = data.copy()
        
        if 'keypoints' in data and 'original_size' in data:
            try:
                keypoints = data['keypoints']
                original_size = data['original_size']
                
                if isinstance(keypoints, np.ndarray) and len(keypoints.shape) == 2:
                    scale_x = original_size[0] / self.detailed_data_spec.input_shapes.get('image', (512, 512))[1]
                    scale_y = original_size[1] / self.detailed_data_spec.input_shapes.get('image', (512, 512))[0]
                    
                    scaled_keypoints = keypoints.copy()
                    scaled_keypoints[:, 0] *= scale_x
                    scaled_keypoints[:, 1] *= scale_y
                    result['scaled_keypoints'] = scaled_keypoints
                    
            except Exception as e:
                self.logger.debug(f"ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨: {e}")
        
        return result
    
    def _filter_by_confidence(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§"""
        result = data.copy()
        
        confidence_threshold = self.config.confidence_threshold
        
        for key, value in data.items():
            try:
                if key.endswith('_confidence') or key.endswith('_scores'):
                    if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                        valid_mask = value > confidence_threshold
                        result[f'{key}_filtered'] = value[valid_mask]
                        
                        base_key = key.replace('_confidence', '').replace('_scores', '')
                        if base_key in data:
                            base_data = data[base_key]
                            if isinstance(base_data, np.ndarray) and len(base_data) == len(value):
                                result[f'{base_key}_filtered'] = base_data[valid_mask]
                                
            except Exception as e:
                self.logger.debug(f"ì‹ ë¢°ë„ í•„í„°ë§ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _enhance_details(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì„¸ë¶€ì‚¬í•­ í–¥ìƒ"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray) and len(value.shape) >= 2:
                    if CV2_AVAILABLE and len(value.shape) == 3:
                        blurred = cv2.GaussianBlur(value, (3, 3), 1.0)
                        sharpened = cv2.addWeighted(value, 1.5, blurred, -0.5, 0)
                        result[f'{key}_enhanced'] = np.clip(sharpened, 0, 1)
                        
            except Exception as e:
                self.logger.debug(f"ì„¸ë¶€ì‚¬í•­ í–¥ìƒ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _final_compositing(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ìµœì¢… í•©ì„±"""
        result = data.copy()
        
        if 'person_image' in data and 'clothing_image' in data and 'mask' in data:
            try:
                person = data['person_image']
                clothing = data['clothing_image']
                mask = data['mask']
                
                if all(isinstance(x, np.ndarray) for x in [person, clothing, mask]):
                    composited = person * (1 - mask) + clothing * mask
                    result['final_composited'] = composited
                    
            except Exception as e:
                self.logger.debug(f"ìµœì¢… í•©ì„± ì‹¤íŒ¨: {e}")
        
        return result
    
    def _generate_quality_report(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        result = data.copy()
        
        quality_metrics = {
            'overall_quality': 0.0,
            'detail_preservation': 0.0,
            'color_consistency': 0.0,
            'artifact_level': 0.0,
            'recommendations': []
        }
        
        try:
            if 'final_result' in data:
                final_result = data['final_result']
                if NUMPY_AVAILABLE and isinstance(final_result, np.ndarray):
                    mean_intensity = np.mean(final_result)
                    std_intensity = np.std(final_result)
                    
                    quality_metrics['overall_quality'] = min(1.0, (mean_intensity + std_intensity) / 2.0)
                    quality_metrics['detail_preservation'] = min(1.0, std_intensity * 2.0)
                    quality_metrics['color_consistency'] = 1.0 - abs(0.5 - mean_intensity)
                    
                    if quality_metrics['overall_quality'] < 0.7:
                        quality_metrics['recommendations'].append('ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„  í•„ìš”')
                    if quality_metrics['detail_preservation'] < 0.5:
                        quality_metrics['recommendations'].append('ì„¸ë¶€ì‚¬í•­ ë³´ì¡´ ê°œì„  í•„ìš”')
            
            result['quality_assessment'] = quality_metrics
            
        except Exception as e:
            self.logger.debug(f"í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            result['quality_assessment'] = quality_metrics
        
        return result
    
    def _apply_nms(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Non-Maximum Suppression ì ìš©"""
        result = data.copy()
        
        if 'detections' in data and 'scores' in data:
            try:
                detections = data['detections']
                scores = data['scores']
                
                if NUMPY_AVAILABLE and isinstance(detections, np.ndarray) and isinstance(scores, np.ndarray):
                    sorted_indices = np.argsort(scores)[::-1]
                    
                    top_k = min(10, len(sorted_indices))
                    result['detections_nms'] = detections[sorted_indices[:top_k]]
                    result['scores_nms'] = scores[sorted_indices[:top_k]]
                    
            except Exception as e:
                self.logger.debug(f"NMS ì ìš© ì‹¤íŒ¨: {e}")
        
        return result
    
    def _resize_to_original(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ"""
        result = data.copy()
        
        if 'original_size' in data:
            original_size = data['original_size']
            
            for key, value in data.items():
                try:
                    if key != 'original_size' and isinstance(value, np.ndarray) and len(value.shape) >= 2:
                        if CV2_AVAILABLE:
                            if len(value.shape) == 3:
                                resized = cv2.resize(value, tuple(original_size))
                            elif len(value.shape) == 2:
                                resized = cv2.resize(value, tuple(original_size))
                            else:
                                continue
                            result[f'{key}_original_size'] = resized
                            
                except Exception as e:
                    self.logger.debug(f"ì›ë³¸ í¬ê¸° ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _convert_to_api_format(self, processed_result: Dict[str, Any]) -> Dict[str, Any]:
        """AI ê²°ê³¼ â†’ API ì‘ë‹µ í˜•ì‹ ë³€í™˜"""
        api_response = {}
        
        try:
            for api_field, api_type in self.detailed_data_spec.api_output_mapping.items():
                if api_field in processed_result:
                    value = processed_result[api_field]
                    
                    if api_type == "base64_string":
                        api_response[api_field] = self._array_to_base64(value)
                    elif api_type == "List[Dict]":
                        api_response[api_field] = self._convert_to_list_dict(value)
                    elif api_type == "List[Dict[str, float]]":
                        api_response[api_field] = self._convert_keypoints_to_dict_list(value)
                    elif api_type == "float":
                        api_response[api_field] = float(value) if value is not None else 0.0
                    elif api_type == "List[float]":
                        if isinstance(value, (list, tuple)):
                            api_response[api_field] = [float(x) for x in value]
                        elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                            api_response[api_field] = value.flatten().tolist()
                        else:
                            api_response[api_field] = [float(value)] if value is not None else []
                    else:
                        api_response[api_field] = value
            
            if not api_response:
                api_response = self._create_fallback_api_response(processed_result)
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} API í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨: {e}")
            api_response = self._create_fallback_api_response(processed_result)
        
        return api_response
    
    def _prepare_next_step_data(self, processed_result: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¤ìŒ Stepë“¤ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        next_step_data = {}
        
        try:
            for next_step, data_schema in self.detailed_data_spec.provides_to_next_step.items():
                step_data = {}
                
                for data_key, data_type in data_schema.items():
                    if data_key in processed_result:
                        value = processed_result[data_key]
                        
                        if data_type == "np.ndarray" and NUMPY_AVAILABLE:
                            if TORCH_AVAILABLE and torch.is_tensor(value):
                                step_data[data_key] = value.detach().cpu().numpy()
                            elif not isinstance(value, np.ndarray):
                                step_data[data_key] = np.array(value)
                            else:
                                step_data[data_key] = value
                                
                        elif data_type == "torch.Tensor" and TORCH_AVAILABLE:
                            if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                                step_data[data_key] = torch.from_numpy(value)
                            elif not torch.is_tensor(value):
                                step_data[data_key] = torch.tensor(value)
                            else:
                                step_data[data_key] = value
                                
                        else:
                            step_data[data_key] = value
                
                if step_data:
                    next_step_data[next_step] = step_data
                    self.performance_metrics.step_data_transfers += 1
        
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ë‹¤ìŒ Step ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
        
        return next_step_data
    
    # ==============================================
    # ðŸ”¥ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    def _validate_input_data(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ìž…ë ¥ ë°ì´í„° ê²€ì¦"""
        validated = input_data.copy()
        
        try:
            for key, value in input_data.items():
                # ë°ì´í„° íƒ€ìž… ê²€ì¦
                if key in self.detailed_data_spec.input_shapes:
                    expected_shape = self.detailed_data_spec.input_shapes[key]
                    if hasattr(value, 'shape'):
                        actual_shape = value.shape
                        if len(actual_shape) > len(expected_shape):
                            if actual_shape[1:] != tuple(expected_shape):
                                self.logger.warning(f"âš ï¸ {self.step_name} Shape mismatch for {key}")
                        elif actual_shape != tuple(expected_shape):
                            self.logger.warning(f"âš ï¸ {self.step_name} Shape mismatch for {key}")
                
                # ê°’ ë²”ìœ„ ê²€ì¦
                if key in self.detailed_data_spec.input_value_ranges:
                    min_val, max_val = self.detailed_data_spec.input_value_ranges[key]
                    if hasattr(value, 'min') and hasattr(value, 'max'):
                        actual_min, actual_max = float(value.min()), float(value.max())
                        if actual_min < min_val or actual_max > max_val:
                            self.logger.warning(f"âš ï¸ {self.step_name} Value range warning for {key}")
                            
                            # ìžë™ í´ë¦¬í•‘
                            if self.config.strict_data_validation:
                                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                                    validated[key] = np.clip(value, min_val, max_val)
                                elif TORCH_AVAILABLE and torch.is_tensor(value):
                                    validated[key] = torch.clamp(value, min_val, max_val)
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ìž…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            self.performance_metrics.validation_failures += 1
        
        return validated
    
    # ==============================================
    # ðŸ”¥ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    def _array_to_base64(self, array: Any) -> str:
        """NumPy ë°°ì—´/í…ì„œ â†’ Base64 ë¬¸ìžì—´ ë³€í™˜"""
        try:
            if TORCH_AVAILABLE and torch.is_tensor(array):
                array = array.detach().cpu().numpy()
            
            if not NUMPY_AVAILABLE or not isinstance(array, np.ndarray):
                return ""
            
            # ê°’ ë²”ìœ„ ì •ê·œí™”
            if array.dtype != np.uint8:
                if array.max() <= 1.0:
                    array = (array * 255).astype(np.uint8)
                else:
                    array = np.clip(array, 0, 255).astype(np.uint8)
            
            # PIL Imageë¡œ ë³€í™˜
            if PIL_AVAILABLE:
                if len(array.shape) == 3:
                    if array.shape[0] in [1, 3, 4] and array.shape[0] < array.shape[1]:
                        array = np.transpose(array, (1, 2, 0))
                    
                    if array.shape[2] == 1:
                        array = array.squeeze(2)
                        image = Image.fromarray(array, mode='L')
                    elif array.shape[2] == 3:
                        image = Image.fromarray(array, mode='RGB')
                    elif array.shape[2] == 4:
                        image = Image.fromarray(array, mode='RGBA')
                    else:
                        image = Image.fromarray(array[:, :, 0], mode='L')
                        
                elif len(array.shape) == 2:
                    image = Image.fromarray(array, mode='L')
                else:
                    raise ValueError(f"Unsupported array shape: {array.shape}")
                
                # Base64 ì¸ì½”ë”©
                buffer = BytesIO()
                image.save(buffer, format='PNG')
                buffer.seek(0)
                
                return base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            return ""
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} Base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""
    
    def _convert_to_list_dict(self, value: Any) -> List[Dict]:
        """ê°’ì„ List[Dict] í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            if isinstance(value, (list, tuple)):
                if all(isinstance(item, dict) for item in value):
                    return list(value)
                else:
                    return [{'value': item, 'index': i} for i, item in enumerate(value)]
            
            elif isinstance(value, dict):
                return [value]
            
            elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                if len(value.shape) == 1:
                    return [{'value': float(item), 'index': i} for i, item in enumerate(value)]
                elif len(value.shape) == 2:
                    return [{'row': i, 'data': row.tolist()} for i, row in enumerate(value)]
                else:
                    return [{'data': value.tolist()}]
            
            else:
                return [{'value': value}]
                
        except Exception as e:
            self.logger.debug(f"List[Dict] ë³€í™˜ ì‹¤íŒ¨: {e}")
            return [{'value': str(value)}]
    
    def _convert_keypoints_to_dict_list(self, keypoints: Any) -> List[Dict[str, float]]:
        """í‚¤í¬ì¸íŠ¸ë¥¼ List[Dict[str, float]] í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            if NUMPY_AVAILABLE and isinstance(keypoints, np.ndarray):
                if len(keypoints.shape) == 2 and keypoints.shape[1] >= 2:
                    result = []
                    for i, point in enumerate(keypoints):
                        point_dict = {
                            'x': float(point[0]),
                            'y': float(point[1])
                        }
                        if keypoints.shape[1] > 2:
                            point_dict['confidence'] = float(point[2])
                        if keypoints.shape[1] > 3:
                            point_dict['visibility'] = float(point[3])
                        
                        point_dict['index'] = i
                        result.append(point_dict)
                    
                    return result
            
            elif isinstance(keypoints, (list, tuple)):
                result = []
                for i, point in enumerate(keypoints):
                    if isinstance(point, (list, tuple)) and len(point) >= 2:
                        point_dict = {
                            'x': float(point[0]),
                            'y': float(point[1]),
                            'index': i
                        }
                        if len(point) > 2:
                            point_dict['confidence'] = float(point[2])
                        result.append(point_dict)
                
                return result
            
            return []
            
        except Exception as e:
            self.logger.debug(f"í‚¤í¬ì¸íŠ¸ Dict ë³€í™˜ ì‹¤íŒ¨: {e}")
            return []
    
    def _create_fallback_api_response(self, processed_result: Dict[str, Any]) -> Dict[str, Any]:
        """í´ë°± API ì‘ë‹µ ìƒì„±"""
        fallback_response = {}
        
        try:
            # ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í‚¤ë“¤ì— ëŒ€í•œ ê¸°ë³¸ ë§¤í•‘
            common_mappings = {
                'parsing_mask': 'base64_string',
                'segmentation_mask': 'base64_string',
                'fitted_image': 'base64_string',
                'enhanced_image': 'base64_string',
                'final_result': 'base64_string',
                'result_image': 'base64_string',
                'output_image': 'base64_string',
                
                'keypoints': 'List[Dict[str, float]]',
                'pose_keypoints': 'List[Dict[str, float]]',
                
                'confidence': 'float',
                'quality_score': 'float'
            }
            
            for key, value in processed_result.items():
                if key in common_mappings:
                    api_type = common_mappings[key]
                    
                    if api_type == 'base64_string':
                        fallback_response[key] = self._array_to_base64(value)
                    elif api_type == 'List[Dict[str, float]]':
                        fallback_response[key] = self._convert_keypoints_to_dict_list(value)
                    elif api_type == 'float':
                        fallback_response[key] = float(value) if value is not None else 0.0
            
            # ê¸°ë³¸ ì‘ë‹µì´ ì—†ëŠ” ê²½ìš° ì²« ë²ˆì§¸ ì´ë¯¸ì§€í˜• ë°ì´í„°ë¥¼ resultë¡œ ì„¤ì •
            if not fallback_response:
                for key, value in processed_result.items():
                    if NUMPY_AVAILABLE and isinstance(value, np.ndarray) and len(value.shape) >= 2:
                        fallback_response['result'] = self._array_to_base64(value)
                        break
                    elif PIL_AVAILABLE and isinstance(value, Image.Image):
                        fallback_response['result'] = self._array_to_base64(np.array(value))
                        break
            
        except Exception as e:
            self.logger.debug(f"í´ë°± API ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return fallback_response
    
    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """í‘œì¤€ ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'step_name': self.step_name,
            'step_id': self.step_id,
            'github_compatible': True,
            'detailed_data_spec_applied': False,
            'processing_time': 0.0,
            'timestamp': time.time()
        }
    
    # ==============================================
    # ðŸ”¥ ê¸°ì¡´ GitHub í˜¸í™˜ ë©”ì„œë“œë“¤ (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    def _create_github_config(self, **kwargs) -> GitHubStepConfig:
        """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì„¤ì • ìƒì„±"""
        config = GitHubStepConfig()
        for key, value in kwargs.items():
            if hasattr(config, key):
                setattr(config, key, value)
        
        # GitHub í”„ë¡œì íŠ¸ íŠ¹ë³„ ì„¤ì •
        config.github_compatibility_mode = True
        config.real_ai_pipeline_support = True
        config.enable_detailed_data_spec = True
        
        # í™˜ê²½ë³„ ì„¤ì • ì ìš©
        if CONDA_INFO['is_target_env']:
            config.conda_optimized = True
            config.conda_env = CONDA_INFO['conda_env']
        
        if IS_M3_MAX:
            config.m3_max_optimized = True
            config.memory_gb = MEMORY_GB
            config.use_unified_memory = True
        
        return config
    
    def _apply_github_environment_optimization(self):
        """GitHub í”„ë¡œì íŠ¸ í™˜ê²½ ìµœì í™”"""
        try:
            # M3 Max GitHub ìµœì í™”
            if self.is_m3_max:
                if self.device == "auto" and MPS_AVAILABLE:
                    self.device = "mps"
                
                if self.config.batch_size == 1 and self.memory_gb >= 64:
                    self.config.batch_size = 2
                
                self.config.auto_memory_cleanup = True
                self.logger.debug(f"âœ… GitHub M3 Max ìµœì í™”: {self.memory_gb:.1f}GB, device={self.device}")
            
            # GitHub conda í™˜ê²½ ìµœì í™”
            if self.conda_info['is_target_env']:
                self.config.optimization_enabled = True
                self.real_ai_pipeline_ready = True
                self.logger.debug(f"âœ… GitHub conda í™˜ê²½ ìµœì í™”: {self.conda_info['conda_env']}")
            
        except Exception as e:
            self.logger.debug(f"GitHub í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _github_emergency_setup(self, error: Exception):
        """GitHub í˜¸í™˜ ê¸´ê¸‰ ì„¤ì •"""
        self.step_name = getattr(self, 'step_name', self.__class__.__name__)
        self.logger = logging.getLogger("github_emergency")
        self.device = "cpu"
        self.is_initialized = False
        self.github_compatible = False
        self.performance_metrics = GitHubPerformanceMetrics()
        self.detailed_data_spec = DetailedDataSpecConfig()
        self.dependency_manager = GitHubDependencyManager(self.step_name)
        self.logger.error(f"ðŸš¨ {self.step_name} GitHub ê¸´ê¸‰ ì´ˆê¸°í™”: {error}")
    
    def _resolve_device(self, device: str) -> str:
        """GitHub ë””ë°”ì´ìŠ¤ í•´ê²° (í™˜ê²½ ìµœì í™”)"""
        if device == "auto":
            # GitHub M3 Max ìš°ì„  ì²˜ë¦¬
            if IS_M3_MAX and MPS_AVAILABLE:
                return "mps"
            
            if TORCH_AVAILABLE:
                if torch.cuda.is_available():
                    return "cuda"
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    return "mps"
            return "cpu"
        return device
    
    def _update_performance_metrics(self, processing_time: float, success: bool):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        try:
            self.performance_metrics.process_count += 1
            self.performance_metrics.total_process_time += processing_time
            self.performance_metrics.average_process_time = (
                self.performance_metrics.total_process_time / 
                self.performance_metrics.process_count
            )
            
            if success:
                self.performance_metrics.success_count += 1
            else:
                self.performance_metrics.error_count += 1
            
            # ìµœê·¼ ì²˜ë¦¬ ì‹œê°„ ì €ìž¥
            self._last_processing_time = processing_time
            
            # GitHub íŒŒì´í”„ë¼ì¸ ì„±ê³µë¥  ê³„ì‚°
            if self.performance_metrics.github_process_calls > 0:
                success_rate = (
                    self.performance_metrics.success_count /
                    self.performance_metrics.process_count * 100
                )
                self.performance_metrics.pipeline_success_rate = success_rate
                
        except Exception as e:
            self.logger.debug(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ðŸ”¥ GitHub í˜¸í™˜ ì˜ì¡´ì„± ì£¼ìž… ì¸í„°íŽ˜ì´ìŠ¤ (ìˆœí™˜ì°¸ì¡° í•´ê²°)
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """GitHub í‘œì¤€ ModelLoader ì˜ì¡´ì„± ì£¼ìž…"""
        try:
            success = self.dependency_manager.inject_model_loader(model_loader)
            if success:
                self.model_loader = model_loader
                self.has_model = True
                self.model_loaded = True
                self.real_ai_pipeline_ready = True
                self.performance_metrics.dependencies_injected += 1
                self.logger.info(f"âœ… {self.step_name} GitHub ModelLoader ì˜ì¡´ì„± ì£¼ìž… ì™„ë£Œ")
        except Exception as e:
            self.performance_metrics.injection_failures += 1
            self.logger.error(f"âŒ {self.step_name} GitHub ModelLoader ì˜ì¡´ì„± ì£¼ìž… ì˜¤ë¥˜: {e}")

    def set_memory_manager(self, memory_manager):
        """GitHub í‘œì¤€ MemoryManager ì˜ì¡´ì„± ì£¼ìž…"""
        try:
            success = self.dependency_manager.inject_memory_manager(memory_manager)
            if success:
                self.memory_manager = memory_manager
                self.performance_metrics.dependencies_injected += 1
        except Exception as e:
            self.performance_metrics.injection_failures += 1
            self.logger.warning(f"âš ï¸ {self.step_name} GitHub MemoryManager ì˜ì¡´ì„± ì£¼ìž… ì˜¤ë¥˜: {e}")
    
    # ==============================================
    # ðŸ”¥ GitHub í˜¸í™˜ ì˜ì¡´ì„± ê²€ì¦ (ìˆœí™˜ì°¸ì¡° í•´ê²°)
    # ==============================================
    
    def validate_dependencies(self, format_type: DependencyValidationFormat = None) -> Union[Dict[str, bool], Dict[str, Any]]:
        """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì˜ì¡´ì„± ê²€ì¦ (v19.2)"""
        try:
            return self.dependency_manager.validate_dependencies_github_format(format_type)
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} GitHub ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            
            if format_type == DependencyValidationFormat.BOOLEAN_DICT:
                return {'model_loader': False, 'step_interface': False, 'memory_manager': False, 'data_converter': False}
            else:
                return {'success': False, 'error': str(e), 'github_compatible': False}
    
    def validate_dependencies_boolean(self) -> Dict[str, bool]:
        """GitHub Step í´ëž˜ìŠ¤ í˜¸í™˜ (GeometricMatchingStep ë“±)"""
        return self.validate_dependencies(DependencyValidationFormat.BOOLEAN_DICT)
    
    def validate_dependencies_detailed(self) -> Dict[str, Any]:
        """StepFactory í˜¸í™˜ (ìƒì„¸ ì •ë³´)"""
        return self.validate_dependencies(DependencyValidationFormat.DETAILED_DICT)
    
    # ==============================================
    # ðŸ”¥ GitHub í‘œì¤€ ì´ˆê¸°í™” ë° ìƒíƒœ ê´€ë¦¬
    # ==============================================
    
    def initialize(self) -> bool:
        """GitHub í‘œì¤€ ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"ðŸ”„ {self.step_name} GitHub í‘œì¤€ ì´ˆê¸°í™” ì‹œìž‘...")
            
            # DetailedDataSpec ê²€ì¦
            if not self.data_conversion_ready:
                self.logger.warning(f"âš ï¸ {self.step_name} DetailedDataSpec ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ë¯¸ì™„ë£Œ")
            
            # ì´ˆê¸°í™” ìƒíƒœ ì„¤ì •
            self.dependency_manager.dependency_status.base_initialized = True
            self.dependency_manager.dependency_status.github_compatible = True
            
            self.is_initialized = True
            
            self.logger.info(f"âœ… {self.step_name} GitHub í‘œì¤€ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} GitHub ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if hasattr(self, 'performance_metrics'):
                self.performance_metrics.error_count += 1
            return False

    def get_status(self) -> Dict[str, Any]:
        """GitHub í†µí•© ìƒíƒœ ì¡°íšŒ (v19.2)"""
        try:
            return {
                'step_info': {
                    'step_name': self.step_name,
                    'step_id': self.step_id,
                    'version': 'BaseStepMixin v19.2 Circular Reference Fix'
                },
                'github_status_flags': {
                    'is_initialized': self.is_initialized,
                    'is_ready': self.is_ready,
                    'has_model': self.has_model,
                    'model_loaded': self.model_loaded,
                    'github_compatible': self.github_compatible,
                    'real_ai_pipeline_ready': self.real_ai_pipeline_ready,
                    'data_conversion_ready': self.data_conversion_ready
                },
                'detailed_data_spec_status': {
                    'spec_loaded': self.dependency_manager.dependency_status.detailed_data_spec_loaded,
                    'data_conversion_ready': self.dependency_manager.dependency_status.data_conversion_ready,
                    'preprocessing_configured': bool(self.detailed_data_spec.preprocessing_steps),
                    'postprocessing_configured': bool(self.detailed_data_spec.postprocessing_steps),
                    'api_mapping_configured': bool(self.detailed_data_spec.api_input_mapping and self.detailed_data_spec.api_output_mapping),
                    'step_flow_configured': bool(self.detailed_data_spec.provides_to_next_step or self.detailed_data_spec.accepts_from_previous_step)
                },
                'github_performance': {
                    'data_conversions': self.performance_metrics.data_conversions,
                    'preprocessing_operations': self.performance_metrics.preprocessing_operations,
                    'postprocessing_operations': self.performance_metrics.postprocessing_operations,
                    'api_conversions': self.performance_metrics.api_conversions,
                    'step_data_transfers': self.performance_metrics.step_data_transfers,
                    'validation_failures': self.performance_metrics.validation_failures
                },
                'timestamp': time.time()
            }
        except Exception as e:
            self.logger.error(f"âŒ GitHub ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'version': 'BaseStepMixin v19.2 Circular Reference Fix'}

# ==============================================
# ðŸ”¥ Export
# ==============================================

__all__ = [
    # ë©”ì¸ í´ëž˜ìŠ¤
    'BaseStepMixin',
    'GitHubDependencyManager',
    
    # ì„¤ì • ë° ìƒíƒœ í´ëž˜ìŠ¤ë“¤
    'DetailedDataSpecConfig',
    'GitHubStepConfig',
    'GitHubDependencyStatus', 
    'GitHubPerformanceMetrics',
    
    # GitHub ì—´ê±°í˜•ë“¤
    'ProcessMethodSignature',
    'DependencyValidationFormat',
    'DataConversionMethod',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'CV2_AVAILABLE',
    'CONDA_INFO',
    'IS_M3_MAX',
    'MEMORY_GB'
]

# ==============================================
# ðŸ”¥ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë¡œê·¸
# ==============================================

logger = logging.getLogger(__name__)
logger.info("=" * 100)
logger.info("ðŸ”¥ BaseStepMixin v19.2 - ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° + ëª¨ë“  ê¸°ëŠ¥ í¬í•¨")
logger.info("=" * 100)
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (GitHubDependencyManager ë‚´ìž¥)")
logger.info("âœ… step_model_requirements.py DetailedDataSpec ì™„ì „ í™œìš©")
logger.info("âœ… API â†” AI ëª¨ë¸ ê°„ ë°ì´í„° ë³€í™˜ í‘œì¤€í™” ì™„ë£Œ")
logger.info("âœ… Step ê°„ ë°ì´í„° íë¦„ ìžë™ ì²˜ë¦¬")
logger.info("âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ìžë™ ì ìš©")
logger.info("âœ… GitHub í”„ë¡œì íŠ¸ Step í´ëž˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜")
logger.info("âœ… process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ì™„ì „ í‘œì¤€í™”")
logger.info("âœ… ì‹¤ì œ Step í´ëž˜ìŠ¤ë“¤ì€ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„í•˜ë©´ ë¨")
logger.info("âœ… validate_dependencies() ì˜¤ë²„ë¡œë“œ ì§€ì›")
logger.info("âœ… ëª¨ë“  ê¸°ëŠ¥ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ ìˆœí™˜ì°¸ì¡°ë§Œ í•´ê²°")

logger.info("ðŸ”§ ìˆœí™˜ì°¸ì¡° í•´ê²° ë°©ë²•:")
logger.info("   ðŸ”— GitHubDependencyManager ë‚´ìž¥ìœ¼ë¡œ ì™¸ë¶€ ì˜ì¡´ì„± ì°¨ë‹¨")
logger.info("   ðŸ”— TYPE_CHECKING + ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€")
logger.info("   ðŸ”— ì‹¤ì œ ì˜ì¡´ì„±ë§Œ ì‚¬ìš© - Mock í´ë°± ì œê±°")
logger.info("   ðŸ”— ëª¨ë“  ê¸°ëŠ¥ ê·¸ëŒ€ë¡œ ìœ ì§€")

logger.info("ðŸŽ¯ ì™„ì „ ë³µì›ëœ ì „ì²˜ë¦¬ (12ê°œ):")
logger.info("   - ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (512x512, 768x1024, 256x192, 224x224, 368x368, 1024x1024)")
logger.info("   - ì •ê·œí™” (ImageNet, CLIP, Diffusion)")
logger.info("   - í…ì„œ ë³€í™˜, SAM í”„ë¡¬í”„íŠ¸, Diffusion ìž…ë ¥, OOTD ìž…ë ¥, í¬ì¦ˆ íŠ¹ì§•, SR ìž…ë ¥")

logger.info("ðŸŽ¯ ì™„ì „ ë³µì›ëœ í›„ì²˜ë¦¬ (15ê°œ):")
logger.info("   - Softmax, Argmax, ì›ë³¸ í¬ê¸° ë¦¬ì‚¬ì´ì¦ˆ, NumPy ë³€í™˜, ìž„ê³„ê°’, NMS")
logger.info("   - ì—­ì •ê·œí™” (Diffusion, ImageNet), ê°’ í´ë¦¬í•‘, ë§ˆìŠ¤í¬ ì ìš©")
logger.info("   - í˜•íƒœí•™ì  ì—°ì‚°, í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ, ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§, ì‹ ë¢°ë„ í•„í„°ë§")
logger.info("   - ì„¸ë¶€ì‚¬í•­ í–¥ìƒ, ìµœì¢… í•©ì„±, í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±")

logger.info(f"ðŸ”§ í˜„ìž¬ conda í™˜ê²½: {CONDA_INFO['conda_env']} ({'âœ… ìµœì í™”ë¨' if CONDA_INFO['is_target_env'] else 'âš ï¸ ê¶Œìž¥: mycloset-ai-clean'})")
logger.info(f"ðŸ–¥ï¸  í˜„ìž¬ ì‹œìŠ¤í…œ: M3 Max={IS_M3_MAX}, ë©”ëª¨ë¦¬={MEMORY_GB:.1f}GB")
logger.info(f"ðŸš€ GitHub AI íŒŒì´í”„ë¼ì¸ ì¤€ë¹„: {TORCH_AVAILABLE and (MPS_AVAILABLE or (torch.cuda.is_available() if TORCH_AVAILABLE else False))}")
logger.info("=" * 100)
logger.info("ðŸŽ‰ BaseStepMixin v19.2 ìˆœí™˜ì°¸ì¡° í•´ê²° + ì™„ì „í•œ ê¸°ëŠ¥ ë³µì› ì™„ë£Œ!")
logger.info("ðŸ’¡ ì´ì œ ì‹¤ì œ Step í´ëž˜ìŠ¤ë“¤ì€ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„í•˜ë©´ ë©ë‹ˆë‹¤!")
logger.info("ðŸ’¡ ëª¨ë“  ë°ì´í„° ë³€í™˜ì´ BaseStepMixinì—ì„œ ìžë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤!")
logger.info("ðŸ’¡ ìˆœí™˜ì°¸ì¡° ë¬¸ì œê°€ ì™„ì „ížˆ í•´ê²°ë˜ê³  ì‹¤ì œ ì˜ì¡´ì„±ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤!")
logger.info("ðŸ’¡ Mock í´ë°±ì´ ì œê±°ë˜ì–´ ì‹¤ì œ ModelLoader, MemoryManager, DataConverterë§Œ í—ˆìš©ë©ë‹ˆë‹¤!")
logger.info("=" * 100)