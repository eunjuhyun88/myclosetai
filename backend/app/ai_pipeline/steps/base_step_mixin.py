"""
ğŸ”¥ BaseStepMixin v20.0 - Central Hub DI Container ì™„ì „ ì—°ë™ + ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°
================================================================================

âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - ì¤‘ì•™ í—ˆë¸Œ íŒ¨í„´ ì ìš©
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - TYPE_CHECKING + ì§€ì—° import ì™„ë²½ ì ìš©
âœ… ë‹¨ë°©í–¥ ì˜ì¡´ì„± ê·¸ë˜í”„ - DI Containerë§Œì„ í†µí•œ ì˜ì¡´ì„± ì£¼ì…
âœ… step_model_requirements.py DetailedDataSpec ì™„ì „ í™œìš© 
âœ… API â†” AI ëª¨ë¸ ê°„ ë°ì´í„° ë³€í™˜ í‘œì¤€í™” ì™„ë£Œ
âœ… Step ê°„ ë°ì´í„° íë¦„ ìë™ ì²˜ë¦¬
âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ìë™ ì ìš©
âœ… GitHub í”„ë¡œì íŠ¸ Step í´ë˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜
âœ… ëª¨ë“  ê¸°ëŠ¥ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ êµ¬ì¡°ë§Œ ê°œì„ 
âœ… ê¸°ì¡´ API 100% í˜¸í™˜ì„± ë³´ì¥
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”

í•µì‹¬ ì„¤ê³„ ì›ì¹™:
1. Single Source of Truth - ëª¨ë“  ì„œë¹„ìŠ¤ëŠ” Central Hub DI Containerë¥¼ ê±°ì¹¨
2. Central Hub Pattern - DI Containerê°€ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì˜ ì¤‘ì‹¬
3. Dependency Inversion - ìƒìœ„ ëª¨ë“ˆì´ í•˜ìœ„ ëª¨ë“ˆì„ ì œì–´
4. Zero Circular Reference - ìˆœí™˜ì°¸ì¡° ì›ì²œ ì°¨ë‹¨

Author: MyCloset AI Team
Date: 2025-07-30
Version: 20.0 (Central Hub DI Container Integration)
"""

import os
import gc
import time
import asyncio
import logging
import threading
import traceback
import weakref
import subprocess
import platform
import inspect
import base64
import warnings
from io import BytesIO
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, Type, TYPE_CHECKING, Awaitable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from functools import wraps
from contextlib import asynccontextmanager
from enum import Enum

# ğŸ”¥ ìˆ˜ì •: ì¶”ê°€ í•„ìˆ˜ importë“¤
from concurrent.futures import ThreadPoolExecutor

# ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=ImportWarning)

# ğŸ”¥ ì—ëŸ¬ ì²˜ë¦¬ í—¬í¼ í•¨ìˆ˜ë“¤ import
try:
    from app.core.exceptions import (
        handle_step_initialization_error,
        handle_dependency_injection_error,
        handle_data_conversion_error,
        handle_central_hub_error,
        create_step_error_response,
        validate_step_environment,
        log_step_performance
    )
    EXCEPTION_HELPERS_AVAILABLE = True
except ImportError:
    EXCEPTION_HELPERS_AVAILABLE = False
    # í´ë°± í•¨ìˆ˜ë“¤ ì •ì˜
    def handle_step_initialization_error(step_name, error, context=None):
        return {'success': False, 'error': 'INIT_ERROR', 'message': str(error)}
    
    def handle_dependency_injection_error(step_name, service_name, error):
        return {'success': False, 'error': 'DI_ERROR', 'message': str(error)}
    
    def handle_data_conversion_error(step_name, conversion_type, error, data_info=None):
        return {'success': False, 'error': 'CONVERSION_ERROR', 'message': str(error)}
    
    def handle_central_hub_error(step_name, operation, error):
        return {'success': False, 'error': 'CENTRAL_HUB_ERROR', 'message': str(error)}
    
    def create_step_error_response(step_name, error, operation="unknown"):
        return {'success': False, 'error': 'STEP_ERROR', 'message': str(error)}
    
    def validate_step_environment(step_name):
        return {'success': True, 'step_name': step_name, 'checks': {}}
    
    def log_step_performance(step_name, operation, start_time, success, error=None):
        return {'step_name': step_name, 'operation': operation, 'success': success}

# ğŸ”¥ ìˆ˜ì •: ì•ˆì „í•œ Logger ì´ˆê¸°í™”
_LOGGER_INITIALIZED = False
_MODULE_LOGGER = None

def get_safe_logger():
    """Thread-safe Logger ì´ˆê¸°í™” (threading ì‚¬ìš©)"""
    global _LOGGER_INITIALIZED, _MODULE_LOGGER
    
    # ğŸ”¥ ìˆ˜ì •: threading.Lock ì‚¬ìš©
    if not hasattr(get_safe_logger, '_lock'):
        get_safe_logger._lock = threading.Lock()
    
    with get_safe_logger._lock:
        if _LOGGER_INITIALIZED and _MODULE_LOGGER is not None:
            return _MODULE_LOGGER
        
        try:
            logger_name = __name__
            _MODULE_LOGGER = logging.getLogger(logger_name)
            
            if not _MODULE_LOGGER.handlers:
                handler = logging.StreamHandler()
                formatter = logging.Formatter(
                    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
                )
                handler.setFormatter(formatter)
                _MODULE_LOGGER.addHandler(handler)
                _MODULE_LOGGER.setLevel(logging.INFO)
            
            _LOGGER_INITIALIZED = True
            return _MODULE_LOGGER
            
        except Exception as e:
            print(f"âš ï¸ Logger ì´ˆê¸°í™” ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
            
            class FallbackLogger:
                def info(self, msg): print(f"INFO: {msg}")
                def error(self, msg): print(f"ERROR: {msg}")
                def warning(self, msg): print(f"WARNING: {msg}")
                def debug(self, msg): print(f"DEBUG: {msg}")
            
            return FallbackLogger()

logger = get_safe_logger()

# ==============================================
# ğŸ”¥ Central Hub DI Container ì•ˆì „ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def _get_central_hub_container():

    """Central Hub DI Container ì•ˆì „í•œ ë™ì  í•´ê²°"""
    try:
        import importlib
        module = importlib.import_module('app.core.di_container')
        get_global_fn = getattr(module, 'get_global_container', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError:
        return None
    except Exception:
        return None

def _inject_dependencies_safe(step_instance):
    """ğŸ”¥ Central Hub v7.0 - ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì… (ì™„ì „í•œ ì„œë¹„ìŠ¤ ì„¸íŠ¸)"""
    try:
        container = _get_central_hub_container()
        if container and hasattr(container, 'inject_to_step'):
            # Central Hub v7.0ì˜ ì™„ì „í•œ inject_to_step ì‚¬ìš©
            injections_made = container.inject_to_step(step_instance)
            logger.debug(f"âœ… Central Hub v7.0 ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {injections_made}ê°œ")
            return injections_made
        else:
            logger.warning("âš ï¸ Central Hub Containerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return 0
    except Exception as e:
        logger.error(f"âŒ Central Hub v7.0 ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        return 0

def _get_service_from_central_hub(service_key: str):
    """Central Hubë¥¼ í†µí•œ ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ"""
    try:
        container = _get_central_hub_container()
        if container:
            return container.get(service_key)
        return None
    except Exception:
        return None

# TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from ..utils.model_loader import ModelLoader, StepModelInterface
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
    from app.core.di_container import CentralHubDIContainer
  
# ==============================================
# ğŸ”¥ í™˜ê²½ ì„¤ì • ë° ì‹œìŠ¤í…œ ì •ë³´
# ==============================================

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean'
}

# ì‹œìŠ¤í…œ ì •ë³´
IS_M3_MAX = False
MEMORY_GB = 16.0

try:
    if platform.system() == 'Darwin':
        result = subprocess.run(
            ['sysctl', '-n', 'machdep.cpu.brand_string'],
            capture_output=True, text=True, timeout=5
        )
        IS_M3_MAX = 'M3' in result.stdout
        
        memory_result = subprocess.run(
            ['sysctl', '-n', 'hw.memsize'],
            capture_output=True, text=True, timeout=5
        )
        if memory_result.stdout.strip():
            MEMORY_GB = int(memory_result.stdout.strip()) / 1024**3
except:
    pass

# ==============================================
# ğŸ”¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì „ import
# ==============================================

# PyTorch ì•ˆì „ import
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
except ImportError:
    torch = None

# PIL ì•ˆì „ import
PIL_AVAILABLE = False
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    Image = None

# NumPy ì•ˆì „ import
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None

# OpenCV ì•ˆì „ import
CV2_AVAILABLE = False
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    cv2 = None

# ==============================================
# ğŸ”¥ GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ (v20.0)
# ==============================================

class ProcessMethodSignature(Enum):
    """GitHub í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” process ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ íŒ¨í„´"""
    STANDARD = "async def process(self, **kwargs) -> Dict[str, Any]"
    INPUT_DATA = "async def process(self, input_data: Any) -> Dict[str, Any]"
    PIPELINE = "async def process_pipeline(self, input_data: Dict[str, Any]) -> Dict[str, Any]"
    LEGACY = "def process(self, *args, **kwargs) -> Dict[str, Any]"

class DependencyValidationFormat(Enum):
    """ì˜ì¡´ì„± ê²€ì¦ ë°˜í™˜ í˜•ì‹"""
    BOOLEAN_DICT = "dict_bool"  # GeometricMatchingStep í˜•ì‹: {'model_loader': True, ...}
    DETAILED_DICT = "dict_detailed"  # BaseStepMixin v18.0 í˜•ì‹: {'success': True, 'details': {...}}
    AUTO_DETECT = "auto"  # í˜¸ì¶œìì— ë”°ë¼ ìë™ ì„ íƒ

class DataConversionMethod(Enum):
    """ë°ì´í„° ë³€í™˜ ë°©ë²•"""
    AUTOMATIC = "auto"      # DetailedDataSpec ê¸°ë°˜ ìë™ ë³€í™˜
    MANUAL = "manual"       # í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ìˆ˜ë™ ë³€í™˜
    HYBRID = "hybrid"       # ìë™ + ìˆ˜ë™ ì¡°í•©

# ==============================================
# ğŸ”¥ ì„¤ì • ë° ìƒíƒœ í´ë˜ìŠ¤ (v20.0 Central Hub ê¸°ë°˜)
# ==============================================

@dataclass
class DetailedDataSpecConfig:
    """DetailedDataSpec ì„¤ì • ê´€ë¦¬"""
    # ì…ë ¥ ì‚¬ì–‘
    input_data_types: List[str] = field(default_factory=list)
    input_shapes: Dict[str, Tuple[int, ...]] = field(default_factory=dict)
    input_value_ranges: Dict[str, Tuple[float, float]] = field(default_factory=dict)
    preprocessing_required: List[str] = field(default_factory=list)
    
    # ì¶œë ¥ ì‚¬ì–‘  
    output_data_types: List[str] = field(default_factory=list)
    output_shapes: Dict[str, Tuple[int, ...]] = field(default_factory=dict)
    output_value_ranges: Dict[str, Tuple[float, float]] = field(default_factory=dict)
    postprocessing_required: List[str] = field(default_factory=list)
    
    # API í˜¸í™˜ì„±
    api_input_mapping: Dict[str, str] = field(default_factory=dict)
    api_output_mapping: Dict[str, str] = field(default_factory=dict)
    
    # Step ê°„ ì—°ë™
    step_input_schema: Dict[str, Any] = field(default_factory=dict)
    step_output_schema: Dict[str, Any] = field(default_factory=dict)
    
    # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­
    normalization_mean: Tuple[float, ...] = field(default_factory=lambda: (0.485, 0.456, 0.406))
    normalization_std: Tuple[float, ...] = field(default_factory=lambda: (0.229, 0.224, 0.225))
    preprocessing_steps: List[str] = field(default_factory=list)
    postprocessing_steps: List[str] = field(default_factory=list)
    
    # Step ê°„ ë°ì´í„° ì „ë‹¬ ìŠ¤í‚¤ë§ˆ
    accepts_from_previous_step: Dict[str, Dict[str, str]] = field(default_factory=dict)
    provides_to_next_step: Dict[str, Dict[str, str]] = field(default_factory=dict)

@dataclass
class CentralHubStepConfig:
    """Central Hub ê¸°ë°˜ Step ì„¤ì • (v20.0)"""
    step_name: str = "BaseStep"
    step_id: int = 0
    device: str = "auto"
    use_fp16: bool = True
    batch_size: int = 1
    confidence_threshold: float = 0.8
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True
    optimization_enabled: bool = True
    quality_level: str = "balanced"
    strict_mode: bool = False
    
    # Central Hub DI Container ì„¤ì •
    auto_inject_dependencies: bool = True
    require_model_loader: bool = True
    require_memory_manager: bool = False
    require_data_converter: bool = False
    dependency_timeout: float = 30.0
    dependency_retry_count: int = 3
    central_hub_integration: bool = True
    
    # GitHub í”„ë¡œì íŠ¸ íŠ¹ë³„ ì„¤ì •
    process_method_signature: ProcessMethodSignature = ProcessMethodSignature.STANDARD
    dependency_validation_format: DependencyValidationFormat = DependencyValidationFormat.AUTO_DETECT
    github_compatibility_mode: bool = True
    real_ai_pipeline_support: bool = True
    
    # DetailedDataSpec ì„¤ì • (v20.0)
    enable_detailed_data_spec: bool = True
    data_conversion_method: DataConversionMethod = DataConversionMethod.AUTOMATIC
    strict_data_validation: bool = True
    auto_preprocessing: bool = True
    auto_postprocessing: bool = True
    
    # í™˜ê²½ ìµœì í™”
    conda_optimized: bool = False
    conda_env: str = "none"
    m3_max_optimized: bool = False
    memory_gb: float = 16.0
    use_unified_memory: bool = False

@dataclass
class CentralHubDependencyStatus:
    """Central Hub ê¸°ë°˜ ì˜ì¡´ì„± ìƒíƒœ (v20.0)"""
    model_loader: bool = False
    step_interface: bool = False
    memory_manager: bool = False
    data_converter: bool = False
    central_hub_container: bool = False
    base_initialized: bool = False
    custom_initialized: bool = False
    dependencies_validated: bool = False
    
    # GitHub íŠ¹ë³„ ìƒíƒœ
    github_compatible: bool = False
    process_method_validated: bool = False
    real_ai_models_loaded: bool = False
    
    # DetailedDataSpec ìƒíƒœ
    detailed_data_spec_loaded: bool = False
    data_conversion_ready: bool = False
    preprocessing_configured: bool = False
    postprocessing_configured: bool = False
    api_mapping_configured: bool = False
    step_flow_configured: bool = False
    
    # Central Hub íŠ¹ë³„ ìƒíƒœ
    central_hub_connected: bool = False
    single_source_of_truth: bool = False
    dependency_inversion_applied: bool = False
    base_initialized: bool = False
    detailed_data_spec_loaded: bool = False
    
    # í™˜ê²½ ìƒíƒœ
    conda_optimized: bool = False
    m3_max_optimized: bool = False
    
    # ì£¼ì… ì‹œë„ ì¶”ì 
    injection_attempts: Dict[str, int] = field(default_factory=dict)
    injection_errors: Dict[str, List[str]] = field(default_factory=dict)
    last_injection_time: float = field(default_factory=time.time)

@dataclass
class CentralHubPerformanceMetrics:
    """Central Hub ê¸°ë°˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ (v20.0)"""
    process_count: int = 0
    total_process_time: float = 0.0
    average_process_time: float = 0.0
    error_count: int = 0
    success_count: int = 0
    cache_hits: int = 0
    
    # ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­
    peak_memory_usage_mb: float = 0.0
    average_memory_usage_mb: float = 0.0
    memory_optimizations: int = 0
    
    # AI ëª¨ë¸ ë©”íŠ¸ë¦­
    models_loaded: int = 0
    total_model_size_gb: float = 0.0
    inference_count: int = 0
    
    # Central Hub ì˜ì¡´ì„± ë©”íŠ¸ë¦­
    dependencies_injected: int = 0
    injection_failures: int = 0
    average_injection_time: float = 0.0
    central_hub_requests: int = 0
    service_resolutions: int = 0
    
    # GitHub íŠ¹ë³„ ë©”íŠ¸ë¦­
    github_process_calls: int = 0
    real_ai_inferences: int = 0
    pipeline_success_rate: float = 0.0
    
    # DetailedDataSpec ë©”íŠ¸ë¦­
    data_conversions: int = 0
    preprocessing_operations: int = 0
    postprocessing_operations: int = 0
    api_conversions: int = 0
    step_data_transfers: int = 0
    validation_failures: int = 0


 # ğŸ”¥ ìˆ˜ì •: threading.Lock ì¶”ê°€
def __post_init__(self):
    self._lock = threading.RLock()

def update_status(self, **kwargs):
    """ğŸ”¥ ìˆ˜ì •: thread-safe ìƒíƒœ ì—…ë°ì´íŠ¸"""
    with self._lock:
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
def get_completion_percentage(self) -> float:
    """ğŸ”¥ ìˆ˜ì •: thread-safe ì™„ë£Œìœ¨ ê³„ì‚°"""
    with self._lock:
        total_fields = 10  # ì´ í•„ë“œ ìˆ˜
        completed_fields = sum([
            self.model_loader,
            self.memory_manager, 
            self.data_converter,
            self.step_interface,
            self.central_hub_container,
            self.central_hub_connected,
            self.single_source_of_truth,
            self.dependency_inversion_applied,
            self.base_initialized,
            self.detailed_data_spec_loaded
        ])
        return (completed_fields / total_fields) * 100

# ğŸ”¥ ìˆ˜ì •: Central Hub DI Container ì§€ì—° import (ìˆœí™˜ì°¸ì¡° ë°©ì§€ + threading ì•ˆì „)
def _get_central_hub_container():
    """ğŸ”¥ ìˆ˜ì •: Central Hub DI Container ì•ˆì „í•œ ë™ì  í•´ê²° (threading ì•ˆì „)"""
    if not hasattr(_get_central_hub_container, '_container_cache'):
        _get_central_hub_container._container_cache = None
        _get_central_hub_container._lock = threading.Lock()
    
    with _get_central_hub_container._lock:
        if _get_central_hub_container._container_cache is not None:
            return _get_central_hub_container._container_cache
        
        try:
            import importlib
            module = importlib.import_module('app.core.di_container')
            get_global_fn = getattr(module, 'get_global_container', None)
            if get_global_fn:
                container = get_global_fn()
                _get_central_hub_container._container_cache = container
                return container
        except ImportError:
            pass
        except Exception:
            pass
        
        # Mock ìƒì„±
        _get_central_hub_container._container_cache = _create_mock_container()
        return _get_central_hub_container._container_cache


def _get_service_from_central_hub(service_key: str):
    """ğŸ”¥ ìˆ˜ì •: Central Hubë¥¼ í†µí•œ ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ (threading ì•ˆì „)"""
    try:
        container = _get_central_hub_container()
        if container:
            return container.get(service_key)
        return None
    except Exception:
        return None

def _inject_dependencies_safe(step_instance):
    """ğŸ”¥ ìˆ˜ì •: Central Hub DI Containerë¥¼ í†µí•œ ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì… (threading ì•ˆì „)"""
    try:
        container = _get_central_hub_container()
        if container and hasattr(container, 'inject_to_step'):
            return container.inject_to_step(step_instance)
        return 0
    except Exception:
        return 0
# ==============================================
# ğŸ”¥ Central Hub ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬ì (v20.0)
# ==============================================

class CentralHubDependencyManager:
    """ğŸ”¥ Central Hub DI Container ì™„ì „ í†µí•© ì˜ì¡´ì„± ê´€ë¦¬ì v20.0"""
    
    def __init__(self, step_name: str, **kwargs):
        """Central Hub DI Container ì™„ì „ í†µí•© ì´ˆê¸°í™”"""
        self.step_name = step_name
        self.logger = logging.getLogger(f"CentralHubDependencyManager.{step_name}")
        
        # ğŸ”¥ í•µì‹¬ ì†ì„±ë“¤
        self.step_instance = None
        self.injected_dependencies = {}
        self.injection_attempts = {}
        self.injection_errors = {}
        
        # ğŸ”¥ Central Hub DI Container ì°¸ì¡° (ì§€ì—° ì´ˆê¸°í™”)
        self._central_hub_container = None
        self._container_initialized = False
        
        # ğŸ”¥ dependency_status ì†ì„± (Central Hub ê¸°ë°˜)
        self.dependency_status = CentralHubDependencyStatus()
        
        # ì‹œê°„ ì¶”ì 
        self.last_injection_time = time.time()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.dependencies_injected = 0
        self.injection_failures = 0
        self.validation_attempts = 0
        self.central_hub_requests = 0
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.RLock()
        
        self.logger.debug(f"âœ… Central Hub ì™„ì „ í†µí•© ì˜ì¡´ì„± ê´€ë¦¬ì ì´ˆê¸°í™”: {step_name}")
    
    def _get_central_hub_container(self):
        """Central Hub DI Container ì§€ì—° ì´ˆê¸°í™” (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        if not self._container_initialized:
            try:
                self._central_hub_container = _get_central_hub_container()
                self._container_initialized = True
                if self._central_hub_container:
                    self.dependency_status.central_hub_connected = True
                    self.dependency_status.single_source_of_truth = True
                    self.logger.debug(f"âœ… {self.step_name} Central Hub Container ì—°ê²° ì„±ê³µ")
                else:
                    self.logger.warning(f"âš ï¸ {self.step_name} Central Hub Container ì—°ê²° ì‹¤íŒ¨")
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} Central Hub Container ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self._central_hub_container = None
                self._container_initialized = True
        
        return self._central_hub_container
    
    def set_step_instance(self, step_instance):
        """Step ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •"""
        try:
            with self._lock:
                self.step_instance = step_instance
                self.logger.debug(f"âœ… {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • ì™„ë£Œ")
                return True
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def auto_inject_dependencies(self) -> bool:
        """ğŸ”¥ Central Hub DI Container ì™„ì „ í†µí•© ìë™ ì˜ì¡´ì„± ì£¼ì…"""
        try:
            with self._lock:
                self.logger.info(f"ğŸ”„ {self.step_name} Central Hub ì™„ì „ í†µí•© ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œì‘...")
                self.central_hub_requests += 1
                
                if not self.step_instance:
                    self.logger.warning(f"âš ï¸ {self.step_name} Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    return False
                
                container = self._get_central_hub_container()
                if not container:
                    self.logger.error(f"âŒ {self.step_name} Central Hub Container ì‚¬ìš© ë¶ˆê°€")
                    return False
                
                # ğŸ”¥ Central Hubì˜ inject_to_step ë©”ì„œë“œ ì‚¬ìš© (í•µì‹¬ ê¸°ëŠ¥)
                injections_made = 0
                try:
                    if hasattr(container, 'inject_to_step'):
                        injections_made = container.inject_to_step(self.step_instance)
                        self.logger.info(f"âœ… {self.step_name} Central Hub inject_to_step ì™„ë£Œ: {injections_made}ê°œ")
                    else:
                        # ìˆ˜ë™ ì£¼ì… (í´ë°±)
                        injections_made = self._manual_injection_fallback(container)
                        self.logger.info(f"âœ… {self.step_name} Central Hub ìˆ˜ë™ ì£¼ì… ì™„ë£Œ: {injections_made}ê°œ")
                        
                except Exception as e:
                    self.logger.error(f"âŒ {self.step_name} Central Hub inject_to_step ì‹¤íŒ¨: {e}")
                    injections_made = self._manual_injection_fallback(container)
                
                # ì£¼ì… ìƒíƒœ ì—…ë°ì´íŠ¸
                if injections_made > 0:
                    self.dependencies_injected += injections_made
                    self.dependency_status.base_initialized = True
                    self.dependency_status.github_compatible = True
                    self.dependency_status.dependency_inversion_applied = True
                    
                    # ê°œë³„ ì˜ì¡´ì„± ìƒíƒœ í™•ì¸
                    self._update_dependency_status()
                    
                    self.logger.info(f"âœ… {self.step_name} Central Hub ì™„ì „ í†µí•© ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                    return True
                else:
                    self.logger.warning(f"âš ï¸ {self.step_name} Central Hub ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨")
                    self.injection_failures += 1
                    return False
                    
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} Central Hub ì™„ì „ í†µí•© ìë™ ì˜ì¡´ì„± ì£¼ì… ì¤‘ ì˜¤ë¥˜: {e}")
            self.injection_failures += 1
            return False
    
    def _manual_injection_fallback(self, container) -> int:
        """ìˆ˜ë™ ì£¼ì… í´ë°± (Central Hub Container ê¸°ë°˜)"""
        injections_made = 0
        
        try:
            # ModelLoader ì£¼ì…
            if not hasattr(self.step_instance, 'model_loader') or self.step_instance.model_loader is None:
                model_loader = container.get('model_loader')
                if model_loader:
                    self.step_instance.model_loader = model_loader
                    self.injected_dependencies['model_loader'] = model_loader
                    injections_made += 1
                    self.logger.debug(f"âœ… {self.step_name} ModelLoader ìˆ˜ë™ ì£¼ì… ì™„ë£Œ")
            
            # MemoryManager ì£¼ì…
            if not hasattr(self.step_instance, 'memory_manager') or self.step_instance.memory_manager is None:
                memory_manager = container.get('memory_manager')
                if memory_manager:
                    self.step_instance.memory_manager = memory_manager
                    self.injected_dependencies['memory_manager'] = memory_manager
                    injections_made += 1
                    self.logger.debug(f"âœ… {self.step_name} MemoryManager ìˆ˜ë™ ì£¼ì… ì™„ë£Œ")
            
            # DataConverter ì£¼ì…
            if not hasattr(self.step_instance, 'data_converter') or self.step_instance.data_converter is None:
                data_converter = container.get('data_converter')
                if data_converter:
                    self.step_instance.data_converter = data_converter
                    self.injected_dependencies['data_converter'] = data_converter
                    injections_made += 1
                    self.logger.debug(f"âœ… {self.step_name} DataConverter ìˆ˜ë™ ì£¼ì… ì™„ë£Œ")
            
            # DI Container ìì²´ ì£¼ì…
            if not hasattr(self.step_instance, 'central_hub_container') or self.step_instance.central_hub_container is None:
                self.step_instance.central_hub_container = container
                self.step_instance.di_container = container  # ê¸°ì¡´ í˜¸í™˜ì„±
                self.injected_dependencies['central_hub_container'] = container
                injections_made += 1
                self.logger.debug(f"âœ… {self.step_name} Central Hub Container ìˆ˜ë™ ì£¼ì… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ìˆ˜ë™ ì£¼ì… í´ë°± ì‹¤íŒ¨: {e}")
        
        return injections_made
    
    def _update_dependency_status(self):
        """ì˜ì¡´ì„± ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            if self.step_instance:
                self.dependency_status.model_loader = hasattr(self.step_instance, 'model_loader') and self.step_instance.model_loader is not None
                self.dependency_status.memory_manager = hasattr(self.step_instance, 'memory_manager') and self.step_instance.memory_manager is not None
                self.dependency_status.data_converter = hasattr(self.step_instance, 'data_converter') and self.step_instance.data_converter is not None
                self.dependency_status.central_hub_container = hasattr(self.step_instance, 'central_hub_container') and self.step_instance.central_hub_container is not None
                
        except Exception as e:
            self.logger.debug(f"ì˜ì¡´ì„± ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def validate_dependencies_central_hub_format(self, format_type=None):
        """Central Hub í˜•ì‹ ì˜ì¡´ì„± ê²€ì¦"""
        try:
            with self._lock:
                self.validation_attempts += 1
                
                container = self._get_central_hub_container()
                if container:
                    self.logger.debug(f"ğŸ” validate_dependencies - Central Hub Container type: {type(container).__name__}")
                
                # Step ì¸ìŠ¤í„´ìŠ¤ í™•ì¸
                if not self.step_instance:
                    dependencies = {
                        'model_loader': False,
                        'memory_manager': False,
                        'data_converter': False,
                        'step_interface': False,
                        'central_hub_container': False
                    }
                else:
                    # ì‹¤ì œ ì˜ì¡´ì„± ìƒíƒœ í™•ì¸
                    dependencies = {
                        'model_loader': hasattr(self.step_instance, 'model_loader') and self.step_instance.model_loader is not None,
                        'memory_manager': hasattr(self.step_instance, 'memory_manager') and self.step_instance.memory_manager is not None,
                        'data_converter': hasattr(self.step_instance, 'data_converter') and self.step_instance.data_converter is not None,
                        'step_interface': True,  # Step ì¸ìŠ¤í„´ìŠ¤ê°€ ì¡´ì¬í•˜ë©´ ì¸í„°í˜ì´ìŠ¤ OK
                        'central_hub_container': hasattr(self.step_instance, 'central_hub_container') and self.step_instance.central_hub_container is not None
                    }
                
                # ë°˜í™˜ í˜•ì‹ ê²°ì •
                if format_type:
                    # format_typeì´ ë¬¸ìì—´ì¸ ê²½ìš°
                    if isinstance(format_type, str) and format_type.upper() == 'BOOLEAN_DICT':
                        return dependencies
                    # format_typeì´ enumì¸ ê²½ìš°
                    elif hasattr(format_type, 'value') and format_type.value == 'dict_bool':
                        return dependencies
                    elif hasattr(format_type, 'value') and format_type.value == 'boolean_dict':
                        return dependencies
                
                # ê¸°ë³¸ê°’: ìƒì„¸ ì •ë³´ ë°˜í™˜
                return {
                    'success': all(dep for key, dep in dependencies.items() if key != 'central_hub_container'),
                    'dependencies': dependencies,
                    'github_compatible': True,
                    'central_hub_integrated': True,
                    'injected_count': len(self.injected_dependencies),
                    'step_name': self.step_name,
                    'dependency_status': {
                        'model_loader': self.dependency_status.model_loader,
                        'memory_manager': self.dependency_status.memory_manager,
                        'data_converter': self.dependency_status.data_converter,
                        'central_hub_container': self.dependency_status.central_hub_container,
                        'base_initialized': self.dependency_status.base_initialized,
                        'github_compatible': self.dependency_status.github_compatible,
                        'central_hub_connected': self.dependency_status.central_hub_connected,
                        'single_source_of_truth': self.dependency_status.single_source_of_truth,
                        'dependency_inversion_applied': self.dependency_status.dependency_inversion_applied
                    },
                    'metrics': {
                        'injected': self.dependencies_injected,
                        'failures': self.injection_failures,
                        'validation_attempts': self.validation_attempts,
                        'central_hub_requests': self.central_hub_requests
                    },
                    'central_hub_stats': container.get_stats() if container and hasattr(container, 'get_stats') else {'error': 'get_stats method not available'},
                    'timestamp': time.time()
                }
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} Central Hub ê¸°ë°˜ ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'github_compatible': False,
                'central_hub_integrated': True,
                'step_name': self.step_name
            }

    def get_dependency_status(self) -> Dict[str, Any]:
        """Central Hub ê¸°ë°˜ ì˜ì¡´ì„± ìƒíƒœ ì¡°íšŒ"""
        try:
            with self._lock:
                container = self._get_central_hub_container()
                
                return {
                    'step_name': self.step_name,
                    'step_instance_set': self.step_instance is not None,
                    'injected_dependencies': list(self.injected_dependencies.keys()),
                    'dependency_status': {
                        'model_loader': self.dependency_status.model_loader,
                        'memory_manager': self.dependency_status.memory_manager,
                        'data_converter': self.dependency_status.data_converter,
                        'central_hub_container': self.dependency_status.central_hub_container,
                        'base_initialized': self.dependency_status.base_initialized,
                        'github_compatible': self.dependency_status.github_compatible,
                        'detailed_data_spec_loaded': self.dependency_status.detailed_data_spec_loaded,
                        'data_conversion_ready': self.dependency_status.data_conversion_ready,
                        'central_hub_connected': self.dependency_status.central_hub_connected,
                        'single_source_of_truth': self.dependency_status.single_source_of_truth,
                        'dependency_inversion_applied': self.dependency_status.dependency_inversion_applied
                    },
                    'central_hub_info': {
                        'connected': container is not None,
                        'initialized': self._container_initialized,
                        'stats': container.get_stats() if container and hasattr(container, 'get_stats') else {'error': 'get_stats method not available'}
                    },
                    'metrics': {
                        'dependencies_injected': self.dependencies_injected,
                        'injection_failures': self.injection_failures,
                        'validation_attempts': self.validation_attempts,
                        'central_hub_requests': self.central_hub_requests,
                        'last_injection_time': self.last_injection_time
                    },
                    'timestamp': time.time()
                }
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} Central Hub ê¸°ë°˜ ì˜ì¡´ì„± ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'step_name': self.step_name,
                'error': str(e),
                'central_hub_integrated': True,
                'timestamp': time.time()
            }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (Central Hub ê¸°ë°˜)"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} Central Hub ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬ì ì •ë¦¬ ì‹œì‘...")
            
            # Central Hub Containerë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ìµœì í™”
            if self._central_hub_container:
                try:
                    cleanup_stats = self._central_hub_container.optimize_memory()
                    self.logger.debug(f"Central Hub Container ë©”ëª¨ë¦¬ ìµœì í™”: {cleanup_stats}")
                except Exception as e:
                    self.logger.debug(f"Central Hub Container ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            
            # ì •ë¦¬
            self.injected_dependencies.clear()
            self.injection_attempts.clear()
            self.injection_errors.clear()
            
            self.logger.info(f"âœ… {self.step_name} Central Hub ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬ì ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} Central Hub ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬ì ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ê¸°ì¡´ ì†ì„± ë³´ì¥ ì‹œìŠ¤í…œ - BaseStepMixinì— ì¶”ê°€í•  ì½”ë“œ
# ==============================================

class StepPropertyGuarantee:
    """Step ì†ì„± ë³´ì¥ ì‹œìŠ¤í…œ - ëª¨ë“  ê¸°ì¡´ ì†ì„±ë“¤ ìë™ ì´ˆê¸°í™”"""
    
    # ğŸ”¥ ëª¨ë“  Stepì—ì„œ í•„ìš”í•œ í•„ìˆ˜ ì†ì„±ë“¤ ì •ì˜ (í”„ë¡œì íŠ¸ ì§€ì‹ ë¶„ì„ ê²°ê³¼)
    ESSENTIAL_PROPERTIES = {
        # AI ëª¨ë¸ ê´€ë ¨ (HumanParsingStep ë“±ì—ì„œ í•„ìˆ˜)
        'ai_models': dict,
        'models_loading_status': dict,
        'loaded_models': dict,
        'model_interface': type(None),
        'model_loader': type(None),
        
        # ë©”ëª¨ë¦¬ ë° ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
        'memory_manager': type(None),
        'data_converter': type(None),
        'di_container': type(None),
        
        # Step ìƒíƒœ ê´€ë¦¬ (ëª¨ë“  Stepì—ì„œ ì‚¬ìš©)
        'is_initialized': bool,
        'is_ready': bool,
        'has_model': bool,
        'model_loaded': bool,
        'warmup_completed': bool,
        
        # ì„±ëŠ¥ ë° í†µê³„ (PostProcessingStep ë“±ì—ì„œ í•„ìˆ˜)
        'ai_stats': dict,
        'performance_metrics': dict,
        'performance_stats': dict,
        'process_count': int,
        'success_count': int,
        'error_count': int,
        'total_processing_count': int,
        'last_processing_time': float,
        
        # ì˜ì¡´ì„± ìƒíƒœ ì¶”ì 
        'dependencies_injected': dict,
        'dependency_status': dict,
        'dependency_manager': type(None),
        
        # ì„¤ì • ë° í™˜ê²½
        'config': dict,
        'device': str,
        'strict_mode': bool,
        'step_name': str,
        'step_id': int,
        
        # GitHub í˜¸í™˜ì„± ë° DetailedDataSpec
        'github_compatible': bool,
        'detailed_data_spec': type(None),
        'data_conversion_ready': bool,
        'real_ai_pipeline_ready': bool,
        
        # ì¶”ê°€ ì‹¤í–‰ ê´€ë ¨ ì†ì„±ë“¤
        'executor': type(None),
        'parsing_cache': dict,
        'segmentation_cache': dict,
        'quality_cache': dict,
        'available_methods': list,
        'fabric_properties': dict,
        
        # í™˜ê²½ ì •ë³´
        'is_m3_max': bool,
        'memory_gb': float,
        'conda_info': dict,
        
        # DetailedDataSpec ê´€ë ¨
        'api_input_mapping': dict,
        'api_output_mapping': dict,
        'preprocessing_steps': list,
        'postprocessing_steps': list,
        
        # ê¸°íƒ€ ì¤‘ìš” ì†ì„±ë“¤
        'logger': type(None),
        'initialization_time': float,
        'processing_results': dict,
    }
    
    # ğŸ”¥ íŠ¹ë³„í•œ ê¸°ë³¸ê°’ ìƒì„± í•¨ìˆ˜ë“¤ (Stepë³„ ìš”êµ¬ì‚¬í•­ ë°˜ì˜)
    @staticmethod
    def _create_ai_models():
        """AI ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ ìƒì„± - ëª¨ë“  ê°€ëŠ¥í•œ AI ëª¨ë¸ ìŠ¬ë¡¯"""
        return {
            # Step 01 - Human Parsing
            'graphonomy': None,
            'primary_model': None,
            'parsing_model': None,
            
            # Step 03 - Cloth Segmentation  
            'u2net': None,
            'sam_model': None,
            'segmentation_model': None,
            'u2net_alternative': None,
            
            # Step 05 - Cloth Warping
            'realvisx_model': None,
            'warping_model': None,
            'fabric_simulation_model': None,
            
            # Step 06 - Virtual Fitting
            'ootd_diffusion': None,
            'fitting_model': None,
            'diffusion_model': None,
            
            # Step 07 - Post Processing
            'esrgan_model': None,
            'swinir_model': None,
            'real_esrgan_model': None,
            'enhancement_model': None,
            
            # Step 08 - Quality Assessment
            'clip_model': None,
            'quality_model': None,
            'assessment_model': None,
            
            # ê³µí†µ ëª¨ë¸ë“¤
            'secondary_model': None,
            'backup_model': None,
            'classification_model': None,
            'pose_model': None,
        }
    
    @staticmethod
    def _create_models_loading_status():
        """ëª¨ë¸ ë¡œë”© ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ìƒì„± - ëª¨ë“  ëª¨ë¸ì˜ ë¡œë”© ìƒíƒœ ì¶”ì """
        return {
            # ë¡œë”© í†µê³„
            'total_models': 0,
            'loaded_models': 0,
            'failed_models': 0,
            'loading_errors': [],
            'loading_time': 0.0,
            'success_rate': 0.0,
            
            # Step 01 - Human Parsing ëª¨ë¸ë“¤
            'graphonomy': False,
            'parsing_model': False,
            
            # Step 03 - Cloth Segmentation ëª¨ë¸ë“¤
            'u2net': False,
            'sam_model': False,
            'segmentation_model': False,
            'u2net_alternative': False,
            
            # Step 05 - Cloth Warping ëª¨ë¸ë“¤
            'realvisx_model': False,
            'warping_model': False,
            'fabric_simulation_model': False,
            
            # Step 06 - Virtual Fitting ëª¨ë¸ë“¤
            'ootd_diffusion': False,
            'fitting_model': False,
            'diffusion_model': False,
            
            # Step 07 - Post Processing ëª¨ë¸ë“¤
            'esrgan_model': False,
            'swinir_model': False,
            'real_esrgan_model': False,
            'enhancement_model': False,
            
            # Step 08 - Quality Assessment ëª¨ë¸ë“¤
            'clip_model': False,
            'quality_model': False,
            'assessment_model': False,
            
            # ê³µí†µ ëª¨ë¸ë“¤
            'pose_model': False,
            'classification_model': False,
            'primary_model': False,
            'secondary_model': False,
            'backup_model': False,
        }
    
    @staticmethod
    def _create_ai_stats():
        """AI í†µê³„ ë”•ì…”ë„ˆë¦¬ ìƒì„± - ModelLoader ë° íŒ©í† ë¦¬ íŒ¨í„´ í†µê³„"""
        return {
            'model_loader_calls': 0,
            'factory_pattern_calls': 0,
            'inference_calls': 0,
            'total_processing_time': 0.0,
            'average_processing_time': 0.0,
            'memory_usage_mb': 0.0,
            'gpu_utilization': 0.0,
            'cache_hits': 0,
            'cache_misses': 0,
            'step_interface_calls': 0,
            'di_container_requests': 0,
            'dependency_injections': 0,
            'real_ai_inferences': 0,
            'fallback_usages': 0,
        }
    
    @staticmethod
    def _create_performance_metrics():
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ ìƒì„± - ìƒì„¸í•œ ì„±ëŠ¥ ì¶”ì """
        return {
            'initialization_time': 0.0,
            'first_inference_time': 0.0,
            'warmup_time': 0.0,
            'total_inference_time': 0.0,
            'average_inference_time': 0.0,
            'peak_memory_usage': 0.0,
            'model_load_time': 0.0,
            'data_conversion_time': 0.0,
            'preprocessing_time': 0.0,
            'postprocessing_time': 0.0,
            'api_response_time': 0.0,
            'step_to_step_time': 0.0,
            'dependency_injection_time': 0.0,
        }
    
    @staticmethod
    def _create_performance_stats():
        """ì„±ëŠ¥ í†µê³„ ë”•ì…”ë„ˆë¦¬ ìƒì„± - ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€"""
        return {
            'total_processed': 0,
            'avg_processing_time': 0.0,
            'error_count': 0,
            'success_rate': 1.0,
            'memory_usage_mb': 0.0,
            'models_loaded': 0,
            'cache_hits': 0,
            'ai_inference_count': 0,
            'torch_errors': 0,
            'mps_optimizations': 0,
            'conda_optimizations': 0,
        }
    
    @staticmethod
    def _create_dependencies_injected():
        """ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ìƒì„± - Central Hub í˜¸í™˜"""
        return {
            'model_loader': False,
            'memory_manager': False,
            'data_converter': False,
            'di_container': False,
            'central_hub_container': False,
            'step_interface': False,
            'dependency_manager': False,
            'base_step_mixin': True,  # ê¸°ë³¸ê°’ True
            'github_compatible': True,  # ê¸°ë³¸ê°’ True
            'property_injection': False,
        }
    
    @staticmethod
    def _create_dependency_status():
        """ì˜ì¡´ì„± ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ìƒì„± - ìƒì„¸í•œ ì˜ì¡´ì„± ì¶”ì """
        return {
            'base_initialized': False,
            'github_compatible': True,
            'detailed_data_spec_loaded': False,
            'data_conversion_ready': False,
            'model_loader': False,
            'memory_manager': False,
            'data_converter': False,
            'di_container': False,
            'central_hub_connected': False,
            'property_injection_completed': False,
            'model_interface_ready': False,
            'checkpoint_loading_ready': False,
            'auto_injection_attempted': False,
            'manual_injection_attempted': False,
        }
    
    @staticmethod
    def _create_detailed_data_spec():
        """DetailedDataSpec ë”•ì…”ë„ˆë¦¬ ìƒì„± - ê¸°ë³¸ ë°ì´í„° ìŠ¤í™"""
        return {
            'loaded': False,
            'api_input_mapping': {
                'person_image': 'fastapi.UploadFile -> PIL.Image.Image',
                'clothing_image': 'fastapi.UploadFile -> PIL.Image.Image',
                'data': 'Dict[str, Any] -> Dict[str, Any]'
            },
            'api_output_mapping': {
                'result': 'numpy.ndarray -> base64_string',
                'success': 'bool -> bool',
                'processing_time': 'float -> float',
                'confidence': 'float -> float',
                'quality_score': 'float -> float'
            },
            'preprocessing_requirements': {
                'resize_512x512': True,
                'normalize_imagenet': True,
                'to_tensor': True
            },
            'postprocessing_requirements': {
                'to_numpy': True,
                'clip_0_1': True,
                'resize_original': True
            },
            'data_flow': {
                'input_validation': True,
                'output_formatting': True,
                'error_handling': True
            },
            'step_specific_config': {}
        }
    
    @staticmethod
    def _create_conda_info():
        """Conda í™˜ê²½ ì •ë³´ ìƒì„±"""
        import os
        return {
            'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
            'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
            'is_target_env': os.environ.get('CONDA_DEFAULT_ENV') == 'mycloset-ai-clean',
            'conda_optimized': False
        }
    
    @staticmethod
    def _create_processing_results():
        """ì²˜ë¦¬ ê²°ê³¼ ìºì‹œ ìƒì„±"""
        return {
            'last_result': None,
            'cached_results': {},
            'result_history': [],
            'error_history': [],
            'timing_history': [],
            'memory_snapshots': []
        }
    
    @classmethod
    def guarantee_properties(cls, step_instance):
        """Step ì¸ìŠ¤í„´ìŠ¤ì˜ ëª¨ë“  ì†ì„± ë³´ì¥"""
        try:
            guaranteed_count = 0
            missing_properties = []
            
            for prop_name, prop_type in cls.ESSENTIAL_PROPERTIES.items():
                if not hasattr(step_instance, prop_name):
                    # ì†ì„±ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒì„±
                    default_value = cls._get_default_value(prop_name, prop_type)
                    setattr(step_instance, prop_name, default_value)
                    guaranteed_count += 1
                    missing_properties.append(prop_name)
                elif getattr(step_instance, prop_name) is None and prop_type != type(None):
                    # ì†ì„±ì´ Noneì¸ë° Noneì´ ì•„ë‹ˆì–´ì•¼ í•˜ëŠ” ê²½ìš°
                    default_value = cls._get_default_value(prop_name, prop_type)
                    setattr(step_instance, prop_name, default_value)
                    guaranteed_count += 1
                    missing_properties.append(f"{prop_name}(None->filled)")
            
            # ë¡œê±°ê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not hasattr(step_instance, 'logger') or step_instance.logger is None:
                import logging
                step_instance.logger = logging.getLogger(step_instance.__class__.__name__)
                guaranteed_count += 1
                missing_properties.append('logger')
            
            # Step ê¸°ë³¸ ì •ë³´ ì„¤ì •
            if not hasattr(step_instance, 'step_name') or not step_instance.step_name:
                step_instance.step_name = step_instance.__class__.__name__
                guaranteed_count += 1
                missing_properties.append('step_name')
            
            if guaranteed_count > 0:
                step_instance.logger.info(f"âœ… ì†ì„± ë³´ì¥ ì™„ë£Œ: {guaranteed_count}ê°œ ì†ì„± ì´ˆê¸°í™”")
                step_instance.logger.debug(f"ğŸ”§ ë³´ì¥ëœ ì†ì„±ë“¤: {missing_properties}")
            
            # ì˜ì¡´ì„± ìƒíƒœ ì—…ë°ì´íŠ¸
            if hasattr(step_instance, 'dependency_status'):
                step_instance.dependency_status['property_injection_completed'] = True
                step_instance.dependency_status['base_initialized'] = True
            
            return guaranteed_count
            
        except Exception as e:
            # ë¡œê±°ê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ print ì‚¬ìš©
            print(f"âŒ ì†ì„± ë³´ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    @classmethod
    def _get_default_value(cls, prop_name: str, prop_type: type):
        """ì†ì„±ë³„ ê¸°ë³¸ê°’ ë°˜í™˜"""
        # íŠ¹ë³„í•œ ìƒì„± í•¨ìˆ˜ê°€ ìˆëŠ” ì†ì„±ë“¤
        special_creators = {
            'ai_models': cls._create_ai_models,
            'models_loading_status': cls._create_models_loading_status,
            'loaded_models': lambda: {},
            'ai_stats': cls._create_ai_stats,
            'performance_metrics': cls._create_performance_metrics,
            'performance_stats': cls._create_performance_stats,
            'dependencies_injected': cls._create_dependencies_injected,
            'dependency_status': cls._create_dependency_status,
            'detailed_data_spec': cls._create_detailed_data_spec,
            'config': lambda: {},
            'parsing_cache': lambda: {},
            'segmentation_cache': lambda: {},
            'quality_cache': lambda: {},
            'conda_info': cls._create_conda_info,
            'processing_results': cls._create_processing_results,
            'api_input_mapping': lambda: {},
            'api_output_mapping': lambda: {},
            'preprocessing_steps': lambda: [],
            'postprocessing_steps': lambda: [],
            'available_methods': lambda: [],
            'fabric_properties': lambda: {},
        }
        
        if prop_name in special_creators:
            return special_creators[prop_name]()
        
        # íƒ€ì…ë³„ ê¸°ë³¸ê°’
        if prop_type == dict:
            return {}
        elif prop_type == list:
            return []
        elif prop_type == bool:
            # ê¸°ë³¸ì ìœ¼ë¡œ False, íŠ¹ë³„í•œ ê²½ìš°ë§Œ True
            if prop_name in ['github_compatible', 'data_conversion_ready']:
                return True
            return False
        elif prop_type == int:
            if prop_name == 'step_id':
                return 0
            return 0
        elif prop_type == float:
            if prop_name == 'memory_gb':
                return 16.0  # ê¸°ë³¸ ë©”ëª¨ë¦¬
            return 0.0
        elif prop_type == str:
            if prop_name == 'device':
                return "cpu"
            elif prop_name == 'step_name':
                return "BaseStep"
            return ""
        else:
            return None

# ==============================================
# ğŸ”¥ BaseStepMixinì— ì¶”ê°€í•  ì´ˆê¸°í™” ì½”ë“œ
# ==============================================

def enhance_base_step_mixin_init(original_init):
    """BaseStepMixin.__init__ ë©”ì„œë“œë¥¼ ê°•í™”í•˜ëŠ” ë°ì½”ë ˆì´í„°"""
    def enhanced_init(self, *args, **kwargs):
        # ğŸ”¥ 1ë‹¨ê³„: ì›ë³¸ ì´ˆê¸°í™” ì‹¤í–‰
        try:
            original_init(self, *args, **kwargs)
        except Exception as e:
            # ì›ë³¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œì—ë„ ì†ì„± ë³´ì¥ì€ ì‹¤í–‰
            print(f"âš ï¸ ì›ë³¸ ì´ˆê¸°í™” ì‹¤íŒ¨, ì†ì„± ë³´ì¥ ì§„í–‰: {e}")
        
        # ğŸ”¥ 2ë‹¨ê³„: ëª¨ë“  ê¸°ì¡´ ì†ì„±ë“¤ ë³´ì¥ (ì—ëŸ¬ ë°©ì§€)
        guaranteed_count = StepPropertyGuarantee.guarantee_properties(self)
        
        # ğŸ”¥ 3ë‹¨ê³„: ì¶”ê°€ í˜¸í™˜ì„± ë³´ì¥
        try:
            # M3 Max í™˜ê²½ ê°ì§€
            if not hasattr(self, 'is_m3_max'):
                import platform
                import subprocess
                try:
                    if platform.system() == 'Darwin':
                        result = subprocess.run(
                            ['sysctl', '-n', 'machdep.cpu.brand_string'],
                            capture_output=True, text=True, timeout=5
                        )
                        self.is_m3_max = 'M3' in result.stdout
                    else:
                        self.is_m3_max = False
                except:
                    self.is_m3_max = False
            
            # ë©”ëª¨ë¦¬ ì •ë³´ ì„¤ì •
            if not hasattr(self, 'memory_gb') or self.memory_gb == 0.0:
                self.memory_gb = 128.0 if self.is_m3_max else 16.0
            
            # ë¡œê±° ë©”ì‹œì§€
            if guaranteed_count > 0:
                self.logger.info(f"ğŸ›¡ï¸ BaseStepMixin ì†ì„± ë³´ì¥ ì‹œìŠ¤í…œ í™œì„±í™”: {guaranteed_count}ê°œ ì†ì„± ìë™ ìƒì„±")
                self.logger.info(f"ğŸ”§ í™˜ê²½: M3 Max={self.is_m3_max}, ë©”ëª¨ë¦¬={self.memory_gb:.1f}GB")
            
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.debug(f"âš ï¸ ì¶”ê°€ í˜¸í™˜ì„± ë³´ì¥ ì‹¤íŒ¨: {e}")
    
    return enhanced_init

# ==============================================
# ğŸ”¥ ì‚¬ìš©ë²• - BaseStepMixin í´ë˜ìŠ¤ì— ì ìš©
# ==============================================

# BaseStepMixin í´ë˜ìŠ¤ ì •ì˜ì—ì„œ __init__ ë©”ì„œë“œì— ë‹¤ìŒ ì½”ë“œ ì¶”ê°€:

class BaseStepMixin:
        
    def __init__(self, device: str = "auto", strict_mode: bool = False, **kwargs):
        """BaseStepMixin ì´ˆê¸°í™” - PropertyInjectionMixin ê¸°ëŠ¥ ì§ì ‘ ë‚´ì¥"""
        try:
            # ğŸ”¥ 1. PropertyInjectionMixin ê¸°ëŠ¥ì„ ì§ì ‘ ë‚´ì¥
            self._di_container = None
            self.central_hub_container = None
            self.di_container = None  # ê¸°ì¡´ í˜¸í™˜ì„±
            
            # ğŸ”¥ 2. ì˜ì¡´ì„± ì£¼ì…ëœ ì„œë¹„ìŠ¤ë“¤ ì§ì ‘ ì„ ì–¸
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            
            # ğŸ”¥ 3. ê¸°ì¡´ BaseStepMixin ì´ˆê¸°í™” ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            self.config = self._create_central_hub_config(**kwargs)
            # ğŸ”¥ ìˆ˜ì •: step_name ì¤‘ë³µ ì „ë‹¬ ë°©ì§€ - kwargsì—ì„œ ì œê±°
            if 'step_name' in kwargs:
                self.step_name = kwargs.pop('step_name')
            else:
                self.step_name = self.__class__.__name__
            self.step_id = kwargs.get('step_id', getattr(self, 'STEP_ID', 0))
            
            # Logger ì„¤ì • (ì œì¼ ë¨¼ì €)
            self.logger = logging.getLogger(f"steps.{self.step_name}")
            if not self.logger.handlers:
                handler = logging.StreamHandler()
                formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
                handler.setFormatter(formatter)
                self.logger.addHandler(handler)
                self.logger.setLevel(logging.INFO)

            # ğŸ”¥ ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ ì¶”ì  (Central Hub ê¸°ë°˜)
            self.dependencies_injected = {
                'model_loader': False,
                'memory_manager': False,
                'data_converter': False,
                'central_hub_container': False
            }

            # ê¸°ë³¸ ì†ì„±ë“¤ ì´ˆê¸°í™”
            self.device = device if device != "auto" else ("mps" if TORCH_AVAILABLE and MPS_AVAILABLE else "cpu")
            self.strict_mode = strict_mode
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False

            # GitHub í˜¸í™˜ ì†ì„±ë“¤ (Central Hub ê¸°ë°˜)
            self.model_interface = None

            # ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™”
            self._initialize_performance_stats()

            # ğŸ”¥ DetailedDataSpec ì •ë³´ ì €ì¥
            self.detailed_data_spec = self._load_detailed_data_spec_from_kwargs(**kwargs)
            
            # ğŸ”¥ Central Hub ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬ì (ìˆœí™˜ì°¸ì¡° í•´ê²°)
            self.dependency_manager = CentralHubDependencyManager(self.step_name)
            self.dependency_manager.set_step_instance(self)

            # ì‹œìŠ¤í…œ ì •ë³´
            self.is_m3_max = IS_M3_MAX
            self.memory_gb = MEMORY_GB
            self.conda_info = CONDA_INFO
            
            # GitHub í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±ë“¤
            self.github_compatible = True
            self.real_ai_pipeline_ready = False
            self.process_method_signature = self.config.process_method_signature
            
            # Central Hub í˜¸í™˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­
            self.performance_metrics = CentralHubPerformanceMetrics()
            
            # ğŸ”¥ DetailedDataSpec ìƒíƒœ
            self.data_conversion_ready = self._validate_data_conversion_readiness()
            
            # í™˜ê²½ ìµœì í™” ì ìš©
            self._apply_central_hub_environment_optimization()
            
            # ğŸ”¥ 4. PropertyInjectionMixin ê¸°ëŠ¥ ì§ì ‘ êµ¬í˜„ - Central Hub DI Container ìë™ ì—°ë™
            self._auto_connect_central_hub()
            
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ (PropertyInjectionMixin ê¸°ëŠ¥ ë‚´ì¥)")
            
        except Exception as e:
            self._central_hub_emergency_setup(e)

    def _auto_connect_central_hub(self):
        """Central Hub DI Container ìë™ ì—°ê²° - PropertyInjectionMixin ê¸°ëŠ¥ ëŒ€ì²´"""
        try:
            container = _get_central_hub_container()
            if container:
                self.set_di_container(container)
                self.logger.debug(f"âœ… {self.step_name} Central Hub ìë™ ì—°ê²° ì™„ë£Œ")
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¡°ìš©íˆ ë¬´ì‹œ (ì˜ì¡´ì„± ì£¼ì…ì€ ì„ íƒì‚¬í•­)
            self.logger.debug(f"Central Hub ìë™ ì—°ê²° ì‹¤íŒ¨: {e}")

    def set_di_container(self, container):
        """DI Container ì„¤ì • - PropertyInjectionMixin ê¸°ëŠ¥ ë‚´ì¥"""
        try:
            self._di_container = container
            self.central_hub_container = container
            self.di_container = container  # ê¸°ì¡´ í˜¸í™˜ì„±
            self._auto_inject_properties()
            
            # dependency_manager ì—…ë°ì´íŠ¸
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                self.dependency_manager._central_hub_container = container
                self.dependency_manager._container_initialized = True
                self.dependency_manager.dependency_status.central_hub_connected = True
            
            self.dependencies_injected['central_hub_container'] = True
            self.logger.debug(f"âœ… {self.step_name} DI Container ì„¤ì • ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} DI Container ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    def _auto_inject_properties(self):
        """ìë™ ì†ì„± ì£¼ì… - PropertyInjectionMixin ê¸°ëŠ¥ ë‚´ì¥"""
        if not self._di_container:
            return
        
        injection_map = {
            'model_loader': 'model_loader',
            'memory_manager': 'memory_manager', 
            'data_converter': 'data_converter'
        }
        
        for attr_name, service_key in injection_map.items():
            if not hasattr(self, attr_name) or getattr(self, attr_name) is None:
                try:
                    service = self._di_container.get(service_key)
                    if service:
                        setattr(self, attr_name, service)
                        self.dependencies_injected[attr_name] = True
                        self.logger.debug(f"âœ… {self.step_name} {attr_name} ìë™ ì£¼ì… ì™„ë£Œ")
                except Exception as e:
                    # ì„œë¹„ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ë„ ê³„ì† ì§„í–‰
                    self.logger.debug(f"âš ï¸ {self.step_name} {attr_name} ìë™ ì£¼ì… ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ ê²€ì¦ í•¨ìˆ˜ë“¤
# ==============================================

def validate_step_properties(step_instance) -> Dict[str, Any]:
    """Step ì†ì„± ê²€ì¦"""
    try:
        missing_properties = []
        present_properties = []
        
        for prop_name in StepPropertyGuarantee.ESSENTIAL_PROPERTIES:
            if hasattr(step_instance, prop_name):
                present_properties.append(prop_name)
            else:
                missing_properties.append(prop_name)
        
        return {
            'valid': len(missing_properties) == 0,
            'missing_properties': missing_properties,
            'present_properties': present_properties,
            'total_properties': len(StepPropertyGuarantee.ESSENTIAL_PROPERTIES),
            'coverage_percentage': (len(present_properties) / len(StepPropertyGuarantee.ESSENTIAL_PROPERTIES)) * 100,
            'critical_properties_status': {
                'ai_models': hasattr(step_instance, 'ai_models'),
                'models_loading_status': hasattr(step_instance, 'models_loading_status'),
                'dependencies_injected': hasattr(step_instance, 'dependencies_injected'),
                'logger': hasattr(step_instance, 'logger') and step_instance.logger is not None,
            }
        }
        
    except Exception as e:
        return {
            'valid': False,
            'error': str(e),
            'missing_properties': [],
            'present_properties': [],
            'coverage_percentage': 0
        }

def create_step_with_guaranteed_properties(step_class, **kwargs):
    """ì†ì„± ë³´ì¥ê³¼ í•¨ê»˜ Step ìƒì„±"""
    try:
        # Step ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        step_instance = step_class(**kwargs)
        
        # ì¶”ê°€ ì†ì„± ë³´ì¥ (ìƒì„±ìì—ì„œ ëˆ„ë½ë  ìˆ˜ ìˆëŠ” ê²½ìš° ëŒ€ë¹„)
        StepPropertyGuarantee.guarantee_properties(step_instance)
        
        return step_instance
        
    except Exception as e:
        import logging
        logger = logging.getLogger("StepCreator")
        logger.error(f"âŒ Step ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def fix_step_attribute_errors(step_instance):
    """ê¸°ì¡´ Step ì¸ìŠ¤í„´ìŠ¤ì˜ ì†ì„± ì—ëŸ¬ ìˆ˜ì •"""
    try:
        # ì†ì„± ë³´ì¥ ì‹¤í–‰
        guaranteed_count = StepPropertyGuarantee.guarantee_properties(step_instance)
        
        # ê²€ì¦ ì‹¤í–‰
        validation_result = validate_step_properties(step_instance)
        
        return {
            'success': True,
            'guaranteed_properties': guaranteed_count,
            'validation_result': validation_result,
            'fixed': guaranteed_count > 0
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'guaranteed_properties': 0,
            'fixed': False
        }

# ==============================================
# ğŸ”¥ Export
# ==============================================

class BaseStepMixin:
    """
    ğŸ”¥ BaseStepMixin v20.0 - Central Hub DI Container ì™„ì „ ì—°ë™ + ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì§€ì›
    
    í•µì‹¬ ê°œì„ ì‚¬í•­:
    âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° (TYPE_CHECKING + ì§€ì—° import)
    âœ… ë‹¨ë°©í–¥ ì˜ì¡´ì„± ê·¸ë˜í”„ (Central Hub íŒ¨í„´)
    âœ… ModelLoader v5.1 ì™„ì „ í˜¸í™˜
    âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦ ì‹œìŠ¤í…œ
    âœ… Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ìë™ ë“±ë¡
    âœ… ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰ (Mock ì œê±°)
    âœ… DetailedDataSpec ì •ë³´ ì €ì¥ ë° ê´€ë¦¬
    âœ… í‘œì¤€í™”ëœ process ë©”ì„œë“œ ì¬ì„¤ê³„
    âœ… API â†” AI ëª¨ë¸ ê°„ ë°ì´í„° ë³€í™˜ í‘œì¤€í™”
    âœ… Step ê°„ ë°ì´í„° íë¦„ ìë™ ì²˜ë¦¬
    âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ìë™ ì ìš©
    âœ… GitHub í”„ë¡œì íŠ¸ Step í´ë˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜
    âœ… ê¸°ì¡´ API 100% í˜¸í™˜ì„± ë³´ì¥
    """
    
    def __init__(self, device: str = "auto", strict_mode: bool = False, **kwargs):
        """BaseStepMixin ì´ˆê¸°í™” - Central Hub DI Container ì™„ì „ ì—°ë™"""
        start_time = time.time()
        
        try:
            # ê¸°ë³¸ ì„¤ì •
            self.config = self._create_central_hub_config(**kwargs)
            self.step_name = kwargs.get('step_name', self.__class__.__name__)
            self.step_id = kwargs.get('step_id', getattr(self, 'STEP_ID', 0))
            
            # Logger ì„¤ì • (ì œì¼ ë¨¼ì €)
            self.logger = logging.getLogger(f"steps.{self.step_name}")
            if not self.logger.handlers:
                handler = logging.StreamHandler()
                formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
                handler.setFormatter(formatter)
                self.logger.addHandler(handler)
                self.logger.setLevel(logging.INFO)

            # ğŸ”¥ ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ ì¶”ì  (Central Hub ê¸°ë°˜)
            self.dependencies_injected = {
                'model_loader': False,
                'memory_manager': False,
                'data_converter': False,
                'central_hub_container': False
            }
            
            self._inject_detailed_data_spec_attributes(kwargs)

            # ê¸°ë³¸ ì†ì„±ë“¤ ì´ˆê¸°í™”
            self.device = device if device != "auto" else ("mps" if TORCH_AVAILABLE and MPS_AVAILABLE else "cpu")
            self.strict_mode = strict_mode
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False

            # GitHub í˜¸í™˜ ì†ì„±ë“¤ (Central Hub ê¸°ë°˜)
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
            self.central_hub_container = None
            self.di_container = None  # ê¸°ì¡´ í˜¸í™˜ì„±

            # ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™”
            self._initialize_performance_stats()

            # ğŸ”¥ DetailedDataSpec ì •ë³´ ì €ì¥
            self.detailed_data_spec = self._load_detailed_data_spec_from_kwargs(**kwargs)
            
            # ğŸ”¥ Central Hub ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬ì (ìˆœí™˜ì°¸ì¡° í•´ê²°)
            self.dependency_manager = CentralHubDependencyManager(self.step_name)
            self.dependency_manager.set_step_instance(self)

            # ì‹œìŠ¤í…œ ì •ë³´
            self.is_m3_max = IS_M3_MAX
            self.memory_gb = MEMORY_GB
            self.conda_info = CONDA_INFO
            
            # GitHub í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±ë“¤
            self.github_compatible = True
            self.real_ai_pipeline_ready = False
            self.process_method_signature = self.config.process_method_signature
            
            # Central Hub í˜¸í™˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­
            self.performance_metrics = CentralHubPerformanceMetrics()
            
            # ğŸ”¥ DetailedDataSpec ìƒíƒœ
            self.data_conversion_ready = self._validate_data_conversion_readiness()
            
            # í™˜ê²½ ìµœì í™” ì ìš©
            self._apply_central_hub_environment_optimization()
            
            # ğŸ”¥ Central Hub DI Container ìë™ ì—°ë™
            self._setup_central_hub_integration()
            
            # ì„±ëŠ¥ ë¡œê¹…
            log_step_performance(self.step_name, "initialization", start_time, True)
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ (Central Hub ì™„ì „ ì—°ë™)")
            
        except Exception as e:
            # ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
            error_response = create_step_error_response(self.step_name, e, "initialization")
            log_step_performance(self.step_name, "initialization", start_time, False, e)
            
            # ë¡œê±°ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            try:
                self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.logger.error(f"ğŸ’¡ ì œì•ˆ: {error_response.get('suggestion', 'ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”')}")
            except:
                print(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # ê¸°ë³¸ ì†ì„±ë“¤ë§Œ ì„¤ì •í•˜ì—¬ ìµœì†Œí•œì˜ ë™ì‘ ë³´ì¥
            self.step_name = kwargs.get('step_name', self.__class__.__name__)
            self.step_id = kwargs.get('step_id', 0)
            self.device = device if device != "auto" else "cpu"
            self.strict_mode = strict_mode
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            self.dependencies_injected = {}
            self.performance_stats = {}
            self.github_compatible = False
            self.real_ai_pipeline_ready = False

    def _inject_detailed_data_spec_attributes(self, kwargs: Dict[str, Any]):
        """DetailedDataSpec ì†ì„± ìë™ ì£¼ì…"""
        # âœ… API ë§¤í•‘ ì†ì„± ì£¼ì…
        self.api_input_mapping = kwargs.get('api_input_mapping', {})
        self.api_output_mapping = kwargs.get('api_output_mapping', {})
        
        # âœ… Step ê°„ ë°ì´í„° íë¦„ ì†ì„± ì£¼ì…  
        self.accepts_from_previous_step = kwargs.get('accepts_from_previous_step', {})
        self.provides_to_next_step = kwargs.get('provides_to_next_step', {})
        
        # âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ì†ì„± ì£¼ì…
        self.preprocessing_steps = kwargs.get('preprocessing_steps', [])
        self.postprocessing_steps = kwargs.get('postprocessing_steps', [])
        self.preprocessing_required = kwargs.get('preprocessing_required', [])
        self.postprocessing_required = kwargs.get('postprocessing_required', [])
        
        # âœ… ë°ì´í„° íƒ€ì… ë° ìŠ¤í‚¤ë§ˆ ì†ì„± ì£¼ì…
        self.input_data_types = kwargs.get('input_data_types', [])
        self.output_data_types = kwargs.get('output_data_types', [])
        self.step_input_schema = kwargs.get('step_input_schema', {})
        self.step_output_schema = kwargs.get('step_output_schema', {})
        
        # âœ… ì •ê·œí™” íŒŒë¼ë¯¸í„° ì£¼ì…
        self.normalization_mean = kwargs.get('normalization_mean', (0.485, 0.456, 0.406))
        self.normalization_std = kwargs.get('normalization_std', (0.229, 0.224, 0.225))
        
        # âœ… ë©”íƒ€ì •ë³´ ì£¼ì…
        self.detailed_data_spec_loaded = kwargs.get('detailed_data_spec_loaded', True)
        self.detailed_data_spec_version = kwargs.get('detailed_data_spec_version', 'v11.2')
        self.step_model_requirements_integrated = kwargs.get('step_model_requirements_integrated', True)
        self.central_hub_integrated = kwargs.get('central_hub_integrated', True)
        
        # âœ… FastAPI í˜¸í™˜ì„± í”Œë˜ê·¸
        self.fastapi_compatible = len(self.api_input_mapping) > 0
        
        self.logger.debug(f"âœ… {self.step_name} DetailedDataSpec ì†ì„± ì£¼ì… ì™„ë£Œ")

    # ğŸ”¥ API ë³€í™˜ ë©”ì„œë“œ í™œì„±í™” (ê¸°ì¡´ ì½”ë“œ ìˆ˜ì •)
    async def convert_api_input_to_step_input(self, api_input: Dict[str, Any]) -> Dict[str, Any]:
        """API ì…ë ¥ì„ Step ì…ë ¥ìœ¼ë¡œ ë³€í™˜ - ë¹„ë™ê¸° ë²„ì „"""
        if not self.api_input_mapping:
            # ë§¤í•‘ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            self.logger.debug(f"{self.step_name} API ë§¤í•‘ ì—†ìŒ, ì›ë³¸ ë°˜í™˜")
            return api_input
        
        converted = {}
        
        # âœ… API ë§¤í•‘ ê¸°ë°˜ ë³€í™˜
        for api_param, api_type in self.api_input_mapping.items():
            if api_param in api_input:
                converted_value = await self._convert_api_input_type(
                    api_input[api_param], api_type, api_param
                )
                converted[api_param] = converted_value
        
        # âœ… ëˆ„ë½ëœ í•„ìˆ˜ ì…ë ¥ ë°ì´í„° í™•ì¸
        for param_name in self.api_input_mapping.keys():
            if param_name not in converted and param_name in api_input:
                converted[param_name] = api_input[param_name]
        
        self.logger.debug(f"âœ… {self.step_name} API â†’ Step ë³€í™˜ ì™„ë£Œ")
        return converted

    def convert_api_input_to_step_input(self, api_input: Dict[str, Any]) -> Dict[str, Any]:
        """API ì…ë ¥ì„ Step ì…ë ¥ìœ¼ë¡œ ë³€í™˜ - ë™ê¸° ë²„ì „ (ì˜¤ë²„ë¼ì´ë“œ)"""
        if not self.api_input_mapping:
            # ë§¤í•‘ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            self.logger.debug(f"{self.step_name} API ë§¤í•‘ ì—†ìŒ, ì›ë³¸ ë°˜í™˜")
            return api_input
        
        converted = {}
        
        # âœ… API ë§¤í•‘ ê¸°ë°˜ ë³€í™˜ (ë™ê¸° ë²„ì „)
        for api_param, api_type in self.api_input_mapping.items():
            if api_param in api_input:
                converted_value = self._convert_api_input_type_sync(
                    api_input[api_param], api_type, api_param
                )
                converted[api_param] = converted_value
        
        # âœ… ëˆ„ë½ëœ í•„ìˆ˜ ì…ë ¥ ë°ì´í„° í™•ì¸
        for param_name in self.api_input_mapping.keys():
            if param_name not in converted and param_name in api_input:
                converted[param_name] = api_input[param_name]
        
        self.logger.debug(f"âœ… {self.step_name} API â†’ Step ë³€í™˜ ì™„ë£Œ (ë™ê¸°)")
        return converted

    def convert_step_output_to_api_response(self, step_output: Dict[str, Any]) -> Dict[str, Any]:
        """Step ì¶œë ¥ì„ API ì‘ë‹µìœ¼ë¡œ ë³€í™˜ - í™œì„±í™”"""
        if not self.api_output_mapping:
            # ë§¤í•‘ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return step_output
        
        api_response = {}
        
        # âœ… API ì¶œë ¥ ë§¤í•‘ ê¸°ë°˜ ë³€í™˜
        for step_key, api_type in self.api_output_mapping.items():
            if step_key in step_output:
                converted_value = self._convert_step_output_type_sync(
                    step_output[step_key], api_type, step_key
                )
                api_response[step_key] = converted_value
        
        # âœ… ë©”íƒ€ë°ì´í„° ì¶”ê°€
        api_response.update({
            'step_name': self.step_name,
            'processing_time': step_output.get('processing_time', 0),
            'confidence': step_output.get('confidence', 0.95),
            'success': step_output.get('success', True)
        })
        
        self.logger.debug(f"âœ… {self.step_name} Step â†’ API ë³€í™˜ ì™„ë£Œ")
        return api_response

    def _convert_step_output_type_sync(self, value: Any, api_type: str, param_name: str) -> Any:
        """Step ì¶œë ¥ íƒ€ì…ì„ API íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (ë™ê¸° ë²„ì „)"""
        if api_type == "base64_string":
            return self._array_to_base64(value)
        elif api_type == "List[Dict]":
            return self._convert_to_list_dict(value)
        elif api_type == "List[Dict[str, float]]":
            return self._convert_keypoints_to_dict_list(value)
        elif api_type == "float":
            return float(value) if value is not None else 0.0
        elif api_type == "List[float]":
            if isinstance(value, (list, tuple)):
                return [float(x) for x in value]
            elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                return value.flatten().tolist()
            else:
                return [float(value)] if value is not None else []
        else:
            return value


    def _setup_central_hub_integration(self):
        """ğŸ”¥ Central Hub DI Container ìë™ ì—°ë™"""
        # Central Hub Container ìë™ ì—°ë™
        injections_made = _inject_dependencies_safe(self)
        if injections_made > 0:
            self.logger.info(f"âœ… Central Hub ìë™ ì—°ë™ ì™„ë£Œ: {injections_made}ê°œ ì˜ì¡´ì„± ì£¼ì…")
            
            # ì£¼ì…ëœ ì˜ì¡´ì„±ë“¤ í™•ì¸ ë° ìƒíƒœ ì—…ë°ì´íŠ¸
            if hasattr(self, 'model_loader') and self.model_loader:
                self.dependencies_injected['model_loader'] = True
            if hasattr(self, 'memory_manager') and self.memory_manager:
                self.dependencies_injected['memory_manager'] = True
            if hasattr(self, 'data_converter') and self.data_converter:
                self.dependencies_injected['data_converter'] = True
            if hasattr(self, 'central_hub_container') and self.central_hub_container:
                self.dependencies_injected['central_hub_container'] = True
            
            # ğŸ”¥ ModelLoaderì— ìì‹ ì„ ë“±ë¡ ì‹œë„
            if hasattr(self, 'model_loader') and self.model_loader:
                if hasattr(self.model_loader, 'register_step_requirements'):
                    requirements = self._get_step_requirements()
                    self.model_loader.register_step_requirements(self.step_name, requirements)
                    self.logger.debug("âœ… ModelLoaderì— Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì™„ë£Œ")
        else:
            self.logger.debug("âš ï¸ Central Hub ìë™ ì—°ë™ì—ì„œ ì£¼ì…ëœ ì˜ì¡´ì„±ì´ ì—†ìŒ")
            
            # ìˆ˜ë™ ì—°ë™ ì‹œë„
            self._manual_central_hub_integration()

    def _manual_central_hub_integration(self):
        """ìˆ˜ë™ Central Hub ì—°ë™ (í´ë°±)"""
        container = _get_central_hub_container()
        if container:
            self.central_hub_container = container
            self.di_container = container  # ê¸°ì¡´ í˜¸í™˜ì„±
            self.dependencies_injected['central_hub_container'] = True
            
            # ê°œë³„ ì„œë¹„ìŠ¤ ì¡°íšŒ ë° ì£¼ì…
            model_loader = _get_service_from_central_hub('model_loader')
            if model_loader:
                self.set_model_loader(model_loader)
            
            memory_manager = _get_service_from_central_hub('memory_manager')
            if memory_manager:
                self.set_memory_manager(memory_manager)
            
            data_converter = _get_service_from_central_hub('data_converter')
            if data_converter:
                self.set_data_converter(data_converter)
            
            self.logger.info("âœ… Central Hub ìˆ˜ë™ ì—°ë™ ì™„ë£Œ")
        else:
            self.logger.warning("âš ï¸ Central Hub Container ì‚¬ìš© ë¶ˆê°€")

    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (Central Hub í˜¸í™˜)"""
        self.model_loader = model_loader
        
        # ğŸ”¥ Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        if hasattr(model_loader, 'create_step_interface'):
            self.model_interface = model_loader.create_step_interface(self.step_name)
            self.logger.debug("âœ… Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í…ŒìŠ¤íŠ¸
        if hasattr(model_loader, 'validate_di_container_integration'):
            validation_result = model_loader.validate_di_container_integration()
            if validation_result.get('di_container_available', False):
                self.logger.debug("âœ… ModelLoader Central Hub ì—°ë™ í™•ì¸ë¨")
        
        # ì˜ì¡´ì„± ìƒíƒœ ì—…ë°ì´íŠ¸
        self.dependencies_injected['model_loader'] = True
        if hasattr(self, 'dependency_manager') and self.dependency_manager:
            self.dependency_manager.dependency_status.model_loader = True
            self.dependency_manager.dependency_status.base_initialized = True
        
        self.has_model = True
        self.model_loaded = True
        self.real_ai_pipeline_ready = True
        
        self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ (Central Hub í˜¸í™˜)")
        return True

    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì… (Central Hub í˜¸í™˜)"""
        self.memory_manager = memory_manager
        
        # ì˜ì¡´ì„± ìƒíƒœ ì—…ë°ì´íŠ¸
        self.dependencies_injected['memory_manager'] = True
        if hasattr(self, 'dependency_manager') and self.dependency_manager:
            self.dependency_manager.dependency_status.memory_manager = True
        
        self.logger.debug("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ (Central Hub í˜¸í™˜)")
        return True

    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì… (Central Hub í˜¸í™˜)"""
        self.data_converter = data_converter
        
        # ì˜ì¡´ì„± ìƒíƒœ ì—…ë°ì´íŠ¸
        self.dependencies_injected['data_converter'] = True
        if hasattr(self, 'dependency_manager') and self.dependency_manager:
            self.dependency_manager.dependency_status.data_converter = True
        
        self.logger.debug("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ (Central Hub í˜¸í™˜)")
        return True

    def set_central_hub_container(self, central_hub_container):
        """Central Hub Container ì„¤ì •"""
        try:
            # dependency_managerë¥¼ í†µí•œ ì£¼ì…
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                self.dependency_manager._central_hub_container = central_hub_container
                self.dependency_manager._container_initialized = True
                self.dependency_manager.dependency_status.central_hub_connected = True
                self.dependency_manager.dependency_status.single_source_of_truth = True
            
            self.central_hub_container = central_hub_container
            self.di_container = central_hub_container  # ê¸°ì¡´ í˜¸í™˜ì„±
            self.dependencies_injected['central_hub_container'] = True
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            if hasattr(self, 'performance_metrics'):
                self.performance_metrics.dependencies_injected += 1
            
            self.logger.debug(f"âœ… {self.step_name} Central Hub Container ì„¤ì • ì™„ë£Œ")
            
            # Central Hub Containerë¥¼ í†µí•œ ì¶”ê°€ ì˜ì¡´ì„± ìë™ ì£¼ì… ì‹œë„
            self._try_additional_central_hub_injections()
            
            return True
                
        except Exception as e:
            if hasattr(self, 'performance_metrics'):
                self.performance_metrics.injection_failures += 1
            self.logger.error(f"âŒ {self.step_name} Central Hub Container ì„¤ì • ì˜¤ë¥˜: {e}")
            return False

    def set_di_container(self, di_container):
        """DI Container ì„¤ì • (ê¸°ì¡´ API í˜¸í™˜ì„±)"""
        return self.set_central_hub_container(di_container)

    def _try_additional_central_hub_injections(self):
        """Central Hub Container ì„¤ì • í›„ ì¶”ê°€ ì˜ì¡´ì„± ìë™ ì£¼ì… ì‹œë„"""
        try:
            if not self.central_hub_container:
                return
            
            # ëˆ„ë½ëœ ì˜ì¡´ì„±ë“¤ ìë™ ì£¼ì… ì‹œë„
            if not self.model_loader:
                model_loader = self.central_hub_container.get('model_loader')
                if model_loader:
                    self.set_model_loader(model_loader)
                    self.logger.debug(f"âœ… {self.step_name} ModelLoader Central Hub ì¶”ê°€ ì£¼ì…")
            
            if not self.memory_manager:
                memory_manager = self.central_hub_container.get('memory_manager')
                if memory_manager:
                    self.set_memory_manager(memory_manager)
                    self.logger.debug(f"âœ… {self.step_name} MemoryManager Central Hub ì¶”ê°€ ì£¼ì…")
            
            if not self.data_converter:
                data_converter = self.central_hub_container.get('data_converter')
                if data_converter:
                    self.set_data_converter(data_converter)
                    self.logger.debug(f"âœ… {self.step_name} DataConverter Central Hub ì¶”ê°€ ì£¼ì…")
                    
        except Exception as e:
            self.logger.debug(f"Central Hub ì¶”ê°€ ì£¼ì… ì‹¤íŒ¨: {e}")

    def _get_step_requirements(self) -> Dict[str, Any]:
        """Stepë³„ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ ë°˜í™˜ (fix_checkpoints.py ê²€ì¦ ê²°ê³¼ ê¸°ë°˜)"""
        
        # ğŸ”¥ ê²€ì¦ëœ ëª¨ë¸ ê²½ë¡œë“¤ (fix_checkpoints.py ê²°ê³¼)
        step_model_mappings = {
            "HumanParsingStep": {
                "required_models": ["graphonomy.pth"],
                "verified_paths": ["checkpoints/step_01_human_parsing/graphonomy.pth"],
                "model_configs": {
                    "graphonomy.pth": {
                        "size_mb": 170.5,
                        "ai_class": "RealGraphonomyModel",
                        "verified": True
                    }
                }
            },
            "ClothSegmentationStep": {
                "required_models": ["sam_vit_h_4b8939.pth", "u2net_alternative.pth"],
                "verified_paths": [
                    "checkpoints/step_03_cloth_segmentation/sam_vit_h_4b8939.pth",
                    "checkpoints/step_03_cloth_segmentation/u2net_alternative.pth"
                ],
                "model_configs": {
                    "sam_vit_h_4b8939.pth": {
                        "size_mb": 2445.7,
                        "ai_class": "RealSAMModel",
                        "verified": True
                    },
                    "u2net_alternative.pth": {
                        "size_mb": 38.8,
                        "ai_class": "RealSAMModel",
                        "verified": True
                    }
                }
            },
            "ClothWarpingStep": {
                "required_models": ["RealVisXL_V4.0.safetensors"],
                "verified_paths": ["checkpoints/step_05_cloth_warping/RealVisXL_V4.0.safetensors"],
                "model_configs": {
                    "RealVisXL_V4.0.safetensors": {
                        "size_mb": 6616.6,
                        "ai_class": "RealVisXLModel",
                        "verified": True
                    }
                }
            },
            "VirtualFittingStep": {
                "required_models": ["diffusion_pytorch_model.safetensors"],
                "verified_paths": [
                    "step_06_virtual_fitting/ootdiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors",
                    "step_06_virtual_fitting/unet/diffusion_pytorch_model.safetensors"
                ],
                "model_configs": {
                    "diffusion_pytorch_model.safetensors": {
                        "size_mb": 3278.9,
                        "ai_class": "RealOOTDDiffusionModel",
                        "verified": True
                    }
                }
            },
            "QualityAssessmentStep": {
                "required_models": ["open_clip_pytorch_model.bin"],
                "verified_paths": ["step_08_quality_assessment/ultra_models/open_clip_pytorch_model.bin"],
                "model_configs": {
                    "open_clip_pytorch_model.bin": {
                        "size_mb": 5213.7,
                        "ai_class": "RealCLIPModel",
                        "verified": True
                    }
                }
            },
            "PoseEstimationStep": {
                "required_models": ["diffusion_pytorch_model.safetensors"],
                "verified_paths": ["step_02_pose_estimation/ultra_models/diffusion_pytorch_model.safetensors"],
                "model_configs": {
                    "diffusion_pytorch_model.safetensors": {
                        "size_mb": 1378.2,
                        "ai_class": "RealPoseModel",
                        "verified": True
                    }
                }
            }
        }
        
        # ê¸°ë³¸ ìš”êµ¬ì‚¬í•­
        default_requirements = {
            "step_id": self.step_id,
            "required_models": [],
            "optional_models": [],
            "primary_model": None,
            "model_configs": {},
            "batch_size": 1,
            "precision": "fp16" if self.device == "mps" else "fp32",
            "preprocessing_required": [],
            "postprocessing_required": [],
            "verified_paths": []
        }
        
        # Stepë³„ íŠ¹í™” ìš”êµ¬ì‚¬í•­
        if self.step_name in step_model_mappings:
            mapping = step_model_mappings[self.step_name]
            default_requirements.update({
                "required_models": mapping.get("required_models", []),
                "primary_model": mapping.get("required_models", [None])[0],
                "model_configs": mapping.get("model_configs", {}),
                "verified_paths": mapping.get("verified_paths", [])
            })
        
        return default_requirements

    def _run_ai_inference(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ”¥ ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰ (Central Hub ê¸°ë°˜)"""
        try:
            # ModelLoader ì˜ì¡´ì„± í™•ì¸
            if not hasattr(self, 'model_loader') or not self.model_loader:
                raise ValueError("ModelLoaderê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ - Central Hub ì—°ë™ í•„ìš”")
            
            # ğŸ”¥ Stepë³„ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©
            primary_model = self._load_primary_model()
            if not primary_model:
                raise ValueError(f"{self.step_name} ì£¼ìš” ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            
            # ğŸ”¥ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì‚¬ìš©
            checkpoint_data = None
            if hasattr(primary_model, 'get_checkpoint_data'):
                checkpoint_data = primary_model.get_checkpoint_data()
            
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            if not input_data:
                raise ValueError("ì…ë ¥ ë°ì´í„° ì—†ìŒ")
            
            self.logger.info(f"ğŸ”„ {self.step_name} ì‹¤ì œ AI ì¶”ë¡  ì‹œì‘ (Central Hub ê¸°ë°˜)")
            start_time = time.time()
            
            # GPU/MPS ì²˜ë¦¬
            device = 'mps' if TORCH_AVAILABLE and MPS_AVAILABLE else 'cpu'
            
            # ğŸ”¥ Stepë³„ íŠ¹í™” AI ì¶”ë¡  (ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)
            ai_result = self._run_step_specific_inference(input_data, checkpoint_data, device)
            
            inference_time = time.time() - start_time
            
            return {
                **ai_result,
                'processing_time': inference_time,
                'device_used': device,
                'model_loaded': True,
                'checkpoint_used': checkpoint_data is not None,
                'step_name': self.step_name,
                'central_hub_integrated': True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self._create_error_response(str(e))

    def _load_primary_model(self):
        """ì£¼ìš” ëª¨ë¸ ë¡œë”© (Central Hub ê¸°ë°˜)"""
        try:
            if hasattr(self, 'model_interface') and self.model_interface:
                # Step ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”©
                if hasattr(self.model_interface, 'get_model'):
                    return self.model_interface.get_model()
                    
            elif hasattr(self, 'model_loader') and self.model_loader:
                # ì§ì ‘ ModelLoader ì‚¬ìš©
                requirements = self._get_step_requirements()
                primary_model_name = requirements.get('primary_model')
                if primary_model_name:
                    if hasattr(self.model_loader, 'load_model'):
                        return self.model_loader.load_model(
                            primary_model_name,
                            step_name=self.step_name,
                            validate=True
                        )
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì£¼ìš” ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

    def _run_step_specific_inference(self, input_data: Dict[str, Any], checkpoint_data: Any, device: str) -> Dict[str, Any]:
        """Stepë³„ íŠ¹í™” AI ì¶”ë¡  (ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)"""
        
        # ê¸°ë³¸ êµ¬í˜„ - ê° Stepì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
        return {
            'inference_result': f"{self.step_name} ì‹¤ì œ ì¶”ë¡  ê²°ê³¼ (Central Hub ê¸°ë°˜)",
            'confidence': 0.95,
            'model_info': {
                'checkpoint_loaded': checkpoint_data is not None,
                'device': device,
                'step_type': self.step_name,
                'central_hub_integrated': True
            }
        }

    def validate_dependencies(self, format_type: DependencyValidationFormat = None) -> Union[Dict[str, bool], Dict[str, Any]]:
        """ğŸ”¥ ì˜ì¡´ì„± ê²€ì¦ (Central Hub ê¸°ë°˜)"""
        
        # ê¸°ë³¸ ì˜ì¡´ì„± ê²€ì¦
        validation_result = {
            'model_loader': False,
            'memory_manager': False,
            'data_converter': False,
            'central_hub_container': False,
            'checkpoint_loading': False,
            'model_interface': False
        }
        
        # ModelLoader ê²€ì¦
        if hasattr(self, 'model_loader') and self.model_loader:
            validation_result['model_loader'] = True
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦
            if hasattr(self.model_loader, 'validate_di_container_integration'):
                di_validation = self.model_loader.validate_di_container_integration()
                validation_result['checkpoint_loading'] = di_validation.get('di_container_available', False)
        
        # Model Interface ê²€ì¦
        if hasattr(self, 'model_interface') and self.model_interface:
            validation_result['model_interface'] = True
        
        # ê¸°íƒ€ ì˜ì¡´ì„±ë“¤
        validation_result['memory_manager'] = hasattr(self, 'memory_manager') and self.memory_manager is not None
        validation_result['data_converter'] = hasattr(self, 'data_converter') and self.data_converter is not None
        validation_result['central_hub_container'] = hasattr(self, 'central_hub_container') and self.central_hub_container is not None
        
        self.logger.debug(f"âœ… {self.step_name} ì˜ì¡´ì„± ê²€ì¦ ì™„ë£Œ (Central Hub): {sum(validation_result.values())}/{len(validation_result)}")
        
        # ë°˜í™˜ í˜•ì‹ ê²°ì •
        if format_type == DependencyValidationFormat.BOOLEAN_DICT:
            return validation_result
        else:
            # ìƒì„¸ ì •ë³´ ë°˜í™˜
            return {
                'success': all(validation_result[key] for key in ['model_loader', 'central_hub_container']),
                'dependencies': validation_result,
                'github_compatible': True,
                'central_hub_integrated': True,
                'step_name': self.step_name,
                'checkpoint_loading_ready': validation_result['checkpoint_loading'],
                'model_interface_ready': validation_result['model_interface'],
                'timestamp': time.time()
            }

    def validate_dependencies_boolean(self) -> Dict[str, bool]:
        """GitHub Step í´ë˜ìŠ¤ í˜¸í™˜ (GeometricMatchingStep ë“±)"""
        return self.validate_dependencies(DependencyValidationFormat.BOOLEAN_DICT)
    
    def validate_dependencies_detailed(self) -> Dict[str, Any]:
        """StepFactory í˜¸í™˜ (ìƒì„¸ ì •ë³´)"""
        return self.validate_dependencies(DependencyValidationFormat.DETAILED_DICT)

    def _load_detailed_data_spec_from_kwargs(self, **kwargs) -> DetailedDataSpecConfig:
        """StepFactoryì—ì„œ ì£¼ì…ë°›ì€ DetailedDataSpec ì •ë³´ ë¡œë”©"""
        return DetailedDataSpecConfig(
            # ì…ë ¥ ì‚¬ì–‘
            input_data_types=kwargs.get('input_data_types', []),
            input_shapes=kwargs.get('input_shapes', {}),
            input_value_ranges=kwargs.get('input_value_ranges', {}),
            preprocessing_required=kwargs.get('preprocessing_required', []),
            
            # ì¶œë ¥ ì‚¬ì–‘
            output_data_types=kwargs.get('output_data_types', []),
            output_shapes=kwargs.get('output_shapes', {}),
            output_value_ranges=kwargs.get('output_value_ranges', {}),
            postprocessing_required=kwargs.get('postprocessing_required', []),
            
            # API í˜¸í™˜ì„±
            api_input_mapping=kwargs.get('api_input_mapping', {}),
            api_output_mapping=kwargs.get('api_output_mapping', {}),
            
            # Step ê°„ ì—°ë™
            step_input_schema=kwargs.get('step_input_schema', {}),
            step_output_schema=kwargs.get('step_output_schema', {}),
            
            # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­
            normalization_mean=kwargs.get('normalization_mean', (0.485, 0.456, 0.406)),
            normalization_std=kwargs.get('normalization_std', (0.229, 0.224, 0.225)),
            preprocessing_steps=kwargs.get('preprocessing_steps', []),
            postprocessing_steps=kwargs.get('postprocessing_steps', []),
            
            # Step ê°„ ë°ì´í„° ì „ë‹¬ ìŠ¤í‚¤ë§ˆ
            accepts_from_previous_step=kwargs.get('accepts_from_previous_step', {}),
            provides_to_next_step=kwargs.get('provides_to_next_step', {})
        )

    def _validate_data_conversion_readiness(self) -> bool:
        """ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ìƒíƒœ ê²€ì¦ (ì›Œë‹ ë°©ì§€)"""
        # DetailedDataSpec ì¡´ì¬ í™•ì¸ ë° ìë™ ìƒì„±
        if not hasattr(self, 'detailed_data_spec') or not self.detailed_data_spec:
            self._create_emergency_detailed_data_spec()
            self.logger.debug(f"âœ… {self.step_name} DetailedDataSpec ê¸°ë³¸ê°’ ìë™ ìƒì„±")
        
        # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ í™•ì¸ ë° ìë™ ë³´ì™„
        missing_fields = []
        required_fields = ['input_data_types', 'output_data_types', 'api_input_mapping', 'api_output_mapping']
        
        for field in required_fields:
            if not hasattr(self.detailed_data_spec, field):
                missing_fields.append(field)
            else:
                value = getattr(self.detailed_data_spec, field)
                if not value:
                    missing_fields.append(field)
        
        # ëˆ„ë½ëœ í•„ë“œ ìë™ ë³´ì™„
        if missing_fields:
            self._fill_missing_fields(missing_fields)
            self.logger.debug(f"{self.step_name} DetailedDataSpec í•„ë“œ ë³´ì™„: {missing_fields}")
        
        # dependency_manager ìƒíƒœ ì—…ë°ì´íŠ¸
        if hasattr(self, 'dependency_manager') and self.dependency_manager:
            self.dependency_manager.dependency_status.detailed_data_spec_loaded = True
            self.dependency_manager.dependency_status.data_conversion_ready = True
        
        self.logger.debug(f"âœ… {self.step_name} DetailedDataSpec ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ì™„ë£Œ")
        return True

    def _initialize_performance_stats(self):
        """ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™”"""
        self.performance_stats = {
            'total_processed': 0,
            'avg_processing_time': 0.0,
            'error_count': 0,
            'success_rate': 1.0,
            'memory_usage_mb': 0.0,
            'models_loaded': 0,
            'cache_hits': 0,
            'ai_inference_count': 0,
            'torch_errors': 0,
            'central_hub_requests': 0
        }
        
        self.total_processing_count = 0
        self.error_count = 0
        self.last_processing_time = 0.0
        
        self.logger.debug(f"âœ… {self.step_name} ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™” ì™„ë£Œ")

    def _create_emergency_detailed_data_spec(self):
        """ì‘ê¸‰ DetailedDataSpec ìƒì„±"""
        if not hasattr(self, 'detailed_data_spec') or not self.detailed_data_spec:
            class EmergencyDataSpec:
                def __init__(self):
                    self.input_data_types = {
                        'person_image': 'PIL.Image.Image',
                        'clothing_image': 'PIL.Image.Image',
                        'data': 'Any'
                    }
                    self.output_data_types = {
                        'result': 'numpy.ndarray',
                        'success': 'bool',
                        'processing_time': 'float'
                    }
                    self.api_input_mapping = {
                        'person_image': 'fastapi.UploadFile -> PIL.Image.Image',
                        'clothing_image': 'fastapi.UploadFile -> PIL.Image.Image'
                    }
                    self.api_output_mapping = {
                        'result': 'numpy.ndarray -> base64_string',
                        'success': 'bool -> bool'
                    }
                    self.preprocessing_steps = ['validate_input', 'resize_image']
                    self.postprocessing_steps = ['format_output']
                    self.accepts_from_previous_step = {}
                    self.provides_to_next_step = {}
                    self.segmentation_models = {}  # ClothSegmentationStepìš©
                    self.logger = self._setup_logger()  # ëª¨ë“  Stepìš©
                    self._load_single_model = self._default_load_single_model  # PostProcessingStepìš©

            self.detailed_data_spec = EmergencyDataSpec()

    def _fill_missing_fields(self, missing_fields):
        """ëˆ„ë½ëœ DetailedDataSpec í•„ë“œ ì±„ìš°ê¸°"""
        default_values = {
            'input_data_types': {
                'person_image': 'PIL.Image.Image',
                'clothing_image': 'PIL.Image.Image',
                'data': 'Any'
            },
            'output_data_types': {
                'result': 'numpy.ndarray',
                'success': 'bool',
                'processing_time': 'float'
            },
            'api_input_mapping': {
                'person_image': 'fastapi.UploadFile -> PIL.Image.Image',
                'clothing_image': 'fastapi.UploadFile -> PIL.Image.Image'
            },
            'api_output_mapping': {
                'result': 'numpy.ndarray -> base64_string',
                'success': 'bool -> bool'
            },
            'preprocessing_steps': ['validate_input', 'resize_image'],
            'postprocessing_steps': ['format_output'],
            'accepts_from_previous_step': {},
            'provides_to_next_step': {}
        }
        
        for field in missing_fields:
            if field in default_values:
                if not hasattr(self.detailed_data_spec, field):
                    setattr(self.detailed_data_spec, field, default_values[field])
                elif not getattr(self.detailed_data_spec, field):
                    setattr(self.detailed_data_spec, field, default_values[field])

    # ==============================================
    # ğŸ”¥ í‘œì¤€í™”ëœ process ë©”ì„œë“œ (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    def process(self, **kwargs) -> Dict[str, Any]:
        """ì™„ì „íˆ ì¬ì„¤ê³„ëœ í‘œì¤€í™” process ë©”ì„œë“œ (Central Hub ê¸°ë°˜) - ë™ê¸° ë²„ì „"""
        try:
            start_time = time.time()
            self.performance_metrics.github_process_calls += 1
            
            self.logger.debug(f"ğŸ”„ {self.step_name} process ì‹œì‘ (Central Hub, ì…ë ¥: {list(kwargs.keys())})")
            
            # 1. API ì…ë ¥ì„ Step ì…ë ¥ìœ¼ë¡œ ë³€í™˜ (convert_api_input_to_step_input í˜¸ì¶œ)
            if hasattr(self, 'convert_api_input_to_step_input'):
                try:
                    converted_input = self.convert_api_input_to_step_input(kwargs)
                    self.logger.debug(f"âœ… {self.step_name} API ì…ë ¥ ë³€í™˜ ì™„ë£Œ (convert_api_input_to_step_input)")
                except Exception as convert_error:
                    self.logger.error(f"âŒ {self.step_name} API ì…ë ¥ ë³€í™˜ ì‹¤íŒ¨: {convert_error}")
                    # í´ë°±: DetailedDataSpec ê¸°ë°˜ ë³€í™˜ ì‚¬ìš©
                    converted_input = self._convert_input_to_model_format_sync(kwargs)
            else:
                # convert_api_input_to_step_inputì´ ì—†ëŠ” ê²½ìš° DetailedDataSpec ê¸°ë°˜ ë³€í™˜ ì‚¬ìš©
                converted_input = self._convert_input_to_model_format_sync(kwargs)
            
            # 2. í•˜ìœ„ í´ë˜ìŠ¤ì˜ ìˆœìˆ˜ AI ë¡œì§ ì‹¤í–‰
            ai_result = self._run_ai_inference(converted_input)
            
            # 3. ì¶œë ¥ ë°ì´í„° ë³€í™˜ (AI ëª¨ë¸ â†’ API + Step ê°„) - ë™ê¸°ì ìœ¼ë¡œ í˜¸ì¶œ
            standardized_output = self._convert_output_to_standard_format(ai_result)
            
            # 4. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self._update_performance_metrics(processing_time, True)
            
            self.logger.debug(f"âœ… {self.step_name} process ì™„ë£Œ (Central Hub, {processing_time:.3f}ì´ˆ)")
            
            return standardized_output
            
        except Exception as e:
            processing_time = time.time() - start_time
            self._update_performance_metrics(processing_time, False)
            self.logger.error(f"âŒ {self.step_name} process ì‹¤íŒ¨ (Central Hub, {processing_time:.3f}ì´ˆ): {e}")
            return self._create_error_response(str(e))

    @abstractmethod
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„í•  ìˆœìˆ˜ AI ë¡œì§ (ë™ê¸° ë©”ì„œë“œ)"""
        pass

    # ==============================================
    # ğŸ”¥ ì…ë ¥ ë°ì´í„° ë³€í™˜ ì‹œìŠ¤í…œ (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    def _convert_input_to_model_format_sync(self, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """API/Step ê°„ ë°ì´í„° â†’ AI ëª¨ë¸ ì…ë ¥ í˜•ì‹ ë³€í™˜ (ë™ê¸° ë²„ì „)"""
        try:
            converted = {}
            self.performance_metrics.data_conversions += 1
            
            self.logger.debug(f"ğŸ”„ {self.step_name} ì…ë ¥ ë°ì´í„° ë³€í™˜ ì‹œì‘...")
            
            # 1. API ì…ë ¥ ë§¤í•‘ ì²˜ë¦¬
            for model_param, api_type in self.detailed_data_spec.api_input_mapping.items():
                if model_param in kwargs:
                    converted[model_param] = self._convert_api_input_type_sync(
                        kwargs[model_param], api_type, model_param
                    )
                    self.performance_metrics.api_conversions += 1
            
            # 2. Step ê°„ ë°ì´í„° ì²˜ë¦¬
            for step_name, step_data in kwargs.items():
                if step_name.startswith('from_step_'):
                    step_id = step_name.replace('from_step_', '')
                    if step_id in self.detailed_data_spec.accepts_from_previous_step:
                        step_schema = self.detailed_data_spec.accepts_from_previous_step[step_id]
                        converted.update(self._map_step_input_data(step_data, step_schema))
                        self.performance_metrics.step_data_transfers += 1
            
            # 3. ëˆ„ë½ëœ í•„ìˆ˜ ì…ë ¥ ë°ì´í„° í™•ì¸
            for param_name in self.detailed_data_spec.api_input_mapping.keys():
                if param_name not in converted and param_name in kwargs:
                    converted[param_name] = kwargs[param_name]
            
            # 4. ì „ì²˜ë¦¬ ì ìš© (ë™ê¸°ì ìœ¼ë¡œ)
            if self.config.auto_preprocessing and self.detailed_data_spec.preprocessing_steps:
                converted = self._apply_preprocessing_sync(converted)
                self.performance_metrics.preprocessing_operations += 1
            
            # 5. ë°ì´í„° ê²€ì¦
            if self.config.strict_data_validation:
                validated_input = self._validate_input_data(converted)
            else:
                validated_input = converted
            
            self.logger.debug(f"âœ… {self.step_name} ì…ë ¥ ë°ì´í„° ë³€í™˜ ì™„ë£Œ")
            return validated_input
            
        except Exception as e:
            self.performance_metrics.validation_failures += 1
            self.logger.error(f"âŒ {self.step_name} ì…ë ¥ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise

    async def _convert_input_to_model_format(self, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """API/Step ê°„ ë°ì´í„° â†’ AI ëª¨ë¸ ì…ë ¥ í˜•ì‹ ë³€í™˜ (ë¹„ë™ê¸° ë²„ì „ - í˜¸í™˜ì„±ìš©)"""
        return self._convert_input_to_model_format_sync(kwargs)
    
    def _convert_api_input_type_sync(self, value: Any, api_type: str, param_name: str) -> Any:
        """API íƒ€ì…ë³„ ë³€í™˜ ì²˜ë¦¬ (ë™ê¸° ë²„ì „)"""
        try:
            if api_type == "UploadFile":
                if hasattr(value, 'file'):
                    content = value.file.read() if hasattr(value.file, 'read') else value.file.read()
                    return Image.open(BytesIO(content)) if PIL_AVAILABLE else content
                elif hasattr(value, 'read'):
                    content = value.read()
                    return Image.open(BytesIO(content)) if PIL_AVAILABLE else content
                
            elif api_type == "base64_string":
                if isinstance(value, str):
                    try:
                        image_data = base64.b64decode(value)
                        return Image.open(BytesIO(image_data)) if PIL_AVAILABLE else image_data
                    except Exception:
                        return value
                        
            elif api_type in ["str", "Optional[str]"]:
                return str(value) if value is not None else None
                
            elif api_type in ["int", "Optional[int]"]:
                return int(value) if value is not None else None
                
            elif api_type in ["float", "Optional[float]"]:
                return float(value) if value is not None else None
                
            return value
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ {self.step_name} API íƒ€ì… ë³€í™˜ ì‹¤íŒ¨ ({param_name}: {api_type}): {e}")
            return value
    
    def _map_step_input_data(self, step_data: Dict[str, Any], step_schema: Dict[str, str]) -> Dict[str, Any]:
        """Step ê°„ ë°ì´í„° ë§¤í•‘"""
        mapped_data = {}
        
        for data_key, data_type in step_schema.items():
            if data_key in step_data:
                value = step_data[data_key]
                
                if data_type == "np.ndarray" and NUMPY_AVAILABLE:
                    if TORCH_AVAILABLE and torch.is_tensor(value):
                        mapped_data[data_key] = value.cpu().numpy()
                    else:
                        mapped_data[data_key] = np.array(value) if not isinstance(value, np.ndarray) else value
                        
                elif data_type == "torch.Tensor" and TORCH_AVAILABLE:
                    if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                        mapped_data[data_key] = torch.from_numpy(value)
                    else:
                        mapped_data[data_key] = value
                        
                elif data_type == "PIL.Image" and PIL_AVAILABLE:
                    if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                        mapped_data[data_key] = Image.fromarray(value.astype(np.uint8))
                    else:
                        mapped_data[data_key] = value
                        
                else:
                    mapped_data[data_key] = value
        
        return mapped_data
    
    # ==============================================
    # ğŸ”¥ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    def _apply_preprocessing_sync(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """DetailedDataSpec ê¸°ë°˜ ì „ì²˜ë¦¬ ìë™ ì ìš© (ë™ê¸° ë²„ì „)"""
        try:
            processed = input_data.copy()
            
            self.logger.debug(f"ğŸ”„ {self.step_name} ì „ì²˜ë¦¬ ì ìš©: {self.detailed_data_spec.preprocessing_steps}")
            
            for step_name in self.detailed_data_spec.preprocessing_steps:
                if step_name == "resize_512x512":
                    processed = self._resize_images(processed, (512, 512))
                elif step_name == "resize_768x1024":
                    processed = self._resize_images(processed, (768, 1024))
                elif step_name == "resize_256x192":
                    processed = self._resize_images(processed, (256, 192))
                elif step_name == "resize_224x224":
                    processed = self._resize_images(processed, (224, 224))
                elif step_name == "resize_368x368":
                    processed = self._resize_images(processed, (368, 368))
                elif step_name == "resize_1024x1024":
                    processed = self._resize_images(processed, (1024, 1024))
                elif step_name == "normalize_imagenet":
                    processed = self._normalize_imagenet(processed)
                elif step_name == "normalize_clip":
                    processed = self._normalize_clip(processed)
                elif step_name == "normalize_diffusion":
                    processed = self._normalize_diffusion(processed)
                elif step_name == "to_tensor":
                    processed = self._convert_to_tensor(processed)
                elif step_name == "prepare_sam_prompts":
                    processed = self._prepare_sam_prompts(processed)
                elif step_name == "prepare_diffusion_input":
                    processed = self._prepare_diffusion_input(processed)
                elif step_name == "prepare_ootd_inputs":
                    processed = self._prepare_ootd_inputs(processed)
                elif step_name == "extract_pose_features":
                    processed = self._extract_pose_features(processed)
                elif step_name == "prepare_sr_input":
                    processed = self._prepare_sr_input(processed)
                else:
                    self.logger.debug(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì „ì²˜ë¦¬ ë‹¨ê³„: {step_name}")
            
            self.logger.debug(f"âœ… {self.step_name} ì „ì²˜ë¦¬ ì™„ë£Œ")
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return input_data

    async def _apply_preprocessing(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """DetailedDataSpec ê¸°ë°˜ ì „ì²˜ë¦¬ ìë™ ì ìš© (ë¹„ë™ê¸° ë²„ì „ - í˜¸í™˜ì„±ìš©)"""
        return self._apply_preprocessing_sync(input_data)
    
    def _resize_images(self, data: Dict[str, Any], target_size: Tuple[int, int]) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ì²˜ë¦¬"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if PIL_AVAILABLE and isinstance(value, Image.Image):
                    result[key] = value.resize(target_size, Image.LANCZOS)
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray) and len(value.shape) >= 2:
                    if CV2_AVAILABLE:
                        if len(value.shape) == 3:
                            result[key] = cv2.resize(value, target_size)
                        elif len(value.shape) == 2:
                            result[key] = cv2.resize(value, target_size)
                    else:
                        # PIL í´ë°±
                        if PIL_AVAILABLE:
                            if len(value.shape) == 3:
                                img = Image.fromarray(value.astype(np.uint8))
                                result[key] = np.array(img.resize(target_size, Image.LANCZOS))
                            elif len(value.shape) == 2:
                                img = Image.fromarray(value.astype(np.uint8), mode='L')
                                result[key] = np.array(img.resize(target_size, Image.LANCZOS))
            except Exception as e:
                self.logger.debug(f"ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _normalize_imagenet(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ImageNet ì •ê·œí™”"""
        result = data.copy()
        mean = np.array(self.detailed_data_spec.normalization_mean)
        std = np.array(self.detailed_data_spec.normalization_std)
        
        for key, value in data.items():
            try:
                if PIL_AVAILABLE and isinstance(value, Image.Image):
                    array = np.array(value).astype(np.float32) / 255.0
                    if len(array.shape) == 3 and array.shape[2] == 3:
                        normalized = (array - mean) / std
                        result[key] = normalized
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if value.dtype != np.float32:
                        value = value.astype(np.float32)
                    if value.max() > 1.0:
                        value = value / 255.0
                    if len(value.shape) == 3 and value.shape[2] == 3:
                        normalized = (value - mean) / std
                        result[key] = normalized
            except Exception as e:
                self.logger.debug(f"ImageNet ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _normalize_clip(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """CLIP ì •ê·œí™”"""
        result = data.copy()
        clip_mean = np.array([0.48145466, 0.4578275, 0.40821073])
        clip_std = np.array([0.26862954, 0.26130258, 0.27577711])
        
        for key, value in data.items():
            try:
                if PIL_AVAILABLE and isinstance(value, Image.Image):
                    array = np.array(value).astype(np.float32) / 255.0
                    if len(array.shape) == 3 and array.shape[2] == 3:
                        normalized = (array - clip_mean) / clip_std
                        result[key] = normalized
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if value.dtype != np.float32:
                        value = value.astype(np.float32)
                    if value.max() > 1.0:
                        value = value / 255.0
                    if len(value.shape) == 3 and value.shape[2] == 3:
                        normalized = (value - clip_mean) / clip_std
                        result[key] = normalized
            except Exception as e:
                self.logger.debug(f"CLIP ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _normalize_diffusion(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Diffusion ì •ê·œí™”"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if PIL_AVAILABLE and isinstance(value, Image.Image):
                    array = np.array(value).astype(np.float32) / 255.0
                    normalized = 2.0 * array - 1.0
                    result[key] = normalized
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if value.dtype != np.float32:
                        value = value.astype(np.float32)
                    if value.max() > 1.0:
                        value = value / 255.0
                    normalized = 2.0 * value - 1.0
                    result[key] = normalized
            except Exception as e:
                self.logger.debug(f"Diffusion ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _convert_to_tensor(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """PyTorch í…ì„œ ë³€í™˜ + MPS float64 ë¬¸ì œ í•´ê²°"""
        if not TORCH_AVAILABLE:
            return data
        
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if len(value.shape) == 3 and value.shape[2] in [1, 3, 4]:
                        value = np.transpose(value, (2, 0, 1))
                    tensor = torch.from_numpy(value).float()
                    
                    # ğŸ”¥ MPS ë””ë°”ì´ìŠ¤ì—ì„œ float64 â†’ float32 ë³€í™˜
                    if self.device == 'mps' and tensor.dtype == torch.float64:
                        tensor = tensor.to(torch.float32)
                    
                    result[key] = tensor
                    
                elif PIL_AVAILABLE and isinstance(value, Image.Image):
                    array = np.array(value).astype(np.float32)
                    if len(array.shape) == 3 and array.shape[2] in [1, 3, 4]:
                        array = np.transpose(array, (2, 0, 1))
                    tensor = torch.from_numpy(array)
                    
                    # ğŸ”¥ MPS ë””ë°”ì´ìŠ¤ì—ì„œ float64 â†’ float32 ë³€í™˜
                    if self.device == 'mps' and tensor.dtype == torch.float64:
                        tensor = tensor.to(torch.float32)
                    
                    result[key] = tensor
                    
            except Exception as e:
                self.logger.debug(f"í…ì„œ ë³€í™˜ ì‹¤íŒ¨ ({key}): {e}")
        
        return result



    def _prepare_sam_prompts(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """SAM í”„ë¡¬í”„íŠ¸ ì¤€ë¹„"""
        result = data.copy()
        
        if 'prompt_points' not in result and 'image' in result:
            if PIL_AVAILABLE and isinstance(result['image'], Image.Image):
                w, h = result['image'].size
                result['prompt_points'] = np.array([[w//2, h//2]])
                result['prompt_labels'] = np.array([1])
        
        return result
    
    def _prepare_diffusion_input(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Diffusion ëª¨ë¸ ì…ë ¥ ì¤€ë¹„"""
        result = data.copy()
        
        if 'guidance_scale' not in result:
            result['guidance_scale'] = 7.5
        if 'num_inference_steps' not in result:
            result['num_inference_steps'] = 20
        if 'strength' not in result:
            result['strength'] = 0.8
        
        return result
    
    def _prepare_ootd_inputs(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """OOTD Diffusion ì…ë ¥ ì¤€ë¹„"""
        result = data.copy()
        
        if 'fitting_mode' not in result:
            result['fitting_mode'] = 'hd'
        
        return result
    
    def _extract_pose_features(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í¬ì¦ˆ íŠ¹ì§• ì¶”ì¶œ"""
        return data
    
    def _prepare_sr_input(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Super Resolution ì…ë ¥ ì¤€ë¹„"""
        result = data.copy()
        
        if 'tile_size' not in result:
            result['tile_size'] = 512
        if 'overlap' not in result:
            result['overlap'] = 64
        
        return result
    
# ==============================================
    # ğŸ”¥ ì¶œë ¥ ë°ì´í„° ë³€í™˜ ì‹œìŠ¤í…œ (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    def _convert_output_to_standard_format(self, ai_result: Dict[str, Any]) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì¶œë ¥ â†’ í‘œì¤€ í˜•ì‹ ë³€í™˜"""
        try:
            self.logger.debug(f"ğŸ”„ {self.step_name} ì¶œë ¥ ë°ì´í„° ë³€í™˜ ì‹œì‘...")
            
            # 1. í›„ì²˜ë¦¬ ì ìš© (ë™ê¸°ì ìœ¼ë¡œ í˜¸ì¶œ)
            processed_result = self._apply_postprocessing_sync(ai_result)
            
            # 2. API ì‘ë‹µ í˜•ì‹ ë³€í™˜
            api_response = self._convert_to_api_format(processed_result)
            
            # 3. ë‹¤ìŒ Stepë“¤ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
            next_step_data = self._prepare_next_step_data(processed_result)
            
            # 4. í‘œì¤€ ì‘ë‹µ êµ¬ì¡° ìƒì„±
            standard_response = {
                'success': True,
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': getattr(self, '_last_processing_time', 0.0),
                
                # API ì‘ë‹µ ë°ì´í„°
                **api_response,
                
                # ë‹¤ìŒ Stepë“¤ì„ ìœ„í•œ ë°ì´í„°
                'next_step_data': next_step_data,
                
                # ë©”íƒ€ë°ì´í„°
                'metadata': {
                    'input_shapes': {k: self._get_shape_info(v) for k, v in ai_result.items()},
                    'output_shapes': self.detailed_data_spec.output_shapes,
                    'device': self.device,
                    'github_compatible': True,
                    'detailed_data_spec_applied': True,
                    'central_hub_integrated': True,
                    'data_conversion_version': 'v20.0'
                }
            }
            
            self.logger.debug(f"âœ… {self.step_name} ì¶œë ¥ ë°ì´í„° ë³€í™˜ ì™„ë£Œ")
            return standard_response
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì¶œë ¥ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
            return self._create_error_response(str(e))
    
    def _get_shape_info(self, value: Any) -> Optional[Tuple]:
        """ê°’ì˜ í˜•íƒœ ì •ë³´ ì¶”ì¶œ"""
        try:
            if hasattr(value, 'shape'):
                return tuple(value.shape)
            elif isinstance(value, (list, tuple)):
                return (len(value),)
            else:
                return None
        except:
            return None
    
    def _apply_postprocessing_sync(self, ai_result: Dict[str, Any]) -> Dict[str, Any]:
        """DetailedDataSpec ê¸°ë°˜ í›„ì²˜ë¦¬ ìë™ ì ìš© (ë™ê¸° ë²„ì „)"""
        try:
            if not self.config.auto_postprocessing:
                return ai_result
            
            processed = ai_result.copy()
            
            self.logger.debug(f"ğŸ”„ {self.step_name} í›„ì²˜ë¦¬ ì ìš©: {self.detailed_data_spec.postprocessing_steps}")
            
            for step_name in self.detailed_data_spec.postprocessing_steps:
                if step_name == "softmax":
                    processed = self._apply_softmax(processed)
                elif step_name == "argmax":
                    processed = self._apply_argmax(processed)
                elif step_name == "resize_original":
                    processed = self._resize_to_original(processed)
                elif step_name == "to_numpy":
                    processed = self._convert_to_numpy(processed)
                elif step_name == "threshold_0.5":
                    processed = self._apply_threshold(processed, 0.5)
                elif step_name == "nms":
                    processed = self._apply_nms(processed)
                elif step_name == "denormalize_diffusion" or step_name == "denormalize_centered":
                    processed = self._denormalize_diffusion(processed)
                elif step_name == "denormalize":
                    processed = self._denormalize_imagenet(processed)
                elif step_name == "clip_values" or step_name == "clip_0_1":
                    processed = self._clip_values(processed, 0.0, 1.0)
                elif step_name == "apply_mask" or step_name == "apply_warping_mask":
                    processed = self._apply_mask(processed)
                elif step_name == "morphology_clean":
                    processed = self._morphology_operations(processed)
                elif step_name == "extract_keypoints":
                    processed = self._extract_keypoints(processed)
                elif step_name == "scale_coords":
                    processed = self._scale_coordinates(processed)
                elif step_name == "filter_confidence":
                    processed = self._filter_by_confidence(processed)
                elif step_name == "enhance_details":
                    processed = self._enhance_details(processed)
                elif step_name == "final_compositing":
                    processed = self._final_compositing(processed)
                elif step_name == "generate_quality_report":
                    processed = self._generate_quality_report(processed)
                else:
                    self.logger.debug(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” í›„ì²˜ë¦¬ ë‹¨ê³„: {step_name}")
            
            self.performance_metrics.postprocessing_operations += 1
            self.logger.debug(f"âœ… {self.step_name} í›„ì²˜ë¦¬ ì™„ë£Œ")
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ai_result

    async def _apply_postprocessing(self, ai_result: Dict[str, Any]) -> Dict[str, Any]:
        """DetailedDataSpec ê¸°ë°˜ í›„ì²˜ë¦¬ ìë™ ì ìš© (ë¹„ë™ê¸° ë²„ì „)"""
        return self._apply_postprocessing_sync(ai_result)
    
    def _apply_softmax(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Softmax ì ìš©"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = torch.softmax(value, dim=-1)
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    exp_vals = np.exp(value - np.max(value, axis=-1, keepdims=True))
                    result[key] = exp_vals / np.sum(exp_vals, axis=-1, keepdims=True)
            except Exception as e:
                self.logger.debug(f"Softmax ì ìš© ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _apply_argmax(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Argmax ì ìš©"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = torch.argmax(value, dim=-1)
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = np.argmax(value, axis=-1)
            except Exception as e:
                self.logger.debug(f"Argmax ì ìš© ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _convert_to_numpy(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """NumPy ë³€í™˜"""
        if not NUMPY_AVAILABLE:
            return data
        
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = value.detach().cpu().numpy()
                elif not isinstance(value, np.ndarray):
                    if isinstance(value, (list, tuple)):
                        result[key] = np.array(value)
            except Exception as e:
                self.logger.debug(f"NumPy ë³€í™˜ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _apply_threshold(self, data: Dict[str, Any], threshold: float) -> Dict[str, Any]:
        """ì„ê³„ê°’ ì ìš©"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = (value > threshold).astype(np.float32)
                elif TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = (value > threshold).float()
            except Exception as e:
                self.logger.debug(f"ì„ê³„ê°’ ì ìš© ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _denormalize_diffusion(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Diffusion ì—­ì •ê·œí™” ([-1, 1] â†’ [0, 1])"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = (value + 1.0) / 2.0
                elif TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = (value + 1.0) / 2.0
            except Exception as e:
                self.logger.debug(f"Diffusion ì—­ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _denormalize_imagenet(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ImageNet ì—­ì •ê·œí™”"""
        result = data.copy()
        mean = np.array(self.detailed_data_spec.normalization_mean)
        std = np.array(self.detailed_data_spec.normalization_std)
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if len(value.shape) == 3 and value.shape[2] == 3:
                        denormalized = value * std + mean
                        result[key] = np.clip(denormalized, 0, 1)
            except Exception as e:
                self.logger.debug(f"ImageNet ì—­ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _clip_values(self, data: Dict[str, Any], min_val: float, max_val: float) -> Dict[str, Any]:
        """ê°’ ë²”ìœ„ í´ë¦¬í•‘"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = np.clip(value, min_val, max_val)
                elif TORCH_AVAILABLE and torch.is_tensor(value):
                    result[key] = torch.clamp(value, min_val, max_val)
            except Exception as e:
                self.logger.debug(f"ê°’ í´ë¦¬í•‘ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _apply_mask(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë§ˆìŠ¤í¬ ì ìš©"""
        result = data.copy()
        
        if 'mask' in data and 'image' in data:
            try:
                mask = data['mask']
                image = data['image']
                
                if NUMPY_AVAILABLE:
                    if isinstance(mask, np.ndarray) and isinstance(image, np.ndarray):
                        result['masked_image'] = image * mask
                        
            except Exception as e:
                self.logger.debug(f"ë§ˆìŠ¤í¬ ì ìš© ì‹¤íŒ¨: {e}")
        
        return result
    
    def _morphology_operations(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í˜•íƒœí•™ì  ì—°ì‚° (ë…¸ì´ì¦ˆ ì œê±°)"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if CV2_AVAILABLE and NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if len(value.shape) == 2:
                        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                        opened = cv2.morphologyEx(value.astype(np.uint8), cv2.MORPH_OPEN, kernel)
                        closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)
                        result[key] = closed.astype(np.float32)
                        
            except Exception as e:
                self.logger.debug(f"í˜•íƒœí•™ì  ì—°ì‚° ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _extract_keypoints(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        result = data.copy()
        
        if 'heatmaps' in data:
            try:
                heatmaps = data['heatmaps']
                if NUMPY_AVAILABLE and isinstance(heatmaps, np.ndarray):
                    keypoints = []
                    for i in range(heatmaps.shape[0]):
                        heatmap = heatmaps[i]
                        y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
                        confidence = heatmap[y, x]
                        keypoints.append([x, y, confidence])
                    result['keypoints'] = np.array(keypoints)
                    
            except Exception as e:
                self.logger.debug(f"í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return result
    
    def _scale_coordinates(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§"""
        result = data.copy()
        
        if 'keypoints' in data and 'original_size' in data:
            try:
                keypoints = data['keypoints']
                original_size = data['original_size']
                
                if isinstance(keypoints, np.ndarray) and len(keypoints.shape) == 2:
                    scale_x = original_size[0] / self.detailed_data_spec.input_shapes.get('image', (512, 512))[1]
                    scale_y = original_size[1] / self.detailed_data_spec.input_shapes.get('image', (512, 512))[0]
                    
                    scaled_keypoints = keypoints.copy()
                    scaled_keypoints[:, 0] *= scale_x
                    scaled_keypoints[:, 1] *= scale_y
                    result['scaled_keypoints'] = scaled_keypoints
                    
            except Exception as e:
                self.logger.debug(f"ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨: {e}")
        
        return result
    
    def _filter_by_confidence(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§"""
        result = data.copy()
        
        confidence_threshold = self.config.confidence_threshold
        
        for key, value in data.items():
            try:
                if key.endswith('_confidence') or key.endswith('_scores'):
                    if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                        valid_mask = value > confidence_threshold
                        result[f'{key}_filtered'] = value[valid_mask]
                        
                        base_key = key.replace('_confidence', '').replace('_scores', '')
                        if base_key in data:
                            base_data = data[base_key]
                            if isinstance(base_data, np.ndarray) and len(base_data) == len(value):
                                result[f'{base_key}_filtered'] = base_data[valid_mask]
                                
            except Exception as e:
                self.logger.debug(f"ì‹ ë¢°ë„ í•„í„°ë§ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _enhance_details(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì„¸ë¶€ì‚¬í•­ í–¥ìƒ"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray) and len(value.shape) >= 2:
                    if CV2_AVAILABLE and len(value.shape) == 3:
                        blurred = cv2.GaussianBlur(value, (3, 3), 1.0)
                        sharpened = cv2.addWeighted(value, 1.5, blurred, -0.5, 0)
                        result[f'{key}_enhanced'] = np.clip(sharpened, 0, 1)
                        
            except Exception as e:
                self.logger.debug(f"ì„¸ë¶€ì‚¬í•­ í–¥ìƒ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _final_compositing(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ìµœì¢… í•©ì„±"""
        result = data.copy()
        
        if 'person_image' in data and 'clothing_image' in data and 'mask' in data:
            try:
                person = data['person_image']
                clothing = data['clothing_image']
                mask = data['mask']
                
                if all(isinstance(x, np.ndarray) for x in [person, clothing, mask]):
                    composited = person * (1 - mask) + clothing * mask
                    result['final_composited'] = composited
                    
            except Exception as e:
                self.logger.debug(f"ìµœì¢… í•©ì„± ì‹¤íŒ¨: {e}")
        
        return result
    
    def _generate_quality_report(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        result = data.copy()
        
        quality_metrics = {
            'overall_quality': 0.0,
            'detail_preservation': 0.0,
            'color_consistency': 0.0,
            'artifact_level': 0.0,
            'recommendations': []
        }
        
        try:
            if 'final_result' in data:
                final_result = data['final_result']
                if NUMPY_AVAILABLE and isinstance(final_result, np.ndarray):
                    mean_intensity = np.mean(final_result)
                    std_intensity = np.std(final_result)
                    
                    quality_metrics['overall_quality'] = min(1.0, (mean_intensity + std_intensity) / 2.0)
                    quality_metrics['detail_preservation'] = min(1.0, std_intensity * 2.0)
                    quality_metrics['color_consistency'] = 1.0 - abs(0.5 - mean_intensity)
                    
                    if quality_metrics['overall_quality'] < 0.7:
                        quality_metrics['recommendations'].append('ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„  í•„ìš”')
                    if quality_metrics['detail_preservation'] < 0.5:
                        quality_metrics['recommendations'].append('ì„¸ë¶€ì‚¬í•­ ë³´ì¡´ ê°œì„  í•„ìš”')
            
            result['quality_assessment'] = quality_metrics
            
        except Exception as e:
            self.logger.debug(f"í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            result['quality_assessment'] = quality_metrics
        
        return result
    
    def _apply_nms(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Non-Maximum Suppression ì ìš©"""
        result = data.copy()
        
        if 'detections' in data and 'scores' in data:
            try:
                detections = data['detections']
                scores = data['scores']
                
                if NUMPY_AVAILABLE and isinstance(detections, np.ndarray) and isinstance(scores, np.ndarray):
                    sorted_indices = np.argsort(scores)[::-1]
                    
                    top_k = min(10, len(sorted_indices))
                    result['detections_nms'] = detections[sorted_indices[:top_k]]
                    result['scores_nms'] = scores[sorted_indices[:top_k]]
                    
            except Exception as e:
                self.logger.debug(f"NMS ì ìš© ì‹¤íŒ¨: {e}")
        
        return result
    
    def _resize_to_original(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ"""
        result = data.copy()
        
        if 'original_size' in data:
            original_size = data['original_size']
            
            for key, value in data.items():
                try:
                    if key != 'original_size' and isinstance(value, np.ndarray) and len(value.shape) >= 2:
                        if CV2_AVAILABLE:
                            if len(value.shape) == 3:
                                resized = cv2.resize(value, tuple(original_size))
                            elif len(value.shape) == 2:
                                resized = cv2.resize(value, tuple(original_size))
                            else:
                                continue
                            result[f'{key}_original_size'] = resized
                            
                except Exception as e:
                    self.logger.debug(f"ì›ë³¸ í¬ê¸° ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨ ({key}): {e}")
        
        return result
    
    def _convert_to_api_format(self, processed_result: Dict[str, Any]) -> Dict[str, Any]:
        """AI ê²°ê³¼ â†’ API ì‘ë‹µ í˜•ì‹ ë³€í™˜"""
        api_response = {}
        
        try:
            for api_field, api_type in self.detailed_data_spec.api_output_mapping.items():
                if api_field in processed_result:
                    value = processed_result[api_field]
                    
                    if api_type == "base64_string":
                        api_response[api_field] = self._array_to_base64(value)
                    elif api_type == "List[Dict]":
                        api_response[api_field] = self._convert_to_list_dict(value)
                    elif api_type == "List[Dict[str, float]]":
                        api_response[api_field] = self._convert_keypoints_to_dict_list(value)
                    elif api_type == "float":
                        api_response[api_field] = float(value) if value is not None else 0.0
                    elif api_type == "List[float]":
                        if isinstance(value, (list, tuple)):
                            api_response[api_field] = [float(x) for x in value]
                        elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                            api_response[api_field] = value.flatten().tolist()
                        else:
                            api_response[api_field] = [float(value)] if value is not None else []
                    else:
                        api_response[api_field] = value
            
            if not api_response:
                api_response = self._create_fallback_api_response(processed_result)
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} API í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨: {e}")
            api_response = self._create_fallback_api_response(processed_result)
        
        return api_response
    
    def _prepare_next_step_data(self, processed_result: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¤ìŒ Stepë“¤ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        next_step_data = {}
        
        try:
            for next_step, data_schema in self.detailed_data_spec.provides_to_next_step.items():
                step_data = {}
                
                for data_key, data_type in data_schema.items():
                    if data_key in processed_result:
                        value = processed_result[data_key]
                        
                        if data_type == "np.ndarray" and NUMPY_AVAILABLE:
                            if TORCH_AVAILABLE and torch.is_tensor(value):
                                step_data[data_key] = value.detach().cpu().numpy()
                            elif not isinstance(value, np.ndarray):
                                step_data[data_key] = np.array(value)
                            else:
                                step_data[data_key] = value
                                
                        elif data_type == "torch.Tensor" and TORCH_AVAILABLE:
                            if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                                step_data[data_key] = torch.from_numpy(value)
                            elif not torch.is_tensor(value):
                                step_data[data_key] = torch.tensor(value)
                            else:
                                step_data[data_key] = value
                                
                        else:
                            step_data[data_key] = value
                
                if step_data:
                    next_step_data[next_step] = step_data
                    self.performance_metrics.step_data_transfers += 1
        
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ë‹¤ìŒ Step ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
        
        return next_step_data
    
    # ==============================================
    # ğŸ”¥ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    def _validate_input_data(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦"""
        validated = input_data.copy()
        
        try:
            for key, value in input_data.items():
                # ë°ì´í„° íƒ€ì… ê²€ì¦
                if key in self.detailed_data_spec.input_shapes:
                    expected_shape = self.detailed_data_spec.input_shapes[key]
                    if hasattr(value, 'shape'):
                        actual_shape = value.shape
                        if len(actual_shape) > len(expected_shape):
                            if actual_shape[1:] != tuple(expected_shape):
                                self.logger.warning(f"âš ï¸ {self.step_name} Shape mismatch for {key}")
                        elif actual_shape != tuple(expected_shape):
                            self.logger.warning(f"âš ï¸ {self.step_name} Shape mismatch for {key}")
                
                # ê°’ ë²”ìœ„ ê²€ì¦
                if key in self.detailed_data_spec.input_value_ranges:
                    min_val, max_val = self.detailed_data_spec.input_value_ranges[key]
                    if hasattr(value, 'min') and hasattr(value, 'max'):
                        actual_min, actual_max = float(value.min()), float(value.max())
                        if actual_min < min_val or actual_max > max_val:
                            self.logger.warning(f"âš ï¸ {self.step_name} Value range warning for {key}")
                            
                            # ìë™ í´ë¦¬í•‘
                            if self.config.strict_data_validation:
                                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                                    validated[key] = np.clip(value, min_val, max_val)
                                elif TORCH_AVAILABLE and torch.is_tensor(value):
                                    validated[key] = torch.clamp(value, min_val, max_val)
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            self.performance_metrics.validation_failures += 1
        
        return validated
    
    # ==============================================
    # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
    # ==============================================
    
    def _array_to_base64(self, array: Any) -> str:
        """NumPy ë°°ì—´/í…ì„œ â†’ Base64 ë¬¸ìì—´ ë³€í™˜"""
        try:
            if TORCH_AVAILABLE and torch.is_tensor(array):
                array = array.detach().cpu().numpy()
            
            if not NUMPY_AVAILABLE or not isinstance(array, np.ndarray):
                return ""
            
            # ê°’ ë²”ìœ„ ì •ê·œí™”
            if array.dtype != np.uint8:
                if array.max() <= 1.0:
                    array = (array * 255).astype(np.uint8)
                else:
                    array = np.clip(array, 0, 255).astype(np.uint8)
            
            # PIL Imageë¡œ ë³€í™˜
            if PIL_AVAILABLE:
                # ğŸ”¥ 4ì°¨ì› tensor ìë™ ì²˜ë¦¬ (batch dimension ì œê±°)
                if len(array.shape) == 4:
                    # (B, C, H, W) â†’ (C, H, W)
                    array = array.squeeze(0)
                
                if len(array.shape) == 3:
                    if array.shape[0] in [1, 3, 4] and array.shape[0] < array.shape[1]:
                        array = np.transpose(array, (1, 2, 0))
                    
                    if array.shape[2] == 1:
                        array = array.squeeze(2)
                        image = Image.fromarray(array, mode='L')
                    elif array.shape[2] == 3:
                        image = Image.fromarray(array, mode='RGB')
                    elif array.shape[2] == 4:
                        image = Image.fromarray(array, mode='RGBA')
                    else:
                        image = Image.fromarray(array[:, :, 0], mode='L')
                        
                elif len(array.shape) == 2:
                    image = Image.fromarray(array, mode='L')
                else:
                    raise ValueError(f"Unsupported array shape: {array.shape}")
                
                # Base64 ì¸ì½”ë”©
                buffer = BytesIO()
                image.save(buffer, format='PNG')
                buffer.seek(0)
                
                return base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            return ""
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} Base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""
    
    def _convert_to_list_dict(self, value: Any) -> List[Dict]:
        """ê°’ì„ List[Dict] í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            if isinstance(value, (list, tuple)):
                if all(isinstance(item, dict) for item in value):
                    return list(value)
                else:
                    return [{'value': item, 'index': i} for i, item in enumerate(value)]
            
            elif isinstance(value, dict):
                return [value]
            
            elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                if len(value.shape) == 1:
                    return [{'value': float(item), 'index': i} for i, item in enumerate(value)]
                elif len(value.shape) == 2:
                    return [{'row': i, 'data': row.tolist()} for i, row in enumerate(value)]
                else:
                    return [{'data': value.tolist()}]
            
            else:
                return [{'value': value}]
                
        except Exception as e:
            self.logger.debug(f"List[Dict] ë³€í™˜ ì‹¤íŒ¨: {e}")
            return [{'value': str(value)}]
    
    def _convert_keypoints_to_dict_list(self, keypoints: Any) -> List[Dict[str, float]]:
        """í‚¤í¬ì¸íŠ¸ë¥¼ List[Dict[str, float]] í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            if NUMPY_AVAILABLE and isinstance(keypoints, np.ndarray):
                if len(keypoints.shape) == 2 and keypoints.shape[1] >= 2:
                    result = []
                    for i, point in enumerate(keypoints):
                        point_dict = {
                            'x': float(point[0]),
                            'y': float(point[1])
                        }
                        if keypoints.shape[1] > 2:
                            point_dict['confidence'] = float(point[2])
                        if keypoints.shape[1] > 3:
                            point_dict['visibility'] = float(point[3])
                        
                        point_dict['index'] = i
                        result.append(point_dict)
                    
                    return result
            
            elif isinstance(keypoints, (list, tuple)):
                result = []
                for i, point in enumerate(keypoints):
                    if isinstance(point, (list, tuple)) and len(point) >= 2:
                        point_dict = {
                            'x': float(point[0]),
                            'y': float(point[1]),
                            'index': i
                        }
                        if len(point) > 2:
                            point_dict['confidence'] = float(point[2])
                        result.append(point_dict)
                
                return result
            
            return []
            
        except Exception as e:
            self.logger.debug(f"í‚¤í¬ì¸íŠ¸ Dict ë³€í™˜ ì‹¤íŒ¨: {e}")
            return []
    
    def _create_fallback_api_response(self, processed_result: Dict[str, Any]) -> Dict[str, Any]:
        """í´ë°± API ì‘ë‹µ ìƒì„±"""
        fallback_response = {}
        
        try:
            # ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í‚¤ë“¤ì— ëŒ€í•œ ê¸°ë³¸ ë§¤í•‘
            common_mappings = {
                'parsing_mask': 'base64_string',
                'segmentation_mask': 'base64_string',
                'fitted_image': 'base64_string',
                'enhanced_image': 'base64_string',
                'final_result': 'base64_string',
                'result_image': 'base64_string',
                'output_image': 'base64_string',
                
                'keypoints': 'List[Dict[str, float]]',
                'pose_keypoints': 'List[Dict[str, float]]',
                
                'confidence': 'float',
                'quality_score': 'float'
            }
            
            for key, value in processed_result.items():
                if key in common_mappings:
                    api_type = common_mappings[key]
                    
                    if api_type == 'base64_string':
                        fallback_response[key] = self._array_to_base64(value)
                    elif api_type == 'List[Dict[str, float]]':
                        fallback_response[key] = self._convert_keypoints_to_dict_list(value)
                    elif api_type == 'float':
                        fallback_response[key] = float(value) if value is not None else 0.0
            
            # ê¸°ë³¸ ì‘ë‹µì´ ì—†ëŠ” ê²½ìš° ì²« ë²ˆì§¸ ì´ë¯¸ì§€í˜• ë°ì´í„°ë¥¼ resultë¡œ ì„¤ì •
            if not fallback_response:
                for key, value in processed_result.items():
                    if NUMPY_AVAILABLE and isinstance(value, np.ndarray) and len(value.shape) >= 2:
                        fallback_response['result'] = self._array_to_base64(value)
                        break
                    elif PIL_AVAILABLE and isinstance(value, Image.Image):
                        fallback_response['result'] = self._array_to_base64(np.array(value))
                        break
            
        except Exception as e:
            self.logger.debug(f"í´ë°± API ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return fallback_response
    
    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """í‘œì¤€ ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'step_name': self.step_name,
            'step_id': self.step_id,
            'github_compatible': True,
            'central_hub_integrated': True,
            'detailed_data_spec_applied': False,
            'processing_time': 0.0,
            'timestamp': time.time()
        }
    
    # ==============================================
    # ğŸ”¥ Central Hub í˜¸í™˜ ë©”ì„œë“œë“¤ (v20.0)
    # ==============================================
    
    def _create_central_hub_config(self, **kwargs) -> CentralHubStepConfig:
        """Central Hub í˜¸í™˜ ì„¤ì • ìƒì„±"""
        config = CentralHubStepConfig()
        for key, value in kwargs.items():
            if hasattr(config, key):
                setattr(config, key, value)
        
        # Central Hub íŠ¹ë³„ ì„¤ì •
        config.github_compatibility_mode = True
        config.real_ai_pipeline_support = True
        config.enable_detailed_data_spec = True
        config.central_hub_integration = True
        
        # í™˜ê²½ë³„ ì„¤ì • ì ìš©
        if CONDA_INFO['is_target_env']:
            config.conda_optimized = True
            config.conda_env = CONDA_INFO['conda_env']
        
        if IS_M3_MAX:
            config.m3_max_optimized = True
            config.memory_gb = MEMORY_GB
            config.use_unified_memory = True
        
        return config
    
    def _apply_central_hub_environment_optimization(self):
        """Central Hub í™˜ê²½ ìµœì í™”"""
        try:
            # M3 Max Central Hub ìµœì í™”
            if self.is_m3_max:
                if self.device == "auto" and MPS_AVAILABLE:
                    self.device = "mps"
                
                if self.config.batch_size == 1 and self.memory_gb >= 64:
                    self.config.batch_size = 2
                
                self.config.auto_memory_cleanup = True
                self.logger.debug(f"âœ… Central Hub M3 Max ìµœì í™”: {self.memory_gb:.1f}GB, device={self.device}")
            
            # Central Hub conda í™˜ê²½ ìµœì í™”
            if self.conda_info['is_target_env']:
                self.config.optimization_enabled = True
                self.real_ai_pipeline_ready = True
                self.logger.debug(f"âœ… Central Hub conda í™˜ê²½ ìµœì í™”: {self.conda_info['conda_env']}")
            
        except Exception as e:
            self.logger.debug(f"Central Hub í™˜ê²½ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _central_hub_emergency_setup(self, error: Exception):
        """Central Hub í˜¸í™˜ ê¸´ê¸‰ ì„¤ì •"""
        self.step_name = getattr(self, 'step_name', self.__class__.__name__)
        self.logger = logging.getLogger("central_hub_emergency")
        self.device = "cpu"
        self.is_initialized = False
        self.github_compatible = False
        self.performance_metrics = CentralHubPerformanceMetrics()
        self.detailed_data_spec = DetailedDataSpecConfig()
        self.dependency_manager = CentralHubDependencyManager(self.step_name)
        self.logger.error(f"ğŸš¨ {self.step_name} Central Hub ê¸´ê¸‰ ì´ˆê¸°í™”: {error}")
    
    def _resolve_device(self, device: str) -> str:
        """Central Hub ë””ë°”ì´ìŠ¤ í•´ê²° (í™˜ê²½ ìµœì í™”)"""
        if device == "auto":
            # Central Hub M3 Max ìš°ì„  ì²˜ë¦¬
            if IS_M3_MAX and MPS_AVAILABLE:
                return "mps"
            
            if TORCH_AVAILABLE:
                if torch.cuda.is_available():
                    return "cuda"
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    return "mps"
            return "cpu"
        return device
    
    def _update_performance_metrics(self, processing_time: float, success: bool):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (Central Hub ê¸°ë°˜)"""
        try:
            self.performance_metrics.process_count += 1
            self.performance_metrics.total_process_time += processing_time
            self.performance_metrics.average_process_time = (
                self.performance_metrics.total_process_time / 
                self.performance_metrics.process_count
            )
            
            if success:
                self.performance_metrics.success_count += 1
            else:
                self.performance_metrics.error_count += 1
            
            # ìµœê·¼ ì²˜ë¦¬ ì‹œê°„ ì €ì¥
            self._last_processing_time = processing_time
            
            # Central Hub íŒŒì´í”„ë¼ì¸ ì„±ê³µë¥  ê³„ì‚°
            if self.performance_metrics.github_process_calls > 0:
                success_rate = (
                    self.performance_metrics.success_count /
                    self.performance_metrics.process_count * 100
                )
                self.performance_metrics.pipeline_success_rate = success_rate
                
        except Exception as e:
            self.logger.debug(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ Central Hub DI Container í¸ì˜ ë©”ì„œë“œë“¤ (v20.0)
    # ==============================================
    
    def get_service(self, service_key: str):
        """Central Hub DI Containerë¥¼ í†µí•œ ì„œë¹„ìŠ¤ ì¡°íšŒ"""
        try:
            if self.central_hub_container:
                return self.central_hub_container.get(service_key)
            else:
                # Central Hub Containerê°€ ì—†ìœ¼ë©´ ì „ì—­ ì»¨í…Œì´ë„ˆ ì‚¬ìš©
                return _get_service_from_central_hub(service_key)
        except Exception as e:
            self.logger.debug(f"ì„œë¹„ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨ ({service_key}): {e}")
            return None
    
    def register_service(self, service_key: str, service_instance: Any, singleton: bool = True):
        """Central Hub DI Containerì— ì„œë¹„ìŠ¤ ë“±ë¡"""
        try:
            if self.central_hub_container:
                self.central_hub_container.register(service_key, service_instance, singleton)
                self.logger.debug(f"âœ… {self.step_name} ì„œë¹„ìŠ¤ ë“±ë¡: {service_key}")
                return True
            else:
                self.logger.warning(f"âš ï¸ {self.step_name} Central Hub Container ì—†ìŒ - ì„œë¹„ìŠ¤ ë“±ë¡ ì‹¤íŒ¨: {service_key}")
                return False
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì„œë¹„ìŠ¤ ë“±ë¡ ì‹¤íŒ¨ ({service_key}): {e}")
            return False
    
    def optimize_central_hub_memory(self):
        """Central Hub DI Containerë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            if self.central_hub_container:
                cleanup_stats = self.central_hub_container.optimize_memory()
                self.logger.debug(f"âœ… {self.step_name} Central Hub Container ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {cleanup_stats}")
                return cleanup_stats
            else:
                # Central Hub Containerê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
                return {'gc_collected': True}
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} Central Hub Container ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {}
    
    def get_central_hub_stats(self) -> Dict[str, Any]:
        """Central Hub DI Container í†µê³„ ì¡°íšŒ"""
        try:
            if self.central_hub_container:
                return self.central_hub_container.get_stats()
            else:
                return {'error': 'Central Hub Container not available'}
        except Exception as e:
            return {'error': str(e)}
    
    # ==============================================
    # ğŸ”¥ GitHub í‘œì¤€ ì´ˆê¸°í™” ë° ìƒíƒœ ê´€ë¦¬ (Central Hub ê¸°ë°˜)
    # ==============================================
    
    def initialize(self) -> bool:
        """GitHub í‘œì¤€ ì´ˆê¸°í™” (Central Hub ê¸°ë°˜)"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"ğŸ”„ {self.step_name} GitHub í‘œì¤€ ì´ˆê¸°í™” ì‹œì‘ (Central Hub ê¸°ë°˜)...")
            
            # DetailedDataSpec ê²€ì¦
            if not self.data_conversion_ready:
                self.logger.warning(f"âš ï¸ {self.step_name} DetailedDataSpec ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ë¯¸ì™„ë£Œ")
            
            # ì´ˆê¸°í™” ìƒíƒœ ì„¤ì •
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                self.dependency_manager.dependency_status.base_initialized = True
                self.dependency_manager.dependency_status.github_compatible = True
                self.dependency_manager.dependency_status.central_hub_connected = True
                self.dependency_manager.dependency_status.single_source_of_truth = True
                self.dependency_manager.dependency_status.dependency_inversion_applied = True
            
            self.is_initialized = True
            
            self.logger.info(f"âœ… {self.step_name} GitHub í‘œì¤€ ì´ˆê¸°í™” ì™„ë£Œ (Central Hub ê¸°ë°˜)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} GitHub ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if hasattr(self, 'performance_metrics'):
                self.performance_metrics.error_count += 1
            return False

    def get_status(self) -> Dict[str, Any]:
        """GitHub í†µí•© ìƒíƒœ ì¡°íšŒ (v20.0 Central Hub)"""
        try:
            return {
                'step_info': {
                    'step_name': self.step_name,
                    'step_id': self.step_id,
                    'version': 'BaseStepMixin v20.0 Central Hub DI Container ì™„ì „ ì—°ë™'
                },
                'github_status_flags': {
                    'is_initialized': self.is_initialized,
                    'is_ready': self.is_ready,
                    'has_model': self.has_model,
                    'model_loaded': self.model_loaded,
                    'github_compatible': self.github_compatible,
                    'real_ai_pipeline_ready': self.real_ai_pipeline_ready,
                    'data_conversion_ready': self.data_conversion_ready
                },
                'dependencies_status': self.dependencies_injected,
                'detailed_data_spec_status': {
                    'spec_loaded': hasattr(self, 'dependency_manager') and self.dependency_manager.dependency_status.detailed_data_spec_loaded,
                    'data_conversion_ready': hasattr(self, 'dependency_manager') and self.dependency_manager.dependency_status.data_conversion_ready,
                    'preprocessing_configured': bool(self.detailed_data_spec.preprocessing_steps),
                    'postprocessing_configured': bool(self.detailed_data_spec.postprocessing_steps),
                    'api_mapping_configured': bool(self.detailed_data_spec.api_input_mapping and self.detailed_data_spec.api_output_mapping),
                    'step_flow_configured': bool(self.detailed_data_spec.provides_to_next_step or self.detailed_data_spec.accepts_from_previous_step)
                },
                'central_hub_status': {
                    'connected': hasattr(self, 'dependency_manager') and self.dependency_manager.dependency_status.central_hub_connected,
                    'single_source_of_truth': hasattr(self, 'dependency_manager') and self.dependency_manager.dependency_status.single_source_of_truth,
                    'dependency_inversion_applied': hasattr(self, 'dependency_manager') and self.dependency_manager.dependency_status.dependency_inversion_applied,
                    'central_hub_requests': hasattr(self, 'dependency_manager') and self.dependency_manager.central_hub_requests,
                    'container_available': self.central_hub_container is not None
                },
                'github_performance': {
                    'data_conversions': self.performance_metrics.data_conversions,
                    'preprocessing_operations': self.performance_metrics.preprocessing_operations,
                    'postprocessing_operations': self.performance_metrics.postprocessing_operations,
                    'api_conversions': self.performance_metrics.api_conversions,
                    'step_data_transfers': self.performance_metrics.step_data_transfers,
                    'validation_failures': self.performance_metrics.validation_failures,
                    'central_hub_requests': self.performance_metrics.central_hub_requests,
                    'service_resolutions': self.performance_metrics.service_resolutions
                },
                'central_hub_integration_info': {
                    'version': 'v20.0',
                    'integration_enabled': True,
                    'connected': self.central_hub_container is not None,
                    'model_loader_injected': self.dependencies_injected.get('model_loader', False),
                    'checkpoint_loading_ready': self.dependencies_injected.get('model_loader', False),
                    'step_requirements_registered': hasattr(self, 'model_loader') and self.model_loader is not None,
                    'dependency_inversion_pattern': True,
                    'zero_circular_reference': True,
                    'single_source_of_truth_pattern': True
                },
                'timestamp': time.time()
            }
        except Exception as e:
            self.logger.error(f"âŒ GitHub ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'version': 'BaseStepMixin v20.0 Central Hub DI Container ì™„ì „ ì—°ë™'}

    # ==============================================
    # ğŸ”¥ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (Central Hub ê¸°ë°˜)
    # ==============================================
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (Central Hub ê¸°ë°˜)"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} BaseStepMixin Central Hub ê¸°ë°˜ ì •ë¦¬ ì‹œì‘...")
            
            # Central Hub DI Containerë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ìµœì í™”
            if self.central_hub_container:
                try:
                    cleanup_stats = self.central_hub_container.optimize_memory()
                    self.logger.debug(f"Central Hub Container ë©”ëª¨ë¦¬ ìµœì í™”: {cleanup_stats}")
                except Exception as e:
                    self.logger.debug(f"Central Hub Container ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            
            # Central Hub Dependency Manager ì •ë¦¬
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                try:
                    self.dependency_manager.cleanup()
                except Exception as e:
                    self.logger.debug(f"Central Hub Dependency Manager ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì •ë¦¬
            if hasattr(self, 'performance_metrics'):
                try:
                    # ìµœì¢… í†µê³„ ë¡œê·¸
                    total_processes = self.performance_metrics.process_count
                    if total_processes > 0:
                        success_rate = (self.performance_metrics.success_count / total_processes) * 100
                        avg_time = self.performance_metrics.average_process_time
                        self.logger.info(f"ğŸ“Š {self.step_name} ìµœì¢… í†µê³„: {total_processes}ê°œ ì²˜ë¦¬, {success_rate:.1f}% ì„±ê³µ, í‰ê·  {avg_time:.3f}ì´ˆ")
                        self.logger.info(f"ğŸ“Š Central Hub ìš”ì²­: {self.performance_metrics.central_hub_requests}íšŒ")
                except Exception as e:
                    self.logger.debug(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ê¸°ë³¸ ì†ì„± ì •ë¦¬
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            self.central_hub_container = None
            self.di_container = None
            self.is_initialized = False
            self.is_ready = False
            
            self.logger.info(f"âœ… {self.step_name} BaseStepMixin Central Hub ê¸°ë°˜ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} BaseStepMixin ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì - ì•ˆì „í•œ ì •ë¦¬ (Central Hub ê¸°ë°˜)"""
        try:
            # ë¹„ë™ê¸° cleanupì„ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ (ì†Œë©¸ìì—ì„œëŠ” async ë¶ˆê°€)
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                self.dependency_manager.cleanup()
            
            if hasattr(self, 'central_hub_container') and self.central_hub_container:
                self.central_hub_container.optimize_memory()
        except:
            pass  # ì†Œë©¸ìì—ì„œëŠ” ì˜ˆì™¸ ë¬´ì‹œ

    # ==============================================
    # ğŸ”¥ ë¹ ì§„ í•µì‹¬ ë©”ì„œë“œë“¤ ì¶”ê°€ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
    # ==============================================

    def _direct_model_loader_injection(self, model_loader):
        """ModelLoader ì§ì ‘ ì£¼ì… (fallback) - ê¸°ì¡´ í˜¸í™˜ì„±"""
        try:
            self.model_loader = model_loader
            
            # ğŸ”¥ Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            if hasattr(model_loader, 'create_step_interface'):
                try:
                    self.model_interface = model_loader.create_step_interface(self.step_name)
                    self.logger.debug("âœ… Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    self.logger.debug(f"âš ï¸ Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í…ŒìŠ¤íŠ¸
            if hasattr(model_loader, 'validate_di_container_integration'):
                try:
                    validation_result = model_loader.validate_di_container_integration()
                    if validation_result.get('di_container_available', False):
                        self.logger.debug("âœ… ModelLoader Central Hub ì—°ë™ í™•ì¸ë¨")
                except Exception as e:
                    self.logger.debug(f"âš ï¸ ModelLoader Central Hub ì—°ë™ ê²€ì¦ ì‹¤íŒ¨: {e}")
            
            # ì˜ì¡´ì„± ìƒíƒœ ì—…ë°ì´íŠ¸
            self.dependencies_injected['model_loader'] = True
            self.has_model = True
            self.model_loaded = True
            self.real_ai_pipeline_ready = True
            
            self.logger.info("âœ… ModelLoader ì§ì ‘ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì§ì ‘ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            self.dependencies_injected['model_loader'] = False
            return False

    def _direct_validate_dependencies(self, format_type=None):
        """ì§ì ‘ ì˜ì¡´ì„± ê²€ì¦ (fallback) - ê¸°ì¡´ í˜¸í™˜ì„±"""
        try:
            # ê¸°ë³¸ ì˜ì¡´ì„± ê²€ì¦
            validation_result = {
                'model_loader': hasattr(self, 'model_loader') and self.model_loader is not None,
                'memory_manager': hasattr(self, 'memory_manager') and self.memory_manager is not None,
                'data_converter': hasattr(self, 'data_converter') and self.data_converter is not None,
                'central_hub_container': hasattr(self, 'central_hub_container') and self.central_hub_container is not None,
                'checkpoint_loading': False,
                'model_interface': hasattr(self, 'model_interface') and self.model_interface is not None
            }
            
            # ModelLoader ê²€ì¦
            if validation_result['model_loader']:
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦
                if hasattr(self.model_loader, 'validate_di_container_integration'):
                    try:
                        di_validation = self.model_loader.validate_di_container_integration()
                        validation_result['checkpoint_loading'] = di_validation.get('di_container_available', False)
                    except Exception as e:
                        self.logger.debug(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê²€ì¦ ì‹¤íŒ¨: {e}")
            
            self.logger.debug(f"âœ… {self.step_name} ì§ì ‘ ì˜ì¡´ì„± ê²€ì¦ ì™„ë£Œ: {sum(validation_result.values())}/{len(validation_result)}")
            
            # ë°˜í™˜ í˜•ì‹ ê²°ì •
            if format_type == DependencyValidationFormat.BOOLEAN_DICT:
                return validation_result
            else:
                # ìƒì„¸ ì •ë³´ ë°˜í™˜
                return {
                    'success': all(validation_result[key] for key in ['model_loader', 'central_hub_container']),
                    'dependencies': validation_result,
                    'github_compatible': True,
                    'central_hub_integrated': True,
                    'step_name': self.step_name,
                    'checkpoint_loading_ready': validation_result['checkpoint_loading'],
                    'model_interface_ready': validation_result['model_interface'],
                    'timestamp': time.time()
                }
            
        except Exception as e:
            self.logger.error(f"âŒ ì§ì ‘ ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            
            if format_type == DependencyValidationFormat.BOOLEAN_DICT:
                return {'model_loader': False, 'memory_manager': False, 'data_converter': False, 'central_hub_container': False}
            else:
                return {
                    'success': False,
                    'error': str(e),
                    'github_compatible': False,
                    'central_hub_integrated': True,
                    'step_name': self.step_name
                }

    # ==============================================
    # ğŸ”¥ GitHub í˜¸í™˜ ì˜ì¡´ì„± ì£¼ì… ì¸í„°í˜ì´ìŠ¤ (ê¸°ì¡´ ë©”ì„œë“œ ë³µì›)
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """GitHub í‘œì¤€ ModelLoader ì˜ì¡´ì„± ì£¼ì… (ê¸°ì¡´ ë©”ì„œë“œ ì˜¤ë²„ë¼ì´ë“œ)"""
        try:
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                # dependency_managerë¥¼ í†µí•œ ì£¼ì…
                self.dependency_manager._central_hub_container = self.central_hub_container
                success = True  # dependency_manager ê¸°ë³¸ ì„±ê³µ ì²˜ë¦¬
                if success:
                    self.model_loader = model_loader
                    
                    # ğŸ”¥ Stepë³„ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                    if hasattr(model_loader, 'create_step_interface'):
                        try:
                            self.model_interface = model_loader.create_step_interface(self.step_name)
                            self.logger.debug("âœ… Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
                        except Exception as e:
                            self.logger.debug(f"âš ï¸ Step ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    
                    # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í…ŒìŠ¤íŠ¸
                    if hasattr(model_loader, 'validate_di_container_integration'):
                        try:
                            validation_result = model_loader.validate_di_container_integration()
                            if validation_result.get('di_container_available', False):
                                self.logger.debug("âœ… ModelLoader Central Hub ì—°ë™ í™•ì¸ë¨")
                        except Exception as e:
                            self.logger.debug(f"âš ï¸ ModelLoader Central Hub ì—°ë™ ê²€ì¦ ì‹¤íŒ¨: {e}")
                    
                    self.has_model = True
                    self.model_loaded = True
                    self.real_ai_pipeline_ready = True
                    self.dependencies_injected['model_loader'] = True
                    
                    if hasattr(self, 'performance_metrics'):
                        self.performance_metrics.dependencies_injected += 1
                    
                    self.logger.info(f"âœ… {self.step_name} GitHub ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                    return True
            else:
                # dependency_managerê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ì£¼ì…
                return self._direct_model_loader_injection(model_loader)
                
        except Exception as e:
            if hasattr(self, 'performance_metrics'):
                self.performance_metrics.injection_failures += 1
            self.logger.error(f"âŒ {self.step_name} GitHub ModelLoader ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜: {e}")
            return False

    def set_memory_manager(self, memory_manager):
        """GitHub í‘œì¤€ MemoryManager ì˜ì¡´ì„± ì£¼ì… (ê¸°ì¡´ ë©”ì„œë“œ ì˜¤ë²„ë¼ì´ë“œ)"""
        try:
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                # dependency_managerë¥¼ í†µí•œ ì£¼ì… (ë‹¨ìˆœí™”)
                self.memory_manager = memory_manager
                self.dependencies_injected['memory_manager'] = True
                
                if hasattr(self.dependency_manager, 'dependency_status'):
                    self.dependency_manager.dependency_status.memory_manager = True
                
                if hasattr(self, 'performance_metrics'):
                    self.performance_metrics.dependencies_injected += 1
                
                self.logger.debug(f"âœ… {self.step_name} GitHub MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                return True
            else:
                # dependency_managerê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ì£¼ì…
                self.memory_manager = memory_manager
                self.dependencies_injected['memory_manager'] = True
                self.logger.debug("âœ… MemoryManager ì§ì ‘ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                return True
                
        except Exception as e:
            if hasattr(self, 'performance_metrics'):
                self.performance_metrics.injection_failures += 1
            self.logger.warning(f"âš ï¸ {self.step_name} GitHub MemoryManager ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜: {e}")
            return False

    def set_data_converter(self, data_converter):
        """GitHub í‘œì¤€ DataConverter ì˜ì¡´ì„± ì£¼ì… (ê¸°ì¡´ ë©”ì„œë“œ ì˜¤ë²„ë¼ì´ë“œ)"""
        try:
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                # dependency_managerë¥¼ í†µí•œ ì£¼ì… (ë‹¨ìˆœí™”)
                self.data_converter = data_converter
                self.dependencies_injected['data_converter'] = True
                
                if hasattr(self.dependency_manager, 'dependency_status'):
                    self.dependency_manager.dependency_status.data_converter = True
                
                if hasattr(self, 'performance_metrics'):
                    self.performance_metrics.dependencies_injected += 1
                
                self.logger.debug(f"âœ… {self.step_name} GitHub DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                return True
            else:
                # dependency_managerê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ì£¼ì…
                self.data_converter = data_converter
                self.dependencies_injected['data_converter'] = True
                self.logger.debug("âœ… DataConverter ì§ì ‘ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                return True
                
        except Exception as e:
            if hasattr(self, 'performance_metrics'):
                self.performance_metrics.injection_failures += 1
            self.logger.error(f"âŒ {self.step_name} GitHub DataConverter ì˜ì¡´ì„± ì£¼ì… ì˜¤ë¥˜: {e}")
            return False

    # ==============================================
    # ğŸ”¥ GitHub í˜¸í™˜ ì˜ì¡´ì„± ê²€ì¦ (ê¸°ì¡´ ë©”ì„œë“œë“¤ ë³µì›)
    # ==============================================
    
    def validate_dependencies(self, format_type: DependencyValidationFormat = None) -> Union[Dict[str, bool], Dict[str, Any]]:
        """GitHub í”„ë¡œì íŠ¸ í˜¸í™˜ ì˜ì¡´ì„± ê²€ì¦ (ê¸°ì¡´ ë©”ì„œë“œ ì˜¤ë²„ë¼ì´ë“œ)"""
        try:
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                # dependency_managerê°€ ìˆëŠ” ê²½ìš°
                return self.dependency_manager.validate_dependencies_central_hub_format(format_type)
            else:
                # dependency_managerê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ê²€ì¦
                return self._direct_validate_dependencies(format_type)
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} GitHub ì˜ì¡´ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            
            if format_type == DependencyValidationFormat.BOOLEAN_DICT:
                return {'model_loader': False, 'step_interface': False, 'memory_manager': False, 'data_converter': False}
            else:
                return {'success': False, 'error': str(e), 'github_compatible': False, 'central_hub_integrated': True}

    # ==============================================
    # ğŸ”¥ ì¶”ê°€ ê¸°ì¡´ í˜¸í™˜ì„± ë©”ì„œë“œë“¤
    # ==============================================

    def get_model_loader(self):
        """ModelLoader ì¡°íšŒ í¸ì˜ ë©”ì„œë“œ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        return getattr(self, 'model_loader', None)

    def get_memory_manager(self):
        """MemoryManager ì¡°íšŒ í¸ì˜ ë©”ì„œë“œ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        return getattr(self, 'memory_manager', None)

    def get_data_converter(self):
        """DataConverter ì¡°íšŒ í¸ì˜ ë©”ì„œë“œ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        return getattr(self, 'data_converter', None)

    def get_step_interface(self):
        """Step Interface ì¡°íšŒ í¸ì˜ ë©”ì„œë“œ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        return getattr(self, 'model_interface', None)

    def is_model_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        return getattr(self, 'model_loaded', False)

    def is_step_ready(self) -> bool:
        """Step ì¤€ë¹„ ìƒíƒœ í™•ì¸ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        return getattr(self, 'is_ready', False)

    def get_step_name(self) -> str:
        """Step ì´ë¦„ ì¡°íšŒ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        return getattr(self, 'step_name', self.__class__.__name__)

    def get_step_id(self) -> int:
        """Step ID ì¡°íšŒ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        return getattr(self, 'step_id', 0)

    def get_device(self) -> str:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ì¡°íšŒ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        return getattr(self, 'device', 'cpu')

    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ì¡°íšŒ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        if hasattr(self, 'performance_metrics'):
            return {
                'process_count': self.performance_metrics.process_count,
                'total_process_time': self.performance_metrics.total_process_time,
                'average_process_time': self.performance_metrics.average_process_time,
                'error_count': self.performance_metrics.error_count,
                'success_count': self.performance_metrics.success_count,
                'dependencies_injected': self.performance_metrics.dependencies_injected,
                'central_hub_requests': getattr(self.performance_metrics, 'central_hub_requests', 0)
            }
        else:
            return getattr(self, 'performance_stats', {})

    async def warmup(self) -> bool:
        """Step ì›Œë°ì—… (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        try:
            if not self.is_initialized:
                await self.initialize()
            
            if hasattr(self, 'model_loader') and self.model_loader:
                # ëª¨ë¸ ë¡œë”ë¥¼ í†µí•œ ì›Œë°ì—…
                try:
                    if hasattr(self.model_loader, 'warmup_models'):
                        await self.model_loader.warmup_models(self.step_name)
                        self.warmup_completed = True
                        self.logger.info(f"âœ… {self.step_name} ì›Œë°ì—… ì™„ë£Œ")
                        return True
                except Exception as e:
                    self.logger.debug(f"âš ï¸ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            self.warmup_completed = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return False

    def create_step_interface(self):
        """Step Interface ìƒì„± (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        try:
            if hasattr(self, 'model_loader') and self.model_loader:
                if hasattr(self.model_loader, 'create_step_interface'):
                    interface = self.model_loader.create_step_interface(self.step_name)
                    self.model_interface = interface
                    return interface
            return None
        except Exception as e:
            self.logger.error(f"âŒ Step Interface ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def log_performance(self, processing_time: float, success: bool = True):
        """ì„±ëŠ¥ ë¡œê¹… (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        try:
            if hasattr(self, 'performance_stats'):
                self.performance_stats['total_processed'] += 1
                if not success:
                    self.performance_stats['error_count'] += 1
                
                # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
                total = self.performance_stats['total_processed']
                if total > 0:
                    current_avg = self.performance_stats.get('avg_processing_time', 0.0)
                    self.performance_stats['avg_processing_time'] = (current_avg * (total - 1) + processing_time) / total
                    self.performance_stats['success_rate'] = (total - self.performance_stats['error_count']) / total
        except Exception as e:
            self.logger.debug(f"ì„±ëŠ¥ ë¡œê¹… ì‹¤íŒ¨: {e}")

    def reset_performance_stats(self):
        """ì„±ëŠ¥ í†µê³„ ë¦¬ì…‹ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        try:
            if hasattr(self, 'performance_metrics'):
                self.performance_metrics.process_count = 0
                self.performance_metrics.total_process_time = 0.0
                self.performance_metrics.average_process_time = 0.0
                self.performance_metrics.error_count = 0
                self.performance_metrics.success_count = 0
            
            if hasattr(self, 'performance_stats'):
                self.performance_stats.update({
                    'total_processed': 0,
                    'avg_processing_time': 0.0,
                    'error_count': 0,
                    'success_rate': 1.0
                })
            
            self.logger.debug(f"âœ… {self.step_name} ì„±ëŠ¥ í†µê³„ ë¦¬ì…‹ ì™„ë£Œ")
        except Exception as e:
            self.logger.debug(f"ì„±ëŠ¥ í†µê³„ ë¦¬ì…‹ ì‹¤íŒ¨: {e}")

    # ==============================================
    # ğŸ”¥ ê¸°ì¡´ API í˜¸í™˜ì„± ë©”ì„œë“œë“¤ (ì™„ì „ êµ¬í˜„)
    # ==============================================

    def set_di_container(self, di_container):
        """DI Container ì„¤ì • (ê¸°ì¡´ API í˜¸í™˜ì„±)"""
        return self.set_central_hub_container(di_container)

    def get_di_container_stats(self) -> Dict[str, Any]:
        """DI Container í†µê³„ ì¡°íšŒ (ê¸°ì¡´ API í˜¸í™˜ì„±)"""
        return self.get_central_hub_stats()

    def optimize_di_memory(self):
        """DI Containerë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ìµœì í™” (ê¸°ì¡´ API í˜¸í™˜ì„±)"""
        return self.optimize_central_hub_memory()

    def validate_dependencies_boolean(self) -> Dict[str, bool]:
        """GitHub Step í´ë˜ìŠ¤ í˜¸í™˜ (GeometricMatchingStep ë“±) - ê¸°ì¡´ í˜¸í™˜ì„±"""
        return self.validate_dependencies(DependencyValidationFormat.BOOLEAN_DICT)
    
    def validate_dependencies_detailed(self) -> Dict[str, Any]:
        """StepFactory í˜¸í™˜ (ìƒì„¸ ì •ë³´) - ê¸°ì¡´ í˜¸í™˜ì„±"""
        return self.validate_dependencies(DependencyValidationFormat.DETAILED_DICT)

# ==============================================
# ğŸ”¥ Export
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤
    'BaseStepMixin',
    'CentralHubDependencyManager',
    
    # ì„¤ì • ë° ìƒíƒœ í´ë˜ìŠ¤ë“¤
    'DetailedDataSpecConfig',
    'CentralHubStepConfig',
    'CentralHubDependencyStatus', 
    'CentralHubPerformanceMetrics',
    
    # GitHub ì—´ê±°í˜•ë“¤
    'ProcessMethodSignature',
    'DependencyValidationFormat',
    'DataConversionMethod',
    
    'StepPropertyGuarantee',
    'enhance_base_step_mixin_init',
    'validate_step_properties',
    'create_step_with_guaranteed_properties',
    'fix_step_attribute_errors'
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'CV2_AVAILABLE',
    'CONDA_INFO',
    'IS_M3_MAX',
    'MEMORY_GB'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë¡œê·¸
# ==============================================

logger = logging.getLogger(__name__)
logger.info("=" * 100)
logger.info("ğŸ”¥ BaseStepMixin v20.0 - Central Hub DI Container ì™„ì „ ì—°ë™ + ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²°")
logger.info("=" * 100)
logger.info("âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - ì¤‘ì•™ í—ˆë¸Œ íŒ¨í„´ ì ìš©")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - TYPE_CHECKING + ì§€ì—° import ì™„ë²½ ì ìš©")
logger.info("âœ… ë‹¨ë°©í–¥ ì˜ì¡´ì„± ê·¸ë˜í”„ - DI Containerë§Œì„ í†µí•œ ì˜ì¡´ì„± ì£¼ì…")
logger.info("âœ… step_model_requirements.py DetailedDataSpec ì™„ì „ í™œìš©")
logger.info("âœ… API â†” AI ëª¨ë¸ ê°„ ë°ì´í„° ë³€í™˜ í‘œì¤€í™” ì™„ë£Œ")
logger.info("âœ… Step ê°„ ë°ì´í„° íë¦„ ìë™ ì²˜ë¦¬")
logger.info("âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ ìë™ ì ìš©")
logger.info("âœ… GitHub í”„ë¡œì íŠ¸ Step í´ë˜ìŠ¤ë“¤ê³¼ 100% í˜¸í™˜")
logger.info("âœ… process() ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ì™„ì „ í‘œì¤€í™”")
logger.info("âœ… ì‹¤ì œ Step í´ë˜ìŠ¤ë“¤ì€ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„í•˜ë©´ ë¨")
logger.info("âœ… validate_dependencies() ì˜¤ë²„ë¡œë“œ ì§€ì›")
logger.info("âœ… ëª¨ë“  ê¸°ëŠ¥ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ êµ¬ì¡°ë§Œ ê°œì„ ")
logger.info("âœ… ê¸°ì¡´ API 100% í˜¸í™˜ì„± ë³´ì¥")

logger.info("ğŸ”§ Central Hub DI Container v7.0 ì—°ë™:")
logger.info("   ğŸ”— Single Source of Truth - ëª¨ë“  ì„œë¹„ìŠ¤ëŠ” Central Hubë¥¼ ê±°ì¹¨")
logger.info("   ğŸ”— Central Hub Pattern - DI Containerê°€ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì˜ ì¤‘ì‹¬")
logger.info("   ğŸ”— Dependency Inversion - ìƒìœ„ ëª¨ë“ˆì´ í•˜ìœ„ ëª¨ë“ˆì„ ì œì–´")
logger.info("   ğŸ”— Zero Circular Reference - ìˆœí™˜ì°¸ì¡° ì›ì²œ ì°¨ë‹¨")

logger.info("ğŸ”§ ìˆœí™˜ì°¸ì¡° í•´ê²° ë°©ë²•:")
logger.info("   ğŸ”— CentralHubDependencyManager ë‚´ì¥ìœ¼ë¡œ ì™¸ë¶€ ì˜ì¡´ì„± ì°¨ë‹¨")
logger.info("   ğŸ”— TYPE_CHECKING + ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€")
logger.info("   ğŸ”— Central Hub DI Containerë¥¼ í†µí•œ ë‹¨ë°©í–¥ ì˜ì¡´ì„± ì£¼ì…")
logger.info("   ğŸ”— ëª¨ë“  ê¸°ëŠ¥ ê·¸ëŒ€ë¡œ ìœ ì§€")

logger.info("ğŸ¯ ì™„ì „ ë³µì›ëœ ì „ì²˜ë¦¬ (12ê°œ):")
logger.info("   - ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (512x512, 768x1024, 256x192, 224x224, 368x368, 1024x1024)")
logger.info("   - ì •ê·œí™” (ImageNet, CLIP, Diffusion)")
logger.info("   - í…ì„œ ë³€í™˜, SAM í”„ë¡¬í”„íŠ¸, Diffusion ì…ë ¥, OOTD ì…ë ¥, í¬ì¦ˆ íŠ¹ì§•, SR ì…ë ¥")

logger.info("ğŸ¯ ì™„ì „ ë³µì›ëœ í›„ì²˜ë¦¬ (15ê°œ):")
logger.info("   - Softmax, Argmax, ì›ë³¸ í¬ê¸° ë¦¬ì‚¬ì´ì¦ˆ, NumPy ë³€í™˜, ì„ê³„ê°’, NMS")
logger.info("   - ì—­ì •ê·œí™” (Diffusion, ImageNet), ê°’ í´ë¦¬í•‘, ë§ˆìŠ¤í¬ ì ìš©")
logger.info("   - í˜•íƒœí•™ì  ì—°ì‚°, í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ, ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§, ì‹ ë¢°ë„ í•„í„°ë§")
logger.info("   - ì„¸ë¶€ì‚¬í•­ í–¥ìƒ, ìµœì¢… í•©ì„±, í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±")

logger.info(f"ğŸ”§ í˜„ì¬ conda í™˜ê²½: {CONDA_INFO['conda_env']} ({'âœ… ìµœì í™”ë¨' if CONDA_INFO['is_target_env'] else 'âš ï¸ ê¶Œì¥: mycloset-ai-clean'})")
logger.info(f"ğŸ–¥ï¸  í˜„ì¬ ì‹œìŠ¤í…œ: M3 Max={IS_M3_MAX}, ë©”ëª¨ë¦¬={MEMORY_GB:.1f}GB")
logger.info(f"ğŸš€ Central Hub AI íŒŒì´í”„ë¼ì¸ ì¤€ë¹„: {TORCH_AVAILABLE and (MPS_AVAILABLE or (torch.cuda.is_available() if TORCH_AVAILABLE else False))}")
logger.info("=" * 100)
logger.info("ğŸ‰ BaseStepMixin v20.0 Central Hub DI Container ì™„ì „ ì—°ë™ + ìˆœí™˜ì°¸ì¡° í•´ê²° ì™„ë£Œ!")
logger.info("ğŸ’¡ ì´ì œ ì‹¤ì œ Step í´ë˜ìŠ¤ë“¤ì€ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„í•˜ë©´ ë©ë‹ˆë‹¤!")
logger.info("ğŸ’¡ ëª¨ë“  ë°ì´í„° ë³€í™˜ì´ BaseStepMixinì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤!")
logger.info("ğŸ’¡ ìˆœí™˜ì°¸ì¡° ë¬¸ì œê°€ ì™„ì „íˆ í•´ê²°ë˜ê³  Central Hub DI Containerë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤!")
logger.info("ğŸ’¡ Central Hub íŒ¨í„´ìœ¼ë¡œ ëª¨ë“  ì˜ì¡´ì„±ì´ ë‹¨ì¼ ì§€ì ì„ í†µí•´ ê´€ë¦¬ë©ë‹ˆë‹¤!")
logger.info("ğŸ’¡ ê¸°ì¡´ API 100% í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ë©´ì„œ êµ¬ì¡°ë§Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤!")
logger.info("=" * 100)# backend/app/ai_pipeline/steps/base_step_mixin.py