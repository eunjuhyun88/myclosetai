# app/ai_pipeline/steps/base_step_mixin.py
"""
ğŸ”¥ BaseStepMixin v4.1 - 1ë²ˆ+2ë²ˆ ì™„ì „ í†µí•© ë²„ì „
===============================================

âœ… 1ë²ˆ íŒŒì¼ì˜ ì™„ì„±ë„ ë†’ì€ ê¸°ëŠ¥ë“¤ + 2ë²ˆ íŒŒì¼ì˜ í•µì‹¬ ìˆ˜ì •ì‚¬í•­ í†µí•©
âœ… 'dict' object is not callable ì™„ì „ í•´ê²°  
âœ… missing positional argument ì™„ì „ í•´ê²°
âœ… VirtualFittingConfig get ì†ì„± ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… M3 Max GPU íƒ€ì… ì¶©ëŒ ì™„ì „ í•´ê²°
âœ… NumPy 2.x í˜¸í™˜ì„± ì™„ì „ ì§€ì›
âœ… conda í™˜ê²½ ì™„ë²½ ìµœì í™”
âœ… ëª¨ë“  Step í´ë˜ìŠ¤ 100% í˜¸í™˜ì„±
"""

import os
import gc
import time
import asyncio
import logging
import threading
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor

# ==============================================
# ğŸ”¥ NumPy 2.x í˜¸í™˜ì„± ë¬¸ì œ ì™„ì „ í•´ê²°
# ==============================================

# NumPy ë²„ì „ í™•ì¸ ë° ê°•ì œ ë‹¤ìš´ê·¸ë ˆì´ë“œ ì²´í¬  
try:
    import numpy as np
    numpy_version = np.__version__
    major_version = int(numpy_version.split('.')[0])
    
    if major_version >= 2:
        logging.warning(f"âš ï¸ NumPy {numpy_version} ê°ì§€ë¨. NumPy 1.x ê¶Œì¥")
        logging.warning("ğŸ”§ í•´ê²°ë°©ë²•: conda install numpy=1.24.4 -y --force-reinstall")
        # NumPy 2.xì—ì„œë„ ë™ì‘í•˜ë„ë¡ í˜¸í™˜ì„± ì„¤ì •
        try:
            np.set_printoptions(legacy='1.25')
        except:
            pass
    
    NUMPY_AVAILABLE = True
    
except ImportError as e:
    NUMPY_AVAILABLE = False
    logging.error(f"âŒ NumPy import ì‹¤íŒ¨: {e}")
    np = None

# ì•ˆì „í•œ PyTorch import (NumPy ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°)
try:
    # PyTorch import ì „ì— í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
    
    # M3 Max MPS ì§€ì› í™•ì¸
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        DEFAULT_DEVICE = "mps"
        logging.info("âœ… M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
    else:
        MPS_AVAILABLE = False
        DEFAULT_DEVICE = "cpu"
        logging.info("â„¹ï¸ CPU ëª¨ë“œ ì‚¬ìš©")
        
except ImportError as e:
    TORCH_AVAILABLE = False
    MPS_AVAILABLE = False
    DEFAULT_DEVICE = "cpu"
    torch = None
    logging.warning(f"âš ï¸ PyTorch ì—†ìŒ: {e}")

# ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì „í•œ import
try:
    import cv2
    from PIL import Image
    CV_AVAILABLE = True
    PIL_AVAILABLE = True
except ImportError:
    CV_AVAILABLE = False
    PIL_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì™„ì „ ìˆ˜ì •ëœ SafeConfig í´ë˜ìŠ¤ v2.1
# ==============================================

class SafeConfig:
    """
    ğŸ”§ ì•ˆì „í•œ ì„¤ì • í´ë˜ìŠ¤ v2.1 - VirtualFittingConfig í˜¸í™˜ì„± ì™„ì „ í•´ê²°
    
    âœ… NumPy 2.x í˜¸í™˜ì„± ì™„ì „ ì§€ì›
    âœ… ë”•ì…”ë„ˆë¦¬ì™€ ê°ì²´ ëª¨ë‘ ì§€ì›  
    âœ… callable ê°ì²´ ì•ˆì „ ì²˜ë¦¬
    âœ… get() ë©”ì„œë“œ ì§€ì›
    âœ… VirtualFittingConfig ì™„ì „ í˜¸í™˜ì„±
    """
    
    def __init__(self, data: Any = None):
        self._data = {}
        self._original_data = data
        
        try:
            if data is None:
                self._data = {}
            elif hasattr(data, '__dict__'):
                # ì„¤ì • ê°ì²´ì¸ ê²½ìš° (VirtualFittingConfig ë“±)
                self._data = data.__dict__.copy()
                
                # ì¶”ê°€ë¡œ ê³µê°œ ì†ì„±ë“¤ í™•ì¸
                for attr_name in dir(data):
                    if not attr_name.startswith('_'):
                        try:
                            attr_value = getattr(data, attr_name)
                            if not callable(attr_value):
                                self._data[attr_name] = attr_value
                        except:
                            pass
                            
            elif isinstance(data, dict):
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
                self._data = data.copy()
            elif callable(data):
                # ğŸ”¥ callable ê°ì²´ ì™„ì „ í•´ê²°
                logger.warning("âš ï¸ callable ì„¤ì • ê°ì²´ ê°ì§€ë¨, ë¹ˆ ì„¤ì •ìœ¼ë¡œ ì²˜ë¦¬")
                self._data = {}
            else:
                # ê¸°íƒ€ ê²½ìš° - ë¬¸ìì—´ì´ë‚˜ ìˆ«ì ë“±
                self._data = {}
                
        except Exception as e:
            logger.warning(f"âš ï¸ ì„¤ì • ê°ì²´ íŒŒì‹± ì‹¤íŒ¨: {e}, ë¹ˆ ì„¤ì • ì‚¬ìš©")
            self._data = {}
        
        # ì†ì„±ìœ¼ë¡œ ì„¤ì • (ì•ˆì „í•˜ê²Œ)
        for key, value in self._data.items():
            try:
                if isinstance(key, str) and key.isidentifier():
                    setattr(self, key, value)
            except:
                pass
    
    def get(self, key: str, default=None):
        """ë”•ì…”ë„ˆë¦¬ì²˜ëŸ¼ get ë©”ì„œë“œ ì§€ì› - VirtualFittingConfig í˜¸í™˜ì„±"""
        return self._data.get(key, default)
    
    def __getitem__(self, key):
        return self._data.get(key, None)
    
    def __setitem__(self, key, value):
        self._data[key] = value
        if isinstance(key, str) and key.isidentifier():
            try:
                setattr(self, key, value)
            except:
                pass
    
    def __contains__(self, key):
        return key in self._data
    
    def keys(self):
        return self._data.keys()
    
    def values(self):
        return self._data.values()
    
    def items(self):
        return self._data.items()
    
    def update(self, other):
        if isinstance(other, dict):
            self._data.update(other)
            for key, value in other.items():
                if isinstance(key, str) and key.isidentifier():
                    try:
                        setattr(self, key, value)
                    except:
                        pass
    
    def __str__(self):
        return str(self._data)
    
    def __repr__(self):
        return f"SafeConfig({self._data})"
    
    def __bool__(self):
        return bool(self._data)

# ==============================================
# ğŸ”¥ ì™„ì „ ìˆ˜ì •ëœ BaseStepMixin v4.1
# ==============================================

class BaseStepMixin:
    """
    ğŸ”¥ ì™„ì „ í†µí•©ëœ BaseStepMixin v4.1 - ëª¨ë“  ë¬¸ì œ í•´ê²°
    
    ëª¨ë“  Step í´ë˜ìŠ¤ê°€ ìƒì†ë°›ëŠ” ê¸°ë³¸ Mixin í´ë˜ìŠ¤
    âœ… 1ë²ˆ íŒŒì¼ì˜ ì™„ì„±ë„ + 2ë²ˆ íŒŒì¼ì˜ í•µì‹¬ ìˆ˜ì •ì‚¬í•­ í†µí•©
    âœ… NumPy 2.x í˜¸í™˜ì„± ë¬¸ì œ ì™„ì „ í•´ê²°
    âœ… ëª¨ë“  ì´ˆê¸°í™” ë¬¸ì œ ì™„ì „ í•´ê²°  
    âœ… callable ê°ì²´ ì˜¤ë¥˜ í•´ê²°
    âœ… missing argument ì˜¤ë¥˜ í•´ê²°
    âœ… VirtualFittingConfig í˜¸í™˜ì„± í•´ê²°
    âœ… M3 Max GPU íƒ€ì… ì¶©ëŒ í•´ê²°
    """
    
    def __init__(self, *args, **kwargs):
        """
        ğŸ”¥ ì™„ì „ ì•ˆì „í•œ ì´ˆê¸°í™” - ëª¨ë“  ì˜¤ë¥˜ í•´ê²° + NumPy 2.x ì§€ì›
        """
        
        # ğŸ”¥ Step 0: NumPy 2.x í˜¸í™˜ì„± ì²´í¬ ë° ê²½ê³ 
        self._check_numpy_compatibility()
        
        # ğŸ”¥ Step 1: ë‹¤ì¤‘ ìƒì† ì•ˆì „í•œ ì²˜ë¦¬
        try:
            # MRO ì²´í¬í•˜ì—¬ object.__init__ í˜¸ì¶œ ì—¬ë¶€ ê²°ì •
            mro = type(self).__mro__
            if len(mro) > 2:  # BaseStepMixin, ì‹¤ì œí´ë˜ìŠ¤, object ì´ìƒ
                # ë‹¤ì¤‘ ìƒì†ì¸ ê²½ìš° super() í˜¸ì¶œ ì‹œë„
                super().__init__()
            # objectë§Œ ìƒì†ë°›ì€ ê²½ìš° super() í˜¸ì¶œ ì•ˆí•¨
        except TypeError:
            # object.__init__() íŒŒë¼ë¯¸í„° ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¬´ì‹œ
            pass
        except Exception as e:
            logger.warning(f"âš ï¸ super().__init__() ì‹¤íŒ¨: {e}")
        
        # ğŸ”¥ Step 2: ê¸°ë³¸ ì†ì„±ë“¤ ë¨¼ì € ì„¤ì •
        self.step_name = getattr(self, 'step_name', self.__class__.__name__)
        
        # logger ì†ì„± ë°˜ë“œì‹œ ë¨¼ì € ì„¤ì •
        if not hasattr(self, 'logger'):
            self.logger = logging.getLogger(f"pipeline.{self.step_name}")
        
        # ğŸ”¥ Step 3: device ì†ì„± ì•ˆì „í•˜ê²Œ ì„¤ì • (2ë²ˆ íŒŒì¼ì˜ í†µí•© ë°©ì‹ ì ìš©)
        self.device = self._safe_device_setup(kwargs)
        self.device_type = kwargs.get('device_type', self._detect_device_type())
        
        # ğŸ”¥ Step 4: ì‹œìŠ¤í…œ ì •ë³´ ì„¤ì •
        self.memory_gb = kwargs.get('memory_gb', self._detect_memory())
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        
        # ğŸ”¥ Step 5: í’ˆì§ˆ ì„¤ì •
        self.quality_level = kwargs.get('quality_level', 'balanced')
        self.batch_size = kwargs.get('batch_size', self._calculate_optimal_batch_size())
        
        # ğŸ”¥ Step 6: ì„¤ì • ì²˜ë¦¬ (config ê°ì²´ í˜¸ì¶œ ì˜¤ë¥˜ í•´ê²°)
        raw_config = kwargs.get('config', {})
        self.config = SafeConfig(raw_config)
        
        # ğŸ”¥ Step 7: ìƒíƒœ ê´€ë¦¬ ì´ˆê¸°í™”
        self.is_initialized = False
        self.model_interface = None
        self.model_loader = None
        self.performance_metrics = {}
        self.error_count = 0
        self.last_error = None
        self.last_processing_time = 0.0
        self.total_processing_count = 0
        
        # ğŸ”¥ Step 8: ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.cache_dir = Path(kwargs.get('cache_dir', './cache'))
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ğŸ”¥ Step 9: M3 Max ìµœì í™” ì„¤ì •
        self._setup_m3_max_optimization()
        
        # ğŸ”¥ Step 10: PyTorch ìµœì í™” ì„¤ì •
        self._setup_pytorch_optimization()
        
        # ğŸ”¥ Step 11: ì›Œë°ì—… í•¨ìˆ˜ë“¤ ì•ˆì „í•˜ê²Œ ì„¤ì • (dict callable ë¬¸ì œ í•´ê²°)
        self._setup_warmup_functions()
        
        # ğŸ”¥ ì´ˆê¸°í™” ì™„ë£Œ ë¡œê¹…
        self.logger.info(f"âœ… {self.step_name} BaseStepMixin v4.1 ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ”§ Device: {self.device} ({self.device_type})")
        self.logger.info(f"ğŸ“Š Memory: {self.memory_gb}GB, M3 Max: {'âœ…' if self.is_m3_max else 'âŒ'}")
        self.logger.info(f"ğŸ”¢ NumPy: {np.__version__ if NUMPY_AVAILABLE else 'N/A'}")
    
    def _check_numpy_compatibility(self):
        """NumPy 2.x í˜¸í™˜ì„± ì²´í¬ ë° ê²½ê³ """
        try:
            if NUMPY_AVAILABLE:
                numpy_version = np.__version__
                major_version = int(numpy_version.split('.')[0])
                
                if major_version >= 2:
                    self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
                    self.logger.warning(f"âš ï¸ NumPy {numpy_version} ê°ì§€ë¨ (2.x)")
                    self.logger.warning("ğŸ”§ í˜¸í™˜ì„±ì„ ìœ„í•´ NumPy 1.24.4ë¡œ ë‹¤ìš´ê·¸ë ˆì´ë“œ ê¶Œì¥")
                    self.logger.warning("ğŸ’¡ ì‹¤í–‰: conda install numpy=1.24.4 -y --force-reinstall")
                    
                    # NumPy 2.xìš© í˜¸í™˜ì„± ì„¤ì •
                    try:
                        np.set_printoptions(legacy='1.25')
                        self.logger.info("âœ… NumPy 2.x í˜¸í™˜ì„± ëª¨ë“œ í™œì„±í™”")
                    except:
                        pass
                else:
                    self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
                    self.logger.info(f"âœ… NumPy {numpy_version} (1.x) í˜¸í™˜ ë²„ì „")
        except Exception as e:
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            self.logger.warning(f"âš ï¸ NumPy ë²„ì „ ì²´í¬ ì‹¤íŒ¨: {e}")
    
    def _safe_device_setup(self, kwargs: Dict[str, Any]) -> str:
        """ğŸ”§ ì•ˆì „í•œ ë””ë°”ì´ìŠ¤ ì„¤ì • - ëª¨ë“  Step í´ë˜ìŠ¤ì™€ í˜¸í™˜ (2ë²ˆ íŒŒì¼ í†µí•©)"""
        try:
            # ğŸ”¥ ëª¨ë“  ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ íŒŒë¼ë¯¸í„° í™•ì¸ (2ë²ˆ íŒŒì¼ ê°œì„ ì‚¬í•­)
            device_from_kwargs = (
                kwargs.get('device') or 
                kwargs.get('preferred_device') or
                kwargs.get('target_device')
            )
            
            if device_from_kwargs and device_from_kwargs != "auto":
                return device_from_kwargs
            
            # ìë™ íƒì§€
            return self._auto_detect_device()
            
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return DEFAULT_DEVICE
    
    def _auto_detect_device(self, preferred_device: Optional[str] = None, device: Optional[str] = None) -> str:
        """
        ğŸ” í†µì¼ëœ ë””ë°”ì´ìŠ¤ ìë™ íƒì§€ ë©”ì„œë“œ (2ë²ˆ íŒŒì¼ì˜ í•µì‹¬ ê°œì„ ì‚¬í•­)
        
        ğŸ”¥ í•µì‹¬ í•´ê²°ì :
        - ëª¨ë“  Step í´ë˜ìŠ¤ì—ì„œ í˜¸ì¶œí•˜ëŠ” ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ í†µì¼
        - preferred_device, device íŒŒë¼ë¯¸í„° ëª¨ë‘ ì„ íƒì ìœ¼ë¡œ ì²˜ë¦¬  
        - missing positional argument ì™„ì „ í•´ê²°
        """
        try:
            # íŒŒë¼ë¯¸í„° ìš°ì„ ìˆœìœ„ ì²˜ë¦¬ (2ë²ˆ íŒŒì¼ ê°œì„ ì‚¬í•­)
            target_device = preferred_device or device
            
            if target_device and target_device != "auto":
                return target_device
                
            if not TORCH_AVAILABLE:
                return "cpu"
            
            # M3 Max MPS ì§€ì› í™•ì¸ (ìµœìš°ì„ )
            if MPS_AVAILABLE:
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
                
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ íƒì§€ ì‹¤íŒ¨: {e}")
            return "cpu"
    
    def _detect_device_type(self) -> str:
        """ë””ë°”ì´ìŠ¤ íƒ€ì… íƒì§€"""
        try:
            import platform
            system = platform.system()
            machine = platform.machine()
            
            if system == "Darwin" and ("arm64" in machine or "M3" in str(platform.processor())):
                return "apple_silicon"
            elif self.device == "cuda":
                return "nvidia_gpu"
            else:
                return "generic_cpu"
        except:
            return "unknown"
    
    def _detect_memory(self) -> float:
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ íƒì§€"""
        try:
            import psutil
            return round(psutil.virtual_memory().total / (1024**3), 1)
        except:
            return 16.0  # ê¸°ë³¸ê°’
    
    def _detect_m3_max(self) -> bool:
        """M3 Max íƒì§€"""
        try:
            import platform
            processor = str(platform.processor())
            return "M3" in processor or (self.device == "mps" and self.memory_gb > 64)
        except:
            return False
    
    def _calculate_optimal_batch_size(self) -> int:
        """ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        try:
            if self.is_m3_max and self.memory_gb >= 128:
                return 8  # M3 Max 128GB
            elif self.memory_gb >= 64:
                return 4  # 64GB+
            elif self.memory_gb >= 32:
                return 2  # 32GB+
            else:
                return 1  # 16GB ì´í•˜
        except:
            return 1
    
    def _setup_m3_max_optimization(self):
        """ğŸ M3 Max ìµœì í™” ì„¤ì • (2ë²ˆ íŒŒì¼ GPU íƒ€ì… ì¶©ëŒ í•´ê²° í¬í•¨)"""
        try:
            if self.device == "mps" and TORCH_AVAILABLE:
                # M3 Max íŠ¹í™” í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                # ğŸ”¥ M3 Max GPU íƒ€ì… ì¶©ëŒ í•´ê²° (2ë²ˆ íŒŒì¼ ê°œì„ ì‚¬í•­)
                if hasattr(torch.backends.mps, 'enable_fallback'):
                    torch.backends.mps.enable_fallback = True
                
                # OMP ìŠ¤ë ˆë“œ ìˆ˜ ì„¤ì • (M3 Max 16ì½”ì–´ í™œìš©)
                if self.is_m3_max:
                    os.environ['OMP_NUM_THREADS'] = '16'
                
                # MPS ìºì‹œ ì •ë¦¬
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                
                if hasattr(self, 'logger'):
                    self.logger.info("ğŸ M3 Max MPS ìµœì í™” ì„¤ì • ì™„ë£Œ")
                
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _setup_pytorch_optimization(self):
        """PyTorch ìµœì í™” ì„¤ì • (2ë²ˆ íŒŒì¼ GPU íƒ€ì… ì¶©ëŒ í•´ê²°)"""
        try:
            if not TORCH_AVAILABLE:
                return
            
            # ğŸ”¥ ë°ì´í„° íƒ€ì… í†µì¼ (GPU íƒ€ì… ì¶©ëŒ í•´ê²° - 2ë²ˆ íŒŒì¼ ê°œì„ ì‚¬í•­)
            if self.device == "mps":
                # M3 Maxì—ì„œ float32 ì‚¬ìš© (íƒ€ì… ì¶©ëŒ ë°©ì§€)
                self.dtype = torch.float32
                # fallback í™œì„±í™”
                if hasattr(torch.backends.mps, 'enable_fallback'):
                    torch.backends.mps.enable_fallback = True
            else:
                self.dtype = torch.float32
            
            # autograd ìµœì í™”
            torch.backends.cudnn.benchmark = True if self.device == "cuda" else False
            
            if hasattr(self, 'logger'):
                self.logger.debug(f"ğŸ”§ PyTorch ìµœì í™” ì„¤ì •: dtype={self.dtype}")
            
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ PyTorch ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
            self.dtype = torch.float32 if TORCH_AVAILABLE else None
    
    def _setup_warmup_functions(self):
        """
        ğŸ”¥ ì›Œë°ì—… í•¨ìˆ˜ë“¤ ì•ˆì „í•˜ê²Œ ì„¤ì • (dict callable ë¬¸ì œ ì™„ì „ í•´ê²°)
        
        âœ… 'dict' object is not callable ë¬¸ì œ í•´ê²°
        âœ… ì‹¤ì œ í•¨ìˆ˜ ê°ì²´ë¡œ ì„¤ì •í•˜ì—¬ í˜¸ì¶œ ê°€ëŠ¥ì„± ë³´ì¥
        """
        try:
            # ğŸ”¥ ì›Œë°ì—… í•¨ìˆ˜ë“¤ì„ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ì‹¤ì œ í•¨ìˆ˜ë¡œ ì„¤ì •
            self.warmup_functions = {
                'model_warmup': self._safe_model_warmup,
                'device_warmup': self._safe_device_warmup,
                'memory_warmup': self._safe_memory_warmup,
                'pipeline_warmup': self._safe_pipeline_warmup
            }
            
            # ì›Œë°ì—… ì„¤ì •
            self.warmup_config = SafeConfig({
                'enabled': True,
                'timeout': 30.0,
                'retry_count': 3,
                'warm_cache': True
            })
            
            if hasattr(self, 'logger'):
                self.logger.debug("ğŸ”¥ ì›Œë°ì—… í•¨ìˆ˜ë“¤ ì•ˆì „í•˜ê²Œ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ ì›Œë°ì—… í•¨ìˆ˜ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.warmup_functions = {}
            self.warmup_config = SafeConfig({})
    
    async def _safe_model_warmup(self) -> bool:
        """ğŸ”¥ ì•ˆì „í•œ ëª¨ë¸ ì›Œë°ì—…"""
        try:
            if hasattr(self, 'logger'):
                self.logger.info(f"ğŸ”¥ {self.step_name} ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")
            
            # ê¸°ë³¸ ì›Œë°ì—… ì‘ì—…
            if TORCH_AVAILABLE and self.device == "mps":
                # MPS ì›Œë°ì—… í…ì„œ ìƒì„±
                warmup_tensor = torch.randn(1, 3, 224, 224, device=self.device, dtype=self.dtype)
                _ = warmup_tensor * 2.0  # ê¸°ë³¸ ì—°ì‚° ìˆ˜í–‰
                del warmup_tensor
                
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            
            await asyncio.sleep(0.1)  # ì§§ì€ ëŒ€ê¸°
            if hasattr(self, 'logger'):
                self.logger.info(f"âœ… {self.step_name} ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            return True
            
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ {self.step_name} ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return False
    
    async def _safe_device_warmup(self) -> bool:
        """ğŸ”¥ ì•ˆì „í•œ ë””ë°”ì´ìŠ¤ ì›Œë°ì—…"""
        try:
            if hasattr(self, 'logger'):
                self.logger.debug(f"ğŸ”¥ {self.step_name} ë””ë°”ì´ìŠ¤ ì›Œë°ì—…...")
            
            if TORCH_AVAILABLE:
                # ë””ë°”ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
                test_tensor = torch.tensor([1.0], device=self.device, dtype=self.dtype)
                result = test_tensor + 1.0
                del test_tensor, result
            
            return True
            
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ {self.step_name} ë””ë°”ì´ìŠ¤ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return False
    
    async def _safe_memory_warmup(self) -> bool:
        """ğŸ”¥ ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì›Œë°ì—…"""
        try:
            if hasattr(self, 'logger'):
                self.logger.debug(f"ğŸ”¥ {self.step_name} ë©”ëª¨ë¦¬ ì›Œë°ì—…...")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            if TORCH_AVAILABLE and self.device == "mps":
                if hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
            
            return True
            
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ {self.step_name} ë©”ëª¨ë¦¬ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return False
    
    async def _safe_pipeline_warmup(self) -> bool:
        """ğŸ”¥ ì•ˆì „í•œ íŒŒì´í”„ë¼ì¸ ì›Œë°ì—…"""
        try:
            if hasattr(self, 'logger'):
                self.logger.debug(f"ğŸ”¥ {self.step_name} íŒŒì´í”„ë¼ì¸ ì›Œë°ì—…...")
            
            # ê¸°ë³¸ ì„¤ì • í™•ì¸
            if not hasattr(self, 'config') or not self.config:
                self.config = SafeConfig({})
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
            if not hasattr(self, 'performance_metrics'):
                self.performance_metrics = {}
            
            return True
            
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ {self.step_name} íŒŒì´í”„ë¼ì¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_step(self) -> bool:
        """ğŸš€ Step ì™„ì „ ì´ˆê¸°í™”"""
        try:
            if hasattr(self, 'logger'):
                self.logger.info(f"ğŸš€ {self.step_name} ì´ˆê¸°í™” ì‹œì‘...")
            
            # ê¸°ë³¸ ì´ˆê¸°í™” í™•ì¸
            if not hasattr(self, 'logger'):
                self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            
            if not hasattr(self, 'device'):
                self.device = self._auto_detect_device()
            
            # ì›Œë°ì—… ì‹¤í–‰ (ì•ˆì „í•˜ê²Œ)
            await self._execute_safe_warmup()
            
            # ì»¤ìŠ¤í…€ ì´ˆê¸°í™” í˜¸ì¶œ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
            if hasattr(self, '_custom_initialize'):
                await self._custom_initialize()
            
            self.is_initialized = True
            if hasattr(self, 'logger'):
                self.logger.info(f"âœ… {self.step_name} ì™„ì „ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.logger.error(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            self.last_error = str(e)
            self.error_count += 1
            return False
    
    async def _execute_safe_warmup(self):
        """ğŸ”¥ ì•ˆì „í•œ ì›Œë°ì—… ì‹¤í–‰ (dict callable ë¬¸ì œ í•´ê²°)"""
        try:
            if not hasattr(self, 'warmup_functions') or not self.warmup_functions:
                return
            
            # ğŸ”¥ ê° ì›Œë°ì—… í•¨ìˆ˜ ì•ˆì „í•˜ê²Œ ì‹¤í–‰ (callable ì²´í¬ ê°•í™”)
            for warmup_name, warmup_func in self.warmup_functions.items():
                try:
                    if callable(warmup_func):  # í˜¸ì¶œ ê°€ëŠ¥í•œì§€ í™•ì¸
                        await warmup_func()
                    else:
                        if hasattr(self, 'logger'):
                            self.logger.warning(f"âš ï¸ {warmup_name}ì´ callableì´ ì•„ë‹˜: {type(warmup_func)}")
                except Exception as e:
                    if hasattr(self, 'logger'):
                        self.logger.warning(f"âš ï¸ {warmup_name} ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ ì›Œë°ì—… ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    def get_step_info(self) -> Dict[str, Any]:
        """ğŸ“‹ Step ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'step_number': getattr(self, 'step_number', 0),
            'step_type': getattr(self, 'step_type', 'unknown'),
            'device': self.device,
            'device_type': self.device_type,
            'memory_gb': self.memory_gb,
            'is_m3_max': self.is_m3_max,
            'optimization_enabled': self.optimization_enabled,
            'quality_level': self.quality_level,
            'batch_size': self.batch_size,
            'is_initialized': self.is_initialized,
            'has_model_interface': self.model_interface is not None,
            'has_model_loader': self.model_loader is not None,
            'last_processing_time': self.last_processing_time,
            'total_processing_count': self.total_processing_count,
            'error_count': self.error_count,
            'last_error': self.last_error,
            'cache_dir': str(self.cache_dir),
            'config_keys': list(self.config.keys()) if hasattr(self.config, 'keys') else [],
            'performance_metrics': self.performance_metrics,
            'torch_available': TORCH_AVAILABLE,
            'mps_available': MPS_AVAILABLE,
            'numpy_available': NUMPY_AVAILABLE,
            'numpy_version': np.__version__ if NUMPY_AVAILABLE else 'N/A',
            'dtype': str(getattr(self, 'dtype', 'None')),
            'warmup_functions': list(getattr(self, 'warmup_functions', {}).keys())
        }
    
    def cleanup_models(self):
        """ğŸ§¹ ëª¨ë¸ ì •ë¦¬"""
        try:
            if hasattr(self, 'model_interface') and self.model_interface:
                # ğŸ”¥ ì•ˆì „í•œ ëª¨ë¸ ì •ë¦¬ (callable ì²´í¬)
                cleanup_func = getattr(self.model_interface, 'unload_models', None)
                if callable(cleanup_func):
                    cleanup_func()
                else:
                    if hasattr(self, 'logger'):
                        self.logger.warning("âš ï¸ unload_modelsê°€ callableì´ ì•„ë‹˜")
                
                if hasattr(self, 'logger'):
                    self.logger.info(f"ğŸ§¹ {self.step_name} ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "mps" and hasattr(torch.backends.mps, 'empty_cache'):
                    torch.backends.mps.empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
                
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                gc.collect()
                
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.warning(f"âš ï¸ {self.step_name} ëª¨ë¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.cleanup_models()
        except:
            pass

# ==============================================
# ğŸ”¥ Stepë³„ íŠ¹í™” Mixinë“¤ (ëª¨ë“  ì˜¤ë¥˜ ìˆ˜ì •ë¨)
# ==============================================

class HumanParsingMixin(BaseStepMixin):
    """Step 1: Human Parsing íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 1
        self.step_type = "human_parsing"
        self.num_classes = 20
        self.output_format = "segmentation_mask"

class PoseEstimationMixin(BaseStepMixin):
    """Step 2: Pose Estimation íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 2
        self.step_type = "pose_estimation"
        self.num_keypoints = 18
        self.output_format = "keypoints_heatmap"

class ClothSegmentationMixin(BaseStepMixin):
    """Step 3: Cloth Segmentation íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 3
        self.step_type = "cloth_segmentation"
        self.binary_output = True
        self.output_format = "binary_mask"

class GeometricMatchingMixin(BaseStepMixin):
    """Step 4: Geometric Matching íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 4
        self.step_type = "geometric_matching"
        self.num_control_points = 25
        self.output_format = "transformation_matrix"

class ClothWarpingMixin(BaseStepMixin):
    """Step 5: Cloth Warping íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 5
        self.step_type = "cloth_warping"
        self.enable_physics = True
        self.output_format = "warped_cloth"

class VirtualFittingMixin(BaseStepMixin):
    """Step 6: Virtual Fitting íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 6
        self.step_type = "virtual_fitting"
        self.diffusion_steps = 50
        self.output_format = "rgb_image"

class PostProcessingMixin(BaseStepMixin):
    """Step 7: Post Processing íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 7
        self.step_type = "post_processing"
        self.upscale_factor = 2
        self.output_format = "enhanced_image"

class QualityAssessmentMixin(BaseStepMixin):
    """Step 8: Quality Assessment íŠ¹í™” Mixin"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.step_number = 8
        self.step_type = "quality_assessment"
        self.quality_threshold = 0.7
        self.output_format = "quality_scores"

# ==============================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° ë°ì½”ë ˆì´í„° ë° í—¬í¼ í•¨ìˆ˜ë“¤
# ==============================================

def ensure_step_initialization(func: Callable) -> Callable:
    """Step í´ë˜ìŠ¤ ì´ˆê¸°í™” ë³´ì¥ ë°ì½”ë ˆì´í„°"""
    async def wrapper(self, *args, **kwargs):
        # logger ì†ì„± í™•ì¸ ë° ì„¤ì •
        if not hasattr(self, 'logger'):
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        # BaseStepMixin ì´ˆê¸°í™” í™•ì¸
        if not hasattr(self, 'is_initialized') or not self.is_initialized:
            await self.initialize_step()
        
        return await func(self, *args, **kwargs)
    return wrapper

def safe_step_method(func: Callable) -> Callable:
    """Step ë©”ì„œë“œ ì•ˆì „ ì‹¤í–‰ ë°ì½”ë ˆì´í„°"""
    async def wrapper(self, *args, **kwargs):
        try:
            # logger í™•ì¸
            if not hasattr(self, 'logger'):
                self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            
            return await func(self, *args, **kwargs)
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.error(f"âŒ {func.__name__} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                if hasattr(self, 'error_count'):
                    self.error_count += 1
                if hasattr(self, 'last_error'):
                    self.last_error = str(e)
            
            # ê¸°ë³¸ ì—ëŸ¬ ì‘ë‹µ ë°˜í™˜
            return {
                'success': False,
                'error': str(e),
                'step_name': getattr(self, 'step_name', self.__class__.__name__),
                'method_name': func.__name__
            }
    return wrapper

def performance_monitor(operation_name: str) -> Callable:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable) -> Callable:
        async def wrapper(self, *args, **kwargs):
            start_time = time.time()
            success = True
            try:
                result = await func(self, *args, **kwargs)
                return result
            except Exception as e:
                success = False
                raise e
            finally:
                duration = time.time() - start_time
                if hasattr(self, 'record_performance'):
                    self.record_performance(operation_name, duration, success)
        return wrapper
    return decorator

def memory_optimize(func: Callable) -> Callable:
    """ë©”ëª¨ë¦¬ ìµœì í™” ë°ì½”ë ˆì´í„°"""
    async def wrapper(self, *args, **kwargs):
        try:
            result = await func(self, *args, **kwargs)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if hasattr(self, 'device'):
                    if self.device == "mps" and hasattr(torch.backends.mps, 'empty_cache'):
                        torch.backends.mps.empty_cache()
                    elif self.device == "cuda":
                        torch.cuda.empty_cache()
                
                gc.collect()
            
            return result
        except Exception as e:
            # ì˜¤ë¥˜ ì‹œì—ë„ ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                gc.collect()
            raise e
    return wrapper

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    # ê¸°ë³¸ í´ë˜ìŠ¤
    'SafeConfig',
    'BaseStepMixin',
    
    # Stepë³„ íŠ¹í™” Mixinë“¤
    'HumanParsingMixin',
    'PoseEstimationMixin', 
    'ClothSegmentationMixin',
    'GeometricMatchingMixin',
    'ClothWarpingMixin',
    'VirtualFittingMixin',
    'PostProcessingMixin',
    'QualityAssessmentMixin',
    
    # ìœ í‹¸ë¦¬í‹° ë°ì½”ë ˆì´í„°
    'ensure_step_initialization',
    'safe_step_method',
    'performance_monitor',
    'memory_optimize',
    
    # ìƒìˆ˜
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'CV_AVAILABLE',
    'NUMPY_AVAILABLE',
    'DEFAULT_DEVICE'
]

# ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê·¸
logger.info("âœ… BaseStepMixin v4.1 ì™„ì „ í†µí•© í•´ê²° ë²„ì „ ë¡œë“œ ì™„ë£Œ")
logger.info("ğŸ”— 1ë²ˆ+2ë²ˆ íŒŒì¼ ëª¨ë“  ì¥ì  í†µí•©")
logger.info("ğŸ”¥ ëª¨ë“  í˜¸ì¶œ ì˜¤ë¥˜ ì™„ì „ í•´ê²°")
logger.info("ğŸ M3 Max 128GB ìµœì í™” ì§€ì›")
logger.info("ğŸ conda í™˜ê²½ ì™„ë²½ ì§€ì›")
logger.info(f"ğŸ”§ PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}, MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"ğŸ”¢ NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'} v{np.__version__ if NUMPY_AVAILABLE else 'N/A'}")
logger.info(f"ğŸ¯ ê¸°ë³¸ ë””ë°”ì´ìŠ¤: {DEFAULT_DEVICE}")

if NUMPY_AVAILABLE and int(np.__version__.split('.')[0]) >= 2:
    logger.warning("âš ï¸ NumPy 2.x ê°ì§€ë¨ - conda install numpy=1.24.4 ê¶Œì¥")
else:
    logger.info("âœ… NumPy í˜¸í™˜ì„± í™•ì¸ë¨")