# backend/app/ai_pipeline/steps/base_step_mixin.py
"""
ğŸ”¥ BaseStepMixin v13.0 - ê°„ì†Œí™”ëœ ì™„ë²½í•œ êµ¬í˜„
==================================================

âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ì œê±° (ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´)
âœ… 3ë‹¨ê³„ ê°„ë‹¨í•œ ì´ˆê¸°í™” (17ë‹¨ê³„ â†’ 3ë‹¨ê³„)
âœ… ëª¨ë“  Step íŒŒì¼ì´ ìš”êµ¬í•˜ëŠ” ê¸°ëŠ¥ ì™„ì „ ì œê³µ
âœ… ModelLoader ì—°ë™ (89.8GB ì²´í¬í¬ì¸íŠ¸ í™œìš©)
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›
âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²°
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ê¹”ë”í•œ ì•„í‚¤í…ì²˜

í•µì‹¬ ì² í•™:
- ê°„ë‹¨í•¨ì´ ìµœê³ ë‹¤ (Simplicity is Best)
- ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
- Stepë“¤ì´ ì›í•˜ëŠ” ê²ƒë§Œ ì œê³µ
- ë³µì¡í•œ ê¸°ëŠ¥ì€ ì œê±°

Author: MyCloset AI Team
Date: 2025-07-22
Version: 13.0 (Simplified Perfect Implementation)
"""

# ==============================================
# ğŸ”¥ 1. í•„ìˆ˜ importë§Œ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================
import os
import gc
import time
import asyncio
import logging
import threading
import traceback
import weakref
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple
from abc import ABC, abstractmethod
from dataclasses import dataclass
from functools import wraps
import platform
import subprocess
import psutil
from datetime import datetime

# ==============================================
# ğŸ”¥ 2. conda í™˜ê²½ ìš°ì„  ì²´í¬
# ==============================================
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'python_path': os.path.dirname(os.__file__)
}

if CONDA_INFO['conda_env'] != 'none':
    print(f"âœ… conda í™˜ê²½ ê°ì§€: {CONDA_INFO['conda_env']}")
else:
    print("âš ï¸ conda í™˜ê²½ ê¶Œì¥: conda activate mycloset-ai")

# ==============================================
# ğŸ”¥ 3. ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import
# ==============================================

# PyTorch ì•ˆì „ Import (MPS í´ë°± ì„¤ì •)
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        print("ğŸ M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
    
except ImportError:
    print("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch torchvision torchaudio -c pytorch")

# NumPy ì•ˆì „ Import
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    print("âš ï¸ NumPy ì—†ìŒ - conda install numpy")

# PIL ì•ˆì „ Import
PIL_AVAILABLE = False
try:
    from PIL import Image, ImageEnhance, ImageFilter
    PIL_AVAILABLE = True
except ImportError:
    print("âš ï¸ PIL ì—†ìŒ - conda install pillow")

# ==============================================
# ğŸ”¥ 4. ê°„ë‹¨í•œ ì„¤ì • í´ë˜ìŠ¤
# ==============================================
@dataclass
class StepConfig:
    """ê°„ë‹¨í•œ Step ì„¤ì •"""
    step_name: str = "BaseStep"
    step_id: int = 0
    device: str = "auto"
    use_fp16: bool = True
    batch_size: int = 1
    confidence_threshold: float = 0.8
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True

# ==============================================
# ğŸ”¥ 5. ë©”ëª¨ë¦¬ ìµœì í™” í´ë˜ìŠ¤
# ==============================================
class SimpleMemoryOptimizer:
    """ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìµœì í™”"""
    
    def __init__(self, device: str = "mps"):
        self.device = device
        self.is_m3_max = self._detect_m3_max()
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def optimize(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰"""
        try:
            # Python GC
            before = len(gc.get_objects())
            gc.collect()
            after = len(gc.get_objects())
            
            results = [f"Python GC: {before - after}ê°œ ê°ì²´ í•´ì œ"]
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    results.append("CUDA ìºì‹œ ì •ë¦¬")
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        # MPS ìºì‹œ ì •ë¦¬
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        results.append("MPS ìºì‹œ ì •ë¦¬")
                    except Exception:
                        results.append("MPS ìºì‹œ ì •ë¦¬ ì‹œë„")
            
            # M3 Max íŠ¹ë³„ ìµœì í™”
            if self.is_m3_max and aggressive:
                for _ in range(3):
                    gc.collect()
                results.append("M3 Max í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”")
            
            return {
                "success": True,
                "results": results,
                "device": self.device,
                "is_m3_max": self.is_m3_max
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def optimize_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, lambda: self.optimize(aggressive))
        except Exception as e:
            return {"success": False, "error": str(e)}

# ==============================================
# ğŸ”¥ 6. ë©”ì¸ BaseStepMixin í´ë˜ìŠ¤
# ==============================================
class BaseStepMixin:
    """
    ğŸ”¥ BaseStepMixin v13.0 - ê°„ì†Œí™”ëœ ì™„ë²½í•œ êµ¬í˜„
    
    âœ… 3ë‹¨ê³„ ê°„ë‹¨í•œ ì´ˆê¸°í™”
    âœ… ëª¨ë“  Stepì´ ìš”êµ¬í•˜ëŠ” ê¸°ëŠ¥ ì œê³µ
    âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ (ì˜ì¡´ì„± ì£¼ì…)
    âœ… M3 Max ìµœì í™”
    âœ… conda í™˜ê²½ ìš°ì„ 
    âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ì§€ì›
    """
    
    def __init__(self, **kwargs):
        """3ë‹¨ê³„ ê°„ë‹¨í•œ ì´ˆê¸°í™”"""
        try:
            # STEP 1: ê¸°ë³¸ ì„¤ì •
            self._setup_basic(**kwargs)
            
            # STEP 2: ì‹œìŠ¤í…œ ì„¤ì •
            self._setup_system()
            
            # STEP 3: ì™„ë£Œ
            self._finalize()
            
            self.logger.info(f"âœ… {self.step_name} BaseStepMixin v13.0 ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self._emergency_setup(e)
    
    def _setup_basic(self, **kwargs):
        """STEP 1: ê¸°ë³¸ ì„¤ì •"""
        # ì„¤ì •
        self.config = StepConfig()
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
        
        # ê¸°ë³¸ ì†ì„±
        self.step_name = kwargs.get('step_name', self.__class__.__name__)
        self.step_id = kwargs.get('step_id', 0)
        
        # Logger ì„¤ì • (Stepë“¤ì´ í•„ìˆ˜ë¡œ ìš”êµ¬)
        self.logger = logging.getLogger(f"pipeline.steps.{self.step_name}")
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
        
        # ìƒíƒœ í”Œë˜ê·¸ë“¤ (Stepë“¤ì´ ì²´í¬í•˜ëŠ” ì†ì„±ë“¤)
        self.is_initialized = False
        self.is_ready = False
        self.has_model = False
        self.model_loaded = False
        self.warmup_completed = False
        
        # ì˜ì¡´ì„± ì£¼ì…ì„ ìœ„í•œ ì†ì„±ë“¤ (ë‚˜ì¤‘ì— ì£¼ì…ë°›ìŒ)
        self.model_loader = None
        self.memory_manager = None
        self.data_converter = None
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_metrics = {
            'process_count': 0,
            'total_process_time': 0.0,
            'average_process_time': 0.0,
            'error_history': []
        }
        
        # ì—ëŸ¬ ì¶”ì 
        self.error_count = 0
        self.last_error = None
        self.total_processing_count = 0
        self.last_processing_time = None
    
    def _setup_system(self):
        """STEP 2: ì‹œìŠ¤í…œ ì„¤ì •"""
        # ë””ë°”ì´ìŠ¤ ê°ì§€
        if self.config.device == "auto":
            self.device = self._detect_optimal_device()
        else:
            self.device = self.config.device
        
        # M3 Max ê°ì§€
        self.is_m3_max = self._detect_m3_max()
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        self.memory_gb = self._get_memory_info()
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ
        self.memory_optimizer = SimpleMemoryOptimizer(self.device)
        
        # ëª¨ë¸ ìºì‹œ (Stepë“¤ì´ ì‚¬ìš©)
        self.model_cache = {}
        self.loaded_models = {}
        
        # í˜„ì¬ ëª¨ë¸ (Stepë“¤ì´ ì ‘ê·¼)
        self._ai_model = None
        self._ai_model_name = None
    
    def _finalize(self):
        """STEP 3: ì™„ë£Œ"""
        self.is_initialized = True
        
        # ìë™ ì›Œë°ì—… (ì„¤ì •ëœ ê²½ìš°)
        if self.config.auto_warmup:
            try:
                warmup_result = self.warmup()
                if warmup_result.get('success', False):
                    self.warmup_completed = True
                    self.is_ready = True
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìë™ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    def _emergency_setup(self, error: Exception):
        """ê¸´ê¸‰ ì„¤ì •"""
        self.step_name = getattr(self, 'step_name', self.__class__.__name__)
        self.logger = logging.getLogger("emergency")
        self.device = "cpu"
        self.is_initialized = False
        self.error_count = 1
        self.last_error = str(error)
        print(f"ğŸš¨ {self.step_name} ê¸´ê¸‰ ì´ˆê¸°í™”: {error}")
    
    # ==============================================
    # ğŸ”¥ 7. ì‹œìŠ¤í…œ ê°ì§€ ë©”ì„œë“œë“¤
    # ==============================================
    
    def _detect_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            if TORCH_AVAILABLE:
                if MPS_AVAILABLE:
                    return "mps"
                elif hasattr(torch, 'cuda') and torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        except:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def _get_memory_info(self) -> float:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        try:
            memory = psutil.virtual_memory()
            return memory.total / 1024**3
        except:
            return 16.0
    
    # ==============================================
    # ğŸ”¥ 8. ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (ìˆœí™˜ì°¸ì¡° ë°©ì§€)"""
        self.model_loader = model_loader
        self.logger.info("âœ… ModelLoader ì£¼ì… ì™„ë£Œ")
    
    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        self.memory_manager = memory_manager
        self.logger.info("âœ… MemoryManager ì£¼ì… ì™„ë£Œ")
    
    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        self.data_converter = data_converter
        self.logger.info("âœ… DataConverter ì£¼ì… ì™„ë£Œ")
    
    # ==============================================
    # ğŸ”¥ 9. Stepë“¤ì´ ìš”êµ¬í•˜ëŠ” í•µì‹¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ë™ê¸°) - Stepë“¤ì´ í•„ìˆ˜ë¡œ ì‚¬ìš©"""
        try:
            # ìºì‹œ í™•ì¸
            cache_key = model_name or "default"
            if cache_key in self.model_cache:
                return self.model_cache[cache_key]
            
            # ModelLoaderë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ (ì˜ì¡´ì„± ì£¼ì…ëœ ê²½ìš°)
            if self.model_loader:
                try:
                    model = self.model_loader.get_model(model_name or "default")
                    if model:
                        self.model_cache[cache_key] = model
                        self.has_model = True
                        self.model_loaded = True
                        self._ai_model = model
                        self._ai_model_name = model_name
                        return model
                except Exception as e:
                    self.logger.debug(f"ModelLoaderë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ì§ì ‘ ëª¨ë¸ ë¡œë” import ì‹œë„ (ì•ˆì „í•œ ë°©í–¥)
            try:
                # ë™ì  importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
                import importlib
                loader_module = importlib.import_module('app.ai_pipeline.utils.model_loader')
                get_global_loader = getattr(loader_module, 'get_global_model_loader', None)
                if get_global_loader:
                    loader = get_global_loader()
                    if loader:
                        model = loader.get_model(model_name or "default")
                        if model:
                            self.model_cache[cache_key] = model
                            self.has_model = True
                            self.model_loaded = True
                            return model
            except Exception as e:
                self.logger.debug(f"í´ë°± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            return None
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸°) - Stepë“¤ì´ ì‚¬ìš©"""
        try:
            # ìºì‹œ í™•ì¸
            cache_key = model_name or "default"
            if cache_key in self.model_cache:
                return self.model_cache[cache_key]
            
            # ë¹„ë™ê¸° ModelLoader ì‚¬ìš©
            if self.model_loader:
                try:
                    if hasattr(self.model_loader, 'get_model_async'):
                        model = await self.model_loader.get_model_async(model_name or "default")
                    else:
                        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                        loop = asyncio.get_event_loop()
                        model = await loop.run_in_executor(
                            None, 
                            lambda: self.model_loader.get_model(model_name or "default")
                        )
                    
                    if model:
                        self.model_cache[cache_key] = model
                        self.has_model = True
                        self.model_loaded = True
                        return model
                        
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° ModelLoader ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, lambda: self.get_model(model_name))
                
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (ë™ê¸°) - Stepë“¤ì´ ì‚¬ìš©"""
        try:
            # ì˜ì¡´ì„± ì£¼ì…ëœ MemoryManager ì‚¬ìš©
            if self.memory_manager:
                try:
                    return self.memory_manager.optimize_memory(aggressive=aggressive)
                except Exception as e:
                    self.logger.debug(f"MemoryManager ì‹¤íŒ¨: {e}")
            
            # ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” ì‚¬ìš©
            return self.memory_optimizer.optimize(aggressive=aggressive)
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def optimize_memory_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (ë¹„ë™ê¸°) - Stepë“¤ì´ ì‚¬ìš©"""
        try:
            # ì˜ì¡´ì„± ì£¼ì…ëœ MemoryManager ì‚¬ìš©
            if self.memory_manager:
                try:
                    if hasattr(self.memory_manager, 'optimize_memory_async'):
                        return await self.memory_manager.optimize_memory_async(aggressive=aggressive)
                    else:
                        loop = asyncio.get_event_loop()
                        return await loop.run_in_executor(
                            None, 
                            lambda: self.memory_manager.optimize_memory(aggressive=aggressive)
                        )
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° MemoryManager ì‹¤íŒ¨: {e}")
            
            # ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” ì‚¬ìš©
            return await self.memory_optimizer.optimize_async(aggressive=aggressive)
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def warmup(self) -> Dict[str, Any]:
        """ì›Œë°ì—… ì‹¤í–‰ (ë™ê¸°) - Stepë“¤ì´ ì‚¬ìš©"""
        try:
            if self.warmup_completed:
                return {'success': True, 'message': 'ì´ë¯¸ ì›Œë°ì—… ì™„ë£Œë¨', 'cached': True}
            
            self.logger.info(f"ğŸ”¥ {self.step_name} ì›Œë°ì—… ì‹œì‘...")
            start_time = time.time()
            results = []
            
            # 1. ë©”ëª¨ë¦¬ ì›Œë°ì—…
            try:
                memory_result = self.optimize_memory()
                results.append('memory_success' if memory_result.get('success') else 'memory_failed')
            except:
                results.append('memory_failed')
            
            # 2. ëª¨ë¸ ì›Œë°ì—… (ìˆëŠ” ê²½ìš°)
            try:
                if self.model_loader:
                    test_model = self.get_model("warmup_test")
                    results.append('model_success' if test_model else 'model_skipped')
                else:
                    results.append('model_skipped')
            except:
                results.append('model_failed')
            
            # 3. ë””ë°”ì´ìŠ¤ ì›Œë°ì—…
            try:
                if TORCH_AVAILABLE:
                    test_tensor = torch.randn(10, 10)
                    if self.device != 'cpu':
                        test_tensor = test_tensor.to(self.device)
                    _ = torch.matmul(test_tensor, test_tensor.t())
                    results.append('device_success')
                else:
                    results.append('device_skipped')
            except:
                results.append('device_failed')
            
            duration = time.time() - start_time
            success_count = sum(1 for r in results if 'success' in r)
            overall_success = success_count > 0
            
            if overall_success:
                self.warmup_completed = True
                self.is_ready = True
            
            self.logger.info(f"ğŸ”¥ ì›Œë°ì—… ì™„ë£Œ: {success_count}/{len(results)} ì„±ê³µ ({duration:.2f}ì´ˆ)")
            
            return {
                "success": overall_success,
                "duration": duration,
                "results": results,
                "success_count": success_count,
                "total_count": len(results)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def warmup_async(self) -> Dict[str, Any]:
        """ì›Œë°ì—… ì‹¤í–‰ (ë¹„ë™ê¸°) - Stepë“¤ì´ ì‚¬ìš©"""
        try:
            if self.warmup_completed:
                return {'success': True, 'message': 'ì´ë¯¸ ì›Œë°ì—… ì™„ë£Œë¨', 'cached': True}
            
            self.logger.info(f"ğŸ”¥ {self.step_name} ë¹„ë™ê¸° ì›Œë°ì—… ì‹œì‘...")
            start_time = time.time()
            results = []
            
            # 1. ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì›Œë°ì—…
            try:
                memory_result = await self.optimize_memory_async()
                results.append('memory_async_success' if memory_result.get('success') else 'memory_async_failed')
            except:
                results.append('memory_async_failed')
            
            # 2. ë¹„ë™ê¸° ëª¨ë¸ ì›Œë°ì—…
            try:
                if self.model_loader:
                    test_model = await self.get_model_async("warmup_test")
                    results.append('model_async_success' if test_model else 'model_async_skipped')
                else:
                    results.append('model_async_skipped')
            except:
                results.append('model_async_failed')
            
            # 3. ë¹„ë™ê¸° ë””ë°”ì´ìŠ¤ ì›Œë°ì—…
            try:
                if TORCH_AVAILABLE:
                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(None, self._device_warmup_sync)
                    results.append('device_async_success')
                else:
                    results.append('device_async_skipped')
            except:
                results.append('device_async_failed')
            
            duration = time.time() - start_time
            success_count = sum(1 for r in results if 'success' in r)
            overall_success = success_count > 0
            
            if overall_success:
                self.warmup_completed = True
                self.is_ready = True
            
            self.logger.info(f"ğŸ”¥ ë¹„ë™ê¸° ì›Œë°ì—… ì™„ë£Œ: {success_count}/{len(results)} ì„±ê³µ ({duration:.2f}ì´ˆ)")
            
            return {
                "success": overall_success,
                "duration": duration,
                "results": results,
                "success_count": success_count,
                "total_count": len(results),
                "async": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "async": True}
    
    def _device_warmup_sync(self):
        """ë™ê¸° ë””ë°”ì´ìŠ¤ ì›Œë°ì—…"""
        try:
            test_tensor = torch.randn(10, 10)
            if self.device != 'cpu':
                test_tensor = test_tensor.to(self.device)
            _ = torch.matmul(test_tensor, test_tensor.t())
            return True
        except:
            return False
    
    # BaseStepMixin í˜¸í™˜ìš© ë³„ì¹­
    async def warmup_step(self) -> Dict[str, Any]:
        """Step ì›Œë°ì—… (BaseStepMixin í˜¸í™˜ìš©)"""
        return await self.warmup_async()
    
    def initialize(self) -> bool:
        """ì´ˆê¸°í™” ë©”ì„œë“œ - Stepë“¤ì´ ì‚¬ìš©"""
        try:
            if self.is_initialized:
                return True
            
            self.is_initialized = True
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_async(self) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™” ë©”ì„œë“œ"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.initialize)
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def cleanup(self) -> Dict[str, Any]:
        """ì •ë¦¬ (ë¹„ë™ê¸°) - Stepë“¤ì´ ì‚¬ìš©"""
        try:
            self.logger.info(f"ğŸ§¹ {self.step_name} ì •ë¦¬ ì‹œì‘...")
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            self.model_cache.clear()
            self.loaded_models.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_result = await self.optimize_memory_async(aggressive=True)
            
            # ìƒíƒœ ë¦¬ì…‹
            self.is_ready = False
            self.warmup_completed = False
            
            self.logger.info(f"âœ… {self.step_name} ì •ë¦¬ ì™„ë£Œ")
            
            return {
                "success": True,
                "cleanup_result": cleanup_result,
                "step_name": self.step_name
            }
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def cleanup_models(self):
        """ëª¨ë¸ ì •ë¦¬ - Stepë“¤ì´ ì‚¬ìš©"""
        try:
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            self.model_cache.clear()
            self.loaded_models.clear()
            
            # í˜„ì¬ ëª¨ë¸ ì´ˆê¸°í™”
            self._ai_model = None
            self._ai_model_name = None
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                    except:
                        pass
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
                
                gc.collect()
            
            self.has_model = False
            self.model_loaded = False
            self.logger.info(f"ğŸ§¹ {self.step_name} ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """Step ìƒíƒœ ì¡°íšŒ - Stepë“¤ì´ ì‚¬ìš©"""
        try:
            return {
                'step_name': self.step_name,
                'step_id': getattr(self, 'step_id', 0),
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'has_model': self.has_model,
                'model_loaded': self.model_loaded,
                'warmup_completed': self.warmup_completed,
                'device': self.device,
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb,
                'error_count': self.error_count,
                'last_error': self.last_error,
                'total_processing_count': self.total_processing_count,
                'dependencies': {
                    'model_loader': self.model_loader is not None,
                    'memory_manager': self.memory_manager is not None,
                    'data_converter': self.data_converter is not None,
                },
                'performance_metrics': self.performance_metrics,
                'conda_info': CONDA_INFO,
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'step_name': getattr(self, 'step_name', 'unknown'),
                'error': str(e),
                'timestamp': time.time()
            }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ - Stepë“¤ì´ ì‚¬ìš©"""
        try:
            return {
                'total_processing_count': self.total_processing_count,
                'last_processing_time': self.last_processing_time,
                'error_count': self.error_count,
                'success_rate': self._calculate_success_rate(),
                'average_process_time': self.performance_metrics.get('average_process_time', 0.0),
                'total_process_time': self.performance_metrics.get('total_process_time', 0.0)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_success_rate(self) -> float:
        """ì„±ê³µë¥  ê³„ì‚°"""
        try:
            total = self.total_processing_count
            errors = self.error_count
            if total > 0:
                return (total - errors) / total
            return 0.0
        except:
            return 0.0
    
    # ==============================================
    # ğŸ”¥ 10. ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def record_processing(self, duration: float, success: bool = True):
        """ì²˜ë¦¬ ê¸°ë¡"""
        try:
            self.total_processing_count += 1
            self.last_processing_time = time.time()
            
            if not success:
                self.error_count += 1
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.performance_metrics['process_count'] = self.total_processing_count
            self.performance_metrics['total_process_time'] += duration
            self.performance_metrics['average_process_time'] = (
                self.performance_metrics['total_process_time'] / self.total_processing_count
            )
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²˜ë¦¬ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì (ì•ˆì „í•œ ì •ë¦¬)"""
        try:
            if hasattr(self, 'model_cache'):
                self.model_cache.clear()
        except:
            pass

# ==============================================
# ğŸ”¥ 11. Stepë³„ íŠ¹í™” Mixinë“¤ (8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸)
# ==============================================

class HumanParsingMixin(BaseStepMixin):
    """Step 1: Human Parsing íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'HumanParsingStep')
        kwargs.setdefault('step_id', 1)
        super().__init__(**kwargs)
        
        self.num_classes = 20
        self.parsing_categories = [
            'background', 'hat', 'hair', 'glove', 'sunglasses', 'upperclothes',
            'dress', 'coat', 'socks', 'pants', 'jumpsuits', 'scarf', 'skirt',
            'face', 'left_arm', 'right_arm', 'left_leg', 'right_leg', 'left_shoe', 'right_shoe'
        ]

class PoseEstimationMixin(BaseStepMixin):
    """Step 2: Pose Estimation íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'PoseEstimationStep')
        kwargs.setdefault('step_id', 2)
        super().__init__(**kwargs)
        
        self.num_keypoints = 18
        self.keypoint_names = [
            'nose', 'neck', 'right_shoulder', 'right_elbow', 'right_wrist',
            'left_shoulder', 'left_elbow', 'left_wrist', 'right_hip', 'right_knee',
            'right_ankle', 'left_hip', 'left_knee', 'left_ankle', 'right_eye',
            'left_eye', 'right_ear', 'left_ear'
        ]

class ClothSegmentationMixin(BaseStepMixin):
    """Step 3: Cloth Segmentation íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'ClothSegmentationStep')
        kwargs.setdefault('step_id', 3)
        super().__init__(**kwargs)
        
        self.segmentation_methods = ['traditional', 'u2net', 'deeplab', 'auto', 'hybrid']
        self.segmentation_method = kwargs.get('segmentation_method', 'u2net')

class GeometricMatchingMixin(BaseStepMixin):
    """Step 4: Geometric Matching íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'GeometricMatchingStep')
        kwargs.setdefault('step_id', 4)
        super().__init__(**kwargs)
        
        self.matching_methods = ['thin_plate_spline', 'affine', 'perspective', 'flow_based']
        self.matching_method = kwargs.get('matching_method', 'thin_plate_spline')
        self.grid_size = kwargs.get('grid_size', (5, 5))

class ClothWarpingMixin(BaseStepMixin):
    """Step 5: Cloth Warping íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'ClothWarpingStep')
        kwargs.setdefault('step_id', 5)
        super().__init__(**kwargs)
        
        self.warping_stages = ['preprocessing', 'geometric_transformation', 'texture_mapping', 'postprocessing']
        self.warping_quality = kwargs.get('warping_quality', 'high')
        self.preserve_texture = kwargs.get('preserve_texture', True)

class VirtualFittingMixin(BaseStepMixin):
    """Step 6: Virtual Fitting íŠ¹í™” Mixin (í•µì‹¬ ë‹¨ê³„)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'VirtualFittingStep')
        kwargs.setdefault('step_id', 6)
        super().__init__(**kwargs)
        
        self.fitting_modes = ['standard', 'high_quality', 'fast', 'experimental']
        self.fitting_mode = kwargs.get('fitting_mode', 'high_quality')
        self.diffusion_steps = kwargs.get('diffusion_steps', 50)
        self.guidance_scale = kwargs.get('guidance_scale', 7.5)
        self.use_ootd = kwargs.get('use_ootd', True)

class PostProcessingMixin(BaseStepMixin):
    """Step 7: Post Processing íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'PostProcessingStep')
        kwargs.setdefault('step_id', 7)
        super().__init__(**kwargs)
        
        self.processing_methods = ['super_resolution', 'denoising', 'color_correction', 'sharpening']
        self.enhancement_level = kwargs.get('enhancement_level', 'medium')
        self.super_resolution_factor = kwargs.get('super_resolution_factor', 2.0)

class QualityAssessmentMixin(BaseStepMixin):
    """Step 8: Quality Assessment íŠ¹í™” Mixin"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'QualityAssessmentStep')
        kwargs.setdefault('step_id', 8)
        super().__init__(**kwargs)
        
        self.assessment_criteria = ['perceptual_quality', 'technical_quality', 'aesthetic_quality', 'overall_quality']
        self.quality_threshold = kwargs.get('quality_threshold', 0.7)
        self.use_clip_score = kwargs.get('use_clip_score', True)

# ==============================================
# ğŸ”¥ 12. í¸ì˜ í•¨ìˆ˜ë“¤
# ==============================================

def create_step_mixin(step_name: str, step_id: int, **kwargs) -> BaseStepMixin:
    """BaseStepMixin ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    kwargs.update({'step_name': step_name, 'step_id': step_id})
    return BaseStepMixin(**kwargs)

def create_human_parsing_step(**kwargs) -> HumanParsingMixin:
    """Human Parsing Step ìƒì„±"""
    return HumanParsingMixin(**kwargs)

def create_pose_estimation_step(**kwargs) -> PoseEstimationMixin:
    """Pose Estimation Step ìƒì„±"""
    return PoseEstimationMixin(**kwargs)

def create_cloth_segmentation_step(**kwargs) -> ClothSegmentationMixin:
    """Cloth Segmentation Step ìƒì„±"""
    return ClothSegmentationMixin(**kwargs)

def create_geometric_matching_step(**kwargs) -> GeometricMatchingMixin:
    """Geometric Matching Step ìƒì„±"""
    return GeometricMatchingMixin(**kwargs)

def create_cloth_warping_step(**kwargs) -> ClothWarpingMixin:
    """Cloth Warping Step ìƒì„±"""
    return ClothWarpingMixin(**kwargs)

def create_virtual_fitting_step(**kwargs) -> VirtualFittingMixin:
    """Virtual Fitting Step ìƒì„± (í•µì‹¬)"""
    return VirtualFittingMixin(**kwargs)

def create_post_processing_step(**kwargs) -> PostProcessingMixin:
    """Post Processing Step ìƒì„±"""
    return PostProcessingMixin(**kwargs)

def create_quality_assessment_step(**kwargs) -> QualityAssessmentMixin:
    """Quality Assessment Step ìƒì„±"""
    return QualityAssessmentMixin(**kwargs)

def create_m3_max_optimized_step(step_type: str, **kwargs) -> BaseStepMixin:
    """M3 Max ìµœì í™”ëœ Step ìƒì„±"""
    kwargs.update({
        'device': 'mps',
        'auto_memory_cleanup': True,
        'use_fp16': True
    })
    
    step_creators = {
        'human_parsing': create_human_parsing_step,
        'pose_estimation': create_pose_estimation_step,
        'cloth_segmentation': create_cloth_segmentation_step,
        'geometric_matching': create_geometric_matching_step,
        'cloth_warping': create_cloth_warping_step,
        'virtual_fitting': create_virtual_fitting_step,
        'post_processing': create_post_processing_step,
        'quality_assessment': create_quality_assessment_step,
    }
    
    creator = step_creators.get(step_type, create_step_mixin)
    return creator(**kwargs)

# ==============================================
# ğŸ”¥ 13. ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'BaseStepMixin',
    'StepConfig',
    'SimpleMemoryOptimizer',
    
    # Stepë³„ íŠ¹í™” Mixinë“¤ (8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸)
    'HumanParsingMixin',
    'PoseEstimationMixin', 
    'ClothSegmentationMixin',
    'GeometricMatchingMixin',
    'ClothWarpingMixin',
    'VirtualFittingMixin',
    'PostProcessingMixin',
    'QualityAssessmentMixin',
    
    # í¸ì˜ í•¨ìˆ˜ë“¤
    'create_step_mixin',
    'create_human_parsing_step',
    'create_pose_estimation_step',
    'create_cloth_segmentation_step',
    'create_geometric_matching_step',
    'create_cloth_warping_step',
    'create_virtual_fitting_step',
    'create_post_processing_step',
    'create_quality_assessment_step',
    'create_m3_max_optimized_step',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'CONDA_INFO'
]

# ==============================================
# ğŸ”¥ 14. ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

print("=" * 80)
print("âœ… BaseStepMixin v13.0 - ê°„ì†Œí™”ëœ ì™„ë²½í•œ êµ¬í˜„ ë¡œë“œ ì™„ë£Œ")
print("=" * 80)
print("ğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­:")
print("   âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ì œê±° (ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´)")
print("   âœ… 3ë‹¨ê³„ ê°„ë‹¨í•œ ì´ˆê¸°í™” (17ë‹¨ê³„ â†’ 3ë‹¨ê³„)")
print("   âœ… ëª¨ë“  Step íŒŒì¼ì´ ìš”êµ¬í•˜ëŠ” ê¸°ëŠ¥ ì™„ì „ ì œê³µ")
print("   âœ… ModelLoader ì—°ë™ (89.8GB ì²´í¬í¬ì¸íŠ¸ í™œìš©)")
print("   âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
print("   âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›")
print("   âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ í•´ê²°")
print("   âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")
print("")
print("ğŸš€ Stepë“¤ì´ ì‚¬ìš©í•˜ëŠ” í•µì‹¬ ë©”ì„œë“œë“¤:")
print("   ğŸ¤– ëª¨ë¸ ì—°ë™: get_model(), get_model_async()")
print("   ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™”: optimize_memory(), optimize_memory_async()")
print("   ğŸ”¥ ì›Œë°ì—…: warmup(), warmup_async(), warmup_step()")
print("   ğŸ“Š ìƒíƒœ ê´€ë¦¬: get_status(), get_performance_summary()")
print("   ğŸ”§ ì´ˆê¸°í™”: initialize(), initialize_async()")
print("   ğŸ§¹ ì •ë¦¬: cleanup(), cleanup_models()")
print("   ğŸ“ ê¸°ë¡: record_processing()")
print("")
print("ğŸ¯ ì˜ì¡´ì„± ì£¼ì… (ìˆœí™˜ì°¸ì¡° ë°©ì§€):")
print("   ğŸ’‰ set_model_loader() - ModelLoader ì£¼ì…")
print("   ğŸ’‰ set_memory_manager() - MemoryManager ì£¼ì…")
print("   ğŸ’‰ set_data_converter() - DataConverter ì£¼ì…")
print("")
print("ğŸ¯ 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ Stepë³„ Mixin:")
print("   1ï¸âƒ£ HumanParsingMixin - ì‹ ì²´ ì˜ì—­ ë¶„í• ")
print("   2ï¸âƒ£ PoseEstimationMixin - í¬ì¦ˆ ê°ì§€")
print("   3ï¸âƒ£ ClothSegmentationMixin - ì˜ë¥˜ ë¶„í• ")
print("   4ï¸âƒ£ GeometricMatchingMixin - ê¸°í•˜í•™ì  ë§¤ì¹­")
print("   5ï¸âƒ£ ClothWarpingMixin - ì˜ë¥˜ ë³€í˜•")
print("   6ï¸âƒ£ VirtualFittingMixin - ê°€ìƒ í”¼íŒ… (í•µì‹¬)")
print("   7ï¸âƒ£ PostProcessingMixin - í›„ì²˜ë¦¬")
print("   8ï¸âƒ£ QualityAssessmentMixin -")