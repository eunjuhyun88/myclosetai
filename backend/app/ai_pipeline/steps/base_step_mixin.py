# backend/app/ai_pipeline/steps/base_step_mixin.py
"""
ğŸ”¥ BaseStepMixin v15.0 - ì™„ì „ ê²©ë¦¬ + ìˆœìˆ˜ ì˜ì¡´ì„± ì£¼ì… ë²„ì „ (ì™„ì „ì²´)
====================================================================

âœ… ë™ì  import ì™„ì „ ì œê±° - ìˆœí™˜ì°¸ì¡° 100% ì°¨ë‹¨
âœ… ìˆœìˆ˜ ì˜ì¡´ì„± ì£¼ì…ë§Œ ì‚¬ìš©
âœ… ëª¨ë“  ì™¸ë¶€ ëª¨ë“ˆ ì°¸ì¡° ì œê±°
âœ… ì™„ì „ ê²©ë¦¬ëœ ì•„í‚¤í…ì²˜
âœ… M3 Max 128GB ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„ 
âœ… 3ë‹¨ê³„ ê°„ë‹¨í•œ ì´ˆê¸°í™”
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ëª¨ë“  Stepì—ì„œ ì°¸ì¡°í•˜ëŠ” ê¸°ëŠ¥ ì™„ì „ í¬í•¨
âœ… ë¹ ì§„ í•¨ìˆ˜ë“¤ ëª¨ë‘ ë³µì›

ğŸ”¥ í•µì‹¬ ì›ì¹™:
- BaseStepMixinì€ ì–´ë–¤ ëª¨ë“ˆë„ ì§ì ‘ import í•˜ì§€ ì•ŠìŒ
- ëª¨ë“  ê¸°ëŠ¥ì€ ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œë§Œ ì œê³µ
- ì™¸ë¶€ ëª¨ë“ˆê³¼ ì™„ì „íˆ ë…ë¦½ì 
- ìˆœí™˜ì°¸ì¡° ë¶ˆê°€ëŠ¥í•œ êµ¬ì¡°
- Step íŒŒì¼ë“¤ì´ ìš”êµ¬í•˜ëŠ” ëª¨ë“  ë©”ì„œë“œ ì œê³µ

Author: MyCloset AI Team
Date: 2025-07-22
Version: 15.0 (Pure DI Isolated - Complete)
"""

# ==============================================
# ğŸ”¥ 1. ìµœì†Œí•œì˜ ê¸°ë³¸ importë§Œ (ì™¸ë¶€ ëª¨ë“ˆ ì—†ìŒ)
# ==============================================
import os
import gc
import time
import asyncio
import logging
import threading
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Type, Callable, Tuple
from abc import ABC, abstractmethod
from dataclasses import dataclass
from functools import wraps
import platform
import subprocess
from datetime import datetime

# ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ (ì„ íƒì )
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# ==============================================
# ğŸ”¥ 2. conda í™˜ê²½ ë° ì‹œìŠ¤í…œ ì²´í¬
# ==============================================
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'python_path': os.path.dirname(os.__file__)
}

if CONDA_INFO['conda_env'] != 'none':
    print(f"âœ… conda í™˜ê²½ ê°ì§€: {CONDA_INFO['conda_env']}")
else:
    print("âš ï¸ conda í™˜ê²½ ê¶Œì¥: conda activate mycloset-ai")

# ==============================================
# ğŸ”¥ 3. ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import (ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ)
# ==============================================

# PyTorch ì•ˆì „ Import (ìˆœìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ)
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        print("ğŸ M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
    
except ImportError:
    print("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch torchvision torchaudio -c pytorch")

# NumPy ì•ˆì „ Import
NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    print("âš ï¸ NumPy ì—†ìŒ - conda install numpy")

# PIL ì•ˆì „ Import  
PIL_AVAILABLE = False
try:
    from PIL import Image, ImageEnhance, ImageFilter
    PIL_AVAILABLE = True
except ImportError:
    print("âš ï¸ PIL ì—†ìŒ - conda install pillow")

# ==============================================
# ğŸ”¥ 4. ê°„ë‹¨í•œ ì„¤ì • í´ë˜ìŠ¤
# ==============================================
@dataclass
class StepConfig:
    """Step ì„¤ì •"""
    step_name: str = "BaseStep"
    step_id: int = 0
    device: str = "auto"
    use_fp16: bool = True
    batch_size: int = 1
    confidence_threshold: float = 0.8
    auto_memory_cleanup: bool = True
    auto_warmup: bool = True
    optimization_enabled: bool = True
    quality_level: str = "balanced"
    device_type: str = "auto"
    strict_mode: bool = True

# ==============================================
# ğŸ”¥ 5. ì˜ì¡´ì„± ì£¼ì…ìš© ì¸í„°í˜ì´ìŠ¤ (ì¶”ìƒí™”)
# ==============================================
class IModelProvider(ABC):
    """ëª¨ë¸ ì œê³µì ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def get_model(self, model_name: str) -> Optional[Any]:
        pass
    
    @abstractmethod
    async def get_model_async(self, model_name: str) -> Optional[Any]:
        pass
    
    @abstractmethod
    def is_model_available(self, model_name: str) -> bool:
        pass

class IMemoryManager(ABC):
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        pass
    
    @abstractmethod
    async def optimize_memory_async(self, aggressive: bool = False) -> Dict[str, Any]:
        pass

class IDataConverter(ABC):
    """ë°ì´í„° ë³€í™˜ê¸° ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def convert_data(self, data: Any, target_format: str) -> Any:
        pass
    
    @abstractmethod
    async def convert_data_async(self, data: Any, target_format: str) -> Any:
        pass

# ==============================================
# ğŸ”¥ 6. ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” (ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ)
# ==============================================
class InternalMemoryOptimizer:
    """ë‚´ì¥ ë©”ëª¨ë¦¬ ìµœì í™” (ì™„ì „ ë…ë¦½ì )"""
    
    def __init__(self, device: str = "mps"):
        self.device = device
        self.is_m3_max = self._detect_m3_max()
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def optimize(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰"""
        try:
            results = []
            
            # Python GC
            before = len(gc.get_objects()) if hasattr(gc, 'get_objects') else 0
            gc.collect()
            after = len(gc.get_objects()) if hasattr(gc, 'get_objects') else 0
            results.append(f"Python GC: {before - after}ê°œ ê°ì²´ í•´ì œ")
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    results.append("CUDA ìºì‹œ ì •ë¦¬")
                elif self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                        results.append("MPS ìºì‹œ ì •ë¦¬")
                    except Exception:
                        results.append("MPS ìºì‹œ ì •ë¦¬ ì‹œë„")
            
            # M3 Max íŠ¹ë³„ ìµœì í™”
            if self.is_m3_max and aggressive:
                for _ in range(3):
                    gc.collect()
                results.append("M3 Max í†µí•© ë©”ëª¨ë¦¬ ìµœì í™”")
            
            return {
                "success": True,
                "results": results,
                "device": self.device,
                "is_m3_max": self.is_m3_max
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def optimize_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, lambda: self.optimize(aggressive))
        except Exception as e:
            return {"success": False, "error": str(e)}

# SimpleMemoryOptimizer ë³„ì¹­ (ê¸°ì¡´ í˜¸í™˜ì„±)
SimpleMemoryOptimizer = InternalMemoryOptimizer

# ==============================================
# ğŸ”¥ 7. BaseStepMixin v15.0 - ì™„ì „ ê²©ë¦¬ ë²„ì „ (ì™„ì „ì²´)
# ==============================================
class BaseStepMixin:
    """
    ğŸ”¥ BaseStepMixin v15.0 - ì™„ì „ ê²©ë¦¬ + ìˆœìˆ˜ ì˜ì¡´ì„± ì£¼ì… (ì™„ì „ì²´)
    
    âœ… ë™ì  import ì™„ì „ ì œê±°
    âœ… ìˆœìˆ˜ ì˜ì¡´ì„± ì£¼ì…ë§Œ ì‚¬ìš©  
    âœ… ëª¨ë“  ì™¸ë¶€ ëª¨ë“ˆ ì°¸ì¡° ì œê±°
    âœ… ì™„ì „ ê²©ë¦¬ëœ ì•„í‚¤í…ì²˜
    âœ… ìˆœí™˜ì°¸ì¡° ë¶ˆê°€ëŠ¥
    âœ… Step íŒŒì¼ë“¤ì´ ìš”êµ¬í•˜ëŠ” ëª¨ë“  ë©”ì„œë“œ ì œê³µ
    âœ… ë¹ ì§„ í•¨ìˆ˜ë“¤ ëª¨ë‘ ë³µì›
    """
    
    def __init__(self, **kwargs):
        """3ë‹¨ê³„ ê²©ë¦¬ëœ ì´ˆê¸°í™”"""
        try:
            # STEP 1: ê¸°ë³¸ ì„¤ì • (ê²©ë¦¬)
            self._setup_basic_isolated(**kwargs)
            
            # STEP 2: ì‹œìŠ¤í…œ ì„¤ì • (ê²©ë¦¬)
            self._setup_system_isolated()
            
            # STEP 3: ì™„ë£Œ (ê²©ë¦¬)
            self._finalize_isolated()
            
            self.logger.info(f"âœ… {self.step_name} BaseStepMixin v15.0 ì™„ì „ ê²©ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self._emergency_setup_isolated(e)
    
    def _setup_basic_isolated(self, **kwargs):
        """STEP 1: ê¸°ë³¸ ì„¤ì • (ì™„ì „ ê²©ë¦¬)"""
        # ì„¤ì •
        self.config = StepConfig()
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
        
        # ê¸°ë³¸ ì†ì„±
        self.step_name = kwargs.get('step_name', self.__class__.__name__)
        self.step_id = kwargs.get('step_id', 0)
        
        # Logger ì„¤ì •
        self.logger = logging.getLogger(f"pipeline.steps.{self.step_name}")
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
        
        # ìƒíƒœ í”Œë˜ê·¸ë“¤
        self.is_initialized = False
        self.is_ready = False
        self.has_model = False
        self.model_loaded = False
        self.warmup_completed = False
        
        # ğŸ”¥ ì˜ì¡´ì„± ì£¼ì…ì„ ìœ„í•œ ì¸í„°í˜ì´ìŠ¤ ì†ì„±ë“¤ (ì™„ì „ ê²©ë¦¬)
        self.model_provider: Optional[IModelProvider] = None
        self.memory_manager: Optional[IMemoryManager] = None 
        self.data_converter: Optional[IDataConverter] = None
        
        # ğŸ”¥ íê¸° ì˜ˆì •ì´ì§€ë§Œ í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±ë“¤
        self.model_loader = None  # ModelLoader í˜¸í™˜ì„±
        
        # ë‚´ë¶€ ìºì‹œ (ì™„ì „ ë…ë¦½ì )
        self.model_cache = {}
        self.loaded_models = {}
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_metrics = {
            'process_count': 0,
            'total_process_time': 0.0,
            'average_process_time': 0.0,
            'error_history': [],
            'success_count': 0,
            'cache_hits': 0
        }
        
        # ì—ëŸ¬ ì¶”ì 
        self.error_count = 0
        self.last_error = None
        self.total_processing_count = 0
        self.last_processing_time = None
    
    def _setup_system_isolated(self):
        """STEP 2: ì‹œìŠ¤í…œ ì„¤ì • (ì™„ì „ ê²©ë¦¬)"""
        # ë””ë°”ì´ìŠ¤ ê°ì§€
        if self.config.device == "auto":
            self.device = self._detect_optimal_device()
        else:
            self.device = self.config.device
        
        # M3 Max ê°ì§€
        self.is_m3_max = self._detect_m3_max()
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        self.memory_gb = self._get_memory_info()
        
        # ğŸ”¥ ì™„ì „ ë…ë¦½ì ì¸ ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ
        self.internal_memory_optimizer = InternalMemoryOptimizer(self.device)
        
        # í˜„ì¬ ëª¨ë¸ (ê²©ë¦¬ëœ ìƒíƒœ)
        self._current_model = None
        self._current_model_name = None
    
    def _finalize_isolated(self):
        """STEP 3: ì™„ë£Œ (ì™„ì „ ê²©ë¦¬)"""
        self.is_initialized = True
        
        # ìë™ ì›Œë°ì—… (ì„¤ì •ëœ ê²½ìš°)
        if self.config.auto_warmup:
            try:
                warmup_result = self.warmup_isolated()
                if warmup_result.get('success', False):
                    self.warmup_completed = True
                    self.is_ready = True
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìë™ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    def _emergency_setup_isolated(self, error: Exception):
        """ê¸´ê¸‰ ì„¤ì • (ì™„ì „ ê²©ë¦¬)"""
        self.step_name = getattr(self, 'step_name', self.__class__.__name__)
        self.logger = logging.getLogger("emergency")
        self.device = "cpu"
        self.is_initialized = False
        self.error_count = 1
        self.last_error = str(error)
        print(f"ğŸš¨ {self.step_name} ê¸´ê¸‰ ê²©ë¦¬ ì´ˆê¸°í™”: {error}")
    
    # ==============================================
    # ğŸ”¥ 8. ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤ (ìˆœìˆ˜ ì¸í„°í˜ì´ìŠ¤)
    # ==============================================
    
    def set_model_provider(self, model_provider: IModelProvider):
        """ëª¨ë¸ ì œê³µì ì˜ì¡´ì„± ì£¼ì…"""
        self.model_provider = model_provider
        self.logger.info("âœ… ModelProvider ì£¼ì… ì™„ë£Œ")
        if model_provider:
            self.has_model = True
            self.model_loaded = True
    
    def set_memory_manager(self, memory_manager: IMemoryManager):
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì˜ì¡´ì„± ì£¼ì…"""
        self.memory_manager = memory_manager
        self.logger.info("âœ… MemoryManager ì£¼ì… ì™„ë£Œ")
    
    def set_data_converter(self, data_converter: IDataConverter):
        """ë°ì´í„° ë³€í™˜ê¸° ì˜ì¡´ì„± ì£¼ì…"""
        self.data_converter = data_converter
        self.logger.info("âœ… DataConverter ì£¼ì… ì™„ë£Œ")
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (íê¸° ì˜ˆì •, í˜¸í™˜ì„±ë§Œ ìœ ì§€)"""
        self.model_loader = model_loader
        # ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ ë³€í™˜ ì‹œë„
        if hasattr(model_loader, 'get_model'):
            self.model_provider = ModelLoaderAdapter(model_loader)
        if model_loader:
            self.has_model = True
            self.model_loaded = True
        self.logger.info("âœ… ModelLoader ì£¼ì… ì™„ë£Œ (íê¸° ì˜ˆì • - ModelProvider ì‚¬ìš© ê¶Œì¥)")
    
    # ==============================================
    # ğŸ”¥ 9. ì‹œìŠ¤í…œ ê°ì§€ ë©”ì„œë“œë“¤ (ì™„ì „ ë…ë¦½)
    # ==============================================
    
    def _detect_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            if TORCH_AVAILABLE:
                if MPS_AVAILABLE:
                    return "mps"
                elif hasattr(torch, 'cuda') and torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        except:
            return "cpu"
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def _get_memory_info(self) -> float:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        try:
            if PSUTIL_AVAILABLE:
                import psutil
                memory = psutil.virtual_memory()
                return memory.total / 1024**3
        except:
            pass
        return 16.0
    
    # ==============================================
    # ğŸ”¥ 10. í•µì‹¬ ê¸°ëŠ¥ ë©”ì„œë“œë“¤ (ìˆœìˆ˜ DI ê¸°ë°˜)
    # ==============================================
    
    def get_model(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ìˆœìˆ˜ DI ê¸°ë°˜)"""
        try:
            # ìºì‹œ í™•ì¸
            cache_key = model_name or "default"
            if cache_key in self.model_cache:
                self.performance_metrics['cache_hits'] += 1
                return self.model_cache[cache_key]
            
            # ğŸ”¥ ìˆœìˆ˜ DI: ModelProvider ìš°ì„  ì‚¬ìš©
            if self.model_provider:
                try:
                    model = self.model_provider.get_model(model_name or "default")
                    if model:
                        self.model_cache[cache_key] = model
                        self.has_model = True
                        self.model_loaded = True
                        self._current_model = model
                        self._current_model_name = model_name
                        return model
                except Exception as e:
                    self.logger.debug(f"ModelProviderë¥¼ í†µí•œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ í´ë°±: ê¸°ì¡´ ModelLoader í˜¸í™˜ì„± (íê¸° ì˜ˆì •)
            if self.model_loader and hasattr(self.model_loader, 'get_model'):
                try:
                    model = self.model_loader.get_model(model_name or "default")
                    if model:
                        self.model_cache[cache_key] = model
                        self.has_model = True
                        self.model_loaded = True
                        self._current_model = model
                        self._current_model_name = model_name
                        return model
                except Exception as e:
                    self.logger.debug(f"ModelLoader í´ë°± ì‹¤íŒ¨: {e}")
            
            self.logger.warning("âš ï¸ ëª¨ë¸ ì œê³µìê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ - ì˜ì¡´ì„± ì£¼ì… í•„ìš”")
            return None
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    async def get_model_async(self, model_name: Optional[str] = None) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸°, ìˆœìˆ˜ DI ê¸°ë°˜)"""
        try:
            # ìºì‹œ í™•ì¸
            cache_key = model_name or "default"
            if cache_key in self.model_cache:
                self.performance_metrics['cache_hits'] += 1
                return self.model_cache[cache_key]
            
            # ìˆœìˆ˜ DI: ModelProvider ë¹„ë™ê¸° ì‚¬ìš©
            if self.model_provider:
                try:
                    model = await self.model_provider.get_model_async(model_name or "default")
                    if model:
                        self.model_cache[cache_key] = model
                        self.has_model = True
                        self.model_loaded = True
                        return model
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° ModelProvider ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ê¸°ì¡´ ModelLoader ë¹„ë™ê¸° í˜¸í™˜ì„±
            if self.model_loader:
                try:
                    if hasattr(self.model_loader, 'get_model_async'):
                        model = await self.model_loader.get_model_async(model_name or "default")
                    elif hasattr(self.model_loader, 'get_model'):
                        loop = asyncio.get_event_loop()
                        model = await loop.run_in_executor(None, 
                            lambda: self.model_loader.get_model(model_name or "default"))
                    else:
                        model = None
                    
                    if model:
                        self.model_cache[cache_key] = model
                        self.has_model = True
                        self.model_loaded = True
                        return model
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° ModelLoader í´ë°± ì‹¤íŒ¨: {e}")
            
            self.logger.warning("âš ï¸ ëª¨ë¸ ì œê³µìê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ - ì˜ì¡´ì„± ì£¼ì… í•„ìš”")
            return None
                
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def optimize_memory(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (ìˆœìˆ˜ DI ê¸°ë°˜)"""
        try:
            # ìˆœìˆ˜ DI: MemoryManager ìš°ì„  ì‚¬ìš©
            if self.memory_manager:
                try:
                    result = self.memory_manager.optimize_memory(aggressive=aggressive)
                    result["source"] = "injected_memory_manager"
                    return result
                except Exception as e:
                    self.logger.debug(f"ì£¼ì…ëœ MemoryManager ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ë‚´ì¥ ë…ë¦½ì  ë©”ëª¨ë¦¬ ìµœì í™” ì‚¬ìš©
            result = self.internal_memory_optimizer.optimize(aggressive=aggressive)
            result["source"] = "internal_optimizer"
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def optimize_memory_async(self, aggressive: bool = False) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” (ë¹„ë™ê¸°, ìˆœìˆ˜ DI ê¸°ë°˜)"""
        try:
            # ìˆœìˆ˜ DI: MemoryManager ë¹„ë™ê¸° ì‚¬ìš©
            if self.memory_manager:
                try:
                    result = await self.memory_manager.optimize_memory_async(aggressive=aggressive)
                    result["source"] = "injected_memory_manager"
                    return result
                except Exception as e:
                    self.logger.debug(f"ë¹„ë™ê¸° MemoryManager ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ë‚´ì¥ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™”
            result = await self.internal_memory_optimizer.optimize_async(aggressive=aggressive)
            result["source"] = "internal_optimizer"
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    # ==============================================
    # ğŸ”¥ 11. ì›Œë°ì—… ë©”ì„œë“œë“¤ (ì™„ì „ ë³µì›)
    # ==============================================
    
    def warmup_isolated(self) -> Dict[str, Any]:
        """ê²©ë¦¬ëœ ì›Œë°ì—… ì‹¤í–‰"""
        try:
            if self.warmup_completed:
                return {'success': True, 'message': 'ì´ë¯¸ ì›Œë°ì—… ì™„ë£Œë¨', 'cached': True}
            
            self.logger.info(f"ğŸ”¥ {self.step_name} ê²©ë¦¬ëœ ì›Œë°ì—… ì‹œì‘...")
            start_time = time.time()
            results = []
            
            # 1. ë©”ëª¨ë¦¬ ì›Œë°ì—…
            try:
                memory_result = self.optimize_memory()
                results.append('memory_success' if memory_result.get('success') else 'memory_failed')
            except:
                results.append('memory_failed')
            
            # 2. ëª¨ë¸ ì›Œë°ì—… (ì˜ì¡´ì„± ì£¼ì… ê¸°ë°˜)
            try:
                if self.model_provider or self.model_loader:
                    test_model = self.get_model("warmup_test")
                    results.append('model_success' if test_model else 'model_skipped')
                else:
                    results.append('model_skipped_no_provider')
            except:
                results.append('model_failed')
            
            # 3. ë””ë°”ì´ìŠ¤ ì›Œë°ì—…
            try:
                if TORCH_AVAILABLE:
                    test_tensor = torch.randn(10, 10)
                    if self.device != 'cpu':
                        test_tensor = test_tensor.to(self.device)
                    _ = torch.matmul(test_tensor, test_tensor.t())
                    results.append('device_success')
                else:
                    results.append('device_skipped')
            except:
                results.append('device_failed')
            
            duration = time.time() - start_time
            success_count = sum(1 for r in results if 'success' in r)
            overall_success = success_count > 0
            
            if overall_success:
                self.warmup_completed = True
                self.is_ready = True
            
            self.logger.info(f"ğŸ”¥ ê²©ë¦¬ëœ ì›Œë°ì—… ì™„ë£Œ: {success_count}/{len(results)} ì„±ê³µ ({duration:.2f}ì´ˆ)")
            
            return {
                "success": overall_success,
                "duration": duration,
                "results": results,
                "success_count": success_count,
                "total_count": len(results),
                "isolated": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê²©ë¦¬ëœ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def warmup(self) -> Dict[str, Any]:
        """ì›Œë°ì—… (ë™ê¸° ë²„ì „)"""
        return self.warmup_isolated()
    
    async def warmup_async(self) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì›Œë°ì—…"""
        try:
            if self.warmup_completed:
                return {'success': True, 'message': 'ì´ë¯¸ ì›Œë°ì—… ì™„ë£Œë¨', 'cached': True}
            
            self.logger.info(f"ğŸ”¥ {self.step_name} ë¹„ë™ê¸° ì›Œë°ì—… ì‹œì‘...")
            start_time = time.time()
            results = []
            
            # 1. ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ì›Œë°ì—…
            try:
                memory_result = await self.optimize_memory_async()
                results.append('memory_async_success' if memory_result.get('success') else 'memory_async_failed')
            except:
                results.append('memory_async_failed')
            
            # 2. ë¹„ë™ê¸° ëª¨ë¸ ì›Œë°ì—…
            try:
                if self.model_provider or self.model_loader:
                    test_model = await self.get_model_async("warmup_test")
                    results.append('model_async_success' if test_model else 'model_async_skipped')
                else:
                    results.append('model_async_skipped')
            except:
                results.append('model_async_failed')
            
            # 3. ë¹„ë™ê¸° ë””ë°”ì´ìŠ¤ ì›Œë°ì—…
            try:
                if TORCH_AVAILABLE:
                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(None, self._device_warmup_sync)
                    results.append('device_async_success')
                else:
                    results.append('device_async_skipped')
            except:
                results.append('device_async_failed')
            
            duration = time.time() - start_time
            success_count = sum(1 for r in results if 'success' in r)
            overall_success = success_count > 0
            
            if overall_success:
                self.warmup_completed = True
                self.is_ready = True
            
            self.logger.info(f"ğŸ”¥ ë¹„ë™ê¸° ì›Œë°ì—… ì™„ë£Œ: {success_count}/{len(results)} ì„±ê³µ ({duration:.2f}ì´ˆ)")
            
            return {
                "success": overall_success,
                "duration": duration,
                "results": results,
                "success_count": success_count,
                "total_count": len(results),
                "async": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "async": True}
    
    def _device_warmup_sync(self):
        """ë™ê¸° ë””ë°”ì´ìŠ¤ ì›Œë°ì—…"""
        try:
            if TORCH_AVAILABLE:
                test_tensor = torch.randn(10, 10)
                if self.device != 'cpu':
                    test_tensor = test_tensor.to(self.device)
                _ = torch.matmul(test_tensor, test_tensor.t())
                return True
        except:
            pass
        return False
    
    async def warmup_step(self) -> Dict[str, Any]:
        """Step ì›Œë°ì—… (BaseStepMixin í˜¸í™˜ìš©)"""
        return await self.warmup_async()
    
    # ==============================================
    # ğŸ”¥ 12. ì´ˆê¸°í™” ë° ì •ë¦¬ ë©”ì„œë“œë“¤ (ì™„ì „ ë³µì›)
    # ==============================================
    
    def initialize(self) -> bool:
        """ì´ˆê¸°í™” ë©”ì„œë“œ - Stepë“¤ì´ ì‚¬ìš©"""
        try:
            if self.is_initialized:
                return True
            
            self.is_initialized = True
            self.logger.info(f"âœ… {self.step_name} ê²©ë¦¬ëœ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_async(self) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™” ë©”ì„œë“œ"""
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.initialize)
        except Exception as e:
            self.logger.error(f"âŒ ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def cleanup(self) -> Dict[str, Any]:
        """ì •ë¦¬ (ì™„ì „ ê²©ë¦¬)"""
        try:
            self.logger.info(f"ğŸ§¹ {self.step_name} ê²©ë¦¬ëœ ì •ë¦¬ ì‹œì‘...")
            
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            self.model_cache.clear()
            self.loaded_models.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_result = await self.optimize_memory_async(aggressive=True)
            
            # ìƒíƒœ ë¦¬ì…‹
            self.is_ready = False
            self.warmup_completed = False
            
            # ì˜ì¡´ì„± í•´ì œ (ì°¸ì¡°ë§Œ ì œê±°)
            self.model_provider = None
            self.memory_manager = None
            self.data_converter = None
            self.model_loader = None  # í˜¸í™˜ì„±
            
            self.logger.info(f"âœ… {self.step_name} ê²©ë¦¬ëœ ì •ë¦¬ ì™„ë£Œ")
            
            return {
                "success": True,
                "cleanup_result": cleanup_result,
                "step_name": self.step_name,
                "isolated": True
            }
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def cleanup_models(self):
        """ëª¨ë¸ ì •ë¦¬ - Stepë“¤ì´ ì‚¬ìš©"""
        try:
            # ëª¨ë¸ ìºì‹œ ì •ë¦¬
            self.model_cache.clear()
            self.loaded_models.clear()
            
            # í˜„ì¬ ëª¨ë¸ ì´ˆê¸°í™”
            self._current_model = None
            self._current_model_name = None
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "mps" and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                    except:
                        pass
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
                
                gc.collect()
            
            self.has_model = False
            self.model_loaded = False
            self.logger.info(f"ğŸ§¹ {self.step_name} ê²©ë¦¬ëœ ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ==============================================
    # ğŸ”¥ 13. ìƒíƒœ ë° ì„±ëŠ¥ ë©”ì„œë“œë“¤ (ì™„ì „ ë³µì›)
    # ==============================================
    
    def get_status(self) -> Dict[str, Any]:
        """Step ìƒíƒœ ì¡°íšŒ (ì™„ì „ ê²©ë¦¬)"""
        try:
            return {
                'step_name': self.step_name,
                'step_id': getattr(self, 'step_id', 0),
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'has_model': self.has_model,
                'model_loaded': self.model_loaded,
                'warmup_completed': self.warmup_completed,
                'device': self.device,
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb,
                'error_count': self.error_count,
                'last_error': self.last_error,
                'total_processing_count': self.total_processing_count,
                # ì˜ì¡´ì„± ì •ë³´ (ê²©ë¦¬ëœ)
                'dependencies': {
                    'model_provider': self.model_provider is not None,
                    'memory_manager': self.memory_manager is not None,
                    'data_converter': self.data_converter is not None,
                    'model_loader_deprecated': self.model_loader is not None,
                },
                'performance_metrics': self.performance_metrics,
                'conda_info': CONDA_INFO,
                'timestamp': time.time(),
                'version': '15.0-pure-di-isolated',
                'isolated': True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'step_name': getattr(self, 'step_name', 'unknown'),
                'error': str(e),
                'version': '15.0-pure-di-isolated',
                'timestamp': time.time(),
                'isolated': True
            }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ (ì™„ì „ ê²©ë¦¬)"""
        try:
            return {
                'total_processing_count': self.total_processing_count,
                'last_processing_time': self.last_processing_time,
                'error_count': self.error_count,
                'success_rate': self._calculate_success_rate(),
                'average_process_time': self.performance_metrics.get('average_process_time', 0.0),
                'total_process_time': self.performance_metrics.get('total_process_time', 0.0),
                'cache_hits': self.performance_metrics.get('cache_hits', 0),
                'cache_hit_rate': self._calculate_cache_hit_rate(),
                'version': '15.0-pure-di-isolated',
                'isolated': True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'version': '15.0-pure-di-isolated', 'error': str(e), 'isolated': True}
    
    def _calculate_success_rate(self) -> float:
        """ì„±ê³µë¥  ê³„ì‚° (ì™„ì „ ê²©ë¦¬)"""
        try:
            total = self.total_processing_count
            errors = self.error_count
            if total > 0:
                return (total - errors) / total * 100.0
            return 0.0
        except:
            return 0.0
    
    def _calculate_cache_hit_rate(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚°"""
        try:
            hits = self.performance_metrics.get('cache_hits', 0)
            total = self.total_processing_count
            if total > 0:
                return (hits / total) * 100.0
            return 0.0
        except:
            return 0.0
    
    def record_processing(self, duration: float, success: bool = True):
        """ì²˜ë¦¬ ê¸°ë¡ - Stepë“¤ì´ ì‚¬ìš©"""
        try:
            self.total_processing_count += 1
            self.last_processing_time = time.time()
            
            if success:
                self.performance_metrics['success_count'] += 1
            else:
                self.error_count += 1
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.performance_metrics['process_count'] = self.total_processing_count
            self.performance_metrics['total_process_time'] += duration
            
            if self.total_processing_count > 0:
                self.performance_metrics['average_process_time'] = (
                    self.performance_metrics['total_process_time'] / self.total_processing_count
                )
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²˜ë¦¬ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì (ì•ˆì „í•œ ì •ë¦¬)"""
        try:
            if hasattr(self, 'model_cache'):
                self.model_cache.clear()
        except:
            pass

# ==============================================
# ğŸ”¥ 14. í˜¸í™˜ì„± ì–´ëŒ‘í„° (ê¸°ì¡´ ModelLoader ì§€ì›)
# ==============================================
class ModelLoaderAdapter(IModelProvider):
    """ê¸°ì¡´ ModelLoaderë¥¼ IModelProviderë¡œ ë³€í™˜í•˜ëŠ” ì–´ëŒ‘í„°"""
    
    def __init__(self, model_loader):
        self.model_loader = model_loader
    
    def get_model(self, model_name: str) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if hasattr(self.model_loader, 'get_model'):
                return self.model_loader.get_model(model_name)
            elif hasattr(self.model_loader, 'load_model'):
                return self.model_loader.load_model(model_name)
            return None
        except Exception:
            return None
    
    async def get_model_async(self, model_name: str) -> Optional[Any]:
        """ë¹„ë™ê¸° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if hasattr(self.model_loader, 'get_model_async'):
                return await self.model_loader.get_model_async(model_name)
            elif hasattr(self.model_loader, 'load_model_async'):
                return await self.model_loader.load_model_async(model_name)
            else:
                # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, lambda: self.get_model(model_name))
        except Exception:
            return None
    
    def is_model_available(self, model_name: str) -> bool:
        """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        try:
            if hasattr(self.model_loader, 'is_model_available'):
                return self.model_loader.is_model_available(model_name)
            return self.get_model(model_name) is not None
        except Exception:
            return False

# ==============================================
# ğŸ”¥ 15. Stepë³„ íŠ¹í™” Mixinë“¤ (ì™„ì „ ê²©ë¦¬)
# ==============================================

class HumanParsingMixin(BaseStepMixin):
    """Step 1: Human Parsing íŠ¹í™” Mixin (ì™„ì „ ê²©ë¦¬)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'HumanParsingStep')
        kwargs.setdefault('step_id', 1)
        super().__init__(**kwargs)
        
        self.num_classes = 20
        self.parsing_categories = [
            'background', 'hat', 'hair', 'glove', 'sunglasses', 'upperclothes',
            'dress', 'coat', 'socks', 'pants', 'jumpsuits', 'scarf', 'skirt',
            'face', 'left_arm', 'right_arm', 'left_leg', 'right_leg', 'left_shoe', 'right_shoe'
        ]

class PoseEstimationMixin(BaseStepMixin):
    """Step 2: Pose Estimation íŠ¹í™” Mixin (ì™„ì „ ê²©ë¦¬)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'PoseEstimationStep')
        kwargs.setdefault('step_id', 2)
        super().__init__(**kwargs)
        
        self.num_keypoints = 18
        self.keypoint_names = [
            'nose', 'neck', 'right_shoulder', 'right_elbow', 'right_wrist',
            'left_shoulder', 'left_elbow', 'left_wrist', 'right_hip', 'right_knee',
            'right_ankle', 'left_hip', 'left_knee', 'left_ankle', 'right_eye',
            'left_eye', 'right_ear', 'left_ear'
        ]

class ClothSegmentationMixin(BaseStepMixin):
    """Step 3: Cloth Segmentation íŠ¹í™” Mixin (ì™„ì „ ê²©ë¦¬)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'ClothSegmentationStep')
        kwargs.setdefault('step_id', 3)
        super().__init__(**kwargs)
        
        self.segmentation_methods = ['traditional', 'u2net', 'deeplab', 'auto', 'hybrid']
        self.segmentation_method = kwargs.get('segmentation_method', 'u2net')

class GeometricMatchingMixin(BaseStepMixin):
    """Step 4: Geometric Matching íŠ¹í™” Mixin (ì™„ì „ ê²©ë¦¬)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'GeometricMatchingStep')
        kwargs.setdefault('step_id', 4)
        super().__init__(**kwargs)
        
        self.matching_methods = ['thin_plate_spline', 'affine', 'perspective', 'flow_based']
        self.matching_method = kwargs.get('matching_method', 'thin_plate_spline')
        self.grid_size = kwargs.get('grid_size', (5, 5))

class ClothWarpingMixin(BaseStepMixin):
    """Step 5: Cloth Warping íŠ¹í™” Mixin (ì™„ì „ ê²©ë¦¬)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'ClothWarpingStep')
        kwargs.setdefault('step_id', 5)
        super().__init__(**kwargs)
        
        self.warping_stages = ['preprocessing', 'geometric_transformation', 'texture_mapping', 'postprocessing']
        self.warping_quality = kwargs.get('warping_quality', 'high')
        self.preserve_texture = kwargs.get('preserve_texture', True)

class VirtualFittingMixin(BaseStepMixin):
    """Step 6: Virtual Fitting íŠ¹í™” Mixin (ì™„ì „ ê²©ë¦¬) - í•µì‹¬ ë‹¨ê³„"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'VirtualFittingStep')
        kwargs.setdefault('step_id', 6)
        super().__init__(**kwargs)
        
        self.fitting_modes = ['standard', 'high_quality', 'fast', 'experimental']
        self.fitting_mode = kwargs.get('fitting_mode', 'high_quality')
        self.diffusion_steps = kwargs.get('diffusion_steps', 50)
        self.guidance_scale = kwargs.get('guidance_scale', 7.5)
        self.use_ootd = kwargs.get('use_ootd', True)

class PostProcessingMixin(BaseStepMixin):
    """Step 7: Post Processing íŠ¹í™” Mixin (ì™„ì „ ê²©ë¦¬)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'PostProcessingStep')
        kwargs.setdefault('step_id', 7)
        super().__init__(**kwargs)
        
        self.processing_methods = ['super_resolution', 'denoising', 'color_correction', 'sharpening']
        self.enhancement_level = kwargs.get('enhancement_level', 'medium')
        self.super_resolution_factor = kwargs.get('super_resolution_factor', 2.0)

class QualityAssessmentMixin(BaseStepMixin):
    """Step 8: Quality Assessment íŠ¹í™” Mixin (ì™„ì „ ê²©ë¦¬)"""
    
    def __init__(self, **kwargs):
        kwargs.setdefault('step_name', 'QualityAssessmentStep')
        kwargs.setdefault('step_id', 8)
        super().__init__(**kwargs)
        
        self.assessment_criteria = ['perceptual_quality', 'technical_quality', 'aesthetic_quality', 'overall_quality']
        self.quality_threshold = kwargs.get('quality_threshold', 0.7)
        self.use_clip_score = kwargs.get('use_clip_score', True)

# ==============================================
# ğŸ”¥ 16. í¸ì˜ í•¨ìˆ˜ë“¤ (ì™„ì „ ê²©ë¦¬) + ê¸°ì¡´ í˜¸í™˜ì„± (ğŸ”¥ ë¬¸ì œ í•´ê²°!)
# ==============================================

def create_isolated_step_mixin(step_name: str, step_id: int, **kwargs) -> BaseStepMixin:
    """ê²©ë¦¬ëœ BaseStepMixin ì¸ìŠ¤í„´ìŠ¤ ìƒì„± - ğŸ”¥ ë¹ ì§„ í•¨ìˆ˜ ë³µì›!"""
    kwargs.update({'step_name': step_name, 'step_id': step_id})
    return BaseStepMixin(**kwargs)

def create_step_mixin(step_name: str, step_id: int, **kwargs) -> BaseStepMixin:
    """BaseStepMixin ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    return create_isolated_step_mixin(step_name, step_id, **kwargs)

def create_human_parsing_step(**kwargs) -> HumanParsingMixin:
    """Human Parsing Step ìƒì„± (ê²©ë¦¬)"""
    return HumanParsingMixin(**kwargs)

def create_pose_estimation_step(**kwargs) -> PoseEstimationMixin:
    """Pose Estimation Step ìƒì„± (ê²©ë¦¬)"""
    return PoseEstimationMixin(**kwargs)

def create_cloth_segmentation_step(**kwargs) -> ClothSegmentationMixin:
    """Cloth Segmentation Step ìƒì„± (ê²©ë¦¬)"""
    return ClothSegmentationMixin(**kwargs)

def create_geometric_matching_step(**kwargs) -> GeometricMatchingMixin:
    """Geometric Matching Step ìƒì„± (ê²©ë¦¬)"""
    return GeometricMatchingMixin(**kwargs)

def create_cloth_warping_step(**kwargs) -> ClothWarpingMixin:
    """Cloth Warping Step ìƒì„± (ê²©ë¦¬)"""
    return ClothWarpingMixin(**kwargs)

def create_virtual_fitting_step(**kwargs) -> VirtualFittingMixin:
    """Virtual Fitting Step ìƒì„± (ê²©ë¦¬) - í•µì‹¬"""
    return VirtualFittingMixin(**kwargs)

def create_post_processing_step(**kwargs) -> PostProcessingMixin:
    """Post Processing Step ìƒì„± (ê²©ë¦¬)"""
    return PostProcessingMixin(**kwargs)

def create_quality_assessment_step(**kwargs) -> QualityAssessmentMixin:
    """Quality Assessment Step ìƒì„± (ê²©ë¦¬)"""
    return QualityAssessmentMixin(**kwargs)

def create_m3_max_optimized_step(step_type: str, **kwargs) -> BaseStepMixin:
    """M3 Max ìµœì í™”ëœ Step ìƒì„± (ê²©ë¦¬)"""
    kwargs.update({
        'device': 'mps',
        'auto_memory_cleanup': True,
        'use_fp16': True
    })
    
    step_creators = {
        'human_parsing': create_human_parsing_step,
        'pose_estimation': create_pose_estimation_step,
        'cloth_segmentation': create_cloth_segmentation_step,
        'geometric_matching': create_geometric_matching_step,
        'cloth_warping': create_cloth_warping_step,
        'virtual_fitting': create_virtual_fitting_step,
        'post_processing': create_post_processing_step,
        'quality_assessment': create_quality_assessment_step,
    }
    
    creator = step_creators.get(step_type, create_isolated_step_mixin)
    return creator(**kwargs)

# ==============================================
# ğŸ”¥ 17. ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸° (ì™„ì „ ê²©ë¦¬) + ë¹ ì§„ ê¸°ëŠ¥ ì¶”ê°€
# ==============================================

__all__ = [
    # ğŸ”¥ ë©”ì¸ í´ë˜ìŠ¤ë“¤ (ì™„ì „ ê²©ë¦¬)
    'BaseStepMixin',
    'StepConfig',
    'InternalMemoryOptimizer',
    'SimpleMemoryOptimizer',  # ğŸ”¥ ê¸°ì¡´ í˜¸í™˜ì„± ë³„ì¹­
    
    # ğŸ”¥ ì¸í„°í˜ì´ìŠ¤ë“¤
    'IModelProvider',
    'IMemoryManager', 
    'IDataConverter',
    'ModelLoaderAdapter',
    
    # ğŸ”¥ Stepë³„ íŠ¹í™” Mixinë“¤ (ì™„ì „ ê²©ë¦¬)
    'HumanParsingMixin',
    'PoseEstimationMixin', 
    'ClothSegmentationMixin',
    'GeometricMatchingMixin',
    'ClothWarpingMixin',
    'VirtualFittingMixin',
    'PostProcessingMixin',
    'QualityAssessmentMixin',
    
    # í¸ì˜ í•¨ìˆ˜ë“¤ (ì™„ì „ ê²©ë¦¬) + ğŸ”¥ ê¸°ì¡´ í˜¸í™˜ì„± í•¨ìˆ˜ëª…ë“¤ ì¶”ê°€
    'create_isolated_step_mixin',  # ğŸ”¥ ë¹ ì§„ í•¨ìˆ˜ ë³µì›!
    'create_step_mixin',  # ğŸ”¥ ê¸°ì¡´ í˜¸í™˜ì„±
    'create_human_parsing_step',
    'create_pose_estimation_step',
    'create_cloth_segmentation_step',
    'create_geometric_matching_step',
    'create_cloth_warping_step',
    'create_virtual_fitting_step',
    'create_post_processing_step',
    'create_quality_assessment_step',
    'create_m3_max_optimized_step',
    
    # ìƒìˆ˜ë“¤
    'TORCH_AVAILABLE',
    'MPS_AVAILABLE',
    'NUMPY_AVAILABLE',
    'PIL_AVAILABLE',
    'CONDA_INFO'
]

# ==============================================
# ğŸ”¥ 18. ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

print("=" * 80)
print("ğŸ‰ BaseStepMixin v15.0 - ì™„ì „ ê²©ë¦¬ + ìˆœìˆ˜ ì˜ì¡´ì„± ì£¼ì… (ì™„ì „ì²´)!")
print("=" * 80)
print("ğŸ”¥ ì™„ì „ ê²©ë¦¬ ë‹¬ì„±:")
print("   âœ… ë™ì  import ì™„ì „ ì œê±° - ìˆœí™˜ì°¸ì¡° 100% ì°¨ë‹¨")
print("   âœ… ìˆœìˆ˜ ì˜ì¡´ì„± ì£¼ì…ë§Œ ì‚¬ìš©")
print("   âœ… ëª¨ë“  ì™¸ë¶€ ëª¨ë“ˆ ì°¸ì¡° ì œê±°")
print("   âœ… ì™„ì „ ë…ë¦½ì ì¸ ì•„í‚¤í…ì²˜")
print("   âœ… ìˆœí™˜ì°¸ì¡° ë¶ˆê°€ëŠ¥í•œ êµ¬ì¡°")
print("   âœ… create_isolated_step_mixin í•¨ìˆ˜ ë³µì›!")
print("")
print("ğŸ”¥ ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ì„¤ê³„:")
print("   ğŸ”Œ IModelProvider - ëª¨ë¸ ì œê³µì ì¸í„°í˜ì´ìŠ¤")
print("   ğŸ§  IMemoryManager - ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸í„°í˜ì´ìŠ¤") 
print("   ğŸ“Š IDataConverter - ë°ì´í„° ë³€í™˜ê¸° ì¸í„°í˜ì´ìŠ¤")
print("   ğŸ”§ ModelLoaderAdapter - ê¸°ì¡´ í˜¸í™˜ì„± ì–´ëŒ‘í„°")
print("")
print("ğŸ”¥ ìˆœìˆ˜ ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œ:")
print("   ğŸ’‰ set_model_provider() - ëª¨ë¸ ì œê³µì ì£¼ì…")
print("   ğŸ’‰ set_memory_manager() - ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì£¼ì…")
print("   ğŸ’‰ set_data_converter() - ë°ì´í„° ë³€í™˜ê¸° ì£¼ì…")
print("   âš ï¸  set_model_loader() - íê¸° ì˜ˆì • (í˜¸í™˜ì„±ë§Œ)")
print("")
print("ğŸš€ Stepë“¤ì´ ì‚¬ìš©í•˜ëŠ” í•µì‹¬ ë©”ì„œë“œë“¤ (ì™„ì „ ë³µì›):")
print("   ğŸ¤– get_model(), get_model_async() - ìˆœìˆ˜ DI")
print("   ğŸ§¹ optimize_memory(), optimize_memory_async() - ìˆœìˆ˜ DI")
print("   ğŸ”¥ warmup(), warmup_async(), warmup_step() - ê²©ë¦¬ëœ ì›Œë°ì—…")
print("   ğŸ“Š get_status(), get_performance_summary() - ê²©ë¦¬ëœ ìƒíƒœ")
print("   ğŸ”§ initialize(), initialize_async() - ì´ˆê¸°í™”")
print("   ğŸ§¹ cleanup(), cleanup_models() - ê²©ë¦¬ëœ ì •ë¦¬")
print("   ğŸ“ record_processing() - ì²˜ë¦¬ ê¸°ë¡")
print("")
print("ğŸ¯ 8ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ Stepë³„ Mixin (ì™„ì „ ê²©ë¦¬):")
print("   1ï¸âƒ£ HumanParsingMixin - ì‹ ì²´ ì˜ì—­ ë¶„í• ")
print("   2ï¸âƒ£ PoseEstimationMixin - í¬ì¦ˆ ê°ì§€")
print("   3ï¸âƒ£ ClothSegmentationMixin - ì˜ë¥˜ ë¶„í• ")
print("   4ï¸âƒ£ GeometricMatchingMixin - ê¸°í•˜í•™ì  ë§¤ì¹­")
print("   5ï¸âƒ£ ClothWarpingMixin - ì˜ë¥˜ ë³€í˜•")
print("   6ï¸âƒ£ VirtualFittingMixin - ê°€ìƒ í”¼íŒ… (í•µì‹¬)")
print("   7ï¸âƒ£ PostProcessingMixin - í›„ì²˜ë¦¬")
print("   8ï¸âƒ£ QualityAssessmentMixin - í’ˆì§ˆ í‰ê°€")
print("")
print("ğŸ”¥ ì™„ì „ ë³µì›ëœ í¸ì˜ í•¨ìˆ˜ë“¤:")
print("   âœ… create_isolated_step_mixin() - ë¹ ì§„ í•¨ìˆ˜ ë³µì›!")
print("   âœ… create_step_mixin() - ê¸°ì¡´ í˜¸í™˜ì„±")
print("   âœ… create_*_step() - ëª¨ë“  Step ìƒì„±ì")
print("   âœ… create_m3_max_optimized_step() - M3 Max ìµœì í™”")
print("")
print(f"ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
print(f"   conda í™˜ê²½: {CONDA_INFO['conda_env']}")
print(f"   PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
print(f"   MPS (M3 Max): {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
print(f"   NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'}")
print(f"   PIL: {'âœ…' if PIL_AVAILABLE else 'âŒ'}")
print("")
print("ğŸ‰ ì™„ì „ ê²©ë¦¬ ì„±ê³µ! - ìˆœí™˜ì°¸ì¡° ì›ì²œ ì°¨ë‹¨!")
print("ğŸ‰ ì´ì œ BaseStepMixinì€ ì–´ë–¤ ëª¨ë“ˆë„ ì§ì ‘ ì°¸ì¡°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
print("ğŸ‰ ëª¨ë“  ê¸°ëŠ¥ì€ ìˆœìˆ˜ ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œë§Œ ì œê³µë©ë‹ˆë‹¤!")
print("ğŸ”¥ NameError: create_isolated_step_mixin ì™„ì „ í•´ê²°!")
print("=" * 80)