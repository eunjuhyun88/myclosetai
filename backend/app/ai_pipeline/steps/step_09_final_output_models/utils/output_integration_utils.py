#!/usr/bin/env python3
"""
ğŸ”¥ Output Integration Utils - ì¶œë ¥ í†µí•© ìœ í‹¸ë¦¬í‹°
================================================================================

âœ… ì¶œë ¥ í†µí•© í•¨ìˆ˜
âœ… ë°ì´í„° ê²€ì¦
âœ… í’ˆì§ˆ í‰ê°€
âœ… ë©”íŠ¸ë¦­ ê³„ì‚°

Author: MyCloset AI Team
Date: 2025-08-13
Version: 1.0
"""

import logging
import traceback
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import numpy as np

logger = logging.getLogger(__name__)

class OutputIntegrationUtils:
    """ì¶œë ¥ í†µí•© ìœ í‹¸ë¦¬í‹°"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
    
    def validate_step_outputs(self, step_outputs: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """ë‹¨ê³„ë³„ ì¶œë ¥ ê²€ì¦"""
        try:
            errors = []
            
            if not step_outputs:
                errors.append("ë‹¨ê³„ë³„ ì¶œë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                return False, errors
            
            # ê° ë‹¨ê³„ ê²°ê³¼ ê²€ì¦
            for step_name, step_result in step_outputs.items():
                if not isinstance(step_result, dict):
                    errors.append(f"{step_name}: ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤")
                    continue
                
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                required_fields = ['status', 'step_version']
                for field in required_fields:
                    if field not in step_result:
                        errors.append(f"{step_name}: {field} í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤")
                
                # ìƒíƒœ ê²€ì¦
                if 'status' in step_result:
                    status = step_result['status']
                    if status not in ['success', 'error', 'partial']:
                        errors.append(f"{step_name}: ì˜ëª»ëœ ìƒíƒœê°’: {status}")
            
            return len(errors) == 0, errors
            
        except Exception as e:
            logger.error(f"ì¶œë ¥ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False, [f"ê²€ì¦ ì˜¤ë¥˜: {e}"]
    
    def calculate_integration_metrics(self, step_outputs: Dict[str, Any]) -> Dict[str, Any]:
        """í†µí•© ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            metrics = {
                'total_steps': len(step_outputs),
                'successful_steps': 0,
                'failed_steps': 0,
                'total_processing_time': 0.0,
                'average_quality_score': 0.0,
                'average_confidence_score': 0.0
            }
            
            quality_scores = []
            confidence_scores = []
            
            for step_result in step_outputs.values():
                if isinstance(step_result, dict):
                    # ìƒíƒœë³„ ì¹´ìš´íŠ¸
                    status = step_result.get('status', 'unknown')
                    if status == 'success':
                        metrics['successful_steps'] += 1
                    elif status == 'error':
                        metrics['failed_steps'] += 1
                    
                    # ì²˜ë¦¬ ì‹œê°„ ëˆ„ì 
                    processing_time = step_result.get('processing_time', 0.0)
                    metrics['total_processing_time'] += processing_time
                    
                    # í’ˆì§ˆ ì ìˆ˜ ìˆ˜ì§‘
                    if 'quality_score' in step_result:
                        quality_score = step_result['quality_score']
                        if isinstance(quality_score, (int, float)):
                            quality_scores.append(float(quality_score))
                    
                    # ì‹ ë¢°ë„ ì ìˆ˜ ìˆ˜ì§‘
                    if 'confidence_score' in step_result:
                        confidence_score = step_result['confidence_score']
                        if isinstance(confidence_score, (int, float)):
                            confidence_scores.append(float(confidence_score))
            
            # í‰ê·  ê³„ì‚°
            if quality_scores:
                metrics['average_quality_score'] = sum(quality_scores) / len(quality_scores)
            if confidence_scores:
                metrics['average_confidence_score'] = sum(confidence_scores) / len(confidence_scores)
            
            # ì„±ê³µë¥  ê³„ì‚°
            if metrics['total_steps'] > 0:
                metrics['success_rate'] = metrics['successful_steps'] / metrics['total_steps']
            else:
                metrics['success_rate'] = 0.0
            
            return metrics
            
        except Exception as e:
            logger.error(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def generate_integration_summary(self, step_outputs: Dict[str, Any], 
                                   metrics: Dict[str, Any]) -> Dict[str, Any]:
        """í†µí•© ìš”ì•½ ìƒì„±"""
        try:
            summary = {
                'integration_timestamp': datetime.now().isoformat(),
                'pipeline_status': self._determine_pipeline_status(metrics),
                'key_achievements': self._identify_achievements(metrics),
                'areas_for_improvement': self._identify_improvements(metrics),
                'recommendations': self._generate_recommendations(metrics),
                'next_steps': self._suggest_next_steps(metrics)
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _determine_pipeline_status(self, metrics: Dict[str, Any]) -> str:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ê²°ì •"""
        try:
            success_rate = metrics.get('success_rate', 0.0)
            avg_quality = metrics.get('average_quality_score', 0.0)
            
            if success_rate >= 0.9 and avg_quality >= 0.8:
                return 'excellent'
            elif success_rate >= 0.8 and avg_quality >= 0.7:
                return 'good'
            elif success_rate >= 0.7 and avg_quality >= 0.6:
                return 'acceptable'
            elif success_rate >= 0.6:
                return 'needs_improvement'
            else:
                return 'critical_issues'
                
        except Exception as e:
            logger.warning(f"ìƒíƒœ ê²°ì • ì‹¤íŒ¨: {e}")
            return 'unknown'
    
    def _identify_achievements(self, metrics: Dict[str, Any]) -> List[str]:
        """ì„±ê³¼ ì‹ë³„"""
        achievements = []
        
        try:
            success_rate = metrics.get('success_rate', 0.0)
            avg_quality = metrics.get('average_quality_score', 0.0)
            total_steps = metrics.get('total_steps', 0)
            
            if success_rate >= 0.9:
                achievements.append("ë†’ì€ ì„±ê³µë¥  ë‹¬ì„±")
            if success_rate == 1.0:
                achievements.append("ëª¨ë“  ë‹¨ê³„ ì™„ë²½ ì‹¤í–‰")
            if avg_quality >= 0.8:
                achievements.append("ë†’ì€ í’ˆì§ˆ ë‹¬ì„±")
            if total_steps >= 5:
                achievements.append("ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ ì„±ê³µì  ì²˜ë¦¬")
                
        except Exception as e:
            logger.warning(f"ì„±ê³¼ ì‹ë³„ ì‹¤íŒ¨: {e}")
        
        return achievements
    
    def _identify_improvements(self, metrics: Dict[str, Any]) -> List[str]:
        """ê°œì„  ì˜ì—­ ì‹ë³„"""
        improvements = []
        
        try:
            success_rate = metrics.get('success_rate', 0.0)
            avg_quality = metrics.get('average_quality_score', 0.0)
            failed_steps = metrics.get('failed_steps', 0)
            
            if success_rate < 0.8:
                improvements.append("ì„±ê³µë¥  ê°œì„  í•„ìš”")
            if avg_quality < 0.7:
                improvements.append("í’ˆì§ˆ í–¥ìƒ í•„ìš”")
            if failed_steps > 0:
                improvements.append("ì‹¤íŒ¨í•œ ë‹¨ê³„ ì¬ì²˜ë¦¬ í•„ìš”")
                
        except Exception as e:
            logger.warning(f"ê°œì„  ì˜ì—­ ì‹ë³„ ì‹¤íŒ¨: {e}")
        
        return improvements
    
    def _generate_recommendations(self, metrics: Dict[str, Any]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        try:
            success_rate = metrics.get('success_rate', 0.0)
            avg_quality = metrics.get('average_quality_score', 0.0)
            
            if success_rate < 0.7:
                recommendations.append("ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰ ê³ ë ¤")
            if avg_quality < 0.6:
                recommendations.append("í’ˆì§ˆì´ ë‚®ì€ ë‹¨ê³„ ì¬ì²˜ë¦¬")
            if success_rate >= 0.9 and avg_quality >= 0.8:
                recommendations.append("í˜„ì¬ ì„¤ì • ìœ ì§€ ê¶Œì¥")
                
        except Exception as e:
            logger.warning(f"ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return recommendations
    
    def _suggest_next_steps(self, metrics: Dict[str, Any]) -> List[str]:
        """ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ"""
        next_steps = []
        
        try:
            success_rate = metrics.get('success_rate', 0.0)
            
            if success_rate >= 0.8:
                next_steps.append("ê²°ê³¼ ê²€ì¦ ë° í’ˆì§ˆ í™•ì¸")
                next_steps.append("ìµœì¢… ì¶œë ¥ ìƒì„±")
            else:
                next_steps.append("ì‹¤íŒ¨í•œ ë‹¨ê³„ ë””ë²„ê¹…")
                next_steps.append("íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰")
                
        except Exception as e:
            logger.warning(f"ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ ì‹¤íŒ¨: {e}")
        
        return next_steps
    
    def merge_step_data(self, step_outputs: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ê³„ë³„ ë°ì´í„° ë³‘í•©"""
        try:
            merged_data = {
                'ai_results': {},
                'traditional_metrics': {},
                'additional_data': {}
            }
            
            for step_name, step_result in step_outputs.items():
                if isinstance(step_result, dict) and step_result.get('status') == 'success':
                    # AI ê²°ê³¼ ì¶”ì¶œ
                    if 'ai_quality_assessment' in step_result:
                        merged_data['ai_results'][step_name] = step_result['ai_quality_assessment']
                    
                    # ì „í†µì  ë©”íŠ¸ë¦­ ì¶”ì¶œ
                    if 'traditional_metrics' in step_result:
                        merged_data['traditional_metrics'][step_name] = step_result['traditional_metrics']
                    
                    # ê¸°íƒ€ ë°ì´í„° ì¶”ì¶œ
                    for key, value in step_result.items():
                        if key not in ['status', 'step_version', 'processing_time', 'device_used']:
                            merged_data['additional_data'][step_name] = {key: value}
            
            return merged_data
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë³‘í•© ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
