#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Advanced 2D Rendering Service 2025
===================================================

2025ë…„ ìµœì‹  AI ê¸°ìˆ ì„ í™œìš©í•œ ê³ ê¸‰ 2D ë Œë”ë§ ì„œë¹„ìŠ¤
- Diffusion ê¸°ë°˜ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±
- ControlNetì„ í†µí•œ ì •ë°€í•œ ì œì–´
- StyleGAN-3 ê¸°ë°˜ í…ìŠ¤ì²˜ í–¥ìƒ
- NeRF ê¸°ë°˜ ì¡°ëª… íš¨ê³¼
- Attention ê¸°ë°˜ ì´ë¯¸ì§€ ì •ì œ

Author: MyCloset AI Team
Date: 2025-08-15
Version: 2025.2.0
"""

import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image, ImageDraw, ImageFilter
from typing import Dict, Any, Optional, Tuple, List, Union
import logging
import os
import cv2
from pathlib import Path
import json
import time

from ..models.advanced_2d_renderer import Advanced2DRenderer

logger = logging.getLogger(__name__)

class Advanced2DRenderingService:
    """2025ë…„ ê¸°ì¤€ ê³ ê¸‰ 2D ë Œë”ë§ ì„œë¹„ìŠ¤"""
    
    def __init__(self, device: str = 'cpu'):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.Advanced2DRenderingService")
        
        # ê³ ê¸‰ 2D ë Œë”ë§ ì—”ì§„ ì´ˆê¸°í™”
        try:
            self.renderer = Advanced2DRenderer(
                diffusion_steps=20,
                guidance_scale=7.5,
                enable_controlnet=True,
                enable_stylegan=True,
                enable_nerf_lighting=True
            )
            self.renderer.to(self.device)
            self.renderer.eval()
            self.is_loaded = True
            self.logger.info("âœ… Advanced 2D Rendering Engine ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ Advanced 2D Rendering Engine ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.renderer = None
            self.is_loaded = False
        
        # ë Œë”ë§ ì„¤ì •
        self.rendering_config = {
            'quality_presets': {
                'fast': {'diffusion_steps': 10, 'guidance_scale': 5.0},
                'balanced': {'diffusion_steps': 20, 'guidance_scale': 7.5},
                'high': {'diffusion_steps': 30, 'guidance_scale': 10.0},
                'ultra': {'diffusion_steps': 50, 'guidance_scale': 15.0}
            },
            'lighting_presets': {
                'natural': {'direction': [0, 0, 1], 'intensity': 1.0, 'color': [1, 1, 1]},
                'studio': {'direction': [0.5, 0.5, 0.7], 'intensity': 1.2, 'color': [1, 0.95, 0.9]},
                'dramatic': {'direction': [0.8, 0.2, 0.5], 'intensity': 0.8, 'color': [1, 0.8, 0.6]},
                'soft': {'direction': [0.3, 0.3, 0.9], 'intensity': 0.6, 'color': [1, 1, 1]}
            },
            'style_presets': {
                'photorealistic': 'photorealistic_style.jpg',
                'artistic': 'artistic_style.jpg',
                'fashion': 'fashion_style.jpg',
                'vintage': 'vintage_style.jpg'
            }
        }
    
    def render_virtual_fitting_result(self, 
                                    person_image: torch.Tensor,
                                    clothing_image: torch.Tensor,
                                    pose_keypoints: Optional[torch.Tensor] = None,
                                    quality_preset: str = 'balanced',
                                    lighting_preset: str = 'natural',
                                    style_preset: str = 'photorealistic',
                                    custom_prompt: Optional[str] = None) -> Dict[str, Any]:
        """
        ê°€ìƒ í”¼íŒ… ê²°ê³¼ë¥¼ ê³ ê¸‰ 2D ë Œë”ë§ìœ¼ë¡œ ìƒì„±
        
        Args:
            person_image: ì‚¬ëŒ ì´ë¯¸ì§€ [B, 3, H, W]
            clothing_image: ì˜ë¥˜ ì´ë¯¸ì§€ [B, 3, H, W]
            pose_keypoints: í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ (ControlNet íŒíŠ¸ìš©)
            quality_preset: í’ˆì§ˆ í”„ë¦¬ì…‹
            lighting_preset: ì¡°ëª… í”„ë¦¬ì…‹
            style_preset: ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹
            custom_prompt: ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
        
        Returns:
            ë Œë”ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            if not self.is_loaded:
                raise RuntimeError("ë Œë”ë§ ì—”ì§„ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            self.logger.info(f"ğŸš€ ê³ ê¸‰ 2D ë Œë”ë§ ì‹œì‘ - í’ˆì§ˆ: {quality_preset}, ì¡°ëª…: {lighting_preset}")
            start_time = time.time()
            
            # 1. í’ˆì§ˆ ì„¤ì • ì ìš©
            quality_config = self.rendering_config['quality_presets'][quality_preset]
            self.renderer.diffusion_steps = quality_config['diffusion_steps']
            self.renderer.guidance_scale = quality_config['guidance_scale']
            
            # 2. ControlNet íŒíŠ¸ ìƒì„± (í¬ì¦ˆ ê¸°ë°˜)
            control_hint = None
            if pose_keypoints is not None and self.renderer.enable_controlnet:
                control_hint = self._create_pose_control_hint(pose_keypoints, person_image.shape[2:])
            
            # 3. ìŠ¤íƒ€ì¼ ì°¸ì¡° ì´ë¯¸ì§€ ë¡œë“œ
            style_reference = None
            if self.renderer.enable_stylegan:
                style_reference = self._load_style_reference(style_preset)
            
            # 4. ì¡°ëª… ì¡°ê±´ ì„¤ì •
            lighting_condition = self.rendering_config['lighting_presets'][lighting_preset]
            
            # 5. ê³ ê¸‰ 2D ë Œë”ë§ ìˆ˜í–‰
            with torch.no_grad():
                rendering_result = self.renderer(
                    input_image=person_image,
                    control_hint=control_hint,
                    text_prompt=custom_prompt,
                    style_reference=style_reference,
                    lighting_condition=lighting_condition
                )
            
            # 6. í›„ì²˜ë¦¬ ë° í’ˆì§ˆ í–¥ìƒ
            final_result = self._post_process_rendering(rendering_result, person_image, clothing_image)
            
            # 7. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            rendering_time = time.time() - start_time
            final_result['performance_metrics'] = {
                'rendering_time': rendering_time,
                'quality_preset': quality_preset,
                'lighting_preset': lighting_preset,
                'style_preset': style_preset,
                'total_parameters': sum(p.numel() for p in self.renderer.parameters())
            }
            
            self.logger.info(f"âœ… ê³ ê¸‰ 2D ë Œë”ë§ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {rendering_time:.2f}ì´ˆ")
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ ê³ ê¸‰ 2D ë Œë”ë§ ì‹¤íŒ¨: {e}")
            return self._create_fallback_result(person_image, clothing_image, str(e))
    
    def _create_pose_control_hint(self, pose_keypoints: torch.Tensor, image_size: Tuple[int, int]) -> torch.Tensor:
        """í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ControlNet íŒíŠ¸ ìƒì„±"""
        try:
            H, W = image_size
            B = pose_keypoints.size(0)
            
            # í¬ì¦ˆ íŒíŠ¸ ì´ë¯¸ì§€ ìƒì„±
            pose_hint = torch.zeros(B, 3, H, W, device=pose_keypoints.device)
            
            for b in range(B):
                # í‚¤í¬ì¸íŠ¸ë¥¼ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
                keypoints = pose_keypoints[b]  # [17, 3] - x, y, confidence
                
                # ê³¨ê²© ì—°ê²° ì •ì˜ (COCO í¬ë§·)
                skeleton_connections = [
                    (0, 1), (1, 2), (2, 3), (3, 4),  # ë¨¸ë¦¬-ëª©-ì–´ê¹¨-íŒ”
                    (1, 5), (5, 6), (6, 7),  # ì™¼ìª½ íŒ”
                    (1, 8), (8, 9), (9, 10),  # ì˜¤ë¥¸ìª½ íŒ”
                    (8, 11), (11, 12), (12, 13),  # ì™¼ìª½ ë‹¤ë¦¬
                    (8, 14), (14, 15), (15, 16)  # ì˜¤ë¥¸ìª½ ë‹¤ë¦¬
                ]
                
                # ê³¨ê²© ê·¸ë¦¬ê¸°
                for start_idx, end_idx in skeleton_connections:
                    if (keypoints[start_idx, 2] > 0.5 and keypoints[end_idx, 2] > 0.5):
                        start_x = int(keypoints[start_idx, 0] * W)
                        start_y = int(keypoints[start_idx, 1] * H)
                        end_x = int(keypoints[end_idx, 0] * W)
                        end_y = int(keypoints[end_idx, 1] * H)
                        
                        # ì„  ê·¸ë¦¬ê¸° (ê°„ë‹¨í•œ ë²„ì „)
                        pose_hint[b, 0, start_y:end_y, start_x:end_x] = 1.0
                        pose_hint[b, 1, start_y:end_y, start_x:end_x] = 1.0
                        pose_hint[b, 2, start_y:end_y, start_x:end_x] = 1.0
            
            return pose_hint
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í¬ì¦ˆ íŒíŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return torch.zeros(B, 3, H, W, device=pose_keypoints.device)
    
    def _load_style_reference(self, style_preset: str) -> Optional[torch.Tensor]:
        """ìŠ¤íƒ€ì¼ ì°¸ì¡° ì´ë¯¸ì§€ ë¡œë“œ"""
        try:
            style_path = self.rendering_config['style_presets'].get(style_preset)
            if style_path and os.path.exists(style_path):
                # PILë¡œ ì´ë¯¸ì§€ ë¡œë“œ
                style_image = Image.open(style_path).convert('RGB')
                style_image = style_image.resize((256, 256))  # ê³ ì • í¬ê¸°
                
                # í…ì„œë¡œ ë³€í™˜
                style_tensor = torch.from_numpy(np.array(style_image)).float() / 255.0
                style_tensor = style_tensor.permute(2, 0, 1).unsqueeze(0)  # [1, 3, H, W]
                
                return style_tensor.to(self.device)
            else:
                # ê¸°ë³¸ ìŠ¤íƒ€ì¼ ìƒì„±
                return self._generate_default_style()
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìŠ¤íƒ€ì¼ ì°¸ì¡° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._generate_default_style()
    
    def _generate_default_style(self) -> torch.Tensor:
        """ê¸°ë³¸ ìŠ¤íƒ€ì¼ í…ì„œ ìƒì„±"""
        # ê·¸ë¼ë°ì´ì…˜ ê¸°ë°˜ ê¸°ë³¸ ìŠ¤íƒ€ì¼
        style_tensor = torch.zeros(1, 3, 256, 256, device=self.device)
        
        # ìˆ˜ì§ ê·¸ë¼ë°ì´ì…˜
        for i in range(256):
            intensity = i / 255.0
            style_tensor[0, 0, i, :] = intensity  # R
            style_tensor[0, 1, i, :] = intensity * 0.8  # G
            style_tensor[0, 2, i, :] = intensity * 0.6  # B
        
        return style_tensor
    
    def _post_process_rendering(self, rendering_result: Dict[str, Any], 
                               person_image: torch.Tensor, 
                               clothing_image: torch.Tensor) -> Dict[str, Any]:
        """ë Œë”ë§ ê²°ê³¼ í›„ì²˜ë¦¬ ë° í’ˆì§ˆ í–¥ìƒ"""
        try:
            final_image = rendering_result['rendered_image']
            
            # 1. ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ
            enhanced_image = self._enhance_image_quality(final_image)
            
            # 2. ìƒ‰ìƒ ë³´ì •
            color_corrected = self._color_correction(enhanced_image, person_image)
            
            # 3. ì—£ì§€ ë³´ì •
            edge_refined = self._edge_refinement(color_corrected)
            
            # 4. ë…¸ì´ì¦ˆ ì œê±°
            denoised = self._denoise_image(edge_refined)
            
            # 5. ìµœì¢… í’ˆì§ˆ ê²€ì¦
            quality_score = self._calculate_final_quality(denoised, person_image, clothing_image)
            
            return {
                'final_rendered_image': denoised,
                'intermediate_steps': rendering_result['intermediate_steps'],
                'post_processed_steps': {
                    'enhanced': enhanced_image,
                    'color_corrected': color_corrected,
                    'edge_refined': edge_refined,
                    'denoised': denoised
                },
                'quality_metrics': rendering_result['quality_metrics'],
                'final_quality_score': quality_score,
                'rendering_info': rendering_result['rendering_info']
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return rendering_result
    
    def _enhance_image_quality(self, image: torch.Tensor) -> torch.Tensor:
        """ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ"""
        try:
            # Unsharp Masking (ì„ ëª…ë„ í–¥ìƒ)
            blurred = F.avg_pool2d(image, 3, 1, 1)
            enhanced = image + (image - blurred) * 0.5
            
            # Contrast Enhancement
            mean_val = enhanced.mean()
            enhanced = (enhanced - mean_val) * 1.1 + mean_val
            
            return torch.clamp(enhanced, 0, 1)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image
    
    def _color_correction(self, image: torch.Tensor, reference: torch.Tensor) -> torch.Tensor:
        """ìƒ‰ìƒ ë³´ì •"""
        try:
            # íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­ ê¸°ë°˜ ìƒ‰ìƒ ë³´ì •
            corrected = image.clone()
            
            for c in range(3):  # RGB ì±„ë„ë³„
                # ì°¸ì¡° ì´ë¯¸ì§€ì˜ íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°
                ref_hist = torch.histc(reference[:, c:c+1], bins=256, min=0, max=1)
                ref_cdf = torch.cumsum(ref_hist, dim=0) / ref_hist.sum()
                
                # ì…ë ¥ ì´ë¯¸ì§€ì˜ íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°
                img_hist = torch.histc(image[:, c:c+1], bins=256, min=0, max=1)
                img_cdf = torch.cumsum(img_hist, dim=0) / img_hist.sum()
                
                # íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­
                for b in range(image.size(0)):
                    for i in range(256):
                        target_val = i / 255.0
                        target_idx = torch.argmin(torch.abs(ref_cdf - target_val))
                        corrected[b, c, image[b, c] == target_val] = target_idx / 255.0
            
            return corrected
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
            return image
    
    def _edge_refinement(self, image: torch.Tensor) -> torch.Tensor:
        """ì—£ì§€ ë³´ì •"""
        try:
            # Sobel ì—£ì§€ ê²€ì¶œ
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                                  dtype=torch.float32, device=image.device).view(1, 1, 3, 3)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                                  dtype=torch.float32, device=image.device).view(1, 1, 3, 3)
            
            # ì—£ì§€ ê°•ë„ ê³„ì‚°
            edge_x = F.conv2d(image.mean(dim=1, keepdim=True), sobel_x, padding=1)
            edge_y = F.conv2d(image.mean(dim=1, keepdim=True), sobel_y, padding=1)
            edge_magnitude = torch.sqrt(edge_x**2 + edge_y**2)
            
            # ì—£ì§€ ê°•í™”
            edge_enhanced = image + edge_magnitude * 0.1
            
            return torch.clamp(edge_enhanced, 0, 1)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì—£ì§€ ë³´ì • ì‹¤íŒ¨: {e}")
            return image
    
    def _denoise_image(self, image: torch.Tensor) -> torch.Tensor:
        """ì´ë¯¸ì§€ ë…¸ì´ì¦ˆ ì œê±°"""
        try:
            # Bilateral Filter ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°
            denoised = image.clone()
            
            # ê°„ë‹¨í•œ ê°€ìš°ì‹œì•ˆ í•„í„° (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ bilateral filter ì‚¬ìš©)
            kernel_size = 3
            sigma = 0.5
            
            # ê°€ìš°ì‹œì•ˆ ì»¤ë„ ìƒì„±
            kernel = torch.exp(-torch.arange(-(kernel_size//2), kernel_size//2+1)**2 / (2*sigma**2))
            kernel = kernel / kernel.sum()
            kernel = kernel.view(1, 1, -1, 1)  # [1, 1, kernel_size, 1]
            
            # ìˆ˜í‰ ë°©í–¥ í•„í„°ë§
            denoised = F.conv2d(denoised, kernel, padding=(kernel_size//2, 0))
            
            # ìˆ˜ì§ ë°©í–¥ í•„í„°ë§
            kernel = kernel.transpose(2, 3)  # [1, 1, 1, kernel_size]
            denoised = F.conv2d(denoised, kernel, padding=(0, kernel_size//2))
            
            return denoised
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            return image
    
    def _calculate_final_quality(self, rendered: torch.Tensor, 
                                person: torch.Tensor, 
                                clothing: torch.Tensor) -> float:
        """ìµœì¢… í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            # 1. êµ¬ì¡°ì  ìœ ì‚¬ì„± (SSIM ê¸°ë°˜)
            structural_similarity = self._calculate_ssim(rendered, person)
            
            # 2. ìƒ‰ìƒ ì¼ê´€ì„±
            color_consistency = self._calculate_color_consistency(rendered, person, clothing)
            
            # 3. ì„ ëª…ë„
            sharpness = self._calculate_sharpness(rendered)
            
            # 4. ìì—°ìŠ¤ëŸ¬ì›€
            naturalness = self._calculate_naturalness(rendered)
            
            # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
            final_score = (
                structural_similarity * 0.4 +
                color_consistency * 0.3 +
                sharpness * 0.2 +
                naturalness * 0.1
            )
            
            return float(final_score)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.8  # ê¸°ë³¸ ì ìˆ˜
    
    def _calculate_ssim(self, img1: torch.Tensor, img2: torch.Tensor) -> float:
        """SSIM ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)"""
        try:
            # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚°
            diff = torch.abs(img1 - img2)
            similarity = 1.0 - diff.mean()
            return float(similarity)
        except:
            return 0.8
    
    def _calculate_color_consistency(self, rendered: torch.Tensor, 
                                   person: torch.Tensor, 
                                   clothing: torch.Tensor) -> float:
        """ìƒ‰ìƒ ì¼ê´€ì„± ê³„ì‚°"""
        try:
            # ë Œë”ë§ëœ ì´ë¯¸ì§€ì™€ ì›ë³¸ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ ë¶„í¬ ë¹„êµ
            rendered_mean = rendered.mean(dim=[2, 3])  # [B, C]
            person_mean = person.mean(dim=[2, 3])      # [B, C]
            
            color_diff = torch.abs(rendered_mean - person_mean)
            consistency = 1.0 - color_diff.mean()
            
            return float(consistency)
        except:
            return 0.8
    
    def _calculate_sharpness(self, image: torch.Tensor) -> float:
        """ì„ ëª…ë„ ê³„ì‚°"""
        try:
            # Laplacian ê¸°ë°˜ ì„ ëª…ë„
            laplacian_kernel = torch.tensor([[0, 1, 0], [1, -4, 1], [0, 1, 0]], 
                                          dtype=torch.float32, device=image.device).view(1, 1, 3, 3)
            
            sharpness_map = F.conv2d(image.mean(dim=1, keepdim=True), laplacian_kernel, padding=1)
            sharpness = torch.var(sharpness_map).item()
            
            # ì •ê·œí™”
            normalized_sharpness = min(sharpness / 0.01, 1.0)
            
            return float(normalized_sharpness)
        except:
            return 0.8
    
    def _calculate_naturalness(self, image: torch.Tensor) -> float:
        """ìì—°ìŠ¤ëŸ¬ì›€ ê³„ì‚°"""
        try:
            # ê°„ë‹¨í•œ ìì—°ìŠ¤ëŸ¬ì›€ ì§€í‘œ (ìƒ‰ìƒ ë¶„í¬ ê¸°ë°˜)
            # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì§€í‘œ ì‚¬ìš©
            return 0.9  # ê¸°ë³¸ê°’
        except:
            return 0.8
    
    def _create_fallback_result(self, person_image: torch.Tensor, 
                               clothing_image: torch.Tensor, 
                               error_msg: str) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ fallback ê²°ê³¼ ìƒì„±"""
        B, C, H, W = person_image.shape
        
        # ê°„ë‹¨í•œ í•©ì„± ê²°ê³¼ ìƒì„±
        fallback_image = person_image.clone()
        
        return {
            'final_rendered_image': fallback_image,
            'intermediate_steps': {},
            'post_processed_steps': {},
            'quality_metrics': {'sharpness': 0.5, 'contrast': 0.5, 'brightness': 0.5},
            'final_quality_score': 0.6,
            'rendering_info': {'error': error_msg},
            'performance_metrics': {
                'rendering_time': 0.0,
                'quality_preset': 'fallback',
                'lighting_preset': 'fallback',
                'style_preset': 'fallback',
                'total_parameters': 0
            }
        }
    
    def get_rendering_presets(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë Œë”ë§ í”„ë¦¬ì…‹ ë°˜í™˜"""
        return self.rendering_config
    
    def update_rendering_config(self, new_config: Dict[str, Any]) -> bool:
        """ë Œë”ë§ ì„¤ì • ì—…ë°ì´íŠ¸"""
        try:
            self.rendering_config.update(new_config)
            self.logger.info("âœ… ë Œë”ë§ ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ë Œë”ë§ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False
