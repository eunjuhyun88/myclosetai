"""
ğŸ”¥ Final Output Service
=======================

ìµœì¢… ì¶œë ¥ ìƒì„± ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
ë…¼ë¬¸ ê¸°ë°˜ì˜ AI ëª¨ë¸ êµ¬ì¡°ì— ë§ì¶° êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
from typing import Dict, Any, Optional, Union, List, Tuple
import numpy as np
from PIL import Image
import logging
import time
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë¡œê¹… ì„¤ì • import
try:
    from backend.app.core.logging_config import get_logger
    logger = get_logger(__name__)
except ImportError:
    logger = logging.getLogger(__name__)

class FinalOutputService:
    """
    ìµœì¢… ì¶œë ¥ ìƒì„± ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
    """

    def __init__(self, model_loader=None, processor=None, inference_engine=None):
        """
        Args:
            model_loader: ëª¨ë¸ ë¡œë” ì¸ìŠ¤í„´ìŠ¤
            processor: ë°ì´í„° í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
            inference_engine: ì¶”ë¡  ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
        """
        self.model_loader = model_loader
        self.processor = processor
        self.inference_engine = inference_engine
        
        # ì„œë¹„ìŠ¤ ì„¤ì •
        self.service_config = {
            'default_model': 'final_generator',
            'batch_size': 16,
            'enable_caching': True,
            'quality_threshold': 0.8,
            'output_formats': ['png', 'jpg', 'tiff'],
            'compression_quality': 95
        }
        
        # ìºì‹œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
        self._output_cache = {}
        self._max_cache_size = 500
        
        logger.info("âœ… FinalOutputService initialized")

    def generate_final_output(self, input_data: Union[np.ndarray, Image.Image, torch.Tensor],
                            model_type: str = None,
                            output_format: str = 'png',
                            **kwargs) -> Dict[str, Any]:
        """
        ìµœì¢… ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            input_data: ì…ë ¥ ë°ì´í„°
            model_type: ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì…
            output_format: ì¶œë ¥ í˜•ì‹
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            ìµœì¢… ì¶œë ¥ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            start_time = time.time()
            
            # ëª¨ë¸ íƒ€ì… ì„¤ì •
            if model_type is None:
                model_type = self.service_config['default_model']
            
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(input_data, model_type, output_format)
            if self.service_config['enable_caching'] and cache_key in self._output_cache:
                logger.info("âœ… Final output result found in cache")
                return self._output_cache[cache_key]
            
            # ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬
            if self.processor:
                processed_input = self.processor.preprocess_for_final_output(
                    input_data, **kwargs
                )
            else:
                processed_input = input_data
            
            # ìµœì¢… ì¶œë ¥ ìƒì„±
            if self.inference_engine:
                output_result = self.inference_engine.generate_final_output(
                    processed_input, model_type, **kwargs
                )
            else:
                # ê¸°ë³¸ ì¶œë ¥ ìƒì„± (ê°„ë‹¨í•œ ì²˜ë¦¬)
                output_result = self._basic_output_generation(processed_input, model_type)
            
            # ì¶œë ¥ í›„ì²˜ë¦¬
            if self.processor:
                final_output = self.processor.postprocess_final_output(
                    output_result['output'], **kwargs
                )
            else:
                final_output = output_result['output']
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'output': final_output,
                'quality_score': output_result.get('quality_score', 0.0),
                'quality_grade': output_result.get('quality_grade', 'Unknown'),
                'model_type': model_type,
                'output_format': output_format,
                'processing_time': time.time() - start_time,
                'timestamp': datetime.now().isoformat()
            }
            
            # ìºì‹œì— ì €ì¥
            if self.service_config['enable_caching']:
                self._add_to_cache(cache_key, result)
            
            logger.info(f"âœ… Final output generation completed: {result['quality_grade']}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Final output generation failed: {e}")
            return {
                'error': str(e),
                'quality_score': 0.0,
                'quality_grade': 'Error',
                'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                'timestamp': datetime.now().isoformat()
            }

    def generate_batch_outputs(self, input_data_list: List[Union[np.ndarray, Image.Image, torch.Tensor]],
                             model_type: str = None,
                             output_format: str = 'png',
                             **kwargs) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì…ë ¥ì— ëŒ€í•´ ìµœì¢… ì¶œë ¥ì„ ì¼ê´„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            input_data_list: ì…ë ¥ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            model_type: ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì…
            output_format: ì¶œë ¥ í˜•ì‹
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            ìµœì¢… ì¶œë ¥ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            start_time = time.time()
            batch_size = self.service_config['batch_size']
            results = []
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, len(input_data_list), batch_size):
                batch_inputs = input_data_list[i:i + batch_size]
                batch_results = []
                
                for j, input_data in enumerate(batch_inputs):
                    try:
                        result = self.generate_final_output(
                            input_data, model_type, output_format, **kwargs
                        )
                        result['batch_index'] = i + j
                        batch_results.append(result)
                    except Exception as e:
                        logger.error(f"âŒ Failed to generate output for input {i + j}: {e}")
                        batch_results.append({
                            'batch_index': i + j,
                            'error': str(e),
                            'quality_score': 0.0,
                            'quality_grade': 'Error',
                            'timestamp': datetime.now().isoformat()
                        })
                
                results.extend(batch_results)
                
                # ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ìƒí™© ë¡œê¹…
                logger.info(f"âœ… Batch {i//batch_size + 1} completed: {len(batch_results)} outputs")
            
            # ì „ì²´ í†µê³„ ê³„ì‚°
            total_time = time.time() - start_time
            self._log_batch_statistics(results, total_time)
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Batch output generation failed: {e}")
            return []

    def _basic_output_generation(self, input_data: torch.Tensor, 
                                model_type: str) -> Dict[str, Any]:
        """
        ê¸°ë³¸ ì¶œë ¥ ìƒì„± (ì¶”ë¡  ì—”ì§„ì´ ì—†ì„ ë•Œ ì‚¬ìš©)
        """
        try:
            # ê°„ë‹¨í•œ í’ˆì§ˆ í–¥ìƒ
            if len(input_data.shape) == 4:
                input_data = input_data.squeeze(0)
            
            # ê¸°ë³¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            brightness = input_data.mean().item()
            contrast = input_data.std().item()
            
            # ì„ ëª…ë„ í–¥ìƒ (ê°„ë‹¨í•œ ì–¸ìƒ¤í”„ ë§ˆìŠ¤í¬)
            if input_data.shape[0] == 3:  # RGB
                gray = 0.299 * input_data[0] + 0.587 * input_data[1] + 0.114 * input_data[2]
            else:
                gray = input_data[0]
            
            # Sobel í•„í„°ë¡œ ì—ì§€ ê²€ì¶œ
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                                 dtype=torch.float32, device=input_data.device)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                                 dtype=torch.float32, device=input_data.device)
            
            edge_x = torch.conv2d(gray.unsqueeze(0).unsqueeze(0), 
                                sobel_x.unsqueeze(0).unsqueeze(0), padding=1)
            edge_y = torch.conv2d(gray.unsqueeze(0).unsqueeze(0), 
                                sobel_y.unsqueeze(0).unsqueeze(0), padding=1)
            
            edge_magnitude = torch.sqrt(edge_x**2 + edge_y**2)
            sharpness = edge_magnitude.mean().item()
            
            # ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-1 ë²”ìœ„)
            quality_score = min(1.0, (brightness * 0.3 + contrast * 0.3 + sharpness * 0.4))
            
            # ê¸°ë³¸ ì¶œë ¥ (ì…ë ¥ê³¼ ë™ì¼)
            output = input_data
            
            return {
                'output': output,
                'quality_score': quality_score,
                'model_type': 'basic'
            }
            
        except Exception as e:
            logger.error(f"âŒ Basic output generation failed: {e}")
            return {
                'output': input_data,
                'quality_score': 0.0,
                'model_type': 'basic',
                'error': str(e)
            }

    def _generate_cache_key(self, input_data: Union[np.ndarray, Image.Image, torch.Tensor],
                           model_type: str,
                           output_format: str) -> str:
        """
        ìºì‹œ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        try:
            # ì…ë ¥ ë°ì´í„° í•´ì‹œ ìƒì„± (ê°„ë‹¨í•œ ë°©ë²•)
            if isinstance(input_data, np.ndarray):
                # numpy arrayì˜ í‰ê· ê°’ê³¼ í‘œì¤€í¸ì°¨ë¡œ í•´ì‹œ ìƒì„±
                hash_value = f"{input_data.mean():.6f}_{input_data.std():.6f}_{input_data.shape}"
            elif isinstance(input_data, Image.Image):
                # PIL Imageì˜ í¬ê¸°ì™€ ëª¨ë“œë¡œ í•´ì‹œ ìƒì„±
                hash_value = f"{input_data.size}_{input_data.mode}"
            elif isinstance(input_data, torch.Tensor):
                # torch tensorì˜ í†µê³„ë¡œ í•´ì‹œ ìƒì„±
                hash_value = f"{input_data.mean().item():.6f}_{input_data.std().item():.6f}_{input_data.shape}"
            else:
                hash_value = str(hash(str(input_data)))
            
            return f"{model_type}_{output_format}_{hash_value}"
            
        except Exception as e:
            logger.warning(f"âš ï¸ Cache key generation failed: {e}")
            return f"{model_type}_{output_format}_{hash(str(input_data))}"

    def _add_to_cache(self, key: str, result: Dict[str, Any]):
        """
        ê²°ê³¼ë¥¼ ìºì‹œì— ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        try:
            # ìºì‹œ í¬ê¸° ì œí•œ í™•ì¸
            if len(self._output_cache) >= self._max_cache_size:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = next(iter(self._output_cache))
                del self._output_cache[oldest_key]
                logger.debug("ğŸ—‘ï¸ Oldest cache entry removed")
            
            self._output_cache[key] = result
            logger.debug(f"ğŸ’¾ Result cached: {key}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Cache addition failed: {e}")

    def _log_batch_statistics(self, results: List[Dict[str, Any]], total_time: float):
        """
        ë°°ì¹˜ ì²˜ë¦¬ í†µê³„ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.
        """
        try:
            if not results:
                return
            
            # ì„±ê³µí•œ ìƒì„± ìˆ˜
            successful_results = [r for r in results if 'error' not in r]
            error_count = len(results) - len(successful_results)
            
            # í’ˆì§ˆ ë“±ê¸‰ë³„ ë¶„í¬
            grade_counts = {}
            for result in successful_results:
                grade = result.get('quality_grade', 'Unknown')
                grade_counts[grade] = grade_counts.get(grade, 0) + 1
            
            # í‰ê·  í’ˆì§ˆ ì ìˆ˜
            quality_scores = [r.get('quality_score', 0.0) for r in successful_results if 'quality_score' in r]
            avg_quality = np.mean(quality_scores) if quality_scores else 0.0
            
            # í†µê³„ ë¡œê¹…
            logger.info(f"ğŸ“Š Batch Statistics:")
            logger.info(f"   Total inputs: {len(results)}")
            logger.info(f"   Successful: {len(successful_results)}")
            logger.info(f"   Errors: {error_count}")
            logger.info(f"   Average quality score: {avg_quality:.3f}")
            logger.info(f"   Quality grades: {grade_counts}")
            logger.info(f"   Total processing time: {total_time:.2f}s")
            logger.info(f"   Average time per input: {total_time/len(results):.3f}s")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Statistics logging failed: {e}")

    def get_service_info(self) -> Dict[str, Any]:
        """
        ì„œë¹„ìŠ¤ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        return {
            'service_name': 'FinalOutputService',
            'version': '1.0.0',
            'supported_models': self.inference_engine.supported_models if self.inference_engine else [],
            'cache_enabled': self.service_config['enable_caching'],
            'cache_size': len(self._output_cache),
            'max_cache_size': self._max_cache_size,
            'default_model': self.service_config['default_model'],
            'output_formats': self.service_config['output_formats'],
            'quality_threshold': self.service_config['quality_threshold']
        }

    def clear_cache(self):
        """
        ìºì‹œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
        """
        try:
            cache_size = len(self._output_cache)
            self._output_cache.clear()
            logger.info(f"ğŸ—‘ï¸ Cache cleared: {cache_size} entries removed")
        except Exception as e:
            logger.error(f"âŒ Cache clearing failed: {e}")

    def update_service_config(self, **kwargs):
        """
        ì„œë¹„ìŠ¤ ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        """
        try:
            for key, value in kwargs.items():
                if key in self.service_config:
                    self.service_config[key] = value
                    logger.info(f"âœ… Service config updated: {key} = {value}")
                else:
                    logger.warning(f"âš ï¸ Unknown config key: {key}")
        except Exception as e:
            logger.error(f"âŒ Service config update failed: {e}")

    def validate_output_quality(self, output: torch.Tensor, 
                              threshold: float = None) -> Dict[str, Any]:
        """
        ì¶œë ¥ í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤.
        
        Args:
            output: ì¶œë ¥ í…ì„œ
            threshold: í’ˆì§ˆ ì„ê³„ê°’
            
        Returns:
            í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
        """
        try:
            if threshold is None:
                threshold = self.service_config['quality_threshold']
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            if len(output.shape) == 4:
                output = output.squeeze(0)
            
            # ë°ê¸°
            brightness = output.mean().item()
            
            # ëŒ€ë¹„
            contrast = output.std().item()
            
            # ì„ ëª…ë„
            if output.shape[0] == 3:  # RGB
                gray = 0.299 * output[0] + 0.587 * output[1] + 0.114 * output[2]
            else:
                gray = output[0]
            
            # ë¼í”Œë¼ì‹œì•ˆ í•„í„°
            laplacian_kernel = torch.tensor([
                [0, 1, 0],
                [1, -4, 1],
                [0, 1, 0]
            ], dtype=torch.float32, device=output.device).unsqueeze(0).unsqueeze(0)
            
            laplacian = torch.conv2d(gray.unsqueeze(1), laplacian_kernel, padding=1)
            sharpness = laplacian.var().item()
            
            # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
            quality_score = min(1.0, (brightness * 0.3 + contrast * 0.3 + sharpness * 0.4))
            
            # í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
            validation_result = {
                'quality_score': quality_score,
                'brightness': brightness,
                'contrast': contrast,
                'sharpness': sharpness,
                'meets_threshold': quality_score >= threshold,
                'threshold': threshold,
                'validation_passed': quality_score >= threshold
            }
            
            return validation_result
            
        except Exception as e:
            logger.error(f"âŒ Output quality validation failed: {e}")
            return {
                'quality_score': 0.0,
                'validation_passed': False,
                'error': str(e)
            }

    def export_output(self, output: torch.Tensor, 
                     output_format: str = 'png',
                     file_path: str = None,
                     **kwargs) -> Dict[str, Any]:
        """
        ì¶œë ¥ì„ íŒŒì¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.
        
        Args:
            output: ì¶œë ¥ í…ì„œ
            output_format: ì¶œë ¥ í˜•ì‹
            file_path: íŒŒì¼ ê²½ë¡œ
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            ë‚´ë³´ë‚´ê¸° ê²°ê³¼
        """
        try:
            # í…ì„œë¥¼ numpy arrayë¡œ ë³€í™˜
            if len(output.shape) == 4:
                output = output.squeeze(0)
            
            if output.shape[0] == 3:  # RGB
                output_array = output.permute(1, 2, 0).cpu().numpy()
            else:
                output_array = output.squeeze(0).cpu().numpy()
            
            # ì •ê·œí™”
            if output_array.max() <= 1.0:
                output_array = (output_array * 255).astype(np.uint8)
            else:
                output_array = output_array.astype(np.uint8)
            
            # PIL Imageë¡œ ë³€í™˜
            if output_array.shape[2] == 3:
                image = Image.fromarray(output_array, 'RGB')
            else:
                image = Image.fromarray(output_array, 'L')
            
            # íŒŒì¼ ê²½ë¡œ ì„¤ì •
            if file_path is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                file_path = f"final_output_{timestamp}.{output_format}"
            
            # íŒŒì¼ ì €ì¥
            save_kwargs = {}
            if output_format == 'jpg':
                save_kwargs['quality'] = self.service_config['compression_quality']
            
            image.save(file_path, format=output_format.upper(), **save_kwargs)
            
            logger.info(f"âœ… Output exported: {file_path}")
            
            return {
                'file_path': file_path,
                'output_format': output_format,
                'file_size_mb': Path(file_path).stat().st_size / (1024 * 1024),
                'export_successful': True
            }
            
        except Exception as e:
            logger.error(f"âŒ Output export failed: {e}")
            return {
                'export_successful': False,
                'error': str(e)
            }
