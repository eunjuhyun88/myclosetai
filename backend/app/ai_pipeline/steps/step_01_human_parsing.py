# backend/app/ai_pipeline/steps/step_01_human_parsing.py
"""
1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± (Human Parsing) - M3 Max ìµœì í™” ë²„ì „ (ìˆ˜ì •ë¨)
- Graphonomy ëª¨ë¸ ê¸°ë°˜ 20ê°œ ë¶€ìœ„ ë¶„í• 
- M3 Max Neural Engine í™œìš© ìµœì í™”
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬ ë° ì‹¤ì‹œê°„ ìºì‹±
- ê¸°ì¡´ AI íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ì™€ ì™„ë²½ í˜¸í™˜
- device ì¸ì ë¬¸ì œ í•´ê²°
"""

import os
import logging
import time
import asyncio
from typing import Dict, Any, Optional, Tuple, List
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
import cv2
from concurrent.futures import ThreadPoolExecutor

# ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° import - ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
try:
    from ..utils.model_loader import ModelLoader, ModelConfig
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
    
except ImportError:
    # í´ë°± - ì „ì—­ì—ì„œ ê°€ì ¸ì˜¤ê¸°
    from app.ai_pipeline.utils.model_loader import ModelLoader, ModelConfig
    from app.ai_pipeline.utils.memory_manager import MemoryManager
    from app.ai_pipeline.utils.data_converter import DataConverter

# Apple Metal Performance Shaders ì§€ì›
try:
    import torch.backends.mps
    MPS_AVAILABLE = torch.backends.mps.is_available()
except ImportError:
    MPS_AVAILABLE = False

# CoreML ì§€ì› (ì„ íƒì‚¬í•­)
try:
    import coremltools as ct
    COREML_AVAILABLE = True
except ImportError:
    COREML_AVAILABLE = False

logger = logging.getLogger(__name__)

class HumanParsingStep:
    """1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± - M3 Max ìµœì í™” (ìˆ˜ì •ëœ ë²„ì „)"""
    
    # LIP (Look Into Person) ë°ì´í„°ì…‹ ê¸°ë°˜ 20ê°œ ë¶€ìœ„ ë¼ë²¨
    BODY_PARTS = {
        0: "Background",
        1: "Hat", 2: "Hair", 3: "Glove", 4: "Sunglasses",
        5: "Upper-clothes", 6: "Dress", 7: "Coat", 8: "Socks",
        9: "Pants", 10: "Jumpsuits", 11: "Scarf", 12: "Skirt",
        13: "Face", 14: "Left-arm", 15: "Right-arm",
        16: "Left-leg", 17: "Right-leg", 18: "Left-shoe", 19: "Right-shoe"
    }
    
    # ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ë‹¤ìŒ ë‹¨ê³„ë“¤ì„ ìœ„í•œ)
    CLOTHING_CATEGORIES = {
        "upper": [5, 7],      # Upper-clothes, Coat
        "lower": [9, 12],     # Pants, Skirt
        "dress": [6],         # Dress
        "full_body": [10],    # Jumpsuits
        "accessories": [1, 3, 4, 8, 11, 18, 19]  # Hat, Glove, etc.
    }
    
    def __init__(self, device: str, config: Optional[Dict[str, Any]] = None):
        """
        ìˆ˜ì •ëœ ìƒì„±ì - deviceë¥¼ ì²« ë²ˆì§¸ ì¸ìë¡œ ë°›ìŒ
        
        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cpu', 'cuda', 'mps')  # ì²« ë²ˆì§¸ ì¸ì
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ì„ íƒì )  # ë‘ ë²ˆì§¸ ì¸ì
        """
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._get_optimal_device(device)
        self.config = config or {}
        
        # ëª¨ë¸ ë¡œë” ìƒì„± (ë‚´ë¶€ì—ì„œ ìƒì„±)
        self.model_loader = self._create_model_loader()
        self.memory_manager = self._create_memory_manager()
        self.data_converter = self._create_data_converter()
        
        # M3 Max ìµœì í™” ì„¤ì •
        self.use_coreml = self.config.get('use_coreml', True) and COREML_AVAILABLE
        self.enable_quantization = self.config.get('enable_quantization', True)
        
        # ëª¨ë¸ ì„¤ì •
        self.input_size = self.config.get('input_size', (512, 512))
        self.num_classes = self.config.get('num_classes', 20)
        self.model_name = self.config.get('model_name', 'graphonomy')
        self.model_path = self.config.get('model_path', 'ai_models/checkpoints/human_parsing')
        
        # ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° (ImageNet í‘œì¤€)
        self.mean = np.array([0.485, 0.456, 0.406])
        self.std = np.array([0.229, 0.224, 0.225])
        
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        self.pytorch_model = None
        self.coreml_model = None
        self.is_initialized = False
        
        # ì„±ëŠ¥ ìµœì í™”
        self.batch_size = self.config.get('batch_size', 1)
        self.cache_size = self.config.get('cache_size', 50)
        self.result_cache = {}
        
        # ë³‘ë ¬ ì²˜ë¦¬
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            "total_inferences": 0,
            "average_time": 0.0,
            "cache_hits": 0,
            "model_switches": 0
        }
        
        logger.info(f"ğŸ¯ 1ë‹¨ê³„ ì¸ì²´ íŒŒì‹± ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _create_model_loader(self) -> ModelLoader:
        """ëª¨ë¸ ë¡œë” ìƒì„± - ì•ˆì „í•œ ë°©ì‹"""
        try:
            return ModelLoader(device=self.device)
        except Exception as e:
            logger.warning(f"ëª¨ë¸ ë¡œë” ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ë¡œë” ì‚¬ìš©: {e}")
            # ê¸°ë³¸ ëª¨ë¸ ë¡œë” í´ë˜ìŠ¤
            class SimpleModelLoader:
                def __init__(self, device):
                    self.device = device
                
                async def load_model(self, model_name: str, config: Any = None):
                    # ë”ë¯¸ ëª¨ë¸ ë°˜í™˜
                    return self._create_dummy_model()
                
                def _create_dummy_model(self):
                    class DummyModel(torch.nn.Module):
                        def __init__(self):
                            super().__init__()
                            self.conv = torch.nn.Conv2d(3, 20, 1)
                        
                        def forward(self, x):
                            return self.conv(x)
                    
                    return DummyModel()
            
            return SimpleModelLoader(self.device)
    
    def _create_memory_manager(self):
        """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ìƒì„± - ì•ˆì „í•œ ë°©ì‹"""
        try:
            return MemoryManager(self.device)
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ë§¤ë‹ˆì € ì‚¬ìš©: {e}")
            # ê¸°ë³¸ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €
            class SimpleMemoryManager:
                def __init__(self, device):
                    self.device = device
                
                async def get_usage_stats(self):
                    return {"memory_used": "N/A"}
                
                async def cleanup(self):
                    pass
            
            return SimpleMemoryManager(self.device)
    
    def _create_data_converter(self):
        """ë°ì´í„° ì»¨ë²„í„° ìƒì„± - ì•ˆì „í•œ ë°©ì‹"""
        try:
            return DataConverter()
        except Exception as e:
            logger.warning(f"ë°ì´í„° ì»¨ë²„í„° ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ì»¨ë²„í„° ì‚¬ìš©: {e}")
            # ê¸°ë³¸ ë°ì´í„° ì»¨ë²„í„°
            class SimpleDataConverter:
                def convert(self, data):
                    return data
            
            return SimpleDataConverter()
    
    def _get_optimal_device(self, preferred_device: str) -> str:
        """M3 Maxì— ìµœì í™”ëœ ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if preferred_device == "mps" and MPS_AVAILABLE:
            logger.info("ğŸš€ Apple Metal Performance Shaders í™œì„±í™”")
            return "mps"
        elif preferred_device == "cuda" and torch.cuda.is_available():
            return "cuda"
        else:
            return "cpu"
    
    async def initialize(self) -> bool:
        """ëª¨ë¸ ì´ˆê¸°í™” (ë¹„ë™ê¸°) - ìˆ˜ì •ëœ ë²„ì „"""
        try:
            logger.info("ğŸ”„ 1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            # CoreML ëª¨ë¸ ìš°ì„  ë¡œë“œ
            if self.use_coreml:
                coreml_loaded = await self._load_coreml_model()
                if coreml_loaded:
                    logger.info("âœ… CoreML ëª¨ë¸ ë¡œë“œ ì„±ê³µ (Neural Engine ê°€ì†)")
            
            # PyTorch ëª¨ë¸ ë¡œë“œ (ë°±ì—… ë˜ëŠ” ë³‘í–‰)
            pytorch_loaded = await self._load_pytorch_model()
            
            if not (self.coreml_model or self.pytorch_model):
                logger.warning("âš ï¸ ì‹¤ì œ ëª¨ë¸ ì—†ìŒ - ë°ëª¨ ëª¨ë¸ë¡œ ì§„í–‰")
                self.pytorch_model = self._create_demo_model()
                self.pytorch_model = self.pytorch_model.to(self.device)
                self.pytorch_model.eval()
            
            # ëª¨ë¸ ì›Œë°ì—…
            await self._warmup_models()
            
            self.is_initialized = True
            logger.info("âœ… 1ë‹¨ê³„ ì¸ì²´ íŒŒì‹± ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ 1ë‹¨ê³„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ë°ëª¨ ëª¨ë“œë¡œë¼ë„ ì§„í–‰
            try:
                self.pytorch_model = self._create_demo_model()
                self.pytorch_model = self.pytorch_model.to(self.device)
                self.pytorch_model.eval()
                self.is_initialized = True
                logger.info("âœ… 1ë‹¨ê³„ ë°ëª¨ ëª¨ë“œë¡œ ì´ˆê¸°í™” ì™„ë£Œ")
                return True
            except Exception as e2:
                logger.error(f"âŒ ë°ëª¨ ëª¨ë“œ ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {e2}")
                self.is_initialized = False
                return False
    
    async def _load_coreml_model(self) -> bool:
        """CoreML ëª¨ë¸ ë¡œë“œ (Neural Engine í™œìš©)"""
        coreml_path = os.path.join(self.model_path, "human_parser_optimized.mlpackage")
        
        if not COREML_AVAILABLE:
            logger.warning("âš ï¸ CoreML ì§€ì› ì•ˆë¨")
            return False
        
        if os.path.exists(coreml_path):
            try:
                def _load_coreml():
                    return ct.models.MLModel(coreml_path)
                
                loop = asyncio.get_event_loop()
                self.coreml_model = await loop.run_in_executor(self.executor, _load_coreml)
                return True
                
            except Exception as e:
                logger.warning(f"âš ï¸ CoreML ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            logger.info("ğŸ“¦ CoreML ëª¨ë¸ ì—†ìŒ - PyTorch ìš°ì„  ì‚¬ìš©")
        
        return False
    
    async def _load_pytorch_model(self) -> bool:
        """PyTorch ëª¨ë¸ ë¡œë“œ - ìˆ˜ì •ëœ ë²„ì „"""
        try:
            # ModelConfig ê°ì²´ ìƒì„± (config.get_hash() ì—ëŸ¬ í•´ê²°)
            model_config = ModelConfig(
                name="human_parsing",
                model_type=self.model_name,
                device=self.device,
                use_fp16=True,
                max_memory_gb=4.0
            )
            
            # ëª¨ë¸ ë¡œë”ë¥¼ í†µí•œ ë¡œë“œ
            try:
                self.pytorch_model = await self.model_loader.load_model(
                    "human_parsing", 
                    model_config  # ModelConfig ê°ì²´ ì „ë‹¬
                )
            except Exception as e:
                logger.warning(f"âš ï¸ ëª¨ë¸ ë¡œë”ë¥¼ í†µí•œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                logger.warning("âš ï¸ Graphonomy ëª¨ë¸ ì—†ìŒ - ë°ëª¨ ëª¨ë¸ ìƒì„±")
                self.pytorch_model = self._create_demo_model()
            
            if self.pytorch_model is None:
                logger.warning("ëª¨ë¸ ë¡œë“œ ê²°ê³¼ê°€ None - ë°ëª¨ ëª¨ë¸ ìƒì„±")
                self.pytorch_model = self._create_demo_model()
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.pytorch_model = self.pytorch_model.to(self.device)
            
            # M3 Max ìµœì í™”
            if self.device == "mps":
                # MPS ìµœì í™”
                self.pytorch_model = self._optimize_for_mps(self.pytorch_model)
            elif self.enable_quantization and self.device == "cpu":
                # CPU ì–‘ìí™”
                self.pytorch_model = self._quantize_model(self.pytorch_model)
            
            self.pytorch_model.eval()
            return True
            
        except Exception as e:
            logger.error(f"âŒ PyTorch ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _create_demo_model(self):
        """ë°ëª¨ìš© ê²½ëŸ‰ ì¸ì²´ íŒŒì‹± ëª¨ë¸ - ì•ˆì •í™”ëœ ë²„ì „"""
        
        class EfficientHumanParser(torch.nn.Module):
            """MobileNet ê¸°ë°˜ ê²½ëŸ‰ ì¸ì²´ íŒŒì‹± ëª¨ë¸"""
            
            def __init__(self, num_classes=20):
                super().__init__()
                
                # ê°„ë‹¨í•œ backbone
                self.backbone = torch.nn.Sequential(
                    # Initial conv
                    torch.nn.Conv2d(3, 32, 3, stride=2, padding=1, bias=False),
                    torch.nn.BatchNorm2d(32),
                    torch.nn.ReLU(inplace=True),
                    
                    # Feature extraction
                    torch.nn.Conv2d(32, 64, 3, stride=2, padding=1, bias=False),
                    torch.nn.BatchNorm2d(64),
                    torch.nn.ReLU(inplace=True),
                    
                    torch.nn.Conv2d(64, 128, 3, stride=2, padding=1, bias=False),
                    torch.nn.BatchNorm2d(128),
                    torch.nn.ReLU(inplace=True),
                    
                    torch.nn.Conv2d(128, 256, 3, stride=2, padding=1, bias=False),
                    torch.nn.BatchNorm2d(256),
                    torch.nn.ReLU(inplace=True),
                )
                
                # Simple decoder
                self.decoder = torch.nn.Sequential(
                    torch.nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
                    torch.nn.ReLU(inplace=True),
                    torch.nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
                    torch.nn.ReLU(inplace=True),
                    torch.nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),
                    torch.nn.ReLU(inplace=True),
                    torch.nn.ConvTranspose2d(32, num_classes, 4, stride=2, padding=1)
                )
            
            def forward(self, x):
                # Encoder
                features = self.backbone(x)
                
                # Decoder
                out = self.decoder(features)
                
                # ìµœì¢… í¬ê¸° ë§ì¶¤
                out = F.interpolate(out, size=x.shape[2:], mode='bilinear', align_corners=False)
                
                return out
        
        return EfficientHumanParser(self.num_classes)
    
    def _optimize_for_mps(self, model):
        """M3 Max MPS ìµœì í™”"""
        # MPSì— ìµœì í™”ëœ ì„¤ì • ì ìš©
        if hasattr(model, 'eval'):
            model.eval()
        
        # Mixed precisionì„ ìœ„í•œ ì¤€ë¹„ (M3 MaxëŠ” FP16 ì§€ì›)
        for param in model.parameters():
            param.requires_grad_(False)
        
        return model
    
    def _quantize_model(self, model):
        """ëª¨ë¸ ì–‘ìí™” (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)"""
        try:
            quantized_model = torch.quantization.quantize_dynamic(
                model, 
                {torch.nn.Linear, torch.nn.Conv2d}, 
                dtype=torch.qint8
            )
            logger.info("âœ… ëª¨ë¸ ì–‘ìí™” ì™„ë£Œ")
            return quantized_model
        except Exception as e:
            logger.warning(f"âš ï¸ ì–‘ìí™” ì‹¤íŒ¨: {e}")
            return model
    
    async def _warmup_models(self):
        """ëª¨ë¸ ì›Œë°ì—… (ì²« ì¶”ë¡  ìµœì í™”)"""
        logger.info("ğŸ”¥ 1ë‹¨ê³„ ëª¨ë¸ ì›Œë°ì—… ì¤‘...")
        
        try:
            # ë”ë¯¸ ì…ë ¥ ìƒì„±
            dummy_input = torch.randn(1, 3, *self.input_size)
            
            # PyTorch ëª¨ë¸ ì›Œë°ì—…
            if self.pytorch_model:
                dummy_input = dummy_input.to(self.device)
                with torch.no_grad():
                    _ = self.pytorch_model(dummy_input)
            
            # CoreML ëª¨ë¸ ì›Œë°ì—…
            if self.coreml_model:
                dummy_np = dummy_input.cpu().numpy()
                try:
                    _ = self.coreml_model.predict({"input": dummy_np})
                except Exception as e:
                    logger.warning(f"CoreML ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            logger.info("âœ… 1ë‹¨ê³„ ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def process(self, person_image_tensor: torch.Tensor) -> Dict[str, Any]:
        """
        1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± ì²˜ë¦¬ (ë¹„ë™ê¸°) - ì•ˆì •í™”ëœ ë²„ì „
        
        Args:
            person_image_tensor: ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ [1, 3, H, W]
            
        Returns:
            Dict[str, Any]: ì²˜ë¦¬ ê²°ê³¼
        """
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        
        try:
            # ìºì‹œ í™•ì¸
            cache_key = self._get_cache_key(person_image_tensor)
            if cache_key in self.result_cache:
                self.stats["cache_hits"] += 1
                logger.info("ğŸ’¾ 1ë‹¨ê³„: ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜")
                cached_result = self.result_cache[cache_key].copy()
                cached_result["from_cache"] = True
                return cached_result
            
            # ì…ë ¥ ì „ì²˜ë¦¬
            preprocessed_tensor = await self._preprocess_input(person_image_tensor)
            
            # ëª¨ë¸ ì¶”ë¡ 
            parsing_output = await self._run_inference(preprocessed_tensor)
            
            # í›„ì²˜ë¦¬ ë° ê²°ê³¼ ìƒì„±
            result = await self._postprocess_result(
                parsing_output, 
                person_image_tensor.shape[2:],
                start_time
            )
            
            # ìºì‹œ ì €ì¥
            self._update_cache(cache_key, result)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(time.time() - start_time)
            
            logger.info(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ - {result['processing_time']:.3f}ì´ˆ")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ 1ë‹¨ê³„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # í´ë°± ê²°ê³¼ ë°˜í™˜
            processing_time = time.time() - start_time
            return {
                "parsing_map": np.zeros(person_image_tensor.shape[2:], dtype=np.uint8),
                "body_masks": {},
                "clothing_regions": {"categories_detected": [], "dominant_category": None},
                "confidence": 0.5,
                "body_parts_detected": {},
                "processing_time": processing_time,
                "step_info": {
                    "step_name": "human_parsing",
                    "step_number": 1,
                    "model_used": "fallback",
                    "device": self.device,
                    "error": str(e)
                },
                "from_cache": False,
                "success": False
            }
    
    async def _preprocess_input(self, image_tensor: torch.Tensor) -> torch.Tensor:
        """ì…ë ¥ ì „ì²˜ë¦¬"""
        def _preprocess():
            # í¬ê¸° ì¡°ì •
            if image_tensor.shape[2:] != self.input_size:
                resized = F.interpolate(
                    image_tensor, 
                    size=self.input_size, 
                    mode='bilinear', 
                    align_corners=False
                )
            else:
                resized = image_tensor
            
            # ì •ê·œí™” (0-1 ë²”ìœ„ ê°€ì •)
            if resized.max() > 1.0:
                resized = resized / 255.0
            
            # ImageNet ì •ê·œí™”
            mean = torch.tensor(self.mean).view(1, 3, 1, 1)
            std = torch.tensor(self.std).view(1, 3, 1, 1)
            
            if self.device != "cpu":
                mean = mean.to(self.device)
                std = std.to(self.device)
            
            normalized = (resized - mean) / std
            
            return normalized.to(self.device)
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, _preprocess)
    
    async def _run_inference(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ - ì•ˆì •í™”ëœ ë²„ì „"""
        # CoreML ëª¨ë¸ ìš°ì„  ì‚¬ìš© (Neural Engine ê°€ì†)
        if self.coreml_model:
            try:
                def _coreml_inference():
                    input_np = input_tensor.cpu().numpy()
                    result = self.coreml_model.predict({"input": input_np})
                    # CoreML ì¶œë ¥ í‚¤ëŠ” ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
                    output_key = list(result.keys())[0]
                    return torch.from_numpy(result[output_key]).to(self.device)
                
                loop = asyncio.get_event_loop()
                output = await loop.run_in_executor(self.executor, _coreml_inference)
                logger.debug("ğŸš€ CoreML ì¶”ë¡  ì™„ë£Œ")
                return output
                
            except Exception as e:
                logger.warning(f"âš ï¸ CoreML ì¶”ë¡  ì‹¤íŒ¨, PyTorchë¡œ ì „í™˜: {e}")
                self.stats["model_switches"] += 1
        
        # PyTorch ëª¨ë¸ ì‚¬ìš©
        if self.pytorch_model:
            try:
                with torch.no_grad():
                    output = self.pytorch_model(input_tensor)
                    logger.debug("ğŸ”¥ PyTorch ì¶”ë¡  ì™„ë£Œ")
                    return output
            except Exception as e:
                logger.error(f"PyTorch ì¶”ë¡  ì‹¤íŒ¨: {e}")
                # í´ë°± - ë”ë¯¸ ê²°ê³¼ ìƒì„±
                batch_size, _, height, width = input_tensor.shape
                dummy_output = torch.randn(batch_size, self.num_classes, height, width)
                return dummy_output.to(self.device)
        
        # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš°
        logger.error("ëª¨ë“  ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨ - ë”ë¯¸ ê²°ê³¼ ìƒì„±")
        batch_size, _, height, width = input_tensor.shape
        dummy_output = torch.randn(batch_size, self.num_classes, height, width)
        return dummy_output.to(self.device)
    
    async def _postprocess_result(self, output: torch.Tensor, original_size: Tuple[int, int], 
                                 start_time: float) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        def _postprocess():
            # í™•ë¥ ì„ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
            parsing_map = torch.argmax(output, dim=1).squeeze().cpu().numpy()
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
            if parsing_map.shape != original_size:
                parsing_map = cv2.resize(
                    parsing_map.astype(np.uint8),
                    (original_size[1], original_size[0]),
                    interpolation=cv2.INTER_NEAREST
                )
            
            # ë…¸ì´ì¦ˆ ì œê±° (ëª¨í´ë¡œì§€ ì—°ì‚°)
            try:
                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                parsing_map = cv2.morphologyEx(parsing_map, cv2.MORPH_CLOSE, kernel)
                parsing_map = cv2.morphologyEx(parsing_map, cv2.MORPH_OPEN, kernel)
            except Exception as e:
                logger.warning(f"ëª¨í´ë¡œì§€ ì—°ì‚° ì‹¤íŒ¨: {e}")
            
            return parsing_map
        
        loop = asyncio.get_event_loop()
        parsing_map = await loop.run_in_executor(self.executor, _postprocess)
        
        # ë¶€ìœ„ë³„ ë§ˆìŠ¤í¬ ìƒì„±
        body_masks = self._create_body_masks(parsing_map)
        
        # ì˜ë¥˜ ì˜ì—­ ë¶„ì„
        clothing_regions = self._analyze_clothing_regions(parsing_map)
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_confidence(output)
        
        processing_time = time.time() - start_time
        
        return {
            "parsing_map": parsing_map.astype(np.uint8),
            "body_masks": body_masks,
            "clothing_regions": clothing_regions,
            "confidence": float(confidence),
            "body_parts_detected": self._get_detected_parts(parsing_map),
            "processing_time": processing_time,
            "step_info": {
                "step_name": "human_parsing",
                "step_number": 1,
                "model_used": "CoreML" if self.coreml_model else "PyTorch",
                "device": self.device,
                "input_size": self.input_size,
                "num_classes": self.num_classes
            },
            "from_cache": False,
            "success": True
        }
    
    def _create_body_masks(self, parsing_map: np.ndarray) -> Dict[str, np.ndarray]:
        """ì‹ ì²´ ë¶€ìœ„ë³„ ë§ˆìŠ¤í¬ ìƒì„±"""
        body_masks = {}
        
        for part_id, part_name in self.BODY_PARTS.items():
            if part_id == 0:  # ë°°ê²½ ì œì™¸
                continue
            
            mask = (parsing_map == part_id).astype(np.uint8)
            if mask.sum() > 0:
                # í‚¤ ì´ë¦„ ì •ê·œí™”
                key_name = part_name.lower().replace('-', '_').replace(' ', '_')
                body_masks[key_name] = mask
        
        return body_masks
    
    def _analyze_clothing_regions(self, parsing_map: np.ndarray) -> Dict[str, Any]:
        """ì˜ë¥˜ ì˜ì—­ ë¶„ì„ (ë‹¤ìŒ ë‹¨ê³„ë“¤ì„ ìœ„í•œ)"""
        analysis = {
            "categories_detected": [],
            "coverage_ratio": {},
            "bounding_boxes": {},
            "dominant_category": None
        }
        
        total_pixels = parsing_map.size
        max_coverage = 0.0
        
        for category, part_ids in self.CLOTHING_CATEGORIES.items():
            category_mask = np.zeros_like(parsing_map, dtype=bool)
            
            for part_id in part_ids:
                category_mask |= (parsing_map == part_id)
            
            if category_mask.sum() > 0:
                coverage = category_mask.sum() / total_pixels
                
                analysis["categories_detected"].append(category)
                analysis["coverage_ratio"][category] = coverage
                analysis["bounding_boxes"][category] = self._get_bounding_box(category_mask)
                
                if coverage > max_coverage:
                    max_coverage = coverage
                    analysis["dominant_category"] = category
        
        return analysis
    
    def _calculate_confidence(self, output: torch.Tensor) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            max_probs = torch.max(F.softmax(output, dim=1), dim=1)[0]
            confidence = torch.mean(max_probs).item()
            return confidence
        except Exception as e:
            logger.warning(f"ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.8  # ê¸°ë³¸ê°’
    
    def _get_detected_parts(self, parsing_map: np.ndarray) -> Dict[str, Any]:
        """ê°ì§€ëœ ì‹ ì²´ ë¶€ìœ„ ì •ë³´"""
        detected_parts = {}
        
        for part_id, part_name in self.BODY_PARTS.items():
            if part_id == 0:
                continue
            
            mask = (parsing_map == part_id)
            pixel_count = mask.sum()
            
            if pixel_count > 0:
                key_name = part_name.lower().replace('-', '_').replace(' ', '_')
                detected_parts[key_name] = {
                    "pixel_count": int(pixel_count),
                    "percentage": float(pixel_count / parsing_map.size * 100),
                    "bounding_box": self._get_bounding_box(mask),
                    "part_id": part_id
                }
        
        return detected_parts
    
    def _get_bounding_box(self, mask: np.ndarray) -> Dict[str, int]:
        """ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
        coords = np.where(mask)
        if len(coords[0]) == 0:
            return {"x": 0, "y": 0, "width": 0, "height": 0}
        
        y_min, y_max = coords[0].min(), coords[0].max()
        x_min, x_max = coords[1].min(), coords[1].max()
        
        return {
            "x": int(x_min),
            "y": int(y_min),
            "width": int(x_max - x_min),
            "height": int(y_max - y_min)
        }
    
    def _get_cache_key(self, tensor: torch.Tensor) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            tensor_hash = hash(tensor.cpu().numpy().tobytes())
            return f"step01_{tensor_hash}_{self.input_size[0]}x{self.input_size[1]}"
        except Exception as e:
            logger.warning(f"ìºì‹œ í‚¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"step01_fallback_{time.time()}"
    
    def _update_cache(self, key: str, result: Dict[str, Any]):
        """ê²°ê³¼ ìºì‹± (LRU ë°©ì‹)"""
        try:
            if len(self.result_cache) >= self.cache_size:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = next(iter(self.result_cache))
                del self.result_cache[oldest_key]
            
            # ìºì‹œì— ì €ì¥í•  ë•ŒëŠ” ë¬´ê±°ìš´ ë°ì´í„°ëŠ” ë³µì‚¬ë³¸ ìƒì„±
            cached_result = {
                k: v.copy() if isinstance(v, (np.ndarray, dict)) else v 
                for k, v in result.items()
            }
            self.result_cache[key] = cached_result
        except Exception as e:
            logger.warning(f"ìºì‹œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _update_stats(self, processing_time: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.stats["total_inferences"] += 1
        current_avg = self.stats["average_time"]
        new_avg = (current_avg * (self.stats["total_inferences"] - 1) + 
                  processing_time) / self.stats["total_inferences"]
        self.stats["average_time"] = new_avg
    
    def get_clothing_mask(self, parsing_map: np.ndarray, category: str) -> np.ndarray:
        """íŠ¹ì • ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ì˜ í†µí•© ë§ˆìŠ¤í¬ ë°˜í™˜ (ë‹¤ìŒ ë‹¨ê³„ë“¤ì„ ìœ„í•œ)"""
        if category not in self.CLOTHING_CATEGORIES:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬: {category}")
        
        combined_mask = np.zeros_like(parsing_map, dtype=np.uint8)
        
        for part_id in self.CLOTHING_CATEGORIES[category]:
            combined_mask |= (parsing_map == part_id).astype(np.uint8)
        
        return combined_mask
    
    def visualize_parsing(self, parsing_map: np.ndarray) -> np.ndarray:
        """íŒŒì‹± ê²°ê³¼ ì‹œê°í™” (ë””ë²„ê¹…ìš©)"""
        # 20ê°œ ë¶€ìœ„ë³„ ìƒ‰ìƒ ë§¤í•‘
        colors = np.array([
            [0, 0, 0],       # 0: Background
            [128, 0, 0],     # 1: Hat
            [255, 0, 0],     # 2: Hair
            [0, 85, 0],      # 3: Glove
            [170, 0, 51],    # 4: Sunglasses
            [255, 85, 0],    # 5: Upper-clothes
            [0, 0, 85],      # 6: Dress
            [0, 119, 221],   # 7: Coat
            [85, 85, 0],     # 8: Socks
            [0, 85, 85],     # 9: Pants
            [85, 51, 0],     # 10: Jumpsuits
            [52, 86, 128],   # 11: Scarf
            [0, 128, 0],     # 12: Skirt
            [0, 0, 255],     # 13: Face
            [51, 170, 221],  # 14: Left-arm
            [0, 255, 255],   # 15: Right-arm
            [85, 255, 170],  # 16: Left-leg
            [170, 255, 85],  # 17: Right-leg
            [255, 255, 0],   # 18: Left-shoe
            [255, 170, 0]    # 19: Right-shoe
        ])
        
        colored_parsing = colors[parsing_map]
        return colored_parsing.astype(np.uint8)
    
    async def get_step_stats(self) -> Dict[str, Any]:
        """1ë‹¨ê³„ ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        try:
            memory_stats = await self.memory_manager.get_usage_stats()
        except:
            memory_stats = {"memory_used": "N/A"}
        
        return {
            "step_name": "human_parsing",
            "step_number": 1,
            "performance": self.stats,
            "cache_size": len(self.result_cache),
            "device": self.device,
            "models_available": {
                "pytorch": self.pytorch_model is not None,
                "coreml": self.coreml_model is not None
            },
            "memory_usage": memory_stats,
            "configuration": {
                "input_size": self.input_size,
                "num_classes": self.num_classes,
                "cache_limit": self.cache_size,
                "use_quantization": self.enable_quantization
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("ğŸ§¹ 1ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        try:
            # ëª¨ë¸ ì •ë¦¬
            if self.pytorch_model:
                del self.pytorch_model
                self.pytorch_model = None
            
            if self.coreml_model:
                del self.coreml_model
                self.coreml_model = None
            
            # ìºì‹œ ì •ë¦¬
            self.result_cache.clear()
            
            # ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            self.executor.shutdown(wait=True)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            await self.memory_manager.cleanup()
            
            if self.device == "mps":
                torch.mps.empty_cache()
            elif self.device == "cuda":
                torch.cuda.empty_cache()
            
            self.is_initialized = False
            logger.info("âœ… 1ë‹¨ê³„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        
        except Exception as e:
            logger.warning(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


# íŒ©í† ë¦¬ í•¨ìˆ˜ (ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ í˜¸í™˜) - ìˆ˜ì •ëœ ë²„ì „
async def create_human_parsing_step(
    device: str = "auto",
    config: Dict[str, Any] = None
) -> HumanParsingStep:
    """1ë‹¨ê³„ ì¸ì²´ íŒŒì‹± ìŠ¤í… ìƒì„± - ìˆ˜ì •ëœ ë²„ì „"""
    
    if device == "auto":
        # M3 Max ìë™ ê°ì§€
        if MPS_AVAILABLE:
            device = "mps"
        elif torch.cuda.is_available():
            device = "cuda"
        else:
            device = "cpu"
    
    default_config = {
        "use_coreml": True,
        "enable_quantization": True,
        "input_size": (512, 512),
        "num_classes": 20,
        "cache_size": 50,
        "batch_size": 1,
        "model_name": "graphonomy",
        "model_path": "ai_models/checkpoints/human_parsing"
    }
    
    final_config = {**default_config, **(config or {})}
    
    # deviceë¥¼ ì²« ë²ˆì§¸ ì¸ìë¡œ ì „ë‹¬
    step = HumanParsingStep(device, final_config)
    
    if not await step.initialize():
        logger.warning("1ë‹¨ê³„ ì´ˆê¸°í™” ì‹¤íŒ¨í–ˆì§€ë§Œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    return step