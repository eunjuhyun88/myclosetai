#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 01: Human Parsing v8.0 - Common Imports Integration
=======================================================================

âœ… Common Imports ì‹œìŠ¤í…œ ì™„ì „ í†µí•© - ì¤‘ë³µ import ë¸”ë¡ ì œê±°
âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - ì¤‘ì•™ í—ˆë¸Œ íŒ¨í„´ ì ìš©
âœ… BaseStepMixin v20.0 ì™„ì „ ìƒì† - super().__init__() í˜¸ì¶œ
âœ… í•„ìˆ˜ ì†ì„± ì´ˆê¸°í™” - ai_models, models_loading_status, model_interface, loaded_models
âœ… _load_ai_models_via_central_hub() êµ¬í˜„ - ModelLoaderë¥¼ í†µí•œ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©
âœ… ê°„ì†Œí™”ëœ process() ë©”ì„œë“œ - í•µì‹¬ Human Parsing ë¡œì§ë§Œ
âœ… ì—ëŸ¬ ë°©ì§€ìš© í´ë°± ë¡œì§ - Mock ëª¨ë¸ ìƒì„± (ì‹¤ì œ AI ëª¨ë¸ ëŒ€ì²´ìš©)
âœ… GitHubDependencyManager ì™„ì „ ì‚­ì œ - ë³µì¡í•œ ì˜ì¡´ì„± ê´€ë¦¬ ì½”ë“œ ì œê±°
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - TYPE_CHECKING + ì§€ì—° import
âœ… Graphonomy ëª¨ë¸ ë¡œë”© - 1.2GB ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ì§€ì›
âœ… Human body parsing - 20ê°œ í´ë˜ìŠ¤ ì •í™• ë¶„ë¥˜
âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ - ì™„ì „ êµ¬í˜„

í•µì‹¬ êµ¬í˜„ ê¸°ëŠ¥:
1. Graphonomy ResNet-101 + ASPP ì•„í‚¤í…ì²˜ (ì‹¤ì œ 1.2GB ì²´í¬í¬ì¸íŠ¸)
2. U2Net í´ë°± ëª¨ë¸ (ê²½ëŸ‰í™” ëŒ€ì•ˆ)
3. 20ê°œ ì¸ì²´ ë¶€ìœ„ ì •í™• íŒŒì‹± (ë°°ê²½ í¬í•¨)
4. 512x512 ì…ë ¥ í¬ê¸° í‘œì¤€í™”
5. MPS/CUDA ë””ë°”ì´ìŠ¤ ìµœì í™”

Author: MyCloset AI Team
Date: 2025-07-31
Version: 8.1 (Common Imports Integration)
"""

# ğŸ”¥ Common Imports ì‚¬ìš©
from app.ai_pipeline.utils.common_imports import (
    # í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
    os, sys, gc, logging, threading, traceback, warnings,
    Path, Dict, Any, Optional, Tuple, List, Union, TYPE_CHECKING,
    dataclass, field, Enum, BytesIO, ThreadPoolExecutor,
    
    # AI/ML ë¼ì´ë¸ŒëŸ¬ë¦¬
    torch, nn, F, transforms, TORCH_AVAILABLE, MPS_AVAILABLE,
    np, cv2, NUMPY_AVAILABLE, CV2_AVAILABLE,
    Image, ImageFilter, ImageEnhance, PIL_AVAILABLE,
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    detect_m3_max, get_available_libraries, log_library_status,
    
    # ìƒìˆ˜
    DEVICE_CPU, DEVICE_CUDA, DEVICE_MPS,
    DEFAULT_INPUT_SIZE, DEFAULT_CONFIDENCE_THRESHOLD, DEFAULT_QUALITY_THRESHOLD,
    
    # ì—ëŸ¬ ì²˜ë¦¬
    EXCEPTIONS_AVAILABLE, convert_to_mycloset_exception, track_exception, create_exception_response
)

# ğŸ”¥ ì§ì ‘ import (common_importsì—ì„œ ëˆ„ë½ëœ ëª¨ë“ˆë“¤)
import time

# ğŸ”¥ Human Parsing Step í´ë˜ìŠ¤ìš© time ëª¨ë“ˆ ì¬í™•ì¸
import time as time_module

# ğŸ”¥ Human Parsing ì „ìš© ì—ëŸ¬ ì²˜ë¦¬ í—¬í¼ í•¨ìˆ˜ë“¤ (ì¶”ê°€)
try:
    from app.core.exceptions import (
        handle_human_parsing_model_loading_error, handle_human_parsing_inference_error,
        handle_image_preprocessing_error, create_human_parsing_error_response,
        validate_human_parsing_environment, log_human_parsing_performance
    )
    HUMAN_PARSING_HELPERS_AVAILABLE = True
except ImportError:
    HUMAN_PARSING_HELPERS_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("Human Parsing ì „ìš© ì—ëŸ¬ ì²˜ë¦¬ í—¬í¼ í•¨ìˆ˜ë“¤ì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ğŸ”¥ Mock ë°ì´í„° ì§„ë‹¨ ì‹œìŠ¤í…œ (ì¶”ê°€)
try:
    from app.core.mock_data_diagnostic import (
        detect_mock_data, diagnose_step_data, get_diagnostic_summary, diagnostic_decorator
    )
    MOCK_DIAGNOSTIC_AVAILABLE = True
except ImportError:
    MOCK_DIAGNOSTIC_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("Mock ë°ì´í„° ì§„ë‹¨ ì‹œìŠ¤í…œì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# BaseStepMixinì€ ì´ë¯¸ importë¨

# BaseStepMixinì„ base_step_mixin.pyì—ì„œ import
from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin



# ==============================================
# ğŸ”¥ í™˜ê²½ ì„¤ì • ë° ìµœì í™”
# ==============================================

# M3 Max ê°ì§€ (common_importsì—ì„œ ê°€ì ¸ì˜´)
IS_M3_MAX = detect_m3_max()

# M3 Max ìµœì í™” ì„¤ì •
if IS_M3_MAX and TORCH_AVAILABLE and MPS_AVAILABLE:
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    os.environ['TORCH_MPS_PREFER_METAL'] = '1'

# ==============================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class HumanParsingModel(Enum):
    """ì¸ì²´ íŒŒì‹± ëª¨ë¸ íƒ€ì…"""
    GRAPHONOMY = "graphonomy"
    U2NET = "u2net"
    MOCK = "mock"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"

# 20ê°œ ì¸ì²´ ë¶€ìœ„ ì •ì˜ (Graphonomy í‘œì¤€)
BODY_PARTS = {
    0: 'background',    1: 'hat',          2: 'hair',
    3: 'glove',         4: 'sunglasses',   5: 'upper_clothes',
    6: 'dress',         7: 'coat',         8: 'socks',
    9: 'pants',         10: 'torso_skin',  11: 'scarf',
    12: 'skirt',        13: 'face',        14: 'left_arm',
    15: 'right_arm',    16: 'left_leg',    17: 'right_leg',
    18: 'left_shoe',    19: 'right_shoe'
}

# ì‹œê°í™” ìƒ‰ìƒ (20ê°œ í´ë˜ìŠ¤)
VISUALIZATION_COLORS = {
    0: (0, 0, 0),           # Background
    1: (255, 0, 0),         # Hat
    2: (255, 165, 0),       # Hair
    3: (255, 255, 0),       # Glove
    4: (0, 255, 0),         # Sunglasses
    5: (0, 255, 255),       # Upper-clothes
    6: (0, 0, 255),         # Dress
    7: (255, 0, 255),       # Coat
    8: (128, 0, 128),       # Socks
    9: (255, 192, 203),     # Pants
    10: (255, 218, 185),    # Torso-skin
    11: (210, 180, 140),    # Scarf
    12: (255, 20, 147),     # Skirt
    13: (255, 228, 196),    # Face
    14: (255, 160, 122),    # Left-arm
    15: (255, 182, 193),    # Right-arm
    16: (173, 216, 230),    # Left-leg
    17: (144, 238, 144),    # Right-leg
    18: (139, 69, 19),      # Left-shoe
    19: (160, 82, 45)       # Right-shoe
}

@dataclass
class EnhancedHumanParsingConfig:
    """ê°•í™”ëœ Human Parsing ì„¤ì • (ì›ë³¸ í”„ë¡œì íŠ¸ ì™„ì „ ë°˜ì˜)"""
    method: HumanParsingModel = HumanParsingModel.GRAPHONOMY
    quality_level: QualityLevel = QualityLevel.HIGH
    input_size: Tuple[int, int] = (512, 512)
    
    # ì „ì²˜ë¦¬ ì„¤ì •
    enable_quality_assessment: bool = True
    enable_lighting_normalization: bool = True
    enable_color_correction: bool = True
    enable_roi_detection: bool = True
    enable_background_analysis: bool = True
    
    # ì¸ì²´ ë¶„ë¥˜ ì„¤ì •
    enable_body_classification: bool = True
    classification_confidence_threshold: float = 0.8
    
    # Graphonomy í”„ë¡¬í”„íŠ¸ ì„¤ì •
    enable_advanced_prompts: bool = True
    use_box_prompts: bool = True
    use_mask_prompts: bool = True
    enable_iterative_refinement: bool = True
    max_refinement_iterations: int = 3
    
    # í›„ì²˜ë¦¬ ì„¤ì • (ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜)
    enable_crf_postprocessing: bool = True
    enable_edge_refinement: bool = True
    enable_hole_filling: bool = True
    enable_multiscale_processing: bool = True
    
    # í’ˆì§ˆ ê²€ì¦ ì„¤ì •
    enable_quality_validation: bool = True
    quality_threshold: float = 0.7
    enable_auto_retry: bool = True
    max_retry_attempts: int = 3
    
    # ê¸°ë³¸ ì„¤ì •
    enable_visualization: bool = True
    use_fp16: bool = True
    confidence_threshold: float = 0.7
    remove_noise: bool = True
    overlay_opacity: float = 0.6
    
    # ìë™ ì „ì²˜ë¦¬ ì„¤ì •
    auto_preprocessing: bool = True
    
    # ë°ì´í„° ê²€ì¦ ì„¤ì •
    strict_data_validation: bool = True
    
    # ìë™ í›„ì²˜ë¦¬ ì„¤ì •
    auto_postprocessing: bool = True

# ==============================================
# ğŸ”¥ ê³ ê¸‰ AI ì•„í‚¤í…ì²˜ë“¤ (ì›ë³¸ í”„ë¡œì íŠ¸ ì™„ì „ ë°˜ì˜)
# ==============================================

class ASPPModule(nn.Module):
    """ASPP (Atrous Spatial Pyramid Pooling) - Multi-scale context aggregation"""
    
    def __init__(self, in_channels=2048, out_channels=256, atrous_rates=[6, 12, 18]):
        super().__init__()
        
        # 1x1 convolution
        self.conv1x1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # Atrous convolutions with different rates
        self.atrous_convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 3, padding=rate, 
                         dilation=rate, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            ) for rate in atrous_rates
        ])
        
        # Global average pooling branch
        self.global_avg_pool = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # Feature fusion
        total_channels = out_channels * (1 + len(atrous_rates) + 1)  # 1x1 + atrous + global
        self.project = nn.Sequential(
            nn.Conv2d(total_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5)
        )
    
    def forward(self, x):
        h, w = x.shape[2:]
        
        # 1x1 convolution
        feat1 = self.conv1x1(x)
        
        # Atrous convolutions
        atrous_feats = [conv(x) for conv in self.atrous_convs]
        
        # Global average pooling
        global_feat = self.global_avg_pool(x)
        global_feat = F.interpolate(global_feat, size=(h, w), 
                                   mode='bilinear', align_corners=False)
        
        # Concatenate all features
        concat_feat = torch.cat([feat1] + atrous_feats + [global_feat], dim=1)
        
        # Project to final features
        return self.project(concat_feat)

class SelfAttentionBlock(nn.Module):
    """Self-Attention ë©”ì»¤ë‹ˆì¦˜"""
    
    def __init__(self, in_channels, reduction=8):
        super().__init__()
        self.in_channels = in_channels
        
        # Query, Key, Value ë³€í™˜
        self.query_conv = nn.Conv2d(in_channels, in_channels // reduction, 1)
        self.key_conv = nn.Conv2d(in_channels, in_channels // reduction, 1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)
        
        # Output projection
        self.out_conv = nn.Conv2d(in_channels, in_channels, 1)
        self.softmax = nn.Softmax(dim=-1)
        
        # Learnable parameter
        self.gamma = nn.Parameter(torch.zeros(1))
    
    def forward(self, x):
        batch_size, C, H, W = x.shape
        
        # Generate query, key, value
        proj_query = self.query_conv(x).view(batch_size, -1, H * W).permute(0, 2, 1)
        proj_key = self.key_conv(x).view(batch_size, -1, H * W)
        proj_value = self.value_conv(x).view(batch_size, -1, H * W)
        
        # Attention computation
        attention = torch.bmm(proj_query, proj_key)
        attention = self.softmax(attention)
        
        # Apply attention to values
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, H, W)
        
        # Residual connection with learnable weight
        out = self.gamma * self.out_conv(out) + x
        
        return out

class SelfCorrectionModule(nn.Module):
    """Self-Correction Learning - SCHP í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, num_classes=20, hidden_dim=256):
        super().__init__()
        self.num_classes = num_classes
        
        # Context aggregation
        self.context_conv = nn.Sequential(
            nn.Conv2d(num_classes, hidden_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        # Self-attention mechanism
        self.self_attention = SelfAttentionBlock(hidden_dim)
        
        # Edge detection branch
        self.edge_detector = nn.Sequential(
            nn.Conv2d(hidden_dim, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        )
        
        # Correction prediction with multi-scale
        self.correction_pyramid = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(hidden_dim, hidden_dim // 2, 3, padding=rate, dilation=rate),
                nn.BatchNorm2d(hidden_dim // 2),
                nn.ReLU(inplace=True)
            ) for rate in [1, 2, 4]
        ])
        
        self.correction_fusion = nn.Sequential(
            nn.Conv2d(hidden_dim // 2 * 3, hidden_dim, 1),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, num_classes, 1)
        )
        
        # Confidence estimation with spatial attention
        self.confidence_spatial = nn.Sequential(
            nn.Conv2d(hidden_dim, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, 1),
            nn.Sigmoid()
        )
        
        # Quality assessment
        self.quality_branch = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(hidden_dim, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, initial_parsing, features):
        try:
            print(f"ğŸ” SelfCorrectionModule ë””ë²„ê¹…:")
            print(f"  initial_parsing shape: {initial_parsing.shape}")
            print(f"  features shape: {features.shape}")
            
            # Context aggregation from initial parsing (20 channels -> 256 channels)
            parsing_probs = F.softmax(initial_parsing, dim=1)
            print(f"  parsing_probs shape: {parsing_probs.shape}")
            
            # SelfCorrectionModuleì€ initial_parsing (20 channels)ë§Œ ì‚¬ìš©
            # features (256 channels)ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            context_feat = self.context_conv(parsing_probs)  # 20 -> 256
            print(f"  context_feat shape: {context_feat.shape}")
            
            # Self-attention refinement
            refined_feat = self.self_attention(context_feat)  # 256 -> 256
            print(f"  refined_feat shape: {refined_feat.shape}")
            
            # Edge detection for boundary refinement
            edge_map = self.edge_detector(refined_feat)  # 256 -> 1
            print(f"  edge_map shape: {edge_map.shape}")
            
        except Exception as e:
            print(f"âŒ SelfCorrectionModule forward ì˜¤ë¥˜: {e}")
            print(f"  initial_parsing shape: {initial_parsing.shape}")
            print(f"  features shape: {features.shape}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ initial_parsingì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return initial_parsing, {
                'spatial_confidence': torch.ones_like(initial_parsing[:, :1]),
                'quality_score': torch.tensor(0.5),
                'edge_map': torch.zeros_like(initial_parsing[:, :1]),
                'correction_magnitude': torch.tensor(0.0)
            }
        
        # Multi-scale correction prediction
        pyramid_feats = [conv(refined_feat) for conv in self.correction_pyramid]
        fused_feats = torch.cat(pyramid_feats, dim=1)
        correction = self.correction_fusion(fused_feats)
        
        # Spatial confidence estimation
        spatial_confidence = self.confidence_spatial(refined_feat)
        
        # Quality assessment
        quality_score = self.quality_branch(refined_feat)
        
        # Apply correction with confidence weighting and edge guidance
        edge_weight = 1.0 + 2.0 * edge_map  # Emphasize boundaries
        weighted_correction = correction * spatial_confidence * edge_weight
        
        corrected_parsing = initial_parsing + weighted_correction * 0.3  # Conservative update
        
        return corrected_parsing, {
            'spatial_confidence': spatial_confidence,
            'quality_score': quality_score,
            'edge_map': edge_map,
            'correction_magnitude': torch.abs(weighted_correction).mean()
        }
class ProgressiveParsingModule(nn.Module):
    """Progressive Parsing - ë‹¨ê³„ë³„ ì •ì œ ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, num_classes=20, num_stages=3, hidden_dim=256):
        super().__init__()
        self.num_stages = num_stages
        self.hidden_dim = hidden_dim
        
        # Stageë³„ íŠ¹ì„± ì¶”ì¶œê¸°
        self.stage_extractors = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(num_classes + (hidden_dim if i > 0 else 0), hidden_dim, 3, padding=1),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU(inplace=True),
                nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU(inplace=True)
            ) for i in range(num_stages)
        ])
        
        # Stageë³„ attention ëª¨ë“ˆ
        self.stage_attention = nn.ModuleList([
            SelfAttentionBlock(hidden_dim) for _ in range(num_stages)
        ])
        
        # Stageë³„ ì˜ˆì¸¡ê¸°
        self.stage_predictors = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(hidden_dim, hidden_dim // 2, 3, padding=1),
                nn.BatchNorm2d(hidden_dim // 2),
                nn.ReLU(inplace=True),
                nn.Conv2d(hidden_dim // 2, num_classes, 1)
            ) for _ in range(num_stages)
        ])
        
        # Stageë³„ confidence ì˜ˆì¸¡ê¸°
        self.confidence_predictors = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(hidden_dim, 64, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 32, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(32, 1, 1),
                nn.Sigmoid()
            ) for _ in range(num_stages)
        ])
        
        # Cross-stage fusion
        self.cross_stage_fusion = nn.Sequential(
            nn.Conv2d(hidden_dim * num_stages, hidden_dim, 1),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        # Final refinement
        self.final_refiner = nn.Sequential(
            nn.Conv2d(hidden_dim + num_classes, hidden_dim, 3, padding=1),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, num_classes, 1)
        )
    
    def forward(self, initial_parsing, base_features):
        stage_results = []
        stage_features = []
        current_input = initial_parsing
        
        for i in range(self.num_stages):
            # Feature extraction
            if i == 0:
                feat_input = current_input
            else:
                feat_input = torch.cat([current_input, stage_features[-1]], dim=1)
            
            stage_feat = self.stage_extractors[i](feat_input)
            
            # Apply attention
            attended_feat = self.stage_attention[i](stage_feat)
            stage_features.append(attended_feat)
            
            # Prediction
            parsing_pred = self.stage_predictors[i](attended_feat)
            confidence = self.confidence_predictors[i](attended_feat)
            
            # Progressive refinement with residual connection
            if i == 0:
                refined_parsing = parsing_pred
            else:
                # Weighted combination with previous stage
                weight = confidence
                refined_parsing = (1 - weight) * current_input + weight * parsing_pred
            
            stage_results.append({
                'parsing': refined_parsing,
                'confidence': confidence,
                'features': attended_feat,
                'stage': i
            })
            
            current_input = refined_parsing
        
        # Cross-stage feature fusion
        if len(stage_features) > 1:
            fused_features = self.cross_stage_fusion(torch.cat(stage_features, dim=1))
            
            # Final refinement
            final_input = torch.cat([current_input, fused_features], dim=1)
            final_refinement = self.final_refiner(final_input)
            
            # Add refined result as final stage
            stage_results.append({
                'parsing': current_input + final_refinement * 0.2,
                'confidence': torch.ones_like(confidence) * 0.9,
                'features': fused_features,
                'stage': 'final'
            })
        
        return stage_results


# ì¤‘ë³µ ì œê±° ì™„ë£Œ - ì™„ì „í•œ ProgressiveParsingModule ìœ ì§€

class HybridEnsembleModule(nn.Module):
    """í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” - ë‹¤ì¤‘ ëª¨ë¸ ê²°í•© ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, num_classes=20, num_models=3, hidden_dim=256):
        super().__init__()
        self.num_models = num_models
        self.num_classes = num_classes
        
        # Dynamic weight learning with context
        self.context_encoder = nn.Sequential(
            nn.Conv2d(num_classes * num_models, hidden_dim, 3, padding=1),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        # Spatial attention for weight generation
        self.spatial_attention = SelfAttentionBlock(hidden_dim)
        
        # Model-specific weight predictors
        self.weight_predictors = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(hidden_dim, 64, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 1, 1),
                nn.Sigmoid()
            ) for _ in range(num_models)
        ])
        
        # Confidence-aware fusion
        self.confidence_fusion = nn.Sequential(
            nn.Conv2d(num_models, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, 1),
            nn.Sigmoid()
        )
        
        # Quality assessment branch
        self.quality_assessor = nn.Sequential(
            nn.Conv2d(num_classes, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        
        # Ensemble refinement with residual learning
        self.ensemble_refiner = nn.Sequential(
            nn.Conv2d(num_classes * 2, hidden_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, hidden_dim // 2, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim // 2, num_classes, 1)
        )
        
        # Uncertainty estimation
        self.uncertainty_estimator = nn.Sequential(
            nn.Conv2d(num_classes * num_models, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        )
    
    def forward(self, model_outputs, confidences):
        batch_size = model_outputs[0].shape[0]
        
        # Concatenate all model outputs
        concat_outputs = torch.cat(model_outputs, dim=1)
        
        # Context encoding
        context_feat = self.context_encoder(concat_outputs)
        attended_context = self.spatial_attention(context_feat)
        
        # Generate model-specific weights
        model_weights = []
        for i, weight_pred in enumerate(self.weight_predictors):
            weight = weight_pred(attended_context)
            # Incorporate confidence
            if i < len(confidences):
                weight = weight * confidences[i]
            model_weights.append(weight)
        
        # Normalize weights (soft attention)
        weight_stack = torch.cat(model_weights, dim=1)
        normalized_weights = F.softmax(weight_stack, dim=1)
        
        # Confidence-aware fusion weight
        fusion_weight = self.confidence_fusion(weight_stack)
        
        # Weighted ensemble
        ensemble_output = torch.zeros_like(model_outputs[0])
        for i, output in enumerate(model_outputs):
            weight = normalized_weights[:, i:i+1]
            ensemble_output += output * weight
        
        # Uncertainty estimation
        uncertainty = self.uncertainty_estimator(concat_outputs)
        
        # Quality assessment
        quality_score = self.quality_assessor(ensemble_output)
        
        # Ensemble refinement with residual learning
        refine_input = torch.cat([ensemble_output, concat_outputs.mean(dim=1, keepdim=True)], dim=1)
        residual = self.ensemble_refiner(refine_input)
        
        # Final output with uncertainty-weighted residual
        uncertainty_weight = (1.0 - uncertainty) * fusion_weight
        final_output = ensemble_output + residual * uncertainty_weight * 0.2
        
        # Return detailed results
        return {
            'ensemble_output': final_output,
            'model_weights': normalized_weights,
            'uncertainty': uncertainty,
            'quality_score': quality_score,
            'fusion_weight': fusion_weight,
            'residual': residual
        }

class IterativeRefinementModule(nn.Module):
    """ë°˜ë³µì  ì •ì œ ëª¨ë“ˆ - ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, num_classes=20, hidden_dim=256, max_iterations=3):
        super().__init__()
        self.max_iterations = max_iterations
        self.hidden_dim = hidden_dim
        
        # ì •ì œ ë„¤íŠ¸ì›Œí¬ (ë” ê°•ë ¥í•œ ì•„í‚¤í…ì²˜)
        self.refine_encoder = nn.Sequential(
            nn.Conv2d(num_classes * 2, hidden_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        # Attention for refinement
        self.refine_attention = SelfAttentionBlock(hidden_dim)
        
        # Multi-scale refinement
        self.refine_pyramid = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(hidden_dim, hidden_dim // 4, 3, padding=rate, dilation=rate),
                nn.BatchNorm2d(hidden_dim // 4),
                nn.ReLU(inplace=True)
            ) for rate in [1, 2, 4, 8]
        ])
        
        self.refine_fusion = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim, 1),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, num_classes, 1)
        )
        
        # ìˆ˜ë ´ íŒì • (ë” ì •í™•í•œ ë©”íŠ¸ë¦­)
        self.convergence_encoder = nn.Sequential(
            nn.Conv2d(num_classes, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten()
        )
        
        self.convergence_predictor = nn.Sequential(
            nn.Linear(32, 16),
            nn.ReLU(inplace=True),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
        
        # Change magnitude estimation
        self.change_estimator = nn.Sequential(
            nn.Conv2d(num_classes, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
    
    def forward(self, initial_parsing):
        current_parsing = initial_parsing
        iteration_results = []
        convergence_threshold = 0.95
        
        for i in range(self.max_iterations):
            # ì´ì „ ê²°ê³¼ì™€ í•¨ê»˜ ì…ë ¥
            if i == 0:
                refine_input = torch.cat([current_parsing, current_parsing], dim=1)
            else:
                refine_input = torch.cat([current_parsing, iteration_results[-1]['parsing']], dim=1)
            
            # ì •ì œ ê³¼ì •
            encoded_feat = self.refine_encoder(refine_input)
            attended_feat = self.refine_attention(encoded_feat)
            
            # Multi-scale pyramid
            pyramid_feats = [conv(attended_feat) for conv in self.refine_pyramid]
            pyramid_combined = torch.cat(pyramid_feats, dim=1)
            
            # Refinement prediction
            residual = self.refine_fusion(pyramid_combined)
            
            # Adaptive update rate based on iteration
            update_rate = 0.3 * (0.8 ** i)  # Decreasing update rate
            refined_parsing = current_parsing + residual * update_rate
            
            # Calculate change magnitude
            change_magnitude = torch.abs(refined_parsing - current_parsing)
            avg_change = self.change_estimator(change_magnitude)
            
            # ìˆ˜ë ´ ì²´í¬
            convergence_input = torch.abs(refined_parsing - current_parsing)
            convergence_feat = self.convergence_encoder(convergence_input)
            convergence_score = self.convergence_predictor(convergence_feat)
            
            # Quality metrics
            entropy = self._calculate_entropy(F.softmax(refined_parsing, dim=1))
            consistency = self._calculate_consistency(refined_parsing)
            
            iteration_results.append({
                'parsing': refined_parsing,
                'residual': residual,
                'convergence': convergence_score,
                'change_magnitude': avg_change,
                'entropy': entropy,
                'consistency': consistency,
                'iteration': i,
                'update_rate': update_rate
            })
            
            current_parsing = refined_parsing
            
            # Early convergence check
            if convergence_score > convergence_threshold and avg_change < 0.01:
                break
        
        return iteration_results
    
    def _calculate_entropy(self, probs):
        """ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ë¶ˆí™•ì‹¤ì„± ì¸¡ì •)"""
        entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)
        return entropy.mean()
    
    def _calculate_consistency(self, parsing):
        """ì¼ê´€ì„± ê³„ì‚° (ê³µê°„ì  ì—°ì†ì„±)"""
        # Gradient magnitude as consistency measure
        grad_x = torch.abs(parsing[:, :, :, 1:] - parsing[:, :, :, :-1])
        grad_y = torch.abs(parsing[:, :, 1:, :] - parsing[:, :, :-1, :])
        
        consistency = 1.0 / (1.0 + grad_x.mean() + grad_y.mean())
        return consistency


# ì¤‘ë³µëœ AdvancedGraphonomyResNetASPP í´ë˜ìŠ¤ ì œê±° - ë‘ ë²ˆì§¸ ë²„ì „ì´ ë” ì™„ì „í•¨
class ResNetBottleneck(nn.Module):
    """ResNet Bottleneck ë¸”ë¡ ì™„ì „ êµ¬í˜„"""
    expansion = 4
    
    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):
        super().__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=dilation, dilation=dilation, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        
    def forward(self, x):
        identity = x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        
        out = self.conv3(out)
        out = self.bn3(out)
        
        if self.downsample is not None:
            identity = self.downsample(x)
        
        out += identity
        out = self.relu(out)
        
        return out
class ResNet101Backbone(nn.Module):
    """ResNet-101 ë°±ë³¸ ì™„ì „ êµ¬í˜„"""
    
    def __init__(self):
        super().__init__()
        self.inplanes = 64
        
        # Initial layers
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # Residual layers
        self.layer1 = self._make_layer(ResNetBottleneck, 64, 3)
        self.layer2 = self._make_layer(ResNetBottleneck, 128, 4, stride=2)
        self.layer3 = self._make_layer(ResNetBottleneck, 256, 23, stride=2)
        self.layer4 = self._make_layer(ResNetBottleneck, 512, 3, stride=1, dilation=2)
        
        # Initialize weights
        self._init_weights()
    
    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )
        
        layers = []
        layers.append(block(self.inplanes, planes, stride, dilation, downsample))
        self.inplanes = planes * block.expansion
        
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, dilation=dilation))
        
        return nn.Sequential(*layers)
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        x1 = self.layer1(x)
        x2 = self.layer2(x1)
        x3 = self.layer3(x2)
        x4 = self.layer4(x3)
        
        return {
            'layer1': x1,
            'layer2': x2, 
            'layer3': x3,
            'layer4': x4
        }


# ==============================================
# ğŸ”¥ ì™„ì „ êµ¬í˜„ëœ AdvancedGraphonomyResNetASPP
# ==============================================

class AdvancedGraphonomyResNetASPP(nn.Module):
    """ê³ ê¸‰ Graphonomy ResNet-101 + ASPP + Self-Attention + Progressive Parsing ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        self.num_classes = num_classes
        
        # ResNet-101 ë°±ë³¸
        self.backbone = ResNet101Backbone()
        
        # ASPP ëª¨ë“ˆ (2048 -> 256)
        self.aspp = ASPPModule(in_channels=2048, out_channels=256)
        
        # Self-Attention ëª¨ë“ˆ
        self.self_attention = SelfAttentionBlock(in_channels=256)
        
        # Feature pyramid for multi-scale processing
        self.fpn = FeaturePyramidNetwork(
            in_channels_list=[256, 512, 1024, 2048],
            out_channels=256
        )
        
        # Progressive Parsing ëª¨ë“ˆ
        self.progressive_parsing = ProgressiveParsingModule(
            num_classes=num_classes, 
            num_stages=3,
            hidden_dim=256
        )
        
        # Self-Correction ëª¨ë“ˆ
        self.self_correction = SelfCorrectionModule(
            num_classes=num_classes,
            hidden_dim=256
        )
        
        # Iterative Refinement ëª¨ë“ˆ
        self.iterative_refine = IterativeRefinementModule(
            num_classes=num_classes,
            hidden_dim=256,
            max_iterations=3
        )
        
        # Hybrid Ensemble ëª¨ë“ˆ
        self.hybrid_ensemble = HybridEnsembleModule(
            num_classes=num_classes,
            num_models=3,
            hidden_dim=256
        )
        
        # ê¸°ë³¸ ë¶„ë¥˜ê¸°
        self.classifier = nn.Sequential(
            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.1),
            nn.Conv2d(256, num_classes, kernel_size=1)
        )
        
        # ë³´ì¡° ë¶„ë¥˜ê¸°ë“¤ (ë‹¤ì¤‘ ìŠ¤ì¼€ì¼)
        self.aux_classifiers = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(256, 128, 3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),
                nn.Conv2d(128, num_classes, 1)
            ) for _ in range(3)
        ])
        
        # Edge detection branch
        self.edge_classifier = nn.Sequential(
            nn.Conv2d(256, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, kernel_size=1)
        )
        
        # Boundary refinement
        self.boundary_refiner = BoundaryRefinementModule(num_classes, 256)
        
        # Final fusion module
        self.final_fusion = FinalFusionModule(num_classes, 256)
        
        self._init_weights()
    
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """ê³ ê¸‰ ìˆœì „íŒŒ (ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ì™„ì „ ì ìš©)"""
        input_size = x.shape[2:]
        
        # 1. Backbone features (ResNet-101)
        backbone_features = self.backbone(x)
        high_level_features = backbone_features['layer4']  # 2048 channels
        
        # 2. ASPP (Multi-scale context aggregation)
        aspp_features = self.aspp(high_level_features)  # 256 channels
        
        # 3. Self-Attention (Spatial attention mechanism)
        attended_features = self.self_attention(aspp_features)
        
        # 4. Feature Pyramid Network for multi-scale features
        fpn_features = self.fpn({
            'layer1': backbone_features['layer1'],
            'layer2': backbone_features['layer2'],
            'layer3': backbone_features['layer3'],
            'layer4': aspp_features
        })
        
        # 5. ê¸°ë³¸ ë¶„ë¥˜ (ì´ˆê¸° íŒŒì‹±)
        initial_parsing = self.classifier(attended_features)
        
        # 6. ë³´ì¡° ë¶„ë¥˜ê¸°ë“¤ (ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì˜ˆì¸¡)
        aux_outputs = []
        for i, aux_classifier in enumerate(self.aux_classifiers):
            if i < len(fpn_features):
                aux_pred = aux_classifier(fpn_features[f'layer{i+2}'])
                aux_pred_resized = F.interpolate(
                    aux_pred, size=initial_parsing.shape[2:],
                    mode='bilinear', align_corners=False
                )
                aux_outputs.append(aux_pred_resized)
        
        # 7. Progressive Parsing (3ë‹¨ê³„ ì •ì œ)
        progressive_results = self.progressive_parsing(initial_parsing, attended_features)
        final_progressive = progressive_results[-1]['parsing']
        
        # 8. Self-Correction Learning (SCHP ì•Œê³ ë¦¬ì¦˜)
        corrected_parsing, correction_info = self.self_correction(
            final_progressive, attended_features
        )
        
        # 9. Iterative Refinement (ìˆ˜ë ´ ê¸°ë°˜ ì •ì œ)
        refinement_results = self.iterative_refine(corrected_parsing)
        final_refined = refinement_results[-1]['parsing']
        
        # 10. Edge detection ë° boundary refinement
        edge_output = self.edge_classifier(attended_features)
        boundary_refined = self.boundary_refiner(
            final_refined, edge_output, attended_features
        )
        
        # 11. Hybrid Ensemble (ë‹¤ì¤‘ ì˜ˆì¸¡ ê²°í•©)
        if len(aux_outputs) >= 2:
            ensemble_inputs = [final_refined, boundary_refined] + aux_outputs[:1]
            ensemble_confidences = [
                correction_info.get('spatial_confidence', torch.ones_like(edge_output)),
                torch.sigmoid(edge_output),
                torch.ones_like(edge_output) * 0.8
            ]
            
            ensemble_result = self.hybrid_ensemble(ensemble_inputs, ensemble_confidences)
            ensemble_parsing = ensemble_result['ensemble_output']
        else:
            ensemble_parsing = boundary_refined
            ensemble_result = {'ensemble_output': boundary_refined}
        
        # 12. Final fusion (ëª¨ë“  ì •ë³´ í†µí•©)
        final_output = self.final_fusion(
            ensemble_parsing, attended_features, edge_output
        )
        
        # 13. ì…ë ¥ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œë§
        final_parsing = F.interpolate(
            final_output, size=input_size,
            mode='bilinear', align_corners=False
        )
        
        edge_output_resized = F.interpolate(
            edge_output, size=input_size,
            mode='bilinear', align_corners=False
        )
        
        return {
            'parsing': final_parsing,
            'edge': edge_output_resized,
            'progressive_results': progressive_results,
            'correction_info': correction_info,
            'refinement_results': refinement_results,
            'ensemble_result': ensemble_result,
            'aux_outputs': aux_outputs,
            'intermediate_features': {
                'backbone': backbone_features,
                'aspp': aspp_features,
                'attention': attended_features,
                'fpn': fpn_features
            }
        }

class FeaturePyramidNetwork(nn.Module):
    """Feature Pyramid Network for multi-scale feature processing"""
    
    def __init__(self, in_channels_list, out_channels):
        super().__init__()
        self.inner_blocks = nn.ModuleList()
        self.layer_blocks = nn.ModuleList()
        
        for in_channels in in_channels_list:
            inner_block = nn.Conv2d(in_channels, out_channels, 1)
            layer_block = nn.Conv2d(out_channels, out_channels, 3, padding=1)
            self.inner_blocks.append(inner_block)
            self.layer_blocks.append(layer_block)
    
    def forward(self, x_dict):
        """
        x_dict: {'layer1': tensor, 'layer2': tensor, 'layer3': tensor, 'layer4': tensor}
        """
        names = list(x_dict.keys())
        x_list = [x_dict[name] for name in names]
        
        # Start from the highest resolution
        last_inner = self.inner_blocks[-1](x_list[-1])
        results = []
        results.append(self.layer_blocks[-1](last_inner))
        
        for i in range(len(x_list) - 2, -1, -1):
            inner_lateral = self.inner_blocks[i](x_list[i])
            
            # Upsample and add
            inner_top_down = F.interpolate(
                last_inner, size=inner_lateral.shape[-2:],
                mode='bilinear', align_corners=False
            )
            last_inner = inner_lateral + inner_top_down
            results.insert(0, self.layer_blocks[i](last_inner))
        
        # Return as dict
        out_dict = {}
        for i, name in enumerate(names):
            out_dict[name] = results[i]
        
        return out_dict

class BoundaryRefinementModule(nn.Module):
    """Boundary refinement using edge information"""
    
    def __init__(self, num_classes, feature_dim):
        super().__init__()
        
        # Edge-guided attention
        self.edge_attention = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, 1),
            nn.Sigmoid()
        )
        
        # Boundary-aware convolution
        self.boundary_conv = nn.Sequential(
            nn.Conv2d(num_classes + feature_dim + 1, feature_dim, 3, padding=1),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_dim, feature_dim // 2, 3, padding=1),
            nn.BatchNorm2d(feature_dim // 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_dim // 2, num_classes, 1)
        )
        
        # Boundary loss prediction
        self.boundary_loss_pred = nn.Sequential(
            nn.Conv2d(feature_dim, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, 1),
            nn.Sigmoid()
        )
    
    def forward(self, parsing, edge_map, features):
        # Edge-guided attention
        edge_attention = self.edge_attention(edge_map)
        
        # Combine all information
        combined_input = torch.cat([parsing, features, edge_map], dim=1)
        
        # Boundary-aware refinement
        boundary_refined = self.boundary_conv(combined_input)
        
        # Apply edge attention
        refined_parsing = parsing + boundary_refined * edge_attention * 0.3
        
        # Predict boundary quality
        boundary_quality = self.boundary_loss_pred(features)
        
        return refined_parsing

class FinalFusionModule(nn.Module):
    """Final fusion of all information"""
    
    def __init__(self, num_classes, feature_dim):
        super().__init__()
        
        # Multi-modal fusion
        self.fusion_conv = nn.Sequential(
            nn.Conv2d(num_classes + feature_dim + 1, feature_dim, 3, padding=1),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(inplace=True)
        )
        
        # Self-attention for final refinement
        self.final_attention = SelfAttentionBlock(feature_dim)
        
        # Output projection
        self.output_proj = nn.Sequential(
            nn.Conv2d(feature_dim, feature_dim // 2, 3, padding=1),
            nn.BatchNorm2d(feature_dim // 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_dim // 2, num_classes, 1)
        )
        
        # Residual scaling
        self.residual_scale = nn.Parameter(torch.tensor(0.1))
    
    def forward(self, parsing, features, edge_map):
        # Fuse all modalities
        fused_input = torch.cat([parsing, features, edge_map], dim=1)
        fused_features = self.fusion_conv(fused_input)
        
        # Apply self-attention
        attended_features = self.final_attention(fused_features)
        
        # Generate residual
        residual = self.output_proj(attended_features)
        
        # Apply residual with learnable scaling
        final_output = parsing + residual * self.residual_scale
        
        return final_output

# ==============================================
# ğŸ”¥ U2Net ê²½ëŸ‰ ëª¨ë¸ (í´ë°±ìš©)
# ==============================================

class U2NetForParsing(nn.Module):
    """U2Net ê¸°ë°˜ ì¸ì²´ íŒŒì‹± ëª¨ë¸ (í´ë°±ìš©)"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        self.num_classes = num_classes
        
        # ì¸ì½”ë”
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(256, 512, 3, padding=1),
            nn.ReLU(inplace=True),
        )
        
        # ë””ì½”ë”
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, num_classes, 1),
        )
        
        # U2Net ëª¨ë¸ìš© ë©”íƒ€ë°ì´í„°
        self.checkpoint_path = "u2net_model"
        self.checkpoint_data = {"u2net": True}
        self.has_model = True
        self.memory_usage_mb = 50.0
        self.load_time = 1.0
    
    def get_checkpoint_data(self):
        """U2Net ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ë°˜í™˜"""
        return self.checkpoint_data
    
    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return {'parsing': decoded}
# ==============================================
# ğŸ”¥ ê³ ê¸‰ í›„ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ë“¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

class AdvancedPostProcessor:
    """ê³ ê¸‰ í›„ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ë“¤ (ì›ë³¸ í”„ë¡œì íŠ¸ ì™„ì „ ë°˜ì˜)"""
    
    @staticmethod
    def apply_crf_postprocessing(parsing_map: np.ndarray, image: np.ndarray, num_iterations: int = 10) -> np.ndarray:
        """CRF í›„ì²˜ë¦¬ë¡œ ê²½ê³„ì„  ê°œì„  (20ê°œ í´ë˜ìŠ¤ Human Parsing íŠ¹í™”)"""
        try:
            if not DENSECRF_AVAILABLE:
                return parsing_map
            
            h, w = parsing_map.shape
            
            # í™•ë¥  ë§µ ìƒì„± (20ê°œ í´ë˜ìŠ¤)
            num_classes = 20
            probs = np.zeros((num_classes, h, w), dtype=np.float32)
            
            for class_id in range(num_classes):
                probs[class_id] = (parsing_map == class_id).astype(np.float32)
            
            # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì •ê·œí™”
            probs = probs / (np.sum(probs, axis=0, keepdims=True) + 1e-8)
            
            # Unary potential
            unary = unary_from_softmax(probs)
            
            # Setup CRF
            d = dcrf.DenseCRF2D(w, h, num_classes)
            d.setUnaryEnergy(unary)
            
            # Add pairwise energies (Human Parsing íŠ¹í™” íŒŒë¼ë¯¸í„°)
            d.addPairwiseGaussian(sxy=(3, 3), compat=3)
            d.addPairwiseBilateral(sxy=(80, 80), srgb=(13, 13, 13), 
                                  rgbim=image, compat=10)
            
            # Inference
            Q = d.inference(num_iterations)
            map_result = np.argmax(Q, axis=0).reshape((h, w))
            
            return map_result.astype(np.uint8)
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"âš ï¸ CRF í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return parsing_map
    
    @staticmethod
    def apply_multiscale_processing(image: np.ndarray, initial_parsing: np.ndarray) -> np.ndarray:
        """ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬ (Human Parsing íŠ¹í™”)"""
        try:
            scales = [0.5, 1.0, 1.5]
            processed_parsings = []
            
            for scale in scales:
                if scale != 1.0:
                    h, w = initial_parsing.shape
                    new_h, new_w = int(h * scale), int(w * scale)
                    
                    scaled_image = np.array(Image.fromarray(image).resize((new_w, new_h), Image.LANCZOS))
                    scaled_parsing = np.array(Image.fromarray(initial_parsing).resize((new_w, new_h), Image.NEAREST))
                    
                    # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
                    processed = np.array(Image.fromarray(scaled_parsing).resize((w, h), Image.NEAREST))
                else:
                    processed = initial_parsing
                
                processed_parsings.append(processed.astype(np.float32))
            
            # ìŠ¤ì¼€ì¼ë³„ ê²°ê³¼ í†µí•© (íˆ¬í‘œ ë°©ì‹)
            if len(processed_parsings) > 1:
                votes = np.zeros_like(processed_parsings[0])
                for parsing in processed_parsings:
                    votes += parsing
                
                # ê°€ì¥ ë§ì€ íˆ¬í‘œë¥¼ ë°›ì€ í´ë˜ìŠ¤ë¡œ ê²°ì •
                final_parsing = (votes / len(processed_parsings)).astype(np.uint8)
            else:
                final_parsing = processed_parsings[0].astype(np.uint8)
            
            return final_parsing
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"âš ï¸ ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return initial_parsing
    
    @staticmethod
    def apply_edge_refinement(parsing_map: np.ndarray, image: np.ndarray) -> np.ndarray:
        """ì—£ì§€ ê¸°ë°˜ ê²½ê³„ì„  ì •ì œ"""
        try:
            if not CV2_AVAILABLE:
                return parsing_map
            
            # ì—£ì§€ ê°ì§€
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            
            # ê²½ê³„ì„  ê°•í™”ë¥¼ ìœ„í•œ ëª¨í´ë¡œì§€ ì—°ì‚°
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            
            refined_parsing = parsing_map.copy()
            
            # ê° í´ë˜ìŠ¤ë³„ë¡œ ì—£ì§€ ê¸°ë°˜ ì •ì œ
            for class_id in np.unique(parsing_map):
                if class_id == 0:  # ë°°ê²½ ì œì™¸
                    continue
                
                class_mask = (parsing_map == class_id).astype(np.uint8) * 255
                
                # ì—£ì§€ì™€ì˜ êµì§‘í•© ê³„ì‚°
                edge_intersection = cv2.bitwise_and(class_mask, edges)
                
                # ì—£ì§€ ê¸°ë°˜ ê²½ê³„ì„  ì •ì œ
                if np.sum(edge_intersection) > 0:
                    refined_mask = cv2.morphologyEx(class_mask, cv2.MORPH_CLOSE, kernel)
                    refined_parsing[refined_mask > 0] = class_id
            
            return refined_parsing
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"âš ï¸ ì—£ì§€ ì •ì œ ì‹¤íŒ¨: {e}")
            return parsing_map
    
    @staticmethod
    def apply_hole_filling_and_noise_removal(parsing_map: np.ndarray) -> np.ndarray:
        """í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±° (Human Parsing íŠ¹í™”)"""
        try:
            if not SCIPY_AVAILABLE or ndimage is None:
                return parsing_map
            
            # í´ë˜ìŠ¤ë³„ë¡œ ì²˜ë¦¬
            processed_map = np.zeros_like(parsing_map)
            
            for class_id in np.unique(parsing_map):
                if class_id == 0:  # ë°°ê²½ì€ ë§ˆì§€ë§‰ì— ì²˜ë¦¬
                    continue
                
                mask = (parsing_map == class_id)
                
                # í™€ ì±„ìš°ê¸°
                filled = ndimage.binary_fill_holes(mask)
                
                # ì‘ì€ ë…¸ì´ì¦ˆ ì œê±° (morphological operations)
                structure = ndimage.generate_binary_structure(2, 2)
                # ì—´ê¸° ì—°ì‚° (ë…¸ì´ì¦ˆ ì œê±°)
                opened = ndimage.binary_opening(filled, structure=structure, iterations=1)
                # ë‹«ê¸° ì—°ì‚° (í™€ ì±„ìš°ê¸°)
                closed = ndimage.binary_closing(opened, structure=structure, iterations=2)
                
                processed_map[closed] = class_id
            
            # ë°°ê²½ ì²˜ë¦¬
            processed_map[processed_map == 0] = 0
            
            return processed_map.astype(np.uint8)
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"âš ï¸ í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            return parsing_map




    @staticmethod
    def apply_quality_enhancement(parsing_map: np.ndarray, image: np.ndarray, confidence_map: Optional[np.ndarray] = None) -> np.ndarray:
        """í’ˆì§ˆ í–¥ìƒ ì•Œê³ ë¦¬ì¦˜"""
        try:
            enhanced_map = parsing_map.copy()
            
            # ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§
            if confidence_map is not None:
                low_confidence_mask = confidence_map < 0.5
                # ì €ì‹ ë¢°ë„ ì˜ì—­ì„ ì£¼ë³€ í´ë˜ìŠ¤ë¡œ ë³´ê°„
                if SCIPY_AVAILABLE:
                    for class_id in np.unique(parsing_map):
                        if class_id == 0:
                            continue
                        
                        class_mask = (parsing_map == class_id) & (~low_confidence_mask)
                        if np.sum(class_mask) > 0:
                            # ê±°ë¦¬ ë³€í™˜ ê¸°ë°˜ ë³´ê°„
                            distance = ndimage.distance_transform_edt(~class_mask)
                            enhanced_map[low_confidence_mask & (distance < 10)] = class_id
            
            # ê²½ê³„ì„  ìŠ¤ë¬´ë”©
            if SKIMAGE_AVAILABLE:
                try:
                    from skimage.filters import gaussian
                    # ê°€ìš°ì‹œì•ˆ í•„í„°ë¡œ ë¶€ë“œëŸ½ê²Œ
                    smoothed = gaussian(enhanced_map.astype(np.float64), sigma=0.5)
                    enhanced_map = np.round(smoothed).astype(np.uint8)
                except:
                    pass
            
            return enhanced_map
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"âš ï¸ í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return parsing_map

class MockHumanParsingModel(nn.Module):
    """Mock Human Parsing ëª¨ë¸ (ì—ëŸ¬ ë°©ì§€ìš©)"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        self.num_classes = num_classes
        
        # ë‹¨ìˆœí•œ CNN
        self.conv = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(64, num_classes)
        )
        
        # Mock ëª¨ë¸ìš© ë©”íƒ€ë°ì´í„°
        self.checkpoint_path = "mock_model"
        self.checkpoint_data = {"mock": True}
        self.has_model = True
        self.memory_usage_mb = 0.1
        self.load_time = 0.1
    
    def get_checkpoint_data(self):
        """Mock ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ë°˜í™˜"""
        return self.checkpoint_data
    
    def forward(self, x):
        # ë‹¨ìˆœí•œ ë¶„ë¥˜ í›„ ì—…ìƒ˜í”Œë§
        features = self.conv(x)
        batch_size = x.shape[0]
        height, width = x.shape[2], x.shape[3]
        
        # í´ë˜ìŠ¤ë³„ í™•ë¥ ì„ ê³µê°„ì ìœ¼ë¡œ í™•ì¥
        parsing = features.unsqueeze(-1).unsqueeze(-1)
        parsintimeg = parsing.expand(batch_size, self.num_classes, height, width)
        
        # ì¤‘ì•™ ì˜ì—­ì„ ì¸ì²´ë¡œ ê°€ì •
        center_mask = torch.zeros_like(parsing[:, 0:1])
        h_start, h_end = height//4, 3*height//4
        w_start, w_end = width//4, 3*width//4
        center_mask[:, :, h_start:h_end, w_start:w_end] = 1.0
        
        # ë°°ê²½ê³¼ ì¸ì²´ ì˜ì—­ êµ¬ë¶„
        mock_parsing = torch.zeros_like(parsing)
        mock_parsing[:, 0] = 1.0 - center_mask.squeeze(1)  # ë°°ê²½
        mock_parsing[:, 10] = center_mask.squeeze(1)  # í”¼ë¶€
        
        return {'parsing': mock_parsing}

# ==============================================
# ğŸ”¥ HumanParsingStep - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
# ==============================================

# BaseStepMixin ì‚¬ìš© ê°€ëŠ¥
# ğŸ”¥ HumanParsingStep í´ë˜ìŠ¤ìš© time ëª¨ë“ˆ ëª…ì‹œì  import
import time

# ğŸ”¥ ì „ì—­ ìŠ¤ì½”í”„ì—ì„œ time ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡
globals()['time'] = time

# ğŸ”¥ í´ë˜ìŠ¤ ì •ì˜ ì‹œì ì— time ëª¨ë“ˆì„ ë¡œì»¬ ìŠ¤ì½”í”„ì—ë„ ì¶”ê°€
locals()['time'] = time

class HumanParsingStep(BaseStepMixin):
        """
        ğŸ”¥ Step 01: Human Parsing v8.0 - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
        
        BaseStepMixin v20.0ì—ì„œ ìë™ ì œê³µ:
        âœ… í‘œì¤€í™”ëœ process() ë©”ì„œë“œ (ë°ì´í„° ë³€í™˜ ìë™ ì²˜ë¦¬)
        âœ… API â†” AI ëª¨ë¸ ë°ì´í„° ë³€í™˜ ìë™í™”
        âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìë™ ì ìš©
        âœ… Central Hub DI Container ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ
        âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
        âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë° ë©”ëª¨ë¦¬ ìµœì í™”
        
        ì´ í´ë˜ìŠ¤ëŠ” _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„!
        """
        
        def __init__(self, **kwargs):
            """Central Hub DI Container ê¸°ë°˜ ì´ˆê¸°í™”"""
            print(f"ğŸ” HumanParsingStep __init__ ì‹œì‘")
            try:
                print(f"ğŸ” super().__init__() í˜¸ì¶œ ì „")
                # ğŸ”¥ BaseStepMixin v20.0 ì™„ì „ ìƒì† - super().__init__() í˜¸ì¶œ
                super().__init__(
                    step_name="HumanParsingStep",
                    **kwargs
                )
                print(f"âœ… super().__init__() í˜¸ì¶œ ì™„ë£Œ")
                
                # ğŸ”¥ time ëª¨ë“ˆ ì°¸ì¡° ì €ì¥ (í´ë˜ìŠ¤ ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´)
                print(f"ğŸ” time ëª¨ë“ˆ import ì‹œì‘")
                import time
                print(f"âœ… time ëª¨ë“ˆ import ì„±ê³µ")
                self.time = time
                print(f"âœ… time ëª¨ë“ˆ ì°¸ì¡° ì €ì¥ ì™„ë£Œ")
                
                # ğŸ”¥ í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™” (Central Hub DI Container ìš”êµ¬ì‚¬í•­)
                print(f"ğŸ” AI ëª¨ë¸ ì €ì¥ì†Œ ì´ˆê¸°í™” ì‹œì‘")
                self.ai_models = {}  # AI ëª¨ë¸ ì €ì¥ì†Œ
                print(f"âœ… AI ëª¨ë¸ ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
                
                print(f"ğŸ” ëª¨ë¸ ë¡œë”© ìƒíƒœ ì´ˆê¸°í™” ì‹œì‘")
                self.models_loading_status = {  # ëª¨ë¸ ë¡œë”© ìƒíƒœ
                    'graphonomy': False,
                    'u2net': False,
                    'mock': False
                }
                print(f"âœ… ëª¨ë¸ ë¡œë”© ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ")
                
                print(f"ğŸ” ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì‹œì‘")
                self.model_interface = None  # ModelLoader ì¸í„°í˜ì´ìŠ¤
                self.model_loader = None  # ModelLoader ì§ì ‘ ì°¸ì¡°
                self.loaded_models = []  # ë¡œë“œëœ ëª¨ë¸ ëª©ë¡
                print(f"âœ… ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
                
                # Human Parsing ì„¤ì •
                print(f"ğŸ” Human Parsing ì„¤ì • ì´ˆê¸°í™” ì‹œì‘")
                self.config = EnhancedHumanParsingConfig()
                print(f"âœ… EnhancedHumanParsingConfig ìƒì„± ì™„ë£Œ")
                
                if 'parsing_config' in kwargs:
                    print(f"ğŸ” parsing_config ì²˜ë¦¬ ì‹œì‘")
                    config_dict = kwargs['parsing_config']
                    if isinstance(config_dict, dict):
                        print(f"ğŸ” dict íƒ€ì… parsing_config ì²˜ë¦¬")
                        for key, value in config_dict.items():
                            if hasattr(self.config, key):
                                setattr(self.config, key, value)
                        print(f"âœ… dict íƒ€ì… parsing_config ì²˜ë¦¬ ì™„ë£Œ")
                    elif isinstance(config_dict, EnhancedHumanParsingConfig):
                        print(f"ğŸ” EnhancedHumanParsingConfig íƒ€ì… parsing_config ì²˜ë¦¬")
                        self.config = config_dict
                        print(f"âœ… EnhancedHumanParsingConfig íƒ€ì… parsing_config ì²˜ë¦¬ ì™„ë£Œ")
                print(f"âœ… Human Parsing ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ")
                
                # ğŸ”¥ ê³ ê¸‰ í›„ì²˜ë¦¬ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
                print(f"ğŸ” ê³ ê¸‰ í›„ì²˜ë¦¬ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì‹œì‘")
                self.postprocessor = AdvancedPostProcessor()
                print(f"âœ… ê³ ê¸‰ í›„ì²˜ë¦¬ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
                
                # ì„±ëŠ¥ í†µê³„ í™•ì¥
                print(f"ğŸ” ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™” ì‹œì‘")
                self.ai_stats = {
                    'total_processed': 0,
                    'preprocessing_time': 0.0,
                    'parsing_time': 0.0,
                    'postprocessing_time': 0.0,
                    'graphonomy_calls': 0,
                    'u2net_calls': 0,
                    'crf_postprocessing_calls': 0,
                    'multiscale_processing_calls': 0,
                    'edge_refinement_calls': 0,
                    'quality_enhancement_calls': 0,
                    'progressive_parsing_calls': 0,
                    'self_correction_calls': 0,
                    'iterative_refinement_calls': 0,
                    'hybrid_ensemble_calls': 0,
                    'aspp_module_calls': 0,
                    'self_attention_calls': 0,
                    'average_confidence': 0.0,
                    'total_algorithms_applied': 0
                }
                print(f"âœ… ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™” ì™„ë£Œ")
                
                # ì„±ëŠ¥ ìµœì í™”
                print(f"ğŸ” ThreadPoolExecutor ì´ˆê¸°í™” ì‹œì‘")
                from concurrent.futures import ThreadPoolExecutor
                print(f"âœ… ThreadPoolExecutor import ì„±ê³µ")
                self.executor = ThreadPoolExecutor(
                    max_workers=4 if IS_M3_MAX else 2,
                    thread_name_prefix="human_parsing"
                )
                print(f"âœ… ThreadPoolExecutor ì´ˆê¸°í™” ì™„ë£Œ")
                
                print(f"ğŸ” ë¡œê±° ì •ë³´ ì¶œë ¥ ì‹œì‘")
                self.logger.info(f"âœ… {self.step_name} Central Hub DI Container v7.0 ê¸°ë°˜ ì´ˆê¸°í™” ì™„ë£Œ")
                self.logger.info(f"   - Device: {self.device}")
                self.logger.info(f"   - M3 Max: {IS_M3_MAX}")
                print(f"âœ… ë¡œê±° ì •ë³´ ì¶œë ¥ ì™„ë£Œ")
                
                print(f"ğŸ‰ HumanParsingStep __init__ ì™„ë£Œ!")
                
            except Exception as e:
                print(f"âŒ HumanParsingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print(f"âŒ ì˜¤ë¥˜ íƒ€ì…: {type(e)}")
                import traceback
                print(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                self.logger.error(f"âŒ HumanParsingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self._emergency_setup(**kwargs)
        
        def _emergency_setup(self, **kwargs):
            """ê¸´ê¸‰ ì„¤ì • (ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ)"""
            print(f"ğŸ” HumanParsingStep _emergency_setup ì‹œì‘")
            try:
                print(f"ğŸ” step_name ì„¤ì • ì‹œì‘")
                self.step_name = "HumanParsingStep"
                print(f"âœ… step_name ì„¤ì • ì™„ë£Œ")
                
                print(f"ğŸ” step_id ì„¤ì • ì‹œì‘")
                self.step_id = 1
                print(f"âœ… step_id ì„¤ì • ì™„ë£Œ")
                
                print(f"ğŸ” device ì„¤ì • ì‹œì‘")
                self.device = kwargs.get('device', 'cpu')
                print(f"âœ… device ì„¤ì • ì™„ë£Œ: {self.device}")
                
                print(f"ğŸ” ai_models ì„¤ì • ì‹œì‘")
                self.ai_models = {}
                print(f"âœ… ai_models ì„¤ì • ì™„ë£Œ")
                
                print(f"ğŸ” models_loading_status ì„¤ì • ì‹œì‘")
                self.models_loading_status = {'mock': True}
                print(f"âœ… models_loading_status ì„¤ì • ì™„ë£Œ")
                
                print(f"ğŸ” model_interface ì„¤ì • ì‹œì‘")
                self.model_interface = None
                print(f"âœ… model_interface ì„¤ì • ì™„ë£Œ")
                
                print(f"ğŸ” loaded_models ì„¤ì • ì‹œì‘")
                self.loaded_models = []
                print(f"âœ… loaded_models ì„¤ì • ì™„ë£Œ")
                
                print(f"ğŸ” config ì„¤ì • ì‹œì‘")
                self.config = EnhancedHumanParsingConfig()
                print(f"âœ… config ì„¤ì • ì™„ë£Œ")
                
                print(f"âœ… ê¸´ê¸‰ ì„¤ì • ì™„ë£Œ")
                self.logger.warning("âš ï¸ ê¸´ê¸‰ ì„¤ì • ëª¨ë“œë¡œ ì´ˆê¸°í™”ë¨")
            except Exception as e:
                print(f"âŒ ê¸´ê¸‰ ì„¤ì •ë„ ì‹¤íŒ¨: {e}")
                print(f"âŒ ê¸´ê¸‰ ì„¤ì • ì˜¤ë¥˜ íƒ€ì…: {type(e)}")
                import traceback
                print(f"âŒ ê¸´ê¸‰ ì„¤ì • ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        
        # ==============================================
        # ğŸ”¥ Central Hub DI Container ì—°ë™ ë©”ì„œë“œë“¤
        # ==============================================
        
        def _load_ai_models_via_central_hub(self) -> bool:
            """ğŸ”¥ Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”© (í•„ìˆ˜ êµ¬í˜„)"""
            try:
                self.logger.info("ğŸ”„ Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
                
                # Central Hub DI Container ê°€ì ¸ì˜¤ê¸° (ì•ˆì „í•œ ë°©ë²•)
                container = None
                try:
                    # ì „ì—­ í•¨ìˆ˜ë¡œ ì •ì˜ëœ _get_central_hub_container ì‚¬ìš©
                    container = _get_central_hub_container()
                except NameError:
                    # í•¨ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš° ì•ˆì „í•œ ëŒ€ì•ˆ ì‚¬ìš©
                    try:
                        if hasattr(self, 'central_hub_container'):
                            container = self.central_hub_container
                        elif hasattr(self, 'di_container'):
                            container = self.di_container
                    except Exception:
                        pass
                if not container:
                    self.logger.warning("âš ï¸ Central Hub DI Container ì—†ìŒ - í´ë°± ëª¨ë¸ ì‚¬ìš©")
                    return self._load_fallback_models()
                
                # ModelLoader ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                model_loader = container.get('model_loader')
                if not model_loader:
                    self.logger.warning("âš ï¸ ModelLoader ì„œë¹„ìŠ¤ ì—†ìŒ - í´ë°± ëª¨ë¸ ì‚¬ìš©")
                    return self._load_fallback_models()
                
                self.model_interface = model_loader
                self.model_loader = model_loader  # ì§ì ‘ ì°¸ì¡° ì¶”ê°€
                success_count = 0
                
                # 1. Graphonomy ëª¨ë¸ ë¡œë”© ì‹œë„ (1.2GB ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸)
                try:
                    graphonomy_model = self._load_graphonomy_via_central_hub(model_loader)
                    if graphonomy_model:
                        self.ai_models['graphonomy'] = graphonomy_model
                        self.models_loading_status['graphonomy'] = True
                        self.loaded_models.append('graphonomy')
                        success_count += 1
                        self.logger.info("âœ… Graphonomy ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                    else:
                        self.logger.warning("âš ï¸ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                # 2. U2Net í´ë°± ëª¨ë¸ ë¡œë”© ì‹œë„
                try:
                    u2net_model = self._load_u2net_via_central_hub(model_loader)
                    if u2net_model:
                        self.ai_models['u2net'] = u2net_model
                        self.models_loading_status['u2net'] = True
                        self.loaded_models.append('u2net')
                        success_count += 1
                        self.logger.info("âœ… U2Net ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                    else:
                        self.logger.warning("âš ï¸ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                # 3. ìµœì†Œ 1ê°œ ëª¨ë¸ì´ë¼ë„ ë¡œë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if success_count > 0:
                    self.logger.info(f"âœ… Central Hub ê¸°ë°˜ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {success_count}ê°œ ëª¨ë¸")
                    return True
                else:
                    self.logger.warning("âš ï¸ ëª¨ë“  ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - Mock ëª¨ë¸ ì‚¬ìš©")
                    return self._load_fallback_models()
                
            except Exception as e:
                self.logger.error(f"âŒ Central Hub ê¸°ë°˜ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return self._load_fallback_models()
        
        def _load_graphonomy_via_central_hub(self, model_loader) -> Optional[nn.Module]:
            """Central Hubë¥¼ í†µí•œ Graphonomy ëª¨ë¸ ë¡œë”©"""
            try:
                # ModelLoaderë¥¼ í†µí•œ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                model_request = {
                    'model_name': 'graphonomy.pth',
                    'step_name': 'HumanParsingStep',
                    'device': self.device,
                    'model_type': 'human_parsing'
                }
                
                loaded_model = model_loader.load_model(**model_request)
                
                if loaded_model and hasattr(loaded_model, 'model'):
                    # ì‹¤ì œ ë¡œë“œëœ ëª¨ë¸ ë°˜í™˜
                    return loaded_model.model
                elif loaded_model and hasattr(loaded_model, 'checkpoint_data'):
                    # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ ëª¨ë¸ ìƒì„±
                    return self._create_graphonomy_from_checkpoint(loaded_model.checkpoint_data)
                else:
                    # í´ë°±: ì•„í‚¤í…ì²˜ë§Œ ìƒì„±
                    self.logger.warning("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ - ì•„í‚¤í…ì²˜ë§Œ ìƒì„±")
                    return self._create_model('graphonomy')
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return self._create_model('graphonomy')
        
        def _load_u2net_via_central_hub(self, model_loader) -> Optional[nn.Module]:
            """Central Hubë¥¼ í†µí•œ U2Net ëª¨ë¸ ë¡œë”©"""
            try:
                # U2Net ëª¨ë¸ ìš”ì²­
                model_request = {
                    'model_name': 'u2net.pth',
                    'step_name': 'HumanParsingStep',
                    'device': self.device,
                    'model_type': 'cloth_segmentation'
                }
                
                loaded_model = model_loader.load_model(**model_request)
                
                if loaded_model and hasattr(loaded_model, 'model'):
                    return loaded_model.model
                else:
                    # í´ë°±: U2Net ì•„í‚¤í…ì²˜ ìƒì„±
                    return self._create_model('u2net')
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return self._create_model('u2net')
        
        def _load_fallback_models(self) -> bool:
            """í´ë°± ëª¨ë¸ ë¡œë”© (ì—ëŸ¬ ë°©ì§€ìš©)"""
            try:
                self.logger.info("ğŸ”„ í´ë°± ëª¨ë¸ ë¡œë”©...")
                
                # Mock ëª¨ë¸ ìƒì„±
                mock_model = self._create_model('mock')
                if mock_model:
                    self.ai_models['mock'] = mock_model
                    self.models_loading_status['mock'] = True
                    self.loaded_models.append('mock')
                    self.logger.info("âœ… Mock ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                    return True
                
                return False
                
            except Exception as e:
                self.logger.error(f"âŒ í´ë°± ëª¨ë¸ ë¡œë”©ë„ ì‹¤íŒ¨: {e}")
                return False
        
        # ==============================================
        # ğŸ”¥ ëª¨ë¸ ìƒì„± í—¬í¼ ë©”ì„œë“œë“¤
        # ==============================================
        
        def _create_graphonomy_from_checkpoint(self, checkpoint_data) -> Optional[nn.Module]:
            """ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ Graphonomy ëª¨ë¸ ìƒì„±"""
            try:
                model = AdvancedGraphonomyResNetASPP(num_classes=20)
                
                # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ë¡œë”©
                if isinstance(checkpoint_data, dict):
                    if 'state_dict' in checkpoint_data:
                        state_dict = checkpoint_data['state_dict']
                    elif 'model' in checkpoint_data:
                        state_dict = checkpoint_data['model']
                    else:
                        state_dict = checkpoint_data
                else:
                    state_dict = checkpoint_data
                
                # state_dict ë¡œë”© (strict=Falseë¡œ í˜¸í™˜ì„± ë³´ì¥)
                model.load_state_dict(state_dict, strict=False)
                model.to(self.device)
                model.eval()
                
                return model
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œ Graphonomy ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
                return self._create_model('graphonomy')
        
        def _create_model(self, model_type: str = 'graphonomy', checkpoint_data=None, device=None, **kwargs) -> nn.Module:
            """í†µí•© ëª¨ë¸ ìƒì„± í•¨ìˆ˜ (ì²´í¬í¬ì¸íŠ¸ ì§€ì›)"""
            try:
                if device is None:
                    device = self.device
                
                self.logger.info(f"ğŸ”„ {model_type} ëª¨ë¸ ìƒì„± ì¤‘...")
                
                # ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ” ê²½ìš° ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒì„±
                if checkpoint_data is not None:
                    try:
                        from ..utils.graphonomy_checkpoint_system import UnifiedGraphonomyCheckpointSystem
                        checkpoint_system = UnifiedGraphonomyCheckpointSystem()
                        model = checkpoint_system.create_model_from_checkpoint(checkpoint_data, device)
                        
                        if model is not None:
                            self.logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ìƒì„± ì„±ê³µ")
                            return model
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                # ëª¨ë¸ íƒ€ì…ë³„ ìƒì„± (í´ë°±)
                if model_type == 'graphonomy':
                    model = AdvancedGraphonomyResNetASPP(num_classes=20)
                    model.checkpoint_path = "fallback_graphonomy_model"
                    model.checkpoint_data = {"graphonomy": True, "fallback": True, "model_type": "AdvancedGraphonomyResNetASPP"}
                    model.memory_usage_mb = 1200.0
                    model.load_time = 2.5
                elif model_type == 'u2net':
                    model = U2NetForParsing(num_classes=20)
                    model.checkpoint_path = "u2net_model"
                    model.checkpoint_data = {"u2net": True, "model_type": "U2NetForParsing"}
                    model.memory_usage_mb = 50.0
                    model.load_time = 1.0
                elif model_type == 'mock':
                    model = MockHumanParsingModel(num_classes=20)
                    model.checkpoint_path = "fallback_mock_model"
                    model.checkpoint_data = {"mock": True, "fallback": True, "model_type": "MockHumanParsingModel"}
                    model.memory_usage_mb = 0.1
                    model.load_time = 0.1
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
                
                # ê³µí†µ ì„¤ì •
                model.to(device)
                model.eval()
                model.get_checkpoint_data = lambda: model.checkpoint_data
                model.has_model = True
                
                self.logger.info(f"âœ… {model_type} ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                return model
                
            except Exception as e:
                self.logger.error(f"âŒ {model_type} ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
                # ìµœì¢… í´ë°±: Mock ëª¨ë¸
                return self._create_model('mock', device=device)
        # ==============================================
        # ğŸ”¥ í•µì‹¬: _run_ai_inference() ë©”ì„œë“œ (BaseStepMixin ìš”êµ¬ì‚¬í•­)
        # ==============================================
        
        def _run_ai_inference(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
            """ğŸ”¥ ì‹¤ì œ Human Parsing AI ì¶”ë¡  (Mock ì œê±°, ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©) + ëª©ì—… ë°ì´í„° ì§„ë‹¨"""
            try:
                self.logger.info("ğŸ”„ _run_ai_inference ì‹œì‘")
                
                # ğŸ”¥ ëª©ì—… ë°ì´í„° ì§„ë‹¨ (ìƒˆë¡œ ì¶”ê°€)
                if MOCK_DIAGNOSTIC_AVAILABLE:
                    try:
                        # ì…ë ¥ ë°ì´í„°ì—ì„œ ëª©ì—… ë°ì´í„° ê°ì§€
                        for key, value in input_data.items():
                            if value is not None:
                                mock_detection = detect_mock_data(value)
                                if mock_detection['is_mock']:
                                    self.logger.warning(f"AI ì¶”ë¡  ì…ë ¥ ë°ì´í„° '{key}'ì—ì„œ ëª©ì—… ë°ì´í„° ê°ì§€: {mock_detection}")
                                    # ì—ëŸ¬ ì¶”ì 
                                    log_detailed_error(
                                        MockDataDetectionError(
                                            message=f"AI ì¶”ë¡  ì…ë ¥ ë°ì´í„° '{key}'ì—ì„œ ëª©ì—… ë°ì´í„° ê°ì§€",
                                            error_code="MOCK_DATA_DETECTED",
                                            context={'input_key': key, 'detection_result': mock_detection}
                                        ),
                                        {
                                            'step_name': self.step_name,
                                            'step_id': getattr(self, 'step_id', 1),
                                            'operation': '_run_ai_inference',
                                            'input_key': key
                                        },
                                        getattr(self, 'step_id', 1)
                                    )
                    except Exception as e:
                        self.logger.warning(f"AI ì¶”ë¡  ì…ë ¥ ë°ì´í„° ëª©ì—… ì§„ë‹¨ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # ğŸ”¥ ë””ë²„ê¹…: ì…ë ¥ ë°ì´í„° ìƒì„¸ ë¡œê¹…
                self.logger.info(f"ğŸ” [DEBUG] Human Parsing ì…ë ¥ ë°ì´í„° í‚¤ë“¤: {list(input_data.keys())}")
                self.logger.info(f"ğŸ” [DEBUG] Human Parsing ì…ë ¥ ë°ì´í„° íƒ€ì…ë“¤: {[(k, type(v).__name__) for k, v in input_data.items()]}")
                
                # ì…ë ¥ ë°ì´í„° ê²€ì¦
                if not input_data:
                    error_msg = "ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"
                    self.logger.error(f"âŒ [DEBUG] Human Parsing {error_msg}")
                    
                    # í†µí•©ëœ ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‚¬ìš©
                    if EXCEPTIONS_AVAILABLE:
                        error = DataValidationError(
                            error_msg, 
                            ErrorCodes.DATA_VALIDATION_FAILED, 
                            {
                                'step_name': 'HumanParsingStep',
                                'operation': '_run_ai_inference',
                                'input_data_keys': list(input_data.keys()) if input_data else []
                            }
                        )
                        track_exception(error, {'operation': '_run_ai_inference'}, 1)
                        raise error
                    else:
                        raise ValueError(error_msg)
                
                self.logger.info(f"âœ… [DEBUG] Human Parsing ì…ë ¥ ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
                
                # ğŸ”¥ 1. ModelLoader ì˜ì¡´ì„± í™•ì¸
                self.logger.debug("ğŸ”„ ModelLoader ì˜ì¡´ì„± í™•ì¸ ì¤‘...")
                try:
                    has_model_loader = hasattr(self, 'model_loader')
                    self.logger.debug(f"ğŸ”„ hasattr(self, 'model_loader'): {has_model_loader}")
                    
                    if has_model_loader:
                        model_loader_value = self.model_loader
                        self.logger.debug(f"ğŸ”„ self.model_loader ê°’: {type(model_loader_value)}")
                        
                        # ì•ˆì „í•œ boolean ê²€ì¦
                        if model_loader_value is None:
                            error_msg = "ModelLoaderê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ - DI Container ì—°ë™ í•„ìš”"
                            self.logger.debug(f"ğŸ”„ {error_msg}")
                            
                            # í†µí•©ëœ ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‚¬ìš©
                            if EXCEPTIONS_AVAILABLE:
                                error = ConfigurationError(
                                    error_msg, 
                                    ErrorCodes.CONFIGURATION_ERROR, 
                                    {
                                        'step_name': 'HumanParsingStep',
                                        'operation': '_run_ai_inference',
                                        'model_loader_type': 'None'
                                    }
                                )
                                track_exception(error, {'operation': '_run_ai_inference'}, 1)
                                raise error
                            else:
                                raise ValueError(error_msg)
                        elif hasattr(model_loader_value, '__bool__'):
                            # __bool__ ë©”ì„œë“œê°€ ìˆëŠ” ê²½ìš°
                            try:
                                bool_result = bool(model_loader_value)
                                self.logger.debug(f"ğŸ”„ bool(model_loader): {bool_result}")
                                if not bool_result:
                                    raise ValueError("ModelLoaderê°€ False - DI Container ì—°ë™ í•„ìš”")
                            except Exception as bool_error:
                                self.logger.debug(f"ğŸ”„ bool() í˜¸ì¶œ ì‹¤íŒ¨: {bool_error}")
                                # bool() í˜¸ì¶œì´ ì‹¤íŒ¨í•´ë„ Noneì´ ì•„ë‹ˆë©´ ê³„ì† ì§„í–‰
                        else:
                            self.logger.debug("ğŸ”„ model_loaderì— __bool__ ë©”ì„œë“œ ì—†ìŒ, Noneì´ ì•„ë‹ˆë¯€ë¡œ ê³„ì† ì§„í–‰")
                    else:
                        self.logger.debug("ğŸ”„ model_loader ì†ì„±ì´ ì—†ìŒ")
                        raise ValueError("ModelLoaderê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ - DI Container ì—°ë™ í•„ìš”")
                        
                    self.logger.debug("âœ… ModelLoader ì˜ì¡´ì„± í™•ì¸ ì™„ë£Œ")
                except Exception as e:
                    self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± í™•ì¸ ì‹¤íŒ¨: {e}")
                    raise
                
                # ğŸ”¥ 2. ì…ë ¥ ë°ì´í„° ê²€ì¦ (ë‹¤ì–‘í•œ í‚¤ ì´ë¦„ ì§€ì› + ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ)
                self.logger.debug("ğŸ”„ ì…ë ¥ ë°ì´í„° ê²€ì¦ ì¤‘...")
                try:
                    image = input_data.get('image')
                    self.logger.debug(f"ğŸ”„ input_data.get('image'): {type(image)}")
                    
                    if image is None:
                        image = input_data.get('person_image')
                        self.logger.debug(f"ğŸ”„ input_data.get('person_image'): {type(image)}")
                    
                    if image is None:
                        image = input_data.get('input_image')
                        self.logger.debug(f"ğŸ”„ input_data.get('input_image'): {type(image)}")
                    
                    # ğŸ”¥ ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ (ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°)
                    if image is None and 'session_id' in input_data:
                        try:
                            self.logger.info("ğŸ”„ ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹œë„...")
                            session_manager = self._get_service_from_central_hub('session_manager')
                            if session_manager:
                                person_image, clothing_image = None, None
                                
                                try:
                                    # ì„¸ì…˜ ë§¤ë‹ˆì €ê°€ ë™ê¸° ë©”ì„œë“œë¥¼ ì œê³µí•˜ëŠ”ì§€ í™•ì¸
                                    if hasattr(session_manager, 'get_session_images_sync'):
                                        person_image, clothing_image = session_manager.get_session_images_sync(input_data['session_id'])
                                    elif hasattr(session_manager, 'get_session_images'):
                                        # ë¹„ë™ê¸° ë©”ì„œë“œë¥¼ ë™ê¸°ì ìœ¼ë¡œ í˜¸ì¶œ
                                        import asyncio
                                        import concurrent.futures
                                        
                                        def run_async_session_load():
                                            try:
                                                return asyncio.run(session_manager.get_session_images(input_data['session_id']))
                                            except Exception as async_error:
                                                self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨: {async_error}")
                                                return None, None
                                        
                                        try:
                                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                                future = executor.submit(run_async_session_load)
                                                person_image, clothing_image = future.result(timeout=10)
                                        except Exception as executor_error:
                                            self.logger.warning(f"âš ï¸ ì„¸ì…˜ ë¡œë“œ ThreadPoolExecutor ì‹¤íŒ¨: {executor_error}")
                                            person_image, clothing_image = None, None
                                    else:
                                        self.logger.warning("âš ï¸ ì„¸ì…˜ ë§¤ë‹ˆì €ì— ì ì ˆí•œ ë©”ì„œë“œê°€ ì—†ìŒ")
                                except Exception as e:
                                    self.logger.warning(f"âš ï¸ ì„¸ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
                                    person_image, clothing_image = None, None
                                
                                if person_image:
                                    image = person_image
                                    self.logger.info("âœ… ì„¸ì…˜ì—ì„œ person_image ë¡œë“œ ì™„ë£Œ")
                                else:
                                    self.logger.warning("âš ï¸ ì„¸ì…˜ì—ì„œ person_imageë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    
                    if image is None:
                        # ë””ë²„ê¹…ì„ ìœ„í•œ ì…ë ¥ ë°ì´í„° ë¡œê¹…
                        self.logger.warning(f"âš ï¸ ì…ë ¥ ë°ì´í„° í‚¤ë“¤: {list(input_data.keys())}")
                        
                        error_msg = "ì…ë ¥ ì´ë¯¸ì§€ ì—†ìŒ"
                        
                        # ì—ëŸ¬ ì¶”ì 
                        track_exception(
                            DataValidationError(error_msg, ErrorCodes.DATA_VALIDATION_FAILED, {
                                'step_name': 'HumanParsingStep',
                                'operation': '_run_ai_inference',
                                'input_data_keys': list(input_data.keys()),
                                'session_id': input_data.get('session_id', 'unknown')
                            }),
                            context={'operation': '_run_ai_inference'},
                            step_id=1
                        )
                        
                        raise DataValidationError(error_msg, ErrorCodes.DATA_VALIDATION_FAILED)
                    
                    self.logger.debug(f"ğŸ”„ ìµœì¢… ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
                    self.logger.debug("âœ… ì…ë ¥ ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
                except Exception as e:
                    self.logger.error(f"âŒ ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
                    raise
                
                self.logger.info("ğŸ”„ Human Parsing ì‹¤ì œ AI ì¶”ë¡  ì‹œì‘")
                start_time = time.time()
                
                # ğŸ”¥ 3. Graphonomy ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)
                try:
                    self.logger.debug("ğŸ”„ _load_graphonomy_model í˜¸ì¶œ ì‹œì‘")
                    graphonomy_model = self._load_graphonomy_model()
                    self.logger.debug(f"ğŸ”„ _load_graphonomy_model ê²°ê³¼: {type(graphonomy_model)}")
                    if graphonomy_model is None:
                        error_msg = "Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"
                        
                        # ì—ëŸ¬ ì¶”ì 
                        track_exception(
                            ModelLoadingError(error_msg, ErrorCodes.MODEL_LOADING_FAILED, {
                                'step_name': 'HumanParsingStep',
                                'operation': '_run_ai_inference',
                                'model_name': 'Graphonomy',
                                'device': self.device
                            }),
                            context={'operation': '_run_ai_inference'},
                            step_id=1
                        )
                        
                        raise ModelLoadingError(error_msg, ErrorCodes.MODEL_LOADING_FAILED)
                    self.logger.debug("âœ… Graphonomy ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ë‹¨ê³„ ì‹¤íŒ¨: {e}")
                    import traceback
                    self.logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                    raise
                
                # ğŸ”¥ 4. ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì‚¬ìš© (ì‹¤ì œ AI ì¶”ë¡  ê°•ì œ)
                try:
                    checkpoint_data = graphonomy_model.get_checkpoint_data()
                    if checkpoint_data is None:
                        self.logger.error("âŒ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì—†ìŒ - ì‹¤ì œ íŒŒì¼ì—ì„œ ë¡œë”©ëœ ëª¨ë¸ì´ì–´ì•¼ í•¨")
                        raise ValueError("ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì—†ìŒ")
                    
                    self.logger.debug(f"âœ… ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì‚¬ìš©: {len(checkpoint_data)}ê°œ í‚¤")
                except Exception as e:
                    self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì ‘ê·¼ ì‹¤íŒ¨: {e}")
                    raise ValueError(f"ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                # ğŸ”¥ 5. GPU/MPS ë””ë°”ì´ìŠ¤ ì„¤ì •
                device = 'mps' if torch.backends.mps.is_available() else 'cpu'
                
                # ğŸ”¥ 6. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                try:
                    processed_input = self._preprocess_image_for_graphonomy(image, device)
                except Exception as e:
                    self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
                    raise
                
                # ğŸ”¥ 7. ëª¨ë¸ ì¶”ë¡  (ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)
                try:
                    with torch.no_grad():
                        parsing_output = self._run_graphonomy_inference(processed_input, checkpoint_data, device)
                except Exception as e:
                    self.logger.error(f"âŒ ëª¨ë¸ ì¶”ë¡  ë‹¨ê³„ ì‹¤íŒ¨: {e}")
                    raise
                
                # ğŸ”¥ 8. í›„ì²˜ë¦¬
                try:
                    self.logger.info(f"ğŸ” parsing_output íƒ€ì…: {type(parsing_output)}")
                    if isinstance(parsing_output, dict):
                        self.logger.debug(f"ğŸ” parsing_output í‚¤ë“¤: {list(parsing_output.keys())}")
                    
                    # original_size ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    if hasattr(image, 'size'):
                        if isinstance(image.size, (tuple, list)) and len(image.size) >= 2:
                            original_size = (image.size[0], image.size[1])
                        else:
                            original_size = (512, 512)
                    else:
                        original_size = (512, 512)
                    
                    self.logger.info(f"ğŸ” original_size: {original_size} (íƒ€ì…: {type(original_size)})")
                    parsing_result = self._postprocess_graphonomy_output(parsing_output, original_size)
                except Exception as e:
                    self.logger.error(f"âŒ í›„ì²˜ë¦¬ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
                    import traceback
                    self.logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                    raise
                
                # ì‹ ë¢°ë„ ê³„ì‚°
                confidence = self._calculate_parsing_confidence(parsing_output)
                
                inference_time = time.time() - start_time
                
                return {
                    'success': True,
                    'parsing_result': parsing_result,
                    'original_image': image,  # ğŸ”¥ ì›ë³¸ ì´ë¯¸ì§€ ì¶”ê°€
                    'confidence': confidence,
                    'processing_time': inference_time,
                    'device_used': device,
                    'model_loaded': True,
                    'checkpoint_used': True,
                    'step_name': self.step_name,
                    'model_info': {
                        'model_name': 'Graphonomy',
                        'checkpoint_size_mb': graphonomy_model.memory_usage_mb,
                        'load_time': graphonomy_model.load_time
                    }
                }
                
            except Exception as e:
                self.logger.error(f"âŒ Human Parsing AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
                return self._create_error_response(str(e))
        
        def _load_graphonomy_model(self):
            """Graphonomy ëª¨ë¸ ë¡œë”© (ì‹¤ì œ íŒŒì¼ ê°•ì œ ë¡œë”©)"""
            try:
                self.logger.debug("ğŸ”„ Graphonomy ëª¨ë¸ ë¡œë”© ì‹œì‘...")
                
                # ğŸ”¥ ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì§ì ‘ ë¡œë”©
                import torch
                from pathlib import Path
                
                # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë“¤ (í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ì‹¤ì œ íŒŒì¼ë“¤)
                possible_paths = [
                    "ai_models/Self-Correction-Human-Parsing/exp-schp-201908261155-atr.pth",
                    "ai_models/human_parsing/schp/pytorch_model.bin",
                    "ai_models/human_parsing/models--mattmdjaga--segformer_b2_clothes/snapshots/c4d76e5d0058ab0e3e805d5382c44d5bd059fee3/pytorch_model.bin",
                    "ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/humanparsing/exp-schp-201908301523-atr.pth",
                    "u2net.pth"
                ]
                
                for model_path in possible_paths:
                    try:
                        full_path = Path(model_path)
                        if full_path.exists():
                            self.logger.info(f"ğŸ”„ ì‹¤ì œ íŒŒì¼ ë¡œë”© ì‹œë„: {model_path}")
                            
                            # ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                            checkpoint = torch.load(str(full_path), map_location='cpu')
                            self.logger.debug(f"âœ… ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ: {len(checkpoint)}ê°œ í‚¤")
                            
                            # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ìƒì„¸ ë¶„ì„ (DEBUG ë ˆë²¨ë¡œ ë³€ê²½)
                            self.logger.debug(f"ğŸ” ì²´í¬í¬ì¸íŠ¸ í‚¤ë“¤: {list(checkpoint.keys())}")
                            for key, value in checkpoint.items():
                                if hasattr(value, 'shape'):
                                    self.logger.debug(f"ğŸ” {key}: {value.shape}")
                                else:
                                    self.logger.debug(f"ğŸ” {key}: {type(value)}")
                            
                            # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë¶„ì„ (full_pathë¥¼ ì „ë‹¬)
                            model = self._create_dynamic_model_from_checkpoint(checkpoint, str(full_path))
                            
                            # ì‹¤ì œ íŒŒì¼ ë¡œë”© ì„±ê³µ í™•ì¸
                            self.logger.info(f"ğŸ¯ ì‹¤ì œ íŒŒì¼ ë¡œë”© ì„±ê³µ: {model_path}")
                            self.logger.info(f"ğŸ¯ ëª¨ë¸ íƒ€ì…: {type(model)}")
                            self.logger.debug(f"ğŸ¯ ì²´í¬í¬ì¸íŠ¸ í‚¤ ìˆ˜: {len(checkpoint)}")
                            self.logger.info(f"âœ… ë™ì  ëª¨ë¸ ìƒì„± ì™„ë£Œ: {type(model)}")
                            self.logger.info(f"ğŸ‰ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ! Mock ëª¨ë“œ ì‚¬ìš© ì•ˆí•¨!")
                            model.eval()
                            
                            # ëª¨ë¸ì— ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì¶”ê°€
                            model.checkpoint_data = checkpoint
                            model.get_checkpoint_data = lambda: checkpoint
                            model.has_model = True
                            model.memory_usage_mb = full_path.stat().st_size / (1024 * 1024)
                            model.load_time = 2.5
                            
                            self.logger.info(f"âœ… ì‹¤ì œ Graphonomy ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_path}")
                            # ì‹¤ì œ ë¡œë”©ëœ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥
                            self._loaded_model = model
                            return model
                            
                    except Exception as e:
                        self.logger.debug(f"âš ï¸ {model_path} ë¡œë”© ì‹¤íŒ¨: {e}")
                        continue
                
                # ğŸ”¥ ì‹¤ì œ íŒŒì¼ì´ ì—†ìœ¼ë©´ Mock ëª¨ë¸ ì‚¬ìš©
                self.logger.warning("âš ï¸ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - Mock ëª¨ë¸ ì‚¬ìš©")
                mock_model = self._create_model('mock')
                self.logger.info("âœ… Mock ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                return mock_model
                
            except Exception as e:
                self.logger.error(f"âŒ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise ValueError(f"ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        def _run_actual_graphonomy_inference(self, input_tensor, device: str):
            """ğŸ”¥ ì‹¤ì œ Graphonomy ë…¼ë¬¸ ê¸°ë°˜ AI ì¶”ë¡  (Mock ì œê±°)"""
            try:
                # ğŸ”¥ ì•ˆì „í•œ ì¶”ë¡ ì„ ìœ„í•œ ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”
                self.logger.info("ğŸ¯ ê³ ê¸‰ Graphonomy ì¶”ë¡  ì‹œì‘")
                
                # ì…ë ¥ í…ì„œ ê²€ì¦
                if input_tensor is None:
                    raise ValueError("ì…ë ¥ í…ì„œê°€ Noneì…ë‹ˆë‹¤")
                
                if input_tensor.dim() != 4:
                    raise ValueError(f"ì…ë ¥ í…ì„œ ì°¨ì› ì˜¤ë¥˜: {input_tensor.dim()}, ì˜ˆìƒ: 4")
                
                self.logger.info(f"âœ… ì…ë ¥ í…ì„œ ê²€ì¦ ì™„ë£Œ: {input_tensor.shape}")
                # ğŸ”¥ 1. ì‹¤ì œ Graphonomy ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡°
                class GraphonomyResNet101ASPP(nn.Module):
                    """Graphonomy ë…¼ë¬¸ì˜ ì‹¤ì œ ì‹ ê²½ë§ êµ¬ì¡°"""
                    def __init__(self, num_classes=20):
                        super().__init__()
                        
                        # ResNet-101 ë°±ë³¸ (ë…¼ë¬¸ê³¼ ë™ì¼)
                        self.backbone = self._create_resnet101_backbone()
                        
                        # ASPP ëª¨ë“ˆ (Atrous Spatial Pyramid Pooling)
                        self.aspp = ASPPModule(in_channels=2048, out_channels=256)
                        
                        # Self-Attention ëª¨ë“ˆ
                        self.self_attention = SelfAttentionBlock(in_channels=256)
                        
                        # Progressive Parsing ëª¨ë“ˆ
                        self.progressive_parsing = ProgressiveParsingModule(num_classes=num_classes)
                        
                        # Self-Correction ëª¨ë“ˆ
                        self.self_correction = SelfCorrectionModule(num_classes=num_classes)
                        
                        # Iterative Refinement ëª¨ë“ˆ
                        self.iterative_refinement = IterativeRefinementModule(num_classes=num_classes)
                        
                        # ìµœì¢… ë¶„ë¥˜ í—¤ë“œ
                        self.classifier = nn.Conv2d(256, num_classes, kernel_size=1)
                        
                        # Edge Detection í—¤ë“œ
                        self.edge_head = nn.Conv2d(256, 1, kernel_size=1)
                        
                        self._init_weights()
                    
                    def _create_resnet101_backbone(self):
                        """ResNet-101 ë°±ë³¸ ìƒì„± (ë…¼ë¬¸ê³¼ ë™ì¼)"""
                        backbone = nn.Sequential(
                            # Conv1: 7x7, 64 channels
                            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
                            nn.BatchNorm2d(64),
                            nn.ReLU(inplace=True),
                            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
                            
                            # Layer1: 3 blocks, 256 channels
                            self._make_layer(64, 64, 3, stride=1),
                            
                            # Layer2: 4 blocks, 512 channels
                            self._make_layer(256, 128, 4, stride=2),
                            
                            # Layer3: 23 blocks, 1024 channels
                            self._make_layer(512, 256, 23, stride=2),
                            
                            # Layer4: 3 blocks, 2048 channels
                            self._make_layer(1024, 512, 3, stride=2)
                        )
                        return backbone
                    
                    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
                        """ResNet Bottleneck ë¸”ë¡ ìƒì„±"""
                        layers = []
                        layers.append(ResNetBottleneck(in_channels, out_channels, stride))
                        for _ in range(1, blocks):
                            layers.append(ResNetBottleneck(out_channels * 4, out_channels))
                        return nn.Sequential(*layers)
                    
                    def _init_weights(self):
                        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
                        for m in self.modules():
                            if isinstance(m, nn.Conv2d):
                                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                            elif isinstance(m, nn.BatchNorm2d):
                                nn.init.constant_(m.weight, 1)
                                nn.init.constant_(m.bias, 0)
                    
                    def forward(self, x):
                        # ğŸ”¥ ì‹¤ì œ Graphonomy ë…¼ë¬¸ì˜ forward pass
                        
                        # 1. ResNet-101 ë°±ë³¸ íŠ¹ì§• ì¶”ì¶œ
                        features = self.backbone(x)
                        
                        # 2. ASPP ëª¨ë“ˆ ì ìš©
                        aspp_features = self.aspp(features)
                        
                        # 3. Self-Attention ì ìš©
                        attended_features = self.self_attention(aspp_features)
                        
                        # 4. ì´ˆê¸° íŒŒì‹± ì˜ˆì¸¡
                        initial_parsing = self.classifier(attended_features)
                        
                        # 5. Progressive Parsing
                        progressive_results = self.progressive_parsing(initial_parsing, attended_features)
                        
                        # 6. Self-Correction
                        corrected_parsing = self.self_correction(initial_parsing, attended_features)
                        
                        # 7. Iterative Refinement
                        refined_parsing = self.iterative_refinement(corrected_parsing)
                        
                        # 8. Edge Detection
                        edge_output = self.edge_head(attended_features)
                        
                        return {
                            'parsing_pred': refined_parsing,
                            'initial_parsing': initial_parsing,
                            'progressive_results': progressive_results,
                            'corrected_parsing': corrected_parsing,
                            'edge_output': edge_output,
                            'features': attended_features
                        }
                
                # ğŸ”¥ 2. ì‹¤ì œ ëª¨ë¸ ìƒì„± ë° ì¶”ë¡ 
                try:
                    model = GraphonomyResNet101ASPP(num_classes=20).to(device)
                    model.eval()
                    
                    self.logger.info("âœ… Graphonomy ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                    
                    with torch.no_grad():
                        # ì‹¤ì œ ì¶”ë¡  ì‹¤í–‰
                        self.logger.info("ğŸ¯ ëª¨ë¸ ì¶”ë¡  ì‹œì‘")
                        output = model(input_tensor)
                        self.logger.info("âœ… ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ")
                        
                except Exception as model_error:
                    self.logger.error(f"âŒ ëª¨ë¸ ìƒì„±/ì¶”ë¡  ì‹¤íŒ¨: {model_error}")
                    # ğŸ”¥ í´ë°±: ë‹¨ìˆœí™”ëœ ëª¨ë¸ ì‚¬ìš©
                    self.logger.info("ğŸ”„ ë‹¨ìˆœí™”ëœ ëª¨ë¸ë¡œ í´ë°±")
                    
                    class SimpleGraphonomyModel(nn.Module):
                        def __init__(self, num_classes=20):
                            super().__init__()
                            self.encoder = nn.Sequential(
                                nn.Conv2d(3, 64, 3, padding=1),
                                nn.BatchNorm2d(64),
                                nn.ReLU(inplace=True),
                                nn.Conv2d(64, 128, 3, padding=1),
                                nn.BatchNorm2d(128),
                                nn.ReLU(inplace=True),
                                nn.Conv2d(128, 256, 3, padding=1),
                                nn.BatchNorm2d(256),
                                nn.ReLU(inplace=True),
                            )
                            self.classifier = nn.Conv2d(256, num_classes, 1)
                            self.decoder = nn.Sequential(
                                nn.ConvTranspose2d(num_classes, 128, 4, 2, 1),
                                nn.BatchNorm2d(128),
                                nn.ReLU(inplace=True),
                                nn.ConvTranspose2d(128, num_classes, 4, 2, 1),
                            )
                        
                        def forward(self, x):
                            features = self.encoder(x)
                            parsing = self.classifier(features)
                            output = self.decoder(parsing)
                            return {
                                'parsing_pred': output,
                                'confidence_map': torch.sigmoid(output),
                                'final_confidence': torch.sigmoid(output),
                                'edge_output': torch.zeros_like(output[:, :1]),
                                'progressive_results': [output],
                                'actual_ai_mode': True
                            }
                    
                    model = SimpleGraphonomyModel(num_classes=20).to(device)
                    model.eval()
                    
                    with torch.no_grad():
                        output = model(input_tensor)
                    
                    # ğŸ”¥ 3. ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜ ì ìš©
                    try:
                        # 3.1 Confidence ê³„ì‚° (ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜)
                        parsing_probs = F.softmax(output['parsing_pred'], dim=1)
                        confidence_map = torch.max(parsing_probs, dim=1)[0]
                        
                        # 3.2 Edge-guided refinement
                        edge_confidence = torch.sigmoid(output['edge_output'])
                        refined_confidence = confidence_map * edge_confidence.squeeze(1)
                        
                        # 3.3 Multi-scale consistency check
                        multi_scale_confidence = self._calculate_multi_scale_confidence(
                            output['parsing_pred'], output['progressive_results']
                        )
                        
                        # 3.4 Spatial consistency validation
                        spatial_consistency = self._calculate_spatial_consistency(output['parsing_pred'])
                        
                        # ğŸ”¥ 3.5 ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜ ì ìš©
                        
                        # 3.5.1 Adaptive Thresholding
                        adaptive_threshold = self._calculate_adaptive_threshold(output['parsing_pred'])
                        
                        # 3.5.2 Boundary-aware refinement
                        boundary_refined = self._apply_boundary_aware_refinement(
                            output['parsing_pred'], output['edge_output']
                        )
                        
                        # 3.5.3 Context-aware parsing
                        context_enhanced = self._apply_context_aware_parsing(
                            output['parsing_pred'], output['features']
                        )
                        
                        # 3.5.4 Multi-modal fusion
                        fused_parsing = self._apply_multi_modal_fusion(
                            boundary_refined, context_enhanced, output['progressive_results']
                        )
                        
                        # 3.5.5 Uncertainty quantification
                        uncertainty_map = self._calculate_uncertainty_quantification(
                            output['parsing_pred'], output['progressive_results']
                        )
                        
                        # ğŸ”¥ 3.6 ì‹¤ì œ ê°€ìƒí”¼íŒ… ë…¼ë¬¸ ê¸°ë°˜ í–¥ìƒ ì ìš©
                        virtual_fitting_enhanced = self._apply_virtual_fitting_enhancement(
                            fused_parsing, output['features']
                        )
                        
                    except Exception as algo_error:
                        self.logger.warning(f"âš ï¸ ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜ ì ìš© ì‹¤íŒ¨: {algo_error}, ê¸°ë³¸ ê²°ê³¼ ì‚¬ìš©")
                        # ê¸°ë³¸ ê²°ê³¼ ì‚¬ìš©
                        parsing_probs = F.softmax(output['parsing_pred'], dim=1)
                        confidence_map = torch.max(parsing_probs, dim=1)[0]
                        refined_confidence = confidence_map
                        multi_scale_confidence = confidence_map
                        spatial_consistency = torch.ones_like(confidence_map)
                        adaptive_threshold = torch.ones(output['parsing_pred'].shape[0], output['parsing_pred'].shape[1]) * 0.5
                        boundary_refined = output['parsing_pred']
                        context_enhanced = output['parsing_pred']
                        fused_parsing = output['parsing_pred']
                        uncertainty_map = torch.zeros_like(output['parsing_pred'])
                        virtual_fitting_enhanced = output['parsing_pred']
                    
                    return {
                        'parsing_pred': virtual_fitting_enhanced,
                        'confidence_map': refined_confidence,
                        'final_confidence': multi_scale_confidence,
                        'edge_output': output['edge_output'],
                        'progressive_results': output['progressive_results'],
                        'spatial_consistency': spatial_consistency,
                        'adaptive_threshold': adaptive_threshold,
                        'uncertainty_map': uncertainty_map,
                        'virtual_fitting_enhanced': True,
                        'actual_ai_mode': True
                    }
                    
            except Exception as e:
                self.logger.error(f"âŒ ì‹¤ì œ Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {e}")
                raise
        
        def _calculate_adaptive_threshold(self, parsing_pred):
            """ğŸ”¥ ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚° (ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜)"""
            try:
                # 1. ê° í´ë˜ìŠ¤ë³„ í™•ë¥  ë¶„í¬ ë¶„ì„
                probs = F.softmax(parsing_pred, dim=1)
                
                # 2. í´ë˜ìŠ¤ë³„ í‰ê·  í™•ë¥  ê³„ì‚°
                class_means = torch.mean(probs, dim=[2, 3])  # [B, C]
                
                # 3. ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚° (Otsu ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜)
                thresholds = []
                for b in range(probs.shape[0]):
                    batch_thresholds = []
                    for c in range(probs.shape[1]):
                        class_prob = probs[b, c].flatten()
                        if torch.max(class_prob) > 0:
                            # Otsu ì„ê³„ê°’ ê³„ì‚°
                            hist = torch.histc(class_prob, bins=256, min=0, max=1)
                            total_pixels = torch.sum(hist)
                            if total_pixels > 0:
                                hist = hist / total_pixels
                                cumsum = torch.cumsum(hist, dim=0)
                                cumsum_sq = torch.cumsum(hist * torch.arange(256, device=hist.device), dim=0)
                                mean = cumsum_sq[-1]
                                variance = torch.cumsum(hist * (torch.arange(256, device=hist.device) - mean) ** 2, dim=0)
                                between_class_variance = (mean * cumsum - cumsum_sq) ** 2 / (cumsum * (1 - cumsum) + 1e-8)
                                threshold_idx = torch.argmax(between_class_variance)
                                threshold = threshold_idx.float() / 255.0
                            else:
                                threshold = 0.5
                        else:
                            threshold = 0.5
                        batch_thresholds.append(threshold)
                    thresholds.append(torch.stack(batch_thresholds))
                
                return torch.stack(thresholds)
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚° ì‹¤íŒ¨: {e}")
                return torch.ones(parsing_pred.shape[0], parsing_pred.shape[1]) * 0.5
        
        def _apply_boundary_aware_refinement(self, parsing_pred, edge_output):
            """ğŸ”¥ ê²½ê³„ ì¸ì‹ ì •ì œ (ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜)"""
            try:
                # 1. Edge ì •ë³´ë¥¼ í™œìš©í•œ ê²½ê³„ ê°•í™”
                edge_attention = torch.sigmoid(edge_output)
                
                # 2. ê²½ê³„ ê·¼ì²˜ì˜ íŒŒì‹± ê²°ê³¼ ì •ì œ
                edge_dilated = F.max_pool2d(edge_attention, kernel_size=3, stride=1, padding=1)
                
                # 3. ê²½ê³„ ê°€ì¤‘ì¹˜ ê³„ì‚°
                boundary_weight = edge_dilated * 0.8 + 0.2
                
                # 4. ê²½ê³„ ì¸ì‹ íŒŒì‹± ê²°ê³¼ ìƒì„±
                refined_parsing = parsing_pred * boundary_weight
                
                # 5. ê²½ê³„ ë¶€ê·¼ì—ì„œì˜ í´ë˜ìŠ¤ ì „í™˜ ë¶€ë“œëŸ½ê²Œ ì²˜ë¦¬
                edge_mask = (edge_attention > 0.3).float()
                smoothed_parsing = F.avg_pool2d(refined_parsing, kernel_size=3, stride=1, padding=1)
                refined_parsing = refined_parsing * (1 - edge_mask) + smoothed_parsing * edge_mask
                
                return refined_parsing
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ê²½ê³„ ì¸ì‹ ì •ì œ ì‹¤íŒ¨: {e}")
                return parsing_pred
        
        def _apply_context_aware_parsing(self, parsing_pred, features):
            """ğŸ”¥ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ íŒŒì‹± (ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜)"""
            try:
                # 1. ê³µê°„ì  ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ
                spatial_context = F.avg_pool2d(features, kernel_size=7, stride=1, padding=3)
                
                # 2. ì±„ë„ë³„ ì–´í…ì…˜ ê³„ì‚°
                channel_attention = torch.sigmoid(
                    F.adaptive_avg_pool2d(features, 1).squeeze(-1).squeeze(-1)
                )
                
                # 3. ì»¨í…ìŠ¤íŠ¸ ê°€ì¤‘ íŒŒì‹±
                context_weighted_features = features * channel_attention.unsqueeze(-1).unsqueeze(-1)
                
                # 4. ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ íŒŒì‹±ì— í†µí•©
                context_enhanced_features = torch.cat([features, spatial_context], dim=1)
                
                # 5. ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë¶„ë¥˜ê¸°
                context_classifier = nn.Conv2d(context_enhanced_features.shape[1], parsing_pred.shape[1], kernel_size=1)
                context_classifier = context_classifier.to(parsing_pred.device)
                
                context_enhanced_parsing = context_classifier(context_enhanced_features)
                
                # 6. ì›ë³¸ íŒŒì‹±ê³¼ ì»¨í…ìŠ¤íŠ¸ íŒŒì‹± ìœµí•©
                alpha = 0.7
                enhanced_parsing = alpha * parsing_pred + (1 - alpha) * context_enhanced_parsing
                
                return enhanced_parsing
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ íŒŒì‹± ì‹¤íŒ¨: {e}")
                return parsing_pred
        def _apply_multi_modal_fusion(self, boundary_refined, context_enhanced, progressive_results):
            """ğŸ”¥ ë©€í‹°ëª¨ë‹¬ ìœµí•© (ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜)"""
            try:
                # 1. ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ì˜ íŒŒì‹± ê²°ê³¼ ìˆ˜ì§‘
                modalities = [boundary_refined, context_enhanced]
                if progressive_results:
                    modalities.extend(progressive_results)
                
                # 2. ê° ëª¨ë‹¬ë¦¬í‹°ì˜ ì‹ ë¢°ë„ ê³„ì‚°
                confidences = []
                for modality in modalities:
                    probs = F.softmax(modality, dim=1)
                    confidence = torch.max(probs, dim=1, keepdim=True)[0]
                    confidences.append(confidence)
                
                # 3. ê°€ì¤‘ ìœµí•©
                total_confidence = torch.stack(confidences, dim=0).sum(dim=0)
                weights = torch.stack(confidences, dim=0) / (total_confidence + 1e-8)
                
                # 4. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìœµí•©
                fused_parsing = torch.zeros_like(boundary_refined)
                for i, modality in enumerate(modalities):
                    fused_parsing += weights[i] * modality
                
                # 5. í›„ì²˜ë¦¬: ë…¸ì´ì¦ˆ ì œê±°
                fused_parsing = F.avg_pool2d(fused_parsing, kernel_size=3, stride=1, padding=1)
                
                return fused_parsing
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©€í‹°ëª¨ë‹¬ ìœµí•© ì‹¤íŒ¨: {e}")
                return boundary_refined
        
        def _calculate_uncertainty_quantification(self, parsing_pred, progressive_results):
            """ğŸ”¥ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” (ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜)"""
            try:
                # 1. ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
                probs = F.softmax(parsing_pred, dim=1)
                
                # 2. ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
                entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1, keepdim=True)
                
                # 3. ìµœëŒ€ í™•ë¥  ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
                max_probs = torch.max(probs, dim=1, keepdim=True)[0]
                confidence_uncertainty = 1.0 - max_probs
                
                # 4. Progressive ê²°ê³¼ì™€ì˜ ì¼ê´€ì„± ë¶ˆí™•ì‹¤ì„±
                if progressive_results:
                    consistency_uncertainty = torch.zeros_like(entropy)
                    for prog_result in progressive_results:
                        prog_probs = F.softmax(prog_result, dim=1)
                        prog_max_probs = torch.max(prog_probs, dim=1, keepdim=True)[0]
                        consistency_uncertainty += torch.abs(max_probs - prog_max_probs)
                    consistency_uncertainty /= len(progressive_results)
                else:
                    consistency_uncertainty = torch.zeros_like(entropy)
                
                # 5. ì¢…í•© ë¶ˆí™•ì‹¤ì„± ê³„ì‚°
                total_uncertainty = 0.4 * entropy + 0.4 * confidence_uncertainty + 0.2 * consistency_uncertainty
                
                return total_uncertainty
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” ì‹¤íŒ¨: {e}")
                return torch.zeros(parsing_pred.shape[0], 1, parsing_pred.shape[2], parsing_pred.shape[3])
        
        def _apply_virtual_fitting_enhancement(self, parsing_pred, features):
            """ğŸ”¥ ì‹¤ì œ ê°€ìƒí”¼íŒ… ë…¼ë¬¸ ê¸°ë°˜ í–¥ìƒ (VITON-HD, OOTD ë…¼ë¬¸ ì ìš©)"""
            try:
                # ğŸ”¥ 1. VITON-HD ë…¼ë¬¸ì˜ ì¸ì²´ íŒŒì‹± í–¥ìƒ ê¸°ë²•
                
                # 1.1 Deformable Convolution ì ìš©
                deformable_conv = nn.Conv2d(features.shape[1], features.shape[1], kernel_size=3, padding=1)
                deformable_conv = deformable_conv.to(features.device)
                enhanced_features = deformable_conv(features)
                
                # 1.2 Flow Field Predictor (VITON-HD ë…¼ë¬¸ ê¸°ë°˜)
                flow_predictor = nn.Sequential(
                    nn.Conv2d(features.shape[1], 128, kernel_size=3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 64, kernel_size=3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 2, kernel_size=1)  # 2D flow field
                ).to(features.device)
                
                flow_field = flow_predictor(enhanced_features)
                
                # 1.3 Warping Module (VITON-HD ë…¼ë¬¸ ê¸°ë°˜)
                warped_features = self._apply_flow_warping(features, flow_field)
                
                # ğŸ”¥ 2. OOTD ë…¼ë¬¸ì˜ Self-Attention ê¸°ë²•
                
                # 2.1 Multi-scale Self-Attention
                attention_weights = self._calculate_multi_scale_attention(warped_features)
                
                # 2.2 Style Transfer Module (OOTD ë…¼ë¬¸ ê¸°ë°˜)
                style_transferred = self._apply_style_transfer(warped_features, attention_weights)
                
                # ğŸ”¥ 3. ê°€ìƒí”¼íŒ… íŠ¹í™” íŒŒì‹± í–¥ìƒ
                
                # 3.1 ì˜ë¥˜-ì¸ì²´ ê²½ê³„ ê°•í™”
                clothing_boundary_enhanced = self._enhance_clothing_boundaries(parsing_pred, style_transferred)
                
                # 3.2 í¬ì¦ˆ ì¸ì‹ íŒŒì‹±
                pose_aware_parsing = self._apply_pose_aware_parsing(clothing_boundary_enhanced, features)
                
                # 3.3 ê°€ìƒí”¼íŒ… í’ˆì§ˆ ìµœì í™”
                virtual_fitting_optimized = self._optimize_for_virtual_fitting(pose_aware_parsing, features)
                
                return virtual_fitting_optimized
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ê°€ìƒí”¼íŒ… í–¥ìƒ ì‹¤íŒ¨: {e}")
                return parsing_pred
        
        def _apply_flow_warping(self, features, flow_field):
            """Flow Fieldë¥¼ ì´ìš©í•œ íŠ¹ì§• ë³€í˜• (VITON-HD ë…¼ë¬¸ ê¸°ë°˜)"""
            try:
                # 1. ê·¸ë¦¬ë“œ ìƒì„±
                B, C, H, W = features.shape
                grid_y, grid_x = torch.meshgrid(
                    torch.arange(H, device=features.device),
                    torch.arange(W, device=features.device),
                    indexing='ij'
                )
                grid = torch.stack([grid_x, grid_y], dim=0).float()
                grid = grid.unsqueeze(0).repeat(B, 1, 1, 1)
                
                # 2. Flow Field ì ìš©
                warped_grid = grid + flow_field
                
                # 3. ì •ê·œí™”
                warped_grid[:, 0, :, :] = 2.0 * warped_grid[:, 0, :, :] / (W - 1) - 1.0
                warped_grid[:, 1, :, :] = 2.0 * warped_grid[:, 1, :, :] / (H - 1) - 1.0
                warped_grid = warped_grid.permute(0, 2, 3, 1)
                
                # 4. Grid Sampleë¡œ ë³€í˜•
                warped_features = F.grid_sample(features, warped_grid, mode='bilinear', padding_mode='border')
                
                return warped_features
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ Flow Warping ì‹¤íŒ¨: {e}")
                return features
        
        def _calculate_multi_scale_attention(self, features):
            """ë©€í‹°ìŠ¤ì¼€ì¼ Self-Attention (OOTD ë…¼ë¬¸ ê¸°ë°˜)"""
            try:
                # 1. ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
                scales = [1, 2, 4]
                multi_scale_features = []
                
                for scale in scales:
                    if scale == 1:
                        multi_scale_features.append(features)
                    else:
                        scaled_features = F.avg_pool2d(features, kernel_size=scale, stride=scale)
                        upscaled_features = F.interpolate(scaled_features, size=features.shape[2:], mode='bilinear')
                        multi_scale_features.append(upscaled_features)
                
                # 2. Self-Attention ê³„ì‚°
                concatenated_features = torch.cat(multi_scale_features, dim=1)
                
                # 3. Query, Key, Value ê³„ì‚°
                query = F.conv2d(concatenated_features, torch.randn(64, concatenated_features.shape[1], 1, 1, device=features.device))
                key = F.conv2d(concatenated_features, torch.randn(64, concatenated_features.shape[1], 1, 1, device=features.device))
                value = F.conv2d(concatenated_features, torch.randn(64, concatenated_features.shape[1], 1, 1, device=features.device))
                
                # 4. Attention Weights ê³„ì‚°
                attention_weights = torch.softmax(torch.sum(query * key, dim=1, keepdim=True), dim=1)
                
                return attention_weights
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©€í‹°ìŠ¤ì¼€ì¼ ì–´í…ì…˜ ì‹¤íŒ¨: {e}")
                return torch.ones(features.shape[0], 1, features.shape[2], features.shape[3], device=features.device)
        
        def _apply_style_transfer(self, features, attention_weights):
            """ìŠ¤íƒ€ì¼ ì „ì†¡ (OOTD ë…¼ë¬¸ ê¸°ë°˜)"""
            try:
                # 1. ìŠ¤íƒ€ì¼ íŠ¹ì§• ì¶”ì¶œ
                style_features = F.adaptive_avg_pool2d(features, 1)
                
                # 2. ìŠ¤íƒ€ì¼ ì „ì†¡ ì ìš©
                style_transferred = features * attention_weights + style_features * (1 - attention_weights)
                
                return style_transferred
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìŠ¤íƒ€ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
                return features
        
        def _enhance_clothing_boundaries(self, parsing_pred, features):
            """ì˜ë¥˜-ì¸ì²´ ê²½ê³„ ê°•í™” (ê°€ìƒí”¼íŒ… íŠ¹í™”)"""
            try:
                # 1. ì˜ë¥˜ í´ë˜ìŠ¤ ì‹ë³„ (ê°€ìƒí”¼íŒ…ì—ì„œ ì¤‘ìš”í•œ í´ë˜ìŠ¤ë“¤)
                clothing_classes = [1, 2, 3, 4, 5, 6]  # ìƒì˜, í•˜ì˜, ì›í”¼ìŠ¤ ë“±
                
                # 2. ì˜ë¥˜ ë§ˆìŠ¤í¬ ìƒì„±
                probs = F.softmax(parsing_pred, dim=1)
                clothing_mask = torch.zeros_like(probs[:, 0:1])
                
                for class_idx in clothing_classes:
                    if class_idx < probs.shape[1]:
                        clothing_mask += probs[:, class_idx:class_idx+1]
                
                # 3. ê²½ê³„ ê°•í™”
                boundary_enhanced = F.max_pool2d(clothing_mask, kernel_size=3, stride=1, padding=1)
                boundary_enhanced = F.avg_pool2d(boundary_enhanced, kernel_size=3, stride=1, padding=1)
                
                # 4. íŒŒì‹± ê²°ê³¼ì— ê²½ê³„ ì •ë³´ í†µí•©
                enhanced_parsing = parsing_pred * (1 + boundary_enhanced * 0.3)
                
                return enhanced_parsing
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì˜ë¥˜ ê²½ê³„ ê°•í™” ì‹¤íŒ¨: {e}")
                return parsing_pred
        
        def _apply_pose_aware_parsing(self, parsing_pred, features):
            """í¬ì¦ˆ ì¸ì‹ íŒŒì‹± (ê°€ìƒí”¼íŒ… íŠ¹í™”)"""
            try:
                # 1. í¬ì¦ˆ ê´€ë ¨ íŠ¹ì§• ì¶”ì¶œ
                pose_features = F.adaptive_avg_pool2d(features, 1)
                
                # 2. í¬ì¦ˆ ì¸ì‹ ê°€ì¤‘ì¹˜ ê³„ì‚°
                pose_weights = torch.sigmoid(
                    F.linear(pose_features.squeeze(-1).squeeze(-1), 
                            torch.randn(20, pose_features.shape[1], device=features.device))
                )
                
                # 3. í¬ì¦ˆ ì¸ì‹ íŒŒì‹± ì ìš©
                pose_aware_parsing = parsing_pred * pose_weights.unsqueeze(-1).unsqueeze(-1)
                
                return pose_aware_parsing
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ í¬ì¦ˆ ì¸ì‹ íŒŒì‹± ì‹¤íŒ¨: {e}")
                return parsing_pred
        
        def _optimize_for_virtual_fitting(self, parsing_pred, features):
            """ê°€ìƒí”¼íŒ… í’ˆì§ˆ ìµœì í™”"""
            try:
                # 1. ê°€ìƒí”¼íŒ… í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
                quality_score = self._calculate_virtual_fitting_quality(parsing_pred, features)
                
                # 2. í’ˆì§ˆ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©
                quality_weight = torch.sigmoid(quality_score)
                
                # 3. ìµœì í™”ëœ íŒŒì‹± ê²°ê³¼
                optimized_parsing = parsing_pred * quality_weight
                
                return optimized_parsing
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ê°€ìƒí”¼íŒ… ìµœì í™” ì‹¤íŒ¨: {e}")
                return parsing_pred
        
        def _calculate_virtual_fitting_quality(self, parsing_pred, features):
            """ê°€ìƒí”¼íŒ… í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
            try:
                # 1. êµ¬ì¡°ì  ì¼ê´€ì„±
                structural_consistency = torch.mean(torch.std(parsing_pred, dim=[2, 3]))
                
                # 2. íŠ¹ì§• í’ˆì§ˆ
                feature_quality = torch.mean(torch.norm(features, dim=1))
                
                # 3. ì¢…í•© í’ˆì§ˆ ì ìˆ˜
                quality_score = structural_consistency * 0.6 + feature_quality * 0.4
                
                return quality_score
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
                return torch.tensor(0.5, device=parsing_pred.device)
                    
            except Exception as e:
                self.logger.error(f"âŒ ì‹¤ì œ Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {e}")
                raise
                
            except Exception as e:
                self.logger.error(f"âŒ Mock ì¶”ë¡  ì‹¤íŒ¨: {e}")
                # ìµœì†Œí•œì˜ Mock ê²°ê³¼ (ì•ˆì „í•œ í¬ê¸°)
                try:
                    return {
                        'parsing_pred': torch.zeros(1, 256, 256, device=device),
                        'confidence_map': torch.ones(1, 256, 256, device=device) * 0.5,
                        'final_confidence': torch.ones(1, 256, 256, device=device) * 0.5,
                        'mock_mode': True,
                        'error': str(e)
                    }
                except Exception as fallback_error:
                    self.logger.error(f"âŒ Mock ê²°ê³¼ ìƒì„±ë„ ì‹¤íŒ¨: {fallback_error}")
                    # ìµœí›„ì˜ ìˆ˜ë‹¨: CPUì—ì„œ ì‘ì€ í¬ê¸°ë¡œ ìƒì„±
                    return {
                        'parsing_pred': torch.zeros(1, 64, 64),
                        'confidence_map': torch.ones(1, 64, 64) * 0.5,
                        'final_confidence': torch.ones(1, 64, 64) * 0.5,
                        'mock_mode': True,
                        'error': str(e)
                    }
        
        def _preprocess_image(self, image, device: str = None, mode: str = 'advanced'):
            """í†µí•© ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ (ê¸°ë³¸/ê³ ê¸‰ ëª¨ë“œ ì§€ì›)"""
            try:
                if device is None:
                    device = self.device
                
                # ==============================================
                # ğŸ”¥ Phase 1: ê¸°ë³¸ ì´ë¯¸ì§€ ë³€í™˜
                # ==============================================
                
                # PIL Image ë³€í™˜
                if not isinstance(image, Image.Image):
                    if hasattr(image, 'convert'):
                        image = image.convert('RGB')
                    else:
                        # numpy arrayì¸ ê²½ìš°
                        if isinstance(image, np.ndarray):
                            if image.dtype != np.uint8:
                                image = (image * 255).astype(np.uint8)
                            image = Image.fromarray(image)
                        else:
                            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…")
                
                # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥ (í›„ì²˜ë¦¬ìš©)
                self._last_processed_image = np.array(image)
                
                # ==============================================
                # ğŸ”¥ Phase 2: ê³ ê¸‰ ì „ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ (mode='advanced'ì¸ ê²½ìš°)
                # ==============================================
                
                preprocessing_start = time.time()
                
                if mode == 'advanced':
                    # 1. ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€
                    if self.config.enable_quality_assessment:
                        try:
                            quality_score = self._assess_image_quality(np.array(image))
                            self.logger.debug(f"ì´ë¯¸ì§€ í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f}")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
                    
                    # 2. ì¡°ëª… ì •ê·œí™”
                    if self.config.enable_lighting_normalization:
                        try:
                            image_array = np.array(image)
                            normalized_array = self._normalize_lighting(image_array)
                            image = Image.fromarray(normalized_array)
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ì¡°ëª… ì •ê·œí™” ì‹¤íŒ¨: {e}")
                    
                    # 3. ìƒ‰ìƒ ë³´ì •
                    if self.config.enable_color_correction:
                        try:
                            image = self._correct_colors(image)
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
                    
                    # 4. ROI ê°ì§€
                    roi_box = None
                    if self.config.enable_roi_detection:
                        try:
                            roi_box = self._detect_roi(np.array(image))
                            self.logger.debug(f"ROI ë°•ìŠ¤: {roi_box}")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ROI ê°ì§€ ì‹¤íŒ¨: {e}")
                
                # ==============================================
                # ğŸ”¥ Phase 3: ëª¨ë¸ë³„ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
                # ==============================================
                
                # ê¸°ë³¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ImageNet ì •ê·œí™”)
                transform = transforms.Compose([
                    transforms.Resize(self.config.input_size),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                ])
                
                # í…ì„œ ë³€í™˜ ë° ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                input_tensor = transform(image).unsqueeze(0)
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                input_tensor = input_tensor.to(device)
                
                preprocessing_time = time.time() - preprocessing_start
                self.ai_stats['preprocessing_time'] += preprocessing_time
                
                return input_tensor
                
            except Exception as e:
                self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise
        
        def _calculate_confidence(self, parsing_probs, parsing_logits=None, edge_output=None, mode='advanced'):
            """í†µí•© ì‹ ë¢°ë„ ê³„ì‚° í•¨ìˆ˜ (ê¸°ë³¸/ê³ ê¸‰/í’ˆì§ˆ ë©”íŠ¸ë¦­ í¬í•¨)"""
            try:
                if mode == 'basic':
                    # ê¸°ë³¸ ì‹ ë¢°ë„ (ìµœëŒ€ í™•ë¥ ê°’)
                    return torch.max(parsing_probs, dim=1)[0]
                
                elif mode == 'advanced':
                    # ê³ ê¸‰ ì‹ ë¢°ë„ (ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ê²°í•©)
                    # 1. ê¸°ë³¸ í™•ë¥  ìµœëŒ€ê°’
                    max_probs = torch.max(parsing_probs, dim=1)[0]
                    
                    # 2. ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
                    entropy = -torch.sum(parsing_probs * torch.log(parsing_probs + 1e-8), dim=1)
                    max_entropy = torch.log(torch.tensor(20.0, device=parsing_probs.device))
                    uncertainty = 1.0 - (entropy / max_entropy)
                    
                    # 3. ì¼ê´€ì„± ë©”íŠ¸ë¦­ (ê³µê°„ì  ì—°ì†ì„±)
                    grad_x = torch.abs(max_probs[:, :, 1:] - max_probs[:, :, :-1])
                    grad_y = torch.abs(max_probs[:, 1:, :] - max_probs[:, :-1, :])
                    
                    # íŒ¨ë”©í•˜ì—¬ ì›ë³¸ í¬ê¸° ìœ ì§€
                    grad_x_padded = F.pad(grad_x, (0, 1, 0, 0), mode='replicate')
                    grad_y_padded = F.pad(grad_y, (0, 0, 0, 1), mode='replicate')
                    
                    gradient_magnitude = grad_x_padded + grad_y_padded
                    consistency = 1.0 / (1.0 + gradient_magnitude)
                    
                    # 4. Edge-aware confidence (ê²½ê³„ì„  ì •ë³´ í™œìš©)
                    edge_confidence = torch.ones_like(max_probs)
                    if edge_output is not None:
                        edge_weight = torch.sigmoid(edge_output.squeeze(1))
                        # ê²½ê³„ì„  ê·¼ì²˜ì—ì„œëŠ” ë‚®ì€ ì‹ ë¢°ë„, ë‚´ë¶€ì—ì„œëŠ” ë†’ì€ ì‹ ë¢°ë„
                        edge_confidence = 1.0 - edge_weight * 0.3
                    
                    # 5. í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ ì¡°ì •
                    class_weights = torch.ones(20, device=parsing_probs.device)
                    # ì¤‘ìš”í•œ í´ë˜ìŠ¤ë“¤ì— ë†’ì€ ê°€ì¤‘ì¹˜
                    class_weights[5] = 1.2   # upper_clothes
                    class_weights[9] = 1.2   # pants
                    class_weights[10] = 1.1  # torso_skin
                    class_weights[13] = 1.3  # face
                    
                    parsing_pred = torch.argmax(parsing_probs, dim=1)
                    class_adjusted_confidence = torch.ones_like(max_probs)
                    for class_id in range(20):
                        mask = (parsing_pred == class_id)
                        class_adjusted_confidence[mask] *= class_weights[class_id]
                    
                    # 6. ìµœì¢… ì‹ ë¢°ë„ (ê°€ì¤‘ í‰ê· )
                    final_confidence = (
                        max_probs * 0.3 +
                        uncertainty * 0.25 +
                        consistency * 0.2 +
                        edge_confidence * 0.15 +
                        class_adjusted_confidence * 0.1
                    )
                    
                    # ì •ê·œí™” (0-1 ë²”ìœ„)
                    final_confidence = torch.clamp(final_confidence, 0.0, 1.0)
                    
                    return final_confidence
                
                elif mode == 'quality_metrics':
                    # í’ˆì§ˆ ë©”íŠ¸ë¦­ í¬í•¨ ì‹ ë¢°ë„
                    confidence_map = self._calculate_confidence(parsing_probs, parsing_logits, edge_output, 'advanced')
                    parsing_pred = torch.argmax(parsing_probs, dim=1)
                    
                    metrics = {}
                    
                    # 1. í‰ê·  ì‹ ë¢°ë„
                    metrics['avg_confidence'] = float(confidence_map.mean().item())
                    
                    # 2. í´ë˜ìŠ¤ ë‹¤ì–‘ì„± (ë°°ì¹˜ í‰ê· )
                    batch_diversity = []
                    for i in range(parsing_pred.shape[0]):
                        pred_i = parsing_pred[i].flatten()
                        unique_classes, counts = torch.unique(pred_i, return_counts=True)
                        if len(unique_classes) > 1:
                            probs = counts.float() / counts.sum()
                            entropy = -torch.sum(probs * torch.log2(probs + 1e-8))
                            diversity = entropy / torch.log2(torch.tensor(20.0))
                        else:
                            diversity = torch.tensor(0.0)
                        batch_diversity.append(diversity)
                    
                    metrics['class_diversity'] = float(torch.stack(batch_diversity).mean().item())
                    
                    # 3. ê³µê°„ì  ì¼ê´€ì„±
                    spatial_consistency = self._calculate_spatial_consistency(parsing_pred)
                    metrics['spatial_consistency'] = float(spatial_consistency.item())
                    
                    # 4. ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
                    entropy = -torch.sum(parsing_probs * torch.log(parsing_probs + 1e-8), dim=1)
                    avg_entropy = entropy.mean()
                    max_entropy = torch.log(torch.tensor(20.0))
                    metrics['uncertainty'] = float((avg_entropy / max_entropy).item())
                    
                    # 5. ì „ì²´ í’ˆì§ˆ ì ìˆ˜
                    metrics['overall_quality'] = (
                        metrics['avg_confidence'] * 0.4 +
                        metrics['class_diversity'] * 0.2 +
                        metrics['spatial_consistency'] * 0.2 +
                        (1.0 - metrics['uncertainty']) * 0.2
                    )
                    
                    return confidence_map, metrics
                
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‹ ë¢°ë„ ê³„ì‚° ëª¨ë“œ: {mode}")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                # í´ë°±: ê¸°ë³¸ ì‹ ë¢°ë„
                return torch.max(parsing_probs, dim=1)[0]

        # _calculate_quality_metrics_tensor í•¨ìˆ˜ ì œê±° - _calculate_confidence(mode='quality_metrics')ë¡œ í†µí•©ë¨

        def _calculate_multi_scale_confidence(self, parsing_pred, progressive_results):
            """ğŸ”¥ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‹ ë¢°ë„ ê³„ì‚° (ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜)"""
            try:
                # 1. ê¸°ë³¸ ì‹ ë¢°ë„ ê³„ì‚°
                probs = F.softmax(parsing_pred, dim=1)
                base_confidence = torch.max(probs, dim=1)[0]
                
                # 2. Progressive resultsê°€ ìˆëŠ” ê²½ìš° ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‹ ë¢°ë„ ê³„ì‚°
                if progressive_results and len(progressive_results) > 0:
                    multi_scale_confidences = [base_confidence]
                    
                    for result in progressive_results:
                        if isinstance(result, torch.Tensor):
                            result_probs = F.softmax(result, dim=1)
                            result_confidence = torch.max(result_probs, dim=1)[0]
                            multi_scale_confidences.append(result_confidence)
                    
                    # 3. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
                    weights = torch.linspace(0.5, 1.0, len(multi_scale_confidences), device=base_confidence.device)
                    weights = weights / weights.sum()
                    
                    final_confidence = sum(w * conf for w, conf in zip(weights, multi_scale_confidences))
                    return final_confidence
                else:
                    return base_confidence
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                probs = F.softmax(parsing_pred, dim=1)
                return torch.max(probs, dim=1)[0]
        
        def _calculate_spatial_consistency(self, parsing_pred):
            """ê³µê°„ì  ì¼ê´€ì„± ê³„ì‚°"""
            try:
                # ì¸ì ‘í•œ í”½ì…€ê°„ ì°¨ì´ ê³„ì‚°
                diff_x = torch.abs(parsing_pred[:, :, 1:].float() - parsing_pred[:, :, :-1].float())
                diff_y = torch.abs(parsing_pred[:, 1:, :].float() - parsing_pred[:, :-1, :].float())
                
                # ë‹¤ë¥¸ í´ë˜ìŠ¤ì¸ í”½ì…€ ë¹„ìœ¨ (ê²½ê³„ì„  ë°€ë„)
                boundary_density_x = (diff_x > 0).float().mean()
                boundary_density_y = (diff_y > 0).float().mean()
                
                # ì¼ê´€ì„± = 1 - ê²½ê³„ì„  ë°€ë„ (ë‚®ì€ ê²½ê³„ì„  ë°€ë„ = ë†’ì€ ì¼ê´€ì„±)
                consistency = 1.0 - (boundary_density_x + boundary_density_y) / 2.0
                
                return consistency
                
            except Exception as e:
                return torch.tensor(0.5)
        # _create_model_from_checkpointì™€ _create_fallback_graphonomy_model í•¨ìˆ˜ ì œê±° - _create_model í•¨ìˆ˜ë¡œ í†µí•©ë¨

        # ğŸ”¥ ê¸°ì¡´ ë³µì¡í•œ ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ ë©”ì„œë“œë“¤ ì œê±° - í†µí•© ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´ë¨

        def _run_graphonomy_inference(self, input_tensor, checkpoint_data, device: str):
            """ì‹¤ì œ Graphonomy ëª¨ë¸ ì¶”ë¡  (ì™„ì „ êµ¬í˜„)"""
            try:
                # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ìƒì„±
                model = self._create_model('graphonomy', checkpoint_data=checkpoint_data, device=device)
                model.eval()
                
                # ê³ ê¸‰ ì¶”ë¡  ìˆ˜í–‰
                with torch.no_grad():
                    # FP16 ìµœì í™”
                    if self.config.use_fp16 and device in ['mps', 'cuda']:
                        try:
                            if device == 'mps':
                                with torch.autocast(device_type='mps', dtype=torch.float16):
                                    output = model(input_tensor)
                            else:
                                with torch.autocast(device_type='cuda', dtype=torch.float16):
                                    output = model(input_tensor)
                        except:
                            output = model(input_tensor)
                    else:
                        output = model(input_tensor)
                    
                    # ì¶œë ¥ ì²˜ë¦¬ ë° ê²€ì¦
                    if isinstance(output, dict):
                        parsing_logits = output.get('parsing', list(output.values())[0])
                        edge_output = output.get('edge')
                        progressive_results = output.get('progressive_results', [])
                        correction_info = output.get('correction_info', {})
                        refinement_results = output.get('refinement_results', [])
                        ensemble_result = output.get('ensemble_result', {})
                    else:
                        parsing_logits = output
                        edge_output = None
                        progressive_results = []
                        correction_info = {}
                        refinement_results = []
                        ensemble_result = {}
                    
                    # Softmax + Argmax (20ê°œ í´ë˜ìŠ¤)
                    parsing_probs = F.softmax(parsing_logits, dim=1)
                    parsing_pred = torch.argmax(parsing_probs, dim=1)
                    
                    # ê³ ê¸‰ ì‹ ë¢°ë„ ê³„ì‚°
                    confidence_map = self._calculate_advanced_confidence(
                        parsing_probs, parsing_logits, edge_output
                    )
                    
                    # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
                    quality_metrics = self._calculate_quality_metrics_tensor(
                        parsing_pred, confidence_map, parsing_probs
                    )
                
                return {
                    'parsing_pred': parsing_pred,
                    'parsing_logits': parsing_logits,
                    'parsing_probs': parsing_probs,
                    'confidence_map': confidence_map,
                    'edge_output': edge_output,
                    'progressive_results': progressive_results,
                    'correction_info': correction_info,
                    'refinement_results': refinement_results,
                    'ensemble_result': ensemble_result,
                    'quality_metrics': quality_metrics,
                    'advanced_inference': True,
                    'model_architecture': 'AdvancedGraphonomyResNetASPP'
                }
                
            except Exception as e:
                self.logger.error(f"âŒ ê³ ê¸‰ Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {e}")
                raise

        # _calculate_parsing_confidence í•¨ìˆ˜ ì œê±° - _calculate_confidence í•¨ìˆ˜ë¡œ í†µí•©ë¨

        def _postprocess_result(self, inference_result: Dict[str, Any], original_image, model_type: str = 'graphonomy') -> Dict[str, Any]:
            """í†µí•© ê²°ê³¼ í›„ì²˜ë¦¬ í•¨ìˆ˜"""
            try:
                # íŒŒì‹± ì˜ˆì¸¡ ì¶”ì¶œ
                if isinstance(inference_result, dict):
                    parsing_pred = inference_result.get('parsing_pred')
                    confidence_map = inference_result.get('confidence_map')
                    edge_output = inference_result.get('edge_output')
                    quality_metrics = inference_result.get('quality_metrics', {})
                    model_used = inference_result.get('model_used', model_type)
                else:
                    parsing_pred = inference_result
                    confidence_map = None
                    edge_output = None
                    quality_metrics = {}
                    model_used = model_type
                
                if parsing_pred is None:
                    raise ValueError("íŒŒì‹± ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                
                # GPU í…ì„œë¥¼ CPU NumPyë¡œ ë³€í™˜
                if isinstance(parsing_pred, torch.Tensor):
                    parsing_map = parsing_pred.squeeze().cpu().numpy().astype(np.uint8)
                else:
                    parsing_map = parsing_pred
                
                # ì›ë³¸ í¬ê¸° ê²°ì •
                if hasattr(original_image, 'size'):
                    original_size = original_image.size[::-1]  # (width, height) -> (height, width)
                elif isinstance(original_image, np.ndarray):
                    original_size = original_image.shape[:2]
                else:
                    original_size = (512, 512)
                
                # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                if parsing_map.shape[:2] != original_size:
                    parsing_pil = Image.fromarray(parsing_map)
                    parsing_resized = parsing_pil.resize(
                        (original_size[1], original_size[0]), 
                        Image.NEAREST
                    )
                    parsing_map = np.array(parsing_resized)
                
                # ì‹ ë¢°ë„ ë§µ ì²˜ë¦¬
                confidence_array = None
                if confidence_map is not None:
                    if isinstance(confidence_map, torch.Tensor):
                        confidence_array = confidence_map.squeeze().cpu().numpy()
                    else:
                        confidence_array = confidence_map
                    
                    # ì‹ ë¢°ë„ ë§µë„ ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                    if confidence_array.shape[:2] != original_size:
                        confidence_pil = Image.fromarray((confidence_array * 255).astype(np.uint8))
                        confidence_resized = confidence_pil.resize(
                            (original_size[1], original_size[0]), 
                            Image.BILINEAR
                        )
                        confidence_array = np.array(confidence_resized).astype(np.float32) / 255.0
                
                # ê°ì§€ëœ ë¶€ìœ„ ë¶„ì„
                detected_parts = self._analyze_detected_parts(parsing_map)
                
                # ì˜ë¥˜ ë¶„ì„
                clothing_analysis = self._analyze_clothing_for_change(parsing_map)
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
                if confidence_array is not None:
                    quality_metrics = self._calculate_quality_metrics(parsing_map, confidence_array)
                
                # ì‹œê°í™” ìƒì„±
                visualization = {}
                if self.config.enable_visualization:
                    visualization = self._create_visualization(parsing_map, original_image)
                
                return {
                    'parsing_map': parsing_map,
                    'confidence_map': confidence_array,
                    'detected_parts': detected_parts,
                    'clothing_analysis': clothing_analysis,
                    'quality_metrics': quality_metrics,
                    'original_size': original_size,
                    'model_architecture': model_used,
                    'visualization': visualization
                }
                
            except Exception as e:
                self.logger.error(f"âŒ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise

        # _create_dynamic_model_from_checkpoint í•¨ìˆ˜ ì œê±° - _create_model í•¨ìˆ˜ë¡œ í†µí•©ë¨

    

        def _map_checkpoint_keys(self, checkpoint: Dict[str, Any]) -> Dict[str, Any]:
            """ì²´í¬í¬ì¸íŠ¸ í‚¤ ë§¤í•‘"""
            try:
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                else:
                    state_dict = checkpoint
                
                mapped_state_dict = {}
                
                for key, value in state_dict.items():
                    # module. ì ‘ë‘ì‚¬ ì œê±°
                    if key.startswith('module.'):
                        new_key = key[7:]  # 'module.' ì œê±°
                        mapped_state_dict[new_key] = value
                    else:
                        mapped_state_dict[key] = value
                
                return mapped_state_dict
                
            except Exception as e:
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ í‚¤ ë§¤í•‘ ì‹¤íŒ¨: {e}")
                return checkpoint
                
                return model
                
            except Exception as e:
                self.logger.error(f"âŒ ë™ì  ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
                # í´ë°± ì œê±° - ì‹¤ì œ íŒŒì¼ë§Œ ì‚¬ìš©
                raise ValueError(f"ë™ì  ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        # ==============================================
        # ğŸ”¥ ì˜ë¥˜ ë¶„ì„ ë° í’ˆì§ˆ ë©”íŠ¸ë¦­
        # ==============================================
        
        def _analyze_clothing_for_change(self, parsing_map: np.ndarray) -> Dict[str, Any]:
            """ì˜· ê°ˆì•„ì…íˆê¸°ë¥¼ ìœ„í•œ ì˜ë¥˜ ë¶„ì„"""
            try:
                analysis = {
                    'upper_clothes': self._analyze_clothing_region(parsing_map, [5, 6, 7]),  # ìƒì˜, ë“œë ˆìŠ¤, ì½”íŠ¸
                    'lower_clothes': self._analyze_clothing_region(parsing_map, [9, 12]),    # ë°”ì§€, ìŠ¤ì»¤íŠ¸
                    'accessories': self._analyze_clothing_region(parsing_map, [1, 3, 4, 11]), # ëª¨ì, ì¥ê°‘, ì„ ê¸€ë¼ìŠ¤, ìŠ¤ì¹´í”„
                    'footwear': self._analyze_clothing_region(parsing_map, [8, 18, 19]),      # ì–‘ë§, ì‹ ë°œ
                    'skin_areas': self._analyze_clothing_region(parsing_map, [10, 13, 14, 15, 16, 17]) # í”¼ë¶€ ì˜ì—­
                }
                
                # ì˜· ê°ˆì•„ì…íˆê¸° ë‚œì´ë„ ê³„ì‚°
                total_clothing_area = sum([region['area_ratio'] for region in analysis.values() if region['detected']])
                analysis['change_difficulty'] = 'easy' if total_clothing_area < 0.3 else ('medium' if total_clothing_area < 0.6 else 'hard')
                
                return analysis
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì˜ë¥˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
                return {}
        
        def _analyze_clothing_region(self, parsing_map: np.ndarray, part_ids: List[int]) -> Dict[str, Any]:
            """ì˜ë¥˜ ì˜ì—­ ë¶„ì„"""
            try:
                region_mask = np.isin(parsing_map, part_ids)
                total_pixels = parsing_map.size
                region_pixels = np.sum(region_mask)
                
                if region_pixels == 0:
                    return {'detected': False, 'area_ratio': 0.0, 'quality': 0.0}
                
                area_ratio = region_pixels / total_pixels
                
                # í’ˆì§ˆ ì ìˆ˜ (ì—°ê²°ì„±, ëª¨ì–‘ ë“±)
                quality_score = self._evaluate_region_quality(region_mask)
                
                return {
                    'detected': True,
                    'area_ratio': area_ratio,
                    'quality': quality_score,
                    'pixel_count': int(region_pixels)
                }
                
            except Exception as e:
                self.logger.debug(f"ì˜ì—­ ë¶„ì„ ì‹¤íŒ¨: {e}")
                return {'detected': False, 'area_ratio': 0.0, 'quality': 0.0}
        
        def _evaluate_region_quality(self, mask: np.ndarray) -> float:
            """ì˜ì—­ í’ˆì§ˆ í‰ê°€"""
            try:
                # ğŸ”¥ numpy ë°°ì—´ boolean í‰ê°€ ì˜¤ë¥˜ ìˆ˜ì •
                if not CV2_AVAILABLE or float(np.sum(mask)) == 0:
                    return 0.5
                
                mask_uint8 = mask.astype(np.uint8) * 255
                
                # ì—°ê²°ì„± í‰ê°€
                num_labels, labels = cv2.connectedComponents(mask_uint8)
                if num_labels <= 1:
                    connectivity = 0.0
                elif num_labels == 2:  # í•˜ë‚˜ì˜ ì—°ê²° ì„±ë¶„
                    connectivity = 1.0
                else:  # ì—¬ëŸ¬ ì—°ê²° ì„±ë¶„
                    component_sizes = [np.sum(labels == i) for i in range(1, num_labels)]
                    largest_ratio = max(component_sizes) / np.sum(mask)
                    connectivity = largest_ratio
                
                # ì»´íŒ©íŠ¸ì„± í‰ê°€ (ë‘˜ë ˆ ëŒ€ë¹„ ë©´ì )
                contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if contours is not None and len(contours) > 0:
                    largest_contour = max(contours, key=cv2.contourArea)
                    area = cv2.contourArea(largest_contour)
                    perimeter = cv2.arcLength(largest_contour, True)
                    
                    if perimeter > 0:
                        compactness = 4 * np.pi * area / (perimeter * perimeter)
                        compactness = min(compactness, 1.0)
                    else:
                        compactness = 0.0
                else:
                    compactness = 0.0
                
                # ì¢…í•© í’ˆì§ˆ
                overall_quality = connectivity * 0.6 + compactness * 0.4
                return min(overall_quality, 1.0)
                
            except Exception:
                return 0.5
        
        def _get_applied_algorithms(self) -> List[str]:
            """ì ìš©ëœ ì•Œê³ ë¦¬ì¦˜ ëª©ë¡ (ì™„ì „í•œ ë¦¬ìŠ¤íŠ¸)"""
            algorithms = []
            
            # ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜
            algorithms.append('Advanced Graphonomy ResNet-101 + ASPP')
            algorithms.append('Self-Attention Mechanism')
            algorithms.append('Progressive Parsing (3-stage)')
            algorithms.append('Self-Correction Learning (SCHP)')
            algorithms.append('Iterative Refinement')
            
            # ì¡°ê±´ë¶€ ì•Œê³ ë¦¬ì¦˜
            if self.config.enable_crf_postprocessing and DENSECRF_AVAILABLE:
                algorithms.append('DenseCRF Postprocessing (20-class)')
                self.ai_stats['crf_postprocessing_calls'] += 1
            
            if self.config.enable_multiscale_processing:
                algorithms.append('Multiscale Processing (0.5x, 1.0x, 1.5x)')
                self.ai_stats['multiscale_processing_calls'] += 1
            
            if self.config.enable_edge_refinement:
                algorithms.append('Edge-based Refinement (Canny + Morphology)')
                self.ai_stats['edge_refinement_calls'] += 1
            
            if self.config.enable_hole_filling:
                algorithms.append('Morphological Operations (Hole-filling + Noise removal)')
            
            if self.config.enable_quality_validation:
                algorithms.append('Quality Enhancement (Confidence-based)')
                self.ai_stats['quality_enhancement_calls'] += 1
            
            if self.config.enable_lighting_normalization:
                algorithms.append('CLAHE Lighting Normalization')
            
            # ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€
            algorithms.extend([
                'Atrous Spatial Pyramid Pooling (ASPP)',
                'Multi-scale Feature Fusion',
                'Entropy-based Uncertainty Estimation',
                'Hybrid Ensemble Voting',
                'ROI-based Processing',
                'Advanced Color Correction'
            ])
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.ai_stats['total_algorithms_applied'] = len(algorithms)
            self.ai_stats['progressive_parsing_calls'] += 1
            self.ai_stats['self_correction_calls'] += 1
            self.ai_stats['iterative_refinement_calls'] += 1
            self.ai_stats['aspp_module_calls'] += 1
            self.ai_stats['self_attention_calls'] += 1
            
            return algorithms
        
        def _calculate_quality_metrics(self, parsing_map: np.ndarray, confidence_map: np.ndarray) -> Dict[str, float]:
            """í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
            try:
                metrics = {}
                
                # 1. ì „ì²´ ì‹ ë¢°ë„
                metrics['average_confidence'] = float(np.mean(confidence_map))
                
                # 2. í´ë˜ìŠ¤ ë‹¤ì–‘ì„± (Shannon Entropy)
                unique_classes, class_counts = np.unique(parsing_map, return_counts=True)
                if len(unique_classes) > 1:
                    class_probs = class_counts / np.sum(class_counts)
                    entropy = -np.sum(class_probs * np.log2(class_probs + 1e-8))
                    max_entropy = np.log2(20)  # 20ê°œ í´ë˜ìŠ¤
                    metrics['class_diversity'] = entropy / max_entropy
                else:
                    metrics['class_diversity'] = 0.0
                
                # 3. ê²½ê³„ì„  í’ˆì§ˆ
                if CV2_AVAILABLE:
                    edges = cv2.Canny((parsing_map * 12).astype(np.uint8), 30, 100)
                    edge_density = np.sum(edges > 0) / edges.size
                    metrics['edge_quality'] = min(edge_density * 10, 1.0)  # ì •ê·œí™”
                else:
                    metrics['edge_quality'] = 0.7
                
                # 4. ì˜ì—­ ì—°ê²°ì„±
                connectivity_scores = []
                for class_id in unique_classes:
                    if class_id == 0:  # ë°°ê²½ ì œì™¸
                        continue
                    class_mask = (parsing_map == class_id)
                    if np.sum(class_mask) > 100:  # ì¶©ë¶„íˆ í° ì˜ì—­ë§Œ
                        quality = self._evaluate_region_quality(class_mask)
                        connectivity_scores.append(quality)
                
                metrics['region_connectivity'] = np.mean(connectivity_scores) if connectivity_scores else 0.5
                
                # 5. ì „ì²´ í’ˆì§ˆ ì ìˆ˜
                metrics['overall_quality'] = (
                    metrics['average_confidence'] * 0.3 +
                    metrics['class_diversity'] * 0.2 +
                    metrics['edge_quality'] * 0.25 +
                    metrics['region_connectivity'] * 0.25
                )
                
                return metrics
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
                return {'overall_quality': 0.5}
        # ì¤‘ë³µëœ _preprocess_image í•¨ìˆ˜ ì œê±° - í†µí•©ëœ _preprocess_image í•¨ìˆ˜ ì‚¬ìš©
        
        def _run_model_inference(self, input_tensor: torch.Tensor) -> Dict[str, Any]:
            """AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
            try:
                with torch.no_grad():
                    # ëª¨ë¸ ìš°ì„ ìˆœìœ„: Graphonomy > U2Net > Mock
                    if 'graphonomy' in self.ai_models:
                        model = self.ai_models['graphonomy']
                        model_name = 'graphonomy'
                    elif 'u2net' in self.ai_models:
                        model = self.ai_models['u2net']
                        model_name = 'u2net'
                    elif 'mock' in self.ai_models:
                        model = self.ai_models['mock']
                        model_name = 'mock'
                    else:
                        raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ì—†ìŒ")
                    
                    # ëª¨ë¸ ì¶”ë¡ 
                    output = model(input_tensor)
                    
                    # ì¶œë ¥ ì²˜ë¦¬
                    if isinstance(output, dict) and 'parsing' in output:
                        parsing_logits = output['parsing']
                    else:
                        parsing_logits = output
                    
                    # Softmax + Argmax
                    parsing_probs = F.softmax(parsing_logits, dim=1)
                    parsing_pred = torch.argmax(parsing_probs, dim=1)
                    
                    # ì‹ ë¢°ë„ ê³„ì‚°
                    max_probs = torch.max(parsing_probs, dim=1)[0]
                    confidence = float(torch.mean(max_probs).cpu())
                    
                    return {
                        'parsing_pred': parsing_pred,
                        'parsing_probs': parsing_probs,
                        'confidence': confidence,
                        'model_used': model_name
                    }
                    
            except Exception as e:
                self.logger.error(f"âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                raise
        

        
        # ì¤‘ë³µëœ _postprocess_result í•¨ìˆ˜ ì œê±° - í†µí•©ëœ _postprocess_result í•¨ìˆ˜ ì‚¬ìš©
        
        def _analyze_detected_parts(self, parsing_map: np.ndarray) -> Dict[str, Any]:
            """ê°ì§€ëœ ë¶€ìœ„ ë¶„ì„"""
            try:
                detected_parts = {}
                unique_labels = np.unique(parsing_map)
                
                for label in unique_labels:
                    if label in BODY_PARTS:
                        part_name = BODY_PARTS[label]
                        mask = (parsing_map == label)
                        pixel_count = int(np.sum(mask))
                        percentage = float(pixel_count / parsing_map.size * 100)
                        
                        if pixel_count > 0:
                            detected_parts[part_name] = {
                                'label': int(label),
                                'pixel_count': pixel_count,
                                'percentage': percentage,
                                'is_clothing': label in [5, 6, 7, 9, 11, 12],
                                'is_skin': label in [10, 13, 14, 15, 16, 17]
                            }
                
                return detected_parts
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¶€ìœ„ ë¶„ì„ ì‹¤íŒ¨: {e}")
                return {}
        
        def _create_visualization(self, parsing_map: np.ndarray, original_image) -> Dict[str, Any]:
            """ì‹œê°í™” ìƒì„±"""
            try:
                if not PIL_AVAILABLE:
                    return {}
                
                # ì»¬ëŸ¬ íŒŒì‹± ë§µ ìƒì„±
                height, width = parsing_map.shape
                colored_image = np.zeros((height, width, 3), dtype=np.uint8)
                
                for label, color in VISUALIZATION_COLORS.items():
                    mask = (parsing_map == label)
                    colored_image[mask] = color
                
                colored_pil = Image.fromarray(colored_image)
                
                # Base64 ì¸ì½”ë”©
                buffer = BytesIO()
                colored_pil.save(buffer, format='PNG')
                import base64
                colored_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
                
                return {
                    'colored_parsing_base64': colored_base64,
                    'parsing_shape': parsing_map.shape,
                    'unique_labels': list(np.unique(parsing_map).astype(int))
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
                return {}
        
        def _create_error_response(self, error_message: str) -> Dict[str, Any]:
            """ì—ëŸ¬ ì‘ë‹µ ìƒì„± - í†µí•©ëœ ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‚¬ìš©"""
            if EXCEPTIONS_AVAILABLE:
                error = MyClosetAIException(error_message, "UNEXPECTED_ERROR")
                response = create_exception_response(
                    error, 
                    self.step_name, 
                    getattr(self, 'step_id', 1), 
                    "unknown"
                )
                # Human Parsing íŠ¹í™” í•„ë“œ ì¶”ê°€
                response.update({
                    'parsing_result': None,
                    'confidence': 0.0,
                    'processing_time': 0.0,
                    'device_used': 'cpu',
                    'model_loaded': False,
                    'checkpoint_used': False
                })
                return response
            else:
                return {
                    'success': False,
                    'error': error_message,
                    'parsing_result': None,
                    'confidence': 0.0,
                    'processing_time': 0.0,
                    'device_used': 'cpu',
                    'model_loaded': False,
                    'checkpoint_used': False,
                    'step_name': self.step_name
                }
        
        def _assess_image_quality(self, image):
            """ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€"""
            try:
                # ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€ ë¡œì§
                if image is None:
                    return 0.0
                
                # ì´ë¯¸ì§€ í¬ê¸° ê¸°ë°˜ í’ˆì§ˆ í‰ê°€
                height, width = image.shape[:2] if hasattr(image, 'shape') else (0, 0)
                size_quality = min(height * width / (512 * 512), 1.0)
                
                return size_quality
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
                return 0.5
        
        def _normalize_lighting(self, image):
            """ì¡°ëª… ì •ê·œí™”"""
            try:
                if image is None:
                    return image
                
                # ê°„ë‹¨í•œ ì¡°ëª… ì •ê·œí™”
                if len(image.shape) == 3:
                    # RGB ì´ë¯¸ì§€
                    import cv2
                    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
                    l, a, b = cv2.split(lab)
                    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
                    l = clahe.apply(l)
                    lab = cv2.merge([l, a, b])
                    normalized = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
                    return normalized
                else:
                    return image
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì¡°ëª… ì •ê·œí™” ì‹¤íŒ¨: {e}")
                return image
        
        def _correct_colors(self, image):
            """ìƒ‰ìƒ ë³´ì •"""
            try:
                if image is None:
                    return image
                
                # ğŸ”¥ numpy importë¥¼ ë©”ì„œë“œ ì‹œì‘ ë¶€ë¶„ìœ¼ë¡œ ì´ë™
                import numpy as np
                
                # PIL Imageë¥¼ numpy arrayë¡œ ë³€í™˜
                if hasattr(image, 'convert'):
                    # PIL Imageì¸ ê²½ìš°
                    image_array = np.array(image)
                elif hasattr(image, 'shape'):
                    # numpy arrayì¸ ê²½ìš°
                    image_array = image
                else:
                    return image
                
                # ê°„ë‹¨í•œ ìƒ‰ìƒ ë³´ì •
                if len(image_array.shape) == 3:
                    import cv2
                    # í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ ì ìš©
                    result = cv2.cvtColor(image_array, cv2.COLOR_RGB2LAB)
                    avg_a = np.average(result[:, :, 1])
                    avg_b = np.average(result[:, :, 2])
                    result[:, :, 1] = result[:, :, 1] - ((avg_a - 128) * (result[:, :, 0] / 255.0) * 1.1)
                    result[:, :, 2] = result[:, :, 2] - ((avg_b - 128) * (result[:, :, 0] / 255.0) * 1.1)
                    corrected = cv2.cvtColor(result, cv2.COLOR_LAB2RGB)
                    
                    # PIL Imageë¡œ ë‹¤ì‹œ ë³€í™˜
                    if hasattr(image, 'convert'):
                        return Image.fromarray(corrected)
                    else:
                        return corrected
                else:
                    return image
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
                return image
        
        def _detect_roi(self, image):
            """ROI ê°ì§€"""
            try:
                if image is None:
                    return None
                
                # ê°„ë‹¨í•œ ROI ê°ì§€ (ì „ì²´ ì´ë¯¸ì§€ë¥¼ ROIë¡œ ì„¤ì •)
                height, width = image.shape[:2] if hasattr(image, 'shape') else (0, 0)
                return {
                    'x': 0,
                    'y': 0,
                    'width': width,
                    'height': height
                }
            except Exception as e:
                self.logger.warning(f"âš ï¸ ROI ê°ì§€ ì‹¤íŒ¨: {e}")
                return None
        
        # ==============================================
        # ğŸ”¥ ê°„ì†Œí™”ëœ process() ë©”ì„œë“œ (í•µì‹¬ ë¡œì§ë§Œ)
        # ==============================================
        
        def process(self, **kwargs) -> Dict[str, Any]:
            """ğŸ”¥ ë‹¨ê³„ë³„ ì„¸ë¶„í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ê°€ ì ìš©ëœ Human Parsing process ë©”ì„œë“œ"""
            print(f"ğŸ” HumanParsingStep process ì‹œì‘")
            print(f"ğŸ” kwargs: {list(kwargs.keys()) if kwargs else 'None'}")
            
            try:
                start_time = time.time()
                print(f"âœ… start_time ì„¤ì • ì™„ë£Œ: {start_time}")
                errors = []
                stage_status = {}
                print(f"âœ… ê¸°ë³¸ ë³€ìˆ˜ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ process ë©”ì„œë“œ ì‹œì‘ ë¶€ë¶„ ì˜¤ë¥˜: {e}")
                return {'success': False, 'error': f'Process ì‹œì‘ ì˜¤ë¥˜: {e}'}
            
            try:
                # ğŸ”¥ 1ë‹¨ê³„: ì…ë ¥ ë°ì´í„° ê²€ì¦
                try:
                    if not kwargs:
                        raise ValueError("ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                    
                    # í•„ìˆ˜ ì…ë ¥ í•„ë“œ í™•ì¸
                    required_fields = ['image', 'person_image', 'input_image']
                    has_required_field = any(field in kwargs for field in required_fields)
                    if not has_required_field:
                        raise ValueError("í•„ìˆ˜ ì…ë ¥ í•„ë“œ(image, person_image, input_image ì¤‘ í•˜ë‚˜)ê°€ ì—†ìŠµë‹ˆë‹¤")
                    
                    stage_status['input_validation'] = 'success'
                    self.logger.info("âœ… ì…ë ¥ ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
                    
                except Exception as e:
                    stage_status['input_validation'] = 'failed'
                    error_info = {
                        'stage': 'input_validation',
                        'error_type': type(e).__name__,
                        'message': str(e),
                        'input_keys': list(kwargs.keys()) if kwargs else []
                    }
                    errors.append(error_info)
                    
                    # ì—ëŸ¬ ì¶”ì 
                    if EXCEPTIONS_AVAILABLE:
                        log_detailed_error(
                            DataValidationError(f"ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {str(e)}", 
                                              ErrorCodes.DATA_VALIDATION_FAILED, 
                                              {'input_keys': list(kwargs.keys()) if kwargs else []}),
                            {'step_name': self.step_name, 'step_id': getattr(self, 'step_id', 1)},
                            getattr(self, 'step_id', 1)
                        )
                    
                    return {
                        'success': False,
                        'errors': errors,
                        'stage_status': stage_status,
                        'step_name': self.step_name,
                        'processing_time': time.time() - start_time
                    }
                
                # ğŸ”¥ 2ë‹¨ê³„: ëª©ì—… ë°ì´í„° ì§„ë‹¨
                try:
                    if MOCK_DIAGNOSTIC_AVAILABLE:
                        mock_detections = []
                        for key, value in kwargs.items():
                            if value is not None:
                                mock_detection = detect_mock_data(value)
                                if mock_detection['is_mock']:
                                    mock_detections.append({
                                        'input_key': key,
                                        'detection_result': mock_detection
                                    })
                                    self.logger.warning(f"ì…ë ¥ ë°ì´í„° '{key}'ì—ì„œ ëª©ì—… ë°ì´í„° ê°ì§€: {mock_detection}")
                        
                        if mock_detections:
                            stage_status['mock_detection'] = 'warning'
                            errors.append({
                                'stage': 'mock_detection',
                                'error_type': 'MockDataDetectionError',
                                'message': 'ëª©ì—… ë°ì´í„°ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤',
                                'mock_detections': mock_detections
                            })
                        else:
                            stage_status['mock_detection'] = 'success'
                    else:
                        stage_status['mock_detection'] = 'skipped'
                        
                except Exception as e:
                    stage_status['mock_detection'] = 'failed'
                    self.logger.warning(f"ëª©ì—… ë°ì´í„° ì§„ë‹¨ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # ğŸ”¥ 3ë‹¨ê³„: ì…ë ¥ ë°ì´í„° ë³€í™˜
                try:
                    if hasattr(self, 'convert_api_input_to_step_input'):
                        converted_input = self.convert_api_input_to_step_input(kwargs)
                    else:
                        converted_input = kwargs
                    
                    stage_status['input_conversion'] = 'success'
                    self.logger.info("âœ… ì…ë ¥ ë°ì´í„° ë³€í™˜ ì™„ë£Œ")
                    
                except Exception as e:
                    stage_status['input_conversion'] = 'failed'
                    error_info = {
                        'stage': 'input_conversion',
                        'error_type': type(e).__name__,
                        'message': str(e)
                    }
                    errors.append(error_info)
                    
                    if EXCEPTIONS_AVAILABLE:
                        log_detailed_error(
                            DataValidationError(f"ì…ë ¥ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {str(e)}", 
                                              ErrorCodes.DATA_VALIDATION_FAILED),
                            {'step_name': self.step_name, 'step_id': getattr(self, 'step_id', 1)},
                            getattr(self, 'step_id', 1)
                        )
                    
                    return {
                        'success': False,
                        'errors': errors,
                        'stage_status': stage_status,
                        'step_name': self.step_name,
                        'processing_time': time.time() - start_time
                    }
                
                # ğŸ”¥ 4ë‹¨ê³„: AI ëª¨ë¸ ë¡œë”© í™•ì¸
                try:
                    if not hasattr(self, 'ai_models') or not self.ai_models:
                        raise RuntimeError("AI ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    
                    # ì‹¤ì œ ëª¨ë¸ vs Mock ëª¨ë¸ í™•ì¸
                    loaded_models = list(self.ai_models.keys())
                    is_mock_only = all('mock' in model_name.lower() for model_name in loaded_models)
                    
                    if is_mock_only:
                        stage_status['model_loading'] = 'warning'
                        errors.append({
                            'stage': 'model_loading',
                            'error_type': 'MockModelWarning',
                            'message': 'ì‹¤ì œ AI ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•„ Mock ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤',
                            'loaded_models': loaded_models
                        })
                    else:
                        stage_status['model_loading'] = 'success'
                        self.logger.info(f"âœ… AI ëª¨ë¸ ë¡œë”© í™•ì¸ ì™„ë£Œ: {loaded_models}")
                    
                except Exception as e:
                    stage_status['model_loading'] = 'failed'
                    error_info = {
                        'stage': 'model_loading',
                        'error_type': type(e).__name__,
                        'message': str(e)
                    }
                    errors.append(error_info)
                    
                    if EXCEPTIONS_AVAILABLE:
                        log_detailed_error(
                            ModelLoadingError(f"AI ëª¨ë¸ ë¡œë”© í™•ì¸ ì‹¤íŒ¨: {str(e)}", 
                                            ErrorCodes.MODEL_LOADING_FAILED),
                            {'step_name': self.step_name, 'step_id': getattr(self, 'step_id', 1)},
                            getattr(self, 'step_id', 1)
                        )
                    
                    return {
                        'success': False,
                        'errors': errors,
                        'stage_status': stage_status,
                        'step_name': self.step_name,
                        'processing_time': time.time() - start_time
                    }
                
                # ğŸ”¥ 5ë‹¨ê³„: AI ì¶”ë¡  ì‹¤í–‰
                try:
                    result = self._run_ai_inference(converted_input)
                    
                    # ì¶”ë¡  ê²°ê³¼ ê²€ì¦
                    if not result or 'success' not in result:
                        raise RuntimeError("AI ì¶”ë¡  ê²°ê³¼ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
                    
                    if not result.get('success', False):
                        raise RuntimeError(f"AI ì¶”ë¡  ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                    
                    stage_status['ai_inference'] = 'success'
                    self.logger.info("âœ… AI ì¶”ë¡  ì™„ë£Œ")
                    
                except Exception as e:
                    stage_status['ai_inference'] = 'failed'
                    error_info = {
                        'stage': 'ai_inference',
                        'error_type': type(e).__name__,
                        'message': str(e)
                    }
                    errors.append(error_info)
                    
                    if EXCEPTIONS_AVAILABLE:
                        log_detailed_error(
                            ModelInferenceError(f"AI ì¶”ë¡  ì‹¤íŒ¨: {str(e)}", 
                                              ErrorCodes.AI_INFERENCE_FAILED),
                            {'step_name': self.step_name, 'step_id': getattr(self, 'step_id', 1)},
                            getattr(self, 'step_id', 1)
                        )
                    
                    return {
                        'success': False,
                        'errors': errors,
                        'stage_status': stage_status,
                        'step_name': self.step_name,
                        'processing_time': time.time() - start_time
                    }
                
                # ğŸ”¥ 6ë‹¨ê³„: ì¶œë ¥ ë°ì´í„° ê²€ì¦
                try:
                    # ì¶œë ¥ ë°ì´í„°ì—ì„œ ëª©ì—… ë°ì´í„° ê°ì§€
                    if MOCK_DIAGNOSTIC_AVAILABLE:
                        output_mock_detections = []
                        for key, value in result.items():
                            if value is not None:
                                mock_detection = detect_mock_data(value)
                                if mock_detection['is_mock']:
                                    output_mock_detections.append({
                                        'output_key': key,
                                        'detection_result': mock_detection
                                    })
                        
                        if output_mock_detections:
                            stage_status['output_validation'] = 'warning'
                            errors.append({
                                'stage': 'output_validation',
                                'error_type': 'MockOutputWarning',
                                'message': 'ì¶œë ¥ ë°ì´í„°ì—ì„œ ëª©ì—… ë°ì´í„°ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤',
                                'mock_detections': output_mock_detections
                            })
                        else:
                            stage_status['output_validation'] = 'success'
                    else:
                        stage_status['output_validation'] = 'skipped'
                    
                except Exception as e:
                    stage_status['output_validation'] = 'failed'
                    self.logger.warning(f"ì¶œë ¥ ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # ğŸ”¥ ìµœì¢… ì‘ë‹µ ìƒì„±
                processing_time = time.time() - start_time
                
                # ì„±ê³µ ì—¬ë¶€ ê²°ì • (ì¹˜ëª…ì  ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì‹¤íŒ¨)
                critical_errors = [e for e in errors if e['stage'] in ['input_validation', 'input_conversion', 'ai_inference']]
                is_success = len(critical_errors) == 0
                
                final_result = {
                    'success': is_success,
                    'errors': errors,
                    'stage_status': stage_status,
                    'step_name': self.step_name,
                    'processing_time': processing_time,
                    'is_mock_used': any('mock' in e.get('error_type', '').lower() for e in errors),
                    'critical_error_count': len(critical_errors),
                    'warning_count': len(errors) - len(critical_errors)
                }
                
                # ì„±ê³µí•œ ê²½ìš° ì›ë³¸ ê²°ê³¼ë„ í¬í•¨
                if is_success:
                    final_result.update(result)
                
                return final_result
                
            except Exception as e:
                # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜
                processing_time = time.time() - start_time
                
                if EXCEPTIONS_AVAILABLE:
                    error = convert_to_mycloset_exception(e, {
                        'step_name': self.step_name,
                        'step_id': getattr(self, 'step_id', 1),
                        'operation': 'process'
                    })
                    track_exception(error, {
                        'step_name': self.step_name,
                        'step_id': getattr(self, 'step_id', 1),
                        'operation': 'process'
                    }, getattr(self, 'step_id', 1))
                    
                    return create_exception_response(
                        error,
                        self.step_name,
                        getattr(self, 'step_id', 1),
                        kwargs.get('session_id', 'unknown')
                    )
                else:
                    return {
                        'success': False,
                        'error': 'UNEXPECTED_ERROR',
                        'message': f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                        'step_name': self.step_name,
                        'processing_time': processing_time
                    }
        
        # ==============================================
        # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
        # ==============================================
        
        def get_step_requirements(self) -> Dict[str, Any]:
            """Step ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
            return {
                'required_models': ['graphonomy.pth', 'u2net.pth'],
                'primary_model': 'graphonomy.pth',
                'model_size_mb': 1200.0,
                'input_format': 'RGB image',
                'output_format': '20-class segmentation map',
                'device_support': ['cpu', 'mps', 'cuda'],
                'memory_requirement_gb': 2.0,
                'central_hub_required': True
            }

        def _get_service_from_central_hub(self, service_key: str):
            """Central Hubì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
            try:
                if hasattr(self, 'di_container') and self.di_container:
                    return self.di_container.get_service(service_key)
                return None
            except Exception as e:
                self.logger.warning(f"âš ï¸ Central Hub ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                return None

        def convert_api_input_to_step_input(self, api_input: Dict[str, Any]) -> Dict[str, Any]:
            """API ì…ë ¥ì„ Step ì…ë ¥ìœ¼ë¡œ ë³€í™˜ (kwargs ë°©ì‹) - ê°•í™”ëœ ì´ë¯¸ì§€ ì „ë‹¬"""
            try:
                step_input = api_input.copy()
                
                # ğŸ”¥ ê°•í™”ëœ ì´ë¯¸ì§€ ì ‘ê·¼ ë°©ì‹
                image = None
                
                # 1ìˆœìœ„: ì„¸ì…˜ ë°ì´í„°ì—ì„œ ë¡œë“œ (base64 â†’ PIL ë³€í™˜)
                if 'session_data' in step_input:
                    session_data = step_input['session_data']
                    self.logger.info(f"ğŸ” ì„¸ì…˜ ë°ì´í„° í‚¤ë“¤: {list(session_data.keys())}")
                    
                    if 'original_person_image' in session_data:
                        try:
                            import base64
                            from io import BytesIO
                            from PIL import Image
                            
                            person_b64 = session_data['original_person_image']
                            if person_b64 and len(person_b64) > 100:  # ìœ íš¨í•œ base64ì¸ì§€ í™•ì¸
                                person_bytes = base64.b64decode(person_b64)
                                image = Image.open(BytesIO(person_bytes)).convert('RGB')
                                self.logger.info(f"âœ… ì„¸ì…˜ ë°ì´í„°ì—ì„œ original_person_image ë¡œë“œ: {image.size}")
                            else:
                                self.logger.warning("âš ï¸ original_person_imageê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŒ")
                        except Exception as session_error:
                            self.logger.warning(f"âš ï¸ ì„¸ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {session_error}")
                
                # 2ìˆœìœ„: ì§ì ‘ ì „ë‹¬ëœ ì´ë¯¸ì§€ (ì´ë¯¸ PIL Imageì¸ ê²½ìš°)
                if image is None:
                    if 'person_image' in step_input and step_input['person_image'] is not None:
                        image = step_input['person_image']
                        self.logger.info(f"âœ… ì§ì ‘ ì „ë‹¬ëœ person_image ì‚¬ìš©: {getattr(image, 'size', 'unknown')}")
                    elif 'image' in step_input and step_input['image'] is not None:
                        image = step_input['image']
                        self.logger.info(f"âœ… ì§ì ‘ ì „ë‹¬ëœ image ì‚¬ìš©: {getattr(image, 'size', 'unknown')}")
                    elif 'clothing_image' in step_input and step_input['clothing_image'] is not None:
                        image = step_input['clothing_image']
                        self.logger.info(f"âœ… ì§ì ‘ ì „ë‹¬ëœ clothing_image ì‚¬ìš©: {getattr(image, 'size', 'unknown')}")
                
                # 3ìˆœìœ„: ê¸°ë³¸ê°’
                if image is None:
                    self.logger.warning("âš ï¸ ì´ë¯¸ì§€ê°€ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                    image = None
                
                # ë³€í™˜ëœ ì…ë ¥ êµ¬ì„±
                converted_input = {
                    'image': image,
                    'person_image': image,
                    'session_id': step_input.get('session_id'),
                    'confidence_threshold': step_input.get('confidence_threshold', 0.7),
                    'enhance_quality': step_input.get('enhance_quality', True),
                    'force_ai_processing': step_input.get('force_ai_processing', True)
                }
                
                # ğŸ”¥ ìƒì„¸ ë¡œê¹…
                self.logger.info(f"âœ… API ì…ë ¥ ë³€í™˜ ì™„ë£Œ: {len(converted_input)}ê°œ í‚¤")
                self.logger.info(f"âœ… ì´ë¯¸ì§€ ìƒíƒœ: {'ìˆìŒ' if image is not None else 'ì—†ìŒ'}")
                if image is not None:
                    self.logger.info(f"âœ… ì´ë¯¸ì§€ ì •ë³´: íƒ€ì…={type(image)}, í¬ê¸°={getattr(image, 'size', 'unknown')}")
                else:
                    self.logger.error("âŒ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - AI ì²˜ë¦¬ ë¶ˆê°€ëŠ¥")
                
                return converted_input
                
            except Exception as e:
                self.logger.error(f"âŒ API ì…ë ¥ ë³€í™˜ ì‹¤íŒ¨: {e}")
                return api_input
        
        def _convert_step_output_type(self, step_output: Dict[str, Any], *args, **kwargs) -> Dict[str, Any]:
            """Step ì¶œë ¥ì„ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
            try:
                if not isinstance(step_output, dict):
                    return {
                        'success': False,
                        'error': 'Invalid step output format',
                        'step_name': self.step_name
                    }
                
                # ê¸°ë³¸ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                api_response = {
                    'success': step_output.get('success', True),
                    'step_name': self.step_name,
                    'step_id': self.step_id,
                    'processing_time': step_output.get('processing_time', 0.0),
                    'central_hub_used': True
                }
                
                # ê²°ê³¼ ë°ì´í„° í¬í•¨
                if 'result' in step_output:
                    api_response['result'] = step_output['result']
                elif 'parsing_map' in step_output:
                    api_response['result'] = {
                        'parsing_map': step_output['parsing_map'],
                        'confidence': step_output.get('confidence', 0.0),
                        'detected_parts': step_output.get('detected_parts', [])
                    }
                else:
                    api_response['result'] = step_output
                
                return api_response
                
            except Exception as e:
                self.logger.error(f"âŒ _convert_step_output_type ì‹¤íŒ¨: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'step_name': self.step_name,
                    'step_id': self.step_id
                }
        
        def convert_step_output_to_api_response(self, step_output: Dict[str, Any]) -> Dict[str, Any]:
            """Step ì¶œë ¥ì„ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (step_service.py í˜¸í™˜)"""
            try:
                return self._convert_step_output_type(step_output)
            except Exception as e:
                self.logger.error(f"âŒ convert_step_output_to_api_response ì‹¤íŒ¨: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'message': 'API ì‘ë‹µ ë³€í™˜ ì‹¤íŒ¨',
                    'step_name': self.step_name,
                    'step_id': self.step_id,
                    'timestamp': time.time()
                }
                
                # ì˜¤ë¥˜ ì •ë³´ í¬í•¨
                if 'error' in step_output:
                    api_response['error'] = step_output['error']
                
                return api_response
                
            except Exception as e:
                return {
                    'success': False,
                    'error': f'Output conversion failed: {str(e)}',
                    'step_name': self.step_name
                }
        
        def cleanup_resources(self):
            """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
            try:
                # AI ëª¨ë¸ ì •ë¦¬
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                    except:
                        pass
                
                self.ai_models.clear()
                self.loaded_models.clear()
                
                # ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
                if hasattr(self, 'executor'):
                    self.executor.shutdown(wait=False)
                
                # ğŸ”¥ 128GB M3 Max ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
                for _ in range(3):
                    gc.collect()
                if TORCH_AVAILABLE and MPS_AVAILABLE:
                    try:
                        torch.mps.empty_cache()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                self.logger.info("âœ… HumanParsingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_human_parsing_step(**kwargs) -> HumanParsingStep:
    """HumanParsingStep ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return HumanParsingStep(**kwargs)
def create_optimized_human_parsing_step(**kwargs) -> HumanParsingStep:
    """ìµœì í™”ëœ HumanParsingStep ìƒì„± (M3 Max íŠ¹í™”)"""
    optimized_config = {
        'method': HumanParsingModel.GRAPHONOMY,
        'quality_level': QualityLevel.HIGH,
        'input_size': (768, 768) if IS_M3_MAX else (512, 512),
        'use_fp16': True,
        'enable_visualization': True
    }
    
    if 'parsing_config' in kwargs:
        kwargs['parsing_config'].update(optimized_config)
    else:
        kwargs['parsing_config'] = optimized_config
    
    return HumanParsingStep(**kwargs)


# ==============================================
# ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
# ==============================================

__all__ = [
    # ë©”ì¸ Step í´ë˜ìŠ¤ (í•µì‹¬)
    "HumanParsingStep",
    
]
