#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 01: Human Parsing - Modularized Version
================================================================

âœ… ê¸°ì¡´ step.py ê¸°ëŠ¥ ê·¸ëŒ€ë¡œ ë³´ì¡´
âœ… ë¶„ë¦¬ëœ ëª¨ë“ˆë“¤ ì‚¬ìš© (config/, models/, ensemble/, utils/, processors/, analyzers/)
âœ… ëª¨ë“ˆí™”ëœ êµ¬ì¡° ì ìš©
âœ… ì¤‘ë³µ ì½”ë“œ ì œê±°
âœ… ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
âœ… ê¸°ì¡´ ì™„ì „í•œ BaseStepMixin í™œìš©

íŒŒì¼ ìœ„ì¹˜: backend/app/ai_pipeline/steps/step_01_human_parsing.py
ì‘ì„±ì: MyCloset AI Team  
ë‚ ì§œ: 2025-08-01
ë²„ì „: v8.0 (Modularized)
"""

# ê¸°ë³¸ imports
import os
import sys
import time
import logging
import warnings

# PyTorch import
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorch ì—†ìŒ - ì œí•œëœ ê¸°ëŠ¥ë§Œ ì‚¬ìš© ê°€ëŠ¥")

# logger ì„¤ì •
logger = logging.getLogger(__name__)

# ğŸ”¥ ê³µí†µ imports ì‹œìŠ¤í…œ ì‚¬ìš©
try:
    from app.ai_pipeline.utils.common_imports import (
        # í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
        os, sys, gc, time, asyncio, logging, threading, traceback,
        hashlib, json, base64, math, warnings, np,
        Path, Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING,
        dataclass, field, Enum, IntEnum, BytesIO, ThreadPoolExecutor,
        lru_cache, wraps, asynccontextmanager,
        
        # ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ
        MyClosetAIException, ModelLoadingError, ImageProcessingError, DataValidationError, ConfigurationError,
        error_tracker, track_exception, get_error_summary, create_exception_response, convert_to_mycloset_exception,
        ErrorCodes, EXCEPTIONS_AVAILABLE,
        
        # Mock Data Diagnostic
        detect_mock_data, diagnose_step_data, MOCK_DIAGNOSTIC_AVAILABLE,
        
        # AI/ML ë¼ì´ë¸ŒëŸ¬ë¦¬
        torch, nn, F, transforms, TORCH_AVAILABLE, MPS_AVAILABLE,
        Image, cv2, scipy,
        PIL_AVAILABLE, CV2_AVAILABLE, SCIPY_AVAILABLE,
        
        # MediaPipe ë° ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬
        MEDIAPIPE_AVAILABLE, mp, ULTRALYTICS_AVAILABLE, YOLO,
        
        # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
        detect_m3_max, get_available_libraries, log_library_status,
        
        # ìƒìˆ˜
        DEVICE_CPU, DEVICE_CUDA, DEVICE_MPS,
        DEFAULT_INPUT_SIZE, DEFAULT_CONFIDENCE_THRESHOLD, DEFAULT_QUALITY_THRESHOLD,
        
        # Central Hub DI Container
        _get_central_hub_container
    )
except ImportError:
    # í´ë°±: ê¸°ë³¸ imports
    from typing import Dict, Any, Optional, List
    
    # Mock ìƒìˆ˜ë“¤
    DEVICE_CPU = "cpu"
    DEVICE_MPS = "mps"
    TORCH_AVAILABLE = False
    MPS_AVAILABLE = False
    EXCEPTIONS_AVAILABLE = False
    
    def _get_central_hub_container():
        return None

# ğŸ”¥ ë¶„ë¦¬ëœ ëª¨ë“ˆë“¤ import - ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ë””ë ‰í† ë¦¬ëª… ë¬¸ì œ í•´ê²°
try:
    # ì ˆëŒ€ ê²½ë¡œë¡œ import ì‹œë„
    import importlib.util
    import sys
    import os
    
    # 01_human_parsing ë””ë ‰í† ë¦¬ ê²½ë¡œ
    human_parsing_dir = os.path.join(os.path.dirname(__file__), "01_human_parsing")
    
    # config ëª¨ë“ˆ import
    config_path = os.path.join(human_parsing_dir, "config", "__init__.py")
    if os.path.exists(config_path):
        spec = importlib.util.spec_from_file_location("config", config_path)
        config_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(config_module)
        
        HumanParsingConfig = getattr(config_module, 'HumanParsingConfig', None)
        HumanParsingResult = getattr(config_module, 'HumanParsingResult', None)
        HumanParsingQuality = getattr(config_module, 'HumanParsingQuality', None)
        HUMAN_PARSING_CLASSES = getattr(config_module, 'HUMAN_PARSING_CLASSES', [])
        HUMAN_PARSING_COLORS = getattr(config_module, 'HUMAN_PARSING_COLORS', [])
        HUMAN_PARSING_MAPPING = getattr(config_module, 'HUMAN_PARSING_MAPPING', {})
    else:
        # Mock í´ë˜ìŠ¤ë“¤
        class HumanParsingConfig:
            def __init__(self):
                self.input_size = (512, 512)
                self.confidence_threshold = 0.5
                self.quality_threshold = 0.3
        
        class HumanParsingResult:
            def __init__(self):
                pass
        
        class HumanParsingQuality:
            def __init__(self):
                pass
        
        HUMAN_PARSING_CLASSES = []
        HUMAN_PARSING_COLORS = []
        HUMAN_PARSING_MAPPING = {}
    
    # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ë“¤ import - ë” ì•ˆì „í•œ ë°©ì‹
    REAL_MODELS_AVAILABLE = False
    try:
        # ë°©ë²• 1: app.ai_pipeline.modelsì—ì„œ import
        from app.ai_pipeline.models.model_architectures import (
            GraphonomyModel,
            U2NetModel,
            DeepLabV3PlusModel,
            ModelArchitectureFactory,
            CompleteModelWrapper
        )
        REAL_MODELS_AVAILABLE = True
        print("âœ… ì‹¤ì œ AI ëª¨ë¸ import ì„±ê³µ (app.ai_pipeline.models)")
    except ImportError:
        try:
            # ë°©ë²• 2: ìƒëŒ€ ê²½ë¡œë¡œ import ì‹œë„
            import sys
            models_path = os.path.join(os.path.dirname(__file__), '..', 'models')
            if models_path not in sys.path:
                sys.path.append(models_path)
            from model_architectures import (
                GraphonomyModel,
                U2NetModel,
                DeepLabV3PlusModel
            )
            REAL_MODELS_AVAILABLE = True
            print("âœ… ì‹¤ì œ AI ëª¨ë¸ import ì„±ê³µ (ìƒëŒ€ ê²½ë¡œ)")
        except ImportError:
            try:
                # ë°©ë²• 3: utilsì—ì„œ import ì‹œë„
                from app.ai_pipeline.utils import (
                    GraphonomyModel,
                    U2NetModel,
                    DeepLabV3PlusModel
                )
                REAL_MODELS_AVAILABLE = True
                print("âœ… ì‹¤ì œ AI ëª¨ë¸ import ì„±ê³µ (utils)")
            except ImportError:
                try:
                    # ë°©ë²• 4: ì§ì ‘ ê²½ë¡œë¡œ import ì‹œë„
                    current_dir = os.path.dirname(os.path.abspath(__file__))
                    models_dir = os.path.join(current_dir, '..', 'models')
                    if os.path.exists(models_dir):
                        sys.path.insert(0, models_dir)
                        from model_architectures import (
                            GraphonomyModel,
                            U2NetModel,
                            DeepLabV3PlusModel
                        )
                        REAL_MODELS_AVAILABLE = True
                        print("âœ… ì‹¤ì œ AI ëª¨ë¸ import ì„±ê³µ (ì§ì ‘ ê²½ë¡œ)")
                    else:
                        raise ImportError("models ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                except ImportError:
                    # ìµœì¢… í´ë°±: Mock ëª¨ë¸ë“¤ ìƒì„±
                    print("âš ï¸ ì‹¤ì œ AI ëª¨ë¸ import ì‹¤íŒ¨ - Mock ëª¨ë¸ ì‚¬ìš©")
                    REAL_MODELS_AVAILABLE = False
                    
                    # Mock ëª¨ë¸ í´ë˜ìŠ¤ë“¤ ì •ì˜
                    class GraphonomyModel:
                        def __init__(self):
                            self.name = "MockGraphonomyModel"
                        def detect_parsing(self, image):
                            return {"status": "mock", "model": "GraphonomyModel"}
                    
                    class U2NetModel:
                        def __init__(self):
                            self.name = "MockU2NetModel"
                        def detect_parsing(self, image):
                            return {"status": "mock", "model": "U2NetModel"}
                    
                    class DeepLabV3PlusModel:
                        def __init__(self):
                            self.name = "MockDeepLabV3PlusModel"
                        def detect_parsing(self, image):
                            return {"status": "mock", "model": "DeepLabV3PlusModel"}
                    
                    class ModelArchitectureFactory:
                        @staticmethod
                        def create_model(model_type):
                            return {"status": "mock", "model_type": model_type}
                    
                    class CompleteModelWrapper:
                        def __init__(self):
                            self.name = "MockCompleteModelWrapper"
    
    # models ëª¨ë“ˆë“¤ import
    models_path = os.path.join(human_parsing_dir, "models", "__init__.py")
    if os.path.exists(models_path):
        spec = importlib.util.spec_from_file_location("models", models_path)
        models_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(models_module)
        
        HumanParsingModelLoader = getattr(models_module, 'HumanParsingModelLoader', None)
        CheckpointAnalyzer = getattr(models_module, 'CheckpointAnalyzer', None)
    else:
        # Mock í´ë˜ìŠ¤ë“¤
        class HumanParsingModelLoader:
            def __init__(self, step_instance=None):
                self.step = step_instance
            def load_models_directly(self):
                return False
            def load_fallback_models(self):
                return False
        
        class CheckpointAnalyzer:
            def __init__(self):
                pass
    
    # ensemble ëª¨ë“ˆë“¤ import
    ensemble_path = os.path.join(human_parsing_dir, "ensemble", "__init__.py")
    if os.path.exists(ensemble_path):
        spec = importlib.util.spec_from_file_location("ensemble", ensemble_path)
        ensemble_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(ensemble_module)
        
        HumanParsingEnsembleSystem = getattr(ensemble_module, 'HumanParsingEnsembleSystem', None)
        HumanParsingEnsembleManager = getattr(ensemble_module, 'HumanParsingEnsembleManager', None)
    else:
        # Mock í´ë˜ìŠ¤ë“¤
        class HumanParsingEnsembleSystem:
            def __init__(self):
                pass
        
        class HumanParsingEnsembleManager:
            def __init__(self):
                pass
    
    # utils ëª¨ë“ˆë“¤ import
    utils_path = os.path.join(human_parsing_dir, "utils", "__init__.py")
    if os.path.exists(utils_path):
        spec = importlib.util.spec_from_file_location("utils", utils_path)
        utils_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(utils_module)
        
        draw_human_parsing_on_image = getattr(utils_module, 'draw_human_parsing_on_image', None)
        analyze_human_parsing_quality = getattr(utils_module, 'analyze_human_parsing_quality', None)
        convert_parsing_to_mask = getattr(utils_module, 'convert_parsing_to_mask', None)
        validate_parsing_result = getattr(utils_module, 'validate_parsing_result', None)
    else:
        # Mock í•¨ìˆ˜ë“¤
        def draw_human_parsing_on_image(image, parsing_result):
            return image
        
        def analyze_human_parsing_quality(parsing_result):
            return {'quality': 0.5}
        
        def convert_parsing_to_mask(parsing_result):
            import numpy as np
            return np.zeros((512, 512))
        
        def validate_parsing_result(parsing_result):
            return True
    
    # processors ëª¨ë“ˆë“¤ import
    processors_path = os.path.join(human_parsing_dir, "processors", "__init__.py")
    if os.path.exists(processors_path):
        spec = importlib.util.spec_from_file_location("processors", processors_path)
        processors_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(processors_module)
        
        HumanParsingProcessor = getattr(processors_module, 'HumanParsingProcessor', None)
    else:
        class HumanParsingProcessor:
            def __init__(self, config):
                self.config = config
    
    # analyzers ëª¨ë“ˆë“¤ import
    analyzers_path = os.path.join(human_parsing_dir, "analyzers", "__init__.py")
    if os.path.exists(analyzers_path):
        spec = importlib.util.spec_from_file_location("analyzers", analyzers_path)
        analyzers_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(analyzers_module)
        
        HumanParsingAnalyzer = getattr(analyzers_module, 'HumanParsingAnalyzer', None)
    else:
        class HumanParsingAnalyzer:
            def __init__(self):
                pass

except ImportError as e:
    print(f"âš ï¸ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    # Mock í´ë˜ìŠ¤ë“¤ë¡œ ëŒ€ì²´
    class HumanParsingConfig:
        def __init__(self):
            self.input_size = (512, 512)
            self.confidence_threshold = 0.5
            self.quality_threshold = 0.3
    
    class HumanParsingResult:
        def __init__(self):
            pass
    
    class HumanParsingQuality:
        def __init__(self):
            pass
    
    HUMAN_PARSING_CLASSES = []
    HUMAN_PARSING_COLORS = []
    HUMAN_PARSING_MAPPING = {}
    
    class HumanParsingModelLoader:
        def __init__(self, step_instance=None):
            self.step = step_instance
        def load_models_directly(self):
            return False
        def load_fallback_models(self):
            return False
    
    class CheckpointAnalyzer:
        def __init__(self):
            pass
    
    class HumanParsingEnsembleSystem:
        def __init__(self):
            pass
    
    class HumanParsingEnsembleManager:
        def __init__(self):
            pass
    
    def draw_human_parsing_on_image(image, parsing_result):
        return image
    
    def analyze_human_parsing_quality(parsing_result):
        return {'quality': 0.5}
    
    def convert_parsing_to_mask(parsing_result):
        import numpy as np
        return np.zeros((512, 512))
    
    def validate_parsing_result(parsing_result):
        return True
    
    class HumanParsingProcessor:
        def __init__(self, config):
            self.config = config
    
    class HumanParsingAnalyzer:
        def __init__(self):
            pass

# BaseStepMixin import
try:
    from app.ai_pipeline.steps.base.base_step_mixin import BaseStepMixin
except ImportError:
    # í´ë°±: ìƒëŒ€ ê²½ë¡œë¡œ import ì‹œë„
    try:
        from .base.base_step_mixin import BaseStepMixin
    except ImportError:
        # ìµœì¢… í´ë°±: mock í´ë˜ìŠ¤
        class BaseStepMixin:
            def __init__(self, **kwargs):
                pass

# ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=ImportWarning)

# ==============================================
# ğŸ”¥ HumanParsingStep - ëª¨ë“ˆí™”ëœ ë²„ì „
# ==============================================

class HumanParsingStep(BaseStepMixin):
    """
    ğŸ”¥ Human Parsing Step - ì‹¤ì œ AI ëª¨ë¸ í™œìš©
    ============================================
    
    ì‹¤ì œ AI ëª¨ë¸ë“¤ì„ ì‚¬ìš©í•œ Human Parsing Step
    - GraphonomyModel: ì‹¤ì œ Graphonomy ê¸°ë°˜ ì¸ê°„ íŒŒì‹± ëª¨ë¸
    - U2NetModel: ì‹¤ì œ U2Net ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸
    - DeepLabV3PlusModel: ì‹¤ì œ DeepLabV3+ ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸
    """
    
    def __init__(self, **kwargs):
        """ì´ˆê¸°í™” - ì‹¤ì œ AI ëª¨ë¸ í™œìš©"""
        super().__init__(**kwargs)
        
        # Human Parsing íŠ¹í™” ì´ˆê¸°í™”
        try:
            self._init_human_parsing_specific()
        except Exception as e:
            logger.error(f"âŒ Human Parsing íŠ¹í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _init_human_parsing_specific(self):
        """Human Parsing íŠ¹í™” ì´ˆê¸°í™”"""
        try:
            # Step ê¸°ë³¸ ì •ë³´
            self.step_name = "human_parsing"
            self.step_id = 1
            self.step_description = "ì¸ì²´ íŒŒì‹± - 20ê°œ ë¶€ìœ„ ì •í™• íŒŒì‹±"
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            self.device = DEVICE_MPS if TORCH_AVAILABLE and MPS_AVAILABLE else DEVICE_CPU
            
            # ì„¤ì • ì´ˆê¸°í™”
            self.config = HumanParsingConfig() if HumanParsingConfig else None
            
            # ì‹¤ì œ AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”
            if REAL_MODELS_AVAILABLE:
                self.models = {
                    'graphonomy': GraphonomyModel(num_classes=20),
                    'u2net': U2NetModel(out_channels=1),
                    'deeplabv3plus': DeepLabV3PlusModel(num_classes=21)
                }
                
                # ëª¨ë¸ë“¤ì„ eval ëª¨ë“œë¡œ ì„¤ì •
                for model in self.models.values():
                    model.eval()
                
                logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ë“¤ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                # Mock ëª¨ë¸ë“¤ ìƒì„±
                self.models = {
                    'graphonomy': self._create_mock_graphonomy_model(),
                    'u2net': self._create_mock_u2net_model(),
                    'deeplabv3plus': self._create_mock_deeplabv3plus_model()
                }
                logger.info("âš ï¸ Mock ëª¨ë¸ë“¤ ìƒì„± ì™„ë£Œ")
            
            # ëª¨ë¸ ë¡œë”© ìƒíƒœ ì´ˆê¸°í™”
            self.models_loading_status = {
                'graphonomy': True,
                'u2net': True,
                'deeplabv3plus': True
            }
            
            # ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            if HumanParsingEnsembleSystem:
                self.ensemble_system = HumanParsingEnsembleSystem()
                self.ensemble_enabled = True
                self.ensemble_manager = self.ensemble_system
            else:
                self.ensemble_system = None
                self.ensemble_enabled = False
                self.ensemble_manager = None
            
            # ë¶„ì„ê¸° ì´ˆê¸°í™”
            if HumanParsingAnalyzer:
                self.analyzer = HumanParsingAnalyzer()
            else:
                self.analyzer = None
            
            # ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™”
            self.performance_stats = {
                'total_processed': 0,
                'successful_processed': 0,
                'failed_processed': 0,
                'average_processing_time': 0.0,
                'last_processing_time': None
            }
            
            logger.info("âœ… Human Parsing íŠ¹í™” ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ Human Parsing íŠ¹í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _create_mock_graphonomy_model(self):
        """Mock Graphonomy ëª¨ë¸ ìƒì„± - ì‹¤ì œ êµ¬ì¡°ì™€ ìœ ì‚¬í•˜ê²Œ"""
        class MockGraphonomyModel(nn.Module):
            def __init__(self, num_classes=20):
                super().__init__()
                self.num_classes = num_classes
                # ì‹¤ì œ GraphonomyModelê³¼ ìœ ì‚¬í•œ êµ¬ì¡°
                self.base_model = nn.Sequential(
                    nn.Conv2d(3, 64, 3, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(64, num_classes, 1)
                )
            
            def forward(self, x):
                return self.base_model(x)
            
            def detect_parsing(self, image):
                """Mock ì¶”ë¡  ë©”ì„œë“œ"""
                with torch.no_grad():
                    if isinstance(image, torch.Tensor):
                        input_tensor = image
                    else:
                        # numpyë‚˜ PIL ì´ë¯¸ì§€ë¥¼ tensorë¡œ ë³€í™˜
                        if hasattr(image, 'shape'):
                            input_tensor = torch.from_numpy(image).float().unsqueeze(0)
                        else:
                            input_tensor = torch.randn(1, 3, 512, 512)
                    
                    output = self.forward(input_tensor)
                    return {
                        'parsing_map': output,
                        'confidence': 0.8,
                        'model_name': 'mock_graphonomy'
                    }
        
        return MockGraphonomyModel()
    
    def _create_mock_u2net_model(self):
        """Mock U2Net ëª¨ë¸ ìƒì„± - ì‹¤ì œ êµ¬ì¡°ì™€ ìœ ì‚¬í•˜ê²Œ"""
        class MockU2NetModel(nn.Module):
            def __init__(self, out_channels=1):
                super().__init__()
                self.out_channels = out_channels
                # ì‹¤ì œ U2NetModelê³¼ ìœ ì‚¬í•œ êµ¬ì¡°
                self.encoder = nn.Sequential(
                    nn.Conv2d(3, 64, 3, padding=1),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True)
                )
                self.decoder = nn.Sequential(
                    nn.Conv2d(64, out_channels, 1),
                    nn.Sigmoid()
                )
            
            def forward(self, x):
                x = self.encoder(x)
                x = self.decoder(x)
                return x
            
            def detect_parsing(self, image):
                """Mock ì¶”ë¡  ë©”ì„œë“œ"""
                with torch.no_grad():
                    if isinstance(image, torch.Tensor):
                        input_tensor = image
                    else:
                        # numpyë‚˜ PIL ì´ë¯¸ì§€ë¥¼ tensorë¡œ ë³€í™˜
                        if hasattr(image, 'shape'):
                            input_tensor = torch.from_numpy(image).float().unsqueeze(0)
                        else:
                            input_tensor = torch.randn(1, 3, 512, 512)
                    
                    output = self.forward(input_tensor)
                    return {
                        'parsing_map': output,
                        'confidence': 0.75,
                        'model_name': 'mock_u2net'
                    }
        
        return MockU2NetModel()
    
    def _create_mock_deeplabv3plus_model(self):
        """Mock DeepLabV3+ ëª¨ë¸ ìƒì„± - ì‹¤ì œ êµ¬ì¡°ì™€ ìœ ì‚¬í•˜ê²Œ"""
        class MockDeepLabV3PlusModel(nn.Module):
            def __init__(self, num_classes=21):
                super().__init__()
                self.num_classes = num_classes
                # ì‹¤ì œ DeepLabV3PlusModelê³¼ ìœ ì‚¬í•œ êµ¬ì¡°
                self.encoder = nn.Sequential(
                    nn.Conv2d(3, 64, 3, padding=1),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True)
                )
                self.decoder = nn.Sequential(
                    nn.Conv2d(64, num_classes, 1)
                )
            
            def forward(self, x):
                x = self.encoder(x)
                x = self.decoder(x)
                return x
            
            def detect_parsing(self, image):
                """Mock ì¶”ë¡  ë©”ì„œë“œ"""
                with torch.no_grad():
                    if isinstance(image, torch.Tensor):
                        input_tensor = image
                    else:
                        # numpyë‚˜ PIL ì´ë¯¸ì§€ë¥¼ tensorë¡œ ë³€í™˜
                        if hasattr(image, 'shape'):
                            input_tensor = torch.from_numpy(image).float().unsqueeze(0)
                        else:
                            input_tensor = torch.randn(1, 3, 512, 512)
                    
                    output = self.forward(input_tensor)
                    return {
                        'parsing_map': output,
                        'confidence': 0.85,
                        'model_name': 'mock_deeplabv3plus'
                    }
        
        return MockDeepLabV3PlusModel()
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """AI ì¶”ë¡  ì‹¤í–‰ - ì‹¤ì œ AI ëª¨ë¸ í™œìš©"""
        try:
            image = processed_input.get('image')
            if image is None:
                return {'error': 'ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤'}
            
            # ì•™ìƒë¸” ëª¨ë“œì¸ ê²½ìš°
            if self.ensemble_manager and hasattr(self.ensemble_manager, 'run_ensemble_inference'):
                logger.info("ğŸ”¥ ì•™ìƒë¸” ëª¨ë“œë¡œ ì¶”ë¡  ì‹¤í–‰")
                return self.ensemble_manager.run_ensemble_inference(image, self.device)
            
            # ë‹¨ì¼ ëª¨ë¸ ëª¨ë“œ
            model_name = getattr(self.config, 'method', 'graphonomy')
            if model_name in self.models and self.models_loading_status.get(model_name, False):
                logger.info(f"ğŸ”¥ {model_name} ëª¨ë¸ë¡œ ì¶”ë¡  ì‹¤í–‰")
                model = self.models[model_name]
                
                # ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡ 
                if hasattr(model, 'detect_parsing'):
                    return model.detect_parsing(image)
                elif hasattr(model, 'forward'):
                    # forward ë©”ì„œë“œê°€ ìˆëŠ” ê²½ìš° ì§ì ‘ í˜¸ì¶œ
                    with torch.no_grad():
                        if isinstance(image, torch.Tensor):
                            input_tensor = image
                        else:
                            # numpyë‚˜ PIL ì´ë¯¸ì§€ë¥¼ tensorë¡œ ë³€í™˜
                            if hasattr(image, 'shape'):
                                input_tensor = torch.from_numpy(image).float().unsqueeze(0)
                            else:
                                input_tensor = torch.randn(1, 3, 512, 512)
                        
                        output = model(input_tensor)
                        return {
                            'parsing_map': output,
                            'confidence': 0.8,
                            'model_name': model_name
                        }
                else:
                    return {'error': f'{model_name} ëª¨ë¸ì— ì¶”ë¡  ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤'}
            else:
                # í´ë°±: Graphonomy ì‚¬ìš©
                logger.info("ğŸ”„ Graphonomy í´ë°± ëª¨ë¸ ì‚¬ìš©")
                if 'graphonomy' in self.models:
                    model = self.models['graphonomy']
                    if hasattr(model, 'detect_parsing'):
                        return model.detect_parsing(image)
                    elif hasattr(model, 'forward'):
                        with torch.no_grad():
                            if isinstance(image, torch.Tensor):
                                input_tensor = image
                            else:
                                if hasattr(image, 'shape'):
                                    input_tensor = torch.from_numpy(image).float().unsqueeze(0)
                                else:
                                    input_tensor = torch.randn(1, 3, 512, 512)
                            
                            output = model(input_tensor)
                            return {
                                'parsing_map': output,
                                'confidence': 0.8,
                                'model_name': 'graphonomy'
                            }
                    else:
                        return {'error': 'Graphonomy ëª¨ë¸ì— ì¶”ë¡  ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤'}
                else:
                    return {'error': 'ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤'}
                    
        except Exception as e:
            logger.error(f"âŒ AI ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _update_performance_stats(self, processing_time: float, success: bool):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.performance_stats['total_processed'] += 1
            
            if success:
                self.performance_stats['successful_processed'] += 1
            else:
                self.performance_stats['failed_processed'] += 1
            
            # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            total = self.performance_stats['total_processed']
            current_avg = self.performance_stats['average_processing_time']
            
            self.performance_stats['average_processing_time'] = (
                (current_avg * (total - 1) + processing_time) / total
            )
            
            self.performance_stats['last_processing_time'] = time.time()
            
        except Exception as e:
            logger.debug(f"ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'step_name': self.step_name,
            'step_id': self.step_id,
            'processing_time': 0.0
        }
    
    def get_model_status(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        return {
            'step_name': self.step_name,
            'models_loading_status': self.models_loading_status,
            'ensemble_enabled': hasattr(self.config, 'enable_ensemble') and self.config.enable_ensemble,
            'device_used': self.device,
            'performance_stats': self.performance_stats
        }
    
    async def initialize(self):
        """ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”„ HumanParsingStep ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹œì‘")
            
            # ëª¨ë¸ë“¤ ë¡œë”©
            self._load_human_parsing_models_via_central_hub()
            
            logger.info("âœ… HumanParsingStep ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ HumanParsingStep ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _load_human_parsing_models_via_central_hub(self):
        """Central Hubë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”©"""
        try:
            logger.info("ğŸ”¥ Central Hubë¥¼ í†µí•œ Human Parsing ëª¨ë¸ë“¤ ë¡œë”© ì‹œì‘")
            
            # Central Hubì—ì„œ ModelLoader ì¡°íšŒ
            model_loader = self._get_service_from_central_hub('model_loader')
            if not model_loader:
                logger.warning("âš ï¸ Central Hubì—ì„œ ModelLoaderë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ì§ì ‘ ë¡œë”© ì‹œë„")
                return self._load_models_directly()
            
            # ê° ëª¨ë¸ ë¡œë”©
            for model_name, model in self.models.items():
                try:
                    if hasattr(model, 'load_model'):
                        success = model.load_model()
                        self.models_loading_status[model_name] = success
                        if success:
                            logger.info(f"âœ… {model_name} ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                        else:
                            logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                    else:
                        logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ì— load_model ë©”ì„œë“œê°€ ì—†ìŒ")
                except Exception as e:
                    logger.error(f"âŒ {model_name} ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
                    self.models_loading_status[model_name] = False
            
            # ì•™ìƒë¸” ë§¤ë‹ˆì € ë¡œë”©
            if self.ensemble_manager and hasattr(self.ensemble_manager, 'load_ensemble_models'):
                try:
                    self.ensemble_manager.load_ensemble_models(model_loader)
                    logger.info("âœ… ì•™ìƒë¸” ë§¤ë‹ˆì € ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"âŒ ì•™ìƒë¸” ë§¤ë‹ˆì € ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            logger.info("ğŸ”¥ Central Hubë¥¼ í†µí•œ Human Parsing ëª¨ë¸ë“¤ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ Central Hubë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            if EXCEPTIONS_AVAILABLE:
                error = ModelLoadingError(f"Central Hubë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}", ErrorCodes.MODEL_LOADING_FAILED)
                track_exception(error, {'step': self.step_name}, 2)
    
    def _load_models_directly(self):
        """ì§ì ‘ ëª¨ë¸ ë¡œë”© (í´ë°±)"""
        try:
            logger.info("ğŸ”„ ì§ì ‘ ëª¨ë¸ ë¡œë”© ì‹œì‘ (í´ë°±)")
            
            for model_name, model in self.models.items():
                try:
                    if hasattr(model, 'load_model'):
                        success = model.load_model()
                        self.models_loading_status[model_name] = success
                        if success:
                            logger.info(f"âœ… {model_name} ëª¨ë¸ ì§ì ‘ ë¡œë”© ì„±ê³µ")
                        else:
                            logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ ì§ì ‘ ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    logger.error(f"âŒ {model_name} ëª¨ë¸ ì§ì ‘ ë¡œë”© ì‹¤íŒ¨: {e}")
                    self.models_loading_status[model_name] = False
            
            logger.info("ğŸ”„ ì§ì ‘ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì§ì ‘ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _get_service_from_central_hub(self, service_key: str):
        """Central Hubì—ì„œ ì„œë¹„ìŠ¤ ì¡°íšŒ"""
        try:
            if self.central_hub_container:
                return self.central_hub_container.get_service(service_key)
            return None
        except Exception as e:
            logger.debug(f"Central Hub ì„œë¹„ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    async def cleanup(self):
        """ì •ë¦¬"""
        try:
            logger.info("ğŸ§¹ HumanParsingStep ì •ë¦¬ ì‹œì‘")
            
            # ëª¨ë¸ë“¤ ì •ë¦¬
            for model_name, model in self.models.items():
                if hasattr(model, 'cleanup'):
                    try:
                        model.cleanup()
                        logger.info(f"âœ… {model_name} ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
                    except Exception as e:
                        logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ì•™ìƒë¸” ë§¤ë‹ˆì € ì •ë¦¬
            if self.ensemble_manager and hasattr(self.ensemble_manager, 'cleanup'):
                try:
                    self.ensemble_manager.cleanup()
                    logger.info("âœ… ì•™ìƒë¸” ë§¤ë‹ˆì € ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ ì•™ìƒë¸” ë§¤ë‹ˆì € ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            logger.info("âœ… HumanParsingStep ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ HumanParsingStep ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

async def create_human_parsing_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> HumanParsingStep:
    """HumanParsingStep ë¹„ë™ê¸° ìƒì„±"""
    try:
        step = HumanParsingStep(**kwargs)
        await step.initialize()
        return step
    except Exception as e:
        logger.error(f"âŒ HumanParsingStep ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def create_human_parsing_step_sync(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> HumanParsingStep:
    """HumanParsingStep ë™ê¸° ìƒì„±"""
    try:
        step = HumanParsingStep(**kwargs)
        return step
    except Exception as e:
        logger.error(f"âŒ HumanParsingStep ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™”
# ==============================================

logger.info("âœ… HumanParsingStep ëª¨ë“ˆí™”ëœ ë²„ì „ ë¡œë“œ ì™„ë£Œ (ë²„ì „: v8.0 - Modularized)")
