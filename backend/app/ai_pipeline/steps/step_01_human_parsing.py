#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 01: Human Parsing v8.0 - Common Imports Integration
=======================================================================

âœ… Common Imports ì‹œìŠ¤í…œ ì™„ì „ í†µí•© - ì¤‘ë³µ import ë¸”ë¡ ì œê±°
âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - ì¤‘ì•™ í—ˆë¸Œ íŒ¨í„´ ì ìš©
âœ… BaseStepMixin v20.0 ì™„ì „ ìƒì† - super().__init__() í˜¸ì¶œ
âœ… í•„ìˆ˜ ì†ì„± ì´ˆê¸°í™” - ai_models, models_loading_status, model_interface, loaded_models
âœ… _load_ai_models_via_central_hub() êµ¬í˜„ - ModelLoaderë¥¼ í†µí•œ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©
âœ… ê°„ì†Œí™”ëœ process() ë©”ì„œë“œ - í•µì‹¬ Human Parsing ë¡œì§ë§Œ
âœ… ì—ëŸ¬ ë°©ì§€ìš© í´ë°± ë¡œì§ - Mock ëª¨ë¸ ìƒì„± (ì‹¤ì œ AI ëª¨ë¸ ëŒ€ì²´ìš©)
âœ… GitHubDependencyManager ì™„ì „ ì‚­ì œ - ë³µì¡í•œ ì˜ì¡´ì„± ê´€ë¦¬ ì½”ë“œ ì œê±°
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - TYPE_CHECKING + ì§€ì—° import
âœ… Graphonomy ëª¨ë¸ ë¡œë”© - 1.2GB ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ì§€ì›
âœ… Human body parsing - 20ê°œ í´ë˜ìŠ¤ ì •í™• ë¶„ë¥˜
âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ - ì™„ì „ êµ¬í˜„

í•µì‹¬ êµ¬í˜„ ê¸°ëŠ¥:
1. Graphonomy ResNet-101 + ASPP ì•„í‚¤í…ì²˜ (ì‹¤ì œ 1.2GB ì²´í¬í¬ì¸íŠ¸)
2. U2Net í´ë°± ëª¨ë¸ (ê²½ëŸ‰í™” ëŒ€ì•ˆ)
3. 20ê°œ ì¸ì²´ ë¶€ìœ„ ì •í™• íŒŒì‹± (ë°°ê²½ í¬í•¨)
4. 512x512 ì…ë ¥ í¬ê¸° í‘œì¤€í™”
5. MPS/CUDA ë””ë°”ì´ìŠ¤ ìµœì í™”

Author: MyCloset AI Team
Date: 2025-07-31
Version: 8.1 (Common Imports Integration)
"""

# ğŸ”¥ Common Imports ì‚¬ìš©
from app.ai_pipeline.utils.common_imports import (
    # í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
    os, sys, gc, logging, threading, traceback, warnings,
    Path, Dict, Any, Optional, Tuple, List, Union, TYPE_CHECKING,
    dataclass, field, Enum, BytesIO, ThreadPoolExecutor,
    
    # AI/ML ë¼ì´ë¸ŒëŸ¬ë¦¬
    torch, nn, F, transforms, TORCH_AVAILABLE, MPS_AVAILABLE,
    np, cv2, NUMPY_AVAILABLE, CV2_AVAILABLE,
    Image, ImageFilter, ImageEnhance, PIL_AVAILABLE,
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    detect_m3_max, get_available_libraries, log_library_status,
    
    # ìƒìˆ˜
    DEVICE_CPU, DEVICE_CUDA, DEVICE_MPS,
    DEFAULT_INPUT_SIZE, DEFAULT_CONFIDENCE_THRESHOLD, DEFAULT_QUALITY_THRESHOLD,
    
    # ì—ëŸ¬ ì²˜ë¦¬
    EXCEPTIONS_AVAILABLE, convert_to_mycloset_exception, track_exception, create_exception_response,
    
    # Mock ì§„ë‹¨ ì‹œìŠ¤í…œ
    MOCK_DIAGNOSTIC_AVAILABLE, detect_mock_data,
    
    # Central Hub í•¨ìˆ˜
    _get_central_hub_container
)

# ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¶”ê°€
from app.ai_pipeline.utils.memory_monitor import log_step_memory, cleanup_step_memory

# ğŸ”¥ ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ í™•ì¸
try:
    import pydensecrf.densecrf as dcrf
    from pydensecrf.utils import unary_from_softmax
    DENSECRF_AVAILABLE = True
except ImportError:
    DENSECRF_AVAILABLE = False

# ğŸ”¥ Human Parsing ëª¨ë“ˆ imports (ìˆ˜ì •ë¨)
try:
    from .human_parsing.config import (
        HumanParsingModel, QualityLevel, EnhancedHumanParsingConfig,
        BODY_PARTS, VISUALIZATION_COLORS
    )
    from .human_parsing.processors import (
        HighResolutionProcessor, SpecialCaseProcessor
    )
    from .human_parsing.ensemble import (
        ModelEnsembleManager, MemoryEfficientEnsembleSystem
    )
    from .human_parsing.postprocessing import (
        AdvancedPostProcessor, QualityEnhancer
    )
    from .human_parsing.utils import (
        ParsingValidator, ConfidenceCalculator, ParsingMapValidator, get_original_size_safely
    )
    from .human_parsing.models import (
        U2NetForParsing
    )
    HUMAN_PARSING_MODULES_AVAILABLE = True
except ImportError as e:
    HUMAN_PARSING_MODULES_AVAILABLE = False
    print(f"âš ï¸ Human Parsing ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")

# ğŸ”¥ Human Parsing ëª¨ë“ˆë“¤ ì§ì ‘ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
try:
    from .human_parsing.config import EnhancedHumanParsingConfig
    from .human_parsing.postprocessing import AdvancedPostProcessor
    from .human_parsing.ensemble import ModelEnsembleManager
    from .human_parsing.processors import HighResolutionProcessor, SpecialCaseProcessor
    from .human_parsing.utils import ParsingMapValidator, get_original_size_safely
    from .human_parsing.models import U2NetForParsing
    HUMAN_PARSING_MODULES_AVAILABLE = True
except ImportError as e:
    HUMAN_PARSING_MODULES_AVAILABLE = False

# ğŸ”¥ ìƒˆë¡œ ë¶„ë¦¬ëœ ëª¨ë“ˆë“¤ import
try:
    # ë¶„ë¦¬ëœ ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    from .human_parsing.models.graphonomy_models import (
        AdvancedGraphonomyResNetASPP,
        ResNet101Backbone,
        ResNetBottleneck,
        ASPPModule,
        SelfAttentionBlock
    )
    from .human_parsing.models.mock_model import MockHumanParsingModel
    
    # ë¶„ë¦¬ëœ ì¶”ë¡  ì—”ì§„ë“¤
    from .human_parsing.inference_engines import (
        GraphonomyInferenceEngine,
        U2NetInferenceEngine,
        HRNetInferenceEngine,
        DeepLabV3PlusInferenceEngine,
        GenericInferenceEngine
    )
    
    # ë¶„ë¦¬ëœ ìœ í‹¸ë¦¬í‹°ë“¤
    from .human_parsing.utils.processing_utils import ProcessingUtils
    from .human_parsing.utils.quality_assessment import QualityAssessment
    
    REFACTORED_MODULES_AVAILABLE = True
except ImportError as e:
    REFACTORED_MODULES_AVAILABLE = False
    print(f"âš ï¸ ë¦¬íŒ©í† ë§ëœ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    print(f"âš ï¸ Human Parsing ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")

# ğŸ”¥ ëˆ„ë½ëœ ëª¨ë¸ í´ë˜ìŠ¤ë“¤ import
try:
    from .human_parsing.models.iterative_refinement import IterativeRefinementModule
    from .human_parsing.models.u2net_model import U2NetForParsing
    MODEL_CLASSES_AVAILABLE = True
except ImportError as e:
    MODEL_CLASSES_AVAILABLE = False
    print(f"âš ï¸ ëª¨ë¸ í´ë˜ìŠ¤ import ì‹¤íŒ¨: {e}")

# ğŸ”¥ ì§ì ‘ import (common_importsì—ì„œ ëˆ„ë½ëœ ëª¨ë“ˆë“¤)
import time
import time as time_module
# ğŸ”¥ Human Parsing ëª¨ë“ˆë“¤ì€ common_importsì—ì„œ ìë™ìœ¼ë¡œ importë¨
# ğŸ”¥ Mock ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin

# ğŸ”¥ ë¶„ë¦¬ëœ ëª¨ë“ˆë“¤ import
from .human_parsing.config import (
    HumanParsingModel, QualityLevel, EnhancedHumanParsingConfig,
    BODY_PARTS, VISUALIZATION_COLORS
)
from .human_parsing.postprocessing.post_processor import AdvancedPostProcessor
from .human_parsing.utils.validation_utils import (
    get_original_size_safely, parsing_validator, validate_confidence_map
)

# ğŸ”¥ ê¸°ì¡´ Graphonomy ëª¨ë“ˆë“¤ import
from app.ai_pipeline.utils.graphonomy_models import (
    ASPPModule, SelfAttentionBlock, ResNetBottleneck, ResNet101Backbone
)
from app.ai_pipeline.utils.graphonomy_processor import (
    DynamicGraphonomyModel, GraphonomyModelProcessor
)
from app.ai_pipeline.utils.graphonomy_checkpoint_system import (
    GraphonomyCheckpointAnalyzer, GraphonomyModelFactory, GraphonomyCheckpointLoader
)

# ==============================================
# ğŸ”¥ í™˜ê²½ ì„¤ì • ë° ìµœì í™”
# ==============================================

# M3 Max ê°ì§€ (common_importsì—ì„œ ê°€ì ¸ì˜´)
IS_M3_MAX = detect_m3_max()

# M3 Max ìµœì í™” ì„¤ì •
if IS_M3_MAX and TORCH_AVAILABLE and MPS_AVAILABLE:
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    os.environ['TORCH_MPS_PREFER_METAL'] = '1'

# ==============================================
# ğŸ”¥ HumanParsingStep - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
# ==============================================

# BaseStepMixin ì‚¬ìš© ê°€ëŠ¥
# ğŸ”¥ HumanParsingStep í´ë˜ìŠ¤ìš© time ëª¨ë“ˆ ëª…ì‹œì  import
import time
# ğŸ”¥ ì „ì—­ ìŠ¤ì½”í”„ì—ì„œ time ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡
globals()['time'] = time
# ğŸ”¥ í´ë˜ìŠ¤ ì •ì˜ ì‹œì ì— time ëª¨ë“ˆì„ ë¡œì»¬ ìŠ¤ì½”í”„ì—ë„ ì¶”ê°€
locals()['time'] = time

class HumanParsingStep(BaseStepMixin):
        """
        ğŸ”¥ Step 01: Human Parsing v8.0 - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
        
        BaseStepMixin v20.0ì—ì„œ ìë™ ì œê³µ:
        âœ… í‘œì¤€í™”ëœ process() ë©”ì„œë“œ (ë°ì´í„° ë³€í™˜ ìë™ ì²˜ë¦¬)
        âœ… API â†” AI ëª¨ë¸ ë°ì´í„° ë³€í™˜ ìë™í™”
        âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìë™ ì ìš©
        âœ… Central Hub DI Container ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ
        âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
        âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë° ë©”ëª¨ë¦¬ ìµœì í™”
        
        ì´ í´ë˜ìŠ¤ëŠ” _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„!
        """
        
        def __init__(self, **kwargs):
            """Central Hub DI Container ê¸°ë°˜ ì´ˆê¸°í™”"""
            print(f"ğŸ” HumanParsingStep __init__ ì‹œì‘")
            try:
                print(f"ğŸ” super().__init__() í˜¸ì¶œ ì „")
                # ğŸ”¥ BaseStepMixin v20.0 ì™„ì „ ìƒì† - super().__init__() í˜¸ì¶œ
                super().__init__(
                    step_name="HumanParsingStep",
                    **kwargs
                )
                print(f"âœ… super().__init__() í˜¸ì¶œ ì™„ë£Œ")
                
                # ğŸ”¥ time ëª¨ë“ˆ ì°¸ì¡° ì €ì¥ (í´ë˜ìŠ¤ ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´)
                print(f"ğŸ” time ëª¨ë“ˆ import ì‹œì‘")
                import time
                print(f"âœ… time ëª¨ë“ˆ import ì„±ê³µ")
                self.time = time
                print(f"âœ… time ëª¨ë“ˆ ì°¸ì¡° ì €ì¥ ì™„ë£Œ")
                
                # ğŸ”¥ í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™” (Central Hub DI Container ìš”êµ¬ì‚¬í•­)
                print(f"ğŸ” AI ëª¨ë¸ ì €ì¥ì†Œ ì´ˆê¸°í™” ì‹œì‘")
                self.ai_models = {}  # AI ëª¨ë¸ ì €ì¥ì†Œ
                print(f"âœ… AI ëª¨ë¸ ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
                
                print(f"ğŸ” ëª¨ë¸ ë¡œë”© ìƒíƒœ ì´ˆê¸°í™” ì‹œì‘")
                self.models_loading_status = {  # ëª¨ë¸ ë¡œë”© ìƒíƒœ
                    'graphonomy': False,
                    'u2net': False,
                    'mock': False
                }
                print(f"âœ… ëª¨ë¸ ë¡œë”© ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ")
                
                print(f"ğŸ” ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì‹œì‘")
                self.model_interface = None  # ModelLoader ì¸í„°í˜ì´ìŠ¤
                self.model_loader = None  # ModelLoader ì§ì ‘ ì°¸ì¡°
                self.loaded_models = {}  # ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ (ë”•ì…”ë„ˆë¦¬ë¡œ ë³€ê²½)
                print(f"âœ… ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
                
                # Human Parsing ì„¤ì •
                print(f"ğŸ” Human Parsing ì„¤ì • ì´ˆê¸°í™” ì‹œì‘")
                # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ì‚¬ìš© ì„¤ì •
                self.config = EnhancedHumanParsingConfig(
                    method=HumanParsingModel.GRAPHONOMY,  # ğŸ”¥ ì‹¤ì œ Graphonomy ëª¨ë¸ ì‚¬ìš©
                    quality_level=QualityLevel.HIGH,  # ğŸ”¥ ê³ í’ˆì§ˆ ì²˜ë¦¬
                    enable_ensemble=True,  # ğŸ”¥ ì•™ìƒë¸” í™œì„±í™”
                    enable_high_resolution=True,  # ğŸ”¥ ê³ í•´ìƒë„ ì²˜ë¦¬ í™œì„±í™”
                    enable_special_case_handling=True,  # ğŸ”¥ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ í™œì„±í™”
                    enable_crf_postprocessing=True,  # ğŸ”¥ CRF í›„ì²˜ë¦¬ í™œì„±í™”
                    enable_edge_refinement=True,  # ğŸ”¥ ì—£ì§€ ì •ì œ í™œì„±í™”
                    enable_hole_filling=True,  # ğŸ”¥ í™€ ì±„ìš°ê¸° í™œì„±í™”
                    enable_multiscale_processing=True,  # ğŸ”¥ ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬ í™œì„±í™”
                    enable_quality_validation=True,  # ğŸ”¥ í’ˆì§ˆ ê²€ì¦ í™œì„±í™”
                    enable_auto_retry=True,  # ğŸ”¥ ìë™ ì¬ì‹œë„ í™œì„±í™”
                    enable_visualization=True,  # ğŸ”¥ ì‹œê°í™” í™œì„±í™”
                    use_fp16=True,  # ğŸ”¥ FP16 í™œì„±í™”
                    remove_noise=True,  # ğŸ”¥ ë…¸ì´ì¦ˆ ì œê±° í™œì„±í™”
                    auto_preprocessing=True,  # ğŸ”¥ ìë™ ì „ì²˜ë¦¬ í™œì„±í™”
                    strict_data_validation=True,  # ğŸ”¥ ì—„ê²©í•œ ë°ì´í„° ê²€ì¦ í™œì„±í™”
                    auto_postprocessing=True,  # ğŸ”¥ ìë™ í›„ì²˜ë¦¬ í™œì„±í™”
                    enable_uncertainty_quantification=True,  # ğŸ”¥ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” í™œì„±í™”
                    enable_confidence_calibration=True,  # ğŸ”¥ ì‹ ë¢°ë„ ë³´ì • í™œì„±í™”
                    enable_super_resolution=True,  # ğŸ”¥ ìŠˆí¼ í•´ìƒë„ í™œì„±í™”
                    enable_noise_reduction=True,  # ğŸ”¥ ë…¸ì´ì¦ˆ ê°ì†Œ í™œì„±í™”
                    enable_lighting_normalization=True,  # ğŸ”¥ ì¡°ëª… ì •ê·œí™” í™œì„±í™”
                    enable_color_correction=True,  # ğŸ”¥ ìƒ‰ìƒ ë³´ì • í™œì„±í™”
                    enable_transparent_clothing=True,  # ğŸ”¥ íˆ¬ëª… ì˜ë¥˜ ì²˜ë¦¬ í™œì„±í™”
                    enable_layered_clothing=True,  # ğŸ”¥ ë ˆì´ì–´ë“œ ì˜ë¥˜ ì²˜ë¦¬ í™œì„±í™”
                    enable_complex_patterns=True,  # ğŸ”¥ ë³µì¡í•œ íŒ¨í„´ ì²˜ë¦¬ í™œì„±í™”
                    enable_reflective_materials=True,  # ğŸ”¥ ë°˜ì‚¬ ì¬ì§ˆ ì²˜ë¦¬ í™œì„±í™”
                    enable_oversized_clothing=True,  # ğŸ”¥ ì˜¤ë²„ì‚¬ì´ì¦ˆ ì˜ë¥˜ ì²˜ë¦¬ í™œì„±í™”
                    enable_tight_clothing=True,  # ğŸ”¥ íƒ€ì´íŠ¸ ì˜ë¥˜ ì²˜ë¦¬ í™œì„±í™”
                    enable_adaptive_thresholding=True,  # ğŸ”¥ ì ì‘í˜• ì„ê³„ê°’ í™œì„±í™”
                    enable_context_aware_parsing=True,  # ğŸ”¥ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ íŒŒì‹± í™œì„±í™”
                )
                print(f"âœ… EnhancedHumanParsingConfig ìƒì„± ì™„ë£Œ")
                
                if 'parsing_config' in kwargs:
                    print(f"ğŸ” parsing_config ì²˜ë¦¬ ì‹œì‘")
                    config_dict = kwargs['parsing_config']
                    if isinstance(config_dict, dict):
                        print(f"ğŸ” dict íƒ€ì… parsing_config ì²˜ë¦¬")
                        for key, value in config_dict.items():
                            if hasattr(self.config, key):
                                setattr(self.config, key, value)
                        print(f"âœ… dict íƒ€ì… parsing_config ì²˜ë¦¬ ì™„ë£Œ")
                    elif isinstance(config_dict, EnhancedHumanParsingConfig):
                        print(f"ğŸ” EnhancedHumanParsingConfig íƒ€ì… parsing_config ì²˜ë¦¬")
                        self.config = config_dict
                        print(f"âœ… EnhancedHumanParsingConfig íƒ€ì… parsing_config ì²˜ë¦¬ ì™„ë£Œ")
                print(f"âœ… Human Parsing ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ")
                
                # ğŸ”¥ ê³ ê¸‰ í›„ì²˜ë¦¬ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
                print(f"ğŸ” ê³ ê¸‰ í›„ì²˜ë¦¬ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì‹œì‘")
                self.postprocessor = AdvancedPostProcessor()
                print(f"âœ… ê³ ê¸‰ í›„ì²˜ë¦¬ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
                
                # ğŸ”¥ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìƒˆë¡œ ì¶”ê°€)
                print(f"ğŸ” ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
                self.ensemble_manager = None
                if self.config.enable_ensemble and HUMAN_PARSING_MODULES_AVAILABLE:
                    try:
                        self.ensemble_manager = ModelEnsembleManager(self.config)
                        print(f"âœ… ModelEnsembleManager ìƒì„± ì™„ë£Œ")
                    except Exception as e:
                        print(f"âš ï¸ ModelEnsembleManager ìƒì„± ì‹¤íŒ¨: {e}")
                print(f"âœ… ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                
                # ğŸ”¥ ê³ í•´ìƒë„ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìƒˆë¡œ ì¶”ê°€)
                print(f"ğŸ” ê³ í•´ìƒë„ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
                self.high_resolution_processor = None
                if self.config.enable_high_resolution and HUMAN_PARSING_MODULES_AVAILABLE:
                    try:
                        self.high_resolution_processor = HighResolutionProcessor(self.config)
                        print(f"âœ… HighResolutionProcessor ìƒì„± ì™„ë£Œ")
                    except Exception as e:
                        print(f"âš ï¸ HighResolutionProcessor ìƒì„± ì‹¤íŒ¨: {e}")
                print(f"âœ… ê³ í•´ìƒë„ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                
                # ğŸ”¥ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìƒˆë¡œ ì¶”ê°€)
                print(f"ğŸ” íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
                self.special_case_processor = None
                if self.config.enable_special_case_handling and HUMAN_PARSING_MODULES_AVAILABLE:
                    try:
                        self.special_case_processor = SpecialCaseProcessor(self.config)
                        print(f"âœ… SpecialCaseProcessor ìƒì„± ì™„ë£Œ")
                    except Exception as e:
                        print(f"âš ï¸ SpecialCaseProcessor ìƒì„± ì‹¤íŒ¨: {e}")
                print(f"âœ… íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                
                # ì„±ëŠ¥ í†µê³„ í™•ì¥
                print(f"ğŸ” ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™” ì‹œì‘")
                self.ai_stats = {
                    'total_processed': 0,
                    'preprocessing_time': 0.0,
                    'parsing_time': 0.0,
                    'postprocessing_time': 0.0,
                    'graphonomy_calls': 0,
                    'u2net_calls': 0,
                    'hrnet_calls': 0,
                    'deeplabv3plus_calls': 0,
                    'mask2former_calls': 0,
                    'ensemble_calls': 0,
                    'crf_postprocessing_calls': 0,
                    'multiscale_processing_calls': 0,
                    'edge_refinement_calls': 0,
                    'quality_enhancement_calls': 0,
                    'progressive_parsing_calls': 0,
                    'self_correction_calls': 0,
                    'iterative_refinement_calls': 0,
                    'hybrid_ensemble_calls': 0,
                    'advanced_ensemble_calls': 0,
                    'cross_attention_calls': 0,
                    'uncertainty_quantification_calls': 0,
                    'confidence_calibration_calls': 0,
                    'aspp_module_calls': 0,
                    'self_attention_calls': 0,
                    'average_confidence': 0.0,
                    'ensemble_quality_score': 0.0,
                    'high_resolution_calls': 0,
                    'super_resolution_calls': 0,
                    'noise_reduction_calls': 0,
                    'lighting_normalization_calls': 0,
                    'color_correction_calls': 0,
                    'adaptive_resolution_calls': 0,
                    'special_case_calls': 0,
                    'transparent_clothing_calls': 0,
                    'layered_clothing_calls': 0,
                    'complex_pattern_calls': 0,
                    'reflective_material_calls': 0,
                    'oversized_clothing_calls': 0,
                    'tight_clothing_calls': 0,
                    'total_algorithms_applied': 0
                }
                print(f"âœ… ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™” ì™„ë£Œ")
                
                # ì„±ëŠ¥ ìµœì í™”
                print(f"ğŸ” ThreadPoolExecutor ì´ˆê¸°í™” ì‹œì‘")
                from concurrent.futures import ThreadPoolExecutor
                print(f"âœ… ThreadPoolExecutor import ì„±ê³µ")
                self.executor = ThreadPoolExecutor(
                    max_workers=4 if IS_M3_MAX else 2,
                    thread_name_prefix="human_parsing"
                )
                print(f"âœ… ThreadPoolExecutor ì´ˆê¸°í™” ì™„ë£Œ")
                
                print(f"ğŸ” ë¡œê±° ì •ë³´ ì¶œë ¥ ì‹œì‘")
                self.logger.info(f"âœ… {self.step_name} Central Hub DI Container v7.0 ê¸°ë°˜ ì´ˆê¸°í™” ì™„ë£Œ")
                self.logger.info(f"   - Device: {self.device}")
                self.logger.info(f"   - M3 Max: {IS_M3_MAX}")
                print(f"âœ… ë¡œê±° ì •ë³´ ì¶œë ¥ ì™„ë£Œ")
                
                # ğŸ”¥ AI ëª¨ë¸ ë¡œë”© ì‹œì‘
                print(f"ğŸ” AI ëª¨ë¸ ë¡œë”© ì‹œì‘")
                self.logger.info("ğŸ”„ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
                
                # 1. Central Hubë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”© ì‹œë„
                print(f"ğŸ” Central Hub ëª¨ë¸ ë¡œë”© ì‹œë„")
                central_hub_success = self._load_ai_models_via_central_hub()
                print(f"ğŸ”¥ [DEBUG] Central Hub ëª¨ë¸ ë¡œë”© ê²°ê³¼: {central_hub_success}")
                
                # 2. Central Hub ì‹¤íŒ¨ ì‹œ ì§ì ‘ ë¡œë”© ì‹œë„
                if not central_hub_success:
                    print(f"ğŸ” ì§ì ‘ ëª¨ë¸ ë¡œë”© ì‹œë„")
                    direct_success = self._load_models_directly()
                    print(f"ğŸ”¥ [DEBUG] ì§ì ‘ ëª¨ë¸ ë¡œë”© ê²°ê³¼: {direct_success}")
                    
                    if not direct_success:
                        print(f"ğŸ” í´ë°± ëª¨ë¸ ë¡œë”© ì‹œë„")
                        fallback_success = self._load_fallback_models()
                        print(f"ğŸ”¥ [DEBUG] í´ë°± ëª¨ë¸ ë¡œë”© ê²°ê³¼: {fallback_success}")
                
                print(f"ğŸ”¥ [DEBUG] ìµœì¢… ëª¨ë¸ ë¡œë”© ìƒíƒœ: {self.models_loading_status}")
                print(f"ğŸ”¥ [DEBUG] ë¡œë“œëœ ëª¨ë¸ë“¤: {list(self.loaded_models.keys()) if isinstance(self.loaded_models, dict) else self.loaded_models}")
                print(f"ğŸ”¥ [DEBUG] ai_models í‚¤ë“¤: {list(self.ai_models.keys()) if self.ai_models else 'None'}")
                
                self.logger.info(f"âœ… AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.models_loading_status}")
                print(f"âœ… AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                
                print(f"ğŸ‰ HumanParsingStep __init__ ì™„ë£Œ!")
                
            except Exception as e:
                print(f"âŒ HumanParsingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print(f"âŒ ì˜¤ë¥˜ íƒ€ì…: {type(e)}")
                import traceback
                print(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                self.logger.error(f"âŒ HumanParsingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self._emergency_setup(**kwargs)
        
        def _emergency_setup(self, **kwargs):
            """ê¸´ê¸‰ ì„¤ì • (ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ)"""
            print(f"ğŸ” HumanParsingStep _emergency_setup ì‹œì‘")
            try:
                print(f"ğŸ” step_name ì„¤ì • ì‹œì‘")
                self.step_name = "HumanParsingStep"
                print(f"âœ… step_name ì„¤ì • ì™„ë£Œ")
                
                print(f"ğŸ” step_id ì„¤ì • ì‹œì‘")
                self.step_id = 1
                print(f"âœ… step_id ì„¤ì • ì™„ë£Œ")
                
                print(f"ğŸ” device ì„¤ì • ì‹œì‘")
                self.device = kwargs.get('device', 'cpu')
                print(f"âœ… device ì„¤ì • ì™„ë£Œ: {self.device}")
                
                print(f"ğŸ” ai_models ì„¤ì • ì‹œì‘")
                self.ai_models = {}
                print(f"âœ… ai_models ì„¤ì • ì™„ë£Œ")
                
                print(f"ğŸ” models_loading_status ì„¤ì • ì‹œì‘")
                self.models_loading_status = {'mock': True}
                print(f"âœ… models_loading_status ì„¤ì • ì™„ë£Œ")
                
                print(f"ğŸ” model_interface ì„¤ì • ì‹œì‘")
                self.model_interface = None
                print(f"âœ… model_interface ì„¤ì • ì™„ë£Œ")
                
                print(f"ğŸ” loaded_models ì„¤ì • ì‹œì‘")
                self.loaded_models = {}
                print(f"âœ… loaded_models ì„¤ì • ì™„ë£Œ")
                
                print(f"ğŸ” config ì„¤ì • ì‹œì‘")
                self.config = EnhancedHumanParsingConfig()
                print(f"âœ… config ì„¤ì • ì™„ë£Œ")
                
                print(f"âœ… ê¸´ê¸‰ ì„¤ì • ì™„ë£Œ")
                self.logger.warning("âš ï¸ ê¸´ê¸‰ ì„¤ì • ëª¨ë“œë¡œ ì´ˆê¸°í™”ë¨")
            except Exception as e:
                print(f"âŒ ê¸´ê¸‰ ì„¤ì •ë„ ì‹¤íŒ¨: {e}")
                print(f"âŒ ê¸´ê¸‰ ì„¤ì • ì˜¤ë¥˜ íƒ€ì…: {type(e)}")
                import traceback
                print(f"âŒ ê¸´ê¸‰ ì„¤ì • ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        
        # ==============================================
        # ğŸ”¥ Central Hub DI Container ì—°ë™ ë©”ì„œë“œë“¤
        # ==============================================
        
        def _load_ai_models_via_central_hub(self) -> bool:
            """ğŸ”¥ Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”© (í•„ìˆ˜ êµ¬í˜„)"""
            try:
                self.logger.info("ğŸ”„ Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
                
                # Central Hub DI Container ê°€ì ¸ì˜¤ê¸° (ì•ˆì „í•œ ë°©ë²•)
                container = None
                try:
                    # ì „ì—­ í•¨ìˆ˜ë¡œ ì •ì˜ëœ _get_central_hub_container ì‚¬ìš©
                    container = _get_central_hub_container()
                except NameError:
                    # í•¨ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš° ì•ˆì „í•œ ëŒ€ì•ˆ ì‚¬ìš©
                    try:
                        if hasattr(self, 'central_hub_container'):
                            container = self.central_hub_container
                        elif hasattr(self, 'di_container'):
                            container = self.di_container
                    except Exception:
                        pass
                
                # ModelLoader ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                model_loader = None
                if container:
                    model_loader = container.get('model_loader')
                
                # ğŸ”¥ ModelLoaderê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨ (ì§ì ‘ ë¡œë”© ì œê±°)
                if not model_loader:
                    self.logger.error("âŒ Central Hub ModelLoaderê°€ ì—†ìŠµë‹ˆë‹¤")
                    return False
                
                self.model_interface = model_loader
                self.model_loader = model_loader  # ì§ì ‘ ì°¸ì¡° ì¶”ê°€
                success_count = 0
                
                # 1. Graphonomy ëª¨ë¸ ë¡œë”© ì‹œë„ (1.2GB ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸)
                try:
                    graphonomy_model = self._load_graphonomy_via_central_hub(model_loader)
                    if graphonomy_model:
                        self.ai_models['graphonomy'] = graphonomy_model
                        self.models_loading_status['graphonomy'] = True
                        self.loaded_models['graphonomy'] = graphonomy_model
                        success_count += 1
                        self.logger.info("âœ… Graphonomy ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                    else:
                        self.logger.warning("âš ï¸ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.error(f"âŒ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                    # ëª¨ë¸ ë¡œë”ê°€ ì‹¤íŒ¨í•˜ë©´ ì˜¤ë¥˜ ë°œìƒ
                    raise e
                
                # 2. U2Net í´ë°± ëª¨ë¸ ë¡œë”© ì‹œë„
                try:
                    u2net_model = self._load_u2net_via_central_hub(model_loader)
                    if u2net_model:
                        self.ai_models['u2net'] = u2net_model
                        self.models_loading_status['u2net'] = True
                        self.loaded_models['u2net'] = u2net_model
                        success_count += 1
                        self.logger.info("âœ… U2Net ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                    else:
                        self.logger.warning("âš ï¸ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                # ğŸ”¥ 3. ì•™ìƒë¸” ëª¨ë¸ë“¤ ë¡œë”© ì‹œë„ (ìƒˆë¡œ ì¶”ê°€)
                if self.config.enable_ensemble and self.ensemble_manager:
                    try:
                        ensemble_success = self.ensemble_manager.load_ensemble_models(model_loader)
                        if ensemble_success:
                            self.logger.info("âœ… ì•™ìƒë¸” ëª¨ë¸ë“¤ ë¡œë”© ì„±ê³µ")
                            # ì•™ìƒë¸” ë§¤ë‹ˆì €ì˜ ëª¨ë¸ë“¤ì„ ai_modelsì— ì¶”ê°€
                            for model_name, model in self.ensemble_manager.loaded_models.items():
                                self.ai_models[model_name] = model
                                self.models_loading_status[model_name] = True
                                self.loaded_models[model_name] = model
                                success_count += 1
                        else:
                            self.logger.warning("âš ï¸ ì•™ìƒë¸” ëª¨ë¸ë“¤ ë¡œë”© ì‹¤íŒ¨")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ì•™ìƒë¸” ëª¨ë¸ë“¤ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                # 4. ìµœì†Œ 1ê°œ ëª¨ë¸ì´ë¼ë„ ë¡œë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if success_count > 0:
                    self.logger.info(f"âœ… Central Hub ê¸°ë°˜ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {success_count}ê°œ ëª¨ë¸")
                    return True
                else:
                    self.logger.error("âŒ Central Hub ê¸°ë°˜ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                    return False
                
            except Exception as e:
                self.logger.error(f"âŒ Central Hub ê¸°ë°˜ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return False
        
        def _load_models_directly(self) -> bool:
            """ğŸ”¥ ì§ì ‘ ëª¨ë¸ ë¡œë”© (Central Hub ì‹¤íŒ¨ ì‹œ)"""
            try:
                self.logger.info("ğŸ”„ ì§ì ‘ ëª¨ë¸ ë¡œë”© ì‹œì‘...")
                success_count = 0
                
                # 1. Graphonomy ëª¨ë¸ ì§ì ‘ ë¡œë”©
                try:
                    graphonomy_model = self._load_graphonomy_directly()
                    if graphonomy_model:
                        self.ai_models['graphonomy'] = graphonomy_model
                        self.models_loading_status['graphonomy'] = True
                        self.loaded_models['graphonomy'] = graphonomy_model
                        success_count += 1
                        self.logger.info("âœ… Graphonomy ëª¨ë¸ ì§ì ‘ ë¡œë”© ì„±ê³µ")
                    else:
                        self.logger.warning("âš ï¸ Graphonomy ëª¨ë¸ ì§ì ‘ ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Graphonomy ëª¨ë¸ ì§ì ‘ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                # 2. U2Net ëª¨ë¸ ì§ì ‘ ë¡œë”©
                try:
                    u2net_model = self._load_u2net_directly()
                    if u2net_model:
                        self.ai_models['u2net'] = u2net_model
                        self.models_loading_status['u2net'] = True
                        self.loaded_models['u2net'] = u2net_model
                        success_count += 1
                        self.logger.info("âœ… U2Net ëª¨ë¸ ì§ì ‘ ë¡œë”© ì„±ê³µ")
                    else:
                        self.logger.warning("âš ï¸ U2Net ëª¨ë¸ ì§ì ‘ ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ U2Net ëª¨ë¸ ì§ì ‘ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                # 3. ìµœì†Œ 1ê°œ ëª¨ë¸ì´ë¼ë„ ë¡œë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if success_count > 0:
                    self.logger.info(f"âœ… ì§ì ‘ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {success_count}ê°œ ëª¨ë¸")
                    return True
                else:
                    self.logger.warning("âš ï¸ ëª¨ë“  ì§ì ‘ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - Mock ëª¨ë¸ ì‚¬ìš©")
                    return self._load_fallback_models()
                
            except Exception as e:
                self.logger.error(f"âŒ ì§ì ‘ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return self._load_fallback_models()
        
        def _load_graphonomy_via_central_hub(self, model_loader) -> Optional[nn.Module]:
            """Central Hubë¥¼ í†µí•œ Graphonomy ëª¨ë¸ ë¡œë”©"""
            try:
                # ModelLoaderë¥¼ í†µí•œ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (ìˆ˜ì •ëœ ë°©ì‹)
                loaded_model = model_loader.load_model_for_step(
                    step_type='human_parsing',
                    model_name='human_parsing_schp',
                    checkpoint_path=None
                )
                
                if loaded_model:
                    # RealAIModelì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                    actual_model = loaded_model.get_model_instance()
                    if actual_model is not None:
                        self.logger.info("âœ… Graphonomy ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë¡œë”© ì„±ê³µ")
                        return actual_model
                    else:
                        self.logger.warning("âš ï¸ Graphonomy ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ê°€ None - ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒì„± ì‹œë„")
                        # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ ëª¨ë¸ ìƒì„± ì‹œë„
                        checkpoint_data = loaded_model.get_checkpoint_data()
                        if checkpoint_data is not None:
                            return self._create_graphonomy_from_checkpoint(checkpoint_data)
                        else:
                            self.logger.warning("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ë„ None - ì•„í‚¤í…ì²˜ë§Œ ìƒì„±")
                            return self._create_model('graphonomy')
                else:
                    # í´ë°±: ì•„í‚¤í…ì²˜ë§Œ ìƒì„±
                    self.logger.warning("âš ï¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì•„í‚¤í…ì²˜ë§Œ ìƒì„±")
                    return self._create_model('graphonomy')
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return self._create_model('graphonomy')
        
        def _load_u2net_via_central_hub(self, model_loader) -> Optional[nn.Module]:
            """Central Hubë¥¼ í†µí•œ U2Net ëª¨ë¸ ë¡œë”©"""
            try:
                # U2Net ëª¨ë¸ ìš”ì²­ (ìˆ˜ì •ëœ ë°©ì‹)
                loaded_model = model_loader.load_model_for_step(
                    step_type='human_parsing',
                    model_name='u2net.pth',
                    checkpoint_path=None
                )
                
                if loaded_model:
                    # RealAIModelì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                    actual_model = loaded_model.get_model_instance()
                    if actual_model is not None:
                        self.logger.info("âœ… U2Net ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë¡œë”© ì„±ê³µ")
                        return actual_model
                    else:
                        self.logger.warning("âš ï¸ U2Net ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ê°€ None - ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒì„± ì‹œë„")
                        # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ ëª¨ë¸ ìƒì„± ì‹œë„
                        checkpoint_data = loaded_model.get_checkpoint_data()
                        if checkpoint_data is not None:
                            return self._create_model('u2net', checkpoint_data)
                        else:
                            self.logger.warning("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ë„ None - ì•„í‚¤í…ì²˜ë§Œ ìƒì„±")
                            return self._create_model('u2net')
                else:
                    # í´ë°±: U2Net ì•„í‚¤í…ì²˜ ìƒì„±
                    self.logger.warning("âš ï¸ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì•„í‚¤í…ì²˜ë§Œ ìƒì„±")
                    return self._create_model('u2net')
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return self._create_model('u2net')
        
        def _load_graphonomy_directly(self) -> Optional[nn.Module]:
            """ğŸ”¥ Graphonomy ëª¨ë¸ ì§ì ‘ ë¡œë”© - ëª¨ë“  ê°€ëŠ¥í•œ íŒŒì¼ ì‹œë„"""
            try:
                self.logger.info("ğŸ”„ Graphonomy ëª¨ë¸ ì§ì ‘ ë¡œë”© ì‹œì‘...")
                
                # ê°€ëŠ¥í•œ ëª¨ë¸ ê²½ë¡œë“¤ (ìš°ì„ ìˆœìœ„ ìˆœì„œ) - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë“¤
                model_paths = [
                    # 1. ì‹¤ì œ ì¡´ì¬í•˜ëŠ” Graphonomy ëª¨ë¸ë“¤ (ìš°ì„ ìˆœìœ„)
                    "ai_models/step_01_human_parsing/graphonomy_fixed.pth",      # 267MB - ì‹¤ì œ ì¡´ì¬
                    "ai_models/step_01_human_parsing/graphonomy_new.pth",        # 109MB - ì‹¤ì œ ì¡´ì¬
                    "ai_models/step_01_human_parsing/pytorch_model.bin",         # 109MB - ì‹¤ì œ ì¡´ì¬
                    
                    # 2. Graphonomy ë””ë ‰í† ë¦¬ ëª¨ë¸ë“¤ (ì‹¤ì œ ì¡´ì¬)
                    "ai_models/Graphonomy/inference.pth",                        # 267MB - ì‹¤ì œ ì¡´ì¬
                    "ai_models/Graphonomy/pytorch_model.bin",                    # 109MB - ì‹¤ì œ ì¡´ì¬
                    "ai_models/Graphonomy/model.safetensors",                    # 109MB - ì‹¤ì œ ì¡´ì¬
                    
                    # 3. SCHP ëª¨ë¸ë“¤ (ì‹¤ì œ ì¡´ì¬)
                    "ai_models/human_parsing/schp/pytorch_model.bin",            # 109MB - ì‹¤ì œ ì¡´ì¬
                    "ai_models/Self-Correction-Human-Parsing/exp-schp-201908261155-atr.pth",  # SCHP ATR - ì‹¤ì œ ì¡´ì¬
                    "ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/humanparsing/exp-schp-201908301523-atr.pth",  # SCHP ATR - ì‹¤ì œ ì¡´ì¬
                    
                    # 4. ê¸°íƒ€ Human Parsing ëª¨ë¸ë“¤
                    "ai_models/step_01_human_parsing/deeplabv3plus.pth",         # 244MB - ì‹¤ì œ ì¡´ì¬
                    "ai_models/step_01_human_parsing/ultra_models/deeplab_resnet101.pth",  # ì‹¤ì œ ì¡´ì¬
                ]
                
                for model_path in model_paths:
                    try:
                        if os.path.exists(model_path):
                            self.logger.info(f"ğŸ”„ Graphonomy ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_path}")
                            
                            # íŒŒì¼ í¬ê¸° í™•ì¸
                            file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB
                            self.logger.info(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:.1f}MB")
                            
                            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                            if model_path.endswith('.safetensors'):
                                try:
                                    import safetensors.torch
                                    checkpoint = safetensors.torch.load_file(model_path)
                                    self.logger.info(f"âœ… Safetensors ë¡œë”© ì„±ê³µ: {model_path}")
                                except ImportError:
                                    self.logger.warning(f"âš ï¸ Safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, ê±´ë„ˆëœ€: {model_path}")
                                    continue
                            else:
                                checkpoint = torch.load(model_path, map_location='cpu')
                                self.logger.info(f"âœ… PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ: {model_path}")
                            
                            # ëª¨ë¸ ìƒì„±
                            model = self._create_graphonomy_from_checkpoint(checkpoint)
                            if model:
                                self.logger.info(f"âœ… Graphonomy ëª¨ë¸ ì§ì ‘ ë¡œë”© ì„±ê³µ: {model_path}")
                                return model
                            else:
                                self.logger.warning(f"âš ï¸ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {model_path}")
                            
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ({model_path}): {e}")
                        continue
                
                self.logger.warning("âš ï¸ ëª¨ë“  Graphonomy ëª¨ë¸ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨")
                return None
                
            except Exception as e:
                self.logger.error(f"âŒ Graphonomy ëª¨ë¸ ì§ì ‘ ë¡œë”© ì‹¤íŒ¨: {e}")
                return None
        
        def _load_u2net_directly(self) -> Optional[nn.Module]:
            """ğŸ”¥ U2Net ëª¨ë¸ ì§ì ‘ ë¡œë”© - ëª¨ë“  ê°€ëŠ¥í•œ íŒŒì¼ ì‹œë„"""
            try:
                self.logger.info("ğŸ”„ U2Net ëª¨ë¸ ì§ì ‘ ë¡œë”© ì‹œì‘...")
                
                # ê°€ëŠ¥í•œ ëª¨ë¸ ê²½ë¡œë“¤ (ìš°ì„ ìˆœìœ„ ìˆœì„œ) - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë“¤
                model_paths = [
                    # 1. ì‹¤ì œ ì¡´ì¬í•˜ëŠ” U2Net ëª¨ë¸ë“¤ (ìš°ì„ ìˆœìœ„)
                    "ai_models/step_03_cloth_segmentation/u2net.pth",              # 40MB - ì‹¤ì œ ì¡´ì¬
                    "ai_models/step_03_cloth_segmentation/u2net.pth.1",            # 176MB - ì‹¤ì œ ì¡´ì¬
                    "ai_models/step_03_cloth_segmentation/u2net_official.pth",     # 2.3KB - ì‹¤ì œ ì¡´ì¬
                    
                    # 2. ëŒ€ì•ˆ U2Net ëª¨ë¸ë“¤
                    "ai_models/step_03_cloth_segmentation/mobile_sam.pt",          # 40MB - ì‹¤ì œ ì¡´ì¬
                    "ai_models/step_03_cloth_segmentation/pytorch_model.bin",      # 2.5GB - ì‹¤ì œ ì¡´ì¬
                    "ai_models/step_06_virtual_fitting/u2net_fixed.pth",           # ì‹¤ì œ ì¡´ì¬
                    "ai_models/step_05_cloth_warping/u2net_warping.pth",           # ì‹¤ì œ ì¡´ì¬
                ]
                
                for model_path in model_paths:
                    try:
                        if os.path.exists(model_path):
                            self.logger.info(f"ğŸ”„ U2Net ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_path}")
                            
                            # íŒŒì¼ í¬ê¸° í™•ì¸
                            file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB
                            self.logger.info(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:.1f}MB")
                            
                            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                            checkpoint = torch.load(model_path, map_location='cpu')
                            self.logger.info(f"âœ… U2Net ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ: {model_path}")
                            
                            # ëª¨ë¸ ìƒì„±
                            model = self._create_model('u2net', checkpoint)
                            if model:
                                self.logger.info(f"âœ… U2Net ëª¨ë¸ ì§ì ‘ ë¡œë”© ì„±ê³µ: {model_path}")
                                return model
                            else:
                                self.logger.warning(f"âš ï¸ U2Net ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {model_path}")
                            
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ({model_path}): {e}")
                        continue
                
                self.logger.warning("âš ï¸ ëª¨ë“  U2Net ëª¨ë¸ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨")
                return None
                
            except Exception as e:
                self.logger.error(f"âŒ U2Net ëª¨ë¸ ì§ì ‘ ë¡œë”© ì‹¤íŒ¨: {e}")
                return None
        
        def _load_fallback_models(self) -> bool:
            """í´ë°± ëª¨ë¸ ë¡œë”© (model_architectures.py ì‚¬ìš©)"""
            try:
                self.logger.info("ğŸ”„ model_architectures.py í´ë°± ëª¨ë¸ ë¡œë”©...")
                
                # model_architectures.pyì—ì„œ ëª¨ë¸ë“¤ import ì‹œë„
                try:
                    from app.ai_pipeline.utils.model_architectures import (
                        GraphonomyModel, U2NetModel, HRNetPoseModel
                    )
                    
                    # Graphonomy ëª¨ë¸ ìƒì„±
                    graphonomy_model = GraphonomyModel(num_classes=20)
                    graphonomy_model.checkpoint_path = "model_architectures_graphonomy"
                    graphonomy_model.checkpoint_data = {"graphonomy": True, "model_type": "GraphonomyModel", "source": "model_architectures"}
                    graphonomy_model.memory_usage_mb = 1200.0
                    graphonomy_model.load_time = 1.0
                    
                    self.ai_models['graphonomy'] = graphonomy_model
                    self.models_loading_status['graphonomy'] = True
                    self.loaded_models['graphonomy'] = graphonomy_model
                    self.logger.info("âœ… model_architectures.py GraphonomyModel ë¡œë”© ì„±ê³µ")
                    
                    # U2Net ëª¨ë¸ ìƒì„±
                    u2net_model = U2NetModel(out_channels=1)
                    u2net_model.checkpoint_path = "model_architectures_u2net"
                    u2net_model.checkpoint_data = {"u2net": True, "model_type": "U2NetModel", "source": "model_architectures"}
                    u2net_model.memory_usage_mb = 50.0
                    u2net_model.load_time = 0.5
                    
                    self.ai_models['u2net'] = u2net_model
                    self.models_loading_status['u2net'] = True
                    self.loaded_models['u2net'] = u2net_model
                    self.logger.info("âœ… model_architectures.py U2NetModel ë¡œë”© ì„±ê³µ")
                    
                    return True
                    
                except ImportError as e:
                    self.logger.warning(f"âš ï¸ model_architectures.py import ì‹¤íŒ¨: {e}")
                
                # model_architectures.py ì‹¤íŒ¨ ì‹œ Mock ëª¨ë¸ ìƒì„±
                mock_model = self._create_model('mock')
                if mock_model:
                    self.ai_models['mock'] = mock_model
                    self.models_loading_status['mock'] = True
                    self.loaded_models['mock'] = mock_model
                    self.logger.info("âœ… Mock ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                    return True
                
                return False
                
            except Exception as e:
                self.logger.error(f"âŒ í´ë°± ëª¨ë¸ ë¡œë”©ë„ ì‹¤íŒ¨: {e}")
                return False
        
        # ==============================================
        # ğŸ”¥ ëª¨ë¸ ìƒì„± í—¬í¼ ë©”ì„œë“œë“¤
        # ==============================================
        
        def _create_graphonomy_from_checkpoint(self, checkpoint_data) -> Optional[nn.Module]:
            """ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ Graphonomy ëª¨ë¸ ìƒì„±"""
            try:
                model = AdvancedGraphonomyResNetASPP(num_classes=20)
                
                # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ë¡œë”©
                if isinstance(checkpoint_data, dict):
                    if 'state_dict' in checkpoint_data:
                        state_dict = checkpoint_data['state_dict']
                    elif 'model' in checkpoint_data:
                        state_dict = checkpoint_data['model']
                    else:
                        state_dict = checkpoint_data
                else:
                    state_dict = checkpoint_data
                
                # state_dict ë¡œë”© (strict=Falseë¡œ í˜¸í™˜ì„± ë³´ì¥)
                model.load_state_dict(state_dict, strict=False)
                model.to(self.device)
                model.eval()
                
                return model
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œ Graphonomy ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
                return self._create_model('graphonomy')
        
        def _create_model(self, model_type: str = 'graphonomy', checkpoint_data=None, device=None, **kwargs) -> nn.Module:
            """í†µí•© ëª¨ë¸ ìƒì„± í•¨ìˆ˜ (ì²´í¬í¬ì¸íŠ¸ ì§€ì›)"""
            try:
                if device is None:
                    device = self.device
                
                self.logger.info(f"ğŸ”¥ [DEBUG] _create_model() ì§„ì… - model_type: {model_type}")
                self.logger.info(f"ğŸ”„ {model_type} ëª¨ë¸ ìƒì„± ì¤‘...")
                
                # ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ” ê²½ìš° ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒì„±
                if checkpoint_data is not None:
                    try:
                        # AdvancedGraphonomyResNetASPP í´ë˜ìŠ¤ ì§ì ‘ ì‚¬ìš© (ì´ì œ íŒŒì¼ ë‚´ì— ì •ì˜ë¨)
                        model = AdvancedGraphonomyResNetASPP(num_classes=20)
                        
                        # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë¡œë“œ
                        if hasattr(model, 'load_state_dict'):
                            # ì²´í¬í¬ì¸íŠ¸ í‚¤ ë§¤í•‘ (ì¶œë ¥ ì œê±°)
                            mapped_checkpoint = self._map_checkpoint_keys(checkpoint_data)
                            model.load_state_dict(mapped_checkpoint, strict=False)
                        
                        model.to(device)
                        model.eval()
                        model.checkpoint_data = checkpoint_data
                        model.get_checkpoint_data = lambda: checkpoint_data
                        model.has_model = True
                        
                        self.logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ìƒì„± ì„±ê³µ")
                        return model
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                # ëª¨ë¸ íƒ€ì…ë³„ ìƒì„± (í´ë°±)
                if model_type == 'graphonomy':
                    # AdvancedGraphonomyResNetASPP í´ë˜ìŠ¤ ì§ì ‘ ì‚¬ìš© (ì´ì œ íŒŒì¼ ë‚´ì— ì •ì˜ë¨)
                    model = AdvancedGraphonomyResNetASPP(num_classes=20)
                    model.checkpoint_path = "fallback_graphonomy_model"
                    model.checkpoint_data = {"graphonomy": True, "fallback": True, "model_type": "AdvancedGraphonomyResNetASPP"}
                    model.memory_usage_mb = 1200.0
                    model.load_time = 2.5
                elif model_type == 'u2net':
                    if MODEL_CLASSES_AVAILABLE and 'U2NetForParsing' in globals():
                        model = U2NetForParsing(num_classes=20)
                    else:
                        self.logger.warning("âš ï¸ U2NetForParsing í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ, Mock ëª¨ë¸ ì‚¬ìš©")
                        model = MockHumanParsingModel(num_classes=20)
                    model.checkpoint_path = "u2net_model"
                    model.checkpoint_data = {"u2net": True, "model_type": "U2NetForParsing"}
                    model.memory_usage_mb = 50.0
                    model.load_time = 1.0
                elif model_type == 'mock':
                    self.logger.info("ğŸ”¥ [DEBUG] Mock ëª¨ë¸ ìƒì„± ì‹œì‘")
                    model = MockHumanParsingModel(num_classes=20)
                    model.checkpoint_path = "fallback_mock_model"
                    model.checkpoint_data = {"mock": True, "fallback": True, "model_type": "MockHumanParsingModel"}
                    model.memory_usage_mb = 0.1
                    model.load_time = 0.1
                    self.logger.info("ğŸ”¥ [DEBUG] Mock ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
                
                # ê³µí†µ ì„¤ì •
                model.to(device)
                model.eval()
                model.get_checkpoint_data = lambda: model.checkpoint_data
                model.has_model = True
                
                self.logger.info(f"âœ… {model_type} ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                return model
                
            except Exception as e:
                self.logger.error(f"âŒ {model_type} ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
                # ìµœì¢… í´ë°±: Mock ëª¨ë¸ (ë¬´í•œ ì¬ê·€ ë°©ì§€)
                if model_type != 'mock':
                    return self._create_model('mock', device=device)
                else:
                    # Mock ëª¨ë¸ë„ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜
                    self.logger.warning("âš ï¸ Mock ëª¨ë¸ ìƒì„±ë„ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜")
                    return MockHumanParsingModel(num_classes=20)
       
       
        # ==============================================
        # ğŸ”¥ ì•ˆì „í•œ ë³€í™˜ ë©”ì„œë“œë“¤
        # ==============================================
        
        def _safe_tensor_to_scalar(self, tensor_value):
            """í…ì„œë¥¼ ì•ˆì „í•˜ê²Œ ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ"""
            try:
                if isinstance(tensor_value, torch.Tensor):
                    if tensor_value.numel() == 1:
                        return tensor_value.item()
                    else:
                        # í…ì„œì˜ í‰ê· ê°’ ì‚¬ìš©
                        return tensor_value.mean().item()
                else:
                    return float(tensor_value)
            except Exception as e:
                self.logger.warning(f"âš ï¸ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
                return 0.8  # ê¸°ë³¸ê°’

        def _safe_extract_tensor_from_list(self, data_list):
            """ë¦¬ìŠ¤íŠ¸ì—ì„œ ì•ˆì „í•˜ê²Œ í…ì„œë¥¼ ì¶”ì¶œí•˜ëŠ” ë©”ì„œë“œ"""
            try:
                if not isinstance(data_list, list) or len(data_list) == 0:
                    return None
                
                first_element = data_list[0]
                
                # ì§ì ‘ í…ì„œì¸ ê²½ìš°
                if isinstance(first_element, torch.Tensor):
                    return first_element
                
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° í…ì„œ ì°¾ê¸°
                elif isinstance(first_element, dict):
                    # ğŸ”¥ ìš°ì„ ìˆœìœ„ í‚¤ ìˆœì„œë¡œ í…ì„œ ì°¾ê¸°
                    priority_keys = ['parsing_pred', 'parsing_output', 'output', 'parsing']
                    for key in priority_keys:
                        if key in first_element and isinstance(first_element[key], torch.Tensor):
                            return first_element[key]
                    
                    # ğŸ”¥ ëª¨ë“  ê°’ì—ì„œ í…ì„œ ì°¾ê¸°
                    for key, value in first_element.items():
                        if isinstance(value, torch.Tensor):
                            return value
                
                return None
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ í…ì„œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                return None

        def _safe_convert_to_numpy(self, data):
            """ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ"""
            try:
                if isinstance(data, np.ndarray):
                    return data
                elif isinstance(data, torch.Tensor):
                    # ğŸ”¥ ê·¸ë˜ë””ì–¸íŠ¸ ë¬¸ì œ í•´ê²°: detach() ì‚¬ìš©
                    return data.detach().cpu().numpy()
                elif isinstance(data, list):
                    tensor = self._safe_extract_tensor_from_list(data)
                    if tensor is not None:
                        return tensor.detach().cpu().numpy()
                elif isinstance(data, dict):
                    for key in ['parsing', 'parsing_pred', 'output', 'parsing_output']:
                        if key in data and isinstance(data[key], torch.Tensor):
                            return data[key].detach().cpu().numpy()
                
                # ê¸°ë³¸ê°’ ë°˜í™˜
                return np.zeros((512, 512), dtype=np.uint8)
            except Exception as e:
                self.logger.warning(f"âš ï¸ NumPy ë³€í™˜ ì‹¤íŒ¨: {e}")
                return np.zeros((512, 512), dtype=np.uint8)

        # ğŸ”¥ í•µì‹¬: _run_ai_inference() ë©”ì„œë“œ (BaseStepMixin ìš”êµ¬ì‚¬í•­)
        # ==============================================
        def _run_ai_inference(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
            """ï¿½ï¿½ M3 Max ìµœì í™” ê³ ë„í™”ëœ AI ì•™ìƒë¸” ì¸ì²´ íŒŒì‹± ì¶”ë¡  ì‹œìŠ¤í…œ"""
            print(f"ğŸ”¥ [M3 Max ìµœì í™” AI] _run_ai_inference() ì§„ì…!")
            
            # ï¿½ï¿½ ë””ë°”ì´ìŠ¤ ì„¤ì •
            device = 'mps:0' if torch.backends.mps.is_available() else 'cpu'
            device_str = str(device)
            self.device = device
            self.device_str = device_str
            
            try:
                # ğŸ”¥ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
                if self.config and self.config.enable_memory_monitoring:
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None
                    print(f"ğŸ”¥ [ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§] M3 Max ë©”ëª¨ë¦¬ ìµœì í™” ë ˆë²¨: {self.config.memory_optimization_level}")
                    print(f"ğŸ”¥ [ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§] ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {self.config.max_memory_usage_gb}GB")
                
                self.logger.info("ğŸš€ M3 Max ìµœì í™” AI ì•™ìƒë¸” ì¸ì²´ íŒŒì‹± ì‹œì‘")
                self.logger.info(f"ğŸ”¥ [DEBUG] self.ai_models ìƒíƒœ: {list(self.ai_models.keys()) if self.ai_models else 'None'}")
                self.logger.info(f"ğŸ”¥ [DEBUG] self.models_loading_status: {self.models_loading_status}")
                start_time = time.time()
                
                # ï¿½ï¿½ 1. ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì´ë¯¸ì§€ ì¶”ì¶œ
                print(f"ğŸ”¥ [ë””ë²„ê¹…] 1ë‹¨ê³„: ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œì‘")
                image = self._extract_input_image(input_data)
                if image is None:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨ - í´ë°±ìœ¼ë¡œ ì´ë™")
                    raise ValueError("ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] âœ… ì´ë¯¸ì§€ ì¶”ì¶œ ì„±ê³µ: {type(image)}, shape={getattr(image, 'shape', 'N/A')}")
                
                # ğŸ”¥ 2. ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
                ensemble_results = {}
                model_confidences = {}
                use_ensemble = False
                
                # ğŸ”¥ 3. ê³ í•´ìƒë„ ì²˜ë¦¬ (ì•ˆì „í•œ ë°©ì‹)
                try:
                    if self.config.enable_high_resolution and self.high_resolution_processor:
                        # ì´ë¯¸ì§€ëŠ” ì´ë¯¸ NumPy ë°°ì—´ì´ë¯€ë¡œ ì§ì ‘ ì‚¬ìš©
                        if isinstance(image, np.ndarray):
                            # ê³ í•´ìƒë„ ì²˜ë¦¬
                            enhanced_result = self.high_resolution_processor.process(image)
                            if isinstance(enhanced_result, dict) and 'processed_image' in enhanced_result:
                                image = enhanced_result['processed_image']
                            else:
                                image = enhanced_result
                            self.logger.info("âœ… ê³ í•´ìƒë„ ì²˜ë¦¬ ì™„ë£Œ")
                        else:
                            self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ê°€ NumPy ë°°ì—´ì´ ì•„ë‹˜: {type(image)}")
                            
                except Exception as hr_error:
                    self.logger.warning(f"âš ï¸ ê³ í•´ìƒë„ ì²˜ë¦¬ ì‹¤íŒ¨: {hr_error}")
                    # ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©
                
                # ï¿½ï¿½ 3. ì•™ìƒë¸” ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡ 
                print(f"ğŸ”¥ [ë””ë²„ê¹…] 3ë‹¨ê³„: ì•™ìƒë¸” ì‹œìŠ¤í…œ ì‹œì‘")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] config.enable_ensemble: {getattr(self.config, 'enable_ensemble', 'N/A')}")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] ensemble_manager ì¡´ì¬: {self.ensemble_manager is not None}")
                
                if self.config.enable_ensemble and self.ensemble_manager:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] âœ… ì•™ìƒë¸” ì‹œìŠ¤í…œ í™œì„±í™”")
                    self.logger.info("ğŸ”¥ ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” ì‹œìŠ¤í…œ í™œì„±í™”")
                    
                    try:
                        # ì•™ìƒë¸” ëª¨ë¸ë“¤ ë¡œë”©
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] ì•™ìƒë¸” ëª¨ë¸ ë¡œë”© ì‹œì‘")
                        ensemble_success = self.ensemble_manager.load_ensemble_models(self.model_loader)
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] ì•™ìƒë¸” ëª¨ë¸ ë¡œë”© ê²°ê³¼: {ensemble_success}")
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] ë¡œë“œëœ ëª¨ë¸ ìˆ˜: {len(self.ensemble_manager.loaded_models) if hasattr(self.ensemble_manager, 'loaded_models') else 'N/A'}")
                        
                        if ensemble_success and len(self.ensemble_manager.loaded_models) >= 2:
                            available_models = self.ensemble_manager.loaded_models
                            
                            # ğŸ”¥ ê° ëª¨ë¸ë³„ ì¶”ë¡  ì‹¤í–‰
                            for model_name, model in available_models.items():
                                try:
                                    self.logger.info(f"ï¿½ï¿½ {model_name} ëª¨ë¸ ì¶”ë¡  ì‹œì‘")
                                    
                                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                                    processed_input = self._preprocess_image_for_model(image, model_name)
                                    
                                    # ğŸ”¥ ëª¨ë¸ë³„ ì•ˆì „ ì¶”ë¡  ì‹¤í–‰
                                    if model_name == 'graphonomy':
                                        result = self._run_graphonomy_safe_inference(processed_input, model, device_str)
                                    elif model_name == 'hrnet':
                                        result = self._run_hrnet_safe_inference(processed_input, model, device_str)
                                    elif model_name == 'deeplabv3plus':
                                        result = self._run_deeplabv3plus_safe_inference(processed_input, model, device_str)
                                    elif model_name == 'u2net':
                                        result = self._run_u2net_safe_inference(processed_input, model, device_str)
                                    else:
                                        result = self._run_generic_safe_inference(processed_input, model, device_str)
                                    
                                    # ğŸ”¥ ê²°ê³¼ ìœ íš¨ì„± ê²€ì¦
                                    if result and 'parsing_output' in result and result['parsing_output'] is not None:
                                        ensemble_results[model_name] = result['parsing_output']
                                        model_confidences[model_name] = result.get('confidence', 0.8)
                                        confidence_value = self._safe_tensor_to_scalar(result.get('confidence', 0.8))
                                        self.logger.info(f"âœ… {model_name} ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ (ì‹ ë¢°ë„: {confidence_value:.3f})")
                                    else:
                                        self.logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                                        continue
                                    
                                except Exception as e:
                                    self.logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                                    continue
                            
                            # ğŸ”¥ ì•™ìƒë¸” ìœµí•© ì‹¤í–‰
                            if len(ensemble_results) >= 2:
                                self.logger.info("ğŸ”¥ ê³ ê¸‰ ì•™ìƒë¸” ìœµí•© ì‹œìŠ¤í…œ ì‹¤í–‰")
                                
                                try:
                                    # ëª¨ë¸ ì¶œë ¥ë“¤ì„ í…ì„œë¡œ ë³€í™˜
                                    model_outputs_list = []
                                    for model_name, output in ensemble_results.items():
                                        if isinstance(output, dict):
                                            if 'parsing_output' in output:
                                                model_outputs_list.append(output['parsing_output'])
                                            else:
                                                # ì²« ë²ˆì§¸ í…ì„œ ê°’ ì°¾ê¸°
                                                for key, value in output.items():
                                                    if isinstance(value, torch.Tensor):
                                                        model_outputs_list.append(value)
                                                        break
                                        else:
                                            model_outputs_list.append(output)
                                    
                                    # ê° ëª¨ë¸ ì¶œë ¥ì˜ ì±„ë„ ìˆ˜ë¥¼ 20ê°œë¡œ í†µì¼ (MPS íƒ€ì… ì¼ê´€ì„± ìœ ì§€)
                                    standardized_outputs = []
                                    for output in model_outputs_list:
                                        # MPS íƒ€ì… ì¼ê´€ì„± í™•ì¸ ë° ìˆ˜ì •
                                        if hasattr(output, 'device') and str(output.device).startswith('mps'):
                                            # MPS ë””ë°”ì´ìŠ¤ì˜ ê²½ìš° float32ë¡œ í†µì¼
                                            if output.dtype != torch.float32:
                                                output = output.to(torch.float32)
                                        else:
                                            # CPU ë””ë°”ì´ìŠ¤ì˜ ê²½ìš° float32ë¡œ í†µì¼
                                            output = output.to(torch.float32)
                                        
                                        if output.shape[1] != 20:
                                            if output.shape[1] > 20:
                                                output = output[:, :20, :, :]
                                            else:
                                                padding = torch.zeros(
                                                    output.shape[0], 
                                                    20 - output.shape[1], 
                                                    output.shape[2], 
                                                    output.shape[3],
                                                    device=output.device,
                                                    dtype=torch.float32  # ëª…ì‹œì ìœ¼ë¡œ float32 ì‚¬ìš©
                                                )
                                                output = torch.cat([output, padding], dim=1)
                                        standardized_outputs.append(output)
                                    
                                    # ì•™ìƒë¸” ìœµí•© ì‹¤í–‰
                                    ensemble_fusion = MemoryEfficientEnsembleSystem(
                                        num_classes=20,
                                        ensemble_models=list(ensemble_results.keys()),
                                        hidden_dim=128,
                                        config=self.config
                                    )
                                    
                                    ensemble_output = ensemble_fusion(
                                        standardized_outputs,
                                        list(model_confidences.values())
                                    )
                                    
                                    # ensemble_outputì´ dictì¸ ê²½ìš° í‚¤ ì¶”ì¶œ
                                    if isinstance(ensemble_output, dict):
                                        if 'ensemble_output' in ensemble_output:
                                            ensemble_output = ensemble_output['ensemble_output']
                                        elif 'final_output' in ensemble_output:
                                            ensemble_output = ensemble_output['final_output']
                                    
                                    # ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
                                    uncertainty = self._calculate_ensemble_uncertainty(ensemble_results)
                                    
                                    # ì‹ ë¢°ë„ ë³´ì •
                                    calibrated_confidence = self._calibrate_ensemble_confidence(
                                        model_confidences, uncertainty
                                    )
                                    
                                    parsing_output = ensemble_output
                                    confidence = calibrated_confidence
                                    use_ensemble = True
                                    
                                    self.logger.info(f"âœ… ì•™ìƒë¸” ìœµí•© ì™„ë£Œ (ëª¨ë¸ ìˆ˜: {len(ensemble_results)})")
                                    
                                except Exception as e:
                                    self.logger.warning(f"âš ï¸ ì•™ìƒë¸” ìœµí•© ì‹¤íŒ¨: {e}")
                                    # í´ë°±: ì²« ë²ˆì§¸ ëª¨ë¸ ì¶œë ¥ ì‚¬ìš©
                                    parsing_output = list(ensemble_results.values())[0]
                                    if isinstance(parsing_output, dict):
                                        parsing_output = parsing_output.get('parsing_output', parsing_output)
                                    confidence = list(model_confidences.values())[0]
                                    use_ensemble = False
                            else:
                                self.logger.warning("âš ï¸ ì•™ìƒë¸” ëª¨ë¸ ë¶€ì¡±, ë‹¨ì¼ ëª¨ë¸ë¡œ í´ë°±")
                                use_ensemble = False
                        else:
                            self.logger.warning("âš ï¸ ì•™ìƒë¸” ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, ë‹¨ì¼ ëª¨ë¸ë¡œ í´ë°±")
                            use_ensemble = False
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                        use_ensemble = False
                
                # ï¿½ï¿½ 4. ë‹¨ì¼ ëª¨ë¸ ì¶”ë¡  (ì•™ìƒë¸” ì‹¤íŒ¨ ì‹œ)
                if not use_ensemble:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] 4ë‹¨ê³„: ë‹¨ì¼ ëª¨ë¸ ì¶”ë¡  ì‹œì‘ (ì•™ìƒë¸” ì‹¤íŒ¨)")
                    self.logger.info("ğŸ”„ ë‹¨ì¼ ëª¨ë¸ ì¶”ë¡  ì‹œì‘")
                    
                    # ğŸ”¥ ì‹¤ì œ ë¡œë”©ëœ ëª¨ë¸ë“¤ ì‚¬ìš©
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤: {list(self.ai_models.keys()) if self.ai_models else 'None'}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] graphonomy ëª¨ë¸ ì¡´ì¬: {'graphonomy' in self.ai_models if self.ai_models else False}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] graphonomy ëª¨ë¸ ê°’: {self.ai_models.get('graphonomy') if self.ai_models else 'None'}")
                    
                    if 'graphonomy' in self.ai_models and self.ai_models['graphonomy'] is not None:
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] âœ… Graphonomy ëª¨ë¸ ì‚¬ìš© ì‹œì‘")
                        self.logger.info("âœ… ì‹¤ì œ ë¡œë”©ëœ Graphonomy ëª¨ë¸ ì‚¬ìš©")
                        graphonomy_model = self.ai_models['graphonomy']
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] Graphonomy ëª¨ë¸ íƒ€ì…: {type(graphonomy_model)}")
                        
                        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹œì‘")
                        processed_input = self._preprocess_image(image, device_str)
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] ì „ì²˜ë¦¬ëœ ì…ë ¥ íƒ€ì…: {type(processed_input)}")
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] ì „ì²˜ë¦¬ëœ ì…ë ¥ shape: {getattr(processed_input, 'shape', 'N/A')}")
                        
                        # ëª¨ë¸ ì¶”ë¡ 
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] Graphonomy ì¶”ë¡  ì‹œì‘")
                        with torch.no_grad():
                            try:
                                print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_actual_graphonomy_inference í˜¸ì¶œ")
                                inference_result = self._run_actual_graphonomy_inference(
                                    processed_input, 
                                    device_str
                                )
                                print(f"ğŸ”¥ [ë””ë²„ê¹…] Graphonomy ì¶”ë¡  ê²°ê³¼ íƒ€ì…: {type(inference_result)}")
                                print(f"ğŸ”¥ [ë””ë²„ê¹…] Graphonomy ì¶”ë¡  ê²°ê³¼ í‚¤ë“¤: {list(inference_result.keys()) if isinstance(inference_result, dict) else 'Not a dict'}")
                                
                                # ê²°ê³¼ì—ì„œ parsing_pred ì¶”ì¶œ
                                print(f"ğŸ”¥ [ë””ë²„ê¹…] ê²°ê³¼ ì¶”ì¶œ ì‹œì‘")
                                if isinstance(inference_result, dict):
                                    print(f"ğŸ”¥ [ë””ë²„ê¹…] ì¶”ë¡  ê²°ê³¼ê°€ dict íƒ€ì…")
                                    parsing_output = inference_result.get('parsing_pred')
                                    confidence = inference_result.get('confidence', 0.8)
                                    print(f"ğŸ”¥ [ë””ë²„ê¹…] parsing_pred ì¶”ì¶œ: {parsing_output is not None}")
                                    print(f"ğŸ”¥ [ë””ë²„ê¹…] confidence ì¶”ì¶œ: {confidence}")
                                    
                                    if parsing_output is None:
                                        print(f"ğŸ”¥ [ë””ë²„ê¹…] parsing_predê°€ None - parsing_probs ì‹œë„")
                                        parsing_probs = inference_result.get('parsing_probs')
                                        print(f"ğŸ”¥ [ë””ë²„ê¹…] parsing_probs ì¡´ì¬: {parsing_probs is not None}")
                                        if parsing_probs is not None:
                                            parsing_output = torch.argmax(parsing_probs, dim=1)
                                            print(f"ğŸ”¥ [ë””ë²„ê¹…] argmax ì ìš© í›„ parsing_output: {parsing_output is not None}")
                                else:
                                    print(f"ğŸ”¥ [ë””ë²„ê¹…] ì¶”ë¡  ê²°ê³¼ê°€ dictê°€ ì•„ë‹˜: {type(inference_result)}")
                                    parsing_output = inference_result
                                    confidence = 0.8
                                
                                print(f"ğŸ”¥ [ë””ë²„ê¹…] ìµœì¢… parsing_output íƒ€ì…: {type(parsing_output)}")
                                print(f"ğŸ”¥ [ë””ë²„ê¹…] ìµœì¢… parsing_output shape: {getattr(parsing_output, 'shape', 'N/A') if parsing_output is not None else 'None'}")
                                self.logger.info(f"âœ… Graphonomy ì¶”ë¡  ì™„ë£Œ: {type(parsing_output)}")
                                
                            except Exception as e:
                                print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {e}")
                                self.logger.warning(f"âš ï¸ Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {e}")
                                print(f"ğŸ”¥ [ë””ë²„ê¹…] í´ë°± íŒŒì‹± ìƒì„± ì‹œì‘")
                                parsing_output = self._create_fallback_parsing(image)
                                confidence = 0.5
                                print(f"ğŸ”¥ [ë””ë²„ê¹…] í´ë°± íŒŒì‹± ìƒì„± ì™„ë£Œ")
                                
                    elif 'u2net' in self.ai_models and self.ai_models['u2net'] is not None:
                        self.logger.info("âœ… ì‹¤ì œ ë¡œë”©ëœ U2Net ëª¨ë¸ ì‚¬ìš©")
                        u2net_model = self.ai_models['u2net']
                        
                        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                        processed_input = self._preprocess_image(image, device_str)
                        
                        # ëª¨ë¸ ì¶”ë¡ 
                        with torch.no_grad():
                            try:
                                inference_result = self._run_u2net_ensemble_inference(
                                    processed_input, 
                                    u2net_model
                                )
                                
                                confidence = inference_result.get('confidence', 0.8)
                                parsing_output = inference_result.get('parsing_output', inference_result)
                                
                                self.logger.info(f"âœ… U2Net ì¶”ë¡  ì™„ë£Œ: {type(parsing_output)}")
                                
                            except Exception as e:
                                self.logger.warning(f"âš ï¸ U2Net ì¶”ë¡  ì‹¤íŒ¨: {e}")
                                parsing_output = self._create_fallback_parsing(image)
                                confidence = 0.5
                    else:
                        # í´ë°±: Mock ëª¨ë¸ ì‚¬ìš©
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ ì‹¤ì œ ë¡œë”©ëœ ëª¨ë¸ ì—†ìŒ - í´ë°± ìƒì„±")
                        self.logger.warning("âš ï¸ ì‹¤ì œ ë¡œë”©ëœ ëª¨ë¸ ì—†ìŒ - í´ë°± ìƒì„±")
                        parsing_output = self._create_fallback_parsing(image)
                        confidence = 0.5
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] í´ë°± íŒŒì‹± ìƒì„± ì™„ë£Œ")
                
                # ğŸ”¥ 5. ê²°ê³¼ ê²€ì¦ ë° í‘œì¤€í™”
                print(f"ğŸ”¥ [ë””ë²„ê¹…] 5ë‹¨ê³„: ê²°ê³¼ ê²€ì¦ ì‹œì‘")
                print(f"ğŸ”¥ [ë””ë²„ê¹…] parsing_output ì¡´ì¬: {parsing_output is not None}")
                if parsing_output is None:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] âŒ ì¶”ë¡  ê²°ê³¼ê°€ None - í´ë°± ìƒì„±")
                    self.logger.warning("âš ï¸ ì¶”ë¡  ê²°ê³¼ê°€ None - í´ë°± ìƒì„±")
                    parsing_output = self._create_fallback_parsing(image)
                    confidence = 0.5
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] None í´ë°± íŒŒì‹± ìƒì„± ì™„ë£Œ")
                
                # ğŸ”¥ 6. í…ì„œë¥¼ NumPyë¡œ ë³€í™˜ (gradient ë¬¸ì œ í•´ê²°)
                if isinstance(parsing_output, torch.Tensor):
                    parsing_output_np = parsing_output.detach().cpu().numpy()
                    self.logger.info(f"âœ… í…ì„œ ë³€í™˜ ì™„ë£Œ: {parsing_output.shape} -> {parsing_output_np.shape}")
                else:
                    parsing_output_np = parsing_output
                    self.logger.info(f"âœ… ì§ì ‘ NumPy ì‚¬ìš©: {type(parsing_output_np)}")
                
                # ğŸ”¥ 7. ì°¨ì› ê²€ì¦ ë° ìˆ˜ì •
                self.logger.info(f"ğŸ” ì°¨ì› ê²€ì¦ ì‹œì‘: {parsing_output_np.shape}, ndim: {parsing_output_np.ndim}")
                
                if parsing_output_np.ndim == 3 and parsing_output_np.shape[0] == 1:
                    # (1, H, W) -> (H, W)
                    parsing_output_np = parsing_output_np[0]
                    self.logger.info(f"âœ… 3D í…ì„œ ì²˜ë¦¬: (1, H, W) -> (H, W) = {parsing_output_np.shape}")
                elif parsing_output_np.ndim == 4 and parsing_output_np.shape[0] == 1:
                    # (1, C, H, W) -> (H, W) ë˜ëŠ” (C, H, W)
                    if parsing_output_np.shape[1] == 20:
                        # 20ê°œ í´ë˜ìŠ¤ì¸ ê²½ìš° argmax ì ìš©
                        parsing_output_np = np.argmax(parsing_output_np[0], axis=0)
                        self.logger.info(f"âœ… 4D í…ì„œ ì²˜ë¦¬ (20í´ë˜ìŠ¤): argmax ì ìš© -> {parsing_output_np.shape}")
                    else:
                        parsing_output_np = parsing_output_np[0, 0]  # ì²« ë²ˆì§¸ ì±„ë„
                        self.logger.info(f"âœ… 4D í…ì„œ ì²˜ë¦¬ (ê¸°íƒ€): ì²« ë²ˆì§¸ ì±„ë„ -> {parsing_output_np.shape}")
                else:
                    self.logger.info(f"âœ… ì°¨ì› ë³€ê²½ ì—†ìŒ: {parsing_output_np.shape}")
                
                # ğŸ”¥ 8. ìµœì¢… ê²€ì¦
                self.logger.info(f"ğŸ” ìµœì¢… ê²€ì¦: íƒ€ì…={type(parsing_output_np)}, shape={getattr(parsing_output_np, 'shape', 'N/A')}")
                
                if not isinstance(parsing_output_np, np.ndarray):
                    self.logger.warning("âš ï¸ NumPy ë°°ì—´ ë³€í™˜ ì‹¤íŒ¨ - ê¸°ë³¸ê°’ ìƒì„±")
                    parsing_output_np = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)
                else:
                    self.logger.info(f"âœ… ìµœì¢… íŒŒì‹± ë§µ: shape={parsing_output_np.shape}, dtype={parsing_output_np.dtype}")
                    unique_values = np.unique(parsing_output_np)
                    self.logger.info(f"âœ… ê³ ìœ  ê°’ë“¤: {unique_values}")
                    self.logger.info(f"âœ… ê°’ ë²”ìœ„: {parsing_output_np.min()} ~ {parsing_output_np.max()}")
                    
                    # unique_labelsê°€ 0ì¸ ê²½ìš° ìƒì„¸ ë¶„ì„
                    if len(unique_values) == 0 or (len(unique_values) == 1 and unique_values[0] == 0):
                        self.logger.warning("âš ï¸ unique_labelsê°€ 0ì…ë‹ˆë‹¤! ìƒì„¸ ë¶„ì„:")
                        self.logger.warning(f"âš ï¸ íŒŒì‹± ë§µ ì „ì²´ê°€ 0ì¸ì§€ í™•ì¸: {np.all(parsing_output_np == 0)}")
                        self.logger.warning(f"âš ï¸ íŒŒì‹± ë§µ ì „ì²´ê°€ ê°™ì€ ê°’ì¸ì§€ í™•ì¸: {np.all(parsing_output_np == parsing_output_np[0, 0])}")
                        self.logger.warning(f"âš ï¸ íŒŒì‹± ë§µ í†µê³„: mean={parsing_output_np.mean():.4f}, std={parsing_output_np.std():.4f}")
                        
                        # ğŸ”¥ ê°•í™”ëœ ê¸°ë³¸ê°’ ì„¤ì • (ì‹¤ì œ ì¸ì²´ ê°ì§€ ê¸°ë°˜)
                        if np.all(parsing_output_np == 0):
                            self.logger.warning("âš ï¸ íŒŒì‹± ë§µì´ ëª¨ë‘ 0ì…ë‹ˆë‹¤. ê°•í™”ëœ ê¸°ë³¸ê°’ìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.")
                            
                            # ğŸ”¥ 1ë‹¨ê³„: ì´ë¯¸ì§€ì—ì„œ ì¸ì²´ ì˜ì—­ ê°ì§€
                            try:
                                # HSV ìƒ‰ìƒ ê³µê°„ì—ì„œ í”¼ë¶€ìƒ‰ ê°ì§€
                                hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                                
                                # í”¼ë¶€ìƒ‰ ë²”ìœ„ (ë” ë„“ì€ ë²”ìœ„)
                                lower_skin = np.array([0, 10, 60], dtype=np.uint8)
                                upper_skin = np.array([25, 255, 255], dtype=np.uint8)
                                
                                # í”¼ë¶€ìƒ‰ ë§ˆìŠ¤í¬ ìƒì„±
                                skin_mask = cv2.inRange(hsv, lower_skin, upper_skin)
                                
                                # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
                                kernel = np.ones((5, 5), np.uint8)
                                skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_CLOSE, kernel)
                                skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_OPEN, kernel)
                                
                                # ê°€ì¥ í° ì—°ê²° ìš”ì†Œ ì°¾ê¸°
                                contours, _ = cv2.findContours(skin_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                                
                                if contours:
                                    # ê°€ì¥ í° ì»¨íˆ¬ì–´ ì„ íƒ
                                    largest_contour = max(contours, key=cv2.contourArea)
                                    
                                    # ì»¨íˆ¬ì–´ ë‚´ë¶€ë¥¼ 1ë¡œ ì±„ì›€ (ì¸ì²´ë¡œ ê°„ì£¼)
                                    cv2.fillPoly(parsing_output_np, [largest_contour], 1)
                                    
                                    self.logger.info(f"âœ… í”¼ë¶€ìƒ‰ ê¸°ë°˜ ì¸ì²´ ê°ì§€ë¡œ ê¸°ë³¸ê°’ ì„¤ì •: {np.unique(parsing_output_np)}")
                                else:
                                    # í”¼ë¶€ìƒ‰ ê°ì§€ ì‹¤íŒ¨ ì‹œ ì¤‘ì•™ ì˜ì—­ ì„¤ì •
                                    h, w = parsing_output_np.shape
                                    center_h, center_w = h // 2, w // 2
                                    parsing_output_np[center_h-50:center_h+50, center_w-30:center_w+30] = 1
                                    self.logger.info(f"âœ… ì¤‘ì•™ ì˜ì—­ ê¸°ë°˜ ê¸°ë³¸ê°’ ì„¤ì •: {np.unique(parsing_output_np)}")
                                    
                            except Exception as e:
                                self.logger.warning(f"âš ï¸ í”¼ë¶€ìƒ‰ ê°ì§€ ì‹¤íŒ¨: {e}")
                                # ìµœí›„ì˜ ìˆ˜ë‹¨: ì¤‘ì•™ ì˜ì—­ ì„¤ì •
                                h, w = parsing_output_np.shape
                                center_h, center_w = h // 2, w // 2
                                parsing_output_np[center_h-50:center_h+50, center_w-30:center_w+30] = 1
                                self.logger.info(f"âœ… ìµœí›„ ìˆ˜ë‹¨ ê¸°ë³¸ê°’ ì„¤ì •: {np.unique(parsing_output_np)}")
                        
                        # ğŸ”¥ 2ë‹¨ê³„: ì¶”ê°€ ê²€ì¦ ë° ë³´ì •
                        final_unique_values = np.unique(parsing_output_np)
                        if len(final_unique_values) == 0 or (len(final_unique_values) == 1 and final_unique_values[0] == 0):
                            self.logger.error("âŒ ëª¨ë“  ìˆ˜ì • ì‹œë„ í›„ì—ë„ unique_labelsê°€ 0ì…ë‹ˆë‹¤!")
                            # ê°•ì œë¡œ ìµœì†Œí•œì˜ ê°’ ì„¤ì •
                            parsing_output_np[0, 0] = 1
                            self.logger.info(f"âœ… ê°•ì œ ìµœì†Œê°’ ì„¤ì •: {np.unique(parsing_output_np)}")
                
                # ğŸ”¥ 9. ê²°ê³¼ êµ¬ì„±
                inference_time = time.time() - start_time
                
                # ğŸ”¥ 10. í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
                try:
                    quality_metrics = self._calculate_quality_metrics(parsing_output_np, np.full_like(parsing_output_np, confidence, dtype=np.float32))
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
                    quality_metrics = {'overall_quality': confidence}
                
                # ğŸ”¥ 11. ìµœì¢… ê²°ê³¼ ë°˜í™˜
                unique_labels_count = len(np.unique(parsing_output_np))
                self.logger.info(f"ğŸ¯ [Step 1] ìµœì¢… ê²°ê³¼ - unique_labels: {unique_labels_count}, confidence: {confidence:.3f}")
                
                result = {
                    'success': True,
                    'parsing_result': {
                        'parsing_map': parsing_output_np,
                        'confidence': confidence,
                        'model_used': 'ensemble' if use_ensemble else 'single',
                        'unique_labels': unique_labels_count,
                        'shape': parsing_output_np.shape
                    },
                    'original_image': image,
                    'confidence': confidence,
                    'processing_time': inference_time,
                    'device_used': self.device,
                    'model_loaded': True,
                    'checkpoint_used': True,
                    'ensemble_used': use_ensemble,
                    'step_name': self.step_name,
                    'model_info': {
                        'model_name': 'Advanced Ensemble' if use_ensemble else 'Single Model',
                        'ensemble_used': use_ensemble,
                        'ensemble_models': list(ensemble_results.keys()) if use_ensemble else None,
                        'ensemble_uncertainty': uncertainty if use_ensemble and 'uncertainty' in locals() else None,
                        'model_confidences': model_confidences if use_ensemble else None,
                        'processing_time': inference_time,
                        'device_used': self.device,
                        'quality_metrics': quality_metrics
                    },
                    'quality_metrics': quality_metrics,
                    'special_cases': {},
                    'advanced_features': {
                        'high_resolution_processing': False,
                        'special_case_handling': False,
                        'iterative_refinement': False,
                        'ensemble_fusion': use_ensemble,
                        'uncertainty_quantification': use_ensemble and 'uncertainty' in locals()
                    }
                }
                
                self.logger.info(f"âœ… ê³ ë„í™”ëœ AI ì•™ìƒë¸” ì¸ì²´ íŒŒì‹± ì™„ë£Œ (ì‹œê°„: {inference_time:.2f}ì´ˆ)")
                self.logger.info(f"âœ… íŒŒì‹± ë§µ í˜•íƒœ: {parsing_output_np.shape}")
                self.logger.info(f"âœ… ê³ ìœ  ë¼ë²¨ ìˆ˜: {len(np.unique(parsing_output_np))}")
                self.logger.info(f"âœ… ì•™ìƒë¸” ì‚¬ìš©: {use_ensemble}")
                
                return result
                
            except Exception as e:
                self.logger.error(f"âŒ ê³ ë„í™”ëœ AI ì•™ìƒë¸” ì¸ì²´ íŒŒì‹± ì‹¤íŒ¨: {e}")
                import traceback
                self.logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                
                # ğŸ”¥ ì¶”ê°€ ë””ë²„ê¹… ì •ë³´
                self.logger.error(f"ğŸ” ë””ë²„ê¹… ì •ë³´:")
                self.logger.error(f"   - ì…ë ¥ ë°ì´í„° í‚¤: {list(input_data.keys()) if input_data else 'None'}")
                self.logger.error(f"   - ì´ë¯¸ì§€ íƒ€ì…: {type(input_data.get('image')) if input_data else 'None'}")
                self.logger.error(f"   - ë””ë°”ì´ìŠ¤: {getattr(self, 'device', 'Unknown')}")
                self.logger.error(f"   - ëª¨ë¸ ë¡œë” ìƒíƒœ: {getattr(self, 'model_loader', 'None')}")
                self.logger.error(f"   - ì•™ìƒë¸” ë§¤ë‹ˆì €: {getattr(self, 'ensemble_manager', 'None')}")
                
                return self._create_error_response(str(e))

        def _create_safe_input_tensor(self, image, device_str: str) -> torch.Tensor:
            """ì•ˆì „í•œ ì…ë ¥ í…ì„œ ìƒì„± (ì „ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ í´ë°±)"""
            try:
                # ì´ë¯¸ì§€ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
                if isinstance(image, dict):
                    # dictì—ì„œ ì‹¤ì œ ì´ë¯¸ì§€ ì¶”ì¶œ
                    if 'image' in image:
                        image_data = image['image']
                    elif 'person_image' in image:
                        image_data = image['person_image']
                    else:
                        image_data = list(image.values())[0]
                else:
                    image_data = image
                
                # PIL Imageë¡œ ë³€í™˜
                if hasattr(image_data, 'convert'):
                    pil_image = image_data.convert('RGB')
                elif isinstance(image_data, np.ndarray):
                    pil_image = Image.fromarray(image_data.astype(np.uint8))
                else:
                    pil_image = Image.fromarray(np.array(image_data).astype(np.uint8))
                
                # ê¸°ë³¸ ì „ì²˜ë¦¬
                transform = transforms.Compose([
                    transforms.Resize((512, 512)),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                ])
                
                tensor = transform(pil_image).unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                if device_str == 'mps' and torch.backends.mps.is_available():
                    tensor = tensor.to('mps', dtype=torch.float32)
                elif device_str == 'cuda' and torch.cuda.is_available():
                    tensor = tensor.to('cuda', dtype=torch.float32)
                else:
                    tensor = tensor.to('cpu', dtype=torch.float32)
                
                return tensor
                
            except Exception as e:
                self.logger.error(f"âŒ ì•ˆì „í•œ ì…ë ¥ í…ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
                # ì™„ì „í•œ í´ë°±: ê¸°ë³¸ í…ì„œ ìƒì„±
                fallback_tensor = torch.randn(1, 3, 512, 512, dtype=torch.float32)
                if device_str == 'mps' and torch.backends.mps.is_available():
                    fallback_tensor = fallback_tensor.to('mps')
                elif device_str == 'cuda' and torch.cuda.is_available():
                    fallback_tensor = fallback_tensor.to('cuda')
                return fallback_tensor

        def _create_fallback_parsing(self, image: np.ndarray) -> np.ndarray:
            """í´ë°± íŒŒì‹± ë§µ ìƒì„±"""
            self.logger.info("ï¿½ï¿½ í´ë°± íŒŒì‹± ë§µ ìƒì„±")
            
            # dict íƒ€ì… ì´ë¯¸ì§€ ì²˜ë¦¬
            if isinstance(image, dict):
                if 'image' in image:
                    image = image['image']
                elif 'person_image' in image:
                    image = image['person_image']
                else:
                    image = list(image.values())[0]
            
            # PIL Imageë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
            if hasattr(image, 'convert'):
                image = np.array(image)
            elif not isinstance(image, np.ndarray):
                image = np.array(image)
            
            # ê¸°ë³¸ íŒŒì‹± ë§µ ìƒì„± (ë°°ê²½ë§Œ)
            h, w = image.shape[:2]
            fallback_parsing = np.zeros((h, w), dtype=np.uint8)
            
            # ğŸ”¥ ê°•í™”ëœ ë‹¤ì¤‘ ë°©ë²• ê¸°ë°˜ ì¸ì²´ ê°ì§€
            try:
                # ğŸ”¥ ë°©ë²• 1: HSV ìƒ‰ìƒ ê¸°ë°˜ í”¼ë¶€ìƒ‰ ê°ì§€ (ê°œì„ ëœ ë²”ìœ„)
                hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                
                # ë” ë„“ì€ í”¼ë¶€ìƒ‰ ë²”ìœ„ (ë‹¤ì–‘í•œ í”¼ë¶€í†¤ ì§€ì›)
                skin_ranges = [
                    (np.array([0, 10, 60], dtype=np.uint8), np.array([25, 255, 255], dtype=np.uint8)),  # ë°ì€ í”¼ë¶€
                    (np.array([0, 20, 70], dtype=np.uint8), np.array([20, 255, 255], dtype=np.uint8)),  # ì¤‘ê°„ í”¼ë¶€
                    (np.array([0, 30, 80], dtype=np.uint8), np.array([15, 255, 255], dtype=np.uint8)),  # ì–´ë‘ìš´ í”¼ë¶€
                ]
                
                combined_skin_mask = np.zeros((h, w), dtype=np.uint8)
                
                for lower_skin, upper_skin in skin_ranges:
                    skin_mask = cv2.inRange(hsv, lower_skin, upper_skin)
                    combined_skin_mask = cv2.bitwise_or(combined_skin_mask, skin_mask)
                
                # ğŸ”¥ ë°©ë²• 2: ìƒ‰ìƒ ë¶„í¬ ê¸°ë°˜ ê°ì§€
                # RGB ì±„ë„ë³„ íˆìŠ¤í† ê·¸ë¨ ë¶„ì„
                r_channel = image[:, :, 0]
                g_channel = image[:, :, 1]
                b_channel = image[:, :, 2]
                
                # í”¼ë¶€ìƒ‰ íŠ¹ì„±: R > G > B (ì¼ë°˜ì ìœ¼ë¡œ)
                skin_color_mask = np.logical_and.reduce([
                    r_channel > g_channel,
                    g_channel > b_channel,
                    r_channel > 100,  # ì¶©ë¶„íˆ ë°ì€ í”½ì…€
                ]).astype(np.uint8) * 255
                
                # ğŸ”¥ ëª¨ë“  ë§ˆìŠ¤í¬ ê²°í•©
                final_mask = cv2.bitwise_or(combined_skin_mask, skin_color_mask)
                
                # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±° ë° ì˜ì—­ í™•ì¥
                kernel = np.ones((7, 7), np.uint8)
                final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_CLOSE, kernel)
                final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_OPEN, kernel)
                
                # ê°€ì¥ í° ì—°ê²° ìš”ì†Œ ì°¾ê¸°
                contours, _ = cv2.findContours(final_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                if contours:
                    # ê°€ì¥ í° ì»¨íˆ¬ì–´ ì„ íƒ
                    largest_contour = max(contours, key=cv2.contourArea)
                    
                    # ìµœì†Œ í¬ê¸° ê²€ì¦ (ë„ˆë¬´ ì‘ì€ ì˜ì—­ ì œì™¸)
                    contour_area = cv2.contourArea(largest_contour)
                    min_area = (h * w) * 0.01  # ì „ì²´ ì´ë¯¸ì§€ì˜ 1% ì´ìƒ
                    
                    if contour_area > min_area:
                        # ì»¨íˆ¬ì–´ ë‚´ë¶€ë¥¼ 1ë¡œ ì±„ì›€ (ì¸ì²´ë¡œ ê°„ì£¼)
                        cv2.fillPoly(fallback_parsing, [largest_contour], 1)
                        
                        self.logger.info(f"âœ… ê°•í™”ëœ í´ë°± íŒŒì‹± ë§µ ìƒì„± ì™„ë£Œ (ì¸ì²´ ê°ì§€: {contour_area:.0f} í”½ì…€)")
                    else:
                        self.logger.warning(f"âš ï¸ ê°ì§€ëœ ì˜ì—­ì´ ë„ˆë¬´ ì‘ìŒ: {contour_area:.0f} < {min_area:.0f}")
                        # ì¤‘ì•™ ì˜ì—­ ì„¤ì •
                        center_h, center_w = h // 2, w // 2
                        fallback_parsing[center_h-50:center_h+50, center_w-30:center_w+30] = 1
                        self.logger.info("âœ… ì¤‘ì•™ ì˜ì—­ ê¸°ë°˜ ê¸°ë³¸ê°’ ì„¤ì •")
                else:
                    self.logger.warning("âš ï¸ ëª¨ë“  ë°©ë²•ìœ¼ë¡œ ì¸ì²´ ê°ì§€ ì‹¤íŒ¨ - ì¤‘ì•™ ì˜ì—­ ì„¤ì •")
                    # ì¤‘ì•™ ì˜ì—­ ì„¤ì •
                    center_h, center_w = h // 2, w // 2
                    fallback_parsing[center_h-50:center_h+50, center_w-30:center_w+30] = 1
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ ê°•í™”ëœ í´ë°± íŒŒì‹± ìƒì„± ì‹¤íŒ¨: {e}")
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ì¤‘ì•™ ì˜ì—­ ì„¤ì •
                center_h, center_w = h // 2, w // 2
                fallback_parsing[center_h-50:center_h+50, center_w-30:center_w+30] = 1
                self.logger.info("âœ… ìµœí›„ ìˆ˜ë‹¨ ì¤‘ì•™ ì˜ì—­ ì„¤ì •")
            
            # ğŸ”¥ ìµœì¢… ê²€ì¦
            unique_values = np.unique(fallback_parsing)
            if len(unique_values) == 0 or (len(unique_values) == 1 and unique_values[0] == 0):
                self.logger.error("âŒ í´ë°± íŒŒì‹± ë§µì´ ëª¨ë‘ 0ì…ë‹ˆë‹¤! ê°•ì œ ìµœì†Œê°’ ì„¤ì •")
                fallback_parsing[0, 0] = 1
            
            self.logger.info(f"ğŸ”¥ í´ë°± íŒŒì‹± ë§µ ì™„ë£Œ: ê³ ìœ ê°’ {np.unique(fallback_parsing)}")
            
            return fallback_parsing
          
        def _extract_input_image(self, input_data: Dict[str, Any]) -> Optional[np.ndarray]:
            """ì…ë ¥ ë°ì´í„°ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ (ë‹¤ì–‘í•œ í‚¤ ì´ë¦„ ì§€ì›)"""
            self.logger.info(f"ğŸ”¥ [Step 1] ì…ë ¥ ë°ì´í„° í‚¤ë“¤: {list(input_data.keys())}")
            
            image = input_data.get('image')
            if image is not None:
                self.logger.info(f"âœ… [Step 1] 'image' í‚¤ì—ì„œ ì´ë¯¸ì§€ ë°œê²¬: {type(image)}")
            
            if image is None:
                image = input_data.get('person_image')
                if image is not None:
                    self.logger.info(f"âœ… [Step 1] 'person_image' í‚¤ì—ì„œ ì´ë¯¸ì§€ ë°œê²¬: {type(image)}")
            
            if image is None:
                image = input_data.get('input_image')
                if image is not None:
                    self.logger.info(f"âœ… [Step 1] 'input_image' í‚¤ì—ì„œ ì´ë¯¸ì§€ ë°œê²¬: {type(image)}")
            
            # ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ (ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°)
            if image is None and 'session_id' in input_data:
                self.logger.info(f"ğŸ”¥ [Step 1] ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹œë„: {input_data['session_id']}")
                try:
                    session_manager = self._get_service_from_central_hub('session_manager')
                    if session_manager:
                        if hasattr(session_manager, 'get_session_images_sync'):
                            self.logger.info(f"âœ… [Step 1] get_session_images_sync ì‚¬ìš©")
                            person_image, _ = session_manager.get_session_images_sync(input_data['session_id'])
                            image = person_image
                            self.logger.info(f"âœ… [Step 1] ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì„±ê³µ: {type(image)}")
                        elif hasattr(session_manager, 'get_session_images'):
                            self.logger.info(f"âœ… [Step 1] get_session_images ì‚¬ìš©")
                            import asyncio
                            import concurrent.futures
                            
                            def run_async_session_load():
                                try:
                                    return asyncio.run(session_manager.get_session_images(input_data['session_id']))
                                except Exception:
                                    return None, None
                            
                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                future = executor.submit(run_async_session_load)
                                person_image, _ = future.result(timeout=10)
                                image = person_image
                            self.logger.info(f"âœ… [Step 1] ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì„±ê³µ: {type(image)}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ [Step 1] ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            if image is None:
                self.logger.warning(f"âš ï¸ [Step 1] ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
            else:
                self.logger.info(f"âœ… [Step 1] ìµœì¢… ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
            
                # ğŸ”¥ PIL Imageë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
                if hasattr(image, 'convert'):  # PIL Image ê°ì²´
                    self.logger.info("ğŸ”„ PIL Imageë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜")
                    image_np = np.array(image.convert('RGB'))
                    self.logger.info(f"âœ… ë³€í™˜ ì™„ë£Œ: {image_np.shape}")
                    return image_np

                elif hasattr(image, 'shape'):  # NumPy ë°°ì—´
                    self.logger.info(f"âœ… NumPy ë°°ì—´ í™•ì¸: {image.shape}")
                    return image
                else:
                    self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
                    return None

        def _preprocess_image_for_model(self, image: np.ndarray, model_name: str) -> torch.Tensor:
            """ëª¨ë¸ë³„ íŠ¹í™” ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
            if model_name == 'graphonomy':
                return self._preprocess_image(image, self.device, mode='graphonomy')
            elif model_name == 'hrnet':
                return self._preprocess_image(image, self.device, mode='hrnet')
            elif model_name == 'deeplabv3plus':
                return self._preprocess_image(image, self.device, mode='deeplabv3plus')
            elif model_name == 'u2net':
                return self._preprocess_image(image, self.device, mode='u2net')
            else:
                return self._preprocess_image(image, self.device, mode='advanced')

        def _run_graphonomy_ensemble_inference(self, input_tensor: torch.Tensor, model: nn.Module) -> Dict[str, Any]:
            """Graphonomy ì•™ìƒë¸” ì¶”ë¡  - ê·¼ë³¸ì  í•´ê²°"""
            try:
                # ğŸ”¥ 1. ëª¨ë¸ ê²€ì¦ ë° í‘œì¤€í™”
                if model is None:
                    self.logger.warning("âš ï¸ Graphonomy ëª¨ë¸ì´ Noneì…ë‹ˆë‹¤")
                    return self._create_standard_output(input_tensor.device)
                
                # ğŸ”¥ 2. ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ (í‘œì¤€í™”)
                actual_model = self._extract_actual_model(model)
                if actual_model is None:
                    return self._create_standard_output(input_tensor.device)
                
                # ğŸ”¥ 3. MPS íƒ€ì… ì¼ì¹˜ (ê·¼ë³¸ì  í•´ê²°)
                device = input_tensor.device
                dtype = torch.float32  # ëª¨ë“  í…ì„œë¥¼ float32ë¡œ í†µì¼
                
                # ëª¨ë¸ì„ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ì™€ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                actual_model = actual_model.to(device, dtype=dtype)
                input_tensor = input_tensor.to(device, dtype=dtype)
                
                # ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ë™ì¼í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                for param in actual_model.parameters():
                    param.data = param.data.to(dtype)
                
                # ğŸ”¥ 4. ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (ì•ˆì „í•œ ë°©ì‹)
                try:
                    with torch.no_grad():
                        # í…ì„œ í¬ë§· ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì™„ì „í•œ ë¡œê¹… ë¹„í™œì„±í™”
                        import logging
                        import sys
                        import io
                        
                        # ëª¨ë“  ë¡œê¹… ë¹„í™œì„±í™”
                        original_level = logging.getLogger().level
                        logging.getLogger().setLevel(logging.CRITICAL)
                        
                        # stdout/stderr ë¦¬ë‹¤ì´ë ‰ì…˜ìœ¼ë¡œ í…ì„œ í¬ë§· ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨
                        original_stdout = sys.stdout
                        original_stderr = sys.stderr
                        sys.stdout = io.StringIO()
                        sys.stderr = io.StringIO()
                        
                        try:
                            output = actual_model(input_tensor)
                        finally:
                            # ì¶œë ¥ ë³µì›
                            sys.stdout = original_stdout
                            sys.stderr = original_stderr
                            logging.getLogger().setLevel(original_level)
                        
                except Exception as inference_error:
                    self.logger.warning(f"âš ï¸ Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {inference_error}")
                    return self._create_standard_output(device)
                
                # ğŸ”¥ 5. ì¶œë ¥ì—ì„œ íŒŒì‹± ì¶”ì¶œ (í‘œì¤€í™” ì—†ì´)
                parsing_output, edge_output = self._extract_parsing_from_output(output, device)
                
                # ğŸ”¥ 6. 4ì°¨ì› í…ì„œë¥¼ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (ê·¼ë³¸ì  í•´ê²°)
                if len(parsing_output.shape) == 4:
                    # (batch, channels, height, width) -> (batch, height, width)
                    parsing_output = torch.argmax(parsing_output, dim=1)
                    self.logger.info(f"âœ… 4ì°¨ì› í…ì„œë¥¼ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜: {parsing_output.shape}")
                
                # ğŸ”¥ 7. ì±„ë„ ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (ê° ëª¨ë¸ì˜ ê³ ìœ í•œ ì¶œë ¥)
                print(f"ğŸ”§ Graphonomy ì¶œë ¥ ì±„ë„ ìˆ˜: {parsing_output.shape[1] if len(parsing_output.shape) > 2 else '2D'}")
                
                # ğŸ”¥ 8. ì‹ ë¢°ë„ ê³„ì‚°
                confidence = self._calculate_confidence(parsing_output, edge_output=edge_output)
                
                return {
                    'parsing_pred': parsing_output,  # ì¼ê´€ëœ í‚¤ ì´ë¦„ ì‚¬ìš©
                    'parsing_output': parsing_output,
                    'confidence': confidence,
                    'edge_output': edge_output
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ Graphonomy ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
                return self._create_standard_output(input_tensor.device)
        
        def _extract_actual_model(self, model) -> Optional[nn.Module]:
            """ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ (í‘œì¤€í™”)"""
            try:
                if hasattr(model, 'model_instance') and model.model_instance is not None:
                    return model.model_instance
                elif hasattr(model, 'get_model_instance'):
                    return model.get_model_instance()
                elif callable(model):
                    return model
                else:
                    return None
            except Exception as e:
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                return None
        
        def _create_standard_output(self, device) -> Dict[str, Any]:
            """í‘œì¤€ ì¶œë ¥ ìƒì„±"""
            return {
                'parsing_pred': torch.zeros((1, 20, 512, 512), device=device),  # ì¼ê´€ëœ í‚¤ ì´ë¦„ ì‚¬ìš©
                'parsing_output': torch.zeros((1, 20, 512, 512), device=device),
                'confidence': 0.5,
                'edge_output': None
            }
        
        def _extract_parsing_from_output(self, output, device) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
            """ëª¨ë¸ ì¶œë ¥ì—ì„œ íŒŒì‹± ê²°ê³¼ ì¶”ì¶œ (ê·¼ë³¸ì  í•´ê²°)"""
            try:
                # ğŸ”¥ 1ë‹¨ê³„: ì¶œë ¥ íƒ€ì… ê²€ì¦ ë° ì •ê·œí™”
                if output is None:
                    self.logger.warning("âš ï¸ AI ëª¨ë¸ ì¶œë ¥ì´ Noneì…ë‹ˆë‹¤.")
                    return torch.zeros((1, 20, 512, 512), device=device), None
                
                # ğŸ”¥ 2ë‹¨ê³„: ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ì¶œë ¥ ì²˜ë¦¬
                if isinstance(output, dict):
                    self.logger.debug(f"ğŸ”¥ ë”•ì…”ë„ˆë¦¬ ì¶œë ¥ í‚¤ë“¤: {list(output.keys())}")
                    
                    # ê°€ëŠ¥í•œ í‚¤ë“¤ì—ì„œ íŒŒì‹± ê²°ê³¼ ì°¾ê¸°
                    parsing_keys = ['parsing', 'parsing_pred', 'output', 'parsing_output', 'logits', 'pred', 'prediction']
                    parsing_tensor = None
                    confidence_tensor = None
                    
                    for key in parsing_keys:
                        if key in output and output[key] is not None:
                            if isinstance(output[key], torch.Tensor):
                                parsing_tensor = output[key]
                                self.logger.debug(f"âœ… íŒŒì‹± í…ì„œ ë°œê²¬: {key} - {parsing_tensor.shape}")
                                break
                            elif isinstance(output[key], (list, tuple)) and len(output[key]) > 0:
                                if isinstance(output[key][0], torch.Tensor):
                                    parsing_tensor = output[key][0]
                                    self.logger.debug(f"âœ… íŒŒì‹± í…ì„œ ë°œê²¬ (ë¦¬ìŠ¤íŠ¸): {key} - {parsing_tensor.shape}")
                                    break
                    
                    # ì‹ ë¢°ë„ í…ì„œ ì°¾ê¸°
                    confidence_keys = ['confidence', 'conf', 'prob', 'probability']
                    for key in confidence_keys:
                        if key in output and output[key] is not None:
                            if isinstance(output[key], torch.Tensor):
                                confidence_tensor = output[key]
                                self.logger.debug(f"âœ… ì‹ ë¢°ë„ í…ì„œ ë°œê²¬: {key} - {confidence_tensor.shape}")
                                break
                    
                    # ğŸ”¥ 3ë‹¨ê³„: í…ì„œê°€ ì—†ëŠ” ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                    if parsing_tensor is None:
                        first_value = next(iter(output.values()))
                        if isinstance(first_value, torch.Tensor):
                            parsing_tensor = first_value
                            self.logger.debug(f"âœ… ì²« ë²ˆì§¸ ê°’ì—ì„œ íŒŒì‹± í…ì„œ ì¶”ì¶œ: {parsing_tensor.shape}")
                        elif isinstance(first_value, (list, tuple)) and len(first_value) > 0:
                            if isinstance(first_value[0], torch.Tensor):
                                parsing_tensor = first_value[0]
                                self.logger.debug(f"âœ… ì²« ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ íŒŒì‹± í…ì„œ ì¶”ì¶œ: {parsing_tensor.shape}")
                    
                    if parsing_tensor is None:
                        raise ValueError("ë”•ì…”ë„ˆë¦¬ì—ì„œ íŒŒì‹± í…ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    return parsing_tensor, confidence_tensor
                
                # ğŸ”¥ 4ë‹¨ê³„: ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ì¶œë ¥ ì²˜ë¦¬
                elif isinstance(output, (list, tuple)):
                    self.logger.debug(f"ğŸ”¥ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ ê¸¸ì´: {len(output)}")
                    
                    if len(output) == 0:
                        raise ValueError("ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ì…ë‹ˆë‹¤.")
                    
                    # ì²« ë²ˆì§¸ ìš”ì†Œê°€ í…ì„œì¸ì§€ í™•ì¸
                    first_element = output[0]
                    if isinstance(first_element, torch.Tensor):
                        parsing_tensor = first_element
                        self.logger.debug(f"âœ… ë¦¬ìŠ¤íŠ¸ ì²« ë²ˆì§¸ ìš”ì†Œì—ì„œ íŒŒì‹± í…ì„œ ì¶”ì¶œ: {parsing_tensor.shape}")
                        
                        # ë‘ ë²ˆì§¸ ìš”ì†Œê°€ ì‹ ë¢°ë„ í…ì„œì¸ì§€ í™•ì¸
                        confidence_tensor = None
                        if len(output) > 1 and isinstance(output[1], torch.Tensor):
                            confidence_tensor = output[1]
                            self.logger.debug(f"âœ… ë¦¬ìŠ¤íŠ¸ ë‘ ë²ˆì§¸ ìš”ì†Œì—ì„œ ì‹ ë¢°ë„ í…ì„œ ì¶”ì¶œ: {confidence_tensor.shape}")
                        
                        return parsing_tensor, confidence_tensor
                    else:
                        self.logger.warning(f"âš ï¸ ë¦¬ìŠ¤íŠ¸ ì²« ë²ˆì§¸ ìš”ì†Œê°€ í…ì„œê°€ ì•„ë‹˜: {type(first_element)}")
                        # ë”•ì…”ë„ˆë¦¬ë¡œ ì²˜ë¦¬
                        if isinstance(first_element, dict):
                            return self._extract_parsing_from_output(first_element, device)
                        else:
                            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¶œë ¥ íƒ€ì…: {type(first_element)}")
                
                # ğŸ”¥ 5ë‹¨ê³„: ì§ì ‘ í…ì„œ ì¶œë ¥ ì²˜ë¦¬
                elif isinstance(output, torch.Tensor):
                    self.logger.debug(f"âœ… ì§ì ‘ í…ì„œ ì¶œë ¥: {output.shape}")
                    # ì›ë³¸ í…ì„œ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì°¨ì› ë³€í™˜ì€ í˜¸ì¶œí•˜ëŠ” ê³³ì—ì„œ ì²˜ë¦¬)
                    return output, None
                
                # ğŸ”¥ 6ë‹¨ê³„: ê¸°íƒ€ íƒ€ì… ì²˜ë¦¬
                else:
                    self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¶œë ¥ íƒ€ì…: {type(output)}")
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¶œë ¥ íƒ€ì…: {type(output)}")
                    
            except Exception as e:
                self.logger.error(f"âŒ íŒŒì‹± ì¶œë ¥ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                # ê¸°ë³¸ê°’ ë°˜í™˜
                return torch.zeros((1, 20, 512, 512), device=device), None
        
        def _standardize_channels(self, tensor: torch.Tensor, target_channels: int = 20) -> torch.Tensor:
            """ì±„ë„ ìˆ˜ í‘œì¤€í™” (ê·¼ë³¸ì  í•´ê²°)"""
            try:
                # ğŸ”¥ ì…ë ¥ ê²€ì¦
                if tensor is None:
                    self.logger.warning("âš ï¸ í…ì„œê°€ Noneì…ë‹ˆë‹¤.")
                    return torch.zeros((1, target_channels, 512, 512), device='cpu', dtype=torch.float32)
                
                # ğŸ”¥ ì°¨ì› ê²€ì¦
                if len(tensor.shape) != 4:
                    self.logger.warning(f"âš ï¸ í…ì„œ ì°¨ì›ì´ 4ê°€ ì•„ë‹˜: {tensor.shape}")
                    if len(tensor.shape) == 3:
                        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                        tensor = tensor.unsqueeze(0)
                    elif len(tensor.shape) == 2:
                        # ë°°ì¹˜ì™€ ì±„ë„ ì°¨ì› ì¶”ê°€
                        tensor = tensor.unsqueeze(0).unsqueeze(0)
                    else:
                        return torch.zeros((1, target_channels, 512, 512), device=tensor.device, dtype=tensor.dtype)
                
                # ğŸ”¥ ì±„ë„ ìˆ˜ í‘œì¤€í™”
                if tensor.shape[1] == target_channels:
                    return tensor
                elif tensor.shape[1] > target_channels:
                    # ğŸ”¥ ì±„ë„ ìˆ˜ê°€ ë§ìœ¼ë©´ ì•ìª½ ì±„ë„ë§Œ ì‚¬ìš©
                    return tensor[:, :target_channels, :, :]
                else:
                    # ğŸ”¥ ì±„ë„ ìˆ˜ê°€ ì ìœ¼ë©´ íŒ¨ë”©
                    padding = torch.zeros(
                        tensor.shape[0], 
                        target_channels - tensor.shape[1], 
                        tensor.shape[2], 
                        tensor.shape[3],
                        device=tensor.device,
                        dtype=tensor.dtype
                    )
                    return torch.cat([tensor, padding], dim=1)
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì±„ë„ ìˆ˜ í‘œì¤€í™” ì‹¤íŒ¨: {e}")
                # ê¸°ë³¸ê°’ ë°˜í™˜
                return torch.zeros((1, target_channels, 512, 512), device='cpu', dtype=torch.float32)

        def _run_hrnet_ensemble_inference(self, input_tensor: torch.Tensor, model: nn.Module) -> Dict[str, Any]:
            """HRNet ì•™ìƒë¸” ì¶”ë¡  - ê·¼ë³¸ì  í•´ê²°"""
            try:
                # ğŸ”¥ 1. ëª¨ë¸ ê²€ì¦ ë° í‘œì¤€í™”
                if model is None:
                    self.logger.warning("âš ï¸ HRNet ëª¨ë¸ì´ Noneì…ë‹ˆë‹¤")
                    return self._create_standard_output(input_tensor.device)
                
                # ğŸ”¥ 2. ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ (í‘œì¤€í™”)
                actual_model = self._extract_actual_model(model)
                if actual_model is None:
                    return self._create_standard_output(input_tensor.device)
                
                # ğŸ”¥ 3. MPS íƒ€ì… ì¼ì¹˜ (ê·¼ë³¸ì  í•´ê²°)
                device = input_tensor.device
                dtype = torch.float32  # ëª¨ë“  í…ì„œë¥¼ float32ë¡œ í†µì¼
                
                # ëª¨ë¸ì„ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ì™€ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                actual_model = actual_model.to(device, dtype=dtype)
                input_tensor = input_tensor.to(device, dtype=dtype)
                
                # ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ë™ì¼í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                for param in actual_model.parameters():
                    param.data = param.data.to(dtype)
                
                # ğŸ”¥ 4. ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (ì•ˆì „í•œ ë°©ì‹)
                try:
                    with torch.no_grad():
                        # í…ì„œ í¬ë§· ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì™„ì „í•œ ë¡œê¹… ë¹„í™œì„±í™”
                        import logging
                        import sys
                        import io
                        
                        # ëª¨ë“  ë¡œê¹… ë¹„í™œì„±í™”
                        original_level = logging.getLogger().level
                        logging.getLogger().setLevel(logging.CRITICAL)
                        
                        # stdout/stderr ë¦¬ë‹¤ì´ë ‰ì…˜ìœ¼ë¡œ í…ì„œ í¬ë§· ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨
                        original_stdout = sys.stdout
                        original_stderr = sys.stderr
                        sys.stdout = io.StringIO()
                        sys.stderr = io.StringIO()
                        
                        try:
                            output = actual_model(input_tensor)
                        finally:
                            # ì¶œë ¥ ë³µì›
                            sys.stdout = original_stdout
                            sys.stderr = original_stderr
                            logging.getLogger().setLevel(original_level)
                        
                except Exception as inference_error:
                    self.logger.warning(f"âš ï¸ HRNet ì¶”ë¡  ì‹¤íŒ¨: {inference_error}")
                    return self._create_standard_output(input_tensor.device)
                
                # ğŸ”¥ 5. ì¶œë ¥ í‘œì¤€í™” (ê·¼ë³¸ì  í•´ê²°)
                parsing_output, _ = self._extract_parsing_from_output(output, input_tensor.device)
                
                # ğŸ”¥ 6. ì±„ë„ ìˆ˜ í‘œì¤€í™” (20ê°œë¡œ í†µì¼)
                parsing_output = self._standardize_channels(parsing_output, target_channels=20)
                
                # ğŸ”¥ 7. ì‹ ë¢°ë„ ê³„ì‚°
                confidence = self._calculate_confidence(parsing_output)
                
                return {
                    'parsing_pred': parsing_output,  # ì¼ê´€ëœ í‚¤ ì´ë¦„ ì‚¬ìš©
                    'parsing_output': parsing_output,
                    'confidence': confidence
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ HRNet ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
                return self._create_standard_output(input_tensor.device)

        def _run_deeplabv3plus_ensemble_inference(self, input_tensor: torch.Tensor, model: nn.Module) -> Dict[str, Any]:
            """DeepLabV3+ ì•™ìƒë¸” ì¶”ë¡ """
            try:
                # RealAIModelì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ
                if hasattr(model, 'model_instance') and model.model_instance is not None:
                    actual_model = model.model_instance
                    self.logger.info("âœ… DeepLabV3+ - RealAIModelì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ ì„±ê³µ")
                elif hasattr(model, 'get_model_instance'):
                    actual_model = model.get_model_instance()
                    self.logger.info("âœ… DeepLabV3+ - get_model_instance()ë¡œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ ì„±ê³µ")
                else:
                    actual_model = model
                    self.logger.info("âš ï¸ DeepLabV3+ - ì§ì ‘ ëª¨ë¸ ì‚¬ìš© (RealAIModel ì•„ë‹˜)")
                
                # ëª¨ë¸ì„ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ì™€ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (MPS íƒ€ì… ì¼ì¹˜)
                device = input_tensor.device
                dtype = torch.float32  # ëª¨ë“  í…ì„œë¥¼ float32ë¡œ í†µì¼
                
                if hasattr(actual_model, 'to'):
                    actual_model = actual_model.to(device, dtype=dtype)
                    self.logger.info(f"âœ… DeepLabV3+ ëª¨ë¸ì„ {device} ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (float32)")
                
                # ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ë™ì¼í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                for param in actual_model.parameters():
                    param.data = param.data.to(dtype)
                
                # ëª¨ë¸ì´ callableí•œì§€ í™•ì¸
                if not callable(actual_model):
                    self.logger.warning("âš ï¸ DeepLabV3+ ëª¨ë¸ì´ callableí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                    # ì‹¤ì œ ëª¨ë¸ì´ ì•„ë‹Œ ê²½ìš° ì˜¤ë¥˜ ë°œìƒ
                    raise ValueError("DeepLabV3+ ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                
                # í…ì„œ í¬ë§· ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì™„ì „í•œ ë¡œê¹… ë¹„í™œì„±í™”
                import logging
                import sys
                import io
                
                # ëª¨ë“  ë¡œê¹… ë¹„í™œì„±í™”
                original_level = logging.getLogger().level
                logging.getLogger().setLevel(logging.CRITICAL)
                
                # stdout/stderr ë¦¬ë‹¤ì´ë ‰ì…˜ìœ¼ë¡œ í…ì„œ í¬ë§· ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨
                original_stdout = sys.stdout
                original_stderr = sys.stderr
                sys.stdout = io.StringIO()
                sys.stderr = io.StringIO()
                
                try:
                    output = actual_model(input_tensor)
                finally:
                    # ì¶œë ¥ ë³µì›
                    sys.stdout = original_stdout
                    sys.stderr = original_stderr
                    logging.getLogger().setLevel(original_level)
                
                # DeepLabV3+ ì¶œë ¥ ì²˜ë¦¬
                if isinstance(output, (tuple, list)):
                    parsing_output = output[0]
                else:
                    parsing_output = output
                
                confidence = self._calculate_confidence(parsing_output)
                
                return {
                    'parsing_pred': parsing_output,  # ì¼ê´€ëœ í‚¤ ì´ë¦„ ì‚¬ìš©
                    'parsing_output': parsing_output,
                    'confidence': confidence
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ DeepLabV3+ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
                return {
                    'parsing_pred': torch.zeros((1, 20, 512, 512)),
                    'parsing_output': torch.zeros((1, 20, 512, 512)),
                    'confidence': 0.5
                }

        def _run_u2net_ensemble_inference(self, input_tensor: torch.Tensor, model: nn.Module) -> Dict[str, Any]:
            """U2Net ì•™ìƒë¸” ì¶”ë¡ """
            # RealAIModelì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ
            if hasattr(model, 'model_instance') and model.model_instance is not None:
                actual_model = model.model_instance
                self.logger.info("âœ… U2Net - RealAIModelì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ ì„±ê³µ")
            elif hasattr(model, 'get_model_instance'):
                actual_model = model.get_model_instance()
                self.logger.info("âœ… U2Net - get_model_instance()ë¡œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ ì„±ê³µ")
                
                # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì¶œë ¥ ë°©ì§€
                if isinstance(actual_model, dict):
                    self.logger.info(f"âœ… U2Net - ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ê°ì§€ë¨")
                else:
                    self.logger.info(f"âœ… U2Net - ëª¨ë¸ íƒ€ì…: {type(actual_model)}")
            else:
                actual_model = model
                self.logger.info("âš ï¸ U2Net - ì§ì ‘ ëª¨ë¸ ì‚¬ìš© (RealAIModel ì•„ë‹˜)")
            
            # ëª¨ë¸ì„ MPS ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if hasattr(actual_model, 'to'):
                actual_model = actual_model.to(self.device)
                self.logger.info(f"âœ… U2Net ëª¨ë¸ì„ {self.device} ë””ë°”ì´ìŠ¤ë¡œ ì´ë™")
            
            output = actual_model(input_tensor)
            
            # U2Net ì¶œë ¥ ì²˜ë¦¬
            if isinstance(output, (tuple, list)):
                parsing_output = output[0]
            else:
                parsing_output = output
            
            confidence = self._calculate_confidence(parsing_output)
            
            return {
                'parsing_output': parsing_output,
                'confidence': confidence
            }

        def _run_generic_ensemble_inference(self, input_tensor: torch.Tensor, model: nn.Module) -> Dict[str, Any]:
            """ì¼ë°˜ ëª¨ë¸ ì•™ìƒë¸” ì¶”ë¡  - MPS í˜¸í™˜ì„± ê°œì„ """
            return self._run_graphonomy_ensemble_inference_mps_safe(input_tensor, model)
        
        def _run_graphonomy_safe_inference(self, input_tensor: torch.Tensor, model: nn.Module, device: str) -> Dict[str, Any]:
            """ğŸ”¥ Graphonomy ì•ˆì „ ì¶”ë¡  - í…ì„œ í¬ë§· ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨"""
            try:
                # ğŸ”¥ 1. ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì„¤ì •
                if device is None:
                    device = input_tensor.device
                device_str = str(device)
                
                # ğŸ”¥ 2. ëª¨ë¸ ì¶”ì¶œ
                actual_model = self._extract_actual_model(model)
                if actual_model is None:
                    return self._create_standard_output(device_str)
                
                # ğŸ”¥ 3. MPS íƒ€ì… í†µì¼
                actual_model = actual_model.to(device_str, dtype=torch.float32)
                input_tensor = input_tensor.to(device_str, dtype=torch.float32)
                
                # ğŸ”¥ 4. ì™„ì „í•œ ì¶œë ¥ ì°¨ë‹¨ìœ¼ë¡œ ì•ˆì „ ì¶”ë¡ 
                import os
                import sys
                import io
                
                # í™˜ê²½ ë³€ìˆ˜ë¡œ í…ì„œ í¬ë§· ì˜¤ë¥˜ ë°©ì§€
                os.environ['PYTORCH_DISABLE_TENSOR_FORMAT'] = '1'
                
                # stdout/stderr ì™„ì „ ì°¨ë‹¨
                original_stdout = sys.stdout
                original_stderr = sys.stderr
                sys.stdout = io.StringIO()
                sys.stderr = io.StringIO()
                
                try:
                    with torch.no_grad():
                        output = actual_model(input_tensor)
                finally:
                    # ì¶œë ¥ ë³µì›
                    sys.stdout = original_stdout
                    sys.stderr = original_stderr
                
                # ğŸ”¥ 5. ì¶œë ¥ ì²˜ë¦¬
                parsing_output, _ = self._extract_parsing_from_output(output, device_str)
                confidence = self._calculate_confidence(parsing_output)
                
                return {
                    'parsing_pred': parsing_output,
                    'parsing_output': parsing_output,
                    'confidence': confidence,
                    'edge_output': None
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ Graphonomy ì•ˆì „ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
                return self._create_standard_output(device_str if 'device_str' in locals() else 'cpu')
        
        def _run_hrnet_safe_inference(self, input_tensor: torch.Tensor, model: nn.Module, device: str) -> Dict[str, Any]:
            """ğŸ”¥ HRNet ì•ˆì „ ì¶”ë¡  - í…ì„œ í¬ë§· ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨"""
            try:
                # ğŸ”¥ 1. ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì„¤ì •
                if device is None:
                    device = input_tensor.device
                device_str = str(device)
                
                # ğŸ”¥ 2. ëª¨ë¸ ì¶”ì¶œ
                actual_model = self._extract_actual_model(model)
                if actual_model is None:
                    return self._create_standard_output(device_str)
                
                # ğŸ”¥ 3. MPS íƒ€ì… í†µì¼
                actual_model = actual_model.to(device_str, dtype=torch.float32)
                input_tensor = input_tensor.to(device_str, dtype=torch.float32)
                
                # ğŸ”¥ 4. ì™„ì „í•œ ì¶œë ¥ ì°¨ë‹¨ìœ¼ë¡œ ì•ˆì „ ì¶”ë¡ 
                import os
                import sys
                import io
                
                # í™˜ê²½ ë³€ìˆ˜ë¡œ í…ì„œ í¬ë§· ì˜¤ë¥˜ ë°©ì§€
                os.environ['PYTORCH_DISABLE_TENSOR_FORMAT'] = '1'
                
                # stdout/stderr ì™„ì „ ì°¨ë‹¨
                original_stdout = sys.stdout
                original_stderr = sys.stderr
                sys.stdout = io.StringIO()
                sys.stderr = io.StringIO()
                
                try:
                    with torch.no_grad():
                        output = actual_model(input_tensor)
                finally:
                    # ì¶œë ¥ ë³µì›
                    sys.stdout = original_stdout
                    sys.stderr = original_stderr
                
                # ğŸ”¥ 5. ì¶œë ¥ ì²˜ë¦¬
                parsing_output, _ = self._extract_parsing_from_output(output, device_str)
                confidence = self._calculate_confidence(parsing_output)
                
                return {
                    'parsing_pred': parsing_output,
                    'parsing_output': parsing_output,
                    'confidence': confidence
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ HRNet ì•ˆì „ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
                return self._create_standard_output(device_str if 'device_str' in locals() else 'cpu')
        
        def _run_deeplabv3plus_safe_inference(self, input_tensor: torch.Tensor, model: nn.Module, device: str) -> Dict[str, Any]:
            """ğŸ”¥ DeepLabV3+ ì•ˆì „ ì¶”ë¡  - í…ì„œ í¬ë§· ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨"""
            try:
                # ğŸ”¥ 1. ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì„¤ì •
                if device is None:
                    device = input_tensor.device
                device_str = str(device)
                
                # ğŸ”¥ 2. ëª¨ë¸ ì¶”ì¶œ
                actual_model = self._extract_actual_model(model)
                if actual_model is None:
                    return self._create_standard_output(device_str)
                
                # ğŸ”¥ 3. MPS íƒ€ì… í†µì¼
                actual_model = actual_model.to(device_str, dtype=torch.float32)
                input_tensor = input_tensor.to(device_str, dtype=torch.float32)
                
                # ğŸ”¥ 4. ì™„ì „í•œ ì¶œë ¥ ì°¨ë‹¨ìœ¼ë¡œ ì•ˆì „ ì¶”ë¡ 
                import os
                import sys
                import io
                
                # í™˜ê²½ ë³€ìˆ˜ë¡œ í…ì„œ í¬ë§· ì˜¤ë¥˜ ë°©ì§€
                os.environ['PYTORCH_DISABLE_TENSOR_FORMAT'] = '1'
                
                # stdout/stderr ì™„ì „ ì°¨ë‹¨
                original_stdout = sys.stdout
                original_stderr = sys.stderr
                sys.stdout = io.StringIO()
                sys.stderr = io.StringIO()
                
                try:
                    with torch.no_grad():
                        output = actual_model(input_tensor)
                finally:
                    # ì¶œë ¥ ë³µì›
                    sys.stdout = original_stdout
                    sys.stderr = original_stderr
                
                # ğŸ”¥ 5. ì¶œë ¥ ì²˜ë¦¬
                parsing_output, _ = self._extract_parsing_from_output(output, device_str)
                confidence = self._calculate_confidence(parsing_output)
                
                return {
                    'parsing_pred': parsing_output,
                    'parsing_output': parsing_output,
                    'confidence': confidence
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ DeepLabV3+ ì•ˆì „ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
                return self._create_standard_output(device_str if 'device_str' in locals() else 'cpu')
        
        def _run_u2net_safe_inference(self, input_tensor: torch.Tensor, model: nn.Module, device: str) -> Dict[str, Any]:
            """ğŸ”¥ U2Net ì•ˆì „ ì¶”ë¡  - í…ì„œ í¬ë§· ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨"""
            try:
                # ğŸ”¥ 1. ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì„¤ì •
                if device is None:
                    device = input_tensor.device
                device_str = str(device)
                
                # ğŸ”¥ 2. ëª¨ë¸ ì¶”ì¶œ
                actual_model = self._extract_actual_model(model)
                if actual_model is None:
                    return self._create_standard_output(device_str)
                
                # ğŸ”¥ 3. MPS íƒ€ì… í†µì¼
                actual_model = actual_model.to(device_str, dtype=torch.float32)
                input_tensor = input_tensor.to(device_str, dtype=torch.float32)
                
                # ğŸ”¥ 4. ì™„ì „í•œ ì¶œë ¥ ì°¨ë‹¨ìœ¼ë¡œ ì•ˆì „ ì¶”ë¡ 
                import os
                import sys
                import io
                
                # í™˜ê²½ ë³€ìˆ˜ë¡œ í…ì„œ í¬ë§· ì˜¤ë¥˜ ë°©ì§€
                os.environ['PYTORCH_DISABLE_TENSOR_FORMAT'] = '1'
                
                # stdout/stderr ì™„ì „ ì°¨ë‹¨
                original_stdout = sys.stdout
                original_stderr = sys.stderr
                sys.stdout = io.StringIO()
                sys.stderr = io.StringIO()
                
                try:
                    with torch.no_grad():
                        output = actual_model(input_tensor)
                finally:
                    # ì¶œë ¥ ë³µì›
                    sys.stdout = original_stdout
                    sys.stderr = original_stderr
                
                # ğŸ”¥ 5. ì¶œë ¥ ì²˜ë¦¬
                parsing_output, _ = self._extract_parsing_from_output(output, device_str)
                confidence = self._calculate_confidence(parsing_output)
                
                return {
                    'parsing_pred': parsing_output,
                    'parsing_output': parsing_output,
                    'confidence': confidence
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ U2Net ì•ˆì „ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
                return self._create_standard_output(device_str if 'device_str' in locals() else 'cpu')
        
        def _run_generic_safe_inference(self, input_tensor: torch.Tensor, model: nn.Module, device: str) -> Dict[str, Any]:
            """ğŸ”¥ ì¼ë°˜ ëª¨ë¸ ì•ˆì „ ì¶”ë¡  - í…ì„œ í¬ë§· ì˜¤ë¥˜ ì™„ì „ ì°¨ë‹¨"""
            try:
                # ğŸ”¥ 1. ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì„¤ì •
                if device is None:
                    device = input_tensor.device
                device_str = str(device)
                
                # ğŸ”¥ 2. MPS íƒ€ì… í†µì¼
                model = model.to(device_str, dtype=torch.float32)
                input_tensor = input_tensor.to(device_str, dtype=torch.float32)
                
                # ğŸ”¥ 3. ì™„ì „í•œ ì¶œë ¥ ì°¨ë‹¨ìœ¼ë¡œ ì•ˆì „ ì¶”ë¡ 
                import os
                import sys
                import io
                
                # í™˜ê²½ ë³€ìˆ˜ë¡œ í…ì„œ í¬ë§· ì˜¤ë¥˜ ë°©ì§€
                os.environ['PYTORCH_DISABLE_TENSOR_FORMAT'] = '1'
                
                # stdout/stderr ì™„ì „ ì°¨ë‹¨
                original_stdout = sys.stdout
                original_stderr = sys.stderr
                sys.stdout = io.StringIO()
                sys.stderr = io.StringIO()
                
                try:
                    with torch.no_grad():
                        output = model(input_tensor)
                finally:
                    # ì¶œë ¥ ë³µì›
                    sys.stdout = original_stdout
                    sys.stderr = original_stderr
                
                # ğŸ”¥ 4. ì¶œë ¥ ì²˜ë¦¬
                parsing_output, _ = self._extract_parsing_from_output(output, device_str)
                confidence = self._calculate_confidence(parsing_output)
                
                return {
                    'parsing_pred': parsing_output,
                    'parsing_output': parsing_output,
                    'confidence': confidence
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì¼ë°˜ ëª¨ë¸ ì•ˆì „ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
                return self._create_standard_output(device_str if 'device_str' in locals() else 'cpu')

        def _calculate_ensemble_uncertainty(self, ensemble_results: Dict[str, torch.Tensor]) -> float:
            """ì•™ìƒë¸” ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”"""
            if len(ensemble_results) < 2:
                return 0.0
            
            # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ í™•ë¥ ë¡œ ë³€í™˜
            predictions = []
            for model_name, output in ensemble_results.items():
                try:
                    if isinstance(output, torch.Tensor):
                        # í…ì„œë¥¼ numpyë¡œ ë³€í™˜í•˜ê¸° ì „ì— ì°¨ì› í™•ì¸
                        if output.dim() >= 3:  # (B, C, H, W) í˜•íƒœ
                            probs = torch.softmax(output, dim=1)
                            # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ ì‚¬ìš©í•˜ê³  ê³µê°„ ì°¨ì›ì„ í‰ê· 
                            probs_np = probs[0].detach().cpu().numpy()  # (C, H, W)
                            # ê³µê°„ ì°¨ì›ì„ í‰ê· í•˜ì—¬ (C,) í˜•íƒœë¡œ ë³€í™˜
                            probs_avg = np.mean(probs_np, axis=(1, 2))  # (C,)
                            predictions.append(probs_avg)
                        else:
                            # 1D ë˜ëŠ” 2D í…ì„œì¸ ê²½ìš°
                            probs = torch.softmax(output, dim=-1)
                            probs_np = probs.detach().cpu().numpy()
                            predictions.append(probs_np.flatten())
                    else:
                        # í…ì„œê°€ ì•„ë‹Œ ê²½ìš° ê±´ë„ˆë›°ê¸°
                        continue
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_name} ë¶ˆí™•ì‹¤ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
                    continue
            
            if not predictions:
                return 0.0
            
            try:
                # ëª¨ë“  ì˜ˆì¸¡ì„ ë™ì¼í•œ ê¸¸ì´ë¡œ ë§ì¶¤
                max_len = max(len(p) for p in predictions)
                padded_predictions = []
                for p in predictions:
                    if len(p) < max_len:
                        # íŒ¨ë”©ìœ¼ë¡œ ê¸¸ì´ ë§ì¶¤
                        padded = np.pad(p, (0, max_len - len(p)), mode='constant', constant_values=0)
                        padded_predictions.append(padded)
                    else:
                        padded_predictions.append(p[:max_len])
                
                # ì˜ˆì¸¡ ë¶„ì‚° ê³„ì‚°
                predictions_array = np.array(padded_predictions)
                variance = np.var(predictions_array, axis=0)
                uncertainty = np.mean(variance)
                
                return float(uncertainty)
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¶ˆí™•ì‹¤ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
                return 0.5  # ê¸°ë³¸ê°’

        def _calibrate_ensemble_confidence(self, model_confidences: Dict[str, float], uncertainty: float) -> float:
            """ì•™ìƒë¸” ì‹ ë¢°ë„ ë³´ì •"""
            if not model_confidences:
                return 0.0
            
            # ê¸°ë³¸ ì‹ ë¢°ë„ (ê°€ì¤‘ í‰ê· ) - ì‹œí€€ìŠ¤ ì˜¤ë¥˜ ë°©ì§€
            try:
                # ê°’ë“¤ì´ ìˆ«ìì¸ì§€ í™•ì¸í•˜ê³  ë³€í™˜
                confidence_values = []
                for key, value in model_confidences.items():
                    try:
                        if isinstance(value, (list, tuple)):
                            # ì‹œí€€ìŠ¤ì¸ ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                            if value:
                                confidence_values.append(float(value[0]))
                            else:
                                confidence_values.append(0.5)
                        elif isinstance(value, (int, float)):
                            confidence_values.append(float(value))
                        elif isinstance(value, np.ndarray):
                            # numpy ë°°ì—´ì¸ ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                            confidence_values.append(float(value.flatten()[0]))
                        else:
                            # ê¸°íƒ€ íƒ€ì…ì€ 0.5ë¡œ ì„¤ì •
                            confidence_values.append(0.5)
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ì‹ ë¢°ë„ ê°’ ë³€í™˜ ì‹¤íŒ¨ ({key}): {e}")
                        confidence_values.append(0.5)
                
                if not confidence_values:
                    return 0.5
                
                weights = np.array(confidence_values)
                base_confidence = np.average(weights, weights=weights)
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì‹ ë¢°ë„ ë³´ì • ì‹¤íŒ¨: {e}")
                # í´ë°±: ë‹¨ìˆœ í‰ê· 
                base_confidence = 0.8
            
            # ë¶ˆí™•ì‹¤ì„±ì— ë”°ë¥¸ ë³´ì •
            uncertainty_penalty = uncertainty * 0.5  # ë¶ˆí™•ì‹¤ì„± í˜ë„í‹°
            calibrated_confidence = max(0.0, min(1.0, base_confidence - uncertainty_penalty))
            
            return calibrated_confidence

        def _load_graphonomy_model(self):
            """Graphonomy ëª¨ë¸ ë¡œë”© (ì‹¤ì œ íŒŒì¼ ê°•ì œ ë¡œë”©)"""
            try:
                self.logger.info("ğŸ”¥ [DEBUG] _load_graphonomy_model() ì§„ì…!")
                self.logger.debug("ğŸ”„ Graphonomy ëª¨ë¸ ë¡œë”© ì‹œì‘...")
                
                # ğŸ”¥ ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì§ì ‘ ë¡œë”©
                import torch
                from pathlib import Path
                
                # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë“¤ (í„°ë¯¸ë„ì—ì„œ í™•ì¸ëœ ì‹¤ì œ íŒŒì¼ë“¤)
                possible_paths = [
                    "ai_models/Self-Correction-Human-Parsing/exp-schp-201908261155-atr.pth",
                    "ai_models/human_parsing/schp/pytorch_model.bin",
                    "ai_models/human_parsing/models--mattmdjaga--segformer_b2_clothes/snapshots/c4d76e5d0058ab0e3e805d5382c44d5bd059fee3/pytorch_model.bin",
                    "ai_models/step_06_virtual_fitting/ootdiffusion/checkpoints/humanparsing/exp-schp-201908301523-atr.pth",
                    "u2net.pth"
                ]
                
                for model_path in possible_paths:
                    try:
                        full_path = Path(model_path)
                        if full_path.exists():
                            self.logger.info(f"ğŸ”„ ì‹¤ì œ íŒŒì¼ ë¡œë”© ì‹œë„: {model_path}")
                            
                            # ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                            checkpoint = torch.load(str(full_path), map_location='cpu')
                            self.logger.debug(f"âœ… ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ: {len(checkpoint)}ê°œ í‚¤")
                            
                            # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ìƒì„¸ ë¶„ì„ (DEBUG ë ˆë²¨ë¡œ ë³€ê²½)
                            self.logger.debug(f"ğŸ” ì²´í¬í¬ì¸íŠ¸ í‚¤ë“¤: {list(checkpoint.keys())}")
                            for key, value in checkpoint.items():
                                if hasattr(value, 'shape'):
                                    self.logger.debug(f"ğŸ” {key}: {value.shape}")
                                else:
                                    self.logger.debug(f"ğŸ” {key}: {type(value)}")
                            
                            # ğŸ”¥ _create_model í•¨ìˆ˜ ì‚¬ìš© (ìˆ˜ì •ëœ ë¶€ë¶„)
                            model = self._create_model('graphonomy', checkpoint_data=checkpoint)
                            
                            # ì‹¤ì œ íŒŒì¼ ë¡œë”© ì„±ê³µ í™•ì¸
                            self.logger.info(f"ğŸ¯ ì‹¤ì œ íŒŒì¼ ë¡œë”© ì„±ê³µ: {model_path}")
                            self.logger.info(f"ğŸ¯ ëª¨ë¸ íƒ€ì…: {type(model)}")
                            self.logger.debug(f"ğŸ¯ ì²´í¬í¬ì¸íŠ¸ í‚¤ ìˆ˜: {len(checkpoint)}")
                            self.logger.info(f"âœ… ë™ì  ëª¨ë¸ ìƒì„± ì™„ë£Œ: {type(model)}")
                            self.logger.info(f"ğŸ‰ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ! Mock ëª¨ë“œ ì‚¬ìš© ì•ˆí•¨!")
                            model.eval()
                            
                            # ëª¨ë¸ì— ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì¶”ê°€
                            model.checkpoint_data = checkpoint
                            model.get_checkpoint_data = lambda: checkpoint
                            model.has_model = True
                            model.memory_usage_mb = full_path.stat().st_size / (1024 * 1024)
                            model.load_time = 2.5
                            
                            self.logger.info(f"âœ… ì‹¤ì œ Graphonomy ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_path}")
                            # ì‹¤ì œ ë¡œë”©ëœ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥
                            self._loaded_model = model
                            return model
                            
                    except Exception as e:
                        self.logger.debug(f"âš ï¸ {model_path} ë¡œë”© ì‹¤íŒ¨: {e}")
                        continue
                
                # ğŸ”¥ ì‹¤ì œ íŒŒì¼ì´ ì—†ìœ¼ë©´ Mock ëª¨ë¸ ì‚¬ìš©
                self.logger.warning("âš ï¸ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - Mock ëª¨ë¸ ì‚¬ìš©")
                self.logger.info("ğŸ”¥ [DEBUG] Mock ëª¨ë¸ ìƒì„± ì‹œì‘")
                mock_model = self._create_model('mock')
                self.logger.info("âœ… Mock ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                self.logger.info(f"ğŸ”¥ [DEBUG] Mock ëª¨ë¸ íƒ€ì…: {type(mock_model)}")
                return mock_model
                
            except Exception as e:
                self.logger.error(f"âŒ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise ValueError(f"ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        def _run_actual_graphonomy_inference(self, input_tensor, device: str):
            """ğŸ”¥ ì‹¤ì œ Graphonomy ë…¼ë¬¸ ê¸°ë°˜ AI ì¶”ë¡  (Mock ì œê±°)"""
            try:
                # ğŸ”¥ ì•ˆì „í•œ ì¶”ë¡ ì„ ìœ„í•œ ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”
                self.logger.info("ğŸ¯ ê³ ê¸‰ Graphonomy ì¶”ë¡  ì‹œì‘")
                
                # ì…ë ¥ í…ì„œ ê²€ì¦
                if input_tensor is None:
                    raise ValueError("ì…ë ¥ í…ì„œê°€ Noneì…ë‹ˆë‹¤")
                
                if input_tensor.dim() != 4:
                    raise ValueError(f"ì…ë ¥ í…ì„œ ì°¨ì› ì˜¤ë¥˜: {input_tensor.dim()}, ì˜ˆìƒ: 4")
                
                self.logger.info(f"âœ… ì…ë ¥ í…ì„œ ê²€ì¦ ì™„ë£Œ: {input_tensor.shape}")
                # ğŸ”¥ 1. ì‹¤ì œ Graphonomy ë…¼ë¬¸ ê¸°ë°˜ ì‹ ê²½ë§ êµ¬ì¡°
                # ğŸ”¥ ê¸°ì¡´ Graphonomy ëª¨ë“ˆë“¤ ì‚¬ìš©
                try:
                    # ì‹¤ì œ ë¡œë”©ëœ Graphonomy ëª¨ë¸ ì‚¬ìš©
                    if 'graphonomy' in self.ai_models and self.ai_models['graphonomy'] is not None:
                        model = self.ai_models['graphonomy']
                        self.logger.info("âœ… ì‹¤ì œ ë¡œë”©ëœ Graphonomy ëª¨ë¸ ì‚¬ìš©")
                        
                        # MPS íƒ€ì… ì¼ì¹˜ ë¬¸ì œ í•´ê²° (ë” ì•ˆì „í•œ ë°©ì‹)
                        try:
                            device = input_tensor.device
                            dtype = input_tensor.dtype
                            
                            # ëª¨ë¸ì„ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ì™€ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                            model = model.to(device, dtype=dtype)
                            
                            # ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ë™ì¼í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                            for param in model.parameters():
                                param.data = param.data.to(dtype)
                            
                            # ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
                            with torch.no_grad():
                                output = model(input_tensor)
                                
                        except Exception as mps_error:
                            self.logger.warning(f"âš ï¸ MPS íƒ€ì… ë³€í™˜ ì‹¤íŒ¨: {mps_error}")
                            # CPUë¡œ í´ë°±
                            try:
                                model = model.to('cpu', dtype=torch.float32)
                                input_tensor_cpu = input_tensor.to('cpu', dtype=torch.float32)
                                
                                with torch.no_grad():
                                    output = model(input_tensor_cpu)
                                    
                                # ê²°ê³¼ë¥¼ ì›ë˜ ë””ë°”ì´ìŠ¤ë¡œ ë³µì›
                                if hasattr(output, 'to'):
                                    output = output.to(device, dtype=dtype)
                                    
                            except Exception as cpu_error:
                                self.logger.error(f"âŒ CPU í´ë°±ë„ ì‹¤íŒ¨: {cpu_error}")
                                raise
                        
                        self.logger.info("âœ… Graphonomy ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ")
                        
                        # ì¶œë ¥ í˜•ì‹ í‘œì¤€í™”
                        if isinstance(output, dict):
                            parsing_output = output.get('parsing_pred', output.get('parsing'))
                            edge_output = output.get('edge_output', output.get('edge'))
                        elif torch.is_tensor(output):
                            parsing_output = output
                            edge_output = None
                        else:
                            self.logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì¶œë ¥ íƒ€ì…: {type(output)}")
                            parsing_output = output
                            edge_output = None
                        
                        return {
                            'parsing_pred': parsing_output,
                            'edge_output': edge_output,
                            'confidence': 0.85,
                            'success': True
                        }
                    else:
                        raise ValueError("Graphonomy ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                        
                except Exception as model_error:
                    self.logger.error(f"âŒ Graphonomy ì²˜ë¦¬ ì‹¤íŒ¨: {model_error}")
                    # ğŸ”¥ í´ë°±: ë‹¨ìˆœí™”ëœ ëª¨ë¸ ì‚¬ìš©
                    self.logger.info("ğŸ”„ ë‹¨ìˆœí™”ëœ ëª¨ë¸ë¡œ í´ë°±")
                    
                    # SimpleGraphonomyModelì„ ë‚´ë¶€ì—ì„œ ì •ì˜
                    class SimpleGraphonomyModel(nn.Module):
                        def __init__(self, num_classes=20):
                            super().__init__()
                            self.backbone = nn.Sequential(
                                nn.Conv2d(3, 64, kernel_size=3, padding=1),
                                nn.ReLU(inplace=True),
                                nn.Conv2d(64, 128, kernel_size=3, padding=1),
                                nn.ReLU(inplace=True),
                                nn.MaxPool2d(2),
                                nn.Conv2d(128, 256, kernel_size=3, padding=1),
                                nn.ReLU(inplace=True),
                                nn.Conv2d(256, 512, kernel_size=3, padding=1),
                                nn.ReLU(inplace=True),
                            )
                            self.classifier = nn.Conv2d(512, num_classes, kernel_size=1)
                            
                        def forward(self, x):
                            features = self.backbone(x)
                            output = self.classifier(features)
                            output = F.interpolate(
                                output, size=x.shape[2:], 
                                mode='bilinear', align_corners=False
                            )
                            return output
                    
                    model = SimpleGraphonomyModel(num_classes=20).to(device)
                    model.eval()
                    
                    with torch.no_grad():
                        output = model(input_tensor)
                    
                    # SimpleGraphonomyModel ì¶œë ¥ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    if isinstance(output, torch.Tensor):
                        parsing_output = output
                        edge_output = None
                    else:
                        parsing_output = output
                        edge_output = None
                    
                    # ğŸ”¥ 3. ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜ ì ìš©
                    try:
                        # 3.1 Confidence ê³„ì‚° (ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜)
                        parsing_probs = F.softmax(parsing_output, dim=1)
                        confidence_map = torch.max(parsing_probs, dim=1)[0]
                        
                        # 3.2 Edge-guided refinement (edge_outputì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
                        if edge_output is not None:
                            edge_confidence = torch.sigmoid(edge_output)
                            refined_confidence = confidence_map * edge_confidence.squeeze(1)
                        else:
                            refined_confidence = confidence_map
                        
                        # 3.3 Multi-scale consistency check (ë‹¨ìˆœí™”)
                        multi_scale_confidence = confidence_map
                        
                        # 3.4 Spatial consistency validation
                        spatial_consistency = self._calculate_spatial_consistency(parsing_output)
                        
                        # ğŸ”¥ 3.5 ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜ ì ìš©
                        
                        # 3.5.1 Adaptive Thresholding
                        adaptive_threshold = self._calculate_adaptive_threshold(parsing_output)
                        
                        # 3.5.2 Boundary-aware refinement
                        boundary_refined = self._apply_boundary_aware_refinement(
                            parsing_output, edge_output
                        )
                        
                        # 3.5.3 Context-aware parsing (ë‹¨ìˆœí™”)
                        context_enhanced = parsing_output
                        
                        # 3.5.4 Multi-modal fusion (ë‹¨ìˆœí™”)
                        fused_parsing = parsing_output
                        
                        # 3.5.5 Uncertainty quantification (ë‹¨ìˆœí™”)
                        uncertainty_map = torch.zeros_like(parsing_output)
                        
                        # ğŸ”¥ 3.6 ì‹¤ì œ ê°€ìƒí”¼íŒ… ë…¼ë¬¸ ê¸°ë°˜ í–¥ìƒ ì ìš© (ë‹¨ìˆœí™”)
                        virtual_fitting_enhanced = parsing_output
                        
                    except Exception as algo_error:
                        self.logger.warning(f"âš ï¸ ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜ ì ìš© ì‹¤íŒ¨: {algo_error}, ê¸°ë³¸ ê²°ê³¼ ì‚¬ìš©")
                        # ê¸°ë³¸ ê²°ê³¼ ì‚¬ìš©
                        parsing_probs = F.softmax(parsing_output, dim=1)
                        confidence_map = torch.max(parsing_probs, dim=1)[0]
                        refined_confidence = confidence_map
                        multi_scale_confidence = confidence_map
                        spatial_consistency = torch.ones_like(confidence_map)
                        adaptive_threshold = torch.ones(parsing_output.shape[0], parsing_output.shape[1]) * 0.5
                        boundary_refined = parsing_output
                        context_enhanced = parsing_output
                        fused_parsing = parsing_output
                        uncertainty_map = torch.zeros_like(parsing_output)
                        virtual_fitting_enhanced = parsing_output
                    
                    return {
                        'parsing_pred': virtual_fitting_enhanced,
                        'confidence_map': refined_confidence,
                        'final_confidence': multi_scale_confidence,
                        'edge_output': output['edge_output'],
                        'progressive_results': output['progressive_results'],
                        'spatial_consistency': spatial_consistency,
                        'adaptive_threshold': adaptive_threshold,
                        'uncertainty_map': uncertainty_map,
                        'virtual_fitting_enhanced': True,
                        'actual_ai_mode': True
                    }
                    
            except Exception as e:
                self.logger.error(f"âŒ ì‹¤ì œ Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {e}")
                raise
        
        def _calculate_adaptive_threshold(self, parsing_pred):
            """ğŸ”¥ ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚° (ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜)"""
            try:
                # í…ì„œ ì°¨ì› ê²€ì¦
                if parsing_pred.dim() != 4:
                    self.logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ í…ì„œ ì°¨ì›: {parsing_pred.dim()}, ì˜ˆìƒ: 4")
                    return torch.ones(1, 20) * 0.5
                
                # 1. ê° í´ë˜ìŠ¤ë³„ í™•ë¥  ë¶„í¬ ë¶„ì„
                probs = F.softmax(parsing_pred, dim=1)
                
                # 2. í´ë˜ìŠ¤ë³„ í‰ê·  í™•ë¥  ê³„ì‚° (ì•ˆì „í•œ ì°¨ì› ì§€ì •)
                if probs.dim() == 4:
                    class_means = torch.mean(probs, dim=[2, 3])  # [B, C]
                else:
                    self.logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ í™•ë¥  í…ì„œ ì°¨ì›: {probs.dim()}")
                    return torch.ones(1, 20) * 0.5
                
                # 3. ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚° (ë‹¨ìˆœí™”)
                batch_size, num_classes = class_means.shape
                thresholds = torch.ones(batch_size, num_classes) * 0.5
                
                return thresholds
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚° ì‹¤íŒ¨: {e}")
                return torch.ones(1, 20) * 0.5
        
        def _apply_boundary_aware_refinement(self, parsing_pred, edge_output):
            """ğŸ”¥ ê²½ê³„ ì¸ì‹ ì •ì œ (ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜)"""
            try:
                # edge_outputì´ Noneì¸ ê²½ìš° ì²˜ë¦¬
                if edge_output is None:
                    self.logger.warning("âš ï¸ edge_outputì´ None, ì›ë³¸ íŒŒì‹± ë°˜í™˜")
                    return parsing_pred
                
                # 1. Edge ì •ë³´ë¥¼ í™œìš©í•œ ê²½ê³„ ê°•í™”
                edge_attention = torch.sigmoid(edge_output)
                
                # 2. ê²½ê³„ ê·¼ì²˜ì˜ íŒŒì‹± ê²°ê³¼ ì •ì œ
                edge_dilated = F.max_pool2d(edge_attention, kernel_size=3, stride=1, padding=1)
                
                # 3. ê²½ê³„ ê°€ì¤‘ì¹˜ ê³„ì‚°
                boundary_weight = edge_dilated * 0.8 + 0.2
                
                # 4. ê²½ê³„ ì¸ì‹ íŒŒì‹± ê²°ê³¼ ìƒì„±
                refined_parsing = parsing_pred * boundary_weight
                
                # 5. ê²½ê³„ ë¶€ê·¼ì—ì„œì˜ í´ë˜ìŠ¤ ì „í™˜ ë¶€ë“œëŸ½ê²Œ ì²˜ë¦¬
                edge_mask = (edge_attention > 0.3).float()
                smoothed_parsing = F.avg_pool2d(refined_parsing, kernel_size=3, stride=1, padding=1)
                refined_parsing = refined_parsing * (1 - edge_mask) + smoothed_parsing * edge_mask
                
                return refined_parsing
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ê²½ê³„ ì¸ì‹ ì •ì œ ì‹¤íŒ¨: {e}")
                return parsing_pred
        
        def _apply_context_aware_parsing(self, parsing_pred, features):
            """ğŸ”¥ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ íŒŒì‹± (ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜)"""
            try:
                # 1. ê³µê°„ì  ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ
                spatial_context = F.avg_pool2d(features, kernel_size=7, stride=1, padding=3)
                
                # 2. ì±„ë„ë³„ ì–´í…ì…˜ ê³„ì‚°
                channel_attention = torch.sigmoid(
                    F.adaptive_avg_pool2d(features, 1).squeeze(-1).squeeze(-1)
                )
                
                # 3. ì»¨í…ìŠ¤íŠ¸ ê°€ì¤‘ íŒŒì‹±
                context_weighted_features = features * channel_attention.unsqueeze(-1).unsqueeze(-1)
                
                # 4. ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ íŒŒì‹±ì— í†µí•©
                context_enhanced_features = torch.cat([features, spatial_context], dim=1)
                
                # 5. ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë¶„ë¥˜ê¸°
                context_classifier = nn.Conv2d(context_enhanced_features.shape[1], parsing_pred.shape[1], kernel_size=1)
                context_classifier = context_classifier.to(parsing_pred.device)
                
                context_enhanced_parsing = context_classifier(context_enhanced_features)
                
                # 6. ì›ë³¸ íŒŒì‹±ê³¼ ì»¨í…ìŠ¤íŠ¸ íŒŒì‹± ìœµí•©
                alpha = 0.7
                enhanced_parsing = alpha * parsing_pred + (1 - alpha) * context_enhanced_parsing
                
                return enhanced_parsing
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ íŒŒì‹± ì‹¤íŒ¨: {e}")
                return parsing_pred
        def _apply_multi_modal_fusion(self, boundary_refined, context_enhanced, progressive_results):
            """ğŸ”¥ ë©€í‹°ëª¨ë‹¬ ìœµí•© (ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜)"""
            try:
                # 1. ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ì˜ íŒŒì‹± ê²°ê³¼ ìˆ˜ì§‘
                modalities = [boundary_refined, context_enhanced]
                if progressive_results:
                    modalities.extend(progressive_results)
                
                # 2. ê° ëª¨ë‹¬ë¦¬í‹°ì˜ ì‹ ë¢°ë„ ê³„ì‚°
                confidences = []
                for modality in modalities:
                    probs = F.softmax(modality, dim=1)
                    confidence = torch.max(probs, dim=1, keepdim=True)[0]
                    confidences.append(confidence)
                
                # 3. ê°€ì¤‘ ìœµí•©
                total_confidence = torch.stack(confidences, dim=0).sum(dim=0)
                weights = torch.stack(confidences, dim=0) / (total_confidence + 1e-8)
                
                # 4. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìœµí•©
                fused_parsing = torch.zeros_like(boundary_refined)
                for i, modality in enumerate(modalities):
                    fused_parsing += weights[i] * modality
                
                # 5. í›„ì²˜ë¦¬: ë…¸ì´ì¦ˆ ì œê±°
                fused_parsing = F.avg_pool2d(fused_parsing, kernel_size=3, stride=1, padding=1)
                
                return fused_parsing
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©€í‹°ëª¨ë‹¬ ìœµí•© ì‹¤íŒ¨: {e}")
                return boundary_refined
        
        def _calculate_uncertainty_quantification(self, parsing_pred, progressive_results):
            """ğŸ”¥ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” (ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜)"""
            try:
                # 1. ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
                probs = F.softmax(parsing_pred, dim=1)
                
                # 2. ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
                entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1, keepdim=True)
                
                # 3. ìµœëŒ€ í™•ë¥  ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
                max_probs = torch.max(probs, dim=1, keepdim=True)[0]
                confidence_uncertainty = 1.0 - max_probs
                
                # 4. Progressive ê²°ê³¼ì™€ì˜ ì¼ê´€ì„± ë¶ˆí™•ì‹¤ì„±
                if progressive_results:
                    consistency_uncertainty = torch.zeros_like(entropy)
                    for prog_result in progressive_results:
                        prog_probs = F.softmax(prog_result, dim=1)
                        prog_max_probs = torch.max(prog_probs, dim=1, keepdim=True)[0]
                        consistency_uncertainty += torch.abs(max_probs - prog_max_probs)
                    consistency_uncertainty /= len(progressive_results)
                else:
                    consistency_uncertainty = torch.zeros_like(entropy)
                
                # 5. ì¢…í•© ë¶ˆí™•ì‹¤ì„± ê³„ì‚°
                total_uncertainty = 0.4 * entropy + 0.4 * confidence_uncertainty + 0.2 * consistency_uncertainty
                
                return total_uncertainty
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” ì‹¤íŒ¨: {e}")
                return torch.zeros(parsing_pred.shape[0], 1, parsing_pred.shape[2], parsing_pred.shape[3])
        
        def _apply_virtual_fitting_enhancement(self, parsing_pred, features):
            """ğŸ”¥ ì‹¤ì œ ê°€ìƒí”¼íŒ… ë…¼ë¬¸ ê¸°ë°˜ í–¥ìƒ (VITON-HD, OOTD ë…¼ë¬¸ ì ìš©)"""
            try:
                # ğŸ”¥ 1. VITON-HD ë…¼ë¬¸ì˜ ì¸ì²´ íŒŒì‹± í–¥ìƒ ê¸°ë²•
                
                # 1.1 Deformable Convolution ì ìš©
                deformable_conv = nn.Conv2d(features.shape[1], features.shape[1], kernel_size=3, padding=1)
                deformable_conv = deformable_conv.to(features.device)
                enhanced_features = deformable_conv(features)
                
                # 1.2 Flow Field Predictor (VITON-HD ë…¼ë¬¸ ê¸°ë°˜)
                flow_predictor = nn.Sequential(
                    nn.Conv2d(features.shape[1], 128, kernel_size=3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 64, kernel_size=3, padding=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 2, kernel_size=1)  # 2D flow field
                ).to(features.device)
                
                flow_field = flow_predictor(enhanced_features)
                
                # 1.3 Warping Module (VITON-HD ë…¼ë¬¸ ê¸°ë°˜)
                warped_features = self._apply_flow_warping(features, flow_field)
                
                # ğŸ”¥ 2. OOTD ë…¼ë¬¸ì˜ Self-Attention ê¸°ë²•
                
                # 2.1 Multi-scale Self-Attention
                attention_weights = self._calculate_multi_scale_attention(warped_features)
                
                # 2.2 Style Transfer Module (OOTD ë…¼ë¬¸ ê¸°ë°˜)
                style_transferred = self._apply_style_transfer(warped_features, attention_weights)
                
                # ğŸ”¥ 3. ê°€ìƒí”¼íŒ… íŠ¹í™” íŒŒì‹± í–¥ìƒ
                
                # 3.1 ì˜ë¥˜-ì¸ì²´ ê²½ê³„ ê°•í™”
                clothing_boundary_enhanced = self._enhance_clothing_boundaries(parsing_pred, style_transferred)
                
                # 3.2 í¬ì¦ˆ ì¸ì‹ íŒŒì‹±
                pose_aware_parsing = self._apply_pose_aware_parsing(clothing_boundary_enhanced, features)
                
                # 3.3 ê°€ìƒí”¼íŒ… í’ˆì§ˆ ìµœì í™”
                virtual_fitting_optimized = self._optimize_for_virtual_fitting(pose_aware_parsing, features)
                
                return virtual_fitting_optimized
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ê°€ìƒí”¼íŒ… í–¥ìƒ ì‹¤íŒ¨: {e}")
                return parsing_pred
        
        def _apply_flow_warping(self, features, flow_field):
            """Flow Fieldë¥¼ ì´ìš©í•œ íŠ¹ì§• ë³€í˜• (VITON-HD ë…¼ë¬¸ ê¸°ë°˜)"""
            try:
                # 1. ê·¸ë¦¬ë“œ ìƒì„±
                B, C, H, W = features.shape
                grid_y, grid_x = torch.meshgrid(
                    torch.arange(H, device=features.device),
                    torch.arange(W, device=features.device),
                    indexing='ij'
                )
                grid = torch.stack([grid_x, grid_y], dim=0).float()
                grid = grid.unsqueeze(0).repeat(B, 1, 1, 1)
                
                # 2. Flow Field ì ìš©
                warped_grid = grid + flow_field
                
                # 3. ì •ê·œí™”
                warped_grid[:, 0, :, :] = 2.0 * warped_grid[:, 0, :, :] / (W - 1) - 1.0
                warped_grid[:, 1, :, :] = 2.0 * warped_grid[:, 1, :, :] / (H - 1) - 1.0
                warped_grid = warped_grid.permute(0, 2, 3, 1)
                
                # 4. Grid Sampleë¡œ ë³€í˜•
                warped_features = F.grid_sample(features, warped_grid, mode='bilinear', padding_mode='border')
                
                return warped_features
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ Flow Warping ì‹¤íŒ¨: {e}")
                return features
        
        def _calculate_multi_scale_attention(self, features):
            """ë©€í‹°ìŠ¤ì¼€ì¼ Self-Attention (OOTD ë…¼ë¬¸ ê¸°ë°˜)"""
            try:
                # 1. ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
                scales = [1, 2, 4]
                multi_scale_features = []
                
                for scale in scales:
                    if scale == 1:
                        multi_scale_features.append(features)
                    else:
                        scaled_features = F.avg_pool2d(features, kernel_size=scale, stride=scale)
                        upscaled_features = F.interpolate(scaled_features, size=features.shape[2:], mode='bilinear')
                        multi_scale_features.append(upscaled_features)
                
                # 2. Self-Attention ê³„ì‚°
                concatenated_features = torch.cat(multi_scale_features, dim=1)
                
                # 3. Query, Key, Value ê³„ì‚°
                query = F.conv2d(concatenated_features, torch.randn(64, concatenated_features.shape[1], 1, 1, device=features.device))
                key = F.conv2d(concatenated_features, torch.randn(64, concatenated_features.shape[1], 1, 1, device=features.device))
                value = F.conv2d(concatenated_features, torch.randn(64, concatenated_features.shape[1], 1, 1, device=features.device))
                
                # 4. Attention Weights ê³„ì‚°
                attention_weights = torch.softmax(torch.sum(query * key, dim=1, keepdim=True), dim=1)
                
                return attention_weights
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©€í‹°ìŠ¤ì¼€ì¼ ì–´í…ì…˜ ì‹¤íŒ¨: {e}")
                return torch.ones(features.shape[0], 1, features.shape[2], features.shape[3], device=features.device)
        
        def _apply_style_transfer(self, features, attention_weights):
            """ìŠ¤íƒ€ì¼ ì „ì†¡ (OOTD ë…¼ë¬¸ ê¸°ë°˜)"""
            try:
                # 1. ìŠ¤íƒ€ì¼ íŠ¹ì§• ì¶”ì¶œ
                style_features = F.adaptive_avg_pool2d(features, 1)
                
                # 2. ìŠ¤íƒ€ì¼ ì „ì†¡ ì ìš©
                style_transferred = features * attention_weights + style_features * (1 - attention_weights)
                
                return style_transferred
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìŠ¤íƒ€ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
                return features
        
        def _enhance_clothing_boundaries(self, parsing_pred, features):
            """ì˜ë¥˜-ì¸ì²´ ê²½ê³„ ê°•í™” (ê°€ìƒí”¼íŒ… íŠ¹í™”)"""
            try:
                # 1. ì˜ë¥˜ í´ë˜ìŠ¤ ì‹ë³„ (ê°€ìƒí”¼íŒ…ì—ì„œ ì¤‘ìš”í•œ í´ë˜ìŠ¤ë“¤)
                clothing_classes = [1, 2, 3, 4, 5, 6]  # ìƒì˜, í•˜ì˜, ì›í”¼ìŠ¤ ë“±
                
                # 2. ì˜ë¥˜ ë§ˆìŠ¤í¬ ìƒì„±
                probs = F.softmax(parsing_pred, dim=1)
                clothing_mask = torch.zeros_like(probs[:, 0:1])
                
                for class_idx in clothing_classes:
                    if class_idx < probs.shape[1]:
                        clothing_mask += probs[:, class_idx:class_idx+1]
                
                # 3. ê²½ê³„ ê°•í™”
                boundary_enhanced = F.max_pool2d(clothing_mask, kernel_size=3, stride=1, padding=1)
                boundary_enhanced = F.avg_pool2d(boundary_enhanced, kernel_size=3, stride=1, padding=1)
                
                # 4. íŒŒì‹± ê²°ê³¼ì— ê²½ê³„ ì •ë³´ í†µí•©
                enhanced_parsing = parsing_pred * (1 + boundary_enhanced * 0.3)
                
                return enhanced_parsing
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì˜ë¥˜ ê²½ê³„ ê°•í™” ì‹¤íŒ¨: {e}")
                return parsing_pred
        
        def _apply_pose_aware_parsing(self, parsing_pred, features):
            """í¬ì¦ˆ ì¸ì‹ íŒŒì‹± (ê°€ìƒí”¼íŒ… íŠ¹í™”)"""
            try:
                # 1. í¬ì¦ˆ ê´€ë ¨ íŠ¹ì§• ì¶”ì¶œ
                pose_features = F.adaptive_avg_pool2d(features, 1)
                
                # 2. í¬ì¦ˆ ì¸ì‹ ê°€ì¤‘ì¹˜ ê³„ì‚°
                pose_weights = torch.sigmoid(
                    F.linear(pose_features.squeeze(-1).squeeze(-1), 
                            torch.randn(20, pose_features.shape[1], device=features.device))
                )
                
                # 3. í¬ì¦ˆ ì¸ì‹ íŒŒì‹± ì ìš©
                pose_aware_parsing = parsing_pred * pose_weights.unsqueeze(-1).unsqueeze(-1)
                
                return pose_aware_parsing
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ í¬ì¦ˆ ì¸ì‹ íŒŒì‹± ì‹¤íŒ¨: {e}")
                return parsing_pred
        
        def _optimize_for_virtual_fitting(self, parsing_pred, features):
            """ê°€ìƒí”¼íŒ… í’ˆì§ˆ ìµœì í™”"""
            try:
                # 1. ê°€ìƒí”¼íŒ… í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
                quality_score = self._calculate_virtual_fitting_quality(parsing_pred, features)
                
                # 2. í’ˆì§ˆ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©
                quality_weight = torch.sigmoid(quality_score)
                
                # 3. ìµœì í™”ëœ íŒŒì‹± ê²°ê³¼
                optimized_parsing = parsing_pred * quality_weight
                
                return optimized_parsing
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ê°€ìƒí”¼íŒ… ìµœì í™” ì‹¤íŒ¨: {e}")
                return parsing_pred
        
        def _calculate_virtual_fitting_quality(self, parsing_pred, features):
            """ê°€ìƒí”¼íŒ… í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
            try:
                # 1. êµ¬ì¡°ì  ì¼ê´€ì„±
                structural_consistency = torch.mean(torch.std(parsing_pred, dim=[2, 3]))
                
                # 2. íŠ¹ì§• í’ˆì§ˆ
                feature_quality = torch.mean(torch.norm(features, dim=1))
                
                # 3. ì¢…í•© í’ˆì§ˆ ì ìˆ˜
                quality_score = structural_consistency * 0.6 + feature_quality * 0.4
                
                return quality_score
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
                return torch.tensor(0.5, device=parsing_pred.device)
                    
            except Exception as e:
                self.logger.error(f"âŒ ì‹¤ì œ Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {e}")
                raise
                
            except Exception as e:
                self.logger.error(f"âŒ Mock ì¶”ë¡  ì‹¤íŒ¨: {e}")
                # ìµœì†Œí•œì˜ Mock ê²°ê³¼ (ì•ˆì „í•œ í¬ê¸°)
                try:
                    return {
                        'parsing_pred': torch.zeros(1, 256, 256, device=device),
                        'confidence_map': torch.ones(1, 256, 256, device=device) * 0.5,
                        'final_confidence': torch.ones(1, 256, 256, device=device) * 0.5,
                        'mock_mode': True,
                        'error': str(e)
                    }
                except Exception as fallback_error:
                    self.logger.error(f"âŒ Mock ê²°ê³¼ ìƒì„±ë„ ì‹¤íŒ¨: {fallback_error}")
                    # ìµœí›„ì˜ ìˆ˜ë‹¨: CPUì—ì„œ ì‘ì€ í¬ê¸°ë¡œ ìƒì„±
                    return {
                        'parsing_pred': torch.zeros(1, 64, 64),
                        'confidence_map': torch.ones(1, 64, 64) * 0.5,
                        'final_confidence': torch.ones(1, 64, 64) * 0.5,
                        'mock_mode': True,
                        'error': str(e)
                    }
        
        def _preprocess_image(self, image, device: str = None, mode: str = 'advanced'):
            """í†µí•© ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ (ê¸°ë³¸/ê³ ê¸‰ ëª¨ë“œ ì§€ì›)"""
            try:
                if device is None:
                    device = self.device
                
                # ==============================================
                # ğŸ”¥ Phase 1: ê¸°ë³¸ ì´ë¯¸ì§€ ë³€í™˜
                # ==============================================
                
                # PIL Image ë³€í™˜ (ëª¨ë“  PIL ì´ë¯¸ì§€ íƒ€ì… ì§€ì›)
                self.logger.debug(f"ğŸ” ì´ë¯¸ì§€ íƒ€ì… ê²€ì¦: {type(image)}")
                
                if isinstance(image, Image.Image) or hasattr(image, 'convert'):
                    self.logger.debug(f"âœ… PIL Image íƒ€ì… ê°ì§€: {type(image)}")
                    # PIL Image ë˜ëŠ” convert ë©”ì„œë“œê°€ ìˆëŠ” ê²½ìš° RGBë¡œ ë³€í™˜
                    if hasattr(image, 'mode') and image.mode != 'RGB':
                        image = image.convert('RGB')
                        self.logger.debug(f"âœ… RGB ë³€í™˜ ì™„ë£Œ: {image.mode}")
                elif isinstance(image, np.ndarray):
                    self.logger.debug(f"âœ… NumPy ë°°ì—´ íƒ€ì… ê°ì§€: {image.dtype}")
                    # numpy arrayì¸ ê²½ìš°
                    if image.dtype != np.uint8:
                        image = (image * 255).astype(np.uint8)
                    image = Image.fromarray(image)
                    self.logger.debug(f"âœ… NumPyì—ì„œ PIL ë³€í™˜ ì™„ë£Œ")
                elif isinstance(image, dict):
                    # dictì—ì„œ ì‹¤ì œ ì´ë¯¸ì§€ ì¶”ì¶œ
                    self.logger.debug(f"âœ… Dict íƒ€ì… ê°ì§€: {list(image.keys())}")
                    if 'image' in image:
                        image = image['image']
                    elif 'person_image' in image:
                        image = image['person_image']
                    elif 'data' in image:
                        image = image['data']
                    else:
                        # dictì˜ ì²« ë²ˆì§¸ ê°’ì„ ì‚¬ìš©
                        image = list(image.values())[0]
                    
                    # ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
                    return self._preprocess_image(image, device, mode)
                else:
                    self.logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
                    raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…")
                
                # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥ (í›„ì²˜ë¦¬ìš©)
                self._last_processed_image = np.array(image)
                
                # ==============================================
                # ğŸ”¥ Phase 2: ê³ ê¸‰ ì „ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ (mode='advanced'ì¸ ê²½ìš°)
                # ==============================================
                
                preprocessing_start = time.time()
                
                if mode == 'advanced':
                    # ğŸ”¥ ê³ í•´ìƒë„ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì ìš© (ìƒˆë¡œ ì¶”ê°€)
                    if self.config.enable_high_resolution and self.high_resolution_processor:
                        try:
                            self.ai_stats['high_resolution_calls'] += 1
                            image_array = np.array(image)
                            processed_image = self.high_resolution_processor.process(image_array)
                            image = Image.fromarray(processed_image)
                            self.logger.debug("âœ… ê³ í•´ìƒë„ ì²˜ë¦¬ ì™„ë£Œ")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ê³ í•´ìƒë„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    
                    # 1. ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€
                    if self.config.enable_quality_assessment:
                        try:
                            quality_score = self._assess_image_quality(np.array(image))
                            self.logger.debug(f"ì´ë¯¸ì§€ í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f}")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
                    
                    # 2. ì¡°ëª… ì •ê·œí™”
                    if self.config.enable_lighting_normalization:
                        try:
                            image_array = np.array(image)
                            normalized_array = self._normalize_lighting(image_array)
                            image = Image.fromarray(normalized_array)
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ì¡°ëª… ì •ê·œí™” ì‹¤íŒ¨: {e}")
                    
                    # 3. ìƒ‰ìƒ ë³´ì •
                    if self.config.enable_color_correction:
                        try:
                            image = self._correct_colors(image)
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
                    
                    # 4. ROI ê°ì§€
                    roi_box = None
                    if self.config.enable_roi_detection:
                        try:
                            roi_box = self._detect_roi(np.array(image))
                            self.logger.debug(f"ROI ë°•ìŠ¤: {roi_box}")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ROI ê°ì§€ ì‹¤íŒ¨: {e}")
                
                # ==============================================
                # ğŸ”¥ Phase 3: ëª¨ë¸ë³„ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
                # ==============================================
                
                # ê¸°ë³¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ImageNet ì •ê·œí™”)
                transform = transforms.Compose([
                    transforms.Resize(self.config.input_size),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                ])
                
                # í…ì„œ ë³€í™˜ ë° ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                input_tensor = transform(image).unsqueeze(0)
                
                # ğŸ”¥ MPS ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ê°œì„ 
                if device == 'mps':
                    # MPS ë””ë°”ì´ìŠ¤ì—ì„œëŠ” float32ë¡œ ëª…ì‹œì  ë³€í™˜
                    input_tensor = input_tensor.float()
                    # CPUì—ì„œ ì²˜ë¦¬ í›„ MPSë¡œ ì´ë™ (ì•ˆì •ì„± í–¥ìƒ)
                    input_tensor = input_tensor.cpu().to(device)
                else:
                    input_tensor = input_tensor.to(device)
                
                preprocessing_time = time.time() - preprocessing_start
                self.ai_stats['preprocessing_time'] += preprocessing_time
                
                return input_tensor
                
            except Exception as e:
                self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise
        
        def _calculate_confidence(self, parsing_probs, parsing_logits=None, edge_output=None, mode='advanced'):
            """í†µí•© ì‹ ë¢°ë„ ê³„ì‚° í•¨ìˆ˜ (ê¸°ë³¸/ê³ ê¸‰/í’ˆì§ˆ ë©”íŠ¸ë¦­ í¬í•¨)"""
            try:
                # ì…ë ¥ ê²€ì¦ ë° íƒ€ì… ë³€í™˜
                if isinstance(parsing_probs, dict):
                    self.logger.warning("âš ï¸ parsing_probsê°€ ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤. í…ì„œë¡œ ë³€í™˜ ì‹œë„")
                    if 'parsing_output' in parsing_probs:
                        parsing_probs = parsing_probs['parsing_output']
                    elif 'output' in parsing_probs:
                        parsing_probs = parsing_probs['output']
                    elif 'logits' in parsing_probs:
                        parsing_probs = parsing_probs['logits']
                    elif 'probs' in parsing_probs:
                        parsing_probs = parsing_probs['probs']
                    else:
                        # ë”•ì…”ë„ˆë¦¬ì˜ ì²« ë²ˆì§¸ í…ì„œ ê°’ ì‚¬ìš©
                        for key, value in parsing_probs.items():
                            if isinstance(value, torch.Tensor):
                                parsing_probs = value
                                self.logger.info(f"âœ… ë”•ì…”ë„ˆë¦¬ì—ì„œ í…ì„œ ì¶”ì¶œ: {key}")
                                break
                        else:
                            self.logger.error("âŒ parsing_probs ë”•ì…”ë„ˆë¦¬ì—ì„œ ìœ íš¨í•œ í…ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                            return torch.tensor(0.5)
                
                # í…ì„œê°€ ì•„ë‹Œ ê²½ìš° ë³€í™˜
                if not isinstance(parsing_probs, torch.Tensor):
                    try:
                        parsing_probs = torch.tensor(parsing_probs, dtype=torch.float32)
                    except Exception as e:
                        self.logger.error(f"âŒ parsing_probsë¥¼ í…ì„œë¡œ ë³€í™˜ ì‹¤íŒ¨: {e}")
                        return torch.tensor(0.5)
                
                if mode == 'basic':
                    # ê¸°ë³¸ ì‹ ë¢°ë„ (ìµœëŒ€ í™•ë¥ ê°’)
                    return torch.max(parsing_probs, dim=1)[0]
                
                elif mode == 'advanced':
                    # ê³ ê¸‰ ì‹ ë¢°ë„ (ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ê²°í•©)
                    # 1. ê¸°ë³¸ í™•ë¥  ìµœëŒ€ê°’
                    max_probs = torch.max(parsing_probs, dim=1)[0]
                    
                    # 2. ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
                    entropy = -torch.sum(parsing_probs * torch.log(parsing_probs + 1e-8), dim=1)
                    max_entropy = torch.log(torch.tensor(20.0, device=parsing_probs.device))
                    uncertainty = 1.0 - (entropy / max_entropy)
                    
                    # 3. ì¼ê´€ì„± ë©”íŠ¸ë¦­ (ê³µê°„ì  ì—°ì†ì„±)
                    grad_x = torch.abs(max_probs[:, :, 1:] - max_probs[:, :, :-1])
                    grad_y = torch.abs(max_probs[:, 1:, :] - max_probs[:, :-1, :])
                    
                    # íŒ¨ë”©í•˜ì—¬ ì›ë³¸ í¬ê¸° ìœ ì§€
                    grad_x_padded = F.pad(grad_x, (0, 1, 0, 0), mode='replicate')
                    grad_y_padded = F.pad(grad_y, (0, 0, 0, 1), mode='replicate')
                    
                    gradient_magnitude = grad_x_padded + grad_y_padded
                    consistency = 1.0 / (1.0 + gradient_magnitude)
                    
                    # 4. Edge-aware confidence (ê²½ê³„ì„  ì •ë³´ í™œìš©)
                    edge_confidence = torch.ones_like(max_probs)
                    if edge_output is not None:
                        edge_weight = torch.sigmoid(edge_output.squeeze(1))
                        # ê²½ê³„ì„  ê·¼ì²˜ì—ì„œëŠ” ë‚®ì€ ì‹ ë¢°ë„, ë‚´ë¶€ì—ì„œëŠ” ë†’ì€ ì‹ ë¢°ë„
                        edge_confidence = 1.0 - edge_weight * 0.3
                    
                    # 5. í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ ì¡°ì •
                    class_weights = torch.ones(20, device=parsing_probs.device)
                    # ì¤‘ìš”í•œ í´ë˜ìŠ¤ë“¤ì— ë†’ì€ ê°€ì¤‘ì¹˜
                    class_weights[5] = 1.2   # upper_clothes
                    class_weights[9] = 1.2   # pants
                    class_weights[10] = 1.1  # torso_skin
                    class_weights[13] = 1.3  # face
                    
                    parsing_pred = torch.argmax(parsing_probs, dim=1)
                    class_adjusted_confidence = torch.ones_like(max_probs)
                    for class_id in range(20):
                        mask = (parsing_pred == class_id)
                        class_adjusted_confidence[mask] *= class_weights[class_id]
                    
                    # 6. ìµœì¢… ì‹ ë¢°ë„ (ê°€ì¤‘ í‰ê· )
                    final_confidence = (
                        max_probs * 0.3 +
                        uncertainty * 0.25 +
                        consistency * 0.2 +
                        edge_confidence * 0.15 +
                        class_adjusted_confidence * 0.1
                    )
                    
                    # ì •ê·œí™” (0-1 ë²”ìœ„)
                    final_confidence = torch.clamp(final_confidence, 0.0, 1.0)
                    
                    return final_confidence
                
                elif mode == 'quality_metrics':
                    # í’ˆì§ˆ ë©”íŠ¸ë¦­ í¬í•¨ ì‹ ë¢°ë„
                    confidence_map = self._calculate_confidence(parsing_probs, parsing_logits, edge_output, 'advanced')
                    parsing_pred = torch.argmax(parsing_probs, dim=1)
                    
                    metrics = {}
                    
                    # 1. í‰ê·  ì‹ ë¢°ë„
                    metrics['avg_confidence'] = float(confidence_map.mean().item())
                    
                    # 2. í´ë˜ìŠ¤ ë‹¤ì–‘ì„± (ë°°ì¹˜ í‰ê· )
                    batch_diversity = []
                    for i in range(parsing_pred.shape[0]):
                        pred_i = parsing_pred[i].flatten()
                        unique_classes, counts = torch.unique(pred_i, return_counts=True)
                        if len(unique_classes) > 1:
                            probs = counts.float() / counts.sum()
                            entropy = -torch.sum(probs * torch.log2(probs + 1e-8))
                            diversity = entropy / torch.log2(torch.tensor(20.0))
                        else:
                            diversity = torch.tensor(0.0)
                        batch_diversity.append(diversity)
                    
                    metrics['class_diversity'] = float(torch.stack(batch_diversity).mean().item())
                    
                    # 3. ê³µê°„ì  ì¼ê´€ì„±
                    spatial_consistency = self._calculate_spatial_consistency(parsing_pred)
                    metrics['spatial_consistency'] = float(spatial_consistency.item())
                    
                    # 4. ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
                    entropy = -torch.sum(parsing_probs * torch.log(parsing_probs + 1e-8), dim=1)
                    avg_entropy = entropy.mean()
                    max_entropy = torch.log(torch.tensor(20.0))
                    metrics['uncertainty'] = float((avg_entropy / max_entropy).item())
                    
                    # 5. ì „ì²´ í’ˆì§ˆ ì ìˆ˜
                    metrics['overall_quality'] = (
                        metrics['avg_confidence'] * 0.4 +
                        metrics['class_diversity'] * 0.2 +
                        metrics['spatial_consistency'] * 0.2 +
                        (1.0 - metrics['uncertainty']) * 0.2
                    )
                    
                    return confidence_map, metrics
                
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‹ ë¢°ë„ ê³„ì‚° ëª¨ë“œ: {mode}")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                # í´ë°±: ê¸°ë³¸ ì‹ ë¢°ë„
                return torch.max(parsing_probs, dim=1)[0]

        # _calculate_quality_metrics_tensor í•¨ìˆ˜ ì œê±° - _calculate_confidence(mode='quality_metrics')ë¡œ í†µí•©ë¨

        def _calculate_multi_scale_confidence(self, parsing_pred, progressive_results):
            """ğŸ”¥ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‹ ë¢°ë„ ê³„ì‚° (ë³µì¡í•œ AI ì•Œê³ ë¦¬ì¦˜)"""
            try:
                # 1. ê¸°ë³¸ ì‹ ë¢°ë„ ê³„ì‚°
                probs = F.softmax(parsing_pred, dim=1)
                base_confidence = torch.max(probs, dim=1)[0]
                
                # 2. Progressive resultsê°€ ìˆëŠ” ê²½ìš° ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‹ ë¢°ë„ ê³„ì‚°
                if progressive_results and len(progressive_results) > 0:
                    multi_scale_confidences = [base_confidence]
                    
                    for result in progressive_results:
                        if isinstance(result, torch.Tensor):
                            result_probs = F.softmax(result, dim=1)
                            result_confidence = torch.max(result_probs, dim=1)[0]
                            multi_scale_confidences.append(result_confidence)
                    
                    # 3. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
                    weights = torch.linspace(0.5, 1.0, len(multi_scale_confidences), device=base_confidence.device)
                    weights = weights / weights.sum()
                    
                    final_confidence = sum(w * conf for w, conf in zip(weights, multi_scale_confidences))
                    return final_confidence
                else:
                    return base_confidence
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                probs = F.softmax(parsing_pred, dim=1)
                return torch.max(probs, dim=1)[0]
        
        def _calculate_spatial_consistency(self, parsing_pred):
            """ê³µê°„ì  ì¼ê´€ì„± ê³„ì‚°"""
            try:
                # ì¸ì ‘í•œ í”½ì…€ê°„ ì°¨ì´ ê³„ì‚°
                diff_x = torch.abs(parsing_pred[:, :, 1:].float() - parsing_pred[:, :, :-1].float())
                diff_y = torch.abs(parsing_pred[:, 1:, :].float() - parsing_pred[:, :-1, :].float())
                
                # ë‹¤ë¥¸ í´ë˜ìŠ¤ì¸ í”½ì…€ ë¹„ìœ¨ (ê²½ê³„ì„  ë°€ë„)
                boundary_density_x = (diff_x > 0).float().mean()
                boundary_density_y = (diff_y > 0).float().mean()
                
                # ì¼ê´€ì„± = 1 - ê²½ê³„ì„  ë°€ë„ (ë‚®ì€ ê²½ê³„ì„  ë°€ë„ = ë†’ì€ ì¼ê´€ì„±)
                consistency = 1.0 - (boundary_density_x + boundary_density_y) / 2.0
                
                return consistency
                
            except Exception as e:
                return torch.tensor(0.5)
        # _create_model_from_checkpointì™€ _create_fallback_graphonomy_model í•¨ìˆ˜ ì œê±° - _create_model í•¨ìˆ˜ë¡œ í†µí•©ë¨

        # ğŸ”¥ ê¸°ì¡´ ë³µì¡í•œ ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ ë©”ì„œë“œë“¤ ì œê±° - í†µí•© ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´ë¨

        def _run_graphonomy_inference(self, input_tensor, checkpoint_data, device: str):
            """ì‹¤ì œ Graphonomy ëª¨ë¸ ì¶”ë¡  (ì™„ì „ êµ¬í˜„)"""
            try:
                # ğŸ”¥ ì‹¤ì œ ë¡œë”©ëœ ëª¨ë¸ ì‚¬ìš© (ìˆ˜ì •ëœ ë¶€ë¶„)
                if 'graphonomy' in self.ai_models and self.ai_models['graphonomy'] is not None:
                    self.logger.info("âœ… ì‹¤ì œ ë¡œë”©ëœ Graphonomy ëª¨ë¸ ì‚¬ìš©")
                    real_ai_model = self.ai_models['graphonomy']
                    
                    # RealAIModelì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                    if hasattr(real_ai_model, 'model_instance') and real_ai_model.model_instance is not None:
                        model = real_ai_model.model_instance
                        self.logger.info("âœ… RealAIModelì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œ ì„±ê³µ")
                    else:
                        # í´ë°±: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ìƒì„±
                        self.logger.info("âš ï¸ RealAIModelì— ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ - ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒì„±")
                        model = self._create_model('graphonomy', checkpoint_data=checkpoint_data, device=device)
                else:
                    # í´ë°±: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ìƒì„±
                    self.logger.info("âš ï¸ ì‹¤ì œ ë¡œë”©ëœ ëª¨ë¸ ì—†ìŒ - ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒì„±")
                    model = self._create_model('graphonomy', checkpoint_data=checkpoint_data, device=device)
                
                # ëª¨ë¸ì´ eval() ë©”ì„œë“œë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
                if hasattr(model, 'eval'):
                    model.eval()
                else:
                    self.logger.warning("âš ï¸ ëª¨ë¸ì— eval() ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
                
                # ê³ ê¸‰ ì¶”ë¡  ìˆ˜í–‰
                with torch.no_grad():
                    # FP16 ìµœì í™”
                    if self.config.use_fp16 and device in ['mps', 'cuda']:
                        try:
                            if device == 'mps':
                                with torch.autocast(device_type='mps', dtype=torch.float16):
                                    output = model(input_tensor)
                            else:
                                with torch.autocast(device_type='cuda', dtype=torch.float16):
                                    output = model(input_tensor)
                        except:
                            output = model(input_tensor)
                    else:
                        output = model(input_tensor)
                    
                    # ì¶œë ¥ ì²˜ë¦¬ ë° ê²€ì¦
                    if isinstance(output, dict):
                        parsing_logits = output.get('parsing', list(output.values())[0])
                        edge_output = output.get('edge')
                        progressive_results = output.get('progressive_results', [])
                        correction_info = output.get('correction_info', {})
                        refinement_results = output.get('refinement_results', [])
                        ensemble_result = output.get('ensemble_result', {})
                    else:
                        parsing_logits = output
                        edge_output = None
                        progressive_results = []
                        correction_info = {}
                        refinement_results = []
                        ensemble_result = {}
                    
                    # Softmax + Argmax (20ê°œ í´ë˜ìŠ¤)
                    parsing_probs = F.softmax(parsing_logits, dim=1)
                    parsing_pred = torch.argmax(parsing_probs, dim=1)
                    
                    # ê³ ê¸‰ ì‹ ë¢°ë„ ê³„ì‚°
                    confidence_map = self._calculate_confidence(
                        parsing_probs, parsing_logits, edge_output
                    )
                    
                    # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
                    quality_metrics = self._calculate_quality_metrics(
                        parsing_pred.cpu().numpy(), confidence_map.cpu().numpy()
                    )
                
                return {
                    'parsing_pred': parsing_pred,
                    'parsing_logits': parsing_logits,
                    'parsing_probs': parsing_probs,
                    'confidence_map': confidence_map,
                    'edge_output': edge_output,
                    'progressive_results': progressive_results,
                    'correction_info': correction_info,
                    'refinement_results': refinement_results,
                    'ensemble_result': ensemble_result,
                    'quality_metrics': quality_metrics,
                    'advanced_inference': True,
                    'model_architecture': 'AdvancedGraphonomyResNetASPP'
                }
                
            except Exception as e:
                self.logger.error(f"âŒ ê³ ê¸‰ Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {e}")
                raise

        # _calculate_parsing_confidence í•¨ìˆ˜ ì œê±° - _calculate_confidence í•¨ìˆ˜ë¡œ í†µí•©ë¨

        def _postprocess_result(self, inference_result: Dict[str, Any], original_image, model_type: str = 'graphonomy') -> Dict[str, Any]:
            """í†µí•© ê²°ê³¼ í›„ì²˜ë¦¬ í•¨ìˆ˜"""
            try:
                # íŒŒì‹± ì˜ˆì¸¡ ì¶”ì¶œ
                if isinstance(inference_result, dict):
                    parsing_pred = inference_result.get('parsing_pred')
                    confidence_map = inference_result.get('confidence_map')
                    edge_output = inference_result.get('edge_output')
                    quality_metrics = inference_result.get('quality_metrics', {})
                    model_used = inference_result.get('model_used', model_type)
                else:
                    parsing_pred = inference_result
                    confidence_map = None
                    edge_output = None
                    quality_metrics = {}
                    model_used = model_type
                
                if parsing_pred is None:
                    raise ValueError("íŒŒì‹± ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                
                # ğŸ”¥ ìƒˆë¡œìš´ ê²€ì¦ ìœ í‹¸ë¦¬í‹° ì‚¬ìš© (common_importsì—ì„œ ì´ë¯¸ importë¨)
                # ì›ë³¸ í¬ê¸° ì•ˆì „í•˜ê²Œ ê²°ì •
                original_size = get_original_size_safely(original_image)
                
                # íŒŒì‹± ë§µ ê²€ì¦ ë° ì •ì œ (ëª¨ë“  ë³µì¡í•œ ë¡œì§ì„ í•œ ë²ˆì— ì²˜ë¦¬)
                parsing_validator = ParsingMapValidator()
                parsing_map = parsing_validator.validate_parsing_map(parsing_pred, original_size)
                
                # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                if parsing_map.shape[:2] != original_size:
                    parsing_pil = Image.fromarray(parsing_map)
                    parsing_resized = parsing_pil.resize(
                        (original_size[1], original_size[0]), 
                        Image.NEAREST
                    )
                    parsing_map = np.array(parsing_resized)
                
                # ğŸ”¥ ì‹ ë¢°ë„ ë§µ ì²˜ë¦¬ (ë°ì´í„° íƒ€ì… ì˜¤ë¥˜ í•´ê²°)
                confidence_array = None
                if confidence_map is not None:
                    if isinstance(confidence_map, torch.Tensor):
                        confidence_array = confidence_map.squeeze().cpu().numpy()
                    elif isinstance(confidence_map, (int, float, np.float64)):
                        confidence_array = np.array([float(confidence_map)])
                    elif isinstance(confidence_map, dict):
                        # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                        first_value = next(iter(confidence_map.values()))
                        if isinstance(first_value, (int, float, np.float64)):
                            confidence_array = np.array([float(first_value)])
                        else:
                            confidence_array = np.array([0.5])
                    else:
                        try:
                            confidence_array = np.array(confidence_map, dtype=np.float32)
                        except:
                            confidence_array = np.array([0.5])
                    
                    # ì‹ ë¢°ë„ ë§µë„ ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                    if confidence_array is not None and hasattr(confidence_array, 'shape') and len(confidence_array.shape) >= 2:
                        if confidence_array.shape[:2] != original_size:
                            try:
                                confidence_pil = Image.fromarray((confidence_array * 255).astype(np.uint8))
                                confidence_resized = confidence_pil.resize(
                                    (original_size[1], original_size[0]), 
                                    Image.BILINEAR
                                )
                                confidence_array = np.array(confidence_resized).astype(np.float32) / 255.0
                            except Exception as e:
                                self.logger.warning(f"âš ï¸ confidence_array ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨: {e}")
                                # ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
                                confidence_array = np.ones(original_size, dtype=np.float32) * 0.8
                    else:
                        # ğŸ”¥ confidence_arrayê°€ Noneì´ê±°ë‚˜ ì˜ëª»ëœ í˜•íƒœì¸ ê²½ìš° ê·¼ë³¸ì  í•´ê²°
                        self.logger.warning(f"âš ï¸ confidence_arrayê°€ Noneì´ê±°ë‚˜ ì˜ëª»ëœ í˜•íƒœ: {type(confidence_array)}")
                        
                        # ğŸ”¥ íƒ€ì…ë³„ ì²˜ë¦¬
                        if confidence_array is None:
                            confidence_array = np.ones(original_size, dtype=np.float32) * 0.8
                        elif isinstance(confidence_array, np.ndarray):
                            # NumPy ë°°ì—´ì´ì§€ë§Œ í˜•íƒœê°€ ë‹¤ë¥¸ ê²½ìš°
                            if len(confidence_array.shape) != 2:
                                # ì°¨ì› ì •ê·œí™”
                                if len(confidence_array.shape) == 3:
                                    confidence_array = confidence_array[0] if confidence_array.shape[0] == 1 else confidence_array[:, :, 0]
                                elif len(confidence_array.shape) == 4:
                                    # 4ì°¨ì› í…ì„œì¸ ê²½ìš° ì²« ë²ˆì§¸ ë°°ì¹˜ ì‚¬ìš©
                                    confidence_array = confidence_array[0]
                                else:
                                    confidence_array = np.ones(original_size, dtype=np.float32) * 0.8
                            
                            # í¬ê¸° ì •ê·œí™”
                            if confidence_array.shape != original_size:
                                try:
                                    confidence_pil = Image.fromarray((confidence_array * 255).astype(np.uint8))
                                    confidence_resized = confidence_pil.resize(
                                        (original_size[1], original_size[0]), 
                                        Image.BILINEAR
                                    )
                                    confidence_array = np.array(confidence_resized).astype(np.float32) / 255.0
                                except Exception as resize_error:
                                    self.logger.warning(f"âš ï¸ confidence_array ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨: {resize_error}")
                                    confidence_array = np.ones(original_size, dtype=np.float32) * 0.8
                        else:
                            # ê¸°íƒ€ íƒ€ì…ì€ ê¸°ë³¸ê°’ ì‚¬ìš©
                            confidence_array = np.ones(original_size, dtype=np.float32) * 0.8
                
                # ê°ì§€ëœ ë¶€ìœ„ ë¶„ì„
                detected_parts = self._analyze_detected_parts(parsing_map)
                
                # ì˜ë¥˜ ë¶„ì„
                clothing_analysis = self._analyze_clothing_for_change(parsing_map)
                
                # ğŸ”¥ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì ìš© (ìƒˆë¡œ ì¶”ê°€)
                special_cases = {}
                if self.config.enable_special_case_handling and self.special_case_processor:
                    try:
                        self.ai_stats['special_case_calls'] += 1
                        # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ê°ì§€
                        special_cases = self.special_case_processor.detect_special_cases(original_image)
                        
                        # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ì— ë”°ë¥¸ íŒŒì‹± ë§µ í–¥ìƒ
                        if any(special_cases.values()):
                            parsing_map = self.special_case_processor.apply_special_case_enhancement(
                                parsing_map, original_image, special_cases
                            )
                            self.logger.debug(f"âœ… íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ ì™„ë£Œ: {special_cases}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
                try:
                    if confidence_array is not None:
                        # NumPy ë°°ì—´ì¸ì§€ í™•ì¸
                        if isinstance(parsing_map, np.ndarray) and isinstance(confidence_array, np.ndarray):
                            quality_metrics = self._calculate_quality_metrics(parsing_map, confidence_array)
                        else:
                            self.logger.warning(f"âš ï¸ parsing_map ë˜ëŠ” confidence_arrayê°€ NumPy ë°°ì—´ì´ ì•„ë‹˜: {type(parsing_map)}, {type(confidence_array)}")
                            quality_metrics = {}
                    else:
                        quality_metrics = {}
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
                    quality_metrics = {}
                
                # ì‹œê°í™” ìƒì„±
                visualization = {}
                if self.config.enable_visualization:
                    visualization = self._create_visualization(parsing_map, original_image)
                
                # ğŸ”¥ ìµœì¢… ê²°ê³¼ ë°˜í™˜ (API ì‘ë‹µìš©)
                final_result = {
                    # ğŸ”¥ ê¸°ë³¸ ê²°ê³¼ ë°ì´í„°
                    'parsing_map': parsing_map,
                    'confidence_map': confidence_array,
                    'detected_parts': detected_parts,
                    'clothing_analysis': clothing_analysis,
                    'quality_metrics': quality_metrics,
                    'original_size': original_size,
                    'model_architecture': model_used,
                    
                    # ğŸ”¥ ì‹œê°í™” ê²°ê³¼ë¬¼ ì¶”ê°€
                    'parsing_visualization': visualization.get('parsing_visualization'),
                    'overlay_image': visualization.get('overlay_image'),
                    'visualization_created': visualization.get('visualization_created', False),
                    
                    # ğŸ”¥ ì¤‘ê°„ ì²˜ë¦¬ ê²°ê³¼ë¬¼ë“¤ (ë‹¤ìŒ Stepìœ¼ë¡œ ì „ë‹¬)
                    'intermediate_results': {
                        # ğŸ”¥ ë‹¤ìŒ AI ëª¨ë¸ì´ ì‚¬ìš©í•  ì‹¤ì œ ë°ì´í„°
                        'parsing_map': parsing_map,  # NumPy ë°°ì—´ - ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥
                        'confidence_map': confidence_array,  # NumPy ë°°ì—´ - ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥
                        'parsing_map_numpy': parsing_map,  # í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
                        'confidence_map_numpy': confidence_array,  # í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
                        
                        # ğŸ”¥ ë¶„ì„ ê²°ê³¼ ë°ì´í„°
                        'detected_body_parts': detected_parts,
                        'clothing_regions': clothing_analysis,
                        'unique_labels': list(np.unique(parsing_map).astype(int)),
                        'parsing_shape': parsing_map.shape,
                        
                        # ğŸ”¥ ì‹œê°í™” ë°ì´í„° (ë””ë²„ê¹…ìš©)
                        'parsing_visualization': visualization.get('parsing_visualization'),
                        'overlay_image': visualization.get('overlay_image'),
                        
                        # ğŸ”¥ ë©”íƒ€ë°ì´í„°
                        'model_used': model_used,
                        'processing_metadata': {
                            'step_id': 1,
                            'step_name': 'HumanParsing',
                            'model_type': model_type,
                            'confidence_threshold': self.config.confidence_threshold,
                            'quality_level': self.config.quality_level.value,
                            'applied_algorithms': self._get_applied_algorithms()
                        },
                        
                        # ğŸ”¥ ë‹¤ìŒ Stepì—ì„œ í•„ìš”í•œ íŠ¹ì • ë°ì´í„°
                        'body_mask': (parsing_map > 0).astype(np.uint8),  # ì‹ ì²´ ë§ˆìŠ¤í¬
                        'clothing_mask': np.isin(parsing_map, [5, 6, 7, 9, 11, 12]).astype(np.uint8),  # ì˜ë¥˜ ë§ˆìŠ¤í¬
                        'skin_mask': np.isin(parsing_map, [10, 13, 14, 15, 16, 17]).astype(np.uint8),  # í”¼ë¶€ ë§ˆìŠ¤í¬
                        'face_mask': (parsing_map == 14).astype(np.uint8),  # ì–¼êµ´ ë§ˆìŠ¤í¬
                        'arms_mask': np.isin(parsing_map, [15, 16]).astype(np.uint8),  # íŒ” ë§ˆìŠ¤í¬
                        'legs_mask': np.isin(parsing_map, [17, 18]).astype(np.uint8),  # ë‹¤ë¦¬ ë§ˆìŠ¤í¬
                        
                        # ğŸ”¥ ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´
                        'body_bbox': self._get_bounding_box(parsing_map > 0),
                        'clothing_bbox': self._get_bounding_box(np.isin(parsing_map, [5, 6, 7, 9, 11, 12])),
                        'face_bbox': self._get_bounding_box(parsing_map == 14)
                    }
                }
                
                return final_result
                
            except Exception as e:
                self.logger.error(f"âŒ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise

        # _create_dynamic_model_from_checkpoint í•¨ìˆ˜ ì œê±° - _create_model í•¨ìˆ˜ë¡œ í†µí•©ë¨

    

        def _map_checkpoint_keys(self, checkpoint: Dict[str, Any]) -> Dict[str, Any]:
            """ì²´í¬í¬ì¸íŠ¸ í‚¤ ë§¤í•‘ - ê²€ì¦ëœ ì•„í‚¤í…ì²˜ ì •ë³´ ì ìš©"""
            try:
                # ğŸ”¥ ê²€ì¦ëœ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ì²˜ë¦¬
                if isinstance(checkpoint, dict):
                    # state_dict êµ¬ì¡° í™•ì¸
                    if 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    elif 'model_state_dict' in checkpoint:
                        state_dict = checkpoint['model_state_dict']
                    elif 'params_ema' in checkpoint:
                        # RealESRGAN ë“±ì—ì„œ ì‚¬ìš©í•˜ëŠ” EMA íŒŒë¼ë¯¸í„°
                        state_dict = checkpoint['params_ema']
                    else:
                        state_dict = checkpoint
                else:
                    # ì§ì ‘ tensorì¸ ê²½ìš°
                    return checkpoint
                
                mapped_state_dict = {}
                
                for key, value in state_dict.items():
                    # ğŸ”¥ ê²€ì¦ëœ í‚¤ ë§¤í•‘ íŒ¨í„´ ì ìš©
                    new_key = key
                    
                    # module. ì ‘ë‘ì‚¬ ì œê±° (DataParallel)
                    if key.startswith('module.'):
                        new_key = key[7:]
                    
                    # encoder. ì ‘ë‘ì‚¬ ì œê±° (ì¼ë¶€ ëª¨ë¸)
                    elif key.startswith('encoder.'):
                        new_key = key[8:]
                    
                    # model. ì ‘ë‘ì‚¬ ì œê±° (ì¼ë¶€ ëª¨ë¸)
                    elif key.startswith('model.'):
                        new_key = key[6:]
                    
                    # backbone. ì ‘ë‘ì‚¬ ì œê±° (ì¼ë¶€ ëª¨ë¸)
                    elif key.startswith('backbone.'):
                        new_key = key[9:]
                    
                    # head. ì ‘ë‘ì‚¬ ì œê±° (ì¼ë¶€ ëª¨ë¸)
                    elif key.startswith('head.'):
                        new_key = key[5:]
                    
                    # net. ì ‘ë‘ì‚¬ ì œê±° (U2Net ë“±)
                    elif key.startswith('net.'):
                        new_key = key[4:]
                    
                    # decoder. ì ‘ë‘ì‚¬ ì œê±° (DeepLabV3+ ë“±)
                    elif key.startswith('decoder.'):
                        new_key = key[8:]
                    
                    # ğŸ”¥ ê²€ì¦ëœ ì•„í‚¤í…ì²˜ë³„ íŠ¹í™” ë§¤í•‘
                    # Graphonomy (ResNet-101 + ASPP ì•„í‚¤í…ì²˜)
                    if any(keyword in key.lower() for keyword in ['backbone', 'decoder', 'classifier', 'schp', 'hrnet']):
                        # Graphonomy íŠ¹í™” ë§¤í•‘ì€ ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë¨
                        pass
                    
                    # U2Net (U-Net ê¸°ë°˜ ì•„í‚¤í…ì²˜)
                    elif any(keyword in key.lower() for keyword in ['stage1', 'stage2', 'stage3', 'stage4', 'side', 'u2net']):
                        # U2Net íŠ¹í™” ë§¤í•‘ì€ ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë¨
                        pass
                    
                    # DeepLabV3+ (ResNet + ASPP + Decoder ì•„í‚¤í…ì²˜)
                    elif any(keyword in key.lower() for keyword in ['backbone', 'decoder', 'classifier', 'aspp', 'deeplab']):
                        # DeepLabV3+ íŠ¹í™” ë§¤í•‘ì€ ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë¨
                        pass
                    
                    mapped_state_dict[new_key] = value
                
                return mapped_state_dict
                
            except Exception as e:
                self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ í‚¤ ë§¤í•‘ ì‹¤íŒ¨: {e}")
                return checkpoint
                
                return model
                
            except Exception as e:
                self.logger.error(f"âŒ ë™ì  ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
                # í´ë°± ì œê±° - ì‹¤ì œ íŒŒì¼ë§Œ ì‚¬ìš©
                raise ValueError(f"ë™ì  ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        # ==============================================
        # ğŸ”¥ ì˜ë¥˜ ë¶„ì„ ë° í’ˆì§ˆ ë©”íŠ¸ë¦­
        # ==============================================
        
        def _analyze_clothing_for_change(self, parsing_map: np.ndarray) -> Dict[str, Any]:
            """ì˜· ê°ˆì•„ì…íˆê¸°ë¥¼ ìœ„í•œ ì˜ë¥˜ ë¶„ì„"""
            try:
                analysis = {
                    'upper_clothes': self._analyze_clothing_region(parsing_map, [5, 6, 7]),  # ìƒì˜, ë“œë ˆìŠ¤, ì½”íŠ¸
                    'lower_clothes': self._analyze_clothing_region(parsing_map, [9, 12]),    # ë°”ì§€, ìŠ¤ì»¤íŠ¸
                    'accessories': self._analyze_clothing_region(parsing_map, [1, 3, 4, 11]), # ëª¨ì, ì¥ê°‘, ì„ ê¸€ë¼ìŠ¤, ìŠ¤ì¹´í”„
                    'footwear': self._analyze_clothing_region(parsing_map, [8, 18, 19]),      # ì–‘ë§, ì‹ ë°œ
                    'skin_areas': self._analyze_clothing_region(parsing_map, [10, 13, 14, 15, 16, 17]) # í”¼ë¶€ ì˜ì—­
                }
                
                # ì˜· ê°ˆì•„ì…íˆê¸° ë‚œì´ë„ ê³„ì‚°
                total_clothing_area = sum([region['area_ratio'] for region in analysis.values() if region['detected']])
                analysis['change_difficulty'] = 'easy' if total_clothing_area < 0.3 else ('medium' if total_clothing_area < 0.6 else 'hard')
                
                return analysis
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì˜ë¥˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
                return {}
        
        def _analyze_clothing_region(self, parsing_map: np.ndarray, part_ids: List[int]) -> Dict[str, Any]:
            """ì˜ë¥˜ ì˜ì—­ ë¶„ì„"""
            try:
                region_mask = np.isin(parsing_map, part_ids)
                total_pixels = parsing_map.size
                region_pixels = np.sum(region_mask)
                
                if region_pixels == 0:
                    return {'detected': False, 'area_ratio': 0.0, 'quality': 0.0}
                
                area_ratio = region_pixels / total_pixels
                
                # í’ˆì§ˆ ì ìˆ˜ (ì—°ê²°ì„±, ëª¨ì–‘ ë“±)
                quality_score = self._evaluate_region_quality(region_mask)
                
                return {
                    'detected': True,
                    'area_ratio': area_ratio,
                    'quality': quality_score,
                    'pixel_count': int(region_pixels)
                }
                
            except Exception as e:
                self.logger.debug(f"ì˜ì—­ ë¶„ì„ ì‹¤íŒ¨: {e}")
                return {'detected': False, 'area_ratio': 0.0, 'quality': 0.0}
        
        def _evaluate_region_quality(self, mask: np.ndarray) -> float:
            """ì˜ì—­ í’ˆì§ˆ í‰ê°€"""
            try:
                # ğŸ”¥ numpy ë°°ì—´ boolean í‰ê°€ ì˜¤ë¥˜ ìˆ˜ì •
                if not CV2_AVAILABLE or float(np.sum(mask)) == 0:
                    return 0.5
                
                mask_uint8 = mask.astype(np.uint8) * 255
                
                # ì—°ê²°ì„± í‰ê°€
                num_labels, labels = cv2.connectedComponents(mask_uint8)
                if num_labels <= 1:
                    connectivity = 0.0
                elif num_labels == 2:  # í•˜ë‚˜ì˜ ì—°ê²° ì„±ë¶„
                    connectivity = 1.0
                else:  # ì—¬ëŸ¬ ì—°ê²° ì„±ë¶„
                    component_sizes = [np.sum(labels == i) for i in range(1, num_labels)]
                    largest_ratio = max(component_sizes) / np.sum(mask)
                    connectivity = largest_ratio
                
                # ì»´íŒ©íŠ¸ì„± í‰ê°€ (ë‘˜ë ˆ ëŒ€ë¹„ ë©´ì )
                contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if contours is not None and len(contours) > 0:
                    largest_contour = max(contours, key=cv2.contourArea)
                    area = cv2.contourArea(largest_contour)
                    perimeter = cv2.arcLength(largest_contour, True)
                    
                    if perimeter > 0:
                        compactness = 4 * np.pi * area / (perimeter * perimeter)
                        compactness = min(compactness, 1.0)
                    else:
                        compactness = 0.0
                else:
                    compactness = 0.0
                
                # ì¢…í•© í’ˆì§ˆ
                overall_quality = connectivity * 0.6 + compactness * 0.4
                return min(overall_quality, 1.0)
                
            except Exception:
                return 0.5
        
        def _get_applied_algorithms(self) -> List[str]:
            """ì ìš©ëœ ì•Œê³ ë¦¬ì¦˜ ëª©ë¡ (ì™„ì „í•œ ë¦¬ìŠ¤íŠ¸)"""
            algorithms = []
            
            # ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜
            algorithms.append('Advanced Graphonomy ResNet-101 + ASPP')
            algorithms.append('Self-Attention Mechanism')
            algorithms.append('Progressive Parsing (3-stage)')
            algorithms.append('Self-Correction Learning (SCHP)')
            algorithms.append('Iterative Refinement')
            
            # ì¡°ê±´ë¶€ ì•Œê³ ë¦¬ì¦˜
            if self.config.enable_crf_postprocessing and DENSECRF_AVAILABLE:
                algorithms.append('DenseCRF Postprocessing (20-class)')
                self.ai_stats['crf_postprocessing_calls'] += 1
            
            if self.config.enable_multiscale_processing:
                algorithms.append('Multiscale Processing (0.5x, 1.0x, 1.5x)')
                self.ai_stats['multiscale_processing_calls'] += 1
            
            if self.config.enable_edge_refinement:
                algorithms.append('Edge-based Refinement (Canny + Morphology)')
                self.ai_stats['edge_refinement_calls'] += 1
            
            if self.config.enable_hole_filling:
                algorithms.append('Morphological Operations (Hole-filling + Noise removal)')
            
            if self.config.enable_quality_validation:
                algorithms.append('Quality Enhancement (Confidence-based)')
                self.ai_stats['quality_enhancement_calls'] += 1
            
            if self.config.enable_lighting_normalization:
                algorithms.append('CLAHE Lighting Normalization')
            
            # ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€
            algorithms.extend([
                'Atrous Spatial Pyramid Pooling (ASPP)',
                'Multi-scale Feature Fusion',
                'Entropy-based Uncertainty Estimation',
                'Hybrid Ensemble Voting',
                'ROI-based Processing',
                'Advanced Color Correction'
            ])
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.ai_stats['total_algorithms_applied'] = len(algorithms)
            self.ai_stats['progressive_parsing_calls'] += 1
            self.ai_stats['self_correction_calls'] += 1
            self.ai_stats['iterative_refinement_calls'] += 1
            self.ai_stats['aspp_module_calls'] += 1
            self.ai_stats['self_attention_calls'] += 1
            
            return algorithms
        
        def _calculate_quality_metrics(self, parsing_map: np.ndarray, confidence_map: np.ndarray) -> Dict[str, float]:
            """í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
            try:
                metrics = {}
                
                # ì…ë ¥ ë°ì´í„° ê²€ì¦
                if parsing_map is None or confidence_map is None:
                    return {'overall_quality': 0.5}
                
                # numpy ë°°ì—´ë¡œ ë³€í™˜
                if isinstance(parsing_map, torch.Tensor):
                    parsing_map = parsing_map.cpu().numpy()
                if isinstance(confidence_map, torch.Tensor):
                    confidence_map = confidence_map.cpu().numpy()
                
                # 1. ì „ì²´ ì‹ ë¢°ë„
                try:
                    metrics['average_confidence'] = float(np.mean(confidence_map))
                except:
                    metrics['average_confidence'] = 0.5
                
                # 2. í´ë˜ìŠ¤ ë‹¤ì–‘ì„± (Shannon Entropy)
                try:
                    unique_classes, class_counts = np.unique(parsing_map, return_counts=True)
                    if len(unique_classes) > 1:
                        class_probs = class_counts / np.sum(class_counts)
                        entropy = -np.sum(class_probs * np.log2(class_probs + 1e-8))
                        max_entropy = np.log2(20)  # 20ê°œ í´ë˜ìŠ¤
                        metrics['class_diversity'] = entropy / max_entropy
                    else:
                        metrics['class_diversity'] = 0.0
                except:
                    metrics['class_diversity'] = 0.0
                
                # 3. ê²½ê³„ì„  í’ˆì§ˆ
                try:
                    if CV2_AVAILABLE:
                        edges = cv2.Canny((parsing_map * 12).astype(np.uint8), 30, 100)
                        edge_density = np.sum(edges > 0) / edges.size
                        metrics['edge_quality'] = min(edge_density * 10, 1.0)  # ì •ê·œí™”
                    else:
                        metrics['edge_quality'] = 0.7
                except:
                    metrics['edge_quality'] = 0.7
                
                # 4. ì˜ì—­ ì—°ê²°ì„±
                try:
                    connectivity_scores = []
                    for class_id in unique_classes:
                        if class_id == 0:  # ë°°ê²½ ì œì™¸
                            continue
                        class_mask = (parsing_map == class_id)
                        if np.sum(class_mask) > 100:  # ì¶©ë¶„íˆ í° ì˜ì—­ë§Œ
                            quality = self._evaluate_region_quality(class_mask)
                            connectivity_scores.append(quality)
                    
                    metrics['region_connectivity'] = np.mean(connectivity_scores) if connectivity_scores else 0.5
                except:
                    metrics['region_connectivity'] = 0.5
                
                # 5. ì „ì²´ í’ˆì§ˆ ì ìˆ˜
                try:
                    metrics['overall_quality'] = (
                        metrics['average_confidence'] * 0.3 +
                        metrics['class_diversity'] * 0.2 +
                        metrics['edge_quality'] * 0.25 +
                        metrics['region_connectivity'] * 0.25
                    )
                except:
                    metrics['overall_quality'] = 0.5
                
                return metrics
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
                return {'overall_quality': 0.5}
        # ì¤‘ë³µëœ _preprocess_image í•¨ìˆ˜ ì œê±° - í†µí•©ëœ _preprocess_image í•¨ìˆ˜ ì‚¬ìš©
        # ì¤‘ë³µëœ _postprocess_result í•¨ìˆ˜ ì œê±° - í†µí•©ëœ _postprocess_result í•¨ìˆ˜ ì‚¬ìš©
        def _analyze_detected_parts(self, parsing_map: np.ndarray) -> Dict[str, Any]:
            """ê°ì§€ëœ ë¶€ìœ„ ë¶„ì„"""
            try:
                detected_parts = {}
                unique_labels = np.unique(parsing_map)
                
                self.logger.info(f"ğŸ” íŒŒì‹± ë§µì—ì„œ ë°œê²¬ëœ ë¼ë²¨ë“¤: {unique_labels}")
                
                for label in unique_labels:
                    if label in BODY_PARTS:
                        part_name = BODY_PARTS[label]
                        mask = (parsing_map == label)
                        pixel_count = int(np.sum(mask))
                        percentage = float(pixel_count / parsing_map.size * 100)
                        
                        if pixel_count > 0:
                            detected_parts[part_name] = {
                                'label': int(label),
                                'pixel_count': pixel_count,
                                'percentage': percentage,
                                'is_clothing': label in [5, 6, 7, 9, 11, 12],
                                'is_skin': label in [10, 13, 14, 15, 16, 17]
                            }
                            self.logger.info(f"âœ… {part_name} ê°ì§€ë¨: {pixel_count} í”½ì…€ ({percentage:.2f}%)")
                
                if not detected_parts:
                    self.logger.warning(f"âš ï¸ ê°ì§€ëœ ë¶€ìœ„ê°€ ì—†ìŒ. íŒŒì‹± ë§µ ê°’ ë²”ìœ„: {parsing_map.min()} ~ {parsing_map.max()}")
                
                return detected_parts
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¶€ìœ„ ë¶„ì„ ì‹¤íŒ¨: {e}")
                return {}
        
        def _create_visualization(self, parsing_map: np.ndarray, original_image) -> Dict[str, Any]:
            """ì‹œê°í™” ìƒì„± - Base64 ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
            try:
                if not PIL_AVAILABLE:
                    return {}
                
                # ì»¬ëŸ¬ íŒŒì‹± ë§µ ìƒì„±
                height, width = parsing_map.shape
                colored_image = np.zeros((height, width, 3), dtype=np.uint8)
                
                # 20ê°œ í´ë˜ìŠ¤ì— ëŒ€í•œ ì»¬ëŸ¬ íŒ”ë ˆíŠ¸ ì •ì˜
                color_palette = [
                    [0, 0, 0],      # background
                    [128, 0, 0],    # hat
                    [255, 0, 0],    # hair
                    [0, 85, 0],     # glove
                    [170, 0, 51],   # sunglasses
                    [255, 85, 0],   # upper_clothes
                    [0, 0, 85],     # dress
                    [0, 119, 221],  # coat
                    [85, 85, 0],    # socks
                    [0, 0, 255],    # pants
                    [51, 170, 221], # torso_skin
                    [0, 85, 85],    # scarf
                    [0, 170, 170],  # skirt
                    [85, 255, 170], # face
                    [170, 255, 85], # left_arm
                    [255, 255, 0],  # right_arm
                    [255, 170, 0],  # left_leg
                    [170, 170, 255], # right_leg
                    [85, 0, 255],   # left_shoe
                    [255, 0, 255]   # right_shoe
                ]
                
                # íŒŒì‹± ë§µì„ ì»¬ëŸ¬ë¡œ ë³€í™˜
                for class_id in range(len(color_palette)):
                    mask = (parsing_map == class_id)
                    colored_image[mask] = color_palette[class_id]
                
                # ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„± (ì›ë³¸ + íŒŒì‹± ë§µ)
                overlay_image = self._create_overlay_image(original_image, colored_image)
                
                # Base64 ì¸ì½”ë”©
                import base64
                from io import BytesIO
                
                # íŒŒì‹± ë§µ Base64
                colored_pil = Image.fromarray(colored_image)
                buffer = BytesIO()
                colored_pil.save(buffer, format='JPEG', quality=95)
                colored_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
                
                # ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ Base64
                overlay_pil = Image.fromarray(overlay_image)
                buffer = BytesIO()
                overlay_pil.save(buffer, format='JPEG', quality=95)
                overlay_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
                
                return {
                    'parsing_visualization': f"data:image/jpeg;base64,{colored_base64}",
                    'overlay_image': f"data:image/jpeg;base64,{overlay_base64}",
                    'parsing_shape': parsing_map.shape,
                    'unique_labels': list(np.unique(parsing_map).astype(int)),
                    'visualization_created': True
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
                return {'visualization_created': False}
    
        def _create_overlay_image(self, original_image: np.ndarray, colored_parsing: np.ndarray) -> np.ndarray:
            """ì›ë³¸ ì´ë¯¸ì§€ì™€ íŒŒì‹± ë§µì„ ì˜¤ë²„ë ˆì´"""
            try:
                # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ì¶° íŒŒì‹± ë§µ ë¦¬ì‚¬ì´ì¦ˆ
                if colored_parsing.shape[:2] != original_image.shape[:2]:
                    colored_parsing = cv2.resize(colored_parsing, (original_image.shape[1], original_image.shape[0]))
                
                # ì•ŒíŒŒ ë¸”ë Œë”© (0.7: ì›ë³¸, 0.3: íŒŒì‹± ë§µ)
                overlay = cv2.addWeighted(original_image, 0.7, colored_parsing, 0.3, 0)
                return overlay
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨: {e}")
                return original_image
        
        def _get_bounding_box(self, mask: np.ndarray) -> Dict[str, int]:
            """ë§ˆìŠ¤í¬ì—ì„œ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
            try:
                if not np.any(mask):
                    return {'x1': 0, 'y1': 0, 'x2': 0, 'y2': 0, 'width': 0, 'height': 0}
                
                # ë§ˆìŠ¤í¬ì—ì„œ 0ì´ ì•„ë‹Œ ì¢Œí‘œ ì°¾ê¸°
                coords = np.where(mask > 0)
                if len(coords[0]) == 0:
                    return {'x1': 0, 'y1': 0, 'x2': 0, 'y2': 0, 'width': 0, 'height': 0}
                
                y_coords = coords[0]
                x_coords = coords[1]
                
                x1, x2 = int(np.min(x_coords)), int(np.max(x_coords))
                y1, y2 = int(np.min(y_coords)), int(np.max(y_coords))
                
                return {
                    'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2,
                    'width': x2 - x1, 'height': y2 - y1,
                    'center_x': (x1 + x2) // 2,
                    'center_y': (y1 + y2) // 2
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° ì‹¤íŒ¨: {e}")
                return {'x1': 0, 'y1': 0, 'x2': 0, 'y2': 0, 'width': 0, 'height': 0}
        
        def _create_error_response(self, error_message: str) -> Dict[str, Any]:
            """ì—ëŸ¬ ì‘ë‹µ ìƒì„± - í†µí•©ëœ ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‚¬ìš©"""
            if EXCEPTIONS_AVAILABLE:
                error = MyClosetAIException(error_message, "UNEXPECTED_ERROR")
                response = create_exception_response(
                    error, 
                    self.step_name, 
                    getattr(self, 'step_id', 1), 
                    "unknown"
                )
                # Human Parsing íŠ¹í™” í•„ë“œ ì¶”ê°€
                response.update({
                    'parsing_result': None,
                    'confidence': 0.0,
                    'processing_time': 0.0,
                    'device_used': 'cpu',
                    'model_loaded': False,
                    'checkpoint_used': False
                })
                return response
            else:
                return {
                    'success': False,
                    'error': error_message,
                    'parsing_result': None,
                    'confidence': 0.0,
                    'processing_time': 0.0,
                    'device_used': 'cpu',
                    'model_loaded': False,
                    'checkpoint_used': False,
                    'step_name': self.step_name
                }
        
        def _assess_image_quality(self, image):
            """M3 Max ìµœì í™” ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€"""
            try:
                # ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€ ë¡œì§
                if image is None:
                    return 0.0
                
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í’ˆì§ˆ í‰ê°€
                if hasattr(image, 'shape') and (image.shape[0] > 1024 or image.shape[1] > 1024):
                    # í° ì´ë¯¸ì§€ëŠ” ë‹¤ìš´ìƒ˜í”Œë§í•˜ì—¬ í‰ê°€
                    scale_factor = min(1024 / image.shape[0], 1024 / image.shape[1])
                    new_size = (int(image.shape[1] * scale_factor), int(image.shape[0] * scale_factor))
                    import cv2
                    image = cv2.resize(image, new_size)
                
                # ì´ë¯¸ì§€ í¬ê¸° ê¸°ë°˜ í’ˆì§ˆ í‰ê°€
                height, width = image.shape[:2] if hasattr(image, 'shape') else (0, 0)
                size_quality = min(height * width / (512 * 512), 1.0)
                
                # ì¶”ê°€ í’ˆì§ˆ ë©”íŠ¸ë¦­ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
                if hasattr(image, 'shape') and len(image.shape) == 3:
                    import cv2
                    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
                    sharpness_quality = min(laplacian_var / 1000, 1.0)
                    return (size_quality + sharpness_quality) / 2
                
                return size_quality
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
                return 0.5
        
        def _memory_efficient_resize(self, image, target_size):
            """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§•"""
            try:
                if not hasattr(image, 'shape'):
                    return image
                
                if image.shape[0] == target_size and image.shape[1] == target_size:
                    return image
                
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¦¬ì‚¬ì´ì§•
                if target_size > 2048:
                    # ë§¤ìš° í° í•´ìƒë„ëŠ” ë‹¨ê³„ë³„ ë¦¬ì‚¬ì´ì§•
                    current_size = max(image.shape[0], image.shape[1])
                    while current_size < target_size:
                        current_size = min(current_size * 2, target_size)
                        new_size = (int(image.shape[1] * current_size / max(image.shape[0], image.shape[1])),
                                   int(image.shape[0] * current_size / max(image.shape[0], image.shape[1])))
                        import cv2
                        image = cv2.resize(image, new_size, interpolation=cv2.INTER_LANCZOS4)
                else:
                    # ì¼ë°˜ì ì¸ ë¦¬ì‚¬ì´ì§•
                    new_size = (target_size, target_size)
                    import cv2
                    image = cv2.resize(image, new_size, interpolation=cv2.INTER_LANCZOS4)
                
                return image
            except Exception as e:
                self.logger.warning(f"ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¦¬ì‚¬ì´ì§• ì‹¤íŒ¨: {e}")
                return image
        
        def _standardize_tensor_sizes(self, tensors, target_size=None):
            """í…ì„œ í¬ê¸° í‘œì¤€í™”"""
            try:
                if not tensors:
                    return tensors
                
                # ëª©í‘œ í¬ê¸° ê²°ì •
                if target_size is None:
                    # ê°€ì¥ í° í¬ê¸°ë¥¼ ëª©í‘œë¡œ ì„¤ì •
                    max_height = max(tensor.shape[2] for tensor in tensors)
                    max_width = max(tensor.shape[3] for tensor in tensors)
                    target_size = (max_height, max_width)
                else:
                    max_height, max_width = target_size
                
                # ëª¨ë“  í…ì„œë¥¼ ë™ì¼í•œ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                standardized_tensors = []
                for tensor in tensors:
                    if tensor.shape[2] != max_height or tensor.shape[3] != max_width:
                        resized_tensor = F.interpolate(
                            tensor, 
                            size=(max_height, max_width),
                            mode='bilinear', 
                            align_corners=False
                        )
                    else:
                        resized_tensor = tensor
                    standardized_tensors.append(resized_tensor)
                
                return standardized_tensors
            except Exception as e:
                self.logger.warning(f"í…ì„œ í¬ê¸° í‘œì¤€í™” ì‹¤íŒ¨: {e}")
                return tensors
        
        def _normalize_lighting(self, image):
            """ì¡°ëª… ì •ê·œí™”"""
            try:
                if image is None:
                    return image
                
                # ê°„ë‹¨í•œ ì¡°ëª… ì •ê·œí™”
                if len(image.shape) == 3:
                    # RGB ì´ë¯¸ì§€
                    import cv2
                    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
                    l, a, b = cv2.split(lab)
                    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
                    l = clahe.apply(l)
                    lab = cv2.merge([l, a, b])
                    normalized = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
                    return normalized
                else:
                    return image
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì¡°ëª… ì •ê·œí™” ì‹¤íŒ¨: {e}")
                return image
        
        def _correct_colors(self, image):
            """ìƒ‰ìƒ ë³´ì •"""
            try:
                if image is None:
                    return image
                
                # ğŸ”¥ numpy importë¥¼ ë©”ì„œë“œ ì‹œì‘ ë¶€ë¶„ìœ¼ë¡œ ì´ë™
                import numpy as np
                
                # PIL Imageë¥¼ numpy arrayë¡œ ë³€í™˜
                if hasattr(image, 'convert'):
                    # PIL Imageì¸ ê²½ìš°
                    image_array = np.array(image)
                elif hasattr(image, 'shape'):
                    # numpy arrayì¸ ê²½ìš°
                    image_array = image
                else:
                    return image
                
                # ê°„ë‹¨í•œ ìƒ‰ìƒ ë³´ì •
                if len(image_array.shape) == 3:
                    import cv2
                    # í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ ì ìš©
                    result = cv2.cvtColor(image_array, cv2.COLOR_RGB2LAB)
                    avg_a = np.average(result[:, :, 1])
                    avg_b = np.average(result[:, :, 2])
                    result[:, :, 1] = result[:, :, 1] - ((avg_a - 128) * (result[:, :, 0] / 255.0) * 1.1)
                    result[:, :, 2] = result[:, :, 2] - ((avg_b - 128) * (result[:, :, 0] / 255.0) * 1.1)
                    corrected = cv2.cvtColor(result, cv2.COLOR_LAB2RGB)
                    
                    # PIL Imageë¡œ ë‹¤ì‹œ ë³€í™˜
                    if hasattr(image, 'convert'):
                        return Image.fromarray(corrected)
                    else:
                        return corrected
                else:
                    return image
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
                return image
        
        def _detect_roi(self, image):
            """ROI ê°ì§€"""
            try:
                if image is None:
                    return None
                
                # ê°„ë‹¨í•œ ROI ê°ì§€ (ì „ì²´ ì´ë¯¸ì§€ë¥¼ ROIë¡œ ì„¤ì •)
                height, width = image.shape[:2] if hasattr(image, 'shape') else (0, 0)
                return {
                    'x': 0,
                    'y': 0,
                    'width': width,
                    'height': height
                }
            except Exception as e:
                self.logger.warning(f"âš ï¸ ROI ê°ì§€ ì‹¤íŒ¨: {e}")
                return None
        
        # ==============================================
        # ğŸ”¥ ê°„ì†Œí™”ëœ process() ë©”ì„œë“œ (í•µì‹¬ ë¡œì§ë§Œ)
        # ==============================================
        
        def process(self, **kwargs) -> Dict[str, Any]:
            """ğŸ”¥ ë‹¨ê³„ë³„ ì„¸ë¶„í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ê°€ ì ìš©ëœ Human Parsing process ë©”ì„œë“œ"""
            print(f"ğŸ”¥ [ë””ë²„ê¹…] HumanParsingStep.process() ì§„ì…!")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] kwargs í‚¤ë“¤: {list(kwargs.keys()) if kwargs else 'None'}")
            print(f"ğŸ”¥ [ë””ë²„ê¹…] kwargs ê°’ë“¤: {[(k, type(v).__name__) for k, v in kwargs.items()] if kwargs else 'None'}")
            
            # ğŸ”¥ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
            log_step_memory("Step 1 - Human Parsing ì‹œì‘", kwargs.get('session_id', 'unknown'))
            
            # ğŸ”¥ ì„¸ì…˜ í‚¤ ì¼ê´€ì„± í™•ì¸ ë¡œê¹… ì¶”ê°€
            session_id = kwargs.get('session_id', 'unknown')
            self.logger.info(f"ğŸ¯ [Step 1] ì„¸ì…˜ ì‹œì‘ - session_id: {session_id}")
            
            # ğŸ”¥ ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸ ë¡œê¹…
            loaded_models = list(self.ai_models.keys()) if hasattr(self, 'ai_models') and self.ai_models else []
            self.logger.info(f"ğŸ¯ [Step 1] ëª¨ë¸ ë¡œë”© ìƒíƒœ - ë¡œë“œëœ ëª¨ë¸: {loaded_models}")
            
            # ğŸ”¥ ë””ë°”ì´ìŠ¤ ì •ë³´ ë¡œê¹…
            device_info = getattr(self, 'device', 'unknown')
            self.logger.info(f"ğŸ¯ [Step 1] ë””ë°”ì´ìŠ¤ ì •ë³´ - device: {device_info}")
            
            # ğŸ”¥ ì…ë ¥ ë°ì´í„° ê²€ì¦ ë¡œê¹…
            input_keys = list(kwargs.keys()) if kwargs else []
            self.logger.info(f"ğŸ¯ [Step 1] ì…ë ¥ ë°ì´í„° - í‚¤ ê°œìˆ˜: {len(input_keys)}, í‚¤ë“¤: {input_keys}")
            
            try:
                start_time = time.time()
                print(f"âœ… start_time ì„¤ì • ì™„ë£Œ: {start_time}")
                errors = []
                stage_status = {}
                print(f"âœ… ê¸°ë³¸ ë³€ìˆ˜ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ process ë©”ì„œë“œ ì‹œì‘ ë¶€ë¶„ ì˜¤ë¥˜: {e}")
                return {'success': False, 'error': f'Process ì‹œì‘ ì˜¤ë¥˜: {e}'}
            
            try:
                # ğŸ”¥ 1ë‹¨ê³„: ì…ë ¥ ë°ì´í„° ê²€ì¦
                try:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] 1ë‹¨ê³„: ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹œì‘")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] kwargs ì¡´ì¬ ì—¬ë¶€: {kwargs is not None}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] kwargs í‚¤ë“¤: {list(kwargs.keys()) if kwargs else 'None'}")
                    
                    if not kwargs:
                        raise ValueError("ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                    
                    # í•„ìˆ˜ ì…ë ¥ í•„ë“œ í™•ì¸
                    required_fields = ['image', 'person_image', 'input_image']
                    has_required_field = any(field in kwargs for field in required_fields)
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] í•„ìˆ˜ í•„ë“œ ì¡´ì¬ ì—¬ë¶€: {has_required_field}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] í•„ìˆ˜ í•„ë“œ: {required_fields}")
                    
                    if not has_required_field:
                        raise ValueError("í•„ìˆ˜ ì…ë ¥ í•„ë“œ(image, person_image, input_image ì¤‘ í•˜ë‚˜)ê°€ ì—†ìŠµë‹ˆë‹¤")
                    
                    stage_status['input_validation'] = 'success'
                    self.logger.info("âœ… ì…ë ¥ ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] 1ë‹¨ê³„: ì…ë ¥ ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
                    
                except Exception as e:
                    stage_status['input_validation'] = 'failed'
                    error_info = {
                        'stage': 'input_validation',
                        'error_type': type(e).__name__,
                        'message': str(e),
                        'input_keys': list(kwargs.keys()) if kwargs else []
                    }
                    errors.append(error_info)
                    
                    # ì—ëŸ¬ ì¶”ì 
                    if EXCEPTIONS_AVAILABLE:
                        log_detailed_error(
                            DataValidationError(f"ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {str(e)}", 
                                              ErrorCodes.DATA_VALIDATION_FAILED, 
                                              {'input_keys': list(kwargs.keys()) if kwargs else []}),
                            {'step_name': self.step_name, 'step_id': getattr(self, 'step_id', 1)},
                            getattr(self, 'step_id', 1)
                        )
                    
                    return {
                        'success': False,
                        'errors': errors,
                        'stage_status': stage_status,
                        'step_name': self.step_name,
                        'processing_time': time.time() - start_time
                    }
                
                # ğŸ”¥ 2ë‹¨ê³„: ëª©ì—… ë°ì´í„° ì§„ë‹¨
                try:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] 2ë‹¨ê³„: ëª©ì—… ë°ì´í„° ì§„ë‹¨ ì‹œì‘")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] MOCK_DIAGNOSTIC_AVAILABLE: {MOCK_DIAGNOSTIC_AVAILABLE}")
                    
                    if MOCK_DIAGNOSTIC_AVAILABLE:
                        mock_detections = []
                        for key, value in kwargs.items():
                            if value is not None:
                                mock_detection = detect_mock_data(value)
                                if mock_detection['is_mock']:
                                    mock_detections.append({
                                        'input_key': key,
                                        'detection_result': mock_detection
                                    })
                                    self.logger.warning(f"ì…ë ¥ ë°ì´í„° '{key}'ì—ì„œ ëª©ì—… ë°ì´í„° ê°ì§€: {mock_detection}")
                        
                        if mock_detections:
                            stage_status['mock_detection'] = 'warning'
                            errors.append({
                                'stage': 'mock_detection',
                                'error_type': 'MockDataDetectionError',
                                'message': 'ëª©ì—… ë°ì´í„°ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤',
                                'mock_detections': mock_detections
                            })
                        else:
                            stage_status['mock_detection'] = 'success'
                    else:
                        stage_status['mock_detection'] = 'skipped'
                    
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] 2ë‹¨ê³„: ëª©ì—… ë°ì´í„° ì§„ë‹¨ ì™„ë£Œ")
                        
                except Exception as e:
                    stage_status['mock_detection'] = 'failed'
                    self.logger.warning(f"ëª©ì—… ë°ì´í„° ì§„ë‹¨ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # ğŸ”¥ 3ë‹¨ê³„: ì…ë ¥ ë°ì´í„° ë³€í™˜
                try:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] 3ë‹¨ê³„: ì…ë ¥ ë°ì´í„° ë³€í™˜ ì‹œì‘")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] convert_api_input_to_step_input ì¡´ì¬ ì—¬ë¶€: {hasattr(self, 'convert_api_input_to_step_input')}")
                    
                    if hasattr(self, 'convert_api_input_to_step_input'):
                        converted_input = self.convert_api_input_to_step_input(kwargs)
                    else:
                        converted_input = kwargs
                    
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] ë³€í™˜ëœ ì…ë ¥ í‚¤ë“¤: {list(converted_input.keys()) if converted_input else 'None'}")
                    
                    stage_status['input_conversion'] = 'success'
                    self.logger.info("âœ… ì…ë ¥ ë°ì´í„° ë³€í™˜ ì™„ë£Œ")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] 3ë‹¨ê³„: ì…ë ¥ ë°ì´í„° ë³€í™˜ ì™„ë£Œ")
                    
                except Exception as e:
                    stage_status['input_conversion'] = 'failed'
                    error_info = {
                        'stage': 'input_conversion',
                        'error_type': type(e).__name__,
                        'message': str(e)
                    }
                    errors.append(error_info)
                    
                    if EXCEPTIONS_AVAILABLE:
                        log_detailed_error(
                            DataValidationError(f"ì…ë ¥ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {str(e)}", 
                                              ErrorCodes.DATA_VALIDATION_FAILED),
                            {'step_name': self.step_name, 'step_id': getattr(self, 'step_id', 1)},
                            getattr(self, 'step_id', 1)
                        )
                    
                    return {
                        'success': False,
                        'errors': errors,
                        'stage_status': stage_status,
                        'step_name': self.step_name,
                        'processing_time': time.time() - start_time
                    }
                
                # ğŸ”¥ 4ë‹¨ê³„: AI ëª¨ë¸ ë¡œë”© í™•ì¸
                try:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] 4ë‹¨ê³„: AI ëª¨ë¸ ë¡œë”© í™•ì¸ ì‹œì‘")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] self.ai_models ì¡´ì¬ ì—¬ë¶€: {hasattr(self, 'ai_models')}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] self.ai_models í‚¤ë“¤: {list(self.ai_models.keys()) if hasattr(self, 'ai_models') and self.ai_models else 'None'}")
                    
                    if not hasattr(self, 'ai_models') or not self.ai_models:
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] AI ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ - ê°•ì œ ë¡œë”© ì‹œë„")
                        central_hub_success = self._load_ai_models_via_central_hub()
                        direct_load_success = self._load_models_directly()
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] Central Hub ë¡œë”© ê²°ê³¼: {central_hub_success}")
                        print(f"ğŸ”¥ [ë””ë²„ê¹…] ì§ì ‘ ë¡œë”© ê²°ê³¼: {direct_load_success}")
                    
                    # ì‹¤ì œ ëª¨ë¸ vs Mock ëª¨ë¸ í™•ì¸
                    loaded_models = list(self.ai_models.keys()) if hasattr(self, 'ai_models') and self.ai_models else []
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] ë¡œë”©ëœ ëª¨ë¸ ëª©ë¡: {loaded_models}")
                    
                    is_mock_only = all('mock' in model_name.lower() for model_name in loaded_models) if loaded_models else True
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] Mock ëª¨ë¸ë§Œ ìˆëŠ”ì§€: {is_mock_only}")
                    
                    if not loaded_models:
                        raise RuntimeError("AI ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    
                    if is_mock_only:
                        stage_status['model_loading'] = 'warning'
                        errors.append({
                            'stage': 'model_loading',
                            'error_type': 'MockModelWarning',
                            'message': 'ì‹¤ì œ AI ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•„ Mock ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤',
                            'loaded_models': loaded_models
                        })
                    else:
                        stage_status['model_loading'] = 'success'
                        self.logger.info(f"âœ… AI ëª¨ë¸ ë¡œë”© í™•ì¸ ì™„ë£Œ: {loaded_models}")
                    
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] 4ë‹¨ê³„: AI ëª¨ë¸ ë¡œë”© í™•ì¸ ì™„ë£Œ")
                    
                except Exception as e:
                    stage_status['model_loading'] = 'failed'
                    error_info = {
                        'stage': 'model_loading',
                        'error_type': type(e).__name__,
                        'message': str(e)
                    }
                    errors.append(error_info)
                    
                    if EXCEPTIONS_AVAILABLE:
                        log_detailed_error(
                            ModelLoadingError(f"AI ëª¨ë¸ ë¡œë”© í™•ì¸ ì‹¤íŒ¨: {str(e)}", 
                                            ErrorCodes.MODEL_LOADING_FAILED),
                            {'step_name': self.step_name, 'step_id': getattr(self, 'step_id', 1)},
                            getattr(self, 'step_id', 1)
                        )
                    
                    return {
                        'success': False,
                        'errors': errors,
                        'stage_status': stage_status,
                        'step_name': self.step_name,
                        'processing_time': time.time() - start_time
                    }
                
                # ğŸ”¥ 5ë‹¨ê³„: AI ì¶”ë¡  ì‹¤í–‰
                try:
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] 5ë‹¨ê³„: AI ì¶”ë¡  ì‹¤í–‰ ì‹œì‘")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_ai_inference í˜¸ì¶œ ì „")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] converted_input í‚¤ë“¤: {list(converted_input.keys()) if converted_input else 'None'}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] converted_input ê°’ë“¤: {[(k, type(v).__name__) for k, v in converted_input.items()] if converted_input else 'None'}")
                    
                    result = self._run_ai_inference(converted_input)
                    
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] _run_ai_inference í˜¸ì¶œ ì™„ë£Œ")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] result íƒ€ì…: {type(result)}")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] result í‚¤ë“¤: {list(result.keys()) if result else 'None'}")
                    
                    # ì¶”ë¡  ê²°ê³¼ ê²€ì¦
                    if not result or 'success' not in result:
                        raise RuntimeError("AI ì¶”ë¡  ê²°ê³¼ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
                    
                    if not result.get('success', False):
                        raise RuntimeError(f"AI ì¶”ë¡  ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                    
                    stage_status['ai_inference'] = 'success'
                    self.logger.info("âœ… AI ì¶”ë¡  ì™„ë£Œ")
                    print(f"ğŸ”¥ [ë””ë²„ê¹…] 5ë‹¨ê³„: AI ì¶”ë¡  ì‹¤í–‰ ì™„ë£Œ")
                    
                except Exception as e:
                    stage_status['ai_inference'] = 'failed'
                    error_info = {
                        'stage': 'ai_inference',
                        'error_type': type(e).__name__,
                        'message': str(e)
                    }
                    errors.append(error_info)
                    
                    if EXCEPTIONS_AVAILABLE:
                        log_detailed_error(
                            ModelInferenceError(f"AI ì¶”ë¡  ì‹¤íŒ¨: {str(e)}", 
                                              ErrorCodes.AI_INFERENCE_FAILED),
                            {'step_name': self.step_name, 'step_id': getattr(self, 'step_id', 1)},
                            getattr(self, 'step_id', 1)
                        )
                    
                    return {
                        'success': False,
                        'errors': errors,
                        'stage_status': stage_status,
                        'step_name': self.step_name,
                        'processing_time': time.time() - start_time
                    }
                
                # ğŸ”¥ 6ë‹¨ê³„: ì¶œë ¥ ë°ì´í„° ê²€ì¦
                try:
                    # ì¶œë ¥ ë°ì´í„°ì—ì„œ ëª©ì—… ë°ì´í„° ê°ì§€
                    if MOCK_DIAGNOSTIC_AVAILABLE:
                        output_mock_detections = []
                        for key, value in result.items():
                            if value is not None:
                                mock_detection = detect_mock_data(value)
                                if mock_detection['is_mock']:
                                    output_mock_detections.append({
                                        'output_key': key,
                                        'detection_result': mock_detection
                                    })
                        
                        if output_mock_detections:
                            stage_status['output_validation'] = 'warning'
                            errors.append({
                                'stage': 'output_validation',
                                'error_type': 'MockOutputWarning',
                                'message': 'ì¶œë ¥ ë°ì´í„°ì—ì„œ ëª©ì—… ë°ì´í„°ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤',
                                'mock_detections': output_mock_detections
                            })
                        else:
                            stage_status['output_validation'] = 'success'
                    else:
                        stage_status['output_validation'] = 'skipped'
                    
                except Exception as e:
                    stage_status['output_validation'] = 'failed'
                    self.logger.warning(f"ì¶œë ¥ ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # ğŸ”¥ ìµœì¢… ì‘ë‹µ ìƒì„±
                processing_time = time.time() - start_time
                
                # ì„±ê³µ ì—¬ë¶€ ê²°ì • (ì¹˜ëª…ì  ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì‹¤íŒ¨)
                critical_errors = [e for e in errors if e['stage'] in ['input_validation', 'input_conversion', 'ai_inference']]
                is_success = len(critical_errors) == 0
                
                final_result = {
                    'success': is_success,
                    'errors': errors,
                    'stage_status': stage_status,
                    'step_name': self.step_name,
                    'processing_time': processing_time,
                    'is_mock_used': any('mock' in e.get('error_type', '').lower() for e in errors),
                    'critical_error_count': len(critical_errors),
                    'warning_count': len(errors) - len(critical_errors)
                }
                
                # ì„±ê³µí•œ ê²½ìš° ì›ë³¸ ê²°ê³¼ë„ í¬í•¨
                if is_success:
                    final_result.update(result)
                
                # ğŸ”¥ ì„¸ì…˜ ë°ì´í„° ì €ì¥ ë¡œê¹… ì¶”ê°€
                print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 1 ì™„ë£Œ - session_id: {session_id}")
                print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 1 ê²°ê³¼ ë°ì´í„° í¬ê¸°: {len(str(final_result))} bytes")
                print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 1 ì„±ê³µ ì—¬ë¶€: {is_success}")
                print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 1 ì²˜ë¦¬ ì‹œê°„: {processing_time:.3f}ì´ˆ")
                
                # ğŸ”¥ ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ë¡œê¹…
                if is_success and 'parsing_result' in final_result:
                    parsing_data = final_result['parsing_result']
                    print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] Step 1 â†’ Step 2 ì „ë‹¬ ë°ì´í„° ì¤€ë¹„:")
                    print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] - parsing_result íƒ€ì…: {type(parsing_data)}")
                    print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] - parsing_result í‚¤ë“¤: {list(parsing_data.keys()) if isinstance(parsing_data, dict) else 'N/A'}")
                    if isinstance(parsing_data, dict) and 'parsing_map' in parsing_data:
                        parsing_map = parsing_data['parsing_map']
                        print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] - parsing_map íƒ€ì…: {type(parsing_map)}")
                        if hasattr(parsing_map, 'shape'):
                            print(f"ğŸ”¥ [ì„¸ì…˜ ì¶”ì ] - parsing_map í¬ê¸°: {parsing_map.shape}")
                
                # ğŸ”¥ ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ëª¨ë‹ˆí„°ë§
                log_step_memory("Step 1 - Human Parsing ì™„ë£Œ", session_id)
                cleanup_result = cleanup_step_memory(aggressive=False)
                print(f"ğŸ”¥ [ë©”ëª¨ë¦¬ ì •ë¦¬] Step 1 ì™„ë£Œ í›„ ì •ë¦¬: {cleanup_result.get('memory_freed_gb', 0):.2f}GB í•´ì œ")
                
                return final_result
                
            except Exception as e:
                # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜
                processing_time = time.time() - start_time
                
                if EXCEPTIONS_AVAILABLE:
                    error = convert_to_mycloset_exception(e, {
                        'step_name': self.step_name,
                        'step_id': getattr(self, 'step_id', 1),
                        'operation': 'process'
                    })
                    track_exception(error, {
                        'step_name': self.step_name,
                        'step_id': getattr(self, 'step_id', 1),
                        'operation': 'process'
                    }, getattr(self, 'step_id', 1))
                    
                    return create_exception_response(
                        error,
                        self.step_name,
                        getattr(self, 'step_id', 1),
                        kwargs.get('session_id', 'unknown')
                    )
                else:
                    return {
                        'success': False,
                        'error': 'UNEXPECTED_ERROR',
                        'message': f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                        'step_name': self.step_name,
                        'processing_time': processing_time
                    }
        
        # ==============================================
        # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
        # ==============================================
        
        def get_step_requirements(self) -> Dict[str, Any]:
            """Step ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
            return {
                'required_models': ['graphonomy.pth', 'u2net.pth'],
                'primary_model': 'graphonomy.pth',
                'model_size_mb': 1200.0,
                'input_format': 'RGB image',
                'output_format': '20-class segmentation map',
                'device_support': ['cpu', 'mps', 'cuda'],
                'memory_requirement_gb': 2.0,
                'central_hub_required': True
            }

        def _get_service_from_central_hub(self, service_key: str):
            """Central Hubì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
            try:
                if hasattr(self, 'di_container') and self.di_container:
                    return self.di_container.get_service(service_key)
                return None
            except Exception as e:
                self.logger.warning(f"âš ï¸ Central Hub ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                return None

        def convert_api_input_to_step_input(self, api_input: Dict[str, Any]) -> Dict[str, Any]:
            """API ì…ë ¥ì„ Step ì…ë ¥ìœ¼ë¡œ ë³€í™˜ (kwargs ë°©ì‹) - ê°•í™”ëœ ì´ë¯¸ì§€ ì „ë‹¬"""
            try:
                step_input = api_input.copy()
                
                # ğŸ”¥ ê°•í™”ëœ ì´ë¯¸ì§€ ì ‘ê·¼ ë°©ì‹
                image = None
                
                # 1ìˆœìœ„: ì„¸ì…˜ ë°ì´í„°ì—ì„œ ë¡œë“œ (base64 â†’ PIL ë³€í™˜)
                if 'session_data' in step_input:
                    session_data = step_input['session_data']
                    self.logger.info(f"ğŸ” ì„¸ì…˜ ë°ì´í„° í‚¤ë“¤: {list(session_data.keys())}")
                    
                    if 'original_person_image' in session_data:
                        try:
                            import base64
                            from io import BytesIO
                            from PIL import Image
                            
                            person_b64 = session_data['original_person_image']
                            if person_b64 and len(person_b64) > 100:  # ìœ íš¨í•œ base64ì¸ì§€ í™•ì¸
                                person_bytes = base64.b64decode(person_b64)
                                image = Image.open(BytesIO(person_bytes)).convert('RGB')
                                self.logger.info(f"âœ… ì„¸ì…˜ ë°ì´í„°ì—ì„œ original_person_image ë¡œë“œ: {image.size}")
                            else:
                                self.logger.warning("âš ï¸ original_person_imageê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŒ")
                        except Exception as session_error:
                            self.logger.warning(f"âš ï¸ ì„¸ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {session_error}")
                
                # 2ìˆœìœ„: ì§ì ‘ ì „ë‹¬ëœ ì´ë¯¸ì§€ (ì´ë¯¸ PIL Imageì¸ ê²½ìš°)
                if image is None:
                    if 'person_image' in step_input and step_input['person_image'] is not None:
                        image = step_input['person_image']
                        self.logger.info(f"âœ… ì§ì ‘ ì „ë‹¬ëœ person_image ì‚¬ìš©: {getattr(image, 'size', 'unknown')}")
                    elif 'image' in step_input and step_input['image'] is not None:
                        image = step_input['image']
                        self.logger.info(f"âœ… ì§ì ‘ ì „ë‹¬ëœ image ì‚¬ìš©: {getattr(image, 'size', 'unknown')}")
                    elif 'clothing_image' in step_input and step_input['clothing_image'] is not None:
                        image = step_input['clothing_image']
                        self.logger.info(f"âœ… ì§ì ‘ ì „ë‹¬ëœ clothing_image ì‚¬ìš©: {getattr(image, 'size', 'unknown')}")
                
                # 3ìˆœìœ„: ê¸°ë³¸ê°’
                if image is None:
                    self.logger.warning("âš ï¸ ì´ë¯¸ì§€ê°€ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                    image = None
                
                # ë³€í™˜ëœ ì…ë ¥ êµ¬ì„±
                converted_input = {
                    'image': image,
                    'person_image': image,
                    'session_id': step_input.get('session_id'),
                    'confidence_threshold': step_input.get('confidence_threshold', 0.7),
                    'enhance_quality': step_input.get('enhance_quality', True),
                    'force_ai_processing': step_input.get('force_ai_processing', True)
                }
                
                # ğŸ”¥ ìƒì„¸ ë¡œê¹…
                self.logger.info(f"âœ… API ì…ë ¥ ë³€í™˜ ì™„ë£Œ: {len(converted_input)}ê°œ í‚¤")
                self.logger.info(f"âœ… ì´ë¯¸ì§€ ìƒíƒœ: {'ìˆìŒ' if image is not None else 'ì—†ìŒ'}")
                if image is not None:
                    self.logger.info(f"âœ… ì´ë¯¸ì§€ ì •ë³´: íƒ€ì…={type(image)}, í¬ê¸°={getattr(image, 'size', 'unknown')}")
                else:
                    self.logger.error("âŒ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - AI ì²˜ë¦¬ ë¶ˆê°€ëŠ¥")
                
                return converted_input
                
            except Exception as e:
                self.logger.error(f"âŒ API ì…ë ¥ ë³€í™˜ ì‹¤íŒ¨: {e}")
                return api_input
        
        def _convert_step_output_type(self, step_output: Dict[str, Any], *args, **kwargs) -> Dict[str, Any]:
            """Step ì¶œë ¥ì„ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
            try:
                if not isinstance(step_output, dict):
                    return {
                        'success': False,
                        'error': 'Invalid step output format',
                        'step_name': self.step_name
                    }
                
                # ê¸°ë³¸ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                api_response = {
                    'success': step_output.get('success', True),
                    'step_name': self.step_name,
                    'step_id': self.step_id,
                    'processing_time': step_output.get('processing_time', 0.0),
                    'central_hub_used': True
                }
                
                # ê²°ê³¼ ë°ì´í„° í¬í•¨
                if 'result' in step_output:
                    api_response['result'] = step_output['result']
                elif 'parsing_map' in step_output:
                    api_response['result'] = {
                        'parsing_map': step_output['parsing_map'],
                        'confidence': step_output.get('confidence', 0.0),
                        'detected_parts': step_output.get('detected_parts', [])
                    }
                else:
                    api_response['result'] = step_output
                
                return api_response
                
            except Exception as e:
                self.logger.error(f"âŒ _convert_step_output_type ì‹¤íŒ¨: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'step_name': self.step_name,
                    'step_id': self.step_id
                }
        
        def convert_step_output_to_api_response(self, step_output: Dict[str, Any]) -> Dict[str, Any]:
            """Step ì¶œë ¥ì„ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (step_service.py í˜¸í™˜)"""
            try:
                return self._convert_step_output_type(step_output)
            except Exception as e:
                self.logger.error(f"âŒ convert_step_output_to_api_response ì‹¤íŒ¨: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'message': 'API ì‘ë‹µ ë³€í™˜ ì‹¤íŒ¨',
                    'step_name': self.step_name,
                    'step_id': self.step_id,
                    'timestamp': time.time()
                }
                
                # ì˜¤ë¥˜ ì •ë³´ í¬í•¨
                if 'error' in step_output:
                    api_response['error'] = step_output['error']
                
                return api_response
                
            except Exception as e:
                return {
                    'success': False,
                    'error': f'Output conversion failed: {str(e)}',
                    'step_name': self.step_name
                }
        
        def cleanup_resources(self):
            """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
            try:
                # AI ëª¨ë¸ ì •ë¦¬
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                    except:
                        pass
                
                self.ai_models.clear()
                self.loaded_models.clear()
                
                # ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
                if hasattr(self, 'executor'):
                    self.executor.shutdown(wait=False)
                
                # ğŸ”¥ 128GB M3 Max ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
                for _ in range(3):
                    gc.collect()
                if TORCH_AVAILABLE and MPS_AVAILABLE:
                    try:
                        torch.mps.empty_cache()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                self.logger.info("âœ… HumanParsingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸°
# ==============================================

__all__ = [
    # ë©”ì¸ Step í´ë˜ìŠ¤ (í•µì‹¬)
    "HumanParsingStep",
]

