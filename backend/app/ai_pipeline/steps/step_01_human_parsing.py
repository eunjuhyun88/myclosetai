# backend/app/ai_pipeline/steps/step_01_human_parsing.py
"""
ğŸ”¥ MyCloset AI Step 01 - Human Parsing (ì™„ì „í•œ AI ì—°ë™ ë° ì˜ì¡´ì„± ì£¼ì…)
================================================================
âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ModelLoader ì—°ë™
âœ… ì‹¤ì œ AI ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬í˜„ (Graphonomy ê¸°ë°˜)
âœ… ì²´í¬í¬ì¸íŠ¸ â†’ ëª¨ë¸ í´ë˜ìŠ¤ ë³€í™˜ ë¡œì§
âœ… BaseStepMixin ì™„ì „ ìƒì† ë° í™œìš©
âœ… 20ê°œ ë¶€ìœ„ ì •ë°€ ì¸ì²´ íŒŒì‹±
âœ… M3 Max ìµœì í™”
âœ… ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›

í•µì‹¬ êµ¬ì¡°:
StepFactory â†’ ModelLoader (ìƒì„±) â†’ BaseStepMixin (ìƒì„±) â†’ ì˜ì¡´ì„± ì£¼ì… â†’ ì™„ì„±ëœ Step

Author: MyCloset AI Team
Date: 2025-07-22
Version: 2.0 (Complete AI Integration)
"""

import os
import gc
import time
import asyncio
import logging
import threading
import base64
import json
import hashlib
import traceback
import weakref
from typing import Dict, Any, Optional, Tuple, List, Union
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from io import BytesIO
from enum import Enum
import platform
# íŒŒì¼ ìƒë‹¨ import ì„¹ì…˜ì—
from ..utils.pytorch_safe_ops import (
    safe_max, safe_amax, safe_argmax,
    extract_keypoints_from_heatmaps,
    tensor_to_pil_conda_optimized
)

# ==============================================
# ğŸ”¥ 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import (ì•ˆì „í•œ ë°©ì‹)
# ==============================================

# conda í™˜ê²½ ì²´í¬
CONDA_ENV = os.environ.get('CONDA_DEFAULT_ENV', 'none')
if CONDA_ENV != 'none':
    print(f"âœ… conda í™˜ê²½ ê°ì§€: {CONDA_ENV}")

logger = logging.getLogger(__name__)

# PyTorch ì•ˆì „ import
TORCH_AVAILABLE = False
MPS_AVAILABLE = False
try:
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    TORCH_AVAILABLE = True
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        MPS_AVAILABLE = True
        logger.info("ğŸ M3 Max MPS ì‚¬ìš© ê°€ëŠ¥")
    
except ImportError:
    logger.warning("âš ï¸ PyTorch ì—†ìŒ - conda install pytorch torchvision -c pytorch")

# ê¸°íƒ€ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    logger.warning("âš ï¸ NumPy ì—†ìŒ - conda install numpy")

try:
    from PIL import Image, ImageDraw, ImageFont, ImageEnhance
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    logger.warning("âš ï¸ PIL ì—†ìŒ - conda install pillow")

# OpenCV ì•ˆì „ import
OPENCV_AVAILABLE = False
try:
    os.environ['OPENCV_IO_ENABLE_OPENEXR'] = '0'
    os.environ['OPENCV_IO_ENABLE_JASPER'] = '0'
    
    import cv2
    OPENCV_AVAILABLE = True
    logger.info(f"âœ… OpenCV {cv2.__version__} ë¡œë“œ ì„±ê³µ")
    
except ImportError:
    logger.warning("âš ï¸ OpenCV ì—†ìŒ - conda install opencv -c conda-forge")
    
    # OpenCV í´ë°±
    class OpenCVFallback:
        def __init__(self):
            self.INTER_LINEAR = 1
            self.INTER_CUBIC = 2
            self.COLOR_BGR2RGB = 4
            self.COLOR_RGB2BGR = 3
        
        def resize(self, img, size, interpolation=1):
            try:
                if PIL_AVAILABLE:
                    pil_img = Image.fromarray(img) if hasattr(img, 'shape') else img
                    resized = pil_img.resize(size)
                    return np.array(resized)
                return img
            except:
                return img
        
        def cvtColor(self, img, code):
            if hasattr(img, 'shape') and len(img.shape) == 3:
                if code in [3, 4]:  # BGR<->RGB
                    return img[:, :, ::-1]
            return img
        
        def imread(self, path):
            try:
                if PIL_AVAILABLE:
                    return np.array(Image.open(path))
                return None
            except:
                return None
    
    cv2 = OpenCVFallback()

# ==============================================
# ğŸ”¥ 2. BaseStepMixin ì˜ì¡´ì„± ì£¼ì… Import
# ==============================================

try:
    from .base_step_mixin import BaseStepMixin, HumanParsingMixin
    BASE_STEP_MIXIN_AVAILABLE = True
    logger.info("âœ… BaseStepMixin import ì„±ê³µ")
except ImportError as e:
    BASE_STEP_MIXIN_AVAILABLE = False
    logger.error(f"âŒ BaseStepMixin import ì‹¤íŒ¨: {e}")
    
    # BaseStepMixin í´ë°± í´ë˜ìŠ¤ (ì˜ì¡´ì„± ì£¼ì… ì§€ì›)
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.step_name = kwargs.get('step_name', 'BaseStep')
            self.device = kwargs.get('device', 'cpu')
            self.logger = logging.getLogger(f"pipeline.{self.step_name}")
            self.is_initialized = False
            
            # ì˜ì¡´ì„± ì£¼ì…ì„ ìœ„í•œ ì†ì„±ë“¤
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            
            # ëª¨ë¸ ìºì‹œ
            self.model_cache = {}
            self.loaded_models = {}
            
        def set_model_loader(self, model_loader):
            """ModelLoader ì˜ì¡´ì„± ì£¼ì…"""
            self.model_loader = model_loader
            self.logger.info("âœ… ModelLoader ì£¼ì… ì™„ë£Œ")
        
        def set_memory_manager(self, memory_manager):
            """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
            self.memory_manager = memory_manager
            self.logger.info("âœ… MemoryManager ì£¼ì… ì™„ë£Œ")
        
        def set_data_converter(self, data_converter):
            """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
            self.data_converter = data_converter
            self.logger.info("âœ… DataConverter ì£¼ì… ì™„ë£Œ")
        
        async def cleanup(self):
            """ê¸°ë³¸ ì •ë¦¬"""
            self.model_cache.clear()
            self.loaded_models.clear()
            gc.collect()
    
    class HumanParsingMixin(BaseStepMixin):
        def __init__(self, **kwargs):
            super().__init__(**kwargs)
            self.step_name = "HumanParsingStep"
            self.num_classes = 20

# ==============================================
# ğŸ”¥ 3. ì¸ì²´ íŒŒì‹± AI ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬í˜„
# ==============================================

class GraphonomyModel(nn.Module if TORCH_AVAILABLE else object):
    """ì‹¤ì œ Graphonomy ì¸ì²´ íŒŒì‹± ëª¨ë¸ ì•„í‚¤í…ì²˜"""
    
    def __init__(self, num_classes: int = 20, device: str = "mps"):
        if TORCH_AVAILABLE:
            super(GraphonomyModel, self).__init__()
        
        self.num_classes = num_classes
        self.device = device
        self.model_name = "GraphonomyModel"
        
        if TORCH_AVAILABLE:
            self._build_model()
        
        self.logger = logging.getLogger(f"{__name__}.GraphonomyModel")
    
    def _build_model(self):
        """Graphonomy ëª¨ë¸ êµ¬ì¡° ìƒì„±"""
        # Backbone: ResNet-like encoder
        self.backbone = nn.Sequential(
            # Initial Conv Block
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            # Layer 1 (64 channels)
            self._make_layer(64, 64, 2, stride=1),
            
            # Layer 2 (128 channels)  
            self._make_layer(64, 128, 2, stride=2),
            
            # Layer 3 (256 channels)
            self._make_layer(128, 256, 2, stride=2),
            
            # Layer 4 (512 channels)
            self._make_layer(256, 512, 2, stride=2),
        )
        
        # ASPP (Atrous Spatial Pyramid Pooling)
        self.aspp = nn.ModuleList([
            nn.Conv2d(512, 256, kernel_size=1, bias=False),
            nn.Conv2d(512, 256, kernel_size=3, padding=6, dilation=6, bias=False),
            nn.Conv2d(512, 256, kernel_size=3, padding=12, dilation=12, bias=False),
            nn.Conv2d(512, 256, kernel_size=3, padding=18, dilation=18, bias=False),
        ])
        
        # Global Average Pooling
        self.global_avg_pool = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Conv2d(512, 256, 1, stride=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Conv2d(1280, 256, kernel_size=3, padding=1, bias=False),  # 5*256=1280
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
        )
        
        # Final Classification Layer
        self.classifier = nn.Conv2d(256, self.num_classes, kernel_size=1)
        
        # Edge Detection Branch (Graphonomy íŠ¹ì§•)
        self.edge_classifier = nn.Conv2d(256, 1, kernel_size=1)
        
        self.logger.info(f"âœ… Graphonomy ëª¨ë¸ êµ¬ì¡° ìƒì„± ì™„ë£Œ (í´ë˜ìŠ¤: {self.num_classes}ê°œ)")
    
    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        """ResNet ìŠ¤íƒ€ì¼ ë ˆì´ì–´ ìƒì„±"""
        layers = []
        
        # Downsampling layer
        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                               stride=stride, padding=1, bias=False))
        layers.append(nn.BatchNorm2d(out_channels))
        layers.append(nn.ReLU(inplace=True))
        
        # Additional blocks
        for _ in range(1, blocks):
            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, 
                                   stride=1, padding=1, bias=False))
            layers.append(nn.BatchNorm2d(out_channels))
            layers.append(nn.ReLU(inplace=True))
        
        return nn.Sequential(*layers)
    
    def forward(self, x):
        """ìˆœì „íŒŒ"""
        if not TORCH_AVAILABLE:
            return x
        
        batch_size, _, h, w = x.shape
        
        # Backbone feature extraction
        features = self.backbone(x)
        
        # ASPP feature extraction
        aspp_features = []
        
        for aspp_layer in self.aspp:
            aspp_features.append(aspp_layer(features))
        
        # Global average pooling
        global_feat = self.global_avg_pool(features)
        global_feat = F.interpolate(global_feat, size=features.shape[2:], 
                                   mode='bilinear', align_corners=True)
        aspp_features.append(global_feat)
        
        # Concatenate ASPP features
        aspp_concat = torch.cat(aspp_features, dim=1)
        
        # Decode
        decoded = self.decoder(aspp_concat)
        
        # Classification
        parsing_logits = self.classifier(decoded)
        edge_logits = self.edge_classifier(decoded)
        
        # Upsample to original size
        parsing_logits = F.interpolate(parsing_logits, size=(h, w), 
                                      mode='bilinear', align_corners=True)
        edge_logits = F.interpolate(edge_logits, size=(h, w), 
                                   mode='bilinear', align_corners=True)
        
        return {
            'parsing': parsing_logits,
            'edge': edge_logits
        }
    
    def load_checkpoint(self, checkpoint_data):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            if not TORCH_AVAILABLE:
                return False
            
            # ë‹¤ì–‘í•œ ì²´í¬í¬ì¸íŠ¸ í˜•íƒœ ì²˜ë¦¬
            if isinstance(checkpoint_data, dict):
                if 'model' in checkpoint_data:
                    state_dict = checkpoint_data['model']
                elif 'state_dict' in checkpoint_data:
                    state_dict = checkpoint_data['state_dict']
                else:
                    state_dict = checkpoint_data
            else:
                state_dict = checkpoint_data
            
            # í‚¤ ì´ë¦„ ë§¤í•‘ (í•„ìš”í•œ ê²½ìš°)
            new_state_dict = {}
            for key, value in state_dict.items():
                # ì¼ë°˜ì ì¸ í‚¤ ë³€í™˜
                new_key = key
                if key.startswith('module.'):
                    new_key = key[7:]  # 'module.' ì œê±°
                new_state_dict[new_key] = value
            
            # ëª¨ë¸ì— ë¡œë“œ (strict=Falseë¡œ ëˆ„ë½ëœ í‚¤ ë¬´ì‹œ)
            self.load_state_dict(new_state_dict, strict=False)
            
            self.logger.info("âœ… Graphonomy ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

class HumanParsingU2Net(nn.Module if TORCH_AVAILABLE else object):
    """ë°±ì—…ìš© U2Net ê¸°ë°˜ ì¸ì²´ íŒŒì‹± ëª¨ë¸"""
    
    def __init__(self, num_classes: int = 20, device: str = "mps"):
        if TORCH_AVAILABLE:
            super(HumanParsingU2Net, self).__init__()
        
        self.num_classes = num_classes
        self.device = device
        self.model_name = "HumanParsingU2Net"
        
        if TORCH_AVAILABLE:
            self._build_model()
        
        self.logger = logging.getLogger(f"{__name__}.HumanParsingU2Net")
    
    def _build_model(self):
        """ê°„ì†Œí™”ëœ U-Net êµ¬ì¡°"""
        # Encoder
        self.encoder = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            # Block 2
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            # Block 3
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(256, 128, 2, stride=2),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(128, 64, 2, stride=2),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(64, 32, 2, stride=2),
            nn.Conv2d(32, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
        )
        
        # Final classifier
        self.classifier = nn.Conv2d(32, self.num_classes, 1)
        
        self.logger.info("âœ… U2Net ê¸°ë°˜ ëª¨ë¸ êµ¬ì¡° ìƒì„± ì™„ë£Œ")
    
    def forward(self, x):
        """ìˆœì „íŒŒ"""
        if not TORCH_AVAILABLE:
            return x
        
        # Encode
        features = self.encoder(x)
        
        # Decode
        decoded = self.decoder(features)
        
        # Classify
        output = self.classifier(decoded)
        
        return {'parsing': output}
    
    def load_checkpoint(self, checkpoint_data):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            if not TORCH_AVAILABLE:
                return False
            
            if isinstance(checkpoint_data, dict):
                if 'model' in checkpoint_data:
                    state_dict = checkpoint_data['model']
                elif 'state_dict' in checkpoint_data:
                    state_dict = checkpoint_data['state_dict']
                else:
                    state_dict = checkpoint_data
            else:
                state_dict = checkpoint_data
            
            self.load_state_dict(state_dict, strict=False)
            self.logger.info("âœ… U2Net ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ U2Net ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

# ==============================================
# ğŸ”¥ 4. ì„¤ì • í´ë˜ìŠ¤
# ==============================================

@dataclass
class HumanParsingConfig:
    """ì¸ì²´ íŒŒì‹± Step ì„¤ì •"""
    
    # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
    model_name: str = "human_parsing_graphonomy"
    backup_model: str = "human_parsing_u2net"
    device: Optional[str] = None
    strict_mode: bool = True
    
    # ì…ë ¥/ì¶œë ¥ ì„¤ì •
    input_size: Tuple[int, int] = (512, 512)
    num_classes: int = 20
    confidence_threshold: float = 0.5
    
    # M3 Max ìµœì í™”
    use_fp16: bool = True
    use_coreml: bool = False
    enable_neural_engine: bool = True
    memory_efficient: bool = True
    
    # ì²˜ë¦¬ ì„¤ì •
    batch_size: int = 1
    apply_postprocessing: bool = True
    noise_reduction: bool = True
    edge_refinement: bool = True
    
    # ì‹œê°í™” ì„¤ì •
    enable_visualization: bool = True
    visualization_quality: str = "high"
    show_part_labels: bool = True
    overlay_opacity: float = 0.7
    
    # ìºì‹œ ë° ì„±ëŠ¥
    max_cache_size: int = 10
    warmup_enabled: bool = True
    
    # í˜¸í™˜ì„± íŒŒë¼ë¯¸í„°ë“¤
    optimization_enabled: bool = True
    quality_level: str = "balanced"
    device_type: str = "auto"
    
    def __post_init__(self):
        """í›„ì²˜ë¦¬ ì´ˆê¸°í™”"""
        if self.device is None:
            self.device = self._detect_device()
    
    def _detect_device(self) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if MPS_AVAILABLE:
            return 'mps'
        elif TORCH_AVAILABLE and torch.cuda.is_available():
            return 'cuda'
        else:
            return 'cpu'

# ==============================================
# ğŸ”¥ 5. ì¸ì²´ ë¶€ìœ„ ë° ìƒ‰ìƒ ì •ì˜
# ==============================================

BODY_PARTS = {
    0: 'background',
    1: 'hat',
    2: 'hair', 
    3: 'glove',
    4: 'sunglasses',
    5: 'upper_clothes',
    6: 'dress',
    7: 'coat',
    8: 'socks',
    9: 'pants',
    10: 'torso_skin',
    11: 'scarf',
    12: 'skirt',
    13: 'face',
    14: 'left_arm',
    15: 'right_arm',
    16: 'left_leg',
    17: 'right_leg',
    18: 'left_shoe',
    19: 'right_shoe'
}

VISUALIZATION_COLORS = {
    0: (0, 0, 0),           # Background
    1: (255, 0, 0),         # Hat
    2: (255, 165, 0),       # Hair
    3: (255, 255, 0),       # Glove
    4: (0, 255, 0),         # Sunglasses
    5: (0, 255, 255),       # Upper-clothes
    6: (0, 0, 255),         # Dress
    7: (255, 0, 255),       # Coat
    8: (128, 0, 128),       # Socks
    9: (255, 192, 203),     # Pants
    10: (255, 218, 185),    # Torso-skin
    11: (210, 180, 140),    # Scarf
    12: (255, 20, 147),     # Skirt
    13: (255, 228, 196),    # Face
    14: (255, 160, 122),    # Left-arm
    15: (255, 182, 193),    # Right-arm
    16: (173, 216, 230),    # Left-leg
    17: (144, 238, 144),    # Right-leg
    18: (139, 69, 19),      # Left-shoe
    19: (160, 82, 45)       # Right-shoe
}

CLOTHING_CATEGORIES = {
    'upper_body': [5, 6, 7, 11],     # ìƒì˜, ë“œë ˆìŠ¤, ì½”íŠ¸, ìŠ¤ì¹´í”„
    'lower_body': [9, 12],           # ë°”ì§€, ìŠ¤ì»¤íŠ¸
    'accessories': [1, 3, 4],        # ëª¨ì, ì¥ê°‘, ì„ ê¸€ë¼ìŠ¤
    'footwear': [8, 18, 19],         # ì–‘ë§, ì‹ ë°œ
    'skin': [10, 13, 14, 15, 16, 17] # í”¼ë¶€ ë¶€ìœ„
}

# ==============================================
# ğŸ”¥ 6. ë©”ì¸ HumanParsingStep í´ë˜ìŠ¤ (ì˜ì¡´ì„± ì£¼ì… ì™„ì „ ì§€ì›)
# ==============================================

class HumanParsingStep(HumanParsingMixin):
    """
    ğŸ”¥ ì™„ì „í•œ AI ì—°ë™ ì¸ì²´ íŒŒì‹± Step (ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´)
    
    âœ… ì˜ì¡´ì„± ì£¼ì…ì„ í†µí•œ ModelLoader ì—°ë™
    âœ… ì‹¤ì œ AI ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬í˜„ (Graphonomy)
    âœ… ì²´í¬í¬ì¸íŠ¸ â†’ ëª¨ë¸ í´ë˜ìŠ¤ ë³€í™˜ ë¡œì§
    âœ… BaseStepMixin ì™„ì „ í™œìš©
    âœ… 20ê°œ ë¶€ìœ„ ì •ë°€ ì¸ì²´ íŒŒì‹±
    âœ… M3 Max ìµœì í™”
    âœ… ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±
    âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], HumanParsingConfig]] = None,
        **kwargs
    ):
        """ì´ˆê¸°í™” (ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´)"""
        
        # BaseStepMixin/HumanParsingMixin ì´ˆê¸°í™”
        super().__init__(step_name="HumanParsingStep", device=device, **kwargs)
        
        # Step ì„¤ì •
        self.config = self._setup_config(config, kwargs)
        self.device = device or self.config.device
        self.step_number = 1
        
        # AI ëª¨ë¸ ê´€ë ¨ ì†ì„±
        self._ai_models = {}  # ì‹¤ì œ AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë“¤
        self._model_checkpoints = {}  # ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ë“¤
        
        # ìƒíƒœ ì¶”ì 
        self.processing_stats = {
            'total_processed': 0,
            'average_time': 0.0,
            'success_count': 0,
            'error_count': 0,
            'model_loader_calls': 0,
            'ai_inference_calls': 0
        }
        
        # ìºì‹œ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.result_cache = {}
        self.cache_lock = threading.RLock()
        self.executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="human_parsing")
        
        self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ - ì˜ì¡´ì„± ì£¼ì… ëŒ€ê¸° ì¤‘")
    
    def _setup_config(self, config, kwargs) -> HumanParsingConfig:
        """ì„¤ì • ê°ì²´ ìƒì„±"""
        try:
            if isinstance(config, HumanParsingConfig):
                # kwargsë¡œ ì„¤ì • ì—…ë°ì´íŠ¸
                for key, value in kwargs.items():
                    if hasattr(config, key):
                        setattr(config, key, value)
                return config
            
            elif isinstance(config, dict):
                merged_config = {**config, **kwargs}
                return HumanParsingConfig(**merged_config)
            
            else:
                return HumanParsingConfig(**kwargs)
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì„¤ì • ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            return HumanParsingConfig(
                device=self.device,
                strict_mode=kwargs.get('strict_mode', True)
            )
    
    # ==============================================
    # ğŸ”¥ 7. ì˜ì¡´ì„± ì£¼ì… í›„ ì´ˆê¸°í™” ë©”ì„œë“œë“¤
    # ==============================================
    
    async def initialize(self) -> bool:
        """Step ì´ˆê¸°í™” (ì˜ì¡´ì„± ì£¼ì… í›„ í˜¸ì¶œ)"""
        try:
            self.logger.info("ğŸ”„ Step 01: ì¸ì²´ íŒŒì‹± ì´ˆê¸°í™” ì‹œì‘...")
            
            if self.is_initialized:
                self.logger.info("âœ… ì´ë¯¸ ì´ˆê¸°í™”ë¨")
                return True
            
            # 1. ì˜ì¡´ì„± ì£¼ì… í™•ì¸
            if not self._check_dependencies():
                if self.config.strict_mode:
                    raise RuntimeError("âŒ strict_mode: í•„ìˆ˜ ì˜ì¡´ì„± ëˆ„ë½")
                self.logger.warning("âš ï¸ ì¼ë¶€ ì˜ì¡´ì„± ëˆ„ë½ - ì œí•œëœ ê¸°ëŠ¥ìœ¼ë¡œ ë™ì‘")
            
            # 2. AI ëª¨ë¸ ë¡œë“œ
            model_load_success = await self._load_ai_models()
            
            if self.config.strict_mode and not model_load_success:
                raise RuntimeError("âŒ strict_mode: AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            
            # 3. ëª¨ë¸ ì›Œë°ì—…
            if self.config.warmup_enabled and model_load_success:
                await self._warmup_models()
            
            # 4. M3 Max ìµœì í™”
            if self.device == 'mps':
                self._apply_m3_max_optimizations()
            
            self.is_initialized = True
            self.logger.info("âœ… Step 01: ì¸ì²´ íŒŒì‹± ì´ˆê¸°í™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Step 01 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            self.is_initialized = True  # ë¶€ë¶„ ì´ˆê¸°í™” í—ˆìš©
            return False
    
    def _check_dependencies(self) -> bool:
        """ì˜ì¡´ì„± ì£¼ì… í™•ì¸"""
        dependencies_ok = True
        
        if not self.model_loader:
            self.logger.warning("âš ï¸ ModelLoader ì£¼ì…ë˜ì§€ ì•ŠìŒ")
            dependencies_ok = False
        else:
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± í™•ì¸ë¨")
        
        if not self.memory_manager:
            self.logger.debug("ğŸ“ MemoryManager ì„ íƒì  ì˜ì¡´ì„± ì—†ìŒ")
        else:
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± í™•ì¸ë¨")
        
        if not self.data_converter:
            self.logger.debug("ğŸ“ DataConverter ì„ íƒì  ì˜ì¡´ì„± ì—†ìŒ")
        else:
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± í™•ì¸ë¨")
        
        return dependencies_ok
    
    async def _load_ai_models(self) -> bool:
        """AI ëª¨ë¸ë“¤ ë¡œë“œ (ModelLoader â†’ ì²´í¬í¬ì¸íŠ¸ â†’ AI ëª¨ë¸ í´ë˜ìŠ¤)"""
        try:
            success = False
            
            # 1. ì£¼ ëª¨ë¸ ë¡œë“œ (Graphonomy)
            primary_success = await self._load_primary_ai_model()
            if primary_success:
                success = True
                self.logger.info("âœ… ì£¼ AI ëª¨ë¸ (Graphonomy) ë¡œë“œ ì„±ê³µ")
            
            # 2. ë°±ì—… ëª¨ë¸ ë¡œë“œ (U2Net)
            backup_success = await self._load_backup_ai_model()
            if backup_success:
                success = True
                self.logger.info("âœ… ë°±ì—… AI ëª¨ë¸ (U2Net) ë¡œë“œ ì„±ê³µ")
            
            if not success:
                self.logger.error("âŒ ëª¨ë“  AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            else:
                self.logger.info(f"ğŸ“Š ë¡œë“œëœ AI ëª¨ë¸ ìˆ˜: {len(self._ai_models)}")
            
            return success
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def _load_primary_ai_model(self) -> bool:
        """ì£¼ AI ëª¨ë¸ (Graphonomy) ë¡œë“œ"""
        try:
            if not self.model_loader:
                self.logger.warning("âš ï¸ ModelLoaderê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ")
                return False
            
            self.logger.info(f"ğŸ“¦ ModelLoaderë¡œë¶€í„° ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {self.config.model_name}")
            
            # Step 1: ModelLoaderë¥¼ í†µí•´ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = None
            if hasattr(self.model_loader, 'load_model_async'):
                checkpoint = await self.model_loader.load_model_async(self.config.model_name)
            elif hasattr(self.model_loader, 'get_model'):
                checkpoint = self.model_loader.get_model(self.config.model_name)
            elif hasattr(self.model_loader, 'load_model'):
                checkpoint = self.model_loader.load_model(self.config.model_name)
            
            if checkpoint is None:
                self.logger.warning(f"âš ï¸ ModelLoaderì—ì„œ ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜ ì‹¤íŒ¨: {self.config.model_name}")
                return False
            
            self.processing_stats['model_loader_calls'] += 1
            
            # Step 2: ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë¡œ ë³€í™˜
            ai_model = await self._checkpoint_to_ai_model(
                checkpoint, 
                model_class=GraphonomyModel,
                model_name='graphonomy'
            )
            
            if ai_model:
                self._ai_models['primary'] = ai_model
                self._model_checkpoints['primary'] = checkpoint
                self.logger.info("âœ… Graphonomy AI ëª¨ë¸ ìƒì„± ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                self.logger.error("âŒ ì²´í¬í¬ì¸íŠ¸ â†’ AI ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ì£¼ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def _load_backup_ai_model(self) -> bool:
        """ë°±ì—… AI ëª¨ë¸ (U2Net) ë¡œë“œ"""
        try:
            if not self.model_loader:
                return False
            
            self.logger.info(f"ğŸ“¦ ë°±ì—… ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {self.config.backup_model}")
            
            # ModelLoaderë¥¼ í†µí•´ ë°±ì—… ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = None
            if hasattr(self.model_loader, 'load_model_async'):
                checkpoint = await self.model_loader.load_model_async(self.config.backup_model)
            elif hasattr(self.model_loader, 'get_model'):
                checkpoint = self.model_loader.get_model(self.config.backup_model)
            elif hasattr(self.model_loader, 'load_model'):
                checkpoint = self.model_loader.load_model(self.config.backup_model)
            
            if checkpoint is None:
                self.logger.info("â„¹ï¸ ë°±ì—… ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ - ê±´ë„ˆë›°ê¸°")
                return False
            
            # ì²´í¬í¬ì¸íŠ¸ â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ ë³€í™˜
            ai_model = await self._checkpoint_to_ai_model(
                checkpoint,
                model_class=HumanParsingU2Net,
                model_name='u2net'
            )
            
            if ai_model:
                self._ai_models['backup'] = ai_model
                self._model_checkpoints['backup'] = checkpoint
                self.logger.info("âœ… U2Net ë°±ì—… AI ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                return True
            else:
                self.logger.warning("âš ï¸ ë°±ì—… ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë°±ì—… AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def _checkpoint_to_ai_model(
        self, 
        checkpoint_data: Any, 
        model_class: type, 
        model_name: str
    ) -> Optional[Any]:
        """ğŸ”¥ í•µì‹¬! ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë¡œ ë³€í™˜"""
        try:
            if not TORCH_AVAILABLE:
                self.logger.warning("âš ï¸ PyTorch ì—†ìŒ - AI ëª¨ë¸ ìƒì„± ë¶ˆê°€")
                return None
            
            self.logger.info(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ â†’ AI ëª¨ë¸ ë³€í™˜ ì‹œì‘: {model_name}")
            
            # Step 1: AI ëª¨ë¸ í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            ai_model = model_class(
                num_classes=self.config.num_classes,
                device=self.device
            )
            
            # Step 2: ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            ai_model = ai_model.to(self.device)
            
            # Step 3: ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            if hasattr(ai_model, 'load_checkpoint'):
                load_success = ai_model.load_checkpoint(checkpoint_data)
                if not load_success:
                    self.logger.error(f"âŒ {model_name} ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨")
                    return None
            else:
                self.logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ì— load_checkpoint ë©”ì„œë“œ ì—†ìŒ")
            
            # Step 4: í‰ê°€ ëª¨ë“œ ì„¤ì •
            ai_model.eval()
            
            # Step 5: ì •ë°€ë„ ìµœì í™” (M3 Max)
            if self.config.use_fp16 and self.device != 'cpu':
                try:
                    if hasattr(ai_model, 'half'):
                        ai_model = ai_model.half()
                        self.logger.debug(f"{model_name} FP16 ë³€í™˜ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_name} FP16 ë³€í™˜ ì‹¤íŒ¨: {e}")
            
            # Step 6: M3 Max ìµœì í™”
            if self.device == 'mps':
                try:
                    ai_model = ai_model.float()  # MPSì—ì„œëŠ” float32ê°€ ë” ì•ˆì •ì 
                    self.logger.debug(f"{model_name} M3 Max ìµœì í™” ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_name} M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
            
            self.logger.info(f"âœ… {model_name} AI ëª¨ë¸ ë³€í™˜ ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
            
            return ai_model
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ â†’ AI ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨ ({model_name}): {e}")
            return None
    
    async def _warmup_models(self):
        """AI ëª¨ë¸ë“¤ ì›Œë°ì—…"""
        try:
            self.logger.info("ğŸ”¥ AI ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")
            
            if not TORCH_AVAILABLE:
                return
            
            # ë”ë¯¸ ì…ë ¥ ìƒì„±
            dummy_input = torch.randn(
                1, 3, *self.config.input_size, 
                device=self.device, 
                dtype=torch.float16 if self.config.use_fp16 and self.device != 'cpu' else torch.float32
            )
            
            # ê° AI ëª¨ë¸ ì›Œë°ì—…
            for model_name, ai_model in self._ai_models.items():
                try:
                    self.logger.info(f"ğŸ”¥ {model_name} AI ëª¨ë¸ ì›Œë°ì—…...")
                    
                    with torch.no_grad():
                        _ = ai_model(dummy_input)
                    
                    self.logger.info(f"âœ… {model_name} ì›Œë°ì—… ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_name} ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            
            self.logger.info("âœ… AI ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    def _apply_m3_max_optimizations(self):
        """M3 Max íŠ¹í™” ìµœì í™”"""
        try:
            if not MPS_AVAILABLE:
                return
            
            optimizations = []
            
            # 1. MPS ë°±ì—”ë“œ ì„¤ì •
            try:
                if hasattr(torch.backends.mps, 'set_per_process_memory_fraction'):
                    torch.backends.mps.set_per_process_memory_fraction(0.8)
                optimizations.append("MPS memory fraction")
            except Exception:
                pass
            
            # 2. ë©”ëª¨ë¦¬ í’€ë§
            try:
                torch.backends.mps.allow_tf32 = True
                optimizations.append("TF32 acceleration")
            except Exception:
                pass
            
            # 3. AI ëª¨ë¸ë³„ ìµœì í™”
            for model_name, ai_model in self._ai_models.items():
                try:
                    # ëª¨ë¸ì„ float32ë¡œ ìœ ì§€ (M3 Maxì—ì„œ ì•ˆì •ì )
                    if hasattr(ai_model, 'float'):
                        ai_model.float()
                    optimizations.append(f"{model_name} float32")
                except Exception:
                    pass
            
            if optimizations:
                self.logger.info(f"ğŸ M3 Max ìµœì í™” ì ìš©: {', '.join(optimizations)}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ 8. ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ (ì‹¤ì œ AI ì¶”ë¡ )
    # ==============================================
    
    async def process(
        self,
        person_image_tensor: torch.Tensor,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ - ì‹¤ì œ AI ì¶”ë¡ ì„ í†µí•œ ì¸ì²´ íŒŒì‹±
        
        Args:
            person_image_tensor: ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ [B, C, H, W]
            **kwargs: ì¶”ê°€ ì˜µì…˜
            
        Returns:
            Dict[str, Any]: ì¸ì²´ íŒŒì‹± ê²°ê³¼ + ì‹œê°í™”
        """
        
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        
        try:
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(person_image_tensor)
            cached_result = self._get_cached_result(cache_key)
            if cached_result:
                self.processing_stats['total_processed'] += 1
                self.logger.info("ğŸ’¾ ìºì‹œëœ ê²°ê³¼ ë°˜í™˜")
                return cached_result
            
            # ì…ë ¥ ì „ì²˜ë¦¬
            preprocessed_input = await self._preprocess_input(person_image_tensor)
            
            # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ì„ í†µí•œ ì¶”ë¡ 
            parsing_result = await self._run_ai_inference(preprocessed_input)
            
            # í›„ì²˜ë¦¬ ë° ê²°ê³¼ ìƒì„±
            final_result = await self._postprocess_result(
                parsing_result,
                person_image_tensor.shape[2:],
                person_image_tensor,
                start_time
            )
            
            # ìºì‹œ ì €ì¥
            self._cache_result(cache_key, final_result)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_processing_stats(time.time() - start_time, True)
            
            self.logger.info(f"âœ… Step 01 ì™„ë£Œ - {final_result['processing_time']:.3f}ì´ˆ")
            
            return final_result
            
        except Exception as e:
            error_msg = f"Step 01 ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
            self.logger.error(f"âŒ {error_msg}")
            
            self._update_processing_stats(time.time() - start_time, False)
            
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: {error_msg}")
            
            # í´ë°± ê²°ê³¼ ìƒì„±
            return self._create_fallback_result(
                person_image_tensor.shape[2:],
                time.time() - start_time,
                str(e)
            )
    
    async def _preprocess_input(self, image_tensor: torch.Tensor) -> torch.Tensor:
        """ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            if not TORCH_AVAILABLE:
                return image_tensor
            
            # í¬ê¸° ì •ê·œí™”
            if image_tensor.shape[2:] != self.config.input_size:
                resized = F.interpolate(
                    image_tensor,
                    size=self.config.input_size,
                    mode='bilinear',
                    align_corners=False
                )
            else:
                resized = image_tensor
            
            # ê°’ ë²”ìœ„ ì •ê·œí™” (0-1)
            if resized.max() > 1.0:
                resized = resized.float() / 255.0
            
            # ImageNet ì •ê·œí™”
            mean = torch.tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)
            normalized = (resized - mean) / std
            
            # ë””ë°”ì´ìŠ¤ ì´ë™
            normalized = normalized.to(self.device)
            
            # ì •ë°€ë„ ë³€í™˜
            if self.config.use_fp16 and self.device != 'cpu':
                try:
                    normalized = normalized.half()
                except Exception:
                    pass
            
            return normalized
            
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"âŒ ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image_tensor.to(self.device)
    
    async def _run_ai_inference(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ì„ í†µí•œ ì¶”ë¡ """
        try:
            if not TORCH_AVAILABLE:
                raise RuntimeError("PyTorch ì‚¬ìš© ë¶ˆê°€")
            
            # ì£¼ ëª¨ë¸ (Graphonomy) ìš°ì„  ì‹œë„
            if 'primary' in self._ai_models:
                ai_model = self._ai_models['primary']
                try:
                    self.logger.debug("ğŸš€ Graphonomy AI ëª¨ë¸ ì¶”ë¡  ì‹œì‘")
                    
                    with torch.no_grad():
                        if self.config.use_fp16 and self.device != 'cpu':
                            with torch.autocast(device_type=self.device.replace(':', '_'), dtype=torch.float16):
                                model_output = ai_model(input_tensor)
                        else:
                            model_output = ai_model(input_tensor)
                    
                    # Graphonomy ì¶œë ¥ ì²˜ë¦¬
                    if isinstance(model_output, dict) and 'parsing' in model_output:
                        output_tensor = model_output['parsing']
                    else:
                        output_tensor = model_output
                    
                    self.processing_stats['ai_inference_calls'] += 1
                    self.logger.info(f"âœ… Graphonomy AI ì¶”ë¡  ì™„ë£Œ - ì¶œë ¥ í˜•íƒœ: {output_tensor.shape}")
                    
                    return output_tensor
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Graphonomy AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            # ë°±ì—… ëª¨ë¸ (U2Net) ì‹œë„
            if 'backup' in self._ai_models:
                ai_model = self._ai_models['backup']
                try:
                    self.logger.debug("ğŸ”„ U2Net AI ëª¨ë¸ ì¶”ë¡  ì‹œì‘")
                    
                    with torch.no_grad():
                        model_output = ai_model(input_tensor)
                    
                    # U2Net ì¶œë ¥ ì²˜ë¦¬
                    if isinstance(model_output, dict) and 'parsing' in model_output:
                        output_tensor = model_output['parsing']
                    else:
                        output_tensor = model_output
                    
                    self.processing_stats['ai_inference_calls'] += 1
                    self.logger.info(f"âœ… U2Net AI ì¶”ë¡  ì™„ë£Œ - ì¶œë ¥ í˜•íƒœ: {output_tensor.shape}")
                    
                    return output_tensor
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ U2Net AI ëª¨ë¸ ì¶”ë¡ ë„ ì‹¤íŒ¨: {e}")
            
            # ëª¨ë“  AI ëª¨ë¸ ì‹¤íŒ¨
            error_msg = "ëª¨ë“  AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨"
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: {error_msg}")
            
            self.logger.error(f"âŒ {error_msg}")
            
            # ë”ë¯¸ ì¶œë ¥ ìƒì„± (í´ë°±)
            dummy_output = torch.zeros(
                input_tensor.shape[0], 
                self.config.num_classes, 
                *input_tensor.shape[2:],
                device=self.device
            )
            
            return dummy_output
            
        except Exception as e:
            error_msg = f"AI ì¶”ë¡  ì™„ì „ ì‹¤íŒ¨: {e}"
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: {error_msg}")
            self.logger.error(f"âŒ {error_msg}")
            raise
    
    async def _postprocess_result(
        self,
        model_output: torch.Tensor,
        original_size: Tuple[int, int],
        original_image_tensor: torch.Tensor,
        start_time: float
    ) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬ ë° ì‹œê°í™”"""
        try:
            if not TORCH_AVAILABLE:
                raise RuntimeError("PyTorch ì‚¬ìš© ë¶ˆê°€")
            
            # í™•ë¥ ì„ í´ë˜ìŠ¤ë¡œ ë³€í™˜
            if model_output.dim() == 4:
                parsing_map = torch.argmax(model_output, dim=1).squeeze(0)
            else:
                parsing_map = model_output.squeeze(0)
            
            # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
            parsing_map = parsing_map.cpu().numpy().astype(np.uint8)
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
            if parsing_map.shape != original_size:
                if OPENCV_AVAILABLE:
                    parsing_map = cv2.resize(
                        parsing_map,
                        (original_size[1], original_size[0]),
                        interpolation=cv2.INTER_NEAREST
                    )
                elif PIL_AVAILABLE:
                    pil_img = Image.fromarray(parsing_map)
                    resized = pil_img.resize((original_size[1], original_size[0]), Image.Resampling.NEAREST)
                    parsing_map = np.array(resized)
            
            # í›„ì²˜ë¦¬ ì ìš©
            if self.config.apply_postprocessing:
                parsing_map = self._apply_postprocessing(parsing_map)
            
            # ë¶€ìœ„ë³„ ë¶„ì„
            body_masks = self._create_body_masks(parsing_map)
            clothing_regions = self._analyze_clothing_regions(parsing_map)
            detected_parts = self._get_detected_parts(parsing_map)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(model_output)
            
            # ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±
            visualization_results = await self._create_visualization(
                parsing_map,
                original_image_tensor
            )
            
            processing_time = time.time() - start_time
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                "success": True,
                "message": "ì¸ì²´ íŒŒì‹± ì™„ë£Œ",
                "confidence": float(confidence),
                "processing_time": processing_time,
                "details": {
                    # í”„ë¡ íŠ¸ì—”ë“œìš© ì‹œê°í™” ì´ë¯¸ì§€ë“¤
                    "result_image": visualization_results.get("colored_parsing", ""),
                    "overlay_image": visualization_results.get("overlay_image", ""),
                    
                    # ê¸°ë³¸ ì •ë³´
                    "detected_parts": len(detected_parts),
                    "total_parts": 20,
                    "body_parts": list(detected_parts.keys()),
                    
                    # ì˜ë¥˜ ì •ë³´
                    "clothing_info": {
                        "categories_detected": clothing_regions.get("categories_detected", []),
                        "dominant_category": clothing_regions.get("dominant_category"),
                        "total_clothing_area": clothing_regions.get("total_clothing_area", 0.0)
                    },
                    
                    # ìƒì„¸ ë¶„ì„
                    "parsing_map": parsing_map.tolist(),
                    "body_masks_info": {name: {"pixel_count": int(mask.sum())} 
                                       for name, mask in body_masks.items()},
                    "part_details": detected_parts,
                    
                    # ì‹œìŠ¤í…œ ì •ë³´
                    "step_info": {
                        "step_name": "human_parsing",
                        "step_number": 1,
                        "ai_models_loaded": list(self._ai_models.keys()),
                        "device": self.device,
                        "input_size": self.config.input_size,
                        "num_classes": self.config.num_classes,
                        "model_loader_calls": self.processing_stats.get('model_loader_calls', 0),
                        "ai_inference_calls": self.processing_stats.get('ai_inference_calls', 0),
                        "strict_mode": self.config.strict_mode
                    },
                    
                    # í’ˆì§ˆ ë©”íŠ¸ë¦­
                    "quality_metrics": {
                        "segmentation_coverage": float(np.sum(parsing_map > 0) / parsing_map.size),
                        "part_count": len(detected_parts),
                        "confidence": float(confidence),
                        "ai_model_success": True
                    }
                },
                
                # ë ˆê±°ì‹œ í˜¸í™˜ì„± í•„ë“œë“¤
                "parsing_map": parsing_map,
                "body_masks": body_masks,
                "clothing_regions": clothing_regions,
                "body_parts_detected": detected_parts,
                "from_cache": False
            }
            
            return result
            
        except Exception as e:
            error_msg = f"ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: {error_msg}")
            self.logger.error(f"âŒ {error_msg}")
            return self._create_fallback_result(
                original_size,
                time.time() - start_time,
                str(e)
            )
    
    # ==============================================
    # ğŸ”¥ 9. í›„ì²˜ë¦¬ ë° ë¶„ì„ ë©”ì„œë“œë“¤
    # ==============================================
    
    def _apply_postprocessing(self, parsing_map: np.ndarray) -> np.ndarray:
        """í›„ì²˜ë¦¬ ì ìš©"""
        try:
            if not self.config.apply_postprocessing or not OPENCV_AVAILABLE:
                return parsing_map
            
            # ë…¸ì´ì¦ˆ ì œê±°
            if self.config.noise_reduction:
                kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                parsing_map = cv2.morphologyEx(parsing_map, cv2.MORPH_CLOSE, kernel_close)
                
                kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
                parsing_map = cv2.morphologyEx(parsing_map, cv2.MORPH_OPEN, kernel_open)
            
            # ì—£ì§€ ì •êµí™”
            if self.config.edge_refinement:
                try:
                    blurred = cv2.GaussianBlur(parsing_map.astype(np.float32), (3, 3), 0.5)
                    parsing_map = np.round(blurred).astype(np.uint8)
                except Exception:
                    pass
            
            return parsing_map
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return parsing_map
    
    def _create_body_masks(self, parsing_map: np.ndarray) -> Dict[str, np.ndarray]:
        """ì‹ ì²´ ë¶€ìœ„ë³„ ë§ˆìŠ¤í¬ ìƒì„±"""
        body_masks = {}
        
        try:
            for part_id, part_name in BODY_PARTS.items():
                if part_id == 0:  # ë°°ê²½ ì œì™¸
                    continue
                
                mask = (parsing_map == part_id).astype(np.uint8)
                if mask.sum() > 0:  # í•´ë‹¹ ë¶€ìœ„ê°€ ê°ì§€ëœ ê²½ìš°ë§Œ
                    body_masks[part_name] = mask
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹ ì²´ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return body_masks
    
    def _analyze_clothing_regions(self, parsing_map: np.ndarray) -> Dict[str, Any]:
        """ì˜ë¥˜ ì˜ì—­ ë¶„ì„"""
        analysis = {
            "categories_detected": [],
            "coverage_ratio": {},
            "dominant_category": None,
            "total_clothing_area": 0.0
        }
        
        try:
            total_pixels = parsing_map.size
            max_coverage = 0.0
            total_clothing_pixels = 0
            
            for category, part_ids in CLOTHING_CATEGORIES.items():
                if category == 'skin':  # í”¼ë¶€ëŠ” ì˜ë¥˜ê°€ ì•„ë‹˜
                    continue
                
                try:
                    category_mask = np.zeros_like(parsing_map, dtype=bool)
                    
                    for part_id in part_ids:
                        category_mask |= (parsing_map == part_id)
                    
                    if category_mask.sum() > 0:
                        coverage = category_mask.sum() / total_pixels
                        
                        analysis["categories_detected"].append(category)
                        analysis["coverage_ratio"][category] = coverage
                        
                        total_clothing_pixels += category_mask.sum()
                        
                        if coverage > max_coverage:
                            max_coverage = coverage
                            analysis["dominant_category"] = category
                            
                except Exception as e:
                    self.logger.debug(f"ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì‹¤íŒ¨ ({category}): {e}")
            
            analysis["total_clothing_area"] = total_clothing_pixels / total_pixels
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ ì˜ì—­ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return analysis
    
    def _get_detected_parts(self, parsing_map: np.ndarray) -> Dict[str, Any]:
        """ê°ì§€ëœ ë¶€ìœ„ ì •ë³´ ìˆ˜ì§‘"""
        detected_parts = {}
        
        try:
            for part_id, part_name in BODY_PARTS.items():
                if part_id == 0:  # ë°°ê²½ ì œì™¸
                    continue
                
                try:
                    mask = (parsing_map == part_id)
                    pixel_count = mask.sum()
                    
                    if pixel_count > 0:
                        detected_parts[part_name] = {
                            "pixel_count": int(pixel_count),
                            "percentage": float(pixel_count / parsing_map.size * 100),
                            "part_id": part_id,
                            "bounding_box": self._get_bounding_box(mask),
                            "centroid": self._get_centroid(mask)
                        }
                except Exception as e:
                    self.logger.debug(f"ë¶€ìœ„ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ ({part_name}): {e}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì „ì²´ ë¶€ìœ„ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return detected_parts
    
    def _get_bounding_box(self, mask: np.ndarray) -> Dict[str, int]:
        """ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
        try:
            coords = np.where(mask)
            if len(coords[0]) == 0:
                return {"x": 0, "y": 0, "width": 0, "height": 0}
            
            y_min, y_max = int(coords[0].min()), int(coords[0].max())
            x_min, x_max = int(coords[1].min()), int(coords[1].max())
            
            return {
                "x": x_min,
                "y": y_min,
                "width": x_max - x_min + 1,
                "height": y_max - y_min + 1
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {"x": 0, "y": 0, "width": 0, "height": 0}
    
    def _get_centroid(self, mask: np.ndarray) -> Dict[str, float]:
        """ì¤‘ì‹¬ì  ê³„ì‚°"""
        try:
            coords = np.where(mask)
            if len(coords[0]) == 0:
                return {"x": 0.0, "y": 0.0}
            
            y_center = float(np.mean(coords[0]))
            x_center = float(np.mean(coords[1]))
            
            return {"x": x_center, "y": y_center}
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¤‘ì‹¬ì  ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {"x": 0.0, "y": 0.0}
    
    def _calculate_confidence(self, model_output: torch.Tensor) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            if not TORCH_AVAILABLE:
                return 0.8
            
            if model_output.dim() == 4 and model_output.shape[1] > 1:
                # ì†Œí”„íŠ¸ë§¥ìŠ¤ í™•ë¥ ì—ì„œ ìµœëŒ€ê°’ë“¤ì˜ í‰ê· 
                probs = F.softmax(model_output, dim=1)
                max_probs = torch.max(probs, dim=1)[0]
                confidence = torch.mean(max_probs).item()
            else:
                # ë°”ì´ë„ˆë¦¬ ì¶œë ¥ì˜ ê²½ìš°
                confidence = float(torch.mean(torch.abs(model_output)).item())
            
            return max(0.0, min(1.0, confidence))  # 0-1 ë²”ìœ„ë¡œ í´ë¨í•‘
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.8
    
    # ==============================================
    # ğŸ”¥ 10. ì‹œê°í™” ìƒì„± ë©”ì„œë“œë“¤
    # ==============================================
    
    async def _create_visualization(
        self,
        parsing_map: np.ndarray,
        original_image_tensor: torch.Tensor
    ) -> Dict[str, str]:
        """ì‹œê°í™” ì´ë¯¸ì§€ë“¤ ìƒì„±"""
        try:
            if not self.config.enable_visualization:
                return {"colored_parsing": "", "overlay_image": "", "legend_image": ""}
            
            def _create_visualizations():
                try:
                    # ì›ë³¸ ì´ë¯¸ì§€ë¥¼ PILë¡œ ë³€í™˜
                    original_pil = self._tensor_to_pil(original_image_tensor)
                    
                    # 1. ìƒ‰ê¹”ë¡œ êµ¬ë¶„ëœ íŒŒì‹± ê²°ê³¼ ìƒì„±
                    colored_parsing = self._create_colored_parsing_map(parsing_map)
                    
                    # 2. ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„±
                    overlay_image = self._create_overlay_image(original_pil, colored_parsing)
                    
                    # 3. ë²”ë¡€ ì´ë¯¸ì§€ ìƒì„± (ì˜µì…˜)
                    legend_image = ""
                    if self.config.show_part_labels:
                        try:
                            legend_img = self._create_legend_image(parsing_map)
                            legend_image = self._pil_to_base64(legend_img)
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ë²”ë¡€ ìƒì„± ì‹¤íŒ¨: {e}")
                    
                    return {
                        "colored_parsing": self._pil_to_base64(colored_parsing),
                        "overlay_image": self._pil_to_base64(overlay_image),
                        "legend_image": legend_image
                    }
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
                    return {"colored_parsing": "", "overlay_image": "", "legend_image": ""}
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            try:
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(self.executor, _create_visualizations)
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ì‹œê°í™” ì‹¤íŒ¨: {e}")
                return _create_visualizations()
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹œê°í™” ìƒì„± ì™„ì „ ì‹¤íŒ¨: {e}")
            return {"colored_parsing": "", "overlay_image": "", "legend_image": ""}
    
    def _tensor_to_pil(self, tensor: torch.Tensor) -> Image.Image:
        """í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            if not PIL_AVAILABLE:
                # PIL ì—†ì„ ë•Œ ê¸°ë³¸ ì´ë¯¸ì§€
                return Image.new('RGB', (512, 512), (128, 128, 128))
            
            # [B, C, H, W] -> [C, H, W]
            if tensor.dim() == 4:
                tensor = tensor.squeeze(0)
            
            # CPUë¡œ ì´ë™
            tensor = tensor.cpu()
            
            # ì •ê·œí™” í•´ì œ (0-1 ë²”ìœ„ë¡œ)
            if tensor.max() <= 1.0:
                tensor = tensor.clamp(0, 1)
            else:
                tensor = tensor / 255.0
            
            # [C, H, W] -> [H, W, C]
            tensor = tensor.permute(1, 2, 0)
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            numpy_array = (tensor.numpy() * 255).astype(np.uint8)
            
            return Image.fromarray(numpy_array)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í…ì„œ->PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€
            if PIL_AVAILABLE:
                return Image.new('RGB', (512, 512), (128, 128, 128))
            else:
                return None
    
    def _create_colored_parsing_map(self, parsing_map: np.ndarray) -> Image.Image:
        """ì»¬ëŸ¬ íŒŒì‹± ë§µ ìƒì„±"""
        try:
            if not PIL_AVAILABLE:
                return None
            
            height, width = parsing_map.shape
            colored_image = np.zeros((height, width, 3), dtype=np.uint8)
            
            # ê° ë¶€ìœ„ë³„ë¡œ ìƒ‰ìƒ ì ìš©
            for part_id, color in VISUALIZATION_COLORS.items():
                try:
                    mask = (parsing_map == part_id)
                    colored_image[mask] = color
                except Exception as e:
                    self.logger.debug(f"ìƒ‰ìƒ ì ìš© ì‹¤íŒ¨ (ë¶€ìœ„ {part_id}): {e}")
            
            return Image.fromarray(colored_image)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì»¬ëŸ¬ íŒŒì‹± ë§µ ìƒì„± ì‹¤íŒ¨: {e}")
            if PIL_AVAILABLE:
                return Image.new('RGB', (512, 512), (128, 128, 128))
            return None
    
    def _create_overlay_image(self, original_pil: Image.Image, colored_parsing: Image.Image) -> Image.Image:
        """ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            if not PIL_AVAILABLE or original_pil is None or colored_parsing is None:
                return original_pil or colored_parsing
            
            # í¬ê¸° ë§ì¶”ê¸°
            width, height = original_pil.size
            colored_parsing = colored_parsing.resize((width, height), Image.Resampling.NEAREST)
            
            # ì•ŒíŒŒ ë¸”ë Œë”©
            opacity = self.config.overlay_opacity
            overlay = Image.blend(original_pil, colored_parsing, opacity)
            
            return overlay
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨: {e}")
            return original_pil
    
    def _create_legend_image(self, parsing_map: np.ndarray) -> Image.Image:
        """ë²”ë¡€ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            if not PIL_AVAILABLE:
                return None
            
            # ì‹¤ì œ ê°ì§€ëœ ë¶€ìœ„ë“¤ë§Œ í¬í•¨
            detected_parts = np.unique(parsing_map)
            detected_parts = detected_parts[detected_parts > 0]  # ë°°ê²½ ì œì™¸
            
            # ë²”ë¡€ ì´ë¯¸ì§€ í¬ê¸° ê³„ì‚°
            legend_width = 200
            item_height = 25
            legend_height = max(100, len(detected_parts) * item_height + 40)
            
            # ë²”ë¡€ ì´ë¯¸ì§€ ìƒì„±
            legend_img = Image.new('RGB', (legend_width, legend_height), (255, 255, 255))
            draw = ImageDraw.Draw(legend_img)
            
            # í°íŠ¸ ë¡œë”©
            try:
                font = ImageFont.truetype("arial.ttf", 14)
                title_font = ImageFont.truetype("arial.ttf", 16)
            except Exception:
                font = ImageFont.load_default()
                title_font = ImageFont.load_default()
            
            # ì œëª©
            draw.text((10, 10), "Detected Parts", fill=(0, 0, 0), font=title_font)
            
            # ê° ë¶€ìœ„ë³„ ë²”ë¡€ í•­ëª©
            y_offset = 35
            for part_id in detected_parts:
                try:
                    if part_id in BODY_PARTS and part_id in VISUALIZATION_COLORS:
                        part_name = BODY_PARTS[part_id]
                        color = VISUALIZATION_COLORS[part_id]
                        
                        # ìƒ‰ìƒ ë°•ìŠ¤
                        draw.rectangle([10, y_offset, 30, y_offset + 15], 
                                     fill=color, outline=(0, 0, 0))
                        
                        # í…ìŠ¤íŠ¸
                        draw.text((35, y_offset), part_name, fill=(0, 0, 0), font=font)
                        
                        y_offset += item_height
                except Exception as e:
                    self.logger.debug(f"ë²”ë¡€ í•­ëª© ìƒì„± ì‹¤íŒ¨ (ë¶€ìœ„ {part_id}): {e}")
            
            return legend_img
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë²”ë¡€ ìƒì„± ì‹¤íŒ¨: {e}")
            if PIL_AVAILABLE:
                return Image.new('RGB', (200, 100), (240, 240, 240))
            return None
    
    def _pil_to_base64(self, pil_image: Image.Image) -> str:
        """PIL ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜"""
        try:
            if pil_image is None:
                return ""
            
            buffer = BytesIO()
            
            # í’ˆì§ˆ ì„¤ì •
            quality = 85
            if self.config.visualization_quality == "high":
                quality = 95
            elif self.config.visualization_quality == "low":
                quality = 70
            
            pil_image.save(buffer, format='JPEG', quality=quality)
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""
    
    # ==============================================
    # ğŸ”¥ 11. ìºì‹œ ë° ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def _generate_cache_key(self, tensor: torch.Tensor) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            tensor_bytes = tensor.cpu().numpy().tobytes()
            hash_value = hashlib.md5(tensor_bytes).hexdigest()[:16]
            return f"step01_{hash_value}_{self.config.input_size[0]}x{self.config.input_size[1]}"
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ í‚¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"step01_fallback_{int(time.time())}"
    
    def _get_cached_result(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ"""
        try:
            with self.cache_lock:
                if cache_key in self.result_cache:
                    cached = self.result_cache[cache_key].copy()
                    cached["from_cache"] = True
                    return cached
                return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _cache_result(self, cache_key: str, result: Dict[str, Any]):
        """ê²°ê³¼ ìºì‹±"""
        try:
            with self.cache_lock:
                # ìºì‹œ í¬ê¸° ì œí•œ
                if len(self.result_cache) >= self.config.max_cache_size:
                    # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                    try:
                        oldest_key = next(iter(self.result_cache))
                        del self.result_cache[oldest_key]
                    except Exception:
                        self.result_cache.clear()
                
                # ìƒˆ ê²°ê³¼ ì €ì¥
                cached_result = result.copy()
                cached_result["from_cache"] = False
                self.result_cache[cache_key] = cached_result
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _update_processing_stats(self, processing_time: float, success: bool):
        """ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.processing_stats['total_processed'] += 1
            
            if success:
                self.processing_stats['success_count'] += 1
            else:
                self.processing_stats['error_count'] += 1
            
            # ì´ë™ í‰ê·  ê³„ì‚°
            current_avg = self.processing_stats['average_time']
            count = self.processing_stats['total_processed']
            new_avg = (current_avg * (count - 1) + processing_time) / count
            self.processing_stats['average_time'] = new_avg
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _create_fallback_result(
        self,
        original_size: Tuple[int, int],
        processing_time: float,
        error_msg: str
    ) -> Dict[str, Any]:
        """í´ë°± ê²°ê³¼ ìƒì„± (ì—ëŸ¬ ë°œìƒ ì‹œ)"""
        try:
            return {
                "success": False,
                "message": f"ì¸ì²´ íŒŒì‹± ì‹¤íŒ¨: {error_msg}",
                "confidence": 0.0,
                "processing_time": processing_time,
                "details": {
                    "result_image": "",
                    "overlay_image": "",
                    "detected_parts": 0,
                    "total_parts": 20,
                    "body_parts": [],
                    "clothing_info": {
                        "categories_detected": [],
                        "dominant_category": None,
                        "total_clothing_area": 0.0
                    },
                    "error": error_msg,
                    "step_info": {
                        "step_name": "human_parsing",
                        "step_number": 1,
                        "ai_models_loaded": list(self._ai_models.keys()),
                        "device": self.device,
                        "error": error_msg,
                        "model_loader_calls": self.processing_stats.get('model_loader_calls', 0),
                        "ai_inference_calls": self.processing_stats.get('ai_inference_calls', 0),
                        "strict_mode": self.config.strict_mode
                    },
                    "quality_metrics": {
                        "segmentation_coverage": 0.0,
                        "part_count": 0,
                        "confidence": 0.0,
                        "ai_model_success": False
                    }
                },
                "parsing_map": np.zeros(original_size, dtype=np.uint8),
                "body_masks": {},
                "clothing_regions": {
                    "categories_detected": [],
                    "coverage_ratio": {},
                    "dominant_category": None,
                    "total_clothing_area": 0.0
                },
                "body_parts_detected": {},
                "from_cache": False
            }
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ê²°ê³¼ ìƒì„±ë„ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "message": "ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ",
                "confidence": 0.0,
                "processing_time": processing_time,
                "details": {"error": f"Fallback failed: {e}"},
                "parsing_map": np.zeros((512, 512), dtype=np.uint8),
                "body_masks": {},
                "clothing_regions": {},
                "body_parts_detected": {},
                "from_cache": False
            }
    
    # ==============================================
    # ğŸ”¥ 12. ë¹ ì§„ í•µì‹¬ ê¸°ëŠ¥ë“¤ ì¶”ê°€ (ì›ë³¸ í˜¸í™˜ì„±)
    # ==============================================
    
    def load_parsing_result(self, input_path: Union[str, Path]) -> Optional[Dict[str, Any]]:
        """ì €ì¥ëœ íŒŒì‹± ê²°ê³¼ ë¡œë“œ"""
        try:
            input_path = Path(input_path)
            
            if input_path.suffix.lower() == '.json':
                with open(input_path, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                
                # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³µì›
                if 'parsing_map' in result and isinstance(result['parsing_map'], list):
                    result['parsing_map'] = np.array(result['parsing_map'], dtype=np.uint8)
                
                self.logger.info(f"ğŸ“‚ íŒŒì‹± ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {input_path}")
                return result
            
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: íŒŒì‹± ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.logger.error(f"âŒ íŒŒì‹± ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def save_parsing_result(
        self, 
        result: Dict[str, Any], 
        output_path: Union[str, Path],
        save_format: str = "json"
    ) -> bool:
        """íŒŒì‹± ê²°ê³¼ ì €ì¥"""
        try:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            if save_format.lower() == "json":
                # JSONìœ¼ë¡œ ì €ì¥ (ì´ë¯¸ì§€ëŠ” base64)
                save_data = result.copy()
                
                # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                if 'parsing_map' in save_data and isinstance(save_data['parsing_map'], np.ndarray):
                    save_data['parsing_map'] = save_data['parsing_map'].tolist()
                
                with open(output_path.with_suffix('.json'), 'w', encoding='utf-8') as f:
                    json.dump(save_data, f, indent=2, ensure_ascii=False)
            
            elif save_format.lower() == "images":
                # ì´ë¯¸ì§€ë“¤ì„ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥
                if 'details' in result:
                    details = result['details']
                    
                    # ì»¬ëŸ¬ íŒŒì‹± ì´ë¯¸ì§€
                    if 'result_image' in details and details['result_image']:
                        try:
                            img_data = base64.b64decode(details['result_image'])
                            with open(output_path.with_name(f"{output_path.stem}_colored.jpg"), 'wb') as f:
                                f.write(img_data)
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ì»¬ëŸ¬ íŒŒì‹± ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
                    
                    # ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€
                    if 'overlay_image' in details and details['overlay_image']:
                        try:
                            img_data = base64.b64decode(details['overlay_image'])
                            with open(output_path.with_name(f"{output_path.stem}_overlay.jpg"), 'wb') as f:
                                f.write(img_data)
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
            
            self.logger.info(f"ğŸ’¾ íŒŒì‹± ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
            return True
            
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: íŒŒì‹± ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            self.logger.error(f"âŒ íŒŒì‹± ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def export_body_masks(
        self, 
        result: Dict[str, Any], 
        output_dir: Union[str, Path]
    ) -> bool:
        """ì‹ ì²´ ë§ˆìŠ¤í¬ë“¤ì„ ê°œë³„ ì´ë¯¸ì§€ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            
            if 'body_masks' not in result:
                error_msg = "ê²°ê³¼ì— body_masksê°€ ì—†ìŠµë‹ˆë‹¤"
                if self.config.strict_mode:
                    raise RuntimeError(f"âŒ strict_mode: {error_msg}")
                self.logger.warning(f"âš ï¸ {error_msg}")
                return False
            
            body_masks = result['body_masks']
            
            for part_name, mask in body_masks.items():
                try:
                    # ë§ˆìŠ¤í¬ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (0-255)
                    mask_image = (mask * 255).astype(np.uint8)
                    
                    if PIL_AVAILABLE:
                        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
                        pil_image = Image.fromarray(mask_image, mode='L')
                        
                        # ì €ì¥
                        output_path = output_dir / f"mask_{part_name}.png"
                        pil_image.save(output_path)
                    elif OPENCV_AVAILABLE:
                        # OpenCVë¡œ ì €ì¥
                        output_path = output_dir / f"mask_{part_name}.png"
                        cv2.imwrite(str(output_path), mask_image)
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {part_name} ë§ˆìŠ¤í¬ ì €ì¥ ì‹¤íŒ¨: {e}")
            
            self.logger.info(f"ğŸ’¾ ì‹ ì²´ ë§ˆìŠ¤í¬ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_dir}")
            return True
            
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: ì‹ ì²´ ë§ˆìŠ¤í¬ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            self.logger.error(f"âŒ ì‹ ì²´ ë§ˆìŠ¤í¬ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False

    def create_parsing_animation(
        self, 
        results: List[Dict[str, Any]], 
        output_path: Union[str, Path],
        fps: int = 10
    ) -> bool:
        """íŒŒì‹± ê²°ê³¼ë“¤ë¡œ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±"""
        try:
            if not results:
                error_msg = "ë¹ˆ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤"
                if self.config.strict_mode:
                    raise RuntimeError(f"âŒ strict_mode: {error_msg}")
                self.logger.warning(f"âš ï¸ {error_msg}")
                return False
            
            if not PIL_AVAILABLE:
                self.logger.warning("âš ï¸ PIL ì—†ìŒ - ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ë¶ˆê°€")
                return False
            
            frames = []
            
            for result in results:
                try:
                    if 'details' in result and 'result_image' in result['details']:
                        img_data = base64.b64decode(result['details']['result_image'])
                        img = Image.open(BytesIO(img_data))
                        frames.append(img)
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í”„ë ˆì„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            if frames:
                # GIFë¡œ ì €ì¥
                output_path = Path(output_path).with_suffix('.gif')
                frames[0].save(
                    output_path,
                    save_all=True,
                    append_images=frames[1:],
                    duration=int(1000/fps),
                    loop=0
                )
                
                self.logger.info(f"ğŸ¬ íŒŒì‹± ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì™„ë£Œ: {output_path}")
                return True
            
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: íŒŒì‹± ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            self.logger.error(f"âŒ íŒŒì‹± ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    def get_clothing_mask(self, parsing_map: np.ndarray, category: str) -> np.ndarray:
        """íŠ¹ì • ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ì˜ í†µí•© ë§ˆìŠ¤í¬ ë°˜í™˜"""
        try:
            if category not in CLOTHING_CATEGORIES:
                error_msg = f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬: {category}"
                if self.config.strict_mode:
                    raise ValueError(f"âŒ strict_mode: {error_msg}")
                raise ValueError(error_msg)
            
            combined_mask = np.zeros_like(parsing_map, dtype=np.uint8)
            
            for part_id in CLOTHING_CATEGORIES[category]:
                combined_mask |= (parsing_map == part_id).astype(np.uint8)
            
            return combined_mask
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: ì˜ë¥˜ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros_like(parsing_map, dtype=np.uint8)

    def visualize_parsing(self, parsing_map: np.ndarray) -> np.ndarray:
        """íŒŒì‹± ê²°ê³¼ ì‹œê°í™” (ë””ë²„ê¹…ìš©)"""
        try:
            # 20ê°œ ë¶€ìœ„ë³„ ìƒ‰ìƒ ë§¤í•‘
            colors = np.array([
                [0, 0, 0],       # 0: Background
                [128, 0, 0],     # 1: Hat
                [255, 0, 0],     # 2: Hair
                [0, 85, 0],      # 3: Glove
                [170, 0, 51],    # 4: Sunglasses
                [255, 85, 0],    # 5: Upper-clothes
                [0, 0, 85],      # 6: Dress
                [0, 119, 221],   # 7: Coat
                [85, 85, 0],     # 8: Socks
                [0, 85, 85],     # 9: Pants
                [85, 51, 0],     # 10: Torso-skin
                [52, 86, 128],   # 11: Scarf
                [0, 128, 0],     # 12: Skirt
                [0, 0, 255],     # 13: Face
                [51, 170, 221],  # 14: Left-arm
                [0, 255, 255],   # 15: Right-arm
                [85, 255, 170],  # 16: Left-leg
                [170, 255, 85],  # 17: Right-leg
                [255, 255, 0],   # 18: Left-shoe
                [255, 170, 0]    # 19: Right-shoe
            ])
            
            colored_parsing = colors[parsing_map]
            return colored_parsing.astype(np.uint8)
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: íŒŒì‹± ì‹œê°í™” ì‹¤íŒ¨: {e}")
            self.logger.warning(f"âš ï¸ íŒŒì‹± ì‹œê°í™” ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€
            return np.stack([parsing_map] * 3, axis=-1)

    def create_detailed_visualization(
        self,
        parsing_map: np.ndarray,
        original_image: np.ndarray,
        show_labels: bool = True,
        show_confidence: bool = True
    ) -> Image.Image:
        """ìƒì„¸ ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±"""
        try:
            if not PIL_AVAILABLE:
                self.logger.warning("âš ï¸ PIL ì—†ìŒ - ìƒì„¸ ì‹œê°í™” ë¶ˆê°€")
                return None
            
            # matplotlib ì‹œë„
            try:
                import matplotlib.pyplot as plt
                import matplotlib.patches as patches
                
                fig, axes = plt.subplots(1, 3, figsize=(12, 8))
                
                # 1. ì›ë³¸ ì´ë¯¸ì§€
                axes[0].imshow(original_image)
                axes[0].set_title('Original Image')
                axes[0].axis('off')
                
                # 2. íŒŒì‹± ê²°ê³¼
                colored_parsing = self.visualize_parsing(parsing_map)
                axes[1].imshow(colored_parsing)
                axes[1].set_title('Human Parsing')
                axes[1].axis('off')
                
                # 3. ì˜¤ë²„ë ˆì´
                if OPENCV_AVAILABLE:
                    overlay = cv2.addWeighted(original_image, 0.6, colored_parsing, 0.4, 0)
                else:
                    # ê°„ë‹¨í•œ ë¸”ë Œë”©
                    overlay = (original_image * 0.6 + colored_parsing * 0.4).astype(np.uint8)
                axes[2].imshow(overlay)
                axes[2].set_title('Overlay')
                axes[2].axis('off')
                
                # ë²”ë¡€ ì¶”ê°€
                if show_labels:
                    detected_parts = np.unique(parsing_map)
                    detected_parts = detected_parts[detected_parts > 0]
                    
                    legend_elements = []
                    for part_id in detected_parts[:10]:  # ìµœëŒ€ 10ê°œë§Œ
                        if part_id in BODY_PARTS and part_id in VISUALIZATION_COLORS:
                            color = np.array(VISUALIZATION_COLORS[part_id]) / 255.0
                            legend_elements.append(
                                patches.Patch(color=color, label=BODY_PARTS[part_id])
                            )
                    
                    if legend_elements:
                        fig.legend(handles=legend_elements, loc='lower center', ncol=5)
                
                plt.tight_layout()
                
                # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
                buffer = BytesIO()
                plt.savefig(buffer, format='png', dpi=100, bbox_inches='tight')
                buffer.seek(0)
                result_image = Image.open(buffer)
                plt.close(fig)
                
                return result_image
                
            except ImportError:
                # matplotlib ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì‹œê°í™”
                return self._create_basic_detailed_visualization(parsing_map, original_image)
                
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: ìƒì„¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            self.logger.warning(f"âš ï¸ ìƒì„¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            if PIL_AVAILABLE:
                return Image.new('RGB', (800, 600), (128, 128, 128))
            return None

    def _create_basic_detailed_visualization(
        self, 
        parsing_map: np.ndarray, 
        original_image: np.ndarray
    ) -> Image.Image:
        """ê¸°ë³¸ ìƒì„¸ ì‹œê°í™” (matplotlib ì—†ì´)"""
        try:
            if not PIL_AVAILABLE:
                return None
            
            # 3ê°œ ì´ë¯¸ì§€ë¥¼ ê°€ë¡œë¡œ ë°°ì¹˜
            height, width = parsing_map.shape
            
            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ë§ì¶”ê¸°
            if original_image.shape[:2] != (height, width):
                if OPENCV_AVAILABLE:
                    original_image = cv2.resize(original_image, (width, height))
                elif PIL_AVAILABLE:
                    pil_img = Image.fromarray(original_image)
                    resized = pil_img.resize((width, height))
                    original_image = np.array(resized)
            
            # ì»¬ëŸ¬ íŒŒì‹± ì´ë¯¸ì§€ ìƒì„±
            colored_parsing = self.visualize_parsing(parsing_map)
            
            # ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„±
            if OPENCV_AVAILABLE:
                overlay = cv2.addWeighted(original_image, 0.6, colored_parsing, 0.4, 0)
            else:
                overlay = (original_image * 0.6 + colored_parsing * 0.4).astype(np.uint8)
            
            # 3ê°œ ì´ë¯¸ì§€ë¥¼ ê°€ë¡œë¡œ í•©ì¹˜ê¸°
            combined = np.hstack([original_image, colored_parsing, overlay])
            
            return Image.fromarray(combined)
            
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: ê¸°ë³¸ ìƒì„¸ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            self.logger.warning(f"âš ï¸ ê¸°ë³¸ ìƒì„¸ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            if PIL_AVAILABLE:
                return Image.new('RGB', (800, 600), (128, 128, 128))
            return None

    def reset_statistics(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.processing_stats = {
            'total_processed': 0,
            'average_time': 0.0,
            'success_count': 0,
            'error_count': 0,
            'model_loader_calls': 0,
            'ai_inference_calls': 0
        }
        self.logger.info("ğŸ“Š í†µê³„ ì´ˆê¸°í™” ì™„ë£Œ")

    def get_cache_info(self) -> Dict[str, Any]:
        """ìºì‹œ ìƒíƒœ ì •ë³´"""
        try:
            with self.cache_lock:
                cache_info = {
                    "cache_size": len(self.result_cache),
                    "max_cache_size": self.config.max_cache_size,
                    "memory_usage_estimate": 0.0
                }
                
                # ìºì‹œ ì‚¬ìš©ë¥  ê³„ì‚°
                if self.processing_stats['total_processed'] > 0:
                    cache_info["cache_hit_rate"] = (
                        self.processing_stats.get('cache_hits', 0) / 
                        self.processing_stats['total_processed']
                    ) * 100
                else:
                    cache_info["cache_hit_rate"] = 0.0
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
                try:
                    import sys
                    total_size = sum(
                        sys.getsizeof(result) for result in self.result_cache.values()
                    )
                    cache_info["memory_usage_estimate"] = total_size / 1024 / 1024  # MB
                except Exception:
                    cache_info["memory_usage_estimate"] = 0.0
                
                return cache_info
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def clear_cache(self):
        """ìºì‹œ ìˆ˜ë™ ì •ë¦¬"""
        try:
            with self.cache_lock:
                cleared_count = len(self.result_cache)
                self.result_cache.clear()
                self.logger.info(f"ğŸ§¹ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {cleared_count}ê°œ í•­ëª©")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def set_quality_level(self, quality_level: str):
        """í’ˆì§ˆ ë ˆë²¨ ë™ì  ë³€ê²½"""
        try:
            old_quality = self.config.quality_level
            self.config.quality_level = quality_level
            
            # í’ˆì§ˆì— ë”°ë¥¸ ì„¤ì • ì¡°ì •
            if quality_level == "fast":
                self.config.apply_postprocessing = False
                self.config.noise_reduction = False
                self.config.edge_refinement = False
                self.config.input_size = (256, 256)
            elif quality_level == "balanced":
                self.config.apply_postprocessing = True
                self.config.noise_reduction = True
                self.config.edge_refinement = False
                self.config.input_size = (512, 512)
            elif quality_level in ["high", "maximum"]:
                self.config.apply_postprocessing = True
                self.config.noise_reduction = True
                self.config.edge_refinement = True
                self.config.input_size = (512, 512)
            
            self.logger.info(f"ğŸ›ï¸ í’ˆì§ˆ ë ˆë²¨ ë³€ê²½: {old_quality} -> {quality_level}")
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: í’ˆì§ˆ ë ˆë²¨ ë³€ê²½ ì‹¤íŒ¨: {e}")
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë ˆë²¨ ë³€ê²½ ì‹¤íŒ¨: {e}")

    def enable_debug_mode(self):
        """ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”"""
        self.logger.setLevel(logging.DEBUG)
        self.config.enable_visualization = True
        self.config.show_part_labels = True
        self.logger.debug("ğŸ› ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")

    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            try:
                import psutil
                process = psutil.Process()
                memory_mb = process.memory_info().rss / 1024 / 1024
            except Exception:
                memory_mb = 0.0
            
            return {
                "processing_stats": self.processing_stats.copy(),
                "cache_info": self.get_cache_info(),
                "device_info": {
                    "device": self.device,
                    "mps_available": MPS_AVAILABLE,
                    "opencv_available": OPENCV_AVAILABLE,
                    "pil_available": PIL_AVAILABLE,
                    "torch_available": TORCH_AVAILABLE
                },
                "model_info": {
                    "ai_models_loaded": list(self._ai_models.keys()),
                    "total_models": len(self._ai_models)
                },
                "memory_usage": {
                    "process_memory_mb": memory_mb,
                    "cache_memory_mb": self.get_cache_info().get("memory_usage_estimate", 0)
                },
                "config_info": {
                    "strict_mode": self.config.strict_mode,
                    "quality_level": self.config.quality_level,
                    "enable_visualization": self.config.enable_visualization,
                    "input_size": self.config.input_size
                }
            }
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            self.logger.warning(f"âš ï¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def switch_device(self, new_device: str) -> bool:
        """ë””ë°”ì´ìŠ¤ ì „í™˜"""
        try:
            old_device = self.device
            self.device = new_device
            
            # ë¡œë“œëœ AI ëª¨ë¸ë“¤ì„ ìƒˆ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            for model_name, model in self._ai_models.items():
                if hasattr(model, 'to'):
                    model.to(new_device)
                    self.logger.info(f"ğŸ“± {model_name} -> {new_device}")
            
            self.logger.info(f"ğŸ“± ë””ë°”ì´ìŠ¤ ì „í™˜: {old_device} -> {new_device}")
            return True
            
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: ë””ë°”ì´ìŠ¤ ì „í™˜ ì‹¤íŒ¨: {e}")
            self.logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ì „í™˜ ì‹¤íŒ¨: {e}")
            return False

    async def warmup_step(self) -> bool:
        """Step ì›Œë°ì—… (BaseStepMixin í˜¸í™˜)"""
        try:
            if not self.is_initialized:
                await self.initialize()
            
            # ì›Œë°ì—…ìš© ë”ë¯¸ ì…ë ¥ ìƒì„±
            if TORCH_AVAILABLE:
                dummy_input = torch.randn(1, 3, *self.config.input_size, device=self.device)
                
                # ì›Œë°ì—… ì‹¤í–‰
                await self._warmup_models()
                
                self.logger.info(f"ğŸ”¥ {self.step_name} ì›Œë°ì—… ì™„ë£Œ")
                return True
            else:
                self.logger.warning("âš ï¸ PyTorch ì—†ìŒ - ì›Œë°ì—… ê±´ë„ˆëœ€")
                return False
            
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: Step ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            self.logger.warning(f"âš ï¸ Step ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return False

    # ==============================================
    # ğŸ”¥ 13. ì¶”ê°€ ê¸°ëŠ¥ ë©”ì„œë“œë“¤ (BaseStepMixin í˜¸í™˜ì„±)
    # ==============================================
    
    async def process_batch(
        self,
        image_batch: List[torch.Tensor],
        **kwargs
    ) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›"""
        results = []
        
        try:
            self.logger.info(f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(image_batch)}ê°œ ì´ë¯¸ì§€")
            
            for i, image_tensor in enumerate(image_batch):
                self.logger.info(f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ {i+1}/{len(image_batch)}")
                result = await self.process(image_tensor, **kwargs)
                results.append(result)
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ì¤‘ìš”)
                if i % 5 == 4:  # 5ê°œë§ˆë‹¤ ì •ë¦¬
                    gc.collect()
                    if self.device == 'mps' and MPS_AVAILABLE:
                        try:
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                        except Exception:
                            pass
            
            self.logger.info(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ")
            return results
            
        except Exception as e:
            if self.config.strict_mode:
                raise RuntimeError(f"âŒ strict_mode: ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return results
    
    def get_model_info(self) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        try:
            model_info = {
                "total_models": len(self._ai_models),
                "loaded_models": list(self._ai_models.keys()),
                "model_types": {
                    name: model.__class__.__name__ 
                    for name, model in self._ai_models.items()
                },
                "device": self.device,
                "checkpoints_loaded": list(self._model_checkpoints.keys()),
                "model_loader_available": self.model_loader is not None,
                "dependencies": {
                    "pytorch": TORCH_AVAILABLE,
                    "mps": MPS_AVAILABLE,
                    "opencv": OPENCV_AVAILABLE,
                    "pil": PIL_AVAILABLE
                }
            }
            
            # ëª¨ë¸ë³„ ìƒì„¸ ì •ë³´
            for name, model in self._ai_models.items():
                try:
                    if hasattr(model, 'parameters') and TORCH_AVAILABLE:
                        param_count = sum(p.numel() for p in model.parameters())
                        model_info[f"{name}_parameters"] = param_count
                    if hasattr(model, 'model_name'):
                        model_info[f"{name}_model_name"] = model.model_name
                except Exception:
                    pass
            
            return model_info
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {
                "total_models": len(self._ai_models),
                "error": str(e)
            }
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        try:
            stats = self.processing_stats.copy()
            
            # ì„±ê³µë¥  ê³„ì‚°
            total = stats.get('total_processed', 0)
            success = stats.get('success_count', 0)
            if total > 0:
                stats['success_rate'] = (success / total) * 100.0
            else:
                stats['success_rate'] = 0.0
            
            # AI ëª¨ë¸ ì •ë³´ ì¶”ê°€
            stats['ai_model_info'] = {
                'models_loaded': len(self._ai_models),
                'primary_model_available': 'primary' in self._ai_models,
                'backup_model_available': 'backup' in self._ai_models,
                'device': self.device
            }
            
            # ì˜ì¡´ì„± ì •ë³´
            stats['dependencies_status'] = {
                'model_loader': self.model_loader is not None,
                'memory_manager': self.memory_manager is not None,
                'data_converter': self.data_converter is not None,
                'pytorch': TORCH_AVAILABLE,
                'mps': MPS_AVAILABLE
            }
            
            return stats
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return self.processing_stats.copy()
    
    async def get_step_info(self) -> Dict[str, Any]:
        """Step ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        try:
            return {
                "step_name": "human_parsing",
                "step_number": 1,
                "device": self.device,
                "initialized": self.is_initialized,
                "ai_models": list(self._ai_models.keys()),
                "checkpoints": list(self._model_checkpoints.keys()),
                "dependencies": {
                    "model_loader": self.model_loader is not None,
                    "memory_manager": self.memory_manager is not None,
                    "data_converter": self.data_converter is not None,
                },
                "config": {
                    "model_name": self.config.model_name,
                    "backup_model": self.config.backup_model,
                    "input_size": self.config.input_size,
                    "num_classes": self.config.num_classes,
                    "use_fp16": self.config.use_fp16,
                    "enable_visualization": self.config.enable_visualization,
                    "strict_mode": self.config.strict_mode
                },
                "performance": self.processing_stats,
                "cache": {
                    "size": len(self.result_cache),
                    "max_size": self.config.max_cache_size
                }
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ Step ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {
                "step_name": "human_parsing",
                "step_number": 1,
                "device": self.device,
                "initialized": self.is_initialized,
                "error": str(e)
            }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ Step 01 ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
            
            # AI ëª¨ë¸ ì •ë¦¬
            for model_name, model in list(self._ai_models.items()):
                try:
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                    self.logger.debug(f"AI ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ: {model_name}")
                except Exception as e:
                    self.logger.debug(f"AI ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨ ({model_name}): {e}")
            
            self._ai_models.clear()
            self._model_checkpoints.clear()
            
            # ìºì‹œ ì •ë¦¬
            with self.cache_lock:
                self.result_cache.clear()
            
            # ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            try:
                self.executor.shutdown(wait=True)
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œ í’€ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.memory_manager and hasattr(self.memory_manager, 'cleanup'):
                try:
                    await self.memory_manager.cleanup()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == 'mps' and MPS_AVAILABLE:
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                    except Exception:
                        pass
                elif self.device == 'cuda':
                    try:
                        torch.cuda.empty_cache()
                    except Exception:
                        pass
                
                gc.collect()
            
            # ìƒíƒœ ì´ˆê¸°í™”
            self.is_initialized = False
            
            self.logger.info("âœ… Step 01 ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ==============================================
# ğŸ”¥ 13. íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤ (í˜¸í™˜ì„±)
# ==============================================

async def create_human_parsing_step(
    device: str = "auto",
    config: Optional[Union[Dict[str, Any], HumanParsingConfig]] = None,
    strict_mode: bool = True,
    **kwargs
) -> HumanParsingStep:
    """
    Step 01 íŒ©í† ë¦¬ í•¨ìˆ˜ (ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´)
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ("auto"ëŠ” ìë™ ê°ì§€)
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” HumanParsingConfig
        strict_mode: ì—„ê²© ëª¨ë“œ (ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì—ëŸ¬)
        **kwargs: ì¶”ê°€ ì„¤ì •
        
    Returns:
        HumanParsingStep: ìƒì„±ëœ Step (ì˜ì¡´ì„± ì£¼ì… í•„ìš”)
        
    Note:
        ë°˜í™˜ëœ Stepì—ëŠ” ë°˜ë“œì‹œ ì˜ì¡´ì„± ì£¼ì… í›„ initialize() í˜¸ì¶œ í•„ìš”
    """
    
    try:
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        device_param = None if device == "auto" else device
        
        # ê¸°ë³¸ ì„¤ì • ìƒì„±
        default_config = HumanParsingConfig(
            model_name="human_parsing_graphonomy",
            backup_model="human_parsing_u2net",
            device=device_param,
            use_fp16=True,
            warmup_enabled=True,
            apply_postprocessing=True,
            enable_visualization=True,
            visualization_quality="high",
            show_part_labels=True,
            optimization_enabled=kwargs.get('optimization_enabled', True),
            quality_level=kwargs.get('quality_level', 'balanced'),
            strict_mode=strict_mode
        )
        
        # ì‚¬ìš©ì ì„¤ì • ë³‘í•©
        if isinstance(config, dict):
            for key, value in config.items():
                if hasattr(default_config, key):
                    try:
                        setattr(default_config, key, value)
                    except Exception:
                        pass
            final_config = default_config
        elif isinstance(config, HumanParsingConfig):
            final_config = config
        else:
            final_config = default_config
        
        # kwargs ì ìš©
        for key, value in kwargs.items():
            if hasattr(final_config, key):
                try:
                    setattr(final_config, key, value)
                except Exception:
                    pass
        
        # Step ìƒì„± (ì˜ì¡´ì„± ì£¼ì…ì€ ì™¸ë¶€ì—ì„œ ìˆ˜í–‰)
        step = HumanParsingStep(device=device_param, config=final_config)
        
        logger.info("âœ… HumanParsingStep ìƒì„± ì™„ë£Œ - ì˜ì¡´ì„± ì£¼ì… ëŒ€ê¸° ì¤‘")
        
        return step
        
    except Exception as e:
        if strict_mode:
            raise RuntimeError(f"âŒ strict_mode: create_human_parsing_step ì‹¤íŒ¨: {e}")
        logger.error(f"âŒ create_human_parsing_step ì‹¤íŒ¨: {e}")
        
        # í´ë°±: ìµœì†Œí•œì˜ Step ìƒì„± (strict_mode=Falseì¸ ê²½ìš°ë§Œ)
        step = HumanParsingStep(
            device='cpu', 
            config=HumanParsingConfig(strict_mode=False)
        )
        return step

def create_human_parsing_step_sync(
    device: str = "auto",
    config: Optional[Union[Dict[str, Any], HumanParsingConfig]] = None,
    strict_mode: bool = True,
    **kwargs
) -> HumanParsingStep:
    """ë™ê¸°ì‹ Step 01 ìƒì„± (ë ˆê±°ì‹œ í˜¸í™˜)"""
    try:
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            create_human_parsing_step(device, config, strict_mode, **kwargs)
        )
    except Exception as e:
        if strict_mode:
            raise RuntimeError(f"âŒ strict_mode: create_human_parsing_step_sync ì‹¤íŒ¨: {e}")
        logger.error(f"âŒ create_human_parsing_step_sync ì‹¤íŒ¨: {e}")
        
        # ì•ˆì „í•œ í´ë°± (strict_mode=Falseì¸ ê²½ìš°ë§Œ)
        return HumanParsingStep(
            device='cpu', 
            config=HumanParsingConfig(strict_mode=False)
        )

# ==============================================
# ğŸ”¥ 15. ê³ ê¸‰ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ (ì›ë³¸ ì™„ì „ ë³µì›)
# ==============================================

async def test_all_features():
    """ğŸ”¥ ëª¨ë“  ëˆ„ë½ ê¸°ëŠ¥ë“¤ í¬í•¨í•œ ì™„ì „ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì™„ì „ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ëª¨ë“  ëˆ„ë½ ê¸°ëŠ¥ í¬í•¨)")
    
    try:
        # Step ìƒì„±
        step = await create_human_parsing_step(
            device="auto",
            config={
                "enable_visualization": True,
                "visualization_quality": "high",
                "show_part_labels": True,
                "strict_mode": False  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ False
            }
        )
        
        # ì˜ì¡´ì„± ì£¼ì… ì‹œë®¬ë ˆì´ì…˜
        class MockModelLoader:
            def load_model(self, model_name):
                return {"mock_checkpoint": True, "model_name": model_name}
        
        step.set_model_loader(MockModelLoader())
        await step.initialize()
        
        # ë”ë¯¸ ì´ë¯¸ì§€ë“¤ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ìš©)
        if TORCH_AVAILABLE:
            dummy_images = [torch.randn(1, 3, 512, 512) for _ in range(3)]
        else:
            dummy_images = [np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8) for _ in range(3)]
        
        print("ğŸ”„ 1. ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
        result = await step.process(dummy_images[0])
        print(f"   âœ… ì²˜ë¦¬ ì„±ê³µ: {result['success']}")
        
        print("ğŸ”„ 2. ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
        if TORCH_AVAILABLE:
            batch_results = await step.process_batch(dummy_images)
            print(f"   âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(batch_results)}ê°œ")
        else:
            print("   âš ï¸ PyTorch ì—†ìŒ - ë°°ì¹˜ ì²˜ë¦¬ ê±´ë„ˆëœ€")
            batch_results = [result] * 3
        
        print("ğŸ”„ 3. ê²°ê³¼ ì €ì¥ í…ŒìŠ¤íŠ¸")
        save_success = step.save_parsing_result(result, "/tmp/test_result.json")
        print(f"   âœ… ì €ì¥ ì„±ê³µ: {save_success}")
        
        print("ğŸ”„ 4. ë§ˆìŠ¤í¬ ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸")
        export_success = step.export_body_masks(result, "/tmp/masks/")
        print(f"   âœ… ë§ˆìŠ¤í¬ ë‚´ë³´ë‚´ê¸°: {export_success}")
        
        print("ğŸ”„ 5. ì• ë‹ˆë©”ì´ì…˜ ìƒì„± í…ŒìŠ¤íŠ¸")
        animation_success = step.create_parsing_animation(batch_results, "/tmp/animation.gif")
        print(f"   âœ… ì• ë‹ˆë©”ì´ì…˜ ìƒì„±: {animation_success}")
        
        print("ğŸ”„ 6. í†µê³„ í™•ì¸")
        stats = step.get_processing_statistics()
        print(f"   ğŸ“Š ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {stats['total_processed']}")
        print(f"   ğŸ“Š ì„±ê³µë¥ : {stats.get('success_rate', 0):.1f}%")
        
        print("ğŸ”„ 7. ìºì‹œ ì •ë³´ í™•ì¸")
        cache_info = step.get_cache_info()
        print(f"   ğŸ’¾ ìºì‹œ í¬ê¸°: {cache_info.get('cache_size', 0)}")
        
        print("ğŸ”„ 8. ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±")
        performance_report = step.get_performance_report()
        print(f"   ğŸ“ˆ ë¦¬í¬íŠ¸ ìƒì„±: {'error' not in performance_report}")
        
        print("ğŸ”„ 9. ì˜ë¥˜ ë§ˆìŠ¤í¬ í…ŒìŠ¤íŠ¸")
        if 'parsing_map' in result:
            upper_mask = step.get_clothing_mask(result['parsing_map'], 'upper_body')
            print(f"   ğŸ‘• ìƒì˜ ë§ˆìŠ¤í¬ í¬ê¸°: {upper_mask.shape}")
        
        print("ğŸ”„ 10. ìƒì„¸ ì‹œê°í™” í…ŒìŠ¤íŠ¸")
        if 'parsing_map' in result:
            dummy_orig = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
            detailed_viz = step.create_detailed_visualization(result['parsing_map'], dummy_orig)
            print(f"   ğŸ¨ ìƒì„¸ ì‹œê°í™”: {'ì„±ê³µ' if detailed_viz else 'ì‹¤íŒ¨'}")
        
        print("ğŸ”„ 11. íŒŒì‹± ì‹œê°í™” í…ŒìŠ¤íŠ¸")
        if 'parsing_map' in result:
            visualized = step.visualize_parsing(result['parsing_map'])
            print(f"   ğŸŒˆ íŒŒì‹± ì‹œê°í™” í¬ê¸°: {visualized.shape}")
        
        print("ğŸ”„ 12. ê²°ê³¼ ë¡œë“œ í…ŒìŠ¤íŠ¸")
        loaded_result = step.load_parsing_result("/tmp/test_result.json")
        print(f"   ğŸ“‚ ê²°ê³¼ ë¡œë“œ: {'ì„±ê³µ' if loaded_result else 'ì‹¤íŒ¨'}")
        
        print("ğŸ”„ 13. ëª¨ë¸ ì •ë³´ í…ŒìŠ¤íŠ¸")
        model_info = step.get_model_info()
        print(f"   ğŸ¤– AI ëª¨ë¸ ìˆ˜: {model_info['total_models']}ê°œ")
        
        print("ğŸ”„ 14. ìºì‹œ ê´€ë¦¬ í…ŒìŠ¤íŠ¸")
        step.clear_cache()
        print(f"   ğŸ§¹ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        
        print("ğŸ”„ 15. í’ˆì§ˆ ë ˆë²¨ ë³€ê²½ í…ŒìŠ¤íŠ¸")
        step.set_quality_level("high")
        print(f"   ğŸ›ï¸ í’ˆì§ˆ ë ˆë²¨ ë³€ê²½ ì™„ë£Œ")
        
        print("ğŸ”„ 16. ë””ë²„ê·¸ ëª¨ë“œ í…ŒìŠ¤íŠ¸")
        step.enable_debug_mode()
        print(f"   ğŸ› ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
        
        print("ğŸ”„ 17. í†µê³„ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸")
        step.reset_statistics()
        print(f"   ğŸ“Š í†µê³„ ì´ˆê¸°í™” ì™„ë£Œ")
        
        print("ğŸ”„ 18. ë””ë°”ì´ìŠ¤ ì „í™˜ í…ŒìŠ¤íŠ¸")
        switch_success = step.switch_device("cpu")
        print(f"   ğŸ“± ë””ë°”ì´ìŠ¤ ì „í™˜: {switch_success}")
        
        print("ğŸ”„ 19. ì›Œë°ì—… í…ŒìŠ¤íŠ¸")
        warmup_success = await step.warmup_step()
        print(f"   ğŸ”¥ ì›Œë°ì—…: {warmup_success}")
        
        # ì •ë¦¬
        await step.cleanup()
        print("âœ… ëª¨ë“  ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (19ê°œ ê¸°ëŠ¥ ëª¨ë‘ í™•ì¸)")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

async def test_strict_mode():
    """ğŸš¨ strict_mode í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª strict_mode í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # strict_mode=Trueë¡œ Step ìƒì„±
        step = await create_human_parsing_step(
            device="auto",
            config={
                "enable_visualization": True,
                "strict_mode": True  # ğŸ”¥ strict ëª¨ë“œ í™œì„±í™”
            }
        )
        
        print(f"âœ… strict_mode Step ìƒì„± ì„±ê³µ")
        print(f"ğŸš¨ Strict Mode: {step.config.strict_mode}")
        
        # ì˜ì¡´ì„± ì£¼ì… ì‹œë®¬ë ˆì´ì…˜
        class MockModelLoader:
            def load_model(self, model_name):
                return {"mock_checkpoint": True, "model_name": model_name}
        
        step.set_model_loader(MockModelLoader())
        await step.initialize()
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ì²˜ë¦¬
        if TORCH_AVAILABLE:
            dummy_image = torch.randn(1, 3, 512, 512)
            result = await step.process(dummy_image)
            print(f"âœ… strict_mode ì²˜ë¦¬ ì„±ê³µ: {result['success']}")
        else:
            print("âš ï¸ PyTorch ì—†ìŒ - ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€")
        
        await step.cleanup()
        print("âœ… strict_mode í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except RuntimeError as e:
        print(f"ğŸš¨ ì˜ˆìƒëœ strict_mode ì—ëŸ¬: {e}")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")

async def test_real_model_loading():
    """ğŸ”¥ ì‹¤ì œ ModelLoader ì—°ë™ í…ŒìŠ¤íŠ¸ (strict_mode)"""
    print("ğŸ§ª ì‹¤ì œ ModelLoader ì—°ë™ í…ŒìŠ¤íŠ¸ ì‹œì‘ (strict_mode)")
    
    try:
        # Step ìƒì„± (strict_mode=True)
        step = await create_human_parsing_step(
            device="auto",
            config={
                "enable_visualization": True,
                "visualization_quality": "high",
                "warmup_enabled": True,
                "model_name": "human_parsing_graphonomy",
                "backup_model": "human_parsing_u2net",
                "strict_mode": True  # ğŸ”¥ strict_mode í™œì„±í™”
            }
        )
        
        # ModelLoader ì—°ë™ ìƒíƒœ í™•ì¸ (ì‹¤ì œë¡œëŠ” StepFactoryì—ì„œ ì£¼ì…)
        print(f"ğŸ“Š Step ì´ë¦„: {step.step_name}")
        print(f"ğŸ”— ì˜ì¡´ì„± ì£¼ì… ëŒ€ê¸°: {step.model_loader is None}")
        print(f"ğŸ“¦ AI ëª¨ë¸ ìˆ˜: {len(step._ai_models)}")
        print(f"ğŸš¨ Strict Mode: {step.config.strict_mode}")
        
        # ì˜ì¡´ì„± ì£¼ì… ì‹œë®¬ë ˆì´ì…˜
        class RealModelLoader:
            def load_model_async(self, model_name):
                print(f"ğŸ“¦ RealModelLoader: {model_name} ë¹„ë™ê¸° ë¡œë“œ")
                # ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ì‹œë®¬ë ˆì´ì…˜
                return {
                    "state_dict": {"conv1.weight": torch.randn(64, 3, 7, 7) if TORCH_AVAILABLE else "mock"},
                    "model_name": model_name,
                    "epoch": 100
                }
        
        step.set_model_loader(RealModelLoader())
        print("âœ… RealModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        
        # ì´ˆê¸°í™” (ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ)
        init_success = await step.initialize()
        print(f"âœ… ì´ˆê¸°í™” ì„±ê³µ: {init_success}")
        print(f"ğŸ“¦ ë¡œë“œëœ AI ëª¨ë¸: {list(step._ai_models.keys())}")
        
        # ë”ë¯¸ ì´ë¯¸ì§€ í…ì„œ ìƒì„±
        if TORCH_AVAILABLE:
            dummy_image = torch.randn(1, 3, 512, 512)
            
            # ì²˜ë¦¬ ì‹¤í–‰
            result = await step.process(dummy_image)
            
            # ê²°ê³¼ í™•ì¸
            if result["success"]:
                print("âœ… ì²˜ë¦¬ ì„±ê³µ!")
                print(f"ğŸ“Š ê°ì§€ëœ ë¶€ìœ„: {result['details']['detected_parts']}/20")
                print(f"ğŸ¨ ì‹œê°í™” ì´ë¯¸ì§€: {'ìˆìŒ' if result['details']['result_image'] else 'ì—†ìŒ'}")
                print(f"ğŸŒˆ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€: {'ìˆìŒ' if result['details']['overlay_image'] else 'ì—†ìŒ'}")
                
                # ModelLoader ì‚¬ìš© í†µê³„
                step_info = result['details']['step_info']
                print(f"ğŸ”¥ ModelLoader í˜¸ì¶œ: {step_info.get('model_loader_calls', 0)}íšŒ")
                print(f"ğŸš€ AI ì¶”ë¡  í˜¸ì¶œ: {step_info.get('ai_inference_calls', 0)}íšŒ")
                print(f"ğŸš¨ Strict Mode: {step_info.get('strict_mode', False)}")
            else:
                print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('message', 'Unknown error')}")
        else:
            print("âš ï¸ PyTorch ì—†ìŒ - ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€")
        
        # í†µê³„ í™•ì¸
        stats = step.get_processing_statistics()
        print(f"ğŸ“ˆ ì„±ê³µë¥ : {stats.get('success_rate', 0):.1f}%")
        
        # ì •ë¦¬
        await step.cleanup()
        print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        
    except RuntimeError as e:
        print(f"ğŸš¨ strict_mode ì—ëŸ¬: {e}")
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 16. ëª¨ë“ˆ export (ì›ë³¸ ì™„ì „ ë³µì›)
# ==============================================

async def test_complete_pipeline():
    """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (ì˜ì¡´ì„± ì£¼ì… í¬í•¨)"""
    print("ğŸ§ª ì™„ì „í•œ Step 01 íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # 1. Step ìƒì„±
        step = await create_human_parsing_step(
            device="auto",
            config={
                "enable_visualization": True,
                "visualization_quality": "high",
                "strict_mode": False  # í…ŒìŠ¤íŠ¸ìš©
            }
        )
        
        print(f"âœ… Step ìƒì„± ì™„ë£Œ: {step.step_name}")
        
        # 2. ì˜ì¡´ì„± ì£¼ì… ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” StepFactoryì—ì„œ ìˆ˜í–‰)
        class MockModelLoader:
            def load_model(self, model_name):
                print(f"ğŸ“¦ MockModelLoader: {model_name} ë¡œë“œ ì¤‘...")
                # ë”ë¯¸ ì²´í¬í¬ì¸íŠ¸ ë°˜í™˜
                return {"mock_checkpoint": True, "model_name": model_name}
        
        mock_loader = MockModelLoader()
        step.set_model_loader(mock_loader)
        print("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        
        # 3. ì´ˆê¸°í™”
        success = await step.initialize()
        print(f"âœ… ì´ˆê¸°í™” ì„±ê³µ: {success}")
        
        # 4. ë”ë¯¸ ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        if TORCH_AVAILABLE:
            dummy_image = torch.randn(1, 3, 512, 512)
            result = await step.process(dummy_image)
            print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {result['success']}")
            print(f"ğŸ“Š ê°ì§€ëœ ë¶€ìœ„: {result['details']['detected_parts']}/20")
        else:
            print("âš ï¸ PyTorch ì—†ìŒ - ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ìƒëµ")
        
        # 5. í†µê³„ í™•ì¸
        stats = step.get_processing_statistics()
        print(f"ğŸ“Š ì²˜ë¦¬ í†µê³„: ì´ {stats['total_processed']}ê±´")
        
        # 6. ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = step.get_model_info()
        print(f"ğŸ¤– AI ëª¨ë¸ ìˆ˜: {model_info['total_models']}ê°œ")
        
        # 7. ì •ë¦¬
        await step.cleanup()
        print("ğŸ§¹ ì •ë¦¬ ì™„ë£Œ")
        
        print("âœ… ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

# ==============================================
# ğŸ”¥ 15. ëª¨ë“ˆ export
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'HumanParsingStep',
    'HumanParsingConfig',
    'GraphonomyModel',
    'HumanParsingU2Net',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_human_parsing_step',
    'create_human_parsing_step_sync',
    
    # ìƒìˆ˜ë“¤
    'BODY_PARTS',
    'CLOTHING_CATEGORIES',
    'VISUALIZATION_COLORS',
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
    'test_complete_pipeline'
]

# ==============================================
# ğŸ”¥ 16. ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€
# ==============================================

logger.info("=" * 80)
logger.info("âœ… Step 01 Human Parsing - ì™„ì „í•œ AI ì—°ë™ ë° ì˜ì¡´ì„± ì£¼ì… ë²„ì „ ë¡œë“œ ì™„ë£Œ")
logger.info("=" * 80)
logger.info("ğŸ”¥ í•µì‹¬ ê¸°ëŠ¥:")
logger.info("   âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ModelLoader ì—°ë™")
logger.info("   âœ… ì‹¤ì œ AI ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬í˜„ (Graphonomy + U2Net)")
logger.info("   âœ… ì²´í¬í¬ì¸íŠ¸ â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ ë³€í™˜ ë¡œì§")
logger.info("   âœ… BaseStepMixin ì™„ì „ í™œìš© ë° ìƒì†")
logger.info("   âœ… 20ê°œ ë¶€ìœ„ ì •ë°€ ì¸ì²´ íŒŒì‹±")
logger.info("   âœ… M3 Max MPS ìµœì í™”")
logger.info("   âœ… ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„± (colored parsing + overlay)")
logger.info("   âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± ë° ì—ëŸ¬ ì²˜ë¦¬")
logger.info("   âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›")
logger.info("")
logger.info("ğŸ—ï¸ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´:")
logger.info("   StepFactory â†’ ModelLoader (ìƒì„±) â†’ BaseStepMixin (ìƒì„±)")
logger.info("   â†’ set_model_loader() ì£¼ì… â†’ initialize() â†’ ì™„ì„±ëœ Step")
logger.info("")
logger.info("ğŸ¤– AI ëª¨ë¸ êµ¬ì¡°:")
logger.info("   ğŸ“¦ ModelLoader: ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ")
logger.info("   ğŸ”„ Checkpoint â†’ AI Model: ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‹¤ì œ PyTorch ëª¨ë¸ í´ë˜ìŠ¤ë¡œ ë³€í™˜")
logger.info("   ğŸš€ AI Inference: ì‹¤ì œ ë”¥ëŸ¬ë‹ ì¶”ë¡  ìˆ˜í–‰")
logger.info("   ğŸ¨ Visualization: 20ê°œ ë¶€ìœ„ë³„ ì»¬ëŸ¬ ì‹œê°í™”")
logger.info("")
logger.info("ğŸ¯ ì§€ì› ëª¨ë¸:")
logger.info("   1ï¸âƒ£ GraphonomyModel - ì£¼ ëª¨ë¸ (20í´ë˜ìŠ¤ ì¸ì²´ íŒŒì‹±)")
logger.info("   2ï¸âƒ£ HumanParsingU2Net - ë°±ì—… ëª¨ë¸ (ê²½ëŸ‰í™”)")
logger.info("")
logger.info("ğŸ“‹ ì‚¬ìš©ë²•:")
logger.info("   # StepFactoryì—ì„œ ì‚¬ìš©")
logger.info("   step = await create_human_parsing_step()")
logger.info("   step.set_model_loader(model_loader)  # ì˜ì¡´ì„± ì£¼ì…")
logger.info("   await step.initialize()  # AI ëª¨ë¸ ë¡œë“œ")
logger.info("   result = await step.process(image_tensor)  # ì‹¤ì œ AI ì¶”ë¡ ")
logger.info("")
logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ:")
logger.info(f"   - conda í™˜ê²½: {CONDA_ENV}")
logger.info(f"   - PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
logger.info(f"   - MPS: {'âœ…' if MPS_AVAILABLE else 'âŒ'}")
logger.info(f"   - OpenCV: {'âœ…' if OPENCV_AVAILABLE else 'âŒ'}")
logger.info(f"   - PIL: {'âœ…' if PIL_AVAILABLE else 'âŒ'}")
logger.info("")
logger.info("=" * 80)
logger.info("ğŸš€ Step 01 Human Parsing v2.0 ì¤€ë¹„ ì™„ë£Œ!")
logger.info("   âœ… ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì™„ì „ êµ¬í˜„")
logger.info("   âœ… ì‹¤ì œ AI ëª¨ë¸ ì•„í‚¤í…ì²˜ í¬í•¨")
logger.info("   âœ… ModelLoader â†’ ì²´í¬í¬ì¸íŠ¸ â†’ AI ëª¨ë¸ ì™„ì „ ì—°ë™")
logger.info("   âœ… M3 Max ìµœì í™” ë° í”„ë¡œë•ì…˜ ì•ˆì •ì„±")
logger.info("=" * 80)

# ëª¨ë“ˆ ë¡œë”© ì‹œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("ğŸ§ª Step 01 Human Parsing ì „ì²´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    print("=" * 80)
    
    # ì „ì²´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    asyncio.run(test_all_features())
    
    print("\n" + "=" * 80)
    print("ğŸš¨ strict_mode í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # strict_mode í…ŒìŠ¤íŠ¸
    asyncio.run(test_strict_mode())
    
    print("\n" + "=" * 80)
    print("ğŸ”¥ ì‹¤ì œ ModelLoader ì—°ë™ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # ì‹¤ì œ ModelLoader ì—°ë™ í…ŒìŠ¤íŠ¸
    asyncio.run(test_real_model_loading())
    
    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 80)