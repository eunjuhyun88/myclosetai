"""
backend/app/ai_pipeline/steps/step_01_human_parsing.py

ğŸ M3 Max ìµœì í™” í”„ë¡œë•ì…˜ ë ˆë²¨ ì¸ì²´ íŒŒì‹± Step
âœ… ì‹¤ì œ AI ëª¨ë¸ (Graphonomy, UÂ²-Net) ì™„ë²½ ì—°ë™
âœ… ModelLoader ì¸í„°í˜ì´ìŠ¤ ì™„ì „ êµ¬í˜„
âœ… 128GB ë©”ëª¨ë¦¬ ìµœì í™” ë° CoreML ê°€ì†
âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë° ì—ëŸ¬ ì²˜ë¦¬
âœ… ê¸°ì¡´ API í˜¸í™˜ì„± 100% ìœ ì§€

ì²˜ë¦¬ ìˆœì„œ:
1. ModelLoaderë¥¼ í†µí•œ ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ
2. Graphonomy ëª¨ë¸ë¡œ 20ê°œ ë¶€ìœ„ ì¸ì²´ íŒŒì‹±
3. UÂ²-Net ëª¨ë¸ë¡œ ì •ë°€ ì„¸ê·¸ë©˜í…Œì´ì…˜
4. ë¶€ìœ„ë³„ ë§ˆìŠ¤í¬ ìƒì„± ë° ì˜ë¥˜ ì˜ì—­ ë¶„ì„
5. M3 Max ìµœì í™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
"""

import os
import gc
import time
import asyncio
import logging
import threading
from typing import Dict, Any, Optional, Tuple, List, Union
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field

import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
import cv2

# ğŸ”¥ ModelLoader ì—°ë™ - í•µì‹¬ ì„í¬íŠ¸
try:
    from ..utils.model_loader import (
        ModelLoader,
        ModelConfig,
        ModelType,
        BaseStepMixin,
        get_global_model_loader,
        preprocess_image,
        postprocess_segmentation
    )
    MODEL_LOADER_AVAILABLE = True
except ImportError as e:
    logging.error(f"ModelLoader ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    MODEL_LOADER_AVAILABLE = False
    BaseStepMixin = object  # í´ë°±

# ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìœ í‹¸ë¦¬í‹°
try:
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter
except ImportError:
    MemoryManager = None
    DataConverter = None

# Apple Metal Performance Shaders
try:
    import torch.backends.mps
    MPS_AVAILABLE = torch.backends.mps.is_available()
except (ImportError, AttributeError):
    MPS_AVAILABLE = False

# CoreML ì§€ì›
try:
    import coremltools as ct
    COREML_AVAILABLE = True
except ImportError:
    COREML_AVAILABLE = False

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì¸ì²´ íŒŒì‹± ì„¤ì • ë° ìƒìˆ˜
# ==============================================

@dataclass
class HumanParsingConfig:
    """ì¸ì²´ íŒŒì‹± ì „ìš© ì„¤ì •"""
    
    # ëª¨ë¸ ì„¤ì •
    model_name: str = "human_parsing_graphonomy"
    backup_model: str = "human_parsing_u2net"
    device: Optional[str] = None  # ìë™ ê°ì§€
    
    # ì…ë ¥/ì¶œë ¥ ì„¤ì •
    input_size: Tuple[int, int] = (512, 512)
    num_classes: int = 20
    confidence_threshold: float = 0.3
    
    # M3 Max ìµœì í™”
    use_fp16: bool = True
    use_coreml: bool = True
    enable_neural_engine: bool = True
    memory_efficient: bool = True
    
    # ì„±ëŠ¥ ì„¤ì •
    batch_size: int = 1
    max_cache_size: int = 50
    warmup_enabled: bool = True
    
    # í’ˆì§ˆ ì„¤ì •
    apply_postprocessing: bool = True
    noise_reduction: bool = True
    edge_refinement: bool = True
    
    def __post_init__(self):
        """í›„ì²˜ë¦¬ ì´ˆê¸°í™”"""
        if self.device is None:
            self.device = self._auto_detect_device()
        
        # M3 Max íŠ¹í™” ì„¤ì •
        if MPS_AVAILABLE:
            self.use_fp16 = True
            self.enable_neural_engine = True
            if COREML_AVAILABLE:
                self.use_coreml = True
    
    def _auto_detect_device(self) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if MPS_AVAILABLE:
            return 'mps'
        elif torch.cuda.is_available():
            return 'cuda'
        else:
            return 'cpu'

# ì¸ì²´ ë¶€ìœ„ ì •ì˜ (Graphonomy í‘œì¤€)
BODY_PARTS = {
    0: 'background',
    1: 'hat',
    2: 'hair',
    3: 'glove',
    4: 'sunglasses',
    5: 'upper_clothes',
    6: 'dress',
    7: 'coat',
    8: 'socks',
    9: 'pants',
    10: 'torso_skin',
    11: 'scarf',
    12: 'skirt',
    13: 'face',
    14: 'left_arm',
    15: 'right_arm',
    16: 'left_leg',
    17: 'right_leg',
    18: 'left_shoe',
    19: 'right_shoe'
}

# ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í•‘
CLOTHING_CATEGORIES = {
    'upper_body': [5, 6, 7, 11],     # ìƒì˜, ë“œë ˆìŠ¤, ì½”íŠ¸, ìŠ¤ì¹´í”„
    'lower_body': [9, 12],           # ë°”ì§€, ìŠ¤ì»¤íŠ¸
    'accessories': [1, 3, 4],        # ëª¨ì, ì¥ê°‘, ì„ ê¸€ë¼ìŠ¤
    'footwear': [8, 18, 19],         # ì–‘ë§, ì‹ ë°œ
    'skin': [10, 13, 14, 15, 16, 17] # í”¼ë¶€ ë¶€ìœ„
}

# ==============================================
# ğŸ”¥ ë©”ì¸ HumanParsingStep í´ë˜ìŠ¤
# ==============================================

class HumanParsingStep(BaseStepMixin):
    """
    ğŸ M3 Max ìµœì í™” í”„ë¡œë•ì…˜ ë ˆë²¨ ì¸ì²´ íŒŒì‹± Step
    
    âœ… ì‹¤ì œ AI ëª¨ë¸ ì™„ë²½ ì—°ë™
    âœ… ModelLoader ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
    âœ… 20ê°œ ë¶€ìœ„ ì •ë°€ ì¸ì²´ íŒŒì‹±
    âœ… M3 Max Neural Engine ê°€ì†
    âœ… í”„ë¡œë•ì…˜ ì•ˆì •ì„± ë³´ì¥
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Union[Dict[str, Any], HumanParsingConfig]] = None,
        **kwargs
    ):
        """
        ğŸ”¥ Step + ModelLoader í†µí•© ìƒì„±ì
        
        Args:
            device: ë””ë°”ì´ìŠ¤ ('mps', 'cuda', 'cpu', None=ìë™ê°ì§€)
            config: ì„¤ì • (dict ë˜ëŠ” HumanParsingConfig)
            **kwargs: ì¶”ê°€ ì„¤ì •
        """
        
        # === ê¸°ë³¸ Step ì„¤ì • ===
        self.device = device or self._auto_detect_device()
        self.config = self._setup_config(config, kwargs)
        self.step_name = "HumanParsingStep"
        self.step_number = 1
        self.logger = logging.getLogger(f"pipeline.{self.step_name}")
        
        # === ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ===
        if MODEL_LOADER_AVAILABLE:
            self._setup_model_interface()
        else:
            self.logger.error("âŒ ModelLoaderê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
            self.model_interface = None
        
        # === ìƒíƒœ ë³€ìˆ˜ ===
        self.is_initialized = False
        self.models_loaded = {}
        self.processing_stats = {
            'total_processed': 0,
            'average_time': 0.0,
            'cache_hits': 0,
            'model_switches': 0
        }
        
        # === ë©”ëª¨ë¦¬ ë° ìºì‹œ ê´€ë¦¬ ===
        self.result_cache: Dict[str, Any] = {}
        self.cache_lock = threading.RLock()
        self.executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="human_parsing")
        
        # === ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ===
        self.memory_manager = self._create_memory_manager()
        self.data_converter = self._create_data_converter()
        
        self.logger.info(f"ğŸ¯ {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _setup_config(self, config: Optional[Union[Dict, HumanParsingConfig]], kwargs: Dict[str, Any]) -> HumanParsingConfig:
        """ì„¤ì • ê°ì²´ ìƒì„±"""
        if isinstance(config, HumanParsingConfig):
            # ê¸°ì¡´ configì— kwargs ë®ì–´ì“°ê¸°
            for key, value in kwargs.items():
                if hasattr(config, key):
                    setattr(config, key, value)
            return config
        elif isinstance(config, dict):
            # dictë¥¼ HumanParsingConfigë¡œ ë³€í™˜
            merged_config = {**config, **kwargs}
            return HumanParsingConfig(**merged_config)
        else:
            # kwargsë¡œë§Œ ìƒì„±
            return HumanParsingConfig(**kwargs)
    
    def _auto_detect_device(self) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if MPS_AVAILABLE:
            return 'mps'
        elif torch.cuda.is_available():
            return 'cuda'
        else:
            return 'cpu'
    
    def _create_memory_manager(self):
        """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ìƒì„±"""
        if MemoryManager:
            return MemoryManager(device=self.device)
        else:
            # ê¸°ë³¸ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €
            class SimpleMemoryManager:
                def __init__(self, device): self.device = device
                async def get_usage_stats(self): return {"memory_used": "N/A"}
                async def cleanup(self): 
                    gc.collect()
                    if device == 'mps' and MPS_AVAILABLE:
                        try:
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                        except: pass
            return SimpleMemoryManager(self.device)
    
    def _create_data_converter(self):
        """ë°ì´í„° ì»¨ë²„í„° ìƒì„±"""
        if DataConverter:
            return DataConverter()
        else:
            # ê¸°ë³¸ ì»¨ë²„í„°
            class SimpleDataConverter:
                def convert(self, data): return data
                def to_tensor(self, data): return torch.from_numpy(data) if isinstance(data, np.ndarray) else data
                def to_numpy(self, data): return data.cpu().numpy() if torch.is_tensor(data) else data
            return SimpleDataConverter()
    
    async def initialize(self) -> bool:
        """
        âœ… Step ì´ˆê¸°í™” - ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ
        
        Returns:
            bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            self.logger.info("ğŸ”„ 1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            
            if not MODEL_LOADER_AVAILABLE:
                self.logger.error("âŒ ModelLoaderê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥ - í”„ë¡œë•ì…˜ ëª¨ë“œì—ì„œëŠ” í•„ìˆ˜")
                return False
            
            # === ì£¼ ëª¨ë¸ ë¡œë“œ (Graphonomy) ===
            primary_model = await self._load_primary_model()
            
            # === ë°±ì—… ëª¨ë¸ ë¡œë“œ (UÂ²-Net) ===
            backup_model = await self._load_backup_model()
            
            # === ëª¨ë¸ ë¡œë“œ ê²°ê³¼ í™•ì¸ ===
            if not (primary_model or backup_model):
                self.logger.error("âŒ ëª¨ë“  ì¸ì²´ íŒŒì‹± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                return False
            
            # === ëª¨ë¸ ì›Œë°ì—… ===
            if self.config.warmup_enabled:
                await self._warmup_models()
            
            # === M3 Max ìµœì í™” ì ìš© ===
            if self.device == 'mps':
                await self._apply_m3_max_optimizations()
            
            self.is_initialized = True
            self.logger.info("âœ… 1ë‹¨ê³„: ì¸ì²´ íŒŒì‹± ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ 1ë‹¨ê³„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    async def _load_primary_model(self) -> Optional[Any]:
        """ì£¼ ëª¨ë¸ (Graphonomy) ë¡œë“œ"""
        try:
            if not self.model_interface:
                self.logger.error("âŒ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            self.logger.info(f"ğŸ“¦ ì£¼ ëª¨ë¸ ë¡œë“œ ì¤‘: {self.config.model_name}")
            
            # ModelLoaderë¥¼ í†µí•œ ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ
            model = await self.model_interface.get_model(self.config.model_name)
            
            if model:
                self.models_loaded['primary'] = model
                self.logger.info(f"âœ… ì£¼ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.config.model_name}")
                return model
            else:
                self.logger.warning(f"âš ï¸ ì£¼ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {self.config.model_name}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ì£¼ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None
    
    async def _load_backup_model(self) -> Optional[Any]:
        """ë°±ì—… ëª¨ë¸ (UÂ²-Net) ë¡œë“œ"""
        try:
            if not self.model_interface:
                return None
            
            self.logger.info(f"ğŸ“¦ ë°±ì—… ëª¨ë¸ ë¡œë“œ ì¤‘: {self.config.backup_model}")
            
            backup_model = await self.model_interface.get_model(self.config.backup_model)
            
            if backup_model:
                self.models_loaded['backup'] = backup_model
                self.logger.info(f"âœ… ë°±ì—… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.config.backup_model}")
                return backup_model
            else:
                self.logger.info(f"â„¹ï¸ ë°±ì—… ëª¨ë¸ ë¡œë“œ ê±´ë„ˆëœ€: {self.config.backup_model}")
                return None
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë°±ì—… ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None
    
    async def _warmup_models(self):
        """ëª¨ë¸ ì›Œë°ì—… (ì²« ì¶”ë¡  ìµœì í™”)"""
        self.logger.info("ğŸ”¥ 1ë‹¨ê³„ ëª¨ë¸ ì›Œë°ì—… ì¤‘...")
        
        try:
            # ë”ë¯¸ ì…ë ¥ ìƒì„±
            dummy_input = torch.randn(1, 3, *self.config.input_size, device=self.device)
            
            # ì£¼ ëª¨ë¸ ì›Œë°ì—…
            if 'primary' in self.models_loaded:
                model = self.models_loaded['primary']
                if hasattr(model, 'eval'):
                    model.eval()
                with torch.no_grad():
                    _ = model(dummy_input)
                self.logger.info("ğŸ”¥ ì£¼ ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            
            # ë°±ì—… ëª¨ë¸ ì›Œë°ì—…
            if 'backup' in self.models_loaded:
                model = self.models_loaded['backup']
                if hasattr(model, 'eval'):
                    model.eval()
                with torch.no_grad():
                    _ = model(dummy_input)
                self.logger.info("ğŸ”¥ ë°±ì—… ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    async def _apply_m3_max_optimizations(self):
        """M3 Max íŠ¹í™” ìµœì í™” ì ìš©"""
        try:
            optimizations = []
            
            # 1. MPS ë°±ì—”ë“œ ìµœì í™”
            if hasattr(torch.backends.mps, 'set_per_process_memory_fraction'):
                torch.backends.mps.set_per_process_memory_fraction(0.8)
                optimizations.append("MPS memory optimization")
            
            # 2. Neural Engine ì¤€ë¹„
            if self.config.enable_neural_engine and COREML_AVAILABLE:
                # CoreML ìµœì í™” ì¤€ë¹„
                optimizations.append("Neural Engine ready")
            
            # 3. ë©”ëª¨ë¦¬ í’€ë§
            if self.config.memory_efficient:
                torch.backends.mps.allow_tf32 = True
                optimizations.append("Memory pooling")
            
            if optimizations:
                self.logger.info(f"ğŸ M3 Max ìµœì í™” ì ìš©: {', '.join(optimizations)}")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì‹¤íŒ¨: {e}")
    
    async def process(
        self,
        person_image_tensor: torch.Tensor,
        **kwargs
    ) -> Dict[str, Any]:
        """
        âœ… ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ - ì‹¤ì œ AI ì¸ì²´ íŒŒì‹±
        
        Args:
            person_image_tensor: ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ [B, C, H, W]
            **kwargs: ì¶”ê°€ ì˜µì…˜
            
        Returns:
            Dict[str, Any]: ì¸ì²´ íŒŒì‹± ê²°ê³¼
        """
        
        if not self.is_initialized:
            self.logger.warning("âš ï¸ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - ìë™ ì´ˆê¸°í™” ì‹œë„")
            await self.initialize()
        
        start_time = time.time()
        
        try:
            # === ìºì‹œ í™•ì¸ ===
            cache_key = self._generate_cache_key(person_image_tensor)
            cached_result = self._get_cached_result(cache_key)
            if cached_result:
                self.processing_stats['cache_hits'] += 1
                self.logger.info("ğŸ’¾ 1ë‹¨ê³„: ìºì‹œëœ ê²°ê³¼ ë°˜í™˜")
                return cached_result
            
            # === ì…ë ¥ ì „ì²˜ë¦¬ ===
            preprocessed_input = await self._preprocess_input(person_image_tensor)
            
            # === ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ===
            parsing_result = await self._run_inference(preprocessed_input)
            
            # === í›„ì²˜ë¦¬ ë° ê²°ê³¼ ìƒì„± ===
            final_result = await self._postprocess_result(
                parsing_result,
                person_image_tensor.shape[2:],
                start_time
            )
            
            # === ìºì‹œ ì €ì¥ ===
            self._cache_result(cache_key, final_result)
            
            # === í†µê³„ ì—…ë°ì´íŠ¸ ===
            self._update_processing_stats(time.time() - start_time)
            
            self.logger.info(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ - {final_result['processing_time']:.3f}ì´ˆ")
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ 1ë‹¨ê³„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
            return self._create_fallback_result(person_image_tensor.shape[2:], time.time() - start_time, str(e))
    
    async def _preprocess_input(self, image_tensor: torch.Tensor) -> torch.Tensor:
        """ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # í¬ê¸° ì •ê·œí™”
            if image_tensor.shape[2:] != self.config.input_size:
                resized = F.interpolate(
                    image_tensor,
                    size=self.config.input_size,
                    mode='bilinear',
                    align_corners=False
                )
            else:
                resized = image_tensor
            
            # ê°’ ë²”ìœ„ ì •ê·œí™” (0-1)
            if resized.max() > 1.0:
                resized = resized.float() / 255.0
            
            # ImageNet ì •ê·œí™”
            mean = torch.tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)
            normalized = (resized - mean) / std
            
            # FP16 ë³€í™˜ (M3 Max ìµœì í™”)
            if self.config.use_fp16 and self.device != 'cpu':
                normalized = normalized.half()
            
            return normalized.to(self.device)
            
        except Exception as e:
            self.logger.error(f"âŒ ì…ë ¥ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    async def _run_inference(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡ """
        try:
            # ì£¼ ëª¨ë¸ (Graphonomy) ìš°ì„  ì‹œë„
            if 'primary' in self.models_loaded:
                model = self.models_loaded['primary']
                try:
                    with torch.no_grad():
                        if self.config.use_fp16 and self.device != 'cpu':
                            with torch.autocast(device_type=self.device.replace(':', '_'), dtype=torch.float16):
                                output = model(input_tensor)
                        else:
                            output = model(input_tensor)
                    
                    self.logger.debug("ğŸš€ ì£¼ ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ (Graphonomy)")
                    return output
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì£¼ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                    self.processing_stats['model_switches'] += 1
            
            # ë°±ì—… ëª¨ë¸ (UÂ²-Net) ì‹œë„
            if 'backup' in self.models_loaded:
                model = self.models_loaded['backup']
                try:
                    with torch.no_grad():
                        if self.config.use_fp16 and self.device != 'cpu':
                            with torch.autocast(device_type=self.device.replace(':', '_'), dtype=torch.float16):
                                output = model(input_tensor)
                        else:
                            output = model(input_tensor)
                    
                    self.logger.debug("ğŸ”„ ë°±ì—… ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ (UÂ²-Net)")
                    return output
                    
                except Exception as e:
                    self.logger.error(f"âŒ ë°±ì—… ëª¨ë¸ ì¶”ë¡ ë„ ì‹¤íŒ¨: {e}")
            
            # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš°
            self.logger.error("âŒ ëª¨ë“  AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨")
            raise RuntimeError("All human parsing models failed")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise
    
    async def _postprocess_result(
        self,
        model_output: torch.Tensor,
        original_size: Tuple[int, int],
        start_time: float
    ) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬ ë° ë¶„ì„"""
        try:
            def _postprocess_sync():
                # í™•ë¥ ì„ í´ë˜ìŠ¤ë¡œ ë³€í™˜
                if model_output.dim() == 4:
                    parsing_map = torch.argmax(model_output, dim=1).squeeze(0)
                else:
                    parsing_map = model_output.squeeze(0)
                
                # CPUë¡œ ì´ë™
                parsing_map = parsing_map.cpu().numpy().astype(np.uint8)
                
                # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
                if parsing_map.shape != original_size:
                    parsing_map = cv2.resize(
                        parsing_map,
                        (original_size[1], original_size[0]),
                        interpolation=cv2.INTER_NEAREST
                    )
                
                # ë…¸ì´ì¦ˆ ì œê±° (í›„ì²˜ë¦¬)
                if self.config.apply_postprocessing:
                    parsing_map = self._apply_morphological_operations(parsing_map)
                
                return parsing_map
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            loop = asyncio.get_event_loop()
            parsing_map = await loop.run_in_executor(self.executor, _postprocess_sync)
            
            # ë¶€ìœ„ë³„ ë§ˆìŠ¤í¬ ìƒì„±
            body_masks = self._create_body_masks(parsing_map)
            
            # ì˜ë¥˜ ì˜ì—­ ë¶„ì„
            clothing_regions = self._analyze_clothing_regions(parsing_map)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(model_output)
            
            # ê°ì§€ëœ ë¶€ìœ„ ì •ë³´
            detected_parts = self._get_detected_parts(parsing_map)
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "parsing_map": parsing_map,
                "body_masks": body_masks,
                "clothing_regions": clothing_regions,
                "confidence": float(confidence),
                "body_parts_detected": detected_parts,
                "processing_time": processing_time,
                "step_info": {
                    "step_name": "human_parsing",
                    "step_number": 1,
                    "model_used": self._get_active_model_name(),
                    "device": self.device,
                    "input_size": self.config.input_size,
                    "num_classes": self.config.num_classes,
                    "optimization": "M3 Max" if self.device == 'mps' else self.device
                },
                "from_cache": False,
                "quality_metrics": {
                    "segmentation_coverage": float(np.sum(parsing_map > 0) / parsing_map.size),
                    "part_count": len(detected_parts),
                    "confidence": float(confidence)
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _apply_morphological_operations(self, parsing_map: np.ndarray) -> np.ndarray:
        """ëª¨í´ë¡œì§€ ì—°ì‚°ì„ í†µí•œ ë…¸ì´ì¦ˆ ì œê±°"""
        try:
            if not self.config.noise_reduction:
                return parsing_map
            
            # ì‘ì€ êµ¬ë© ë©”ìš°ê¸°
            kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            cleaned = cv2.morphologyEx(parsing_map, cv2.MORPH_CLOSE, kernel_close)
            
            # ì‘ì€ ë…¸ì´ì¦ˆ ì œê±°
            kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
            cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, kernel_open)
            
            # ì—£ì§€ ì •êµí™”
            if self.config.edge_refinement:
                # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ê²½ê³„ ë¶€ë“œëŸ½ê²Œ
                blurred = cv2.GaussianBlur(cleaned.astype(np.float32), (3, 3), 0.5)
                cleaned = np.round(blurred).astype(np.uint8)
            
            return cleaned
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨í´ë¡œì§€ ì—°ì‚° ì‹¤íŒ¨: {e}")
            return parsing_map
    
    def _create_body_masks(self, parsing_map: np.ndarray) -> Dict[str, np.ndarray]:
        """ì‹ ì²´ ë¶€ìœ„ë³„ ë°”ì´ë„ˆë¦¬ ë§ˆìŠ¤í¬ ìƒì„±"""
        body_masks = {}
        
        for part_id, part_name in BODY_PARTS.items():
            if part_id == 0:  # ë°°ê²½ ì œì™¸
                continue
            
            mask = (parsing_map == part_id).astype(np.uint8)
            if mask.sum() > 0:  # í•´ë‹¹ ë¶€ìœ„ê°€ ê°ì§€ëœ ê²½ìš°ë§Œ
                body_masks[part_name] = mask
        
        return body_masks
    
    def _analyze_clothing_regions(self, parsing_map: np.ndarray) -> Dict[str, Any]:
        """ì˜ë¥˜ ì˜ì—­ ë¶„ì„ (ë‹¤ìŒ ë‹¨ê³„ë“¤ì„ ìœ„í•œ ì •ë³´)"""
        analysis = {
            "categories_detected": [],
            "coverage_ratio": {},
            "bounding_boxes": {},
            "dominant_category": None,
            "total_clothing_area": 0
        }
        
        total_pixels = parsing_map.size
        max_coverage = 0.0
        total_clothing_pixels = 0
        
        for category, part_ids in CLOTHING_CATEGORIES.items():
            if category == 'skin':  # í”¼ë¶€ëŠ” ì˜ë¥˜ê°€ ì•„ë‹˜
                continue
            
            category_mask = np.zeros_like(parsing_map, dtype=bool)
            
            for part_id in part_ids:
                category_mask |= (parsing_map == part_id)
            
            if category_mask.sum() > 0:
                coverage = category_mask.sum() / total_pixels
                
                analysis["categories_detected"].append(category)
                analysis["coverage_ratio"][category] = coverage
                analysis["bounding_boxes"][category] = self._get_bounding_box(category_mask)
                
                total_clothing_pixels += category_mask.sum()
                
                if coverage > max_coverage:
                    max_coverage = coverage
                    analysis["dominant_category"] = category
        
        analysis["total_clothing_area"] = total_clothing_pixels / total_pixels
        
        return analysis
    
    def _calculate_confidence(self, model_output: torch.Tensor) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            if model_output.dim() == 4 and model_output.shape[1] > 1:
                # ì†Œí”„íŠ¸ë§¥ìŠ¤ í™•ë¥ ì—ì„œ ìµœëŒ€ê°’ë“¤ì˜ í‰ê· 
                probs = F.softmax(model_output, dim=1)
                max_probs = torch.max(probs, dim=1)[0]
                confidence = torch.mean(max_probs).item()
            else:
                # ë°”ì´ë„ˆë¦¬ ì¶œë ¥ì˜ ê²½ìš°
                confidence = float(torch.mean(torch.abs(model_output)).item())
            
            return max(0.0, min(1.0, confidence))  # 0-1 ë²”ìœ„ë¡œ í´ë¨í•‘
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.8  # ê¸°ë³¸ê°’
    
    def _get_detected_parts(self, parsing_map: np.ndarray) -> Dict[str, Any]:
        """ê°ì§€ëœ ì‹ ì²´ ë¶€ìœ„ ìƒì„¸ ì •ë³´"""
        detected_parts = {}
        
        for part_id, part_name in BODY_PARTS.items():
            if part_id == 0:  # ë°°ê²½ ì œì™¸
                continue
            
            mask = (parsing_map == part_id)
            pixel_count = mask.sum()
            
            if pixel_count > 0:
                detected_parts[part_name] = {
                    "pixel_count": int(pixel_count),
                    "percentage": float(pixel_count / parsing_map.size * 100),
                    "bounding_box": self._get_bounding_box(mask),
                    "part_id": part_id,
                    "centroid": self._get_centroid(mask)
                }
        
        return detected_parts
    
    def _get_bounding_box(self, mask: np.ndarray) -> Dict[str, int]:
        """ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
        coords = np.where(mask)
        if len(coords[0]) == 0:
            return {"x": 0, "y": 0, "width": 0, "height": 0}
        
        y_min, y_max = int(coords[0].min()), int(coords[0].max())
        x_min, x_max = int(coords[1].min()), int(coords[1].max())
        
        return {
            "x": x_min,
            "y": y_min,
            "width": x_max - x_min + 1,
            "height": y_max - y_min + 1
        }
    
    def _get_centroid(self, mask: np.ndarray) -> Dict[str, float]:
        """ì¤‘ì‹¬ì  ê³„ì‚°"""
        coords = np.where(mask)
        if len(coords[0]) == 0:
            return {"x": 0.0, "y": 0.0}
        
        y_center = float(np.mean(coords[0]))
        x_center = float(np.mean(coords[1]))
        
        return {"x": x_center, "y": y_center}
    
    def _get_active_model_name(self) -> str:
        """í˜„ì¬ í™œì„± ëª¨ë¸ ì´ë¦„ ë°˜í™˜"""
        if 'primary' in self.models_loaded:
            return self.config.model_name
        elif 'backup' in self.models_loaded:
            return self.config.backup_model
        else:
            return "none"
    
    # ==============================================
    # ğŸ”§ ìºì‹œ ë° ì„±ëŠ¥ ê´€ë¦¬
    # ==============================================
    
    def _generate_cache_key(self, tensor: torch.Tensor) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            # í…ì„œì˜ í•´ì‹œê°’ ê¸°ë°˜ í‚¤ ìƒì„±
            tensor_bytes = tensor.cpu().numpy().tobytes()
            import hashlib
            hash_value = hashlib.md5(tensor_bytes).hexdigest()[:16]
            return f"step01_{hash_value}_{self.config.input_size[0]}x{self.config.input_size[1]}"
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ í‚¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"step01_fallback_{int(time.time())}"
    
    def _get_cached_result(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ"""
        try:
            with self.cache_lock:
                if cache_key in self.result_cache:
                    cached = self.result_cache[cache_key].copy()
                    cached["from_cache"] = True
                    return cached
                return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _cache_result(self, cache_key: str, result: Dict[str, Any]):
        """ê²°ê³¼ ìºì‹± (LRU ë°©ì‹)"""
        try:
            with self.cache_lock:
                # ìºì‹œ í¬ê¸° ì œí•œ
                if len(self.result_cache) >= self.config.max_cache_size:
                    # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                    oldest_key = next(iter(self.result_cache))
                    del self.result_cache[oldest_key]
                
                # ìƒˆ ê²°ê³¼ ì €ì¥
                cached_result = result.copy()
                cached_result["from_cache"] = False
                self.result_cache[cache_key] = cached_result
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _update_processing_stats(self, processing_time: float):
        """ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.processing_stats['total_processed'] += 1
        
        # ì´ë™ í‰ê·  ê³„ì‚°
        current_avg = self.processing_stats['average_time']
        count = self.processing_stats['total_processed']
        new_avg = (current_avg * (count - 1) + processing_time) / count
        self.processing_stats['average_time'] = new_avg
    
    def _create_fallback_result(self, original_size: Tuple[int, int], processing_time: float, error_msg: str) -> Dict[str, Any]:
        """í´ë°± ê²°ê³¼ ìƒì„± (ì—ëŸ¬ ë°œìƒ ì‹œ)"""
        return {
            "success": False,
            "parsing_map": np.zeros(original_size, dtype=np.uint8),
            "body_masks": {},
            "clothing_regions": {
                "categories_detected": [],
                "coverage_ratio": {},
                "bounding_boxes": {},
                "dominant_category": None,
                "total_clothing_area": 0.0
            },
            "confidence": 0.0,
            "body_parts_detected": {},
            "processing_time": processing_time,
            "step_info": {
                "step_name": "human_parsing",
                "step_number": 1,
                "model_used": "fallback",
                "device": self.device,
                "error": error_msg
            },
            "from_cache": False,
            "quality_metrics": {
                "segmentation_coverage": 0.0,
                "part_count": 0,
                "confidence": 0.0
            }
        }
    
    # ==============================================
    # ğŸ”§ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def get_clothing_mask(self, parsing_map: np.ndarray, category: str) -> np.ndarray:
        """íŠ¹ì • ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ì˜ í†µí•© ë§ˆìŠ¤í¬ ë°˜í™˜"""
        if category not in CLOTHING_CATEGORIES:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬: {category}")
        
        combined_mask = np.zeros_like(parsing_map, dtype=np.uint8)
        
        for part_id in CLOTHING_CATEGORIES[category]:
            combined_mask |= (parsing_map == part_id).astype(np.uint8)
        
        return combined_mask
    
    def visualize_parsing(self, parsing_map: np.ndarray) -> np.ndarray:
        """íŒŒì‹± ê²°ê³¼ ì‹œê°í™” (ë””ë²„ê¹…ìš©)"""
        # 20ê°œ ë¶€ìœ„ë³„ ìƒ‰ìƒ ë§¤í•‘
        colors = np.array([
            [0, 0, 0],       # 0: Background
            [128, 0, 0],     # 1: Hat
            [255, 0, 0],     # 2: Hair
            [0, 85, 0],      # 3: Glove
            [170, 0, 51],    # 4: Sunglasses
            [255, 85, 0],    # 5: Upper-clothes
            [0, 0, 85],      # 6: Dress
            [0, 119, 221],   # 7: Coat
            [85, 85, 0],     # 8: Socks
            [0, 85, 85],     # 9: Pants
            [85, 51, 0],     # 10: Torso-skin
            [52, 86, 128],   # 11: Scarf
            [0, 128, 0],     # 12: Skirt
            [0, 0, 255],     # 13: Face
            [51, 170, 221],  # 14: Left-arm
            [0, 255, 255],   # 15: Right-arm
            [85, 255, 170],  # 16: Left-leg
            [170, 255, 85],  # 17: Right-leg
            [255, 255, 0],   # 18: Left-shoe
            [255, 170, 0]    # 19: Right-shoe
        ])
        
        colored_parsing = colors[parsing_map]
        return colored_parsing.astype(np.uint8)
    
    async def get_step_info(self) -> Dict[str, Any]:
        """ğŸ” 1ë‹¨ê³„ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        try:
            memory_stats = await self.memory_manager.get_usage_stats()
        except:
            memory_stats = {"memory_used": "N/A"}
        
        return {
            "step_name": "human_parsing",
            "step_number": 1,
            "device": self.device,
            "initialized": self.is_initialized,
            "models_loaded": list(self.models_loaded.keys()),
            "config": {
                "model_name": self.config.model_name,
                "backup_model": self.config.backup_model,
                "input_size": self.config.input_size,
                "num_classes": self.config.num_classes,
                "use_fp16": self.config.use_fp16,
                "use_coreml": self.config.use_coreml,
                "confidence_threshold": self.config.confidence_threshold
            },
            "performance": self.processing_stats,
            "cache": {
                "size": len(self.result_cache),
                "max_size": self.config.max_cache_size,
                "hit_rate": (self.processing_stats['cache_hits'] / 
                           max(1, self.processing_stats['total_processed'])) * 100
            },
            "memory_usage": memory_stats,
            "optimization": {
                "m3_max_enabled": self.device == 'mps',
                "neural_engine": self.config.enable_neural_engine,
                "memory_efficient": self.config.memory_efficient,
                "fp16_enabled": self.config.use_fp16,
                "coreml_available": COREML_AVAILABLE
            }
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.logger.info("ğŸ§¹ 1ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        try:
            # ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'models_loaded'):
                for model_name, model in self.models_loaded.items():
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                self.models_loaded.clear()
            
            # ìºì‹œ ì •ë¦¬
            with self.cache_lock:
                self.result_cache.clear()
            
            # ModelLoader ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            if hasattr(self, 'model_interface') and self.model_interface:
                self.model_interface.unload_models()
            
            # ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            await self.memory_manager.cleanup()
            
            # MPS ìºì‹œ ì •ë¦¬
            if self.device == 'mps' and MPS_AVAILABLE:
                try:
                    if hasattr(torch.mps, 'empty_cache'):
                        torch.mps.empty_cache()
                except:
                    pass
            
            self.is_initialized = False
            self.logger.info("âœ… 1ë‹¨ê³„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ==============================================
# ğŸ”„ í•˜ìœ„ í˜¸í™˜ì„± ë° íŒ©í† ë¦¬ í•¨ìˆ˜
# ==============================================

async def create_human_parsing_step(
    device: str = "auto",
    config: Optional[Union[Dict[str, Any], HumanParsingConfig]] = None,
    **kwargs
) -> HumanParsingStep:
    """
    ğŸ”„ Step 01 íŒ©í† ë¦¬ í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„±)
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ("auto"ëŠ” ìë™ ê°ì§€)
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” HumanParsingConfig
        **kwargs: ì¶”ê°€ ì„¤ì •
        
    Returns:
        HumanParsingStep: ì´ˆê¸°í™”ëœ 1ë‹¨ê³„ ìŠ¤í…
    """
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device_param = None if device == "auto" else device
    
    # ê¸°ë³¸ ì„¤ì • ë³‘í•©
    default_config = HumanParsingConfig(
        model_name="human_parsing_graphonomy",
        backup_model="human_parsing_u2net",
        device=device_param,
        use_fp16=True,
        use_coreml=COREML_AVAILABLE,
        warmup_enabled=True,
        apply_postprocessing=True
    )
    
    # ì‚¬ìš©ì ì„¤ì • ë³‘í•©
    if isinstance(config, dict):
        for key, value in config.items():
            if hasattr(default_config, key):
                setattr(default_config, key, value)
        final_config = default_config
    elif isinstance(config, HumanParsingConfig):
        final_config = config
    else:
        final_config = default_config
    
    # kwargs ì ìš©
    for key, value in kwargs.items():
        if hasattr(final_config, key):
            setattr(final_config, key, value)
    
    # Step ìƒì„± ë° ì´ˆê¸°í™”
    step = HumanParsingStep(device=device_param, config=final_config)
    
    if not await step.initialize():
        logger.warning("âš ï¸ 1ë‹¨ê³„ ì´ˆê¸°í™” ì‹¤íŒ¨ - í”„ë¡œë•ì…˜ ëª¨ë“œì—ì„œëŠ” ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    
    return step

def create_human_parsing_step_sync(
    device: str = "auto",
    config: Optional[Union[Dict[str, Any], HumanParsingConfig]] = None,
    **kwargs
) -> HumanParsingStep:
    """ë™ê¸°ì‹ Step 01 ìƒì„± (ë ˆê±°ì‹œ í˜¸í™˜)"""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    return loop.run_until_complete(
        create_human_parsing_step(device, config, **kwargs)
    )

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ Export
# ==============================================

__all__ = [
    'HumanParsingStep',
    'HumanParsingConfig',
    'create_human_parsing_step',
    'create_human_parsing_step_sync',
    'BODY_PARTS',
    'CLOTHING_CATEGORIES'
]

# ëª¨ë“ˆ ë¡œë”© í™•ì¸
logger.info("âœ… Step 01 Human Parsing ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - ì‹¤ì œ AI ëª¨ë¸ ì—°ë™")