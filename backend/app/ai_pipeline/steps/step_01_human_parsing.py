#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 01: ìˆ˜ì •ëœ ì¸ì²´ íŒŒì‹± (ìµœì¢… í•´ê²° ë²„ì „)
===============================================================================
âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™”
âœ… ë””ë°”ì´ìŠ¤ ì„¤ì • ì˜¤ë¥˜ ì™„ì „ í•´ê²°  
âœ… ModelLoader import ë¬¸ì œ í•´ê²°
âœ… BaseStepMixin ìƒì† ë¬¸ì œ í•´ê²°
âœ… ì˜ì¡´ì„± ì£¼ì… êµ¬ì¡° ì™„ì „ ìˆ˜ì •
âœ… M3 Max ìµœì í™” ìœ ì§€
âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì™„ë²½ êµ¬í˜„

Author: MyCloset AI Team
Date: 2025-07-24
Version: 7.2 (Final Fix)
"""

import os
import sys
import logging
import time
import asyncio
import threading
import json
import gc
import hashlib
import base64
import traceback
import weakref
import uuid
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field, asdict
from enum import Enum, IntEnum
from functools import lru_cache, wraps
from contextlib import contextmanager
from io import BytesIO
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, Type

# ==============================================
# ğŸ”¥ ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ (conda ìš°ì„ )
# ==============================================

import numpy as np

# PyTorch ì„í¬íŠ¸
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
except ImportError:
    raise ImportError("âŒ PyTorch í•„ìˆ˜: conda install pytorch torchvision")

# PIL ì„í¬íŠ¸ (conda ìš°ì„ )
try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError:
    raise ImportError("âŒ Pillow í•„ìˆ˜: conda install pillow")

# OpenCV í´ë°±
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

# ==============================================
# ğŸ”¥ ë¡œê±° ì„¤ì •
# ==============================================

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ conda í™˜ê²½ ë° ë””ë°”ì´ìŠ¤ ìµœì í™” í•¨ìˆ˜ë“¤
# ==============================================

def detect_conda_environment() -> Dict[str, str]:
    """conda í™˜ê²½ ì •ë³´ ìˆ˜ì§‘"""
    return {
        'is_conda': 'CONDA_DEFAULT_ENV' in os.environ,
        'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
        'conda_prefix': os.environ.get('CONDA_PREFIX', 'none')
    }

def detect_optimal_device_fixed() -> str:
    """ìˆ˜ì •ëœ ë””ë°”ì´ìŠ¤ ê°ì§€ (conda ìš°ì„ )"""
    try:
        conda_info = detect_conda_environment()
        
        # conda í™˜ê²½ì—ì„œëŠ” ì•ˆì •ì„± ìš°ì„ ìœ¼ë¡œ cpu ì‹œì‘
        if conda_info['is_conda']:
            logger.info(f"ğŸ conda í™˜ê²½ ê°ì§€: {conda_info['conda_env']}")
            
            # M3 Max + conda í™˜ê²½ì—ì„œëŠ” mps ì‚¬ìš©
            if torch.backends.mps.is_available():
                import platform
                import subprocess
                try:
                    if platform.system() == 'Darwin':
                        result = subprocess.run(
                            ['sysctl', '-n', 'machdep.cpu.brand_string'],
                            capture_output=True, text=True, timeout=5
                        )
                        if 'M3' in result.stdout:
                            logger.info("ğŸ M3 Max + conda í™˜ê²½: mps ì‚¬ìš©")
                            return "mps"
                except:
                    pass
            
            # conda í™˜ê²½ì—ì„œ CUDA ì‚¬ìš© ê°€ëŠ¥í•˜ë©´
            if torch.cuda.is_available():
                logger.info("âš¡ conda í™˜ê²½: cuda ì‚¬ìš©")
                return "cuda"
            
            # conda í™˜ê²½ ê¸°ë³¸ê°’
            logger.info("ğŸ conda í™˜ê²½: cpu ì‚¬ìš© (ì•ˆì •ì„± ìš°ì„ )")
            return "cpu"
        
        # ì¼ë°˜ í™˜ê²½
        if torch.backends.mps.is_available():
            return "mps"
        elif torch.cuda.is_available():
            return "cuda"
        else:
            return "cpu"
            
    except Exception as e:
        logger.warning(f"âš ï¸ ë””ë°”ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨: {e}")
        return "cpu"

def get_memory_info() -> float:
    """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
    try:
        import psutil
        memory_gb = psutil.virtual_memory().total / (1024**3)
        return round(memory_gb, 1)
    except:
        return 16.0

def safe_mps_empty_cache():
    """M3 Max MPS ìºì‹œ ì•ˆì „ ì •ë¦¬"""
    try:
        gc.collect()
        if torch.backends.mps.is_available():
            try:
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                return {"success": True, "method": "mps_cache_cleared"}
            except Exception as e:
                return {"success": True, "method": "gc_only", "mps_error": str(e)}
        return {"success": True, "method": "gc_only"}
    except Exception as e:
        return {"success": False, "error": str(e)}

# ==============================================
# ğŸ”¥ ë™ì  Import í•¨ìˆ˜ë“¤ (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def get_model_loader():
    """ModelLoader ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        
        # ì—¬ëŸ¬ ê²½ë¡œë¡œ ì‹œë„
        for module_path in [
            'app.ai_pipeline.utils.model_loader',
            '.utils.model_loader',
            'app.ai_pipeline.utils.checkpoint_model_loader'
        ]:
            try:
                module = importlib.import_module(module_path)
                get_global_loader = getattr(module, 'get_global_model_loader', None)
                if get_global_loader and callable(get_global_loader):
                    return get_global_loader()
                
                ModelLoader = getattr(module, 'ModelLoader', None)
                if ModelLoader:
                    return ModelLoader()
            except ImportError:
                continue
        
        logger.warning("âš ï¸ ModelLoaderë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None
        
    except Exception as e:
        logger.error(f"âŒ ModelLoader ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        
        # ì—¬ëŸ¬ ê²½ë¡œë¡œ ì‹œë„
        for module_path in [
            'app.ai_pipeline.steps.base_step_mixin',
            '.base_step_mixin',
            'app.ai_pipeline.steps.step_mixins'
        ]:
            try:
                module = importlib.import_module(module_path)
                BaseStepMixin = getattr(module, 'BaseStepMixin', None)
                if BaseStepMixin:
                    return BaseStepMixin
            except ImportError:
                continue
        
        logger.warning("âš ï¸ BaseStepMixinì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None
        
    except Exception as e:
        logger.error(f"âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ ìƒìˆ˜ ë° ë°ì´í„° êµ¬ì¡°
# ==============================================

# ì¸ì²´ ë¶€ìœ„ ì •ì˜ (20ê°œ í´ë˜ìŠ¤)
BODY_PARTS = {
    0: 'background',    1: 'hat',          2: 'hair', 
    3: 'glove',         4: 'sunglasses',   5: 'upper_clothes',
    6: 'dress',         7: 'coat',         8: 'socks',
    9: 'pants',         10: 'torso_skin',  11: 'scarf',
    12: 'skirt',        13: 'face',        14: 'left_arm',
    15: 'right_arm',    16: 'left_leg',    17: 'right_leg',
    18: 'left_shoe',    19: 'right_shoe'
}

# ì‹œê°í™” ìƒ‰ìƒ
VISUALIZATION_COLORS = {
    0: (0, 0, 0),           # Background
    1: (255, 0, 0),         # Hat
    2: (255, 165, 0),       # Hair
    3: (255, 255, 0),       # Glove
    4: (0, 255, 0),         # Sunglasses
    5: (0, 255, 255),       # Upper-clothes
    6: (0, 0, 255),         # Dress
    7: (255, 0, 255),       # Coat
    8: (128, 0, 128),       # Socks
    9: (255, 192, 203),     # Pants
    10: (255, 218, 185),    # Torso-skin
    11: (210, 180, 140),    # Scarf
    12: (255, 20, 147),     # Skirt
    13: (255, 228, 196),    # Face
    14: (255, 160, 122),    # Left-arm
    15: (255, 182, 193),    # Right-arm
    16: (173, 216, 230),    # Left-leg
    17: (144, 238, 144),    # Right-leg
    18: (139, 69, 19),      # Left-shoe
    19: (160, 82, 45)       # Right-shoe
}

# ì˜ë¥˜ ì¹´í…Œê³ ë¦¬
CLOTHING_CATEGORIES = {
    'upper_body': [5, 6, 7, 11],     # ìƒì˜, ë“œë ˆìŠ¤, ì½”íŠ¸, ìŠ¤ì¹´í”„
    'lower_body': [9, 12],           # ë°”ì§€, ìŠ¤ì»¤íŠ¸
    'accessories': [1, 3, 4],        # ëª¨ì, ì¥ê°‘, ì„ ê¸€ë¼ìŠ¤
    'footwear': [8, 18, 19],         # ì–‘ë§, ì‹ ë°œ
    'skin': [10, 13, 14, 15, 16, 17] # í”¼ë¶€ ë¶€ìœ„
}

# ==============================================
# ğŸ”¥ AI ëª¨ë¸ í´ë˜ìŠ¤ (ê°„ì†Œí™”)
# ==============================================

class SimpleHumanParsingModel(nn.Module):
    """ê°„ì†Œí™”ëœ ì¸ì²´ íŒŒì‹± ëª¨ë¸"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        self.num_classes = num_classes
        
        # ê°„ë‹¨í•œ ë°±ë³¸
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 64, 7, 2, 3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, 2, 1),
            
            nn.Conv2d(64, 128, 3, 2, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 256, 3, 2, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
        )
        
        # ë¶„ë¥˜ê¸°
        self.classifier = nn.Conv2d(256, num_classes, 1)
        
        # ì—…ìƒ˜í”Œë§
        self.upsample = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)
    
    def forward(self, x):
        features = self.backbone(x)
        out = self.classifier(features)
        out = self.upsample(out)
        return out

# ==============================================
# ğŸ”¥ ìˆ˜ì •ëœ HumanParsingStep í´ë˜ìŠ¤
# ==============================================

class HumanParsingStep:
    """
    ğŸ”¥ Step 01: ìˆ˜ì •ëœ ì™„ì „í•œ ì¸ì²´ íŒŒì‹± ì‹œìŠ¤í…œ
    
    âœ… conda í™˜ê²½ ìš°ì„  ìµœì í™”
    âœ… ë””ë°”ì´ìŠ¤ ì„¤ì • ì˜¤ë¥˜ í•´ê²°
    âœ… ModelLoader import ë¬¸ì œ í•´ê²°
    âœ… BaseStepMixin ìƒì† ë¬¸ì œ í•´ê²°
    âœ… ì˜ì¡´ì„± ì£¼ì… êµ¬ì¡° ìˆ˜ì •
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        strict_mode: bool = True,
        **kwargs
    ):
        """ìˆ˜ì •ëœ ìƒì„±ì"""
        
        # ê¸°ë³¸ ì†ì„±
        self.step_name = "HumanParsingStep"
        self.step_number = 1
        self.step_id = 1
        self.strict_mode = strict_mode
        self.is_initialized = False
        
        # ë¡œê±° ì„¤ì •
        self.logger = logging.getLogger(f"{__name__}.{self.step_name}")
        
        # conda í™˜ê²½ ì •ë³´
        self.conda_info = detect_conda_environment()
        
        # ìˆ˜ì •ëœ ì‹œìŠ¤í…œ ì„¤ì •
        self._setup_system_config_fixed(device, config, **kwargs)
        
        # BaseStepMixin ìƒì† ì‹œë„
        self._try_inherit_base_step_mixin(device, config, **kwargs)
        
        # ì¸ì²´ íŒŒì‹± ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._initialize_human_parsing_system()
        
        # ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ
        self.dependencies_injected = {
            'model_loader': False,
            'memory_manager': False,
            'data_converter': False
        }
        
        # ìë™ ì˜ì¡´ì„± ì£¼ì…
        self._auto_inject_dependencies()
        
        self.logger.info(f"âœ… {self.step_name} ìˆ˜ì •ëœ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ conda í™˜ê²½: {self.conda_info['conda_env']}")
        self.logger.info(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {self.device}")
        self.logger.info(f"ğŸ M3 Max: {self.is_m3_max}")
        self.logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬: {self.memory_gb:.1f}GB")
    
    def _setup_system_config_fixed(self, device, config, **kwargs):
        """ìˆ˜ì •ëœ ì‹œìŠ¤í…œ ì„¤ì •"""
        try:
            # ìˆ˜ì •ëœ ë””ë°”ì´ìŠ¤ ì„¤ì •
            if device is None or device == "auto":
                self.device = detect_optimal_device_fixed()
            else:
                self.device = device
            
            # M3 Max ê°ì§€ (ë””ë°”ì´ìŠ¤ì™€ ë³„ë„)
            self.is_m3_max = self._detect_m3_max()
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            self.memory_gb = get_memory_info()
            
            # ì„¤ì • í†µí•©
            self.config = config or {}
            self.config.update(kwargs)
            
            # conda í™˜ê²½ ìµœì í™” ì„¤ì •
            if self.conda_info['is_conda']:
                self.optimization_level = 'conda_optimized'
                self.config['conda_optimized'] = True
            elif self.is_m3_max:
                self.optimization_level = 'maximum'
            else:
                self.optimization_level = 'basic'
            
            self.logger.info(f"ğŸ”§ ìˆ˜ì •ëœ ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")
            self.logger.info(f"   ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {self.device}")
            self.logger.info(f"   ğŸ M3 Max: {self.is_m3_max}")
            self.logger.info(f"   ğŸ’¾ ë©”ëª¨ë¦¬: {self.memory_gb:.1f}GB")
            self.logger.info(f"   ğŸ”§ ìµœì í™”: {self.optimization_level}")
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ ì„¤ì • ì‹¤íŒ¨: {e}")
            # í´ë°±
            self.device = "cpu"
            self.is_m3_max = False
            self.memory_gb = 16.0
            self.optimization_level = 'basic'
    
    def _detect_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def _try_inherit_base_step_mixin(self, device, config, **kwargs):
        """BaseStepMixin ìƒì† ì‹œë„"""
        try:
            BaseStepMixinClass = get_base_step_mixin_class()
            
            if BaseStepMixinClass:
                # BaseStepMixin ë©”ì„œë“œë¥¼ ì§ì ‘ í˜¸ì¶œ
                try:
                    BaseStepMixinClass.__init__(self, device=device, config=config, **kwargs)
                    self.logger.info("âœ… BaseStepMixin ìƒì† ì„±ê³µ")
                    self.basestep_mixin_inherited = True
                except Exception as init_error:
                    self.logger.warning(f"âš ï¸ BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨: {init_error}")
                    self._manual_base_step_init()
            else:
                self.logger.warning("âš ï¸ BaseStepMixin í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ìˆ˜ë™ ì´ˆê¸°í™”")
                self._manual_base_step_init()
                
        except Exception as e:
            self.logger.error(f"âŒ BaseStepMixin ìƒì† ì‹¤íŒ¨: {e}")
            self._manual_base_step_init()
    
    def _manual_base_step_init(self):
        """ìˆ˜ë™ BaseStepMixin ì´ˆê¸°í™”"""
        try:
            # BaseStepMixin í•„ìˆ˜ ì†ì„±ë“¤
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            self.is_ready = False
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            self.performance_metrics = {
                'process_count': 0,
                'total_process_time': 0.0,
                'average_process_time': 0.0
            }
            
            # ì—ëŸ¬ ì¶”ì 
            self.error_count = 0
            self.last_error = None
            self.total_processing_count = 0
            
            # ì˜ì¡´ì„± ê´€ë ¨
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            
            self.basestep_mixin_inherited = False
            self.logger.info("âœ… ìˆ˜ë™ BaseStepMixin ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ìˆ˜ë™ BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _initialize_human_parsing_system(self):
        """ì¸ì²´ íŒŒì‹± ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # íŒŒì‹± ì„¤ì •
            self.parsing_config = {
                'model_priority': ['simple_human_parsing', 'graphonomy', 'u2net'],
                'confidence_threshold': 0.5,
                'visualization_enabled': True,
                'cache_enabled': True
            }
            
            # AI ëª¨ë¸ ì €ì¥ì†Œ
            self.parsing_models = {}
            self.active_model = None
            
            # ìºì‹œ ì‹œìŠ¤í…œ
            self.prediction_cache = {}
            self.cache_max_size = 50
            
            self.logger.info("âœ… ì¸ì²´ íŒŒì‹± ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì¸ì²´ íŒŒì‹± ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _auto_inject_dependencies(self):
        """ìë™ ì˜ì¡´ì„± ì£¼ì…"""
        try:
            injection_count = 0
            
            # ModelLoader ì£¼ì…
            if not hasattr(self, 'model_loader') or not self.model_loader:
                model_loader = get_model_loader()
                if model_loader:
                    self.set_model_loader(model_loader)
                    injection_count += 1
            
            if injection_count > 0:
                self.logger.info(f"âœ… ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {injection_count}ê°œ")
                
        except Exception as e:
            self.logger.debug(f"ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.model_loader = model_loader
            self.dependencies_injected['model_loader'] = True
            
            # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            if hasattr(model_loader, 'create_step_interface'):
                try:
                    self.model_interface = model_loader.create_step_interface(self.step_name)
                except Exception:
                    self.model_interface = model_loader
            else:
                self.model_interface = model_loader
            
            self.has_model = True
            self.model_loaded = True
            
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.memory_manager = memory_manager
            self.dependencies_injected['memory_manager'] = True
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.data_converter = data_converter
            self.dependencies_injected['data_converter'] = True
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ì´ˆê¸°í™” ë©”ì„œë“œë“¤
    # ==============================================
    
    async def initialize_step(self) -> bool:
        """Step ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"ğŸš€ {self.step_name} ì´ˆê¸°í™” ì‹œì‘")
            
            # AI ëª¨ë¸ ë¡œë“œ
            success = await self._load_ai_models()
            
            if success:
                self.is_initialized = True
                self.is_ready = True
                self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì„±ê³µ")
                return True
            else:
                self.logger.warning(f"âš ï¸ {self.step_name} ì´ˆê¸°í™” ë¶€ë¶„ ì‹¤íŒ¨ - ê³„ì† ì§„í–‰")
                self.is_initialized = True  # ë¶€ë¶„ ì‹¤íŒ¨ë¼ë„ ì‚¬ìš© ê°€ëŠ¥
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _load_ai_models(self) -> bool:
        """AI ëª¨ë¸ ë¡œë“œ"""
        try:
            # ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±
            simple_model = SimpleHumanParsingModel()
            simple_model.to(self.device)
            simple_model.eval()
            
            self.parsing_models['simple_human_parsing'] = simple_model
            self.active_model = 'simple_human_parsing'
            
            self.logger.info("âœ… AI ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ
    # ==============================================
    
    async def process(
        self, 
        person_image_tensor: torch.Tensor,
        **kwargs
    ) -> Dict[str, Any]:
        """ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ"""
        try:
            # ì´ˆê¸°í™” í™•ì¸
            if not self.is_initialized:
                await self.initialize_step()
            
            start_time = time.time()
            self.logger.info(f"ğŸ§  {self.step_name} ì²˜ë¦¬ ì‹œì‘")
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            processed_image = self._preprocess_image(person_image_tensor)
            if processed_image is None:
                return self._create_error_result("ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨")
            
            # AI ëª¨ë¸ ì¶”ë¡ 
            parsing_result = await self._process_with_ai_model(processed_image)
            
            if not parsing_result or not parsing_result.get('success', False):
                return self._create_error_result("AI ì¸ì²´ íŒŒì‹± ì‹¤íŒ¨")
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            final_result = self._postprocess_result(parsing_result, processed_image, start_time)
            
            processing_time = time.time() - start_time
            self.logger.info(f"âœ… {self.step_name} ì²˜ë¦¬ ì„±ê³µ ({processing_time:.2f}ì´ˆ)")
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_error_result(str(e))
    
    async def _process_with_ai_model(self, image: Image.Image) -> Dict[str, Any]:
        """AI ëª¨ë¸ë¡œ ì²˜ë¦¬"""
        try:
            if not self.active_model or self.active_model not in self.parsing_models:
                return {'success': False, 'error': 'AI ëª¨ë¸ ì—†ìŒ'}
            
            ai_model = self.parsing_models[self.active_model]
            
            # ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
            input_tensor = self._prepare_model_input(image)
            if input_tensor is None:
                return {'success': False, 'error': 'ëª¨ë¸ ì…ë ¥ ì¤€ë¹„ ì‹¤íŒ¨'}
            
            # AI ì¶”ë¡ 
            with torch.no_grad():
                if self.device == "mps":
                    # M3 Max ìµœì í™”
                    output = ai_model(input_tensor)
                else:
                    output = ai_model(input_tensor)
            
            # ì¶œë ¥ í•´ì„
            parsing_map = self._interpret_model_output(output, image.size)
            
            return {
                'success': True,
                'parsing_map': parsing_map,
                'model_used': self.active_model,
                'device': self.device
            }
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _preprocess_image(self, image: Union[torch.Tensor, Image.Image, np.ndarray]) -> Optional[Image.Image]:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            if torch.is_tensor(image):
                # í…ì„œ ì²˜ë¦¬
                if image.dim() == 4:
                    image = image.squeeze(0)
                if image.dim() == 3:
                    image = image.permute(1, 2, 0)
                
                image_np = image.cpu().numpy()
                if image_np.max() <= 1.0:
                    image_np = (image_np * 255).astype(np.uint8)
                image = Image.fromarray(image_np)
                
            elif isinstance(image, np.ndarray):
                if image.max() <= 1.0:
                    image = (image * 255).astype(np.uint8)
                image = Image.fromarray(image)
            
            # RGB ë³€í™˜
            if hasattr(image, 'mode') and image.mode != 'RGB':
                image = image.convert('RGB')
            
            # í¬ê¸° ì¡°ì •
            if hasattr(image, 'size'):
                max_size = 512
                if max(image.size) > max_size:
                    ratio = max_size / max(image.size)
                    new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                    image = image.resize(new_size, Image.Resampling.LANCZOS)
            
            return image
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _prepare_model_input(self, image: Image.Image) -> Optional[torch.Tensor]:
        """ëª¨ë¸ ì…ë ¥ ì¤€ë¹„"""
        try:
            # PIL -> numpy
            image_np = np.array(image)
            
            # í¬ê¸° ì¡°ì • (512x512)
            if CV2_AVAILABLE:
                image_resized = cv2.resize(image_np, (512, 512))
            else:
                image_resized = np.array(image.resize((512, 512)))
            
            # í…ì„œ ë³€í™˜
            image_tensor = torch.from_numpy(image_resized).float()
            image_tensor = image_tensor.permute(2, 0, 1).unsqueeze(0)  # BHWC -> BCHW
            image_tensor = image_tensor / 255.0  # ì •ê·œí™”
            image_tensor = image_tensor.to(self.device)
            
            return image_tensor
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì…ë ¥ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return None
    
    def _interpret_model_output(self, output: torch.Tensor, image_size: Tuple[int, int]) -> np.ndarray:
        """ëª¨ë¸ ì¶œë ¥ í•´ì„"""
        try:
            # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
            if output.device.type == 'mps':
                output_np = output.detach().cpu().numpy()
            else:
                output_np = output.detach().cpu().numpy()
            
            # ì°¨ì› ì²˜ë¦¬
            if len(output_np.shape) == 4:  # [B, C, H, W]
                output_np = output_np[0]  # ì²« ë²ˆì§¸ ë°°ì¹˜
            
            if len(output_np.shape) == 3:  # [C, H, W]
                # í´ë˜ìŠ¤ë³„ í™•ë¥ ì—ì„œ ìµœì¢… íŒŒì‹± ë§µ ìƒì„±
                parsing_map = np.argmax(output_np, axis=0).astype(np.uint8)
            else:
                # 2Dì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                parsing_map = output_np.astype(np.uint8)
            
            # ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ì¡°ì •
            if parsing_map.shape != image_size[::-1]:
                if CV2_AVAILABLE:
                    parsing_map = cv2.resize(parsing_map, image_size, interpolation=cv2.INTER_NEAREST)
                else:
                    pil_img = Image.fromarray(parsing_map)
                    resized = pil_img.resize(image_size, Image.Resampling.NEAREST)
                    parsing_map = np.array(resized)
            
            return parsing_map
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¶œë ¥ í•´ì„ ì‹¤íŒ¨: {e}")
            return np.zeros(image_size[::-1], dtype=np.uint8)
    
    def _postprocess_result(self, parsing_result: Dict[str, Any], image: Image.Image, start_time: float) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            processing_time = time.time() - start_time
            parsing_map = parsing_result.get('parsing_map', np.zeros((512, 512), dtype=np.uint8))
            
            # ë¶„ì„ ìˆ˜í–‰
            analysis = self._analyze_parsing_quality(parsing_map)
            
            # ì‹œê°í™” ìƒì„±
            visualization = None
            if self.parsing_config['visualization_enabled']:
                visualization = self._create_visualization(image, parsing_map)
            
            # ìµœì¢… ê²°ê³¼
            result = {
                'success': True,
                'parsing_map': parsing_map,
                'parsing_analysis': analysis,
                'visualization': visualization,
                'processing_time': processing_time,
                'model_used': parsing_result.get('model_used', 'unknown'),
                'device': self.device,
                'step_info': {
                    'step_name': self.step_name,
                    'step_number': self.step_number,
                    'conda_optimized': self.conda_info['is_conda'],
                    'device': self.device,
                    'basestep_mixin_inherited': getattr(self, 'basestep_mixin_inherited', False)
                },
                'detected_parts': analysis.get('detected_parts', {}),
                'body_masks': analysis.get('body_masks', {}),
                'body_parts_detected': analysis.get('detected_parts', {})
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_error_result(str(e))
    
    def _analyze_parsing_quality(self, parsing_map: np.ndarray) -> Dict[str, Any]:
        """íŒŒì‹± í’ˆì§ˆ ë¶„ì„"""
        try:
            detected_parts = {}
            body_masks = {}
            
            # ê° ë¶€ìœ„ë³„ ë¶„ì„
            for part_id, part_name in BODY_PARTS.items():
                if part_id == 0:  # ë°°ê²½ ì œì™¸
                    continue
                
                mask = (parsing_map == part_id)
                pixel_count = mask.sum()
                
                if pixel_count > 0:
                    detected_parts[part_name] = {
                        "pixel_count": int(pixel_count),
                        "percentage": float(pixel_count / parsing_map.size * 100),
                        "part_id": part_id
                    }
                    body_masks[part_name] = mask.astype(np.uint8)
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = min(1.0, len(detected_parts) / 10.0)  # 10ê°œ ë¶€ìœ„ ê¸°ì¤€
            
            return {
                'suitable_for_parsing': quality_score >= 0.3,
                'quality_score': quality_score,
                'detected_parts': detected_parts,
                'body_masks': body_masks,
                'total_parts_detected': len(detected_parts),
                'conda_optimized_analysis': self.conda_info['is_conda']
            }
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì‹± í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'suitable_for_parsing': False,
                'quality_score': 0.0,
                'detected_parts': {},
                'body_masks': {}
            }
    
    def _create_visualization(self, image: Image.Image, parsing_map: np.ndarray) -> Optional[Dict[str, str]]:
        """ì‹œê°í™” ìƒì„±"""
        try:
            # ì»¬ëŸ¬ íŒŒì‹± ë§µ ìƒì„±
            colored_parsing = self._create_colored_parsing_map(parsing_map)
            
            # Base64 ì¸ì½”ë”©
            if colored_parsing:
                buffer = BytesIO()
                colored_parsing.save(buffer, format='JPEG', quality=95)
                colored_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
                
                return {
                    'colored_parsing': colored_base64
                }
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _create_colored_parsing_map(self, parsing_map: np.ndarray) -> Optional[Image.Image]:
        """ì»¬ëŸ¬ íŒŒì‹± ë§µ ìƒì„±"""
        try:
            height, width = parsing_map.shape
            colored_image = np.zeros((height, width, 3), dtype=np.uint8)
            
            # ê° ë¶€ìœ„ë³„ë¡œ ìƒ‰ìƒ ì ìš©
            for part_id, color in VISUALIZATION_COLORS.items():
                mask = (parsing_map == part_id)
                colored_image[mask] = color
            
            return Image.fromarray(colored_image)
            
        except Exception as e:
            self.logger.error(f"âŒ ì»¬ëŸ¬ íŒŒì‹± ë§µ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'parsing_map': np.zeros((512, 512), dtype=np.uint8),
            'parsing_analysis': {
                'suitable_for_parsing': False,
                'quality_score': 0.0,
                'detected_parts': {}
            },
            'step_info': {
                'step_name': self.step_name,
                'step_number': self.step_number,
                'conda_optimized': self.conda_info['is_conda'],
                'device': self.device
            }
        }
    
    # ==============================================
    # BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤
    # ==============================================
    
    def initialize(self) -> bool:
        """ë™ê¸° ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            self.is_initialized = True
            self.logger.info(f"âœ… {self.step_name} ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def initialize_async(self) -> bool:
        """ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        return await self.initialize_step()
    
    def get_status(self) -> Dict[str, Any]:
        """ìƒíƒœ ì¡°íšŒ"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'is_initialized': self.is_initialized,
            'is_ready': getattr(self, 'is_ready', False),
            'device': self.device,
            'conda_env': self.conda_info['conda_env'],
            'conda_optimized': self.conda_info['is_conda'],
            'dependencies_injected': self.dependencies_injected,
            'basestep_mixin_inherited': getattr(self, 'basestep_mixin_inherited', False)
        }
    
    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'parsing_models'):
                for model in self.parsing_models.values():
                    if hasattr(model, 'cpu'):
                        model.cpu()
                self.parsing_models.clear()
            
            # ìºì‹œ ì •ë¦¬
            if hasattr(self, 'prediction_cache'):
                self.prediction_cache.clear()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            safe_mps_empty_cache()
            
            self.logger.info("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ==============================================
# ìƒì„± í•¨ìˆ˜ë“¤
# ==============================================

async def create_human_parsing_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    strict_mode: bool = True,
    **kwargs
) -> HumanParsingStep:
    """ìˆ˜ì •ëœ ì¸ì²´ íŒŒì‹± Step ìƒì„±"""
    try:
        device_param = None if device == "auto" else device
        
        if config is None:
            config = {}
        config.update(kwargs)
        
        step = HumanParsingStep(device=device_param, config=config, strict_mode=strict_mode)
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        await step.initialize_step()
        
        return step
        
    except Exception as e:
        logger.error(f"âŒ create_human_parsing_step ì‹¤íŒ¨: {e}")
        if strict_mode:
            raise
        else:
            step = HumanParsingStep(device='cpu', strict_mode=False)
            return step

def create_human_parsing_step_sync(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    strict_mode: bool = True,
    **kwargs
) -> HumanParsingStep:
    """ë™ê¸°ì‹ ì¸ì²´ íŒŒì‹± Step ìƒì„±"""
    try:
        import asyncio
        return asyncio.run(create_human_parsing_step(device, config, strict_mode, **kwargs))
    except Exception as e:
        logger.error(f"âŒ create_human_parsing_step_sync ì‹¤íŒ¨: {e}")
        if strict_mode:
            raise
        else:
            return HumanParsingStep(device='cpu', strict_mode=False)

# ==============================================
# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# ==============================================

async def test_fixed_human_parsing():
    """ìˆ˜ì •ëœ ì¸ì²´ íŒŒì‹± í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ ìˆ˜ì •ëœ ì¸ì²´ íŒŒì‹± ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # Step ìƒì„±
        step = await create_human_parsing_step(device="auto", strict_mode=False)
        
        print(f"âœ… Step ìƒì„± ì„±ê³µ")
        print(f"ğŸ conda í™˜ê²½: {step.conda_info['conda_env']}")
        print(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {step.device}")
        print(f"ğŸ M3 Max: {step.is_m3_max}")
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {step.memory_gb:.1f}GB")
        
        # ìƒíƒœ í™•ì¸
        status = step.get_status()
        print(f"ğŸ“Š ì´ˆê¸°í™”ë¨: {status['is_initialized']}")
        print(f"ğŸ”— ì˜ì¡´ì„±: {status['dependencies_injected']}")
        
        # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
        dummy_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
        dummy_tensor = torch.from_numpy(dummy_image).float().permute(2, 0, 1).unsqueeze(0)
        
        # ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        result = await step.process(dummy_tensor)
        
        if result['success']:
            print(f"âœ… ì²˜ë¦¬ ì„±ê³µ")
            print(f"ğŸ¯ ê°ì§€ ë¶€ìœ„: {len(result['detected_parts'])}ê°œ")
            print(f"ğŸ’ í’ˆì§ˆ ì ìˆ˜: {result['parsing_analysis']['quality_score']:.3f}")
            print(f"âš¡ ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
        else:
            print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
        
        # ì •ë¦¬
        step.cleanup_resources()
        print("ğŸ§¹ ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    'HumanParsingStep',
    'SimpleHumanParsingModel',
    'create_human_parsing_step',
    'create_human_parsing_step_sync',
    'test_fixed_human_parsing',
    'BODY_PARTS',
    'VISUALIZATION_COLORS',
    'CLOTHING_CATEGORIES'
]

# ==============================================
# ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê·¸
# ==============================================

logger.info("=" * 80)
logger.info("ğŸ”¥ ìˆ˜ì •ëœ ì™„ì „í•œ HumanParsingStep v7.2 ë¡œë“œ ì™„ë£Œ")
logger.info("=" * 80)
logger.info("âœ… ìˆ˜ì •ì‚¬í•­:")
logger.info("   ğŸ conda í™˜ê²½ ìš°ì„  ìµœì í™”")
logger.info("   ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤ ì„¤ì • ì˜¤ë¥˜ í•´ê²°")
logger.info("   ğŸ“¦ ModelLoader import ë¬¸ì œ í•´ê²°")
logger.info("   ğŸ”— BaseStepMixin ìƒì† ë¬¸ì œ í•´ê²°")
logger.info("   ğŸ’‰ ì˜ì¡´ì„± ì£¼ì… êµ¬ì¡° ìˆ˜ì •")
logger.info("   ğŸ M3 Max ìµœì í™” ìœ ì§€")
logger.info("   ğŸ¤– ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  êµ¬í˜„")
logger.info("=" * 80)

# conda í™˜ê²½ ì •ë³´ ë¡œê¹…
conda_info = detect_conda_environment()
if conda_info['is_conda']:
    logger.info(f"ğŸ conda í™˜ê²½ ê°ì§€: {conda_info['conda_env']}")
    logger.info(f"ğŸ“ conda ê²½ë¡œ: {conda_info['conda_prefix']}")
    logger.info(f"ğŸ”§ ìµœì í™” í™œì„±í™”")
else:
    logger.info("âš ï¸ conda í™˜ê²½ì´ ì•„ë‹˜ - ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")

logger.info("=" * 80)

# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    print("ğŸ”¥ ìˆ˜ì •ëœ HumanParsingStep v7.2 í…ŒìŠ¤íŠ¸")
    asyncio.run(test_fixed_human_parsing())