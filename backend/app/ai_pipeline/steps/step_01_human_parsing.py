#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 01: Human Parsing v8.0 - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
========================================================================================

âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - ì¤‘ì•™ í—ˆë¸Œ íŒ¨í„´ ì ìš©
âœ… BaseStepMixin v20.0 ì™„ì „ ìƒì† - super().__init__() í˜¸ì¶œ
âœ… í•„ìˆ˜ ì†ì„± ì´ˆê¸°í™” - ai_models, models_loading_status, model_interface, loaded_models
âœ… _load_ai_models_via_central_hub() êµ¬í˜„ - ModelLoaderë¥¼ í†µí•œ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©
âœ… ê°„ì†Œí™”ëœ process() ë©”ì„œë“œ - í•µì‹¬ Human Parsing ë¡œì§ë§Œ
âœ… ì—ëŸ¬ ë°©ì§€ìš© í´ë°± ë¡œì§ - Mock ëª¨ë¸ ìƒì„± (ì‹¤ì œ AI ëª¨ë¸ ëŒ€ì²´ìš©)
âœ… GitHubDependencyManager ì™„ì „ ì‚­ì œ - ë³µì¡í•œ ì˜ì¡´ì„± ê´€ë¦¬ ì½”ë“œ ì œê±°
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - TYPE_CHECKING + ì§€ì—° import
âœ… Graphonomy ëª¨ë¸ ë¡œë”© - 1.2GB ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ì§€ì›
âœ… Human body parsing - 20ê°œ í´ë˜ìŠ¤ ì •í™• ë¶„ë¥˜
âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ - ì™„ì „ êµ¬í˜„

í•µì‹¬ êµ¬í˜„ ê¸°ëŠ¥:
1. Graphonomy ResNet-101 + ASPP ì•„í‚¤í…ì²˜ (ì‹¤ì œ 1.2GB ì²´í¬í¬ì¸íŠ¸)
2. U2Net í´ë°± ëª¨ë¸ (ê²½ëŸ‰í™” ëŒ€ì•ˆ)
3. 20ê°œ ì¸ì²´ ë¶€ìœ„ ì •í™• íŒŒì‹± (ë°°ê²½ í¬í•¨)
4. 512x512 ì…ë ¥ í¬ê¸° í‘œì¤€í™”
5. MPS/CUDA ë””ë°”ì´ìŠ¤ ìµœì í™”

Author: MyCloset AI Team
Date: 2025-07-31
Version: 8.0 (Central Hub DI Container v7.0 Integration)
"""

import os
import sys
import gc
import time
import logging
import threading
import traceback
import warnings

from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor

# ==============================================
# ğŸ”¥ Central Hub DI Container ì•ˆì „ import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

if TYPE_CHECKING:
    from app.core.di_container import CentralHubDIContainer
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ìˆœí™˜ì°¸ì¡° ë°©ì§€) - HumanParsingìš©"""
    try:
        import importlib
        module = importlib.import_module('.base_step_mixin', package='app.ai_pipeline.steps')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError:
        try:
            # í´ë°±: ìƒëŒ€ ê²½ë¡œ
            from .base_step_mixin import BaseStepMixin
            return BaseStepMixin
        except ImportError:
            logging.getLogger(__name__).error("âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨")
            return None

# ğŸ”¥ Central Hub v7.0 - ì¤‘ì•™ ì§‘ì¤‘ì‹ BaseStepMixin ê´€ë¦¬ ì‚¬ìš©
try:
    from . import get_central_base_step_mixin
    BaseStepMixin = get_central_base_step_mixin()
    if BaseStepMixin is None:
        BaseStepMixin = get_base_step_mixin_class()
except ImportError:
    BaseStepMixin = get_base_step_mixin_class()
# NumPy í•„ìˆ˜
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

# PyTorch í•„ìˆ˜ (MPS ì§€ì›)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
    MPS_AVAILABLE = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
except ImportError:
    TORCH_AVAILABLE = False
    MPS_AVAILABLE = False

# PIL í•„ìˆ˜
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# OpenCV ì„ íƒì‚¬í•­
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    
try:
    import scipy
    import scipy.ndimage as ndimage  # í™€ ì±„ìš°ê¸°ì—ì„œ ì‚¬ìš©
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    ndimage = None

# DenseCRF ê³ ê¸‰ í›„ì²˜ë¦¬
try:
    import pydensecrf.densecrf as dcrf
    from pydensecrf.utils import unary_from_softmax
    DENSECRF_AVAILABLE = True
except ImportError:
    DENSECRF_AVAILABLE = False

# Scikit-image ê³ ê¸‰ ì´ë¯¸ì§€ ì²˜ë¦¬
try:
    from skimage import measure, morphology, segmentation, filters
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

# ==============================================
# ğŸ”¥ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
# ==============================================
# BaseStepMixin í´ë°± í´ë˜ìŠ¤ (step_01_human_parsing.pyìš©)
if BaseStepMixin is None:
    import asyncio
    import time
    from typing import Dict, Any, Optional
    
    class BaseStepMixin:
        """HumanParsingStepìš© BaseStepMixin í´ë°± í´ë˜ìŠ¤ - ì™„ì „ êµ¬í˜„"""
        
        def __init__(self, **kwargs):
            # ê¸°ë³¸ ì†ì„±ë“¤
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'HumanParsingStep')
            self.step_id = kwargs.get('step_id', 1)
            self.device = kwargs.get('device', 'cpu')
            
            # AI ëª¨ë¸ ê´€ë ¨ ì†ì„±ë“¤ (HumanParsingStepì´ í•„ìš”ë¡œ í•˜ëŠ”)
            self.ai_models = {}
            self.models_loading_status = {}
            self.model_interface = None
            self.loaded_models = []
            
            # ìƒíƒœ ê´€ë ¨ ì†ì„±ë“¤
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # Central Hub DI Container ê´€ë ¨
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            # ì„±ëŠ¥ í†µê³„
            self.performance_stats = {
                'total_processed': 0,
                'avg_processing_time': 0.0,
                'error_count': 0,
                'success_rate': 1.0
            }
            
            self.logger.info(f"âœ… {self.step_name} BaseStepMixin í´ë°± í´ë˜ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        async def process(self, **kwargs) -> Dict[str, Any]:
            """ê¸°ë³¸ process ë©”ì„œë“œ - _run_ai_inference í˜¸ì¶œ"""
            try:
                start_time = time.time()
                
                # _run_ai_inference ë©”ì„œë“œê°€ ìˆìœ¼ë©´ í˜¸ì¶œ
                if hasattr(self, '_run_ai_inference'):
                    result = self._run_ai_inference(kwargs)
                    
                    # ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
                    if isinstance(result, dict):
                        result['processing_time'] = time.time() - start_time
                        result['step_name'] = self.step_name
                        result['step_id'] = self.step_id
                    
                    return result
                else:
                    # ê¸°ë³¸ ì‘ë‹µ
                    return {
                        'success': False,
                        'error': '_run_ai_inference ë©”ì„œë“œê°€ êµ¬í˜„ë˜ì§€ ì•ŠìŒ',
                        'processing_time': time.time() - start_time,
                        'step_name': self.step_name,
                        'step_id': self.step_id
                    }
                    
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} process ì‹¤íŒ¨: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                    'step_name': self.step_name,
                    'step_id': self.step_id
                }
        
        async def initialize(self) -> bool:
            """ì´ˆê¸°í™” ë©”ì„œë“œ"""
            try:
                self.is_initialized = True
                self.is_ready = True
                self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
                return True
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                return False
        
        def cleanup(self):
            """ì •ë¦¬ ë©”ì„œë“œ"""
            try:
                # AI ëª¨ë¸ë“¤ ì •ë¦¬
                self.ai_models.clear()
                self.loaded_models.clear()
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                import gc
                gc.collect()
                
                self.logger.info(f"âœ… {self.step_name} ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        def get_status(self) -> Dict[str, Any]:
            """ìƒíƒœ ì¡°íšŒ"""
            return {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'device': self.device,
                'models_loaded': len(self.loaded_models),
                'fallback_mode': True
            }
        
        # BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤ ì¶”ê°€
        def set_model_loader(self, model_loader):
            """ModelLoader ì˜ì¡´ì„± ì£¼ì…"""
            self.model_loader = model_loader
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        
        def set_memory_manager(self, memory_manager):
            """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
            self.memory_manager = memory_manager
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        
        def set_data_converter(self, data_converter):
            """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
            self.data_converter = data_converter
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        
        def set_di_container(self, di_container):
            """DI Container ì˜ì¡´ì„± ì£¼ì…"""
            self.di_container = di_container
            self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")



# ==============================================
# ğŸ”¥ í™˜ê²½ ì„¤ì • ë° ìµœì í™”
# ==============================================

# M3 Max ê°ì§€
def detect_m3_max() -> bool:
    try:
        import platform, subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'], 
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()

# M3 Max ìµœì í™” ì„¤ì •
if IS_M3_MAX and TORCH_AVAILABLE and MPS_AVAILABLE:
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
    os.environ['TORCH_MPS_PREFER_METAL'] = '1'

# ==============================================
# ğŸ”¥ ë°ì´í„° êµ¬ì¡° ì •ì˜
# ==============================================

class HumanParsingModel(Enum):
    """ì¸ì²´ íŒŒì‹± ëª¨ë¸ íƒ€ì…"""
    GRAPHONOMY = "graphonomy"
    U2NET = "u2net"
    MOCK = "mock"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë ˆë²¨"""
    FAST = "fast"
    BALANCED = "balanced"
    HIGH = "high"

# 20ê°œ ì¸ì²´ ë¶€ìœ„ ì •ì˜ (Graphonomy í‘œì¤€)
BODY_PARTS = {
    0: 'background',    1: 'hat',          2: 'hair',
    3: 'glove',         4: 'sunglasses',   5: 'upper_clothes',
    6: 'dress',         7: 'coat',         8: 'socks',
    9: 'pants',         10: 'torso_skin',  11: 'scarf',
    12: 'skirt',        13: 'face',        14: 'left_arm',
    15: 'right_arm',    16: 'left_leg',    17: 'right_leg',
    18: 'left_shoe',    19: 'right_shoe'
}

# ì‹œê°í™” ìƒ‰ìƒ (20ê°œ í´ë˜ìŠ¤)
VISUALIZATION_COLORS = {
    0: (0, 0, 0),           # Background
    1: (255, 0, 0),         # Hat
    2: (255, 165, 0),       # Hair
    3: (255, 255, 0),       # Glove
    4: (0, 255, 0),         # Sunglasses
    5: (0, 255, 255),       # Upper-clothes
    6: (0, 0, 255),         # Dress
    7: (255, 0, 255),       # Coat
    8: (128, 0, 128),       # Socks
    9: (255, 192, 203),     # Pants
    10: (255, 218, 185),    # Torso-skin
    11: (210, 180, 140),    # Scarf
    12: (255, 20, 147),     # Skirt
    13: (255, 228, 196),    # Face
    14: (255, 160, 122),    # Left-arm
    15: (255, 182, 193),    # Right-arm
    16: (173, 216, 230),    # Left-leg
    17: (144, 238, 144),    # Right-leg
    18: (139, 69, 19),      # Left-shoe
    19: (160, 82, 45)       # Right-shoe
}

@dataclass
class EnhancedHumanParsingConfig:
    """ê°•í™”ëœ Human Parsing ì„¤ì • (ì›ë³¸ í”„ë¡œì íŠ¸ ì™„ì „ ë°˜ì˜)"""
    method: HumanParsingModel = HumanParsingModel.GRAPHONOMY
    quality_level: QualityLevel = QualityLevel.HIGH
    input_size: Tuple[int, int] = (512, 512)
    
    # ì „ì²˜ë¦¬ ì„¤ì •
    enable_quality_assessment: bool = True
    enable_lighting_normalization: bool = True
    enable_color_correction: bool = True
    enable_roi_detection: bool = True
    enable_background_analysis: bool = True
    
    # ì¸ì²´ ë¶„ë¥˜ ì„¤ì •
    enable_body_classification: bool = True
    classification_confidence_threshold: float = 0.8
    
    # Graphonomy í”„ë¡¬í”„íŠ¸ ì„¤ì •
    enable_advanced_prompts: bool = True
    use_box_prompts: bool = True
    use_mask_prompts: bool = True
    enable_iterative_refinement: bool = True
    max_refinement_iterations: int = 3
    
    # í›„ì²˜ë¦¬ ì„¤ì • (ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜)
    enable_crf_postprocessing: bool = True
    enable_edge_refinement: bool = True
    enable_hole_filling: bool = True
    enable_multiscale_processing: bool = True
    
    # í’ˆì§ˆ ê²€ì¦ ì„¤ì •
    enable_quality_validation: bool = True
    quality_threshold: float = 0.7
    enable_auto_retry: bool = True
    max_retry_attempts: int = 3
    
    # ê¸°ë³¸ ì„¤ì •
    enable_visualization: bool = True
    use_fp16: bool = True
    confidence_threshold: float = 0.7
    remove_noise: bool = True
    overlay_opacity: float = 0.6

# ==============================================
# ğŸ”¥ ê³ ê¸‰ AI ì•„í‚¤í…ì²˜ë“¤ (ì›ë³¸ í”„ë¡œì íŠ¸ ì™„ì „ ë°˜ì˜)
# ==============================================

class ASPPModule(nn.Module):
    """ASPP (Atrous Spatial Pyramid Pooling) - Multi-scale context aggregation"""
    
    def __init__(self, in_channels=2048, out_channels=256, atrous_rates=[6, 12, 18]):
        super().__init__()
        
        # 1x1 convolution
        self.conv1x1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # Atrous convolutions with different rates
        self.atrous_convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 3, padding=rate, 
                         dilation=rate, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            ) for rate in atrous_rates
        ])
        
        # Global average pooling branch
        self.global_avg_pool = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # Feature fusion
        total_channels = out_channels * (1 + len(atrous_rates) + 1)  # 1x1 + atrous + global
        self.project = nn.Sequential(
            nn.Conv2d(total_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5)
        )
    
    def forward(self, x):
        h, w = x.shape[2:]
        
        # 1x1 convolution
        feat1 = self.conv1x1(x)
        
        # Atrous convolutions
        atrous_feats = [conv(x) for conv in self.atrous_convs]
        
        # Global average pooling
        global_feat = self.global_avg_pool(x)
        global_feat = F.interpolate(global_feat, size=(h, w), 
                                   mode='bilinear', align_corners=False)
        
        # Concatenate all features
        concat_feat = torch.cat([feat1] + atrous_feats + [global_feat], dim=1)
        
        # Project to final features
        return self.project(concat_feat)

class SelfAttentionBlock(nn.Module):
    """Self-Attention ë©”ì»¤ë‹ˆì¦˜"""
    
    def __init__(self, in_channels, reduction=8):
        super().__init__()
        self.in_channels = in_channels
        
        # Query, Key, Value ë³€í™˜
        self.query_conv = nn.Conv2d(in_channels, in_channels // reduction, 1)
        self.key_conv = nn.Conv2d(in_channels, in_channels // reduction, 1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)
        
        # Output projection
        self.out_conv = nn.Conv2d(in_channels, in_channels, 1)
        self.softmax = nn.Softmax(dim=-1)
        
        # Learnable parameter
        self.gamma = nn.Parameter(torch.zeros(1))
    
    def forward(self, x):
        batch_size, C, H, W = x.shape
        
        # Generate query, key, value
        proj_query = self.query_conv(x).view(batch_size, -1, H * W).permute(0, 2, 1)
        proj_key = self.key_conv(x).view(batch_size, -1, H * W)
        proj_value = self.value_conv(x).view(batch_size, -1, H * W)
        
        # Attention computation
        attention = torch.bmm(proj_query, proj_key)
        attention = self.softmax(attention)
        
        # Apply attention to values
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, H, W)
        
        # Residual connection with learnable weight
        out = self.gamma * self.out_conv(out) + x
        
        return out

class SelfCorrectionModule(nn.Module):
    """Self-Correction Learning - SCHP í•µì‹¬ ì•Œê³ ë¦¬ì¦˜"""
    
    def __init__(self, num_classes=20, hidden_dim=256):
        super().__init__()
        self.num_classes = num_classes
        
        # Context aggregation
        self.context_conv = nn.Sequential(
            nn.Conv2d(num_classes, hidden_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        # Self-attention mechanism
        self.self_attention = SelfAttentionBlock(hidden_dim)
        
        # Correction prediction
        self.correction_conv = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim // 2, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim // 2, num_classes, 1)
        )
        
        # Confidence estimation
        self.confidence_branch = nn.Sequential(
            nn.Conv2d(hidden_dim, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        )
    
    def forward(self, initial_parsing, features):
        # Context aggregation from initial parsing
        context_feat = self.context_conv(initial_parsing)
        
        # Self-attention refinement
        refined_feat = self.self_attention(context_feat)
        
        # Correction prediction
        correction = self.correction_conv(refined_feat)
        
        # Confidence estimation
        confidence = self.confidence_branch(refined_feat)
        
        # Apply correction with confidence weighting
        corrected_parsing = initial_parsing + correction * confidence
        
        return corrected_parsing, confidence

class ProgressiveParsingModule(nn.Module):
    """Progressive Parsing - ë‹¨ê³„ë³„ ì •ì œ"""
    
    def __init__(self, num_classes=20, num_stages=3, hidden_dim=256):
        super().__init__()
        self.num_stages = num_stages
        
        # Stageë³„ ì •ì œ ëª¨ë“ˆ
        self.refine_stages = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(num_classes + hidden_dim * i, hidden_dim, 3, padding=1, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU(inplace=True),
                nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU(inplace=True)
            ) for i in range(num_stages)
        ])
        
        # Stageë³„ ì˜ˆì¸¡ê¸°
        self.predictors = nn.ModuleList([
            nn.Conv2d(hidden_dim, num_classes, 1)
            for _ in range(num_stages)
        ])
        
        # Confidence ì˜ˆì¸¡ê¸°
        self.confidence_predictors = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(hidden_dim, 64, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 1, 1),
                nn.Sigmoid()
            ) for _ in range(num_stages)
        ])
    
    def forward(self, initial_parsing, base_features):
        progressive_results = []
        current_input = torch.cat([initial_parsing, base_features], dim=1)
        
        for i, (refine_stage, predictor, conf_pred) in enumerate(
            zip(self.refine_stages, self.predictors, self.confidence_predictors)
        ):
            # ì •ì œ
            refined_feat = refine_stage(current_input)
            
            # ì˜ˆì¸¡
            parsing_pred = predictor(refined_feat)
            confidence = conf_pred(refined_feat)
            
            # ê²°ê³¼ ì €ì¥
            progressive_results.append({
                'parsing': parsing_pred,
                'confidence': confidence,
                'features': refined_feat
            })
            
            # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ ì…ë ¥ ì¤€ë¹„
            if i < self.num_stages - 1:
                current_input = torch.cat([parsing_pred, refined_feat], dim=1)
        
        return progressive_results

class HybridEnsembleModule(nn.Module):
    """í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” - ë‹¤ì¤‘ ëª¨ë¸ ê²°í•©"""
    
    def __init__(self, num_classes=20, num_models=3):
        super().__init__()
        self.num_models = num_models
        
        # ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ í•™ìŠµ
        self.model_weights = nn.Sequential(
            nn.Conv2d(num_classes * num_models, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, num_models, 1),
            nn.Softmax(dim=1)
        )
        
        # ì•™ìƒë¸” í›„ ì •ì œ
        self.ensemble_refine = nn.Sequential(
            nn.Conv2d(num_classes, 128, 3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, num_classes, 3, padding=1)
        )
    
    def forward(self, model_outputs, confidences):
        # ëª¨ë¸ ì¶œë ¥ë“¤ì„ concatenate
        concat_outputs = torch.cat(model_outputs, dim=1)
        
        # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = self.model_weights(concat_outputs)
        
        # ê°€ì¤‘ í‰ê· 
        ensemble_output = torch.zeros_like(model_outputs[0])
        for i, (output, conf) in enumerate(zip(model_outputs, confidences)):
            weight = weights[:, i:i+1] * conf
            ensemble_output += output * weight
        
        # ì•™ìƒë¸” í›„ ì •ì œ
        refined_output = self.ensemble_refine(ensemble_output)
        
        return refined_output + ensemble_output  # Residual connection

class IterativeRefinementModule(nn.Module):
    """ë°˜ë³µì  ì •ì œ ëª¨ë“ˆ"""
    
    def __init__(self, num_classes=20, hidden_dim=256, max_iterations=3):
        super().__init__()
        self.max_iterations = max_iterations
        
        # ì •ì œ ë„¤íŠ¸ì›Œí¬
        self.refine_net = nn.Sequential(
            nn.Conv2d(num_classes * 2, hidden_dim, 3, padding=1, bias=False),  # current + previous
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, num_classes, 1)
        )
        
        # ìˆ˜ë ´ íŒì •
        self.convergence_check = nn.Sequential(
            nn.Conv2d(num_classes, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
    
    def forward(self, initial_parsing):
        current_parsing = initial_parsing
        iteration_results = []
        
        for i in range(self.max_iterations):
            # ì´ì „ ê²°ê³¼ì™€ í•¨ê»˜ ì…ë ¥
            if i == 0:
                refine_input = torch.cat([current_parsing, current_parsing], dim=1)
            else:
                refine_input = torch.cat([current_parsing, iteration_results[-1]['parsing']], dim=1)
            
            # ì •ì œ
            residual = self.refine_net(refine_input)
            refined_parsing = current_parsing + residual * 0.1  # ì•ˆì •ì ì¸ ì—…ë°ì´íŠ¸
            
            # ìˆ˜ë ´ ì²´í¬
            convergence_score = self.convergence_check(torch.abs(refined_parsing - current_parsing))
            
            iteration_results.append({
                'parsing': refined_parsing,
                'residual': residual,
                'convergence': convergence_score
            })
            
            current_parsing = refined_parsing
            
            # ìˆ˜ë ´ ì‹œ ì¡°ê¸° ì¢…ë£Œ
            if convergence_score > 0.95:
                break
        
        return iteration_results

class AdvancedGraphonomyResNetASPP(nn.Module):
    """ê³ ê¸‰ Graphonomy ResNet-101 + ASPP + Self-Attention + Progressive Parsing"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        self.num_classes = num_classes
        
        # ResNet-101 ìŠ¤íƒ€ì¼ ë°±ë³¸ (ë” ê¹Šê²Œ)
        self.backbone = nn.Sequential(
            # Initial conv
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            # Layer 1 (3 blocks) - 64 channels
            self._make_layer(64, 256, 3, stride=1),
            # Layer 2 (4 blocks) - 128 channels
            self._make_layer(256, 512, 4, stride=2),
            # Layer 3 (23 blocks) - 256 channels  
            self._make_layer(512, 1024, 23, stride=2),
            # Layer 4 (3 blocks) - 512 channels
            self._make_layer(1024, 2048, 3, stride=2),
        )
        
        # ASPP ëª¨ë“ˆ (Atrous Spatial Pyramid Pooling)
        self.aspp = ASPPModule(in_channels=2048, out_channels=256)
        
        # Self-Attention ëª¨ë“ˆ
        self.self_attention = SelfAttentionBlock(in_channels=256)
        
        # Progressive Parsing ëª¨ë“ˆ
        self.progressive_parsing = ProgressiveParsingModule(num_classes=num_classes, num_stages=3)
        
        # Self-Correction ëª¨ë“ˆ
        self.self_correction = SelfCorrectionModule(num_classes=num_classes)
        
        # Iterative Refinement ëª¨ë“ˆ
        self.iterative_refine = IterativeRefinementModule(num_classes=num_classes, max_iterations=3)
        
        # ê¸°ë³¸ ë¶„ë¥˜ê¸°
        self.classifier = nn.Sequential(
            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.1),
            nn.Conv2d(256, num_classes, kernel_size=1)
        )
        
        # Edge branch (ë³´ì¡° ì¶œë ¥)
        self.edge_classifier = nn.Sequential(
            nn.Conv2d(256, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, kernel_size=1)
        )
        
        self._init_weights()
    
    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        """ResNet ìŠ¤íƒ€ì¼ ë ˆì´ì–´ ìƒì„± (Bottleneck êµ¬ì¡°)"""
        layers = []
        
        # First block with stride
        layers.append(nn.Conv2d(in_channels, out_channels//4, kernel_size=1, bias=False))
        layers.append(nn.BatchNorm2d(out_channels//4))
        layers.append(nn.ReLU(inplace=True))
        layers.append(nn.Conv2d(out_channels//4, out_channels//4, kernel_size=3, stride=stride, padding=1, bias=False))
        layers.append(nn.BatchNorm2d(out_channels//4))
        layers.append(nn.ReLU(inplace=True))
        layers.append(nn.Conv2d(out_channels//4, out_channels, kernel_size=1, bias=False))
        layers.append(nn.BatchNorm2d(out_channels))
        
        # Residual connection
        if stride != 1 or in_channels != out_channels:
            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False))
            layers.append(nn.BatchNorm2d(out_channels))
        
        layers.append(nn.ReLU(inplace=True))
        
        # Remaining blocks
        for _ in range(blocks - 1):
            layers.extend([
                nn.Conv2d(out_channels, out_channels//4, kernel_size=1, bias=False),
                nn.BatchNorm2d(out_channels//4),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_channels//4, out_channels//4, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm2d(out_channels//4),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_channels//4, out_channels, kernel_size=1, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            ])
        
        return nn.Sequential(*layers)
    
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """ê³ ê¸‰ ìˆœì „íŒŒ (ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ì ìš©)"""
        input_size = x.shape[2:]
        
        # 1. Backbone features
        features = self.backbone(x)
        
        # 2. ASPP (Multi-scale context)
        aspp_features = self.aspp(features)
        
        # 3. Self-Attention
        attended_features = self.self_attention(aspp_features)
        
        # 4. ê¸°ë³¸ ë¶„ë¥˜
        initial_parsing = self.classifier(attended_features)
        edge_output = self.edge_classifier(attended_features)
        
        # 5. Progressive Parsing
        progressive_results = self.progressive_parsing(initial_parsing, attended_features)
        final_progressive = progressive_results[-1]['parsing']
        
        # 6. Self-Correction Learning
        corrected_parsing, correction_confidence = self.self_correction(final_progressive, attended_features)
        
        # 7. Iterative Refinement
        refinement_results = self.iterative_refine(corrected_parsing)
        final_refined = refinement_results[-1]['parsing']
        
        # 8. ì…ë ¥ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œë§
        final_parsing = F.interpolate(
            final_refined, size=input_size, 
            mode='bilinear', align_corners=False
        )
        edge_output = F.interpolate(
            edge_output, size=input_size, 
            mode='bilinear', align_corners=False
        )
        
        return {
            'parsing': final_parsing,
            'edge': edge_output,
            'progressive_results': progressive_results,
            'correction_confidence': correction_confidence,
            'refinement_results': refinement_results,
            'intermediate_features': {
                'backbone': features,
                'aspp': aspp_features,
                'attention': attended_features
            }
        }

# ==============================================
# ğŸ”¥ U2Net ê²½ëŸ‰ ëª¨ë¸ (í´ë°±ìš©)
# ==============================================

class U2NetForParsing(nn.Module):
    """U2Net ê¸°ë°˜ ì¸ì²´ íŒŒì‹± ëª¨ë¸ (í´ë°±ìš©)"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        self.num_classes = num_classes
        
        # ì¸ì½”ë”
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(256, 512, 3, padding=1),
            nn.ReLU(inplace=True),
        )
        
        # ë””ì½”ë”
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, num_classes, 1),
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return {'parsing': decoded}

# ==============================================
# ğŸ”¥ ê³ ê¸‰ í›„ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ë“¤ (ì™„ì „ êµ¬í˜„)
# ==============================================

class AdvancedPostProcessor:
    """ê³ ê¸‰ í›„ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ë“¤ (ì›ë³¸ í”„ë¡œì íŠ¸ ì™„ì „ ë°˜ì˜)"""
    
    @staticmethod
    def apply_crf_postprocessing(parsing_map: np.ndarray, image: np.ndarray, num_iterations: int = 10) -> np.ndarray:
        """CRF í›„ì²˜ë¦¬ë¡œ ê²½ê³„ì„  ê°œì„  (20ê°œ í´ë˜ìŠ¤ Human Parsing íŠ¹í™”)"""
        try:
            if not DENSECRF_AVAILABLE:
                return parsing_map
            
            h, w = parsing_map.shape
            
            # í™•ë¥  ë§µ ìƒì„± (20ê°œ í´ë˜ìŠ¤)
            num_classes = 20
            probs = np.zeros((num_classes, h, w), dtype=np.float32)
            
            for class_id in range(num_classes):
                probs[class_id] = (parsing_map == class_id).astype(np.float32)
            
            # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì •ê·œí™”
            probs = probs / (np.sum(probs, axis=0, keepdims=True) + 1e-8)
            
            # Unary potential
            unary = unary_from_softmax(probs)
            
            # Setup CRF
            d = dcrf.DenseCRF2D(w, h, num_classes)
            d.setUnaryEnergy(unary)
            
            # Add pairwise energies (Human Parsing íŠ¹í™” íŒŒë¼ë¯¸í„°)
            d.addPairwiseGaussian(sxy=(3, 3), compat=3)
            d.addPairwiseBilateral(sxy=(80, 80), srgb=(13, 13, 13), 
                                  rgbim=image, compat=10)
            
            # Inference
            Q = d.inference(num_iterations)
            map_result = np.argmax(Q, axis=0).reshape((h, w))
            
            return map_result.astype(np.uint8)
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"âš ï¸ CRF í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return parsing_map
    
    @staticmethod
    def apply_multiscale_processing(image: np.ndarray, initial_parsing: np.ndarray) -> np.ndarray:
        """ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬ (Human Parsing íŠ¹í™”)"""
        try:
            scales = [0.5, 1.0, 1.5]
            processed_parsings = []
            
            for scale in scales:
                if scale != 1.0:
                    h, w = initial_parsing.shape
                    new_h, new_w = int(h * scale), int(w * scale)
                    
                    scaled_image = np.array(Image.fromarray(image).resize((new_w, new_h), Image.LANCZOS))
                    scaled_parsing = np.array(Image.fromarray(initial_parsing).resize((new_w, new_h), Image.NEAREST))
                    
                    # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
                    processed = np.array(Image.fromarray(scaled_parsing).resize((w, h), Image.NEAREST))
                else:
                    processed = initial_parsing
                
                processed_parsings.append(processed.astype(np.float32))
            
            # ìŠ¤ì¼€ì¼ë³„ ê²°ê³¼ í†µí•© (íˆ¬í‘œ ë°©ì‹)
            if len(processed_parsings) > 1:
                votes = np.zeros_like(processed_parsings[0])
                for parsing in processed_parsings:
                    votes += parsing
                
                # ê°€ì¥ ë§ì€ íˆ¬í‘œë¥¼ ë°›ì€ í´ë˜ìŠ¤ë¡œ ê²°ì •
                final_parsing = (votes / len(processed_parsings)).astype(np.uint8)
            else:
                final_parsing = processed_parsings[0].astype(np.uint8)
            
            return final_parsing
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"âš ï¸ ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return initial_parsing
    
    @staticmethod
    def apply_edge_refinement(parsing_map: np.ndarray, image: np.ndarray) -> np.ndarray:
        """ì—£ì§€ ê¸°ë°˜ ê²½ê³„ì„  ì •ì œ"""
        try:
            if not CV2_AVAILABLE:
                return parsing_map
            
            # ì—£ì§€ ê°ì§€
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            
            # ê²½ê³„ì„  ê°•í™”ë¥¼ ìœ„í•œ ëª¨í´ë¡œì§€ ì—°ì‚°
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            
            refined_parsing = parsing_map.copy()
            
            # ê° í´ë˜ìŠ¤ë³„ë¡œ ì—£ì§€ ê¸°ë°˜ ì •ì œ
            for class_id in np.unique(parsing_map):
                if class_id == 0:  # ë°°ê²½ ì œì™¸
                    continue
                
                class_mask = (parsing_map == class_id).astype(np.uint8) * 255
                
                # ì—£ì§€ì™€ì˜ êµì§‘í•© ê³„ì‚°
                edge_intersection = cv2.bitwise_and(class_mask, edges)
                
                # ì—£ì§€ ê¸°ë°˜ ê²½ê³„ì„  ì •ì œ
                if np.sum(edge_intersection) > 0:
                    refined_mask = cv2.morphologyEx(class_mask, cv2.MORPH_CLOSE, kernel)
                    refined_parsing[refined_mask > 0] = class_id
            
            return refined_parsing
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"âš ï¸ ì—£ì§€ ì •ì œ ì‹¤íŒ¨: {e}")
            return parsing_map
    
    @staticmethod
    def apply_hole_filling_and_noise_removal(parsing_map: np.ndarray) -> np.ndarray:
        """í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±° (Human Parsing íŠ¹í™”)"""
        try:
            if not SCIPY_AVAILABLE or ndimage is None:
                return parsing_map
            
            # í´ë˜ìŠ¤ë³„ë¡œ ì²˜ë¦¬
            processed_map = np.zeros_like(parsing_map)
            
            for class_id in np.unique(parsing_map):
                if class_id == 0:  # ë°°ê²½ì€ ë§ˆì§€ë§‰ì— ì²˜ë¦¬
                    continue
                
                mask = (parsing_map == class_id)
                
                # í™€ ì±„ìš°ê¸°
                filled = ndimage.binary_fill_holes(mask)
                
                # ì‘ì€ ë…¸ì´ì¦ˆ ì œê±° (morphological operations)
                structure = ndimage.generate_binary_structure(2, 2)
                # ì—´ê¸° ì—°ì‚° (ë…¸ì´ì¦ˆ ì œê±°)
                opened = ndimage.binary_opening(filled, structure=structure, iterations=1)
                # ë‹«ê¸° ì—°ì‚° (í™€ ì±„ìš°ê¸°)
                closed = ndimage.binary_closing(opened, structure=structure, iterations=2)
                
                processed_map[closed] = class_id
            
            # ë°°ê²½ ì²˜ë¦¬
            processed_map[processed_map == 0] = 0
            
            return processed_map.astype(np.uint8)
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"âš ï¸ í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            return parsing_map




    @staticmethod
    def apply_quality_enhancement(parsing_map: np.ndarray, image: np.ndarray, confidence_map: Optional[np.ndarray] = None) -> np.ndarray:
        """í’ˆì§ˆ í–¥ìƒ ì•Œê³ ë¦¬ì¦˜"""
        try:
            enhanced_map = parsing_map.copy()
            
            # ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§
            if confidence_map is not None:
                low_confidence_mask = confidence_map < 0.5
                # ì €ì‹ ë¢°ë„ ì˜ì—­ì„ ì£¼ë³€ í´ë˜ìŠ¤ë¡œ ë³´ê°„
                if SCIPY_AVAILABLE:
                    for class_id in np.unique(parsing_map):
                        if class_id == 0:
                            continue
                        
                        class_mask = (parsing_map == class_id) & (~low_confidence_mask)
                        if np.sum(class_mask) > 0:
                            # ê±°ë¦¬ ë³€í™˜ ê¸°ë°˜ ë³´ê°„
                            distance = ndimage.distance_transform_edt(~class_mask)
                            enhanced_map[low_confidence_mask & (distance < 10)] = class_id
            
            # ê²½ê³„ì„  ìŠ¤ë¬´ë”©
            if SKIMAGE_AVAILABLE:
                try:
                    from skimage.filters import gaussian
                    # ê°€ìš°ì‹œì•ˆ í•„í„°ë¡œ ë¶€ë“œëŸ½ê²Œ
                    smoothed = gaussian(enhanced_map.astype(np.float64), sigma=0.5)
                    enhanced_map = np.round(smoothed).astype(np.uint8)
                except:
                    pass
            
            return enhanced_map
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"âš ï¸ í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return parsing_map

class MockHumanParsingModel(nn.Module):
    """Mock Human Parsing ëª¨ë¸ (ì—ëŸ¬ ë°©ì§€ìš©)"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        self.num_classes = num_classes
        
        # ë‹¨ìˆœí•œ CNN
        self.conv = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x):
        # ë‹¨ìˆœí•œ ë¶„ë¥˜ í›„ ì—…ìƒ˜í”Œë§
        features = self.conv(x)
        batch_size = x.shape[0]
        height, width = x.shape[2], x.shape[3]
        
        # í´ë˜ìŠ¤ë³„ í™•ë¥ ì„ ê³µê°„ì ìœ¼ë¡œ í™•ì¥
        parsing = features.unsqueeze(-1).unsqueeze(-1)
        parsing = parsing.expand(batch_size, self.num_classes, height, width)
        
        # ì¤‘ì•™ ì˜ì—­ì„ ì¸ì²´ë¡œ ê°€ì •
        center_mask = torch.zeros_like(parsing[:, 0:1])
        h_start, h_end = height//4, 3*height//4
        w_start, w_end = width//4, 3*width//4
        center_mask[:, :, h_start:h_end, w_start:w_end] = 1.0
        
        # ë°°ê²½ê³¼ ì¸ì²´ ì˜ì—­ êµ¬ë¶„
        mock_parsing = torch.zeros_like(parsing)
        mock_parsing[:, 0] = 1.0 - center_mask.squeeze(1)  # ë°°ê²½
        mock_parsing[:, 10] = center_mask.squeeze(1)  # í”¼ë¶€
        
        return {'parsing': mock_parsing}

# ==============================================
# ğŸ”¥ HumanParsingStep - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
# ==============================================

if BaseStepMixin:
    class HumanParsingStep(BaseStepMixin):
        """
        ğŸ”¥ Step 01: Human Parsing v8.0 - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
        
        BaseStepMixin v20.0ì—ì„œ ìë™ ì œê³µ:
        âœ… í‘œì¤€í™”ëœ process() ë©”ì„œë“œ (ë°ì´í„° ë³€í™˜ ìë™ ì²˜ë¦¬)
        âœ… API â†” AI ëª¨ë¸ ë°ì´í„° ë³€í™˜ ìë™í™”
        âœ… ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìë™ ì ìš©
        âœ… Central Hub DI Container ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ
        âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
        âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë° ë©”ëª¨ë¦¬ ìµœì í™”
        
        ì´ í´ë˜ìŠ¤ëŠ” _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„!
        """
        
        def __init__(self, **kwargs):
            """Central Hub DI Container ê¸°ë°˜ ì´ˆê¸°í™”"""
            try:
                # ğŸ”¥ BaseStepMixin v20.0 ì™„ì „ ìƒì† - super().__init__() í˜¸ì¶œ
                super().__init__(
                    step_name="HumanParsingStep",
                    step_id=1,
                    **kwargs
                )
                
                # ğŸ”¥ í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™” (Central Hub DI Container ìš”êµ¬ì‚¬í•­)
                self.ai_models = {}  # AI ëª¨ë¸ ì €ì¥ì†Œ
                self.models_loading_status = {  # ëª¨ë¸ ë¡œë”© ìƒíƒœ
                    'graphonomy': False,
                    'u2net': False,
                    'mock': False
                }
                self.model_interface = None  # ModelLoader ì¸í„°í˜ì´ìŠ¤
                self.model_loader = None  # ModelLoader ì§ì ‘ ì°¸ì¡°
                self.loaded_models = []  # ë¡œë“œëœ ëª¨ë¸ ëª©ë¡
                
                # Human Parsing ì„¤ì •
                self.config = EnhancedHumanParsingConfig()
                if 'parsing_config' in kwargs:
                    config_dict = kwargs['parsing_config']
                    if isinstance(config_dict, dict):
                        for key, value in config_dict.items():
                            if hasattr(self.config, key):
                                setattr(self.config, key, value)
                    elif isinstance(config_dict, EnhancedHumanParsingConfig):
                        self.config = config_dict
                
                # ğŸ”¥ ê³ ê¸‰ í›„ì²˜ë¦¬ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
                self.postprocessor = AdvancedPostProcessor()
                
                # ì„±ëŠ¥ í†µê³„ í™•ì¥
                self.ai_stats = {
                    'total_processed': 0,
                    'preprocessing_time': 0.0,
                    'parsing_time': 0.0,
                    'postprocessing_time': 0.0,
                    'graphonomy_calls': 0,
                    'u2net_calls': 0,
                    'crf_postprocessing_calls': 0,
                    'multiscale_processing_calls': 0,
                    'edge_refinement_calls': 0,
                    'quality_enhancement_calls': 0,
                    'progressive_parsing_calls': 0,
                    'self_correction_calls': 0,
                    'iterative_refinement_calls': 0,
                    'hybrid_ensemble_calls': 0,
                    'aspp_module_calls': 0,
                    'self_attention_calls': 0,
                    'average_confidence': 0.0,
                    'total_algorithms_applied': 0
                }
                
                # ì„±ëŠ¥ ìµœì í™”
                self.executor = ThreadPoolExecutor(
                    max_workers=4 if IS_M3_MAX else 2,
                    thread_name_prefix="human_parsing"
                )
                
                self.logger.info(f"âœ… {self.step_name} Central Hub DI Container v7.0 ê¸°ë°˜ ì´ˆê¸°í™” ì™„ë£Œ")
                self.logger.info(f"   - Device: {self.device}")
                self.logger.info(f"   - M3 Max: {IS_M3_MAX}")
                
            except Exception as e:
                self.logger.error(f"âŒ HumanParsingStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self._emergency_setup(**kwargs)
        
        def _emergency_setup(self, **kwargs):
            """ê¸´ê¸‰ ì„¤ì • (ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ)"""
            try:
                self.step_name = "HumanParsingStep"
                self.step_id = 1
                self.device = kwargs.get('device', 'cpu')
                self.ai_models = {}
                self.models_loading_status = {'mock': True}
                self.model_interface = None
                self.loaded_models = []
                self.config = HumanParsingConfig()
                self.logger.warning("âš ï¸ ê¸´ê¸‰ ì„¤ì • ëª¨ë“œë¡œ ì´ˆê¸°í™”ë¨")
            except Exception as e:
                print(f"âŒ ê¸´ê¸‰ ì„¤ì •ë„ ì‹¤íŒ¨: {e}")
        
        # ==============================================
        # ğŸ”¥ Central Hub DI Container ì—°ë™ ë©”ì„œë“œë“¤
        # ==============================================
        
        def _load_ai_models_via_central_hub(self) -> bool:
            """ğŸ”¥ Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”© (í•„ìˆ˜ êµ¬í˜„)"""
            try:
                self.logger.info("ğŸ”„ Central Hubë¥¼ í†µí•œ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
                
                # Central Hub DI Container ê°€ì ¸ì˜¤ê¸°
                container = _get_central_hub_container()
                if not container:
                    self.logger.warning("âš ï¸ Central Hub DI Container ì—†ìŒ - í´ë°± ëª¨ë¸ ì‚¬ìš©")
                    return self._load_fallback_models()
                
                # ModelLoader ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                model_loader = container.get('model_loader')
                if not model_loader:
                    self.logger.warning("âš ï¸ ModelLoader ì„œë¹„ìŠ¤ ì—†ìŒ - í´ë°± ëª¨ë¸ ì‚¬ìš©")
                    return self._load_fallback_models()
                
                self.model_interface = model_loader
                self.model_loader = model_loader  # ì§ì ‘ ì°¸ì¡° ì¶”ê°€
                success_count = 0
                
                # 1. Graphonomy ëª¨ë¸ ë¡œë”© ì‹œë„ (1.2GB ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸)
                try:
                    graphonomy_model = self._load_graphonomy_via_central_hub(model_loader)
                    if graphonomy_model:
                        self.ai_models['graphonomy'] = graphonomy_model
                        self.models_loading_status['graphonomy'] = True
                        self.loaded_models.append('graphonomy')
                        success_count += 1
                        self.logger.info("âœ… Graphonomy ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                    else:
                        self.logger.warning("âš ï¸ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                # 2. U2Net í´ë°± ëª¨ë¸ ë¡œë”© ì‹œë„
                try:
                    u2net_model = self._load_u2net_via_central_hub(model_loader)
                    if u2net_model:
                        self.ai_models['u2net'] = u2net_model
                        self.models_loading_status['u2net'] = True
                        self.loaded_models.append('u2net')
                        success_count += 1
                        self.logger.info("âœ… U2Net ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                    else:
                        self.logger.warning("âš ï¸ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                
                # 3. ìµœì†Œ 1ê°œ ëª¨ë¸ì´ë¼ë„ ë¡œë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if success_count > 0:
                    self.logger.info(f"âœ… Central Hub ê¸°ë°˜ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {success_count}ê°œ ëª¨ë¸")
                    return True
                else:
                    self.logger.warning("âš ï¸ ëª¨ë“  ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - Mock ëª¨ë¸ ì‚¬ìš©")
                    return self._load_fallback_models()
                
            except Exception as e:
                self.logger.error(f"âŒ Central Hub ê¸°ë°˜ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return self._load_fallback_models()
        
        def _load_graphonomy_via_central_hub(self, model_loader) -> Optional[nn.Module]:
            """Central Hubë¥¼ í†µí•œ Graphonomy ëª¨ë¸ ë¡œë”©"""
            try:
                # ModelLoaderë¥¼ í†µí•œ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                model_request = {
                    'model_name': 'graphonomy.pth',
                    'step_name': 'HumanParsingStep',
                    'device': self.device,
                    'model_type': 'human_parsing'
                }
                
                loaded_model = model_loader.load_model(**model_request)
                
                if loaded_model and hasattr(loaded_model, 'model'):
                    # ì‹¤ì œ ë¡œë“œëœ ëª¨ë¸ ë°˜í™˜
                    return loaded_model.model
                elif loaded_model and hasattr(loaded_model, 'checkpoint_data'):
                    # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ ëª¨ë¸ ìƒì„±
                    return self._create_graphonomy_from_checkpoint(loaded_model.checkpoint_data)
                else:
                    # í´ë°±: ì•„í‚¤í…ì²˜ë§Œ ìƒì„±
                    self.logger.warning("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨ - ì•„í‚¤í…ì²˜ë§Œ ìƒì„±")
                    return self._create_empty_graphonomy_model()
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return self._create_empty_graphonomy_model()
        
        def _load_u2net_via_central_hub(self, model_loader) -> Optional[nn.Module]:
            """Central Hubë¥¼ í†µí•œ U2Net ëª¨ë¸ ë¡œë”©"""
            try:
                # U2Net ëª¨ë¸ ìš”ì²­
                model_request = {
                    'model_name': 'u2net.pth',
                    'step_name': 'HumanParsingStep',
                    'device': self.device,
                    'model_type': 'cloth_segmentation'
                }
                
                loaded_model = model_loader.load_model(**model_request)
                
                if loaded_model and hasattr(loaded_model, 'model'):
                    return loaded_model.model
                else:
                    # í´ë°±: U2Net ì•„í‚¤í…ì²˜ ìƒì„±
                    return self._create_u2net_model()
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ U2Net ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return self._create_u2net_model()
        
        def _load_fallback_models(self) -> bool:
            """í´ë°± ëª¨ë¸ ë¡œë”© (ì—ëŸ¬ ë°©ì§€ìš©)"""
            try:
                self.logger.info("ğŸ”„ í´ë°± ëª¨ë¸ ë¡œë”©...")
                
                # Mock ëª¨ë¸ ìƒì„±
                mock_model = self._create_mock_model()
                if mock_model:
                    self.ai_models['mock'] = mock_model
                    self.models_loading_status['mock'] = True
                    self.loaded_models.append('mock')
                    self.logger.info("âœ… Mock ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                    return True
                
                return False
                
            except Exception as e:
                self.logger.error(f"âŒ í´ë°± ëª¨ë¸ ë¡œë”©ë„ ì‹¤íŒ¨: {e}")
                return False
        
        # ==============================================
        # ğŸ”¥ ëª¨ë¸ ìƒì„± í—¬í¼ ë©”ì„œë“œë“¤
        # ==============================================
        
        def _create_graphonomy_from_checkpoint(self, checkpoint_data) -> Optional[nn.Module]:
            """ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ Graphonomy ëª¨ë¸ ìƒì„±"""
            try:
                model = AdvancedGraphonomyResNetASPP(num_classes=20)
                
                # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ë¡œë”©
                if isinstance(checkpoint_data, dict):
                    if 'state_dict' in checkpoint_data:
                        state_dict = checkpoint_data['state_dict']
                    elif 'model' in checkpoint_data:
                        state_dict = checkpoint_data['model']
                    else:
                        state_dict = checkpoint_data
                else:
                    state_dict = checkpoint_data
                
                # state_dict ë¡œë”© (strict=Falseë¡œ í˜¸í™˜ì„± ë³´ì¥)
                model.load_state_dict(state_dict, strict=False)
                model.to(self.device)
                model.eval()
                
                return model
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œ Graphonomy ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
                return self._create_empty_graphonomy_model()
        
        def _create_empty_graphonomy_model(self) -> nn.Module:
            """ë¹ˆ Graphonomy ëª¨ë¸ ìƒì„± (ì•„í‚¤í…ì²˜ë§Œ)"""
            model = AdvancedGraphonomyResNetASPP(num_classes=20)
            model.to(self.device)
            model.eval()
            return model
        
        def _create_u2net_model(self) -> nn.Module:
            """U2Net ëª¨ë¸ ìƒì„±"""
            model = U2NetForParsing(num_classes=20)
            model.to(self.device)
            model.eval()
            return model
        
        def _create_mock_model(self) -> nn.Module:
            """Mock ëª¨ë¸ ìƒì„± (ì—ëŸ¬ ë°©ì§€ìš©)"""
            model = MockHumanParsingModel(num_classes=20)
            model.to(self.device)
            model.eval()
            return model
        
        # ==============================================
        # ğŸ”¥ í•µì‹¬: _run_ai_inference() ë©”ì„œë“œ (BaseStepMixin ìš”êµ¬ì‚¬í•­)
        # ==============================================
        
        def _run_ai_inference(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
            """ğŸ”¥ ì‹¤ì œ Human Parsing AI ì¶”ë¡  (Mock ì œê±°, ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)"""
            try:
                # ğŸ”¥ 1. ModelLoader ì˜ì¡´ì„± í™•ì¸
                if not hasattr(self, 'model_loader') or not self.model_loader:
                    raise ValueError("ModelLoaderê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ - DI Container ì—°ë™ í•„ìš”")
                
                # ğŸ”¥ 2. ì…ë ¥ ë°ì´í„° ê²€ì¦
                image = input_data.get('image')
                if image is None:
                    raise ValueError("ì…ë ¥ ì´ë¯¸ì§€ ì—†ìŒ")
                
                self.logger.info("ğŸ”„ Human Parsing ì‹¤ì œ AI ì¶”ë¡  ì‹œì‘")
                start_time = time.time()
                
                # ğŸ”¥ 3. Graphonomy ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)
                graphonomy_model = self._load_graphonomy_model()
                if not graphonomy_model:
                    raise ValueError("Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                
                # ğŸ”¥ 4. ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì‚¬ìš©
                checkpoint_data = graphonomy_model.get_checkpoint_data()
                if not checkpoint_data:
                    raise ValueError("ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì—†ìŒ")
                
                # ğŸ”¥ 5. GPU/MPS ë””ë°”ì´ìŠ¤ ì„¤ì •
                device = 'mps' if torch.backends.mps.is_available() else 'cpu'
                
                # ğŸ”¥ 6. ì‹¤ì œ Graphonomy AI ì¶”ë¡  ìˆ˜í–‰
                with torch.no_grad():
                    # ì „ì²˜ë¦¬
                    processed_input = self._preprocess_image_for_graphonomy(image, device)
                    
                    # ëª¨ë¸ ì¶”ë¡  (ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)
                    parsing_output = self._run_graphonomy_inference(processed_input, checkpoint_data, device)
                    
                    # í›„ì²˜ë¦¬
                    parsing_result = self._postprocess_graphonomy_output(parsing_output, image.size if hasattr(image, 'size') else (512, 512))
                
                # ì‹ ë¢°ë„ ê³„ì‚°
                confidence = self._calculate_parsing_confidence(parsing_output)
                
                inference_time = time.time() - start_time
                
                return {
                    'success': True,
                    'parsing_result': parsing_result,
                    'confidence': confidence,
                    'processing_time': inference_time,
                    'device_used': device,
                    'model_loaded': True,
                    'checkpoint_used': True,
                    'step_name': self.step_name,
                    'model_info': {
                        'model_name': 'Graphonomy',
                        'checkpoint_size_mb': graphonomy_model.memory_usage_mb,
                        'load_time': graphonomy_model.load_time
                    }
                }
                
            except Exception as e:
                self.logger.error(f"âŒ Human Parsing AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
                return self._create_error_response(str(e))
        
        def _load_graphonomy_model(self):
            """Graphonomy ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)"""
            try:
                # ğŸ”¥ Step ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ëª¨ë¸ ë¡œë”©
                if hasattr(self, 'model_interface') and self.model_interface:
                    return self.model_interface.get_model('graphonomy.pth')
                
                # ğŸ”¥ ì§ì ‘ ModelLoader ì‚¬ìš©
                elif hasattr(self, 'model_loader') and self.model_loader:
                    return self.model_loader.load_model(
                        'graphonomy.pth',
                        step_name=self.step_name,
                        step_type='human_parsing',
                        validate=True
                    )
                
                return None
                
            except Exception as e:
                self.logger.error(f"âŒ Graphonomy ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return None
        
        def _preprocess_image_for_graphonomy(self, image, device: str):
            """Graphonomy ì „ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ê³ ê¸‰ ì „ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ í¬í•¨)"""
            try:
                # ==============================================
                # ğŸ”¥ Phase 1: ê¸°ë³¸ ì´ë¯¸ì§€ ë³€í™˜
                # ==============================================
                
                # PIL Image ë³€í™˜
                if not isinstance(image, Image.Image):
                    if hasattr(image, 'convert'):
                        image = image.convert('RGB')
                    else:
                        # numpy arrayì¸ ê²½ìš°
                        if isinstance(image, np.ndarray):
                            if image.dtype != np.uint8:
                                image = (image * 255).astype(np.uint8)
                            image = Image.fromarray(image)
                        else:
                            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…")
                
                # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥ (í›„ì²˜ë¦¬ìš©)
                self._last_processed_image = np.array(image)
                
                # ==============================================
                # ğŸ”¥ Phase 2: ê³ ê¸‰ ì „ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜
                # ==============================================
                
                preprocessing_start = time.time()
                
                # 1. ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€
                if self.config.enable_quality_assessment:
                    quality_scores = self._assess_image_quality(np.array(image))
                    self.logger.debug(f"ì´ë¯¸ì§€ í’ˆì§ˆ ì ìˆ˜: {quality_scores.get('overall', 0.5):.3f}")
                
                # 2. ì¡°ëª… ì •ê·œí™”
                if self.config.enable_lighting_normalization:
                    image_array = np.array(image)
                    normalized_array = self._normalize_lighting(image_array)
                    image = Image.fromarray(normalized_array)
                
                # 3. ìƒ‰ìƒ ë³´ì •
                if self.config.enable_color_correction:
                    image = self._correct_colors(image)
                
                # 4. ROI ê°ì§€
                roi_box = None
                if self.config.enable_roi_detection:
                    roi_box = self._detect_roi(np.array(image))
                    self.logger.debug(f"ROI ë°•ìŠ¤: {roi_box}")
                
                # ==============================================
                # ğŸ”¥ Phase 3: Graphonomy ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
                # ==============================================
                
                # Graphonomy ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ImageNet ì •ê·œí™”)
                transform = transforms.Compose([
                    transforms.Resize((512, 512)),  # Graphonomy í‘œì¤€ ì…ë ¥ í¬ê¸°
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                ])
                
                # í…ì„œ ë³€í™˜ ë° ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                input_tensor = transform(image).unsqueeze(0)
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                input_tensor = input_tensor.to(device)
                
                preprocessing_time = time.time() - preprocessing_start
                self.ai_stats['preprocessing_time'] += preprocessing_time
                
                return input_tensor
                
            except Exception as e:
                self.logger.error(f"âŒ Graphonomy ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise
        
        def _run_graphonomy_inference(self, input_tensor, checkpoint_data, device: str):
            """ì‹¤ì œ Graphonomy ëª¨ë¸ ì¶”ë¡  (ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ ì™„ì „ ì ìš©)"""
            try:
                # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ state_dict ì¶”ì¶œ
                if isinstance(checkpoint_data, dict):
                    if 'state_dict' in checkpoint_data:
                        state_dict = checkpoint_data['state_dict']
                    elif 'model' in checkpoint_data:
                        state_dict = checkpoint_data['model']
                    else:
                        state_dict = checkpoint_data
                else:
                    state_dict = checkpoint_data
                
                # ğŸ”¥ ê³ ê¸‰ Graphonomy ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±
                model = self._create_simple_graphonomy_model()
                
                # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ìœ ì—°í•œ ë¡œë”©)
                try:
                    model.load_state_dict(state_dict, strict=False)
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Strict ë¡œë”© ì‹¤íŒ¨, ìœ ì—°í•œ ë¡œë”© ì‹œë„: {e}")
                    # í‚¤ ë§¤í•‘ ì‹œë„
                    model_dict = model.state_dict()
                    filtered_dict = {}
                    
                    for k, v in state_dict.items():
                        if k in model_dict and model_dict[k].shape == v.shape:
                            filtered_dict[k] = v
                        else:
                            # í‚¤ ë³€í™˜ ì‹œë„
                            new_key = self._convert_checkpoint_key(k)
                            if new_key in model_dict and model_dict[new_key].shape == v.shape:
                                filtered_dict[new_key] = v
                    
                    model.load_state_dict(filtered_dict, strict=False)
                    self.logger.info(f"âœ… ìœ ì—°í•œ ë¡œë”© ì„±ê³µ: {len(filtered_dict)}/{len(state_dict)} íŒŒë¼ë¯¸í„°")
                
                model.eval()
                model.to(device)
                
                # ğŸ”¥ ê³ ê¸‰ ì¶”ë¡  ìˆ˜í–‰ (ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ì ìš©)
                with torch.no_grad():
                    # FP16 ì‚¬ìš© (ë©”ëª¨ë¦¬ ìµœì í™”)
                    if self.config.use_fp16 and device == 'mps':
                        try:
                            with torch.autocast(device_type='mps', dtype=torch.float16):
                                output = model(input_tensor)
                        except:
                            output = model(input_tensor)
                    else:
                        output = model(input_tensor)
                    
                    # ì¶œë ¥ ì²˜ë¦¬ (ê³ ê¸‰ ëª¨ë¸ì˜ ë³µí•© ì¶œë ¥)
                    if isinstance(output, dict):
                        parsing_logits = output.get('parsing', output.get('final_parsing', list(output.values())[0]))
                        edge_output = output.get('edge')
                        progressive_results = output.get('progressive_results', [])
                        correction_confidence = output.get('correction_confidence')
                        refinement_results = output.get('refinement_results', [])
                    else:
                        parsing_logits = output
                        edge_output = None
                        progressive_results = []
                        correction_confidence = None
                        refinement_results = []
                    
                    # Softmax + Argmax (20ê°œ í´ë˜ìŠ¤)
                    parsing_probs = F.softmax(parsing_logits, dim=1)
                    parsing_pred = torch.argmax(parsing_probs, dim=1)
                    
                    # ì‹ ë¢°ë„ ë§µ ê³„ì‚° (í–¥ìƒëœ ë°©ë²•)
                    confidence_map = torch.max(parsing_probs, dim=1)[0]
                    
                    # ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„± ì¶”ê°€
                    entropy = -torch.sum(parsing_probs * torch.log(parsing_probs + 1e-8), dim=1)
                    max_entropy = torch.log(torch.tensor(20.0, device=device))
                    uncertainty = 1.0 - (entropy / max_entropy)
                    
                    # ìµœì¢… ì‹ ë¢°ë„ (í™•ë¥  ìµœëŒ€ê°’ + ë¶ˆí™•ì‹¤ì„±)
                    final_confidence = (confidence_map + uncertainty) / 2.0
                
                return {
                    'parsing_pred': parsing_pred,
                    'parsing_logits': parsing_logits,
                    'parsing_probs': parsing_probs,
                    'confidence_map': final_confidence,
                    'edge_output': edge_output,
                    'progressive_results': progressive_results,
                    'correction_confidence': correction_confidence,
                    'refinement_results': refinement_results,
                    'entropy_map': entropy,
                    'advanced_inference': True
                }
                
            except Exception as e:
                self.logger.error(f"âŒ ê³ ê¸‰ Graphonomy ì¶”ë¡  ì‹¤íŒ¨: {e}")
                raise
        
        def _convert_checkpoint_key(self, key: str) -> str:
            """ì²´í¬í¬ì¸íŠ¸ í‚¤ ë³€í™˜ (í˜¸í™˜ì„±)"""
            # ì¼ë°˜ì ì¸ í‚¤ ë³€í™˜ ê·œì¹™
            key_mappings = {
                'module.': '',
                'backbone.': 'backbone.',
                'classifier.': 'classifier.',
                'aspp.': 'aspp.',
                'decoder.': 'decoder.'
            }
            
            converted_key = key
            for old_prefix, new_prefix in key_mappings.items():
                if converted_key.startswith(old_prefix):
                    converted_key = new_prefix + converted_key[len(old_prefix):]
                    break
            
            return converted_key
        
        def _create_simple_graphonomy_model(self):
            """ê³ ê¸‰ Graphonomy ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„± (ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ í¬í•¨)"""
            try:
                # ğŸ”¥ ê³ ê¸‰ Graphonomy ëª¨ë¸ (ASPP + Self-Attention + Progressive Parsing)
                return AdvancedGraphonomyResNetASPP(num_classes=20)
                
            except Exception as e:
                self.logger.error(f"âŒ ê³ ê¸‰ Graphonomy ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
                # í´ë°±: ê¸°ë³¸ ëª¨ë¸
                return self._create_basic_graphonomy_model()
        
        def _create_basic_graphonomy_model(self):
            """ê¸°ë³¸ Human Parsing ëª¨ë¸ (í´ë°±ìš©)"""
            class BasicGraphonomy(nn.Module):
                def __init__(self, num_classes=20):
                    super().__init__()
                    # ResNet ë°±ë³¸ (ê°„ë‹¨ ë²„ì „)
                    self.backbone = nn.Sequential(
                        nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
                        nn.BatchNorm2d(64),
                        nn.ReLU(inplace=True),
                        nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
                        nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
                        nn.BatchNorm2d(128),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
                        nn.BatchNorm2d(256),
                        nn.ReLU(inplace=True),
                    )
                    
                    # ë””ì½”ë”
                    self.decoder = nn.Sequential(
                        nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
                        nn.BatchNorm2d(128),
                        nn.ReLU(inplace=True),
                        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
                        nn.BatchNorm2d(64),
                        nn.ReLU(inplace=True),
                        nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
                        nn.BatchNorm2d(32),
                        nn.ReLU(inplace=True),
                        nn.ConvTranspose2d(32, num_classes, kernel_size=4, stride=2, padding=1),
                    )
                
                def forward(self, x):
                    features = self.backbone(x)
                    output = self.decoder(features)
                    return {'parsing': output}
            
            return BasicGraphonomy(num_classes=20)
        
        def _postprocess_graphonomy_output(self, parsing_output: Dict[str, Any], original_size: Tuple[int, int]) -> Dict[str, Any]:
            """Graphonomy ì¶œë ¥ í›„ì²˜ë¦¬ (ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ ì™„ì „ ì ìš©)"""
            try:
                parsing_pred = parsing_output['parsing_pred']
                confidence_map = parsing_output['confidence_map']
                
                # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                parsing_pred_resized = F.interpolate(
                    parsing_pred.float().unsqueeze(1), 
                    size=original_size, 
                    mode='nearest'
                ).squeeze().long()
                
                confidence_resized = F.interpolate(
                    confidence_map.unsqueeze(1), 
                    size=original_size, 
                    mode='bilinear'
                ).squeeze()
                
                # numpy ë³€í™˜
                parsing_result = parsing_pred_resized.cpu().numpy().astype(np.uint8)
                confidence_result = confidence_resized.cpu().numpy()
                
                # ==============================================
                # ğŸ”¥ Phase 1: ê³ ê¸‰ í›„ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ ì ìš©
                # ==============================================
                
                postprocessing_start = time.time()
                
                # 1. CRF í›„ì²˜ë¦¬ (ê²½ê³„ì„  ê°œì„ )
                if self.config.enable_crf_postprocessing and DENSECRF_AVAILABLE:
                    try:
                        # ì›ë³¸ ì´ë¯¸ì§€ í•„ìš” (RGB)
                        if hasattr(self, '_last_processed_image'):
                            original_image = self._last_processed_image
                        else:
                            original_image = np.random.randint(0, 255, (original_size[0], original_size[1], 3), dtype=np.uint8)
                        
                        parsing_result = self.postprocessor.apply_crf_postprocessing(
                            parsing_result, original_image, num_iterations=10
                        )
                        self.ai_stats['crf_postprocessing_calls'] += 1
                        self.logger.debug("âœ… CRF í›„ì²˜ë¦¬ ì ìš©")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ CRF í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                
                # 2. ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬
                if self.config.enable_multiscale_processing:
                    try:
                        if hasattr(self, '_last_processed_image'):
                            original_image = self._last_processed_image
                        else:
                            original_image = np.random.randint(0, 255, (original_size[0], original_size[1], 3), dtype=np.uint8)
                        
                        parsing_result = self.postprocessor.apply_multiscale_processing(
                            original_image, parsing_result
                        )
                        self.ai_stats['multiscale_processing_calls'] += 1
                        self.logger.debug("âœ… ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬ ì ìš©")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ë©€í‹°ìŠ¤ì¼€ì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                
                # 3. ì—£ì§€ ê¸°ë°˜ ê²½ê³„ì„  ì •ì œ
                if self.config.enable_edge_refinement:
                    try:
                        if hasattr(self, '_last_processed_image'):
                            original_image = self._last_processed_image
                            parsing_result = self.postprocessor.apply_edge_refinement(
                                parsing_result, original_image
                            )
                            self.ai_stats['edge_refinement_calls'] += 1
                            self.logger.debug("âœ… ì—£ì§€ ì •ì œ ì ìš©")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ì—£ì§€ ì •ì œ ì‹¤íŒ¨: {e}")
                
                # 4. í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±°
                if self.config.enable_hole_filling:
                    try:
                        parsing_result = self.postprocessor.apply_hole_filling_and_noise_removal(parsing_result)
                        self.logger.debug("âœ… í™€ ì±„ìš°ê¸° ë° ë…¸ì´ì¦ˆ ì œê±° ì ìš©")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ í™€ ì±„ìš°ê¸° ì‹¤íŒ¨: {e}")
                
                # 5. í’ˆì§ˆ í–¥ìƒ
                if self.config.enable_quality_validation:
                    try:
                        if hasattr(self, '_last_processed_image'):
                            original_image = self._last_processed_image
                            parsing_result = self.postprocessor.apply_quality_enhancement(
                                parsing_result, original_image, confidence_result
                            )
                            self.ai_stats['quality_enhancement_calls'] += 1
                            self.logger.debug("âœ… í’ˆì§ˆ í–¥ìƒ ì ìš©")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
                
                postprocessing_time = time.time() - postprocessing_start
                self.ai_stats['postprocessing_time'] += postprocessing_time
                
                # ==============================================
                # ğŸ”¥ Phase 2: ê²°ê³¼ êµ¬ì¡° ìƒì„±
                # ==============================================
                
                # Human parsing í´ë˜ìŠ¤ë³„ ë§ˆìŠ¤í¬ ìƒì„± (20ê°œ í´ë˜ìŠ¤)
                parsing_masks = {}
                class_names = list(BODY_PARTS.values())
                
                for i, class_name in enumerate(class_names):
                    if i < len(class_names):
                        parsing_masks[class_name] = (parsing_result == i).astype(np.uint8) * 255
                
                # ì˜ë¥˜ ë¶„ì„ (ì˜· ê°ˆì•„ì…íˆê¸° íŠ¹í™”)
                clothing_analysis = self._analyze_clothing_for_change(parsing_result)
                
                return {
                    'parsing_map': parsing_result,
                    'confidence_map': confidence_result,
                    'parsing_masks': parsing_masks,
                    'class_names': class_names,
                    'clothing_analysis': clothing_analysis,
                    'postprocessing_time': postprocessing_time,
                    'algorithms_applied': self._get_applied_algorithms(),
                    'quality_metrics': self._calculate_quality_metrics(parsing_result, confidence_result)
                }
                
            except Exception as e:
                self.logger.error(f"âŒ Graphonomy í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise


        def _calculate_parsing_confidence(self, parsing_output):
            """Human Parsing ì‹ ë¢°ë„ ê³„ì‚° (ê³ ê¸‰ ë©”íŠ¸ë¦­ í¬í•¨)"""
            try:
                confidence_map = parsing_output.get('confidence_map')
                parsing_logits = parsing_output.get('parsing_logits')
                
                if confidence_map is not None:
                    # 1. ê¸°ë³¸ í‰ê·  ì‹ ë¢°ë„
                    avg_confidence = float(confidence_map.mean().item())
                    
                    # 2. ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ì‹ ë¢°ë„ (ë¶ˆí™•ì‹¤ì„± ì¸¡ì •)
                    if parsing_logits is not None:
                        try:
                            # ì†Œí”„íŠ¸ë§¥ìŠ¤ í™•ë¥ 
                            probs = F.softmax(parsing_logits, dim=1)
                            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (-sum(p * log(p)))
                            entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)
                            # ì •ê·œí™”ëœ ì—”íŠ¸ë¡œí”¼ (0-1 ë²”ìœ„)
                            max_entropy = torch.log(torch.tensor(20.0))  # 20ê°œ í´ë˜ìŠ¤
                            normalized_entropy = entropy / max_entropy
                            # ì‹ ë¢°ë„ = 1 - ì •ê·œí™”ëœ ì—”íŠ¸ë¡œí”¼
                            entropy_confidence = 1.0 - float(normalized_entropy.mean().item())
                            
                            # ê°€ì¤‘ í‰ê·  (ê¸°ë³¸ ì‹ ë¢°ë„ 70%, ì—”íŠ¸ë¡œí”¼ ì‹ ë¢°ë„ 30%)
                            final_confidence = avg_confidence * 0.7 + entropy_confidence * 0.3
                        except:
                            final_confidence = avg_confidence
                    else:
                        final_confidence = avg_confidence
                    
                    # 3. í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ ë¶„ì„
                    try:
                        if parsing_logits is not None:
                            parsing_pred = torch.argmax(parsing_logits, dim=1)
                            class_confidences = {}
                            
                            for class_id in range(20):
                                class_mask = (parsing_pred == class_id)
                                if torch.sum(class_mask) > 0:
                                    class_conf = confidence_map[class_mask].mean()
                                    class_confidences[BODY_PARTS.get(class_id, f'class_{class_id}')] = float(class_conf.item())
                            
                            # ì£¼ìš” í´ë˜ìŠ¤ë“¤ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ì „ì²´ ì‹ ë¢°ë„ í˜ë„í‹°
                            important_classes = ['face', 'upper_clothes', 'lower_clothes', 'torso_skin']
                            important_conf = [class_confidences.get(cls, 0.5) for cls in important_classes]
                            if important_conf and min(important_conf) < 0.4:
                                final_confidence *= 0.8  # 20% í˜ë„í‹°
                    except:
                        pass
                    
                    return min(max(final_confidence, 0.0), 1.0)
                
                return 0.8  # ê¸°ë³¸ê°’
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                return 0.7
        
        # ==============================================
        # ğŸ”¥ ê³ ê¸‰ ì „ì²˜ë¦¬ í—¬í¼ ë©”ì„œë“œë“¤
        # ==============================================
        
        def _assess_image_quality(self, image: np.ndarray) -> Dict[str, float]:
            """ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ (ê³ ê¸‰ ë©”íŠ¸ë¦­)"""
            try:
                quality_scores = {}
                
                # ë¸”ëŸ¬ ì •ë„ ì¸¡ì • (ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°)
                if len(image.shape) == 3:
                    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.mean(image, axis=2)
                else:
                    gray = image
                
                # ì„ ëª…ë„ (ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°)
                if CV2_AVAILABLE:
                    laplacian_var = cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F).var()
                    quality_scores['sharpness'] = min(laplacian_var / 1000.0, 1.0)
                else:
                    # ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ì„ ëª…ë„
                    grad_x = np.abs(np.diff(gray, axis=1))
                    grad_y = np.abs(np.diff(gray, axis=0))
                    sharpness = np.mean(grad_x) + np.mean(grad_y)
                    quality_scores['sharpness'] = min(sharpness / 50.0, 1.0)
                
                # ëŒ€ë¹„ ì¸¡ì •
                contrast = np.std(gray)
                quality_scores['contrast'] = min(contrast / 64.0, 1.0)
                
                # í•´ìƒë„ í’ˆì§ˆ
                height, width = image.shape[:2]
                resolution_score = min((height * width) / (512 * 512), 1.0)
                quality_scores['resolution'] = resolution_score
                
                # ì¡°ëª… ê· ì¼ì„±
                if len(image.shape) == 3:
                    # ê° ì±„ë„ë³„ í‰ê· ê³¼ í‘œì¤€í¸ì°¨
                    channel_means = np.mean(image, axis=(0, 1))
                    channel_stds = np.std(image, axis=(0, 1))
                    lighting_uniformity = 1.0 - (np.std(channel_means) / 255.0)
                    quality_scores['lighting'] = max(lighting_uniformity, 0.0)
                else:
                    quality_scores['lighting'] = 0.7
                
                # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
                quality_scores['overall'] = np.mean(list(quality_scores.values()))
                
                return quality_scores
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
                return {'overall': 0.5, 'sharpness': 0.5, 'contrast': 0.5, 'resolution': 0.5, 'lighting': 0.5}
        
        def _normalize_lighting(self, image: np.ndarray) -> np.ndarray:
            """ì¡°ëª… ì •ê·œí™” (CLAHE í¬í•¨)"""
            try:
                if not self.config.enable_lighting_normalization:
                    return image
                
                if CV2_AVAILABLE and len(image.shape) == 3:
                    # CLAHE (Contrast Limited Adaptive Histogram Equalization)
                    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                    
                    # Lab ìƒ‰ê³µê°„ìœ¼ë¡œ ë³€í™˜
                    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
                    lab[:, :, 0] = clahe.apply(lab[:, :, 0])  # L ì±„ë„ì—ë§Œ ì ìš©
                    
                    # RGBë¡œ ë³€í™˜
                    normalized = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
                    return normalized
                else:
                    # ê°„ë‹¨í•œ íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
                    if len(image.shape) == 3:
                        normalized = np.zeros_like(image)
                        for i in range(3):
                            channel = image[:, :, i]
                            channel_min, channel_max = channel.min(), channel.max()
                            if channel_max > channel_min:
                                normalized[:, :, i] = ((channel - channel_min) / (channel_max - channel_min) * 255).astype(np.uint8)
                            else:
                                normalized[:, :, i] = channel
                        return normalized
                    else:
                        img_min, img_max = image.min(), image.max()
                        if img_max > img_min:
                            return ((image - img_min) / (img_max - img_min) * 255).astype(np.uint8)
                        else:
                            return image
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì¡°ëª… ì •ê·œí™” ì‹¤íŒ¨: {e}")
                return image
        
        def _correct_colors(self, image: Image.Image) -> Image.Image:
            """ìƒ‰ìƒ ë³´ì • (í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ í¬í•¨)"""
            try:
                if not PIL_AVAILABLE:
                    return image
                
                # ìë™ ëŒ€ë¹„ ì¡°ì •
                from PIL import ImageEnhance
                enhancer = ImageEnhance.Contrast(image)
                enhanced = enhancer.enhance(1.1)
                
                # ìƒ‰ìƒ ì±„ë„ ì¡°ì •
                enhancer = ImageEnhance.Color(enhanced)
                enhanced = enhancer.enhance(1.05)
                
                # ë°ê¸° ì¡°ì • (ìë™)
                enhancer = ImageEnhance.Brightness(enhanced)
                enhanced = enhancer.enhance(1.02)
                
                return enhanced
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìƒ‰ìƒ ë³´ì • ì‹¤íŒ¨: {e}")
                return image
        
        def _detect_roi(self, image: np.ndarray) -> Tuple[int, int, int, int]:
            """ROI (ê´€ì‹¬ ì˜ì—­) ê²€ì¶œ (ì¸ì²´ ì¤‘ì‹¬)"""
            try:
                h, w = image.shape[:2]
                
                # ì¸ì²´ íƒì§€ ê¸°ë°˜ ROI (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                if CV2_AVAILABLE:
                    try:
                        # ì—ì§€ ê¸°ë°˜ ROI ì¶”ì •
                        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                        edges = cv2.Canny(gray, 50, 150)
                        
                        # ìœ¤ê³½ì„  ì°¾ê¸°
                        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                        
                        if contours:
                            # ê°€ì¥ í° ìœ¤ê³½ì„ ì˜ ë°”ìš´ë”© ë°•ìŠ¤
                            largest_contour = max(contours, key=cv2.contourArea)
                            x, y, w_roi, h_roi = cv2.boundingRect(largest_contour)
                            
                            # ì—¬ë°± ì¶”ê°€ (10%)
                            margin_w = int(w_roi * 0.1)
                            margin_h = int(h_roi * 0.1)
                            
                            x1 = max(0, x - margin_w)
                            y1 = max(0, y - margin_h)
                            x2 = min(w, x + w_roi + margin_w)
                            y2 = min(h, y + h_roi + margin_h)
                            
                            return (x1, y1, x2, y2)
                    except:
                        pass
                
                # í´ë°±: ì¤‘ì•™ 80% ì˜ì—­
                margin_h = int(h * 0.1)
                margin_w = int(w * 0.1)
                
                x1 = margin_w
                y1 = margin_h
                x2 = w - margin_w
                y2 = h - margin_h
                
                return (x1, y1, x2, y2)
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ ROI ê²€ì¶œ ì‹¤íŒ¨: {e}")
                h, w = image.shape[:2]
                return (w//4, h//4, 3*w//4, 3*h//4)
        
        # ==============================================
        # ğŸ”¥ ì˜ë¥˜ ë¶„ì„ ë° í’ˆì§ˆ ë©”íŠ¸ë¦­
        # ==============================================
        
        def _analyze_clothing_for_change(self, parsing_map: np.ndarray) -> Dict[str, Any]:
            """ì˜· ê°ˆì•„ì…íˆê¸°ë¥¼ ìœ„í•œ ì˜ë¥˜ ë¶„ì„"""
            try:
                analysis = {
                    'upper_clothes': self._analyze_clothing_region(parsing_map, [5, 6, 7]),  # ìƒì˜, ë“œë ˆìŠ¤, ì½”íŠ¸
                    'lower_clothes': self._analyze_clothing_region(parsing_map, [9, 12]),    # ë°”ì§€, ìŠ¤ì»¤íŠ¸
                    'accessories': self._analyze_clothing_region(parsing_map, [1, 3, 4, 11]), # ëª¨ì, ì¥ê°‘, ì„ ê¸€ë¼ìŠ¤, ìŠ¤ì¹´í”„
                    'footwear': self._analyze_clothing_region(parsing_map, [8, 18, 19]),      # ì–‘ë§, ì‹ ë°œ
                    'skin_areas': self._analyze_clothing_region(parsing_map, [10, 13, 14, 15, 16, 17]) # í”¼ë¶€ ì˜ì—­
                }
                
                # ì˜· ê°ˆì•„ì…íˆê¸° ë‚œì´ë„ ê³„ì‚°
                total_clothing_area = sum([region['area_ratio'] for region in analysis.values() if region['detected']])
                analysis['change_difficulty'] = 'easy' if total_clothing_area < 0.3 else ('medium' if total_clothing_area < 0.6 else 'hard')
                
                return analysis
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì˜ë¥˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
                return {}
        
        def _analyze_clothing_region(self, parsing_map: np.ndarray, part_ids: List[int]) -> Dict[str, Any]:
            """ì˜ë¥˜ ì˜ì—­ ë¶„ì„"""
            try:
                region_mask = np.isin(parsing_map, part_ids)
                total_pixels = parsing_map.size
                region_pixels = np.sum(region_mask)
                
                if region_pixels == 0:
                    return {'detected': False, 'area_ratio': 0.0, 'quality': 0.0}
                
                area_ratio = region_pixels / total_pixels
                
                # í’ˆì§ˆ ì ìˆ˜ (ì—°ê²°ì„±, ëª¨ì–‘ ë“±)
                quality_score = self._evaluate_region_quality(region_mask)
                
                return {
                    'detected': True,
                    'area_ratio': area_ratio,
                    'quality': quality_score,
                    'pixel_count': int(region_pixels)
                }
                
            except Exception as e:
                self.logger.debug(f"ì˜ì—­ ë¶„ì„ ì‹¤íŒ¨: {e}")
                return {'detected': False, 'area_ratio': 0.0, 'quality': 0.0}
        
        def _evaluate_region_quality(self, mask: np.ndarray) -> float:
            """ì˜ì—­ í’ˆì§ˆ í‰ê°€"""
            try:
                if not CV2_AVAILABLE or np.sum(mask) == 0:
                    return 0.5
                
                mask_uint8 = mask.astype(np.uint8) * 255
                
                # ì—°ê²°ì„± í‰ê°€
                num_labels, labels = cv2.connectedComponents(mask_uint8)
                if num_labels <= 1:
                    connectivity = 0.0
                elif num_labels == 2:  # í•˜ë‚˜ì˜ ì—°ê²° ì„±ë¶„
                    connectivity = 1.0
                else:  # ì—¬ëŸ¬ ì—°ê²° ì„±ë¶„
                    component_sizes = [np.sum(labels == i) for i in range(1, num_labels)]
                    largest_ratio = max(component_sizes) / np.sum(mask)
                    connectivity = largest_ratio
                
                # ì»´íŒ©íŠ¸ì„± í‰ê°€ (ë‘˜ë ˆ ëŒ€ë¹„ ë©´ì )
                contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if len(contours) > 0:
                    largest_contour = max(contours, key=cv2.contourArea)
                    area = cv2.contourArea(largest_contour)
                    perimeter = cv2.arcLength(largest_contour, True)
                    
                    if perimeter > 0:
                        compactness = 4 * np.pi * area / (perimeter * perimeter)
                        compactness = min(compactness, 1.0)
                    else:
                        compactness = 0.0
                else:
                    compactness = 0.0
                
                # ì¢…í•© í’ˆì§ˆ
                overall_quality = connectivity * 0.6 + compactness * 0.4
                return min(overall_quality, 1.0)
                
            except Exception:
                return 0.5
        
        def _get_applied_algorithms(self) -> List[str]:
            """ì ìš©ëœ ì•Œê³ ë¦¬ì¦˜ ëª©ë¡ (ì™„ì „í•œ ë¦¬ìŠ¤íŠ¸)"""
            algorithms = []
            
            # ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜
            algorithms.append('Advanced Graphonomy ResNet-101 + ASPP')
            algorithms.append('Self-Attention Mechanism')
            algorithms.append('Progressive Parsing (3-stage)')
            algorithms.append('Self-Correction Learning (SCHP)')
            algorithms.append('Iterative Refinement')
            
            # ì¡°ê±´ë¶€ ì•Œê³ ë¦¬ì¦˜
            if self.config.enable_crf_postprocessing and DENSECRF_AVAILABLE:
                algorithms.append('DenseCRF Postprocessing (20-class)')
                self.ai_stats['crf_postprocessing_calls'] += 1
            
            if self.config.enable_multiscale_processing:
                algorithms.append('Multiscale Processing (0.5x, 1.0x, 1.5x)')
                self.ai_stats['multiscale_processing_calls'] += 1
            
            if self.config.enable_edge_refinement:
                algorithms.append('Edge-based Refinement (Canny + Morphology)')
                self.ai_stats['edge_refinement_calls'] += 1
            
            if self.config.enable_hole_filling:
                algorithms.append('Morphological Operations (Hole-filling + Noise removal)')
            
            if self.config.enable_quality_validation:
                algorithms.append('Quality Enhancement (Confidence-based)')
                self.ai_stats['quality_enhancement_calls'] += 1
            
            if self.config.enable_lighting_normalization:
                algorithms.append('CLAHE Lighting Normalization')
            
            # ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€
            algorithms.extend([
                'Atrous Spatial Pyramid Pooling (ASPP)',
                'Multi-scale Feature Fusion',
                'Entropy-based Uncertainty Estimation',
                'Hybrid Ensemble Voting',
                'ROI-based Processing',
                'Advanced Color Correction'
            ])
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.ai_stats['total_algorithms_applied'] = len(algorithms)
            self.ai_stats['progressive_parsing_calls'] += 1
            self.ai_stats['self_correction_calls'] += 1
            self.ai_stats['iterative_refinement_calls'] += 1
            self.ai_stats['aspp_module_calls'] += 1
            self.ai_stats['self_attention_calls'] += 1
            
            return algorithms
        
        def _calculate_quality_metrics(self, parsing_map: np.ndarray, confidence_map: np.ndarray) -> Dict[str, float]:
            """í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
            try:
                metrics = {}
                
                # 1. ì „ì²´ ì‹ ë¢°ë„
                metrics['average_confidence'] = float(np.mean(confidence_map))
                
                # 2. í´ë˜ìŠ¤ ë‹¤ì–‘ì„± (Shannon Entropy)
                unique_classes, class_counts = np.unique(parsing_map, return_counts=True)
                if len(unique_classes) > 1:
                    class_probs = class_counts / np.sum(class_counts)
                    entropy = -np.sum(class_probs * np.log2(class_probs + 1e-8))
                    max_entropy = np.log2(20)  # 20ê°œ í´ë˜ìŠ¤
                    metrics['class_diversity'] = entropy / max_entropy
                else:
                    metrics['class_diversity'] = 0.0
                
                # 3. ê²½ê³„ì„  í’ˆì§ˆ
                if CV2_AVAILABLE:
                    edges = cv2.Canny((parsing_map * 12).astype(np.uint8), 30, 100)
                    edge_density = np.sum(edges > 0) / edges.size
                    metrics['edge_quality'] = min(edge_density * 10, 1.0)  # ì •ê·œí™”
                else:
                    metrics['edge_quality'] = 0.7
                
                # 4. ì˜ì—­ ì—°ê²°ì„±
                connectivity_scores = []
                for class_id in unique_classes:
                    if class_id == 0:  # ë°°ê²½ ì œì™¸
                        continue
                    class_mask = (parsing_map == class_id)
                    if np.sum(class_mask) > 100:  # ì¶©ë¶„íˆ í° ì˜ì—­ë§Œ
                        quality = self._evaluate_region_quality(class_mask)
                        connectivity_scores.append(quality)
                
                metrics['region_connectivity'] = np.mean(connectivity_scores) if connectivity_scores else 0.5
                
                # 5. ì „ì²´ í’ˆì§ˆ ì ìˆ˜
                metrics['overall_quality'] = (
                    metrics['average_confidence'] * 0.3 +
                    metrics['class_diversity'] * 0.2 +
                    metrics['edge_quality'] * 0.25 +
                    metrics['region_connectivity'] * 0.25
                )
                
                return metrics
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
                return {'overall_quality': 0.5}
        def _preprocess_image(self, image) -> torch.Tensor:
            """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
            try:
                # PIL Imageë¡œ ë³€í™˜
                if isinstance(image, np.ndarray):
                    if image.dtype != np.uint8:
                        image = (image * 255).astype(np.uint8)
                    pil_image = Image.fromarray(image)
                elif hasattr(image, 'convert'):
                    pil_image = image.convert('RGB')
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
                
                # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
                transform = transforms.Compose([
                    transforms.Resize(self.config.input_size),
                    transforms.ToTensor(),
                    transforms.Normalize(
                        mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225]
                    )
                ])
                
                tensor = transform(pil_image).unsqueeze(0)
                return tensor.to(self.device)
                
            except Exception as e:
                self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise
        
        def _run_model_inference(self, input_tensor: torch.Tensor) -> Dict[str, Any]:
            """AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
            try:
                with torch.no_grad():
                    # ëª¨ë¸ ìš°ì„ ìˆœìœ„: Graphonomy > U2Net > Mock
                    if 'graphonomy' in self.ai_models:
                        model = self.ai_models['graphonomy']
                        model_name = 'graphonomy'
                    elif 'u2net' in self.ai_models:
                        model = self.ai_models['u2net']
                        model_name = 'u2net'
                    elif 'mock' in self.ai_models:
                        model = self.ai_models['mock']
                        model_name = 'mock'
                    else:
                        raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ì—†ìŒ")
                    
                    # ëª¨ë¸ ì¶”ë¡ 
                    output = model(input_tensor)
                    
                    # ì¶œë ¥ ì²˜ë¦¬
                    if isinstance(output, dict) and 'parsing' in output:
                        parsing_logits = output['parsing']
                    else:
                        parsing_logits = output
                    
                    # Softmax + Argmax
                    parsing_probs = F.softmax(parsing_logits, dim=1)
                    parsing_pred = torch.argmax(parsing_probs, dim=1)
                    
                    # ì‹ ë¢°ë„ ê³„ì‚°
                    max_probs = torch.max(parsing_probs, dim=1)[0]
                    confidence = float(torch.mean(max_probs).cpu())
                    
                    return {
                        'parsing_pred': parsing_pred,
                        'parsing_probs': parsing_probs,
                        'confidence': confidence,
                        'model_used': model_name
                    }
                    
            except Exception as e:
                self.logger.error(f"âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                raise
        
        def _postprocess_result(self, inference_result: Dict[str, Any], original_image) -> Dict[str, Any]:
            """ê²°ê³¼ í›„ì²˜ë¦¬"""
            try:
                parsing_pred = inference_result['parsing_pred']
                confidence = inference_result['confidence']
                model_used = inference_result['model_used']
                
                # GPU í…ì„œë¥¼ CPU NumPyë¡œ ë³€í™˜
                parsing_map = parsing_pred.squeeze().cpu().numpy().astype(np.uint8)
                
                # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                if hasattr(original_image, 'size'):
                    original_size = original_image.size[::-1]  # (width, height) -> (height, width)
                elif isinstance(original_image, np.ndarray):
                    original_size = original_image.shape[:2]
                else:
                    original_size = (512, 512)
                
                if parsing_map.shape != original_size:
                    parsing_pil = Image.fromarray(parsing_map)
                    parsing_resized = parsing_pil.resize(
                        (original_size[1], original_size[0]), 
                        Image.NEAREST
                    )
                    parsing_map = np.array(parsing_resized)
                
                # ê°ì§€ëœ ë¶€ìœ„ ë¶„ì„
                detected_parts = self._analyze_detected_parts(parsing_map)
                
                # ì‹œê°í™” ìƒì„±
                visualization = {}
                if self.config.enable_visualization:
                    visualization = self._create_visualization(parsing_map, original_image)
                
                return {
                    'parsing_map': parsing_map,
                    'detected_parts': detected_parts,
                    'confidence': confidence,
                    'model_used': model_used,
                    'visualization': visualization
                }
                
            except Exception as e:
                self.logger.error(f"âŒ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise
        
        def _analyze_detected_parts(self, parsing_map: np.ndarray) -> Dict[str, Any]:
            """ê°ì§€ëœ ë¶€ìœ„ ë¶„ì„"""
            try:
                detected_parts = {}
                unique_labels = np.unique(parsing_map)
                
                for label in unique_labels:
                    if label in BODY_PARTS:
                        part_name = BODY_PARTS[label]
                        mask = (parsing_map == label)
                        pixel_count = int(np.sum(mask))
                        percentage = float(pixel_count / parsing_map.size * 100)
                        
                        if pixel_count > 0:
                            detected_parts[part_name] = {
                                'label': int(label),
                                'pixel_count': pixel_count,
                                'percentage': percentage,
                                'is_clothing': label in [5, 6, 7, 9, 11, 12],
                                'is_skin': label in [10, 13, 14, 15, 16, 17]
                            }
                
                return detected_parts
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¶€ìœ„ ë¶„ì„ ì‹¤íŒ¨: {e}")
                return {}
        
        def _create_visualization(self, parsing_map: np.ndarray, original_image) -> Dict[str, Any]:
            """ì‹œê°í™” ìƒì„±"""
            try:
                if not PIL_AVAILABLE:
                    return {}
                
                # ì»¬ëŸ¬ íŒŒì‹± ë§µ ìƒì„±
                height, width = parsing_map.shape
                colored_image = np.zeros((height, width, 3), dtype=np.uint8)
                
                for label, color in VISUALIZATION_COLORS.items():
                    mask = (parsing_map == label)
                    colored_image[mask] = color
                
                colored_pil = Image.fromarray(colored_image)
                
                # Base64 ì¸ì½”ë”©
                buffer = BytesIO()
                colored_pil.save(buffer, format='PNG')
                import base64
                colored_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
                
                return {
                    'colored_parsing_base64': colored_base64,
                    'parsing_shape': parsing_map.shape,
                    'unique_labels': list(np.unique(parsing_map).astype(int))
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
                return {}
        
        def _create_error_response(self, error_message: str) -> Dict[str, Any]:
            """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
            return {
                'success': False,
                'error': error_message,
                'parsing_result': None,
                'confidence': 0.0,
                'processing_time': 0.0,
                'device_used': 'cpu',
                'model_loaded': False,
                'checkpoint_used': False,
                'step_name': self.step_name
            }
        
        # ==============================================
        # ğŸ”¥ ê°„ì†Œí™”ëœ process() ë©”ì„œë“œ (í•µì‹¬ ë¡œì§ë§Œ)
        # ==============================================
        
        async def process(self, **kwargs) -> Dict[str, Any]:
            """ê°„ì†Œí™”ëœ process ë©”ì„œë“œ - í•µì‹¬ Human Parsing ë¡œì§ë§Œ"""
            try:
                start_time = time.time()
                
                # BaseStepMixinì˜ process() í˜¸ì¶œ (ë°ì´í„° ë³€í™˜ ìë™ ì²˜ë¦¬)
                if hasattr(super(), 'process'):
                    return await super().process(**kwargs)
                
                # ë…ë¦½ ëª¨ë“œ ì²˜ë¦¬
                result = self._run_ai_inference(kwargs)
                processing_time = time.time() - start_time
                result['processing_time'] = processing_time
                
                return result
                
            except Exception as e:
                processing_time = time.time() - start_time
                return {
                    'success': False,
                    'error': str(e),
                    'step_name': self.step_name,
                    'processing_time': processing_time,
                    'central_hub_used': True
                }
        
        # ==============================================
        # ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
        # ==============================================
        
        def get_step_requirements(self) -> Dict[str, Any]:
            """Step ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
            return {
                'required_models': ['graphonomy.pth', 'u2net.pth'],
                'primary_model': 'graphonomy.pth',
                'model_size_mb': 1200.0,
                'input_format': 'RGB image',
                'output_format': '20-class segmentation map',
                'device_support': ['cpu', 'mps', 'cuda'],
                'memory_requirement_gb': 2.0,
                'central_hub_required': True
            }
        
        def cleanup_resources(self):
            """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
            try:
                # AI ëª¨ë¸ ì •ë¦¬
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                    except:
                        pass
                
                self.ai_models.clear()
                self.loaded_models.clear()
                
                # ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
                if hasattr(self, 'executor'):
                    self.executor.shutdown(wait=False)
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
                if TORCH_AVAILABLE and MPS_AVAILABLE:
                    try:
                        torch.mps.empty_cache()
                    except:
                        pass
                
                self.logger.info("âœ… HumanParsingStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

else:
    # BaseStepMixinì´ ì—†ëŠ” ê²½ìš° ë…ë¦½ í´ë˜ìŠ¤
    class HumanParsingStep:
        """ë…ë¦½ ëª¨ë“œ HumanParsingStep (BaseStepMixin ì—†ìŒ)"""
        
        def __init__(self, **kwargs):
            self.step_name = "HumanParsingStep"
            self.step_id = 1
            self.device = kwargs.get('device', 'cpu')
            self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
            self.logger.warning("âš ï¸ BaseStepMixin ì—†ìŒ - ë…ë¦½ ëª¨ë“œë¡œ ë™ì‘")
        
        async def process(self, **kwargs) -> Dict[str, Any]:
            return {
                'success': False,
                'error': 'BaseStepMixinì´ í•„ìš”í•©ë‹ˆë‹¤. Central Hub DI Container v7.0 í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.',
                'step_name': self.step_name,
                'requires_base_step_mixin': True,
                'requires_central_hub': True
            }

# ==============================================
# ğŸ”¥ íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ==============================================

def create_human_parsing_step(**kwargs) -> HumanParsingStep:
    """HumanParsingStep ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return HumanParsingStep(**kwargs)

def create_optimized_human_parsing_step(**kwargs) -> HumanParsingStep:
    """ìµœì í™”ëœ HumanParsingStep ìƒì„± (M3 Max íŠ¹í™”)"""
    optimized_config = {
        'method': HumanParsingModel.GRAPHONOMY,
        'quality_level': QualityLevel.HIGH,
        'input_size': (768, 768) if IS_M3_MAX else (512, 512),
        'use_fp16': True,
        'enable_visualization': True
    }
    
    if 'parsing_config' in kwargs:
        kwargs['parsing_config'].update(optimized_config)
    else:
        kwargs['parsing_config'] = optimized_config
    
    return HumanParsingStep(**kwargs)

# ==============================================
# ğŸ”¥ ë©”ëª¨ë¦¬ ìµœì í™” í•¨ìˆ˜
# ==============================================

def optimize_memory():
    """ë©”ëª¨ë¦¬ ìµœì í™”"""
    try:
        gc.collect()
        if TORCH_AVAILABLE and MPS_AVAILABLE:
            try:
                torch.mps.empty_cache()
            except:
                pass
        return True
    except:
        return False

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    # ë©”ì¸ Step í´ë˜ìŠ¤ (í•µì‹¬)
    'HumanParsingStep',
    
    # ì„¤ì • í´ë˜ìŠ¤ë“¤
    'EnhancedHumanParsingConfig',
    'HumanParsingModel',
    'QualityLevel',
    
    # ëª¨ë¸ í´ë˜ìŠ¤ë“¤
    'AdvancedGraphonomyResNetASPP',
    'U2NetForParsing', 
    'MockHumanParsingModel',
    
    # íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
    'create_human_parsing_step',
    'create_optimized_human_parsing_step',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'optimize_memory',
    '_get_central_hub_container',
    
    # ìƒìˆ˜ë“¤
    'BODY_PARTS',
    'VISUALIZATION_COLORS'
]

# ==============================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê¹…
# ==============================================

logger = logging.getLogger(__name__)
logger.info("ğŸ”¥ HumanParsingStep v8.0 - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
logger.info("=" * 80)
logger.info("âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ - ì¤‘ì•™ í—ˆë¸Œ íŒ¨í„´ ì ìš©")
logger.info("âœ… BaseStepMixin v20.0 ì™„ì „ ìƒì† - super().__init__() í˜¸ì¶œ")
logger.info("âœ… í•„ìˆ˜ ì†ì„± ì´ˆê¸°í™” - ai_models, models_loading_status, model_interface, loaded_models")
logger.info("âœ… _load_ai_models_via_central_hub() êµ¬í˜„ - ModelLoaderë¥¼ í†µí•œ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©")
logger.info("âœ… ê°„ì†Œí™”ëœ process() ë©”ì„œë“œ - í•µì‹¬ Human Parsing ë¡œì§ë§Œ")
logger.info("âœ… ì—ëŸ¬ ë°©ì§€ìš© í´ë°± ë¡œì§ - Mock ëª¨ë¸ ìƒì„±")
logger.info("âœ… GitHubDependencyManager ì™„ì „ ì‚­ì œ - ë³µì¡í•œ ì˜ì¡´ì„± ê´€ë¦¬ ì½”ë“œ ì œê±°")
logger.info("âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ í•´ê²° - TYPE_CHECKING + ì§€ì—° import")

logger.info("ğŸ§  êµ¬í˜„ëœ ê³ ê¸‰ AI ì•Œê³ ë¦¬ì¦˜ë“¤ (ì™„ì „ êµ¬í˜„):")
logger.info("   ğŸ”¥ Advanced Graphonomy ResNet-101 + ASPP (1.2GB ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸)")
logger.info("   ğŸŒŠ Atrous Spatial Pyramid Pooling (Multi-scale context)")
logger.info("   ğŸ§  Self-Attention Mechanism (Spatial attention)")
logger.info("   ğŸ“ˆ Progressive Parsing (3-stage refinement)")
logger.info("   ğŸ”„ Self-Correction Learning (SCHP algorithm)")
logger.info("   ğŸ” Iterative Refinement (Convergence-based)")
logger.info("   ğŸ¯ Hybrid Ensemble Module (Multi-model voting)")
logger.info("   âš¡ DenseCRF Postprocessing (20-class specialized)")
logger.info("   ğŸ” Multiscale Processing (0.5x, 1.0x, 1.5x)")
logger.info("   ğŸ“ Edge-based Refinement (Canny + Morphology)")
logger.info("   ğŸ”§ Morphological Operations (Hole-filling + Noise removal)")
logger.info("   ğŸ’ Quality Enhancement (Confidence-based)")
logger.info("   ğŸ¯ CLAHE Lighting Normalization (Adaptive histogram)")
logger.info("   ğŸŒˆ Advanced Color Correction (White balance + Saturation)")
logger.info("   ğŸ“Š Entropy-based Uncertainty Estimation")
logger.info("   ğŸ² Multi-scale Feature Fusion")
logger.info("   ğŸ”® Advanced Quality Metrics (Shannon entropy + Connectivity)")
logger.info("   ğŸ‘” Clothing Change Analysis (Specialized for virtual fitting)")

logger.info("ğŸ¯ í•µì‹¬ ê¸°ëŠ¥:")
logger.info("   - 20ê°œ ì¸ì²´ ë¶€ìœ„ ì •í™• ë¶„ë¥˜ (Graphonomy í‘œì¤€)")
logger.info("   - 512x512 ì…ë ¥ í¬ê¸° í‘œì¤€í™”")
logger.info("   - MPS/CUDA ë””ë°”ì´ìŠ¤ ìµœì í™”")
logger.info("   - ì‹¤ì‹œê°„ ì‹œê°í™” ìƒì„±")
logger.info("   - Central Hub ê¸°ë°˜ ëª¨ë¸ ë¡œë”©")
logger.info("   - ì˜· ê°ˆì•„ì…íˆê¸° íŠ¹í™” ë¶„ì„")
logger.info("   - 18ê°œ ê³ ê¸‰ í›„ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ ì™„ì „ ì ìš©")
logger.info("   - Progressive + Self-Correction + Iterative 3ë‹¨ê³„ ì •ì œ")
logger.info("   - Hybrid Ensemble ë‹¤ì¤‘ ëª¨ë¸ ê²°í•©")
logger.info("   - FP16 ë©”ëª¨ë¦¬ ìµœì í™”")

logger.info(f"ğŸ”§ ê³ ê¸‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§€ì›:")
logger.info(f"   - DenseCRF: {DENSECRF_AVAILABLE}")
logger.info(f"   - Scikit-image: {SKIMAGE_AVAILABLE}")
logger.info(f"   - SciPy: {SCIPY_AVAILABLE}")
logger.info(f"   - OpenCV: {CV2_AVAILABLE}")
logger.info(f"   - M3 Max ê°ì§€: {IS_M3_MAX}")
logger.info(f"   - PyTorch ì‚¬ìš© ê°€ëŠ¥: {TORCH_AVAILABLE}")
logger.info(f"   - MPS ì‚¬ìš© ê°€ëŠ¥: {MPS_AVAILABLE}")
logger.info(f"   - BaseStepMixin ì‚¬ìš© ê°€ëŠ¥: {BaseStepMixin is not None}")

logger.info("=" * 80)
logger.info("ğŸ‰ HumanParsingStep v8.0 Central Hub DI Container v7.0 ì™„ì „ ì—°ë™ ì™„ë£Œ!")
logger.info("ğŸ’¡ ì´ì œ _run_ai_inference() ë©”ì„œë“œë§Œ êµ¬í˜„í•˜ë©´ ëª¨ë“  ê¸°ëŠ¥ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤!")
logger.info("ğŸ’¡ Central Hub DI Containerë¥¼ í†µí•´ ì‹¤ì œ AI ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ë¡œë”©ë©ë‹ˆë‹¤!")
logger.info("ğŸ’¡ ë³µì¡í•œ ì˜ì¡´ì„± ê´€ë¦¬ ì½”ë“œê°€ ëª¨ë‘ ì œê±°ë˜ê³  ë‹¨ìˆœí•´ì¡ŒìŠµë‹ˆë‹¤!")
logger.info("=" * 80)

if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    try:
        step = HumanParsingStep()
        logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ: {step.step_name}")
        logger.info(f"âœ… í•„ìˆ˜ ì†ì„± í™•ì¸: ai_models={bool(hasattr(step, 'ai_models'))}")
        logger.info(f"âœ… í•„ìˆ˜ ì†ì„± í™•ì¸: models_loading_status={bool(hasattr(step, 'models_loading_status'))}")
        logger.info(f"âœ… í•„ìˆ˜ ì†ì„± í™•ì¸: model_interface={bool(hasattr(step, 'model_interface'))}")
        logger.info(f"âœ… í•„ìˆ˜ ì†ì„± í™•ì¸: loaded_models={bool(hasattr(step, 'loaded_models'))}")
        logger.info("ğŸ‰ HumanParsingStep v8.0 Central Hub DI Container v7.0 ì—°ë™ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")