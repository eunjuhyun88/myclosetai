#!/usr/bin/env python3
"""
üî• MyCloset AI - Step 02: AI Í∏∞Î∞ò Ìè¨Ï¶à Ï∂îÏ†ï - ÏôÑÏ†ÑÌïú Ïã§Ï†ú AI Î™®Îç∏ Ïó∞Îèô v5.0
==========================================================================================

‚úÖ Ïã§Ï†ú AI Î™®Îç∏ ÌååÏùº ÌôúÏö© (3.4GB): OpenPose, YOLOv8, Diffusion, Body Pose
‚úÖ ModelLoader ÏôÑÏ†Ñ Ïó∞Îèô - Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ‚Üí AI Î™®Îç∏ ÌÅ¥ÎûòÏä§ ‚Üí Ïã§Ï†ú Ï∂îÎ°†
‚úÖ BaseStepMixin v16.0 ÏôÑÏ†Ñ Ìò∏Ìôò - ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ìå®ÌÑ¥
‚úÖ TYPE_CHECKING Ìå®ÌÑ¥ÏúºÎ°ú ÏàúÌôòÏ∞∏Ï°∞ Î∞©ÏßÄ
‚úÖ StepFactory ‚Üí ModelLoader ‚Üí BaseStepMixin ‚Üí ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ‚Üí ÏôÑÏÑ±Îêú Step
‚úÖ SmartModelPathMapper ÌôúÏö©Ìïú ÎèôÏ†Å ÌååÏùº Í≤ΩÎ°ú ÌÉêÏßÄ
‚úÖ Ïã§Ï†ú AI Ï∂îÎ°† ÏóîÏßÑ Íµ¨ÌòÑ (YOLOv8, OpenPose, Diffusion)
‚úÖ 18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏ ÏôÑÏ†Ñ Í≤ÄÏ∂ú Î∞è Ïä§ÏºàÎ†àÌÜ§ Íµ¨Ï°∞ ÏÉùÏÑ±
‚úÖ M3 Max MPS Í∞ÄÏÜç ÏµúÏ†ÅÌôî
‚úÖ conda ÌôòÍ≤Ω Ïö∞ÏÑ† ÏßÄÏõê

ÌïµÏã¨ ÏïÑÌÇ§ÌÖçÏ≤ò:
StepFactory ‚Üí ModelLoader (Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©) ‚Üí AI Î™®Îç∏ ÌÅ¥ÎûòÏä§ ‚Üí Ïã§Ï†ú Ï∂îÎ°†

Ï≤òÎ¶¨ ÌùêÎ¶Ñ:
1. ModelLoaderÍ∞Ä Ïã§Ï†ú Î™®Îç∏ ÌååÏùºÎì§ÏùÑ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î°ú Î°úÎî©
2. Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ‚Üí AI Î™®Îç∏ ÌÅ¥ÎûòÏä§ Î≥ÄÌôò (RealYOLOv8PoseModel, RealOpenPoseModel Îì±)
3. Ïã§Ï†ú AI Ï∂îÎ°† Ïã§Ìñâ ‚Üí 18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏ Í≤ÄÏ∂ú
4. Ìè¨Ï¶à ÌíàÏßà Î∂ÑÏÑù ‚Üí Ïä§ÏºàÎ†àÌÜ§ Íµ¨Ï°∞ ÏÉùÏÑ± ‚Üí API ÏùëÎãµ

Ïã§Ï†ú ÌôúÏö© ÌååÏùºÎì§:
- ai_models/step_02_pose_estimation/yolov8n-pose.pt (6.5MB)
- ai_models/step_02_pose_estimation/openpose.pth (97.8MB)
- ai_models/step_02_pose_estimation/diffusion_pytorch_model.safetensors (1378.2MB)
- ai_models/step_02_pose_estimation/body_pose_model.pth (97.8MB)

ÌååÏùº ÏúÑÏπò: backend/app/ai_pipeline/steps/step_02_pose_estimation.py
ÏûëÏÑ±Ïûê: MyCloset AI Team  
ÎÇ†Ïßú: 2025-07-25
Î≤ÑÏ†Ñ: v5.0 (Complete Real AI Model Integration)
"""

# ==============================================
# üî• 1. Import ÏÑπÏÖò (TYPE_CHECKING Ìå®ÌÑ¥)
# ==============================================

import os
import sys
import gc
import time
import asyncio
import logging
import threading
import traceback
import hashlib
import json
import base64
import weakref
import math
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from contextlib import asynccontextmanager

# TYPE_CHECKINGÏúºÎ°ú ÏàúÌôòÏ∞∏Ï°∞ Î∞©ÏßÄ
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from ..factories.step_factory import StepFactory
    from ..steps.base_step_mixin import BaseStepMixin

# ==============================================
# üî• 2. conda ÌôòÍ≤Ω Î∞è ÌïÑÏàò Ìå®ÌÇ§ÏßÄ Ï≤¥ÌÅ¨
# ==============================================

# conda ÌôòÍ≤Ω Ï†ïÎ≥¥
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'), 
    'python_path': os.path.dirname(os.__file__)
}

# M3 Max Í∞êÏßÄ
def detect_m3_max() -> bool:
    """M3 Max Í∞êÏßÄ"""
    try:
        import platform
        import subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()

# PyTorch (ÌïÑÏàò)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    import torchvision.transforms as transforms
    from torchvision.transforms.functional import resize, to_pil_image, to_tensor
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
    
    # MPS Ïû•Ïπò ÏÑ§Ï†ï
    if IS_M3_MAX and torch.backends.mps.is_available():
        DEVICE = "mps"
        torch.mps.set_per_process_memory_fraction(0.8)  # M3 Max ÏµúÏ†ÅÌôî
    elif torch.cuda.is_available():
        DEVICE = "cuda"
    else:
        DEVICE = "cpu"
        
except ImportError as e:
    raise ImportError(f"‚ùå PyTorch ÌïÑÏàò: conda install pytorch torchvision -c pytorch\nÏÑ∏Î∂Ä Ïò§Î•ò: {e}")

# PIL (ÌïÑÏàò)
try:
    from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
    PIL_AVAILABLE = True
except ImportError as e:
    raise ImportError(f"‚ùå Pillow ÌïÑÏàò: conda install pillow -c conda-forge\nÏÑ∏Î∂Ä Ïò§Î•ò: {e}")

# NumPy (ÌïÑÏàò)
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError as e:
    raise ImportError(f"‚ùå NumPy ÌïÑÏàò: conda install numpy -c conda-forge\nÏÑ∏Î∂Ä Ïò§Î•ò: {e}")

# AI Î™®Îç∏ ÎùºÏù¥Î∏åÎü¨Î¶¨Îì§ (ÏÑ†ÌÉùÏ†Å)
try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

try:
    from transformers import pipeline, CLIPProcessor, CLIPModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False

# safetensors (Diffusion Î™®Îç∏Ïö©)
try:
    import safetensors.torch as st
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False

# Î°úÍ±∞ ÏÑ§Ï†ï
logger = logging.getLogger(__name__)

# ==============================================
# üî• 3. ÎèôÏ†Å import Ìï®ÏàòÎì§ (TYPE_CHECKING Ìò∏Ìôò)
# ==============================================

def get_base_step_mixin_class():
    """BaseStepMixin ÌÅ¥ÎûòÏä§Î•º ÎèôÏ†ÅÏúºÎ°ú Í∞ÄÏ†∏Ïò§Í∏∞"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logger.error(f"‚ùå BaseStepMixin ÎèôÏ†Å import Ïã§Ìå®: {e}")
        return None

def get_model_loader():
    """ModelLoaderÎ•º ÏïàÏ†ÑÌïòÍ≤å Í∞ÄÏ†∏Ïò§Í∏∞"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.model_loader')
        get_global_loader = getattr(module, 'get_global_model_loader', None)
        if get_global_loader:
            return get_global_loader()
        else:
            ModelLoader = getattr(module, 'ModelLoader', None)
            if ModelLoader:
                return ModelLoader()
        return None
    except ImportError as e:
        logger.error(f"‚ùå ModelLoader ÎèôÏ†Å import Ïã§Ìå®: {e}")
        return None

def get_memory_manager():
    """MemoryManagerÎ•º ÏïàÏ†ÑÌïòÍ≤å Í∞ÄÏ†∏Ïò§Í∏∞"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.memory_manager')
        get_global_manager = getattr(module, 'get_global_memory_manager', None)
        if get_global_manager:
            return get_global_manager()
        return None
    except ImportError as e:
        logger.debug(f"MemoryManager ÎèôÏ†Å import Ïã§Ìå®: {e}")
        return None

def get_step_factory():
    """StepFactoryÎ•º ÏïàÏ†ÑÌïòÍ≤å Í∞ÄÏ†∏Ïò§Í∏∞"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.factories.step_factory')
        get_global_factory = getattr(module, 'get_global_step_factory', None)
        if get_global_factory:
            return get_global_factory()
        return None
    except ImportError as e:
        logger.debug(f"StepFactory ÎèôÏ†Å import Ïã§Ìå®: {e}")
        return None

# BaseStepMixin ÎèôÏ†Å Î°úÎî©
BaseStepMixin = get_base_step_mixin_class()

if BaseStepMixin is None:
    # Ìè¥Î∞± ÌÅ¥ÎûòÏä§ Ï†ïÏùò (BaseStepMixin v16.0 Ìò∏Ìôò)
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'BaseStep')
            self.step_id = kwargs.get('step_id', 0)
            self.device = kwargs.get('device', DEVICE)
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # BaseStepMixin v16.0 Ìò∏Ìôò ÏÜçÏÑ±Îì§
            self.config = type('StepConfig', (), kwargs)()
            self.dependency_manager = type('DependencyManager', (), {
                'dependency_status': type('DependencyStatus', (), {
                    'model_loader': False,
                    'step_interface': False,
                    'memory_manager': False,
                    'data_converter': False,
                    'di_container': False
                })(),
                'auto_inject_dependencies': lambda: False
            })()
            
            # ÏÑ±Îä• Î©îÌä∏Î¶≠
            self.performance_metrics = {
                'process_count': 0,
                'total_process_time': 0.0,
                'average_process_time': 0.0,
                'error_count': 0,
                'success_count': 0,
                'cache_hits': 0
            }
        
        async def initialize(self):
            self.is_initialized = True
            return True
        
        def set_model_loader(self, model_loader):
            self.model_loader = model_loader
            self.dependency_manager.dependency_status.model_loader = True
        
        def set_memory_manager(self, memory_manager):
            self.memory_manager = memory_manager
            self.dependency_manager.dependency_status.memory_manager = True
        
        def set_data_converter(self, data_converter):
            self.data_converter = data_converter
            self.dependency_manager.dependency_status.data_converter = True
        
        def set_di_container(self, di_container):
            self.di_container = di_container
            self.dependency_manager.dependency_status.di_container = True
        
        async def cleanup(self):
            pass
        
        def get_status(self):
            return {
                'step_name': self.step_name,
                'is_initialized': self.is_initialized,
                'device': self.device,
                'dependencies': {
                    'model_loader': self.dependency_manager.dependency_status.model_loader,
                    'step_interface': self.dependency_manager.dependency_status.step_interface,
                    'memory_manager': self.dependency_manager.dependency_status.memory_manager,
                    'data_converter': self.dependency_manager.dependency_status.data_converter,
                    'di_container': self.dependency_manager.dependency_status.di_container,
                },
                'version': '16.0-compatible'
            }

# ==============================================
# üî• 4. Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ Î∞è ÏÉÅÏàò Ï†ïÏùò
# ==============================================

class PoseModel(Enum):
    """Ìè¨Ï¶à Ï∂îÏ†ï Î™®Îç∏ ÌÉÄÏûÖ"""
    YOLOV8_POSE = "yolov8_pose"
    OPENPOSE = "openpose"
    DIFFUSION_POSE = "diffusion_pose"
    BODY_POSE = "body_pose"

class PoseQuality(Enum):
    """Ìè¨Ï¶à ÌíàÏßà Îì±Í∏â"""
    EXCELLENT = "excellent"     # 90-100Ï†ê
    GOOD = "good"              # 75-89Ï†ê  
    ACCEPTABLE = "acceptable"   # 60-74Ï†ê
    POOR = "poor"              # 40-59Ï†ê
    VERY_POOR = "very_poor"    # 0-39Ï†ê

# OpenPose 18 ÌÇ§Ìè¨Ïù∏Ìä∏ Ï†ïÏùò
OPENPOSE_18_KEYPOINTS = [
    "nose", "neck", "right_shoulder", "right_elbow", "right_wrist",
    "left_shoulder", "left_elbow", "left_wrist", "middle_hip", "right_hip", 
    "right_knee", "right_ankle", "left_hip", "left_knee", "left_ankle",
    "right_eye", "left_eye", "right_ear", "left_ear"
]

# ÌÇ§Ìè¨Ïù∏Ìä∏ ÏÉâÏÉÅ Î∞è Ïó∞Í≤∞ Ï†ïÎ≥¥
KEYPOINT_COLORS = [
    (255, 0, 0), (255, 85, 0), (255, 170, 0), (255, 255, 0), (170, 255, 0),
    (85, 255, 0), (0, 255, 0), (0, 255, 85), (0, 255, 170), (0, 255, 255),
    (0, 170, 255), (0, 85, 255), (0, 0, 255), (85, 0, 255), (170, 0, 255),
    (255, 0, 255), (255, 0, 170), (255, 0, 85), (255, 0, 0)
]

SKELETON_CONNECTIONS = [
    (0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7), (1, 8),
    (8, 9), (9, 10), (10, 11), (8, 12), (12, 13), (13, 14), (0, 15),
    (15, 17), (0, 16), (16, 18)
]

@dataclass
class PoseMetrics:
    """Ìè¨Ï¶à Ï∏°Ï†ï Îç∞Ïù¥ÌÑ∞"""
    keypoints: List[List[float]] = field(default_factory=list)
    confidence_scores: List[float] = field(default_factory=list)
    bbox: Tuple[int, int, int, int] = field(default_factory=lambda: (0, 0, 0, 0))
    pose_quality: PoseQuality = PoseQuality.POOR
    overall_score: float = 0.0
    
    # Ïã†Ï≤¥ Î∂ÄÏúÑÎ≥Ñ Ï†êÏàò
    head_score: float = 0.0
    torso_score: float = 0.0  
    arms_score: float = 0.0
    legs_score: float = 0.0
    
    # Í≥†Í∏â Î∂ÑÏÑù Ï†êÏàò
    symmetry_score: float = 0.0
    visibility_score: float = 0.0
    pose_angles: Dict[str, float] = field(default_factory=dict)
    body_proportions: Dict[str, float] = field(default_factory=dict)
    
    # ÏùòÎ•ò Ï∞©Ïö© Ï†ÅÌï©ÏÑ±
    suitable_for_fitting: bool = False
    issues: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    
    # Ï≤òÎ¶¨ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
    model_used: str = ""
    processing_time: float = 0.0
    image_resolution: Tuple[int, int] = field(default_factory=lambda: (0, 0))
    ai_confidence: float = 0.0

# ==============================================
# üî• 5. SmartModelPathMapper (Ïã§Ï†ú ÌååÏùº ÌÉêÏßÄ)
# ==============================================

class SmartModelPathMapper:
    """Ïã§Ï†ú ÌååÏùº ÏúÑÏπòÎ•º ÎèôÏ†ÅÏúºÎ°ú Ï∞æÏïÑÏÑú Îß§ÌïëÌïòÎäî ÏãúÏä§ÌÖú"""
    
    def __init__(self, ai_models_root: str = "ai_models"):
        self.ai_models_root = Path(ai_models_root)
        self.model_cache = {}  # Ï∫êÏãúÎ°ú ÏÑ±Îä• ÏµúÏ†ÅÌôî
        self.logger = logging.getLogger(f"{__name__}.SmartModelPathMapper")
    
    def _search_models(self, model_files: Dict[str, List[str]], search_priority: List[str]) -> Dict[str, Optional[Path]]:
        """Î™®Îç∏ ÌååÏùºÎì§ÏùÑ Ïö∞ÏÑ†ÏàúÏúÑ Í≤ΩÎ°úÏóêÏÑú Í≤ÄÏÉâ"""
        found_paths = {}
        
        for model_name, filenames in model_files.items():
            found_path = None
            for filename in filenames:
                for search_path in search_priority:
                    candidate_path = self.ai_models_root / search_path / filename
                    if candidate_path.exists() and candidate_path.is_file():
                        found_path = candidate_path
                        self.logger.info(f"‚úÖ {model_name} Î™®Îç∏ Î∞úÍ≤¨: {found_path}")
                        break
                if found_path:
                    break
            found_paths[model_name] = found_path
            
            if not found_path:
                self.logger.warning(f"‚ö†Ô∏è {model_name} Î™®Îç∏ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå: {filenames}")
        
        return found_paths

class Step02ModelMapper(SmartModelPathMapper):
    """Step 02 Pose Estimation Ï†ÑÏö© ÎèôÏ†Å Í≤ΩÎ°ú Îß§Ìïë"""
    
    def get_step02_model_paths(self) -> Dict[str, Optional[Path]]:
        """Step 02 Î™®Îç∏ Í≤ΩÎ°ú ÏûêÎèô ÌÉêÏßÄ - HRNet Ìè¨Ìï®"""
        model_files = {
            "yolov8": ["yolov8n-pose.pt", "yolov8s-pose.pt"],
            "openpose": ["openpose.pth", "body_pose_model.pth"],
            "hrnet": [
                "hrnet_w48_coco_256x192.pth", 
                "hrnet_w32_coco_256x192.pth", 
                "pose_hrnet_w48_256x192.pth",
                "hrnet_w48_256x192.pth"
            ],  # üî• HRNet ÌååÏùºÎì§ Ï∂îÍ∞Ä üî•
            "diffusion": ["diffusion_pytorch_model.safetensors", "diffusion_pytorch_model.bin"],
            "body_pose": ["body_pose_model.pth"]
        }
        
        search_priority = [
            "step_02_pose_estimation/",
            "step_02_pose_estimation/ultra_models/",
            "step_06_virtual_fitting/ootdiffusion/checkpoints/openpose/",
            "checkpoints/step_02_pose_estimation/",
            "pose_estimation/",
            "hrnet/",  # üî• HRNet Ï†ÑÏö© Ìè¥Îçî üî•
            "checkpoints/hrnet/",  # üî• HRNet Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ üî•
            ""  # Î£®Ìä∏ ÎîîÎ†âÌÜ†Î¶¨ÎèÑ Í≤ÄÏÉâ
        ]
        
        return self._search_models(model_files, search_priority)


# ==============================================
# üî• 6. ÌååÏù¥ÌîÑÎùºÏù∏ Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ (StepInterface Ìò∏Ìôò)
# ==============================================

@dataclass
class PipelineStepResult:
    """ÌååÏù¥ÌîÑÎùºÏù∏ Step Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞"""
    step_id: int
    step_name: str
    success: bool
    error: Optional[str] = None
    
    # Îã§Ïùå StepÎì§Î°ú Ï†ÑÎã¨Ìï† Îç∞Ïù¥ÌÑ∞
    for_step_03: Dict[str, Any] = field(default_factory=dict)
    for_step_04: Dict[str, Any] = field(default_factory=dict)
    for_step_05: Dict[str, Any] = field(default_factory=dict)
    for_step_06: Dict[str, Any] = field(default_factory=dict)
    for_step_07: Dict[str, Any] = field(default_factory=dict)
    for_step_08: Dict[str, Any] = field(default_factory=dict)
    
    # Ïù¥Ï†Ñ Îã®Í≥Ñ Îç∞Ïù¥ÌÑ∞ Î≥¥Ï°¥
    previous_data: Dict[str, Any] = field(default_factory=dict)
    original_data: Dict[str, Any] = field(default_factory=dict)
    
    # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
    metadata: Dict[str, Any] = field(default_factory=dict)
    processing_time: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """ÎîïÏÖîÎÑàÎ¶¨Î°ú Î≥ÄÌôò"""
        return asdict(self)

@dataclass 
class PipelineInputData:
    """ÌååÏù¥ÌîÑÎùºÏù∏ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞"""
    person_image: Union[np.ndarray, Image.Image, str]
    clothing_image: Optional[Union[np.ndarray, Image.Image, str]] = None
    session_id: Optional[str] = None
    config: Dict[str, Any] = field(default_factory=dict)

# StepInterface ÎèôÏ†Å Î°úÎî©
def get_step_interface_class():
    """StepInterface ÌÅ¥ÎûòÏä§Î•º ÎèôÏ†ÅÏúºÎ°ú Í∞ÄÏ†∏Ïò§Í∏∞"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.interface.step_interface')
        return getattr(module, 'StepInterface', None)
    except ImportError as e:
        logger.error(f"‚ùå StepInterface ÎèôÏ†Å import Ïã§Ìå®: {e}")
        return None

StepInterface = get_step_interface_class()

if StepInterface is None:
    # Ìè¥Î∞± StepInterface Ï†ïÏùò
    class StepInterface:
        def __init__(self, step_id: int, step_name: str, config: Dict[str, Any], **kwargs):
            self.step_id = step_id
            self.step_name = step_name
            self.config = config
            self.pipeline_mode = config.get("pipeline_mode", False)
        
        async def process_pipeline(self, input_data: PipelineStepResult) -> PipelineStepResult:
            """ÌååÏù¥ÌîÑÎùºÏù∏ Î™®Îìú Ï≤òÎ¶¨ (Ìè¥Î∞±)"""
            return PipelineStepResult(
                step_id=self.step_id,
                step_name=self.step_name,
                success=False,
                error="StepInterface Ìè¥Î∞± Î™®Îìú"
            )

# ==============================================
# üî• 7. Ïã§Ï†ú AI Î™®Îç∏ ÌÅ¥ÎûòÏä§Îì§ (Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© + Ï∂îÎ°†)
# ==============================================

class RealYOLOv8PoseModel:
    """YOLOv8 6.5MB Ïã§ÏãúÍ∞Ñ Ìè¨Ï¶à Í≤ÄÏ∂ú - Ïã§Ï†ú AI Ï∂îÎ°†"""
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealYOLOv8PoseModel")
    
    def load_yolo_checkpoint(self) -> bool:
        """Ïã§Ï†ú YOLOv8-Pose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©"""
        try:
            if not ULTRALYTICS_AVAILABLE:
                self.logger.error("‚ùå ultralytics ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏóÜÏùå")
                return False
            
            # YOLOv8 Ìè¨Ï¶à Î™®Îç∏ Î°úÎî©
            self.model = YOLO(str(self.model_path))
            
            # MPS ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï (M3 Max ÏµúÏ†ÅÌôî)
            if self.device == "mps" and torch.backends.mps.is_available():
                # YOLOv8ÏùÄ ÏûêÎèôÏúºÎ°ú MPS ÏÇ¨Ïö©
                pass
            elif self.device == "cuda":
                self.model.to("cuda")
            
            self.loaded = True
            self.logger.info(f"‚úÖ YOLOv8-Pose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏôÑÎ£å: {self.model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå YOLOv8 Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    def detect_poses_realtime(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """Ïã§ÏãúÍ∞Ñ Ìè¨Ï¶à Í≤ÄÏ∂ú (Ïã§Ï†ú AI Ï∂îÎ°†)"""
        if not self.loaded:
            raise RuntimeError("YOLOv8 Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏùå")
        
        start_time = time.time()
        
        try:
            # Ïã§Ï†ú AI Ï∂îÎ°† Ïã§Ìñâ
            results = self.model(image, verbose=False)
            
            poses = []
            for result in results:
                if hasattr(result, 'keypoints') and result.keypoints is not None:
                    keypoints = result.keypoints.data  # [N, 17, 3] (x, y, confidence)
                    
                    for person_kpts in keypoints:
                        # YOLOv8ÏùÄ COCO 17 ÌòïÏãùÏù¥ÎØÄÎ°ú OpenPose 18Î°ú Î≥ÄÌôò
                        openpose_kpts = self._convert_coco17_to_openpose18(person_kpts.cpu().numpy())
                        
                        pose_data = {
                            "keypoints": openpose_kpts,
                            "bbox": result.boxes.xyxy.cpu().numpy()[0] if result.boxes else None,
                            "confidence": float(result.boxes.conf.mean()) if result.boxes else 0.0
                        }
                        poses.append(pose_data)
            
            processing_time = time.time() - start_time
            
            return {
                "poses": poses,
                "keypoints": poses[0]["keypoints"] if poses else [],
                "num_persons": len(poses),
                "processing_time": processing_time,
                "model_type": "yolov8_pose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå YOLOv8 AI Ï∂îÎ°† Ïã§Ìå®: {e}")
            return {
                "poses": [],
                "keypoints": [],
                "num_persons": 0,
                "processing_time": time.time() - start_time,
                "model_type": "yolov8_pose",
                "success": False,
                "error": str(e)
            }
    
    def _convert_coco17_to_openpose18(self, coco_keypoints: np.ndarray) -> List[List[float]]:
        """COCO 17 Ìè¨Îß∑ÏùÑ OpenPose 18Î°ú Î≥ÄÌôò"""
        # COCO 17 ‚Üí OpenPose 18 Îß§Ìïë
        coco_to_openpose_mapping = {
            0: 0,   # nose
            5: 2,   # left_shoulder ‚Üí right_shoulder (Ï¢åÏö∞ Î∞òÏ†Ñ)
            6: 5,   # right_shoulder ‚Üí left_shoulder
            7: 3,   # left_elbow ‚Üí right_elbow
            8: 6,   # right_elbow ‚Üí left_elbow
            9: 4,   # left_wrist ‚Üí right_wrist
            10: 7,  # right_wrist ‚Üí left_wrist
            11: 9,  # left_hip ‚Üí right_hip
            12: 12, # right_hip ‚Üí left_hip
            13: 10, # left_knee ‚Üí right_knee
            14: 13, # right_knee ‚Üí left_knee
            15: 11, # left_ankle ‚Üí right_ankle
            16: 14, # right_ankle ‚Üí left_ankle
            1: 15,  # left_eye ‚Üí right_eye
            2: 16,  # right_eye ‚Üí left_eye
            3: 17,  # left_ear ‚Üí right_ear
            4: 18   # right_ear ‚Üí left_ear
        }
        
        openpose_keypoints = [[0.0, 0.0, 0.0] for _ in range(19)]  # 18 + neck
        
        # neck Í≥ÑÏÇ∞ (Ïñ¥Íπ® Ï§ëÏ†ê)
        if len(coco_keypoints) > 6:
            left_shoulder = coco_keypoints[5]
            right_shoulder = coco_keypoints[6]
            if left_shoulder[2] > 0.1 and right_shoulder[2] > 0.1:
                neck_x = (left_shoulder[0] + right_shoulder[0]) / 2
                neck_y = (left_shoulder[1] + right_shoulder[1]) / 2
                neck_conf = (left_shoulder[2] + right_shoulder[2]) / 2
                openpose_keypoints[1] = [float(neck_x), float(neck_y), float(neck_conf)]
        
        # ÎÇòÎ®∏ÏßÄ ÌÇ§Ìè¨Ïù∏Ìä∏ Îß§Ìïë
        for coco_idx, openpose_idx in coco_to_openpose_mapping.items():
            if coco_idx < len(coco_keypoints):
                openpose_keypoints[openpose_idx] = [
                    float(coco_keypoints[coco_idx][0]),
                    float(coco_keypoints[coco_idx][1]),
                    float(coco_keypoints[coco_idx][2])
                ]
        
        return openpose_keypoints[:18]  # OpenPose 18Í∞úÎßå Î∞òÌôò

class RealOpenPoseModel:
    """OpenPose 97.8MB Ï†ïÎ∞Ä Ìè¨Ï¶à Í≤ÄÏ∂ú - Ïã§Ï†ú AI Ï∂îÎ°†"""
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealOpenPoseModel")
    
    def load_openpose_checkpoint(self) -> bool:
        """Ïã§Ï†ú OpenPose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©"""
        try:
            # PyTorchÎ°ú ÏßÅÏ†ë Î°úÎî©
            checkpoint = torch.load(str(self.model_path), map_location=self.device, weights_only=True)
            
            # OpenPose ÎÑ§Ìä∏ÏõåÌÅ¨ Íµ¨Ï°∞ ÏÉùÏÑ±
            self.model = self._create_openpose_network()
            
            # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏóêÏÑú state_dict Î°úÎî©
            if 'state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['state_dict'], strict=False)
            elif 'model' in checkpoint:
                self.model.load_state_dict(checkpoint['model'], strict=False)
            else:
                self.model.load_state_dict(checkpoint, strict=False)
            
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            
            self.logger.info(f"‚úÖ OpenPose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏôÑÎ£å: {self.model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå OpenPose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®: {e}")
            # Ìè¥Î∞±: Í∞ÑÎã®Ìïú Î™®Îç∏ ÏÉùÏÑ±
            self.model = self._create_simple_pose_model()
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            self.logger.warning("‚ö†Ô∏è OpenPose Ìè¥Î∞± Î™®Îç∏ ÏÇ¨Ïö©")
            return True
    
    def _create_openpose_network(self) -> nn.Module:
        """OpenPose ÎÑ§Ìä∏ÏõåÌÅ¨ Íµ¨Ï°∞ ÏÉùÏÑ±"""
        class OpenPoseNetwork(nn.Module):
            def __init__(self):
                super().__init__()
                # VGG19 Î∞±Î≥∏
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(),
                    nn.MaxPool2d(2, 2),
                    nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(),
                    nn.MaxPool2d(2, 2),
                    nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(),
                    nn.MaxPool2d(2, 2),
                    nn.Conv2d(256, 512, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(),
                )
                
                # PAF (Part Affinity Fields) Î∏åÎûúÏπò
                self.paf_branch = nn.Sequential(
                    nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(128, 38, 1, 1, 0)  # 19 connections * 2
                )
                
                # ÌÇ§Ìè¨Ïù∏Ìä∏ ÌûàÌä∏Îßµ Î∏åÎûúÏπò
                self.keypoint_branch = nn.Sequential(
                    nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(128, 19, 1, 1, 0)  # 18 keypoints + background
                )
            
            def forward(self, x):
                features = self.backbone(x)
                paf = self.paf_branch(features)
                keypoints = self.keypoint_branch(features)
                return keypoints, paf
        
        return OpenPoseNetwork()
    
    def _create_simple_pose_model(self) -> nn.Module:
        """Í∞ÑÎã®Ìïú Ìè¨Ï¶à Î™®Îç∏ (Ìè¥Î∞±Ïö©)"""
        class SimplePoseModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.MaxPool2d(3, 2, 1),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),
                    nn.AdaptiveAvgPool2d((1, 1))
                )
                self.keypoint_head = nn.Linear(256, 18 * 3)  # 18 keypoints * (x, y, conf)
            
            def forward(self, x):
                features = self.backbone(x)
                features = features.flatten(1)
                keypoints = self.keypoint_head(features)
                return keypoints.view(-1, 18, 3)
        
        return SimplePoseModel()
    
    def detect_keypoints_precise(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """Ï†ïÎ∞Ä ÌÇ§Ìè¨Ïù∏Ìä∏ Í≤ÄÏ∂ú (Ïã§Ï†ú AI Ï∂îÎ°†)"""
        if not self.loaded:
            raise RuntimeError("OpenPose Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏùå")
        
        start_time = time.time()
        
        try:
            # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
            if isinstance(image, Image.Image):
                image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
            elif isinstance(image, np.ndarray):
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(self.device) / 255.0
            else:
                image_tensor = image.to(self.device)
            
            # Ïã§Ï†ú AI Ï∂îÎ°† Ïã§Ìñâ
            with torch.no_grad():
                if hasattr(self.model, 'keypoint_head'):  # Simple model
                    output = self.model(image_tensor)
                    keypoints = output[0].cpu().numpy()
                    
                    # ÌÇ§Ìè¨Ïù∏Ìä∏ Ï†ïÍ∑úÌôî (Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Í∏∞Ï§Ä)
                    h, w = image_tensor.shape[-2:]
                    keypoints_list = []
                    for kp in keypoints:
                        x, y, conf = float(kp[0] * w), float(kp[1] * h), float(torch.sigmoid(torch.tensor(kp[2])))
                        keypoints_list.append([x, y, conf])
                    
                else:  # OpenPose model
                    keypoint_heatmaps, paf = self.model(image_tensor)
                    keypoints_list = self._extract_keypoints_from_heatmaps(keypoint_heatmaps[0])
            
            processing_time = time.time() - start_time
            
            return {
                "keypoints": keypoints_list,
                "processing_time": processing_time,
                "model_type": "openpose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå OpenPose AI Ï∂îÎ°† Ïã§Ìå®: {e}")
            return {
                "keypoints": [],
                "processing_time": time.time() - start_time,
                "model_type": "openpose",
                "success": False,
                "error": str(e)
            }
    
    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """ÌûàÌä∏ÎßµÏóêÏÑú ÌÇ§Ìè¨Ïù∏Ìä∏ Ï∂îÏ∂ú"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(18):  # 18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏
            heatmap = heatmaps[i].cpu().numpy()
            
            # ÏµúÎåÄÍ∞í ÏúÑÏπò Ï∞æÍ∏∞
            y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            confidence = float(heatmap[y_idx, x_idx])
            
            # Ï¢åÌëú Ï†ïÍ∑úÌôî
            x = float(x_idx / w * 512)  # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Î°ú Î≥ÄÌôò
            y = float(y_idx / h * 512)
            
            keypoints.append([x, y, confidence])
        
        return keypoints

class RealDiffusionPoseModel:
    """Diffusion 1378MB Í≥†ÌíàÏßà Ìè¨Ï¶à ÏÉùÏÑ± - Ïã§Ï†ú AI Ï∂îÎ°†"""
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealDiffusionPoseModel")
    
    def load_diffusion_checkpoint(self) -> bool:
        """Ïã§Ï†ú 1.4GB Diffusion Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©"""
        try:
            if SAFETENSORS_AVAILABLE and str(self.model_path).endswith('.safetensors'):
                # safetensors ÌååÏùº Î°úÎî©
                checkpoint = st.load_file(str(self.model_path))
            else:
                # ÏùºÎ∞ò PyTorch ÌååÏùº Î°úÎî©
                checkpoint = torch.load(str(self.model_path), map_location=self.device, weights_only=True)
            
            # Diffusion UNet ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉùÏÑ±
            self.model = self._create_diffusion_unet()
            
            # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© (ÌÇ§ Îß§Ïπ≠)
            model_dict = self.model.state_dict()
            filtered_dict = {}
            
            for k, v in checkpoint.items():
                if k in model_dict and model_dict[k].shape == v.shape:
                    filtered_dict[k] = v
            
            model_dict.update(filtered_dict)
            self.model.load_state_dict(model_dict, strict=False)
            
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            
            self.logger.info(f"‚úÖ Diffusion Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏôÑÎ£å: {self.model_path} ({len(filtered_dict)} layers)")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Diffusion Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®: {e}")
            # Ìè¥Î∞±: Í∞ÑÎã®Ìïú Î™®Îç∏
            self.model = self._create_simple_diffusion_model()
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            self.logger.warning("‚ö†Ô∏è Diffusion Ìè¥Î∞± Î™®Îç∏ ÏÇ¨Ïö©")
            return True
    
    def _create_diffusion_unet(self) -> nn.Module:
        """Diffusion UNet ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉùÏÑ±"""
        class DiffusionUNet(nn.Module):
            def __init__(self):
                super().__init__()
                # Í∞ÑÏÜåÌôîÎêú UNet Íµ¨Ï°∞
                self.encoder = nn.Sequential(
                    nn.Conv2d(3, 64, 3, 1, 1), nn.GroupNorm(8, 64), nn.SiLU(),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.GroupNorm(16, 128), nn.SiLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.GroupNorm(32, 256), nn.SiLU(),
                    nn.Conv2d(256, 512, 3, 2, 1), nn.GroupNorm(32, 512), nn.SiLU(),
                )
                
                self.middle = nn.Sequential(
                    nn.Conv2d(512, 512, 3, 1, 1), nn.GroupNorm(32, 512), nn.SiLU(),
                    nn.Conv2d(512, 512, 3, 1, 1), nn.GroupNorm(32, 512), nn.SiLU(),
                )
                
                self.decoder = nn.Sequential(
                    nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.GroupNorm(32, 256), nn.SiLU(),
                    nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.GroupNorm(16, 128), nn.SiLU(),
                    nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.GroupNorm(8, 64), nn.SiLU(),
                    nn.Conv2d(64, 18 * 3, 3, 1, 1)  # 18 keypoints * 3
                )
            
            def forward(self, x):
                encoded = self.encoder(x)
                middle = self.middle(encoded)
                decoded = self.decoder(middle)
                return decoded
        
        return DiffusionUNet()
    
    def _create_simple_diffusion_model(self) -> nn.Module:
        """Í∞ÑÎã®Ìïú Diffusion Î™®Îç∏ (Ìè¥Î∞±Ïö©)"""
        return self._create_diffusion_unet()  # Í∞ôÏùÄ Íµ¨Ï°∞ ÏÇ¨Ïö©
    
    def enhance_pose_quality(self, keypoints: Union[torch.Tensor, List[List[float]]], image: Union[torch.Tensor, Image.Image] = None) -> Dict[str, Any]:
        """Ìè¨Ï¶à ÌíàÏßà Ìñ•ÏÉÅ (Ïã§Ï†ú AI Ï∂îÎ°†)"""
        if not self.loaded:
            raise RuntimeError("Diffusion Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏùå")
        
        start_time = time.time()
        
        try:
            # ÌÇ§Ìè¨Ïù∏Ìä∏Î•º ÌÖêÏÑúÎ°ú Î≥ÄÌôò
            if isinstance(keypoints, list):
                keypoints_tensor = torch.tensor(keypoints, dtype=torch.float32, device=self.device)
            else:
                keypoints_tensor = keypoints.to(self.device)
            
            # ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ± (Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞)
            if image is None:
                batch_size = 1
                image_tensor = torch.randn(batch_size, 3, 512, 512, device=self.device)
            else:
                if isinstance(image, Image.Image):
                    image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
                else:
                    image_tensor = image.to(self.device)
            
            # Ïã§Ï†ú Diffusion AI Ï∂îÎ°†
            with torch.no_grad():
                enhanced_output = self.model(image_tensor)
                
                # Ï∂úÎ†• Ìï¥ÏÑù (18 keypoints * 3 Ï±ÑÎÑê)
                b, c, h, w = enhanced_output.shape
                enhanced_keypoints = enhanced_output.view(b, 18, 3, h, w)
                
                # ÌÇ§Ìè¨Ïù∏Ìä∏ Ï∂îÏ∂ú (ÏµúÎåÄÍ∞í ÏúÑÏπò)
                enhanced_kpts = []
                for i in range(18):
                    for j in range(3):  # x, y, confidence
                        channel = enhanced_keypoints[0, i, j]
                        if j < 2:  # x, y Ï¢åÌëú
                            max_val = torch.max(channel)
                            enhanced_kpts.append(float(max_val * 512))  # Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Î°ú Ï†ïÍ∑úÌôî
                        else:  # confidence
                            enhanced_kpts.append(float(torch.sigmoid(torch.mean(channel))))
                
                # 18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏Î°ú Ïû¨Íµ¨ÏÑ±
                result_keypoints = []
                for i in range(18):
                    x = enhanced_kpts[i*3]
                    y = enhanced_kpts[i*3+1]
                    conf = enhanced_kpts[i*3+2]
                    result_keypoints.append([x, y, conf])
            
            processing_time = time.time() - start_time
            
            return {
                "enhanced_keypoints": result_keypoints,
                "processing_time": processing_time,
                "model_type": "diffusion_pose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Diffusion AI Ï∂îÎ°† Ïã§Ìå®: {e}")
            # Ìè¥Î∞±: ÏõêÎ≥∏ ÌÇ§Ìè¨Ïù∏Ìä∏ Î∞òÌôò
            if isinstance(keypoints, list):
                original_keypoints = keypoints
            else:
                original_keypoints = keypoints.cpu().numpy().tolist()
            
            return {
                "enhanced_keypoints": original_keypoints,
                "processing_time": time.time() - start_time,
                "model_type": "diffusion_pose",
                "success": False,
                "error": str(e)
            }


# ==============================================
# üî• RealHRNetModel ÏôÑÏ†Ñ Íµ¨ÌòÑ (Í≥†Ï†ïÎ∞Ä Ìè¨Ï¶à Ï∂îÏ†ï)
# ==============================================

class BasicBlock(nn.Module):
    """HRNet BasicBlock Íµ¨ÌòÑ"""
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, momentum=0.1)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes, momentum=0.1)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    """HRNet Bottleneck Íµ¨ÌòÑ"""
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, momentum=0.1)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes, momentum=0.1)
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion, momentum=0.1)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class HighResolutionModule(nn.Module):
    """HRNet Í≥†Ìï¥ÏÉÅÎèÑ Î™®Îìà"""
    
    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,
                 num_channels, fuse_method, multi_scale_output=True):
        super(HighResolutionModule, self).__init__()
        self._check_branches(num_branches, blocks, num_blocks, num_inchannels, num_channels)

        self.num_inchannels = num_inchannels
        self.fuse_method = fuse_method
        self.num_branches = num_branches

        self.multi_scale_output = multi_scale_output

        self.branches = self._make_branches(num_branches, blocks, num_blocks, num_channels)
        self.fuse_layers = self._make_fuse_layers()
        self.relu = nn.ReLU(True)

    def _check_branches(self, num_branches, blocks, num_blocks, num_inchannels, num_channels):
        if num_branches != len(num_blocks):
            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(num_branches, len(num_blocks))
            raise ValueError(error_msg)

        if num_branches != len(num_channels):
            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(num_branches, len(num_channels))
            raise ValueError(error_msg)

        if num_branches != len(num_inchannels):
            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(num_branches, len(num_inchannels))
            raise ValueError(error_msg)

    def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1):
        downsample = None
        if stride != 1 or self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(
                    self.num_inchannels[branch_index],
                    num_channels[branch_index] * block.expansion,
                    kernel_size=1, stride=stride, bias=False
                ),
                nn.BatchNorm2d(num_channels[branch_index] * block.expansion, momentum=0.1),
            )

        layers = []
        layers.append(
            block(
                self.num_inchannels[branch_index],
                num_channels[branch_index],
                stride,
                downsample
            )
        )
        self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion
        for i in range(1, num_blocks[branch_index]):
            layers.append(
                block(
                    self.num_inchannels[branch_index],
                    num_channels[branch_index]
                )
            )

        return nn.Sequential(*layers)

    def _make_branches(self, num_branches, block, num_blocks, num_channels):
        branches = []

        for i in range(num_branches):
            branches.append(
                self._make_one_branch(i, block, num_blocks, num_channels)
            )

        return nn.ModuleList(branches)

    def _make_fuse_layers(self):
        if self.num_branches == 1:
            return None

        num_branches = self.num_branches
        num_inchannels = self.num_inchannels
        fuse_layers = []
        for i in range(num_branches if self.multi_scale_output else 1):
            fuse_layer = []
            for j in range(num_branches):
                if j > i:
                    fuse_layer.append(
                        nn.Sequential(
                            nn.Conv2d(
                                num_inchannels[j],
                                num_inchannels[i],
                                1, 1, 0, bias=False
                            ),
                            nn.BatchNorm2d(num_inchannels[i]),
                            nn.Upsample(scale_factor=2**(j-i), mode='nearest')
                        )
                    )
                elif j == i:
                    fuse_layer.append(None)
                else:
                    conv3x3s = []
                    for k in range(i-j):
                        if k == i - j - 1:
                            num_outchannels_conv3x3 = num_inchannels[i]
                            conv3x3s.append(
                                nn.Sequential(
                                    nn.Conv2d(
                                        num_inchannels[j],
                                        num_outchannels_conv3x3,
                                        3, 2, 1, bias=False
                                    ),
                                    nn.BatchNorm2d(num_outchannels_conv3x3)
                                )
                            )
                        else:
                            num_outchannels_conv3x3 = num_inchannels[j]
                            conv3x3s.append(
                                nn.Sequential(
                                    nn.Conv2d(
                                        num_inchannels[j],
                                        num_outchannels_conv3x3,
                                        3, 2, 1, bias=False
                                    ),
                                    nn.BatchNorm2d(num_outchannels_conv3x3),
                                    nn.ReLU(True)
                                )
                            )
                    fuse_layer.append(nn.Sequential(*conv3x3s))
            fuse_layers.append(nn.ModuleList(fuse_layer))

        return nn.ModuleList(fuse_layers)

    def get_num_inchannels(self):
        return self.num_inchannels

    def forward(self, x):
        if self.num_branches == 1:
            return [self.branches[0](x[0])]

        for i in range(self.num_branches):
            x[i] = self.branches[i](x[i])

        x_fuse = []

        for i in range(len(self.fuse_layers)):
            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])
            for j in range(1, self.num_branches):
                if i == j:
                    y = y + x[j]
                else:
                    y = y + self.fuse_layers[i][j](x[j])
            x_fuse.append(self.relu(y))

        return x_fuse


class RealHRNetModel(nn.Module):
    """Ïã§Ï†ú HRNet Í≥†Ï†ïÎ∞Ä Ìè¨Ï¶à Ï∂îÏ†ï Î™®Îç∏ (hrnet_w48_coco_256x192.pth ÌôúÏö©)"""
    
    def __init__(self, cfg=None, **kwargs):
        super(RealHRNetModel, self).__init__()
        
        # Î™®Îç∏ Ï†ïÎ≥¥
        self.model_name = "RealHRNetModel"
        self.version = "2.0"
        self.parameter_count = 0
        self.is_loaded = False
        
        # HRNet-W48 Í∏∞Î≥∏ ÏÑ§Ï†ï
        if cfg is None:
            cfg = {
                'MODEL': {
                    'EXTRA': {
                        'STAGE1': {
                            'NUM_CHANNELS': [64],
                            'BLOCK': 'BOTTLENECK',
                            'NUM_BLOCKS': [4]
                        },
                        'STAGE2': {
                            'NUM_MODULES': 1,
                            'NUM_BRANCHES': 2,
                            'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [4, 4],
                            'NUM_CHANNELS': [48, 96]
                        },
                        'STAGE3': {
                            'NUM_MODULES': 4,
                            'NUM_BRANCHES': 3,
                            'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [4, 4, 4],
                            'NUM_CHANNELS': [48, 96, 192]
                        },
                        'STAGE4': {
                            'NUM_MODULES': 3,
                            'NUM_BRANCHES': 4,
                            'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [4, 4, 4, 4],
                            'NUM_CHANNELS': [48, 96, 192, 384]
                        }
                    }
                }
            }
        
        self.cfg = cfg
        extra = cfg['MODEL']['EXTRA']
        
        # stem net
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64, momentum=0.1)
        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(64, momentum=0.1)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self._make_layer(Bottleneck, 64, 64, extra['STAGE1']['NUM_BLOCKS'][0])

        # stage 2
        self.stage2_cfg = extra['STAGE2']
        num_channels = self.stage2_cfg['NUM_CHANNELS']
        block = BasicBlock
        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]
        self.transition1 = self._make_transition_layer([256], num_channels)
        self.stage2, pre_stage_channels = self._make_stage(
            self.stage2_cfg, num_channels)

        # stage 3
        self.stage3_cfg = extra['STAGE3']
        num_channels = self.stage3_cfg['NUM_CHANNELS']
        block = BasicBlock
        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]
        self.transition2 = self._make_transition_layer(pre_stage_channels, num_channels)
        self.stage3, pre_stage_channels = self._make_stage(
            self.stage3_cfg, num_channels)

        # stage 4
        self.stage4_cfg = extra['STAGE4']
        num_channels = self.stage4_cfg['NUM_CHANNELS']
        block = BasicBlock
        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]
        self.transition3 = self._make_transition_layer(pre_stage_channels, num_channels)
        self.stage4, pre_stage_channels = self._make_stage(
            self.stage4_cfg, num_channels, multi_scale_output=False)

        # ÏµúÏ¢Ö Î†àÏù¥Ïñ¥ (18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏ Ï∂úÎ†•)
        self.final_layer = nn.Conv2d(
            in_channels=pre_stage_channels[0],
            out_channels=18,  # OpenPose 18 ÌÇ§Ìè¨Ïù∏Ìä∏
            kernel_size=1,
            stride=1,
            padding=0
        )

        # ÌååÎùºÎØ∏ÌÑ∞ Ïàò Í≥ÑÏÇ∞
        self.parameter_count = self._count_parameters()

    def _make_layer(self, block, inplanes, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion, momentum=0.1),
            )

        layers = []
        layers.append(block(inplanes, planes, stride, downsample))
        inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(inplanes, planes))

        return nn.Sequential(*layers)

    def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer):
        num_branches_cur = len(num_channels_cur_layer)
        num_branches_pre = len(num_channels_pre_layer)

        transition_layers = []
        for i in range(num_branches_cur):
            if i < num_branches_pre:
                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:
                    transition_layers.append(
                        nn.Sequential(
                            nn.Conv2d(num_channels_pre_layer[i],
                                      num_channels_cur_layer[i],
                                      3, 1, 1, bias=False),
                            nn.BatchNorm2d(num_channels_cur_layer[i]),
                            nn.ReLU(inplace=True)
                        )
                    )
                else:
                    transition_layers.append(None)
            else:
                conv3x3s = []
                for j in range(i+1-num_branches_pre):
                    inchannels = num_channels_pre_layer[-1]
                    outchannels = num_channels_cur_layer[i] if j == i-num_branches_pre else inchannels
                    conv3x3s.append(
                        nn.Sequential(
                            nn.Conv2d(inchannels, outchannels, 3, 2, 1, bias=False),
                            nn.BatchNorm2d(outchannels),
                            nn.ReLU(inplace=True)
                        )
                    )
                transition_layers.append(nn.Sequential(*conv3x3s))

        return nn.ModuleList(transition_layers)

    def _make_stage(self, layer_config, num_inchannels, multi_scale_output=True):
        num_modules = layer_config['NUM_MODULES']
        num_branches = layer_config['NUM_BRANCHES']
        num_blocks = layer_config['NUM_BLOCKS']
        num_channels = layer_config['NUM_CHANNELS']
        block = BasicBlock
        fuse_method = 'SUM'

        modules = []
        for i in range(num_modules):
            # multi_scale_output is only used last module
            if not multi_scale_output and i == num_modules - 1:
                reset_multi_scale_output = False
            else:
                reset_multi_scale_output = True

            modules.append(
                HighResolutionModule(
                    num_branches,
                    block,
                    num_blocks,
                    num_inchannels,
                    num_channels,
                    fuse_method,
                    reset_multi_scale_output
                )
            )
            num_inchannels = modules[-1].get_num_inchannels()

        return nn.Sequential(*modules), num_inchannels

    def _count_parameters(self):
        """ÌååÎùºÎØ∏ÌÑ∞ Ïàò Í≥ÑÏÇ∞"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

    def forward(self, x):
        """HRNet ÏàúÏ†ÑÌåå"""
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.layer1(x)

        x_list = []
        for i in range(self.stage2_cfg['NUM_BRANCHES']):
            if self.transition1[i] is not None:
                x_list.append(self.transition1[i](x))
            else:
                x_list.append(x)
        y_list = self.stage2(x_list)

        x_list = []
        for i in range(self.stage3_cfg['NUM_BRANCHES']):
            if self.transition2[i] is not None:
                x_list.append(self.transition2[i](y_list[-1]))
            else:
                x_list.append(y_list[i])
        y_list = self.stage3(x_list)

        x_list = []
        for i in range(self.stage4_cfg['NUM_BRANCHES']):
            if self.transition3[i] is not None:
                x_list.append(self.transition3[i](y_list[-1]))
            else:
                x_list.append(y_list[i])
        y_list = self.stage4(x_list)

        x = self.final_layer(y_list[0])
        return x

    def detect_high_precision_pose(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """Í≥†Ï†ïÎ∞Ä Ìè¨Ï¶à Í≤ÄÏ∂ú (Ïã§Ï†ú AI Ï∂îÎ°†)"""
        if not self.is_loaded:
            raise RuntimeError("HRNet Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏùå")
        
        start_time = time.time()
        
        try:
            # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
            if isinstance(image, Image.Image):
                image_tensor = transforms.ToTensor()(image).unsqueeze(0).to(next(self.parameters()).device)
            elif isinstance(image, np.ndarray):
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(next(self.parameters()).device) / 255.0
            else:
                image_tensor = image.to(next(self.parameters()).device)
            
            # ÏûÖÎ†• ÌÅ¨Í∏∞ Ï†ïÍ∑úÌôî (256x192)
            image_tensor = F.interpolate(image_tensor, size=(256, 192), mode='bilinear', align_corners=False)
            
            # Ïã§Ï†ú HRNet AI Ï∂îÎ°† Ïã§Ìñâ
            with torch.no_grad():
                heatmaps = self(image_tensor)  # [1, 18, 64, 48]
            
            # ÌûàÌä∏ÎßµÏóêÏÑú ÌÇ§Ìè¨Ïù∏Ìä∏ Ï∂îÏ∂ú
            keypoints = self._extract_keypoints_from_heatmaps(heatmaps[0])
            
            # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Î°ú Ïä§ÏºÄÏùºÎßÅ
            if isinstance(image, Image.Image):
                orig_w, orig_h = image.size
            elif isinstance(image, np.ndarray):
                orig_h, orig_w = image.shape[:2]
            else:
                orig_h, orig_w = 256, 192
            
            # Ï¢åÌëú Ïä§ÏºÄÏùºÎßÅ
            scale_x = orig_w / 192
            scale_y = orig_h / 256
            
            scaled_keypoints = []
            for kp in keypoints:
                scaled_keypoints.append([
                    kp[0] * scale_x,
                    kp[1] * scale_y,
                    kp[2]
                ])
            
            processing_time = time.time() - start_time
            
            return {
                "keypoints": scaled_keypoints,
                "processing_time": processing_time,
                "model_type": "hrnet",
                "success": True,
                "confidence": np.mean([kp[2] for kp in scaled_keypoints])
            }
            
        except Exception as e:
            logger.error(f"‚ùå HRNet AI Ï∂îÎ°† Ïã§Ìå®: {e}")
            return {
                "keypoints": [],
                "processing_time": time.time() - start_time,
                "model_type": "hrnet",
                "success": False,
                "error": str(e)
            }

    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """ÌûàÌä∏ÎßµÏóêÏÑú ÌÇ§Ìè¨Ïù∏Ìä∏ Ï∂îÏ∂ú (Í≥†Ï†ïÎ∞Ä ÏÑúÎ∏åÌîΩÏÖÄ Ï†ïÌôïÎèÑ)"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(18):  # 18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏
            heatmap = heatmaps[i].cpu().numpy()
            
            # ÏµúÎåÄÍ∞í ÏúÑÏπò Ï∞æÍ∏∞
            y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            max_val = heatmap[y_idx, x_idx]
            
            # ÏÑúÎ∏åÌîΩÏÖÄ Ï†ïÌôïÎèÑÎ•º ÏúÑÌïú Í∞ÄÏö∞ÏãúÏïà ÌîºÌåÖ
            if (1 <= x_idx < w-1) and (1 <= y_idx < h-1):
                # x Î∞©Ìñ• ÏÑúÎ∏åÌîΩÏÖÄ Î≥¥Ï†ï
                dx = 0.5 * (heatmap[y_idx, x_idx+1] - heatmap[y_idx, x_idx-1]) / (
                    heatmap[y_idx, x_idx+1] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx, x_idx-1] + 1e-8)
                
                # y Î∞©Ìñ• ÏÑúÎ∏åÌîΩÏÖÄ Î≥¥Ï†ï
                dy = 0.5 * (heatmap[y_idx+1, x_idx] - heatmap[y_idx-1, x_idx]) / (
                    heatmap[y_idx+1, x_idx] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx-1, x_idx] + 1e-8)
                
                # ÏÑúÎ∏åÌîΩÏÖÄ Ï¢åÌëú
                x_subpixel = x_idx + dx
                y_subpixel = y_idx + dy
            else:
                x_subpixel = x_idx
                y_subpixel = y_idx
            
            # Ï¢åÌëú Ï†ïÍ∑úÌôî (0-1 Î≤îÏúÑ)
            x_normalized = x_subpixel / w
            y_normalized = y_subpixel / h
            
            # Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ Ï¢åÌëúÎ°ú Î≥ÄÌôò (192x256 Í∏∞Ï§Ä)
            x_coord = x_normalized * 192
            y_coord = y_normalized * 256
            confidence = float(max_val)
            
            keypoints.append([x_coord, y_coord, confidence])
        
        return keypoints

    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu"):
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏóêÏÑú HRNet Î™®Îç∏ Î°úÎìú (hrnet_w48_coco_256x192.pth)"""
        model = cls()
        
        if checkpoint_path and os.path.exists(checkpoint_path):
            try:
                logger.info(f"üîÑ HRNet Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©: {checkpoint_path}")
                checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
                
                # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌòïÌÉúÏóê Îî∞Î•∏ Ï≤òÎ¶¨
                if isinstance(checkpoint, dict):
                    if 'model' in checkpoint:
                        state_dict = checkpoint['model']
                    elif 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # ÌÇ§ Ïù¥Î¶Ñ Îß§Ìïë (ÌïÑÏöîÌïú Í≤ΩÏö∞)
                model_dict = model.state_dict()
                filtered_dict = {}
                
                for k, v in state_dict.items():
                    # ÌÇ§ Ïù¥Î¶Ñ Ï†ïÎ¶¨
                    key = k
                    if key.startswith('module.'):
                        key = key[7:]  # 'module.' Ï†úÍ±∞
                    
                    if key in model_dict and model_dict[key].shape == v.shape:
                        filtered_dict[key] = v
                    else:
                        logger.debug(f"HRNet ÌÇ§ Î∂àÏùºÏπò: {key}, ÌòïÌÉú: {v.shape if hasattr(v, 'shape') else 'unknown'}")
                
                # ÌïÑÌÑ∞ÎßÅÎêú Í∞ÄÏ§ëÏπò Î°úÎìú
                model_dict.update(filtered_dict)
                model.load_state_dict(model_dict, strict=False)
                model.is_loaded = True
                
                logger.info(f"‚úÖ HRNet Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏôÑÎ£å: {len(filtered_dict)}/{len(model_dict)} Î†àÏù¥Ïñ¥")
                logger.info(f"üìä HRNet ÌååÎùºÎØ∏ÌÑ∞: {model.parameter_count:,}Í∞ú")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è HRNet Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®: {e}")
                logger.info("üîÑ Í∏∞Î≥∏ HRNet Í∞ÄÏ§ëÏπòÎ°ú Ï¥àÍ∏∞Ìôî")
        else:
            logger.warning(f"‚ö†Ô∏è HRNet Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùº ÏóÜÏùå: {checkpoint_path}")
            logger.info("üîÑ ÎûúÎç§ Ï¥àÍ∏∞ÌôîÎêú HRNet ÏÇ¨Ïö©")
        
        model.to(device)
        model.eval()
        return model

    def get_model_info(self) -> Dict[str, Any]:
        """HRNet Î™®Îç∏ Ï†ïÎ≥¥ Î∞òÌôò"""
        return {
            'model_name': self.model_name,
            'version': self.version,
            'parameter_count': self.parameter_count,
            'is_loaded': self.is_loaded,
            'architecture': 'HRNet-W48',
            'input_size': '256x192',
            'output_keypoints': 18,
            'precision': 'high',
            'subpixel_accuracy': True,
            'multi_scale_fusion': True
        }


# ==============================================
# üî• Í∏∞Ï°¥ Step02 ÏΩîÎìúÏóê Ï∂îÍ∞ÄÌï† Î∂ÄÎ∂Ñ
# ==============================================

# Ïù¥ Î∂ÄÎ∂ÑÏùÑ Í∏∞Ï°¥ _load_all_ai_models Î©îÏÑúÎìúÏóê Ï∂îÍ∞Ä:
"""
# HRNet Î™®Îç∏ Î°úÎî© (Í≥†Ï†ïÎ∞Ä - ÏÉàÎ°ú Ï∂îÍ∞Ä)
if 'hrnet' in self.model_paths:
    try:
        self.logger.info("üîÑ HRNet Î°úÎî© Ï§ë (Í≥†Ï†ïÎ∞Ä)...")
        hrnet_model = RealHRNetModel.from_checkpoint(
            checkpoint_path=self.model_paths['hrnet'],
            device=self.device
        )
        self.ai_models['hrnet'] = hrnet_model
        self.loaded_models.append("hrnet")
        success_count += 1
        
        # Î™®Îç∏ Ï†ïÎ≥¥ Î°úÍπÖ
        model_info = hrnet_model.get_model_info()
        self.logger.info(f"‚úÖ HRNet Î°úÎî© ÏôÑÎ£å - ÌååÎùºÎØ∏ÌÑ∞: {model_info['parameter_count']:,}")
        self.logger.info(f"   - ÏïÑÌÇ§ÌÖçÏ≤ò: {model_info['architecture']}")
        self.logger.info(f"   - ÏÑúÎ∏åÌîΩÏÖÄ Ï†ïÌôïÎèÑ: {model_info['subpixel_accuracy']}")
        
    except Exception as e:
        self.logger.error(f"‚ùå HRNet Î°úÎî© Ïã§Ìå®: {e}")
"""

# Ïù¥ Î∂ÄÎ∂ÑÏùÑ Í∏∞Ï°¥ _run_ai_method Î©îÏÑúÎìúÏóê Ï∂îÍ∞Ä:
"""
elif method == SegmentationMethod.HRNET:
    return await self._run_hrnet_inference(image)
"""

# ÏÉàÎ°úÏö¥ HRNet Ï∂îÎ°† Î©îÏÑúÎìú Ï∂îÍ∞Ä:
async def _run_hrnet_inference(self, image: Image.Image) -> Tuple[Optional[np.ndarray], float]:
    """HRNet Ïã§Ï†ú AI Ï∂îÎ°† (Í≥†Ï†ïÎ∞Ä Ìè¨Ï¶à Í≤ÄÏ∂ú)"""
    try:
        if 'hrnet' not in self.ai_models:
            raise RuntimeError("‚ùå HRNet Î™®Îç∏Ïù¥ Î°úÎìúÎêòÏßÄ ÏïäÏùå")
        
        hrnet_model = self.ai_models['hrnet']
        
        # üî• Ïã§Ï†ú HRNet AI Ï∂îÎ°† (Í≥†Ï†ïÎ∞Ä Î™®Îç∏)
        result = hrnet_model.detect_high_precision_pose(image)
        
        if result['success']:
            keypoints = result['keypoints']
            confidence = result['confidence']
            
            # ÌÇ§Ìè¨Ïù∏Ìä∏Î•º OpenPose 18 Ìè¨Îß∑ÏúºÎ°ú Î≥ÄÌôò (Ïù¥ÎØ∏ 18Í∞ú)
            if len(keypoints) == 18:
                hrnet_keypoints = keypoints
            else:
                # ÌïÑÏöîÏãú ÌÇ§Ìè¨Ïù∏Ìä∏ Ïàò Ï°∞Ï†ï
                hrnet_keypoints = keypoints[:18] + [[0.0, 0.0, 0.0]] * (18 - len(keypoints))
            
            self.logger.info(f"‚úÖ HRNet AI Ï∂îÎ°† ÏôÑÎ£å - Ïã†Î¢∞ÎèÑ: {confidence:.3f}")
            return hrnet_keypoints, confidence
        else:
            raise RuntimeError(f"HRNet Ï∂îÎ°† Ïã§Ìå®: {result.get('error', 'Unknown')}")
            
    except Exception as e:
        self.logger.error(f"‚ùå HRNet AI Ï∂îÎ°† Ïã§Ìå®: {e}")
        raise

class RealBodyPoseModel:
    """Body Pose 97.8MB Î≥¥Ï°∞ Ìè¨Ï¶à Í≤ÄÏ∂ú - Ïã§Ï†ú AI Ï∂îÎ°†"""
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealBodyPoseModel")
    
    def load_body_pose_checkpoint(self) -> bool:
        """Ïã§Ï†ú Body Pose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©"""
        try:
            checkpoint = torch.load(str(self.model_path), map_location=self.device, weights_only=True)
            
            # Body Pose ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉùÏÑ±
            self.model = self._create_body_pose_network()
            
            # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©
            if 'state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['state_dict'], strict=False)
            else:
                self.model.load_state_dict(checkpoint, strict=False)
            
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            
            self.logger.info(f"‚úÖ Body Pose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ÏôÑÎ£å: {self.model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Body Pose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    def _create_body_pose_network(self) -> nn.Module:
        """Body Pose ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉùÏÑ±"""
        class BodyPoseNetwork(nn.Module):
            def __init__(self):
                super().__init__()
                # ResNet Ïä§ÌÉÄÏùº Î∞±Î≥∏
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.MaxPool2d(3, 2, 1),
                    nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),
                    nn.Conv2d(256, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(),
                    nn.AdaptiveAvgPool2d((8, 8))
                )
                
                self.pose_head = nn.Sequential(
                    nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(128, 18, 1, 1, 0),  # 18 keypoints heatmaps
                    nn.AdaptiveAvgPool2d((32, 32))
                )
            
            def forward(self, x):
                features = self.backbone(x)
                heatmaps = self.pose_head(features)
                return heatmaps
        
        return BodyPoseNetwork()
    
    def detect_body_pose(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """Î≥¥Ï°∞ Ìè¨Ï¶à Í≤ÄÏ∂ú (Ïã§Ï†ú AI Ï∂îÎ°†)"""
        if not self.loaded:
            raise RuntimeError("Body Pose Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏùå")
        
        start_time = time.time()
        
        try:
            # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
            if isinstance(image, Image.Image):
                image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
            elif isinstance(image, np.ndarray):
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(self.device) / 255.0
            else:
                image_tensor = image.to(self.device)
            
            # Ïã§Ï†ú AI Ï∂îÎ°† Ïã§Ìñâ
            with torch.no_grad():
                heatmaps = self.model(image_tensor)
                keypoints = self._extract_keypoints_from_heatmaps(heatmaps[0])
            
            processing_time = time.time() - start_time
            
            return {
                "keypoints": keypoints,
                "processing_time": processing_time,
                "model_type": "body_pose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Body Pose AI Ï∂îÎ°† Ïã§Ìå®: {e}")
            return {
                "keypoints": [],
                "processing_time": time.time() - start_time,
                "model_type": "body_pose",
                "success": False,
                "error": str(e)
            }
    
    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """ÌûàÌä∏ÎßµÏóêÏÑú ÌÇ§Ìè¨Ïù∏Ìä∏ Ï∂îÏ∂ú"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(18):
            heatmap = heatmaps[i].cpu().numpy()
            
            # ÏµúÎåÄÍ∞í ÏúÑÏπò Î∞è Ïã†Î¢∞ÎèÑ
            y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            confidence = float(heatmap[y_idx, x_idx])
            
            # Ï¢åÌëú Ï†ïÍ∑úÌôî
            x = float(x_idx / w * 512)
            y = float(y_idx / h * 512)
            
            keypoints.append([x, y, confidence])
        
        return keypoints

# ==============================================
# üî• 8. Î©îÏù∏ PoseEstimationStep ÌÅ¥ÎûòÏä§ (ÏôÑÏ†ÑÌïú AI Ïó∞Îèô + ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÄÏõê)
# ==============================================

class PoseEstimationStep(BaseStepMixin, StepInterface):
    """
    üî• Step 02: AI Í∏∞Î∞ò Ìè¨Ï¶à Ï∂îÏ†ï ÏãúÏä§ÌÖú - ÏôÑÏ†ÑÌïú Ïã§Ï†ú AI Î™®Îç∏ Ïó∞Îèô + ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÄÏõê
    
    ‚úÖ Ïã§Ï†ú AI Î™®Îç∏ ÌååÏùº ÌôúÏö© (3.4GB): OpenPose, YOLOv8, Diffusion, HRNet
    ‚úÖ ModelLoader ÏôÑÏ†Ñ Ïó∞Îèô - Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ‚Üí AI Î™®Îç∏ ÌÅ¥ÎûòÏä§ ‚Üí Ïã§Ï†ú Ï∂îÎ°†
    ‚úÖ BaseStepMixin v16.0 + StepInterface Îã§Ï§ë ÏÉÅÏÜç
    ‚úÖ Ïù¥Ï§ë Í∏∞Îä• ÏßÄÏõê: Í∞úÎ≥Ñ Ïã§Ìñâ + ÌååÏù¥ÌîÑÎùºÏù∏ Ïó∞Í≤∞
    ‚úÖ SmartModelPathMapper ÌôúÏö©Ìïú ÎèôÏ†Å ÌååÏùº Í≤ΩÎ°ú ÌÉêÏßÄ
    ‚úÖ 18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏ ÏôÑÏ†Ñ Í≤ÄÏ∂ú Î∞è Ïä§ÏºàÎ†àÌÜ§ Íµ¨Ï°∞ ÏÉùÏÑ±
    ‚úÖ M3 Max MPS Í∞ÄÏÜç ÏµúÏ†ÅÌôî
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        strict_mode: bool = True,
        **kwargs
    ):
        """
        ÏôÑÏ†ÑÌïú AI Î™®Îç∏ Ïó∞Îèô + ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÄÏõê PoseEstimationStep ÏÉùÏÑ±Ïûê
        
        Args:
            device: ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï ('auto', 'mps', 'cuda', 'cpu')
            config: ÏÑ§Ï†ï ÎîïÏÖîÎÑàÎ¶¨
            strict_mode: ÏóÑÍ≤© Î™®Îìú
            **kwargs: Ï∂îÍ∞Ä ÏÑ§Ï†ï
        """
        
        # Í∏∞Î≥∏ ÏÑ§Ï†ï
        if config is None:
            config = {}
        config.update(kwargs)
        
        # BaseStepMixin Ìò∏Ìôò ÏÑ§Ï†ï
        kwargs.setdefault('step_name', 'PoseEstimationStep')
        kwargs.setdefault('step_id', 2)
        kwargs.setdefault('device', device or DEVICE)
        kwargs.setdefault('strict_mode', strict_mode)
        
        # PoseEstimationStep ÌäπÌôî ÏÜçÏÑ±Îì§
        self.step_name = "PoseEstimationStep"
        self.step_number = 2
        self.step_id = 2
        self.step_description = "Ïã§Ï†ú AI Î™®Îç∏ Í∏∞Î∞ò Ïù∏Ï≤¥ Ìè¨Ï¶à Ï∂îÏ†ï Î∞è 18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏ Í≤ÄÏ∂ú"
        self.strict_mode = strict_mode
        self.num_keypoints = 18
        self.keypoint_names = OPENPOSE_18_KEYPOINTS.copy()
        
        # ÌååÏù¥ÌîÑÎùºÏù∏ Î™®Îìú ÏÑ§Ï†ï
        self.pipeline_mode = config.get("pipeline_mode", False)
        
        # BaseStepMixin Ï¥àÍ∏∞Ìôî
        try:
            BaseStepMixin.__init__(self, **kwargs)
            self.logger.info(f"ü§∏ BaseStepMixin v16.0 Ìò∏Ìôò Ï¥àÍ∏∞Ìôî ÏôÑÎ£å - Ïã§Ï†ú AI Î™®Îç∏ Ïó∞Îèô")
        except Exception as e:
            self.logger.error(f"‚ùå BaseStepMixin Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            if strict_mode:
                raise RuntimeError(f"Strict Mode: BaseStepMixin Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            # Ìè¥Î∞± Ï¥àÍ∏∞Ìôî
            self._fallback_initialization(**kwargs)
        
        # StepInterface Ï¥àÍ∏∞Ìôî
        try:
            StepInterface.__init__(self, step_id=2, step_name="pose_estimation", config=config)
            self.logger.info(f"üîó StepInterface Ï¥àÍ∏∞Ìôî ÏôÑÎ£å - ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÄÏõê")
        except Exception as e:
            self.logger.error(f"‚ùå StepInterface Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            if strict_mode:
                raise RuntimeError(f"Strict Mode: StepInterface Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        
        # ÏãúÏä§ÌÖú ÏÑ§Ï†ï Ï¥àÍ∏∞Ìôî
        self._setup_system_config(config=config, **kwargs)
        
        # SmartModelPathMapper Ï¥àÍ∏∞Ìôî
        self.model_mapper = Step02ModelMapper()
        
        # Ïã§Ï†ú AI Î™®Îç∏Îì§
        self.ai_models: Dict[str, Any] = {}
        self.model_paths: Dict[str, Optional[Path]] = {}
        self.loaded_models: List[str] = []
        
        # Ï≤òÎ¶¨ ÏÑ§Ï†ï
        self.target_input_size = (512, 512)
        self.confidence_threshold = config.get('confidence_threshold', 0.5) if config else 0.5
        self.visualization_enabled = config.get('visualization_enabled', True) if config else True
        
        # Ï∫êÏãú ÏãúÏä§ÌÖú
        self.prediction_cache: Dict[str, Any] = {}
        self.cache_max_size = 100 if IS_M3_MAX else 50
        
        # ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏÉÅÌÉú
        self.dependencies_injected = {
            'model_loader': False,
            'step_interface': False,
            'memory_manager': False,
            'data_converter': False,
            'di_container': False
        }
        
        # ÌååÏù¥ÌîÑÎùºÏù∏ ÏÉÅÌÉú
        self.pipeline_position = "middle"  # Step 02Îäî Ï§ëÍ∞Ñ Îã®Í≥Ñ
        self.accepts_pipeline_input = True
        self.provides_pipeline_output = True
        
        self.logger.info(f"üéØ {self.step_name} Ïã§Ï†ú AI Î™®Îç∏ Ïó∞Îèô + ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÄÏõê Step ÏÉùÏÑ± ÏôÑÎ£å")
        self.logger.info(f"üîó ÌååÏù¥ÌîÑÎùºÏù∏ Î™®Îìú: {self.pipeline_mode}")
    
    # ==============================================
    # üî• ÌååÏù¥ÌîÑÎùºÏù∏ Ï≤òÎ¶¨ Î©îÏÑúÎìú (StepInterface Íµ¨ÌòÑ)
    # ==============================================
    
    async def process_pipeline(self, input_data: PipelineStepResult) -> PipelineStepResult:
        """
        ÌååÏù¥ÌîÑÎùºÏù∏ Î™®Îìú Ï≤òÎ¶¨ - Step 01 Í≤∞Í≥ºÎ•º Î∞õÏïÑ Ìè¨Ï¶à Ï∂îÏ†ï ÌõÑ Step 03, 04Î°ú Ï†ÑÎã¨
        
        Args:
            input_data: Step 01ÏóêÏÑú Ï†ÑÎã¨Î∞õÏùÄ ÌååÏù¥ÌîÑÎùºÏù∏ Îç∞Ïù¥ÌÑ∞
            
        Returns:
            PipelineStepResult: Step 03, 04Î°ú Ï†ÑÎã¨Ìï† Ìè¨Ï¶à Ï∂îÏ†ï Í≤∞Í≥º
        """
        try:
            start_time = time.time()
            self.logger.info(f"üîó {self.step_name} ÌååÏù¥ÌîÑÎùºÏù∏ Î™®Îìú Ï≤òÎ¶¨ ÏãúÏûë")
            
            # Ï¥àÍ∏∞Ìôî Í≤ÄÏ¶ù
            if not self.is_initialized:
                if not await self.initialize():
                    error_msg = "ÌååÏù¥ÌîÑÎùºÏù∏: AI Ï¥àÍ∏∞Ìôî Ïã§Ìå®"
                    return PipelineStepResult(
                        step_id=2, step_name="pose_estimation",
                        success=False, error=error_msg
                    )
            
            # Step 01 Í≤∞Í≥º Î∞õÍ∏∞
            if not hasattr(input_data, 'for_step_02') or not input_data.for_step_02:
                error_msg = "Step 01 Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏùå"
                self.logger.error(f"‚ùå {error_msg}")
                return PipelineStepResult(
                    step_id=2, step_name="pose_estimation",
                    success=False, error=error_msg
                )
            
            step01_data = input_data.for_step_02
            parsed_image = step01_data.get("parsed_image")
            body_masks = step01_data.get("body_masks", {})
            human_region = step01_data.get("human_region")
            
            if parsed_image is None:
                error_msg = "Step 01ÏóêÏÑú ÌååÏã±Îêú Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏùå"
                self.logger.error(f"‚ùå {error_msg}")
                return PipelineStepResult(
                    step_id=2, step_name="pose_estimation",
                    success=False, error=error_msg
                )
            
            # ÌååÏù¥ÌîÑÎùºÏù∏Ïö© Ìè¨Ï¶à Ï∂îÏ†ï AI Ï≤òÎ¶¨
            pose_result = await self._run_pose_estimation_pipeline_ai(parsed_image, body_masks, human_region)
            
            if not pose_result.get('success', False):
                error_msg = f"ÌååÏù¥ÌîÑÎùºÏù∏ Ìè¨Ï¶à Ï∂îÏ†ï Ïã§Ìå®: {pose_result.get('error', 'Unknown Error')}"
                self.logger.error(f"‚ùå {error_msg}")
                return PipelineStepResult(
                    step_id=2, step_name="pose_estimation",
                    success=False, error=error_msg
                )
            
            # ÌååÏù¥ÌîÑÎùºÏù∏ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
            pipeline_data = PipelineStepResult(
                step_id=2,
                step_name="pose_estimation",
                success=True,
                
                # Step 03 (Cloth Segmentation)ÏúºÎ°ú Ï†ÑÎã¨Ìï† Îç∞Ïù¥ÌÑ∞
                for_step_03={
                    **getattr(input_data, 'for_step_03', {}),  # Step 01 Îç∞Ïù¥ÌÑ∞ Í≥ÑÏäπ
                    "pose_keypoints": pose_result["keypoints"],
                    "pose_skeleton": pose_result.get("skeleton_structure", {}),
                    "pose_confidence": pose_result.get("confidence_scores", []),
                    "joint_connections": pose_result.get("joint_connections", []),
                    "visible_keypoints": pose_result.get("visible_keypoints", [])
                },
                
                # Step 04 (Geometric Matching)Î°ú Ï†ÑÎã¨Ìï† Îç∞Ïù¥ÌÑ∞
                for_step_04={
                    "keypoints_for_matching": pose_result["keypoints"],
                    "joint_connections": pose_result.get("joint_connections", []),
                    "pose_angles": pose_result.get("joint_angles", {}),
                    "body_orientation": pose_result.get("body_orientation", {}),
                    "pose_landmarks": pose_result.get("landmarks", {}),
                    "skeleton_structure": pose_result.get("skeleton_structure", {})
                },
                
                # Step 05 (Cloth Warping)Î°ú Ï†ÑÎã¨Ìï† Îç∞Ïù¥ÌÑ∞
                for_step_05={
                    "reference_keypoints": pose_result["keypoints"],
                    "body_proportions": pose_result.get("body_proportions", {}),
                    "pose_type": pose_result.get("pose_type", "standing")
                },
                
                # Step 06 (Virtual Fitting)Î°ú Ï†ÑÎã¨Ìï† Îç∞Ïù¥ÌÑ∞
                for_step_06={
                    "person_keypoints": pose_result["keypoints"],
                    "pose_confidence": pose_result.get("confidence_scores", []),
                    "body_orientation": pose_result.get("body_orientation", {})
                },
                
                # Step 07 (Post Processing)Î°ú Ï†ÑÎã¨Ìï† Îç∞Ïù¥ÌÑ∞
                for_step_07={
                    "original_keypoints": pose_result["keypoints"]
                },
                
                # Step 08 (Quality Assessment)Î°ú Ï†ÑÎã¨Ìï† Îç∞Ïù¥ÌÑ∞
                for_step_08={
                    "pose_quality_metrics": pose_result.get("pose_analysis", {}),
                    "keypoints_confidence": pose_result.get("confidence_scores", [])
                },
                
                # Ïù¥Ï†Ñ Îã®Í≥Ñ Îç∞Ïù¥ÌÑ∞ Î≥¥Ï°¥ Î∞è ÌôïÏû•
                previous_data={
                    **getattr(input_data, 'original_data', {}),
                    "step01_results": getattr(input_data, 'for_step_02', {}),
                    "step02_results": pose_result
                },
                
                original_data=getattr(input_data, 'original_data', {}),
                
                # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
                metadata={
                    "processing_time": time.time() - start_time,
                    "ai_models_used": pose_result.get("models_used", []),
                    "num_keypoints_detected": len(pose_result.get("keypoints", [])),
                    "ready_for_next_steps": ["step_03", "step_04", "step_05", "step_06"],
                    "execution_mode": "pipeline",
                    "pipeline_progress": "2/8 Îã®Í≥Ñ ÏôÑÎ£å",
                    "primary_model": pose_result.get("primary_model", "unknown"),
                    "enhanced_by_diffusion": pose_result.get("enhanced_by_diffusion", False)
                },
                
                processing_time=time.time() - start_time
            )
            
            self.logger.info(f"‚úÖ {self.step_name} ÌååÏù¥ÌîÑÎùºÏù∏ Î™®Îìú Ï≤òÎ¶¨ ÏôÑÎ£å")
            self.logger.info(f"üéØ Í≤ÄÏ∂úÎêú ÌÇ§Ìè¨Ïù∏Ìä∏: {len(pose_result.get('keypoints', []))}Í∞ú")
            self.logger.info(f"‚û°Ô∏è Îã§Ïùå Îã®Í≥ÑÎ°ú Îç∞Ïù¥ÌÑ∞ Ï†ÑÎã¨ Ï§ÄÎπÑ ÏôÑÎ£å")
            
            return pipeline_data
            
        except Exception as e:
            self.logger.error(f"‚ùå {self.step_name} ÌååÏù¥ÌîÑÎùºÏù∏ Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
            self.logger.error(f"üìã Ïò§Î•ò Ïä§ÌÉù: {traceback.format_exc()}")
            return PipelineStepResult(
                step_id=2, step_name="pose_estimation",
                success=False, error=str(e),
                processing_time=time.time() - start_time if 'start_time' in locals() else 0.0
            )
    
    async def _run_pose_estimation_pipeline_ai(
        self, 
        parsed_image: Union[torch.Tensor, np.ndarray, Image.Image], 
        body_masks: Dict[str, Any],
        human_region: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ÑÏö© Ìè¨Ï¶à Ï∂îÏ†ï AI Ï≤òÎ¶¨"""
        try:
            inference_start = time.time()
            self.logger.info(f"üß† ÌååÏù¥ÌîÑÎùºÏù∏ Ìè¨Ï¶à Ï∂îÏ†ï AI ÏãúÏûë...")
            
            if not self.ai_models:
                error_msg = "Î°úÎî©Îêú AI Î™®Îç∏Ïù¥ ÏóÜÏùå"
                self.logger.error(f"‚ùå {error_msg}")
                return {'success': False, 'error': error_msg}
            
            # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ (ÌååÏù¥ÌîÑÎùºÏù∏Ïö©)
            if isinstance(parsed_image, torch.Tensor):
                image = to_pil_image(parsed_image.cpu())
            elif isinstance(parsed_image, np.ndarray):
                image = Image.fromarray(parsed_image)
            else:
                image = parsed_image
            
            # Body masks ÌôúÏö©Ìïú Í¥ÄÏã¨ ÏòÅÏó≠ Ï∂îÏ∂ú
            if body_masks and human_region:
                # Ïù∏Ï≤¥ ÏòÅÏó≠Ïóê ÏßëÏ§ëÌïú Ìè¨Ï¶à Ï∂îÏ†ï
                image = self._focus_on_human_region(image, human_region)
            
            # Ïã§Ï†ú AI Ï∂îÎ°† Ïã§Ìñâ (Í∏∞Ï°¥ Î°úÏßÅ Ïû¨ÏÇ¨Ïö©)
            ai_result = await self._run_real_ai_inference(image, clothing_type=None)
            
            if not ai_result.get('success', False):
                return ai_result
            
            # ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ÑÏö© Ï∂îÍ∞Ä Î∂ÑÏÑù
            pipeline_analysis = self._analyze_for_pipeline(ai_result, body_masks)
            ai_result.update(pipeline_analysis)
            
            inference_time = time.time() - inference_start
            ai_result['inference_time'] = inference_time
            
            self.logger.info(f"‚úÖ ÌååÏù¥ÌîÑÎùºÏù∏ Ìè¨Ï¶à Ï∂îÏ†ï AI ÏôÑÎ£å ({inference_time:.3f}Ï¥à)")
            
            return ai_result
            
        except Exception as e:
            self.logger.error(f"‚ùå ÌååÏù¥ÌîÑÎùºÏù∏ Ìè¨Ï¶à Ï∂îÏ†ï AI Ïã§Ìå®: {e}")
            return {'success': False, 'error': str(e)}
    
    def _focus_on_human_region(self, image: Image.Image, human_region: Dict[str, Any]) -> Image.Image:
        """Ïù∏Ï≤¥ ÏòÅÏó≠Ïóê ÏßëÏ§ëÌïú Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨"""
        try:
            if 'bbox' in human_region:
                bbox = human_region['bbox']
                x1, y1, x2, y2 = bbox
                # Ïù∏Ï≤¥ ÏòÅÏó≠ ÌÅ¨Î°≠
                cropped = image.crop((x1, y1, x2, y2))
                # ÏõêÎ≥∏ ÌÅ¨Í∏∞Î°ú Î¶¨ÏÇ¨Ïù¥Ï¶à
                return cropped.resize(image.size, Image.Resampling.BILINEAR)
            return image
        except Exception as e:
            self.logger.debug(f"Ïù∏Ï≤¥ ÏòÅÏó≠ ÏßëÏ§ë Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
            return image
    
    def _analyze_for_pipeline(self, ai_result: Dict[str, Any], body_masks: Dict[str, Any]) -> Dict[str, Any]:
        """ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ÑÏö© Ï∂îÍ∞Ä Î∂ÑÏÑù"""
        try:
            keypoints = ai_result.get('keypoints', [])
            
            # Í∞ÄÏãúÏÑ± Î∂ÑÏÑù (Îã§Ïùå Îã®Í≥ÑÏóêÏÑú ÌôúÏö©)
            visible_keypoints = []
            confidence_threshold = 0.5
            
            for i, kp in enumerate(keypoints):
                if len(kp) >= 3 and kp[2] > confidence_threshold:
                    keypoint_name = OPENPOSE_18_KEYPOINTS[i] if i < len(OPENPOSE_18_KEYPOINTS) else f"kp_{i}"
                    visible_keypoints.append({
                        'index': i,
                        'name': keypoint_name,
                        'position': [kp[0], kp[1]],
                        'confidence': kp[2]
                    })
            
            # Ìè¨Ï¶à ÌÉÄÏûÖ Î∂ÑÎ•ò (Îã§Ïùå Îã®Í≥Ñ ÏµúÏ†ÅÌôîÏö©)
            pose_type = self._classify_pose_type(keypoints)
            
            # Body masksÏôÄÏùò ÏùºÏπòÏÑ± Î∂ÑÏÑù
            mask_consistency = self._analyze_mask_consistency(keypoints, body_masks)
            
            return {
                'visible_keypoints': visible_keypoints,
                'pose_type': pose_type,
                'mask_consistency': mask_consistency,
                'pipeline_ready': True
            }
            
        except Exception as e:
            self.logger.debug(f"ÌååÏù¥ÌîÑÎùºÏù∏ Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {'pipeline_ready': False}
    
    def _classify_pose_type(self, keypoints: List[List[float]]) -> str:
        """Ìè¨Ï¶à ÌÉÄÏûÖ Î∂ÑÎ•ò"""
        try:
            if not keypoints or len(keypoints) < 18:
                return "unknown"
            
            # Ìåî Í∞ÅÎèÑ Î∂ÑÏÑù
            arms_extended = False
            if all(i < len(keypoints) and len(keypoints[i]) >= 3 for i in [2, 3, 4, 5, 6, 7]):
                # ÌåîÏù¥ ÌéºÏ≥êÏ†∏ ÏûàÎäîÏßÄ ÌôïÏù∏
                right_arm_angle = self._calculate_arm_angle(keypoints[2], keypoints[3], keypoints[4])
                left_arm_angle = self._calculate_arm_angle(keypoints[5], keypoints[6], keypoints[7])
                
                if right_arm_angle > 150 and left_arm_angle > 150:
                    arms_extended = True
            
            # Îã§Î¶¨ Î∂ÑÏÑù
            legs_apart = False
            if all(i < len(keypoints) and len(keypoints[i]) >= 3 for i in [9, 12, 11, 14]):
                hip_distance = abs(keypoints[9][0] - keypoints[12][0])
                ankle_distance = abs(keypoints[11][0] - keypoints[14][0])
                if ankle_distance > hip_distance * 1.5:
                    legs_apart = True
            
            # Ìè¨Ï¶à Î∂ÑÎ•ò
            if arms_extended and not legs_apart:
                return "t_pose"
            elif arms_extended and legs_apart:
                return "star_pose" 
            elif not arms_extended and not legs_apart:
                return "standing"
            else:
                return "dynamic"
                
        except Exception as e:
            self.logger.debug(f"Ìè¨Ï¶à ÌÉÄÏûÖ Î∂ÑÎ•ò Ïã§Ìå®: {e}")
            return "unknown"
    
    def _calculate_arm_angle(self, shoulder: List[float], elbow: List[float], wrist: List[float]) -> float:
        """Ìåî Í∞ÅÎèÑ Í≥ÑÏÇ∞"""
        try:
            if all(len(kp) >= 3 and kp[2] > 0.3 for kp in [shoulder, elbow, wrist]):
                v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]])
                v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]])
                
                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
                cos_angle = np.clip(cos_angle, -1.0, 1.0)
                angle = np.arccos(cos_angle)
                
                return np.degrees(angle)
            return 0.0
        except:
            return 0.0
    
    def _analyze_mask_consistency(self, keypoints: List[List[float]], body_masks: Dict[str, Any]) -> Dict[str, Any]:
        """Body masksÏôÄ ÌÇ§Ìè¨Ïù∏Ìä∏ ÏùºÏπòÏÑ± Î∂ÑÏÑù"""
        try:
            consistency = {
                'overall_score': 0.0,
                'detailed_scores': {},
                'issues': []
            }
            
            # Í∞ÑÎã®Ìïú ÏùºÏπòÏÑ± Î∂ÑÏÑù (Ïã§Ï†úÎ°úÎäî Îçî Î≥µÏû°Ìïú Î°úÏßÅ ÌïÑÏöî)
            visible_keypoints = sum(1 for kp in keypoints if len(kp) >= 3 and kp[2] > 0.5)
            total_keypoints = len(keypoints)
            
            if total_keypoints > 0:
                consistency['overall_score'] = visible_keypoints / total_keypoints
            
            if consistency['overall_score'] < 0.6:
                consistency['issues'].append("ÌÇ§Ìè¨Ïù∏Ìä∏ÏôÄ ÎßàÏä§ÌÅ¨ Î∂àÏùºÏπò")
            
            return consistency
            
        except Exception as e:
            self.logger.debug(f"ÎßàÏä§ÌÅ¨ ÏùºÏπòÏÑ± Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {'overall_score': 0.0, 'issues': ['Î∂ÑÏÑù Ïã§Ìå®']}
    
    # ==============================================
    # üî• Í∞úÎ≥Ñ Ï≤òÎ¶¨ Î©îÏÑúÎìú (Í∏∞Ï°¥ process Î©îÏÑúÎìú Ïú†ÏßÄ)
    # ==============================================
    """
    üî• Step 02: AI Í∏∞Î∞ò Ìè¨Ï¶à Ï∂îÏ†ï ÏãúÏä§ÌÖú - ÏôÑÏ†ÑÌïú Ïã§Ï†ú AI Î™®Îç∏ Ïó∞Îèô
    
    ‚úÖ Ïã§Ï†ú AI Î™®Îç∏ ÌååÏùº ÌôúÏö© (3.4GB): OpenPose, YOLOv8, Diffusion
    ‚úÖ ModelLoader ÏôÑÏ†Ñ Ïó∞Îèô - Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ‚Üí AI Î™®Îç∏ ÌÅ¥ÎûòÏä§ ‚Üí Ïã§Ï†ú Ï∂îÎ°†
    ‚úÖ BaseStepMixin v16.0 ÏôÑÏ†Ñ Ìò∏Ìôò - ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ìå®ÌÑ¥
    ‚úÖ SmartModelPathMapper ÌôúÏö©Ìïú ÎèôÏ†Å ÌååÏùº Í≤ΩÎ°ú ÌÉêÏßÄ
    ‚úÖ 18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏ ÏôÑÏ†Ñ Í≤ÄÏ∂ú Î∞è Ïä§ÏºàÎ†àÌÜ§ Íµ¨Ï°∞ ÏÉùÏÑ±
    ‚úÖ M3 Max MPS Í∞ÄÏÜç ÏµúÏ†ÅÌôî
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        strict_mode: bool = True,
        **kwargs
    ):
        """
        ÏôÑÏ†ÑÌïú AI Î™®Îç∏ Ïó∞Îèô PoseEstimationStep ÏÉùÏÑ±Ïûê
        
        Args:
            device: ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï ('auto', 'mps', 'cuda', 'cpu')
            config: ÏÑ§Ï†ï ÎîïÏÖîÎÑàÎ¶¨
            strict_mode: ÏóÑÍ≤© Î™®Îìú
            **kwargs: Ï∂îÍ∞Ä ÏÑ§Ï†ï
        """
        
        # BaseStepMixin Ìò∏Ìôò ÏÑ§Ï†ï
        kwargs.setdefault('step_name', 'PoseEstimationStep')
        kwargs.setdefault('step_id', 2)
        kwargs.setdefault('device', device or DEVICE)
        kwargs.setdefault('strict_mode', strict_mode)
        
        # PoseEstimationStep ÌäπÌôî ÏÜçÏÑ±Îì§
        self.step_name = "PoseEstimationStep"
        self.step_number = 2
        self.step_description = "Ïã§Ï†ú AI Î™®Îç∏ Í∏∞Î∞ò Ïù∏Ï≤¥ Ìè¨Ï¶à Ï∂îÏ†ï Î∞è 18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏ Í≤ÄÏ∂ú"
        self.strict_mode = strict_mode
        self.num_keypoints = 18
        self.keypoint_names = OPENPOSE_18_KEYPOINTS.copy()
        
        # BaseStepMixin Ï¥àÍ∏∞Ìôî
        try:
            super(PoseEstimationStep, self).__init__(**kwargs)
            self.logger.info(f"ü§∏ BaseStepMixin v16.0 Ìò∏Ìôò Ï¥àÍ∏∞Ìôî ÏôÑÎ£å - Ïã§Ï†ú AI Î™®Îç∏ Ïó∞Îèô")
        except Exception as e:
            self.logger.error(f"‚ùå BaseStepMixin Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            if strict_mode:
                raise RuntimeError(f"Strict Mode: BaseStepMixin Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            # Ìè¥Î∞± Ï¥àÍ∏∞Ìôî
            self._fallback_initialization(**kwargs)
        
        # ÏãúÏä§ÌÖú ÏÑ§Ï†ï Ï¥àÍ∏∞Ìôî
        self._setup_system_config(config=config, **kwargs)
        
        # SmartModelPathMapper Ï¥àÍ∏∞Ìôî
        self.model_mapper = Step02ModelMapper()
        
        # Ïã§Ï†ú AI Î™®Îç∏Îì§
        self.ai_models: Dict[str, Any] = {}
        self.model_paths: Dict[str, Optional[Path]] = {}
        self.loaded_models: List[str] = []
        
        # Ï≤òÎ¶¨ ÏÑ§Ï†ï
        self.target_input_size = (512, 512)
        self.confidence_threshold = config.get('confidence_threshold', 0.5) if config else 0.5
        self.visualization_enabled = config.get('visualization_enabled', True) if config else True
        
        # Ï∫êÏãú ÏãúÏä§ÌÖú
        self.prediction_cache: Dict[str, Any] = {}
        self.cache_max_size = 100 if IS_M3_MAX else 50
        
        # ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏÉÅÌÉú
        self.dependencies_injected = {
            'model_loader': False,
            'step_interface': False,
            'memory_manager': False,
            'data_converter': False,
            'di_container': False
        }
        
        self.logger.info(f"üéØ {self.step_name} Ïã§Ï†ú AI Î™®Îç∏ Ïó∞Îèô Step ÏÉùÏÑ± ÏôÑÎ£å (Strict Mode: {self.strict_mode})")
    
    def _fallback_initialization(self, **kwargs):
        """Ìè¥Î∞± Ï¥àÍ∏∞Ìôî"""
        self.device = kwargs.get('device', DEVICE)
        self.config = type('StepConfig', (), kwargs)()
        self.is_initialized = False
        self.is_ready = False
        self.has_model = False
        self.model_loaded = False
        self.warmup_completed = False
        
        # BaseStepMixin Ìò∏Ìôò ÏÜçÏÑ±Îì§
        self.dependency_manager = type('DependencyManager', (), {
            'dependency_status': type('DependencyStatus', (), {
                'model_loader': False,
                'step_interface': False,
                'memory_manager': False,
                'data_converter': False,
                'di_container': False
            })(),
            'auto_inject_dependencies': lambda: self._manual_auto_inject()
        })()
        
        self.performance_metrics = {
            'process_count': 0,
            'total_process_time': 0.0,
            'average_process_time': 0.0,
            'error_count': 0,
            'success_count': 0,
            'cache_hits': 0
        }
        
        self.logger.info("‚úÖ Ìè¥Î∞± Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    def _setup_system_config(self, config: Optional[Dict[str, Any]], **kwargs):
        """ÏãúÏä§ÌÖú ÏÑ§Ï†ï Ï¥àÍ∏∞Ìôî"""
        try:
            # ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
            device = kwargs.get('device')
            if device is None or device == "auto":
                self.device = DEVICE
            else:
                self.device = device
            
            # ÏÑ§Ï†ï ÌÜµÌï©
            if config is None:
                config = {}
            config.update(kwargs)
            
            # Í∏∞Î≥∏ ÏÑ§Ï†ï Ï†ÅÏö©
            default_config = {
                'confidence_threshold': 0.5,
                'visualization_enabled': True,
                'return_analysis': True,
                'cache_enabled': True,
                'detailed_analysis': True,
                'strict_mode': self.strict_mode,
                'real_ai_models': True
            }
            
            for key, default_value in default_config.items():
                if key not in config:
                    config[key] = default_value
            
            # config Í∞ùÏ≤¥ ÏÑ§Ï†ï
            if hasattr(self, 'config') and hasattr(self.config, '__dict__'):
                self.config.__dict__.update(config)
            else:
                self.config = type('StepConfig', (), config)()
            
            self.logger.info(f"üîß Ïã§Ï†ú AI ÏãúÏä§ÌÖú ÏÑ§Ï†ï ÏôÑÎ£å: {self.device}, M3 Max: {IS_M3_MAX}")
            
        except Exception as e:
            self.logger.error(f"‚ùå ÏãúÏä§ÌÖú ÏÑ§Ï†ï Ïã§Ìå®: {e}")
            if self.strict_mode:
                raise RuntimeError(f"Strict Mode: ÏãúÏä§ÌÖú ÏÑ§Ï†ï Ïã§Ìå®: {e}")
    
    def _manual_auto_inject(self) -> bool:
        """ÏàòÎèô ÏûêÎèô ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ"""
        try:
            injection_count = 0
            
            # ModelLoader ÏûêÎèô Ï£ºÏûÖ
            model_loader = get_model_loader()
            if model_loader:
                self.set_model_loader(model_loader)
                injection_count += 1
                self.logger.debug("‚úÖ ModelLoader ÏàòÎèô ÏûêÎèô Ï£ºÏûÖ ÏôÑÎ£å")
            
            # MemoryManager ÏûêÎèô Ï£ºÏûÖ
            memory_manager = get_memory_manager()
            if memory_manager:
                self.set_memory_manager(memory_manager)
                injection_count += 1
                self.logger.debug("‚úÖ MemoryManager ÏàòÎèô ÏûêÎèô Ï£ºÏûÖ ÏôÑÎ£å")
            
            if injection_count > 0:
                self.logger.info(f"üéâ ÏàòÎèô ÏûêÎèô ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ£å: {injection_count}Í∞ú")
                return True
                
            return False
        except Exception as e:
            self.logger.debug(f"ÏàòÎèô ÏûêÎèô ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ïã§Ìå®: {e}")
            return False
    
    # ==============================================
    # üî• BaseStepMixin v16.0 Ìò∏Ìôò ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Î©îÏÑúÎìúÎì§
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ (v16.0 Ìò∏Ìôò)"""
        try:
            self.model_loader = model_loader
            self.model_interface = model_loader
            self.dependencies_injected['model_loader'] = True
            self.has_model = True
            self.model_loaded = True
            
            # v16.0 dependency_manager Ìò∏Ìôò
            if hasattr(self, 'dependency_manager') and hasattr(self.dependency_manager, 'dependency_status'):
                self.dependency_manager.dependency_status.model_loader = True
                self.dependency_manager.dependency_status.step_interface = True
            
            self.logger.info("‚úÖ ModelLoader ÏÑ§Ï†ï ÏôÑÎ£å (v16.0 Ìò∏Ìôò)")
        except Exception as e:
            self.logger.error(f"‚ùå ModelLoader ÏÑ§Ï†ï Ïã§Ìå®: {e}")
            self.dependencies_injected['model_loader'] = False
    
    def set_memory_manager(self, memory_manager):
        """MemoryManager ÏÑ§Ï†ï (v16.0 Ìò∏Ìôò)"""
        try:
            self.memory_manager = memory_manager
            self.dependencies_injected['memory_manager'] = True
            
            # v16.0 dependency_manager Ìò∏Ìôò
            if hasattr(self, 'dependency_manager') and hasattr(self.dependency_manager, 'dependency_status'):
                self.dependency_manager.dependency_status.memory_manager = True
            
            self.logger.debug("‚úÖ MemoryManager ÏÑ§Ï†ï ÏôÑÎ£å (v16.0 Ìò∏Ìôò)")
        except Exception as e:
            self.logger.error(f"‚ùå MemoryManager ÏÑ§Ï†ï Ïã§Ìå®: {e}")
            self.dependencies_injected['memory_manager'] = False
    
    def set_data_converter(self, data_converter):
        """DataConverter ÏÑ§Ï†ï (v16.0 Ìò∏Ìôò)"""
        try:
            self.data_converter = data_converter
            self.dependencies_injected['data_converter'] = True
            
            # v16.0 dependency_manager Ìò∏Ìôò
            if hasattr(self, 'dependency_manager') and hasattr(self.dependency_manager, 'dependency_status'):
                self.dependency_manager.dependency_status.data_converter = True
            
            self.logger.debug("‚úÖ DataConverter ÏÑ§Ï†ï ÏôÑÎ£å (v16.0 Ìò∏Ìôò)")
        except Exception as e:
            self.logger.error(f"‚ùå DataConverter ÏÑ§Ï†ï Ïã§Ìå®: {e}")
            self.dependencies_injected['data_converter'] = False
    
    def set_di_container(self, di_container):
        """DIContainer ÏÑ§Ï†ï (v16.0 Ìò∏Ìôò)"""
        try:
            self.di_container = di_container
            self.dependencies_injected['di_container'] = True
            
            # v16.0 dependency_manager Ìò∏Ìôò
            if hasattr(self, 'dependency_manager') and hasattr(self.dependency_manager, 'dependency_status'):
                self.dependency_manager.dependency_status.di_container = True
            
            self.logger.debug("‚úÖ DIContainer ÏÑ§Ï†ï ÏôÑÎ£å (v16.0 Ìò∏Ìôò)")
        except Exception as e:
            self.logger.error(f"‚ùå DIContainer ÏÑ§Ï†ï Ïã§Ìå®: {e}")
            self.dependencies_injected['di_container'] = False
    
    # process Î©îÏÑúÎìúÎäî Í∏∞Ï°¥Í≥º ÎèôÏùºÌïòÍ≤å Ïú†ÏßÄ (Í∞úÎ≥Ñ Ïã§ÌñâÏö©)
    
    # ==============================================
    # üî• Ïã§Ï†ú AI Î™®Îç∏ Î°úÎî© (ModelLoader Ïó∞Îèô) - HRNet Ï∂îÍ∞Ä
    # ==============================================
    
    async def _load_real_ai_models(self) -> bool:
        """Ïã§Ï†ú 3.4GB AI Î™®Îç∏Îì§ Î°úÎî© (ModelLoader Ïó∞Îèô) - HRNet Ìè¨Ìï®"""
        try:
            self.logger.info("üîÑ Ïã§Ï†ú AI Î™®Îç∏ ÌååÏùºÎì§ Î°úÎî© ÏãúÏûë...")
            
            # 1. SmartModelPathMapperÎ°ú Ïã§Ï†ú ÌååÏùº Í≤ΩÎ°ú ÌÉêÏßÄ
            self.model_paths = self.model_mapper.get_step02_model_paths()
            
            if not any(self.model_paths.values()):
                error_msg = "Ïã§Ï†ú AI Î™®Îç∏ ÌååÏùºÎì§ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå"
                self.logger.error(f"‚ùå {error_msg}")
                if self.strict_mode:
                    raise FileNotFoundError(f"Strict Mode: {error_msg}")
                return False
            
            # 2. ModelLoaderÎ•º ÌÜµÌïú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Î∞è AI Î™®Îç∏ ÌÅ¥ÎûòÏä§ ÏÉùÏÑ±
            success_count = 0
            
            # YOLOv8-Pose Î™®Îç∏ Î°úÎî© (6.5MB - Ïã§ÏãúÍ∞Ñ)
            if self.model_paths.get("yolov8"):
                try:
                    yolo_model = RealYOLOv8PoseModel(self.model_paths["yolov8"], self.device)
                    if yolo_model.load_yolo_checkpoint():
                        self.ai_models["yolov8"] = yolo_model
                        self.loaded_models.append("yolov8")
                        success_count += 1
                        self.logger.info("‚úÖ YOLOv8-Pose Î™®Îç∏ Î°úÎî© ÏôÑÎ£å")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è YOLOv8-Pose Î°úÎî© Ïã§Ìå®: {e}")
            
            # OpenPose Î™®Îç∏ Î°úÎî© (97.8MB - Ï†ïÎ∞Ä)
            if self.model_paths.get("openpose"):
                try:
                    openpose_model = RealOpenPoseModel(self.model_paths["openpose"], self.device)
                    if openpose_model.load_openpose_checkpoint():
                        self.ai_models["openpose"] = openpose_model
                        self.loaded_models.append("openpose")
                        success_count += 1
                        self.logger.info("‚úÖ OpenPose Î™®Îç∏ Î°úÎî© ÏôÑÎ£å")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è OpenPose Î°úÎî© Ïã§Ìå®: {e}")
            
            # üî• HRNet Î™®Îç∏ Î°úÎî© (Í≥†Ï†ïÎ∞Ä - ÏÉàÎ°ú Ï∂îÍ∞Ä) üî•
            if self.model_paths.get("hrnet"):
                try:
                    self.logger.info("üîÑ HRNet Î°úÎî© Ï§ë (Í≥†Ï†ïÎ∞Ä)...")
                    hrnet_model = RealHRNetModel.from_checkpoint(
                        checkpoint_path=self.model_paths["hrnet"],
                        device=self.device
                    )
                    self.ai_models["hrnet"] = hrnet_model
                    self.loaded_models.append("hrnet")
                    success_count += 1
                    
                    # Î™®Îç∏ Ï†ïÎ≥¥ Î°úÍπÖ
                    model_info = hrnet_model.get_model_info()
                    self.logger.info(f"‚úÖ HRNet Î°úÎî© ÏôÑÎ£å - ÌååÎùºÎØ∏ÌÑ∞: {model_info['parameter_count']:,}")
                    self.logger.info(f"   - ÏïÑÌÇ§ÌÖçÏ≤ò: {model_info['architecture']}")
                    self.logger.info(f"   - ÏÑúÎ∏åÌîΩÏÖÄ Ï†ïÌôïÎèÑ: {model_info['subpixel_accuracy']}")
                    
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è HRNet Î°úÎî© Ïã§Ìå®: {e}")
            
            # Diffusion Pose Î™®Îç∏ Î°úÎî© (1378MB - ÎåÄÌòï Í≥†ÌíàÏßà)
            if self.model_paths.get("diffusion"):
                try:
                    diffusion_model = RealDiffusionPoseModel(self.model_paths["diffusion"], self.device)
                    if diffusion_model.load_diffusion_checkpoint():
                        self.ai_models["diffusion"] = diffusion_model
                        self.loaded_models.append("diffusion")
                        success_count += 1
                        self.logger.info("‚úÖ Diffusion Pose Î™®Îç∏ Î°úÎî© ÏôÑÎ£å")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Diffusion Pose Î°úÎî© Ïã§Ìå®: {e}")
            
            # Body Pose Î™®Îç∏ Î°úÎî© (97.8MB - Î≥¥Ï°∞)
            if self.model_paths.get("body_pose"):
                try:
                    body_pose_model = RealBodyPoseModel(self.model_paths["body_pose"], self.device)
                    if body_pose_model.load_body_pose_checkpoint():
                        self.ai_models["body_pose"] = body_pose_model
                        self.loaded_models.append("body_pose")
                        success_count += 1
                        self.logger.info("‚úÖ Body Pose Î™®Îç∏ Î°úÎî© ÏôÑÎ£å")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Body Pose Î°úÎî© Ïã§Ìå®: {e}")
            
            # 3. ModelLoader Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ïó∞Îèô (ÏûàÎäî Í≤ΩÏö∞)
            if hasattr(self, 'model_loader') and self.model_loader:
                try:
                    # ModelLoaderÏóê AI Î™®Îç∏Îì§ Îì±Î°ù
                    for model_name, model_instance in self.ai_models.items():
                        if hasattr(self.model_loader, 'register_model'):
                            self.model_loader.register_model(f"step_02_{model_name}", model_instance)
                    
                    self.logger.info("‚úÖ ModelLoader Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ïó∞Îèô ÏôÑÎ£å")
                except Exception as e:
                    self.logger.debug(f"ModelLoader Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ïó∞Îèô Ïã§Ìå®: {e}")
            
            if success_count > 0:
                self.logger.info(f"üéâ Ïã§Ï†ú AI Î™®Îç∏ Î°úÎî© ÏôÑÎ£å: {success_count}Í∞ú ({self.loaded_models})")
                return True
            else:
                error_msg = "Î™®Îì† AI Î™®Îç∏ Î°úÎî© Ïã§Ìå®"
                self.logger.error(f"‚ùå {error_msg}")
                if self.strict_mode:
                    raise RuntimeError(f"Strict Mode: {error_msg}")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Ïã§Ï†ú AI Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            if self.strict_mode:
                raise
            return False


    # ==============================================
    # üî• SmartModelPathMapper ÏóÖÎç∞Ïù¥Ìä∏ (HRNet Ï∂îÍ∞Ä)
    # ==============================================
    
    async def _load_real_ai_models(self) -> bool:
        """Ïã§Ï†ú 3.4GB AI Î™®Îç∏Îì§ Î°úÎî© (ModelLoader Ïó∞Îèô)"""
        try:
            self.logger.info("üîÑ Ïã§Ï†ú AI Î™®Îç∏ ÌååÏùºÎì§ Î°úÎî© ÏãúÏûë...")
            
            # 1. SmartModelPathMapperÎ°ú Ïã§Ï†ú ÌååÏùº Í≤ΩÎ°ú ÌÉêÏßÄ
            self.model_paths = self.model_mapper.get_step02_model_paths()
            
            if not any(self.model_paths.values()):
                error_msg = "Ïã§Ï†ú AI Î™®Îç∏ ÌååÏùºÎì§ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå"
                self.logger.error(f"‚ùå {error_msg}")
                if self.strict_mode:
                    raise FileNotFoundError(f"Strict Mode: {error_msg}")
                return False
            
            # 2. ModelLoaderÎ•º ÌÜµÌïú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Î∞è AI Î™®Îç∏ ÌÅ¥ÎûòÏä§ ÏÉùÏÑ±
            success_count = 0
            
            # YOLOv8-Pose Î™®Îç∏ Î°úÎî© (6.5MB - Ïã§ÏãúÍ∞Ñ)
            if self.model_paths.get("yolov8"):
                try:
                    yolo_model = RealYOLOv8PoseModel(self.model_paths["yolov8"], self.device)
                    if yolo_model.load_yolo_checkpoint():
                        self.ai_models["yolov8"] = yolo_model
                        self.loaded_models.append("yolov8")
                        success_count += 1
                        self.logger.info("‚úÖ YOLOv8-Pose Î™®Îç∏ Î°úÎî© ÏôÑÎ£å")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è YOLOv8-Pose Î°úÎî© Ïã§Ìå®: {e}")
            
            # OpenPose Î™®Îç∏ Î°úÎî© (97.8MB - Ï†ïÎ∞Ä)
            if self.model_paths.get("openpose"):
                try:
                    openpose_model = RealOpenPoseModel(self.model_paths["openpose"], self.device)
                    if openpose_model.load_openpose_checkpoint():
                        self.ai_models["openpose"] = openpose_model
                        self.loaded_models.append("openpose")
                        success_count += 1
                        self.logger.info("‚úÖ OpenPose Î™®Îç∏ Î°úÎî© ÏôÑÎ£å")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è OpenPose Î°úÎî© Ïã§Ìå®: {e}")
            
            # Diffusion Pose Î™®Îç∏ Î°úÎî© (1378MB - ÎåÄÌòï Í≥†ÌíàÏßà)
            if self.model_paths.get("diffusion"):
                try:
                    diffusion_model = RealDiffusionPoseModel(self.model_paths["diffusion"], self.device)
                    if diffusion_model.load_diffusion_checkpoint():
                        self.ai_models["diffusion"] = diffusion_model
                        self.loaded_models.append("diffusion")
                        success_count += 1
                        self.logger.info("‚úÖ Diffusion Pose Î™®Îç∏ Î°úÎî© ÏôÑÎ£å")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Diffusion Pose Î°úÎî© Ïã§Ìå®: {e}")
            
            # Body Pose Î™®Îç∏ Î°úÎî© (97.8MB - Î≥¥Ï°∞)
            if self.model_paths.get("body_pose"):
                try:
                    body_pose_model = RealBodyPoseModel(self.model_paths["body_pose"], self.device)
                    if body_pose_model.load_body_pose_checkpoint():
                        self.ai_models["body_pose"] = body_pose_model
                        self.loaded_models.append("body_pose")
                        success_count += 1
                        self.logger.info("‚úÖ Body Pose Î™®Îç∏ Î°úÎî© ÏôÑÎ£å")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Body Pose Î°úÎî© Ïã§Ìå®: {e}")
            
            # 3. ModelLoader Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ïó∞Îèô (ÏûàÎäî Í≤ΩÏö∞)
            if hasattr(self, 'model_loader') and self.model_loader:
                try:
                    # ModelLoaderÏóê AI Î™®Îç∏Îì§ Îì±Î°ù
                    for model_name, model_instance in self.ai_models.items():
                        if hasattr(self.model_loader, 'register_model'):
                            self.model_loader.register_model(f"step_02_{model_name}", model_instance)
                    
                    self.logger.info("‚úÖ ModelLoader Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ïó∞Îèô ÏôÑÎ£å")
                except Exception as e:
                    self.logger.debug(f"ModelLoader Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ïó∞Îèô Ïã§Ìå®: {e}")
            
            if success_count > 0:
                self.logger.info(f"üéâ Ïã§Ï†ú AI Î™®Îç∏ Î°úÎî© ÏôÑÎ£å: {success_count}Í∞ú ({self.loaded_models})")
                return True
            else:
                error_msg = "Î™®Îì† AI Î™®Îç∏ Î°úÎî© Ïã§Ìå®"
                self.logger.error(f"‚ùå {error_msg}")
                if self.strict_mode:
                    raise RuntimeError(f"Strict Mode: {error_msg}")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Ïã§Ï†ú AI Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            if self.strict_mode:
                raise
            return False
    
    # ==============================================
    # üî• Î©îÏù∏ Ï≤òÎ¶¨ Î©îÏÑúÎìú - Ïã§Ï†ú AI Ï∂îÎ°† Ïã§Ìñâ
    # ==============================================
    
    async def process(
        self, 
        image: Union[np.ndarray, Image.Image, str],
        clothing_type: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Ïã§Ï†ú AI Î™®Îç∏ Í∏∞Î∞ò Ìè¨Ï¶à Ï∂îÏ†ï Ï≤òÎ¶¨
        
        Args:
            image: ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ
            clothing_type: ÏùòÎ•ò ÌÉÄÏûÖ (ÏÑ†ÌÉùÏ†Å)
            **kwargs: Ï∂îÍ∞Ä ÏÑ§Ï†ï
            
        Returns:
            Dict[str, Any]: ÏôÑÏ†ÑÌïú Ïã§Ï†ú AI Ìè¨Ï¶à Ï∂îÏ†ï Í≤∞Í≥º
        """
        try:
            # Ï¥àÍ∏∞Ìôî Í≤ÄÏ¶ù
            if not self.is_initialized:
                if not await self.initialize():
                    error_msg = "Ïã§Ï†ú AI Ï¥àÍ∏∞Ìôî Ïã§Ìå®"
                    if self.strict_mode:
                        raise RuntimeError(f"Strict Mode: {error_msg}")
                    return self._create_error_result(error_msg)
            
            start_time = time.time()
            self.logger.info(f"üß† {self.step_name} Ïã§Ï†ú AI Î™®Îç∏ Í∏∞Î∞ò Ï≤òÎ¶¨ ÏãúÏûë")
            
            # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
            processed_image = self._preprocess_image(image)
            if processed_image is None:
                error_msg = "Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå®"
                if self.strict_mode:
                    raise ValueError(f"Strict Mode: {error_msg}")
                return self._create_error_result(error_msg)
            
            # Ï∫êÏãú ÌôïÏù∏
            cache_key = None
            if getattr(self.config, 'cache_enabled', True):
                cache_key = self._generate_cache_key(processed_image, clothing_type)
                if cache_key in self.prediction_cache:
                    self.logger.info("üìã Ï∫êÏãúÏóêÏÑú Í≤∞Í≥º Î∞òÌôò")
                    self.performance_metrics['cache_hits'] += 1
                    return self.prediction_cache[cache_key]
            
            # Ïã§Ï†ú AI Î™®Îç∏ Ï∂îÎ°† Ïã§Ìñâ
            pose_result = await self._run_real_ai_inference(processed_image, clothing_type, **kwargs)
            
            if not pose_result or not pose_result.get('success', False):
                error_msg = f"Ïã§Ï†ú AI Ìè¨Ï¶à Ï∂îÏ†ï Ïã§Ìå®: {pose_result.get('error', 'Unknown AI Error') if pose_result else 'No Result'}"
                self.logger.error(f"‚ùå {error_msg}")
                self.performance_metrics['error_count'] += 1
                if self.strict_mode:
                    raise RuntimeError(f"Strict Mode: {error_msg}")
                return self._create_error_result(error_msg)
            
            # ÏôÑÏ†ÑÌïú Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨
            final_result = self._postprocess_ai_result(pose_result, processed_image, start_time)
            
            # Ï∫êÏãú Ï†ÄÏû•
            if getattr(self.config, 'cache_enabled', True) and cache_key:
                self._save_to_cache(cache_key, final_result)
            
            # ÏÑ±Îä• Î©îÌä∏Î¶≠ ÏóÖÎç∞Ïù¥Ìä∏
            processing_time = time.time() - start_time
            self.performance_metrics['process_count'] += 1
            self.performance_metrics['success_count'] += 1
            self.performance_metrics['total_process_time'] += processing_time
            self.performance_metrics['average_process_time'] = (
                self.performance_metrics['total_process_time'] / self.performance_metrics['process_count']
            )
            
            self.logger.info(f"‚úÖ {self.step_name} Ïã§Ï†ú AI Î™®Îç∏ Í∏∞Î∞ò Ï≤òÎ¶¨ ÏÑ±Í≥µ ({processing_time:.2f}Ï¥à)")
            self.logger.info(f"üéØ Í≤ÄÏ∂úÎêú ÌÇ§Ìè¨Ïù∏Ìä∏ Ïàò: {len(final_result.get('keypoints', []))}")
            self.logger.info(f"üéñÔ∏è AI Ïã†Î¢∞ÎèÑ: {final_result.get('pose_analysis', {}).get('ai_confidence', 0):.3f}")
            self.logger.info(f"ü§ñ ÏÇ¨Ïö©Îêú AI Î™®Îç∏Îì§: {final_result.get('models_used', [])}")
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"‚ùå {self.step_name} Ïã§Ï†ú AI Î™®Îç∏ Í∏∞Î∞ò Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
            self.logger.error(f"üìã Ïò§Î•ò Ïä§ÌÉù: {traceback.format_exc()}")
            self.performance_metrics['error_count'] += 1
            if self.strict_mode:
                raise
            return self._create_error_result(str(e))
    
    async def _run_real_ai_inference(
        self, 
        image: Image.Image, 
        clothing_type: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Ïã§Ï†ú AI Î™®Îç∏Îì§ÏùÑ ÌÜµÌïú Ìè¨Ï¶à Ï∂îÏ†ï Ï∂îÎ°†"""
        try:
            inference_start = time.time()
            self.logger.info(f"üß† Ïã§Ï†ú AI Î™®Îç∏ Ï∂îÎ°† ÏãúÏûë...")
            
            if not self.ai_models:
                error_msg = "Î°úÎî©Îêú AI Î™®Îç∏Ïù¥ ÏóÜÏùå"
                self.logger.error(f"‚ùå {error_msg}")
                return {'success': False, 'error': error_msg}
            
            # 1. YOLOv8-PoseÎ°ú Ïã§ÏãúÍ∞Ñ Í≤ÄÏ∂ú (Ïö∞ÏÑ†ÏàúÏúÑ 1)
            yolo_result = None
            if "yolov8" in self.ai_models:
                try:
                    yolo_result = self.ai_models["yolov8"].detect_poses_realtime(image)
                    self.logger.info(f"‚úÖ YOLOv8 Ï∂îÎ°† ÏôÑÎ£å: {yolo_result.get('success', False)}")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è YOLOv8 Ï∂îÎ°† Ïã§Ìå®: {e}")
            
            # 2. OpenPoseÎ°ú Ï†ïÎ∞Ä Í≤ÄÏ∂ú (Ïö∞ÏÑ†ÏàúÏúÑ 2)
            openpose_result = None
            if "openpose" in self.ai_models:
                try:
                    openpose_result = self.ai_models["openpose"].detect_keypoints_precise(image)
                    self.logger.info(f"‚úÖ OpenPose Ï∂îÎ°† ÏôÑÎ£å: {openpose_result.get('success', False)}")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è OpenPose Ï∂îÎ°† Ïã§Ìå®: {e}")
            
            # 3. Body PoseÎ°ú Î≥¥Ï°∞ Í≤ÄÏ∂ú (Ïö∞ÏÑ†ÏàúÏúÑ 3)
            body_pose_result = None
            if "body_pose" in self.ai_models:
                try:
                    body_pose_result = self.ai_models["body_pose"].detect_body_pose(image)
                    self.logger.info(f"‚úÖ Body Pose Ï∂îÎ°† ÏôÑÎ£å: {body_pose_result.get('success', False)}")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Body Pose Ï∂îÎ°† Ïã§Ìå®: {e}")
            
            # HRNetÏúºÎ°ú Í≥†Ï†ïÎ∞Ä Í≤ÄÏ∂ú (Ïö∞ÏÑ†ÏàúÏúÑ 3 - ÏÉàÎ°ú Ï∂îÍ∞Ä)
            hrnet_result = None
            if "hrnet" in self.ai_models:
                try:
                    hrnet_result = self.ai_models["hrnet"].detect_high_precision_pose(image)
                    self.logger.info(f"‚úÖ HRNet Ï∂îÎ°† ÏôÑÎ£å: {hrnet_result.get('success', False)}")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è HRNet Ï∂îÎ°† Ïã§Ìå®: {e}")
            
            # 4. ÏµúÏ†Å Í≤∞Í≥º ÏÑ†ÌÉù Î∞è ÌÜµÌï© (HRNet Ìè¨Ìï®)
            primary_result = self._select_best_pose_result(yolo_result, openpose_result, body_pose_result, hrnet_result)
            
            if not primary_result or not primary_result.get('keypoints'):
                error_msg = "Î™®Îì† AI Î™®Îç∏ÏóêÏÑú Ïú†Ìö®Ìïú Ìè¨Ï¶àÎ•º Í≤ÄÏ∂úÌïòÏßÄ Î™ªÌï®"
                self.logger.error(f"‚ùå {error_msg}")
                return {'success': False, 'error': error_msg}
            
            # 5. Diffusion PoseÎ°ú ÌíàÏßà Ìñ•ÏÉÅ (ÏÑ†ÌÉùÏ†Å)
            enhanced_result = primary_result
            if "diffusion" in self.ai_models and primary_result.get('keypoints'):
                try:
                    diffusion_result = self.ai_models["diffusion"].enhance_pose_quality(
                        primary_result['keypoints'], image
                    )
                    if diffusion_result.get('success', False):
                        enhanced_result['keypoints'] = diffusion_result['enhanced_keypoints']
                        enhanced_result['enhanced_by_diffusion'] = True
                        self.logger.info("‚úÖ Diffusion ÌíàÏßà Ìñ•ÏÉÅ ÏôÑÎ£å")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Diffusion ÌíàÏßà Ìñ•ÏÉÅ Ïã§Ìå®: {e}")
            
            # 6. Í≤∞Í≥º ÌÜµÌï© Î∞è Î∂ÑÏÑù
            combined_keypoints = enhanced_result['keypoints']
            combined_result = {
                'keypoints': combined_keypoints,
                'skeleton_structure': self._build_skeleton_structure(combined_keypoints),
                'joint_connections': self._get_joint_connections(combined_keypoints),
                'joint_angles': self._calculate_joint_angles(combined_keypoints),
                'body_orientation': self._get_body_orientation(combined_keypoints),
                'landmarks': self._extract_landmarks(combined_keypoints),
                'confidence_scores': [kp[2] for kp in combined_keypoints if len(kp) > 2],
                'processing_time': time.time() - inference_start,
                'models_used': self.loaded_models,
                'primary_model': primary_result.get('model_type', 'unknown'),
                'enhanced_by_diffusion': enhanced_result.get('enhanced_by_diffusion', False),
                'success': True
            }
            
            inference_time = time.time() - inference_start
            self.logger.info(f"‚úÖ Ïã§Ï†ú AI Î™®Îç∏ Ï∂îÎ°† ÏôÑÎ£å ({inference_time:.3f}Ï¥à)")
            
            return combined_result
            
        except Exception as e:
            self.logger.error(f"‚ùå Ïã§Ï†ú AI Î™®Îç∏ Ï∂îÎ°† Ïã§Ìå®: {e}")
            if self.strict_mode:
                raise
            return {'success': False, 'error': str(e)}
    
    def _select_best_pose_result(self, yolo_result, openpose_result, body_pose_result, hrnet_result=None) -> Optional[Dict[str, Any]]:
        """ÏµúÏ†ÅÏùò Ìè¨Ï¶à Í≤∞Í≥º ÏÑ†ÌÉù (HRNet Ìè¨Ìï®)"""
        results = []
        
        # Í∞Å Í≤∞Í≥ºÏùò ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞
        if yolo_result and yolo_result.get('success') and yolo_result.get('keypoints'):
            confidence = np.mean([kp[2] for kp in yolo_result['keypoints'] if len(kp) > 2])
            visible_kpts = sum(1 for kp in yolo_result['keypoints'] if len(kp) > 2 and kp[2] > 0.3)
            quality_score = confidence * 0.7 + (visible_kpts / 18) * 0.3
            results.append((quality_score, yolo_result))
        
        if openpose_result and openpose_result.get('success') and openpose_result.get('keypoints'):
            confidence = np.mean([kp[2] for kp in openpose_result['keypoints'] if len(kp) > 2])
            visible_kpts = sum(1 for kp in openpose_result['keypoints'] if len(kp) > 2 and kp[2] > 0.3)
            quality_score = confidence * 0.8 + (visible_kpts / 18) * 0.2  # OpenPoseÎäî Ïã†Î¢∞ÎèÑ Í∞ÄÏ§ëÏπò ÎÜíÏùå
            results.append((quality_score, openpose_result))
        
        if hrnet_result and hrnet_result.get('success') and hrnet_result.get('keypoints'):
            confidence = np.mean([kp[2] for kp in hrnet_result['keypoints'] if len(kp) > 2])
            visible_kpts = sum(1 for kp in hrnet_result['keypoints'] if len(kp) > 2 and kp[2] > 0.3)
            quality_score = confidence * 0.85 + (visible_kpts / 18) * 0.15  # HRNetÏùÄ Í≥†Ï†ïÎ∞ÄÏù¥ÎØÄÎ°ú Ïã†Î¢∞ÎèÑ ÏµúÏö∞ÏÑ†
            results.append((quality_score, hrnet_result))
        
        if body_pose_result and body_pose_result.get('success') and body_pose_result.get('keypoints'):
            confidence = np.mean([kp[2] for kp in body_pose_result['keypoints'] if len(kp) > 2])
            visible_kpts = sum(1 for kp in body_pose_result['keypoints'] if len(kp) > 2 and kp[2] > 0.3)
            quality_score = confidence * 0.6 + (visible_kpts / 18) * 0.4  # Body PoseÎäî Î≥¥Ï°∞ Ïó≠Ìï†
            results.append((quality_score, body_pose_result))
        
        if not results:
            return None
        
        # ÏµúÍ≥† ÌíàÏßà Ï†êÏàò Í≤∞Í≥º ÏÑ†ÌÉù
        best_score, best_result = max(results, key=lambda x: x[0])
        self.logger.info(f"üèÜ ÏµúÏ†Å Ìè¨Ï¶à Í≤∞Í≥º ÏÑ†ÌÉù: {best_result.get('model_type', 'unknown')} (Ï†êÏàò: {best_score:.3f})")
        
        return best_result
    
    # ==============================================
    # üî• Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Î∞è ÌõÑÏ≤òÎ¶¨
    # ==============================================
    
    def _preprocess_image(self, image: Union[np.ndarray, Image.Image, str]) -> Optional[Image.Image]:
        """Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨"""
        try:
            # Ïù¥ÎØ∏ÏßÄ Î°úÎî© Î∞è Î≥ÄÌôò
            if isinstance(image, str):
                if os.path.exists(image):
                    image = Image.open(image)
                else:
                    try:
                        image_data = base64.b64decode(image)
                        image = Image.open(BytesIO(image_data))
                    except Exception:
                        return None
            elif isinstance(image, np.ndarray):
                if image.size == 0:
                    return None
                image = Image.fromarray(image)
            elif not isinstance(image, Image.Image):
                return None
            
            # RGB Î≥ÄÌôò
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # ÌÅ¨Í∏∞ Í≤ÄÏ¶ù
            if image.size[0] < 64 or image.size[1] < 64:
                return None
            
            # ÌÅ¨Í∏∞ Ï°∞Ï†ï (AI Î™®Îç∏ ÏûÖÎ†•Ïö©)
            max_size = 1024 if IS_M3_MAX else 512
            if max(image.size) > max_size:
                ratio = max_size / max(image.size)
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = image.resize(new_size, Image.Resampling.BILINEAR)
            
            return image
            
        except Exception as e:
            self.logger.error(f"‚ùå Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
            return None
    
    def _postprocess_ai_result(self, pose_result: Dict[str, Any], image: Image.Image, start_time: float) -> Dict[str, Any]:
        """AI Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨"""
        try:
            processing_time = time.time() - start_time
            
            # PoseMetrics ÏÉùÏÑ±
            pose_metrics = PoseMetrics(
                keypoints=pose_result.get('keypoints', []),
                confidence_scores=pose_result.get('confidence_scores', []),
                model_used=pose_result.get('primary_model', 'unknown'),
                processing_time=processing_time,
                image_resolution=image.size,
                ai_confidence=np.mean(pose_result.get('confidence_scores', [0])) if pose_result.get('confidence_scores') else 0.0
            )
            
            # Ìè¨Ï¶à Î∂ÑÏÑù
            pose_analysis = self._analyze_pose_quality(pose_metrics, clothing_type=None)
            
            # ÏãúÍ∞ÅÌôî ÏÉùÏÑ±
            visualization = None
            if self.visualization_enabled:
                visualization = self._create_pose_visualization(image, pose_metrics)
            
            # ÏµúÏ¢Ö Í≤∞Í≥º Íµ¨ÏÑ±
            result = {
                'success': pose_result.get('success', False),
                'keypoints': pose_metrics.keypoints,
                'confidence_scores': pose_metrics.confidence_scores,
                'skeleton_structure': pose_result.get('skeleton_structure', {}),
                'joint_connections': pose_result.get('joint_connections', []),
                'joint_angles': pose_result.get('joint_angles', {}),
                'body_orientation': pose_result.get('body_orientation', {}),
                'landmarks': pose_result.get('landmarks', {}),
                'pose_analysis': pose_analysis,
                'visualization': visualization,
                'processing_time': processing_time,
                'inference_time': pose_result.get('processing_time', 0.0),
                'models_used': pose_result.get('models_used', []),
                'primary_model': pose_result.get('primary_model', 'unknown'),
                'enhanced_by_diffusion': pose_result.get('enhanced_by_diffusion', False),
                'image_resolution': pose_metrics.image_resolution,
                'step_info': {
                    'step_name': self.step_name,
                    'step_number': self.step_number,
                    'device': self.device,
                    'loaded_models': self.loaded_models,
                    'dependency_injection_status': self.dependencies_injected,
                    'real_ai_models': True,
                    'basestep_version': '16.0-compatible'
                }
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå AI Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨ Ïã§Ìå®: {e}")
            return self._create_error_result(str(e))
    
    # ==============================================
    # üî• Ìè¨Ï¶à Î∂ÑÏÑù Î∞è ÌíàÏßà ÌèâÍ∞Ä
    # ==============================================
    
    def _analyze_pose_quality(self, pose_metrics: PoseMetrics, clothing_type: Optional[str] = None) -> Dict[str, Any]:
        """Ìè¨Ï¶à ÌíàÏßà Î∂ÑÏÑù"""
        try:
            if not pose_metrics.keypoints:
                return {
                    'suitable_for_fitting': False,
                    'issues': ['AI Î™®Îç∏ÏóêÏÑú Ìè¨Ï¶àÎ•º Í≤ÄÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§'],
                    'recommendations': ['Îçî ÏÑ†Î™ÖÌïú Ïù¥ÎØ∏ÏßÄÎ•º ÏÇ¨Ïö©ÌïòÍ±∞ÎÇò Ìè¨Ï¶àÎ•º Î™ÖÌôïÌûà Ìï¥Ï£ºÏÑ∏Ïöî'],
                    'quality_score': 0.0,
                    'ai_confidence': 0.0
                }
            
            # AI Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
            ai_confidence = np.mean(pose_metrics.confidence_scores) if pose_metrics.confidence_scores else 0.0
            
            # Ïã†Ï≤¥ Î∂ÄÏúÑÎ≥Ñ Ï†êÏàò Í≥ÑÏÇ∞
            head_score = self._calculate_body_part_score(pose_metrics.keypoints, [0, 15, 16, 17, 18])
            torso_score = self._calculate_body_part_score(pose_metrics.keypoints, [1, 2, 5, 8])
            arms_score = self._calculate_body_part_score(pose_metrics.keypoints, [2, 3, 4, 5, 6, 7])
            legs_score = self._calculate_body_part_score(pose_metrics.keypoints, [9, 10, 11, 12, 13, 14])
            
            # Í≥†Í∏â Î∂ÑÏÑù
            symmetry_score = self._calculate_symmetry_score(pose_metrics.keypoints)
            visibility_score = self._calculate_visibility_score(pose_metrics.keypoints)
            pose_angles = self._calculate_joint_angles(pose_metrics.keypoints)
            body_proportions = self._calculate_body_proportions(pose_metrics.keypoints, pose_metrics.image_resolution)
            
            # Ï†ÑÏ≤¥ ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞
            quality_score = self._calculate_overall_quality_score(
                head_score, torso_score, arms_score, legs_score, 
                symmetry_score, visibility_score, ai_confidence
            )
            
            # Ï†ÅÌï©ÏÑ± ÌåêÎã®
            min_score = 0.75 if self.strict_mode else 0.65
            min_confidence = 0.7 if self.strict_mode else 0.6
            visible_keypoints = sum(1 for kp in pose_metrics.keypoints if len(kp) > 2 and kp[2] > 0.5)
            suitable_for_fitting = (quality_score >= min_score and 
                                  ai_confidence >= min_confidence and
                                  visible_keypoints >= 10)
            
            # Ïù¥Ïäà Î∞è Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±
            issues = []
            recommendations = []
            
            if ai_confidence < min_confidence:
                issues.append(f'AI Î™®Îç∏Ïùò Ïã†Î¢∞ÎèÑÍ∞Ä ÎÇÆÏäµÎãàÎã§ ({ai_confidence:.2f})')
                recommendations.append('Ï°∞Î™ÖÏù¥ Ï¢ãÏùÄ ÌôòÍ≤ΩÏóêÏÑú Îã§Ïãú Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî')
            
            if visible_keypoints < 10:
                issues.append('Ï£ºÏöî ÌÇ§Ìè¨Ïù∏Ìä∏ Í∞ÄÏãúÏÑ±Ïù¥ Î∂ÄÏ°±Ìï©ÎãàÎã§')
                recommendations.append('Ï†ÑÏã†Ïù¥ Î™ÖÌôïÌûà Î≥¥Ïù¥ÎèÑÎ°ù Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî')
            
            if symmetry_score < 0.6:
                issues.append('Ï¢åÏö∞ ÎåÄÏπ≠ÏÑ±Ïù¥ Î∂ÄÏ°±Ìï©ÎãàÎã§')
                recommendations.append('Ï†ïÎ©¥ÏùÑ Ìñ•Ìï¥ Í∑†ÌòïÏû°Ìûå ÏûêÏÑ∏Î°ú Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî')
            
            if torso_score < 0.7:
                issues.append('ÏÉÅÏ≤¥ Ìè¨Ï¶àÍ∞Ä Î∂àÎ∂ÑÎ™ÖÌï©ÎãàÎã§')
                recommendations.append('Ïñ¥Íπ®ÏôÄ ÌåîÏù¥ Î™ÖÌôïÌûà Î≥¥Ïù¥ÎèÑÎ°ù Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî')
            
            return {
                'suitable_for_fitting': suitable_for_fitting,
                'issues': issues,
                'recommendations': recommendations,
                'quality_score': quality_score,
                'ai_confidence': ai_confidence,
                'visible_keypoints': visible_keypoints,
                'total_keypoints': len(pose_metrics.keypoints),
                
                # Ïã†Ï≤¥ Î∂ÄÏúÑÎ≥Ñ ÏÉÅÏÑ∏ Ï†êÏàò
                'detailed_scores': {
                    'head': head_score,
                    'torso': torso_score,
                    'arms': arms_score,
                    'legs': legs_score
                },
                
                # Í≥†Í∏â Î∂ÑÏÑù Í≤∞Í≥º
                'advanced_analysis': {
                    'symmetry_score': symmetry_score,
                    'visibility_score': visibility_score,
                    'pose_angles': pose_angles,
                    'body_proportions': body_proportions
                },
                
                # AI Î™®Îç∏ ÏÑ±Îä• Ï†ïÎ≥¥
                'model_performance': {
                    'model_name': pose_metrics.model_used,
                    'processing_time': pose_metrics.processing_time,
                    'real_ai_models': True
                }
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Ìè¨Ï¶à ÌíàÏßà Î∂ÑÏÑù Ïã§Ìå®: {e}")
            if self.strict_mode:
                raise
            return {
                'suitable_for_fitting': False,
                'issues': ['Î∂ÑÏÑù Ïã§Ìå®'],
                'recommendations': ['Îã§Ïãú ÏãúÎèÑÌï¥ Ï£ºÏÑ∏Ïöî'],
                'quality_score': 0.0,
                'ai_confidence': 0.0
            }
    
    def _calculate_body_part_score(self, keypoints: List[List[float]], part_indices: List[int]) -> float:
        """Ïã†Ï≤¥ Î∂ÄÏúÑÎ≥Ñ Ï†êÏàò Í≥ÑÏÇ∞"""
        try:
            if not keypoints or not part_indices:
                return 0.0
            
            visible_count = 0
            total_confidence = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > self.confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            
            return visibility_ratio * avg_confidence
            
        except Exception as e:
            self.logger.debug(f"Ïã†Ï≤¥ Î∂ÄÏúÑ Ï†êÏàò Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 0.0
    
    def _calculate_symmetry_score(self, keypoints: List[List[float]]) -> float:
        """Ï¢åÏö∞ ÎåÄÏπ≠ÏÑ± Ï†êÏàò Í≥ÑÏÇ∞"""
        try:
            if not keypoints or len(keypoints) < 18:
                return 0.0
            
            # ÎåÄÏπ≠ Î∂ÄÏúÑ Ïåç Ï†ïÏùò
            symmetric_pairs = [
                (2, 5),   # right_shoulder, left_shoulder
                (3, 6),   # right_elbow, left_elbow
                (4, 7),   # right_wrist, left_wrist
                (9, 12),  # right_hip, left_hip
                (10, 13), # right_knee, left_knee
                (11, 14), # right_ankle, left_ankle
                (15, 16), # right_eye, left_eye
                (17, 18)  # right_ear, left_ear
            ]
            
            symmetry_scores = []
            confidence_threshold = 0.3
            
            for right_idx, left_idx in symmetric_pairs:
                if (right_idx < len(keypoints) and left_idx < len(keypoints) and
                    len(keypoints[right_idx]) >= 3 and len(keypoints[left_idx]) >= 3):
                    
                    right_kp = keypoints[right_idx]
                    left_kp = keypoints[left_idx]
                    
                    if right_kp[2] > confidence_threshold and left_kp[2] > confidence_threshold:
                        # Ï§ëÏã¨ÏÑ† Í≥ÑÏÇ∞
                        center_x = sum(kp[0] for kp in keypoints if len(kp) >= 3 and kp[2] > confidence_threshold) / \
                                 max(len([kp for kp in keypoints if len(kp) >= 3 and kp[2] > confidence_threshold]), 1)
                        
                        right_dist = abs(right_kp[0] - center_x)
                        left_dist = abs(left_kp[0] - center_x)
                        
                        max_dist = max(right_dist, left_dist)
                        if max_dist > 0:
                            symmetry = 1.0 - abs(right_dist - left_dist) / max_dist
                            weighted_symmetry = symmetry * min(right_kp[2], left_kp[2])
                            symmetry_scores.append(weighted_symmetry)
            
            if not symmetry_scores:
                return 0.0
            
            return np.mean(symmetry_scores)
            
        except Exception as e:
            self.logger.debug(f"ÎåÄÏπ≠ÏÑ± Ï†êÏàò Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 0.0
    
    def _calculate_visibility_score(self, keypoints: List[List[float]]) -> float:
        """ÌÇ§Ìè¨Ïù∏Ìä∏ Í∞ÄÏãúÏÑ± Ï†êÏàò Í≥ÑÏÇ∞"""
        try:
            if not keypoints:
                return 0.0
            
            visible_count = 0
            total_confidence = 0.0
            
            for kp in keypoints:
                if len(kp) >= 3:
                    if kp[2] > self.confidence_threshold:
                        visible_count += 1
                        total_confidence += kp[2]
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(keypoints)
            avg_confidence = total_confidence / visible_count
            
            return visibility_ratio * avg_confidence
            
        except Exception as e:
            self.logger.debug(f"Í∞ÄÏãúÏÑ± Ï†êÏàò Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 0.0
    
    def _calculate_overall_quality_score(
        self, head_score: float, torso_score: float, arms_score: float, legs_score: float,
        symmetry_score: float, visibility_score: float, ai_confidence: float
    ) -> float:
        """Ï†ÑÏ≤¥ ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞"""
        try:
            base_scores = [
                head_score * 0.15,
                torso_score * 0.35,
                arms_score * 0.25,
                legs_score * 0.25
            ]
            
            advanced_scores = [
                symmetry_score * 0.3,
                visibility_score * 0.7
            ]
            
            base_score = sum(base_scores)
            advanced_score = sum(advanced_scores)
            
            overall_score = (base_score * 0.7 + advanced_score * 0.3) * ai_confidence
            
            return max(0.0, min(1.0, overall_score))
            
        except Exception as e:
            self.logger.debug(f"Ï†ÑÏ≤¥ ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 0.0
    
    # ==============================================
    # üî• Ïä§ÏºàÎ†àÌÜ§ Íµ¨Ï°∞ Î∞è Í∏∞ÌïòÌïôÏ†Å Î∂ÑÏÑù
    # ==============================================
    
    def _build_skeleton_structure(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """Ïä§ÏºàÎ†àÌÜ§ Íµ¨Ï°∞ ÏÉùÏÑ±"""
        try:
            skeleton = {
                'connections': [],
                'bone_lengths': {},
                'joint_positions': {},
                'structure_valid': False
            }
            
            if not keypoints or len(keypoints) < 18:
                return skeleton
            
            # Ïó∞Í≤∞ Íµ¨Ï°∞ ÏÉùÏÑ±
            for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
                if (start_idx < len(keypoints) and end_idx < len(keypoints) and
                    len(keypoints[start_idx]) >= 3 and len(keypoints[end_idx]) >= 3):
                    
                    start_kp = keypoints[start_idx]
                    end_kp = keypoints[end_idx]
                    
                    if start_kp[2] > 0.3 and end_kp[2] > 0.3:
                        connection = {
                            'start': start_idx,
                            'end': end_idx,
                            'start_name': OPENPOSE_18_KEYPOINTS[start_idx] if start_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{start_idx}",
                            'end_name': OPENPOSE_18_KEYPOINTS[end_idx] if end_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{end_idx}",
                            'length': np.sqrt((start_kp[0] - end_kp[0])**2 + (start_kp[1] - end_kp[1])**2),
                            'confidence': (start_kp[2] + end_kp[2]) / 2
                        }
                        skeleton['connections'].append(connection)
            
            # Í¥ÄÏ†à ÏúÑÏπò
            for i, kp in enumerate(keypoints):
                if len(kp) >= 3 and kp[2] > 0.3:
                    joint_name = OPENPOSE_18_KEYPOINTS[i] if i < len(OPENPOSE_18_KEYPOINTS) else f"joint_{i}"
                    skeleton['joint_positions'][joint_name] = {
                        'x': kp[0],
                        'y': kp[1],
                        'confidence': kp[2]
                    }
            
            skeleton['structure_valid'] = len(skeleton['connections']) >= 8
            
            return skeleton
            
        except Exception as e:
            self.logger.debug(f"Ïä§ÏºàÎ†àÌÜ§ Íµ¨Ï°∞ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return {'connections': [], 'bone_lengths': {}, 'joint_positions': {}, 'structure_valid': False}
    
    def _get_joint_connections(self, keypoints: List[List[float]]) -> List[Dict[str, Any]]:
        """Í¥ÄÏ†à Ïó∞Í≤∞ Ï†ïÎ≥¥ Î∞òÌôò"""
        try:
            connections = []
            
            for start_idx, end_idx in SKELETON_CONNECTIONS:
                if (start_idx < len(keypoints) and end_idx < len(keypoints) and
                    len(keypoints[start_idx]) >= 3 and len(keypoints[end_idx]) >= 3):
                    
                    start_kp = keypoints[start_idx]
                    end_kp = keypoints[end_idx]
                    
                    if start_kp[2] > 0.3 and end_kp[2] > 0.3:
                        connection = {
                            'start_joint': start_idx,
                            'end_joint': end_idx,
                            'start_name': OPENPOSE_18_KEYPOINTS[start_idx] if start_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{start_idx}",
                            'end_name': OPENPOSE_18_KEYPOINTS[end_idx] if end_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{end_idx}",
                            'connection_strength': (start_kp[2] + end_kp[2]) / 2
                        }
                        connections.append(connection)
            
            return connections
            
        except Exception as e:
            self.logger.debug(f"Í¥ÄÏ†à Ïó∞Í≤∞ Ï†ïÎ≥¥ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return []
    
    def _calculate_joint_angles(self, keypoints: List[List[float]]) -> Dict[str, float]:
        """Í¥ÄÏ†à Í∞ÅÎèÑ Í≥ÑÏÇ∞"""
        try:
            angles = {}
            
            if not keypoints or len(keypoints) < 18:
                return angles
            
            def calculate_angle(p1, p2, p3):
                """ÏÑ∏ Ï†ê ÏÇ¨Ïù¥Ïùò Í∞ÅÎèÑ Í≥ÑÏÇ∞"""
                try:
                    v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
                    v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
                    
                    cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
                    cos_angle = np.clip(cos_angle, -1.0, 1.0)
                    angle = np.arccos(cos_angle)
                    
                    return np.degrees(angle)
                except:
                    return 0.0
            
            confidence_threshold = 0.3
            
            # ÌåîÍøàÏπò Í∞ÅÎèÑ (Ïò§Î•∏Ï™Ω)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [2, 3, 4]):
                angles['right_elbow'] = calculate_angle(keypoints[2], keypoints[3], keypoints[4])
            
            # ÌåîÍøàÏπò Í∞ÅÎèÑ (ÏôºÏ™Ω)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [5, 6, 7]):
                angles['left_elbow'] = calculate_angle(keypoints[5], keypoints[6], keypoints[7])
            
            # Î¨¥Î¶é Í∞ÅÎèÑ (Ïò§Î•∏Ï™Ω)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [9, 10, 11]):
                angles['right_knee'] = calculate_angle(keypoints[9], keypoints[10], keypoints[11])
            
            # Î¨¥Î¶é Í∞ÅÎèÑ (ÏôºÏ™Ω)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [12, 13, 14]):
                angles['left_knee'] = calculate_angle(keypoints[12], keypoints[13], keypoints[14])
            
            # Ïñ¥Íπ® Í∏∞Ïö∏Í∏∞
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [2, 5]):
                shoulder_slope = np.degrees(np.arctan2(
                    keypoints[5][1] - keypoints[2][1],
                    keypoints[5][0] - keypoints[2][0] + 1e-8
                ))
                angles['shoulder_slope'] = abs(shoulder_slope)
            
            return angles
            
        except Exception as e:
            self.logger.debug(f"Í¥ÄÏ†à Í∞ÅÎèÑ Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return {}
    
    def _get_body_orientation(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """Ïã†Ï≤¥ Î∞©Ìñ• Î∂ÑÏÑù"""
        try:
            orientation = {
                'facing_direction': 'unknown',
                'body_angle': 0.0,
                'shoulder_line_angle': 0.0,
                'hip_line_angle': 0.0,
                'is_frontal': False
            }
            
            if not keypoints or len(keypoints) < 18:
                return orientation
            
            # Ïñ¥Íπ® ÎùºÏù∏ Í∞ÅÎèÑ
            if (2 < len(keypoints) and 5 < len(keypoints) and
                len(keypoints[2]) >= 3 and len(keypoints[5]) >= 3 and
                keypoints[2][2] > 0.3 and keypoints[5][2] > 0.3):
                
                shoulder_angle = np.degrees(np.arctan2(
                    keypoints[5][1] - keypoints[2][1],
                    keypoints[5][0] - keypoints[2][0]
                ))
                orientation['shoulder_line_angle'] = shoulder_angle
                
                # Ï†ïÎ©¥ Ïó¨Î∂Ä ÌåêÎã®
                orientation['is_frontal'] = abs(shoulder_angle) < 15
            
            # ÏóâÎç©Ïù¥ ÎùºÏù∏ Í∞ÅÎèÑ
            if (9 < len(keypoints) and 12 < len(keypoints) and
                len(keypoints[9]) >= 3 and len(keypoints[12]) >= 3 and
                keypoints[9][2] > 0.3 and keypoints[12][2] > 0.3):
                
                hip_angle = np.degrees(np.arctan2(
                    keypoints[12][1] - keypoints[9][1],
                    keypoints[12][0] - keypoints[9][0]
                ))
                orientation['hip_line_angle'] = hip_angle
            
            # Ï†ÑÏ≤¥ Ïã†Ï≤¥ Í∞ÅÎèÑ (Ïñ¥Íπ®ÏôÄ ÏóâÎç©Ïù¥ ÌèâÍ∑†)
            if orientation['shoulder_line_angle'] != 0.0 and orientation['hip_line_angle'] != 0.0:
                orientation['body_angle'] = (orientation['shoulder_line_angle'] + orientation['hip_line_angle']) / 2
            elif orientation['shoulder_line_angle'] != 0.0:
                orientation['body_angle'] = orientation['shoulder_line_angle']
            elif orientation['hip_line_angle'] != 0.0:
                orientation['body_angle'] = orientation['hip_line_angle']
            
            # Î∞©Ìñ• Î∂ÑÎ•ò
            if abs(orientation['body_angle']) < 15:
                orientation['facing_direction'] = 'front'
            elif orientation['body_angle'] > 15:
                orientation['facing_direction'] = 'left'
            elif orientation['body_angle'] < -15:
                orientation['facing_direction'] = 'right'
            
            return orientation
            
        except Exception as e:
            self.logger.debug(f"Ïã†Ï≤¥ Î∞©Ìñ• Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {'facing_direction': 'unknown', 'body_angle': 0.0, 'is_frontal': False}
    
    def _extract_landmarks(self, keypoints: List[List[float]]) -> Dict[str, Dict[str, float]]:
        """Ï£ºÏöî ÎûúÎìúÎßàÌÅ¨ Ï∂îÏ∂ú"""
        try:
            landmarks = {}
            
            for i, kp in enumerate(keypoints):
                if len(kp) >= 3 and kp[2] > 0.3:
                    landmark_name = OPENPOSE_18_KEYPOINTS[i] if i < len(OPENPOSE_18_KEYPOINTS) else f"landmark_{i}"
                    landmarks[landmark_name] = {
                        'x': float(kp[0]),
                        'y': float(kp[1]),
                        'confidence': float(kp[2])
                    }
            
            return landmarks
            
        except Exception as e:
            self.logger.debug(f"ÎûúÎìúÎßàÌÅ¨ Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return {}
    
    def _calculate_body_proportions(self, keypoints: List[List[float]], image_resolution: Tuple[int, int]) -> Dict[str, float]:
        """Ïã†Ï≤¥ ÎπÑÏú® Í≥ÑÏÇ∞"""
        try:
            proportions = {}
            
            if not keypoints or len(keypoints) < 18 or not image_resolution:
                return proportions
            
            width, height = image_resolution
            confidence_threshold = 0.3
            
            def get_valid_keypoint(idx):
                if (idx < len(keypoints) and len(keypoints[idx]) >= 3 and 
                    keypoints[idx][2] > confidence_threshold):
                    return keypoints[idx]
                return None
            
            def euclidean_distance(p1, p2):
                if p1 and p2:
                    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)
                return 0.0
            
            # Î®∏Î¶¨-Î™© Í∏∏Ïù¥
            nose = get_valid_keypoint(0)
            neck = get_valid_keypoint(1)
            if nose and neck:
                proportions['head_neck_ratio'] = euclidean_distance(nose, neck) / height
            
            # ÏÉÅÏ≤¥ Í∏∏Ïù¥ (Î™©-ÏóâÎç©Ïù¥)
            if neck:
                mid_hip = get_valid_keypoint(8)
                if mid_hip:
                    proportions['torso_ratio'] = euclidean_distance(neck, mid_hip) / height
            
            # Ìåî Í∏∏Ïù¥ (Ïñ¥Íπ®-ÏÜêÎ™©)
            right_shoulder = get_valid_keypoint(2)
            right_wrist = get_valid_keypoint(4)
            if right_shoulder and right_wrist:
                proportions['right_arm_ratio'] = euclidean_distance(right_shoulder, right_wrist) / height
            
            left_shoulder = get_valid_keypoint(5)
            left_wrist = get_valid_keypoint(7)
            if left_shoulder and left_wrist:
                proportions['left_arm_ratio'] = euclidean_distance(left_shoulder, left_wrist) / height
            
            # Îã§Î¶¨ Í∏∏Ïù¥ (ÏóâÎç©Ïù¥-Î∞úÎ™©)
            right_hip = get_valid_keypoint(9)
            right_ankle = get_valid_keypoint(11)
            if right_hip and right_ankle:
                proportions['right_leg_ratio'] = euclidean_distance(right_hip, right_ankle) / height
            
            left_hip = get_valid_keypoint(12)
            left_ankle = get_valid_keypoint(14)
            if left_hip and left_ankle:
                proportions['left_leg_ratio'] = euclidean_distance(left_hip, left_ankle) / height
            
            # Ïñ¥Íπ® ÎÑàÎπÑ
            if right_shoulder and left_shoulder:
                proportions['shoulder_width_ratio'] = euclidean_distance(right_shoulder, left_shoulder) / width
            
            # ÏóâÎç©Ïù¥ ÎÑàÎπÑ
            if right_hip and left_hip:
                proportions['hip_width_ratio'] = euclidean_distance(right_hip, left_hip) / width
            
            return proportions
            
        except Exception as e:
            self.logger.debug(f"Ïã†Ï≤¥ ÎπÑÏú® Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return {}
    
    # ==============================================
    # üî• ÏãúÍ∞ÅÌôî Î∞è Ïú†Ìã∏Î¶¨Ìã∞
    # ==============================================
    
    def _create_pose_visualization(self, image: Image.Image, pose_metrics: PoseMetrics) -> Optional[str]:
        """Ìè¨Ï¶à ÏãúÍ∞ÅÌôî ÏÉùÏÑ±"""
        try:
            if not pose_metrics.keypoints:
                return None
            
            vis_image = image.copy()
            draw = ImageDraw.Draw(vis_image)
            
            # ÌÇ§Ìè¨Ïù∏Ìä∏ Í∑∏Î¶¨Í∏∞
            for i, kp in enumerate(pose_metrics.keypoints):
                if len(kp) >= 3 and kp[2] > self.confidence_threshold:
                    x, y = int(kp[0]), int(kp[1])
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    # Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò ÌÅ¨Í∏∞ Ï°∞Ï†à
                    radius = int(4 + kp[2] * 8)
                    
                    draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                               fill=color, outline=(255, 255, 255), width=2)
            
            # Ïä§ÏºàÎ†àÌÜ§ Ïó∞Í≤∞ÏÑ† Í∑∏Î¶¨Í∏∞
            for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
                if (start_idx < len(pose_metrics.keypoints) and 
                    end_idx < len(pose_metrics.keypoints)):
                    
                    start_kp = pose_metrics.keypoints[start_idx]
                    end_kp = pose_metrics.keypoints[end_idx]
                    
                    if (len(start_kp) >= 3 and len(end_kp) >= 3 and
                        start_kp[2] > self.confidence_threshold and end_kp[2] > self.confidence_threshold):
                        
                        start_point = (int(start_kp[0]), int(start_kp[1]))
                        end_point = (int(end_kp[0]), int(end_kp[1]))
                        color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                        
                        # Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò ÏÑ† ÎëêÍªò
                        avg_confidence = (start_kp[2] + end_kp[2]) / 2
                        line_width = int(2 + avg_confidence * 6)
                        
                        draw.line([start_point, end_point], fill=color, width=line_width)
            
            # AI Ïã†Î¢∞ÎèÑ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            if hasattr(pose_metrics, 'ai_confidence'):
                ai_info = f"AI Ïã†Î¢∞ÎèÑ: {pose_metrics.ai_confidence:.3f}"
                try:
                    font = ImageFont.load_default()
                    draw.text((10, 10), ai_info, fill=(255, 255, 255), font=font)
                except:
                    draw.text((10, 10), ai_info, fill=(255, 255, 255))
            
            # Base64Î°ú Ïù∏ÏΩîÎî©
            buffer = BytesIO()
            vis_image.save(buffer, format='JPEG', quality=95)
            image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            return f"data:image/jpeg;base64,{image_base64}"
            
        except Exception as e:
            self.logger.error(f"‚ùå Ìè¨Ï¶à ÏãúÍ∞ÅÌôî ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None
    
    def _generate_cache_key(self, image: Image.Image, clothing_type: Optional[str]) -> str:
        """Ï∫êÏãú ÌÇ§ ÏÉùÏÑ±"""
        try:
            image_bytes = BytesIO()
            image.save(image_bytes, format='JPEG', quality=50)
            image_hash = hashlib.md5(image_bytes.getvalue()).hexdigest()[:16]
            
            config_str = f"{clothing_type}_{self.confidence_threshold}_{len(self.loaded_models)}"
            config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]
            
            return f"pose_{image_hash}_{config_hash}"
            
        except Exception:
            return f"pose_{int(time.time())}"
    
    def _save_to_cache(self, cache_key: str, result: Dict[str, Any]):
        """Ï∫êÏãúÏóê Ï†ÄÏû•"""
        try:
            if len(self.prediction_cache) >= self.cache_max_size:
                oldest_key = next(iter(self.prediction_cache))
                del self.prediction_cache[oldest_key]
            
            cached_result = result.copy()
            cached_result['visualization'] = None  # Î©îÎ™®Î¶¨ Ï†àÏïΩ
            
            self.prediction_cache[cache_key] = cached_result
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Ï∫êÏãú Ï†ÄÏû• Ïã§Ìå®: {e}")
    
    def _create_error_result(self, error_message: str, processing_time: float = 0.0) -> Dict[str, Any]:
        """ÏóêÎü¨ Í≤∞Í≥º ÏÉùÏÑ±"""
        return {
            'success': False,
            'error': error_message,
            'keypoints': [],
            'confidence_scores': [],
            'skeleton_structure': {},
            'joint_connections': [],
            'joint_angles': {},
            'body_orientation': {},
            'landmarks': {},
            'pose_analysis': {
                'suitable_for_fitting': False,
                'issues': [error_message],
                'recommendations': ['Îã§Ïãú ÏãúÎèÑÌï¥ Ï£ºÏÑ∏Ïöî'],
                'quality_score': 0.0,
                'ai_confidence': 0.0
            },
            'visualization': None,
            'processing_time': processing_time,
            'inference_time': 0.0,
            'models_used': [],
            'primary_model': 'error',
            'enhanced_by_diffusion': False,
            'step_info': {
                'step_name': self.step_name,
                'step_number': self.step_number,
                'device': self.device,
                'loaded_models': self.loaded_models,
                'dependency_injection_status': self.dependencies_injected,
                'real_ai_models': True,
                'basestep_version': '16.0-compatible'
            }
        }
    
    # ==============================================
    # üî• BaseStepMixin v16.0 Ìò∏Ìôò Î©îÏÑúÎìúÎì§
    # ==============================================
    
    async def initialize(self) -> bool:
        """BaseStepMixin v16.0 Ìò∏Ìôò Ï¥àÍ∏∞Ìôî"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"üöÄ {self.step_name} Ïã§Ï†ú AI Î™®Îç∏ Í∏∞Î∞ò Ï¥àÍ∏∞Ìôî ÏãúÏûë")
            start_time = time.time()
            
            # ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Í≤ÄÏ¶ù
            if not hasattr(self, 'model_loader') or not self.model_loader:
                # ÏûêÎèô ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏãúÎèÑ
                if hasattr(self, 'dependency_manager'):
                    success = self.dependency_manager.auto_inject_dependencies()
                    if not success:
                        self.logger.warning("‚ö†Ô∏è ÏûêÎèô ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ïã§Ìå® - ÏàòÎèô ÏãúÎèÑ")
                        success = self._manual_auto_inject()
                else:
                    success = self._manual_auto_inject()
                
                if not success:
                    error_msg = "ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ïã§Ìå®"
                    if self.strict_mode:
                        raise RuntimeError(f"Strict Mode: {error_msg}")
                    self.logger.warning(f"‚ö†Ô∏è {error_msg} - Í∏∞Î≥∏ ÏÑ§Ï†ïÏúºÎ°ú ÏßÑÌñâ")
            
            # Ïã§Ï†ú AI Î™®Îç∏ Î°úÎî©
            model_loading_success = await self._load_real_ai_models()
            
            if not model_loading_success:
                error_msg = "Ïã§Ï†ú AI Î™®Îç∏ Î°úÎî© Ïã§Ìå®"
                if self.strict_mode:
                    raise RuntimeError(f"Strict Mode: {error_msg}")
                self.logger.warning(f"‚ö†Ô∏è {error_msg} - Í∏∞Î≥∏ ÏÑ§Ï†ïÏúºÎ°ú ÏßÑÌñâ")
            
            # Ï¥àÍ∏∞Ìôî ÏôÑÎ£å
            self.is_initialized = True
            self.is_ready = True
            self.has_model = len(self.ai_models) > 0
            self.model_loaded = self.has_model
            
            elapsed_time = time.time() - start_time
            self.logger.info(f"‚úÖ {self.step_name} Ïã§Ï†ú AI Î™®Îç∏ Í∏∞Î∞ò Ï¥àÍ∏∞Ìôî ÏôÑÎ£å ({elapsed_time:.2f}Ï¥à)")
            self.logger.info(f"ü§ñ Î°úÎî©Îêú AI Î™®Îç∏Îì§: {self.loaded_models}")
            self.logger.info(f"üîó ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏÉÅÌÉú: {self.dependencies_injected}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå {self.step_name} Ïã§Ï†ú AI Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            if self.strict_mode:
                raise
            return False
    
    def get_status(self) -> Dict[str, Any]:
        """BaseStepMixin v16.0 Ìò∏Ìôò ÏÉÅÌÉú Î∞òÌôò"""
        try:
            return {
                'step_name': self.step_name,
                'step_number': self.step_number,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'has_model': self.has_model,
                'model_loaded': self.model_loaded,
                'device': self.device,
                'is_m3_max': IS_M3_MAX,
                'dependencies': self.dependencies_injected,
                'performance_metrics': self.performance_metrics,
                'loaded_models': self.loaded_models,
                'model_paths': {k: str(v) if v else None for k, v in self.model_paths.items()},
                'real_ai_models': True,
                'ai_libraries_available': {
                    'torch': TORCH_AVAILABLE,
                    'ultralytics': ULTRALYTICS_AVAILABLE,
                    'mediapipe': MEDIAPIPE_AVAILABLE,
                    'transformers': TRANSFORMERS_AVAILABLE,
                    'safetensors': SAFETENSORS_AVAILABLE
                },
                'version': '16.0-compatible',
                'timestamp': time.time()
            }
        except Exception as e:
            self.logger.error(f"‚ùå ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {'error': str(e), 'version': '16.0-compatible', 'real_ai_models': True}
    
    async def cleanup(self) -> Dict[str, Any]:
        """BaseStepMixin v16.0 Ìò∏Ìôò Ï†ïÎ¶¨"""
        try:
            self.logger.info(f"üßπ {self.step_name} Ïã§Ï†ú AI Î™®Îç∏ Ï†ïÎ¶¨ ÏãúÏûë...")
            
            # AI Î™®Îç∏ Ï†ïÎ¶¨
            cleanup_count = 0
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'model') and model.model:
                        if hasattr(model.model, 'cpu'):
                            model.model.cpu()
                        del model.model
                        model.model = None
                        model.loaded = False
                    cleanup_count += 1
                except Exception as e:
                    self.logger.debug(f"AI Î™®Îç∏ Ï†ïÎ¶¨ Ïã§Ìå® {model_name}: {e}")
            
            self.ai_models.clear()
            self.loaded_models.clear()
            self.model_paths.clear()
            
            # Ï∫êÏãú Ï†ïÎ¶¨
            self.prediction_cache.clear()
            
            # GPU/MPS Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            if TORCH_AVAILABLE:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif IS_M3_MAX and torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                    except:
                        pass
            
            gc.collect()
            
            # ÏÉÅÌÉú Î¶¨ÏÖã
            self.is_ready = False
            self.warmup_completed = False
            self.has_model = False
            self.model_loaded = False
            
            # ÏùòÏ°¥ÏÑ± Ìï¥Ï†ú (Ï∞∏Ï°∞Îßå Ï†úÍ±∞)
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            self.logger.info(f"‚úÖ {self.step_name} Ïã§Ï†ú AI Î™®Îç∏ Ï†ïÎ¶¨ ÏôÑÎ£å ({cleanup_count}Í∞ú)")
            
            return {
                "success": True,
                "cleaned_models": cleanup_count,
                "step_name": self.step_name,
                "real_ai_models": True,
                "version": "16.0-compatible"
            }
        except Exception as e:
            self.logger.error(f"‚ùå Ïã§Ï†ú AI Î™®Îç∏ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            return {"success": False, "error": str(e), "real_ai_models": True}


# =================================================================
# üî• Ìò∏ÌôòÏÑ± ÏßÄÏõê Ìï®ÏàòÎì§ (Ïã§Ï†ú AI Î™®Îç∏ Í∏∞Î∞ò)
# =================================================================

async def create_pose_estimation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    strict_mode: bool = True,
    **kwargs
) -> PoseEstimationStep:
    """
    BaseStepMixin v16.0 Ìò∏Ìôò Ïã§Ï†ú AI Í∏∞Î∞ò Ìè¨Ï¶à Ï∂îÏ†ï Step ÏÉùÏÑ± Ìï®Ïàò
    
    Args:
        device: ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
        config: ÏÑ§Ï†ï ÎîïÏÖîÎÑàÎ¶¨
        strict_mode: ÏóÑÍ≤© Î™®Îìú
        **kwargs: Ï∂îÍ∞Ä ÏÑ§Ï†ï
        
    Returns:
        PoseEstimationStep: Ï¥àÍ∏∞ÌôîÎêú Ïã§Ï†ú AI Í∏∞Î∞ò Ìè¨Ï¶à Ï∂îÏ†ï Step
    """
    try:
        # ÎîîÎ∞îÏù¥Ïä§ Ï≤òÎ¶¨
        device_param = None if device == "auto" else device
        
        # config ÌÜµÌï©
        if config is None:
            config = {}
        config.update(kwargs)
        config['real_ai_models'] = True
        config['basestep_version'] = '16.0-compatible'
        
        # Step ÏÉùÏÑ± (BaseStepMixin v16.0 Ìò∏Ìôò + Ïã§Ï†ú AI Í∏∞Î∞ò)
        step = PoseEstimationStep(device=device_param, config=config, strict_mode=strict_mode)
        
        # Ïã§Ï†ú AI Í∏∞Î∞ò Ï¥àÍ∏∞Ìôî Ïã§Ìñâ
        initialization_success = await step.initialize()
        
        if not initialization_success:
            error_msg = "BaseStepMixin v16.0 Ìò∏Ìôò: Ïã§Ï†ú AI Î™®Îç∏ Ï¥àÍ∏∞Ìôî Ïã§Ìå®"
            if strict_mode:
                raise RuntimeError(f"Strict Mode: {error_msg}")
            else:
                step.logger.warning(f"‚ö†Ô∏è {error_msg} - Step ÏÉùÏÑ±ÏùÄ ÏôÑÎ£åÎê®")
        
        return step
        
    except Exception as e:
        logger.error(f"‚ùå BaseStepMixin v16.0 Ìò∏Ìôò create_pose_estimation_step Ïã§Ìå®: {e}")
        if strict_mode:
            raise
        else:
            step = PoseEstimationStep(device='cpu', strict_mode=False)
            return step

def create_pose_estimation_step_sync(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    strict_mode: bool = True,
    **kwargs
) -> PoseEstimationStep:
    """ÎèôÍ∏∞Ïãù BaseStepMixin v16.0 Ìò∏Ìôò Ïã§Ï†ú AI Í∏∞Î∞ò Ìè¨Ï¶à Ï∂îÏ†ï Step ÏÉùÏÑ±"""
    try:
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            create_pose_estimation_step(device, config, strict_mode, **kwargs)
        )
    except Exception as e:
        logger.error(f"‚ùå BaseStepMixin v16.0 Ìò∏Ìôò create_pose_estimation_step_sync Ïã§Ìå®: {e}")
        if strict_mode:
            raise
        else:
            return PoseEstimationStep(device='cpu', strict_mode=False)

# =================================================================
# üî• Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§ (Ïã§Ï†ú AI Î™®Îç∏ Í∏∞Î∞ò)
# =================================================================

def validate_keypoints(keypoints_18: List[List[float]]) -> bool:
    """OpenPose 18 keypoints Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù"""
    try:
        if len(keypoints_18) != 18:
            return False
        
        for kp in keypoints_18:
            if len(kp) != 3:
                return False
            if not all(isinstance(x, (int, float)) for x in kp):
                return False
            if kp[2] < 0 or kp[2] > 1:
                return False
        
        return True
        
    except Exception:
        return False

def convert_keypoints_to_coco(keypoints_18: List[List[float]]) -> List[List[float]]:
    """OpenPose 18ÏùÑ COCO 17 ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò"""
    try:
        # OpenPose 18 -> COCO 17 Îß§Ìïë
        op_to_coco_mapping = {
            0: 0,   # nose
            15: 1,  # right_eye -> left_eye (COCO Í¥ÄÏ†ê)
            16: 2,  # left_eye -> right_eye
            17: 3,  # right_ear -> left_ear
            18: 4,  # left_ear -> right_ear
            2: 5,   # right_shoulder -> left_shoulder (COCO Í¥ÄÏ†ê)
            5: 6,   # left_shoulder -> right_shoulder
            3: 7,   # right_elbow -> left_elbow
            6: 8,   # left_elbow -> right_elbow
            4: 9,   # right_wrist -> left_wrist
            7: 10,  # left_wrist -> right_wrist
            9: 11,  # right_hip -> left_hip
            12: 12, # left_hip -> right_hip
            10: 13, # right_knee -> left_knee
            13: 14, # left_knee -> right_knee
            11: 15, # right_ankle -> left_ankle
            14: 16  # left_ankle -> right_ankle
        }
        
        coco_keypoints = []
        for coco_idx in range(17):
            if coco_idx in op_to_coco_mapping.values():
                op_idx = next(k for k, v in op_to_coco_mapping.items() if v == coco_idx)
                if op_idx < len(keypoints_18):
                    coco_keypoints.append(keypoints_18[op_idx].copy())
                else:
                    coco_keypoints.append([0.0, 0.0, 0.0])
            else:
                coco_keypoints.append([0.0, 0.0, 0.0])
        
        return coco_keypoints
        
    except Exception as e:
        logger.error(f"ÌÇ§Ìè¨Ïù∏Ìä∏ Î≥ÄÌôò Ïã§Ìå®: {e}")
        return [[0.0, 0.0, 0.0]] * 17

def draw_pose_on_image(
    image: Union[np.ndarray, Image.Image],
    keypoints: List[List[float]],
    confidence_threshold: float = 0.5,
    keypoint_size: int = 4,
    line_width: int = 3
) -> Image.Image:
    """Ïù¥ÎØ∏ÏßÄÏóê Ìè¨Ï¶à Í∑∏Î¶¨Í∏∞"""
    try:
        # Ïù¥ÎØ∏ÏßÄ Î≥ÄÌôò
        if isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        else:
            pil_image = image.copy()
        
        draw = ImageDraw.Draw(pil_image)
        
        # ÌÇ§Ìè¨Ïù∏Ìä∏ Í∑∏Î¶¨Í∏∞
        for i, kp in enumerate(keypoints):
            if len(kp) >= 3 and kp[2] > confidence_threshold:
                x, y = int(kp[0]), int(kp[1])
                color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                
                radius = int(keypoint_size + kp[2] * 6)
                draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                           fill=color, outline=(255, 255, 255), width=2)
        
        # Ïä§ÏºàÎ†àÌÜ§ Í∑∏Î¶¨Í∏∞
        for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
            if (start_idx < len(keypoints) and end_idx < len(keypoints)):
                start_kp = keypoints[start_idx]
                end_kp = keypoints[end_idx]
                
                if (len(start_kp) >= 3 and len(end_kp) >= 3 and
                    start_kp[2] > confidence_threshold and end_kp[2] > confidence_threshold):
                    
                    start_point = (int(start_kp[0]), int(start_kp[1]))
                    end_point = (int(end_kp[0]), int(end_kp[1]))
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    avg_confidence = (start_kp[2] + end_kp[2]) / 2
                    adjusted_width = int(line_width * avg_confidence)
                    
                    draw.line([start_point, end_point], fill=color, width=max(1, adjusted_width))
        
        return pil_image
        
    except Exception as e:
        logger.error(f"Ìè¨Ï¶à Í∑∏Î¶¨Í∏∞ Ïã§Ìå®: {e}")
        return image if isinstance(image, Image.Image) else Image.fromarray(image)

def analyze_pose_for_clothing(
    keypoints: List[List[float]],
    clothing_type: str = "default",
    confidence_threshold: float = 0.5,
    strict_analysis: bool = True
) -> Dict[str, Any]:
    """ÏùòÎ•òÎ≥Ñ Ìè¨Ï¶à Ï†ÅÌï©ÏÑ± Î∂ÑÏÑù"""
    try:
        if not keypoints:
            return {
                'suitable_for_fitting': False,
                'issues': ["Ìè¨Ï¶àÎ•º Í≤ÄÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§"],
                'recommendations': ["Îçî ÏÑ†Î™ÖÌïú Ïù¥ÎØ∏ÏßÄÎ•º ÏÇ¨Ïö©Ìï¥ Ï£ºÏÑ∏Ïöî"],
                'pose_score': 0.0,
                'ai_confidence': 0.0
            }
        
        # ÏùòÎ•òÎ≥Ñ Í∞ÄÏ§ëÏπò
        clothing_weights = {
            'shirt': {'arms': 0.4, 'torso': 0.4, 'visibility': 0.2},
            'dress': {'torso': 0.5, 'arms': 0.3, 'legs': 0.2},
            'pants': {'legs': 0.6, 'torso': 0.3, 'visibility': 0.1},
            'jacket': {'arms': 0.5, 'torso': 0.4, 'visibility': 0.1},
            'skirt': {'torso': 0.4, 'legs': 0.4, 'visibility': 0.2},
            'top': {'torso': 0.5, 'arms': 0.4, 'visibility': 0.1},
            'default': {'torso': 0.4, 'arms': 0.3, 'legs': 0.2, 'visibility': 0.1}
        }
        
        weights = clothing_weights.get(clothing_type, clothing_weights['default'])
        
        # Ïã†Ï≤¥ Î∂ÄÏúÑÎ≥Ñ Ï†êÏàò Í≥ÑÏÇ∞
        def calculate_body_part_score(part_indices: List[int]) -> float:
            visible_count = 0
            total_confidence = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            
            return visibility_ratio * avg_confidence
        
        # Î∂ÄÏúÑÎ≥Ñ Ï†êÏàò
        head_indices = [0, 15, 16, 17, 18]
        torso_indices = [1, 2, 5, 8, 9, 12]
        arm_indices = [2, 3, 4, 5, 6, 7]
        leg_indices = [9, 10, 11, 12, 13, 14]
        
        head_score = calculate_body_part_score(head_indices)
        torso_score = calculate_body_part_score(torso_indices)
        arms_score = calculate_body_part_score(arm_indices)
        legs_score = calculate_body_part_score(leg_indices)
        
        # AI Ïã†Î¢∞ÎèÑ
        ai_confidence = np.mean([kp[2] for kp in keypoints if len(kp) > 2]) if keypoints else 0.0
        
        # Ìè¨Ï¶à Ï†êÏàò
        pose_score = (
            torso_score * weights.get('torso', 0.4) +
            arms_score * weights.get('arms', 0.3) +
            legs_score * weights.get('legs', 0.2) +
            weights.get('visibility', 0.1) * min(head_score, 1.0)
        ) * ai_confidence
        
        # Ï†ÅÌï©ÏÑ± ÌåêÎã®
        min_score = 0.8 if strict_analysis else 0.7
        min_confidence = 0.75 if strict_analysis else 0.65
        
        suitable_for_fitting = (pose_score >= min_score and 
                              ai_confidence >= min_confidence)
        
        # Ïù¥Ïäà Î∞è Í∂åÏû•ÏÇ¨Ìï≠
        issues = []
        recommendations = []
        
        if ai_confidence < min_confidence:
            issues.append(f'AI Î™®Îç∏Ïùò Ïã†Î¢∞ÎèÑÍ∞Ä ÎÇÆÏäµÎãàÎã§ ({ai_confidence:.3f})')
            recommendations.append('Ï°∞Î™ÖÏù¥ Ï¢ãÏùÄ ÌôòÍ≤ΩÏóêÏÑú Îçî ÏÑ†Î™ÖÌïòÍ≤å Îã§Ïãú Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî')
        
        if torso_score < 0.5:
            issues.append(f'{clothing_type} Ï∞©Ïö©Ïóê Ï§ëÏöîÌïú ÏÉÅÏ≤¥Í∞Ä Î∂àÎ∂ÑÎ™ÖÌï©ÎãàÎã§')
            recommendations.append('ÏÉÅÏ≤¥ Ï†ÑÏ≤¥Í∞Ä Î≥¥Ïù¥ÎèÑÎ°ù Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî')
        
        return {
            'suitable_for_fitting': suitable_for_fitting,
            'issues': issues,
            'recommendations': recommendations,
            'pose_score': pose_score,
            'ai_confidence': ai_confidence,
            'detailed_scores': {
                'head': head_score,
                'torso': torso_score,
                'arms': arms_score,
                'legs': legs_score
            },
            'clothing_type': clothing_type,
            'weights_used': weights,
            'real_ai_analysis': True,
            'strict_analysis': strict_analysis
        }
        
    except Exception as e:
        logger.error(f"ÏùòÎ•òÎ≥Ñ Ìè¨Ï¶à Î∂ÑÏÑù Ïã§Ìå®: {e}")
        return {
            'suitable_for_fitting': False,
            'issues': ["Î∂ÑÏÑù Ïã§Ìå®"],
            'recommendations': ["Îã§Ïãú ÏãúÎèÑÌï¥ Ï£ºÏÑ∏Ïöî"],
            'pose_score': 0.0,
            'ai_confidence': 0.0,
            'real_ai_analysis': True
        }

# =================================================================
# üî• ÌÖåÏä§Ìä∏ Ìï®ÏàòÎì§ (BaseStepMixin v16.0 Ìò∏Ìôò + Ïã§Ï†ú AI Í∏∞Î∞ò)
# =================================================================

async def test_pose_estimation_step():
    """BaseStepMixin v16.0 Ìò∏Ìôò Ïã§Ï†ú AI Í∏∞Î∞ò Ìè¨Ï¶à Ï∂îÏ†ï ÌÖåÏä§Ìä∏"""
    try:
        print("üî• BaseStepMixin v16.0 Ìò∏Ìôò Ïã§Ï†ú AI Í∏∞Î∞ò Ìè¨Ï¶à Ï∂îÏ†ï ÏãúÏä§ÌÖú ÌÖåÏä§Ìä∏")
        print("=" * 80)
        
        # Ïã§Ï†ú AI Í∏∞Î∞ò Step ÏÉùÏÑ±
        step = await create_pose_estimation_step(
            device="auto",
            strict_mode=True,
            config={
                'confidence_threshold': 0.5,
                'visualization_enabled': True,
                'cache_enabled': True,
                'detailed_analysis': True,
                'real_ai_models': True,
                'basestep_version': '16.0-compatible'
            }
        )
        
        # ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄÎ°ú ÌÖåÏä§Ìä∏
        dummy_image = np.zeros((512, 512, 3), dtype=np.uint8)
        dummy_image_pil = Image.fromarray(dummy_image)
        
        print(f"üìã BaseStepMixin v16.0 Ìò∏Ìôò Ïã§Ï†ú AI Step Ï†ïÎ≥¥:")
        step_status = step.get_status()
        print(f"   üéØ Step: {step_status['step_name']}")
        print(f"   üî¢ Î≤ÑÏ†Ñ: {step_status['version']}")
        print(f"   ü§ñ Î°úÎî©Îêú AI Î™®Îç∏Îì§: {step_status.get('loaded_models', [])}")
        print(f"   üîí Strict Mode: {step_status.get('strict_mode', False)}")
        print(f"   üíâ ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ: {step_status.get('dependencies', {})}")
        print(f"   üíé Ï¥àÍ∏∞Ìôî ÏÉÅÌÉú: {step_status.get('is_initialized', False)}")
        print(f"   üß† Ïã§Ï†ú AI Î™®Îç∏ Î°úÎìú: {step_status.get('has_model', False)}")
        print(f"   ü§ñ Ïã§Ï†ú AI Í∏∞Î∞ò: {step_status.get('real_ai_models', False)}")
        print(f"   üì¶ AI ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÇ¨Ïö© Í∞ÄÎä•: {step_status.get('ai_libraries_available', {})}")
        print(f"   üìÅ AI Î™®Îç∏ Í≤ΩÎ°ú: {step_status.get('model_paths', {})}")
        
        # Ïã§Ï†ú AI Î™®Îç∏Î°ú Ï≤òÎ¶¨
        result = await step.process(dummy_image_pil, clothing_type="shirt")
        
        if result['success']:
            print(f"‚úÖ BaseStepMixin v16.0 Ìò∏Ìôò Ïã§Ï†ú AI Ìè¨Ï¶à Ï∂îÏ†ï ÏÑ±Í≥µ")
            print(f"üéØ Í≤ÄÏ∂úÎêú ÌÇ§Ìè¨Ïù∏Ìä∏ Ïàò: {len(result['keypoints'])}")
            print(f"üéñÔ∏è AI Ïã†Î¢∞ÎèÑ: {result['pose_analysis']['ai_confidence']:.3f}")
            print(f"üíé ÌíàÏßà Ï†êÏàò: {result['pose_analysis']['quality_score']:.3f}")
            print(f"üëï ÏùòÎ•ò Ï†ÅÌï©ÏÑ±: {result['pose_analysis']['suitable_for_fitting']}")
            print(f"ü§ñ ÏÇ¨Ïö©Îêú AI Î™®Îç∏Îì§: {result['models_used']}")
            print(f"üèÜ Ï£º AI Î™®Îç∏: {result['primary_model']}")
            print(f"‚ö° Ï∂îÎ°† ÏãúÍ∞Ñ: {result.get('inference_time', 0):.3f}Ï¥à")
            print(f"üé® Diffusion Ìñ•ÏÉÅ: {result.get('enhanced_by_diffusion', False)}")
            print(f"üîó BaseStepMixin Î≤ÑÏ†Ñ: {result['step_info']['basestep_version']}")
            print(f"ü§ñ Ïã§Ï†ú AI Í∏∞Î∞ò: {result['step_info']['real_ai_models']}")
            print(f"üìä ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏÉÅÌÉú: {result['step_info']['dependency_injection_status']}")
        else:
            print(f"‚ùå BaseStepMixin v16.0 Ìò∏Ìôò Ïã§Ï†ú AI Ìè¨Ï¶à Ï∂îÏ†ï Ïã§Ìå®: {result.get('error', 'Unknown Error')}")
        
        # Ï†ïÎ¶¨
        cleanup_result = await step.cleanup()
        print(f"üßπ BaseStepMixin v16.0 Ìò∏Ìôò Ïã§Ï†ú AI Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨: {cleanup_result['success']}")
        print(f"üßπ Ï†ïÎ¶¨Îêú AI Î™®Îç∏ Ïàò: {cleanup_result.get('cleaned_models', 0)}")
        
    except Exception as e:
        print(f"‚ùå BaseStepMixin v16.0 Ìò∏Ìôò Ïã§Ï†ú AI ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")

async def test_dependency_injection():
    """BaseStepMixin v16.0 Ïã§Ï†ú AI Í∏∞Î∞ò ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÌÖåÏä§Ìä∏"""
    try:
        print("ü§ñ BaseStepMixin v16.0 Ïã§Ï†ú AI Í∏∞Î∞ò ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÌÜµÌï© ÌÖåÏä§Ìä∏")
        print("=" * 80)
        
        # ÎèôÏ†Å import Ìï®ÏàòÎì§ ÌÖåÏä§Ìä∏
        base_step_class = get_base_step_mixin_class()
        model_loader = get_model_loader()
        memory_manager = get_memory_manager()
        step_factory = get_step_factory()
        
        print(f"‚úÖ BaseStepMixin v16.0 ÎèôÏ†Å import: {base_step_class is not None}")
        print(f"‚úÖ ModelLoader ÎèôÏ†Å import: {model_loader is not None}")
        print(f"‚úÖ MemoryManager ÎèôÏ†Å import: {memory_manager is not None}")
        print(f"‚úÖ StepFactory ÎèôÏ†Å import: {step_factory is not None}")
        
        # Ïã§Ï†ú AI Í∏∞Î∞ò Step ÏÉùÏÑ± Î∞è ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÌôïÏù∏
        step = PoseEstimationStep(device="auto", strict_mode=True)
        
        print(f"üîó ÏùòÏ°¥ÏÑ± ÏÉÅÌÉú: {step.dependencies_injected}")
        print(f"ü§ñ Ïã§Ï†ú AI Î™®Îç∏ Í≤ΩÎ°ú: {step.model_paths}")
        
        # ÏàòÎèô ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÌÖåÏä§Ìä∏
        if model_loader:
            step.set_model_loader(model_loader)
            print("‚úÖ ModelLoader ÏàòÎèô Ï£ºÏûÖ ÏôÑÎ£å")
        
        if memory_manager:
            step.set_memory_manager(memory_manager)
            print("‚úÖ MemoryManager ÏàòÎèô Ï£ºÏûÖ ÏôÑÎ£å")
        
        # Ïã§Ï†ú AI Í∏∞Î∞ò Ï¥àÍ∏∞Ìôî ÌÖåÏä§Ìä∏
        init_result = await step.initialize()
        print(f"üöÄ Ïã§Ï†ú AI Í∏∞Î∞ò Ï¥àÍ∏∞Ìôî ÏÑ±Í≥µ: {init_result}")
        
        if init_result:
            final_status = step.get_status()
            print(f"üéØ ÏµúÏ¢Ö ÏÉÅÌÉú: {final_status['version']}")
            print(f"üì¶ ÏùòÏ°¥ÏÑ± ÏôÑÎ£å: {final_status['dependencies']}")
            print(f"ü§ñ Ïã§Ï†ú AI Í∏∞Î∞ò: {final_status['real_ai_models']}")
            print(f"üß† Î°úÎî©Îêú AI Î™®Îç∏Îì§: {final_status['loaded_models']}")
            print(f"üìÅ AI Î™®Îç∏ Í≤ΩÎ°úÎì§: {final_status['model_paths']}")
        
        # Ï†ïÎ¶¨
        await step.cleanup()
        
    except Exception as e:
        print(f"‚ùå BaseStepMixin v16.0 Ïã§Ï†ú AI Í∏∞Î∞ò ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")

def test_real_ai_models():
    """Ïã§Ï†ú AI Î™®Îç∏ ÌÅ¥ÎûòÏä§ ÌÖåÏä§Ìä∏"""
    try:
        print("üß† Ïã§Ï†ú AI Î™®Îç∏ ÌÅ¥ÎûòÏä§ ÌÖåÏä§Ìä∏")
        print("=" * 60)
        
        # SmartModelPathMapper ÌÖåÏä§Ìä∏
        try:
            mapper = Step02ModelMapper()
            model_paths = mapper.get_step02_model_paths()
            print(f"‚úÖ SmartModelPathMapper ÎèôÏûë: {len(model_paths)}Í∞ú Í≤ΩÎ°ú")
            for model_name, path in model_paths.items():
                status = "Ï°¥Ïû¨" if path and path.exists() else "ÏóÜÏùå"
                print(f"   {model_name}: {status} ({path})")
        except Exception as e:
            print(f"‚ùå SmartModelPathMapper ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        
        # ÎçîÎØ∏ Î™®Îç∏ ÌååÏùºÎ°ú AI Î™®Îç∏ ÌÅ¥ÎûòÏä§ ÌÖåÏä§Ìä∏
        dummy_model_path = Path("dummy_model.pt")
        
        # RealYOLOv8PoseModel ÌÖåÏä§Ìä∏
        try:
            yolo_model = RealYOLOv8PoseModel(dummy_model_path, "cpu")
            print(f"‚úÖ RealYOLOv8PoseModel ÏÉùÏÑ± ÏÑ±Í≥µ: {yolo_model}")
            
            # ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄÎ°ú ÌÖåÏä§Ìä∏ (Î™®Îç∏ Î°úÎî© ÏóÜÏù¥)
            dummy_image = Image.new('RGB', (256, 256), (128, 128, 128))
            if not yolo_model.loaded:
                print("‚ö†Ô∏è YOLOv8 Î™®Îç∏ ÎØ∏Î°úÎî© ÏÉÅÌÉú (ÏòàÏÉÅÎê®)")
        except Exception as e:
            print(f"‚ùå RealYOLOv8PoseModel ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        
        # RealOpenPoseModel ÌÖåÏä§Ìä∏
        try:
            openpose_model = RealOpenPoseModel(dummy_model_path, "cpu")
            print(f"‚úÖ RealOpenPoseModel ÏÉùÏÑ± ÏÑ±Í≥µ: {openpose_model}")
        except Exception as e:
            print(f"‚ùå RealOpenPoseModel ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        
        # RealDiffusionPoseModel ÌÖåÏä§Ìä∏
        try:
            diffusion_model = RealDiffusionPoseModel(dummy_model_path, "cpu")
            print(f"‚úÖ RealDiffusionPoseModel ÏÉùÏÑ± ÏÑ±Í≥µ: {diffusion_model}")
        except Exception as e:
            print(f"‚ùå RealDiffusionPoseModel ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        
        # RealBodyPoseModel ÌÖåÏä§Ìä∏
        try:
            body_pose_model = RealBodyPoseModel(dummy_model_path, "cpu")
            print(f"‚úÖ RealBodyPoseModel ÏÉùÏÑ± ÏÑ±Í≥µ: {body_pose_model}")
        except Exception as e:
            print(f"‚ùå RealBodyPoseModel ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
        
    except Exception as e:
        print(f"‚ùå Ïã§Ï†ú AI Î™®Îç∏ ÌÅ¥ÎûòÏä§ ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")

def test_utilities():
    """Ïú†Ìã∏Î¶¨Ìã∞ Ìï®Ïàò ÌÖåÏä§Ìä∏"""
    try:
        print("üîÑ Ïú†Ìã∏Î¶¨Ìã∞ Í∏∞Îä• ÌÖåÏä§Ìä∏")
        print("=" * 60)
        
        # ÎçîÎØ∏ OpenPose 18 ÌÇ§Ìè¨Ïù∏Ìä∏
        openpose_keypoints = [
            [100, 50, 0.9],   # nose
            [100, 80, 0.8],   # neck
            [80, 100, 0.7],   # right_shoulder
            [70, 130, 0.6],   # right_elbow
            [60, 160, 0.5],   # right_wrist
            [120, 100, 0.7],  # left_shoulder
            [130, 130, 0.6],  # left_elbow
            [140, 160, 0.5],  # left_wrist
            [100, 200, 0.8],  # middle_hip
            [90, 200, 0.7],   # right_hip
            [85, 250, 0.6],   # right_knee
            [80, 300, 0.5],   # right_ankle
            [110, 200, 0.7],  # left_hip
            [115, 250, 0.6],  # left_knee
            [120, 300, 0.5],  # left_ankle
            [95, 40, 0.8],    # right_eye
            [105, 40, 0.8],   # left_eye
            [90, 45, 0.7],    # right_ear
            [110, 45, 0.7]    # left_ear
        ]
        
        # Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù
        is_valid = validate_keypoints(openpose_keypoints)
        print(f"‚úÖ OpenPose 18 Ïú†Ìö®ÏÑ±: {is_valid}")
        
        # COCO 17Î°ú Î≥ÄÌôò
        coco_keypoints = convert_keypoints_to_coco(openpose_keypoints)
        print(f"üîÑ COCO 17 Î≥ÄÌôò: {len(coco_keypoints)}Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏")
        
        # ÏùòÎ•òÎ≥Ñ Î∂ÑÏÑù
        analysis = analyze_pose_for_clothing(
            openpose_keypoints, 
            clothing_type="shirt",
            strict_analysis=True
        )
        print(f"üëï ÏùòÎ•ò Ï†ÅÌï©ÏÑ± Î∂ÑÏÑù:")
        print(f"   Ï†ÅÌï©ÏÑ±: {analysis['suitable_for_fitting']}")
        print(f"   Ï†êÏàò: {analysis['pose_score']:.3f}")
        print(f"   AI Ïã†Î¢∞ÎèÑ: {analysis['ai_confidence']:.3f}")
        print(f"   Ïã§Ï†ú AI Î∂ÑÏÑù: {analysis['real_ai_analysis']}")
        
        # Ïù¥ÎØ∏ÏßÄÏóê Ìè¨Ï¶à Í∑∏Î¶¨Í∏∞ ÌÖåÏä§Ìä∏
        dummy_image = Image.new('RGB', (256, 256), (128, 128, 128))
        pose_image = draw_pose_on_image(dummy_image, openpose_keypoints)
        print(f"üñºÔ∏è Ìè¨Ï¶à Í∑∏Î¶¨Í∏∞: {pose_image.size}")
        
    except Exception as e:
        print(f"‚ùå Ïú†Ìã∏Î¶¨Ìã∞ ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")

# =================================================================
# üî• Î™®Îìà ÏùµÏä§Ìè¨Ìä∏ (BaseStepMixin v16.0 Ìò∏Ìôò + Ïã§Ï†ú AI Í∏∞Î∞ò)
# =================================================================

__all__ = [
    # Î©îÏù∏ ÌÅ¥ÎûòÏä§Îì§ (Ïã§Ï†ú AI Í∏∞Î∞ò + ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÄÏõê)
    'PoseEstimationStep',
    'RealYOLOv8PoseModel',
    'RealOpenPoseModel',
    'RealHRNetModel',  # ÏÉàÎ°ú Ï∂îÍ∞Ä
    'RealDiffusionPoseModel',
    'RealBodyPoseModel',
    'SmartModelPathMapper',
    'Step02ModelMapper',
    'PoseMetrics',
    'PoseModel',
    'PoseQuality',
    
    # ÌååÏù¥ÌîÑÎùºÏù∏ Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞
    'PipelineStepResult',
    'PipelineInputData',
    
    # ÏÉùÏÑ± Ìï®ÏàòÎì§ (BaseStepMixin v16.0 Ìò∏Ìôò + Ïã§Ï†ú AI Í∏∞Î∞ò)
    'create_pose_estimation_step',
    'create_pose_estimation_step_sync',
    
    # ÎèôÏ†Å import Ìï®ÏàòÎì§
    'get_base_step_mixin_class',
    'get_model_loader',
    'get_memory_manager',
    'get_step_factory',
    
    # Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§ (Ïã§Ï†ú AI Í∏∞Î∞ò)
    'validate_keypoints',
    'convert_keypoints_to_coco',
    'draw_pose_on_image',
    'analyze_pose_for_clothing',
    
    # ÏÉÅÏàòÎì§
    'OPENPOSE_18_KEYPOINTS',
    'KEYPOINT_COLORS',
    'SKELETON_CONNECTIONS',
    
    # ÌÖåÏä§Ìä∏ Ìï®ÏàòÎì§ (BaseStepMixin v16.0 Ìò∏Ìôò + Ïã§Ï†ú AI Í∏∞Î∞ò)
    'test_pose_estimation_step',
    'test_dependency_injection',
    'test_real_ai_models',
    'test_utilities'
]

# =================================================================
# üî• Î™®Îìà Ï¥àÍ∏∞Ìôî Î°úÍ∑∏ (BaseStepMixin v16.0 Ìò∏Ìôò + Ïã§Ï†ú AI Í∏∞Î∞ò)
# =================================================================

logger.info("üî• BaseStepMixin v16.0 + StepInterface Ìò∏Ìôò Ïã§Ï†ú AI Í∏∞Î∞ò PoseEstimationStep v5.0 Î°úÎìú ÏôÑÎ£å")
logger.info("‚úÖ BaseStepMixin v16.0 + StepInterface Îã§Ï§ë ÏÉÅÏÜç ÏôÑÏ†Ñ Ìò∏Ìôò")
logger.info("‚úÖ Ïù¥Ï§ë Í∏∞Îä• ÏßÄÏõê: Í∞úÎ≥Ñ Ïã§Ìñâ + ÌååÏù¥ÌîÑÎùºÏù∏ Ïó∞Í≤∞")
logger.info("‚úÖ TYPE_CHECKING Ìå®ÌÑ¥ÏúºÎ°ú ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Î∞©ÏßÄ")
logger.info("‚úÖ StepFactory ‚Üí ModelLoader ‚Üí BaseStepMixin ‚Üí ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÏÑ±")
logger.info("ü§ñ Ïã§Ï†ú AI Î™®Îç∏ ÌååÏùº ÌôúÏö© (3.4GB): OpenPose, YOLOv8, HRNet, Diffusion, Body Pose")
logger.info("üîó SmartModelPathMapper ÌôúÏö©Ìïú ÎèôÏ†Å ÌååÏùº Í≤ΩÎ°ú ÌÉêÏßÄ")
logger.info("üß† Ïã§Ï†ú AI Ï∂îÎ°† ÏóîÏßÑ Íµ¨ÌòÑ (YOLOv8, OpenPose, HRNet, Diffusion)")
logger.info("üéØ 18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏ ÏôÑÏ†Ñ Í≤ÄÏ∂ú Î∞è Ïä§ÏºàÎ†àÌÜ§ Íµ¨Ï°∞ ÏÉùÏÑ±")
logger.info("üîó ÌååÏù¥ÌîÑÎùºÏù∏ Ïó∞Í≤∞: Step 01 ‚Üí Step 02 ‚Üí Step 03, 04, 05, 06")
logger.info("üìä PipelineStepResult Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ ÏôÑÏ†Ñ ÏßÄÏõê")
logger.info("üçé M3 Max MPS Í∞ÄÏÜç ÏµúÏ†ÅÌôî")
logger.info("üêç conda ÌôòÍ≤Ω Ïö∞ÏÑ† ÏßÄÏõê")
logger.info("‚ö° Ïã§Ï†ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ‚Üí AI Î™®Îç∏ ÌÅ¥ÎûòÏä§ ‚Üí Ïã§Ï†ú Ï∂îÎ°†")
logger.info("üé® Diffusion Í∏∞Î∞ò Ìè¨Ï¶à ÌíàÏßà Ìñ•ÏÉÅ")
logger.info("üìä ÏôÑÏ†ÑÌïú Ìè¨Ï¶à Î∂ÑÏÑù - Í∞ÅÎèÑ, ÎπÑÏú®, ÎåÄÏπ≠ÏÑ±, Í∞ÄÏãúÏÑ±, ÌíàÏßà ÌèâÍ∞Ä")
logger.info("üîí Strict Mode ÏßÄÏõê - Ïã§Ìå® Ïãú Ï¶âÏãú ÏóêÎü¨")
logger.info("üöÄ ÌîÑÎ°úÎçïÏÖò Î†àÎ≤® ÏïàÏ†ïÏÑ± + Ïã§Ï†ú AI Î™®Îç∏ Í∏∞Î∞ò + ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÄÏõê")# ÏãúÏä§ÌÖú ÏÉÅÌÉú Î°úÍπÖ
logger.info(f"üìä ÏãúÏä§ÌÖú ÏÉÅÌÉú: PyTorch={TORCH_AVAILABLE}, M3 Max={IS_M3_MAX}, Device={DEVICE}")
logger.info(f"ü§ñ AI ÎùºÏù¥Î∏åÎü¨Î¶¨: Ultralytics={ULTRALYTICS_AVAILABLE}, MediaPipe={MEDIAPIPE_AVAILABLE}, Transformers={TRANSFORMERS_AVAILABLE}")
logger.info(f"üîß ÎùºÏù¥Î∏åÎü¨Î¶¨ Î≤ÑÏ†Ñ: PyTorch={TORCH_VERSION}")
logger.info(f"üíæ Safetensors: {'ÌôúÏÑ±Ìôî' if SAFETENSORS_AVAILABLE else 'ÎπÑÌôúÏÑ±Ìôî'}")
logger.info(f"üîó BaseStepMixin v16.0 + StepInterface Ìò∏Ìôò: ÏôÑÏ†ÑÌïú ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ + ÌååÏù¥ÌîÑÎùºÏù∏ Ìå®ÌÑ¥")
logger.info(f"ü§ñ Ïã§Ï†ú AI Í∏∞Î∞ò Ïó∞ÏÇ∞: Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ‚Üí Î™®Îç∏ ÌÅ¥ÎûòÏä§ ‚Üí Ï∂îÎ°† ÏóîÏßÑ")
logger.info(f"üéØ Ïã§Ï†ú AI Î™®Îç∏ ÌååÏùºÎì§: YOLOv8 6.5MB, OpenPose 97.8MB, HRNet (Í≥†Ï†ïÎ∞Ä), Diffusion 1378MB, Body Pose 97.8MB")
logger.info(f"üîó ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÄÏõê: Í∞úÎ≥Ñ Ïã§Ìñâ(process) + ÌååÏù¥ÌîÑÎùºÏù∏ Ïó∞Í≤∞(process_pipeline)")

# =================================================================
# üî• Î©îÏù∏ Ïã§ÌñâÎ∂Ä (BaseStepMixin v16.0 + StepInterface Ìò∏Ìôò + Ïã§Ï†ú AI Í∏∞Î∞ò Í≤ÄÏ¶ù)
# =================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("üéØ MyCloset AI Step 02 - BaseStepMixin v16.0 + StepInterface Ìò∏Ìôò + Ïã§Ï†ú AI Î™®Îç∏ Í∏∞Î∞ò")
    print("=" * 80)
    
    # ÎπÑÎèôÍ∏∞ ÌÖåÏä§Ìä∏ Ïã§Ìñâ
    async def run_all_tests():
        await test_pose_estimation_step()
        print("\n" + "=" * 80)
        await test_dependency_injection()
        print("\n" + "=" * 80)
        test_real_ai_models()
        print("\n" + "=" * 80)
        test_utilities()
        print("\n" + "=" * 80)
        await test_pipeline_functionality()
    
    async def test_pipeline_functionality():
        """ÌååÏù¥ÌîÑÎùºÏù∏ Í∏∞Îä• ÌÖåÏä§Ìä∏"""
        try:
            print("üîó ÌååÏù¥ÌîÑÎùºÏù∏ Ïó∞Í≤∞ Í∏∞Îä• ÌÖåÏä§Ìä∏")
            print("=" * 60)
            
            # ÌååÏù¥ÌîÑÎùºÏù∏Ïö© Step ÏÉùÏÑ±
            step = await create_pose_estimation_step(
                device="auto",
                config={
                    'pipeline_mode': True,
                    'confidence_threshold': 0.5,
                    'real_ai_models': True
                }
            )
            
            # ÎçîÎØ∏ ÌååÏù¥ÌîÑÎùºÏù∏ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
            dummy_step01_result = PipelineStepResult(
                step_id=1,
                step_name="human_parsing",
                success=True,
                for_step_02={
                    "parsed_image": Image.new('RGB', (512, 512), (128, 128, 128)),
                    "body_masks": {"person": "dummy_mask"},
                    "human_region": {"bbox": [50, 50, 450, 450]}
                },
                for_step_03={
                    "person_parsing": "dummy_parsing",
                    "clothing_areas": "dummy_areas"
                },
                original_data={
                    "person_image": "original_image"
                }
            )
            
            print(f"üìã ÌååÏù¥ÌîÑÎùºÏù∏ Step Ï†ïÎ≥¥:")
            step_status = step.get_status()
            print(f"   üéØ Step: {step_status['step_name']}")
            print(f"   üîó ÌååÏù¥ÌîÑÎùºÏù∏ Î™®Îìú: {getattr(step, 'pipeline_mode', False)}")
            print(f"   ü§ñ Î°úÎî©Îêú AI Î™®Îç∏Îì§: {step_status.get('loaded_models', [])}")
            
            # ÌååÏù¥ÌîÑÎùºÏù∏ Ï≤òÎ¶¨ ÌÖåÏä§Ìä∏
            pipeline_result = await step.process_pipeline(dummy_step01_result)
            
            if pipeline_result.success:
                print(f"‚úÖ ÌååÏù¥ÌîÑÎùºÏù∏ Ï≤òÎ¶¨ ÏÑ±Í≥µ")
                print(f"üéØ Step 03 Ï†ÑÎã¨ Îç∞Ïù¥ÌÑ∞: {len(pipeline_result.for_step_03)}Í∞ú Ìï≠Î™©")
                print(f"üéØ Step 04 Ï†ÑÎã¨ Îç∞Ïù¥ÌÑ∞: {len(pipeline_result.for_step_04)}Í∞ú Ìï≠Î™©")
                print(f"üéØ Step 05 Ï†ÑÎã¨ Îç∞Ïù¥ÌÑ∞: {len(pipeline_result.for_step_05)}Í∞ú Ìï≠Î™©")
                print(f"üéØ Step 06 Ï†ÑÎã¨ Îç∞Ïù¥ÌÑ∞: {len(pipeline_result.for_step_06)}Í∞ú Ìï≠Î™©")
                print(f"‚ö° ÌååÏù¥ÌîÑÎùºÏù∏ Ï≤òÎ¶¨ ÏãúÍ∞Ñ: {pipeline_result.processing_time:.3f}Ï¥à")
                print(f"üìä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {pipeline_result.metadata.get('pipeline_progress', 'unknown')}")
            else:
                print(f"‚ùå ÌååÏù¥ÌîÑÎùºÏù∏ Ï≤òÎ¶¨ Ïã§Ìå®: {pipeline_result.error}")
            
            # Í∞úÎ≥Ñ Ï≤òÎ¶¨ÎèÑ ÌÖåÏä§Ìä∏
            individual_result = await step.process(
                Image.new('RGB', (512, 512), (128, 128, 128)),
                clothing_type="shirt"
            )
            
            if individual_result['success']:
                print(f"‚úÖ Í∞úÎ≥Ñ Ï≤òÎ¶¨ÎèÑ Ï†ïÏÉÅ ÏûëÎèô")
                print(f"üéØ ÌÇ§Ìè¨Ïù∏Ìä∏ Ïàò: {len(individual_result['keypoints'])}")
            else:
                print(f"‚ùå Í∞úÎ≥Ñ Ï≤òÎ¶¨ Ïã§Ìå®: {individual_result.get('error', 'Unknown')}")
            
            await step.cleanup()
            
        except Exception as e:
            print(f"‚ùå ÌååÏù¥ÌîÑÎùºÏù∏ Í∏∞Îä• ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
    
    try:
        asyncio.run(run_all_tests())
    except Exception as e:
        print(f"‚ùå BaseStepMixin v16.0 + StepInterface Ìò∏Ìôò Ïã§Ï†ú AI Í∏∞Î∞ò ÌÖåÏä§Ìä∏ Ïã§Ìñâ Ïã§Ìå®: {e}")
    
    print("\n" + "=" * 80)
    print("‚ú® BaseStepMixin v16.0 + StepInterface Ìò∏Ìôò + Ïã§Ï†ú AI Í∏∞Î∞ò Ìè¨Ï¶à Ï∂îÏ†ï ÏãúÏä§ÌÖú ÌÖåÏä§Ìä∏ ÏôÑÎ£å")
    print("üîó BaseStepMixin v16.0 + StepInterface Îã§Ï§ë ÏÉÅÏÜç ÏôÑÏ†Ñ Ìò∏Ìôò")
    print("üîó Ïù¥Ï§ë Í∏∞Îä• ÏßÄÏõê: Í∞úÎ≥Ñ Ïã§Ìñâ(process) + ÌååÏù¥ÌîÑÎùºÏù∏ Ïó∞Í≤∞(process_pipeline)")
    print("ü§ñ TYPE_CHECKINGÏúºÎ°ú ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Î∞©ÏßÄ")
    print("üîó StepFactory ‚Üí ModelLoader ‚Üí BaseStepMixin ‚Üí ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÏÑ±")
    print("üß† Ïã§Ï†ú AI Î™®Îç∏ ÌååÏùº ÌôúÏö© (3.4GB): OpenPose, YOLOv8, HRNet, Diffusion")
    print("‚ö° Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© ‚Üí AI Î™®Îç∏ ÌÅ¥ÎûòÏä§ ‚Üí Ïã§Ï†ú Ï∂îÎ°†")
    print("üéØ 18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏ ÏôÑÏ†Ñ Í≤ÄÏ∂ú + Ïä§ÏºàÎ†àÌÜ§ Íµ¨Ï°∞ ÏÉùÏÑ±")
    print("üîó ÌååÏù¥ÌîÑÎùºÏù∏ Îç∞Ïù¥ÌÑ∞ Ï†ÑÎã¨: Step 01 ‚Üí Step 02 ‚Üí Step 03,04,05,06")
    print("üíâ ÏôÑÎ≤ΩÌïú ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ìå®ÌÑ¥")
    print("üîí Strict Mode + ÏôÑÏ†ÑÌïú Ïã§Ï†ú AI Í∏∞Î∞ò Î∂ÑÏÑù Í∏∞Îä•")
    print("üéØ Ïã§Ï†ú AI Ïó∞ÏÇ∞ + ÏßÑÏßú ÌÇ§Ìè¨Ïù∏Ìä∏ Í≤ÄÏ∂ú + ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÄÏõê")
    print("=" * 80)