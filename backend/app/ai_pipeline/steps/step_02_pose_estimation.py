#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 02: Pose Estimation - Central Hub DI Container v7.0 ì™„ì „ ë¦¬íŒ©í† ë§ 
================================================================================

âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
âœ… BaseStepMixin ìƒì† íŒ¨í„´ (Human Parsing Stepê³¼ ë™ì¼)
âœ… MediaPipe Pose ëª¨ë¸ ì§€ì› (ìš°ì„ ìˆœìœ„ 1)
âœ… OpenPose ëª¨ë¸ ì§€ì› (í´ë°± ì˜µì…˜)
âœ… YOLOv8-Pose ëª¨ë¸ ì§€ì› (ì‹¤ì‹œê°„)
âœ… HRNet ëª¨ë¸ ì§€ì› (ê³ ì •ë°€)
âœ… 17ê°œ COCO keypoints ê°ì§€
âœ… confidence score ê³„ì‚°
âœ… Mock ëª¨ë¸ ì™„ì „ ì œê±°
âœ… ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
âœ… ë‹¤ì¤‘ ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ

íŒŒì¼ ìœ„ì¹˜: backend/app/ai_pipeline/steps/step_02_pose_estimation.py
ì‘ì„±ì: MyCloset AI Team  
ë‚ ì§œ: 2025-08-01
ë²„ì „: v7.0 (Central Hub DI Container ì™„ì „ ë¦¬íŒ©í† ë§)
"""

# ==============================================
# ğŸ”¥ 1. Import ì„¹ì…˜ (Central Hub íŒ¨í„´)
# ==============================================

import os
import sys
import gc
import time
import asyncio
import logging
import threading
import traceback
import hashlib
import json
import base64
import math
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache, wraps
from contextlib import asynccontextmanager

# TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
if TYPE_CHECKING:
    # from app.ai_pipeline.utils.model_loader import ModelLoader  # ìˆœí™˜ì°¸ì¡°ë¡œ ì§€ì—° import
    from ..factories.step_factory import StepFactory
    from ..steps.base_step_mixin import BaseStepMixin

logger = logging.getLogger(__name__)

def _get_central_hub_container():
    """Central Hub DI Container ì•ˆì „í•œ ë™ì  í•´ê²°"""
    try:
        import importlib
        module = importlib.import_module('app.core.di_container')
        get_global_fn = getattr(module, 'get_global_container', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError:
        return None
    except Exception:
        return None

def _inject_dependencies_safe(step_instance):
    """Central Hub DI Containerë¥¼ í†µí•œ ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì…"""
    try:
        container = _get_central_hub_container()
        if container and hasattr(container, 'inject_to_step'):
            return container.inject_to_step(step_instance)
        return 0
    except Exception:
        return 0

def _get_service_from_central_hub(service_key: str):
    """Central Hubë¥¼ í†µí•œ ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ"""
    try:
        container = _get_central_hub_container()
        if container:
            return container.get(service_key)
        return None
    except Exception:
        return None

logger = logging.getLogger(__name__)

# BaseStepMixin ë™ì  import (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€)
def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError:
        logger.error("âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨")
        return None

BaseStepMixin = get_base_step_mixin_class()

# BaseStepMixin í´ë°± í´ë˜ìŠ¤ (step_02_pose_estimation.pyìš©)
if BaseStepMixin is None:
    import asyncio
    from typing import Dict, Any, Optional, List
    
    class BaseStepMixin:
        """PoseEstimationStepìš© BaseStepMixin í´ë°± í´ë˜ìŠ¤"""
        
        def __init__(self, **kwargs):
            # ê¸°ë³¸ ì†ì„±ë“¤
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'PoseEstimationStep')
            self.step_id = kwargs.get('step_id', 2)
            self.device = kwargs.get('device', 'cpu')
            
            # AI ëª¨ë¸ ê´€ë ¨ ì†ì„±ë“¤ (PoseEstimationStepì´ í•„ìš”ë¡œ í•˜ëŠ”)
            self.ai_models = {}
            self.models_loading_status = {
                'mediapipe': False,
                'openpose': False,
                'yolov8': False,
                'hrnet': False,
                'total_loaded': 0,
                'loading_errors': []
            }
            self.model_interface = None
            self.loaded_models = {}
            
            # Pose Estimation íŠ¹í™” ì†ì„±ë“¤
            self.pose_models = {}
            self.pose_ready = False
            self.keypoints_cache = {}
            
            # ìƒíƒœ ê´€ë ¨ ì†ì„±ë“¤
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # Central Hub DI Container ê´€ë ¨
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            # ì„±ëŠ¥ í†µê³„
            self.performance_stats = {
                'total_processed': 0,
                'avg_processing_time': 0.0,
                'error_count': 0,
                'success_rate': 1.0
            }
            
            # Pose Estimation ì„¤ì •
            self.confidence_threshold = 0.5
            self.use_subpixel = True
            
            # ëª¨ë¸ ìš°ì„ ìˆœìœ„ (MediaPipe ìš°ì„ )
            self.model_priority = [
                'mediapipe',
                'yolov8_pose', 
                'openpose',
                'hrnet'
            ]
            
            self.logger.info(f"âœ… {self.step_name} BaseStepMixin í´ë°± í´ë˜ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        async def process(self, **kwargs) -> Dict[str, Any]:
            """ê¸°ë³¸ process ë©”ì„œë“œ - _run_ai_inference í˜¸ì¶œ"""
            try:
                start_time = time.time()
                
                # _run_ai_inference ë©”ì„œë“œê°€ ìˆìœ¼ë©´ í˜¸ì¶œ
                if hasattr(self, '_run_ai_inference'):
                    result = self._run_ai_inference(kwargs)
                    
                    # ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
                    if isinstance(result, dict):
                        result['processing_time'] = time.time() - start_time
                        result['step_name'] = self.step_name
                        result['step_id'] = self.step_id
                    
                    return result
                    else:
                    # ê¸°ë³¸ ì‘ë‹µ
                    return {
                        'success': False,
                        'error': '_run_ai_inference ë©”ì„œë“œê°€ êµ¬í˜„ë˜ì§€ ì•ŠìŒ',
                        'processing_time': time.time() - start_time,
                        'step_name': self.step_name,
                        'step_id': self.step_id
                    }
                    
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} process ì‹¤íŒ¨: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                    'step_name': self.step_name,
                    'step_id': self.step_id
                }
        
        async def initialize(self) -> bool:
            """ì´ˆê¸°í™” ë©”ì„œë“œ"""
            try:
                if self.is_initialized:
                    return True
                
                self.logger.info(f"ğŸ”„ {self.step_name} ì´ˆê¸°í™” ì‹œì‘...")
                
                # í¬ì¦ˆ ëª¨ë¸ë“¤ ë¡œë”© (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” _load_pose_models_via_central_hub í˜¸ì¶œ)
                if hasattr(self, '_load_pose_models_via_central_hub'):
                    loaded_count = self._load_pose_models_via_central_hub()
                    if loaded_count == 0:
                        self.logger.error("âŒ í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì´ˆê¸°í™” ì‹¤íŒ¨")
                        return False
                
                self.is_initialized = True
                self.is_ready = True
                self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
                return True
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                return False
        
        def cleanup(self):
            """ì •ë¦¬ ë©”ì„œë“œ"""
            try:
                self.logger.info(f"ğŸ”„ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
                
                # AI ëª¨ë¸ë“¤ ì •ë¦¬
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'cleanup'):
                            model.cleanup()
                        del model
                    except Exception as e:
                        self.logger.debug(f"ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨ ({model_name}): {e}")
                
                # ìºì‹œ ì •ë¦¬
                self.ai_models.clear()
                if hasattr(self, 'pose_models'):
                    self.pose_models.clear()
                if hasattr(self, 'keypoints_cache'):
                    self.keypoints_cache.clear()
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        torch.mps.empty_cache()
                    except:
                    pass
                
                import gc
                gc.collect()
                
                self.logger.info(f"âœ… {self.step_name} ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        def get_status(self) -> Dict[str, Any]:
            """ìƒíƒœ ì¡°íšŒ"""
            return {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'device': self.device,
                'pose_ready': getattr(self, 'pose_ready', False),
                'models_loaded': len(getattr(self, 'loaded_models', {})),
                'model_priority': getattr(self, 'model_priority', []),
                'confidence_threshold': getattr(self, 'confidence_threshold', 0.5),
                'use_subpixel': getattr(self, 'use_subpixel', True),
                'fallback_mode': True
            }
        
        def get_model_status(self) -> Dict[str, Any]:
            """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ (PoseEstimationStep í˜¸í™˜)"""
            return {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'pose_ready': getattr(self, 'pose_ready', False),
                'models_loading_status': getattr(self, 'models_loading_status', {}),
                'loaded_models': list(getattr(self, 'ai_models', {}).keys()),
                'model_priority': getattr(self, 'model_priority', []),
                'confidence_threshold': getattr(self, 'confidence_threshold', 0.5),
                'use_subpixel': getattr(self, 'use_subpixel', True)
            }
        
        # BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤
        def set_model_loader(self, model_loader):
            """ModelLoader ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.model_loader = model_loader
                self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                
                # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹œë„
                if hasattr(model_loader, 'create_step_interface'):
                    try:
                        self.model_interface = model_loader.create_step_interface(self.step_name)
                        self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì£¼ì… ì™„ë£Œ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨, ModelLoader ì§ì ‘ ì‚¬ìš©: {e}")
                        self.model_interface = model_loader
                    else:
                    self.model_interface = model_loader
                    
            except Exception as e:
                self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
                self.model_loader = None
                self.model_interface = None
        
        def set_memory_manager(self, memory_manager):
            """MemoryManager ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.memory_manager = memory_manager
                self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        def set_data_converter(self, data_converter):
            """DataConverter ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.data_converter = data_converter
                self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        def set_di_container(self, di_container):
            """DI Container ì˜ì¡´ì„± ì£¼ì…"""
            try:
                self.di_container = di_container
                self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DI Container ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    import torchvision.transforms as transforms
    from torchvision.transforms.functional import resize, to_pil_image, to_tensor
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if torch.backends.mps.is_available():
        DEVICE = "mps"
        torch.mps.set_per_process_memory_fraction(0.7)
    elif torch.cuda.is_available():
        DEVICE = "cuda"
        else:
        DEVICE = "cpu"
        
except ImportError as e:
    raise ImportError(f"âŒ PyTorch í•„ìˆ˜: {e}")

try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError as e:
    raise ImportError(f"âŒ Pillow í•„ìˆ˜: {e}")

# ì„ íƒì  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

try:
    from transformers import AutoModel, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import safetensors.torch as st
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False

try:
    from scipy.optimize import curve_fit
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# ==============================================
# ğŸ”¥ 2. í¬ì¦ˆ ì¶”ì • ìƒìˆ˜ ë° ë°ì´í„° êµ¬ì¡°
# ==============================================

class PoseModel(Enum):
    """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ íƒ€ì…"""
    MEDIAPIPE = "mediapipe"
    OPENPOSE = "openpose"
    YOLOV8_POSE = "yolov8_pose"
    HRNET = "hrnet"
    DIFFUSION_POSE = "diffusion_pose"

class PoseQuality(Enum):
    """í¬ì¦ˆ í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"     # 90-100ì 
    GOOD = "good"              # 75-89ì   
    ACCEPTABLE = "acceptable"   # 60-74ì 
    POOR = "poor"              # 40-59ì 
    VERY_POOR = "very_poor"    # 0-39ì 

# COCO 17 í‚¤í¬ì¸íŠ¸ ì •ì˜ (MediaPipe, YOLOv8 í‘œì¤€)
COCO_17_KEYPOINTS = [
    "nose", "left_eye", "right_eye", "left_ear", "right_ear",
    "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
    "left_wrist", "right_wrist", "left_hip", "right_hip",
    "left_knee", "right_knee", "left_ankle", "right_ankle"
]

# OpenPose 18 í‚¤í¬ì¸íŠ¸ ì •ì˜ 
OPENPOSE_18_KEYPOINTS = [
    "nose", "neck", "right_shoulder", "right_elbow", "right_wrist",
    "left_shoulder", "left_elbow", "left_wrist", "middle_hip", "right_hip", 
    "right_knee", "right_ankle", "left_hip", "left_knee", "left_ankle",
    "right_eye", "left_eye", "right_ear", "left_ear"
]

# í‚¤í¬ì¸íŠ¸ ì—°ê²° êµ¬ì¡° (ìŠ¤ì¼ˆë ˆí†¤)
SKELETON_CONNECTIONS = [
    (0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7), (1, 8),
    (8, 9), (9, 10), (10, 11), (8, 12), (12, 13), (13, 14), (0, 15),
    (15, 17), (0, 16), (16, 18)
]

# í‚¤í¬ì¸íŠ¸ ìƒ‰ìƒ ë§¤í•‘
KEYPOINT_COLORS = [
    (255, 0, 0), (255, 85, 0), (255, 170, 0), (255, 255, 0), (170, 255, 0),
    (85, 255, 0), (0, 255, 0), (0, 255, 85), (0, 255, 170), (0, 255, 255),
    (0, 170, 255), (0, 85, 255), (0, 0, 255), (85, 0, 255), (170, 0, 255),
    (255, 0, 255), (255, 0, 170), (255, 0, 85), (255, 0, 0)
]

@dataclass
class PoseResult:
    """í¬ì¦ˆ ì¶”ì • ê²°ê³¼"""
    keypoints: List[List[float]] = field(default_factory=list)
    confidence_scores: List[float] = field(default_factory=list)
    joint_angles: Dict[str, float] = field(default_factory=dict)
    body_proportions: Dict[str, float] = field(default_factory=dict)
    pose_quality: PoseQuality = PoseQuality.POOR
    overall_confidence: float = 0.0
    processing_time: float = 0.0
    model_used: str = ""
    subpixel_accuracy: bool = False
    
    # ê³ ê¸‰ ë¶„ì„ ê²°ê³¼
    keypoints_with_uncertainty: List[Dict[str, Any]] = field(default_factory=list)
    advanced_body_metrics: Dict[str, Any] = field(default_factory=dict)
    skeleton_structure: Dict[str, Any] = field(default_factory=dict)
    ensemble_info: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# ğŸ”¥ 3. ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
# ==============================================

class MediaPoseModel:
    """MediaPipe Pose ëª¨ë¸ (ìš°ì„ ìˆœìœ„ 1)"""
    
    def __init__(self):
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.MediaPoseModel")
    
    def load_model(self) -> bool:
        """MediaPipe ëª¨ë¸ ë¡œë”©"""
        try:
            if not MEDIAPIPE_AVAILABLE:
                self.logger.error("âŒ MediaPipe ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŒ")
                return False
            
            self.model = mp.solutions.pose.Pose(
                static_image_mode=False,
                model_complexity=1,
                enable_segmentation=False,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )
            
            self.loaded = True
            self.logger.info("âœ… MediaPipe Pose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ MediaPipe ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def detect_poses(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """MediaPipe í¬ì¦ˆ ê²€ì¶œ"""
        if not self.loaded:
            raise RuntimeError("MediaPipe ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, Image.Image):
                image_np = np.array(image)
            elif isinstance(image, torch.Tensor):
                image_np = image.cpu().numpy()
                if image_np.ndim == 4:  # ë°°ì¹˜ ì°¨ì› ì œê±°
                    image_np = image_np[0]
                if image_np.shape[0] == 3:  # CHW -> HWC
                    image_np = np.transpose(image_np, (1, 2, 0))
                image_np = (image_np * 255).astype(np.uint8)
                else:
                image_np = image
            
            # RGB ë³€í™˜
            if image_np.shape[-1] == 4:  # RGBA -> RGB
                image_np = image_np[:, :, :3]
            
            # MediaPipe ì²˜ë¦¬
            results = self.model.process(image_np)
            
            keypoints = []
            if results.pose_landmarks:
                for landmark in results.pose_landmarks.landmark:
                    # MediaPipeëŠ” normalized coordinates (0-1)
                    x = landmark.x * image_np.shape[1]
                    y = landmark.y * image_np.shape[0]
                    confidence = landmark.visibility
                    keypoints.append([float(x), float(y), float(confidence)])
                
                # MediaPipe 33 â†’ COCO 17 ë³€í™˜
                keypoints = self._convert_mediapipe_to_coco17(keypoints)
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "keypoints": keypoints,
                "num_persons": 1 if keypoints else 0,
                "processing_time": processing_time,
                "model_type": "mediapipe",
                "confidence": np.mean([kp[2] for kp in keypoints]) if keypoints else 0.0
            }
            
        except Exception as e:
            self.logger.error(f"âŒ MediaPipe ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "keypoints": [],
                "error": str(e),
                "processing_time": time.time() - start_time,
                "model_type": "mediapipe"
            }
    
    def _convert_mediapipe_to_coco17(self, mp_keypoints: List[List[float]]) -> List[List[float]]:
        """MediaPipe 33 â†’ COCO 17 ë³€í™˜"""
        if len(mp_keypoints) < 33:
            return [[0.0, 0.0, 0.0] for _ in range(17)]
        
        # MediaPipe â†’ COCO 17 ë§¤í•‘
        mp_to_coco = {
            0: 0,   # nose
            2: 1,   # left_eye
            5: 2,   # right_eye
            7: 3,   # left_ear
            8: 4,   # right_ear
            11: 5,  # left_shoulder
            12: 6,  # right_shoulder
            13: 7,  # left_elbow
            14: 8,  # right_elbow
            15: 9,  # left_wrist
            16: 10, # right_wrist
            23: 11, # left_hip
            24: 12, # right_hip
            25: 13, # left_knee
            26: 14, # right_knee
            27: 15, # left_ankle
            28: 16  # right_ankle
        }
        
        coco_keypoints = [[0.0, 0.0, 0.0] for _ in range(17)]
        
        for mp_idx, coco_idx in mp_to_coco.items():
            if mp_idx < len(mp_keypoints):
                coco_keypoints[coco_idx] = mp_keypoints[mp_idx]
        
        return coco_keypoints

class YOLOv8PoseModel:
    """YOLOv8 Pose ëª¨ë¸ (ì‹¤ì‹œê°„)"""
    
    def __init__(self, model_path: Optional[Path] = None):
        self.model_path = model_path
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.YOLOv8PoseModel")
    
    def load_model(self) -> bool:
        """YOLOv8 ëª¨ë¸ ë¡œë”©"""
        try:
            if not ULTRALYTICS_AVAILABLE:
                self.logger.error("âŒ ultralytics ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŒ")
                return False
            
            if self.model_path and self.model_path.exists():
                self.model = YOLO(str(self.model_path))
                self.logger.info(f"âœ… YOLOv8 ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {self.model_path}")
                else:
                # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©
                self.model = YOLO('yolov8n-pose.pt')
                self.logger.info("âœ… YOLOv8 ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ë¡œë”©")
            
            self.loaded = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def detect_poses(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """YOLOv8 í¬ì¦ˆ ê²€ì¶œ"""
        if not self.loaded:
            raise RuntimeError("YOLOv8 ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            results = self.model(image, verbose=False)
            
            poses = []
            for result in results:
                if hasattr(result, 'keypoints') and result.keypoints is not None:
                    keypoints = result.keypoints.data  # [N, 17, 3] (x, y, confidence)
                    
                    for person_kpts in keypoints:
                        # COCO 17 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        pose_keypoints = person_kpts.cpu().numpy().tolist()
                        
                        pose_data = {
                            "keypoints": pose_keypoints,
                            "bbox": result.boxes.xyxy.cpu().numpy()[0] if result.boxes else None,
                            "confidence": float(result.boxes.conf.mean()) if result.boxes else 0.0
                        }
                        poses.append(pose_data)
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "poses": poses,
                "keypoints": poses[0]["keypoints"] if poses else [],
                "num_persons": len(poses),
                "processing_time": processing_time,
                "model_type": "yolov8_pose",
                "confidence": poses[0]["confidence"] if poses else 0.0
            }
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "keypoints": [],
                "error": str(e),
                "processing_time": time.time() - start_time,
                "model_type": "yolov8_pose"
            }

class OpenPoseModel:
    """OpenPose ëª¨ë¸"""
    
    def __init__(self, model_path: Optional[Path] = None):
        self.model_path = model_path
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.OpenPoseModel")
    
    def load_model(self) -> bool:
        """OpenPose ëª¨ë¸ ë¡œë”©"""
        try:
            if self.model_path and self.model_path.exists():
                # ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=True)
                
                self.model = self._create_openpose_network()
                self.model.load_state_dict(checkpoint, strict=False)
                self.model.eval()
                self.model.to(DEVICE)
                
                self.loaded = True
                self.logger.info(f"âœ… OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {self.model_path}")
                return True
                else:
                # ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ì—†ëŠ” ê²½ìš°)
                self.model = self._create_simple_pose_model()
                self.model.eval()
                self.model.to(DEVICE)
                
                self.loaded = True
                self.logger.info("âœ… OpenPose ë² ì´ìŠ¤ ëª¨ë¸ ìƒì„±")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ OpenPose ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _create_openpose_network(self) -> nn.Module:
        """OpenPose ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ìƒì„±"""
        class OpenPoseNetwork(nn.Module):
            def __init__(self):
                super().__init__()
                # VGG ë°±ë³¸ (ê°„ì†Œí™”)
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
                    nn.MaxPool2d(2, 2),
                    nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
                    nn.MaxPool2d(2, 2),
                    nn.AdaptiveAvgPool2d((32, 32))
                )
                
                # í‚¤í¬ì¸íŠ¸ ë¸Œëœì¹˜
                self.keypoint_branch = nn.Sequential(
                    nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(inplace=True),
                    nn.Conv2d(128, 18, 1)  # 18 keypoints
                )
            
            def forward(self, x):
                features = self.backbone(x)
                keypoints = self.keypoint_branch(features)
                return keypoints
        
        return OpenPoseNetwork()
    
    def _create_simple_pose_model(self) -> nn.Module:
        """ê°„ë‹¨í•œ í¬ì¦ˆ ëª¨ë¸ (í´ë°±ìš©)"""
        class SimplePoseModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.MaxPool2d(3, 2, 1),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),
                    nn.AdaptiveAvgPool2d((1, 1))
                )
                self.pose_head = nn.Linear(128, 18 * 3)  # 18 keypoints * (x, y, conf)
            
            def forward(self, x):
                features = self.backbone(x)
                features = features.flatten(1)
                keypoints = self.pose_head(features)
                return keypoints.view(-1, 18, 3)
        
        return SimplePoseModel()
    
    def detect_poses(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """OpenPose í¬ì¦ˆ ê²€ì¶œ"""
        if not self.loaded:
            raise RuntimeError("OpenPose ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, Image.Image):
                image_tensor = to_tensor(image).unsqueeze(0).to(DEVICE)
            elif isinstance(image, np.ndarray):
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(DEVICE) / 255.0
                else:
                image_tensor = image.to(DEVICE)
            
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                output = self.model(image_tensor)
                
                if len(output.shape) == 4:  # íˆíŠ¸ë§µ ì¶œë ¥
                    keypoints = self._extract_keypoints_from_heatmaps(output[0])
                    else:  # ì§ì ‘ ì¢Œí‘œ ì¶œë ¥
                    keypoints = output[0].cpu().numpy()
                    # ì¢Œí‘œ ì •ê·œí™”
                    h, w = image_tensor.shape[-2:]
                    keypoints_list = []
                    for kp in keypoints:
                        x, y, conf = float(kp[0] * w), float(kp[1] * h), float(torch.sigmoid(torch.tensor(kp[2])))
                        keypoints_list.append([x, y, conf])
                    keypoints = keypoints_list
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "keypoints": keypoints,
                "processing_time": processing_time,
                "model_type": "openpose",
                "confidence": np.mean([kp[2] for kp in keypoints]) if keypoints else 0.0
            }
            
        except Exception as e:
            self.logger.error(f"âŒ OpenPose AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "keypoints": [],
                "error": str(e),
                "processing_time": time.time() - start_time,
                "model_type": "openpose"
            }
    
    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(18):  # 18ê°œ í‚¤í¬ì¸íŠ¸
            if i < heatmaps.shape[0]:
                heatmap = heatmaps[i].cpu().numpy()
                
                # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
                confidence = float(heatmap[y_idx, x_idx])
                
                # ì¢Œí‘œ ì •ê·œí™”
                x = float(x_idx / w * 512)
                y = float(y_idx / h * 512)
                
                keypoints.append([x, y, confidence])
                else:
                keypoints.append([0.0, 0.0, 0.0])
        
        return keypoints

class HRNetModel:
    """HRNet ê³ ì •ë°€ ëª¨ë¸"""
    
    def __init__(self, model_path: Optional[Path] = None):
        self.model_path = model_path
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.HRNetModel")
    
    def load_model(self) -> bool:
        """HRNet ëª¨ë¸ ë¡œë”©"""
        try:
            self.model = self._create_hrnet_model()
            
            if self.model_path and self.model_path.exists():
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=True)
                self.model.load_state_dict(checkpoint, strict=False)
                self.logger.info(f"âœ… HRNet ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {self.model_path}")
                else:
                self.logger.info("âœ… HRNet ë² ì´ìŠ¤ ëª¨ë¸ ìƒì„±")
            
            self.model.eval()
            self.model.to(DEVICE)
            self.loaded = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ HRNet ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _create_hrnet_model(self) -> nn.Module:
        """HRNet ëª¨ë¸ ìƒì„±"""
        class HRNetSimple(nn.Module):
            def __init__(self):
                super().__init__()
                # ê°„ì†Œí™”ëœ HRNet êµ¬ì¡°
                self.stem = nn.Sequential(
                    nn.Conv2d(3, 64, 3, 2, 1), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.Conv2d(64, 64, 3, 2, 1), nn.BatchNorm2d(64), nn.ReLU()
                )
                
                self.stage1 = nn.Sequential(
                    nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(),
                    nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU()
                )
                
                self.final_layer = nn.Conv2d(256, 17, 1)  # COCO 17 keypoints
            
            def forward(self, x):
                x = self.stem(x)
                x = self.stage1(x)
                x = self.final_layer(x)
                return x
        
        return HRNetSimple()
    
    def detect_poses(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """HRNet ê³ ì •ë°€ í¬ì¦ˆ ê²€ì¶œ"""
        if not self.loaded:
            raise RuntimeError("HRNet ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, Image.Image):
                image_tensor = transforms.ToTensor()(image).unsqueeze(0).to(DEVICE)
            elif isinstance(image, np.ndarray):
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(DEVICE) / 255.0
                else:
                image_tensor = image.to(DEVICE)
            
            # ì…ë ¥ í¬ê¸° ì •ê·œí™” (256x192)
            image_tensor = F.interpolate(image_tensor, size=(256, 192), mode='bilinear', align_corners=False)
            
            # ì‹¤ì œ HRNet AI ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                heatmaps = self.model(image_tensor)  # [1, 17, 64, 48]
            
            # íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
            keypoints = self._extract_keypoints_from_heatmaps(heatmaps[0])
            
            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§
            if isinstance(image, Image.Image):
                orig_w, orig_h = image.size
            elif isinstance(image, np.ndarray):
                orig_h, orig_w = image.shape[:2]
                else:
                orig_h, orig_w = 256, 192
            
            # ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§
            scale_x = orig_w / 192
            scale_y = orig_h / 256
            
            scaled_keypoints = []
            for kp in keypoints:
                scaled_keypoints.append([
                    kp[0] * scale_x,
                    kp[1] * scale_y,
                    kp[2]
                ])
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "keypoints": scaled_keypoints,
                "processing_time": processing_time,
                "model_type": "hrnet",
                "confidence": np.mean([kp[2] for kp in scaled_keypoints]) if scaled_keypoints else 0.0
            }
            
        except Exception as e:
            self.logger.error(f"âŒ HRNet AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "keypoints": [],
                "error": str(e),
                "processing_time": time.time() - start_time,
                "model_type": "hrnet"
            }
    
    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ê³ ì •ë°€ ì„œë¸Œí”½ì…€ ì •í™•ë„)"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(17):  # COCO 17 keypoints
            if i < heatmaps.shape[0]:
                heatmap = heatmaps[i].cpu().numpy()
                
                # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
                max_val = heatmap[y_idx, x_idx]
                
                # ì„œë¸Œí”½ì…€ ì •í™•ë„ë¥¼ ìœ„í•œ ê°€ìš°ì‹œì•ˆ í”¼íŒ…
                if (1 <= x_idx < w-1) and (1 <= y_idx < h-1):
                    # x ë°©í–¥ ì„œë¸Œí”½ì…€ ë³´ì •
                    dx = 0.5 * (heatmap[y_idx, x_idx+1] - heatmap[y_idx, x_idx-1]) / (
                        heatmap[y_idx, x_idx+1] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx, x_idx-1] + 1e-8)
                    
                    # y ë°©í–¥ ì„œë¸Œí”½ì…€ ë³´ì •
                    dy = 0.5 * (heatmap[y_idx+1, x_idx] - heatmap[y_idx-1, x_idx]) / (
                        heatmap[y_idx+1, x_idx] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx-1, x_idx] + 1e-8)
                    
                    # ì„œë¸Œí”½ì…€ ì¢Œí‘œ
                    x_subpixel = x_idx + dx
                    y_subpixel = y_idx + dy
                    else:
                    x_subpixel = x_idx
                    y_subpixel = y_idx
                
                # ì¢Œí‘œ ì •ê·œí™”
                x_normalized = x_subpixel / w
                y_normalized = y_subpixel / h
                
                # ì‹¤ì œ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
                x_coord = x_normalized * 192
                y_coord = y_normalized * 256
                confidence = float(max_val)
                
                keypoints.append([x_coord, y_coord, confidence])
                else:
                keypoints.append([0.0, 0.0, 0.0])
        
        return keypoints

# ==============================================
# ğŸ”¥ 4. í¬ì¦ˆ ë¶„ì„ ì•Œê³ ë¦¬ì¦˜
# ==============================================

class PoseAnalyzer:
    """í¬ì¦ˆ ë¶„ì„ ì•Œê³ ë¦¬ì¦˜"""
    
    @staticmethod
    def calculate_joint_angles(keypoints: List[List[float]]) -> Dict[str, float]:
        """ê´€ì ˆ ê°ë„ ê³„ì‚°"""
        angles = {}
        
        def angle_between_vectors(p1, p2, p3):
            """ì„¸ ì  ì‚¬ì´ì˜ ê°ë„ ê³„ì‚°"""
            try:
                v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
                v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
                
                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
                cos_angle = np.clip(cos_angle, -1.0, 1.0)
                angle = np.arccos(cos_angle)
                
                return np.degrees(angle)
                except:
                return 0.0
        
        if len(keypoints) >= 17:
            # COCO 17 í‚¤í¬ì¸íŠ¸ ê¸°ì¤€
            # ì™¼ìª½ íŒ”ê¿ˆì¹˜ ê°ë„ (ì–´ê¹¨-íŒ”ê¿ˆì¹˜-ì†ëª©)
            if all(kp[2] > 0.3 for kp in [keypoints[5], keypoints[7], keypoints[9]]):
                angles['left_elbow'] = angle_between_vectors(keypoints[5], keypoints[7], keypoints[9])
            
            # ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜ ê°ë„
            if all(kp[2] > 0.3 for kp in [keypoints[6], keypoints[8], keypoints[10]]):
                angles['right_elbow'] = angle_between_vectors(keypoints[6], keypoints[8], keypoints[10])
            
            # ì™¼ìª½ ë¬´ë¦ ê°ë„
            if all(kp[2] > 0.3 for kp in [keypoints[11], keypoints[13], keypoints[15]]):
                angles['left_knee'] = angle_between_vectors(keypoints[11], keypoints[13], keypoints[15])
            
            # ì˜¤ë¥¸ìª½ ë¬´ë¦ ê°ë„
            if all(kp[2] > 0.3 for kp in [keypoints[12], keypoints[14], keypoints[16]]):
                angles['right_knee'] = angle_between_vectors(keypoints[12], keypoints[14], keypoints[16])
        
        return angles
    
    @staticmethod
    def calculate_body_proportions(keypoints: List[List[float]]) -> Dict[str, float]:
        """ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°"""
        proportions = {}
        
        if len(keypoints) >= 17:
            # ì–´ê¹¨ ë„ˆë¹„
            if all(kp[2] > 0.3 for kp in [keypoints[5], keypoints[6]]):
                shoulder_width = np.linalg.norm(
                    np.array(keypoints[5][:2]) - np.array(keypoints[6][:2])
                )
                proportions['shoulder_width'] = shoulder_width
            
            # ì—‰ë©ì´ ë„ˆë¹„
            if all(kp[2] > 0.3 for kp in [keypoints[11], keypoints[12]]):
                hip_width = np.linalg.norm(
                    np.array(keypoints[11][:2]) - np.array(keypoints[12][:2])
                )
                proportions['hip_width'] = hip_width
            
            # ì „ì²´ í‚¤ (ì½”-ë°œëª©)
            if (keypoints[0][2] > 0.3 and 
                (keypoints[15][2] > 0.3 or keypoints[16][2] > 0.3)):
                if keypoints[15][2] > keypoints[16][2]:
                    height = np.linalg.norm(
                        np.array(keypoints[0][:2]) - np.array(keypoints[15][:2])
                    )
                    else:
                    height = np.linalg.norm(
                        np.array(keypoints[0][:2]) - np.array(keypoints[16][:2])
                    )
                proportions['total_height'] = height
        
        return proportions
    
    @staticmethod
    def assess_pose_quality(keypoints: List[List[float]], 
                          joint_angles: Dict[str, float], 
                          body_proportions: Dict[str, float]) -> Dict[str, Any]:
        """í¬ì¦ˆ í’ˆì§ˆ í‰ê°€"""
        assessment = {
            'overall_score': 0.0,
            'quality_grade': PoseQuality.POOR,
            'detailed_scores': {},
            'issues': [],
            'recommendations': []
        }
        
        # í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„± ì ìˆ˜
        visible_keypoints = sum(1 for kp in keypoints if kp[2] > 0.5)
        visibility_score = visible_keypoints / len(keypoints)
        
        # ì‹ ë¢°ë„ ì ìˆ˜
        confidence_scores = [kp[2] for kp in keypoints if kp[2] > 0.1]
        avg_confidence = np.mean(confidence_scores) if confidence_scores else 0.0
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = (visibility_score * 0.5 + avg_confidence * 0.5)
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if overall_score >= 0.9:
            quality_grade = PoseQuality.EXCELLENT
        elif overall_score >= 0.75:
            quality_grade = PoseQuality.GOOD
        elif overall_score >= 0.6:
            quality_grade = PoseQuality.ACCEPTABLE
            else:
            quality_grade = PoseQuality.POOR
        
        assessment.update({
            'overall_score': overall_score,
            'quality_grade': quality_grade,
            'detailed_scores': {
                'visibility': visibility_score,
                'confidence': avg_confidence
            }
        })
        
        return assessment

# ==============================================
# ğŸ”¥ 5. ë©”ì¸ PoseEstimationStep í´ë˜ìŠ¤
# ==============================================

class PoseEstimationStep(BaseStepMixin):
    """
    ğŸ”¥ Step 02: Pose Estimation - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
    
    âœ… BaseStepMixin ìƒì† íŒ¨í„´ (Human Parsing Stepê³¼ ë™ì¼)
    âœ… MediaPipe Pose ëª¨ë¸ ì§€ì› (ìš°ì„ ìˆœìœ„ 1)
    âœ… OpenPose ëª¨ë¸ ì§€ì› (í´ë°± ì˜µì…˜)
    âœ… YOLOv8-Pose ëª¨ë¸ ì§€ì› (ì‹¤ì‹œê°„)
    âœ… HRNet ëª¨ë¸ ì§€ì› (ê³ ì •ë°€)
    âœ… 17ê°œ COCO keypoints ê°ì§€
    âœ… Mock ëª¨ë¸ ì™„ì „ ì œê±°
    âœ… ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
    âœ… ë‹¤ì¤‘ ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ
    """
    
    def __init__(self, **kwargs):
        """í¬ì¦ˆ ì¶”ì • Step ì´ˆê¸°í™”"""
        self._lock = threading.RLock()  # âœ… threading ì‚¬ìš©

        # ğŸ”¥ 1. í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™” (ì—ëŸ¬ ë°©ì§€)
        self._initialize_step_attributes()
        
        # ğŸ”¥ 2. BaseStepMixin ì´ˆê¸°í™” (Central Hub ìë™ ì—°ë™)
        super().__init__(step_name="PoseEstimationStep", step_id=2, **kwargs)
        
        # ğŸ”¥ 3. Pose Estimation íŠ¹í™” ì´ˆê¸°í™”
        self._initialize_pose_estimation_specifics()
    
    def _initialize_step_attributes(self):
        """Step í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™”"""
        self.ai_models = {}
        self.models_loading_status = {
            'mediapipe': False,
            'openpose': False,
            'yolov8': False,
            'hrnet': False,
            'total_loaded': 0,
            'loading_errors': []
        }
        self.model_interface = None
        self.loaded_models = {}
        
        # Pose Estimation íŠ¹í™” ì†ì„±ë“¤
        self.pose_models = {}
        self.pose_ready = False
        self.keypoints_cache = {}
    
    def _initialize_pose_estimation_specifics(self):
        """Pose Estimation íŠ¹í™” ì´ˆê¸°í™”"""
        
        # ì„¤ì •
        self.confidence_threshold = 0.5
        self.use_subpixel = True
        
        # í¬ì¦ˆ ë¶„ì„ê¸°
        self.analyzer = PoseAnalyzer()
        
        # ëª¨ë¸ ìš°ì„ ìˆœìœ„ (MediaPipe ìš°ì„ )
        self.model_priority = [
            PoseModel.MEDIAPIPE,
            PoseModel.YOLOV8_POSE,
            PoseModel.OPENPOSE,
            PoseModel.HRNET
        ]
        
        self.logger.info(f"âœ… {self.step_name} í¬ì¦ˆ ì¶”ì • íŠ¹í™” ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_pose_models_via_central_hub(self):
        """Central Hubë¥¼ í†µí•œ Pose ëª¨ë¸ ë¡œë”©"""
        loaded_count = 0
        
        if self.model_loader:  # Central Hubì—ì„œ ìë™ ì£¼ì…ë¨
            # MediaPipe ëª¨ë¸ ë¡œë”©
            try:
                mediapipe_model = MediaPoseModel()
                if mediapipe_model.load_model():
                    self.ai_models['mediapipe'] = mediapipe_model
                    self.models_loading_status['mediapipe'] = True
                    loaded_count += 1
                    self.logger.info("âœ… MediaPipe ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ MediaPipe ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.models_loading_status['loading_errors'].append(f"MediaPipe: {e}")
            
            # YOLOv8 ëª¨ë¸ ë¡œë”©
            try:
                # Central Hubì—ì„œ YOLOv8 ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì¡°íšŒ
                yolo_path = self._get_model_path_from_central_hub('yolov8n-pose.pt')
                yolo_model = YOLOv8PoseModel(yolo_path)
                if yolo_model.load_model():
                    self.ai_models['yolov8'] = yolo_model
                    self.models_loading_status['yolov8'] = True
                    loaded_count += 1
                    self.logger.info("âœ… YOLOv8 ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ YOLOv8 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.models_loading_status['loading_errors'].append(f"YOLOv8: {e}")
            
            # OpenPose ëª¨ë¸ ë¡œë”©
            try:
                openpose_path = self._get_model_path_from_central_hub('body_pose_model.pth')
                openpose_model = OpenPoseModel(openpose_path)
                if openpose_model.load_model():
                    self.ai_models['openpose'] = openpose_model
                    self.models_loading_status['openpose'] = True
                    loaded_count += 1
                    self.logger.info("âœ… OpenPose ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ OpenPose ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.models_loading_status['loading_errors'].append(f"OpenPose: {e}")
            
            # HRNet ëª¨ë¸ ë¡œë”©
            try:
                hrnet_path = self._get_model_path_from_central_hub('hrnet_w48_coco_256x192.pth')
                hrnet_model = HRNetModel(hrnet_path)
                if hrnet_model.load_model():
                    self.ai_models['hrnet'] = hrnet_model
                    self.models_loading_status['hrnet'] = True
                    loaded_count += 1
                    self.logger.info("âœ… HRNet ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ HRNet ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.models_loading_status['loading_errors'].append(f"HRNet: {e}")
        
            else:
            # í´ë°±: MediaPipeë§Œ ë¡œë”© ì‹œë„
            self.logger.warning("âš ï¸ ModelLoaderê°€ ì—†ìŒ - MediaPipeë§Œ ë¡œë”© ì‹œë„")
            try:
                mediapipe_model = MediaPoseModel()
                if mediapipe_model.load_model():
                    self.ai_models['mediapipe'] = mediapipe_model
                    self.models_loading_status['mediapipe'] = True
                    loaded_count += 1
            except Exception as e:
                self.logger.error(f"âŒ MediaPipe í´ë°± ë¡œë”©ë„ ì‹¤íŒ¨: {e}")
        
        self.models_loading_status['total_loaded'] = loaded_count
        self.pose_ready = loaded_count > 0
        
        if loaded_count > 0:
            self.logger.info(f"ğŸ‰ í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}ê°œ")
            else:
            self.logger.error("âŒ ëª¨ë“  í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        
        return loaded_count
    
    def _get_model_path_from_central_hub(self, model_name: str) -> Optional[Path]:
        """Central Hubë¥¼ í†µí•œ ëª¨ë¸ ê²½ë¡œ ì¡°íšŒ"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'get_model_path'):
                return self.model_loader.get_model_path(model_name, step_name=self.step_name)
            return None
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ê²½ë¡œ ì¡°íšŒ ì‹¤íŒ¨ ({model_name}): {e}")
            return None
    
    async def initialize(self):
        """Step ì´ˆê¸°í™” (BaseStepMixin í˜¸í™˜)"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"ğŸ”„ {self.step_name} ì´ˆê¸°í™” ì‹œì‘...")
            
            # Pose ëª¨ë¸ë“¤ ë¡œë”©
            loaded_count = self._load_pose_models_via_central_hub()
            
            if loaded_count == 0:
                self.logger.error("âŒ í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            self.is_ready = True
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ ({loaded_count}ê°œ ëª¨ë¸)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ”¥ ì‹¤ì œ Pose Estimation AI ì¶”ë¡  (BaseStepMixin v20.0 í˜¸í™˜)"""
        try:
            start_time = time.time()
            
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            image = processed_input.get('image')
            if image is None:
                raise ValueError("ì…ë ¥ ì´ë¯¸ì§€ ì—†ìŒ")
            
            self.logger.info("ğŸ§  Pose Estimation ì‹¤ì œ AI ì¶”ë¡  ì‹œì‘")
            
            # ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì€ ê²½ìš° ì´ˆê¸°í™” ì‹œë„
            if not self.pose_ready:
                self.logger.warning("âš ï¸ í¬ì¦ˆ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ - ì¬ë¡œë”© ì‹œë„")
                loaded = self._load_pose_models_via_central_hub()
                if loaded == 0:
                    raise RuntimeError("í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            
            # ë‹¤ì¤‘ ëª¨ë¸ë¡œ í¬ì¦ˆ ì¶”ì • ì‹œë„ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
            best_result = None
            best_confidence = 0.0
            
            for model_type in self.model_priority:
                model_key = model_type.value
                
                if model_key in self.ai_models:
                    try:
                        self.logger.debug(f"ğŸ”„ {model_key} ëª¨ë¸ë¡œ í¬ì¦ˆ ì¶”ì • ì‹œë„")
                        result = self.ai_models[model_key].detect_poses(image)
                        
                        if result.get('success') and result.get('keypoints'):
                            confidence = result.get('confidence', 0.0)
                            
                            # ìµœê³  ì‹ ë¢°ë„ ê²°ê³¼ ì„ íƒ
                            if confidence > best_confidence:
                                best_result = result
                                best_confidence = confidence
                                best_result['primary_model'] = model_key
                            
                            self.logger.debug(f"âœ… {model_key} ì„±ê³µ (ì‹ ë¢°ë„: {confidence:.3f})")
                            
                            else:
                            self.logger.debug(f"âš ï¸ {model_key} ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
                            
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {model_key} ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        continue
            
            if not best_result or not best_result.get('keypoints'):
                raise RuntimeError("ëª¨ë“  í¬ì¦ˆ ëª¨ë¸ì—ì„œ ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ë¥¼ ê²€ì¶œí•˜ì§€ ëª»í•¨")
            
            # í‚¤í¬ì¸íŠ¸ í›„ì²˜ë¦¬ ë° ë¶„ì„
            keypoints = best_result['keypoints']
            
            # ê´€ì ˆ ê°ë„ ê³„ì‚°
            joint_angles = self.analyzer.calculate_joint_angles(keypoints)
            
            # ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°
            body_proportions = self.analyzer.calculate_body_proportions(keypoints)
            
            # í¬ì¦ˆ í’ˆì§ˆ í‰ê°€
            quality_assessment = self.analyzer.assess_pose_quality(
                keypoints, joint_angles, body_proportions
            )
            
            inference_time = time.time() - start_time
            
            return {
                'success': True,
                'keypoints': keypoints,
                'confidence_scores': [kp[2] for kp in keypoints],
                'joint_angles': joint_angles,
                'body_proportions': body_proportions,
                'pose_quality': quality_assessment['overall_score'],
                'quality_grade': quality_assessment['quality_grade'].value,
                'processing_time': inference_time,
                'model_used': best_result.get('primary_model', 'unknown'),
                'real_ai_inference': True,
                'pose_estimation_ready': True,
                'num_keypoints_detected': len([kp for kp in keypoints if kp[2] > 0.3]),
                
                # ê³ ê¸‰ ë¶„ì„ ê²°ê³¼
                'detailed_scores': quality_assessment.get('detailed_scores', {}),
                'pose_recommendations': quality_assessment.get('recommendations', []),
                'skeleton_structure': self._build_skeleton_structure(keypoints),
                'landmarks': self._extract_landmarks(keypoints)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Pose Estimation AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'keypoints': [],
                'confidence_scores': [],
                'pose_quality': 0.0,
                'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                'model_used': 'error',
                'real_ai_inference': False,
                'pose_estimation_ready': False
            }
    
    def _build_skeleton_structure(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±"""
        skeleton = {
            'connections': [],
            'bone_lengths': {},
            'valid_connections': 0
        }
        
        # COCO 17 ì—°ê²° êµ¬ì¡°
        coco_connections = [
            (0, 1), (0, 2), (1, 3), (2, 4),  # ë¨¸ë¦¬
            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # íŒ”
            (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)  # ë‹¤ë¦¬
        ]
        
        for i, (start_idx, end_idx) in enumerate(coco_connections):
            if (start_idx < len(keypoints) and end_idx < len(keypoints) and
                len(keypoints[start_idx]) >= 3 and len(keypoints[end_idx]) >= 3):
                
                start_kp = keypoints[start_idx]
                end_kp = keypoints[end_idx]
                
                if start_kp[2] > self.confidence_threshold and end_kp[2] > self.confidence_threshold:
                    bone_length = np.sqrt(
                        (start_kp[0] - end_kp[0])**2 + (start_kp[1] - end_kp[1])**2
                    )
                    
                    connection = {
                        'start': start_idx,
                        'end': end_idx,
                        'start_name': COCO_17_KEYPOINTS[start_idx] if start_idx < len(COCO_17_KEYPOINTS) else f"point_{start_idx}",
                        'end_name': COCO_17_KEYPOINTS[end_idx] if end_idx < len(COCO_17_KEYPOINTS) else f"point_{end_idx}",
                        'length': bone_length,
                        'confidence': (start_kp[2] + end_kp[2]) / 2
                    }
                    
                    skeleton['connections'].append(connection)
                    skeleton['bone_lengths'][f"{start_idx}_{end_idx}"] = bone_length
                    skeleton['valid_connections'] += 1
        
        return skeleton
    
    def _extract_landmarks(self, keypoints: List[List[float]]) -> Dict[str, Dict[str, float]]:
        """ì£¼ìš” ëœë“œë§ˆí¬ ì¶”ì¶œ"""
        landmarks = {}
        
        for i, kp in enumerate(keypoints):
            if len(kp) >= 3 and kp[2] > self.confidence_threshold:
                landmark_name = COCO_17_KEYPOINTS[i] if i < len(COCO_17_KEYPOINTS) else f"landmark_{i}"
                landmarks[landmark_name] = {
                    'x': float(kp[0]),
                    'y': float(kp[1]),
                    'confidence': float(kp[2])
                }
        
        return landmarks
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
        try:
            self.model_loader = model_loader
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            
            # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹œë„
            if hasattr(model_loader, 'create_step_interface'):
                try:
                    self.model_interface = model_loader.create_step_interface(self.step_name)
                    self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì£¼ì… ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨, ModelLoader ì§ì ‘ ì‚¬ìš©: {e}")
                    self.model_interface = model_loader
                else:
                self.model_interface = model_loader
                
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            self.model_loader = None
            self.model_interface = None
            
    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
        try:
            self.memory_manager = memory_manager
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
        try:
            self.data_converter = data_converter
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_di_container(self, di_container):
        """DI Container ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.di_container = di_container
            self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
            
            # AI ëª¨ë¸ë“¤ ì •ë¦¬
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'cleanup'):
                        model.cleanup()
                    del model
                except Exception as e:
                    self.logger.debug(f"ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨ ({model_name}): {e}")
            
            # ìºì‹œ ì •ë¦¬
            self.ai_models.clear()
            self.pose_models.clear()
            self.keypoints_cache.clear()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif torch.backends.mps.is_available():
                    torch.mps.empty_cache()
            
            gc.collect()
            
            self.logger.info(f"âœ… {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_model_status(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'pose_ready': self.pose_ready,
            'models_loading_status': self.models_loading_status,
            'loaded_models': list(self.ai_models.keys()),
            'model_priority': [model.value for model in self.model_priority],
            'confidence_threshold': self.confidence_threshold,
            'use_subpixel': self.use_subpixel
        }

# ==============================================
# ğŸ”¥ 6. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def validate_keypoints(keypoints: List[List[float]]) -> bool:
    """í‚¤í¬ì¸íŠ¸ ìœ íš¨ì„± ê²€ì¦"""
    try:
        if not keypoints:
            return False
        
        for kp in keypoints:
            if len(kp) < 3:
                return False
            if not all(isinstance(x, (int, float)) for x in kp):
                return False
            if kp[2] < 0 or kp[2] > 1:
                return False
        
        return True
        
    except Exception:
        return False

def draw_pose_on_image(
    image: Union[np.ndarray, Image.Image],
    keypoints: List[List[float]],
    confidence_threshold: float = 0.5,
    keypoint_size: int = 4,
    line_width: int = 3
) -> Image.Image:
    """ì´ë¯¸ì§€ì— í¬ì¦ˆ ê·¸ë¦¬ê¸°"""
    try:
        if isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
            else:
            pil_image = image.copy()
        
        draw = ImageDraw.Draw(pil_image)
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for i, kp in enumerate(keypoints):
            if len(kp) >= 3 and kp[2] > confidence_threshold:
                x, y = int(kp[0]), int(kp[1])
                color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                
                radius = int(keypoint_size + kp[2] * 6)
                draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                           fill=color, outline=(255, 255, 255), width=2)
        
        # ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸° (COCO 17 ì—°ê²° êµ¬ì¡°)
        coco_connections = [
            (0, 1), (0, 2), (1, 3), (2, 4),  # ë¨¸ë¦¬
            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # íŒ”
            (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)  # ë‹¤ë¦¬
        ]
        
        for i, (start_idx, end_idx) in enumerate(coco_connections):
            if (start_idx < len(keypoints) and end_idx < len(keypoints)):
                start_kp = keypoints[start_idx]
                end_kp = keypoints[end_idx]
                
                if (len(start_kp) >= 3 and len(end_kp) >= 3 and
                    start_kp[2] > confidence_threshold and end_kp[2] > confidence_threshold):
                    
                    start_point = (int(start_kp[0]), int(start_kp[1]))
                    end_point = (int(end_kp[0]), int(end_kp[1]))
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    avg_confidence = (start_kp[2] + end_kp[2]) / 2
                    adjusted_width = int(line_width * avg_confidence)
                    
                    draw.line([start_point, end_point], fill=color, width=max(1, adjusted_width))
        
        return pil_image
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
        return image if isinstance(image, Image.Image) else Image.fromarray(image)

def analyze_pose_for_clothing(
    keypoints: List[List[float]],
    clothing_type: str = "default",
    confidence_threshold: float = 0.5
) -> Dict[str, Any]:
    """ì˜ë¥˜ë³„ í¬ì¦ˆ ì í•©ì„± ë¶„ì„"""
    try:
        if not keypoints:
            return {
                'suitable_for_fitting': False,
                'issues': ["í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"],
                'recommendations': ["ë” ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”"],
                'pose_score': 0.0
            }
        
        # ì˜ë¥˜ë³„ ê°€ì¤‘ì¹˜
        clothing_weights = {
            'shirt': {'arms': 0.4, 'torso': 0.4, 'visibility': 0.2},
            'dress': {'torso': 0.5, 'arms': 0.3, 'legs': 0.2},
            'pants': {'legs': 0.6, 'torso': 0.3, 'visibility': 0.1},
            'jacket': {'arms': 0.5, 'torso': 0.4, 'visibility': 0.1},
            'default': {'torso': 0.4, 'arms': 0.3, 'legs': 0.2, 'visibility': 0.1}
        }
        
        weights = clothing_weights.get(clothing_type, clothing_weights['default'])
        
        # ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°
        def calculate_body_part_score(part_indices: List[int]) -> float:
            visible_count = 0
            total_confidence = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            
            return visibility_ratio * avg_confidence
        
        # COCO 17 ë¶€ìœ„ë³„ ì¸ë±ìŠ¤
        torso_indices = [5, 6, 11, 12]  # ì–´ê¹¨, ì—‰ë©ì´
        arm_indices = [5, 6, 7, 8, 9, 10]  # ì–´ê¹¨, íŒ”ê¿ˆì¹˜, ì†ëª©
        leg_indices = [11, 12, 13, 14, 15, 16]  # ì—‰ë©ì´, ë¬´ë¦, ë°œëª©
        
        torso_score = calculate_body_part_score(torso_indices)
        arms_score = calculate_body_part_score(arm_indices)
        legs_score = calculate_body_part_score(leg_indices)
        
        # í¬ì¦ˆ ì ìˆ˜
        pose_score = (
            torso_score * weights.get('torso', 0.4) +
            arms_score * weights.get('arms', 0.3) +
            legs_score * weights.get('legs', 0.2) +
            weights.get('visibility', 0.1) * min(1.0, (torso_score + arms_score + legs_score) / 3)
        )
        
        # ì í•©ì„± íŒë‹¨
        suitable_for_fitting = pose_score >= 0.7
        
        # ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­
        issues = []
        recommendations = []
        
        if pose_score < 0.7:
            issues.append(f'{clothing_type} ì°©ìš©ì— ì í•©í•˜ì§€ ì•Šì€ í¬ì¦ˆ')
            recommendations.append('ë” ëª…í™•í•œ í¬ì¦ˆë¡œ ë‹¤ì‹œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        if torso_score < 0.5:
            issues.append('ìƒì²´ê°€ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤')
            recommendations.append('ìƒì²´ ì „ì²´ê°€ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        return {
            'suitable_for_fitting': suitable_for_fitting,
            'issues': issues,
            'recommendations': recommendations,
            'pose_score': pose_score,
            'detailed_scores': {
                'torso': torso_score,
                'arms': arms_score,
                'legs': legs_score
            },
            'clothing_type': clothing_type
        }
        
    except Exception as e:
        logger.error(f"ì˜ë¥˜ë³„ í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            'suitable_for_fitting': False,
            'issues': ["ë¶„ì„ ì‹¤íŒ¨"],
            'recommendations': ["ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”"],
            'pose_score': 0.0
        }

def convert_coco17_to_openpose18(coco_keypoints: List[List[float]]) -> List[List[float]]:
    """COCO 17 â†’ OpenPose 18 ë³€í™˜"""
    if len(coco_keypoints) < 17:
        return [[0.0, 0.0, 0.0] for _ in range(18)]
    
    openpose_keypoints = [[0.0, 0.0, 0.0] for _ in range(18)]
    
    # COCO 17 â†’ OpenPose 18 ë§¤í•‘
    coco_to_openpose = {
        0: 0,   # nose
        1: 15,  # left_eye â†’ right_eye
        2: 16,  # right_eye â†’ left_eye
        3: 17,  # left_ear â†’ right_ear
        4: 18,  # right_ear â†’ left_ear
        5: 5,   # left_shoulder
        6: 2,   # right_shoulder
        7: 6,   # left_elbow
        8: 3,   # right_elbow
        9: 7,   # left_wrist
        10: 4,  # right_wrist
        11: 12, # left_hip
        12: 9,  # right_hip
        13: 13, # left_knee
        14: 10, # right_knee
        15: 14, # left_ankle
        16: 11  # right_ankle
    }
    
    # neck ê³„ì‚° (ì–´ê¹¨ ì¤‘ì )
    if len(coco_keypoints) > 6:
        left_shoulder = coco_keypoints[5]
        right_shoulder = coco_keypoints[6]
        if left_shoulder[2] > 0.1 and right_shoulder[2] > 0.1:
            neck_x = (left_shoulder[0] + right_shoulder[0]) / 2
            neck_y = (left_shoulder[1] + right_shoulder[1]) / 2
            neck_conf = (left_shoulder[2] + right_shoulder[2]) / 2
            openpose_keypoints[1] = [float(neck_x), float(neck_y), float(neck_conf)]
    
    # middle_hip ê³„ì‚° (ì—‰ë©ì´ ì¤‘ì )
    if len(coco_keypoints) > 12:
        left_hip = coco_keypoints[11]
        right_hip = coco_keypoints[12]
        if left_hip[2] > 0.1 and right_hip[2] > 0.1:
            middle_hip_x = (left_hip[0] + right_hip[0]) / 2
            middle_hip_y = (left_hip[1] + right_hip[1]) / 2
            middle_hip_conf = (left_hip[2] + right_hip[2]) / 2
            openpose_keypoints[8] = [float(middle_hip_x), float(middle_hip_y), float(middle_hip_conf)]
    
    # ë‚˜ë¨¸ì§€ í‚¤í¬ì¸íŠ¸ ë§¤í•‘
    for coco_idx, openpose_idx in coco_to_openpose.items():
        if coco_idx < len(coco_keypoints) and openpose_idx < 18:
            openpose_keypoints[openpose_idx] = [
                float(coco_keypoints[coco_idx][0]),
                float(coco_keypoints[coco_idx][1]),
                float(coco_keypoints[coco_idx][2])
            ]
    
    return openpose_keypoints

# ==============================================
# ğŸ”¥ 7. Step ìƒì„± í•¨ìˆ˜ë“¤
# ==============================================

async def create_pose_estimation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> PoseEstimationStep:
    """í¬ì¦ˆ ì¶”ì • Step ìƒì„± í•¨ìˆ˜"""
    try:
        device_param = None if device == "auto" else device
        
        if config is None:
            config = {}
        config.update(kwargs)
        config['production_ready'] = True
        
        step = PoseEstimationStep(device=device_param, config=config)
        
        initialization_success = await step.initialize()
        
        if not initialization_success:
            raise RuntimeError("í¬ì¦ˆ ì¶”ì • Step ì´ˆê¸°í™” ì‹¤íŒ¨")
        
        return step
        
    except Exception as e:
        logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • Step ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def create_pose_estimation_step_sync(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> PoseEstimationStep:
    """ë™ê¸°ì‹ í¬ì¦ˆ ì¶”ì • Step ìƒì„±"""
    try:
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            create_pose_estimation_step(device, config, **kwargs)
        )
    except Exception as e:
        logger.error(f"âŒ ë™ê¸°ì‹ í¬ì¦ˆ ì¶”ì • Step ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# ==============================================
# ğŸ”¥ 8. í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ==============================================

async def test_pose_estimation():
    """í¬ì¦ˆ ì¶”ì • í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ Pose Estimation Step í…ŒìŠ¤íŠ¸")
        print("=" * 80)
        
        # Step ìƒì„±
        step = await create_pose_estimation_step(
            device="auto",
            config={
                'confidence_threshold': 0.5,
                'use_subpixel': True,
                'production_ready': True
            }
        )
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
        test_image = Image.new('RGB', (512, 512), (128, 128, 128))
        
        print(f"ğŸ“‹ Step ì •ë³´:")
        status = step.get_model_status()
        print(f"   ğŸ¯ Step: {status['step_name']}")
        print(f"   ğŸ’ ì¤€ë¹„ ìƒíƒœ: {status['pose_ready']}")
        print(f"   ğŸ¤– ë¡œë”©ëœ ëª¨ë¸: {len(status['loaded_models'])}ê°œ")
        print(f"   ğŸ“‹ ëª¨ë¸ ëª©ë¡: {', '.join(status['loaded_models'])}")
        
        # ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸
        result = await step.process(image=test_image)
        
        if result['success']:
            print(f"âœ… í¬ì¦ˆ ì¶”ì • ì„±ê³µ")
            print(f"ğŸ¯ ê²€ì¶œëœ í‚¤í¬ì¸íŠ¸: {len(result.get('keypoints', []))}")
            print(f"ğŸ–ï¸ í¬ì¦ˆ í’ˆì§ˆ: {result.get('pose_quality', 0):.3f}")
            print(f"ğŸ† ì‚¬ìš©ëœ ëª¨ë¸: {result.get('model_used', 'unknown')}")
            print(f"âš¡ ì¶”ë¡  ì‹œê°„: {result.get('processing_time', 0):.3f}ì´ˆ")
            print(f"ğŸ” ì‹¤ì œ AI ì¶”ë¡ : {result.get('real_ai_inference', False)}")
            else:
            print(f"âŒ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
        
        await step.cleanup()
        print(f"ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_pose_algorithms():
    """í¬ì¦ˆ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ§  í¬ì¦ˆ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # ë”ë¯¸ COCO 17 í‚¤í¬ì¸íŠ¸
        keypoints = [
            [128, 50, 0.9],   # nose
            [120, 40, 0.8],   # left_eye
            [136, 40, 0.8],   # right_eye
            [115, 45, 0.7],   # left_ear
            [141, 45, 0.7],   # right_ear
            [100, 100, 0.7],  # left_shoulder
            [156, 100, 0.7],  # right_shoulder
            [80, 130, 0.6],   # left_elbow
            [176, 130, 0.6],  # right_elbow
            [60, 160, 0.5],   # left_wrist
            [196, 160, 0.5],  # right_wrist
            [108, 180, 0.7],  # left_hip
            [148, 180, 0.7],  # right_hip
            [98, 220, 0.6],   # left_knee
            [158, 220, 0.6],  # right_knee
            [88, 260, 0.5],   # left_ankle
            [168, 260, 0.5],  # right_ankle
        ]
        
        # ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸
        analyzer = PoseAnalyzer()
        
        # ê´€ì ˆ ê°ë„ ê³„ì‚°
        joint_angles = analyzer.calculate_joint_angles(keypoints)
        print(f"âœ… ê´€ì ˆ ê°ë„ ê³„ì‚°: {len(joint_angles)}ê°œ")
        
        # ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°
        body_proportions = analyzer.calculate_body_proportions(keypoints)
        print(f"âœ… ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°: {len(body_proportions)}ê°œ")
        
        # í¬ì¦ˆ í’ˆì§ˆ í‰ê°€
        quality = analyzer.assess_pose_quality(keypoints, joint_angles, body_proportions)
        print(f"âœ… í¬ì¦ˆ í’ˆì§ˆ í‰ê°€: {quality['quality_grade'].value}")
        print(f"   ì „ì²´ ì ìˆ˜: {quality['overall_score']:.3f}")
        
        # ì˜ë¥˜ ì í•©ì„± ë¶„ì„
        clothing_analysis = analyze_pose_for_clothing(keypoints, "shirt")
        print(f"âœ… ì˜ë¥˜ ì í•©ì„±: {clothing_analysis['suitable_for_fitting']}")
        print(f"   ì ìˆ˜: {clothing_analysis['pose_score']:.3f}")
        
        # ì´ë¯¸ì§€ ê·¸ë¦¬ê¸° í…ŒìŠ¤íŠ¸
        test_image = Image.new('RGB', (256, 256), (128, 128, 128))
        pose_image = draw_pose_on_image(test_image, keypoints)
        print(f"âœ… í¬ì¦ˆ ì‹œê°í™”: {pose_image.size}")
        
        # í‚¤í¬ì¸íŠ¸ ìœ íš¨ì„± ê²€ì¦
        is_valid = validate_keypoints(keypoints)
        print(f"âœ… í‚¤í¬ì¸íŠ¸ ìœ íš¨ì„±: {is_valid}")
        
        # COCO 17 â†’ OpenPose 18 ë³€í™˜
        openpose_kpts = convert_coco17_to_openpose18(keypoints)
        print(f"âœ… COCOâ†’OpenPose ë³€í™˜: {len(openpose_kpts)}ê°œ")
        
    except Exception as e:
        print(f"âŒ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 9. ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'PoseEstimationStep',
    'MediaPoseModel',
    'YOLOv8PoseModel', 
    'OpenPoseModel',
    'HRNetModel',
    'PoseAnalyzer',
    
    # ë°ì´í„° êµ¬ì¡°
    'PoseResult',
    'PoseModel',
    'PoseQuality',
    
    # ìƒì„± í•¨ìˆ˜ë“¤
    'create_pose_estimation_step',
    'create_pose_estimation_step_sync',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'validate_keypoints',
    'draw_pose_on_image', 
    'analyze_pose_for_clothing',
    'convert_coco17_to_openpose18',
    
    # ìƒìˆ˜ë“¤
    'COCO_17_KEYPOINTS',
    'OPENPOSE_18_KEYPOINTS',
    'SKELETON_CONNECTIONS',
    'KEYPOINT_COLORS',
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
    'test_pose_estimation',
    'test_pose_algorithms'
]

# ==============================================
# ğŸ”¥ 10. ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê·¸
# ==============================================

logger.info("ğŸ”¥ Pose Estimation Step v7.0 - Central Hub DI Container ì™„ì „ ë¦¬íŒ©í† ë§ ì™„ë£Œ")
logger.info("âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
logger.info("âœ… BaseStepMixin ìƒì† íŒ¨í„´ (Human Parsing Stepê³¼ ë™ì¼)")
logger.info("âœ… MediaPipe Pose ëª¨ë¸ ì§€ì› (ìš°ì„ ìˆœìœ„ 1)")
logger.info("âœ… OpenPose ëª¨ë¸ ì§€ì› (í´ë°± ì˜µì…˜)")
logger.info("âœ… YOLOv8-Pose ëª¨ë¸ ì§€ì› (ì‹¤ì‹œê°„)")
logger.info("âœ… HRNet ëª¨ë¸ ì§€ì› (ê³ ì •ë°€)")
logger.info("âœ… 17ê°œ COCO keypoints ê°ì§€")
logger.info("âœ… confidence score ê³„ì‚°")
logger.info("âœ… Mock ëª¨ë¸ ì™„ì „ ì œê±°")
logger.info("âœ… ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰")
logger.info("âœ… ë‹¤ì¤‘ ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ")

logger.info("ğŸ§  ì§€ì› AI ëª¨ë¸ë“¤:")
logger.info("   - MediaPipe Pose (ìš°ì„ ìˆœìœ„ 1, ì‹¤ì‹œê°„)")
logger.info("   - YOLOv8-Pose (ì‹¤ì‹œê°„, 6.2MB)")
logger.info("   - OpenPose (ì •ë°€, PAF + íˆíŠ¸ë§µ)")
logger.info("   - HRNet (ê³ ì •ë°€, ì„œë¸Œí”½ì…€ ì •í™•ë„)")

logger.info("ğŸ¯ í•µì‹¬ ê¸°ëŠ¥ë“¤:")
logger.info("   - 17ê°œ COCO keypoints ì™„ì „ ê²€ì¶œ")
logger.info("   - ê´€ì ˆ ê°ë„ + ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°")
logger.info("   - í¬ì¦ˆ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ")
logger.info("   - ì˜ë¥˜ë³„ í¬ì¦ˆ ì í•©ì„± ë¶„ì„")
logger.info("   - ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±")
logger.info("   - ì„œë¸Œí”½ì…€ ì •í™•ë„ ì§€ì›")

logger.info(f"ğŸ“Š ì‹œìŠ¤í…œ: PyTorch={TORCH_AVAILABLE}, Device={DEVICE}")
logger.info(f"ğŸ¤– AI ë¼ì´ë¸ŒëŸ¬ë¦¬: YOLO={ULTRALYTICS_AVAILABLE}, MediaPipe={MEDIAPIPE_AVAILABLE}")
logger.info(f"ğŸ”§ ë¼ì´ë¸ŒëŸ¬ë¦¬: OpenCV={OPENCV_AVAILABLE}, Transformers={TRANSFORMERS_AVAILABLE}")
logger.info("ğŸš€ Production Ready - Central Hub DI Container v7.0!")

# ==============================================
# ğŸ”¥ 11. ë©”ì¸ ì‹¤í–‰ë¶€
# ==============================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¯ MyCloset AI Step 02 - Pose Estimation")
    print("ğŸ”¥ Central Hub DI Container v7.0 ì™„ì „ ë¦¬íŒ©í† ë§")
    print("=" * 80)
    
    async def run_all_tests():
        await test_pose_estimation()
        print("\n" + "=" * 80)
        test_pose_algorithms()
    
    try:
        asyncio.run(run_all_tests())
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 80)
    print("âœ¨ Pose Estimation Step í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("ğŸ”¥ Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
    print("ğŸ§  MediaPipe + YOLOv8 + OpenPose + HRNet í†µí•©")
    print("ğŸ¯ 17ê°œ COCO keypoints ì™„ì „ ê²€ì¶œ")
    print("âš¡ ì‹¤ì œ AI ì¶”ë¡  + ë‹¤ì¤‘ ëª¨ë¸ í´ë°±")
    print("ğŸ“Š ê´€ì ˆ ê°ë„ + ì‹ ì²´ ë¹„ìœ¨ + í¬ì¦ˆ í’ˆì§ˆ í‰ê°€")
    print("ğŸ’‰ ì™„ì „í•œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´")
    print("ğŸ”’ BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜")
    print("ğŸš€ Production Ready!")
    print("=" * 80)