# app/ai_pipeline/steps/step_02_pose_estimation.py
"""
2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • (Pose Estimation) - ì™„ì „ êµ¬í˜„ + ì‹œê°í™” ê¸°ëŠ¥ (ì˜µì…˜ A)
âœ… Model Loader ì™„ì „ ì—°ë™ (BaseStepMixin ìƒì†)
âœ… MediaPipe + YOLOv8 ë“€ì–¼ ëª¨ë¸ ì§€ì›
âœ… M3 Max ìµœì í™” ë° Neural Engine í™œìš©
âœ… ì™„ì „í•œ ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°± ë©”ì»¤ë‹ˆì¦˜
âœ… 18-keypoint OpenPose í˜¸í™˜ í¬ë§·
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±
âœ… ğŸ†• 18ê°œ í‚¤í¬ì¸íŠ¸ ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„± ê¸°ëŠ¥ ì¶”ê°€
"""

import os
import gc
import time
import asyncio
import json
import logging
import base64
from typing import Dict, Any, Optional, List, Tuple, Union, Callable
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
from io import BytesIO
import numpy as np
import cv2
from PIL import Image, ImageDraw, ImageFont

# PyTorch ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import torch
    import torch.nn as nn
    import torchvision.transforms as transforms
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

# MediaPipe (ì£¼ í¬ì¦ˆ ì¶”ì • ì—”ì§„)
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False
    mp = None

# YOLOv8 (ë°±ì—… í¬ì¦ˆ ì¶”ì • ì—”ì§„)
try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    YOLO = None

# Model Loader ì—°ë™
from ..utils.model_loader import BaseStepMixin, get_global_model_loader

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ¨ ì‹œê°í™” ê´€ë ¨ ìƒìˆ˜ ë° ì„¤ì •
# ==============================================

# OpenPose 18 í‚¤í¬ì¸íŠ¸ ì •ì˜
OPENPOSE_18_KEYPOINTS = {
    0: "nose",
    1: "neck", 
    2: "right_shoulder",
    3: "right_elbow",
    4: "right_wrist",
    5: "left_shoulder",
    6: "left_elbow", 
    7: "left_wrist",
    8: "mid_hip",
    9: "right_hip",
    10: "right_knee",
    11: "right_ankle",
    12: "left_hip",
    13: "left_knee",
    14: "left_ankle",
    15: "right_eye",
    16: "left_eye",
    17: "right_ear"
}

# ğŸ¨ í‚¤í¬ì¸íŠ¸ë³„ ìƒ‰ìƒ ì •ì˜
KEYPOINT_COLORS = {
    # ë¨¸ë¦¬ ë¶€ìœ„ - ë¹¨ê°• ê³„ì—´
    0: (255, 0, 0),      # nose - ë¹¨ê°•
    15: (255, 100, 100), # right_eye - ì—°ë¹¨ê°•
    16: (255, 150, 150), # left_eye - ë” ì—°ë¹¨ê°•
    17: (200, 0, 0),     # right_ear - ì–´ë‘ìš´ ë¹¨ê°•
    
    # ëª©ê³¼ ëª¸í†µ - ë…¸ë‘ ê³„ì—´
    1: (255, 255, 0),    # neck - ë…¸ë‘
    8: (255, 200, 0),    # mid_hip - ì£¼í™©ë…¸ë‘
    
    # ì˜¤ë¥¸ìª½ íŒ” - íŒŒë‘ ê³„ì—´
    2: (0, 0, 255),      # right_shoulder - íŒŒë‘
    3: (0, 100, 255),    # right_elbow - ì—°íŒŒë‘
    4: (0, 150, 255),    # right_wrist - ë” ì—°íŒŒë‘
    
    # ì™¼ìª½ íŒ” - ì´ˆë¡ ê³„ì—´
    5: (0, 255, 0),      # left_shoulder - ì´ˆë¡
    6: (100, 255, 100),  # left_elbow - ì—°ì´ˆë¡
    7: (150, 255, 150),  # left_wrist - ë” ì—°ì´ˆë¡
    
    # ì˜¤ë¥¸ìª½ ë‹¤ë¦¬ - ìì£¼ ê³„ì—´
    9: (255, 0, 255),    # right_hip - ìì£¼
    10: (200, 0, 200),   # right_knee - ì–´ë‘ìš´ ìì£¼
    11: (150, 0, 150),   # right_ankle - ë” ì–´ë‘ìš´ ìì£¼
    
    # ì™¼ìª½ ë‹¤ë¦¬ - ì²­ë¡ ê³„ì—´
    12: (0, 255, 255),   # left_hip - ì²­ë¡
    13: (0, 200, 200),   # left_knee - ì–´ë‘ìš´ ì²­ë¡
    14: (0, 150, 150),   # left_ankle - ë” ì–´ë‘ìš´ ì²­ë¡
}

# ğŸ”— ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²°ì„  ì •ì˜
SKELETON_CONNECTIONS = [
    # ë¨¸ë¦¬ ì—°ê²°
    (0, 1),   # nose -> neck
    (0, 15),  # nose -> right_eye
    (0, 16),  # nose -> left_eye
    (15, 17), # right_eye -> right_ear
    
    # ëª¸í†µ ì—°ê²°
    (1, 2),   # neck -> right_shoulder
    (1, 5),   # neck -> left_shoulder
    (1, 8),   # neck -> mid_hip
    (2, 8),   # right_shoulder -> mid_hip (ëª¸í†µ ë¼ì¸)
    (5, 8),   # left_shoulder -> mid_hip (ëª¸í†µ ë¼ì¸)
    
    # ì˜¤ë¥¸ìª½ íŒ”
    (2, 3),   # right_shoulder -> right_elbow
    (3, 4),   # right_elbow -> right_wrist
    
    # ì™¼ìª½ íŒ”
    (5, 6),   # left_shoulder -> left_elbow
    (6, 7),   # left_elbow -> left_wrist
    
    # ì—‰ë©ì´ ì—°ê²°
    (8, 9),   # mid_hip -> right_hip
    (8, 12),  # mid_hip -> left_hip
    
    # ì˜¤ë¥¸ìª½ ë‹¤ë¦¬
    (9, 10),  # right_hip -> right_knee
    (10, 11), # right_knee -> right_ankle
    
    # ì™¼ìª½ ë‹¤ë¦¬
    (12, 13), # left_hip -> left_knee
    (13, 14), # left_knee -> left_ankle
]

# ğŸ¨ ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²°ì„  ìƒ‰ìƒ
SKELETON_COLORS = {
    # ë¨¸ë¦¬ - ë¹¨ê°•
    (0, 1): (255, 0, 0),
    (0, 15): (255, 100, 100),
    (0, 16): (255, 100, 100),
    (15, 17): (200, 0, 0),
    
    # ëª¸í†µ - ë…¸ë‘
    (1, 2): (255, 255, 0),
    (1, 5): (255, 255, 0),
    (1, 8): (255, 200, 0),
    (2, 8): (255, 180, 0),
    (5, 8): (255, 180, 0),
    
    # ì˜¤ë¥¸ìª½ íŒ” - íŒŒë‘
    (2, 3): (0, 0, 255),
    (3, 4): (0, 100, 255),
    
    # ì™¼ìª½ íŒ” - ì´ˆë¡
    (5, 6): (0, 255, 0),
    (6, 7): (100, 255, 100),
    
    # ì—‰ë©ì´ - ì£¼í™©
    (8, 9): (255, 165, 0),
    (8, 12): (255, 165, 0),
    
    # ì˜¤ë¥¸ìª½ ë‹¤ë¦¬ - ìì£¼
    (9, 10): (255, 0, 255),
    (10, 11): (200, 0, 200),
    
    # ì™¼ìª½ ë‹¤ë¦¬ - ì²­ë¡
    (12, 13): (0, 255, 255),
    (13, 14): (0, 200, 200),
}

class PoseEstimationStep(BaseStepMixin):
    """
    ğŸƒ 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • - ì™„ì „ êµ¬í˜„ + ì‹œê°í™”
    âœ… Model Loader ì™„ì „ ì—°ë™
    âœ… MediaPipe + YOLOv8 ë“€ì–¼ ì—”ì§„
    âœ… M3 Max 128GB ìµœì í™”
    âœ… 18-keypoint OpenPose í‘œì¤€
    âœ… ğŸ†• 18ê°œ í‚¤í¬ì¸íŠ¸ ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """âœ… ì™„ì „ í†µí•© ìƒì„±ì - Model Loader ì—°ë™"""
        
        # === BaseStepMixin ì´ˆê¸°í™” (Model Loader ì—°ë™) ===
        self._setup_model_interface(kwargs.get('model_loader'))
        
        # === ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ===
        self.device = self._auto_detect_device(device)
        self.device_type = self._get_device_type()
        self.is_m3_max = self._check_m3_max()
        
        # === ì„¤ì • ì´ˆê¸°í™” ===
        self.config = self._setup_config(config)
        self.step_name = "PoseEstimationStep"
        self.logger = logging.getLogger(f"pipeline.{self.step_name}")
        
        # === ëª¨ë¸ ìƒíƒœ ===
        self.pose_model_primary = None      # MediaPipe ëª¨ë¸
        self.pose_model_secondary = None    # YOLOv8 ëª¨ë¸
        self.current_model_type = None
        self.is_initialized = False
        
        # === MediaPipe ì»´í¬ë„ŒíŠ¸ ===
        self.mp_pose = None
        self.mp_drawing = None
        self.mp_drawing_styles = None
        self.pose_detector = None
        
        # === ì„±ëŠ¥ ìµœì í™” ===
        self.model_cache = {}
        self.prediction_cache = {}
        self.processing_stats = {
            'total_processed': 0,
            'mediapipe_usage': 0,
            'yolo_usage': 0,
            'fallback_usage': 0,
            'avg_processing_time': 0.0,
            'cache_hits': 0
        }
        
        # === M3 Max ìµœì í™” ì„¤ì • ===
        if self.is_m3_max:
            self._setup_m3_max_optimization()
        
        # === ìŠ¤ë ˆë“œ í’€ ===
        self.executor = ThreadPoolExecutor(
            max_workers=min(4, os.cpu_count() or 4),
            thread_name_prefix="pose_estimation"
        )
        
        self.logger.info(f"ğŸƒ PoseEstimationStep ì´ˆê¸°í™” ì™„ë£Œ - Device: {self.device}")
        
    def _auto_detect_device(self, device: Optional[str]) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if device and device != "auto":
            return device
            
        if TORCH_AVAILABLE:
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
        return "cpu"
    
    def _get_device_type(self) -> str:
        """ë””ë°”ì´ìŠ¤ íƒ€ì… ë°˜í™˜"""
        if self.device == "mps":
            return "apple_silicon"
        elif self.device.startswith("cuda"):
            return "nvidia_gpu"
        else:
            return "cpu"
    
    def _check_m3_max(self) -> bool:
        """M3 Max ì¹© í™•ì¸"""
        try:
            if self.device_type == "apple_silicon":
                # M3 MaxëŠ” ì¼ë°˜ì ìœ¼ë¡œ 128GB ë©”ëª¨ë¦¬ë¥¼ ê°€ì§
                import psutil
                total_memory_gb = psutil.virtual_memory().total / (1024**3)
                return total_memory_gb > 100  # 100GB ì´ìƒì´ë©´ M3 Maxë¡œ ê°€ì •
            return False
        except:
            return False
    
    def _setup_config(self, config: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """ì„¤ì • ì´ˆê¸°í™”"""
        default_config = {
            # === MediaPipe ì„¤ì • ===
            "mediapipe": {
                "model_complexity": 2,  # 0, 1, 2 (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
                "min_detection_confidence": 0.7,
                "min_tracking_confidence": 0.5,
                "enable_segmentation": False,
                "smooth_landmarks": True,
                "static_image_mode": True
            },
            
            # === YOLOv8 ì„¤ì • ===
            "yolo": {
                "model_size": "n",  # n, s, m, l, x
                "confidence": 0.6,
                "iou": 0.5,
                "max_det": 1,  # í•œ ì‚¬ëŒë§Œ ê²€ì¶œ
                "device": self.device
            },
            
            # === ì²˜ë¦¬ ì„¤ì • ===
            "processing": {
                "max_image_size": 1024,
                "resize_method": "proportional",
                "normalize_input": True,
                "output_format": "openpose_18",
                "enable_face_keypoints": False,
                "enable_hand_keypoints": False
            },
            
            # === í’ˆì§ˆ ì„¤ì • ===
            "quality": {
                "min_keypoints_detected": 10,
                "min_pose_confidence": 0.5,
                "enable_pose_validation": True,
                "filter_low_confidence": True
            },
            
            # === ìºì‹± ì„¤ì • ===
            "cache": {
                "enable_prediction_cache": True,
                "cache_size": 100,
                "cache_ttl": 3600  # 1ì‹œê°„
            },
            
            # === M3 Max ìµœì í™” ===
            "m3_optimization": {
                "enable_neural_engine": True,
                "batch_size": 8,
                "memory_fraction": 0.8,
                "precision": "fp16"
            },
            
            # === ğŸ†• ì‹œê°í™” ì„¤ì • ===
            "visualization": {
                "enable_visualization": True,
                "keypoint_radius": 5,
                "skeleton_thickness": 3,
                "confidence_threshold": 0.5,
                "show_keypoint_labels": True,
                "show_confidence_values": True,
                "image_quality": "high",  # low, medium, high
                "overlay_opacity": 0.8,
                "background_color": (0, 0, 0),  # ê²€ì • ë°°ê²½
                "font_size": 12
            }
        }
        
        if config:
            # ë”¥ ì—…ë°ì´íŠ¸
            def deep_update(base_dict, update_dict):
                for key, value in update_dict.items():
                    if isinstance(value, dict) and key in base_dict:
                        deep_update(base_dict[key], value)
                    else:
                        base_dict[key] = value
            
            deep_update(default_config, config)
        
        return default_config
    
    def _setup_m3_max_optimization(self):
        """M3 Max ì „ìš© ìµœì í™” ì„¤ì •"""
        try:
            if TORCH_AVAILABLE and self.device == "mps":
                # MPS ë°±ì—”ë“œ ìµœì í™”
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
                
                # Neural Engine í™œì„±í™”
                if torch.backends.mps.is_available():
                    torch.backends.mps.is_built()
                    
            # ë©”ëª¨ë¦¬ ìµœì í™”
            os.environ['OMP_NUM_THREADS'] = '16'  # M3 Max 16ì½”ì–´
            
            self.logger.info("ğŸ M3 Max ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ M3 Max ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    async def initialize(self) -> bool:
        """
        âœ… ì™„ì „ ì´ˆê¸°í™” - Model Loader ì—°ë™
        ìš°ì„ ìˆœìœ„: MediaPipe > YOLOv8 > ë”ë¯¸
        """
        try:
            self.logger.info("ğŸš€ PoseEstimationStep ì´ˆê¸°í™” ì‹œì‘...")
            
            # === 1. MediaPipe ì´ˆê¸°í™” ì‹œë„ ===
            mediapipe_success = await self._initialize_mediapipe()
            
            # === 2. YOLOv8 ì´ˆê¸°í™” ì‹œë„ ===
            yolo_success = await self._initialize_yolo()
            
            # === 3. Model Loaderì—ì„œ ì¶”ê°€ ëª¨ë¸ ë¡œë“œ ì‹œë„ ===
            await self._initialize_from_model_loader()
            
            # === 4. ì´ˆê¸°í™” ê²°ê³¼ í‰ê°€ ===
            if mediapipe_success:
                self.current_model_type = "mediapipe"
                self.logger.info("âœ… MediaPipe í¬ì¦ˆ ëª¨ë¸ ë¡œë“œ ì„±ê³µ (Primary)")
            elif yolo_success:
                self.current_model_type = "yolo"
                self.logger.info("âœ… YOLOv8 í¬ì¦ˆ ëª¨ë¸ ë¡œë“œ ì„±ê³µ (Secondary)")
            else:
                self.current_model_type = "dummy"
                self.logger.warning("âš ï¸ ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ë”ë¯¸ ëª¨ë¸ë¡œ í´ë°±")
                await self._initialize_dummy_model()
            
            # === 5. ìºì‹œ ì´ˆê¸°í™” ===
            self.prediction_cache.clear()
            
            # === 6. M3 Max ì›Œë°ì—… ===
            if self.is_m3_max and self.current_model_type != "dummy":
                await self._warmup_models()
            
            self.is_initialized = True
            self.logger.info(f"âœ… PoseEstimationStep ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë¸: {self.current_model_type}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ PoseEstimationStep ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ì™„ì „ ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ëª¨ë¸ë¡œ í´ë°±
            await self._initialize_dummy_model()
            self.current_model_type = "dummy"
            self.is_initialized = True
            return False
    
    async def _initialize_mediapipe(self) -> bool:
        """MediaPipe í¬ì¦ˆ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if not MEDIAPIPE_AVAILABLE:
                self.logger.warning("MediaPipeê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                return False
            
            # MediaPipe ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
            self.mp_pose = mp.solutions.pose
            self.mp_drawing = mp.solutions.drawing_utils
            self.mp_drawing_styles = mp.solutions.drawing_styles
            
            # Model Loaderì—ì„œ MediaPipe ëª¨ë¸ ë¡œë“œ ì‹œë„
            pose_model = await self.get_model("pose_estimation_mediapipe")
            
            if pose_model is None:
                # ì§ì ‘ MediaPipe ì´ˆê¸°í™”
                mp_config = self.config["mediapipe"]
                self.pose_detector = self.mp_pose.Pose(
                    static_image_mode=mp_config["static_image_mode"],
                    model_complexity=mp_config["model_complexity"],
                    enable_segmentation=mp_config["enable_segmentation"],
                    smooth_landmarks=mp_config["smooth_landmarks"],
                    min_detection_confidence=mp_config["min_detection_confidence"],
                    min_tracking_confidence=mp_config["min_tracking_confidence"]
                )
            else:
                self.pose_detector = pose_model
            
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ê²€ì¦
            test_success = await self._test_mediapipe()
            
            if test_success:
                self.pose_model_primary = self.pose_detector
                self.logger.info("âœ… MediaPipe í¬ì¦ˆ ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
                return True
            else:
                self.logger.warning("âŒ MediaPipe í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _initialize_yolo(self) -> bool:
        """YOLOv8 í¬ì¦ˆ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if not YOLO_AVAILABLE:
                self.logger.warning("YOLOv8ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                return False
            
            # Model Loaderì—ì„œ YOLOv8 ëª¨ë¸ ë¡œë“œ ì‹œë„
            yolo_model = await self.get_model("pose_estimation_yolo")
            
            if yolo_model is None:
                # ì§ì ‘ YOLOv8 ì´ˆê¸°í™”
                model_size = self.config["yolo"]["model_size"]
                model_name = f"yolov8{model_size}-pose.pt"
                
                # ë¡œì»¬ íŒŒì¼ í™•ì¸
                local_path = Path(f"ai_models/checkpoints/step_02_pose_estimation/{model_name}")
                if local_path.exists():
                    self.pose_model_secondary = YOLO(str(local_path))
                else:
                    # ì˜¨ë¼ì¸ì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹œë„
                    self.pose_model_secondary = YOLO(model_name)
            else:
                self.pose_model_secondary = yolo_model
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            if hasattr(self.pose_model_secondary, 'to'):
                if self.device == "mps" and TORCH_AVAILABLE:
                    # MPS ì§€ì› í™•ì¸
                    try:
                        self.pose_model_secondary.to(self.device)
                    except:
                        self.pose_model_secondary.to("cpu")
                        self.logger.warning("YOLOv8 MPS ì§€ì› ì‹¤íŒ¨, CPUë¡œ í´ë°±")
                else:
                    self.pose_model_secondary.to(self.device)
            
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ê²€ì¦
            test_success = await self._test_yolo()
            
            if test_success:
                self.logger.info("âœ… YOLOv8 í¬ì¦ˆ ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
                return True
            else:
                self.logger.warning("âŒ YOLOv8 í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def _initialize_from_model_loader(self):
        """Model Loaderì—ì„œ ì¶”ê°€ ëª¨ë¸ ë¡œë“œ"""
        try:
            # ê¶Œì¥ ëª¨ë¸ ë¡œë“œ ì‹œë„
            recommended_model = await self.get_recommended_model()
            if recommended_model:
                self.logger.info("âœ… Model Loaderì—ì„œ ê¶Œì¥ í¬ì¦ˆ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                
                # ëª¨ë¸ íƒ€ì… ê°ì§€ ë° í• ë‹¹
                if hasattr(recommended_model, 'process'):  # MediaPipe ìŠ¤íƒ€ì¼
                    self.pose_model_primary = recommended_model
                elif hasattr(recommended_model, 'predict'):  # YOLO ìŠ¤íƒ€ì¼
                    self.pose_model_secondary = recommended_model
                    
        except Exception as e:
            self.logger.warning(f"Model Loader ì¶”ê°€ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def _initialize_dummy_model(self):
        """ë”ë¯¸ í¬ì¦ˆ ëª¨ë¸ ì´ˆê¸°í™” (í´ë°±)"""
        self.pose_detector = self._create_dummy_detector()
        self.logger.info("ğŸ”„ ë”ë¯¸ í¬ì¦ˆ ëª¨ë¸ë¡œ í´ë°± ì™„ë£Œ")
    
    def _create_dummy_detector(self):
        """ë”ë¯¸ í¬ì¦ˆ ê²€ì¶œê¸° ìƒì„±"""
        class DummyPoseDetector:
            def process(self, image):
                # ê¸°ë³¸ T-í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ë°˜í™˜
                dummy_landmarks = self._generate_dummy_landmarks(image.shape)
                return type('Result', (), {'pose_landmarks': dummy_landmarks})()
            
            def _generate_dummy_landmarks(self, image_shape):
                height, width = image_shape[:2]
                # 33ê°œ MediaPipe í‚¤í¬ì¸íŠ¸ ìƒì„±
                landmarks = []
                for i in range(33):
                    x = 0.5 + 0.1 * np.sin(i)  # ì¤‘ì•™ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì‚°
                    y = 0.3 + 0.4 * (i / 32)   # ìœ„ì—ì„œ ì•„ë˜ë¡œ
                    z = 0.0
                    landmarks.append(type('Landmark', (), {'x': x, 'y': y, 'z': z, 'visibility': 0.8})())
                
                return type('Landmarks', (), {'landmark': landmarks})()
        
        return DummyPoseDetector()
    
    async def _test_mediapipe(self) -> bool:
        """MediaPipe ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        try:
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
            test_image = np.zeros((480, 640, 3), dtype=np.uint8)
            test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)
            
            # í¬ì¦ˆ ê²€ì¶œ í…ŒìŠ¤íŠ¸
            results = self.pose_detector.process(test_image)
            return True  # ì—ëŸ¬ ì—†ì´ ì²˜ë¦¬ë˜ë©´ ì„±ê³µ
            
        except Exception as e:
            self.logger.error(f"MediaPipe í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    async def _test_yolo(self) -> bool:
        """YOLOv8 ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        try:
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
            test_image = np.zeros((480, 640, 3), dtype=np.uint8)
            
            # í¬ì¦ˆ ê²€ì¶œ í…ŒìŠ¤íŠ¸
            results = self.pose_model_secondary(test_image, verbose=False)
            return len(results) > 0  # ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì„±ê³µ
            
        except Exception as e:
            self.logger.error(f"YOLOv8 í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    async def _warmup_models(self):
        """M3 Max ëª¨ë¸ ì›Œë°ì—…"""
        try:
            self.logger.info("ğŸ”¥ M3 Max ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")
            
            # ì›Œë°ì—… ì´ë¯¸ì§€ ìƒì„±
            warmup_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
            
            # MediaPipe ì›Œë°ì—…
            if self.pose_model_primary:
                for _ in range(3):
                    _ = self.pose_model_primary.process(warmup_image)
            
            # YOLOv8 ì›Œë°ì—…
            if self.pose_model_secondary:
                for _ in range(3):
                    _ = self.pose_model_secondary(warmup_image, verbose=False)
            
            # MPS ìºì‹œ ì •ë¦¬
            if TORCH_AVAILABLE and self.device == "mps":
                torch.mps.empty_cache()
            
            self.logger.info("âœ… M3 Max ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    async def process(
        self,
        person_image: Union[str, np.ndarray, Image.Image],
        **kwargs
    ) -> Dict[str, Any]:
        """
        âœ… ë©”ì¸ í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬ + ì‹œê°í™”
        
        Args:
            person_image: ì…ë ¥ ì´ë¯¸ì§€
            **kwargs: ì¶”ê°€ ì˜µì…˜
                - force_model: str = None (ê°•ì œ ëª¨ë¸ ì§€ì •)
                - return_confidence: bool = True
                - return_analysis: bool = True
                - cache_result: bool = True
        
        Returns:
            Dict[str, Any]: í¬ì¦ˆ ì¶”ì • ê²°ê³¼ + ì‹œê°í™” ì´ë¯¸ì§€
        """
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        
        try:
            self.logger.info("ğŸƒ í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬ ì‹œì‘")
            
            # === 1. ìºì‹œ í™•ì¸ ===
            cache_key = self._generate_cache_key(person_image)
            if kwargs.get('cache_result', True) and cache_key in self.prediction_cache:
                self.processing_stats['cache_hits'] += 1
                cached_result = self.prediction_cache[cache_key].copy()
                cached_result['from_cache'] = True
                self.logger.info("ğŸ’¾ ìºì‹œì—ì„œ í¬ì¦ˆ ê²°ê³¼ ë°˜í™˜")
                return cached_result
            
            # === 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ===
            processed_image = await self._preprocess_image(person_image)
            if processed_image is None:
                return self._create_error_result("ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨")
            
            # === 3. ëª¨ë¸ ì„ íƒ ë° ì¶”ë¡  ===
            force_model = kwargs.get('force_model')
            pose_result = await self._perform_pose_estimation(
                processed_image, 
                force_model
            )
            
            # === 4. ê²°ê³¼ í›„ì²˜ë¦¬ + ì‹œê°í™” ===
            final_result = await self._postprocess_results(
                pose_result, 
                processed_image.shape,
                processed_image,  # ğŸ†• ì›ë³¸ ì´ë¯¸ì§€ ì „ë‹¬ (ì‹œê°í™”ìš©)
                **kwargs
            )
            
            # === 5. í’ˆì§ˆ í‰ê°€ ===
            if kwargs.get('return_analysis', True):
                final_result['pose_analysis'] = self._analyze_pose_quality(final_result)
            
            # === 6. ìºì‹± ===
            if kwargs.get('cache_result', True):
                self.prediction_cache[cache_key] = final_result.copy()
                # ìºì‹œ í¬ê¸° ì œí•œ
                if len(self.prediction_cache) > self.config['cache']['cache_size']:
                    oldest_key = next(iter(self.prediction_cache))
                    del self.prediction_cache[oldest_key]
            
            # === 7. í†µê³„ ì—…ë°ì´íŠ¸ ===
            processing_time = time.time() - start_time
            self._update_processing_stats(processing_time)
            
            final_result.update({
                'success': True,
                'message': "í¬ì¦ˆ ì¶”ì • ì™„ë£Œ",
                'processing_time': processing_time,
                'model_used': self.current_model_type,
                'device': self.device,
                'm3_max_optimized': self.is_m3_max,
                'from_cache': False
            })
            
            self.logger.info(f"âœ… í¬ì¦ˆ ì¶”ì • ì™„ë£Œ - {processing_time:.3f}ì´ˆ")
            return final_result
            
        except Exception as e:
            error_msg = f"í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
            self.logger.error(f"âŒ {error_msg}")
            return self._create_error_result(error_msg)
    
    async def _preprocess_image(
        self, 
        image_input: Union[str, np.ndarray, Image.Image]
    ) -> Optional[np.ndarray]:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            if isinstance(image_input, str):
                if os.path.exists(image_input):
                    image = cv2.imread(image_input)
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                else:
                    self.logger.error(f"ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {image_input}")
                    return None
            elif isinstance(image_input, Image.Image):
                image = np.array(image_input.convert('RGB'))
            elif isinstance(image_input, np.ndarray):
                image = image_input.copy()
                if len(image.shape) == 3 and image.shape[2] == 3:
                    # BGR to RGB ë³€í™˜ì´ í•„ìš”í•œì§€ í™•ì¸
                    if image.max() <= 1.0:  # ì •ê·œí™”ëœ ì´ë¯¸ì§€
                        image = (image * 255).astype(np.uint8)
            else:
                self.logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {type(image_input)}")
                return None
            
            # í¬ê¸° ì¡°ì •
            max_size = self.config['processing']['max_image_size']
            height, width = image.shape[:2]
            
            if max(height, width) > max_size:
                if height > width:
                    new_height = max_size
                    new_width = int(width * (max_size / height))
                else:
                    new_width = max_size
                    new_height = int(height * (max_size / width))
                
                image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)
            
            return image
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    async def _perform_pose_estimation(
        self, 
        image: np.ndarray, 
        force_model: Optional[str] = None
    ) -> Dict[str, Any]:
        """í¬ì¦ˆ ì¶”ì • ìˆ˜í–‰"""
        try:
            if force_model:
                model_type = force_model
            else:
                model_type = self.current_model_type
            
            if model_type == "mediapipe" and self.pose_model_primary:
                result = await self._estimate_pose_mediapipe(image)
                self.processing_stats['mediapipe_usage'] += 1
            elif model_type == "yolo" and self.pose_model_secondary:
                result = await self._estimate_pose_yolo(image)
                self.processing_stats['yolo_usage'] += 1
            else:
                # í´ë°±: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì‹œë„
                if self.pose_model_primary:
                    result = await self._estimate_pose_mediapipe(image)
                    self.processing_stats['mediapipe_usage'] += 1
                elif self.pose_model_secondary:
                    result = await self._estimate_pose_yolo(image)
                    self.processing_stats['yolo_usage'] += 1
                else:
                    result = await self._estimate_pose_dummy(image)
                    self.processing_stats['fallback_usage'] += 1
            
            return result
            
        except Exception as e:
            self.logger.error(f"í¬ì¦ˆ ì¶”ì • ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return await self._estimate_pose_dummy(image)
    
    async def _estimate_pose_mediapipe(self, image: np.ndarray) -> Dict[str, Any]:
        """MediaPipe í¬ì¦ˆ ì¶”ì •"""
        try:
            # MediaPipe ì²˜ë¦¬
            results = self.pose_model_primary.process(image)
            
            if results.pose_landmarks:
                # MediaPipe 33 keypointsë¥¼ OpenPose 18 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                keypoints_18 = self._convert_mediapipe_to_openpose18(
                    results.pose_landmarks, 
                    image.shape
                )
                
                # ì‹ ë¢°ë„ ê³„ì‚°
                confidence = self._calculate_mediapipe_confidence(results.pose_landmarks)
                
                return {
                    'keypoints_18': keypoints_18,
                    'raw_results': results,
                    'confidence': confidence,
                    'model_type': 'mediapipe',
                    'keypoints_detected': sum(1 for kp in keypoints_18 if kp[2] > 0.5)
                }
            else:
                return self._create_empty_pose_result('mediapipe')
                
        except Exception as e:
            self.logger.error(f"MediaPipe í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return self._create_empty_pose_result('mediapipe')
    
    async def _estimate_pose_yolo(self, image: np.ndarray) -> Dict[str, Any]:
        """YOLOv8 í¬ì¦ˆ ì¶”ì •"""
        try:
            # YOLOv8 ì²˜ë¦¬
            results = self.pose_model_secondary(image, verbose=False)
            
            if len(results) > 0 and results[0].keypoints is not None:
                # YOLOv8 17 keypointsë¥¼ OpenPose 18 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                yolo_keypoints = results[0].keypoints.xy[0].cpu().numpy()  # ì²« ë²ˆì§¸ ì‚¬ëŒ
                yolo_confidence = results[0].keypoints.conf[0].cpu().numpy()
                
                keypoints_18 = self._convert_yolo_to_openpose18(
                    yolo_keypoints, 
                    yolo_confidence, 
                    image.shape
                )
                
                # ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
                confidence = float(np.mean(yolo_confidence[yolo_confidence > 0]))
                
                return {
                    'keypoints_18': keypoints_18,
                    'raw_results': results,
                    'confidence': confidence,
                    'model_type': 'yolo',
                    'keypoints_detected': sum(1 for kp in keypoints_18 if kp[2] > 0.5)
                }
            else:
                return self._create_empty_pose_result('yolo')
                
        except Exception as e:
            self.logger.error(f"YOLOv8 í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return self._create_empty_pose_result('yolo')
    
    async def _estimate_pose_dummy(self, image: np.ndarray) -> Dict[str, Any]:
        """ë”ë¯¸ í¬ì¦ˆ ì¶”ì • (í´ë°±)"""
        try:
            height, width = image.shape[:2]
            
            # ê¸°ë³¸ T-í¬ì¦ˆ ìƒì„±
            keypoints_18 = self._generate_dummy_openpose18(width, height)
            
            return {
                'keypoints_18': keypoints_18,
                'raw_results': None,
                'confidence': 0.5,  # ì¤‘ê°„ ì‹ ë¢°ë„
                'model_type': 'dummy',
                'keypoints_detected': 18
            }
            
        except Exception as e:
            self.logger.error(f"ë”ë¯¸ í¬ì¦ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_empty_pose_result('dummy')
    
    def _convert_mediapipe_to_openpose18(
        self, 
        mediapipe_landmarks, 
        image_shape: Tuple[int, int, int]
    ) -> List[List[float]]:
        """MediaPipe 33 keypointsë¥¼ OpenPose 18 keypointsë¡œ ë³€í™˜"""
        height, width = image_shape[:2]
        
        # MediaPipe -> OpenPose ë§¤í•‘
        mp_to_op_mapping = {
            0: 0,    # nose
            11: 1,   # neck (left_shoulderì™€ right_shoulderì˜ ì¤‘ì ìœ¼ë¡œ ê³„ì‚°)
            12: 2,   # right_shoulder
            14: 3,   # right_elbow
            16: 4,   # right_wrist
            11: 5,   # left_shoulder
            13: 6,   # left_elbow
            15: 7,   # left_wrist
            24: 8,   # mid_hip (left_hipê³¼ right_hipì˜ ì¤‘ì ìœ¼ë¡œ ê³„ì‚°)
            26: 9,   # right_hip
            28: 10,  # right_knee
            32: 11,  # right_ankle
            23: 12,  # left_hip
            27: 13,  # left_knee
            31: 14,  # left_ankle
            2: 15,   # right_eye
            5: 16,   # left_eye
            4: 17,   # right_ear
        }
        
        keypoints_18 = [[0, 0, 0] for _ in range(18)]
        landmarks = mediapipe_landmarks.landmark
        
        for op_idx in range(18):
            if op_idx == 1:  # neck - shoulder ì¤‘ì 
                if len(landmarks) > 11 and len(landmarks) > 12:
                    x = (landmarks[11].x + landmarks[12].x) / 2 * width
                    y = (landmarks[11].y + landmarks[12].y) / 2 * height
                    conf = min(landmarks[11].visibility, landmarks[12].visibility)
                    keypoints_18[1] = [float(x), float(y), float(conf)]
            elif op_idx == 8:  # mid_hip - hip ì¤‘ì 
                if len(landmarks) > 23 and len(landmarks) > 24:
                    x = (landmarks[23].x + landmarks[24].x) / 2 * width
                    y = (landmarks[23].y + landmarks[24].y) / 2 * height
                    conf = min(landmarks[23].visibility, landmarks[24].visibility)
                    keypoints_18[8] = [float(x), float(y), float(conf)]
            else:
                # ì§ì ‘ ë§¤í•‘
                for mp_idx, mapped_op_idx in mp_to_op_mapping.items():
                    if mapped_op_idx == op_idx and mp_idx < len(landmarks):
                        landmark = landmarks[mp_idx]
                        x = landmark.x * width
                        y = landmark.y * height
                        conf = landmark.visibility
                        keypoints_18[op_idx] = [float(x), float(y), float(conf)]
                        break
        
        return keypoints_18
    
    def _convert_yolo_to_openpose18(
        self, 
        yolo_keypoints: np.ndarray, 
        yolo_confidence: np.ndarray,
        image_shape: Tuple[int, int, int]
    ) -> List[List[float]]:
        """YOLOv8 17 keypointsë¥¼ OpenPose 18 keypointsë¡œ ë³€í™˜"""
        
        # YOLO COCO 17 -> OpenPose 18 ë§¤í•‘
        yolo_to_op_mapping = {
            0: 0,    # nose
            5: 2,    # right_shoulder 
            6: 5,    # left_shoulder
            7: 3,    # right_elbow
            8: 6,    # left_elbow
            9: 4,    # right_wrist
            10: 7,   # left_wrist
            11: 9,   # right_hip
            12: 12,  # left_hip
            13: 10,  # right_knee
            14: 13,  # left_knee
            15: 11,  # right_ankle
            16: 14,  # left_ankle
            1: 15,   # right_eye
            2: 16,   # left_eye
            3: 17,   # right_ear
        }
        
        keypoints_18 = [[0, 0, 0] for _ in range(18)]
        
        for yolo_idx, op_idx in yolo_to_op_mapping.items():
            if yolo_idx < len(yolo_keypoints) and op_idx < 18:
                x, y = yolo_keypoints[yolo_idx]
                conf = yolo_confidence[yolo_idx] if yolo_idx < len(yolo_confidence) else 0.0
                keypoints_18[op_idx] = [float(x), float(y), float(conf)]
        
        # Neck (1)ê³¼ Mid-hip (8) ê³„ì‚°
        # Neck = (left_shoulder + right_shoulder) / 2
        if keypoints_18[2][2] > 0 and keypoints_18[5][2] > 0:  # ì–‘ìª½ ì–´ê¹¨ê°€ ìˆì„ ë•Œ
            neck_x = (keypoints_18[2][0] + keypoints_18[5][0]) / 2
            neck_y = (keypoints_18[2][1] + keypoints_18[5][1]) / 2
            neck_conf = min(keypoints_18[2][2], keypoints_18[5][2])
            keypoints_18[1] = [neck_x, neck_y, neck_conf]
        
        # Mid-hip = (left_hip + right_hip) / 2
        if keypoints_18[9][2] > 0 and keypoints_18[12][2] > 0:  # ì–‘ìª½ ì—‰ë©ì´ê°€ ìˆì„ ë•Œ
            mid_hip_x = (keypoints_18[9][0] + keypoints_18[12][0]) / 2
            mid_hip_y = (keypoints_18[9][1] + keypoints_18[12][1]) / 2
            mid_hip_conf = min(keypoints_18[9][2], keypoints_18[12][2])
            keypoints_18[8] = [mid_hip_x, mid_hip_y, mid_hip_conf]
        
        return keypoints_18
    
    def _generate_dummy_openpose18(self, width: int, height: int) -> List[List[float]]:
        """ë”ë¯¸ OpenPose 18 keypoints ìƒì„±"""
        # ê¸°ë³¸ T-í¬ì¦ˆ í˜•íƒœ
        center_x, center_y = width // 2, height // 2
        
        keypoints = [
            [center_x, center_y - height * 0.35, 0.8],      # 0: nose
            [center_x, center_y - height * 0.25, 0.8],      # 1: neck
            [center_x + width * 0.15, center_y - height * 0.2, 0.8],   # 2: right_shoulder
            [center_x + width * 0.25, center_y - height * 0.1, 0.8],   # 3: right_elbow
            [center_x + width * 0.35, center_y, 0.8],       # 4: right_wrist
            [center_x - width * 0.15, center_y - height * 0.2, 0.8],   # 5: left_shoulder
            [center_x - width * 0.25, center_y - height * 0.1, 0.8],   # 6: left_elbow
            [center_x - width * 0.35, center_y, 0.8],       # 7: left_wrist
            [center_x, center_y + height * 0.1, 0.8],       # 8: mid_hip
            [center_x + width * 0.1, center_y + height * 0.1, 0.8],    # 9: right_hip
            [center_x + width * 0.1, center_y + height * 0.25, 0.8],   # 10: right_knee
            [center_x + width * 0.1, center_y + height * 0.4, 0.8],    # 11: right_ankle
            [center_x - width * 0.1, center_y + height * 0.1, 0.8],    # 12: left_hip
            [center_x - width * 0.1, center_y + height * 0.25, 0.8],   # 13: left_knee
            [center_x - width * 0.1, center_y + height * 0.4, 0.8],    # 14: left_ankle
            [center_x + width * 0.05, center_y - height * 0.37, 0.8],  # 15: right_eye
            [center_x - width * 0.05, center_y - height * 0.37, 0.8],  # 16: left_eye
            [center_x + width * 0.08, center_y - height * 0.34, 0.8],  # 17: right_ear
        ]
        
        return keypoints
    
    def _calculate_mediapipe_confidence(self, landmarks) -> float:
        """MediaPipe ëœë“œë§ˆí¬ ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not landmarks or not landmarks.landmark:
            return 0.0
        
        confidences = [lm.visibility for lm in landmarks.landmark if lm.visibility > 0]
        return float(np.mean(confidences)) if confidences else 0.0
    
    def _create_empty_pose_result(self, model_type: str) -> Dict[str, Any]:
        """ë¹ˆ í¬ì¦ˆ ê²°ê³¼ ìƒì„±"""
        return {
            'keypoints_18': [[0, 0, 0] for _ in range(18)],
            'raw_results': None,
            'confidence': 0.0,
            'model_type': model_type,
            'keypoints_detected': 0
        }
    
    async def _postprocess_results(
        self, 
        pose_result: Dict[str, Any], 
        image_shape: Tuple[int, int, int],
        original_image: np.ndarray,  # ğŸ†• ì›ë³¸ ì´ë¯¸ì§€ ì¶”ê°€ (ì‹œê°í™”ìš©)
        **kwargs
    ) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬ + ì‹œê°í™”"""
        try:
            keypoints_18 = pose_result['keypoints_18']
            
            # í’ˆì§ˆ í•„í„°ë§
            if self.config['quality']['filter_low_confidence']:
                min_conf = self.config['quality']['min_pose_confidence']
                for kp in keypoints_18:
                    if kp[2] < min_conf:
                        kp[2] = 0.0  # ë‚®ì€ ì‹ ë¢°ë„ëŠ” 0ìœ¼ë¡œ ì„¤ì •
            
            # í‚¤í¬ì¸íŠ¸ ê²€ì¦
            detected_count = sum(1 for kp in keypoints_18 if kp[2] > 0.5)
            min_required = self.config['quality']['min_keypoints_detected']
            
            quality_passed = detected_count >= min_required
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
            bbox = self._calculate_pose_bbox(keypoints_18, image_shape)
            
            # í¬ì¦ˆ ê°ë„ ê³„ì‚°
            pose_angles = self._calculate_pose_angles(keypoints_18)
            
            # ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°
            body_proportions = self._calculate_body_proportions(keypoints_18)
            
            # ğŸ†• ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±
            visualization_results = await self._create_pose_visualization(
                keypoints_18, 
                original_image
            )
            
            return {
                'keypoints_18': keypoints_18,
                'pose_confidence': pose_result['confidence'],
                'keypoints_detected': detected_count,
                'quality_passed': quality_passed,
                'bbox': bbox,
                'pose_angles': pose_angles,
                'body_proportions': body_proportions,
                'raw_model_result': pose_result.get('raw_results'),
                'model_type': pose_result['model_type'],
                
                # ğŸ†• í”„ë¡ íŠ¸ì—”ë“œìš© ì‹œê°í™” ì´ë¯¸ì§€ë“¤
                'details': {
                    'result_image': visualization_results["keypoints_image"],  # ë©”ì¸ ì‹œê°í™”
                    'overlay_image': visualization_results["overlay_image"],   # ì˜¤ë²„ë ˆì´
                    'skeleton_image': visualization_results["skeleton_image"], # ìŠ¤ì¼ˆë ˆí†¤ë§Œ
                    
                    # ê¸°ì¡´ ë°ì´í„°ë“¤
                    'detected_keypoints': detected_count,
                    'total_keypoints': 18,
                    'keypoint_names': [OPENPOSE_18_KEYPOINTS[i] for i in range(18)],
                    'confidence_values': [kp[2] for kp in keypoints_18],
                    
                    # ìƒì„¸ ë¶„ì„ ì •ë³´
                    'pose_type': self._classify_pose_type(keypoints_18, pose_angles),
                    'symmetry_score': self._calculate_symmetry_score(keypoints_18),
                    'pose_quality': 'excellent' if detected_count >= 16 else 'good' if detected_count >= 12 else 'poor',
                    
                    # ì‹œìŠ¤í…œ ì •ë³´
                    'step_info': {
                        'step_name': 'pose_estimation',
                        'step_number': 2,
                        'model_used': pose_result['model_type'],
                        'device': self.device,
                        'optimization': 'M3 Max' if self.is_m3_max else self.device
                    },
                    
                    # í’ˆì§ˆ ë©”íŠ¸ë¦­
                    'quality_metrics': {
                        'detection_rate': float(detected_count / 18),
                        'avg_confidence': float(np.mean([kp[2] for kp in keypoints_18 if kp[2] > 0])) if detected_count > 0 else 0.0,
                        'pose_confidence': pose_result['confidence'],
                        'quality_passed': quality_passed
                    }
                }
            }
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return pose_result

    # ==============================================
    # ğŸ†• ì‹œê°í™” í•¨ìˆ˜ë“¤
    # ==============================================
    
    async def _create_pose_visualization(
        self, 
        keypoints_18: List[List[float]], 
        original_image: np.ndarray
    ) -> Dict[str, str]:
        """
        ğŸ†• 18ê°œ í‚¤í¬ì¸íŠ¸ë¥¼ ì‹œê°í™”í•œ ì´ë¯¸ì§€ë“¤ ìƒì„±
        
        Args:
            keypoints_18: OpenPose 18 í‚¤í¬ì¸íŠ¸ [x, y, confidence]
            original_image: ì›ë³¸ ì´ë¯¸ì§€ np.ndarray
            
        Returns:
            Dict[str, str]: base64 ì¸ì½”ë”©ëœ ì‹œê°í™” ì´ë¯¸ì§€ë“¤
        """
        try:
            if not self.config['visualization']['enable_visualization']:
                # ì‹œê°í™” ë¹„í™œì„±í™” ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜
                return {
                    "keypoints_image": "",
                    "overlay_image": "",
                    "skeleton_image": ""
                }
            
            def _create_visualizations():
                height, width = original_image.shape[:2]
                
                # 1. ğŸ¯ í‚¤í¬ì¸íŠ¸ë§Œ í‘œì‹œëœ ì´ë¯¸ì§€ ìƒì„±
                keypoints_image = self._create_keypoints_only_image(keypoints_18, (width, height))
                
                # 2. ğŸŒˆ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„± (ì›ë³¸ + í¬ì¦ˆ)
                overlay_image = self._create_overlay_pose_image(original_image, keypoints_18)
                
                # 3. ğŸ¦´ ìŠ¤ì¼ˆë ˆí†¤ë§Œ í‘œì‹œëœ ì´ë¯¸ì§€ ìƒì„±
                skeleton_image = self._create_skeleton_only_image(keypoints_18, (width, height))
                
                # base64 ì¸ì½”ë”©
                result = {
                    "keypoints_image": self._pil_to_base64(keypoints_image),
                    "overlay_image": self._pil_to_base64(overlay_image),
                    "skeleton_image": self._pil_to_base64(skeleton_image)
                }
                
                return result
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(self.executor, _create_visualizations)
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ë¹ˆ ê²°ê³¼ ë°˜í™˜
            return {
                "keypoints_image": "",
                "overlay_image": "",
                "skeleton_image": ""
            }
    
    def _create_keypoints_only_image(
        self, 
        keypoints_18: List[List[float]], 
        image_size: Tuple[int, int]
    ) -> Image.Image:
        """í‚¤í¬ì¸íŠ¸ë§Œ í‘œì‹œëœ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            width, height = image_size
            config = self.config['visualization']
            
            # ê²€ì • ë°°ê²½ ì´ë¯¸ì§€ ìƒì„±
            bg_color = config['background_color']
            image = Image.new('RGB', (width, height), bg_color)
            draw = ImageDraw.Draw(image)
            
            # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
            radius = config['keypoint_radius']
            threshold = config['confidence_threshold']
            
            for i, (x, y, conf) in enumerate(keypoints_18):
                if conf > threshold:
                    # í‚¤í¬ì¸íŠ¸ë³„ ìƒ‰ìƒ
                    color = KEYPOINT_COLORS.get(i, (255, 255, 255))
                    
                    # ì‹ ë¢°ë„ì— ë”°ë¥¸ íˆ¬ëª…ë„ ì¡°ì •
                    alpha = int(255 * conf)
                    if alpha > 255: alpha = 255
                    
                    # í‚¤í¬ì¸íŠ¸ ì› ê·¸ë¦¬ê¸°
                    draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                                fill=color, outline=(255, 255, 255), width=2)
                    
                    # ë¼ë²¨ í‘œì‹œ (ì˜µì…˜)
                    if config['show_keypoint_labels']:
                        keypoint_name = OPENPOSE_18_KEYPOINTS.get(i, f"kp_{i}")
                        try:
                            font = ImageFont.truetype("arial.ttf", config['font_size'])
                        except:
                            font = ImageFont.load_default()
                        
                        # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ê³„ì‚°
                        text_x = x + radius + 2
                        text_y = y - radius
                        draw.text((text_x, text_y), keypoint_name, fill=(255, 255, 255), font=font)
                    
                    # ì‹ ë¢°ë„ ê°’ í‘œì‹œ (ì˜µì…˜)
                    if config['show_confidence_values']:
                        conf_text = f"{conf:.2f}"
                        try:
                            small_font = ImageFont.truetype("arial.ttf", config['font_size'] - 2)
                        except:
                            small_font = ImageFont.load_default()
                        
                        draw.text((x, y + radius + 2), conf_text, fill=color, font=small_font)
            
            return image
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í‚¤í¬ì¸íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
            return Image.new('RGB', image_size, (50, 50, 50))
    
    def _create_overlay_pose_image(
        self, 
        original_image: np.ndarray, 
        keypoints_18: List[List[float]]
    ) -> Image.Image:
        """ì›ë³¸ ì´ë¯¸ì§€ ìœ„ì— í¬ì¦ˆë¥¼ ì˜¤ë²„ë ˆì´í•œ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # numpy ë°°ì—´ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            if original_image.max() <= 1.0:
                original_pil = Image.fromarray((original_image * 255).astype(np.uint8))
            else:
                original_pil = Image.fromarray(original_image.astype(np.uint8))
            
            # íˆ¬ëª…í•œ ì˜¤ë²„ë ˆì´ ë ˆì´ì–´ ìƒì„±
            overlay = Image.new('RGBA', original_pil.size, (0, 0, 0, 0))
            draw = ImageDraw.Draw(overlay)
            
            config = self.config['visualization']
            threshold = config['confidence_threshold']
            keypoint_radius = config['keypoint_radius']
            skeleton_thickness = config['skeleton_thickness']
            
            # 1. ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²°ì„  ê·¸ë¦¬ê¸°
            for start_idx, end_idx in SKELETON_CONNECTIONS:
                if (start_idx < len(keypoints_18) and end_idx < len(keypoints_18) and
                    keypoints_18[start_idx][2] > threshold and keypoints_18[end_idx][2] > threshold):
                    
                    start_point = (int(keypoints_18[start_idx][0]), int(keypoints_18[start_idx][1]))
                    end_point = (int(keypoints_18[end_idx][0]), int(keypoints_18[end_idx][1]))
                    
                    # ì—°ê²°ì„ ë³„ ìƒ‰ìƒ
                    line_color = SKELETON_COLORS.get((start_idx, end_idx), (255, 255, 255))
                    
                    # ì‹ ë¢°ë„ ê¸°ë°˜ íˆ¬ëª…ë„
                    avg_conf = (keypoints_18[start_idx][2] + keypoints_18[end_idx][2]) / 2
                    alpha = int(255 * avg_conf * config['overlay_opacity'])
                    line_color_alpha = line_color + (alpha,)
                    
                    draw.line([start_point, end_point], fill=line_color_alpha, width=skeleton_thickness)
            
            # 2. í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
            for i, (x, y, conf) in enumerate(keypoints_18):
                if conf > threshold:
                    color = KEYPOINT_COLORS.get(i, (255, 255, 255))
                    alpha = int(255 * conf * config['overlay_opacity'])
                    color_alpha = color + (alpha,)
                    
                    # í‚¤í¬ì¸íŠ¸ ì›
                    draw.ellipse([x-keypoint_radius, y-keypoint_radius, 
                                x+keypoint_radius, y+keypoint_radius], 
                               fill=color_alpha, outline=(255, 255, 255, alpha), width=2)
            
            # ì›ë³¸ ì´ë¯¸ì§€ì™€ ì˜¤ë²„ë ˆì´ í•©ì„±
            original_rgba = original_pil.convert('RGBA')
            combined = Image.alpha_composite(original_rgba, overlay)
            
            return combined.convert('RGB')
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ì›ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
            if original_image.max() <= 1.0:
                return Image.fromarray((original_image * 255).astype(np.uint8))
            else:
                return Image.fromarray(original_image.astype(np.uint8))
    
    def _create_skeleton_only_image(
        self, 
        keypoints_18: List[List[float]], 
        image_size: Tuple[int, int]
    ) -> Image.Image:
        """ìŠ¤ì¼ˆë ˆí†¤ë§Œ í‘œì‹œëœ ì´ë¯¸ì§€ ìƒì„±"""
        try:
            width, height = image_size
            config = self.config['visualization']
            
            # ê²€ì • ë°°ê²½ ì´ë¯¸ì§€ ìƒì„±
            bg_color = config['background_color']
            image = Image.new('RGB', (width, height), bg_color)
            draw = ImageDraw.Draw(image)
            
            threshold = config['confidence_threshold']
            thickness = config['skeleton_thickness']
            
            # ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²°ì„ ë§Œ ê·¸ë¦¬ê¸°
            for start_idx, end_idx in SKELETON_CONNECTIONS:
                if (start_idx < len(keypoints_18) and end_idx < len(keypoints_18) and
                    keypoints_18[start_idx][2] > threshold and keypoints_18[end_idx][2] > threshold):
                    
                    start_point = (int(keypoints_18[start_idx][0]), int(keypoints_18[start_idx][1]))
                    end_point = (int(keypoints_18[end_idx][0]), int(keypoints_18[end_idx][1]))
                    
                    # ì—°ê²°ì„ ë³„ ìƒ‰ìƒ
                    line_color = SKELETON_COLORS.get((start_idx, end_idx), (255, 255, 255))
                    
                    draw.line([start_point, end_point], fill=line_color, width=thickness)
            
            # ê´€ì ˆì ì„ ì‘ì€ ì›ìœ¼ë¡œ í‘œì‹œ
            for i, (x, y, conf) in enumerate(keypoints_18):
                if conf > threshold:
                    color = KEYPOINT_COLORS.get(i, (255, 255, 255))
                    small_radius = max(2, config['keypoint_radius'] // 2)
                    
                    draw.ellipse([x-small_radius, y-small_radius, 
                                x+small_radius, y+small_radius], 
                               fill=color, outline=(255, 255, 255), width=1)
            
            return image
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìŠ¤ì¼ˆë ˆí†¤ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
            return Image.new('RGB', image_size, (50, 50, 50))
    
    def _pil_to_base64(self, pil_image: Image.Image) -> str:
        """PIL ì´ë¯¸ì§€ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜"""
        try:
            buffer = BytesIO()
            
            # í’ˆì§ˆ ì„¤ì •
            quality = 85
            if self.config['visualization']['image_quality'] == "high":
                quality = 95
            elif self.config['visualization']['image_quality'] == "low":
                quality = 70
            
            pil_image.save(buffer, format='JPEG', quality=quality)
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ base64 ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""
    
    # ==============================================
    # ğŸ”§ ê¸°ì¡´ í•¨ìˆ˜ë“¤ (ë³€ê²½ ì—†ìŒ)
    # ==============================================
    
    def _calculate_pose_bbox(
        self, 
        keypoints_18: List[List[float]], 
        image_shape: Tuple[int, int, int]
    ) -> Dict[str, int]:
        """í¬ì¦ˆ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
        valid_points = [(kp[0], kp[1]) for kp in keypoints_18 if kp[2] > 0.5]
        
        if not valid_points:
            return {"x": 0, "y": 0, "width": 0, "height": 0}
        
        xs, ys = zip(*valid_points)
        x_min, x_max = min(xs), max(xs)
        y_min, y_max = min(ys), max(ys)
        
        # ì—¬ë°± ì¶”ê°€ (15%)
        margin_x = int((x_max - x_min) * 0.15)
        margin_y = int((y_max - y_min) * 0.15)
        
        height, width = image_shape[:2]
        
        return {
            "x": max(0, int(x_min - margin_x)),
            "y": max(0, int(y_min - margin_y)),
            "width": min(width, int(x_max - x_min + 2 * margin_x)),
            "height": min(height, int(y_max - y_min + 2 * margin_y))
        }
    
    def _calculate_pose_angles(self, keypoints_18: List[List[float]]) -> Dict[str, float]:
        """ì£¼ìš” ê´€ì ˆ ê°ë„ ê³„ì‚°"""
        def calculate_angle(p1, p2, p3):
            """ì„¸ ì ìœ¼ë¡œ ê°ë„ ê³„ì‚°"""
            if any(p[2] < 0.5 for p in [p1, p2, p3]):
                return 0.0
            
            v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
            v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
            
            cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-6)
            angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))
            return float(np.degrees(angle))
        
        try:
            angles = {}
            
            # ì˜¤ë¥¸ìª½ íŒ” ê°ë„ (ì–´ê¹¨-íŒ”ê¿ˆì¹˜-ì†ëª©)
            if all(keypoints_18[i][2] > 0.5 for i in [2, 3, 4]):
                angles['right_arm'] = calculate_angle(
                    keypoints_18[2], keypoints_18[3], keypoints_18[4]
                )
            
            # ì™¼ìª½ íŒ” ê°ë„
            if all(keypoints_18[i][2] > 0.5 for i in [5, 6, 7]):
                angles['left_arm'] = calculate_angle(
                    keypoints_18[5], keypoints_18[6], keypoints_18[7]
                )
            
            # ì˜¤ë¥¸ìª½ ë‹¤ë¦¬ ê°ë„ (ì—‰ë©ì´-ë¬´ë¦-ë°œëª©)
            if all(keypoints_18[i][2] > 0.5 for i in [9, 10, 11]):
                angles['right_leg'] = calculate_angle(
                    keypoints_18[9], keypoints_18[10], keypoints_18[11]
                )
            
            # ì™¼ìª½ ë‹¤ë¦¬ ê°ë„
            if all(keypoints_18[i][2] > 0.5 for i in [12, 13, 14]):
                angles['left_leg'] = calculate_angle(
                    keypoints_18[12], keypoints_18[13], keypoints_18[14]
                )
            
            # ëª© ê°ë„ (ëª©-ì–´ê¹¨ ì¤‘ì -ì—‰ë©ì´ ì¤‘ì )
            if all(keypoints_18[i][2] > 0.5 for i in [1, 2, 5, 8]):
                shoulder_center = [
                    (keypoints_18[2][0] + keypoints_18[5][0]) / 2,
                    (keypoints_18[2][1] + keypoints_18[5][1]) / 2,
                    1.0
                ]
                angles['torso'] = calculate_angle(
                    keypoints_18[1], shoulder_center, keypoints_18[8]
                )
            
            return angles
            
        except Exception as e:
            self.logger.warning(f"ê°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_body_proportions(self, keypoints_18: List[List[float]]) -> Dict[str, float]:
        """ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°"""
        try:
            proportions = {}
            
            # ì „ì²´ í‚¤ (ë¨¸ë¦¬-ë°œëª©)
            if keypoints_18[0][2] > 0.5 and any(keypoints_18[i][2] > 0.5 for i in [11, 14]):
                head_y = keypoints_18[0][1]
                ankle_y = min(
                    keypoints_18[11][1] if keypoints_18[11][2] > 0.5 else float('inf'),
                    keypoints_18[14][1] if keypoints_18[14][2] > 0.5 else float('inf')
                )
                if ankle_y != float('inf'):
                    total_height = abs(ankle_y - head_y)
                    proportions['total_height'] = total_height
            
            # ìƒì²´ ê¸¸ì´ (ëª©-ì—‰ë©ì´)
            if keypoints_18[1][2] > 0.5 and keypoints_18[8][2] > 0.5:
                torso_length = abs(keypoints_18[8][1] - keypoints_18[1][1])
                proportions['torso_length'] = torso_length
            
            # ì–´ê¹¨ ë„ˆë¹„
            if keypoints_18[2][2] > 0.5 and keypoints_18[5][2] > 0.5:
                shoulder_width = abs(keypoints_18[2][0] - keypoints_18[5][0])
                proportions['shoulder_width'] = shoulder_width
            
            # ì—‰ë©ì´ ë„ˆë¹„
            if keypoints_18[9][2] > 0.5 and keypoints_18[12][2] > 0.5:
                hip_width = abs(keypoints_18[9][0] - keypoints_18[12][0])
                proportions['hip_width'] = hip_width
            
            # íŒ” ê¸¸ì´ (ì–´ê¹¨-ì†ëª©)
            if keypoints_18[2][2] > 0.5 and keypoints_18[4][2] > 0.5:
                right_arm_length = np.sqrt(
                    (keypoints_18[4][0] - keypoints_18[2][0])**2 + 
                    (keypoints_18[4][1] - keypoints_18[2][1])**2
                )
                proportions['right_arm_length'] = right_arm_length
            
            if keypoints_18[5][2] > 0.5 and keypoints_18[7][2] > 0.5:
                left_arm_length = np.sqrt(
                    (keypoints_18[7][0] - keypoints_18[5][0])**2 + 
                    (keypoints_18[7][1] - keypoints_18[5][1])**2
                )
                proportions['left_arm_length'] = left_arm_length
            
            return proportions
            
        except Exception as e:
            self.logger.warning(f"ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _analyze_pose_quality(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„"""
        try:
            keypoints_18 = result['keypoints_18']
            detected_count = result['keypoints_detected']
            
            # 1. ê²€ì¶œë¥ 
            detection_rate = detected_count / 18
            
            # 2. ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê²€ì¶œë¥ 
            major_keypoints = [0, 1, 2, 5, 8, 9, 12]  # ë¨¸ë¦¬, ëª©, ì–´ê¹¨, ì—‰ë©ì´
            major_detected = sum(1 for idx in major_keypoints if keypoints_18[idx][2] > 0.5)
            major_detection_rate = major_detected / len(major_keypoints)
            
            # 3. í‰ê·  ì‹ ë¢°ë„
            confidences = [kp[2] for kp in keypoints_18 if kp[2] > 0]
            avg_confidence = np.mean(confidences) if confidences else 0.0
            
            # 4. ëŒ€ì¹­ì„± ì ìˆ˜
            symmetry_score = self._calculate_symmetry_score(keypoints_18)
            
            # 5. í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜
            pose_type = self._classify_pose_type(keypoints_18, result.get('pose_angles', {}))
            
            # 6. ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            quality_score = (
                detection_rate * 0.3 +
                major_detection_rate * 0.3 +
                avg_confidence * 0.2 +
                symmetry_score * 0.2
            )
            
            return {
                'detection_rate': float(detection_rate),
                'major_detection_rate': float(major_detection_rate),
                'avg_confidence': float(avg_confidence),
                'symmetry_score': float(symmetry_score),
                'pose_type': pose_type,
                'quality_score': float(quality_score),
                'quality_grade': self._get_quality_grade(quality_score)
            }
            
        except Exception as e:
            self.logger.warning(f"í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'detection_rate': 0.0,
                'major_detection_rate': 0.0,
                'avg_confidence': 0.0,
                'symmetry_score': 0.0,
                'pose_type': 'unknown',
                'quality_score': 0.0,
                'quality_grade': 'F'
            }
    
    def _calculate_symmetry_score(self, keypoints_18: List[List[float]]) -> float:
        """í¬ì¦ˆ ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            # ëŒ€ì¹­ í‚¤í¬ì¸íŠ¸ ìŒë“¤
            symmetric_pairs = [
                (2, 5),   # ì–´ê¹¨
                (3, 6),   # íŒ”ê¿ˆì¹˜
                (4, 7),   # ì†ëª©
                (9, 12),  # ì—‰ë©ì´
                (10, 13), # ë¬´ë¦
                (11, 14), # ë°œëª©
                (15, 16), # ëˆˆ
            ]
            
            symmetry_scores = []
            
            for left_idx, right_idx in symmetric_pairs:
                if keypoints_18[left_idx][2] > 0.5 and keypoints_18[right_idx][2] > 0.5:
                    # ì¤‘ì‹¬ì„  ê¸°ì¤€ìœ¼ë¡œ ëŒ€ì¹­ì„± ê³„ì‚°
                    center_x = (keypoints_18[left_idx][0] + keypoints_18[right_idx][0]) / 2
                    
                    left_dist = abs(keypoints_18[left_idx][0] - center_x)
                    right_dist = abs(keypoints_18[right_idx][0] - center_x)
                    
                    if left_dist + right_dist > 0:
                        symmetry = 1.0 - abs(left_dist - right_dist) / (left_dist + right_dist)
                        symmetry_scores.append(symmetry)
            
            return float(np.mean(symmetry_scores)) if symmetry_scores else 0.5
            
        except Exception as e:
            self.logger.warning(f"ëŒ€ì¹­ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _classify_pose_type(
        self, 
        keypoints_18: List[List[float]], 
        pose_angles: Dict[str, float]
    ) -> str:
        """í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜"""
        try:
            # íŒ” ê°ë„ ê¸°ë°˜ ë¶„ë¥˜
            right_arm = pose_angles.get('right_arm', 180)
            left_arm = pose_angles.get('left_arm', 180)
            
            # T-í¬ì¦ˆ (íŒ”ì´ ìˆ˜í‰)
            if abs(right_arm - 180) < 20 and abs(left_arm - 180) < 20:
                return 't_pose'
            
            # A-í¬ì¦ˆ (íŒ”ì´ ì•½ê°„ ì•„ë˜)
            elif 140 < right_arm < 170 and 140 < left_arm < 170:
                return 'a_pose'
            
            # íŒ” ì˜¬ë¦° í¬ì¦ˆ
            elif right_arm < 90 or left_arm < 90:
                return 'arms_up'
            
            # ë‹¤ë¦¬ ìƒíƒœ í™•ì¸
            right_leg = pose_angles.get('right_leg', 180)
            left_leg = pose_angles.get('left_leg', 180)
            
            # ì•‰ì€ í¬ì¦ˆ
            if right_leg < 140 or left_leg < 140:
                return 'sitting'
            
            # ê±·ê¸°/ë›°ê¸° (ë‹¤ë¦¬ ë¹„ëŒ€ì¹­)
            elif abs(right_leg - left_leg) > 30:
                return 'walking'
            
            # ê¸°ë³¸ ì„œìˆëŠ” í¬ì¦ˆ
            else:
                return 'standing'
                
        except Exception as e:
            self.logger.warning(f"í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return 'unknown'
    
    def _get_quality_grade(self, quality_score: float) -> str:
        """í’ˆì§ˆ ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if quality_score >= 0.9:
            return 'A+'
        elif quality_score >= 0.8:
            return 'A'
        elif quality_score >= 0.7:
            return 'B'
        elif quality_score >= 0.6:
            return 'C'
        elif quality_score >= 0.5:
            return 'D'
        else:
            return 'F'
    
    def _generate_cache_key(self, image_input: Union[str, np.ndarray, Image.Image]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            if isinstance(image_input, str):
                # íŒŒì¼ ê²½ë¡œì˜ í•´ì‹œ
                import hashlib
                return hashlib.md5(image_input.encode()).hexdigest()[:16]
            elif isinstance(image_input, (np.ndarray, Image.Image)):
                # ì´ë¯¸ì§€ ë°ì´í„°ì˜ í•´ì‹œ
                if isinstance(image_input, Image.Image):
                    image_array = np.array(image_input)
                else:
                    image_array = image_input
                
                # ì‘ì€ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆí•´ì„œ í•´ì‹œ ê³„ì‚°
                small_image = cv2.resize(image_array, (64, 64))
                image_hash = hash(small_image.tobytes())
                return f"img_{abs(image_hash) % (10**16):016d}"
            else:
                return f"unknown_{time.time():.6f}"
                
        except Exception as e:
            self.logger.warning(f"ìºì‹œ í‚¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"fallback_{time.time():.6f}"
    
    def _update_processing_stats(self, processing_time: float):
        """ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.processing_stats['total_processed'] += 1
            
            # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            total = self.processing_stats['total_processed']
            current_avg = self.processing_stats['avg_processing_time']
            new_avg = (current_avg * (total - 1) + processing_time) / total
            self.processing_stats['avg_processing_time'] = new_avg
            
        except Exception as e:
            self.logger.warning(f"í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'keypoints_18': [[0, 0, 0] for _ in range(18)],
            'pose_confidence': 0.0,
            'keypoints_detected': 0,
            'quality_passed': False,
            'bbox': {"x": 0, "y": 0, "width": 0, "height": 0},
            'pose_angles': {},
            'body_proportions': {},
            'pose_analysis': {
                'detection_rate': 0.0,
                'quality_score': 0.0,
                'quality_grade': 'F',
                'pose_type': 'unknown'
            },
            'details': {
                'result_image': "",  # ë¹ˆ ì‹œê°í™” ì´ë¯¸ì§€
                'overlay_image': "",
                'skeleton_image': "",
                'detected_keypoints': 0,
                'error': error_message,
                'step_info': {
                    'step_name': 'pose_estimation',
                    'step_number': 2,
                    'model_used': 'error',
                    'device': self.device,
                    'error': error_message
                },
                'quality_metrics': {
                    'detection_rate': 0.0,
                    'avg_confidence': 0.0,
                    'pose_confidence': 0.0,
                    'quality_passed': False
                }
            },
            'model_type': 'error',
            'processing_time': 0.0,
            'device': self.device,
            'm3_max_optimized': self.is_m3_max
        }
    
    async def get_processing_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        return {
            'step_name': self.step_name,
            'is_initialized': self.is_initialized,
            'current_model': self.current_model_type,
            'device': self.device,
            'device_type': self.device_type,
            'm3_max_optimized': self.is_m3_max,
            'processing_stats': self.processing_stats.copy(),
            'cache_size': len(self.prediction_cache),
            'models_loaded': {
                'mediapipe': self.pose_model_primary is not None,
                'yolo': self.pose_model_secondary is not None
            }
        }
    
    async def warmup(self, num_iterations: int = 3) -> Dict[str, Any]:
        """ëª¨ë¸ ì›Œë°ì—…"""
        try:
            self.logger.info(f"ğŸ”¥ {num_iterations}íšŒ ì›Œë°ì—… ì‹œì‘...")
            warmup_times = []
            
            for i in range(num_iterations):
                # ëœë¤ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
                test_image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
                
                start_time = time.time()
                result = await self.process(test_image, cache_result=False)
                warmup_time = time.time() - start_time
                warmup_times.append(warmup_time)
                
                self.logger.info(f"ì›Œë°ì—… {i+1}/{num_iterations}: {warmup_time:.3f}ì´ˆ")
            
            avg_warmup_time = np.mean(warmup_times)
            
            # MPS ìºì‹œ ì •ë¦¬
            if TORCH_AVAILABLE and self.device == "mps":
                torch.mps.empty_cache()
            
            self.logger.info(f"âœ… ì›Œë°ì—… ì™„ë£Œ - í‰ê·  ì‹œê°„: {avg_warmup_time:.3f}ì´ˆ")
            
            return {
                'success': True,
                'iterations': num_iterations,
                'avg_time': avg_warmup_time,
                'times': warmup_times,
                'model_type': self.current_model_type
            }
            
        except Exception as e:
            error_msg = f"ì›Œë°ì—… ì‹¤íŒ¨: {e}"
            self.logger.error(f"âŒ {error_msg}")
            return {'success': False, 'error': error_msg}
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ PoseEstimationStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
            
            # === ëª¨ë¸ ì •ë¦¬ ===
            if self.pose_model_primary and hasattr(self.pose_model_primary, 'close'):
                self.pose_model_primary.close()
            
            if self.pose_model_secondary:
                if hasattr(self.pose_model_secondary, 'cpu'):
                    self.pose_model_secondary.cpu()
                del self.pose_model_secondary
            
            # === MediaPipe ì •ë¦¬ ===
            if self.pose_detector and hasattr(self.pose_detector, 'close'):
                self.pose_detector.close()
            
            self.pose_model_primary = None
            self.pose_model_secondary = None
            self.pose_detector = None
            self.mp_pose = None
            self.mp_drawing = None
            self.mp_drawing_styles = None
            
            # === ìºì‹œ ì •ë¦¬ ===
            self.model_cache.clear()
            self.prediction_cache.clear()
            
            # === Model Loader ì •ë¦¬ ===
            self.cleanup_models()
            
            # === ìŠ¤ë ˆë“œ í’€ ì •ë¦¬ ===
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True)
            
            # === GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ===
            if TORCH_AVAILABLE:
                if self.device == "mps":
                    torch.mps.empty_cache()
                elif self.device.startswith("cuda"):
                    torch.cuda.empty_cache()
                
                # ì¼ë°˜ ì •ë¦¬
                gc.collect()
            
            self.is_initialized = False
            self.logger.info("âœ… PoseEstimationStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            asyncio.create_task(self.cleanup())
        except:
            pass

# =================================================================
# ğŸ”„ í•˜ìœ„ í˜¸í™˜ì„± ì§€ì› ë° íŒ©í† ë¦¬ í•¨ìˆ˜
# =================================================================

async def create_pose_estimation_step(
    device: str = "auto",
    config: Dict[str, Any] = None,
    **kwargs
) -> PoseEstimationStep:
    """
    ğŸ”„ ê¸°ì¡´ íŒ©í† ë¦¬ í•¨ìˆ˜ í˜¸í™˜ (ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ í˜¸í™˜)
    
    Args:
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ("auto"ëŠ” ìë™ ê°ì§€)
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜
        
    Returns:
        PoseEstimationStep: ì´ˆê¸°í™”ëœ 2ë‹¨ê³„ ìŠ¤í…
    """
    try:
        # ê¸°ì¡´ ë°©ì‹ í˜¸í™˜
        device_param = None if device == "auto" else device
        
        # ê¸°ë³¸ ì„¤ì •
        default_config = {
            "mediapipe": {
                "model_complexity": 2,
                "min_detection_confidence": 0.7,
                "min_tracking_confidence": 0.5,
                "enable_segmentation": False,
                "static_image_mode": True
            },
            "processing": {
                "max_image_size": 1024,
                "output_format": "openpose_18"
            },
            "quality": {
                "min_keypoints_detected": 10,
                "min_pose_confidence": 0.5
            },
            "visualization": {
                "enable_visualization": True,
                "keypoint_radius": 5,
                "skeleton_thickness": 3,
                "image_quality": "high"
            }
        }
        
        # ì„¤ì • ë³‘í•©
        final_config = {**default_config}
        if config:
            def deep_update(base_dict, update_dict):
                for key, value in update_dict.items():
                    if isinstance(value, dict) and key in base_dict:
                        deep_update(base_dict[key], value)
                    else:
                        base_dict[key] = value
            deep_update(final_config, config)
        
        # âœ… ìƒˆë¡œìš´ í†µì¼ëœ ìƒì„±ì ì‚¬ìš©
        step = PoseEstimationStep(device=device_param, config=final_config, **kwargs)
        
        # ì´ˆê¸°í™”
        init_success = await step.initialize()
        if not init_success:
            logger.warning("PoseEstimationStep ì´ˆê¸°í™”ì— ë¬¸ì œê°€ ìˆì—ˆì§€ë§Œ ì§„í–‰í•©ë‹ˆë‹¤.")
        
        return step
        
    except Exception as e:
        logger.error(f"âŒ PoseEstimationStep ìƒì„± ì‹¤íŒ¨: {e}")
        # ìµœì†Œí•œì˜ ë”ë¯¸ ìŠ¤í…ì´ë¼ë„ ë°˜í™˜
        step = PoseEstimationStep(device=device_param, config=final_config, **kwargs)
        await step._initialize_dummy_model()
        step.is_initialized = True
        return step

# =================================================================
# ğŸ”„ ê¸°ì¡´ í´ë˜ìŠ¤ëª… ë³„ì¹­ (ì™„ì „ í˜¸í™˜)
# =================================================================

PoseEstimationStepLegacy = PoseEstimationStep

# =================================================================
# ğŸ”¥ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ - ì¶”ê°€ ìœ í‹¸ë¦¬í‹°
# =================================================================

def validate_openpose_keypoints(keypoints_18: List[List[float]]) -> bool:
    """OpenPose 18 keypoints ìœ íš¨ì„± ê²€ì¦"""
    try:
        if len(keypoints_18) != 18:
            return False
        
        for kp in keypoints_18:
            if len(kp) != 3:
                return False
            if not all(isinstance(x, (int, float)) for x in kp):
                return False
            if kp[2] < 0 or kp[2] > 1:  # ì‹ ë¢°ë„ëŠ” 0~1 ì‚¬ì´
                return False
        
        return True
        
    except Exception:
        return False

def convert_keypoints_to_coco(keypoints_18: List[List[float]]) -> List[List[float]]:
    """OpenPose 18ì„ COCO 17 í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        # OpenPose 18 -> COCO 17 ë§¤í•‘
        op_to_coco_mapping = {
            0: 0,   # nose
            15: 1,  # right_eye -> left_eye (COCO ê´€ì )
            16: 2,  # left_eye -> right_eye
            17: 3,  # right_ear -> left_ear
            2: 5,   # right_shoulder -> left_shoulder
            5: 6,   # left_shoulder -> right_shoulder
            3: 7,   # right_elbow -> left_elbow
            6: 8,   # left_elbow -> right_elbow
            4: 9,   # right_wrist -> left_wrist
            7: 10,  # left_wrist -> right_wrist
            9: 11,  # right_hip -> left_hip
            12: 12, # left_hip -> right_hip
            10: 13, # right_knee -> left_knee
            13: 14, # left_knee -> right_knee
            11: 15, # right_ankle -> left_ankle
            14: 16, # left_ankle -> right_ankle
        }
        
        coco_keypoints = [[0, 0, 0] for _ in range(17)]
        
        for op_idx, coco_idx in op_to_coco_mapping.items():
            if op_idx < len(keypoints_18) and coco_idx < 17:
                coco_keypoints[coco_idx] = keypoints_18[op_idx].copy()
        
        return coco_keypoints
        
    except Exception as e:
        logger.error(f"COCO ë³€í™˜ ì‹¤íŒ¨: {e}")
        return [[0, 0, 0] for _ in range(17)]

def draw_pose_on_image(
    image: np.ndarray, 
    keypoints_18: List[List[float]],
    draw_skeleton: bool = True,
    draw_keypoints: bool = True,
    line_thickness: int = 2,
    keypoint_radius: int = 3
) -> np.ndarray:
    """ì´ë¯¸ì§€ì— í¬ì¦ˆ ê·¸ë¦¬ê¸° (OpenCV ê¸°ë°˜)"""
    try:
        result_image = image.copy()
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’
        threshold = 0.5
        
        # ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸°
        if draw_skeleton:
            for start_idx, end_idx in SKELETON_CONNECTIONS:
                if (start_idx < len(keypoints_18) and end_idx < len(keypoints_18) and
                    keypoints_18[start_idx][2] > threshold and keypoints_18[end_idx][2] > threshold):
                    
                    start_point = (int(keypoints_18[start_idx][0]), int(keypoints_18[start_idx][1]))
                    end_point = (int(keypoints_18[end_idx][0]), int(keypoints_18[end_idx][1]))
                    
                    # ì—°ê²°ì„ ë³„ ìƒ‰ìƒ (BGR í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
                    line_color = SKELETON_COLORS.get((start_idx, end_idx), (255, 255, 255))
                    line_color_bgr = (line_color[2], line_color[1], line_color[0])  # RGB to BGR
                    
                    cv2.line(result_image, start_point, end_point, line_color_bgr, line_thickness)
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        if draw_keypoints:
            for i, (x, y, conf) in enumerate(keypoints_18):
                if conf > threshold:
                    center = (int(x), int(y))
                    
                    # í‚¤í¬ì¸íŠ¸ë³„ ìƒ‰ìƒ (BGR í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
                    color = KEYPOINT_COLORS.get(i, (255, 255, 255))
                    color_bgr = (color[2], color[1], color[0])  # RGB to BGR
                    
                    cv2.circle(result_image, center, keypoint_radius, color_bgr, -1)
                    cv2.circle(result_image, center, keypoint_radius + 1, (255, 255, 255), 1)
        
        return result_image
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
        return image

def analyze_pose_for_clothing(keypoints_18: List[List[float]]) -> Dict[str, Any]:
    """ì˜ë¥˜ í”¼íŒ…ì„ ìœ„í•œ í¬ì¦ˆ ë¶„ì„"""
    try:
        analysis = {
            'suitable_for_fitting': False,
            'issues': [],
            'recommendations': [],
            'pose_score': 0.0
        }
        
        # 1. í•„ìˆ˜ í‚¤í¬ì¸íŠ¸ í™•ì¸
        essential_points = [0, 1, 2, 5, 8, 9, 12]  # ë¨¸ë¦¬, ëª©, ì–´ê¹¨, ì—‰ë©ì´
        essential_detected = sum(1 for idx in essential_points if keypoints_18[idx][2] > 0.5)
        
        if essential_detected < len(essential_points) * 0.8:
            analysis['issues'].append("í•„ìˆ˜ í‚¤í¬ì¸íŠ¸ ë¶€ì¡±")
            analysis['recommendations'].append("ì „ì‹ ì´ ì˜ ë³´ì´ëŠ” ì‚¬ì§„ì„ ì‚¬ìš©í•˜ì„¸ìš”")
        
        # 2. íŒ” ìœ„ì¹˜ ë¶„ì„
        arms_visible = (keypoints_18[2][2] > 0.5 and keypoints_18[3][2] > 0.5 and 
                       keypoints_18[5][2] > 0.5 and keypoints_18[6][2] > 0.5)
        
        if not arms_visible:
            analysis['issues'].append("íŒ”ì´ ì˜ ë³´ì´ì§€ ì•ŠìŒ")
            analysis['recommendations'].append("T-í¬ì¦ˆë‚˜ A-í¬ì¦ˆë¥¼ ì·¨í•´ì£¼ì„¸ìš”")
        
        # 3. ë‹¤ë¦¬ ìœ„ì¹˜ ë¶„ì„
        legs_visible = (keypoints_18[9][2] > 0.5 and keypoints_18[10][2] > 0.5 and 
                       keypoints_18[12][2] > 0.5 and keypoints_18[13][2] > 0.5)
        
        if not legs_visible:
            analysis['issues'].append("ë‹¤ë¦¬ê°€ ì˜ ë³´ì´ì§€ ì•ŠìŒ")
            analysis['recommendations'].append("ë‹¤ë¦¬ê°€ ë¶„ë¦¬ë˜ì–´ ë³´ì´ëŠ” ìì„¸ë¥¼ ì·¨í•´ì£¼ì„¸ìš”")
        
        # 4. ì •ë©´ ë°©í–¥ í™•ì¸ (ì–´ê¹¨ ëŒ€ì¹­ì„±)
        if keypoints_18[2][2] > 0.5 and keypoints_18[5][2] > 0.5:
            shoulder_diff = abs(keypoints_18[2][1] - keypoints_18[5][1])
            shoulder_width = abs(keypoints_18[2][0] - keypoints_18[5][0])
            
            if shoulder_width > 0 and shoulder_diff / shoulder_width > 0.3:
                analysis['issues'].append("ëª¸ì´ ê¸°ìš¸ì–´ì ¸ ìˆìŒ")
                analysis['recommendations'].append("ì¹´ë©”ë¼ë¥¼ ì •ë©´ìœ¼ë¡œ ë°”ë¼ë´ ì£¼ì„¸ìš”")
        
        # 5. ì „ì²´ ì ìˆ˜ ê³„ì‚°
        base_score = essential_detected / len(essential_points)
        arm_bonus = 0.2 if arms_visible else 0.0
        leg_bonus = 0.2 if legs_visible else 0.0
        
        analysis['pose_score'] = min(1.0, base_score + arm_bonus + leg_bonus)
        
        # 6. í”¼íŒ… ì í•©ì„± íŒë‹¨
        analysis['suitable_for_fitting'] = (
            len(analysis['issues']) <= 1 and 
            analysis['pose_score'] >= 0.7
        )
        
        if analysis['suitable_for_fitting']:
            analysis['recommendations'].append("í¬ì¦ˆê°€ ê°€ìƒ í”¼íŒ…ì— ì í•©í•©ë‹ˆë‹¤!")
        
        return analysis
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            'suitable_for_fitting': False,
            'issues': ["ë¶„ì„ ì‹¤íŒ¨"],
            'recommendations': ["ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”"],
            'pose_score': 0.0
        }

# =================================================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# =================================================================

__all__ = [
    'PoseEstimationStep',
    'create_pose_estimation_step', 
    'PoseEstimationStepLegacy',
    'validate_openpose_keypoints',
    'convert_keypoints_to_coco',
    'draw_pose_on_image',
    'analyze_pose_for_clothing',
    'OPENPOSE_18_KEYPOINTS',
    'KEYPOINT_COLORS',
    'SKELETON_CONNECTIONS',
    'SKELETON_COLORS'
]

# ëª¨ë“ˆ ì´ˆê¸°í™” ì‹œ ë¡œê¹…
logger.info("âœ… PoseEstimationStep ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - ì‹œê°í™” ê¸°ëŠ¥ í¬í•¨")