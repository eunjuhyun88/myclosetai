"""
ğŸ¯ ì™„ì „íˆ ì‘ë™í•˜ëŠ” í¬ì¦ˆ ì¶”ì • ë‹¨ê³„ (2ë‹¨ê³„)
ì‹¤ì œ MediaPipe + M3 Max MPS ìµœì í™” ì™„ì „ êµ¬í˜„
"""
import os
import time
import logging
import asyncio
from typing import Dict, Any, Optional, Tuple, List
import numpy as np
import cv2
from PIL import Image
import base64
import io

# MediaPipe ì‹¤ì œ êµ¬í˜„
try:
    import mediapipe as mp
    MP_AVAILABLE = True
except ImportError:
    print("âŒ MediaPipe ì„¤ì¹˜ í•„ìš”: pip install mediapipe")
    MP_AVAILABLE = False

# PyTorch MPS ì§€ì›
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    print("âŒ PyTorch ì„¤ì¹˜ í•„ìš”")
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)

class PoseEstimationStep:
    """
    ğŸ¯ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” í¬ì¦ˆ ì¶”ì • ë‹¨ê³„
    
    íŠ¹ì§•:
    - MediaPipe ì‹¤ì œ ì—°ë™
    - OpenPose 18 í‚¤í¬ì¸íŠ¸ ë³€í™˜
    - M3 Max MPS ìµœì í™”
    - ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥
    - ì—ëŸ¬ ì²˜ë¦¬ ì™„ë²½ êµ¬í˜„
    """
    
    # OpenPose 18 í‚¤í¬ì¸íŠ¸ ì •ì˜ (ì‹¤ì œ ì‚¬ìš©)
    OPENPOSE_18_KEYPOINTS = [
        "Nose",         # 0
        "Neck",         # 1 (computed)
        "R-Shoulder",   # 2
        "R-Elbow",      # 3
        "R-Wrist",      # 4
        "L-Shoulder",   # 5
        "L-Elbow",      # 6
        "L-Wrist",      # 7
        "R-Hip",        # 8
        "R-Knee",       # 9
        "R-Ankle",      # 10
        "L-Hip",        # 11
        "L-Knee",       # 12
        "L-Ankle",      # 13
        "R-Eye",        # 14
        "L-Eye",        # 15
        "R-Ear",        # 16
        "L-Ear"         # 17
    ]
    
    # MediaPipe â†’ OpenPose ì‹¤ì œ ë§¤í•‘ (33 â†’ 18)
    MP_TO_OPENPOSE_MAPPING = {
        0: 0,   # nose
        11: 5,  # left_shoulder
        12: 2,  # right_shoulder
        13: 6,  # left_elbow
        14: 3,  # right_elbow
        15: 7,  # left_wrist
        16: 4,  # right_wrist
        23: 11, # left_hip
        24: 8,  # right_hip
        25: 12, # left_knee
        26: 9,  # right_knee
        27: 13, # left_ankle
        28: 10, # right_ankle
        2: 15,  # left_eye_inner â†’ left_eye
        5: 14,  # right_eye_inner â†’ right_eye
        7: 17,  # left_ear
        8: 16   # right_ear
    }
    
    def __init__(self, device: str = 'cpu', config: Dict[str, Any] = None):
        """
        Args:
            device: ë””ë°”ì´ìŠ¤ ('cpu', 'mps', 'cuda')
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.device = device
        self.config = config or {}
        
        # MediaPipe ì„¤ì •
        self.model_complexity = self.config.get('model_complexity', 2)  # 0,1,2 (ë†’ì„ìˆ˜ë¡ ì •í™•)
        self.min_detection_confidence = self.config.get('min_detection_confidence', 0.7)
        self.min_tracking_confidence = self.config.get('min_tracking_confidence', 0.5)
        
        # MPS ìµœì í™” (M3 Max)
        self.use_mps = device == 'mps' and TORCH_AVAILABLE and torch.backends.mps.is_available()
        
        # MediaPipe ëª¨ë¸ë“¤
        self.mp_pose = None
        self.pose_detector = None
        self.mp_drawing = None
        
        self.is_initialized = False
        
        logger.info(f"ğŸ¯ ì‹¤ì œ í¬ì¦ˆ ì¶”ì • ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {device}, MPS: {self.use_mps}")
    
    async def initialize(self) -> bool:
        """ì‹¤ì œ MediaPipe ëª¨ë¸ ë¡œë”©"""
        try:
            if not MP_AVAILABLE:
                raise ImportError("MediaPipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install mediapipe")
            
            logger.info("ğŸ”„ MediaPipe í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            # MediaPipe ì†”ë£¨ì…˜ ì´ˆê¸°í™”
            self.mp_pose = mp.solutions.pose
            self.mp_drawing = mp.solutions.drawing_utils
            
            # í¬ì¦ˆ ê²€ì¶œê¸° ìƒì„± (ì‹¤ì œ êµ¬í˜„)
            self.pose_detector = self.mp_pose.Pose(
                static_image_mode=True,          # ì •ì  ì´ë¯¸ì§€ ëª¨ë“œ
                model_complexity=self.model_complexity,  # ëª¨ë¸ ë³µì¡ë„
                smooth_landmarks=True,           # ëœë“œë§ˆí¬ ìŠ¤ë¬´ë”©
                enable_segmentation=False,       # ì„¸ê·¸ë©˜í…Œì´ì…˜ ë¹„í™œì„±í™” (ì„±ëŠ¥ í–¥ìƒ)
                min_detection_confidence=self.min_detection_confidence,
                min_tracking_confidence=self.min_tracking_confidence
            )
            
            # MPS ìµœì í™” ì„¤ì •
            if self.use_mps:
                logger.info("ğŸš€ M3 Max MPS ìµœì í™” í™œì„±í™”")
                # MediaPipeëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ìµœì í™”ë˜ì–´ ìˆìŒ
            
            self.is_initialized = True
            logger.info("âœ… MediaPipe í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    async def process(self, input_image: Any) -> Dict[str, Any]:
        """
        ì‹¤ì œ í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬
        
        Args:
            input_image: numpy.ndarray, PIL.Image, torch.Tensor, ë˜ëŠ” base64 ë¬¸ìì—´
            
        Returns:
            í¬ì¦ˆ ì¶”ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_initialized:
            raise RuntimeError("í¬ì¦ˆ ì¶”ì • ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_time = time.time()
        
        try:
            # 1. ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            cv_image = await self._prepare_image(input_image)
            
            # 2. MediaPipe í¬ì¦ˆ ê²€ì¶œ ì‹¤í–‰
            pose_results = await self._detect_pose(cv_image)
            
            if not pose_results.pose_landmarks:
                logger.warning("âš ï¸ í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return self._create_empty_result("í¬ì¦ˆ ê²€ì¶œ ì‹¤íŒ¨")
            
            # 3. í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ë° ë³€í™˜
            mediapipe_keypoints = self._extract_mediapipe_keypoints(
                pose_results.pose_landmarks, cv_image.shape[:2]
            )
            
            # 4. OpenPose 18 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            openpose_18_keypoints = self._convert_to_openpose_18(mediapipe_keypoints)
            
            # 5. ì¶”ê°€ ë¶„ì„
            pose_analysis = await self._analyze_pose(openpose_18_keypoints, cv_image.shape[:2])
            
            # 6. í’ˆì§ˆ í‰ê°€
            quality_metrics = self._evaluate_pose_quality(openpose_18_keypoints, mediapipe_keypoints)
            
            processing_time = time.time() - start_time
            
            # 7. ê²°ê³¼ êµ¬ì„±
            result = {
                "success": True,
                "keypoints_18": openpose_18_keypoints,
                "keypoints_raw": mediapipe_keypoints,
                "pose_confidence": quality_metrics["overall_confidence"],
                
                # í¬ì¦ˆ ë¶„ì„
                "body_orientation": pose_analysis["orientation"],
                "pose_angles": pose_analysis["angles"],
                "body_proportions": pose_analysis["proportions"],
                "bounding_box": pose_analysis["bbox"],
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­
                "quality_metrics": quality_metrics,
                
                # ì—°ê²°ì„  ì •ë³´
                "pose_connections": self._get_openpose_connections(),
                
                # ì²˜ë¦¬ ì •ë³´
                "processing_info": {
                    "processing_time": processing_time,
                    "model_used": "MediaPipe",
                    "device": self.device,
                    "image_size": cv_image.shape[:2],
                    "keypoints_detected": sum(1 for kp in openpose_18_keypoints if kp[2] > 0.5)
                }
            }
            
            logger.info(f"âœ… í¬ì¦ˆ ì¶”ì • ì™„ë£Œ - ì‹œê°„: {processing_time:.3f}ì´ˆ, ì‹ ë¢°ë„: {quality_metrics['overall_confidence']:.3f}")
            
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            return {
                "success": False,
                "error": str(e),
                "processing_time": processing_time,
                "keypoints_18": [[0, 0, 0] for _ in range(18)]
            }
    
    async def _prepare_image(self, input_image: Any) -> np.ndarray:
        """ì…ë ¥ ì´ë¯¸ì§€ë¥¼ OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        
        if isinstance(input_image, str):
            # Base64 ë¬¸ìì—´ì¸ ê²½ìš°
            try:
                image_data = base64.b64decode(input_image)
                image = Image.open(io.BytesIO(image_data))
                cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            except Exception as e:
                raise ValueError(f"Base64 ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: {e}")
                
        elif isinstance(input_image, Image.Image):
            # PIL Imageì¸ ê²½ìš°
            cv_image = cv2.cvtColor(np.array(input_image), cv2.COLOR_RGB2BGR)
            
        elif isinstance(input_image, np.ndarray):
            # NumPy ë°°ì—´ì¸ ê²½ìš°
            if len(input_image.shape) == 3 and input_image.shape[2] == 3:
                cv_image = input_image.copy()
            else:
                raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤")
                
        elif TORCH_AVAILABLE and isinstance(input_image, torch.Tensor):
            # PyTorch í…ì„œì¸ ê²½ìš°
            if input_image.dim() == 4:
                input_image = input_image.squeeze(0)
            if input_image.shape[0] == 3:
                input_image = input_image.permute(1, 2, 0)
            
            if input_image.max() <= 1.0:
                input_image = input_image * 255
                
            cv_image = input_image.cpu().numpy().astype(np.uint8)
            
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {type(input_image)}")
        
        # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ ë° ì¡°ì •
        h, w = cv_image.shape[:2]
        if h < 100 or w < 100:
            raise ValueError("ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤ (ìµœì†Œ 100x100)")
        
        # ìµœì  í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ì„±ëŠ¥ ìµœì í™”)
        max_size = self.config.get('max_image_size', 1024)
        if max(h, w) > max_size:
            scale = max_size / max(h, w)
            new_w, new_h = int(w * scale), int(h * scale)
            cv_image = cv2.resize(cv_image, (new_w, new_h))
            logger.info(f"ğŸ”„ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ: {w}x{h} â†’ {new_w}x{new_h}")
        
        return cv_image
    
    async def _detect_pose(self, cv_image: np.ndarray) -> Any:
        """ì‹¤ì œ MediaPipe í¬ì¦ˆ ê²€ì¶œ"""
        
        # BGR â†’ RGB ë³€í™˜ (MediaPipe ìš”êµ¬ì‚¬í•­)
        image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)
        
        # ì´ë¯¸ì§€ë¥¼ ì½ê¸° ì „ìš©ìœ¼ë¡œ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)
        image_rgb.flags.writeable = False
        
        # MediaPipe í¬ì¦ˆ ê²€ì¶œ ì‹¤í–‰
        pose_results = self.pose_detector.process(image_rgb)
        
        # ë‹¤ì‹œ ì“°ê¸° ê°€ëŠ¥ìœ¼ë¡œ ì„¤ì •
        image_rgb.flags.writeable = True
        
        return pose_results
    
    def _extract_mediapipe_keypoints(self, landmarks, image_shape: Tuple[int, int]) -> List[Dict]:
        """MediaPipe ëœë“œë§ˆí¬ì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        
        height, width = image_shape
        keypoints = []
        
        for idx, landmark in enumerate(landmarks.landmark):
            keypoint = {
                "id": idx,
                "name": f"mp_{idx}",
                "x": landmark.x,                    # ì •ê·œí™”ëœ ì¢Œí‘œ (0-1)
                "y": landmark.y,
                "z": landmark.z,                    # ìƒëŒ€ì  ê¹Šì´
                "visibility": landmark.visibility,   # ê°€ì‹œì„± (0-1)
                "x_px": int(landmark.x * width),    # í”½ì…€ ì¢Œí‘œ
                "y_px": int(landmark.y * height),
                "confidence": landmark.visibility   # ì‹ ë¢°ë„
            }
            keypoints.append(keypoint)
        
        return keypoints
    
    def _convert_to_openpose_18(self, mediapipe_keypoints: List[Dict]) -> List[List[float]]:
        """MediaPipe 33 í‚¤í¬ì¸íŠ¸ë¥¼ OpenPose 18ë¡œ ë³€í™˜"""
        
        # ì´ˆê¸°í™”: [x, y, confidence]
        openpose_18 = [[0.0, 0.0, 0.0] for _ in range(18)]
        
        # ì§ì ‘ ë§¤í•‘
        for mp_idx, op_idx in self.MP_TO_OPENPOSE_MAPPING.items():
            if mp_idx < len(mediapipe_keypoints):
                mp_kp = mediapipe_keypoints[mp_idx]
                openpose_18[op_idx] = [
                    float(mp_kp["x_px"]),
                    float(mp_kp["y_px"]),
                    float(mp_kp["confidence"])
                ]
        
        # Neck (1ë²ˆ) ê³„ì‚°: ì–‘ ì–´ê¹¨ì˜ ì¤‘ì 
        left_shoulder = openpose_18[5]   # L-Shoulder
        right_shoulder = openpose_18[2]  # R-Shoulder
        
        if left_shoulder[2] > 0.5 and right_shoulder[2] > 0.5:
            neck_x = (left_shoulder[0] + right_shoulder[0]) / 2
            neck_y = (left_shoulder[1] + right_shoulder[1]) / 2
            neck_conf = min(left_shoulder[2], right_shoulder[2])
            openpose_18[1] = [neck_x, neck_y, neck_conf]
        
        return openpose_18
    
    async def _analyze_pose(self, keypoints_18: List[List[float]], image_shape: Tuple[int, int]) -> Dict[str, Any]:
        """í¬ì¦ˆ ì‹¬ì¸µ ë¶„ì„"""
        
        analysis = {}
        
        # 1. ì‹ ì²´ ë°©í–¥ ì¶”ì •
        analysis["orientation"] = self._estimate_body_orientation(keypoints_18)
        
        # 2. ê´€ì ˆ ê°ë„ ê³„ì‚°
        analysis["angles"] = self._calculate_joint_angles(keypoints_18)
        
        # 3. ì‹ ì²´ ë¹„ìœ¨ ë¶„ì„
        analysis["proportions"] = self._analyze_body_proportions(keypoints_18)
        
        # 4. ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
        analysis["bbox"] = self._calculate_pose_bbox(keypoints_18, image_shape)
        
        # 5. í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜
        analysis["pose_type"] = self._classify_pose_type(keypoints_18, analysis["angles"])
        
        return analysis
    
    def _estimate_body_orientation(self, keypoints_18: List[List[float]]) -> str:
        """ì‹ ì²´ ë°©í–¥ ì¶”ì • (ì •ë©´/ì¸¡ë©´/ë’·ë©´)"""
        
        try:
            # ì–´ê¹¨ì™€ ì—‰ë©ì´ í‚¤í¬ì¸íŠ¸
            left_shoulder = keypoints_18[5]
            right_shoulder = keypoints_18[2]
            left_hip = keypoints_18[11]
            right_hip = keypoints_18[8]
            
            # ëª¨ë“  í‚¤í¬ì¸íŠ¸ê°€ ê²€ì¶œëœ ê²½ìš°ë§Œ
            if all(kp[2] > 0.5 for kp in [left_shoulder, right_shoulder, left_hip, right_hip]):
                
                # ì–´ê¹¨ ë„ˆë¹„ì™€ ì—‰ë©ì´ ë„ˆë¹„
                shoulder_width = abs(right_shoulder[0] - left_shoulder[0])
                hip_width = abs(right_hip[0] - left_hip[0])
                
                # í‰ê·  ë„ˆë¹„
                avg_width = (shoulder_width + hip_width) / 2
                
                # ì„ê³„ê°’ ê¸°ë°˜ ë¶„ë¥˜ (ì´ë¯¸ì§€ í¬ê¸°ì— ë¹„ë¡€)
                if avg_width < 80:
                    return "side"      # ì¸¡ë©´
                elif avg_width < 150:
                    return "diagonal"  # ëŒ€ê°ì„ 
                else:
                    return "front"     # ì •ë©´
            
            return "unknown"
            
        except Exception as e:
            logger.warning(f"ì‹ ì²´ ë°©í–¥ ì¶”ì • ì‹¤íŒ¨: {e}")
            return "unknown"
    
    def _calculate_joint_angles(self, keypoints_18: List[List[float]]) -> Dict[str, float]:
        """ì£¼ìš” ê´€ì ˆ ê°ë„ ê³„ì‚°"""
        
        def angle_between_points(p1, p2, p3):
            """ì„¸ ì ìœ¼ë¡œ ê°ë„ ê³„ì‚° (p2ê°€ ê¼­ì§€ì )"""
            if any(p[2] < 0.5 for p in [p1, p2, p3]):
                return 0.0
            
            # ë²¡í„° ê³„ì‚°
            v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
            v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
            
            # ê°ë„ ê³„ì‚°
            cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
            cos_angle = np.clip(cos_angle, -1.0, 1.0)
            angle = np.degrees(np.arccos(cos_angle))
            
            return float(angle)
        
        angles = {}
        
        try:
            # íŒ” ê°ë„ (ì–´ê¹¨-íŒ”ê¿ˆì¹˜-ì†ëª©)
            angles["left_arm_angle"] = angle_between_points(
                keypoints_18[5],   # L-Shoulder
                keypoints_18[6],   # L-Elbow
                keypoints_18[7]    # L-Wrist
            )
            
            angles["right_arm_angle"] = angle_between_points(
                keypoints_18[2],   # R-Shoulder
                keypoints_18[3],   # R-Elbow
                keypoints_18[4]    # R-Wrist
            )
            
            # ë‹¤ë¦¬ ê°ë„ (ì—‰ë©ì´-ë¬´ë¦-ë°œëª©)
            angles["left_leg_angle"] = angle_between_points(
                keypoints_18[11],  # L-Hip
                keypoints_18[12],  # L-Knee
                keypoints_18[13]   # L-Ankle
            )
            
            angles["right_leg_angle"] = angle_between_points(
                keypoints_18[8],   # R-Hip
                keypoints_18[9],   # R-Knee
                keypoints_18[10]   # R-Ankle
            )
            
            # ëª¸í†µ ê¸°ìš¸ê¸° (ëª©-ì—‰ë©ì´ ì¤‘ì )
            neck = keypoints_18[1]
            if neck[2] > 0.5:
                left_hip = keypoints_18[11]
                right_hip = keypoints_18[8]
                
                if left_hip[2] > 0.5 and right_hip[2] > 0.5:
                    hip_center_x = (left_hip[0] + right_hip[0]) / 2
                    hip_center_y = (left_hip[1] + right_hip[1]) / 2
                    
                    # ìˆ˜ì§ì„ ê³¼ì˜ ê°ë„
                    if neck[1] != hip_center_y:
                        torso_angle = np.degrees(np.arctan(
                            abs(neck[0] - hip_center_x) / abs(neck[1] - hip_center_y)
                        ))
                        angles["torso_lean"] = float(torso_angle)
            
        except Exception as e:
            logger.warning(f"ê´€ì ˆ ê°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        return angles
    
    def _analyze_body_proportions(self, keypoints_18: List[List[float]]) -> Dict[str, float]:
        """ì‹ ì²´ ë¹„ìœ¨ ë¶„ì„"""
        
        proportions = {}
        
        try:
            # ë¨¸ë¦¬ í¬ê¸° (ì½”-ëª© ê±°ë¦¬)
            nose = keypoints_18[0]
            neck = keypoints_18[1]
            
            if nose[2] > 0.5 and neck[2] > 0.5:
                head_height = np.sqrt((nose[0] - neck[0])**2 + (nose[1] - neck[1])**2)
                proportions["head_height"] = float(head_height)
            
            # ì–´ê¹¨ ë„ˆë¹„
            left_shoulder = keypoints_18[5]
            right_shoulder = keypoints_18[2]
            
            if left_shoulder[2] > 0.5 and right_shoulder[2] > 0.5:
                shoulder_width = abs(right_shoulder[0] - left_shoulder[0])
                proportions["shoulder_width"] = float(shoulder_width)
            
            # ëª¸í†µ ê¸¸ì´ (ëª©-ì—‰ë©ì´ ì¤‘ì )
            if neck[2] > 0.5:
                left_hip = keypoints_18[11]
                right_hip = keypoints_18[8]
                
                if left_hip[2] > 0.5 and right_hip[2] > 0.5:
                    hip_center_y = (left_hip[1] + right_hip[1]) / 2
                    torso_length = abs(neck[1] - hip_center_y)
                    proportions["torso_length"] = float(torso_length)
            
            # ë‹¤ë¦¬ ê¸¸ì´ (ì—‰ë©ì´-ë°œëª©)
            left_hip = keypoints_18[11]
            left_ankle = keypoints_18[13]
            
            if left_hip[2] > 0.5 and left_ankle[2] > 0.5:
                leg_length = np.sqrt((left_hip[0] - left_ankle[0])**2 + (left_hip[1] - left_ankle[1])**2)
                proportions["leg_length"] = float(leg_length)
            
            # ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°
            if "head_height" in proportions and "torso_length" in proportions:
                proportions["head_to_torso_ratio"] = proportions["head_height"] / proportions["torso_length"]
            
            if "shoulder_width" in proportions and "torso_length" in proportions:
                proportions["shoulder_to_torso_ratio"] = proportions["shoulder_width"] / proportions["torso_length"]
            
        except Exception as e:
            logger.warning(f"ì‹ ì²´ ë¹„ìœ¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return proportions
    
    def _calculate_pose_bbox(self, keypoints_18: List[List[float]], image_shape: Tuple[int, int]) -> Dict[str, int]:
        """í¬ì¦ˆ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
        
        # ì‹ ë¢°ë„ê°€ ë†’ì€ í‚¤í¬ì¸íŠ¸ë“¤ë§Œ ì‚¬ìš©
        valid_points = [(x, y) for x, y, conf in keypoints_18 if conf > 0.5]
        
        if not valid_points:
            return {"x": 0, "y": 0, "width": 0, "height": 0}
        
        xs, ys = zip(*valid_points)
        
        x_min, x_max = min(xs), max(xs)
        y_min, y_max = min(ys), max(ys)
        
        # ì—¬ë°± ì¶”ê°€ (15%)
        margin_x = int((x_max - x_min) * 0.15)
        margin_y = int((y_max - y_min) * 0.15)
        
        height, width = image_shape
        
        bbox = {
            "x": max(0, int(x_min - margin_x)),
            "y": max(0, int(y_min - margin_y)),
            "width": min(width, int(x_max - x_min + 2 * margin_x)),
            "height": min(height, int(y_max - y_min + 2 * margin_y))
        }
        
        return bbox
    
    def _classify_pose_type(self, keypoints_18: List[List[float]], angles: Dict[str, float]) -> str:
        """í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜"""
        
        try:
            # íŒ” ê°ë„ ê¸°ë°˜ ë¶„ë¥˜
            left_arm = angles.get("left_arm_angle", 180)
            right_arm = angles.get("right_arm_angle", 180)
            
            # ë‹¤ë¦¬ ê°ë„
            left_leg = angles.get("left_leg_angle", 180)
            right_leg = angles.get("right_leg_angle", 180)
            
            # T-í¬ì¦ˆ (íŒ”ì´ ìˆ˜í‰)
            if abs(left_arm - 180) < 20 and abs(right_arm - 180) < 20:
                return "t_pose"
            
            # A-í¬ì¦ˆ (íŒ”ì´ ì•½ê°„ ì•„ë˜)
            elif 140 < left_arm < 170 and 140 < right_arm < 170:
                return "a_pose"
            
            # ê±·ê¸° í¬ì¦ˆ
            elif abs(left_leg - right_leg) > 30:
                return "walking"
            
            # ì•‰ê¸° í¬ì¦ˆ
            elif left_leg < 140 or right_leg < 140:
                return "sitting"
            
            # ì„œìˆëŠ” í¬ì¦ˆ (ê¸°ë³¸)
            else:
                return "standing"
                
        except Exception as e:
            logger.warning(f"í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return "unknown"
    
    def _evaluate_pose_quality(self, keypoints_18: List[List[float]], mediapipe_keypoints: List[Dict]) -> Dict[str, float]:
        """í¬ì¦ˆ í’ˆì§ˆ í‰ê°€"""
        
        try:
            # 1. ê²€ì¶œ ë¹„ìœ¨
            detected_18 = sum(1 for kp in keypoints_18 if kp[2] > 0.5)
            detection_rate = detected_18 / 18
            
            # 2. ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë¹„ìœ¨
            major_indices = [0, 1, 2, 5, 8, 11]  # nose, neck, shoulders, hips
            major_detected = sum(1 for idx in major_indices if keypoints_18[idx][2] > 0.5)
            major_detection_rate = major_detected / len(major_indices)
            
            # 3. í‰ê·  ì‹ ë¢°ë„
            confidences = [kp[2] for kp in keypoints_18 if kp[2] > 0]
            avg_confidence = np.mean(confidences) if confidences else 0.0
            
            # 4. ëŒ€ì¹­ì„± ì ìˆ˜
            symmetry_score = self._calculate_symmetry_score(keypoints_18)
            
            # 5. ì™„ì „ì„± ì ìˆ˜ (ìƒì²´, í•˜ì²´ ëª¨ë‘ ê²€ì¶œ)
            upper_body_indices = [0, 1, 2, 3, 4, 5, 6, 7, 14, 15, 16, 17]
            lower_body_indices = [8, 9, 10, 11, 12, 13]
            
            upper_detected = sum(1 for idx in upper_body_indices if keypoints_18[idx][2] > 0.5)
            lower_detected = sum(1 for idx in lower_body_indices if keypoints_18[idx][2] > 0.5)
            
            upper_completeness = upper_detected / len(upper_body_indices)
            lower_completeness = lower_detected / len(lower_body_indices)
            overall_completeness = (upper_completeness + lower_completeness) / 2
            
            # 6. ì „ì²´ í’ˆì§ˆ ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
            overall_confidence = (
                detection_rate * 0.25 +
                major_detection_rate * 0.25 +
                avg_confidence * 0.20 +
                symmetry_score * 0.15 +
                overall_completeness * 0.15
            )
            
            return {
                "overall_confidence": float(overall_confidence),
                "detection_rate": float(detection_rate),
                "major_detection_rate": float(major_detection_rate),
                "average_confidence": float(avg_confidence),
                "symmetry_score": float(symmetry_score),
                "upper_body_completeness": float(upper_completeness),
                "lower_body_completeness": float(lower_completeness),
                "detected_keypoints": detected_18,
                "quality_grade": self._get_quality_grade(overall_confidence)
            }
            
        except Exception as e:
            logger.warning(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                "overall_confidence": 0.0,
                "quality_grade": "poor"
            }
    
    def _calculate_symmetry_score(self, keypoints_18: List[List[float]]) -> float:
        """ì¢Œìš° ëŒ€ì¹­ì„± ì ìˆ˜"""
        
        # ëŒ€ì¹­ ìŒë“¤
        symmetric_pairs = [
            (2, 5),   # shoulders  
            (3, 6),   # elbows
            (4, 7),   # wrists
            (8, 11),  # hips
            (9, 12),  # knees
            (10, 13), # ankles
            (14, 15), # eyes
            (16, 17)  # ears
        ]
        
        symmetry_scores = []
        
        for right_idx, left_idx in symmetric_pairs:
            right_kp = keypoints_18[right_idx]
            left_kp = keypoints_18[left_idx]
            
            if right_kp[2] > 0.5 and left_kp[2] > 0.5:
                # ì‹ ë¢°ë„ ì°¨ì´
                conf_diff = abs(right_kp[2] - left_kp[2])
                conf_similarity = 1.0 - conf_diff
                
                # ìœ„ì¹˜ ëŒ€ì¹­ì„± (Y ì¢Œí‘œ ì°¨ì´)
                y_diff = abs(right_kp[1] - left_kp[1])
                max_y = max(right_kp[1], left_kp[1])
                if max_y > 0:
                    y_similarity = 1.0 - min(y_diff / max_y, 1.0)
                else:
                    y_similarity = 1.0
                
                # ì¢…í•© ëŒ€ì¹­ì„±
                pair_symmetry = (conf_similarity + y_similarity) / 2
                symmetry_scores.append(pair_symmetry)
        
        return np.mean(symmetry_scores) if symmetry_scores else 0.0
    
    def _get_quality_grade(self, overall_confidence: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if overall_confidence >= 0.9:
            return "excellent"
        elif overall_confidence >= 0.8:
            return "good"
        elif overall_confidence >= 0.6:
            return "fair"
        elif overall_confidence >= 0.4:
            return "poor"
        else:
            return "very_poor"
    
    def _get_openpose_connections(self) -> List[List[int]]:
        """OpenPose ì—°ê²°ì„  ì •ë³´"""
        return [
            # ëª¸í†µ
            [1, 2], [1, 5], [2, 8], [5, 11], [8, 11],
            
            # ì˜¤ë¥¸íŒ”
            [2, 3], [3, 4],
            
            # ì™¼íŒ”  
            [5, 6], [6, 7],
            
            # ì˜¤ë¥¸ë‹¤ë¦¬
            [8, 9], [9, 10],
            
            # ì™¼ë‹¤ë¦¬
            [11, 12], [12, 13],
            
            # ë¨¸ë¦¬
            [1, 0], [0, 14], [0, 15], [14, 16], [15, 17]
        ]
    
    def _create_empty_result(self, reason: str) -> Dict[str, Any]:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        return {
            "success": False,
            "error": reason,
            "keypoints_18": [[0, 0, 0] for _ in range(18)],
            "keypoints_raw": [],
            "pose_confidence": 0.0,
            "body_orientation": "unknown",
            "pose_angles": {},
            "body_proportions": {},
            "bounding_box": {"x": 0, "y": 0, "width": 0, "height": 0},
            "quality_metrics": {
                "overall_confidence": 0.0,
                "quality_grade": "failed"
            },
            "pose_connections": [],
            "processing_info": {
                "processing_time": 0.0,
                "model_used": "None",
                "keypoints_detected": 0
            }
        }
    
    def visualize_pose(self, image: np.ndarray, keypoints_18: List[List[float]], 
                      save_path: Optional[str] = None) -> np.ndarray:
        """í¬ì¦ˆ ì‹œê°í™”"""
        
        vis_image = image.copy()
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for i, (x, y, conf) in enumerate(keypoints_18):
            if conf > 0.5:
                # ì‹ ë¢°ë„ì— ë”°ë¥¸ ìƒ‰ìƒ
                if conf > 0.8:
                    color = (0, 255, 0)      # ì´ˆë¡ (ë†’ì€ ì‹ ë¢°ë„)
                elif conf > 0.6:
                    color = (0, 255, 255)    # ë…¸ë‘ (ì¤‘ê°„ ì‹ ë¢°ë„)
                else:
                    color = (0, 0, 255)      # ë¹¨ê°• (ë‚®ì€ ì‹ ë¢°ë„)
                
                # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
                cv2.circle(vis_image, (int(x), int(y)), 6, color, -1)
                
                # í‚¤í¬ì¸íŠ¸ ë²ˆí˜¸ í‘œì‹œ
                cv2.putText(vis_image, str(i), (int(x+8), int(y-8)), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        
        # ì—°ê²°ì„  ê·¸ë¦¬ê¸°
        connections = self._get_openpose_connections()
        for connection in connections:
            pt1_idx, pt2_idx = connection
            pt1 = keypoints_18[pt1_idx]
            pt2 = keypoints_18[pt2_idx]
            
            if pt1[2] > 0.5 and pt2[2] > 0.5:
                cv2.line(vis_image, 
                        (int(pt1[0]), int(pt1[1])), 
                        (int(pt2[0]), int(pt2[1])), 
                        (255, 0, 0), 3)
        
        # ì €ì¥
        if save_path:
            cv2.imwrite(save_path, vis_image)
            logger.info(f"ğŸ’¾ í¬ì¦ˆ ì‹œê°í™” ì €ì¥: {save_path}")
        
        return vis_image
    
    def export_keypoints(self, keypoints_18: List[List[float]], format: str = "json") -> str:
        """í‚¤í¬ì¸íŠ¸ ë‚´ë³´ë‚´ê¸°"""
        
        if format.lower() == "json":
            export_data = {
                "format": "openpose_18",
                "keypoints": keypoints_18,
                "keypoint_names": self.OPENPOSE_18_KEYPOINTS,
                "connections": self._get_openpose_connections()
            }
            return json.dumps(export_data, indent=2)
        
        elif format.lower() == "csv":
            lines = ["id,name,x,y,confidence"]
            for i, (x, y, conf) in enumerate(keypoints_18):
                lines.append(f"{i},{self.OPENPOSE_18_KEYPOINTS[i]},{x},{y},{conf}")
            return "\n".join(lines)
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.pose_detector:
            self.pose_detector.close()
            self.pose_detector = None
        
        self.mp_pose = None
        self.mp_drawing = None
        self.is_initialized = False
        
        logger.info("ğŸ§¹ ì‹¤ì œ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")


# === ì‚¬ìš© ì˜ˆì‹œ ===
async def test_pose_estimation():
    """ì‹¤ì œ í¬ì¦ˆ ì¶”ì • í…ŒìŠ¤íŠ¸"""
    
    # 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    pose_estimator = PoseEstimationStep(
        device='mps',  # M3 Max
        config={
            'model_complexity': 2,
            'min_detection_confidence': 0.7,
            'max_image_size': 1024
        }
    )
    
    success = await pose_estimator.initialize()
    if not success:
        print("âŒ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # 2. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ
    test_image_path = "test_person.jpg"
    
    if os.path.exists(test_image_path):
        test_image = cv2.imread(test_image_path)
    else:
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        print("âš ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ì—†ì–´ ë”ë¯¸ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # 3. í¬ì¦ˆ ì¶”ì • ì‹¤í–‰
    result = await pose_estimator.process(test_image)
    
    if result["success"]:
        print(f"âœ… í¬ì¦ˆ ì¶”ì • ì„±ê³µ!")
        print(f"ğŸ“Š ì „ì²´ ì‹ ë¢°ë„: {result['pose_confidence']:.3f}")
        print(f"ğŸ‘¥ ê²€ì¶œëœ í‚¤í¬ì¸íŠ¸: {result['processing_info']['keypoints_detected']}/18")
        print(f"ğŸ“ ì‹ ì²´ ë°©í–¥: {result['body_orientation']}")
        print(f"ğŸƒ í¬ì¦ˆ íƒ€ì…: {result['pose_analysis']['pose_type']}")
        print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result['processing_info']['processing_time']:.3f}ì´ˆ")
        print(f"ğŸ¯ í’ˆì§ˆ ë“±ê¸‰: {result['quality_metrics']['quality_grade']}")
        
        # ì‹œê°í™” ë° ì €ì¥
        vis_image = pose_estimator.visualize_pose(
            test_image, result["keypoints_18"], "output_pose.jpg"
        )
        
        # í‚¤í¬ì¸íŠ¸ ë‚´ë³´ë‚´ê¸°
        json_export = pose_estimator.export_keypoints(result["keypoints_18"], "json")
        with open("keypoints.json", "w") as f:
            f.write(json_export)
        
        print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: output_pose.jpg, keypoints.json")
        
    else:
        print(f"âŒ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {result['error']}")
    
    # 4. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    await pose_estimator.cleanup()

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_pose_estimation())