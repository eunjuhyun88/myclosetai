"""
2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • (Pose Estimation) - 18ê°œ í‚¤í¬ì¸íŠ¸ 
M3 Max ìµœì í™” ë²„ì „ (MediaPipe + MPS ë°±ì—”ë“œ)
"""
import os
import logging
import time
from typing import Dict, Any, Optional, Tuple, List
import numpy as np
import torch
import cv2
from PIL import Image
import json

# MediaPipe ê´€ë ¨ ì„í¬íŠ¸
try:
    import mediapipe as mp
    MP_AVAILABLE = True
except ImportError:
    logging.warning("MediaPipe ì„¤ì¹˜ í•„ìš”: pip install mediapipe")
    MP_AVAILABLE = False

# CoreML ì§€ì› (M3 Max ì „ìš©)
try:
    import coremltools as ct
    COREML_AVAILABLE = True
except ImportError:
    COREML_AVAILABLE = False

logger = logging.getLogger(__name__)

class PoseEstimationStep:
    """í¬ì¦ˆ ì¶”ì • ìŠ¤í… - 18ê°œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ (M3 Max ìµœì í™”)"""
    
    # OpenPose í˜¸í™˜ 18ê°œ í‚¤í¬ì¸íŠ¸ ì •ì˜
    OPENPOSE_18_KEYPOINTS = {
        0: "Nose",
        1: "Neck", 
        2: "R-Shoulder",
        3: "R-Elbow", 
        4: "R-Wrist",
        5: "L-Shoulder",
        6: "L-Elbow",
        7: "L-Wrist",
        8: "R-Hip",
        9: "R-Knee",
        10: "R-Ankle",
        11: "L-Hip", 
        12: "L-Knee",
        13: "L-Ankle",
        14: "R-Eye",
        15: "L-Eye",
        16: "R-Ear",
        17: "L-Ear"
    }
    
    # MediaPipe â†’ OpenPose ë§¤í•‘
    MP_TO_OPENPOSE_18 = {
        0: 0,   # nose
        # neckì€ ì–´ê¹¨ ì¤‘ì ìœ¼ë¡œ ê³„ì‚°
        11: 5,  # left_shoulder
        12: 2,  # right_shoulder  
        13: 6,  # left_elbow
        14: 3,  # right_elbow
        15: 7,  # left_wrist
        16: 4,  # right_wrist
        23: 11, # left_hip
        24: 8,  # right_hip
        25: 12, # left_knee  
        26: 9,  # right_knee
        27: 13, # left_ankle
        28: 10, # right_ankle
        2: 15,  # left_eye_inner â†’ left_eye
        5: 14,  # right_eye_inner â†’ right_eye  
        7: 17,  # left_ear
        8: 16   # right_ear
    }
    
    def __init__(self, model_loader, device: str, config: Dict[str, Any] = None):
        """
        Args:
            model_loader: ëª¨ë¸ ë¡œë” ì¸ìŠ¤í„´ìŠ¤
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cpu', 'cuda', 'mps')
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.model_loader = model_loader
        self.device = device
        self.config = config or {}
        
        # ê¸°ë³¸ ì„¤ì •
        self.model_complexity = self.config.get('model_complexity', 2)  # 0, 1, 2
        self.min_detection_confidence = self.config.get('min_detection_confidence', 0.7)
        self.min_tracking_confidence = self.config.get('min_tracking_confidence', 0.5)
        self.static_image_mode = self.config.get('static_image_mode', True)
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì • (M3 Max)
        self.use_mps = device == 'mps' and torch.backends.mps.is_available()
        self.use_coreml = COREML_AVAILABLE and self.config.get('use_coreml', True)
        
        # ëª¨ë¸ ê´€ë ¨
        self.mp_pose = None
        self.pose_model = None
        self.coreml_model = None
        self.is_initialized = False
        
        logger.info(f"ğŸ¯ í¬ì¦ˆ ì¶”ì • ìŠ¤í… ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {device}, MPS: {self.use_mps}, CoreML: {self.use_coreml}")
    
    async def initialize(self) -> bool:
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”„ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            # MediaPipe ì´ˆê¸°í™”
            if MP_AVAILABLE:
                self.mp_pose = mp.solutions.pose
                self.pose_model = self.mp_pose.Pose(
                    static_image_mode=self.static_image_mode,
                    model_complexity=self.model_complexity,
                    min_detection_confidence=self.min_detection_confidence,
                    min_tracking_confidence=self.min_tracking_confidence,
                    enable_segmentation=False  # ì„±ëŠ¥ ìµœì í™”
                )
                logger.info("âœ… MediaPipe Pose ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ MediaPipe ì‚¬ìš© ë¶ˆê°€, ë°ëª¨ ëª¨ë“œë¡œ ì‹¤í–‰")
                self._create_demo_model()
            
            # CoreML ëª¨ë¸ ë¡œë“œ ì‹œë„ (M3 Max ìµœì í™”)
            if self.use_coreml:
                await self._load_coreml_model()
            
            self.is_initialized = True
            logger.info("âœ… í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_initialized = False
            return False
    
    async def _load_coreml_model(self):
        """CoreML ëª¨ë¸ ë¡œë“œ (M3 Max ì „ìš©)"""
        try:
            model_path = self.config.get('coreml_model_path', 'app/models/ai_models/pose_estimation.mlmodel')
            
            if os.path.exists(model_path):
                self.coreml_model = ct.models.MLModel(model_path)
                logger.info("âœ… CoreML í¬ì¦ˆ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                logger.info("â„¹ï¸ CoreML ëª¨ë¸ íŒŒì¼ ì—†ìŒ, MediaPipe ì‚¬ìš©")
                
        except Exception as e:
            logger.warning(f"âš ï¸ CoreML ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _create_demo_model(self):
        """ë°ëª¨ìš© í¬ì¦ˆ ì¶”ì • ëª¨ë¸"""
        logger.info("ğŸ”§ ë°ëª¨ í¬ì¦ˆ ëª¨ë¸ ìƒì„± ì¤‘...")
        # ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” ì œê±°
        pass
    
    def process(self, person_image_tensor: torch.Tensor) -> Dict[str, Any]:
        """
        í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬
        
        Args:
            person_image_tensor: ì‚¬ìš©ì ì´ë¯¸ì§€ í…ì„œ [1, 3, H, W]
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_initialized:
            raise RuntimeError("í¬ì¦ˆ ì¶”ì • ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_time = time.time()
        
        try:
            # í…ì„œë¥¼ OpenCV ì´ë¯¸ì§€ë¡œ ë³€í™˜
            cv_image = self._tensor_to_cv_image(person_image_tensor)
            
            # CoreML ìš°ì„  ì‹œë„ (M3 Max ìµœì í™”)
            if self.coreml_model is not None:
                result = self._process_with_coreml(cv_image)
            else:
                # MediaPipeë¡œ ì²˜ë¦¬
                result = self._process_with_mediapipe(cv_image)
            
            processing_time = time.time() - start_time
            
            # ê²°ê³¼ êµ¬ì„±
            final_result = {
                "success": result["success"],
                "keypoints_18": result["keypoints_18"],
                "keypoints_raw": result.get("keypoints_raw", []),
                "pose_confidence": result["confidence"],
                "body_orientation": result["orientation"],
                "pose_angles": result["angles"],
                "bounding_box": result["bbox"],
                "pose_connections": self._get_pose_connections(),
                "processing_time": processing_time,
                "model_used": result["model_used"],
                "quality_metrics": result["quality_metrics"]
            }
            
            logger.info(f"âœ… í¬ì¦ˆ ì¶”ì • ì™„ë£Œ - ì²˜ë¦¬ì‹œê°„: {processing_time:.3f}ì´ˆ, ì‹ ë¢°ë„: {result['confidence']:.3f}")
            
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _tensor_to_cv_image(self, tensor: torch.Tensor) -> np.ndarray:
        """í…ì„œë¥¼ OpenCV ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        # [1, 3, H, W] â†’ [H, W, 3]
        if tensor.dim() == 4:
            tensor = tensor.squeeze(0)
        
        # [3, H, W] â†’ [H, W, 3]
        if tensor.shape[0] == 3:
            tensor = tensor.permute(1, 2, 0)
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        if tensor.max() > 1.0:
            tensor = tensor / 255.0
        
        # numpy ë³€í™˜ ë° BGRë¡œ ë³€í™˜
        image = (tensor.cpu().numpy() * 255).astype(np.uint8)
        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        
        return image_bgr
    
    def _process_with_coreml(self, cv_image: np.ndarray) -> Dict[str, Any]:
        """CoreML ëª¨ë¸ë¡œ í¬ì¦ˆ ì¶”ì • (M3 Max ìµœì í™”)"""
        try:
            # CoreML ì…ë ¥ ì¤€ë¹„
            image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(image_rgb)
            
            # ëª¨ë¸ ì˜ˆì¸¡
            prediction = self.coreml_model.predict({'image': pil_image})
            
            # ê²°ê³¼ íŒŒì‹± (ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¦„)
            keypoints = self._parse_coreml_output(prediction, cv_image.shape[:2])
            
            return {
                "success": True,
                "keypoints_18": keypoints["openpose_18"],
                "keypoints_raw": keypoints["raw"],
                "confidence": keypoints["confidence"],
                "orientation": self._estimate_orientation(keypoints["openpose_18"]),
                "angles": self._calculate_pose_angles(keypoints["openpose_18"]),
                "bbox": self._calculate_bbox(keypoints["openpose_18"]),
                "model_used": "CoreML",
                "quality_metrics": self._evaluate_pose_quality(keypoints["openpose_18"])
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ CoreML ì²˜ë¦¬ ì‹¤íŒ¨, MediaPipeë¡œ fallback: {e}")
            return self._process_with_mediapipe(cv_image)
    
    def _process_with_mediapipe(self, cv_image: np.ndarray) -> Dict[str, Any]:
        """MediaPipeë¡œ í¬ì¦ˆ ì¶”ì •"""
        try:
            # RGB ë³€í™˜
            image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)
            
            # í¬ì¦ˆ ê²€ì¶œ
            results = self.pose_model.process(image_rgb)
            
            if not results.pose_landmarks:
                return self._create_empty_result("MediaPipe - í¬ì¦ˆ ê²€ì¶œ ì‹¤íŒ¨")
            
            # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ë° ë³€í™˜
            keypoints = self._extract_mediapipe_keypoints(results.pose_landmarks, cv_image.shape[:2])
            
            # OpenPose 18 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            openpose_18 = self._convert_to_openpose_18(keypoints)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_pose_confidence(keypoints)
            
            return {
                "success": True,
                "keypoints_18": openpose_18,
                "keypoints_raw": keypoints,
                "confidence": confidence,
                "orientation": self._estimate_orientation(openpose_18),
                "angles": self._calculate_pose_angles(openpose_18),
                "bbox": self._calculate_bbox(openpose_18),
                "model_used": "MediaPipe",
                "quality_metrics": self._evaluate_pose_quality(openpose_18)
            }
            
        except Exception as e:
            logger.error(f"MediaPipe ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_empty_result(f"MediaPipe ì—ëŸ¬: {str(e)}")
    
    def _extract_mediapipe_keypoints(self, landmarks, image_shape: Tuple[int, int]) -> List[Dict]:
        """MediaPipe ëœë“œë§ˆí¬ì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        height, width = image_shape
        keypoints = []
        
        for idx, landmark in enumerate(landmarks.landmark):
            keypoint = {
                "id": idx,
                "x": landmark.x,           # ì •ê·œí™”ëœ ì¢Œí‘œ (0-1)
                "y": landmark.y,
                "z": landmark.z,           # ìƒëŒ€ì  ê¹Šì´
                "visibility": landmark.visibility,
                "x_px": int(landmark.x * width),   # í”½ì…€ ì¢Œí‘œ
                "y_px": int(landmark.y * height),
                "confidence": landmark.visibility
            }
            keypoints.append(keypoint)
        
        return keypoints
    
    def _convert_to_openpose_18(self, mediapipe_keypoints: List[Dict]) -> List[List[float]]:
        """MediaPipe í‚¤í¬ì¸íŠ¸ë¥¼ OpenPose 18 í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        openpose_18 = [[0, 0, 0] for _ in range(18)]  # [x, y, confidence]
        
        # ë§¤í•‘ ì ìš©
        for mp_idx, op_idx in self.MP_TO_OPENPOSE_18.items():
            if mp_idx < len(mediapipe_keypoints):
                mp_kp = mediapipe_keypoints[mp_idx]
                openpose_18[op_idx] = [
                    mp_kp["x_px"],
                    mp_kp["y_px"], 
                    mp_kp["confidence"]
                ]
        
        # Neck (1ë²ˆ) ê³„ì‚°: ì–‘ ì–´ê¹¨ì˜ ì¤‘ì 
        if openpose_18[2][2] > 0 and openpose_18[5][2] > 0:  # ì–‘ ì–´ê¹¨ê°€ ëª¨ë‘ ê²€ì¶œëœ ê²½ìš°
            neck_x = (openpose_18[2][0] + openpose_18[5][0]) / 2
            neck_y = (openpose_18[2][1] + openpose_18[5][1]) / 2
            neck_conf = min(openpose_18[2][2], openpose_18[5][2])
            openpose_18[1] = [neck_x, neck_y, neck_conf]
        
        return openpose_18
    
    def _calculate_pose_confidence(self, keypoints: List[Dict]) -> float:
        """ì „ì²´ í¬ì¦ˆì˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not keypoints:
            return 0.0
        
        # ì£¼ìš” í‚¤í¬ì¸íŠ¸ë“¤ì˜ ê°€ì¤‘í‰ê· 
        major_keypoints = [0, 11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]  # MediaPipe ì¸ë±ìŠ¤
        
        total_weight = 0
        weighted_confidence = 0
        
        for idx in major_keypoints:
            if idx < len(keypoints):
                weight = 1.5 if idx in [11, 12, 23, 24] else 1.0  # ì–´ê¹¨ì™€ ì—‰ë©ì´ ê°€ì¤‘ì¹˜ ì¦ê°€
                confidence = keypoints[idx]["confidence"]
                
                weighted_confidence += confidence * weight
                total_weight += weight
        
        return weighted_confidence / total_weight if total_weight > 0 else 0.0
    
    def _estimate_orientation(self, keypoints_18: List[List[float]]) -> str:
        """ì‹ ì²´ ë°©í–¥ ì¶”ì •"""
        try:
            # ì–´ê¹¨ í‚¤í¬ì¸íŠ¸ í™•ì¸
            left_shoulder = keypoints_18[5]   # L-Shoulder
            right_shoulder = keypoints_18[2]  # R-Shoulder
            
            if left_shoulder[2] == 0 or right_shoulder[2] == 0:
                return "unknown"
            
            # ì–´ê¹¨ ë„ˆë¹„
            shoulder_width = abs(right_shoulder[0] - left_shoulder[0])
            
            # ì´ë¯¸ì§€ ë„ˆë¹„ ëŒ€ë¹„ ì–´ê¹¨ ë„ˆë¹„ ë¹„ìœ¨ë¡œ ë°©í–¥ ì¶”ì •
            if shoulder_width < 50:  # í”½ì…€ ê¸°ì¤€, ì¡°ì • í•„ìš”
                return "side"
            elif shoulder_width < 100:
                return "diagonal"
            else:
                return "front"
                
        except Exception:
            return "unknown"
    
    def _calculate_pose_angles(self, keypoints_18: List[List[float]]) -> Dict[str, float]:
        """ì£¼ìš” ê´€ì ˆ ê°ë„ ê³„ì‚°"""
        def angle_between_points(p1, p2, p3):
            """ì„¸ ì  ì‚¬ì´ì˜ ê°ë„ ê³„ì‚° (p2ê°€ ê¼­ì§€ì )"""
            if any(p[2] == 0 for p in [p1, p2, p3]):  # ì‹ ë¢°ë„ 0ì¸ ê²½ìš°
                return 0.0
            
            v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
            v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
            
            cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-6)
            cos_angle = np.clip(cos_angle, -1.0, 1.0)
            angle = np.degrees(np.arccos(cos_angle))
            
            return float(angle)
        
        angles = {}
        
        try:
            # ì™¼íŒ” ê°ë„ (ì–´ê¹¨-íŒ”ê¿ˆì¹˜-ì†ëª©)
            angles["left_arm"] = angle_between_points(
                keypoints_18[5],   # L-Shoulder
                keypoints_18[6],   # L-Elbow  
                keypoints_18[7]    # L-Wrist
            )
            
            # ì˜¤ë¥¸íŒ” ê°ë„
            angles["right_arm"] = angle_between_points(
                keypoints_18[2],   # R-Shoulder
                keypoints_18[3],   # R-Elbow
                keypoints_18[4]    # R-Wrist
            )
            
            # ì™¼ë‹¤ë¦¬ ê°ë„ (ì—‰ë©ì´-ë¬´ë¦-ë°œëª©)
            angles["left_leg"] = angle_between_points(
                keypoints_18[11],  # L-Hip
                keypoints_18[12],  # L-Knee
                keypoints_18[13]   # L-Ankle
            )
            
            # ì˜¤ë¥¸ë‹¤ë¦¬ ê°ë„
            angles["right_leg"] = angle_between_points(
                keypoints_18[8],   # R-Hip
                keypoints_18[9],   # R-Knee  
                keypoints_18[10]   # R-Ankle
            )
            
        except Exception as e:
            logger.warning(f"ê´€ì ˆ ê°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        return angles
    
    def _calculate_bbox(self, keypoints_18: List[List[float]]) -> Dict[str, int]:
        """í¬ì¦ˆ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
        valid_points = [(x, y) for x, y, c in keypoints_18 if c > 0]
        
        if not valid_points:
            return {"x": 0, "y": 0, "width": 0, "height": 0}
        
        xs, ys = zip(*valid_points)
        
        x_min, x_max = min(xs), max(xs)
        y_min, y_max = min(ys), max(ys)
        
        # ì—¬ë°± ì¶”ê°€ (10%)
        margin_x = int((x_max - x_min) * 0.1)
        margin_y = int((y_max - y_min) * 0.1)
        
        return {
            "x": max(0, int(x_min - margin_x)),
            "y": max(0, int(y_min - margin_y)),
            "width": int(x_max - x_min + 2 * margin_x),
            "height": int(y_max - y_min + 2 * margin_y)
        }
    
    def _evaluate_pose_quality(self, keypoints_18: List[List[float]]) -> Dict[str, float]:
        """í¬ì¦ˆ í’ˆì§ˆ í‰ê°€"""
        # ê²€ì¶œëœ í‚¤í¬ì¸íŠ¸ ìˆ˜
        detected_count = sum(1 for kp in keypoints_18 if kp[2] > 0.5)
        detection_rate = detected_count / 18
        
        # ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì—¬ë¶€
        major_keypoints = [0, 1, 2, 5, 8, 11]  # nose, neck, shoulders, hips
        major_detected = sum(1 for idx in major_keypoints if keypoints_18[idx][2] > 0.5)
        major_rate = major_detected / len(major_keypoints)
        
        # ëŒ€ì¹­ì„± í‰ê°€
        symmetry_score = self._calculate_symmetry_score(keypoints_18)
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        overall_quality = (detection_rate * 0.4 + major_rate * 0.4 + symmetry_score * 0.2)
        
        return {
            "overall": overall_quality,
            "detection_rate": detection_rate,
            "major_keypoints_rate": major_rate,
            "symmetry_score": symmetry_score,
            "detected_keypoints": detected_count
        }
    
    def _calculate_symmetry_score(self, keypoints_18: List[List[float]]) -> float:
        """ì¢Œìš° ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚°"""
        symmetric_pairs = [
            (2, 5),   # shoulders
            (3, 6),   # elbows
            (4, 7),   # wrists
            (8, 11),  # hips
            (9, 12),  # knees
            (10, 13), # ankles
            (14, 15), # eyes
            (16, 17)  # ears
        ]
        
        symmetry_scores = []
        
        for left_idx, right_idx in symmetric_pairs:
            left_kp = keypoints_18[left_idx]
            right_kp = keypoints_18[right_idx]
            
            if left_kp[2] > 0.5 and right_kp[2] > 0.5:
                # ì‹ ë¢°ë„ ì°¨ì´
                conf_diff = abs(left_kp[2] - right_kp[2])
                symmetry_scores.append(1.0 - conf_diff)
        
        return np.mean(symmetry_scores) if symmetry_scores else 0.0
    
    def _get_pose_connections(self) -> List[List[int]]:
        """í¬ì¦ˆ ì—°ê²°ì„  ì •ë³´ (OpenPose 18 ê¸°ì¤€)"""
        return [
            [1, 2], [1, 5], [2, 3], [3, 4], [5, 6], [6, 7],     # ìƒì²´
            [1, 8], [8, 9], [9, 10], [1, 11], [11, 12], [12, 13], # í•˜ì²´  
            [1, 0], [0, 14], [14, 16], [0, 15], [15, 17],        # ë¨¸ë¦¬
            [2, 16], [5, 17]                                      # ì–´ê¹¨-ê·€ ì—°ê²°
        ]
    
    def _create_empty_result(self, reason: str) -> Dict[str, Any]:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        return {
            "success": False,
            "keypoints_18": [[0, 0, 0] for _ in range(18)],
            "keypoints_raw": [],
            "confidence": 0.0,
            "orientation": "unknown",
            "angles": {},
            "bbox": {"x": 0, "y": 0, "width": 0, "height": 0},
            "model_used": "None",
            "quality_metrics": {
                "overall": 0.0,
                "detection_rate": 0.0,
                "major_keypoints_rate": 0.0,
                "symmetry_score": 0.0,
                "detected_keypoints": 0
            },
            "error": reason
        }
    
    def _parse_coreml_output(self, prediction: Dict, image_shape: Tuple[int, int]) -> Dict:
        """CoreML ì¶œë ¥ íŒŒì‹± (ëª¨ë¸ë³„ êµ¬í˜„ í•„ìš”)"""
        # ì‹¤ì œ CoreML ëª¨ë¸ ì¶œë ¥ì— ë”°ë¼ êµ¬í˜„
        # ì˜ˆì‹œ êµ¬ì¡°
        return {
            "raw": [],
            "openpose_18": [[0, 0, 0] for _ in range(18)],
            "confidence": 0.0
        }
    
    def visualize_pose(self, image: np.ndarray, keypoints_18: List[List[float]]) -> np.ndarray:
        """í¬ì¦ˆ ì‹œê°í™”"""
        vis_image = image.copy()
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for i, (x, y, conf) in enumerate(keypoints_18):
            if conf > 0.5:
                color = (0, 255, 0) if conf > 0.8 else (0, 255, 255)
                cv2.circle(vis_image, (int(x), int(y)), 5, color, -1)
                # í‚¤í¬ì¸íŠ¸ ë²ˆí˜¸ í‘œì‹œ
                cv2.putText(vis_image, str(i), (int(x+5), int(y-5)), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)
        
        # ì—°ê²°ì„  ê·¸ë¦¬ê¸°
        connections = self._get_pose_connections()
        for connection in connections:
            pt1_idx, pt2_idx = connection
            pt1 = keypoints_18[pt1_idx]
            pt2 = keypoints_18[pt2_idx]
            
            if pt1[2] > 0.5 and pt2[2] > 0.5:
                cv2.line(vis_image, 
                        (int(pt1[0]), int(pt1[1])), 
                        (int(pt2[0]), int(pt2[1])), 
                        (255, 0, 0), 2)
        
        return vis_image
    
    async def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": "PoseEstimation",
            "keypoint_format": "OpenPose_18",
            "device": self.device,
            "use_mps": self.use_mps,
            "use_coreml": self.use_coreml,
            "mediapipe_available": MP_AVAILABLE,
            "coreml_available": COREML_AVAILABLE,
            "initialized": self.is_initialized,
            "model_complexity": self.model_complexity,
            "min_detection_confidence": self.min_detection_confidence,
            "keypoints": list(self.OPENPOSE_18_KEYPOINTS.values())
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.pose_model:
            self.pose_model.close()
            self.pose_model = None
        
        if self.coreml_model:
            del self.coreml_model
            self.coreml_model = None
        
        self.is_initialized = False
        logger.info("ğŸ§¹ í¬ì¦ˆ ì¶”ì • ìŠ¤í… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")