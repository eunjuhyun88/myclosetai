"""
ğŸ”¥ MyCloset AI - Step 02: ì™„ì „í•œ í¬ì¦ˆ ì¶”ì • (Pose Estimation) - ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ ë° ì¶”ë¡ 
=====================================================================================

âœ… StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… â†’ ì™„ì„±ëœ Step êµ¬ì¡° ì™„ì „ êµ¬í˜„
âœ… ì²´í¬í¬ì¸íŠ¸ â†’ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ ë³€í™˜ (Step 01 ì´ìŠˆ í•´ê²°)
âœ… OpenPose, YOLOv8, ê²½ëŸ‰ ëª¨ë¸ ë“± ì‹¤ì œ AI ì¶”ë¡  ì—”ì§„ ë‚´ì¥
âœ… 18ê°œ í‚¤í¬ì¸íŠ¸ OpenPose í‘œì¤€ + COCO 17 ë³€í™˜ ì§€ì›
âœ… BaseStepMixin ì™„ì „ ìƒì† - ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì™„ë²½ êµ¬í˜„
âœ… ModelLoader ì™„ì „ ì—°ë™ - ìˆœí™˜ì°¸ì¡° ì—†ëŠ” í•œë°©í–¥ ì°¸ì¡°
âœ… M3 Max 128GB ìµœì í™” + conda í™˜ê²½ ìš°ì„ 
âœ… Strict Mode ì§€ì› - ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì—ëŸ¬
âœ… ì™„ì „í•œ ë¶„ì„ ë©”ì„œë“œ - ê°ë„, ë¹„ìœ¨, ëŒ€ì¹­ì„±, ê°€ì‹œì„±, í’ˆì§ˆ í‰ê°€
âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±

íŒŒì¼ ìœ„ì¹˜: backend/app/ai_pipeline/steps/step_02_pose_estimation.py
ì‘ì„±ì: MyCloset AI Team  
ë‚ ì§œ: 2025-07-22
ë²„ì „: v8.0 (ì™„ì „í•œ AI ì—°ë™ ë° ì¶”ë¡ )
"""

import os
import sys
import logging
import time
import asyncio
import threading
import json
import gc
import hashlib
import base64
import traceback
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, Type
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field, asdict
from enum import Enum, IntEnum
from functools import lru_cache, wraps
from contextlib import contextmanager
import numpy as np
import io
# íŒŒì¼ ìƒë‹¨ import ì„¹ì…˜ì—
from ..utils.pytorch_safe_ops import (
    safe_max, safe_amax, safe_argmax,
    extract_keypoints_from_heatmaps,
    tensor_to_pil_conda_optimized
)
# ==============================================
# ğŸ”¥ í•„ìˆ˜ íŒ¨í‚¤ì§€ ê²€ì¦ (conda í™˜ê²½ ìš°ì„ )
# ==============================================

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
except ImportError as e:
    raise ImportError(f"âŒ PyTorch í•„ìˆ˜: conda install pytorch torchvision pytorch-cuda -c pytorch -c nvidia\nì„¸ë¶€ ì˜¤ë¥˜: {e}")

try:
    import cv2
    CV2_AVAILABLE = True
    CV2_VERSION = cv2.__version__
except ImportError as e:
    print(f"âš ï¸ OpenCV import ì‹¤íŒ¨: {e}")
    # OpenCV í´ë°± êµ¬í˜„
    class OpenCVFallback:
        def __init__(self):
            self.INTER_LINEAR = 1
            self.INTER_CUBIC = 2
            self.COLOR_BGR2RGB = 4
            self.COLOR_RGB2BGR = 3
        
        def resize(self, img, size, interpolation=1):
            try:
                from PIL import Image
                if hasattr(img, 'shape'):
                    pil_img = Image.fromarray(img)
                    resized = pil_img.resize(size)
                    return np.array(resized)
                return img
            except:
                return img
        
        def cvtColor(self, img, code):
            if hasattr(img, 'shape') and len(img.shape) == 3:
                if code in [3, 4]:  # BGR<->RGB
                    return img[:, :, ::-1]
            return img
    
    cv2 = OpenCVFallback()
    CV2_AVAILABLE = False

try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
    PIL_VERSION = Image.__version__ if hasattr(Image, '__version__') else "Unknown"
except ImportError as e:
    raise ImportError(f"âŒ Pillow í•„ìˆ˜: conda install pillow -c conda-forge\nì„¸ë¶€ ì˜¤ë¥˜: {e}")

try:
    import psutil
    PSUTIL_AVAILABLE = True
    PSUTIL_VERSION = psutil.__version__
except ImportError:
    PSUTIL_AVAILABLE = False
    PSUTIL_VERSION = "Not Available"

# ì•ˆì „í•œ MPS ìºì‹œ ì •ë¦¬
try:
    from app.core.gpu_config import safe_mps_empty_cache
except ImportError:
    def safe_mps_empty_cache():
        gc.collect()
        return {"success": True, "method": "fallback_gc"}

# ==============================================
# ğŸ”¥ ì˜ì¡´ì„± ì£¼ì… êµ¬ì¡° - ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
# ==============================================

# 1. BaseStepMixin ì„í¬íŠ¸ (í•„ìˆ˜)
try:
    from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
    BASE_STEP_MIXIN_AVAILABLE = True
except ImportError as e:
    raise ImportError(f"âŒ BaseStepMixin í•„ìˆ˜ - ì˜ì¡´ì„± ì£¼ì… ë¶ˆê°€ëŠ¥: {e}")

# 2. ModelLoader ì„í¬íŠ¸ (í•„ìˆ˜)
try:
    from app.ai_pipeline.utils.model_loader import (
        ModelLoader, get_global_model_loader
    )
    MODEL_LOADER_AVAILABLE = True
except ImportError as e:
    raise ImportError(f"âŒ ModelLoader í•„ìˆ˜ - AI ëª¨ë¸ ì—°ë™ ë¶ˆê°€ëŠ¥: {e}")

# 3. ì„ íƒì  ì˜ì¡´ì„±ë“¤
try:
    from app.ai_pipeline.utils.memory_manager import get_global_memory_manager
    MEMORY_MANAGER_AVAILABLE = True
except ImportError:
    MEMORY_MANAGER_AVAILABLE = False

try:
    from app.ai_pipeline.utils.data_converter import get_global_data_converter  
    DATA_CONVERTER_AVAILABLE = True
except ImportError:
    DATA_CONVERTER_AVAILABLE = False

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ í¬ì¦ˆ ì¶”ì • ë°ì´í„° êµ¬ì¡° ë° ìƒìˆ˜
# ==============================================

class PoseModel(Enum):
    """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ íƒ€ì…"""
    OPENPOSE = "pose_estimation_openpose"
    YOLOV8_POSE = "pose_estimation_sk" 
    LIGHTWEIGHT = "pose_estimation_lightweight"

class PoseQuality(Enum):
    """í¬ì¦ˆ í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"     # 90-100ì 
    GOOD = "good"              # 75-89ì   
    ACCEPTABLE = "acceptable"   # 60-74ì 
    POOR = "poor"              # 40-59ì 
    VERY_POOR = "very_poor"    # 0-39ì 

class PoseType(Enum):
    """í¬ì¦ˆ íƒ€ì…"""
    T_POSE = "t_pose"          # Tì í¬ì¦ˆ
    A_POSE = "a_pose"          # Aì í¬ì¦ˆ
    STANDING = "standing"      # ì¼ë°˜ ì„œìˆëŠ” í¬ì¦ˆ
    SITTING = "sitting"        # ì•‰ì€ í¬ì¦ˆ
    ACTION = "action"          # ì•¡ì…˜ í¬ì¦ˆ
    UNKNOWN = "unknown"        # ì•Œ ìˆ˜ ì—†ëŠ” í¬ì¦ˆ

# OpenPose 18 í‚¤í¬ì¸íŠ¸ ì •ì˜
OPENPOSE_18_KEYPOINTS = [
    "nose", "neck", "right_shoulder", "right_elbow", "right_wrist",
    "left_shoulder", "left_elbow", "left_wrist", "middle_hip", "right_hip", 
    "right_knee", "right_ankle", "left_hip", "left_knee", "left_ankle",
    "right_eye", "left_eye", "right_ear", "left_ear"
]

# í‚¤í¬ì¸íŠ¸ ìƒ‰ìƒ ë° ì—°ê²° ì •ë³´
KEYPOINT_COLORS = [
    (255, 0, 0), (255, 85, 0), (255, 170, 0), (255, 255, 0), (170, 255, 0),
    (85, 255, 0), (0, 255, 0), (0, 255, 85), (0, 255, 170), (0, 255, 255),
    (0, 170, 255), (0, 85, 255), (0, 0, 255), (85, 0, 255), (170, 0, 255),
    (255, 0, 255), (255, 0, 170), (255, 0, 85), (255, 0, 0)
]

SKELETON_CONNECTIONS = [
    (0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7), (1, 8),
    (8, 9), (9, 10), (10, 11), (8, 12), (12, 13), (13, 14), (0, 15),
    (15, 17), (0, 16), (16, 18)
]

# ==============================================
# ğŸ”¥ ì™„ì „í•œ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
# ==============================================

class RealOpenPoseModel(nn.Module):
    """ì™„ì „í•œ ì‹¤ì œ OpenPose AI ëª¨ë¸ - ì²´í¬í¬ì¸íŠ¸ â†’ ì‹¤ì œ ëª¨ë¸ ë³€í™˜"""
    
    def __init__(self, num_keypoints: int = 18):
        super(RealOpenPoseModel, self).__init__()
        self.num_keypoints = num_keypoints
        
        # VGG-like backbone
        self.backbone = self._build_backbone()
        
        # PAF (Part Affinity Field) branch
        self.paf_branch = self._build_paf_branch()
        
        # Keypoint heatmap branch
        self.keypoint_branch = self._build_keypoint_branch()
        
        self.logger = logging.getLogger(f"{__name__}.RealOpenPoseModel")
    
    def _build_backbone(self) -> nn.Module:
        """VGG-like backbone êµ¬ì„±"""
        return nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 2
            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 3
            nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 4
            nn.Conv2d(256, 512, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(inplace=True),
        )
    
    def _build_paf_branch(self) -> nn.Module:
        """Part Affinity Field ë¸Œëœì¹˜"""
        return nn.Sequential(
            nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(128, 512, 1, 1, 0), nn.ReLU(inplace=True),
            nn.Conv2d(512, 38, 1, 1, 0)  # 19 pairs * 2
        )
    
    def _build_keypoint_branch(self) -> nn.Module:
        """í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ ë¸Œëœì¹˜"""
        return nn.Sequential(
            nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(128, 512, 1, 1, 0), nn.ReLU(inplace=True),
            nn.Conv2d(512, self.num_keypoints, 1, 1, 0)
        )
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """ìˆœì „íŒŒ"""
        features = self.backbone(x)
        
        # PAFì™€ í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ ìƒì„±
        paf = self.paf_branch(features)
        keypoints = self.keypoint_branch(features)
        
        return keypoints, paf
    
    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu") -> 'RealOpenPoseModel':
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‹¤ì œ AI ëª¨ë¸ ìƒì„± - Step 01 ì´ìŠˆ í•´ê²°"""
        try:
            # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            model = cls()
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            if os.path.exists(checkpoint_path):
                checkpoint = torch.load(checkpoint_path, map_location=device)
                
                # ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ì¶”ì¶œ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
                if isinstance(checkpoint, dict):
                    if 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    elif 'model' in checkpoint:
                        state_dict = checkpoint['model']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # í‚¤ ì´ë¦„ ì •ë¦¬ (module. ì œê±° ë“±)
                cleaned_state_dict = {}
                for key, value in state_dict.items():
                    clean_key = key.replace('module.', '').replace('model.', '')
                    cleaned_state_dict[clean_key] = value
                
                # ê°€ì¤‘ì¹˜ ë¡œë“œ
                model.load_state_dict(cleaned_state_dict, strict=False)
                logger.info(f"âœ… OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ: {checkpoint_path}")
            else:
                logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ - ë¬´ì‘ìœ„ ì´ˆê¸°í™”: {checkpoint_path}")
            
            model.to(device)
            model.eval()
            
            return model
            
        except Exception as e:
            logger.error(f"âŒ OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ë¬´ì‘ìœ„ ì´ˆê¸°í™” ëª¨ë¸ ë°˜í™˜
            model = cls()
            model.to(device)
            model.eval()
            return model

class RealYOLOv8PoseModel:
    """ì™„ì „í•œ ì‹¤ì œ YOLOv8 í¬ì¦ˆ ì¶”ì • ëª¨ë¸"""
    
    def __init__(self, checkpoint_path: str, device: str = "cpu"):
        self.device = device
        self.checkpoint_path = checkpoint_path
        self.model = None
        self.logger = logging.getLogger(f"{__name__}.RealYOLOv8PoseModel")
        
        # YOLOv8 ëª¨ë¸ ë¡œë“œ ì‹œë„
        self._load_yolov8_model()
    
    def _load_yolov8_model(self):
        """YOLOv8 ëª¨ë¸ ë¡œë“œ"""
        try:
            # ultralytics ì‚¬ìš© ì‹œë„
            try:
                from ultralytics import YOLO
                
                if os.path.exists(self.checkpoint_path):
                    self.model = YOLO(self.checkpoint_path)
                    self.logger.info(f"âœ… YOLOv8 ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ: {self.checkpoint_path}")
                else:
                    # ê¸°ë³¸ YOLOv8n-pose ëª¨ë¸
                    self.model = YOLO('yolov8n-pose.pt')
                    self.logger.info("âœ… ê¸°ë³¸ YOLOv8n-pose ëª¨ë¸ ë¡œë“œ")
                    
            except ImportError:
                self.logger.warning("âš ï¸ ultralytics íŒ¨í‚¤ì§€ ì—†ìŒ - ì§ì ‘ êµ¬í˜„ ì‚¬ìš©")
                self.model = self._create_simple_yolo_model()
                
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.model = self._create_simple_yolo_model()
    
    def _create_simple_yolo_model(self) -> nn.Module:
        """ê°„ë‹¨í•œ YOLO ìŠ¤íƒ€ì¼ í¬ì¦ˆ ëª¨ë¸"""
        class SimpleYOLOPose(nn.Module):
            def __init__(self):
                super().__init__()
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.MaxPool2d(3, 2, 1),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),
                    nn.AdaptiveAvgPool2d((1, 1))
                )
                self.pose_head = nn.Linear(256, 17 * 3)  # COCO 17 keypoints
                
            def forward(self, x):
                features = self.backbone(x)
                features = features.flatten(1)
                keypoints = self.pose_head(features)
                return keypoints.view(-1, 17, 3)  # [B, 17, 3]
        
        return SimpleYOLOPose().to(self.device)
    
    def predict(self, image: np.ndarray) -> List[Dict]:
        """í¬ì¦ˆ ì˜ˆì¸¡"""
        try:
            if hasattr(self.model, 'predict'):
                # ultralytics YOLO
                results = self.model.predict(image)
                return results
            else:
                # ì§ì ‘ êµ¬í˜„ ëª¨ë¸
                image_tensor = torch.from_numpy(image).float().permute(2, 0, 1).unsqueeze(0)
                image_tensor = image_tensor.to(self.device) / 255.0
                
                with torch.no_grad():
                    keypoints = self.model(image_tensor)
                
                # ê²°ê³¼ í¬ë§·íŒ…
                return [{
                    'keypoints': keypoints[0].cpu().numpy(),
                    'confidence': 0.8
                }]
                
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return []

class RealLightweightPoseModel(nn.Module):
    """ê²½ëŸ‰í™”ëœ ì‹¤ì œ í¬ì¦ˆ ì¶”ì • ëª¨ë¸"""
    
    def __init__(self, num_keypoints: int = 17):
        super(RealLightweightPoseModel, self).__init__()
        self.num_keypoints = num_keypoints
        
        # MobileNet ìŠ¤íƒ€ì¼ backbone
        self.backbone = self._build_lightweight_backbone()
        self.pose_head = self._build_pose_head()
        
        self.logger = logging.getLogger(f"{__name__}.RealLightweightPoseModel")
    
    def _build_lightweight_backbone(self) -> nn.Module:
        """ê²½ëŸ‰ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬"""
        return nn.Sequential(
            # Initial conv
            nn.Conv2d(3, 32, 3, 2, 1), nn.BatchNorm2d(32), nn.ReLU6(),
            
            # Depthwise separable convolutions
            self._depthwise_separable(32, 64, 1),
            self._depthwise_separable(64, 128, 2),
            self._depthwise_separable(128, 128, 1),
            self._depthwise_separable(128, 256, 2),
            self._depthwise_separable(256, 256, 1),
            self._depthwise_separable(256, 512, 2),
            
            # Global average pooling
            nn.AdaptiveAvgPool2d((7, 7))
        )
    
    def _depthwise_separable(self, in_channels: int, out_channels: int, stride: int) -> nn.Module:
        """Depthwise Separable Convolution"""
        return nn.Sequential(
            # Depthwise
            nn.Conv2d(in_channels, in_channels, 3, stride, 1, groups=in_channels),
            nn.BatchNorm2d(in_channels), nn.ReLU6(),
            
            # Pointwise
            nn.Conv2d(in_channels, out_channels, 1, 1, 0),
            nn.BatchNorm2d(out_channels), nn.ReLU6()
        )
    
    def _build_pose_head(self) -> nn.Module:
        """í¬ì¦ˆ ì¶”ì • í—¤ë“œ"""
        return nn.Sequential(
            nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(),
            nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(),
            nn.Conv2d(128, self.num_keypoints, 1, 1, 0),
            nn.Sigmoid()
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ"""
        features = self.backbone(x)
        heatmaps = self.pose_head(features)
        return heatmaps
    
    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu") -> 'RealLightweightPoseModel':
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ìƒì„±"""
        try:
            model = cls()
            
            if os.path.exists(checkpoint_path):
                checkpoint = torch.load(checkpoint_path, map_location=device)
                
                if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                else:
                    state_dict = checkpoint
                
                model.load_state_dict(state_dict, strict=False)
                logger.info(f"âœ… ê²½ëŸ‰ í¬ì¦ˆ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
            else:
                logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ - ë¬´ì‘ìœ„ ì´ˆê¸°í™”: {checkpoint_path}")
            
            model.to(device)
            model.eval()
            
            return model
            
        except Exception as e:
            logger.error(f"âŒ ê²½ëŸ‰ í¬ì¦ˆ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            model = cls()
            model.to(device)
            model.eval()
            return model

# ==============================================
# ğŸ”¥ í¬ì¦ˆ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤
# ==============================================

@dataclass
class PoseMetrics:
    """ì™„ì „í•œ í¬ì¦ˆ ì¸¡ì • ë°ì´í„°"""
    keypoints: List[List[float]] = field(default_factory=list)
    confidence_scores: List[float] = field(default_factory=list)
    bbox: Tuple[int, int, int, int] = field(default_factory=lambda: (0, 0, 0, 0))
    pose_type: PoseType = PoseType.UNKNOWN
    pose_quality: PoseQuality = PoseQuality.POOR
    overall_score: float = 0.0
    
    # ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜
    head_score: float = 0.0
    torso_score: float = 0.0  
    arms_score: float = 0.0
    legs_score: float = 0.0
    
    # ê³ ê¸‰ ë¶„ì„ ì ìˆ˜
    symmetry_score: float = 0.0
    visibility_score: float = 0.0
    pose_angles: Dict[str, float] = field(default_factory=dict)
    body_proportions: Dict[str, float] = field(default_factory=dict)
    
    # ì˜ë¥˜ ì°©ìš© ì í•©ì„±
    suitable_for_fitting: bool = False
    issues: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    
    # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„°
    model_used: str = ""
    processing_time: float = 0.0
    image_resolution: Tuple[int, int] = field(default_factory=lambda: (0, 0))
    ai_confidence: float = 0.0
    
    def calculate_overall_score(self) -> float:
        """ì „ì²´ ì ìˆ˜ ê³„ì‚°"""
        try:
            if not self.confidence_scores:
                self.overall_score = 0.0
                return 0.0
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚° (AI ì‹ ë¢°ë„ ë°˜ì˜)
            base_scores = [
                self.head_score * 0.15,
                self.torso_score * 0.35,
                self.arms_score * 0.25,
                self.legs_score * 0.25
            ]
            
            advanced_scores = [
                self.symmetry_score * 0.3,
                self.visibility_score * 0.7
            ]
            
            base_score = sum(base_scores)
            advanced_score = sum(advanced_scores)
            
            # AI ì‹ ë¢°ë„ë¡œ ê°€ì¤‘
            self.overall_score = (base_score * 0.7 + advanced_score * 0.3) * self.ai_confidence
            return self.overall_score
            
        except Exception as e:
            logger.error(f"ì „ì²´ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            self.overall_score = 0.0
            return 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)

# ==============================================
# ğŸ”¥ ë©”ì¸ PoseEstimationStep í´ë˜ìŠ¤
# ==============================================

class PoseEstimationStep(BaseStepMixin):
    """
    ğŸ”¥ Step 02: ì™„ì „í•œ ì‹¤ì œ AI í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ
    
    âœ… StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… â†’ ì™„ì„±ëœ Step êµ¬ì¡°
    âœ… ì²´í¬í¬ì¸íŠ¸ â†’ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ ë³€í™˜ ì™„ì „ êµ¬í˜„
    âœ… OpenPose, YOLOv8, ê²½ëŸ‰ ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì—”ì§„
    âœ… BaseStepMixin ì™„ì „ ìƒì† - ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´
    âœ… PoseEstimationMixin íŠ¹í™” ê¸°ëŠ¥ í¬í•¨
    âœ… ModelLoader ì™„ì „ ì—°ë™ - ìˆœí™˜ì°¸ì¡° ì—†ìŒ
    âœ… 18ê°œ í‚¤í¬ì¸íŠ¸ OpenPose + COCO 17 ë³€í™˜
    âœ… ì™„ì „í•œ ë¶„ì„ - ê°ë„, ë¹„ìœ¨, ëŒ€ì¹­ì„±, í’ˆì§ˆ í‰ê°€
    âœ… M3 Max ìµœì í™” + Strict Mode
    """
    
    # ì˜ë¥˜ íƒ€ì…ë³„ í¬ì¦ˆ ê°€ì¤‘ì¹˜
    CLOTHING_POSE_WEIGHTS = {
        'shirt': {'arms': 0.4, 'torso': 0.4, 'visibility': 0.2},
        'dress': {'torso': 0.5, 'arms': 0.3, 'legs': 0.2},
        'pants': {'legs': 0.6, 'torso': 0.3, 'visibility': 0.1},
        'jacket': {'arms': 0.5, 'torso': 0.4, 'visibility': 0.1},
        'skirt': {'torso': 0.4, 'legs': 0.4, 'visibility': 0.2},
        'top': {'torso': 0.5, 'arms': 0.4, 'visibility': 0.1},
        'default': {'torso': 0.4, 'arms': 0.3, 'legs': 0.2, 'visibility': 0.1}
    }
    
    # PoseEstimationMixin íŠ¹í™” ì†ì„±ë“¤ (BaseStepMixinì—ì„œ ìƒì†)
    MIXIN_KEYPOINT_NAMES = [
        'nose', 'neck', 'right_shoulder', 'right_elbow', 'right_wrist',
        'left_shoulder', 'left_elbow', 'left_wrist', 'right_hip', 'right_knee',
        'right_ankle', 'left_hip', 'left_knee', 'left_ankle', 'right_eye',
        'left_eye', 'right_ear', 'left_ear'
    ]
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        strict_mode: bool = True,
        **kwargs
    ):
        """
        ì™„ì „í•œ Step 02 ìƒì„±ì - ì˜ì¡´ì„± ì£¼ì… êµ¬ì¡° + PoseEstimationMixin íŠ¹í™”
        
        Args:
            device: ë””ë°”ì´ìŠ¤ ì„¤ì • ('auto', 'mps', 'cuda', 'cpu')
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            strict_mode: ì—„ê²© ëª¨ë“œ (Trueì‹œ AI ì‹¤íŒ¨ â†’ ì¦‰ì‹œ ì—ëŸ¬)
            **kwargs: ì¶”ê°€ ì„¤ì •
        """
        
        # ğŸ”¥ PoseEstimationMixin íŠ¹í™” ì„¤ì • (BaseStepMixin ì´ˆê¸°í™” ì „)
        kwargs.setdefault('step_name', 'PoseEstimationStep')
        kwargs.setdefault('step_number', 2)
        kwargs.setdefault('step_type', 'pose_estimation')
        
        # PoseEstimationMixin íŠ¹í™” ì†ì„±ë“¤
        self.num_keypoints = kwargs.get('num_keypoints', 18)
        self.keypoint_names = self.MIXIN_KEYPOINT_NAMES.copy()
        
        # ğŸ”¥ í•µì‹¬ ì†ì„±ë“¤ì„ BaseStepMixin ì´ˆê¸°í™” ì „ì— ì„¤ì •
        self.step_name = "PoseEstimationStep"
        self.step_number = 2
        self.step_description = "ì™„ì „í•œ ì‹¤ì œ AI ì¸ì²´ í¬ì¦ˆ ì¶”ì • ë° í‚¤í¬ì¸íŠ¸ ê²€ì¶œ"
        self.strict_mode = strict_mode
        self.is_initialized = False
        self.initialization_lock = threading.Lock()
        
        # logger ì†ì„± ë³´ì¥ (BaseStepMixin ì´ˆê¸°í™” ì „)
        if not hasattr(self, 'logger'):
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
        
        # ğŸ”¥ BaseStepMixin ì™„ì „ ì´ˆê¸°í™” (ì˜ì¡´ì„± ì£¼ì… ì¤€ë¹„)
        try:
            super().__init__(device=device, config=config, **kwargs)
            self.logger.info(f"ğŸ¤¸ Pose Estimation Mixin íŠ¹í™” ì´ˆê¸°í™” ì™„ë£Œ - {self.num_keypoints}ê°œ í‚¤í¬ì¸íŠ¸")
        except Exception as e:
            self.logger.error(f"âŒ BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise RuntimeError(f"Strict Mode: BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ğŸ”¥ ì‹œìŠ¤í…œ ì„¤ì • ì´ˆê¸°í™”
        self._setup_system_config(device, config, **kwargs)
        
        # ğŸ”¥ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._initialize_pose_estimation_system()
        
        # ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ ì¶”ì 
        self.dependencies_injected = {
            'model_loader': False,
            'memory_manager': False,
            'data_converter': False,
            'step_interface': False
        }
        
        self.logger.info(f"ğŸ¯ {self.step_name} ìƒì„± ì™„ë£Œ (Strict Mode: {self.strict_mode})")
    
    def _step_specific_warmup(self):
        """Pose Estimation íŠ¹í™” ì›Œë°ì—… (PoseEstimationMixinì—ì„œ ìƒì†)"""
        self.logger.debug("ğŸ”¥ Pose Estimation íŠ¹í™” ì›Œë°ì—…")
        
        # í¬ì¦ˆ ëª¨ë¸ë³„ ì›Œë°ì—…
        if hasattr(self, 'pose_models') and self.pose_models:
            for model_name, model in self.pose_models.items():
                try:
                    if hasattr(model, 'eval'):
                        model.eval()
                    self.logger.debug(f"ğŸ”¥ {model_name} ì›Œë°ì—… ì™„ë£Œ")
                except Exception as e:
                    self.logger.debug(f"âš ï¸ {model_name} ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    async def _step_specific_warmup_async(self):
        """Pose Estimation íŠ¹í™” ì›Œë°ì—… (ë¹„ë™ê¸°, PoseEstimationMixinì—ì„œ ìƒì†)"""
        self.logger.debug("ğŸ”¥ Pose Estimation ë¹„ë™ê¸° íŠ¹í™” ì›Œë°ì—…")
        
        # ë¹„ë™ê¸° í¬ì¦ˆ ëª¨ë¸ ì›Œë°ì—…
        if hasattr(self, 'pose_models') and self.pose_models:
            for model_name, model in self.pose_models.items():
                try:
                    if hasattr(model, 'eval'):
                        model.eval()
                    await asyncio.sleep(0.001)  # ë¹„ë™ê¸° ì²˜ë¦¬
                    self.logger.debug(f"ğŸ”¥ {model_name} ë¹„ë™ê¸° ì›Œë°ì—… ì™„ë£Œ")
                except Exception as e:
                    self.logger.debug(f"âš ï¸ {model_name} ë¹„ë™ê¸° ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ì¶”ê°€ ëˆ„ë½ëœ ê¸°ëŠ¥ë“¤
    # ==============================================
    
    def get_keypoint_names(self) -> List[str]:
        """í‚¤í¬ì¸íŠ¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (PoseEstimationMixin í˜¸í™˜)"""
        return self.keypoint_names.copy()
    
    def get_skeleton_connections(self) -> List[Tuple[int, int]]:
        """ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²° ì •ë³´ ë°˜í™˜"""
        return SKELETON_CONNECTIONS.copy()
    
    def get_keypoint_colors(self) -> List[Tuple[int, int, int]]:
        """í‚¤í¬ì¸íŠ¸ ìƒ‰ìƒ ì •ë³´ ë°˜í™˜"""
        return KEYPOINT_COLORS.copy()
    
    def validate_keypoints_format(self, keypoints: List[List[float]]) -> bool:
        """í‚¤í¬ì¸íŠ¸ í˜•ì‹ ê²€ì¦"""
        try:
            if not isinstance(keypoints, list):
                return False
            
            if len(keypoints) != self.num_keypoints:
                return False
            
            for kp in keypoints:
                if not isinstance(kp, list) or len(kp) != 3:
                    return False
                if not all(isinstance(x, (int, float)) for x in kp):
                    return False
                if not (0 <= kp[2] <= 1):  # ì‹ ë¢°ë„ ë²”ìœ„ ì²´í¬
                    return False
            
            return True
            
        except Exception as e:
            self.logger.debug(f"í‚¤í¬ì¸íŠ¸ í˜•ì‹ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def normalize_keypoints_to_image(self, keypoints: List[List[float]], image_size: Tuple[int, int]) -> List[List[float]]:
        """í‚¤í¬ì¸íŠ¸ë¥¼ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ì •ê·œí™”"""
        try:
            normalized = []
            width, height = image_size
            
            for kp in keypoints:
                if len(kp) >= 3:
                    x = max(0, min(width - 1, kp[0]))
                    y = max(0, min(height - 1, kp[1]))
                    conf = max(0.0, min(1.0, kp[2]))
                    normalized.append([x, y, conf])
                else:
                    normalized.append([0.0, 0.0, 0.0])
            
            return normalized
            
        except Exception as e:
            self.logger.error(f"í‚¤í¬ì¸íŠ¸ ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return [[0.0, 0.0, 0.0] for _ in range(self.num_keypoints)]
    
    def calculate_pose_bbox(self, keypoints: List[List[float]]) -> Tuple[int, int, int, int]:
        """í¬ì¦ˆ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
        try:
            valid_points = [kp for kp in keypoints if len(kp) >= 3 and kp[2] > self.pose_config.get('confidence_threshold', 0.5)]
            
            if not valid_points:
                return (0, 0, 0, 0)
            
            xs = [kp[0] for kp in valid_points]
            ys = [kp[1] for kp in valid_points]
            
            x1, y1 = int(min(xs)), int(min(ys))
            x2, y2 = int(max(xs)), int(max(ys))
            
            # ì—¬ë°± ì¶”ê°€ (10%)
            width = x2 - x1
            height = y2 - y1
            margin_x = int(width * 0.1)
            margin_y = int(height * 0.1)
            
            return (
                max(0, x1 - margin_x),
                max(0, y1 - margin_y),
                x2 + margin_x,
                y2 + margin_y
            )
            
        except Exception as e:
            self.logger.debug(f"í¬ì¦ˆ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return (0, 0, 0, 0)
    
    def estimate_pose_confidence(self, keypoints: List[List[float]]) -> float:
        """í¬ì¦ˆ ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            if not keypoints:
                return 0.0
            
            # ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê°€ì¤‘ì¹˜
            major_weights = {
                0: 0.1,   # nose
                1: 0.15,  # neck
                2: 0.1, 5: 0.1,   # shoulders
                8: 0.15,  # middle_hip
                9: 0.075, 12: 0.075,  # hips
                10: 0.05, 13: 0.05,   # knees
                11: 0.025, 14: 0.025  # ankles
            }
            
            weighted_confidence = 0.0
            total_weight = 0.0
            
            for idx, weight in major_weights.items():
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    weighted_confidence += keypoints[idx][2] * weight
                    total_weight += weight
            
            return weighted_confidence / total_weight if total_weight > 0 else 0.0
            
        except Exception as e:
            self.logger.debug(f"í¬ì¦ˆ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def get_visible_keypoints(self, keypoints: List[List[float]], confidence_threshold: Optional[float] = None) -> List[int]:
        """ê°€ì‹œì ì¸ í‚¤í¬ì¸íŠ¸ ì¸ë±ìŠ¤ ë°˜í™˜"""
        try:
            threshold = confidence_threshold or self.pose_config.get('confidence_threshold', 0.5)
            visible_indices = []
            
            for i, kp in enumerate(keypoints):
                if len(kp) >= 3 and kp[2] > threshold:
                    visible_indices.append(i)
            
            return visible_indices
            
        except Exception as e:
            self.logger.debug(f"ê°€ì‹œì  í‚¤í¬ì¸íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def filter_keypoints_by_confidence(self, keypoints: List[List[float]], min_confidence: float = 0.5) -> List[List[float]]:
        """ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ í‚¤í¬ì¸íŠ¸ í•„í„°ë§"""
        try:
            filtered = []
            
            for kp in keypoints:
                if len(kp) >= 3:
                    if kp[2] >= min_confidence:
                        filtered.append(kp)
                    else:
                        filtered.append([0.0, 0.0, 0.0])  # ë‚®ì€ ì‹ ë¢°ë„ëŠ” ë¬´íš¨ ì²˜ë¦¬
                else:
                    filtered.append([0.0, 0.0, 0.0])
            
            return filtered
            
        except Exception as e:
            self.logger.debug(f"í‚¤í¬ì¸íŠ¸ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            return keypoints
    
    
    def interpolate_missing_keypoints(self, keypoints: List[List[float]]) -> List[List[float]]:
        """ëˆ„ë½ëœ í‚¤í¬ì¸íŠ¸ ë³´ê°„"""
        try:
            interpolated = keypoints.copy()
            confidence_threshold = self.pose_config.get('confidence_threshold', 0.5)
            
            # ëŒ€ì¹­ í‚¤í¬ì¸íŠ¸ ìŒì„ ì´ìš©í•œ ë³´ê°„
            symmetric_pairs = [
                (2, 5), (3, 6), (4, 7),    # shoulders, elbows, wrists
                (9, 12), (10, 13), (11, 14), # hips, knees, ankles
                (15, 16), (17, 18)         # eyes, ears
            ]
            
            for left_idx, right_idx in symmetric_pairs:
                if left_idx >= len(interpolated) or right_idx >= len(interpolated):
                    continue
                
                left_kp = interpolated[left_idx]
                right_kp = interpolated[right_idx]
                
                # í•œìª½ì´ ìœ íš¨í•˜ê³  ë‹¤ë¥¸ ìª½ì´ ë¬´íš¨í•œ ê²½ìš° ëŒ€ì¹­ìœ¼ë¡œ ë³´ê°„
                if (len(left_kp) >= 3 and left_kp[2] > confidence_threshold and
                    (len(right_kp) < 3 or right_kp[2] <= confidence_threshold)):
                    
                    # ì¤‘ì‹¬ì  ê³„ì‚° (neck ë˜ëŠ” middle_hip ì‚¬ìš©)
                    if 1 < len(interpolated) and len(interpolated[1]) >= 3:  # neck
                        center_x = interpolated[1][0]
                    elif 8 < len(interpolated) and len(interpolated[8]) >= 3:  # middle_hip
                        center_x = interpolated[8][0]
                    else:
                        center_x = 320  # ê¸°ë³¸ê°’
                    
                    # ëŒ€ì¹­ ì¢Œí‘œ ê³„ì‚°
                    mirrored_x = 2 * center_x - left_kp[0]
                    interpolated[right_idx] = [mirrored_x, left_kp[1], left_kp[2] * 0.8]  # ì‹ ë¢°ë„ëŠ” ì•½ê°„ ë‚®ì¶¤
                
                elif (len(right_kp) >= 3 and right_kp[2] > confidence_threshold and
                      (len(left_kp) < 3 or left_kp[2] <= confidence_threshold)):
                    
                    # ë°˜ëŒ€ ë°©í–¥ ë³´ê°„
                    if 1 < len(interpolated) and len(interpolated[1]) >= 3:
                        center_x = interpolated[1][0]
                    elif 8 < len(interpolated) and len(interpolated[8]) >= 3:
                        center_x = interpolated[8][0]
                    else:
                        center_x = 320
                    
                    mirrored_x = 2 * center_x - right_kp[0]
                    interpolated[left_idx] = [mirrored_x, right_kp[1], right_kp[2] * 0.8]
            
            # ì—°ê²°ëœ í‚¤í¬ì¸íŠ¸ë¥¼ ì´ìš©í•œ ì„ í˜• ë³´ê°„
            # neckì´ ìˆê³  shouldersê°€ ìˆìœ¼ë©´ ì¤‘ê°„ í‚¤í¬ì¸íŠ¸ë“¤ ë³´ê°„ ê°€ëŠ¥
            if (1 < len(interpolated) and 2 < len(interpolated) and 5 < len(interpolated) and
                all(len(kp) >= 3 and kp[2] > confidence_threshold for kp in [interpolated[1], interpolated[2], interpolated[5]])):
                
                # elbow ë³´ê°„
                if len(interpolated[3]) < 3 or interpolated[3][2] <= confidence_threshold:
                    # right shoulderì™€ right wristì˜ ì¤‘ì 
                    if 4 < len(interpolated) and len(interpolated[4]) >= 3 and interpolated[4][2] > confidence_threshold:
                        mid_x = (interpolated[2][0] + interpolated[4][0]) / 2
                        mid_y = (interpolated[2][1] + interpolated[4][1]) / 2
                        interpolated[3] = [mid_x, mid_y, min(interpolated[2][2], interpolated[4][2]) * 0.7]
                
                if len(interpolated[6]) < 3 or interpolated[6][2] <= confidence_threshold:
                    # left shoulderì™€ left wristì˜ ì¤‘ì 
                    if 7 < len(interpolated) and len(interpolated[7]) >= 3 and interpolated[7][2] > confidence_threshold:
                        mid_x = (interpolated[5][0] + interpolated[7][0]) / 2
                        mid_y = (interpolated[5][1] + interpolated[7][1]) / 2
                        interpolated[6] = [mid_x, mid_y, min(interpolated[5][2], interpolated[7][2]) * 0.7]
            
            return interpolated
            
        except Exception as e:
            self.logger.debug(f"í‚¤í¬ì¸íŠ¸ ë³´ê°„ ì‹¤íŒ¨: {e}")
            return keypoints
    
    def smooth_keypoints_temporal(self, current_keypoints: List[List[float]], 
                                 previous_keypoints: Optional[List[List[float]]] = None,
                                 smoothing_factor: float = 0.7) -> List[List[float]]:
        """ì‹œê°„ì  í‚¤í¬ì¸íŠ¸ ìŠ¤ë¬´ë”© (ë™ì˜ìƒ ì²˜ë¦¬ìš©)"""
        try:
            if not previous_keypoints or len(previous_keypoints) != len(current_keypoints):
                return current_keypoints
            
            smoothed = []
            confidence_threshold = self.pose_config.get('confidence_threshold', 0.5)
            
            for i, (curr_kp, prev_kp) in enumerate(zip(current_keypoints, previous_keypoints)):
                if (len(curr_kp) >= 3 and len(prev_kp) >= 3 and
                    curr_kp[2] > confidence_threshold and prev_kp[2] > confidence_threshold):
                    
                    # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìŠ¤ë¬´ë”©
                    smooth_x = curr_kp[0] * (1 - smoothing_factor) + prev_kp[0] * smoothing_factor
                    smooth_y = curr_kp[1] * (1 - smoothing_factor) + prev_kp[1] * smoothing_factor
                    smooth_conf = max(curr_kp[2], prev_kp[2])  # ì‹ ë¢°ë„ëŠ” ìµœëŒ€ê°’ ì‚¬ìš©
                    
                    smoothed.append([smooth_x, smooth_y, smooth_conf])
                else:
                    smoothed.append(curr_kp)
            
            return smoothed
            
        except Exception as e:
            self.logger.debug(f"ì‹œê°„ì  ìŠ¤ë¬´ë”© ì‹¤íŒ¨: {e}")
            return current_keypoints
    
    def export_keypoints_to_format(self, keypoints: List[List[float]], 
                                  format_type: str = "openpose") -> Dict[str, Any]:
        """í‚¤í¬ì¸íŠ¸ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            if format_type.lower() == "openpose":
                return {
                    "version": 1.3,
                    "people": [{
                        "pose_keypoints_2d": [coord for kp in keypoints for coord in kp],
                        "face_keypoints_2d": [],
                        "hand_left_keypoints_2d": [],
                        "hand_right_keypoints_2d": [],
                        "pose_keypoints_3d": [],
                        "face_keypoints_3d": [],
                        "hand_left_keypoints_3d": [],
                        "hand_right_keypoints_3d": []
                    }]
                }
            
            elif format_type.lower() == "coco":
                coco_keypoints = convert_keypoints_to_coco(keypoints)
                return {
                    "keypoints": [coord for kp in coco_keypoints for coord in kp],
                    "num_keypoints": len([kp for kp in coco_keypoints if kp[2] > 0]),
                    "score": self.estimate_pose_confidence(keypoints)
                }
            
            elif format_type.lower() == "mediapipe":
                # MediaPipe í˜•ì‹ ê·¼ì‚¬
                return {
                    "pose_landmarks": [
                        {"x": kp[0], "y": kp[1], "z": 0.0, "visibility": kp[2]}
                        for kp in keypoints
                    ],
                    "pose_world_landmarks": []
                }
            
            elif format_type.lower() == "custom":
                return {
                    "keypoints": keypoints,
                    "keypoint_names": self.keypoint_names,
                    "connections": SKELETON_CONNECTIONS,
                    "confidence_threshold": self.pose_config.get('confidence_threshold', 0.5),
                    "image_resolution": getattr(self, 'last_image_resolution', (640, 480)),
                    "model_used": getattr(self, 'active_model', 'unknown'),
                    "processing_time": getattr(self, 'last_processing_time', 0.0)
                }
            
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format_type}")
                
        except Exception as e:
            self.logger.error(f"í‚¤í¬ì¸íŠ¸ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def calculate_pose_similarity(self, keypoints1: List[List[float]], 
                                keypoints2: List[List[float]]) -> float:
        """ë‘ í¬ì¦ˆ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            if len(keypoints1) != len(keypoints2):
                return 0.0
            
            confidence_threshold = self.pose_config.get('confidence_threshold', 0.5)
            similarities = []
            
            for kp1, kp2 in zip(keypoints1, keypoints2):
                if (len(kp1) >= 3 and len(kp2) >= 3 and
                    kp1[2] > confidence_threshold and kp2[2] > confidence_threshold):
                    
                    # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°
                    distance = np.sqrt((kp1[0] - kp2[0])**2 + (kp1[1] - kp2[1])**2)
                    
                    # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬ë„ ë†’ìŒ)
                    max_distance = 100.0  # ìµœëŒ€ ê±°ë¦¬ ì„ê³„ê°’
                    similarity = max(0.0, 1.0 - distance / max_distance)
                    
                    # ì‹ ë¢°ë„ ê°€ì¤‘ ì ìš©
                    weighted_similarity = similarity * min(kp1[2], kp2[2])
                    similarities.append(weighted_similarity)
            
            return float(np.mean(similarities)) if similarities else 0.0
            
        except Exception as e:
            self.logger.debug(f"í¬ì¦ˆ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def get_pose_statistics(self) -> Dict[str, Any]:
        """í¬ì¦ˆ ì¶”ì • í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            stats = {
                "step_info": {
                    "step_name": self.step_name,
                    "step_number": self.step_number,
                    "is_initialized": self.is_initialized,
                    "strict_mode": self.strict_mode
                },
                "model_info": {
                    "active_model": getattr(self, 'active_model', None),
                    "loaded_models": list(getattr(self, 'pose_models', {}).keys()),
                    "num_keypoints": self.num_keypoints,
                    "device": self.device
                },
                "performance_info": {
                    "cache_size": len(getattr(self, 'prediction_cache', {})),
                    "cache_max_size": getattr(self, 'cache_max_size', 0),
                    "optimization_level": getattr(self, 'optimization_level', 'unknown'),
                    "memory_gb": getattr(self, 'memory_gb', 0.0)
                },
                "configuration": {
                    "confidence_threshold": self.pose_config.get('confidence_threshold', 0.5),
                    "visualization_enabled": self.pose_config.get('visualization_enabled', True),
                    "cache_enabled": self.pose_config.get('cache_enabled', True),
                    "detailed_analysis": self.pose_config.get('detailed_analysis', True)
                },
                "dependencies": {
                    "injected_dependencies": getattr(self, 'dependencies_injected', {}),
                    "model_loader_available": MODEL_LOADER_AVAILABLE,
                    "memory_manager_available": MEMORY_MANAGER_AVAILABLE,
                    "data_converter_available": DATA_CONVERTER_AVAILABLE
                },
                "supported_features": {
                    "keypoint_formats": ["openpose_18", "coco_17"],
                    "export_formats": ["openpose", "coco", "mediapipe", "custom"],
                    "analysis_features": [
                        "pose_angles", "body_proportions", "symmetry_score",
                        "visibility_score", "clothing_suitability", "pose_similarity"
                    ],
                    "interpolation_methods": ["symmetric", "linear", "temporal"],
                    "visualization_features": ["keypoints", "skeleton", "heatmap", "bbox"]
                }
            }
            
            return stats
            
        except Exception as e:
            self.logger.error(f"í¬ì¦ˆ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def benchmark_pose_estimation(self, test_images: List[Union[Image.Image, np.ndarray]], 
                                num_runs: int = 5) -> Dict[str, Any]:
        """í¬ì¦ˆ ì¶”ì • ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        try:
            if not self.is_initialized:
                return {"error": "Stepì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"}
            
            if not test_images:
                # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
                test_images = [np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8) for _ in range(3)]
            
            results = {
                "total_images": len(test_images),
                "num_runs": num_runs,
                "per_image_times": [],
                "per_run_times": [],
                "total_time": 0.0,
                "average_time_per_image": 0.0,
                "fps": 0.0,
                "successful_detections": 0,
                "failed_detections": 0,
                "average_keypoints_detected": 0.0,
                "average_confidence": 0.0
            }
            
            total_start_time = time.time()
            total_keypoints = 0
            total_confidences = []
            
            for run in range(num_runs):
                run_start_time = time.time()
                
                for i, test_image in enumerate(test_images):
                    try:
                        # PIL Imageë¡œ ë³€í™˜
                        if isinstance(test_image, np.ndarray):
                            pil_image = Image.fromarray(test_image)
                        else:
                            pil_image = test_image
                        
                        # í¬ì¦ˆ ì¶”ì • ì‹¤í–‰
                        image_start_time = time.time()
                        result = asyncio.run(self.process(pil_image))
                        image_time = time.time() - image_start_time
                        
                        results["per_image_times"].append(image_time)
                        
                        if result.get("success", False):
                            results["successful_detections"] += 1
                            keypoints = result.get("keypoints", [])
                            total_keypoints += len([kp for kp in keypoints if len(kp) > 2 and kp[2] > 0.5])
                            
                            if result.get("confidence_scores"):
                                total_confidences.extend(result["confidence_scores"])
                        else:
                            results["failed_detections"] += 1
                        
                    except Exception as e:
                        self.logger.debug(f"ë²¤ì¹˜ë§ˆí¬ ì´ë¯¸ì§€ {i} ì‹¤íŒ¨: {e}")
                        results["failed_detections"] += 1
                
                run_time = time.time() - run_start_time
                results["per_run_times"].append(run_time)
            
            # í†µê³„ ê³„ì‚°
            total_time = time.time() - total_start_time
            total_processed = len(test_images) * num_runs
            
            results["total_time"] = total_time
            results["average_time_per_image"] = total_time / total_processed if total_processed > 0 else 0.0
            results["fps"] = total_processed / total_time if total_time > 0 else 0.0
            results["average_keypoints_detected"] = total_keypoints / max(1, results["successful_detections"])
            results["average_confidence"] = np.mean(total_confidences) if total_confidences else 0.0
            
            results["summary"] = {
                "success_rate": results["successful_detections"] / total_processed if total_processed > 0 else 0.0,
                "avg_fps": results["fps"],
                "performance_grade": "Excellent" if results["fps"] > 30 else "Good" if results["fps"] > 15 else "Fair" if results["fps"] > 5 else "Poor"
            }
            
            self.logger.info(f"ğŸƒ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {results['fps']:.1f} FPS, ì„±ê³µë¥ : {results['summary']['success_rate']:.1%}")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
            
            width, height = image_size
            heatmap = np.zeros((self.num_keypoints, height, width), dtype=np.float32)
            
            for i, kp in enumerate(keypoints):
                if len(kp) >= 3 and kp[2] > 0.1:  # ìµœì†Œ ì‹ ë¢°ë„
                    x, y, conf = int(kp[0]), int(kp[1]), kp[2]
                    
                    # ê°€ìš°ì‹œì•ˆ íˆíŠ¸ë§µ ìƒì„±
                    y_range = np.arange(height).reshape(-1, 1)
                    x_range = np.arange(width).reshape(1, -1)
                    
                    gaussian = np.exp(-((x_range - x)**2 + (y_range - y)**2) / (2 * sigma**2))
                    heatmap[i] = gaussian * conf
            
            return heatmap
            
        except Exception as e:
            self.logger.error(f"í¬ì¦ˆ íˆíŠ¸ë§µ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _setup_system_config(self, device: Optional[str], config: Optional[Dict[str, Any]], **kwargs):
        """ì‹œìŠ¤í…œ ì„¤ì • ì´ˆê¸°í™”"""
        try:
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            if device is None or device == "auto":
                if TORCH_AVAILABLE and torch.backends.mps.is_available():
                    self.device = "mps"
                    self.is_m3_max = True
                elif TORCH_AVAILABLE and torch.cuda.is_available():
                    self.device = "cuda"
                    self.is_m3_max = False
                else:
                    self.device = "cpu"
                    self.is_m3_max = False
            else:
                self.device = device
                self.is_m3_max = device == "mps"
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            if PSUTIL_AVAILABLE:
                try:
                    memory = psutil.virtual_memory()
                    self.memory_gb = memory.total / (1024**3)
                except:
                    self.memory_gb = 16.0
            else:
                self.memory_gb = 16.0
            
            # ì„¤ì • í†µí•©
            self.config = config or {}
            self.config.update(kwargs)
            
            # ê¸°ë³¸ ì„¤ì • ì ìš©
            default_config = {
                'confidence_threshold': 0.5,
                'visualization_enabled': True,
                'return_analysis': True,
                'cache_enabled': True,
                'detailed_analysis': True,
                'strict_mode': self.strict_mode,
                'real_ai_only': True
            }
            
            for key, default_value in default_config.items():
                if key not in self.config:
                    self.config[key] = default_value
            
            self.logger.info(f"ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ: {self.device}, M3 Max: {self.is_m3_max}, ë©”ëª¨ë¦¬: {self.memory_gb:.1f}GB")
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ ì„¤ì • ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise RuntimeError(f"Strict Mode: ì‹œìŠ¤í…œ ì„¤ì • ì‹¤íŒ¨: {e}")
            
            # ì•ˆì „í•œ í´ë°± ì„¤ì •
            self.device = "cpu"
            self.is_m3_max = False
            self.memory_gb = 16.0
            self.config = default_config
    
    def _initialize_pose_estimation_system(self):
        """í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # í¬ì¦ˆ ì‹œìŠ¤í…œ ì„¤ì •
            self.pose_config = {
                'model_priority': [
                    'pose_estimation_openpose', 
                    'pose_estimation_sk', 
                    'pose_estimation_lightweight'
                ],
                'confidence_threshold': self.config.get('confidence_threshold', 0.5),
                'visualization_enabled': self.config.get('visualization_enabled', True),
                'return_analysis': self.config.get('return_analysis', True),
                'cache_enabled': self.config.get('cache_enabled', True),
                'detailed_analysis': self.config.get('detailed_analysis', True),
                'real_ai_only': True
            }
            
            # ìµœì í™” ë ˆë²¨ ì„¤ì •
            if self.is_m3_max:
                self.optimization_level = 'maximum'
                self.batch_processing = True
                self.use_neural_engine = True
            elif self.memory_gb >= 32:
                self.optimization_level = 'high'
                self.batch_processing = True
                self.use_neural_engine = False
            else:
                self.optimization_level = 'basic'
                self.batch_processing = False
                self.use_neural_engine = False
            
            # ìºì‹œ ì‹œìŠ¤í…œ
            cache_size = min(100 if self.is_m3_max else 50, int(self.memory_gb * 2))
            self.prediction_cache = {}
            self.cache_max_size = cache_size
            
            # AI ëª¨ë¸ ì €ì¥ì†Œ ì´ˆê¸°í™”
            self.pose_models = {}
            self.active_model = None
            
            self.logger.info(f"ğŸ¯ í¬ì¦ˆ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ - ìµœì í™”: {self.optimization_level}")
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise RuntimeError(f"Strict Mode: í¬ì¦ˆ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # ìµœì†Œí•œì˜ ì„¤ì •
            self.pose_config = {'confidence_threshold': 0.5, 'real_ai_only': True}
            self.optimization_level = 'basic'
            self.prediction_cache = {}
            self.cache_max_size = 50
            self.pose_models = {}
            self.active_model = None
    
    # ==============================================
    # ğŸ”¥ ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤ (BaseStepMixin íŒ¨í„´)
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.model_loader = model_loader
            self.dependencies_injected['model_loader'] = True
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            
            # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„±
            if hasattr(model_loader, 'create_step_interface'):
                try:
                    self.model_interface = model_loader.create_step_interface(self.step_name)
                    self.dependencies_injected['step_interface'] = True
                    self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì£¼ì… ì™„ë£Œ")
                except Exception as e:
                    self.logger.debug(f"Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    self.model_interface = model_loader
            else:
                self.model_interface = model_loader
            
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise RuntimeError(f"Strict Mode: ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.memory_manager = memory_manager
            self.dependencies_injected['memory_manager'] = True
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.data_converter = data_converter
            self.dependencies_injected['data_converter'] = True
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def get_injected_dependencies(self) -> Dict[str, bool]:
        """ì£¼ì…ëœ ì˜ì¡´ì„± ìƒíƒœ ë°˜í™˜"""
        return self.dependencies_injected.copy()
    
    # ==============================================
    # ğŸ”¥ Step ìš”êµ¬ì‚¬í•­ ë° ì´ˆê¸°í™”
    # ==============================================
    
    def _get_step_model_requirements(self) -> Dict[str, Any]:
        """step_model_requests.py ì™„ë²½ í˜¸í™˜ ìš”êµ¬ì‚¬í•­"""
        return {
            "step_name": "PoseEstimationStep",
            "model_name": "pose_estimation_openpose",
            "step_priority": "HIGH",
            "model_class": "OpenPoseModel",
            "input_size": (368, 368),
            "num_classes": 18,
            "output_format": "keypoints_heatmap",
            "device": self.device,
            "precision": "fp16" if self.is_m3_max else "fp32",
            
            # ì²´í¬í¬ì¸íŠ¸ íƒì§€ íŒ¨í„´
            "checkpoint_patterns": [
                r".*openpose\.pth$",
                r".*yolov8.*pose\.pt$",
                r".*pose.*model.*\.pth$",
                r".*body.*pose.*\.pth$"
            ],
            "file_extensions": [".pth", ".pt", ".tflite"],
            "size_range_mb": (6.5, 199.6),
            
            # ìµœì í™” íŒŒë¼ë¯¸í„°
            "optimization_params": {
                "batch_size": 1,
                "memory_fraction": 0.25,
                "inference_threads": 4,
                "enable_tensorrt": self.is_m3_max,
                "enable_neural_engine": self.is_m3_max,
                "precision": "fp16" if self.is_m3_max else "fp32"
            },
            
            # ëŒ€ì²´ ëª¨ë¸ë“¤
            "alternative_models": [
                "pose_estimation_sk",
                "pose_estimation_lightweight"
            ],
            
            # ë©”íƒ€ë°ì´í„°
            "metadata": {
                "description": "ì™„ì „í•œ ì‹¤ì œ AI 18ê°œ í‚¤í¬ì¸íŠ¸ í¬ì¦ˆ ì¶”ì •",
                "keypoints_format": "openpose_18",
                "supports_hands": True,
                "supports_face": True,
                "clothing_types_supported": list(self.CLOTHING_POSE_WEIGHTS.keys()),
                "quality_assessment": True,
                "visualization_support": True,
                "strict_mode_compatible": True,
                "real_ai_only": True,
                "analysis_features": [
                    "pose_angles", "body_proportions", "symmetry_score", 
                    "visibility_score", "clothing_suitability"
                ],
                "format_conversion": ["coco_17", "openpose_18"]
            }
        }
    
    async def initialize(self) -> bool:
        """
        ì™„ì „í•œ ì‹¤ì œ AI ëª¨ë¸ ì´ˆê¸°í™” - ì˜ì¡´ì„± ì£¼ì… êµ¬ì¡°
        
        Returns:
            bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            with self.initialization_lock:
                if self.is_initialized:
                    return True
                
                self.logger.info(f"ğŸš€ {self.step_name} ì™„ì „í•œ AI ì´ˆê¸°í™” ì‹œì‘")
                start_time = time.time()
                
                # ğŸ”¥ 1. ì˜ì¡´ì„± ì£¼ì… ê²€ì¦
                if not hasattr(self, 'model_loader') or not self.model_loader:
                    error_msg = "ModelLoader ì˜ì¡´ì„± ì£¼ì… í•„ìš” - StepFactoryë¥¼ í†µí•´ ìƒì„±í•˜ì„¸ìš”"
                    self.logger.error(f"âŒ {error_msg}")
                    if self.strict_mode:
                        raise RuntimeError(f"Strict Mode: {error_msg}")
                    
                    # ìë™ ì˜ì¡´ì„± í•´ê²° ì‹œë„
                    try:
                        self.model_loader = get_global_model_loader()
                        if self.model_loader:
                            self.model_interface = self.model_loader
                            self.logger.info("âœ… ìë™ ì˜ì¡´ì„± í•´ê²° ì„±ê³µ")
                        else:
                            return False
                    except Exception as e:
                        self.logger.error(f"âŒ ìë™ ì˜ì¡´ì„± í•´ê²° ì‹¤íŒ¨: {e}")
                        return False
                
                # ğŸ”¥ 2. Step ìš”êµ¬ì‚¬í•­ ë“±ë¡
                requirements = self._get_step_model_requirements()
                await self._register_step_requirements(requirements)
                
                # ğŸ”¥ 3. ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ (ì²´í¬í¬ì¸íŠ¸ â†’ ëª¨ë¸ í´ë˜ìŠ¤ ë³€í™˜)
                models_loaded = await self._load_real_ai_models(requirements)
                
                if not models_loaded:
                    error_msg = "ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ì—†ìŒ"
                    self.logger.error(f"âŒ {error_msg}")
                    if self.strict_mode:
                        raise RuntimeError(f"Strict Mode: {error_msg}")
                    return False
                
                # ğŸ”¥ 4. AI ëª¨ë¸ ê²€ì¦ ë° ìµœì í™”
                validation_success = await self._validate_ai_models()
                if validation_success:
                    self._apply_ai_model_optimization()
                
                # ğŸ”¥ 5. AI ëª¨ë¸ ì›Œë°ì—…
                warmup_success = await self._warmup_ai_models()
                
                self.is_initialized = True
                elapsed_time = time.time() - start_time
                
                self.logger.info(f"âœ… {self.step_name} ì™„ì „í•œ AI ì´ˆê¸°í™” ì„±ê³µ ({elapsed_time:.2f}ì´ˆ)")
                self.logger.info(f"ğŸ¤– ë¡œë“œëœ AI ëª¨ë¸: {list(self.pose_models.keys())}")
                self.logger.info(f"ğŸ¯ í™œì„± AI ëª¨ë¸: {self.active_model}")
                self.logger.info(f"ğŸ’‰ ì£¼ì…ëœ ì˜ì¡´ì„±: {sum(self.dependencies_injected.values())}/4")
                
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} AI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise
            return False
    
    async def _register_step_requirements(self, requirements: Dict[str, Any]) -> bool:
        """Step ìš”êµ¬ì‚¬í•­ ë“±ë¡"""
        try:
            if hasattr(self.model_interface, 'register_step_requirements'):
                await self.model_interface.register_step_requirements(
                    step_name=requirements["step_name"],
                    requirements=requirements
                )
                self.logger.info("âœ… Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì„±ê³µ")
                return True
            else:
                self.logger.debug("âš ï¸ ModelInterfaceì— register_step_requirements ë©”ì„œë“œ ì—†ìŒ")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ Step ìš”êµ¬ì‚¬í•­ ë“±ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    async def _load_real_ai_models(self, requirements: Dict[str, Any]) -> bool:
        """ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ - ì²´í¬í¬ì¸íŠ¸ â†’ ëª¨ë¸ í´ë˜ìŠ¤ ë³€í™˜ ì™„ì „ êµ¬í˜„"""
        try:
            self.pose_models = {}
            self.active_model = None
            
            self.logger.info("ğŸ§  ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ ì‹œì‘ (ì²´í¬í¬ì¸íŠ¸ â†’ ëª¨ë¸ ë³€í™˜)...")
            
            # 1. ìš°ì„ ìˆœìœ„ ëª¨ë¸ ë¡œë“œ
            primary_model = requirements["model_name"]
            
            try:
                real_ai_model = await self._load_and_convert_checkpoint_to_model(primary_model)
                if real_ai_model:
                    self.pose_models[primary_model] = real_ai_model
                    self.active_model = primary_model
                    self.logger.info(f"âœ… ì£¼ AI ëª¨ë¸ ë¡œë“œ ë° ë³€í™˜ ì„±ê³µ: {primary_model}")
                else:
                    raise ValueError(f"ì£¼ ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨: {primary_model}")
                    
            except Exception as e:
                self.logger.error(f"âŒ ì£¼ AI ëª¨ë¸ ì‹¤íŒ¨: {e}")
                
                # ëŒ€ì²´ AI ëª¨ë¸ ì‹œë„
                for alt_model in requirements["alternative_models"]:
                    try:
                        real_ai_model = await self._load_and_convert_checkpoint_to_model(alt_model)
                        if real_ai_model:
                            self.pose_models[alt_model] = real_ai_model
                            self.active_model = alt_model
                            self.logger.info(f"âœ… ëŒ€ì²´ AI ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {alt_model}")
                            break
                    except Exception as alt_e:
                        self.logger.warning(f"âš ï¸ ëŒ€ì²´ AI ëª¨ë¸ ì‹¤íŒ¨: {alt_model} - {alt_e}")
                        continue
            
            # 2. AI ëª¨ë¸ ë¡œë“œ ê²€ì¦
            if not self.pose_models:
                self.logger.error("âŒ ëª¨ë“  AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                return False
            
            self.logger.info(f"âœ… {len(self.pose_models)}ê°œ ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def _load_and_convert_checkpoint_to_model(self, model_name: str) -> Optional[nn.Module]:
        """ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë¡œ ë³€í™˜ - Step 01 ì´ìŠˆ ì™„ì „ í•´ê²°"""
        try:
            self.logger.info(f"ğŸ”„ {model_name} ì²´í¬í¬ì¸íŠ¸ â†’ AI ëª¨ë¸ ë³€í™˜ ì‹œì‘")
            
            # 1. ModelLoaderì—ì„œ ì²´í¬í¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
            if hasattr(self.model_interface, 'get_model'):
                checkpoint_data = self.model_interface.get_model(model_name)
                if not checkpoint_data:
                    self.logger.warning(f"âš ï¸ {model_name} ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ì—†ìŒ")
                    return None
            else:
                self.logger.error(f"âŒ ModelInterfaceì— get_model ë©”ì„œë“œ ì—†ìŒ")
                return None
            
            # 2. ì²´í¬í¬ì¸íŠ¸ê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° â†’ ì‹¤ì œ AI ëª¨ë¸ë¡œ ë³€í™˜
            if isinstance(checkpoint_data, dict):
                self.logger.info(f"ğŸ”§ {model_name} ë”•ì…”ë„ˆë¦¬ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‹¤ì œ AI ëª¨ë¸ë¡œ ë³€í™˜")
                
                # ëª¨ë¸ íƒ€ì…ë³„ ë³€í™˜
                if 'openpose' in model_name.lower():
                    real_model = await self._convert_checkpoint_to_openpose_model(checkpoint_data, model_name)
                elif 'yolo' in model_name.lower() or 'sk' in model_name.lower():
                    real_model = await self._convert_checkpoint_to_yolo_model(checkpoint_data, model_name)
                elif 'lightweight' in model_name.lower():
                    real_model = await self._convert_checkpoint_to_lightweight_model(checkpoint_data, model_name)
                else:
                    # ê¸°ë³¸ OpenPoseë¡œ ì²˜ë¦¬
                    real_model = await self._convert_checkpoint_to_openpose_model(checkpoint_data, model_name)
                
                if real_model:
                    self.logger.info(f"âœ… {model_name} ì²´í¬í¬ì¸íŠ¸ â†’ AI ëª¨ë¸ ë³€í™˜ ì„±ê³µ")
                    return real_model
                else:
                    self.logger.error(f"âŒ {model_name} ì²´í¬í¬ì¸íŠ¸ â†’ AI ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨")
                    return None
            
            # 3. ì´ë¯¸ ëª¨ë¸ ê°ì²´ì¸ ê²½ìš°
            elif hasattr(checkpoint_data, '__call__') or hasattr(checkpoint_data, 'forward'):
                self.logger.info(f"âœ… {model_name} ì´ë¯¸ AI ëª¨ë¸ ê°ì²´ì„")
                return checkpoint_data
            
            # 4. ê¸°íƒ€ í˜•ì‹
            else:
                self.logger.warning(f"âš ï¸ {model_name} ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹: {type(checkpoint_data)}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ {model_name} ì²´í¬í¬ì¸íŠ¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None
    
    async def _convert_checkpoint_to_openpose_model(self, checkpoint_data: Dict, model_name: str) -> Optional[RealOpenPoseModel]:
        """ì²´í¬í¬ì¸íŠ¸ë¥¼ OpenPose AI ëª¨ë¸ë¡œ ë³€í™˜"""
        try:
            self.logger.info(f"ğŸ”§ OpenPose AI ëª¨ë¸ ë³€í™˜: {model_name}")
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            checkpoint_path = None
            if 'checkpoint_path' in checkpoint_data:
                checkpoint_path = checkpoint_data['checkpoint_path']
            elif 'path' in checkpoint_data:
                checkpoint_path = checkpoint_data['path']
            elif 'file_path' in checkpoint_data:
                checkpoint_path = checkpoint_data['file_path']
            
            # ì‹¤ì œ OpenPose ëª¨ë¸ ìƒì„±
            if checkpoint_path and os.path.exists(str(checkpoint_path)):
                real_openpose_model = RealOpenPoseModel.from_checkpoint(str(checkpoint_path), self.device)
                self.logger.info(f"âœ… OpenPose AI ëª¨ë¸ ìƒì„± ì„±ê³µ: {checkpoint_path}")
                return real_openpose_model
            else:
                # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ ì§ì ‘ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹œë„
                self.logger.info("ğŸ”§ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ ì§ì ‘ OpenPose AI ëª¨ë¸ ìƒì„±")
                real_openpose_model = RealOpenPoseModel()
                
                # ê°€ì¤‘ì¹˜ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë¡œë“œ
                if 'state_dict' in checkpoint_data:
                    try:
                        real_openpose_model.load_state_dict(checkpoint_data['state_dict'], strict=False)
                        self.logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨ - ë¬´ì‘ìœ„ ì´ˆê¸°í™” ì‚¬ìš©: {e}")
                
                real_openpose_model.to(self.device)
                real_openpose_model.eval()
                
                return real_openpose_model
                
        except Exception as e:
            self.logger.error(f"âŒ OpenPose AI ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None
    
    async def _convert_checkpoint_to_yolo_model(self, checkpoint_data: Dict, model_name: str) -> Optional[RealYOLOv8PoseModel]:
        """ì²´í¬í¬ì¸íŠ¸ë¥¼ YOLOv8 AI ëª¨ë¸ë¡œ ë³€í™˜"""
        try:
            self.logger.info(f"ğŸ”§ YOLOv8 AI ëª¨ë¸ ë³€í™˜: {model_name}")
            
            checkpoint_path = ""
            if 'checkpoint_path' in checkpoint_data:
                checkpoint_path = str(checkpoint_data['checkpoint_path'])
            elif 'path' in checkpoint_data:
                checkpoint_path = str(checkpoint_data['path'])
            
            real_yolo_model = RealYOLOv8PoseModel(checkpoint_path, self.device)
            self.logger.info(f"âœ… YOLOv8 AI ëª¨ë¸ ìƒì„± ì„±ê³µ")
            
            return real_yolo_model
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 AI ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None
    
    async def _convert_checkpoint_to_lightweight_model(self, checkpoint_data: Dict, model_name: str) -> Optional[RealLightweightPoseModel]:
        """ì²´í¬í¬ì¸íŠ¸ë¥¼ ê²½ëŸ‰ AI ëª¨ë¸ë¡œ ë³€í™˜"""
        try:
            self.logger.info(f"ğŸ”§ ê²½ëŸ‰ AI ëª¨ë¸ ë³€í™˜: {model_name}")
            
            checkpoint_path = ""
            if 'checkpoint_path' in checkpoint_data:
                checkpoint_path = str(checkpoint_data['checkpoint_path'])
            elif 'path' in checkpoint_data:
                checkpoint_path = str(checkpoint_data['path'])
            
            real_lightweight_model = RealLightweightPoseModel.from_checkpoint(checkpoint_path, self.device)
            self.logger.info(f"âœ… ê²½ëŸ‰ AI ëª¨ë¸ ìƒì„± ì„±ê³µ")
            
            return real_lightweight_model
            
        except Exception as e:
            self.logger.error(f"âŒ ê²½ëŸ‰ AI ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None
    
    async def _validate_ai_models(self) -> bool:
        """ë¡œë“œëœ AI ëª¨ë¸ ê²€ì¦"""
        try:
            if not self.pose_models or not self.active_model:
                self.logger.error("âŒ ê²€ì¦í•  AI ëª¨ë¸ ì—†ìŒ")
                return False
            
            active_model = self.pose_models.get(self.active_model)
            if not active_model:
                self.logger.error(f"âŒ í™œì„± AI ëª¨ë¸ ì—†ìŒ: {self.active_model}")
                return False
            
            # AI ëª¨ë¸ íŠ¹ì„± ê²€ì¦
            model_type = type(active_model).__name__
            self.logger.info(f"ğŸ” AI ëª¨ë¸ íƒ€ì… ê²€ì¦: {model_type}")
            
            # í˜¸ì¶œ ê°€ëŠ¥ì„± ê²€ì¦
            if not (hasattr(active_model, '__call__') or hasattr(active_model, 'forward') or hasattr(active_model, 'predict')):
                self.logger.error(f"âŒ AI ëª¨ë¸ì´ í˜¸ì¶œ ë¶ˆê°€ëŠ¥: {model_type}")
                return False
            
            self.logger.info(f"âœ… AI ëª¨ë¸ ê²€ì¦ ì„±ê³µ: {self.active_model} ({model_type})")
            return True
                
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def _apply_ai_model_optimization(self):
        """AI ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©"""
        try:
            if TORCH_AVAILABLE:
                if self.device == "mps":
                    safe_mps_empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
            
            gc.collect()
            
            # í™œì„± AI ëª¨ë¸ë³„ ìµœì í™”
            if self.active_model == 'pose_estimation_openpose':
                self.target_input_size = (368, 368)
                self.output_format = "keypoints_heatmap"
                self.num_keypoints = 18
            elif 'yolov8' in self.active_model or 'sk' in self.active_model:
                self.target_input_size = (640, 640)
                self.output_format = "keypoints_tensor"
                self.num_keypoints = 17  # COCO format
            else:
                self.target_input_size = (256, 256)
                self.output_format = "keypoints_simple"
                self.num_keypoints = 17
            
            self.logger.info(f"âœ… {self.active_model} AI ëª¨ë¸ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    async def _warmup_ai_models(self) -> bool:
        """AI ëª¨ë¸ ì›Œë°ì—…"""
        try:
            if not self.active_model or self.active_model not in self.pose_models:
                self.logger.error("âŒ ì›Œë°ì—…í•  AI ëª¨ë¸ ì—†ìŒ")
                return False
            
            # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ ì›Œë°ì—…
            dummy_image = np.zeros((256, 256, 3), dtype=np.uint8)
            dummy_image_pil = Image.fromarray(dummy_image)
            
            self.logger.info(f"ğŸ”¥ {self.active_model} AI ëª¨ë¸ ì›Œë°ì—… ì‹œì‘")
            
            try:
                warmup_result = await self._process_with_real_ai_model(dummy_image_pil, warmup=True)
                if warmup_result and warmup_result.get('success', False):
                    self.logger.info(f"âœ… {self.active_model} AI ëª¨ë¸ ì›Œë°ì—… ì„±ê³µ")
                    return True
                else:
                    self.logger.warning(f"âš ï¸ {self.active_model} AI ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨")
                    return False
            except Exception as e:
                self.logger.error(f"âŒ AI ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ - ì™„ì „í•œ AI ì¶”ë¡ 
    # ==============================================
    
    async def process(
        self, 
        image: Union[np.ndarray, Image.Image, str],
        clothing_type: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ì™„ì „í•œ ì‹¤ì œ AI í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
            clothing_type: ì˜ë¥˜ íƒ€ì… (ì„ íƒì )
            **kwargs: ì¶”ê°€ ì„¤ì •
            
        Returns:
            Dict[str, Any]: ì™„ì „í•œ AI í¬ì¦ˆ ì¶”ì • ê²°ê³¼
        """
        try:
            # ì´ˆê¸°í™” ê²€ì¦
            if not self.is_initialized:
                if not await self.initialize():
                    error_msg = "AI ì´ˆê¸°í™” ì‹¤íŒ¨"
                    if self.strict_mode:
                        raise RuntimeError(f"Strict Mode: {error_msg}")
                    return self._create_error_result(error_msg)
            
            start_time = time.time()
            self.logger.info(f"ğŸ§  {self.step_name} ì™„ì „í•œ AI ì²˜ë¦¬ ì‹œì‘")
            
            # ğŸ”¥ 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            processed_image = self._preprocess_image_strict(image)
            if processed_image is None:
                error_msg = "ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨"
                if self.strict_mode:
                    raise ValueError(f"Strict Mode: {error_msg}")
                return self._create_error_result(error_msg)
            
            # ğŸ”¥ 2. ìºì‹œ í™•ì¸
            cache_key = None
            if self.pose_config['cache_enabled']:
                cache_key = self._generate_cache_key(processed_image, clothing_type)
                if cache_key in self.prediction_cache:
                    self.logger.info("ğŸ“‹ ìºì‹œì—ì„œ AI ê²°ê³¼ ë°˜í™˜")
                    return self.prediction_cache[cache_key]
            
            # ğŸ”¥ 3. ì™„ì „í•œ ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡ 
            pose_result = await self._process_with_real_ai_model(processed_image, clothing_type, **kwargs)
            
            if not pose_result or not pose_result.get('success', False):
                error_msg = f"AI í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {pose_result.get('error', 'Unknown AI Error') if pose_result else 'No Result'}"
                self.logger.error(f"âŒ {error_msg}")
                if self.strict_mode:
                    raise RuntimeError(f"Strict Mode: {error_msg}")
                return self._create_error_result(error_msg)
            
            # ğŸ”¥ 4. ì™„ì „í•œ ê²°ê³¼ í›„ì²˜ë¦¬
            final_result = self._postprocess_complete_result(pose_result, processed_image, start_time)
            
            # ğŸ”¥ 5. ìºì‹œ ì €ì¥
            if self.pose_config['cache_enabled'] and cache_key:
                self._save_to_cache(cache_key, final_result)
            
            processing_time = time.time() - start_time
            self.logger.info(f"âœ… {self.step_name} ì™„ì „í•œ AI ì²˜ë¦¬ ì„±ê³µ ({processing_time:.2f}ì´ˆ)")
            self.logger.info(f"ğŸ¯ AI í‚¤í¬ì¸íŠ¸ ìˆ˜: {len(final_result.get('keypoints', []))}")
            self.logger.info(f"ğŸ–ï¸ AI ì‹ ë¢°ë„: {final_result.get('pose_analysis', {}).get('ai_confidence', 0):.3f}")
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì™„ì „í•œ AI ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìŠ¤íƒ: {traceback.format_exc()}")
            if self.strict_mode:
                raise
            return self._create_error_result(str(e))
    
    async def _process_with_real_ai_model(
        self, 
        image: Image.Image, 
        clothing_type: Optional[str] = None,
        warmup: bool = False,
        **kwargs
    ) -> Dict[str, Any]:
        """ì™„ì „í•œ ì‹¤ì œ AI ëª¨ë¸ì„ í†µí•œ í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬"""
        try:
            if not self.active_model or self.active_model not in self.pose_models:
                error_msg = "í™œì„± AI ëª¨ë¸ ì—†ìŒ"
                if self.strict_mode:
                    raise RuntimeError(f"Strict Mode: {error_msg}")
                return {'success': False, 'error': error_msg}
            
            ai_model = self.pose_models[self.active_model]
            
            self.logger.info(f"ğŸ§  {self.active_model} ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹œì‘")
            
            # ğŸ”¥ AI ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
            model_input = self._prepare_ai_model_input(image)
            if model_input is None:
                error_msg = "AI ëª¨ë¸ ì…ë ¥ ì¤€ë¹„ ì‹¤íŒ¨"
                if self.strict_mode:
                    raise ValueError(f"Strict Mode: {error_msg}")
                return {'success': False, 'error': error_msg}
            
            # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
            try:
                inference_start = time.time()
                
                if isinstance(ai_model, RealOpenPoseModel):
                    model_output = await self._run_openpose_inference(ai_model, model_input)
                elif isinstance(ai_model, RealYOLOv8PoseModel):
                    model_output = await self._run_yolo_inference(ai_model, model_input, image)
                elif isinstance(ai_model, RealLightweightPoseModel):
                    model_output = await self._run_lightweight_inference(ai_model, model_input)
                else:
                    # ì¼ë°˜ AI ëª¨ë¸ ì²˜ë¦¬
                    model_output = await self._run_generic_ai_inference(ai_model, model_input)
                
                inference_time = time.time() - inference_start
                
            except Exception as e:
                error_msg = f"AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}"
                self.logger.error(f"âŒ {error_msg}")
                if self.strict_mode:
                    raise RuntimeError(f"Strict Mode: {error_msg}")
                return {'success': False, 'error': error_msg}
            
            # ì›Œë°ì—… ëª¨ë“œì¸ ê²½ìš° ê°„ë‹¨í•œ ì„±ê³µ ê²°ê³¼ ë°˜í™˜
            if warmup:
                return {"success": True, "warmup": True, "model_used": self.active_model}
            
            # ğŸ”¥ AI ëª¨ë¸ ì¶œë ¥ í•´ì„
            pose_result = self._interpret_ai_model_output(model_output, image.size, self.active_model)
            
            if not pose_result.get('success', False):
                error_msg = "AI ëª¨ë¸ ì¶œë ¥ í•´ì„ ì‹¤íŒ¨"
                if self.strict_mode:
                    raise ValueError(f"Strict Mode: {error_msg}")
                return {'success': False, 'error': error_msg}
            
            # ì¶”ë¡  ì‹œê°„ ì¶”ê°€
            pose_result['inference_time'] = inference_time
            
            self.logger.info(f"âœ… {self.active_model} AI ì¶”ë¡  ì™„ì „ ì„±ê³µ ({inference_time:.3f}ì´ˆ)")
            return pose_result
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise
            return {'success': False, 'error': str(e)}
    
    # ==============================================
    # ğŸ”¥ AI ëª¨ë¸ë³„ ì¶”ë¡  ì‹¤í–‰ ë©”ì„œë“œë“¤
    # ==============================================
    
    async def _run_openpose_inference(self, model: RealOpenPoseModel, input_tensor: torch.Tensor) -> torch.Tensor:
        """OpenPose AI ëª¨ë¸ ì¶”ë¡ """
        try:
            with torch.no_grad():
                if self.device == "mps" and hasattr(torch, 'mps'):
                    with autocast("cpu"):  # MPSì—ì„œëŠ” CPU autocast ì‚¬ìš©
                        keypoints, paf = model(input_tensor)
                else:
                    keypoints, paf = model(input_tensor)
                
                return keypoints  # í‚¤í¬ì¸íŠ¸ë§Œ ë°˜í™˜
                
        except Exception as e:
            self.logger.error(f"âŒ OpenPose ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise
    
    async def _run_yolo_inference(self, model: RealYOLOv8PoseModel, input_data: Any, original_image: Image.Image) -> Any:
        """YOLOv8 AI ëª¨ë¸ ì¶”ë¡ """
        try:
            # PIL ì´ë¯¸ì§€ë¥¼ numpyë¡œ ë³€í™˜
            image_np = np.array(original_image)
            
            # YOLOv8 ì˜ˆì¸¡ ì‹¤í–‰
            results = model.predict(image_np)
            
            return results
                
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise
    
    async def _run_lightweight_inference(self, model: RealLightweightPoseModel, input_tensor: torch.Tensor) -> torch.Tensor:
        """ê²½ëŸ‰ AI ëª¨ë¸ ì¶”ë¡ """
        try:
            with torch.no_grad():
                heatmaps = model(input_tensor)
                return heatmaps
                
        except Exception as e:
            self.logger.error(f"âŒ ê²½ëŸ‰ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise
    
    async def _run_generic_ai_inference(self, model: Any, input_data: Any) -> Any:
        """ì¼ë°˜ AI ëª¨ë¸ ì¶”ë¡ """
        try:
            if hasattr(model, '__call__'):
                if asyncio.iscoroutinefunction(model.__call__):
                    return await model(input_data)
                else:
                    return model(input_data)
            elif hasattr(model, 'predict'):
                if asyncio.iscoroutinefunction(model.predict):
                    return await model.predict(input_data)
                else:
                    return model.predict(input_data)
            elif hasattr(model, 'forward'):
                with torch.no_grad():
                    return model.forward(input_data)
            else:
                raise ValueError(f"AI ëª¨ë¸ í˜¸ì¶œ ë°©ë²• ì—†ìŒ: {type(model)}")
                
        except Exception as e:
            self.logger.error(f"âŒ ì¼ë°˜ AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise
    
    # ==============================================
    # ğŸ”¥ AI ëª¨ë¸ ì…ë ¥/ì¶œë ¥ ì²˜ë¦¬
    # ==============================================
    
    def _prepare_ai_model_input(self, image: Image.Image) -> Optional[torch.Tensor]:
        """AI ëª¨ë¸ ì…ë ¥ ì¤€ë¹„"""
        try:
            # ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            image_np = np.array(image)
            
            # ì‹¤ì œ AI ëª¨ë¸ë³„ ì…ë ¥ í¬ê¸° ì¡°ì •
            if hasattr(self, 'target_input_size'):
                target_size = self.target_input_size
                image_resized = cv2.resize(image_np, target_size)
            else:
                image_resized = image_np
            
            # PyTorch í…ì„œë¡œ ë³€í™˜ (TORCH_AVAILABLE í™•ì¸ë¨)
            if len(image_resized.shape) == 3:
                # ì •ê·œí™” ë° í…ì„œ ë³€í™˜
                image_tensor = torch.from_numpy(image_resized).float()
                image_tensor = image_tensor.permute(2, 0, 1).unsqueeze(0)  # BHWC -> BCHW
                image_tensor = image_tensor / 255.0  # ì •ê·œí™”
                image_tensor = image_tensor.to(self.device)
                
                return image_tensor
            else:
                self.logger.error(f"âŒ ì˜ëª»ëœ ì´ë¯¸ì§€ ì°¨ì›: {image_resized.shape}")
                return None
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì…ë ¥ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return None
    
    def _interpret_ai_model_output(self, model_output: Any, image_size: Tuple[int, int], model_name: str) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì¶œë ¥ í•´ì„"""
        try:
            if 'openpose' in model_name.lower():
                return self._interpret_openpose_output(model_output, image_size)
            elif 'yolo' in model_name.lower() or 'sk' in model_name.lower():
                return self._interpret_yolo_output(model_output, image_size)
            elif 'lightweight' in model_name.lower():
                return self._interpret_lightweight_output(model_output, image_size)
            else:
                return self._interpret_generic_ai_output(model_output, image_size)
                
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì¶œë ¥ í•´ì„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _interpret_openpose_output(self, output: torch.Tensor, image_size: Tuple[int, int]) -> Dict[str, Any]:
        """OpenPose AI ì¶œë ¥ í•´ì„ - ìˆ˜ì •ëœ ì•ˆì „í•œ ë²„ì „"""
        try:
            keypoints = []
            confidence_scores = []
            
            if torch.is_tensor(output):
                # âœ… ìˆ˜ì • 1: ì•ˆì „í•œ ë””ë°”ì´ìŠ¤ ì´ë™
                if output.device.type == 'mps':
                    with torch.no_grad():
                        output_np = output.detach().cpu().numpy()
                else:
                    output_np = output.detach().cpu().numpy()
                
                # âœ… ìˆ˜ì • 2: ì°¨ì› ê²€ì‚¬ ì¶”ê°€
                if len(output_np.shape) == 4:  # [B, C, H, W]
                    if output_np.shape[0] > 0:
                        output_np = output_np[0]  # ì²« ë²ˆì§¸ ë°°ì¹˜
                    else:
                        return {
                            'keypoints': [],
                            'confidence_scores': [],
                            'model_used': 'openpose_real_ai',
                            'success': False,
                            'ai_model_type': 'openpose',
                            'error': 'Empty batch dimension'
                        }
                
                # âœ… ìˆ˜ì • 3: ì•ˆì „í•œ ë²”ìœ„ ê²€ì‚¬
                num_keypoints = min(output_np.shape[0], 18)
                if num_keypoints <= 0:
                    return {
                        'keypoints': [],
                        'confidence_scores': [],
                        'model_used': 'openpose_real_ai', 
                        'success': False,
                        'ai_model_type': 'openpose',
                        'error': 'No keypoints in output'
                    }
                
                for i in range(num_keypoints):  # 18ê°œ í‚¤í¬ì¸íŠ¸
                    heatmap = output_np[i]
                    
                    # âœ… ìˆ˜ì • 4: ì•ˆì „í•œ argmax ì²˜ë¦¬
                    if heatmap.size == 0:
                        keypoints.append([0.0, 0.0, 0.0])
                        confidence_scores.append(0.0)
                        continue
                    
                    # âœ… ìˆ˜ì • 5: unravel_index ëŒ€ì‹  divmod ì‚¬ìš©
                    max_idx = np.argmax(heatmap.flatten())  # 1ì°¨ì›ìœ¼ë¡œ í‰ë©´í™”
                    y, x = np.divmod(max_idx, heatmap.shape[1])  # ì•ˆì „í•œ 2D ì¢Œí‘œ ë³€í™˜
                    confidence = float(heatmap[y, x])
                    
                    # âœ… ìˆ˜ì • 6: ì•ˆì „í•œ ìŠ¤ì¼€ì¼ë§ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
                    x_scaled = x * image_size[0] / max(heatmap.shape[1] - 1, 1)
                    y_scaled = y * image_size[1] / max(heatmap.shape[0] - 1, 1)
                    
                    keypoints.append([float(x_scaled), float(y_scaled), confidence])
                    confidence_scores.append(confidence)
            
            return {
                'keypoints': keypoints,
                'confidence_scores': confidence_scores,
                'model_used': 'openpose_real_ai',
                'success': len(keypoints) > 0,
                'ai_model_type': 'openpose'
            }
                
        except Exception as e:
            self.logger.error(f"âŒ OpenPose AI ì¶œë ¥ í•´ì„ ì‹¤íŒ¨: {e}")
            return {
                'keypoints': [],
                'confidence_scores': [],
                'model_used': 'openpose_real_ai',
                'success': False, 
                'ai_model_type': 'openpose',
                'error': str(e)
            }


    # ğŸš€ conda + M3 Max ìµœì í™” ë²„ì „
    def _interpret_openpose_output_optimized(self, output: torch.Tensor, image_size: Tuple[int, int]) -> Dict[str, Any]:
        """OpenPose AI ì¶œë ¥ í•´ì„ - conda í™˜ê²½ ìµœì í™”"""
        
        try:
            # conda í™˜ê²½ ê°ì§€
            is_conda = hasattr(self, 'conda_optimized') and self.conda_optimized
            is_m3_max = hasattr(self, 'is_m3_max') and self.is_m3_max
            
            # Step 1: ì…ë ¥ ë° í™˜ê²½ ê²€ì¦
            if not self._validate_openpose_input(output, image_size):
                return self._create_empty_result('validation_failed')
            
            # Step 2: í™˜ê²½ë³„ ìµœì í™” ë¶„ê¸°
            if is_m3_max and output.device.type == 'mps':
                return self._extract_keypoints_mps_optimized(output, image_size)
            elif is_conda:
                return self._extract_keypoints_conda_optimized(output, image_size)
            else:
                return self._extract_keypoints_standard(output, image_size)
            
        except Exception as e:
            self.logger.error(f"âŒ ìµœì í™”ëœ OpenPose ì¶œë ¥ í•´ì„ ì‹¤íŒ¨: {e}")
            return self._create_empty_result('optimization_failed', str(e))

    def _extract_keypoints_mps_optimized(self, output: torch.Tensor, image_size: Tuple[int, int]) -> Dict[str, Any]:
        """M3 Max MPS ìµœì í™” í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        
        with torch.inference_mode():  # MPS ìµœì í™”
            # ì°¨ì› ì •ê·œí™”
            if output.dim() == 4:
                output = output.squeeze(0)  # (B=1, C, H, W) -> (C, H, W)
            
            num_keypoints = min(output.size(0), 18)
            height, width = output.size(-2), output.size(-1)
            
            keypoints = []
            confidence_scores = []
            
            for i in range(num_keypoints):
                heatmap = output[i]  # (H, W)
                
                # MPS ìµœì í™”ëœ argmax
                flat_heatmap = heatmap.view(-1)
                max_val, max_idx = torch.max(flat_heatmap, dim=0)
                
                # 2D ì¢Œí‘œ ê³„ì‚°
                y = max_idx // width
                x = max_idx % width
                
                # ìŠ¤ì¼€ì¼ë§
                x_scaled = float(x.item()) * image_size[0] / max(width - 1, 1)
                y_scaled = float(y.item()) * image_size[1] / max(height - 1, 1)
                confidence = float(max_val.item())
                
                keypoints.append([x_scaled, y_scaled, confidence])
                confidence_scores.append(confidence)
        
        return self._create_openpose_result(keypoints, confidence_scores, image_size, (height, width))

    def _extract_keypoints_conda_optimized(self, output: torch.Tensor, image_size: Tuple[int, int]) -> Dict[str, Any]:
        """conda í™˜ê²½ ìµœì í™” í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        
        # ì•ˆì „í•œ CPU ì´ë™
        if output.device.type != 'cpu':
            output_np = output.detach().cpu().numpy()
        else:
            output_np = output.detach().numpy()
        
        # ì°¨ì› ì •ê·œí™”
        if output_np.ndim == 4 and output_np.shape[0] > 0:
            output_np = output_np[0]
        
        num_keypoints = min(output_np.shape[0], 18)
        height, width = output_np.shape[-2], output_np.shape[-1]
        
        keypoints = []
        confidence_scores = []
        
        # ë²¡í„°í™”ëœ ì²˜ë¦¬ (conda í™˜ê²½ ìµœì í™”)
        for i in range(num_keypoints):
            heatmap = output_np[i]
            
            # ì•ˆì „í•œ argmax
            if heatmap.size > 0:
                max_idx = np.argmax(heatmap.ravel())
                y, x = np.divmod(max_idx, width)
                confidence = float(heatmap.flat[max_idx])
                
                # ìŠ¤ì¼€ì¼ë§
                x_scaled = float(x) * image_size[0] / max(width - 1, 1)
                y_scaled = float(y) * image_size[1] / max(height - 1, 1)
                
                keypoints.append([x_scaled, y_scaled, confidence])
                confidence_scores.append(confidence)
            else:
                keypoints.append([0.0, 0.0, 0.0])
                confidence_scores.append(0.0)
        
        return self._create_openpose_result(keypoints, confidence_scores, image_size, (height, width))

    def _extract_keypoints_standard(self, output: torch.Tensor, image_size: Tuple[int, int]) -> Dict[str, Any]:
        """í‘œì¤€ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (í´ë°±ìš©)"""
        
        # ê¸°ì¡´ ìˆ˜ì •ëœ ë²„ì „ ì‚¬ìš©
        return self._interpret_openpose_output_fixed(output, image_size)

# ==============================================
# ğŸ”§ í—¬í¼ í•¨ìˆ˜ë“¤
# ==============================================

def _validate_openpose_input(self, output: torch.Tensor, image_size: Tuple[int, int]) -> bool:
    """OpenPose ì…ë ¥ ê²€ì¦"""
    
    if not torch.is_tensor(output):
        self.logger.error("âŒ ì¶œë ¥ì´ í…ì„œê°€ ì•„ë‹˜")
        return False
    
    if output.numel() == 0:
        self.logger.error("âŒ ë¹ˆ í…ì„œ")
        return False
    
    if len(image_size) != 2 or any(s <= 0 for s in image_size):
        self.logger.error(f"âŒ ì˜ëª»ëœ ì´ë¯¸ì§€ í¬ê¸°: {image_size}")
        return False
    
    if output.dim() not in [2, 3, 4]:
        self.logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í…ì„œ ì°¨ì›: {output.dim()}")
        return False
    
    return True

def _create_empty_result(self, reason: str, error_msg: str = "") -> Dict[str, Any]:
    """ë¹ˆ ê²°ê³¼ ìƒì„±"""
    
    return {
        'keypoints': [],
        'confidence_scores': [],
        'model_used': 'openpose_real_ai',
        'success': False,
        'ai_model_type': 'openpose',
        'error_reason': reason,
        'error_message': error_msg,
        'num_keypoints': 0
    }

def _create_openpose_result(self, keypoints: List[List[float]], 
                          confidence_scores: List[float],
                          image_size: Tuple[int, int],
                          heatmap_size: Tuple[int, int]) -> Dict[str, Any]:
    """OpenPose ê²°ê³¼ ìƒì„±"""
    
    return {
        'keypoints': keypoints,
        'confidence_scores': confidence_scores,
        'model_used': 'openpose_real_ai',
        'success': len(keypoints) > 0,
        'ai_model_type': 'openpose',
        'num_keypoints': len(keypoints),
        'image_size': image_size,
        'heatmap_size': heatmap_size,
        'valid_keypoints': sum(1 for conf in confidence_scores if conf > 0.1)
    }



    def _interpret_yolo_output(self, results: Any, image_size: Tuple[int, int]) -> Dict[str, Any]:
        """YOLOv8 AI ì¶œë ¥ í•´ì„"""
        try:
            keypoints = []
            confidence_scores = []
            
            # YOLOv8 ê²°ê³¼ ì²˜ë¦¬
            if isinstance(results, list) and len(results) > 0:
                for result in results:
                    if hasattr(result, 'keypoints') and hasattr(result.keypoints, 'data'):
                        # ultralytics YOLO ê²°ê³¼
                        kps_data = result.keypoints.data
                        if len(kps_data) > 0:
                            kps = kps_data[0]  # ì²« ë²ˆì§¸ ì‚¬ëŒ
                            for kp in kps:
                                x, y, conf = float(kp[0]), float(kp[1]), float(kp[2])
                                keypoints.append([x, y, conf])
                                confidence_scores.append(conf)
                            break
                    elif isinstance(result, dict) and 'keypoints' in result:
                        # ì§ì ‘ êµ¬í˜„ ê²°ê³¼
                        kps = result['keypoints']
                        if isinstance(kps, np.ndarray):
                            for kp in kps:
                                if len(kp) >= 3:
                                    x, y, conf = float(kp[0]), float(kp[1]), float(kp[2])
                                    keypoints.append([x, y, conf])
                                    confidence_scores.append(conf)
                        break
            
            # COCO 17ì„ OpenPose 18ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
            if len(keypoints) == 17:
                keypoints = self._convert_coco_to_openpose(keypoints, image_size)
                confidence_scores = [kp[2] for kp in keypoints]
            
            return {
                'keypoints': keypoints,
                'confidence_scores': confidence_scores,
                'model_used': 'yolov8_real_ai',
                'success': len(keypoints) > 0,
                'ai_model_type': 'yolov8'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 AI ì¶œë ¥ í•´ì„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _interpret_lightweight_output(self, output: torch.Tensor, image_size: Tuple[int, int]) -> Dict[str, Any]:
        """ê²½ëŸ‰ AI ëª¨ë¸ ì¶œë ¥ í•´ì„"""
        try:
            keypoints = []
            confidence_scores = []
            
            if torch.is_tensor(output):
                output_np = output.cpu().numpy()
                
                # íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
                if len(output_np.shape) == 4:  # [B, C, H, W]
                    output_np = output_np[0]  # ì²« ë²ˆì§¸ ë°°ì¹˜
                
                for i in range(min(output_np.shape[0], 17)):  # 17ê°œ í‚¤í¬ì¸íŠ¸ (COCO)
                    heatmap = output_np[i]
                    
                    # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                    y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
                    confidence = float(heatmap[y, x])
                    
                    # ì´ë¯¸ì§€ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§
                    x_scaled = x * image_size[0] / heatmap.shape[1]
                    y_scaled = y * image_size[1] / heatmap.shape[0]
                    
                    keypoints.append([float(x_scaled), float(y_scaled), confidence])
                    confidence_scores.append(confidence)
            
            # COCO 17ì„ OpenPose 18ë¡œ ë³€í™˜
            if len(keypoints) == 17:
                keypoints = self._convert_coco_to_openpose(keypoints, image_size)
                confidence_scores = [kp[2] for kp in keypoints]
            
            return {
                'keypoints': keypoints,
                'confidence_scores': confidence_scores,
                'model_used': 'lightweight_real_ai',
                'success': len(keypoints) > 0,
                'ai_model_type': 'lightweight'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê²½ëŸ‰ AI ëª¨ë¸ ì¶œë ¥ í•´ì„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _interpret_generic_ai_output(self, output: Any, image_size: Tuple[int, int]) -> Dict[str, Any]:
        """ì¼ë°˜ AI ëª¨ë¸ ì¶œë ¥ í•´ì„"""
        try:
            keypoints = []
            confidence_scores = []
            
            # ë‹¤ì–‘í•œ ì¶œë ¥ í˜•ì‹ ì²˜ë¦¬
            if isinstance(output, (list, tuple)):
                for item in output:
                    if len(item) >= 3:
                        keypoints.append([float(item[0]), float(item[1]), float(item[2])])
                        confidence_scores.append(float(item[2]))
            elif isinstance(output, np.ndarray):
                if len(output.shape) == 2 and output.shape[1] >= 3:
                    for i in range(min(output.shape[0], 18)):
                        keypoints.append([float(output[i, 0]), float(output[i, 1]), float(output[i, 2])])
                        confidence_scores.append(float(output[i, 2]))
            elif torch.is_tensor(output):
                output_np = output.cpu().numpy()
                return self._interpret_generic_ai_output(output_np, image_size)
            
            return {
                'keypoints': keypoints,
                'confidence_scores': confidence_scores,
                'model_used': 'generic_real_ai',
                'success': len(keypoints) > 0,
                'ai_model_type': 'generic'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì¼ë°˜ AI ëª¨ë¸ ì¶œë ¥ í•´ì„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _convert_coco_to_openpose(self, coco_keypoints: List[List[float]], image_size: Tuple[int, int]) -> List[List[float]]:
        """COCO 17ì„ OpenPose 18ë¡œ ë³€í™˜"""
        try:
            # COCO 17 -> OpenPose 18 ë§¤í•‘
            coco_to_op_mapping = {
                0: 0,   # nose
                1: 16,  # left_eye -> left_eye (OpenPose index)
                2: 15,  # right_eye -> right_eye
                3: 18,  # left_ear -> left_ear
                4: 17,  # right_ear -> right_ear
                5: 5,   # left_shoulder -> left_shoulder
                6: 2,   # right_shoulder -> right_shoulder
                7: 6,   # left_elbow -> left_elbow
                8: 3,   # right_elbow -> right_elbow
                9: 7,   # left_wrist -> left_wrist
                10: 4,  # right_wrist -> right_wrist
                11: 12, # left_hip -> left_hip
                12: 9,  # right_hip -> right_hip
                13: 13, # left_knee -> left_knee
                14: 10, # right_knee -> right_knee
                15: 14, # left_ankle -> left_ankle
                16: 11  # right_ankle -> right_ankle
            }
            
            # OpenPose 18 í‚¤í¬ì¸íŠ¸ ì´ˆê¸°í™”
            openpose_18 = [[0.0, 0.0, 0.0] for _ in range(19)]  # 0-18 ì¸ë±ìŠ¤
            
            # COCOì—ì„œ OpenPoseë¡œ ë³€í™˜
            for coco_idx, op_idx in coco_to_op_mapping.items():
                if coco_idx < len(coco_keypoints) and op_idx < 19:
                    openpose_18[op_idx] = coco_keypoints[coco_idx]
            
            # neck í‚¤í¬ì¸íŠ¸ ì¶”ì • (OpenPose index 1)
            left_shoulder = openpose_18[5]
            right_shoulder = openpose_18[2]
            if left_shoulder[2] > 0.3 and right_shoulder[2] > 0.3:
                neck_x = (left_shoulder[0] + right_shoulder[0]) / 2
                neck_y = (left_shoulder[1] + right_shoulder[1]) / 2
                neck_conf = min(left_shoulder[2], right_shoulder[2])
                openpose_18[1] = [neck_x, neck_y, neck_conf]
            
            # mid_hip í‚¤í¬ì¸íŠ¸ ì¶”ì • (OpenPose index 8)
            left_hip = openpose_18[12]
            right_hip = openpose_18[9]
            if left_hip[2] > 0.3 and right_hip[2] > 0.3:
                mid_hip_x = (left_hip[0] + right_hip[0]) / 2
                mid_hip_y = (left_hip[1] + right_hip[1]) / 2
                mid_hip_conf = min(left_hip[2], right_hip[2])
                openpose_18[8] = [mid_hip_x, mid_hip_y, mid_hip_conf]
            
            return openpose_18[:18]  # 18ê°œë§Œ ë°˜í™˜
            
        except Exception as e:
            self.logger.error(f"âŒ COCO to OpenPose ë³€í™˜ ì‹¤íŒ¨: {e}")
            return [[0.0, 0.0, 0.0] for _ in range(18)]
    
    # ==============================================
    # ğŸ”¥ ì™„ì „í•œ í¬ì¦ˆ ë¶„ì„ ë©”ì„œë“œë“¤
    # ==============================================
    
    def _analyze_pose_quality_complete(self, pose_metrics: PoseMetrics) -> Dict[str, Any]:
        """ì™„ì „í•œ í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„"""
        try:
            if not pose_metrics.keypoints:
                return {
                    'suitable_for_fitting': False,
                    'issues': ['ì‹¤ì œ AI ëª¨ë¸ì—ì„œ í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'],
                    'recommendations': ['ë” ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ í¬ì¦ˆë¥¼ ëª…í™•íˆ í•´ì£¼ì„¸ìš”'],
                    'quality_score': 0.0,
                    'ai_confidence': 0.0,
                    'real_ai_analysis': True
                }
            
            # ğŸ”¥ ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°
            head_score = self._calculate_head_score(pose_metrics.keypoints)
            torso_score = self._calculate_torso_score(pose_metrics.keypoints)
            arms_score = self._calculate_arms_score(pose_metrics.keypoints)
            legs_score = self._calculate_legs_score(pose_metrics.keypoints)
            
            # ğŸ”¥ ê³ ê¸‰ ë¶„ì„ ë©”ì„œë“œë“¤
            pose_angles = self._calculate_pose_angles(pose_metrics.keypoints)
            body_proportions = self._calculate_body_proportions(pose_metrics.keypoints)
            symmetry_score = self._calculate_symmetry_score(pose_metrics.keypoints)
            visibility_score = self._calculate_visibility_score(pose_metrics.keypoints)
            major_keypoints_rate = self._calculate_major_keypoints_rate(pose_metrics.keypoints)
            
            # AI ì‹ ë¢°ë„ ê³„ì‚°
            ai_confidence = np.mean(pose_metrics.confidence_scores) if pose_metrics.confidence_scores else 0.0
            
            # ì „ì²´ ì ìˆ˜ (AI ì‹ ë¢°ë„ ë°˜ì˜)
            base_score = (head_score * 0.2 + torso_score * 0.3 + 
                         arms_score * 0.25 + legs_score * 0.25)
            
            advanced_score = (symmetry_score * 0.2 + visibility_score * 0.3 + 
                            major_keypoints_rate * 0.5)
            
            overall_score = (base_score * 0.7 + advanced_score * 0.3) * ai_confidence
            
            # ì—„ê²©í•œ ì í•©ì„± íŒë‹¨
            min_score = 0.75 if self.strict_mode else 0.65
            min_confidence = 0.7 if self.strict_mode else 0.6
            suitable_for_fitting = (overall_score >= min_score and 
                                  ai_confidence >= min_confidence and
                                  major_keypoints_rate >= 0.6)
            
            # ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
            issues, recommendations = self._generate_issues_and_recommendations(
                ai_confidence, min_confidence, symmetry_score, visibility_score,
                head_score, torso_score, arms_score, legs_score
            )
            
            # í¬ì¦ˆ íƒ€ì… ë° í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
            pose_type = self._determine_pose_type(pose_metrics.keypoints, pose_angles)
            quality_grade = self._determine_quality_grade(overall_score)
            
            return {
                'suitable_for_fitting': suitable_for_fitting,
                'issues': issues,
                'recommendations': recommendations,
                'quality_score': overall_score,
                'quality_grade': quality_grade.value,
                'pose_type': pose_type.value,
                'ai_confidence': ai_confidence,
                'detailed_scores': {
                    'head': head_score,
                    'torso': torso_score,
                    'arms': arms_score,
                    'legs': legs_score,
                    'symmetry': symmetry_score,
                    'visibility': visibility_score,
                    'major_keypoints_rate': major_keypoints_rate
                },
                'pose_angles': pose_angles,
                'body_proportions': body_proportions,
                'model_performance': {
                    'model_name': pose_metrics.model_used,
                    'keypoints_detected': len([kp for kp in pose_metrics.keypoints if len(kp) > 2 and kp[2] > self.pose_config['confidence_threshold']]),
                    'avg_confidence': ai_confidence,
                    'processing_time': pose_metrics.processing_time,
                    'real_ai_model': True
                },
                'real_ai_analysis': True,
                'strict_mode': self.strict_mode
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì™„ì „í•œ í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise
            return {
                'suitable_for_fitting': False,
                'issues': ['ì™„ì „í•œ AI ë¶„ì„ ì‹¤íŒ¨'],
                'recommendations': ['ì‹¤ì œ AI ëª¨ë¸ ìƒíƒœë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”'],
                'quality_score': 0.0,
                'ai_confidence': 0.0,
                'real_ai_analysis': True
            }
    
    def _calculate_head_score(self, keypoints: List[List[float]]) -> float:
        """ë¨¸ë¦¬ ë¶€ìœ„ ì ìˆ˜ ê³„ì‚°"""
        try:
            if len(keypoints) < 19:
                return 0.0
            
            head_indices = [0, 15, 16, 17, 18]  # nose, eyes, ears
            visible_count = sum(1 for idx in head_indices 
                              if idx < len(keypoints) and 
                              len(keypoints[idx]) > 2 and 
                              keypoints[idx][2] > self.pose_config['confidence_threshold'])
            
            return min(visible_count / 3.0, 1.0)
            
        except Exception:
            return 0.0
    
    def _calculate_torso_score(self, keypoints: List[List[float]]) -> float:
        """ìƒì²´ ì ìˆ˜ ê³„ì‚°"""
        try:
            if len(keypoints) < 19:
                return 0.0
            
            torso_indices = [1, 2, 5, 8, 9, 12]  # neck, shoulders, hips
            visible_count = sum(1 for idx in torso_indices 
                              if idx < len(keypoints) and 
                              len(keypoints[idx]) > 2 and 
                              keypoints[idx][2] > self.pose_config['confidence_threshold'])
            
            return min(visible_count / len(torso_indices), 1.0)
            
        except Exception:
            return 0.0
    
    def _calculate_arms_score(self, keypoints: List[List[float]]) -> float:
        """íŒ” ì ìˆ˜ ê³„ì‚°"""
        try:
            if len(keypoints) < 19:
                return 0.0
            
            arm_indices = [2, 3, 4, 5, 6, 7]  # shoulders, elbows, wrists
            visible_count = sum(1 for idx in arm_indices 
                              if idx < len(keypoints) and 
                              len(keypoints[idx]) > 2 and 
                              keypoints[idx][2] > self.pose_config['confidence_threshold'])
            
            return min(visible_count / len(arm_indices), 1.0)
            
        except Exception:
            return 0.0
    
    def _calculate_legs_score(self, keypoints: List[List[float]]) -> float:
        """ë‹¤ë¦¬ ì ìˆ˜ ê³„ì‚°"""
        try:
            if len(keypoints) < 19:
                return 0.0
            
            leg_indices = [9, 10, 11, 12, 13, 14]  # hips, knees, ankles
            visible_count = sum(1 for idx in leg_indices 
                              if idx < len(keypoints) and 
                              len(keypoints[idx]) > 2 and 
                              keypoints[idx][2] > self.pose_config['confidence_threshold'])
            
            return min(visible_count / len(leg_indices), 1.0)
            
        except Exception:
            return 0.0
    
    def _calculate_pose_angles(self, keypoints_18: List[List[float]]) -> Dict[str, float]:
        """í¬ì¦ˆ ê°ë„ ê³„ì‚°"""
        try:
            angles = {}
            confidence_threshold = self.pose_config['confidence_threshold']
            
            def calculate_angle(p1, p2, p3):
                """ì„¸ ì ìœ¼ë¡œ ê°ë„ ê³„ì‚°"""
                try:
                    if all(len(p) >= 3 and p[2] > confidence_threshold for p in [p1, p2, p3]):
                        v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
                        v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
                        
                        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
                        cos_angle = np.clip(cos_angle, -1.0, 1.0)
                        return float(np.degrees(np.arccos(cos_angle)))
                    return 0.0
                except Exception:
                    return 0.0
            
            if len(keypoints_18) >= 18:
                # ê´€ì ˆ ê°ë„ ê³„ì‚°
                angles['right_elbow'] = calculate_angle(keypoints_18[2], keypoints_18[3], keypoints_18[4])
                angles['left_elbow'] = calculate_angle(keypoints_18[5], keypoints_18[6], keypoints_18[7])
                angles['right_knee'] = calculate_angle(keypoints_18[9], keypoints_18[10], keypoints_18[11])
                angles['left_knee'] = calculate_angle(keypoints_18[12], keypoints_18[13], keypoints_18[14])
                angles['right_shoulder'] = calculate_angle(keypoints_18[1], keypoints_18[2], keypoints_18[3])
                angles['left_shoulder'] = calculate_angle(keypoints_18[1], keypoints_18[5], keypoints_18[6])
                
                # ì²™ì¶” ê°ë„
                if all(len(kp) >= 3 and kp[2] > confidence_threshold for kp in [keypoints_18[1], keypoints_18[8]]):
                    spine_vector = np.array([keypoints_18[8][0] - keypoints_18[1][0], keypoints_18[8][1] - keypoints_18[1][1]])
                    vertical_vector = np.array([0, 1])
                    cos_spine = np.dot(spine_vector, vertical_vector) / (np.linalg.norm(spine_vector) * np.linalg.norm(vertical_vector))
                    angles['spine_vertical'] = float(np.degrees(np.arccos(np.clip(cos_spine, -1.0, 1.0))))
            
            return angles
            
        except Exception as e:
            self.logger.debug(f"í¬ì¦ˆ ê°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_body_proportions(self, keypoints_18: List[List[float]]) -> Dict[str, float]:
        """ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°"""
        try:
            proportions = {}
            confidence_threshold = self.pose_config['confidence_threshold']
            
            if len(keypoints_18) >= 18:
                def distance(p1, p2):
                    """ë‘ ì  ê°„ ê±°ë¦¬"""
                    if (len(p1) >= 3 and len(p2) >= 3 and 
                        p1[2] > confidence_threshold and p2[2] > confidence_threshold):
                        return float(np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2))
                    return 0.0
                
                # ì£¼ìš” ê±°ë¦¬ ì¸¡ì •
                head_neck = distance(keypoints_18[0], keypoints_18[1])
                neck_hip = distance(keypoints_18[1], keypoints_18[8])
                hip_knee = distance(keypoints_18[9], keypoints_18[10])
                knee_ankle = distance(keypoints_18[10], keypoints_18[11])
                shoulder_width = distance(keypoints_18[2], keypoints_18[5])
                hip_width = distance(keypoints_18[9], keypoints_18[12])
                
                # ë¹„ìœ¨ ê³„ì‚°
                total_height = head_neck + neck_hip + hip_knee + knee_ankle
                if total_height > 0:
                    proportions['head_to_total'] = head_neck / total_height
                    proportions['torso_to_total'] = neck_hip / total_height
                    proportions['upper_leg_to_total'] = hip_knee / total_height
                    proportions['lower_leg_to_total'] = knee_ankle / total_height
                    proportions['shoulder_to_hip_ratio'] = shoulder_width / hip_width if hip_width > 0 else 0.0
                
                # í˜„ì‹¤ì„± ê²€ì¦
                proportions['is_realistic'] = (
                    0.1 <= proportions.get('head_to_total', 0) <= 0.25 and
                    0.25 <= proportions.get('torso_to_total', 0) <= 0.45 and
                    0.8 <= proportions.get('shoulder_to_hip_ratio', 0) <= 1.5
                )
            
            return proportions
            
        except Exception as e:
            self.logger.debug(f"ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_symmetry_score(self, keypoints_18: List[List[float]]) -> float:
        """ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            symmetry_pairs = [
                (2, 5), (3, 6), (4, 7), (9, 12), (10, 13), (11, 14), (15, 16)
            ]
            
            symmetry_scores = []
            center_x = np.mean([kp[0] for kp in keypoints_18 if len(kp) >= 3 and kp[2] > self.pose_config['confidence_threshold']])
            
            for left_idx, right_idx in symmetry_pairs:
                if (left_idx < len(keypoints_18) and right_idx < len(keypoints_18) and
                    len(keypoints_18[left_idx]) >= 3 and len(keypoints_18[right_idx]) >= 3 and
                    keypoints_18[left_idx][2] > self.pose_config['confidence_threshold'] and 
                    keypoints_18[right_idx][2] > self.pose_config['confidence_threshold']):
                    
                    left_point = keypoints_18[left_idx]
                    right_point = keypoints_18[right_idx]
                    
                    left_dist = abs(left_point[0] - center_x)
                    right_dist = abs(right_point[0] - center_x)
                    
                    if max(left_dist, right_dist) > 0:
                        symmetry = 1.0 - abs(left_dist - right_dist) / max(left_dist, right_dist)
                        symmetry_scores.append(max(0.0, symmetry))
            
            return float(np.mean(symmetry_scores)) if symmetry_scores else 0.0
            
        except Exception as e:
            self.logger.debug(f"ëŒ€ì¹­ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_visibility_score(self, keypoints_18: List[List[float]]) -> float:
        """ê°€ì‹œì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            if not keypoints_18 or len(keypoints_18) < 18:
                return 0.0
            
            # ì£¼ìš” í‚¤í¬ì¸íŠ¸ë³„ ê°€ì¤‘ì¹˜
            major_keypoints = {
                0: 0.1, 1: 0.15, 2: 0.1, 5: 0.1, 8: 0.15,
                9: 0.1, 12: 0.1, 10: 0.075, 13: 0.075, 11: 0.05, 14: 0.05
            }
            
            weighted_visibility = 0.0
            total_weight = 0.0
            
            for idx, weight in major_keypoints.items():
                if idx < len(keypoints_18) and len(keypoints_18[idx]) >= 3:
                    confidence = keypoints_18[idx][2]
                    weighted_visibility += confidence * weight
                    total_weight += weight
            
            return weighted_visibility / total_weight if total_weight > 0 else 0.0
            
        except Exception as e:
            self.logger.debug(f"ê°€ì‹œì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_major_keypoints_rate(self, keypoints_18: List[List[float]]) -> float:
        """ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê²€ì¶œë¥  ê³„ì‚°"""
        try:
            major_indices = [0, 1, 2, 5, 8, 9, 10, 12, 13]
            detected_major = sum(1 for idx in major_indices 
                               if idx < len(keypoints_18) and 
                               len(keypoints_18[idx]) >= 3 and
                               keypoints_18[idx][2] > self.pose_config['confidence_threshold'])
            return detected_major / len(major_indices)
        except Exception as e:
            self.logger.debug(f"ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _generate_issues_and_recommendations(
        self, ai_confidence, min_confidence, symmetry_score, visibility_score,
        head_score, torso_score, arms_score, legs_score
    ) -> Tuple[List[str], List[str]]:
        """ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        issues = []
        recommendations = []
        
        if ai_confidence < min_confidence:
            issues.append(f'ì‹¤ì œ AI ëª¨ë¸ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({ai_confidence:.2f} < {min_confidence})')
            recommendations.append('ì¡°ëª…ì´ ì¢‹ì€ í™˜ê²½ì—ì„œ ë‹¤ì‹œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        if symmetry_score < 0.5:
            issues.append('ì‹ ì²´ ëŒ€ì¹­ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤')
            recommendations.append('ì •ë©´ì„ í–¥í•´ ëŒ€ì¹­ì ì¸ ìì„¸ë¡œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        if visibility_score < 0.6:
            issues.append('ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„±ì´ ë‚®ìŠµë‹ˆë‹¤')
            recommendations.append('ì „ì‹ ì´ ëª…í™•íˆ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        if head_score < 0.5:
            issues.append('ì–¼êµ´ ì˜ì—­ ê²€ì¶œì´ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤')
            recommendations.append('ì–¼êµ´ì´ ì •ë©´ì„ í–¥í•˜ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        if torso_score < 0.5:
            issues.append('ìƒì²´ ì˜ì—­ì´ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤')
            recommendations.append('ìƒì²´ ì „ì²´ê°€ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        if arms_score < 0.5:
            issues.append('íŒ”ì˜ ìœ„ì¹˜ê°€ ë¶€ì ì ˆí•©ë‹ˆë‹¤')
            recommendations.append('íŒ”ì„ ë²Œë¦¬ê±°ë‚˜ ìì—°ìŠ¤ëŸ½ê²Œ ë‚´ë ¤ì£¼ì„¸ìš”')
        
        if legs_score < 0.5:
            issues.append('ë‹¤ë¦¬ê°€ ê°€ë ¤ì ¸ ìˆìŠµë‹ˆë‹¤')
            recommendations.append('ì „ì‹ ì´ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        return issues, recommendations
    
    def _determine_pose_type(self, keypoints: List[List[float]], pose_angles: Dict[str, float]) -> PoseType:
        """í¬ì¦ˆ íƒ€ì… ê²°ì •"""
        try:
            if not keypoints or not pose_angles:
                return PoseType.UNKNOWN
            
            # T-í¬ì¦ˆ ê°ì§€
            if (pose_angles.get('right_shoulder', 0) > 160 and 
                pose_angles.get('left_shoulder', 0) > 160):
                return PoseType.T_POSE
            
            # A-í¬ì¦ˆ ê°ì§€
            if (120 < pose_angles.get('right_shoulder', 0) < 160 and
                120 < pose_angles.get('left_shoulder', 0) < 160):
                return PoseType.A_POSE
            
            # ì•‰ì€ ìì„¸ ê°ì§€
            if (pose_angles.get('right_knee', 180) < 120 and
                pose_angles.get('left_knee', 180) < 120):
                return PoseType.SITTING
            
            # ì•¡ì…˜ í¬ì¦ˆ ê°ì§€
            elbow_diff = abs(pose_angles.get('right_elbow', 180) - pose_angles.get('left_elbow', 180))
            if elbow_diff > 60:
                return PoseType.ACTION
            
            return PoseType.STANDING
            
        except Exception as e:
            self.logger.debug(f"í¬ì¦ˆ íƒ€ì… ê²°ì • ì‹¤íŒ¨: {e}")
            return PoseType.UNKNOWN
    
    def _determine_quality_grade(self, overall_score: float) -> PoseQuality:
        """í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        if overall_score >= 0.9:
            return PoseQuality.EXCELLENT
        elif overall_score >= 0.75:
            return PoseQuality.GOOD
        elif overall_score >= 0.6:
            return PoseQuality.ACCEPTABLE
        elif overall_score >= 0.4:
            return PoseQuality.POOR
        else:
            return PoseQuality.VERY_POOR
    
    # ==============================================
    # ğŸ”¥ ì‹œê°í™” ë° ìœ í‹¸ë¦¬í‹°
    # ==============================================
    
    def _create_advanced_pose_visualization(self, image: Image.Image, pose_metrics: PoseMetrics) -> Optional[str]:
        """ê³ ê¸‰ í¬ì¦ˆ ì‹œê°í™” ìƒì„±"""
        try:
            if not pose_metrics.keypoints:
                return None
            
            vis_image = image.copy()
            draw = ImageDraw.Draw(vis_image)
            
            confidence_threshold = self.pose_config['confidence_threshold']
            
            # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
            for i, kp in enumerate(pose_metrics.keypoints):
                if len(kp) >= 3 and kp[2] > confidence_threshold:
                    x, y = int(kp[0]), int(kp[1])
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    radius = int(4 + kp[2] * 6)
                    draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                               fill=color, outline=(255, 255, 255), width=2)
            
            # ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²°ì„  ê·¸ë¦¬ê¸°
            for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
                if (start_idx < len(pose_metrics.keypoints) and 
                    end_idx < len(pose_metrics.keypoints)):
                    
                    start_kp = pose_metrics.keypoints[start_idx]
                    end_kp = pose_metrics.keypoints[end_idx]
                    
                    if (len(start_kp) >= 3 and len(end_kp) >= 3 and
                        start_kp[2] > confidence_threshold and end_kp[2] > confidence_threshold):
                        
                        start_point = (int(start_kp[0]), int(start_kp[1]))
                        end_point = (int(end_kp[0]), int(end_kp[1]))
                        color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                        
                        avg_confidence = (start_kp[2] + end_kp[2]) / 2
                        line_width = int(2 + avg_confidence * 4)
                        
                        draw.line([start_point, end_point], fill=color, width=line_width)
            
            # ì •ë³´ ì˜¤ë²„ë ˆì´ ì¶”ê°€
            self._add_info_overlay(draw, pose_metrics)
            
            # Base64ë¡œ ì¸ì½”ë”©
            buffer = io.BytesIO()
            vis_image.save(buffer, format='JPEG', quality=95)
            image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            return f"data:image/jpeg;base64,{image_base64}"
            
        except Exception as e:
            self.logger.error(f"âŒ ê³ ê¸‰ í¬ì¦ˆ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _add_info_overlay(self, draw: ImageDraw.Draw, pose_metrics: PoseMetrics):
        """ì •ë³´ ì˜¤ë²„ë ˆì´ ì¶”ê°€"""
        try:
            try:
                font = ImageFont.load_default()
            except:
                font = None
            
            detected_keypoints = len([kp for kp in pose_metrics.keypoints if len(kp) > 2 and kp[2] > self.pose_config['confidence_threshold']])
            avg_confidence = np.mean([kp[2] for kp in pose_metrics.keypoints if len(kp) > 2]) if pose_metrics.keypoints else 0.0
            
            info_lines = [
                f"Real AI Model: {pose_metrics.model_used}",
                f"Keypoints: {detected_keypoints}/18",
                f"AI Confidence: {avg_confidence:.3f}",
                f"Processing: {pose_metrics.processing_time:.2f}s",
                f"Strict Mode: {'ON' if self.strict_mode else 'OFF'}"
            ]
            
            y_offset = 10
            for i, line in enumerate(info_lines):
                text_y = y_offset + i * 22
                draw.rectangle([5, text_y-2, 250, text_y+20], fill=(0, 0, 0, 150))
                draw.text((10, text_y), line, fill=(255, 255, 255), font=font)
                
        except Exception as e:
            self.logger.debug(f"ì •ë³´ ì˜¤ë²„ë ˆì´ ì¶”ê°€ ì‹¤íŒ¨: {e}")
    
    def _preprocess_image_strict(self, image: Union[np.ndarray, Image.Image, str]) -> Optional[Image.Image]:
        """ì—„ê²©í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            if isinstance(image, str):
                if os.path.exists(image):
                    image = Image.open(image)
                else:
                    try:
                        image_data = base64.b64decode(image)
                        image = Image.open(io.BytesIO(image_data))
                    except Exception as e:
                        self.logger.error(f"âŒ Base64 ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: {e}")
                        return None
            elif isinstance(image, np.ndarray):
                if image.size == 0:
                    return None
                image = Image.fromarray(image)
            elif not isinstance(image, Image.Image):
                return None
            
            # RGB ë³€í™˜
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # í¬ê¸° ê²€ì¦
            if image.size[0] < 64 or image.size[1] < 64:
                return None
            
            # í¬ê¸° ì¡°ì •
            max_size = 1024 if self.is_m3_max else 512
            if max(image.size) > max_size:
                ratio = max_size / max(image.size)
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
            
            return image
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_cache_key(self, image: Image.Image, clothing_type: Optional[str]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            image_bytes = io.BytesIO()
            image.save(image_bytes, format='JPEG', quality=50)
            image_hash = hashlib.md5(image_bytes.getvalue()).hexdigest()[:16]
            
            config_str = f"{clothing_type}_{self.active_model}_{self.pose_config['confidence_threshold']}"
            config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]
            
            return f"real_ai_pose_{image_hash}_{config_hash}"
            
        except Exception:
            return f"real_ai_pose_{int(time.time())}"
    
    def _save_to_cache(self, cache_key: str, result: Dict[str, Any]):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        try:
            if len(self.prediction_cache) >= self.cache_max_size:
                oldest_key = next(iter(self.prediction_cache))
                del self.prediction_cache[oldest_key]
            
            cached_result = result.copy()
            cached_result['visualization'] = None  # ë©”ëª¨ë¦¬ ì ˆì•½
            
            self.prediction_cache[cache_key] = cached_result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _postprocess_complete_result(self, pose_result: Dict[str, Any], image: Image.Image, start_time: float) -> Dict[str, Any]:
        """ì™„ì „í•œ ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            processing_time = time.time() - start_time
            
            # PoseMetrics ìƒì„±
            pose_metrics = PoseMetrics(
                keypoints=pose_result.get('keypoints', []),
                confidence_scores=pose_result.get('confidence_scores', []),
                model_used=pose_result.get('model_used', 'unknown'),
                processing_time=processing_time,
                image_resolution=image.size,
                ai_confidence=np.mean(pose_result.get('confidence_scores', [])) if pose_result.get('confidence_scores') else 0.0
            )
            
            # ì™„ì „í•œ í¬ì¦ˆ ë¶„ì„
            complete_pose_analysis = self._analyze_pose_quality_complete(pose_metrics)
            
            # ì‹œê°í™” ìƒì„±
            visualization = None
            if self.pose_config['visualization_enabled']:
                visualization = self._create_advanced_pose_visualization(image, pose_metrics)
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                'success': pose_result.get('success', False),
                'keypoints': pose_metrics.keypoints,
                'confidence_scores': pose_metrics.confidence_scores,
                'pose_analysis': complete_pose_analysis,
                'visualization': visualization,
                'processing_time': processing_time,
                'inference_time': pose_result.get('inference_time', 0.0),
                'model_used': pose_metrics.model_used,
                'image_resolution': pose_metrics.image_resolution,
                'step_info': {
                    'step_name': self.step_name,
                    'step_number': self.step_number,
                    'optimization_level': self.optimization_level,
                    'strict_mode': self.strict_mode,
                    'real_ai_model_name': self.active_model,
                    'ai_model_type': pose_result.get('ai_model_type', 'unknown'),
                    'dependencies_injected': sum(self.dependencies_injected.values())
                }
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ì™„ì „í•œ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_error_result(str(e))
    
    def _create_error_result(self, error_message: str, processing_time: float = 0.0) -> Dict[str, Any]:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'keypoints': [],
            'confidence_scores': [],
            'pose_analysis': {
                'suitable_for_fitting': False,
                'issues': [error_message],
                'recommendations': ['ì‹¤ì œ AI ëª¨ë¸ ìƒíƒœë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”'],
                'quality_score': 0.0,
                'ai_confidence': 0.0,
                'real_ai_analysis': True
            },
            'visualization': None,
            'processing_time': processing_time,
            'inference_time': 0.0,
            'model_used': 'error',
            'step_info': {
                'step_name': self.step_name,
                'step_number': self.step_number,
                'optimization_level': getattr(self, 'optimization_level', 'unknown'),
                'strict_mode': self.strict_mode,
                'real_ai_model_name': getattr(self, 'active_model', 'none'),
                'dependencies_injected': sum(getattr(self, 'dependencies_injected', {}).values())
            }
        }
    
    # ==============================================
    # ğŸ”¥ ìƒíƒœ ì¡°íšŒ ë° ê´€ë¦¬ ë©”ì„œë“œë“¤
    # ==============================================
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        try:
            self.prediction_cache.clear()
            self.logger.info("ğŸ“‹ AI ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_cache_status(self) -> Dict[str, Any]:
        """ìºì‹œ ìƒíƒœ ë°˜í™˜"""
        return {
            'cache_size': len(self.prediction_cache),
            'cache_max_size': self.cache_max_size,
            'cache_enabled': self.pose_config['cache_enabled'],
            'real_ai_cache': True
        }
    
    def get_step_info(self) -> Dict[str, Any]:
        """Step ì •ë³´ ë°˜í™˜"""
        
        # ê¸°ë³¸ Step ì •ë³´
        base_info = {
            "step_name": self.step_name,
            "step_number": self.step_number,
            "step_description": self.step_description,
            "is_initialized": self.is_initialized,
            "device": self.device,
            "optimization_level": getattr(self, 'optimization_level', 'unknown'),
            "strict_mode": self.strict_mode
        }
        
        # ì‹¤ì œ AI ëª¨ë¸ ìƒíƒœ ì •ë³´
        model_status = {
            "loaded_models": list(getattr(self, 'pose_models', {}).keys()),
            "active_model": getattr(self, 'active_model', None),
            "model_priority": self.pose_config['model_priority'],
            "model_interface_connected": hasattr(self, 'model_interface') and self.model_interface is not None,
            "real_ai_models_only": True,
            "dependencies_injected": self.dependencies_injected
        }
        
        # ì²˜ë¦¬ ì„¤ì • ì •ë³´
        processing_settings = {
            "confidence_threshold": self.pose_config['confidence_threshold'],
            "optimization_level": getattr(self, 'optimization_level', 'unknown'),
            "batch_processing": getattr(self, 'batch_processing', False),
            "cache_enabled": self.pose_config['cache_enabled'],
            "cache_status": self.get_cache_status(),
            "strict_mode_enabled": self.strict_mode,
            "real_ai_only": True
        }
        
        # step_model_requests.py í˜¸í™˜ ì •ë³´
        step_requirements = self._get_step_model_requirements()
        
        compliance_info = {
            "step_model_requests_compliance": True,
            "required_model_name": step_requirements["model_name"],
            "step_priority": step_requirements["step_priority"],
            "target_input_size": getattr(self, 'target_input_size', step_requirements["input_size"]),
            "optimization_params": step_requirements["optimization_params"],
            "checkpoint_patterns": step_requirements["checkpoint_patterns"],
            "alternative_models": step_requirements["alternative_models"]
        }
        
        # ì„±ëŠ¥ ë° ë©”íƒ€ë°ì´í„°
        performance_info = {
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "use_neural_engine": getattr(self, 'use_neural_engine', False),
            "supported_clothing_types": list(self.CLOTHING_POSE_WEIGHTS.keys()),
            "keypoints_format": getattr(self, 'num_keypoints', 18),
            "visualization_enabled": self.pose_config['visualization_enabled'],
            "analysis_features": [
                "pose_angles", "body_proportions", "symmetry_score", 
                "visibility_score", "clothing_suitability", "pose_type_detection"
            ]
        }
        
        return {
            **base_info,
            "model_status": model_status,
            "processing_settings": processing_settings,
            "step_requirements_compliance": compliance_info,
            "performance_info": performance_info,
            "metadata": step_requirements["metadata"]
        }
    
    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ì‹¤ì œ AI í¬ì¦ˆ ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'pose_models'):
                for model_name, model in self.pose_models.items():
                    try:
                        if hasattr(model, 'cleanup'):
                            model.cleanup()
                        elif hasattr(model, 'close'):
                            model.close()
                        elif hasattr(model, 'cpu'):
                            model.cpu()
                    except Exception as e:
                        self.logger.debug(f"AI ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨ {model_name}: {e}")
                    del model
                self.pose_models.clear()
            
            # ìºì‹œ ì •ë¦¬
            self.clear_cache()
            
            # ModelLoader ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            if hasattr(self, 'model_interface') and self.model_interface:
                try:
                    if hasattr(self.model_interface, 'unload_models'):
                        self.model_interface.unload_models()
                except Exception as e:
                    self.logger.debug(f"ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if self.device == "mps":
                    safe_mps_empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
            
            gc.collect()
            
            self.logger.info("âœ… ì™„ì „í•œ ì‹¤ì œ AI PoseEstimationStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.cleanup_resources()
        except Exception:
            pass

# =================================================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì™„ì „í•œ ê¸°ëŠ¥)
# =================================================================

def validate_openpose_keypoints(keypoints_18: List[List[float]]) -> bool:
    """OpenPose 18 keypoints ìœ íš¨ì„± ê²€ì¦"""
    try:
        if len(keypoints_18) != 18:
            return False
        
        for kp in keypoints_18:
            if len(kp) != 3:
                return False
            if not all(isinstance(x, (int, float)) for x in kp):
                return False
            if kp[2] < 0 or kp[2] > 1:
                return False
        
        return True
        
    except Exception:
        return False

def convert_keypoints_to_coco(keypoints_18: List[List[float]]) -> List[List[float]]:
    """OpenPose 18ì„ COCO 17 í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        # OpenPose 18 -> COCO 17 ë§¤í•‘
        op_to_coco_mapping = {
            0: 0,   # nose
            15: 1,  # right_eye -> left_eye (COCO ê´€ì )
            16: 2,  # left_eye -> right_eye
            17: 3,  # right_ear -> left_ear
            18: 4,  # left_ear -> right_ear
            2: 5,   # right_shoulder -> left_shoulder (COCO ê´€ì )
            5: 6,   # left_shoulder -> right_shoulder
            3: 7,   # right_elbow -> left_elbow
            6: 8,   # left_elbow -> right_elbow
            4: 9,   # right_wrist -> left_wrist
            7: 10,  # left_wrist -> right_wrist
            9: 11,  # right_hip -> left_hip
            12: 12, # left_hip -> right_hip
            10: 13, # right_knee -> left_knee
            13: 14, # left_knee -> right_knee
            11: 15, # right_ankle -> left_ankle
            14: 16  # left_ankle -> right_ankle
        }
        
        coco_keypoints = []
        for coco_idx in range(17):
            if coco_idx in op_to_coco_mapping.values():
                op_idx = next(k for k, v in op_to_coco_mapping.items() if v == coco_idx)
                if op_idx < len(keypoints_18):
                    coco_keypoints.append(keypoints_18[op_idx])
                else:
                    coco_keypoints.append([0.0, 0.0, 0.0])
            else:
                coco_keypoints.append([0.0, 0.0, 0.0])
        
        return coco_keypoints
        
    except Exception as e:
        logger.error(f"í‚¤í¬ì¸íŠ¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return [[0.0, 0.0, 0.0]] * 17

def draw_pose_on_image(
    image: Union[np.ndarray, Image.Image],
    keypoints: List[List[float]],
    confidence_threshold: float = 0.5,
    keypoint_size: int = 4,
    line_width: int = 3
) -> Image.Image:
    """ì´ë¯¸ì§€ì— í¬ì¦ˆ ê·¸ë¦¬ê¸°"""
    try:
        # ì´ë¯¸ì§€ ë³€í™˜
        if isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        else:
            pil_image = image.copy()
        
        draw = ImageDraw.Draw(pil_image)
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for i, kp in enumerate(keypoints):
            if len(kp) >= 3 and kp[2] > confidence_threshold:
                x, y = int(kp[0]), int(kp[1])
                color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                
                radius = int(keypoint_size + kp[2] * 4)
                draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                           fill=color, outline=(255, 255, 255), width=2)
        
        # ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸°
        for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
            if (start_idx < len(keypoints) and end_idx < len(keypoints)):
                start_kp = keypoints[start_idx]
                end_kp = keypoints[end_idx]
                
                if (len(start_kp) >= 3 and len(end_kp) >= 3 and
                    start_kp[2] > confidence_threshold and end_kp[2] > confidence_threshold):
                    
                    start_point = (int(start_kp[0]), int(start_kp[1]))
                    end_point = (int(end_kp[0]), int(end_kp[1]))
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    avg_confidence = (start_kp[2] + end_kp[2]) / 2
                    adjusted_width = int(line_width * avg_confidence)
                    
                    draw.line([start_point, end_point], fill=color, width=max(1, adjusted_width))
        
        return pil_image
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
        return image if isinstance(image, Image.Image) else Image.fromarray(image)

def analyze_pose_for_clothing(
    keypoints: List[List[float]],
    clothing_type: str = "default",
    confidence_threshold: float = 0.5,
    strict_analysis: bool = True
) -> Dict[str, Any]:
    """ì˜ë¥˜ë³„ í¬ì¦ˆ ì í•©ì„± ë¶„ì„"""
    try:
        if not keypoints:
            return {
                'suitable_for_fitting': False,
                'issues': ["ì™„ì „í•œ ì‹¤ì œ AI ëª¨ë¸ì—ì„œ í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"],
                'recommendations': ["ì‹¤ì œ AI ëª¨ë¸ ìƒíƒœë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë” ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”"],
                'pose_score': 0.0,
                'ai_confidence': 0.0,
                'real_ai_based_analysis': True
            }
        
        # ì˜ë¥˜ë³„ ê°€ì¤‘ì¹˜
        weights = PoseEstimationStep.CLOTHING_POSE_WEIGHTS.get(
            clothing_type, 
            PoseEstimationStep.CLOTHING_POSE_WEIGHTS['default']
        )
        
        # ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°
        def calculate_body_part_score(part_indices: List[int]) -> float:
            visible_count = 0
            total_confidence = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
            
            if visible_count == 0:
                return 0.0
            
            return (visible_count / len(part_indices)) * (total_confidence / visible_count)
        
        # ë¶€ìœ„ë³„ ì ìˆ˜
        head_indices = [0, 15, 16, 17, 18]
        torso_indices = [1, 2, 5, 8, 9, 12]
        arm_indices = [2, 3, 4, 5, 6, 7]
        leg_indices = [9, 10, 11, 12, 13, 14]
        
        head_score = calculate_body_part_score(head_indices)
        torso_score = calculate_body_part_score(torso_indices)
        arms_score = calculate_body_part_score(arm_indices)
        legs_score = calculate_body_part_score(leg_indices)
        
        # AI ì‹ ë¢°ë„ ë°˜ì˜ ê°€ì¤‘ í‰ê· 
        ai_confidence = np.mean([kp[2] for kp in keypoints if len(kp) > 2]) if keypoints else 0.0
        
        pose_score = (
            torso_score * weights.get('torso', 0.4) +
            arms_score * weights.get('arms', 0.3) +
            legs_score * weights.get('legs', 0.2) +
            weights.get('visibility', 0.1) * min(head_score, 1.0)
        ) * ai_confidence
        
        # ì í•©ì„± íŒë‹¨
        min_score = 0.8 if strict_analysis else 0.7
        min_confidence = 0.75 if strict_analysis else 0.65
        suitable_for_fitting = (pose_score >= min_score and 
                              ai_confidence >= min_confidence)
        
        # ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­
        issues = []
        recommendations = []
        
        if ai_confidence < min_confidence:
            issues.append(f'ì‹¤ì œ AI ëª¨ë¸ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({ai_confidence:.3f})')
            recommendations.append('ì¡°ëª…ì´ ì¢‹ì€ í™˜ê²½ì—ì„œ ë” ì„ ëª…í•˜ê²Œ ë‹¤ì‹œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        if torso_score < 0.5:
            issues.append(f'{clothing_type} ì°©ìš©ì— ì¤‘ìš”í•œ ìƒì²´ê°€ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤')
            recommendations.append('ìƒì²´ ì „ì²´ê°€ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        return {
            'suitable_for_fitting': suitable_for_fitting,
            'issues': issues,
            'recommendations': recommendations,
            'pose_score': pose_score,
            'ai_confidence': ai_confidence,
            'detailed_scores': {
                'head': head_score,
                'torso': torso_score,
                'arms': arms_score,
                'legs': legs_score
            },
            'clothing_type': clothing_type,
            'weights_used': weights,
            'real_ai_based_analysis': True,
            'strict_analysis': strict_analysis
        }
        
    except Exception as e:
        logger.error(f"ì˜ë¥˜ë³„ í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            'suitable_for_fitting': False,
            'issues': ["ì™„ì „í•œ ì‹¤ì œ AI ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨"],
            'recommendations': ["ì‹¤ì œ AI ëª¨ë¸ ìƒíƒœë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”"],
            'pose_score': 0.0,
            'ai_confidence': 0.0,
            'real_ai_based_analysis': True
        }

# =================================================================
# ğŸ”¥ í˜¸í™˜ì„± ì§€ì› í•¨ìˆ˜ë“¤ (StepFactory ì—°ë™)
# =================================================================

async def create_pose_estimation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    strict_mode: bool = True,
    **kwargs
) -> PoseEstimationStep:
    """
    ì™„ì „í•œ ì‹¤ì œ AI Step 02 ìƒì„± í•¨ìˆ˜ - StepFactory ì—°ë™ êµ¬ì¡°
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        strict_mode: ì—„ê²© ëª¨ë“œ
        **kwargs: ì¶”ê°€ ì„¤ì •
        
    Returns:
        PoseEstimationStep: ì´ˆê¸°í™”ëœ ì‹¤ì œ AI í¬ì¦ˆ ì¶”ì • Step
    """
    try:
        # ë””ë°”ì´ìŠ¤ ì²˜ë¦¬
        device_param = None if device == "auto" else device
        
        # config í†µí•©
        if config is None:
            config = {}
        config.update(kwargs)
        config['real_ai_only'] = True
        
        # Step ìƒì„±
        step = PoseEstimationStep(device=device_param, config=config, strict_mode=strict_mode)
        
        # ì™„ì „í•œ AI ì´ˆê¸°í™” ì‹¤í–‰
        initialization_success = await step.initialize()
        
        if not initialization_success:
            error_msg = "ì™„ì „í•œ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨"
            if strict_mode:
                raise RuntimeError(f"Strict Mode: {error_msg}")
            else:
                step.logger.warning(f"âš ï¸ {error_msg} - Step ìƒì„±ì€ ì™„ë£Œë¨")
        
        return step
        
    except Exception as e:
        logger.error(f"âŒ create_pose_estimation_step ì‹¤íŒ¨: {e}")
        if strict_mode:
            raise
        else:
            step = PoseEstimationStep(device='cpu', strict_mode=False)
            return step

def create_pose_estimation_step_sync(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    strict_mode: bool = True,
    **kwargs
) -> PoseEstimationStep:
    """ë™ê¸°ì‹ ì™„ì „í•œ AI Step 02 ìƒì„±"""
    try:
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            create_pose_estimation_step(device, config, strict_mode, **kwargs)
        )
    except Exception as e:
        logger.error(f"âŒ create_pose_estimation_step_sync ì‹¤íŒ¨: {e}")
        if strict_mode:
            raise
        else:
            return PoseEstimationStep(device='cpu', strict_mode=False)

# =================================================================
# ğŸ”¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# =================================================================

async def test_complete_real_ai_pose_estimation():
    """ì™„ì „í•œ ì‹¤ì œ AI í¬ì¦ˆ ì¶”ì • í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ ì™„ì „í•œ ì‹¤ì œ AI í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        print("=" * 70)
        
        # Step ìƒì„±
        step = await create_pose_estimation_step(
            device="auto",
            strict_mode=True,
            config={
                'confidence_threshold': 0.5,
                'visualization_enabled': True,
                'cache_enabled': True,
                'detailed_analysis': True,
                'real_ai_only': True
            }
        )
        
        # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
        dummy_image = np.zeros((512, 512, 3), dtype=np.uint8)
        dummy_image_pil = Image.fromarray(dummy_image)
        
        print(f"ğŸ“‹ ì™„ì „í•œ AI Step ì •ë³´:")
        step_info = step.get_step_info()
        print(f"   ğŸ¯ Step: {step_info['step_name']}")
        print(f"   ğŸ¤– AI ëª¨ë¸: {step_info['model_status']['active_model']}")
        print(f"   ğŸ”’ Strict Mode: {step_info['strict_mode']}")
        print(f"   ğŸ’‰ ì˜ì¡´ì„± ì£¼ì…: {step_info['model_status']['dependencies_injected']}")
        print(f"   ğŸ’ ì‹¤ì œ AI ì „ìš©: {step_info['processing_settings']['real_ai_only']}")
        
        # AI ëª¨ë¸ë¡œ ì²˜ë¦¬
        result = await step.process(dummy_image_pil, clothing_type="shirt")
        
        if result['success']:
            print(f"âœ… ì™„ì „í•œ AI í¬ì¦ˆ ì¶”ì • ì„±ê³µ")
            print(f"ğŸ¯ AI í‚¤í¬ì¸íŠ¸ ìˆ˜: {len(result['keypoints'])}")
            print(f"ğŸ–ï¸ AI ì‹ ë¢°ë„: {result['pose_analysis']['ai_confidence']:.3f}")
            print(f"ğŸ’ í’ˆì§ˆ ì ìˆ˜: {result['pose_analysis']['quality_score']:.3f}")
            print(f"ğŸ”¬ í¬ì¦ˆ íƒ€ì…: {result['pose_analysis']['pose_type']}")
            print(f"ğŸ‘• ì˜ë¥˜ ì í•©ì„±: {result['pose_analysis']['suitable_for_fitting']}")
            print(f"ğŸ¤– ì‚¬ìš©ëœ AI ëª¨ë¸: {result['model_used']}")
            print(f"âš¡ ì¶”ë¡  ì‹œê°„: {result.get('inference_time', 0):.3f}ì´ˆ")
        else:
            print(f"âŒ ì™„ì „í•œ AI í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {result.get('error', 'Unknown Error')}")
        
        # ì •ë¦¬
        step.cleanup_resources()
        print("ğŸ§¹ ì™„ì „í•œ AI ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ì™„ì „í•œ AI í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

async def test_model_loader_integration_complete():
    """ì™„ì „í•œ ModelLoader í†µí•© í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ¤– ì™„ì „í•œ AI ModelLoader í†µí•© í…ŒìŠ¤íŠ¸")
        print("=" * 70)
        
        # ModelLoader ìƒíƒœ í™•ì¸
        if not MODEL_LOADER_AVAILABLE:
            print("âŒ ModelLoaderë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # Global ModelLoader ê°€ì ¸ì˜¤ê¸°
        model_loader = get_global_model_loader()
        if not model_loader:
            print("âŒ Global ModelLoaderë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        print(f"âœ… ì™„ì „í•œ AI ModelLoader ì‚¬ìš© ê°€ëŠ¥")
        
        # Step ìƒì„± ë° ì˜ì¡´ì„± ì£¼ì… í™•ì¸
        step = PoseEstimationStep(device="auto", strict_mode=True)
        
        # ìˆ˜ë™ ì˜ì¡´ì„± ì£¼ì… í…ŒìŠ¤íŠ¸
        step.set_model_loader(model_loader)
        
        await step.initialize()
        
        print(f"ğŸ”— AI Model Interface: {step.model_interface is not None}")
        print(f"ğŸ¯ Active AI Model: {step.active_model}")
        print(f"ğŸ“¦ Loaded AI Models: {list(step.pose_models.keys()) if hasattr(step, 'pose_models') else []}")
        print(f"ğŸ’‰ ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ: {step.get_injected_dependencies()}")
        print(f"ğŸ’ Real AI Only: {step.pose_config.get('real_ai_only', False)}")
        
        # ì •ë¦¬
        step.cleanup_resources()
        
    except Exception as e:
        print(f"âŒ ì™„ì „í•œ AI ModelLoader í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_keypoint_conversion():
    """í‚¤í¬ì¸íŠ¸ ë³€í™˜ í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”„ í‚¤í¬ì¸íŠ¸ ë³€í™˜ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        # ë”ë¯¸ OpenPose 18 í‚¤í¬ì¸íŠ¸
        openpose_keypoints = [
            [100, 50, 0.9],   # nose
            [100, 80, 0.8],   # neck
            [80, 100, 0.7],   # right_shoulder
            [70, 130, 0.6],   # right_elbow
            [60, 160, 0.5],   # right_wrist
            [120, 100, 0.7],  # left_shoulder
            [130, 130, 0.6],  # left_elbow
            [140, 160, 0.5],  # left_wrist
            [100, 200, 0.8],  # middle_hip
            [90, 200, 0.7],   # right_hip
            [85, 250, 0.6],   # right_knee
            [80, 300, 0.5],   # right_ankle
            [110, 200, 0.7],  # left_hip
            [115, 250, 0.6],  # left_knee
            [120, 300, 0.5],  # left_ankle
            [95, 40, 0.8],    # right_eye
            [105, 40, 0.8],   # left_eye
            [90, 45, 0.7],    # right_ear
            [110, 45, 0.7]    # left_ear
        ]
        
        # ìœ íš¨ì„± ê²€ì¦
        is_valid = validate_openpose_keypoints(openpose_keypoints)
        print(f"âœ… OpenPose 18 ìœ íš¨ì„±: {is_valid}")
        
        # COCO 17ë¡œ ë³€í™˜
        coco_keypoints = convert_keypoints_to_coco(openpose_keypoints)
        print(f"ğŸ”„ COCO 17 ë³€í™˜: {len(coco_keypoints)}ê°œ í‚¤í¬ì¸íŠ¸")
        
        # ì˜ë¥˜ë³„ ë¶„ì„
        analysis = analyze_pose_for_clothing(
            openpose_keypoints, 
            clothing_type="shirt",
            strict_analysis=True
        )
        print(f"ğŸ‘• ì˜ë¥˜ ì í•©ì„± ë¶„ì„:")
        print(f"   ì í•©ì„±: {analysis['suitable_for_fitting']}")
        print(f"   ì ìˆ˜: {analysis['pose_score']:.3f}")
        print(f"   AI ì‹ ë¢°ë„: {analysis['ai_confidence']:.3f}")
        
    except Exception as e:
        print(f"âŒ í‚¤í¬ì¸íŠ¸ ë³€í™˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# =================================================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# =================================================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'PoseEstimationStep',
    'RealOpenPoseModel',
    'RealYOLOv8PoseModel', 
    'RealLightweightPoseModel',
    'PoseMetrics',
    'PoseModel',
    'PoseQuality', 
    'PoseType',
    
    # ìƒì„± í•¨ìˆ˜ë“¤
    'create_pose_estimation_step',
    'create_pose_estimation_step_sync',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'validate_openpose_keypoints',
    'convert_keypoints_to_coco',
    'draw_pose_on_image',
    'analyze_pose_for_clothing',
    
    # ìƒìˆ˜ë“¤
    'OPENPOSE_18_KEYPOINTS',
    'KEYPOINT_COLORS',
    'SKELETON_CONNECTIONS',
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
    'test_complete_real_ai_pose_estimation',
    'test_model_loader_integration_complete',
    'test_keypoint_conversion'
]

# =================================================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê·¸
# =================================================================

logger.info("ğŸ”¥ ì™„ì „í•œ ì‹¤ì œ AI PoseEstimationStep v8.0 ë¡œë“œ ì™„ë£Œ")
logger.info("âœ… StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… â†’ ì™„ì„±ëœ Step êµ¬ì¡° ì™„ì „ êµ¬í˜„")
logger.info("ğŸ”§ ì²´í¬í¬ì¸íŠ¸ â†’ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ ë³€í™˜ ì™„ì „ í•´ê²° (Step 01 ì´ìŠˆ í•´ê²°)")
logger.info("ğŸ§  OpenPose, YOLOv8, ê²½ëŸ‰ ëª¨ë¸ ì‹¤ì œ AI ì¶”ë¡  ì—”ì§„ ë‚´ì¥")
logger.info("ğŸ”— BaseStepMixin ì™„ì „ ìƒì† - ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì™„ë²½ êµ¬í˜„")
logger.info("ğŸ’‰ ModelLoader ì™„ì „ ì—°ë™ - ìˆœí™˜ì°¸ì¡° ì—†ëŠ” í•œë°©í–¥ ì°¸ì¡°")
logger.info("ğŸ¯ 18ê°œ í‚¤í¬ì¸íŠ¸ OpenPose í‘œì¤€ + COCO 17 ë³€í™˜ ì§€ì›")
logger.info("ğŸ”’ Strict Mode ì§€ì› - ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì—ëŸ¬")
logger.info("ğŸ”¬ ì™„ì „í•œ ë¶„ì„ - ê°ë„, ë¹„ìœ¨, ëŒ€ì¹­ì„±, ê°€ì‹œì„±, í’ˆì§ˆ í‰ê°€")
logger.info("ğŸ M3 Max 128GB ìµœì í™” + conda í™˜ê²½ ìš°ì„ ")
logger.info("ğŸš€ í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„±")

# ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê¹…
logger.info(f"ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ: PyTorch={TORCH_AVAILABLE}, OpenCV={CV2_AVAILABLE}, PIL={PIL_AVAILABLE}")
logger.info(f"ğŸ”§ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „: PyTorch={TORCH_VERSION}, OpenCV={CV2_VERSION if CV2_AVAILABLE else 'Fallback'}, PIL={PIL_VERSION}")
logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§: {'í™œì„±í™”' if PSUTIL_AVAILABLE else 'ë¹„í™œì„±í™”'}")
logger.info(f"ğŸ”— ì˜ì¡´ì„± ìƒíƒœ: ModelLoader={MODEL_LOADER_AVAILABLE}, MemoryManager={MEMORY_MANAGER_AVAILABLE}")

# =================================================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ë¶€ (ê°œë°œ ë° í…ŒìŠ¤íŠ¸ìš©)
# =================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¯ MyCloset AI Step 02 - ì™„ì „í•œ ì‹¤ì œ AI ì—°ë™ ë° ì¶”ë¡  ë²„ì „")
    print("=" * 80)
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    async def run_all_tests():
        await test_complete_real_ai_pose_estimation()
        print("\n" + "=" * 80)
        await test_model_loader_integration_complete()
        print("\n" + "=" * 80)
        test_keypoint_conversion()
    
    try:
        asyncio.run(run_all_tests())
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 80)
    print("âœ¨ ì™„ì „í•œ ì‹¤ì œ AI í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("ğŸ”¥ StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… â†’ ì™„ì„±ëœ Step êµ¬ì¡°")
    print("ğŸ§  ì²´í¬í¬ì¸íŠ¸ â†’ ã…Œì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ ë³€í™˜ ì™„ì „ êµ¬í˜„")
    print("âš¡ OpenPose, YOLOv8, ê²½ëŸ‰ ëª¨ë¸ ì‹¤ì œ ì¶”ë¡  ì—”ì§„")
    print("ğŸ’‰ ì™„ë²½í•œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´")
    print("ğŸ”’ Strict Mode + ì™„ì „í•œ ë¶„ì„ ê¸°ëŠ¥")
    print("=" * 80)