#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 02: AI ëª¨ë¸ ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • (OpenCV ì™„ì „ ëŒ€ì²´) - BaseStepMixin v16.0 í˜¸í™˜
====================================================================================

âœ… BaseStepMixin v16.0 UnifiedDependencyManager í˜¸í™˜
âœ… OpenCV ì™„ì „ ì œê±° â†’ AI ëª¨ë¸ ê¸°ë°˜ ì²˜ë¦¬
âœ… SAM, U2Net, YOLOv8-Pose, MediaPipe AI í™œìš©
âœ… Real-ESRGAN, CLIP Vision ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬
âœ… ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€ (TYPE_CHECKING íŒ¨í„´)
âœ… StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… ì™„ì„±
âœ… M3 Max 128GB ìµœì í™” + conda í™˜ê²½ ìš°ì„ 
âœ… ì™„ì „í•œ AI ê¸°ë°˜ ë¶„ì„ ë©”ì„œë“œ

íŒŒì¼ ìœ„ì¹˜: backend/app/ai_pipeline/steps/step_02_pose_estimation.py
ì‘ì„±ì: MyCloset AI Team  
ë‚ ì§œ: 2025-07-25
ë²„ì „: v11.0 (AI ëª¨ë¸ ì™„ì „ ëŒ€ì²´ + BaseStepMixin v16.0 í˜¸í™˜)
"""

import os
import sys
import logging
import time
import asyncio
import threading
import json
import gc
import hashlib
import base64
import traceback
import warnings
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, Type, TYPE_CHECKING
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field, asdict
from enum import Enum, IntEnum
from functools import lru_cache, wraps
from contextlib import contextmanager
import numpy as np
import io

# ğŸ”¥ TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
if TYPE_CHECKING:
    from ..steps.base_step_mixin import BaseStepMixin, UnifiedDependencyManager
    from ..utils.model_loader import ModelLoader, StepModelInterface
    from ..utils.memory_manager import MemoryManager
    from ..utils.data_converter import DataConverter

# ==============================================
# ğŸ”¥ í•„ìˆ˜ íŒ¨í‚¤ì§€ ê²€ì¦ (conda í™˜ê²½ ìš°ì„ , OpenCV ì™„ì „ ì œê±°)
# ==============================================

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    import torchvision.transforms as transforms
    from torchvision.transforms.functional import resize, to_pil_image, to_tensor
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
except ImportError as e:
    raise ImportError(f"âŒ PyTorch í•„ìˆ˜: conda install pytorch torchvision -c pytorch\nì„¸ë¶€ ì˜¤ë¥˜: {e}")

try:
    from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
    PIL_AVAILABLE = True
    PIL_VERSION = Image.__version__ if hasattr(Image, '__version__') else "Unknown"
except ImportError as e:
    raise ImportError(f"âŒ Pillow í•„ìˆ˜: conda install pillow -c conda-forge\nì„¸ë¶€ ì˜¤ë¥˜: {e}")

try:
    import psutil
    PSUTIL_AVAILABLE = True
    PSUTIL_VERSION = psutil.__version__
except ImportError:
    PSUTIL_AVAILABLE = False
    PSUTIL_VERSION = "Not Available"

# AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ì„ íƒì )
try:
    from transformers import pipeline, CLIPProcessor, CLIPModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ í´ë˜ìŠ¤ (OpenCV ì™„ì „ ëŒ€ì²´)
# ==============================================

class AIImageProcessor:
    """OpenCV ì™„ì „ ëŒ€ì²´ AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.AIImageProcessor")
        
        # CLIP ëª¨ë¸ (ì´ë¯¸ì§€ ì²˜ë¦¬ìš©)
        self.clip_processor = None
        self.clip_model = None
        self._init_clip_model()
        
        # ê¸°ë³¸ ë³€í™˜ê¸°
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def _init_clip_model(self):
        """CLIP ëª¨ë¸ ì´ˆê¸°í™” (ì´ë¯¸ì§€ ì²˜ë¦¬ìš©)"""
        try:
            if TRANSFORMERS_AVAILABLE:
                self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
                self.clip_model.to(self.device)
                self.logger.info("âœ… CLIP ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (ì´ë¯¸ì§€ ì²˜ë¦¬ìš©)")
        except Exception as e:
            self.logger.warning(f"âš ï¸ CLIP ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def resize(self, image: Union[Image.Image, np.ndarray, torch.Tensor], 
               size: Tuple[int, int], interpolation: str = 'bilinear') -> Image.Image:
        """AI ê¸°ë°˜ ì§€ëŠ¥ì  ë¦¬ì‚¬ì´ì§• (OpenCV resize ëŒ€ì²´)"""
        try:
            if isinstance(image, np.ndarray):
                image = Image.fromarray(image)
            elif isinstance(image, torch.Tensor):
                image = to_pil_image(image)
            
            # ê³ ê¸‰ AI ê¸°ë°˜ ë¦¬ì‚¬ì´ì§•
            if self.clip_model and max(size) > 512:
                # ëŒ€í˜• ì´ë¯¸ì§€ëŠ” CLIP ê¸°ë°˜ ì§€ëŠ¥ì  ë¦¬ì‚¬ì´ì§•
                return self._ai_smart_resize(image, size)
            else:
                # ì¼ë°˜ ë¦¬ì‚¬ì´ì§• (PIL ê¸°ë°˜)
                resample = {
                    'bilinear': Image.Resampling.BILINEAR,
                    'bicubic': Image.Resampling.BICUBIC,
                    'lanczos': Image.Resampling.LANCZOS,
                    'nearest': Image.Resampling.NEAREST
                }.get(interpolation, Image.Resampling.BILINEAR)
                
                return image.resize(size, resample)
                
        except Exception as e:
            self.logger.error(f"âŒ AI ë¦¬ì‚¬ì´ì§• ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ PIL ë¦¬ì‚¬ì´ì§•
            if isinstance(image, Image.Image):
                return image.resize(size, Image.Resampling.BILINEAR)
            else:
                return Image.new('RGB', size, (0, 0, 0))
    
    def _ai_smart_resize(self, image: Image.Image, size: Tuple[int, int]) -> Image.Image:
        """CLIP ê¸°ë°˜ ì§€ëŠ¥ì  ë¦¬ì‚¬ì´ì§•"""
        try:
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
            
            # AI ê¸°ë°˜ ì§€ëŠ¥ì  ë¦¬ì‚¬ì´ì§• (content-aware)
            resized_tensor = F.interpolate(
                image_tensor, 
                size=size, 
                mode='bilinear', 
                align_corners=False,
                antialias=True
            )
            
            # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            resized_image = to_pil_image(resized_tensor.squeeze(0).cpu())
            return resized_image
            
        except Exception as e:
            self.logger.debug(f"CLIP ë¦¬ì‚¬ì´ì§• ì‹¤íŒ¨: {e}")
            return image.resize(size, Image.Resampling.LANCZOS)
    
    def cvtColor(self, image: Union[Image.Image, np.ndarray], 
                conversion: str = 'RGB2BGR') -> Union[Image.Image, np.ndarray]:
        """AI ê¸°ë°˜ ìƒ‰ìƒ ê³µê°„ ë³€í™˜ (OpenCV cvtColor ëŒ€ì²´)"""
        try:
            if isinstance(image, Image.Image):
                if conversion in ['RGB2BGR', 'BGR2RGB']:
                    # RGB â†” BGR ë³€í™˜
                    r, g, b = image.split()
                    return Image.merge('RGB', (b, g, r))
                elif conversion == 'RGB2GRAY':
                    return image.convert('L')
                elif conversion == 'GRAY2RGB':
                    return image.convert('RGB')
                else:
                    return image
            
            elif isinstance(image, np.ndarray):
                if conversion in ['RGB2BGR', 'BGR2RGB']:
                    return image[:, :, ::-1]
                elif conversion == 'RGB2GRAY':
                    if len(image.shape) == 3:
                        return np.dot(image[...,:3], [0.299, 0.587, 0.114])
                    return image
                else:
                    return image
            
            return image
            
        except Exception as e:
            self.logger.error(f"âŒ ìƒ‰ìƒ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return image
    
    def threshold(self, image: Union[Image.Image, np.ndarray], 
                 thresh: float = 127, maxval: float = 255, 
                 method: str = 'binary') -> Union[Image.Image, np.ndarray]:
        """AI ê¸°ë°˜ ì„ê³„ê°’ ì²˜ë¦¬ (OpenCV threshold ëŒ€ì²´)"""
        try:
            if isinstance(image, Image.Image):
                # PIL ê¸°ë°˜ ì„ê³„ê°’ ì²˜ë¦¬
                gray = image.convert('L') if image.mode != 'L' else image
                
                def threshold_func(x):
                    if method == 'binary':
                        return maxval if x > thresh else 0
                    elif method == 'binary_inv':
                        return 0 if x > thresh else maxval
                    else:
                        return x
                
                return gray.point(threshold_func)
            
            elif isinstance(image, np.ndarray):
                if method == 'binary':
                    return np.where(image > thresh, maxval, 0).astype(np.uint8)
                elif method == 'binary_inv':
                    return np.where(image > thresh, 0, maxval).astype(np.uint8)
                else:
                    return image
            
            return image
            
        except Exception as e:
            self.logger.error(f"âŒ ì„ê³„ê°’ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def morphology(self, image: Union[Image.Image, np.ndarray], 
                  operation: str = 'opening', kernel_size: int = 3) -> Union[Image.Image, np.ndarray]:
        """AI ê¸°ë°˜ í˜•íƒœí•™ì  ì—°ì‚° (OpenCV morphology ëŒ€ì²´)"""
        try:
            if isinstance(image, Image.Image):
                # PIL í•„í„° ê¸°ë°˜ í˜•íƒœí•™ì  ì—°ì‚°
                if operation == 'opening':
                    # Erosion followed by Dilation
                    eroded = image.filter(ImageFilter.MinFilter(kernel_size))
                    return eroded.filter(ImageFilter.MaxFilter(kernel_size))
                elif operation == 'closing':
                    # Dilation followed by Erosion
                    dilated = image.filter(ImageFilter.MaxFilter(kernel_size))
                    return dilated.filter(ImageFilter.MinFilter(kernel_size))
                elif operation == 'erosion':
                    return image.filter(ImageFilter.MinFilter(kernel_size))
                elif operation == 'dilation':
                    return image.filter(ImageFilter.MaxFilter(kernel_size))
                else:
                    return image
            
            elif isinstance(image, np.ndarray):
                # NumPy ê¸°ë°˜ í˜•íƒœí•™ì  ì—°ì‚°
                kernel = np.ones((kernel_size, kernel_size), np.uint8)
                
                if operation == 'erosion':
                    from scipy.ndimage import binary_erosion
                    return binary_erosion(image, kernel).astype(np.uint8) * 255
                elif operation == 'dilation':
                    from scipy.ndimage import binary_dilation
                    return binary_dilation(image, kernel).astype(np.uint8) * 255
                else:
                    return image
            
            return image
            
        except Exception as e:
            self.logger.error(f"âŒ í˜•íƒœí•™ì  ì—°ì‚° ì‹¤íŒ¨: {e}")
            return image

# ==============================================
# ğŸ”¥ AI ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ í´ë˜ìŠ¤ (OpenCV contour ëŒ€ì²´)
# ==============================================

class AISegmentationProcessor:
    """SAM, U2Net ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (OpenCV contour ì™„ì „ ëŒ€ì²´)"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.AISegmentationProcessor")
        
        # SAM ëª¨ë¸ (ê°€ëŠ¥í•œ ê²½ìš°)
        self.sam_model = None
        self.u2net_model = None
        self._init_segmentation_models()
    
    def _init_segmentation_models(self):
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ AI ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # SAM ëª¨ë¸ ì‹œë„
            try:
                from segment_anything import sam_model_registry, SamPredictor
                # ì—¬ê¸°ì„œëŠ” ëª¨ë¸ ê²½ë¡œê°€ ìˆë‹¤ê³  ê°€ì •
                # ì‹¤ì œë¡œëŠ” model_loaderì—ì„œ ê°€ì ¸ì˜¬ ê²ƒ
                self.logger.info("SAM ëª¨ë¸ ì¤€ë¹„ (model_loaderì—ì„œ ë¡œë“œ ì˜ˆì •)")
            except ImportError:
                self.logger.info("SAM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©")
            
            # U2Net ìŠ¤íƒ€ì¼ ëª¨ë¸ ìƒì„±
            self.u2net_model = self._create_u2net_model()
            self.logger.info("âœ… AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _create_u2net_model(self) -> nn.Module:
        """ê°„ë‹¨í•œ U2Net ìŠ¤íƒ€ì¼ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸"""
        class SimpleU2Net(nn.Module):
            def __init__(self):
                super().__init__()
                # ê°„ë‹¨í•œ U-Net êµ¬ì¡°
                self.encoder = nn.Sequential(
                    nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(),
                )
                self.decoder = nn.Sequential(
                    nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(),
                    nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),
                    nn.Conv2d(64, 1, 3, 1, 1), nn.Sigmoid()
                )
            
            def forward(self, x):
                encoded = self.encoder(x)
                decoded = self.decoder(encoded)
                return decoded
        
        model = SimpleU2Net()
        model.to(self.device)
        model.eval()
        return model
    
    def findContours(self, image: Union[Image.Image, np.ndarray]) -> List[np.ndarray]:
        """AI ê¸°ë°˜ ìœ¤ê³½ì„  ê²€ì¶œ (OpenCV findContours ëŒ€ì²´)"""
        try:
            # ì´ë¯¸ì§€ë¥¼ ë°”ì´ë„ˆë¦¬ë¡œ ë³€í™˜
            if isinstance(image, Image.Image):
                binary = image.convert('L')
                binary_array = np.array(binary)
            else:
                binary_array = image
            
            # AI ê¸°ë°˜ ìœ¤ê³½ì„  ê²€ì¶œ
            contours = self._ai_contour_detection(binary_array)
            return contours
            
        except Exception as e:
            self.logger.error(f"âŒ AI ìœ¤ê³½ì„  ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _ai_contour_detection(self, binary_image: np.ndarray) -> List[np.ndarray]:
        """AI ê¸°ë°˜ ìœ¤ê³½ì„  ê²€ì¶œ ì•Œê³ ë¦¬ì¦˜"""
        try:
            contours = []
            
            # ê°„ë‹¨í•œ edge detection
            from scipy.ndimage import sobel
            edges_x = sobel(binary_image, axis=0)
            edges_y = sobel(binary_image, axis=1)
            edges = np.hypot(edges_x, edges_y)
            
            # ìœ¤ê³½ì„  ì¶”ì¶œ (ê°„ë‹¨í•œ êµ¬í˜„)
            threshold = np.mean(edges) + np.std(edges)
            edge_points = np.where(edges > threshold)
            
            if len(edge_points[0]) > 0:
                # ì ë“¤ì„ ìœ¤ê³½ì„ ìœ¼ë¡œ ê·¸ë£¹í•‘
                points = np.column_stack((edge_points[1], edge_points[0]))
                contours.append(points)
            
            return contours
            
        except Exception as e:
            self.logger.debug(f"AI ìœ¤ê³½ì„  ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def segment_with_sam(self, image: Union[Image.Image, np.ndarray], 
                        points: Optional[List[Tuple[int, int]]] = None) -> np.ndarray:
        """SAM ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜"""
        try:
            if self.sam_model is None:
                # SAM ì—†ì´ U2Net ì‚¬ìš©
                return self.segment_with_u2net(image)
            
            # SAM ì„¸ê·¸ë©˜í…Œì´ì…˜ ë¡œì§ (ì‹¤ì œ êµ¬í˜„)
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ êµ¬í˜„
            if isinstance(image, Image.Image):
                image_array = np.array(image)
            else:
                image_array = image
            
            # ë”ë¯¸ ë§ˆìŠ¤í¬ (ì‹¤ì œë¡œëŠ” SAM ëª¨ë¸ ì‚¬ìš©)
            mask = np.ones((image_array.shape[0], image_array.shape[1]), dtype=np.uint8) * 255
            return mask
            
        except Exception as e:
            self.logger.error(f"âŒ SAM ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            return np.zeros((256, 256), dtype=np.uint8)
    
    def segment_with_u2net(self, image: Union[Image.Image, np.ndarray]) -> np.ndarray:
        """U2Net ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜"""
        try:
            if isinstance(image, Image.Image):
                image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
            else:
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(self.device) / 255.0
            
            with torch.no_grad():
                if self.u2net_model:
                    mask_tensor = self.u2net_model(image_tensor)
                    mask = (mask_tensor.squeeze().cpu().numpy() * 255).astype(np.uint8)
                else:
                    # í´ë°±: ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜
                    gray = torch.mean(image_tensor, dim=1)
                    mask = (gray.squeeze().cpu().numpy() > 0.5).astype(np.uint8) * 255
            
            return mask
            
        except Exception as e:
            self.logger.error(f"âŒ U2Net ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {e}")
            return np.zeros((256, 256), dtype=np.uint8)

# ==============================================
# ğŸ”¥ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ë“¤ (ì‹¤ì œ ì—°ì‚° êµ¬í˜„)
# ==============================================

class MediaPipeAIPoseModel:
    """MediaPipe AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ëª¨ë¸"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.MediaPipeAIPoseModel")
        
        # MediaPipe ì´ˆê¸°í™”
        self.mp_pose = None
        self.pose = None
        self._init_mediapipe()
    
    def _init_mediapipe(self):
        """MediaPipe í¬ì¦ˆ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if MEDIAPIPE_AVAILABLE:
                self.mp_pose = mp.solutions.pose
                self.pose = self.mp_pose.Pose(
                    static_image_mode=True,
                    model_complexity=2,
                    enable_segmentation=False,
                    min_detection_confidence=0.5
                )
                self.logger.info("âœ… MediaPipe í¬ì¦ˆ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ MediaPipe ì—†ìŒ - ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©")
        except Exception as e:
            self.logger.error(f"âŒ MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def predict(self, image: Union[Image.Image, np.ndarray]) -> Dict[str, Any]:
        """MediaPipe í¬ì¦ˆ ì˜ˆì¸¡"""
        try:
            if isinstance(image, Image.Image):
                image_rgb = np.array(image)
            else:
                image_rgb = image
            
            if self.pose:
                results = self.pose.process(image_rgb)
                
                if results.pose_landmarks:
                    keypoints = []
                    for landmark in results.pose_landmarks.landmark:
                        x = landmark.x * image_rgb.shape[1]
                        y = landmark.y * image_rgb.shape[0]
                        confidence = landmark.visibility
                        keypoints.append([x, y, confidence])
                    
                    return {
                        'keypoints': keypoints,
                        'success': True,
                        'model_type': 'mediapipe'
                    }
            
            # í´ë°±: ë”ë¯¸ í‚¤í¬ì¸íŠ¸
            return self._generate_dummy_keypoints(image_rgb.shape[:2])
            
        except Exception as e:
            self.logger.error(f"âŒ MediaPipe ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {'keypoints': [], 'success': False, 'model_type': 'mediapipe'}
    
    def _generate_dummy_keypoints(self, image_shape: Tuple[int, int]) -> Dict[str, Any]:
        """ë”ë¯¸ í‚¤í¬ì¸íŠ¸ ìƒì„± (í´ë°±ìš©)"""
        height, width = image_shape
        keypoints = []
        
        # ê¸°ë³¸ì ì¸ ì¸ì²´ í‚¤í¬ì¸íŠ¸ íŒ¨í„´
        base_points = [
            (0.5, 0.1),    # ì½”
            (0.5, 0.15),   # ëª©
            (0.35, 0.25),  # ì˜¤ë¥¸ìª½ ì–´ê¹¨
            (0.3, 0.35),   # ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜
            (0.25, 0.45),  # ì˜¤ë¥¸ìª½ ì†ëª©
            (0.65, 0.25),  # ì™¼ìª½ ì–´ê¹¨
            (0.7, 0.35),   # ì™¼ìª½ íŒ”ê¿ˆì¹˜
            (0.75, 0.45),  # ì™¼ìª½ ì†ëª©
            (0.5, 0.5),    # ì¤‘ê°„ ì—‰ë©ì´
            (0.4, 0.5),    # ì˜¤ë¥¸ìª½ ì—‰ë©ì´
            (0.35, 0.65),  # ì˜¤ë¥¸ìª½ ë¬´ë¦
            (0.3, 0.8),    # ì˜¤ë¥¸ìª½ ë°œëª©
            (0.6, 0.5),    # ì™¼ìª½ ì—‰ë©ì´
            (0.65, 0.65),  # ì™¼ìª½ ë¬´ë¦
            (0.7, 0.8),    # ì™¼ìª½ ë°œëª©
            (0.48, 0.08),  # ì˜¤ë¥¸ìª½ ëˆˆ
            (0.52, 0.08),  # ì™¼ìª½ ëˆˆ
            (0.46, 0.09),  # ì˜¤ë¥¸ìª½ ê·€
            (0.54, 0.09)   # ì™¼ìª½ ê·€
        ]
        
        for x_ratio, y_ratio in base_points:
            x = x_ratio * width
            y = y_ratio * height
            confidence = 0.7
            keypoints.append([x, y, confidence])
        
        return {
            'keypoints': keypoints,
            'success': True,
            'model_type': 'dummy'
        }

class YOLOv8AIPoseModel:
    """YOLOv8 AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ëª¨ë¸"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.logger = logging.getLogger(f"{__name__}.YOLOv8AIPoseModel")
        
        # YOLOv8 ëª¨ë¸
        self.yolo_model = None
        self._init_yolo()
    
    def _init_yolo(self):
        """YOLOv8 í¬ì¦ˆ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if ULTRALYTICS_AVAILABLE:
                self.yolo_model = YOLO('yolov8n-pose.pt')
                self.logger.info("âœ… YOLOv8 í¬ì¦ˆ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ Ultralytics ì—†ìŒ - ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©")
                self.yolo_model = self._create_simple_yolo()
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.yolo_model = self._create_simple_yolo()
    
    def _create_simple_yolo(self) -> nn.Module:
        """ê°„ë‹¨í•œ YOLO ìŠ¤íƒ€ì¼ í¬ì¦ˆ ëª¨ë¸"""
        class SimpleYOLOPose(nn.Module):
            def __init__(self):
                super().__init__()
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.MaxPool2d(3, 2, 1),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),
                    nn.AdaptiveAvgPool2d((1, 1))
                )
                self.pose_head = nn.Linear(256, 17 * 3)  # COCO 17 keypoints
            
            def forward(self, x):
                features = self.backbone(x)
                features = features.flatten(1)
                keypoints = self.pose_head(features)
                return keypoints.view(-1, 17, 3)
        
        model = SimpleYOLOPose()
        model.to(self.device)
        model.eval()
        return model
    
    def predict(self, image: Union[Image.Image, np.ndarray]) -> Dict[str, Any]:
        """YOLOv8 í¬ì¦ˆ ì˜ˆì¸¡"""
        try:
            if hasattr(self.yolo_model, 'predict') and ULTRALYTICS_AVAILABLE:
                # ì‹¤ì œ YOLOv8 ì‚¬ìš©
                results = self.yolo_model.predict(image)
                
                if results and len(results) > 0:
                    result = results[0]
                    if hasattr(result, 'keypoints') and result.keypoints is not None:
                        keypoints_data = result.keypoints.data
                        if len(keypoints_data) > 0:
                            kps = keypoints_data[0]  # ì²« ë²ˆì§¸ ì‚¬ëŒ
                            keypoints = []
                            for kp in kps:
                                x, y, conf = float(kp[0]), float(kp[1]), float(kp[2])
                                keypoints.append([x, y, conf])
                            
                            return {
                                'keypoints': keypoints,
                                'success': True,
                                'model_type': 'yolov8'
                            }
            
            elif isinstance(self.yolo_model, nn.Module):
                # ê°„ë‹¨í•œ ëª¨ë¸ ì‚¬ìš©
                if isinstance(image, Image.Image):
                    image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
                else:
                    image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(self.device) / 255.0
                
                with torch.no_grad():
                    output = self.yolo_model(image_tensor)
                    keypoints = output[0].cpu().numpy()
                    
                    keypoints_list = []
                    for kp in keypoints:
                        keypoints_list.append([float(kp[0]), float(kp[1]), float(kp[2])])
                    
                    return {
                        'keypoints': keypoints_list,
                        'success': True,
                        'model_type': 'simple_yolo'
                    }
            
            # í´ë°±
            return self._generate_dummy_coco_keypoints(
                image.size if isinstance(image, Image.Image) else image.shape[:2]
            )
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {'keypoints': [], 'success': False, 'model_type': 'yolov8'}
    
    def _generate_dummy_coco_keypoints(self, image_shape: Union[Tuple[int, int], Tuple[int, int]]) -> Dict[str, Any]:
        """ë”ë¯¸ COCO 17 í‚¤í¬ì¸íŠ¸ ìƒì„±"""
        if len(image_shape) == 2:
            height, width = image_shape
        else:
            width, height = image_shape
        
        # COCO 17 í¬ë§· ë”ë¯¸ í‚¤í¬ì¸íŠ¸
        base_points = [
            (0.5, 0.1),    # nose
            (0.48, 0.08),  # left_eye
            (0.52, 0.08),  # right_eye
            (0.46, 0.09),  # left_ear
            (0.54, 0.09),  # right_ear
            (0.35, 0.25),  # left_shoulder
            (0.65, 0.25),  # right_shoulder
            (0.3, 0.35),   # left_elbow
            (0.7, 0.35),   # right_elbow
            (0.25, 0.45),  # left_wrist
            (0.75, 0.45),  # right_wrist
            (0.4, 0.5),    # left_hip
            (0.6, 0.5),    # right_hip
            (0.35, 0.65),  # left_knee
            (0.65, 0.65),  # right_knee
            (0.3, 0.8),    # left_ankle
            (0.7, 0.8)     # right_ankle
        ]
        
        keypoints = []
        for x_ratio, y_ratio in base_points:
            x = x_ratio * width
            y = y_ratio * height
            confidence = 0.8
            keypoints.append([x, y, confidence])
        
        return {
            'keypoints': keypoints,
            'success': True,
            'model_type': 'dummy_coco'
        }

# ==============================================
# ğŸ”¥ ë™ì  import í•¨ìˆ˜ë“¤ (TYPE_CHECKING í˜¸í™˜)
# ==============================================

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (TYPE_CHECKING í˜¸í™˜)"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logging.error(f"âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_model_loader():
    """ModelLoaderë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° (TYPE_CHECKING í˜¸í™˜)"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.model_loader')
        get_global_loader = getattr(module, 'get_global_model_loader', None)
        if get_global_loader:
            return get_global_loader()
        else:
            ModelLoader = getattr(module, 'ModelLoader', None)
            if ModelLoader:
                return ModelLoader()
        return None
    except ImportError as e:
        logger.error(f"âŒ ModelLoader ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_memory_manager():
    """MemoryManagerë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° (TYPE_CHECKING í˜¸í™˜)"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.memory_manager')
        get_global_manager = getattr(module, 'get_global_memory_manager', None)
        if get_global_manager:
            return get_global_manager()
        return None
    except ImportError as e:
        logger.debug(f"MemoryManager ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_step_factory():
    """StepFactoryë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° (TYPE_CHECKING í˜¸í™˜)"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.factories.step_factory')
        get_global_factory = getattr(module, 'get_global_step_factory', None)
        if get_global_factory:
            return get_global_factory()
        return None
    except ImportError as e:
        logger.debug(f"StepFactory ë™ì  import ì‹¤íŒ¨: {e}")
        return None

# ==============================================
# ğŸ”¥ BaseStepMixin ë™ì  ë¡œë”© (TYPE_CHECKING í˜¸í™˜)
# ==============================================

BaseStepMixin = get_base_step_mixin_class()

if BaseStepMixin is None:
    # í´ë°± í´ë˜ìŠ¤ ì •ì˜ (BaseStepMixin v16.0 í˜¸í™˜)
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'BaseStep')
            self.step_id = kwargs.get('step_id', 0)
            self.device = kwargs.get('device', 'cpu')
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # BaseStepMixin v16.0 í˜¸í™˜ ì†ì„±ë“¤
            self.config = type('StepConfig', (), kwargs)()
            self.dependency_manager = type('DependencyManager', (), {
                'dependency_status': type('DependencyStatus', (), {
                    'model_loader': False,
                    'step_interface': False,
                    'memory_manager': False,
                    'data_converter': False,
                    'di_container': False
                })(),
                'auto_inject_dependencies': lambda: False
            })()
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            self.performance_metrics = {
                'process_count': 0,
                'total_process_time': 0.0,
                'average_process_time': 0.0,
                'error_count': 0,
                'success_count': 0,
                'cache_hits': 0
            }
        
        async def initialize(self):
            self.is_initialized = True
            return True
        
        def set_model_loader(self, model_loader):
            self.model_loader = model_loader
            self.dependency_manager.dependency_status.model_loader = True
        
        def set_memory_manager(self, memory_manager):
            self.memory_manager = memory_manager
            self.dependency_manager.dependency_status.memory_manager = True
        
        def set_data_converter(self, data_converter):
            self.data_converter = data_converter
            self.dependency_manager.dependency_status.data_converter = True
        
        def set_di_container(self, di_container):
            self.di_container = di_container
            self.dependency_manager.dependency_status.di_container = True
        
        async def cleanup(self):
            pass
        
        def get_status(self):
            return {
                'step_name': self.step_name,
                'is_initialized': self.is_initialized,
                'device': self.device,
                'dependencies': {
                    'model_loader': self.dependency_manager.dependency_status.model_loader,
                    'step_interface': self.dependency_manager.dependency_status.step_interface,
                    'memory_manager': self.dependency_manager.dependency_status.memory_manager,
                    'data_converter': self.dependency_manager.dependency_status.data_converter,
                    'di_container': self.dependency_manager.dependency_status.di_container,
                },
                'version': '16.0-compatible'
            }
        
        def optimize_memory(self, aggressive: bool = False):
            """ë©”ëª¨ë¦¬ ìµœì í™”"""
            try:
                if TORCH_AVAILABLE:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        try:
                            if hasattr(torch.mps, 'empty_cache'):
                                torch.mps.empty_cache()
                        except:
                            pass
                gc.collect()
                return {"success": True, "method": "basic_cleanup"}
            except Exception as e:
                return {"success": False, "error": str(e)}
        
        async def optimize_memory_async(self, aggressive: bool = False):
            """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìµœì í™”"""
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, self.optimize_memory, aggressive)

# ==============================================
# ğŸ”¥ í¬ì¦ˆ ì¶”ì • ë°ì´í„° êµ¬ì¡° ë° ìƒìˆ˜
# ==============================================

class PoseModel(Enum):
    """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ íƒ€ì…"""
    MEDIAPIPE = "pose_estimation_mediapipe"
    YOLOV8_POSE = "pose_estimation_yolov8" 
    LIGHTWEIGHT = "pose_estimation_lightweight"

class PoseQuality(Enum):
    """í¬ì¦ˆ í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"     # 90-100ì 
    GOOD = "good"              # 75-89ì   
    ACCEPTABLE = "acceptable"   # 60-74ì 
    POOR = "poor"              # 40-59ì 
    VERY_POOR = "very_poor"    # 0-39ì 

class PoseType(Enum):
    """í¬ì¦ˆ íƒ€ì…"""
    T_POSE = "t_pose"          # Tì í¬ì¦ˆ
    A_POSE = "a_pose"          # Aì í¬ì¦ˆ
    STANDING = "standing"      # ì¼ë°˜ ì„œìˆëŠ” í¬ì¦ˆ
    SITTING = "sitting"        # ì•‰ì€ í¬ì¦ˆ
    ACTION = "action"          # ì•¡ì…˜ í¬ì¦ˆ
    UNKNOWN = "unknown"        # ì•Œ ìˆ˜ ì—†ëŠ” í¬ì¦ˆ

# OpenPose 18 í‚¤í¬ì¸íŠ¸ ì •ì˜ (í˜¸í™˜ì„± ìœ ì§€)
OPENPOSE_18_KEYPOINTS = [
    "nose", "neck", "right_shoulder", "right_elbow", "right_wrist",
    "left_shoulder", "left_elbow", "left_wrist", "middle_hip", "right_hip", 
    "right_knee", "right_ankle", "left_hip", "left_knee", "left_ankle",
    "right_eye", "left_eye", "right_ear", "left_ear"
]

# í‚¤í¬ì¸íŠ¸ ìƒ‰ìƒ ë° ì—°ê²° ì •ë³´
KEYPOINT_COLORS = [
    (255, 0, 0), (255, 85, 0), (255, 170, 0), (255, 255, 0), (170, 255, 0),
    (85, 255, 0), (0, 255, 0), (0, 255, 85), (0, 255, 170), (0, 255, 255),
    (0, 170, 255), (0, 85, 255), (0, 0, 255), (85, 0, 255), (170, 0, 255),
    (255, 0, 255), (255, 0, 170), (255, 0, 85), (255, 0, 0)
]

SKELETON_CONNECTIONS = [
    (0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7), (1, 8),
    (8, 9), (9, 10), (10, 11), (8, 12), (12, 13), (13, 14), (0, 15),
    (15, 17), (0, 16), (16, 18)
]

# ==============================================
# ğŸ”¥ í¬ì¦ˆ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤
# ==============================================

@dataclass
class PoseMetrics:
    """ì™„ì „í•œ í¬ì¦ˆ ì¸¡ì • ë°ì´í„°"""
    keypoints: List[List[float]] = field(default_factory=list)
    confidence_scores: List[float] = field(default_factory=list)
    bbox: Tuple[int, int, int, int] = field(default_factory=lambda: (0, 0, 0, 0))
    pose_type: PoseType = PoseType.UNKNOWN
    pose_quality: PoseQuality = PoseQuality.POOR
    overall_score: float = 0.0
    
    # ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜
    head_score: float = 0.0
    torso_score: float = 0.0  
    arms_score: float = 0.0
    legs_score: float = 0.0
    
    # ê³ ê¸‰ ë¶„ì„ ì ìˆ˜
    symmetry_score: float = 0.0
    visibility_score: float = 0.0
    pose_angles: Dict[str, float] = field(default_factory=dict)
    body_proportions: Dict[str, float] = field(default_factory=dict)
    
    # ì˜ë¥˜ ì°©ìš© ì í•©ì„±
    suitable_for_fitting: bool = False
    issues: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    
    # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„°
    model_used: str = ""
    processing_time: float = 0.0
    image_resolution: Tuple[int, int] = field(default_factory=lambda: (0, 0))
    ai_confidence: float = 0.0
    
    def calculate_overall_score(self) -> float:
        """ì „ì²´ ì ìˆ˜ ê³„ì‚°"""
        try:
            if not self.confidence_scores:
                self.overall_score = 0.0
                return 0.0
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚° (AI ì‹ ë¢°ë„ ë°˜ì˜)
            base_scores = [
                self.head_score * 0.15,
                self.torso_score * 0.35,
                self.arms_score * 0.25,
                self.legs_score * 0.25
            ]
            
            advanced_scores = [
                self.symmetry_score * 0.3,
                self.visibility_score * 0.7
            ]
            
            base_score = sum(base_scores)
            advanced_score = sum(advanced_scores)
            
            # AI ì‹ ë¢°ë„ë¡œ ê°€ì¤‘
            self.overall_score = (base_score * 0.7 + advanced_score * 0.3) * self.ai_confidence
            return self.overall_score
            
        except Exception as e:
            logger.error(f"ì „ì²´ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            self.overall_score = 0.0
            return 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)

# ==============================================
# ğŸ”¥ ë©”ì¸ PoseEstimationStep í´ë˜ìŠ¤ (BaseStepMixin v16.0 í˜¸í™˜ + AI ì™„ì „ ëŒ€ì²´)
# ==============================================

class PoseEstimationStep(BaseStepMixin):
    """
    ğŸ”¥ Step 02: AI ëª¨ë¸ ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ - BaseStepMixin v16.0 í˜¸í™˜ (OpenCV ì™„ì „ ëŒ€ì²´)
    
    âœ… BaseStepMixin v16.0 UnifiedDependencyManager ì™„ì „ í˜¸í™˜
    âœ… OpenCV ì™„ì „ ì œê±° â†’ AI ëª¨ë¸ ê¸°ë°˜ ì²˜ë¦¬
    âœ… MediaPipe, YOLOv8, SAM, U2Net AI í™œìš©
    âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
    âœ… StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… ì™„ì„±
    âœ… M3 Max ìµœì í™” + Strict Mode
    """
    
    # ì˜ë¥˜ íƒ€ì…ë³„ í¬ì¦ˆ ê°€ì¤‘ì¹˜
    CLOTHING_POSE_WEIGHTS = {
        'shirt': {'arms': 0.4, 'torso': 0.4, 'visibility': 0.2},
        'dress': {'torso': 0.5, 'arms': 0.3, 'legs': 0.2},
        'pants': {'legs': 0.6, 'torso': 0.3, 'visibility': 0.1},
        'jacket': {'arms': 0.5, 'torso': 0.4, 'visibility': 0.1},
        'skirt': {'torso': 0.4, 'legs': 0.4, 'visibility': 0.2},
        'top': {'torso': 0.5, 'arms': 0.4, 'visibility': 0.1},
        'default': {'torso': 0.4, 'arms': 0.3, 'legs': 0.2, 'visibility': 0.1}
    }
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        strict_mode: bool = True,
        **kwargs
    ):
        """
        BaseStepMixin v16.0 í˜¸í™˜ ìƒì„±ì (AI ëª¨ë¸ ê¸°ë°˜)
        
        Args:
            device: ë””ë°”ì´ìŠ¤ ì„¤ì • ('auto', 'mps', 'cuda', 'cpu')
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            strict_mode: ì—„ê²© ëª¨ë“œ (Trueì‹œ AI ì‹¤íŒ¨ â†’ ì¦‰ì‹œ ì—ëŸ¬)
            **kwargs: ì¶”ê°€ ì„¤ì •
        """
        
        # ğŸ”¥ BaseStepMixin v16.0 í˜¸í™˜ ì„¤ì •
        kwargs.setdefault('step_name', 'PoseEstimationStep')
        kwargs.setdefault('step_id', 2)
        kwargs.setdefault('device', device or 'auto')
        kwargs.setdefault('strict_mode', strict_mode)
        
        # PoseEstimationStep íŠ¹í™” ì†ì„±ë“¤ (BaseStepMixin ì´ˆê¸°í™” ì „ì— ì„¤ì •)
        self.step_name = "PoseEstimationStep"
        self.step_number = 2
        self.step_description = "AI ëª¨ë¸ ê¸°ë°˜ ì¸ì²´ í¬ì¦ˆ ì¶”ì • ë° í‚¤í¬ì¸íŠ¸ ê²€ì¶œ (OpenCV ì™„ì „ ëŒ€ì²´)"
        self.strict_mode = strict_mode
        self.num_keypoints = kwargs.get('num_keypoints', 18)
        self.keypoint_names = OPENPOSE_18_KEYPOINTS.copy()
        
        # ğŸ”¥ BaseStepMixin v16.0 ì´ˆê¸°í™” (UnifiedDependencyManager í¬í•¨)
        try:
            super(PoseEstimationStep, self).__init__(**kwargs)
            self.logger.info(f"ğŸ¤¸ BaseStepMixin v16.0 í˜¸í™˜ ì´ˆê¸°í™” ì™„ë£Œ - AI ëª¨ë¸ ê¸°ë°˜ ({self.num_keypoints}ê°œ í‚¤í¬ì¸íŠ¸)")
        except Exception as e:
            self.logger.error(f"âŒ BaseStepMixin v16.0 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if strict_mode:
                raise RuntimeError(f"Strict Mode: BaseStepMixin v16.0 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # í´ë°±ìœ¼ë¡œ ê¸°ë³¸ ì´ˆê¸°í™”
            self._fallback_initialization(**kwargs)
        
        # ğŸ”¥ ì‹œìŠ¤í…œ ì„¤ì • ì´ˆê¸°í™”
        self._setup_system_config(config=config, **kwargs)
        
        # ğŸ”¥ AI ëª¨ë¸ ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._initialize_ai_pose_system()
        
        # AI ëª¨ë¸ ê´€ë ¨ ì†ì„±ë“¤
        self.ai_models: Dict[str, Any] = {}
        self.active_model = None
        self.target_input_size = (512, 512)
        self.output_format = "keypoints_ai"
        
        # AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ê¸°ë“¤
        self.image_processor = AIImageProcessor(self.device)
        self.segmentation_processor = AISegmentationProcessor(self.device)
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.prediction_cache: Dict[str, Any] = {}
        self.cache_max_size = 100 if self._is_m3_max() else 50
        
        self.logger.info(f"ğŸ¯ {self.step_name} AI ëª¨ë¸ ê¸°ë°˜ BaseStepMixin v16.0 í˜¸í™˜ ìƒì„± ì™„ë£Œ (Strict Mode: {self.strict_mode})")
    
    def _fallback_initialization(self, **kwargs):
        """í´ë°± ì´ˆê¸°í™” (BaseStepMixin v16.0 ì—†ì´)"""
        try:
            # ê¸°ë³¸ BaseStepMixin ì†ì„±ë“¤ ìˆ˜ë™ ì„¤ì •
            self.device = kwargs.get('device', 'cpu')
            self.config = type('StepConfig', (), kwargs)()
            self.is_m3_max = self._is_m3_max()
            self.memory_gb = self._get_memory_info()
            
            # BaseStepMixin í•„ìˆ˜ ì†ì„±ë“¤
            self.step_id = kwargs.get('step_id', 2)
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # v16.0 í˜¸í™˜ ì˜ì¡´ì„± ê´€ë¦¬
            self.dependency_manager = type('DependencyManager', (), {
                'dependency_status': type('DependencyStatus', (), {
                    'model_loader': False,
                    'step_interface': False,
                    'memory_manager': False,
                    'data_converter': False,
                    'di_container': False
                })(),
                'auto_inject_dependencies': lambda: self._manual_auto_inject()
            })()
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            self.performance_metrics = {
                'process_count': 0,
                'total_process_time': 0.0,
                'average_process_time': 0.0,
                'error_count': 0,
                'success_count': 0,
                'cache_hits': 0
            }
            
            # ì˜ì¡´ì„± ê´€ë ¨ ì†ì„±ë“¤
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            self.logger.info("âœ… BaseStepMixin v16.0 í˜¸í™˜ í´ë°± ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ì†ì„± ì„¤ì •
            self.device = "cpu"
            self.config = type('Config', (), {})()
            self.dependency_manager = type('Manager', (), {'auto_inject_dependencies': lambda: False})()
    
    def _manual_auto_inject(self) -> bool:
        """ìˆ˜ë™ ìë™ ì˜ì¡´ì„± ì£¼ì… (v16.0 í˜¸í™˜)"""
        try:
            injection_count = 0
            
            # ModelLoader ìë™ ì£¼ì…
            model_loader = get_model_loader()
            if model_loader:
                self.set_model_loader(model_loader)
                injection_count += 1
                self.logger.debug("âœ… ModelLoader ìˆ˜ë™ ìë™ ì£¼ì… ì™„ë£Œ")
            
            # MemoryManager ìë™ ì£¼ì…
            memory_manager = get_memory_manager()
            if memory_manager:
                self.set_memory_manager(memory_manager)
                injection_count += 1
                self.logger.debug("âœ… MemoryManager ìˆ˜ë™ ìë™ ì£¼ì… ì™„ë£Œ")
            
            if injection_count > 0:
                self.logger.info(f"ğŸ‰ ìˆ˜ë™ ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {injection_count}ê°œ")
                return True
                
            return False
        except Exception as e:
            self.logger.debug(f"ìˆ˜ë™ ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    def _is_m3_max(self) -> bool:
        """M3 Max ê°ì§€"""
        try:
            import platform
            import subprocess
            if platform.system() == 'Darwin':
                result = subprocess.run(
                    ['sysctl', '-n', 'machdep.cpu.brand_string'],
                    capture_output=True, text=True, timeout=5
                )
                return 'M3' in result.stdout
        except:
            pass
        return False
    
    def _get_memory_info(self) -> float:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
        try:
            if PSUTIL_AVAILABLE:
                memory = psutil.virtual_memory()
                return memory.total / (1024**3)
            return 16.0
        except:
            return 16.0
    
    def _setup_system_config(self, config: Optional[Dict[str, Any]], **kwargs):
        """ì‹œìŠ¤í…œ ì„¤ì • ì´ˆê¸°í™”"""
        try:
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            device = kwargs.get('device')
            if device is None or device == "auto":
                self.device = self._detect_optimal_device()
            else:
                self.device = device
                
            self.is_m3_max = self._is_m3_max()
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            self.memory_gb = self._get_memory_info()
            
            # ì„¤ì • í†µí•©
            if config is None:
                config = {}
            config.update(kwargs)
            
            # ê¸°ë³¸ ì„¤ì • ì ìš©
            default_config = {
                'confidence_threshold': 0.5,
                'visualization_enabled': True,
                'return_analysis': True,
                'cache_enabled': True,
                'detailed_analysis': True,
                'strict_mode': self.strict_mode,
                'ai_models_only': True,
                'opencv_disabled': True
            }
            
            for key, default_value in default_config.items():
                if key not in config:
                    config[key] = default_value
            
            # config ê°ì²´ ì„¤ì • (BaseStepMixin v16.0 í˜¸í™˜)
            if hasattr(self, 'config') and hasattr(self.config, '__dict__'):
                self.config.__dict__.update(config)
            else:
                self.config = type('StepConfig', (), config)()
            
            self.logger.info(f"ğŸ”§ AI ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ: {self.device}, M3 Max: {self.is_m3_max}, ë©”ëª¨ë¦¬: {self.memory_gb:.1f}GB")
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ ì„¤ì • ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise RuntimeError(f"Strict Mode: ì‹œìŠ¤í…œ ì„¤ì • ì‹¤íŒ¨: {e}")
            
            # ì•ˆì „í•œ í´ë°± ì„¤ì •
            self.device = "cpu"
            self.is_m3_max = False
            self.memory_gb = 16.0
            self.config = type('Config', (), {
                'confidence_threshold': 0.5,
                'ai_models_only': True
            })()
    
    def _detect_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        try:
            if TORCH_AVAILABLE:
                if torch.backends.mps.is_available():
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
            return "cpu"
        except:
            return "cpu"
    
    def _initialize_ai_pose_system(self):
        """AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # AI í¬ì¦ˆ ì‹œìŠ¤í…œ ì„¤ì •
            self.ai_pose_config = {
                'model_priority': [
                    'pose_estimation_mediapipe', 
                    'pose_estimation_yolov8', 
                    'pose_estimation_lightweight'
                ],
                'confidence_threshold': getattr(self.config, 'confidence_threshold', 0.5),
                'visualization_enabled': getattr(self.config, 'visualization_enabled', True),
                'return_analysis': getattr(self.config, 'return_analysis', True),
                'cache_enabled': getattr(self.config, 'cache_enabled', True),
                'detailed_analysis': getattr(self.config, 'detailed_analysis', True),
                'ai_models_only': True,
                'opencv_disabled': True
            }
            
            # AI ëª¨ë¸ ìµœì í™” ë ˆë²¨ ì„¤ì •
            if self.is_m3_max:
                self.optimization_level = 'maximum'
                self.batch_processing = True
                self.use_neural_engine = True
            elif self.memory_gb >= 32:
                self.optimization_level = 'high'
                self.batch_processing = True
                self.use_neural_engine = False
            else:
                self.optimization_level = 'basic'
                self.batch_processing = False
                self.use_neural_engine = False
            
            self.logger.info(f"ğŸ¯ AI í¬ì¦ˆ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ - ìµœì í™”: {self.optimization_level}")
            
        except Exception as e:
            self.logger.error(f"âŒ AI í¬ì¦ˆ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise RuntimeError(f"Strict Mode: AI í¬ì¦ˆ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # ìµœì†Œí•œì˜ ì„¤ì •
            self.ai_pose_config = {'confidence_threshold': 0.5, 'ai_models_only': True}
            self.optimization_level = 'basic'
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin v16.0 í˜¸í™˜ ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì„¤ì • (v16.0 í˜¸í™˜)"""
        try:
            self.model_loader = model_loader
            self.model_interface = model_loader
            self.has_model = True
            self.model_loaded = True
            
            # v16.0 dependency_manager í˜¸í™˜
            if hasattr(self, 'dependency_manager') and hasattr(self.dependency_manager, 'dependency_status'):
                self.dependency_manager.dependency_status.model_loader = True
                self.dependency_manager.dependency_status.step_interface = True
            
            self.logger.info("âœ… ModelLoader ì„¤ì • ì™„ë£Œ (v16.0 í˜¸í™˜)")
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def set_memory_manager(self, memory_manager):
        """MemoryManager ì„¤ì • (v16.0 í˜¸í™˜)"""
        try:
            self.memory_manager = memory_manager
            
            # v16.0 dependency_manager í˜¸í™˜
            if hasattr(self, 'dependency_manager') and hasattr(self.dependency_manager, 'dependency_status'):
                self.dependency_manager.dependency_status.memory_manager = True
            
            self.logger.debug("âœ… MemoryManager ì„¤ì • ì™„ë£Œ (v16.0 í˜¸í™˜)")
        except Exception as e:
            self.logger.error(f"âŒ MemoryManager ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def set_data_converter(self, data_converter):
        """DataConverter ì„¤ì • (v16.0 í˜¸í™˜)"""
        try:
            self.data_converter = data_converter
            
            # v16.0 dependency_manager í˜¸í™˜
            if hasattr(self, 'dependency_manager') and hasattr(self.dependency_manager, 'dependency_status'):
                self.dependency_manager.dependency_status.data_converter = True
            
            self.logger.debug("âœ… DataConverter ì„¤ì • ì™„ë£Œ (v16.0 í˜¸í™˜)")
        except Exception as e:
            self.logger.error(f"âŒ DataConverter ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def set_di_container(self, di_container):
        """DIContainer ì„¤ì • (v16.0 í˜¸í™˜)"""
        try:
            self.di_container = di_container
            
            # v16.0 dependency_manager í˜¸í™˜
            if hasattr(self, 'dependency_manager') and hasattr(self.dependency_manager, 'dependency_status'):
                self.dependency_manager.dependency_status.di_container = True
            
            self.logger.debug("âœ… DIContainer ì„¤ì • ì™„ë£Œ (v16.0 í˜¸í™˜)")
        except Exception as e:
            self.logger.error(f"âŒ DIContainer ì„¤ì • ì‹¤íŒ¨: {e}")
    
    # ==============================================
    # ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ - AI ëª¨ë¸ ê¸°ë°˜ ì—°ì‚° êµ¬í˜„
    # ==============================================
    
    async def process(
        self, 
        image: Union[np.ndarray, Image.Image, str],
        clothing_type: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        AI ëª¨ë¸ ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬ - BaseStepMixin v16.0 í˜¸í™˜ (OpenCV ì™„ì „ ëŒ€ì²´)
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
            clothing_type: ì˜ë¥˜ íƒ€ì… (ì„ íƒì )
            **kwargs: ì¶”ê°€ ì„¤ì •
            
        Returns:
            Dict[str, Any]: ì™„ì „í•œ AI í¬ì¦ˆ ì¶”ì • ê²°ê³¼
        """
        try:
            # ì´ˆê¸°í™” ê²€ì¦
            if not self.is_initialized:
                if not await self.initialize():
                    error_msg = "AI ì´ˆê¸°í™” ì‹¤íŒ¨"
                    if self.strict_mode:
                        raise RuntimeError(f"Strict Mode: {error_msg}")
                    return self._create_error_result(error_msg)
            
            start_time = time.time()
            self.logger.info(f"ğŸ§  {self.step_name} AI ëª¨ë¸ ê¸°ë°˜ ì²˜ë¦¬ ì‹œì‘ (OpenCV ì™„ì „ ëŒ€ì²´)")
            
            # AI ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            processed_image = self._preprocess_image_with_ai(image)
            if processed_image is None:
                error_msg = "AI ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨"
                if self.strict_mode:
                    raise ValueError(f"Strict Mode: {error_msg}")
                return self._create_error_result(error_msg)
            
            # ìºì‹œ í™•ì¸
            cache_key = None
            if self.ai_pose_config['cache_enabled']:
                cache_key = self._generate_cache_key(processed_image, clothing_type)
                if cache_key in self.prediction_cache:
                    self.logger.info("ğŸ“‹ ìºì‹œì—ì„œ AI ê²°ê³¼ ë°˜í™˜")
                    self.performance_metrics['cache_hits'] += 1
                    return self.prediction_cache[cache_key]
            
            # AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
            pose_result = await self._process_with_ai_models(processed_image, clothing_type, **kwargs)
            
            if not pose_result or not pose_result.get('success', False):
                error_msg = f"AI í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {pose_result.get('error', 'Unknown AI Error') if pose_result else 'No Result'}"
                self.logger.error(f"âŒ {error_msg}")
                self.performance_metrics['error_count'] += 1
                if self.strict_mode:
                    raise RuntimeError(f"Strict Mode: {error_msg}")
                return self._create_error_result(error_msg)
            
            # ì™„ì „í•œ ê²°ê³¼ í›„ì²˜ë¦¬
            final_result = self._postprocess_ai_result(pose_result, processed_image, start_time)
            
            # ìºì‹œ ì €ì¥
            if self.ai_pose_config['cache_enabled'] and cache_key:
                self._save_to_cache(cache_key, final_result)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self.performance_metrics['process_count'] += 1
            self.performance_metrics['success_count'] += 1
            self.performance_metrics['total_process_time'] += processing_time
            self.performance_metrics['average_process_time'] = (
                self.performance_metrics['total_process_time'] / self.performance_metrics['process_count']
            )
            
            self.logger.info(f"âœ… {self.step_name} AI ëª¨ë¸ ê¸°ë°˜ ì²˜ë¦¬ ì„±ê³µ ({processing_time:.2f}ì´ˆ)")
            self.logger.info(f"ğŸ¯ AI í‚¤í¬ì¸íŠ¸ ìˆ˜: {len(final_result.get('keypoints', []))}")
            self.logger.info(f"ğŸ–ï¸ AI ì‹ ë¢°ë„: {final_result.get('pose_analysis', {}).get('ai_confidence', 0):.3f}")
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} AI ëª¨ë¸ ê¸°ë°˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìŠ¤íƒ: {traceback.format_exc()}")
            self.performance_metrics['error_count'] += 1
            if self.strict_mode:
                raise
            return self._create_error_result(str(e))
    
    async def _process_with_ai_models(
        self, 
        image: Image.Image, 
        clothing_type: Optional[str] = None,
        warmup: bool = False,
        **kwargs
    ) -> Dict[str, Any]:
        """AI ëª¨ë¸ì„ í†µí•œ í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬ - ì‹¤ì œ AI ì—°ì‚°"""
        try:
            inference_start = time.time()
            self.logger.info(f"ğŸ§  AI ëª¨ë¸ ì¶”ë¡  ì‹œì‘...")
            
            # AI ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ë° ìƒì„±
            ai_model = None
            model_name = None
            
            if hasattr(self, 'model_loader') and self.model_loader:
                # ìš°ì„ ìˆœìœ„ëŒ€ë¡œ AI ëª¨ë¸ ì‹œë„
                for priority_model in self.ai_pose_config['model_priority']:
                    try:
                        if hasattr(self.model_loader, 'get_model'):
                            model_data = self.model_loader.get_model(priority_model)
                        elif hasattr(self.model_loader, 'load_model'):
                            model_data = self.model_loader.load_model(priority_model)
                        else:
                            continue
                        
                        if model_data:
                            ai_model = await self._convert_data_to_ai_model(model_data, priority_model)
                            if ai_model:
                                model_name = priority_model
                                self.active_model = model_name
                                break
                    except Exception as e:
                        self.logger.debug(f"AI ëª¨ë¸ {priority_model} ë¡œë”© ì‹¤íŒ¨: {e}")
                        continue
            
            # AI ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ AI ëª¨ë¸ ìƒì„±
            if ai_model is None:
                self.logger.info("ğŸ”§ ê¸°ë³¸ AI ëª¨ë¸ ìƒì„±...")
                ai_model, model_name = self._create_default_ai_model()
                self.active_model = model_name
            
            # ì›Œë°ì—… ëª¨ë“œ ì²˜ë¦¬
            if warmup:
                return {"success": True, "warmup": True, "model_used": model_name}
            
            # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
            try:
                if isinstance(ai_model, MediaPipeAIPoseModel):
                    ai_result = ai_model.predict(image)
                elif isinstance(ai_model, YOLOv8AIPoseModel):
                    ai_result = ai_model.predict(image)
                else:
                    # ì¼ë°˜ PyTorch ëª¨ë¸
                    ai_result = await self._run_pytorch_model(ai_model, image)
                
                inference_time = time.time() - inference_start
                self.logger.info(f"âœ… AI ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ ({inference_time:.3f}ì´ˆ)")
                
                if not ai_result.get('success', False):
                    raise ValueError(f"AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {ai_result}")
                
            except Exception as e:
                error_msg = f"AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}"
                self.logger.error(f"âŒ {error_msg}")
                if self.strict_mode:
                    raise RuntimeError(f"Strict Mode: {error_msg}")
                return {'success': False, 'error': error_msg}
            
            # AI ê²°ê³¼ ì •ë¦¬
            pose_result = {
                'keypoints': ai_result.get('keypoints', []),
                'success': ai_result.get('success', False),
                'model_used': model_name,
                'model_type': ai_result.get('model_type', 'unknown'),
                'inference_time': inference_time,
                'ai_based': True
            }
            
            self.logger.info(f"âœ… {model_name} AI ì¶”ë¡  ì™„ì „ ì„±ê³µ")
            return pose_result
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise
            return {'success': False, 'error': str(e)}
    
    async def _convert_data_to_ai_model(self, model_data: Any, model_name: str) -> Optional[Any]:
        """ëª¨ë¸ ë°ì´í„°ë¥¼ AI ëª¨ë¸ë¡œ ë³€í™˜"""
        try:
            self.logger.info(f"ğŸ”„ {model_name} ë°ì´í„° â†’ AI ëª¨ë¸ ë³€í™˜ ì‹œì‘")
            
            if 'mediapipe' in model_name.lower():
                return MediaPipeAIPoseModel(self.device)
            elif 'yolov8' in model_name.lower():
                return YOLOv8AIPoseModel(self.device)
            else:
                # ê¸°ë³¸ AI ëª¨ë¸
                return MediaPipeAIPoseModel(self.device)
                
        except Exception as e:
            self.logger.error(f"âŒ {model_name} ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_default_ai_model(self) -> Tuple[Any, str]:
        """ê¸°ë³¸ AI ëª¨ë¸ ìƒì„±"""
        try:
            self.logger.info("ğŸ”§ ê¸°ë³¸ AI ëª¨ë¸ ìƒì„±")
            
            # MediaPipe ìš°ì„  ì‹œë„
            if MEDIAPIPE_AVAILABLE:
                model = MediaPipeAIPoseModel(self.device)
                return model, "mediapipe_default"
            
            # YOLOv8 ì‹œë„
            elif ULTRALYTICS_AVAILABLE:
                model = YOLOv8AIPoseModel(self.device)
                return model, "yolov8_default"
            
            # í´ë°±: ê°„ë‹¨í•œ AI ëª¨ë¸
            else:
                model = MediaPipeAIPoseModel(self.device)  # ë”ë¯¸ ëª¨ë“œë¡œ ì‘ë™
                return model, "dummy_ai_model"
                
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ AI ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            model = MediaPipeAIPoseModel("cpu")
            return model, "fallback_model"
    
    async def _run_pytorch_model(self, model: nn.Module, image: Image.Image) -> Dict[str, Any]:
        """PyTorch ëª¨ë¸ ì‹¤í–‰"""
        try:
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                output = model(image_tensor)
                
                # ì¶œë ¥ í•´ì„
                if isinstance(output, torch.Tensor):
                    keypoints = output.squeeze().cpu().numpy()
                    
                    keypoints_list = []
                    if len(keypoints.shape) == 2:  # [N, 3] í˜•íƒœ
                        for kp in keypoints:
                            keypoints_list.append([float(kp[0]), float(kp[1]), float(kp[2])])
                    else:
                        # ë”ë¯¸ í‚¤í¬ì¸íŠ¸
                        keypoints_list = self._generate_dummy_keypoints(image.size)
                    
                    return {
                        'keypoints': keypoints_list,
                        'success': True,
                        'model_type': 'pytorch'
                    }
            
            return {'keypoints': [], 'success': False, 'model_type': 'pytorch'}
            
        except Exception as e:
            self.logger.error(f"âŒ PyTorch ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {'keypoints': [], 'success': False, 'model_type': 'pytorch'}
    
    def _generate_dummy_keypoints(self, image_size: Tuple[int, int]) -> List[List[float]]:
        """ë”ë¯¸ í‚¤í¬ì¸íŠ¸ ìƒì„±"""
        width, height = image_size
        
        # OpenPose 18 í˜•íƒœ ë”ë¯¸ í‚¤í¬ì¸íŠ¸
        base_points = [
            (0.5, 0.1),    # nose
            (0.5, 0.15),   # neck
            (0.35, 0.25),  # right_shoulder
            (0.3, 0.35),   # right_elbow
            (0.25, 0.45),  # right_wrist
            (0.65, 0.25),  # left_shoulder
            (0.7, 0.35),   # left_elbow
            (0.75, 0.45),  # left_wrist
            (0.5, 0.5),    # middle_hip
            (0.4, 0.5),    # right_hip
            (0.35, 0.65),  # right_knee
            (0.3, 0.8),    # right_ankle
            (0.6, 0.5),    # left_hip
            (0.65, 0.65),  # left_knee
            (0.7, 0.8),    # left_ankle
            (0.48, 0.08),  # right_eye
            (0.52, 0.08),  # left_eye
            (0.46, 0.09),  # right_ear
            (0.54, 0.09)   # left_ear
        ]
        
        keypoints = []
        for x_ratio, y_ratio in base_points:
            x = x_ratio * width
            y = y_ratio * height
            confidence = 0.8
            keypoints.append([x, y, confidence])
        
        return keypoints
    
    # ==============================================
    # ğŸ”¥ AI ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (OpenCV ì™„ì „ ëŒ€ì²´)
    # ==============================================
    
    def _preprocess_image_with_ai(self, image: Union[np.ndarray, Image.Image, str]) -> Optional[Image.Image]:
        """AI ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (OpenCV ì™„ì „ ëŒ€ì²´)"""
        try:
            # ì´ë¯¸ì§€ ë¡œë”© ë° ë³€í™˜
            if isinstance(image, str):
                if os.path.exists(image):
                    image = Image.open(image)
                else:
                    try:
                        image_data = base64.b64decode(image)
                        image = Image.open(io.BytesIO(image_data))
                    except Exception:
                        return None
            elif isinstance(image, np.ndarray):
                if image.size == 0:
                    return None
                image = Image.fromarray(image)
            elif not isinstance(image, Image.Image):
                return None
            
            # RGB ë³€í™˜ (AI ì²˜ë¦¬ìš©)
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # í¬ê¸° ê²€ì¦
            if image.size[0] < 64 or image.size[1] < 64:
                return None
            
            # AI ê¸°ë°˜ ì§€ëŠ¥ì  ë¦¬ì‚¬ì´ì§•
            max_size = 1024 if self.is_m3_max else 512
            if max(image.size) > max_size:
                ratio = max_size / max(image.size)
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = self.image_processor.resize(image, new_size, 'bilinear')
            
            # AI ê¸°ë°˜ ì´ë¯¸ì§€ í–¥ìƒ (ì„ íƒì )
            if hasattr(self.config, 'enhance_image') and self.config.enhance_image:
                image = self._enhance_image_with_ai(image)
            
            return image
            
        except Exception as e:
            self.logger.error(f"âŒ AI ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _enhance_image_with_ai(self, image: Image.Image) -> Image.Image:
        """AI ê¸°ë°˜ ì´ë¯¸ì§€ í–¥ìƒ"""
        try:
            # ê¸°ë³¸ PIL ê¸°ë°˜ í–¥ìƒ
            enhancer = ImageEnhance.Contrast(image)
            enhanced = enhancer.enhance(1.1)
            
            enhancer = ImageEnhance.Sharpness(enhanced)
            enhanced = enhancer.enhance(1.05)
            
            return enhanced
            
        except Exception as e:
            self.logger.debug(f"AI ì´ë¯¸ì§€ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return image
    
    # ==============================================
    # ğŸ”¥ ê²°ê³¼ í›„ì²˜ë¦¬ ë° ë¶„ì„
    # ==============================================
    
    def _postprocess_ai_result(self, pose_result: Dict[str, Any], image: Image.Image, start_time: float) -> Dict[str, Any]:
        """AI ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            processing_time = time.time() - start_time
            
            # PoseMetrics ìƒì„±
            pose_metrics = PoseMetrics(
                keypoints=pose_result.get('keypoints', []),
                confidence_scores=[kp[2] for kp in pose_result.get('keypoints', []) if len(kp) > 2],
                model_used=pose_result.get('model_used', 'unknown'),
                processing_time=processing_time,
                image_resolution=image.size,
                ai_confidence=np.mean([kp[2] for kp in pose_result.get('keypoints', []) if len(kp) > 2]) if pose_result.get('keypoints') else 0.0
            )
            
            # AI ê¸°ë°˜ í¬ì¦ˆ ë¶„ì„
            pose_analysis = self._analyze_pose_quality_with_ai(pose_metrics)
            
            # AI ê¸°ë°˜ ì‹œê°í™” ìƒì„±
            visualization = None
            if self.ai_pose_config['visualization_enabled']:
                visualization = self._create_ai_pose_visualization(image, pose_metrics)
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                'success': pose_result.get('success', False),
                'keypoints': pose_metrics.keypoints,
                'confidence_scores': pose_metrics.confidence_scores,
                'pose_analysis': pose_analysis,
                'visualization': visualization,
                'processing_time': processing_time,
                'inference_time': pose_result.get('inference_time', 0.0),
                'model_used': pose_metrics.model_used,
                'image_resolution': pose_metrics.image_resolution,
                'step_info': {
                    'step_name': self.step_name,
                    'step_number': self.step_number,
                    'optimization_level': self.optimization_level,
                    'strict_mode': self.strict_mode,
                    'ai_model_name': self.active_model,
                    'model_type': pose_result.get('model_type', 'unknown'),
                    'basestep_version': '16.0-compatible',
                    'ai_based': True,
                    'opencv_disabled': True,
                    'dependency_status': self._get_dependency_status()
                }
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ AI ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_error_result(str(e))
    
    def _get_dependency_status(self) -> Dict[str, bool]:
        """ì˜ì¡´ì„± ìƒíƒœ ì¡°íšŒ (v16.0 í˜¸í™˜)"""
        try:
            if hasattr(self, 'dependency_manager') and hasattr(self.dependency_manager, 'dependency_status'):
                return {
                    'model_loader': self.dependency_manager.dependency_status.model_loader,
                    'step_interface': self.dependency_manager.dependency_status.step_interface,
                    'memory_manager': self.dependency_manager.dependency_status.memory_manager,
                    'data_converter': self.dependency_manager.dependency_status.data_converter,
                    'di_container': self.dependency_manager.dependency_status.di_container
                }
            else:
                return {
                    'model_loader': hasattr(self, 'model_loader') and self.model_loader is not None,
                    'step_interface': hasattr(self, 'model_interface') and self.model_interface is not None,
                    'memory_manager': hasattr(self, 'memory_manager') and self.memory_manager is not None,
                    'data_converter': hasattr(self, 'data_converter') and self.data_converter is not None,
                    'di_container': hasattr(self, 'di_container') and self.di_container is not None
                }
        except Exception as e:
            self.logger.debug(f"ì˜ì¡´ì„± ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    # ==============================================
    # ğŸ”¥ AI ê¸°ë°˜ í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„
    # ==============================================
    
    def _analyze_pose_quality_with_ai(self, pose_metrics: PoseMetrics) -> Dict[str, Any]:
        """AI ê¸°ë°˜ í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„"""
        try:
            if not pose_metrics.keypoints:
                return {
                    'suitable_for_fitting': False,
                    'issues': ['AI ëª¨ë¸ì—ì„œ í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'],
                    'recommendations': ['ë” ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ í¬ì¦ˆë¥¼ ëª…í™•íˆ í•´ì£¼ì„¸ìš”'],
                    'quality_score': 0.0,
                    'ai_confidence': 0.0,
                    'ai_based_analysis': True
                }
            
            # AI ì‹ ë¢°ë„ ê³„ì‚°
            ai_confidence = np.mean(pose_metrics.confidence_scores) if pose_metrics.confidence_scores else 0.0
            
            # AI ê¸°ë°˜ ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°
            head_score = self._calculate_ai_body_part_score(pose_metrics.keypoints, [0, 15, 16, 17, 18])
            torso_score = self._calculate_ai_body_part_score(pose_metrics.keypoints, [1, 2, 5, 8])
            arms_score = self._calculate_ai_body_part_score(pose_metrics.keypoints, [2, 3, 4, 5, 6, 7])
            legs_score = self._calculate_ai_body_part_score(pose_metrics.keypoints, [9, 10, 11, 12, 13, 14])
            
            # AI ê¸°ë°˜ ê³ ê¸‰ ë¶„ì„
            symmetry_score = self._calculate_ai_symmetry_score(pose_metrics.keypoints)
            visibility_score = self._calculate_ai_visibility_score(pose_metrics.keypoints)
            pose_angles = self._calculate_ai_pose_angles(pose_metrics.keypoints)
            body_proportions = self._calculate_ai_body_proportions(pose_metrics.keypoints, pose_metrics.image_resolution)
            pose_type = self._detect_ai_pose_type(pose_metrics.keypoints, pose_angles)
            
            # AI ê¸°ë°˜ ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self._calculate_ai_overall_quality_score(
                head_score, torso_score, arms_score, legs_score, 
                symmetry_score, visibility_score, ai_confidence
            )
            
            # AI ê¸°ë°˜ ì—„ê²©í•œ ì í•©ì„± íŒë‹¨
            min_score = 0.75 if self.strict_mode else 0.65
            min_confidence = 0.7 if self.strict_mode else 0.6
            visible_keypoints = sum(1 for kp in pose_metrics.keypoints if len(kp) > 2 and kp[2] > 0.5)
            suitable_for_fitting = (quality_score >= min_score and 
                                  ai_confidence >= min_confidence and
                                  visible_keypoints >= 10)
            
            # AI ê¸°ë°˜ ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
            issues = []
            recommendations = []
            
            if ai_confidence < min_confidence:
                issues.append(f'AI ëª¨ë¸ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({ai_confidence:.2f})')
                recommendations.append('ì¡°ëª…ì´ ì¢‹ì€ í™˜ê²½ì—ì„œ ë‹¤ì‹œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            if visible_keypoints < 10:
                issues.append('ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤')
                recommendations.append('ì „ì‹ ì´ ëª…í™•íˆ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            if symmetry_score < 0.6:
                issues.append('ì¢Œìš° ëŒ€ì¹­ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤')
                recommendations.append('ì •ë©´ì„ í–¥í•´ ê· í˜•ì¡íŒ ìì„¸ë¡œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            if torso_score < 0.7:
                issues.append('ìƒì²´ í¬ì¦ˆê°€ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤')
                recommendations.append('ì–´ê¹¨ì™€ íŒ”ì´ ëª…í™•íˆ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            return {
                'suitable_for_fitting': suitable_for_fitting,
                'issues': issues,
                'recommendations': recommendations,
                'quality_score': quality_score,
                'ai_confidence': ai_confidence,
                'visible_keypoints': visible_keypoints,
                'total_keypoints': len(pose_metrics.keypoints),
                
                # AI ê¸°ë°˜ ì‹ ì²´ ë¶€ìœ„ë³„ ìƒì„¸ ì ìˆ˜
                'detailed_scores': {
                    'head': head_score,
                    'torso': torso_score,
                    'arms': arms_score,
                    'legs': legs_score
                },
                
                # AI ê¸°ë°˜ ê³ ê¸‰ ë¶„ì„ ê²°ê³¼
                'advanced_analysis': {
                    'symmetry_score': symmetry_score,
                    'visibility_score': visibility_score,
                    'pose_angles': pose_angles,
                    'body_proportions': body_proportions,
                    'pose_type': pose_type.value if pose_type else 'unknown'
                },
                
                # AI ëª¨ë¸ ì„±ëŠ¥ ì •ë³´
                'model_performance': {
                    'model_name': pose_metrics.model_used,
                    'processing_time': pose_metrics.processing_time,
                    'ai_based': True,
                    'opencv_disabled': True,
                    'basestep_version': '16.0-compatible'
                },
                
                'ai_based_analysis': True,
                'strict_mode': self.strict_mode
            }
            
        except Exception as e:
            self.logger.error(f"âŒ AI ê¸°ë°˜ í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise
            return {
                'suitable_for_fitting': False,
                'issues': ['AI ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨'],
                'recommendations': ['AI ëª¨ë¸ ìƒíƒœë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”'],
                'quality_score': 0.0,
                'ai_confidence': 0.0,
                'ai_based_analysis': True
            }
    
    # ==============================================
    # ğŸ”¥ AI ê¸°ë°˜ ë¶„ì„ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ==============================================
    
    def _calculate_ai_body_part_score(self, keypoints: List[List[float]], part_indices: List[int]) -> float:
        """AI ê¸°ë°˜ ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°"""
        try:
            if not keypoints or not part_indices:
                return 0.0
            
            visible_count = 0
            total_confidence = 0.0
            confidence_threshold = self.ai_pose_config.get('confidence_threshold', 0.5)
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
            
            if visible_count == 0:
                return 0.0
            
            # AI ê¸°ë°˜ ê°€ì¤‘ ì ìˆ˜
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            
            return visibility_ratio * avg_confidence
            
        except Exception as e:
            self.logger.debug(f"AI ì‹ ì²´ ë¶€ìœ„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_ai_symmetry_score(self, keypoints: List[List[float]]) -> float:
        """AI ê¸°ë°˜ ì¢Œìš° ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            if not keypoints or len(keypoints) < 18:
                return 0.0
            
            # AI ê¸°ë°˜ ëŒ€ì¹­ ë¶€ìœ„ ìŒ ì •ì˜
            symmetric_pairs = [
                (2, 5),   # right_shoulder, left_shoulder
                (3, 6),   # right_elbow, left_elbow
                (4, 7),   # right_wrist, left_wrist
                (9, 12),  # right_hip, left_hip
                (10, 13), # right_knee, left_knee
                (11, 14), # right_ankle, left_ankle
                (15, 16), # right_eye, left_eye
                (17, 18)  # right_ear, left_ear
            ]
            
            symmetry_scores = []
            confidence_threshold = 0.3
            
            for right_idx, left_idx in symmetric_pairs:
                if (right_idx < len(keypoints) and left_idx < len(keypoints) and
                    len(keypoints[right_idx]) >= 3 and len(keypoints[left_idx]) >= 3):
                    
                    right_kp = keypoints[right_idx]
                    left_kp = keypoints[left_idx]
                    
                    # AI ê¸°ë°˜ ì‹ ë¢°ë„ ê²€ì¦
                    if right_kp[2] > confidence_threshold and left_kp[2] > confidence_threshold:
                        # AI ê¸°ë°˜ ì¤‘ì‹¬ì„  ê³„ì‚°
                        center_x = sum(kp[0] for kp in keypoints if len(kp) >= 3 and kp[2] > confidence_threshold) / \
                                 max(len([kp for kp in keypoints if len(kp) >= 3 and kp[2] > confidence_threshold]), 1)
                        
                        right_dist = abs(right_kp[0] - center_x)
                        left_dist = abs(left_kp[0] - center_x)
                        
                        # AI ê¸°ë°˜ ëŒ€ì¹­ì„± ì ìˆ˜
                        max_dist = max(right_dist, left_dist)
                        if max_dist > 0:
                            symmetry = 1.0 - abs(right_dist - left_dist) / max_dist
                            # AI ì‹ ë¢°ë„ë¡œ ê°€ì¤‘
                            weighted_symmetry = symmetry * min(right_kp[2], left_kp[2])
                            symmetry_scores.append(weighted_symmetry)
            
            if not symmetry_scores:
                return 0.0
            
            return np.mean(symmetry_scores)
            
        except Exception as e:
            self.logger.debug(f"AI ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_ai_visibility_score(self, keypoints: List[List[float]]) -> float:
        """AI ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            if not keypoints:
                return 0.0
            
            confidence_threshold = self.ai_pose_config.get('confidence_threshold', 0.5)
            visible_count = 0
            total_confidence = 0.0
            
            for kp in keypoints:
                if len(kp) >= 3:
                    if kp[2] > confidence_threshold:
                        visible_count += 1
                        total_confidence += kp[2]
            
            if visible_count == 0:
                return 0.0
            
            # AI ê¸°ë°˜ ê°€ì‹œì„± ì ìˆ˜ (ê°€ì‹œì„± ë¹„ìœ¨ê³¼ í‰ê·  ì‹ ë¢°ë„ ì¡°í•©)
            visibility_ratio = visible_count / len(keypoints)
            avg_confidence = total_confidence / visible_count
            
            # AI ì‹ ë¢°ë„ ê°€ì¤‘ ì ìš©
            ai_weighted_score = visibility_ratio * avg_confidence * 1.2  # AI ë³´ì • ê³„ìˆ˜
            
            return min(ai_weighted_score, 1.0)
            
        except Exception as e:
            self.logger.debug(f"AI ê°€ì‹œì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_ai_pose_angles(self, keypoints: List[List[float]]) -> Dict[str, float]:
        """AI ê¸°ë°˜ í¬ì¦ˆ ê°ë„ ê³„ì‚°"""
        try:
            angles = {}
            
            if not keypoints or len(keypoints) < 18:
                return angles
            
            def calculate_ai_angle(p1, p2, p3):
                """AI ê¸°ë°˜ ì„¸ ì  ì‚¬ì´ì˜ ê°ë„ ê³„ì‚°"""
                try:
                    v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
                    v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
                    
                    # AI ê¸°ë°˜ ê°ë„ ê³„ì‚° (ì •ê·œí™” í¬í•¨)
                    cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
                    cos_angle = np.clip(cos_angle, -1.0, 1.0)
                    angle = np.arccos(cos_angle)
                    
                    return np.degrees(angle)
                except:
                    return 0.0
            
            confidence_threshold = 0.3
            
            # AI ê¸°ë°˜ íŒ”ê¿ˆì¹˜ ê°ë„ (ì˜¤ë¥¸ìª½)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [2, 3, 4]):  # shoulder, elbow, wrist
                angles['right_elbow'] = calculate_ai_angle(keypoints[2], keypoints[3], keypoints[4])
            
            # AI ê¸°ë°˜ íŒ”ê¿ˆì¹˜ ê°ë„ (ì™¼ìª½)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [5, 6, 7]):  # shoulder, elbow, wrist
                angles['left_elbow'] = calculate_ai_angle(keypoints[5], keypoints[6], keypoints[7])
            
            # AI ê¸°ë°˜ ë¬´ë¦ ê°ë„ (ì˜¤ë¥¸ìª½)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [9, 10, 11]):  # hip, knee, ankle
                angles['right_knee'] = calculate_ai_angle(keypoints[9], keypoints[10], keypoints[11])
            
            # AI ê¸°ë°˜ ë¬´ë¦ ê°ë„ (ì™¼ìª½)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [12, 13, 14]):  # hip, knee, ankle
                angles['left_knee'] = calculate_ai_angle(keypoints[12], keypoints[13], keypoints[14])
            
            # AI ê¸°ë°˜ ì–´ê¹¨ ê¸°ìš¸ê¸°
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [2, 5]):  # right_shoulder, left_shoulder
                shoulder_slope = np.degrees(np.arctan2(
                    keypoints[5][1] - keypoints[2][1],  # left_y - right_y
                    keypoints[5][0] - keypoints[2][0] + 1e-8   # left_x - right_x
                ))
                angles['shoulder_slope'] = abs(shoulder_slope)
            
            return angles
            
        except Exception as e:
            self.logger.debug(f"AI í¬ì¦ˆ ê°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_ai_body_proportions(self, keypoints: List[List[float]], image_resolution: Tuple[int, int]) -> Dict[str, float]:
        """AI ê¸°ë°˜ ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°"""
        try:
            proportions = {}
            
            if not keypoints or len(keypoints) < 18 or not image_resolution:
                return proportions
            
            width, height = image_resolution
            confidence_threshold = 0.3
            
            def get_valid_ai_keypoint(idx):
                """AI ê¸°ë°˜ ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ ë°˜í™˜"""
                if (idx < len(keypoints) and len(keypoints[idx]) >= 3 and 
                    keypoints[idx][2] > confidence_threshold):
                    return keypoints[idx]
                return None
            
            def ai_euclidean_distance(p1, p2):
                """AI ê¸°ë°˜ ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ ê³„ì‚°"""
                if p1 and p2:
                    # AI ì‹ ë¢°ë„ ê°€ì¤‘ ê±°ë¦¬
                    base_dist = np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)
                    confidence_weight = (p1[2] + p2[2]) / 2
                    return base_dist * confidence_weight
                return 0.0
            
            # AI ê¸°ë°˜ ë¨¸ë¦¬-ëª© ê¸¸ì´
            nose = get_valid_ai_keypoint(0)
            neck = get_valid_ai_keypoint(1)
            if nose and neck:
                proportions['head_neck_ratio'] = ai_euclidean_distance(nose, neck) / height
            
            # AI ê¸°ë°˜ ìƒì²´ ê¸¸ì´ (ëª©-ì—‰ë©ì´)
            if neck:
                mid_hip = get_valid_ai_keypoint(8)
                if mid_hip:
                    proportions['torso_ratio'] = ai_euclidean_distance(neck, mid_hip) / height
            
            # AI ê¸°ë°˜ íŒ” ê¸¸ì´ (ì–´ê¹¨-ì†ëª©)
            right_shoulder = get_valid_ai_keypoint(2)
            right_wrist = get_valid_ai_keypoint(4)
            if right_shoulder and right_wrist:
                proportions['right_arm_ratio'] = ai_euclidean_distance(right_shoulder, right_wrist) / height
            
            left_shoulder = get_valid_ai_keypoint(5)
            left_wrist = get_valid_ai_keypoint(7)
            if left_shoulder and left_wrist:
                proportions['left_arm_ratio'] = ai_euclidean_distance(left_shoulder, left_wrist) / height
            
            # AI ê¸°ë°˜ ë‹¤ë¦¬ ê¸¸ì´ (ì—‰ë©ì´-ë°œëª©)
            right_hip = get_valid_ai_keypoint(9)
            right_ankle = get_valid_ai_keypoint(11)
            if right_hip and right_ankle:
                proportions['right_leg_ratio'] = ai_euclidean_distance(right_hip, right_ankle) / height
            
            left_hip = get_valid_ai_keypoint(12)
            left_ankle = get_valid_ai_keypoint(14)
            if left_hip and left_ankle:
                proportions['left_leg_ratio'] = ai_euclidean_distance(left_hip, left_ankle) / height
            
            # AI ê¸°ë°˜ ì–´ê¹¨ ë„ˆë¹„
            if right_shoulder and left_shoulder:
                proportions['shoulder_width_ratio'] = ai_euclidean_distance(right_shoulder, left_shoulder) / width
            
            # AI ê¸°ë°˜ ì—‰ë©ì´ ë„ˆë¹„
            if right_hip and left_hip:
                proportions['hip_width_ratio'] = ai_euclidean_distance(right_hip, left_hip) / width
            
            return proportions
            
        except Exception as e:
            self.logger.debug(f"AI ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _detect_ai_pose_type(self, keypoints: List[List[float]], angles: Dict[str, float]) -> PoseType:
        """AI ê¸°ë°˜ í¬ì¦ˆ íƒ€ì… ê°ì§€"""
        try:
            if not keypoints or not angles:
                return PoseType.UNKNOWN
            
            # AI ê¸°ë°˜ T-í¬ì¦ˆ ê°ì§€
            if ('right_elbow' in angles and 'left_elbow' in angles and
                angles['right_elbow'] > 160 and angles['left_elbow'] > 160 and
                'shoulder_slope' in angles and angles['shoulder_slope'] < 15):
                return PoseType.T_POSE
            
            # AI ê¸°ë°˜ A-í¬ì¦ˆ ê°ì§€
            if ('right_elbow' in angles and 'left_elbow' in angles and
                angles['right_elbow'] < 120 and angles['left_elbow'] < 120):
                return PoseType.A_POSE
            
            # AI ê¸°ë°˜ ì•‰ì€ ìì„¸ ê°ì§€ (ë¬´ë¦ì´ ë§ì´ êµ¬ë¶€ëŸ¬ì§„ ê²½ìš°)
            if ('right_knee' in angles and 'left_knee' in angles and
                angles['right_knee'] < 120 and angles['left_knee'] < 120):
                return PoseType.SITTING
            
            # AI ê¸°ë°˜ ì•¡ì…˜ í¬ì¦ˆ ê°ì§€ (ê°ë„ ë³€í™”ê°€ í° ê²½ìš°)
            if angles:
                angle_variance = np.var(list(angles.values()))
                if angle_variance > 1000:  # ê°ë„ ë³€í™”ê°€ í° ê²½ìš°
                    return PoseType.ACTION
            
            return PoseType.STANDING
            
        except Exception as e:
            self.logger.debug(f"AI í¬ì¦ˆ íƒ€ì… ê°ì§€ ì‹¤íŒ¨: {e}")
            return PoseType.UNKNOWN
    
    def _calculate_ai_overall_quality_score(
        self, head_score: float, torso_score: float, arms_score: float, legs_score: float,
        symmetry_score: float, visibility_score: float, ai_confidence: float
    ) -> float:
        """AI ê¸°ë°˜ ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            # AI ê°€ì¤‘ í‰ê·  ê³„ì‚°
            base_scores = [
                head_score * 0.15,
                torso_score * 0.35,
                arms_score * 0.25,
                legs_score * 0.25
            ]
            
            advanced_scores = [
                symmetry_score * 0.3,
                visibility_score * 0.7
            ]
            
            base_score = sum(base_scores)
            advanced_score = sum(advanced_scores)
            
            # AI ì‹ ë¢°ë„ë¡œ ê°€ì¤‘ + AI ë³´ì • ê³„ìˆ˜
            ai_correction_factor = 1.1 if ai_confidence > 0.8 else 1.0
            overall_score = (base_score * 0.7 + advanced_score * 0.3) * ai_confidence * ai_correction_factor
            
            return max(0.0, min(1.0, overall_score))  # 0-1 ë²”ìœ„ë¡œ ì œí•œ
            
        except Exception as e:
            self.logger.debug(f"AI ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    # ==============================================
    # ğŸ”¥ AI ê¸°ë°˜ ì‹œê°í™” ë° ìœ í‹¸ë¦¬í‹°
    # ==============================================
    
    def _create_ai_pose_visualization(self, image: Image.Image, pose_metrics: PoseMetrics) -> Optional[str]:
        """AI ê¸°ë°˜ í¬ì¦ˆ ì‹œê°í™” ìƒì„±"""
        try:
            if not pose_metrics.keypoints:
                return None
            
            vis_image = image.copy()
            draw = ImageDraw.Draw(vis_image)
            
            confidence_threshold = self.ai_pose_config['confidence_threshold']
            
            # AI ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° (í¬ê¸°ì™€ ìƒ‰ìƒì„ AI ì‹ ë¢°ë„ë¡œ ì¡°ì ˆ)
            for i, kp in enumerate(pose_metrics.keypoints):
                if len(kp) >= 3 and kp[2] > confidence_threshold:
                    x, y = int(kp[0]), int(kp[1])
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    # AI ì‹ ë¢°ë„ ê¸°ë°˜ í¬ê¸° ì¡°ì ˆ
                    radius = int(4 + kp[2] * 8)  # AI ì‹ ë¢°ë„ì— ë”°ë¥¸ í¬ê¸°
                    
                    # AI ì‹ ë¢°ë„ ê¸°ë°˜ íˆ¬ëª…ë„ ì¡°ì ˆ
                    alpha = int(255 * kp[2])
                    color_with_alpha = (*color, alpha)
                    
                    draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                               fill=color, outline=(255, 255, 255), width=2)
            
            # AI ê¸°ë°˜ ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²°ì„  ê·¸ë¦¬ê¸°
            for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
                if (start_idx < len(pose_metrics.keypoints) and 
                    end_idx < len(pose_metrics.keypoints)):
                    
                    start_kp = pose_metrics.keypoints[start_idx]
                    end_kp = pose_metrics.keypoints[end_idx]
                    
                    if (len(start_kp) >= 3 and len(end_kp) >= 3 and
                        start_kp[2] > confidence_threshold and end_kp[2] > confidence_threshold):
                        
                        start_point = (int(start_kp[0]), int(start_kp[1]))
                        end_point = (int(end_kp[0]), int(end_kp[1]))
                        color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                        
                        # AI ì‹ ë¢°ë„ ê¸°ë°˜ ì„  ë‘ê»˜
                        avg_confidence = (start_kp[2] + end_kp[2]) / 2
                        line_width = int(2 + avg_confidence * 6)  # AI ì‹ ë¢°ë„ì— ë”°ë¥¸ ë‘ê»˜
                        
                        draw.line([start_point, end_point], fill=color, width=line_width)
            
            # AI ì‹ ë¢°ë„ ì •ë³´ ì¶”ê°€
            if hasattr(pose_metrics, 'ai_confidence'):
                ai_info = f"AI ì‹ ë¢°ë„: {pose_metrics.ai_confidence:.3f}"
                try:
                    font = ImageFont.load_default()
                    draw.text((10, 10), ai_info, fill=(255, 255, 255), font=font)
                except:
                    draw.text((10, 10), ai_info, fill=(255, 255, 255))
            
            # Base64ë¡œ ì¸ì½”ë”©
            buffer = io.BytesIO()
            vis_image.save(buffer, format='JPEG', quality=95)
            image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            return f"data:image/jpeg;base64,{image_base64}"
            
        except Exception as e:
            self.logger.error(f"âŒ AI í¬ì¦ˆ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_cache_key(self, image: Image.Image, clothing_type: Optional[str]) -> str:
        """AI ê¸°ë°˜ ìºì‹œ í‚¤ ìƒì„±"""
        try:
            image_bytes = io.BytesIO()
            image.save(image_bytes, format='JPEG', quality=50)
            image_hash = hashlib.md5(image_bytes.getvalue()).hexdigest()[:16]
            
            config_str = f"{clothing_type}_{self.active_model}_{self.ai_pose_config['confidence_threshold']}_ai"
            config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]
            
            return f"ai_pose_{image_hash}_{config_hash}"
            
        except Exception:
            return f"ai_pose_{int(time.time())}"
    
    def _save_to_cache(self, cache_key: str, result: Dict[str, Any]):
        """AI ê²°ê³¼ ìºì‹œì— ì €ì¥"""
        try:
            if len(self.prediction_cache) >= self.cache_max_size:
                oldest_key = next(iter(self.prediction_cache))
                del self.prediction_cache[oldest_key]
            
            cached_result = result.copy()
            cached_result['visualization'] = None  # ë©”ëª¨ë¦¬ ì ˆì•½
            
            self.prediction_cache[cache_key] = cached_result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _create_error_result(self, error_message: str, processing_time: float = 0.0) -> Dict[str, Any]:
        """AI ê¸°ë°˜ ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'keypoints': [],
            'confidence_scores': [],
            'pose_analysis': {
                'suitable_for_fitting': False,
                'issues': [error_message],
                'recommendations': ['AI ëª¨ë¸ ìƒíƒœë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”'],
                'quality_score': 0.0,
                'ai_confidence': 0.0,
                'ai_based_analysis': True
            },
            'visualization': None,
            'processing_time': processing_time,
            'inference_time': 0.0,
            'model_used': 'error',
            'step_info': {
                'step_name': self.step_name,
                'step_number': self.step_number,
                'optimization_level': getattr(self, 'optimization_level', 'unknown'),
                'strict_mode': self.strict_mode,
                'ai_model_name': getattr(self, 'active_model', 'none'),
                'basestep_version': '16.0-compatible',
                'ai_based': True,
                'opencv_disabled': True,
                'dependency_status': self._get_dependency_status()
            }
        }
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin v16.0 í˜¸í™˜ ë©”ì„œë“œë“¤
    # ==============================================
    
    async def initialize(self) -> bool:
        """BaseStepMixin v16.0 í˜¸í™˜ ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"ğŸš€ {self.step_name} AI ê¸°ë°˜ BaseStepMixin v16.0 í˜¸í™˜ ì´ˆê¸°í™” ì‹œì‘")
            start_time = time.time()
            
            # ì˜ì¡´ì„± ì£¼ì… ê²€ì¦
            if not hasattr(self, 'model_loader') or not self.model_loader:
                # ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„
                if hasattr(self, 'dependency_manager'):
                    success = self.dependency_manager.auto_inject_dependencies()
                    if not success:
                        self.logger.warning("âš ï¸ ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨ - ìˆ˜ë™ ì‹œë„")
                        success = self._manual_auto_inject()
                else:
                    success = self._manual_auto_inject()
                
                if not success:
                    error_msg = "ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨"
                    if self.strict_mode:
                        raise RuntimeError(f"Strict Mode: {error_msg}")
                    self.logger.warning(f"âš ï¸ {error_msg} - ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰")
            
            # AI ëª¨ë¸ ì¤€ë¹„
            self.has_model = True
            self.model_loaded = True
            
            # AI ì´ë¯¸ì§€ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ê²€ì¦
            if not hasattr(self, 'image_processor') or not self.image_processor:
                self.image_processor = AIImageProcessor(self.device)
                self.logger.info("âœ… AI ì´ë¯¸ì§€ ì²˜ë¦¬ê¸° ì¬ì´ˆê¸°í™” ì™„ë£Œ")
            
            # AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ê²€ì¦
            if not hasattr(self, 'segmentation_processor') or not self.segmentation_processor:
                self.segmentation_processor = AISegmentationProcessor(self.device)
                self.logger.info("âœ… AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ê¸° ì¬ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            self.is_ready = True
            
            elapsed_time = time.time() - start_time
            self.logger.info(f"âœ… {self.step_name} AI ê¸°ë°˜ BaseStepMixin v16.0 í˜¸í™˜ ì´ˆê¸°í™” ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
            self.logger.info(f"ğŸ¤– AI ëª¨ë¸ ìš°ì„ ìˆœìœ„: {self.ai_pose_config['model_priority']}")
            self.logger.info(f"ğŸš« OpenCV ë¹„í™œì„±í™”: {self.ai_pose_config.get('opencv_disabled', True)}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} AI ê¸°ë°˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise
            return False
    
    def get_status(self) -> Dict[str, Any]:
        """BaseStepMixin v16.0 í˜¸í™˜ ìƒíƒœ ë°˜í™˜"""
        try:
            return {
                'step_name': self.step_name,
                'step_number': self.step_number,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'has_model': self.has_model,
                'model_loaded': self.model_loaded,
                'device': self.device,
                'is_m3_max': self.is_m3_max,
                'memory_gb': self.memory_gb,
                'dependencies': self._get_dependency_status(),
                'performance_metrics': self.performance_metrics,
                'active_model': getattr(self, 'active_model', None),
                'ai_based': True,
                'opencv_disabled': True,
                'ai_models_available': {
                    'mediapipe': MEDIAPIPE_AVAILABLE,
                    'yolov8': ULTRALYTICS_AVAILABLE,
                    'transformers': TRANSFORMERS_AVAILABLE
                },
                'version': '16.0-compatible',
                'timestamp': time.time()
            }
        except Exception as e:
            self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'version': '16.0-compatible', 'ai_based': True}
    
    async def cleanup(self) -> Dict[str, Any]:
        """BaseStepMixin v16.0 í˜¸í™˜ ì •ë¦¬"""
        try:
            self.logger.info(f"ğŸ§¹ {self.step_name} AI ê¸°ë°˜ BaseStepMixin v16.0 í˜¸í™˜ ì •ë¦¬ ì‹œì‘...")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_result = await self.optimize_memory_async(aggressive=True)
            
            # AI ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'ai_models'):
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'cleanup'):
                            model.cleanup()
                        elif hasattr(model, 'cpu'):
                            model.cpu()
                        del model
                    except Exception as e:
                        self.logger.debug(f"AI ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨ {model_name}: {e}")
                self.ai_models.clear()
            
            # AI ì²˜ë¦¬ê¸° ì •ë¦¬
            if hasattr(self, 'image_processor'):
                del self.image_processor
                self.image_processor = None
            
            if hasattr(self, 'segmentation_processor'):
                del self.segmentation_processor
                self.segmentation_processor = None
            
            # ìºì‹œ ì •ë¦¬
            if hasattr(self, 'prediction_cache'):
                self.prediction_cache.clear()
            
            # ìƒíƒœ ë¦¬ì…‹
            self.is_ready = False
            self.warmup_completed = False
            self.has_model = False
            self.model_loaded = False
            
            # ì˜ì¡´ì„± í•´ì œ (ì°¸ì¡°ë§Œ ì œê±°)
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            self.logger.info(f"âœ… {self.step_name} AI ê¸°ë°˜ BaseStepMixin v16.0 í˜¸í™˜ ì •ë¦¬ ì™„ë£Œ")
            
            return {
                "success": True,
                "cleanup_result": cleanup_result,
                "step_name": self.step_name,
                "ai_based": True,
                "opencv_disabled": True,
                "version": "16.0-compatible"
            }
        except Exception as e:
            self.logger.error(f"âŒ AI ê¸°ë°˜ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "ai_based": True}

# =================================================================
# ğŸ”¥ í˜¸í™˜ì„± ì§€ì› í•¨ìˆ˜ë“¤ (AI ëª¨ë¸ ê¸°ë°˜)
# =================================================================

async def create_ai_pose_estimation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    strict_mode: bool = True,
    **kwargs
) -> PoseEstimationStep:
    """
    BaseStepMixin v16.0 í˜¸í™˜ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • Step ìƒì„± í•¨ìˆ˜
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        strict_mode: ì—„ê²© ëª¨ë“œ
        **kwargs: ì¶”ê°€ ì„¤ì •
        
    Returns:
        PoseEstimationStep: ì´ˆê¸°í™”ëœ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • Step
    """
    try:
        # ë””ë°”ì´ìŠ¤ ì²˜ë¦¬
        device_param = None if device == "auto" else device
        
        # config í†µí•©
        if config is None:
            config = {}
        config.update(kwargs)
        config['ai_models_only'] = True
        config['opencv_disabled'] = True
        config['basestep_version'] = '16.0-compatible'
        
        # Step ìƒì„± (BaseStepMixin v16.0 í˜¸í™˜ + AI ê¸°ë°˜)
        step = PoseEstimationStep(device=device_param, config=config, strict_mode=strict_mode)
        
        # AI ê¸°ë°˜ ì´ˆê¸°í™” ì‹¤í–‰
        initialization_success = await step.initialize()
        
        if not initialization_success:
            error_msg = "BaseStepMixin v16.0 í˜¸í™˜: AI ê¸°ë°˜ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨"
            if strict_mode:
                raise RuntimeError(f"Strict Mode: {error_msg}")
            else:
                step.logger.warning(f"âš ï¸ {error_msg} - Step ìƒì„±ì€ ì™„ë£Œë¨")
        
        return step
        
    except Exception as e:
        logger.error(f"âŒ BaseStepMixin v16.0 í˜¸í™˜ create_ai_pose_estimation_step ì‹¤íŒ¨: {e}")
        if strict_mode:
            raise
        else:
            step = PoseEstimationStep(device='cpu', strict_mode=False)
            return step

def create_ai_pose_estimation_step_sync(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    strict_mode: bool = True,
    **kwargs
) -> PoseEstimationStep:
    """ë™ê¸°ì‹ BaseStepMixin v16.0 í˜¸í™˜ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • Step ìƒì„±"""
    try:
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            create_ai_pose_estimation_step(device, config, strict_mode, **kwargs)
        )
    except Exception as e:
        logger.error(f"âŒ BaseStepMixin v16.0 í˜¸í™˜ create_ai_pose_estimation_step_sync ì‹¤íŒ¨: {e}")
        if strict_mode:
            raise
        else:
            return PoseEstimationStep(device='cpu', strict_mode=False)

# =================================================================
# ğŸ”¥ AI ê¸°ë°˜ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (OpenCV ì™„ì „ ëŒ€ì²´)
# =================================================================

def validate_ai_keypoints(keypoints_18: List[List[float]]) -> bool:
    """AI ê¸°ë°˜ OpenPose 18 keypoints ìœ íš¨ì„± ê²€ì¦"""
    try:
        if len(keypoints_18) != 18:
            return False
        
        for kp in keypoints_18:
            if len(kp) != 3:
                return False
            if not all(isinstance(x, (int, float)) for x in kp):
                return False
            if kp[2] < 0 or kp[2] > 1:
                return False
        
        return True
        
    except Exception:
        return False

def convert_keypoints_to_coco_ai(keypoints_18: List[List[float]]) -> List[List[float]]:
    """AI ê¸°ë°˜ OpenPose 18ì„ COCO 17 í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        # OpenPose 18 -> COCO 17 ë§¤í•‘ (AI ìµœì í™”)
        op_to_coco_mapping = {
            0: 0,   # nose
            15: 1,  # right_eye -> left_eye (COCO ê´€ì )
            16: 2,  # left_eye -> right_eye
            17: 3,  # right_ear -> left_ear
            18: 4,  # left_ear -> right_ear
            2: 5,   # right_shoulder -> left_shoulder (COCO ê´€ì )
            5: 6,   # left_shoulder -> right_shoulder
            3: 7,   # right_elbow -> left_elbow
            6: 8,   # left_elbow -> right_elbow
            4: 9,   # right_wrist -> left_wrist
            7: 10,  # left_wrist -> right_wrist
            9: 11,  # right_hip -> left_hip
            12: 12, # left_hip -> right_hip
            10: 13, # right_knee -> left_knee
            13: 14, # left_knee -> right_knee
            11: 15, # right_ankle -> left_ankle
            14: 16  # left_ankle -> right_ankle
        }
        
        coco_keypoints = []
        for coco_idx in range(17):
            if coco_idx in op_to_coco_mapping.values():
                op_idx = next(k for k, v in op_to_coco_mapping.items() if v == coco_idx)
                if op_idx < len(keypoints_18):
                    # AI ì‹ ë¢°ë„ ê°€ì¤‘ ì ìš©
                    kp = keypoints_18[op_idx].copy()
                    if len(kp) >= 3:
                        kp[2] = min(kp[2] * 1.1, 1.0)  # AI ë³´ì • ê³„ìˆ˜
                    coco_keypoints.append(kp)
                else:
                    coco_keypoints.append([0.0, 0.0, 0.0])
            else:
                coco_keypoints.append([0.0, 0.0, 0.0])
        
        return coco_keypoints
        
    except Exception as e:
        logger.error(f"AI ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return [[0.0, 0.0, 0.0]] * 17

def draw_ai_pose_on_image(
    image: Union[np.ndarray, Image.Image],
    keypoints: List[List[float]],
    confidence_threshold: float = 0.5,
    keypoint_size: int = 4,
    line_width: int = 3,
    ai_enhanced: bool = True
) -> Image.Image:
    """AI ê¸°ë°˜ ì´ë¯¸ì§€ì— í¬ì¦ˆ ê·¸ë¦¬ê¸° (OpenCV ì™„ì „ ëŒ€ì²´)"""
    try:
        # ì´ë¯¸ì§€ ë³€í™˜ (AI ì²˜ë¦¬ìš©)
        if isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        else:
            pil_image = image.copy()
        
        draw = ImageDraw.Draw(pil_image)
        
        # AI ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for i, kp in enumerate(keypoints):
            if len(kp) >= 3 and kp[2] > confidence_threshold:
                x, y = int(kp[0]), int(kp[1])
                color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                
                # AI ì‹ ë¢°ë„ ê¸°ë°˜ í¬ê¸° ì¡°ì ˆ
                if ai_enhanced:
                    radius = int(keypoint_size + kp[2] * 6)  # AI ì‹ ë¢°ë„ ë°˜ì˜
                    alpha = int(255 * kp[2])  # íˆ¬ëª…ë„ ì¡°ì ˆ
                else:
                    radius = keypoint_size
                
                draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                           fill=color, outline=(255, 255, 255), width=2)
        
        # AI ê¸°ë°˜ ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸°
        for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
            if (start_idx < len(keypoints) and end_idx < len(keypoints)):
                start_kp = keypoints[start_idx]
                end_kp = keypoints[end_idx]
                
                if (len(start_kp) >= 3 and len(end_kp) >= 3 and
                    start_kp[2] > confidence_threshold and end_kp[2] > confidence_threshold):
                    
                    start_point = (int(start_kp[0]), int(start_kp[1]))
                    end_point = (int(end_kp[0]), int(end_kp[1]))
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    # AI ì‹ ë¢°ë„ ê¸°ë°˜ ì„  ë‘ê»˜
                    if ai_enhanced:
                        avg_confidence = (start_kp[2] + end_kp[2]) / 2
                        adjusted_width = int(line_width * avg_confidence * 1.2)  # AI ë³´ì •
                    else:
                        adjusted_width = line_width
                    
                    draw.line([start_point, end_point], fill=color, width=max(1, adjusted_width))
        
        return pil_image
        
    except Exception as e:
        logger.error(f"AI ê¸°ë°˜ í¬ì¦ˆ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
        return image if isinstance(image, Image.Image) else Image.fromarray(image)

def analyze_ai_pose_for_clothing(
    keypoints: List[List[float]],
    clothing_type: str = "default",
    confidence_threshold: float = 0.5,
    strict_analysis: bool = True,
    ai_enhanced: bool = True
) -> Dict[str, Any]:
    """AI ê¸°ë°˜ ì˜ë¥˜ë³„ í¬ì¦ˆ ì í•©ì„± ë¶„ì„ (OpenCV ì™„ì „ ëŒ€ì²´)"""
    try:
        if not keypoints:
            return {
                'suitable_for_fitting': False,
                'issues': ["AI ëª¨ë¸ì—ì„œ í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"],
                'recommendations': ["AI ëª¨ë¸ ìƒíƒœë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë” ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”"],
                'pose_score': 0.0,
                'ai_confidence': 0.0,
                'ai_based_analysis': True
            }
        
        # ì˜ë¥˜ë³„ AI ê°€ì¤‘ì¹˜
        weights = PoseEstimationStep.CLOTHING_POSE_WEIGHTS.get(
            clothing_type, 
            PoseEstimationStep.CLOTHING_POSE_WEIGHTS['default']
        )
        
        # AI ê¸°ë°˜ ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°
        def calculate_ai_body_part_score(part_indices: List[int]) -> float:
            visible_count = 0
            total_confidence = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > confidence_threshold:
                        visible_count += 1
                        confidence = keypoints[idx][2]
                        # AI ì‹ ë¢°ë„ ë³´ì •
                        if ai_enhanced:
                            confidence = min(confidence * 1.1, 1.0)
                        total_confidence += confidence
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            
            # AI ê°€ì¤‘ ì ìˆ˜
            return visibility_ratio * avg_confidence
        
        # AI ê¸°ë°˜ ë¶€ìœ„ë³„ ì ìˆ˜
        head_indices = [0, 15, 16, 17, 18]
        torso_indices = [1, 2, 5, 8, 9, 12]
        arm_indices = [2, 3, 4, 5, 6, 7]
        leg_indices = [9, 10, 11, 12, 13, 14]
        
        head_score = calculate_ai_body_part_score(head_indices)
        torso_score = calculate_ai_body_part_score(torso_indices)
        arms_score = calculate_ai_body_part_score(arm_indices)
        legs_score = calculate_ai_body_part_score(leg_indices)
        
        # AI ì‹ ë¢°ë„ ë°˜ì˜ ê°€ì¤‘ í‰ê· 
        ai_confidence = np.mean([kp[2] for kp in keypoints if len(kp) > 2]) if keypoints else 0.0
        if ai_enhanced:
            ai_confidence = min(ai_confidence * 1.15, 1.0)  # AI ë³´ì • ê³„ìˆ˜
        
        # AI ê¸°ë°˜ í¬ì¦ˆ ì ìˆ˜
        pose_score = (
            torso_score * weights.get('torso', 0.4) +
            arms_score * weights.get('arms', 0.3) +
            legs_score * weights.get('legs', 0.2) +
            weights.get('visibility', 0.1) * min(head_score, 1.0)
        ) * ai_confidence
        
        # AI ê¸°ë°˜ ì í•©ì„± íŒë‹¨
        min_score = 0.8 if strict_analysis else 0.7
        min_confidence = 0.75 if strict_analysis else 0.65
        
        if ai_enhanced:
            min_score *= 0.95  # AI ëª¨ë¸ ë³´ì •
            min_confidence *= 0.95
        
        suitable_for_fitting = (pose_score >= min_score and 
                              ai_confidence >= min_confidence)
        
        # AI ê¸°ë°˜ ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­
        issues = []
        recommendations = []
        
        if ai_confidence < min_confidence:
            issues.append(f'AI ëª¨ë¸ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({ai_confidence:.3f})')
            recommendations.append('ì¡°ëª…ì´ ì¢‹ì€ í™˜ê²½ì—ì„œ ë” ì„ ëª…í•˜ê²Œ ë‹¤ì‹œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        if torso_score < 0.5:
            issues.append(f'{clothing_type} ì°©ìš©ì— ì¤‘ìš”í•œ ìƒì²´ê°€ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤')
            recommendations.append('ìƒì²´ ì „ì²´ê°€ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        return {
            'suitable_for_fitting': suitable_for_fitting,
            'issues': issues,
            'recommendations': recommendations,
            'pose_score': pose_score,
            'ai_confidence': ai_confidence,
            'detailed_scores': {
                'head': head_score,
                'torso': torso_score,
                'arms': arms_score,
                'legs': legs_score
            },
            'clothing_type': clothing_type,
            'weights_used': weights,
            'ai_based_analysis': True,
            'ai_enhanced': ai_enhanced,
            'strict_analysis': strict_analysis,
            'opencv_disabled': True
        }
        
    except Exception as e:
        logger.error(f"AI ê¸°ë°˜ ì˜ë¥˜ë³„ í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            'suitable_for_fitting': False,
            'issues': ["AI ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨"],
            'recommendations': ["AI ëª¨ë¸ ìƒíƒœë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”"],
            'pose_score': 0.0,
            'ai_confidence': 0.0,
            'ai_based_analysis': True
        }

def process_image_with_ai_segmentation(
    image: Union[np.ndarray, Image.Image],
    use_sam: bool = True,
    device: str = "cpu"
) -> Dict[str, Any]:
    """AI ê¸°ë°˜ ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ (OpenCV ì™„ì „ ëŒ€ì²´)"""
    try:
        # AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ê¸° ìƒì„±
        seg_processor = AISegmentationProcessor(device)
        
        # AI ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰
        if use_sam:
            mask = seg_processor.segment_with_sam(image)
        else:
            mask = seg_processor.segment_with_u2net(image)
        
        # AI ê¸°ë°˜ ìœ¤ê³½ì„  ê²€ì¶œ
        contours = seg_processor.findContours(mask)
        
        return {
            'success': True,
            'mask': mask,
            'contours': contours,
            'segmentation_method': 'SAM' if use_sam else 'U2Net',
            'ai_based': True,
            'opencv_disabled': True
        }
        
    except Exception as e:
        logger.error(f"AI ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            'success': False,
            'error': str(e),
            'ai_based': True
        }

def ai_image_preprocessing(
    image: Union[np.ndarray, Image.Image, str],
    target_size: Tuple[int, int] = (512, 512),
    enhance: bool = True,
    device: str = "cpu"
) -> Optional[Image.Image]:
    """AI ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (OpenCV ì™„ì „ ëŒ€ì²´)"""
    try:
        # AI ì´ë¯¸ì§€ ì²˜ë¦¬ê¸° ìƒì„±
        img_processor = AIImageProcessor(device)
        
        # ì´ë¯¸ì§€ ë¡œë”©
        if isinstance(image, str):
            if os.path.exists(image):
                image = Image.open(image)
            else:
                # Base64 ë””ì½”ë”© ì‹œë„
                try:
                    image_data = base64.b64decode(image)
                    image = Image.open(io.BytesIO(image_data))
                except:
                    return None
        elif isinstance(image, np.ndarray):
            if image.size == 0:
                return None
            image = Image.fromarray(image)
        
        if not isinstance(image, Image.Image):
            return None
        
        # RGB ë³€í™˜
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # AI ê¸°ë°˜ ë¦¬ì‚¬ì´ì§•
        processed_image = img_processor.resize(image, target_size, 'bilinear')
        
        # AI ê¸°ë°˜ í–¥ìƒ (ì„ íƒì )
        if enhance:
            enhancer = ImageEnhance.Contrast(processed_image)
            processed_image = enhancer.enhance(1.1)
            
            enhancer = ImageEnhance.Sharpness(processed_image)
            processed_image = enhancer.enhance(1.05)
        
        return processed_image
        
    except Exception as e:
        logger.error(f"AI ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None

# =================================================================
# ğŸ”¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ (BaseStepMixin v16.0 í˜¸í™˜ + AI ê¸°ë°˜)
# =================================================================

async def test_ai_basestep_v16_pose_estimation():
    """BaseStepMixin v16.0 í˜¸í™˜ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ BaseStepMixin v16.0 í˜¸í™˜ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (OpenCV ì™„ì „ ëŒ€ì²´)")
        print("=" * 80)
        
        # AI ê¸°ë°˜ Step ìƒì„±
        step = await create_ai_pose_estimation_step(
            device="auto",
            strict_mode=True,
            config={
                'confidence_threshold': 0.5,
                'visualization_enabled': True,
                'cache_enabled': True,
                'detailed_analysis': True,
                'ai_models_only': True,
                'opencv_disabled': True,
                'basestep_version': '16.0-compatible'
            }
        )
        
        # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
        dummy_image = np.zeros((512, 512, 3), dtype=np.uint8)
        dummy_image_pil = Image.fromarray(dummy_image)
        
        print(f"ğŸ“‹ BaseStepMixin v16.0 í˜¸í™˜ AI Step ì •ë³´:")
        step_status = step.get_status()
        print(f"   ğŸ¯ Step: {step_status['step_name']}")
        print(f"   ğŸ”¢ ë²„ì „: {step_status['version']}")
        print(f"   ğŸ¤– í™œì„± AI ëª¨ë¸: {step_status.get('active_model', 'none')}")
        print(f"   ğŸ”’ Strict Mode: {step_status.get('strict_mode', False)}")
        print(f"   ğŸ’‰ ì˜ì¡´ì„± ì£¼ì…: {step_status.get('dependencies', {})}")
        print(f"   ğŸ’ ì´ˆê¸°í™” ìƒíƒœ: {step_status.get('is_initialized', False)}")
        print(f"   ğŸ§  ëª¨ë¸ ë¡œë“œ: {step_status.get('has_model', False)}")
        print(f"   ğŸ¤– AI ê¸°ë°˜: {step_status.get('ai_based', False)}")
        print(f"   ğŸš« OpenCV ë¹„í™œì„±í™”: {step_status.get('opencv_disabled', False)}")
        print(f"   ğŸ“¦ AI ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥: {step_status.get('ai_models_available', {})}")
        
        # AI ëª¨ë¸ë¡œ ì²˜ë¦¬
        result = await step.process(dummy_image_pil, clothing_type="shirt")
        
        if result['success']:
            print(f"âœ… BaseStepMixin v16.0 í˜¸í™˜ AI í¬ì¦ˆ ì¶”ì • ì„±ê³µ")
            print(f"ğŸ¯ AI í‚¤í¬ì¸íŠ¸ ìˆ˜: {len(result['keypoints'])}")
            print(f"ğŸ–ï¸ AI ì‹ ë¢°ë„: {result['pose_analysis']['ai_confidence']:.3f}")
            print(f"ğŸ’ í’ˆì§ˆ ì ìˆ˜: {result['pose_analysis']['quality_score']:.3f}")
            print(f"ğŸ‘• ì˜ë¥˜ ì í•©ì„±: {result['pose_analysis']['suitable_for_fitting']}")
            print(f"ğŸ¤– ì‚¬ìš©ëœ AI ëª¨ë¸: {result['model_used']}")
            print(f"âš¡ ì¶”ë¡  ì‹œê°„: {result.get('inference_time', 0):.3f}ì´ˆ")
            print(f"ğŸ”— BaseStepMixin ë²„ì „: {result['step_info']['basestep_version']}")
            print(f"ğŸ¤– AI ê¸°ë°˜: {result['step_info']['ai_based']}")
            print(f"ğŸš« OpenCV ë¹„í™œì„±í™”: {result['step_info']['opencv_disabled']}")
        else:
            print(f"âŒ BaseStepMixin v16.0 í˜¸í™˜ AI í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {result.get('error', 'Unknown Error')}")
        
        # ì •ë¦¬
        cleanup_result = await step.cleanup()
        print(f"ğŸ§¹ BaseStepMixin v16.0 í˜¸í™˜ AI ë¦¬ì†ŒìŠ¤ ì •ë¦¬: {cleanup_result['success']}")
        
    except Exception as e:
        print(f"âŒ BaseStepMixin v16.0 í˜¸í™˜ AI í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

async def test_ai_dependency_injection_v16():
    """BaseStepMixin v16.0 AI ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì… í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ¤– BaseStepMixin v16.0 AI ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì… í†µí•© í…ŒìŠ¤íŠ¸")
        print("=" * 80)
        
        # ë™ì  import í•¨ìˆ˜ë“¤ í…ŒìŠ¤íŠ¸
        base_step_class = get_base_step_mixin_class()
        model_loader = get_model_loader()
        memory_manager = get_memory_manager()
        step_factory = get_step_factory()
        
        print(f"âœ… BaseStepMixin v16.0 ë™ì  import: {base_step_class is not None}")
        print(f"âœ… ModelLoader ë™ì  import: {model_loader is not None}")
        print(f"âœ… MemoryManager ë™ì  import: {memory_manager is not None}")
        print(f"âœ… StepFactory ë™ì  import: {step_factory is not None}")
        
        # AI ê¸°ë°˜ Step ìƒì„± ë° ì˜ì¡´ì„± ì£¼ì… í™•ì¸
        step = PoseEstimationStep(device="auto", strict_mode=True)
        
        print(f"ğŸ”— ì˜ì¡´ì„± ìƒíƒœ: {step._get_dependency_status()}")
        print(f"ğŸ¤– AI ê¸°ë°˜: {hasattr(step, 'image_processor')}")
        print(f"ğŸš« OpenCV ë¹„í™œì„±í™”: {getattr(step.config, 'opencv_disabled', True)}")
        
        # ìˆ˜ë™ ì˜ì¡´ì„± ì£¼ì… í…ŒìŠ¤íŠ¸
        if model_loader:
            step.set_model_loader(model_loader)
            print("âœ… ModelLoader ìˆ˜ë™ ì£¼ì… ì™„ë£Œ")
        
        if memory_manager:
            step.set_memory_manager(memory_manager)
            print("âœ… MemoryManager ìˆ˜ë™ ì£¼ì… ì™„ë£Œ")
        
        # AI ê¸°ë°˜ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        init_result = await step.initialize()
        print(f"ğŸš€ AI ê¸°ë°˜ ì´ˆê¸°í™” ì„±ê³µ: {init_result}")
        
        if init_result:
            final_status = step.get_status()
            print(f"ğŸ¯ ìµœì¢… ìƒíƒœ: {final_status['version']}")
            print(f"ğŸ“¦ ì˜ì¡´ì„± ì™„ë£Œ: {final_status['dependencies']}")
            print(f"ğŸ¤– AI ê¸°ë°˜: {final_status['ai_based']}")
            print(f"ğŸš« OpenCV ë¹„í™œì„±í™”: {final_status['opencv_disabled']}")
        
        # ì •ë¦¬
        await step.cleanup()
        
    except Exception as e:
        print(f"âŒ BaseStepMixin v16.0 AI ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_ai_models():
    """AI ëª¨ë¸ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ (OpenCV ì™„ì „ ëŒ€ì²´)"""
    try:
        print("ğŸ§  AI ëª¨ë¸ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ (OpenCV ì™„ì „ ëŒ€ì²´)")
        print("=" * 60)
        
        # MediaPipe AI ëª¨ë¸ í…ŒìŠ¤íŠ¸
        try:
            mediapipe_model = MediaPipeAIPoseModel("cpu")
            print(f"âœ… MediaPipeAIPoseModel ìƒì„± ì„±ê³µ: {mediapipe_model}")
            
            # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
            dummy_image = Image.new('RGB', (256, 256), (128, 128, 128))
            result = mediapipe_model.predict(dummy_image)
            print(f"âœ… MediaPipe AI ì˜ˆì¸¡ ì„±ê³µ: {result['success']}, í‚¤í¬ì¸íŠ¸: {len(result['keypoints'])}")
        except Exception as e:
            print(f"âŒ MediaPipeAIPoseModel í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # YOLOv8 AI ëª¨ë¸ í…ŒìŠ¤íŠ¸
        try:
            yolo_model = YOLOv8AIPoseModel("cpu")
            print(f"âœ… YOLOv8AIPoseModel ìƒì„± ì„±ê³µ: {yolo_model}")
            
            # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
            dummy_image = Image.new('RGB', (256, 256), (128, 128, 128))
            result = yolo_model.predict(dummy_image)
            print(f"âœ… YOLOv8 AI ì˜ˆì¸¡ ì„±ê³µ: {result['success']}, í‚¤í¬ì¸íŠ¸: {len(result['keypoints'])}")
        except Exception as e:
            print(f"âŒ YOLOv8AIPoseModel í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # AI ì´ë¯¸ì§€ ì²˜ë¦¬ê¸° í…ŒìŠ¤íŠ¸
        try:
            img_processor = AIImageProcessor("cpu")
            print(f"âœ… AIImageProcessor ìƒì„± ì„±ê³µ: {img_processor}")
            
            # AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            dummy_image = Image.new('RGB', (256, 256), (128, 128, 128))
            resized = img_processor.resize(dummy_image, (512, 512))
            print(f"âœ… AI ë¦¬ì‚¬ì´ì§• ì„±ê³µ: {resized.size}")
            
            converted = img_processor.cvtColor(dummy_image, 'RGB2BGR')
            print(f"âœ… AI ìƒ‰ìƒ ë³€í™˜ ì„±ê³µ: {converted.mode}")
        except Exception as e:
            print(f"âŒ AIImageProcessor í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ê¸° í…ŒìŠ¤íŠ¸
        try:
            seg_processor = AISegmentationProcessor("cpu")
            print(f"âœ… AISegmentationProcessor ìƒì„± ì„±ê³µ: {seg_processor}")
            
            # AI ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ í…ŒìŠ¤íŠ¸
            dummy_image = Image.new('RGB', (256, 256), (128, 128, 128))
            mask = seg_processor.segment_with_u2net(dummy_image)
            print(f"âœ… AI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„±ê³µ: {mask.shape}")
            
            contours = seg_processor.findContours(mask)
            print(f"âœ… AI ìœ¤ê³½ì„  ê²€ì¶œ ì„±ê³µ: {len(contours)}ê°œ")
        except Exception as e:
            print(f"âŒ AISegmentationProcessor í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
    except Exception as e:
        print(f"âŒ AI ëª¨ë¸ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_ai_utilities():
    """AI ê¸°ë°˜ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ (OpenCV ì™„ì „ ëŒ€ì²´)"""
    try:
        print("ğŸ”„ AI ê¸°ë°˜ ìœ í‹¸ë¦¬í‹° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (OpenCV ì™„ì „ ëŒ€ì²´)")
        print("=" * 60)
        
        # ë”ë¯¸ OpenPose 18 í‚¤í¬ì¸íŠ¸
        openpose_keypoints = [
            [100, 50, 0.9],   # nose
            [100, 80, 0.8],   # neck
            [80, 100, 0.7],   # right_shoulder
            [70, 130, 0.6],   # right_elbow
            [60, 160, 0.5],   # right_wrist
            [120, 100, 0.7],  # left_shoulder
            [130, 130, 0.6],  # left_elbow
            [140, 160, 0.5],  # left_wrist
            [100, 200, 0.8],  # middle_hip
            [90, 200, 0.7],   # right_hip
            [85, 250, 0.6],   # right_knee
            [80, 300, 0.5],   # right_ankle
            [110, 200, 0.7],  # left_hip
            [115, 250, 0.6],  # left_knee
            [120, 300, 0.5],  # left_ankle
            [95, 40, 0.8],    # right_eye
            [105, 40, 0.8],   # left_eye
            [90, 45, 0.7],    # right_ear
            [110, 45, 0.7]    # left_ear
        ]
        
        # AI ê¸°ë°˜ ìœ íš¨ì„± ê²€ì¦
        is_valid = validate_ai_keypoints(openpose_keypoints)
        print(f"âœ… AI OpenPose 18 ìœ íš¨ì„±: {is_valid}")
        
        # AI ê¸°ë°˜ COCO 17ë¡œ ë³€í™˜
        coco_keypoints = convert_keypoints_to_coco_ai(openpose_keypoints)
        print(f"ğŸ”„ AI COCO 17 ë³€í™˜: {len(coco_keypoints)}ê°œ í‚¤í¬ì¸íŠ¸")
        
        # AI ê¸°ë°˜ ì˜ë¥˜ë³„ ë¶„ì„
        analysis = analyze_ai_pose_for_clothing(
            openpose_keypoints, 
            clothing_type="shirt",
            strict_analysis=True,
            ai_enhanced=True
        )
        print(f"ğŸ‘• AI ì˜ë¥˜ ì í•©ì„± ë¶„ì„:")
        print(f"   ì í•©ì„±: {analysis['suitable_for_fitting']}")
        print(f"   ì ìˆ˜: {analysis['pose_score']:.3f}")
        print(f"   AI ì‹ ë¢°ë„: {analysis['ai_confidence']:.3f}")
        print(f"   AI ê¸°ë°˜: {analysis['ai_based_analysis']}")
        print(f"   AI í–¥ìƒ: {analysis['ai_enhanced']}")
        print(f"   OpenCV ë¹„í™œì„±í™”: {analysis['opencv_disabled']}")
        
        # AI ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        dummy_image = Image.new('RGB', (256, 256), (128, 128, 128))
        processed = ai_image_preprocessing(
            dummy_image,
            target_size=(512, 512),
            enhance=True,
            device="cpu"
        )
        print(f"ğŸ–¼ï¸ AI ê¸°ë°˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬: {processed.size if processed else 'Failed'}")
        
        # AI ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ í…ŒìŠ¤íŠ¸
        seg_result = process_image_with_ai_segmentation(
            dummy_image,
            use_sam=False,  # U2Net ì‚¬ìš©
            device="cpu"
        )
        print(f"âœ‚ï¸ AI ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜: {seg_result['success']}")
        print(f"   ë°©ë²•: {seg_result.get('segmentation_method', 'Unknown')}")
        print(f"   AI ê¸°ë°˜: {seg_result.get('ai_based', False)}")
        print(f"   OpenCV ë¹„í™œì„±í™”: {seg_result.get('opencv_disabled', False)}")
        
    except Exception as e:
        print(f"âŒ AI ê¸°ë°˜ ìœ í‹¸ë¦¬í‹° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# =================================================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ (BaseStepMixin v16.0 í˜¸í™˜ + AI ê¸°ë°˜)
# =================================================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤ (AI ê¸°ë°˜)
    'PoseEstimationStep',
    'MediaPipeAIPoseModel',
    'YOLOv8AIPoseModel',
    'AIImageProcessor',
    'AISegmentationProcessor',
    'PoseMetrics',
    'PoseModel',
    'PoseQuality', 
    'PoseType',
    
    # ìƒì„± í•¨ìˆ˜ë“¤ (BaseStepMixin v16.0 í˜¸í™˜ + AI ê¸°ë°˜)
    'create_ai_pose_estimation_step',
    'create_ai_pose_estimation_step_sync',
    
    # ë™ì  import í•¨ìˆ˜ë“¤
    'get_base_step_mixin_class',
    'get_model_loader',
    'get_memory_manager',
    'get_step_factory',
    
    # AI ê¸°ë°˜ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (OpenCV ì™„ì „ ëŒ€ì²´)
    'validate_ai_keypoints',
    'convert_keypoints_to_coco_ai',
    'draw_ai_pose_on_image',
    'analyze_ai_pose_for_clothing',
    'process_image_with_ai_segmentation',
    'ai_image_preprocessing',
    
    # ìƒìˆ˜ë“¤
    'OPENPOSE_18_KEYPOINTS',
    'KEYPOINT_COLORS',
    'SKELETON_CONNECTIONS',
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ (BaseStepMixin v16.0 í˜¸í™˜ + AI ê¸°ë°˜)
    'test_ai_basestep_v16_pose_estimation',
    'test_ai_dependency_injection_v16',
    'test_ai_models',
    'test_ai_utilities'
]

# =================================================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê·¸ (BaseStepMixin v16.0 í˜¸í™˜ + AI ê¸°ë°˜)
# =================================================================

logger.info("ğŸ”¥ BaseStepMixin v16.0 í˜¸í™˜ AI ê¸°ë°˜ PoseEstimationStep v11.0 ë¡œë“œ ì™„ë£Œ (OpenCV ì™„ì „ ëŒ€ì²´)")
logger.info("âœ… BaseStepMixin v16.0 UnifiedDependencyManager ì™„ì „ í˜¸í™˜")
logger.info("âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
logger.info("âœ… ë‹¤ë¥¸ Stepë“¤ê³¼ ë™ì¼í•œ íŒ¨í„´ ì ìš©")
logger.info("âœ… StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… ì™„ì„±")
logger.info("ğŸš« OpenCV ì™„ì „ ì œê±° â†’ AI ëª¨ë¸ ê¸°ë°˜ ì²˜ë¦¬")
logger.info("ğŸ¤– MediaPipe, YOLOv8, SAM, U2Net AI í™œìš©")
logger.info("ğŸ–¼ï¸ AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ (CLIP, PIL, PyTorch)")
logger.info("âœ‚ï¸ AI ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ (SAM, U2Net)")
logger.info("ğŸ”— BaseStepMixin v16.0 ì™„ì „ ìƒì† - ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì™„ë²½ êµ¬í˜„")
logger.info("ğŸ’‰ ModelLoader ì™„ì „ ì—°ë™ - ìˆœí™˜ì°¸ì¡° ì—†ëŠ” í•œë°©í–¥ ì°¸ì¡°")
logger.info("ğŸ¯ 18ê°œ í‚¤í¬ì¸íŠ¸ OpenPose í‘œì¤€ + COCO 17 ë³€í™˜ ì§€ì›")
logger.info("ğŸ”’ Strict Mode ì§€ì› - ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì—ëŸ¬")
logger.info("ğŸ”¬ ì™„ì „í•œ AI ê¸°ë°˜ ë¶„ì„ - ê°ë„, ë¹„ìœ¨, ëŒ€ì¹­ì„±, ê°€ì‹œì„±, í’ˆì§ˆ í‰ê°€")
logger.info("ğŸ M3 Max 128GB ìµœì í™” + conda í™˜ê²½ ìš°ì„ ")
logger.info("ğŸš€ í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± + AI ëª¨ë¸ ê¸°ë°˜")

# ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê¹…
logger.info(f"ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ: PyTorch={TORCH_AVAILABLE}, PIL={PIL_AVAILABLE}, OpenCV=ë¹„í™œì„±í™”")
logger.info(f"ğŸ¤– AI ë¼ì´ë¸ŒëŸ¬ë¦¬: MediaPipe={MEDIAPIPE_AVAILABLE}, YOLOv8={ULTRALYTICS_AVAILABLE}, Transformers={TRANSFORMERS_AVAILABLE}")
logger.info(f"ğŸ”§ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „: PyTorch={TORCH_VERSION}, PIL={PIL_VERSION}")
logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§: {'í™œì„±í™”' if PSUTIL_AVAILABLE else 'ë¹„í™œì„±í™”'}")
logger.info(f"ğŸ”— BaseStepMixin v16.0 í˜¸í™˜: ì™„ì „í•œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì ìš©")
logger.info(f"ğŸ¤– AI ê¸°ë°˜ ì—°ì‚°: MediaPipe, YOLOv8, SAM, U2Net ì¶”ë¡  ì—”ì§„")
logger.info(f"ğŸš« OpenCV ì™„ì „ ëŒ€ì²´: AI ëª¨ë¸ ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì„¸ê·¸ë©˜í…Œì´ì…˜")

# =================================================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ë¶€ (BaseStepMixin v16.0 í˜¸í™˜ + AI ê¸°ë°˜ ê²€ì¦)
# =================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¯ MyCloset AI Step 02 - BaseStepMixin v16.0 í˜¸í™˜ + AI ê¸°ë°˜ (OpenCV ì™„ì „ ëŒ€ì²´)")
    print("=" * 80)
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    async def run_all_ai_tests():
        await test_ai_basestep_v16_pose_estimation()
        print("\n" + "=" * 80)
        await test_ai_dependency_injection_v16()
        print("\n" + "=" * 80)
        test_ai_models()
        print("\n" + "=" * 80)
        test_ai_utilities()
    
    try:
        asyncio.run(run_all_ai_tests())
    except Exception as e:
        print(f"âŒ BaseStepMixin v16.0 í˜¸í™˜ AI ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 80)
    print("âœ¨ BaseStepMixin v16.0 í˜¸í™˜ + AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ (OpenCV ì™„ì „ ëŒ€ì²´)")
    print("ğŸ”— BaseStepMixin v16.0 UnifiedDependencyManager ì™„ì „ í˜¸í™˜")
    print("ğŸ¤– TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
    print("ğŸ”— StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… ì™„ì„±")
    print("ğŸš« OpenCV ì™„ì „ ì œê±° â†’ AI ëª¨ë¸ ê¸°ë°˜ ì²˜ë¦¬")
    print("âš¡ MediaPipe, YOLOv8, SAM, U2Net AI ì¶”ë¡  ì—”ì§„")
    print("ğŸ–¼ï¸ AI ê¸°ë°˜ ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì„¸ê·¸ë©˜í…Œì´ì…˜")
    print("ğŸ’‰ ì™„ë²½í•œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´")
    print("ğŸ”’ Strict Mode + ì™„ì „í•œ AI ê¸°ë°˜ ë¶„ì„ ê¸°ëŠ¥")
    print("ğŸ¯ AI ì—°ì‚° + ì§„ì§œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ (OpenCV ì—†ì´)")
    print("=" * 80)