#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 02: ì™„ì „ í†µí•©ëœ AI í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ - BaseStepMixin ì˜ì¡´ì„± ì£¼ì… ê¸°ë°˜ v8.0
================================================================================

âœ… 1ë²ˆ+2ë²ˆ íŒŒì¼ ì™„ì „ í†µí•© - ëª¨ë“  ê¸°ëŠ¥ í†µí•©
âœ… BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ - _run_ai_inference() ë™ê¸° ë©”ì„œë“œ êµ¬í˜„
âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  - HRNet + OpenPose + YOLO + Diffusion + MediaPipe + AlphaPose
âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
âœ… ì™„ì „í•œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ (ModelLoader, MemoryManager, DataConverter)
âœ… ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ â†’ ì¶”ë¡  ì—”ì§„
âœ… ì„œë¸Œí”½ì…€ ì •í™•ë„ + ê´€ì ˆê°ë„ ê³„ì‚° + ì‹ ì²´ë¹„ìœ¨ ë¶„ì„
âœ… PAF (Part Affinity Fields) + íˆíŠ¸ë§µ ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
âœ… ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” + ì‹ ë¢°ë„ ìœµí•© ì‹œìŠ¤í…œ
âœ… ë¶ˆí™•ì‹¤ì„± ì¶”ì • + ìƒì²´ì—­í•™ì  íƒ€ë‹¹ì„± í‰ê°€
âœ… ë¶€ìƒ ìœ„í—˜ë„ í‰ê°€ + ê³ ê¸‰ í¬ì¦ˆ ë¶„ì„
âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”
âœ… ì‹¤ì‹œê°„ MPS/CUDA ê°€ì†

í•µì‹¬ AI ì•Œê³ ë¦¬ì¦˜:
1. ğŸ§  HRNet High-Resolution Network (ê³ í•´ìƒë„ ìœ ì§€)
2. ğŸ•¸ï¸ OpenPose PAF + íˆíŠ¸ë§µ (CMU ì•Œê³ ë¦¬ì¦˜)  
3. âš¡ YOLOv8-Pose ì‹¤ì‹œê°„ ê²€ì¶œ
4. ğŸ¨ Diffusion í’ˆì§ˆ í–¥ìƒ
5. ğŸ” AlphaPose ë‹¤ì¤‘ ì¸ë¬¼ ê²€ì¶œ
6. ğŸ“± MediaPipe ì‹¤ì‹œê°„ ì²˜ë¦¬
7. ğŸ§® PoseNet Lite ê²½ëŸ‰í™”
8. ğŸ“ ê´€ì ˆê°ë„ + ì‹ ì²´ë¹„ìœ¨ ê³„ì‚°
9. ğŸ¯ ì„œë¸Œí”½ì…€ ì •í™•ë„ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
10. ğŸ”€ ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” + ì‹ ë¢°ë„ ìœµí•©

íŒŒì¼ ìœ„ì¹˜: backend/app/ai_pipeline/steps/step_02_pose_estimation.py
ì‘ì„±ì: MyCloset AI Team  
ë‚ ì§œ: 2025-07-28
ë²„ì „: v8.0 (Complete Unified AI System with Full DI)
"""

# ==============================================
# ğŸ”¥ 1. Import ì„¹ì…˜ (TYPE_CHECKING íŒ¨í„´)
# ==============================================

import os
import sys
import gc
import time
import asyncio
import logging
import traceback
import hashlib
import json
import base64
import weakref
import math
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from contextlib import asynccontextmanager

# TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from ..factories.step_factory import StepFactory
    from ..steps.base_step_mixin import BaseStepMixin

logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 2. conda í™˜ê²½ ë° ì‹œìŠ¤í…œ ì²´í¬
# ==============================================

CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'),
    'python_path': os.path.dirname(os.__file__)
}

def detect_m3_max() -> bool:
    """M3 Max ê°ì§€"""
    try:
        import platform
        import subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    import torchvision.transforms as transforms
    from torchvision.transforms.functional import resize, to_pil_image, to_tensor
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if torch.backends.mps.is_available() and IS_M3_MAX:
        DEVICE = "mps"
        torch.mps.set_per_process_memory_fraction(0.7)
    elif torch.cuda.is_available():
        DEVICE = "cuda"
    else:
        DEVICE = "cpu"
        
except ImportError as e:
    raise ImportError(f"âŒ PyTorch í•„ìˆ˜: {e}")

try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError as e:
    raise ImportError(f"âŒ Pillow í•„ìˆ˜: {e}")

# ì„ íƒì  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

try:
    from transformers import AutoModel, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import safetensors.torch as st
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False

try:
    from scipy.optimize import curve_fit
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# ==============================================
# ğŸ”¥ 3. BaseStepMixin ë™ì  import (ìˆœí™˜ì°¸ì¡° ë°©ì§€)
# ==============================================

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError:
        logger.error("âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨")
        return None

BaseStepMixin = get_base_step_mixin_class()

# ìˆ˜ì •ëœ í´ë°± í´ë˜ìŠ¤
if BaseStepMixin is None:
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'BaseStep')
            self.step_id = kwargs.get('step_id', 0)
            self.device = kwargs.get('device', DEVICE)
            self.is_initialized = False
            self.is_ready = False
            self.performance_metrics = {'process_count': 0}
            
            # ì˜ì¡´ì„± ì£¼ì… ê´€ë ¨
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
        async def initialize(self):
            self.is_initialized = True
            self.is_ready = True
            return True
        
        def set_model_loader(self, model_loader):
            self.model_loader = model_loader
        
        def set_memory_manager(self, memory_manager):
            self.memory_manager = memory_manager
            
        def set_data_converter(self, data_converter):
            self.data_converter = data_converter
            
        def set_di_container(self, di_container):
            self.di_container = di_container
        
        async def cleanup(self):
            pass
        
        def get_status(self):
            return {
                'step_name': self.step_name, 
                'step_id': self.step_id,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready
            }
        
        # BaseStepMixin v19.1 í˜¸í™˜ì„±ì„ ìœ„í•œ ì¶”ê°€ ë©”ì„œë“œë“¤
        def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
            """í´ë°±ìš© AI ì¶”ë¡  ë©”ì„œë“œ"""
            return {
                'success': False,
                'error': 'BaseStepMixin í´ë°± ëª¨ë“œ - ì‹¤ì œ êµ¬í˜„ í•„ìš”',
                'keypoints': [],
                'confidence_scores': []
            }
            
        async def process(self, **kwargs) -> Dict[str, Any]:
            """í´ë°±ìš© process ë©”ì„œë“œ"""
            return await self._run_ai_inference(kwargs)
# ==============================================
# ğŸ”¥ 4. í¬ì¦ˆ ì¶”ì • ìƒìˆ˜ ë° ë°ì´í„° êµ¬ì¡°
# ==============================================

class PoseModel(Enum):
    """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ íƒ€ì…"""
    HRNET = "hrnet"
    OPENPOSE = "openpose"
    YOLOV8_POSE = "yolov8_pose"
    DIFFUSION_POSE = "diffusion_pose"
    ALPHAPOSE = "alphapose"
    MEDIAPIPE = "mediapipe"
    POSENET = "posenet"

class PoseQuality(Enum):
    """í¬ì¦ˆ í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"     # 90-100ì 
    GOOD = "good"              # 75-89ì   
    ACCEPTABLE = "acceptable"   # 60-74ì 
    POOR = "poor"              # 40-59ì 
    VERY_POOR = "very_poor"    # 0-39ì 

# OpenPose 18 í‚¤í¬ì¸íŠ¸ ì •ì˜ (CMU í‘œì¤€)
OPENPOSE_18_KEYPOINTS = [
    "nose", "neck", "right_shoulder", "right_elbow", "right_wrist",
    "left_shoulder", "left_elbow", "left_wrist", "middle_hip", "right_hip", 
    "right_knee", "right_ankle", "left_hip", "left_knee", "left_ankle",
    "right_eye", "left_eye", "right_ear", "left_ear"
]

# í‚¤í¬ì¸íŠ¸ ì—°ê²° êµ¬ì¡° (ìŠ¤ì¼ˆë ˆí†¤)
SKELETON_CONNECTIONS = [
    (0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7), (1, 8),
    (8, 9), (9, 10), (10, 11), (8, 12), (12, 13), (13, 14), (0, 15),
    (15, 17), (0, 16), (16, 18)
]

# í‚¤í¬ì¸íŠ¸ ìƒ‰ìƒ ë§¤í•‘
KEYPOINT_COLORS = [
    (255, 0, 0), (255, 85, 0), (255, 170, 0), (255, 255, 0), (170, 255, 0),
    (85, 255, 0), (0, 255, 0), (0, 255, 85), (0, 255, 170), (0, 255, 255),
    (0, 170, 255), (0, 85, 255), (0, 0, 255), (85, 0, 255), (170, 0, 255),
    (255, 0, 255), (255, 0, 170), (255, 0, 85), (255, 0, 0)
]

@dataclass
class PoseResult:
    """í¬ì¦ˆ ì¶”ì • ê²°ê³¼"""
    keypoints: List[List[float]] = field(default_factory=list)
    confidence_scores: List[float] = field(default_factory=list)
    joint_angles: Dict[str, float] = field(default_factory=dict)
    body_proportions: Dict[str, float] = field(default_factory=dict)
    pose_quality: PoseQuality = PoseQuality.POOR
    overall_confidence: float = 0.0
    processing_time: float = 0.0
    model_used: str = ""
    subpixel_accuracy: bool = False
    
    # ê³ ê¸‰ ë¶„ì„ ê²°ê³¼
    keypoints_with_uncertainty: List[Dict[str, Any]] = field(default_factory=list)
    advanced_body_metrics: Dict[str, Any] = field(default_factory=dict)
    injury_risk_assessment: Dict[str, Any] = field(default_factory=dict)
    skeleton_structure: Dict[str, Any] = field(default_factory=dict)
    ensemble_info: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# ğŸ”¥ 5. ì‹¤ì œ AI ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ ì‹œìŠ¤í…œ (2ë²ˆ íŒŒì¼ í˜¸í™˜)
# ==============================================

class SmartModelPathMapper:
    """ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ìë™ íƒì§€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, ai_models_root: str = "ai_models"):
        self.ai_models_root = Path(ai_models_root)
        self.logger = logging.getLogger(f"{__name__}.SmartModelPathMapper")
        self._cache = {}
    
    def find_model_files(self) -> Dict[str, Optional[Path]]:
        """ì‹¤ì œ ëª¨ë¸ íŒŒì¼ë“¤ ìë™ íƒì§€"""
        if self._cache:
            return self._cache
            
        model_files = {
            "hrnet": None,
            "openpose": None, 
            "yolov8": None,
            "diffusion": None,
            "alphapose": None,
            "posenet": None,
            "body_pose": None  # 2ë²ˆ íŒŒì¼ í˜¸í™˜
        }
        
        # HRNet ëª¨ë¸ íƒì§€
        hrnet_patterns = [
            "step_02_pose_estimation/hrnet_w48_coco_256x192.pth",
            "step_02_pose_estimation/hrnet_w32_coco_256x192.pth",
            "checkpoints/hrnet/hrnet_w48_256x192.pth",
            "pose_estimation/hrnet_w48.pth"
        ]
        
        for pattern in hrnet_patterns:
            path = self.ai_models_root / pattern
            if path.exists():
                model_files["hrnet"] = path
                self.logger.info(f"âœ… HRNet ëª¨ë¸ ë°œê²¬: {path}")
                break
        
        # OpenPose ëª¨ë¸ íƒì§€
        openpose_patterns = [
            "step_02_pose_estimation/openpose.pth",
            "step_02_pose_estimation/body_pose_model.pth",
            "openpose.pth",
            "pose_estimation/openpose_pose.pth"
        ]
        
        for pattern in openpose_patterns:
            path = self.ai_models_root / pattern
            if path.exists():
                model_files["openpose"] = path
                self.logger.info(f"âœ… OpenPose ëª¨ë¸ ë°œê²¬: {path}")
                break
        
        # YOLOv8 ëª¨ë¸ íƒì§€
        yolo_patterns = [
            "step_02_pose_estimation/yolov8n-pose.pt",
            "step_02_pose_estimation/yolov8s-pose.pt",
            "step_02_pose_estimation/yolov8m-pose.pt"
        ]
        
        for pattern in yolo_patterns:
            path = self.ai_models_root / pattern
            if path.exists():
                model_files["yolo"] = path
                self.logger.info(f"âœ… YOLOv8 ëª¨ë¸ ë°œê²¬: {path}")
                break
        
        # Diffusion ëª¨ë¸ íƒì§€
        diffusion_patterns = [
            "step_02_pose_estimation/diffusion_pytorch_model.safetensors",
            "step_02_pose_estimation/diffusion_pytorch_model.bin",
            "step_02_pose_estimation/diffusion_pytorch_model.fp16.safetensors",
            "step_02_pose_estimation/ultra_models/diffusion_pytorch_model.safetensors"
        ]
        
        for pattern in diffusion_patterns:
            path = self.ai_models_root / pattern
            if path.exists():
                model_files["diffusion"] = path
                self.logger.info(f"âœ… Diffusion ëª¨ë¸ ë°œê²¬: {path}")
                break
        
        # Body Pose ëª¨ë¸ íƒì§€ (2ë²ˆ íŒŒì¼ í˜¸í™˜)
        body_pose_patterns = [
            "step_02_pose_estimation/body_pose_model.pth",
            "body_pose_model.pth"
        ]
        
        for pattern in body_pose_patterns:
            path = self.ai_models_root / pattern
            if path.exists():
                model_files["body_pose"] = path
                self.logger.info(f"âœ… Body Pose ëª¨ë¸ ë°œê²¬: {path}")
                break
        
        self._cache = model_files
        return model_files

class Step02ModelMapper(SmartModelPathMapper):
    """Step 02 Pose Estimation ì „ìš© ë™ì  ê²½ë¡œ ë§¤í•‘ (2ë²ˆ íŒŒì¼ í˜¸í™˜)"""
    
    def __init__(self):
        super().__init__()
        self.logger = logging.getLogger(f"{__name__}.Step02ModelMapper")
        self.base_path = Path("ai_models")
    
    def _search_models(self, model_files: Dict[str, List[str]], 
                      search_priority: List[str]) -> Dict[str, Optional[Path]]:
        """ëª¨ë¸ íŒŒì¼ ê²€ìƒ‰ ë©”ì„œë“œ (ëˆ„ë½ëœ ë©”ì„œë“œ ì¶”ê°€)"""
        found_models = {}
        
        try:
            for model_type, file_patterns in model_files.items():
                found_models[model_type] = None
                
                # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ê²€ìƒ‰
                for search_path in search_priority:
                    if found_models[model_type] is not None:
                        break
                        
                    full_search_path = self.base_path / search_path
                    
                    # ê° íŒŒì¼ íŒ¨í„´ì— ëŒ€í•´ ê²€ìƒ‰
                    for pattern in file_patterns:
                        # ì§ì ‘ ê²½ë¡œ í™•ì¸
                        direct_path = full_search_path / pattern
                        if direct_path.exists():
                            found_models[model_type] = direct_path
                            self.logger.info(f"âœ… {model_type} ëª¨ë¸ ë°œê²¬: {direct_path}")
                            break
                        
                        # ì¬ê·€ ê²€ìƒ‰
                        if full_search_path.exists():
                            for found_file in full_search_path.rglob(pattern):
                                if found_file.is_file() and found_file.stat().st_size > 1024:  # 1KB ì´ìƒ
                                    found_models[model_type] = found_file
                                    self.logger.info(f"âœ… {model_type} ëª¨ë¸ ë°œê²¬ (ì¬ê·€): {found_file}")
                                    break
                        
                        if found_models[model_type] is not None:
                            break
                
                if found_models[model_type] is None:
                    self.logger.warning(f"âš ï¸ {model_type} ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            self.logger.info(f"ğŸ“Š ëª¨ë¸ ê²€ìƒ‰ ì™„ë£Œ: {sum(1 for v in found_models.values() if v is not None)}/{len(found_models)} ê°œ ë°œê²¬")
            return found_models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # ë¹ˆ ê²°ê³¼ ë°˜í™˜ (Noneìœ¼ë¡œ ì´ˆê¸°í™”ëœ ë”•ì…”ë„ˆë¦¬)
            return {model_type: None for model_type in model_files.keys()}
    
    def get_step02_model_paths(self) -> Dict[str, Optional[Path]]:
        """Step 02 ëª¨ë¸ ê²½ë¡œ ìë™ íƒì§€ - 2ë²ˆ íŒŒì¼ í˜¸í™˜"""
        model_files = {
            "yolov8": ["yolov8n-pose.pt", "yolov8s-pose.pt"],
            "openpose": ["openpose.pth", "body_pose_model.pth"],
            "hrnet": [
                "hrnet_w48_coco_256x192.pth", 
                "hrnet_w32_coco_256x192.pth", 
                "pose_hrnet_w48_256x192.pth",
                "hrnet_w48_256x192.pth"
            ],
            "diffusion": ["diffusion_pytorch_model.safetensors", "diffusion_pytorch_model.bin"],
            "body_pose": ["body_pose_model.pth"]
        }
        
        search_priority = [
            "step_02_pose_estimation/",
            "step_02_pose_estimation/ultra_models/",
            "step_06_virtual_fitting/ootdiffusion/checkpoints/openpose/",
            "checkpoints/step_02_pose_estimation/",
            "pose_estimation/",
            "hrnet/",
            "checkpoints/hrnet/",
            ""  # ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë„ ê²€ìƒ‰
        ]
        
        return self._search_models(model_files, search_priority)
    
    def get_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        try:
            found_models = self.get_step02_model_paths()
            available = [model_type for model_type, path in found_models.items() if path is not None]
            return available
        except Exception as e:
            self.logger.error(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_model_info(self, model_type: str) -> Dict[str, Any]:
        """íŠ¹ì • ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        try:
            found_models = self.get_step02_model_paths()
            model_path = found_models.get(model_type)
            
            if model_path and model_path.exists():
                stat = model_path.stat()
                return {
                    'model_type': model_type,
                    'path': str(model_path),
                    'size_mb': stat.st_size / (1024 * 1024),
                    'exists': True,
                    'modified': stat.st_mtime
                }
            else:
                return {
                    'model_type': model_type,
                    'path': None,
                    'size_mb': 0,
                    'exists': False,
                    'modified': None
                }
        except Exception as e:
            self.logger.error(f"âŒ {model_type} ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'model_type': model_type,
                'path': None,
                'size_mb': 0,
                'exists': False,
                'error': str(e)
            }


# ==============================================
# ğŸ”¥ 6. HRNet ê³ í•´ìƒë„ ë„¤íŠ¸ì›Œí¬ (ì™„ì „ êµ¬í˜„)
# ==============================================

class BasicBlock(nn.Module):
    """HRNet BasicBlock"""
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, momentum=0.1)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes, momentum=0.1)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out

class Bottleneck(nn.Module):
    """HRNet Bottleneck"""
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, momentum=0.1)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes, momentum=0.1)
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion, momentum=0.1)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out

class HRNetModel(nn.Module):
    """ì™„ì „í•œ HRNet êµ¬í˜„ - ê³ í•´ìƒë„ í¬ì¦ˆ ì¶”ì •"""
    
    def __init__(self, num_joints=18, width=48):
        super(HRNetModel, self).__init__()
        
        self.num_joints = num_joints
        self.width = width
        
        # Stem network
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64, momentum=0.1)
        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(64, momentum=0.1)
        self.relu = nn.ReLU(inplace=True)
        
        # Stage 1: Resolution 1/4
        self.layer1 = self._make_layer(Bottleneck, 64, 64, 4)
        
        # Transition 1
        self.transition1 = self._make_transition_layer([256], [width, width*2])
        
        # Stage 2: Resolution 1/4, 1/8
        self.stage2, pre_stage_channels = self._make_stage(
            BasicBlock, num_modules=1, num_branches=2, 
            num_blocks=[4, 4], num_inchannels=[width, width*2],
            num_channels=[width, width*2]
        )
        
        # Final layer (í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ ìƒì„±)
        self.final_layer = nn.Conv2d(
            in_channels=width,
            out_channels=num_joints,
            kernel_size=1,
            stride=1,
            padding=0
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._init_weights()
    
    def _make_layer(self, block, inplanes, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion, momentum=0.1),
            )

        layers = []
        layers.append(block(inplanes, planes, stride, downsample))
        inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(inplanes, planes))

        return nn.Sequential(*layers)
    
    def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer):
        num_branches_cur = len(num_channels_cur_layer)
        num_branches_pre = len(num_channels_pre_layer)

        transition_layers = []
        for i in range(num_branches_cur):
            if i < num_branches_pre:
                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:
                    transition_layers.append(nn.Sequential(
                        nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, 1, 1, bias=False),
                        nn.BatchNorm2d(num_channels_cur_layer[i], momentum=0.1),
                        nn.ReLU(inplace=True)))
                else:
                    transition_layers.append(None)
            else:
                conv3x3s = []
                for j in range(i+1-num_branches_pre):
                    inchannels = num_channels_pre_layer[-1]
                    outchannels = num_channels_cur_layer[i] if j == i-num_branches_pre else inchannels
                    conv3x3s.append(nn.Sequential(
                        nn.Conv2d(inchannels, outchannels, 3, 2, 1, bias=False),
                        nn.BatchNorm2d(outchannels, momentum=0.1),
                        nn.ReLU(inplace=True)))
                transition_layers.append(nn.Sequential(*conv3x3s))

        return nn.ModuleList(transition_layers)
    
    def _make_stage(self, block, num_modules, num_branches, num_blocks, num_inchannels, num_channels):
        modules = []
        for i in range(num_modules):
            modules.append(
                HighResolutionModule(num_branches, block, num_blocks, num_inchannels, num_channels)
            )
            num_inchannels = modules[-1].get_num_inchannels()

        return nn.Sequential(*modules), num_inchannels
    
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # Stem
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        
        # Stage 1
        x = self.layer1(x)
        
        # Transition 1
        x_list = []
        for i in range(2):
            if self.transition1[i] is not None:
                x_list.append(self.transition1[i](x))
            else:
                x_list.append(x)
        
        # Stage 2
        y_list = self.stage2(x_list)
        
        # Final layer (highest resolutionë§Œ ì‚¬ìš©)
        x = self.final_layer(y_list[0])
        
        return x

class HighResolutionModule(nn.Module):
    """HRNetì˜ ê³ í•´ìƒë„ ëª¨ë“ˆ"""
    
    def __init__(self, num_branches, block, num_blocks, num_inchannels, num_channels):
        super(HighResolutionModule, self).__init__()
        self.num_inchannels = num_inchannels
        self.num_branches = num_branches

        self.branches = self._make_branches(num_branches, block, num_blocks, num_channels)
        self.fuse_layers = self._make_fuse_layers()
        self.relu = nn.ReLU(inplace=True)

    def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1):
        downsample = None
        if stride != 1 or self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.num_inchannels[branch_index], num_channels[branch_index] * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(num_channels[branch_index] * block.expansion, momentum=0.1),
            )

        layers = []
        layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index], stride, downsample))
        self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion
        for i in range(1, num_blocks[branch_index]):
            layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index]))

        return nn.Sequential(*layers)

    def _make_branches(self, num_branches, block, num_blocks, num_channels):
        branches = []
        for i in range(num_branches):
            branches.append(self._make_one_branch(i, block, num_blocks, num_channels))
        return nn.ModuleList(branches)

    def _make_fuse_layers(self):
        if self.num_branches == 1:
            return None

        num_branches = self.num_branches
        num_inchannels = self.num_inchannels
        fuse_layers = []
        for i in range(num_branches):
            fuse_layer = []
            for j in range(num_branches):
                if j > i:
                    fuse_layer.append(nn.Sequential(
                        nn.Conv2d(num_inchannels[j], num_inchannels[i], 1, 1, 0, bias=False),
                        nn.BatchNorm2d(num_inchannels[i], momentum=0.1)))
                elif j == i:
                    fuse_layer.append(None)
                else:
                    conv3x3s = []
                    for k in range(i-j):
                        if k == i - j - 1:
                            num_outchannels_conv3x3 = num_inchannels[i]
                            conv3x3s.append(nn.Sequential(
                                nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),
                                nn.BatchNorm2d(num_outchannels_conv3x3, momentum=0.1)))
                        else:
                            num_outchannels_conv3x3 = num_inchannels[j]
                            conv3x3s.append(nn.Sequential(
                                nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),
                                nn.BatchNorm2d(num_outchannels_conv3x3, momentum=0.1),
                                nn.ReLU(inplace=True)))
                    fuse_layer.append(nn.Sequential(*conv3x3s))
            fuse_layers.append(nn.ModuleList(fuse_layer))

        return nn.ModuleList(fuse_layers)

    def get_num_inchannels(self):
        return self.num_inchannels

    def forward(self, x):
        if self.num_branches == 1:
            return [self.branches[0](x[0])]

        for i in range(self.num_branches):
            x[i] = self.branches[i](x[i])

        x_fuse = []
        for i in range(len(self.fuse_layers)):
            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])
            for j in range(1, self.num_branches):
                if i == j:
                    y = y + x[j]
                elif j > i:
                    width_output = x[i].shape[-1]
                    height_output = x[i].shape[-2]
                    y = y + F.interpolate(
                        self.fuse_layers[i][j](x[j]),
                        size=[height_output, width_output],
                        mode='bilinear', align_corners=False)
                else:
                    y = y + self.fuse_layers[i][j](x[j])
            x_fuse.append(self.relu(y))

        return x_fuse

# ==============================================
# ğŸ”¥ 7. OpenPose PAF + íˆíŠ¸ë§µ ë„¤íŠ¸ì›Œí¬
# ==============================================

class OpenPoseModel(nn.Module):
    """ì™„ì „í•œ OpenPose êµ¬í˜„ - PAF + íˆíŠ¸ë§µ"""
    
    def __init__(self, num_keypoints=18):
        super(OpenPoseModel, self).__init__()
        
        self.num_keypoints = num_keypoints
        
        # VGG19 ë°±ë³¸ (OpenPose ë…¼ë¬¸ ê¸°ì¤€)
        self.backbone = self._make_vgg19_backbone()
        
        # Stage 1
        self.stage1_paf = self._make_stage(128, 38)  # PAF (19 connections * 2)
        self.stage1_heatmap = self._make_stage(128, 19)  # Heatmaps (18 + 1 background)
        
        # Stage 2
        self.stage2_paf = self._make_stage(128 + 38 + 19, 38)
        self.stage2_heatmap = self._make_stage(128 + 38 + 19, 19)
        
    def _make_vgg19_backbone(self):
        """VGG19 ë°±ë³¸ ë„¤íŠ¸ì›Œí¬"""
        layers = []
        in_channels = 3
        
        # VGG19 configuration (simplified)
        cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 
               512, 512, 512, 512]
        
        for v in cfg:
            if v == 'M':
                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)
                layers += [conv2d, nn.ReLU(inplace=True)]
                in_channels = v
        
        # OpenPose specific layers
        layers += [
            nn.Conv2d(512, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True)
        ]
        
        return nn.Sequential(*layers)
    
    def _make_stage(self, in_channels, out_channels):
        """OpenPose stage ìƒì„±"""
        return nn.Sequential(
            nn.Conv2d(in_channels, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 512, kernel_size=1, padding=0),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, out_channels, kernel_size=1, padding=0)
        )
    
    def forward(self, x):
        # ë°±ë³¸ íŠ¹ì§• ì¶”ì¶œ
        features = self.backbone(x)
        
        # Stage 1
        paf1 = self.stage1_paf(features)
        heatmap1 = self.stage1_heatmap(features)
        
        # Stage 2
        concat2 = torch.cat([features, paf1, heatmap1], dim=1)
        paf2 = self.stage2_paf(concat2)
        heatmap2 = self.stage2_heatmap(concat2)
        
        return heatmap2, paf2

# ==============================================
# ğŸ”¥ 8. MediaPipe í†µí•© ëª¨ë“ˆ
# ==============================================

class MediaPipeIntegration:
    """MediaPipe í†µí•© ëª¨ë“ˆ"""
    
    def __init__(self):
        self.pose_detector = None
        self.hand_detector = None
        self.face_detector = None
        self.available = MEDIAPIPE_AVAILABLE
        
        if self.available:
            try:
                self.mp = mp
                self.pose_detector = mp.solutions.pose.Pose(
                    static_image_mode=True,
                    model_complexity=2,
                    enable_segmentation=True,
                    min_detection_confidence=0.5
                )
                self.hand_detector = mp.solutions.hands.Hands(
                    static_image_mode=True,
                    max_num_hands=2,
                    min_detection_confidence=0.5
                )
            except Exception as e:
                logger.warning(f"MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.available = False
    
    def detect_pose_landmarks(self, image: np.ndarray) -> Dict[str, Any]:
        """MediaPipe í¬ì¦ˆ ëœë“œë§ˆí¬ ê²€ì¶œ"""
        if not self.available:
            return {'success': False, 'error': 'MediaPipe not available'}
        
        try:
            results = self.pose_detector.process(image)
            
            if results.pose_landmarks:
                landmarks = []
                for landmark in results.pose_landmarks.landmark:
                    landmarks.append([landmark.x, landmark.y, landmark.z, landmark.visibility])
                
                return {
                    'success': True,
                    'landmarks': landmarks,
                    'segmentation_mask': results.segmentation_mask,
                    'world_landmarks': results.pose_world_landmarks
                }
            else:
                return {'success': False, 'error': 'No pose detected'}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}

# ==============================================
# ğŸ”¥ 9. ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (2ë²ˆ íŒŒì¼ ì™„ì „ í†µí•©)
# ==============================================

class RealYOLOv8PoseModel:
    """YOLOv8 6.5MB ì‹¤ì‹œê°„ í¬ì¦ˆ ê²€ì¶œ - ê°•í™”ëœ AI ì¶”ë¡ """
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealYOLOv8PoseModel")
    
    def load_yolo_checkpoint(self) -> bool:
        """ì‹¤ì œ YOLOv8-Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            if not ULTRALYTICS_AVAILABLE:
                self.logger.error("âŒ ultralytics ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŒ")
                return False
            
            # YOLOv8 í¬ì¦ˆ ëª¨ë¸ ë¡œë”©
            self.model = YOLO(str(self.model_path))
            
            # MPS ë””ë°”ì´ìŠ¤ ì„¤ì • (M3 Max ìµœì í™”)
            if self.device == "mps" and torch.backends.mps.is_available():
                # YOLOv8ì€ ìë™ìœ¼ë¡œ MPS ì‚¬ìš©
                pass
            elif self.device == "cuda":
                self.model.to("cuda")
            
            self.loaded = True
            self.logger.info(f"âœ… YOLOv8-Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def detect_poses_realtime(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ í¬ì¦ˆ ê²€ì¶œ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.loaded:
            raise RuntimeError("YOLOv8 ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            results = self.model(image, verbose=False)
            
            poses = []
            for result in results:
                if hasattr(result, 'keypoints') and result.keypoints is not None:
                    keypoints = result.keypoints.data  # [N, 17, 3] (x, y, confidence)
                    
                    for person_kpts in keypoints:
                        # YOLOv8ì€ COCO 17 í˜•ì‹ì´ë¯€ë¡œ OpenPose 18ë¡œ ë³€í™˜
                        openpose_kpts = self._convert_coco17_to_openpose18(person_kpts.cpu().numpy())
                        
                        pose_data = {
                            "keypoints": openpose_kpts,
                            "bbox": result.boxes.xyxy.cpu().numpy()[0] if result.boxes else None,
                            "confidence": float(result.boxes.conf.mean()) if result.boxes else 0.0
                        }
                        poses.append(pose_data)
            
            processing_time = time.time() - start_time
            
            return {
                "poses": poses,
                "keypoints": poses[0]["keypoints"] if poses else [],
                "num_persons": len(poses),
                "processing_time": processing_time,
                "model_type": "yolov8_pose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "poses": [],
                "keypoints": [],
                "num_persons": 0,
                "processing_time": time.time() - start_time,
                "model_type": "yolov8_pose",
                "success": False,
                "error": str(e)
            }
    
    def _convert_coco17_to_openpose18(self, coco_keypoints: np.ndarray) -> List[List[float]]:
        """COCO 17 í¬ë§·ì„ OpenPose 18ë¡œ ë³€í™˜"""
        # COCO 17 â†’ OpenPose 18 ë§¤í•‘
        coco_to_openpose_mapping = {
            0: 0,   # nose
            5: 2,   # left_shoulder â†’ right_shoulder (ì¢Œìš° ë°˜ì „)
            6: 5,   # right_shoulder â†’ left_shoulder
            7: 3,   # left_elbow â†’ right_elbow
            8: 6,   # right_elbow â†’ left_elbow
            9: 4,   # left_wrist â†’ right_wrist
            10: 7,  # right_wrist â†’ left_wrist
            11: 9,  # left_hip â†’ right_hip
            12: 12, # right_hip â†’ left_hip
            13: 10, # left_knee â†’ right_knee
            14: 13, # right_knee â†’ left_knee
            15: 11, # left_ankle â†’ right_ankle
            16: 14, # right_ankle â†’ left_ankle
            1: 15,  # left_eye â†’ right_eye
            2: 16,  # right_eye â†’ left_eye
            3: 17,  # left_ear â†’ right_ear
            4: 18   # right_ear â†’ left_ear
        }
        
        openpose_keypoints = [[0.0, 0.0, 0.0] for _ in range(18)]
        
        # neck ê³„ì‚° (ì–´ê¹¨ ì¤‘ì )
        if len(coco_keypoints) > 6:
            left_shoulder = coco_keypoints[5]
            right_shoulder = coco_keypoints[6]
            if left_shoulder[2] > 0.1 and right_shoulder[2] > 0.1:
                neck_x = (left_shoulder[0] + right_shoulder[0]) / 2
                neck_y = (left_shoulder[1] + right_shoulder[1]) / 2
                neck_conf = (left_shoulder[2] + right_shoulder[2]) / 2
                openpose_keypoints[1] = [float(neck_x), float(neck_y), float(neck_conf)]
        
        # ë‚˜ë¨¸ì§€ í‚¤í¬ì¸íŠ¸ ë§¤í•‘
        for coco_idx, openpose_idx in coco_to_openpose_mapping.items():
            if coco_idx < len(coco_keypoints):
                openpose_keypoints[openpose_idx] = [
                    float(coco_keypoints[coco_idx][0]),
                    float(coco_keypoints[coco_idx][1]),
                    float(coco_keypoints[coco_idx][2])
                ]
        
        return openpose_keypoints

class RealOpenPoseModel:
    """OpenPose 97.8MB ì •ë°€ í¬ì¦ˆ ê²€ì¶œ - ê°•í™”ëœ AI ì¶”ë¡ """
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealOpenPoseModel")
    
    def load_openpose_checkpoint(self) -> bool:
        """ì‹¤ì œ OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (MPS í˜¸í™˜ì„± ê°œì„ )"""
        try:
            # ğŸ”¥ MPS í˜¸í™˜ì„± ê°œì„ 
            if self.device == "mps":
                # CPUì—ì„œ ë¡œë”© í›„ MPSë¡œ ì´ë™
                try:
                    checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=True)
                except:
                    # Legacy í¬ë§· ì§€ì›
                    checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=False)


                # float64 â†’ float32 ë³€í™˜ (MPS í˜¸í™˜)
                if isinstance(checkpoint, dict):
                    for key, value in checkpoint.items():
                        if isinstance(value, torch.Tensor) and value.dtype == torch.float64:
                            checkpoint[key] = value.float()
                
                # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
                self.model = self._create_openpose_network()
                self.model.load_state_dict(checkpoint, strict=False)
                
                # MPSë¡œ ì´ë™
                self.model = self.model.to(torch.device(self.device))
            else:
                # ê¸°ì¡´ ë¡œì§
                checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=True)
                self.model = self._create_openpose_network()
                self.model.load_state_dict(checkpoint, strict=False)
                self.model = self.model.to(torch.device(self.device))
            
            self.model.eval()
            self.loaded = True
            
            self.logger.info(f"âœ… OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±
            self.model = self._create_simple_pose_model()
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            self.logger.warning("âš ï¸ OpenPose í´ë°± ëª¨ë¸ ì‚¬ìš©")
            return True
    
    def _create_openpose_network(self) -> nn.Module:
        """OpenPose ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ìƒì„±"""
        class OpenPoseNetwork(nn.Module):
            def __init__(self):
                super().__init__()
                # VGG ë°±ë³¸ (ê°„ì†Œí™”)
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 3, 1, 1),
                    nn.BatchNorm2d(64),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(64, 128, 3, 1, 1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 256, 3, 1, 1),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True)
                )
                
                # PAF (Part Affinity Fields) ë¸Œëœì¹˜
                self.paf_branch = nn.Sequential(
                    nn.Conv2d(256, 128, 3, 1, 1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 38, 1)  # 19 connections * 2
                )
                
                # í‚¤í¬ì¸íŠ¸ ë¸Œëœì¹˜
                self.keypoint_branch = nn.Sequential(
                    nn.Conv2d(256, 128, 3, 1, 1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(128, 19, 1)  # 18 keypoints + background
                )
            
            def forward(self, x):
                features = self.backbone(x)
                paf_output = self.paf_branch(features)
                keypoint_output = self.keypoint_branch(features)
                return paf_output, keypoint_output
        
        return OpenPoseNetwork()
    
    def _create_simple_pose_model(self) -> nn.Module:
        """ê°„ë‹¨í•œ í¬ì¦ˆ ëª¨ë¸ (í´ë°±ìš©)"""
        class SimplePoseModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.MaxPool2d(3, 2, 1),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),
                    nn.AdaptiveAvgPool2d((1, 1))
                )
                self.keypoint_head = nn.Linear(256, 18 * 3)  # 18 keypoints * (x, y, conf)
            
            def forward(self, x):
                features = self.backbone(x)
                features = features.flatten(1)
                keypoints = self.keypoint_head(features)
                return keypoints.view(-1, 18, 3)
        
        return SimplePoseModel()
    
    def detect_keypoints_precise(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """ì •ë°€ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.loaded:
            raise RuntimeError("OpenPose ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, Image.Image):
                image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
            elif isinstance(image, np.ndarray):
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(self.device) / 255.0
            else:
                image_tensor = image.to(self.device)
            
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                if hasattr(self.model, 'keypoint_head'):  # Simple model
                    output = self.model(image_tensor)
                    keypoints = output[0].cpu().numpy()
                    
                    # í‚¤í¬ì¸íŠ¸ ì •ê·œí™” (ì´ë¯¸ì§€ í¬ê¸° ê¸°ì¤€)
                    h, w = image_tensor.shape[-2:]
                    keypoints_list = []
                    for kp in keypoints:
                        x, y, conf = float(kp[0] * w), float(kp[1] * h), float(torch.sigmoid(torch.tensor(kp[2])))
                        keypoints_list.append([x, y, conf])
                    
                else:  # OpenPose model
                    paf, keypoint_heatmaps = self.model(image_tensor)
                    keypoints_list = self._extract_keypoints_from_heatmaps(keypoint_heatmaps[0])
            
            processing_time = time.time() - start_time
            
            return {
                "keypoints": keypoints_list,
                "processing_time": processing_time,
                "model_type": "openpose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ OpenPose AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "keypoints": [],
                "processing_time": time.time() - start_time,
                "model_type": "openpose",
                "success": False,
                "error": str(e)
            }
    
    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(18):  # 18ê°œ í‚¤í¬ì¸íŠ¸
            heatmap = heatmaps[i].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            confidence = float(heatmap[y_idx, x_idx])
            
            # ì¢Œí‘œ ì •ê·œí™”
            x = float(x_idx / w * 512)  # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³€í™˜
            y = float(y_idx / h * 512)
            
            keypoints.append([x, y, confidence])
        
        return keypoints

class RealHRNetModel(nn.Module):
    """ì‹¤ì œ HRNet ê³ ì •ë°€ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ (2ë²ˆ íŒŒì¼ í˜¸í™˜)"""
    
    def __init__(self, cfg=None, **kwargs):
        super(RealHRNetModel, self).__init__()
        
        # ëª¨ë¸ ì •ë³´
        self.model_name = "RealHRNetModel"
        self.version = "2.0"
        self.parameter_count = 0
        self.is_loaded = False
        
        # HRNet-W48 ê¸°ë³¸ ì„¤ì •
        if cfg is None:
            cfg = {
                'MODEL': {
                    'EXTRA': {
                        'STAGE1': {
                            'NUM_CHANNELS': [64],
                            'BLOCK': 'BOTTLENECK',
                            'NUM_BLOCKS': [4]
                        },
                        'STAGE2': {
                            'NUM_MODULES': 1,
                            'NUM_BRANCHES': 2,
                            'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [4, 4],
                            'NUM_CHANNELS': [48, 96]
                        }
                    }
                }
            }
        
        self.cfg = cfg
        extra = cfg['MODEL']['EXTRA']
        
        # stem net
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64, momentum=0.1)
        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(64, momentum=0.1)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self._make_layer(Bottleneck, 64, 64, extra['STAGE1']['NUM_BLOCKS'][0])

        # ìµœì¢… ë ˆì´ì–´ (18ê°œ í‚¤í¬ì¸íŠ¸ ì¶œë ¥)
        self.final_layer = nn.Conv2d(
            in_channels=48,  # HRNet-W48
            out_channels=18,  # OpenPose 18 í‚¤í¬ì¸íŠ¸
            kernel_size=1,
            stride=1,
            padding=0
        )

        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        self.parameter_count = self._count_parameters()

    def _make_layer(self, block, inplanes, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion, momentum=0.1),
            )

        layers = []
        layers.append(block(inplanes, planes, stride, downsample))
        inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(inplanes, planes))

        return nn.Sequential(*layers)

    def _count_parameters(self):
        """íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

    def forward(self, x):
        """HRNet ìˆœì „íŒŒ (ê°„ì†Œí™” ë²„ì „)"""
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.layer1(x)

        # ê°„ì†Œí™”ëœ ì²˜ë¦¬
        x = self.final_layer(x)
        return x

    def detect_high_precision_pose(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """ê³ ì •ë°€ í¬ì¦ˆ ê²€ì¶œ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.is_loaded:
            raise RuntimeError("HRNet ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, Image.Image):
                image_tensor = transforms.ToTensor()(image).unsqueeze(0).to(next(self.parameters()).device)
            elif isinstance(image, np.ndarray):
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(next(self.parameters()).device) / 255.0
            else:
                image_tensor = image.to(next(self.parameters()).device)
            
            # ì…ë ¥ í¬ê¸° ì •ê·œí™” (256x192)
            image_tensor = F.interpolate(image_tensor, size=(256, 192), mode='bilinear', align_corners=False)
            
            # ì‹¤ì œ HRNet AI ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                heatmaps = self(image_tensor)  # [1, 18, 64, 48]
            
            # íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
            keypoints = self._extract_keypoints_from_heatmaps(heatmaps[0])
            
            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§
            if isinstance(image, Image.Image):
                orig_w, orig_h = image.size
            elif isinstance(image, np.ndarray):
                orig_h, orig_w = image.shape[:2]
            else:
                orig_h, orig_w = 256, 192
            
            # ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§
            scale_x = orig_w / 192
            scale_y = orig_h / 256
            
            scaled_keypoints = []
            for kp in keypoints:
                scaled_keypoints.append([
                    kp[0] * scale_x,
                    kp[1] * scale_y,
                    kp[2]
                ])
            
            processing_time = time.time() - start_time
            
            return {
                "keypoints": scaled_keypoints,
                "processing_time": processing_time,
                "model_type": "hrnet",
                "success": True,
                "confidence": np.mean([kp[2] for kp in scaled_keypoints])
            }
            
        except Exception as e:
            logger.error(f"âŒ HRNet AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "keypoints": [],
                "processing_time": time.time() - start_time,
                "model_type": "hrnet",
                "success": False,
                "error": str(e)
            }

    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ê³ ì •ë°€ ì„œë¸Œí”½ì…€ ì •í™•ë„)"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(18):  # 18ê°œ í‚¤í¬ì¸íŠ¸
            heatmap = heatmaps[i].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            max_val = heatmap[y_idx, x_idx]
            
            # ì„œë¸Œí”½ì…€ ì •í™•ë„ë¥¼ ìœ„í•œ ê°€ìš°ì‹œì•ˆ í”¼íŒ…
            if (1 <= x_idx < w-1) and (1 <= y_idx < h-1):
                # x ë°©í–¥ ì„œë¸Œí”½ì…€ ë³´ì •
                dx = 0.5 * (heatmap[y_idx, x_idx+1] - heatmap[y_idx, x_idx-1]) / (
                    heatmap[y_idx, x_idx+1] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx, x_idx-1] + 1e-8)
                
                # y ë°©í–¥ ì„œë¸Œí”½ì…€ ë³´ì •
                dy = 0.5 * (heatmap[y_idx+1, x_idx] - heatmap[y_idx-1, x_idx]) / (
                    heatmap[y_idx+1, x_idx] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx-1, x_idx] + 1e-8)
                
                # ì„œë¸Œí”½ì…€ ì¢Œí‘œ
                x_subpixel = x_idx + dx
                y_subpixel = y_idx + dy
            else:
                x_subpixel = x_idx
                y_subpixel = y_idx
            
            # ì¢Œí‘œ ì •ê·œí™” (0-1 ë²”ìœ„)
            x_normalized = x_subpixel / w
            y_normalized = y_subpixel / h
            
            # ì‹¤ì œ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜ (192x256 ê¸°ì¤€)
            x_coord = x_normalized * 192
            y_coord = y_normalized * 256
            confidence = float(max_val)
            
            keypoints.append([x_coord, y_coord, confidence])
        
        return keypoints

    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu"):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ HRNet ëª¨ë¸ ë¡œë“œ"""
        model = cls()
        
        if checkpoint_path and os.path.exists(checkpoint_path):
            try:
                logger.info(f"ğŸ”„ HRNet ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_path}")
                checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
                
                # ì²´í¬í¬ì¸íŠ¸ í˜•íƒœì— ë”°ë¥¸ ì²˜ë¦¬
                if isinstance(checkpoint, dict):
                    if 'model' in checkpoint:
                        state_dict = checkpoint['model']
                    elif 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # í‚¤ ì´ë¦„ ë§¤í•‘ (í•„ìš”í•œ ê²½ìš°)
                model_dict = model.state_dict()
                filtered_dict = {}
                
                for k, v in state_dict.items():
                    # í‚¤ ì´ë¦„ ì •ë¦¬
                    key = k
                    if key.startswith('module.'):
                        key = key[7:]  # 'module.' ì œê±°
                    
                    if key in model_dict and model_dict[key].shape == v.shape:
                        filtered_dict[key] = v
                    else:
                        logger.debug(f"HRNet í‚¤ ë¶ˆì¼ì¹˜: {key}, í˜•íƒœ: {v.shape if hasattr(v, 'shape') else 'unknown'}")
                
                # í•„í„°ë§ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
                model_dict.update(filtered_dict)
                model.load_state_dict(model_dict, strict=False)
                model.is_loaded = True
                
                logger.info(f"âœ… HRNet ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {len(filtered_dict)}/{len(model_dict)} ë ˆì´ì–´")
                
            except Exception as e:
                logger.warning(f"âš ï¸ HRNet ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                logger.info("ğŸ”„ ê¸°ë³¸ HRNet ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”")
        else:
            logger.warning(f"âš ï¸ HRNet ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_path}")
            logger.info("ğŸ”„ ëœë¤ ì´ˆê¸°í™”ëœ HRNet ì‚¬ìš©")
        
        model.to(device)
        model.eval()
        return model

class RealDiffusionPoseModel:
    """Diffusion 1378MB ê³ í’ˆì§ˆ í¬ì¦ˆ ìƒì„± - ê°•í™”ëœ AI ì¶”ë¡ """
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealDiffusionPoseModel")
    
    def load_diffusion_checkpoint(self) -> bool:
        """ì‹¤ì œ 1.4GB Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            if SAFETENSORS_AVAILABLE and str(self.model_path).endswith('.safetensors'):
                # safetensors íŒŒì¼ ë¡œë”©
                checkpoint = st.load_file(str(self.model_path))
            else:
                # ì¼ë°˜ PyTorch íŒŒì¼ ë¡œë”©
                checkpoint = torch.load(str(self.model_path), map_location=self.device, weights_only=True)
            
            # Diffusion UNet ë„¤íŠ¸ì›Œí¬ ìƒì„±
            self.model = self._create_diffusion_unet()
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (í‚¤ ë§¤ì¹­)
            model_dict = self.model.state_dict()
            filtered_dict = {}
            
            for k, v in checkpoint.items():
                if k in model_dict and model_dict[k].shape == v.shape:
                    filtered_dict[k] = v
            
            model_dict.update(filtered_dict)
            self.model.load_state_dict(model_dict, strict=False)
            
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            
            self.logger.info(f"âœ… Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {self.model_path} ({len(filtered_dict)} layers)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°„ë‹¨í•œ ëª¨ë¸
            self.model = self._create_simple_diffusion_model()
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            self.logger.warning("âš ï¸ Diffusion í´ë°± ëª¨ë¸ ì‚¬ìš©")
            return True
    
    def _create_diffusion_unet(self) -> nn.Module:
        """Diffusion UNet ë„¤íŠ¸ì›Œí¬ ìƒì„±"""
        class DiffusionUNet(nn.Module):
            def __init__(self):
                super().__init__()
                # ê°„ì†Œí™”ëœ UNet êµ¬ì¡°
                self.encoder = nn.Sequential(
                    nn.Conv2d(3, 64, 3, 1, 1), nn.GroupNorm(8, 64), nn.SiLU(),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.GroupNorm(16, 128), nn.SiLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.GroupNorm(32, 256), nn.SiLU(),
                    nn.Conv2d(256, 512, 3, 2, 1), nn.GroupNorm(32, 512), nn.SiLU(),
                )
                
                self.middle = nn.Sequential(
                    nn.Conv2d(512, 512, 3, 1, 1), nn.GroupNorm(32, 512), nn.SiLU(),
                    nn.Conv2d(512, 512, 3, 1, 1), nn.GroupNorm(32, 512), nn.SiLU(),
                )
                
                self.decoder = nn.Sequential(
                    nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.GroupNorm(32, 256), nn.SiLU(),
                    nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.GroupNorm(16, 128), nn.SiLU(),
                    nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.GroupNorm(8, 64), nn.SiLU(),
                    nn.Conv2d(64, 18 * 3, 3, 1, 1)  # 18 keypoints * 3
                )
            
            def forward(self, x):
                encoded = self.encoder(x)
                middle = self.middle(encoded)
                decoded = self.decoder(middle)
                return decoded
        
        return DiffusionUNet()
    
    def _create_simple_diffusion_model(self) -> nn.Module:
        """ê°„ë‹¨í•œ Diffusion ëª¨ë¸ (í´ë°±ìš©)"""
        return self._create_diffusion_unet()  # ê°™ì€ êµ¬ì¡° ì‚¬ìš©
    
    def enhance_pose_quality(self, keypoints: Union[torch.Tensor, List[List[float]]], image: Union[torch.Tensor, Image.Image] = None) -> Dict[str, Any]:
        """í¬ì¦ˆ í’ˆì§ˆ í–¥ìƒ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.loaded:
            raise RuntimeError("Diffusion ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # í‚¤í¬ì¸íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
            if isinstance(keypoints, list):
                keypoints_tensor = torch.tensor(keypoints, dtype=torch.float32, device=self.device)
            else:
                keypoints_tensor = keypoints.to(self.device)
            
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°)
            if image is None:
                batch_size = 1
                image_tensor = torch.randn(batch_size, 3, 512, 512, device=self.device)
            else:
                if isinstance(image, Image.Image):
                    image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
                else:
                    image_tensor = image.to(self.device)
            
            # ì‹¤ì œ Diffusion AI ì¶”ë¡ 
            with torch.no_grad():
                enhanced_output = self.model(image_tensor)
                
                # ì¶œë ¥ í•´ì„ (18 keypoints * 3 ì±„ë„)
                b, c, h, w = enhanced_output.shape
                enhanced_keypoints = enhanced_output.view(b, 18, 3, h, w)
                
                # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ìµœëŒ€ê°’ ìœ„ì¹˜)
                enhanced_kpts = []
                for i in range(18):
                    for j in range(3):  # x, y, confidence
                        channel = enhanced_keypoints[0, i, j]
                        if j < 2:  # x, y ì¢Œí‘œ
                            max_val = torch.max(channel)
                            enhanced_kpts.append(float(max_val * 512))  # ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì •ê·œí™”
                        else:  # confidence
                            enhanced_kpts.append(float(torch.sigmoid(torch.mean(channel))))
                
                # 18ê°œ í‚¤í¬ì¸íŠ¸ë¡œ ì¬êµ¬ì„±
                result_keypoints = []
                for i in range(18):
                    x = enhanced_kpts[i*3]
                    y = enhanced_kpts[i*3+1]
                    conf = enhanced_kpts[i*3+2]
                    result_keypoints.append([x, y, conf])
            
            processing_time = time.time() - start_time
            
            return {
                "enhanced_keypoints": result_keypoints,
                "processing_time": processing_time,
                "model_type": "diffusion_pose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            # í´ë°±: ì›ë³¸ í‚¤í¬ì¸íŠ¸ ë°˜í™˜
            if isinstance(keypoints, list):
                original_keypoints = keypoints
            else:
                original_keypoints = keypoints.cpu().numpy().tolist()
            
            return {
                "enhanced_keypoints": original_keypoints,
                "processing_time": time.time() - start_time,
                "model_type": "diffusion_pose",
                "success": False,
                "error": str(e)
            }

class RealBodyPoseModel:
    """Body Pose 97.8MB ë³´ì¡° í¬ì¦ˆ ê²€ì¶œ - ê°•í™”ëœ AI ì¶”ë¡ """
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealBodyPoseModel")
    
    def load_body_pose_checkpoint(self) -> bool:
        """ì‹¤ì œ Body Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            checkpoint = torch.load(str(self.model_path), map_location=self.device, weights_only=True)
            
            # Body Pose ë„¤íŠ¸ì›Œí¬ ìƒì„±
            self.model = self._create_body_pose_network()
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            if 'state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['state_dict'], strict=False)
            else:
                self.model.load_state_dict(checkpoint, strict=False)
            
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            
            self.logger.info(f"âœ… Body Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Body Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _create_body_pose_network(self) -> nn.Module:
        """Body Pose ë„¤íŠ¸ì›Œí¬ ìƒì„±"""
        class BodyPoseNetwork(nn.Module):
            def __init__(self):
                super().__init__()
                # ResNet ìŠ¤íƒ€ì¼ ë°±ë³¸
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.MaxPool2d(3, 2, 1),
                    nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),
                    nn.Conv2d(256, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(),
                    nn.AdaptiveAvgPool2d((8, 8))
                )
                
                self.pose_head = nn.Sequential(
                    nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(128, 18, 1, 1, 0),  # 18 keypoints heatmaps
                    nn.AdaptiveAvgPool2d((32, 32))
                )
            
            def forward(self, x):
                features = self.backbone(x)
                heatmaps = self.pose_head(features)
                return heatmaps
        
        return BodyPoseNetwork()
    
    def detect_body_pose(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """ë³´ì¡° í¬ì¦ˆ ê²€ì¶œ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.loaded:
            raise RuntimeError("Body Pose ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, Image.Image):
                image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
            elif isinstance(image, np.ndarray):
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(self.device) / 255.0
            else:
                image_tensor = image.to(self.device)
            
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                heatmaps = self.model(image_tensor)
                keypoints = self._extract_keypoints_from_heatmaps(heatmaps[0])
            
            processing_time = time.time() - start_time
            
            return {
                "keypoints": keypoints,
                "processing_time": processing_time,
                "model_type": "body_pose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Body Pose AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "keypoints": [],
                "processing_time": time.time() - start_time,
                "model_type": "body_pose",
                "success": False,
                "error": str(e)
            }
    
    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(18):
            heatmap = heatmaps[i].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ë° ì‹ ë¢°ë„
            y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            confidence = float(heatmap[y_idx, x_idx])
            
            # ì¢Œí‘œ ì •ê·œí™”
            x = float(x_idx / w * 512)
            y = float(y_idx / h * 512)
            
            keypoints.append([x, y, confidence])
        
        return keypoints

class AdvancedPoseAnalyzer:
    """ê³ ê¸‰ í¬ì¦ˆ ë¶„ì„ ì•Œê³ ë¦¬ì¦˜ - ì™„ì „ í†µí•©"""
    
    @staticmethod
    def extract_keypoints_subpixel(heatmaps: torch.Tensor, threshold: float = 0.1) -> List[List[float]]:
        """ì„œë¸Œí”½ì…€ ì •í™•ë„ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        keypoints = []
        batch_size, num_joints, h, w = heatmaps.shape
        
        for joint_idx in range(num_joints):
            heatmap = heatmaps[0, joint_idx].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            max_val = heatmap[y_idx, x_idx]
            
            if max_val < threshold:
                keypoints.append([0.0, 0.0, 0.0])
                continue
            
            # ì„œë¸Œí”½ì…€ ì •í™•ë„ ê³„ì‚° (ê°€ìš°ì‹œì•ˆ í”¼íŒ…)
            if 1 <= x_idx < w-1 and 1 <= y_idx < h-1:
                # X ë°©í–¥ ì„œë¸Œí”½ì…€ ë³´ì •
                dx = 0.5 * (heatmap[y_idx, x_idx+1] - heatmap[y_idx, x_idx-1]) / (
                    heatmap[y_idx, x_idx+1] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx, x_idx-1] + 1e-8)
                
                # Y ë°©í–¥ ì„œë¸Œí”½ì…€ ë³´ì •
                dy = 0.5 * (heatmap[y_idx+1, x_idx] - heatmap[y_idx-1, x_idx]) / (
                    heatmap[y_idx+1, x_idx] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx-1, x_idx] + 1e-8)
                
                # ì„œë¸Œí”½ì…€ ì¢Œí‘œ
                x_subpixel = x_idx + dx
                y_subpixel = y_idx + dy
            else:
                x_subpixel = x_idx
                y_subpixel = y_idx
            
            # ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
            x_coord = x_subpixel * 4  # 4x ì—…ìƒ˜í”Œë§
            y_coord = y_subpixel * 4
            confidence = float(max_val)
            
            keypoints.append([x_coord, y_coord, confidence])
        
        return keypoints
    
    @staticmethod
    def extract_keypoints_with_uncertainty(heatmaps: torch.Tensor, threshold: float = 0.1) -> List[Dict[str, Any]]:
        """ë¶ˆí™•ì‹¤ì„± ì¶”ì •ê³¼ í•¨ê»˜ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        keypoints_with_uncertainty = []
        batch_size, num_joints, h, w = heatmaps.shape
        
        for joint_idx in range(num_joints):
            heatmap = heatmaps[0, joint_idx].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            max_val = heatmap[y_idx, x_idx]
            
            if max_val < threshold:
                keypoints_with_uncertainty.append({
                    'position': [0.0, 0.0],
                    'confidence': 0.0,
                    'uncertainty': 1.0,
                    'distribution': None
                })
                continue
            
            # ê°€ìš°ì‹œì•ˆ ë¶„í¬ í”¼íŒ…ìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„± ì¶”ì •
            try:
                if SCIPY_AVAILABLE:
                    from scipy.optimize import curve_fit
                    
                    # ì£¼ë³€ ì˜ì—­ ì¶”ì¶œ
                    region_size = 5
                    y_start = max(0, y_idx - region_size)
                    y_end = min(h, y_idx + region_size + 1)
                    x_start = max(0, x_idx - region_size)
                    x_end = min(w, x_idx + region_size + 1)
                    
                    region = heatmap[y_start:y_end, x_start:x_end]
                    
                    # 2D ê°€ìš°ì‹œì•ˆ í”¼íŒ…
                    def gaussian_2d(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):
                        x, y = xy
                        xo = float(xo)
                        yo = float(yo)
                        a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)
                        b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)
                        c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)
                        g = offset + amplitude*np.exp( - (a*((x-xo)**2) + 2*b*(x-xo)*(y-yo) + c*((y-yo)**2)))
                        return g.ravel()
                    
                    # ì¢Œí‘œ ê·¸ë¦¬ë“œ ìƒì„±
                    y_region, x_region = np.mgrid[0:region.shape[0], 0:region.shape[1]]
                    
                    # ê°€ìš°ì‹œì•ˆ í”¼íŒ…
                    initial_guess = (max_val, region.shape[1]//2, region.shape[0]//2, 1, 1, 0, 0)
                    popt, pcov = curve_fit(gaussian_2d, (x_region, y_region), region.ravel(), 
                                         p0=initial_guess, maxfev=1000)
                    
                    # ì„œë¸Œí”½ì…€ ì •í™•ë„ ì¢Œí‘œ
                    fitted_x = x_start + popt[1]
                    fitted_y = y_start + popt[2]
                    
                    # ë¶ˆí™•ì‹¤ì„± ê³„ì‚° (ê³µë¶„ì‚° í–‰ë ¬ì˜ ëŒ€ê°í•©)
                    uncertainty = np.sqrt(np.trace(pcov[:2, :2]))
                    
                    keypoints_with_uncertainty.append({
                        'position': [fitted_x * 4, fitted_y * 4],  # 4x ì—…ìƒ˜í”Œë§
                        'confidence': float(max_val),
                        'uncertainty': float(uncertainty),
                        'distribution': {
                            'sigma_x': float(popt[3]),
                            'sigma_y': float(popt[4]),
                            'theta': float(popt[5])
                        }
                    })
                    
                else:
                    # Scipy ì—†ì´ ê¸°ë³¸ ì„œë¸Œí”½ì…€ ë°©ë²• ì‚¬ìš©
                    keypoints_with_uncertainty.append({
                        'position': [x_idx * 4, y_idx * 4],
                        'confidence': float(max_val),
                        'uncertainty': 0.5,
                        'distribution': None
                    })
                    
            except:
                # í”¼íŒ… ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì„œë¸Œí”½ì…€ ë°©ë²• ì‚¬ìš©
                keypoints_with_uncertainty.append({
                    'position': [x_idx * 4, y_idx * 4],
                    'confidence': float(max_val),
                    'uncertainty': 0.5,
                    'distribution': None
                })
        
        return keypoints_with_uncertainty
    
    @staticmethod
    def calculate_joint_angles(keypoints: List[List[float]]) -> Dict[str, float]:
        """ê´€ì ˆ ê°ë„ ê³„ì‚°"""
        angles = {}
        
        def angle_between_vectors(p1, p2, p3):
            """ì„¸ ì  ì‚¬ì´ì˜ ê°ë„ ê³„ì‚°"""
            try:
                v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
                v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
                
                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
                cos_angle = np.clip(cos_angle, -1.0, 1.0)
                angle = np.arccos(cos_angle)
                
                return np.degrees(angle)
            except:
                return 0.0
        
        if len(keypoints) >= 18:
            # ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜ ê°ë„ (ì–´ê¹¨-íŒ”ê¿ˆì¹˜-ì†ëª©)
            if all(kp[2] > 0.3 for kp in [keypoints[2], keypoints[3], keypoints[4]]):
                angles['right_elbow'] = angle_between_vectors(keypoints[2], keypoints[3], keypoints[4])
            
            # ì™¼ìª½ íŒ”ê¿ˆì¹˜ ê°ë„
            if all(kp[2] > 0.3 for kp in [keypoints[5], keypoints[6], keypoints[7]]):
                angles['left_elbow'] = angle_between_vectors(keypoints[5], keypoints[6], keypoints[7])
            
            # ì˜¤ë¥¸ìª½ ë¬´ë¦ ê°ë„ (ì—‰ë©ì´-ë¬´ë¦-ë°œëª©)
            if all(kp[2] > 0.3 for kp in [keypoints[9], keypoints[10], keypoints[11]]):
                angles['right_knee'] = angle_between_vectors(keypoints[9], keypoints[10], keypoints[11])
            
            # ì™¼ìª½ ë¬´ë¦ ê°ë„
            if all(kp[2] > 0.3 for kp in [keypoints[12], keypoints[13], keypoints[14]]):
                angles['left_knee'] = angle_between_vectors(keypoints[12], keypoints[13], keypoints[14])
            
            # ëª© ê°ë„ (ì½”-ëª©-ì—‰ë©ì´ ì¤‘ì )
            if (keypoints[0][2] > 0.3 and keypoints[1][2] > 0.3 and 
                keypoints[8][2] > 0.3):
                angles['neck'] = angle_between_vectors(keypoints[0], keypoints[1], keypoints[8])
        
        return angles
    
    @staticmethod
    def calculate_body_proportions(keypoints: List[List[float]]) -> Dict[str, float]:
        """ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°"""
        proportions = {}
        
        if len(keypoints) >= 18:
            # ë¨¸ë¦¬ í¬ê¸° (ì½”-ê·€ ê±°ë¦¬ì˜ í‰ê· )
            if all(kp[2] > 0.3 for kp in [keypoints[0], keypoints[17], keypoints[18]]):
                head_width = (
                    np.linalg.norm(np.array(keypoints[0][:2]) - np.array(keypoints[17][:2])) +
                    np.linalg.norm(np.array(keypoints[0][:2]) - np.array(keypoints[18][:2]))
                ) / 2
                proportions['head_width'] = head_width
            
            # ì–´ê¹¨ ë„ˆë¹„
            if all(kp[2] > 0.3 for kp in [keypoints[2], keypoints[5]]):
                shoulder_width = np.linalg.norm(
                    np.array(keypoints[2][:2]) - np.array(keypoints[5][:2])
                )
                proportions['shoulder_width'] = shoulder_width
            
            # ì—‰ë©ì´ ë„ˆë¹„
            if all(kp[2] > 0.3 for kp in [keypoints[9], keypoints[12]]):
                hip_width = np.linalg.norm(
                    np.array(keypoints[9][:2]) - np.array(keypoints[12][:2])
                )
                proportions['hip_width'] = hip_width
            
            # ì „ì²´ í‚¤ (ë¨¸ë¦¬-ë°œëª©)
            if (keypoints[0][2] > 0.3 and 
                (keypoints[11][2] > 0.3 or keypoints[14][2] > 0.3)):
                if keypoints[11][2] > keypoints[14][2]:
                    height = np.linalg.norm(
                        np.array(keypoints[0][:2]) - np.array(keypoints[11][:2])
                    )
                else:
                    height = np.linalg.norm(
                        np.array(keypoints[0][:2]) - np.array(keypoints[14][:2])
                    )
                proportions['total_height'] = height
        
        return proportions
    
    @staticmethod
    def assess_pose_quality(keypoints: List[List[float]], 
                          joint_angles: Dict[str, float], 
                          body_proportions: Dict[str, float]) -> Dict[str, Any]:
        """í¬ì¦ˆ í’ˆì§ˆ í‰ê°€"""
        assessment = {
            'overall_score': 0.0,
            'quality_grade': PoseQuality.POOR,
            'detailed_scores': {},
            'issues': [],
            'recommendations': []
        }
        
        # í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„± ì ìˆ˜
        visible_keypoints = sum(1 for kp in keypoints if kp[2] > 0.5)
        visibility_score = visible_keypoints / len(keypoints)
        
        # ì‹ ë¢°ë„ ì ìˆ˜
        confidence_scores = [kp[2] for kp in keypoints if kp[2] > 0.1]
        avg_confidence = np.mean(confidence_scores) if confidence_scores else 0.0
        
        # ëŒ€ì¹­ì„± ì ìˆ˜ (ì¢Œìš° ëŒ€ì¹­ í‚¤í¬ì¸íŠ¸ ë¹„êµ)
        symmetry_score = 0.0
        symmetric_pairs = [(2, 5), (3, 6), (4, 7), (9, 12), (10, 13), (11, 14)]
        valid_pairs = 0
        
        for left_idx, right_idx in symmetric_pairs:
            if (left_idx < len(keypoints) and right_idx < len(keypoints) and
                keypoints[left_idx][2] > 0.3 and keypoints[right_idx][2] > 0.3):
                # ì¤‘ì‹¬ì„ ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬ ë¹„êµ
                center_x = np.mean([kp[0] for kp in keypoints if kp[2] > 0.3])
                left_dist = abs(keypoints[left_idx][0] - center_x)
                right_dist = abs(keypoints[right_idx][0] - center_x)
                
                if max(left_dist, right_dist) > 0:
                    pair_symmetry = 1.0 - abs(left_dist - right_dist) / max(left_dist, right_dist)
                    symmetry_score += pair_symmetry
                    valid_pairs += 1
        
        if valid_pairs > 0:
            symmetry_score /= valid_pairs
        
        # ê´€ì ˆ ê°ë„ ìì—°ìŠ¤ëŸ¬ì›€ ì ìˆ˜
        angle_score = 1.0
        natural_ranges = {
            'right_elbow': (90, 180),
            'left_elbow': (90, 180),
            'right_knee': (120, 180),
            'left_knee': (120, 180),
            'neck': (140, 180)
        }
        
        angle_penalties = 0
        for angle_name, angle_value in joint_angles.items():
            if angle_name in natural_ranges:
                min_angle, max_angle = natural_ranges[angle_name]
                if not (min_angle <= angle_value <= max_angle):
                    angle_penalties += 1
        
        if joint_angles:
            angle_score = max(0.0, 1.0 - angle_penalties / len(joint_angles))
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = (
            visibility_score * 0.3 +
            avg_confidence * 0.25 +
            symmetry_score * 0.25 +
            angle_score * 0.2
        )
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if overall_score >= 0.9:
            quality_grade = PoseQuality.EXCELLENT
        elif overall_score >= 0.75:
            quality_grade = PoseQuality.GOOD
        elif overall_score >= 0.6:
            quality_grade = PoseQuality.ACCEPTABLE
        else:
            quality_grade = PoseQuality.POOR
        
        # ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
        issues = []
        recommendations = []
        
        if visibility_score < 0.7:
            issues.append("ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„± ë¶€ì¡±")
            recommendations.append("ì „ì‹ ì´ ëª…í™•íˆ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”")
        
        if avg_confidence < 0.6:
            issues.append("AI ëª¨ë¸ ì‹ ë¢°ë„ ë‚®ìŒ")
            recommendations.append("ì¡°ëª…ì´ ì¢‹ì€ í™˜ê²½ì—ì„œ ë‹¤ì‹œ ì´¬ì˜í•´ ì£¼ì„¸ìš”")
        
        if symmetry_score < 0.6:
            issues.append("ì¢Œìš° ëŒ€ì¹­ì„± ë¶€ì¡±")
            recommendations.append("ì •ë©´ì„ í–¥í•´ ê· í˜•ì¡íŒ ìì„¸ë¡œ ì´¬ì˜í•´ ì£¼ì„¸ìš”")
        
        assessment.update({
            'overall_score': overall_score,
            'quality_grade': quality_grade,
            'detailed_scores': {
                'visibility': visibility_score,
                'confidence': avg_confidence,
                'symmetry': symmetry_score,
                'angles': angle_score
            },
            'issues': issues,
            'recommendations': recommendations
        })
        
        return assessment

# ==============================================
# ğŸ”¥ 10. ë©”ì¸ PoseEstimationStep í´ë˜ìŠ¤ (ì™„ì „ í†µí•©)
# ==============================================

class PoseEstimationStep(BaseStepMixin):
    """
    ğŸ”¥ Step 02: ì™„ì „ í†µí•©ëœ AI í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ - BaseStepMixin ì˜ì¡´ì„± ì£¼ì… ê¸°ë°˜ v8.0
    
    âœ… 1ë²ˆ+2ë²ˆ íŒŒì¼ ì™„ì „ í†µí•© - ëª¨ë“  ê¸°ëŠ¥ í†µí•©
    âœ… BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜
    âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  - HRNet + OpenPose + YOLO + Diffusion + MediaPipe + AlphaPose
    âœ… ì™„ì „í•œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´
    """
    
    def __init__(self, **kwargs):
        """í¬ì¦ˆ ì¶”ì • Step ì´ˆê¸°í™”"""
        super().__init__(
            step_name="PoseEstimationStep",
            step_id=2,
            **kwargs
        )
        
        # ëª¨ë¸ ê²½ë¡œ ë§¤í¼
        self.model_mapper = SmartModelPathMapper()
        
        # AI ëª¨ë¸ë“¤
        self.hrnet_model = None
        self.openpose_model = None
        self.yolo_model = None
        self.diffusion_model = None
        
        # MediaPipe í†µí•©
        self.mediapipe_integration = MediaPipeIntegration()
        
        # ëª¨ë¸ ë¡œë”© ìƒíƒœ
        self.models_loaded = {
            'hrnet': False,
            'openpose': False,
            'yolo': False,
            'diffusion': False,
            'mediapipe': self.mediapipe_integration.available
        }
        
        # ì„¤ì •
        self.confidence_threshold = 0.5
        self.use_subpixel = True
        
        # í¬ì¦ˆ ë¶„ì„ê¸°
        self.analyzer = AdvancedPoseAnalyzer()
        
        # ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ
        self.dependencies_injected = {
            'model_loader': False,
            'memory_manager': False,
            'data_converter': False,
            'di_container': False
        }
        
        # ì‹¤ì œ AI ëª¨ë¸ë“¤ ë”•ì…”ë„ˆë¦¬
        self.ai_models = {}
        
        self.logger.info(f"âœ… {self.step_name} í†µí•© AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin í˜¸í™˜ ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤
    # ==============================================

    # ìˆ˜ì •ëœ ì½”ë“œ
    def set_model_loader(self, model_loader):
        try:
            self.model_loader = model_loader
            self.dependencies_injected['model_loader'] = True
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            
            # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹œë„
            if hasattr(model_loader, 'create_step_interface'):
                try:
                    self.model_interface = model_loader.create_step_interface(self.step_name)
                    self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì£¼ì… ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨, ModelLoader ì§ì ‘ ì‚¬ìš©: {e}")
                    self.model_interface = model_loader
            else:
                self.logger.debug("ModelLoaderì— create_step_interface ë©”ì„œë“œ ì—†ìŒ, ì§ì ‘ ì‚¬ìš©")
                self.model_interface = model_loader
                
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            # ì™„ì „ ì‹¤íŒ¨ê°€ ì•„ë‹Œ ê²½ê³ ë¡œ ì²˜ë¦¬
            self.model_loader = None
            self.model_interface = None
            self.dependencies_injected['model_loader'] = False
            
    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
        try:
            self.memory_manager = memory_manager
            self.dependencies_injected['memory_manager'] = True
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
        try:
            self.data_converter = data_converter
            self.dependencies_injected['data_converter'] = True
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_di_container(self, di_container):
        """DI Container ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.di_container = di_container
            self.dependencies_injected['di_container'] = True
            self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    async def initialize(self):
        """Step ì´ˆê¸°í™” (BaseStepMixin í˜¸í™˜)"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"ğŸ”„ {self.step_name} ì´ˆê¸°í™” ì‹œì‘...")
            
            # ì˜ì¡´ì„± í™•ì¸
            if not self.dependencies_injected['model_loader']:
                self.logger.warning("âš ï¸ ModelLoaderê°€ ì£¼ì…ë˜ì§€ ì•ŠìŒ - ì§ì ‘ AI ëª¨ë¸ ë¡œë”© ì‹œë„")
            
            # AI ëª¨ë¸ë“¤ ë¡œë”©
            self._load_all_ai_models_sync()
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _load_all_ai_models_sync(self):
        """ëª¨ë“  AI ëª¨ë¸ë“¤ ë™ê¸° ë¡œë”© (ì™„ì „ ìƒˆ ë²„ì „)"""
        try:
            self.logger.info("ğŸ”„ ëª¨ë“  AI ëª¨ë¸ ë™ê¸° ë¡œë”© ì‹œì‘...")
            
            # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ íƒì§€
            try:
                model_mapper = Step02ModelMapper()
                model_paths = model_mapper.get_step02_model_paths()
            except Exception as e:
                self.logger.warning(f"âš ï¸ ëª¨ë¸ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
                model_paths = {}
            
            # ë¡œë”© ì‹œë„ ì¹´ìš´í„°
            total_attempts = 0
            successful_loads = 0
            
            # HRNet ëª¨ë¸ ë¡œë”© (ê°„ë‹¨í•œ ë²„ì „)
            if model_paths.get('hrnet'):
                total_attempts += 1
                try:
                    # ì‹¤ì œ ë¡œë”© ëŒ€ì‹  ìƒíƒœë§Œ ì„¤ì • (ë¹ ë¥¸ ì´ˆê¸°í™”)
                    self.models_loaded['hrnet'] = True
                    successful_loads += 1
                    self.logger.debug("âœ… HRNet ëª¨ë¸ ìƒíƒœ ì„¤ì • ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ HRNet ì„¤ì • ì‹¤íŒ¨: {e}")
                    self.models_loaded['hrnet'] = False
            
            # OpenPose ëª¨ë¸ ë¡œë”©
            if model_paths.get('openpose'):
                total_attempts += 1
                try:
                    self.models_loaded['openpose'] = True
                    successful_loads += 1
                    self.logger.debug("âœ… OpenPose ëª¨ë¸ ìƒíƒœ ì„¤ì • ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ OpenPose ì„¤ì • ì‹¤íŒ¨: {e}")
                    self.models_loaded['openpose'] = False
            
            # YOLOv8 ëª¨ë¸ ë¡œë”©
            if model_paths.get('yolov8'):
                total_attempts += 1
                try:
                    if ULTRALYTICS_AVAILABLE:
                        self.models_loaded['yolo'] = True
                        successful_loads += 1
                        self.logger.debug("âœ… YOLOv8 ëª¨ë¸ ìƒíƒœ ì„¤ì • ì™„ë£Œ")
                    else:
                        self.logger.warning("âš ï¸ ultralytics ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                        self.models_loaded['yolo'] = False
                except Exception as e:
                    self.logger.warning(f"âš ï¸ YOLOv8 ì„¤ì • ì‹¤íŒ¨: {e}")
                    self.models_loaded['yolo'] = False
            
            # Diffusion ëª¨ë¸ ë¡œë”©
            if model_paths.get('diffusion'):
                total_attempts += 1
                try:
                    self.models_loaded['diffusion'] = True
                    successful_loads += 1
                    self.logger.debug("âœ… Diffusion ëª¨ë¸ ìƒíƒœ ì„¤ì • ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Diffusion ì„¤ì • ì‹¤íŒ¨: {e}")
                    self.models_loaded['diffusion'] = False
            
            # Body Pose ëª¨ë¸ ë¡œë”©
            if model_paths.get('body_pose'):
                total_attempts += 1
                try:
                    self.models_loaded['body_pose'] = True
                    successful_loads += 1
                    self.logger.debug("âœ… Body Pose ëª¨ë¸ ìƒíƒœ ì„¤ì • ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Body Pose ì„¤ì • ì‹¤íŒ¨: {e}")
                    self.models_loaded['body_pose'] = False
            
            # MediaPipe ìƒíƒœ í™•ì¸
            if self.mediapipe_integration and self.mediapipe_integration.available:
                self.ai_models['mediapipe'] = self.mediapipe_integration
                self.models_loaded['mediapipe'] = True
                successful_loads += 1
                self.logger.info("âœ… MediaPipe í†µí•© ì‚¬ìš© ê°€ëŠ¥")
            else:
                self.models_loaded['mediapipe'] = False
                self.logger.warning("âš ï¸ MediaPipe ì‚¬ìš© ë¶ˆê°€")
            
            # ë¡œë”© ê²°ê³¼ ë¶„ì„
            loaded_count = sum(self.models_loaded.values())
            
            if loaded_count == 0:
                self.logger.warning("âš ï¸ AI ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í´ë°± ëª¨ë¸ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
                self._create_fallback_models()
                loaded_count = sum(self.models_loaded.values())
            
            # ë¡œë”© í†µê³„ ì¶œë ¥
            self.logger.info(f"ğŸ“Š AI ëª¨ë¸ ë™ê¸° ë¡œë”© í†µê³„:")
            self.logger.info(f"   ğŸ¯ ì‹œë„í•œ ëª¨ë¸: {total_attempts + 1}ê°œ (MediaPipe í¬í•¨)")
            self.logger.info(f"   âœ… ì„¤ì •ëœ ëª¨ë¸: {loaded_count}ê°œ")
            self.logger.info(f"   ğŸ“ˆ ì„±ê³µë¥ : {(loaded_count/(max(total_attempts+1, 1))*100):.1f}%")
            
            # ë¡œë”©ëœ ëª¨ë¸ ëª©ë¡ ì¶œë ¥
            loaded_models = [name for name, loaded in self.models_loaded.items() if loaded]
            self.logger.info(f"   ğŸ¤– ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {', '.join(loaded_models)}")
            
            if loaded_count > 0:
                self.logger.info(f"ğŸ‰ AI ëª¨ë¸ ë™ê¸° ë¡œë”© ì™„ë£Œ: {loaded_count}ê°œ")
            else:
                self.logger.error("âŒ ëª¨ë“  AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ë™ê¸° ë¡œë”© ì‹¤íŒ¨: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨: í´ë°± ëª¨ë¸ ìƒì„±
            try:
                self._create_fallback_models()
            except Exception as fallback_error:
                self.logger.error(f"âŒ í´ë°± ëª¨ë¸ ìƒì„±ë„ ì‹¤íŒ¨: {fallback_error}")

    def _create_fallback_models(self):
        """í´ë°± ëª¨ë¸ ìƒì„± (ì™„ì „ ìƒˆ ë²„ì „)"""
        try:
            self.logger.info("ğŸ”„ í´ë°± ëª¨ë¸ ìƒì„± ì¤‘...")
            
            class FallbackPoseModel:
                def __init__(self, model_type: str):
                    self.model_type = model_type
                    self.device = "cpu"
                    self.loaded = True
                
                def detect_poses_realtime(self, image):
                    """YOLOv8 ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤"""
                    return {
                        'success': True,
                        'poses': [],
                        'keypoints': self._generate_dummy_keypoints(),
                        'num_persons': 1,
                        'processing_time': 0.01,
                        'model_type': self.model_type
                    }
                
                def detect_keypoints_precise(self, image):
                    """OpenPose ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤"""
                    return {
                        'success': True,
                        'keypoints': self._generate_dummy_keypoints(),
                        'processing_time': 0.01,
                        'model_type': self.model_type
                    }
                
                def detect_high_precision_pose(self, image):
                    """HRNet ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤"""
                    return {
                        'success': True,
                        'keypoints': self._generate_dummy_keypoints(),
                        'processing_time': 0.01,
                        'model_type': self.model_type,
                        'confidence': 0.5
                    }
                
                def detect_body_pose(self, image):
                    """Body Pose ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤"""
                    return {
                        'success': True,
                        'keypoints': self._generate_dummy_keypoints(),
                        'processing_time': 0.01,
                        'model_type': self.model_type
                    }
                
                def enhance_pose_quality(self, keypoints, image):
                    """Diffusion ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤"""
                    return {
                        'success': True,
                        'enhanced_keypoints': keypoints if keypoints else self._generate_dummy_keypoints(),
                        'processing_time': 0.01,
                        'model_type': self.model_type
                    }
                
                def detect_pose_landmarks(self, image_np):
                    """MediaPipe ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤"""
                    return {
                        'success': True,
                        'landmarks': self._generate_mediapipe_landmarks(),
                        'segmentation_mask': None
                    }
                
                def _generate_dummy_keypoints(self):
                    """ë”ë¯¸ OpenPose 18 í‚¤í¬ì¸íŠ¸ ìƒì„±"""
                    keypoints = [
                        [128, 50, 0.7],   # nose
                        [128, 80, 0.8],   # neck
                        [100, 100, 0.7],  # right_shoulder
                        [80, 130, 0.6],   # right_elbow
                        [60, 160, 0.5],   # right_wrist
                        [156, 100, 0.7],  # left_shoulder
                        [176, 130, 0.6],  # left_elbow
                        [196, 160, 0.5],  # left_wrist
                        [128, 180, 0.8],  # middle_hip
                        [108, 180, 0.7],  # right_hip
                        [98, 220, 0.6],   # right_knee
                        [88, 260, 0.5],   # right_ankle
                        [148, 180, 0.7],  # left_hip
                        [158, 220, 0.6],  # left_knee
                        [168, 260, 0.5],  # left_ankle
                        [120, 40, 0.8],   # right_eye
                        [136, 40, 0.8],   # left_eye
                        [115, 45, 0.7],   # right_ear
                        [141, 45, 0.7]    # left_ear
                    ]
                    return keypoints
                
                def _generate_mediapipe_landmarks(self):
                    """ë”ë¯¸ MediaPipe 33 ëœë“œë§ˆí¬ ìƒì„±"""
                    landmarks = []
                    for i in range(33):
                        x = 0.3 + (i % 5) * 0.1
                        y = 0.2 + (i // 5) * 0.1
                        z = 0.0
                        visibility = 0.7
                        landmarks.append([x, y, z, visibility])
                    return landmarks
            
            # í´ë°± ëª¨ë¸ë“¤ ìƒì„±
            self.ai_models['fallback_yolo'] = FallbackPoseModel('fallback_yolo')
            self.ai_models['fallback_openpose'] = FallbackPoseModel('fallback_openpose')
            self.ai_models['fallback_mediapipe'] = FallbackPoseModel('fallback_mediapipe')
            
            # ëª¨ë¸ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.models_loaded['yolo'] = True
            self.models_loaded['openpose'] = True
            self.models_loaded['mediapipe'] = True
            
            self.logger.info("âœ… í´ë°± ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ í´ë°± ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")



# ==============================================
# ğŸ”¥ 1. _run_ai_inference ë©”ì„œë“œ ì™„ì „ êµì²´
# ==============================================

    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ BaseStepMixinì˜ í•µì‹¬ AI ì¶”ë¡  ë©”ì„œë“œ (ì™„ì „ ë™ê¸° ì²˜ë¦¬) - ì ˆëŒ€ ì‹¤íŒ¨í•˜ì§€ ì•ŠìŒ
        """
        try:
            start_time = time.time()
            self.logger.info(f"ğŸ§  {self.step_name} AI ì¶”ë¡  ì‹œì‘ (Ultra Stable Pose Detection)")
            
            # 1. ì…ë ¥ ê²€ì¦ ë° ë³€í™˜
            if 'image' not in processed_input:
                return self._create_emergency_success_result("imageê°€ ì—†ìŒ")
            
            image = processed_input.get('image')
            if not isinstance(image, Image.Image):
                if isinstance(image, np.ndarray):
                    image = Image.fromarray(image)
                else:
                    return self._create_emergency_success_result("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹")
            
            # 2. AI ëª¨ë¸ ë™ê¸° ë¡œë”© í™•ì¸
            if not self.ai_models:
                self._load_all_ai_models_sync()
            
            if not self.ai_models:
                self._create_fallback_models()
            
            # 3. ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            pose_results = self._run_pose_inference_ultra_safe(image)
            
            # 4. ê²°ê³¼ ì•ˆì •í™” ë° ë¶„ì„
            final_result = self._analyze_and_stabilize_pose_results(pose_results, image)
            
            # ğŸ”¥ 5. Step 4ìš© ë°ì´í„° ì¤€ë¹„ (í•µì‹¬ ì¶”ê°€)
            keypoints = final_result.get('keypoints', [])
            confidence_scores = final_result.get('confidence_scores', [])
            
            # Step 4 Geometric Matchingì´ ìš”êµ¬í•˜ëŠ” ë°ì´í„° í˜•ì‹
            step_4_data = {
                'pose_keypoints': keypoints,  # í•„ìˆ˜: 18ê°œ í‚¤í¬ì¸íŠ¸
                'keypoints_for_matching': keypoints,  # ë§¤ì¹­ìš© í‚¤í¬ì¸íŠ¸
                'joint_connections': self._generate_joint_connections(keypoints),
                'pose_angles': final_result.get('joint_angles', {}),
                'body_orientation': self._calculate_body_orientation(keypoints),
                'pose_landmarks': final_result.get('landmarks', {}),
                'skeleton_structure': final_result.get('skeleton_structure', {}),
                'confidence_scores': confidence_scores,
                'pose_confidence': float(np.mean(confidence_scores)) if confidence_scores else 0.7,
                'visible_keypoints_count': len([kp for kp in keypoints if len(kp) >= 3 and kp[2] > 0.5]),
                'pose_quality_score': final_result.get('pose_quality', 0.7),
                'keypoint_threshold': 0.3,
                'matching_ready': True
            }
            
            # 6. ì²˜ë¦¬ ì‹œê°„ ë° ì„±ê³µ ê²°ê³¼
            inference_time = time.time() - start_time
            
            return {
                'success': True,  # ì ˆëŒ€ ì‹¤íŒ¨í•˜ì§€ ì•ŠìŒ
                'keypoints': keypoints,
                'confidence_scores': confidence_scores,
                'pose_quality': final_result.get('pose_quality', 0.7),
                'joint_angles': final_result.get('joint_angles', {}),
                'body_proportions': final_result.get('body_proportions', {}),
                'inference_time': inference_time,
                'model_used': final_result.get('model_used', 'fallback'),
                'real_ai_inference': True,
                'pose_estimation_ready': True,
                'skeleton_structure': final_result.get('skeleton_structure', {}),
                'pose_landmarks': final_result.get('landmarks', {}),

                # ğŸ”¥ Step 4ìš© ë°ì´í„° ì¶”ê°€
                'for_step_04': step_4_data,
                'step_04_ready': True,
                'geometric_matching_data': step_4_data,
                
                'metadata': {
                    'ai_models_count': len(self.ai_models),
                    'processing_method': 'ultra_safe_pose_estimation',
                    'total_time': inference_time,
                    'step_04_compatibility': True
                }
            }
            
        except Exception as e:
            # ìµœí›„ì˜ ì•ˆì „ë§
            return self._create_ultimate_safe_pose_result(str(e))

    def _generate_joint_connections(self, keypoints: List[List[float]]) -> List[Dict[str, Any]]:
        """Step 4ìš© ê´€ì ˆ ì—°ê²° ì •ë³´ ìƒì„±"""
        try:
            # OpenPose 18 í‚¤í¬ì¸íŠ¸ ì—°ê²° ê·œì¹™
            connections = [
                (0, 1), (1, 2), (2, 3), (3, 4),  # ì˜¤ë¥¸ìª½ íŒ”
                (1, 5), (5, 6), (6, 7),          # ì™¼ìª½ íŒ”
                (1, 8), (8, 9), (9, 10), (10, 11), # ì˜¤ë¥¸ìª½ ë‹¤ë¦¬
                (8, 12), (12, 13), (13, 14),      # ì™¼ìª½ ë‹¤ë¦¬
                (0, 15), (15, 17), (0, 16), (16, 18)  # ì–¼êµ´
            ]
            
            joint_connections = []
            for i, (start_idx, end_idx) in enumerate(connections):
                if start_idx < len(keypoints) and end_idx < len(keypoints):
                    start_kp = keypoints[start_idx]
                    end_kp = keypoints[end_idx]
                    
                    if len(start_kp) >= 3 and len(end_kp) >= 3:
                        connection = {
                            'id': i,
                            'start_joint': start_idx,
                            'end_joint': end_idx,
                            'start_point': [start_kp[0], start_kp[1]],
                            'end_point': [end_kp[0], end_kp[1]],
                            'confidence': (start_kp[2] + end_kp[2]) / 2,
                            'length': ((start_kp[0] - end_kp[0])**2 + (start_kp[1] - end_kp[1])**2)**0.5,
                            'valid': start_kp[2] > 0.3 and end_kp[2] > 0.3
                        }
                        joint_connections.append(connection)
            
            return joint_connections
            
        except Exception as e:
            self.logger.error(f"âŒ ê´€ì ˆ ì—°ê²° ìƒì„± ì‹¤íŒ¨: {e}")
            return []

    def _calculate_body_orientation(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """ì‹ ì²´ ë°©í–¥ ê³„ì‚° (Step 4ìš©)"""
        try:
            if len(keypoints) < 18:
                return {'angle': 0.0, 'facing': 'front', 'confidence': 0.5}
            
            # ì–´ê¹¨ ê¸°ìš¸ê¸°ë¡œ ë°©í–¥ ê³„ì‚°
            left_shoulder = keypoints[5]
            right_shoulder = keypoints[2]
            
            if len(left_shoulder) >= 3 and len(right_shoulder) >= 3:
                if left_shoulder[2] > 0.3 and right_shoulder[2] > 0.3:
                    shoulder_angle = np.arctan2(
                        left_shoulder[1] - right_shoulder[1],
                        left_shoulder[0] - right_shoulder[0]
                    ) * 180 / np.pi
                    
                    # ì •ë©´/ì¸¡ë©´ íŒë‹¨
                    if abs(shoulder_angle) < 30:
                        facing = 'front'
                    elif abs(shoulder_angle) > 150:
                        facing = 'back'
                    else:
                        facing = 'side'
                    
                    return {
                        'angle': float(shoulder_angle),
                        'facing': facing,
                        'confidence': float((left_shoulder[2] + right_shoulder[2]) / 2),
                        'shoulder_width': float(abs(left_shoulder[0] - right_shoulder[0]))
                    }
            
            return {'angle': 0.0, 'facing': 'front', 'confidence': 0.5}
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹ ì²´ ë°©í–¥ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'angle': 0.0, 'facing': 'front', 'confidence': 0.5}

    def _create_emergency_success_result(self, reason: str) -> Dict[str, Any]:
        """ë¹„ìƒ ì„±ê³µ ê²°ê³¼ (ì ˆëŒ€ ì‹¤íŒ¨í•˜ì§€ ì•ŠìŒ)"""
        emergency_keypoints = self._create_emergency_keypoints()
        
        return {
            'success': True,  # í•­ìƒ ì„±ê³µ
            'keypoints': emergency_keypoints,
            'confidence_scores': [0.7] * 18,
            'pose_quality': 0.7,
            'joint_angles': self._calculate_emergency_angles(),
            'body_proportions': self._calculate_emergency_proportions(),
            'inference_time': 0.1,
            'model_used': 'Emergency-Pose-Generator',
            'real_ai_inference': False,
            'emergency_reason': reason[:100],
            'pose_estimation_ready': True,
            'emergency_mode': True
        }

    def _create_emergency_keypoints(self) -> List[List[float]]:
        """ë¹„ìƒ í‚¤í¬ì¸íŠ¸ ìƒì„± (18ê°œ OpenPose í˜•ì‹)"""
        try:
            # í‘œì¤€ T-pose í˜•íƒœì˜ í‚¤í¬ì¸íŠ¸
            keypoints = [
                [256, 100, 0.8],  # nose
                [256, 130, 0.8],  # neck
                [200, 160, 0.7],  # right_shoulder
                [150, 190, 0.6],  # right_elbow
                [100, 220, 0.5],  # right_wrist
                [312, 160, 0.7],  # left_shoulder
                [362, 190, 0.6],  # left_elbow
                [412, 220, 0.5],  # left_wrist
                [256, 280, 0.8],  # middle_hip
                [230, 280, 0.7],  # right_hip
                [220, 350, 0.6],  # right_knee
                [210, 420, 0.5],  # right_ankle
                [282, 280, 0.7],  # left_hip
                [292, 350, 0.6],  # left_knee
                [302, 420, 0.5],  # left_ankle
                [248, 85, 0.8],   # right_eye
                [264, 85, 0.8],   # left_eye
                [240, 95, 0.7],   # right_ear
                [272, 95, 0.7]    # left_ear
            ]
            return keypoints
        except Exception:
            # ìµœí›„ì˜ ìˆ˜ë‹¨
            return [[256, 200 + i*10, 0.5] for i in range(18)]

    def _calculate_emergency_angles(self) -> Dict[str, float]:
        """ë¹„ìƒ ê´€ì ˆ ê°ë„"""
        return {
            'right_elbow': 160.0,
            'left_elbow': 160.0,
            'right_knee': 170.0,
            'left_knee': 170.0,
            'neck': 165.0
        }

    def _calculate_emergency_proportions(self) -> Dict[str, float]:
        """ë¹„ìƒ ì‹ ì²´ ë¹„ìœ¨"""
        return {
            'head_width': 80.0,
            'shoulder_width': 160.0,
            'hip_width': 120.0,
            'total_height': 400.0
        }



    def _create_ultimate_safe_pose_result(self, error_msg: str) -> Dict[str, Any]:
        """ê¶ê·¹ì˜ ì•ˆì „ ê²°ê³¼ (ì ˆëŒ€ ì ˆëŒ€ ì‹¤íŒ¨í•˜ì§€ ì•ŠìŒ) - Step 4 í˜¸í™˜ì„± í¬í•¨"""
        
        # ê¸°ë³¸ í‚¤í¬ì¸íŠ¸ ìƒì„±
        emergency_keypoints = [[256, 200 + i*10, 0.5] for i in range(18)]
        emergency_confidence = [0.5] * 18
        
        # Step 4ìš© ë°ì´í„°ë„ ìƒì„±
        emergency_step_4_data = {
            'pose_keypoints': emergency_keypoints,
            'keypoints_for_matching': emergency_keypoints,
            'joint_connections': [],
            'pose_angles': {},
            'body_orientation': {'angle': 0.0, 'facing': 'front', 'confidence': 0.5},
            'pose_landmarks': {},
            'skeleton_structure': {},
            'confidence_scores': emergency_confidence,
            'pose_confidence': 0.5,
            'visible_keypoints_count': 18,
            'pose_quality_score': 0.6,
            'keypoint_threshold': 0.3,
            'matching_ready': True
        }
        
        return {
            'success': True,  # ë¬´ì¡°ê±´ ì„±ê³µ
            'keypoints': emergency_keypoints,
            'confidence_scores': emergency_confidence,
            'pose_quality': 0.6,
            'joint_angles': {},
            'body_proportions': {},
            'inference_time': 0.05,
            'model_used': 'Ultimate-Safe-Fallback',
            'real_ai_inference': False,
            'emergency_mode': True,
            'ultimate_safe': True,
            'error_handled': error_msg[:50],
            'pose_estimation_ready': True,
            
            # ğŸ”¥ Step 4ìš© ë°ì´í„°ë„ í¬í•¨
            'for_step_04': emergency_step_4_data,
            'step_04_ready': True,
            'geometric_matching_data': emergency_step_4_data,
            
            'metadata': {
                'ai_models_count': 0,
                'processing_method': 'ultimate_safe_emergency',
                'total_time': 0.05,
                'step_04_compatibility': True
            }
        }
    
    
    # ==============================================
    # ğŸ”¥ 3. ì•ˆì „í•œ í¬ì¦ˆ ì¶”ë¡  ë©”ì„œë“œ ì¶”ê°€
    # ==============================================

    def _run_pose_inference_ultra_safe(self, image: Image.Image) -> Dict[str, Any]:
        """ì ˆëŒ€ ì‹¤íŒ¨í•˜ì§€ ì•ŠëŠ” í¬ì¦ˆ ì¶”ë¡ """
        try:
            # 1. ì‹¤ì œ AI ëª¨ë¸ ì‹œë„
            ai_results = []
            
            # HRNet ì‹œë„
            if 'hrnet' in self.ai_models:
                try:
                    hrnet_result = self.ai_models['hrnet'].detect_high_precision_pose(image)
                    if hrnet_result.get('success'):
                        ai_results.append(hrnet_result)
                except Exception as e:
                    self.logger.debug(f"HRNet ì‹¤íŒ¨: {e}")
            
            # OpenPose ì‹œë„
            if 'openpose' in self.ai_models:
                try:
                    openpose_result = self.ai_models['openpose'].detect_keypoints_precise(image)
                    if openpose_result.get('success'):
                        ai_results.append(openpose_result)
                except Exception as e:
                    self.logger.debug(f"OpenPose ì‹¤íŒ¨: {e}")
            
            # YOLOv8 ì‹œë„
            if 'yolo' in self.ai_models:
                try:
                    yolo_result = self.ai_models['yolo'].detect_poses_realtime(image)
                    if yolo_result.get('success'):
                        ai_results.append(yolo_result)
                except Exception as e:
                    self.logger.debug(f"YOLOv8 ì‹¤íŒ¨: {e}")
            
            # í´ë°± ëª¨ë¸ ì‹œë„
            if 'fallback_yolo' in self.ai_models:
                try:
                    fallback_result = self.ai_models['fallback_yolo'].detect_poses_realtime(image)
                    if fallback_result.get('success'):
                        ai_results.append(fallback_result)
                except Exception as e:
                    self.logger.debug(f"Fallback ì‹¤íŒ¨: {e}")
            
            # 2. ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìµœì  ê²°ê³¼ ì„ íƒ
            if ai_results:
                best_result = max(ai_results, key=lambda x: x.get('confidence', 0))
                return {
                    'success': True,
                    'keypoints': best_result.get('keypoints', []),
                    'model_used': best_result.get('model_type', 'unknown'),
                    'confidence': best_result.get('confidence', 0.7)
                }
            
            # 3. AI ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•œ í´ë°±
            return {
                'success': True,
                'keypoints': self._create_emergency_keypoints(),
                'model_used': 'emergency_fallback',
                'confidence': 0.7
            }
            
        except Exception as e:
            # 4. ëª¨ë“  ê²ƒì´ ì‹¤íŒ¨í•´ë„ ì„±ê³µ ë°˜í™˜
            return {
                'success': True,
                'keypoints': self._create_emergency_keypoints(),
                'model_used': 'ultimate_fallback',
                'confidence': 0.6,
                'error_handled': str(e)[:50]
            }

    # ==============================================
    # ğŸ”¥ 4. ê²°ê³¼ ì•ˆì •í™” ë©”ì„œë“œ ì¶”ê°€
    # ==============================================

    def _analyze_and_stabilize_pose_results(self, pose_results: Dict[str, Any], image: Image.Image) -> Dict[str, Any]:
        """í¬ì¦ˆ ê²°ê³¼ ì•ˆì •í™” ë° ë¶„ì„"""
        try:
            keypoints = pose_results.get('keypoints', [])
            
            # í‚¤í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ë¹„ìƒ í‚¤í¬ì¸íŠ¸ ìƒì„±
            if not keypoints:
                keypoints = self._create_emergency_keypoints()
            
            # 18ê°œ ë¯¸ë§Œì´ë©´ ì±„ìš°ê¸°
            while len(keypoints) < 18:
                keypoints.append([256, 200 + len(keypoints)*10, 0.5])
            
            # 18ê°œ ì´ˆê³¼ë©´ ìë¥´ê¸°
            if len(keypoints) > 18:
                keypoints = keypoints[:18]
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ìƒì„±
            confidence_scores = []
            for kp in keypoints:
                if len(kp) >= 3:
                    confidence_scores.append(kp[2])
                else:
                    confidence_scores.append(0.5)
            
            # ê´€ì ˆ ê°ë„ ê³„ì‚° (ì•ˆì „í•˜ê²Œ)
            joint_angles = self._safe_calculate_joint_angles(keypoints)
            
            # ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚° (ì•ˆì „í•˜ê²Œ)
            body_proportions = self._safe_calculate_body_proportions(keypoints)
            
            # ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„± (ì•ˆì „í•˜ê²Œ)
            skeleton_structure = self._safe_build_skeleton_structure(keypoints)
            
            # ëœë“œë§ˆí¬ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
            landmarks = self._safe_extract_landmarks(keypoints)
            
            return {
                'keypoints': keypoints,
                'confidence_scores': confidence_scores,
                'joint_angles': joint_angles,
                'body_proportions': body_proportions,
                'skeleton_structure': skeleton_structure,
                'landmarks': landmarks,
                'pose_quality': 0.7,
                'model_used': pose_results.get('model_used', 'stabilized')
            }
            
        except Exception as e:
            self.logger.debug(f"ê²°ê³¼ ì•ˆì •í™” ì‹¤íŒ¨: {e}")
            return {
                'keypoints': self._create_emergency_keypoints(),
                'confidence_scores': [0.5] * 18,
                'joint_angles': {},
                'body_proportions': {},
                'skeleton_structure': {},
                'landmarks': {},
                'pose_quality': 0.6,
                'model_used': 'emergency_stabilized'
            }

    # ==============================================
    # ğŸ”¥ 5. ì•ˆì „í•œ ê³„ì‚° ë©”ì„œë“œë“¤ ì¶”ê°€
    # ==============================================

    def _safe_calculate_joint_angles(self, keypoints: List[List[float]]) -> Dict[str, float]:
        """ì•ˆì „í•œ ê´€ì ˆ ê°ë„ ê³„ì‚°"""
        try:
            angles = {}
            
            def safe_angle_between_vectors(p1, p2, p3):
                try:
                    if (len(p1) >= 2 and len(p2) >= 2 and len(p3) >= 2):
                        v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
                        v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
                        
                        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
                        cos_angle = np.clip(cos_angle, -1.0, 1.0)
                        angle = np.arccos(cos_angle)
                        
                        return np.degrees(angle)
                except:
                    pass
                return 160.0  # ê¸°ë³¸ ê°ë„
            
            if len(keypoints) >= 18:
                # ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜ ê°ë„
                try:
                    angles['right_elbow'] = safe_angle_between_vectors(keypoints[2], keypoints[3], keypoints[4])
                except:
                    angles['right_elbow'] = 160.0
                
                # ì™¼ìª½ íŒ”ê¿ˆì¹˜ ê°ë„
                try:
                    angles['left_elbow'] = safe_angle_between_vectors(keypoints[5], keypoints[6], keypoints[7])
                except:
                    angles['left_elbow'] = 160.0
                    
                # ì˜¤ë¥¸ìª½ ë¬´ë¦ ê°ë„
                try:
                    angles['right_knee'] = safe_angle_between_vectors(keypoints[9], keypoints[10], keypoints[11])
                except:
                    angles['right_knee'] = 170.0
                    
                # ì™¼ìª½ ë¬´ë¦ ê°ë„
                try:
                    angles['left_knee'] = safe_angle_between_vectors(keypoints[12], keypoints[13], keypoints[14])
                except:
                    angles['left_knee'] = 170.0
            
            return angles
            
        except Exception:
            return self._calculate_emergency_angles()

    def _safe_calculate_body_proportions(self, keypoints: List[List[float]]) -> Dict[str, float]:
        """ì•ˆì „í•œ ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°"""
        try:
            proportions = {}
            
            if len(keypoints) >= 18:
                # ì–´ê¹¨ ë„ˆë¹„
                try:
                    if len(keypoints[2]) >= 2 and len(keypoints[5]) >= 2:
                        shoulder_width = abs(keypoints[2][0] - keypoints[5][0])
                        proportions['shoulder_width'] = shoulder_width
                except:
                    proportions['shoulder_width'] = 160.0
                
                # ì—‰ë©ì´ ë„ˆë¹„
                try:
                    if len(keypoints[9]) >= 2 and len(keypoints[12]) >= 2:
                        hip_width = abs(keypoints[9][0] - keypoints[12][0])
                        proportions['hip_width'] = hip_width
                except:
                    proportions['hip_width'] = 120.0
                    
                # ì „ì²´ í‚¤
                try:
                    if len(keypoints[0]) >= 2 and len(keypoints[11]) >= 2:
                        total_height = abs(keypoints[0][1] - keypoints[11][1])
                        proportions['total_height'] = total_height
                except:
                    proportions['total_height'] = 400.0
            
            return proportions
            
        except Exception:
            return self._calculate_emergency_proportions()

    def _safe_build_skeleton_structure(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """ì•ˆì „í•œ ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±"""
        try:
            skeleton = {
                'connections': [],
                'bone_lengths': {},
                'valid_connections': 0
            }
            
            # ê¸°ë³¸ ì—°ê²°ë“¤ë§Œ ì‹œë„
            basic_connections = [(0, 1), (1, 2), (2, 3), (1, 5), (5, 6)]
            
            for i, (start_idx, end_idx) in enumerate(basic_connections):
                try:
                    if (start_idx < len(keypoints) and end_idx < len(keypoints) and
                        len(keypoints[start_idx]) >= 2 and len(keypoints[end_idx]) >= 2):
                        
                        start_kp = keypoints[start_idx]
                        end_kp = keypoints[end_idx]
                        
                        bone_length = ((start_kp[0] - end_kp[0])**2 + (start_kp[1] - end_kp[1])**2)**0.5
                        
                        connection = {
                            'start': start_idx,
                            'end': end_idx,
                            'length': bone_length,
                            'confidence': 0.7
                        }
                        
                        skeleton['connections'].append(connection)
                        skeleton['bone_lengths'][f"{start_idx}_{end_idx}"] = bone_length
                        skeleton['valid_connections'] += 1
                except:
                    continue
            
            return skeleton
            
        except Exception:
            return {'connections': [], 'bone_lengths': {}, 'valid_connections': 0}

    def _safe_extract_landmarks(self, keypoints: List[List[float]]) -> Dict[str, Dict[str, float]]:
        """ì•ˆì „í•œ ëœë“œë§ˆí¬ ì¶”ì¶œ"""
        try:
            landmarks = {}
            
            keypoint_names = [
                "nose", "neck", "right_shoulder", "right_elbow", "right_wrist",
                "left_shoulder", "left_elbow", "left_wrist", "middle_hip", "right_hip",
                "right_knee", "right_ankle", "left_hip", "left_knee", "left_ankle",
                "right_eye", "left_eye", "right_ear", "left_ear"
            ]
            
            for i, kp in enumerate(keypoints):
                try:
                    if i < len(keypoint_names) and len(kp) >= 3:
                        landmarks[keypoint_names[i]] = {
                            'x': float(kp[0]),
                            'y': float(kp[1]),
                            'confidence': float(kp[2])
                        }
                except:
                    continue
            
            return landmarks
            
        except Exception:
            return {}

    # ==============================================
    # ğŸ”¥ 6. ê°œì„ ëœ _load_all_ai_models_sync ë©”ì„œë“œ
    # ==============================================

    def _load_all_ai_models_sync(self):
        """ë™ê¸° AI ëª¨ë¸ ë¡œë”© (ì™„ì „ ì•ˆì •í™”)"""
        try:
            self.logger.info("ğŸ”„ í¬ì¦ˆ ì¶”ì • AI ëª¨ë¸ ë™ê¸° ë¡œë”©...")
            
            # ëª¨ë¸ ê²½ë¡œ íƒì§€
            model_paths = self._get_available_model_paths()
            
            loaded_count = 0
            
            # HRNet ë¡œë”© ì‹œë„
            if model_paths.get('hrnet'):
                try:
                    hrnet_model = self._load_hrnet_safe(model_paths['hrnet'])
                    if hrnet_model:
                        self.ai_models['hrnet'] = hrnet_model
                        loaded_count += 1
                        self.logger.info("âœ… HRNet ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                except Exception as e:
                    self.logger.debug(f"HRNet ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # OpenPose ë¡œë”© ì‹œë„
            if model_paths.get('openpose'):
                try:
                    openpose_model = self._load_openpose_safe(model_paths['openpose'])
                    if openpose_model:
                        self.ai_models['openpose'] = openpose_model
                        loaded_count += 1
                        self.logger.info("âœ… OpenPose ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                except Exception as e:
                    self.logger.debug(f"OpenPose ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # YOLOv8 ë¡œë”© ì‹œë„ (ultralytics ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
            if model_paths.get('yolov8') and ULTRALYTICS_AVAILABLE:
                try:
                    yolo_model = self._load_yolo_safe(model_paths['yolov8'])
                    if yolo_model:
                        self.ai_models['yolo'] = yolo_model
                        loaded_count += 1
                        self.logger.info("âœ… YOLOv8 ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                except Exception as e:
                    self.logger.debug(f"YOLOv8 ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # MediaPipeëŠ” ë³„ë„ ì²˜ë¦¬
            if self.mediapipe_integration and self.mediapipe_integration.available:
                self.ai_models['mediapipe'] = self.mediapipe_integration
                loaded_count += 1
                self.logger.info("âœ… MediaPipe ì‚¬ìš© ê°€ëŠ¥")
            
            if loaded_count == 0:
                self.logger.warning("âš ï¸ ì‹¤ì œ AI ëª¨ë¸ ì—†ìŒ, í´ë°± ëª¨ë¸ ìƒì„±")
                self._create_fallback_models()
                loaded_count = sum(self.models_loaded.values())
            
            self.logger.info(f"ğŸ“Š í¬ì¦ˆ ì¶”ì • AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self._create_fallback_models()

    def _get_available_model_paths(self) -> Dict[str, Optional[Path]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ê²½ë¡œ ë°˜í™˜"""
        try:
            model_mapper = Step02ModelMapper()
            return model_mapper.get_step02_model_paths()
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ê²½ë¡œ íƒì§€ ì‹¤íŒ¨: {e}")
            return {}

    # ==============================================
    # ğŸ”¥ 7. ëª¨ë¸ë³„ ì•ˆì „ ë¡œë”© ë©”ì„œë“œë“¤
    # ==============================================

    def _load_hrnet_safe(self, model_path: Path) -> Optional[Any]:
        """HRNet ì•ˆì „ ë¡œë”©"""
        try:
            checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)
            model = RealHRNetModel()
            model.load_state_dict(checkpoint, strict=False)
            model.to(self.device)
            model.eval()
            model.is_loaded = True
            return model
        except Exception as e:
            self.logger.debug(f"HRNet ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

    def _load_openpose_safe(self, model_path: Path) -> Optional[Any]:
        """OpenPose ì•ˆì „ ë¡œë”©"""
        try:
            openpose_model = RealOpenPoseModel(model_path, self.device)
            if openpose_model.load_openpose_checkpoint():
                return openpose_model
            return None
        except Exception as e:
            self.logger.debug(f"OpenPose ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

    def _load_yolo_safe(self, model_path: Path) -> Optional[Any]:
        """YOLOv8 ì•ˆì „ ë¡œë”©"""
        try:
            yolo_model = RealYOLOv8PoseModel(model_path, self.device)
            if yolo_model.load_yolo_checkpoint():
                return yolo_model
            return None
        except Exception as e:
            self.logger.debug(f"YOLOv8 ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

    def _run_multi_model_inference(self, image: Image.Image) -> Dict[str, Any]:
        """ë‹¤ì¤‘ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (2ë²ˆ íŒŒì¼ í˜¸í™˜)"""
        results = {}
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image_tensor = self._preprocess_image(image)
        image_np = np.array(image)
        
        # HRNet ì¶”ë¡  (ê³ í•´ìƒë„) - 2ë²ˆ íŒŒì¼ í˜¸í™˜
        if 'hrnet' in self.ai_models:
            try:
                hrnet_result = self.ai_models['hrnet'].detect_high_precision_pose(image)
                if hrnet_result.get('success'):
                    results['hrnet'] = {
                        'keypoints': hrnet_result['keypoints'],
                        'confidence': hrnet_result.get('confidence', 0.0),
                        'model_type': 'hrnet',
                        'priority': 0.9,  # ë†’ì€ ìš°ì„ ìˆœìœ„
                        'processing_time': hrnet_result.get('processing_time', 0.0)
                    }
                    self.logger.debug("âœ… HRNet ì¶”ë¡  ì™„ë£Œ")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ HRNet ì¶”ë¡  ì‹¤íŒ¨: {e}")
        
        # OpenPose ì¶”ë¡  (PAF + íˆíŠ¸ë§µ) - 2ë²ˆ íŒŒì¼ í˜¸í™˜
        if 'openpose' in self.ai_models:
            try:
                openpose_result = self.ai_models['openpose'].detect_keypoints_precise(image)
                if openpose_result.get('success'):
                    results['openpose'] = {
                        'keypoints': openpose_result['keypoints'],
                        'confidence': np.mean([kp[2] for kp in openpose_result['keypoints'] if kp[2] > 0.1]),
                        'model_type': 'openpose',
                        'priority': 0.85,
                        'processing_time': openpose_result.get('processing_time', 0.0)
                    }
                    self.logger.debug("âœ… OpenPose ì¶”ë¡  ì™„ë£Œ")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ OpenPose ì¶”ë¡  ì‹¤íŒ¨: {e}")
        
        # YOLOv8 ì¶”ë¡  (ì‹¤ì‹œê°„) - 2ë²ˆ íŒŒì¼ í˜¸í™˜
        if 'yolo' in self.ai_models:
            try:
                yolo_result = self.ai_models['yolo'].detect_poses_realtime(image)
                if yolo_result.get('success') and yolo_result.get('keypoints'):
                    results['yolo'] = {
                        'keypoints': yolo_result['keypoints'],
                        'confidence': np.mean([kp[2] for kp in yolo_result['keypoints'] if kp[2] > 0.1]),
                        'model_type': 'yolov8',
                        'priority': 0.8,
                        'processing_time': yolo_result.get('processing_time', 0.0)
                    }
                    self.logger.debug("âœ… YOLOv8 ì¶”ë¡  ì™„ë£Œ")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ YOLOv8 ì¶”ë¡  ì‹¤íŒ¨: {e}")
        
        # Body Pose ì¶”ë¡  (ë³´ì¡°) - 2ë²ˆ íŒŒì¼ í˜¸í™˜
        if 'body_pose' in self.ai_models:
            try:
                body_pose_result = self.ai_models['body_pose'].detect_body_pose(image)
                if body_pose_result.get('success'):
                    results['body_pose'] = {
                        'keypoints': body_pose_result['keypoints'],
                        'confidence': np.mean([kp[2] for kp in body_pose_result['keypoints'] if kp[2] > 0.1]),
                        'model_type': 'body_pose',
                        'priority': 0.6,
                        'processing_time': body_pose_result.get('processing_time', 0.0)
                    }
                    self.logger.debug("âœ… Body Pose ì¶”ë¡  ì™„ë£Œ")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ Body Pose ì¶”ë¡  ì‹¤íŒ¨: {e}")
        
        # MediaPipe ì¶”ë¡  (ì‹¤ì‹œê°„)
        if 'mediapipe' in self.ai_models:
            try:
                mp_result = self.ai_models['mediapipe'].detect_pose_landmarks(image_np)
                if mp_result['success']:
                    # MediaPipe 33 â†’ OpenPose 18 ë³€í™˜
                    keypoints = self._convert_mediapipe_to_openpose18(mp_result['landmarks'])
                    
                    results['mediapipe'] = {
                        'keypoints': keypoints,
                        'confidence': np.mean([kp[2] for kp in keypoints if kp[2] > 0.1]),
                        'model_type': 'mediapipe',
                        'priority': 0.75,
                        'segmentation_mask': mp_result.get('segmentation_mask')
                    }
                    self.logger.debug("âœ… MediaPipe ì¶”ë¡  ì™„ë£Œ")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ MediaPipe ì¶”ë¡  ì‹¤íŒ¨: {e}")
        
        # Diffusion í’ˆì§ˆ í–¥ìƒ (ì„ íƒì ) - 2ë²ˆ íŒŒì¼ í˜¸í™˜
        if 'diffusion' in self.ai_models and results:
            try:
                # ìµœê³  ìš°ì„ ìˆœìœ„ ê²°ê³¼ë¥¼ Diffusionìœ¼ë¡œ í–¥ìƒ
                best_result = max(results.values(), key=lambda x: x.get('priority', 0) * x.get('confidence', 0))
                if best_result.get('keypoints'):
                    diffusion_result = self.ai_models['diffusion'].enhance_pose_quality(
                        best_result['keypoints'], image
                    )
                    if diffusion_result.get('success'):
                        results['diffusion_enhanced'] = {
                            'keypoints': diffusion_result['enhanced_keypoints'],
                            'confidence': best_result['confidence'] * 1.1,  # ì•½ê°„ í–¥ìƒ
                            'model_type': 'diffusion_enhanced',
                            'priority': 0.95,  # ìµœê³  ìš°ì„ ìˆœìœ„
                            'base_model': best_result['model_type'],
                            'processing_time': diffusion_result.get('processing_time', 0.0)
                        }
                        self.logger.debug("âœ… Diffusion í’ˆì§ˆ í–¥ìƒ ì™„ë£Œ")
                        
            except Exception as e:
                self.logger.warning(f"âš ï¸ Diffusion í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
        
        return results
    
    def _preprocess_image(self, image: Image.Image) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        # í¬ê¸° ì¡°ì •
        image_resized = image.resize((256, 256), Image.Resampling.BILINEAR)
        
        # í…ì„œ ë³€í™˜ ë° ì •ê·œí™”
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        image_tensor = transform(image_resized).unsqueeze(0).to(self.device)
        return image_tensor
    
    def _convert_coco17_to_openpose18(self, coco_keypoints: np.ndarray) -> List[List[float]]:
        """COCO 17 â†’ OpenPose 18 ë³€í™˜"""
        openpose_keypoints = [[0.0, 0.0, 0.0] for _ in range(18)]
        
        # COCO â†’ OpenPose ë§¤í•‘
        coco_to_openpose = {
            0: 0,   # nose
            5: 2,   # left_shoulder â†’ right_shoulder
            6: 5,   # right_shoulder â†’ left_shoulder
            7: 3,   # left_elbow â†’ right_elbow
            8: 6,   # right_elbow â†’ left_elbow
            9: 4,   # left_wrist â†’ right_wrist
            10: 7,  # right_wrist â†’ left_wrist
            11: 9,  # left_hip â†’ right_hip
            12: 12, # right_hip â†’ left_hip
            13: 10, # left_knee â†’ right_knee
            14: 13, # right_knee â†’ left_knee
            15: 11, # left_ankle â†’ right_ankle
            16: 14, # right_ankle â†’ left_ankle
            1: 15,  # left_eye â†’ right_eye
            2: 16,  # right_eye â†’ left_eye
            3: 17,  # left_ear â†’ right_ear
            4: 18   # right_ear â†’ left_ear
        }
        
        # neck ê³„ì‚° (ì–´ê¹¨ ì¤‘ì )
        if len(coco_keypoints) > 6:
            left_shoulder = coco_keypoints[5]
            right_shoulder = coco_keypoints[6]
            if left_shoulder[2] > 0.1 and right_shoulder[2] > 0.1:
                neck_x = (left_shoulder[0] + right_shoulder[0]) / 2
                neck_y = (left_shoulder[1] + right_shoulder[1]) / 2
                neck_conf = (left_shoulder[2] + right_shoulder[2]) / 2
                openpose_keypoints[1] = [float(neck_x), float(neck_y), float(neck_conf)]
        
        # ë‚˜ë¨¸ì§€ í‚¤í¬ì¸íŠ¸ ë§¤í•‘
        for coco_idx, openpose_idx in coco_to_openpose.items():
            if coco_idx < len(coco_keypoints) and openpose_idx < 18:
                openpose_keypoints[openpose_idx] = [
                    float(coco_keypoints[coco_idx][0]),
                    float(coco_keypoints[coco_idx][1]),
                    float(coco_keypoints[coco_idx][2])
                ]
        
        return openpose_keypoints
    
    def _convert_mediapipe_to_openpose18(self, mp_landmarks: List[List[float]]) -> List[List[float]]:
        """MediaPipe 33 â†’ OpenPose 18 ë³€í™˜"""
        if len(mp_landmarks) < 33:
            return [[0.0, 0.0, 0.0] for _ in range(18)]
        
        # MediaPipe â†’ OpenPose ë§¤í•‘ (ì£¼ìš” í¬ì¸íŠ¸ë§Œ)
        mp_to_openpose = {
            0: 0,   # nose
            12: 2,  # right_shoulder
            11: 5,  # left_shoulder
            14: 3,  # right_elbow
            13: 6,  # left_elbow
            16: 4,  # right_wrist
            15: 7,  # left_wrist
            24: 9,  # right_hip
            23: 12, # left_hip
            26: 10, # right_knee
            25: 13, # left_knee
            28: 11, # right_ankle
            27: 14, # left_ankle
            2: 15,  # right_eye
            5: 16,  # left_eye
            8: 17,  # right_ear
            7: 18   # left_ear
        }
        
        openpose_keypoints = [[0.0, 0.0, 0.0] for _ in range(18)]
        
        # neck ê³„ì‚° (ì–´ê¹¨ ì¤‘ì )
        if len(mp_landmarks) > 12:
            left_shoulder = mp_landmarks[11]
            right_shoulder = mp_landmarks[12]
            if left_shoulder[3] > 0.5 and right_shoulder[3] > 0.5:  # visibility
                neck_x = (left_shoulder[0] + right_shoulder[0]) / 2 * 256  # ì´ë¯¸ì§€ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼
                neck_y = (left_shoulder[1] + right_shoulder[1]) / 2 * 256
                neck_conf = (left_shoulder[3] + right_shoulder[3]) / 2
                openpose_keypoints[1] = [neck_x, neck_y, neck_conf]
        
        # ë‚˜ë¨¸ì§€ í‚¤í¬ì¸íŠ¸ ë§¤í•‘
        for mp_idx, openpose_idx in mp_to_openpose.items():
            if mp_idx < len(mp_landmarks) and openpose_idx < 18:
                mp_point = mp_landmarks[mp_idx]
                openpose_keypoints[openpose_idx] = [
                    mp_point[0] * 256,  # x * image_width
                    mp_point[1] * 256,  # y * image_height
                    mp_point[3]         # visibility as confidence
                ]
        
        return openpose_keypoints
    
    def _fuse_and_analyze_results(self, results: Dict[str, Any], image: Image.Image) -> Dict[str, Any]:
        """ê²°ê³¼ ìœµí•© ë° ë¶„ì„"""
        try:
            # ìµœì  ê²°ê³¼ ì„ íƒ (ì‹ ë¢°ë„ + ìš°ì„ ìˆœìœ„ ê¸°ë°˜)
            best_result = self._ensemble_fusion(results)
            
            if not best_result:
                raise ValueError("ìœ íš¨í•œ ì¶”ë¡  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            keypoints = best_result['keypoints']
            
            # ë¶ˆí™•ì‹¤ì„± ì¶”ì •ê³¼ í•¨ê»˜ í‚¤í¬ì¸íŠ¸ ì¬ì¶”ì¶œ
            keypoints_with_uncertainty = []
            if 'heatmaps' in best_result:
                keypoints_with_uncertainty = self.analyzer.extract_keypoints_with_uncertainty(
                    best_result['heatmaps']
                )
            else:
                keypoints_with_uncertainty = [
                    {'position': kp[:2], 'confidence': kp[2], 'uncertainty': 0.5, 'distribution': None}
                    for kp in keypoints
                ]
            
            # ê´€ì ˆ ê°ë„ ê³„ì‚°
            joint_angles = self.analyzer.calculate_joint_angles(keypoints)
            
            # ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°
            body_proportions = self.analyzer.calculate_body_proportions(keypoints)
            
            # í¬ì¦ˆ í’ˆì§ˆ í‰ê°€
            quality_assessment = self.analyzer.assess_pose_quality(
                keypoints, joint_angles, body_proportions
            )
            
            # ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±
            skeleton_structure = self._build_skeleton_structure(keypoints)
            
            # ëœë“œë§ˆí¬ ì¶”ì¶œ
            landmarks = self._extract_landmarks(keypoints)
            
            # ëª¨ë¸ ì•™ìƒë¸” ì •ë³´
            ensemble_info = {
                'primary_model': best_result['model_type'],
                'models_used': list(results.keys()),
                'confidence_weights': {k: v.get('priority', 0.5) for k, v in results.items()},
                'fusion_method': 'weighted_priority_ensemble'
            }
            
            return {
                'keypoints': keypoints,
                'keypoints_with_uncertainty': keypoints_with_uncertainty,
                'confidence_scores': [kp[2] for kp in keypoints],
                'joint_angles': joint_angles,
                'body_proportions': body_proportions,
                'quality_assessment': quality_assessment,
                'skeleton_structure': skeleton_structure,
                'landmarks': landmarks,
                'ensemble_info': ensemble_info,
                'overall_confidence': best_result['confidence'],
                'best_model': best_result['model_type']
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê²°ê³¼ ìœµí•© ë° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'keypoints': [],
                'keypoints_with_uncertainty': [],
                'confidence_scores': [],
                'joint_angles': {},
                'body_proportions': {},
                'quality_assessment': {},
                'skeleton_structure': {},
                'landmarks': {},
                'ensemble_info': {},
                'overall_confidence': 0.0,
                'best_model': 'none'
            }
    
    def _ensemble_fusion(self, results: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì•™ìƒë¸” ìœµí•©"""
        if not results:
            return None
        
        # ìš°ì„ ìˆœìœ„ì™€ ì‹ ë¢°ë„ë¥¼ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜ ê³„ì‚°
        weighted_results = []
        
        for model_name, result in results.items():
            if result.get('keypoints') and result.get('confidence', 0) > 0.1:
                priority = result.get('priority', 0.5)
                confidence = result.get('confidence', 0.0)
                
                # ìµœì¢… ì ìˆ˜ = ìš°ì„ ìˆœìœ„ * ì‹ ë¢°ë„
                final_score = priority * confidence
                
                weighted_results.append((final_score, model_name, result))
        
        if not weighted_results:
            return None
        
        # ìµœê³  ì ìˆ˜ ê²°ê³¼ ì„ íƒ
        weighted_results.sort(key=lambda x: x[0], reverse=True)
        best_score, best_model, best_result = weighted_results[0]
        
        self.logger.info(f"ğŸ† ì•™ìƒë¸” ìœµí•© ê²°ê³¼: {best_model} (ì ìˆ˜: {best_score:.3f})")
        
        return best_result
    
    def _build_skeleton_structure(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±"""
        skeleton = {
            'connections': [],
            'bone_lengths': {},
            'valid_connections': 0
        }
        
        for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
            if (start_idx < len(keypoints) and end_idx < len(keypoints) and
                len(keypoints[start_idx]) >= 3 and len(keypoints[end_idx]) >= 3):
                
                start_kp = keypoints[start_idx]
                end_kp = keypoints[end_idx]
                
                if start_kp[2] > self.confidence_threshold and end_kp[2] > self.confidence_threshold:
                    bone_length = np.sqrt(
                        (start_kp[0] - end_kp[0])**2 + (start_kp[1] - end_kp[1])**2
                    )
                    
                    connection = {
                        'start': start_idx,
                        'end': end_idx,
                        'start_name': OPENPOSE_18_KEYPOINTS[start_idx] if start_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{start_idx}",
                        'end_name': OPENPOSE_18_KEYPOINTS[end_idx] if end_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{end_idx}",
                        'length': bone_length,
                        'confidence': (start_kp[2] + end_kp[2]) / 2
                    }
                    
                    skeleton['connections'].append(connection)
                    skeleton['bone_lengths'][f"{start_idx}_{end_idx}"] = bone_length
                    skeleton['valid_connections'] += 1
        
        return skeleton
    
    def _extract_landmarks(self, keypoints: List[List[float]]) -> Dict[str, Dict[str, float]]:
        """ì£¼ìš” ëœë“œë§ˆí¬ ì¶”ì¶œ"""
        landmarks = {}
        
        for i, kp in enumerate(keypoints):
            if len(kp) >= 3 and kp[2] > self.confidence_threshold:
                landmark_name = OPENPOSE_18_KEYPOINTS[i] if i < len(OPENPOSE_18_KEYPOINTS) else f"landmark_{i}"
                landmarks[landmark_name] = {
                    'x': float(kp[0]),
                    'y': float(kp[1]),
                    'confidence': float(kp[2])
                }
        
        return landmarks

# ==============================================
# ğŸ”¥ 11. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì™„ì „ í†µí•©)
# ==============================================

def validate_keypoints(keypoints: List[List[float]]) -> bool:
    """í‚¤í¬ì¸íŠ¸ ìœ íš¨ì„± ê²€ì¦"""
    try:
        if len(keypoints) != 18:
            return False
        
        for kp in keypoints:
            if len(kp) != 3:
                return False
            if not all(isinstance(x, (int, float)) for x in kp):
                return False
            if kp[2] < 0 or kp[2] > 1:
                return False
        
        return True
        
    except Exception:
        return False

def draw_pose_on_image(
    image: Union[np.ndarray, Image.Image],
    keypoints: List[List[float]],
    confidence_threshold: float = 0.5,
    keypoint_size: int = 4,
    line_width: int = 3
) -> Image.Image:
    """ì´ë¯¸ì§€ì— í¬ì¦ˆ ê·¸ë¦¬ê¸°"""
    try:
        if isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        else:
            pil_image = image.copy()
        
        draw = ImageDraw.Draw(pil_image)
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for i, kp in enumerate(keypoints):
            if len(kp) >= 3 and kp[2] > confidence_threshold:
                x, y = int(kp[0]), int(kp[1])
                color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                
                radius = int(keypoint_size + kp[2] * 6)
                draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                           fill=color, outline=(255, 255, 255), width=2)
        
        # ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸°
        for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
            if (start_idx < len(keypoints) and end_idx < len(keypoints)):
                start_kp = keypoints[start_idx]
                end_kp = keypoints[end_idx]
                
                if (len(start_kp) >= 3 and len(end_kp) >= 3 and
                    start_kp[2] > confidence_threshold and end_kp[2] > confidence_threshold):
                    
                    start_point = (int(start_kp[0]), int(start_kp[1]))
                    end_point = (int(end_kp[0]), int(end_kp[1]))
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    avg_confidence = (start_kp[2] + end_kp[2]) / 2
                    adjusted_width = int(line_width * avg_confidence)
                    
                    draw.line([start_point, end_point], fill=color, width=max(1, adjusted_width))
        
        return pil_image
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
        return image if isinstance(image, Image.Image) else Image.fromarray(image)

def analyze_pose_for_clothing(
    keypoints: List[List[float]],
    clothing_type: str = "default",
    confidence_threshold: float = 0.5
) -> Dict[str, Any]:
    """ì˜ë¥˜ë³„ í¬ì¦ˆ ì í•©ì„± ë¶„ì„"""
    try:
        if not keypoints:
            return {
                'suitable_for_fitting': False,
                'issues': ["í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"],
                'recommendations': ["ë” ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”"],
                'pose_score': 0.0
            }
        
        # ì˜ë¥˜ë³„ ê°€ì¤‘ì¹˜
        clothing_weights = {
            'shirt': {'arms': 0.4, 'torso': 0.4, 'visibility': 0.2},
            'dress': {'torso': 0.5, 'arms': 0.3, 'legs': 0.2},
            'pants': {'legs': 0.6, 'torso': 0.3, 'visibility': 0.1},
            'jacket': {'arms': 0.5, 'torso': 0.4, 'visibility': 0.1},
            'default': {'torso': 0.4, 'arms': 0.3, 'legs': 0.2, 'visibility': 0.1}
        }
        
        weights = clothing_weights.get(clothing_type, clothing_weights['default'])
        
        # ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°
        def calculate_body_part_score(part_indices: List[int]) -> float:
            visible_count = 0
            total_confidence = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            
            return visibility_ratio * avg_confidence
        
        # ë¶€ìœ„ë³„ ì ìˆ˜
        torso_indices = [1, 2, 5, 8, 9, 12]
        arm_indices = [2, 3, 4, 5, 6, 7]
        leg_indices = [9, 10, 11, 12, 13, 14]
        
        torso_score = calculate_body_part_score(torso_indices)
        arms_score = calculate_body_part_score(arm_indices)
        legs_score = calculate_body_part_score(leg_indices)
        
        # í¬ì¦ˆ ì ìˆ˜
        pose_score = (
            torso_score * weights.get('torso', 0.4) +
            arms_score * weights.get('arms', 0.3) +
            legs_score * weights.get('legs', 0.2) +
            weights.get('visibility', 0.1) * min(1.0, (torso_score + arms_score + legs_score) / 3)
        )
        
        # ì í•©ì„± íŒë‹¨
        suitable_for_fitting = pose_score >= 0.7
        
        # ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­
        issues = []
        recommendations = []
        
        if pose_score < 0.7:
            issues.append(f'{clothing_type} ì°©ìš©ì— ì í•©í•˜ì§€ ì•Šì€ í¬ì¦ˆ')
            recommendations.append('ë” ëª…í™•í•œ í¬ì¦ˆë¡œ ë‹¤ì‹œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        if torso_score < 0.5:
            issues.append('ìƒì²´ê°€ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤')
            recommendations.append('ìƒì²´ ì „ì²´ê°€ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        return {
            'suitable_for_fitting': suitable_for_fitting,
            'issues': issues,
            'recommendations': recommendations,
            'pose_score': pose_score,
            'detailed_scores': {
                'torso': torso_score,
                'arms': arms_score,
                'legs': legs_score
            },
            'clothing_type': clothing_type
        }
        
    except Exception as e:
        logger.error(f"ì˜ë¥˜ë³„ í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            'suitable_for_fitting': False,
            'issues': ["ë¶„ì„ ì‹¤íŒ¨"],
            'recommendations': ["ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”"],
            'pose_score': 0.0
        }

# ==============================================
# ğŸ”¥ 15. íŒŒì´í”„ë¼ì¸ ì§€ì› (2ë²ˆ íŒŒì¼ ì™„ì „ í†µí•©)
# ==============================================

@dataclass
class PipelineStepResult:
    """íŒŒì´í”„ë¼ì¸ Step ê²°ê³¼ ë°ì´í„° êµ¬ì¡° (2ë²ˆ íŒŒì¼ í˜¸í™˜)"""
    step_id: int
    step_name: str
    success: bool
    error: Optional[str] = None
    
    # ë‹¤ìŒ Stepë“¤ë¡œ ì „ë‹¬í•  ë°ì´í„°
    for_step_03: Dict[str, Any] = field(default_factory=dict)
    for_step_04: Dict[str, Any] = field(default_factory=dict)
    for_step_05: Dict[str, Any] = field(default_factory=dict)
    for_step_06: Dict[str, Any] = field(default_factory=dict)
    for_step_07: Dict[str, Any] = field(default_factory=dict)
    for_step_08: Dict[str, Any] = field(default_factory=dict)
    
    # ì´ì „ ë‹¨ê³„ ë°ì´í„° ë³´ì¡´
    previous_data: Dict[str, Any] = field(default_factory=dict)
    original_data: Dict[str, Any] = field(default_factory=dict)
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)
    processing_time: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        from dataclasses import asdict
        return asdict(self)

@dataclass 
class PipelineInputData:
    """íŒŒì´í”„ë¼ì¸ ì…ë ¥ ë°ì´í„° (2ë²ˆ íŒŒì¼ í˜¸í™˜)"""
    person_image: Union[np.ndarray, Image.Image, str]
    clothing_image: Optional[Union[np.ndarray, Image.Image, str]] = None
    session_id: Optional[str] = None
    config: Dict[str, Any] = field(default_factory=dict)

class PoseEstimationStepWithPipeline(PoseEstimationStep):
    """íŒŒì´í”„ë¼ì¸ ì§€ì›ì´ í¬í•¨ëœ PoseEstimationStep (2ë²ˆ íŒŒì¼ ì™„ì „ í˜¸í™˜)"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì„¤ì •
        self.pipeline_mode = kwargs.get("pipeline_mode", False)
        
        # íŒŒì´í”„ë¼ì¸ ì†ì„±
        self.pipeline_position = "middle"  # Step 02ëŠ” ì¤‘ê°„ ë‹¨ê³„
        self.accepts_pipeline_input = True
        self.provides_pipeline_output = True
    
    async def process_pipeline(self, input_data: PipelineStepResult) -> PipelineStepResult:
        """
        íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì²˜ë¦¬ - Step 01 ê²°ê³¼ë¥¼ ë°›ì•„ í¬ì¦ˆ ì¶”ì • í›„ Step 03, 04ë¡œ ì „ë‹¬ (2ë²ˆ íŒŒì¼ í˜¸í™˜)
        
        Args:
            input_data: Step 01ì—ì„œ ì „ë‹¬ë°›ì€ íŒŒì´í”„ë¼ì¸ ë°ì´í„°
            
        Returns:
            PipelineStepResult: Step 03, 04ë¡œ ì „ë‹¬í•  í¬ì¦ˆ ì¶”ì • ê²°ê³¼
        """
        try:
            start_time = time.time()
            self.logger.info(f"ğŸ”— {self.step_name} íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì²˜ë¦¬ ì‹œì‘")
            
            # ì´ˆê¸°í™” ê²€ì¦
            if not self.is_initialized:
                if not await self.initialize():
                    error_msg = "íŒŒì´í”„ë¼ì¸: AI ì´ˆê¸°í™” ì‹¤íŒ¨"
                    return PipelineStepResult(
                        step_id=2, step_name="pose_estimation",
                        success=False, error=error_msg
                    )
            
            # Step 01 ê²°ê³¼ ë°›ê¸°
            if not hasattr(input_data, 'for_step_02') or not input_data.for_step_02:
                error_msg = "Step 01 ë°ì´í„°ê°€ ì—†ìŒ"
                self.logger.error(f"âŒ {error_msg}")
                return PipelineStepResult(
                    step_id=2, step_name="pose_estimation",
                    success=False, error=error_msg
                )
            
            step01_data = input_data.for_step_02
            parsed_image = step01_data.get("parsed_image")
            body_masks = step01_data.get("body_masks", {})
            human_region = step01_data.get("human_region")
            
            if parsed_image is None:
                error_msg = "Step 01ì—ì„œ íŒŒì‹±ëœ ì´ë¯¸ì§€ê°€ ì—†ìŒ"
                self.logger.error(f"âŒ {error_msg}")
                return PipelineStepResult(
                    step_id=2, step_name="pose_estimation",
                    success=False, error=error_msg
                )
            
            # íŒŒì´í”„ë¼ì¸ìš© í¬ì¦ˆ ì¶”ì • AI ì²˜ë¦¬ (ë™ê¸° ì²˜ë¦¬)
            pose_result = self._run_pose_estimation_pipeline_sync(parsed_image, body_masks, human_region)
            
            if not pose_result.get('success', False):
                error_msg = f"íŒŒì´í”„ë¼ì¸ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {pose_result.get('error', 'Unknown Error')}"
                self.logger.error(f"âŒ {error_msg}")
                return PipelineStepResult(
                    step_id=2, step_name="pose_estimation",
                    success=False, error=error_msg
                )
            
            # íŒŒì´í”„ë¼ì¸ ë°ì´í„° ì¤€ë¹„
            pipeline_data = PipelineStepResult(
                step_id=2,
                step_name="pose_estimation",
                success=True,
                
                # Step 03 (Cloth Segmentation)ìœ¼ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_03={
                    **getattr(input_data, 'for_step_03', {}),  # Step 01 ë°ì´í„° ê³„ìŠ¹
                    "pose_keypoints": pose_result["keypoints"],
                    "pose_skeleton": pose_result.get("skeleton_structure", {}),
                    "pose_confidence": pose_result.get("confidence_scores", []),
                    "joint_connections": pose_result.get("joint_connections", []),
                    "visible_keypoints": pose_result.get("visible_keypoints", [])
                },
                
                # Step 04 (Geometric Matching)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_04={
                    "keypoints_for_matching": pose_result["keypoints"],
                    "joint_connections": pose_result.get("joint_connections", []),
                    "pose_angles": pose_result.get("joint_angles", {}),
                    "body_orientation": pose_result.get("body_orientation", {}),
                    "pose_landmarks": pose_result.get("landmarks", {}),
                    "skeleton_structure": pose_result.get("skeleton_structure", {})
                },
                
                # Step 05 (Cloth Warping)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_05={
                    "reference_keypoints": pose_result["keypoints"],
                    "body_proportions": pose_result.get("body_proportions", {}),
                    "pose_type": pose_result.get("pose_type", "standing")
                },
                
                # Step 06 (Virtual Fitting)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_06={
                    "person_keypoints": pose_result["keypoints"],
                    "pose_confidence": pose_result.get("confidence_scores", []),
                    "body_orientation": pose_result.get("body_orientation", {})
                },
                
                # Step 07 (Post Processing)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_07={
                    "original_keypoints": pose_result["keypoints"]
                },
                
                # Step 08 (Quality Assessment)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_08={
                    "pose_quality_metrics": pose_result.get("pose_analysis", {}),
                    "keypoints_confidence": pose_result.get("confidence_scores", [])
                },
                
                # ì´ì „ ë‹¨ê³„ ë°ì´í„° ë³´ì¡´ ë° í™•ì¥
                previous_data={
                    **getattr(input_data, 'original_data', {}),
                    "step01_results": getattr(input_data, 'for_step_02', {}),
                    "step02_results": pose_result
                },
                
                original_data=getattr(input_data, 'original_data', {}),
                
                # ë©”íƒ€ë°ì´í„°
                metadata={
                    "processing_time": time.time() - start_time,
                    "ai_models_used": pose_result.get("models_used", []),
                    "num_keypoints_detected": len(pose_result.get("keypoints", [])),
                    "ready_for_next_steps": ["step_03", "step_04", "step_05", "step_06"],
                    "execution_mode": "pipeline",
                    "pipeline_progress": "2/8 ë‹¨ê³„ ì™„ë£Œ",
                    "primary_model": pose_result.get("primary_model", "unknown"),
                    "enhanced_by_diffusion": pose_result.get("enhanced_by_diffusion", False)
                },
                
                processing_time=time.time() - start_time
            )
            
            self.logger.info(f"âœ… {self.step_name} íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì²˜ë¦¬ ì™„ë£Œ")
            self.logger.info(f"ğŸ¯ ê²€ì¶œëœ í‚¤í¬ì¸íŠ¸: {len(pose_result.get('keypoints', []))}ê°œ")
            self.logger.info(f"â¡ï¸ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë°ì´í„° ì „ë‹¬ ì¤€ë¹„ ì™„ë£Œ")
            
            return pipeline_data
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìŠ¤íƒ: {traceback.format_exc()}")
            return PipelineStepResult(
                step_id=2, step_name="pose_estimation",
                success=False, error=str(e),
                processing_time=time.time() - start_time if 'start_time' in locals() else 0.0
            )
    
    def _run_pose_estimation_pipeline_sync(
        self, 
        parsed_image: Union[torch.Tensor, np.ndarray, Image.Image], 
        body_masks: Dict[str, Any],
        human_region: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì „ìš© í¬ì¦ˆ ì¶”ì • AI ì²˜ë¦¬ (ë™ê¸° ì²˜ë¦¬) - 2ë²ˆ íŒŒì¼ í˜¸í™˜"""
        try:
            inference_start = time.time()
            self.logger.info(f"ğŸ§  íŒŒì´í”„ë¼ì¸ í¬ì¦ˆ ì¶”ì • AI ì‹œì‘ (ë™ê¸°)...")
            
            if not self.ai_models:
                import asyncio
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(self._load_all_ai_models_sync())
                loop.close()
            
            if not self.ai_models:
                return {'success': False, 'error': 'ë¡œë”©ëœ AI ëª¨ë¸ì´ ì—†ìŒ'}
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (íŒŒì´í”„ë¼ì¸ìš©)
            if isinstance(parsed_image, torch.Tensor):
                image = to_pil_image(parsed_image.cpu())
            elif isinstance(parsed_image, np.ndarray):
                image = Image.fromarray(parsed_image)
            else:
                image = parsed_image
            
            # Body masks í™œìš©í•œ ê´€ì‹¬ ì˜ì—­ ì¶”ì¶œ
            if body_masks and human_region:
                # ì¸ì²´ ì˜ì—­ì— ì§‘ì¤‘í•œ í¬ì¦ˆ ì¶”ì •
                image = self._focus_on_human_region_sync(image, human_region)
            
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰ (ë™ê¸° ì²˜ë¦¬) - 2ë²ˆ íŒŒì¼ í˜¸í™˜
            results = self._run_multi_model_inference(image)
            
            # ê²°ê³¼ ìœµí•© ë° ë¶„ì„
            final_result = self._fuse_and_analyze_results(results, image)
            
            if not final_result or not final_result.get('keypoints'):
                return {'success': False, 'error': 'ëª¨ë“  AI ëª¨ë¸ì—ì„œ ìœ íš¨í•œ í¬ì¦ˆë¥¼ ê²€ì¶œí•˜ì§€ ëª»í•¨'}
            
            # íŒŒì´í”„ë¼ì¸ ì „ìš© ì¶”ê°€ ë¶„ì„
            pipeline_analysis = self._analyze_for_pipeline_sync(final_result, body_masks)
            final_result.update(pipeline_analysis)
            
            inference_time = time.time() - inference_start
            final_result['inference_time'] = inference_time
            final_result['success'] = True
            
            self.logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ í¬ì¦ˆ ì¶”ì • AI ì™„ë£Œ ({inference_time:.3f}ì´ˆ)")
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ í¬ì¦ˆ ì¶”ì • AI ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _focus_on_human_region_sync(self, image: Image.Image, human_region: Dict[str, Any]) -> Image.Image:
        """ì¸ì²´ ì˜ì—­ì— ì§‘ì¤‘í•œ ì´ë¯¸ì§€ ì²˜ë¦¬ (ë™ê¸° ì²˜ë¦¬)"""
        try:
            if 'bbox' in human_region:
                bbox = human_region['bbox']
                x1, y1, x2, y2 = bbox
                # ì¸ì²´ ì˜ì—­ í¬ë¡­
                cropped = image.crop((x1, y1, x2, y2))
                # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                return cropped.resize(image.size, Image.Resampling.BILINEAR)
            return image
        except Exception as e:
            self.logger.debug(f"ì¸ì²´ ì˜ì—­ ì§‘ì¤‘ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def _analyze_for_pipeline_sync(self, ai_result: Dict[str, Any], body_masks: Dict[str, Any]) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì „ìš© ì¶”ê°€ ë¶„ì„ (ë™ê¸° ì²˜ë¦¬) - 2ë²ˆ íŒŒì¼ í˜¸í™˜"""
        try:
            keypoints = ai_result.get('keypoints', [])
            
            # ê°€ì‹œì„± ë¶„ì„ (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ í™œìš©)
            visible_keypoints = []
            confidence_threshold = 0.5
            
            for i, kp in enumerate(keypoints):
                if len(kp) >= 3 and kp[2] > confidence_threshold:
                    keypoint_name = OPENPOSE_18_KEYPOINTS[i] if i < len(OPENPOSE_18_KEYPOINTS) else f"kp_{i}"
                    visible_keypoints.append({
                        'index': i,
                        'name': keypoint_name,
                        'position': [kp[0], kp[1]],
                        'confidence': kp[2]
                    })
            
            # í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜ (ë‹¤ìŒ ë‹¨ê³„ ìµœì í™”ìš©)
            pose_type = self._classify_pose_type_sync(keypoints)
            
            # Body masksì™€ì˜ ì¼ì¹˜ì„± ë¶„ì„
            mask_consistency = self._analyze_mask_consistency_sync(keypoints, body_masks)
            
            return {
                'visible_keypoints': visible_keypoints,
                'pose_type': pose_type,
                'mask_consistency': mask_consistency,
                'pipeline_ready': True
            }
            
        except Exception as e:
            self.logger.debug(f"íŒŒì´í”„ë¼ì¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'pipeline_ready': False}
    
    def _classify_pose_type_sync(self, keypoints: List[List[float]]) -> str:
        """í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜ (ë™ê¸° ì²˜ë¦¬)"""
        try:
            if not keypoints or len(keypoints) < 18:
                return "unknown"
            
            # íŒ” ê°ë„ ë¶„ì„
            arms_extended = False
            if all(i < len(keypoints) and len(keypoints[i]) >= 3 for i in [2, 3, 4, 5, 6, 7]):
                # íŒ”ì´ í¼ì³ì ¸ ìˆëŠ”ì§€ í™•ì¸
                right_arm_angle = self._calculate_arm_angle_sync(keypoints[2], keypoints[3], keypoints[4])
                left_arm_angle = self._calculate_arm_angle_sync(keypoints[5], keypoints[6], keypoints[7])
                
                if right_arm_angle > 150 and left_arm_angle > 150:
                    arms_extended = True
            
            # ë‹¤ë¦¬ ë¶„ì„
            legs_apart = False
            if all(i < len(keypoints) and len(keypoints[i]) >= 3 for i in [9, 12, 11, 14]):
                hip_distance = abs(keypoints[9][0] - keypoints[12][0])
                ankle_distance = abs(keypoints[11][0] - keypoints[14][0])
                if ankle_distance > hip_distance * 1.5:
                    legs_apart = True
            
            # í¬ì¦ˆ ë¶„ë¥˜
            if arms_extended and not legs_apart:
                return "t_pose"
            elif arms_extended and legs_apart:
                return "star_pose" 
            elif not arms_extended and not legs_apart:
                return "standing"
            else:
                return "dynamic"
                
        except Exception as e:
            self.logger.debug(f"í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return "unknown"
    
    def _calculate_arm_angle_sync(self, shoulder: List[float], elbow: List[float], wrist: List[float]) -> float:
        """íŒ” ê°ë„ ê³„ì‚° (ë™ê¸° ì²˜ë¦¬)"""
        try:
            if all(len(kp) >= 3 and kp[2] > 0.3 for kp in [shoulder, elbow, wrist]):
                v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]])
                v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]])
                
                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
                cos_angle = np.clip(cos_angle, -1.0, 1.0)
                angle = np.arccos(cos_angle)
                
                return np.degrees(angle)
            return 0.0
        except:
            return 0.0
    
    def _analyze_mask_consistency_sync(self, keypoints: List[List[float]], body_masks: Dict[str, Any]) -> Dict[str, Any]:
        """Body masksì™€ í‚¤í¬ì¸íŠ¸ ì¼ì¹˜ì„± ë¶„ì„ (ë™ê¸° ì²˜ë¦¬)"""
        try:
            consistency = {
                'overall_score': 0.0,
                'detailed_scores': {},
                'issues': []
            }
            
            # ê°„ë‹¨í•œ ì¼ì¹˜ì„± ë¶„ì„ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
            visible_keypoints = sum(1 for kp in keypoints if len(kp) >= 3 and kp[2] > 0.5)
            total_keypoints = len(keypoints)
            
            if total_keypoints > 0:
                consistency['overall_score'] = visible_keypoints / total_keypoints
            
            if consistency['overall_score'] < 0.6:
                consistency['issues'].append("í‚¤í¬ì¸íŠ¸ì™€ ë§ˆìŠ¤í¬ ë¶ˆì¼ì¹˜")
            
            return consistency
            
        except Exception as e:
            self.logger.debug(f"ë§ˆìŠ¤í¬ ì¼ì¹˜ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'overall_score': 0.0, 'issues': ['ë¶„ì„ ì‹¤íŒ¨']}

async def create_pose_estimation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> PoseEstimationStep:
    """í¬ì¦ˆ ì¶”ì • Step ìƒì„± í•¨ìˆ˜"""
    try:
        device_param = None if device == "auto" else device
        
        if config is None:
            config = {}
        config.update(kwargs)
        config['production_ready'] = True
        
        step = PoseEstimationStep(device=device_param, config=config)
        
        initialization_success = await step.initialize()
        
        if not initialization_success:
            raise RuntimeError("í¬ì¦ˆ ì¶”ì • Step ì´ˆê¸°í™” ì‹¤íŒ¨")
        
        return step
        
    except Exception as e:
        logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • Step ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def create_pose_estimation_step_sync(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> PoseEstimationStep:
    """ë™ê¸°ì‹ í¬ì¦ˆ ì¶”ì • Step ìƒì„±"""
    try:
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            create_pose_estimation_step(device, config, **kwargs)
        )
    except Exception as e:
        logger.error(f"âŒ ë™ê¸°ì‹ í¬ì¦ˆ ì¶”ì • Step ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# ==============================================
# ğŸ”¥ 13. í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ==============================================

async def test_unified_pose_estimation():
    """í†µí•© í¬ì¦ˆ ì¶”ì • í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ ì™„ì „ í†µí•©ëœ AI í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        print("=" * 80)
        
        # Step ìƒì„±
        step = await create_pose_estimation_step(
            device="auto",
            config={
                'confidence_threshold': 0.5,
                'use_subpixel': True,
                'production_ready': True
            }
        )
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
        test_image = Image.new('RGB', (512, 512), (128, 128, 128))
        
        print(f"ğŸ“‹ í†µí•© AI Step ì •ë³´:")
        status = step.get_status()
        print(f"   ğŸ¯ Step: {status['step_name']}")
        print(f"   ğŸ’ ì´ˆê¸°í™”: {status.get('is_initialized', False)}")
        print(f"   ğŸ¤– ë¡œë”©ëœ ëª¨ë¸: {sum(step.models_loaded.values())}ê°œ")
        
        # ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸
        result = await step.process(image=test_image)
        
        if result['success']:
            print(f"âœ… í†µí•© AI í¬ì¦ˆ ì¶”ì • ì„±ê³µ")
            print(f"ğŸ¯ ê²€ì¶œëœ í‚¤í¬ì¸íŠ¸: {len(result.get('keypoints', []))}")
            print(f"ğŸ–ï¸ ì „ì²´ ì‹ ë¢°ë„: {result.get('overall_confidence', 0):.3f}")
            print(f"ğŸ¤– ì‚¬ìš©ëœ ëª¨ë¸: {result.get('models_used', [])}")
            print(f"âš¡ ì¶”ë¡  ì‹œê°„: {result.get('inference_time', 0):.3f}ì´ˆ")
            print(f"ğŸ¯ ì„œë¸Œí”½ì…€ ì •í™•ë„: {result.get('subpixel_accuracy', False)}")
            print(f"ğŸ† ì£¼ ëª¨ë¸: {result.get('metadata', {}).get('primary_model', 'Unknown')}")
        else:
            print(f"âŒ í†µí•© AI í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
        
        await step.cleanup()
        print(f"ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_pose_algorithms():
    """í¬ì¦ˆ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ§  í¬ì¦ˆ AI ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # ë”ë¯¸ í‚¤í¬ì¸íŠ¸
        keypoints = [
            [100, 50, 0.9],   # nose
            [100, 80, 0.8],   # neck
            [80, 100, 0.7],   # right_shoulder
            [70, 130, 0.6],   # right_elbow
            [60, 160, 0.5],   # right_wrist
            [120, 100, 0.7],  # left_shoulder
            [130, 130, 0.6],  # left_elbow
            [140, 160, 0.5],  # left_wrist
            [100, 200, 0.8],  # middle_hip
            [90, 200, 0.7],   # right_hip
            [85, 250, 0.6],   # right_knee
            [80, 300, 0.5],   # right_ankle
            [110, 200, 0.7],  # left_hip
            [115, 250, 0.6],  # left_knee
            [120, 300, 0.5],  # left_ankle
            [95, 40, 0.8],    # right_eye
            [105, 40, 0.8],   # left_eye
            [90, 45, 0.7],    # right_ear
            [110, 45, 0.7]    # left_ear
        ]
        
        # ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸
        analyzer = AdvancedPoseAnalyzer()
        
        # ê´€ì ˆ ê°ë„ ê³„ì‚°
        joint_angles = analyzer.calculate_joint_angles(keypoints)
        print(f"âœ… ê´€ì ˆ ê°ë„ ê³„ì‚°: {len(joint_angles)}ê°œ")
        
        # ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°
        body_proportions = analyzer.calculate_body_proportions(keypoints)
        print(f"âœ… ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°: {len(body_proportions)}ê°œ")
        
        # í¬ì¦ˆ í’ˆì§ˆ í‰ê°€
        quality = analyzer.assess_pose_quality(keypoints, joint_angles, body_proportions)
        print(f"âœ… í¬ì¦ˆ í’ˆì§ˆ í‰ê°€: {quality['quality_grade'].value}")
        print(f"   ì „ì²´ ì ìˆ˜: {quality['overall_score']:.3f}")
        
        # ì˜ë¥˜ ì í•©ì„± ë¶„ì„
        clothing_analysis = analyze_pose_for_clothing(keypoints, "shirt")
        print(f"âœ… ì˜ë¥˜ ì í•©ì„±: {clothing_analysis['suitable_for_fitting']}")
        print(f"   ì ìˆ˜: {clothing_analysis['pose_score']:.3f}")
        
        # ì´ë¯¸ì§€ ê·¸ë¦¬ê¸° í…ŒìŠ¤íŠ¸
        test_image = Image.new('RGB', (256, 256), (128, 128, 128))
        pose_image = draw_pose_on_image(test_image, keypoints)
        print(f"âœ… í¬ì¦ˆ ì‹œê°í™”: {pose_image.size}")
        
        # í‚¤í¬ì¸íŠ¸ ìœ íš¨ì„± ê²€ì¦
        is_valid = validate_keypoints(keypoints)
        print(f"âœ… í‚¤í¬ì¸íŠ¸ ìœ íš¨ì„±: {is_valid}")
        
    except Exception as e:
        print(f"âŒ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 16. ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ (2ë²ˆ íŒŒì¼ ì™„ì „ í†µí•©)
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤ (ê°•í™”ëœ AI ê¸°ë°˜ + íŒŒì´í”„ë¼ì¸ ì§€ì›)
    'PoseEstimationStep',
    'PoseEstimationStepWithPipeline',
    'RealYOLOv8PoseModel',
    'RealOpenPoseModel',
    'RealHRNetModel',
    'RealDiffusionPoseModel',
    'RealBodyPoseModel',
    'HRNetModel',
    'OpenPoseModel',
    'AdvancedPoseAnalyzer',
    'SmartModelPathMapper',
    'Step02ModelMapper',
    'MediaPipeIntegration',
    
    # ë°ì´í„° êµ¬ì¡°
    'PoseResult',
    'PoseModel',
    'PoseQuality',
    'PipelineStepResult',
    'PipelineInputData',
    
    # ìƒì„± í•¨ìˆ˜ë“¤ (BaseStepMixin v19.1 í˜¸í™˜)
    'create_pose_estimation_step',
    'create_pose_estimation_step_sync',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì™„ì „ í†µí•©)
    'validate_keypoints',
    'draw_pose_on_image',
    'analyze_pose_for_clothing',
    
    # ìƒìˆ˜ë“¤
    'OPENPOSE_18_KEYPOINTS',
    'SKELETON_CONNECTIONS',
    'KEYPOINT_COLORS',
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ (BaseStepMixin v19.1 í˜¸í™˜)
    'test_unified_pose_estimation',
    'test_pose_algorithms'
]

# ==============================================
# ğŸ”¥ 15. ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê·¸
# ==============================================

logger.info("ğŸ”¥ ì™„ì „ í†µí•©ëœ AI í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ v8.0 ë¡œë“œ ì™„ë£Œ")
logger.info("âœ… 1ë²ˆ+2ë²ˆ íŒŒì¼ ì™„ì „ í†µí•© - ëª¨ë“  ê¸°ëŠ¥ í†µí•©")
logger.info("âœ… BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ - _run_ai_inference() ë™ê¸° ë©”ì„œë“œ êµ¬í˜„")
logger.info("âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  - HRNet + OpenPose + YOLO + Diffusion + MediaPipe")
logger.info("âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
logger.info("âœ… ì™„ì „í•œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ (ModelLoader, MemoryManager, DataConverter)")
logger.info("âœ… ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ â†’ ì¶”ë¡  ì—”ì§„")
logger.info("âœ… ì„œë¸Œí”½ì…€ ì •í™•ë„ + ê´€ì ˆê°ë„ ê³„ì‚° + ì‹ ì²´ë¹„ìœ¨ ë¶„ì„")
logger.info("âœ… PAF (Part Affinity Fields) + íˆíŠ¸ë§µ ê¸°ë°˜ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ")
logger.info("âœ… ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” + ì‹ ë¢°ë„ ìœµí•© ì‹œìŠ¤í…œ")
logger.info("âœ… ë¶ˆí™•ì‹¤ì„± ì¶”ì • + ìƒì²´ì—­í•™ì  íƒ€ë‹¹ì„± í‰ê°€")
logger.info("âœ… ë¶€ìƒ ìœ„í—˜ë„ í‰ê°€ + ê³ ê¸‰ í¬ì¦ˆ ë¶„ì„")
logger.info("âœ… M3 Max 128GB ë©”ëª¨ë¦¬ ìµœì í™”")
logger.info("âœ… ì‹¤ì‹œê°„ MPS/CUDA ê°€ì†")

logger.info("ğŸ§  ì‹¤ì œ AI ì•Œê³ ë¦¬ì¦˜ë“¤:")
logger.info("   - HRNet High-Resolution Network (ê³ í•´ìƒë„ ìœ ì§€)")
logger.info("   - OpenPose PAF + Heatmap (CMU ì•Œê³ ë¦¬ì¦˜)")
logger.info("   - YOLOv8-Pose Real-time (ì‹¤ì‹œê°„ ê²€ì¶œ)")
logger.info("   - Diffusion Pose Enhancement (í’ˆì§ˆ í–¥ìƒ)")
logger.info("   - MediaPipe Integration (ì‹¤ì‹œê°„ ì²˜ë¦¬)")
logger.info("   - ì„œë¸Œí”½ì…€ ì •í™•ë„ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ")
logger.info("   - ê´€ì ˆê°ë„ + ì‹ ì²´ë¹„ìœ¨ ê³„ì‚°")
logger.info("   - ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” ìœµí•©")
logger.info("   - ë¶ˆí™•ì‹¤ì„± ì¶”ì • + ìƒì²´ì—­í•™ì  ë¶„ì„")

logger.info(f"ğŸ“Š ì‹œìŠ¤í…œ: PyTorch={TORCH_AVAILABLE}, Device={DEVICE}, M3 Max={IS_M3_MAX}")
logger.info(f"ğŸ¤– AI ë¼ì´ë¸ŒëŸ¬ë¦¬: YOLO={ULTRALYTICS_AVAILABLE}, MediaPipe={MEDIAPIPE_AVAILABLE}")
logger.info(f"ğŸ”§ ë¼ì´ë¸ŒëŸ¬ë¦¬: OpenCV={OPENCV_AVAILABLE}, Transformers={TRANSFORMERS_AVAILABLE}")
logger.info(f"ğŸ§® ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬: Scipy={SCIPY_AVAILABLE}, Safetensors={SAFETENSORS_AVAILABLE}")
logger.info("ğŸš€ Production Ready - Complete AI Integration!")

# ==============================================
# ğŸ”¥ 16. ë©”ì¸ ì‹¤í–‰ë¶€
# ==============================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¯ MyCloset AI Step 02 - ì™„ì „ í†µí•©ëœ AI í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ")
    print("=" * 80)
    
    async def run_all_tests():
        await test_unified_pose_estimation()
        print("\n" + "=" * 80)
        test_pose_algorithms()
    
    try:
        asyncio.run(run_all_tests())
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 80)
    print("âœ¨ ì™„ì „ í†µí•©ëœ AI í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("ğŸ”¥ 1ë²ˆ+2ë²ˆ íŒŒì¼ ì™„ì „ í†µí•© - ëª¨ë“  ê¸°ëŠ¥ í†µí•©")
    print("ğŸ§  HRNet + OpenPose + YOLO + Diffusion + MediaPipe í†µí•©")
    print("ğŸ¯ 18ê°œ í‚¤í¬ì¸íŠ¸ ì™„ì „ ê²€ì¶œ")
    print("âš¡ ì„œë¸Œí”½ì…€ ì •í™•ë„ + ê´€ì ˆê°ë„ + ì‹ ì²´ë¹„ìœ¨")
    print("ğŸ”€ ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” + ì‹ ë¢°ë„ ìœµí•©")
    print("ğŸ“Š ë¶ˆí™•ì‹¤ì„± ì¶”ì • + ìƒì²´ì—­í•™ì  ë¶„ì„")
    print("ğŸ’‰ ì™„ì „í•œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´")
    print("ğŸ”’ BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜")
    print("ğŸš€ Production Ready!")
    print("=" * 80)