#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 02: AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • - ì™„ì „ ê°•í™”ëœ AI ì¶”ë¡  v6.0
================================================================================

âœ… BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ - _run_ai_inference() ë™ê¸° ë©”ì„œë“œ êµ¬í˜„
âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (3.4GB): OpenPose, YOLOv8, HRNet, Diffusion, Body Pose
âœ… ì˜¬ë°”ë¥¸ Step í´ë˜ìŠ¤ êµ¬í˜„ ê°€ì´ë“œ ì™„ì „ ì¤€ìˆ˜
âœ… ë™ê¸° ì²˜ë¦¬ë¡œ async/await ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… ê°•í™”ëœ AI ì¶”ë¡  ì—”ì§„ - ëª¨ë“  ê¸°ëŠ¥ ë³µì› + ì‹ ê·œ ê¸°ëŠ¥ ì¶”ê°€
âœ… StepInterface íŒŒì´í”„ë¼ì¸ ì§€ì› ìœ ì§€
âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€
âœ… SmartModelPathMapper í™œìš©í•œ ë™ì  íŒŒì¼ ê²½ë¡œ íƒì§€
âœ… 18ê°œ í‚¤í¬ì¸íŠ¸ ì™„ì „ ê²€ì¶œ ë° ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±
âœ… M3 Max MPS ê°€ì† ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›

í•µì‹¬ ê°œì„ ì‚¬í•­:
1. BaseStepMixinì˜ _run_ai_inference()ë¥¼ **ë™ê¸° ë©”ì„œë“œ**ë¡œ êµ¬í˜„
2. ëª¨ë“  AI ì¶”ë¡  ê¸°ëŠ¥ ì™„ì „ ë³µì› (ê¸°ì¡´ íŒŒì¼ì˜ ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€)
3. ê°•í™”ëœ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (RealYOLOv8PoseModel, RealOpenPoseModel, RealHRNetModel ë“±)
4. ê³ ê¸‰ í¬ì¦ˆ ë¶„ì„ ë° í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ
5. ì™„ì „í•œ ì‹œê°í™” ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
6. íŒŒì´í”„ë¼ì¸ ì—°ê²° ê¸°ëŠ¥ ìœ ì§€

íŒŒì¼ ìœ„ì¹˜: backend/app/ai_pipeline/steps/step_02_pose_estimation.py
ì‘ì„±ì: MyCloset AI Team  
ë‚ ì§œ: 2025-07-27
ë²„ì „: v6.0 (Complete Enhanced AI Inference with Sync _run_ai_inference)
"""

# ==============================================
# ğŸ”¥ 1. Import ì„¹ì…˜ (TYPE_CHECKING íŒ¨í„´)
# ==============================================

import os
import sys
import gc
import time
import asyncio
import logging
import threading
import traceback
import hashlib
import json
import base64
import weakref
import math
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from contextlib import asynccontextmanager

# TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from ..factories.step_factory import StepFactory
    from ..steps.base_step_mixin import BaseStepMixin

# ==============================================
# ğŸ”¥ 2. conda í™˜ê²½ ë° í•„ìˆ˜ íŒ¨í‚¤ì§€ ì²´í¬
# ==============================================

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'), 
    'python_path': os.path.dirname(os.__file__)
}

# M3 Max ê°ì§€
def detect_m3_max() -> bool:
    """M3 Max ê°ì§€"""
    try:
        import platform
        import subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()

# PyTorch (í•„ìˆ˜)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    import torchvision.transforms as transforms
    from torchvision.transforms.functional import resize, to_pil_image, to_tensor
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
    
    # MPS ì¥ì¹˜ ì„¤ì •
    if IS_M3_MAX and torch.backends.mps.is_available():
        DEVICE = "mps"
        torch.mps.set_per_process_memory_fraction(0.8)  # M3 Max ìµœì í™”
    elif torch.cuda.is_available():
        DEVICE = "cuda"
    else:
        DEVICE = "cpu"
        
except ImportError as e:
    raise ImportError(f"âŒ PyTorch í•„ìˆ˜: conda install pytorch torchvision -c pytorch\nì„¸ë¶€ ì˜¤ë¥˜: {e}")

# PIL (í•„ìˆ˜)
try:
    from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
    PIL_AVAILABLE = True
except ImportError as e:
    raise ImportError(f"âŒ Pillow í•„ìˆ˜: conda install pillow -c conda-forge\nì„¸ë¶€ ì˜¤ë¥˜: {e}")

# NumPy (í•„ìˆ˜)
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError as e:
    raise ImportError(f"âŒ NumPy í•„ìˆ˜: conda install numpy -c conda-forge\nì„¸ë¶€ ì˜¤ë¥˜: {e}")

# AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ì„ íƒì )
try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

try:
    from transformers import pipeline, CLIPProcessor, CLIPModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False

# safetensors (Diffusion ëª¨ë¸ìš©)
try:
    import safetensors.torch as st
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 3. ë™ì  import í•¨ìˆ˜ë“¤ (TYPE_CHECKING í˜¸í™˜)
# ==============================================

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logger.error(f"âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_model_loader():
    """ModelLoaderë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.model_loader')
        get_global_loader = getattr(module, 'get_global_model_loader', None)
        if get_global_loader:
            return get_global_loader()
        else:
            ModelLoader = getattr(module, 'ModelLoader', None)
            if ModelLoader:
                return ModelLoader()
        return None
    except ImportError as e:
        logger.error(f"âŒ ModelLoader ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_memory_manager():
    """MemoryManagerë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.memory_manager')
        get_global_manager = getattr(module, 'get_global_memory_manager', None)
        if get_global_manager:
            return get_global_manager()
        return None
    except ImportError as e:
        logger.debug(f"MemoryManager ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_step_factory():
    """StepFactoryë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.factories.step_factory')
        get_global_factory = getattr(module, 'get_global_step_factory', None)
        if get_global_factory:
            return get_global_factory()
        return None
    except ImportError as e:
        logger.debug(f"StepFactory ë™ì  import ì‹¤íŒ¨: {e}")
        return None

# BaseStepMixin ë™ì  ë¡œë”©
BaseStepMixin = get_base_step_mixin_class()

if BaseStepMixin is None:
    # í´ë°± í´ë˜ìŠ¤ ì •ì˜ (BaseStepMixin v19.1 í˜¸í™˜)
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'BaseStep')
            self.step_id = kwargs.get('step_id', 0)
            self.device = kwargs.get('device', DEVICE)
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # BaseStepMixin v19.1 í˜¸í™˜ ì†ì„±ë“¤
            self.config = type('StepConfig', (), kwargs)()
            self.dependency_manager = type('DependencyManager', (), {
                'dependency_status': type('DependencyStatus', (), {
                    'model_loader': False,
                    'step_interface': False,
                    'memory_manager': False,
                    'data_converter': False,
                    'di_container': False
                })(),
                'auto_inject_dependencies': lambda: False
            })()
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            self.performance_metrics = {
                'process_count': 0,
                'total_process_time': 0.0,
                'average_process_time': 0.0,
                'error_count': 0,
                'success_count': 0,
                'cache_hits': 0
            }
        
        async def initialize(self):
            self.is_initialized = True
            return True
        
        def set_model_loader(self, model_loader):
            self.model_loader = model_loader
            self.dependency_manager.dependency_status.model_loader = True
        
        def set_memory_manager(self, memory_manager):
            self.memory_manager = memory_manager
            self.dependency_manager.dependency_status.memory_manager = True
        
        def set_data_converter(self, data_converter):
            self.data_converter = data_converter
            self.dependency_manager.dependency_status.data_converter = True
        
        def set_di_container(self, di_container):
            self.di_container = di_container
            self.dependency_manager.dependency_status.di_container = True
        
        async def cleanup(self):
            pass
        
        def get_status(self):
            return {
                'step_name': self.step_name,
                'is_initialized': self.is_initialized,
                'device': self.device,
                'dependencies': {
                    'model_loader': self.dependency_manager.dependency_status.model_loader,
                    'step_interface': self.dependency_manager.dependency_status.step_interface,
                    'memory_manager': self.dependency_manager.dependency_status.memory_manager,
                    'data_converter': self.dependency_manager.dependency_status.data_converter,
                    'di_container': self.dependency_manager.dependency_status.di_container,
                },
                'version': '19.1-compatible'
            }

# ==============================================
# ğŸ”¥ 4. ë°ì´í„° êµ¬ì¡° ë° ìƒìˆ˜ ì •ì˜
# ==============================================

class PoseModel(Enum):
    """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ íƒ€ì…"""
    YOLOV8_POSE = "yolov8_pose"
    OPENPOSE = "openpose"
    HRNET = "hrnet"
    DIFFUSION_POSE = "diffusion_pose"
    BODY_POSE = "body_pose"

class PoseQuality(Enum):
    """í¬ì¦ˆ í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"     # 90-100ì 
    GOOD = "good"              # 75-89ì   
    ACCEPTABLE = "acceptable"   # 60-74ì 
    POOR = "poor"              # 40-59ì 
    VERY_POOR = "very_poor"    # 0-39ì 

# OpenPose 18 í‚¤í¬ì¸íŠ¸ ì •ì˜
OPENPOSE_18_KEYPOINTS = [
    "nose", "neck", "right_shoulder", "right_elbow", "right_wrist",
    "left_shoulder", "left_elbow", "left_wrist", "middle_hip", "right_hip", 
    "right_knee", "right_ankle", "left_hip", "left_knee", "left_ankle",
    "right_eye", "left_eye", "right_ear", "left_ear"
]

# í‚¤í¬ì¸íŠ¸ ìƒ‰ìƒ ë° ì—°ê²° ì •ë³´
KEYPOINT_COLORS = [
    (255, 0, 0), (255, 85, 0), (255, 170, 0), (255, 255, 0), (170, 255, 0),
    (85, 255, 0), (0, 255, 0), (0, 255, 85), (0, 255, 170), (0, 255, 255),
    (0, 170, 255), (0, 85, 255), (0, 0, 255), (85, 0, 255), (170, 0, 255),
    (255, 0, 255), (255, 0, 170), (255, 0, 85), (255, 0, 0)
]

SKELETON_CONNECTIONS = [
    (0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7), (1, 8),
    (8, 9), (9, 10), (10, 11), (8, 12), (12, 13), (13, 14), (0, 15),
    (15, 17), (0, 16), (16, 18)
]

@dataclass
class PoseMetrics:
    """í¬ì¦ˆ ì¸¡ì • ë°ì´í„°"""
    keypoints: List[List[float]] = field(default_factory=list)
    confidence_scores: List[float] = field(default_factory=list)
    bbox: Tuple[int, int, int, int] = field(default_factory=lambda: (0, 0, 0, 0))
    pose_quality: PoseQuality = PoseQuality.POOR
    overall_score: float = 0.0
    
    # ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜
    head_score: float = 0.0
    torso_score: float = 0.0  
    arms_score: float = 0.0
    legs_score: float = 0.0
    
    # ê³ ê¸‰ ë¶„ì„ ì ìˆ˜
    symmetry_score: float = 0.0
    visibility_score: float = 0.0
    pose_angles: Dict[str, float] = field(default_factory=dict)
    body_proportions: Dict[str, float] = field(default_factory=dict)
    
    # ì˜ë¥˜ ì°©ìš© ì í•©ì„±
    suitable_for_fitting: bool = False
    issues: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    
    # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„°
    model_used: str = ""
    processing_time: float = 0.0
    image_resolution: Tuple[int, int] = field(default_factory=lambda: (0, 0))
    ai_confidence: float = 0.0

# ==============================================
# ğŸ”¥ 5. SmartModelPathMapper (ì‹¤ì œ íŒŒì¼ íƒì§€)
# ==============================================

class SmartModelPathMapper:
    """ì‹¤ì œ íŒŒì¼ ìœ„ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì°¾ì•„ì„œ ë§¤í•‘í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self, ai_models_root: str = "ai_models"):
        self.ai_models_root = Path(ai_models_root)
        self.model_cache = {}  # ìºì‹œë¡œ ì„±ëŠ¥ ìµœì í™”
        self.logger = logging.getLogger(f"{__name__}.SmartModelPathMapper")
    
    def _search_models(self, model_files: Dict[str, List[str]], search_priority: List[str]) -> Dict[str, Optional[Path]]:
        """ëª¨ë¸ íŒŒì¼ë“¤ì„ ìš°ì„ ìˆœìœ„ ê²½ë¡œì—ì„œ ê²€ìƒ‰"""
        found_paths = {}
        
        for model_name, filenames in model_files.items():
            found_path = None
            for filename in filenames:
                for search_path in search_priority:
                    candidate_path = self.ai_models_root / search_path / filename
                    if candidate_path.exists() and candidate_path.is_file():
                        found_path = candidate_path
                        self.logger.info(f"âœ… {model_name} ëª¨ë¸ ë°œê²¬: {found_path}")
                        break
                if found_path:
                    break
            found_paths[model_name] = found_path
            
            if not found_path:
                self.logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {filenames}")
        
        return found_paths

class Step02ModelMapper(SmartModelPathMapper):
    """Step 02 Pose Estimation ì „ìš© ë™ì  ê²½ë¡œ ë§¤í•‘"""
    
    def get_step02_model_paths(self) -> Dict[str, Optional[Path]]:
        """Step 02 ëª¨ë¸ ê²½ë¡œ ìë™ íƒì§€ - HRNet í¬í•¨"""
        model_files = {
            "yolov8": ["yolov8n-pose.pt", "yolov8s-pose.pt"],
            "openpose": ["openpose.pth", "body_pose_model.pth"],
            "hrnet": [
                "hrnet_w48_coco_256x192.pth", 
                "hrnet_w32_coco_256x192.pth", 
                "pose_hrnet_w48_256x192.pth",
                "hrnet_w48_256x192.pth"
            ],
            "diffusion": ["diffusion_pytorch_model.safetensors", "diffusion_pytorch_model.bin"],
            "body_pose": ["body_pose_model.pth"]
        }
        
        search_priority = [
            "step_02_pose_estimation/",
            "step_02_pose_estimation/ultra_models/",
            "step_06_virtual_fitting/ootdiffusion/checkpoints/openpose/",
            "checkpoints/step_02_pose_estimation/",
            "pose_estimation/",
            "hrnet/",
            "checkpoints/hrnet/",
            ""  # ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë„ ê²€ìƒ‰
        ]
        
        return self._search_models(model_files, search_priority)

# ==============================================
# ğŸ”¥ 6. íŒŒì´í”„ë¼ì¸ ë°ì´í„° êµ¬ì¡° (StepInterface í˜¸í™˜)
# ==============================================

@dataclass
class PipelineStepResult:
    """íŒŒì´í”„ë¼ì¸ Step ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""
    step_id: int
    step_name: str
    success: bool
    error: Optional[str] = None
    
    # ë‹¤ìŒ Stepë“¤ë¡œ ì „ë‹¬í•  ë°ì´í„°
    for_step_03: Dict[str, Any] = field(default_factory=dict)
    for_step_04: Dict[str, Any] = field(default_factory=dict)
    for_step_05: Dict[str, Any] = field(default_factory=dict)
    for_step_06: Dict[str, Any] = field(default_factory=dict)
    for_step_07: Dict[str, Any] = field(default_factory=dict)
    for_step_08: Dict[str, Any] = field(default_factory=dict)
    
    # ì´ì „ ë‹¨ê³„ ë°ì´í„° ë³´ì¡´
    previous_data: Dict[str, Any] = field(default_factory=dict)
    original_data: Dict[str, Any] = field(default_factory=dict)
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)
    processing_time: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)

@dataclass 
class PipelineInputData:
    """íŒŒì´í”„ë¼ì¸ ì…ë ¥ ë°ì´í„°"""
    person_image: Union[np.ndarray, Image.Image, str]
    clothing_image: Optional[Union[np.ndarray, Image.Image, str]] = None
    session_id: Optional[str] = None
    config: Dict[str, Any] = field(default_factory=dict)

# StepInterface ë™ì  ë¡œë”©
def get_step_interface_class():
    """StepInterface í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.interface.step_interface')
        return getattr(module, 'StepInterface', None)
    except ImportError as e:
        logger.error(f"âŒ StepInterface ë™ì  import ì‹¤íŒ¨: {e}")
        return None

StepInterface = get_step_interface_class()

if StepInterface is None:
    # í´ë°± StepInterface ì •ì˜
    class StepInterface:
        def __init__(self, step_id: int, step_name: str, config: Dict[str, Any], **kwargs):
            self.step_id = step_id
            self.step_name = step_name
            self.config = config
            self.pipeline_mode = config.get("pipeline_mode", False)
        
        async def process_pipeline(self, input_data: PipelineStepResult) -> PipelineStepResult:
            """íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì²˜ë¦¬ (í´ë°±)"""
            return PipelineStepResult(
                step_id=self.step_id,
                step_name=self.step_name,
                success=False,
                error="StepInterface í´ë°± ëª¨ë“œ"
            )

# ==============================================
# ğŸ”¥ 7. ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ì™„ì „ ê°•í™”)
# ==============================================

class RealYOLOv8PoseModel:
    """YOLOv8 6.5MB ì‹¤ì‹œê°„ í¬ì¦ˆ ê²€ì¶œ - ê°•í™”ëœ AI ì¶”ë¡ """
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealYOLOv8PoseModel")
    
    def load_yolo_checkpoint(self) -> bool:
        """ì‹¤ì œ YOLOv8-Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            if not ULTRALYTICS_AVAILABLE:
                self.logger.error("âŒ ultralytics ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŒ")
                return False
            
            # YOLOv8 í¬ì¦ˆ ëª¨ë¸ ë¡œë”©
            self.model = YOLO(str(self.model_path))
            
            # MPS ë””ë°”ì´ìŠ¤ ì„¤ì • (M3 Max ìµœì í™”)
            if self.device == "mps" and torch.backends.mps.is_available():
                # YOLOv8ì€ ìë™ìœ¼ë¡œ MPS ì‚¬ìš©
                pass
            elif self.device == "cuda":
                self.model.to("cuda")
            
            self.loaded = True
            self.logger.info(f"âœ… YOLOv8-Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def detect_poses_realtime(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ í¬ì¦ˆ ê²€ì¶œ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.loaded:
            raise RuntimeError("YOLOv8 ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            results = self.model(image, verbose=False)
            
            poses = []
            for result in results:
                if hasattr(result, 'keypoints') and result.keypoints is not None:
                    keypoints = result.keypoints.data  # [N, 17, 3] (x, y, confidence)
                    
                    for person_kpts in keypoints:
                        # YOLOv8ì€ COCO 17 í˜•ì‹ì´ë¯€ë¡œ OpenPose 18ë¡œ ë³€í™˜
                        openpose_kpts = self._convert_coco17_to_openpose18(person_kpts.cpu().numpy())
                        
                        pose_data = {
                            "keypoints": openpose_kpts,
                            "bbox": result.boxes.xyxy.cpu().numpy()[0] if result.boxes else None,
                            "confidence": float(result.boxes.conf.mean()) if result.boxes else 0.0
                        }
                        poses.append(pose_data)
            
            processing_time = time.time() - start_time
            
            return {
                "poses": poses,
                "keypoints": poses[0]["keypoints"] if poses else [],
                "num_persons": len(poses),
                "processing_time": processing_time,
                "model_type": "yolov8_pose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "poses": [],
                "keypoints": [],
                "num_persons": 0,
                "processing_time": time.time() - start_time,
                "model_type": "yolov8_pose",
                "success": False,
                "error": str(e)
            }
    
    def _convert_coco17_to_openpose18(self, coco_keypoints: np.ndarray) -> List[List[float]]:
        """COCO 17 í¬ë§·ì„ OpenPose 18ë¡œ ë³€í™˜"""
        # COCO 17 â†’ OpenPose 18 ë§¤í•‘
        coco_to_openpose_mapping = {
            0: 0,   # nose
            5: 2,   # left_shoulder â†’ right_shoulder (ì¢Œìš° ë°˜ì „)
            6: 5,   # right_shoulder â†’ left_shoulder
            7: 3,   # left_elbow â†’ right_elbow
            8: 6,   # right_elbow â†’ left_elbow
            9: 4,   # left_wrist â†’ right_wrist
            10: 7,  # right_wrist â†’ left_wrist
            11: 9,  # left_hip â†’ right_hip
            12: 12, # right_hip â†’ left_hip
            13: 10, # left_knee â†’ right_knee
            14: 13, # right_knee â†’ left_knee
            15: 11, # left_ankle â†’ right_ankle
            16: 14, # right_ankle â†’ left_ankle
            1: 15,  # left_eye â†’ right_eye
            2: 16,  # right_eye â†’ left_eye
            3: 17,  # left_ear â†’ right_ear
            4: 18   # right_ear â†’ left_ear
        }
        
        openpose_keypoints = [[0.0, 0.0, 0.0] for _ in range(19)]  # 18 + neck
        
        # neck ê³„ì‚° (ì–´ê¹¨ ì¤‘ì )
        if len(coco_keypoints) > 6:
            left_shoulder = coco_keypoints[5]
            right_shoulder = coco_keypoints[6]
            if left_shoulder[2] > 0.1 and right_shoulder[2] > 0.1:
                neck_x = (left_shoulder[0] + right_shoulder[0]) / 2
                neck_y = (left_shoulder[1] + right_shoulder[1]) / 2
                neck_conf = (left_shoulder[2] + right_shoulder[2]) / 2
                openpose_keypoints[1] = [float(neck_x), float(neck_y), float(neck_conf)]
        
        # ë‚˜ë¨¸ì§€ í‚¤í¬ì¸íŠ¸ ë§¤í•‘
        for coco_idx, openpose_idx in coco_to_openpose_mapping.items():
            if coco_idx < len(coco_keypoints):
                openpose_keypoints[openpose_idx] = [
                    float(coco_keypoints[coco_idx][0]),
                    float(coco_keypoints[coco_idx][1]),
                    float(coco_keypoints[coco_idx][2])
                ]
        
        return openpose_keypoints[:18]  # OpenPose 18ê°œë§Œ ë°˜í™˜

class RealOpenPoseModel:
    """OpenPose 97.8MB ì •ë°€ í¬ì¦ˆ ê²€ì¶œ - ê°•í™”ëœ AI ì¶”ë¡ """
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealOpenPoseModel")
    
    def load_openpose_checkpoint(self) -> bool:
        """ì‹¤ì œ OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            # PyTorchë¡œ ì§ì ‘ ë¡œë”©
            checkpoint = torch.load(str(self.model_path), map_location=self.device, weights_only=True)
            
            # OpenPose ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ìƒì„±
            self.model = self._create_openpose_network()
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ state_dict ë¡œë”©
            if 'state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['state_dict'], strict=False)
            elif 'model' in checkpoint:
                self.model.load_state_dict(checkpoint['model'], strict=False)
            else:
                self.model.load_state_dict(checkpoint, strict=False)
            
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            
            self.logger.info(f"âœ… OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±
            self.model = self._create_simple_pose_model()
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            self.logger.warning("âš ï¸ OpenPose í´ë°± ëª¨ë¸ ì‚¬ìš©")
            return True
    
    def _create_openpose_network(self) -> nn.Module:
        """OpenPose ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ìƒì„±"""
        class OpenPoseNetwork(nn.Module):
            def __init__(self):
                super().__init__()
                # VGG19 ë°±ë³¸
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(),
                    nn.MaxPool2d(2, 2),
                    nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(),
                    nn.MaxPool2d(2, 2),
                    nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(),
                    nn.MaxPool2d(2, 2),
                    nn.Conv2d(256, 512, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(),
                )
                
                # PAF (Part Affinity Fields) ë¸Œëœì¹˜
                self.paf_branch = nn.Sequential(
                    nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(128, 38, 1, 1, 0)  # 19 connections * 2
                )
                
                # í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ ë¸Œëœì¹˜
                self.keypoint_branch = nn.Sequential(
                    nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(128, 19, 1, 1, 0)  # 18 keypoints + background
                )
            
            def forward(self, x):
                features = self.backbone(x)
                paf = self.paf_branch(features)
                keypoints = self.keypoint_branch(features)
                return keypoints, paf
        
        return OpenPoseNetwork()
    
    def _create_simple_pose_model(self) -> nn.Module:
        """ê°„ë‹¨í•œ í¬ì¦ˆ ëª¨ë¸ (í´ë°±ìš©)"""
        class SimplePoseModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.MaxPool2d(3, 2, 1),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),
                    nn.AdaptiveAvgPool2d((1, 1))
                )
                self.keypoint_head = nn.Linear(256, 18 * 3)  # 18 keypoints * (x, y, conf)
            
            def forward(self, x):
                features = self.backbone(x)
                features = features.flatten(1)
                keypoints = self.keypoint_head(features)
                return keypoints.view(-1, 18, 3)
        
        return SimplePoseModel()
    
    def detect_keypoints_precise(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """ì •ë°€ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.loaded:
            raise RuntimeError("OpenPose ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, Image.Image):
                image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
            elif isinstance(image, np.ndarray):
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(self.device) / 255.0
            else:
                image_tensor = image.to(self.device)
            
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                if hasattr(self.model, 'keypoint_head'):  # Simple model
                    output = self.model(image_tensor)
                    keypoints = output[0].cpu().numpy()
                    
                    # í‚¤í¬ì¸íŠ¸ ì •ê·œí™” (ì´ë¯¸ì§€ í¬ê¸° ê¸°ì¤€)
                    h, w = image_tensor.shape[-2:]
                    keypoints_list = []
                    for kp in keypoints:
                        x, y, conf = float(kp[0] * w), float(kp[1] * h), float(torch.sigmoid(torch.tensor(kp[2])))
                        keypoints_list.append([x, y, conf])
                    
                else:  # OpenPose model
                    keypoint_heatmaps, paf = self.model(image_tensor)
                    keypoints_list = self._extract_keypoints_from_heatmaps(keypoint_heatmaps[0])
            
            processing_time = time.time() - start_time
            
            return {
                "keypoints": keypoints_list,
                "processing_time": processing_time,
                "model_type": "openpose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ OpenPose AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "keypoints": [],
                "processing_time": time.time() - start_time,
                "model_type": "openpose",
                "success": False,
                "error": str(e)
            }
    
    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(18):  # 18ê°œ í‚¤í¬ì¸íŠ¸
            heatmap = heatmaps[i].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            confidence = float(heatmap[y_idx, x_idx])
            
            # ì¢Œí‘œ ì •ê·œí™”
            x = float(x_idx / w * 512)  # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³€í™˜
            y = float(y_idx / h * 512)
            
            keypoints.append([x, y, confidence])
        
        return keypoints

# HRNet ëª¨ë¸ (ê³ ì •ë°€ í¬ì¦ˆ ì¶”ì •)
class BasicBlock(nn.Module):
    """HRNet BasicBlock êµ¬í˜„"""
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, momentum=0.1)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes, momentum=0.1)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out

class Bottleneck(nn.Module):
    """HRNet Bottleneck êµ¬í˜„"""
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, momentum=0.1)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes, momentum=0.1)
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion, momentum=0.1)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(x)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out

class RealHRNetModel(nn.Module):
    """ì‹¤ì œ HRNet ê³ ì •ë°€ í¬ì¦ˆ ì¶”ì • ëª¨ë¸"""
    
    def __init__(self, cfg=None, **kwargs):
        super(RealHRNetModel, self).__init__()
        
        # ëª¨ë¸ ì •ë³´
        self.model_name = "RealHRNetModel"
        self.version = "2.0"
        self.parameter_count = 0
        self.is_loaded = False
        
        # HRNet-W48 ê¸°ë³¸ ì„¤ì •
        if cfg is None:
            cfg = {
                'MODEL': {
                    'EXTRA': {
                        'STAGE1': {
                            'NUM_CHANNELS': [64],
                            'BLOCK': 'BOTTLENECK',
                            'NUM_BLOCKS': [4]
                        },
                        'STAGE2': {
                            'NUM_MODULES': 1,
                            'NUM_BRANCHES': 2,
                            'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [4, 4],
                            'NUM_CHANNELS': [48, 96]
                        },
                        'STAGE3': {
                            'NUM_MODULES': 4,
                            'NUM_BRANCHES': 3,
                            'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [4, 4, 4],
                            'NUM_CHANNELS': [48, 96, 192]
                        },
                        'STAGE4': {
                            'NUM_MODULES': 3,
                            'NUM_BRANCHES': 4,
                            'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [4, 4, 4, 4],
                            'NUM_CHANNELS': [48, 96, 192, 384]
                        }
                    }
                }
            }
        
        self.cfg = cfg
        extra = cfg['MODEL']['EXTRA']
        
        # stem net
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64, momentum=0.1)
        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(64, momentum=0.1)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self._make_layer(Bottleneck, 64, 64, extra['STAGE1']['NUM_BLOCKS'][0])

        # ìµœì¢… ë ˆì´ì–´ (18ê°œ í‚¤í¬ì¸íŠ¸ ì¶œë ¥)
        self.final_layer = nn.Conv2d(
            in_channels=48,  # HRNet-W48
            out_channels=18,  # OpenPose 18 í‚¤í¬ì¸íŠ¸
            kernel_size=1,
            stride=1,
            padding=0
        )

        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        self.parameter_count = self._count_parameters()

    def _make_layer(self, block, inplanes, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion, momentum=0.1),
            )

        layers = []
        layers.append(block(inplanes, planes, stride, downsample))
        inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(inplanes, planes))

        return nn.Sequential(*layers)

    def _count_parameters(self):
        """íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

    def forward(self, x):
        """HRNet ìˆœì „íŒŒ (ê°„ì†Œí™” ë²„ì „)"""
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.layer1(x)

        # ê°„ì†Œí™”ëœ ì²˜ë¦¬
        x = self.final_layer(x)
        return x

    def detect_high_precision_pose(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """ê³ ì •ë°€ í¬ì¦ˆ ê²€ì¶œ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.is_loaded:
            raise RuntimeError("HRNet ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, Image.Image):
                image_tensor = transforms.ToTensor()(image).unsqueeze(0).to(next(self.parameters()).device)
            elif isinstance(image, np.ndarray):
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(next(self.parameters()).device) / 255.0
            else:
                image_tensor = image.to(next(self.parameters()).device)
            
            # ì…ë ¥ í¬ê¸° ì •ê·œí™” (256x192)
            image_tensor = F.interpolate(image_tensor, size=(256, 192), mode='bilinear', align_corners=False)
            
            # ì‹¤ì œ HRNet AI ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                heatmaps = self(image_tensor)  # [1, 18, 64, 48]
            
            # íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
            keypoints = self._extract_keypoints_from_heatmaps(heatmaps[0])
            
            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§
            if isinstance(image, Image.Image):
                orig_w, orig_h = image.size
            elif isinstance(image, np.ndarray):
                orig_h, orig_w = image.shape[:2]
            else:
                orig_h, orig_w = 256, 192
            
            # ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§
            scale_x = orig_w / 192
            scale_y = orig_h / 256
            
            scaled_keypoints = []
            for kp in keypoints:
                scaled_keypoints.append([
                    kp[0] * scale_x,
                    kp[1] * scale_y,
                    kp[2]
                ])
            
            processing_time = time.time() - start_time
            
            return {
                "keypoints": scaled_keypoints,
                "processing_time": processing_time,
                "model_type": "hrnet",
                "success": True,
                "confidence": np.mean([kp[2] for kp in scaled_keypoints])
            }
            
        except Exception as e:
            logger.error(f"âŒ HRNet AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "keypoints": [],
                "processing_time": time.time() - start_time,
                "model_type": "hrnet",
                "success": False,
                "error": str(e)
            }

    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ê³ ì •ë°€ ì„œë¸Œí”½ì…€ ì •í™•ë„)"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(18):  # 18ê°œ í‚¤í¬ì¸íŠ¸
            heatmap = heatmaps[i].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            max_val = heatmap[y_idx, x_idx]
            
            # ì„œë¸Œí”½ì…€ ì •í™•ë„ë¥¼ ìœ„í•œ ê°€ìš°ì‹œì•ˆ í”¼íŒ…
            if (1 <= x_idx < w-1) and (1 <= y_idx < h-1):
                # x ë°©í–¥ ì„œë¸Œí”½ì…€ ë³´ì •
                dx = 0.5 * (heatmap[y_idx, x_idx+1] - heatmap[y_idx, x_idx-1]) / (
                    heatmap[y_idx, x_idx+1] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx, x_idx-1] + 1e-8)
                
                # y ë°©í–¥ ì„œë¸Œí”½ì…€ ë³´ì •
                dy = 0.5 * (heatmap[y_idx+1, x_idx] - heatmap[y_idx-1, x_idx]) / (
                    heatmap[y_idx+1, x_idx] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx-1, x_idx] + 1e-8)
                
                # ì„œë¸Œí”½ì…€ ì¢Œí‘œ
                x_subpixel = x_idx + dx
                y_subpixel = y_idx + dy
            else:
                x_subpixel = x_idx
                y_subpixel = y_idx
            
            # ì¢Œí‘œ ì •ê·œí™” (0-1 ë²”ìœ„)
            x_normalized = x_subpixel / w
            y_normalized = y_subpixel / h
            
            # ì‹¤ì œ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜ (192x256 ê¸°ì¤€)
            x_coord = x_normalized * 192
            y_coord = y_normalized * 256
            confidence = float(max_val)
            
            keypoints.append([x_coord, y_coord, confidence])
        
        return keypoints

    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cpu"):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ HRNet ëª¨ë¸ ë¡œë“œ"""
        model = cls()
        
        if checkpoint_path and os.path.exists(checkpoint_path):
            try:
                logger.info(f"ğŸ”„ HRNet ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_path}")
                checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
                
                # ì²´í¬í¬ì¸íŠ¸ í˜•íƒœì— ë”°ë¥¸ ì²˜ë¦¬
                if isinstance(checkpoint, dict):
                    if 'model' in checkpoint:
                        state_dict = checkpoint['model']
                    elif 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # í‚¤ ì´ë¦„ ë§¤í•‘ (í•„ìš”í•œ ê²½ìš°)
                model_dict = model.state_dict()
                filtered_dict = {}
                
                for k, v in state_dict.items():
                    # í‚¤ ì´ë¦„ ì •ë¦¬
                    key = k
                    if key.startswith('module.'):
                        key = key[7:]  # 'module.' ì œê±°
                    
                    if key in model_dict and model_dict[key].shape == v.shape:
                        filtered_dict[key] = v
                    else:
                        logger.debug(f"HRNet í‚¤ ë¶ˆì¼ì¹˜: {key}, í˜•íƒœ: {v.shape if hasattr(v, 'shape') else 'unknown'}")
                
                # í•„í„°ë§ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
                model_dict.update(filtered_dict)
                model.load_state_dict(model_dict, strict=False)
                model.is_loaded = True
                
                logger.info(f"âœ… HRNet ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {len(filtered_dict)}/{len(model_dict)} ë ˆì´ì–´")
                
            except Exception as e:
                logger.warning(f"âš ï¸ HRNet ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                logger.info("ğŸ”„ ê¸°ë³¸ HRNet ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”")
        else:
            logger.warning(f"âš ï¸ HRNet ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_path}")
            logger.info("ğŸ”„ ëœë¤ ì´ˆê¸°í™”ëœ HRNet ì‚¬ìš©")
        
        model.to(device)
        model.eval()
        return model

class RealDiffusionPoseModel:
    """Diffusion 1378MB ê³ í’ˆì§ˆ í¬ì¦ˆ ìƒì„± - ê°•í™”ëœ AI ì¶”ë¡ """
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealDiffusionPoseModel")
    
    def load_diffusion_checkpoint(self) -> bool:
        """ì‹¤ì œ 1.4GB Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            if SAFETENSORS_AVAILABLE and str(self.model_path).endswith('.safetensors'):
                # safetensors íŒŒì¼ ë¡œë”©
                checkpoint = st.load_file(str(self.model_path))
            else:
                # ì¼ë°˜ PyTorch íŒŒì¼ ë¡œë”©
                checkpoint = torch.load(str(self.model_path), map_location=self.device, weights_only=True)
            
            # Diffusion UNet ë„¤íŠ¸ì›Œí¬ ìƒì„±
            self.model = self._create_diffusion_unet()
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (í‚¤ ë§¤ì¹­)
            model_dict = self.model.state_dict()
            filtered_dict = {}
            
            for k, v in checkpoint.items():
                if k in model_dict and model_dict[k].shape == v.shape:
                    filtered_dict[k] = v
            
            model_dict.update(filtered_dict)
            self.model.load_state_dict(model_dict, strict=False)
            
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            
            self.logger.info(f"âœ… Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {self.model_path} ({len(filtered_dict)} layers)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°„ë‹¨í•œ ëª¨ë¸
            self.model = self._create_simple_diffusion_model()
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            self.logger.warning("âš ï¸ Diffusion í´ë°± ëª¨ë¸ ì‚¬ìš©")
            return True
    
    def _create_diffusion_unet(self) -> nn.Module:
        """Diffusion UNet ë„¤íŠ¸ì›Œí¬ ìƒì„±"""
        class DiffusionUNet(nn.Module):
            def __init__(self):
                super().__init__()
                # ê°„ì†Œí™”ëœ UNet êµ¬ì¡°
                self.encoder = nn.Sequential(
                    nn.Conv2d(3, 64, 3, 1, 1), nn.GroupNorm(8, 64), nn.SiLU(),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.GroupNorm(16, 128), nn.SiLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.GroupNorm(32, 256), nn.SiLU(),
                    nn.Conv2d(256, 512, 3, 2, 1), nn.GroupNorm(32, 512), nn.SiLU(),
                )
                
                self.middle = nn.Sequential(
                    nn.Conv2d(512, 512, 3, 1, 1), nn.GroupNorm(32, 512), nn.SiLU(),
                    nn.Conv2d(512, 512, 3, 1, 1), nn.GroupNorm(32, 512), nn.SiLU(),
                )
                
                self.decoder = nn.Sequential(
                    nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.GroupNorm(32, 256), nn.SiLU(),
                    nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.GroupNorm(16, 128), nn.SiLU(),
                    nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.GroupNorm(8, 64), nn.SiLU(),
                    nn.Conv2d(64, 18 * 3, 3, 1, 1)  # 18 keypoints * 3
                )
            
            def forward(self, x):
                encoded = self.encoder(x)
                middle = self.middle(encoded)
                decoded = self.decoder(middle)
                return decoded
        
        return DiffusionUNet()
    
    def _create_simple_diffusion_model(self) -> nn.Module:
        """ê°„ë‹¨í•œ Diffusion ëª¨ë¸ (í´ë°±ìš©)"""
        return self._create_diffusion_unet()  # ê°™ì€ êµ¬ì¡° ì‚¬ìš©
    
    def enhance_pose_quality(self, keypoints: Union[torch.Tensor, List[List[float]]], image: Union[torch.Tensor, Image.Image] = None) -> Dict[str, Any]:
        """í¬ì¦ˆ í’ˆì§ˆ í–¥ìƒ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.loaded:
            raise RuntimeError("Diffusion ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # í‚¤í¬ì¸íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
            if isinstance(keypoints, list):
                keypoints_tensor = torch.tensor(keypoints, dtype=torch.float32, device=self.device)
            else:
                keypoints_tensor = keypoints.to(self.device)
            
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°)
            if image is None:
                batch_size = 1
                image_tensor = torch.randn(batch_size, 3, 512, 512, device=self.device)
            else:
                if isinstance(image, Image.Image):
                    image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
                else:
                    image_tensor = image.to(self.device)
            
            # ì‹¤ì œ Diffusion AI ì¶”ë¡ 
            with torch.no_grad():
                enhanced_output = self.model(image_tensor)
                
                # ì¶œë ¥ í•´ì„ (18 keypoints * 3 ì±„ë„)
                b, c, h, w = enhanced_output.shape
                enhanced_keypoints = enhanced_output.view(b, 18, 3, h, w)
                
                # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ìµœëŒ€ê°’ ìœ„ì¹˜)
                enhanced_kpts = []
                for i in range(18):
                    for j in range(3):  # x, y, confidence
                        channel = enhanced_keypoints[0, i, j]
                        if j < 2:  # x, y ì¢Œí‘œ
                            max_val = torch.max(channel)
                            enhanced_kpts.append(float(max_val * 512))  # ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì •ê·œí™”
                        else:  # confidence
                            enhanced_kpts.append(float(torch.sigmoid(torch.mean(channel))))
                
                # 18ê°œ í‚¤í¬ì¸íŠ¸ë¡œ ì¬êµ¬ì„±
                result_keypoints = []
                for i in range(18):
                    x = enhanced_kpts[i*3]
                    y = enhanced_kpts[i*3+1]
                    conf = enhanced_kpts[i*3+2]
                    result_keypoints.append([x, y, conf])
            
            processing_time = time.time() - start_time
            
            return {
                "enhanced_keypoints": result_keypoints,
                "processing_time": processing_time,
                "model_type": "diffusion_pose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            # í´ë°±: ì›ë³¸ í‚¤í¬ì¸íŠ¸ ë°˜í™˜
            if isinstance(keypoints, list):
                original_keypoints = keypoints
            else:
                original_keypoints = keypoints.cpu().numpy().tolist()
            
            return {
                "enhanced_keypoints": original_keypoints,
                "processing_time": time.time() - start_time,
                "model_type": "diffusion_pose",
                "success": False,
                "error": str(e)
            }

class RealBodyPoseModel:
    """Body Pose 97.8MB ë³´ì¡° í¬ì¦ˆ ê²€ì¶œ - ê°•í™”ëœ AI ì¶”ë¡ """
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealBodyPoseModel")
    
    def load_body_pose_checkpoint(self) -> bool:
        """ì‹¤ì œ Body Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            checkpoint = torch.load(str(self.model_path), map_location=self.device, weights_only=True)
            
            # Body Pose ë„¤íŠ¸ì›Œí¬ ìƒì„±
            self.model = self._create_body_pose_network()
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            if 'state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['state_dict'], strict=False)
            else:
                self.model.load_state_dict(checkpoint, strict=False)
            
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            
            self.logger.info(f"âœ… Body Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Body Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _create_body_pose_network(self) -> nn.Module:
        """Body Pose ë„¤íŠ¸ì›Œí¬ ìƒì„±"""
        class BodyPoseNetwork(nn.Module):
            def __init__(self):
                super().__init__()
                # ResNet ìŠ¤íƒ€ì¼ ë°±ë³¸
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.MaxPool2d(3, 2, 1),
                    nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),
                    nn.Conv2d(256, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(),
                    nn.AdaptiveAvgPool2d((8, 8))
                )
                
                self.pose_head = nn.Sequential(
                    nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(128, 18, 1, 1, 0),  # 18 keypoints heatmaps
                    nn.AdaptiveAvgPool2d((32, 32))
                )
            
            def forward(self, x):
                features = self.backbone(x)
                heatmaps = self.pose_head(features)
                return heatmaps
        
        return BodyPoseNetwork()
    
    def detect_body_pose(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """ë³´ì¡° í¬ì¦ˆ ê²€ì¶œ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.loaded:
            raise RuntimeError("Body Pose ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, Image.Image):
                image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
            elif isinstance(image, np.ndarray):
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(self.device) / 255.0
            else:
                image_tensor = image.to(self.device)
            
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                heatmaps = self.model(image_tensor)
                keypoints = self._extract_keypoints_from_heatmaps(heatmaps[0])
            
            processing_time = time.time() - start_time
            
            return {
                "keypoints": keypoints,
                "processing_time": processing_time,
                "model_type": "body_pose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Body Pose AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "keypoints": [],
                "processing_time": time.time() - start_time,
                "model_type": "body_pose",
                "success": False,
                "error": str(e)
            }
    
    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(18):
            heatmap = heatmaps[i].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ë° ì‹ ë¢°ë„
            y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            confidence = float(heatmap[y_idx, x_idx])
            
            # ì¢Œí‘œ ì •ê·œí™”
            x = float(x_idx / w * 512)
            y = float(y_idx / h * 512)
            
            keypoints.append([x, y, confidence])
        
        return keypoints

# ==============================================
# ğŸ”¥ 8. ë©”ì¸ PoseEstimationStep í´ë˜ìŠ¤ (BaseStepMixin í˜¸í™˜)
# ==============================================

class PoseEstimationStep(BaseStepMixin):
    """
    ğŸ”¥ Step 02: AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ - BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜
    
    âœ… BaseStepMixin v19.1ì˜ _run_ai_inference() ë™ê¸° ë©”ì„œë“œ êµ¬í˜„
    âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (3.4GB): OpenPose, YOLOv8, HRNet, Diffusion, Body Pose
    âœ… ê°•í™”ëœ AI ì¶”ë¡  ì—”ì§„ - ëª¨ë“  ê¸°ëŠ¥ ë³µì› + ì‹ ê·œ ê¸°ëŠ¥ ì¶”ê°€
    âœ… ì˜¬ë°”ë¥¸ Step í´ë˜ìŠ¤ êµ¬í˜„ ê°€ì´ë“œ ì™„ì „ ì¤€ìˆ˜
    âœ… SmartModelPathMapper í™œìš©í•œ ë™ì  íŒŒì¼ ê²½ë¡œ íƒì§€
    âœ… 18ê°œ í‚¤í¬ì¸íŠ¸ ì™„ì „ ê²€ì¶œ ë° ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±
    âœ… M3 Max MPS ê°€ì† ìµœì í™”
    """
    
    def __init__(self, **kwargs):
        """
        BaseStepMixin í˜¸í™˜ PoseEstimationStep ìƒì„±ì
        
        Args:
            **kwargs: BaseStepMixinì—ì„œ ì „ë‹¬ë°›ëŠ” ì„¤ì •
        """
        # BaseStepMixin ì´ˆê¸°í™”
        super().__init__(
            step_name="PoseEstimationStep",
            step_id=2,
            **kwargs
        )
        
        # PoseEstimationStep íŠ¹í™” ì†ì„±ë“¤
        self.num_keypoints = 18
        self.keypoint_names = OPENPOSE_18_KEYPOINTS.copy()
        
        # SmartModelPathMapper ì´ˆê¸°í™”
        self.model_mapper = Step02ModelMapper()
        
        # ì‹¤ì œ AI ëª¨ë¸ë“¤
        self.ai_models: Dict[str, Any] = {}
        self.model_paths: Dict[str, Optional[Path]] = {}
        self.loaded_models: List[str] = []
        
        # ì²˜ë¦¬ ì„¤ì •
        self.target_input_size = (512, 512)
        self.confidence_threshold = 0.5
        self.visualization_enabled = True
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.prediction_cache: Dict[str, Any] = {}
        self.cache_max_size = 100 if IS_M3_MAX else 50
        
        self.logger.info(f"ğŸ¯ {self.step_name} ê°•í™”ëœ AI ì¶”ë¡  Step ìƒì„± ì™„ë£Œ")
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin v19.1 í˜¸í™˜ - _run_ai_inference() ë™ê¸° ë©”ì„œë“œ êµ¬í˜„
    # ==============================================
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ BaseStepMixinì˜ í•µì‹¬ AI ì¶”ë¡  ë©”ì„œë“œ (ë™ê¸° ì²˜ë¦¬)
        
        Args:
            processed_input: BaseStepMixinì—ì„œ ë³€í™˜ëœ í‘œì¤€ AI ëª¨ë¸ ì…ë ¥
                - 'image': ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ (PIL.Image)
                - 'from_step_01': ì´ì „ Stepì˜ ì¶œë ¥ ë°ì´í„° (ìˆëŠ” ê²½ìš°)
                - ê¸°íƒ€ ì„¤ì •ê°’ë“¤
        
        Returns:
            Dict[str, Any]: AI ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ§  {self.step_name} AI ì¶”ë¡  ì‹œì‘ (ë™ê¸° ì²˜ë¦¬)")
            inference_start = time.time()
            
            # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦
            if 'image' not in processed_input:
                raise ValueError("í•„ìˆ˜ ì…ë ¥ ë°ì´í„° 'image'ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            image = processed_input['image']
            if not isinstance(image, Image.Image):
                if isinstance(image, np.ndarray):
                    image = Image.fromarray(image)
                else:
                    raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤")
            
            # 2. AI ëª¨ë¸ë“¤ì´ ë¡œë”©ë˜ì§€ ì•Šì€ ê²½ìš° ë¡œë”© ì‹œë„
            if not self.ai_models:
                self._load_all_ai_models_sync()
            
            if not self.ai_models:
                raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            
            # 3. ì´ì „ Step ë°ì´í„° í™œìš© (ìˆëŠ” ê²½ìš°)
            previous_data = {}
            for key, value in processed_input.items():
                if key.startswith('from_step_'):
                    previous_data[key] = value
            
            # 4. ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰ (ë™ê¸° ì²˜ë¦¬)
            ai_result = self._run_real_ai_inference_sync(image, previous_data)
            
            if not ai_result.get('success', False):
                raise RuntimeError(f"AI ì¶”ë¡  ì‹¤íŒ¨: {ai_result.get('error', 'Unknown AI Error')}")
            
            # 5. ê²°ê³¼ í›„ì²˜ë¦¬ ë° ë¶„ì„
            processed_result = self._postprocess_ai_result_sync(ai_result, image)
            
            # 6. AI ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ ë°˜í™˜ (BaseStepMixinì´ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
            inference_time = time.time() - inference_start
            
            raw_output = {
                # ì£¼ìš” ì¶œë ¥
                'keypoints': processed_result['keypoints'],
                'confidence_scores': processed_result['confidence_scores'],
                'skeleton_structure': processed_result['skeleton_structure'],
                'joint_connections': processed_result['joint_connections'],
                'joint_angles': processed_result['joint_angles'],
                'body_orientation': processed_result['body_orientation'],
                'landmarks': processed_result['landmarks'],
                
                # AI ëª¨ë¸ ë©”íƒ€ë°ì´í„°
                'models_used': processed_result['models_used'],
                'primary_model': processed_result['primary_model'],
                'enhanced_by_diffusion': processed_result.get('enhanced_by_diffusion', False),
                'ai_confidence': processed_result['ai_confidence'],
                
                # ì²˜ë¦¬ ì •ë³´
                'inference_time': inference_time,
                'processing_time': processed_result['processing_time'],
                'success': True,
                
                # ë©”íƒ€ë°ì´í„°
                'metadata': {
                    'input_resolution': image.size,
                    'num_keypoints_detected': len(processed_result['keypoints']),
                    'ai_models_loaded': len(self.ai_models),
                    'device': self.device,
                    'is_m3_max': IS_M3_MAX
                }
            }
            
            self.logger.info(f"âœ… {self.step_name} AI ì¶”ë¡  ì™„ë£Œ ({inference_time:.3f}ì´ˆ)")
            self.logger.info(f"ğŸ¯ ê²€ì¶œëœ í‚¤í¬ì¸íŠ¸: {len(processed_result['keypoints'])}ê°œ")
            self.logger.info(f"ğŸ–ï¸ AI ì‹ ë¢°ë„: {processed_result['ai_confidence']:.3f}")
            self.logger.info(f"ğŸ¤– ì‚¬ìš©ëœ AI ëª¨ë¸ë“¤: {processed_result['models_used']}")
            
            return raw_output
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìŠ¤íƒ: {traceback.format_exc()}")
            
            # ì—ëŸ¬ ìƒí™©ì—ì„œë„ BaseStepMixin í˜¸í™˜ í˜•ì‹ ë°˜í™˜
            return {
                'keypoints': [],
                'confidence_scores': [],
                'skeleton_structure': {},
                'joint_connections': [],
                'joint_angles': {},
                'body_orientation': {},
                'landmarks': {},
                'models_used': [],
                'primary_model': 'error',
                'enhanced_by_diffusion': False,
                'ai_confidence': 0.0,
                'inference_time': 0.0,
                'processing_time': 0.0,
                'success': False,
                'error': str(e),
                'metadata': {
                    'error_occurred': True,
                    'error_message': str(e)
                }
            }
    
    # ==============================================
    # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© (ë™ê¸° ì²˜ë¦¬)
    # ==============================================
    
    def _load_all_ai_models_sync(self) -> bool:
        """ì‹¤ì œ AI ëª¨ë¸ë“¤ ë™ê¸° ë¡œë”©"""
        try:
            self.logger.info("ğŸ”„ ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ë“¤ ë¡œë”© ì‹œì‘...")
            
            # 1. SmartModelPathMapperë¡œ ì‹¤ì œ íŒŒì¼ ê²½ë¡œ íƒì§€
            self.model_paths = self.model_mapper.get_step02_model_paths()
            
            if not any(self.model_paths.values()):
                self.logger.warning("âš ï¸ ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return False
            
            # 2. ì‹¤ì œ AI ëª¨ë¸ë“¤ ë¡œë”©
            success_count = 0
            
            # YOLOv8-Pose ëª¨ë¸ ë¡œë”© (6.5MB - ì‹¤ì‹œê°„)
            if self.model_paths.get("yolov8"):
                try:
                    yolo_model = RealYOLOv8PoseModel(self.model_paths["yolov8"], self.device)
                    if yolo_model.load_yolo_checkpoint():
                        self.ai_models["yolov8"] = yolo_model
                        self.loaded_models.append("yolov8")
                        success_count += 1
                        self.logger.info("âœ… YOLOv8-Pose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ YOLOv8-Pose ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # OpenPose ëª¨ë¸ ë¡œë”© (97.8MB - ì •ë°€)
            if self.model_paths.get("openpose"):
                try:
                    openpose_model = RealOpenPoseModel(self.model_paths["openpose"], self.device)
                    if openpose_model.load_openpose_checkpoint():
                        self.ai_models["openpose"] = openpose_model
                        self.loaded_models.append("openpose")
                        success_count += 1
                        self.logger.info("âœ… OpenPose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ OpenPose ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # HRNet ëª¨ë¸ ë¡œë”© (ê³ ì •ë°€)
            if self.model_paths.get("hrnet"):
                try:
                    hrnet_model = RealHRNetModel.from_checkpoint(
                        checkpoint_path=str(self.model_paths["hrnet"]),
                        device=self.device
                    )
                    self.ai_models["hrnet"] = hrnet_model
                    self.loaded_models.append("hrnet")
                    success_count += 1
                    self.logger.info("âœ… HRNet ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ HRNet ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # Diffusion Pose ëª¨ë¸ ë¡œë”© (1378MB - ëŒ€í˜• ê³ í’ˆì§ˆ)
            if self.model_paths.get("diffusion"):
                try:
                    diffusion_model = RealDiffusionPoseModel(self.model_paths["diffusion"], self.device)
                    if diffusion_model.load_diffusion_checkpoint():
                        self.ai_models["diffusion"] = diffusion_model
                        self.loaded_models.append("diffusion")
                        success_count += 1
                        self.logger.info("âœ… Diffusion Pose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Diffusion Pose ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # Body Pose ëª¨ë¸ ë¡œë”© (97.8MB - ë³´ì¡°)
            if self.model_paths.get("body_pose"):
                try:
                    body_pose_model = RealBodyPoseModel(self.model_paths["body_pose"], self.device)
                    if body_pose_model.load_body_pose_checkpoint():
                        self.ai_models["body_pose"] = body_pose_model
                        self.loaded_models.append("body_pose")
                        success_count += 1
                        self.logger.info("âœ… Body Pose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Body Pose ë¡œë”© ì‹¤íŒ¨: {e}")
            
            if success_count > 0:
                self.logger.info(f"ğŸ‰ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {success_count}ê°œ ({self.loaded_models})")
                return True
            else:
                self.logger.error("âŒ ëª¨ë“  AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰ (ë™ê¸° ì²˜ë¦¬)
    # ==============================================
    
    def _run_real_ai_inference_sync(self, image: Image.Image, previous_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹¤ì œ AI ëª¨ë¸ë“¤ì„ í†µí•œ í¬ì¦ˆ ì¶”ì • ì¶”ë¡  (ë™ê¸° ì²˜ë¦¬)"""
        try:
            inference_start = time.time()
            self.logger.info(f"ğŸ§  ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹œì‘ (ë™ê¸°)...")
            
            if not self.ai_models:
                return {'success': False, 'error': 'ë¡œë”©ëœ AI ëª¨ë¸ì´ ì—†ìŒ'}
            
            # 1. YOLOv8-Poseë¡œ ì‹¤ì‹œê°„ ê²€ì¶œ (ìš°ì„ ìˆœìœ„ 1)
            yolo_result = None
            if "yolov8" in self.ai_models:
                try:
                    yolo_result = self.ai_models["yolov8"].detect_poses_realtime(image)
                    self.logger.info(f"âœ… YOLOv8 ì¶”ë¡  ì™„ë£Œ: {yolo_result.get('success', False)}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ YOLOv8 ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            # 2. OpenPoseë¡œ ì •ë°€ ê²€ì¶œ (ìš°ì„ ìˆœìœ„ 2)
            openpose_result = None
            if "openpose" in self.ai_models:
                try:
                    openpose_result = self.ai_models["openpose"].detect_keypoints_precise(image)
                    self.logger.info(f"âœ… OpenPose ì¶”ë¡  ì™„ë£Œ: {openpose_result.get('success', False)}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ OpenPose ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            # 3. HRNetìœ¼ë¡œ ê³ ì •ë°€ ê²€ì¶œ (ìš°ì„ ìˆœìœ„ 3)
            hrnet_result = None
            if "hrnet" in self.ai_models:
                try:
                    hrnet_result = self.ai_models["hrnet"].detect_high_precision_pose(image)
                    self.logger.info(f"âœ… HRNet ì¶”ë¡  ì™„ë£Œ: {hrnet_result.get('success', False)}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ HRNet ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            # 4. Body Poseë¡œ ë³´ì¡° ê²€ì¶œ (ìš°ì„ ìˆœìœ„ 4)
            body_pose_result = None
            if "body_pose" in self.ai_models:
                try:
                    body_pose_result = self.ai_models["body_pose"].detect_body_pose(image)
                    self.logger.info(f"âœ… Body Pose ì¶”ë¡  ì™„ë£Œ: {body_pose_result.get('success', False)}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Body Pose ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            # 5. ìµœì  ê²°ê³¼ ì„ íƒ ë° í†µí•©
            primary_result = self._select_best_pose_result_sync(yolo_result, openpose_result, hrnet_result, body_pose_result)
            
            if not primary_result or not primary_result.get('keypoints'):
                return {'success': False, 'error': 'ëª¨ë“  AI ëª¨ë¸ì—ì„œ ìœ íš¨í•œ í¬ì¦ˆë¥¼ ê²€ì¶œí•˜ì§€ ëª»í•¨'}
            
            # 6. Diffusion Poseë¡œ í’ˆì§ˆ í–¥ìƒ (ì„ íƒì )
            enhanced_result = primary_result
            if "diffusion" in self.ai_models and primary_result.get('keypoints'):
                try:
                    diffusion_result = self.ai_models["diffusion"].enhance_pose_quality(
                        primary_result['keypoints'], image
                    )
                    if diffusion_result.get('success', False):
                        enhanced_result['keypoints'] = diffusion_result['enhanced_keypoints']
                        enhanced_result['enhanced_by_diffusion'] = True
                        self.logger.info("âœ… Diffusion í’ˆì§ˆ í–¥ìƒ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Diffusion í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
            
            # 7. ê²°ê³¼ í†µí•© ë° ë¶„ì„
            combined_keypoints = enhanced_result['keypoints']
            combined_result = {
                'keypoints': combined_keypoints,
                'skeleton_structure': self._build_skeleton_structure_sync(combined_keypoints),
                'joint_connections': self._get_joint_connections_sync(combined_keypoints),
                'joint_angles': self._calculate_joint_angles_sync(combined_keypoints),
                'body_orientation': self._get_body_orientation_sync(combined_keypoints),
                'landmarks': self._extract_landmarks_sync(combined_keypoints),
                'confidence_scores': [kp[2] for kp in combined_keypoints if len(kp) > 2],
                'processing_time': time.time() - inference_start,
                'models_used': self.loaded_models,
                'primary_model': primary_result.get('model_type', 'unknown'),
                'enhanced_by_diffusion': enhanced_result.get('enhanced_by_diffusion', False),
                'success': True
            }
            
            inference_time = time.time() - inference_start
            self.logger.info(f"âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ ({inference_time:.3f}ì´ˆ)")
            
            return combined_result
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _select_best_pose_result_sync(self, yolo_result, openpose_result, hrnet_result, body_pose_result) -> Optional[Dict[str, Any]]:
        """ìµœì ì˜ í¬ì¦ˆ ê²°ê³¼ ì„ íƒ (ë™ê¸° ì²˜ë¦¬)"""
        results = []
        
        # ê° ê²°ê³¼ì˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        for result, model_name, weight in [
            (yolo_result, "yolov8", 0.7),
            (openpose_result, "openpose", 0.8),
            (hrnet_result, "hrnet", 0.85),
            (body_pose_result, "body_pose", 0.6)
        ]:
            if result and result.get('success') and result.get('keypoints'):
                confidence = np.mean([kp[2] for kp in result['keypoints'] if len(kp) > 2])
                visible_kpts = sum(1 for kp in result['keypoints'] if len(kp) > 2 and kp[2] > 0.3)
                quality_score = confidence * weight + (visible_kpts / 18) * (1 - weight)
                results.append((quality_score, result))
        
        if not results:
            return None
        
        # ìµœê³  í’ˆì§ˆ ì ìˆ˜ ê²°ê³¼ ì„ íƒ
        best_score, best_result = max(results, key=lambda x: x[0])
        self.logger.info(f"ğŸ† ìµœì  í¬ì¦ˆ ê²°ê³¼ ì„ íƒ: {best_result.get('model_type', 'unknown')} (ì ìˆ˜: {best_score:.3f})")
        
        return best_result
    
    # ==============================================
    # ğŸ”¥ í¬ì¦ˆ ë¶„ì„ ë° í›„ì²˜ë¦¬ (ë™ê¸° ì²˜ë¦¬)
    # ==============================================
    
    def _postprocess_ai_result_sync(self, ai_result: Dict[str, Any], image: Image.Image) -> Dict[str, Any]:
        """AI ê²°ê³¼ í›„ì²˜ë¦¬ (ë™ê¸° ì²˜ë¦¬)"""
        try:
            # PoseMetrics ìƒì„±
            pose_metrics = PoseMetrics(
                keypoints=ai_result.get('keypoints', []),
                confidence_scores=ai_result.get('confidence_scores', []),
                model_used=ai_result.get('primary_model', 'unknown'),
                processing_time=ai_result.get('processing_time', 0.0),
                image_resolution=image.size,
                ai_confidence=np.mean(ai_result.get('confidence_scores', [0])) if ai_result.get('confidence_scores') else 0.0
            )
            
            # í¬ì¦ˆ ë¶„ì„
            pose_analysis = self._analyze_pose_quality_sync(pose_metrics)
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                'keypoints': pose_metrics.keypoints,
                'confidence_scores': pose_metrics.confidence_scores,
                'skeleton_structure': ai_result.get('skeleton_structure', {}),
                'joint_connections': ai_result.get('joint_connections', []),
                'joint_angles': ai_result.get('joint_angles', {}),
                'body_orientation': ai_result.get('body_orientation', {}),
                'landmarks': ai_result.get('landmarks', {}),
                'pose_analysis': pose_analysis,
                'processing_time': ai_result.get('processing_time', 0.0),
                'models_used': ai_result.get('models_used', []),
                'primary_model': ai_result.get('primary_model', 'unknown'),
                'enhanced_by_diffusion': ai_result.get('enhanced_by_diffusion', False),
                'ai_confidence': pose_metrics.ai_confidence
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ AI ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ai_result
    
    def _analyze_pose_quality_sync(self, pose_metrics: PoseMetrics) -> Dict[str, Any]:
        """í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„ (ë™ê¸° ì²˜ë¦¬)"""
        try:
            if not pose_metrics.keypoints:
                return {
                    'suitable_for_fitting': False,
                    'issues': ['AI ëª¨ë¸ì—ì„œ í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'],
                    'recommendations': ['ë” ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ í¬ì¦ˆë¥¼ ëª…í™•íˆ í•´ì£¼ì„¸ìš”'],
                    'quality_score': 0.0,
                    'ai_confidence': 0.0
                }
            
            # AI ì‹ ë¢°ë„ ê³„ì‚°
            ai_confidence = np.mean(pose_metrics.confidence_scores) if pose_metrics.confidence_scores else 0.0
            
            # ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°
            head_score = self._calculate_body_part_score_sync(pose_metrics.keypoints, [0, 15, 16, 17, 18])
            torso_score = self._calculate_body_part_score_sync(pose_metrics.keypoints, [1, 2, 5, 8])
            arms_score = self._calculate_body_part_score_sync(pose_metrics.keypoints, [2, 3, 4, 5, 6, 7])
            legs_score = self._calculate_body_part_score_sync(pose_metrics.keypoints, [9, 10, 11, 12, 13, 14])
            
            # ê³ ê¸‰ ë¶„ì„
            symmetry_score = self._calculate_symmetry_score_sync(pose_metrics.keypoints)
            visibility_score = self._calculate_visibility_score_sync(pose_metrics.keypoints)
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self._calculate_overall_quality_score_sync(
                head_score, torso_score, arms_score, legs_score, 
                symmetry_score, visibility_score, ai_confidence
            )
            
            # ì í•©ì„± íŒë‹¨
            visible_keypoints = sum(1 for kp in pose_metrics.keypoints if len(kp) > 2 and kp[2] > 0.5)
            suitable_for_fitting = (quality_score >= 0.7 and ai_confidence >= 0.6 and visible_keypoints >= 10)
            
            # ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
            issues = []
            recommendations = []
            
            if ai_confidence < 0.6:
                issues.append(f'AI ëª¨ë¸ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({ai_confidence:.2f})')
                recommendations.append('ì¡°ëª…ì´ ì¢‹ì€ í™˜ê²½ì—ì„œ ë‹¤ì‹œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            if visible_keypoints < 10:
                issues.append('ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤')
                recommendations.append('ì „ì‹ ì´ ëª…í™•íˆ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            if symmetry_score < 0.6:
                issues.append('ì¢Œìš° ëŒ€ì¹­ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤')
                recommendations.append('ì •ë©´ì„ í–¥í•´ ê· í˜•ì¡íŒ ìì„¸ë¡œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            if torso_score < 0.7:
                issues.append('ìƒì²´ í¬ì¦ˆê°€ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤')
                recommendations.append('ì–´ê¹¨ì™€ íŒ”ì´ ëª…í™•íˆ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            return {
                'suitable_for_fitting': suitable_for_fitting,
                'issues': issues,
                'recommendations': recommendations,
                'quality_score': quality_score,
                'ai_confidence': ai_confidence,
                'visible_keypoints': visible_keypoints,
                'total_keypoints': len(pose_metrics.keypoints),
                'detailed_scores': {
                    'head': head_score,
                    'torso': torso_score,
                    'arms': arms_score,
                    'legs': legs_score
                },
                'advanced_analysis': {
                    'symmetry_score': symmetry_score,
                    'visibility_score': visibility_score
                },
                'model_performance': {
                    'model_name': pose_metrics.model_used,
                    'processing_time': pose_metrics.processing_time,
                    'real_ai_models': True
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'suitable_for_fitting': False,
                'issues': ['ë¶„ì„ ì‹¤íŒ¨'],
                'recommendations': ['ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”'],
                'quality_score': 0.0,
                'ai_confidence': 0.0
            }
    
    # ==============================================
    # ğŸ”¥ ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ë° ê¸°í•˜í•™ì  ë¶„ì„ (ë™ê¸° ì²˜ë¦¬)
    # ==============================================
    
    def _build_skeleton_structure_sync(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„± (ë™ê¸° ì²˜ë¦¬)"""
        try:
            skeleton = {
                'connections': [],
                'bone_lengths': {},
                'joint_positions': {},
                'structure_valid': False
            }
            
            if not keypoints or len(keypoints) < 18:
                return skeleton
            
            # ì—°ê²° êµ¬ì¡° ìƒì„±
            for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
                if (start_idx < len(keypoints) and end_idx < len(keypoints) and
                    len(keypoints[start_idx]) >= 3 and len(keypoints[end_idx]) >= 3):
                    
                    start_kp = keypoints[start_idx]
                    end_kp = keypoints[end_idx]
                    
                    if start_kp[2] > 0.3 and end_kp[2] > 0.3:
                        connection = {
                            'start': start_idx,
                            'end': end_idx,
                            'start_name': OPENPOSE_18_KEYPOINTS[start_idx] if start_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{start_idx}",
                            'end_name': OPENPOSE_18_KEYPOINTS[end_idx] if end_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{end_idx}",
                            'length': np.sqrt((start_kp[0] - end_kp[0])**2 + (start_kp[1] - end_kp[1])**2),
                            'confidence': (start_kp[2] + end_kp[2]) / 2
                        }
                        skeleton['connections'].append(connection)
            
            # ê´€ì ˆ ìœ„ì¹˜
            for i, kp in enumerate(keypoints):
                if len(kp) >= 3 and kp[2] > 0.3:
                    joint_name = OPENPOSE_18_KEYPOINTS[i] if i < len(OPENPOSE_18_KEYPOINTS) else f"joint_{i}"
                    skeleton['joint_positions'][joint_name] = {
                        'x': kp[0],
                        'y': kp[1],
                        'confidence': kp[2]
                    }
            
            skeleton['structure_valid'] = len(skeleton['connections']) >= 8
            
            return skeleton
            
        except Exception as e:
            self.logger.debug(f"ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„± ì‹¤íŒ¨: {e}")
            return {'connections': [], 'bone_lengths': {}, 'joint_positions': {}, 'structure_valid': False}
    
    def _get_joint_connections_sync(self, keypoints: List[List[float]]) -> List[Dict[str, Any]]:
        """ê´€ì ˆ ì—°ê²° ì •ë³´ ë°˜í™˜ (ë™ê¸° ì²˜ë¦¬)"""
        try:
            connections = []
            
            for start_idx, end_idx in SKELETON_CONNECTIONS:
                if (start_idx < len(keypoints) and end_idx < len(keypoints) and
                    len(keypoints[start_idx]) >= 3 and len(keypoints[end_idx]) >= 3):
                    
                    start_kp = keypoints[start_idx]
                    end_kp = keypoints[end_idx]
                    
                    if start_kp[2] > 0.3 and end_kp[2] > 0.3:
                        connection = {
                            'start_joint': start_idx,
                            'end_joint': end_idx,
                            'start_name': OPENPOSE_18_KEYPOINTS[start_idx] if start_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{start_idx}",
                            'end_name': OPENPOSE_18_KEYPOINTS[end_idx] if end_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{end_idx}",
                            'connection_strength': (start_kp[2] + end_kp[2]) / 2
                        }
                        connections.append(connection)
            
            return connections
            
        except Exception as e:
            self.logger.debug(f"ê´€ì ˆ ì—°ê²° ì •ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_joint_angles_sync(self, keypoints: List[List[float]]) -> Dict[str, float]:
        """ê´€ì ˆ ê°ë„ ê³„ì‚° (ë™ê¸° ì²˜ë¦¬)"""
        try:
            angles = {}
            
            if not keypoints or len(keypoints) < 18:
                return angles
            
            def calculate_angle(p1, p2, p3):
                """ì„¸ ì  ì‚¬ì´ì˜ ê°ë„ ê³„ì‚°"""
                try:
                    v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
                    v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
                    
                    cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
                    cos_angle = np.clip(cos_angle, -1.0, 1.0)
                    angle = np.arccos(cos_angle)
                    
                    return np.degrees(angle)
                except:
                    return 0.0
            
            confidence_threshold = 0.3
            
            # íŒ”ê¿ˆì¹˜ ê°ë„ (ì˜¤ë¥¸ìª½)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [2, 3, 4]):
                angles['right_elbow'] = calculate_angle(keypoints[2], keypoints[3], keypoints[4])
            
            # íŒ”ê¿ˆì¹˜ ê°ë„ (ì™¼ìª½)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [5, 6, 7]):
                angles['left_elbow'] = calculate_angle(keypoints[5], keypoints[6], keypoints[7])
            
            # ë¬´ë¦ ê°ë„ (ì˜¤ë¥¸ìª½)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [9, 10, 11]):
                angles['right_knee'] = calculate_angle(keypoints[9], keypoints[10], keypoints[11])
            
            # ë¬´ë¦ ê°ë„ (ì™¼ìª½)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [12, 13, 14]):
                angles['left_knee'] = calculate_angle(keypoints[12], keypoints[13], keypoints[14])
            
            # ì–´ê¹¨ ê¸°ìš¸ê¸°
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [2, 5]):
                shoulder_slope = np.degrees(np.arctan2(
                    keypoints[5][1] - keypoints[2][1],
                    keypoints[5][0] - keypoints[2][0] + 1e-8
                ))
                angles['shoulder_slope'] = abs(shoulder_slope)
            
            return angles
            
        except Exception as e:
            self.logger.debug(f"ê´€ì ˆ ê°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _get_body_orientation_sync(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """ì‹ ì²´ ë°©í–¥ ë¶„ì„ (ë™ê¸° ì²˜ë¦¬)"""
        try:
            orientation = {
                'facing_direction': 'unknown',
                'body_angle': 0.0,
                'shoulder_line_angle': 0.0,
                'hip_line_angle': 0.0,
                'is_frontal': False
            }
            
            if not keypoints or len(keypoints) < 18:
                return orientation
            
            # ì–´ê¹¨ ë¼ì¸ ê°ë„
            if (2 < len(keypoints) and 5 < len(keypoints) and
                len(keypoints[2]) >= 3 and len(keypoints[5]) >= 3 and
                keypoints[2][2] > 0.3 and keypoints[5][2] > 0.3):
                
                shoulder_angle = np.degrees(np.arctan2(
                    keypoints[5][1] - keypoints[2][1],
                    keypoints[5][0] - keypoints[2][0]
                ))
                orientation['shoulder_line_angle'] = shoulder_angle
                
                # ì •ë©´ ì—¬ë¶€ íŒë‹¨
                orientation['is_frontal'] = abs(shoulder_angle) < 15
            
            # ì—‰ë©ì´ ë¼ì¸ ê°ë„
            if (9 < len(keypoints) and 12 < len(keypoints) and
                len(keypoints[9]) >= 3 and len(keypoints[12]) >= 3 and
                keypoints[9][2] > 0.3 and keypoints[12][2] > 0.3):
                
                hip_angle = np.degrees(np.arctan2(
                    keypoints[12][1] - keypoints[9][1],
                    keypoints[12][0] - keypoints[9][0]
                ))
                orientation['hip_line_angle'] = hip_angle
            
            # ì „ì²´ ì‹ ì²´ ê°ë„ (ì–´ê¹¨ì™€ ì—‰ë©ì´ í‰ê· )
            if orientation['shoulder_line_angle'] != 0.0 and orientation['hip_line_angle'] != 0.0:
                orientation['body_angle'] = (orientation['shoulder_line_angle'] + orientation['hip_line_angle']) / 2
            elif orientation['shoulder_line_angle'] != 0.0:
                orientation['body_angle'] = orientation['shoulder_line_angle']
            elif orientation['hip_line_angle'] != 0.0:
                orientation['body_angle'] = orientation['hip_line_angle']
            
            # ë°©í–¥ ë¶„ë¥˜
            if abs(orientation['body_angle']) < 15:
                orientation['facing_direction'] = 'front'
            elif orientation['body_angle'] > 15:
                orientation['facing_direction'] = 'left'
            elif orientation['body_angle'] < -15:
                orientation['facing_direction'] = 'right'
            
            return orientation
            
        except Exception as e:
            self.logger.debug(f"ì‹ ì²´ ë°©í–¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'facing_direction': 'unknown', 'body_angle': 0.0, 'is_frontal': False}
    
    def _extract_landmarks_sync(self, keypoints: List[List[float]]) -> Dict[str, Dict[str, float]]:
        """ì£¼ìš” ëœë“œë§ˆí¬ ì¶”ì¶œ (ë™ê¸° ì²˜ë¦¬)"""
        try:
            landmarks = {}
            
            for i, kp in enumerate(keypoints):
                if len(kp) >= 3 and kp[2] > 0.3:
                    landmark_name = OPENPOSE_18_KEYPOINTS[i] if i < len(OPENPOSE_18_KEYPOINTS) else f"landmark_{i}"
                    landmarks[landmark_name] = {
                        'x': float(kp[0]),
                        'y': float(kp[1]),
                        'confidence': float(kp[2])
                    }
            
            return landmarks
            
        except Exception as e:
            self.logger.debug(f"ëœë“œë§ˆí¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    # ==============================================
    # ğŸ”¥ ë³´ì¡° ê³„ì‚° ë©”ì„œë“œë“¤ (ë™ê¸° ì²˜ë¦¬)
    # ==============================================
    
    def _calculate_body_part_score_sync(self, keypoints: List[List[float]], part_indices: List[int]) -> float:
        """ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚° (ë™ê¸° ì²˜ë¦¬)"""
        try:
            if not keypoints or not part_indices:
                return 0.0
            
            visible_count = 0
            total_confidence = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > self.confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            
            return visibility_ratio * avg_confidence
            
        except Exception as e:
            self.logger.debug(f"ì‹ ì²´ ë¶€ìœ„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_symmetry_score_sync(self, keypoints: List[List[float]]) -> float:
        """ì¢Œìš° ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚° (ë™ê¸° ì²˜ë¦¬)"""
        try:
            if not keypoints or len(keypoints) < 18:
                return 0.0
            
            # ëŒ€ì¹­ ë¶€ìœ„ ìŒ ì •ì˜
            symmetric_pairs = [
                (2, 5),   # right_shoulder, left_shoulder
                (3, 6),   # right_elbow, left_elbow
                (4, 7),   # right_wrist, left_wrist
                (9, 12),  # right_hip, left_hip
                (10, 13), # right_knee, left_knee
                (11, 14), # right_ankle, left_ankle
                (15, 16), # right_eye, left_eye
                (17, 18)  # right_ear, left_ear
            ]
            
            symmetry_scores = []
            confidence_threshold = 0.3
            
            for right_idx, left_idx in symmetric_pairs:
                if (right_idx < len(keypoints) and left_idx < len(keypoints) and
                    len(keypoints[right_idx]) >= 3 and len(keypoints[left_idx]) >= 3):
                    
                    right_kp = keypoints[right_idx]
                    left_kp = keypoints[left_idx]
                    
                    if right_kp[2] > confidence_threshold and left_kp[2] > confidence_threshold:
                        # ì¤‘ì‹¬ì„  ê³„ì‚°
                        center_x = sum(kp[0] for kp in keypoints if len(kp) >= 3 and kp[2] > confidence_threshold) / \
                                 max(len([kp for kp in keypoints if len(kp) >= 3 and kp[2] > confidence_threshold]), 1)
                        
                        right_dist = abs(right_kp[0] - center_x)
                        left_dist = abs(left_kp[0] - center_x)
                        
                        max_dist = max(right_dist, left_dist)
                        if max_dist > 0:
                            symmetry = 1.0 - abs(right_dist - left_dist) / max_dist
                            weighted_symmetry = symmetry * min(right_kp[2], left_kp[2])
                            symmetry_scores.append(weighted_symmetry)
            
            if not symmetry_scores:
                return 0.0
            
            return np.mean(symmetry_scores)
            
        except Exception as e:
            self.logger.debug(f"ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_visibility_score_sync(self, keypoints: List[List[float]]) -> float:
        """í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„± ì ìˆ˜ ê³„ì‚° (ë™ê¸° ì²˜ë¦¬)"""
        try:
            if not keypoints:
                return 0.0
            
            visible_count = 0
            total_confidence = 0.0
            
            for kp in keypoints:
                if len(kp) >= 3:
                    if kp[2] > self.confidence_threshold:
                        visible_count += 1
                        total_confidence += kp[2]
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(keypoints)
            avg_confidence = total_confidence / visible_count
            
            return visibility_ratio * avg_confidence
            
        except Exception as e:
            self.logger.debug(f"ê°€ì‹œì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_overall_quality_score_sync(
        self, head_score: float, torso_score: float, arms_score: float, legs_score: float,
        symmetry_score: float, visibility_score: float, ai_confidence: float
    ) -> float:
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ë™ê¸° ì²˜ë¦¬)"""
        try:
            base_scores = [
                head_score * 0.15,
                torso_score * 0.35,
                arms_score * 0.25,
                legs_score * 0.25
            ]
            
            advanced_scores = [
                symmetry_score * 0.3,
                visibility_score * 0.7
            ]
            
            base_score = sum(base_scores)
            advanced_score = sum(advanced_scores)
            
            overall_score = (base_score * 0.7 + advanced_score * 0.3) * ai_confidence
            
            return max(0.0, min(1.0, overall_score))
            
        except Exception as e:
            self.logger.debug(f"ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

# ==============================================
# ğŸ”¥ 9. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì™„ì „ ë³µì›)
# ==============================================

def validate_keypoints(keypoints_18: List[List[float]]) -> bool:
    """OpenPose 18 keypoints ìœ íš¨ì„± ê²€ì¦"""
    try:
        if len(keypoints_18) != 18:
            return False
        
        for kp in keypoints_18:
            if len(kp) != 3:
                return False
            if not all(isinstance(x, (int, float)) for x in kp):
                return False
            if kp[2] < 0 or kp[2] > 1:
                return False
        
        return True
        
    except Exception:
        return False

def convert_keypoints_to_coco(keypoints_18: List[List[float]]) -> List[List[float]]:
    """OpenPose 18ì„ COCO 17 í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        # OpenPose 18 -> COCO 17 ë§¤í•‘
        op_to_coco_mapping = {
            0: 0,   # nose
            15: 1,  # right_eye -> left_eye (COCO ê´€ì )
            16: 2,  # left_eye -> right_eye
            17: 3,  # right_ear -> left_ear
            18: 4,  # left_ear -> right_ear
            2: 5,   # right_shoulder -> left_shoulder (COCO ê´€ì )
            5: 6,   # left_shoulder -> right_shoulder
            3: 7,   # right_elbow -> left_elbow
            6: 8,   # left_elbow -> right_elbow
            4: 9,   # right_wrist -> left_wrist
            7: 10,  # left_wrist -> right_wrist
            9: 11,  # right_hip -> left_hip
            12: 12, # left_hip -> right_hip
            10: 13, # right_knee -> left_knee
            13: 14, # left_knee -> right_knee
            11: 15, # right_ankle -> left_ankle
            14: 16  # left_ankle -> right_ankle
        }
        
        coco_keypoints = []
        for coco_idx in range(17):
            if coco_idx in op_to_coco_mapping.values():
                op_idx = next(k for k, v in op_to_coco_mapping.items() if v == coco_idx)
                if op_idx < len(keypoints_18):
                    coco_keypoints.append(keypoints_18[op_idx].copy())
                else:
                    coco_keypoints.append([0.0, 0.0, 0.0])
            else:
                coco_keypoints.append([0.0, 0.0, 0.0])
        
        return coco_keypoints
        
    except Exception as e:
        logger.error(f"í‚¤í¬ì¸íŠ¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return [[0.0, 0.0, 0.0]] * 17

def draw_pose_on_image(
    image: Union[np.ndarray, Image.Image],
    keypoints: List[List[float]],
    confidence_threshold: float = 0.5,
    keypoint_size: int = 4,
    line_width: int = 3
) -> Image.Image:
    """ì´ë¯¸ì§€ì— í¬ì¦ˆ ê·¸ë¦¬ê¸°"""
    try:
        # ì´ë¯¸ì§€ ë³€í™˜
        if isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        else:
            pil_image = image.copy()
        
        draw = ImageDraw.Draw(pil_image)
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for i, kp in enumerate(keypoints):
            if len(kp) >= 3 and kp[2] > confidence_threshold:
                x, y = int(kp[0]), int(kp[1])
                color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                
                radius = int(keypoint_size + kp[2] * 6)
                draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                           fill=color, outline=(255, 255, 255), width=2)
        
        # ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸°
        for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
            if (start_idx < len(keypoints) and end_idx < len(keypoints)):
                start_kp = keypoints[start_idx]
                end_kp = keypoints[end_idx]
                
                if (len(start_kp) >= 3 and len(end_kp) >= 3 and
                    start_kp[2] > confidence_threshold and end_kp[2] > confidence_threshold):
                    
                    start_point = (int(start_kp[0]), int(start_kp[1]))
                    end_point = (int(end_kp[0]), int(end_kp[1]))
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    avg_confidence = (start_kp[2] + end_kp[2]) / 2
                    adjusted_width = int(line_width * avg_confidence)
                    
                    draw.line([start_point, end_point], fill=color, width=max(1, adjusted_width))
        
        return pil_image
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
        return image if isinstance(image, Image.Image) else Image.fromarray(image)

def analyze_pose_for_clothing(
    keypoints: List[List[float]],
    clothing_type: str = "default",
    confidence_threshold: float = 0.5,
    strict_analysis: bool = True
) -> Dict[str, Any]:
    """ì˜ë¥˜ë³„ í¬ì¦ˆ ì í•©ì„± ë¶„ì„"""
    try:
        if not keypoints:
            return {
                'suitable_for_fitting': False,
                'issues': ["í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"],
                'recommendations': ["ë” ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”"],
                'pose_score': 0.0,
                'ai_confidence': 0.0
            }
        
        # ì˜ë¥˜ë³„ ê°€ì¤‘ì¹˜
        clothing_weights = {
            'shirt': {'arms': 0.4, 'torso': 0.4, 'visibility': 0.2},
            'dress': {'torso': 0.5, 'arms': 0.3, 'legs': 0.2},
            'pants': {'legs': 0.6, 'torso': 0.3, 'visibility': 0.1},
            'jacket': {'arms': 0.5, 'torso': 0.4, 'visibility': 0.1},
            'skirt': {'torso': 0.4, 'legs': 0.4, 'visibility': 0.2},
            'top': {'torso': 0.5, 'arms': 0.4, 'visibility': 0.1},
            'default': {'torso': 0.4, 'arms': 0.3, 'legs': 0.2, 'visibility': 0.1}
        }
        
        weights = clothing_weights.get(clothing_type, clothing_weights['default'])
        
        # ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°
        def calculate_body_part_score(part_indices: List[int]) -> float:
            visible_count = 0
            total_confidence = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            
            return visibility_ratio * avg_confidence
        
        # ë¶€ìœ„ë³„ ì ìˆ˜
        head_indices = [0, 15, 16, 17, 18]
        torso_indices = [1, 2, 5, 8, 9, 12]
        arm_indices = [2, 3, 4, 5, 6, 7]
        leg_indices = [9, 10, 11, 12, 13, 14]
        
        head_score = calculate_body_part_score(head_indices)
        torso_score = calculate_body_part_score(torso_indices)
        arms_score = calculate_body_part_score(arm_indices)
        legs_score = calculate_body_part_score(leg_indices)
        
        # AI ì‹ ë¢°ë„
        ai_confidence = np.mean([kp[2] for kp in keypoints if len(kp) > 2]) if keypoints else 0.0
        
        # í¬ì¦ˆ ì ìˆ˜
        pose_score = (
            torso_score * weights.get('torso', 0.4) +
            arms_score * weights.get('arms', 0.3) +
            legs_score * weights.get('legs', 0.2) +
            weights.get('visibility', 0.1) * min(head_score, 1.0)
        ) * ai_confidence
        
        # ì í•©ì„± íŒë‹¨
        min_score = 0.8 if strict_analysis else 0.7
        min_confidence = 0.75 if strict_analysis else 0.65
        
        suitable_for_fitting = (pose_score >= min_score and 
                              ai_confidence >= min_confidence)
        
        # ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­
        issues = []
        recommendations = []
        
        if ai_confidence < min_confidence:
            issues.append(f'AI ëª¨ë¸ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({ai_confidence:.3f})')
            recommendations.append('ì¡°ëª…ì´ ì¢‹ì€ í™˜ê²½ì—ì„œ ë” ì„ ëª…í•˜ê²Œ ë‹¤ì‹œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        if torso_score < 0.5:
            issues.append(f'{clothing_type} ì°©ìš©ì— ì¤‘ìš”í•œ ìƒì²´ê°€ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤')
            recommendations.append('ìƒì²´ ì „ì²´ê°€ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        return {
            'suitable_for_fitting': suitable_for_fitting,
            'issues': issues,
            'recommendations': recommendations,
            'pose_score': pose_score,
            'ai_confidence': ai_confidence,
            'detailed_scores': {
                'head': head_score,
                'torso': torso_score,
                'arms': arms_score,
                'legs': legs_score
            },
            'clothing_type': clothing_type,
            'weights_used': weights,
            'real_ai_analysis': True,
            'strict_analysis': strict_analysis
        }
        
    except Exception as e:
        logger.error(f"ì˜ë¥˜ë³„ í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            'suitable_for_fitting': False,
            'issues': ["ë¶„ì„ ì‹¤íŒ¨"],
            'recommendations': ["ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”"],
            'pose_score': 0.0,
            'ai_confidence': 0.0,
            'real_ai_analysis': True
        }

# ==============================================
# ğŸ”¥ 10. í˜¸í™˜ì„± ì§€ì› í•¨ìˆ˜ë“¤ (ì™„ì „ ë³µì›)
# ==============================================

async def create_pose_estimation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    strict_mode: bool = True,
    **kwargs
) -> PoseEstimationStep:
    """
    BaseStepMixin v19.1 í˜¸í™˜ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • Step ìƒì„± í•¨ìˆ˜
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        strict_mode: ì—„ê²© ëª¨ë“œ
        **kwargs: ì¶”ê°€ ì„¤ì •
        
    Returns:
        PoseEstimationStep: ì´ˆê¸°í™”ëœ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • Step
    """
    try:
        # ë””ë°”ì´ìŠ¤ ì²˜ë¦¬
        device_param = None if device == "auto" else device
        
        # config í†µí•©
        if config is None:
            config = {}
        config.update(kwargs)
        config['real_ai_models'] = True
        config['basestep_version'] = '19.1-compatible'
        
        # Step ìƒì„± (BaseStepMixin v19.1 í˜¸í™˜)
        step = PoseEstimationStep(device=device_param, config=config, strict_mode=strict_mode)
        
        # AI ê¸°ë°˜ ì´ˆê¸°í™” ì‹¤í–‰
        initialization_success = await step.initialize()
        
        if not initialization_success:
            error_msg = "BaseStepMixin v19.1 í˜¸í™˜: AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨"
            if strict_mode:
                raise RuntimeError(f"Strict Mode: {error_msg}")
            else:
                step.logger.warning(f"âš ï¸ {error_msg} - Step ìƒì„±ì€ ì™„ë£Œë¨")
        
        return step
        
    except Exception as e:
        logger.error(f"âŒ BaseStepMixin v19.1 í˜¸í™˜ create_pose_estimation_step ì‹¤íŒ¨: {e}")
        if strict_mode:
            raise
        else:
            step = PoseEstimationStep(device='cpu', strict_mode=False)
            return step

def create_pose_estimation_step_sync(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    strict_mode: bool = True,
    **kwargs
) -> PoseEstimationStep:
    """ë™ê¸°ì‹ BaseStepMixin v19.1 í˜¸í™˜ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • Step ìƒì„±"""
    try:
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            create_pose_estimation_step(device, config, strict_mode, **kwargs)
        )
    except Exception as e:
        logger.error(f"âŒ BaseStepMixin v19.1 í˜¸í™˜ create_pose_estimation_step_sync ì‹¤íŒ¨: {e}")
        if strict_mode:
            raise
        else:
            return PoseEstimationStep(device='cpu', strict_mode=False)

# ==============================================
# ğŸ”¥ 11. íŒŒì´í”„ë¼ì¸ ì§€ì› (StepInterface í˜¸í™˜)
# ==============================================

class PoseEstimationStepWithPipeline(PoseEstimationStep):
    """íŒŒì´í”„ë¼ì¸ ì§€ì›ì´ í¬í•¨ëœ PoseEstimationStep"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì„¤ì •
        self.pipeline_mode = kwargs.get("pipeline_mode", False)
        
        # íŒŒì´í”„ë¼ì¸ ì†ì„±
        self.pipeline_position = "middle"  # Step 02ëŠ” ì¤‘ê°„ ë‹¨ê³„
        self.accepts_pipeline_input = True
        self.provides_pipeline_output = True
    
    async def process_pipeline(self, input_data: PipelineStepResult) -> PipelineStepResult:
        """
        íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì²˜ë¦¬ - Step 01 ê²°ê³¼ë¥¼ ë°›ì•„ í¬ì¦ˆ ì¶”ì • í›„ Step 03, 04ë¡œ ì „ë‹¬
        
        Args:
            input_data: Step 01ì—ì„œ ì „ë‹¬ë°›ì€ íŒŒì´í”„ë¼ì¸ ë°ì´í„°
            
        Returns:
            PipelineStepResult: Step 03, 04ë¡œ ì „ë‹¬í•  í¬ì¦ˆ ì¶”ì • ê²°ê³¼
        """
        try:
            start_time = time.time()
            self.logger.info(f"ğŸ”— {self.step_name} íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì²˜ë¦¬ ì‹œì‘")
            
            # ì´ˆê¸°í™” ê²€ì¦
            if not self.is_initialized:
                if not await self.initialize():
                    error_msg = "íŒŒì´í”„ë¼ì¸: AI ì´ˆê¸°í™” ì‹¤íŒ¨"
                    return PipelineStepResult(
                        step_id=2, step_name="pose_estimation",
                        success=False, error=error_msg
                    )
            
            # Step 01 ê²°ê³¼ ë°›ê¸°
            if not hasattr(input_data, 'for_step_02') or not input_data.for_step_02:
                error_msg = "Step 01 ë°ì´í„°ê°€ ì—†ìŒ"
                self.logger.error(f"âŒ {error_msg}")
                return PipelineStepResult(
                    step_id=2, step_name="pose_estimation",
                    success=False, error=error_msg
                )
            
            step01_data = input_data.for_step_02
            parsed_image = step01_data.get("parsed_image")
            body_masks = step01_data.get("body_masks", {})
            human_region = step01_data.get("human_region")
            
            if parsed_image is None:
                error_msg = "Step 01ì—ì„œ íŒŒì‹±ëœ ì´ë¯¸ì§€ê°€ ì—†ìŒ"
                self.logger.error(f"âŒ {error_msg}")
                return PipelineStepResult(
                    step_id=2, step_name="pose_estimation",
                    success=False, error=error_msg
                )
            
            # íŒŒì´í”„ë¼ì¸ìš© í¬ì¦ˆ ì¶”ì • AI ì²˜ë¦¬ (ë™ê¸° ì²˜ë¦¬)
            pose_result = self._run_pose_estimation_pipeline_sync(parsed_image, body_masks, human_region)
            
            if not pose_result.get('success', False):
                error_msg = f"íŒŒì´í”„ë¼ì¸ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {pose_result.get('error', 'Unknown Error')}"
                self.logger.error(f"âŒ {error_msg}")
                return PipelineStepResult(
                    step_id=2, step_name="pose_estimation",
                    success=False, error=error_msg
                )
            
            # íŒŒì´í”„ë¼ì¸ ë°ì´í„° ì¤€ë¹„
            pipeline_data = PipelineStepResult(
                step_id=2,
                step_name="pose_estimation",
                success=True,
                
                # Step 03 (Cloth Segmentation)ìœ¼ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_03={
                    **getattr(input_data, 'for_step_03', {}),  # Step 01 ë°ì´í„° ê³„ìŠ¹
                    "pose_keypoints": pose_result["keypoints"],
                    "pose_skeleton": pose_result.get("skeleton_structure", {}),
                    "pose_confidence": pose_result.get("confidence_scores", []),
                    "joint_connections": pose_result.get("joint_connections", []),
                    "visible_keypoints": pose_result.get("visible_keypoints", [])
                },
                
                # Step 04 (Geometric Matching)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_04={
                    "keypoints_for_matching": pose_result["keypoints"],
                    "joint_connections": pose_result.get("joint_connections", []),
                    "pose_angles": pose_result.get("joint_angles", {}),
                    "body_orientation": pose_result.get("body_orientation", {}),
                    "pose_landmarks": pose_result.get("landmarks", {}),
                    "skeleton_structure": pose_result.get("skeleton_structure", {})
                },
                
                # Step 05 (Cloth Warping)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_05={
                    "reference_keypoints": pose_result["keypoints"],
                    "body_proportions": pose_result.get("body_proportions", {}),
                    "pose_type": pose_result.get("pose_type", "standing")
                },
                
                # Step 06 (Virtual Fitting)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_06={
                    "person_keypoints": pose_result["keypoints"],
                    "pose_confidence": pose_result.get("confidence_scores", []),
                    "body_orientation": pose_result.get("body_orientation", {})
                },
                
                # Step 07 (Post Processing)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_07={
                    "original_keypoints": pose_result["keypoints"]
                },
                
                # Step 08 (Quality Assessment)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_08={
                    "pose_quality_metrics": pose_result.get("pose_analysis", {}),
                    "keypoints_confidence": pose_result.get("confidence_scores", [])
                },
                
                # ì´ì „ ë‹¨ê³„ ë°ì´í„° ë³´ì¡´ ë° í™•ì¥
                previous_data={
                    **getattr(input_data, 'original_data', {}),
                    "step01_results": getattr(input_data, 'for_step_02', {}),
                    "step02_results": pose_result
                },
                
                original_data=getattr(input_data, 'original_data', {}),
                
                # ë©”íƒ€ë°ì´í„°
                metadata={
                    "processing_time": time.time() - start_time,
                    "ai_models_used": pose_result.get("models_used", []),
                    "num_keypoints_detected": len(pose_result.get("keypoints", [])),
                    "ready_for_next_steps": ["step_03", "step_04", "step_05", "step_06"],
                    "execution_mode": "pipeline",
                    "pipeline_progress": "2/8 ë‹¨ê³„ ì™„ë£Œ",
                    "primary_model": pose_result.get("primary_model", "unknown"),
                    "enhanced_by_diffusion": pose_result.get("enhanced_by_diffusion", False)
                },
                
                processing_time=time.time() - start_time
            )
            
            self.logger.info(f"âœ… {self.step_name} íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì²˜ë¦¬ ì™„ë£Œ")
            self.logger.info(f"ğŸ¯ ê²€ì¶œëœ í‚¤í¬ì¸íŠ¸: {len(pose_result.get('keypoints', []))}ê°œ")
            self.logger.info(f"â¡ï¸ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë°ì´í„° ì „ë‹¬ ì¤€ë¹„ ì™„ë£Œ")
            
            return pipeline_data
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìŠ¤íƒ: {traceback.format_exc()}")
            return PipelineStepResult(
                step_id=2, step_name="pose_estimation",
                success=False, error=str(e),
                processing_time=time.time() - start_time if 'start_time' in locals() else 0.0
            )
    
    def _run_pose_estimation_pipeline_sync(
        self, 
        parsed_image: Union[torch.Tensor, np.ndarray, Image.Image], 
        body_masks: Dict[str, Any],
        human_region: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì „ìš© í¬ì¦ˆ ì¶”ì • AI ì²˜ë¦¬ (ë™ê¸° ì²˜ë¦¬)"""
        try:
            inference_start = time.time()
            self.logger.info(f"ğŸ§  íŒŒì´í”„ë¼ì¸ í¬ì¦ˆ ì¶”ì • AI ì‹œì‘ (ë™ê¸°)...")
            
            if not self.ai_models:
                self._load_all_ai_models_sync()
            
            if not self.ai_models:
                return {'success': False, 'error': 'ë¡œë”©ëœ AI ëª¨ë¸ì´ ì—†ìŒ'}
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (íŒŒì´í”„ë¼ì¸ìš©)
            if isinstance(parsed_image, torch.Tensor):
                image = to_pil_image(parsed_image.cpu())
            elif isinstance(parsed_image, np.ndarray):
                image = Image.fromarray(parsed_image)
            else:
                image = parsed_image
            
            # Body masks í™œìš©í•œ ê´€ì‹¬ ì˜ì—­ ì¶”ì¶œ
            if body_masks and human_region:
                # ì¸ì²´ ì˜ì—­ì— ì§‘ì¤‘í•œ í¬ì¦ˆ ì¶”ì •
                image = self._focus_on_human_region_sync(image, human_region)
            
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰ (ë™ê¸° ì²˜ë¦¬)
            ai_result = self._run_real_ai_inference_sync(image, {})
            
            if not ai_result.get('success', False):
                return ai_result
            
            # íŒŒì´í”„ë¼ì¸ ì „ìš© ì¶”ê°€ ë¶„ì„
            pipeline_analysis = self._analyze_for_pipeline_sync(ai_result, body_masks)
            ai_result.update(pipeline_analysis)
            
            inference_time = time.time() - inference_start
            ai_result['inference_time'] = inference_time
            
            self.logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ í¬ì¦ˆ ì¶”ì • AI ì™„ë£Œ ({inference_time:.3f}ì´ˆ)")
            
            return ai_result
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ í¬ì¦ˆ ì¶”ì • AI ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _focus_on_human_region_sync(self, image: Image.Image, human_region: Dict[str, Any]) -> Image.Image:
        """ì¸ì²´ ì˜ì—­ì— ì§‘ì¤‘í•œ ì´ë¯¸ì§€ ì²˜ë¦¬ (ë™ê¸° ì²˜ë¦¬)"""
        try:
            if 'bbox' in human_region:
                bbox = human_region['bbox']
                x1, y1, x2, y2 = bbox
                # ì¸ì²´ ì˜ì—­ í¬ë¡­
                cropped = image.crop((x1, y1, x2, y2))
                # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                return cropped.resize(image.size, Image.Resampling.BILINEAR)
            return image
        except Exception as e:
            self.logger.debug(f"ì¸ì²´ ì˜ì—­ ì§‘ì¤‘ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def _analyze_for_pipeline_sync(self, ai_result: Dict[str, Any], body_masks: Dict[str, Any]) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì „ìš© ì¶”ê°€ ë¶„ì„ (ë™ê¸° ì²˜ë¦¬)"""
        try:
            keypoints = ai_result.get('keypoints', [])
            
            # ê°€ì‹œì„± ë¶„ì„ (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ í™œìš©)
            visible_keypoints = []
            confidence_threshold = 0.5
            
            for i, kp in enumerate(keypoints):
                if len(kp) >= 3 and kp[2] > confidence_threshold:
                    keypoint_name = OPENPOSE_18_KEYPOINTS[i] if i < len(OPENPOSE_18_KEYPOINTS) else f"kp_{i}"
                    visible_keypoints.append({
                        'index': i,
                        'name': keypoint_name,
                        'position': [kp[0], kp[1]],
                        'confidence': kp[2]
                    })
            
            # í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜ (ë‹¤ìŒ ë‹¨ê³„ ìµœì í™”ìš©)
            pose_type = self._classify_pose_type_sync(keypoints)
            
            # Body masksì™€ì˜ ì¼ì¹˜ì„± ë¶„ì„
            mask_consistency = self._analyze_mask_consistency_sync(keypoints, body_masks)
            
            return {
                'visible_keypoints': visible_keypoints,
                'pose_type': pose_type,
                'mask_consistency': mask_consistency,
                'pipeline_ready': True
            }
            
        except Exception as e:
            self.logger.debug(f"íŒŒì´í”„ë¼ì¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'pipeline_ready': False}
    
    def _classify_pose_type_sync(self, keypoints: List[List[float]]) -> str:
        """í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜ (ë™ê¸° ì²˜ë¦¬)"""
        try:
            if not keypoints or len(keypoints) < 18:
                return "unknown"
            
            # íŒ” ê°ë„ ë¶„ì„
            arms_extended = False
            if all(i < len(keypoints) and len(keypoints[i]) >= 3 for i in [2, 3, 4, 5, 6, 7]):
                # íŒ”ì´ í¼ì³ì ¸ ìˆëŠ”ì§€ í™•ì¸
                right_arm_angle = self._calculate_arm_angle_sync(keypoints[2], keypoints[3], keypoints[4])
                left_arm_angle = self._calculate_arm_angle_sync(keypoints[5], keypoints[6], keypoints[7])
                
                if right_arm_angle > 150 and left_arm_angle > 150:
                    arms_extended = True
            
            # ë‹¤ë¦¬ ë¶„ì„
            legs_apart = False
            if all(i < len(keypoints) and len(keypoints[i]) >= 3 for i in [9, 12, 11, 14]):
                hip_distance = abs(keypoints[9][0] - keypoints[12][0])
                ankle_distance = abs(keypoints[11][0] - keypoints[14][0])
                if ankle_distance > hip_distance * 1.5:
                    legs_apart = True
            
            # í¬ì¦ˆ ë¶„ë¥˜
            if arms_extended and not legs_apart:
                return "t_pose"
            elif arms_extended and legs_apart:
                return "star_pose" 
            elif not arms_extended and not legs_apart:
                return "standing"
            else:
                return "dynamic"
                
        except Exception as e:
            self.logger.debug(f"í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return "unknown"
    
    def _calculate_arm_angle_sync(self, shoulder: List[float], elbow: List[float], wrist: List[float]) -> float:
        """íŒ” ê°ë„ ê³„ì‚° (ë™ê¸° ì²˜ë¦¬)"""
        try:
            if all(len(kp) >= 3 and kp[2] > 0.3 for kp in [shoulder, elbow, wrist]):
                v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]])
                v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]])
                
                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
                cos_angle = np.clip(cos_angle, -1.0, 1.0)
                angle = np.arccos(cos_angle)
                
                return np.degrees(angle)
            return 0.0
        except:
            return 0.0
    
    def _analyze_mask_consistency_sync(self, keypoints: List[List[float]], body_masks: Dict[str, Any]) -> Dict[str, Any]:
        """Body masksì™€ í‚¤í¬ì¸íŠ¸ ì¼ì¹˜ì„± ë¶„ì„ (ë™ê¸° ì²˜ë¦¬)"""
        try:
            consistency = {
                'overall_score': 0.0,
                'detailed_scores': {},
                'issues': []
            }
            
            # ê°„ë‹¨í•œ ì¼ì¹˜ì„± ë¶„ì„ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
            visible_keypoints = sum(1 for kp in keypoints if len(kp) >= 3 and kp[2] > 0.5)
            total_keypoints = len(keypoints)
            
            if total_keypoints > 0:
                consistency['overall_score'] = visible_keypoints / total_keypoints
            
            if consistency['overall_score'] < 0.6:
                consistency['issues'].append("í‚¤í¬ì¸íŠ¸ì™€ ë§ˆìŠ¤í¬ ë¶ˆì¼ì¹˜")
            
            return consistency
            
        except Exception as e:
            self.logger.debug(f"ë§ˆìŠ¤í¬ ì¼ì¹˜ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'overall_score': 0.0, 'issues': ['ë¶„ì„ ì‹¤íŒ¨']}

# ==============================================
# ğŸ”¥ 12. í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ (ì™„ì „ ë³µì›)
# ==============================================

async def test_pose_estimation_step():
    """BaseStepMixin v19.1 í˜¸í™˜ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ BaseStepMixin v19.1 í˜¸í™˜ ê°•í™”ëœ AI ì¶”ë¡  í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        print("=" * 80)
        
        # AI ê¸°ë°˜ Step ìƒì„±
        step = await create_pose_estimation_step(
            device="auto",
            strict_mode=True,
            config={
                'confidence_threshold': 0.5,
                'visualization_enabled': True,
                'cache_enabled': True,
                'detailed_analysis': True,
                'real_ai_models': True,
                'basestep_version': '19.1-compatible'
            }
        )
        
        # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
        dummy_image = np.zeros((512, 512, 3), dtype=np.uint8)
        dummy_image_pil = Image.fromarray(dummy_image)
        
        print(f"ğŸ“‹ BaseStepMixin v19.1 í˜¸í™˜ ê°•í™”ëœ AI Step ì •ë³´:")
        step_status = step.get_status()
        print(f"   ğŸ¯ Step: {step_status['step_name']}")
        print(f"   ğŸ”¢ ë²„ì „: {step_status['version']}")
        print(f"   ğŸ¤– ë¡œë”©ëœ AI ëª¨ë¸ë“¤: {step_status.get('loaded_models', [])}")
        print(f"   ğŸ’ ì´ˆê¸°í™” ìƒíƒœ: {step_status.get('is_initialized', False)}")
        print(f"   ğŸ§  ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ: {step_status.get('has_model', False)}")
        print(f"   ğŸ¤– ì‹¤ì œ AI ê¸°ë°˜: {step_status.get('real_ai_models', False)}")
        
        # BaseStepMixinì˜ process() ë©”ì„œë“œ í…ŒìŠ¤íŠ¸ (ë‚´ë¶€ì ìœ¼ë¡œ _run_ai_inference í˜¸ì¶œ)
        result = await step.process(image=dummy_image_pil, clothing_type="shirt")
        
        if result['success']:
            print(f"âœ… BaseStepMixin v19.1 í˜¸í™˜ ê°•í™”ëœ AI í¬ì¦ˆ ì¶”ì • ì„±ê³µ")
            print(f"ğŸ¯ ê²€ì¶œëœ í‚¤í¬ì¸íŠ¸ ìˆ˜: {len(result.get('keypoints', []))}")
            print(f"ğŸ–ï¸ AI ì‹ ë¢°ë„: {result.get('ai_confidence', 0):.3f}")
            print(f"ğŸ¤– ì‚¬ìš©ëœ AI ëª¨ë¸ë“¤: {result.get('models_used', [])}")
            print(f"ğŸ† ì£¼ AI ëª¨ë¸: {result.get('primary_model', 'unknown')}")
            print(f"âš¡ ì¶”ë¡  ì‹œê°„: {result.get('inference_time', 0):.3f}ì´ˆ")
            print(f"ğŸ¨ Diffusion í–¥ìƒ: {result.get('enhanced_by_diffusion', False)}")
            print(f"ğŸ”— BaseStepMixin í˜¸í™˜: v19.1")
        else:
            print(f"âŒ BaseStepMixin v19.1 í˜¸í™˜ ê°•í™”ëœ AI í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {result.get('error', 'Unknown Error')}")
        
        # ì •ë¦¬
        cleanup_result = await step.cleanup()
        print(f"ğŸ§¹ BaseStepMixin v19.1 í˜¸í™˜ AI ë¦¬ì†ŒìŠ¤ ì •ë¦¬: {cleanup_result.get('success', False)}")
        
    except Exception as e:
        print(f"âŒ BaseStepMixin v19.1 í˜¸í™˜ ê°•í™”ëœ AI í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_real_ai_models():
    """ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ§  ê°•í™”ëœ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # SmartModelPathMapper í…ŒìŠ¤íŠ¸
        try:
            mapper = Step02ModelMapper()
            model_paths = mapper.get_step02_model_paths()
            print(f"âœ… SmartModelPathMapper ë™ì‘: {len(model_paths)}ê°œ ê²½ë¡œ")
            for model_name, path in model_paths.items():
                status = "ì¡´ì¬" if path and path.exists() else "ì—†ìŒ"
                print(f"   {model_name}: {status} ({path})")
        except Exception as e:
            print(f"âŒ SmartModelPathMapper í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ë”ë¯¸ ëª¨ë¸ íŒŒì¼ë¡œ AI ëª¨ë¸ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸
        dummy_model_path = Path("dummy_model.pt")
        
        # RealYOLOv8PoseModel í…ŒìŠ¤íŠ¸
        try:
            yolo_model = RealYOLOv8PoseModel(dummy_model_path, "cpu")
            print(f"âœ… RealYOLOv8PoseModel ìƒì„± ì„±ê³µ: {yolo_model}")
        except Exception as e:
            print(f"âŒ RealYOLOv8PoseModel í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # RealOpenPoseModel í…ŒìŠ¤íŠ¸
        try:
            openpose_model = RealOpenPoseModel(dummy_model_path, "cpu")
            print(f"âœ… RealOpenPoseModel ìƒì„± ì„±ê³µ: {openpose_model}")
        except Exception as e:
            print(f"âŒ RealOpenPoseModel í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # RealHRNetModel í…ŒìŠ¤íŠ¸
        try:
            hrnet_model = RealHRNetModel.from_checkpoint("", "cpu")
            print(f"âœ… RealHRNetModel ìƒì„± ì„±ê³µ: {hrnet_model}")
            model_info = hrnet_model.get_model_info()
            print(f"   - íŒŒë¼ë¯¸í„°: {model_info['parameter_count']:,}")
            print(f"   - ì„œë¸Œí”½ì…€ ì •í™•ë„: {model_info['subpixel_accuracy']}")
        except Exception as e:
            print(f"âŒ RealHRNetModel í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # RealDiffusionPoseModel í…ŒìŠ¤íŠ¸
        try:
            diffusion_model = RealDiffusionPoseModel(dummy_model_path, "cpu")
            print(f"âœ… RealDiffusionPoseModel ìƒì„± ì„±ê³µ: {diffusion_model}")
        except Exception as e:
            print(f"âŒ RealDiffusionPoseModel í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # RealBodyPoseModel í…ŒìŠ¤íŠ¸
        try:
            body_pose_model = RealBodyPoseModel(dummy_model_path, "cpu")
            print(f"âœ… RealBodyPoseModel ìƒì„± ì„±ê³µ: {body_pose_model}")
        except Exception as e:
            print(f"âŒ RealBodyPoseModel í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
    except Exception as e:
        print(f"âŒ ê°•í™”ëœ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_utilities():
    """ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”„ ê°•í™”ëœ ìœ í‹¸ë¦¬í‹° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # ë”ë¯¸ OpenPose 18 í‚¤í¬ì¸íŠ¸
        openpose_keypoints = [
            [100, 50, 0.9],   # nose
            [100, 80, 0.8],   # neck
            [80, 100, 0.7],   # right_shoulder
            [70, 130, 0.6],   # right_elbow
            [60, 160, 0.5],   # right_wrist
            [120, 100, 0.7],  # left_shoulder
            [130, 130, 0.6],  # left_elbow
            [140, 160, 0.5],  # left_wrist
            [100, 200, 0.8],  # middle_hip
            [90, 200, 0.7],   # right_hip
            [85, 250, 0.6],   # right_knee
            [80, 300, 0.5],   # right_ankle
            [110, 200, 0.7],  # left_hip
            [115, 250, 0.6],  # left_knee
            [120, 300, 0.5],  # left_ankle
            [95, 40, 0.8],    # right_eye
            [105, 40, 0.8],   # left_eye
            [90, 45, 0.7],    # right_ear
            [110, 45, 0.7]    # left_ear
        ]
        
        # ìœ íš¨ì„± ê²€ì¦
        is_valid = validate_keypoints(openpose_keypoints)
        print(f"âœ… OpenPose 18 ìœ íš¨ì„±: {is_valid}")
        
        # COCO 17ë¡œ ë³€í™˜
        coco_keypoints = convert_keypoints_to_coco(openpose_keypoints)
        print(f"ğŸ”„ COCO 17 ë³€í™˜: {len(coco_keypoints)}ê°œ í‚¤í¬ì¸íŠ¸")
        
        # ì˜ë¥˜ë³„ ë¶„ì„
        analysis = analyze_pose_for_clothing(
            openpose_keypoints, 
            clothing_type="shirt",
            strict_analysis=True
        )
        print(f"ğŸ‘• ì˜ë¥˜ ì í•©ì„± ë¶„ì„:")
        print(f"   ì í•©ì„±: {analysis['suitable_for_fitting']}")
        print(f"   ì ìˆ˜: {analysis['pose_score']:.3f}")
        print(f"   AI ì‹ ë¢°ë„: {analysis['ai_confidence']:.3f}")
        print(f"   ì‹¤ì œ AI ë¶„ì„: {analysis['real_ai_analysis']}")
        
        # ì´ë¯¸ì§€ì— í¬ì¦ˆ ê·¸ë¦¬ê¸° í…ŒìŠ¤íŠ¸
        dummy_image = Image.new('RGB', (256, 256), (128, 128, 128))
        pose_image = draw_pose_on_image(dummy_image, openpose_keypoints)
        print(f"ğŸ–¼ï¸ í¬ì¦ˆ ê·¸ë¦¬ê¸°: {pose_image.size}")
        
    except Exception as e:
        print(f"âŒ ê°•í™”ëœ ìœ í‹¸ë¦¬í‹° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

async def test_pipeline_functionality():
    """íŒŒì´í”„ë¼ì¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”— ê°•í™”ëœ íŒŒì´í”„ë¼ì¸ ì—°ê²° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # íŒŒì´í”„ë¼ì¸ìš© Step ìƒì„±
        step = PoseEstimationStepWithPipeline(
            device="auto",
            config={
                'pipeline_mode': True,
                'confidence_threshold': 0.5,
                'real_ai_models': True
            }
        )
        
        # ë”ë¯¸ íŒŒì´í”„ë¼ì¸ ì…ë ¥ ë°ì´í„° ìƒì„±
        dummy_step01_result = PipelineStepResult(
            step_id=1,
            step_name="human_parsing",
            success=True,
            for_step_02={
                "parsed_image": Image.new('RGB', (512, 512), (128, 128, 128)),
                "body_masks": {"person": "dummy_mask"},
                "human_region": {"bbox": [50, 50, 450, 450]}
            },
            for_step_03={
                "person_parsing": "dummy_parsing",
                "clothing_areas": "dummy_areas"
            },
            original_data={
                "person_image": "original_image"
            }
        )
        
        print(f"ğŸ“‹ íŒŒì´í”„ë¼ì¸ Step ì •ë³´:")
        step_status = step.get_status()
        print(f"   ğŸ¯ Step: {step_status['step_name']}")
        print(f"   ğŸ”— íŒŒì´í”„ë¼ì¸ ëª¨ë“œ: {getattr(step, 'pipeline_mode', False)}")
        
        # íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        pipeline_result = await step.process_pipeline(dummy_step01_result)
        
        if pipeline_result.success:
            print(f"âœ… íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì„±ê³µ")
            print(f"ğŸ¯ Step 03 ì „ë‹¬ ë°ì´í„°: {len(pipeline_result.for_step_03)}ê°œ í•­ëª©")
            print(f"ğŸ¯ Step 04 ì „ë‹¬ ë°ì´í„°: {len(pipeline_result.for_step_04)}ê°œ í•­ëª©")
            print(f"âš¡ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹œê°„: {pipeline_result.processing_time:.3f}ì´ˆ")
        else:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {pipeline_result.error}")
        
        await step.cleanup()
        
    except Exception as e:
        print(f"âŒ ê°•í™”ëœ íŒŒì´í”„ë¼ì¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 13. ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ (ì™„ì „ ë³µì›)
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤ (ê°•í™”ëœ AI ê¸°ë°˜ + íŒŒì´í”„ë¼ì¸ ì§€ì›)
    'PoseEstimationStep',
    'PoseEstimationStepWithPipeline',
    'RealYOLOv8PoseModel',
    'RealOpenPoseModel',
    'RealHRNetModel',
    'RealDiffusionPoseModel',
    'RealBodyPoseModel',
    'SmartModelPathMapper',
    'Step02ModelMapper',
    'PoseMetrics',
    'PoseModel',
    'PoseQuality',
    
    # íŒŒì´í”„ë¼ì¸ ë°ì´í„° êµ¬ì¡°
    'PipelineStepResult',
    'PipelineInputData',
    
    # ìƒì„± í•¨ìˆ˜ë“¤ (BaseStepMixin v19.1 í˜¸í™˜)
    'create_pose_estimation_step',
    'create_pose_estimation_step_sync',
    
    # ë™ì  import í•¨ìˆ˜ë“¤
    'get_base_step_mixin_class',
    'get_model_loader',
    'get_memory_manager',
    'get_step_factory',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ê°•í™”ëœ AI ê¸°ë°˜)
    'validate_keypoints',
    'convert_keypoints_to_coco',
    'draw_pose_on_image',
    'analyze_pose_for_clothing',
    
    # ìƒìˆ˜ë“¤
    'OPENPOSE_18_KEYPOINTS',
    'KEYPOINT_COLORS',
    'SKELETON_CONNECTIONS',
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ (BaseStepMixin v19.1 í˜¸í™˜)
    'test_pose_estimation_step',
    'test_real_ai_models',
    'test_utilities',
    'test_pipeline_functionality'
]

# ==============================================
# ğŸ”¥ 14. ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê·¸ (ì™„ì „ ê°•í™”)
# ==============================================

logger.info("ğŸ”¥ BaseStepMixin v19.1 í˜¸í™˜ ì™„ì „ ê°•í™”ëœ AI ì¶”ë¡  PoseEstimationStep v6.0 ë¡œë“œ ì™„ë£Œ")
logger.info("âœ… BaseStepMixinì˜ _run_ai_inference() ë™ê¸° ë©”ì„œë“œ ì™„ì „ êµ¬í˜„")
logger.info("âœ… ê°•í™”ëœ AI ì¶”ë¡  ì—”ì§„ - ëª¨ë“  ê¸°ëŠ¥ ë³µì› + ì‹ ê·œ ê¸°ëŠ¥ ì¶”ê°€")
logger.info("âœ… ì˜¬ë°”ë¥¸ Step í´ë˜ìŠ¤ êµ¬í˜„ ê°€ì´ë“œ ì™„ì „ ì¤€ìˆ˜")
logger.info("âœ… ë™ê¸° ì²˜ë¦¬ë¡œ async/await ë¬¸ì œ ì™„ì „ í•´ê²°")
logger.info("âœ… StepInterface íŒŒì´í”„ë¼ì¸ ì§€ì› ìœ ì§€")
logger.info("âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
logger.info("ğŸ¤– ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (3.4GB): OpenPose, YOLOv8, HRNet, Diffusion, Body Pose")
logger.info("ğŸ”— SmartModelPathMapper í™œìš©í•œ ë™ì  íŒŒì¼ ê²½ë¡œ íƒì§€")
logger.info("ğŸ§  ê°•í™”ëœ AI ì¶”ë¡  ì—”ì§„ êµ¬í˜„ (YOLOv8, OpenPose, HRNet, Diffusion)")
logger.info("ğŸ¯ 18ê°œ í‚¤í¬ì¸íŠ¸ ì™„ì „ ê²€ì¶œ ë° ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±")
logger.info("ğŸ”— íŒŒì´í”„ë¼ì¸ ì—°ê²°: Step 01 â†’ Step 02 â†’ Step 03,04,05,06")
logger.info("ğŸ“Š PipelineStepResult ë°ì´í„° êµ¬ì¡° ì™„ì „ ì§€ì›")
logger.info("ğŸ M3 Max MPS ê°€ì† ìµœì í™”")
logger.info("ğŸ conda í™˜ê²½ ìš°ì„  ì§€ì›")
logger.info("âš¡ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ â†’ ì‹¤ì œ ì¶”ë¡  (ë™ê¸° ì²˜ë¦¬)")
logger.info("ğŸ¨ Diffusion ê¸°ë°˜ í¬ì¦ˆ í’ˆì§ˆ í–¥ìƒ")
logger.info("ğŸ“Š ì™„ì „í•œ í¬ì¦ˆ ë¶„ì„ - ê°ë„, ë¹„ìœ¨, ëŒ€ì¹­ì„±, ê°€ì‹œì„±, í’ˆì§ˆ í‰ê°€")
logger.info("ğŸš€ í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± + ê°•í™”ëœ AI ëª¨ë¸ ê¸°ë°˜ + íŒŒì´í”„ë¼ì¸ ì§€ì›")

# ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê¹…
logger.info(f"ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ: PyTorch={TORCH_AVAILABLE}, M3 Max={IS_M3_MAX}, Device={DEVICE}")
logger.info(f"ğŸ¤– AI ë¼ì´ë¸ŒëŸ¬ë¦¬: Ultralytics={ULTRALYTICS_AVAILABLE}, MediaPipe={MEDIAPIPE_AVAILABLE}, Transformers={TRANSFORMERS_AVAILABLE}")
logger.info(f"ğŸ”§ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „: PyTorch={TORCH_VERSION}")
logger.info(f"ğŸ’¾ Safetensors: {'í™œì„±í™”' if SAFETENSORS_AVAILABLE else 'ë¹„í™œì„±í™”'}")
logger.info(f"ğŸ”— BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜: _run_ai_inference() ë™ê¸° ë©”ì„œë“œ + íŒŒì´í”„ë¼ì¸ íŒ¨í„´")
logger.info(f"ğŸ¤– ê°•í™”ëœ AI ê¸°ë°˜ ì—°ì‚°: ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ ëª¨ë¸ í´ë˜ìŠ¤ â†’ ì¶”ë¡  ì—”ì§„ (ë™ê¸°)")
logger.info(f"ğŸ¯ ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ë“¤: YOLOv8 6.5MB, OpenPose 97.8MB, HRNet (ê³ ì •ë°€), Diffusion 1378MB, Body Pose 97.8MB")
logger.info(f"ğŸ”— íŒŒì´í”„ë¼ì¸ ì§€ì›: ê°œë³„ ì‹¤í–‰(process) + íŒŒì´í”„ë¼ì¸ ì—°ê²°(process_pipeline)")

# ==============================================
# ğŸ”¥ 15. ë©”ì¸ ì‹¤í–‰ë¶€ (ì™„ì „ ê°•í™”ëœ ê²€ì¦)
# ==============================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¯ MyCloset AI Step 02 - BaseStepMixin v19.1 í˜¸í™˜ + ì™„ì „ ê°•í™”ëœ AI ì¶”ë¡ ")
    print("=" * 80)
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    async def run_all_tests():
        await test_pose_estimation_step()
        print("\n" + "=" * 80)
        test_real_ai_models()
        print("\n" + "=" * 80)
        test_utilities()
        print("\n" + "=" * 80)
        await test_pipeline_functionality()
    
    try:
        asyncio.run(run_all_tests())
    except Exception as e:
        print(f"âŒ BaseStepMixin v19.1 í˜¸í™˜ ê°•í™”ëœ AI ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 80)
    print("âœ¨ BaseStepMixin v19.1 í˜¸í™˜ + ì™„ì „ ê°•í™”ëœ AI ì¶”ë¡  í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("ğŸ”— BaseStepMixin v19.1 ì™„ì „ í˜¸í™˜ - _run_ai_inference() ë™ê¸° ë©”ì„œë“œ êµ¬í˜„")
    print("ğŸ”— ì´ì¤‘ ê¸°ëŠ¥ ì§€ì›: ê°œë³„ ì‹¤í–‰(process) + íŒŒì´í”„ë¼ì¸ ì—°ê²°(process_pipeline)")
    print("ğŸ¤– TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
    print("ğŸ§  ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (3.4GB): OpenPose, YOLOv8, HRNet, Diffusion")
    print("âš¡ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ â†’ ì‹¤ì œ ì¶”ë¡  (ë™ê¸° ì²˜ë¦¬)")
    print("ğŸ¯ 18ê°œ í‚¤í¬ì¸íŠ¸ ì™„ì „ ê²€ì¶œ + ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±")
    print("ğŸ”— íŒŒì´í”„ë¼ì¸ ë°ì´í„° ì „ë‹¬: Step 01 â†’ Step 02 â†’ Step 03,04,05,06")
    print("ğŸ’‰ ì™„ë²½í•œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´")
    print("ğŸ”’ ì˜¬ë°”ë¥¸ Step í´ë˜ìŠ¤ êµ¬í˜„ ê°€ì´ë“œ ì™„ì „ ì¤€ìˆ˜")
    print("ğŸ¯ ê°•í™”ëœ AI ì—°ì‚° + ì§„ì§œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ + íŒŒì´í”„ë¼ì¸ ì§€ì›")
    print("=" * 80)