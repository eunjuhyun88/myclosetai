#!/usr/bin/env python3
"""
üî• MyCloset AI - Step 02: Pose Estimation - Common Imports Integration
====================================================================

‚úÖ Common Imports ÏãúÏä§ÌÖú ÏôÑÏ†Ñ ÌÜµÌï© - Ï§ëÎ≥µ import Î∏îÎ°ù Ï†úÍ±∞
‚úÖ Central Hub DI Container v7.0 ÏôÑÏ†Ñ Ïó∞Îèô
‚úÖ BaseStepMixin ÏÉÅÏÜç Ìå®ÌÑ¥ (Human Parsing StepÍ≥º ÎèôÏùº)
‚úÖ MediaPipe Pose Î™®Îç∏ ÏßÄÏõê (Ïö∞ÏÑ†ÏàúÏúÑ 1)
‚úÖ OpenPose Î™®Îç∏ ÏßÄÏõê (Ìè¥Î∞± ÏòµÏÖò)
‚úÖ YOLOv8-Pose Î™®Îç∏ ÏßÄÏõê (Ïã§ÏãúÍ∞Ñ)
‚úÖ HRNet Î™®Îç∏ ÏßÄÏõê (Í≥†Ï†ïÎ∞Ä)
‚úÖ 17Í∞ú COCO keypoints Í∞êÏßÄ
‚úÖ confidence score Í≥ÑÏÇ∞
‚úÖ Mock Î™®Îç∏ ÏôÑÏ†Ñ Ï†úÍ±∞
‚úÖ Ïã§Ï†ú AI Ï∂îÎ°† Ïã§Ìñâ
‚úÖ Îã§Ï§ë Î™®Îç∏ Ìè¥Î∞± ÏãúÏä§ÌÖú

ÌååÏùº ÏúÑÏπò: backend/app/ai_pipeline/steps/step_02_pose_estimation.py
ÏûëÏÑ±Ïûê: MyCloset AI Team  
ÎÇ†Ïßú: 2025-08-01
Î≤ÑÏ†Ñ: v7.1 (Common Imports Integration)
"""

# üî• Í≥µÌÜµ imports ÏãúÏä§ÌÖú ÏÇ¨Ïö© (Ï§ëÎ≥µ Ï†úÍ±∞)
from app.ai_pipeline.utils.common_imports import (
    # ÌëúÏ§Ä ÎùºÏù¥Î∏åÎü¨Î¶¨
    os, sys, gc, time, asyncio, logging, threading, traceback,
    hashlib, json, base64, math, warnings, np,
    Path, Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING,
    dataclass, field, Enum, IntEnum, BytesIO, ThreadPoolExecutor,
    lru_cache, wraps, asynccontextmanager,
    
    # ÏóêÎü¨ Ï≤òÎ¶¨ ÏãúÏä§ÌÖú
    MyClosetAIException, ModelLoadingError, ImageProcessingError, DataValidationError, ConfigurationError,
    error_tracker, track_exception, get_error_summary, create_exception_response, convert_to_mycloset_exception,
    ErrorCodes, EXCEPTIONS_AVAILABLE,
    
    # Mock Data Diagnostic
    detect_mock_data, diagnose_step_data, MOCK_DIAGNOSTIC_AVAILABLE,
    
    # AI/ML ÎùºÏù¥Î∏åÎü¨Î¶¨
    torch, nn, F, transforms, TORCH_AVAILABLE, MPS_AVAILABLE,
    Image, cv2, scipy,
    PIL_AVAILABLE, CV2_AVAILABLE, SCIPY_AVAILABLE,
    
    # Ïú†Ìã∏Î¶¨Ìã∞ Ìï®Ïàò
    detect_m3_max, get_available_libraries, log_library_status,
    
    # ÏÉÅÏàò
    DEVICE_CPU, DEVICE_CUDA, DEVICE_MPS,
    DEFAULT_INPUT_SIZE, DEFAULT_CONFIDENCE_THRESHOLD, DEFAULT_QUALITY_THRESHOLD
)

# Í≤ΩÍ≥† Î¨¥Ïãú ÏÑ§Ï†ï
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=ImportWarning)

# TYPE_CHECKINGÏúºÎ°ú ÏàúÌôòÏ∞∏Ï°∞ Î∞©ÏßÄ
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from ..factories.step_factory import StepFactory
    from ..steps.base_step_mixin import BaseStepMixin

logger = logging.getLogger(__name__)

# M3 Max Í∞êÏßÄ (common_importsÏóêÏÑú Í∞ÄÏ†∏Ïò¥)
IS_M3_MAX = detect_m3_max()
MEMORY_GB = 16.0

# PyTorch ÏÑ§Ï†ï (common_importsÏóêÏÑú Í∞ÄÏ†∏Ïò¥)
TORCH_VERSION = torch.__version__ if TORCH_AVAILABLE else "N/A"

# ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
if TORCH_AVAILABLE and MPS_AVAILABLE:
    DEVICE = DEVICE_MPS
    torch.mps.set_per_process_memory_fraction(0.7)
elif TORCH_AVAILABLE and torch.cuda.is_available():
    DEVICE = DEVICE_CUDA
else:
    DEVICE = DEVICE_CPU

try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError as e:
    if EXCEPTIONS_AVAILABLE:
        error = ModelLoadingError(f"Pillow ÌïÑÏàò ÎùºÏù¥Î∏åÎü¨Î¶¨ Î°úÎî© Ïã§Ìå®: {e}", ErrorCodes.MODEL_LOADING_FAILED)
        track_exception(error, {'library': 'pillow'}, 2)
        raise error
    else:
        raise ImportError(f"‚ùå Pillow ÌïÑÏàò: {e}")

# ÏÑ†ÌÉùÏ†Å ÎùºÏù¥Î∏åÎü¨Î¶¨Îì§
try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

try:
    from transformers import AutoModel, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import safetensors.torch as st
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False

try:
    from scipy.optimize import curve_fit
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False


def _get_central_hub_container():
    """Central Hub DI Container ÏïàÏ†ÑÌïú ÎèôÏ†Å Ìï¥Í≤∞"""
    try:
        import importlib
        module = importlib.import_module('app.core.di_container')
        get_global_fn = getattr(module, 'get_global_container', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError:
        return None
    except Exception:
        return None

def _inject_dependencies_safe(step_instance):
    """Central Hub DI ContainerÎ•º ÌÜµÌïú ÏïàÏ†ÑÌïú ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ"""
    try:
        container = _get_central_hub_container()
        if container and hasattr(container, 'inject_to_step'):
            return container.inject_to_step(step_instance)
        return 0
    except Exception:
        return 0

def _get_service_from_central_hub(service_key: str):
    """Central HubÎ•º ÌÜµÌïú ÏïàÏ†ÑÌïú ÏÑúÎπÑÏä§ Ï°∞Ìöå"""
    try:
        container = _get_central_hub_container()
        if container:
            return container.get(service_key)
        return None
    except Exception:
        return None

logger = logging.getLogger(__name__)

# BaseStepMixin ÎèôÏ†Å import (ÏàúÌôòÏ∞∏Ï°∞ ÏôÑÏ†Ñ Î∞©ÏßÄ)
def get_base_step_mixin_class():
    """BaseStepMixin ÌÅ¥ÎûòÏä§Î•º ÎèôÏ†ÅÏúºÎ°ú Í∞ÄÏ†∏Ïò§Í∏∞"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError:
        logger.error("‚ùå BaseStepMixin ÎèôÏ†Å import Ïã§Ìå®")
        return None

BaseStepMixin = get_base_step_mixin_class()

# BaseStepMixin Ìè¥Î∞± ÌÅ¥ÎûòÏä§ (step_02_pose_estimation.pyÏö©)
if BaseStepMixin is None:
    import asyncio
    from typing import Dict, Any, Optional, List
    
    class BaseStepMixin:
        """PoseEstimationStepÏö© BaseStepMixin Ìè¥Î∞± ÌÅ¥ÎûòÏä§"""
        
        def __init__(self, **kwargs):
            # Í∏∞Î≥∏ ÏÜçÏÑ±Îì§
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'PoseEstimationStep')
            self.step_id = kwargs.get('step_id', 2)
            self.device = kwargs.get('device', 'cpu')
            
            # AI Î™®Îç∏ Í¥ÄÎ†® ÏÜçÏÑ±Îì§ (PoseEstimationStepÏù¥ ÌïÑÏöîÎ°ú ÌïòÎäî)
            self.ai_models = {}
            self.models_loading_status = {
                'mediapipe': False,
                'openpose': False,
                'yolov8': False,
                'hrnet': False,
                'total_loaded': 0,
                'loading_errors': []
            }
            self.model_interface = None
            self.loaded_models = {}
            
            # Pose Estimation ÌäπÌôî ÏÜçÏÑ±Îì§
            self.pose_models = {}
            self.pose_ready = False
            self.keypoints_cache = {}
            
            # ÏÉÅÌÉú Í¥ÄÎ†® ÏÜçÏÑ±Îì§
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # Central Hub DI Container Í¥ÄÎ†®
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            # ÏÑ±Îä• ÌÜµÍ≥Ñ
            self.performance_stats = {
                'total_processed': 0,
                'avg_processing_time': 0.0,
                'error_count': 0,
                'success_rate': 1.0
            }
            
            # Pose Estimation ÏÑ§Ï†ï
            self.confidence_threshold = 0.5
            self.use_subpixel = True
            
            # Î™®Îç∏ Ïö∞ÏÑ†ÏàúÏúÑ (MediaPipe Ïö∞ÏÑ†)
            self.model_priority = [
                'mediapipe',
                'yolov8_pose', 
                'openpose',
                'hrnet'
            ]
            
            self.logger.info(f"‚úÖ {self.step_name} BaseStepMixin Ìè¥Î∞± ÌÅ¥ÎûòÏä§ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        
        async def process(self, **kwargs) -> Dict[str, Any]:
            """Í∏∞Î≥∏ process Î©îÏÑúÎìú - _run_ai_inference Ìò∏Ï∂ú"""
            try:
                start_time = time.time()
                
                # _run_ai_inference Î©îÏÑúÎìúÍ∞Ä ÏûàÏúºÎ©¥ Ìò∏Ï∂ú
                if hasattr(self, '_run_ai_inference'):
                    result = await self._run_ai_inference(kwargs)
                    
                    # Ï≤òÎ¶¨ ÏãúÍ∞Ñ Ï∂îÍ∞Ä
                    if isinstance(result, dict):
                        result['processing_time'] = time.time() - start_time
                        result['step_name'] = self.step_name
                        result['step_id'] = self.step_id
                    
                    return result
                else:
                    # Í∏∞Î≥∏ ÏùëÎãµ
                    return {
                        'success': False,
                        'error': '_run_ai_inference Î©îÏÑúÎìúÍ∞Ä Íµ¨ÌòÑÎêòÏßÄ ÏïäÏùå',
                        'processing_time': time.time() - start_time,
                        'step_name': self.step_name,
                        'step_id': self.step_id
                    }
                    
            except Exception as e:
                self.logger.error(f"‚ùå {self.step_name} process Ïã§Ìå®: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                    'step_name': self.step_name,
                    'step_id': self.step_id
                }
        
        async def initialize(self) -> bool:
            """Ï¥àÍ∏∞Ìôî Î©îÏÑúÎìú"""
            try:
                if self.is_initialized:
                    return True
                
                self.logger.info(f"üîÑ {self.step_name} Ï¥àÍ∏∞Ìôî ÏãúÏûë...")
                
                # Ìè¨Ï¶à Î™®Îç∏Îì§ Î°úÎî© (Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî _load_pose_models_via_central_hub Ìò∏Ï∂ú)
                if hasattr(self, '_load_pose_models_via_central_hub'):
                    loaded_count = self._load_pose_models_via_central_hub()
                    if loaded_count == 0:
                        self.logger.error("‚ùå Ìè¨Ï¶à Î™®Îç∏ Î°úÎî© Ïã§Ìå® - Ï¥àÍ∏∞Ìôî Ïã§Ìå®")
                        return False
                
                self.is_initialized = True
                self.is_ready = True
                self.logger.info(f"‚úÖ {self.step_name} Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
                return True
            except Exception as e:
                self.logger.error(f"‚ùå {self.step_name} Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
                return False
        
        def cleanup(self):
            """Ï†ïÎ¶¨ Î©îÏÑúÎìú"""
            try:
                self.logger.info(f"üîÑ {self.step_name} Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ ÏãúÏûë...")
                
                # AI Î™®Îç∏Îì§ Ï†ïÎ¶¨
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'cleanup'):
                            model.cleanup()
                        del model
                    except Exception as e:
                        self.logger.debug(f"Î™®Îç∏ Ï†ïÎ¶¨ Ïã§Ìå® ({model_name}): {e}")
                
                # Ï∫êÏãú Ï†ïÎ¶¨
                self.ai_models.clear()
                if hasattr(self, 'pose_models'):
                    self.pose_models.clear()
                if hasattr(self, 'keypoints_cache'):
                    self.keypoints_cache.clear()
                
                # üî• 128GB M3 Max Í∞ïÏ†ú Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif hasattr(torch, 'mps') and torch.mps.is_available():
                        torch.mps.empty_cache()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
                
                # Í∞ïÏ†ú Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖò
                import gc
                for _ in range(3):
                    gc.collect()
                
                self.logger.info(f"‚úÖ {self.step_name} Ï†ïÎ¶¨ ÏôÑÎ£å")
            except Exception as e:
                self.logger.error(f"‚ùå {self.step_name} Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
        
        def get_status(self) -> Dict[str, Any]:
            """ÏÉÅÌÉú Ï°∞Ìöå"""
            return {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'device': self.device,
                'pose_ready': getattr(self, 'pose_ready', False),
                'models_loaded': len(getattr(self, 'loaded_models', {})),
                'model_priority': getattr(self, 'model_priority', []),
                'confidence_threshold': getattr(self, 'confidence_threshold', 0.5),
                'use_subpixel': getattr(self, 'use_subpixel', True),
                'fallback_mode': True
            }

        def _get_service_from_central_hub(self, service_key: str):
            """Central HubÏóêÏÑú ÏÑúÎπÑÏä§ Í∞ÄÏ†∏Ïò§Í∏∞"""
            try:
                if hasattr(self, 'di_container') and self.di_container:
                    return self.di_container.get_service(service_key)
                return None
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Central Hub ÏÑúÎπÑÏä§ Í∞ÄÏ†∏Ïò§Í∏∞ Ïã§Ìå®: {e}")
                return None

        def convert_api_input_to_step_input(self, api_input: Dict[str, Any]) -> Dict[str, Any]:
            """API ÏûÖÎ†•ÏùÑ Step ÏûÖÎ†•ÏúºÎ°ú Î≥ÄÌôò"""
            try:
                step_input = api_input.copy()
                
                # Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú (Îã§ÏñëÌïú ÌÇ§ Ïù¥Î¶Ñ ÏßÄÏõê)
                image = None
                for key in ['image', 'person_image', 'input_image', 'original_image']:
                    if key in step_input:
                        image = step_input[key]
                        break
                
                if image is None and 'session_id' in step_input:
                    # ÏÑ∏ÏÖòÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Î°úÎìú
                    try:
                        session_manager = self._get_service_from_central_hub('session_manager')
                        if session_manager:
                            person_image, clothing_image = None, None
                            
                            try:
                                # ÏÑ∏ÏÖò Îß§ÎãàÏ†ÄÍ∞Ä ÎèôÍ∏∞ Î©îÏÑúÎìúÎ•º Ï†úÍ≥µÌïòÎäîÏßÄ ÌôïÏù∏
                                if hasattr(session_manager, 'get_session_images_sync'):
                                    person_image, clothing_image = session_manager.get_session_images_sync(step_input['session_id'])
                                elif hasattr(session_manager, 'get_session_images'):
                                    # ÎπÑÎèôÍ∏∞ Î©îÏÑúÎìúÎ•º ÎèôÍ∏∞Ï†ÅÏúºÎ°ú Ìò∏Ï∂ú
                                    import asyncio
                                    import concurrent.futures
                                    
                                    def run_async_session_load():
                                        try:
                                            return asyncio.run(session_manager.get_session_images(step_input['session_id']))
                                        except Exception as async_error:
                                            self.logger.warning(f"‚ö†Ô∏è ÎπÑÎèôÍ∏∞ ÏÑ∏ÏÖò Î°úÎìú Ïã§Ìå®: {async_error}")
                                            return None, None
                                    
                                    try:
                                        with concurrent.futures.ThreadPoolExecutor() as executor:
                                            future = executor.submit(run_async_session_load)
                                            person_image, clothing_image = future.result(timeout=10)
                                    except Exception as executor_error:
                                        self.logger.warning(f"‚ö†Ô∏è ÏÑ∏ÏÖò Î°úÎìú ThreadPoolExecutor Ïã§Ìå®: {executor_error}")
                                        person_image, clothing_image = None, None
                                else:
                                    self.logger.warning("‚ö†Ô∏è ÏÑ∏ÏÖò Îß§ÎãàÏ†ÄÏóê Ï†ÅÏ†àÌïú Î©îÏÑúÎìúÍ∞Ä ÏóÜÏùå")
                            except Exception as e:
                                self.logger.warning(f"‚ö†Ô∏è ÏÑ∏ÏÖò Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïã§Ìå®: {e}")
                                person_image, clothing_image = None, None
                            
                            if person_image:
                                image = person_image
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è ÏÑ∏ÏÖòÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïã§Ìå®: {e}")
                
                # Î≥ÄÌôòÎêú ÏûÖÎ†• Íµ¨ÏÑ±
                converted_input = {
                    'image': image,
                    'person_image': image,
                    'session_id': step_input.get('session_id'),
                    'detection_confidence': step_input.get('detection_confidence', 0.5),
                    'clothing_type': step_input.get('clothing_type', 'shirt')
                }
                
                self.logger.info(f"‚úÖ API ÏûÖÎ†• Î≥ÄÌôò ÏôÑÎ£å: {len(converted_input)}Í∞ú ÌÇ§")
                return converted_input
                
            except Exception as e:
                self.logger.error(f"‚ùå API ÏûÖÎ†• Î≥ÄÌôò Ïã§Ìå®: {e}")
                return api_input
        
        def get_model_status(self) -> Dict[str, Any]:
            """Î™®Îç∏ ÏÉÅÌÉú Ï°∞Ìöå (PoseEstimationStep Ìò∏Ìôò)"""
            return {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'pose_ready': getattr(self, 'pose_ready', False),
                'models_loading_status': getattr(self, 'models_loading_status', {}),
                'loaded_models': list(getattr(self, 'ai_models', {}).keys()),
                'model_priority': getattr(self, 'model_priority', []),
                'confidence_threshold': getattr(self, 'confidence_threshold', 0.5),
                'use_subpixel': getattr(self, 'use_subpixel', True)
            }
        
        # BaseStepMixin Ìò∏Ìôò Î©îÏÑúÎìúÎì§
        def set_model_loader(self, model_loader):
            """ModelLoader ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ (BaseStepMixin Ìò∏Ìôò)"""
            try:
                self.model_loader = model_loader
                self.logger.info("‚úÖ ModelLoader ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ£å")
                
                # Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± ÏãúÎèÑ
                if hasattr(model_loader, 'create_step_interface'):
                    try:
                        self.model_interface = model_loader.create_step_interface(self.step_name)
                        self.logger.info("‚úÖ Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Î∞è Ï£ºÏûÖ ÏôÑÎ£å")
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®, ModelLoader ÏßÅÏ†ë ÏÇ¨Ïö©: {e}")
                        self.model_interface = model_loader
                else:
                    self.model_interface = model_loader
                    
            except Exception as e:
                self.logger.error(f"‚ùå ModelLoader ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ïã§Ìå®: {e}")
                self.model_loader = None
                self.model_interface = None
        
        def set_memory_manager(self, memory_manager):
            """MemoryManager ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ (BaseStepMixin Ìò∏Ìôò)"""
            try:
                self.memory_manager = memory_manager
                self.logger.info("‚úÖ MemoryManager ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ£å")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è MemoryManager ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ïã§Ìå®: {e}")
        
        def set_data_converter(self, data_converter):
            """DataConverter ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ (BaseStepMixin Ìò∏Ìôò)"""
            try:
                self.data_converter = data_converter
                self.logger.info("‚úÖ DataConverter ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ£å")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è DataConverter ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ïã§Ìå®: {e}")
        
        def set_di_container(self, di_container):
            """DI Container ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ"""
            try:
                self.di_container = di_container
                self.logger.info("‚úÖ DI Container ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ£å")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è DI Container ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ïã§Ìå®: {e}")

# ÌïÑÏàò ÎùºÏù¥Î∏åÎü¨Î¶¨ import
# ==============================================
# üî• 2. Ìè¨Ï¶à Ï∂îÏ†ï ÏÉÅÏàò Î∞è Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞
# ==============================================

class PoseModel(Enum):
    """Ìè¨Ï¶à Ï∂îÏ†ï Î™®Îç∏ ÌÉÄÏûÖ"""
    MEDIAPIPE = "mediapipe"
    OPENPOSE = "openpose"
    YOLOV8_POSE = "yolov8_pose"
    HRNET = "hrnet"
    DIFFUSION_POSE = "diffusion_pose"

class PoseQuality(Enum):
    """Ìè¨Ï¶à ÌíàÏßà Îì±Í∏â"""
    EXCELLENT = "excellent"     # 90-100Ï†ê
    GOOD = "good"              # 75-89Ï†ê  
    ACCEPTABLE = "acceptable"   # 60-74Ï†ê
    POOR = "poor"              # 40-59Ï†ê
    VERY_POOR = "very_poor"    # 0-39Ï†ê

# COCO 17 ÌÇ§Ìè¨Ïù∏Ìä∏ Ï†ïÏùò (MediaPipe, YOLOv8 ÌëúÏ§Ä)
COCO_17_KEYPOINTS = [
    "nose", "left_eye", "right_eye", "left_ear", "right_ear",
    "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
    "left_wrist", "right_wrist", "left_hip", "right_hip",
    "left_knee", "right_knee", "left_ankle", "right_ankle"
]

# OpenPose 18 ÌÇ§Ìè¨Ïù∏Ìä∏ Ï†ïÏùò 
OPENPOSE_18_KEYPOINTS = [
    "nose", "neck", "right_shoulder", "right_elbow", "right_wrist",
    "left_shoulder", "left_elbow", "left_wrist", "middle_hip", "right_hip", 
    "right_knee", "right_ankle", "left_hip", "left_knee", "left_ankle",
    "right_eye", "left_eye", "right_ear", "left_ear"
]

# ÌÇ§Ìè¨Ïù∏Ìä∏ Ïó∞Í≤∞ Íµ¨Ï°∞ (Ïä§ÏºàÎ†àÌÜ§)
SKELETON_CONNECTIONS = [
    (0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7), (1, 8),
    (8, 9), (9, 10), (10, 11), (8, 12), (12, 13), (13, 14), (0, 15),
    (15, 17), (0, 16), (16, 18)
]

# ÌÇ§Ìè¨Ïù∏Ìä∏ ÏÉâÏÉÅ Îß§Ìïë
KEYPOINT_COLORS = [
    (255, 0, 0), (255, 85, 0), (255, 170, 0), (255, 255, 0), (170, 255, 0),
    (85, 255, 0), (0, 255, 0), (0, 255, 85), (0, 255, 170), (0, 255, 255),
    (0, 170, 255), (0, 85, 255), (0, 0, 255), (85, 0, 255), (170, 0, 255),
    (255, 0, 255), (255, 0, 170), (255, 0, 85), (255, 0, 0)
]

@dataclass
class PoseResult:
    """Ìè¨Ï¶à Ï∂îÏ†ï Í≤∞Í≥º"""
    keypoints: List[List[float]] = field(default_factory=list)
    confidence_scores: List[float] = field(default_factory=list)
    joint_angles: Dict[str, float] = field(default_factory=dict)
    body_proportions: Dict[str, float] = field(default_factory=dict)
    pose_quality: PoseQuality = PoseQuality.POOR
    overall_confidence: float = 0.0
    processing_time: float = 0.0
    model_used: str = ""
    subpixel_accuracy: bool = False
    
    # Í≥†Í∏â Î∂ÑÏÑù Í≤∞Í≥º
    keypoints_with_uncertainty: List[Dict[str, Any]] = field(default_factory=list)
    advanced_body_metrics: Dict[str, Any] = field(default_factory=dict)
    skeleton_structure: Dict[str, Any] = field(default_factory=dict)
    ensemble_info: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# üî• 3. Ïã§Ï†ú AI Î™®Îç∏ ÌÅ¥ÎûòÏä§Îì§
# ==============================================

class MediaPoseModel:
    """MediaPipe Pose Î™®Îç∏ (Ïö∞ÏÑ†ÏàúÏúÑ 1)"""
    
    def __init__(self):
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.MediaPoseModel")
    
    def load_model(self) -> bool:
        """MediaPipe Î™®Îç∏ Î°úÎî©"""
        try:
            if not MEDIAPIPE_AVAILABLE:
                self.logger.error("‚ùå MediaPipe ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏóÜÏùå")
                return False
            
            self.model = mp.solutions.pose.Pose(
                static_image_mode=False,
                model_complexity=1,
                enable_segmentation=False,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )
            
            self.loaded = True
            self.logger.info("‚úÖ MediaPipe Pose Î™®Îç∏ Î°úÎî© ÏôÑÎ£å")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå MediaPipe Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            if EXCEPTIONS_AVAILABLE:
                error = ModelLoadingError(f"MediaPipe Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}", ErrorCodes.MODEL_LOADING_FAILED)
                track_exception(error, {'model_type': 'mediapipe', 'step': 'pose_estimation'}, 2)
            return False
    
    def detect_poses(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """MediaPipe Ìè¨Ï¶à Í≤ÄÏ∂ú"""
        if not self.loaded:
            if EXCEPTIONS_AVAILABLE:
                error = ModelLoadingError("MediaPipe Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏùå", ErrorCodes.MODEL_LOADING_FAILED)
                track_exception(error, {'model_type': 'mediapipe', 'operation': 'detect_poses'}, 2)
                raise error
            else:
                raise RuntimeError("MediaPipe Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏùå")
        
        start_time = time.time()
        
        try:
            # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
            if isinstance(image, Image.Image):
                image_np = np.array(image)
            elif isinstance(image, torch.Tensor):
                image_np = image.cpu().numpy()
                if image_np.ndim == 4:  # Î∞∞Ïπò Ï∞®Ïõê Ï†úÍ±∞
                    image_np = image_np[0]
                if image_np.shape[0] == 3:  # CHW -> HWC
                    image_np = np.transpose(image_np, (1, 2, 0))
                image_np = (image_np * 255).astype(np.uint8)
            else:
                image_np = image
            
            # RGB Î≥ÄÌôò
            if image_np.shape[-1] == 4:  # RGBA -> RGB
                image_np = image_np[:, :, :3]
            
            # MediaPipe Ï≤òÎ¶¨
            results = self.model.process(image_np)
            
            keypoints = []
            if results.pose_landmarks:
                for landmark in results.pose_landmarks.landmark:
                    # MediaPipeÎäî normalized coordinates (0-1)
                    x = landmark.x * image_np.shape[1]
                    y = landmark.y * image_np.shape[0]
                    confidence = landmark.visibility
                    keypoints.append([float(x), float(y), float(confidence)])
                
                # MediaPipe 33 ‚Üí COCO 17 Î≥ÄÌôò
                keypoints = self._convert_mediapipe_to_coco17(keypoints)
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "keypoints": keypoints,
                "num_persons": 1 if keypoints else 0,
                "processing_time": processing_time,
                "model_type": "mediapipe",
                "confidence": np.mean([kp[2] for kp in keypoints]) if keypoints else 0.0
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå MediaPipe Ï∂îÎ°† Ïã§Ìå®: {e}")
            if EXCEPTIONS_AVAILABLE:
                error = ImageProcessingError(f"MediaPipe Ï∂îÎ°† Ïã§Ìå®: {e}", ErrorCodes.MODEL_LOADING_FAILED)
                track_exception(error, {
                    'model_type': 'mediapipe', 
                    'operation': 'detect_poses',
                    'processing_time': time.time() - start_time
                }, 2)
            
            return {
                "success": False,
                "keypoints": [],
                "error": str(e),
                "processing_time": time.time() - start_time,
                "model_type": "mediapipe"
            }
    
    def _convert_mediapipe_to_coco17(self, mp_keypoints: List[List[float]]) -> List[List[float]]:
        """MediaPipe 33 ‚Üí COCO 17 Î≥ÄÌôò"""
        if len(mp_keypoints) < 33:
            return [[0.0, 0.0, 0.0] for _ in range(17)]
        
        # MediaPipe ‚Üí COCO 17 Îß§Ìïë
        mp_to_coco = {
            0: 0,   # nose
            2: 1,   # left_eye
            5: 2,   # right_eye
            7: 3,   # left_ear
            8: 4,   # right_ear
            11: 5,  # left_shoulder
            12: 6,  # right_shoulder
            13: 7,  # left_elbow
            14: 8,  # right_elbow
            15: 9,  # left_wrist
            16: 10, # right_wrist
            23: 11, # left_hip
            24: 12, # right_hip
            25: 13, # left_knee
            26: 14, # right_knee
            27: 15, # left_ankle
            28: 16  # right_ankle
        }
        
        coco_keypoints = [[0.0, 0.0, 0.0] for _ in range(17)]
        
        for mp_idx, coco_idx in mp_to_coco.items():
            if mp_idx < len(mp_keypoints):
                coco_keypoints[coco_idx] = mp_keypoints[mp_idx]
        
        return coco_keypoints

class YOLOv8PoseModel:
    """YOLOv8 Pose Î™®Îç∏ (Ïã§ÏãúÍ∞Ñ)"""
    
    def __init__(self, model_path: Optional[Path] = None):
        self.model_path = model_path
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.YOLOv8PoseModel")
    
    def load_model(self) -> bool:
        """YOLOv8 Î™®Îç∏ Î°úÎî©"""
        try:
            if not ULTRALYTICS_AVAILABLE:
                self.logger.error("‚ùå ultralytics ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏóÜÏùå")
                return False
            
            if self.model_path and self.model_path.exists():
                self.model = YOLO(str(self.model_path))
                self.logger.debug(f"‚úÖ YOLOv8 Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©: {self.model_path}")
            else:
                # ÏÇ¨Ï†Ñ ÌõàÎ†®Îêú Î™®Îç∏ ÏÇ¨Ïö©
                self.model = YOLO('yolov8n-pose.pt')
                self.logger.info("‚úÖ YOLOv8 ÏÇ¨Ï†Ñ ÌõàÎ†® Î™®Îç∏ Î°úÎî©")
            
            self.loaded = True
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå YOLOv8 Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            if EXCEPTIONS_AVAILABLE:
                error = ModelLoadingError(f"YOLOv8 Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}", ErrorCodes.MODEL_LOADING_FAILED)
                track_exception(error, {'model_type': 'yolov8_pose', 'step': 'pose_estimation'}, 2)
            return False
    
    def detect_poses(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """YOLOv8 Ìè¨Ï¶à Í≤ÄÏ∂ú"""
        if not self.loaded:
            if EXCEPTIONS_AVAILABLE:
                error = ModelLoadingError("YOLOv8 Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏùå", ErrorCodes.MODEL_LOADING_FAILED)
                track_exception(error, {'model_type': 'yolov8_pose', 'operation': 'detect_poses'}, 2)
                raise error
            else:
                raise RuntimeError("YOLOv8 Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏùå")
        
        start_time = time.time()
        
        try:
            # Ïã§Ï†ú AI Ï∂îÎ°† Ïã§Ìñâ
            results = self.model(image, verbose=False)
            
            poses = []
            for result in results:
                if hasattr(result, 'keypoints') and result.keypoints is not None:
                    keypoints = result.keypoints.data  # [N, 17, 3] (x, y, confidence)
                    
                    for person_kpts in keypoints:
                        # COCO 17 ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò
                        pose_keypoints = person_kpts.cpu().numpy().tolist()
                        
                        pose_data = {
                            "keypoints": pose_keypoints,
                            "bbox": result.boxes.xyxy.cpu().numpy()[0] if result.boxes else None,
                            "confidence": float(result.boxes.conf.mean()) if result.boxes else 0.0
                        }
                        poses.append(pose_data)
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "poses": poses,
                "keypoints": poses[0]["keypoints"] if poses else [],
                "num_persons": len(poses),
                "processing_time": processing_time,
                "model_type": "yolov8_pose",
                "confidence": poses[0]["confidence"] if poses else 0.0
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå YOLOv8 AI Ï∂îÎ°† Ïã§Ìå®: {e}")
            if EXCEPTIONS_AVAILABLE:
                error = ImageProcessingError(f"YOLOv8 AI Ï∂îÎ°† Ïã§Ìå®: {e}", ErrorCodes.MODEL_LOADING_FAILED)
                track_exception(error, {
                    'model_type': 'yolov8_pose', 
                    'operation': 'detect_poses',
                    'processing_time': time.time() - start_time
                }, 2)
            
            return {
                "success": False,
                "keypoints": [],
                "error": str(e),
                "processing_time": time.time() - start_time,
                "model_type": "yolov8_pose"
            }

class OpenPoseModel:
    """OpenPose Î™®Îç∏ - ÏôÑÏ†ÑÌïú PAF + ÌûàÌä∏Îßµ Ïã†Í≤ΩÎßù Íµ¨Ï°∞"""
    
    def __init__(self, model_path: Optional[Path] = None):
        self.model_path = model_path
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.OpenPoseModel")
        self.device = DEVICE
    
    def load_model(self) -> bool:
        """üî• Ïã§Ï†ú OpenPose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© (ÎÖºÎ¨∏ Í∏∞Î∞ò)"""
        try:
            if self.model_path and self.model_path.exists():
                # üî• Ïã§Ï†ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©
                checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=True)
                
                # üî• Í≥†Í∏â OpenPose ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉùÏÑ±
                self.model = self._create_advanced_openpose_network()
                
                # üî• Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë (Ïã§Ï†ú OpenPose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Íµ¨Ï°∞ÏôÄ Îß§Ïπ≠)
                self._map_openpose_checkpoint(checkpoint)
                
                self.logger.info(f"‚úÖ Ïã§Ï†ú OpenPose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©: {self.model_path}")
            else:
                # üî• Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Í∞Ä ÏóÜÏúºÎ©¥ Í≥†Í∏â ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉùÏÑ±
                self.model = self._create_advanced_openpose_network()
                self.logger.info("‚úÖ Í≥†Í∏â OpenPose ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉùÏÑ± ÏôÑÎ£å")
            
            self.model.eval()
            self.model.to(self.device)
            self.loaded = True
            return True
                
        except Exception as e:
            self.logger.error(f"‚ùå OpenPose Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            if EXCEPTIONS_AVAILABLE:
                error = ModelLoadingError(f"OpenPose Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}", ErrorCodes.MODEL_LOADING_FAILED)
                track_exception(error, {'model_type': 'openpose', 'step': 'pose_estimation'}, 2)
            return False
    
    def _map_openpose_checkpoint(self, checkpoint):
        """üî• Ïã§Ï†ú OpenPose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë (ÎÖºÎ¨∏ Í∏∞Î∞ò)"""
        try:
            model_state_dict = self.model.state_dict()
            mapped_dict = {}
            
            # üî• Ïã§Ï†ú OpenPose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÇ§ Îß§Ìïë Í∑úÏπô
            key_mappings = {
                # VGG19 Î∞±Î≥∏ Îß§Ìïë
                'module.features.0.weight': 'backbone.conv1_1.weight',
                'module.features.0.bias': 'backbone.conv1_1.bias',
                'module.features.2.weight': 'backbone.conv1_2.weight',
                'module.features.2.bias': 'backbone.conv1_2.bias',
                
                'module.features.5.weight': 'backbone.conv2_1.weight',
                'module.features.5.bias': 'backbone.conv2_1.bias',
                'module.features.7.weight': 'backbone.conv2_2.weight',
                'module.features.7.bias': 'backbone.conv2_2.bias',
                
                'module.features.10.weight': 'backbone.conv3_1.weight',
                'module.features.10.bias': 'backbone.conv3_1.bias',
                'module.features.12.weight': 'backbone.conv3_2.weight',
                'module.features.12.bias': 'backbone.conv3_2.bias',
                'module.features.14.weight': 'backbone.conv3_3.weight',
                'module.features.14.bias': 'backbone.conv3_3.bias',
                'module.features.16.weight': 'backbone.conv3_4.weight',
                'module.features.16.bias': 'backbone.conv3_4.bias',
                
                'module.features.19.weight': 'backbone.conv4_1.weight',
                'module.features.19.bias': 'backbone.conv4_1.bias',
                'module.features.21.weight': 'backbone.conv4_2.weight',
                'module.features.21.bias': 'backbone.conv4_2.bias',
                'module.features.23.weight': 'backbone.conv4_3.weight',
                'module.features.23.bias': 'backbone.conv4_3.bias',
                'module.features.25.weight': 'backbone.conv4_4.weight',
                'module.features.25.bias': 'backbone.conv4_4.bias',
                
                'module.features.28.weight': 'backbone.conv5_1.weight',
                'module.features.28.bias': 'backbone.conv5_1.bias',
                'module.features.30.weight': 'backbone.conv5_2.weight',
                'module.features.30.bias': 'backbone.conv5_2.bias',
                'module.features.32.weight': 'backbone.conv5_3.weight',
                'module.features.32.bias': 'backbone.conv5_3.bias',
                'module.features.34.weight': 'backbone.conv5_4.weight',
                'module.features.34.bias': 'backbone.conv5_4.bias',
                
                # OpenPose ÌäπÌôî Î†àÏù¥Ïñ¥ Îß§Ìïë
                'module.conv4_3_CPM.weight': 'backbone.conv4_3_CPM.weight',
                'module.conv4_3_CPM.bias': 'backbone.conv4_3_CPM.bias',
                'module.conv4_4_CPM.weight': 'backbone.conv4_4_CPM.weight',
                'module.conv4_4_CPM.bias': 'backbone.conv4_4_CPM.bias',
                
                # PAF Ïä§ÌÖåÏù¥ÏßÄ Îß§Ìïë
                'module.stage1_paf.conv1.weight': 'stage1_paf.conv1.weight',
                'module.stage1_paf.conv1.bias': 'stage1_paf.conv1.bias',
                'module.stage1_paf.conv2.weight': 'stage1_paf.conv2.weight',
                'module.stage1_paf.conv2.bias': 'stage1_paf.conv2.bias',
                'module.stage1_paf.conv3.weight': 'stage1_paf.conv3.weight',
                'module.stage1_paf.conv3.bias': 'stage1_paf.conv3.bias',
                'module.stage1_paf.conv4.weight': 'stage1_paf.conv4.weight',
                'module.stage1_paf.conv4.bias': 'stage1_paf.conv4.bias',
                'module.stage1_paf.conv5.weight': 'stage1_paf.conv5.weight',
                'module.stage1_paf.conv5.bias': 'stage1_paf.conv5.bias',
                
                # Confidence Ïä§ÌÖåÏù¥ÏßÄ Îß§Ìïë
                'module.stage1_conf.conv1.weight': 'stage1_conf.conv1.weight',
                'module.stage1_conf.conv1.bias': 'stage1_conf.conv1.bias',
                'module.stage1_conf.conv2.weight': 'stage1_conf.conv2.weight',
                'module.stage1_conf.conv2.bias': 'stage1_conf.conv2.bias',
                'module.stage1_conf.conv3.weight': 'stage1_conf.conv3.weight',
                'module.stage1_conf.conv3.bias': 'stage1_conf.conv3.bias',
                'module.stage1_conf.conv4.weight': 'stage1_conf.conv4.weight',
                'module.stage1_conf.conv4.bias': 'stage1_conf.conv4.bias',
                'module.stage1_conf.conv5.weight': 'stage1_conf.conv5.weight',
                'module.stage1_conf.conv5.bias': 'stage1_conf.conv5.bias'
            }
            
            # üî• Ï†ïÌôïÌïú ÌÇ§ Îß§Ìïë Ïã§Ìñâ
            for checkpoint_key, value in checkpoint.items():
                # 1. ÏßÅÏ†ë Îß§Ìïë
                if checkpoint_key in key_mappings:
                    model_key = key_mappings[checkpoint_key]
                    if model_key in model_state_dict:
                        mapped_dict[model_key] = value
                        continue
                
                # 2. Ìå®ÌÑ¥ Í∏∞Î∞ò Îß§Ìïë
                mapped_key = self._advanced_pattern_mapping(checkpoint_key, model_state_dict)
                if mapped_key:
                    mapped_dict[mapped_key] = value
                
                # 3. ÏßÅÏ†ë Îß§Ìïë (ÌÇ§Í∞Ä ÎèôÏùºÌïú Í≤ΩÏö∞)
                if checkpoint_key in model_state_dict:
                    mapped_dict[checkpoint_key] = value
                
                # 4. module. Ï†ëÎëêÏÇ¨ Ï†úÍ±∞ ÌõÑ Îß§Ìïë
                clean_key = checkpoint_key.replace('module.', '')
                if clean_key in model_state_dict:
                    mapped_dict[clean_key] = value
            
            # üî• Îß§ÌïëÎêú Í∞ÄÏ§ëÏπò Î°úÎìú
            if mapped_dict:
                try:
                    self.model.load_state_dict(mapped_dict, strict=False)
                    self.logger.info(f"‚úÖ OpenPose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë ÏÑ±Í≥µ: {len(mapped_dict)}Í∞ú ÌÇ§")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è OpenPose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî© Ïã§Ìå®: {e} - ÎûúÎç§ Ï¥àÍ∏∞Ìôî ÏÇ¨Ïö©")
            else:
                # üî• Ìè¥Î∞±: ÏßÅÏ†ë Î°úÎî© ÏãúÎèÑ
                try:
                    self.model.load_state_dict(checkpoint, strict=False)
                    self.logger.info("‚úÖ OpenPose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏßÅÏ†ë Î°úÎî© ÏÑ±Í≥µ")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è OpenPose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏßÅÏ†ë Î°úÎî©ÎèÑ Ïã§Ìå®: {e} - ÎûúÎç§ Ï¥àÍ∏∞Ìôî ÏÇ¨Ïö©")
            
        except Exception as e:
            self.logger.error(f"‚ùå OpenPose Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë Ïã§Ìå®: {e}")
    
    def _advanced_pattern_mapping(self, checkpoint_key, model_state_dict):
        """üî• Í≥†Í∏â Ìå®ÌÑ¥ Í∏∞Î∞ò ÌÇ§ Îß§Ìïë (OpenPose ÌäπÌôî)"""
        try:
            # module. Ï†ëÎëêÏÇ¨ Ï†úÍ±∞
            clean_key = checkpoint_key.replace('module.', '')
            
            # VGG19 Î†àÏù¥Ïñ¥ Ìå®ÌÑ¥ Îß§Ìïë
            if 'features.' in clean_key:
                for model_key in model_state_dict.keys():
                    if 'backbone.' in model_key and clean_key.split('.')[-1] in model_key:
                        return model_key
            
            # PAF Ïä§ÌÖåÏù¥ÏßÄ Ìå®ÌÑ¥ Îß§Ìïë
            if 'stage' in clean_key and 'paf' in clean_key:
                for model_key in model_state_dict.keys():
                    if 'stage' in model_key and 'paf' in model_key and clean_key.split('.')[-1] in model_key:
                        return model_key
            
            # Confidence Ïä§ÌÖåÏù¥ÏßÄ Ìå®ÌÑ¥ Îß§Ìïë
            if 'stage' in clean_key and 'conf' in clean_key:
                for model_key in model_state_dict.keys():
                    if 'stage' in model_key and 'conf' in model_key and clean_key.split('.')[-1] in model_key:
                        return model_key
            
            return None
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è OpenPose Ìå®ÌÑ¥ Îß§Ìïë Ïã§Ìå®: {e}")
            return None
    
    def _create_advanced_openpose_network(self) -> nn.Module:
        """üî• Ïã§Ï†ú OpenPose ÎÖºÎ¨∏ Í∏∞Î∞ò Í≥†Í∏â Ïã†Í≤ΩÎßù Íµ¨Ï°∞"""
        
        class AdvancedVGG19Backbone(nn.Module):
            """üî• Ïã§Ï†ú OpenPose ÎÖºÎ¨∏Ïùò VGG19 Î∞±Î≥∏ (Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏôÄ Ï†ïÌôïÌûà Îß§Ïπ≠)"""
            def __init__(self):
                super().__init__()
                
                # üî• Ïã§Ï†ú OpenPose ÎÖºÎ¨∏Ïùò VGG19 Íµ¨Ï°∞
                # Block 1
                self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
                self.relu1_1 = nn.ReLU(inplace=True)
                self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
                self.relu1_2 = nn.ReLU(inplace=True)
                self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
                
                # Block 2
                self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
                self.relu2_1 = nn.ReLU(inplace=True)
                self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
                self.relu2_2 = nn.ReLU(inplace=True)
                self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
                
                # Block 3
                self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
                self.relu3_1 = nn.ReLU(inplace=True)
                self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
                self.relu3_2 = nn.ReLU(inplace=True)
                self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
                self.relu3_3 = nn.ReLU(inplace=True)
                self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
                self.relu3_4 = nn.ReLU(inplace=True)
                self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
                
                # Block 4
                self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
                self.relu4_1 = nn.ReLU(inplace=True)
                self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu4_2 = nn.ReLU(inplace=True)
                self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu4_3 = nn.ReLU(inplace=True)
                self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu4_4 = nn.ReLU(inplace=True)
                self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)
                
                # Block 5
                self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu5_1 = nn.ReLU(inplace=True)
                self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu5_2 = nn.ReLU(inplace=True)
                self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu5_3 = nn.ReLU(inplace=True)
                self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu5_4 = nn.ReLU(inplace=True)
                
                # üî• OpenPose ÌäπÌôî Î†àÏù¥Ïñ¥Îì§ (ÎÖºÎ¨∏Í≥º Ï†ïÌôïÌûà Îß§Ïπ≠)
                self.conv4_3_CPM = nn.Conv2d(512, 256, kernel_size=3, padding=1)
                self.relu4_3_CPM = nn.ReLU(inplace=True)
                self.conv4_4_CPM = nn.Conv2d(256, 128, kernel_size=3, padding=1)
                self.relu4_4_CPM = nn.ReLU(inplace=True)
                
                # üî• Í∞ÄÏ§ëÏπò Ï¥àÍ∏∞Ìôî
                self._init_weights()
            
            def _init_weights(self):
                """Ïã§Ï†ú OpenPose ÎÖºÎ¨∏Ïùò Í∞ÄÏ§ëÏπò Ï¥àÍ∏∞Ìôî"""
                for m in self.modules():
                    if isinstance(m, nn.Conv2d):
                        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        if m.bias is not None:
                            nn.init.constant_(m.bias, 0)
            
            def forward(self, x):
                # üî• Ïã§Ï†ú OpenPose ÎÖºÎ¨∏Ïùò forward pass
                # Block 1
                x = self.relu1_1(self.conv1_1(x))
                x = self.relu1_2(self.conv1_2(x))
                x = self.pool1(x)
                
                # Block 2
                x = self.relu2_1(self.conv2_1(x))
                x = self.relu2_2(self.conv2_2(x))
                x = self.pool2(x)
                
                # Block 3
                x = self.relu3_1(self.conv3_1(x))
                x = self.relu3_2(self.conv3_2(x))
                x = self.relu3_3(self.conv3_3(x))
                x = self.relu3_4(self.conv3_4(x))
                x = self.pool3(x)
                
                # Block 4
                x = self.relu4_1(self.conv4_1(x))
                x = self.relu4_2(self.conv4_2(x))
                x = self.relu4_3(self.conv4_3(x))
                x = self.relu4_4(self.conv4_4(x))
                x = self.pool4(x)
                
                # Block 5
                x = self.relu5_1(self.conv5_1(x))
                x = self.relu5_2(self.conv5_2(x))
                x = self.relu5_3(self.conv5_3(x))
                x = self.relu5_4(self.conv5_4(x))
                
                # OpenPose ÌäπÌôî Î†àÏù¥Ïñ¥
                x = self.relu4_3_CPM(self.conv4_3_CPM(x))
                x = self.relu4_4_CPM(self.conv4_4_CPM(x))
                
                return x
            
            def forward(self, x):
                x = self.relu1_1(self.conv1_1(x))
                x = self.relu1_2(self.conv1_2(x))
                x = self.pool1(x)
                
                x = self.relu2_1(self.conv2_1(x))
                x = self.relu2_2(self.conv2_2(x))
                x = self.pool2(x)
                
                x = self.relu3_1(self.conv3_1(x))
                x = self.relu3_2(self.conv3_2(x))
                x = self.relu3_3(self.conv3_3(x))
                x = self.relu3_4(self.conv3_4(x))
                x = self.pool3(x)
                
                x = self.relu4_1(self.conv4_1(x))
                x = self.relu4_2(self.conv4_2(x))
                
                x = self.relu4_3_CPM(self.conv4_3_CPM(x))
                x = self.relu4_4_CPM(self.conv4_4_CPM(x))
                
                return x
        
        class AdvancedPAFStage(nn.Module):
            """üî• Ïã§Ï†ú OpenPose ÎÖºÎ¨∏Ïùò PAF (Part Affinity Fields) Ïä§ÌÖåÏù¥ÏßÄ"""
            def __init__(self, input_channels=128, output_channels=38):  # 19 limbs * 2 = 38
                super().__init__()
                
                # üî• Ïã§Ï†ú OpenPose ÎÖºÎ¨∏Ïùò PAF Íµ¨Ï°∞
                self.conv1 = nn.Conv2d(input_channels, 128, kernel_size=3, padding=1)
                self.relu1 = nn.ReLU(inplace=True)
                self.conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
                self.relu2 = nn.ReLU(inplace=True)
                self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
                self.relu3 = nn.ReLU(inplace=True)
                self.conv4 = nn.Conv2d(128, 512, kernel_size=1)
                self.relu4 = nn.ReLU(inplace=True)
                self.conv5 = nn.Conv2d(512, output_channels, kernel_size=1)
                
                # üî• Í∞ÄÏ§ëÏπò Ï¥àÍ∏∞Ìôî
                self._init_weights()
            
            def _init_weights(self):
                """PAF Ïä§ÌÖåÏù¥ÏßÄ Í∞ÄÏ§ëÏπò Ï¥àÍ∏∞Ìôî"""
                for m in self.modules():
                    if isinstance(m, nn.Conv2d):
                        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        if m.bias is not None:
                            nn.init.constant_(m.bias, 0)
            
            def forward(self, x):
                # üî• Ïã§Ï†ú OpenPose ÎÖºÎ¨∏Ïùò PAF forward pass
                x = self.relu1(self.conv1(x))
                x = self.relu2(self.conv2(x))
                x = self.relu3(self.conv3(x))
                x = self.relu4(self.conv4(x))
                x = self.conv5(x)
                return x
        
        class AdvancedConfidenceStage(nn.Module):
            """üî• Ïã§Ï†ú OpenPose ÎÖºÎ¨∏Ïùò Confidence (ÌÇ§Ìè¨Ïù∏Ìä∏ ÌûàÌä∏Îßµ) Ïä§ÌÖåÏù¥ÏßÄ"""
            def __init__(self, input_channels=128, output_channels=19):  # 18 keypoints + 1 background
                super().__init__()
                
                # üî• Ïã§Ï†ú OpenPose ÎÖºÎ¨∏Ïùò Confidence Íµ¨Ï°∞
                self.conv1 = nn.Conv2d(input_channels, 128, kernel_size=3, padding=1)
                self.relu1 = nn.ReLU(inplace=True)
                self.conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
                self.relu2 = nn.ReLU(inplace=True)
                self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
                self.relu3 = nn.ReLU(inplace=True)
                self.conv4 = nn.Conv2d(128, 512, kernel_size=1)
                self.relu4 = nn.ReLU(inplace=True)
                self.conv5 = nn.Conv2d(512, output_channels, kernel_size=1)
                
                # üî• Í∞ÄÏ§ëÏπò Ï¥àÍ∏∞Ìôî
                self._init_weights()
            
            def _init_weights(self):
                """Confidence Ïä§ÌÖåÏù¥ÏßÄ Í∞ÄÏ§ëÏπò Ï¥àÍ∏∞Ìôî"""
                for m in self.modules():
                    if isinstance(m, nn.Conv2d):
                        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        if m.bias is not None:
                            nn.init.constant_(m.bias, 0)
            
            def forward(self, x):
                # üî• Ïã§Ï†ú OpenPose ÎÖºÎ¨∏Ïùò Confidence forward pass
                x = self.relu1(self.conv1(x))
                x = self.relu2(self.conv2(x))
                x = self.relu3(self.conv3(x))
                x = self.relu4(self.conv4(x))
                x = self.conv5(x)
                return x
        
        class AdvancedOpenPoseNetwork(nn.Module):
            """üî• Ïã§Ï†ú OpenPose ÎÖºÎ¨∏Ïùò ÏôÑÏ†ÑÌïú ÎÑ§Ìä∏ÏõåÌÅ¨ (Îã§Îã®Í≥Ñ refinement)"""
            def __init__(self):
                super().__init__()
                self.backbone = AdvancedVGG19Backbone()
                
                # üî• Stage 1 (Ï¥àÍ∏∞ ÏòàÏ∏°)
                self.stage1_paf = AdvancedPAFStage(128, 38)
                self.stage1_conf = AdvancedConfidenceStage(128, 19)
                
                # üî• Stage 2-6 (Î∞òÎ≥µÏ†Å refinement) - Ïã§Ï†ú ÎÖºÎ¨∏Í≥º Ï†ïÌôïÌûà Îß§Ïπ≠
                self.stages_paf = nn.ModuleList([
                    AdvancedPAFStage(128 + 38 + 19, 38) for _ in range(5)
                ])
                self.stages_conf = nn.ModuleList([
                    AdvancedConfidenceStage(128 + 38 + 19, 19) for _ in range(5)
                ])
                
                # üî• Í∞ÄÏ§ëÏπò Ï¥àÍ∏∞Ìôî
                self._init_weights()
            
            def _init_weights(self):
                """Ï†ÑÏ≤¥ ÎÑ§Ìä∏ÏõåÌÅ¨ Í∞ÄÏ§ëÏπò Ï¥àÍ∏∞Ìôî"""
                for m in self.modules():
                    if isinstance(m, nn.Conv2d):
                        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        if m.bias is not None:
                            nn.init.constant_(m.bias, 0)
            
            def forward(self, x):
                # üî• Ïã§Ï†ú OpenPose ÎÖºÎ¨∏Ïùò forward pass
                # Î∞±Î≥∏ ÌäπÏßï Ï∂îÏ∂ú
                features = self.backbone(x)
                
                # üî• Stage 1
                paf1 = self.stage1_paf(features)
                conf1 = self.stage1_conf(features)
                
                pafs = [paf1]
                confs = [conf1]
                
                # üî• Stage 2-6 (iterative refinement) - Ïã§Ï†ú ÎÖºÎ¨∏Í≥º Ï†ïÌôïÌûà Îß§Ïπ≠
                for stage_paf, stage_conf in zip(self.stages_paf, self.stages_conf):
                    # Ïù¥Ï†Ñ Í≤∞Í≥ºÏôÄ ÌäπÏßïÏùÑ Ïó∞Í≤∞
                    stage_input = torch.cat([features, pafs[-1], confs[-1]], dim=1)
                    
                    # PAFÏôÄ confidence map ÏòàÏ∏°
                    paf = stage_paf(stage_input)
                    conf = stage_conf(stage_input)
                    
                    pafs.append(paf)
                    confs.append(conf)
                
                return {
                    'pafs': pafs,
                    'confs': confs,
                    'final_paf': pafs[-1],
                    'final_conf': confs[-1],
                    'features': features
                }
        
        return AdvancedOpenPoseNetwork()
    
    def detect_poses(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """OpenPose ÏôÑÏ†Ñ Ï∂îÎ°† (PAF + ÌûàÌä∏Îßµ ‚Üí ÌÇ§Ìè¨Ïù∏Ìä∏ Ï°∞Ìï©)"""
        if not self.loaded:
            raise RuntimeError("OpenPose Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏùå")
        
        start_time = time.time()
        
        try:
            # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
            input_tensor = self._preprocess_image(image)
            
            # Ïã§Ï†ú OpenPose AI Ï∂îÎ°† Ïã§Ìñâ
            with torch.no_grad():
                if DEVICE == "cuda" and torch.cuda.is_available():
                    with autocast():
                        outputs = self.model(input_tensor)
                else:
                    outputs = self.model(input_tensor)
            
            # PAFÏôÄ ÌûàÌä∏ÎßµÏóêÏÑú ÌÇ§Ìè¨Ïù∏Ìä∏ Ï∂îÏ∂ú
            keypoints = self._extract_keypoints_from_paf_heatmaps(
                outputs['final_paf'], 
                outputs['final_conf'],
                input_tensor.shape
            )
            
            # OpenPose 18 ‚Üí COCO 17 Î≥ÄÌôò
            coco_keypoints = self._convert_openpose18_to_coco17(keypoints)
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "keypoints": coco_keypoints,
                "openpose_keypoints": keypoints,
                "processing_time": processing_time,
                "model_type": "openpose",
                "confidence": np.mean([kp[2] for kp in coco_keypoints]) if coco_keypoints else 0.0,
                "num_stages": len(outputs['pafs']),
                "paf_shape": outputs['final_paf'].shape,
                "heatmap_shape": outputs['final_conf'].shape
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå OpenPose AI Ï∂îÎ°† Ïã§Ìå®: {e}")
            return {
                "success": False,
                "keypoints": [],
                "error": str(e),
                "processing_time": time.time() - start_time,
                "model_type": "openpose"
            }
    
    def _preprocess_image(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> torch.Tensor:
        """Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ (OpenPose ÏûÖÎ†• ÌòïÏãù)"""
        if isinstance(image, Image.Image):
            image_np = np.array(image)
        elif isinstance(image, torch.Tensor):
            image_np = image.cpu().numpy()
            if image_np.ndim == 4:
                image_np = image_np[0]
            if image_np.shape[0] == 3:
                image_np = np.transpose(image_np, (1, 2, 0))
            image_np = (image_np * 255).astype(np.uint8)
        else:
            image_np = image
        
        # RGB Î≥ÄÌôò
        if image_np.shape[-1] == 4:
            image_np = image_np[:, :, :3]
        
        # ÌÅ¨Í∏∞ Ï°∞Ï†ï (368x368 ÌëúÏ§Ä)
        target_size = 368
        h, w = image_np.shape[:2]
        scale = target_size / max(h, w)
        new_h, new_w = int(h * scale), int(w * scale)
        
        import cv2
        if OPENCV_AVAILABLE:
            resized = cv2.resize(image_np, (new_w, new_h))
        else:
            # PIL ÏÇ¨Ïö©
            pil_img = Image.fromarray(image_np)
            resized = np.array(pil_img.resize((new_w, new_h)))
        
        # Ìå®Îî©
        padded = np.zeros((target_size, target_size, 3), dtype=np.uint8)
        padded[:new_h, :new_w] = resized
        
        # Ï†ïÍ∑úÌôî Î∞è ÌÖêÏÑú Î≥ÄÌôò
        tensor = torch.from_numpy(padded).float().permute(2, 0, 1).unsqueeze(0)
        tensor = (tensor / 255.0 - 0.5) / 0.5  # [-1, 1] Ï†ïÍ∑úÌôî
        
        return tensor.to(self.device)
    
    def _extract_keypoints_from_paf_heatmaps(self, 
                                           pafs: torch.Tensor, 
                                           heatmaps: torch.Tensor, 
                                           input_shape: tuple) -> List[List[float]]:
        """PAFÏôÄ ÌûàÌä∏ÎßµÏóêÏÑú ÌÇ§Ìè¨Ïù∏Ìä∏ Ï∂îÏ∂ú (Ïã§Ï†ú OpenPose ÏïåÍ≥†Î¶¨Ï¶ò)"""
        
        # Non-Maximum SuppressionÏúºÎ°ú ÌÇ§Ìè¨Ïù∏Ìä∏ ÌõÑÎ≥¥ Ï∞æÍ∏∞
        def find_peaks_advanced(heatmap, threshold=0.1):
            """üî• Í≥†Í∏â ÌîºÌÅ¨ Í≤ÄÏ∂ú ÏïåÍ≥†Î¶¨Ï¶ò (Ïã§Ï†ú OpenPose ÎÖºÎ¨∏ Í∏∞Î∞ò)"""
            # 1. Í∞ÄÏö∞ÏãúÏïà ÌïÑÌÑ∞ÎßÅÏúºÎ°ú ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞
            heatmap_smooth = F.avg_pool2d(heatmap.unsqueeze(0).unsqueeze(0), kernel_size=3, stride=1, padding=1).squeeze()
            
            # 2. Ï†ÅÏùëÌòï ÏûÑÍ≥ÑÍ∞í Í≥ÑÏÇ∞ (Otsu ÏïåÍ≥†Î¶¨Ï¶ò Í∏∞Î∞ò)
            heatmap_flat = heatmap_smooth.flatten()
            if torch.max(heatmap_flat) > 0:
                hist = torch.histc(heatmap_flat, bins=256, min=0, max=1)
                total_pixels = torch.sum(hist)
                if total_pixels > 0:
                    hist = hist / total_pixels
                    cumsum = torch.cumsum(hist, dim=0)
                    cumsum_sq = torch.cumsum(hist * torch.arange(256, device=hist.device), dim=0)
                    mean = cumsum_sq[-1]
                    between_class_variance = (mean * cumsum - cumsum_sq) ** 2 / (cumsum * (1 - cumsum) + 1e-8)
                    threshold_idx = torch.argmax(between_class_variance)
                    adaptive_threshold = threshold_idx.float() / 255.0
                else:
                    adaptive_threshold = threshold
            else:
                adaptive_threshold = threshold
            
            # 3. Í≥†Í∏â ÌîºÌÅ¨ Í≤ÄÏ∂ú
            peaks = []
            h, w = heatmap_smooth.shape
            
            # 4. Non-maximum suppression
            for i in range(1, h-1):
                for j in range(1, w-1):
                    if heatmap_smooth[i, j] > adaptive_threshold:
                        # 8-Ïù¥ÏõÉ Í≤ÄÏÇ¨ + Ï∂îÍ∞Ä Ï°∞Í±¥
                        is_peak = True
                        peak_value = heatmap_smooth[i, j]
                        
                        for di in [-1, 0, 1]:
                            for dj in [-1, 0, 1]:
                                if di == 0 and dj == 0:
                                    continue
                                neighbor_value = heatmap_smooth[i+di, j+dj]
                                if neighbor_value >= peak_value:
                                    is_peak = False
                                    break
                            if not is_peak:
                                break
                        
                        if is_peak:
                            # 5. ÏÑúÎ∏åÌîΩÏÖÄ Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞
                            subpixel_x, subpixel_y = calculate_subpixel_accuracy(heatmap_smooth, i, j)
                            confidence = peak_value.item()
                            peaks.append([subpixel_y, subpixel_x, confidence])
            
            return peaks
        
        def calculate_subpixel_accuracy(heatmap, i, j):
            """üî• ÏÑúÎ∏åÌîΩÏÖÄ Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞ (Ïã§Ï†ú OpenPose ÎÖºÎ¨∏ Í∏∞Î∞ò)"""
            # 3x3 ÏúàÎèÑÏö∞ÏóêÏÑú 2Ï∞® Ìï®Ïàò ÌîºÌåÖ
            window = heatmap[max(0, i-1):min(heatmap.shape[0], i+2), 
                           max(0, j-1):min(heatmap.shape[1], j+2)]
            
            if window.shape[0] < 3 or window.shape[1] < 3:
                return float(j), float(i)
            
            # Ï§ëÏã¨Ï†ê Í∏∞Ï§ÄÏúºÎ°ú Ïò§ÌîÑÏÖã Í≥ÑÏÇ∞
            center_value = window[1, 1]
            
            # x Î∞©Ìñ• 2Ï∞® Ìï®Ïàò ÌîºÌåÖ
            x_values = window[1, :]
            if len(x_values) == 3:
                # 2Ï∞® Ìï®Ïàò Í≥ÑÏàò Í≥ÑÏÇ∞
                a = (x_values[0] + x_values[2] - 2 * x_values[1]) / 2
                b = (x_values[2] - x_values[0]) / 2
                if abs(a) > 1e-6:
                    x_offset = -b / (2 * a)
                else:
                    x_offset = 0
            else:
                x_offset = 0
            
            # y Î∞©Ìñ• 2Ï∞® Ìï®Ïàò ÌîºÌåÖ
            y_values = window[:, 1]
            if len(y_values) == 3:
                a = (y_values[0] + y_values[2] - 2 * y_values[1]) / 2
                b = (y_values[2] - y_values[0]) / 2
                if abs(a) > 1e-6:
                    y_offset = -b / (2 * a)
                else:
                    y_offset = 0
            else:
                y_offset = 0
            
            return float(j) + x_offset, float(i) + y_offset
        
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        # Í∞Å ÌÇ§Ìè¨Ïù∏Ìä∏ ÌÉÄÏûÖÎ≥ÑÎ°ú ÌõÑÎ≥¥ Ï∞æÍ∏∞
        for joint_idx in range(18):  # OpenPose 18 joints
            if joint_idx < heatmaps.shape[1] - 1:  # Î∞∞Í≤Ω Ï†úÏô∏
                heatmap = heatmaps[0, joint_idx]
                peaks = find_peaks_advanced(heatmap)
                
                if isinstance(peaks, list) and peaks:
                    # Í∞ÄÏû• ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ ÏÑ†ÌÉù
                    best_peak = max(peaks, key=lambda x: x[2])
                    y, x, conf = best_peak
                    
                    # Ï¢åÌëú Ï†ïÍ∑úÌôî (ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Î°ú)
                    x_norm = (x / w) * input_shape[-1]
                    y_norm = (y / h) * input_shape[-2]
                    
                    keypoints.append([float(x_norm), float(y_norm), float(conf)])
                
                elif torch.is_tensor(peaks) and len(peaks) > 0:
                    # Í∞ÄÏû• ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ ÏÑ†ÌÉù
                    best_idx = torch.argmax(heatmap[peaks[:, 0], peaks[:, 1]])
                    y, x = peaks[best_idx]
                    conf = heatmap[y, x]
                    
                    # Ï¢åÌëú Ï†ïÍ∑úÌôî
                    x_norm = (float(x) / w) * input_shape[-1]
                    y_norm = (float(y) / h) * input_shape[-2]
                    
                    keypoints.append([x_norm, y_norm, float(conf)])
                else:
                    keypoints.append([0.0, 0.0, 0.0])
            else:
                keypoints.append([0.0, 0.0, 0.0])
        
        # 18Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏Î°ú ÎßûÏ∂îÍ∏∞
        while len(keypoints) < 18:
            keypoints.append([0.0, 0.0, 0.0])
        
        # üî• Í∞ÄÏÉÅÌîºÌåÖ ÌäπÌôî Ìè¨Ï¶à Î∂ÑÏÑù Ï†ÅÏö©
        enhanced_keypoints = self._apply_virtual_fitting_pose_analysis(keypoints, pafs, heatmaps)
        
        return enhanced_keypoints[:18]
    
    def _apply_virtual_fitting_pose_analysis(self, keypoints, pafs, heatmaps):
        """üî• Í∞ÄÏÉÅÌîºÌåÖ ÌäπÌôî Ìè¨Ï¶à Î∂ÑÏÑù (VITON-HD, OOTD ÎÖºÎ¨∏ Í∏∞Î∞ò)"""
        try:
            # üî• 1. ÏùòÎ•ò ÌîºÌåÖÏóê Ï§ëÏöîÌïú ÌÇ§Ìè¨Ïù∏Ìä∏ Í∞ïÌôî
            clothing_important_joints = [5, 6, 7, 8, 9, 10, 12, 13]  # Ïñ¥Íπ®, ÌåîÍøàÏπò, ÏÜêÎ™©, ÏóâÎç©Ïù¥, Î¨¥Î¶é
            
            # üî• 2. Ìè¨Ï¶à ÏïàÏ†ïÏÑ± Í≤ÄÏ¶ù
            pose_stability = self._calculate_pose_stability(keypoints)
            
            # üî• 3. ÏùòÎ•ò ÌîºÌåÖ ÏµúÏ†ÅÌôî
            optimized_keypoints = self._optimize_for_clothing_fitting(keypoints, pose_stability)
            
            # üî• 4. Í∞ÄÏÉÅÌîºÌåÖ ÌíàÏßà Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
            fitting_quality = self._calculate_virtual_fitting_quality(optimized_keypoints, pafs)
            
            # üî• 5. Í≤∞Í≥ºÏóê ÌíàÏßà Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            for i, kp in enumerate(optimized_keypoints):
                if i in clothing_important_joints:
                    # ÏùòÎ•ò ÌîºÌåÖÏóê Ï§ëÏöîÌïú Í¥ÄÏ†àÏùÄ Ïã†Î¢∞ÎèÑ Ìñ•ÏÉÅ
                    kp[2] = min(1.0, kp[2] * 1.2)
            
            return optimized_keypoints
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Í∞ÄÏÉÅÌîºÌåÖ Ìè¨Ï¶à Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return keypoints
    
    def _calculate_pose_stability(self, keypoints):
        """üî• Ìè¨Ï¶à ÏïàÏ†ïÏÑ± Í≥ÑÏÇ∞ (Í∞ÄÏÉÅÌîºÌåÖ ÌäπÌôî)"""
        try:
            # 1. Í¥ÄÏ†à Í∞Ñ Í±∞Î¶¨ ÏùºÍ¥ÄÏÑ±
            joint_distances = []
            important_pairs = [(5, 6), (7, 8), (9, 10), (12, 13)]  # Ï¢åÏö∞ ÎåÄÏπ≠ Í¥ÄÏ†àÎì§
            
            for left, right in important_pairs:
                if left < len(keypoints) and right < len(keypoints):
                    left_pos = keypoints[left][:2]
                    right_pos = keypoints[right][:2]
                    if left_pos[0] > 0 and right_pos[0] > 0:  # Ïú†Ìö®Ìïú Ï¢åÌëú
                        distance = math.sqrt((left_pos[0] - right_pos[0])**2 + (left_pos[1] - right_pos[1])**2)
                        joint_distances.append(distance)
            
            # 2. ÏïàÏ†ïÏÑ± Ï†êÏàò Í≥ÑÏÇ∞
            if joint_distances:
                stability_score = 1.0 - (torch.std(torch.tensor(joint_distances)) / torch.mean(torch.tensor(joint_distances)))
                return max(0.0, min(1.0, stability_score))
            else:
                return 0.5
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Ìè¨Ï¶à ÏïàÏ†ïÏÑ± Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 0.5
    
    def _optimize_for_clothing_fitting(self, keypoints, pose_stability):
        """üî• ÏùòÎ•ò ÌîºÌåÖ ÏµúÏ†ÅÌôî (Í∞ÄÏÉÅÌîºÌåÖ ÌäπÌôî)"""
        try:
            optimized_keypoints = keypoints.copy()
            
            # 1. Ïñ¥Íπ® ÎùºÏù∏ Ï†ïÎ†¨ (ÏùòÎ•ò ÌîºÌåÖÏóê Ï§ëÏöî)
            if len(optimized_keypoints) > 6:
                left_shoulder = optimized_keypoints[5]
                right_shoulder = optimized_keypoints[6]
                
                if left_shoulder[2] > 0.3 and right_shoulder[2] > 0.3:
                    # Ïñ¥Íπ® ÎÜíÏù¥ ÌèâÍ∑†Ìôî
                    avg_y = (left_shoulder[1] + right_shoulder[1]) / 2
                    optimized_keypoints[5][1] = avg_y
                    optimized_keypoints[6][1] = avg_y
            
            # 2. ÏóâÎç©Ïù¥ ÎùºÏù∏ Ï†ïÎ†¨
            if len(optimized_keypoints) > 13:
                left_hip = optimized_keypoints[12]
                right_hip = optimized_keypoints[13]
                
                if left_hip[2] > 0.3 and right_hip[2] > 0.3:
                    # ÏóâÎç©Ïù¥ ÎÜíÏù¥ ÌèâÍ∑†Ìôî
                    avg_y = (left_hip[1] + right_hip[1]) / 2
                    optimized_keypoints[12][1] = avg_y
                    optimized_keypoints[13][1] = avg_y
            
            # 3. Ìè¨Ï¶à ÏïàÏ†ïÏÑ± Í∏∞Î∞ò Ïã†Î¢∞ÎèÑ Ï°∞Ï†ï
            for kp in optimized_keypoints:
                kp[2] = kp[2] * (0.7 + 0.3 * pose_stability)
            
            return optimized_keypoints
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ÏùòÎ•ò ÌîºÌåÖ ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
            return keypoints
    
    def _calculate_virtual_fitting_quality(self, keypoints, pafs):
        """üî• Í∞ÄÏÉÅÌîºÌåÖ ÌíàÏßà Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞"""
        try:
            # 1. ÏùòÎ•ò ÌîºÌåÖÏóê Ï§ëÏöîÌïú Í¥ÄÏ†àÎì§Ïùò Ïã†Î¢∞ÎèÑ
            clothing_joints = [5, 6, 7, 8, 9, 10, 12, 13]
            clothing_confidences = [keypoints[i][2] for i in clothing_joints if i < len(keypoints)]
            
            if clothing_confidences:
                avg_confidence = sum(clothing_confidences) / len(clothing_confidences)
            else:
                avg_confidence = 0.5
            
            # 2. PAF ÌíàÏßà (ÏùòÎ•ò Í≤ΩÍ≥Ñ Í∞êÏßÄ)
            paf_quality = torch.mean(torch.abs(pafs)).item() if torch.is_tensor(pafs) else 0.5
            
            # 3. Ï¢ÖÌï© ÌíàÏßà Ï†êÏàò
            quality_score = 0.7 * avg_confidence + 0.3 * paf_quality
            
            return max(0.0, min(1.0, quality_score))
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Í∞ÄÏÉÅÌîºÌåÖ ÌíàÏßà Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 0.5
    
    def _convert_openpose18_to_coco17(self, openpose_keypoints: List[List[float]]) -> List[List[float]]:
        """OpenPose 18 ‚Üí COCO 17 Î≥ÄÌôò"""
        if len(openpose_keypoints) < 18:
            return [[0.0, 0.0, 0.0] for _ in range(17)]
        
        # OpenPose 18 ‚Üí COCO 17 Îß§Ìïë
        openpose_to_coco = {
            0: 0,   # nose
            15: 1,  # left_eye (OpenPose) ‚Üí left_eye (COCO)
            16: 2,  # right_eye
            17: 3,  # left_ear
            18: 4,  # right_ear (if exists)
            5: 5,   # left_shoulder
            2: 6,   # right_shoulder
            6: 7,   # left_elbow
            3: 8,   # right_elbow
            7: 9,   # left_wrist
            4: 10,  # right_wrist
            12: 11, # left_hip
            9: 12,  # right_hip
            13: 13, # left_knee
            10: 14, # right_knee
            14: 15, # left_ankle
            11: 16  # right_ankle
        }
        
        coco_keypoints = [[0.0, 0.0, 0.0] for _ in range(17)]
        
        for openpose_idx, coco_idx in openpose_to_coco.items():
            if openpose_idx < len(openpose_keypoints) and coco_idx < 17:
                coco_keypoints[coco_idx] = openpose_keypoints[openpose_idx]
        
        return coco_keypoints


class HRNetModel:
    """HRNet Í≥†Ï†ïÎ∞Ä Î™®Îç∏"""
    
    def __init__(self, model_path: Optional[Path] = None):
        self.model_path = model_path
        self.model = None
        self.loaded = False
        self.input_size = (256, 192)  # HRNet Í∏∞Î≥∏ ÏûÖÎ†• ÌÅ¨Í∏∞
        self.device = DEVICE  # ÎîîÎ∞îÏù¥Ïä§ ÏÜçÏÑ± Ï∂îÍ∞Ä
        self.logger = logging.getLogger(f"{__name__}.HRNetModel")
    
    def load_model(self) -> bool:
        """HRNet Î™®Îç∏ Î°úÎî©"""
        try:
            self.model = self._create_hrnet_model()
            
            if self.model_path and self.model_path.exists():
                # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎî©
                checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=True)
                
                # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÇ§ ÌôïÏù∏ Î∞è Îß§Ìïë
                if isinstance(checkpoint, dict):
                    if 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    else:
                        state_dict = checkpoint
                    
                    # Î™®Îç∏ state_dictÏôÄ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÇ§ Îß§Ìïë
                    model_state_dict = self.model.state_dict()
                    mapped_state_dict = {}
                    
                    for key, value in state_dict.items():
                        # ÌÇ§ Îß§Ìïë Î°úÏßÅ
                        if key in model_state_dict:
                            if model_state_dict[key].shape == value.shape:
                                mapped_state_dict[key] = value
                            else:
                                self.logger.warning(f"‚ö†Ô∏è HRNet ÌÇ§ {key} ÌòïÌÉú Î∂àÏùºÏπò: {value.shape} vs {model_state_dict[key].shape}")
                        else:
                            # ÌÇ§ Ïù¥Î¶Ñ Î≥ÄÌôò ÏãúÎèÑ
                            mapped_key = self._map_hrnet_checkpoint_key(key)
                            if mapped_key and mapped_key in model_state_dict:
                                if model_state_dict[mapped_key].shape == value.shape:
                                    mapped_state_dict[mapped_key] = value
                                else:
                                    self.logger.warning(f"‚ö†Ô∏è HRNet Îß§ÌïëÎêú ÌÇ§ {mapped_key} ÌòïÌÉú Î∂àÏùºÏπò")
                    
                    if mapped_state_dict:
                        self.model.load_state_dict(mapped_state_dict, strict=False)
                        self.logger.info(f"‚úÖ HRNet Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë ÏÑ±Í≥µ: {len(mapped_state_dict)}Í∞ú ÌÇ§")
                    else:
                        self.logger.warning("‚ö†Ô∏è HRNet Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Îß§Ìïë Ïã§Ìå® - ÎûúÎç§ Ï¥àÍ∏∞Ìôî ÏÇ¨Ïö©")
                else:
                    self.logger.warning("‚ö†Ô∏è HRNet Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌòïÏãù Ïò§Î•ò - ÎûúÎç§ Ï¥àÍ∏∞Ìôî ÏÇ¨Ïö©")
            else:
                self.logger.info("‚úÖ HRNet Î≤†Ïù¥Ïä§ Î™®Îç∏ ÏÉùÏÑ±")
            
            self.model.eval()
            self.model.to(DEVICE)
            self.loaded = True
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå HRNet Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
            return False
    
    def _map_hrnet_checkpoint_key(self, key: str) -> Optional[str]:
        """HRNet Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌÇ§Î•º Î™®Îç∏ Íµ¨Ï°∞Ïóê ÎßûÍ≤å Ï†ïÌôïÌûà Îß§Ìïë"""
        
        # üî• Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∂ÑÏÑù Í≤∞Í≥ºÏóê Îî∞Î•∏ Ï†ïÌôïÌïú Îß§Ìïë
        # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏: backbone.stage1.0.conv1.weight
        # Î™®Îç∏: layer1.0.conv1.weight
        
        # Stage 1 Îß§Ìïë (ResNet-like)
        if key.startswith('backbone.stage1.'):
            return key.replace('backbone.stage1.', 'stage1.')
        
        # Stage 2-4 Îß§Ìïë (HRNet branches)
        elif key.startswith('backbone.stage2.'):
            return key.replace('backbone.stage2.', 'stage2.')
        elif key.startswith('backbone.stage3.'):
            return key.replace('backbone.stage3.', 'stage3.')
        elif key.startswith('backbone.stage4.'):
            return key.replace('backbone.stage4.', 'stage4.')
        
        # Stem Îß§Ìïë (conv1, conv2, bn1, bn2)
        elif key.startswith('backbone.conv1.'):
            return key.replace('backbone.conv1.', 'conv1.')
        elif key.startswith('backbone.conv2.'):
            return key.replace('backbone.conv2.', 'conv2.')
        elif key.startswith('backbone.bn1.'):
            return key.replace('backbone.bn1.', 'bn1.')
        elif key.startswith('backbone.bn2.'):
            return key.replace('backbone.bn2.', 'bn2.')
        
        # Final layer Îß§Ìïë
        elif key.startswith('keypoint_head.final_layer.'):
            return key.replace('keypoint_head.final_layer.', 'final_layer.')
        
        # Í∏∞ÌÉÄ ÏùºÎ∞òÏ†ÅÏù∏ Îß§Ìïë
        key_mappings = {
            'module.': '',
            'model.': '',
            'net.': '',
            'hrnet.': '',
        }
        
        for old_prefix, new_prefix in key_mappings.items():
            if key.startswith(old_prefix):
                return key.replace(old_prefix, new_prefix)
        
        return key
    
    def _create_hrnet_model(self) -> nn.Module:
        """ÏôÑÏ†ÑÌïú HRNet Î™®Îç∏ ÏÉùÏÑ± (Multi-Resolution Parallel Networks)"""
        
        class BasicBlock(nn.Module):
            """HRNet Basic Block"""
            expansion = 1
            
            def __init__(self, inplanes, planes, stride=1, downsample=None):
                super().__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, 3, stride, 1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.relu = nn.ReLU(inplace=True)
                self.conv2 = nn.Conv2d(planes, planes, 3, 1, 1, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.downsample = downsample
                self.stride = stride
            
            def forward(self, x):
                residual = x
                
                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)
                
                out = self.conv2(x)
                out = self.bn2(out)
                
                if self.downsample is not None:
                    residual = self.downsample(x)
                
                out += residual
                out = self.relu(out)
                
                return out
        
        class Bottleneck(nn.Module):
            """HRNet Bottleneck Block"""
            expansion = 4
            
            def __init__(self, inplanes, planes, stride=1, downsample=None):
                super().__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.conv2 = nn.Conv2d(planes, planes, 3, stride, 1, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)
                self.bn3 = nn.BatchNorm2d(planes * self.expansion)
                self.relu = nn.ReLU(inplace=True)
                self.downsample = downsample
                self.stride = stride
            
            def forward(self, x):
                residual = x
                
                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)
                
                out = self.conv2(x)
                out = self.bn2(out)
                out = self.relu(out)
                
                out = self.conv3(x)
                out = self.bn3(out)
                
                if self.downsample is not None:
                    residual = self.downsample(x)
                
                out += residual
                out = self.relu(out)
                
                return out
        
        class HighResolutionModule(nn.Module):
            """HRNetÏùò ÌïµÏã¨ Multi-Resolution Module"""
            
            def __init__(self, num_branches, blocks, num_blocks, num_inchannels,
                         num_channels, fuse_method, multi_scale_output=True):
                super().__init__()
                self._check_branches(num_branches, blocks, num_blocks, num_inchannels, num_channels)
                
                self.num_inchannels = num_inchannels
                self.fuse_method = fuse_method
                self.num_branches = num_branches
                self.multi_scale_output = multi_scale_output
                
                self.branches = self._make_branches(
                    num_branches, blocks, num_blocks, num_channels)
                self.fuse_layers = self._make_fuse_layers()
                self.relu = nn.ReLU(inplace=True)
            
            def _check_branches(self, num_branches, blocks, num_blocks, 
                              num_inchannels, num_channels):
                if num_branches != len(num_blocks):
                    error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(
                        num_branches, len(num_blocks))
                    raise ValueError(error_msg)
                
                if num_branches != len(num_channels):
                    error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(
                        num_branches, len(num_channels))
                    raise ValueError(error_msg)
                
                if num_branches != len(num_inchannels):
                    error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(
                        num_branches, len(num_inchannels))
                    raise ValueError(error_msg)
            
            def _make_one_branch(self, branch_index, block, num_blocks, num_channels,
                               stride=1):
                downsample = None
                if stride != 1 or \
                   self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:
                    downsample = nn.Sequential(
                        nn.Conv2d(
                            self.num_inchannels[branch_index],
                            num_channels[branch_index] * block.expansion,
                            kernel_size=1, stride=stride, bias=False
                        ),
                        nn.BatchNorm2d(num_channels[branch_index] * block.expansion),
                    )
                
                layers = []
                layers.append(
                    block(
                        self.num_inchannels[branch_index],
                        num_channels[branch_index],
                        stride,
                        downsample
                    )
                )
                self.num_inchannels[branch_index] = \
                    num_channels[branch_index] * block.expansion
                for i in range(1, num_blocks[branch_index]):
                    layers.append(
                        block(
                            self.num_inchannels[branch_index],
                            num_channels[branch_index]
                        )
                    )
                
                return nn.Sequential(*layers)
            
            def _make_branches(self, num_branches, block, num_blocks, num_channels):
                branches = []
                
                for i in range(num_branches):
                    branches.append(
                        self._make_one_branch(i, block, num_blocks, num_channels)
                    )
                
                return nn.ModuleList(branches)
            
            def _make_fuse_layers(self):
                if self.num_branches == 1:
                    return None
                
                num_branches = self.num_branches
                num_inchannels = self.num_inchannels
                fuse_layers = []
                for i in range(num_branches if self.multi_scale_output else 1):
                    fuse_layer = []
                    for j in range(num_branches):
                        if j > i:
                            fuse_layer.append(
                                nn.Sequential(
                                    nn.Conv2d(
                                        num_inchannels[j],
                                        num_inchannels[i],
                                        1, 1, 0, bias=False
                                    ),
                                    nn.BatchNorm2d(num_inchannels[i]),
                                    nn.Upsample(scale_factor=2**(j-i), mode='nearest')
                                )
                            )
                        elif j == i:
                            fuse_layer.append(None)
                        else:
                            conv3x3s = []
                            for k in range(i-j):
                                if k == i - j - 1:
                                    num_outchannels_conv3x3 = num_inchannels[i]
                                    conv3x3s.append(
                                        nn.Sequential(
                                            nn.Conv2d(
                                                num_inchannels[j],
                                                num_outchannels_conv3x3,
                                                3, 2, 1, bias=False
                                            ),
                                            nn.BatchNorm2d(num_outchannels_conv3x3)
                                        )
                                    )
                                else:
                                    num_outchannels_conv3x3 = num_inchannels[j]
                                    conv3x3s.append(
                                        nn.Sequential(
                                            nn.Conv2d(
                                                num_inchannels[j],
                                                num_outchannels_conv3x3,
                                                3, 2, 1, bias=False
                                            ),
                                            nn.BatchNorm2d(num_outchannels_conv3x3),
                                            nn.ReLU(inplace=True)
                                        )
                                    )
                            fuse_layer.append(nn.Sequential(*conv3x3s))
                    fuse_layers.append(nn.ModuleList(fuse_layer))
                
                return nn.ModuleList(fuse_layers)
            
            def get_num_inchannels(self):
                return self.num_inchannels
            
            def forward(self, x):
                if self.num_branches == 1:
                    return [self.branches[0](x[0])]
                
                for i in range(self.num_branches):
                    x[i] = self.branches[i](x[i])
                
                x_fuse = []
                
                for i in range(len(self.fuse_layers)):
                    y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])
                    for j in range(1, self.num_branches):
                        if i == j:
                            y = y + x[j]
                        else:
                            y = y + self.fuse_layers[i][j](x[j])
                    x_fuse.append(self.relu(y))
                
                return x_fuse
        
        class PoseHighResolutionNet(nn.Module):
            """ÏôÑÏ†ÑÌïú HRNet Ìè¨Ï¶à Ï∂îÏ†ï ÎÑ§Ìä∏ÏõåÌÅ¨ (Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ìò∏Ìôò)"""
            
            def __init__(self, cfg=None, **kwargs):
                super().__init__()
                
                # HRNet-W48 ÏÑ§Ï†ï (Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏôÄ Ìò∏Ìôò)
                if cfg is None:
                    cfg = {
                        'STAGE2': {
                            'NUM_MODULES': 1,
                            'NUM_BRANCHES': 2,
                            'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [4, 4],
                            'NUM_CHANNELS': [48, 96],
                            'FUSE_METHOD': 'SUM'
                        },
                        'STAGE3': {
                            'NUM_MODULES': 4,
                            'NUM_BRANCHES': 3,
                            'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [4, 4, 4],
                            'NUM_CHANNELS': [48, 96, 192],
                            'FUSE_METHOD': 'SUM'
                        },
                        'STAGE4': {
                            'NUM_MODULES': 3,
                            'NUM_BRANCHES': 4,
                            'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [4, 4, 4, 4],
                            'NUM_CHANNELS': [48, 96, 192, 384],
                            'FUSE_METHOD': 'SUM'
                        }
                    }
                
                self.inplanes = 64
                
                # Stem ÎÑ§Ìä∏ÏõåÌÅ¨ (3Ï±ÑÎÑê ÏûÖÎ†• Î≥¥Ïû•)
                self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)
                self.bn1 = nn.BatchNorm2d(64)
                self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)
                self.bn2 = nn.BatchNorm2d(64)
                self.relu = nn.ReLU(inplace=True)
                
                # Stage 1 (ResNet-like) - BasicBlock ÏÇ¨Ïö©ÌïòÏó¨ 64Ï±ÑÎÑê Ï∂úÎ†•
                self.stage1 = self._make_layer(BasicBlock, 64, 4)
                
                # Stage 2
                stage2_cfg = cfg['STAGE2']
                num_channels = stage2_cfg['NUM_CHANNELS']
                block = BasicBlock
                num_channels = [
                    num_channels[i] * block.expansion for i in range(len(num_channels))
                ]
                # Stage 1Ïùò Ï∂úÎ†•ÏùÄ 64Ï±ÑÎÑê (BasicBlock expansion=1, 64*1=64)
                self.transition1 = self._make_transition_layer([64], num_channels)
                self.stage2, pre_stage_channels = self._make_stage(
                    stage2_cfg, num_channels)
                
                # Stage 3
                stage3_cfg = cfg['STAGE3']
                num_channels = stage3_cfg['NUM_CHANNELS']
                block = BasicBlock
                num_channels = [
                    num_channels[i] * block.expansion for i in range(len(num_channels))
                ]
                self.transition2 = self._make_transition_layer(
                    pre_stage_channels, num_channels)
                self.stage3, pre_stage_channels = self._make_stage(
                    stage3_cfg, num_channels)
                
                # Stage 4
                stage4_cfg = cfg['STAGE4']
                num_channels = stage4_cfg['NUM_CHANNELS']
                block = BasicBlock
                num_channels = [
                    num_channels[i] * block.expansion for i in range(len(num_channels))
                ]
                self.transition3 = self._make_transition_layer(
                    pre_stage_channels, num_channels)
                self.stage4, pre_stage_channels = self._make_stage(
                    stage4_cfg, num_channels, multi_scale_output=True)
                
                # Final layer (ÌÇ§Ìè¨Ïù∏Ìä∏ ÏòàÏ∏°)
                self.final_layer = nn.Conv2d(
                    in_channels=pre_stage_channels[0],
                    out_channels=17,  # COCO 17 keypoints
                    kernel_size=1,
                    stride=1,
                    padding=0
                )
                
                self.pretrained_layers = ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']
            
            def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer):
                num_branches_cur = len(num_channels_cur_layer)
                num_branches_pre = len(num_channels_pre_layer)
                
                transition_layers = []
                for i in range(num_branches_cur):
                    if i < num_branches_pre:
                        if num_channels_cur_layer[i] != num_channels_pre_layer[i]:
                            transition_layers.append(
                                nn.Sequential(
                                    nn.Conv2d(
                                        num_channels_pre_layer[i],
                                        num_channels_cur_layer[i],
                                        3, 1, 1, bias=False
                                    ),
                                    nn.BatchNorm2d(num_channels_cur_layer[i]),
                                    nn.ReLU(inplace=True)
                                )
                            )
                        else:
                            transition_layers.append(None)
                    else:
                        conv3x3s = []
                        for j in range(i+1-num_branches_pre):
                            inchannels = num_channels_pre_layer[-1]
                            outchannels = num_channels_cur_layer[i] \
                                if j == i-num_branches_pre else inchannels
                            conv3x3s.append(
                                nn.Sequential(
                                    nn.Conv2d(inchannels, outchannels, 3, 2, 1, bias=False),
                                    nn.BatchNorm2d(outchannels),
                                    nn.ReLU(inplace=True)
                                )
                            )
                        transition_layers.append(nn.Sequential(*conv3x3s))
                
                return nn.ModuleList(transition_layers)
            
            def _make_layer(self, block, planes, blocks, stride=1):
                downsample = None
                if stride != 1 or self.inplanes != planes * block.expansion:
                    downsample = nn.Sequential(
                        nn.Conv2d(
                            self.inplanes, planes * block.expansion,
                            kernel_size=1, stride=stride, bias=False
                        ),
                        nn.BatchNorm2d(planes * block.expansion),
                    )
                
                layers = []
                layers.append(block(self.inplanes, planes, stride, downsample))
                self.inplanes = planes * block.expansion
                for i in range(1, blocks):
                    layers.append(block(self.inplanes, planes))
                
                return nn.Sequential(*layers)
            
            def _make_stage(self, layer_config, num_inchannels, multi_scale_output=True):
                num_modules = layer_config['NUM_MODULES']
                num_branches = layer_config['NUM_BRANCHES']
                num_blocks = layer_config['NUM_BLOCKS']
                num_channels = layer_config['NUM_CHANNELS']
                block = BasicBlock
                fuse_method = layer_config['FUSE_METHOD']
                
                modules = []
                for i in range(num_modules):
                    # multi_scale_outputÏùÄ ÎßàÏßÄÎßâ Î™®ÎìàÏóêÏÑúÎßå Í≥†Î†§
                    if not multi_scale_output and i == num_modules - 1:
                        reset_multi_scale_output = False
                    else:
                        reset_multi_scale_output = True
                    
                    modules.append(
                        HighResolutionModule(
                            num_branches,
                            block,
                            num_blocks,
                            num_inchannels,
                            num_channels,
                            fuse_method,
                            reset_multi_scale_output
                        )
                    )
                    num_inchannels = modules[-1].get_num_inchannels()
                
                return nn.Sequential(*modules), num_inchannels
            
            def forward(self, x):
                # Stem
                # ÎîîÎ≤ÑÍπÖ: ÏûÖÎ†• ÌÖêÏÑú ÌòïÌÉú ÌôïÏù∏
                if hasattr(self, 'logger'):
                    self.logger.info(f"üîç HRNet ÏûÖÎ†• ÌÖêÏÑú ÌòïÌÉú: {x.shape}")
                    self.logger.info(f"üîç HRNet ÏûÖÎ†• ÌÖêÏÑú Ï±ÑÎÑê: {x.shape[1]}")
                
                x = self.conv1(x)
                x = self.bn1(x)
                x = self.relu(x)
                x = self.conv2(x)
                x = self.bn2(x)
                x = self.relu(x)
                
                # Stage 1
                x = self.stage1(x)
                
                # ÎîîÎ≤ÑÍπÖ: Stage 1 ÌõÑ ÌÖêÏÑú ÌòïÌÉú ÌôïÏù∏
                if hasattr(self, 'logger'):
                    self.logger.info(f"üîç HRNet Stage 1 ÌõÑ ÌÖêÏÑú ÌòïÌÉú: {x.shape}")
                    self.logger.info(f"üîç HRNet Stage 1 ÌõÑ ÌÖêÏÑú Ï±ÑÎÑê: {x.shape[1]}")
                
                # Stage 2
                x_list = []
                for i in range(2):  # stage2 branches
                    if self.transition1[i] is not None:
                        x_list.append(self.transition1[i](x))
                    else:
                        x_list.append(x)
                y_list = self.stage2(x_list)
                
                # Stage 3
                x_list = []
                for i in range(3):  # stage3 branches
                    if self.transition2[i] is not None:
                        x_list.append(self.transition2[i](y_list[-1]))
                    else:
                        x_list.append(y_list[i])
                y_list = self.stage3(x_list)
                
                # Stage 4
                x_list = []
                for i in range(4):  # stage4 branches
                    if self.transition3[i] is not None:
                        x_list.append(self.transition3[i](y_list[-1]))
                    else:
                        x_list.append(y_list[i])
                y_list = self.stage4(x_list)
                
                # Final prediction
                x = self.final_layer(y_list[0])
                
                return x
        
        return PoseHighResolutionNet()
    
    def detect_poses(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """HRNet Í≥†Ï†ïÎ∞Ä Ìè¨Ï¶à Í≤ÄÏ∂ú (ÏÑúÎ∏åÌîΩÏÖÄ Ï†ïÌôïÎèÑ)"""
        if not self.loaded:
            raise RuntimeError("HRNet Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏùå")
        
        start_time = time.time()
        
        try:
            # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
            input_tensor, scale_factor = self._preprocess_image_with_scale(image)
            
            # Ïã§Ï†ú HRNet AI Ï∂îÎ°† Ïã§Ìñâ
            with torch.no_grad():
                if DEVICE == "cuda" and torch.cuda.is_available():
                    with autocast():
                        heatmaps = self.model(input_tensor)
                else:
                    heatmaps = self.model(input_tensor)
            
            # ÌûàÌä∏ÎßµÏóêÏÑú ÌÇ§Ìè¨Ïù∏Ìä∏ Ï∂îÏ∂ú (Í≥†Ï†ïÎ∞Ä ÏÑúÎ∏åÌîΩÏÖÄ)
            keypoints = self._extract_keypoints_with_subpixel_accuracy(
                heatmaps[0], scale_factor
            )
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "keypoints": keypoints,
                "processing_time": processing_time,
                "model_type": "hrnet",
                "confidence": np.mean([kp[2] for kp in keypoints]) if keypoints else 0.0,
                "subpixel_accuracy": True,
                "heatmap_shape": heatmaps.shape,
                "scale_factor": scale_factor
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå HRNet AI Ï∂îÎ°† Ïã§Ìå®: {e}")
            return {
                "success": False,
                "keypoints": [],
                "error": str(e),
                "processing_time": time.time() - start_time,
                "model_type": "hrnet"
            }
    
    def _preprocess_image_with_scale(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Tuple[torch.Tensor, float]:
        """Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Î∞è Ïä§ÏºÄÏùº Ìå©ÌÑ∞ Î∞òÌôò"""
        if isinstance(image, Image.Image):
            image_np = np.array(image)
            orig_h, orig_w = image_np.shape[:2]
        elif isinstance(image, torch.Tensor):
            image_np = image.cpu().numpy()
            if image_np.ndim == 4:
                image_np = image_np[0]
            if image_np.shape[0] == 3:
                image_np = np.transpose(image_np, (1, 2, 0))
            orig_h, orig_w = image_np.shape[:2]
            image_np = (image_np * 255).astype(np.uint8)
        else:
            image_np = image
            orig_h, orig_w = image_np.shape[:2]
        
        # HRNet ÌëúÏ§Ä ÏûÖÎ†• ÌÅ¨Í∏∞Î°ú Ï°∞Ï†ï
        target_h, target_w = self.input_size
        scale_factor = min(target_w / orig_w, target_h / orig_h)
        
        # ÎπÑÏú® Ïú†ÏßÄÌïòÎ©∞ Î¶¨ÏÇ¨Ïù¥Ï¶à
        new_w = int(orig_w * scale_factor)
        new_h = int(orig_h * scale_factor)
        
        if OPENCV_AVAILABLE:
            import cv2
            resized = cv2.resize(image_np, (new_w, new_h))
        else:
            pil_img = Image.fromarray(image_np)
            resized = np.array(pil_img.resize((new_w, new_h)))
        
        # Ï§ëÏïô Ìå®Îî©
        padded = np.zeros((target_h, target_w, 3), dtype=np.uint8)
        start_y = (target_h - new_h) // 2
        start_x = (target_w - new_w) // 2
        padded[start_y:start_y+new_h, start_x:start_x+new_w] = resized
        
        # Ï†ïÍ∑úÌôî Î∞è ÌÖêÏÑú Î≥ÄÌôò
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        tensor = transform(Image.fromarray(padded)).unsqueeze(0)
        
        # ÎîîÎ≤ÑÍπÖ: ÌÖêÏÑú ÌòïÌÉú ÌôïÏù∏
        self.logger.info(f"üîç HRNet Ï†ÑÏ≤òÎ¶¨ ÌõÑ ÌÖêÏÑú ÌòïÌÉú: {tensor.shape}")
        self.logger.info(f"üîç HRNet Ï†ÑÏ≤òÎ¶¨ ÌõÑ ÌÖêÏÑú Ï±ÑÎÑê: {tensor.shape[1]}")
        
        return tensor.to(self.device), scale_factor
    
    def _extract_keypoints_with_subpixel_accuracy(self, heatmaps: torch.Tensor, scale_factor: float) -> List[List[float]]:
        """ÌûàÌä∏ÎßµÏóêÏÑú ÌÇ§Ìè¨Ïù∏Ìä∏ Ï∂îÏ∂ú (Í≥†Ï†ïÎ∞Ä ÏÑúÎ∏åÌîΩÏÖÄ Ï†ïÌôïÎèÑ)"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(17):  # COCO 17 keypoints
            if i < heatmaps.shape[0]:
                heatmap = heatmaps[i].cpu().numpy()
                
                # Gaussian Î∏îÎü¨ Ï†ÅÏö© (ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞)
                if OPENCV_AVAILABLE:
                    import cv2
                    heatmap_blurred = cv2.GaussianBlur(heatmap, (3, 3), 0)
                else:
                    heatmap_blurred = heatmap
                
                # ÏµúÎåÄÍ∞í ÏúÑÏπò Ï∞æÍ∏∞
                y_idx, x_idx = np.unravel_index(np.argmax(heatmap_blurred), heatmap_blurred.shape)
                max_val = heatmap_blurred[y_idx, x_idx]
                
                # ÏÑúÎ∏åÌîΩÏÖÄ Ï†ïÌôïÎèÑÎ•º ÏúÑÌïú Í≥†Í∏â Í∞ÄÏö∞ÏãúÏïà ÌîºÌåÖ
                if (2 <= x_idx < w-2) and (2 <= y_idx < h-2):
                    # 5x5 ÏúàÎèÑÏö∞ÏóêÏÑú Í∞ÄÏö∞ÏãúÏïà ÌîºÌåÖ
                    window = heatmap_blurred[y_idx-2:y_idx+3, x_idx-2:x_idx+3]
                    
                    # 2Ï∞®Ïõê Í∞ÄÏö∞ÏãúÏïà ÌîºÌåÖÏúºÎ°ú ÏÑúÎ∏åÌîΩÏÖÄ ÏúÑÏπò Í≥ÑÏÇ∞
                    try:
                        if SCIPY_AVAILABLE:
                            from scipy.optimize import curve_fit
                            
                            def gaussian_2d(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):
                                x, y = xy
                                xo, yo = float(xo), float(yo)
                                a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)
                                b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)
                                c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)
                                g = offset + amplitude*np.exp(- (a*((x-xo)**2) + 2*b*(x-xo)*(y-yo) + c*((y-yo)**2)))
                                return g.ravel()
                            
                            # ÌîºÌåÖÏùÑ ÏúÑÌïú Ï¢åÌëú Í∑∏Î¶¨Îìú
                            y_grid, x_grid = np.mgrid[0:5, 0:5]
                            
                            # Ï¥àÍ∏∞ Ï∂îÏ†ïÍ∞í
                            initial_guess = (max_val, 2, 2, 1, 1, 0, 0)
                            
                            try:
                                popt, _ = curve_fit(gaussian_2d, (x_grid, y_grid), window.ravel(), 
                                                  p0=initial_guess, maxfev=1000)
                                
                                # ÏÑúÎ∏åÌîΩÏÖÄ Ïò§ÌîÑÏÖã Í≥ÑÏÇ∞
                                subpixel_x = x_idx - 2 + popt[1]
                                subpixel_y = y_idx - 2 + popt[2]
                                confidence = popt[0]  # amplitude
                                
                            except:
                                # ÌîºÌåÖ Ïã§Ìå® Ïãú Í∞ÑÎã®Ìïú Ï§ëÏã¨Í∞í Í≥ÑÏÇ∞
                                subpixel_x = float(x_idx)
                                subpixel_y = float(y_idx)
                                confidence = float(max_val)
                        else:
                            # Scipy ÏóÜÏù¥ Í∞ÑÎã®Ìïú Ï§ëÏã¨Í∞í Í≥ÑÏÇ∞
                            # Ï£ºÎ≥Ä ÌîΩÏÖÄÎì§Ïùò Í∞ÄÏ§ëÌèâÍ∑†ÏúºÎ°ú ÏÑúÎ∏åÌîΩÏÖÄ ÏúÑÏπò Í≥ÑÏÇ∞
                            total_weight = 0
                            weighted_x = 0
                            weighted_y = 0
                            
                            for dy in range(-1, 2):
                                for dx in range(-1, 2):
                                    if 0 <= y_idx+dy < h and 0 <= x_idx+dx < w:
                                        weight = heatmap_blurred[y_idx+dy, x_idx+dx]
                                        weighted_x += (x_idx + dx) * weight
                                        weighted_y += (y_idx + dy) * weight
                                        total_weight += weight
                            
                            if total_weight > 0:
                                subpixel_x = weighted_x / total_weight
                                subpixel_y = weighted_y / total_weight
                            else:
                                subpixel_x = float(x_idx)
                                subpixel_y = float(y_idx)
                            
                            confidence = float(max_val)
                    
                    except Exception:
                        # Ìè¥Î∞±: Í∏∞Î≥∏ ÌîΩÏÖÄ ÏúÑÏπò
                        subpixel_x = float(x_idx)
                        subpixel_y = float(y_idx)
                        confidence = float(max_val)
                else:
                    # Í≤ΩÍ≥Ñ Í∑ºÏ≤ò: Í∏∞Î≥∏ ÌîΩÏÖÄ ÏúÑÏπò
                    subpixel_x = float(x_idx)
                    subpixel_y = float(y_idx)
                    confidence = float(max_val)
                
                # Ï¢åÌëúÎ•º ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Î°ú Î≥ÄÌôò
                x_coord = (subpixel_x / w) * self.input_size[1] / scale_factor
                y_coord = (subpixel_y / h) * self.input_size[0] / scale_factor
                
                # Ïã†Î¢∞ÎèÑ Ï†ïÍ∑úÌôî
                confidence = min(1.0, max(0.0, confidence))
                
                keypoints.append([float(x_coord), float(y_coord), float(confidence)])
            else:
                keypoints.append([0.0, 0.0, 0.0])
        
        return keypoints
    
    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """ÌûàÌä∏ÎßµÏóêÏÑú ÌÇ§Ìè¨Ïù∏Ìä∏ Ï∂îÏ∂ú (Í≥†Ï†ïÎ∞Ä ÏÑúÎ∏åÌîΩÏÖÄ Ï†ïÌôïÎèÑ)"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(17):  # COCO 17 keypoints
            if i < heatmaps.shape[0]:
                heatmap = heatmaps[i].cpu().numpy()
                
                # ÏµúÎåÄÍ∞í ÏúÑÏπò Ï∞æÍ∏∞
                y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
                max_val = heatmap[y_idx, x_idx]
                
                # ÏÑúÎ∏åÌîΩÏÖÄ Ï†ïÌôïÎèÑÎ•º ÏúÑÌïú Í∞ÄÏö∞ÏãúÏïà ÌîºÌåÖ
                if (1 <= x_idx < w-1) and (1 <= y_idx < h-1):
                    # x Î∞©Ìñ• ÏÑúÎ∏åÌîΩÏÖÄ Î≥¥Ï†ï
                    dx = 0.5 * (heatmap[y_idx, x_idx+1] - heatmap[y_idx, x_idx-1]) / (
                        heatmap[y_idx, x_idx+1] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx, x_idx-1] + 1e-8)
                    
                    # y Î∞©Ìñ• ÏÑúÎ∏åÌîΩÏÖÄ Î≥¥Ï†ï
                    dy = 0.5 * (heatmap[y_idx+1, x_idx] - heatmap[y_idx-1, x_idx]) / (
                        heatmap[y_idx+1, x_idx] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx-1, x_idx] + 1e-8)
                    
                    # ÏÑúÎ∏åÌîΩÏÖÄ Ï¢åÌëú
                    x_subpixel = x_idx + dx
                    y_subpixel = y_idx + dy
                else:
                    x_subpixel = x_idx
                    y_subpixel = y_idx
                
                # Ï¢åÌëú Ï†ïÍ∑úÌôî
                x_normalized = x_subpixel / w
                y_normalized = y_subpixel / h
                
                # Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ Ï¢åÌëúÎ°ú Î≥ÄÌôò
                x_coord = x_normalized * 192
                y_coord = y_normalized * 256
                confidence = float(max_val)
                
                keypoints.append([x_coord, y_coord, confidence])
            else:
                keypoints.append([0.0, 0.0, 0.0])
        
        return keypoints

# ==============================================
# üî• 4. Ìè¨Ï¶à Î∂ÑÏÑù ÏïåÍ≥†Î¶¨Ï¶ò
# ==============================================

class PoseAnalyzer:
    """Í≥†Í∏â Ìè¨Ï¶à Î∂ÑÏÑù ÏïåÍ≥†Î¶¨Ï¶ò - ÏÉùÏ≤¥Ïó≠ÌïôÏ†Å Î∂ÑÏÑù Ìè¨Ìï®"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.PoseAnalyzer")
        
        # ÏÉùÏ≤¥Ïó≠ÌïôÏ†Å ÏÉÅÏàòÎì§
        self.joint_angle_ranges = {
            'left_elbow': (0, 180),
            'right_elbow': (0, 180),
            'left_knee': (0, 180),
            'right_knee': (0, 180),
            'left_shoulder': (-45, 180),
            'right_shoulder': (-45, 180),
            'left_hip': (-45, 135),
            'right_hip': (-45, 135)
        }
        
        # Ïã†Ï≤¥ ÎπÑÏú® ÌëúÏ§ÄÍ∞í (ÏÑ±Ïù∏ Í∏∞Ï§Ä)
        self.standard_proportions = {
            'head_to_total': 0.125,      # Î®∏Î¶¨:Ï†ÑÏ≤¥ = 1:8
            'torso_to_total': 0.375,     # ÏÉÅÏ≤¥:Ï†ÑÏ≤¥ = 3:8
            'arm_to_total': 0.375,       # Ìåî:Ï†ÑÏ≤¥ = 3:8
            'leg_to_total': 0.5,         # Îã§Î¶¨:Ï†ÑÏ≤¥ = 4:8
            'shoulder_to_hip': 1.1       # Ïñ¥Íπ®ÎÑàÎπÑ:ÏóâÎç©Ïù¥ÎÑàÎπÑ = 1.1:1
        }
    
    @staticmethod
    def calculate_joint_angles(keypoints: List[List[float]]) -> Dict[str, float]:
        """Í¥ÄÏ†à Í∞ÅÎèÑ Í≥ÑÏÇ∞ (ÏÉùÏ≤¥Ïó≠ÌïôÏ†Å Ï†ïÌôïÎèÑ)"""
        angles = {}
        
        def calculate_angle_3points(p1, p2, p3):
            """ÏÑ∏ Ï†êÏúºÎ°ú Ïù¥Î£®Ïñ¥ÏßÑ Í∞ÅÎèÑ Í≥ÑÏÇ∞ (Î≤°ÌÑ∞ ÎÇ¥Ï†Å ÏÇ¨Ïö©)"""
            try:
                # Î≤°ÌÑ∞ Í≥ÑÏÇ∞
                v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
                v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
                
                # Î≤°ÌÑ∞ ÌÅ¨Í∏∞ Í≥ÑÏÇ∞
                mag_v1 = np.linalg.norm(v1)
                mag_v2 = np.linalg.norm(v2)
                
                if mag_v1 == 0 or mag_v2 == 0:
                    return 0.0
                
                # ÎÇ¥Ï†ÅÏúºÎ°ú ÏΩîÏÇ¨Ïù∏ Í≥ÑÏÇ∞
                cos_angle = np.dot(v1, v2) / (mag_v1 * mag_v2)
                cos_angle = np.clip(cos_angle, -1.0, 1.0)
                
                # ÎùºÎîîÏïàÏùÑ ÎèÑÎ°ú Î≥ÄÌôò
                angle_rad = np.arccos(cos_angle)
                angle_deg = np.degrees(angle_rad)
                
                return float(angle_deg)
            except Exception:
                return 0.0
        
        def calculate_directional_angle(p1, p2, p3):
            """Î∞©Ìñ•ÏÑ±ÏùÑ Í≥†Î†§Ìïú Í∞ÅÎèÑ Í≥ÑÏÇ∞"""
            try:
                # Ïô∏Ï†ÅÏúºÎ°ú Î∞©Ìñ• Í≥ÑÏÇ∞
                v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
                v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
                
                cross_product = np.cross(v1, v2)
                angle = calculate_angle_3points(p1, p2, p3)
                
                # Ïô∏Ï†ÅÏùò Î∂ÄÌò∏Î°ú Î∞©Ìñ• Í≤∞Ï†ï
                if cross_product < 0:
                    angle = 360 - angle
                
                return float(angle)
            except Exception:
                return 0.0
        
        if len(keypoints) >= 17:
            confidence_threshold = 0.3
            
            # ÏôºÏ™Ω ÌåîÍøàÏπò Í∞ÅÎèÑ (Ïñ¥Íπ®-ÌåîÍøàÏπò-ÏÜêÎ™©)
            if all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[7], keypoints[9]]):
                angles['left_elbow'] = calculate_angle_3points(
                    keypoints[5], keypoints[7], keypoints[9]
                )
            
            # Ïò§Î•∏Ï™Ω ÌåîÍøàÏπò Í∞ÅÎèÑ
            if all(kp[2] > confidence_threshold for kp in [keypoints[6], keypoints[8], keypoints[10]]):
                angles['right_elbow'] = calculate_angle_3points(
                    keypoints[6], keypoints[8], keypoints[10]
                )
            
            # ÏôºÏ™Ω Î¨¥Î¶é Í∞ÅÎèÑ (ÏóâÎç©Ïù¥-Î¨¥Î¶é-Î∞úÎ™©)
            if all(kp[2] > confidence_threshold for kp in [keypoints[11], keypoints[13], keypoints[15]]):
                angles['left_knee'] = calculate_angle_3points(
                    keypoints[11], keypoints[13], keypoints[15]
                )
            
            # Ïò§Î•∏Ï™Ω Î¨¥Î¶é Í∞ÅÎèÑ
            if all(kp[2] > confidence_threshold for kp in [keypoints[12], keypoints[14], keypoints[16]]):
                angles['right_knee'] = calculate_angle_3points(
                    keypoints[12], keypoints[14], keypoints[16]
                )
            
            # ÏôºÏ™Ω Ïñ¥Íπ® Í∞ÅÎèÑ (Î™©-Ïñ¥Íπ®-ÌåîÍøàÏπò)
            # Î™© ÏúÑÏπòÎ•º Ïñ¥Íπ® Ï§ëÏ†êÏúºÎ°ú Ï∂îÏ†ï
            if (all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[6], keypoints[7]]) and
                keypoints[5][2] > confidence_threshold and keypoints[6][2] > confidence_threshold):
                
                neck_x = (keypoints[5][0] + keypoints[6][0]) / 2
                neck_y = (keypoints[5][1] + keypoints[6][1]) / 2
                neck_point = [neck_x, neck_y, 1.0]
                
                angles['left_shoulder'] = calculate_directional_angle(
                    neck_point, keypoints[5], keypoints[7]
                )
            
            # Ïò§Î•∏Ï™Ω Ïñ¥Íπ® Í∞ÅÎèÑ
            if (all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[6], keypoints[8]]) and
                keypoints[5][2] > confidence_threshold and keypoints[6][2] > confidence_threshold):
                
                neck_x = (keypoints[5][0] + keypoints[6][0]) / 2
                neck_y = (keypoints[5][1] + keypoints[6][1]) / 2
                neck_point = [neck_x, neck_y, 1.0]
                
                angles['right_shoulder'] = calculate_directional_angle(
                    neck_point, keypoints[6], keypoints[8]
                )
            
            # ÏôºÏ™Ω Í≥†Í¥ÄÏ†à Í∞ÅÎèÑ (ÏÉÅÏ≤¥-Í≥†Í¥ÄÏ†à-Î¨¥Î¶é)
            if all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[11], keypoints[13]]):
                angles['left_hip'] = calculate_directional_angle(
                    keypoints[5], keypoints[11], keypoints[13]
                )
            
            # Ïò§Î•∏Ï™Ω Í≥†Í¥ÄÏ†à Í∞ÅÎèÑ
            if all(kp[2] > confidence_threshold for kp in [keypoints[6], keypoints[12], keypoints[14]]):
                angles['right_hip'] = calculate_directional_angle(
                    keypoints[6], keypoints[12], keypoints[14]
                )
            
            # Î™© Í∞ÅÎèÑ (Ï¢åÏö∞ Ïñ¥Íπ®-ÏΩî)
            if (keypoints[0][2] > confidence_threshold and 
                all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[6]])):
                
                # Ïñ¥Íπ® Ï§ëÏ†ê
                shoulder_center = [
                    (keypoints[5][0] + keypoints[6][0]) / 2,
                    (keypoints[5][1] + keypoints[6][1]) / 2
                ]
                
                # ÏàòÏßÅÏÑ†Í≥º Î™©Ïùò Í∞ÅÎèÑ
                neck_vector = [keypoints[0][0] - shoulder_center[0], 
                              keypoints[0][1] - shoulder_center[1]]
                vertical_vector = [0, -1]  # ÏúÑÏ™Ω Î∞©Ìñ•
                
                dot_product = np.dot(neck_vector, vertical_vector)
                neck_magnitude = np.linalg.norm(neck_vector)
                
                if neck_magnitude > 0:
                    cos_angle = dot_product / neck_magnitude
                    cos_angle = np.clip(cos_angle, -1.0, 1.0)
                    neck_angle = np.degrees(np.arccos(cos_angle))
                    angles['neck_tilt'] = float(neck_angle)
            
            # Ï≤ôÏ∂î Í≥°Î•† Í≥ÑÏÇ∞
            if (all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[6]]) and
                all(kp[2] > confidence_threshold for kp in [keypoints[11], keypoints[12]])):
                
                # Ïñ¥Íπ®ÏôÄ ÏóâÎç©Ïù¥ Ï§ëÏ†ê
                shoulder_center = [(keypoints[5][0] + keypoints[6][0]) / 2,
                                 (keypoints[5][1] + keypoints[6][1]) / 2]
                hip_center = [(keypoints[11][0] + keypoints[12][0]) / 2,
                             (keypoints[11][1] + keypoints[12][1]) / 2]
                
                # Ï≤ôÏ∂î Î≤°ÌÑ∞ÏôÄ ÏàòÏßÅÏÑ†Ïùò Í∞ÅÎèÑ
                spine_vector = [shoulder_center[0] - hip_center[0],
                               shoulder_center[1] - hip_center[1]]
                vertical_vector = [0, -1]
                
                spine_magnitude = np.linalg.norm(spine_vector)
                if spine_magnitude > 0:
                    dot_product = np.dot(spine_vector, vertical_vector)
                    cos_angle = dot_product / spine_magnitude
                    cos_angle = np.clip(cos_angle, -1.0, 1.0)
                    spine_angle = np.degrees(np.arccos(cos_angle))
                    angles['spine_curvature'] = float(spine_angle)
        
        return angles
    
    @staticmethod
    def calculate_body_proportions(keypoints: List[List[float]]) -> Dict[str, float]:
        """Ïã†Ï≤¥ ÎπÑÏú® Í≥ÑÏÇ∞ (Ï†ïÎ∞ÄÌïú Ìï¥Î∂ÄÌïôÏ†Å Ï∏°Ï†ï)"""
        proportions = {}
        
        def calculate_distance(p1, p2):
            """Îëê Ï†ê ÏÇ¨Ïù¥Ïùò Ïú†ÌÅ¥Î¶¨Îìú Í±∞Î¶¨"""
            if len(p1) >= 2 and len(p2) >= 2:
                return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)
            return 0.0
        
        def calculate_body_part_length(keypoint_indices):
            """Ïã†Ï≤¥ Î∂ÄÏúÑÏùò Í∏∏Ïù¥ Í≥ÑÏÇ∞"""
            total_length = 0.0
            for i in range(len(keypoint_indices) - 1):
                idx1, idx2 = keypoint_indices[i], keypoint_indices[i + 1]
                if (idx1 < len(keypoints) and idx2 < len(keypoints) and
                    keypoints[idx1][2] > 0.3 and keypoints[idx2][2] > 0.3):
                    total_length += calculate_distance(keypoints[idx1], keypoints[idx2])
            return total_length
        
        if len(keypoints) >= 17:
            confidence_threshold = 0.3
            
            # Í∏∞Î≥∏ Í±∞Î¶¨ Ï∏°Ï†ïÎì§
            measurements = {}
            
            # Ïñ¥Íπ® ÎÑàÎπÑ
            if all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[6]]):
                measurements['shoulder_width'] = calculate_distance(keypoints[5], keypoints[6])
                proportions['shoulder_width'] = measurements['shoulder_width']
            
            # ÏóâÎç©Ïù¥ ÎÑàÎπÑ
            if all(kp[2] > confidence_threshold for kp in [keypoints[11], keypoints[12]]):
                measurements['hip_width'] = calculate_distance(keypoints[11], keypoints[12])
                proportions['hip_width'] = measurements['hip_width']
            
            # Ï†ÑÏ≤¥ Ïã†Ïû• (Î®∏Î¶¨-Î∞úÎ™©)
            height_candidates = []
            if keypoints[0][2] > confidence_threshold:  # ÏΩî
                if keypoints[15][2] > confidence_threshold:  # ÏôºÎ∞úÎ™©
                    height_candidates.append(calculate_distance(keypoints[0], keypoints[15]))
                if keypoints[16][2] > confidence_threshold:  # Ïò§Î•∏Î∞úÎ™©
                    height_candidates.append(calculate_distance(keypoints[0], keypoints[16]))
            
            if height_candidates:
                measurements['total_height'] = max(height_candidates)
                proportions['total_height'] = measurements['total_height']
            
            # ÏÉÅÏ≤¥ Í∏∏Ïù¥ (Ïñ¥Íπ® Ï§ëÏ†ê - ÏóâÎç©Ïù¥ Ï§ëÏ†ê)
            if ('shoulder_width' in measurements and 'hip_width' in measurements and
                all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[6], keypoints[11], keypoints[12]])):
                
                shoulder_center = [(keypoints[5][0] + keypoints[6][0]) / 2,
                                 (keypoints[5][1] + keypoints[6][1]) / 2]
                hip_center = [(keypoints[11][0] + keypoints[12][0]) / 2,
                             (keypoints[11][1] + keypoints[12][1]) / 2]
                
                measurements['torso_length'] = calculate_distance(shoulder_center, hip_center)
                proportions['torso_length'] = measurements['torso_length']
            
            # Ìåî Í∏∏Ïù¥ (Ïñ¥Íπ®-ÌåîÍøàÏπò-ÏÜêÎ™©)
            left_arm_length = calculate_body_part_length([5, 7, 9])  # ÏôºÌåî
            right_arm_length = calculate_body_part_length([6, 8, 10])  # Ïò§Î•∏Ìåî
            
            if left_arm_length > 0:
                proportions['left_arm_length'] = left_arm_length
            if right_arm_length > 0:
                proportions['right_arm_length'] = right_arm_length
            if left_arm_length > 0 and right_arm_length > 0:
                proportions['avg_arm_length'] = (left_arm_length + right_arm_length) / 2
            
            # Îã§Î¶¨ Í∏∏Ïù¥ (ÏóâÎç©Ïù¥-Î¨¥Î¶é-Î∞úÎ™©)
            left_leg_length = calculate_body_part_length([11, 13, 15])  # ÏôºÎã§Î¶¨
            right_leg_length = calculate_body_part_length([12, 14, 16])  # Ïò§Î•∏Îã§Î¶¨
            
            if left_leg_length > 0:
                proportions['left_leg_length'] = left_leg_length
            if right_leg_length > 0:
                proportions['right_leg_length'] = right_leg_length
            if left_leg_length > 0 and right_leg_length > 0:
                proportions['avg_leg_length'] = (left_leg_length + right_leg_length) / 2
            
            # ÎπÑÏú® Í≥ÑÏÇ∞
            if 'total_height' in measurements and measurements['total_height'] > 0:
                height = measurements['total_height']
                
                # Î®∏Î¶¨ ÌÅ¨Í∏∞ (ÏΩî-Î™© Í±∞Î¶¨ Ï∂îÏ†ï)
                if keypoints[0][2] > confidence_threshold and 'torso_length' in measurements:
                    estimated_head_length = measurements['torso_length'] * 0.25  # Ï∂îÏ†ïÍ∞í
                    proportions['head_to_height_ratio'] = estimated_head_length / height
                
                # ÏÉÅÏ≤¥ ÎåÄ Ï†ÑÏ≤¥ ÎπÑÏú®
                if 'torso_length' in measurements:
                    proportions['torso_to_height_ratio'] = measurements['torso_length'] / height
                
                # Îã§Î¶¨ ÎåÄ Ï†ÑÏ≤¥ ÎπÑÏú®
                if 'avg_leg_length' in proportions:
                    proportions['leg_to_height_ratio'] = proportions['avg_leg_length'] / height
                
                # Ìåî ÎåÄ Ï†ÑÏ≤¥ ÎπÑÏú®
                if 'avg_arm_length' in proportions:
                    proportions['arm_to_height_ratio'] = proportions['avg_arm_length'] / height
            
            # Ï¢åÏö∞ ÎåÄÏπ≠ÏÑ± Í≤ÄÏÇ¨
            if 'left_arm_length' in proportions and 'right_arm_length' in proportions:
                arm_asymmetry = abs(proportions['left_arm_length'] - proportions['right_arm_length'])
                avg_arm = (proportions['left_arm_length'] + proportions['right_arm_length']) / 2
                if avg_arm > 0:
                    proportions['arm_asymmetry_ratio'] = arm_asymmetry / avg_arm
            
            if 'left_leg_length' in proportions and 'right_leg_length' in proportions:
                leg_asymmetry = abs(proportions['left_leg_length'] - proportions['right_leg_length'])
                avg_leg = (proportions['left_leg_length'] + proportions['right_leg_length']) / 2
                if avg_leg > 0:
                    proportions['leg_asymmetry_ratio'] = leg_asymmetry / avg_leg
            
            # Ïñ¥Íπ®-ÏóâÎç©Ïù¥ ÎπÑÏú®
            if 'shoulder_width' in measurements and 'hip_width' in measurements and measurements['hip_width'] > 0:
                proportions['shoulder_to_hip_ratio'] = measurements['shoulder_width'] / measurements['hip_width']
            
            # BMI Ï∂îÏ†ï (Îß§Ïö∞ ÎåÄÎûµÏ†Å)
            if 'total_height' in measurements and 'shoulder_width' in measurements:
                # Ïñ¥Íπ® ÎÑàÎπÑÎ•º Í∏∞Î∞òÏúºÎ°ú Ìïú Ï≤¥Í≤© Ï∂îÏ†ï (Îß§Ïö∞ ÎåÄÎûµÏ†Å)
                estimated_body_mass_index = (measurements['shoulder_width'] / measurements['total_height']) * 100
                proportions['estimated_bmi_indicator'] = estimated_body_mass_index
        
        return proportions
    
    def assess_pose_quality(self, 
                          keypoints: List[List[float]], 
                          joint_angles: Dict[str, float], 
                          body_proportions: Dict[str, float]) -> Dict[str, Any]:
        """Ìè¨Ï¶à ÌíàÏßà ÌèâÍ∞Ä (Îã§Ï∞®Ïõê Î∂ÑÏÑù)"""
        assessment = {
            'overall_score': 0.0,
            'quality_grade': PoseQuality.POOR,
            'detailed_scores': {},
            'issues': [],
            'recommendations': [],
            'confidence_analysis': {},
            'anatomical_plausibility': {},
            'symmetry_analysis': {}
        }
        
        try:
            # 1. ÌÇ§Ìè¨Ïù∏Ìä∏ Í∞ÄÏãúÏÑ± Î∂ÑÏÑù
            visible_keypoints = [kp for kp in keypoints if len(kp) >= 3 and kp[2] > 0.1]
            high_conf_keypoints = [kp for kp in keypoints if len(kp) >= 3 and kp[2] > 0.7]
            
            visibility_score = len(visible_keypoints) / len(keypoints)
            high_confidence_score = len(high_conf_keypoints) / len(keypoints)
            
            # 2. Ïã†Î¢∞ÎèÑ Î∂ÑÏÑù
            confidence_scores = [kp[2] for kp in keypoints if len(kp) >= 3 and kp[2] > 0.1]
            if confidence_scores:
                avg_confidence = np.mean(confidence_scores)
                confidence_std = np.std(confidence_scores)
                min_confidence = np.min(confidence_scores)
                max_confidence = np.max(confidence_scores)
            else:
                avg_confidence = confidence_std = min_confidence = max_confidence = 0.0
            
            assessment['confidence_analysis'] = {
                'average': avg_confidence,
                'std_deviation': confidence_std,
                'min_confidence': min_confidence,
                'max_confidence': max_confidence,
                'confidence_consistency': 1.0 - (confidence_std / (avg_confidence + 1e-8))
            }
            
            # 3. Ìï¥Î∂ÄÌïôÏ†Å ÌÉÄÎãπÏÑ± Í≤ÄÏÇ¨
            anatomical_score = self._assess_anatomical_plausibility(keypoints, joint_angles)
            
            # 4. ÎåÄÏπ≠ÏÑ± Î∂ÑÏÑù
            symmetry_score = self._assess_body_symmetry(keypoints, body_proportions)
            
            # 5. Ìè¨Ï¶à ÏôÑÏÑ±ÎèÑ
            critical_keypoints = [0, 5, 6, 11, 12]  # ÏΩî, Ïñ¥Íπ®Îì§, ÏóâÎç©Ïù¥Îì§
            critical_visible = sum(1 for i in critical_keypoints 
                                 if i < len(keypoints) and len(keypoints[i]) >= 3 and keypoints[i][2] > 0.5)
            completeness_score = critical_visible / len(critical_keypoints)
            
            # 6. Ï†ÑÏ≤¥ Ï†êÏàò Í≥ÑÏÇ∞ (Í∞ÄÏ§ëÌèâÍ∑†)
            weights = {
                'visibility': 0.25,
                'confidence': 0.25,
                'anatomical': 0.20,
                'symmetry': 0.15,
                'completeness': 0.15
            }
            
            overall_score = (
                visibility_score * weights['visibility'] +
                avg_confidence * weights['confidence'] +
                anatomical_score * weights['anatomical'] +
                symmetry_score * weights['symmetry'] +
                completeness_score * weights['completeness']
            )
            
            # 7. ÌíàÏßà Îì±Í∏â Í≤∞Ï†ï
            if overall_score >= 0.9:
                quality_grade = PoseQuality.EXCELLENT
            elif overall_score >= 0.75:
                quality_grade = PoseQuality.GOOD
            elif overall_score >= 0.6:
                quality_grade = PoseQuality.ACCEPTABLE
            elif overall_score >= 0.4:
                quality_grade = PoseQuality.POOR
            else:
                quality_grade = PoseQuality.VERY_POOR
            
            # 8. ÏÑ∏Î∂Ä Ï†êÏàò
            assessment['detailed_scores'] = {
                'visibility': visibility_score,
                'high_confidence_ratio': high_confidence_score,
                'average_confidence': avg_confidence,
                'anatomical_plausibility': anatomical_score,
                'symmetry': symmetry_score,
                'completeness': completeness_score
            }
            
            # 9. Ïù¥Ïäà Î∞è Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±
            assessment['issues'] = self._identify_pose_issues(
                keypoints, joint_angles, body_proportions, assessment['detailed_scores']
            )
            assessment['recommendations'] = self._generate_pose_recommendations(
                assessment['issues'], assessment['detailed_scores']
            )
            
            # 10. ÏµúÏ¢Ö Í≤∞Í≥º ÏóÖÎç∞Ïù¥Ìä∏
            assessment.update({
                'overall_score': overall_score,
                'quality_grade': quality_grade,
                'anatomical_plausibility': {
                    'score': anatomical_score,
                    'joint_angle_validity': self._validate_joint_angles(joint_angles),
                    'proportion_validity': self._validate_body_proportions(body_proportions)
                },
                'symmetry_analysis': {
                    'score': symmetry_score,
                    'left_right_balance': self._analyze_left_right_balance(keypoints),
                    'posture_alignment': self._analyze_posture_alignment(keypoints)
                }
            })
            
        except Exception as e:
            self.logger.error(f"‚ùå Ìè¨Ï¶à ÌíàÏßà ÌèâÍ∞Ä Ïã§Ìå®: {e}")
            assessment['error'] = str(e)
        
        return assessment
    
    def _assess_anatomical_plausibility(self, keypoints: List[List[float]], joint_angles: Dict[str, float]) -> float:
        """Ìï¥Î∂ÄÌïôÏ†Å ÌÉÄÎãπÏÑ± ÌèâÍ∞Ä"""
        plausibility_score = 1.0
        penalty = 0.0
        
        # Í¥ÄÏ†à Í∞ÅÎèÑ Î≤îÏúÑ Í≤ÄÏÇ¨
        for joint, angle in joint_angles.items():
            if joint in self.joint_angle_ranges:
                min_angle, max_angle = self.joint_angle_ranges[joint]
                if not (min_angle <= angle <= max_angle):
                    penalty += 0.1  # Î≤îÏúÑ Î≤óÏñ¥ÎÇ† ÎïåÎßàÎã§ 10% Í∞êÏ†ê
        
        # ÌÇ§Ìè¨Ïù∏Ìä∏ ÏúÑÏπò ÏÉÅÏãùÏÑ± Í≤ÄÏÇ¨
        if len(keypoints) >= 17:
            # Ïñ¥Íπ®Í∞Ä ÏóâÎç©Ïù¥Î≥¥Îã§ ÏúÑÏóê ÏûàÎäîÏßÄ
            if (keypoints[5][2] > 0.3 and keypoints[6][2] > 0.3 and
                keypoints[11][2] > 0.3 and keypoints[12][2] > 0.3):
                
                avg_shoulder_y = (keypoints[5][1] + keypoints[6][1]) / 2
                avg_hip_y = (keypoints[11][1] + keypoints[12][1]) / 2
                
                if avg_shoulder_y >= avg_hip_y:  # Ïñ¥Íπ®Í∞Ä ÏóâÎç©Ïù¥Î≥¥Îã§ ÏïÑÎûòÏóê ÏûàÏùå (ÎπÑÏ†ïÏÉÅ)
                    penalty += 0.2
            
            # ÌåîÍøàÏπòÍ∞Ä Ïñ¥Íπ®ÏôÄ ÏÜêÎ™© ÏÇ¨Ïù¥Ïóê ÏûàÎäîÏßÄ
            for side in ['left', 'right']:
                if side == 'left':
                    shoulder_idx, elbow_idx, wrist_idx = 5, 7, 9
                else:
                    shoulder_idx, elbow_idx, wrist_idx = 6, 8, 10
                
                if all(keypoints[i][2] > 0.3 for i in [shoulder_idx, elbow_idx, wrist_idx]):
                    # ÌåîÍøàÏπòÍ∞Ä Ïñ¥Íπ®-ÏÜêÎ™© ÏÑ†Î∂ÑÏóêÏÑú ÎÑàÎ¨¥ Î©ÄÎ¶¨ Îñ®Ïñ¥Ï†∏ ÏûàÎäîÏßÄ Í≤ÄÏÇ¨
                    arm_length = np.linalg.norm(np.array(keypoints[shoulder_idx][:2]) - 
                                              np.array(keypoints[wrist_idx][:2]))
                    elbow_distance = self._point_to_line_distance(
                        keypoints[elbow_idx][:2], 
                        keypoints[shoulder_idx][:2], 
                        keypoints[wrist_idx][:2]
                    )
                    
                    if arm_length > 0 and elbow_distance / arm_length > 0.3:  # Ìåî Í∏∏Ïù¥Ïùò 30% Ïù¥ÏÉÅ Î≤óÏñ¥ÎÇ®
                        penalty += 0.1
        
        plausibility_score = max(0.0, plausibility_score - penalty)
        return plausibility_score
    
    def _assess_body_symmetry(self, keypoints: List[List[float]], body_proportions: Dict[str, float]) -> float:
        """Ïã†Ï≤¥ ÎåÄÏπ≠ÏÑ± ÌèâÍ∞Ä"""
        symmetry_score = 1.0
        penalty = 0.0
        
        if len(keypoints) >= 17:
            # Ï¢åÏö∞ Ïñ¥Íπ® ÎÜíÏù¥ ÎπÑÍµê
            if keypoints[5][2] > 0.3 and keypoints[6][2] > 0.3:
                shoulder_height_diff = abs(keypoints[5][1] - keypoints[6][1])
                shoulder_width = abs(keypoints[5][0] - keypoints[6][0])
                if shoulder_width > 0:
                    shoulder_asymmetry = shoulder_height_diff / shoulder_width
                    if shoulder_asymmetry > 0.2:  # 20% Ïù¥ÏÉÅ ÎπÑÎåÄÏπ≠
                        penalty += 0.1
            
            # Ï¢åÏö∞ ÏóâÎç©Ïù¥ ÎÜíÏù¥ ÎπÑÍµê
            if keypoints[11][2] > 0.3 and keypoints[12][2] > 0.3:
                hip_height_diff = abs(keypoints[11][1] - keypoints[12][1])
                hip_width = abs(keypoints[11][0] - keypoints[12][0])
                if hip_width > 0:
                    hip_asymmetry = hip_height_diff / hip_width
                    if hip_asymmetry > 0.2:
                        penalty += 0.1
            
            # Ìåî Í∏∏Ïù¥ ÎåÄÏπ≠ÏÑ±
            if 'arm_asymmetry_ratio' in body_proportions:
                if body_proportions['arm_asymmetry_ratio'] > 0.15:  # 15% Ïù¥ÏÉÅ Ï∞®Ïù¥
                    penalty += 0.1
            
            # Îã§Î¶¨ Í∏∏Ïù¥ ÎåÄÏπ≠ÏÑ±
            if 'leg_asymmetry_ratio' in body_proportions:
                if body_proportions['leg_asymmetry_ratio'] > 0.15:
                    penalty += 0.1
        
        symmetry_score = max(0.0, symmetry_score - penalty)
        return symmetry_score
    
    def _point_to_line_distance(self, point, line_start, line_end):
        """Ï†êÏóêÏÑú ÏßÅÏÑ†ÍπåÏßÄÏùò Í±∞Î¶¨ Í≥ÑÏÇ∞"""
        try:
            line_vec = np.array(line_end) - np.array(line_start)
            point_vec = np.array(point) - np.array(line_start)
            
            line_len = np.linalg.norm(line_vec)
            if line_len == 0:
                return np.linalg.norm(point_vec)
            
            line_unitvec = line_vec / line_len
            proj_length = np.dot(point_vec, line_unitvec)
            proj = proj_length * line_unitvec
            
            distance = np.linalg.norm(point_vec - proj)
            return distance
        except:
            return 0.0
    
    def _validate_joint_angles(self, joint_angles: Dict[str, float]) -> Dict[str, bool]:
        """Í¥ÄÏ†à Í∞ÅÎèÑ Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù"""
        validity = {}
        for joint, angle in joint_angles.items():
            if joint in self.joint_angle_ranges:
                min_angle, max_angle = self.joint_angle_ranges[joint]
                validity[joint] = min_angle <= angle <= max_angle
            else:
                validity[joint] = True  # Î≤îÏúÑÍ∞Ä Ï†ïÏùòÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ Ïú†Ìö®Î°ú Í∞ÑÏ£º
        return validity
    
    def _validate_body_proportions(self, body_proportions: Dict[str, float]) -> Dict[str, Any]:
        """Ïã†Ï≤¥ ÎπÑÏú® Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù"""
        validation = {
            'proportions_within_normal_range': True,
            'unusual_proportions': [],
            'proportion_score': 1.0
        }
        
        # ÌëúÏ§Ä ÎπÑÏú®Í≥º ÎπÑÍµê
        for prop_name, standard_value in self.standard_proportions.items():
            if prop_name in body_proportions:
                measured_value = body_proportions[prop_name]
                # ÌëúÏ§ÄÍ∞íÏùò ¬±50% Î≤îÏúÑ ÎÇ¥ÏóêÏÑú Ï†ïÏÉÅÏúºÎ°ú Í∞ÑÏ£º
                tolerance = standard_value * 0.5
                
                if not (standard_value - tolerance <= measured_value <= standard_value + tolerance):
                    validation['proportions_within_normal_range'] = False
                    validation['unusual_proportions'].append({
                        'proportion': prop_name,
                        'measured': measured_value,
                        'standard': standard_value,
                        'deviation_percent': abs(measured_value - standard_value) / standard_value * 100
                    })
        
        # ÎπÑÏú® Ï†êÏàò Í≥ÑÏÇ∞
        if validation['unusual_proportions']:
            penalty = min(0.5, len(validation['unusual_proportions']) * 0.1)
            validation['proportion_score'] = max(0.0, 1.0 - penalty)
        
        return validation
    
    def _analyze_left_right_balance(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """Ï¢åÏö∞ Í∑†Ìòï Î∂ÑÏÑù"""
        balance_analysis = {
            'overall_balance_score': 1.0,
            'shoulder_balance': 1.0,
            'hip_balance': 1.0,
            'limb_position_balance': 1.0
        }
        
        if len(keypoints) >= 17:
            # Ïñ¥Íπ® Í∑†Ìòï
            if keypoints[5][2] > 0.3 and keypoints[6][2] > 0.3:
                shoulder_height_diff = abs(keypoints[5][1] - keypoints[6][1])
                shoulder_center = (keypoints[5][1] + keypoints[6][1]) / 2
                if shoulder_center > 0:
                    balance_analysis['shoulder_balance'] = max(0.0, 1.0 - (shoulder_height_diff / shoulder_center))
            
            # ÏóâÎç©Ïù¥ Í∑†Ìòï
            if keypoints[11][2] > 0.3 and keypoints[12][2] > 0.3:
                hip_height_diff = abs(keypoints[11][1] - keypoints[12][1])
                hip_center = (keypoints[11][1] + keypoints[12][1]) / 2
                if hip_center > 0:
                    balance_analysis['hip_balance'] = max(0.0, 1.0 - (hip_height_diff / hip_center))
            
            # Ï†ÑÏ≤¥ Í∑†Ìòï Ï†êÏàò
            balance_analysis['overall_balance_score'] = (
                balance_analysis['shoulder_balance'] * 0.4 +
                balance_analysis['hip_balance'] * 0.4 +
                balance_analysis['limb_position_balance'] * 0.2
            )
        
        return balance_analysis
    
    def _analyze_posture_alignment(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """ÏûêÏÑ∏ Ï†ïÎ†¨ Î∂ÑÏÑù"""
        alignment_analysis = {
            'spine_alignment_score': 1.0,
            'head_neck_alignment': 1.0,
            'overall_posture_score': 1.0
        }
        
        if len(keypoints) >= 17:
            # Ï≤ôÏ∂î Ï†ïÎ†¨ (Ïñ¥Íπ® Ï§ëÏ†êÍ≥º ÏóâÎç©Ïù¥ Ï§ëÏ†êÏùò ÏàòÏßÅ Ï†ïÎ†¨)
            if (all(keypoints[i][2] > 0.3 for i in [5, 6, 11, 12])):
                shoulder_center_x = (keypoints[5][0] + keypoints[6][0]) / 2
                hip_center_x = (keypoints[11][0] + keypoints[12][0]) / 2
                
                horizontal_offset = abs(shoulder_center_x - hip_center_x)
                body_width = abs(keypoints[5][0] - keypoints[6][0])
                
                if body_width > 0:
                    alignment_ratio = horizontal_offset / body_width
                    alignment_analysis['spine_alignment_score'] = max(0.0, 1.0 - alignment_ratio)
            
            # Î®∏Î¶¨-Î™© Ï†ïÎ†¨
            if (keypoints[0][2] > 0.3 and 
                all(keypoints[i][2] > 0.3 for i in [5, 6])):
                
                neck_center_x = (keypoints[5][0] + keypoints[6][0]) / 2
                head_offset = abs(keypoints[0][0] - neck_center_x)
                neck_width = abs(keypoints[5][0] - keypoints[6][0])
                
                if neck_width > 0:
                    head_alignment_ratio = head_offset / neck_width
                    alignment_analysis['head_neck_alignment'] = max(0.0, 1.0 - head_alignment_ratio)
            
            # Ï†ÑÏ≤¥ ÏûêÏÑ∏ Ï†êÏàò
            alignment_analysis['overall_posture_score'] = (
                alignment_analysis['spine_alignment_score'] * 0.6 +
                alignment_analysis['head_neck_alignment'] * 0.4
            )
        
        return alignment_analysis
    
    def _identify_pose_issues(self, 
                            keypoints: List[List[float]], 
                            joint_angles: Dict[str, float], 
                            body_proportions: Dict[str, float],
                            scores: Dict[str, float]) -> List[str]:
        """Ìè¨Ï¶à Î¨∏Ï†úÏ†ê ÏãùÎ≥Ñ"""
        issues = []
        
        # Í∞ÄÏãúÏÑ± Î¨∏Ï†ú
        if scores.get('visibility', 0) < 0.6:
            issues.append("ÌÇ§Ìè¨Ïù∏Ìä∏ Í∞ÄÏãúÏÑ±Ïù¥ ÎÇÆÏäµÎãàÎã§")
        
        # Ïã†Î¢∞ÎèÑ Î¨∏Ï†ú
        if scores.get('average_confidence', 0) < 0.5:
            issues.append("ÌÇ§Ìè¨Ïù∏Ìä∏ Í≤ÄÏ∂ú Ïã†Î¢∞ÎèÑÍ∞Ä ÎÇÆÏäµÎãàÎã§")
        
        # Ìï¥Î∂ÄÌïôÏ†Å Î¨∏Ï†ú
        if scores.get('anatomical_plausibility', 0) < 0.7:
            issues.append("Ìï¥Î∂ÄÌïôÏ†ÅÏúºÎ°ú Î∂ÄÏûêÏó∞Ïä§Îü¨Ïö¥ Ìè¨Ï¶àÏûÖÎãàÎã§")
        
        # ÎåÄÏπ≠ÏÑ± Î¨∏Ï†ú
        if scores.get('symmetry', 0) < 0.7:
            issues.append("Ïã†Ï≤¥ Ï¢åÏö∞ ÎåÄÏπ≠ÏÑ±Ïù¥ Î∂ÄÏ°±Ìï©ÎãàÎã§")
        
        # ÏôÑÏÑ±ÎèÑ Î¨∏Ï†ú
        if scores.get('completeness', 0) < 0.8:
            issues.append("ÌïµÏã¨ Ïã†Ï≤¥ Î∂ÄÏúÑÍ∞Ä Í≤ÄÏ∂úÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§")
        
        # Í¥ÄÏ†à Í∞ÅÎèÑ Î¨∏Ï†ú
        invalid_joints = [joint for joint, angle in joint_angles.items() 
                         if joint in self.joint_angle_ranges and 
                         not (self.joint_angle_ranges[joint][0] <= angle <= self.joint_angle_ranges[joint][1])]
        
        if invalid_joints:
            issues.append(f"ÎπÑÏ†ïÏÉÅÏ†ÅÏù∏ Í¥ÄÏ†à Í∞ÅÎèÑ: {', '.join(invalid_joints)}")
        
        # ÎπÑÏú® Î¨∏Ï†ú
        unusual_proportions = []
        for prop_name, standard_value in self.standard_proportions.items():
            if prop_name in body_proportions:
                measured_value = body_proportions[prop_name]
                tolerance = standard_value * 0.5
                if not (standard_value - tolerance <= measured_value <= standard_value + tolerance):
                    deviation = abs(measured_value - standard_value) / standard_value * 100
                    unusual_proportions.append(f"{prop_name} ({deviation:.1f}% Ìé∏Ï∞®)")
        
        if unusual_proportions:
            issues.append(f"ÎπÑÏ†ïÏÉÅÏ†ÅÏù∏ Ïã†Ï≤¥ ÎπÑÏú®: {', '.join(unusual_proportions)}")
        
        return issues
    
    def _generate_pose_recommendations(self, issues: List[str], scores: Dict[str, float]) -> List[str]:
        """Ìè¨Ï¶à Í∞úÏÑ† Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±"""
        recommendations = []
        
        # Í∞ÄÏãúÏÑ± Í∞úÏÑ†
        if scores.get('visibility', 0) < 0.6:
            recommendations.extend([
                "Ï†ÑÏã†Ïù¥ ÌîÑÎ†àÏûÑ ÏïàÏóê Îì§Ïñ¥Ïò§ÎèÑÎ°ù Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî",
                "Í∞ÄÎ†§ÏßÑ Ïã†Ï≤¥ Î∂ÄÏúÑÍ∞Ä Î≥¥Ïù¥ÎèÑÎ°ù ÏûêÏÑ∏Î•º Ï°∞Ï†ïÌï¥ Ï£ºÏÑ∏Ïöî",
                "Îçî Î∞ùÏùÄ Ï°∞Î™ÖÏóêÏÑú Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî"
            ])
        
        # Ïã†Î¢∞ÎèÑ Í∞úÏÑ†
        if scores.get('average_confidence', 0) < 0.5:
            recommendations.extend([
                "Îçî ÏÑ†Î™ÖÌïòÍ≥† Í≥†Ìï¥ÏÉÅÎèÑÎ°ú Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî",
                "Î∞∞Í≤ΩÍ≥º ÎåÄÎπÑÎêòÎäî ÏùòÏÉÅÏùÑ Ï∞©Ïö©Ìï¥ Ï£ºÏÑ∏Ïöî",
                "Ïπ¥Î©îÎùº ÌùîÎì§Î¶º ÏóÜÏù¥ Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî"
            ])
        
        # Ìï¥Î∂ÄÌïôÏ†Å Í∞úÏÑ†
        if scores.get('anatomical_plausibility', 0) < 0.7:
            recommendations.extend([
                "ÏûêÏó∞Ïä§Îü¨Ïö¥ ÏûêÏÑ∏Î•º Ï∑®Ìï¥ Ï£ºÏÑ∏Ïöî",
                "Í≥ºÎèÑÌïòÍ≤å Íµ¨Î∂ÄÎü¨ÏßÑ Í¥ÄÏ†àÏùÑ Ìé¥Ï£ºÏÑ∏Ïöî",
                "Ï†ïÎ©¥ ÎòêÎäî Ï∏°Î©¥ÏùÑ Ìñ•Ìïú ÏûêÏÑ∏Î°ú Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî"
            ])
        
        # ÎåÄÏπ≠ÏÑ± Í∞úÏÑ†
        if scores.get('symmetry', 0) < 0.7:
            recommendations.extend([
                "Ïñ¥Íπ®ÏôÄ ÏóâÎç©Ïù¥Í∞Ä ÏàòÌèâÏù¥ ÎêòÎèÑÎ°ù ÏûêÏÑ∏Î•º Ï°∞Ï†ïÌï¥ Ï£ºÏÑ∏Ïöî",
                "Ï¢åÏö∞ ÌåîÎã§Î¶¨Í∞Ä Í∑†ÌòïÏùÑ Ïù¥Î£®ÎèÑÎ°ù Ìï¥Ï£ºÏÑ∏Ïöî",
                "Î™∏Ïùò Ï§ëÏã¨ÏÑ†Ïù¥ ÎòëÎ∞îÎ°ú ÏÑúÎèÑÎ°ù Ìï¥Ï£ºÏÑ∏Ïöî"
            ])
        
        # ÏôÑÏÑ±ÎèÑ Í∞úÏÑ†
        if scores.get('completeness', 0) < 0.8:
            recommendations.extend([
                "Î®∏Î¶¨Î∂ÄÌÑ∞ Î∞úÎÅùÍπåÏßÄ Ï†ÑÏã†Ïù¥ Î≥¥Ïù¥ÎèÑÎ°ù Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî",
                "ÌåîÍ≥º Îã§Î¶¨Í∞Ä Î™∏ÌÜµÏóê Í∞ÄÎ†§ÏßÄÏßÄ ÏïäÎèÑÎ°ù Ìï¥Ï£ºÏÑ∏Ïöî",
                "Ïπ¥Î©îÎùºÏôÄÏùò Í±∞Î¶¨Î•º Ï°∞Ï†ïÌï¥ Ï£ºÏÑ∏Ïöî"
            ])
        
        # ÏùºÎ∞òÏ†ÅÏù∏ Í∂åÏû•ÏÇ¨Ìï≠
        if not recommendations:
            recommendations.extend([
                "ÌòÑÏû¨ Ìè¨Ï¶àÍ∞Ä ÏñëÌò∏Ìï©ÎãàÎã§",
                "Îçî ÎÇòÏùÄ Í≤∞Í≥ºÎ•º ÏúÑÌï¥ Ï°∞Î™ÖÏùÑ Í∞úÏÑ†Ìï¥ Î≥¥ÏÑ∏Ïöî",
                "Îã§ÏñëÌïú Í∞ÅÎèÑÏóêÏÑú Ï¥¨ÏòÅÌï¥ Î≥¥ÏÑ∏Ïöî"
            ])
        
        return recommendations[:5]  # ÏµúÎåÄ 5Í∞ú Í∂åÏû•ÏÇ¨Ìï≠Îßå Î∞òÌôò

# ==============================================
# üî• 5. Î©îÏù∏ PoseEstimationStep ÌÅ¥ÎûòÏä§
# ==============================================

class PoseEstimationStep(BaseStepMixin):
    """
    üî• Step 02: Pose Estimation - Central Hub DI Container v7.0 ÏôÑÏ†Ñ Ïó∞Îèô
    
    ‚úÖ BaseStepMixin ÏÉÅÏÜç Ìå®ÌÑ¥ (Human Parsing StepÍ≥º ÎèôÏùº)
    ‚úÖ MediaPipe Pose Î™®Îç∏ ÏßÄÏõê (Ïö∞ÏÑ†ÏàúÏúÑ 1)
    ‚úÖ OpenPose Î™®Îç∏ ÏßÄÏõê (Ìè¥Î∞± ÏòµÏÖò)
    ‚úÖ YOLOv8-Pose Î™®Îç∏ ÏßÄÏõê (Ïã§ÏãúÍ∞Ñ)
    ‚úÖ HRNet Î™®Îç∏ ÏßÄÏõê (Í≥†Ï†ïÎ∞Ä)
    ‚úÖ 17Í∞ú COCO keypoints Í∞êÏßÄ
    ‚úÖ Mock Î™®Îç∏ ÏôÑÏ†Ñ Ï†úÍ±∞
    ‚úÖ Ïã§Ï†ú AI Ï∂îÎ°† Ïã§Ìñâ
    ‚úÖ Îã§Ï§ë Î™®Îç∏ Ìè¥Î∞± ÏãúÏä§ÌÖú
    """
    
    def __init__(self, **kwargs):
        
        """Ìè¨Ï¶à Ï∂îÏ†ï Step Ï¥àÍ∏∞Ìôî"""
        self._lock = threading.RLock()  # ‚úÖ threading ÏÇ¨Ïö©

        # üî• 1. ÌïÑÏàò ÏÜçÏÑ±Îì§ Ï¥àÍ∏∞Ìôî (ÏóêÎü¨ Î∞©ÏßÄ)
        self._initialize_step_attributes()
        
                # üî• 2. BaseStepMixin Ï¥àÍ∏∞Ìôî (Central Hub ÏûêÎèô Ïó∞Îèô)
        super().__init__(step_name="PoseEstimationStep", **kwargs)
        
        # üî• 3. Pose Estimation ÌäπÌôî Ï¥àÍ∏∞Ìôî
        self._initialize_pose_estimation_specifics()
    
    def _initialize_step_attributes(self):
        """Step ÌïÑÏàò ÏÜçÏÑ±Îì§ Ï¥àÍ∏∞Ìôî"""
        self.ai_models = {}
        self.models_loading_status = {
            'mediapipe': False,
            'openpose': False,
            'yolov8': False,
            'hrnet': False,
            'total_loaded': 0,
            'loading_errors': []
        }
        self.model_interface = None
        self.loaded_models = {}
        
        # Pose Estimation ÌäπÌôî ÏÜçÏÑ±Îì§
        self.pose_models = {}
        self.pose_ready = False
        self.keypoints_cache = {}
    
    def _initialize_pose_estimation_specifics(self):
        """Pose Estimation ÌäπÌôî Ï¥àÍ∏∞Ìôî"""
        
        # ÏÑ§Ï†ï
        self.confidence_threshold = 0.5
        self.use_subpixel = True
        
        # Ìè¨Ï¶à Î∂ÑÏÑùÍ∏∞
        self.analyzer = PoseAnalyzer()
        
        # Î™®Îç∏ Ïö∞ÏÑ†ÏàúÏúÑ (MediaPipe Ïö∞ÏÑ†)
        self.model_priority = [
            PoseModel.MEDIAPIPE,
            PoseModel.YOLOV8_POSE,
            PoseModel.OPENPOSE,
            PoseModel.HRNET
        ]
        
        self.logger.info(f"‚úÖ {self.step_name} Ìè¨Ï¶à Ï∂îÏ†ï ÌäπÌôî Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    def _load_pose_models_via_central_hub(self):
        """Central HubÎ•º ÌÜµÌïú Pose Î™®Îç∏ Î°úÎî©"""
        loaded_count = 0
        
        if self.model_loader:  # Central HubÏóêÏÑú ÏûêÎèô Ï£ºÏûÖÎê®
            # MediaPipe Î™®Îç∏ Î°úÎî©
            try:
                mediapipe_model = MediaPoseModel()
                if mediapipe_model.load_model():
                    self.ai_models['mediapipe'] = mediapipe_model
                    self.models_loading_status['mediapipe'] = True
                    loaded_count += 1
                    self.logger.info("‚úÖ MediaPipe Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è MediaPipe Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
                self.models_loading_status['loading_errors'].append(f"MediaPipe: {e}")
            
            # YOLOv8 Î™®Îç∏ Î°úÎî©
            try:
                # Central HubÏóêÏÑú YOLOv8 Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú Ï°∞Ìöå
                yolo_path = self._get_model_path_from_central_hub('yolov8n-pose.pt')
                yolo_model = YOLOv8PoseModel(yolo_path)
                if yolo_model.load_model():
                    self.ai_models['yolov8'] = yolo_model
                    self.models_loading_status['yolov8'] = True
                    loaded_count += 1
                    self.logger.info("‚úÖ YOLOv8 Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è YOLOv8 Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
                self.models_loading_status['loading_errors'].append(f"YOLOv8: {e}")
            
            # OpenPose Î™®Îç∏ Î°úÎî©
            try:
                openpose_path = self._get_model_path_from_central_hub('body_pose_model.pth')
                openpose_model = OpenPoseModel(openpose_path)
                if openpose_model.load_model():
                    self.ai_models['openpose'] = openpose_model
                    self.models_loading_status['openpose'] = True
                    loaded_count += 1
                    self.logger.info("‚úÖ OpenPose Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è OpenPose Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
                self.models_loading_status['loading_errors'].append(f"OpenPose: {e}")
            
            # HRNet Î™®Îç∏ Î°úÎî©
            try:
                hrnet_path = self._get_model_path_from_central_hub('hrnet_w48_coco_256x192.pth')
                hrnet_model = HRNetModel(hrnet_path)
                if hrnet_model.load_model():
                    self.ai_models['hrnet'] = hrnet_model
                    self.models_loading_status['hrnet'] = True
                    loaded_count += 1
                    self.logger.info("‚úÖ HRNet Î™®Îç∏ Î°úÎî© ÏÑ±Í≥µ")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è HRNet Î™®Îç∏ Î°úÎî© Ïã§Ìå®: {e}")
                self.models_loading_status['loading_errors'].append(f"HRNet: {e}")
        
        else:
            # Ìè¥Î∞±: MediaPipeÎßå Î°úÎî© ÏãúÎèÑ
            self.logger.warning("‚ö†Ô∏è ModelLoaderÍ∞Ä ÏóÜÏùå - MediaPipeÎßå Î°úÎî© ÏãúÎèÑ")
            try:
                mediapipe_model = MediaPoseModel()
                if mediapipe_model.load_model():
                    self.ai_models['mediapipe'] = mediapipe_model
                    self.models_loading_status['mediapipe'] = True
                    loaded_count += 1
            except Exception as e:
                self.logger.error(f"‚ùå MediaPipe Ìè¥Î∞± Î°úÎî©ÎèÑ Ïã§Ìå®: {e}")
        
        self.models_loading_status['total_loaded'] = loaded_count
        self.pose_ready = loaded_count > 0
        
        if loaded_count > 0:
            self.logger.info(f"üéâ Ìè¨Ï¶à Î™®Îç∏ Î°úÎî© ÏôÑÎ£å: {loaded_count}Í∞ú")
        else:
            self.logger.error("‚ùå Î™®Îì† Ìè¨Ï¶à Î™®Îç∏ Î°úÎî© Ïã§Ìå®")
        
        return loaded_count
    
    def _get_model_path_from_central_hub(self, model_name: str) -> Optional[Path]:
        """Central HubÎ•º ÌÜµÌïú Î™®Îç∏ Í≤ΩÎ°ú Ï°∞Ìöå"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'get_model_path'):
                path_str = self.model_loader.get_model_path(model_name, step_name=self.step_name)
                if path_str:
                    return Path(path_str)
            return None
        except Exception as e:
            self.logger.debug(f"Î™®Îç∏ Í≤ΩÎ°ú Ï°∞Ìöå Ïã§Ìå® ({model_name}): {e}")
            return None
    
    def process(self, **kwargs) -> Dict[str, Any]:
        """üî• Îã®Í≥ÑÎ≥Ñ ÏÑ∏Î∂ÑÌôîÎêú ÏóêÎü¨ Ï≤òÎ¶¨Í∞Ä Ï†ÅÏö©Îêú Pose Estimation process Î©îÏÑúÎìú"""
        start_time = time.time()
        errors = []
        stage_status = {}
        
        try:
            # üî• 1Îã®Í≥Ñ: ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
            try:
                if not kwargs:
                    raise ValueError("ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Í∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§")
                
                # ÌïÑÏàò ÏûÖÎ†• ÌïÑÎìú ÌôïÏù∏ (Ìè¨Ï¶à Ï∂îÏ†ïÏö©)
                required_fields = ['image', 'person_image', 'input_image', 'original_image']
                has_required_field = any(field in kwargs for field in required_fields)
                if not has_required_field:
                    raise ValueError("ÌïÑÏàò ÏûÖÎ†• ÌïÑÎìú(image, person_image, input_image, original_image Ï§ë ÌïòÎÇò)Í∞Ä ÏóÜÏäµÎãàÎã§")
                
                stage_status['input_validation'] = 'success'
                self.logger.info("‚úÖ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù ÏôÑÎ£å")
                
            except Exception as e:
                stage_status['input_validation'] = 'failed'
                error_info = {
                    'stage': 'input_validation',
                    'error_type': type(e).__name__,
                    'message': str(e),
                    'input_keys': list(kwargs.keys()) if kwargs else []
                }
                errors.append(error_info)
                
                # ÏóêÎü¨ Ï∂îÏ†Å
                if EXCEPTIONS_AVAILABLE:
                    log_detailed_error(
                        DataValidationError(f"ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù Ïã§Ìå®: {str(e)}", 
                                          ErrorCodes.DATA_VALIDATION_FAILED, 
                                          {'input_keys': list(kwargs.keys()) if kwargs else []}),
                        {'step_name': self.step_name, 'step_id': getattr(self, 'step_id', 2)},
                        getattr(self, 'step_id', 2)
                    )
                
                return {
                    'success': False,
                    'errors': errors,
                    'stage_status': stage_status,
                    'step_name': self.step_name,
                    'processing_time': time.time() - start_time
                }
            
            # üî• 2Îã®Í≥Ñ: Î™©ÏóÖ Îç∞Ïù¥ÌÑ∞ ÏßÑÎã®
            try:
                if MOCK_DIAGNOSTIC_AVAILABLE:
                    mock_detections = []
                    for key, value in kwargs.items():
                        if value is not None:
                            mock_detection = detect_mock_data(value)
                            if mock_detection['is_mock']:
                                mock_detections.append({
                                    'input_key': key,
                                    'detection_result': mock_detection
                                })
                                self.logger.warning(f"ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ '{key}'ÏóêÏÑú Î™©ÏóÖ Îç∞Ïù¥ÌÑ∞ Í∞êÏßÄ: {mock_detection}")
                    
                    if mock_detections:
                        stage_status['mock_detection'] = 'warning'
                        errors.append({
                            'stage': 'mock_detection',
                            'error_type': 'MockDataDetectionError',
                            'message': 'Î™©ÏóÖ Îç∞Ïù¥ÌÑ∞Í∞Ä Í∞êÏßÄÎêòÏóàÏäµÎãàÎã§',
                            'mock_detections': mock_detections
                        })
                    else:
                        stage_status['mock_detection'] = 'success'
                else:
                    stage_status['mock_detection'] = 'skipped'
                    
            except Exception as e:
                stage_status['mock_detection'] = 'failed'
                self.logger.warning(f"Î™©ÏóÖ Îç∞Ïù¥ÌÑ∞ ÏßÑÎã® Ï§ë Ïò§Î•ò: {e}")
            
            # üî• 3Îã®Í≥Ñ: ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò
            try:
                if hasattr(self, 'convert_api_input_to_step_input'):
                    processed_input = self.convert_api_input_to_step_input(kwargs)
                else:
                    processed_input = kwargs
                
                stage_status['input_conversion'] = 'success'
                self.logger.info("‚úÖ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò ÏôÑÎ£å")
                
            except Exception as e:
                stage_status['input_conversion'] = 'failed'
                error_info = {
                    'stage': 'input_conversion',
                    'error_type': type(e).__name__,
                    'message': str(e)
                }
                errors.append(error_info)
                
                if EXCEPTIONS_AVAILABLE:
                    log_detailed_error(
                        DataValidationError(f"ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò Ïã§Ìå®: {str(e)}", 
                                          ErrorCodes.DATA_VALIDATION_FAILED),
                        {'step_name': self.step_name, 'step_id': getattr(self, 'step_id', 2)},
                        getattr(self, 'step_id', 2)
                    )
                
                return {
                    'success': False,
                    'errors': errors,
                    'stage_status': stage_status,
                    'step_name': self.step_name,
                    'processing_time': time.time() - start_time
                }
            
            # üî• 4Îã®Í≥Ñ: Ìè¨Ï¶à Î™®Îç∏ Î°úÎî© ÌôïÏù∏
            try:
                if not hasattr(self, 'pose_models') or not self.pose_models:
                    raise RuntimeError("Ìè¨Ï¶à Ï∂îÏ†ï Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§")
                
                # Ïã§Ï†ú Î™®Îç∏ vs Mock Î™®Îç∏ ÌôïÏù∏
                loaded_models = list(self.pose_models.keys())
                is_mock_only = all('mock' in model_name.lower() for model_name in loaded_models)
                
                if is_mock_only:
                    stage_status['model_loading'] = 'warning'
                    errors.append({
                        'stage': 'model_loading',
                        'error_type': 'MockModelWarning',
                        'message': 'Ïã§Ï†ú Ìè¨Ï¶à Ï∂îÏ†ï Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏïÑ Mock Î™®Îç∏ÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§',
                        'loaded_models': loaded_models
                    })
                else:
                    stage_status['model_loading'] = 'success'
                    self.logger.info(f"‚úÖ Ìè¨Ï¶à Î™®Îç∏ Î°úÎî© ÌôïÏù∏ ÏôÑÎ£å: {loaded_models}")
                
            except Exception as e:
                stage_status['model_loading'] = 'failed'
                error_info = {
                    'stage': 'model_loading',
                    'error_type': type(e).__name__,
                    'message': str(e)
                }
                errors.append(error_info)
                
                if EXCEPTIONS_AVAILABLE:
                    log_detailed_error(
                        ModelLoadingError(f"Ìè¨Ï¶à Î™®Îç∏ Î°úÎî© ÌôïÏù∏ Ïã§Ìå®: {str(e)}", 
                                        ErrorCodes.MODEL_LOADING_FAILED),
                        {'step_name': self.step_name, 'step_id': getattr(self, 'step_id', 2)},
                        getattr(self, 'step_id', 2)
                    )
                
                return {
                    'success': False,
                    'errors': errors,
                    'stage_status': stage_status,
                    'step_name': self.step_name,
                    'processing_time': time.time() - start_time
                }
            
            # üî• 5Îã®Í≥Ñ: AI Ï∂îÎ°† Ïã§Ìñâ
            try:
                result = self._run_ai_inference(processed_input)
                
                # Ï∂îÎ°† Í≤∞Í≥º Í≤ÄÏ¶ù
                if not result or 'success' not in result:
                    raise RuntimeError("Ìè¨Ï¶à Ï∂îÏ†ï Í≤∞Í≥ºÍ∞Ä Ïò¨Î∞îÎ•¥ÏßÄ ÏïäÏäµÎãàÎã§")
                
                if not result.get('success', False):
                    raise RuntimeError(f"Ìè¨Ï¶à Ï∂îÏ†ï Ïã§Ìå®: {result.get('error', 'Ïïå Ïàò ÏóÜÎäî Ïò§Î•ò')}")
                
                # ÌÇ§Ìè¨Ïù∏Ìä∏ Í≤ÄÏ¶ù
                if 'keypoints' in result:
                    keypoints = result['keypoints']
                    if not keypoints or len(keypoints) == 0:
                        raise RuntimeError("Ìè¨Ï¶à ÌÇ§Ìè¨Ïù∏Ìä∏Í∞Ä Í∞êÏßÄÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§")
                    
                    # COCO 17Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏ ÌòïÏãù Í≤ÄÏ¶ù
                    if len(keypoints) != 17:
                        self.logger.warning(f"‚ö†Ô∏è ÌÇ§Ìè¨Ïù∏Ìä∏ Í∞úÏàòÍ∞Ä ÏòàÏÉÅÍ≥º Îã§Î¶ÖÎãàÎã§: {len(keypoints)}Í∞ú (ÏòàÏÉÅ: 17Í∞ú)")
                
                stage_status['ai_inference'] = 'success'
                self.logger.info("‚úÖ Ìè¨Ï¶à Ï∂îÏ†ï ÏôÑÎ£å")
                
            except Exception as e:
                stage_status['ai_inference'] = 'failed'
                error_info = {
                    'stage': 'ai_inference',
                    'error_type': type(e).__name__,
                    'message': str(e)
                }
                errors.append(error_info)
                
                if EXCEPTIONS_AVAILABLE:
                    log_detailed_error(
                        ModelInferenceError(f"Ìè¨Ï¶à Ï∂îÏ†ï Ïã§Ìå®: {str(e)}", 
                                          ErrorCodes.AI_INFERENCE_FAILED),
                        {'step_name': self.step_name, 'step_id': getattr(self, 'step_id', 2)},
                        getattr(self, 'step_id', 2)
                    )
                
                return {
                    'success': False,
                    'errors': errors,
                    'stage_status': stage_status,
                    'step_name': self.step_name,
                    'processing_time': time.time() - start_time
                }
            
            # üî• 6Îã®Í≥Ñ: Ìè¨Ï¶à ÌíàÏßà Î∂ÑÏÑù
            try:
                if 'keypoints' in result and result['keypoints']:
                    # Ìè¨Ï¶à ÌíàÏßà Î∂ÑÏÑù ÏàòÌñâ
                    pose_analyzer = PoseAnalyzer()
                    keypoints = result['keypoints']
                    
                    # Í¥ÄÏ†à Í∞ÅÎèÑ Í≥ÑÏÇ∞
                    joint_angles = pose_analyzer.calculate_joint_angles(keypoints)
                    
                    # Ïã†Ï≤¥ ÎπÑÏú® Í≥ÑÏÇ∞
                    body_proportions = pose_analyzer.calculate_body_proportions(keypoints)
                    
                    # Ìè¨Ï¶à ÌíàÏßà ÌèâÍ∞Ä
                    quality_assessment = pose_analyzer.assess_pose_quality(
                        keypoints, joint_angles, body_proportions
                    )
                    
                    # Í≤∞Í≥ºÏóê ÌíàÏßà Ï†ïÎ≥¥ Ï∂îÍ∞Ä
                    result['joint_angles'] = joint_angles
                    result['body_proportions'] = body_proportions
                    result['pose_quality'] = quality_assessment
                    
                    stage_status['pose_analysis'] = 'success'
                    self.logger.info("‚úÖ Ìè¨Ï¶à ÌíàÏßà Î∂ÑÏÑù ÏôÑÎ£å")
                else:
                    stage_status['pose_analysis'] = 'skipped'
                    self.logger.warning("‚ö†Ô∏è ÌÇ§Ìè¨Ïù∏Ìä∏Í∞Ä ÏóÜÏñ¥ Ìè¨Ï¶à ÌíàÏßà Î∂ÑÏÑùÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§")
                
            except Exception as e:
                stage_status['pose_analysis'] = 'failed'
                self.logger.warning(f"Ìè¨Ï¶à ÌíàÏßà Î∂ÑÏÑù Ï§ë Ïò§Î•ò: {e}")
                # Ìè¨Ï¶à ÌíàÏßà Î∂ÑÏÑù Ïã§Ìå®Îäî ÏπòÎ™ÖÏ†ÅÏù¥ÏßÄ ÏïäÏúºÎØÄÎ°ú Í≥ÑÏÜç ÏßÑÌñâ
            
            # üî• 7Îã®Í≥Ñ: Ï∂úÎ†• Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
            try:
                # Ï∂úÎ†• Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Î™©ÏóÖ Îç∞Ïù¥ÌÑ∞ Í∞êÏßÄ
                if MOCK_DIAGNOSTIC_AVAILABLE:
                    output_mock_detections = []
                    for key, value in result.items():
                        if value is not None:
                            mock_detection = detect_mock_data(value)
                            if mock_detection['is_mock']:
                                output_mock_detections.append({
                                    'output_key': key,
                                    'detection_result': mock_detection
                                })
                    
                    if output_mock_detections:
                        stage_status['output_validation'] = 'warning'
                        errors.append({
                            'stage': 'output_validation',
                            'error_type': 'MockOutputWarning',
                            'message': 'Ï∂úÎ†• Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Î™©ÏóÖ Îç∞Ïù¥ÌÑ∞Í∞Ä Í∞êÏßÄÎêòÏóàÏäµÎãàÎã§',
                            'mock_detections': output_mock_detections
                        })
                    else:
                        stage_status['output_validation'] = 'success'
                else:
                    stage_status['output_validation'] = 'skipped'
                
            except Exception as e:
                stage_status['output_validation'] = 'failed'
                self.logger.warning(f"Ï∂úÎ†• Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù Ï§ë Ïò§Î•ò: {e}")
            
            # üî• ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ±
            processing_time = time.time() - start_time
            
            # ÏÑ±Í≥µ Ïó¨Î∂Ä Í≤∞Ï†ï (ÏπòÎ™ÖÏ†Å ÏóêÎü¨Í∞Ä ÏûàÏúºÎ©¥ Ïã§Ìå®)
            critical_errors = [e for e in errors if e['stage'] in ['input_validation', 'input_conversion', 'ai_inference']]
            is_success = len(critical_errors) == 0
            
            final_result = {
                'success': is_success,
                'errors': errors,
                'stage_status': stage_status,
                'step_name': self.step_name,
                'processing_time': processing_time,
                'is_mock_used': any('mock' in e.get('error_type', '').lower() for e in errors),
                'critical_error_count': len(critical_errors),
                'warning_count': len(errors) - len(critical_errors)
            }
            
            # ÏÑ±Í≥µÌïú Í≤ΩÏö∞ ÏõêÎ≥∏ Í≤∞Í≥ºÎèÑ Ìè¨Ìï®
            if is_success:
                final_result.update(result)
            
            return final_result
            
        except Exception as e:
            # ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò
            processing_time = time.time() - start_time
            
            if EXCEPTIONS_AVAILABLE:
                error = convert_to_mycloset_exception(e, {
                    'step_name': self.step_name,
                    'step_id': getattr(self, 'step_id', 2),
                    'operation': 'process'
                })
                track_exception(error, {
                    'step_name': self.step_name,
                    'step_id': getattr(self, 'step_id', 2),
                    'operation': 'process'
                }, getattr(self, 'step_id', 2))
                
                return create_exception_response(
                    error,
                    self.step_name,
                    getattr(self, 'step_id', 2),
                    kwargs.get('session_id', 'unknown')
                )
            else:
                return {
                    'success': False,
                    'error': 'UNEXPECTED_ERROR',
                    'message': f"ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò Î∞úÏÉù: {str(e)}",
                    'step_name': self.step_name,
                    'processing_time': processing_time
                }
    
    def _get_service_from_central_hub(self, service_key: str):
        """Central HubÏóêÏÑú ÏÑúÎπÑÏä§ Í∞ÄÏ†∏Ïò§Í∏∞"""
        try:
            if hasattr(self, 'di_container') and self.di_container:
                return self.di_container.get_service(service_key)
            return None
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Central Hub ÏÑúÎπÑÏä§ Í∞ÄÏ†∏Ïò§Í∏∞ Ïã§Ìå®: {e}")
            return None
    
    def convert_api_input_to_step_input(self, api_input: Dict[str, Any]) -> Dict[str, Any]:
        """API ÏûÖÎ†•ÏùÑ Step ÏûÖÎ†•ÏúºÎ°ú Î≥ÄÌôò (ÎèôÍ∏∞ Î≤ÑÏ†Ñ)"""
        try:
            step_input = api_input.copy()
            
            # Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú (Îã§ÏñëÌïú ÌÇ§ Ïù¥Î¶Ñ ÏßÄÏõê)
            image = None
            for key in ['image', 'person_image', 'input_image', 'original_image']:
                if key in step_input:
                    image = step_input[key]
                    break
            
            if image is None and 'session_id' in step_input:
                # ÏÑ∏ÏÖòÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Î°úÎìú (ÎèôÍ∏∞Ï†ÅÏúºÎ°ú)
                try:
                    session_manager = self._get_service_from_central_hub('session_manager')
                    if session_manager:
                        person_image, clothing_image = None, None
                        
                        try:
                            # ÏÑ∏ÏÖò Îß§ÎãàÏ†ÄÍ∞Ä ÎèôÍ∏∞ Î©îÏÑúÎìúÎ•º Ï†úÍ≥µÌïòÎäîÏßÄ ÌôïÏù∏
                            if hasattr(session_manager, 'get_session_images_sync'):
                                person_image, clothing_image = session_manager.get_session_images_sync(step_input['session_id'])
                            elif hasattr(session_manager, 'get_session_images'):
                                # ÎπÑÎèôÍ∏∞ Î©îÏÑúÎìúÎ•º ÎèôÍ∏∞Ï†ÅÏúºÎ°ú Ìò∏Ï∂ú
                                import asyncio
                                import concurrent.futures
                                
                                def run_async_session_load():
                                    try:
                                        return asyncio.run(session_manager.get_session_images(step_input['session_id']))
                                    except Exception as async_error:
                                        self.logger.warning(f"‚ö†Ô∏è ÎπÑÎèôÍ∏∞ ÏÑ∏ÏÖò Î°úÎìú Ïã§Ìå®: {async_error}")
                                        return None, None
                                
                                try:
                                    with concurrent.futures.ThreadPoolExecutor() as executor:
                                        future = executor.submit(run_async_session_load)
                                        person_image, clothing_image = future.result(timeout=10)
                                except Exception as executor_error:
                                    self.logger.warning(f"‚ö†Ô∏è ÏÑ∏ÏÖò Î°úÎìú ThreadPoolExecutor Ïã§Ìå®: {executor_error}")
                                    person_image, clothing_image = None, None
                            else:
                                self.logger.warning("‚ö†Ô∏è ÏÑ∏ÏÖò Îß§ÎãàÏ†ÄÏóê Ï†ÅÏ†àÌïú Î©îÏÑúÎìúÍ∞Ä ÏóÜÏùå")
                        except Exception as e:
                            self.logger.warning(f"‚ö†Ô∏è ÏÑ∏ÏÖò Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïã§Ìå®: {e}")
                            person_image, clothing_image = None, None
                        
                        if person_image:
                            image = person_image
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è ÏÑ∏ÏÖòÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïã§Ìå®: {e}")
            
            # Î≥ÄÌôòÎêú ÏûÖÎ†• Íµ¨ÏÑ±
            converted_input = {
                'image': image,
                'person_image': image,
                'session_id': step_input.get('session_id'),
                'detection_confidence': step_input.get('detection_confidence', 0.5),
                'clothing_type': step_input.get('clothing_type', 'shirt')
            }
            
            self.logger.info(f"‚úÖ API ÏûÖÎ†• Î≥ÄÌôò ÏôÑÎ£å: {len(converted_input)}Í∞ú ÌÇ§")
            return converted_input
            
        except Exception as e:
            self.logger.error(f"‚ùå API ÏûÖÎ†• Î≥ÄÌôò Ïã§Ìå®: {e}")
            return api_input
    
    async def initialize(self):
        """Step Ï¥àÍ∏∞Ìôî (BaseStepMixin Ìò∏Ìôò)"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"üîÑ {self.step_name} Ï¥àÍ∏∞Ìôî ÏãúÏûë...")
            
            # Pose Î™®Îç∏Îì§ Î°úÎî©
            loaded_count = self._load_pose_models_via_central_hub()
            
            if loaded_count == 0:
                self.logger.error("‚ùå Ìè¨Ï¶à Î™®Îç∏ Î°úÎî© Ïã§Ìå® - Ï¥àÍ∏∞Ìôî Ïã§Ìå®")
                return False
            
            # Ï¥àÍ∏∞Ìôî ÏôÑÎ£å
            self.is_initialized = True
            self.is_ready = True
            self.logger.info(f"‚úÖ {self.step_name} Ï¥àÍ∏∞Ìôî ÏôÑÎ£å ({loaded_count}Í∞ú Î™®Îç∏)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå {self.step_name} Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """üî• Ïã§Ï†ú Pose Estimation AI Ï∂îÎ°† (BaseStepMixin v20.0 Ìò∏Ìôò)"""
        try:
            start_time = time.time()
            
            # üî• ÎîîÎ≤ÑÍπÖ: ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÏÉÅÏÑ∏ Î°úÍπÖ
            self.logger.info(f"üîç [DEBUG] Pose Estimation ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÌÇ§Îì§: {list(processed_input.keys())}")
            self.logger.info(f"üîç [DEBUG] Pose Estimation ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖÎì§: {[(k, type(v).__name__) for k, v in processed_input.items()]}")
            
            # ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
            if not processed_input:
                self.logger.error("‚ùå [DEBUG] Pose Estimation ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Í∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§")
                if EXCEPTIONS_AVAILABLE:
                    error = DataValidationError("ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Í∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§", ErrorCodes.DATA_VALIDATION_FAILED)
                    track_exception(error, {
                        'step_name': self.step_name,
                        'step_id': self.step_id,
                        'operation': '_run_ai_inference'
                    }, self.step_id)
                    raise error
                else:
                    raise ValueError("ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Í∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§")
            
            self.logger.info(f"‚úÖ [DEBUG] Pose Estimation ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù ÏôÑÎ£å")
            
            # üî• SessionÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞Î•º Î®ºÏ†Ä Í∞ÄÏ†∏Ïò§Í∏∞
            image = None
            if 'session_id' in processed_input:
                try:
                    session_manager = self._get_service_from_central_hub('session_manager')
                    if session_manager:
                        person_image, clothing_image = None, None
                        
                        try:
                            # ÏÑ∏ÏÖò Îß§ÎãàÏ†ÄÍ∞Ä ÎèôÍ∏∞ Î©îÏÑúÎìúÎ•º Ï†úÍ≥µÌïòÎäîÏßÄ ÌôïÏù∏
                            if hasattr(session_manager, 'get_session_images_sync'):
                                person_image, clothing_image = session_manager.get_session_images_sync(processed_input['session_id'])
                            elif hasattr(session_manager, 'get_session_images'):
                                # ÎπÑÎèôÍ∏∞ Î©îÏÑúÎìúÎ•º ÎèôÍ∏∞Ï†ÅÏúºÎ°ú Ìò∏Ï∂ú
                                import asyncio
                                import concurrent.futures
                                
                                def run_async_session_load():
                                    try:
                                        return asyncio.run(session_manager.get_session_images(processed_input['session_id']))
                                    except Exception as async_error:
                                        self.logger.warning(f"‚ö†Ô∏è ÎπÑÎèôÍ∏∞ ÏÑ∏ÏÖò Î°úÎìú Ïã§Ìå®: {async_error}")
                                        return None, None
                                
                                try:
                                    with concurrent.futures.ThreadPoolExecutor() as executor:
                                        future = executor.submit(run_async_session_load)
                                        person_image, clothing_image = future.result(timeout=10)
                                except Exception as executor_error:
                                    self.logger.warning(f"‚ö†Ô∏è ÏÑ∏ÏÖò Î°úÎìú ThreadPoolExecutor Ïã§Ìå®: {executor_error}")
                                    person_image, clothing_image = None, None
                            else:
                                self.logger.warning("‚ö†Ô∏è ÏÑ∏ÏÖò Îß§ÎãàÏ†ÄÏóê Ï†ÅÏ†àÌïú Î©îÏÑúÎìúÍ∞Ä ÏóÜÏùå")
                        except Exception as e:
                            self.logger.warning(f"‚ö†Ô∏è ÏÑ∏ÏÖò Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïã§Ìå®: {e}")
                            person_image, clothing_image = None, None
                        image = person_image  # Ìè¨Ï¶à Ï∂îÏ†ïÏùÄ ÏÇ¨Îûå Ïù¥ÎØ∏ÏßÄ ÏÇ¨Ïö©
                        self.logger.info(f"‚úÖ SessionÏóêÏÑú ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Î°úÎìú ÏôÑÎ£å: {type(image)}")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è sessionÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            
            # üî• ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù (Step 1Í≥º ÎèôÏùºÌïú Ìå®ÌÑ¥)
            self.logger.debug(f"üîç ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÌÇ§Îì§: {list(processed_input.keys())}")
            
            # Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú (Îã§ÏñëÌïú ÌÇ§ÏóêÏÑú ÏãúÎèÑ) - SessionÏóêÏÑú Í∞ÄÏ†∏Ïò§ÏßÄ Î™ªÌïú Í≤ΩÏö∞
            if image is None:
                for key in ['image', 'input_image', 'original_image', 'processed_image']:
                    if key in processed_input:
                        image = processed_input[key]
                        self.logger.info(f"‚úÖ Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ Î∞úÍ≤¨: {key}")
                        break
            
            if image is None:
                self.logger.error("‚ùå ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù Ïã§Ìå®: ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ ÏóÜÏùå (Step 2)")
                return {'success': False, 'error': 'ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ ÏóÜÏùå'}
            
            self.logger.info("üß† Pose Estimation Ïã§Ï†ú AI Ï∂îÎ°† ÏãúÏûë")
            
            # Î™®Îç∏Ïù¥ Î°úÎî©ÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ Ï¥àÍ∏∞Ìôî ÏãúÎèÑ
            if not self.pose_ready:
                self.logger.warning("‚ö†Ô∏è Ìè¨Ï¶à Î™®Îç∏Ïù¥ Ï§ÄÎπÑÎêòÏßÄ ÏïäÏùå - Ïû¨Î°úÎî© ÏãúÎèÑ")
                loaded = self._load_pose_models_via_central_hub()
                if loaded == 0:
                    raise RuntimeError("Ìè¨Ï¶à Î™®Îç∏ Î°úÎî© Ïã§Ìå®")
            
            # Îã§Ï§ë Î™®Îç∏Î°ú Ìè¨Ï¶à Ï∂îÏ†ï ÏãúÎèÑ (Ïö∞ÏÑ†ÏàúÏúÑ ÏàúÏÑú)
            best_result = None
            best_confidence = 0.0
            
            for model_type in self.model_priority:
                model_key = model_type.value
                
                if model_key in self.ai_models:
                    try:
                        self.logger.debug(f"üîÑ {model_key} Î™®Îç∏Î°ú Ìè¨Ï¶à Ï∂îÏ†ï ÏãúÎèÑ")
                        result = self.ai_models[model_key].detect_poses(image)
                        
                        if result.get('success') and result.get('keypoints'):
                            confidence = result.get('confidence', 0.0)
                            
                            # ÏµúÍ≥† Ïã†Î¢∞ÎèÑ Í≤∞Í≥º ÏÑ†ÌÉù
                            if confidence > best_confidence:
                                best_result = result
                                best_confidence = confidence
                                best_result['primary_model'] = model_key
                            
                            self.logger.debug(f"‚úÖ {model_key} ÏÑ±Í≥µ (Ïã†Î¢∞ÎèÑ: {confidence:.3f})")
                            
                        else:
                            self.logger.debug(f"‚ö†Ô∏è {model_key} Ïã§Ìå®: {result.get('error', 'Unknown')}")
                            
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è {model_key} Ï∂îÎ°† Ïã§Ìå®: {e}")
                        continue
            
            if not best_result or not best_result.get('keypoints'):
                raise RuntimeError("Î™®Îì† Ìè¨Ï¶à Î™®Îç∏ÏóêÏÑú Ïú†Ìö®Ìïú ÌÇ§Ìè¨Ïù∏Ìä∏Î•º Í≤ÄÏ∂úÌïòÏßÄ Î™ªÌï®")
            
            # ÌÇ§Ìè¨Ïù∏Ìä∏ ÌõÑÏ≤òÎ¶¨ Î∞è Î∂ÑÏÑù
            keypoints = best_result['keypoints']
            
            # keypointsÍ∞Ä Î¶¨Ïä§Ìä∏Ïù∏ÏßÄ ÌôïÏù∏ÌïòÍ≥† ÎîïÏÖîÎÑàÎ¶¨Î°ú Í∞êÏã∏Í∏∞
            if isinstance(keypoints, list):
                self.logger.info(f"‚úÖ keypointsÍ∞Ä Î¶¨Ïä§Ìä∏Î°ú Î∞òÌôòÎê®: {len(keypoints)}Í∞ú ÌÇ§Ìè¨Ïù∏Ìä∏")
            else:
                self.logger.warning(f"‚ö†Ô∏è keypointsÍ∞Ä Î¶¨Ïä§Ìä∏Í∞Ä ÏïÑÎãò: {type(keypoints)}")
                keypoints = []
            
            # Í¥ÄÏ†à Í∞ÅÎèÑ Í≥ÑÏÇ∞
            joint_angles = self.analyzer.calculate_joint_angles(keypoints)
            
            # Ïã†Ï≤¥ ÎπÑÏú® Í≥ÑÏÇ∞
            body_proportions = self.analyzer.calculate_body_proportions(keypoints)
            
            # Ìè¨Ï¶à ÌíàÏßà ÌèâÍ∞Ä
            quality_assessment = self.analyzer.assess_pose_quality(
                keypoints, joint_angles, body_proportions
            )
            
            inference_time = time.time() - start_time
            
            # ÎîïÏÖîÎÑàÎ¶¨Î°ú Í∞êÏã∏ÏÑú Î∞òÌôò
            result_dict = {
                'success': True,
                'keypoints': keypoints,
                'confidence_scores': [kp[2] for kp in keypoints] if keypoints else [],
                'joint_angles': joint_angles,
                'body_proportions': body_proportions,
                'pose_quality': quality_assessment['overall_score'],
                'quality_grade': quality_assessment['quality_grade'].value,
                'processing_time': inference_time,
                'model_used': best_result.get('primary_model', 'unknown'),
                'real_ai_inference': True,
                'pose_estimation_ready': True,
                'num_keypoints_detected': len([kp for kp in keypoints if kp[2] > 0.3]),
                
                # Í≥†Í∏â Î∂ÑÏÑù Í≤∞Í≥º
                'detailed_scores': quality_assessment.get('detailed_scores', {}),
                'pose_recommendations': quality_assessment.get('recommendations', []),
                'skeleton_structure': self._build_skeleton_structure(keypoints),
                'landmarks': self._extract_landmarks(keypoints)
            }
            
            self.logger.info(f"‚úÖ Pose Estimation Í≤∞Í≥º ÎîïÏÖîÎÑàÎ¶¨ Î∞òÌôò: {len(result_dict)}Í∞ú ÌÇ§")
            return result_dict
            
        except Exception as e:
            self.logger.error(f"‚ùå Pose Estimation AI Ï∂îÎ°† Ïã§Ìå®: {e}")
            if EXCEPTIONS_AVAILABLE:
                error = convert_to_mycloset_exception(e, {
                    'step_name': self.step_name,
                    'step_id': self.step_id,
                    'operation': '_run_ai_inference'
                })
                track_exception(error, {
                    'step_name': self.step_name,
                    'step_id': self.step_id,
                    'operation': '_run_ai_inference'
                }, self.step_id)
            
            return {
                'success': False,
                'error': str(e),
                'keypoints': [],
                'confidence_scores': [],
                'pose_quality': 0.0,
                'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                'model_used': 'error',
                'real_ai_inference': False,
                'pose_estimation_ready': False
            }
    
    def _build_skeleton_structure(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """Ïä§ÏºàÎ†àÌÜ§ Íµ¨Ï°∞ ÏÉùÏÑ±"""
        skeleton = {
            'connections': [],
            'bone_lengths': {},
            'valid_connections': 0
        }
        
        # COCO 17 Ïó∞Í≤∞ Íµ¨Ï°∞
        coco_connections = [
            (0, 1), (0, 2), (1, 3), (2, 4),  # Î®∏Î¶¨
            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # Ìåî
            (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)  # Îã§Î¶¨
        ]
        
        for i, (start_idx, end_idx) in enumerate(coco_connections):
            if (start_idx < len(keypoints) and end_idx < len(keypoints) and
                len(keypoints[start_idx]) >= 3 and len(keypoints[end_idx]) >= 3):
                
                start_kp = keypoints[start_idx]
                end_kp = keypoints[end_idx]
                
                if start_kp[2] > self.confidence_threshold and end_kp[2] > self.confidence_threshold:
                    bone_length = np.sqrt(
                        (start_kp[0] - end_kp[0])**2 + (start_kp[1] - end_kp[1])**2
                    )
                    
                    connection = {
                        'start': start_idx,
                        'end': end_idx,
                        'start_name': COCO_17_KEYPOINTS[start_idx] if start_idx < len(COCO_17_KEYPOINTS) else f"point_{start_idx}",
                        'end_name': COCO_17_KEYPOINTS[end_idx] if end_idx < len(COCO_17_KEYPOINTS) else f"point_{end_idx}",
                        'length': bone_length,
                        'confidence': (start_kp[2] + end_kp[2]) / 2
                    }
                    
                    skeleton['connections'].append(connection)
                    skeleton['bone_lengths'][f"{start_idx}_{end_idx}"] = bone_length
                    skeleton['valid_connections'] += 1
        
        return skeleton
    
    def _extract_landmarks(self, keypoints: List[List[float]]) -> Dict[str, Dict[str, float]]:
        """Ï£ºÏöî ÎûúÎìúÎßàÌÅ¨ Ï∂îÏ∂ú"""
        landmarks = {}
        
        for i, kp in enumerate(keypoints):
            if len(kp) >= 3 and kp[2] > self.confidence_threshold:
                landmark_name = COCO_17_KEYPOINTS[i] if i < len(COCO_17_KEYPOINTS) else f"landmark_{i}"
                landmarks[landmark_name] = {
                    'x': float(kp[0]),
                    'y': float(kp[1]),
                    'confidence': float(kp[2])
                }
        
        return landmarks
    
    # ==============================================
    # üî• BaseStepMixin Ìò∏Ìôò Î©îÏÑúÎìúÎì§
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ (BaseStepMixin Ìò∏Ìôò)"""
        try:
            self.model_loader = model_loader
            self.logger.info("‚úÖ ModelLoader ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ£å")
            
            # Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± ÏãúÎèÑ
            if hasattr(model_loader, 'create_step_interface'):
                try:
                    self.model_interface = model_loader.create_step_interface(self.step_name)
                    self.logger.info("‚úÖ Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Î∞è Ï£ºÏûÖ ÏôÑÎ£å")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Step Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ± Ïã§Ìå®, ModelLoader ÏßÅÏ†ë ÏÇ¨Ïö©: {e}")
                    self.model_interface = model_loader
            else:
                self.model_interface = model_loader
                
        except Exception as e:
            self.logger.error(f"‚ùå ModelLoader ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ïã§Ìå®: {e}")
            self.model_loader = None
            self.model_interface = None
            
    def set_memory_manager(self, memory_manager):
        """MemoryManager ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ (BaseStepMixin Ìò∏Ìôò)"""
        try:
            self.memory_manager = memory_manager
            self.logger.info("‚úÖ MemoryManager ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ£å")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è MemoryManager ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ïã§Ìå®: {e}")
    
    def set_data_converter(self, data_converter):
        """DataConverter ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ (BaseStepMixin Ìò∏Ìôò)"""
        try:
            self.data_converter = data_converter
            self.logger.info("‚úÖ DataConverter ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ£å")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è DataConverter ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ïã§Ìå®: {e}")
    
    def set_di_container(self, di_container):
        """DI Container ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ"""
        try:
            self.di_container = di_container
            self.logger.info("‚úÖ DI Container ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏôÑÎ£å")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è DI Container ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ïã§Ìå®: {e}")
    
    async def cleanup(self):
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        try:
            self.logger.info(f"üîÑ {self.step_name} Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ ÏãúÏûë...")
            
            # AI Î™®Îç∏Îì§ Ï†ïÎ¶¨
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'cleanup'):
                        model.cleanup()
                    del model
                except Exception as e:
                    self.logger.debug(f"Î™®Îç∏ Ï†ïÎ¶¨ Ïã§Ìå® ({model_name}): {e}")
            
            # Ï∫êÏãú Ï†ïÎ¶¨
            self.ai_models.clear()
            self.pose_models.clear()
            self.keypoints_cache.clear()
            
            # üî• 128GB M3 Max Í∞ïÏ†ú Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
            if TORCH_AVAILABLE:
                try:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif hasattr(torch, 'mps') and torch.mps.is_available():
                        torch.mps.empty_cache()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
            
            # Í∞ïÏ†ú Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖò
            for _ in range(3):
                gc.collect()
            
            self.logger.info(f"‚úÖ {self.step_name} Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå {self.step_name} Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    def get_model_status(self) -> Dict[str, Any]:
        """Î™®Îç∏ ÏÉÅÌÉú Ï°∞Ìöå"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'pose_ready': self.pose_ready,
            'models_loading_status': self.models_loading_status,
            'loaded_models': list(self.ai_models.keys()),
            'model_priority': [model.value for model in self.model_priority],
            'confidence_threshold': self.confidence_threshold,
            'use_subpixel': self.use_subpixel
        }

    def _convert_step_output_type(self, step_output: Dict[str, Any], *args, **kwargs) -> Dict[str, Any]:
        """Step Ï∂úÎ†•ÏùÑ API ÏùëÎãµ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò"""
        try:
            if not isinstance(step_output, dict):
                self.logger.warning(f"‚ö†Ô∏è step_outputÏù¥ dictÍ∞Ä ÏïÑÎãò: {type(step_output)}")
                return {
                    'success': False,
                    'error': f'Invalid output type: {type(step_output)}',
                    'step_name': self.step_name,
                    'step_id': self.step_id
                }
            
            # Í∏∞Î≥∏ API ÏùëÎãµ Íµ¨Ï°∞
            api_response = {
                'success': step_output.get('success', True),
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': step_output.get('processing_time', 0.0),
                'timestamp': time.time()
            }
            
            # Ïò§Î•òÍ∞Ä ÏûàÎäî Í≤ΩÏö∞
            if not api_response['success']:
                api_response['error'] = step_output.get('error', 'Unknown error')
                return api_response
            
            # Ìè¨Ï¶à Ï∂îÏ†ï Í≤∞Í≥º Î≥ÄÌôò (ÏßÅÏ†ë ÌÇ§Ìè¨Ïù∏Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©)
            api_response['pose_data'] = {
                'keypoints': step_output.get('keypoints', []),
                'confidence_scores': step_output.get('confidence_scores', []),
                'overall_confidence': step_output.get('pose_quality', 0.0),
                'pose_quality': step_output.get('quality_grade', 'unknown'),
                'model_used': step_output.get('model_used', 'unknown'),
                'joint_angles': step_output.get('joint_angles', {}),
                'body_proportions': step_output.get('body_proportions', {}),
                'skeleton_structure': step_output.get('skeleton_structure', {}),
                'landmarks': step_output.get('landmarks', {}),
                'num_keypoints_detected': step_output.get('num_keypoints_detected', 0),
                'detailed_scores': step_output.get('detailed_scores', {}),
                'pose_recommendations': step_output.get('pose_recommendations', [])
            }
            
            # Ï∂îÍ∞Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
            api_response['metadata'] = {
                'models_available': list(self.pose_models.keys()) if hasattr(self, 'pose_models') else [],
                'device_used': getattr(self, 'device', 'unknown'),
                'input_size': step_output.get('input_size', [0, 0]),
                'output_size': step_output.get('output_size', [0, 0]),
                'real_ai_inference': step_output.get('real_ai_inference', False),
                'pose_estimation_ready': step_output.get('pose_estimation_ready', False)
            }
            
            # ÏãúÍ∞ÅÌôî Îç∞Ïù¥ÌÑ∞ (ÏûàÎäî Í≤ΩÏö∞)
            if 'visualization' in step_output:
                api_response['visualization'] = step_output['visualization']
            
            # Î∂ÑÏÑù Í≤∞Í≥º (ÏûàÎäî Í≤ΩÏö∞)
            if 'analysis' in step_output:
                api_response['analysis'] = step_output['analysis']
            
            self.logger.info(f"‚úÖ PoseEstimationStep Ï∂úÎ†• Î≥ÄÌôò ÏôÑÎ£å: {len(api_response)}Í∞ú ÌÇ§")
            return api_response
            
        except Exception as e:
            self.logger.error(f"‚ùå PoseEstimationStep Ï∂úÎ†• Î≥ÄÌôò Ïã§Ìå®: {e}")
            return {
                'success': False,
                'error': f'Output conversion failed: {str(e)}',
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': step_output.get('processing_time', 0.0) if isinstance(step_output, dict) else 0.0
            }

# ==============================================
# üî• 6. Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
# ==============================================

def validate_keypoints(keypoints: List[List[float]]) -> bool:
    """ÌÇ§Ìè¨Ïù∏Ìä∏ Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù"""
    try:
        if not keypoints:
            return False
        
        for kp in keypoints:
            if len(kp) < 3:
                return False
            if not all(isinstance(x, (int, float)) for x in kp):
                return False
            if kp[2] < 0 or kp[2] > 1:
                return False
        
        return True
        
    except Exception:
        return False

def draw_pose_on_image(
    image: Union[np.ndarray, Image.Image],
    keypoints: List[List[float]],
    confidence_threshold: float = 0.5,
    keypoint_size: int = 4,
    line_width: int = 3
) -> Image.Image:
    """Ïù¥ÎØ∏ÏßÄÏóê Ìè¨Ï¶à Í∑∏Î¶¨Í∏∞"""
    try:
        if isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        else:
            pil_image = image.copy()
        
        draw = ImageDraw.Draw(pil_image)
        
        # ÌÇ§Ìè¨Ïù∏Ìä∏ Í∑∏Î¶¨Í∏∞
        for i, kp in enumerate(keypoints):
            if len(kp) >= 3 and kp[2] > confidence_threshold:
                x, y = int(kp[0]), int(kp[1])
                color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                
                radius = int(keypoint_size + kp[2] * 6)
                draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                           fill=color, outline=(255, 255, 255), width=2)
        
        # Ïä§ÏºàÎ†àÌÜ§ Í∑∏Î¶¨Í∏∞ (COCO 17 Ïó∞Í≤∞ Íµ¨Ï°∞)
        coco_connections = [
            (0, 1), (0, 2), (1, 3), (2, 4),  # Î®∏Î¶¨
            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # Ìåî
            (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)  # Îã§Î¶¨
        ]
        
        for i, (start_idx, end_idx) in enumerate(coco_connections):
            if (start_idx < len(keypoints) and end_idx < len(keypoints)):
                start_kp = keypoints[start_idx]
                end_kp = keypoints[end_idx]
                
                if (len(start_kp) >= 3 and len(end_kp) >= 3 and
                    start_kp[2] > confidence_threshold and end_kp[2] > confidence_threshold):
                    
                    start_point = (int(start_kp[0]), int(start_kp[1]))
                    end_point = (int(end_kp[0]), int(end_kp[1]))
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    avg_confidence = (start_kp[2] + end_kp[2]) / 2
                    adjusted_width = int(line_width * avg_confidence)
                    
                    draw.line([start_point, end_point], fill=color, width=max(1, adjusted_width))
        
        return pil_image
        
    except Exception as e:
        logger.error(f"Ìè¨Ï¶à Í∑∏Î¶¨Í∏∞ Ïã§Ìå®: {e}")
        return image if isinstance(image, Image.Image) else Image.fromarray(image)

def analyze_pose_for_clothing_advanced(
    keypoints: List[List[float]],
    clothing_type: str = "default",
    confidence_threshold: float = 0.5,
    detailed_analysis: bool = True
) -> Dict[str, Any]:
    """Í≥†Í∏â ÏùòÎ•òÎ≥Ñ Ìè¨Ï¶à Ï†ÅÌï©ÏÑ± Î∂ÑÏÑù"""
    try:
        if not keypoints:
            return {
                'suitable_for_fitting': False,
                'issues': ["Ìè¨Ï¶àÎ•º Í≤ÄÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§"],
                'recommendations': ["Îçî ÏÑ†Î™ÖÌïú Ïù¥ÎØ∏ÏßÄÎ•º ÏÇ¨Ïö©Ìï¥ Ï£ºÏÑ∏Ïöî"],
                'pose_score': 0.0,
                'detailed_analysis': {}
            }
        
        # ÏùòÎ•òÎ≥Ñ ÏÑ∏Î∂Ä Í∞ÄÏ§ëÏπò
        clothing_detailed_weights = {
            'shirt': {
                'critical_keypoints': [5, 6, 7, 8, 9, 10],  # Ïñ¥Íπ®, ÌåîÍøàÏπò, ÏÜêÎ™©
                'weights': {'arms': 0.4, 'torso': 0.4, 'posture': 0.2},
                'min_visibility': 0.7,
                'required_angles': ['left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow']
            },
            'dress': {
                'critical_keypoints': [5, 6, 11, 12, 13, 14],  # Ïñ¥Íπ®, ÏóâÎç©Ïù¥, Î¨¥Î¶é
                'weights': {'torso': 0.5, 'arms': 0.2, 'legs': 0.2, 'posture': 0.1},
                'min_visibility': 0.8,
                'required_angles': ['spine_curvature']
            },
            'pants': {
                'critical_keypoints': [11, 12, 13, 14, 15, 16],  # ÏóâÎç©Ïù¥, Î¨¥Î¶é, Î∞úÎ™©
                'weights': {'legs': 0.6, 'torso': 0.3, 'posture': 0.1},
                'min_visibility': 0.8,
                'required_angles': ['left_hip', 'right_hip', 'left_knee', 'right_knee']
            },
            'jacket': {
                'critical_keypoints': [5, 6, 7, 8, 9, 10, 11, 12],  # ÏÉÅÏ≤¥ Ï†ÑÏ≤¥
                'weights': {'arms': 0.4, 'torso': 0.4, 'shoulders': 0.2},
                'min_visibility': 0.75,
                'required_angles': ['left_shoulder', 'right_shoulder', 'spine_curvature']
            },
            'suit': {
                'critical_keypoints': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14],  # Í±∞Ïùò Ï†ÑÏã†
                'weights': {'torso': 0.3, 'arms': 0.3, 'legs': 0.2, 'posture': 0.2},
                'min_visibility': 0.85,
                'required_angles': ['spine_curvature', 'left_shoulder', 'right_shoulder']
            },
            'default': {
                'critical_keypoints': [0, 5, 6, 11, 12],  # Í∏∞Î≥∏ ÌïµÏã¨ Î∂ÄÏúÑ
                'weights': {'torso': 0.4, 'arms': 0.3, 'legs': 0.2, 'visibility': 0.1},
                'min_visibility': 0.6,
                'required_angles': []
            }
        }
        
        config = clothing_detailed_weights.get(clothing_type, clothing_detailed_weights['default'])
        
        # 1. ÌïµÏã¨ ÌÇ§Ìè¨Ïù∏Ìä∏ Í∞ÄÏãúÏÑ± Í≤ÄÏÇ¨
        critical_keypoints = config['critical_keypoints']
        visible_critical = sum(1 for idx in critical_keypoints 
                             if idx < len(keypoints) and len(keypoints[idx]) >= 3 
                             and keypoints[idx][2] > confidence_threshold)
        
        critical_visibility = visible_critical / len(critical_keypoints)
        
        # 2. Ïã†Ï≤¥ Î∂ÄÏúÑÎ≥Ñ Ï†êÏàò Í≥ÑÏÇ∞
        def calculate_body_part_score_advanced(part_indices: List[int]) -> Dict[str, float]:
            visible_count = 0
            total_confidence = 0.0
            position_quality = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
                        
                        # ÏúÑÏπò ÌíàÏßà ÌèâÍ∞Ä (ÌôîÎ©¥ Í≤ΩÍ≥ÑÏóêÏÑúÏùò Í±∞Î¶¨)
                        x, y = keypoints[idx][0], keypoints[idx][1]
                        # Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Î•º Î™®Î•¥ÎØÄÎ°ú ÏÉÅÎåÄÏ†Å ÌèâÍ∞Ä
                        if 0.1 <= x <= 0.9 and 0.1 <= y <= 0.9:  # Ï§ëÏïô 80% ÏòÅÏó≠
                            position_quality += 1.0
                        else:
                            position_quality += 0.5
            
            if visible_count == 0:
                return {'visibility': 0.0, 'confidence': 0.0, 'position': 0.0, 'combined': 0.0}
            
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            avg_position = position_quality / visible_count
            combined_score = (visibility_ratio * 0.4 + avg_confidence * 0.4 + avg_position * 0.2)
            
            return {
                'visibility': visibility_ratio,
                'confidence': avg_confidence,
                'position': avg_position,
                'combined': combined_score
            }
        
        # COCO 17 Î∂ÄÏúÑÎ≥Ñ Ïù∏Îç±Ïä§ (Í≥†Í∏â)
        body_parts = {
            'head': [0, 1, 2, 3, 4],  # ÏΩî, ÎààÎì§, Í∑ÄÎì§
            'torso': [5, 6, 11, 12],  # Ïñ¥Íπ®Îì§, ÏóâÎç©Ïù¥Îì§
            'arms': [5, 6, 7, 8, 9, 10],  # Ïñ¥Íπ®, ÌåîÍøàÏπò, ÏÜêÎ™©
            'legs': [11, 12, 13, 14, 15, 16],  # ÏóâÎç©Ïù¥, Î¨¥Î¶é, Î∞úÎ™©
            'left_arm': [5, 7, 9],
            'right_arm': [6, 8, 10],
            'left_leg': [11, 13, 15],
            'right_leg': [12, 14, 16]
        }
        
        part_scores = {}
        for part_name, indices in body_parts.items():
            part_scores[part_name] = calculate_body_part_score_advanced(indices)
        
        # 3. Í¥ÄÏ†à Í∞ÅÎèÑ Î∂ÑÏÑù
        analyzer = PoseAnalyzer()
        joint_angles = analyzer.calculate_joint_angles(keypoints)
        
        angle_score = 1.0
        missing_angles = []
        for required_angle in config.get('required_angles', []):
            if required_angle not in joint_angles:
                missing_angles.append(required_angle)
                angle_score *= 0.8  # ÌïÑÏàò Í∞ÅÎèÑ ÏóÜÏùÑ ÎïåÎßàÎã§ 20% Í∞êÏ†ê
        
        # 4. ÏûêÏÑ∏ ÏïàÏ†ïÏÑ± ÌèâÍ∞Ä
        posture_stability = analyze_posture_stability(keypoints)
        
        # 5. ÏùòÎ•òÎ≥Ñ ÌäπÌôî Î∂ÑÏÑù
        clothing_specific_score = analyze_clothing_specific_requirements(
            keypoints, clothing_type, joint_angles
        )
        
        # 6. Ï¢ÖÌï© Ï†êÏàò Í≥ÑÏÇ∞
        weights = config['weights']
        
        # Í∏∞Î≥∏ Ï†êÏàòÎì§
        torso_score = part_scores.get('torso', {}).get('combined', 0.0)
        arms_score = part_scores.get('arms', {}).get('combined', 0.0)
        legs_score = part_scores.get('legs', {}).get('combined', 0.0)
        
        # Í∞ÄÏ§ëÌèâÍ∑†
        pose_score = (
            torso_score * weights.get('torso', 0.4) +
            arms_score * weights.get('arms', 0.3) +
            legs_score * weights.get('legs', 0.2) +
            posture_stability * weights.get('posture', 0.1) +
            clothing_specific_score * 0.1
        )
        
        # 7. Ï†ÅÌï©ÏÑ± ÌåêÎã®
        min_visibility = config.get('min_visibility', 0.7)
        suitable_for_fitting = (
            pose_score >= 0.7 and 
            critical_visibility >= min_visibility and
            angle_score >= 0.6
        )
        
        # 8. Ïù¥Ïäà Î∞è Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±
        issues = []
        recommendations = []
        
        if not suitable_for_fitting:
            if critical_visibility < min_visibility:
                issues.append(f'{clothing_type} ÌîºÌåÖÏóê ÌïÑÏöîÌïú Ïã†Ï≤¥ Î∂ÄÏúÑÍ∞Ä Ï∂©Î∂ÑÌûà Î≥¥Ïù¥ÏßÄ ÏïäÏäµÎãàÎã§')
                recommendations.append('ÌïµÏã¨ Ïã†Ï≤¥ Î∂ÄÏúÑÍ∞Ä Î™®Îëê Î≥¥Ïù¥ÎèÑÎ°ù ÏûêÏÑ∏Î•º Ï°∞Ï†ïÌï¥ Ï£ºÏÑ∏Ïöî')
            
            if pose_score < 0.7:
                issues.append(f'{clothing_type} Ï∞©Ïö© ÏãúÎÆ¨Î†àÏù¥ÏÖòÏóê Ï†ÅÌï©ÌïòÏßÄ ÏïäÏùÄ Ìè¨Ï¶àÏûÖÎãàÎã§')
                recommendations.append('Îçî ÏûêÏó∞Ïä§ÎüΩÍ≥† Ï†ïÎ©¥ÏùÑ Ìñ•Ìïú ÏûêÏÑ∏Î°ú Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî')
            
            if missing_angles:
                issues.append(f'ÌïÑÏöîÌïú Í¥ÄÏ†à Í∞ÅÎèÑ Ï†ïÎ≥¥Í∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§: {", ".join(missing_angles)}')
                recommendations.append('Í¥ÄÏ†à Î∂ÄÏúÑÍ∞Ä Î™ÖÌôïÌûà Î≥¥Ïù¥ÎèÑÎ°ù ÏûêÏÑ∏Î•º Ï°∞Ï†ïÌï¥ Ï£ºÏÑ∏Ïöî')
        
        # 9. ÏÑ∏Î∂Ä Î∂ÑÏÑù Í≤∞Í≥º
        detailed_analysis_result = {
            'critical_visibility': critical_visibility,
            'part_scores': part_scores,
            'joint_angles': joint_angles,
            'angle_score': angle_score,
            'missing_angles': missing_angles,
            'posture_stability': posture_stability,
            'clothing_specific_score': clothing_specific_score,
            'min_visibility_threshold': min_visibility,
            'clothing_requirements': config
        } if detailed_analysis else {}
        
        return {
            'suitable_for_fitting': suitable_for_fitting,
            'issues': issues,
            'recommendations': recommendations,
            'pose_score': pose_score,
            'clothing_type': clothing_type,
            'detailed_analysis': detailed_analysis_result,
            'quality_metrics': {
                'overall_score': pose_score,
                'critical_visibility': critical_visibility,
                'angle_completeness': angle_score,
                'posture_stability': posture_stability,
                'clothing_compatibility': clothing_specific_score
            }
        }
        
    except Exception as e:
        logger.error(f"Í≥†Í∏â ÏùòÎ•òÎ≥Ñ Ìè¨Ï¶à Î∂ÑÏÑù Ïã§Ìå®: {e}")
        return {
            'suitable_for_fitting': False,
            'issues': ["Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§"],
            'recommendations': ["Îã§Ïãú ÏãúÎèÑÌï¥ Ï£ºÏÑ∏Ïöî"],
            'pose_score': 0.0,
            'error': str(e)
        }

def analyze_posture_stability(keypoints: List[List[float]]) -> float:
    """ÏûêÏÑ∏ ÏïàÏ†ïÏÑ± Î∂ÑÏÑù"""
    try:
        if len(keypoints) < 17:
            return 0.0
        
        stability_score = 1.0
        
        # 1. Ï§ëÏã¨ ÏïàÏ†ïÏÑ± (Ïñ¥Íπ®ÏôÄ ÏóâÎç©Ïù¥ Ï§ëÏ†êÏùò ÏàòÏßÅ Ï†ïÎ†¨)
        if all(keypoints[i][2] > 0.3 for i in [5, 6, 11, 12]):
            shoulder_center_x = (keypoints[5][0] + keypoints[6][0]) / 2
            hip_center_x = (keypoints[11][0] + keypoints[12][0]) / 2
            
            lateral_offset = abs(shoulder_center_x - hip_center_x)
            body_width = abs(keypoints[5][0] - keypoints[6][0])
            
            if body_width > 0:
                offset_ratio = lateral_offset / body_width
                center_stability = max(0.0, 1.0 - offset_ratio)
                stability_score *= center_stability
        
        # 2. Î∞ú ÏßÄÏßÄ ÏïàÏ†ïÏÑ±
        foot_support = 0.0
        if keypoints[15][2] > 0.3:  # ÏôºÎ∞úÎ™©
            foot_support += 0.5
        if keypoints[16][2] > 0.3:  # Ïò§Î•∏Î∞úÎ™©
            foot_support += 0.5
        
        stability_score *= foot_support
        
        # 3. Í∑†Ìòï ÏïàÏ†ïÏÑ± (Ï¢åÏö∞ ÎåÄÏπ≠)
        balance_score = 1.0
        
        # Ïñ¥Íπ® Í∑†Ìòï
        if keypoints[5][2] > 0.3 and keypoints[6][2] > 0.3:
            shoulder_tilt = abs(keypoints[5][1] - keypoints[6][1])
            shoulder_width = abs(keypoints[5][0] - keypoints[6][0])
            if shoulder_width > 0:
                shoulder_balance = max(0.0, 1.0 - (shoulder_tilt / shoulder_width))
                balance_score *= shoulder_balance
        
        stability_score *= balance_score
        
        return min(1.0, max(0.0, stability_score))
        
    except Exception:
        return 0.0

def analyze_clothing_specific_requirements(
    keypoints: List[List[float]], 
    clothing_type: str, 
    joint_angles: Dict[str, float]
) -> float:
    """ÏùòÎ•òÎ≥Ñ ÌäπÌôî ÏöîÍµ¨ÏÇ¨Ìï≠ Î∂ÑÏÑù"""
    try:
        specific_score = 1.0
        
        if clothing_type == 'shirt':
            # ÏÖîÏ∏†: Ìåî ÏûêÏÑ∏Í∞Ä Ï§ëÏöî
            if 'left_elbow' in joint_angles and 'right_elbow' in joint_angles:
                # ÌåîÍøàÏπòÍ∞Ä ÎÑàÎ¨¥ ÍµΩÌòÄÏ†∏ ÏûàÏúºÎ©¥ Í∞êÏ†ê
                avg_elbow_angle = (joint_angles['left_elbow'] + joint_angles['right_elbow']) / 2
                if avg_elbow_angle < 120:  # ÎÑàÎ¨¥ ÎßéÏù¥ ÍµΩÌòÄÏßê
                    specific_score *= 0.8
            
            # Ïñ¥Íπ®ÏÑ†Ïù¥ ÏàòÌèâÏù∏ÏßÄ ÌôïÏù∏
            if keypoints[5][2] > 0.3 and keypoints[6][2] > 0.3:
                shoulder_tilt = abs(keypoints[5][1] - keypoints[6][1])
                shoulder_width = abs(keypoints[5][0] - keypoints[6][0])
                if shoulder_width > 0 and (shoulder_tilt / shoulder_width) > 0.1:
                    specific_score *= 0.9
        
        elif clothing_type == 'dress':
            # ÎìúÎ†àÏä§: Ï†ÑÏ≤¥Ï†ÅÏù∏ ÏûêÏÑ∏ÏôÄ Ïã§Î£®Ïó£Ïù¥ Ï§ëÏöî
            if 'spine_curvature' in joint_angles:
                # Ï≤ôÏ∂îÍ∞Ä ÎÑàÎ¨¥ ÍµΩÏñ¥ÏûàÏúºÎ©¥ Í∞êÏ†ê
                if joint_angles['spine_curvature'] > 20:
                    specific_score *= 0.8
            
            # Îã§Î¶¨Í∞Ä ÎÑàÎ¨¥ Î≤åÏñ¥Ï†∏ ÏûàÏúºÎ©¥ Í∞êÏ†ê
            if all(keypoints[i][2] > 0.3 for i in [15, 16]):
                foot_distance = abs(keypoints[15][0] - keypoints[16][0])
                hip_width = abs(keypoints[11][0] - keypoints[12][0]) if keypoints[11][2] > 0.3 and keypoints[12][2] > 0.3 else 100
                if hip_width > 0 and (foot_distance / hip_width) > 1.5:
                    specific_score *= 0.9
        
        elif clothing_type == 'pants':
            # Î∞îÏßÄ: Îã§Î¶¨ ÏûêÏÑ∏ÏôÄ Ìûô ÎùºÏù∏Ïù¥ Ï§ëÏöî
            if 'left_knee' in joint_angles and 'right_knee' in joint_angles:
                # Î¨¥Î¶éÏù¥ ÎÑàÎ¨¥ ÍµΩÌòÄÏ†∏ ÏûàÏúºÎ©¥ Í∞êÏ†ê
                avg_knee_angle = (joint_angles['left_knee'] + joint_angles['right_knee']) / 2
                if avg_knee_angle < 150:  # ÎÑàÎ¨¥ ÎßéÏù¥ ÍµΩÌòÄÏßê
                    specific_score *= 0.8
            
            # ÏóâÎç©Ïù¥ ÎùºÏù∏Ïù¥ ÏàòÌèâÏù∏ÏßÄ ÌôïÏù∏
            if keypoints[11][2] > 0.3 and keypoints[12][2] > 0.3:
                hip_tilt = abs(keypoints[11][1] - keypoints[12][1])
                hip_width = abs(keypoints[11][0] - keypoints[12][0])
                if hip_width > 0 and (hip_tilt / hip_width) > 0.1:
                    specific_score *= 0.9
        
        elif clothing_type == 'jacket':
            # Ïû¨ÌÇ∑: Ïñ¥Íπ®ÏôÄ ÌåîÏùò ÏûêÏÑ∏Í∞Ä Îß§Ïö∞ Ï§ëÏöî
            if 'left_shoulder' in joint_angles and 'right_shoulder' in joint_angles:
                # Ïñ¥Íπ® Í∞ÅÎèÑÍ∞Ä ÎÑàÎ¨¥ Í∑πÎã®Ï†ÅÏù¥Î©¥ Í∞êÏ†ê
                for shoulder_angle in [joint_angles['left_shoulder'], joint_angles['right_shoulder']]:
                    if shoulder_angle < 30 or shoulder_angle > 150:
                        specific_score *= 0.8
                        break
        
        return min(1.0, max(0.0, specific_score))
        
    except Exception:
        return 0.5  # Î∂ÑÏÑù Ïã§Ìå® Ïãú Ï§ëÍ∞Ñ Ï†êÏàò

def analyze_pose_for_clothing(
    keypoints: List[List[float]],
    clothing_type: str = "default",
    confidence_threshold: float = 0.5
) -> Dict[str, Any]:
    """ÏùòÎ•òÎ≥Ñ Ìè¨Ï¶à Ï†ÅÌï©ÏÑ± Î∂ÑÏÑù (Í∏∞Î≥∏ Î≤ÑÏ†Ñ)"""
    try:
        if not keypoints:
            return {
                'suitable_for_fitting': False,
                'issues': ["Ìè¨Ï¶àÎ•º Í≤ÄÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§"],
                'recommendations': ["Îçî ÏÑ†Î™ÖÌïú Ïù¥ÎØ∏ÏßÄÎ•º ÏÇ¨Ïö©Ìï¥ Ï£ºÏÑ∏Ïöî"],
                'pose_score': 0.0
            }
        
        # ÏùòÎ•òÎ≥Ñ Í∞ÄÏ§ëÏπò
        clothing_weights = {
            'shirt': {'arms': 0.4, 'torso': 0.4, 'posture': 0.2},
            'dress': {'torso': 0.5, 'arms': 0.2, 'legs': 0.2, 'posture': 0.1},
            'pants': {'legs': 0.6, 'torso': 0.3, 'posture': 0.1},
            'jacket': {'arms': 0.4, 'torso': 0.4, 'shoulders': 0.2},
            'suit': {'torso': 0.3, 'arms': 0.3, 'legs': 0.2, 'posture': 0.2},
            'default': {'torso': 0.4, 'arms': 0.3, 'legs': 0.2, 'visibility': 0.1}
        }
        
        weights = clothing_weights.get(clothing_type, clothing_weights['default'])
        
        # Ïã†Ï≤¥ Î∂ÄÏúÑÎ≥Ñ Ï†êÏàò Í≥ÑÏÇ∞
        def calculate_body_part_score(part_indices: List[int]) -> float:
            visible_count = 0
            total_confidence = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            
            return (visibility_ratio * 0.6 + avg_confidence * 0.4)
        
        # COCO 17 Î∂ÄÏúÑÎ≥Ñ Ïù∏Îç±Ïä§
        body_parts = {
            'torso': [5, 6, 11, 12],  # Ïñ¥Íπ®Îì§, ÏóâÎç©Ïù¥Îì§
            'arms': [5, 6, 7, 8, 9, 10],  # Ïñ¥Íπ®, ÌåîÍøàÏπò, ÏÜêÎ™©
            'legs': [11, 12, 13, 14, 15, 16],  # ÏóâÎç©Ïù¥, Î¨¥Î¶é, Î∞úÎ™©
            'shoulders': [5, 6],  # Ïñ¥Íπ®
            'visibility': list(range(17))  # Ï†ÑÏ≤¥ ÌÇ§Ìè¨Ïù∏Ìä∏
        }
        
        # Í∞Å Î∂ÄÏúÑ Ï†êÏàò Í≥ÑÏÇ∞
        part_scores = {}
        for part_name, indices in body_parts.items():
            part_scores[part_name] = calculate_body_part_score(indices)
        
        # Ï¢ÖÌï© Ï†êÏàò Í≥ÑÏÇ∞
        pose_score = sum(
            part_scores.get(part, 0.0) * weight 
            for part, weight in weights.items()
        )
        
        # Ï†ÅÌï©ÏÑ± ÌåêÎã®
        suitable_for_fitting = pose_score >= 0.7
        
        # Ïù¥Ïäà Î∞è Í∂åÏû•ÏÇ¨Ìï≠
        issues = []
        recommendations = []
        
        if not suitable_for_fitting:
            issues.append(f'{clothing_type} Ï∞©Ïö© ÏãúÎÆ¨Î†àÏù¥ÏÖòÏóê Ï†ÅÌï©ÌïòÏßÄ ÏïäÏùÄ Ìè¨Ï¶àÏûÖÎãàÎã§')
            recommendations.append('Îçî ÏûêÏó∞Ïä§ÎüΩÍ≥† Ï†ïÎ©¥ÏùÑ Ìñ•Ìïú ÏûêÏÑ∏Î°ú Ï¥¨ÏòÅÌï¥ Ï£ºÏÑ∏Ïöî')
            
            if part_scores.get('torso', 0.0) < 0.6:
                issues.append('ÏÉÅÏ≤¥Í∞Ä Ï∂©Î∂ÑÌûà Î≥¥Ïù¥ÏßÄ ÏïäÏäµÎãàÎã§')
                recommendations.append('ÏÉÅÏ≤¥Í∞Ä Î™ÖÌôïÌûà Î≥¥Ïù¥ÎèÑÎ°ù ÏûêÏÑ∏Î•º Ï°∞Ï†ïÌï¥ Ï£ºÏÑ∏Ïöî')
            
            if part_scores.get('arms', 0.0) < 0.6 and clothing_type in ['shirt', 'jacket']:
                issues.append('Ìåî Î∂ÄÏúÑÍ∞Ä Ï∂©Î∂ÑÌûà Î≥¥Ïù¥ÏßÄ ÏïäÏäµÎãàÎã§')
                recommendations.append('ÌåîÏù¥ Î™ÖÌôïÌûà Î≥¥Ïù¥ÎèÑÎ°ù ÏûêÏÑ∏Î•º Ï°∞Ï†ïÌï¥ Ï£ºÏÑ∏Ïöî')
            
            if part_scores.get('legs', 0.0) < 0.6 and clothing_type in ['pants', 'dress']:
                issues.append('Îã§Î¶¨ Î∂ÄÏúÑÍ∞Ä Ï∂©Î∂ÑÌûà Î≥¥Ïù¥ÏßÄ ÏïäÏäµÎãàÎã§')
                recommendations.append('Îã§Î¶¨Í∞Ä Î™ÖÌôïÌûà Î≥¥Ïù¥ÎèÑÎ°ù ÏûêÏÑ∏Î•º Ï°∞Ï†ïÌï¥ Ï£ºÏÑ∏Ïöî')
        
        return {
            'suitable_for_fitting': suitable_for_fitting,
            'issues': issues,
            'recommendations': recommendations,
            'pose_score': pose_score,
            'clothing_type': clothing_type,
            'part_scores': part_scores
        }
        
    except Exception as e:
        logger.error(f"ÏùòÎ•òÎ≥Ñ Ìè¨Ï¶à Î∂ÑÏÑù Ïã§Ìå®: {e}")
        return {
            'suitable_for_fitting': False,
            'issues': ["Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§"],
            'recommendations': ["Îã§Ïãú ÏãúÎèÑÌï¥ Ï£ºÏÑ∏Ïöî"],
            'pose_score': 0.0,
            'error': str(e)
        }

def convert_coco17_to_openpose18(coco_keypoints: List[List[float]]) -> List[List[float]]:
    """COCO 17 ‚Üí OpenPose 18 Î≥ÄÌôò"""
    if len(coco_keypoints) < 17:
        return [[0.0, 0.0, 0.0] for _ in range(18)]
    
    openpose_keypoints = [[0.0, 0.0, 0.0] for _ in range(18)]
    
    # COCO 17 ‚Üí OpenPose 18 Îß§Ìïë
    coco_to_openpose = {
        0: 0,   # nose
        1: 15,  # left_eye ‚Üí right_eye
        2: 16,  # right_eye ‚Üí left_eye
        3: 17,  # left_ear ‚Üí right_ear
        4: 18,  # right_ear ‚Üí left_ear
        5: 5,   # left_shoulder
        6: 2,   # right_shoulder
        7: 6,   # left_elbow
        8: 3,   # right_elbow
        9: 7,   # left_wrist
        10: 4,  # right_wrist
        11: 12, # left_hip
        12: 9,  # right_hip
        13: 13, # left_knee
        14: 10, # right_knee
        15: 14, # left_ankle
        16: 11  # right_ankle
    }
    
    # neck Í≥ÑÏÇ∞ (Ïñ¥Íπ® Ï§ëÏ†ê)
    if len(coco_keypoints) > 6:
        left_shoulder = coco_keypoints[5]
        right_shoulder = coco_keypoints[6]
        if left_shoulder[2] > 0.1 and right_shoulder[2] > 0.1:
            neck_x = (left_shoulder[0] + right_shoulder[0]) / 2
            neck_y = (left_shoulder[1] + right_shoulder[1]) / 2
            neck_conf = (left_shoulder[2] + right_shoulder[2]) / 2
            openpose_keypoints[1] = [float(neck_x), float(neck_y), float(neck_conf)]
    
    # middle_hip Í≥ÑÏÇ∞ (ÏóâÎç©Ïù¥ Ï§ëÏ†ê)
    if len(coco_keypoints) > 12:
        left_hip = coco_keypoints[11]
        right_hip = coco_keypoints[12]
        if left_hip[2] > 0.1 and right_hip[2] > 0.1:
            middle_hip_x = (left_hip[0] + right_hip[0]) / 2
            middle_hip_y = (left_hip[1] + right_hip[1]) / 2
            middle_hip_conf = (left_hip[2] + right_hip[2]) / 2
            openpose_keypoints[8] = [float(middle_hip_x), float(middle_hip_y), float(middle_hip_conf)]
    
    # ÎÇòÎ®∏ÏßÄ ÌÇ§Ìè¨Ïù∏Ìä∏ Îß§Ìïë
    for coco_idx, openpose_idx in coco_to_openpose.items():
        if coco_idx < len(coco_keypoints) and openpose_idx < 18:
            openpose_keypoints[openpose_idx] = [
                float(coco_keypoints[coco_idx][0]),
                float(coco_keypoints[coco_idx][1]),
                float(coco_keypoints[coco_idx][2])
            ]
    
    return openpose_keypoints

# ==============================================
# üî• 7. Step ÏÉùÏÑ± Ìï®ÏàòÎì§
# ==============================================

async def create_pose_estimation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> PoseEstimationStep:
    """Ìè¨Ï¶à Ï∂îÏ†ï Step ÏÉùÏÑ± Ìï®Ïàò"""
    try:
        device_param = None if device == "auto" else device
        
        if config is None:
            config = {}
        config.update(kwargs)
        config['production_ready'] = True
        
        step = PoseEstimationStep(device=device_param, config=config)
        
        initialization_success = await step.initialize()
        
        if not initialization_success:
            raise RuntimeError("Ìè¨Ï¶à Ï∂îÏ†ï Step Ï¥àÍ∏∞Ìôî Ïã§Ìå®")
        
        return step
        
    except Exception as e:
        logger.error(f"‚ùå Ìè¨Ï¶à Ï∂îÏ†ï Step ÏÉùÏÑ± Ïã§Ìå®: {e}")
        raise

def create_pose_estimation_step_sync(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> PoseEstimationStep:
    """ÎèôÍ∏∞Ïãù Ìè¨Ï¶à Ï∂îÏ†ï Step ÏÉùÏÑ±"""
    try:
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            create_pose_estimation_step(device, config, **kwargs)
        )
    except Exception as e:
        logger.error(f"‚ùå ÎèôÍ∏∞Ïãù Ìè¨Ï¶à Ï∂îÏ†ï Step ÏÉùÏÑ± Ïã§Ìå®: {e}")
        raise

# ==============================================
# üî• 8. ÌÖåÏä§Ìä∏ Ìï®ÏàòÎì§
# ==============================================

async def test_pose_estimation():
    """Ìè¨Ï¶à Ï∂îÏ†ï ÌÖåÏä§Ìä∏"""
    try:
        print("üî• Pose Estimation Step ÌÖåÏä§Ìä∏")
        print("=" * 80)
        
        # Step ÏÉùÏÑ±
        step = await create_pose_estimation_step(
            device="auto",
            config={
                'confidence_threshold': 0.5,
                'use_subpixel': True,
                'production_ready': True
            }
        )
        
        # ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄ
        test_image = Image.new('RGB', (512, 512), (128, 128, 128))
        
        print(f"üìã Step Ï†ïÎ≥¥:")
        status = step.get_model_status()
        print(f"   üéØ Step: {status['step_name']}")
        print(f"   üíé Ï§ÄÎπÑ ÏÉÅÌÉú: {status['pose_ready']}")
        print(f"   ü§ñ Î°úÎî©Îêú Î™®Îç∏: {len(status['loaded_models'])}Í∞ú")
        print(f"   üìã Î™®Îç∏ Î™©Î°ù: {', '.join(status['loaded_models'])}")
        
        # Ïã§Ï†ú AI Ï∂îÎ°† ÌÖåÏä§Ìä∏
        result = await step.process(image=test_image)
        
        if result['success']:
            print(f"‚úÖ Ìè¨Ï¶à Ï∂îÏ†ï ÏÑ±Í≥µ")
            print(f"üéØ Í≤ÄÏ∂úÎêú ÌÇ§Ìè¨Ïù∏Ìä∏: {len(result.get('keypoints', []))}")
            print(f"üéñÔ∏è Ìè¨Ï¶à ÌíàÏßà: {result.get('pose_quality', 0):.3f}")
            print(f"üèÜ ÏÇ¨Ïö©Îêú Î™®Îç∏: {result.get('model_used', 'unknown')}")
            print(f"‚ö° Ï∂îÎ°† ÏãúÍ∞Ñ: {result.get('processing_time', 0):.3f}Ï¥à")
            print(f"üîç Ïã§Ï†ú AI Ï∂îÎ°†: {result.get('real_ai_inference', False)}")
        else:
            print(f"‚ùå Ìè¨Ï¶à Ï∂îÏ†ï Ïã§Ìå®: {result.get('error', 'Unknown')}")
        
        await step.cleanup()
        print(f"üßπ Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ ÏôÑÎ£å")
        
    except Exception as e:
        print(f"‚ùå ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")

def test_pose_algorithms():
    """Ìè¨Ï¶à ÏïåÍ≥†Î¶¨Ï¶ò ÌÖåÏä§Ìä∏"""
    try:
        print("üß† Ìè¨Ï¶à ÏïåÍ≥†Î¶¨Ï¶ò ÌÖåÏä§Ìä∏")
        print("=" * 60)
        
        # ÎçîÎØ∏ COCO 17 ÌÇ§Ìè¨Ïù∏Ìä∏
        keypoints = [
            [128, 50, 0.9],   # nose
            [120, 40, 0.8],   # left_eye
            [136, 40, 0.8],   # right_eye
            [115, 45, 0.7],   # left_ear
            [141, 45, 0.7],   # right_ear
            [100, 100, 0.7],  # left_shoulder
            [156, 100, 0.7],  # right_shoulder
            [80, 130, 0.6],   # left_elbow
            [176, 130, 0.6],  # right_elbow
            [60, 160, 0.5],   # left_wrist
            [196, 160, 0.5],  # right_wrist
            [108, 180, 0.7],  # left_hip
            [148, 180, 0.7],  # right_hip
            [98, 220, 0.6],   # left_knee
            [158, 220, 0.6],  # right_knee
            [88, 260, 0.5],   # left_ankle
            [168, 260, 0.5],  # right_ankle
        ]
        
        # Î∂ÑÏÑùÍ∏∞ ÌÖåÏä§Ìä∏
        analyzer = PoseAnalyzer()
        
        # Í¥ÄÏ†à Í∞ÅÎèÑ Í≥ÑÏÇ∞
        joint_angles = analyzer.calculate_joint_angles(keypoints)
        print(f"‚úÖ Í¥ÄÏ†à Í∞ÅÎèÑ Í≥ÑÏÇ∞: {len(joint_angles)}Í∞ú")
        
        # Ïã†Ï≤¥ ÎπÑÏú® Í≥ÑÏÇ∞
        body_proportions = analyzer.calculate_body_proportions(keypoints)
        print(f"‚úÖ Ïã†Ï≤¥ ÎπÑÏú® Í≥ÑÏÇ∞: {len(body_proportions)}Í∞ú")
        
        # Ìè¨Ï¶à ÌíàÏßà ÌèâÍ∞Ä
        quality = analyzer.assess_pose_quality(keypoints, joint_angles, body_proportions)
        print(f"‚úÖ Ìè¨Ï¶à ÌíàÏßà ÌèâÍ∞Ä: {quality['quality_grade'].value}")
        print(f"   Ï†ÑÏ≤¥ Ï†êÏàò: {quality['overall_score']:.3f}")
        
        # ÏùòÎ•ò Ï†ÅÌï©ÏÑ± Î∂ÑÏÑù
        clothing_analysis = analyze_pose_for_clothing(keypoints, "shirt")
        print(f"‚úÖ ÏùòÎ•ò Ï†ÅÌï©ÏÑ±: {clothing_analysis['suitable_for_fitting']}")
        print(f"   Ï†êÏàò: {clothing_analysis['pose_score']:.3f}")
        
        # Ïù¥ÎØ∏ÏßÄ Í∑∏Î¶¨Í∏∞ ÌÖåÏä§Ìä∏
        test_image = Image.new('RGB', (256, 256), (128, 128, 128))
        pose_image = draw_pose_on_image(test_image, keypoints)
        print(f"‚úÖ Ìè¨Ï¶à ÏãúÍ∞ÅÌôî: {pose_image.size}")
        
        # ÌÇ§Ìè¨Ïù∏Ìä∏ Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù
        is_valid = validate_keypoints(keypoints)
        print(f"‚úÖ ÌÇ§Ìè¨Ïù∏Ìä∏ Ïú†Ìö®ÏÑ±: {is_valid}")
        
        # COCO 17 ‚Üí OpenPose 18 Î≥ÄÌôò
        openpose_kpts = convert_coco17_to_openpose18(keypoints)
        print(f"‚úÖ COCO‚ÜíOpenPose Î≥ÄÌôò: {len(openpose_kpts)}Í∞ú")
        
    except Exception as e:
        print(f"‚ùå ÏïåÍ≥†Î¶¨Ï¶ò ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")

# ==============================================
# üî• 9. Î™®Îìà ÏùµÏä§Ìè¨Ìä∏
# ==============================================

__all__ = [
    # Î©îÏù∏ ÌÅ¥ÎûòÏä§Îì§
    'PoseEstimationStep',
    'MediaPoseModel',
    'YOLOv8PoseModel', 
    'OpenPoseModel',
    'HRNetModel',
    'PoseAnalyzer',
    
    # Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞
    'PoseResult',
    'PoseModel',
    'PoseQuality',
    
    # ÏÉùÏÑ± Ìï®ÏàòÎì§
    'create_pose_estimation_step',
    'create_pose_estimation_step_sync',
    
    # Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
    'validate_keypoints',
    'draw_pose_on_image', 
    'analyze_pose_for_clothing',
    'convert_coco17_to_openpose18',
    
    # ÏÉÅÏàòÎì§
    'COCO_17_KEYPOINTS',
    'OPENPOSE_18_KEYPOINTS',
    'SKELETON_CONNECTIONS',
    'KEYPOINT_COLORS',
    
    # ÌÖåÏä§Ìä∏ Ìï®ÏàòÎì§
    'test_pose_estimation',
    'test_pose_algorithms'
]

# ==============================================
# üî• 10. Î™®Îìà Ï¥àÍ∏∞Ìôî Î°úÍ∑∏
# ==============================================

logger.info("üî• Pose Estimation Step v7.0 - Central Hub DI Container ÏôÑÏ†Ñ Î¶¨Ìå©ÌÜ†ÎßÅ ÏôÑÎ£å")
logger.info("‚úÖ Central Hub DI Container v7.0 ÏôÑÏ†Ñ Ïó∞Îèô")
logger.info("‚úÖ BaseStepMixin ÏÉÅÏÜç Ìå®ÌÑ¥ (Human Parsing StepÍ≥º ÎèôÏùº)")
logger.info("‚úÖ MediaPipe Pose Î™®Îç∏ ÏßÄÏõê (Ïö∞ÏÑ†ÏàúÏúÑ 1)")
logger.info("‚úÖ OpenPose Î™®Îç∏ ÏßÄÏõê (Ìè¥Î∞± ÏòµÏÖò)")
logger.info("‚úÖ YOLOv8-Pose Î™®Îç∏ ÏßÄÏõê (Ïã§ÏãúÍ∞Ñ)")
logger.info("‚úÖ HRNet Î™®Îç∏ ÏßÄÏõê (Í≥†Ï†ïÎ∞Ä)")
logger.info("‚úÖ 17Í∞ú COCO keypoints Í∞êÏßÄ")
logger.info("‚úÖ confidence score Í≥ÑÏÇ∞")
logger.info("‚úÖ Mock Î™®Îç∏ ÏôÑÏ†Ñ Ï†úÍ±∞")
logger.info("‚úÖ Ïã§Ï†ú AI Ï∂îÎ°† Ïã§Ìñâ")
logger.info("‚úÖ Îã§Ï§ë Î™®Îç∏ Ìè¥Î∞± ÏãúÏä§ÌÖú")

logger.info("üß† ÏßÄÏõê AI Î™®Îç∏Îì§:")
logger.info("   - MediaPipe Pose (Ïö∞ÏÑ†ÏàúÏúÑ 1, Ïã§ÏãúÍ∞Ñ)")
logger.info("   - YOLOv8-Pose (Ïã§ÏãúÍ∞Ñ, 6.2MB)")
logger.info("   - OpenPose (Ï†ïÎ∞Ä, PAF + ÌûàÌä∏Îßµ)")
logger.info("   - HRNet (Í≥†Ï†ïÎ∞Ä, ÏÑúÎ∏åÌîΩÏÖÄ Ï†ïÌôïÎèÑ)")

logger.info("üéØ ÌïµÏã¨ Í∏∞Îä•Îì§:")
logger.info("   - 17Í∞ú COCO keypoints ÏôÑÏ†Ñ Í≤ÄÏ∂ú")
logger.info("   - Í¥ÄÏ†à Í∞ÅÎèÑ + Ïã†Ï≤¥ ÎπÑÏú® Í≥ÑÏÇ∞")
logger.info("   - Ìè¨Ï¶à ÌíàÏßà ÌèâÍ∞Ä ÏãúÏä§ÌÖú")
logger.info("   - ÏùòÎ•òÎ≥Ñ Ìè¨Ï¶à Ï†ÅÌï©ÏÑ± Î∂ÑÏÑù")
logger.info("   - Ïä§ÏºàÎ†àÌÜ§ Íµ¨Ï°∞ ÏÉùÏÑ±")
logger.info("   - ÏÑúÎ∏åÌîΩÏÖÄ Ï†ïÌôïÎèÑ ÏßÄÏõê")

logger.info(f"üìä ÏãúÏä§ÌÖú: PyTorch={TORCH_AVAILABLE}, Device={DEVICE}")
logger.info(f"ü§ñ AI ÎùºÏù¥Î∏åÎü¨Î¶¨: YOLO={ULTRALYTICS_AVAILABLE}, MediaPipe={MEDIAPIPE_AVAILABLE}")
logger.info(f"üîß ÎùºÏù¥Î∏åÎü¨Î¶¨: OpenCV={OPENCV_AVAILABLE}, Transformers={TRANSFORMERS_AVAILABLE}")
logger.info("üöÄ Production Ready - Central Hub DI Container v7.0!")

# ==============================================
# üî• 11. Î©îÏù∏ Ïã§ÌñâÎ∂Ä
# ==============================================

if __name__ == "__main__":
    print("=" * 80)
    print("üéØ MyCloset AI Step 02 - Pose Estimation")
    print("üî• Central Hub DI Container v7.0 ÏôÑÏ†Ñ Î¶¨Ìå©ÌÜ†ÎßÅ")
    print("=" * 80)
    
    async def run_all_tests():
        await test_pose_estimation()
        print("\n" + "=" * 80)
        test_pose_algorithms()
    
    try:
        asyncio.run(run_all_tests())
    except Exception as e:
        print(f"‚ùå ÌÖåÏä§Ìä∏ Ïã§Ìñâ Ïã§Ìå®: {e}")
    
    print("\n" + "=" * 80)
    print("‚ú® Pose Estimation Step ÌÖåÏä§Ìä∏ ÏôÑÎ£å")
    print("üî• Central Hub DI Container v7.0 ÏôÑÏ†Ñ Ïó∞Îèô")
    print("üß† MediaPipe + YOLOv8 + OpenPose + HRNet ÌÜµÌï©")
    print("üéØ 17Í∞ú COCO keypoints ÏôÑÏ†Ñ Í≤ÄÏ∂ú")
    print("‚ö° Ïã§Ï†ú AI Ï∂îÎ°† + Îã§Ï§ë Î™®Îç∏ Ìè¥Î∞±")
    print("üìä Í¥ÄÏ†à Í∞ÅÎèÑ + Ïã†Ï≤¥ ÎπÑÏú® + Ìè¨Ï¶à ÌíàÏßà ÌèâÍ∞Ä")
    print("üíâ ÏôÑÏ†ÑÌïú ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ìå®ÌÑ¥")
    print("üîí BaseStepMixin v20.0 ÏôÑÏ†Ñ Ìò∏Ìôò")
    print("üöÄ Production Ready!")
    print("=" * 80)