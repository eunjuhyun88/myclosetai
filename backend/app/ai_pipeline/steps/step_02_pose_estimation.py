#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 02: AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • - ì™„ì „í•œ ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ v5.0
==========================================================================================

âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (3.4GB): OpenPose, YOLOv8, Diffusion, Body Pose
âœ… ModelLoader ì™„ì „ ì—°ë™ - ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ â†’ ì‹¤ì œ ì¶”ë¡ 
âœ… BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜ - ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´
âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
âœ… StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… â†’ ì™„ì„±ëœ Step
âœ… SmartModelPathMapper í™œìš©í•œ ë™ì  íŒŒì¼ ê²½ë¡œ íƒì§€
âœ… ì‹¤ì œ AI ì¶”ë¡  ì—”ì§„ êµ¬í˜„ (YOLOv8, OpenPose, Diffusion)
âœ… 18ê°œ í‚¤í¬ì¸íŠ¸ ì™„ì „ ê²€ì¶œ ë° ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±
âœ… M3 Max MPS ê°€ì† ìµœì í™”
âœ… conda í™˜ê²½ ìš°ì„  ì§€ì›

í•µì‹¬ ì•„í‚¤í…ì²˜:
StepFactory â†’ ModelLoader (ì²´í¬í¬ì¸íŠ¸ ë¡œë”©) â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ â†’ ì‹¤ì œ ì¶”ë¡ 

ì²˜ë¦¬ íë¦„:
1. ModelLoaderê°€ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ë“¤ì„ ì²´í¬í¬ì¸íŠ¸ë¡œ ë¡œë”©
2. ì²´í¬í¬ì¸íŠ¸ â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ ë³€í™˜ (RealYOLOv8PoseModel, RealOpenPoseModel ë“±)
3. ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰ â†’ 18ê°œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
4. í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„ â†’ ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„± â†’ API ì‘ë‹µ

ì‹¤ì œ í™œìš© íŒŒì¼ë“¤:
- ai_models/step_02_pose_estimation/yolov8n-pose.pt (6.5MB)
- ai_models/step_02_pose_estimation/openpose.pth (97.8MB)
- ai_models/step_02_pose_estimation/diffusion_pytorch_model.safetensors (1378.2MB)
- ai_models/step_02_pose_estimation/body_pose_model.pth (97.8MB)

íŒŒì¼ ìœ„ì¹˜: backend/app/ai_pipeline/steps/step_02_pose_estimation.py
ì‘ì„±ì: MyCloset AI Team  
ë‚ ì§œ: 2025-07-25
ë²„ì „: v5.0 (Complete Real AI Model Integration)
"""

# ==============================================
# ğŸ”¥ 1. Import ì„¹ì…˜ (TYPE_CHECKING íŒ¨í„´)
# ==============================================

import os
import sys
import gc
import time
import asyncio
import logging
import threading
import traceback
import hashlib
import json
import base64
import weakref
import math
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from contextlib import asynccontextmanager

# TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from ..factories.step_factory import StepFactory
    from ..steps.base_step_mixin import BaseStepMixin

# ==============================================
# ğŸ”¥ 2. conda í™˜ê²½ ë° í•„ìˆ˜ íŒ¨í‚¤ì§€ ì²´í¬
# ==============================================

# conda í™˜ê²½ ì •ë³´
CONDA_INFO = {
    'conda_env': os.environ.get('CONDA_DEFAULT_ENV', 'none'),
    'conda_prefix': os.environ.get('CONDA_PREFIX', 'none'), 
    'python_path': os.path.dirname(os.__file__)
}

# M3 Max ê°ì§€
def detect_m3_max() -> bool:
    """M3 Max ê°ì§€"""
    try:
        import platform
        import subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()

# PyTorch (í•„ìˆ˜)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    import torchvision.transforms as transforms
    from torchvision.transforms.functional import resize, to_pil_image, to_tensor
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
    
    # MPS ì¥ì¹˜ ì„¤ì •
    if IS_M3_MAX and torch.backends.mps.is_available():
        DEVICE = "mps"
        torch.mps.set_per_process_memory_fraction(0.8)  # M3 Max ìµœì í™”
    elif torch.cuda.is_available():
        DEVICE = "cuda"
    else:
        DEVICE = "cpu"
        
except ImportError as e:
    raise ImportError(f"âŒ PyTorch í•„ìˆ˜: conda install pytorch torchvision -c pytorch\nì„¸ë¶€ ì˜¤ë¥˜: {e}")

# PIL (í•„ìˆ˜)
try:
    from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
    PIL_AVAILABLE = True
except ImportError as e:
    raise ImportError(f"âŒ Pillow í•„ìˆ˜: conda install pillow -c conda-forge\nì„¸ë¶€ ì˜¤ë¥˜: {e}")

# NumPy (í•„ìˆ˜)
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError as e:
    raise ImportError(f"âŒ NumPy í•„ìˆ˜: conda install numpy -c conda-forge\nì„¸ë¶€ ì˜¤ë¥˜: {e}")

# AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ì„ íƒì )
try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

try:
    from transformers import pipeline, CLIPProcessor, CLIPModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False

# safetensors (Diffusion ëª¨ë¸ìš©)
try:
    import safetensors.torch as st
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ 3. ë™ì  import í•¨ìˆ˜ë“¤ (TYPE_CHECKING í˜¸í™˜)
# ==============================================

def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError as e:
        logger.error(f"âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_model_loader():
    """ModelLoaderë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.model_loader')
        get_global_loader = getattr(module, 'get_global_model_loader', None)
        if get_global_loader:
            return get_global_loader()
        else:
            ModelLoader = getattr(module, 'ModelLoader', None)
            if ModelLoader:
                return ModelLoader()
        return None
    except ImportError as e:
        logger.error(f"âŒ ModelLoader ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_memory_manager():
    """MemoryManagerë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.utils.memory_manager')
        get_global_manager = getattr(module, 'get_global_memory_manager', None)
        if get_global_manager:
            return get_global_manager()
        return None
    except ImportError as e:
        logger.debug(f"MemoryManager ë™ì  import ì‹¤íŒ¨: {e}")
        return None

def get_step_factory():
    """StepFactoryë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.factories.step_factory')
        get_global_factory = getattr(module, 'get_global_step_factory', None)
        if get_global_factory:
            return get_global_factory()
        return None
    except ImportError as e:
        logger.debug(f"StepFactory ë™ì  import ì‹¤íŒ¨: {e}")
        return None

# BaseStepMixin ë™ì  ë¡œë”©
BaseStepMixin = get_base_step_mixin_class()

if BaseStepMixin is None:
    # í´ë°± í´ë˜ìŠ¤ ì •ì˜ (BaseStepMixin v16.0 í˜¸í™˜)
    class BaseStepMixin:
        def __init__(self, **kwargs):
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'BaseStep')
            self.step_id = kwargs.get('step_id', 0)
            self.device = kwargs.get('device', DEVICE)
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # BaseStepMixin v16.0 í˜¸í™˜ ì†ì„±ë“¤
            self.config = type('StepConfig', (), kwargs)()
            self.dependency_manager = type('DependencyManager', (), {
                'dependency_status': type('DependencyStatus', (), {
                    'model_loader': False,
                    'step_interface': False,
                    'memory_manager': False,
                    'data_converter': False,
                    'di_container': False
                })(),
                'auto_inject_dependencies': lambda: False
            })()
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            self.performance_metrics = {
                'process_count': 0,
                'total_process_time': 0.0,
                'average_process_time': 0.0,
                'error_count': 0,
                'success_count': 0,
                'cache_hits': 0
            }
        
        async def initialize(self):
            self.is_initialized = True
            return True
        
        def set_model_loader(self, model_loader):
            self.model_loader = model_loader
            self.dependency_manager.dependency_status.model_loader = True
        
        def set_memory_manager(self, memory_manager):
            self.memory_manager = memory_manager
            self.dependency_manager.dependency_status.memory_manager = True
        
        def set_data_converter(self, data_converter):
            self.data_converter = data_converter
            self.dependency_manager.dependency_status.data_converter = True
        
        def set_di_container(self, di_container):
            self.di_container = di_container
            self.dependency_manager.dependency_status.di_container = True
        
        async def cleanup(self):
            pass
        
        def get_status(self):
            return {
                'step_name': self.step_name,
                'is_initialized': self.is_initialized,
                'device': self.device,
                'dependencies': {
                    'model_loader': self.dependency_manager.dependency_status.model_loader,
                    'step_interface': self.dependency_manager.dependency_status.step_interface,
                    'memory_manager': self.dependency_manager.dependency_status.memory_manager,
                    'data_converter': self.dependency_manager.dependency_status.data_converter,
                    'di_container': self.dependency_manager.dependency_status.di_container,
                },
                'version': '16.0-compatible'
            }

# ==============================================
# ğŸ”¥ 4. ë°ì´í„° êµ¬ì¡° ë° ìƒìˆ˜ ì •ì˜
# ==============================================

class PoseModel(Enum):
    """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ íƒ€ì…"""
    YOLOV8_POSE = "yolov8_pose"
    OPENPOSE = "openpose"
    DIFFUSION_POSE = "diffusion_pose"
    BODY_POSE = "body_pose"

class PoseQuality(Enum):
    """í¬ì¦ˆ í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"     # 90-100ì 
    GOOD = "good"              # 75-89ì   
    ACCEPTABLE = "acceptable"   # 60-74ì 
    POOR = "poor"              # 40-59ì 
    VERY_POOR = "very_poor"    # 0-39ì 

# OpenPose 18 í‚¤í¬ì¸íŠ¸ ì •ì˜
OPENPOSE_18_KEYPOINTS = [
    "nose", "neck", "right_shoulder", "right_elbow", "right_wrist",
    "left_shoulder", "left_elbow", "left_wrist", "middle_hip", "right_hip", 
    "right_knee", "right_ankle", "left_hip", "left_knee", "left_ankle",
    "right_eye", "left_eye", "right_ear", "left_ear"
]

# í‚¤í¬ì¸íŠ¸ ìƒ‰ìƒ ë° ì—°ê²° ì •ë³´
KEYPOINT_COLORS = [
    (255, 0, 0), (255, 85, 0), (255, 170, 0), (255, 255, 0), (170, 255, 0),
    (85, 255, 0), (0, 255, 0), (0, 255, 85), (0, 255, 170), (0, 255, 255),
    (0, 170, 255), (0, 85, 255), (0, 0, 255), (85, 0, 255), (170, 0, 255),
    (255, 0, 255), (255, 0, 170), (255, 0, 85), (255, 0, 0)
]

SKELETON_CONNECTIONS = [
    (0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7), (1, 8),
    (8, 9), (9, 10), (10, 11), (8, 12), (12, 13), (13, 14), (0, 15),
    (15, 17), (0, 16), (16, 18)
]

@dataclass
class PoseMetrics:
    """í¬ì¦ˆ ì¸¡ì • ë°ì´í„°"""
    keypoints: List[List[float]] = field(default_factory=list)
    confidence_scores: List[float] = field(default_factory=list)
    bbox: Tuple[int, int, int, int] = field(default_factory=lambda: (0, 0, 0, 0))
    pose_quality: PoseQuality = PoseQuality.POOR
    overall_score: float = 0.0
    
    # ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜
    head_score: float = 0.0
    torso_score: float = 0.0  
    arms_score: float = 0.0
    legs_score: float = 0.0
    
    # ê³ ê¸‰ ë¶„ì„ ì ìˆ˜
    symmetry_score: float = 0.0
    visibility_score: float = 0.0
    pose_angles: Dict[str, float] = field(default_factory=dict)
    body_proportions: Dict[str, float] = field(default_factory=dict)
    
    # ì˜ë¥˜ ì°©ìš© ì í•©ì„±
    suitable_for_fitting: bool = False
    issues: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    
    # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„°
    model_used: str = ""
    processing_time: float = 0.0
    image_resolution: Tuple[int, int] = field(default_factory=lambda: (0, 0))
    ai_confidence: float = 0.0

# ==============================================
# ğŸ”¥ 5. SmartModelPathMapper (ì‹¤ì œ íŒŒì¼ íƒì§€)
# ==============================================

class SmartModelPathMapper:
    """ì‹¤ì œ íŒŒì¼ ìœ„ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì°¾ì•„ì„œ ë§¤í•‘í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self, ai_models_root: str = "ai_models"):
        self.ai_models_root = Path(ai_models_root)
        self.model_cache = {}  # ìºì‹œë¡œ ì„±ëŠ¥ ìµœì í™”
        self.logger = logging.getLogger(f"{__name__}.SmartModelPathMapper")
    
    def _search_models(self, model_files: Dict[str, List[str]], search_priority: List[str]) -> Dict[str, Optional[Path]]:
        """ëª¨ë¸ íŒŒì¼ë“¤ì„ ìš°ì„ ìˆœìœ„ ê²½ë¡œì—ì„œ ê²€ìƒ‰"""
        found_paths = {}
        
        for model_name, filenames in model_files.items():
            found_path = None
            for filename in filenames:
                for search_path in search_priority:
                    candidate_path = self.ai_models_root / search_path / filename
                    if candidate_path.exists() and candidate_path.is_file():
                        found_path = candidate_path
                        self.logger.info(f"âœ… {model_name} ëª¨ë¸ ë°œê²¬: {found_path}")
                        break
                if found_path:
                    break
            found_paths[model_name] = found_path
            
            if not found_path:
                self.logger.warning(f"âš ï¸ {model_name} ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {filenames}")
        
        return found_paths

class Step02ModelMapper(SmartModelPathMapper):
    """Step 02 Pose Estimation ì „ìš© ë™ì  ê²½ë¡œ ë§¤í•‘"""
    
    def get_step02_model_paths(self) -> Dict[str, Optional[Path]]:
        """Step 02 ëª¨ë¸ ê²½ë¡œ ìë™ íƒì§€"""
        model_files = {
            "yolov8": ["yolov8n-pose.pt", "yolov8s-pose.pt"],
            "openpose": ["openpose.pth", "body_pose_model.pth"],
            "hrnet": ["hrnet_w48_coco_256x192.pth", "hrnet_w32_coco_256x192.pth", "pose_hrnet_w48_256x192.pth"],
            "diffusion": ["diffusion_pytorch_model.safetensors", "diffusion_pytorch_model.bin"],
            "body_pose": ["body_pose_model.pth"]
        }
        
        search_priority = [
            "step_02_pose_estimation/",
            "step_02_pose_estimation/ultra_models/",
            "step_06_virtual_fitting/ootdiffusion/checkpoints/openpose/",
            "checkpoints/step_02_pose_estimation/",
            "pose_estimation/",
            ""  # ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë„ ê²€ìƒ‰
        ]
        
        return self._search_models(model_files, search_priority)

# ==============================================
# ğŸ”¥ 6. íŒŒì´í”„ë¼ì¸ ë°ì´í„° êµ¬ì¡° (StepInterface í˜¸í™˜)
# ==============================================

@dataclass
class PipelineStepResult:
    """íŒŒì´í”„ë¼ì¸ Step ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""
    step_id: int
    step_name: str
    success: bool
    error: Optional[str] = None
    
    # ë‹¤ìŒ Stepë“¤ë¡œ ì „ë‹¬í•  ë°ì´í„°
    for_step_03: Dict[str, Any] = field(default_factory=dict)
    for_step_04: Dict[str, Any] = field(default_factory=dict)
    for_step_05: Dict[str, Any] = field(default_factory=dict)
    for_step_06: Dict[str, Any] = field(default_factory=dict)
    for_step_07: Dict[str, Any] = field(default_factory=dict)
    for_step_08: Dict[str, Any] = field(default_factory=dict)
    
    # ì´ì „ ë‹¨ê³„ ë°ì´í„° ë³´ì¡´
    previous_data: Dict[str, Any] = field(default_factory=dict)
    original_data: Dict[str, Any] = field(default_factory=dict)
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)
    processing_time: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)

@dataclass 
class PipelineInputData:
    """íŒŒì´í”„ë¼ì¸ ì…ë ¥ ë°ì´í„°"""
    person_image: Union[np.ndarray, Image.Image, str]
    clothing_image: Optional[Union[np.ndarray, Image.Image, str]] = None
    session_id: Optional[str] = None
    config: Dict[str, Any] = field(default_factory=dict)

# StepInterface ë™ì  ë¡œë”©
def get_step_interface_class():
    """StepInterface í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.interface.step_interface')
        return getattr(module, 'StepInterface', None)
    except ImportError as e:
        logger.error(f"âŒ StepInterface ë™ì  import ì‹¤íŒ¨: {e}")
        return None

StepInterface = get_step_interface_class()

if StepInterface is None:
    # í´ë°± StepInterface ì •ì˜
    class StepInterface:
        def __init__(self, step_id: int, step_name: str, config: Dict[str, Any], **kwargs):
            self.step_id = step_id
            self.step_name = step_name
            self.config = config
            self.pipeline_mode = config.get("pipeline_mode", False)
        
        async def process_pipeline(self, input_data: PipelineStepResult) -> PipelineStepResult:
            """íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì²˜ë¦¬ (í´ë°±)"""
            return PipelineStepResult(
                step_id=self.step_id,
                step_name=self.step_name,
                success=False,
                error="StepInterface í´ë°± ëª¨ë“œ"
            )

# ==============================================
# ğŸ”¥ 7. ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ì²´í¬í¬ì¸íŠ¸ ë¡œë”© + ì¶”ë¡ )
# ==============================================

class RealYOLOv8PoseModel:
    """YOLOv8 6.5MB ì‹¤ì‹œê°„ í¬ì¦ˆ ê²€ì¶œ - ì‹¤ì œ AI ì¶”ë¡ """
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealYOLOv8PoseModel")
    
    def load_yolo_checkpoint(self) -> bool:
        """ì‹¤ì œ YOLOv8-Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            if not ULTRALYTICS_AVAILABLE:
                self.logger.error("âŒ ultralytics ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŒ")
                return False
            
            # YOLOv8 í¬ì¦ˆ ëª¨ë¸ ë¡œë”©
            self.model = YOLO(str(self.model_path))
            
            # MPS ë””ë°”ì´ìŠ¤ ì„¤ì • (M3 Max ìµœì í™”)
            if self.device == "mps" and torch.backends.mps.is_available():
                # YOLOv8ì€ ìë™ìœ¼ë¡œ MPS ì‚¬ìš©
                pass
            elif self.device == "cuda":
                self.model.to("cuda")
            
            self.loaded = True
            self.logger.info(f"âœ… YOLOv8-Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def detect_poses_realtime(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ í¬ì¦ˆ ê²€ì¶œ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.loaded:
            raise RuntimeError("YOLOv8 ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            results = self.model(image, verbose=False)
            
            poses = []
            for result in results:
                if hasattr(result, 'keypoints') and result.keypoints is not None:
                    keypoints = result.keypoints.data  # [N, 17, 3] (x, y, confidence)
                    
                    for person_kpts in keypoints:
                        # YOLOv8ì€ COCO 17 í˜•ì‹ì´ë¯€ë¡œ OpenPose 18ë¡œ ë³€í™˜
                        openpose_kpts = self._convert_coco17_to_openpose18(person_kpts.cpu().numpy())
                        
                        pose_data = {
                            "keypoints": openpose_kpts,
                            "bbox": result.boxes.xyxy.cpu().numpy()[0] if result.boxes else None,
                            "confidence": float(result.boxes.conf.mean()) if result.boxes else 0.0
                        }
                        poses.append(pose_data)
            
            processing_time = time.time() - start_time
            
            return {
                "poses": poses,
                "keypoints": poses[0]["keypoints"] if poses else [],
                "num_persons": len(poses),
                "processing_time": processing_time,
                "model_type": "yolov8_pose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "poses": [],
                "keypoints": [],
                "num_persons": 0,
                "processing_time": time.time() - start_time,
                "model_type": "yolov8_pose",
                "success": False,
                "error": str(e)
            }
    
    def _convert_coco17_to_openpose18(self, coco_keypoints: np.ndarray) -> List[List[float]]:
        """COCO 17 í¬ë§·ì„ OpenPose 18ë¡œ ë³€í™˜"""
        # COCO 17 â†’ OpenPose 18 ë§¤í•‘
        coco_to_openpose_mapping = {
            0: 0,   # nose
            5: 2,   # left_shoulder â†’ right_shoulder (ì¢Œìš° ë°˜ì „)
            6: 5,   # right_shoulder â†’ left_shoulder
            7: 3,   # left_elbow â†’ right_elbow
            8: 6,   # right_elbow â†’ left_elbow
            9: 4,   # left_wrist â†’ right_wrist
            10: 7,  # right_wrist â†’ left_wrist
            11: 9,  # left_hip â†’ right_hip
            12: 12, # right_hip â†’ left_hip
            13: 10, # left_knee â†’ right_knee
            14: 13, # right_knee â†’ left_knee
            15: 11, # left_ankle â†’ right_ankle
            16: 14, # right_ankle â†’ left_ankle
            1: 15,  # left_eye â†’ right_eye
            2: 16,  # right_eye â†’ left_eye
            3: 17,  # left_ear â†’ right_ear
            4: 18   # right_ear â†’ left_ear
        }
        
        openpose_keypoints = [[0.0, 0.0, 0.0] for _ in range(19)]  # 18 + neck
        
        # neck ê³„ì‚° (ì–´ê¹¨ ì¤‘ì )
        if len(coco_keypoints) > 6:
            left_shoulder = coco_keypoints[5]
            right_shoulder = coco_keypoints[6]
            if left_shoulder[2] > 0.1 and right_shoulder[2] > 0.1:
                neck_x = (left_shoulder[0] + right_shoulder[0]) / 2
                neck_y = (left_shoulder[1] + right_shoulder[1]) / 2
                neck_conf = (left_shoulder[2] + right_shoulder[2]) / 2
                openpose_keypoints[1] = [float(neck_x), float(neck_y), float(neck_conf)]
        
        # ë‚˜ë¨¸ì§€ í‚¤í¬ì¸íŠ¸ ë§¤í•‘
        for coco_idx, openpose_idx in coco_to_openpose_mapping.items():
            if coco_idx < len(coco_keypoints):
                openpose_keypoints[openpose_idx] = [
                    float(coco_keypoints[coco_idx][0]),
                    float(coco_keypoints[coco_idx][1]),
                    float(coco_keypoints[coco_idx][2])
                ]
        
        return openpose_keypoints[:18]  # OpenPose 18ê°œë§Œ ë°˜í™˜

class RealOpenPoseModel:
    """OpenPose 97.8MB ì •ë°€ í¬ì¦ˆ ê²€ì¶œ - ì‹¤ì œ AI ì¶”ë¡ """
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealOpenPoseModel")
    
    def load_openpose_checkpoint(self) -> bool:
        """ì‹¤ì œ OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            # PyTorchë¡œ ì§ì ‘ ë¡œë”©
            checkpoint = torch.load(str(self.model_path), map_location=self.device, weights_only=True)
            
            # OpenPose ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ìƒì„±
            self.model = self._create_openpose_network()
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ state_dict ë¡œë”©
            if 'state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['state_dict'], strict=False)
            elif 'model' in checkpoint:
                self.model.load_state_dict(checkpoint['model'], strict=False)
            else:
                self.model.load_state_dict(checkpoint, strict=False)
            
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            
            self.logger.info(f"âœ… OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±
            self.model = self._create_simple_pose_model()
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            self.logger.warning("âš ï¸ OpenPose í´ë°± ëª¨ë¸ ì‚¬ìš©")
            return True
    
    def _create_openpose_network(self) -> nn.Module:
        """OpenPose ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ìƒì„±"""
        class OpenPoseNetwork(nn.Module):
            def __init__(self):
                super().__init__()
                # VGG19 ë°±ë³¸
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(),
                    nn.MaxPool2d(2, 2),
                    nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(),
                    nn.MaxPool2d(2, 2),
                    nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(),
                    nn.MaxPool2d(2, 2),
                    nn.Conv2d(256, 512, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(),
                )
                
                # PAF (Part Affinity Fields) ë¸Œëœì¹˜
                self.paf_branch = nn.Sequential(
                    nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(128, 38, 1, 1, 0)  # 19 connections * 2
                )
                
                # í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ ë¸Œëœì¹˜
                self.keypoint_branch = nn.Sequential(
                    nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(128, 19, 1, 1, 0)  # 18 keypoints + background
                )
            
            def forward(self, x):
                features = self.backbone(x)
                paf = self.paf_branch(features)
                keypoints = self.keypoint_branch(features)
                return keypoints, paf
        
        return OpenPoseNetwork()
    
    def _create_simple_pose_model(self) -> nn.Module:
        """ê°„ë‹¨í•œ í¬ì¦ˆ ëª¨ë¸ (í´ë°±ìš©)"""
        class SimplePoseModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.MaxPool2d(3, 2, 1),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),
                    nn.AdaptiveAvgPool2d((1, 1))
                )
                self.keypoint_head = nn.Linear(256, 18 * 3)  # 18 keypoints * (x, y, conf)
            
            def forward(self, x):
                features = self.backbone(x)
                features = features.flatten(1)
                keypoints = self.keypoint_head(features)
                return keypoints.view(-1, 18, 3)
        
        return SimplePoseModel()
    
    def detect_keypoints_precise(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """ì •ë°€ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.loaded:
            raise RuntimeError("OpenPose ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, Image.Image):
                image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
            elif isinstance(image, np.ndarray):
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(self.device) / 255.0
            else:
                image_tensor = image.to(self.device)
            
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                if hasattr(self.model, 'keypoint_head'):  # Simple model
                    output = self.model(image_tensor)
                    keypoints = output[0].cpu().numpy()
                    
                    # í‚¤í¬ì¸íŠ¸ ì •ê·œí™” (ì´ë¯¸ì§€ í¬ê¸° ê¸°ì¤€)
                    h, w = image_tensor.shape[-2:]
                    keypoints_list = []
                    for kp in keypoints:
                        x, y, conf = float(kp[0] * w), float(kp[1] * h), float(torch.sigmoid(torch.tensor(kp[2])))
                        keypoints_list.append([x, y, conf])
                    
                else:  # OpenPose model
                    keypoint_heatmaps, paf = self.model(image_tensor)
                    keypoints_list = self._extract_keypoints_from_heatmaps(keypoint_heatmaps[0])
            
            processing_time = time.time() - start_time
            
            return {
                "keypoints": keypoints_list,
                "processing_time": processing_time,
                "model_type": "openpose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ OpenPose AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "keypoints": [],
                "processing_time": time.time() - start_time,
                "model_type": "openpose",
                "success": False,
                "error": str(e)
            }
    
    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(18):  # 18ê°œ í‚¤í¬ì¸íŠ¸
            heatmap = heatmaps[i].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
            y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            confidence = float(heatmap[y_idx, x_idx])
            
            # ì¢Œí‘œ ì •ê·œí™”
            x = float(x_idx / w * 512)  # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³€í™˜
            y = float(y_idx / h * 512)
            
            keypoints.append([x, y, confidence])
        
        return keypoints

class RealDiffusionPoseModel:
    """Diffusion 1378MB ê³ í’ˆì§ˆ í¬ì¦ˆ ìƒì„± - ì‹¤ì œ AI ì¶”ë¡ """
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealDiffusionPoseModel")
    
    def load_diffusion_checkpoint(self) -> bool:
        """ì‹¤ì œ 1.4GB Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            if SAFETENSORS_AVAILABLE and str(self.model_path).endswith('.safetensors'):
                # safetensors íŒŒì¼ ë¡œë”©
                checkpoint = st.load_file(str(self.model_path))
            else:
                # ì¼ë°˜ PyTorch íŒŒì¼ ë¡œë”©
                checkpoint = torch.load(str(self.model_path), map_location=self.device, weights_only=True)
            
            # Diffusion UNet ë„¤íŠ¸ì›Œí¬ ìƒì„±
            self.model = self._create_diffusion_unet()
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (í‚¤ ë§¤ì¹­)
            model_dict = self.model.state_dict()
            filtered_dict = {}
            
            for k, v in checkpoint.items():
                if k in model_dict and model_dict[k].shape == v.shape:
                    filtered_dict[k] = v
            
            model_dict.update(filtered_dict)
            self.model.load_state_dict(model_dict, strict=False)
            
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            
            self.logger.info(f"âœ… Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {self.model_path} ({len(filtered_dict)} layers)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°„ë‹¨í•œ ëª¨ë¸
            self.model = self._create_simple_diffusion_model()
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            self.logger.warning("âš ï¸ Diffusion í´ë°± ëª¨ë¸ ì‚¬ìš©")
            return True
    
    def _create_diffusion_unet(self) -> nn.Module:
        """Diffusion UNet ë„¤íŠ¸ì›Œí¬ ìƒì„±"""
        class DiffusionUNet(nn.Module):
            def __init__(self):
                super().__init__()
                # ê°„ì†Œí™”ëœ UNet êµ¬ì¡°
                self.encoder = nn.Sequential(
                    nn.Conv2d(3, 64, 3, 1, 1), nn.GroupNorm(8, 64), nn.SiLU(),
                    nn.Conv2d(64, 128, 3, 2, 1), nn.GroupNorm(16, 128), nn.SiLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.GroupNorm(32, 256), nn.SiLU(),
                    nn.Conv2d(256, 512, 3, 2, 1), nn.GroupNorm(32, 512), nn.SiLU(),
                )
                
                self.middle = nn.Sequential(
                    nn.Conv2d(512, 512, 3, 1, 1), nn.GroupNorm(32, 512), nn.SiLU(),
                    nn.Conv2d(512, 512, 3, 1, 1), nn.GroupNorm(32, 512), nn.SiLU(),
                )
                
                self.decoder = nn.Sequential(
                    nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.GroupNorm(32, 256), nn.SiLU(),
                    nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.GroupNorm(16, 128), nn.SiLU(),
                    nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.GroupNorm(8, 64), nn.SiLU(),
                    nn.Conv2d(64, 18 * 3, 3, 1, 1)  # 18 keypoints * 3
                )
            
            def forward(self, x):
                encoded = self.encoder(x)
                middle = self.middle(encoded)
                decoded = self.decoder(middle)
                return decoded
        
        return DiffusionUNet()
    
    def _create_simple_diffusion_model(self) -> nn.Module:
        """ê°„ë‹¨í•œ Diffusion ëª¨ë¸ (í´ë°±ìš©)"""
        return self._create_diffusion_unet()  # ê°™ì€ êµ¬ì¡° ì‚¬ìš©
    
    def enhance_pose_quality(self, keypoints: Union[torch.Tensor, List[List[float]]], image: Union[torch.Tensor, Image.Image] = None) -> Dict[str, Any]:
        """í¬ì¦ˆ í’ˆì§ˆ í–¥ìƒ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.loaded:
            raise RuntimeError("Diffusion ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # í‚¤í¬ì¸íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
            if isinstance(keypoints, list):
                keypoints_tensor = torch.tensor(keypoints, dtype=torch.float32, device=self.device)
            else:
                keypoints_tensor = keypoints.to(self.device)
            
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°)
            if image is None:
                batch_size = 1
                image_tensor = torch.randn(batch_size, 3, 512, 512, device=self.device)
            else:
                if isinstance(image, Image.Image):
                    image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
                else:
                    image_tensor = image.to(self.device)
            
            # ì‹¤ì œ Diffusion AI ì¶”ë¡ 
            with torch.no_grad():
                enhanced_output = self.model(image_tensor)
                
                # ì¶œë ¥ í•´ì„ (18 keypoints * 3 ì±„ë„)
                b, c, h, w = enhanced_output.shape
                enhanced_keypoints = enhanced_output.view(b, 18, 3, h, w)
                
                # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ìµœëŒ€ê°’ ìœ„ì¹˜)
                enhanced_kpts = []
                for i in range(18):
                    for j in range(3):  # x, y, confidence
                        channel = enhanced_keypoints[0, i, j]
                        if j < 2:  # x, y ì¢Œí‘œ
                            max_val = torch.max(channel)
                            enhanced_kpts.append(float(max_val * 512))  # ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì •ê·œí™”
                        else:  # confidence
                            enhanced_kpts.append(float(torch.sigmoid(torch.mean(channel))))
                
                # 18ê°œ í‚¤í¬ì¸íŠ¸ë¡œ ì¬êµ¬ì„±
                result_keypoints = []
                for i in range(18):
                    x = enhanced_kpts[i*3]
                    y = enhanced_kpts[i*3+1]
                    conf = enhanced_kpts[i*3+2]
                    result_keypoints.append([x, y, conf])
            
            processing_time = time.time() - start_time
            
            return {
                "enhanced_keypoints": result_keypoints,
                "processing_time": processing_time,
                "model_type": "diffusion_pose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Diffusion AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            # í´ë°±: ì›ë³¸ í‚¤í¬ì¸íŠ¸ ë°˜í™˜
            if isinstance(keypoints, list):
                original_keypoints = keypoints
            else:
                original_keypoints = keypoints.cpu().numpy().tolist()
            
            return {
                "enhanced_keypoints": original_keypoints,
                "processing_time": time.time() - start_time,
                "model_type": "diffusion_pose",
                "success": False,
                "error": str(e)
            }

class RealBodyPoseModel:
    """Body Pose 97.8MB ë³´ì¡° í¬ì¦ˆ ê²€ì¶œ - ì‹¤ì œ AI ì¶”ë¡ """
    
    def __init__(self, model_path: Path, device: str = "mps"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.RealBodyPoseModel")
    
    def load_body_pose_checkpoint(self) -> bool:
        """ì‹¤ì œ Body Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        try:
            checkpoint = torch.load(str(self.model_path), map_location=self.device, weights_only=True)
            
            # Body Pose ë„¤íŠ¸ì›Œí¬ ìƒì„±
            self.model = self._create_body_pose_network()
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            if 'state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['state_dict'], strict=False)
            else:
                self.model.load_state_dict(checkpoint, strict=False)
            
            self.model.to(self.device)
            self.model.eval()
            self.loaded = True
            
            self.logger.info(f"âœ… Body Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {self.model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Body Pose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _create_body_pose_network(self) -> nn.Module:
        """Body Pose ë„¤íŠ¸ì›Œí¬ ìƒì„±"""
        class BodyPoseNetwork(nn.Module):
            def __init__(self):
                super().__init__()
                # ResNet ìŠ¤íƒ€ì¼ ë°±ë³¸
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.MaxPool2d(3, 2, 1),
                    nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(),
                    nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),
                    nn.Conv2d(256, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(),
                    nn.AdaptiveAvgPool2d((8, 8))
                )
                
                self.pose_head = nn.Sequential(
                    nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(),
                    nn.Conv2d(128, 18, 1, 1, 0),  # 18 keypoints heatmaps
                    nn.AdaptiveAvgPool2d((32, 32))
                )
            
            def forward(self, x):
                features = self.backbone(x)
                heatmaps = self.pose_head(features)
                return heatmaps
        
        return BodyPoseNetwork()
    
    def detect_body_pose(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """ë³´ì¡° í¬ì¦ˆ ê²€ì¶œ (ì‹¤ì œ AI ì¶”ë¡ )"""
        if not self.loaded:
            raise RuntimeError("Body Pose ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, Image.Image):
                image_tensor = to_tensor(image).unsqueeze(0).to(self.device)
            elif isinstance(image, np.ndarray):
                image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(self.device) / 255.0
            else:
                image_tensor = image.to(self.device)
            
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                heatmaps = self.model(image_tensor)
                keypoints = self._extract_keypoints_from_heatmaps(heatmaps[0])
            
            processing_time = time.time() - start_time
            
            return {
                "keypoints": keypoints,
                "processing_time": processing_time,
                "model_type": "body_pose",
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Body Pose AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "keypoints": [],
                "processing_time": time.time() - start_time,
                "model_type": "body_pose",
                "success": False,
                "error": str(e)
            }
    
    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(18):
            heatmap = heatmaps[i].cpu().numpy()
            
            # ìµœëŒ€ê°’ ìœ„ì¹˜ ë° ì‹ ë¢°ë„
            y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            confidence = float(heatmap[y_idx, x_idx])
            
            # ì¢Œí‘œ ì •ê·œí™”
            x = float(x_idx / w * 512)
            y = float(y_idx / h * 512)
            
            keypoints.append([x, y, confidence])
        
        return keypoints

# ==============================================
# ğŸ”¥ 8. ë©”ì¸ PoseEstimationStep í´ë˜ìŠ¤ (ì™„ì „í•œ AI ì—°ë™ + íŒŒì´í”„ë¼ì¸ ì§€ì›)
# ==============================================

class PoseEstimationStep(BaseStepMixin, StepInterface):
    """
    ğŸ”¥ Step 02: AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ - ì™„ì „í•œ ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ + íŒŒì´í”„ë¼ì¸ ì§€ì›
    
    âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (3.4GB): OpenPose, YOLOv8, Diffusion, HRNet
    âœ… ModelLoader ì™„ì „ ì—°ë™ - ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ â†’ ì‹¤ì œ ì¶”ë¡ 
    âœ… BaseStepMixin v16.0 + StepInterface ë‹¤ì¤‘ ìƒì†
    âœ… ì´ì¤‘ ê¸°ëŠ¥ ì§€ì›: ê°œë³„ ì‹¤í–‰ + íŒŒì´í”„ë¼ì¸ ì—°ê²°
    âœ… SmartModelPathMapper í™œìš©í•œ ë™ì  íŒŒì¼ ê²½ë¡œ íƒì§€
    âœ… 18ê°œ í‚¤í¬ì¸íŠ¸ ì™„ì „ ê²€ì¶œ ë° ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±
    âœ… M3 Max MPS ê°€ì† ìµœì í™”
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        strict_mode: bool = True,
        **kwargs
    ):
        """
        ì™„ì „í•œ AI ëª¨ë¸ ì—°ë™ + íŒŒì´í”„ë¼ì¸ ì§€ì› PoseEstimationStep ìƒì„±ì
        
        Args:
            device: ë””ë°”ì´ìŠ¤ ì„¤ì • ('auto', 'mps', 'cuda', 'cpu')
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            strict_mode: ì—„ê²© ëª¨ë“œ
            **kwargs: ì¶”ê°€ ì„¤ì •
        """
        
        # ê¸°ë³¸ ì„¤ì •
        if config is None:
            config = {}
        config.update(kwargs)
        
        # BaseStepMixin í˜¸í™˜ ì„¤ì •
        kwargs.setdefault('step_name', 'PoseEstimationStep')
        kwargs.setdefault('step_id', 2)
        kwargs.setdefault('device', device or DEVICE)
        kwargs.setdefault('strict_mode', strict_mode)
        
        # PoseEstimationStep íŠ¹í™” ì†ì„±ë“¤
        self.step_name = "PoseEstimationStep"
        self.step_number = 2
        self.step_id = 2
        self.step_description = "ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜ ì¸ì²´ í¬ì¦ˆ ì¶”ì • ë° 18ê°œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ"
        self.strict_mode = strict_mode
        self.num_keypoints = 18
        self.keypoint_names = OPENPOSE_18_KEYPOINTS.copy()
        
        # íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì„¤ì •
        self.pipeline_mode = config.get("pipeline_mode", False)
        
        # BaseStepMixin ì´ˆê¸°í™”
        try:
            BaseStepMixin.__init__(self, **kwargs)
            self.logger.info(f"ğŸ¤¸ BaseStepMixin v16.0 í˜¸í™˜ ì´ˆê¸°í™” ì™„ë£Œ - ì‹¤ì œ AI ëª¨ë¸ ì—°ë™")
        except Exception as e:
            self.logger.error(f"âŒ BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if strict_mode:
                raise RuntimeError(f"Strict Mode: BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # í´ë°± ì´ˆê¸°í™”
            self._fallback_initialization(**kwargs)
        
        # StepInterface ì´ˆê¸°í™”
        try:
            StepInterface.__init__(self, step_id=2, step_name="pose_estimation", config=config)
            self.logger.info(f"ğŸ”— StepInterface ì´ˆê¸°í™” ì™„ë£Œ - íŒŒì´í”„ë¼ì¸ ì§€ì›")
        except Exception as e:
            self.logger.error(f"âŒ StepInterface ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if strict_mode:
                raise RuntimeError(f"Strict Mode: StepInterface ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ì‹œìŠ¤í…œ ì„¤ì • ì´ˆê¸°í™”
        self._setup_system_config(config=config, **kwargs)
        
        # SmartModelPathMapper ì´ˆê¸°í™”
        self.model_mapper = Step02ModelMapper()
        
        # ì‹¤ì œ AI ëª¨ë¸ë“¤
        self.ai_models: Dict[str, Any] = {}
        self.model_paths: Dict[str, Optional[Path]] = {}
        self.loaded_models: List[str] = []
        
        # ì²˜ë¦¬ ì„¤ì •
        self.target_input_size = (512, 512)
        self.confidence_threshold = config.get('confidence_threshold', 0.5) if config else 0.5
        self.visualization_enabled = config.get('visualization_enabled', True) if config else True
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.prediction_cache: Dict[str, Any] = {}
        self.cache_max_size = 100 if IS_M3_MAX else 50
        
        # ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ
        self.dependencies_injected = {
            'model_loader': False,
            'step_interface': False,
            'memory_manager': False,
            'data_converter': False,
            'di_container': False
        }
        
        # íŒŒì´í”„ë¼ì¸ ìƒíƒœ
        self.pipeline_position = "middle"  # Step 02ëŠ” ì¤‘ê°„ ë‹¨ê³„
        self.accepts_pipeline_input = True
        self.provides_pipeline_output = True
        
        self.logger.info(f"ğŸ¯ {self.step_name} ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ + íŒŒì´í”„ë¼ì¸ ì§€ì› Step ìƒì„± ì™„ë£Œ")
        self.logger.info(f"ğŸ”— íŒŒì´í”„ë¼ì¸ ëª¨ë“œ: {self.pipeline_mode}")
    
    # ==============================================
    # ğŸ”¥ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ë©”ì„œë“œ (StepInterface êµ¬í˜„)
    # ==============================================
    
    async def process_pipeline(self, input_data: PipelineStepResult) -> PipelineStepResult:
        """
        íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì²˜ë¦¬ - Step 01 ê²°ê³¼ë¥¼ ë°›ì•„ í¬ì¦ˆ ì¶”ì • í›„ Step 03, 04ë¡œ ì „ë‹¬
        
        Args:
            input_data: Step 01ì—ì„œ ì „ë‹¬ë°›ì€ íŒŒì´í”„ë¼ì¸ ë°ì´í„°
            
        Returns:
            PipelineStepResult: Step 03, 04ë¡œ ì „ë‹¬í•  í¬ì¦ˆ ì¶”ì • ê²°ê³¼
        """
        try:
            start_time = time.time()
            self.logger.info(f"ğŸ”— {self.step_name} íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì²˜ë¦¬ ì‹œì‘")
            
            # ì´ˆê¸°í™” ê²€ì¦
            if not self.is_initialized:
                if not await self.initialize():
                    error_msg = "íŒŒì´í”„ë¼ì¸: AI ì´ˆê¸°í™” ì‹¤íŒ¨"
                    return PipelineStepResult(
                        step_id=2, step_name="pose_estimation",
                        success=False, error=error_msg
                    )
            
            # Step 01 ê²°ê³¼ ë°›ê¸°
            if not hasattr(input_data, 'for_step_02') or not input_data.for_step_02:
                error_msg = "Step 01 ë°ì´í„°ê°€ ì—†ìŒ"
                self.logger.error(f"âŒ {error_msg}")
                return PipelineStepResult(
                    step_id=2, step_name="pose_estimation",
                    success=False, error=error_msg
                )
            
            step01_data = input_data.for_step_02
            parsed_image = step01_data.get("parsed_image")
            body_masks = step01_data.get("body_masks", {})
            human_region = step01_data.get("human_region")
            
            if parsed_image is None:
                error_msg = "Step 01ì—ì„œ íŒŒì‹±ëœ ì´ë¯¸ì§€ê°€ ì—†ìŒ"
                self.logger.error(f"âŒ {error_msg}")
                return PipelineStepResult(
                    step_id=2, step_name="pose_estimation",
                    success=False, error=error_msg
                )
            
            # íŒŒì´í”„ë¼ì¸ìš© í¬ì¦ˆ ì¶”ì • AI ì²˜ë¦¬
            pose_result = await self._run_pose_estimation_pipeline_ai(parsed_image, body_masks, human_region)
            
            if not pose_result.get('success', False):
                error_msg = f"íŒŒì´í”„ë¼ì¸ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {pose_result.get('error', 'Unknown Error')}"
                self.logger.error(f"âŒ {error_msg}")
                return PipelineStepResult(
                    step_id=2, step_name="pose_estimation",
                    success=False, error=error_msg
                )
            
            # íŒŒì´í”„ë¼ì¸ ë°ì´í„° ì¤€ë¹„
            pipeline_data = PipelineStepResult(
                step_id=2,
                step_name="pose_estimation",
                success=True,
                
                # Step 03 (Cloth Segmentation)ìœ¼ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_03={
                    **getattr(input_data, 'for_step_03', {}),  # Step 01 ë°ì´í„° ê³„ìŠ¹
                    "pose_keypoints": pose_result["keypoints"],
                    "pose_skeleton": pose_result.get("skeleton_structure", {}),
                    "pose_confidence": pose_result.get("confidence_scores", []),
                    "joint_connections": pose_result.get("joint_connections", []),
                    "visible_keypoints": pose_result.get("visible_keypoints", [])
                },
                
                # Step 04 (Geometric Matching)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_04={
                    "keypoints_for_matching": pose_result["keypoints"],
                    "joint_connections": pose_result.get("joint_connections", []),
                    "pose_angles": pose_result.get("joint_angles", {}),
                    "body_orientation": pose_result.get("body_orientation", {}),
                    "pose_landmarks": pose_result.get("landmarks", {}),
                    "skeleton_structure": pose_result.get("skeleton_structure", {})
                },
                
                # Step 05 (Cloth Warping)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_05={
                    "reference_keypoints": pose_result["keypoints"],
                    "body_proportions": pose_result.get("body_proportions", {}),
                    "pose_type": pose_result.get("pose_type", "standing")
                },
                
                # Step 06 (Virtual Fitting)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_06={
                    "person_keypoints": pose_result["keypoints"],
                    "pose_confidence": pose_result.get("confidence_scores", []),
                    "body_orientation": pose_result.get("body_orientation", {})
                },
                
                # Step 07 (Post Processing)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_07={
                    "original_keypoints": pose_result["keypoints"]
                },
                
                # Step 08 (Quality Assessment)ë¡œ ì „ë‹¬í•  ë°ì´í„°
                for_step_08={
                    "pose_quality_metrics": pose_result.get("pose_analysis", {}),
                    "keypoints_confidence": pose_result.get("confidence_scores", [])
                },
                
                # ì´ì „ ë‹¨ê³„ ë°ì´í„° ë³´ì¡´ ë° í™•ì¥
                previous_data={
                    **getattr(input_data, 'original_data', {}),
                    "step01_results": getattr(input_data, 'for_step_02', {}),
                    "step02_results": pose_result
                },
                
                original_data=getattr(input_data, 'original_data', {}),
                
                # ë©”íƒ€ë°ì´í„°
                metadata={
                    "processing_time": time.time() - start_time,
                    "ai_models_used": pose_result.get("models_used", []),
                    "num_keypoints_detected": len(pose_result.get("keypoints", [])),
                    "ready_for_next_steps": ["step_03", "step_04", "step_05", "step_06"],
                    "execution_mode": "pipeline",
                    "pipeline_progress": "2/8 ë‹¨ê³„ ì™„ë£Œ",
                    "primary_model": pose_result.get("primary_model", "unknown"),
                    "enhanced_by_diffusion": pose_result.get("enhanced_by_diffusion", False)
                },
                
                processing_time=time.time() - start_time
            )
            
            self.logger.info(f"âœ… {self.step_name} íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì²˜ë¦¬ ì™„ë£Œ")
            self.logger.info(f"ğŸ¯ ê²€ì¶œëœ í‚¤í¬ì¸íŠ¸: {len(pose_result.get('keypoints', []))}ê°œ")
            self.logger.info(f"â¡ï¸ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë°ì´í„° ì „ë‹¬ ì¤€ë¹„ ì™„ë£Œ")
            
            return pipeline_data
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìŠ¤íƒ: {traceback.format_exc()}")
            return PipelineStepResult(
                step_id=2, step_name="pose_estimation",
                success=False, error=str(e),
                processing_time=time.time() - start_time if 'start_time' in locals() else 0.0
            )
    
    async def _run_pose_estimation_pipeline_ai(
        self, 
        parsed_image: Union[torch.Tensor, np.ndarray, Image.Image], 
        body_masks: Dict[str, Any],
        human_region: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì „ìš© í¬ì¦ˆ ì¶”ì • AI ì²˜ë¦¬"""
        try:
            inference_start = time.time()
            self.logger.info(f"ğŸ§  íŒŒì´í”„ë¼ì¸ í¬ì¦ˆ ì¶”ì • AI ì‹œì‘...")
            
            if not self.ai_models:
                error_msg = "ë¡œë”©ëœ AI ëª¨ë¸ì´ ì—†ìŒ"
                self.logger.error(f"âŒ {error_msg}")
                return {'success': False, 'error': error_msg}
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (íŒŒì´í”„ë¼ì¸ìš©)
            if isinstance(parsed_image, torch.Tensor):
                image = to_pil_image(parsed_image.cpu())
            elif isinstance(parsed_image, np.ndarray):
                image = Image.fromarray(parsed_image)
            else:
                image = parsed_image
            
            # Body masks í™œìš©í•œ ê´€ì‹¬ ì˜ì—­ ì¶”ì¶œ
            if body_masks and human_region:
                # ì¸ì²´ ì˜ì—­ì— ì§‘ì¤‘í•œ í¬ì¦ˆ ì¶”ì •
                image = self._focus_on_human_region(image, human_region)
            
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
            ai_result = await self._run_real_ai_inference(image, clothing_type=None)
            
            if not ai_result.get('success', False):
                return ai_result
            
            # íŒŒì´í”„ë¼ì¸ ì „ìš© ì¶”ê°€ ë¶„ì„
            pipeline_analysis = self._analyze_for_pipeline(ai_result, body_masks)
            ai_result.update(pipeline_analysis)
            
            inference_time = time.time() - inference_start
            ai_result['inference_time'] = inference_time
            
            self.logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ í¬ì¦ˆ ì¶”ì • AI ì™„ë£Œ ({inference_time:.3f}ì´ˆ)")
            
            return ai_result
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ í¬ì¦ˆ ì¶”ì • AI ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _focus_on_human_region(self, image: Image.Image, human_region: Dict[str, Any]) -> Image.Image:
        """ì¸ì²´ ì˜ì—­ì— ì§‘ì¤‘í•œ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        try:
            if 'bbox' in human_region:
                bbox = human_region['bbox']
                x1, y1, x2, y2 = bbox
                # ì¸ì²´ ì˜ì—­ í¬ë¡­
                cropped = image.crop((x1, y1, x2, y2))
                # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                return cropped.resize(image.size, Image.Resampling.BILINEAR)
            return image
        except Exception as e:
            self.logger.debug(f"ì¸ì²´ ì˜ì—­ ì§‘ì¤‘ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def _analyze_for_pipeline(self, ai_result: Dict[str, Any], body_masks: Dict[str, Any]) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì „ìš© ì¶”ê°€ ë¶„ì„"""
        try:
            keypoints = ai_result.get('keypoints', [])
            
            # ê°€ì‹œì„± ë¶„ì„ (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ í™œìš©)
            visible_keypoints = []
            confidence_threshold = 0.5
            
            for i, kp in enumerate(keypoints):
                if len(kp) >= 3 and kp[2] > confidence_threshold:
                    keypoint_name = OPENPOSE_18_KEYPOINTS[i] if i < len(OPENPOSE_18_KEYPOINTS) else f"kp_{i}"
                    visible_keypoints.append({
                        'index': i,
                        'name': keypoint_name,
                        'position': [kp[0], kp[1]],
                        'confidence': kp[2]
                    })
            
            # í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜ (ë‹¤ìŒ ë‹¨ê³„ ìµœì í™”ìš©)
            pose_type = self._classify_pose_type(keypoints)
            
            # Body masksì™€ì˜ ì¼ì¹˜ì„± ë¶„ì„
            mask_consistency = self._analyze_mask_consistency(keypoints, body_masks)
            
            return {
                'visible_keypoints': visible_keypoints,
                'pose_type': pose_type,
                'mask_consistency': mask_consistency,
                'pipeline_ready': True
            }
            
        except Exception as e:
            self.logger.debug(f"íŒŒì´í”„ë¼ì¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'pipeline_ready': False}
    
    def _classify_pose_type(self, keypoints: List[List[float]]) -> str:
        """í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜"""
        try:
            if not keypoints or len(keypoints) < 18:
                return "unknown"
            
            # íŒ” ê°ë„ ë¶„ì„
            arms_extended = False
            if all(i < len(keypoints) and len(keypoints[i]) >= 3 for i in [2, 3, 4, 5, 6, 7]):
                # íŒ”ì´ í¼ì³ì ¸ ìˆëŠ”ì§€ í™•ì¸
                right_arm_angle = self._calculate_arm_angle(keypoints[2], keypoints[3], keypoints[4])
                left_arm_angle = self._calculate_arm_angle(keypoints[5], keypoints[6], keypoints[7])
                
                if right_arm_angle > 150 and left_arm_angle > 150:
                    arms_extended = True
            
            # ë‹¤ë¦¬ ë¶„ì„
            legs_apart = False
            if all(i < len(keypoints) and len(keypoints[i]) >= 3 for i in [9, 12, 11, 14]):
                hip_distance = abs(keypoints[9][0] - keypoints[12][0])
                ankle_distance = abs(keypoints[11][0] - keypoints[14][0])
                if ankle_distance > hip_distance * 1.5:
                    legs_apart = True
            
            # í¬ì¦ˆ ë¶„ë¥˜
            if arms_extended and not legs_apart:
                return "t_pose"
            elif arms_extended and legs_apart:
                return "star_pose" 
            elif not arms_extended and not legs_apart:
                return "standing"
            else:
                return "dynamic"
                
        except Exception as e:
            self.logger.debug(f"í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return "unknown"
    
    def _calculate_arm_angle(self, shoulder: List[float], elbow: List[float], wrist: List[float]) -> float:
        """íŒ” ê°ë„ ê³„ì‚°"""
        try:
            if all(len(kp) >= 3 and kp[2] > 0.3 for kp in [shoulder, elbow, wrist]):
                v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]])
                v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]])
                
                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
                cos_angle = np.clip(cos_angle, -1.0, 1.0)
                angle = np.arccos(cos_angle)
                
                return np.degrees(angle)
            return 0.0
        except:
            return 0.0
    
    def _analyze_mask_consistency(self, keypoints: List[List[float]], body_masks: Dict[str, Any]) -> Dict[str, Any]:
        """Body masksì™€ í‚¤í¬ì¸íŠ¸ ì¼ì¹˜ì„± ë¶„ì„"""
        try:
            consistency = {
                'overall_score': 0.0,
                'detailed_scores': {},
                'issues': []
            }
            
            # ê°„ë‹¨í•œ ì¼ì¹˜ì„± ë¶„ì„ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
            visible_keypoints = sum(1 for kp in keypoints if len(kp) >= 3 and kp[2] > 0.5)
            total_keypoints = len(keypoints)
            
            if total_keypoints > 0:
                consistency['overall_score'] = visible_keypoints / total_keypoints
            
            if consistency['overall_score'] < 0.6:
                consistency['issues'].append("í‚¤í¬ì¸íŠ¸ì™€ ë§ˆìŠ¤í¬ ë¶ˆì¼ì¹˜")
            
            return consistency
            
        except Exception as e:
            self.logger.debug(f"ë§ˆìŠ¤í¬ ì¼ì¹˜ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'overall_score': 0.0, 'issues': ['ë¶„ì„ ì‹¤íŒ¨']}
    
    # ==============================================
    # ğŸ”¥ ê°œë³„ ì²˜ë¦¬ ë©”ì„œë“œ (ê¸°ì¡´ process ë©”ì„œë“œ ìœ ì§€)
    # ==============================================
    """
    ğŸ”¥ Step 02: AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ - ì™„ì „í•œ ì‹¤ì œ AI ëª¨ë¸ ì—°ë™
    
    âœ… ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (3.4GB): OpenPose, YOLOv8, Diffusion
    âœ… ModelLoader ì™„ì „ ì—°ë™ - ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ â†’ ì‹¤ì œ ì¶”ë¡ 
    âœ… BaseStepMixin v16.0 ì™„ì „ í˜¸í™˜ - ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´
    âœ… SmartModelPathMapper í™œìš©í•œ ë™ì  íŒŒì¼ ê²½ë¡œ íƒì§€
    âœ… 18ê°œ í‚¤í¬ì¸íŠ¸ ì™„ì „ ê²€ì¶œ ë° ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±
    âœ… M3 Max MPS ê°€ì† ìµœì í™”
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        strict_mode: bool = True,
        **kwargs
    ):
        """
        ì™„ì „í•œ AI ëª¨ë¸ ì—°ë™ PoseEstimationStep ìƒì„±ì
        
        Args:
            device: ë””ë°”ì´ìŠ¤ ì„¤ì • ('auto', 'mps', 'cuda', 'cpu')
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            strict_mode: ì—„ê²© ëª¨ë“œ
            **kwargs: ì¶”ê°€ ì„¤ì •
        """
        
        # BaseStepMixin í˜¸í™˜ ì„¤ì •
        kwargs.setdefault('step_name', 'PoseEstimationStep')
        kwargs.setdefault('step_id', 2)
        kwargs.setdefault('device', device or DEVICE)
        kwargs.setdefault('strict_mode', strict_mode)
        
        # PoseEstimationStep íŠ¹í™” ì†ì„±ë“¤
        self.step_name = "PoseEstimationStep"
        self.step_number = 2
        self.step_description = "ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜ ì¸ì²´ í¬ì¦ˆ ì¶”ì • ë° 18ê°œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ"
        self.strict_mode = strict_mode
        self.num_keypoints = 18
        self.keypoint_names = OPENPOSE_18_KEYPOINTS.copy()
        
        # BaseStepMixin ì´ˆê¸°í™”
        try:
            super(PoseEstimationStep, self).__init__(**kwargs)
            self.logger.info(f"ğŸ¤¸ BaseStepMixin v16.0 í˜¸í™˜ ì´ˆê¸°í™” ì™„ë£Œ - ì‹¤ì œ AI ëª¨ë¸ ì—°ë™")
        except Exception as e:
            self.logger.error(f"âŒ BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if strict_mode:
                raise RuntimeError(f"Strict Mode: BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # í´ë°± ì´ˆê¸°í™”
            self._fallback_initialization(**kwargs)
        
        # ì‹œìŠ¤í…œ ì„¤ì • ì´ˆê¸°í™”
        self._setup_system_config(config=config, **kwargs)
        
        # SmartModelPathMapper ì´ˆê¸°í™”
        self.model_mapper = Step02ModelMapper()
        
        # ì‹¤ì œ AI ëª¨ë¸ë“¤
        self.ai_models: Dict[str, Any] = {}
        self.model_paths: Dict[str, Optional[Path]] = {}
        self.loaded_models: List[str] = []
        
        # ì²˜ë¦¬ ì„¤ì •
        self.target_input_size = (512, 512)
        self.confidence_threshold = config.get('confidence_threshold', 0.5) if config else 0.5
        self.visualization_enabled = config.get('visualization_enabled', True) if config else True
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.prediction_cache: Dict[str, Any] = {}
        self.cache_max_size = 100 if IS_M3_MAX else 50
        
        # ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ
        self.dependencies_injected = {
            'model_loader': False,
            'step_interface': False,
            'memory_manager': False,
            'data_converter': False,
            'di_container': False
        }
        
        self.logger.info(f"ğŸ¯ {self.step_name} ì‹¤ì œ AI ëª¨ë¸ ì—°ë™ Step ìƒì„± ì™„ë£Œ (Strict Mode: {self.strict_mode})")
    
    def _fallback_initialization(self, **kwargs):
        """í´ë°± ì´ˆê¸°í™”"""
        self.device = kwargs.get('device', DEVICE)
        self.config = type('StepConfig', (), kwargs)()
        self.is_initialized = False
        self.is_ready = False
        self.has_model = False
        self.model_loaded = False
        self.warmup_completed = False
        
        # BaseStepMixin í˜¸í™˜ ì†ì„±ë“¤
        self.dependency_manager = type('DependencyManager', (), {
            'dependency_status': type('DependencyStatus', (), {
                'model_loader': False,
                'step_interface': False,
                'memory_manager': False,
                'data_converter': False,
                'di_container': False
            })(),
            'auto_inject_dependencies': lambda: self._manual_auto_inject()
        })()
        
        self.performance_metrics = {
            'process_count': 0,
            'total_process_time': 0.0,
            'average_process_time': 0.0,
            'error_count': 0,
            'success_count': 0,
            'cache_hits': 0
        }
        
        self.logger.info("âœ… í´ë°± ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_system_config(self, config: Optional[Dict[str, Any]], **kwargs):
        """ì‹œìŠ¤í…œ ì„¤ì • ì´ˆê¸°í™”"""
        try:
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            device = kwargs.get('device')
            if device is None or device == "auto":
                self.device = DEVICE
            else:
                self.device = device
            
            # ì„¤ì • í†µí•©
            if config is None:
                config = {}
            config.update(kwargs)
            
            # ê¸°ë³¸ ì„¤ì • ì ìš©
            default_config = {
                'confidence_threshold': 0.5,
                'visualization_enabled': True,
                'return_analysis': True,
                'cache_enabled': True,
                'detailed_analysis': True,
                'strict_mode': self.strict_mode,
                'real_ai_models': True
            }
            
            for key, default_value in default_config.items():
                if key not in config:
                    config[key] = default_value
            
            # config ê°ì²´ ì„¤ì •
            if hasattr(self, 'config') and hasattr(self.config, '__dict__'):
                self.config.__dict__.update(config)
            else:
                self.config = type('StepConfig', (), config)()
            
            self.logger.info(f"ğŸ”§ ì‹¤ì œ AI ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ: {self.device}, M3 Max: {IS_M3_MAX}")
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œìŠ¤í…œ ì„¤ì • ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise RuntimeError(f"Strict Mode: ì‹œìŠ¤í…œ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _manual_auto_inject(self) -> bool:
        """ìˆ˜ë™ ìë™ ì˜ì¡´ì„± ì£¼ì…"""
        try:
            injection_count = 0
            
            # ModelLoader ìë™ ì£¼ì…
            model_loader = get_model_loader()
            if model_loader:
                self.set_model_loader(model_loader)
                injection_count += 1
                self.logger.debug("âœ… ModelLoader ìˆ˜ë™ ìë™ ì£¼ì… ì™„ë£Œ")
            
            # MemoryManager ìë™ ì£¼ì…
            memory_manager = get_memory_manager()
            if memory_manager:
                self.set_memory_manager(memory_manager)
                injection_count += 1
                self.logger.debug("âœ… MemoryManager ìˆ˜ë™ ìë™ ì£¼ì… ì™„ë£Œ")
            
            if injection_count > 0:
                self.logger.info(f"ğŸ‰ ìˆ˜ë™ ìë™ ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ: {injection_count}ê°œ")
                return True
                
            return False
        except Exception as e:
            self.logger.debug(f"ìˆ˜ë™ ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            return False
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin v16.0 í˜¸í™˜ ì˜ì¡´ì„± ì£¼ì… ë©”ì„œë“œë“¤
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (v16.0 í˜¸í™˜)"""
        try:
            self.model_loader = model_loader
            self.model_interface = model_loader
            self.dependencies_injected['model_loader'] = True
            self.has_model = True
            self.model_loaded = True
            
            # v16.0 dependency_manager í˜¸í™˜
            if hasattr(self, 'dependency_manager') and hasattr(self.dependency_manager, 'dependency_status'):
                self.dependency_manager.dependency_status.model_loader = True
                self.dependency_manager.dependency_status.step_interface = True
            
            self.logger.info("âœ… ModelLoader ì„¤ì • ì™„ë£Œ (v16.0 í˜¸í™˜)")
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì„¤ì • ì‹¤íŒ¨: {e}")
            self.dependencies_injected['model_loader'] = False
    
    def set_memory_manager(self, memory_manager):
        """MemoryManager ì„¤ì • (v16.0 í˜¸í™˜)"""
        try:
            self.memory_manager = memory_manager
            self.dependencies_injected['memory_manager'] = True
            
            # v16.0 dependency_manager í˜¸í™˜
            if hasattr(self, 'dependency_manager') and hasattr(self.dependency_manager, 'dependency_status'):
                self.dependency_manager.dependency_status.memory_manager = True
            
            self.logger.debug("âœ… MemoryManager ì„¤ì • ì™„ë£Œ (v16.0 í˜¸í™˜)")
        except Exception as e:
            self.logger.error(f"âŒ MemoryManager ì„¤ì • ì‹¤íŒ¨: {e}")
            self.dependencies_injected['memory_manager'] = False
    
    def set_data_converter(self, data_converter):
        """DataConverter ì„¤ì • (v16.0 í˜¸í™˜)"""
        try:
            self.data_converter = data_converter
            self.dependencies_injected['data_converter'] = True
            
            # v16.0 dependency_manager í˜¸í™˜
            if hasattr(self, 'dependency_manager') and hasattr(self.dependency_manager, 'dependency_status'):
                self.dependency_manager.dependency_status.data_converter = True
            
            self.logger.debug("âœ… DataConverter ì„¤ì • ì™„ë£Œ (v16.0 í˜¸í™˜)")
        except Exception as e:
            self.logger.error(f"âŒ DataConverter ì„¤ì • ì‹¤íŒ¨: {e}")
            self.dependencies_injected['data_converter'] = False
    
    def set_di_container(self, di_container):
        """DIContainer ì„¤ì • (v16.0 í˜¸í™˜)"""
        try:
            self.di_container = di_container
            self.dependencies_injected['di_container'] = True
            
            # v16.0 dependency_manager í˜¸í™˜
            if hasattr(self, 'dependency_manager') and hasattr(self.dependency_manager, 'dependency_status'):
                self.dependency_manager.dependency_status.di_container = True
            
            self.logger.debug("âœ… DIContainer ì„¤ì • ì™„ë£Œ (v16.0 í˜¸í™˜)")
        except Exception as e:
            self.logger.error(f"âŒ DIContainer ì„¤ì • ì‹¤íŒ¨: {e}")
            self.dependencies_injected['di_container'] = False
    
    # process ë©”ì„œë“œëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ (ê°œë³„ ì‹¤í–‰ìš©)
    
    # ==============================================
    # ğŸ”¥ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© (ModelLoader ì—°ë™) - HRNet ì¶”ê°€
    # ==============================================
    
    async def _load_real_ai_models(self) -> bool:
        """ì‹¤ì œ 3.4GB AI ëª¨ë¸ë“¤ ë¡œë”© (ModelLoader ì—°ë™) - HRNet í¬í•¨"""
        try:
            self.logger.info("ğŸ”„ ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ë“¤ ë¡œë”© ì‹œì‘...")
            
            # 1. SmartModelPathMapperë¡œ ì‹¤ì œ íŒŒì¼ ê²½ë¡œ íƒì§€
            self.model_paths = self.model_mapper.get_step02_model_paths()
            
            if not any(self.model_paths.values()):
                error_msg = "ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
                self.logger.error(f"âŒ {error_msg}")
                if self.strict_mode:
                    raise FileNotFoundError(f"Strict Mode: {error_msg}")
                return False
            
            # 2. ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë° AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„±
            success_count = 0
            
            # YOLOv8-Pose ëª¨ë¸ ë¡œë”© (6.5MB - ì‹¤ì‹œê°„)
            if self.model_paths.get("yolov8"):
                try:
                    yolo_model = RealYOLOv8PoseModel(self.model_paths["yolov8"], self.device)
                    if yolo_model.load_yolo_checkpoint():
                        self.ai_models["yolov8"] = yolo_model
                        self.loaded_models.append("yolov8")
                        success_count += 1
                        self.logger.info("âœ… YOLOv8-Pose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ YOLOv8-Pose ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # OpenPose ëª¨ë¸ ë¡œë”© (97.8MB - ì •ë°€)
            if self.model_paths.get("openpose"):
                try:
                    openpose_model = RealOpenPoseModel(self.model_paths["openpose"], self.device)
                    if openpose_model.load_openpose_checkpoint():
                        self.ai_models["openpose"] = openpose_model
                        self.loaded_models.append("openpose")
                        success_count += 1
                        self.logger.info("âœ… OpenPose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ OpenPose ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # HRNet ëª¨ë¸ ë¡œë”© (ê³ ì •ë°€ - ìƒˆë¡œ ì¶”ê°€)
            if self.model_paths.get("hrnet"):
                try:
                    hrnet_model = RealHRNetModel(self.model_paths["hrnet"], self.device)
                    if hrnet_model.load_hrnet_checkpoint():
                        self.ai_models["hrnet"] = hrnet_model
                        self.loaded_models.append("hrnet")
                        success_count += 1
                        self.logger.info("âœ… HRNet ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ HRNet ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # Diffusion Pose ëª¨ë¸ ë¡œë”© (1378MB - ëŒ€í˜• ê³ í’ˆì§ˆ)
            if self.model_paths.get("diffusion"):
                try:
                    diffusion_model = RealDiffusionPoseModel(self.model_paths["diffusion"], self.device)
                    if diffusion_model.load_diffusion_checkpoint():
                        self.ai_models["diffusion"] = diffusion_model
                        self.loaded_models.append("diffusion")
                        success_count += 1
                        self.logger.info("âœ… Diffusion Pose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Diffusion Pose ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # Body Pose ëª¨ë¸ ë¡œë”© (97.8MB - ë³´ì¡°)
            if self.model_paths.get("body_pose"):
                try:
                    body_pose_model = RealBodyPoseModel(self.model_paths["body_pose"], self.device)
                    if body_pose_model.load_body_pose_checkpoint():
                        self.ai_models["body_pose"] = body_pose_model
                        self.loaded_models.append("body_pose")
                        success_count += 1
                        self.logger.info("âœ… Body Pose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Body Pose ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # 3. ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ë™ (ìˆëŠ” ê²½ìš°)
            if hasattr(self, 'model_loader') and self.model_loader:
                try:
                    # ModelLoaderì— AI ëª¨ë¸ë“¤ ë“±ë¡
                    for model_name, model_instance in self.ai_models.items():
                        if hasattr(self.model_loader, 'register_model'):
                            self.model_loader.register_model(f"step_02_{model_name}", model_instance)
                    
                    self.logger.info("âœ… ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ë™ ì™„ë£Œ")
                except Exception as e:
                    self.logger.debug(f"ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ë™ ì‹¤íŒ¨: {e}")
            
            if success_count > 0:
                self.logger.info(f"ğŸ‰ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {success_count}ê°œ ({self.loaded_models})")
                return True
            else:
                error_msg = "ëª¨ë“  AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"
                self.logger.error(f"âŒ {error_msg}")
                if self.strict_mode:
                    raise RuntimeError(f"Strict Mode: {error_msg}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise
            return False
    
    # ==============================================
    # ğŸ”¥ SmartModelPathMapper ì—…ë°ì´íŠ¸ (HRNet ì¶”ê°€)
    # ==============================================
    
    async def _load_real_ai_models(self) -> bool:
        """ì‹¤ì œ 3.4GB AI ëª¨ë¸ë“¤ ë¡œë”© (ModelLoader ì—°ë™)"""
        try:
            self.logger.info("ğŸ”„ ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ë“¤ ë¡œë”© ì‹œì‘...")
            
            # 1. SmartModelPathMapperë¡œ ì‹¤ì œ íŒŒì¼ ê²½ë¡œ íƒì§€
            self.model_paths = self.model_mapper.get_step02_model_paths()
            
            if not any(self.model_paths.values()):
                error_msg = "ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
                self.logger.error(f"âŒ {error_msg}")
                if self.strict_mode:
                    raise FileNotFoundError(f"Strict Mode: {error_msg}")
                return False
            
            # 2. ModelLoaderë¥¼ í†µí•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë° AI ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„±
            success_count = 0
            
            # YOLOv8-Pose ëª¨ë¸ ë¡œë”© (6.5MB - ì‹¤ì‹œê°„)
            if self.model_paths.get("yolov8"):
                try:
                    yolo_model = RealYOLOv8PoseModel(self.model_paths["yolov8"], self.device)
                    if yolo_model.load_yolo_checkpoint():
                        self.ai_models["yolov8"] = yolo_model
                        self.loaded_models.append("yolov8")
                        success_count += 1
                        self.logger.info("âœ… YOLOv8-Pose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ YOLOv8-Pose ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # OpenPose ëª¨ë¸ ë¡œë”© (97.8MB - ì •ë°€)
            if self.model_paths.get("openpose"):
                try:
                    openpose_model = RealOpenPoseModel(self.model_paths["openpose"], self.device)
                    if openpose_model.load_openpose_checkpoint():
                        self.ai_models["openpose"] = openpose_model
                        self.loaded_models.append("openpose")
                        success_count += 1
                        self.logger.info("âœ… OpenPose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ OpenPose ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # Diffusion Pose ëª¨ë¸ ë¡œë”© (1378MB - ëŒ€í˜• ê³ í’ˆì§ˆ)
            if self.model_paths.get("diffusion"):
                try:
                    diffusion_model = RealDiffusionPoseModel(self.model_paths["diffusion"], self.device)
                    if diffusion_model.load_diffusion_checkpoint():
                        self.ai_models["diffusion"] = diffusion_model
                        self.loaded_models.append("diffusion")
                        success_count += 1
                        self.logger.info("âœ… Diffusion Pose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Diffusion Pose ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # Body Pose ëª¨ë¸ ë¡œë”© (97.8MB - ë³´ì¡°)
            if self.model_paths.get("body_pose"):
                try:
                    body_pose_model = RealBodyPoseModel(self.model_paths["body_pose"], self.device)
                    if body_pose_model.load_body_pose_checkpoint():
                        self.ai_models["body_pose"] = body_pose_model
                        self.loaded_models.append("body_pose")
                        success_count += 1
                        self.logger.info("âœ… Body Pose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Body Pose ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # 3. ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ë™ (ìˆëŠ” ê²½ìš°)
            if hasattr(self, 'model_loader') and self.model_loader:
                try:
                    # ModelLoaderì— AI ëª¨ë¸ë“¤ ë“±ë¡
                    for model_name, model_instance in self.ai_models.items():
                        if hasattr(self.model_loader, 'register_model'):
                            self.model_loader.register_model(f"step_02_{model_name}", model_instance)
                    
                    self.logger.info("âœ… ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ë™ ì™„ë£Œ")
                except Exception as e:
                    self.logger.debug(f"ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ë™ ì‹¤íŒ¨: {e}")
            
            if success_count > 0:
                self.logger.info(f"ğŸ‰ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {success_count}ê°œ ({self.loaded_models})")
                return True
            else:
                error_msg = "ëª¨ë“  AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"
                self.logger.error(f"âŒ {error_msg}")
                if self.strict_mode:
                    raise RuntimeError(f"Strict Mode: {error_msg}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise
            return False
    
    # ==============================================
    # ğŸ”¥ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ - ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
    # ==============================================
    
    async def process(
        self, 
        image: Union[np.ndarray, Image.Image, str],
        clothing_type: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
            clothing_type: ì˜ë¥˜ íƒ€ì… (ì„ íƒì )
            **kwargs: ì¶”ê°€ ì„¤ì •
            
        Returns:
            Dict[str, Any]: ì™„ì „í•œ ì‹¤ì œ AI í¬ì¦ˆ ì¶”ì • ê²°ê³¼
        """
        try:
            # ì´ˆê¸°í™” ê²€ì¦
            if not self.is_initialized:
                if not await self.initialize():
                    error_msg = "ì‹¤ì œ AI ì´ˆê¸°í™” ì‹¤íŒ¨"
                    if self.strict_mode:
                        raise RuntimeError(f"Strict Mode: {error_msg}")
                    return self._create_error_result(error_msg)
            
            start_time = time.time()
            self.logger.info(f"ğŸ§  {self.step_name} ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜ ì²˜ë¦¬ ì‹œì‘")
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            processed_image = self._preprocess_image(image)
            if processed_image is None:
                error_msg = "ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨"
                if self.strict_mode:
                    raise ValueError(f"Strict Mode: {error_msg}")
                return self._create_error_result(error_msg)
            
            # ìºì‹œ í™•ì¸
            cache_key = None
            if getattr(self.config, 'cache_enabled', True):
                cache_key = self._generate_cache_key(processed_image, clothing_type)
                if cache_key in self.prediction_cache:
                    self.logger.info("ğŸ“‹ ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜")
                    self.performance_metrics['cache_hits'] += 1
                    return self.prediction_cache[cache_key]
            
            # ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
            pose_result = await self._run_real_ai_inference(processed_image, clothing_type, **kwargs)
            
            if not pose_result or not pose_result.get('success', False):
                error_msg = f"ì‹¤ì œ AI í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {pose_result.get('error', 'Unknown AI Error') if pose_result else 'No Result'}"
                self.logger.error(f"âŒ {error_msg}")
                self.performance_metrics['error_count'] += 1
                if self.strict_mode:
                    raise RuntimeError(f"Strict Mode: {error_msg}")
                return self._create_error_result(error_msg)
            
            # ì™„ì „í•œ ê²°ê³¼ í›„ì²˜ë¦¬
            final_result = self._postprocess_ai_result(pose_result, processed_image, start_time)
            
            # ìºì‹œ ì €ì¥
            if getattr(self.config, 'cache_enabled', True) and cache_key:
                self._save_to_cache(cache_key, final_result)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self.performance_metrics['process_count'] += 1
            self.performance_metrics['success_count'] += 1
            self.performance_metrics['total_process_time'] += processing_time
            self.performance_metrics['average_process_time'] = (
                self.performance_metrics['total_process_time'] / self.performance_metrics['process_count']
            )
            
            self.logger.info(f"âœ… {self.step_name} ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜ ì²˜ë¦¬ ì„±ê³µ ({processing_time:.2f}ì´ˆ)")
            self.logger.info(f"ğŸ¯ ê²€ì¶œëœ í‚¤í¬ì¸íŠ¸ ìˆ˜: {len(final_result.get('keypoints', []))}")
            self.logger.info(f"ğŸ–ï¸ AI ì‹ ë¢°ë„: {final_result.get('pose_analysis', {}).get('ai_confidence', 0):.3f}")
            self.logger.info(f"ğŸ¤– ì‚¬ìš©ëœ AI ëª¨ë¸ë“¤: {final_result.get('models_used', [])}")
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.logger.error(f"ğŸ“‹ ì˜¤ë¥˜ ìŠ¤íƒ: {traceback.format_exc()}")
            self.performance_metrics['error_count'] += 1
            if self.strict_mode:
                raise
            return self._create_error_result(str(e))
    
    async def _run_real_ai_inference(
        self, 
        image: Image.Image, 
        clothing_type: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """ì‹¤ì œ AI ëª¨ë¸ë“¤ì„ í†µí•œ í¬ì¦ˆ ì¶”ì • ì¶”ë¡ """
        try:
            inference_start = time.time()
            self.logger.info(f"ğŸ§  ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹œì‘...")
            
            if not self.ai_models:
                error_msg = "ë¡œë”©ëœ AI ëª¨ë¸ì´ ì—†ìŒ"
                self.logger.error(f"âŒ {error_msg}")
                return {'success': False, 'error': error_msg}
            
            # 1. YOLOv8-Poseë¡œ ì‹¤ì‹œê°„ ê²€ì¶œ (ìš°ì„ ìˆœìœ„ 1)
            yolo_result = None
            if "yolov8" in self.ai_models:
                try:
                    yolo_result = self.ai_models["yolov8"].detect_poses_realtime(image)
                    self.logger.info(f"âœ… YOLOv8 ì¶”ë¡  ì™„ë£Œ: {yolo_result.get('success', False)}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ YOLOv8 ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            # 2. OpenPoseë¡œ ì •ë°€ ê²€ì¶œ (ìš°ì„ ìˆœìœ„ 2)
            openpose_result = None
            if "openpose" in self.ai_models:
                try:
                    openpose_result = self.ai_models["openpose"].detect_keypoints_precise(image)
                    self.logger.info(f"âœ… OpenPose ì¶”ë¡  ì™„ë£Œ: {openpose_result.get('success', False)}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ OpenPose ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            # 3. Body Poseë¡œ ë³´ì¡° ê²€ì¶œ (ìš°ì„ ìˆœìœ„ 3)
            body_pose_result = None
            if "body_pose" in self.ai_models:
                try:
                    body_pose_result = self.ai_models["body_pose"].detect_body_pose(image)
                    self.logger.info(f"âœ… Body Pose ì¶”ë¡  ì™„ë£Œ: {body_pose_result.get('success', False)}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Body Pose ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            # HRNetìœ¼ë¡œ ê³ ì •ë°€ ê²€ì¶œ (ìš°ì„ ìˆœìœ„ 3 - ìƒˆë¡œ ì¶”ê°€)
            hrnet_result = None
            if "hrnet" in self.ai_models:
                try:
                    hrnet_result = self.ai_models["hrnet"].detect_high_precision_pose(image)
                    self.logger.info(f"âœ… HRNet ì¶”ë¡  ì™„ë£Œ: {hrnet_result.get('success', False)}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ HRNet ì¶”ë¡  ì‹¤íŒ¨: {e}")
            
            # 4. ìµœì  ê²°ê³¼ ì„ íƒ ë° í†µí•© (HRNet í¬í•¨)
            primary_result = self._select_best_pose_result(yolo_result, openpose_result, body_pose_result, hrnet_result)
            
            if not primary_result or not primary_result.get('keypoints'):
                error_msg = "ëª¨ë“  AI ëª¨ë¸ì—ì„œ ìœ íš¨í•œ í¬ì¦ˆë¥¼ ê²€ì¶œí•˜ì§€ ëª»í•¨"
                self.logger.error(f"âŒ {error_msg}")
                return {'success': False, 'error': error_msg}
            
            # 5. Diffusion Poseë¡œ í’ˆì§ˆ í–¥ìƒ (ì„ íƒì )
            enhanced_result = primary_result
            if "diffusion" in self.ai_models and primary_result.get('keypoints'):
                try:
                    diffusion_result = self.ai_models["diffusion"].enhance_pose_quality(
                        primary_result['keypoints'], image
                    )
                    if diffusion_result.get('success', False):
                        enhanced_result['keypoints'] = diffusion_result['enhanced_keypoints']
                        enhanced_result['enhanced_by_diffusion'] = True
                        self.logger.info("âœ… Diffusion í’ˆì§ˆ í–¥ìƒ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Diffusion í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
            
            # 6. ê²°ê³¼ í†µí•© ë° ë¶„ì„
            combined_keypoints = enhanced_result['keypoints']
            combined_result = {
                'keypoints': combined_keypoints,
                'skeleton_structure': self._build_skeleton_structure(combined_keypoints),
                'joint_connections': self._get_joint_connections(combined_keypoints),
                'joint_angles': self._calculate_joint_angles(combined_keypoints),
                'body_orientation': self._get_body_orientation(combined_keypoints),
                'landmarks': self._extract_landmarks(combined_keypoints),
                'confidence_scores': [kp[2] for kp in combined_keypoints if len(kp) > 2],
                'processing_time': time.time() - inference_start,
                'models_used': self.loaded_models,
                'primary_model': primary_result.get('model_type', 'unknown'),
                'enhanced_by_diffusion': enhanced_result.get('enhanced_by_diffusion', False),
                'success': True
            }
            
            inference_time = time.time() - inference_start
            self.logger.info(f"âœ… ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ ({inference_time:.3f}ì´ˆ)")
            
            return combined_result
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise
            return {'success': False, 'error': str(e)}
    
    def _select_best_pose_result(self, yolo_result, openpose_result, body_pose_result, hrnet_result=None) -> Optional[Dict[str, Any]]:
        """ìµœì ì˜ í¬ì¦ˆ ê²°ê³¼ ì„ íƒ (HRNet í¬í•¨)"""
        results = []
        
        # ê° ê²°ê³¼ì˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        if yolo_result and yolo_result.get('success') and yolo_result.get('keypoints'):
            confidence = np.mean([kp[2] for kp in yolo_result['keypoints'] if len(kp) > 2])
            visible_kpts = sum(1 for kp in yolo_result['keypoints'] if len(kp) > 2 and kp[2] > 0.3)
            quality_score = confidence * 0.7 + (visible_kpts / 18) * 0.3
            results.append((quality_score, yolo_result))
        
        if openpose_result and openpose_result.get('success') and openpose_result.get('keypoints'):
            confidence = np.mean([kp[2] for kp in openpose_result['keypoints'] if len(kp) > 2])
            visible_kpts = sum(1 for kp in openpose_result['keypoints'] if len(kp) > 2 and kp[2] > 0.3)
            quality_score = confidence * 0.8 + (visible_kpts / 18) * 0.2  # OpenPoseëŠ” ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ë†’ìŒ
            results.append((quality_score, openpose_result))
        
        if hrnet_result and hrnet_result.get('success') and hrnet_result.get('keypoints'):
            confidence = np.mean([kp[2] for kp in hrnet_result['keypoints'] if len(kp) > 2])
            visible_kpts = sum(1 for kp in hrnet_result['keypoints'] if len(kp) > 2 and kp[2] > 0.3)
            quality_score = confidence * 0.85 + (visible_kpts / 18) * 0.15  # HRNetì€ ê³ ì •ë°€ì´ë¯€ë¡œ ì‹ ë¢°ë„ ìµœìš°ì„ 
            results.append((quality_score, hrnet_result))
        
        if body_pose_result and body_pose_result.get('success') and body_pose_result.get('keypoints'):
            confidence = np.mean([kp[2] for kp in body_pose_result['keypoints'] if len(kp) > 2])
            visible_kpts = sum(1 for kp in body_pose_result['keypoints'] if len(kp) > 2 and kp[2] > 0.3)
            quality_score = confidence * 0.6 + (visible_kpts / 18) * 0.4  # Body PoseëŠ” ë³´ì¡° ì—­í• 
            results.append((quality_score, body_pose_result))
        
        if not results:
            return None
        
        # ìµœê³  í’ˆì§ˆ ì ìˆ˜ ê²°ê³¼ ì„ íƒ
        best_score, best_result = max(results, key=lambda x: x[0])
        self.logger.info(f"ğŸ† ìµœì  í¬ì¦ˆ ê²°ê³¼ ì„ íƒ: {best_result.get('model_type', 'unknown')} (ì ìˆ˜: {best_score:.3f})")
        
        return best_result
    
    # ==============================================
    # ğŸ”¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° í›„ì²˜ë¦¬
    # ==============================================
    
    def _preprocess_image(self, image: Union[np.ndarray, Image.Image, str]) -> Optional[Image.Image]:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ ë¡œë”© ë° ë³€í™˜
            if isinstance(image, str):
                if os.path.exists(image):
                    image = Image.open(image)
                else:
                    try:
                        image_data = base64.b64decode(image)
                        image = Image.open(BytesIO(image_data))
                    except Exception:
                        return None
            elif isinstance(image, np.ndarray):
                if image.size == 0:
                    return None
                image = Image.fromarray(image)
            elif not isinstance(image, Image.Image):
                return None
            
            # RGB ë³€í™˜
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # í¬ê¸° ê²€ì¦
            if image.size[0] < 64 or image.size[1] < 64:
                return None
            
            # í¬ê¸° ì¡°ì • (AI ëª¨ë¸ ì…ë ¥ìš©)
            max_size = 1024 if IS_M3_MAX else 512
            if max(image.size) > max_size:
                ratio = max_size / max(image.size)
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = image.resize(new_size, Image.Resampling.BILINEAR)
            
            return image
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _postprocess_ai_result(self, pose_result: Dict[str, Any], image: Image.Image, start_time: float) -> Dict[str, Any]:
        """AI ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            processing_time = time.time() - start_time
            
            # PoseMetrics ìƒì„±
            pose_metrics = PoseMetrics(
                keypoints=pose_result.get('keypoints', []),
                confidence_scores=pose_result.get('confidence_scores', []),
                model_used=pose_result.get('primary_model', 'unknown'),
                processing_time=processing_time,
                image_resolution=image.size,
                ai_confidence=np.mean(pose_result.get('confidence_scores', [0])) if pose_result.get('confidence_scores') else 0.0
            )
            
            # í¬ì¦ˆ ë¶„ì„
            pose_analysis = self._analyze_pose_quality(pose_metrics, clothing_type=None)
            
            # ì‹œê°í™” ìƒì„±
            visualization = None
            if self.visualization_enabled:
                visualization = self._create_pose_visualization(image, pose_metrics)
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                'success': pose_result.get('success', False),
                'keypoints': pose_metrics.keypoints,
                'confidence_scores': pose_metrics.confidence_scores,
                'skeleton_structure': pose_result.get('skeleton_structure', {}),
                'joint_connections': pose_result.get('joint_connections', []),
                'joint_angles': pose_result.get('joint_angles', {}),
                'body_orientation': pose_result.get('body_orientation', {}),
                'landmarks': pose_result.get('landmarks', {}),
                'pose_analysis': pose_analysis,
                'visualization': visualization,
                'processing_time': processing_time,
                'inference_time': pose_result.get('processing_time', 0.0),
                'models_used': pose_result.get('models_used', []),
                'primary_model': pose_result.get('primary_model', 'unknown'),
                'enhanced_by_diffusion': pose_result.get('enhanced_by_diffusion', False),
                'image_resolution': pose_metrics.image_resolution,
                'step_info': {
                    'step_name': self.step_name,
                    'step_number': self.step_number,
                    'device': self.device,
                    'loaded_models': self.loaded_models,
                    'dependency_injection_status': self.dependencies_injected,
                    'real_ai_models': True,
                    'basestep_version': '16.0-compatible'
                }
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ AI ê²°ê³¼ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_error_result(str(e))
    
    # ==============================================
    # ğŸ”¥ í¬ì¦ˆ ë¶„ì„ ë° í’ˆì§ˆ í‰ê°€
    # ==============================================
    
    def _analyze_pose_quality(self, pose_metrics: PoseMetrics, clothing_type: Optional[str] = None) -> Dict[str, Any]:
        """í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„"""
        try:
            if not pose_metrics.keypoints:
                return {
                    'suitable_for_fitting': False,
                    'issues': ['AI ëª¨ë¸ì—ì„œ í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'],
                    'recommendations': ['ë” ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ í¬ì¦ˆë¥¼ ëª…í™•íˆ í•´ì£¼ì„¸ìš”'],
                    'quality_score': 0.0,
                    'ai_confidence': 0.0
                }
            
            # AI ì‹ ë¢°ë„ ê³„ì‚°
            ai_confidence = np.mean(pose_metrics.confidence_scores) if pose_metrics.confidence_scores else 0.0
            
            # ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°
            head_score = self._calculate_body_part_score(pose_metrics.keypoints, [0, 15, 16, 17, 18])
            torso_score = self._calculate_body_part_score(pose_metrics.keypoints, [1, 2, 5, 8])
            arms_score = self._calculate_body_part_score(pose_metrics.keypoints, [2, 3, 4, 5, 6, 7])
            legs_score = self._calculate_body_part_score(pose_metrics.keypoints, [9, 10, 11, 12, 13, 14])
            
            # ê³ ê¸‰ ë¶„ì„
            symmetry_score = self._calculate_symmetry_score(pose_metrics.keypoints)
            visibility_score = self._calculate_visibility_score(pose_metrics.keypoints)
            pose_angles = self._calculate_joint_angles(pose_metrics.keypoints)
            body_proportions = self._calculate_body_proportions(pose_metrics.keypoints, pose_metrics.image_resolution)
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self._calculate_overall_quality_score(
                head_score, torso_score, arms_score, legs_score, 
                symmetry_score, visibility_score, ai_confidence
            )
            
            # ì í•©ì„± íŒë‹¨
            min_score = 0.75 if self.strict_mode else 0.65
            min_confidence = 0.7 if self.strict_mode else 0.6
            visible_keypoints = sum(1 for kp in pose_metrics.keypoints if len(kp) > 2 and kp[2] > 0.5)
            suitable_for_fitting = (quality_score >= min_score and 
                                  ai_confidence >= min_confidence and
                                  visible_keypoints >= 10)
            
            # ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
            issues = []
            recommendations = []
            
            if ai_confidence < min_confidence:
                issues.append(f'AI ëª¨ë¸ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({ai_confidence:.2f})')
                recommendations.append('ì¡°ëª…ì´ ì¢‹ì€ í™˜ê²½ì—ì„œ ë‹¤ì‹œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            if visible_keypoints < 10:
                issues.append('ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤')
                recommendations.append('ì „ì‹ ì´ ëª…í™•íˆ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            if symmetry_score < 0.6:
                issues.append('ì¢Œìš° ëŒ€ì¹­ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤')
                recommendations.append('ì •ë©´ì„ í–¥í•´ ê· í˜•ì¡íŒ ìì„¸ë¡œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            if torso_score < 0.7:
                issues.append('ìƒì²´ í¬ì¦ˆê°€ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤')
                recommendations.append('ì–´ê¹¨ì™€ íŒ”ì´ ëª…í™•íˆ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            return {
                'suitable_for_fitting': suitable_for_fitting,
                'issues': issues,
                'recommendations': recommendations,
                'quality_score': quality_score,
                'ai_confidence': ai_confidence,
                'visible_keypoints': visible_keypoints,
                'total_keypoints': len(pose_metrics.keypoints),
                
                # ì‹ ì²´ ë¶€ìœ„ë³„ ìƒì„¸ ì ìˆ˜
                'detailed_scores': {
                    'head': head_score,
                    'torso': torso_score,
                    'arms': arms_score,
                    'legs': legs_score
                },
                
                # ê³ ê¸‰ ë¶„ì„ ê²°ê³¼
                'advanced_analysis': {
                    'symmetry_score': symmetry_score,
                    'visibility_score': visibility_score,
                    'pose_angles': pose_angles,
                    'body_proportions': body_proportions
                },
                
                # AI ëª¨ë¸ ì„±ëŠ¥ ì •ë³´
                'model_performance': {
                    'model_name': pose_metrics.model_used,
                    'processing_time': pose_metrics.processing_time,
                    'real_ai_models': True
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise
            return {
                'suitable_for_fitting': False,
                'issues': ['ë¶„ì„ ì‹¤íŒ¨'],
                'recommendations': ['ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”'],
                'quality_score': 0.0,
                'ai_confidence': 0.0
            }
    
    def _calculate_body_part_score(self, keypoints: List[List[float]], part_indices: List[int]) -> float:
        """ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°"""
        try:
            if not keypoints or not part_indices:
                return 0.0
            
            visible_count = 0
            total_confidence = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > self.confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            
            return visibility_ratio * avg_confidence
            
        except Exception as e:
            self.logger.debug(f"ì‹ ì²´ ë¶€ìœ„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_symmetry_score(self, keypoints: List[List[float]]) -> float:
        """ì¢Œìš° ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            if not keypoints or len(keypoints) < 18:
                return 0.0
            
            # ëŒ€ì¹­ ë¶€ìœ„ ìŒ ì •ì˜
            symmetric_pairs = [
                (2, 5),   # right_shoulder, left_shoulder
                (3, 6),   # right_elbow, left_elbow
                (4, 7),   # right_wrist, left_wrist
                (9, 12),  # right_hip, left_hip
                (10, 13), # right_knee, left_knee
                (11, 14), # right_ankle, left_ankle
                (15, 16), # right_eye, left_eye
                (17, 18)  # right_ear, left_ear
            ]
            
            symmetry_scores = []
            confidence_threshold = 0.3
            
            for right_idx, left_idx in symmetric_pairs:
                if (right_idx < len(keypoints) and left_idx < len(keypoints) and
                    len(keypoints[right_idx]) >= 3 and len(keypoints[left_idx]) >= 3):
                    
                    right_kp = keypoints[right_idx]
                    left_kp = keypoints[left_idx]
                    
                    if right_kp[2] > confidence_threshold and left_kp[2] > confidence_threshold:
                        # ì¤‘ì‹¬ì„  ê³„ì‚°
                        center_x = sum(kp[0] for kp in keypoints if len(kp) >= 3 and kp[2] > confidence_threshold) / \
                                 max(len([kp for kp in keypoints if len(kp) >= 3 and kp[2] > confidence_threshold]), 1)
                        
                        right_dist = abs(right_kp[0] - center_x)
                        left_dist = abs(left_kp[0] - center_x)
                        
                        max_dist = max(right_dist, left_dist)
                        if max_dist > 0:
                            symmetry = 1.0 - abs(right_dist - left_dist) / max_dist
                            weighted_symmetry = symmetry * min(right_kp[2], left_kp[2])
                            symmetry_scores.append(weighted_symmetry)
            
            if not symmetry_scores:
                return 0.0
            
            return np.mean(symmetry_scores)
            
        except Exception as e:
            self.logger.debug(f"ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_visibility_score(self, keypoints: List[List[float]]) -> float:
        """í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            if not keypoints:
                return 0.0
            
            visible_count = 0
            total_confidence = 0.0
            
            for kp in keypoints:
                if len(kp) >= 3:
                    if kp[2] > self.confidence_threshold:
                        visible_count += 1
                        total_confidence += kp[2]
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(keypoints)
            avg_confidence = total_confidence / visible_count
            
            return visibility_ratio * avg_confidence
            
        except Exception as e:
            self.logger.debug(f"ê°€ì‹œì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_overall_quality_score(
        self, head_score: float, torso_score: float, arms_score: float, legs_score: float,
        symmetry_score: float, visibility_score: float, ai_confidence: float
    ) -> float:
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            base_scores = [
                head_score * 0.15,
                torso_score * 0.35,
                arms_score * 0.25,
                legs_score * 0.25
            ]
            
            advanced_scores = [
                symmetry_score * 0.3,
                visibility_score * 0.7
            ]
            
            base_score = sum(base_scores)
            advanced_score = sum(advanced_scores)
            
            overall_score = (base_score * 0.7 + advanced_score * 0.3) * ai_confidence
            
            return max(0.0, min(1.0, overall_score))
            
        except Exception as e:
            self.logger.debug(f"ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    # ==============================================
    # ğŸ”¥ ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ë° ê¸°í•˜í•™ì  ë¶„ì„
    # ==============================================
    
    def _build_skeleton_structure(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±"""
        try:
            skeleton = {
                'connections': [],
                'bone_lengths': {},
                'joint_positions': {},
                'structure_valid': False
            }
            
            if not keypoints or len(keypoints) < 18:
                return skeleton
            
            # ì—°ê²° êµ¬ì¡° ìƒì„±
            for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
                if (start_idx < len(keypoints) and end_idx < len(keypoints) and
                    len(keypoints[start_idx]) >= 3 and len(keypoints[end_idx]) >= 3):
                    
                    start_kp = keypoints[start_idx]
                    end_kp = keypoints[end_idx]
                    
                    if start_kp[2] > 0.3 and end_kp[2] > 0.3:
                        connection = {
                            'start': start_idx,
                            'end': end_idx,
                            'start_name': OPENPOSE_18_KEYPOINTS[start_idx] if start_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{start_idx}",
                            'end_name': OPENPOSE_18_KEYPOINTS[end_idx] if end_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{end_idx}",
                            'length': np.sqrt((start_kp[0] - end_kp[0])**2 + (start_kp[1] - end_kp[1])**2),
                            'confidence': (start_kp[2] + end_kp[2]) / 2
                        }
                        skeleton['connections'].append(connection)
            
            # ê´€ì ˆ ìœ„ì¹˜
            for i, kp in enumerate(keypoints):
                if len(kp) >= 3 and kp[2] > 0.3:
                    joint_name = OPENPOSE_18_KEYPOINTS[i] if i < len(OPENPOSE_18_KEYPOINTS) else f"joint_{i}"
                    skeleton['joint_positions'][joint_name] = {
                        'x': kp[0],
                        'y': kp[1],
                        'confidence': kp[2]
                    }
            
            skeleton['structure_valid'] = len(skeleton['connections']) >= 8
            
            return skeleton
            
        except Exception as e:
            self.logger.debug(f"ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„± ì‹¤íŒ¨: {e}")
            return {'connections': [], 'bone_lengths': {}, 'joint_positions': {}, 'structure_valid': False}
    
    def _get_joint_connections(self, keypoints: List[List[float]]) -> List[Dict[str, Any]]:
        """ê´€ì ˆ ì—°ê²° ì •ë³´ ë°˜í™˜"""
        try:
            connections = []
            
            for start_idx, end_idx in SKELETON_CONNECTIONS:
                if (start_idx < len(keypoints) and end_idx < len(keypoints) and
                    len(keypoints[start_idx]) >= 3 and len(keypoints[end_idx]) >= 3):
                    
                    start_kp = keypoints[start_idx]
                    end_kp = keypoints[end_idx]
                    
                    if start_kp[2] > 0.3 and end_kp[2] > 0.3:
                        connection = {
                            'start_joint': start_idx,
                            'end_joint': end_idx,
                            'start_name': OPENPOSE_18_KEYPOINTS[start_idx] if start_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{start_idx}",
                            'end_name': OPENPOSE_18_KEYPOINTS[end_idx] if end_idx < len(OPENPOSE_18_KEYPOINTS) else f"point_{end_idx}",
                            'connection_strength': (start_kp[2] + end_kp[2]) / 2
                        }
                        connections.append(connection)
            
            return connections
            
        except Exception as e:
            self.logger.debug(f"ê´€ì ˆ ì—°ê²° ì •ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_joint_angles(self, keypoints: List[List[float]]) -> Dict[str, float]:
        """ê´€ì ˆ ê°ë„ ê³„ì‚°"""
        try:
            angles = {}
            
            if not keypoints or len(keypoints) < 18:
                return angles
            
            def calculate_angle(p1, p2, p3):
                """ì„¸ ì  ì‚¬ì´ì˜ ê°ë„ ê³„ì‚°"""
                try:
                    v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
                    v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
                    
                    cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
                    cos_angle = np.clip(cos_angle, -1.0, 1.0)
                    angle = np.arccos(cos_angle)
                    
                    return np.degrees(angle)
                except:
                    return 0.0
            
            confidence_threshold = 0.3
            
            # íŒ”ê¿ˆì¹˜ ê°ë„ (ì˜¤ë¥¸ìª½)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [2, 3, 4]):
                angles['right_elbow'] = calculate_angle(keypoints[2], keypoints[3], keypoints[4])
            
            # íŒ”ê¿ˆì¹˜ ê°ë„ (ì™¼ìª½)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [5, 6, 7]):
                angles['left_elbow'] = calculate_angle(keypoints[5], keypoints[6], keypoints[7])
            
            # ë¬´ë¦ ê°ë„ (ì˜¤ë¥¸ìª½)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [9, 10, 11]):
                angles['right_knee'] = calculate_angle(keypoints[9], keypoints[10], keypoints[11])
            
            # ë¬´ë¦ ê°ë„ (ì™¼ìª½)
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [12, 13, 14]):
                angles['left_knee'] = calculate_angle(keypoints[12], keypoints[13], keypoints[14])
            
            # ì–´ê¹¨ ê¸°ìš¸ê¸°
            if all(idx < len(keypoints) and len(keypoints[idx]) >= 3 and keypoints[idx][2] > confidence_threshold 
                   for idx in [2, 5]):
                shoulder_slope = np.degrees(np.arctan2(
                    keypoints[5][1] - keypoints[2][1],
                    keypoints[5][0] - keypoints[2][0] + 1e-8
                ))
                angles['shoulder_slope'] = abs(shoulder_slope)
            
            return angles
            
        except Exception as e:
            self.logger.debug(f"ê´€ì ˆ ê°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _get_body_orientation(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """ì‹ ì²´ ë°©í–¥ ë¶„ì„"""
        try:
            orientation = {
                'facing_direction': 'unknown',
                'body_angle': 0.0,
                'shoulder_line_angle': 0.0,
                'hip_line_angle': 0.0,
                'is_frontal': False
            }
            
            if not keypoints or len(keypoints) < 18:
                return orientation
            
            # ì–´ê¹¨ ë¼ì¸ ê°ë„
            if (2 < len(keypoints) and 5 < len(keypoints) and
                len(keypoints[2]) >= 3 and len(keypoints[5]) >= 3 and
                keypoints[2][2] > 0.3 and keypoints[5][2] > 0.3):
                
                shoulder_angle = np.degrees(np.arctan2(
                    keypoints[5][1] - keypoints[2][1],
                    keypoints[5][0] - keypoints[2][0]
                ))
                orientation['shoulder_line_angle'] = shoulder_angle
                
                # ì •ë©´ ì—¬ë¶€ íŒë‹¨
                orientation['is_frontal'] = abs(shoulder_angle) < 15
            
            # ì—‰ë©ì´ ë¼ì¸ ê°ë„
            if (9 < len(keypoints) and 12 < len(keypoints) and
                len(keypoints[9]) >= 3 and len(keypoints[12]) >= 3 and
                keypoints[9][2] > 0.3 and keypoints[12][2] > 0.3):
                
                hip_angle = np.degrees(np.arctan2(
                    keypoints[12][1] - keypoints[9][1],
                    keypoints[12][0] - keypoints[9][0]
                ))
                orientation['hip_line_angle'] = hip_angle
            
            # ì „ì²´ ì‹ ì²´ ê°ë„ (ì–´ê¹¨ì™€ ì—‰ë©ì´ í‰ê· )
            if orientation['shoulder_line_angle'] != 0.0 and orientation['hip_line_angle'] != 0.0:
                orientation['body_angle'] = (orientation['shoulder_line_angle'] + orientation['hip_line_angle']) / 2
            elif orientation['shoulder_line_angle'] != 0.0:
                orientation['body_angle'] = orientation['shoulder_line_angle']
            elif orientation['hip_line_angle'] != 0.0:
                orientation['body_angle'] = orientation['hip_line_angle']
            
            # ë°©í–¥ ë¶„ë¥˜
            if abs(orientation['body_angle']) < 15:
                orientation['facing_direction'] = 'front'
            elif orientation['body_angle'] > 15:
                orientation['facing_direction'] = 'left'
            elif orientation['body_angle'] < -15:
                orientation['facing_direction'] = 'right'
            
            return orientation
            
        except Exception as e:
            self.logger.debug(f"ì‹ ì²´ ë°©í–¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'facing_direction': 'unknown', 'body_angle': 0.0, 'is_frontal': False}
    
    def _extract_landmarks(self, keypoints: List[List[float]]) -> Dict[str, Dict[str, float]]:
        """ì£¼ìš” ëœë“œë§ˆí¬ ì¶”ì¶œ"""
        try:
            landmarks = {}
            
            for i, kp in enumerate(keypoints):
                if len(kp) >= 3 and kp[2] > 0.3:
                    landmark_name = OPENPOSE_18_KEYPOINTS[i] if i < len(OPENPOSE_18_KEYPOINTS) else f"landmark_{i}"
                    landmarks[landmark_name] = {
                        'x': float(kp[0]),
                        'y': float(kp[1]),
                        'confidence': float(kp[2])
                    }
            
            return landmarks
            
        except Exception as e:
            self.logger.debug(f"ëœë“œë§ˆí¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_body_proportions(self, keypoints: List[List[float]], image_resolution: Tuple[int, int]) -> Dict[str, float]:
        """ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°"""
        try:
            proportions = {}
            
            if not keypoints or len(keypoints) < 18 or not image_resolution:
                return proportions
            
            width, height = image_resolution
            confidence_threshold = 0.3
            
            def get_valid_keypoint(idx):
                if (idx < len(keypoints) and len(keypoints[idx]) >= 3 and 
                    keypoints[idx][2] > confidence_threshold):
                    return keypoints[idx]
                return None
            
            def euclidean_distance(p1, p2):
                if p1 and p2:
                    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)
                return 0.0
            
            # ë¨¸ë¦¬-ëª© ê¸¸ì´
            nose = get_valid_keypoint(0)
            neck = get_valid_keypoint(1)
            if nose and neck:
                proportions['head_neck_ratio'] = euclidean_distance(nose, neck) / height
            
            # ìƒì²´ ê¸¸ì´ (ëª©-ì—‰ë©ì´)
            if neck:
                mid_hip = get_valid_keypoint(8)
                if mid_hip:
                    proportions['torso_ratio'] = euclidean_distance(neck, mid_hip) / height
            
            # íŒ” ê¸¸ì´ (ì–´ê¹¨-ì†ëª©)
            right_shoulder = get_valid_keypoint(2)
            right_wrist = get_valid_keypoint(4)
            if right_shoulder and right_wrist:
                proportions['right_arm_ratio'] = euclidean_distance(right_shoulder, right_wrist) / height
            
            left_shoulder = get_valid_keypoint(5)
            left_wrist = get_valid_keypoint(7)
            if left_shoulder and left_wrist:
                proportions['left_arm_ratio'] = euclidean_distance(left_shoulder, left_wrist) / height
            
            # ë‹¤ë¦¬ ê¸¸ì´ (ì—‰ë©ì´-ë°œëª©)
            right_hip = get_valid_keypoint(9)
            right_ankle = get_valid_keypoint(11)
            if right_hip and right_ankle:
                proportions['right_leg_ratio'] = euclidean_distance(right_hip, right_ankle) / height
            
            left_hip = get_valid_keypoint(12)
            left_ankle = get_valid_keypoint(14)
            if left_hip and left_ankle:
                proportions['left_leg_ratio'] = euclidean_distance(left_hip, left_ankle) / height
            
            # ì–´ê¹¨ ë„ˆë¹„
            if right_shoulder and left_shoulder:
                proportions['shoulder_width_ratio'] = euclidean_distance(right_shoulder, left_shoulder) / width
            
            # ì—‰ë©ì´ ë„ˆë¹„
            if right_hip and left_hip:
                proportions['hip_width_ratio'] = euclidean_distance(right_hip, left_hip) / width
            
            return proportions
            
        except Exception as e:
            self.logger.debug(f"ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    # ==============================================
    # ğŸ”¥ ì‹œê°í™” ë° ìœ í‹¸ë¦¬í‹°
    # ==============================================
    
    def _create_pose_visualization(self, image: Image.Image, pose_metrics: PoseMetrics) -> Optional[str]:
        """í¬ì¦ˆ ì‹œê°í™” ìƒì„±"""
        try:
            if not pose_metrics.keypoints:
                return None
            
            vis_image = image.copy()
            draw = ImageDraw.Draw(vis_image)
            
            # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
            for i, kp in enumerate(pose_metrics.keypoints):
                if len(kp) >= 3 and kp[2] > self.confidence_threshold:
                    x, y = int(kp[0]), int(kp[1])
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    # ì‹ ë¢°ë„ ê¸°ë°˜ í¬ê¸° ì¡°ì ˆ
                    radius = int(4 + kp[2] * 8)
                    
                    draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                               fill=color, outline=(255, 255, 255), width=2)
            
            # ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²°ì„  ê·¸ë¦¬ê¸°
            for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
                if (start_idx < len(pose_metrics.keypoints) and 
                    end_idx < len(pose_metrics.keypoints)):
                    
                    start_kp = pose_metrics.keypoints[start_idx]
                    end_kp = pose_metrics.keypoints[end_idx]
                    
                    if (len(start_kp) >= 3 and len(end_kp) >= 3 and
                        start_kp[2] > self.confidence_threshold and end_kp[2] > self.confidence_threshold):
                        
                        start_point = (int(start_kp[0]), int(start_kp[1]))
                        end_point = (int(end_kp[0]), int(end_kp[1]))
                        color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                        
                        # ì‹ ë¢°ë„ ê¸°ë°˜ ì„  ë‘ê»˜
                        avg_confidence = (start_kp[2] + end_kp[2]) / 2
                        line_width = int(2 + avg_confidence * 6)
                        
                        draw.line([start_point, end_point], fill=color, width=line_width)
            
            # AI ì‹ ë¢°ë„ ì •ë³´ ì¶”ê°€
            if hasattr(pose_metrics, 'ai_confidence'):
                ai_info = f"AI ì‹ ë¢°ë„: {pose_metrics.ai_confidence:.3f}"
                try:
                    font = ImageFont.load_default()
                    draw.text((10, 10), ai_info, fill=(255, 255, 255), font=font)
                except:
                    draw.text((10, 10), ai_info, fill=(255, 255, 255))
            
            # Base64ë¡œ ì¸ì½”ë”©
            buffer = BytesIO()
            vis_image.save(buffer, format='JPEG', quality=95)
            image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            return f"data:image/jpeg;base64,{image_base64}"
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_cache_key(self, image: Image.Image, clothing_type: Optional[str]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            image_bytes = BytesIO()
            image.save(image_bytes, format='JPEG', quality=50)
            image_hash = hashlib.md5(image_bytes.getvalue()).hexdigest()[:16]
            
            config_str = f"{clothing_type}_{self.confidence_threshold}_{len(self.loaded_models)}"
            config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]
            
            return f"pose_{image_hash}_{config_hash}"
            
        except Exception:
            return f"pose_{int(time.time())}"
    
    def _save_to_cache(self, cache_key: str, result: Dict[str, Any]):
        """ìºì‹œì— ì €ì¥"""
        try:
            if len(self.prediction_cache) >= self.cache_max_size:
                oldest_key = next(iter(self.prediction_cache))
                del self.prediction_cache[oldest_key]
            
            cached_result = result.copy()
            cached_result['visualization'] = None  # ë©”ëª¨ë¦¬ ì ˆì•½
            
            self.prediction_cache[cache_key] = cached_result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _create_error_result(self, error_message: str, processing_time: float = 0.0) -> Dict[str, Any]:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'keypoints': [],
            'confidence_scores': [],
            'skeleton_structure': {},
            'joint_connections': [],
            'joint_angles': {},
            'body_orientation': {},
            'landmarks': {},
            'pose_analysis': {
                'suitable_for_fitting': False,
                'issues': [error_message],
                'recommendations': ['ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”'],
                'quality_score': 0.0,
                'ai_confidence': 0.0
            },
            'visualization': None,
            'processing_time': processing_time,
            'inference_time': 0.0,
            'models_used': [],
            'primary_model': 'error',
            'enhanced_by_diffusion': False,
            'step_info': {
                'step_name': self.step_name,
                'step_number': self.step_number,
                'device': self.device,
                'loaded_models': self.loaded_models,
                'dependency_injection_status': self.dependencies_injected,
                'real_ai_models': True,
                'basestep_version': '16.0-compatible'
            }
        }
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin v16.0 í˜¸í™˜ ë©”ì„œë“œë“¤
    # ==============================================
    
    async def initialize(self) -> bool:
        """BaseStepMixin v16.0 í˜¸í™˜ ì´ˆê¸°í™”"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"ğŸš€ {self.step_name} ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜ ì´ˆê¸°í™” ì‹œì‘")
            start_time = time.time()
            
            # ì˜ì¡´ì„± ì£¼ì… ê²€ì¦
            if not hasattr(self, 'model_loader') or not self.model_loader:
                # ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹œë„
                if hasattr(self, 'dependency_manager'):
                    success = self.dependency_manager.auto_inject_dependencies()
                    if not success:
                        self.logger.warning("âš ï¸ ìë™ ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨ - ìˆ˜ë™ ì‹œë„")
                        success = self._manual_auto_inject()
                else:
                    success = self._manual_auto_inject()
                
                if not success:
                    error_msg = "ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨"
                    if self.strict_mode:
                        raise RuntimeError(f"Strict Mode: {error_msg}")
                    self.logger.warning(f"âš ï¸ {error_msg} - ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰")
            
            # ì‹¤ì œ AI ëª¨ë¸ ë¡œë”©
            model_loading_success = await self._load_real_ai_models()
            
            if not model_loading_success:
                error_msg = "ì‹¤ì œ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"
                if self.strict_mode:
                    raise RuntimeError(f"Strict Mode: {error_msg}")
                self.logger.warning(f"âš ï¸ {error_msg} - ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰")
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            self.is_ready = True
            self.has_model = len(self.ai_models) > 0
            self.model_loaded = self.has_model
            
            elapsed_time = time.time() - start_time
            self.logger.info(f"âœ… {self.step_name} ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜ ì´ˆê¸°í™” ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
            self.logger.info(f"ğŸ¤– ë¡œë”©ëœ AI ëª¨ë¸ë“¤: {self.loaded_models}")
            self.logger.info(f"ğŸ”— ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ: {self.dependencies_injected}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì‹¤ì œ AI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if self.strict_mode:
                raise
            return False
    
    def get_status(self) -> Dict[str, Any]:
        """BaseStepMixin v16.0 í˜¸í™˜ ìƒíƒœ ë°˜í™˜"""
        try:
            return {
                'step_name': self.step_name,
                'step_number': self.step_number,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'has_model': self.has_model,
                'model_loaded': self.model_loaded,
                'device': self.device,
                'is_m3_max': IS_M3_MAX,
                'dependencies': self.dependencies_injected,
                'performance_metrics': self.performance_metrics,
                'loaded_models': self.loaded_models,
                'model_paths': {k: str(v) if v else None for k, v in self.model_paths.items()},
                'real_ai_models': True,
                'ai_libraries_available': {
                    'torch': TORCH_AVAILABLE,
                    'ultralytics': ULTRALYTICS_AVAILABLE,
                    'mediapipe': MEDIAPIPE_AVAILABLE,
                    'transformers': TRANSFORMERS_AVAILABLE,
                    'safetensors': SAFETENSORS_AVAILABLE
                },
                'version': '16.0-compatible',
                'timestamp': time.time()
            }
        except Exception as e:
            self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'version': '16.0-compatible', 'real_ai_models': True}
    
    async def cleanup(self) -> Dict[str, Any]:
        """BaseStepMixin v16.0 í˜¸í™˜ ì •ë¦¬"""
        try:
            self.logger.info(f"ğŸ§¹ {self.step_name} ì‹¤ì œ AI ëª¨ë¸ ì •ë¦¬ ì‹œì‘...")
            
            # AI ëª¨ë¸ ì •ë¦¬
            cleanup_count = 0
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'model') and model.model:
                        if hasattr(model.model, 'cpu'):
                            model.model.cpu()
                        del model.model
                        model.model = None
                        model.loaded = False
                    cleanup_count += 1
                except Exception as e:
                    self.logger.debug(f"AI ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨ {model_name}: {e}")
            
            self.ai_models.clear()
            self.loaded_models.clear()
            self.model_paths.clear()
            
            # ìºì‹œ ì •ë¦¬
            self.prediction_cache.clear()
            
            # GPU/MPS ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                elif IS_M3_MAX and torch.backends.mps.is_available():
                    try:
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                    except:
                        pass
            
            gc.collect()
            
            # ìƒíƒœ ë¦¬ì…‹
            self.is_ready = False
            self.warmup_completed = False
            self.has_model = False
            self.model_loaded = False
            
            # ì˜ì¡´ì„± í•´ì œ (ì°¸ì¡°ë§Œ ì œê±°)
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            self.logger.info(f"âœ… {self.step_name} ì‹¤ì œ AI ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ ({cleanup_count}ê°œ)")
            
            return {
                "success": True,
                "cleaned_models": cleanup_count,
                "step_name": self.step_name,
                "real_ai_models": True,
                "version": "16.0-compatible"
            }
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ AI ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "real_ai_models": True}

# =================================================================
# ğŸ”¥ í˜¸í™˜ì„± ì§€ì› í•¨ìˆ˜ë“¤ (ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜)
# =================================================================

async def create_pose_estimation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    strict_mode: bool = True,
    **kwargs
) -> PoseEstimationStep:
    """
    BaseStepMixin v16.0 í˜¸í™˜ ì‹¤ì œ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • Step ìƒì„± í•¨ìˆ˜
    
    Args:
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        strict_mode: ì—„ê²© ëª¨ë“œ
        **kwargs: ì¶”ê°€ ì„¤ì •
        
    Returns:
        PoseEstimationStep: ì´ˆê¸°í™”ëœ ì‹¤ì œ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • Step
    """
    try:
        # ë””ë°”ì´ìŠ¤ ì²˜ë¦¬
        device_param = None if device == "auto" else device
        
        # config í†µí•©
        if config is None:
            config = {}
        config.update(kwargs)
        config['real_ai_models'] = True
        config['basestep_version'] = '16.0-compatible'
        
        # Step ìƒì„± (BaseStepMixin v16.0 í˜¸í™˜ + ì‹¤ì œ AI ê¸°ë°˜)
        step = PoseEstimationStep(device=device_param, config=config, strict_mode=strict_mode)
        
        # ì‹¤ì œ AI ê¸°ë°˜ ì´ˆê¸°í™” ì‹¤í–‰
        initialization_success = await step.initialize()
        
        if not initialization_success:
            error_msg = "BaseStepMixin v16.0 í˜¸í™˜: ì‹¤ì œ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨"
            if strict_mode:
                raise RuntimeError(f"Strict Mode: {error_msg}")
            else:
                step.logger.warning(f"âš ï¸ {error_msg} - Step ìƒì„±ì€ ì™„ë£Œë¨")
        
        return step
        
    except Exception as e:
        logger.error(f"âŒ BaseStepMixin v16.0 í˜¸í™˜ create_pose_estimation_step ì‹¤íŒ¨: {e}")
        if strict_mode:
            raise
        else:
            step = PoseEstimationStep(device='cpu', strict_mode=False)
            return step

def create_pose_estimation_step_sync(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    strict_mode: bool = True,
    **kwargs
) -> PoseEstimationStep:
    """ë™ê¸°ì‹ BaseStepMixin v16.0 í˜¸í™˜ ì‹¤ì œ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • Step ìƒì„±"""
    try:
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            create_pose_estimation_step(device, config, strict_mode, **kwargs)
        )
    except Exception as e:
        logger.error(f"âŒ BaseStepMixin v16.0 í˜¸í™˜ create_pose_estimation_step_sync ì‹¤íŒ¨: {e}")
        if strict_mode:
            raise
        else:
            return PoseEstimationStep(device='cpu', strict_mode=False)

# =================================================================
# ğŸ”¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜)
# =================================================================

def validate_keypoints(keypoints_18: List[List[float]]) -> bool:
    """OpenPose 18 keypoints ìœ íš¨ì„± ê²€ì¦"""
    try:
        if len(keypoints_18) != 18:
            return False
        
        for kp in keypoints_18:
            if len(kp) != 3:
                return False
            if not all(isinstance(x, (int, float)) for x in kp):
                return False
            if kp[2] < 0 or kp[2] > 1:
                return False
        
        return True
        
    except Exception:
        return False

def convert_keypoints_to_coco(keypoints_18: List[List[float]]) -> List[List[float]]:
    """OpenPose 18ì„ COCO 17 í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        # OpenPose 18 -> COCO 17 ë§¤í•‘
        op_to_coco_mapping = {
            0: 0,   # nose
            15: 1,  # right_eye -> left_eye (COCO ê´€ì )
            16: 2,  # left_eye -> right_eye
            17: 3,  # right_ear -> left_ear
            18: 4,  # left_ear -> right_ear
            2: 5,   # right_shoulder -> left_shoulder (COCO ê´€ì )
            5: 6,   # left_shoulder -> right_shoulder
            3: 7,   # right_elbow -> left_elbow
            6: 8,   # left_elbow -> right_elbow
            4: 9,   # right_wrist -> left_wrist
            7: 10,  # left_wrist -> right_wrist
            9: 11,  # right_hip -> left_hip
            12: 12, # left_hip -> right_hip
            10: 13, # right_knee -> left_knee
            13: 14, # left_knee -> right_knee
            11: 15, # right_ankle -> left_ankle
            14: 16  # left_ankle -> right_ankle
        }
        
        coco_keypoints = []
        for coco_idx in range(17):
            if coco_idx in op_to_coco_mapping.values():
                op_idx = next(k for k, v in op_to_coco_mapping.items() if v == coco_idx)
                if op_idx < len(keypoints_18):
                    coco_keypoints.append(keypoints_18[op_idx].copy())
                else:
                    coco_keypoints.append([0.0, 0.0, 0.0])
            else:
                coco_keypoints.append([0.0, 0.0, 0.0])
        
        return coco_keypoints
        
    except Exception as e:
        logger.error(f"í‚¤í¬ì¸íŠ¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return [[0.0, 0.0, 0.0]] * 17

def draw_pose_on_image(
    image: Union[np.ndarray, Image.Image],
    keypoints: List[List[float]],
    confidence_threshold: float = 0.5,
    keypoint_size: int = 4,
    line_width: int = 3
) -> Image.Image:
    """ì´ë¯¸ì§€ì— í¬ì¦ˆ ê·¸ë¦¬ê¸°"""
    try:
        # ì´ë¯¸ì§€ ë³€í™˜
        if isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        else:
            pil_image = image.copy()
        
        draw = ImageDraw.Draw(pil_image)
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for i, kp in enumerate(keypoints):
            if len(kp) >= 3 and kp[2] > confidence_threshold:
                x, y = int(kp[0]), int(kp[1])
                color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                
                radius = int(keypoint_size + kp[2] * 6)
                draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                           fill=color, outline=(255, 255, 255), width=2)
        
        # ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸°
        for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
            if (start_idx < len(keypoints) and end_idx < len(keypoints)):
                start_kp = keypoints[start_idx]
                end_kp = keypoints[end_idx]
                
                if (len(start_kp) >= 3 and len(end_kp) >= 3 and
                    start_kp[2] > confidence_threshold and end_kp[2] > confidence_threshold):
                    
                    start_point = (int(start_kp[0]), int(start_kp[1]))
                    end_point = (int(end_kp[0]), int(end_kp[1]))
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    avg_confidence = (start_kp[2] + end_kp[2]) / 2
                    adjusted_width = int(line_width * avg_confidence)
                    
                    draw.line([start_point, end_point], fill=color, width=max(1, adjusted_width))
        
        return pil_image
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
        return image if isinstance(image, Image.Image) else Image.fromarray(image)

def analyze_pose_for_clothing(
    keypoints: List[List[float]],
    clothing_type: str = "default",
    confidence_threshold: float = 0.5,
    strict_analysis: bool = True
) -> Dict[str, Any]:
    """ì˜ë¥˜ë³„ í¬ì¦ˆ ì í•©ì„± ë¶„ì„"""
    try:
        if not keypoints:
            return {
                'suitable_for_fitting': False,
                'issues': ["í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"],
                'recommendations': ["ë” ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”"],
                'pose_score': 0.0,
                'ai_confidence': 0.0
            }
        
        # ì˜ë¥˜ë³„ ê°€ì¤‘ì¹˜
        clothing_weights = {
            'shirt': {'arms': 0.4, 'torso': 0.4, 'visibility': 0.2},
            'dress': {'torso': 0.5, 'arms': 0.3, 'legs': 0.2},
            'pants': {'legs': 0.6, 'torso': 0.3, 'visibility': 0.1},
            'jacket': {'arms': 0.5, 'torso': 0.4, 'visibility': 0.1},
            'skirt': {'torso': 0.4, 'legs': 0.4, 'visibility': 0.2},
            'top': {'torso': 0.5, 'arms': 0.4, 'visibility': 0.1},
            'default': {'torso': 0.4, 'arms': 0.3, 'legs': 0.2, 'visibility': 0.1}
        }
        
        weights = clothing_weights.get(clothing_type, clothing_weights['default'])
        
        # ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°
        def calculate_body_part_score(part_indices: List[int]) -> float:
            visible_count = 0
            total_confidence = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            
            return visibility_ratio * avg_confidence
        
        # ë¶€ìœ„ë³„ ì ìˆ˜
        head_indices = [0, 15, 16, 17, 18]
        torso_indices = [1, 2, 5, 8, 9, 12]
        arm_indices = [2, 3, 4, 5, 6, 7]
        leg_indices = [9, 10, 11, 12, 13, 14]
        
        head_score = calculate_body_part_score(head_indices)
        torso_score = calculate_body_part_score(torso_indices)
        arms_score = calculate_body_part_score(arm_indices)
        legs_score = calculate_body_part_score(leg_indices)
        
        # AI ì‹ ë¢°ë„
        ai_confidence = np.mean([kp[2] for kp in keypoints if len(kp) > 2]) if keypoints else 0.0
        
        # í¬ì¦ˆ ì ìˆ˜
        pose_score = (
            torso_score * weights.get('torso', 0.4) +
            arms_score * weights.get('arms', 0.3) +
            legs_score * weights.get('legs', 0.2) +
            weights.get('visibility', 0.1) * min(head_score, 1.0)
        ) * ai_confidence
        
        # ì í•©ì„± íŒë‹¨
        min_score = 0.8 if strict_analysis else 0.7
        min_confidence = 0.75 if strict_analysis else 0.65
        
        suitable_for_fitting = (pose_score >= min_score and 
                              ai_confidence >= min_confidence)
        
        # ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­
        issues = []
        recommendations = []
        
        if ai_confidence < min_confidence:
            issues.append(f'AI ëª¨ë¸ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({ai_confidence:.3f})')
            recommendations.append('ì¡°ëª…ì´ ì¢‹ì€ í™˜ê²½ì—ì„œ ë” ì„ ëª…í•˜ê²Œ ë‹¤ì‹œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        if torso_score < 0.5:
            issues.append(f'{clothing_type} ì°©ìš©ì— ì¤‘ìš”í•œ ìƒì²´ê°€ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤')
            recommendations.append('ìƒì²´ ì „ì²´ê°€ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
        
        return {
            'suitable_for_fitting': suitable_for_fitting,
            'issues': issues,
            'recommendations': recommendations,
            'pose_score': pose_score,
            'ai_confidence': ai_confidence,
            'detailed_scores': {
                'head': head_score,
                'torso': torso_score,
                'arms': arms_score,
                'legs': legs_score
            },
            'clothing_type': clothing_type,
            'weights_used': weights,
            'real_ai_analysis': True,
            'strict_analysis': strict_analysis
        }
        
    except Exception as e:
        logger.error(f"ì˜ë¥˜ë³„ í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            'suitable_for_fitting': False,
            'issues': ["ë¶„ì„ ì‹¤íŒ¨"],
            'recommendations': ["ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”"],
            'pose_score': 0.0,
            'ai_confidence': 0.0,
            'real_ai_analysis': True
        }

# =================================================================
# ğŸ”¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ (BaseStepMixin v16.0 í˜¸í™˜ + ì‹¤ì œ AI ê¸°ë°˜)
# =================================================================

async def test_pose_estimation_step():
    """BaseStepMixin v16.0 í˜¸í™˜ ì‹¤ì œ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ BaseStepMixin v16.0 í˜¸í™˜ ì‹¤ì œ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        print("=" * 80)
        
        # ì‹¤ì œ AI ê¸°ë°˜ Step ìƒì„±
        step = await create_pose_estimation_step(
            device="auto",
            strict_mode=True,
            config={
                'confidence_threshold': 0.5,
                'visualization_enabled': True,
                'cache_enabled': True,
                'detailed_analysis': True,
                'real_ai_models': True,
                'basestep_version': '16.0-compatible'
            }
        )
        
        # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
        dummy_image = np.zeros((512, 512, 3), dtype=np.uint8)
        dummy_image_pil = Image.fromarray(dummy_image)
        
        print(f"ğŸ“‹ BaseStepMixin v16.0 í˜¸í™˜ ì‹¤ì œ AI Step ì •ë³´:")
        step_status = step.get_status()
        print(f"   ğŸ¯ Step: {step_status['step_name']}")
        print(f"   ğŸ”¢ ë²„ì „: {step_status['version']}")
        print(f"   ğŸ¤– ë¡œë”©ëœ AI ëª¨ë¸ë“¤: {step_status.get('loaded_models', [])}")
        print(f"   ğŸ”’ Strict Mode: {step_status.get('strict_mode', False)}")
        print(f"   ğŸ’‰ ì˜ì¡´ì„± ì£¼ì…: {step_status.get('dependencies', {})}")
        print(f"   ğŸ’ ì´ˆê¸°í™” ìƒíƒœ: {step_status.get('is_initialized', False)}")
        print(f"   ğŸ§  ì‹¤ì œ AI ëª¨ë¸ ë¡œë“œ: {step_status.get('has_model', False)}")
        print(f"   ğŸ¤– ì‹¤ì œ AI ê¸°ë°˜: {step_status.get('real_ai_models', False)}")
        print(f"   ğŸ“¦ AI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥: {step_status.get('ai_libraries_available', {})}")
        print(f"   ğŸ“ AI ëª¨ë¸ ê²½ë¡œ: {step_status.get('model_paths', {})}")
        
        # ì‹¤ì œ AI ëª¨ë¸ë¡œ ì²˜ë¦¬
        result = await step.process(dummy_image_pil, clothing_type="shirt")
        
        if result['success']:
            print(f"âœ… BaseStepMixin v16.0 í˜¸í™˜ ì‹¤ì œ AI í¬ì¦ˆ ì¶”ì • ì„±ê³µ")
            print(f"ğŸ¯ ê²€ì¶œëœ í‚¤í¬ì¸íŠ¸ ìˆ˜: {len(result['keypoints'])}")
            print(f"ğŸ–ï¸ AI ì‹ ë¢°ë„: {result['pose_analysis']['ai_confidence']:.3f}")
            print(f"ğŸ’ í’ˆì§ˆ ì ìˆ˜: {result['pose_analysis']['quality_score']:.3f}")
            print(f"ğŸ‘• ì˜ë¥˜ ì í•©ì„±: {result['pose_analysis']['suitable_for_fitting']}")
            print(f"ğŸ¤– ì‚¬ìš©ëœ AI ëª¨ë¸ë“¤: {result['models_used']}")
            print(f"ğŸ† ì£¼ AI ëª¨ë¸: {result['primary_model']}")
            print(f"âš¡ ì¶”ë¡  ì‹œê°„: {result.get('inference_time', 0):.3f}ì´ˆ")
            print(f"ğŸ¨ Diffusion í–¥ìƒ: {result.get('enhanced_by_diffusion', False)}")
            print(f"ğŸ”— BaseStepMixin ë²„ì „: {result['step_info']['basestep_version']}")
            print(f"ğŸ¤– ì‹¤ì œ AI ê¸°ë°˜: {result['step_info']['real_ai_models']}")
            print(f"ğŸ“Š ì˜ì¡´ì„± ì£¼ì… ìƒíƒœ: {result['step_info']['dependency_injection_status']}")
        else:
            print(f"âŒ BaseStepMixin v16.0 í˜¸í™˜ ì‹¤ì œ AI í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {result.get('error', 'Unknown Error')}")
        
        # ì •ë¦¬
        cleanup_result = await step.cleanup()
        print(f"ğŸ§¹ BaseStepMixin v16.0 í˜¸í™˜ ì‹¤ì œ AI ë¦¬ì†ŒìŠ¤ ì •ë¦¬: {cleanup_result['success']}")
        print(f"ğŸ§¹ ì •ë¦¬ëœ AI ëª¨ë¸ ìˆ˜: {cleanup_result.get('cleaned_models', 0)}")
        
    except Exception as e:
        print(f"âŒ BaseStepMixin v16.0 í˜¸í™˜ ì‹¤ì œ AI í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

async def test_dependency_injection():
    """BaseStepMixin v16.0 ì‹¤ì œ AI ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì… í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ¤– BaseStepMixin v16.0 ì‹¤ì œ AI ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì… í†µí•© í…ŒìŠ¤íŠ¸")
        print("=" * 80)
        
        # ë™ì  import í•¨ìˆ˜ë“¤ í…ŒìŠ¤íŠ¸
        base_step_class = get_base_step_mixin_class()
        model_loader = get_model_loader()
        memory_manager = get_memory_manager()
        step_factory = get_step_factory()
        
        print(f"âœ… BaseStepMixin v16.0 ë™ì  import: {base_step_class is not None}")
        print(f"âœ… ModelLoader ë™ì  import: {model_loader is not None}")
        print(f"âœ… MemoryManager ë™ì  import: {memory_manager is not None}")
        print(f"âœ… StepFactory ë™ì  import: {step_factory is not None}")
        
        # ì‹¤ì œ AI ê¸°ë°˜ Step ìƒì„± ë° ì˜ì¡´ì„± ì£¼ì… í™•ì¸
        step = PoseEstimationStep(device="auto", strict_mode=True)
        
        print(f"ğŸ”— ì˜ì¡´ì„± ìƒíƒœ: {step.dependencies_injected}")
        print(f"ğŸ¤– ì‹¤ì œ AI ëª¨ë¸ ê²½ë¡œ: {step.model_paths}")
        
        # ìˆ˜ë™ ì˜ì¡´ì„± ì£¼ì… í…ŒìŠ¤íŠ¸
        if model_loader:
            step.set_model_loader(model_loader)
            print("âœ… ModelLoader ìˆ˜ë™ ì£¼ì… ì™„ë£Œ")
        
        if memory_manager:
            step.set_memory_manager(memory_manager)
            print("âœ… MemoryManager ìˆ˜ë™ ì£¼ì… ì™„ë£Œ")
        
        # ì‹¤ì œ AI ê¸°ë°˜ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        init_result = await step.initialize()
        print(f"ğŸš€ ì‹¤ì œ AI ê¸°ë°˜ ì´ˆê¸°í™” ì„±ê³µ: {init_result}")
        
        if init_result:
            final_status = step.get_status()
            print(f"ğŸ¯ ìµœì¢… ìƒíƒœ: {final_status['version']}")
            print(f"ğŸ“¦ ì˜ì¡´ì„± ì™„ë£Œ: {final_status['dependencies']}")
            print(f"ğŸ¤– ì‹¤ì œ AI ê¸°ë°˜: {final_status['real_ai_models']}")
            print(f"ğŸ§  ë¡œë”©ëœ AI ëª¨ë¸ë“¤: {final_status['loaded_models']}")
            print(f"ğŸ“ AI ëª¨ë¸ ê²½ë¡œë“¤: {final_status['model_paths']}")
        
        # ì •ë¦¬
        await step.cleanup()
        
    except Exception as e:
        print(f"âŒ BaseStepMixin v16.0 ì‹¤ì œ AI ê¸°ë°˜ ì˜ì¡´ì„± ì£¼ì… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_real_ai_models():
    """ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ§  ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # SmartModelPathMapper í…ŒìŠ¤íŠ¸
        try:
            mapper = Step02ModelMapper()
            model_paths = mapper.get_step02_model_paths()
            print(f"âœ… SmartModelPathMapper ë™ì‘: {len(model_paths)}ê°œ ê²½ë¡œ")
            for model_name, path in model_paths.items():
                status = "ì¡´ì¬" if path and path.exists() else "ì—†ìŒ"
                print(f"   {model_name}: {status} ({path})")
        except Exception as e:
            print(f"âŒ SmartModelPathMapper í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ë”ë¯¸ ëª¨ë¸ íŒŒì¼ë¡œ AI ëª¨ë¸ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸
        dummy_model_path = Path("dummy_model.pt")
        
        # RealYOLOv8PoseModel í…ŒìŠ¤íŠ¸
        try:
            yolo_model = RealYOLOv8PoseModel(dummy_model_path, "cpu")
            print(f"âœ… RealYOLOv8PoseModel ìƒì„± ì„±ê³µ: {yolo_model}")
            
            # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸ (ëª¨ë¸ ë¡œë”© ì—†ì´)
            dummy_image = Image.new('RGB', (256, 256), (128, 128, 128))
            if not yolo_model.loaded:
                print("âš ï¸ YOLOv8 ëª¨ë¸ ë¯¸ë¡œë”© ìƒíƒœ (ì˜ˆìƒë¨)")
        except Exception as e:
            print(f"âŒ RealYOLOv8PoseModel í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # RealOpenPoseModel í…ŒìŠ¤íŠ¸
        try:
            openpose_model = RealOpenPoseModel(dummy_model_path, "cpu")
            print(f"âœ… RealOpenPoseModel ìƒì„± ì„±ê³µ: {openpose_model}")
        except Exception as e:
            print(f"âŒ RealOpenPoseModel í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # RealDiffusionPoseModel í…ŒìŠ¤íŠ¸
        try:
            diffusion_model = RealDiffusionPoseModel(dummy_model_path, "cpu")
            print(f"âœ… RealDiffusionPoseModel ìƒì„± ì„±ê³µ: {diffusion_model}")
        except Exception as e:
            print(f"âŒ RealDiffusionPoseModel í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # RealBodyPoseModel í…ŒìŠ¤íŠ¸
        try:
            body_pose_model = RealBodyPoseModel(dummy_model_path, "cpu")
            print(f"âœ… RealBodyPoseModel ìƒì„± ì„±ê³µ: {body_pose_model}")
        except Exception as e:
            print(f"âŒ RealBodyPoseModel í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
    except Exception as e:
        print(f"âŒ ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_utilities():
    """ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”„ ìœ í‹¸ë¦¬í‹° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # ë”ë¯¸ OpenPose 18 í‚¤í¬ì¸íŠ¸
        openpose_keypoints = [
            [100, 50, 0.9],   # nose
            [100, 80, 0.8],   # neck
            [80, 100, 0.7],   # right_shoulder
            [70, 130, 0.6],   # right_elbow
            [60, 160, 0.5],   # right_wrist
            [120, 100, 0.7],  # left_shoulder
            [130, 130, 0.6],  # left_elbow
            [140, 160, 0.5],  # left_wrist
            [100, 200, 0.8],  # middle_hip
            [90, 200, 0.7],   # right_hip
            [85, 250, 0.6],   # right_knee
            [80, 300, 0.5],   # right_ankle
            [110, 200, 0.7],  # left_hip
            [115, 250, 0.6],  # left_knee
            [120, 300, 0.5],  # left_ankle
            [95, 40, 0.8],    # right_eye
            [105, 40, 0.8],   # left_eye
            [90, 45, 0.7],    # right_ear
            [110, 45, 0.7]    # left_ear
        ]
        
        # ìœ íš¨ì„± ê²€ì¦
        is_valid = validate_keypoints(openpose_keypoints)
        print(f"âœ… OpenPose 18 ìœ íš¨ì„±: {is_valid}")
        
        # COCO 17ë¡œ ë³€í™˜
        coco_keypoints = convert_keypoints_to_coco(openpose_keypoints)
        print(f"ğŸ”„ COCO 17 ë³€í™˜: {len(coco_keypoints)}ê°œ í‚¤í¬ì¸íŠ¸")
        
        # ì˜ë¥˜ë³„ ë¶„ì„
        analysis = analyze_pose_for_clothing(
            openpose_keypoints, 
            clothing_type="shirt",
            strict_analysis=True
        )
        print(f"ğŸ‘• ì˜ë¥˜ ì í•©ì„± ë¶„ì„:")
        print(f"   ì í•©ì„±: {analysis['suitable_for_fitting']}")
        print(f"   ì ìˆ˜: {analysis['pose_score']:.3f}")
        print(f"   AI ì‹ ë¢°ë„: {analysis['ai_confidence']:.3f}")
        print(f"   ì‹¤ì œ AI ë¶„ì„: {analysis['real_ai_analysis']}")
        
        # ì´ë¯¸ì§€ì— í¬ì¦ˆ ê·¸ë¦¬ê¸° í…ŒìŠ¤íŠ¸
        dummy_image = Image.new('RGB', (256, 256), (128, 128, 128))
        pose_image = draw_pose_on_image(dummy_image, openpose_keypoints)
        print(f"ğŸ–¼ï¸ í¬ì¦ˆ ê·¸ë¦¬ê¸°: {pose_image.size}")
        
    except Exception as e:
        print(f"âŒ ìœ í‹¸ë¦¬í‹° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# =================================================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸ (BaseStepMixin v16.0 í˜¸í™˜ + ì‹¤ì œ AI ê¸°ë°˜)
# =================================================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤ (ì‹¤ì œ AI ê¸°ë°˜ + íŒŒì´í”„ë¼ì¸ ì§€ì›)
    'PoseEstimationStep',
    'RealYOLOv8PoseModel',
    'RealOpenPoseModel',
    'RealHRNetModel',  # ìƒˆë¡œ ì¶”ê°€
    'RealDiffusionPoseModel',
    'RealBodyPoseModel',
    'SmartModelPathMapper',
    'Step02ModelMapper',
    'PoseMetrics',
    'PoseModel',
    'PoseQuality',
    
    # íŒŒì´í”„ë¼ì¸ ë°ì´í„° êµ¬ì¡°
    'PipelineStepResult',
    'PipelineInputData',
    
    # ìƒì„± í•¨ìˆ˜ë“¤ (BaseStepMixin v16.0 í˜¸í™˜ + ì‹¤ì œ AI ê¸°ë°˜)
    'create_pose_estimation_step',
    'create_pose_estimation_step_sync',
    
    # ë™ì  import í•¨ìˆ˜ë“¤
    'get_base_step_mixin_class',
    'get_model_loader',
    'get_memory_manager',
    'get_step_factory',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ì‹¤ì œ AI ê¸°ë°˜)
    'validate_keypoints',
    'convert_keypoints_to_coco',
    'draw_pose_on_image',
    'analyze_pose_for_clothing',
    
    # ìƒìˆ˜ë“¤
    'OPENPOSE_18_KEYPOINTS',
    'KEYPOINT_COLORS',
    'SKELETON_CONNECTIONS',
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ (BaseStepMixin v16.0 í˜¸í™˜ + ì‹¤ì œ AI ê¸°ë°˜)
    'test_pose_estimation_step',
    'test_dependency_injection',
    'test_real_ai_models',
    'test_utilities'
]

# =================================================================
# ğŸ”¥ ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê·¸ (BaseStepMixin v16.0 í˜¸í™˜ + ì‹¤ì œ AI ê¸°ë°˜)
# =================================================================

logger.info("ğŸ”¥ BaseStepMixin v16.0 + StepInterface í˜¸í™˜ ì‹¤ì œ AI ê¸°ë°˜ PoseEstimationStep v5.0 ë¡œë“œ ì™„ë£Œ")
logger.info("âœ… BaseStepMixin v16.0 + StepInterface ë‹¤ì¤‘ ìƒì† ì™„ì „ í˜¸í™˜")
logger.info("âœ… ì´ì¤‘ ê¸°ëŠ¥ ì§€ì›: ê°œë³„ ì‹¤í–‰ + íŒŒì´í”„ë¼ì¸ ì—°ê²°")
logger.info("âœ… TYPE_CHECKING íŒ¨í„´ìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
logger.info("âœ… StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… ì™„ì„±")
logger.info("ğŸ¤– ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (3.4GB): OpenPose, YOLOv8, HRNet, Diffusion, Body Pose")
logger.info("ğŸ”— SmartModelPathMapper í™œìš©í•œ ë™ì  íŒŒì¼ ê²½ë¡œ íƒì§€")
logger.info("ğŸ§  ì‹¤ì œ AI ì¶”ë¡  ì—”ì§„ êµ¬í˜„ (YOLOv8, OpenPose, HRNet, Diffusion)")
logger.info("ğŸ¯ 18ê°œ í‚¤í¬ì¸íŠ¸ ì™„ì „ ê²€ì¶œ ë° ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±")
logger.info("ğŸ”— íŒŒì´í”„ë¼ì¸ ì—°ê²°: Step 01 â†’ Step 02 â†’ Step 03, 04, 05, 06")
logger.info("ğŸ“Š PipelineStepResult ë°ì´í„° êµ¬ì¡° ì™„ì „ ì§€ì›")
logger.info("ğŸ M3 Max MPS ê°€ì† ìµœì í™”")
logger.info("ğŸ conda í™˜ê²½ ìš°ì„  ì§€ì›")
logger.info("âš¡ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ â†’ ì‹¤ì œ ì¶”ë¡ ")
logger.info("ğŸ¨ Diffusion ê¸°ë°˜ í¬ì¦ˆ í’ˆì§ˆ í–¥ìƒ")
logger.info("ğŸ“Š ì™„ì „í•œ í¬ì¦ˆ ë¶„ì„ - ê°ë„, ë¹„ìœ¨, ëŒ€ì¹­ì„±, ê°€ì‹œì„±, í’ˆì§ˆ í‰ê°€")
logger.info("ğŸ”’ Strict Mode ì§€ì› - ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì—ëŸ¬")
logger.info("ğŸš€ í”„ë¡œë•ì…˜ ë ˆë²¨ ì•ˆì •ì„± + ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜ + íŒŒì´í”„ë¼ì¸ ì§€ì›")# ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê¹…
logger.info(f"ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ: PyTorch={TORCH_AVAILABLE}, M3 Max={IS_M3_MAX}, Device={DEVICE}")
logger.info(f"ğŸ¤– AI ë¼ì´ë¸ŒëŸ¬ë¦¬: Ultralytics={ULTRALYTICS_AVAILABLE}, MediaPipe={MEDIAPIPE_AVAILABLE}, Transformers={TRANSFORMERS_AVAILABLE}")
logger.info(f"ğŸ”§ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „: PyTorch={TORCH_VERSION}")
logger.info(f"ğŸ’¾ Safetensors: {'í™œì„±í™”' if SAFETENSORS_AVAILABLE else 'ë¹„í™œì„±í™”'}")
logger.info(f"ğŸ”— BaseStepMixin v16.0 + StepInterface í˜¸í™˜: ì™„ì „í•œ ì˜ì¡´ì„± ì£¼ì… + íŒŒì´í”„ë¼ì¸ íŒ¨í„´")
logger.info(f"ğŸ¤– ì‹¤ì œ AI ê¸°ë°˜ ì—°ì‚°: ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ ëª¨ë¸ í´ë˜ìŠ¤ â†’ ì¶”ë¡  ì—”ì§„")
logger.info(f"ğŸ¯ ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ë“¤: YOLOv8 6.5MB, OpenPose 97.8MB, HRNet (ê³ ì •ë°€), Diffusion 1378MB, Body Pose 97.8MB")
logger.info(f"ğŸ”— íŒŒì´í”„ë¼ì¸ ì§€ì›: ê°œë³„ ì‹¤í–‰(process) + íŒŒì´í”„ë¼ì¸ ì—°ê²°(process_pipeline)")

# =================================================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ë¶€ (BaseStepMixin v16.0 + StepInterface í˜¸í™˜ + ì‹¤ì œ AI ê¸°ë°˜ ê²€ì¦)
# =================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¯ MyCloset AI Step 02 - BaseStepMixin v16.0 + StepInterface í˜¸í™˜ + ì‹¤ì œ AI ëª¨ë¸ ê¸°ë°˜")
    print("=" * 80)
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    async def run_all_tests():
        await test_pose_estimation_step()
        print("\n" + "=" * 80)
        await test_dependency_injection()
        print("\n" + "=" * 80)
        test_real_ai_models()
        print("\n" + "=" * 80)
        test_utilities()
        print("\n" + "=" * 80)
        await test_pipeline_functionality()
    
    async def test_pipeline_functionality():
        """íŒŒì´í”„ë¼ì¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        try:
            print("ğŸ”— íŒŒì´í”„ë¼ì¸ ì—°ê²° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
            print("=" * 60)
            
            # íŒŒì´í”„ë¼ì¸ìš© Step ìƒì„±
            step = await create_pose_estimation_step(
                device="auto",
                config={
                    'pipeline_mode': True,
                    'confidence_threshold': 0.5,
                    'real_ai_models': True
                }
            )
            
            # ë”ë¯¸ íŒŒì´í”„ë¼ì¸ ì…ë ¥ ë°ì´í„° ìƒì„±
            dummy_step01_result = PipelineStepResult(
                step_id=1,
                step_name="human_parsing",
                success=True,
                for_step_02={
                    "parsed_image": Image.new('RGB', (512, 512), (128, 128, 128)),
                    "body_masks": {"person": "dummy_mask"},
                    "human_region": {"bbox": [50, 50, 450, 450]}
                },
                for_step_03={
                    "person_parsing": "dummy_parsing",
                    "clothing_areas": "dummy_areas"
                },
                original_data={
                    "person_image": "original_image"
                }
            )
            
            print(f"ğŸ“‹ íŒŒì´í”„ë¼ì¸ Step ì •ë³´:")
            step_status = step.get_status()
            print(f"   ğŸ¯ Step: {step_status['step_name']}")
            print(f"   ğŸ”— íŒŒì´í”„ë¼ì¸ ëª¨ë“œ: {getattr(step, 'pipeline_mode', False)}")
            print(f"   ğŸ¤– ë¡œë”©ëœ AI ëª¨ë¸ë“¤: {step_status.get('loaded_models', [])}")
            
            # íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            pipeline_result = await step.process_pipeline(dummy_step01_result)
            
            if pipeline_result.success:
                print(f"âœ… íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì„±ê³µ")
                print(f"ğŸ¯ Step 03 ì „ë‹¬ ë°ì´í„°: {len(pipeline_result.for_step_03)}ê°œ í•­ëª©")
                print(f"ğŸ¯ Step 04 ì „ë‹¬ ë°ì´í„°: {len(pipeline_result.for_step_04)}ê°œ í•­ëª©")
                print(f"ğŸ¯ Step 05 ì „ë‹¬ ë°ì´í„°: {len(pipeline_result.for_step_05)}ê°œ í•­ëª©")
                print(f"ğŸ¯ Step 06 ì „ë‹¬ ë°ì´í„°: {len(pipeline_result.for_step_06)}ê°œ í•­ëª©")
                print(f"âš¡ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹œê°„: {pipeline_result.processing_time:.3f}ì´ˆ")
                print(f"ğŸ“Š ë©”íƒ€ë°ì´í„°: {pipeline_result.metadata.get('pipeline_progress', 'unknown')}")
            else:
                print(f"âŒ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {pipeline_result.error}")
            
            # ê°œë³„ ì²˜ë¦¬ë„ í…ŒìŠ¤íŠ¸
            individual_result = await step.process(
                Image.new('RGB', (512, 512), (128, 128, 128)),
                clothing_type="shirt"
            )
            
            if individual_result['success']:
                print(f"âœ… ê°œë³„ ì²˜ë¦¬ë„ ì •ìƒ ì‘ë™")
                print(f"ğŸ¯ í‚¤í¬ì¸íŠ¸ ìˆ˜: {len(individual_result['keypoints'])}")
            else:
                print(f"âŒ ê°œë³„ ì²˜ë¦¬ ì‹¤íŒ¨: {individual_result.get('error', 'Unknown')}")
            
            await step.cleanup()
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    try:
        asyncio.run(run_all_tests())
    except Exception as e:
        print(f"âŒ BaseStepMixin v16.0 + StepInterface í˜¸í™˜ ì‹¤ì œ AI ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 80)
    print("âœ¨ BaseStepMixin v16.0 + StepInterface í˜¸í™˜ + ì‹¤ì œ AI ê¸°ë°˜ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("ğŸ”— BaseStepMixin v16.0 + StepInterface ë‹¤ì¤‘ ìƒì† ì™„ì „ í˜¸í™˜")
    print("ğŸ”— ì´ì¤‘ ê¸°ëŠ¥ ì§€ì›: ê°œë³„ ì‹¤í–‰(process) + íŒŒì´í”„ë¼ì¸ ì—°ê²°(process_pipeline)")
    print("ğŸ¤– TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€")
    print("ğŸ”— StepFactory â†’ ModelLoader â†’ BaseStepMixin â†’ ì˜ì¡´ì„± ì£¼ì… ì™„ì„±")
    print("ğŸ§  ì‹¤ì œ AI ëª¨ë¸ íŒŒì¼ í™œìš© (3.4GB): OpenPose, YOLOv8, HRNet, Diffusion")
    print("âš¡ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© â†’ AI ëª¨ë¸ í´ë˜ìŠ¤ â†’ ì‹¤ì œ ì¶”ë¡ ")
    print("ğŸ¯ 18ê°œ í‚¤í¬ì¸íŠ¸ ì™„ì „ ê²€ì¶œ + ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±")
    print("ğŸ”— íŒŒì´í”„ë¼ì¸ ë°ì´í„° ì „ë‹¬: Step 01 â†’ Step 02 â†’ Step 03,04,05,06")
    print("ğŸ’‰ ì™„ë²½í•œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´")
    print("ğŸ”’ Strict Mode + ì™„ì „í•œ ì‹¤ì œ AI ê¸°ë°˜ ë¶„ì„ ê¸°ëŠ¥")
    print("ğŸ¯ ì‹¤ì œ AI ì—°ì‚° + ì§„ì§œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ + íŒŒì´í”„ë¼ì¸ ì§€ì›")
    print("=" * 80)