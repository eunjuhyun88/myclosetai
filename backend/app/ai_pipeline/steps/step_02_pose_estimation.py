# app/ai_pipeline/steps/step_02_pose_estimation.py
"""
2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • (Pose Estimation) - ìˆ˜ì •ëœ ë²„ì „
Pipeline Managerì™€ ì™„ì „ í˜¸í™˜ë˜ëŠ” ì‹¤ì œ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ
MediaPipe + OpenPose í˜¸í™˜ + M3 Max ìµœì í™”
"""
import os
import logging
import time
import asyncio
import json
from typing import Dict, Any, Optional, List, Tuple, Union
import numpy as np
import cv2
from PIL import Image
import torch
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

# MediaPipe (ì‹¤ì œ í¬ì¦ˆ ì¶”ì •ìš©)
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False
    logging.warning("MediaPipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëŒ€ì•ˆ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

logger = logging.getLogger(__name__)

class PoseEstimationStep:
    """
    í¬ì¦ˆ ì¶”ì • ìŠ¤í… - Pipeline Manager ì™„ì „ í˜¸í™˜
    - M3 Max MPS ìµœì í™”
    - MediaPipe ê¸°ë°˜ ì‹¤ì œ í¬ì¦ˆ ì¶”ì •
    - OpenPose 18 í‚¤í¬ì¸íŠ¸ í˜¸í™˜
    - ì‹¤ì‹œê°„ í’ˆì§ˆ ë¶„ì„
    """
    
    # OpenPose 18 í‚¤í¬ì¸íŠ¸ ì •ì˜
    OPENPOSE_18_KEYPOINTS = [
        "nose", "neck", "right_shoulder", "right_elbow", "right_wrist",
        "left_shoulder", "left_elbow", "left_wrist", "right_hip", "right_knee",
        "right_ankle", "left_hip", "left_knee", "left_ankle", "right_eye",
        "left_eye", "right_ear", "left_ear"
    ]
    
    def __init__(self, device: str, config: Optional[Dict[str, Any]] = None):
        """
        ì´ˆê¸°í™” - Pipeline Manager ì™„ì „ í˜¸í™˜
        
        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (mps, cuda, cpu)
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ì„ íƒì )
        """
        # model_loaderëŠ” ë‚´ë¶€ì—ì„œ ì „ì—­ í•¨ìˆ˜ë¡œ ê°€ì ¸ì˜´
        from app.ai_pipeline.utils.model_loader import get_global_model_loader
        self.model_loader = get_global_model_loader()
        
        self.device = device
        self.config = config or {}
        self.is_initialized = False
        
        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(__name__)
        
        # í¬ì¦ˆ ì¶”ì • ì„¤ì •
        self.pose_config = self.config.get('pose', {
            'model_complexity': 2,           # MediaPipe ëª¨ë¸ ë³µì¡ë„ (0, 1, 2)
            'min_detection_confidence': 0.7, # ìµœì†Œ ê²€ì¶œ ì‹ ë¢°ë„
            'min_tracking_confidence': 0.5,  # ìµœì†Œ ì¶”ì  ì‹ ë¢°ë„
            'enable_segmentation': False,     # ì„¸ê·¸ë©˜í…Œì´ì…˜ í™œì„±í™”
            'max_image_size': 1024,          # ìµœëŒ€ ì´ë¯¸ì§€ í¬ê¸°
            'use_face': True,                # ì–¼êµ´ í‚¤í¬ì¸íŠ¸ ì‚¬ìš©
            'use_hands': False               # ì† í‚¤í¬ì¸íŠ¸ ì‚¬ìš© (ì„±ëŠ¥ìƒ ë¹„í™œì„±í™”)
        })
        
        # MediaPipe ëª¨ë¸ ë³€ìˆ˜ë“¤
        self.mp_pose = None
        self.mp_drawing = None
        self.mp_drawing_styles = None
        self.pose_detector = None
        
        # í†µê³„ ë° ìºì‹œ
        self.processing_stats = {
            'total_processed': 0,
            'successful_detections': 0,
            'average_processing_time': 0.0,
            'last_quality_score': 0.0
        }
        
        self.logger.info(f"ğŸƒ í¬ì¦ˆ ì¶”ì • ìŠ¤í… ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {device}")
    
    async def initialize(self) -> bool:
        """ì´ˆê¸°í™” ë©”ì„œë“œ"""
        try:
            if MEDIAPIPE_AVAILABLE:
                # MediaPipe ì´ˆê¸°í™”
                self.mp_pose = mp.solutions.pose
                self.mp_drawing = mp.solutions.drawing_utils
                self.mp_drawing_styles = mp.solutions.drawing_styles
                
                # í¬ì¦ˆ ê²€ì¶œê¸° ì´ˆê¸°í™”
                self.pose_detector = self.mp_pose.Pose(
                    static_image_mode=True,
                    model_complexity=self.pose_config['model_complexity'],
                    enable_segmentation=self.pose_config['enable_segmentation'],
                    min_detection_confidence=self.pose_config['min_detection_confidence'],
                    min_tracking_confidence=self.pose_config['min_tracking_confidence']
                )
                
                self.logger.info("âœ… MediaPipe í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                # í´ë°±: ë”ë¯¸ ê²€ì¶œê¸°
                self.pose_detector = self._create_dummy_detector()
                self.logger.warning("âš ï¸ MediaPipe ì—†ìŒ - ë”ë¯¸ í¬ì¦ˆ ê²€ì¶œê¸° ì‚¬ìš©")
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ì—ëŸ¬ ì‹œ ë”ë¯¸ ê²€ì¶œê¸°ë¡œ í´ë°±
            self.pose_detector = self._create_dummy_detector()
            self.is_initialized = True
            return True
    
    async def process(self, person_image: Union[str, np.ndarray, Image.Image], **kwargs) -> Dict[str, Any]:
        """
        í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬
        
        Args:
            person_image: ì…ë ¥ ì´ë¯¸ì§€ (ê²½ë¡œ, numpy ë°°ì—´, PIL ì´ë¯¸ì§€)
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜
            
        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            if not self.is_initialized:
                await self.initialize()
            
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
            image_array = await self._load_and_preprocess_image(person_image)
            if image_array is None:
                return self._create_empty_result("ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
            
            # í¬ì¦ˆ ì¶”ì • ì‹¤í–‰
            if MEDIAPIPE_AVAILABLE and self.pose_detector:
                pose_result = await self._detect_pose_mediapipe(image_array)
            else:
                pose_result = await self._detect_pose_dummy(image_array)
            
            # OpenPose 18 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            keypoints_18 = self._convert_to_openpose_18(pose_result, image_array.shape)
            
            # í¬ì¦ˆ ë¶„ì„
            pose_analysis = self._analyze_pose(keypoints_18, image_array.shape)
            
            # í’ˆì§ˆ í‰ê°€
            quality_metrics = self._evaluate_pose_quality(keypoints_18, pose_result)
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_statistics(processing_time, quality_metrics['overall_confidence'])
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'success': True,
                'keypoints_18': keypoints_18,
                'keypoints_mediapipe': pose_result,
                'pose_confidence': quality_metrics['overall_confidence'],
                'body_orientation': pose_analysis['orientation'],
                'pose_analysis': pose_analysis,
                'quality_metrics': quality_metrics,
                'processing_info': {
                    'processing_time': processing_time,
                    'keypoints_detected': sum(1 for kp in keypoints_18 if kp[2] > 0.5),
                    'total_keypoints': 18,
                    'detection_method': 'mediapipe' if MEDIAPIPE_AVAILABLE else 'dummy'
                }
            }
            
            self.logger.info(f"âœ… í¬ì¦ˆ ì¶”ì • ì™„ë£Œ - ì‹ ë¢°ë„: {quality_metrics['overall_confidence']:.3f}, ì‹œê°„: {processing_time:.3f}ì´ˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return self._create_empty_result(f"ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
    
    async def _load_and_preprocess_image(self, image_input: Union[str, np.ndarray, Image.Image]) -> Optional[np.ndarray]:
        """ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            # ì…ë ¥ íƒ€ì…ì— ë”°ë¥¸ ë¡œë“œ
            if isinstance(image_input, str):
                if not os.path.exists(image_input):
                    self.logger.error(f"ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {image_input}")
                    return None
                image_array = cv2.imread(image_input)
                image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)
            elif isinstance(image_input, np.ndarray):
                image_array = image_input.copy()
                if len(image_array.shape) == 3 and image_array.shape[2] == 3:
                    # BGR to RGB ë³€í™˜ (OpenCV ì´ë¯¸ì§€ì¸ ê²½ìš°)
                    if image_array.max() > 1.0:  # 0-255 ë²”ìœ„ì¸ ê²½ìš°
                        image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)
            elif isinstance(image_input, Image.Image):
                image_array = np.array(image_input.convert('RGB'))
            else:
                self.logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {type(image_input)}")
                return None
            
            # í¬ê¸° ì¡°ì • (í•„ìš”í•œ ê²½ìš°)
            height, width = image_array.shape[:2]
            max_size = self.pose_config['max_image_size']
            
            if max(height, width) > max_size:
                if height > width:
                    new_height = max_size
                    new_width = int(width * max_size / height)
                else:
                    new_width = max_size
                    new_height = int(height * max_size / width)
                
                image_array = cv2.resize(image_array, (new_width, new_height), interpolation=cv2.INTER_AREA)
                self.logger.info(f"ğŸ”„ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •: ({width}, {height}) -> ({new_width}, {new_height})")
            
            return image_array
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    async def _detect_pose_mediapipe(self, image_array: np.ndarray) -> List[Dict]:
        """MediaPipeë¥¼ ì‚¬ìš©í•œ í¬ì¦ˆ ì¶”ì •"""
        try:
            # MediaPipe ì²˜ë¦¬
            results = self.pose_detector.process(image_array)
            
            if results.pose_landmarks:
                # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
                landmarks = []
                for landmark in results.pose_landmarks.landmark:
                    landmarks.append({
                        'x': landmark.x,
                        'y': landmark.y,
                        'z': landmark.z,
                        'visibility': landmark.visibility
                    })
                
                self.logger.debug(f"MediaPipe í‚¤í¬ì¸íŠ¸ ê²€ì¶œ: {len(landmarks)}ê°œ")
                return landmarks
            else:
                self.logger.warning("MediaPipeì—ì„œ í¬ì¦ˆë¥¼ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []
                
        except Exception as e:
            self.logger.error(f"MediaPipe í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return []
    
    async def _detect_pose_dummy(self, image_array: np.ndarray) -> List[Dict]:
        """ë”ë¯¸ í¬ì¦ˆ ì¶”ì • (í´ë°±ìš©)"""
        height, width = image_array.shape[:2]
        
        # ê°€ìƒì˜ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ìƒì„±
        dummy_landmarks = []
        
        # 33ê°œ MediaPipe í‚¤í¬ì¸íŠ¸ ìƒì„±
        for i in range(33):
            # ì´ë¯¸ì§€ ì¤‘ì•™ ê·¼ì²˜ì— ëœë¤í•˜ê²Œ ë°°ì¹˜
            x = 0.4 + np.random.random() * 0.2  # 40-60% ì§€ì 
            y = 0.3 + np.random.random() * 0.4  # 30-70% ì§€ì 
            z = np.random.random() * 0.1
            visibility = 0.7 + np.random.random() * 0.3  # 0.7-1.0
            
            dummy_landmarks.append({
                'x': x,
                'y': y,
                'z': z,
                'visibility': visibility
            })
        
        self.logger.debug("ë”ë¯¸ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ìƒì„± ì™„ë£Œ")
        return dummy_landmarks
    
    def _convert_to_openpose_18(self, mediapipe_landmarks: List[Dict], image_shape: Tuple) -> List[List[float]]:
        """MediaPipe í‚¤í¬ì¸íŠ¸ë¥¼ OpenPose 18 í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        
        height, width = image_shape[:2]
        keypoints_18 = [[0.0, 0.0, 0.0] for _ in range(18)]
        
        if not mediapipe_landmarks:
            return keypoints_18
        
        try:
            # MediaPipe -> OpenPose 18 ë§¤í•‘
            mp_to_op18 = {
                0: 0,   # nose
                12: 1,  # neck (ì–´ê¹¨ ì¤‘ì ìœ¼ë¡œ ê³„ì‚°)
                12: 2,  # right_shoulder
                14: 3,  # right_elbow
                16: 4,  # right_wrist
                11: 5,  # left_shoulder
                13: 6,  # left_elbow
                15: 7,  # left_wrist
                24: 8,  # right_hip
                26: 9,  # right_knee
                28: 10, # right_ankle
                23: 11, # left_hip
                25: 12, # left_knee
                27: 13, # left_ankle
                2: 14,  # right_eye
                5: 15,  # left_eye
                8: 16,  # right_ear
                7: 17   # left_ear
            }
            
            # ê¸°ë³¸ ë§¤í•‘
            for op_idx, mp_idx in mp_to_op18.items():
                if mp_idx < len(mediapipe_landmarks):
                    landmark = mediapipe_landmarks[mp_idx]
                    keypoints_18[op_idx] = [
                        landmark['x'] * width,
                        landmark['y'] * height,
                        landmark['visibility']
                    ]
            
            # ëª© (neck) ê³„ì‚° - ì–‘ìª½ ì–´ê¹¨ì˜ ì¤‘ì 
            if len(mediapipe_landmarks) > 12:
                left_shoulder = mediapipe_landmarks[11]
                right_shoulder = mediapipe_landmarks[12]
                
                neck_x = (left_shoulder['x'] + right_shoulder['x']) / 2 * width
                neck_y = (left_shoulder['y'] + right_shoulder['y']) / 2 * height
                neck_conf = min(left_shoulder['visibility'], right_shoulder['visibility'])
                
                keypoints_18[1] = [neck_x, neck_y, neck_conf]
            
            return keypoints_18
            
        except Exception as e:
            self.logger.error(f"í‚¤í¬ì¸íŠ¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return keypoints_18
    
    def _analyze_pose(self, keypoints_18: List[List[float]], image_shape: Tuple) -> Dict[str, Any]:
        """í¬ì¦ˆ ë¶„ì„"""
        
        analysis = {}
        
        # 1. ì‹ ì²´ ë°©í–¥ ì¶”ì •
        analysis["orientation"] = self._estimate_body_orientation(keypoints_18)
        
        # 2. ê´€ì ˆ ê°ë„ ê³„ì‚°
        analysis["angles"] = self._calculate_joint_angles(keypoints_18)
        
        # 3. ì‹ ì²´ ë¹„ìœ¨ ë¶„ì„
        analysis["proportions"] = self._analyze_body_proportions(keypoints_18)
        
        # 4. ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
        analysis["bbox"] = self._calculate_pose_bbox(keypoints_18, image_shape)
        
        # 5. í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜
        analysis["pose_type"] = self._classify_pose_type(keypoints_18, analysis["angles"])
        
        return analysis
    
    def _estimate_body_orientation(self, keypoints_18: List[List[float]]) -> str:
        """ì‹ ì²´ ë°©í–¥ ì¶”ì • (ì •ë©´/ì¸¡ë©´/ë’·ë©´)"""
        
        try:
            # ì–´ê¹¨ì™€ ì—‰ë©ì´ í‚¤í¬ì¸íŠ¸
            left_shoulder = keypoints_18[5]
            right_shoulder = keypoints_18[2]
            left_hip = keypoints_18[11]
            right_hip = keypoints_18[8]
            
            # ëª¨ë“  í‚¤í¬ì¸íŠ¸ê°€ ê²€ì¶œëœ ê²½ìš°ë§Œ
            if all(kp[2] > 0.5 for kp in [left_shoulder, right_shoulder, left_hip, right_hip]):
                
                # ì–´ê¹¨ ë„ˆë¹„ì™€ ì—‰ë©ì´ ë„ˆë¹„
                shoulder_width = abs(right_shoulder[0] - left_shoulder[0])
                hip_width = abs(right_hip[0] - left_hip[0])
                
                # í‰ê·  ë„ˆë¹„
                avg_width = (shoulder_width + hip_width) / 2
                
                # ì„ê³„ê°’ ê¸°ë°˜ ë¶„ë¥˜ (ì´ë¯¸ì§€ í¬ê¸°ì— ë¹„ë¡€)
                if avg_width < 80:
                    return "side"      # ì¸¡ë©´
                elif avg_width < 150:
                    return "diagonal"  # ëŒ€ê°ì„ 
                else:
                    return "front"     # ì •ë©´
            
            return "unknown"
            
        except Exception as e:
            logger.warning(f"ì‹ ì²´ ë°©í–¥ ì¶”ì • ì‹¤íŒ¨: {e}")
            return "unknown"
    
    def _calculate_joint_angles(self, keypoints_18: List[List[float]]) -> Dict[str, float]:
        """ì£¼ìš” ê´€ì ˆ ê°ë„ ê³„ì‚°"""
        
        def angle_between_points(p1, p2, p3):
            """ì„¸ ì ìœ¼ë¡œ ê°ë„ ê³„ì‚° (p2ê°€ ê¼­ì§€ì )"""
            if any(p[2] < 0.5 for p in [p1, p2, p3]):
                return 0.0
            
            # ë²¡í„° ê³„ì‚°
            v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
            v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
            
            # ê°ë„ ê³„ì‚°
            cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
            cos_angle = np.clip(cos_angle, -1.0, 1.0)
            angle = np.degrees(np.arccos(cos_angle))
            
            return float(angle)
        
        angles = {}
        
        try:
            # íŒ” ê°ë„
            angles["left_arm_angle"] = angle_between_points(
                keypoints_18[5], keypoints_18[6], keypoints_18[7]  # shoulder-elbow-wrist
            )
            angles["right_arm_angle"] = angle_between_points(
                keypoints_18[2], keypoints_18[3], keypoints_18[4]
            )
            
            # ë‹¤ë¦¬ ê°ë„
            angles["left_leg_angle"] = angle_between_points(
                keypoints_18[11], keypoints_18[12], keypoints_18[13]  # hip-knee-ankle
            )
            angles["right_leg_angle"] = angle_between_points(
                keypoints_18[8], keypoints_18[9], keypoints_18[10]
            )
            
            # ëª¸í†µ ê°ë„ (ì–´ê¹¨-ëª©-ì—‰ë©ì´)
            if all(keypoints_18[i][2] > 0.5 for i in [1, 5, 11]):
                angles["torso_angle"] = angle_between_points(
                    keypoints_18[5], keypoints_18[1], keypoints_18[11]
                )
            
        except Exception as e:
            logger.warning(f"ê´€ì ˆ ê°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        return angles
    
    def _analyze_body_proportions(self, keypoints_18: List[List[float]]) -> Dict[str, float]:
        """ì‹ ì²´ ë¹„ìœ¨ ë¶„ì„"""
        
        proportions = {}
        
        try:
            # ë¨¸ë¦¬ ê¸¸ì´ (ì½”-ëª©)
            if keypoints_18[0][2] > 0.5 and keypoints_18[1][2] > 0.5:
                head_length = abs(keypoints_18[1][1] - keypoints_18[0][1])
                proportions["head_length"] = head_length
            
            # ëª¸í†µ ê¸¸ì´ (ëª©-ì—‰ë©ì´)
            if keypoints_18[1][2] > 0.5 and keypoints_18[8][2] > 0.5:
                torso_length = abs(keypoints_18[8][1] - keypoints_18[1][1])
                proportions["torso_length"] = torso_length
            
            # ë‹¤ë¦¬ ê¸¸ì´ (ì—‰ë©ì´-ë°œëª©)
            if keypoints_18[8][2] > 0.5 and keypoints_18[10][2] > 0.5:
                leg_length = abs(keypoints_18[10][1] - keypoints_18[8][1])
                proportions["leg_length"] = leg_length
            
            # ì–´ê¹¨ ë„ˆë¹„
            if keypoints_18[2][2] > 0.5 and keypoints_18[5][2] > 0.5:
                shoulder_width = abs(keypoints_18[5][0] - keypoints_18[2][0])
                proportions["shoulder_width"] = shoulder_width
            
            # ë¹„ìœ¨ ê³„ì‚°
            if "head_length" in proportions and "torso_length" in proportions:
                proportions["head_to_torso_ratio"] = proportions["head_length"] / (proportions["torso_length"] + 1e-8)
            
            if "torso_length" in proportions and "leg_length" in proportions:
                proportions["torso_to_leg_ratio"] = proportions["torso_length"] / (proportions["leg_length"] + 1e-8)
                
        except Exception as e:
            logger.warning(f"ì‹ ì²´ ë¹„ìœ¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return proportions
    
    def _calculate_pose_bbox(self, keypoints_18: List[List[float]], image_shape: Tuple) -> Dict[str, int]:
        """í¬ì¦ˆ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
        
        # ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ë§Œ ì„ íƒ
        valid_points = [(x, y) for x, y, conf in keypoints_18 if conf > 0.5]
        
        if not valid_points:
            return {"x": 0, "y": 0, "width": 0, "height": 0}
        
        xs, ys = zip(*valid_points)
        
        x_min, x_max = min(xs), max(xs)
        y_min, y_max = min(ys), max(ys)
        
        # ì—¬ë°± ì¶”ê°€ (15%)
        margin_x = int((x_max - x_min) * 0.15)
        margin_y = int((y_max - y_min) * 0.15)
        
        height, width = image_shape
        
        bbox = {
            "x": max(0, int(x_min - margin_x)),
            "y": max(0, int(y_min - margin_y)),
            "width": min(width, int(x_max - x_min + 2 * margin_x)),
            "height": min(height, int(y_max - y_min + 2 * margin_y))
        }
        
        return bbox
    
    def _classify_pose_type(self, keypoints_18: List[List[float]], angles: Dict[str, float]) -> str:
        """í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜"""
        
        try:
            # íŒ” ê°ë„ ê¸°ë°˜ ë¶„ë¥˜
            left_arm = angles.get("left_arm_angle", 180)
            right_arm = angles.get("right_arm_angle", 180)
            
            # ë‹¤ë¦¬ ê°ë„
            left_leg = angles.get("left_leg_angle", 180)
            right_leg = angles.get("right_leg_angle", 180)
            
            # T-í¬ì¦ˆ (íŒ”ì´ ìˆ˜í‰)
            if abs(left_arm - 180) < 20 and abs(right_arm - 180) < 20:
                return "t_pose"
            
            # A-í¬ì¦ˆ (íŒ”ì´ ì•½ê°„ ì•„ë˜)
            elif 140 < left_arm < 170 and 140 < right_arm < 170:
                return "a_pose"
            
            # ê±·ê¸° í¬ì¦ˆ
            elif abs(left_leg - right_leg) > 30:
                return "walking"
            
            # ì•‰ê¸° í¬ì¦ˆ
            elif left_leg < 140 or right_leg < 140:
                return "sitting"
            
            # ì„œìˆëŠ” í¬ì¦ˆ (ê¸°ë³¸)
            else:
                return "standing"
                
        except Exception as e:
            logger.warning(f"í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return "unknown"
    
    def _evaluate_pose_quality(self, keypoints_18: List[List[float]], mediapipe_keypoints: List[Dict]) -> Dict[str, float]:
        """í¬ì¦ˆ í’ˆì§ˆ í‰ê°€"""
        
        try:
            # 1. ê²€ì¶œ ë¹„ìœ¨
            detected_18 = sum(1 for kp in keypoints_18 if kp[2] > 0.5)
            detection_rate = detected_18 / 18
            
            # 2. ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë¹„ìœ¨
            major_indices = [0, 1, 2, 5, 8, 11]  # nose, neck, shoulders, hips
            major_detected = sum(1 for idx in major_indices if keypoints_18[idx][2] > 0.5)
            major_detection_rate = major_detected / len(major_indices)
            
            # 3. í‰ê·  ì‹ ë¢°ë„
            confidences = [kp[2] for kp in keypoints_18 if kp[2] > 0]
            avg_confidence = np.mean(confidences) if confidences else 0.0
            
            # 4. ëŒ€ì¹­ì„± ì ìˆ˜
            symmetry_score = self._calculate_symmetry_score(keypoints_18)
            
            # 5. ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
            overall_confidence = (
                detection_rate * 0.3 +
                major_detection_rate * 0.3 +
                avg_confidence * 0.3 +
                symmetry_score * 0.1
            )
            
            return {
                'overall_confidence': overall_confidence,
                'detection_rate': detection_rate,
                'major_detection_rate': major_detection_rate,
                'average_confidence': avg_confidence,
                'symmetry_score': symmetry_score,
                'quality_grade': self._get_quality_grade(overall_confidence)
            }
            
        except Exception as e:
            logger.error(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'overall_confidence': 0.0,
                'detection_rate': 0.0,
                'major_detection_rate': 0.0,
                'average_confidence': 0.0,
                'symmetry_score': 0.0,
                'quality_grade': 'poor'
            }
    
    def _calculate_symmetry_score(self, keypoints_18: List[List[float]]) -> float:
        """ì¢Œìš° ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚°"""
        
        # ëŒ€ì¹­ í‚¤í¬ì¸íŠ¸ ìŒ
        symmetric_pairs = [
            (2, 5),   # shoulders
            (3, 6),   # elbows
            (4, 7),   # wrists
            (8, 11),  # hips
            (9, 12),  # knees
            (10, 13), # ankles
            (14, 15), # eyes
            (16, 17)  # ears
        ]
        
        symmetry_scores = []
        
        for right_idx, left_idx in symmetric_pairs:
            right_kp = keypoints_18[right_idx]
            left_kp = keypoints_18[left_idx]
            
            if right_kp[2] > 0.5 and left_kp[2] > 0.5:
                # ì‹ ë¢°ë„ ì°¨ì´
                conf_diff = abs(right_kp[2] - left_kp[2])
                conf_similarity = 1.0 - conf_diff
                
                # ìœ„ì¹˜ ëŒ€ì¹­ì„± (Y ì¢Œí‘œ ì°¨ì´)
                y_diff = abs(right_kp[1] - left_kp[1])
                max_y = max(right_kp[1], left_kp[1])
                if max_y > 0:
                    y_similarity = 1.0 - min(y_diff / max_y, 1.0)
                else:
                    y_similarity = 1.0
                
                # ì¢…í•© ëŒ€ì¹­ì„±
                pair_symmetry = (conf_similarity + y_similarity) / 2
                symmetry_scores.append(pair_symmetry)
        
        return np.mean(symmetry_scores) if symmetry_scores else 0.0
    
    def _get_quality_grade(self, overall_confidence: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if overall_confidence >= 0.9:
            return "excellent"
        elif overall_confidence >= 0.8:
            return "good"
        elif overall_confidence >= 0.6:
            return "fair"
        elif overall_confidence >= 0.4:
            return "poor"
        else:
            return "very_poor"
    
    def _create_dummy_detector(self):
        """ë”ë¯¸ ê²€ì¶œê¸° ìƒì„±"""
        class DummyDetector:
            def process(self, image):
                return None
        return DummyDetector()
    
    def _create_empty_result(self, reason: str) -> Dict[str, Any]:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        return {
            'success': False,
            'error': reason,
            'keypoints_18': [[0.0, 0.0, 0.0] for _ in range(18)],
            'keypoints_mediapipe': [],
            'pose_confidence': 0.0,
            'body_orientation': 'unknown',
            'pose_analysis': {},
            'quality_metrics': {
                'overall_confidence': 0.0,
                'quality_grade': 'failed'
            },
            'processing_info': {
                'processing_time': 0.0,
                'keypoints_detected': 0,
                'total_keypoints': 18,
                'detection_method': 'none'
            }
        }
    
    def _update_statistics(self, processing_time: float, quality_score: float):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.processing_stats['total_processed'] += 1
        
        if quality_score > 0.5:
            self.processing_stats['successful_detections'] += 1
        
        # ì´ë™ í‰ê· ìœ¼ë¡œ ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        alpha = 0.1
        self.processing_stats['average_processing_time'] = (
            alpha * processing_time + 
            (1 - alpha) * self.processing_stats['average_processing_time']
        )
        
        self.processing_stats['last_quality_score'] = quality_score
    
    def visualize_pose(self, image: np.ndarray, keypoints_18: List[List[float]], save_path: Optional[str] = None) -> np.ndarray:
        """í¬ì¦ˆ ì‹œê°í™”"""
        
        vis_image = image.copy()
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for i, (x, y, conf) in enumerate(keypoints_18):
            if conf > 0.5:
                cv2.circle(vis_image, (int(x), int(y)), 5, (0, 255, 0), -1)
                cv2.putText(vis_image, str(i), (int(x), int(y-10)), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)
        
        # ì—°ê²°ì„  ê·¸ë¦¬ê¸°
        connections = self._get_openpose_connections()
        for pt1_idx, pt2_idx in connections:
            pt1 = keypoints_18[pt1_idx]
            pt2 = keypoints_18[pt2_idx]
            
            if pt1[2] > 0.5 and pt2[2] > 0.5:
                cv2.line(vis_image, 
                        (int(pt1[0]), int(pt1[1])), 
                        (int(pt2[0]), int(pt2[1])), 
                        (255, 0, 0), 3)
        
        # ì €ì¥
        if save_path:
            cv2.imwrite(save_path, vis_image)
            logger.info(f"ğŸ’¾ í¬ì¦ˆ ì‹œê°í™” ì €ì¥: {save_path}")
        
        return vis_image
    
    def export_keypoints(self, keypoints_18: List[List[float]], format: str = "json") -> str:
        """í‚¤í¬ì¸íŠ¸ ë‚´ë³´ë‚´ê¸°"""
        
        if format.lower() == "json":
            export_data = {
                "format": "openpose_18",
                "keypoints": keypoints_18,
                "keypoint_names": self.OPENPOSE_18_KEYPOINTS,
                "connections": self._get_openpose_connections()
            }
            return json.dumps(export_data, indent=2)
        
        elif format.lower() == "csv":
            lines = ["id,name,x,y,confidence"]
            for i, (x, y, conf) in enumerate(keypoints_18):
                lines.append(f"{i},{self.OPENPOSE_18_KEYPOINTS[i]},{x},{y},{conf}")
            return "\n".join(lines)
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")
    
    def _get_openpose_connections(self) -> List[List[int]]:
        """OpenPose ì—°ê²°ì„  ì •ë³´"""
        return [
            # ëª¸í†µ
            [1, 2], [1, 5], [2, 8], [5, 11], [8, 11],
            
            # ì˜¤ë¥¸íŒ”
            [2, 3], [3, 4],
            
            # ì™¼íŒ”  
            [5, 6], [6, 7],
            
            # ì˜¤ë¥¸ë‹¤ë¦¬
            [8, 9], [9, 10],
            
            # ì™¼ë‹¤ë¦¬
            [11, 12], [12, 13],
            
            # ë¨¸ë¦¬
            [1, 0], [0, 14], [0, 15], [14, 16], [15, 17]
        ]
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.pose_detector:
            if hasattr(self.pose_detector, 'close'):
                self.pose_detector.close()
            self.pose_detector = None
        
        self.mp_pose = None
        self.mp_drawing = None
        self.is_initialized = False
        
        logger.info("ğŸ§¹ í¬ì¦ˆ ì¶”ì • ìŠ¤í… ì •ë¦¬ ì™„ë£Œ")