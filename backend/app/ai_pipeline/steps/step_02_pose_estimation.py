# app/ai_pipeline/steps/step_02_pose_estimation.py
"""
âœ… MyCloset AI - 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • (Pose Estimation) - ì™„ì „ ìˆ˜ì •ë³¸
âœ… ëª¨ë“  ì—ëŸ¬ í•´ê²° ë° logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°
âœ… BaseStepMixin ì™„ì „ í†µí•© + ModelLoader ì¸í„°í˜ì´ìŠ¤ ì™„ë²½ ì—°ë™
âœ… Pipeline Manager 100% í˜¸í™˜ + M3 Max 128GB ìµœì í™”
âœ… ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ 100% ìœ ì§€ (í´ë˜ìŠ¤ëª…, í•¨ìˆ˜ëª… ë™ì¼)
âœ… ì‹¤ì œ ì‘ë™í•˜ëŠ” ì™„ì „í•œ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ
âœ… 18ê°œ í‚¤í¬ì¸íŠ¸ OpenPose í˜¸í™˜ + ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›
âœ… ì™„ì „í•œ ì—ëŸ¬ ì²˜ë¦¬ ë° ìºì‹œ ê´€ë¦¬

íŒŒì¼ ìœ„ì¹˜: backend/app/ai_pipeline/steps/step_02_pose_estimation.py
"""

import os
import sys
import logging
import time
import asyncio
import threading
import json
import math
import gc
import hashlib
from typing import Dict, Any, Optional, Tuple, List, Union, Callable
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field, asdict
from enum import Enum
from functools import lru_cache
import numpy as np
import base64
import io

# í•„ìˆ˜ íŒ¨í‚¤ì§€ë“¤ - ì•ˆì „í•œ import
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    TORCH_AVAILABLE = True
except:
    TORCH_AVAILABLE = False
    print("âŒ PyTorch í•„ìˆ˜: pip install torch torchvision")

try:

    import cv2
    CV2_AVAILABLE = True
except:
    CV2_AVAILABLE = False
    print("âŒ OpenCV í•„ìˆ˜: pip install opencv-python")

try:

    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except:
    PIL_AVAILABLE = False
    print("âŒ Pillow í•„ìˆ˜: pip install Pillow")

try:

    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except:
    MEDIAPIPE_AVAILABLE = False
    print("âš ï¸ MediaPipe ê¶Œì¥: pip install mediapipe")

try:

    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except:
    YOLO_AVAILABLE = False
    print("âš ï¸ YOLOv8 ê¶Œì¥: pip install ultralytics")

try:

    import psutil
    PSUTIL_AVAILABLE = True
except:
    PSUTIL_AVAILABLE = False
    print("âš ï¸ psutil ê¶Œì¥: pip install psutil")

# ğŸ”¥ BaseStepMixin import (ì•ˆì „)
try:
    from app.ai_pipeline.steps.base_step_mixin import BaseStepMixin
    BASE_STEP_MIXIN_AVAILABLE = True
except:
    BASE_STEP_MIXIN_AVAILABLE = False
    # í´ë°± BaseStepMixin
    
    def _setup_model_precision:
    
        """M3 Max í˜¸í™˜ ì •ë°€ë„ ì„¤ì •"""
        try:
            if self.device == "mps":
                # M3 Maxì—ì„œëŠ” Float32ê°€ ì•ˆì „
                return model.float()
            elif self.device == "cuda" and hasattr(model, 'half'):
                return model.half()
            else:
                return model.float()
        except:
            self.logger.warning(f"âš ï¸ ì •ë°€ë„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return model.float()

class BaseStepMixin:

    def __init__(self, *args, **kwargs):
            # ğŸ”¥ í•µì‹¬: logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²°
            if:
                class_name = self.__class__.__name__
                self.logger = logging.getLogger(f"pipeline.{class_name}")
                self.logger.info(f"ğŸ”§ {class_name} í´ë°± logger ì´ˆê¸°í™” ì™„ë£Œ")
        
        def _setup_model_interface:
        
            pass

# ğŸ”¥ utils ëª¨ë“ˆ ì—°ë™ (ModelLoader ì¸í„°í˜ì´ìŠ¤)
try:
    from app.ai_pipeline.utils.model_loader import (
        get_global_model_loader, create_model_loader, ModelLoader
    )
    MODEL_LOADER_AVAILABLE = True
except:
    MODEL_LOADER_AVAILABLE = False
    print("âš ï¸ ModelLoader ì‚¬ìš© ë¶ˆê°€")

try:

    from app.ai_pipeline.utils.memory_manager import (
        get_global_memory_manager, MemoryManager
    )
    MEMORY_MANAGER_AVAILABLE = True
except:
    MEMORY_MANAGER_AVAILABLE = False
    print("âš ï¸ MemoryManager ì‚¬ìš© ë¶ˆê°€")

try:

    from app.ai_pipeline.utils.data_converter import (
        get_global_data_converter, DataConverter
    )
    DATA_CONVERTER_AVAILABLE = True
except:
    DATA_CONVERTER_AVAILABLE = False
    print("âš ï¸ DataConverter ì‚¬ìš© ë¶ˆê°€")

# ë¡œê±° ì„¤ì • (ëª¨ë“ˆ ë ˆë²¨)
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ì—´ê±°í˜• ë° ìƒìˆ˜ ì •ì˜
# ==============================================

class PoseModel:

    """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ íƒ€ì…"""
    MEDIAPIPE = "mediapipe"
    OPENPOSE = "openpose"
    YOLOV8 = "yolov8"
    LIGHTWEIGHT = "lightweight"

class PoseQuality:

    """í¬ì¦ˆ í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"     # 90-100ì 
    GOOD = "good"              # 75-89ì 
    ACCEPTABLE = "acceptable"   # 60-74ì 
    POOR = "poor"              # 40-59ì 
    VERY_POOR = "very_poor"    # 0-39ì 

class PoseType:

    """í¬ì¦ˆ íƒ€ì…"""
    T_POSE = "t_pose"          # Tì í¬ì¦ˆ
    A_POSE = "a_pose"          # Aì í¬ì¦ˆ  
    STANDING = "standing"      # ê¸°ë³¸ ì„œìˆê¸°
    SITTING = "sitting"        # ì•‰ê¸°
    WALKING = "walking"        # ê±·ê¸°
    ARMS_UP = "arms_up"        # íŒ” ì˜¬ë¦¬ê¸°
    UNKNOWN = "unknown"        # ì•Œ ìˆ˜ ì—†ìŒ

# OpenPose 18 í‚¤í¬ì¸íŠ¸ ì •ì˜
OPENPOSE_18_KEYPOINTS = {
    0: "nose",
    1: "neck", 
    2: "right_shoulder",
    3: "right_elbow",
    4: "right_wrist",
    5: "left_shoulder",
    6: "left_elbow", 
    7: "left_wrist",
    8: "mid_hip",
    9: "right_hip",
    10: "right_knee",
    11: "right_ankle",
    12: "left_hip",
    13: "left_knee",
    14: "left_ankle",
    15: "right_eye",
    16: "left_eye",
    17: "right_ear"
}

# ì‹œê°í™”ìš© ìƒ‰ìƒ ì •ì˜
KEYPOINT_COLORS = [
    (255, 0, 0),    # nose - ë¹¨ê°•
    (255, 85, 0),   # neck - ì£¼í™©
    (255, 170, 0),  # right_shoulder - ë…¸ë‘
    (255, 255, 0),  # right_elbow - ì—°ë…¸ë‘
    (170, 255, 0),  # right_wrist - ì—°ë‘
    (85, 255, 0),   # left_shoulder - ì´ˆë¡
    (0, 255, 0),    # left_elbow - ì§„ì´ˆë¡
    (0, 255, 85),   # left_wrist - ì²­ë¡
    (0, 255, 170),  # mid_hip - ì—°ì²­ë¡
    (0, 255, 255),  # right_hip - í•˜ëŠ˜
    (0, 170, 255),  # right_knee - ì—°íŒŒë‘
    (0, 85, 255),   # right_ankle - íŒŒë‘
    (0, 0, 255),    # left_hip - ì§„íŒŒë‘
    (85, 0, 255),   # left_knee - ë³´ë¼
    (170, 0, 255),  # left_ankle - ì—°ë³´ë¼
    (255, 0, 255),  # right_eye - ìí™
    (255, 0, 170),  # left_eye - ë¶„í™
    (255, 0, 85)    # right_ear - ì—°ë¶„í™
]

# ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²° ì •ì˜ (ë³¸ ì—°ê²°)
SKELETON_CONNECTIONS = [
    # ë¨¸ë¦¬-ëª©-ëª¸í†µ
    (0, 1),   # nose-neck
    (1, 2),   # neck-right_shoulder  
    (1, 5),   # neck-left_shoulder
    (2, 8),   # right_shoulder-mid_hip (ê°€ì •)
    (5, 8),   # left_shoulder-mid_hip (ê°€ì •)
    
    # ì˜¤ë¥¸íŒ”
    (2, 3),   # right_shoulder-right_elbow
    (3, 4),   # right_elbow-right_wrist
    
    # ì™¼íŒ”  
    (5, 6),   # left_shoulder-left_elbow
    (6, 7),   # left_elbow-left_wrist
    
    # ëª¸í†µ-ì—‰ë©ì´
    (8, 9),   # mid_hip-right_hip
    (8, 12),  # mid_hip-left_hip
    (9, 12),  # right_hip-left_hip
    
    # ì˜¤ë¥¸ë‹¤ë¦¬
    (9, 10),  # right_hip-right_knee
    (10, 11), # right_knee-right_ankle
    
    # ì™¼ë‹¤ë¦¬
    (12, 13), # left_hip-left_knee
    (13, 14), # left_knee-left_ankle
    
    # ì–¼êµ´
    (0, 15),  # nose-right_eye
    (0, 16),  # nose-left_eye
    (15, 17), # right_eye-right_ear
    (16, 17)  # left_eye-right_ear (ìˆ˜ì •)
]

SKELETON_COLORS = [
    (0, 255, 0),    # ì´ˆë¡ (ê¸°ë³¸)
    (255, 255, 0),  # ë…¸ë‘ (íŒ”)
    (255, 0, 255),  # ìí™ (ë‹¤ë¦¬)
    (0, 255, 255)   # í•˜ëŠ˜ (ì–¼êµ´)
]

# ==============================================
# ğŸ”¥ í¬ì¦ˆ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤
# ==============================================

@dataclass
class PoseMetrics:
    """í¬ì¦ˆ ë©”íŠ¸ë¦­"""
    
    # ê¸°ë³¸ í‚¤í¬ì¸íŠ¸ ì •ë³´
    keypoints_18: List[List[float]] = field(default_factory=lambda: [[0, 0, 0] for _ in range(18)])
    keypoints_detected: int = 0
    pose_confidence: float = 0.0
    
    # ì‹ ì²´ ë¹„ìœ¨
    total_height: float = 0.0
    torso_length: float = 0.0
    shoulder_width: float = 0.0
    hip_width: float = 0.0
    left_arm_length: float = 0.0
    right_arm_length: float = 0.0
    left_leg_length: float = 0.0
    right_leg_length: float = 0.0
    
    # í¬ì¦ˆ ê°ë„
    left_arm_angle: float = 0.0
    right_arm_angle: float = 0.0
    left_leg_angle: float = 0.0
    right_leg_angle: float = 0.0
    spine_angle: float = 0.0
    
    # í’ˆì§ˆ ë©”íŠ¸ë¦­
    detection_rate: float = 0.0
    major_keypoints_rate: float = 0.0
    average_confidence: float = 0.0
    symmetry_score: float = 0.0
    visibility_score: float = 0.0
    
    # í¬ì¦ˆ ë¶„ë¥˜
    pose_type: str = "unknown"
    quality_grade: str = "F"
    overall_score: float = 0.0
    
    # í”¼íŒ… ì í•©ì„±
    suitable_for_fitting: bool = False
    fitting_confidence: float = 0.0
    
    # ë©”íƒ€ë°ì´í„°
    detection_method: str = "unknown"
    processing_time: float = 0.0
    
    def calculate_overall_score:
    
        """ì „ì²´ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            scores = [
                self.detection_rate * 0.3,        # ê²€ì¶œë¥  30%
                self.average_confidence * 0.25,   # í‰ê·  ì‹ ë¢°ë„ 25%
                self.symmetry_score * 0.2,        # ëŒ€ì¹­ì„± 20%
                self.visibility_score * 0.15,     # ê°€ì‹œì„± 15%
                self.major_keypoints_rate * 0.1   # ì£¼ìš” í‚¤í¬ì¸íŠ¸ 10%
            ]
            
            self.overall_score = sum(scores)
            return self.overall_score
            
        except:
            
            logger.error(f"ì „ì²´ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            self.overall_score = 0.0
            return 0.0
    
    def get_quality_grade:
    
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if:
            self.quality_grade = "A+"
        elif self.overall_score >= 0.8:
            self.quality_grade = "A"
        elif self.overall_score >= 0.7:
            self.quality_grade = "B"
        elif self.overall_score >= 0.6:
            self.quality_grade = "C"
        elif self.overall_score >= 0.5:
            self.quality_grade = "D"
        else:
            self.quality_grade = "F"
        
        return self.quality_grade

# ==============================================
# ğŸ”¥ ë©”ì¸ PoseEstimationStep í´ë˜ìŠ¤
# ==============================================

class PoseEstimationStep:

    """
    âœ… 2ë‹¨ê³„: ì™„ì „í•œ í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ - ëª¨ë“  ì—ëŸ¬ í•´ê²°
    âœ… BaseStepMixin ì™„ì „ í†µí•© + logger ì†ì„± ëˆ„ë½ ì™„ì „ í•´ê²°
    âœ… ModelLoader ì¸í„°í˜ì´ìŠ¤ ì™„ë²½ ì—°ë™
    âœ… Pipeline Manager í˜¸í™˜ì„± 100%
    âœ… M3 Max ìµœì í™” + ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€
    """
    
    # ì˜ë¥˜ íƒ€ì…ë³„ í¬ì¦ˆ ê°€ì¤‘ì¹˜
    CLOTHING_POSE_WEIGHTS = {
        'shirt': {'arms': 0.4, 'torso': 0.4, 'visibility': 0.2},
        'dress': {'torso': 0.5, 'arms': 0.3, 'legs': 0.2},
        'pants': {'legs': 0.6, 'torso': 0.3, 'visibility': 0.1},
        'jacket': {'arms': 0.5, 'torso': 0.4, 'visibility': 0.1},
        'skirt': {'torso': 0.4, 'legs': 0.4, 'visibility': 0.2},
        'top': {'torso': 0.5, 'arms': 0.4, 'visibility': 0.1},
        'default': {'torso': 0.4, 'arms': 0.3, 'legs': 0.2, 'visibility': 0.1}
    }
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """âœ… ì™„ì „ ìˆ˜ì •ëœ ìƒì„±ì - ëª¨ë“  ì—ëŸ¬ í•´ê²°"""
        
        # ğŸ”¥ 1. logger ì†ì„± ëˆ„ë½ ë¬¸ì œ ì™„ì „ í•´ê²° - ìµœìš°ì„ 
        if:
            self.logger = logging.getLogger(f"pipeline.{self.__class__.__name__}")
            self.logger.info(f"ğŸ”§ {self.__class__.__name__} logger ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ğŸ”¥ 2. BaseStepMixin ì´ˆê¸°í™” (logger ì„¤ì • í›„)
        try:
            super().__init__()
        except:
            self.logger.warning(f"BaseStepMixin ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # 3. ê¸°ë³¸ ì„¤ì •
        self.device = self._auto_detect_device(device)
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.step_number = 2
        
        # 4. ì‹œìŠ¤í…œ ì •ë³´ ì¶”ì¶œ (kwargsì—ì„œ)
        self.device_type = kwargs.get('device_type', self._get_device_type())
        self.memory_gb = float(kwargs.get('memory_gb', self._get_memory_gb()))
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')
        
        # 5. ì„¤ì • ì—…ë°ì´íŠ¸
        self._update_config_from_kwargs(kwargs)
        
        # 6. ì´ˆê¸°í™” ìƒíƒœ
        self.is_initialized = False
        self.initialization_error = None
        self.performance_stats = {
            'total_processed': 0,
            'total_time': 0.0,
            'average_time': 0.0,
            'last_processing_time': 0.0,
            'average_confidence': 0.0,
            'peak_memory_usage': 0.0,
            'error_count': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        
        # 7. í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            self._initialize_step_specific()
            self._setup_model_loader_interface()
            self._setup_pose_models()
            self._setup_processing_pipeline()
            self.is_initialized = True
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ - M3 Max: {self.is_m3_max}")
        except:
            self.initialization_error = str(e)
            self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _auto_detect_device:
    
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ - M3 Max ìµœì í™”"""
        if:
            return device
        
        # M3 Max ê°ì§€
        if:
            try:
                if:
                    return "mps"
                elif torch.cuda.is_available():
                    return "cuda"
            except:
                self.logger.warning(f"ë””ë°”ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨: {e}")
        
        return "cpu"
    
    def _get_device_type:
    
        """ë””ë°”ì´ìŠ¤ íƒ€ì… ë°˜í™˜"""
        try:
            if:
                return "apple_silicon"
            elif self.device == "cuda":
                return "nvidia_gpu"
            else:
                return "cpu"
        except:
            self.logger.warning(f"ë””ë°”ì´ìŠ¤ íƒ€ì… ê°ì§€ ì‹¤íŒ¨: {e}")
            return "cpu"
    
    def _get_memory_gb:
    
        """ë©”ëª¨ë¦¬ í¬ê¸° ê°ì§€"""
        try:
            if:
                return psutil.virtual_memory().total / (1024**3)
            else:
                return 16.0  # ê¸°ë³¸ê°’
        except:
            self.logger.warning(f"ë©”ëª¨ë¦¬ ê°ì§€ ì‹¤íŒ¨: {e}")
            return 16.0
    
    def _detect_m3_max:
    
        """M3 Max ê°ì§€"""
        try:
            import platform
            if:
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                        capture_output=True, text=True, timeout=5)
                return "M3" in result.stdout and "Max" in result.stdout
        except:
            self.logger.debug(f"M3 Max ê°ì§€ ì‹¤íŒ¨: {e}")
            pass
        return False
    
    def _update_config_from_kwargs:
    
        """kwargsì—ì„œ config ì—…ë°ì´íŠ¸"""
        system_params = {
            'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level'
        }
        
        for key, value in kwargs.items():
            if:
                self.config[key] = value
    
    def _setup_model_loader_interface:
    
        """ğŸ”¥ ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • - ì™„ì „ ì•ˆì „"""
        try:
            # ModelLoader ì—°ë™
            if:
                try:
                    self.model_loader = get_global_model_loader()
                    if:
                        self.model_interface = self.model_loader.create_step_interface(self.step_name)
                    else:
                        self.model_interface = self.model_loader
                    
                    self.logger.info(f"ğŸ”— {self.step_name} ModelLoader ì¸í„°í˜ì´ìŠ¤ ì—°ë™ ì™„ë£Œ")
                except:
                    self.logger.warning(f"ModelLoader ì—°ë™ ì‹¤íŒ¨: {e}")
                    self.model_loader = None
                    self.model_interface = None
            else:
                self.model_loader = None
                self.model_interface = None
                self.logger.warning(f"âš ï¸ ModelLoader ì‚¬ìš© ë¶ˆê°€, ë‚´ì¥ ëª¨ë¸ ì‚¬ìš©")
                
            # Memory Manager ì—°ë™
            if:
                try:
                    self.memory_manager = get_global_memory_manager()
                except:
                    self.logger.warning(f"MemoryManager ì—°ë™ ì‹¤íŒ¨: {e}")
                    self.memory_manager = None
            else:
                self.memory_manager = None
            
            # Data Converter ì—°ë™
            if:
                try:
                    self.data_converter = get_global_data_converter()
                except:
                    self.logger.warning(f"DataConverter ì—°ë™ ì‹¤íŒ¨: {e}")
                    self.data_converter = None
            else:
                self.data_converter = None
                
        except:
                
            self.logger.error(f"âŒ ModelLoader ì¸í„°í˜ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.model_loader = None
            self.model_interface = None
            self.memory_manager = None
            self.data_converter = None
    
    def _initialize_step_specific:
    
        """2ë‹¨ê³„ ì „ìš© ì´ˆê¸°í™”"""
        
        # í¬ì¦ˆ ì¶”ì • ì„¤ì •
        self.pose_config = {
            'model_priority': self.config.get('model_priority', ['mediapipe', 'openpose', 'yolov8']),
            'confidence_threshold': self.config.get('confidence_threshold', 0.5),
            'visualization_enabled': self.config.get('visualization_enabled', True),
            'return_analysis': self.config.get('return_analysis', True),
            'cache_enabled': self.config.get('cache_enabled', True),
            'batch_processing': self.config.get('batch_processing', False),
            'detailed_analysis': self.config.get('detailed_analysis', False)
        }
        
        # ìµœì í™” ë ˆë²¨ ì„¤ì •
        if:
            self.optimization_level = 'maximum'
            self.batch_processing = True
            self.use_neural_engine = True
        elif self.memory_gb >= 32:
            self.optimization_level = 'high'
            self.batch_processing = True
            self.use_neural_engine = False
        else:
            self.optimization_level = 'basic'
            self.batch_processing = False
            self.use_neural_engine = False
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        cache_size = min(100 if self.is_m3_max else 50, int(self.memory_gb * 2))
        self.prediction_cache = {}
        self.cache_max_size = cache_size
        
        self.logger.info(f"ğŸ¯ 2ë‹¨ê³„ ì„¤ì • ì™„ë£Œ - ìµœì í™”: {self.optimization_level}")
    
    def _setup_pose_models:
    
        """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ë“¤ ì„¤ì •"""
        self.pose_models = {}
        self.active_model = None
        
        try:
            # 1. MediaPipe ì„¤ì •
            if:
                try:
                    self.pose_models['mediapipe'] = mp.solutions.pose.Pose(
                        static_image_mode=True,
                        model_complexity=2,
                        enable_segmentation=False,
                        min_detection_confidence=0.5,
                        min_tracking_confidence=0.5
                    )
                    self.logger.info("âœ… MediaPipe í¬ì¦ˆ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                except:
                    self.logger.warning(f"âš ï¸ MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # 2. YOLOv8 ì„¤ì • (ë°±ì—…)
            if:
                try:
                    # ê¸°ë³¸ YOLOv8 ëª¨ë¸ ë¡œë“œ
                    self.pose_models['yolov8'] = YOLO('yolov8n-pose.pt')
                    self.logger.info("âœ… YOLOv8 í¬ì¦ˆ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                except:
                    self.logger.warning(f"âš ï¸ YOLOv8 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # 3. ê¸°ë³¸ ëª¨ë¸ ì„ íƒ
            model_priority = self.pose_config['model_priority']
            for model_name in model_priority:
                if:
                    self.active_model = model_name
                    break
            
            if:
            
                self.logger.warning("âš ï¸ í¬ì¦ˆ ëª¨ë¸ ì—†ìŒ, ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘")
                self.active_model = 'simulation'
            else:
                self.logger.info(f"ğŸ¯ í™œì„± í¬ì¦ˆ ëª¨ë¸: {self.active_model}")
                
        except:
                
            self.logger.error(f"âŒ í¬ì¦ˆ ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.pose_models = {}
            self.active_model = 'simulation'
    
    def _setup_processing_pipeline:
    
        """í¬ì¦ˆ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
        
        # ì²˜ë¦¬ ìˆœì„œ ì •ì˜
        self.processing_pipeline = []
        
        # 1. ì „ì²˜ë¦¬
        self.processing_pipeline.append(('preprocessing', self._preprocess_for_pose))
        
        # 2. í¬ì¦ˆ ì¶”ì •
        self.processing_pipeline.append(('pose_estimation', self._perform_pose_estimation))
        
        # 3. í›„ì²˜ë¦¬
        self.processing_pipeline.append(('postprocessing', self._postprocess_pose_results))
        
        # 4. í’ˆì§ˆ ë¶„ì„
        if:
            self.processing_pipeline.append(('quality_analysis', self._analyze_pose_quality))
        
        # 5. ì‹œê°í™”
        if:
            self.processing_pipeline.append(('visualization', self._create_pose_visualization))
        
        self.logger.info(f"ğŸ”„ í¬ì¦ˆ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ - {len(self.processing_pipeline)}ë‹¨ê³„")
    
    # =================================================================
    # ğŸš€ ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ (Pipeline Manager í˜¸ì¶œ)
    # =================================================================
    
    async def process(
        self,
        person_image: Union[np.ndarray, str, Path],
        clothing_type: str = "default",
        **kwargs
    ) -> Dict[str, Any]:
        """
        âœ… ë©”ì¸ í¬ì¦ˆ ì¶”ì • í•¨ìˆ˜ - Pipeline Manager í‘œì¤€ ì¸í„°í˜ì´ìŠ¤
        
        Args:
            person_image: ì¸ë¬¼ ì´ë¯¸ì§€ (numpy array, íŒŒì¼ ê²½ë¡œ, PIL Image)
            clothing_type: ì˜ë¥˜ íƒ€ì… (ê°€ì¤‘ì¹˜ ì¡°ì •ìš©)
            **kwargs: ì¶”ê°€ ì„¤ì •
        
        Returns:
            Dict[str, Any]: í¬ì¦ˆ ì¶”ì • ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            # 1. ì´ˆê¸°í™” ê²€ì¦
            if:
                raise ValueError(f"PoseEstimationStepì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {self.initialization_error}")
            
            # 2. ì´ë¯¸ì§€ ë¡œë“œ ë° ê²€ì¦
            image = self._load_and_validate_image(person_image)
            if:
                raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ person_imageì…ë‹ˆë‹¤")
            
            # 3. ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(image, clothing_type, kwargs)
            if:
                self.logger.info("ğŸ“‹ ìºì‹œì—ì„œ í¬ì¦ˆ ì¶”ì • ê²°ê³¼ ë°˜í™˜")
                self.performance_stats['cache_hits'] += 1
                cached_result = self.prediction_cache[cache_key].copy()
                cached_result['from_cache'] = True
                return cached_result
            
            self.performance_stats['cache_misses'] += 1
            
            # 4. ë©”ëª¨ë¦¬ ìµœì í™”
            if:
                try:
                    await self._optimize_memory()
                except:
                    self.logger.debug(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            
            # 5. ë©”ì¸ í¬ì¦ˆ ì¶”ì • íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            pose_metrics = await self._execute_pose_pipeline(image, clothing_type, **kwargs)
            
            # 6. ê²°ê³¼ í›„ì²˜ë¦¬
            result = self._build_final_result(pose_metrics, clothing_type, time.time() - start_time)
            
            # 7. ìºì‹œ ì €ì¥
            if:
                self._save_to_cache(cache_key, result)
            
            # 8. í†µê³„ ì—…ë°ì´íŠ¸
            self._update_performance_stats(time.time() - start_time, pose_metrics.overall_score)
            
            self.logger.info(f"âœ… í¬ì¦ˆ ì¶”ì • ì™„ë£Œ - í‚¤í¬ì¸íŠ¸: {pose_metrics.keypoints_detected}/18, í’ˆì§ˆ: {pose_metrics.quality_grade}")
            return result
            
        except:
            
            error_msg = f"í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}"
            self.logger.error(f"âŒ {error_msg}")
            
            processing_time = time.time() - start_time
            self._update_performance_stats(processing_time, 0.0, success=False)
            
            return self._create_error_result(error_msg, processing_time)
    
    def _create_error_result:
    
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            "success": False,
            "step_name": self.step_name,
            "error": error_message,
            "processing_time": processing_time,
            "keypoints_18": [[0, 0, 0] for _ in range(18)],
            "pose_confidence": 0.0,
            "keypoints_detected": 0,
            "quality_grade": "F",
            "pose_analysis": {
                "detection_rate": 0.0,
                "quality_score": 0.0,
                "quality_grade": "F",
                "pose_type": "unknown",
                "suitable_for_fitting": False
            },
            "body_proportions": {},
            "pose_angles": {},
            "suitable_for_fitting": False,
            "fitting_confidence": 0.0,
            "keypoint_image": "",
            "skeleton_image": "", 
            "overlay_image": "",
            "from_cache": False,
            "device_info": {
                "device": self.device,
                "error_count": self.performance_stats.get('error_count', 0)
            }
        }
    
    # =================================================================
    # ğŸ”§ í¬ì¦ˆ ì¶”ì • í•µì‹¬ í•¨ìˆ˜ë“¤
    # =================================================================
    
    async def _execute_pose_pipeline(
        self,
        image: np.ndarray,
        clothing_type: str,
        **kwargs
    ) -> PoseMetrics:
        """í¬ì¦ˆ ì¶”ì • íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        metrics = PoseMetrics()
        intermediate_results = {}
        current_data = image
        
        self.logger.info(f"ğŸ”„ í¬ì¦ˆ ì¶”ì • íŒŒì´í”„ë¼ì¸ ì‹œì‘ - ì˜ë¥˜: {clothing_type}")
        
        for step_name, processor_func in self.processing_pipeline:
            try:
                step_start = time.time()
                
                # ë‹¨ê³„ë³„ ì²˜ë¦¬
                if:
                    current_data = await processor_func(current_data, **kwargs)
                elif step_name == 'pose_estimation':
                    step_result = await processor_func(current_data, **kwargs)
                    current_data = step_result
                elif step_name == 'postprocessing':
                    step_result = await processor_func(current_data, image.shape, **kwargs)
                    current_data = step_result
                elif step_name == 'quality_analysis':
                    analysis_result = await processor_func(current_data, clothing_type, **kwargs)
                    if:
                        current_data.update(analysis_result)
                elif step_name == 'visualization':
                    visualization_result = await processor_func(current_data, image, **kwargs)
                    if:
                        current_data.update(visualization_result)
                
                step_time = time.time() - step_start
                intermediate_results[step_name] = {
                    'processing_time': step_time,
                    'success': True
                }
                
                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                if:
                    for key, value in current_data.items():
                        if:
                            setattr(metrics, key, value)
                
                self.logger.debug(f"  âœ“ {step_name} ì™„ë£Œ - {step_time:.3f}ì´ˆ")
                
            except:
                
                self.logger.warning(f"  âš ï¸ {step_name} ì‹¤íŒ¨: {e}")
                intermediate_results[step_name] = {
                    'processing_time': 0,
                    'success': False,
                    'error': str(e)
                }
                continue
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        try:
            clothing_weights = self.CLOTHING_POSE_WEIGHTS.get(clothing_type, self.CLOTHING_POSE_WEIGHTS['default'])
            metrics.calculate_overall_score()
            metrics.get_quality_grade()
            
            # í”¼íŒ… ì í•©ì„± ê³„ì‚°
            metrics.suitable_for_fitting = (
                metrics.keypoints_detected >= 12 and
                metrics.detection_rate >= 0.7 and
                metrics.overall_score >= 0.6
            )
            metrics.fitting_confidence = min(metrics.overall_score * 1.2, 1.0)
        except:
            self.logger.warning(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        self.logger.info(f"âœ… í¬ì¦ˆ ì¶”ì • íŒŒì´í”„ë¼ì¸ ì™„ë£Œ - {len(intermediate_results)}ë‹¨ê³„ ì²˜ë¦¬")
        return metrics
    
    async def _preprocess_for_pose(self, image: np.ndarray, **kwargs) -> np.ndarray:
        """í¬ì¦ˆ ì¶”ì •ì„ ìœ„í•œ ì „ì²˜ë¦¬"""
        try:
            # 1. ì´ë¯¸ì§€ ì •ê·œí™”
            if:
                image = np.clip(image * 255, 0, 255).astype(np.uint8)
            
            # 2. í¬ê¸° ì¡°ì • (ëª¨ë¸ì— ë”°ë¼)
            target_size = kwargs.get('target_size', (512, 512))
            if self.active_model == 'mediapipe':
                # MediaPipeëŠ” ì›ë³¸ í¬ê¸° ìœ ì§€ ì„ í˜¸
                pass
            elif self.active_model == 'yolov8':
                # YOLOëŠ” 640x640 ì„ í˜¸
                target_size = (640, 640)
            
            if:
            
                scale = target_size[0] / max(image.shape[:2])
                new_h, new_w = int(image.shape[0] * scale), int(image.shape[1] * scale)
                if:
                    image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
            
            # 3. ìƒ‰ìƒ ê³µê°„ í™•ì¸
            if len(image.shape) == 3 and image.shape[2] == 3:
                # RGB ìˆœì„œ í™•ì¸ (MediaPipeëŠ” RGB, OpenCVëŠ” BGR)
                if self.active_model == 'mediapipe' and CV2_AVAILABLE:
                    # BGR to RGB ë³€í™˜ (OpenCV ì´ë¯¸ì§€ì¸ ê²½ìš°)
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            return image
            
        except:
            
            self.logger.error(f"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    async def _perform_pose_estimation(self, image: np.ndarray, **kwargs) -> Dict[str, Any]:
        """ì‹¤ì œ í¬ì¦ˆ ì¶”ì • ìˆ˜í–‰"""
        
        try:
        
            if:
        
                return await self._mediapipe_estimation(image, **kwargs)
            elif self.active_model == 'yolov8' and YOLO_AVAILABLE:
                return await self._yolov8_estimation(image, **kwargs)
            elif self.active_model == 'openpose':
                return await self._openpose_estimation(image, **kwargs)
            else:
                # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
                return await self._simulation_estimation(image, **kwargs)
                
        except:
                
            self.logger.error(f"í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return await self._simulation_estimation(image, **kwargs)
    
    async def _mediapipe_estimation(self, image: np.ndarray, **kwargs) -> Dict[str, Any]:
        """MediaPipe í¬ì¦ˆ ì¶”ì •"""
        try:
            model = self.pose_models['mediapipe']
            
            # MediaPipe ì¶”ë¡ 
            results = model.process(image)
            
            if results.pose_landmarks:
                # ëœë“œë§ˆí¬ë¥¼ OpenPose 18 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                keypoints_18 = self._convert_mediapipe_to_openpose(results.pose_landmarks, image.shape)
                
                # ì‹ ë¢°ë„ ê³„ì‚°
                confidences = [kp[2] for kp in keypoints_18]
                pose_confidence = np.mean([c for c in confidences if c > 0])
                keypoints_detected = sum(1 for c in confidences if c > self.pose_config['confidence_threshold'])
                
                return {
                    'keypoints_18': keypoints_18,
                    'pose_confidence': float(pose_confidence),
                    'keypoints_detected': keypoints_detected,
                    'pose_angles': self._calculate_pose_angles(keypoints_18),
                    'body_proportions': self._calculate_body_proportions(keypoints_18),
                    'detection_method': 'mediapipe'
                }
            else:
                # ê²€ì¶œ ì‹¤íŒ¨
                return {
                    'keypoints_18': [[0, 0, 0] for _ in range(18)],
                    'pose_confidence': 0.0,
                    'keypoints_detected': 0,
                    'pose_angles': {},
                    'body_proportions': {},
                    'detection_method': 'mediapipe_failed'
                }
                
        except:
                
            self.logger.error(f"MediaPipe í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return await self._simulation_estimation(image, **kwargs)
    
    async def _yolov8_estimation(self, image: np.ndarray, **kwargs) -> Dict[str, Any]:
        """YOLOv8 í¬ì¦ˆ ì¶”ì •"""
        try:
            model = self.pose_models['yolov8']
            
            # YOLO ì¶”ë¡ 
            results = model(image, verbose=False)
            
            if results and len(results) > 0 and results[0].keypoints is not None:
                # ì²« ë²ˆì§¸ ì‚¬ëŒì˜ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
                keypoints = results[0].keypoints.data[0].cpu().numpy()  # [17, 3] COCO format
                
                # COCO 17ì„ OpenPose 18ë¡œ ë³€í™˜
                keypoints_18 = self._convert_coco_to_openpose(keypoints, image.shape)
                
                # ì‹ ë¢°ë„ ê³„ì‚°
                confidences = [kp[2] for kp in keypoints_18]
                pose_confidence = np.mean([c for c in confidences if c > 0])
                keypoints_detected = sum(1 for c in confidences if c > self.pose_config['confidence_threshold'])
                
                return {
                    'keypoints_18': keypoints_18,
                    'pose_confidence': float(pose_confidence),
                    'keypoints_detected': keypoints_detected,
                    'pose_angles': self._calculate_pose_angles(keypoints_18),
                    'body_proportions': self._calculate_body_proportions(keypoints_18),
                    'detection_method': 'yolov8'
                }
            else:
                # ê²€ì¶œ ì‹¤íŒ¨
                return {
                    'keypoints_18': [[0, 0, 0] for _ in range(18)],
                    'pose_confidence': 0.0,
                    'keypoints_detected': 0,
                    'pose_angles': {},
                    'body_proportions': {},
                    'detection_method': 'yolov8_failed'
                }
                
        except:
                
            self.logger.error(f"YOLOv8 í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return await self._simulation_estimation(image, **kwargs)
    
    async def _openpose_estimation(self, image: np.ndarray, **kwargs) -> Dict[str, Any]:
        """OpenPose í¬ì¦ˆ ì¶”ì • (ModelLoader í†µí•©)"""
        try:
            # ModelLoader ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ OpenPose ëª¨ë¸ ë¡œë“œ
            if:
                openpose_model = await self._get_model_safe("pose_estimation_openpose")
                
                if openpose_model and TORCH_AVAILABLE:
                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                    tensor_input = self._manual_preprocess_for_openpose(image)
                    
                    # ëª¨ë¸ ì¶”ë¡ 
                    with torch.no_grad():
                        if:
                            with autocast(device_type='cpu', dtype=torch.float16):
                                output = openpose_model(tensor_input)
                        else:
                            output = openpose_model(tensor_input)
                    
                    # í›„ì²˜ë¦¬
                    keypoints_18 = self._postprocess_openpose_output(output, image.shape)
                    
                    # ë©”íŠ¸ë¦­ ê³„ì‚°
                    confidences = [kp[2] for kp in keypoints_18]
                    pose_confidence = np.mean([c for c in confidences if c > 0])
                    keypoints_detected = sum(1 for c in confidences if c > self.pose_config['confidence_threshold'])
                    
                    return {
                        'keypoints_18': keypoints_18,
                        'pose_confidence': float(pose_confidence),
                        'keypoints_detected': keypoints_detected,
                        'pose_angles': self._calculate_pose_angles(keypoints_18),
                        'body_proportions': self._calculate_body_proportions(keypoints_18),
                        'detection_method': 'openpose'
                    }
            
            # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì‹œë®¬ë ˆì´ì…˜
            return await self._simulation_estimation(image, **kwargs)
            
        except:
            
            self.logger.error(f"OpenPose í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return await self._simulation_estimation(image, **kwargs)
    
    async def _simulation_estimation(self, image: np.ndarray, **kwargs) -> Dict[str, Any]:
        """ì‹œë®¬ë ˆì´ì…˜ í¬ì¦ˆ ì¶”ì • (í´ë°±)"""
        try:
            h, w = image.shape[:2]
            
            # ì‹œë®¬ë ˆì´ì…˜ëœ í‚¤í¬ì¸íŠ¸ ìƒì„± (í•´ë¶€í•™ì ìœ¼ë¡œ íƒ€ë‹¹í•œ ìœ„ì¹˜)
            keypoints_18 = []
            
            # ê¸°ë³¸ ì¸ì²´ ë¹„ìœ¨ ì‚¬ìš©
            head_y = h * 0.15
            neck_y = h * 0.20
            shoulder_y = h * 0.25
            elbow_y = h * 0.40
            wrist_y = h * 0.55
            hip_y = h * 0.55
            knee_y = h * 0.75
            ankle_y = h * 0.95
            
            center_x = w * 0.5
            shoulder_width = w * 0.15
            hip_width = w * 0.12
            
            # 18ê°œ í‚¤í¬ì¸íŠ¸ ì‹œë®¬ë ˆì´ì…˜
            simulated_points = [
                [center_x, head_y, 0.95],                    # 0: nose
                [center_x, neck_y, 0.90],                    # 1: neck
                [center_x + shoulder_width, shoulder_y, 0.85], # 2: right_shoulder
                [center_x + shoulder_width * 1.5, elbow_y, 0.80], # 3: right_elbow
                [center_x + shoulder_width * 1.8, wrist_y, 0.75], # 4: right_wrist
                [center_x - shoulder_width, shoulder_y, 0.85], # 5: left_shoulder
                [center_x - shoulder_width * 1.5, elbow_y, 0.80], # 6: left_elbow
                [center_x - shoulder_width * 1.8, wrist_y, 0.75], # 7: left_wrist
                [center_x, hip_y, 0.90],                      # 8: mid_hip
                [center_x + hip_width, hip_y, 0.85],          # 9: right_hip
                [center_x + hip_width, knee_y, 0.80],         # 10: right_knee
                [center_x + hip_width, ankle_y, 0.75],        # 11: right_ankle
                [center_x - hip_width, hip_y, 0.85],          # 12: left_hip
                [center_x - hip_width, knee_y, 0.80],         # 13: left_knee
                [center_x - hip_width, ankle_y, 0.75],        # 14: left_ankle
                [center_x + 10, head_y - 20, 0.70],           # 15: right_eye
                [center_x - 10, head_y - 20, 0.70],           # 16: left_eye
                [center_x + 15, head_y - 10, 0.65]            # 17: right_ear
            ]
            
            # ì¢Œí‘œ ì •ìˆ˜ ë³€í™˜ ë° ë²”ìœ„ ì²´í¬
            for point in simulated_points:
                point[0] = max(0, min(w-1, int(point[0])))
                point[1] = max(0, min(h-1, int(point[1])))
            
            keypoints_18 = simulated_points[:18]  # 18ê°œë§Œ ì‚¬ìš©
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            confidences = [kp[2] for kp in keypoints_18]
            pose_confidence = np.mean(confidences)
            keypoints_detected = len([c for c in confidences if c > self.pose_config['confidence_threshold']])
            
            return {
                'keypoints_18': keypoints_18,
                'pose_confidence': float(pose_confidence),
                'keypoints_detected': keypoints_detected,
                'pose_angles': self._calculate_pose_angles(keypoints_18),
                'body_proportions': self._calculate_body_proportions(keypoints_18),
                'detection_method': 'simulation'
            }
            
        except:
            
            self.logger.error(f"ì‹œë®¬ë ˆì´ì…˜ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return {
                'keypoints_18': [[0, 0, 0] for _ in range(18)],
                'pose_confidence': 0.0,
                'keypoints_detected': 0,
                'pose_angles': {},
                'body_proportions': {},
                'detection_method': 'failed'
            }
    
    # =================================================================
    # ğŸ”§ í›„ì²˜ë¦¬ ë° ë¶„ì„ í•¨ìˆ˜ë“¤
    # =================================================================
    
    async def _postprocess_pose_results(self, pose_results: Dict[str, Any], image_shape: Tuple[int, int], **kwargs) -> Dict[str, Any]:
        """í¬ì¦ˆ ê²°ê³¼ í›„ì²˜ë¦¬"""
        try:
            # 1. í‚¤í¬ì¸íŠ¸ ì •ê·œí™” ë° ê²€ì¦
            keypoints_18 = pose_results.get('keypoints_18', [[0, 0, 0] for _ in range(18)])
            keypoints_18 = self._validate_and_normalize_keypoints(keypoints_18, image_shape)
            
            # 2. ì¶”ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
            detection_rate = pose_results.get('keypoints_detected', 0) / 18.0
            major_keypoints_rate = self._calculate_major_keypoints_rate(keypoints_18)
            average_confidence = pose_results.get('pose_confidence', 0.0)
            symmetry_score = self._calculate_symmetry_score(keypoints_18)
            visibility_score = self._calculate_visibility_score(keypoints_18)
            
            # 3. ê²°ê³¼ ì—…ë°ì´íŠ¸
            pose_results.update({
                'keypoints_18': keypoints_18,
                'detection_rate': detection_rate,
                'major_keypoints_rate': major_keypoints_rate,
                'average_confidence': average_confidence,
                'symmetry_score': symmetry_score,
                'visibility_score': visibility_score
            })
            
            return pose_results
            
        except:
            
            self.logger.error(f"í¬ì¦ˆ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return pose_results
    
    async def _analyze_pose_quality(self, pose_results: Dict[str, Any], clothing_type: str, **kwargs) -> Dict[str, Any]:
        """í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„"""
        try:
            keypoints_18 = pose_results.get('keypoints_18', [[0, 0, 0] for _ in range(18)])
            
            # 1. ê¸°ë³¸ í’ˆì§ˆ ë©”íŠ¸ë¦­
            quality_metrics = {
                'detection_rate': pose_results.get('detection_rate', 0.0),
                'major_keypoints_rate': pose_results.get('major_keypoints_rate', 0.0),
                'average_confidence': pose_results.get('average_confidence', 0.0),
                'symmetry_score': pose_results.get('symmetry_score', 0.0),
                'visibility_score': pose_results.get('visibility_score', 0.0)
            }
            
            # 2. í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜
            pose_angles = pose_results.get('pose_angles', {})
            pose_type = self._classify_pose_type(keypoints_18, pose_angles)
            
            # 3. ì˜ë¥˜ë³„ ì í•©ì„± í‰ê°€
            clothing_weights = self.CLOTHING_POSE_WEIGHTS.get(clothing_type, self.CLOTHING_POSE_WEIGHTS['default'])
            clothing_score = self._calculate_clothing_specific_score(keypoints_18, clothing_weights)
            
            # 4. ì „ì²´ ì ìˆ˜ ê³„ì‚°
            overall_score = (
                quality_metrics['detection_rate'] * 0.3 +
                quality_metrics['average_confidence'] * 0.25 +
                quality_metrics['symmetry_score'] * 0.2 +
                quality_metrics['visibility_score'] * 0.15 +
                clothing_score * 0.1
            )
            
            # 5. ë“±ê¸‰ ê²°ì •
            if:
                quality_grade = "A+"
            elif overall_score >= 0.8:
                quality_grade = "A"
            elif overall_score >= 0.7:
                quality_grade = "B"
            elif overall_score >= 0.6:
                quality_grade = "C"
            elif overall_score >= 0.5:
                quality_grade = "D"
            else:
                quality_grade = "F"
            
            # 6. í”¼íŒ… ì í•©ì„±
            suitable_for_fitting = (
                quality_metrics['detection_rate'] >= 0.7 and
                overall_score >= 0.6 and
                pose_type not in ['unknown', 'sitting']
            )
            
            return {
                'pose_type': pose_type,
                'overall_score': overall_score,
                'quality_grade': quality_grade,
                'suitable_for_fitting': suitable_for_fitting,
                'fitting_confidence': min(overall_score * 1.2, 1.0),
                'quality_metrics': quality_metrics,
                'clothing_score': clothing_score
            }
            
        except:
            
            self.logger.error(f"í¬ì¦ˆ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'pose_type': 'unknown',
                'overall_score': 0.0,
                'quality_grade': 'F',
                'suitable_for_fitting': False,
                'fitting_confidence': 0.0
            }
    
    async def _create_pose_visualization(self, pose_results: Dict[str, Any], original_image: np.ndarray, **kwargs) -> Dict[str, Any]:
        """í¬ì¦ˆ ì‹œê°í™” ìƒì„±"""
        try:
            if:
                return pose_results
            
            keypoints_18 = pose_results.get('keypoints_18', [[0, 0, 0] for _ in range(18)])
            
            # 1. í‚¤í¬ì¸íŠ¸ë§Œ í‘œì‹œí•œ ì´ë¯¸ì§€
            keypoint_image = self._draw_keypoints_only(original_image.copy(), keypoints_18)
            
            # 2. ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²° ì´ë¯¸ì§€
            skeleton_image = self._draw_skeleton(original_image.copy(), keypoints_18)
            
            # 3. ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ (ì›ë³¸ + í‚¤í¬ì¸íŠ¸ + ìŠ¤ì¼ˆë ˆí†¤)
            overlay_image = self._draw_full_pose_overlay(original_image.copy(), keypoints_18)
            
            # 4. ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            visualization_results = {}
            
            if:
            
                try:
                    # í‚¤í¬ì¸íŠ¸ ì´ë¯¸ì§€
                    pil_keypoint = Image.fromarray(keypoint_image)
                    keypoint_buffer = io.BytesIO()
                    pil_keypoint.save(keypoint_buffer, format='PNG')
                    visualization_results['keypoint_image'] = base64.b64encode(keypoint_buffer.getvalue()).decode()
                    
                    # ìŠ¤ì¼ˆë ˆí†¤ ì´ë¯¸ì§€
                    pil_skeleton = Image.fromarray(skeleton_image)
                    skeleton_buffer = io.BytesIO()
                    pil_skeleton.save(skeleton_buffer, format='PNG')
                    visualization_results['skeleton_image'] = base64.b64encode(skeleton_buffer.getvalue()).decode()
                    
                    # ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€
                    pil_overlay = Image.fromarray(overlay_image)
                    overlay_buffer = io.BytesIO()
                    pil_overlay.save(overlay_buffer, format='PNG')
                    visualization_results['overlay_image'] = base64.b64encode(overlay_buffer.getvalue()).decode()
                    
                except:
                    
                    self.logger.warning(f"ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
                    visualization_results = {
                        'keypoint_image': "",
                        'skeleton_image': "",
                        'overlay_image': ""
                    }
            else:
                visualization_results = {
                    'keypoint_image': "",
                    'skeleton_image': "",
                    'overlay_image': ""
                }
            
            # ê²°ê³¼ì— ì‹œê°í™” ì¶”ê°€
            pose_results.update(visualization_results)
            
            return pose_results
            
        except:
            
            self.logger.error(f"í¬ì¦ˆ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            pose_results.update({
                'keypoint_image': "",
                'skeleton_image': "",
                'overlay_image': ""
            })
            return pose_results
    
    # =================================================================
    # ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    # =================================================================
    
    async def _get_model_safe(self, model_name: str) -> Optional[Any]:
        """ì•ˆì „í•œ ëª¨ë¸ ë¡œë“œ"""
        try:
            if:
                return await self.model_interface.get_model(model_name)
            else:
                return None
        except:
            self.logger.debug(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            return None
    
    async def _optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            if:
                await self.memory_manager.optimize_memory_usage()
            elif TORCH_AVAILABLE and self.device == "mps":
                torch.mps.empty_cache()
            elif TORCH_AVAILABLE and self.device == "cuda":
                torch.cuda.empty_cache()
            
            gc.collect()
        except:
            self.logger.debug(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _load_and_validate_image:
    
        """ì´ë¯¸ì§€ ë¡œë“œ ë° ê²€ì¦"""
        try:
            if:
                image = image_input
            elif isinstance(image_input, (str, Path)):
                if:
                    pil_img = Image.open(image_input)
                    image = np.array(pil_img.convert('RGB'))
                elif CV2_AVAILABLE:
                    image = cv2.imread(str(image_input))
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                else:
                    raise ImportError("PIL ë˜ëŠ” OpenCVê°€ í•„ìš”í•©ë‹ˆë‹¤")
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_input)}")
            
            # ê²€ì¦
            if:
                raise ValueError("RGB ì´ë¯¸ì§€ì—¬ì•¼ í•©ë‹ˆë‹¤")
            
            if:
            
                raise ValueError("ë¹ˆ ì´ë¯¸ì§€ì…ë‹ˆë‹¤")
            
            return image
            
        except:
            
            self.logger.error(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_cache_key:
    
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            # ì´ë¯¸ì§€ í•´ì‹œ
            image_hash = hashlib.md5(image.tobytes()).hexdigest()[:16]
            
            # ì„¤ì • í•´ì‹œ
            config_data = {
                'clothing_type': clothing_type,
                'confidence_threshold': self.pose_config['confidence_threshold'],
                'active_model': self.active_model,
                **{k: v for k, v in kwargs.items() if isinstance(v, (str, int, float, bool))}
            }
            config_str = json.dumps(config_data, sort_keys=True)
            config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]
            
            return f"pose_{image_hash}_{config_hash}"
            
        except:
            
            return f"pose_fallback_{time.time()}"
    
    def _save_to_cache:
    
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        try:
            if len(self.prediction_cache) >= self.cache_max_size:
                # LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = min(self.prediction_cache.keys())
                del self.prediction_cache[oldest_key]
                self.logger.debug(f"ìºì‹œ í•­ëª© ì œê±°: {oldest_key}")
            
            # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì‹œê°í™” ì´ë¯¸ì§€ëŠ” ìºì‹œì—ì„œ ì œì™¸
            cached_result = result.copy()
            for viz_key in ['keypoint_image', 'skeleton_image', 'overlay_image']:
                if:
                    cached_result[viz_key] = ""
            
            self.prediction_cache[cache_key] = cached_result
            self.logger.debug(f"ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_key}")
            
        except:
            
            self.logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def clear_cache:
    
        """ìºì‹œ ì™„ì „ ì‚­ì œ"""
        try:
            if:
                cache_size = len(self.prediction_cache)
                self.prediction_cache.clear()
                self.logger.info(f"âœ… ìºì‹œ ì‚­ì œ ì™„ë£Œ: {cache_size}ê°œ í•­ëª©")
                return {"success": True, "cleared_items": cache_size}
            else:
                return {"success": True, "cleared_items": 0}
        except:
            self.logger.error(f"âŒ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def get_cache_status:
    
        """ìºì‹œ ìƒíƒœ ì¡°íšŒ"""
        try:
            if:
                return {
                    "cache_enabled": self.pose_config.get('cache_enabled', False),
                    "current_size": len(self.prediction_cache),
                    "max_size": self.cache_max_size,
                    "hit_rate": self.performance_stats['cache_hits'] / max(1, self.performance_stats['cache_hits'] + self.performance_stats['cache_misses']),
                    "cache_hits": self.performance_stats['cache_hits'],
                    "cache_misses": self.performance_stats['cache_misses']
                }
            else:
                return {"cache_enabled": False, "current_size": 0}
        except:
            self.logger.error(f"ìºì‹œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _update_performance_stats:
    
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            if:
                self.performance_stats['total_processed'] += 1
                self.performance_stats['total_time'] += processing_time
                self.performance_stats['average_time'] = (
                    self.performance_stats['total_time'] / self.performance_stats['total_processed']
                )
                
                # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
                current_avg = self.performance_stats.get('average_confidence', 0.0)
                total_processed = self.performance_stats['total_processed']
                self.performance_stats['average_confidence'] = (
                    (current_avg * (total_processed - 1) + confidence_score) / total_processed
                )
            else:
                self.performance_stats['error_count'] += 1
            
            self.performance_stats['last_processing_time'] = processing_time
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì  (M3 Max)
            if:
                try:
                    memory_usage = psutil.virtual_memory().percent
                    self.performance_stats['peak_memory_usage'] = max(
                        self.performance_stats.get('peak_memory_usage', 0),
                        memory_usage
                    )
                except:
                    self.logger.debug(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì  ì‹¤íŒ¨: {e}")
            
        except:
            
            self.logger.warning(f"ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _build_final_result:
    
        """ìµœì¢… ê²°ê³¼ êµ¬ì„±"""
        
        try:
        
            return {
                "success": True,
                "step_name": self.step_name,
                "processing_time": processing_time,
                
                # í•µì‹¬ í¬ì¦ˆ ë°ì´í„°
                "keypoints_18": metrics.keypoints_18,
                "pose_confidence": float(metrics.pose_confidence),
                "keypoints_detected": metrics.keypoints_detected,
                
                # ì‹ ì²´ ì¸¡ì • ë°ì´í„°
                "body_proportions": {
                    "total_height": float(metrics.total_height),
                    "torso_length": float(metrics.torso_length),
                    "shoulder_width": float(metrics.shoulder_width),
                    "hip_width": float(metrics.hip_width),
                    "left_arm_length": float(metrics.left_arm_length),
                    "right_arm_length": float(metrics.right_arm_length),
                    "left_leg_length": float(metrics.left_leg_length),
                    "right_leg_length": float(metrics.right_leg_length)
                },
                
                # í¬ì¦ˆ ê°ë„
                "pose_angles": {
                    "left_arm_angle": float(metrics.left_arm_angle),
                    "right_arm_angle": float(metrics.right_arm_angle),
                    "left_leg_angle": float(metrics.left_leg_angle),
                    "right_leg_angle": float(metrics.right_leg_angle),
                    "spine_angle": float(metrics.spine_angle)
                },
                
                # í’ˆì§ˆ ë¶„ì„
                "pose_analysis": {
                    "detection_rate": float(metrics.detection_rate),
                    "major_keypoints_rate": float(metrics.major_keypoints_rate),
                    "average_confidence": float(metrics.average_confidence),
                    "symmetry_score": float(metrics.symmetry_score),
                    "visibility_score": float(metrics.visibility_score),
                    "pose_type": metrics.pose_type,
                    "quality_grade": metrics.quality_grade,
                    "overall_score": float(metrics.overall_score)
                },
                
                # í”¼íŒ… ì í•©ì„±
                "suitable_for_fitting": metrics.suitable_for_fitting,
                "fitting_confidence": float(metrics.fitting_confidence),
                
                # ë©”íƒ€ë°ì´í„°
                "clothing_type": clothing_type,
                "detection_method": getattr(metrics, 'detection_method', self.active_model or 'unknown'),
                
                # ì‹œìŠ¤í…œ ì •ë³´
                "device_info": {
                    "device": self.device,
                    "device_type": self.device_type,
                    "is_m3_max": self.is_m3_max,
                    "memory_gb": self.memory_gb,
                    "optimization_level": self.optimization_level,
                    "active_model": self.active_model
                },
                
                # ì„±ëŠ¥ í†µê³„
                "performance_stats": self.performance_stats.copy(),
                
                # ì‹œê°í™” ì´ë¯¸ì§€ë“¤ (create_pose_visualizationì—ì„œ ì¶”ê°€ë¨)
                "keypoint_image": "",
                "skeleton_image": "", 
                "overlay_image": "",
                
                "from_cache": False
            }
        except:
            self.logger.error(f"ìµœì¢… ê²°ê³¼ êµ¬ì„± ì‹¤íŒ¨: {e}")
            return self._create_error_result(f"ê²°ê³¼ êµ¬ì„± ì‹¤íŒ¨: {e}", processing_time)
    
    # =================================================================
    # ğŸ”§ í¬ì¦ˆ ë¶„ì„ ë° ë³€í™˜ ìœ í‹¸ë¦¬í‹°ë“¤
    # =================================================================
    
    def _validate_and_normalize_keypoints:
    
        """í‚¤í¬ì¸íŠ¸ ê²€ì¦ ë° ì •ê·œí™”"""
        try:
            h, w = image_shape[:2]
            normalized_keypoints = []
            
            for i, kp in enumerate(keypoints_18):
                if:
                    x, y, conf = float(kp[0]), float(kp[1]), float(kp[2])
                    
                    # ì¢Œí‘œ ë²”ìœ„ ì²´í¬
                    x = max(0, min(w-1, x))
                    y = max(0, min(h-1, y))
                    
                    # ì‹ ë¢°ë„ ë²”ìœ„ ì²´í¬
                    conf = max(0.0, min(1.0, conf))
                    
                    normalized_keypoints.append([x, y, conf])
                else:
                    normalized_keypoints.append([0.0, 0.0, 0.0])
            
            # 18ê°œ í‚¤í¬ì¸íŠ¸ ë³´ì¥
            while len(normalized_keypoints) < 18:
                normalized_keypoints.append([0.0, 0.0, 0.0])
            
            return normalized_keypoints[:18]
            
        except:
            
            self.logger.error(f"í‚¤í¬ì¸íŠ¸ ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return [[0.0, 0.0, 0.0] for _ in range(18)]
    
    def _calculate_major_keypoints_rate:
    
        """ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê²€ì¶œë¥  ê³„ì‚°"""
        try:
            # ì£¼ìš” í‚¤í¬ì¸íŠ¸: ì½”, ëª©, ì–´ê¹¨, ì—‰ë©ì´, ë¬´ë¦
            major_indices = [0, 1, 2, 5, 8, 9, 10, 12, 13]
            detected_major = sum(1 for idx in major_indices if keypoints_18[idx][2] > self.pose_config['confidence_threshold'])
            return detected_major / len(major_indices)
        except:
            self.logger.debug(f"ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_symmetry_score:
    
        """ì‹ ì²´ ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            symmetry_pairs = [
                (2, 5),   # ì–´ê¹¨
                (3, 6),   # íŒ”ê¿ˆì¹˜
                (4, 7),   # ì†ëª©
                (9, 12),  # ì—‰ë©ì´
                (10, 13), # ë¬´ë¦
                (11, 14), # ë°œëª©
                (15, 16)  # ëˆˆ
            ]
            
            symmetry_scores = []
            
            for left_idx, right_idx in symmetry_pairs:
                if:
                    left_kp = keypoints_18[left_idx]
                    right_kp = keypoints_18[right_idx]
                    
                    if left_kp[2] > 0.5 and right_kp[2] > 0.5:
                        # Y ì¢Œí‘œ ì°¨ì´ë¡œ ëŒ€ì¹­ì„± ê³„ì‚° (ìˆ˜í‰ ëŒ€ì¹­)
                        y_diff = abs(left_kp[1] - right_kp[1])
                        max_y = max(left_kp[1], right_kp[1])
                        if:
                            symmetry = 1.0 - min(y_diff / max_y, 1.0)
                            symmetry_scores.append(symmetry)
            
            return np.mean(symmetry_scores) if symmetry_scores else 0.5
            
        except:
            
            self.logger.warning(f"ëŒ€ì¹­ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_visibility_score:
    
        """ê°€ì‹œì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            visible_count = sum(1 for kp in keypoints_18 if kp[2] > 0.3)
            return visible_count / 18.0
        except:
            self.logger.debug(f"ê°€ì‹œì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_pose_angles:
    
        """í¬ì¦ˆ ê°ë„ ê³„ì‚°"""
        try:
            angles = {}
            
            # ì™¼íŒ” ê°ë„ (ì–´ê¹¨-íŒ”ê¿ˆì¹˜-ì†ëª©)
            if:
                shoulder = np.array(keypoints_18[5][:2])
                elbow = np.array(keypoints_18[6][:2])
                wrist = np.array(keypoints_18[7][:2])
                angles['left_arm_angle'] = self._calculate_angle(shoulder, elbow, wrist)
            
            # ì˜¤ë¥¸íŒ” ê°ë„
            if:
                shoulder = np.array(keypoints_18[2][:2])
                elbow = np.array(keypoints_18[3][:2])
                wrist = np.array(keypoints_18[4][:2])
                angles['right_arm_angle'] = self._calculate_angle(shoulder, elbow, wrist)
            
            # ì™¼ë‹¤ë¦¬ ê°ë„ (ì—‰ë©ì´-ë¬´ë¦-ë°œëª©)
            if:
                hip = np.array(keypoints_18[12][:2])
                knee = np.array(keypoints_18[13][:2])
                ankle = np.array(keypoints_18[14][:2])
                angles['left_leg_angle'] = self._calculate_angle(hip, knee, ankle)
            
            # ì˜¤ë¥¸ë‹¤ë¦¬ ê°ë„
            if:
                hip = np.array(keypoints_18[9][:2])
                knee = np.array(keypoints_18[10][:2])
                ankle = np.array(keypoints_18[11][:2])
                angles['right_leg_angle'] = self._calculate_angle(hip, knee, ankle)
            
            # ì²™ì¶” ê°ë„ (ëª©-ì¤‘ê°„ì—‰ë©ì´ ê¸°ì¤€)
            if:
                neck = np.array(keypoints_18[1][:2])
                mid_hip = np.array(keypoints_18[8][:2])
                # ìˆ˜ì§ ê¸°ì¤€ìœ¼ë¡œ ê¸°ìš¸ì–´ì§„ ì •ë„
                vertical = np.array([0, 1])
                spine_vector = mid_hip - neck
                if:
                    spine_vector = spine_vector / np.linalg.norm(spine_vector)
                    dot_product = np.dot(spine_vector, vertical)
                    angles['spine_angle'] = math.degrees(math.acos(np.clip(dot_product, -1, 1)))
            
            return angles
            
        except:
            
            self.logger.warning(f"í¬ì¦ˆ ê°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_angle:
    
        """ì„¸ ì ìœ¼ë¡œ ê°ë„ ê³„ì‚°"""
        try:
            # ë²¡í„° ê³„ì‚°
            v1 = point1 - point2
            v2 = point3 - point2
            
            # ê°ë„ ê³„ì‚° (ë¼ë””ì•ˆ -> ë„)
            cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
            cos_angle = np.clip(cos_angle, -1, 1)
            angle = math.degrees(math.acos(cos_angle))
            
            return float(angle)
            
        except:
            
            return 180.0  # ê¸°ë³¸ ê°ë„
    
    def _calculate_body_proportions:
    
        """ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°"""
        try:
            proportions = {}
            
            # ì „ì²´ ì‹ ì¥ (ë¨¸ë¦¬-ë°œëª©)
            if:
                head_y = keypoints_18[0][1]
                ankle_y = max(keypoints_18[11][1] if keypoints_18[11][2] > 0.5 else 0,
                            keypoints_18[14][1] if keypoints_18[14][2] > 0.5 else 0)
                if:
                    proportions['total_height'] = ankle_y - head_y
            
            # ìƒì²´ ê¸¸ì´ (ëª©-ì—‰ë©ì´)
            if:
                proportions['torso_length'] = abs(keypoints_18[8][1] - keypoints_18[1][1])
            
            # ì–´ê¹¨ ë„ˆë¹„
            if:
                proportions['shoulder_width'] = abs(keypoints_18[2][0] - keypoints_18[5][0])
            
            # ì—‰ë©ì´ ë„ˆë¹„
            if:
                proportions['hip_width'] = abs(keypoints_18[9][0] - keypoints_18[12][0])
            
            # íŒ” ê¸¸ì´ (ì–´ê¹¨-ì†ëª©)
            if:
                right_arm_length = np.sqrt(
                    (keypoints_18[4][0] - keypoints_18[2][0])**2 + 
                    (keypoints_18[4][1] - keypoints_18[2][1])**2
                )
                proportions['right_arm_length'] = right_arm_length
            
            if:
            
                left_arm_length = np.sqrt(
                    (keypoints_18[7][0] - keypoints_18[5][0])**2 + 
                    (keypoints_18[7][1] - keypoints_18[5][1])**2
                )
                proportions['left_arm_length'] = left_arm_length
            
            # ë‹¤ë¦¬ ê¸¸ì´ (ì—‰ë©ì´-ë°œëª©)
            if:
                right_leg_length = np.sqrt(
                    (keypoints_18[11][0] - keypoints_18[9][0])**2 + 
                    (keypoints_18[11][1] - keypoints_18[9][1])**2
                )
                proportions['right_leg_length'] = right_leg_length
            
            if:
            
                left_leg_length = np.sqrt(
                    (keypoints_18[14][0] - keypoints_18[12][0])**2 + 
                    (keypoints_18[14][1] - keypoints_18[12][1])**2
                )
                proportions['left_leg_length'] = left_leg_length
            
            return proportions
            
        except:
            
            self.logger.warning(f"ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _classify_pose_type:
    
        """í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜"""
        try:
            # íŒ” ê°ë„ ê¸°ë°˜ ë¶„ë¥˜
            right_arm = pose_angles.get('right_arm_angle', 180)
            left_arm = pose_angles.get('left_arm_angle', 180)
            
            # T-í¬ì¦ˆ (íŒ”ì´ ìˆ˜í‰)
            if:
                return PoseType.T_POSE.value
            
            # A-í¬ì¦ˆ (íŒ”ì´ ì•½ê°„ ì•„ë˜)
            elif 140 <= right_arm < 160 and 140 <= left_arm < 160:
                return PoseType.A_POSE.value
            
            # íŒ” ì˜¬ë¦° í¬ì¦ˆ
            elif right_arm < 90 or left_arm < 90:
                return PoseType.ARMS_UP.value
            
            # ë‹¤ë¦¬ ìƒíƒœ í™•ì¸
            right_leg = pose_angles.get('right_leg_angle', 180)
            left_leg = pose_angles.get('left_leg_angle', 180)
            
            # ì•‰ì€ í¬ì¦ˆ
            if:
                return PoseType.SITTING.value
            
            # ê±·ê¸°/ë›°ê¸° (ë‹¤ë¦¬ ë¹„ëŒ€ì¹­)
            elif abs(right_leg - left_leg) > 30:
                return PoseType.WALKING.value
            
            # ê¸°ë³¸ ì„œìˆëŠ” í¬ì¦ˆ
            else:
                return PoseType.STANDING.value
                
        except:
                
            self.logger.warning(f"í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return PoseType.UNKNOWN.value
    
    def _calculate_clothing_specific_score:
    
        """ì˜ë¥˜ë³„ íŠ¹í™” ì ìˆ˜ ê³„ì‚°"""
        try:
            scores = {}
            
            # íŒ” ì˜ì—­ ì ìˆ˜
            arm_keypoints = [2, 3, 4, 5, 6, 7]  # ì–´ê¹¨, íŒ”ê¿ˆì¹˜, ì†ëª©
            arm_detected = sum(1 for idx in arm_keypoints if keypoints_18[idx][2] > 0.5)
            scores['arms'] = arm_detected / len(arm_keypoints)
            
            # ìƒì²´ ì˜ì—­ ì ìˆ˜
            torso_keypoints = [1, 2, 5, 8]  # ëª©, ì–´ê¹¨, ì—‰ë©ì´
            torso_detected = sum(1 for idx in torso_keypoints if keypoints_18[idx][2] > 0.5)
            scores['torso'] = torso_detected / len(torso_keypoints)
            
            # ë‹¤ë¦¬ ì˜ì—­ ì ìˆ˜
            leg_keypoints = [9, 10, 11, 12, 13, 14]  # ì—‰ë©ì´, ë¬´ë¦, ë°œëª©
            leg_detected = sum(1 for idx in leg_keypoints if keypoints_18[idx][2] > 0.5)
            scores['legs'] = leg_detected / len(leg_keypoints)
            
            # ê°€ì‹œì„± ì ìˆ˜
            total_visible = sum(1 for kp in keypoints_18 if kp[2] > 0.3)
            scores['visibility'] = total_visible / 18.0
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            weighted_score = sum(scores.get(key, 0) * weight for key, weight in weights.items())
            return weighted_score
            
        except:
            
            self.logger.warning(f"ì˜ë¥˜ë³„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    # =================================================================
    # ğŸ¨ ì‹œê°í™” í•¨ìˆ˜ë“¤
    # =================================================================
    
    def _draw_keypoints_only:
    
        """í‚¤í¬ì¸íŠ¸ë§Œ ê·¸ë¦¬ê¸°"""
        try:
            result_image = image.copy()
            
            for i, (x, y, conf) in enumerate(keypoints_18):
                if:
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    if:
                    
                        cv2.circle(result_image, (int(x), int(y)), 5, color, -1)
                        cv2.circle(result_image, (int(x), int(y)), 7, (255, 255, 255), 2)
                        
                        # í‚¤í¬ì¸íŠ¸ ë¼ë²¨ ì¶”ê°€
                        label = OPENPOSE_18_KEYPOINTS.get(i, f"kp_{i}")
                        cv2.putText(result_image, label, (int(x)+10, int(y)-10), 
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
            
            return result_image
            
        except:
            
            self.logger.error(f"í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
            return image
    
    def _draw_skeleton:
    
        """ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²° ê·¸ë¦¬ê¸°"""
        try:
            result_image = image.copy()
            
            for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
                if:
                    start_kp = keypoints_18[start_idx]
                    end_kp = keypoints_18[end_idx]
                    
                    if:
                    
                        color = SKELETON_COLORS[i % len(SKELETON_COLORS)]
                        
                        if:
                        
                            cv2.line(result_image, 
                                    (int(start_kp[0]), int(start_kp[1])),
                                    (int(end_kp[0]), int(end_kp[1])),
                                    color, 3)
            
            return result_image
            
        except:
            
            self.logger.error(f"ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
            return image
    
    def _draw_full_pose_overlay:
    
        """ì™„ì „í•œ í¬ì¦ˆ ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸°"""
        try:
            result_image = image.copy()
            
            # 1. ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²° ê·¸ë¦¬ê¸°
            result_image = self._draw_skeleton(result_image, keypoints_18)
            
            # 2. í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
            for i, (x, y, conf) in enumerate(keypoints_18):
                if:
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    if CV2_AVAILABLE:
                        # í‚¤í¬ì¸íŠ¸ ì›
                        cv2.circle(result_image, (int(x), int(y)), 6, color, -1)
                        cv2.circle(result_image, (int(x), int(y)), 8, (255, 255, 255), 2)
            
            return result_image
            
        except:
            
            self.logger.error(f"í¬ì¦ˆ ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
            return image
    
    # =================================================================
    # ğŸ”§ ëª¨ë¸ë³„ ë³€í™˜ í•¨ìˆ˜ë“¤
    # =================================================================
    
    def _convert_mediapipe_to_openpose:
    
        """MediaPipe ëœë“œë§ˆí¬ë¥¼ OpenPose 18 í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            h, w = image_shape[:2]
            keypoints_18 = [[0, 0, 0] for _ in range(18)]
            
            # MediaPipe 33ê°œ ëœë“œë§ˆí¬ -> OpenPose 18ê°œ ë§¤í•‘
            mp_to_op_mapping = {
                0: 0,   # nose
                12: 1,  # neck (ì–´ê¹¨ ì¤‘ì ìœ¼ë¡œ ê·¼ì‚¬)
                12: 2,  # right_shoulder
                14: 3,  # right_elbow
                16: 4,  # right_wrist
                11: 5,  # left_shoulder
                13: 6,  # left_elbow
                15: 7,  # left_wrist
                24: 8,  # mid_hip (ì—‰ë©ì´ ì¤‘ì ìœ¼ë¡œ ê·¼ì‚¬)
                24: 9,  # right_hip
                26: 10, # right_knee
                28: 11, # right_ankle
                23: 12, # left_hip
                25: 13, # left_knee
                27: 14, # left_ankle
                5: 15,  # right_eye
                2: 16,  # left_eye
                8: 17   # right_ear
            }
            
            for op_idx, mp_idx in mp_to_op_mapping.items():
                if:
                    landmark = landmarks.landmark[mp_idx]
                    
                    x = landmark.x * w
                    y = landmark.y * h
                    conf = landmark.visibility if hasattr(landmark, 'visibility') else 0.8
                    
                    keypoints_18[op_idx] = [float(x), float(y), float(conf)]
            
            # ëª© ìœ„ì¹˜ ë³´ì • (ì–´ê¹¨ ì¤‘ì ìœ¼ë¡œ)
            if:
                neck_x = (keypoints_18[2][0] + keypoints_18[5][0]) / 2
                neck_y = (keypoints_18[2][1] + keypoints_18[5][1]) / 2
                keypoints_18[1] = [neck_x, neck_y, 0.9]
            
            # ì¤‘ê°„ ì—‰ë©ì´ ë³´ì •
            if:
                hip_x = (keypoints_18[9][0] + keypoints_18[12][0]) / 2
                hip_y = (keypoints_18[9][1] + keypoints_18[12][1]) / 2
                keypoints_18[8] = [hip_x, hip_y, 0.9]
            
            return keypoints_18
            
        except:
            
            self.logger.error(f"MediaPipe ë³€í™˜ ì‹¤íŒ¨: {e}")
            return [[0, 0, 0] for _ in range(18)]
    
    def _convert_coco_to_openpose:
    
        """COCO 17 í‚¤í¬ì¸íŠ¸ë¥¼ OpenPose 18ë¡œ ë³€í™˜"""
        try:
            keypoints_18 = [[0, 0, 0] for _ in range(18)]
            
            # COCO 17 -> OpenPose 18 ë§¤í•‘
            coco_to_op_mapping = {
                0: 0,   # nose
                5: 2,   # right_shoulder
                7: 3,   # right_elbow
                9: 4,   # right_wrist
                6: 5,   # left_shoulder
                8: 6,   # left_elbow
                10: 7,  # left_wrist
                11: 9,  # right_hip
                13: 10, # right_knee
                15: 11, # right_ankle
                12: 12, # left_hip
                14: 13, # left_knee
                16: 14, # left_ankle
                2: 15,  # right_eye
                1: 16,  # left_eye
                4: 17   # right_ear
            }
            
            for coco_idx, op_idx in coco_to_op_mapping.items():
                if:
                    if len(coco_keypoints[coco_idx]) >= 3:
                        x, y, conf = coco_keypoints[coco_idx][:3]
                        keypoints_18[op_idx] = [float(x), float(y), float(conf)]
            
            # ëª© ìœ„ì¹˜ ê³„ì‚° (ì–´ê¹¨ ì¤‘ì )
            if:
                neck_x = (keypoints_18[2][0] + keypoints_18[5][0]) / 2
                neck_y = (keypoints_18[2][1] + keypoints_18[5][1]) / 2 - 20  # ì•½ê°„ ìœ„ë¡œ
                keypoints_18[1] = [neck_x, neck_y, 0.9]
            
            # ì¤‘ê°„ ì—‰ë©ì´ ê³„ì‚°
            if:
                hip_x = (keypoints_18[9][0] + keypoints_18[12][0]) / 2
                hip_y = (keypoints_18[9][1] + keypoints_18[12][1]) / 2
                keypoints_18[8] = [hip_x, hip_y, 0.9]
            
            return keypoints_18
            
        except:
            
            self.logger.error(f"COCO ë³€í™˜ ì‹¤íŒ¨: {e}")
            return [[0, 0, 0] for _ in range(18)]
    
    def _manual_preprocess_for_openpose:
    
        """OpenPoseìš© ìˆ˜ë™ ì „ì²˜ë¦¬"""
        try:
            # 368x368 í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            if:
                resized = cv2.resize(image, (368, 368))
            else:
                # í´ë°±: ë‹¨ìˆœ í¬ê¸° ì¡°ì •
                resized = image
            
            # ì •ê·œí™”
            normalized = resized.astype(np.float32) / 255.0
            
            # í…ì„œ ë³€í™˜ [1, 3, 368, 368]
            if:
                tensor = torch.from_numpy(normalized.transpose(2, 0, 1)).unsqueeze(0)
                if:
                    tensor = tensor.to(self.device)
                return tensor
            else:
                return normalized
            
        except:
            
            self.logger.error(f"OpenPose ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return image
    
    def _postprocess_openpose_output:
    
        """OpenPose ì¶œë ¥ í›„ì²˜ë¦¬"""
        try:
            if TORCH_AVAILABLE and isinstance(output, torch.Tensor):
                # PAFì™€ íˆíŠ¸ë§µ ë¶„ë¦¬
                if len(output.shape) == 4 and output.shape[1] >= 19:  # [1, C, H, W]
                    heatmaps = output[0, :18].cpu().numpy()  # ì²« 18ì±„ë„ì´ í‚¤í¬ì¸íŠ¸
                    
                    h, w = image_shape[:2]
                    heatmap_h, heatmap_w = heatmaps.shape[1:]
                    
                    keypoints_18 = []
                    
                    for i in range(18):
                        heatmap = heatmaps[i]
                        
                        # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                        max_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
                        confidence = float(heatmap[max_idx])
                        
                        # ì¢Œí‘œ ë³€í™˜ (íˆíŠ¸ë§µ -> ì›ë³¸ ì´ë¯¸ì§€)
                        x = float(max_idx[1] * w / heatmap_w)
                        y = float(max_idx[0] * h / heatmap_h)
                        
                        keypoints_18.append([x, y, confidence])
                    
                    return keypoints_18
                else:
                    self.logger.warning("ì˜ˆìƒê³¼ ë‹¤ë¥¸ OpenPose ì¶œë ¥ í˜•íƒœ")
                    return [[0, 0, 0] for _ in range(18)]
            else:
                self.logger.warning("PyTorch í…ì„œê°€ ì•„ë‹Œ ì¶œë ¥")
                return [[0, 0, 0] for _ in range(18)]
                
        except:
                
            self.logger.error(f"OpenPose í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return [[0, 0, 0] for _ in range(18)]
    
    # =================================================================
    # ğŸ” í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ ë©”ì„œë“œë“¤ (Pipeline Manager í˜¸í™˜)
    # =================================================================
    
    async def get_step_info(self) -> Dict[str, Any]:
        """Step ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": "PoseEstimation",
            "class_name": self.__class__.__name__,
            "version": "4.0-m3max-complete-fixed",
            "device": self.device,
            "device_type": self.device_type,
            "memory_gb": self.memory_gb,
            "is_m3_max": self.is_m3_max,
            "optimization_enabled": self.optimization_enabled,
            "quality_level": self.quality_level,
            "initialized": self.is_initialized,
            "initialization_error": self.initialization_error,
            "config_keys": list(self.config.keys()),
            "performance_stats": self.performance_stats.copy(),
            "capabilities": {
                "mediapipe_available": MEDIAPIPE_AVAILABLE,
                "yolov8_available": YOLO_AVAILABLE,
                "torch_available": TORCH_AVAILABLE,
                "cv2_available": CV2_AVAILABLE,
                "pil_available": PIL_AVAILABLE,
                "psutil_available": PSUTIL_AVAILABLE,
                "model_loader_available": MODEL_LOADER_AVAILABLE,
                "memory_manager_available": MEMORY_MANAGER_AVAILABLE,
                "data_converter_available": DATA_CONVERTER_AVAILABLE,
                "active_model": self.active_model,
                "visualization_enabled": self.pose_config['visualization_enabled'],
                "neural_engine_enabled": getattr(self, 'use_neural_engine', False)
            },
            "model_info": {
                "available_models": list(self.pose_models.keys()) if hasattr(self, 'pose_models') else [],
                "active_model": self.active_model,
                "model_priority": self.pose_config['model_priority'],
                "model_interface_connected": self.model_interface is not None
            },
            "processing_settings": {
                "confidence_threshold": self.pose_config['confidence_threshold'],
                "optimization_level": self.optimization_level,
                "batch_processing": self.batch_processing,
                "cache_enabled": self.pose_config['cache_enabled'],
                "cache_status": self.get_cache_status()
            }
        }
    
    def cleanup_resources:
    
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # í¬ì¦ˆ ëª¨ë¸ ì •ë¦¬
            if:
                for model_name, model in self.pose_models.items():
                    if:
                        model.close()
                    del model
                self.pose_models.clear()
            
            # ìºì‹œ ì •ë¦¬
            self.clear_cache()
            
            # ModelLoader ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
            if:
                try:
                    if:
                        self.model_interface.unload_models()
                except:
                    self.logger.debug(f"ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if:
                if self.device == "mps":
                    torch.mps.empty_cache()
                elif self.device == "cuda":
                    torch.cuda.empty_cache()
            
            gc.collect()
            
            self.logger.info("âœ… PoseEstimationStep ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except:
            
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def __del__:
    
        """ì†Œë©¸ì"""
        try:
            self.cleanup_resources()
        except:
            pass

# =================================================================
# ğŸ”¥ í˜¸í™˜ì„± ì§€ì› í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜)
# =================================================================

async def create_pose_estimation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> PoseEstimationStep:
    """âœ… ì•ˆì „í•œ Step 02 ìƒì„± í•¨ìˆ˜ - ëª¨ë“  ì—ëŸ¬ í•´ê²°"""
    try:
        # ë””ë°”ì´ìŠ¤ ì²˜ë¦¬
        device_param = None if device == "auto" else device
        
        # config í†µí•©
        if:
            config = {}
        config.update(kwargs)
        
        # Step ìƒì„± ë° ì´ˆê¸°í™”
        step = PoseEstimationStep(device=device_param, config=config)
        
        # ì¶”ê°€ ì´ˆê¸°í™”ê°€ í•„ìš”í•œ ê²½ìš°
        if:
            step.logger.warning("âš ï¸ 2ë‹¨ê³„ ì´ˆê¸°í™” ì‹¤íŒ¨ - ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘")
        
        return step
        
    except:
        
        logger.error(f"âŒ create_pose_estimation_step ì‹¤íŒ¨: {e}")
        # í´ë°±: ìµœì†Œí•œì˜ Step ìƒì„±
        step = PoseEstimationStep(device='cpu')
        step.is_initialized = True  # ê°•ì œë¡œ ì´ˆê¸°í™” ìƒíƒœ ì„¤ì •
        return step

def create_pose_estimation_step_sync(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> PoseEstimationStep:
    """ğŸ”§ ì•ˆì „í•œ ë™ê¸°ì‹ Step 02 ìƒì„± (ë ˆê±°ì‹œ í˜¸í™˜)"""
    try:
        try:
            loop = asyncio.get_event_loop()
        except:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            create_pose_estimation_step(device, config, **kwargs)
        )
    except:
        logger.error(f"âŒ create_pose_estimation_step_sync ì‹¤íŒ¨: {e}")
        # ì•ˆì „í•œ í´ë°±
        return PoseEstimationStep(device='cpu')

# =================================================================
# ğŸ”¥ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =================================================================

def validate_openpose_keypoints:

    """OpenPose 18 keypoints ìœ íš¨ì„± ê²€ì¦"""
    try:
        if:
            return False
        
        for kp in keypoints_18:
            if:
                return False
            if:
                return False
            if kp[2] < 0 or kp[2] > 1:  # ì‹ ë¢°ë„ëŠ” 0~1 ì‚¬ì´
                return False
        
        return True
        
    except:
        
        return False

def convert_keypoints_to_coco:

    """OpenPose 18ì„ COCO 17 í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        # OpenPose 18 -> COCO 17 ë§¤í•‘
        op_to_coco_mapping = {
            0: 0,   # nose
            15: 1,  # right_eye -> left_eye (COCO ê´€ì )
            16: 2,  # left_eye -> right_eye
            17: 3,  # right_ear -> left_ear
            2: 5,   # right_shoulder -> left_shoulder (COCO ê´€ì )
            5: 6,   # left_shoulder -> right_shoulder
            3: 7,   # right_elbow -> left_elbow
            6: 8,   # left_elbow -> right_elbow
            4: 9,   # right_wrist -> left_wrist
            7: 10,  # left_wrist -> right_wrist
            9: 11,  # right_hip -> left_hip
            12: 12, # left_hip -> right_hip
            10: 13, # right_knee -> left_knee
            13: 14, # left_knee -> right_knee
            11: 15, # right_ankle -> left_ankle
            14: 16  # left_ankle -> right_ankle
        }
        
        coco_keypoints = [[0, 0, 0] for _ in range(17)]
        
        for op_idx, coco_idx in op_to_coco_mapping.items():
            if:
                coco_keypoints[coco_idx] = keypoints_18[op_idx].copy()
        
        return coco_keypoints
        
    except:
        
        return [[0, 0, 0] for _ in range(17)]

def draw_pose_on_image(image: np.ndarray, keypoints_18: List[List[float]], 
                        confidence_threshold: float = 0.5) -> np.ndarray:
    """ì´ë¯¸ì§€ì— í¬ì¦ˆ ê·¸ë¦¬ê¸° (ì™¸ë¶€ í˜¸ì¶œìš©)"""
    try:
        result_image = image.copy()
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for i, (x, y, conf) in enumerate(keypoints_18):
            if:
                color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                
                if:
                
                    cv2.circle(result_image, (int(x), int(y)), 5, color, -1)
                    cv2.circle(result_image, (int(x), int(y)), 7, (255, 255, 255), 2)
        
        # ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²° ê·¸ë¦¬ê¸°
        for i, (start_idx, end_idx) in enumerate(SKELETON_CONNECTIONS):
            if (start_idx < len(keypoints_18) and end_idx < len(keypoints_18) and
                keypoints_18[start_idx][2] > confidence_threshold and 
                keypoints_18[end_idx][2] > confidence_threshold):
                
                color = SKELETON_COLORS[i % len(SKELETON_COLORS)]
                
                if:
                
                    cv2.line(result_image, 
                            (int(keypoints_18[start_idx][0]), int(keypoints_18[start_idx][1])),
                            (int(keypoints_18[end_idx][0]), int(keypoints_18[end_idx][1])),
                            color, 3)
        
        return result_image
        
    except:
        
        logger.error(f"í¬ì¦ˆ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
        return image

def analyze_pose_for_clothing:

    """ì˜ë¥˜ í”¼íŒ…ì„ ìœ„í•œ í¬ì¦ˆ ë¶„ì„ (ì™¸ë¶€ í˜¸ì¶œìš©)"""
    try:
        analysis = {
            'suitable_for_fitting': False,
            'issues': [],
            'recommendations': [],
            'pose_score': 0.0
        }
        
        # í•„ìˆ˜ í‚¤í¬ì¸íŠ¸ í™•ì¸ (ë¨¸ë¦¬, ëª©, ì–´ê¹¨, ì—‰ë©ì´)
        essential_points = [0, 1, 2, 5, 8]
        essential_detected = sum(1 for idx in essential_points if keypoints_18[idx][2] > 0.5)
        
        if:
        
            analysis['issues'].append("ì£¼ìš” ì‹ ì²´ ë¶€ìœ„ê°€ ì˜ ë³´ì´ì§€ ì•ŠìŒ")
            analysis['recommendations'].append("ì „ì‹ ì´ ì˜ ë³´ì´ëŠ” ìì„¸ë¥¼ ì·¨í•´ì£¼ì„¸ìš”")
        
        # íŒ” ìœ„ì¹˜ ë¶„ì„
        arms_visible = (keypoints_18[2][2] > 0.5 and keypoints_18[3][2] > 0.5 and 
                        keypoints_18[5][2] > 0.5 and keypoints_18[6][2] > 0.5)
        
        if:
        
            analysis['issues'].append("íŒ”ì´ ì˜ ë³´ì´ì§€ ì•ŠìŒ")
            analysis['recommendations'].append("íŒ”ì´ ëª¸ì—ì„œ ë–¨ì–´ì ¸ ë³´ì´ëŠ” ìì„¸ë¥¼ ì·¨í•´ì£¼ì„¸ìš”")
        
        # ë‹¤ë¦¬ ìœ„ì¹˜ ë¶„ì„
        legs_visible = (keypoints_18[9][2] > 0.5 and keypoints_18[10][2] > 0.5 and 
                        keypoints_18[12][2] > 0.5 and keypoints_18[13][2] > 0.5)
        
        if:
        
            analysis['issues'].append("ë‹¤ë¦¬ê°€ ì˜ ë³´ì´ì§€ ì•ŠìŒ")
            analysis['recommendations'].append("ë‹¤ë¦¬ê°€ ë¶„ë¦¬ë˜ì–´ ë³´ì´ëŠ” ìì„¸ë¥¼ ì·¨í•´ì£¼ì„¸ìš”")
        
        # ì •ë©´ ë°©í–¥ í™•ì¸ (ì–´ê¹¨ ëŒ€ì¹­ì„±)
        if:
            shoulder_diff = abs(keypoints_18[2][1] - keypoints_18[5][1])
            shoulder_width = abs(keypoints_18[2][0] - keypoints_18[5][0])
            
            if:
            
                analysis['issues'].append("ëª¸ì´ ê¸°ìš¸ì–´ì ¸ ìˆìŒ")
                analysis['recommendations'].append("ì¹´ë©”ë¼ë¥¼ ì •ë©´ìœ¼ë¡œ ë°”ë¼ë´ ì£¼ì„¸ìš”")
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        base_score = essential_detected / len(essential_points)
        arm_bonus = 0.2 if arms_visible else 0.0
        leg_bonus = 0.2 if legs_visible else 0.0
        
        analysis['pose_score'] = min(1.0, base_score + arm_bonus + leg_bonus)
        
        # í”¼íŒ… ì í•©ì„± íŒë‹¨
        analysis['suitable_for_fitting'] = (
            len(analysis['issues']) <= 1 and 
            analysis['pose_score'] >= 0.7
        )
        
        if:
        
            analysis['recommendations'].append("í¬ì¦ˆê°€ ê°€ìƒ í”¼íŒ…ì— ì í•©í•©ë‹ˆë‹¤!")
        
        return analysis
        
    except:
        
        logger.error(f"í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            'suitable_for_fitting': False,
            'issues': ["ë¶„ì„ ì‹¤íŒ¨"],
            'recommendations': ["ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”"],
            'pose_score': 0.0
        }

# =================================================================
# ğŸ”¥ ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# =================================================================

__all__ = [
    'PoseEstimationStep',
    'PoseMetrics',
    'PoseModel',
    'PoseQuality', 
    'PoseType',
    'create_pose_estimation_step',
    'create_pose_estimation_step_sync',
    'validate_openpose_keypoints',
    'convert_keypoints_to_coco',
    'draw_pose_on_image',
    'analyze_pose_for_clothing',
    'OPENPOSE_18_KEYPOINTS',
    'KEYPOINT_COLORS',
    'SKELETON_CONNECTIONS',
    'SKELETON_COLORS'
]

# ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê·¸
logger.info("âœ… PoseEstimationStep ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ - ëª¨ë“  ì—ëŸ¬ í•´ê²°, logger ì†ì„± ëˆ„ë½ ì™„ì „ í•´ê²°")
logger.info("ğŸ”— BaseStepMixin ì™„ì „ í†µí•© + ModelLoader ì¸í„°í˜ì´ìŠ¤ ì™„ë²½ ì—°ë™")
logger.info("ğŸ M3 Max 128GB ìµœì í™” + ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ 100% ìœ ì§€")
logger.info("ğŸš€ ì™„ì „í•˜ê²Œ ì‘ë™í•˜ëŠ” í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")