# app/ai_pipeline/steps/step_02_pose_estimation.py
"""
2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • (Pose Estimation) - í†µì¼ëœ ìƒì„±ì íŒ¨í„´ ì ìš©
âœ… ìµœì í™”ëœ ìƒì„±ì: device ìë™ê°ì§€, M3 Max ìµœì í™”, ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤
MediaPipe + OpenPose í˜¸í™˜ + ê³ ê¸‰ í¬ì¦ˆ ë¶„ì„
"""
import os
import logging
import time
import asyncio
import json
from typing import Dict, Any, Optional, List, Tuple, Union
import numpy as np
import cv2
from PIL import Image
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

# MediaPipe (ì‹¤ì œ í¬ì¦ˆ ì¶”ì •ìš©)
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False
    logging.warning("MediaPipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëŒ€ì•ˆ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

logger = logging.getLogger(__name__)

class PoseEstimationStep:
    """
    âœ… 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • - í†µì¼ëœ ìƒì„±ì íŒ¨í„´
    - ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€
    - M3 Max ìµœì í™”
    - ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤
    - MediaPipe + OpenPose í˜¸í™˜
    """
    
    # OpenPose 18 í‚¤í¬ì¸íŠ¸ ì •ì˜
    OPENPOSE_18_KEYPOINTS = [
        "nose", "neck", "right_shoulder", "right_elbow", "right_wrist",
        "left_shoulder", "left_elbow", "left_wrist", "right_hip", "right_knee",
        "right_ankle", "left_hip", "left_knee", "left_ankle", "right_eye",
        "left_eye", "right_ear", "left_ear"
    ]
    
    # MediaPipe 33 í‚¤í¬ì¸íŠ¸ ë§¤í•‘
    MEDIAPIPE_KEYPOINT_NAMES = [
        "nose", "left_eye_inner", "left_eye", "left_eye_outer", "right_eye_inner",
        "right_eye", "right_eye_outer", "left_ear", "right_ear", "mouth_left",
        "mouth_right", "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
        "left_wrist", "right_wrist", "left_pinky", "right_pinky", "left_index",
        "right_index", "left_thumb", "right_thumb", "left_hip", "right_hip",
        "left_knee", "right_knee", "left_ankle", "right_ankle", "left_heel",
        "right_heel", "left_foot_index", "right_foot_index"
    ]
    
    def __init__(
        self,
        device: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """
        âœ… í†µì¼ëœ ìƒì„±ì - ìµœì í™”ëœ ì¸í„°í˜ì´ìŠ¤
        
        Args:
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (None=ìë™ê°ì§€, 'cpu', 'cuda', 'mps')
            config: ìŠ¤í…ë³„ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            **kwargs: í™•ì¥ íŒŒë¼ë¯¸í„°ë“¤
                - device_type: str = "auto"
                - memory_gb: float = 16.0  
                - is_m3_max: bool = False
                - optimization_enabled: bool = True
                - quality_level: str = "balanced"
                - model_complexity: int = 2 (MediaPipe ëª¨ë¸ ë³µì¡ë„ 0,1,2)
                - min_detection_confidence: float = 0.7
                - min_tracking_confidence: float = 0.5
                - enable_segmentation: bool = False
                - max_image_size: int = 1024
                - use_face: bool = True (ì–¼êµ´ í‚¤í¬ì¸íŠ¸ ì‚¬ìš©)
                - use_hands: bool = False (ì† í‚¤í¬ì¸íŠ¸ ì‚¬ìš©)
        """
        # ğŸ’¡ ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = self._auto_detect_device(device)
        
        # ğŸ“‹ ê¸°ë³¸ ì„¤ì •
        self.config = config or {}
        self.step_name = self.__class__.__name__
        self.logger = logging.getLogger(f"pipeline.{self.step_name}")
        
        # ğŸ”§ í‘œì¤€ ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ì¼ê´€ì„±)
        self.device_type = kwargs.get('device_type', 'auto')
        self.memory_gb = kwargs.get('memory_gb', 16.0)
        self.is_m3_max = kwargs.get('is_m3_max', self._detect_m3_max())
        self.optimization_enabled = kwargs.get('optimization_enabled', True)
        self.quality_level = kwargs.get('quality_level', 'balanced')
        
        # âš™ï¸ ìŠ¤í…ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„°ë¥¼ configì— ë³‘í•©
        self._merge_step_specific_config(kwargs)
        
        # âœ… ìƒíƒœ ì´ˆê¸°í™”
        self.is_initialized = False
        
        # ğŸ¯ ê¸°ì¡´ í´ë˜ìŠ¤ë³„ ê³ ìœ  ì´ˆê¸°í™” ë¡œì§ ì‹¤í–‰
        self._initialize_step_specific()
        
        self.logger.info(f"ğŸ¯ {self.step_name} ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _auto_detect_device(self, preferred_device: Optional[str]) -> str:
        """ğŸ’¡ ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if preferred_device:
            return preferred_device

        try:
            import torch
            if torch.backends.mps.is_available():
                return 'mps'  # M3 Max ìš°ì„ 
            elif torch.cuda.is_available():
                return 'cuda'  # NVIDIA GPU
            else:
                return 'cpu'  # í´ë°±
        except ImportError:
            return 'cpu'

    def _detect_m3_max(self) -> bool:
        """ğŸ M3 Max ì¹© ìë™ ê°ì§€"""
        try:
            import platform
            import subprocess

            if platform.system() == 'Darwin':  # macOS
                # M3 Max ê°ì§€ ë¡œì§
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return 'M3' in result.stdout
        except:
            pass
        return False

    def _merge_step_specific_config(self, kwargs: Dict[str, Any]):
        """âš™ï¸ ìŠ¤í…ë³„ íŠ¹í™” ì„¤ì • ë³‘í•©"""
        # ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì œì™¸í•˜ê³  ëª¨ë“  kwargsë¥¼ configì— ë³‘í•©
        system_params = {
            'device_type', 'memory_gb', 'is_m3_max', 
            'optimization_enabled', 'quality_level'
        }

        for key, value in kwargs.items():
            if key not in system_params:
                self.config[key] = value

    def _initialize_step_specific(self):
        """ğŸ¯ ê¸°ì¡´ ì´ˆê¸°í™” ë¡œì§ ì™„ì „ ìœ ì§€"""
        # 2ë‹¨ê³„ ì „ìš© í¬ì¦ˆ ì¶”ì • ì„¤ì •
        self.pose_config = self.config.get('pose', {})
        
        # MediaPipe ì„¤ì • (M3 Maxì—ì„œ ë” ë†’ì€ í’ˆì§ˆ)
        default_complexity = 2 if self.is_m3_max else 1
        self.model_complexity = self.config.get('model_complexity', default_complexity)
        self.min_detection_confidence = self.config.get('min_detection_confidence', 0.7)
        self.min_tracking_confidence = self.config.get('min_tracking_confidence', 0.5)
        self.enable_segmentation = self.config.get('enable_segmentation', False)
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì •
        default_max_size = 1024 if self.memory_gb >= 32 else 512
        self.max_image_size = self.config.get('max_image_size', default_max_size)
        
        # ì¶”ê°€ í‚¤í¬ì¸íŠ¸ ì„¤ì •
        self.use_face = self.config.get('use_face', True)
        self.use_hands = self.config.get('use_hands', False)  # ì„±ëŠ¥ìƒ ê¸°ë³¸ ë¹„í™œì„±í™”
        
        # MediaPipe ëª¨ë¸ ë³€ìˆ˜ë“¤
        self.mp_pose = None
        self.mp_drawing = None
        self.mp_drawing_styles = None
        self.pose_detector = None
        self.face_detector = None
        self.hands_detector = None
        
        # 2ë‹¨ê³„ ì „ìš© í†µê³„
        self.pose_stats = {
            'total_processed': 0,
            'successful_detections': 0,
            'average_processing_time': 0.0,
            'average_keypoints_detected': 0.0,
            'last_quality_score': 0.0,
            'mediapipe_usage': 0,
            'fallback_usage': 0,
            'face_detections': 0,
            'hands_detections': 0
        }
        
        # ì„±ëŠ¥ ìºì‹œ (M3 Maxì—ì„œ ë” í° ìºì‹œ)
        cache_size = 100 if self.is_m3_max and self.memory_gb >= 128 else 50
        self.detection_cache = {}
        self.cache_max_size = cache_size
        
        # ìŠ¤ë ˆë“œ í’€ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜)
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    async def initialize(self) -> bool:
        """
        âœ… í†µì¼ëœ ì´ˆê¸°í™” ì¸í„°í˜ì´ìŠ¤
        
        Returns:
            bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            self.logger.info("ğŸ”„ 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            
            if MEDIAPIPE_AVAILABLE:
                # MediaPipe ì´ˆê¸°í™”
                self.mp_pose = mp.solutions.pose
                self.mp_drawing = mp.solutions.drawing_utils
                self.mp_drawing_styles = mp.solutions.drawing_styles
                
                # í¬ì¦ˆ ê²€ì¶œê¸° ì´ˆê¸°í™” (M3 Max ìµœì í™” ì„¤ì •)
                self.pose_detector = self.mp_pose.Pose(
                    static_image_mode=True,
                    model_complexity=self.model_complexity,
                    enable_segmentation=self.enable_segmentation,
                    min_detection_confidence=self.min_detection_confidence,
                    min_tracking_confidence=self.min_tracking_confidence
                )
                
                # ì–¼êµ´ ê²€ì¶œê¸° (ì„ íƒì )
                if self.use_face:
                    try:
                        mp_face = mp.solutions.face_mesh
                        self.face_detector = mp_face.FaceMesh(
                            static_image_mode=True,
                            max_num_faces=1,
                            refine_landmarks=self.is_m3_max,  # M3 Maxì—ì„œ ì •ë°€ë„ í–¥ìƒ
                            min_detection_confidence=self.min_detection_confidence
                        )
                        self.logger.info("âœ… ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                    except Exception as e:
                        self.logger.warning(f"ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                
                # ì† ê²€ì¶œê¸° (ì„ íƒì )
                if self.use_hands:
                    try:
                        mp_hands = mp.solutions.hands
                        self.hands_detector = mp_hands.Hands(
                            static_image_mode=True,
                            max_num_hands=2,
                            model_complexity=min(1, self.model_complexity),  # ì†ì€ ë³µì¡ë„ ì œí•œ
                            min_detection_confidence=self.min_detection_confidence
                        )
                        self.logger.info("âœ… ì† ê²€ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                    except Exception as e:
                        self.logger.warning(f"ì† ê²€ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                
                self.logger.info("âœ… MediaPipe í¬ì¦ˆ ì¶”ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                # í´ë°±: ë”ë¯¸ ê²€ì¶œê¸°
                self.pose_detector = self._create_dummy_detector()
                self.logger.warning("âš ï¸ MediaPipe ì—†ìŒ - ë”ë¯¸ í¬ì¦ˆ ê²€ì¶œê¸° ì‚¬ìš©")
            
            # M3 Max ìµœì í™” ì›Œë°ì—…
            if self.is_m3_max and self.optimization_enabled:
                await self._warmup_m3_max()
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            error_msg = f"í¬ì¦ˆ ì¶”ì • ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"
            self.logger.error(f"âŒ {error_msg}")
            
            # ì—ëŸ¬ ì‹œ ë”ë¯¸ ê²€ì¶œê¸°ë¡œ í´ë°±
            self.pose_detector = self._create_dummy_detector()
            self.is_initialized = True
            return True
    
    async def process(
        self, 
        person_image: Union[str, np.ndarray, Image.Image],
        **kwargs
    ) -> Dict[str, Any]:
        """
        âœ… í†µì¼ëœ ì²˜ë¦¬ ì¸í„°í˜ì´ìŠ¤
        
        Args:
            person_image: ì…ë ¥ ì´ë¯¸ì§€ (ê²½ë¡œ, numpy ë°°ì—´, PIL ì´ë¯¸ì§€)
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜
                - return_face_keypoints: bool = False
                - return_hand_keypoints: bool = False
                - enable_pose_analysis: bool = True
                - cache_result: bool = True
            
        Returns:
            Dict[str, Any]: ì²˜ë¦¬ ê²°ê³¼
        """
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        
        try:
            self.logger.info("ğŸƒ í¬ì¦ˆ ì¶”ì • ì²˜ë¦¬ ì‹œì‘")
            
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(person_image)
            if cache_key in self.detection_cache and kwargs.get('cache_result', True):
                self.logger.info("ğŸ’¾ ìºì‹œì—ì„œ í¬ì¦ˆ ê²°ê³¼ ë°˜í™˜")
                cached_result = self.detection_cache[cache_key].copy()
                cached_result['from_cache'] = True
                return cached_result
            
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
            image_array = await self._load_and_preprocess_image(person_image)
            if image_array is None:
                return self._create_empty_result("ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
            
            # í¬ì¦ˆ ì¶”ì • ì‹¤í–‰
            if MEDIAPIPE_AVAILABLE and self.pose_detector:
                pose_result = await self._detect_pose_mediapipe(image_array)
                self.pose_stats['mediapipe_usage'] += 1
            else:
                pose_result = await self._detect_pose_dummy(image_array)
                self.pose_stats['fallback_usage'] += 1
            
            # ì¶”ê°€ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
            face_keypoints = None
            hand_keypoints = None
            
            if kwargs.get('return_face_keypoints', False) and self.face_detector:
                face_keypoints = await self._detect_face_keypoints(image_array)
                if face_keypoints:
                    self.pose_stats['face_detections'] += 1
            
            if kwargs.get('return_hand_keypoints', False) and self.hands_detector:
                hand_keypoints = await self._detect_hand_keypoints(image_array)
                if hand_keypoints:
                    self.pose_stats['hands_detections'] += 1
            
            # OpenPose 18 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            keypoints_18 = self._convert_to_openpose_18(pose_result, image_array.shape)
            
            # í¬ì¦ˆ ë¶„ì„
            pose_analysis = {}
            if kwargs.get('enable_pose_analysis', True):
                pose_analysis = self._analyze_pose(keypoints_18, image_array.shape)
            
            # í’ˆì§ˆ í‰ê°€
            quality_metrics = self._evaluate_pose_quality(keypoints_18, pose_result)
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_pose_stats(processing_time, quality_metrics['overall_confidence'])
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'success': True,
                'keypoints_18': keypoints_18,
                'keypoints_mediapipe': pose_result,
                'pose_confidence': quality_metrics['overall_confidence'],
                'body_orientation': pose_analysis.get('orientation', 'unknown'),
                'pose_analysis': pose_analysis,
                'quality_metrics': quality_metrics,
                'processing_info': {
                    'processing_time': processing_time,
                    'keypoints_detected': sum(1 for kp in keypoints_18 if kp[2] > 0.5),
                    'total_keypoints': 18,
                    'detection_method': 'mediapipe' if MEDIAPIPE_AVAILABLE else 'dummy',
                    'device': self.device,
                    'device_type': self.device_type,
                    'm3_max_optimized': self.is_m3_max,
                    'model_complexity': self.model_complexity
                },
                'additional_keypoints': {
                    'face_keypoints': face_keypoints,
                    'hand_keypoints': hand_keypoints
                } if (face_keypoints or hand_keypoints) else None,
                'from_cache': False
            }
            
            # ìºì‹œ ì €ì¥
            if kwargs.get('cache_result', True):
                self._update_cache(cache_key, result)
            
            self.logger.info(f"âœ… í¬ì¦ˆ ì¶”ì • ì™„ë£Œ - ì‹ ë¢°ë„: {quality_metrics['overall_confidence']:.3f}, ì‹œê°„: {processing_time:.3f}ì´ˆ")
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}"
            self.logger.error(f"âŒ {error_msg}")
            
            return self._create_empty_result(error_msg)
    
    # =================================================================
    # ğŸ”§ í•µì‹¬ ì²˜ë¦¬ ë©”ì„œë“œë“¤
    # =================================================================
    
    async def _warmup_m3_max(self):
        """M3 Max ì›Œë°ì—…"""
        try:
            self.logger.info("ğŸ M3 Max í¬ì¦ˆ ì‹œìŠ¤í…œ ì›Œë°ì—…...")
            
            # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ ì›Œë°ì—…
            dummy_image = np.ones((256, 256, 3), dtype=np.uint8) * 128
            
            if self.pose_detector and MEDIAPIPE_AVAILABLE:
                _ = self.pose_detector.process(dummy_image)
                
            self.logger.info("âœ… M3 Max ì›Œë°ì—… ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"M3 Max ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    async def _load_and_preprocess_image(self, image_input: Union[str, np.ndarray, Image.Image]) -> Optional[np.ndarray]:
        """ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬ (M3 Max ìµœì í™”)"""
        try:
            # ì…ë ¥ íƒ€ì…ì— ë”°ë¥¸ ë¡œë“œ
            if isinstance(image_input, str):
                if not os.path.exists(image_input):
                    self.logger.error(f"ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {image_input}")
                    return None
                image_array = cv2.imread(image_input)
                image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)
            elif isinstance(image_input, np.ndarray):
                image_array = image_input.copy()
                if len(image_array.shape) == 3 and image_array.shape[2] == 3:
                    # BGR to RGB ë³€í™˜ (OpenCV ì´ë¯¸ì§€ì¸ ê²½ìš°)
                    if image_array.max() > 1.0:  # 0-255 ë²”ìœ„ì¸ ê²½ìš°
                        image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)
            elif isinstance(image_input, Image.Image):
                image_array = np.array(image_input.convert('RGB'))
            else:
                self.logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {type(image_input)}")
                return None
            
            # í¬ê¸° ì¡°ì • (M3 Maxì—ì„œ ë” í° í¬ê¸° í—ˆìš©)
            height, width = image_array.shape[:2]
            max_size = self.max_image_size
            
            if max(height, width) > max_size:
                if height > width:
                    new_height = max_size
                    new_width = int(width * max_size / height)
                else:
                    new_width = max_size
                    new_height = int(height * max_size / width)
                
                # M3 Maxì—ì„œ ë” ê³ í’ˆì§ˆ ë³´ê°„
                interpolation = cv2.INTER_LANCZOS4 if self.is_m3_max else cv2.INTER_AREA
                image_array = cv2.resize(image_array, (new_width, new_height), interpolation=interpolation)
                self.logger.info(f"ğŸ”„ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •: ({width}, {height}) -> ({new_width}, {new_height})")
            
            return image_array
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    async def _detect_pose_mediapipe(self, image_array: np.ndarray) -> List[Dict]:
        """MediaPipeë¥¼ ì‚¬ìš©í•œ í¬ì¦ˆ ì¶”ì • (M3 Max ìµœì í™”)"""
        try:
            # MediaPipe ì²˜ë¦¬
            results = self.pose_detector.process(image_array)
            
            if results.pose_landmarks:
                # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
                landmarks = []
                for landmark in results.pose_landmarks.landmark:
                    landmarks.append({
                        'x': landmark.x,
                        'y': landmark.y,
                        'z': landmark.z,
                        'visibility': landmark.visibility
                    })
                
                self.logger.debug(f"MediaPipe í‚¤í¬ì¸íŠ¸ ê²€ì¶œ: {len(landmarks)}ê°œ")
                return landmarks
            else:
                self.logger.warning("MediaPipeì—ì„œ í¬ì¦ˆë¥¼ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []
                
        except Exception as e:
            self.logger.error(f"MediaPipe í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return []
    
    async def _detect_face_keypoints(self, image_array: np.ndarray) -> Optional[List[Dict]]:
        """ì–¼êµ´ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ (M3 Max ê³ ì •ë°€ë„)"""
        if not self.face_detector:
            return None
        
        try:
            results = self.face_detector.process(image_array)
            
            if results.multi_face_landmarks:
                face_landmarks = []
                for landmark in results.multi_face_landmarks[0].landmark:
                    face_landmarks.append({
                        'x': landmark.x,
                        'y': landmark.y,
                        'z': landmark.z
                    })
                
                self.logger.debug(f"ì–¼êµ´ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ: {len(face_landmarks)}ê°œ")
                return face_landmarks
            
            return None
            
        except Exception as e:
            self.logger.warning(f"ì–¼êµ´ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _detect_hand_keypoints(self, image_array: np.ndarray) -> Optional[List[List[Dict]]]:
        """ì† í‚¤í¬ì¸íŠ¸ ê²€ì¶œ"""
        if not self.hands_detector:
            return None
        
        try:
            results = self.hands_detector.process(image_array)
            
            if results.multi_hand_landmarks:
                hands_landmarks = []
                for hand_landmarks in results.multi_hand_landmarks:
                    hand_keypoints = []
                    for landmark in hand_landmarks.landmark:
                        hand_keypoints.append({
                            'x': landmark.x,
                            'y': landmark.y,
                            'z': landmark.z
                        })
                    hands_landmarks.append(hand_keypoints)
                
                self.logger.debug(f"ì† í‚¤í¬ì¸íŠ¸ ê²€ì¶œ: {len(hands_landmarks)}ê°œ ì†")
                return hands_landmarks
            
            return None
            
        except Exception as e:
            self.logger.warning(f"ì† í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _detect_pose_dummy(self, image_array: np.ndarray) -> List[Dict]:
        """ë”ë¯¸ í¬ì¦ˆ ì¶”ì • (í´ë°±ìš©)"""
        height, width = image_array.shape[:2]
        
        # ê°€ìƒì˜ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ìƒì„± (ë” ì •êµí•˜ê²Œ)
        dummy_landmarks = []
        
        # 33ê°œ MediaPipe í‚¤í¬ì¸íŠ¸ ìƒì„±
        for i in range(33):
            # ì´ë¯¸ì§€ ì¤‘ì•™ ê·¼ì²˜ì— ëœë¤í•˜ê²Œ ë°°ì¹˜ (ì¸ì²´ ë¹„ìœ¨ ê³ ë ¤)
            if i < 11:  # ì–¼êµ´ ë¶€ë¶„
                x = 0.45 + np.random.random() * 0.1  # 45-55% ì§€ì 
                y = 0.1 + np.random.random() * 0.2   # 10-30% ì§€ì 
            elif i < 23:  # ìƒì²´ ë¶€ë¶„
                x = 0.3 + np.random.random() * 0.4   # 30-70% ì§€ì 
                y = 0.2 + np.random.random() * 0.3   # 20-50% ì§€ì 
            else:  # í•˜ì²´ ë¶€ë¶„
                x = 0.35 + np.random.random() * 0.3  # 35-65% ì§€ì 
                y = 0.4 + np.random.random() * 0.4   # 40-80% ì§€ì 
            
            z = np.random.random() * 0.1
            visibility = 0.7 + np.random.random() * 0.3  # 0.7-1.0
            
            dummy_landmarks.append({
                'x': x,
                'y': y,
                'z': z,
                'visibility': visibility
            })
        
        self.logger.debug("ë”ë¯¸ í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ìƒì„± ì™„ë£Œ")
        return dummy_landmarks
    
    def _convert_to_openpose_18(self, mediapipe_landmarks: List[Dict], image_shape: Tuple) -> List[List[float]]:
        """MediaPipe í‚¤í¬ì¸íŠ¸ë¥¼ OpenPose 18 í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì •ë°€ë„ í–¥ìƒ)"""
        
        height, width = image_shape[:2]
        keypoints_18 = [[0.0, 0.0, 0.0] for _ in range(18)]
        
        if not mediapipe_landmarks:
            return keypoints_18
        
        try:
            # MediaPipe -> OpenPose 18 ë§¤í•‘ (ì •í™•í•œ ì¸ë±ìŠ¤)
            mp_to_op18 = {
                0: 0,   # nose
                12: 1,  # neck (ì–´ê¹¨ ì¤‘ì ìœ¼ë¡œ ê³„ì‚°)
                12: 2,  # right_shoulder
                14: 3,  # right_elbow
                16: 4,  # right_wrist
                11: 5,  # left_shoulder
                13: 6,  # left_elbow
                15: 7,  # left_wrist
                24: 8,  # right_hip
                26: 9,  # right_knee
                28: 10, # right_ankle
                23: 11, # left_hip
                25: 12, # left_knee
                27: 13, # left_ankle
                2: 14,  # right_eye
                5: 15,  # left_eye
                8: 16,  # right_ear
                7: 17   # left_ear
            }
            
            # ê¸°ë³¸ ë§¤í•‘
            for op_idx, mp_idx in mp_to_op18.items():
                if mp_idx < len(mediapipe_landmarks):
                    landmark = mediapipe_landmarks[mp_idx]
                    keypoints_18[op_idx] = [
                        landmark['x'] * width,
                        landmark['y'] * height,
                        landmark['visibility']
                    ]
            
            # ëª© (neck) ê³„ì‚° - ì–‘ìª½ ì–´ê¹¨ì˜ ì¤‘ì  (ì •ë°€ë„ í–¥ìƒ)
            if len(mediapipe_landmarks) > 12:
                left_shoulder = mediapipe_landmarks[11]
                right_shoulder = mediapipe_landmarks[12]
                
                # ê°€ì¤‘í‰ê·  ê³„ì‚° (visibility ê³ ë ¤)
                left_weight = left_shoulder['visibility']
                right_weight = right_shoulder['visibility']
                total_weight = left_weight + right_weight
                
                if total_weight > 0:
                    neck_x = (left_shoulder['x'] * left_weight + right_shoulder['x'] * right_weight) / total_weight * width
                    neck_y = (left_shoulder['y'] * left_weight + right_shoulder['y'] * right_weight) / total_weight * height
                    neck_conf = min(left_shoulder['visibility'], right_shoulder['visibility'])
                    
                    keypoints_18[1] = [neck_x, neck_y, neck_conf]
            
            return keypoints_18
            
        except Exception as e:
            self.logger.error(f"í‚¤í¬ì¸íŠ¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return keypoints_18
    
    def _analyze_pose(self, keypoints_18: List[List[float]], image_shape: Tuple) -> Dict[str, Any]:
        """í¬ì¦ˆ ë¶„ì„ (M3 Max ê³ ê¸‰ ë¶„ì„)"""
        
        analysis = {}
        
        try:
            # 1. ì‹ ì²´ ë°©í–¥ ì¶”ì •
            analysis["orientation"] = self._estimate_body_orientation(keypoints_18)
            
            # 2. ê´€ì ˆ ê°ë„ ê³„ì‚° 
            analysis["angles"] = self._calculate_joint_angles(keypoints_18)
            
            # 3. ì‹ ì²´ ë¹„ìœ¨ ë¶„ì„
            analysis["proportions"] = self._analyze_body_proportions(keypoints_18)
            
            # 4. ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
            analysis["bbox"] = self._calculate_pose_bbox(keypoints_18, image_shape)
            
            # 5. í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜
            analysis["pose_type"] = self._classify_pose_type(keypoints_18, analysis["angles"])
            
            # 6. M3 Max ê³ ê¸‰ ë¶„ì„
            if self.is_m3_max:
                analysis["advanced_metrics"] = self._advanced_pose_analysis(keypoints_18)
            
        except Exception as e:
            self.logger.warning(f"í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            analysis = {"error": str(e)}
        
        return analysis
    
    def _advanced_pose_analysis(self, keypoints_18: List[List[float]]) -> Dict[str, Any]:
        """M3 Max ê³ ê¸‰ í¬ì¦ˆ ë¶„ì„"""
        try:
            metrics = {}
            
            # í¬ì¦ˆ ì•ˆì •ì„± ë¶„ì„
            valid_keypoints = [kp for kp in keypoints_18 if kp[2] > 0.5]
            metrics['pose_stability'] = len(valid_keypoints) / 18
            
            # ëŒ€ì¹­ì„± ë¶„ì„
            metrics['symmetry_score'] = self._calculate_pose_symmetry(keypoints_18)
            
            # í¬ì¦ˆ ë³µì¡ë„
            metrics['pose_complexity'] = self._calculate_pose_complexity(keypoints_18)
            
            return metrics
            
        except Exception as e:
            self.logger.warning(f"ê³ ê¸‰ í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_pose_symmetry(self, keypoints_18: List[List[float]]) -> float:
        """í¬ì¦ˆ ëŒ€ì¹­ì„± ê³„ì‚°"""
        try:
            # ëŒ€ì¹­ ìŒ ì •ì˜
            symmetric_pairs = [
                (2, 5),   # shoulders
                (3, 6),   # elbows
                (4, 7),   # wrists
                (8, 11),  # hips
                (9, 12),  # knees
                (10, 13), # ankles
                (14, 15), # eyes
                (16, 17)  # ears
            ]
            
            symmetry_scores = []
            
            for right_idx, left_idx in symmetric_pairs:
                right_kp = keypoints_18[right_idx]
                left_kp = keypoints_18[left_idx]
                
                if right_kp[2] > 0.5 and left_kp[2] > 0.5:
                    # Y ì¢Œí‘œ ì°¨ì´ (ëŒ€ì¹­ì„±)
                    y_diff = abs(right_kp[1] - left_kp[1])
                    max_y = max(right_kp[1], left_kp[1])
                    if max_y > 0:
                        symmetry = 1.0 - min(y_diff / max_y, 1.0)
                        symmetry_scores.append(symmetry)
            
            return np.mean(symmetry_scores) if symmetry_scores else 0.0
            
        except Exception as e:
            self.logger.warning(f"ëŒ€ì¹­ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_pose_complexity(self, keypoints_18: List[List[float]]) -> float:
        """í¬ì¦ˆ ë³µì¡ë„ ê³„ì‚°"""
        try:
            # í‚¤í¬ì¸íŠ¸ ê°„ ê±°ë¦¬ ë³€í™”ëŸ‰ìœ¼ë¡œ ë³µì¡ë„ ì¸¡ì •
            valid_points = [(x, y) for x, y, conf in keypoints_18 if conf > 0.5]
            
            if len(valid_points) < 3:
                return 0.0
            
            # ì—°ì†ëœ í‚¤í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬
            distances = []
            for i in range(len(valid_points) - 1):
                p1, p2 = valid_points[i], valid_points[i+1]
                dist = np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)
                distances.append(dist)
            
            # ê±°ë¦¬ì˜ í‘œì¤€í¸ì°¨ë¡œ ë³µì¡ë„ ê³„ì‚°
            complexity = np.std(distances) / (np.mean(distances) + 1e-6)
            return min(complexity, 1.0)
            
        except Exception as e:
            self.logger.warning(f"ë³µì¡ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    # =================================================================
    # ğŸ”§ ê¸°ì¡´ í—¬í¼ ë©”ì„œë“œë“¤ (ê°„ì†Œí™”)
    # =================================================================
    
    def _estimate_body_orientation(self, keypoints_18: List[List[float]]) -> str:
        """ì‹ ì²´ ë°©í–¥ ì¶”ì •"""
        try:
            # ì–´ê¹¨ì™€ ì—‰ë©ì´ í‚¤í¬ì¸íŠ¸
            left_shoulder = keypoints_18[5]
            right_shoulder = keypoints_18[2]
            left_hip = keypoints_18[11]
            right_hip = keypoints_18[8]
            
            if all(kp[2] > 0.5 for kp in [left_shoulder, right_shoulder, left_hip, right_hip]):
                shoulder_width = abs(right_shoulder[0] - left_shoulder[0])
                hip_width = abs(right_hip[0] - left_hip[0])
                avg_width = (shoulder_width + hip_width) / 2
                
                if avg_width < 80:
                    return "side"
                elif avg_width < 150:
                    return "diagonal"
                else:
                    return "front"
            
            return "unknown"
            
        except Exception:
            return "unknown"
    
    def _calculate_joint_angles(self, keypoints_18: List[List[float]]) -> Dict[str, float]:
        """ì£¼ìš” ê´€ì ˆ ê°ë„ ê³„ì‚°"""
        def angle_between_points(p1, p2, p3):
            if any(p[2] < 0.5 for p in [p1, p2, p3]):
                return 0.0
            
            v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
            v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
            
            cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
            cos_angle = np.clip(cos_angle, -1.0, 1.0)
            angle = np.degrees(np.arccos(cos_angle))
            
            return float(angle)
        
        angles = {}
        
        try:
            # íŒ” ê°ë„
            angles["left_arm_angle"] = angle_between_points(
                keypoints_18[5], keypoints_18[6], keypoints_18[7]
            )
            angles["right_arm_angle"] = angle_between_points(
                keypoints_18[2], keypoints_18[3], keypoints_18[4]
            )
            
            # ë‹¤ë¦¬ ê°ë„
            angles["left_leg_angle"] = angle_between_points(
                keypoints_18[11], keypoints_18[12], keypoints_18[13]
            )
            angles["right_leg_angle"] = angle_between_points(
                keypoints_18[8], keypoints_18[9], keypoints_18[10]
            )
            
        except Exception as e:
            self.logger.warning(f"ê´€ì ˆ ê°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        return angles
    
    def _analyze_body_proportions(self, keypoints_18: List[List[float]]) -> Dict[str, float]:
        """ì‹ ì²´ ë¹„ìœ¨ ë¶„ì„ (ê°„ì†Œí™”)"""
        proportions = {}
        
        try:
            # ë¨¸ë¦¬ ê¸¸ì´, ëª¸í†µ ê¸¸ì´, ë‹¤ë¦¬ ê¸¸ì´ ê³„ì‚°
            if keypoints_18[0][2] > 0.5 and keypoints_18[1][2] > 0.5:
                proportions["head_length"] = abs(keypoints_18[1][1] - keypoints_18[0][1])
            
            if keypoints_18[1][2] > 0.5 and keypoints_18[8][2] > 0.5:
                proportions["torso_length"] = abs(keypoints_18[8][1] - keypoints_18[1][1])
                
        except Exception:
            pass
        
        return proportions
    
    def _calculate_pose_bbox(self, keypoints_18: List[List[float]], image_shape: Tuple) -> Dict[str, int]:
        """í¬ì¦ˆ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°"""
        valid_points = [(x, y) for x, y, conf in keypoints_18 if conf > 0.5]
        
        if not valid_points:
            return {"x": 0, "y": 0, "width": 0, "height": 0}
        
        xs, ys = zip(*valid_points)
        
        x_min, x_max = min(xs), max(xs)
        y_min, y_max = min(ys), max(ys)
        
        # ì—¬ë°± ì¶”ê°€
        margin_x = int((x_max - x_min) * 0.15)
        margin_y = int((y_max - y_min) * 0.15)
        
        height, width = image_shape[:2]
        
        return {
            "x": max(0, int(x_min - margin_x)),
            "y": max(0, int(y_min - margin_y)),
            "width": min(width, int(x_max - x_min + 2 * margin_x)),
            "height": min(height, int(y_max - y_min + 2 * margin_y))
        }
    
    def _classify_pose_type(self, keypoints_18: List[List[float]], angles: Dict[str, float]) -> str:
        """í¬ì¦ˆ íƒ€ì… ë¶„ë¥˜"""
        try:
            left_arm = angles.get("left_arm_angle", 180)
            right_arm = angles.get("right_arm_angle", 180)
            left_leg = angles.get("left_leg_angle", 180)
            right_leg = angles.get("right_leg_angle", 180)
            
            # T-í¬ì¦ˆ
            if abs(left_arm - 180) < 20 and abs(right_arm - 180) < 20:
                return "t_pose"
            # A-í¬ì¦ˆ
            elif 140 < left_arm < 170 and 140 < right_arm < 170:
                return "a_pose"
            # ê±·ê¸°
            elif abs(left_leg - right_leg) > 30:
                return "walking"
            # ì•‰ê¸°
            elif left_leg < 140 or right_leg < 140:
                return "sitting"
            else:
                return "standing"
                
        except Exception:
            return "unknown"
    
    def _evaluate_pose_quality(self, keypoints_18: List[List[float]], mediapipe_keypoints: List[Dict]) -> Dict[str, float]:
        """í¬ì¦ˆ í’ˆì§ˆ í‰ê°€ (M3 Max í–¥ìƒ)"""
        try:
            # 1. ê²€ì¶œ ë¹„ìœ¨
            detected_18 = sum(1 for kp in keypoints_18 if kp[2] > 0.5)
            detection_rate = detected_18 / 18
            
            # 2. ì£¼ìš” í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë¹„ìœ¨
            major_indices = [0, 1, 2, 5, 8, 11]  # nose, neck, shoulders, hips
            major_detected = sum(1 for idx in major_indices if keypoints_18[idx][2] > 0.5)
            major_detection_rate = major_detected / len(major_indices)
            
            # 3. í‰ê·  ì‹ ë¢°ë„
            confidences = [kp[2] for kp in keypoints_18 if kp[2] > 0]
            avg_confidence = np.mean(confidences) if confidences else 0.0
            
            # 4. ëŒ€ì¹­ì„± ì ìˆ˜ (M3 Max ì „ìš©)
            symmetry_score = 0.8  # ê¸°ë³¸ê°’
            if self.is_m3_max:
                symmetry_score = self._calculate_pose_symmetry(keypoints_18)
            
            # 5. ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
            overall_confidence = (
                detection_rate * 0.3 +
                major_detection_rate * 0.3 +
                avg_confidence * 0.3 +
                symmetry_score * 0.1
            )
            
            # M3 Max ë³´ë„ˆìŠ¤
            if self.is_m3_max:
                overall_confidence = min(1.0, overall_confidence * 1.05)
            
            return {
                'overall_confidence': overall_confidence,
                'detection_rate': detection_rate,
                'major_detection_rate': major_detection_rate,
                'average_confidence': avg_confidence,
                'symmetry_score': symmetry_score,
                'quality_grade': self._get_quality_grade(overall_confidence)
            }
            
        except Exception as e:
            self.logger.error(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'overall_confidence': 0.0,
                'detection_rate': 0.0,
                'major_detection_rate': 0.0,
                'average_confidence': 0.0,
                'symmetry_score': 0.0,
                'quality_grade': 'poor'
            }
    
    def _get_quality_grade(self, overall_confidence: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
        if overall_confidence >= 0.9:
            return "excellent"
        elif overall_confidence >= 0.8:
            return "good"
        elif overall_confidence >= 0.6:
            return "fair"
        elif overall_confidence >= 0.4:
            return "poor"
        else:
            return "very_poor"
    
    # =================================================================
    # ğŸ”§ ìºì‹œ ë° ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # =================================================================
    
    def _generate_cache_key(self, image_input) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            if isinstance(image_input, str):
                return f"pose_{hash(image_input)}_{self.model_complexity}"
            elif isinstance(image_input, np.ndarray):
                return f"pose_{hash(image_input.tobytes())}_{self.model_complexity}"
            else:
                return f"pose_{hash(str(image_input))}_{self.model_complexity}"
        except Exception:
            return f"pose_fallback_{time.time()}"
    
    def _update_cache(self, key: str, result: Dict[str, Any]):
        """ìºì‹œ ì—…ë°ì´íŠ¸"""
        try:
            if len(self.detection_cache) >= self.cache_max_size:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = next(iter(self.detection_cache))
                del self.detection_cache[oldest_key]
            
            # ê²°ê³¼ ë³µì‚¬í•´ì„œ ì €ì¥
            cached_result = {k: v for k, v in result.items() if k != 'processing_info'}
            self.detection_cache[key] = cached_result
            
        except Exception as e:
            self.logger.warning(f"ìºì‹œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _update_pose_stats(self, processing_time: float, quality_score: float):
        """2ë‹¨ê³„ ì „ìš© í†µê³„ ì—…ë°ì´íŠ¸"""
        self.pose_stats['total_processed'] += 1
        
        if quality_score > 0.5:
            self.pose_stats['successful_detections'] += 1
        
        # ì´ë™ í‰ê· ìœ¼ë¡œ ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        alpha = 0.1
        self.pose_stats['average_processing_time'] = (
            alpha * processing_time + 
            (1 - alpha) * self.pose_stats['average_processing_time']
        )
        
        self.pose_stats['last_quality_score'] = quality_score
    
    def _create_dummy_detector(self):
        """ë”ë¯¸ ê²€ì¶œê¸° ìƒì„±"""
        class DummyDetector:
            def process(self, image):
                return None
        return DummyDetector()
    
    def _create_empty_result(self, reason: str) -> Dict[str, Any]:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        return {
            'success': True,  # íŒŒì´í”„ë¼ì¸ ì§„í–‰ì„ ìœ„í•´ True ìœ ì§€
            'error': reason,
            'keypoints_18': [[0.0, 0.0, 0.0] for _ in range(18)],
            'keypoints_mediapipe': [],
            'pose_confidence': 0.0,
            'body_orientation': 'unknown',
            'pose_analysis': {},
            'quality_metrics': {
                'overall_confidence': 0.0,
                'quality_grade': 'failed'
            },
            'processing_info': {
                'processing_time': 0.0,
                'keypoints_detected': 0,
                'total_keypoints': 18,
                'detection_method': 'none',
                'device': self.device,
                'error_details': reason
            },
            'additional_keypoints': None,
            'from_cache': False
        }
    
    # =================================================================
    # ğŸ”§ Pipeline Manager í˜¸í™˜ ë©”ì„œë“œë“¤
    # =================================================================
    
    async def get_step_info(self) -> Dict[str, Any]:
        """ğŸ” 2ë‹¨ê³„ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        return {
            "step_name": "pose_estimation",
            "step_number": 2,
            "device": self.device,
            "device_type": self.device_type,
            "initialized": self.is_initialized,
            "config_keys": list(self.config.keys()),
            "pose_stats": self.pose_stats.copy(),
            "keypoint_formats": {
                "openpose_18": self.OPENPOSE_18_KEYPOINTS,
                "mediapipe_33": len(self.MEDIAPIPE_KEYPOINT_NAMES)
            },
            "cache_usage": {
                "cache_size": len(self.detection_cache),
                "cache_limit": self.cache_max_size,
                "hit_rate": self.pose_stats.get('cache_hits', 0) / max(1, self.pose_stats['total_processed'])
            },
            "detectors_available": {
                "pose": self.pose_detector is not None,
                "face": self.face_detector is not None,
                "hands": self.hands_detector is not None,
                "mediapipe_enabled": MEDIAPIPE_AVAILABLE
            },
            "capabilities": {
                "model_complexity": self.model_complexity,
                "max_image_size": self.max_image_size,
                "face_detection": self.use_face,
                "hand_detection": self.use_hands,
                "segmentation_enabled": self.enable_segmentation,
                "advanced_analysis": self.is_m3_max,
                "is_m3_max": self.is_m3_max,
                "optimization_enabled": self.optimization_enabled,
                "quality_level": self.quality_level
            }
        }
    
    def visualize_pose(self, image: np.ndarray, keypoints_18: List[List[float]], save_path: Optional[str] = None) -> np.ndarray:
        """í¬ì¦ˆ ì‹œê°í™”"""
        vis_image = image.copy()
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for i, (x, y, conf) in enumerate(keypoints_18):
            if conf > 0.5:
                cv2.circle(vis_image, (int(x), int(y)), 5, (0, 255, 0), -1)
                cv2.putText(vis_image, str(i), (int(x), int(y-10)), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)
        
        # ì—°ê²°ì„  ê·¸ë¦¬ê¸°
        connections = self._get_openpose_connections()
        for pt1_idx, pt2_idx in connections:
            pt1 = keypoints_18[pt1_idx]
            pt2 = keypoints_18[pt2_idx]
            
            if pt1[2] > 0.5 and pt2[2] > 0.5:
                cv2.line(vis_image, 
                        (int(pt1[0]), int(pt1[1])), 
                        (int(pt2[0]), int(pt2[1])), 
                        (255, 0, 0), 3)
        
        if save_path:
            cv2.imwrite(save_path, cv2.cvtColor(vis_image, cv2.COLOR_RGB2BGR))
            self.logger.info(f"ğŸ’¾ í¬ì¦ˆ ì‹œê°í™” ì €ì¥: {save_path}")
        
        return vis_image
    
    def export_keypoints(self, keypoints_18: List[List[float]], format: str = "json") -> str:
        """í‚¤í¬ì¸íŠ¸ ë‚´ë³´ë‚´ê¸°"""
        if format.lower() == "json":
            export_data = {
                "format": "openpose_18",
                "keypoints": keypoints_18,
                "keypoint_names": self.OPENPOSE_18_KEYPOINTS,
                "connections": self._get_openpose_connections(),
                "device_info": {
                    "device": self.device,
                    "m3_max_optimized": self.is_m3_max,
                    "model_complexity": self.model_complexity
                }
            }
            return json.dumps(export_data, indent=2)
        
        elif format.lower() == "csv":
            lines = ["id,name,x,y,confidence"]
            for i, (x, y, conf) in enumerate(keypoints_18):
                lines.append(f"{i},{self.OPENPOSE_18_KEYPOINTS[i]},{x},{y},{conf}")
            return "\n".join(lines)
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")
    
    def _get_openpose_connections(self) -> List[List[int]]:
        """OpenPose ì—°ê²°ì„  ì •ë³´"""
        return [
            # ëª¸í†µ
            [1, 2], [1, 5], [2, 8], [5, 11], [8, 11],
            
            # ì˜¤ë¥¸íŒ”
            [2, 3], [3, 4],
            
            # ì™¼íŒ”  
            [5, 6], [6, 7],
            
            # ì˜¤ë¥¸ë‹¤ë¦¬
            [8, 9], [9, 10],
            
            # ì™¼ë‹¤ë¦¬
            [11, 12], [12, 13],
            
            # ë¨¸ë¦¬
            [1, 0], [0, 14], [0, 15], [14, 16], [15, 17]
        ]
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info("ğŸ§¹ 2ë‹¨ê³„: í¬ì¦ˆ ì¶”ì • ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            
            # ê²€ì¶œê¸°ë“¤ ì •ë¦¬
            if self.pose_detector and hasattr(self.pose_detector, 'close'):
                self.pose_detector.close()
            if self.face_detector and hasattr(self.face_detector, 'close'):
                self.face_detector.close()
            if self.hands_detector and hasattr(self.hands_detector, 'close'):
                self.hands_detector.close()
            
            self.pose_detector = None
            self.face_detector = None
            self.hands_detector = None
            self.mp_pose = None
            self.mp_drawing = None
            
            # ìºì‹œ ì •ë¦¬
            self.detection_cache.clear()
            
            # ìŠ¤ë ˆë“œ í’€ ì •ë¦¬
            self.executor.shutdown(wait=True)
            
            self.is_initialized = False
            self.logger.info("âœ… 2ë‹¨ê³„ í¬ì¦ˆ ì¶”ì • ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


# =================================================================
# ğŸ”„ í•˜ìœ„ í˜¸í™˜ì„± ì§€ì› (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜)
# =================================================================

async def create_pose_estimation_step(
    device: str = "auto",
    config: Dict[str, Any] = None
) -> PoseEstimationStep:
    """
    ğŸ”„ ê¸°ì¡´ íŒ©í† ë¦¬ í•¨ìˆ˜ í˜¸í™˜ (ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ í˜¸í™˜)
    
    Args:
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ("auto"ëŠ” ìë™ ê°ì§€)
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        PoseEstimationStep: ì´ˆê¸°í™”ëœ 2ë‹¨ê³„ ìŠ¤í…
    """
    # ê¸°ì¡´ ë°©ì‹ í˜¸í™˜
    device_param = None if device == "auto" else device
    
    default_config = {
        "model_complexity": 2,
        "min_detection_confidence": 0.7,
        "min_tracking_confidence": 0.5,
        "enable_segmentation": False,
        "max_image_size": 1024,
        "use_face": True,
        "use_hands": False
    }
    
    final_config = {**default_config, **(config or {})}
    
    # âœ… ìƒˆë¡œìš´ í†µì¼ëœ ìƒì„±ì ì‚¬ìš©
    step = PoseEstimationStep(device=device_param, config=final_config)
    
    if not await step.initialize():
        logger.warning("2ë‹¨ê³„ ì´ˆê¸°í™” ì‹¤íŒ¨í–ˆì§€ë§Œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    return step

# ê¸°ì¡´ í´ë˜ìŠ¤ëª… ë³„ì¹­ (ì™„ì „ í˜¸í™˜)
PoseEstimationStepLegacy = PoseEstimationStep