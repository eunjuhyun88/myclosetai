#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 02: Pose Estimation - Central Hub DI Container v7.0 ì™„ì „ ë¦¬íŒ©í† ë§ 
================================================================================

âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
âœ… BaseStepMixin ìƒì† íŒ¨í„´ (Human Parsing Stepê³¼ ë™ì¼)
âœ… MediaPipe Pose ëª¨ë¸ ì§€ì› (ìš°ì„ ìˆœìœ„ 1)
âœ… OpenPose ëª¨ë¸ ì§€ì› (í´ë°± ì˜µì…˜)
âœ… YOLOv8-Pose ëª¨ë¸ ì§€ì› (ì‹¤ì‹œê°„)
âœ… HRNet ëª¨ë¸ ì§€ì› (ê³ ì •ë°€)
âœ… 17ê°œ COCO keypoints ê°ì§€
âœ… confidence score ê³„ì‚°
âœ… Mock ëª¨ë¸ ì™„ì „ ì œê±°
âœ… ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
âœ… ë‹¤ì¤‘ ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ

íŒŒì¼ ìœ„ì¹˜: backend/app/ai_pipeline/steps/step_02_pose_estimation.py
ì‘ì„±ì: MyCloset AI Team  
ë‚ ì§œ: 2025-08-01
ë²„ì „: v7.0 (Central Hub DI Container ì™„ì „ ë¦¬íŒ©í† ë§)
"""

# ==============================================
# ğŸ”¥ 1. Import ì„¹ì…˜ (Central Hub íŒ¨í„´)
# ==============================================

import os
import sys
import gc
import time
import asyncio
import logging
import threading
import traceback
import hashlib
import json
import base64
import math
import warnings
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache, wraps
from contextlib import asynccontextmanager

# ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=ImportWarning)

# ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False

try:
    import scipy
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# TYPE_CHECKINGìœ¼ë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
if TYPE_CHECKING:
    from app.ai_pipeline.utils.model_loader import ModelLoader
    from ..factories.step_factory import StepFactory
    from ..steps.base_step_mixin import BaseStepMixin

logger = logging.getLogger(__name__)
def detect_m3_max():
    """M3 Max ê°ì§€"""
    try:
        import platform, subprocess
        if platform.system() == 'Darwin':
            result = subprocess.run(
                ['sysctl', '-n', 'machdep.cpu.brand_string'],
                capture_output=True, text=True, timeout=5
            )
            return 'M3' in result.stdout
    except:
        pass
    return False

IS_M3_MAX = detect_m3_max()
MEMORY_GB = 16.0

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    import torchvision.transforms as transforms
    from torchvision.transforms.functional import resize, to_pil_image, to_tensor
    TORCH_AVAILABLE = True
    TORCH_VERSION = torch.__version__
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if torch.backends.mps.is_available():
        DEVICE = "mps"
        torch.mps.set_per_process_memory_fraction(0.7)
    elif torch.cuda.is_available():
        DEVICE = "cuda"
    else:
        DEVICE = "cpu"
        
except ImportError as e:
    raise ImportError(f"âŒ PyTorch í•„ìˆ˜: {e}")

try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError as e:
    raise ImportError(f"âŒ Pillow í•„ìˆ˜: {e}")

# ì„ íƒì  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

try:
    from transformers import AutoModel, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import safetensors.torch as st
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False

try:
    from scipy.optimize import curve_fit
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False


def _get_central_hub_container():
    """Central Hub DI Container ì•ˆì „í•œ ë™ì  í•´ê²°"""
    try:
        import importlib
        module = importlib.import_module('app.core.di_container')
        get_global_fn = getattr(module, 'get_global_container', None)
        if get_global_fn:
            return get_global_fn()
        return None
    except ImportError:
        return None
    except Exception:
        return None

def _inject_dependencies_safe(step_instance):
    """Central Hub DI Containerë¥¼ í†µí•œ ì•ˆì „í•œ ì˜ì¡´ì„± ì£¼ì…"""
    try:
        container = _get_central_hub_container()
        if container and hasattr(container, 'inject_to_step'):
            return container.inject_to_step(step_instance)
        return 0
    except Exception:
        return 0

def _get_service_from_central_hub(service_key: str):
    """Central Hubë¥¼ í†µí•œ ì•ˆì „í•œ ì„œë¹„ìŠ¤ ì¡°íšŒ"""
    try:
        container = _get_central_hub_container()
        if container:
            return container.get(service_key)
        return None
    except Exception:
        return None

logger = logging.getLogger(__name__)

# BaseStepMixin ë™ì  import (ìˆœí™˜ì°¸ì¡° ì™„ì „ ë°©ì§€)
def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        import importlib
        module = importlib.import_module('app.ai_pipeline.steps.base_step_mixin')
        return getattr(module, 'BaseStepMixin', None)
    except ImportError:
        logger.error("âŒ BaseStepMixin ë™ì  import ì‹¤íŒ¨")
        return None

BaseStepMixin = get_base_step_mixin_class()

# BaseStepMixin í´ë°± í´ë˜ìŠ¤ (step_02_pose_estimation.pyìš©)
if BaseStepMixin is None:
    import asyncio
    from typing import Dict, Any, Optional, List
    
    class BaseStepMixin:
        """PoseEstimationStepìš© BaseStepMixin í´ë°± í´ë˜ìŠ¤"""
        
        def __init__(self, **kwargs):
            # ê¸°ë³¸ ì†ì„±ë“¤
            self.logger = logging.getLogger(self.__class__.__name__)
            self.step_name = kwargs.get('step_name', 'PoseEstimationStep')
            self.step_id = kwargs.get('step_id', 2)
            self.device = kwargs.get('device', 'cpu')
            
            # AI ëª¨ë¸ ê´€ë ¨ ì†ì„±ë“¤ (PoseEstimationStepì´ í•„ìš”ë¡œ í•˜ëŠ”)
            self.ai_models = {}
            self.models_loading_status = {
                'mediapipe': False,
                'openpose': False,
                'yolov8': False,
                'hrnet': False,
                'total_loaded': 0,
                'loading_errors': []
            }
            self.model_interface = None
            self.loaded_models = {}
            
            # Pose Estimation íŠ¹í™” ì†ì„±ë“¤
            self.pose_models = {}
            self.pose_ready = False
            self.keypoints_cache = {}
            
            # ìƒíƒœ ê´€ë ¨ ì†ì„±ë“¤
            self.is_initialized = False
            self.is_ready = False
            self.has_model = False
            self.model_loaded = False
            self.warmup_completed = False
            
            # Central Hub DI Container ê´€ë ¨
            self.model_loader = None
            self.memory_manager = None
            self.data_converter = None
            self.di_container = None
            
            # ì„±ëŠ¥ í†µê³„
            self.performance_stats = {
                'total_processed': 0,
                'avg_processing_time': 0.0,
                'error_count': 0,
                'success_rate': 1.0
            }
            
            # Pose Estimation ì„¤ì •
            self.confidence_threshold = 0.5
            self.use_subpixel = True
            
            # ëª¨ë¸ ìš°ì„ ìˆœìœ„ (MediaPipe ìš°ì„ )
            self.model_priority = [
                'mediapipe',
                'yolov8_pose', 
                'openpose',
                'hrnet'
            ]
            
            self.logger.info(f"âœ… {self.step_name} BaseStepMixin í´ë°± í´ë˜ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        async def process(self, **kwargs) -> Dict[str, Any]:
            """ê¸°ë³¸ process ë©”ì„œë“œ - _run_ai_inference í˜¸ì¶œ"""
            try:
                start_time = time.time()
                
                # _run_ai_inference ë©”ì„œë“œê°€ ìˆìœ¼ë©´ í˜¸ì¶œ
                if hasattr(self, '_run_ai_inference'):
                    result = await self._run_ai_inference(kwargs)
                    
                    # ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
                    if isinstance(result, dict):
                        result['processing_time'] = time.time() - start_time
                        result['step_name'] = self.step_name
                        result['step_id'] = self.step_id
                    
                    return result
                else:
                    # ê¸°ë³¸ ì‘ë‹µ
                    return {
                        'success': False,
                        'error': '_run_ai_inference ë©”ì„œë“œê°€ êµ¬í˜„ë˜ì§€ ì•ŠìŒ',
                        'processing_time': time.time() - start_time,
                        'step_name': self.step_name,
                        'step_id': self.step_id
                    }
                    
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} process ì‹¤íŒ¨: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                    'step_name': self.step_name,
                    'step_id': self.step_id
                }
        
        async def initialize(self) -> bool:
            """ì´ˆê¸°í™” ë©”ì„œë“œ"""
            try:
                if self.is_initialized:
                    return True
                
                self.logger.info(f"ğŸ”„ {self.step_name} ì´ˆê¸°í™” ì‹œì‘...")
                
                # í¬ì¦ˆ ëª¨ë¸ë“¤ ë¡œë”© (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” _load_pose_models_via_central_hub í˜¸ì¶œ)
                if hasattr(self, '_load_pose_models_via_central_hub'):
                    loaded_count = self._load_pose_models_via_central_hub()
                    if loaded_count == 0:
                        self.logger.error("âŒ í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì´ˆê¸°í™” ì‹¤íŒ¨")
                        return False
                
                self.is_initialized = True
                self.is_ready = True
                self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ")
                return True
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                return False
        
        def cleanup(self):
            """ì •ë¦¬ ë©”ì„œë“œ"""
            try:
                self.logger.info(f"ğŸ”„ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
                
                # AI ëª¨ë¸ë“¤ ì •ë¦¬
                for model_name, model in self.ai_models.items():
                    try:
                        if hasattr(model, 'cleanup'):
                            model.cleanup()
                        del model
                    except Exception as e:
                        self.logger.debug(f"ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨ ({model_name}): {e}")
                
                # ìºì‹œ ì •ë¦¬
                self.ai_models.clear()
                if hasattr(self, 'pose_models'):
                    self.pose_models.clear()
                if hasattr(self, 'keypoints_cache'):
                    self.keypoints_cache.clear()
                
                # ğŸ”¥ 128GB M3 Max ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif hasattr(torch, 'mps') and torch.mps.is_available():
                        torch.mps.empty_cache()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                
                # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                import gc
                for _ in range(3):
                    gc.collect()
                
                self.logger.info(f"âœ… {self.step_name} ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ {self.step_name} ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        def get_status(self) -> Dict[str, Any]:
            """ìƒíƒœ ì¡°íšŒ"""
            return {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'is_initialized': self.is_initialized,
                'is_ready': self.is_ready,
                'device': self.device,
                'pose_ready': getattr(self, 'pose_ready', False),
                'models_loaded': len(getattr(self, 'loaded_models', {})),
                'model_priority': getattr(self, 'model_priority', []),
                'confidence_threshold': getattr(self, 'confidence_threshold', 0.5),
                'use_subpixel': getattr(self, 'use_subpixel', True),
                'fallback_mode': True
            }

        def _get_service_from_central_hub(self, service_key: str):
            """Central Hubì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
            try:
                if hasattr(self, 'di_container') and self.di_container:
                    return self.di_container.get_service(service_key)
                return None
            except Exception as e:
                self.logger.warning(f"âš ï¸ Central Hub ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                return None

        def convert_api_input_to_step_input(self, api_input: Dict[str, Any]) -> Dict[str, Any]:
            """API ì…ë ¥ì„ Step ì…ë ¥ìœ¼ë¡œ ë³€í™˜"""
            try:
                step_input = api_input.copy()
                
                # ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ (ë‹¤ì–‘í•œ í‚¤ ì´ë¦„ ì§€ì›)
                image = None
                for key in ['image', 'person_image', 'input_image', 'original_image']:
                    if key in step_input:
                        image = step_input[key]
                        break
                
                if image is None and 'session_id' in step_input:
                    # ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ
                    try:
                        session_manager = self._get_service_from_central_hub('session_manager')
                        if session_manager:
                            person_image, clothing_image = None, None
                            
                            try:
                                # ì„¸ì…˜ ë§¤ë‹ˆì €ê°€ ë™ê¸° ë©”ì„œë“œë¥¼ ì œê³µí•˜ëŠ”ì§€ í™•ì¸
                                if hasattr(session_manager, 'get_session_images_sync'):
                                    person_image, clothing_image = session_manager.get_session_images_sync(step_input['session_id'])
                                elif hasattr(session_manager, 'get_session_images'):
                                    # ë¹„ë™ê¸° ë©”ì„œë“œë¥¼ ë™ê¸°ì ìœ¼ë¡œ í˜¸ì¶œ
                                    import asyncio
                                    import concurrent.futures
                                    
                                    def run_async_session_load():
                                        try:
                                            return asyncio.run(session_manager.get_session_images(step_input['session_id']))
                                        except Exception as async_error:
                                            self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨: {async_error}")
                                            return None, None
                                    
                                    try:
                                        with concurrent.futures.ThreadPoolExecutor() as executor:
                                            future = executor.submit(run_async_session_load)
                                            person_image, clothing_image = future.result(timeout=10)
                                    except Exception as executor_error:
                                        self.logger.warning(f"âš ï¸ ì„¸ì…˜ ë¡œë“œ ThreadPoolExecutor ì‹¤íŒ¨: {executor_error}")
                                        person_image, clothing_image = None, None
                                else:
                                    self.logger.warning("âš ï¸ ì„¸ì…˜ ë§¤ë‹ˆì €ì— ì ì ˆí•œ ë©”ì„œë“œê°€ ì—†ìŒ")
                            except Exception as e:
                                self.logger.warning(f"âš ï¸ ì„¸ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
                                person_image, clothing_image = None, None
                            
                            if person_image:
                                image = person_image
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
                
                # ë³€í™˜ëœ ì…ë ¥ êµ¬ì„±
                converted_input = {
                    'image': image,
                    'person_image': image,
                    'session_id': step_input.get('session_id'),
                    'detection_confidence': step_input.get('detection_confidence', 0.5),
                    'clothing_type': step_input.get('clothing_type', 'shirt')
                }
                
                self.logger.info(f"âœ… API ì…ë ¥ ë³€í™˜ ì™„ë£Œ: {len(converted_input)}ê°œ í‚¤")
                return converted_input
                
            except Exception as e:
                self.logger.error(f"âŒ API ì…ë ¥ ë³€í™˜ ì‹¤íŒ¨: {e}")
                return api_input
        
        def get_model_status(self) -> Dict[str, Any]:
            """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ (PoseEstimationStep í˜¸í™˜)"""
            return {
                'step_name': self.step_name,
                'step_id': self.step_id,
                'pose_ready': getattr(self, 'pose_ready', False),
                'models_loading_status': getattr(self, 'models_loading_status', {}),
                'loaded_models': list(getattr(self, 'ai_models', {}).keys()),
                'model_priority': getattr(self, 'model_priority', []),
                'confidence_threshold': getattr(self, 'confidence_threshold', 0.5),
                'use_subpixel': getattr(self, 'use_subpixel', True)
            }
        
        # BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤
        def set_model_loader(self, model_loader):
            """ModelLoader ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.model_loader = model_loader
                self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
                
                # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹œë„
                if hasattr(model_loader, 'create_step_interface'):
                    try:
                        self.model_interface = model_loader.create_step_interface(self.step_name)
                        self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì£¼ì… ì™„ë£Œ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨, ModelLoader ì§ì ‘ ì‚¬ìš©: {e}")
                        self.model_interface = model_loader
                else:
                    self.model_interface = model_loader
                    
            except Exception as e:
                self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
                self.model_loader = None
                self.model_interface = None
        
        def set_memory_manager(self, memory_manager):
            """MemoryManager ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.memory_manager = memory_manager
                self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        def set_data_converter(self, data_converter):
            """DataConverter ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
            try:
                self.data_converter = data_converter
                self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
        
        def set_di_container(self, di_container):
            """DI Container ì˜ì¡´ì„± ì£¼ì…"""
            try:
                self.di_container = di_container
                self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ DI Container ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
# ==============================================
# ğŸ”¥ 2. í¬ì¦ˆ ì¶”ì • ìƒìˆ˜ ë° ë°ì´í„° êµ¬ì¡°
# ==============================================

class PoseModel(Enum):
    """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ íƒ€ì…"""
    MEDIAPIPE = "mediapipe"
    OPENPOSE = "openpose"
    YOLOV8_POSE = "yolov8_pose"
    HRNET = "hrnet"
    DIFFUSION_POSE = "diffusion_pose"

class PoseQuality(Enum):
    """í¬ì¦ˆ í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"     # 90-100ì 
    GOOD = "good"              # 75-89ì   
    ACCEPTABLE = "acceptable"   # 60-74ì 
    POOR = "poor"              # 40-59ì 
    VERY_POOR = "very_poor"    # 0-39ì 

# COCO 17 í‚¤í¬ì¸íŠ¸ ì •ì˜ (MediaPipe, YOLOv8 í‘œì¤€)
COCO_17_KEYPOINTS = [
    "nose", "left_eye", "right_eye", "left_ear", "right_ear",
    "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
    "left_wrist", "right_wrist", "left_hip", "right_hip",
    "left_knee", "right_knee", "left_ankle", "right_ankle"
]

# OpenPose 18 í‚¤í¬ì¸íŠ¸ ì •ì˜ 
OPENPOSE_18_KEYPOINTS = [
    "nose", "neck", "right_shoulder", "right_elbow", "right_wrist",
    "left_shoulder", "left_elbow", "left_wrist", "middle_hip", "right_hip", 
    "right_knee", "right_ankle", "left_hip", "left_knee", "left_ankle",
    "right_eye", "left_eye", "right_ear", "left_ear"
]

# í‚¤í¬ì¸íŠ¸ ì—°ê²° êµ¬ì¡° (ìŠ¤ì¼ˆë ˆí†¤)
SKELETON_CONNECTIONS = [
    (0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7), (1, 8),
    (8, 9), (9, 10), (10, 11), (8, 12), (12, 13), (13, 14), (0, 15),
    (15, 17), (0, 16), (16, 18)
]

# í‚¤í¬ì¸íŠ¸ ìƒ‰ìƒ ë§¤í•‘
KEYPOINT_COLORS = [
    (255, 0, 0), (255, 85, 0), (255, 170, 0), (255, 255, 0), (170, 255, 0),
    (85, 255, 0), (0, 255, 0), (0, 255, 85), (0, 255, 170), (0, 255, 255),
    (0, 170, 255), (0, 85, 255), (0, 0, 255), (85, 0, 255), (170, 0, 255),
    (255, 0, 255), (255, 0, 170), (255, 0, 85), (255, 0, 0)
]

@dataclass
class PoseResult:
    """í¬ì¦ˆ ì¶”ì • ê²°ê³¼"""
    keypoints: List[List[float]] = field(default_factory=list)
    confidence_scores: List[float] = field(default_factory=list)
    joint_angles: Dict[str, float] = field(default_factory=dict)
    body_proportions: Dict[str, float] = field(default_factory=dict)
    pose_quality: PoseQuality = PoseQuality.POOR
    overall_confidence: float = 0.0
    processing_time: float = 0.0
    model_used: str = ""
    subpixel_accuracy: bool = False
    
    # ê³ ê¸‰ ë¶„ì„ ê²°ê³¼
    keypoints_with_uncertainty: List[Dict[str, Any]] = field(default_factory=list)
    advanced_body_metrics: Dict[str, Any] = field(default_factory=dict)
    skeleton_structure: Dict[str, Any] = field(default_factory=dict)
    ensemble_info: Dict[str, Any] = field(default_factory=dict)

# ==============================================
# ğŸ”¥ 3. ì‹¤ì œ AI ëª¨ë¸ í´ë˜ìŠ¤ë“¤
# ==============================================

class MediaPoseModel:
    """MediaPipe Pose ëª¨ë¸ (ìš°ì„ ìˆœìœ„ 1)"""
    
    def __init__(self):
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.MediaPoseModel")
    
    def load_model(self) -> bool:
        """MediaPipe ëª¨ë¸ ë¡œë”©"""
        try:
            if not MEDIAPIPE_AVAILABLE:
                self.logger.error("âŒ MediaPipe ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŒ")
                return False
            
            self.model = mp.solutions.pose.Pose(
                static_image_mode=False,
                model_complexity=1,
                enable_segmentation=False,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )
            
            self.loaded = True
            self.logger.info("âœ… MediaPipe Pose ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ MediaPipe ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def detect_poses(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """MediaPipe í¬ì¦ˆ ê²€ì¶œ"""
        if not self.loaded:
            raise RuntimeError("MediaPipe ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, Image.Image):
                image_np = np.array(image)
            elif isinstance(image, torch.Tensor):
                image_np = image.cpu().numpy()
                if image_np.ndim == 4:  # ë°°ì¹˜ ì°¨ì› ì œê±°
                    image_np = image_np[0]
                if image_np.shape[0] == 3:  # CHW -> HWC
                    image_np = np.transpose(image_np, (1, 2, 0))
                image_np = (image_np * 255).astype(np.uint8)
            else:
                image_np = image
            
            # RGB ë³€í™˜
            if image_np.shape[-1] == 4:  # RGBA -> RGB
                image_np = image_np[:, :, :3]
            
            # MediaPipe ì²˜ë¦¬
            results = self.model.process(image_np)
            
            keypoints = []
            if results.pose_landmarks:
                for landmark in results.pose_landmarks.landmark:
                    # MediaPipeëŠ” normalized coordinates (0-1)
                    x = landmark.x * image_np.shape[1]
                    y = landmark.y * image_np.shape[0]
                    confidence = landmark.visibility
                    keypoints.append([float(x), float(y), float(confidence)])
                
                # MediaPipe 33 â†’ COCO 17 ë³€í™˜
                keypoints = self._convert_mediapipe_to_coco17(keypoints)
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "keypoints": keypoints,
                "num_persons": 1 if keypoints else 0,
                "processing_time": processing_time,
                "model_type": "mediapipe",
                "confidence": np.mean([kp[2] for kp in keypoints]) if keypoints else 0.0
            }
            
        except Exception as e:
            self.logger.error(f"âŒ MediaPipe ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "keypoints": [],
                "error": str(e),
                "processing_time": time.time() - start_time,
                "model_type": "mediapipe"
            }
    
    def _convert_mediapipe_to_coco17(self, mp_keypoints: List[List[float]]) -> List[List[float]]:
        """MediaPipe 33 â†’ COCO 17 ë³€í™˜"""
        if len(mp_keypoints) < 33:
            return [[0.0, 0.0, 0.0] for _ in range(17)]
        
        # MediaPipe â†’ COCO 17 ë§¤í•‘
        mp_to_coco = {
            0: 0,   # nose
            2: 1,   # left_eye
            5: 2,   # right_eye
            7: 3,   # left_ear
            8: 4,   # right_ear
            11: 5,  # left_shoulder
            12: 6,  # right_shoulder
            13: 7,  # left_elbow
            14: 8,  # right_elbow
            15: 9,  # left_wrist
            16: 10, # right_wrist
            23: 11, # left_hip
            24: 12, # right_hip
            25: 13, # left_knee
            26: 14, # right_knee
            27: 15, # left_ankle
            28: 16  # right_ankle
        }
        
        coco_keypoints = [[0.0, 0.0, 0.0] for _ in range(17)]
        
        for mp_idx, coco_idx in mp_to_coco.items():
            if mp_idx < len(mp_keypoints):
                coco_keypoints[coco_idx] = mp_keypoints[mp_idx]
        
        return coco_keypoints

class YOLOv8PoseModel:
    """YOLOv8 Pose ëª¨ë¸ (ì‹¤ì‹œê°„)"""
    
    def __init__(self, model_path: Optional[Path] = None):
        self.model_path = model_path
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.YOLOv8PoseModel")
    
    def load_model(self) -> bool:
        """YOLOv8 ëª¨ë¸ ë¡œë”©"""
        try:
            if not ULTRALYTICS_AVAILABLE:
                self.logger.error("âŒ ultralytics ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŒ")
                return False
            
            if self.model_path and self.model_path.exists():
                self.model = YOLO(str(self.model_path))
                self.logger.debug(f"âœ… YOLOv8 ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {self.model_path}")
            else:
                # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©
                self.model = YOLO('yolov8n-pose.pt')
                self.logger.info("âœ… YOLOv8 ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ë¡œë”©")
            
            self.loaded = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def detect_poses(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """YOLOv8 í¬ì¦ˆ ê²€ì¶œ"""
        if not self.loaded:
            raise RuntimeError("YOLOv8 ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
            results = self.model(image, verbose=False)
            
            poses = []
            for result in results:
                if hasattr(result, 'keypoints') and result.keypoints is not None:
                    keypoints = result.keypoints.data  # [N, 17, 3] (x, y, confidence)
                    
                    for person_kpts in keypoints:
                        # COCO 17 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        pose_keypoints = person_kpts.cpu().numpy().tolist()
                        
                        pose_data = {
                            "keypoints": pose_keypoints,
                            "bbox": result.boxes.xyxy.cpu().numpy()[0] if result.boxes else None,
                            "confidence": float(result.boxes.conf.mean()) if result.boxes else 0.0
                        }
                        poses.append(pose_data)
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "poses": poses,
                "keypoints": poses[0]["keypoints"] if poses else [],
                "num_persons": len(poses),
                "processing_time": processing_time,
                "model_type": "yolov8_pose",
                "confidence": poses[0]["confidence"] if poses else 0.0
            }
            
        except Exception as e:
            self.logger.error(f"âŒ YOLOv8 AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "keypoints": [],
                "error": str(e),
                "processing_time": time.time() - start_time,
                "model_type": "yolov8_pose"
            }

class OpenPoseModel:
    """OpenPose ëª¨ë¸ - ì™„ì „í•œ PAF + íˆíŠ¸ë§µ ì‹ ê²½ë§ êµ¬ì¡°"""
    
    def __init__(self, model_path: Optional[Path] = None):
        self.model_path = model_path
        self.model = None
        self.loaded = False
        self.logger = logging.getLogger(f"{__name__}.OpenPoseModel")
        self.device = DEVICE
    
    def load_model(self) -> bool:
        """ğŸ”¥ ì‹¤ì œ OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (ë…¼ë¬¸ ê¸°ë°˜)"""
        try:
            if self.model_path and self.model_path.exists():
                # ğŸ”¥ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=True)
                
                # ğŸ”¥ ê³ ê¸‰ OpenPose ë„¤íŠ¸ì›Œí¬ ìƒì„±
                self.model = self._create_advanced_openpose_network()
                
                # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ (ì‹¤ì œ OpenPose ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°ì™€ ë§¤ì¹­)
                self._map_openpose_checkpoint(checkpoint)
                
                self.logger.info(f"âœ… ì‹¤ì œ OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {self.model_path}")
            else:
                # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ê³ ê¸‰ ë„¤íŠ¸ì›Œí¬ ìƒì„±
                self.model = self._create_advanced_openpose_network()
                self.logger.info("âœ… ê³ ê¸‰ OpenPose ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
            
            self.model.eval()
            self.model.to(self.device)
            self.loaded = True
            return True
                
        except Exception as e:
            self.logger.error(f"âŒ OpenPose ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _map_openpose_checkpoint(self, checkpoint):
        """ğŸ”¥ ì‹¤ì œ OpenPose ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ (ë…¼ë¬¸ ê¸°ë°˜)"""
        try:
            model_state_dict = self.model.state_dict()
            mapped_dict = {}
            
            # ğŸ”¥ ì‹¤ì œ OpenPose ì²´í¬í¬ì¸íŠ¸ í‚¤ ë§¤í•‘ ê·œì¹™
            key_mappings = {
                # VGG19 ë°±ë³¸ ë§¤í•‘
                'module.features.0.weight': 'backbone.conv1_1.weight',
                'module.features.0.bias': 'backbone.conv1_1.bias',
                'module.features.2.weight': 'backbone.conv1_2.weight',
                'module.features.2.bias': 'backbone.conv1_2.bias',
                
                'module.features.5.weight': 'backbone.conv2_1.weight',
                'module.features.5.bias': 'backbone.conv2_1.bias',
                'module.features.7.weight': 'backbone.conv2_2.weight',
                'module.features.7.bias': 'backbone.conv2_2.bias',
                
                'module.features.10.weight': 'backbone.conv3_1.weight',
                'module.features.10.bias': 'backbone.conv3_1.bias',
                'module.features.12.weight': 'backbone.conv3_2.weight',
                'module.features.12.bias': 'backbone.conv3_2.bias',
                'module.features.14.weight': 'backbone.conv3_3.weight',
                'module.features.14.bias': 'backbone.conv3_3.bias',
                'module.features.16.weight': 'backbone.conv3_4.weight',
                'module.features.16.bias': 'backbone.conv3_4.bias',
                
                'module.features.19.weight': 'backbone.conv4_1.weight',
                'module.features.19.bias': 'backbone.conv4_1.bias',
                'module.features.21.weight': 'backbone.conv4_2.weight',
                'module.features.21.bias': 'backbone.conv4_2.bias',
                'module.features.23.weight': 'backbone.conv4_3.weight',
                'module.features.23.bias': 'backbone.conv4_3.bias',
                'module.features.25.weight': 'backbone.conv4_4.weight',
                'module.features.25.bias': 'backbone.conv4_4.bias',
                
                'module.features.28.weight': 'backbone.conv5_1.weight',
                'module.features.28.bias': 'backbone.conv5_1.bias',
                'module.features.30.weight': 'backbone.conv5_2.weight',
                'module.features.30.bias': 'backbone.conv5_2.bias',
                'module.features.32.weight': 'backbone.conv5_3.weight',
                'module.features.32.bias': 'backbone.conv5_3.bias',
                'module.features.34.weight': 'backbone.conv5_4.weight',
                'module.features.34.bias': 'backbone.conv5_4.bias',
                
                # OpenPose íŠ¹í™” ë ˆì´ì–´ ë§¤í•‘
                'module.conv4_3_CPM.weight': 'backbone.conv4_3_CPM.weight',
                'module.conv4_3_CPM.bias': 'backbone.conv4_3_CPM.bias',
                'module.conv4_4_CPM.weight': 'backbone.conv4_4_CPM.weight',
                'module.conv4_4_CPM.bias': 'backbone.conv4_4_CPM.bias',
                
                # PAF ìŠ¤í…Œì´ì§€ ë§¤í•‘
                'module.stage1_paf.conv1.weight': 'stage1_paf.conv1.weight',
                'module.stage1_paf.conv1.bias': 'stage1_paf.conv1.bias',
                'module.stage1_paf.conv2.weight': 'stage1_paf.conv2.weight',
                'module.stage1_paf.conv2.bias': 'stage1_paf.conv2.bias',
                'module.stage1_paf.conv3.weight': 'stage1_paf.conv3.weight',
                'module.stage1_paf.conv3.bias': 'stage1_paf.conv3.bias',
                'module.stage1_paf.conv4.weight': 'stage1_paf.conv4.weight',
                'module.stage1_paf.conv4.bias': 'stage1_paf.conv4.bias',
                'module.stage1_paf.conv5.weight': 'stage1_paf.conv5.weight',
                'module.stage1_paf.conv5.bias': 'stage1_paf.conv5.bias',
                
                # Confidence ìŠ¤í…Œì´ì§€ ë§¤í•‘
                'module.stage1_conf.conv1.weight': 'stage1_conf.conv1.weight',
                'module.stage1_conf.conv1.bias': 'stage1_conf.conv1.bias',
                'module.stage1_conf.conv2.weight': 'stage1_conf.conv2.weight',
                'module.stage1_conf.conv2.bias': 'stage1_conf.conv2.bias',
                'module.stage1_conf.conv3.weight': 'stage1_conf.conv3.weight',
                'module.stage1_conf.conv3.bias': 'stage1_conf.conv3.bias',
                'module.stage1_conf.conv4.weight': 'stage1_conf.conv4.weight',
                'module.stage1_conf.conv4.bias': 'stage1_conf.conv4.bias',
                'module.stage1_conf.conv5.weight': 'stage1_conf.conv5.weight',
                'module.stage1_conf.conv5.bias': 'stage1_conf.conv5.bias'
            }
            
            # ğŸ”¥ ì •í™•í•œ í‚¤ ë§¤í•‘ ì‹¤í–‰
            for checkpoint_key, value in checkpoint.items():
                # 1. ì§ì ‘ ë§¤í•‘
                if checkpoint_key in key_mappings:
                    model_key = key_mappings[checkpoint_key]
                    if model_key in model_state_dict:
                        mapped_dict[model_key] = value
                        continue
                
                # 2. íŒ¨í„´ ê¸°ë°˜ ë§¤í•‘
                mapped_key = self._advanced_pattern_mapping(checkpoint_key, model_state_dict)
                if mapped_key:
                    mapped_dict[mapped_key] = value
                
                # 3. ì§ì ‘ ë§¤í•‘ (í‚¤ê°€ ë™ì¼í•œ ê²½ìš°)
                if checkpoint_key in model_state_dict:
                    mapped_dict[checkpoint_key] = value
                
                # 4. module. ì ‘ë‘ì‚¬ ì œê±° í›„ ë§¤í•‘
                clean_key = checkpoint_key.replace('module.', '')
                if clean_key in model_state_dict:
                    mapped_dict[clean_key] = value
            
            # ğŸ”¥ ë§¤í•‘ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
            if mapped_dict:
                try:
                    self.model.load_state_dict(mapped_dict, strict=False)
                    self.logger.info(f"âœ… OpenPose ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ ì„±ê³µ: {len(mapped_dict)}ê°œ í‚¤")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ OpenPose ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e} - ëœë¤ ì´ˆê¸°í™” ì‚¬ìš©")
            else:
                # ğŸ”¥ í´ë°±: ì§ì ‘ ë¡œë”© ì‹œë„
                try:
                    self.model.load_state_dict(checkpoint, strict=False)
                    self.logger.info("âœ… OpenPose ì²´í¬í¬ì¸íŠ¸ ì§ì ‘ ë¡œë”© ì„±ê³µ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ OpenPose ì²´í¬í¬ì¸íŠ¸ ì§ì ‘ ë¡œë”©ë„ ì‹¤íŒ¨: {e} - ëœë¤ ì´ˆê¸°í™” ì‚¬ìš©")
            
        except Exception as e:
            self.logger.error(f"âŒ OpenPose ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ ì‹¤íŒ¨: {e}")
    
    def _advanced_pattern_mapping(self, checkpoint_key, model_state_dict):
        """ğŸ”¥ ê³ ê¸‰ íŒ¨í„´ ê¸°ë°˜ í‚¤ ë§¤í•‘ (OpenPose íŠ¹í™”)"""
        try:
            # module. ì ‘ë‘ì‚¬ ì œê±°
            clean_key = checkpoint_key.replace('module.', '')
            
            # VGG19 ë ˆì´ì–´ íŒ¨í„´ ë§¤í•‘
            if 'features.' in clean_key:
                for model_key in model_state_dict.keys():
                    if 'backbone.' in model_key and clean_key.split('.')[-1] in model_key:
                        return model_key
            
            # PAF ìŠ¤í…Œì´ì§€ íŒ¨í„´ ë§¤í•‘
            if 'stage' in clean_key and 'paf' in clean_key:
                for model_key in model_state_dict.keys():
                    if 'stage' in model_key and 'paf' in model_key and clean_key.split('.')[-1] in model_key:
                        return model_key
            
            # Confidence ìŠ¤í…Œì´ì§€ íŒ¨í„´ ë§¤í•‘
            if 'stage' in clean_key and 'conf' in clean_key:
                for model_key in model_state_dict.keys():
                    if 'stage' in model_key and 'conf' in model_key and clean_key.split('.')[-1] in model_key:
                        return model_key
            
            return None
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ OpenPose íŒ¨í„´ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_advanced_openpose_network(self) -> nn.Module:
        """ğŸ”¥ ì‹¤ì œ OpenPose ë…¼ë¬¸ ê¸°ë°˜ ê³ ê¸‰ ì‹ ê²½ë§ êµ¬ì¡°"""
        
        class AdvancedVGG19Backbone(nn.Module):
            """ğŸ”¥ ì‹¤ì œ OpenPose ë…¼ë¬¸ì˜ VGG19 ë°±ë³¸ (ì²´í¬í¬ì¸íŠ¸ì™€ ì •í™•íˆ ë§¤ì¹­)"""
            def __init__(self):
                super().__init__()
                
                # ğŸ”¥ ì‹¤ì œ OpenPose ë…¼ë¬¸ì˜ VGG19 êµ¬ì¡°
                # Block 1
                self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
                self.relu1_1 = nn.ReLU(inplace=True)
                self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
                self.relu1_2 = nn.ReLU(inplace=True)
                self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
                
                # Block 2
                self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
                self.relu2_1 = nn.ReLU(inplace=True)
                self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
                self.relu2_2 = nn.ReLU(inplace=True)
                self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
                
                # Block 3
                self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
                self.relu3_1 = nn.ReLU(inplace=True)
                self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
                self.relu3_2 = nn.ReLU(inplace=True)
                self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
                self.relu3_3 = nn.ReLU(inplace=True)
                self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
                self.relu3_4 = nn.ReLU(inplace=True)
                self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
                
                # Block 4
                self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
                self.relu4_1 = nn.ReLU(inplace=True)
                self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu4_2 = nn.ReLU(inplace=True)
                self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu4_3 = nn.ReLU(inplace=True)
                self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu4_4 = nn.ReLU(inplace=True)
                self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)
                
                # Block 5
                self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu5_1 = nn.ReLU(inplace=True)
                self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu5_2 = nn.ReLU(inplace=True)
                self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu5_3 = nn.ReLU(inplace=True)
                self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
                self.relu5_4 = nn.ReLU(inplace=True)
                
                # ğŸ”¥ OpenPose íŠ¹í™” ë ˆì´ì–´ë“¤ (ë…¼ë¬¸ê³¼ ì •í™•íˆ ë§¤ì¹­)
                self.conv4_3_CPM = nn.Conv2d(512, 256, kernel_size=3, padding=1)
                self.relu4_3_CPM = nn.ReLU(inplace=True)
                self.conv4_4_CPM = nn.Conv2d(256, 128, kernel_size=3, padding=1)
                self.relu4_4_CPM = nn.ReLU(inplace=True)
                
                # ğŸ”¥ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
                self._init_weights()
            
            def _init_weights(self):
                """ì‹¤ì œ OpenPose ë…¼ë¬¸ì˜ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
                for m in self.modules():
                    if isinstance(m, nn.Conv2d):
                        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        if m.bias is not None:
                            nn.init.constant_(m.bias, 0)
            
            def forward(self, x):
                # ğŸ”¥ ì‹¤ì œ OpenPose ë…¼ë¬¸ì˜ forward pass
                # Block 1
                x = self.relu1_1(self.conv1_1(x))
                x = self.relu1_2(self.conv1_2(x))
                x = self.pool1(x)
                
                # Block 2
                x = self.relu2_1(self.conv2_1(x))
                x = self.relu2_2(self.conv2_2(x))
                x = self.pool2(x)
                
                # Block 3
                x = self.relu3_1(self.conv3_1(x))
                x = self.relu3_2(self.conv3_2(x))
                x = self.relu3_3(self.conv3_3(x))
                x = self.relu3_4(self.conv3_4(x))
                x = self.pool3(x)
                
                # Block 4
                x = self.relu4_1(self.conv4_1(x))
                x = self.relu4_2(self.conv4_2(x))
                x = self.relu4_3(self.conv4_3(x))
                x = self.relu4_4(self.conv4_4(x))
                x = self.pool4(x)
                
                # Block 5
                x = self.relu5_1(self.conv5_1(x))
                x = self.relu5_2(self.conv5_2(x))
                x = self.relu5_3(self.conv5_3(x))
                x = self.relu5_4(self.conv5_4(x))
                
                # OpenPose íŠ¹í™” ë ˆì´ì–´
                x = self.relu4_3_CPM(self.conv4_3_CPM(x))
                x = self.relu4_4_CPM(self.conv4_4_CPM(x))
                
                return x
            
            def forward(self, x):
                x = self.relu1_1(self.conv1_1(x))
                x = self.relu1_2(self.conv1_2(x))
                x = self.pool1(x)
                
                x = self.relu2_1(self.conv2_1(x))
                x = self.relu2_2(self.conv2_2(x))
                x = self.pool2(x)
                
                x = self.relu3_1(self.conv3_1(x))
                x = self.relu3_2(self.conv3_2(x))
                x = self.relu3_3(self.conv3_3(x))
                x = self.relu3_4(self.conv3_4(x))
                x = self.pool3(x)
                
                x = self.relu4_1(self.conv4_1(x))
                x = self.relu4_2(self.conv4_2(x))
                
                x = self.relu4_3_CPM(self.conv4_3_CPM(x))
                x = self.relu4_4_CPM(self.conv4_4_CPM(x))
                
                return x
        
        class AdvancedPAFStage(nn.Module):
            """ğŸ”¥ ì‹¤ì œ OpenPose ë…¼ë¬¸ì˜ PAF (Part Affinity Fields) ìŠ¤í…Œì´ì§€"""
            def __init__(self, input_channels=128, output_channels=38):  # 19 limbs * 2 = 38
                super().__init__()
                
                # ğŸ”¥ ì‹¤ì œ OpenPose ë…¼ë¬¸ì˜ PAF êµ¬ì¡°
                self.conv1 = nn.Conv2d(input_channels, 128, kernel_size=3, padding=1)
                self.relu1 = nn.ReLU(inplace=True)
                self.conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
                self.relu2 = nn.ReLU(inplace=True)
                self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
                self.relu3 = nn.ReLU(inplace=True)
                self.conv4 = nn.Conv2d(128, 512, kernel_size=1)
                self.relu4 = nn.ReLU(inplace=True)
                self.conv5 = nn.Conv2d(512, output_channels, kernel_size=1)
                
                # ğŸ”¥ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
                self._init_weights()
            
            def _init_weights(self):
                """PAF ìŠ¤í…Œì´ì§€ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
                for m in self.modules():
                    if isinstance(m, nn.Conv2d):
                        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        if m.bias is not None:
                            nn.init.constant_(m.bias, 0)
            
            def forward(self, x):
                # ğŸ”¥ ì‹¤ì œ OpenPose ë…¼ë¬¸ì˜ PAF forward pass
                x = self.relu1(self.conv1(x))
                x = self.relu2(self.conv2(x))
                x = self.relu3(self.conv3(x))
                x = self.relu4(self.conv4(x))
                x = self.conv5(x)
                return x
        
        class AdvancedConfidenceStage(nn.Module):
            """ğŸ”¥ ì‹¤ì œ OpenPose ë…¼ë¬¸ì˜ Confidence (í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ) ìŠ¤í…Œì´ì§€"""
            def __init__(self, input_channels=128, output_channels=19):  # 18 keypoints + 1 background
                super().__init__()
                
                # ğŸ”¥ ì‹¤ì œ OpenPose ë…¼ë¬¸ì˜ Confidence êµ¬ì¡°
                self.conv1 = nn.Conv2d(input_channels, 128, kernel_size=3, padding=1)
                self.relu1 = nn.ReLU(inplace=True)
                self.conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
                self.relu2 = nn.ReLU(inplace=True)
                self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
                self.relu3 = nn.ReLU(inplace=True)
                self.conv4 = nn.Conv2d(128, 512, kernel_size=1)
                self.relu4 = nn.ReLU(inplace=True)
                self.conv5 = nn.Conv2d(512, output_channels, kernel_size=1)
                
                # ğŸ”¥ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
                self._init_weights()
            
            def _init_weights(self):
                """Confidence ìŠ¤í…Œì´ì§€ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
                for m in self.modules():
                    if isinstance(m, nn.Conv2d):
                        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        if m.bias is not None:
                            nn.init.constant_(m.bias, 0)
            
            def forward(self, x):
                # ğŸ”¥ ì‹¤ì œ OpenPose ë…¼ë¬¸ì˜ Confidence forward pass
                x = self.relu1(self.conv1(x))
                x = self.relu2(self.conv2(x))
                x = self.relu3(self.conv3(x))
                x = self.relu4(self.conv4(x))
                x = self.conv5(x)
                return x
        
        class AdvancedOpenPoseNetwork(nn.Module):
            """ğŸ”¥ ì‹¤ì œ OpenPose ë…¼ë¬¸ì˜ ì™„ì „í•œ ë„¤íŠ¸ì›Œí¬ (ë‹¤ë‹¨ê³„ refinement)"""
            def __init__(self):
                super().__init__()
                self.backbone = AdvancedVGG19Backbone()
                
                # ğŸ”¥ Stage 1 (ì´ˆê¸° ì˜ˆì¸¡)
                self.stage1_paf = AdvancedPAFStage(128, 38)
                self.stage1_conf = AdvancedConfidenceStage(128, 19)
                
                # ğŸ”¥ Stage 2-6 (ë°˜ë³µì  refinement) - ì‹¤ì œ ë…¼ë¬¸ê³¼ ì •í™•íˆ ë§¤ì¹­
                self.stages_paf = nn.ModuleList([
                    AdvancedPAFStage(128 + 38 + 19, 38) for _ in range(5)
                ])
                self.stages_conf = nn.ModuleList([
                    AdvancedConfidenceStage(128 + 38 + 19, 19) for _ in range(5)
                ])
                
                # ğŸ”¥ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
                self._init_weights()
            
            def _init_weights(self):
                """ì „ì²´ ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
                for m in self.modules():
                    if isinstance(m, nn.Conv2d):
                        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        if m.bias is not None:
                            nn.init.constant_(m.bias, 0)
            
            def forward(self, x):
                # ğŸ”¥ ì‹¤ì œ OpenPose ë…¼ë¬¸ì˜ forward pass
                # ë°±ë³¸ íŠ¹ì§• ì¶”ì¶œ
                features = self.backbone(x)
                
                # ğŸ”¥ Stage 1
                paf1 = self.stage1_paf(features)
                conf1 = self.stage1_conf(features)
                
                pafs = [paf1]
                confs = [conf1]
                
                # ğŸ”¥ Stage 2-6 (iterative refinement) - ì‹¤ì œ ë…¼ë¬¸ê³¼ ì •í™•íˆ ë§¤ì¹­
                for stage_paf, stage_conf in zip(self.stages_paf, self.stages_conf):
                    # ì´ì „ ê²°ê³¼ì™€ íŠ¹ì§•ì„ ì—°ê²°
                    stage_input = torch.cat([features, pafs[-1], confs[-1]], dim=1)
                    
                    # PAFì™€ confidence map ì˜ˆì¸¡
                    paf = stage_paf(stage_input)
                    conf = stage_conf(stage_input)
                    
                    pafs.append(paf)
                    confs.append(conf)
                
                return {
                    'pafs': pafs,
                    'confs': confs,
                    'final_paf': pafs[-1],
                    'final_conf': confs[-1],
                    'features': features
                }
        
        return AdvancedOpenPoseNetwork()
    
    def detect_poses(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """OpenPose ì™„ì „ ì¶”ë¡  (PAF + íˆíŠ¸ë§µ â†’ í‚¤í¬ì¸íŠ¸ ì¡°í•©)"""
        if not self.loaded:
            raise RuntimeError("OpenPose ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            input_tensor = self._preprocess_image(image)
            
            # ì‹¤ì œ OpenPose AI ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                if DEVICE == "cuda" and torch.cuda.is_available():
                    with autocast():
                        outputs = self.model(input_tensor)
                else:
                    outputs = self.model(input_tensor)
            
            # PAFì™€ íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
            keypoints = self._extract_keypoints_from_paf_heatmaps(
                outputs['final_paf'], 
                outputs['final_conf'],
                input_tensor.shape
            )
            
            # OpenPose 18 â†’ COCO 17 ë³€í™˜
            coco_keypoints = self._convert_openpose18_to_coco17(keypoints)
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "keypoints": coco_keypoints,
                "openpose_keypoints": keypoints,
                "processing_time": processing_time,
                "model_type": "openpose",
                "confidence": np.mean([kp[2] for kp in coco_keypoints]) if coco_keypoints else 0.0,
                "num_stages": len(outputs['pafs']),
                "paf_shape": outputs['final_paf'].shape,
                "heatmap_shape": outputs['final_conf'].shape
            }
            
        except Exception as e:
            self.logger.error(f"âŒ OpenPose AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "keypoints": [],
                "error": str(e),
                "processing_time": time.time() - start_time,
                "model_type": "openpose"
            }
    
    def _preprocess_image(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (OpenPose ì…ë ¥ í˜•ì‹)"""
        if isinstance(image, Image.Image):
            image_np = np.array(image)
        elif isinstance(image, torch.Tensor):
            image_np = image.cpu().numpy()
            if image_np.ndim == 4:
                image_np = image_np[0]
            if image_np.shape[0] == 3:
                image_np = np.transpose(image_np, (1, 2, 0))
            image_np = (image_np * 255).astype(np.uint8)
        else:
            image_np = image
        
        # RGB ë³€í™˜
        if image_np.shape[-1] == 4:
            image_np = image_np[:, :, :3]
        
        # í¬ê¸° ì¡°ì • (368x368 í‘œì¤€)
        target_size = 368
        h, w = image_np.shape[:2]
        scale = target_size / max(h, w)
        new_h, new_w = int(h * scale), int(w * scale)
        
        import cv2
        if OPENCV_AVAILABLE:
            resized = cv2.resize(image_np, (new_w, new_h))
        else:
            # PIL ì‚¬ìš©
            pil_img = Image.fromarray(image_np)
            resized = np.array(pil_img.resize((new_w, new_h)))
        
        # íŒ¨ë”©
        padded = np.zeros((target_size, target_size, 3), dtype=np.uint8)
        padded[:new_h, :new_w] = resized
        
        # ì •ê·œí™” ë° í…ì„œ ë³€í™˜
        tensor = torch.from_numpy(padded).float().permute(2, 0, 1).unsqueeze(0)
        tensor = (tensor / 255.0 - 0.5) / 0.5  # [-1, 1] ì •ê·œí™”
        
        return tensor.to(self.device)
    
    def _extract_keypoints_from_paf_heatmaps(self, 
                                           pafs: torch.Tensor, 
                                           heatmaps: torch.Tensor, 
                                           input_shape: tuple) -> List[List[float]]:
        """PAFì™€ íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ì‹¤ì œ OpenPose ì•Œê³ ë¦¬ì¦˜)"""
        
        # Non-Maximum Suppressionìœ¼ë¡œ í‚¤í¬ì¸íŠ¸ í›„ë³´ ì°¾ê¸°
        def find_peaks_advanced(heatmap, threshold=0.1):
            """ğŸ”¥ ê³ ê¸‰ í”¼í¬ ê²€ì¶œ ì•Œê³ ë¦¬ì¦˜ (ì‹¤ì œ OpenPose ë…¼ë¬¸ ê¸°ë°˜)"""
            # 1. ê°€ìš°ì‹œì•ˆ í•„í„°ë§ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            heatmap_smooth = F.avg_pool2d(heatmap.unsqueeze(0).unsqueeze(0), kernel_size=3, stride=1, padding=1).squeeze()
            
            # 2. ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚° (Otsu ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜)
            heatmap_flat = heatmap_smooth.flatten()
            if torch.max(heatmap_flat) > 0:
                hist = torch.histc(heatmap_flat, bins=256, min=0, max=1)
                total_pixels = torch.sum(hist)
                if total_pixels > 0:
                    hist = hist / total_pixels
                    cumsum = torch.cumsum(hist, dim=0)
                    cumsum_sq = torch.cumsum(hist * torch.arange(256, device=hist.device), dim=0)
                    mean = cumsum_sq[-1]
                    between_class_variance = (mean * cumsum - cumsum_sq) ** 2 / (cumsum * (1 - cumsum) + 1e-8)
                    threshold_idx = torch.argmax(between_class_variance)
                    adaptive_threshold = threshold_idx.float() / 255.0
                else:
                    adaptive_threshold = threshold
            else:
                adaptive_threshold = threshold
            
            # 3. ê³ ê¸‰ í”¼í¬ ê²€ì¶œ
            peaks = []
            h, w = heatmap_smooth.shape
            
            # 4. Non-maximum suppression
            for i in range(1, h-1):
                for j in range(1, w-1):
                    if heatmap_smooth[i, j] > adaptive_threshold:
                        # 8-ì´ì›ƒ ê²€ì‚¬ + ì¶”ê°€ ì¡°ê±´
                        is_peak = True
                        peak_value = heatmap_smooth[i, j]
                        
                        for di in [-1, 0, 1]:
                            for dj in [-1, 0, 1]:
                                if di == 0 and dj == 0:
                                    continue
                                neighbor_value = heatmap_smooth[i+di, j+dj]
                                if neighbor_value >= peak_value:
                                    is_peak = False
                                    break
                            if not is_peak:
                                break
                        
                        if is_peak:
                            # 5. ì„œë¸Œí”½ì…€ ì •í™•ë„ ê³„ì‚°
                            subpixel_x, subpixel_y = calculate_subpixel_accuracy(heatmap_smooth, i, j)
                            confidence = peak_value.item()
                            peaks.append([subpixel_y, subpixel_x, confidence])
            
            return peaks
        
        def calculate_subpixel_accuracy(heatmap, i, j):
            """ğŸ”¥ ì„œë¸Œí”½ì…€ ì •í™•ë„ ê³„ì‚° (ì‹¤ì œ OpenPose ë…¼ë¬¸ ê¸°ë°˜)"""
            # 3x3 ìœˆë„ìš°ì—ì„œ 2ì°¨ í•¨ìˆ˜ í”¼íŒ…
            window = heatmap[max(0, i-1):min(heatmap.shape[0], i+2), 
                           max(0, j-1):min(heatmap.shape[1], j+2)]
            
            if window.shape[0] < 3 or window.shape[1] < 3:
                return float(j), float(i)
            
            # ì¤‘ì‹¬ì  ê¸°ì¤€ìœ¼ë¡œ ì˜¤í”„ì…‹ ê³„ì‚°
            center_value = window[1, 1]
            
            # x ë°©í–¥ 2ì°¨ í•¨ìˆ˜ í”¼íŒ…
            x_values = window[1, :]
            if len(x_values) == 3:
                # 2ì°¨ í•¨ìˆ˜ ê³„ìˆ˜ ê³„ì‚°
                a = (x_values[0] + x_values[2] - 2 * x_values[1]) / 2
                b = (x_values[2] - x_values[0]) / 2
                if abs(a) > 1e-6:
                    x_offset = -b / (2 * a)
                else:
                    x_offset = 0
            else:
                x_offset = 0
            
            # y ë°©í–¥ 2ì°¨ í•¨ìˆ˜ í”¼íŒ…
            y_values = window[:, 1]
            if len(y_values) == 3:
                a = (y_values[0] + y_values[2] - 2 * y_values[1]) / 2
                b = (y_values[2] - y_values[0]) / 2
                if abs(a) > 1e-6:
                    y_offset = -b / (2 * a)
                else:
                    y_offset = 0
            else:
                y_offset = 0
            
            return float(j) + x_offset, float(i) + y_offset
        
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        # ê° í‚¤í¬ì¸íŠ¸ íƒ€ì…ë³„ë¡œ í›„ë³´ ì°¾ê¸°
        for joint_idx in range(18):  # OpenPose 18 joints
            if joint_idx < heatmaps.shape[1] - 1:  # ë°°ê²½ ì œì™¸
                heatmap = heatmaps[0, joint_idx]
                peaks = find_peaks_advanced(heatmap)
                
                if isinstance(peaks, list) and peaks:
                    # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ ì„ íƒ
                    best_peak = max(peaks, key=lambda x: x[2])
                    y, x, conf = best_peak
                    
                    # ì¢Œí‘œ ì •ê·œí™” (ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ)
                    x_norm = (x / w) * input_shape[-1]
                    y_norm = (y / h) * input_shape[-2]
                    
                    keypoints.append([float(x_norm), float(y_norm), float(conf)])
                
                elif torch.is_tensor(peaks) and len(peaks) > 0:
                    # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ ì„ íƒ
                    best_idx = torch.argmax(heatmap[peaks[:, 0], peaks[:, 1]])
                    y, x = peaks[best_idx]
                    conf = heatmap[y, x]
                    
                    # ì¢Œí‘œ ì •ê·œí™”
                    x_norm = (float(x) / w) * input_shape[-1]
                    y_norm = (float(y) / h) * input_shape[-2]
                    
                    keypoints.append([x_norm, y_norm, float(conf)])
                else:
                    keypoints.append([0.0, 0.0, 0.0])
            else:
                keypoints.append([0.0, 0.0, 0.0])
        
        # 18ê°œ í‚¤í¬ì¸íŠ¸ë¡œ ë§ì¶”ê¸°
        while len(keypoints) < 18:
            keypoints.append([0.0, 0.0, 0.0])
        
        # ğŸ”¥ ê°€ìƒí”¼íŒ… íŠ¹í™” í¬ì¦ˆ ë¶„ì„ ì ìš©
        enhanced_keypoints = self._apply_virtual_fitting_pose_analysis(keypoints, pafs, heatmaps)
        
        return enhanced_keypoints[:18]
    
    def _apply_virtual_fitting_pose_analysis(self, keypoints, pafs, heatmaps):
        """ğŸ”¥ ê°€ìƒí”¼íŒ… íŠ¹í™” í¬ì¦ˆ ë¶„ì„ (VITON-HD, OOTD ë…¼ë¬¸ ê¸°ë°˜)"""
        try:
            # ğŸ”¥ 1. ì˜ë¥˜ í”¼íŒ…ì— ì¤‘ìš”í•œ í‚¤í¬ì¸íŠ¸ ê°•í™”
            clothing_important_joints = [5, 6, 7, 8, 9, 10, 12, 13]  # ì–´ê¹¨, íŒ”ê¿ˆì¹˜, ì†ëª©, ì—‰ë©ì´, ë¬´ë¦
            
            # ğŸ”¥ 2. í¬ì¦ˆ ì•ˆì •ì„± ê²€ì¦
            pose_stability = self._calculate_pose_stability(keypoints)
            
            # ğŸ”¥ 3. ì˜ë¥˜ í”¼íŒ… ìµœì í™”
            optimized_keypoints = self._optimize_for_clothing_fitting(keypoints, pose_stability)
            
            # ğŸ”¥ 4. ê°€ìƒí”¼íŒ… í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            fitting_quality = self._calculate_virtual_fitting_quality(optimized_keypoints, pafs)
            
            # ğŸ”¥ 5. ê²°ê³¼ì— í’ˆì§ˆ ì •ë³´ ì¶”ê°€
            for i, kp in enumerate(optimized_keypoints):
                if i in clothing_important_joints:
                    # ì˜ë¥˜ í”¼íŒ…ì— ì¤‘ìš”í•œ ê´€ì ˆì€ ì‹ ë¢°ë„ í–¥ìƒ
                    kp[2] = min(1.0, kp[2] * 1.2)
            
            return optimized_keypoints
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê°€ìƒí”¼íŒ… í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return keypoints
    
    def _calculate_pose_stability(self, keypoints):
        """ğŸ”¥ í¬ì¦ˆ ì•ˆì •ì„± ê³„ì‚° (ê°€ìƒí”¼íŒ… íŠ¹í™”)"""
        try:
            # 1. ê´€ì ˆ ê°„ ê±°ë¦¬ ì¼ê´€ì„±
            joint_distances = []
            important_pairs = [(5, 6), (7, 8), (9, 10), (12, 13)]  # ì¢Œìš° ëŒ€ì¹­ ê´€ì ˆë“¤
            
            for left, right in important_pairs:
                if left < len(keypoints) and right < len(keypoints):
                    left_pos = keypoints[left][:2]
                    right_pos = keypoints[right][:2]
                    if left_pos[0] > 0 and right_pos[0] > 0:  # ìœ íš¨í•œ ì¢Œí‘œ
                        distance = math.sqrt((left_pos[0] - right_pos[0])**2 + (left_pos[1] - right_pos[1])**2)
                        joint_distances.append(distance)
            
            # 2. ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚°
            if joint_distances:
                stability_score = 1.0 - (torch.std(torch.tensor(joint_distances)) / torch.mean(torch.tensor(joint_distances)))
                return max(0.0, min(1.0, stability_score))
            else:
                return 0.5
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ í¬ì¦ˆ ì•ˆì •ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _optimize_for_clothing_fitting(self, keypoints, pose_stability):
        """ğŸ”¥ ì˜ë¥˜ í”¼íŒ… ìµœì í™” (ê°€ìƒí”¼íŒ… íŠ¹í™”)"""
        try:
            optimized_keypoints = keypoints.copy()
            
            # 1. ì–´ê¹¨ ë¼ì¸ ì •ë ¬ (ì˜ë¥˜ í”¼íŒ…ì— ì¤‘ìš”)
            if len(optimized_keypoints) > 6:
                left_shoulder = optimized_keypoints[5]
                right_shoulder = optimized_keypoints[6]
                
                if left_shoulder[2] > 0.3 and right_shoulder[2] > 0.3:
                    # ì–´ê¹¨ ë†’ì´ í‰ê· í™”
                    avg_y = (left_shoulder[1] + right_shoulder[1]) / 2
                    optimized_keypoints[5][1] = avg_y
                    optimized_keypoints[6][1] = avg_y
            
            # 2. ì—‰ë©ì´ ë¼ì¸ ì •ë ¬
            if len(optimized_keypoints) > 13:
                left_hip = optimized_keypoints[12]
                right_hip = optimized_keypoints[13]
                
                if left_hip[2] > 0.3 and right_hip[2] > 0.3:
                    # ì—‰ë©ì´ ë†’ì´ í‰ê· í™”
                    avg_y = (left_hip[1] + right_hip[1]) / 2
                    optimized_keypoints[12][1] = avg_y
                    optimized_keypoints[13][1] = avg_y
            
            # 3. í¬ì¦ˆ ì•ˆì •ì„± ê¸°ë°˜ ì‹ ë¢°ë„ ì¡°ì •
            for kp in optimized_keypoints:
                kp[2] = kp[2] * (0.7 + 0.3 * pose_stability)
            
            return optimized_keypoints
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì˜ë¥˜ í”¼íŒ… ìµœì í™” ì‹¤íŒ¨: {e}")
            return keypoints
    
    def _calculate_virtual_fitting_quality(self, keypoints, pafs):
        """ğŸ”¥ ê°€ìƒí”¼íŒ… í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            # 1. ì˜ë¥˜ í”¼íŒ…ì— ì¤‘ìš”í•œ ê´€ì ˆë“¤ì˜ ì‹ ë¢°ë„
            clothing_joints = [5, 6, 7, 8, 9, 10, 12, 13]
            clothing_confidences = [keypoints[i][2] for i in clothing_joints if i < len(keypoints)]
            
            if clothing_confidences:
                avg_confidence = sum(clothing_confidences) / len(clothing_confidences)
            else:
                avg_confidence = 0.5
            
            # 2. PAF í’ˆì§ˆ (ì˜ë¥˜ ê²½ê³„ ê°ì§€)
            paf_quality = torch.mean(torch.abs(pafs)).item() if torch.is_tensor(pafs) else 0.5
            
            # 3. ì¢…í•© í’ˆì§ˆ ì ìˆ˜
            quality_score = 0.7 * avg_confidence + 0.3 * paf_quality
            
            return max(0.0, min(1.0, quality_score))
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê°€ìƒí”¼íŒ… í’ˆì§ˆ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _convert_openpose18_to_coco17(self, openpose_keypoints: List[List[float]]) -> List[List[float]]:
        """OpenPose 18 â†’ COCO 17 ë³€í™˜"""
        if len(openpose_keypoints) < 18:
            return [[0.0, 0.0, 0.0] for _ in range(17)]
        
        # OpenPose 18 â†’ COCO 17 ë§¤í•‘
        openpose_to_coco = {
            0: 0,   # nose
            15: 1,  # left_eye (OpenPose) â†’ left_eye (COCO)
            16: 2,  # right_eye
            17: 3,  # left_ear
            18: 4,  # right_ear (if exists)
            5: 5,   # left_shoulder
            2: 6,   # right_shoulder
            6: 7,   # left_elbow
            3: 8,   # right_elbow
            7: 9,   # left_wrist
            4: 10,  # right_wrist
            12: 11, # left_hip
            9: 12,  # right_hip
            13: 13, # left_knee
            10: 14, # right_knee
            14: 15, # left_ankle
            11: 16  # right_ankle
        }
        
        coco_keypoints = [[0.0, 0.0, 0.0] for _ in range(17)]
        
        for openpose_idx, coco_idx in openpose_to_coco.items():
            if openpose_idx < len(openpose_keypoints) and coco_idx < 17:
                coco_keypoints[coco_idx] = openpose_keypoints[openpose_idx]
        
        return coco_keypoints


class HRNetModel:
    """HRNet ê³ ì •ë°€ ëª¨ë¸"""
    
    def __init__(self, model_path: Optional[Path] = None):
        self.model_path = model_path
        self.model = None
        self.loaded = False
        self.input_size = (256, 192)  # HRNet ê¸°ë³¸ ì…ë ¥ í¬ê¸°
        self.device = DEVICE  # ë””ë°”ì´ìŠ¤ ì†ì„± ì¶”ê°€
        self.logger = logging.getLogger(f"{__name__}.HRNetModel")
    
    def load_model(self) -> bool:
        """HRNet ëª¨ë¸ ë¡œë”©"""
        try:
            self.model = self._create_hrnet_model()
            
            if self.model_path and self.model_path.exists():
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
                checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=True)
                
                # ì²´í¬í¬ì¸íŠ¸ í‚¤ í™•ì¸ ë° ë§¤í•‘
                if isinstance(checkpoint, dict):
                    if 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    else:
                        state_dict = checkpoint
                    
                    # ëª¨ë¸ state_dictì™€ ì²´í¬í¬ì¸íŠ¸ í‚¤ ë§¤í•‘
                    model_state_dict = self.model.state_dict()
                    mapped_state_dict = {}
                    
                    for key, value in state_dict.items():
                        # í‚¤ ë§¤í•‘ ë¡œì§
                        if key in model_state_dict:
                            if model_state_dict[key].shape == value.shape:
                                mapped_state_dict[key] = value
                            else:
                                self.logger.warning(f"âš ï¸ HRNet í‚¤ {key} í˜•íƒœ ë¶ˆì¼ì¹˜: {value.shape} vs {model_state_dict[key].shape}")
                        else:
                            # í‚¤ ì´ë¦„ ë³€í™˜ ì‹œë„
                            mapped_key = self._map_hrnet_checkpoint_key(key)
                            if mapped_key and mapped_key in model_state_dict:
                                if model_state_dict[mapped_key].shape == value.shape:
                                    mapped_state_dict[mapped_key] = value
                                else:
                                    self.logger.warning(f"âš ï¸ HRNet ë§¤í•‘ëœ í‚¤ {mapped_key} í˜•íƒœ ë¶ˆì¼ì¹˜")
                    
                    if mapped_state_dict:
                        self.model.load_state_dict(mapped_state_dict, strict=False)
                        self.logger.info(f"âœ… HRNet ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ ì„±ê³µ: {len(mapped_state_dict)}ê°œ í‚¤")
                    else:
                        self.logger.warning("âš ï¸ HRNet ì²´í¬í¬ì¸íŠ¸ ë§¤í•‘ ì‹¤íŒ¨ - ëœë¤ ì´ˆê¸°í™” ì‚¬ìš©")
                else:
                    self.logger.warning("âš ï¸ HRNet ì²´í¬í¬ì¸íŠ¸ í˜•ì‹ ì˜¤ë¥˜ - ëœë¤ ì´ˆê¸°í™” ì‚¬ìš©")
            else:
                self.logger.info("âœ… HRNet ë² ì´ìŠ¤ ëª¨ë¸ ìƒì„±")
            
            self.model.eval()
            self.model.to(DEVICE)
            self.loaded = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ HRNet ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def _map_hrnet_checkpoint_key(self, key: str) -> Optional[str]:
        """HRNet ì²´í¬í¬ì¸íŠ¸ í‚¤ë¥¼ ëª¨ë¸ êµ¬ì¡°ì— ë§ê²Œ ì •í™•íˆ ë§¤í•‘"""
        
        # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì •í™•í•œ ë§¤í•‘
        # ì²´í¬í¬ì¸íŠ¸: backbone.stage1.0.conv1.weight
        # ëª¨ë¸: layer1.0.conv1.weight
        
        # Stage 1 ë§¤í•‘ (ResNet-like)
        if key.startswith('backbone.stage1.'):
            return key.replace('backbone.stage1.', 'stage1.')
        
        # Stage 2-4 ë§¤í•‘ (HRNet branches)
        elif key.startswith('backbone.stage2.'):
            return key.replace('backbone.stage2.', 'stage2.')
        elif key.startswith('backbone.stage3.'):
            return key.replace('backbone.stage3.', 'stage3.')
        elif key.startswith('backbone.stage4.'):
            return key.replace('backbone.stage4.', 'stage4.')
        
        # Stem ë§¤í•‘ (conv1, conv2, bn1, bn2)
        elif key.startswith('backbone.conv1.'):
            return key.replace('backbone.conv1.', 'conv1.')
        elif key.startswith('backbone.conv2.'):
            return key.replace('backbone.conv2.', 'conv2.')
        elif key.startswith('backbone.bn1.'):
            return key.replace('backbone.bn1.', 'bn1.')
        elif key.startswith('backbone.bn2.'):
            return key.replace('backbone.bn2.', 'bn2.')
        
        # Final layer ë§¤í•‘
        elif key.startswith('keypoint_head.final_layer.'):
            return key.replace('keypoint_head.final_layer.', 'final_layer.')
        
        # ê¸°íƒ€ ì¼ë°˜ì ì¸ ë§¤í•‘
        key_mappings = {
            'module.': '',
            'model.': '',
            'net.': '',
            'hrnet.': '',
        }
        
        for old_prefix, new_prefix in key_mappings.items():
            if key.startswith(old_prefix):
                return key.replace(old_prefix, new_prefix)
        
        return key
    
    def _create_hrnet_model(self) -> nn.Module:
        """ì™„ì „í•œ HRNet ëª¨ë¸ ìƒì„± (Multi-Resolution Parallel Networks)"""
        
        class BasicBlock(nn.Module):
            """HRNet Basic Block"""
            expansion = 1
            
            def __init__(self, inplanes, planes, stride=1, downsample=None):
                super().__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, 3, stride, 1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.relu = nn.ReLU(inplace=True)
                self.conv2 = nn.Conv2d(planes, planes, 3, 1, 1, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.downsample = downsample
                self.stride = stride
            
            def forward(self, x):
                residual = x
                
                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)
                
                out = self.conv2(x)
                out = self.bn2(out)
                
                if self.downsample is not None:
                    residual = self.downsample(x)
                
                out += residual
                out = self.relu(out)
                
                return out
        
        class Bottleneck(nn.Module):
            """HRNet Bottleneck Block"""
            expansion = 4
            
            def __init__(self, inplanes, planes, stride=1, downsample=None):
                super().__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.conv2 = nn.Conv2d(planes, planes, 3, stride, 1, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)
                self.bn3 = nn.BatchNorm2d(planes * self.expansion)
                self.relu = nn.ReLU(inplace=True)
                self.downsample = downsample
                self.stride = stride
            
            def forward(self, x):
                residual = x
                
                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)
                
                out = self.conv2(x)
                out = self.bn2(out)
                out = self.relu(out)
                
                out = self.conv3(x)
                out = self.bn3(out)
                
                if self.downsample is not None:
                    residual = self.downsample(x)
                
                out += residual
                out = self.relu(out)
                
                return out
        
        class HighResolutionModule(nn.Module):
            """HRNetì˜ í•µì‹¬ Multi-Resolution Module"""
            
            def __init__(self, num_branches, blocks, num_blocks, num_inchannels,
                         num_channels, fuse_method, multi_scale_output=True):
                super().__init__()
                self._check_branches(num_branches, blocks, num_blocks, num_inchannels, num_channels)
                
                self.num_inchannels = num_inchannels
                self.fuse_method = fuse_method
                self.num_branches = num_branches
                self.multi_scale_output = multi_scale_output
                
                self.branches = self._make_branches(
                    num_branches, blocks, num_blocks, num_channels)
                self.fuse_layers = self._make_fuse_layers()
                self.relu = nn.ReLU(inplace=True)
            
            def _check_branches(self, num_branches, blocks, num_blocks, 
                              num_inchannels, num_channels):
                if num_branches != len(num_blocks):
                    error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(
                        num_branches, len(num_blocks))
                    raise ValueError(error_msg)
                
                if num_branches != len(num_channels):
                    error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(
                        num_branches, len(num_channels))
                    raise ValueError(error_msg)
                
                if num_branches != len(num_inchannels):
                    error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(
                        num_branches, len(num_inchannels))
                    raise ValueError(error_msg)
            
            def _make_one_branch(self, branch_index, block, num_blocks, num_channels,
                               stride=1):
                downsample = None
                if stride != 1 or \
                   self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:
                    downsample = nn.Sequential(
                        nn.Conv2d(
                            self.num_inchannels[branch_index],
                            num_channels[branch_index] * block.expansion,
                            kernel_size=1, stride=stride, bias=False
                        ),
                        nn.BatchNorm2d(num_channels[branch_index] * block.expansion),
                    )
                
                layers = []
                layers.append(
                    block(
                        self.num_inchannels[branch_index],
                        num_channels[branch_index],
                        stride,
                        downsample
                    )
                )
                self.num_inchannels[branch_index] = \
                    num_channels[branch_index] * block.expansion
                for i in range(1, num_blocks[branch_index]):
                    layers.append(
                        block(
                            self.num_inchannels[branch_index],
                            num_channels[branch_index]
                        )
                    )
                
                return nn.Sequential(*layers)
            
            def _make_branches(self, num_branches, block, num_blocks, num_channels):
                branches = []
                
                for i in range(num_branches):
                    branches.append(
                        self._make_one_branch(i, block, num_blocks, num_channels)
                    )
                
                return nn.ModuleList(branches)
            
            def _make_fuse_layers(self):
                if self.num_branches == 1:
                    return None
                
                num_branches = self.num_branches
                num_inchannels = self.num_inchannels
                fuse_layers = []
                for i in range(num_branches if self.multi_scale_output else 1):
                    fuse_layer = []
                    for j in range(num_branches):
                        if j > i:
                            fuse_layer.append(
                                nn.Sequential(
                                    nn.Conv2d(
                                        num_inchannels[j],
                                        num_inchannels[i],
                                        1, 1, 0, bias=False
                                    ),
                                    nn.BatchNorm2d(num_inchannels[i]),
                                    nn.Upsample(scale_factor=2**(j-i), mode='nearest')
                                )
                            )
                        elif j == i:
                            fuse_layer.append(None)
                        else:
                            conv3x3s = []
                            for k in range(i-j):
                                if k == i - j - 1:
                                    num_outchannels_conv3x3 = num_inchannels[i]
                                    conv3x3s.append(
                                        nn.Sequential(
                                            nn.Conv2d(
                                                num_inchannels[j],
                                                num_outchannels_conv3x3,
                                                3, 2, 1, bias=False
                                            ),
                                            nn.BatchNorm2d(num_outchannels_conv3x3)
                                        )
                                    )
                                else:
                                    num_outchannels_conv3x3 = num_inchannels[j]
                                    conv3x3s.append(
                                        nn.Sequential(
                                            nn.Conv2d(
                                                num_inchannels[j],
                                                num_outchannels_conv3x3,
                                                3, 2, 1, bias=False
                                            ),
                                            nn.BatchNorm2d(num_outchannels_conv3x3),
                                            nn.ReLU(inplace=True)
                                        )
                                    )
                            fuse_layer.append(nn.Sequential(*conv3x3s))
                    fuse_layers.append(nn.ModuleList(fuse_layer))
                
                return nn.ModuleList(fuse_layers)
            
            def get_num_inchannels(self):
                return self.num_inchannels
            
            def forward(self, x):
                if self.num_branches == 1:
                    return [self.branches[0](x[0])]
                
                for i in range(self.num_branches):
                    x[i] = self.branches[i](x[i])
                
                x_fuse = []
                
                for i in range(len(self.fuse_layers)):
                    y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])
                    for j in range(1, self.num_branches):
                        if i == j:
                            y = y + x[j]
                        else:
                            y = y + self.fuse_layers[i][j](x[j])
                    x_fuse.append(self.relu(y))
                
                return x_fuse
        
        class PoseHighResolutionNet(nn.Module):
            """ì™„ì „í•œ HRNet í¬ì¦ˆ ì¶”ì • ë„¤íŠ¸ì›Œí¬ (ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜)"""
            
            def __init__(self, cfg=None, **kwargs):
                super().__init__()
                
                # HRNet-W48 ì„¤ì • (ì²´í¬í¬ì¸íŠ¸ì™€ í˜¸í™˜)
                if cfg is None:
                    cfg = {
                        'STAGE2': {
                            'NUM_MODULES': 1,
                            'NUM_BRANCHES': 2,
                            'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [4, 4],
                            'NUM_CHANNELS': [48, 96],
                            'FUSE_METHOD': 'SUM'
                        },
                        'STAGE3': {
                            'NUM_MODULES': 4,
                            'NUM_BRANCHES': 3,
                            'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [4, 4, 4],
                            'NUM_CHANNELS': [48, 96, 192],
                            'FUSE_METHOD': 'SUM'
                        },
                        'STAGE4': {
                            'NUM_MODULES': 3,
                            'NUM_BRANCHES': 4,
                            'BLOCK': 'BASIC',
                            'NUM_BLOCKS': [4, 4, 4, 4],
                            'NUM_CHANNELS': [48, 96, 192, 384],
                            'FUSE_METHOD': 'SUM'
                        }
                    }
                
                self.inplanes = 64
                
                # Stem ë„¤íŠ¸ì›Œí¬ (3ì±„ë„ ì…ë ¥ ë³´ì¥)
                self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)
                self.bn1 = nn.BatchNorm2d(64)
                self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)
                self.bn2 = nn.BatchNorm2d(64)
                self.relu = nn.ReLU(inplace=True)
                
                # Stage 1 (ResNet-like) - BasicBlock ì‚¬ìš©í•˜ì—¬ 64ì±„ë„ ì¶œë ¥
                self.stage1 = self._make_layer(BasicBlock, 64, 4)
                
                # Stage 2
                stage2_cfg = cfg['STAGE2']
                num_channels = stage2_cfg['NUM_CHANNELS']
                block = BasicBlock
                num_channels = [
                    num_channels[i] * block.expansion for i in range(len(num_channels))
                ]
                # Stage 1ì˜ ì¶œë ¥ì€ 64ì±„ë„ (BasicBlock expansion=1, 64*1=64)
                self.transition1 = self._make_transition_layer([64], num_channels)
                self.stage2, pre_stage_channels = self._make_stage(
                    stage2_cfg, num_channels)
                
                # Stage 3
                stage3_cfg = cfg['STAGE3']
                num_channels = stage3_cfg['NUM_CHANNELS']
                block = BasicBlock
                num_channels = [
                    num_channels[i] * block.expansion for i in range(len(num_channels))
                ]
                self.transition2 = self._make_transition_layer(
                    pre_stage_channels, num_channels)
                self.stage3, pre_stage_channels = self._make_stage(
                    stage3_cfg, num_channels)
                
                # Stage 4
                stage4_cfg = cfg['STAGE4']
                num_channels = stage4_cfg['NUM_CHANNELS']
                block = BasicBlock
                num_channels = [
                    num_channels[i] * block.expansion for i in range(len(num_channels))
                ]
                self.transition3 = self._make_transition_layer(
                    pre_stage_channels, num_channels)
                self.stage4, pre_stage_channels = self._make_stage(
                    stage4_cfg, num_channels, multi_scale_output=True)
                
                # Final layer (í‚¤í¬ì¸íŠ¸ ì˜ˆì¸¡)
                self.final_layer = nn.Conv2d(
                    in_channels=pre_stage_channels[0],
                    out_channels=17,  # COCO 17 keypoints
                    kernel_size=1,
                    stride=1,
                    padding=0
                )
                
                self.pretrained_layers = ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']
            
            def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer):
                num_branches_cur = len(num_channels_cur_layer)
                num_branches_pre = len(num_channels_pre_layer)
                
                transition_layers = []
                for i in range(num_branches_cur):
                    if i < num_branches_pre:
                        if num_channels_cur_layer[i] != num_channels_pre_layer[i]:
                            transition_layers.append(
                                nn.Sequential(
                                    nn.Conv2d(
                                        num_channels_pre_layer[i],
                                        num_channels_cur_layer[i],
                                        3, 1, 1, bias=False
                                    ),
                                    nn.BatchNorm2d(num_channels_cur_layer[i]),
                                    nn.ReLU(inplace=True)
                                )
                            )
                        else:
                            transition_layers.append(None)
                    else:
                        conv3x3s = []
                        for j in range(i+1-num_branches_pre):
                            inchannels = num_channels_pre_layer[-1]
                            outchannels = num_channels_cur_layer[i] \
                                if j == i-num_branches_pre else inchannels
                            conv3x3s.append(
                                nn.Sequential(
                                    nn.Conv2d(inchannels, outchannels, 3, 2, 1, bias=False),
                                    nn.BatchNorm2d(outchannels),
                                    nn.ReLU(inplace=True)
                                )
                            )
                        transition_layers.append(nn.Sequential(*conv3x3s))
                
                return nn.ModuleList(transition_layers)
            
            def _make_layer(self, block, planes, blocks, stride=1):
                downsample = None
                if stride != 1 or self.inplanes != planes * block.expansion:
                    downsample = nn.Sequential(
                        nn.Conv2d(
                            self.inplanes, planes * block.expansion,
                            kernel_size=1, stride=stride, bias=False
                        ),
                        nn.BatchNorm2d(planes * block.expansion),
                    )
                
                layers = []
                layers.append(block(self.inplanes, planes, stride, downsample))
                self.inplanes = planes * block.expansion
                for i in range(1, blocks):
                    layers.append(block(self.inplanes, planes))
                
                return nn.Sequential(*layers)
            
            def _make_stage(self, layer_config, num_inchannels, multi_scale_output=True):
                num_modules = layer_config['NUM_MODULES']
                num_branches = layer_config['NUM_BRANCHES']
                num_blocks = layer_config['NUM_BLOCKS']
                num_channels = layer_config['NUM_CHANNELS']
                block = BasicBlock
                fuse_method = layer_config['FUSE_METHOD']
                
                modules = []
                for i in range(num_modules):
                    # multi_scale_outputì€ ë§ˆì§€ë§‰ ëª¨ë“ˆì—ì„œë§Œ ê³ ë ¤
                    if not multi_scale_output and i == num_modules - 1:
                        reset_multi_scale_output = False
                    else:
                        reset_multi_scale_output = True
                    
                    modules.append(
                        HighResolutionModule(
                            num_branches,
                            block,
                            num_blocks,
                            num_inchannels,
                            num_channels,
                            fuse_method,
                            reset_multi_scale_output
                        )
                    )
                    num_inchannels = modules[-1].get_num_inchannels()
                
                return nn.Sequential(*modules), num_inchannels
            
            def forward(self, x):
                # Stem
                # ë””ë²„ê¹…: ì…ë ¥ í…ì„œ í˜•íƒœ í™•ì¸
                if hasattr(self, 'logger'):
                    self.logger.info(f"ğŸ” HRNet ì…ë ¥ í…ì„œ í˜•íƒœ: {x.shape}")
                    self.logger.info(f"ğŸ” HRNet ì…ë ¥ í…ì„œ ì±„ë„: {x.shape[1]}")
                
                x = self.conv1(x)
                x = self.bn1(x)
                x = self.relu(x)
                x = self.conv2(x)
                x = self.bn2(x)
                x = self.relu(x)
                
                # Stage 1
                x = self.stage1(x)
                
                # ë””ë²„ê¹…: Stage 1 í›„ í…ì„œ í˜•íƒœ í™•ì¸
                if hasattr(self, 'logger'):
                    self.logger.info(f"ğŸ” HRNet Stage 1 í›„ í…ì„œ í˜•íƒœ: {x.shape}")
                    self.logger.info(f"ğŸ” HRNet Stage 1 í›„ í…ì„œ ì±„ë„: {x.shape[1]}")
                
                # Stage 2
                x_list = []
                for i in range(2):  # stage2 branches
                    if self.transition1[i] is not None:
                        x_list.append(self.transition1[i](x))
                    else:
                        x_list.append(x)
                y_list = self.stage2(x_list)
                
                # Stage 3
                x_list = []
                for i in range(3):  # stage3 branches
                    if self.transition2[i] is not None:
                        x_list.append(self.transition2[i](y_list[-1]))
                    else:
                        x_list.append(y_list[i])
                y_list = self.stage3(x_list)
                
                # Stage 4
                x_list = []
                for i in range(4):  # stage4 branches
                    if self.transition3[i] is not None:
                        x_list.append(self.transition3[i](y_list[-1]))
                    else:
                        x_list.append(y_list[i])
                y_list = self.stage4(x_list)
                
                # Final prediction
                x = self.final_layer(y_list[0])
                
                return x
        
        return PoseHighResolutionNet()
    
    def detect_poses(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Dict[str, Any]:
        """HRNet ê³ ì •ë°€ í¬ì¦ˆ ê²€ì¶œ (ì„œë¸Œí”½ì…€ ì •í™•ë„)"""
        if not self.loaded:
            raise RuntimeError("HRNet ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            input_tensor, scale_factor = self._preprocess_image_with_scale(image)
            
            # ì‹¤ì œ HRNet AI ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                if DEVICE == "cuda" and torch.cuda.is_available():
                    with autocast():
                        heatmaps = self.model(input_tensor)
                else:
                    heatmaps = self.model(input_tensor)
            
            # íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ê³ ì •ë°€ ì„œë¸Œí”½ì…€)
            keypoints = self._extract_keypoints_with_subpixel_accuracy(
                heatmaps[0], scale_factor
            )
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "keypoints": keypoints,
                "processing_time": processing_time,
                "model_type": "hrnet",
                "confidence": np.mean([kp[2] for kp in keypoints]) if keypoints else 0.0,
                "subpixel_accuracy": True,
                "heatmap_shape": heatmaps.shape,
                "scale_factor": scale_factor
            }
            
        except Exception as e:
            self.logger.error(f"âŒ HRNet AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "keypoints": [],
                "error": str(e),
                "processing_time": time.time() - start_time,
                "model_type": "hrnet"
            }
    
    def _preprocess_image_with_scale(self, image: Union[torch.Tensor, np.ndarray, Image.Image]) -> Tuple[torch.Tensor, float]:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ìŠ¤ì¼€ì¼ íŒ©í„° ë°˜í™˜"""
        if isinstance(image, Image.Image):
            image_np = np.array(image)
            orig_h, orig_w = image_np.shape[:2]
        elif isinstance(image, torch.Tensor):
            image_np = image.cpu().numpy()
            if image_np.ndim == 4:
                image_np = image_np[0]
            if image_np.shape[0] == 3:
                image_np = np.transpose(image_np, (1, 2, 0))
            orig_h, orig_w = image_np.shape[:2]
            image_np = (image_np * 255).astype(np.uint8)
        else:
            image_np = image
            orig_h, orig_w = image_np.shape[:2]
        
        # HRNet í‘œì¤€ ì…ë ¥ í¬ê¸°ë¡œ ì¡°ì •
        target_h, target_w = self.input_size
        scale_factor = min(target_w / orig_w, target_h / orig_h)
        
        # ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ
        new_w = int(orig_w * scale_factor)
        new_h = int(orig_h * scale_factor)
        
        if OPENCV_AVAILABLE:
            import cv2
            resized = cv2.resize(image_np, (new_w, new_h))
        else:
            pil_img = Image.fromarray(image_np)
            resized = np.array(pil_img.resize((new_w, new_h)))
        
        # ì¤‘ì•™ íŒ¨ë”©
        padded = np.zeros((target_h, target_w, 3), dtype=np.uint8)
        start_y = (target_h - new_h) // 2
        start_x = (target_w - new_w) // 2
        padded[start_y:start_y+new_h, start_x:start_x+new_w] = resized
        
        # ì •ê·œí™” ë° í…ì„œ ë³€í™˜
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        tensor = transform(Image.fromarray(padded)).unsqueeze(0)
        
        # ë””ë²„ê¹…: í…ì„œ í˜•íƒœ í™•ì¸
        self.logger.info(f"ğŸ” HRNet ì „ì²˜ë¦¬ í›„ í…ì„œ í˜•íƒœ: {tensor.shape}")
        self.logger.info(f"ğŸ” HRNet ì „ì²˜ë¦¬ í›„ í…ì„œ ì±„ë„: {tensor.shape[1]}")
        
        return tensor.to(self.device), scale_factor
    
    def _extract_keypoints_with_subpixel_accuracy(self, heatmaps: torch.Tensor, scale_factor: float) -> List[List[float]]:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ê³ ì •ë°€ ì„œë¸Œí”½ì…€ ì •í™•ë„)"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(17):  # COCO 17 keypoints
            if i < heatmaps.shape[0]:
                heatmap = heatmaps[i].cpu().numpy()
                
                # Gaussian ë¸”ëŸ¬ ì ìš© (ë…¸ì´ì¦ˆ ì œê±°)
                if OPENCV_AVAILABLE:
                    import cv2
                    heatmap_blurred = cv2.GaussianBlur(heatmap, (3, 3), 0)
                else:
                    heatmap_blurred = heatmap
                
                # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                y_idx, x_idx = np.unravel_index(np.argmax(heatmap_blurred), heatmap_blurred.shape)
                max_val = heatmap_blurred[y_idx, x_idx]
                
                # ì„œë¸Œí”½ì…€ ì •í™•ë„ë¥¼ ìœ„í•œ ê³ ê¸‰ ê°€ìš°ì‹œì•ˆ í”¼íŒ…
                if (2 <= x_idx < w-2) and (2 <= y_idx < h-2):
                    # 5x5 ìœˆë„ìš°ì—ì„œ ê°€ìš°ì‹œì•ˆ í”¼íŒ…
                    window = heatmap_blurred[y_idx-2:y_idx+3, x_idx-2:x_idx+3]
                    
                    # 2ì°¨ì› ê°€ìš°ì‹œì•ˆ í”¼íŒ…ìœ¼ë¡œ ì„œë¸Œí”½ì…€ ìœ„ì¹˜ ê³„ì‚°
                    try:
                        if SCIPY_AVAILABLE:
                            from scipy.optimize import curve_fit
                            
                            def gaussian_2d(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):
                                x, y = xy
                                xo, yo = float(xo), float(yo)
                                a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)
                                b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)
                                c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)
                                g = offset + amplitude*np.exp(- (a*((x-xo)**2) + 2*b*(x-xo)*(y-yo) + c*((y-yo)**2)))
                                return g.ravel()
                            
                            # í”¼íŒ…ì„ ìœ„í•œ ì¢Œí‘œ ê·¸ë¦¬ë“œ
                            y_grid, x_grid = np.mgrid[0:5, 0:5]
                            
                            # ì´ˆê¸° ì¶”ì •ê°’
                            initial_guess = (max_val, 2, 2, 1, 1, 0, 0)
                            
                            try:
                                popt, _ = curve_fit(gaussian_2d, (x_grid, y_grid), window.ravel(), 
                                                  p0=initial_guess, maxfev=1000)
                                
                                # ì„œë¸Œí”½ì…€ ì˜¤í”„ì…‹ ê³„ì‚°
                                subpixel_x = x_idx - 2 + popt[1]
                                subpixel_y = y_idx - 2 + popt[2]
                                confidence = popt[0]  # amplitude
                                
                            except:
                                # í”¼íŒ… ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ì¤‘ì‹¬ê°’ ê³„ì‚°
                                subpixel_x = float(x_idx)
                                subpixel_y = float(y_idx)
                                confidence = float(max_val)
                        else:
                            # Scipy ì—†ì´ ê°„ë‹¨í•œ ì¤‘ì‹¬ê°’ ê³„ì‚°
                            # ì£¼ë³€ í”½ì…€ë“¤ì˜ ê°€ì¤‘í‰ê· ìœ¼ë¡œ ì„œë¸Œí”½ì…€ ìœ„ì¹˜ ê³„ì‚°
                            total_weight = 0
                            weighted_x = 0
                            weighted_y = 0
                            
                            for dy in range(-1, 2):
                                for dx in range(-1, 2):
                                    if 0 <= y_idx+dy < h and 0 <= x_idx+dx < w:
                                        weight = heatmap_blurred[y_idx+dy, x_idx+dx]
                                        weighted_x += (x_idx + dx) * weight
                                        weighted_y += (y_idx + dy) * weight
                                        total_weight += weight
                            
                            if total_weight > 0:
                                subpixel_x = weighted_x / total_weight
                                subpixel_y = weighted_y / total_weight
                            else:
                                subpixel_x = float(x_idx)
                                subpixel_y = float(y_idx)
                            
                            confidence = float(max_val)
                    
                    except Exception:
                        # í´ë°±: ê¸°ë³¸ í”½ì…€ ìœ„ì¹˜
                        subpixel_x = float(x_idx)
                        subpixel_y = float(y_idx)
                        confidence = float(max_val)
                else:
                    # ê²½ê³„ ê·¼ì²˜: ê¸°ë³¸ í”½ì…€ ìœ„ì¹˜
                    subpixel_x = float(x_idx)
                    subpixel_y = float(y_idx)
                    confidence = float(max_val)
                
                # ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³€í™˜
                x_coord = (subpixel_x / w) * self.input_size[1] / scale_factor
                y_coord = (subpixel_y / h) * self.input_size[0] / scale_factor
                
                # ì‹ ë¢°ë„ ì •ê·œí™”
                confidence = min(1.0, max(0.0, confidence))
                
                keypoints.append([float(x_coord), float(y_coord), float(confidence)])
            else:
                keypoints.append([0.0, 0.0, 0.0])
        
        return keypoints
    
    def _extract_keypoints_from_heatmaps(self, heatmaps: torch.Tensor) -> List[List[float]]:
        """íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ê³ ì •ë°€ ì„œë¸Œí”½ì…€ ì •í™•ë„)"""
        keypoints = []
        h, w = heatmaps.shape[-2:]
        
        for i in range(17):  # COCO 17 keypoints
            if i < heatmaps.shape[0]:
                heatmap = heatmaps[i].cpu().numpy()
                
                # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                y_idx, x_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)
                max_val = heatmap[y_idx, x_idx]
                
                # ì„œë¸Œí”½ì…€ ì •í™•ë„ë¥¼ ìœ„í•œ ê°€ìš°ì‹œì•ˆ í”¼íŒ…
                if (1 <= x_idx < w-1) and (1 <= y_idx < h-1):
                    # x ë°©í–¥ ì„œë¸Œí”½ì…€ ë³´ì •
                    dx = 0.5 * (heatmap[y_idx, x_idx+1] - heatmap[y_idx, x_idx-1]) / (
                        heatmap[y_idx, x_idx+1] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx, x_idx-1] + 1e-8)
                    
                    # y ë°©í–¥ ì„œë¸Œí”½ì…€ ë³´ì •
                    dy = 0.5 * (heatmap[y_idx+1, x_idx] - heatmap[y_idx-1, x_idx]) / (
                        heatmap[y_idx+1, x_idx] - 2*heatmap[y_idx, x_idx] + heatmap[y_idx-1, x_idx] + 1e-8)
                    
                    # ì„œë¸Œí”½ì…€ ì¢Œí‘œ
                    x_subpixel = x_idx + dx
                    y_subpixel = y_idx + dy
                else:
                    x_subpixel = x_idx
                    y_subpixel = y_idx
                
                # ì¢Œí‘œ ì •ê·œí™”
                x_normalized = x_subpixel / w
                y_normalized = y_subpixel / h
                
                # ì‹¤ì œ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
                x_coord = x_normalized * 192
                y_coord = y_normalized * 256
                confidence = float(max_val)
                
                keypoints.append([x_coord, y_coord, confidence])
            else:
                keypoints.append([0.0, 0.0, 0.0])
        
        return keypoints

# ==============================================
# ğŸ”¥ 4. í¬ì¦ˆ ë¶„ì„ ì•Œê³ ë¦¬ì¦˜
# ==============================================

class PoseAnalyzer:
    """ê³ ê¸‰ í¬ì¦ˆ ë¶„ì„ ì•Œê³ ë¦¬ì¦˜ - ìƒì²´ì—­í•™ì  ë¶„ì„ í¬í•¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.PoseAnalyzer")
        
        # ìƒì²´ì—­í•™ì  ìƒìˆ˜ë“¤
        self.joint_angle_ranges = {
            'left_elbow': (0, 180),
            'right_elbow': (0, 180),
            'left_knee': (0, 180),
            'right_knee': (0, 180),
            'left_shoulder': (-45, 180),
            'right_shoulder': (-45, 180),
            'left_hip': (-45, 135),
            'right_hip': (-45, 135)
        }
        
        # ì‹ ì²´ ë¹„ìœ¨ í‘œì¤€ê°’ (ì„±ì¸ ê¸°ì¤€)
        self.standard_proportions = {
            'head_to_total': 0.125,      # ë¨¸ë¦¬:ì „ì²´ = 1:8
            'torso_to_total': 0.375,     # ìƒì²´:ì „ì²´ = 3:8
            'arm_to_total': 0.375,       # íŒ”:ì „ì²´ = 3:8
            'leg_to_total': 0.5,         # ë‹¤ë¦¬:ì „ì²´ = 4:8
            'shoulder_to_hip': 1.1       # ì–´ê¹¨ë„ˆë¹„:ì—‰ë©ì´ë„ˆë¹„ = 1.1:1
        }
    
    @staticmethod
    def calculate_joint_angles(keypoints: List[List[float]]) -> Dict[str, float]:
        """ê´€ì ˆ ê°ë„ ê³„ì‚° (ìƒì²´ì—­í•™ì  ì •í™•ë„)"""
        angles = {}
        
        def calculate_angle_3points(p1, p2, p3):
            """ì„¸ ì ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê°ë„ ê³„ì‚° (ë²¡í„° ë‚´ì  ì‚¬ìš©)"""
            try:
                # ë²¡í„° ê³„ì‚°
                v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
                v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
                
                # ë²¡í„° í¬ê¸° ê³„ì‚°
                mag_v1 = np.linalg.norm(v1)
                mag_v2 = np.linalg.norm(v2)
                
                if mag_v1 == 0 or mag_v2 == 0:
                    return 0.0
                
                # ë‚´ì ìœ¼ë¡œ ì½”ì‚¬ì¸ ê³„ì‚°
                cos_angle = np.dot(v1, v2) / (mag_v1 * mag_v2)
                cos_angle = np.clip(cos_angle, -1.0, 1.0)
                
                # ë¼ë””ì•ˆì„ ë„ë¡œ ë³€í™˜
                angle_rad = np.arccos(cos_angle)
                angle_deg = np.degrees(angle_rad)
                
                return float(angle_deg)
            except Exception:
                return 0.0
        
        def calculate_directional_angle(p1, p2, p3):
            """ë°©í–¥ì„±ì„ ê³ ë ¤í•œ ê°ë„ ê³„ì‚°"""
            try:
                # ì™¸ì ìœ¼ë¡œ ë°©í–¥ ê³„ì‚°
                v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])
                v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])
                
                cross_product = np.cross(v1, v2)
                angle = calculate_angle_3points(p1, p2, p3)
                
                # ì™¸ì ì˜ ë¶€í˜¸ë¡œ ë°©í–¥ ê²°ì •
                if cross_product < 0:
                    angle = 360 - angle
                
                return float(angle)
            except Exception:
                return 0.0
        
        if len(keypoints) >= 17:
            confidence_threshold = 0.3
            
            # ì™¼ìª½ íŒ”ê¿ˆì¹˜ ê°ë„ (ì–´ê¹¨-íŒ”ê¿ˆì¹˜-ì†ëª©)
            if all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[7], keypoints[9]]):
                angles['left_elbow'] = calculate_angle_3points(
                    keypoints[5], keypoints[7], keypoints[9]
                )
            
            # ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜ ê°ë„
            if all(kp[2] > confidence_threshold for kp in [keypoints[6], keypoints[8], keypoints[10]]):
                angles['right_elbow'] = calculate_angle_3points(
                    keypoints[6], keypoints[8], keypoints[10]
                )
            
            # ì™¼ìª½ ë¬´ë¦ ê°ë„ (ì—‰ë©ì´-ë¬´ë¦-ë°œëª©)
            if all(kp[2] > confidence_threshold for kp in [keypoints[11], keypoints[13], keypoints[15]]):
                angles['left_knee'] = calculate_angle_3points(
                    keypoints[11], keypoints[13], keypoints[15]
                )
            
            # ì˜¤ë¥¸ìª½ ë¬´ë¦ ê°ë„
            if all(kp[2] > confidence_threshold for kp in [keypoints[12], keypoints[14], keypoints[16]]):
                angles['right_knee'] = calculate_angle_3points(
                    keypoints[12], keypoints[14], keypoints[16]
                )
            
            # ì™¼ìª½ ì–´ê¹¨ ê°ë„ (ëª©-ì–´ê¹¨-íŒ”ê¿ˆì¹˜)
            # ëª© ìœ„ì¹˜ë¥¼ ì–´ê¹¨ ì¤‘ì ìœ¼ë¡œ ì¶”ì •
            if (all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[6], keypoints[7]]) and
                keypoints[5][2] > confidence_threshold and keypoints[6][2] > confidence_threshold):
                
                neck_x = (keypoints[5][0] + keypoints[6][0]) / 2
                neck_y = (keypoints[5][1] + keypoints[6][1]) / 2
                neck_point = [neck_x, neck_y, 1.0]
                
                angles['left_shoulder'] = calculate_directional_angle(
                    neck_point, keypoints[5], keypoints[7]
                )
            
            # ì˜¤ë¥¸ìª½ ì–´ê¹¨ ê°ë„
            if (all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[6], keypoints[8]]) and
                keypoints[5][2] > confidence_threshold and keypoints[6][2] > confidence_threshold):
                
                neck_x = (keypoints[5][0] + keypoints[6][0]) / 2
                neck_y = (keypoints[5][1] + keypoints[6][1]) / 2
                neck_point = [neck_x, neck_y, 1.0]
                
                angles['right_shoulder'] = calculate_directional_angle(
                    neck_point, keypoints[6], keypoints[8]
                )
            
            # ì™¼ìª½ ê³ ê´€ì ˆ ê°ë„ (ìƒì²´-ê³ ê´€ì ˆ-ë¬´ë¦)
            if all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[11], keypoints[13]]):
                angles['left_hip'] = calculate_directional_angle(
                    keypoints[5], keypoints[11], keypoints[13]
                )
            
            # ì˜¤ë¥¸ìª½ ê³ ê´€ì ˆ ê°ë„
            if all(kp[2] > confidence_threshold for kp in [keypoints[6], keypoints[12], keypoints[14]]):
                angles['right_hip'] = calculate_directional_angle(
                    keypoints[6], keypoints[12], keypoints[14]
                )
            
            # ëª© ê°ë„ (ì¢Œìš° ì–´ê¹¨-ì½”)
            if (keypoints[0][2] > confidence_threshold and 
                all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[6]])):
                
                # ì–´ê¹¨ ì¤‘ì 
                shoulder_center = [
                    (keypoints[5][0] + keypoints[6][0]) / 2,
                    (keypoints[5][1] + keypoints[6][1]) / 2
                ]
                
                # ìˆ˜ì§ì„ ê³¼ ëª©ì˜ ê°ë„
                neck_vector = [keypoints[0][0] - shoulder_center[0], 
                              keypoints[0][1] - shoulder_center[1]]
                vertical_vector = [0, -1]  # ìœ„ìª½ ë°©í–¥
                
                dot_product = np.dot(neck_vector, vertical_vector)
                neck_magnitude = np.linalg.norm(neck_vector)
                
                if neck_magnitude > 0:
                    cos_angle = dot_product / neck_magnitude
                    cos_angle = np.clip(cos_angle, -1.0, 1.0)
                    neck_angle = np.degrees(np.arccos(cos_angle))
                    angles['neck_tilt'] = float(neck_angle)
            
            # ì²™ì¶” ê³¡ë¥  ê³„ì‚°
            if (all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[6]]) and
                all(kp[2] > confidence_threshold for kp in [keypoints[11], keypoints[12]])):
                
                # ì–´ê¹¨ì™€ ì—‰ë©ì´ ì¤‘ì 
                shoulder_center = [(keypoints[5][0] + keypoints[6][0]) / 2,
                                 (keypoints[5][1] + keypoints[6][1]) / 2]
                hip_center = [(keypoints[11][0] + keypoints[12][0]) / 2,
                             (keypoints[11][1] + keypoints[12][1]) / 2]
                
                # ì²™ì¶” ë²¡í„°ì™€ ìˆ˜ì§ì„ ì˜ ê°ë„
                spine_vector = [shoulder_center[0] - hip_center[0],
                               shoulder_center[1] - hip_center[1]]
                vertical_vector = [0, -1]
                
                spine_magnitude = np.linalg.norm(spine_vector)
                if spine_magnitude > 0:
                    dot_product = np.dot(spine_vector, vertical_vector)
                    cos_angle = dot_product / spine_magnitude
                    cos_angle = np.clip(cos_angle, -1.0, 1.0)
                    spine_angle = np.degrees(np.arccos(cos_angle))
                    angles['spine_curvature'] = float(spine_angle)
        
        return angles
    
    @staticmethod
    def calculate_body_proportions(keypoints: List[List[float]]) -> Dict[str, float]:
        """ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚° (ì •ë°€í•œ í•´ë¶€í•™ì  ì¸¡ì •)"""
        proportions = {}
        
        def calculate_distance(p1, p2):
            """ë‘ ì  ì‚¬ì´ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬"""
            if len(p1) >= 2 and len(p2) >= 2:
                return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)
            return 0.0
        
        def calculate_body_part_length(keypoint_indices):
            """ì‹ ì²´ ë¶€ìœ„ì˜ ê¸¸ì´ ê³„ì‚°"""
            total_length = 0.0
            for i in range(len(keypoint_indices) - 1):
                idx1, idx2 = keypoint_indices[i], keypoint_indices[i + 1]
                if (idx1 < len(keypoints) and idx2 < len(keypoints) and
                    keypoints[idx1][2] > 0.3 and keypoints[idx2][2] > 0.3):
                    total_length += calculate_distance(keypoints[idx1], keypoints[idx2])
            return total_length
        
        if len(keypoints) >= 17:
            confidence_threshold = 0.3
            
            # ê¸°ë³¸ ê±°ë¦¬ ì¸¡ì •ë“¤
            measurements = {}
            
            # ì–´ê¹¨ ë„ˆë¹„
            if all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[6]]):
                measurements['shoulder_width'] = calculate_distance(keypoints[5], keypoints[6])
                proportions['shoulder_width'] = measurements['shoulder_width']
            
            # ì—‰ë©ì´ ë„ˆë¹„
            if all(kp[2] > confidence_threshold for kp in [keypoints[11], keypoints[12]]):
                measurements['hip_width'] = calculate_distance(keypoints[11], keypoints[12])
                proportions['hip_width'] = measurements['hip_width']
            
            # ì „ì²´ ì‹ ì¥ (ë¨¸ë¦¬-ë°œëª©)
            height_candidates = []
            if keypoints[0][2] > confidence_threshold:  # ì½”
                if keypoints[15][2] > confidence_threshold:  # ì™¼ë°œëª©
                    height_candidates.append(calculate_distance(keypoints[0], keypoints[15]))
                if keypoints[16][2] > confidence_threshold:  # ì˜¤ë¥¸ë°œëª©
                    height_candidates.append(calculate_distance(keypoints[0], keypoints[16]))
            
            if height_candidates:
                measurements['total_height'] = max(height_candidates)
                proportions['total_height'] = measurements['total_height']
            
            # ìƒì²´ ê¸¸ì´ (ì–´ê¹¨ ì¤‘ì  - ì—‰ë©ì´ ì¤‘ì )
            if ('shoulder_width' in measurements and 'hip_width' in measurements and
                all(kp[2] > confidence_threshold for kp in [keypoints[5], keypoints[6], keypoints[11], keypoints[12]])):
                
                shoulder_center = [(keypoints[5][0] + keypoints[6][0]) / 2,
                                 (keypoints[5][1] + keypoints[6][1]) / 2]
                hip_center = [(keypoints[11][0] + keypoints[12][0]) / 2,
                             (keypoints[11][1] + keypoints[12][1]) / 2]
                
                measurements['torso_length'] = calculate_distance(shoulder_center, hip_center)
                proportions['torso_length'] = measurements['torso_length']
            
            # íŒ” ê¸¸ì´ (ì–´ê¹¨-íŒ”ê¿ˆì¹˜-ì†ëª©)
            left_arm_length = calculate_body_part_length([5, 7, 9])  # ì™¼íŒ”
            right_arm_length = calculate_body_part_length([6, 8, 10])  # ì˜¤ë¥¸íŒ”
            
            if left_arm_length > 0:
                proportions['left_arm_length'] = left_arm_length
            if right_arm_length > 0:
                proportions['right_arm_length'] = right_arm_length
            if left_arm_length > 0 and right_arm_length > 0:
                proportions['avg_arm_length'] = (left_arm_length + right_arm_length) / 2
            
            # ë‹¤ë¦¬ ê¸¸ì´ (ì—‰ë©ì´-ë¬´ë¦-ë°œëª©)
            left_leg_length = calculate_body_part_length([11, 13, 15])  # ì™¼ë‹¤ë¦¬
            right_leg_length = calculate_body_part_length([12, 14, 16])  # ì˜¤ë¥¸ë‹¤ë¦¬
            
            if left_leg_length > 0:
                proportions['left_leg_length'] = left_leg_length
            if right_leg_length > 0:
                proportions['right_leg_length'] = right_leg_length
            if left_leg_length > 0 and right_leg_length > 0:
                proportions['avg_leg_length'] = (left_leg_length + right_leg_length) / 2
            
            # ë¹„ìœ¨ ê³„ì‚°
            if 'total_height' in measurements and measurements['total_height'] > 0:
                height = measurements['total_height']
                
                # ë¨¸ë¦¬ í¬ê¸° (ì½”-ëª© ê±°ë¦¬ ì¶”ì •)
                if keypoints[0][2] > confidence_threshold and 'torso_length' in measurements:
                    estimated_head_length = measurements['torso_length'] * 0.25  # ì¶”ì •ê°’
                    proportions['head_to_height_ratio'] = estimated_head_length / height
                
                # ìƒì²´ ëŒ€ ì „ì²´ ë¹„ìœ¨
                if 'torso_length' in measurements:
                    proportions['torso_to_height_ratio'] = measurements['torso_length'] / height
                
                # ë‹¤ë¦¬ ëŒ€ ì „ì²´ ë¹„ìœ¨
                if 'avg_leg_length' in proportions:
                    proportions['leg_to_height_ratio'] = proportions['avg_leg_length'] / height
                
                # íŒ” ëŒ€ ì „ì²´ ë¹„ìœ¨
                if 'avg_arm_length' in proportions:
                    proportions['arm_to_height_ratio'] = proportions['avg_arm_length'] / height
            
            # ì¢Œìš° ëŒ€ì¹­ì„± ê²€ì‚¬
            if 'left_arm_length' in proportions and 'right_arm_length' in proportions:
                arm_asymmetry = abs(proportions['left_arm_length'] - proportions['right_arm_length'])
                avg_arm = (proportions['left_arm_length'] + proportions['right_arm_length']) / 2
                if avg_arm > 0:
                    proportions['arm_asymmetry_ratio'] = arm_asymmetry / avg_arm
            
            if 'left_leg_length' in proportions and 'right_leg_length' in proportions:
                leg_asymmetry = abs(proportions['left_leg_length'] - proportions['right_leg_length'])
                avg_leg = (proportions['left_leg_length'] + proportions['right_leg_length']) / 2
                if avg_leg > 0:
                    proportions['leg_asymmetry_ratio'] = leg_asymmetry / avg_leg
            
            # ì–´ê¹¨-ì—‰ë©ì´ ë¹„ìœ¨
            if 'shoulder_width' in measurements and 'hip_width' in measurements and measurements['hip_width'] > 0:
                proportions['shoulder_to_hip_ratio'] = measurements['shoulder_width'] / measurements['hip_width']
            
            # BMI ì¶”ì • (ë§¤ìš° ëŒ€ëµì )
            if 'total_height' in measurements and 'shoulder_width' in measurements:
                # ì–´ê¹¨ ë„ˆë¹„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì²´ê²© ì¶”ì • (ë§¤ìš° ëŒ€ëµì )
                estimated_body_mass_index = (measurements['shoulder_width'] / measurements['total_height']) * 100
                proportions['estimated_bmi_indicator'] = estimated_body_mass_index
        
        return proportions
    
    def assess_pose_quality(self, 
                          keypoints: List[List[float]], 
                          joint_angles: Dict[str, float], 
                          body_proportions: Dict[str, float]) -> Dict[str, Any]:
        """í¬ì¦ˆ í’ˆì§ˆ í‰ê°€ (ë‹¤ì°¨ì› ë¶„ì„)"""
        assessment = {
            'overall_score': 0.0,
            'quality_grade': PoseQuality.POOR,
            'detailed_scores': {},
            'issues': [],
            'recommendations': [],
            'confidence_analysis': {},
            'anatomical_plausibility': {},
            'symmetry_analysis': {}
        }
        
        try:
            # 1. í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„± ë¶„ì„
            visible_keypoints = [kp for kp in keypoints if len(kp) >= 3 and kp[2] > 0.1]
            high_conf_keypoints = [kp for kp in keypoints if len(kp) >= 3 and kp[2] > 0.7]
            
            visibility_score = len(visible_keypoints) / len(keypoints)
            high_confidence_score = len(high_conf_keypoints) / len(keypoints)
            
            # 2. ì‹ ë¢°ë„ ë¶„ì„
            confidence_scores = [kp[2] for kp in keypoints if len(kp) >= 3 and kp[2] > 0.1]
            if confidence_scores:
                avg_confidence = np.mean(confidence_scores)
                confidence_std = np.std(confidence_scores)
                min_confidence = np.min(confidence_scores)
                max_confidence = np.max(confidence_scores)
            else:
                avg_confidence = confidence_std = min_confidence = max_confidence = 0.0
            
            assessment['confidence_analysis'] = {
                'average': avg_confidence,
                'std_deviation': confidence_std,
                'min_confidence': min_confidence,
                'max_confidence': max_confidence,
                'confidence_consistency': 1.0 - (confidence_std / (avg_confidence + 1e-8))
            }
            
            # 3. í•´ë¶€í•™ì  íƒ€ë‹¹ì„± ê²€ì‚¬
            anatomical_score = self._assess_anatomical_plausibility(keypoints, joint_angles)
            
            # 4. ëŒ€ì¹­ì„± ë¶„ì„
            symmetry_score = self._assess_body_symmetry(keypoints, body_proportions)
            
            # 5. í¬ì¦ˆ ì™„ì„±ë„
            critical_keypoints = [0, 5, 6, 11, 12]  # ì½”, ì–´ê¹¨ë“¤, ì—‰ë©ì´ë“¤
            critical_visible = sum(1 for i in critical_keypoints 
                                 if i < len(keypoints) and len(keypoints[i]) >= 3 and keypoints[i][2] > 0.5)
            completeness_score = critical_visible / len(critical_keypoints)
            
            # 6. ì „ì²´ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘í‰ê· )
            weights = {
                'visibility': 0.25,
                'confidence': 0.25,
                'anatomical': 0.20,
                'symmetry': 0.15,
                'completeness': 0.15
            }
            
            overall_score = (
                visibility_score * weights['visibility'] +
                avg_confidence * weights['confidence'] +
                anatomical_score * weights['anatomical'] +
                symmetry_score * weights['symmetry'] +
                completeness_score * weights['completeness']
            )
            
            # 7. í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
            if overall_score >= 0.9:
                quality_grade = PoseQuality.EXCELLENT
            elif overall_score >= 0.75:
                quality_grade = PoseQuality.GOOD
            elif overall_score >= 0.6:
                quality_grade = PoseQuality.ACCEPTABLE
            elif overall_score >= 0.4:
                quality_grade = PoseQuality.POOR
            else:
                quality_grade = PoseQuality.VERY_POOR
            
            # 8. ì„¸ë¶€ ì ìˆ˜
            assessment['detailed_scores'] = {
                'visibility': visibility_score,
                'high_confidence_ratio': high_confidence_score,
                'average_confidence': avg_confidence,
                'anatomical_plausibility': anatomical_score,
                'symmetry': symmetry_score,
                'completeness': completeness_score
            }
            
            # 9. ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
            assessment['issues'] = self._identify_pose_issues(
                keypoints, joint_angles, body_proportions, assessment['detailed_scores']
            )
            assessment['recommendations'] = self._generate_pose_recommendations(
                assessment['issues'], assessment['detailed_scores']
            )
            
            # 10. ìµœì¢… ê²°ê³¼ ì—…ë°ì´íŠ¸
            assessment.update({
                'overall_score': overall_score,
                'quality_grade': quality_grade,
                'anatomical_plausibility': {
                    'score': anatomical_score,
                    'joint_angle_validity': self._validate_joint_angles(joint_angles),
                    'proportion_validity': self._validate_body_proportions(body_proportions)
                },
                'symmetry_analysis': {
                    'score': symmetry_score,
                    'left_right_balance': self._analyze_left_right_balance(keypoints),
                    'posture_alignment': self._analyze_posture_alignment(keypoints)
                }
            })
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ì¦ˆ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            assessment['error'] = str(e)
        
        return assessment
    
    def _assess_anatomical_plausibility(self, keypoints: List[List[float]], joint_angles: Dict[str, float]) -> float:
        """í•´ë¶€í•™ì  íƒ€ë‹¹ì„± í‰ê°€"""
        plausibility_score = 1.0
        penalty = 0.0
        
        # ê´€ì ˆ ê°ë„ ë²”ìœ„ ê²€ì‚¬
        for joint, angle in joint_angles.items():
            if joint in self.joint_angle_ranges:
                min_angle, max_angle = self.joint_angle_ranges[joint]
                if not (min_angle <= angle <= max_angle):
                    penalty += 0.1  # ë²”ìœ„ ë²—ì–´ë‚  ë•Œë§ˆë‹¤ 10% ê°ì 
        
        # í‚¤í¬ì¸íŠ¸ ìœ„ì¹˜ ìƒì‹ì„± ê²€ì‚¬
        if len(keypoints) >= 17:
            # ì–´ê¹¨ê°€ ì—‰ë©ì´ë³´ë‹¤ ìœ„ì— ìˆëŠ”ì§€
            if (keypoints[5][2] > 0.3 and keypoints[6][2] > 0.3 and
                keypoints[11][2] > 0.3 and keypoints[12][2] > 0.3):
                
                avg_shoulder_y = (keypoints[5][1] + keypoints[6][1]) / 2
                avg_hip_y = (keypoints[11][1] + keypoints[12][1]) / 2
                
                if avg_shoulder_y >= avg_hip_y:  # ì–´ê¹¨ê°€ ì—‰ë©ì´ë³´ë‹¤ ì•„ë˜ì— ìˆìŒ (ë¹„ì •ìƒ)
                    penalty += 0.2
            
            # íŒ”ê¿ˆì¹˜ê°€ ì–´ê¹¨ì™€ ì†ëª© ì‚¬ì´ì— ìˆëŠ”ì§€
            for side in ['left', 'right']:
                if side == 'left':
                    shoulder_idx, elbow_idx, wrist_idx = 5, 7, 9
                else:
                    shoulder_idx, elbow_idx, wrist_idx = 6, 8, 10
                
                if all(keypoints[i][2] > 0.3 for i in [shoulder_idx, elbow_idx, wrist_idx]):
                    # íŒ”ê¿ˆì¹˜ê°€ ì–´ê¹¨-ì†ëª© ì„ ë¶„ì—ì„œ ë„ˆë¬´ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ ê²€ì‚¬
                    arm_length = np.linalg.norm(np.array(keypoints[shoulder_idx][:2]) - 
                                              np.array(keypoints[wrist_idx][:2]))
                    elbow_distance = self._point_to_line_distance(
                        keypoints[elbow_idx][:2], 
                        keypoints[shoulder_idx][:2], 
                        keypoints[wrist_idx][:2]
                    )
                    
                    if arm_length > 0 and elbow_distance / arm_length > 0.3:  # íŒ” ê¸¸ì´ì˜ 30% ì´ìƒ ë²—ì–´ë‚¨
                        penalty += 0.1
        
        plausibility_score = max(0.0, plausibility_score - penalty)
        return plausibility_score
    
    def _assess_body_symmetry(self, keypoints: List[List[float]], body_proportions: Dict[str, float]) -> float:
        """ì‹ ì²´ ëŒ€ì¹­ì„± í‰ê°€"""
        symmetry_score = 1.0
        penalty = 0.0
        
        if len(keypoints) >= 17:
            # ì¢Œìš° ì–´ê¹¨ ë†’ì´ ë¹„êµ
            if keypoints[5][2] > 0.3 and keypoints[6][2] > 0.3:
                shoulder_height_diff = abs(keypoints[5][1] - keypoints[6][1])
                shoulder_width = abs(keypoints[5][0] - keypoints[6][0])
                if shoulder_width > 0:
                    shoulder_asymmetry = shoulder_height_diff / shoulder_width
                    if shoulder_asymmetry > 0.2:  # 20% ì´ìƒ ë¹„ëŒ€ì¹­
                        penalty += 0.1
            
            # ì¢Œìš° ì—‰ë©ì´ ë†’ì´ ë¹„êµ
            if keypoints[11][2] > 0.3 and keypoints[12][2] > 0.3:
                hip_height_diff = abs(keypoints[11][1] - keypoints[12][1])
                hip_width = abs(keypoints[11][0] - keypoints[12][0])
                if hip_width > 0:
                    hip_asymmetry = hip_height_diff / hip_width
                    if hip_asymmetry > 0.2:
                        penalty += 0.1
            
            # íŒ” ê¸¸ì´ ëŒ€ì¹­ì„±
            if 'arm_asymmetry_ratio' in body_proportions:
                if body_proportions['arm_asymmetry_ratio'] > 0.15:  # 15% ì´ìƒ ì°¨ì´
                    penalty += 0.1
            
            # ë‹¤ë¦¬ ê¸¸ì´ ëŒ€ì¹­ì„±
            if 'leg_asymmetry_ratio' in body_proportions:
                if body_proportions['leg_asymmetry_ratio'] > 0.15:
                    penalty += 0.1
        
        symmetry_score = max(0.0, symmetry_score - penalty)
        return symmetry_score
    
    def _point_to_line_distance(self, point, line_start, line_end):
        """ì ì—ì„œ ì§ì„ ê¹Œì§€ì˜ ê±°ë¦¬ ê³„ì‚°"""
        try:
            line_vec = np.array(line_end) - np.array(line_start)
            point_vec = np.array(point) - np.array(line_start)
            
            line_len = np.linalg.norm(line_vec)
            if line_len == 0:
                return np.linalg.norm(point_vec)
            
            line_unitvec = line_vec / line_len
            proj_length = np.dot(point_vec, line_unitvec)
            proj = proj_length * line_unitvec
            
            distance = np.linalg.norm(point_vec - proj)
            return distance
        except:
            return 0.0
    
    def _validate_joint_angles(self, joint_angles: Dict[str, float]) -> Dict[str, bool]:
        """ê´€ì ˆ ê°ë„ ìœ íš¨ì„± ê²€ì¦"""
        validity = {}
        for joint, angle in joint_angles.items():
            if joint in self.joint_angle_ranges:
                min_angle, max_angle = self.joint_angle_ranges[joint]
                validity[joint] = min_angle <= angle <= max_angle
            else:
                validity[joint] = True  # ë²”ìœ„ê°€ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš° ìœ íš¨ë¡œ ê°„ì£¼
        return validity
    
    def _validate_body_proportions(self, body_proportions: Dict[str, float]) -> Dict[str, Any]:
        """ì‹ ì²´ ë¹„ìœ¨ ìœ íš¨ì„± ê²€ì¦"""
        validation = {
            'proportions_within_normal_range': True,
            'unusual_proportions': [],
            'proportion_score': 1.0
        }
        
        # í‘œì¤€ ë¹„ìœ¨ê³¼ ë¹„êµ
        for prop_name, standard_value in self.standard_proportions.items():
            if prop_name in body_proportions:
                measured_value = body_proportions[prop_name]
                # í‘œì¤€ê°’ì˜ Â±50% ë²”ìœ„ ë‚´ì—ì„œ ì •ìƒìœ¼ë¡œ ê°„ì£¼
                tolerance = standard_value * 0.5
                
                if not (standard_value - tolerance <= measured_value <= standard_value + tolerance):
                    validation['proportions_within_normal_range'] = False
                    validation['unusual_proportions'].append({
                        'proportion': prop_name,
                        'measured': measured_value,
                        'standard': standard_value,
                        'deviation_percent': abs(measured_value - standard_value) / standard_value * 100
                    })
        
        # ë¹„ìœ¨ ì ìˆ˜ ê³„ì‚°
        if validation['unusual_proportions']:
            penalty = min(0.5, len(validation['unusual_proportions']) * 0.1)
            validation['proportion_score'] = max(0.0, 1.0 - penalty)
        
        return validation
    
    def _analyze_left_right_balance(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """ì¢Œìš° ê· í˜• ë¶„ì„"""
        balance_analysis = {
            'overall_balance_score': 1.0,
            'shoulder_balance': 1.0,
            'hip_balance': 1.0,
            'limb_position_balance': 1.0
        }
        
        if len(keypoints) >= 17:
            # ì–´ê¹¨ ê· í˜•
            if keypoints[5][2] > 0.3 and keypoints[6][2] > 0.3:
                shoulder_height_diff = abs(keypoints[5][1] - keypoints[6][1])
                shoulder_center = (keypoints[5][1] + keypoints[6][1]) / 2
                if shoulder_center > 0:
                    balance_analysis['shoulder_balance'] = max(0.0, 1.0 - (shoulder_height_diff / shoulder_center))
            
            # ì—‰ë©ì´ ê· í˜•
            if keypoints[11][2] > 0.3 and keypoints[12][2] > 0.3:
                hip_height_diff = abs(keypoints[11][1] - keypoints[12][1])
                hip_center = (keypoints[11][1] + keypoints[12][1]) / 2
                if hip_center > 0:
                    balance_analysis['hip_balance'] = max(0.0, 1.0 - (hip_height_diff / hip_center))
            
            # ì „ì²´ ê· í˜• ì ìˆ˜
            balance_analysis['overall_balance_score'] = (
                balance_analysis['shoulder_balance'] * 0.4 +
                balance_analysis['hip_balance'] * 0.4 +
                balance_analysis['limb_position_balance'] * 0.2
            )
        
        return balance_analysis
    
    def _analyze_posture_alignment(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """ìì„¸ ì •ë ¬ ë¶„ì„"""
        alignment_analysis = {
            'spine_alignment_score': 1.0,
            'head_neck_alignment': 1.0,
            'overall_posture_score': 1.0
        }
        
        if len(keypoints) >= 17:
            # ì²™ì¶” ì •ë ¬ (ì–´ê¹¨ ì¤‘ì ê³¼ ì—‰ë©ì´ ì¤‘ì ì˜ ìˆ˜ì§ ì •ë ¬)
            if (all(keypoints[i][2] > 0.3 for i in [5, 6, 11, 12])):
                shoulder_center_x = (keypoints[5][0] + keypoints[6][0]) / 2
                hip_center_x = (keypoints[11][0] + keypoints[12][0]) / 2
                
                horizontal_offset = abs(shoulder_center_x - hip_center_x)
                body_width = abs(keypoints[5][0] - keypoints[6][0])
                
                if body_width > 0:
                    alignment_ratio = horizontal_offset / body_width
                    alignment_analysis['spine_alignment_score'] = max(0.0, 1.0 - alignment_ratio)
            
            # ë¨¸ë¦¬-ëª© ì •ë ¬
            if (keypoints[0][2] > 0.3 and 
                all(keypoints[i][2] > 0.3 for i in [5, 6])):
                
                neck_center_x = (keypoints[5][0] + keypoints[6][0]) / 2
                head_offset = abs(keypoints[0][0] - neck_center_x)
                neck_width = abs(keypoints[5][0] - keypoints[6][0])
                
                if neck_width > 0:
                    head_alignment_ratio = head_offset / neck_width
                    alignment_analysis['head_neck_alignment'] = max(0.0, 1.0 - head_alignment_ratio)
            
            # ì „ì²´ ìì„¸ ì ìˆ˜
            alignment_analysis['overall_posture_score'] = (
                alignment_analysis['spine_alignment_score'] * 0.6 +
                alignment_analysis['head_neck_alignment'] * 0.4
            )
        
        return alignment_analysis
    
    def _identify_pose_issues(self, 
                            keypoints: List[List[float]], 
                            joint_angles: Dict[str, float], 
                            body_proportions: Dict[str, float],
                            scores: Dict[str, float]) -> List[str]:
        """í¬ì¦ˆ ë¬¸ì œì  ì‹ë³„"""
        issues = []
        
        # ê°€ì‹œì„± ë¬¸ì œ
        if scores.get('visibility', 0) < 0.6:
            issues.append("í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„±ì´ ë‚®ìŠµë‹ˆë‹¤")
        
        # ì‹ ë¢°ë„ ë¬¸ì œ
        if scores.get('average_confidence', 0) < 0.5:
            issues.append("í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤")
        
        # í•´ë¶€í•™ì  ë¬¸ì œ
        if scores.get('anatomical_plausibility', 0) < 0.7:
            issues.append("í•´ë¶€í•™ì ìœ¼ë¡œ ë¶€ìì—°ìŠ¤ëŸ¬ìš´ í¬ì¦ˆì…ë‹ˆë‹¤")
        
        # ëŒ€ì¹­ì„± ë¬¸ì œ
        if scores.get('symmetry', 0) < 0.7:
            issues.append("ì‹ ì²´ ì¢Œìš° ëŒ€ì¹­ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤")
        
        # ì™„ì„±ë„ ë¬¸ì œ
        if scores.get('completeness', 0) < 0.8:
            issues.append("í•µì‹¬ ì‹ ì²´ ë¶€ìœ„ê°€ ê²€ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # ê´€ì ˆ ê°ë„ ë¬¸ì œ
        invalid_joints = [joint for joint, angle in joint_angles.items() 
                         if joint in self.joint_angle_ranges and 
                         not (self.joint_angle_ranges[joint][0] <= angle <= self.joint_angle_ranges[joint][1])]
        
        if invalid_joints:
            issues.append(f"ë¹„ì •ìƒì ì¸ ê´€ì ˆ ê°ë„: {', '.join(invalid_joints)}")
        
        # ë¹„ìœ¨ ë¬¸ì œ
        unusual_proportions = []
        for prop_name, standard_value in self.standard_proportions.items():
            if prop_name in body_proportions:
                measured_value = body_proportions[prop_name]
                tolerance = standard_value * 0.5
                if not (standard_value - tolerance <= measured_value <= standard_value + tolerance):
                    deviation = abs(measured_value - standard_value) / standard_value * 100
                    unusual_proportions.append(f"{prop_name} ({deviation:.1f}% í¸ì°¨)")
        
        if unusual_proportions:
            issues.append(f"ë¹„ì •ìƒì ì¸ ì‹ ì²´ ë¹„ìœ¨: {', '.join(unusual_proportions)}")
        
        return issues
    
    def _generate_pose_recommendations(self, issues: List[str], scores: Dict[str, float]) -> List[str]:
        """í¬ì¦ˆ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ê°€ì‹œì„± ê°œì„ 
        if scores.get('visibility', 0) < 0.6:
            recommendations.extend([
                "ì „ì‹ ì´ í”„ë ˆì„ ì•ˆì— ë“¤ì–´ì˜¤ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”",
                "ê°€ë ¤ì§„ ì‹ ì²´ ë¶€ìœ„ê°€ ë³´ì´ë„ë¡ ìì„¸ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”",
                "ë” ë°ì€ ì¡°ëª…ì—ì„œ ì´¬ì˜í•´ ì£¼ì„¸ìš”"
            ])
        
        # ì‹ ë¢°ë„ ê°œì„ 
        if scores.get('average_confidence', 0) < 0.5:
            recommendations.extend([
                "ë” ì„ ëª…í•˜ê³  ê³ í•´ìƒë„ë¡œ ì´¬ì˜í•´ ì£¼ì„¸ìš”",
                "ë°°ê²½ê³¼ ëŒ€ë¹„ë˜ëŠ” ì˜ìƒì„ ì°©ìš©í•´ ì£¼ì„¸ìš”",
                "ì¹´ë©”ë¼ í”ë“¤ë¦¼ ì—†ì´ ì´¬ì˜í•´ ì£¼ì„¸ìš”"
            ])
        
        # í•´ë¶€í•™ì  ê°œì„ 
        if scores.get('anatomical_plausibility', 0) < 0.7:
            recommendations.extend([
                "ìì—°ìŠ¤ëŸ¬ìš´ ìì„¸ë¥¼ ì·¨í•´ ì£¼ì„¸ìš”",
                "ê³¼ë„í•˜ê²Œ êµ¬ë¶€ëŸ¬ì§„ ê´€ì ˆì„ í´ì£¼ì„¸ìš”",
                "ì •ë©´ ë˜ëŠ” ì¸¡ë©´ì„ í–¥í•œ ìì„¸ë¡œ ì´¬ì˜í•´ ì£¼ì„¸ìš”"
            ])
        
        # ëŒ€ì¹­ì„± ê°œì„ 
        if scores.get('symmetry', 0) < 0.7:
            recommendations.extend([
                "ì–´ê¹¨ì™€ ì—‰ë©ì´ê°€ ìˆ˜í‰ì´ ë˜ë„ë¡ ìì„¸ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”",
                "ì¢Œìš° íŒ”ë‹¤ë¦¬ê°€ ê· í˜•ì„ ì´ë£¨ë„ë¡ í•´ì£¼ì„¸ìš”",
                "ëª¸ì˜ ì¤‘ì‹¬ì„ ì´ ë˜‘ë°”ë¡œ ì„œë„ë¡ í•´ì£¼ì„¸ìš”"
            ])
        
        # ì™„ì„±ë„ ê°œì„ 
        if scores.get('completeness', 0) < 0.8:
            recommendations.extend([
                "ë¨¸ë¦¬ë¶€í„° ë°œëê¹Œì§€ ì „ì‹ ì´ ë³´ì´ë„ë¡ ì´¬ì˜í•´ ì£¼ì„¸ìš”",
                "íŒ”ê³¼ ë‹¤ë¦¬ê°€ ëª¸í†µì— ê°€ë ¤ì§€ì§€ ì•Šë„ë¡ í•´ì£¼ì„¸ìš”",
                "ì¹´ë©”ë¼ì™€ì˜ ê±°ë¦¬ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”"
            ])
        
        # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations.extend([
                "í˜„ì¬ í¬ì¦ˆê°€ ì–‘í˜¸í•©ë‹ˆë‹¤",
                "ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•´ ì¡°ëª…ì„ ê°œì„ í•´ ë³´ì„¸ìš”",
                "ë‹¤ì–‘í•œ ê°ë„ì—ì„œ ì´¬ì˜í•´ ë³´ì„¸ìš”"
            ])
        
        return recommendations[:5]  # ìµœëŒ€ 5ê°œ ê¶Œì¥ì‚¬í•­ë§Œ ë°˜í™˜

# ==============================================
# ğŸ”¥ 5. ë©”ì¸ PoseEstimationStep í´ë˜ìŠ¤
# ==============================================

class PoseEstimationStep(BaseStepMixin):
    """
    ğŸ”¥ Step 02: Pose Estimation - Central Hub DI Container v7.0 ì™„ì „ ì—°ë™
    
    âœ… BaseStepMixin ìƒì† íŒ¨í„´ (Human Parsing Stepê³¼ ë™ì¼)
    âœ… MediaPipe Pose ëª¨ë¸ ì§€ì› (ìš°ì„ ìˆœìœ„ 1)
    âœ… OpenPose ëª¨ë¸ ì§€ì› (í´ë°± ì˜µì…˜)
    âœ… YOLOv8-Pose ëª¨ë¸ ì§€ì› (ì‹¤ì‹œê°„)
    âœ… HRNet ëª¨ë¸ ì§€ì› (ê³ ì •ë°€)
    âœ… 17ê°œ COCO keypoints ê°ì§€
    âœ… Mock ëª¨ë¸ ì™„ì „ ì œê±°
    âœ… ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰
    âœ… ë‹¤ì¤‘ ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ
    """
    
    def __init__(self, **kwargs):
        
        """í¬ì¦ˆ ì¶”ì • Step ì´ˆê¸°í™”"""
        self._lock = threading.RLock()  # âœ… threading ì‚¬ìš©

        # ğŸ”¥ 1. í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™” (ì—ëŸ¬ ë°©ì§€)
        self._initialize_step_attributes()
        
                # ğŸ”¥ 2. BaseStepMixin ì´ˆê¸°í™” (Central Hub ìë™ ì—°ë™)
        super().__init__(step_name="PoseEstimationStep", **kwargs)
        
        # ğŸ”¥ 3. Pose Estimation íŠ¹í™” ì´ˆê¸°í™”
        self._initialize_pose_estimation_specifics()
    
    def _initialize_step_attributes(self):
        """Step í•„ìˆ˜ ì†ì„±ë“¤ ì´ˆê¸°í™”"""
        self.ai_models = {}
        self.models_loading_status = {
            'mediapipe': False,
            'openpose': False,
            'yolov8': False,
            'hrnet': False,
            'total_loaded': 0,
            'loading_errors': []
        }
        self.model_interface = None
        self.loaded_models = {}
        
        # Pose Estimation íŠ¹í™” ì†ì„±ë“¤
        self.pose_models = {}
        self.pose_ready = False
        self.keypoints_cache = {}
    
    def _initialize_pose_estimation_specifics(self):
        """Pose Estimation íŠ¹í™” ì´ˆê¸°í™”"""
        
        # ì„¤ì •
        self.confidence_threshold = 0.5
        self.use_subpixel = True
        
        # í¬ì¦ˆ ë¶„ì„ê¸°
        self.analyzer = PoseAnalyzer()
        
        # ëª¨ë¸ ìš°ì„ ìˆœìœ„ (MediaPipe ìš°ì„ )
        self.model_priority = [
            PoseModel.MEDIAPIPE,
            PoseModel.YOLOV8_POSE,
            PoseModel.OPENPOSE,
            PoseModel.HRNET
        ]
        
        self.logger.info(f"âœ… {self.step_name} í¬ì¦ˆ ì¶”ì • íŠ¹í™” ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_pose_models_via_central_hub(self):
        """Central Hubë¥¼ í†µí•œ Pose ëª¨ë¸ ë¡œë”©"""
        loaded_count = 0
        
        if self.model_loader:  # Central Hubì—ì„œ ìë™ ì£¼ì…ë¨
            # MediaPipe ëª¨ë¸ ë¡œë”©
            try:
                mediapipe_model = MediaPoseModel()
                if mediapipe_model.load_model():
                    self.ai_models['mediapipe'] = mediapipe_model
                    self.models_loading_status['mediapipe'] = True
                    loaded_count += 1
                    self.logger.info("âœ… MediaPipe ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ MediaPipe ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.models_loading_status['loading_errors'].append(f"MediaPipe: {e}")
            
            # YOLOv8 ëª¨ë¸ ë¡œë”©
            try:
                # Central Hubì—ì„œ YOLOv8 ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì¡°íšŒ
                yolo_path = self._get_model_path_from_central_hub('yolov8n-pose.pt')
                yolo_model = YOLOv8PoseModel(yolo_path)
                if yolo_model.load_model():
                    self.ai_models['yolov8'] = yolo_model
                    self.models_loading_status['yolov8'] = True
                    loaded_count += 1
                    self.logger.info("âœ… YOLOv8 ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ YOLOv8 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.models_loading_status['loading_errors'].append(f"YOLOv8: {e}")
            
            # OpenPose ëª¨ë¸ ë¡œë”©
            try:
                openpose_path = self._get_model_path_from_central_hub('body_pose_model.pth')
                openpose_model = OpenPoseModel(openpose_path)
                if openpose_model.load_model():
                    self.ai_models['openpose'] = openpose_model
                    self.models_loading_status['openpose'] = True
                    loaded_count += 1
                    self.logger.info("âœ… OpenPose ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ OpenPose ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.models_loading_status['loading_errors'].append(f"OpenPose: {e}")
            
            # HRNet ëª¨ë¸ ë¡œë”©
            try:
                hrnet_path = self._get_model_path_from_central_hub('hrnet_w48_coco_256x192.pth')
                hrnet_model = HRNetModel(hrnet_path)
                if hrnet_model.load_model():
                    self.ai_models['hrnet'] = hrnet_model
                    self.models_loading_status['hrnet'] = True
                    loaded_count += 1
                    self.logger.info("âœ… HRNet ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ HRNet ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.models_loading_status['loading_errors'].append(f"HRNet: {e}")
        
        else:
            # í´ë°±: MediaPipeë§Œ ë¡œë”© ì‹œë„
            self.logger.warning("âš ï¸ ModelLoaderê°€ ì—†ìŒ - MediaPipeë§Œ ë¡œë”© ì‹œë„")
            try:
                mediapipe_model = MediaPoseModel()
                if mediapipe_model.load_model():
                    self.ai_models['mediapipe'] = mediapipe_model
                    self.models_loading_status['mediapipe'] = True
                    loaded_count += 1
            except Exception as e:
                self.logger.error(f"âŒ MediaPipe í´ë°± ë¡œë”©ë„ ì‹¤íŒ¨: {e}")
        
        self.models_loading_status['total_loaded'] = loaded_count
        self.pose_ready = loaded_count > 0
        
        if loaded_count > 0:
            self.logger.info(f"ğŸ‰ í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {loaded_count}ê°œ")
        else:
            self.logger.error("âŒ ëª¨ë“  í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        
        return loaded_count
    
    def _get_model_path_from_central_hub(self, model_name: str) -> Optional[Path]:
        """Central Hubë¥¼ í†µí•œ ëª¨ë¸ ê²½ë¡œ ì¡°íšŒ"""
        try:
            if self.model_loader and hasattr(self.model_loader, 'get_model_path'):
                path_str = self.model_loader.get_model_path(model_name, step_name=self.step_name)
                if path_str:
                    return Path(path_str)
            return None
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ê²½ë¡œ ì¡°íšŒ ì‹¤íŒ¨ ({model_name}): {e}")
            return None
    
    def process(self, **kwargs) -> Dict[str, Any]:
        """ğŸ”¥ PoseEstimationStep ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ (BaseStepMixin ì˜¤ë²„ë¼ì´ë“œ) - ë™ê¸° ë²„ì „"""
        try:
            start_time = time.time()
            
            # ì…ë ¥ ë°ì´í„° ë³€í™˜ (ë™ê¸°ì ìœ¼ë¡œ)
            if hasattr(self, 'convert_api_input_to_step_input'):
                processed_input = self.convert_api_input_to_step_input(kwargs)
            else:
                processed_input = kwargs
            
            # AI ì¶”ë¡  ì‹¤í–‰ (ë™ê¸°ì ìœ¼ë¡œ)
            result = self._run_ai_inference(processed_input)
            
            # ê²°ê³¼ íƒ€ì… í™•ì¸ ë° ë¡œê¹…
            self.logger.info(f"ğŸ” _run_ai_inference ë°˜í™˜ íƒ€ì…: {type(result)}")
            if isinstance(result, list):
                self.logger.warning(f"âš ï¸ _run_ai_inferenceê°€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•¨: {len(result)}ê°œ í•­ëª©")
                # ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                result = {
                    'success': True,
                    'data': result,
                    'step_name': self.step_name,
                    'step_id': self.step_id
                }
            
            # ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
            if isinstance(result, dict):
                result['processing_time'] = time.time() - start_time
                result['step_name'] = self.step_name
                result['step_id'] = self.step_id
            
            self.logger.info(f"ğŸ” process ìµœì¢… ë°˜í™˜ íƒ€ì…: {type(result)}")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} process ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                'step_name': self.step_name,
                'step_id': self.step_id
            }
    
    def _get_service_from_central_hub(self, service_key: str):
        """Central Hubì—ì„œ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if hasattr(self, 'di_container') and self.di_container:
                return self.di_container.get_service(service_key)
            return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ Central Hub ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def convert_api_input_to_step_input(self, api_input: Dict[str, Any]) -> Dict[str, Any]:
        """API ì…ë ¥ì„ Step ì…ë ¥ìœ¼ë¡œ ë³€í™˜ (ë™ê¸° ë²„ì „)"""
        try:
            step_input = api_input.copy()
            
            # ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ (ë‹¤ì–‘í•œ í‚¤ ì´ë¦„ ì§€ì›)
            image = None
            for key in ['image', 'person_image', 'input_image', 'original_image']:
                if key in step_input:
                    image = step_input[key]
                    break
            
            if image is None and 'session_id' in step_input:
                # ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ (ë™ê¸°ì ìœ¼ë¡œ)
                try:
                    session_manager = self._get_service_from_central_hub('session_manager')
                    if session_manager:
                        person_image, clothing_image = None, None
                        
                        try:
                            # ì„¸ì…˜ ë§¤ë‹ˆì €ê°€ ë™ê¸° ë©”ì„œë“œë¥¼ ì œê³µí•˜ëŠ”ì§€ í™•ì¸
                            if hasattr(session_manager, 'get_session_images_sync'):
                                person_image, clothing_image = session_manager.get_session_images_sync(step_input['session_id'])
                            elif hasattr(session_manager, 'get_session_images'):
                                # ë¹„ë™ê¸° ë©”ì„œë“œë¥¼ ë™ê¸°ì ìœ¼ë¡œ í˜¸ì¶œ
                                import asyncio
                                import concurrent.futures
                                
                                def run_async_session_load():
                                    try:
                                        return asyncio.run(session_manager.get_session_images(step_input['session_id']))
                                    except Exception as async_error:
                                        self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨: {async_error}")
                                        return None, None
                                
                                try:
                                    with concurrent.futures.ThreadPoolExecutor() as executor:
                                        future = executor.submit(run_async_session_load)
                                        person_image, clothing_image = future.result(timeout=10)
                                except Exception as executor_error:
                                    self.logger.warning(f"âš ï¸ ì„¸ì…˜ ë¡œë“œ ThreadPoolExecutor ì‹¤íŒ¨: {executor_error}")
                                    person_image, clothing_image = None, None
                            else:
                                self.logger.warning("âš ï¸ ì„¸ì…˜ ë§¤ë‹ˆì €ì— ì ì ˆí•œ ë©”ì„œë“œê°€ ì—†ìŒ")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ì„¸ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
                            person_image, clothing_image = None, None
                        
                        if person_image:
                            image = person_image
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ë³€í™˜ëœ ì…ë ¥ êµ¬ì„±
            converted_input = {
                'image': image,
                'person_image': image,
                'session_id': step_input.get('session_id'),
                'detection_confidence': step_input.get('detection_confidence', 0.5),
                'clothing_type': step_input.get('clothing_type', 'shirt')
            }
            
            self.logger.info(f"âœ… API ì…ë ¥ ë³€í™˜ ì™„ë£Œ: {len(converted_input)}ê°œ í‚¤")
            return converted_input
            
        except Exception as e:
            self.logger.error(f"âŒ API ì…ë ¥ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return api_input
    
    async def initialize(self):
        """Step ì´ˆê¸°í™” (BaseStepMixin í˜¸í™˜)"""
        try:
            if self.is_initialized:
                return True
            
            self.logger.info(f"ğŸ”„ {self.step_name} ì´ˆê¸°í™” ì‹œì‘...")
            
            # Pose ëª¨ë¸ë“¤ ë¡œë”©
            loaded_count = self._load_pose_models_via_central_hub()
            
            if loaded_count == 0:
                self.logger.error("âŒ í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False
            
            # ì´ˆê¸°í™” ì™„ë£Œ
            self.is_initialized = True
            self.is_ready = True
            self.logger.info(f"âœ… {self.step_name} ì´ˆê¸°í™” ì™„ë£Œ ({loaded_count}ê°œ ëª¨ë¸)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _run_ai_inference(self, processed_input: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ”¥ ì‹¤ì œ Pose Estimation AI ì¶”ë¡  (BaseStepMixin v20.0 í˜¸í™˜)"""
        try:
            start_time = time.time()
            
            # ğŸ”¥ ë””ë²„ê¹…: ì…ë ¥ ë°ì´í„° ìƒì„¸ ë¡œê¹…
            self.logger.info(f"ğŸ” [DEBUG] Pose Estimation ì…ë ¥ ë°ì´í„° í‚¤ë“¤: {list(processed_input.keys())}")
            self.logger.info(f"ğŸ” [DEBUG] Pose Estimation ì…ë ¥ ë°ì´í„° íƒ€ì…ë“¤: {[(k, type(v).__name__) for k, v in processed_input.items()]}")
            
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            if not processed_input:
                self.logger.error("âŒ [DEBUG] Pose Estimation ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                raise ValueError("ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            self.logger.info(f"âœ… [DEBUG] Pose Estimation ì…ë ¥ ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
            
            # ğŸ”¥ Sessionì—ì„œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë¨¼ì € ê°€ì ¸ì˜¤ê¸°
            image = None
            if 'session_id' in processed_input:
                try:
                    session_manager = self._get_service_from_central_hub('session_manager')
                    if session_manager:
                        person_image, clothing_image = None, None
                        
                        try:
                            # ì„¸ì…˜ ë§¤ë‹ˆì €ê°€ ë™ê¸° ë©”ì„œë“œë¥¼ ì œê³µí•˜ëŠ”ì§€ í™•ì¸
                            if hasattr(session_manager, 'get_session_images_sync'):
                                person_image, clothing_image = session_manager.get_session_images_sync(processed_input['session_id'])
                            elif hasattr(session_manager, 'get_session_images'):
                                # ë¹„ë™ê¸° ë©”ì„œë“œë¥¼ ë™ê¸°ì ìœ¼ë¡œ í˜¸ì¶œ
                                import asyncio
                                import concurrent.futures
                                
                                def run_async_session_load():
                                    try:
                                        return asyncio.run(session_manager.get_session_images(processed_input['session_id']))
                                    except Exception as async_error:
                                        self.logger.warning(f"âš ï¸ ë¹„ë™ê¸° ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨: {async_error}")
                                        return None, None
                                
                                try:
                                    with concurrent.futures.ThreadPoolExecutor() as executor:
                                        future = executor.submit(run_async_session_load)
                                        person_image, clothing_image = future.result(timeout=10)
                                except Exception as executor_error:
                                    self.logger.warning(f"âš ï¸ ì„¸ì…˜ ë¡œë“œ ThreadPoolExecutor ì‹¤íŒ¨: {executor_error}")
                                    person_image, clothing_image = None, None
                            else:
                                self.logger.warning("âš ï¸ ì„¸ì…˜ ë§¤ë‹ˆì €ì— ì ì ˆí•œ ë©”ì„œë“œê°€ ì—†ìŒ")
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ ì„¸ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
                            person_image, clothing_image = None, None
                        image = person_image  # í¬ì¦ˆ ì¶”ì •ì€ ì‚¬ëŒ ì´ë¯¸ì§€ ì‚¬ìš©
                        self.logger.info(f"âœ… Sessionì—ì„œ ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ: {type(image)}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ sessionì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ğŸ”¥ ì…ë ¥ ë°ì´í„° ê²€ì¦ (Step 1ê³¼ ë™ì¼í•œ íŒ¨í„´)
            self.logger.debug(f"ğŸ” ì…ë ¥ ë°ì´í„° í‚¤ë“¤: {list(processed_input.keys())}")
            
            # ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ (ë‹¤ì–‘í•œ í‚¤ì—ì„œ ì‹œë„) - Sessionì—ì„œ ê°€ì ¸ì˜¤ì§€ ëª»í•œ ê²½ìš°
            if image is None:
                for key in ['image', 'input_image', 'original_image', 'processed_image']:
                    if key in processed_input:
                        image = processed_input[key]
                        self.logger.info(f"âœ… ì´ë¯¸ì§€ ë°ì´í„° ë°œê²¬: {key}")
                        break
            
            if image is None:
                self.logger.error("âŒ ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: ì…ë ¥ ì´ë¯¸ì§€ ì—†ìŒ (Step 2)")
                return {'success': False, 'error': 'ì…ë ¥ ì´ë¯¸ì§€ ì—†ìŒ'}
            
            self.logger.info("ğŸ§  Pose Estimation ì‹¤ì œ AI ì¶”ë¡  ì‹œì‘")
            
            # ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì€ ê²½ìš° ì´ˆê¸°í™” ì‹œë„
            if not self.pose_ready:
                self.logger.warning("âš ï¸ í¬ì¦ˆ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ - ì¬ë¡œë”© ì‹œë„")
                loaded = self._load_pose_models_via_central_hub()
                if loaded == 0:
                    raise RuntimeError("í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            
            # ë‹¤ì¤‘ ëª¨ë¸ë¡œ í¬ì¦ˆ ì¶”ì • ì‹œë„ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
            best_result = None
            best_confidence = 0.0
            
            for model_type in self.model_priority:
                model_key = model_type.value
                
                if model_key in self.ai_models:
                    try:
                        self.logger.debug(f"ğŸ”„ {model_key} ëª¨ë¸ë¡œ í¬ì¦ˆ ì¶”ì • ì‹œë„")
                        result = self.ai_models[model_key].detect_poses(image)
                        
                        if result.get('success') and result.get('keypoints'):
                            confidence = result.get('confidence', 0.0)
                            
                            # ìµœê³  ì‹ ë¢°ë„ ê²°ê³¼ ì„ íƒ
                            if confidence > best_confidence:
                                best_result = result
                                best_confidence = confidence
                                best_result['primary_model'] = model_key
                            
                            self.logger.debug(f"âœ… {model_key} ì„±ê³µ (ì‹ ë¢°ë„: {confidence:.3f})")
                            
                        else:
                            self.logger.debug(f"âš ï¸ {model_key} ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
                            
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {model_key} ì¶”ë¡  ì‹¤íŒ¨: {e}")
                        continue
            
            if not best_result or not best_result.get('keypoints'):
                raise RuntimeError("ëª¨ë“  í¬ì¦ˆ ëª¨ë¸ì—ì„œ ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ë¥¼ ê²€ì¶œí•˜ì§€ ëª»í•¨")
            
            # í‚¤í¬ì¸íŠ¸ í›„ì²˜ë¦¬ ë° ë¶„ì„
            keypoints = best_result['keypoints']
            
            # keypointsê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸í•˜ê³  ë”•ì…”ë„ˆë¦¬ë¡œ ê°ì‹¸ê¸°
            if isinstance(keypoints, list):
                self.logger.info(f"âœ… keypointsê°€ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ë¨: {len(keypoints)}ê°œ í‚¤í¬ì¸íŠ¸")
            else:
                self.logger.warning(f"âš ï¸ keypointsê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: {type(keypoints)}")
                keypoints = []
            
            # ê´€ì ˆ ê°ë„ ê³„ì‚°
            joint_angles = self.analyzer.calculate_joint_angles(keypoints)
            
            # ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°
            body_proportions = self.analyzer.calculate_body_proportions(keypoints)
            
            # í¬ì¦ˆ í’ˆì§ˆ í‰ê°€
            quality_assessment = self.analyzer.assess_pose_quality(
                keypoints, joint_angles, body_proportions
            )
            
            inference_time = time.time() - start_time
            
            # ë”•ì…”ë„ˆë¦¬ë¡œ ê°ì‹¸ì„œ ë°˜í™˜
            result_dict = {
                'success': True,
                'keypoints': keypoints,
                'confidence_scores': [kp[2] for kp in keypoints] if keypoints else [],
                'joint_angles': joint_angles,
                'body_proportions': body_proportions,
                'pose_quality': quality_assessment['overall_score'],
                'quality_grade': quality_assessment['quality_grade'].value,
                'processing_time': inference_time,
                'model_used': best_result.get('primary_model', 'unknown'),
                'real_ai_inference': True,
                'pose_estimation_ready': True,
                'num_keypoints_detected': len([kp for kp in keypoints if kp[2] > 0.3]),
                
                # ê³ ê¸‰ ë¶„ì„ ê²°ê³¼
                'detailed_scores': quality_assessment.get('detailed_scores', {}),
                'pose_recommendations': quality_assessment.get('recommendations', []),
                'skeleton_structure': self._build_skeleton_structure(keypoints),
                'landmarks': self._extract_landmarks(keypoints)
            }
            
            self.logger.info(f"âœ… Pose Estimation ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜: {len(result_dict)}ê°œ í‚¤")
            return result_dict
            
        except Exception as e:
            self.logger.error(f"âŒ Pose Estimation AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'keypoints': [],
                'confidence_scores': [],
                'pose_quality': 0.0,
                'processing_time': time.time() - start_time if 'start_time' in locals() else 0.0,
                'model_used': 'error',
                'real_ai_inference': False,
                'pose_estimation_ready': False
            }
    
    def _build_skeleton_structure(self, keypoints: List[List[float]]) -> Dict[str, Any]:
        """ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±"""
        skeleton = {
            'connections': [],
            'bone_lengths': {},
            'valid_connections': 0
        }
        
        # COCO 17 ì—°ê²° êµ¬ì¡°
        coco_connections = [
            (0, 1), (0, 2), (1, 3), (2, 4),  # ë¨¸ë¦¬
            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # íŒ”
            (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)  # ë‹¤ë¦¬
        ]
        
        for i, (start_idx, end_idx) in enumerate(coco_connections):
            if (start_idx < len(keypoints) and end_idx < len(keypoints) and
                len(keypoints[start_idx]) >= 3 and len(keypoints[end_idx]) >= 3):
                
                start_kp = keypoints[start_idx]
                end_kp = keypoints[end_idx]
                
                if start_kp[2] > self.confidence_threshold and end_kp[2] > self.confidence_threshold:
                    bone_length = np.sqrt(
                        (start_kp[0] - end_kp[0])**2 + (start_kp[1] - end_kp[1])**2
                    )
                    
                    connection = {
                        'start': start_idx,
                        'end': end_idx,
                        'start_name': COCO_17_KEYPOINTS[start_idx] if start_idx < len(COCO_17_KEYPOINTS) else f"point_{start_idx}",
                        'end_name': COCO_17_KEYPOINTS[end_idx] if end_idx < len(COCO_17_KEYPOINTS) else f"point_{end_idx}",
                        'length': bone_length,
                        'confidence': (start_kp[2] + end_kp[2]) / 2
                    }
                    
                    skeleton['connections'].append(connection)
                    skeleton['bone_lengths'][f"{start_idx}_{end_idx}"] = bone_length
                    skeleton['valid_connections'] += 1
        
        return skeleton
    
    def _extract_landmarks(self, keypoints: List[List[float]]) -> Dict[str, Dict[str, float]]:
        """ì£¼ìš” ëœë“œë§ˆí¬ ì¶”ì¶œ"""
        landmarks = {}
        
        for i, kp in enumerate(keypoints):
            if len(kp) >= 3 and kp[2] > self.confidence_threshold:
                landmark_name = COCO_17_KEYPOINTS[i] if i < len(COCO_17_KEYPOINTS) else f"landmark_{i}"
                landmarks[landmark_name] = {
                    'x': float(kp[0]),
                    'y': float(kp[1]),
                    'confidence': float(kp[2])
                }
        
        return landmarks
    
    # ==============================================
    # ğŸ”¥ BaseStepMixin í˜¸í™˜ ë©”ì„œë“œë“¤
    # ==============================================
    
    def set_model_loader(self, model_loader):
        """ModelLoader ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
        try:
            self.model_loader = model_loader
            self.logger.info("âœ… ModelLoader ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
            
            # Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹œë„
            if hasattr(model_loader, 'create_step_interface'):
                try:
                    self.model_interface = model_loader.create_step_interface(self.step_name)
                    self.logger.info("âœ… Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì£¼ì… ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Step ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨, ModelLoader ì§ì ‘ ì‚¬ìš©: {e}")
                    self.model_interface = model_loader
            else:
                self.model_interface = model_loader
                
        except Exception as e:
            self.logger.error(f"âŒ ModelLoader ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
            self.model_loader = None
            self.model_interface = None
            
    def set_memory_manager(self, memory_manager):
        """MemoryManager ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
        try:
            self.memory_manager = memory_manager
            self.logger.info("âœ… MemoryManager ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ MemoryManager ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_data_converter(self, data_converter):
        """DataConverter ì˜ì¡´ì„± ì£¼ì… (BaseStepMixin í˜¸í™˜)"""
        try:
            self.data_converter = data_converter
            self.logger.info("âœ… DataConverter ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DataConverter ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def set_di_container(self, di_container):
        """DI Container ì˜ì¡´ì„± ì£¼ì…"""
        try:
            self.di_container = di_container
            self.logger.info("âœ… DI Container ì˜ì¡´ì„± ì£¼ì… ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ DI Container ì˜ì¡´ì„± ì£¼ì… ì‹¤íŒ¨: {e}")
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.logger.info(f"ğŸ”„ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
            
            # AI ëª¨ë¸ë“¤ ì •ë¦¬
            for model_name, model in self.ai_models.items():
                try:
                    if hasattr(model, 'cleanup'):
                        model.cleanup()
                    del model
                except Exception as e:
                    self.logger.debug(f"ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨ ({model_name}): {e}")
            
            # ìºì‹œ ì •ë¦¬
            self.ai_models.clear()
            self.pose_models.clear()
            self.keypoints_cache.clear()
            
            # ğŸ”¥ 128GB M3 Max ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
            if TORCH_AVAILABLE:
                try:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    elif hasattr(torch, 'mps') and torch.mps.is_available():
                        torch.mps.empty_cache()
                        if hasattr(torch.mps, 'synchronize'):
                            torch.mps.synchronize()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            for _ in range(3):
                gc.collect()
            
            self.logger.info(f"âœ… {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_model_status(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        return {
            'step_name': self.step_name,
            'step_id': self.step_id,
            'pose_ready': self.pose_ready,
            'models_loading_status': self.models_loading_status,
            'loaded_models': list(self.ai_models.keys()),
            'model_priority': [model.value for model in self.model_priority],
            'confidence_threshold': self.confidence_threshold,
            'use_subpixel': self.use_subpixel
        }

    def _convert_step_output_type(self, step_output: Dict[str, Any], *args, **kwargs) -> Dict[str, Any]:
        """Step ì¶œë ¥ì„ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            if not isinstance(step_output, dict):
                self.logger.warning(f"âš ï¸ step_outputì´ dictê°€ ì•„ë‹˜: {type(step_output)}")
                return {
                    'success': False,
                    'error': f'Invalid output type: {type(step_output)}',
                    'step_name': self.step_name,
                    'step_id': self.step_id
                }
            
            # ê¸°ë³¸ API ì‘ë‹µ êµ¬ì¡°
            api_response = {
                'success': step_output.get('success', True),
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': step_output.get('processing_time', 0.0),
                'timestamp': time.time()
            }
            
            # ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš°
            if not api_response['success']:
                api_response['error'] = step_output.get('error', 'Unknown error')
                return api_response
            
            # í¬ì¦ˆ ì¶”ì • ê²°ê³¼ ë³€í™˜ (ì§ì ‘ í‚¤í¬ì¸íŠ¸ ë°ì´í„° ì‚¬ìš©)
            api_response['pose_data'] = {
                'keypoints': step_output.get('keypoints', []),
                'confidence_scores': step_output.get('confidence_scores', []),
                'overall_confidence': step_output.get('pose_quality', 0.0),
                'pose_quality': step_output.get('quality_grade', 'unknown'),
                'model_used': step_output.get('model_used', 'unknown'),
                'joint_angles': step_output.get('joint_angles', {}),
                'body_proportions': step_output.get('body_proportions', {}),
                'skeleton_structure': step_output.get('skeleton_structure', {}),
                'landmarks': step_output.get('landmarks', {}),
                'num_keypoints_detected': step_output.get('num_keypoints_detected', 0),
                'detailed_scores': step_output.get('detailed_scores', {}),
                'pose_recommendations': step_output.get('pose_recommendations', [])
            }
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            api_response['metadata'] = {
                'models_available': list(self.pose_models.keys()) if hasattr(self, 'pose_models') else [],
                'device_used': getattr(self, 'device', 'unknown'),
                'input_size': step_output.get('input_size', [0, 0]),
                'output_size': step_output.get('output_size', [0, 0]),
                'real_ai_inference': step_output.get('real_ai_inference', False),
                'pose_estimation_ready': step_output.get('pose_estimation_ready', False)
            }
            
            # ì‹œê°í™” ë°ì´í„° (ìˆëŠ” ê²½ìš°)
            if 'visualization' in step_output:
                api_response['visualization'] = step_output['visualization']
            
            # ë¶„ì„ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
            if 'analysis' in step_output:
                api_response['analysis'] = step_output['analysis']
            
            self.logger.info(f"âœ… PoseEstimationStep ì¶œë ¥ ë³€í™˜ ì™„ë£Œ: {len(api_response)}ê°œ í‚¤")
            return api_response
            
        except Exception as e:
            self.logger.error(f"âŒ PoseEstimationStep ì¶œë ¥ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': f'Output conversion failed: {str(e)}',
                'step_name': self.step_name,
                'step_id': self.step_id,
                'processing_time': step_output.get('processing_time', 0.0) if isinstance(step_output, dict) else 0.0
            }

# ==============================================
# ğŸ”¥ 6. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==============================================

def validate_keypoints(keypoints: List[List[float]]) -> bool:
    """í‚¤í¬ì¸íŠ¸ ìœ íš¨ì„± ê²€ì¦"""
    try:
        if not keypoints:
            return False
        
        for kp in keypoints:
            if len(kp) < 3:
                return False
            if not all(isinstance(x, (int, float)) for x in kp):
                return False
            if kp[2] < 0 or kp[2] > 1:
                return False
        
        return True
        
    except Exception:
        return False

def draw_pose_on_image(
    image: Union[np.ndarray, Image.Image],
    keypoints: List[List[float]],
    confidence_threshold: float = 0.5,
    keypoint_size: int = 4,
    line_width: int = 3
) -> Image.Image:
    """ì´ë¯¸ì§€ì— í¬ì¦ˆ ê·¸ë¦¬ê¸°"""
    try:
        if isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        else:
            pil_image = image.copy()
        
        draw = ImageDraw.Draw(pil_image)
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for i, kp in enumerate(keypoints):
            if len(kp) >= 3 and kp[2] > confidence_threshold:
                x, y = int(kp[0]), int(kp[1])
                color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                
                radius = int(keypoint_size + kp[2] * 6)
                draw.ellipse([x-radius, y-radius, x+radius, y+radius], 
                           fill=color, outline=(255, 255, 255), width=2)
        
        # ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸° (COCO 17 ì—°ê²° êµ¬ì¡°)
        coco_connections = [
            (0, 1), (0, 2), (1, 3), (2, 4),  # ë¨¸ë¦¬
            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # íŒ”
            (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)  # ë‹¤ë¦¬
        ]
        
        for i, (start_idx, end_idx) in enumerate(coco_connections):
            if (start_idx < len(keypoints) and end_idx < len(keypoints)):
                start_kp = keypoints[start_idx]
                end_kp = keypoints[end_idx]
                
                if (len(start_kp) >= 3 and len(end_kp) >= 3 and
                    start_kp[2] > confidence_threshold and end_kp[2] > confidence_threshold):
                    
                    start_point = (int(start_kp[0]), int(start_kp[1]))
                    end_point = (int(end_kp[0]), int(end_kp[1]))
                    color = KEYPOINT_COLORS[i % len(KEYPOINT_COLORS)]
                    
                    avg_confidence = (start_kp[2] + end_kp[2]) / 2
                    adjusted_width = int(line_width * avg_confidence)
                    
                    draw.line([start_point, end_point], fill=color, width=max(1, adjusted_width))
        
        return pil_image
        
    except Exception as e:
        logger.error(f"í¬ì¦ˆ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
        return image if isinstance(image, Image.Image) else Image.fromarray(image)

def analyze_pose_for_clothing_advanced(
    keypoints: List[List[float]],
    clothing_type: str = "default",
    confidence_threshold: float = 0.5,
    detailed_analysis: bool = True
) -> Dict[str, Any]:
    """ê³ ê¸‰ ì˜ë¥˜ë³„ í¬ì¦ˆ ì í•©ì„± ë¶„ì„"""
    try:
        if not keypoints:
            return {
                'suitable_for_fitting': False,
                'issues': ["í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"],
                'recommendations': ["ë” ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”"],
                'pose_score': 0.0,
                'detailed_analysis': {}
            }
        
        # ì˜ë¥˜ë³„ ì„¸ë¶€ ê°€ì¤‘ì¹˜
        clothing_detailed_weights = {
            'shirt': {
                'critical_keypoints': [5, 6, 7, 8, 9, 10],  # ì–´ê¹¨, íŒ”ê¿ˆì¹˜, ì†ëª©
                'weights': {'arms': 0.4, 'torso': 0.4, 'posture': 0.2},
                'min_visibility': 0.7,
                'required_angles': ['left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow']
            },
            'dress': {
                'critical_keypoints': [5, 6, 11, 12, 13, 14],  # ì–´ê¹¨, ì—‰ë©ì´, ë¬´ë¦
                'weights': {'torso': 0.5, 'arms': 0.2, 'legs': 0.2, 'posture': 0.1},
                'min_visibility': 0.8,
                'required_angles': ['spine_curvature']
            },
            'pants': {
                'critical_keypoints': [11, 12, 13, 14, 15, 16],  # ì—‰ë©ì´, ë¬´ë¦, ë°œëª©
                'weights': {'legs': 0.6, 'torso': 0.3, 'posture': 0.1},
                'min_visibility': 0.8,
                'required_angles': ['left_hip', 'right_hip', 'left_knee', 'right_knee']
            },
            'jacket': {
                'critical_keypoints': [5, 6, 7, 8, 9, 10, 11, 12],  # ìƒì²´ ì „ì²´
                'weights': {'arms': 0.4, 'torso': 0.4, 'shoulders': 0.2},
                'min_visibility': 0.75,
                'required_angles': ['left_shoulder', 'right_shoulder', 'spine_curvature']
            },
            'suit': {
                'critical_keypoints': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14],  # ê±°ì˜ ì „ì‹ 
                'weights': {'torso': 0.3, 'arms': 0.3, 'legs': 0.2, 'posture': 0.2},
                'min_visibility': 0.85,
                'required_angles': ['spine_curvature', 'left_shoulder', 'right_shoulder']
            },
            'default': {
                'critical_keypoints': [0, 5, 6, 11, 12],  # ê¸°ë³¸ í•µì‹¬ ë¶€ìœ„
                'weights': {'torso': 0.4, 'arms': 0.3, 'legs': 0.2, 'visibility': 0.1},
                'min_visibility': 0.6,
                'required_angles': []
            }
        }
        
        config = clothing_detailed_weights.get(clothing_type, clothing_detailed_weights['default'])
        
        # 1. í•µì‹¬ í‚¤í¬ì¸íŠ¸ ê°€ì‹œì„± ê²€ì‚¬
        critical_keypoints = config['critical_keypoints']
        visible_critical = sum(1 for idx in critical_keypoints 
                             if idx < len(keypoints) and len(keypoints[idx]) >= 3 
                             and keypoints[idx][2] > confidence_threshold)
        
        critical_visibility = visible_critical / len(critical_keypoints)
        
        # 2. ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°
        def calculate_body_part_score_advanced(part_indices: List[int]) -> Dict[str, float]:
            visible_count = 0
            total_confidence = 0.0
            position_quality = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
                        
                        # ìœ„ì¹˜ í’ˆì§ˆ í‰ê°€ (í™”ë©´ ê²½ê³„ì—ì„œì˜ ê±°ë¦¬)
                        x, y = keypoints[idx][0], keypoints[idx][1]
                        # ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ëª¨ë¥´ë¯€ë¡œ ìƒëŒ€ì  í‰ê°€
                        if 0.1 <= x <= 0.9 and 0.1 <= y <= 0.9:  # ì¤‘ì•™ 80% ì˜ì—­
                            position_quality += 1.0
                        else:
                            position_quality += 0.5
            
            if visible_count == 0:
                return {'visibility': 0.0, 'confidence': 0.0, 'position': 0.0, 'combined': 0.0}
            
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            avg_position = position_quality / visible_count
            combined_score = (visibility_ratio * 0.4 + avg_confidence * 0.4 + avg_position * 0.2)
            
            return {
                'visibility': visibility_ratio,
                'confidence': avg_confidence,
                'position': avg_position,
                'combined': combined_score
            }
        
        # COCO 17 ë¶€ìœ„ë³„ ì¸ë±ìŠ¤ (ê³ ê¸‰)
        body_parts = {
            'head': [0, 1, 2, 3, 4],  # ì½”, ëˆˆë“¤, ê·€ë“¤
            'torso': [5, 6, 11, 12],  # ì–´ê¹¨ë“¤, ì—‰ë©ì´ë“¤
            'arms': [5, 6, 7, 8, 9, 10],  # ì–´ê¹¨, íŒ”ê¿ˆì¹˜, ì†ëª©
            'legs': [11, 12, 13, 14, 15, 16],  # ì—‰ë©ì´, ë¬´ë¦, ë°œëª©
            'left_arm': [5, 7, 9],
            'right_arm': [6, 8, 10],
            'left_leg': [11, 13, 15],
            'right_leg': [12, 14, 16]
        }
        
        part_scores = {}
        for part_name, indices in body_parts.items():
            part_scores[part_name] = calculate_body_part_score_advanced(indices)
        
        # 3. ê´€ì ˆ ê°ë„ ë¶„ì„
        analyzer = PoseAnalyzer()
        joint_angles = analyzer.calculate_joint_angles(keypoints)
        
        angle_score = 1.0
        missing_angles = []
        for required_angle in config.get('required_angles', []):
            if required_angle not in joint_angles:
                missing_angles.append(required_angle)
                angle_score *= 0.8  # í•„ìˆ˜ ê°ë„ ì—†ì„ ë•Œë§ˆë‹¤ 20% ê°ì 
        
        # 4. ìì„¸ ì•ˆì •ì„± í‰ê°€
        posture_stability = analyze_posture_stability(keypoints)
        
        # 5. ì˜ë¥˜ë³„ íŠ¹í™” ë¶„ì„
        clothing_specific_score = analyze_clothing_specific_requirements(
            keypoints, clothing_type, joint_angles
        )
        
        # 6. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        weights = config['weights']
        
        # ê¸°ë³¸ ì ìˆ˜ë“¤
        torso_score = part_scores.get('torso', {}).get('combined', 0.0)
        arms_score = part_scores.get('arms', {}).get('combined', 0.0)
        legs_score = part_scores.get('legs', {}).get('combined', 0.0)
        
        # ê°€ì¤‘í‰ê· 
        pose_score = (
            torso_score * weights.get('torso', 0.4) +
            arms_score * weights.get('arms', 0.3) +
            legs_score * weights.get('legs', 0.2) +
            posture_stability * weights.get('posture', 0.1) +
            clothing_specific_score * 0.1
        )
        
        # 7. ì í•©ì„± íŒë‹¨
        min_visibility = config.get('min_visibility', 0.7)
        suitable_for_fitting = (
            pose_score >= 0.7 and 
            critical_visibility >= min_visibility and
            angle_score >= 0.6
        )
        
        # 8. ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
        issues = []
        recommendations = []
        
        if not suitable_for_fitting:
            if critical_visibility < min_visibility:
                issues.append(f'{clothing_type} í”¼íŒ…ì— í•„ìš”í•œ ì‹ ì²´ ë¶€ìœ„ê°€ ì¶©ë¶„íˆ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤')
                recommendations.append('í•µì‹¬ ì‹ ì²´ ë¶€ìœ„ê°€ ëª¨ë‘ ë³´ì´ë„ë¡ ìì„¸ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”')
            
            if pose_score < 0.7:
                issues.append(f'{clothing_type} ì°©ìš© ì‹œë®¬ë ˆì´ì…˜ì— ì í•©í•˜ì§€ ì•Šì€ í¬ì¦ˆì…ë‹ˆë‹¤')
                recommendations.append('ë” ìì—°ìŠ¤ëŸ½ê³  ì •ë©´ì„ í–¥í•œ ìì„¸ë¡œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            if missing_angles:
                issues.append(f'í•„ìš”í•œ ê´€ì ˆ ê°ë„ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {", ".join(missing_angles)}')
                recommendations.append('ê´€ì ˆ ë¶€ìœ„ê°€ ëª…í™•íˆ ë³´ì´ë„ë¡ ìì„¸ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”')
        
        # 9. ì„¸ë¶€ ë¶„ì„ ê²°ê³¼
        detailed_analysis_result = {
            'critical_visibility': critical_visibility,
            'part_scores': part_scores,
            'joint_angles': joint_angles,
            'angle_score': angle_score,
            'missing_angles': missing_angles,
            'posture_stability': posture_stability,
            'clothing_specific_score': clothing_specific_score,
            'min_visibility_threshold': min_visibility,
            'clothing_requirements': config
        } if detailed_analysis else {}
        
        return {
            'suitable_for_fitting': suitable_for_fitting,
            'issues': issues,
            'recommendations': recommendations,
            'pose_score': pose_score,
            'clothing_type': clothing_type,
            'detailed_analysis': detailed_analysis_result,
            'quality_metrics': {
                'overall_score': pose_score,
                'critical_visibility': critical_visibility,
                'angle_completeness': angle_score,
                'posture_stability': posture_stability,
                'clothing_compatibility': clothing_specific_score
            }
        }
        
    except Exception as e:
        logger.error(f"ê³ ê¸‰ ì˜ë¥˜ë³„ í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            'suitable_for_fitting': False,
            'issues': ["ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"],
            'recommendations': ["ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”"],
            'pose_score': 0.0,
            'error': str(e)
        }

def analyze_posture_stability(keypoints: List[List[float]]) -> float:
    """ìì„¸ ì•ˆì •ì„± ë¶„ì„"""
    try:
        if len(keypoints) < 17:
            return 0.0
        
        stability_score = 1.0
        
        # 1. ì¤‘ì‹¬ ì•ˆì •ì„± (ì–´ê¹¨ì™€ ì—‰ë©ì´ ì¤‘ì ì˜ ìˆ˜ì§ ì •ë ¬)
        if all(keypoints[i][2] > 0.3 for i in [5, 6, 11, 12]):
            shoulder_center_x = (keypoints[5][0] + keypoints[6][0]) / 2
            hip_center_x = (keypoints[11][0] + keypoints[12][0]) / 2
            
            lateral_offset = abs(shoulder_center_x - hip_center_x)
            body_width = abs(keypoints[5][0] - keypoints[6][0])
            
            if body_width > 0:
                offset_ratio = lateral_offset / body_width
                center_stability = max(0.0, 1.0 - offset_ratio)
                stability_score *= center_stability
        
        # 2. ë°œ ì§€ì§€ ì•ˆì •ì„±
        foot_support = 0.0
        if keypoints[15][2] > 0.3:  # ì™¼ë°œëª©
            foot_support += 0.5
        if keypoints[16][2] > 0.3:  # ì˜¤ë¥¸ë°œëª©
            foot_support += 0.5
        
        stability_score *= foot_support
        
        # 3. ê· í˜• ì•ˆì •ì„± (ì¢Œìš° ëŒ€ì¹­)
        balance_score = 1.0
        
        # ì–´ê¹¨ ê· í˜•
        if keypoints[5][2] > 0.3 and keypoints[6][2] > 0.3:
            shoulder_tilt = abs(keypoints[5][1] - keypoints[6][1])
            shoulder_width = abs(keypoints[5][0] - keypoints[6][0])
            if shoulder_width > 0:
                shoulder_balance = max(0.0, 1.0 - (shoulder_tilt / shoulder_width))
                balance_score *= shoulder_balance
        
        stability_score *= balance_score
        
        return min(1.0, max(0.0, stability_score))
        
    except Exception:
        return 0.0

def analyze_clothing_specific_requirements(
    keypoints: List[List[float]], 
    clothing_type: str, 
    joint_angles: Dict[str, float]
) -> float:
    """ì˜ë¥˜ë³„ íŠ¹í™” ìš”êµ¬ì‚¬í•­ ë¶„ì„"""
    try:
        specific_score = 1.0
        
        if clothing_type == 'shirt':
            # ì…”ì¸ : íŒ” ìì„¸ê°€ ì¤‘ìš”
            if 'left_elbow' in joint_angles and 'right_elbow' in joint_angles:
                # íŒ”ê¿ˆì¹˜ê°€ ë„ˆë¬´ êµ½í˜€ì ¸ ìˆìœ¼ë©´ ê°ì 
                avg_elbow_angle = (joint_angles['left_elbow'] + joint_angles['right_elbow']) / 2
                if avg_elbow_angle < 120:  # ë„ˆë¬´ ë§ì´ êµ½í˜€ì§
                    specific_score *= 0.8
            
            # ì–´ê¹¨ì„ ì´ ìˆ˜í‰ì¸ì§€ í™•ì¸
            if keypoints[5][2] > 0.3 and keypoints[6][2] > 0.3:
                shoulder_tilt = abs(keypoints[5][1] - keypoints[6][1])
                shoulder_width = abs(keypoints[5][0] - keypoints[6][0])
                if shoulder_width > 0 and (shoulder_tilt / shoulder_width) > 0.1:
                    specific_score *= 0.9
        
        elif clothing_type == 'dress':
            # ë“œë ˆìŠ¤: ì „ì²´ì ì¸ ìì„¸ì™€ ì‹¤ë£¨ì—£ì´ ì¤‘ìš”
            if 'spine_curvature' in joint_angles:
                # ì²™ì¶”ê°€ ë„ˆë¬´ êµ½ì–´ìˆìœ¼ë©´ ê°ì 
                if joint_angles['spine_curvature'] > 20:
                    specific_score *= 0.8
            
            # ë‹¤ë¦¬ê°€ ë„ˆë¬´ ë²Œì–´ì ¸ ìˆìœ¼ë©´ ê°ì 
            if all(keypoints[i][2] > 0.3 for i in [15, 16]):
                foot_distance = abs(keypoints[15][0] - keypoints[16][0])
                hip_width = abs(keypoints[11][0] - keypoints[12][0]) if keypoints[11][2] > 0.3 and keypoints[12][2] > 0.3 else 100
                if hip_width > 0 and (foot_distance / hip_width) > 1.5:
                    specific_score *= 0.9
        
        elif clothing_type == 'pants':
            # ë°”ì§€: ë‹¤ë¦¬ ìì„¸ì™€ í™ ë¼ì¸ì´ ì¤‘ìš”
            if 'left_knee' in joint_angles and 'right_knee' in joint_angles:
                # ë¬´ë¦ì´ ë„ˆë¬´ êµ½í˜€ì ¸ ìˆìœ¼ë©´ ê°ì 
                avg_knee_angle = (joint_angles['left_knee'] + joint_angles['right_knee']) / 2
                if avg_knee_angle < 150:  # ë„ˆë¬´ ë§ì´ êµ½í˜€ì§
                    specific_score *= 0.8
            
            # ì—‰ë©ì´ ë¼ì¸ì´ ìˆ˜í‰ì¸ì§€ í™•ì¸
            if keypoints[11][2] > 0.3 and keypoints[12][2] > 0.3:
                hip_tilt = abs(keypoints[11][1] - keypoints[12][1])
                hip_width = abs(keypoints[11][0] - keypoints[12][0])
                if hip_width > 0 and (hip_tilt / hip_width) > 0.1:
                    specific_score *= 0.9
        
        elif clothing_type == 'jacket':
            # ì¬í‚·: ì–´ê¹¨ì™€ íŒ”ì˜ ìì„¸ê°€ ë§¤ìš° ì¤‘ìš”
            if 'left_shoulder' in joint_angles and 'right_shoulder' in joint_angles:
                # ì–´ê¹¨ ê°ë„ê°€ ë„ˆë¬´ ê·¹ë‹¨ì ì´ë©´ ê°ì 
                for shoulder_angle in [joint_angles['left_shoulder'], joint_angles['right_shoulder']]:
                    if shoulder_angle < 30 or shoulder_angle > 150:
                        specific_score *= 0.8
                        break
        
        return min(1.0, max(0.0, specific_score))
        
    except Exception:
        return 0.5  # ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì¤‘ê°„ ì ìˆ˜

def analyze_pose_for_clothing(
    keypoints: List[List[float]],
    clothing_type: str = "default",
    confidence_threshold: float = 0.5
) -> Dict[str, Any]:
    """ì˜ë¥˜ë³„ í¬ì¦ˆ ì í•©ì„± ë¶„ì„ (ê¸°ë³¸ ë²„ì „)"""
    try:
        if not keypoints:
            return {
                'suitable_for_fitting': False,
                'issues': ["í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"],
                'recommendations': ["ë” ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”"],
                'pose_score': 0.0
            }
        
        # ì˜ë¥˜ë³„ ê°€ì¤‘ì¹˜
        clothing_weights = {
            'shirt': {'arms': 0.4, 'torso': 0.4, 'posture': 0.2},
            'dress': {'torso': 0.5, 'arms': 0.2, 'legs': 0.2, 'posture': 0.1},
            'pants': {'legs': 0.6, 'torso': 0.3, 'posture': 0.1},
            'jacket': {'arms': 0.4, 'torso': 0.4, 'shoulders': 0.2},
            'suit': {'torso': 0.3, 'arms': 0.3, 'legs': 0.2, 'posture': 0.2},
            'default': {'torso': 0.4, 'arms': 0.3, 'legs': 0.2, 'visibility': 0.1}
        }
        
        weights = clothing_weights.get(clothing_type, clothing_weights['default'])
        
        # ì‹ ì²´ ë¶€ìœ„ë³„ ì ìˆ˜ ê³„ì‚°
        def calculate_body_part_score(part_indices: List[int]) -> float:
            visible_count = 0
            total_confidence = 0.0
            
            for idx in part_indices:
                if idx < len(keypoints) and len(keypoints[idx]) >= 3:
                    if keypoints[idx][2] > confidence_threshold:
                        visible_count += 1
                        total_confidence += keypoints[idx][2]
            
            if visible_count == 0:
                return 0.0
            
            visibility_ratio = visible_count / len(part_indices)
            avg_confidence = total_confidence / visible_count
            
            return (visibility_ratio * 0.6 + avg_confidence * 0.4)
        
        # COCO 17 ë¶€ìœ„ë³„ ì¸ë±ìŠ¤
        body_parts = {
            'torso': [5, 6, 11, 12],  # ì–´ê¹¨ë“¤, ì—‰ë©ì´ë“¤
            'arms': [5, 6, 7, 8, 9, 10],  # ì–´ê¹¨, íŒ”ê¿ˆì¹˜, ì†ëª©
            'legs': [11, 12, 13, 14, 15, 16],  # ì—‰ë©ì´, ë¬´ë¦, ë°œëª©
            'shoulders': [5, 6],  # ì–´ê¹¨
            'visibility': list(range(17))  # ì „ì²´ í‚¤í¬ì¸íŠ¸
        }
        
        # ê° ë¶€ìœ„ ì ìˆ˜ ê³„ì‚°
        part_scores = {}
        for part_name, indices in body_parts.items():
            part_scores[part_name] = calculate_body_part_score(indices)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        pose_score = sum(
            part_scores.get(part, 0.0) * weight 
            for part, weight in weights.items()
        )
        
        # ì í•©ì„± íŒë‹¨
        suitable_for_fitting = pose_score >= 0.7
        
        # ì´ìŠˆ ë° ê¶Œì¥ì‚¬í•­
        issues = []
        recommendations = []
        
        if not suitable_for_fitting:
            issues.append(f'{clothing_type} ì°©ìš© ì‹œë®¬ë ˆì´ì…˜ì— ì í•©í•˜ì§€ ì•Šì€ í¬ì¦ˆì…ë‹ˆë‹¤')
            recommendations.append('ë” ìì—°ìŠ¤ëŸ½ê³  ì •ë©´ì„ í–¥í•œ ìì„¸ë¡œ ì´¬ì˜í•´ ì£¼ì„¸ìš”')
            
            if part_scores.get('torso', 0.0) < 0.6:
                issues.append('ìƒì²´ê°€ ì¶©ë¶„íˆ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤')
                recommendations.append('ìƒì²´ê°€ ëª…í™•íˆ ë³´ì´ë„ë¡ ìì„¸ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”')
            
            if part_scores.get('arms', 0.0) < 0.6 and clothing_type in ['shirt', 'jacket']:
                issues.append('íŒ” ë¶€ìœ„ê°€ ì¶©ë¶„íˆ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤')
                recommendations.append('íŒ”ì´ ëª…í™•íˆ ë³´ì´ë„ë¡ ìì„¸ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”')
            
            if part_scores.get('legs', 0.0) < 0.6 and clothing_type in ['pants', 'dress']:
                issues.append('ë‹¤ë¦¬ ë¶€ìœ„ê°€ ì¶©ë¶„íˆ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤')
                recommendations.append('ë‹¤ë¦¬ê°€ ëª…í™•íˆ ë³´ì´ë„ë¡ ìì„¸ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”')
        
        return {
            'suitable_for_fitting': suitable_for_fitting,
            'issues': issues,
            'recommendations': recommendations,
            'pose_score': pose_score,
            'clothing_type': clothing_type,
            'part_scores': part_scores
        }
        
    except Exception as e:
        logger.error(f"ì˜ë¥˜ë³„ í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            'suitable_for_fitting': False,
            'issues': ["ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"],
            'recommendations': ["ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”"],
            'pose_score': 0.0,
            'error': str(e)
        }

def convert_coco17_to_openpose18(coco_keypoints: List[List[float]]) -> List[List[float]]:
    """COCO 17 â†’ OpenPose 18 ë³€í™˜"""
    if len(coco_keypoints) < 17:
        return [[0.0, 0.0, 0.0] for _ in range(18)]
    
    openpose_keypoints = [[0.0, 0.0, 0.0] for _ in range(18)]
    
    # COCO 17 â†’ OpenPose 18 ë§¤í•‘
    coco_to_openpose = {
        0: 0,   # nose
        1: 15,  # left_eye â†’ right_eye
        2: 16,  # right_eye â†’ left_eye
        3: 17,  # left_ear â†’ right_ear
        4: 18,  # right_ear â†’ left_ear
        5: 5,   # left_shoulder
        6: 2,   # right_shoulder
        7: 6,   # left_elbow
        8: 3,   # right_elbow
        9: 7,   # left_wrist
        10: 4,  # right_wrist
        11: 12, # left_hip
        12: 9,  # right_hip
        13: 13, # left_knee
        14: 10, # right_knee
        15: 14, # left_ankle
        16: 11  # right_ankle
    }
    
    # neck ê³„ì‚° (ì–´ê¹¨ ì¤‘ì )
    if len(coco_keypoints) > 6:
        left_shoulder = coco_keypoints[5]
        right_shoulder = coco_keypoints[6]
        if left_shoulder[2] > 0.1 and right_shoulder[2] > 0.1:
            neck_x = (left_shoulder[0] + right_shoulder[0]) / 2
            neck_y = (left_shoulder[1] + right_shoulder[1]) / 2
            neck_conf = (left_shoulder[2] + right_shoulder[2]) / 2
            openpose_keypoints[1] = [float(neck_x), float(neck_y), float(neck_conf)]
    
    # middle_hip ê³„ì‚° (ì—‰ë©ì´ ì¤‘ì )
    if len(coco_keypoints) > 12:
        left_hip = coco_keypoints[11]
        right_hip = coco_keypoints[12]
        if left_hip[2] > 0.1 and right_hip[2] > 0.1:
            middle_hip_x = (left_hip[0] + right_hip[0]) / 2
            middle_hip_y = (left_hip[1] + right_hip[1]) / 2
            middle_hip_conf = (left_hip[2] + right_hip[2]) / 2
            openpose_keypoints[8] = [float(middle_hip_x), float(middle_hip_y), float(middle_hip_conf)]
    
    # ë‚˜ë¨¸ì§€ í‚¤í¬ì¸íŠ¸ ë§¤í•‘
    for coco_idx, openpose_idx in coco_to_openpose.items():
        if coco_idx < len(coco_keypoints) and openpose_idx < 18:
            openpose_keypoints[openpose_idx] = [
                float(coco_keypoints[coco_idx][0]),
                float(coco_keypoints[coco_idx][1]),
                float(coco_keypoints[coco_idx][2])
            ]
    
    return openpose_keypoints

# ==============================================
# ğŸ”¥ 7. Step ìƒì„± í•¨ìˆ˜ë“¤
# ==============================================

async def create_pose_estimation_step(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> PoseEstimationStep:
    """í¬ì¦ˆ ì¶”ì • Step ìƒì„± í•¨ìˆ˜"""
    try:
        device_param = None if device == "auto" else device
        
        if config is None:
            config = {}
        config.update(kwargs)
        config['production_ready'] = True
        
        step = PoseEstimationStep(device=device_param, config=config)
        
        initialization_success = await step.initialize()
        
        if not initialization_success:
            raise RuntimeError("í¬ì¦ˆ ì¶”ì • Step ì´ˆê¸°í™” ì‹¤íŒ¨")
        
        return step
        
    except Exception as e:
        logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • Step ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def create_pose_estimation_step_sync(
    device: str = "auto",
    config: Optional[Dict[str, Any]] = None,
    **kwargs
) -> PoseEstimationStep:
    """ë™ê¸°ì‹ í¬ì¦ˆ ì¶”ì • Step ìƒì„±"""
    try:
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            create_pose_estimation_step(device, config, **kwargs)
        )
    except Exception as e:
        logger.error(f"âŒ ë™ê¸°ì‹ í¬ì¦ˆ ì¶”ì • Step ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# ==============================================
# ğŸ”¥ 8. í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ==============================================

async def test_pose_estimation():
    """í¬ì¦ˆ ì¶”ì • í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ”¥ Pose Estimation Step í…ŒìŠ¤íŠ¸")
        print("=" * 80)
        
        # Step ìƒì„±
        step = await create_pose_estimation_step(
            device="auto",
            config={
                'confidence_threshold': 0.5,
                'use_subpixel': True,
                'production_ready': True
            }
        )
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
        test_image = Image.new('RGB', (512, 512), (128, 128, 128))
        
        print(f"ğŸ“‹ Step ì •ë³´:")
        status = step.get_model_status()
        print(f"   ğŸ¯ Step: {status['step_name']}")
        print(f"   ğŸ’ ì¤€ë¹„ ìƒíƒœ: {status['pose_ready']}")
        print(f"   ğŸ¤– ë¡œë”©ëœ ëª¨ë¸: {len(status['loaded_models'])}ê°œ")
        print(f"   ğŸ“‹ ëª¨ë¸ ëª©ë¡: {', '.join(status['loaded_models'])}")
        
        # ì‹¤ì œ AI ì¶”ë¡  í…ŒìŠ¤íŠ¸
        result = await step.process(image=test_image)
        
        if result['success']:
            print(f"âœ… í¬ì¦ˆ ì¶”ì • ì„±ê³µ")
            print(f"ğŸ¯ ê²€ì¶œëœ í‚¤í¬ì¸íŠ¸: {len(result.get('keypoints', []))}")
            print(f"ğŸ–ï¸ í¬ì¦ˆ í’ˆì§ˆ: {result.get('pose_quality', 0):.3f}")
            print(f"ğŸ† ì‚¬ìš©ëœ ëª¨ë¸: {result.get('model_used', 'unknown')}")
            print(f"âš¡ ì¶”ë¡  ì‹œê°„: {result.get('processing_time', 0):.3f}ì´ˆ")
            print(f"ğŸ” ì‹¤ì œ AI ì¶”ë¡ : {result.get('real_ai_inference', False)}")
        else:
            print(f"âŒ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
        
        await step.cleanup()
        print(f"ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def test_pose_algorithms():
    """í¬ì¦ˆ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ§  í¬ì¦ˆ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # ë”ë¯¸ COCO 17 í‚¤í¬ì¸íŠ¸
        keypoints = [
            [128, 50, 0.9],   # nose
            [120, 40, 0.8],   # left_eye
            [136, 40, 0.8],   # right_eye
            [115, 45, 0.7],   # left_ear
            [141, 45, 0.7],   # right_ear
            [100, 100, 0.7],  # left_shoulder
            [156, 100, 0.7],  # right_shoulder
            [80, 130, 0.6],   # left_elbow
            [176, 130, 0.6],  # right_elbow
            [60, 160, 0.5],   # left_wrist
            [196, 160, 0.5],  # right_wrist
            [108, 180, 0.7],  # left_hip
            [148, 180, 0.7],  # right_hip
            [98, 220, 0.6],   # left_knee
            [158, 220, 0.6],  # right_knee
            [88, 260, 0.5],   # left_ankle
            [168, 260, 0.5],  # right_ankle
        ]
        
        # ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸
        analyzer = PoseAnalyzer()
        
        # ê´€ì ˆ ê°ë„ ê³„ì‚°
        joint_angles = analyzer.calculate_joint_angles(keypoints)
        print(f"âœ… ê´€ì ˆ ê°ë„ ê³„ì‚°: {len(joint_angles)}ê°œ")
        
        # ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°
        body_proportions = analyzer.calculate_body_proportions(keypoints)
        print(f"âœ… ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°: {len(body_proportions)}ê°œ")
        
        # í¬ì¦ˆ í’ˆì§ˆ í‰ê°€
        quality = analyzer.assess_pose_quality(keypoints, joint_angles, body_proportions)
        print(f"âœ… í¬ì¦ˆ í’ˆì§ˆ í‰ê°€: {quality['quality_grade'].value}")
        print(f"   ì „ì²´ ì ìˆ˜: {quality['overall_score']:.3f}")
        
        # ì˜ë¥˜ ì í•©ì„± ë¶„ì„
        clothing_analysis = analyze_pose_for_clothing(keypoints, "shirt")
        print(f"âœ… ì˜ë¥˜ ì í•©ì„±: {clothing_analysis['suitable_for_fitting']}")
        print(f"   ì ìˆ˜: {clothing_analysis['pose_score']:.3f}")
        
        # ì´ë¯¸ì§€ ê·¸ë¦¬ê¸° í…ŒìŠ¤íŠ¸
        test_image = Image.new('RGB', (256, 256), (128, 128, 128))
        pose_image = draw_pose_on_image(test_image, keypoints)
        print(f"âœ… í¬ì¦ˆ ì‹œê°í™”: {pose_image.size}")
        
        # í‚¤í¬ì¸íŠ¸ ìœ íš¨ì„± ê²€ì¦
        is_valid = validate_keypoints(keypoints)
        print(f"âœ… í‚¤í¬ì¸íŠ¸ ìœ íš¨ì„±: {is_valid}")
        
        # COCO 17 â†’ OpenPose 18 ë³€í™˜
        openpose_kpts = convert_coco17_to_openpose18(keypoints)
        print(f"âœ… COCOâ†’OpenPose ë³€í™˜: {len(openpose_kpts)}ê°œ")
        
    except Exception as e:
        print(f"âŒ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# ==============================================
# ğŸ”¥ 9. ëª¨ë“ˆ ìµìŠ¤í¬íŠ¸
# ==============================================

__all__ = [
    # ë©”ì¸ í´ë˜ìŠ¤ë“¤
    'PoseEstimationStep',
    'MediaPoseModel',
    'YOLOv8PoseModel', 
    'OpenPoseModel',
    'HRNetModel',
    'PoseAnalyzer',
    
    # ë°ì´í„° êµ¬ì¡°
    'PoseResult',
    'PoseModel',
    'PoseQuality',
    
    # ìƒì„± í•¨ìˆ˜ë“¤
    'create_pose_estimation_step',
    'create_pose_estimation_step_sync',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'validate_keypoints',
    'draw_pose_on_image', 
    'analyze_pose_for_clothing',
    'convert_coco17_to_openpose18',
    
    # ìƒìˆ˜ë“¤
    'COCO_17_KEYPOINTS',
    'OPENPOSE_18_KEYPOINTS',
    'SKELETON_CONNECTIONS',
    'KEYPOINT_COLORS',
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
    'test_pose_estimation',
    'test_pose_algorithms'
]

# ==============================================
# ğŸ”¥ 10. ëª¨ë“ˆ ì´ˆê¸°í™” ë¡œê·¸
# ==============================================

logger.info("ğŸ”¥ Pose Estimation Step v7.0 - Central Hub DI Container ì™„ì „ ë¦¬íŒ©í† ë§ ì™„ë£Œ")
logger.info("âœ… Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
logger.info("âœ… BaseStepMixin ìƒì† íŒ¨í„´ (Human Parsing Stepê³¼ ë™ì¼)")
logger.info("âœ… MediaPipe Pose ëª¨ë¸ ì§€ì› (ìš°ì„ ìˆœìœ„ 1)")
logger.info("âœ… OpenPose ëª¨ë¸ ì§€ì› (í´ë°± ì˜µì…˜)")
logger.info("âœ… YOLOv8-Pose ëª¨ë¸ ì§€ì› (ì‹¤ì‹œê°„)")
logger.info("âœ… HRNet ëª¨ë¸ ì§€ì› (ê³ ì •ë°€)")
logger.info("âœ… 17ê°œ COCO keypoints ê°ì§€")
logger.info("âœ… confidence score ê³„ì‚°")
logger.info("âœ… Mock ëª¨ë¸ ì™„ì „ ì œê±°")
logger.info("âœ… ì‹¤ì œ AI ì¶”ë¡  ì‹¤í–‰")
logger.info("âœ… ë‹¤ì¤‘ ëª¨ë¸ í´ë°± ì‹œìŠ¤í…œ")

logger.info("ğŸ§  ì§€ì› AI ëª¨ë¸ë“¤:")
logger.info("   - MediaPipe Pose (ìš°ì„ ìˆœìœ„ 1, ì‹¤ì‹œê°„)")
logger.info("   - YOLOv8-Pose (ì‹¤ì‹œê°„, 6.2MB)")
logger.info("   - OpenPose (ì •ë°€, PAF + íˆíŠ¸ë§µ)")
logger.info("   - HRNet (ê³ ì •ë°€, ì„œë¸Œí”½ì…€ ì •í™•ë„)")

logger.info("ğŸ¯ í•µì‹¬ ê¸°ëŠ¥ë“¤:")
logger.info("   - 17ê°œ COCO keypoints ì™„ì „ ê²€ì¶œ")
logger.info("   - ê´€ì ˆ ê°ë„ + ì‹ ì²´ ë¹„ìœ¨ ê³„ì‚°")
logger.info("   - í¬ì¦ˆ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ")
logger.info("   - ì˜ë¥˜ë³„ í¬ì¦ˆ ì í•©ì„± ë¶„ì„")
logger.info("   - ìŠ¤ì¼ˆë ˆí†¤ êµ¬ì¡° ìƒì„±")
logger.info("   - ì„œë¸Œí”½ì…€ ì •í™•ë„ ì§€ì›")

logger.info(f"ğŸ“Š ì‹œìŠ¤í…œ: PyTorch={TORCH_AVAILABLE}, Device={DEVICE}")
logger.info(f"ğŸ¤– AI ë¼ì´ë¸ŒëŸ¬ë¦¬: YOLO={ULTRALYTICS_AVAILABLE}, MediaPipe={MEDIAPIPE_AVAILABLE}")
logger.info(f"ğŸ”§ ë¼ì´ë¸ŒëŸ¬ë¦¬: OpenCV={OPENCV_AVAILABLE}, Transformers={TRANSFORMERS_AVAILABLE}")
logger.info("ğŸš€ Production Ready - Central Hub DI Container v7.0!")

# ==============================================
# ğŸ”¥ 11. ë©”ì¸ ì‹¤í–‰ë¶€
# ==============================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¯ MyCloset AI Step 02 - Pose Estimation")
    print("ğŸ”¥ Central Hub DI Container v7.0 ì™„ì „ ë¦¬íŒ©í† ë§")
    print("=" * 80)
    
    async def run_all_tests():
        await test_pose_estimation()
        print("\n" + "=" * 80)
        test_pose_algorithms()
    
    try:
        asyncio.run(run_all_tests())
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 80)
    print("âœ¨ Pose Estimation Step í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("ğŸ”¥ Central Hub DI Container v7.0 ì™„ì „ ì—°ë™")
    print("ğŸ§  MediaPipe + YOLOv8 + OpenPose + HRNet í†µí•©")
    print("ğŸ¯ 17ê°œ COCO keypoints ì™„ì „ ê²€ì¶œ")
    print("âš¡ ì‹¤ì œ AI ì¶”ë¡  + ë‹¤ì¤‘ ëª¨ë¸ í´ë°±")
    print("ğŸ“Š ê´€ì ˆ ê°ë„ + ì‹ ì²´ ë¹„ìœ¨ + í¬ì¦ˆ í’ˆì§ˆ í‰ê°€")
    print("ğŸ’‰ ì™„ì „í•œ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´")
    print("ğŸ”’ BaseStepMixin v20.0 ì™„ì „ í˜¸í™˜")
    print("ğŸš€ Production Ready!")
    print("=" * 80)