#!/usr/bin/env python3
"""
ðŸ”¥ MyCloset AI - Data Processing Mixin
=====================================

ë°ì´í„° ì „ì²˜ë¦¬ ë° í›„ì²˜ë¦¬ ê´€ë ¨ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” Mixin í´ëž˜ìŠ¤
- ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ë¦¬ì‚¬ì´ì¦ˆ, ì •ê·œí™”, í…ì„œ ë³€í™˜)
- ëª¨ë¸ë³„ ìž…ë ¥ ì¤€ë¹„ (SAM, Diffusion, OOTD ë“±)
- í›„ì²˜ë¦¬ (Softmax, Argmax, NMS ë“±)
- ë°ì´í„° ê²€ì¦ ë° ë³€í™˜

Author: MyCloset AI Team
Date: 2025-08-14
Version: 2.0
"""

import logging
from typing import Dict, Any, Optional, Tuple, List, Union

# ì„ íƒì  import
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

class DataProcessingMixin:
    """ë°ì´í„° ì „ì²˜ë¦¬ ë° í›„ì²˜ë¦¬ ê´€ë ¨ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” Mixin"""
    
    def _apply_preprocessing_sync(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì „ì²˜ë¦¬ ì ìš© (ë™ê¸° ë²„ì „)"""
        try:
            if not hasattr(self, 'config') or not self.config.auto_preprocessing:
                return input_data
            
            processed = input_data.copy()
            
            # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
            if self.config.input_size:
                processed = self._resize_images(processed, self.config.input_size)
            
            # ì •ê·œí™” ì ìš©
            if self.config.normalization_type:
                processed = self._apply_normalization(processed, self.config.normalization_type)
            
            # í…ì„œ ë³€í™˜
            if self.config.convert_to_tensor:
                processed = self._convert_to_tensor(processed)
            
            # ëª¨ë¸ë³„ íŠ¹í™” ì „ì²˜ë¦¬
            if hasattr(self, 'model_type'):
                processed = self._apply_model_specific_preprocessing(processed)
            
            self.logger.debug(f"âœ… {self.step_name} ì „ì²˜ë¦¬ ì™„ë£Œ")
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return input_data
    
    def _resize_images(self, data: Dict[str, Any], target_size: Tuple[int, int]) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if PIL_AVAILABLE and isinstance(value, Image.Image):
                    result[key] = value.resize(target_size, Image.Resampling.LANCZOS)
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if len(value.shape) == 3:
                        # (H, W, C) â†’ (C, H, W) ë³€í™˜ í›„ ë¦¬ì‚¬ì´ì¦ˆ
                        if value.shape[2] in [1, 3, 4]:
                            value = np.transpose(value, (2, 0, 1))
                        
                        # ê°„ë‹¨í•œ ë¦¬ì‚¬ì´ì¦ˆ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• ì‚¬ìš© ê¶Œìž¥)
                        from skimage.transform import resize
                        try:
                            resized = resize(value, (value.shape[0], target_size[1], target_size[0]))
                            result[key] = resized
                        except ImportError:
                            # skimageê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ë°©ë²• ì‚¬ìš©
                            result[key] = value
                    else:
                        result[key] = value
                else:
                    result[key] = value
                    
            except Exception as e:
                self.logger.debug(f"ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨ ({key}): {e}")
                result[key] = value
        
        return result
    
    def _apply_normalization(self, data: Dict[str, Any], norm_type: str) -> Dict[str, Any]:
        """ì •ê·œí™” ì ìš©"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if norm_type == "imagenet":
                    result[key] = self._normalize_imagenet(value)
                elif norm_type == "clip":
                    result[key] = self._normalize_clip(value)
                elif norm_type == "diffusion":
                    result[key] = self._normalize_diffusion(value)
                elif norm_type == "centered":
                    result[key] = self._normalize_centered(value)
                else:
                    result[key] = value
                    
            except Exception as e:
                self.logger.debug(f"ì •ê·œí™” ì‹¤íŒ¨ ({key}): {e}")
                result[key] = value
        
        return result
    
    def _normalize_imagenet(self, value: Any) -> Any:
        """ImageNet ì •ê·œí™”"""
        if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            
            if len(value.shape) == 3 and value.shape[0] == 3:
                # (C, H, W) í˜•íƒœ
                for i in range(3):
                    value[i] = (value[i] - mean[i]) / std[i]
            elif len(value.shape) == 3 and value.shape[2] == 3:
                # (H, W, C) í˜•íƒœ
                value = (value - mean) / std
            
        return value
    
    def _normalize_clip(self, value: Any) -> Any:
        """CLIP ì •ê·œí™”"""
        if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
            mean = np.array([0.48145466, 0.4578275, 0.40821073])
            std = np.array([0.26862954, 0.26130258, 0.27577711])
            
            if len(value.shape) == 3 and value.shape[0] == 3:
                for i in range(3):
                    value[i] = (value[i] - mean[i]) / std[i]
            elif len(value.shape) == 3 and value.shape[2] == 3:
                value = (value - mean) / std
        
        return value
    
    def _normalize_diffusion(self, value: Any) -> Any:
        """Diffusion ëª¨ë¸ ì •ê·œí™”"""
        if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
            # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
            if value.max() > 1.0 or value.min() < 0.0:
                value = (value - 0.5) * 2.0
        
        return value
    
    def _normalize_centered(self, value: Any) -> Any:
        """ì¤‘ì•™ ì •ê·œí™”"""
        if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
            # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
            if value.max() > 1.0 or value.min() < 0.0:
                value = (value - 0.5) * 2.0
        
        return value
    
    def _convert_to_tensor(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = torch.from_numpy(value).float()
                elif TORCH_AVAILABLE and PIL_AVAILABLE and isinstance(value, Image.Image):
                    array = np.array(value).astype(np.float32)
                    if len(array.shape) == 3 and array.shape[2] in [1, 3, 4]:
                        array = np.transpose(array, (2, 0, 1))
                    result[key] = torch.from_numpy(array).float()
                else:
                    result[key] = value
                    
            except Exception as e:
                self.logger.debug(f"í…ì„œ ë³€í™˜ ì‹¤íŒ¨ ({key}): {e}")
                result[key] = value
        
        return result
    
    def _apply_model_specific_preprocessing(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ íŠ¹í™” ì „ì²˜ë¦¬"""
        try:
            model_type = getattr(self, 'model_type', '').lower()
            
            if 'sam' in model_type:
                return self._prepare_sam_prompts(data)
            elif 'diffusion' in model_type:
                return self._prepare_diffusion_input(data)
            elif 'ootd' in model_type:
                return self._prepare_ootd_inputs(data)
            elif 'sr' in model_type or 'super' in model_type:
                return self._prepare_sr_input(data)
            else:
                return data
                
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ë³„ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return data
    
    def _prepare_sam_prompts(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """SAM í”„ë¡¬í”„íŠ¸ ì¤€ë¹„"""
        result = data.copy()
        
        if 'prompt_points' not in result and 'image' in result:
            if PIL_AVAILABLE and isinstance(result['image'], Image.Image):
                w, h = result['image'].size
                result['prompt_points'] = np.array([[w//2, h//2]])
                result['prompt_labels'] = np.array([1])
        
        return result
    
    def _prepare_diffusion_input(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Diffusion ëª¨ë¸ ìž…ë ¥ ì¤€ë¹„"""
        result = data.copy()
        
        if 'guidance_scale' not in result:
            result['guidance_scale'] = 7.5
        if 'num_inference_steps' not in result:
            result['num_inference_steps'] = 20
        if 'strength' not in result:
            result['strength'] = 0.8
        
        return result
    
    def _prepare_ootd_inputs(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """OOTD Diffusion ìž…ë ¥ ì¤€ë¹„"""
        result = data.copy()
        
        if 'fitting_mode' not in result:
            result['fitting_mode'] = 'hd'
        
        return result
    
    def _prepare_sr_input(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Super Resolution ìž…ë ¥ ì¤€ë¹„"""
        result = data.copy()
        
        if 'tile_size' not in result:
            result['tile_size'] = 512
        if 'overlap' not in result:
            result['overlap'] = 64
        
        return result
    
    def _apply_postprocessing_sync(self, ai_result: Dict[str, Any]) -> Dict[str, Any]:
        """í›„ì²˜ë¦¬ ì ìš© (ë™ê¸° ë²„ì „)"""
        try:
            if not hasattr(self, 'config') or not self.config.auto_postprocessing:
                return ai_result
            
            processed = ai_result.copy()
            
            # í›„ì²˜ë¦¬ ë‹¨ê³„ë“¤ ì ìš©
            if hasattr(self, 'postprocessing_steps'):
                for step_name in self.postprocessing_steps:
                    processed = self._apply_postprocessing_step(processed, step_name)
            
            self.logger.debug(f"âœ… {self.step_name} í›„ì²˜ë¦¬ ì™„ë£Œ")
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ai_result
    
    def _apply_postprocessing_step(self, data: Dict[str, Any], step_name: str) -> Dict[str, Any]:
        """ê°œë³„ í›„ì²˜ë¦¬ ë‹¨ê³„ ì ìš©"""
        try:
            if step_name == "softmax":
                return self._apply_softmax(data)
            elif step_name == "argmax":
                return self._apply_argmax(data)
            elif step_name == "resize_original":
                return self._resize_to_original(data)
            elif step_name == "to_numpy":
                return self._convert_to_numpy(data)
            elif step_name == "threshold_0.5":
                return self._apply_threshold(data, 0.5)
            elif step_name == "nms":
                return self._apply_nms(data)
            elif step_name in ["denormalize_diffusion", "denormalize_centered"]:
                return self._denormalize_diffusion(data)
            elif step_name == "denormalize":
                return self._denormalize_imagenet(data)
            elif step_name in ["clip_values", "clip_0_1"]:
                return self._clip_values(data, 0.0, 1.0)
            else:
                self.logger.debug(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” í›„ì²˜ë¦¬ ë‹¨ê³„: {step_name}")
                return data
                
        except Exception as e:
            self.logger.debug(f"í›„ì²˜ë¦¬ ë‹¨ê³„ ì‹¤íŒ¨ ({step_name}): {e}")
            return data
    
    def _apply_softmax(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Softmax ì ìš©"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and isinstance(value, torch.Tensor):
                    result[key] = torch.softmax(value, dim=-1)
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    from scipy.special import softmax
                    result[key] = softmax(value, axis=-1)
                else:
                    result[key] = value
            except Exception as e:
                self.logger.debug(f"Softmax ì ìš© ì‹¤íŒ¨ ({key}): {e}")
                result[key] = value
        
        return result
    
    def _apply_argmax(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Argmax ì ìš©"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and isinstance(value, torch.Tensor):
                    result[key] = torch.argmax(value, dim=-1)
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = np.argmax(value, axis=-1)
                else:
                    result[key] = value
            except Exception as e:
                self.logger.debug(f"Argmax ì ìš© ì‹¤íŒ¨ ({key}): {e}")
                result[key] = value
        
        return result
    
    def _apply_threshold(self, data: Dict[str, Any], threshold: float) -> Dict[str, Any]:
        """ìž„ê³„ê°’ ì ìš©"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and isinstance(value, torch.Tensor):
                    result[key] = (value > threshold).float()
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = (value > threshold).astype(np.float32)
                else:
                    result[key] = value
            except Exception as e:
                self.logger.debug(f"ìž„ê³„ê°’ ì ìš© ì‹¤íŒ¨ ({key}): {e}")
                result[key] = value
        
        return result
    
    def _clip_values(self, data: Dict[str, Any], min_val: float, max_val: float) -> Dict[str, Any]:
        """ê°’ ë²”ìœ„ ì œí•œ"""
        result = data.copy()
        
        for key, value in data.items():
            try:
                if TORCH_AVAILABLE and isinstance(value, torch.Tensor):
                    result[key] = torch.clamp(value, min_val, max_val)
                elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    result[key] = np.clip(value, min_val, max_val)
                else:
                    result[key] = value
            except Exception as e:
                self.logger.debug(f"ê°’ ë²”ìœ„ ì œí•œ ì‹¤íŒ¨ ({key}): {e}")
                result[key] = value
        
        return result
