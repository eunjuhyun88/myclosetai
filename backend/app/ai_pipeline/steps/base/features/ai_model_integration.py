#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - AI Model Integration Mixin
==========================================

AI ëª¨ë¸ í†µí•© ë° ì¶”ë¡  ì‹¤í–‰ì„ ë‹´ë‹¹í•˜ëŠ” Mixin í´ë˜ìŠ¤
- ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬
- AI ì¶”ë¡  ì‹¤í–‰
- ì…ë ¥/ì¶œë ¥ ë°ì´í„° ë³€í™˜
- ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

Author: MyCloset AI Team
Date: 2025-08-14
Version: 2.0
"""

import logging
import time
from typing import Dict, Any, Optional, Tuple, List, Union
from pathlib import Path

# ì„ íƒì  import
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None

class AIModelIntegrationMixin:
    """AI ëª¨ë¸ í†µí•© ë° ì¶”ë¡  ì‹¤í–‰ì„ ë‹´ë‹¹í•˜ëŠ” Mixin"""
    
    def _run_ai_inference(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """AI ì¶”ë¡  ì‹¤í–‰ - ê¸°ë³¸ êµ¬í˜„"""
        try:
            self.logger.info(f"ğŸ”¥ {self.step_name} AI ì¶”ë¡  ì‹œì‘")
            start_time = time.time()
            
            # ê¸°ë³¸ ì¶”ë¡  ë¡œì§
            result = self._run_step_specific_inference(input_data)
            
            # ì„±ëŠ¥ ì¸¡ì •
            processing_time = time.time() - start_time
            self._update_performance_metrics('ai_inference', processing_time, True)
            
            self.logger.info(f"âœ… {self.step_name} AI ì¶”ë¡  ì™„ë£Œ ({processing_time:.3f}ì´ˆ)")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            self._update_performance_metrics('ai_inference', 0.0, False, str(e))
            return self._create_error_response(str(e))
    
    def _run_step_specific_inference(self, input_data: Dict[str, Any], checkpoint_data: Any = None, device: str = None) -> Dict[str, Any]:
        """Stepë³„ íŠ¹í™” ì¶”ë¡  ì‹¤í–‰ - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„"""
        try:
            # ê¸°ë³¸ êµ¬í˜„: ì…ë ¥ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
            self.logger.debug(f"ğŸ”„ {self.step_name} ê¸°ë³¸ ì¶”ë¡  ì‹¤í–‰")
            
            # ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ” ê²½ìš° ë¡œë“œ
            if checkpoint_data and hasattr(self, '_load_checkpoint'):
                self._load_checkpoint(checkpoint_data)
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            target_device = device or self.device
            
            # ëª¨ë¸ì´ ìˆëŠ” ê²½ìš° ì¶”ë¡  ì‹¤í–‰
            if hasattr(self, 'model') and self.model is not None:
                return self._execute_model_inference(input_data, target_device)
            
            # ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° Mock ê²°ê³¼ ë°˜í™˜
            return self._create_mock_inference_result(input_data)
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} íŠ¹í™” ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return self._create_error_response(str(e))
    
    def _execute_model_inference(self, input_data: Dict[str, Any], device: str) -> Dict[str, Any]:
        """ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
        try:
            if not TORCH_AVAILABLE:
                return self._create_mock_inference_result(input_data)
            
            # ì…ë ¥ ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜
            tensor_input = self._convert_input_to_tensor(input_data)
            
            # ëª¨ë¸ì„ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if hasattr(self.model, 'to'):
                self.model.to(device)
            
            # ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                if hasattr(self.model, 'forward'):
                    output = self.model(tensor_input)
                elif hasattr(self.model, 'predict'):
                    output = self.model.predict(tensor_input)
                elif hasattr(self.model, 'detect'):
                    output = self.model.detect(tensor_input)
                else:
                    return self._create_mock_inference_result(input_data)
            
            # ì¶œë ¥ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            return self._convert_model_output_to_standard(output)
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return self._create_mock_inference_result(input_data)
    
    def _convert_input_to_tensor(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì…ë ¥ ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        result = {}
        
        for key, value in input_data.items():
            try:
                if NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                    if len(value.shape) == 3 and value.shape[2] in [1, 3, 4]:
                        value = np.transpose(value, (2, 0, 1))
                    tensor = torch.from_numpy(value).float()
                    
                    # MPS ë””ë°”ì´ìŠ¤ì—ì„œ float64 â†’ float32 ë³€í™˜
                    if self.device == 'mps' and tensor.dtype == torch.float64:
                        tensor = tensor.to(torch.float32)
                    
                    result[key] = tensor
                    
                elif PIL_AVAILABLE and isinstance(value, Image.Image):
                    array = np.array(value).astype(np.float32)
                    if len(array.shape) == 3 and array.shape[2] in [1, 3, 4]:
                        array = np.transpose(array, (2, 0, 1))
                    tensor = torch.from_numpy(array)
                    
                    # MPS ë””ë°”ì´ìŠ¤ì—ì„œ float64 â†’ float32 ë³€í™˜
                    if self.device == 'mps' and tensor.dtype == torch.float64:
                        tensor = tensor.to(torch.float32)
                    
                    result[key] = tensor
                    
                else:
                    result[key] = value
                    
            except Exception as e:
                self.logger.debug(f"í…ì„œ ë³€í™˜ ì‹¤íŒ¨ ({key}): {e}")
                result[key] = value
        
        return result
    
    def _convert_model_output_to_standard(self, model_output: Any) -> Dict[str, Any]:
        """ëª¨ë¸ ì¶œë ¥ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            if isinstance(model_output, torch.Tensor):
                # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
                if NUMPY_AVAILABLE:
                    output_array = model_output.detach().cpu().numpy()
                else:
                    output_array = model_output.detach().cpu().tolist()
                
                return {
                    'output': output_array,
                    'output_type': 'tensor',
                    'shape': list(model_output.shape),
                    'dtype': str(model_output.dtype)
                }
            
            elif isinstance(model_output, dict):
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì¶œë ¥
                converted = {}
                for key, value in model_output.items():
                    if isinstance(value, torch.Tensor):
                        if NUMPY_AVAILABLE:
                            converted[key] = value.detach().cpu().numpy()
                        else:
                            converted[key] = value.detach().cpu().tolist()
                    else:
                        converted[key] = value
                
                return converted
            
            else:
                # ê¸°íƒ€ í˜•íƒœì˜ ì¶œë ¥
                return {
                    'output': model_output,
                    'output_type': type(model_output).__name__
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì¶œë ¥ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return {
                'output': model_output,
                'error': str(e)
            }
    
    def _create_mock_inference_result(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Mock ì¶”ë¡  ê²°ê³¼ ìƒì„±"""
        return {
            'status': 'mock',
            'step_name': self.step_name,
            'input_keys': list(input_data.keys()),
            'message': 'Mock ì¶”ë¡  ê²°ê³¼ - ì‹¤ì œ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ'
        }
    
    def _load_primary_model(self):
        """ì£¼ìš” ëª¨ë¸ ë¡œë“œ"""
        try:
            if hasattr(self, 'model_loader') and self.model_loader:
                self.model = self.model_loader.load_primary_model()
                self.has_model = True
                self.model_loaded = True
                self.logger.info(f"âœ… {self.step_name} ì£¼ìš” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                self.logger.warning(f"âš ï¸ {self.step_name} model_loaderê°€ ì—†ìŒ")
                return False
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì£¼ìš” ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _update_performance_metrics(self, operation: str, duration: float, success: bool, error: str = None):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        try:
            if hasattr(self, 'performance_metrics'):
                self.performance_metrics.record_operation(operation, duration, success, error)
        except Exception as e:
            self.logger.debug(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            'success': False,
            'error': error_message,
            'step_name': self.step_name,
            'step_id': getattr(self, 'step_id', 0)
        }
