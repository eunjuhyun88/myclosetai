#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Advanced Data Management Mixin
===============================================

ê³ ê¸‰ ë°ì´í„° ê´€ë¦¬ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” Mixin í´ë˜ìŠ¤
- DetailedDataSpec ê´€ë¦¬
- ë°ì´í„° ì™„ì „ì„± ê²€ì¦
- ë©”ëª¨ë¦¬ ìµœì í™”
- ë°ì´í„° íë¦„ ë¶„ì„

Author: MyCloset AI Team
Date: 2025-08-14
Version: 2.0
"""

import logging
import time
from typing import Dict, Any, Optional, List, Tuple, Union

# ì„ íƒì  import
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

class AdvancedDataManagementMixin:
    """ê³ ê¸‰ ë°ì´í„° ê´€ë¦¬ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” Mixin"""

    def _load_detailed_data_spec_from_kwargs(self, **kwargs):
        """StepFactoryì—ì„œ ì£¼ì…ë°›ì€ DetailedDataSpec ì •ë³´ ë¡œë”©"""
        try:
            from .central_hub import DetailedDataSpecConfig
            
            config = DetailedDataSpecConfig(
                # ì…ë ¥ ì‚¬ì–‘
                input_data_types=kwargs.get('input_data_types', []),
                input_shapes=kwargs.get('input_shapes', {}),
                input_value_ranges=kwargs.get('input_value_ranges', {}),
                preprocessing_required=kwargs.get('preprocessing_required', []),
                
                # ì¶œë ¥ ì‚¬ì–‘
                output_data_types=kwargs.get('output_data_types', []),
                output_shapes=kwargs.get('output_shapes', {}),
                output_value_ranges=kwargs.get('output_value_ranges', {}),
                postprocessing_required=kwargs.get('postprocessing_required', []),
                
                # API í˜¸í™˜ì„±
                api_input_mapping=kwargs.get('api_input_mapping', {}),
                api_output_mapping=kwargs.get('api_output_mapping', {}),
                
                # Step ê°„ ì—°ë™
                step_input_schema=kwargs.get('step_input_schema', {}),
                step_output_schema=kwargs.get('step_output_schema', {}),
                
                # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­
                normalization_mean=kwargs.get('normalization_mean', (0.485, 0.456, 0.406)),
                normalization_std=kwargs.get('normalization_std', (0.229, 0.224, 0.225)),
                preprocessing_steps=kwargs.get('preprocessing_steps', []),
                postprocessing_steps=kwargs.get('postprocessing_steps', []),
                
                # Step ê°„ ë°ì´í„° ì „ë‹¬ ìŠ¤í‚¤ë§ˆ
                accepts_from_previous_step=kwargs.get('accepts_from_previous_step', {}),
                provides_to_next_step=kwargs.get('provides_to_next_step', {})
            )
            
            self.detailed_data_spec = config
            self.logger.debug(f"âœ… {self.step_name} DetailedDataSpec ë¡œë”© ì™„ë£Œ")
            return config
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} DetailedDataSpec ë¡œë”© ì‹¤íŒ¨: {e}")
            return self._create_emergency_detailed_data_spec()

    def _validate_data_conversion_readiness(self) -> bool:
        """ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ìƒíƒœ ê²€ì¦ (ì›Œë‹ ë°©ì§€)"""
        try:
            # DetailedDataSpec ì¡´ì¬ í™•ì¸ ë° ìë™ ìƒì„±
            if not hasattr(self, 'detailed_data_spec') or not self.detailed_data_spec:
                self._create_emergency_detailed_data_spec()
                self.logger.debug(f"âœ… {self.step_name} DetailedDataSpec ê¸°ë³¸ê°’ ìë™ ìƒì„±")
            
            # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ í™•ì¸ ë° ìë™ ë³´ì™„
            missing_fields = []
            required_fields = ['input_data_types', 'output_data_types', 'api_input_mapping', 'api_output_mapping']
            
            for field in required_fields:
                if not hasattr(self.detailed_data_spec, field):
                    missing_fields.append(field)
                else:
                    value = getattr(self.detailed_data_spec, field)
                    if not value:
                        missing_fields.append(field)
            
            # ëˆ„ë½ëœ í•„ë“œ ìë™ ë³´ì™„
            if missing_fields:
                self._fill_missing_fields(missing_fields)
                self.logger.debug(f"{self.step_name} DetailedDataSpec í•„ë“œ ë³´ì™„: {missing_fields}")
            
            # dependency_manager ìƒíƒœ ì—…ë°ì´íŠ¸
            if hasattr(self, 'dependency_manager') and self.dependency_manager:
                self.dependency_manager.dependency_status.detailed_data_spec_loaded = True
                self.dependency_manager.dependency_status.data_conversion_ready = True
            
            self.logger.debug(f"âœ… {self.step_name} DetailedDataSpec ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ë°ì´í„° ë³€í™˜ ì¤€ë¹„ ìƒíƒœ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    def _create_emergency_detailed_data_spec(self):
        """ì‘ê¸‰ DetailedDataSpec ìƒì„±"""
        try:
            if not hasattr(self, 'detailed_data_spec') or not self.detailed_data_spec:
                class EmergencyDataSpec:
                    def __init__(self):
                        self.input_data_types = {
                            'person_image': 'PIL.Image.Image',
                            'clothing_image': 'PIL.Image.Image',
                            'data': 'Any'
                        }
                        self.output_data_types = {
                            'result': 'numpy.ndarray',
                            'confidence': 'float',
                            'metadata': 'dict'
                        }
                        self.api_input_mapping = {
                            'person_image': 'person_image',
                            'clothing_image': 'clothing_image'
                        }
                        self.api_output_mapping = {
                            'result': 'result',
                            'confidence': 'confidence'
                        }
                        self.preprocessing_steps = ['resize', 'normalize']
                        self.postprocessing_steps = ['denormalize', 'to_numpy']
                
                self.detailed_data_spec = EmergencyDataSpec()
                self.logger.info(f"âœ… {self.step_name} ì‘ê¸‰ DetailedDataSpec ìƒì„± ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ì‘ê¸‰ DetailedDataSpec ìƒì„± ì‹¤íŒ¨: {e}")

    def _fill_missing_fields(self, missing_fields: List[str]):
        """ëˆ„ë½ëœ í•„ë“œ ìë™ ë³´ì™„"""
        try:
            for field in missing_fields:
                if field == 'input_data_types':
                    self.detailed_data_spec.input_data_types = {
                        'image': 'PIL.Image.Image',
                        'data': 'Any'
                    }
                elif field == 'output_data_types':
                    self.detailed_data_spec.output_data_types = {
                        'result': 'numpy.ndarray',
                        'confidence': 'float'
                    }
                elif field == 'api_input_mapping':
                    self.detailed_data_spec.api_input_mapping = {
                        'image': 'image',
                        'data': 'data'
                    }
                elif field == 'api_output_mapping':
                    self.detailed_data_spec.api_output_mapping = {
                        'result': 'result',
                        'confidence': 'confidence'
                    }
            
            self.logger.debug(f"âœ… {self.step_name} ëˆ„ë½ëœ í•„ë“œ ë³´ì™„ ì™„ë£Œ: {missing_fields}")
            
        except Exception as e:
            self.logger.error(f"âŒ {self.step_name} ëˆ„ë½ëœ í•„ë“œ ë³´ì™„ ì‹¤íŒ¨: {e}")

    def _optimize_memory_usage(self, data: Dict[str, Any], target_device: str = None) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        optimized_data = {}
        memory_saved_mb = 0.0
        
        try:
            target_device = target_device or getattr(self, 'device', 'cpu')
            
            for key, value in data.items():
                try:
                    # í…ì„œ ìµœì í™”
                    if TORCH_AVAILABLE and torch.is_tensor(value):
                        original_size = value.element_size() * value.nelement() / (1024 * 1024)
                        
                        # ë””ë°”ì´ìŠ¤ ìµœì í™”
                        if target_device == "cpu" and value.device.type != "cpu":
                            value = value.cpu()
                        elif target_device == "mps" and value.device.type != "mps":
                            value = value.to("mps")
                        
                        # FP16 ë³€í™˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
                        if hasattr(self, 'config') and getattr(self.config, 'use_fp16', False):
                            if value.dtype == torch.float32:
                                value = value.half()
                        
                        optimized_size = value.element_size() * value.nelement() / (1024 * 1024)
                        memory_saved_mb += (original_size - optimized_size)
                        
                    # NumPy ë°°ì—´ ìµœì í™”
                    elif NUMPY_AVAILABLE and isinstance(value, np.ndarray):
                        original_size = value.nbytes / (1024 * 1024)
                        
                        # ë¶ˆí•„ìš”í•œ ë³µì‚¬ ë°©ì§€
                        if value.flags['C_CONTIGUOUS']:
                            optimized_value = value
                        else:
                            optimized_value = np.ascontiguousarray(value)
                        
                        # ë°ì´í„° íƒ€ì… ìµœì í™”
                        if hasattr(self, 'config') and getattr(self.config, 'use_fp16', False):
                            if value.dtype == np.float64:
                                optimized_value = optimized_value.astype(np.float16)
                        
                        optimized_size = optimized_value.nbytes / (1024 * 1024)
                        memory_saved_mb += (original_size - optimized_size)
                        value = optimized_value
                    
                    optimized_data[key] = value
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {key} ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
                    optimized_data[key] = value
            
            if memory_saved_mb > 0:
                self.logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {memory_saved_mb:.2f}MB ì ˆì•½")
                if hasattr(self, 'performance_metrics'):
                    self.performance_metrics.memory_optimizations += 1
            
            return optimized_data
            
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return data

    def _analyze_di_container_data_flow(self, step_result: Dict[str, Any], step_id: int) -> Dict[str, Any]:
        """DI Container ë°ì´í„° íë¦„ ë¶„ì„"""
        analysis_result = {
            'di_container_used': False,
            'services_accessed': [],
            'data_flow_path': [],
            'memory_optimizations': 0,
            'data_transfers': 0,
            'errors': []
        }
        
        try:
            # DI Container ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            if hasattr(self, 'di_container') and self.di_container:
                analysis_result['di_container_used'] = True
                
                # Central Hub ì„œë¹„ìŠ¤ ì ‘ê·¼ í™•ì¸
                central_hub_services = ['memory_manager', 'model_loader', 'data_converter']
                for service_name in central_hub_services:
                    try:
                        service = self.get_service(service_name)
                        if service:
                            analysis_result['services_accessed'].append(service_name)
                    except Exception as e:
                        analysis_result['errors'].append(f"ì„œë¹„ìŠ¤ ì ‘ê·¼ ì‹¤íŒ¨ ({service_name}): {e}")
                
                # ë°ì´í„° íë¦„ ê²½ë¡œ ë¶„ì„
                if hasattr(self, 'detailed_data_spec'):
                    provides_to_next = getattr(self.detailed_data_spec, 'provides_to_next_step', {})
                    for next_step, data_mapping in provides_to_next.items():
                        analysis_result['data_flow_path'].append({
                            'from_step': step_id,
                            'to_step': next_step,
                            'data_keys': list(data_mapping.keys())
                        })
                        analysis_result['data_transfers'] += 1
                
                # ë©”ëª¨ë¦¬ ìµœì í™” í™•ì¸
                if hasattr(self, 'performance_metrics'):
                    analysis_result['memory_optimizations'] = getattr(
                        self.performance_metrics, 'memory_optimizations', 0
                    )
            
            # ë¡œê¹…
            if analysis_result['di_container_used']:
                self.logger.info(f"ğŸ”— DI Container ë°ì´í„° íë¦„ ë¶„ì„ ì™„ë£Œ")
                self.logger.debug(f"   - ì‚¬ìš©ëœ ì„œë¹„ìŠ¤: {analysis_result['services_accessed']}")
                self.logger.debug(f"   - ë°ì´í„° ì „ë‹¬ ê²½ë¡œ: {len(analysis_result['data_flow_path'])}ê°œ")
            else:
                self.logger.warning(f"âš ï¸ DI Container ë¯¸ì‚¬ìš©")
            
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"âŒ DI Container ë°ì´í„° íë¦„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            analysis_result['errors'].append(f"ë¶„ì„ ì˜¤ë¥˜: {e}")
            return analysis_result

    def _create_data_transfer_report(self, step_id: int, step_result: Dict[str, Any], 
                                   processing_time: float) -> Dict[str, Any]:
        """ë°ì´í„° ì „ë‹¬ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            'step_id': step_id,
            'step_name': getattr(self, 'step_name', 'Unknown'),
            'processing_time': processing_time,
            'timestamp': time.time(),
            'data_completeness': {},
            'memory_usage': {},
            'di_container_analysis': {},
            'performance_metrics': {},
            'warnings': [],
            'errors': []
        }
        
        try:
            # 1. ë°ì´í„° ì™„ì „ì„± ê²€ì¦
            if hasattr(self, 'detailed_data_spec'):
                expected_outputs = getattr(self.detailed_data_spec, 'provides_to_next_step', {})
                all_expected_keys = []
                for data_mapping in expected_outputs.values():
                    all_expected_keys.extend(data_mapping.keys())
                
                if all_expected_keys:
                    report['data_completeness'] = self._validate_data_completeness(
                        step_result, all_expected_keys
                    )
            
            # 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
            if hasattr(self, 'performance_metrics'):
                report['memory_usage'] = {
                    'peak_memory_mb': getattr(self.performance_metrics, 'peak_memory_usage_mb', 0),
                    'average_memory_mb': getattr(self.performance_metrics, 'average_memory_usage_mb', 0),
                    'optimizations_count': getattr(self.performance_metrics, 'memory_optimizations', 0)
                }
            
            # 3. DI Container ë¶„ì„
            report['di_container_analysis'] = self._analyze_di_container_data_flow(step_result, step_id)
            
            # 4. ì„±ëŠ¥ ë©”íŠ¸ë¦­
            if hasattr(self, 'performance_metrics'):
                report['performance_metrics'] = {
                    'data_conversions': getattr(self.performance_metrics, 'data_conversions', 0),
                    'step_data_transfers': getattr(self.performance_metrics, 'step_data_transfers', 0),
                    'validation_failures': getattr(self.performance_metrics, 'validation_failures', 0),
                    'api_conversions': getattr(self.performance_metrics, 'api_conversions', 0)
                }
            
            # 5. ê²½ê³  ë° ì˜¤ë¥˜ ìˆ˜ì§‘
            if not report['data_completeness'].get('is_complete', True):
                report['warnings'].append("ë°ì´í„° ì™„ì „ì„± ê²€ì¦ ì‹¤íŒ¨")
            
            if report['di_container_analysis'].get('errors'):
                report['errors'].extend(report['di_container_analysis']['errors'])
            
            # 6. ë¦¬í¬íŠ¸ ë¡œê¹…
            self.logger.info(f"ğŸ“Š Step {step_id} ë°ì´í„° ì „ë‹¬ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
            if report['warnings']:
                self.logger.warning(f"âš ï¸ ê²½ê³ : {len(report['warnings'])}ê°œ")
            if report['errors']:
                self.logger.error(f"âŒ ì˜¤ë¥˜: {len(report['errors'])}ê°œ")
            
            return report
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ì „ë‹¬ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            report['errors'].append(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return report

    def _validate_data_completeness(self, step_result: Dict[str, Any], expected_keys: List[str]) -> Dict[str, Any]:
        """ë°ì´í„° ì™„ì „ì„± ê²€ì¦"""
        try:
            present_keys = []
            missing_keys = []
            
            for key in expected_keys:
                if key in step_result and step_result[key] is not None:
                    present_keys.append(key)
                else:
                    missing_keys.append(key)
            
            is_complete = len(missing_keys) == 0
            
            return {
                'is_complete': is_complete,
                'present_keys': present_keys,
                'missing_keys': missing_keys,
                'completeness_ratio': len(present_keys) / len(expected_keys) if expected_keys else 1.0
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ì™„ì „ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {
                'is_complete': False,
                'present_keys': [],
                'missing_keys': expected_keys,
                'completeness_ratio': 0.0,
                'error': str(e)
            }

    def _log_comprehensive_process_report(self, process_report: Dict[str, Any], processing_time: float):
        """ì¢…í•© í”„ë¡œì„¸ìŠ¤ ë¦¬í¬íŠ¸ ë¡œê¹…"""
        try:
            step_name = process_report.get('step_name', 'Unknown')
            step_id = process_report.get('step_id', 0)
            
            # ğŸ”¥ ë‹¨ê³„ë³„ ì„±ëŠ¥ ë¶„ì„
            total_stage_time = 0
            stage_details = []
            
            for stage in process_report.get('processing_stages', []):
                stage_time = stage.get('duration', 0)
                total_stage_time += stage_time
                stage_details.append(f"{stage['stage']}: {stage_time:.3f}s")
                
                if not stage.get('success', True):
                    self.logger.warning(f"âš ï¸ {stage['stage']} ë‹¨ê³„ ì‹¤íŒ¨: {stage.get('error', 'Unknown error')}")
            
            # ğŸ”¥ ë°ì´í„° ì „ë‹¬ ë¶„ì„
            data_report = process_report.get('data_transfer_report', {})
            data_completeness = data_report.get('data_completeness', {})
            memory_usage = data_report.get('memory_usage', {})
            
            # ğŸ”¥ DI Container ë¶„ì„
            di_analysis = process_report.get('di_container_analysis', {})
            
            # ğŸ”¥ ì¢…í•© ë¡œê¹…
            self.logger.info(f"ğŸ“Š {step_name} (Step {step_id}) ì¢…í•© ë¦¬í¬íŠ¸")
            self.logger.info(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.3f}ì´ˆ")
            if stage_details:
                self.logger.info(f"   - ë‹¨ê³„ë³„ ì‹œê°„: {' | '.join(stage_details)}")
            
            # ë°ì´í„° ì™„ì „ì„±
            if data_completeness:
                completeness = data_completeness.get('is_complete', False)
                missing_count = len(data_completeness.get('missing_keys', []))
                present_count = len(data_completeness.get('present_keys', []))
                
                if completeness:
                    self.logger.info(f"   - ë°ì´í„° ì™„ì „ì„±: âœ… ({present_count}ê°œ í¬í•¨)")
                else:
                    self.logger.warning(f"   - ë°ì´í„° ì™„ì „ì„±: âŒ ({missing_count}ê°œ ëˆ„ë½)")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            if memory_usage:
                peak_memory = memory_usage.get('peak_memory_mb', 0)
                optimizations = memory_usage.get('optimizations_count', 0)
                self.logger.info(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {peak_memory:.2f}MB (ìµœì í™”: {optimizations}íšŒ)")
            
            # DI Container ìƒíƒœ
            if di_analysis:
                di_used = di_analysis.get('di_container_used', False)
                services_accessed = di_analysis.get('services_accessed', [])
                data_transfers = di_analysis.get('data_transfers', 0)
                
                if di_used:
                    self.logger.info(f"   - DI Container: âœ… ({len(services_accessed)}ê°œ ì„œë¹„ìŠ¤, {data_transfers}ê°œ ì „ë‹¬)")
                else:
                    self.logger.warning(f"   - DI Container: âŒ ë¯¸ì‚¬ìš©")
            
            # ê²½ê³  ë° ì˜¤ë¥˜ ìš”ì•½
            warnings_count = len(process_report.get('warnings', []))
            errors_count = len(process_report.get('errors', []))
            
            if warnings_count > 0:
                self.logger.warning(f"   - ê²½ê³ : {warnings_count}ê°œ")
            if errors_count > 0:
                self.logger.error(f"   - ì˜¤ë¥˜: {errors_count}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì¢…í•© ë¦¬í¬íŠ¸ ë¡œê¹… ì‹¤íŒ¨: {e}")
