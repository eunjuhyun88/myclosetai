#!/usr/bin/env python3
"""
ğŸ”¥ MyCloset AI - Step 09: Final Output - ê³ ê¸‰ ì‹ ê²½ë§ ê¸°ë°˜ ìµœì¢… ì¶œë ¥ í†µí•©
================================================================================

âœ… ê³ ê¸‰ ì‹ ê²½ë§ êµ¬ì¡° (Transformer, Attention, Integration)
âœ… ë…¼ë¬¸ ìˆ˜ì¤€ ìµœì¢… ì¶œë ¥ í†µí•© ì‹œìŠ¤í…œ
âœ… ë‹¤ì¤‘ ëª¨ë‹¬ë¦¬í‹° ì¶œë ¥ ìƒì„±
âœ… ì‹¤ì œ AI ëª¨ë¸ í™œìš©

Author: MyCloset AI Team
Date: 2025-08-13
Version: 1.0
"""

import os
import sys
import logging
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Tuple
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent.parent

# PyTorch ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torchvision import transforms
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from PIL import Image
    import cv2
    IMAGE_LIBS_AVAILABLE = True
except ImportError:
    IMAGE_LIBS_AVAILABLE = False

# BaseStepMixin ë™ì  ë¡œë“œ
def get_base_step_mixin_class():
    """BaseStepMixin í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œ"""
    try:
        # ë°©ë²• 1: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€
        sys.path.insert(0, str(project_root))
        from backend.app.ai_pipeline.steps.base.base_step_mixin import BaseStepMixin
        return BaseStepMixin
    except ImportError:
        try:
            # ë°©ë²• 2: í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€
            sys.path.insert(0, str(current_dir.parent.parent.parent))
            from app.ai_pipeline.steps.base.base_step_mixin import BaseStepMixin
            return BaseStepMixin
        except ImportError:
            try:
                # ë°©ë²• 3: ì§ì ‘ ê²½ë¡œ
                sys.path.insert(0, str(current_dir.parent.parent.parent.parent))
                from backend.app.ai_pipeline.steps.base.base_step_mixin import BaseStepMixin
                return BaseStepMixin
            except ImportError:
                # ë°©ë²• 4: ìƒëŒ€ ê²½ë¡œ ì‹œë„
                sys.path.insert(0, str(current_dir.parent.parent.parent.parent))
                from ...base.base_step_mixin import BaseStepMixin
                return BaseStepMixin

# BaseStepMixin ë¡œë“œ
BaseStepMixin = get_base_step_mixin_class()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==============================================
# ğŸ”¥ ê³ ê¸‰ ì‹ ê²½ë§ êµ¬ì¡°ë“¤ (ë…¼ë¬¸ ìˆ˜ì¤€)
# ==============================================

if TORCH_AVAILABLE:
    class MultiHeadSelfAttention(nn.Module):
        """Multi-Head Self-Attention ë©”ì»¤ë‹ˆì¦˜"""
        def __init__(self, d_model: int, num_heads: int = 8, dropout: float = 0.1):
            super().__init__()
            assert d_model % num_heads == 0
            
            self.d_model = d_model
            self.num_heads = num_heads
            self.d_k = d_model // num_heads
            
            self.w_q = nn.Linear(d_model, d_model)
            self.w_k = nn.Linear(d_model, d_model)
            self.w_v = nn.Linear(d_model, d_model)
            self.w_o = nn.Linear(d_model, d_model)
            
            self.dropout = nn.Dropout(dropout)
            self.scale = self.d_k ** -0.5
        
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            batch_size, seq_len, d_model = x.size()
            
            # Q, K, V ê³„ì‚°
            Q = self.w_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
            K = self.w_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
            V = self.w_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
            
            # Attention ê³„ì‚°
            scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale
            attn = F.softmax(scores, dim=-1)
            attn = self.dropout(attn)
            
            # ì¶œë ¥ ê³„ì‚°
            out = torch.matmul(attn, V)
            out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
            out = self.w_o(out)
            
            return out

    class TransformerBlock(nn.Module):
        """Transformer ë¸”ë¡"""
        def __init__(self, d_model: int, num_heads: int = 8, d_ff: int = 2048, dropout: float = 0.1):
            super().__init__()
            self.attention = MultiHeadSelfAttention(d_model, num_heads, dropout)
            self.norm1 = nn.LayerNorm(d_model)
            self.norm2 = nn.LayerNorm(d_model)
            
            self.feed_forward = nn.Sequential(
                nn.Linear(d_model, d_ff),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(d_ff, d_model)
            )
            
            self.dropout = nn.Dropout(dropout)
        
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            # Self-Attention + Residual
            attn_out = self.attention(x)
            x = self.norm1(x + self.dropout(attn_out))
            
            # Feed-Forward + Residual
            ff_out = self.feed_forward(x)
            x = self.norm2(x + self.dropout(ff_out))
            
            return x

    class OutputIntegrationTransformer(nn.Module):
        """ì¶œë ¥ í†µí•©ìš© Transformer ëª¨ë¸"""
        def __init__(self, config: Dict[str, Any] = None):
            super().__init__()
            self.config = config or {}
            
            # ëª¨ë¸ íŒŒë¼ë¯¸í„°
            self.d_model = self.config.get('d_model', 512)
            self.num_layers = self.config.get('num_layers', 4)
            self.num_heads = self.config.get('num_heads', 8)
            self.d_ff = self.config.get('d_ff', 2048)
            self.dropout = self.config.get('dropout', 0.1)
            
            # íŠ¹ì§• ì¶”ì¶œê¸° (CNN)
            self.feature_extractor = nn.Sequential(
                nn.Conv2d(3, 64, 7, stride=2, padding=3),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.MaxPool2d(3, stride=2, padding=1),
                
                nn.Conv2d(64, 128, 3, stride=2, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(),
                
                nn.Conv2d(128, 256, 3, stride=2, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(),
                
                nn.Conv2d(256, self.d_model, 3, stride=2, padding=1),
                nn.BatchNorm2d(self.d_model),
                nn.ReLU(),
                
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten()
            )
            
            # Transformer ë ˆì´ì–´ë“¤
            self.transformer_layers = nn.ModuleList([
                TransformerBlock(self.d_model, self.num_heads, self.d_ff, self.dropout)
                for _ in range(self.num_layers)
            ])
            
            # ì¶œë ¥ í†µí•© í—¤ë“œë“¤
            self.integration_heads = nn.ModuleDict({
                'final_image': nn.Linear(self.d_model, 3 * 64 * 64),  # ìµœì¢… ì´ë¯¸ì§€ ìƒì„±
                'confidence': nn.Linear(self.d_model, 1),              # ì‹ ë¢°ë„
                'quality_score': nn.Linear(self.d_model, 1),          # í’ˆì§ˆ ì ìˆ˜
                'metadata': nn.Linear(self.d_model, 128)              # ë©”íƒ€ë°ì´í„°
            })
            
            # ì¶œë ¥ í™œì„±í™”
            self.sigmoid = nn.Sigmoid()
            self.tanh = nn.Tanh()
        
        def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
            # íŠ¹ì§• ì¶”ì¶œ
            features = self.feature_extractor(x)
            features = features.unsqueeze(1)  # [batch, 1, d_model]
            
            # Transformer ì²˜ë¦¬
            for transformer in self.transformer_layers:
                features = transformer(features)
            
            # ì¶œë ¥ í†µí•©
            integration_outputs = {}
            for name, head in self.integration_heads.items():
                if name == 'final_image':
                    # ì´ë¯¸ì§€ ì¶œë ¥: [batch, 3*64*64] -> [batch, 3, 64, 64]
                    img_output = head(features.squeeze(1))
                    img_output = img_output.view(-1, 3, 64, 64)
                    integration_outputs[name] = self.tanh(img_output)  # [-1, 1] ë²”ìœ„
                elif name == 'confidence':
                    integration_outputs[name] = self.sigmoid(head(features.squeeze(1)))
                elif name == 'quality_score':
                    integration_outputs[name] = self.sigmoid(head(features.squeeze(1)))
                else:
                    integration_outputs[name] = head(features.squeeze(1))
            
            return integration_outputs

    class CrossModalAttention(nn.Module):
        """í¬ë¡œìŠ¤ ëª¨ë‹¬ ì–´í…ì…˜"""
        def __init__(self, d_model: int, num_heads: int = 8):
            super().__init__()
            self.d_model = d_model
            self.num_heads = num_heads
            self.attention = MultiHeadSelfAttention(d_model, num_heads)
            
            # ëª¨ë‹¬ë¦¬í‹°ë³„ íŠ¹ì§• ë³€í™˜
            self.image_proj = nn.Linear(d_model, d_model)
            self.text_proj = nn.Linear(d_model, d_model)
            self.metadata_proj = nn.Linear(d_model, d_model)
        
        def forward(self, image_features: torch.Tensor, text_features: torch.Tensor, 
                   metadata_features: torch.Tensor) -> torch.Tensor:
            # ëª¨ë‹¬ë¦¬í‹°ë³„ íŠ¹ì§• ë³€í™˜
            image_proj = self.image_proj(image_features)
            text_proj = self.text_proj(text_features)
            metadata_proj = self.metadata_proj(metadata_features)
            
            # í†µí•© íŠ¹ì§•
            combined_features = torch.cat([image_proj, text_proj, metadata_proj], dim=1)
            
            # Cross-Attention ì ìš©
            attended_features = self.attention(combined_features)
            
            return attended_features

    class FinalOutputGenerator(nn.Module):
        """ìµœì¢… ì¶œë ¥ ìƒì„±ê¸°"""
        def __init__(self, config: Dict[str, Any] = None):
            super().__init__()
            self.config = config or {}
            
            self.d_model = self.config.get('d_model', 512)
            
            # ì¶œë ¥ í†µí•© Transformer
            self.integration_transformer = OutputIntegrationTransformer(config)
            
            # í¬ë¡œìŠ¤ ëª¨ë‹¬ ì–´í…ì…˜
            self.cross_modal_attention = CrossModalAttention(self.d_model)
            
            # ìµœì¢… ì¶œë ¥ í—¤ë“œ
            self.final_output_head = nn.Sequential(
                nn.Linear(self.d_model * 3, 256),  # 3ê°œ ëª¨ë‹¬ë¦¬í‹°
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Linear(64, 3),  # RGB ì¶œë ¥
                nn.Tanh()
            )
            
            # í’ˆì§ˆ í‰ê°€ í—¤ë“œ
            self.quality_head = nn.Sequential(
                nn.Linear(self.d_model * 3, 128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Linear(64, 1),
                nn.Sigmoid()
            )
        
        def forward(self, image_features: torch.Tensor, text_features: torch.Tensor,
                   metadata_features: torch.Tensor) -> Dict[str, torch.Tensor]:
            # í¬ë¡œìŠ¤ ëª¨ë‹¬ ì–´í…ì…˜
            cross_modal_features = self.cross_modal_attention(
                image_features, text_features, metadata_features
            )
            
            # ìµœì¢… ì¶œë ¥ ìƒì„±
            final_output = self.final_output_head(cross_modal_features.flatten(1))
            quality_score = self.quality_head(cross_modal_features.flatten(1))
            
            return {
                'final_output': final_output,
                'quality_score': quality_score,
                'cross_modal_features': cross_modal_features
            }

# ==============================================
# ğŸ”¥ ì¶œë ¥ í†µí•© ì‹œìŠ¤í…œ
# ==============================================

class OutputIntegrationSystem:
    """ì¶œë ¥ í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = logger
    
    def integrate_step_outputs(self, step_outputs: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ê³„ë³„ ì¶œë ¥ë“¤ì„ í†µí•©"""
        try:
            self.logger.info("ğŸ”— ë‹¨ê³„ë³„ ì¶œë ¥ í†µí•© ì‹œì‘...")
            
            integrated_output = {
                'pipeline_version': 'v1.0',
                'total_steps': len(step_outputs),
                'integration_timestamp': self._get_timestamp(),
                'step_results': {},
                'final_metrics': {},
                'quality_assessment': {},
                'output_summary': {}
            }
            
            # ê° ë‹¨ê³„ ê²°ê³¼ í†µí•©
            for step_name, step_result in step_outputs.items():
                if step_result and isinstance(step_result, dict):
                    integrated_output['step_results'][step_name] = {
                        'status': step_result.get('status', 'unknown'),
                        'version': step_result.get('step_version', 'unknown'),
                        'processing_time': step_result.get('processing_time', 0.0),
                        'device_used': step_result.get('device_used', 'unknown')
                    }
                    
                    # ì„±ê³µí•œ ë‹¨ê³„ì˜ ê²°ê³¼ ë°ì´í„° ì¶”ì¶œ
                    if step_result.get('status') == 'success':
                        self._extract_step_data(step_name, step_result, integrated_output)
            
            # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
            integrated_output['final_metrics'] = self._calculate_final_metrics(integrated_output)
            
            # í’ˆì§ˆ í‰ê°€ í†µí•©
            integrated_output['quality_assessment'] = self._integrate_quality_assessment(integrated_output)
            
            # ì¶œë ¥ ìš”ì•½ ìƒì„±
            integrated_output['output_summary'] = self._generate_output_summary(integrated_output)
            
            self.logger.info("âœ… ë‹¨ê³„ë³„ ì¶œë ¥ í†µí•© ì™„ë£Œ")
            return integrated_output
            
        except Exception as e:
            self.logger.error(f"âŒ ì¶œë ¥ í†µí•© ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return {'error': str(e)}
    
    def _extract_step_data(self, step_name: str, step_result: Dict[str, Any], 
                          integrated_output: Dict[str, Any]):
        """ë‹¨ê³„ë³„ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # AI ì¶”ë¡  ê²°ê³¼ ì¶”ì¶œ
            if 'ai_quality_assessment' in step_result:
                integrated_output.setdefault('ai_results', {})[step_name] = \
                    step_result['ai_quality_assessment']
            
            # ì „í†µì  ë©”íŠ¸ë¦­ ì¶”ì¶œ
            if 'traditional_metrics' in step_result:
                integrated_output.setdefault('traditional_metrics', {})[step_name] = \
                    step_result['traditional_metrics']
            
            # ê¸°íƒ€ ê²°ê³¼ ë°ì´í„°
            for key, value in step_result.items():
                if key not in ['status', 'step_version', 'processing_time', 'device_used']:
                    integrated_output.setdefault('additional_data', {})[step_name] = \
                        {key: value}
                        
        except Exception as e:
            self.logger.warning(f"âš ï¸ {step_name} ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    def _calculate_final_metrics(self, integrated_output: Dict[str, Any]) -> Dict[str, Any]:
        """ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            final_metrics = {
                'total_processing_time': 0.0,
                'success_rate': 0.0,
                'average_quality_score': 0.0,
                'step_completion_status': {}
            }
            
            step_results = integrated_output.get('step_results', {})
            total_steps = len(step_results)
            successful_steps = 0
            
            for step_name, step_data in step_results.items():
                # ì²˜ë¦¬ ì‹œê°„ ëˆ„ì 
                final_metrics['total_processing_time'] += step_data.get('processing_time', 0.0)
                
                # ì„±ê³µë¥  ê³„ì‚°
                if step_data.get('status') == 'success':
                    successful_steps += 1
                    final_metrics['step_completion_status'][step_name] = 'completed'
                else:
                    final_metrics['step_completion_status'][step_name] = 'failed'
            
            # ìµœì¢… ê³„ì‚°
            if total_steps > 0:
                final_metrics['success_rate'] = successful_steps / total_steps
            
            # í’ˆì§ˆ ì ìˆ˜ í‰ê·  ê³„ì‚°
            quality_scores = []
            ai_results = integrated_output.get('ai_results', {})
            for step_data in ai_results.values():
                if isinstance(step_data, dict):
                    # Transformer ì ìˆ˜ ì¶”ì¶œ
                    if 'transformer_scores' in step_data:
                        transformer_scores = step_data['transformer_scores']
                        if 'overall' in transformer_scores:
                            quality_scores.append(transformer_scores['overall'][0][0])
                    
                    # ì•™ìƒë¸” ê²°ê³¼ ì¶”ì¶œ
                    if 'ensemble_result' in step_data:
                        ensemble_result = step_data['ensemble_result']
                        if 'final_quality' in ensemble_result:
                            quality_scores.append(ensemble_result['final_quality'][0][0])
            
            if quality_scores:
                final_metrics['average_quality_score'] = sum(quality_scores) / len(quality_scores)
            
            return final_metrics
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _integrate_quality_assessment(self, integrated_output: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ í‰ê°€ í†µí•©"""
        try:
            quality_assessment = {
                'overall_quality': 0.0,
                'quality_breakdown': {},
                'recommendations': []
            }
            
            # AI ê²°ê³¼ì—ì„œ í’ˆì§ˆ ì ìˆ˜ í†µí•©
            ai_results = integrated_output.get('ai_results', {})
            quality_scores = {}
            
            for step_name, step_data in ai_results.items():
                if isinstance(step_data, dict):
                    step_quality = {}
                    
                    # Transformer ì ìˆ˜
                    if 'transformer_scores' in step_data:
                        transformer_scores = step_data['transformer_scores']
                        for metric, score in transformer_scores.items():
                            if isinstance(score, list) and len(score) > 0:
                                step_quality[metric] = score[0][0]
                    
                    # ì•™ìƒë¸” ê²°ê³¼
                    if 'ensemble_result' in step_data:
                        ensemble_result = step_data['ensemble_result']
                        if 'final_quality' in ensemble_result:
                            step_quality['ensemble_final'] = ensemble_result['final_quality'][0][0]
                    
                    if step_quality:
                        quality_scores[step_name] = step_quality
            
            # í’ˆì§ˆ ì ìˆ˜ í†µí•©
            if quality_scores:
                quality_assessment['quality_breakdown'] = quality_scores
                
                # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                all_scores = []
                for step_scores in quality_scores.values():
                    all_scores.extend(step_scores.values())
                
                if all_scores:
                    quality_assessment['overall_quality'] = sum(all_scores) / len(all_scores)
            
            # ê¶Œì¥ì‚¬í•­ ìƒì„±
            if quality_assessment['overall_quality'] < 0.7:
                quality_assessment['recommendations'].append("ì „ë°˜ì ì¸ í’ˆì§ˆ ê°œì„  ê¶Œì¥")
            if quality_assessment['overall_quality'] < 0.5:
                quality_assessment['recommendations'].append("í’ˆì§ˆì´ ë‚®ìŒ - ì¬ì²˜ë¦¬ ê³ ë ¤")
            
            return quality_assessment
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ í‰ê°€ í†µí•© ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _generate_output_summary(self, integrated_output: Dict[str, Any]) -> Dict[str, Any]:
        """ì¶œë ¥ ìš”ì•½ ìƒì„±"""
        try:
            summary = {
                'pipeline_status': 'completed' if integrated_output.get('final_metrics', {}).get('success_rate', 0) > 0.8 else 'partial',
                'total_steps': integrated_output.get('total_steps', 0),
                'processing_time': integrated_output.get('final_metrics', {}).get('total_processing_time', 0.0),
                'overall_quality': integrated_output.get('quality_assessment', {}).get('overall_quality', 0.0),
                'key_achievements': [],
                'areas_for_improvement': []
            }
            
            # ì£¼ìš” ì„±ê³¼ ì‹ë³„
            final_metrics = integrated_output.get('final_metrics', {})
            if final_metrics.get('success_rate', 0) > 0.9:
                summary['key_achievements'].append("ë†’ì€ ì„±ê³µë¥  ë‹¬ì„±")
            if final_metrics.get('success_rate', 0) == 1.0:
                summary['key_achievements'].append("ëª¨ë“  ë‹¨ê³„ ì™„ë²½ ì‹¤í–‰")
            
            # ê°œì„  ì˜ì—­ ì‹ë³„
            if final_metrics.get('success_rate', 0) < 0.8:
                summary['areas_for_improvement'].append("ì„±ê³µë¥  ê°œì„  í•„ìš”")
            if integrated_output.get('quality_assessment', {}).get('overall_quality', 0) < 0.7:
                summary['areas_for_improvement'].append("í’ˆì§ˆ í–¥ìƒ í•„ìš”")
            
            return summary
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¶œë ¥ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _get_timestamp(self) -> str:
        """íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±"""
        from datetime import datetime
        return datetime.now().isoformat()

# ==============================================
# ğŸ”¥ ë©”ì¸ Final Output Step í´ë˜ìŠ¤
# ==============================================

class FinalOutputStep(BaseStepMixin):
    """ìµœì¢… ì¶œë ¥ Step - ê³ ê¸‰ ì‹ ê²½ë§ ê¸°ë°˜"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # Step ì •ë³´
        self.step_name = kwargs.get('step_name', '09_final_output')
        self.step_version = kwargs.get('step_version', '1.0')
        self.step_description = kwargs.get('step_description', 'ê³ ê¸‰ ì‹ ê²½ë§ ê¸°ë°˜ ìµœì¢… ì¶œë ¥ í†µí•©')
        
        # ì¥ì¹˜ ì„¤ì •
        self.device = kwargs.get('device', 'cpu')
        if TORCH_AVAILABLE and torch.cuda.is_available():
            self.device = 'cuda'
        elif TORCH_AVAILABLE and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            self.device = 'mps'
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_models()
        
        # ì¶œë ¥ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.output_integration = OutputIntegrationSystem()
        
        logger.info(f"âœ… FinalOutputStep ì´ˆê¸°í™” ì™„ë£Œ (ì¥ì¹˜: {self.device})")
    
    def _initialize_models(self):
        """AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        try:
            if TORCH_AVAILABLE:
                # ì¶œë ¥ í†µí•© Transformer ëª¨ë¸
                transformer_config = {
                    'd_model': 512,
                    'num_layers': 4,
                    'num_heads': 8,
                    'd_ff': 2048,
                    'dropout': 0.1
                }
                self.integration_transformer = OutputIntegrationTransformer(transformer_config).to(self.device)
                
                # í¬ë¡œìŠ¤ ëª¨ë‹¬ ì–´í…ì…˜ ëª¨ë¸
                cross_modal_config = {
                    'd_model': 512,
                    'num_heads': 8
                }
                self.cross_modal_attention = CrossModalAttention(512, 8).to(self.device)
                
                # ìµœì¢… ì¶œë ¥ ìƒì„±ê¸°
                generator_config = {
                    'd_model': 512,
                    'num_heads': 8,
                    'd_ff': 2048,
                    'dropout': 0.1
                }
                self.final_output_generator = FinalOutputGenerator(generator_config).to(self.device)
                
                logger.info("âœ… ê³ ê¸‰ ì‹ ê²½ë§ ëª¨ë¸ë“¤ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ PyTorch ì—†ìŒ - ëª¨ë¸ ì´ˆê¸°í™” ê±´ë„ˆëœ€")
                
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            traceback.print_exc()
    
    def _run_ai_inference(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """AI ì¶”ë¡  ì‹¤í–‰"""
        try:
            logger.info("ğŸ” AI ìµœì¢… ì¶œë ¥ ìƒì„± ì¶”ë¡  ì‹œì‘...")
            
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            if 'step_outputs' not in input_data:
                raise ValueError("ë‹¨ê³„ë³„ ì¶œë ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            step_outputs = input_data['step_outputs']
            
            # ë”ë¯¸ íŠ¹ì§• ìƒì„± (ì‹¤ì œë¡œëŠ” ê° ë‹¨ê³„ì˜ íŠ¹ì§•ì„ ì‚¬ìš©)
            batch_size = 1
            d_model = 512
            
            # ì´ë¯¸ì§€ íŠ¹ì§• (ë”ë¯¸)
            image_features = torch.randn(batch_size, 1, d_model).to(self.device)
            text_features = torch.randn(batch_size, 1, d_model).to(self.device)
            metadata_features = torch.randn(batch_size, 1, d_model).to(self.device)
            
            # ëª¨ë¸ ì¶”ë¡ 
            with torch.no_grad():
                # ì¶œë ¥ í†µí•© Transformer
                integration_output = self.integration_transformer(image_features)
                
                # í¬ë¡œìŠ¤ ëª¨ë‹¬ ì–´í…ì…˜
                cross_modal_features = self.cross_modal_attention(
                    image_features, text_features, metadata_features
                )
                
                # ìµœì¢… ì¶œë ¥ ìƒì„±
                final_output = self.final_output_generator(
                    image_features, text_features, metadata_features
                )
                
                # ê²°ê³¼ ì •ë¦¬
                ai_results = {
                    'integration_output': {k: v.cpu().numpy().tolist() if torch.is_tensor(v) else v 
                                         for k, v in integration_output.items()},
                    'cross_modal_features': cross_modal_features.cpu().numpy().tolist(),
                    'final_output': {k: v.cpu().numpy().tolist() if torch.is_tensor(v) else v 
                                   for k, v in final_output.items()}
                }
            
            logger.info("âœ… AI ìµœì¢… ì¶œë ¥ ìƒì„± ì¶”ë¡  ì™„ë£Œ")
            return ai_results
            
        except Exception as e:
            logger.error(f"âŒ AI ì¶”ë¡  ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return {'error': str(e)}
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ"""
        try:
            logger.info("ğŸ”— ìµœì¢… ì¶œë ¥ Step ì‹œì‘...")
            
            # ì…ë ¥ ê²€ì¦
            if not input_data:
                raise ValueError("ì…ë ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # AI ì¶”ë¡  ì‹¤í–‰
            ai_results = self._run_ai_inference(input_data)
            
            # ì¶œë ¥ í†µí•© ì‹¤í–‰
            step_outputs = input_data.get('step_outputs', {})
            integrated_output = self.output_integration.integrate_step_outputs(step_outputs)
            
            # AI ê²°ê³¼ í†µí•©
            if 'error' not in ai_results:
                integrated_output['ai_final_output'] = ai_results
            
            # ê²°ê³¼ í†µí•©
            result = {
                'step_name': self.step_name,
                'step_version': self.step_version,
                'status': 'success',
                'integrated_output': integrated_output,
                'ai_results': ai_results,
                'processing_time': 0.0,  # ì‹¤ì œë¡œëŠ” ì‹œê°„ ì¸¡ì •
                'device_used': self.device
            }
            
            logger.info("âœ… ìµœì¢… ì¶œë ¥ Step ì™„ë£Œ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ìµœì¢… ì¶œë ¥ Step ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return {
                'step_name': self.step_name,
                'step_version': self.step_version,
                'status': 'error',
                'error': str(e)
            }

# ==============================================
# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ==============================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # Final Output Step ìƒì„±
        step = FinalOutputStep()
        
        # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
        dummy_step_outputs = {
            'step_01': {
                'status': 'success',
                'step_version': '1.0',
                'processing_time': 1.5,
                'device_used': 'mps',
                'ai_quality_assessment': {
                    'transformer_scores': {
                        'overall': [[0.85]],
                        'sharpness': [[0.78]],
                        'color': [[0.92]]
                    },
                    'ensemble_result': {
                        'final_quality': [[0.88]]
                    }
                }
            },
            'step_02': {
                'status': 'success',
                'step_version': '1.0',
                'processing_time': 2.1,
                'device_used': 'mps'
            }
        }
        
        input_data = {
            'step_outputs': dummy_step_outputs
        }
        
        # ì²˜ë¦¬ ì‹¤í–‰
        result = step.process(input_data)
        
        logger.info("ğŸ‰ Final Output Step í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        logger.info(f"ê²°ê³¼: {result}")
        
    except Exception as e:
        logger.error(f"âŒ ë©”ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()
